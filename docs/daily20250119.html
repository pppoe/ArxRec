<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250116.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "AugRefer: Advancing 3D Visual Grounding via Cross-Modal Augmentation and\n  Spatial Relation-based Referring", "author": "Xinyi Wang and Na Zhao and Zhiyuan Han and Dan Guo and Xun Yang", "abstract": "  3D visual grounding (3DVG), which aims to correlate a natural language\ndescription with the target object within a 3D scene, is a significant yet\nchallenging task. Despite recent advancements in this domain, existing\napproaches commonly encounter a shortage: a limited amount and diversity of\ntext3D pairs available for training. Moreover, they fall short in effectively\nleveraging different contextual clues (e.g., rich spatial relations within the\n3D visual space) for grounding. To address these limitations, we propose\nAugRefer, a novel approach for advancing 3D visual grounding. AugRefer\nintroduces cross-modal augmentation designed to extensively generate diverse\ntext-3D pairs by placing objects into 3D scenes and creating accurate and\nsemantically rich descriptions using foundation models. Notably, the resulting\npairs can be utilized by any existing 3DVG methods for enriching their training\ndata. Additionally, AugRefer presents a language-spatial adaptive decoder that\neffectively adapts the potential referring objects based on the language\ndescription and various 3D spatial relations. Extensive experiments on three\nbenchmark datasets clearly validate the effectiveness of AugRefer.\n", "link": "http://arxiv.org/abs/2501.09428v1", "date": "2025-01-16", "relevancy": 3.0811, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6274}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6274}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AugRefer%3A%20Advancing%203D%20Visual%20Grounding%20via%20Cross-Modal%20Augmentation%20and%0A%20%20Spatial%20Relation-based%20Referring&body=Title%3A%20AugRefer%3A%20Advancing%203D%20Visual%20Grounding%20via%20Cross-Modal%20Augmentation%20and%0A%20%20Spatial%20Relation-based%20Referring%0AAuthor%3A%20Xinyi%20Wang%20and%20Na%20Zhao%20and%20Zhiyuan%20Han%20and%20Dan%20Guo%20and%20Xun%20Yang%0AAbstract%3A%20%20%203D%20visual%20grounding%20%283DVG%29%2C%20which%20aims%20to%20correlate%20a%20natural%20language%0Adescription%20with%20the%20target%20object%20within%20a%203D%20scene%2C%20is%20a%20significant%20yet%0Achallenging%20task.%20Despite%20recent%20advancements%20in%20this%20domain%2C%20existing%0Aapproaches%20commonly%20encounter%20a%20shortage%3A%20a%20limited%20amount%20and%20diversity%20of%0Atext3D%20pairs%20available%20for%20training.%20Moreover%2C%20they%20fall%20short%20in%20effectively%0Aleveraging%20different%20contextual%20clues%20%28e.g.%2C%20rich%20spatial%20relations%20within%20the%0A3D%20visual%20space%29%20for%20grounding.%20To%20address%20these%20limitations%2C%20we%20propose%0AAugRefer%2C%20a%20novel%20approach%20for%20advancing%203D%20visual%20grounding.%20AugRefer%0Aintroduces%20cross-modal%20augmentation%20designed%20to%20extensively%20generate%20diverse%0Atext-3D%20pairs%20by%20placing%20objects%20into%203D%20scenes%20and%20creating%20accurate%20and%0Asemantically%20rich%20descriptions%20using%20foundation%20models.%20Notably%2C%20the%20resulting%0Apairs%20can%20be%20utilized%20by%20any%20existing%203DVG%20methods%20for%20enriching%20their%20training%0Adata.%20Additionally%2C%20AugRefer%20presents%20a%20language-spatial%20adaptive%20decoder%20that%0Aeffectively%20adapts%20the%20potential%20referring%20objects%20based%20on%20the%20language%0Adescription%20and%20various%203D%20spatial%20relations.%20Extensive%20experiments%20on%20three%0Abenchmark%20datasets%20clearly%20validate%20the%20effectiveness%20of%20AugRefer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugRefer%253A%2520Advancing%25203D%2520Visual%2520Grounding%2520via%2520Cross-Modal%2520Augmentation%2520and%250A%2520%2520Spatial%2520Relation-based%2520Referring%26entry.906535625%3DXinyi%2520Wang%2520and%2520Na%2520Zhao%2520and%2520Zhiyuan%2520Han%2520and%2520Dan%2520Guo%2520and%2520Xun%2520Yang%26entry.1292438233%3D%2520%25203D%2520visual%2520grounding%2520%25283DVG%2529%252C%2520which%2520aims%2520to%2520correlate%2520a%2520natural%2520language%250Adescription%2520with%2520the%2520target%2520object%2520within%2520a%25203D%2520scene%252C%2520is%2520a%2520significant%2520yet%250Achallenging%2520task.%2520Despite%2520recent%2520advancements%2520in%2520this%2520domain%252C%2520existing%250Aapproaches%2520commonly%2520encounter%2520a%2520shortage%253A%2520a%2520limited%2520amount%2520and%2520diversity%2520of%250Atext3D%2520pairs%2520available%2520for%2520training.%2520Moreover%252C%2520they%2520fall%2520short%2520in%2520effectively%250Aleveraging%2520different%2520contextual%2520clues%2520%2528e.g.%252C%2520rich%2520spatial%2520relations%2520within%2520the%250A3D%2520visual%2520space%2529%2520for%2520grounding.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%250AAugRefer%252C%2520a%2520novel%2520approach%2520for%2520advancing%25203D%2520visual%2520grounding.%2520AugRefer%250Aintroduces%2520cross-modal%2520augmentation%2520designed%2520to%2520extensively%2520generate%2520diverse%250Atext-3D%2520pairs%2520by%2520placing%2520objects%2520into%25203D%2520scenes%2520and%2520creating%2520accurate%2520and%250Asemantically%2520rich%2520descriptions%2520using%2520foundation%2520models.%2520Notably%252C%2520the%2520resulting%250Apairs%2520can%2520be%2520utilized%2520by%2520any%2520existing%25203DVG%2520methods%2520for%2520enriching%2520their%2520training%250Adata.%2520Additionally%252C%2520AugRefer%2520presents%2520a%2520language-spatial%2520adaptive%2520decoder%2520that%250Aeffectively%2520adapts%2520the%2520potential%2520referring%2520objects%2520based%2520on%2520the%2520language%250Adescription%2520and%2520various%25203D%2520spatial%2520relations.%2520Extensive%2520experiments%2520on%2520three%250Abenchmark%2520datasets%2520clearly%2520validate%2520the%2520effectiveness%2520of%2520AugRefer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AugRefer%3A%20Advancing%203D%20Visual%20Grounding%20via%20Cross-Modal%20Augmentation%20and%0A%20%20Spatial%20Relation-based%20Referring&entry.906535625=Xinyi%20Wang%20and%20Na%20Zhao%20and%20Zhiyuan%20Han%20and%20Dan%20Guo%20and%20Xun%20Yang&entry.1292438233=%20%203D%20visual%20grounding%20%283DVG%29%2C%20which%20aims%20to%20correlate%20a%20natural%20language%0Adescription%20with%20the%20target%20object%20within%20a%203D%20scene%2C%20is%20a%20significant%20yet%0Achallenging%20task.%20Despite%20recent%20advancements%20in%20this%20domain%2C%20existing%0Aapproaches%20commonly%20encounter%20a%20shortage%3A%20a%20limited%20amount%20and%20diversity%20of%0Atext3D%20pairs%20available%20for%20training.%20Moreover%2C%20they%20fall%20short%20in%20effectively%0Aleveraging%20different%20contextual%20clues%20%28e.g.%2C%20rich%20spatial%20relations%20within%20the%0A3D%20visual%20space%29%20for%20grounding.%20To%20address%20these%20limitations%2C%20we%20propose%0AAugRefer%2C%20a%20novel%20approach%20for%20advancing%203D%20visual%20grounding.%20AugRefer%0Aintroduces%20cross-modal%20augmentation%20designed%20to%20extensively%20generate%20diverse%0Atext-3D%20pairs%20by%20placing%20objects%20into%203D%20scenes%20and%20creating%20accurate%20and%0Asemantically%20rich%20descriptions%20using%20foundation%20models.%20Notably%2C%20the%20resulting%0Apairs%20can%20be%20utilized%20by%20any%20existing%203DVG%20methods%20for%20enriching%20their%20training%0Adata.%20Additionally%2C%20AugRefer%20presents%20a%20language-spatial%20adaptive%20decoder%20that%0Aeffectively%20adapts%20the%20potential%20referring%20objects%20based%20on%20the%20language%0Adescription%20and%20various%203D%20spatial%20relations.%20Extensive%20experiments%20on%20three%0Abenchmark%20datasets%20clearly%20validate%20the%20effectiveness%20of%20AugRefer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09428v1&entry.124074799=Read"},
{"title": "The Devil is in the Details: Simple Remedies for Image-to-LiDAR\n  Representation Learning", "author": "Wonjun Jo and Kwon Byung-Ki and Kim Ji-Yeon and Hawook Jeong and Kyungdon Joo and Tae-Hyun Oh", "abstract": "  LiDAR is a crucial sensor in autonomous driving, commonly used alongside\ncameras. By exploiting this camera-LiDAR setup and recent advances in image\nrepresentation learning, prior studies have shown the promising potential of\nimage-to-LiDAR distillation. These prior arts focus on the designs of their own\nlosses to effectively distill the pre-trained 2D image representations into a\n3D model. However, the other parts of the designs have been surprisingly\nunexplored. We find that fundamental design elements, e.g., the LiDAR\ncoordinate system, quantization according to the existing input interface, and\ndata utilization, are more critical than developing loss functions, which have\nbeen overlooked in prior works. In this work, we show that simple fixes to\nthese designs notably outperform existing methods by 16% in 3D semantic\nsegmentation on the nuScenes dataset and 13% in 3D object detection on the\nKITTI dataset in downstream task performance. We focus on overlooked design\nchoices along the spatial and temporal axes. Spatially, prior work has used\ncylindrical coordinate and voxel sizes without considering their side effects\nyielded with a commonly deployed sparse convolution layer input interface,\nleading to spatial quantization errors in 3D models. Temporally, existing work\nhas avoided cumbersome data curation by discarding unsynced data, limiting the\nuse to only the small portion of data that is temporally synced across sensors.\nWe analyze these effects and propose simple solutions for each overlooked\naspect.\n", "link": "http://arxiv.org/abs/2501.09485v1", "date": "2025-01-16", "relevancy": 3.035, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6491}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Devil%20is%20in%20the%20Details%3A%20Simple%20Remedies%20for%20Image-to-LiDAR%0A%20%20Representation%20Learning&body=Title%3A%20The%20Devil%20is%20in%20the%20Details%3A%20Simple%20Remedies%20for%20Image-to-LiDAR%0A%20%20Representation%20Learning%0AAuthor%3A%20Wonjun%20Jo%20and%20Kwon%20Byung-Ki%20and%20Kim%20Ji-Yeon%20and%20Hawook%20Jeong%20and%20Kyungdon%20Joo%20and%20Tae-Hyun%20Oh%0AAbstract%3A%20%20%20LiDAR%20is%20a%20crucial%20sensor%20in%20autonomous%20driving%2C%20commonly%20used%20alongside%0Acameras.%20By%20exploiting%20this%20camera-LiDAR%20setup%20and%20recent%20advances%20in%20image%0Arepresentation%20learning%2C%20prior%20studies%20have%20shown%20the%20promising%20potential%20of%0Aimage-to-LiDAR%20distillation.%20These%20prior%20arts%20focus%20on%20the%20designs%20of%20their%20own%0Alosses%20to%20effectively%20distill%20the%20pre-trained%202D%20image%20representations%20into%20a%0A3D%20model.%20However%2C%20the%20other%20parts%20of%20the%20designs%20have%20been%20surprisingly%0Aunexplored.%20We%20find%20that%20fundamental%20design%20elements%2C%20e.g.%2C%20the%20LiDAR%0Acoordinate%20system%2C%20quantization%20according%20to%20the%20existing%20input%20interface%2C%20and%0Adata%20utilization%2C%20are%20more%20critical%20than%20developing%20loss%20functions%2C%20which%20have%0Abeen%20overlooked%20in%20prior%20works.%20In%20this%20work%2C%20we%20show%20that%20simple%20fixes%20to%0Athese%20designs%20notably%20outperform%20existing%20methods%20by%2016%25%20in%203D%20semantic%0Asegmentation%20on%20the%20nuScenes%20dataset%20and%2013%25%20in%203D%20object%20detection%20on%20the%0AKITTI%20dataset%20in%20downstream%20task%20performance.%20We%20focus%20on%20overlooked%20design%0Achoices%20along%20the%20spatial%20and%20temporal%20axes.%20Spatially%2C%20prior%20work%20has%20used%0Acylindrical%20coordinate%20and%20voxel%20sizes%20without%20considering%20their%20side%20effects%0Ayielded%20with%20a%20commonly%20deployed%20sparse%20convolution%20layer%20input%20interface%2C%0Aleading%20to%20spatial%20quantization%20errors%20in%203D%20models.%20Temporally%2C%20existing%20work%0Ahas%20avoided%20cumbersome%20data%20curation%20by%20discarding%20unsynced%20data%2C%20limiting%20the%0Ause%20to%20only%20the%20small%20portion%20of%20data%20that%20is%20temporally%20synced%20across%20sensors.%0AWe%20analyze%20these%20effects%20and%20propose%20simple%20solutions%20for%20each%20overlooked%0Aaspect.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Devil%2520is%2520in%2520the%2520Details%253A%2520Simple%2520Remedies%2520for%2520Image-to-LiDAR%250A%2520%2520Representation%2520Learning%26entry.906535625%3DWonjun%2520Jo%2520and%2520Kwon%2520Byung-Ki%2520and%2520Kim%2520Ji-Yeon%2520and%2520Hawook%2520Jeong%2520and%2520Kyungdon%2520Joo%2520and%2520Tae-Hyun%2520Oh%26entry.1292438233%3D%2520%2520LiDAR%2520is%2520a%2520crucial%2520sensor%2520in%2520autonomous%2520driving%252C%2520commonly%2520used%2520alongside%250Acameras.%2520By%2520exploiting%2520this%2520camera-LiDAR%2520setup%2520and%2520recent%2520advances%2520in%2520image%250Arepresentation%2520learning%252C%2520prior%2520studies%2520have%2520shown%2520the%2520promising%2520potential%2520of%250Aimage-to-LiDAR%2520distillation.%2520These%2520prior%2520arts%2520focus%2520on%2520the%2520designs%2520of%2520their%2520own%250Alosses%2520to%2520effectively%2520distill%2520the%2520pre-trained%25202D%2520image%2520representations%2520into%2520a%250A3D%2520model.%2520However%252C%2520the%2520other%2520parts%2520of%2520the%2520designs%2520have%2520been%2520surprisingly%250Aunexplored.%2520We%2520find%2520that%2520fundamental%2520design%2520elements%252C%2520e.g.%252C%2520the%2520LiDAR%250Acoordinate%2520system%252C%2520quantization%2520according%2520to%2520the%2520existing%2520input%2520interface%252C%2520and%250Adata%2520utilization%252C%2520are%2520more%2520critical%2520than%2520developing%2520loss%2520functions%252C%2520which%2520have%250Abeen%2520overlooked%2520in%2520prior%2520works.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520simple%2520fixes%2520to%250Athese%2520designs%2520notably%2520outperform%2520existing%2520methods%2520by%252016%2525%2520in%25203D%2520semantic%250Asegmentation%2520on%2520the%2520nuScenes%2520dataset%2520and%252013%2525%2520in%25203D%2520object%2520detection%2520on%2520the%250AKITTI%2520dataset%2520in%2520downstream%2520task%2520performance.%2520We%2520focus%2520on%2520overlooked%2520design%250Achoices%2520along%2520the%2520spatial%2520and%2520temporal%2520axes.%2520Spatially%252C%2520prior%2520work%2520has%2520used%250Acylindrical%2520coordinate%2520and%2520voxel%2520sizes%2520without%2520considering%2520their%2520side%2520effects%250Ayielded%2520with%2520a%2520commonly%2520deployed%2520sparse%2520convolution%2520layer%2520input%2520interface%252C%250Aleading%2520to%2520spatial%2520quantization%2520errors%2520in%25203D%2520models.%2520Temporally%252C%2520existing%2520work%250Ahas%2520avoided%2520cumbersome%2520data%2520curation%2520by%2520discarding%2520unsynced%2520data%252C%2520limiting%2520the%250Ause%2520to%2520only%2520the%2520small%2520portion%2520of%2520data%2520that%2520is%2520temporally%2520synced%2520across%2520sensors.%250AWe%2520analyze%2520these%2520effects%2520and%2520propose%2520simple%2520solutions%2520for%2520each%2520overlooked%250Aaspect.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Devil%20is%20in%20the%20Details%3A%20Simple%20Remedies%20for%20Image-to-LiDAR%0A%20%20Representation%20Learning&entry.906535625=Wonjun%20Jo%20and%20Kwon%20Byung-Ki%20and%20Kim%20Ji-Yeon%20and%20Hawook%20Jeong%20and%20Kyungdon%20Joo%20and%20Tae-Hyun%20Oh&entry.1292438233=%20%20LiDAR%20is%20a%20crucial%20sensor%20in%20autonomous%20driving%2C%20commonly%20used%20alongside%0Acameras.%20By%20exploiting%20this%20camera-LiDAR%20setup%20and%20recent%20advances%20in%20image%0Arepresentation%20learning%2C%20prior%20studies%20have%20shown%20the%20promising%20potential%20of%0Aimage-to-LiDAR%20distillation.%20These%20prior%20arts%20focus%20on%20the%20designs%20of%20their%20own%0Alosses%20to%20effectively%20distill%20the%20pre-trained%202D%20image%20representations%20into%20a%0A3D%20model.%20However%2C%20the%20other%20parts%20of%20the%20designs%20have%20been%20surprisingly%0Aunexplored.%20We%20find%20that%20fundamental%20design%20elements%2C%20e.g.%2C%20the%20LiDAR%0Acoordinate%20system%2C%20quantization%20according%20to%20the%20existing%20input%20interface%2C%20and%0Adata%20utilization%2C%20are%20more%20critical%20than%20developing%20loss%20functions%2C%20which%20have%0Abeen%20overlooked%20in%20prior%20works.%20In%20this%20work%2C%20we%20show%20that%20simple%20fixes%20to%0Athese%20designs%20notably%20outperform%20existing%20methods%20by%2016%25%20in%203D%20semantic%0Asegmentation%20on%20the%20nuScenes%20dataset%20and%2013%25%20in%203D%20object%20detection%20on%20the%0AKITTI%20dataset%20in%20downstream%20task%20performance.%20We%20focus%20on%20overlooked%20design%0Achoices%20along%20the%20spatial%20and%20temporal%20axes.%20Spatially%2C%20prior%20work%20has%20used%0Acylindrical%20coordinate%20and%20voxel%20sizes%20without%20considering%20their%20side%20effects%0Ayielded%20with%20a%20commonly%20deployed%20sparse%20convolution%20layer%20input%20interface%2C%0Aleading%20to%20spatial%20quantization%20errors%20in%203D%20models.%20Temporally%2C%20existing%20work%0Ahas%20avoided%20cumbersome%20data%20curation%20by%20discarding%20unsynced%20data%2C%20limiting%20the%0Ause%20to%20only%20the%20small%20portion%20of%20data%20that%20is%20temporally%20synced%20across%20sensors.%0AWe%20analyze%20these%20effects%20and%20propose%20simple%20solutions%20for%20each%20overlooked%0Aaspect.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09485v1&entry.124074799=Read"},
{"title": "DEFOM-Stereo: Depth Foundation Model Based Stereo Matching", "author": "Hualie Jiang and Zhiqiang Lou and Laiyan Ding and Rui Xu and Minglang Tan and Wenjie Jiang and Rui Huang", "abstract": "  Stereo matching is a key technique for metric depth estimation in computer\nvision and robotics. Real-world challenges like occlusion and non-texture\nhinder accurate disparity estimation from binocular matching cues. Recently,\nmonocular relative depth estimation has shown remarkable generalization using\nvision foundation models. Thus, to facilitate robust stereo matching with\nmonocular depth cues, we incorporate a robust monocular relative depth model\ninto the recurrent stereo-matching framework, building a new framework for\ndepth foundation model-based stereo-matching, DEFOM-Stereo. In the feature\nextraction stage, we construct the combined context and matching feature\nencoder by integrating features from conventional CNNs and DEFOM. In the update\nstage, we use the depth predicted by DEFOM to initialize the recurrent\ndisparity and introduce a scale update module to refine the disparity at the\ncorrect scale. DEFOM-Stereo is verified to have comparable performance on the\nScene Flow dataset with state-of-the-art (SOTA) methods and notably shows much\nstronger zero-shot generalization. Moreover, DEFOM-Stereo achieves SOTA\nperformance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks,\nranking 1st on many metrics. In the joint evaluation under the robust vision\nchallenge, our model simultaneously outperforms previous models on the\nindividual benchmarks. Both results demonstrate the outstanding capabilities of\nthe proposed model.\n", "link": "http://arxiv.org/abs/2501.09466v1", "date": "2025-01-16", "relevancy": 2.9951, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6131}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEFOM-Stereo%3A%20Depth%20Foundation%20Model%20Based%20Stereo%20Matching&body=Title%3A%20DEFOM-Stereo%3A%20Depth%20Foundation%20Model%20Based%20Stereo%20Matching%0AAuthor%3A%20Hualie%20Jiang%20and%20Zhiqiang%20Lou%20and%20Laiyan%20Ding%20and%20Rui%20Xu%20and%20Minglang%20Tan%20and%20Wenjie%20Jiang%20and%20Rui%20Huang%0AAbstract%3A%20%20%20Stereo%20matching%20is%20a%20key%20technique%20for%20metric%20depth%20estimation%20in%20computer%0Avision%20and%20robotics.%20Real-world%20challenges%20like%20occlusion%20and%20non-texture%0Ahinder%20accurate%20disparity%20estimation%20from%20binocular%20matching%20cues.%20Recently%2C%0Amonocular%20relative%20depth%20estimation%20has%20shown%20remarkable%20generalization%20using%0Avision%20foundation%20models.%20Thus%2C%20to%20facilitate%20robust%20stereo%20matching%20with%0Amonocular%20depth%20cues%2C%20we%20incorporate%20a%20robust%20monocular%20relative%20depth%20model%0Ainto%20the%20recurrent%20stereo-matching%20framework%2C%20building%20a%20new%20framework%20for%0Adepth%20foundation%20model-based%20stereo-matching%2C%20DEFOM-Stereo.%20In%20the%20feature%0Aextraction%20stage%2C%20we%20construct%20the%20combined%20context%20and%20matching%20feature%0Aencoder%20by%20integrating%20features%20from%20conventional%20CNNs%20and%20DEFOM.%20In%20the%20update%0Astage%2C%20we%20use%20the%20depth%20predicted%20by%20DEFOM%20to%20initialize%20the%20recurrent%0Adisparity%20and%20introduce%20a%20scale%20update%20module%20to%20refine%20the%20disparity%20at%20the%0Acorrect%20scale.%20DEFOM-Stereo%20is%20verified%20to%20have%20comparable%20performance%20on%20the%0AScene%20Flow%20dataset%20with%20state-of-the-art%20%28SOTA%29%20methods%20and%20notably%20shows%20much%0Astronger%20zero-shot%20generalization.%20Moreover%2C%20DEFOM-Stereo%20achieves%20SOTA%0Aperformance%20on%20the%20KITTI%202012%2C%20KITTI%202015%2C%20Middlebury%2C%20and%20ETH3D%20benchmarks%2C%0Aranking%201st%20on%20many%20metrics.%20In%20the%20joint%20evaluation%20under%20the%20robust%20vision%0Achallenge%2C%20our%20model%20simultaneously%20outperforms%20previous%20models%20on%20the%0Aindividual%20benchmarks.%20Both%20results%20demonstrate%20the%20outstanding%20capabilities%20of%0Athe%20proposed%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEFOM-Stereo%253A%2520Depth%2520Foundation%2520Model%2520Based%2520Stereo%2520Matching%26entry.906535625%3DHualie%2520Jiang%2520and%2520Zhiqiang%2520Lou%2520and%2520Laiyan%2520Ding%2520and%2520Rui%2520Xu%2520and%2520Minglang%2520Tan%2520and%2520Wenjie%2520Jiang%2520and%2520Rui%2520Huang%26entry.1292438233%3D%2520%2520Stereo%2520matching%2520is%2520a%2520key%2520technique%2520for%2520metric%2520depth%2520estimation%2520in%2520computer%250Avision%2520and%2520robotics.%2520Real-world%2520challenges%2520like%2520occlusion%2520and%2520non-texture%250Ahinder%2520accurate%2520disparity%2520estimation%2520from%2520binocular%2520matching%2520cues.%2520Recently%252C%250Amonocular%2520relative%2520depth%2520estimation%2520has%2520shown%2520remarkable%2520generalization%2520using%250Avision%2520foundation%2520models.%2520Thus%252C%2520to%2520facilitate%2520robust%2520stereo%2520matching%2520with%250Amonocular%2520depth%2520cues%252C%2520we%2520incorporate%2520a%2520robust%2520monocular%2520relative%2520depth%2520model%250Ainto%2520the%2520recurrent%2520stereo-matching%2520framework%252C%2520building%2520a%2520new%2520framework%2520for%250Adepth%2520foundation%2520model-based%2520stereo-matching%252C%2520DEFOM-Stereo.%2520In%2520the%2520feature%250Aextraction%2520stage%252C%2520we%2520construct%2520the%2520combined%2520context%2520and%2520matching%2520feature%250Aencoder%2520by%2520integrating%2520features%2520from%2520conventional%2520CNNs%2520and%2520DEFOM.%2520In%2520the%2520update%250Astage%252C%2520we%2520use%2520the%2520depth%2520predicted%2520by%2520DEFOM%2520to%2520initialize%2520the%2520recurrent%250Adisparity%2520and%2520introduce%2520a%2520scale%2520update%2520module%2520to%2520refine%2520the%2520disparity%2520at%2520the%250Acorrect%2520scale.%2520DEFOM-Stereo%2520is%2520verified%2520to%2520have%2520comparable%2520performance%2520on%2520the%250AScene%2520Flow%2520dataset%2520with%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520and%2520notably%2520shows%2520much%250Astronger%2520zero-shot%2520generalization.%2520Moreover%252C%2520DEFOM-Stereo%2520achieves%2520SOTA%250Aperformance%2520on%2520the%2520KITTI%25202012%252C%2520KITTI%25202015%252C%2520Middlebury%252C%2520and%2520ETH3D%2520benchmarks%252C%250Aranking%25201st%2520on%2520many%2520metrics.%2520In%2520the%2520joint%2520evaluation%2520under%2520the%2520robust%2520vision%250Achallenge%252C%2520our%2520model%2520simultaneously%2520outperforms%2520previous%2520models%2520on%2520the%250Aindividual%2520benchmarks.%2520Both%2520results%2520demonstrate%2520the%2520outstanding%2520capabilities%2520of%250Athe%2520proposed%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEFOM-Stereo%3A%20Depth%20Foundation%20Model%20Based%20Stereo%20Matching&entry.906535625=Hualie%20Jiang%20and%20Zhiqiang%20Lou%20and%20Laiyan%20Ding%20and%20Rui%20Xu%20and%20Minglang%20Tan%20and%20Wenjie%20Jiang%20and%20Rui%20Huang&entry.1292438233=%20%20Stereo%20matching%20is%20a%20key%20technique%20for%20metric%20depth%20estimation%20in%20computer%0Avision%20and%20robotics.%20Real-world%20challenges%20like%20occlusion%20and%20non-texture%0Ahinder%20accurate%20disparity%20estimation%20from%20binocular%20matching%20cues.%20Recently%2C%0Amonocular%20relative%20depth%20estimation%20has%20shown%20remarkable%20generalization%20using%0Avision%20foundation%20models.%20Thus%2C%20to%20facilitate%20robust%20stereo%20matching%20with%0Amonocular%20depth%20cues%2C%20we%20incorporate%20a%20robust%20monocular%20relative%20depth%20model%0Ainto%20the%20recurrent%20stereo-matching%20framework%2C%20building%20a%20new%20framework%20for%0Adepth%20foundation%20model-based%20stereo-matching%2C%20DEFOM-Stereo.%20In%20the%20feature%0Aextraction%20stage%2C%20we%20construct%20the%20combined%20context%20and%20matching%20feature%0Aencoder%20by%20integrating%20features%20from%20conventional%20CNNs%20and%20DEFOM.%20In%20the%20update%0Astage%2C%20we%20use%20the%20depth%20predicted%20by%20DEFOM%20to%20initialize%20the%20recurrent%0Adisparity%20and%20introduce%20a%20scale%20update%20module%20to%20refine%20the%20disparity%20at%20the%0Acorrect%20scale.%20DEFOM-Stereo%20is%20verified%20to%20have%20comparable%20performance%20on%20the%0AScene%20Flow%20dataset%20with%20state-of-the-art%20%28SOTA%29%20methods%20and%20notably%20shows%20much%0Astronger%20zero-shot%20generalization.%20Moreover%2C%20DEFOM-Stereo%20achieves%20SOTA%0Aperformance%20on%20the%20KITTI%202012%2C%20KITTI%202015%2C%20Middlebury%2C%20and%20ETH3D%20benchmarks%2C%0Aranking%201st%20on%20many%20metrics.%20In%20the%20joint%20evaluation%20under%20the%20robust%20vision%0Achallenge%2C%20our%20model%20simultaneously%20outperforms%20previous%20models%20on%20the%0Aindividual%20benchmarks.%20Both%20results%20demonstrate%20the%20outstanding%20capabilities%20of%0Athe%20proposed%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09466v1&entry.124074799=Read"},
{"title": "Instruction-Guided Fusion of Multi-Layer Visual Features in Large\n  Vision-Language Models", "author": "Xu Li and Yi Zheng and Haotian Chen and Xiaolei Chen and Yuxuan Liang and Chenghang Lai and Bin Li and Xiangyang Xue", "abstract": "  Large Vision-Language Models (LVLMs) have achieved significant success in\nmultimodal tasks by combining pre-trained vision encoders and large language\nmodels. However, current LVLMs mainly rely on features from the final layers of\nthe vision encoder, neglecting complementary information in shallower layers.\nWhile recent methods have explored multi-layer features, they are often\ntask-agnostic. We investigate the contributions of visual features from\ndifferent encoder layers across 18 benchmarks and 6 task categories. Our\nresults show that multi-layer features provide complementary strengths with\nvarying task dependencies, and uniform fusion performs suboptimally. Based on\nthese findings, we propose an instruction-guided vision aggregator that\ndynamically integrates multi-layer features based on textual instructions,\nwithout increasing the number of visual tokens. Extensive evaluations show\nsuperior performance, and analysis reveals the dominance of mid-to-high-level\nfeatures in semantic tasks and the critical role of low-level features in\nfine-grained perception. This work provides valuable insights into the adaptive\nuse of hierarchical visual features in LVLMs, advancing more flexible\nmultimodal systems.\n", "link": "http://arxiv.org/abs/2501.08443v2", "date": "2025-01-16", "relevancy": 2.9836, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6158}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction-Guided%20Fusion%20of%20Multi-Layer%20Visual%20Features%20in%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20Instruction-Guided%20Fusion%20of%20Multi-Layer%20Visual%20Features%20in%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Xu%20Li%20and%20Yi%20Zheng%20and%20Haotian%20Chen%20and%20Xiaolei%20Chen%20and%20Yuxuan%20Liang%20and%20Chenghang%20Lai%20and%20Bin%20Li%20and%20Xiangyang%20Xue%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20achieved%20significant%20success%20in%0Amultimodal%20tasks%20by%20combining%20pre-trained%20vision%20encoders%20and%20large%20language%0Amodels.%20However%2C%20current%20LVLMs%20mainly%20rely%20on%20features%20from%20the%20final%20layers%20of%0Athe%20vision%20encoder%2C%20neglecting%20complementary%20information%20in%20shallower%20layers.%0AWhile%20recent%20methods%20have%20explored%20multi-layer%20features%2C%20they%20are%20often%0Atask-agnostic.%20We%20investigate%20the%20contributions%20of%20visual%20features%20from%0Adifferent%20encoder%20layers%20across%2018%20benchmarks%20and%206%20task%20categories.%20Our%0Aresults%20show%20that%20multi-layer%20features%20provide%20complementary%20strengths%20with%0Avarying%20task%20dependencies%2C%20and%20uniform%20fusion%20performs%20suboptimally.%20Based%20on%0Athese%20findings%2C%20we%20propose%20an%20instruction-guided%20vision%20aggregator%20that%0Adynamically%20integrates%20multi-layer%20features%20based%20on%20textual%20instructions%2C%0Awithout%20increasing%20the%20number%20of%20visual%20tokens.%20Extensive%20evaluations%20show%0Asuperior%20performance%2C%20and%20analysis%20reveals%20the%20dominance%20of%20mid-to-high-level%0Afeatures%20in%20semantic%20tasks%20and%20the%20critical%20role%20of%20low-level%20features%20in%0Afine-grained%20perception.%20This%20work%20provides%20valuable%20insights%20into%20the%20adaptive%0Ause%20of%20hierarchical%20visual%20features%20in%20LVLMs%2C%20advancing%20more%20flexible%0Amultimodal%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08443v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction-Guided%2520Fusion%2520of%2520Multi-Layer%2520Visual%2520Features%2520in%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DXu%2520Li%2520and%2520Yi%2520Zheng%2520and%2520Haotian%2520Chen%2520and%2520Xiaolei%2520Chen%2520and%2520Yuxuan%2520Liang%2520and%2520Chenghang%2520Lai%2520and%2520Bin%2520Li%2520and%2520Xiangyang%2520Xue%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520achieved%2520significant%2520success%2520in%250Amultimodal%2520tasks%2520by%2520combining%2520pre-trained%2520vision%2520encoders%2520and%2520large%2520language%250Amodels.%2520However%252C%2520current%2520LVLMs%2520mainly%2520rely%2520on%2520features%2520from%2520the%2520final%2520layers%2520of%250Athe%2520vision%2520encoder%252C%2520neglecting%2520complementary%2520information%2520in%2520shallower%2520layers.%250AWhile%2520recent%2520methods%2520have%2520explored%2520multi-layer%2520features%252C%2520they%2520are%2520often%250Atask-agnostic.%2520We%2520investigate%2520the%2520contributions%2520of%2520visual%2520features%2520from%250Adifferent%2520encoder%2520layers%2520across%252018%2520benchmarks%2520and%25206%2520task%2520categories.%2520Our%250Aresults%2520show%2520that%2520multi-layer%2520features%2520provide%2520complementary%2520strengths%2520with%250Avarying%2520task%2520dependencies%252C%2520and%2520uniform%2520fusion%2520performs%2520suboptimally.%2520Based%2520on%250Athese%2520findings%252C%2520we%2520propose%2520an%2520instruction-guided%2520vision%2520aggregator%2520that%250Adynamically%2520integrates%2520multi-layer%2520features%2520based%2520on%2520textual%2520instructions%252C%250Awithout%2520increasing%2520the%2520number%2520of%2520visual%2520tokens.%2520Extensive%2520evaluations%2520show%250Asuperior%2520performance%252C%2520and%2520analysis%2520reveals%2520the%2520dominance%2520of%2520mid-to-high-level%250Afeatures%2520in%2520semantic%2520tasks%2520and%2520the%2520critical%2520role%2520of%2520low-level%2520features%2520in%250Afine-grained%2520perception.%2520This%2520work%2520provides%2520valuable%2520insights%2520into%2520the%2520adaptive%250Ause%2520of%2520hierarchical%2520visual%2520features%2520in%2520LVLMs%252C%2520advancing%2520more%2520flexible%250Amultimodal%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08443v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction-Guided%20Fusion%20of%20Multi-Layer%20Visual%20Features%20in%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Xu%20Li%20and%20Yi%20Zheng%20and%20Haotian%20Chen%20and%20Xiaolei%20Chen%20and%20Yuxuan%20Liang%20and%20Chenghang%20Lai%20and%20Bin%20Li%20and%20Xiangyang%20Xue&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20achieved%20significant%20success%20in%0Amultimodal%20tasks%20by%20combining%20pre-trained%20vision%20encoders%20and%20large%20language%0Amodels.%20However%2C%20current%20LVLMs%20mainly%20rely%20on%20features%20from%20the%20final%20layers%20of%0Athe%20vision%20encoder%2C%20neglecting%20complementary%20information%20in%20shallower%20layers.%0AWhile%20recent%20methods%20have%20explored%20multi-layer%20features%2C%20they%20are%20often%0Atask-agnostic.%20We%20investigate%20the%20contributions%20of%20visual%20features%20from%0Adifferent%20encoder%20layers%20across%2018%20benchmarks%20and%206%20task%20categories.%20Our%0Aresults%20show%20that%20multi-layer%20features%20provide%20complementary%20strengths%20with%0Avarying%20task%20dependencies%2C%20and%20uniform%20fusion%20performs%20suboptimally.%20Based%20on%0Athese%20findings%2C%20we%20propose%20an%20instruction-guided%20vision%20aggregator%20that%0Adynamically%20integrates%20multi-layer%20features%20based%20on%20textual%20instructions%2C%0Awithout%20increasing%20the%20number%20of%20visual%20tokens.%20Extensive%20evaluations%20show%0Asuperior%20performance%2C%20and%20analysis%20reveals%20the%20dominance%20of%20mid-to-high-level%0Afeatures%20in%20semantic%20tasks%20and%20the%20critical%20role%20of%20low-level%20features%20in%0Afine-grained%20perception.%20This%20work%20provides%20valuable%20insights%20into%20the%20adaptive%0Ause%20of%20hierarchical%20visual%20features%20in%20LVLMs%2C%20advancing%20more%20flexible%0Amultimodal%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08443v2&entry.124074799=Read"},
{"title": "A New Teacher-Reviewer-Student Framework for Semi-supervised 2D Human\n  Pose Estimation", "author": "Wulian Yun and Mengshi Qi and Fei Peng and Huadong Ma", "abstract": "  Conventional 2D human pose estimation methods typically require extensive\nlabeled annotations, which are both labor-intensive and expensive. In contrast,\nsemi-supervised 2D human pose estimation can alleviate the above problems by\nleveraging a large amount of unlabeled data along with a small portion of\nlabeled data. Existing semi-supervised 2D human pose estimation methods update\nthe network through backpropagation, ignoring crucial historical information\nfrom the previous training process. Therefore, we propose a novel\nsemi-supervised 2D human pose estimation method by utilizing a newly designed\nTeacher-Reviewer-Student framework. Specifically, we first mimic the phenomenon\nthat human beings constantly review previous knowledge for consolidation to\ndesign our framework, in which the teacher predicts results to guide the\nstudent's learning and the reviewer stores important historical parameters to\nprovide additional supervision signals. Secondly, we introduce a Multi-level\nFeature Learning strategy, which utilizes the outputs from different stages of\nthe backbone to estimate the heatmap to guide network training, enriching the\nsupervisory information while effectively capturing keypoint relationships.\nFinally, we design a data augmentation strategy, i.e., Keypoint-Mix, to perturb\npose information by mixing different keypoints, thus enhancing the network's\nability to discern keypoints. Extensive experiments on publicly available\ndatasets, demonstrate our method achieves significant improvements compared to\nthe existing methods.\n", "link": "http://arxiv.org/abs/2501.09565v1", "date": "2025-01-16", "relevancy": 2.9273, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5873}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5857}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Teacher-Reviewer-Student%20Framework%20for%20Semi-supervised%202D%20Human%0A%20%20Pose%20Estimation&body=Title%3A%20A%20New%20Teacher-Reviewer-Student%20Framework%20for%20Semi-supervised%202D%20Human%0A%20%20Pose%20Estimation%0AAuthor%3A%20Wulian%20Yun%20and%20Mengshi%20Qi%20and%20Fei%20Peng%20and%20Huadong%20Ma%0AAbstract%3A%20%20%20Conventional%202D%20human%20pose%20estimation%20methods%20typically%20require%20extensive%0Alabeled%20annotations%2C%20which%20are%20both%20labor-intensive%20and%20expensive.%20In%20contrast%2C%0Asemi-supervised%202D%20human%20pose%20estimation%20can%20alleviate%20the%20above%20problems%20by%0Aleveraging%20a%20large%20amount%20of%20unlabeled%20data%20along%20with%20a%20small%20portion%20of%0Alabeled%20data.%20Existing%20semi-supervised%202D%20human%20pose%20estimation%20methods%20update%0Athe%20network%20through%20backpropagation%2C%20ignoring%20crucial%20historical%20information%0Afrom%20the%20previous%20training%20process.%20Therefore%2C%20we%20propose%20a%20novel%0Asemi-supervised%202D%20human%20pose%20estimation%20method%20by%20utilizing%20a%20newly%20designed%0ATeacher-Reviewer-Student%20framework.%20Specifically%2C%20we%20first%20mimic%20the%20phenomenon%0Athat%20human%20beings%20constantly%20review%20previous%20knowledge%20for%20consolidation%20to%0Adesign%20our%20framework%2C%20in%20which%20the%20teacher%20predicts%20results%20to%20guide%20the%0Astudent%27s%20learning%20and%20the%20reviewer%20stores%20important%20historical%20parameters%20to%0Aprovide%20additional%20supervision%20signals.%20Secondly%2C%20we%20introduce%20a%20Multi-level%0AFeature%20Learning%20strategy%2C%20which%20utilizes%20the%20outputs%20from%20different%20stages%20of%0Athe%20backbone%20to%20estimate%20the%20heatmap%20to%20guide%20network%20training%2C%20enriching%20the%0Asupervisory%20information%20while%20effectively%20capturing%20keypoint%20relationships.%0AFinally%2C%20we%20design%20a%20data%20augmentation%20strategy%2C%20i.e.%2C%20Keypoint-Mix%2C%20to%20perturb%0Apose%20information%20by%20mixing%20different%20keypoints%2C%20thus%20enhancing%20the%20network%27s%0Aability%20to%20discern%20keypoints.%20Extensive%20experiments%20on%20publicly%20available%0Adatasets%2C%20demonstrate%20our%20method%20achieves%20significant%20improvements%20compared%20to%0Athe%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Teacher-Reviewer-Student%2520Framework%2520for%2520Semi-supervised%25202D%2520Human%250A%2520%2520Pose%2520Estimation%26entry.906535625%3DWulian%2520Yun%2520and%2520Mengshi%2520Qi%2520and%2520Fei%2520Peng%2520and%2520Huadong%2520Ma%26entry.1292438233%3D%2520%2520Conventional%25202D%2520human%2520pose%2520estimation%2520methods%2520typically%2520require%2520extensive%250Alabeled%2520annotations%252C%2520which%2520are%2520both%2520labor-intensive%2520and%2520expensive.%2520In%2520contrast%252C%250Asemi-supervised%25202D%2520human%2520pose%2520estimation%2520can%2520alleviate%2520the%2520above%2520problems%2520by%250Aleveraging%2520a%2520large%2520amount%2520of%2520unlabeled%2520data%2520along%2520with%2520a%2520small%2520portion%2520of%250Alabeled%2520data.%2520Existing%2520semi-supervised%25202D%2520human%2520pose%2520estimation%2520methods%2520update%250Athe%2520network%2520through%2520backpropagation%252C%2520ignoring%2520crucial%2520historical%2520information%250Afrom%2520the%2520previous%2520training%2520process.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%250Asemi-supervised%25202D%2520human%2520pose%2520estimation%2520method%2520by%2520utilizing%2520a%2520newly%2520designed%250ATeacher-Reviewer-Student%2520framework.%2520Specifically%252C%2520we%2520first%2520mimic%2520the%2520phenomenon%250Athat%2520human%2520beings%2520constantly%2520review%2520previous%2520knowledge%2520for%2520consolidation%2520to%250Adesign%2520our%2520framework%252C%2520in%2520which%2520the%2520teacher%2520predicts%2520results%2520to%2520guide%2520the%250Astudent%2527s%2520learning%2520and%2520the%2520reviewer%2520stores%2520important%2520historical%2520parameters%2520to%250Aprovide%2520additional%2520supervision%2520signals.%2520Secondly%252C%2520we%2520introduce%2520a%2520Multi-level%250AFeature%2520Learning%2520strategy%252C%2520which%2520utilizes%2520the%2520outputs%2520from%2520different%2520stages%2520of%250Athe%2520backbone%2520to%2520estimate%2520the%2520heatmap%2520to%2520guide%2520network%2520training%252C%2520enriching%2520the%250Asupervisory%2520information%2520while%2520effectively%2520capturing%2520keypoint%2520relationships.%250AFinally%252C%2520we%2520design%2520a%2520data%2520augmentation%2520strategy%252C%2520i.e.%252C%2520Keypoint-Mix%252C%2520to%2520perturb%250Apose%2520information%2520by%2520mixing%2520different%2520keypoints%252C%2520thus%2520enhancing%2520the%2520network%2527s%250Aability%2520to%2520discern%2520keypoints.%2520Extensive%2520experiments%2520on%2520publicly%2520available%250Adatasets%252C%2520demonstrate%2520our%2520method%2520achieves%2520significant%2520improvements%2520compared%2520to%250Athe%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Teacher-Reviewer-Student%20Framework%20for%20Semi-supervised%202D%20Human%0A%20%20Pose%20Estimation&entry.906535625=Wulian%20Yun%20and%20Mengshi%20Qi%20and%20Fei%20Peng%20and%20Huadong%20Ma&entry.1292438233=%20%20Conventional%202D%20human%20pose%20estimation%20methods%20typically%20require%20extensive%0Alabeled%20annotations%2C%20which%20are%20both%20labor-intensive%20and%20expensive.%20In%20contrast%2C%0Asemi-supervised%202D%20human%20pose%20estimation%20can%20alleviate%20the%20above%20problems%20by%0Aleveraging%20a%20large%20amount%20of%20unlabeled%20data%20along%20with%20a%20small%20portion%20of%0Alabeled%20data.%20Existing%20semi-supervised%202D%20human%20pose%20estimation%20methods%20update%0Athe%20network%20through%20backpropagation%2C%20ignoring%20crucial%20historical%20information%0Afrom%20the%20previous%20training%20process.%20Therefore%2C%20we%20propose%20a%20novel%0Asemi-supervised%202D%20human%20pose%20estimation%20method%20by%20utilizing%20a%20newly%20designed%0ATeacher-Reviewer-Student%20framework.%20Specifically%2C%20we%20first%20mimic%20the%20phenomenon%0Athat%20human%20beings%20constantly%20review%20previous%20knowledge%20for%20consolidation%20to%0Adesign%20our%20framework%2C%20in%20which%20the%20teacher%20predicts%20results%20to%20guide%20the%0Astudent%27s%20learning%20and%20the%20reviewer%20stores%20important%20historical%20parameters%20to%0Aprovide%20additional%20supervision%20signals.%20Secondly%2C%20we%20introduce%20a%20Multi-level%0AFeature%20Learning%20strategy%2C%20which%20utilizes%20the%20outputs%20from%20different%20stages%20of%0Athe%20backbone%20to%20estimate%20the%20heatmap%20to%20guide%20network%20training%2C%20enriching%20the%0Asupervisory%20information%20while%20effectively%20capturing%20keypoint%20relationships.%0AFinally%2C%20we%20design%20a%20data%20augmentation%20strategy%2C%20i.e.%2C%20Keypoint-Mix%2C%20to%20perturb%0Apose%20information%20by%20mixing%20different%20keypoints%2C%20thus%20enhancing%20the%20network%27s%0Aability%20to%20discern%20keypoints.%20Extensive%20experiments%20on%20publicly%20available%0Adatasets%2C%20demonstrate%20our%20method%20achieves%20significant%20improvements%20compared%20to%0Athe%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09565v1&entry.124074799=Read"},
{"title": "Double Visual Defense: Adversarial Pre-training and Instruction Tuning\n  for Improving Vision-Language Model Robustness", "author": "Zeyu Wang and Cihang Xie and Brian Bartoldson and Bhavya Kailkhura", "abstract": "  This paper investigates the robustness of vision-language models against\nadversarial visual perturbations and introduces a novel ``double visual\ndefense\" to enhance this robustness. Unlike previous approaches that resort to\nlightweight adversarial fine-tuning of a pre-trained CLIP model, we perform\nlarge-scale adversarial vision-language pre-training from scratch using\nweb-scale data. We then strengthen the defense by incorporating adversarial\nvisual instruction tuning. The resulting models from each stage, $\\Delta$CLIP\nand $\\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a\nnew state-of-the-art in adversarial defense for vision-language models. For\nexample, the adversarial robustness of $\\Delta$CLIP surpasses that of the\nprevious best models on ImageNet-1k by ~20%. %For example, $\\Delta$CLIP\nsurpasses the previous best models on ImageNet-1k by ~20% in terms of\nadversarial robustness. Similarly, compared to prior art, $\\Delta^2$LLaVA\nbrings a ~30% robustness improvement to image captioning task and a ~20%\nrobustness improvement to visual question answering task. Furthermore, our\nmodels exhibit stronger zero-shot recognition capability, fewer hallucinations,\nand superior reasoning performance compared to baselines. Our project page is\nhttps://doublevisualdefense.github.io/.\n", "link": "http://arxiv.org/abs/2501.09446v1", "date": "2025-01-16", "relevancy": 2.865, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6006}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Double%20Visual%20Defense%3A%20Adversarial%20Pre-training%20and%20Instruction%20Tuning%0A%20%20for%20Improving%20Vision-Language%20Model%20Robustness&body=Title%3A%20Double%20Visual%20Defense%3A%20Adversarial%20Pre-training%20and%20Instruction%20Tuning%0A%20%20for%20Improving%20Vision-Language%20Model%20Robustness%0AAuthor%3A%20Zeyu%20Wang%20and%20Cihang%20Xie%20and%20Brian%20Bartoldson%20and%20Bhavya%20Kailkhura%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20robustness%20of%20vision-language%20models%20against%0Aadversarial%20visual%20perturbations%20and%20introduces%20a%20novel%20%60%60double%20visual%0Adefense%22%20to%20enhance%20this%20robustness.%20Unlike%20previous%20approaches%20that%20resort%20to%0Alightweight%20adversarial%20fine-tuning%20of%20a%20pre-trained%20CLIP%20model%2C%20we%20perform%0Alarge-scale%20adversarial%20vision-language%20pre-training%20from%20scratch%20using%0Aweb-scale%20data.%20We%20then%20strengthen%20the%20defense%20by%20incorporating%20adversarial%0Avisual%20instruction%20tuning.%20The%20resulting%20models%20from%20each%20stage%2C%20%24%5CDelta%24CLIP%0Aand%20%24%5CDelta%5E2%24LLaVA%2C%20show%20substantially%20enhanced%20zero-shot%20robustness%20and%20set%20a%0Anew%20state-of-the-art%20in%20adversarial%20defense%20for%20vision-language%20models.%20For%0Aexample%2C%20the%20adversarial%20robustness%20of%20%24%5CDelta%24CLIP%20surpasses%20that%20of%20the%0Aprevious%20best%20models%20on%20ImageNet-1k%20by%20~20%25.%20%25For%20example%2C%20%24%5CDelta%24CLIP%0Asurpasses%20the%20previous%20best%20models%20on%20ImageNet-1k%20by%20~20%25%20in%20terms%20of%0Aadversarial%20robustness.%20Similarly%2C%20compared%20to%20prior%20art%2C%20%24%5CDelta%5E2%24LLaVA%0Abrings%20a%20~30%25%20robustness%20improvement%20to%20image%20captioning%20task%20and%20a%20~20%25%0Arobustness%20improvement%20to%20visual%20question%20answering%20task.%20Furthermore%2C%20our%0Amodels%20exhibit%20stronger%20zero-shot%20recognition%20capability%2C%20fewer%20hallucinations%2C%0Aand%20superior%20reasoning%20performance%20compared%20to%20baselines.%20Our%20project%20page%20is%0Ahttps%3A//doublevisualdefense.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDouble%2520Visual%2520Defense%253A%2520Adversarial%2520Pre-training%2520and%2520Instruction%2520Tuning%250A%2520%2520for%2520Improving%2520Vision-Language%2520Model%2520Robustness%26entry.906535625%3DZeyu%2520Wang%2520and%2520Cihang%2520Xie%2520and%2520Brian%2520Bartoldson%2520and%2520Bhavya%2520Kailkhura%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520robustness%2520of%2520vision-language%2520models%2520against%250Aadversarial%2520visual%2520perturbations%2520and%2520introduces%2520a%2520novel%2520%2560%2560double%2520visual%250Adefense%2522%2520to%2520enhance%2520this%2520robustness.%2520Unlike%2520previous%2520approaches%2520that%2520resort%2520to%250Alightweight%2520adversarial%2520fine-tuning%2520of%2520a%2520pre-trained%2520CLIP%2520model%252C%2520we%2520perform%250Alarge-scale%2520adversarial%2520vision-language%2520pre-training%2520from%2520scratch%2520using%250Aweb-scale%2520data.%2520We%2520then%2520strengthen%2520the%2520defense%2520by%2520incorporating%2520adversarial%250Avisual%2520instruction%2520tuning.%2520The%2520resulting%2520models%2520from%2520each%2520stage%252C%2520%2524%255CDelta%2524CLIP%250Aand%2520%2524%255CDelta%255E2%2524LLaVA%252C%2520show%2520substantially%2520enhanced%2520zero-shot%2520robustness%2520and%2520set%2520a%250Anew%2520state-of-the-art%2520in%2520adversarial%2520defense%2520for%2520vision-language%2520models.%2520For%250Aexample%252C%2520the%2520adversarial%2520robustness%2520of%2520%2524%255CDelta%2524CLIP%2520surpasses%2520that%2520of%2520the%250Aprevious%2520best%2520models%2520on%2520ImageNet-1k%2520by%2520~20%2525.%2520%2525For%2520example%252C%2520%2524%255CDelta%2524CLIP%250Asurpasses%2520the%2520previous%2520best%2520models%2520on%2520ImageNet-1k%2520by%2520~20%2525%2520in%2520terms%2520of%250Aadversarial%2520robustness.%2520Similarly%252C%2520compared%2520to%2520prior%2520art%252C%2520%2524%255CDelta%255E2%2524LLaVA%250Abrings%2520a%2520~30%2525%2520robustness%2520improvement%2520to%2520image%2520captioning%2520task%2520and%2520a%2520~20%2525%250Arobustness%2520improvement%2520to%2520visual%2520question%2520answering%2520task.%2520Furthermore%252C%2520our%250Amodels%2520exhibit%2520stronger%2520zero-shot%2520recognition%2520capability%252C%2520fewer%2520hallucinations%252C%250Aand%2520superior%2520reasoning%2520performance%2520compared%2520to%2520baselines.%2520Our%2520project%2520page%2520is%250Ahttps%253A//doublevisualdefense.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Double%20Visual%20Defense%3A%20Adversarial%20Pre-training%20and%20Instruction%20Tuning%0A%20%20for%20Improving%20Vision-Language%20Model%20Robustness&entry.906535625=Zeyu%20Wang%20and%20Cihang%20Xie%20and%20Brian%20Bartoldson%20and%20Bhavya%20Kailkhura&entry.1292438233=%20%20This%20paper%20investigates%20the%20robustness%20of%20vision-language%20models%20against%0Aadversarial%20visual%20perturbations%20and%20introduces%20a%20novel%20%60%60double%20visual%0Adefense%22%20to%20enhance%20this%20robustness.%20Unlike%20previous%20approaches%20that%20resort%20to%0Alightweight%20adversarial%20fine-tuning%20of%20a%20pre-trained%20CLIP%20model%2C%20we%20perform%0Alarge-scale%20adversarial%20vision-language%20pre-training%20from%20scratch%20using%0Aweb-scale%20data.%20We%20then%20strengthen%20the%20defense%20by%20incorporating%20adversarial%0Avisual%20instruction%20tuning.%20The%20resulting%20models%20from%20each%20stage%2C%20%24%5CDelta%24CLIP%0Aand%20%24%5CDelta%5E2%24LLaVA%2C%20show%20substantially%20enhanced%20zero-shot%20robustness%20and%20set%20a%0Anew%20state-of-the-art%20in%20adversarial%20defense%20for%20vision-language%20models.%20For%0Aexample%2C%20the%20adversarial%20robustness%20of%20%24%5CDelta%24CLIP%20surpasses%20that%20of%20the%0Aprevious%20best%20models%20on%20ImageNet-1k%20by%20~20%25.%20%25For%20example%2C%20%24%5CDelta%24CLIP%0Asurpasses%20the%20previous%20best%20models%20on%20ImageNet-1k%20by%20~20%25%20in%20terms%20of%0Aadversarial%20robustness.%20Similarly%2C%20compared%20to%20prior%20art%2C%20%24%5CDelta%5E2%24LLaVA%0Abrings%20a%20~30%25%20robustness%20improvement%20to%20image%20captioning%20task%20and%20a%20~20%25%0Arobustness%20improvement%20to%20visual%20question%20answering%20task.%20Furthermore%2C%20our%0Amodels%20exhibit%20stronger%20zero-shot%20recognition%20capability%2C%20fewer%20hallucinations%2C%0Aand%20superior%20reasoning%20performance%20compared%20to%20baselines.%20Our%20project%20page%20is%0Ahttps%3A//doublevisualdefense.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09446v1&entry.124074799=Read"},
{"title": "AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention\n  Mixture", "author": "Jiayi Han and Liang Du and Yiwen Wu and Xiangguo Zhou and Hongwei Du and Weibo Zheng", "abstract": "  The success of VLMs often relies on the dynamic high-resolution schema that\nadaptively augments the input images to multiple crops, so that the details of\nthe images can be retained. However, such approaches result in a large number\nof redundant visual tokens, thus significantly reducing the efficiency of the\nVLMs. To improve the VLMs' efficiency without introducing extra training costs,\nmany research works are proposed to reduce the visual tokens by filtering the\nuninformative visual tokens or aggregating their information. Some approaches\npropose to reduce the visual tokens according to the self-attention of VLMs,\nwhich are biased, to result in inaccurate responses. The token reduction\napproaches solely rely on visual cues are text-agnostic, and fail to focus on\nthe areas that are most relevant to the question, especially when the queried\nobjects are non-salient to the image. In this work, we first conduct\nexperiments to show that the original text embeddings are aligned with the\nvisual tokens, without bias on the tailed visual tokens. We then propose a\nself-adaptive cross-modality attention mixture mechanism that dynamically\nleverages the effectiveness of visual saliency and text-to-image similarity in\nthe pre-LLM layers to select the visual tokens that are informative. Extensive\nexperiments demonstrate that the proposed approach achieves state-of-the-art\ntraining-free VLM acceleration performance, especially when the reduction rate\nis sufficiently large.\n", "link": "http://arxiv.org/abs/2501.09532v1", "date": "2025-01-16", "relevancy": 2.8453, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.588}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5789}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaFV%3A%20Accelerating%20VLMs%20with%20Self-Adaptive%20Cross-Modality%20Attention%0A%20%20Mixture&body=Title%3A%20AdaFV%3A%20Accelerating%20VLMs%20with%20Self-Adaptive%20Cross-Modality%20Attention%0A%20%20Mixture%0AAuthor%3A%20Jiayi%20Han%20and%20Liang%20Du%20and%20Yiwen%20Wu%20and%20Xiangguo%20Zhou%20and%20Hongwei%20Du%20and%20Weibo%20Zheng%0AAbstract%3A%20%20%20The%20success%20of%20VLMs%20often%20relies%20on%20the%20dynamic%20high-resolution%20schema%20that%0Aadaptively%20augments%20the%20input%20images%20to%20multiple%20crops%2C%20so%20that%20the%20details%20of%0Athe%20images%20can%20be%20retained.%20However%2C%20such%20approaches%20result%20in%20a%20large%20number%0Aof%20redundant%20visual%20tokens%2C%20thus%20significantly%20reducing%20the%20efficiency%20of%20the%0AVLMs.%20To%20improve%20the%20VLMs%27%20efficiency%20without%20introducing%20extra%20training%20costs%2C%0Amany%20research%20works%20are%20proposed%20to%20reduce%20the%20visual%20tokens%20by%20filtering%20the%0Auninformative%20visual%20tokens%20or%20aggregating%20their%20information.%20Some%20approaches%0Apropose%20to%20reduce%20the%20visual%20tokens%20according%20to%20the%20self-attention%20of%20VLMs%2C%0Awhich%20are%20biased%2C%20to%20result%20in%20inaccurate%20responses.%20The%20token%20reduction%0Aapproaches%20solely%20rely%20on%20visual%20cues%20are%20text-agnostic%2C%20and%20fail%20to%20focus%20on%0Athe%20areas%20that%20are%20most%20relevant%20to%20the%20question%2C%20especially%20when%20the%20queried%0Aobjects%20are%20non-salient%20to%20the%20image.%20In%20this%20work%2C%20we%20first%20conduct%0Aexperiments%20to%20show%20that%20the%20original%20text%20embeddings%20are%20aligned%20with%20the%0Avisual%20tokens%2C%20without%20bias%20on%20the%20tailed%20visual%20tokens.%20We%20then%20propose%20a%0Aself-adaptive%20cross-modality%20attention%20mixture%20mechanism%20that%20dynamically%0Aleverages%20the%20effectiveness%20of%20visual%20saliency%20and%20text-to-image%20similarity%20in%0Athe%20pre-LLM%20layers%20to%20select%20the%20visual%20tokens%20that%20are%20informative.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20approach%20achieves%20state-of-the-art%0Atraining-free%20VLM%20acceleration%20performance%2C%20especially%20when%20the%20reduction%20rate%0Ais%20sufficiently%20large.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaFV%253A%2520Accelerating%2520VLMs%2520with%2520Self-Adaptive%2520Cross-Modality%2520Attention%250A%2520%2520Mixture%26entry.906535625%3DJiayi%2520Han%2520and%2520Liang%2520Du%2520and%2520Yiwen%2520Wu%2520and%2520Xiangguo%2520Zhou%2520and%2520Hongwei%2520Du%2520and%2520Weibo%2520Zheng%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520VLMs%2520often%2520relies%2520on%2520the%2520dynamic%2520high-resolution%2520schema%2520that%250Aadaptively%2520augments%2520the%2520input%2520images%2520to%2520multiple%2520crops%252C%2520so%2520that%2520the%2520details%2520of%250Athe%2520images%2520can%2520be%2520retained.%2520However%252C%2520such%2520approaches%2520result%2520in%2520a%2520large%2520number%250Aof%2520redundant%2520visual%2520tokens%252C%2520thus%2520significantly%2520reducing%2520the%2520efficiency%2520of%2520the%250AVLMs.%2520To%2520improve%2520the%2520VLMs%2527%2520efficiency%2520without%2520introducing%2520extra%2520training%2520costs%252C%250Amany%2520research%2520works%2520are%2520proposed%2520to%2520reduce%2520the%2520visual%2520tokens%2520by%2520filtering%2520the%250Auninformative%2520visual%2520tokens%2520or%2520aggregating%2520their%2520information.%2520Some%2520approaches%250Apropose%2520to%2520reduce%2520the%2520visual%2520tokens%2520according%2520to%2520the%2520self-attention%2520of%2520VLMs%252C%250Awhich%2520are%2520biased%252C%2520to%2520result%2520in%2520inaccurate%2520responses.%2520The%2520token%2520reduction%250Aapproaches%2520solely%2520rely%2520on%2520visual%2520cues%2520are%2520text-agnostic%252C%2520and%2520fail%2520to%2520focus%2520on%250Athe%2520areas%2520that%2520are%2520most%2520relevant%2520to%2520the%2520question%252C%2520especially%2520when%2520the%2520queried%250Aobjects%2520are%2520non-salient%2520to%2520the%2520image.%2520In%2520this%2520work%252C%2520we%2520first%2520conduct%250Aexperiments%2520to%2520show%2520that%2520the%2520original%2520text%2520embeddings%2520are%2520aligned%2520with%2520the%250Avisual%2520tokens%252C%2520without%2520bias%2520on%2520the%2520tailed%2520visual%2520tokens.%2520We%2520then%2520propose%2520a%250Aself-adaptive%2520cross-modality%2520attention%2520mixture%2520mechanism%2520that%2520dynamically%250Aleverages%2520the%2520effectiveness%2520of%2520visual%2520saliency%2520and%2520text-to-image%2520similarity%2520in%250Athe%2520pre-LLM%2520layers%2520to%2520select%2520the%2520visual%2520tokens%2520that%2520are%2520informative.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520achieves%2520state-of-the-art%250Atraining-free%2520VLM%2520acceleration%2520performance%252C%2520especially%2520when%2520the%2520reduction%2520rate%250Ais%2520sufficiently%2520large.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaFV%3A%20Accelerating%20VLMs%20with%20Self-Adaptive%20Cross-Modality%20Attention%0A%20%20Mixture&entry.906535625=Jiayi%20Han%20and%20Liang%20Du%20and%20Yiwen%20Wu%20and%20Xiangguo%20Zhou%20and%20Hongwei%20Du%20and%20Weibo%20Zheng&entry.1292438233=%20%20The%20success%20of%20VLMs%20often%20relies%20on%20the%20dynamic%20high-resolution%20schema%20that%0Aadaptively%20augments%20the%20input%20images%20to%20multiple%20crops%2C%20so%20that%20the%20details%20of%0Athe%20images%20can%20be%20retained.%20However%2C%20such%20approaches%20result%20in%20a%20large%20number%0Aof%20redundant%20visual%20tokens%2C%20thus%20significantly%20reducing%20the%20efficiency%20of%20the%0AVLMs.%20To%20improve%20the%20VLMs%27%20efficiency%20without%20introducing%20extra%20training%20costs%2C%0Amany%20research%20works%20are%20proposed%20to%20reduce%20the%20visual%20tokens%20by%20filtering%20the%0Auninformative%20visual%20tokens%20or%20aggregating%20their%20information.%20Some%20approaches%0Apropose%20to%20reduce%20the%20visual%20tokens%20according%20to%20the%20self-attention%20of%20VLMs%2C%0Awhich%20are%20biased%2C%20to%20result%20in%20inaccurate%20responses.%20The%20token%20reduction%0Aapproaches%20solely%20rely%20on%20visual%20cues%20are%20text-agnostic%2C%20and%20fail%20to%20focus%20on%0Athe%20areas%20that%20are%20most%20relevant%20to%20the%20question%2C%20especially%20when%20the%20queried%0Aobjects%20are%20non-salient%20to%20the%20image.%20In%20this%20work%2C%20we%20first%20conduct%0Aexperiments%20to%20show%20that%20the%20original%20text%20embeddings%20are%20aligned%20with%20the%0Avisual%20tokens%2C%20without%20bias%20on%20the%20tailed%20visual%20tokens.%20We%20then%20propose%20a%0Aself-adaptive%20cross-modality%20attention%20mixture%20mechanism%20that%20dynamically%0Aleverages%20the%20effectiveness%20of%20visual%20saliency%20and%20text-to-image%20similarity%20in%0Athe%20pre-LLM%20layers%20to%20select%20the%20visual%20tokens%20that%20are%20informative.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20approach%20achieves%20state-of-the-art%0Atraining-free%20VLM%20acceleration%20performance%2C%20especially%20when%20the%20reduction%20rate%0Ais%20sufficiently%20large.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09532v1&entry.124074799=Read"},
{"title": "Mesh2SLAM in VR: A Fast Geometry-Based SLAM Framework for Rapid\n  Prototyping in Virtual Reality Applications", "author": "Carlos Augusto Pinheiro de Sousa and Heiko Hamann and Oliver Deussen", "abstract": "  SLAM is a foundational technique with broad applications in robotics and\nAR/VR. SLAM simulations evaluate new concepts, but testing on\nresource-constrained devices, such as VR HMDs, faces challenges: high\ncomputational cost and restricted sensor data access. This work proposes a\nsparse framework using mesh geometry projections as features, which improves\nefficiency and circumvents direct sensor data access, advancing SLAM research\nas we demonstrate in VR and through numerical evaluation.\n", "link": "http://arxiv.org/abs/2501.09600v1", "date": "2025-01-16", "relevancy": 2.8114, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6086}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5714}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesh2SLAM%20in%20VR%3A%20A%20Fast%20Geometry-Based%20SLAM%20Framework%20for%20Rapid%0A%20%20Prototyping%20in%20Virtual%20Reality%20Applications&body=Title%3A%20Mesh2SLAM%20in%20VR%3A%20A%20Fast%20Geometry-Based%20SLAM%20Framework%20for%20Rapid%0A%20%20Prototyping%20in%20Virtual%20Reality%20Applications%0AAuthor%3A%20Carlos%20Augusto%20Pinheiro%20de%20Sousa%20and%20Heiko%20Hamann%20and%20Oliver%20Deussen%0AAbstract%3A%20%20%20SLAM%20is%20a%20foundational%20technique%20with%20broad%20applications%20in%20robotics%20and%0AAR/VR.%20SLAM%20simulations%20evaluate%20new%20concepts%2C%20but%20testing%20on%0Aresource-constrained%20devices%2C%20such%20as%20VR%20HMDs%2C%20faces%20challenges%3A%20high%0Acomputational%20cost%20and%20restricted%20sensor%20data%20access.%20This%20work%20proposes%20a%0Asparse%20framework%20using%20mesh%20geometry%20projections%20as%20features%2C%20which%20improves%0Aefficiency%20and%20circumvents%20direct%20sensor%20data%20access%2C%20advancing%20SLAM%20research%0Aas%20we%20demonstrate%20in%20VR%20and%20through%20numerical%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesh2SLAM%2520in%2520VR%253A%2520A%2520Fast%2520Geometry-Based%2520SLAM%2520Framework%2520for%2520Rapid%250A%2520%2520Prototyping%2520in%2520Virtual%2520Reality%2520Applications%26entry.906535625%3DCarlos%2520Augusto%2520Pinheiro%2520de%2520Sousa%2520and%2520Heiko%2520Hamann%2520and%2520Oliver%2520Deussen%26entry.1292438233%3D%2520%2520SLAM%2520is%2520a%2520foundational%2520technique%2520with%2520broad%2520applications%2520in%2520robotics%2520and%250AAR/VR.%2520SLAM%2520simulations%2520evaluate%2520new%2520concepts%252C%2520but%2520testing%2520on%250Aresource-constrained%2520devices%252C%2520such%2520as%2520VR%2520HMDs%252C%2520faces%2520challenges%253A%2520high%250Acomputational%2520cost%2520and%2520restricted%2520sensor%2520data%2520access.%2520This%2520work%2520proposes%2520a%250Asparse%2520framework%2520using%2520mesh%2520geometry%2520projections%2520as%2520features%252C%2520which%2520improves%250Aefficiency%2520and%2520circumvents%2520direct%2520sensor%2520data%2520access%252C%2520advancing%2520SLAM%2520research%250Aas%2520we%2520demonstrate%2520in%2520VR%2520and%2520through%2520numerical%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesh2SLAM%20in%20VR%3A%20A%20Fast%20Geometry-Based%20SLAM%20Framework%20for%20Rapid%0A%20%20Prototyping%20in%20Virtual%20Reality%20Applications&entry.906535625=Carlos%20Augusto%20Pinheiro%20de%20Sousa%20and%20Heiko%20Hamann%20and%20Oliver%20Deussen&entry.1292438233=%20%20SLAM%20is%20a%20foundational%20technique%20with%20broad%20applications%20in%20robotics%20and%0AAR/VR.%20SLAM%20simulations%20evaluate%20new%20concepts%2C%20but%20testing%20on%0Aresource-constrained%20devices%2C%20such%20as%20VR%20HMDs%2C%20faces%20challenges%3A%20high%0Acomputational%20cost%20and%20restricted%20sensor%20data%20access.%20This%20work%20proposes%20a%0Asparse%20framework%20using%20mesh%20geometry%20projections%20as%20features%2C%20which%20improves%0Aefficiency%20and%20circumvents%20direct%20sensor%20data%20access%2C%20advancing%20SLAM%20research%0Aas%20we%20demonstrate%20in%20VR%20and%20through%20numerical%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09600v1&entry.124074799=Read"},
{"title": "Fine-Grained Image-Text Correspondence with Cost Aggregation for\n  Open-Vocabulary Part Segmentation", "author": "Jiho Choi and Seonho Lee and Minhyun Lee and Seungho Lee and Hyunjung Shim", "abstract": "  Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing\nfine-grained parts in unseen categories. We identify two primary challenges in\nOVPS: (1) the difficulty in aligning part-level image-text correspondence, and\n(2) the lack of structural understanding in segmenting object parts. To address\nthese issues, we propose PartCATSeg, a novel framework that integrates\nobject-aware part-level cost aggregation, compositional loss, and structural\nguidance from DINO. Our approach employs a disentangled cost aggregation\nstrategy that handles object and part-level costs separately, enhancing the\nprecision of part-level segmentation. We also introduce a compositional loss to\nbetter capture part-object relationships, compensating for the limited part\nannotations. Additionally, structural guidance from DINO features improves\nboundary delineation and inter-part understanding. Extensive experiments on\nPascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that\nour method significantly outperforms state-of-the-art approaches, setting a new\nbaseline for robust generalization to unseen part categories.\n", "link": "http://arxiv.org/abs/2501.09688v1", "date": "2025-01-16", "relevancy": 2.7864, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Grained%20Image-Text%20Correspondence%20with%20Cost%20Aggregation%20for%0A%20%20Open-Vocabulary%20Part%20Segmentation&body=Title%3A%20Fine-Grained%20Image-Text%20Correspondence%20with%20Cost%20Aggregation%20for%0A%20%20Open-Vocabulary%20Part%20Segmentation%0AAuthor%3A%20Jiho%20Choi%20and%20Seonho%20Lee%20and%20Minhyun%20Lee%20and%20Seungho%20Lee%20and%20Hyunjung%20Shim%0AAbstract%3A%20%20%20Open-Vocabulary%20Part%20Segmentation%20%28OVPS%29%20is%20an%20emerging%20field%20for%20recognizing%0Afine-grained%20parts%20in%20unseen%20categories.%20We%20identify%20two%20primary%20challenges%20in%0AOVPS%3A%20%281%29%20the%20difficulty%20in%20aligning%20part-level%20image-text%20correspondence%2C%20and%0A%282%29%20the%20lack%20of%20structural%20understanding%20in%20segmenting%20object%20parts.%20To%20address%0Athese%20issues%2C%20we%20propose%20PartCATSeg%2C%20a%20novel%20framework%20that%20integrates%0Aobject-aware%20part-level%20cost%20aggregation%2C%20compositional%20loss%2C%20and%20structural%0Aguidance%20from%20DINO.%20Our%20approach%20employs%20a%20disentangled%20cost%20aggregation%0Astrategy%20that%20handles%20object%20and%20part-level%20costs%20separately%2C%20enhancing%20the%0Aprecision%20of%20part-level%20segmentation.%20We%20also%20introduce%20a%20compositional%20loss%20to%0Abetter%20capture%20part-object%20relationships%2C%20compensating%20for%20the%20limited%20part%0Aannotations.%20Additionally%2C%20structural%20guidance%20from%20DINO%20features%20improves%0Aboundary%20delineation%20and%20inter-part%20understanding.%20Extensive%20experiments%20on%0APascal-Part-116%2C%20ADE20K-Part-234%2C%20and%20PartImageNet%20datasets%20demonstrate%20that%0Aour%20method%20significantly%20outperforms%20state-of-the-art%20approaches%2C%20setting%20a%20new%0Abaseline%20for%20robust%20generalization%20to%20unseen%20part%20categories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Grained%2520Image-Text%2520Correspondence%2520with%2520Cost%2520Aggregation%2520for%250A%2520%2520Open-Vocabulary%2520Part%2520Segmentation%26entry.906535625%3DJiho%2520Choi%2520and%2520Seonho%2520Lee%2520and%2520Minhyun%2520Lee%2520and%2520Seungho%2520Lee%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3D%2520%2520Open-Vocabulary%2520Part%2520Segmentation%2520%2528OVPS%2529%2520is%2520an%2520emerging%2520field%2520for%2520recognizing%250Afine-grained%2520parts%2520in%2520unseen%2520categories.%2520We%2520identify%2520two%2520primary%2520challenges%2520in%250AOVPS%253A%2520%25281%2529%2520the%2520difficulty%2520in%2520aligning%2520part-level%2520image-text%2520correspondence%252C%2520and%250A%25282%2529%2520the%2520lack%2520of%2520structural%2520understanding%2520in%2520segmenting%2520object%2520parts.%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520PartCATSeg%252C%2520a%2520novel%2520framework%2520that%2520integrates%250Aobject-aware%2520part-level%2520cost%2520aggregation%252C%2520compositional%2520loss%252C%2520and%2520structural%250Aguidance%2520from%2520DINO.%2520Our%2520approach%2520employs%2520a%2520disentangled%2520cost%2520aggregation%250Astrategy%2520that%2520handles%2520object%2520and%2520part-level%2520costs%2520separately%252C%2520enhancing%2520the%250Aprecision%2520of%2520part-level%2520segmentation.%2520We%2520also%2520introduce%2520a%2520compositional%2520loss%2520to%250Abetter%2520capture%2520part-object%2520relationships%252C%2520compensating%2520for%2520the%2520limited%2520part%250Aannotations.%2520Additionally%252C%2520structural%2520guidance%2520from%2520DINO%2520features%2520improves%250Aboundary%2520delineation%2520and%2520inter-part%2520understanding.%2520Extensive%2520experiments%2520on%250APascal-Part-116%252C%2520ADE20K-Part-234%252C%2520and%2520PartImageNet%2520datasets%2520demonstrate%2520that%250Aour%2520method%2520significantly%2520outperforms%2520state-of-the-art%2520approaches%252C%2520setting%2520a%2520new%250Abaseline%2520for%2520robust%2520generalization%2520to%2520unseen%2520part%2520categories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Grained%20Image-Text%20Correspondence%20with%20Cost%20Aggregation%20for%0A%20%20Open-Vocabulary%20Part%20Segmentation&entry.906535625=Jiho%20Choi%20and%20Seonho%20Lee%20and%20Minhyun%20Lee%20and%20Seungho%20Lee%20and%20Hyunjung%20Shim&entry.1292438233=%20%20Open-Vocabulary%20Part%20Segmentation%20%28OVPS%29%20is%20an%20emerging%20field%20for%20recognizing%0Afine-grained%20parts%20in%20unseen%20categories.%20We%20identify%20two%20primary%20challenges%20in%0AOVPS%3A%20%281%29%20the%20difficulty%20in%20aligning%20part-level%20image-text%20correspondence%2C%20and%0A%282%29%20the%20lack%20of%20structural%20understanding%20in%20segmenting%20object%20parts.%20To%20address%0Athese%20issues%2C%20we%20propose%20PartCATSeg%2C%20a%20novel%20framework%20that%20integrates%0Aobject-aware%20part-level%20cost%20aggregation%2C%20compositional%20loss%2C%20and%20structural%0Aguidance%20from%20DINO.%20Our%20approach%20employs%20a%20disentangled%20cost%20aggregation%0Astrategy%20that%20handles%20object%20and%20part-level%20costs%20separately%2C%20enhancing%20the%0Aprecision%20of%20part-level%20segmentation.%20We%20also%20introduce%20a%20compositional%20loss%20to%0Abetter%20capture%20part-object%20relationships%2C%20compensating%20for%20the%20limited%20part%0Aannotations.%20Additionally%2C%20structural%20guidance%20from%20DINO%20features%20improves%0Aboundary%20delineation%20and%20inter-part%20understanding.%20Extensive%20experiments%20on%0APascal-Part-116%2C%20ADE20K-Part-234%2C%20and%20PartImageNet%20datasets%20demonstrate%20that%0Aour%20method%20significantly%20outperforms%20state-of-the-art%20approaches%2C%20setting%20a%20new%0Abaseline%20for%20robust%20generalization%20to%20unseen%20part%20categories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09688v1&entry.124074799=Read"},
{"title": "Improving Zero-Shot Object-Level Change Detection by Incorporating\n  Visual Correspondence", "author": "Hung Huy Nguyen and Pooyan Rahmanzadehgervi and Long Mai and Anh Totti Nguyen", "abstract": "  Detecting object-level changes between two images across possibly different\nviews is a core task in many applications that involve visual inspection or\ncamera surveillance. Existing change-detection approaches suffer from three\nmajor limitations: (1) lack of evaluation on image pairs that contain no\nchanges, leading to unreported false positive rates; (2) lack of\ncorrespondences (i.e., localizing the regions before and after a change); and\n(3) poor zero-shot generalization across different domains. To address these\nissues, we introduce a novel method that leverages change correspondences (a)\nduring training to improve change detection accuracy, and (b) at test time, to\nminimize false positives. That is, we harness the supervision labels of where\nan object is added or removed to supervise change detectors, improving their\naccuracy over previous work by a large margin. Our work is also the first to\npredict correspondences between pairs of detected changes using estimated\nhomography and the Hungarian algorithm. Our model demonstrates superior\nperformance over existing methods, achieving state-of-the-art results in change\ndetection and change correspondence accuracy across both in-distribution and\nzero-shot benchmarks.\n", "link": "http://arxiv.org/abs/2501.05555v2", "date": "2025-01-16", "relevancy": 2.7864, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5791}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5514}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Zero-Shot%20Object-Level%20Change%20Detection%20by%20Incorporating%0A%20%20Visual%20Correspondence&body=Title%3A%20Improving%20Zero-Shot%20Object-Level%20Change%20Detection%20by%20Incorporating%0A%20%20Visual%20Correspondence%0AAuthor%3A%20Hung%20Huy%20Nguyen%20and%20Pooyan%20Rahmanzadehgervi%20and%20Long%20Mai%20and%20Anh%20Totti%20Nguyen%0AAbstract%3A%20%20%20Detecting%20object-level%20changes%20between%20two%20images%20across%20possibly%20different%0Aviews%20is%20a%20core%20task%20in%20many%20applications%20that%20involve%20visual%20inspection%20or%0Acamera%20surveillance.%20Existing%20change-detection%20approaches%20suffer%20from%20three%0Amajor%20limitations%3A%20%281%29%20lack%20of%20evaluation%20on%20image%20pairs%20that%20contain%20no%0Achanges%2C%20leading%20to%20unreported%20false%20positive%20rates%3B%20%282%29%20lack%20of%0Acorrespondences%20%28i.e.%2C%20localizing%20the%20regions%20before%20and%20after%20a%20change%29%3B%20and%0A%283%29%20poor%20zero-shot%20generalization%20across%20different%20domains.%20To%20address%20these%0Aissues%2C%20we%20introduce%20a%20novel%20method%20that%20leverages%20change%20correspondences%20%28a%29%0Aduring%20training%20to%20improve%20change%20detection%20accuracy%2C%20and%20%28b%29%20at%20test%20time%2C%20to%0Aminimize%20false%20positives.%20That%20is%2C%20we%20harness%20the%20supervision%20labels%20of%20where%0Aan%20object%20is%20added%20or%20removed%20to%20supervise%20change%20detectors%2C%20improving%20their%0Aaccuracy%20over%20previous%20work%20by%20a%20large%20margin.%20Our%20work%20is%20also%20the%20first%20to%0Apredict%20correspondences%20between%20pairs%20of%20detected%20changes%20using%20estimated%0Ahomography%20and%20the%20Hungarian%20algorithm.%20Our%20model%20demonstrates%20superior%0Aperformance%20over%20existing%20methods%2C%20achieving%20state-of-the-art%20results%20in%20change%0Adetection%20and%20change%20correspondence%20accuracy%20across%20both%20in-distribution%20and%0Azero-shot%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05555v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Zero-Shot%2520Object-Level%2520Change%2520Detection%2520by%2520Incorporating%250A%2520%2520Visual%2520Correspondence%26entry.906535625%3DHung%2520Huy%2520Nguyen%2520and%2520Pooyan%2520Rahmanzadehgervi%2520and%2520Long%2520Mai%2520and%2520Anh%2520Totti%2520Nguyen%26entry.1292438233%3D%2520%2520Detecting%2520object-level%2520changes%2520between%2520two%2520images%2520across%2520possibly%2520different%250Aviews%2520is%2520a%2520core%2520task%2520in%2520many%2520applications%2520that%2520involve%2520visual%2520inspection%2520or%250Acamera%2520surveillance.%2520Existing%2520change-detection%2520approaches%2520suffer%2520from%2520three%250Amajor%2520limitations%253A%2520%25281%2529%2520lack%2520of%2520evaluation%2520on%2520image%2520pairs%2520that%2520contain%2520no%250Achanges%252C%2520leading%2520to%2520unreported%2520false%2520positive%2520rates%253B%2520%25282%2529%2520lack%2520of%250Acorrespondences%2520%2528i.e.%252C%2520localizing%2520the%2520regions%2520before%2520and%2520after%2520a%2520change%2529%253B%2520and%250A%25283%2529%2520poor%2520zero-shot%2520generalization%2520across%2520different%2520domains.%2520To%2520address%2520these%250Aissues%252C%2520we%2520introduce%2520a%2520novel%2520method%2520that%2520leverages%2520change%2520correspondences%2520%2528a%2529%250Aduring%2520training%2520to%2520improve%2520change%2520detection%2520accuracy%252C%2520and%2520%2528b%2529%2520at%2520test%2520time%252C%2520to%250Aminimize%2520false%2520positives.%2520That%2520is%252C%2520we%2520harness%2520the%2520supervision%2520labels%2520of%2520where%250Aan%2520object%2520is%2520added%2520or%2520removed%2520to%2520supervise%2520change%2520detectors%252C%2520improving%2520their%250Aaccuracy%2520over%2520previous%2520work%2520by%2520a%2520large%2520margin.%2520Our%2520work%2520is%2520also%2520the%2520first%2520to%250Apredict%2520correspondences%2520between%2520pairs%2520of%2520detected%2520changes%2520using%2520estimated%250Ahomography%2520and%2520the%2520Hungarian%2520algorithm.%2520Our%2520model%2520demonstrates%2520superior%250Aperformance%2520over%2520existing%2520methods%252C%2520achieving%2520state-of-the-art%2520results%2520in%2520change%250Adetection%2520and%2520change%2520correspondence%2520accuracy%2520across%2520both%2520in-distribution%2520and%250Azero-shot%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05555v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Zero-Shot%20Object-Level%20Change%20Detection%20by%20Incorporating%0A%20%20Visual%20Correspondence&entry.906535625=Hung%20Huy%20Nguyen%20and%20Pooyan%20Rahmanzadehgervi%20and%20Long%20Mai%20and%20Anh%20Totti%20Nguyen&entry.1292438233=%20%20Detecting%20object-level%20changes%20between%20two%20images%20across%20possibly%20different%0Aviews%20is%20a%20core%20task%20in%20many%20applications%20that%20involve%20visual%20inspection%20or%0Acamera%20surveillance.%20Existing%20change-detection%20approaches%20suffer%20from%20three%0Amajor%20limitations%3A%20%281%29%20lack%20of%20evaluation%20on%20image%20pairs%20that%20contain%20no%0Achanges%2C%20leading%20to%20unreported%20false%20positive%20rates%3B%20%282%29%20lack%20of%0Acorrespondences%20%28i.e.%2C%20localizing%20the%20regions%20before%20and%20after%20a%20change%29%3B%20and%0A%283%29%20poor%20zero-shot%20generalization%20across%20different%20domains.%20To%20address%20these%0Aissues%2C%20we%20introduce%20a%20novel%20method%20that%20leverages%20change%20correspondences%20%28a%29%0Aduring%20training%20to%20improve%20change%20detection%20accuracy%2C%20and%20%28b%29%20at%20test%20time%2C%20to%0Aminimize%20false%20positives.%20That%20is%2C%20we%20harness%20the%20supervision%20labels%20of%20where%0Aan%20object%20is%20added%20or%20removed%20to%20supervise%20change%20detectors%2C%20improving%20their%0Aaccuracy%20over%20previous%20work%20by%20a%20large%20margin.%20Our%20work%20is%20also%20the%20first%20to%0Apredict%20correspondences%20between%20pairs%20of%20detected%20changes%20using%20estimated%0Ahomography%20and%20the%20Hungarian%20algorithm.%20Our%20model%20demonstrates%20superior%0Aperformance%20over%20existing%20methods%2C%20achieving%20state-of-the-art%20results%20in%20change%0Adetection%20and%20change%20correspondence%20accuracy%20across%20both%20in-distribution%20and%0Azero-shot%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05555v2&entry.124074799=Read"},
{"title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction", "author": "Chaoyou Fu and Haojia Lin and Xiong Wang and Yi-Fan Zhang and Yunhang Shen and Xiaoyu Liu and Yangze Li and Zuwei Long and Heting Gao and Ke Li and Long Ma and Xiawu Zheng and Rongrong Ji and Xing Sun and Caifeng Shan and Ran He", "abstract": "  Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.\n", "link": "http://arxiv.org/abs/2501.01957v2", "date": "2025-01-16", "relevancy": 2.7807, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VITA-1.5%3A%20Towards%20GPT-4o%20Level%20Real-Time%20Vision%20and%20Speech%20Interaction&body=Title%3A%20VITA-1.5%3A%20Towards%20GPT-4o%20Level%20Real-Time%20Vision%20and%20Speech%20Interaction%0AAuthor%3A%20Chaoyou%20Fu%20and%20Haojia%20Lin%20and%20Xiong%20Wang%20and%20Yi-Fan%20Zhang%20and%20Yunhang%20Shen%20and%20Xiaoyu%20Liu%20and%20Yangze%20Li%20and%20Zuwei%20Long%20and%20Heting%20Gao%20and%20Ke%20Li%20and%20Long%20Ma%20and%20Xiawu%20Zheng%20and%20Rongrong%20Ji%20and%20Xing%20Sun%20and%20Caifeng%20Shan%20and%20Ran%20He%0AAbstract%3A%20%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20typically%20focused%20on%0Aintegrating%20visual%20and%20textual%20modalities%2C%20with%20less%20emphasis%20placed%20on%20the%0Arole%20of%20speech%20in%20enhancing%20interaction.%20However%2C%20speech%20plays%20a%20crucial%20role%0Ain%20multimodal%20dialogue%20systems%2C%20and%20implementing%20high-performance%20in%20both%0Avision%20and%20speech%20tasks%20remains%20a%20significant%20challenge%20due%20to%20the%20fundamental%0Amodality%20differences.%20In%20this%20paper%2C%20we%20propose%20a%20carefully%20designed%0Amulti-stage%20training%20methodology%20that%20progressively%20trains%20LLM%20to%20understand%0Aboth%20visual%20and%20speech%20information%2C%20ultimately%20enabling%20fluent%20vision%20and%0Aspeech%20interaction.%20Our%20approach%20not%20only%20preserves%20strong%20vision-language%0Acapacity%2C%20but%20also%20enables%20efficient%20speech-to-speech%20dialogue%20capabilities%0Awithout%20separate%20ASR%20and%20TTS%20modules%2C%20significantly%20accelerating%20multimodal%0Aend-to-end%20response%20speed.%20By%20comparing%20our%20method%20against%20state-of-the-art%0Acounterparts%20across%20benchmarks%20for%20image%2C%20video%2C%20and%20speech%20tasks%2C%20we%0Ademonstrate%20that%20our%20model%20is%20equipped%20with%20both%20strong%20visual%20and%20speech%0Acapabilities%2C%20making%20near%20real-time%20vision%20and%20speech%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01957v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVITA-1.5%253A%2520Towards%2520GPT-4o%2520Level%2520Real-Time%2520Vision%2520and%2520Speech%2520Interaction%26entry.906535625%3DChaoyou%2520Fu%2520and%2520Haojia%2520Lin%2520and%2520Xiong%2520Wang%2520and%2520Yi-Fan%2520Zhang%2520and%2520Yunhang%2520Shen%2520and%2520Xiaoyu%2520Liu%2520and%2520Yangze%2520Li%2520and%2520Zuwei%2520Long%2520and%2520Heting%2520Gao%2520and%2520Ke%2520Li%2520and%2520Long%2520Ma%2520and%2520Xiawu%2520Zheng%2520and%2520Rongrong%2520Ji%2520and%2520Xing%2520Sun%2520and%2520Caifeng%2520Shan%2520and%2520Ran%2520He%26entry.1292438233%3D%2520%2520Recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520typically%2520focused%2520on%250Aintegrating%2520visual%2520and%2520textual%2520modalities%252C%2520with%2520less%2520emphasis%2520placed%2520on%2520the%250Arole%2520of%2520speech%2520in%2520enhancing%2520interaction.%2520However%252C%2520speech%2520plays%2520a%2520crucial%2520role%250Ain%2520multimodal%2520dialogue%2520systems%252C%2520and%2520implementing%2520high-performance%2520in%2520both%250Avision%2520and%2520speech%2520tasks%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520fundamental%250Amodality%2520differences.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520carefully%2520designed%250Amulti-stage%2520training%2520methodology%2520that%2520progressively%2520trains%2520LLM%2520to%2520understand%250Aboth%2520visual%2520and%2520speech%2520information%252C%2520ultimately%2520enabling%2520fluent%2520vision%2520and%250Aspeech%2520interaction.%2520Our%2520approach%2520not%2520only%2520preserves%2520strong%2520vision-language%250Acapacity%252C%2520but%2520also%2520enables%2520efficient%2520speech-to-speech%2520dialogue%2520capabilities%250Awithout%2520separate%2520ASR%2520and%2520TTS%2520modules%252C%2520significantly%2520accelerating%2520multimodal%250Aend-to-end%2520response%2520speed.%2520By%2520comparing%2520our%2520method%2520against%2520state-of-the-art%250Acounterparts%2520across%2520benchmarks%2520for%2520image%252C%2520video%252C%2520and%2520speech%2520tasks%252C%2520we%250Ademonstrate%2520that%2520our%2520model%2520is%2520equipped%2520with%2520both%2520strong%2520visual%2520and%2520speech%250Acapabilities%252C%2520making%2520near%2520real-time%2520vision%2520and%2520speech%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01957v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VITA-1.5%3A%20Towards%20GPT-4o%20Level%20Real-Time%20Vision%20and%20Speech%20Interaction&entry.906535625=Chaoyou%20Fu%20and%20Haojia%20Lin%20and%20Xiong%20Wang%20and%20Yi-Fan%20Zhang%20and%20Yunhang%20Shen%20and%20Xiaoyu%20Liu%20and%20Yangze%20Li%20and%20Zuwei%20Long%20and%20Heting%20Gao%20and%20Ke%20Li%20and%20Long%20Ma%20and%20Xiawu%20Zheng%20and%20Rongrong%20Ji%20and%20Xing%20Sun%20and%20Caifeng%20Shan%20and%20Ran%20He&entry.1292438233=%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20typically%20focused%20on%0Aintegrating%20visual%20and%20textual%20modalities%2C%20with%20less%20emphasis%20placed%20on%20the%0Arole%20of%20speech%20in%20enhancing%20interaction.%20However%2C%20speech%20plays%20a%20crucial%20role%0Ain%20multimodal%20dialogue%20systems%2C%20and%20implementing%20high-performance%20in%20both%0Avision%20and%20speech%20tasks%20remains%20a%20significant%20challenge%20due%20to%20the%20fundamental%0Amodality%20differences.%20In%20this%20paper%2C%20we%20propose%20a%20carefully%20designed%0Amulti-stage%20training%20methodology%20that%20progressively%20trains%20LLM%20to%20understand%0Aboth%20visual%20and%20speech%20information%2C%20ultimately%20enabling%20fluent%20vision%20and%0Aspeech%20interaction.%20Our%20approach%20not%20only%20preserves%20strong%20vision-language%0Acapacity%2C%20but%20also%20enables%20efficient%20speech-to-speech%20dialogue%20capabilities%0Awithout%20separate%20ASR%20and%20TTS%20modules%2C%20significantly%20accelerating%20multimodal%0Aend-to-end%20response%20speed.%20By%20comparing%20our%20method%20against%20state-of-the-art%0Acounterparts%20across%20benchmarks%20for%20image%2C%20video%2C%20and%20speech%20tasks%2C%20we%0Ademonstrate%20that%20our%20model%20is%20equipped%20with%20both%20strong%20visual%20and%20speech%0Acapabilities%2C%20making%20near%20real-time%20vision%20and%20speech%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01957v2&entry.124074799=Read"},
{"title": "Evaluating alignment between humans and neural network representations\n  in image-based learning tasks", "author": "Can Demircan and Tankred Saanum and Leonardo Pettini and Marcel Binz and Blazej M Baczkowski and Christian F Doeller and Mona M Garvert and Eric Schulz", "abstract": "  Humans represent scenes and objects in rich feature spaces, carrying\ninformation that allows us to generalise about category memberships and\nabstract functions with few examples. What determines whether a neural network\nmodel generalises like a human? We tested how well the representations of $86$\npretrained neural network models mapped to human learning trajectories across\ntwo tasks where humans had to learn continuous relationships and categories of\nnatural images. In these tasks, both human participants and neural networks\nsuccessfully identified the relevant stimulus features within a few trials,\ndemonstrating effective generalisation. We found that while training dataset\nsize was a core determinant of alignment with human choices, contrastive\ntraining with multi-modal data (text and imagery) was a common feature of\ncurrently publicly available models that predicted human generalisation.\nIntrinsic dimensionality of representations had different effects on alignment\nfor different model types. Lastly, we tested three sets of human-aligned\nrepresentations and found no consistent improvements in predictive accuracy\ncompared to the baselines. In conclusion, pretrained neural networks can serve\nto extract representations for cognitive models, as they appear to capture some\nfundamental aspects of cognition that are transferable across tasks. Both our\nparadigms and modelling approach offer a novel way to quantify alignment\nbetween neural networks and humans and extend cognitive science into more\nnaturalistic domains.\n", "link": "http://arxiv.org/abs/2306.09377v3", "date": "2025-01-16", "relevancy": 2.7538, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5533}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20alignment%20between%20humans%20and%20neural%20network%20representations%0A%20%20in%20image-based%20learning%20tasks&body=Title%3A%20Evaluating%20alignment%20between%20humans%20and%20neural%20network%20representations%0A%20%20in%20image-based%20learning%20tasks%0AAuthor%3A%20Can%20Demircan%20and%20Tankred%20Saanum%20and%20Leonardo%20Pettini%20and%20Marcel%20Binz%20and%20Blazej%20M%20Baczkowski%20and%20Christian%20F%20Doeller%20and%20Mona%20M%20Garvert%20and%20Eric%20Schulz%0AAbstract%3A%20%20%20Humans%20represent%20scenes%20and%20objects%20in%20rich%20feature%20spaces%2C%20carrying%0Ainformation%20that%20allows%20us%20to%20generalise%20about%20category%20memberships%20and%0Aabstract%20functions%20with%20few%20examples.%20What%20determines%20whether%20a%20neural%20network%0Amodel%20generalises%20like%20a%20human%3F%20We%20tested%20how%20well%20the%20representations%20of%20%2486%24%0Apretrained%20neural%20network%20models%20mapped%20to%20human%20learning%20trajectories%20across%0Atwo%20tasks%20where%20humans%20had%20to%20learn%20continuous%20relationships%20and%20categories%20of%0Anatural%20images.%20In%20these%20tasks%2C%20both%20human%20participants%20and%20neural%20networks%0Asuccessfully%20identified%20the%20relevant%20stimulus%20features%20within%20a%20few%20trials%2C%0Ademonstrating%20effective%20generalisation.%20We%20found%20that%20while%20training%20dataset%0Asize%20was%20a%20core%20determinant%20of%20alignment%20with%20human%20choices%2C%20contrastive%0Atraining%20with%20multi-modal%20data%20%28text%20and%20imagery%29%20was%20a%20common%20feature%20of%0Acurrently%20publicly%20available%20models%20that%20predicted%20human%20generalisation.%0AIntrinsic%20dimensionality%20of%20representations%20had%20different%20effects%20on%20alignment%0Afor%20different%20model%20types.%20Lastly%2C%20we%20tested%20three%20sets%20of%20human-aligned%0Arepresentations%20and%20found%20no%20consistent%20improvements%20in%20predictive%20accuracy%0Acompared%20to%20the%20baselines.%20In%20conclusion%2C%20pretrained%20neural%20networks%20can%20serve%0Ato%20extract%20representations%20for%20cognitive%20models%2C%20as%20they%20appear%20to%20capture%20some%0Afundamental%20aspects%20of%20cognition%20that%20are%20transferable%20across%20tasks.%20Both%20our%0Aparadigms%20and%20modelling%20approach%20offer%20a%20novel%20way%20to%20quantify%20alignment%0Abetween%20neural%20networks%20and%20humans%20and%20extend%20cognitive%20science%20into%20more%0Anaturalistic%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.09377v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520alignment%2520between%2520humans%2520and%2520neural%2520network%2520representations%250A%2520%2520in%2520image-based%2520learning%2520tasks%26entry.906535625%3DCan%2520Demircan%2520and%2520Tankred%2520Saanum%2520and%2520Leonardo%2520Pettini%2520and%2520Marcel%2520Binz%2520and%2520Blazej%2520M%2520Baczkowski%2520and%2520Christian%2520F%2520Doeller%2520and%2520Mona%2520M%2520Garvert%2520and%2520Eric%2520Schulz%26entry.1292438233%3D%2520%2520Humans%2520represent%2520scenes%2520and%2520objects%2520in%2520rich%2520feature%2520spaces%252C%2520carrying%250Ainformation%2520that%2520allows%2520us%2520to%2520generalise%2520about%2520category%2520memberships%2520and%250Aabstract%2520functions%2520with%2520few%2520examples.%2520What%2520determines%2520whether%2520a%2520neural%2520network%250Amodel%2520generalises%2520like%2520a%2520human%253F%2520We%2520tested%2520how%2520well%2520the%2520representations%2520of%2520%252486%2524%250Apretrained%2520neural%2520network%2520models%2520mapped%2520to%2520human%2520learning%2520trajectories%2520across%250Atwo%2520tasks%2520where%2520humans%2520had%2520to%2520learn%2520continuous%2520relationships%2520and%2520categories%2520of%250Anatural%2520images.%2520In%2520these%2520tasks%252C%2520both%2520human%2520participants%2520and%2520neural%2520networks%250Asuccessfully%2520identified%2520the%2520relevant%2520stimulus%2520features%2520within%2520a%2520few%2520trials%252C%250Ademonstrating%2520effective%2520generalisation.%2520We%2520found%2520that%2520while%2520training%2520dataset%250Asize%2520was%2520a%2520core%2520determinant%2520of%2520alignment%2520with%2520human%2520choices%252C%2520contrastive%250Atraining%2520with%2520multi-modal%2520data%2520%2528text%2520and%2520imagery%2529%2520was%2520a%2520common%2520feature%2520of%250Acurrently%2520publicly%2520available%2520models%2520that%2520predicted%2520human%2520generalisation.%250AIntrinsic%2520dimensionality%2520of%2520representations%2520had%2520different%2520effects%2520on%2520alignment%250Afor%2520different%2520model%2520types.%2520Lastly%252C%2520we%2520tested%2520three%2520sets%2520of%2520human-aligned%250Arepresentations%2520and%2520found%2520no%2520consistent%2520improvements%2520in%2520predictive%2520accuracy%250Acompared%2520to%2520the%2520baselines.%2520In%2520conclusion%252C%2520pretrained%2520neural%2520networks%2520can%2520serve%250Ato%2520extract%2520representations%2520for%2520cognitive%2520models%252C%2520as%2520they%2520appear%2520to%2520capture%2520some%250Afundamental%2520aspects%2520of%2520cognition%2520that%2520are%2520transferable%2520across%2520tasks.%2520Both%2520our%250Aparadigms%2520and%2520modelling%2520approach%2520offer%2520a%2520novel%2520way%2520to%2520quantify%2520alignment%250Abetween%2520neural%2520networks%2520and%2520humans%2520and%2520extend%2520cognitive%2520science%2520into%2520more%250Anaturalistic%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.09377v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20alignment%20between%20humans%20and%20neural%20network%20representations%0A%20%20in%20image-based%20learning%20tasks&entry.906535625=Can%20Demircan%20and%20Tankred%20Saanum%20and%20Leonardo%20Pettini%20and%20Marcel%20Binz%20and%20Blazej%20M%20Baczkowski%20and%20Christian%20F%20Doeller%20and%20Mona%20M%20Garvert%20and%20Eric%20Schulz&entry.1292438233=%20%20Humans%20represent%20scenes%20and%20objects%20in%20rich%20feature%20spaces%2C%20carrying%0Ainformation%20that%20allows%20us%20to%20generalise%20about%20category%20memberships%20and%0Aabstract%20functions%20with%20few%20examples.%20What%20determines%20whether%20a%20neural%20network%0Amodel%20generalises%20like%20a%20human%3F%20We%20tested%20how%20well%20the%20representations%20of%20%2486%24%0Apretrained%20neural%20network%20models%20mapped%20to%20human%20learning%20trajectories%20across%0Atwo%20tasks%20where%20humans%20had%20to%20learn%20continuous%20relationships%20and%20categories%20of%0Anatural%20images.%20In%20these%20tasks%2C%20both%20human%20participants%20and%20neural%20networks%0Asuccessfully%20identified%20the%20relevant%20stimulus%20features%20within%20a%20few%20trials%2C%0Ademonstrating%20effective%20generalisation.%20We%20found%20that%20while%20training%20dataset%0Asize%20was%20a%20core%20determinant%20of%20alignment%20with%20human%20choices%2C%20contrastive%0Atraining%20with%20multi-modal%20data%20%28text%20and%20imagery%29%20was%20a%20common%20feature%20of%0Acurrently%20publicly%20available%20models%20that%20predicted%20human%20generalisation.%0AIntrinsic%20dimensionality%20of%20representations%20had%20different%20effects%20on%20alignment%0Afor%20different%20model%20types.%20Lastly%2C%20we%20tested%20three%20sets%20of%20human-aligned%0Arepresentations%20and%20found%20no%20consistent%20improvements%20in%20predictive%20accuracy%0Acompared%20to%20the%20baselines.%20In%20conclusion%2C%20pretrained%20neural%20networks%20can%20serve%0Ato%20extract%20representations%20for%20cognitive%20models%2C%20as%20they%20appear%20to%20capture%20some%0Afundamental%20aspects%20of%20cognition%20that%20are%20transferable%20across%20tasks.%20Both%20our%0Aparadigms%20and%20modelling%20approach%20offer%20a%20novel%20way%20to%20quantify%20alignment%0Abetween%20neural%20networks%20and%20humans%20and%20extend%20cognitive%20science%20into%20more%0Anaturalistic%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.09377v3&entry.124074799=Read"},
{"title": "TextureCrop: Enhancing Synthetic Image Detection through Texture-based\n  Cropping", "author": "Despina Konstantinidou and Christos Koutlis and Symeon Papadopoulos", "abstract": "  Generative AI technologies produce increasingly realistic imagery, which,\ndespite its potential for creative applications, can also be misused to produce\nmisleading and harmful content. This renders Synthetic Image Detection (SID)\nmethods essential for identifying AI-generated content online. State-of-the-art\nSID methods typically resize or center-crop input images due to architectural\nor computational constraints, which hampers the detection of artifacts that\nappear in high-resolution images. To address this limitation, we propose\nTextureCrop, an image pre-processing component that can be plugged in any\npre-trained SID model to improve its performance. By focusing on high-frequency\nimage parts where generative artifacts are prevalent, TextureCrop enhances SID\nperformance with manageable memory requirements. Experimental results\ndemonstrate a consistent improvement in AUC across various detectors by 6.1%\ncompared to center cropping and by 15% compared to resizing, across\nhigh-resolution images from the Forensynths, Synthbuster and TWIGMA datasets.\nCode available at https : //github.com/mever-team/texture-crop.\n", "link": "http://arxiv.org/abs/2407.15500v4", "date": "2025-01-16", "relevancy": 2.7521, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5638}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.558}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextureCrop%3A%20Enhancing%20Synthetic%20Image%20Detection%20through%20Texture-based%0A%20%20Cropping&body=Title%3A%20TextureCrop%3A%20Enhancing%20Synthetic%20Image%20Detection%20through%20Texture-based%0A%20%20Cropping%0AAuthor%3A%20Despina%20Konstantinidou%20and%20Christos%20Koutlis%20and%20Symeon%20Papadopoulos%0AAbstract%3A%20%20%20Generative%20AI%20technologies%20produce%20increasingly%20realistic%20imagery%2C%20which%2C%0Adespite%20its%20potential%20for%20creative%20applications%2C%20can%20also%20be%20misused%20to%20produce%0Amisleading%20and%20harmful%20content.%20This%20renders%20Synthetic%20Image%20Detection%20%28SID%29%0Amethods%20essential%20for%20identifying%20AI-generated%20content%20online.%20State-of-the-art%0ASID%20methods%20typically%20resize%20or%20center-crop%20input%20images%20due%20to%20architectural%0Aor%20computational%20constraints%2C%20which%20hampers%20the%20detection%20of%20artifacts%20that%0Aappear%20in%20high-resolution%20images.%20To%20address%20this%20limitation%2C%20we%20propose%0ATextureCrop%2C%20an%20image%20pre-processing%20component%20that%20can%20be%20plugged%20in%20any%0Apre-trained%20SID%20model%20to%20improve%20its%20performance.%20By%20focusing%20on%20high-frequency%0Aimage%20parts%20where%20generative%20artifacts%20are%20prevalent%2C%20TextureCrop%20enhances%20SID%0Aperformance%20with%20manageable%20memory%20requirements.%20Experimental%20results%0Ademonstrate%20a%20consistent%20improvement%20in%20AUC%20across%20various%20detectors%20by%206.1%25%0Acompared%20to%20center%20cropping%20and%20by%2015%25%20compared%20to%20resizing%2C%20across%0Ahigh-resolution%20images%20from%20the%20Forensynths%2C%20Synthbuster%20and%20TWIGMA%20datasets.%0ACode%20available%20at%20https%20%3A%20//github.com/mever-team/texture-crop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15500v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextureCrop%253A%2520Enhancing%2520Synthetic%2520Image%2520Detection%2520through%2520Texture-based%250A%2520%2520Cropping%26entry.906535625%3DDespina%2520Konstantinidou%2520and%2520Christos%2520Koutlis%2520and%2520Symeon%2520Papadopoulos%26entry.1292438233%3D%2520%2520Generative%2520AI%2520technologies%2520produce%2520increasingly%2520realistic%2520imagery%252C%2520which%252C%250Adespite%2520its%2520potential%2520for%2520creative%2520applications%252C%2520can%2520also%2520be%2520misused%2520to%2520produce%250Amisleading%2520and%2520harmful%2520content.%2520This%2520renders%2520Synthetic%2520Image%2520Detection%2520%2528SID%2529%250Amethods%2520essential%2520for%2520identifying%2520AI-generated%2520content%2520online.%2520State-of-the-art%250ASID%2520methods%2520typically%2520resize%2520or%2520center-crop%2520input%2520images%2520due%2520to%2520architectural%250Aor%2520computational%2520constraints%252C%2520which%2520hampers%2520the%2520detection%2520of%2520artifacts%2520that%250Aappear%2520in%2520high-resolution%2520images.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250ATextureCrop%252C%2520an%2520image%2520pre-processing%2520component%2520that%2520can%2520be%2520plugged%2520in%2520any%250Apre-trained%2520SID%2520model%2520to%2520improve%2520its%2520performance.%2520By%2520focusing%2520on%2520high-frequency%250Aimage%2520parts%2520where%2520generative%2520artifacts%2520are%2520prevalent%252C%2520TextureCrop%2520enhances%2520SID%250Aperformance%2520with%2520manageable%2520memory%2520requirements.%2520Experimental%2520results%250Ademonstrate%2520a%2520consistent%2520improvement%2520in%2520AUC%2520across%2520various%2520detectors%2520by%25206.1%2525%250Acompared%2520to%2520center%2520cropping%2520and%2520by%252015%2525%2520compared%2520to%2520resizing%252C%2520across%250Ahigh-resolution%2520images%2520from%2520the%2520Forensynths%252C%2520Synthbuster%2520and%2520TWIGMA%2520datasets.%250ACode%2520available%2520at%2520https%2520%253A%2520//github.com/mever-team/texture-crop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15500v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextureCrop%3A%20Enhancing%20Synthetic%20Image%20Detection%20through%20Texture-based%0A%20%20Cropping&entry.906535625=Despina%20Konstantinidou%20and%20Christos%20Koutlis%20and%20Symeon%20Papadopoulos&entry.1292438233=%20%20Generative%20AI%20technologies%20produce%20increasingly%20realistic%20imagery%2C%20which%2C%0Adespite%20its%20potential%20for%20creative%20applications%2C%20can%20also%20be%20misused%20to%20produce%0Amisleading%20and%20harmful%20content.%20This%20renders%20Synthetic%20Image%20Detection%20%28SID%29%0Amethods%20essential%20for%20identifying%20AI-generated%20content%20online.%20State-of-the-art%0ASID%20methods%20typically%20resize%20or%20center-crop%20input%20images%20due%20to%20architectural%0Aor%20computational%20constraints%2C%20which%20hampers%20the%20detection%20of%20artifacts%20that%0Aappear%20in%20high-resolution%20images.%20To%20address%20this%20limitation%2C%20we%20propose%0ATextureCrop%2C%20an%20image%20pre-processing%20component%20that%20can%20be%20plugged%20in%20any%0Apre-trained%20SID%20model%20to%20improve%20its%20performance.%20By%20focusing%20on%20high-frequency%0Aimage%20parts%20where%20generative%20artifacts%20are%20prevalent%2C%20TextureCrop%20enhances%20SID%0Aperformance%20with%20manageable%20memory%20requirements.%20Experimental%20results%0Ademonstrate%20a%20consistent%20improvement%20in%20AUC%20across%20various%20detectors%20by%206.1%25%0Acompared%20to%20center%20cropping%20and%20by%2015%25%20compared%20to%20resizing%2C%20across%0Ahigh-resolution%20images%20from%20the%20Forensynths%2C%20Synthbuster%20and%20TWIGMA%20datasets.%0ACode%20available%20at%20https%20%3A%20//github.com/mever-team/texture-crop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15500v4&entry.124074799=Read"},
{"title": "Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP\n  Evaluation Benchmark", "author": "Alexis Roger and Prateek Humane and Daniel Z. Kaplan and Kshitij Gupta and Qi Sun and George Adamopoulos and Jonathan Siu Chi Lim and Quentin Anthony and Edwin Fennell and Irina Rish", "abstract": "  The proliferation of Vision-Language Models (VLMs) in the past several years\ncalls for rigorous and comprehensive evaluation methods and benchmarks. This\nwork analyzes existing VLM evaluation techniques, including automated metrics,\nAI-based assessments, and human evaluations across diverse tasks. We first\nintroduce Robin - a novel suite of VLMs that we built by combining Large\nLanguage Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use\nRobin to identify shortcomings of current evaluation approaches across scales.\nNext, to overcome the identified limitations, we introduce CHIRP - a new long\nform response benchmark we developed for more robust and complete VLM\nevaluation. We provide open access to the Robin training code, model suite, and\nCHIRP benchmark to promote reproducibility and advance VLM research.\n", "link": "http://arxiv.org/abs/2501.09672v1", "date": "2025-01-16", "relevancy": 2.7317, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robin%3A%20a%20Suite%20of%20Multi-Scale%20Vision-Language%20Models%20and%20the%20CHIRP%0A%20%20Evaluation%20Benchmark&body=Title%3A%20Robin%3A%20a%20Suite%20of%20Multi-Scale%20Vision-Language%20Models%20and%20the%20CHIRP%0A%20%20Evaluation%20Benchmark%0AAuthor%3A%20Alexis%20Roger%20and%20Prateek%20Humane%20and%20Daniel%20Z.%20Kaplan%20and%20Kshitij%20Gupta%20and%20Qi%20Sun%20and%20George%20Adamopoulos%20and%20Jonathan%20Siu%20Chi%20Lim%20and%20Quentin%20Anthony%20and%20Edwin%20Fennell%20and%20Irina%20Rish%0AAbstract%3A%20%20%20The%20proliferation%20of%20Vision-Language%20Models%20%28VLMs%29%20in%20the%20past%20several%20years%0Acalls%20for%20rigorous%20and%20comprehensive%20evaluation%20methods%20and%20benchmarks.%20This%0Awork%20analyzes%20existing%20VLM%20evaluation%20techniques%2C%20including%20automated%20metrics%2C%0AAI-based%20assessments%2C%20and%20human%20evaluations%20across%20diverse%20tasks.%20We%20first%0Aintroduce%20Robin%20-%20a%20novel%20suite%20of%20VLMs%20that%20we%20built%20by%20combining%20Large%0ALanguage%20Models%20%28LLMs%29%20and%20Vision%20Encoders%20%28VEs%29%20at%20multiple%20scales%2C%20and%20use%0ARobin%20to%20identify%20shortcomings%20of%20current%20evaluation%20approaches%20across%20scales.%0ANext%2C%20to%20overcome%20the%20identified%20limitations%2C%20we%20introduce%20CHIRP%20-%20a%20new%20long%0Aform%20response%20benchmark%20we%20developed%20for%20more%20robust%20and%20complete%20VLM%0Aevaluation.%20We%20provide%20open%20access%20to%20the%20Robin%20training%20code%2C%20model%20suite%2C%20and%0ACHIRP%20benchmark%20to%20promote%20reproducibility%20and%20advance%20VLM%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobin%253A%2520a%2520Suite%2520of%2520Multi-Scale%2520Vision-Language%2520Models%2520and%2520the%2520CHIRP%250A%2520%2520Evaluation%2520Benchmark%26entry.906535625%3DAlexis%2520Roger%2520and%2520Prateek%2520Humane%2520and%2520Daniel%2520Z.%2520Kaplan%2520and%2520Kshitij%2520Gupta%2520and%2520Qi%2520Sun%2520and%2520George%2520Adamopoulos%2520and%2520Jonathan%2520Siu%2520Chi%2520Lim%2520and%2520Quentin%2520Anthony%2520and%2520Edwin%2520Fennell%2520and%2520Irina%2520Rish%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520in%2520the%2520past%2520several%2520years%250Acalls%2520for%2520rigorous%2520and%2520comprehensive%2520evaluation%2520methods%2520and%2520benchmarks.%2520This%250Awork%2520analyzes%2520existing%2520VLM%2520evaluation%2520techniques%252C%2520including%2520automated%2520metrics%252C%250AAI-based%2520assessments%252C%2520and%2520human%2520evaluations%2520across%2520diverse%2520tasks.%2520We%2520first%250Aintroduce%2520Robin%2520-%2520a%2520novel%2520suite%2520of%2520VLMs%2520that%2520we%2520built%2520by%2520combining%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520and%2520Vision%2520Encoders%2520%2528VEs%2529%2520at%2520multiple%2520scales%252C%2520and%2520use%250ARobin%2520to%2520identify%2520shortcomings%2520of%2520current%2520evaluation%2520approaches%2520across%2520scales.%250ANext%252C%2520to%2520overcome%2520the%2520identified%2520limitations%252C%2520we%2520introduce%2520CHIRP%2520-%2520a%2520new%2520long%250Aform%2520response%2520benchmark%2520we%2520developed%2520for%2520more%2520robust%2520and%2520complete%2520VLM%250Aevaluation.%2520We%2520provide%2520open%2520access%2520to%2520the%2520Robin%2520training%2520code%252C%2520model%2520suite%252C%2520and%250ACHIRP%2520benchmark%2520to%2520promote%2520reproducibility%2520and%2520advance%2520VLM%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robin%3A%20a%20Suite%20of%20Multi-Scale%20Vision-Language%20Models%20and%20the%20CHIRP%0A%20%20Evaluation%20Benchmark&entry.906535625=Alexis%20Roger%20and%20Prateek%20Humane%20and%20Daniel%20Z.%20Kaplan%20and%20Kshitij%20Gupta%20and%20Qi%20Sun%20and%20George%20Adamopoulos%20and%20Jonathan%20Siu%20Chi%20Lim%20and%20Quentin%20Anthony%20and%20Edwin%20Fennell%20and%20Irina%20Rish&entry.1292438233=%20%20The%20proliferation%20of%20Vision-Language%20Models%20%28VLMs%29%20in%20the%20past%20several%20years%0Acalls%20for%20rigorous%20and%20comprehensive%20evaluation%20methods%20and%20benchmarks.%20This%0Awork%20analyzes%20existing%20VLM%20evaluation%20techniques%2C%20including%20automated%20metrics%2C%0AAI-based%20assessments%2C%20and%20human%20evaluations%20across%20diverse%20tasks.%20We%20first%0Aintroduce%20Robin%20-%20a%20novel%20suite%20of%20VLMs%20that%20we%20built%20by%20combining%20Large%0ALanguage%20Models%20%28LLMs%29%20and%20Vision%20Encoders%20%28VEs%29%20at%20multiple%20scales%2C%20and%20use%0ARobin%20to%20identify%20shortcomings%20of%20current%20evaluation%20approaches%20across%20scales.%0ANext%2C%20to%20overcome%20the%20identified%20limitations%2C%20we%20introduce%20CHIRP%20-%20a%20new%20long%0Aform%20response%20benchmark%20we%20developed%20for%20more%20robust%20and%20complete%20VLM%0Aevaluation.%20We%20provide%20open%20access%20to%20the%20Robin%20training%20code%2C%20model%20suite%2C%20and%0ACHIRP%20benchmark%20to%20promote%20reproducibility%20and%20advance%20VLM%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09672v1&entry.124074799=Read"},
{"title": "Scaling up self-supervised learning for improved surgical foundation\n  models", "author": "Tim J. M. Jaspers and Ronald L. P. D. de Jong and Yiping Li and Carolus H. J. Kusters and Franciscus H. A. Bakker and Romy C. van Jaarsveld and Gino M. Kuiper and Richard van Hillegersberg and Jelle P. Ruurda and Willem M. Brinkman and Josien P. W. Pluim and Peter H. N. de With and Marcel Breeuwer and Yasmina Al Khalil and Fons van der Sommen", "abstract": "  Foundation models have revolutionized computer vision by achieving vastly\nsuperior performance across diverse tasks through large-scale pretraining on\nextensive datasets. However, their application in surgical computer vision has\nbeen limited. This study addresses this gap by introducing SurgeNetXL, a novel\nsurgical foundation model that sets a new benchmark in surgical computer\nvision. Trained on the largest reported surgical dataset to date, comprising\nover 4.7 million video frames, SurgeNetXL achieves consistent top-tier\nperformance across six datasets spanning four surgical procedures and three\ntasks, including semantic segmentation, phase recognition, and critical view of\nsafety (CVS) classification. Compared with the best-performing surgical\nfoundation models, SurgeNetXL shows mean improvements of 2.4, 9.0, and 12.6\npercent for semantic segmentation, phase recognition, and CVS classification,\nrespectively. Additionally, SurgeNetXL outperforms the best-performing\nImageNet-based variants by 14.4, 4.0, and 1.6 percent in the respective tasks.\nIn addition to advancing model performance, this study provides key insights\ninto scaling pretraining datasets, extending training durations, and optimizing\nmodel architectures specifically for surgical computer vision. These findings\npave the way for improved generalizability and robustness in data-scarce\nscenarios, offering a comprehensive framework for future research in this\ndomain. All models and a subset of the SurgeNetXL dataset, including over 2\nmillion video frames, are publicly available at:\nhttps://github.com/TimJaspers0801/SurgeNet.\n", "link": "http://arxiv.org/abs/2501.09436v1", "date": "2025-01-16", "relevancy": 2.7137, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20up%20self-supervised%20learning%20for%20improved%20surgical%20foundation%0A%20%20models&body=Title%3A%20Scaling%20up%20self-supervised%20learning%20for%20improved%20surgical%20foundation%0A%20%20models%0AAuthor%3A%20Tim%20J.%20M.%20Jaspers%20and%20Ronald%20L.%20P.%20D.%20de%20Jong%20and%20Yiping%20Li%20and%20Carolus%20H.%20J.%20Kusters%20and%20Franciscus%20H.%20A.%20Bakker%20and%20Romy%20C.%20van%20Jaarsveld%20and%20Gino%20M.%20Kuiper%20and%20Richard%20van%20Hillegersberg%20and%20Jelle%20P.%20Ruurda%20and%20Willem%20M.%20Brinkman%20and%20Josien%20P.%20W.%20Pluim%20and%20Peter%20H.%20N.%20de%20With%20and%20Marcel%20Breeuwer%20and%20Yasmina%20Al%20Khalil%20and%20Fons%20van%20der%20Sommen%0AAbstract%3A%20%20%20Foundation%20models%20have%20revolutionized%20computer%20vision%20by%20achieving%20vastly%0Asuperior%20performance%20across%20diverse%20tasks%20through%20large-scale%20pretraining%20on%0Aextensive%20datasets.%20However%2C%20their%20application%20in%20surgical%20computer%20vision%20has%0Abeen%20limited.%20This%20study%20addresses%20this%20gap%20by%20introducing%20SurgeNetXL%2C%20a%20novel%0Asurgical%20foundation%20model%20that%20sets%20a%20new%20benchmark%20in%20surgical%20computer%0Avision.%20Trained%20on%20the%20largest%20reported%20surgical%20dataset%20to%20date%2C%20comprising%0Aover%204.7%20million%20video%20frames%2C%20SurgeNetXL%20achieves%20consistent%20top-tier%0Aperformance%20across%20six%20datasets%20spanning%20four%20surgical%20procedures%20and%20three%0Atasks%2C%20including%20semantic%20segmentation%2C%20phase%20recognition%2C%20and%20critical%20view%20of%0Asafety%20%28CVS%29%20classification.%20Compared%20with%20the%20best-performing%20surgical%0Afoundation%20models%2C%20SurgeNetXL%20shows%20mean%20improvements%20of%202.4%2C%209.0%2C%20and%2012.6%0Apercent%20for%20semantic%20segmentation%2C%20phase%20recognition%2C%20and%20CVS%20classification%2C%0Arespectively.%20Additionally%2C%20SurgeNetXL%20outperforms%20the%20best-performing%0AImageNet-based%20variants%20by%2014.4%2C%204.0%2C%20and%201.6%20percent%20in%20the%20respective%20tasks.%0AIn%20addition%20to%20advancing%20model%20performance%2C%20this%20study%20provides%20key%20insights%0Ainto%20scaling%20pretraining%20datasets%2C%20extending%20training%20durations%2C%20and%20optimizing%0Amodel%20architectures%20specifically%20for%20surgical%20computer%20vision.%20These%20findings%0Apave%20the%20way%20for%20improved%20generalizability%20and%20robustness%20in%20data-scarce%0Ascenarios%2C%20offering%20a%20comprehensive%20framework%20for%20future%20research%20in%20this%0Adomain.%20All%20models%20and%20a%20subset%20of%20the%20SurgeNetXL%20dataset%2C%20including%20over%202%0Amillion%20video%20frames%2C%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/TimJaspers0801/SurgeNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520up%2520self-supervised%2520learning%2520for%2520improved%2520surgical%2520foundation%250A%2520%2520models%26entry.906535625%3DTim%2520J.%2520M.%2520Jaspers%2520and%2520Ronald%2520L.%2520P.%2520D.%2520de%2520Jong%2520and%2520Yiping%2520Li%2520and%2520Carolus%2520H.%2520J.%2520Kusters%2520and%2520Franciscus%2520H.%2520A.%2520Bakker%2520and%2520Romy%2520C.%2520van%2520Jaarsveld%2520and%2520Gino%2520M.%2520Kuiper%2520and%2520Richard%2520van%2520Hillegersberg%2520and%2520Jelle%2520P.%2520Ruurda%2520and%2520Willem%2520M.%2520Brinkman%2520and%2520Josien%2520P.%2520W.%2520Pluim%2520and%2520Peter%2520H.%2520N.%2520de%2520With%2520and%2520Marcel%2520Breeuwer%2520and%2520Yasmina%2520Al%2520Khalil%2520and%2520Fons%2520van%2520der%2520Sommen%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520revolutionized%2520computer%2520vision%2520by%2520achieving%2520vastly%250Asuperior%2520performance%2520across%2520diverse%2520tasks%2520through%2520large-scale%2520pretraining%2520on%250Aextensive%2520datasets.%2520However%252C%2520their%2520application%2520in%2520surgical%2520computer%2520vision%2520has%250Abeen%2520limited.%2520This%2520study%2520addresses%2520this%2520gap%2520by%2520introducing%2520SurgeNetXL%252C%2520a%2520novel%250Asurgical%2520foundation%2520model%2520that%2520sets%2520a%2520new%2520benchmark%2520in%2520surgical%2520computer%250Avision.%2520Trained%2520on%2520the%2520largest%2520reported%2520surgical%2520dataset%2520to%2520date%252C%2520comprising%250Aover%25204.7%2520million%2520video%2520frames%252C%2520SurgeNetXL%2520achieves%2520consistent%2520top-tier%250Aperformance%2520across%2520six%2520datasets%2520spanning%2520four%2520surgical%2520procedures%2520and%2520three%250Atasks%252C%2520including%2520semantic%2520segmentation%252C%2520phase%2520recognition%252C%2520and%2520critical%2520view%2520of%250Asafety%2520%2528CVS%2529%2520classification.%2520Compared%2520with%2520the%2520best-performing%2520surgical%250Afoundation%2520models%252C%2520SurgeNetXL%2520shows%2520mean%2520improvements%2520of%25202.4%252C%25209.0%252C%2520and%252012.6%250Apercent%2520for%2520semantic%2520segmentation%252C%2520phase%2520recognition%252C%2520and%2520CVS%2520classification%252C%250Arespectively.%2520Additionally%252C%2520SurgeNetXL%2520outperforms%2520the%2520best-performing%250AImageNet-based%2520variants%2520by%252014.4%252C%25204.0%252C%2520and%25201.6%2520percent%2520in%2520the%2520respective%2520tasks.%250AIn%2520addition%2520to%2520advancing%2520model%2520performance%252C%2520this%2520study%2520provides%2520key%2520insights%250Ainto%2520scaling%2520pretraining%2520datasets%252C%2520extending%2520training%2520durations%252C%2520and%2520optimizing%250Amodel%2520architectures%2520specifically%2520for%2520surgical%2520computer%2520vision.%2520These%2520findings%250Apave%2520the%2520way%2520for%2520improved%2520generalizability%2520and%2520robustness%2520in%2520data-scarce%250Ascenarios%252C%2520offering%2520a%2520comprehensive%2520framework%2520for%2520future%2520research%2520in%2520this%250Adomain.%2520All%2520models%2520and%2520a%2520subset%2520of%2520the%2520SurgeNetXL%2520dataset%252C%2520including%2520over%25202%250Amillion%2520video%2520frames%252C%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/TimJaspers0801/SurgeNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20up%20self-supervised%20learning%20for%20improved%20surgical%20foundation%0A%20%20models&entry.906535625=Tim%20J.%20M.%20Jaspers%20and%20Ronald%20L.%20P.%20D.%20de%20Jong%20and%20Yiping%20Li%20and%20Carolus%20H.%20J.%20Kusters%20and%20Franciscus%20H.%20A.%20Bakker%20and%20Romy%20C.%20van%20Jaarsveld%20and%20Gino%20M.%20Kuiper%20and%20Richard%20van%20Hillegersberg%20and%20Jelle%20P.%20Ruurda%20and%20Willem%20M.%20Brinkman%20and%20Josien%20P.%20W.%20Pluim%20and%20Peter%20H.%20N.%20de%20With%20and%20Marcel%20Breeuwer%20and%20Yasmina%20Al%20Khalil%20and%20Fons%20van%20der%20Sommen&entry.1292438233=%20%20Foundation%20models%20have%20revolutionized%20computer%20vision%20by%20achieving%20vastly%0Asuperior%20performance%20across%20diverse%20tasks%20through%20large-scale%20pretraining%20on%0Aextensive%20datasets.%20However%2C%20their%20application%20in%20surgical%20computer%20vision%20has%0Abeen%20limited.%20This%20study%20addresses%20this%20gap%20by%20introducing%20SurgeNetXL%2C%20a%20novel%0Asurgical%20foundation%20model%20that%20sets%20a%20new%20benchmark%20in%20surgical%20computer%0Avision.%20Trained%20on%20the%20largest%20reported%20surgical%20dataset%20to%20date%2C%20comprising%0Aover%204.7%20million%20video%20frames%2C%20SurgeNetXL%20achieves%20consistent%20top-tier%0Aperformance%20across%20six%20datasets%20spanning%20four%20surgical%20procedures%20and%20three%0Atasks%2C%20including%20semantic%20segmentation%2C%20phase%20recognition%2C%20and%20critical%20view%20of%0Asafety%20%28CVS%29%20classification.%20Compared%20with%20the%20best-performing%20surgical%0Afoundation%20models%2C%20SurgeNetXL%20shows%20mean%20improvements%20of%202.4%2C%209.0%2C%20and%2012.6%0Apercent%20for%20semantic%20segmentation%2C%20phase%20recognition%2C%20and%20CVS%20classification%2C%0Arespectively.%20Additionally%2C%20SurgeNetXL%20outperforms%20the%20best-performing%0AImageNet-based%20variants%20by%2014.4%2C%204.0%2C%20and%201.6%20percent%20in%20the%20respective%20tasks.%0AIn%20addition%20to%20advancing%20model%20performance%2C%20this%20study%20provides%20key%20insights%0Ainto%20scaling%20pretraining%20datasets%2C%20extending%20training%20durations%2C%20and%20optimizing%0Amodel%20architectures%20specifically%20for%20surgical%20computer%20vision.%20These%20findings%0Apave%20the%20way%20for%20improved%20generalizability%20and%20robustness%20in%20data-scarce%0Ascenarios%2C%20offering%20a%20comprehensive%20framework%20for%20future%20research%20in%20this%0Adomain.%20All%20models%20and%20a%20subset%20of%20the%20SurgeNetXL%20dataset%2C%20including%20over%202%0Amillion%20video%20frames%2C%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/TimJaspers0801/SurgeNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09436v1&entry.124074799=Read"},
{"title": "Latent Space Characterization of Autoencoder Variants", "author": "Anika Shrivastava and Renu Rameshan and Samar Agnihotri", "abstract": "  Understanding the latent spaces learned by deep learning models is crucial in\nexploring how they represent and generate complex data. Autoencoders (AEs) have\nplayed a key role in the area of representation learning, with numerous\nregularization techniques and training principles developed not only to enhance\ntheir ability to learn compact and robust representations, but also to reveal\nhow different architectures influence the structure and smoothness of the\nlower-dimensional non-linear manifold. We strive to characterize the structure\nof the latent spaces learned by different autoencoders including convolutional\nautoencoders (CAEs), denoising autoencoders (DAEs), and variational\nautoencoders (VAEs) and how they change with the perturbations in the input. By\ncharacterizing the matrix manifolds corresponding to the latent spaces, we\nprovide an explanation for the well-known observation that the latent spaces of\nCAE and DAE form non-smooth manifolds, while that of VAE forms a smooth\nmanifold. We also map the points of the matrix manifold to a Hilbert space\nusing distance preserving transforms and provide an alternate view in terms of\nthe subspaces generated in the Hilbert space as a function of the distortion in\nthe input. The results show that the latent manifolds of CAE and DAE are\nstratified with each stratum being a smooth product manifold, while the\nmanifold of VAE is a smooth product manifold of two symmetric positive definite\nmatrices and a symmetric positive semi-definite matrix.\n", "link": "http://arxiv.org/abs/2412.04755v2", "date": "2025-01-16", "relevancy": 2.6991, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.601}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5255}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Space%20Characterization%20of%20Autoencoder%20Variants&body=Title%3A%20Latent%20Space%20Characterization%20of%20Autoencoder%20Variants%0AAuthor%3A%20Anika%20Shrivastava%20and%20Renu%20Rameshan%20and%20Samar%20Agnihotri%0AAbstract%3A%20%20%20Understanding%20the%20latent%20spaces%20learned%20by%20deep%20learning%20models%20is%20crucial%20in%0Aexploring%20how%20they%20represent%20and%20generate%20complex%20data.%20Autoencoders%20%28AEs%29%20have%0Aplayed%20a%20key%20role%20in%20the%20area%20of%20representation%20learning%2C%20with%20numerous%0Aregularization%20techniques%20and%20training%20principles%20developed%20not%20only%20to%20enhance%0Atheir%20ability%20to%20learn%20compact%20and%20robust%20representations%2C%20but%20also%20to%20reveal%0Ahow%20different%20architectures%20influence%20the%20structure%20and%20smoothness%20of%20the%0Alower-dimensional%20non-linear%20manifold.%20We%20strive%20to%20characterize%20the%20structure%0Aof%20the%20latent%20spaces%20learned%20by%20different%20autoencoders%20including%20convolutional%0Aautoencoders%20%28CAEs%29%2C%20denoising%20autoencoders%20%28DAEs%29%2C%20and%20variational%0Aautoencoders%20%28VAEs%29%20and%20how%20they%20change%20with%20the%20perturbations%20in%20the%20input.%20By%0Acharacterizing%20the%20matrix%20manifolds%20corresponding%20to%20the%20latent%20spaces%2C%20we%0Aprovide%20an%20explanation%20for%20the%20well-known%20observation%20that%20the%20latent%20spaces%20of%0ACAE%20and%20DAE%20form%20non-smooth%20manifolds%2C%20while%20that%20of%20VAE%20forms%20a%20smooth%0Amanifold.%20We%20also%20map%20the%20points%20of%20the%20matrix%20manifold%20to%20a%20Hilbert%20space%0Ausing%20distance%20preserving%20transforms%20and%20provide%20an%20alternate%20view%20in%20terms%20of%0Athe%20subspaces%20generated%20in%20the%20Hilbert%20space%20as%20a%20function%20of%20the%20distortion%20in%0Athe%20input.%20The%20results%20show%20that%20the%20latent%20manifolds%20of%20CAE%20and%20DAE%20are%0Astratified%20with%20each%20stratum%20being%20a%20smooth%20product%20manifold%2C%20while%20the%0Amanifold%20of%20VAE%20is%20a%20smooth%20product%20manifold%20of%20two%20symmetric%20positive%20definite%0Amatrices%20and%20a%20symmetric%20positive%20semi-definite%20matrix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Space%2520Characterization%2520of%2520Autoencoder%2520Variants%26entry.906535625%3DAnika%2520Shrivastava%2520and%2520Renu%2520Rameshan%2520and%2520Samar%2520Agnihotri%26entry.1292438233%3D%2520%2520Understanding%2520the%2520latent%2520spaces%2520learned%2520by%2520deep%2520learning%2520models%2520is%2520crucial%2520in%250Aexploring%2520how%2520they%2520represent%2520and%2520generate%2520complex%2520data.%2520Autoencoders%2520%2528AEs%2529%2520have%250Aplayed%2520a%2520key%2520role%2520in%2520the%2520area%2520of%2520representation%2520learning%252C%2520with%2520numerous%250Aregularization%2520techniques%2520and%2520training%2520principles%2520developed%2520not%2520only%2520to%2520enhance%250Atheir%2520ability%2520to%2520learn%2520compact%2520and%2520robust%2520representations%252C%2520but%2520also%2520to%2520reveal%250Ahow%2520different%2520architectures%2520influence%2520the%2520structure%2520and%2520smoothness%2520of%2520the%250Alower-dimensional%2520non-linear%2520manifold.%2520We%2520strive%2520to%2520characterize%2520the%2520structure%250Aof%2520the%2520latent%2520spaces%2520learned%2520by%2520different%2520autoencoders%2520including%2520convolutional%250Aautoencoders%2520%2528CAEs%2529%252C%2520denoising%2520autoencoders%2520%2528DAEs%2529%252C%2520and%2520variational%250Aautoencoders%2520%2528VAEs%2529%2520and%2520how%2520they%2520change%2520with%2520the%2520perturbations%2520in%2520the%2520input.%2520By%250Acharacterizing%2520the%2520matrix%2520manifolds%2520corresponding%2520to%2520the%2520latent%2520spaces%252C%2520we%250Aprovide%2520an%2520explanation%2520for%2520the%2520well-known%2520observation%2520that%2520the%2520latent%2520spaces%2520of%250ACAE%2520and%2520DAE%2520form%2520non-smooth%2520manifolds%252C%2520while%2520that%2520of%2520VAE%2520forms%2520a%2520smooth%250Amanifold.%2520We%2520also%2520map%2520the%2520points%2520of%2520the%2520matrix%2520manifold%2520to%2520a%2520Hilbert%2520space%250Ausing%2520distance%2520preserving%2520transforms%2520and%2520provide%2520an%2520alternate%2520view%2520in%2520terms%2520of%250Athe%2520subspaces%2520generated%2520in%2520the%2520Hilbert%2520space%2520as%2520a%2520function%2520of%2520the%2520distortion%2520in%250Athe%2520input.%2520The%2520results%2520show%2520that%2520the%2520latent%2520manifolds%2520of%2520CAE%2520and%2520DAE%2520are%250Astratified%2520with%2520each%2520stratum%2520being%2520a%2520smooth%2520product%2520manifold%252C%2520while%2520the%250Amanifold%2520of%2520VAE%2520is%2520a%2520smooth%2520product%2520manifold%2520of%2520two%2520symmetric%2520positive%2520definite%250Amatrices%2520and%2520a%2520symmetric%2520positive%2520semi-definite%2520matrix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Space%20Characterization%20of%20Autoencoder%20Variants&entry.906535625=Anika%20Shrivastava%20and%20Renu%20Rameshan%20and%20Samar%20Agnihotri&entry.1292438233=%20%20Understanding%20the%20latent%20spaces%20learned%20by%20deep%20learning%20models%20is%20crucial%20in%0Aexploring%20how%20they%20represent%20and%20generate%20complex%20data.%20Autoencoders%20%28AEs%29%20have%0Aplayed%20a%20key%20role%20in%20the%20area%20of%20representation%20learning%2C%20with%20numerous%0Aregularization%20techniques%20and%20training%20principles%20developed%20not%20only%20to%20enhance%0Atheir%20ability%20to%20learn%20compact%20and%20robust%20representations%2C%20but%20also%20to%20reveal%0Ahow%20different%20architectures%20influence%20the%20structure%20and%20smoothness%20of%20the%0Alower-dimensional%20non-linear%20manifold.%20We%20strive%20to%20characterize%20the%20structure%0Aof%20the%20latent%20spaces%20learned%20by%20different%20autoencoders%20including%20convolutional%0Aautoencoders%20%28CAEs%29%2C%20denoising%20autoencoders%20%28DAEs%29%2C%20and%20variational%0Aautoencoders%20%28VAEs%29%20and%20how%20they%20change%20with%20the%20perturbations%20in%20the%20input.%20By%0Acharacterizing%20the%20matrix%20manifolds%20corresponding%20to%20the%20latent%20spaces%2C%20we%0Aprovide%20an%20explanation%20for%20the%20well-known%20observation%20that%20the%20latent%20spaces%20of%0ACAE%20and%20DAE%20form%20non-smooth%20manifolds%2C%20while%20that%20of%20VAE%20forms%20a%20smooth%0Amanifold.%20We%20also%20map%20the%20points%20of%20the%20matrix%20manifold%20to%20a%20Hilbert%20space%0Ausing%20distance%20preserving%20transforms%20and%20provide%20an%20alternate%20view%20in%20terms%20of%0Athe%20subspaces%20generated%20in%20the%20Hilbert%20space%20as%20a%20function%20of%20the%20distortion%20in%0Athe%20input.%20The%20results%20show%20that%20the%20latent%20manifolds%20of%20CAE%20and%20DAE%20are%0Astratified%20with%20each%20stratum%20being%20a%20smooth%20product%20manifold%2C%20while%20the%0Amanifold%20of%20VAE%20is%20a%20smooth%20product%20manifold%20of%20two%20symmetric%20positive%20definite%0Amatrices%20and%20a%20symmetric%20positive%20semi-definite%20matrix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04755v2&entry.124074799=Read"},
{"title": "Lost in Translation, Found in Context: Sign Language Translation with\n  Contextual Cues", "author": "Youngjoon Jang and Haran Raajesh and Liliane Momeni and G\u00fcl Varol and Andrew Zisserman", "abstract": "  Our objective is to translate continuous sign language into spoken language\ntext. Inspired by the way human interpreters rely on context for accurate\ntranslation, we incorporate additional contextual cues together with the\nsigning video, into a new translation framework. Specifically, besides visual\nsign recognition features that encode the input video, we integrate\ncomplementary textual information from (i) captions describing the background\nshow, (ii) translation of previous sentences, as well as (iii) pseudo-glosses\ntranscribing the signing. These are automatically extracted and inputted along\nwith the visual features to a pre-trained large language model (LLM), which we\nfine-tune to generate spoken language translations in text form. Through\nextensive ablation studies, we show the positive contribution of each input cue\nto the translation performance. We train and evaluate our approach on BOBSL --\nthe largest British Sign Language dataset currently available. We show that our\ncontextual approach significantly enhances the quality of the translations\ncompared to previously reported results on BOBSL, and also to state-of-the-art\nmethods that we implement as baselines. Furthermore, we demonstrate the\ngenerality of our approach by applying it also to How2Sign, an American Sign\nLanguage dataset, and achieve competitive results.\n", "link": "http://arxiv.org/abs/2501.09754v1", "date": "2025-01-16", "relevancy": 2.6841, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Translation%2C%20Found%20in%20Context%3A%20Sign%20Language%20Translation%20with%0A%20%20Contextual%20Cues&body=Title%3A%20Lost%20in%20Translation%2C%20Found%20in%20Context%3A%20Sign%20Language%20Translation%20with%0A%20%20Contextual%20Cues%0AAuthor%3A%20Youngjoon%20Jang%20and%20Haran%20Raajesh%20and%20Liliane%20Momeni%20and%20G%C3%BCl%20Varol%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20Our%20objective%20is%20to%20translate%20continuous%20sign%20language%20into%20spoken%20language%0Atext.%20Inspired%20by%20the%20way%20human%20interpreters%20rely%20on%20context%20for%20accurate%0Atranslation%2C%20we%20incorporate%20additional%20contextual%20cues%20together%20with%20the%0Asigning%20video%2C%20into%20a%20new%20translation%20framework.%20Specifically%2C%20besides%20visual%0Asign%20recognition%20features%20that%20encode%20the%20input%20video%2C%20we%20integrate%0Acomplementary%20textual%20information%20from%20%28i%29%20captions%20describing%20the%20background%0Ashow%2C%20%28ii%29%20translation%20of%20previous%20sentences%2C%20as%20well%20as%20%28iii%29%20pseudo-glosses%0Atranscribing%20the%20signing.%20These%20are%20automatically%20extracted%20and%20inputted%20along%0Awith%20the%20visual%20features%20to%20a%20pre-trained%20large%20language%20model%20%28LLM%29%2C%20which%20we%0Afine-tune%20to%20generate%20spoken%20language%20translations%20in%20text%20form.%20Through%0Aextensive%20ablation%20studies%2C%20we%20show%20the%20positive%20contribution%20of%20each%20input%20cue%0Ato%20the%20translation%20performance.%20We%20train%20and%20evaluate%20our%20approach%20on%20BOBSL%20--%0Athe%20largest%20British%20Sign%20Language%20dataset%20currently%20available.%20We%20show%20that%20our%0Acontextual%20approach%20significantly%20enhances%20the%20quality%20of%20the%20translations%0Acompared%20to%20previously%20reported%20results%20on%20BOBSL%2C%20and%20also%20to%20state-of-the-art%0Amethods%20that%20we%20implement%20as%20baselines.%20Furthermore%2C%20we%20demonstrate%20the%0Agenerality%20of%20our%20approach%20by%20applying%20it%20also%20to%20How2Sign%2C%20an%20American%20Sign%0ALanguage%20dataset%2C%20and%20achieve%20competitive%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Translation%252C%2520Found%2520in%2520Context%253A%2520Sign%2520Language%2520Translation%2520with%250A%2520%2520Contextual%2520Cues%26entry.906535625%3DYoungjoon%2520Jang%2520and%2520Haran%2520Raajesh%2520and%2520Liliane%2520Momeni%2520and%2520G%25C3%25BCl%2520Varol%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520Our%2520objective%2520is%2520to%2520translate%2520continuous%2520sign%2520language%2520into%2520spoken%2520language%250Atext.%2520Inspired%2520by%2520the%2520way%2520human%2520interpreters%2520rely%2520on%2520context%2520for%2520accurate%250Atranslation%252C%2520we%2520incorporate%2520additional%2520contextual%2520cues%2520together%2520with%2520the%250Asigning%2520video%252C%2520into%2520a%2520new%2520translation%2520framework.%2520Specifically%252C%2520besides%2520visual%250Asign%2520recognition%2520features%2520that%2520encode%2520the%2520input%2520video%252C%2520we%2520integrate%250Acomplementary%2520textual%2520information%2520from%2520%2528i%2529%2520captions%2520describing%2520the%2520background%250Ashow%252C%2520%2528ii%2529%2520translation%2520of%2520previous%2520sentences%252C%2520as%2520well%2520as%2520%2528iii%2529%2520pseudo-glosses%250Atranscribing%2520the%2520signing.%2520These%2520are%2520automatically%2520extracted%2520and%2520inputted%2520along%250Awith%2520the%2520visual%2520features%2520to%2520a%2520pre-trained%2520large%2520language%2520model%2520%2528LLM%2529%252C%2520which%2520we%250Afine-tune%2520to%2520generate%2520spoken%2520language%2520translations%2520in%2520text%2520form.%2520Through%250Aextensive%2520ablation%2520studies%252C%2520we%2520show%2520the%2520positive%2520contribution%2520of%2520each%2520input%2520cue%250Ato%2520the%2520translation%2520performance.%2520We%2520train%2520and%2520evaluate%2520our%2520approach%2520on%2520BOBSL%2520--%250Athe%2520largest%2520British%2520Sign%2520Language%2520dataset%2520currently%2520available.%2520We%2520show%2520that%2520our%250Acontextual%2520approach%2520significantly%2520enhances%2520the%2520quality%2520of%2520the%2520translations%250Acompared%2520to%2520previously%2520reported%2520results%2520on%2520BOBSL%252C%2520and%2520also%2520to%2520state-of-the-art%250Amethods%2520that%2520we%2520implement%2520as%2520baselines.%2520Furthermore%252C%2520we%2520demonstrate%2520the%250Agenerality%2520of%2520our%2520approach%2520by%2520applying%2520it%2520also%2520to%2520How2Sign%252C%2520an%2520American%2520Sign%250ALanguage%2520dataset%252C%2520and%2520achieve%2520competitive%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Translation%2C%20Found%20in%20Context%3A%20Sign%20Language%20Translation%20with%0A%20%20Contextual%20Cues&entry.906535625=Youngjoon%20Jang%20and%20Haran%20Raajesh%20and%20Liliane%20Momeni%20and%20G%C3%BCl%20Varol%20and%20Andrew%20Zisserman&entry.1292438233=%20%20Our%20objective%20is%20to%20translate%20continuous%20sign%20language%20into%20spoken%20language%0Atext.%20Inspired%20by%20the%20way%20human%20interpreters%20rely%20on%20context%20for%20accurate%0Atranslation%2C%20we%20incorporate%20additional%20contextual%20cues%20together%20with%20the%0Asigning%20video%2C%20into%20a%20new%20translation%20framework.%20Specifically%2C%20besides%20visual%0Asign%20recognition%20features%20that%20encode%20the%20input%20video%2C%20we%20integrate%0Acomplementary%20textual%20information%20from%20%28i%29%20captions%20describing%20the%20background%0Ashow%2C%20%28ii%29%20translation%20of%20previous%20sentences%2C%20as%20well%20as%20%28iii%29%20pseudo-glosses%0Atranscribing%20the%20signing.%20These%20are%20automatically%20extracted%20and%20inputted%20along%0Awith%20the%20visual%20features%20to%20a%20pre-trained%20large%20language%20model%20%28LLM%29%2C%20which%20we%0Afine-tune%20to%20generate%20spoken%20language%20translations%20in%20text%20form.%20Through%0Aextensive%20ablation%20studies%2C%20we%20show%20the%20positive%20contribution%20of%20each%20input%20cue%0Ato%20the%20translation%20performance.%20We%20train%20and%20evaluate%20our%20approach%20on%20BOBSL%20--%0Athe%20largest%20British%20Sign%20Language%20dataset%20currently%20available.%20We%20show%20that%20our%0Acontextual%20approach%20significantly%20enhances%20the%20quality%20of%20the%20translations%0Acompared%20to%20previously%20reported%20results%20on%20BOBSL%2C%20and%20also%20to%20state-of-the-art%0Amethods%20that%20we%20implement%20as%20baselines.%20Furthermore%2C%20we%20demonstrate%20the%0Agenerality%20of%20our%20approach%20by%20applying%20it%20also%20to%20How2Sign%2C%20an%20American%20Sign%0ALanguage%20dataset%2C%20and%20achieve%20competitive%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09754v1&entry.124074799=Read"},
{"title": "Vision-Language Models Do Not Understand Negation", "author": "Kumail Alhamoud and Shaden Alshammari and Yonglong Tian and Guohao Li and Philip Torr and Yoon Kim and Marzyeh Ghassemi", "abstract": "  Many practical vision-language applications require models that understand\nnegation, e.g., when using natural language to retrieve images which contain\ncertain objects but not others. Despite advancements in vision-language models\n(VLMs) through large-scale training, their ability to comprehend negation\nremains underexplored. This study addresses the question: how well do current\nVLMs understand negation? We introduce NegBench, a new benchmark designed to\nevaluate negation understanding across 18 task variations and 79k examples\nspanning image, video, and medical datasets. The benchmark consists of two core\ntasks designed to evaluate negation understanding in diverse multimodal\nsettings: Retrieval with Negation and Multiple Choice Questions with Negated\nCaptions. Our evaluation reveals that modern VLMs struggle significantly with\nnegation, often performing at chance level. To address these shortcomings, we\nexplore a data-centric approach wherein we finetune CLIP models on large-scale\nsynthetic datasets containing millions of negated captions. We show that this\napproach can result in a 10% increase in recall on negated queries and a 40%\nboost in accuracy on multiple-choice questions with negated captions.\n", "link": "http://arxiv.org/abs/2501.09425v1", "date": "2025-01-16", "relevancy": 2.6809, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5788}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Models%20Do%20Not%20Understand%20Negation&body=Title%3A%20Vision-Language%20Models%20Do%20Not%20Understand%20Negation%0AAuthor%3A%20Kumail%20Alhamoud%20and%20Shaden%20Alshammari%20and%20Yonglong%20Tian%20and%20Guohao%20Li%20and%20Philip%20Torr%20and%20Yoon%20Kim%20and%20Marzyeh%20Ghassemi%0AAbstract%3A%20%20%20Many%20practical%20vision-language%20applications%20require%20models%20that%20understand%0Anegation%2C%20e.g.%2C%20when%20using%20natural%20language%20to%20retrieve%20images%20which%20contain%0Acertain%20objects%20but%20not%20others.%20Despite%20advancements%20in%20vision-language%20models%0A%28VLMs%29%20through%20large-scale%20training%2C%20their%20ability%20to%20comprehend%20negation%0Aremains%20underexplored.%20This%20study%20addresses%20the%20question%3A%20how%20well%20do%20current%0AVLMs%20understand%20negation%3F%20We%20introduce%20NegBench%2C%20a%20new%20benchmark%20designed%20to%0Aevaluate%20negation%20understanding%20across%2018%20task%20variations%20and%2079k%20examples%0Aspanning%20image%2C%20video%2C%20and%20medical%20datasets.%20The%20benchmark%20consists%20of%20two%20core%0Atasks%20designed%20to%20evaluate%20negation%20understanding%20in%20diverse%20multimodal%0Asettings%3A%20Retrieval%20with%20Negation%20and%20Multiple%20Choice%20Questions%20with%20Negated%0ACaptions.%20Our%20evaluation%20reveals%20that%20modern%20VLMs%20struggle%20significantly%20with%0Anegation%2C%20often%20performing%20at%20chance%20level.%20To%20address%20these%20shortcomings%2C%20we%0Aexplore%20a%20data-centric%20approach%20wherein%20we%20finetune%20CLIP%20models%20on%20large-scale%0Asynthetic%20datasets%20containing%20millions%20of%20negated%20captions.%20We%20show%20that%20this%0Aapproach%20can%20result%20in%20a%2010%25%20increase%20in%20recall%20on%20negated%20queries%20and%20a%2040%25%0Aboost%20in%20accuracy%20on%20multiple-choice%20questions%20with%20negated%20captions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Models%2520Do%2520Not%2520Understand%2520Negation%26entry.906535625%3DKumail%2520Alhamoud%2520and%2520Shaden%2520Alshammari%2520and%2520Yonglong%2520Tian%2520and%2520Guohao%2520Li%2520and%2520Philip%2520Torr%2520and%2520Yoon%2520Kim%2520and%2520Marzyeh%2520Ghassemi%26entry.1292438233%3D%2520%2520Many%2520practical%2520vision-language%2520applications%2520require%2520models%2520that%2520understand%250Anegation%252C%2520e.g.%252C%2520when%2520using%2520natural%2520language%2520to%2520retrieve%2520images%2520which%2520contain%250Acertain%2520objects%2520but%2520not%2520others.%2520Despite%2520advancements%2520in%2520vision-language%2520models%250A%2528VLMs%2529%2520through%2520large-scale%2520training%252C%2520their%2520ability%2520to%2520comprehend%2520negation%250Aremains%2520underexplored.%2520This%2520study%2520addresses%2520the%2520question%253A%2520how%2520well%2520do%2520current%250AVLMs%2520understand%2520negation%253F%2520We%2520introduce%2520NegBench%252C%2520a%2520new%2520benchmark%2520designed%2520to%250Aevaluate%2520negation%2520understanding%2520across%252018%2520task%2520variations%2520and%252079k%2520examples%250Aspanning%2520image%252C%2520video%252C%2520and%2520medical%2520datasets.%2520The%2520benchmark%2520consists%2520of%2520two%2520core%250Atasks%2520designed%2520to%2520evaluate%2520negation%2520understanding%2520in%2520diverse%2520multimodal%250Asettings%253A%2520Retrieval%2520with%2520Negation%2520and%2520Multiple%2520Choice%2520Questions%2520with%2520Negated%250ACaptions.%2520Our%2520evaluation%2520reveals%2520that%2520modern%2520VLMs%2520struggle%2520significantly%2520with%250Anegation%252C%2520often%2520performing%2520at%2520chance%2520level.%2520To%2520address%2520these%2520shortcomings%252C%2520we%250Aexplore%2520a%2520data-centric%2520approach%2520wherein%2520we%2520finetune%2520CLIP%2520models%2520on%2520large-scale%250Asynthetic%2520datasets%2520containing%2520millions%2520of%2520negated%2520captions.%2520We%2520show%2520that%2520this%250Aapproach%2520can%2520result%2520in%2520a%252010%2525%2520increase%2520in%2520recall%2520on%2520negated%2520queries%2520and%2520a%252040%2525%250Aboost%2520in%2520accuracy%2520on%2520multiple-choice%2520questions%2520with%2520negated%2520captions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Models%20Do%20Not%20Understand%20Negation&entry.906535625=Kumail%20Alhamoud%20and%20Shaden%20Alshammari%20and%20Yonglong%20Tian%20and%20Guohao%20Li%20and%20Philip%20Torr%20and%20Yoon%20Kim%20and%20Marzyeh%20Ghassemi&entry.1292438233=%20%20Many%20practical%20vision-language%20applications%20require%20models%20that%20understand%0Anegation%2C%20e.g.%2C%20when%20using%20natural%20language%20to%20retrieve%20images%20which%20contain%0Acertain%20objects%20but%20not%20others.%20Despite%20advancements%20in%20vision-language%20models%0A%28VLMs%29%20through%20large-scale%20training%2C%20their%20ability%20to%20comprehend%20negation%0Aremains%20underexplored.%20This%20study%20addresses%20the%20question%3A%20how%20well%20do%20current%0AVLMs%20understand%20negation%3F%20We%20introduce%20NegBench%2C%20a%20new%20benchmark%20designed%20to%0Aevaluate%20negation%20understanding%20across%2018%20task%20variations%20and%2079k%20examples%0Aspanning%20image%2C%20video%2C%20and%20medical%20datasets.%20The%20benchmark%20consists%20of%20two%20core%0Atasks%20designed%20to%20evaluate%20negation%20understanding%20in%20diverse%20multimodal%0Asettings%3A%20Retrieval%20with%20Negation%20and%20Multiple%20Choice%20Questions%20with%20Negated%0ACaptions.%20Our%20evaluation%20reveals%20that%20modern%20VLMs%20struggle%20significantly%20with%0Anegation%2C%20often%20performing%20at%20chance%20level.%20To%20address%20these%20shortcomings%2C%20we%0Aexplore%20a%20data-centric%20approach%20wherein%20we%20finetune%20CLIP%20models%20on%20large-scale%0Asynthetic%20datasets%20containing%20millions%20of%20negated%20captions.%20We%20show%20that%20this%0Aapproach%20can%20result%20in%20a%2010%25%20increase%20in%20recall%20on%20negated%20queries%20and%20a%2040%25%0Aboost%20in%20accuracy%20on%20multiple-choice%20questions%20with%20negated%20captions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09425v1&entry.124074799=Read"},
{"title": "Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective\n  Scenes", "author": "Ji Shi and Xianghua Ying and Ruohao Guo and Bowei Xing and Wenzhen Yue", "abstract": "  Neural Radiance Fields (NeRF) often struggle with reconstructing and\nrendering highly reflective scenes. Recent advancements have developed various\nreflection-aware appearance models to enhance NeRF's capability to render\nspecular reflections. However, the robust reconstruction of highly reflective\nscenes is still hindered by the inherent shape ambiguity on specular surfaces.\nExisting methods typically rely on additional geometry priors to regularize the\nshape prediction, but this can lead to oversmoothed geometry in complex scenes.\nObserving the critical role of surface normals in parameterizing reflections,\nwe introduce a transmittance-gradient-based normal estimation technique that\nremains robust even under ambiguous shape conditions. Furthermore, we propose a\ndual activated densities module that effectively bridges the gap between smooth\nsurface normals and sharp object boundaries. Combined with a reflection-aware\nappearance model, our proposed method achieves robust reconstruction and\nhigh-fidelity rendering of scenes featuring both highly specular reflections\nand intricate geometric structures. Extensive experiments demonstrate that our\nmethod outperforms existing state-of-the-art methods on various datasets.\n", "link": "http://arxiv.org/abs/2501.09460v1", "date": "2025-01-16", "relevancy": 2.6756, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.546}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5405}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normal-NeRF%3A%20Ambiguity-Robust%20Normal%20Estimation%20for%20Highly%20Reflective%0A%20%20Scenes&body=Title%3A%20Normal-NeRF%3A%20Ambiguity-Robust%20Normal%20Estimation%20for%20Highly%20Reflective%0A%20%20Scenes%0AAuthor%3A%20Ji%20Shi%20and%20Xianghua%20Ying%20and%20Ruohao%20Guo%20and%20Bowei%20Xing%20and%20Wenzhen%20Yue%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20often%20struggle%20with%20reconstructing%20and%0Arendering%20highly%20reflective%20scenes.%20Recent%20advancements%20have%20developed%20various%0Areflection-aware%20appearance%20models%20to%20enhance%20NeRF%27s%20capability%20to%20render%0Aspecular%20reflections.%20However%2C%20the%20robust%20reconstruction%20of%20highly%20reflective%0Ascenes%20is%20still%20hindered%20by%20the%20inherent%20shape%20ambiguity%20on%20specular%20surfaces.%0AExisting%20methods%20typically%20rely%20on%20additional%20geometry%20priors%20to%20regularize%20the%0Ashape%20prediction%2C%20but%20this%20can%20lead%20to%20oversmoothed%20geometry%20in%20complex%20scenes.%0AObserving%20the%20critical%20role%20of%20surface%20normals%20in%20parameterizing%20reflections%2C%0Awe%20introduce%20a%20transmittance-gradient-based%20normal%20estimation%20technique%20that%0Aremains%20robust%20even%20under%20ambiguous%20shape%20conditions.%20Furthermore%2C%20we%20propose%20a%0Adual%20activated%20densities%20module%20that%20effectively%20bridges%20the%20gap%20between%20smooth%0Asurface%20normals%20and%20sharp%20object%20boundaries.%20Combined%20with%20a%20reflection-aware%0Aappearance%20model%2C%20our%20proposed%20method%20achieves%20robust%20reconstruction%20and%0Ahigh-fidelity%20rendering%20of%20scenes%20featuring%20both%20highly%20specular%20reflections%0Aand%20intricate%20geometric%20structures.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20outperforms%20existing%20state-of-the-art%20methods%20on%20various%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormal-NeRF%253A%2520Ambiguity-Robust%2520Normal%2520Estimation%2520for%2520Highly%2520Reflective%250A%2520%2520Scenes%26entry.906535625%3DJi%2520Shi%2520and%2520Xianghua%2520Ying%2520and%2520Ruohao%2520Guo%2520and%2520Bowei%2520Xing%2520and%2520Wenzhen%2520Yue%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520often%2520struggle%2520with%2520reconstructing%2520and%250Arendering%2520highly%2520reflective%2520scenes.%2520Recent%2520advancements%2520have%2520developed%2520various%250Areflection-aware%2520appearance%2520models%2520to%2520enhance%2520NeRF%2527s%2520capability%2520to%2520render%250Aspecular%2520reflections.%2520However%252C%2520the%2520robust%2520reconstruction%2520of%2520highly%2520reflective%250Ascenes%2520is%2520still%2520hindered%2520by%2520the%2520inherent%2520shape%2520ambiguity%2520on%2520specular%2520surfaces.%250AExisting%2520methods%2520typically%2520rely%2520on%2520additional%2520geometry%2520priors%2520to%2520regularize%2520the%250Ashape%2520prediction%252C%2520but%2520this%2520can%2520lead%2520to%2520oversmoothed%2520geometry%2520in%2520complex%2520scenes.%250AObserving%2520the%2520critical%2520role%2520of%2520surface%2520normals%2520in%2520parameterizing%2520reflections%252C%250Awe%2520introduce%2520a%2520transmittance-gradient-based%2520normal%2520estimation%2520technique%2520that%250Aremains%2520robust%2520even%2520under%2520ambiguous%2520shape%2520conditions.%2520Furthermore%252C%2520we%2520propose%2520a%250Adual%2520activated%2520densities%2520module%2520that%2520effectively%2520bridges%2520the%2520gap%2520between%2520smooth%250Asurface%2520normals%2520and%2520sharp%2520object%2520boundaries.%2520Combined%2520with%2520a%2520reflection-aware%250Aappearance%2520model%252C%2520our%2520proposed%2520method%2520achieves%2520robust%2520reconstruction%2520and%250Ahigh-fidelity%2520rendering%2520of%2520scenes%2520featuring%2520both%2520highly%2520specular%2520reflections%250Aand%2520intricate%2520geometric%2520structures.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Amethod%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520on%2520various%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normal-NeRF%3A%20Ambiguity-Robust%20Normal%20Estimation%20for%20Highly%20Reflective%0A%20%20Scenes&entry.906535625=Ji%20Shi%20and%20Xianghua%20Ying%20and%20Ruohao%20Guo%20and%20Bowei%20Xing%20and%20Wenzhen%20Yue&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20often%20struggle%20with%20reconstructing%20and%0Arendering%20highly%20reflective%20scenes.%20Recent%20advancements%20have%20developed%20various%0Areflection-aware%20appearance%20models%20to%20enhance%20NeRF%27s%20capability%20to%20render%0Aspecular%20reflections.%20However%2C%20the%20robust%20reconstruction%20of%20highly%20reflective%0Ascenes%20is%20still%20hindered%20by%20the%20inherent%20shape%20ambiguity%20on%20specular%20surfaces.%0AExisting%20methods%20typically%20rely%20on%20additional%20geometry%20priors%20to%20regularize%20the%0Ashape%20prediction%2C%20but%20this%20can%20lead%20to%20oversmoothed%20geometry%20in%20complex%20scenes.%0AObserving%20the%20critical%20role%20of%20surface%20normals%20in%20parameterizing%20reflections%2C%0Awe%20introduce%20a%20transmittance-gradient-based%20normal%20estimation%20technique%20that%0Aremains%20robust%20even%20under%20ambiguous%20shape%20conditions.%20Furthermore%2C%20we%20propose%20a%0Adual%20activated%20densities%20module%20that%20effectively%20bridges%20the%20gap%20between%20smooth%0Asurface%20normals%20and%20sharp%20object%20boundaries.%20Combined%20with%20a%20reflection-aware%0Aappearance%20model%2C%20our%20proposed%20method%20achieves%20robust%20reconstruction%20and%0Ahigh-fidelity%20rendering%20of%20scenes%20featuring%20both%20highly%20specular%20reflections%0Aand%20intricate%20geometric%20structures.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20outperforms%20existing%20state-of-the-art%20methods%20on%20various%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09460v1&entry.124074799=Read"},
{"title": "Enhancing Few-Shot Image Classification through Learnable Multi-Scale\n  Embedding and Attention Mechanisms", "author": "Fatemeh Askari and Amirreza Fateh and Mohammad Reza Mohammadi", "abstract": "  In the context of few-shot classification, the goal is to train a classifier\nusing a limited number of samples while maintaining satisfactory performance.\nHowever, traditional metric-based methods exhibit certain limitations in\nachieving this objective. These methods typically rely on a single distance\nvalue between the query feature and support feature, thereby overlooking the\ncontribution of shallow features. To overcome this challenge, we propose a\nnovel approach in this paper. Our approach involves utilizing a multi-output\nembedding network that maps samples into distinct feature spaces. The proposed\nmethod extracts feature vectors at different stages, enabling the model to\ncapture both global and abstract features. By utilizing these diverse feature\nspaces, our model enhances its performance. Moreover, employing a\nself-attention mechanism improves the refinement of features at each stage,\nleading to even more robust representations and improved overall performance.\nFurthermore, assigning learnable weights to each stage significantly improved\nperformance and results. We conducted comprehensive evaluations on the\nMiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way\n5-shot scenarios. Additionally, we performed cross-domain tasks across eight\nbenchmark datasets, achieving high accuracy in the testing domains. These\nevaluations demonstrate the efficacy of our proposed method in comparison to\nstate-of-the-art approaches. https://github.com/FatemehAskari/MSENet\n", "link": "http://arxiv.org/abs/2409.07989v2", "date": "2025-01-16", "relevancy": 2.6608, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5262}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Few-Shot%20Image%20Classification%20through%20Learnable%20Multi-Scale%0A%20%20Embedding%20and%20Attention%20Mechanisms&body=Title%3A%20Enhancing%20Few-Shot%20Image%20Classification%20through%20Learnable%20Multi-Scale%0A%20%20Embedding%20and%20Attention%20Mechanisms%0AAuthor%3A%20Fatemeh%20Askari%20and%20Amirreza%20Fateh%20and%20Mohammad%20Reza%20Mohammadi%0AAbstract%3A%20%20%20In%20the%20context%20of%20few-shot%20classification%2C%20the%20goal%20is%20to%20train%20a%20classifier%0Ausing%20a%20limited%20number%20of%20samples%20while%20maintaining%20satisfactory%20performance.%0AHowever%2C%20traditional%20metric-based%20methods%20exhibit%20certain%20limitations%20in%0Aachieving%20this%20objective.%20These%20methods%20typically%20rely%20on%20a%20single%20distance%0Avalue%20between%20the%20query%20feature%20and%20support%20feature%2C%20thereby%20overlooking%20the%0Acontribution%20of%20shallow%20features.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%0Anovel%20approach%20in%20this%20paper.%20Our%20approach%20involves%20utilizing%20a%20multi-output%0Aembedding%20network%20that%20maps%20samples%20into%20distinct%20feature%20spaces.%20The%20proposed%0Amethod%20extracts%20feature%20vectors%20at%20different%20stages%2C%20enabling%20the%20model%20to%0Acapture%20both%20global%20and%20abstract%20features.%20By%20utilizing%20these%20diverse%20feature%0Aspaces%2C%20our%20model%20enhances%20its%20performance.%20Moreover%2C%20employing%20a%0Aself-attention%20mechanism%20improves%20the%20refinement%20of%20features%20at%20each%20stage%2C%0Aleading%20to%20even%20more%20robust%20representations%20and%20improved%20overall%20performance.%0AFurthermore%2C%20assigning%20learnable%20weights%20to%20each%20stage%20significantly%20improved%0Aperformance%20and%20results.%20We%20conducted%20comprehensive%20evaluations%20on%20the%0AMiniImageNet%20and%20FC100%20datasets%2C%20specifically%20in%20the%205-way%201-shot%20and%205-way%0A5-shot%20scenarios.%20Additionally%2C%20we%20performed%20cross-domain%20tasks%20across%20eight%0Abenchmark%20datasets%2C%20achieving%20high%20accuracy%20in%20the%20testing%20domains.%20These%0Aevaluations%20demonstrate%20the%20efficacy%20of%20our%20proposed%20method%20in%20comparison%20to%0Astate-of-the-art%20approaches.%20https%3A//github.com/FatemehAskari/MSENet%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07989v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Few-Shot%2520Image%2520Classification%2520through%2520Learnable%2520Multi-Scale%250A%2520%2520Embedding%2520and%2520Attention%2520Mechanisms%26entry.906535625%3DFatemeh%2520Askari%2520and%2520Amirreza%2520Fateh%2520and%2520Mohammad%2520Reza%2520Mohammadi%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520few-shot%2520classification%252C%2520the%2520goal%2520is%2520to%2520train%2520a%2520classifier%250Ausing%2520a%2520limited%2520number%2520of%2520samples%2520while%2520maintaining%2520satisfactory%2520performance.%250AHowever%252C%2520traditional%2520metric-based%2520methods%2520exhibit%2520certain%2520limitations%2520in%250Aachieving%2520this%2520objective.%2520These%2520methods%2520typically%2520rely%2520on%2520a%2520single%2520distance%250Avalue%2520between%2520the%2520query%2520feature%2520and%2520support%2520feature%252C%2520thereby%2520overlooking%2520the%250Acontribution%2520of%2520shallow%2520features.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%2520a%250Anovel%2520approach%2520in%2520this%2520paper.%2520Our%2520approach%2520involves%2520utilizing%2520a%2520multi-output%250Aembedding%2520network%2520that%2520maps%2520samples%2520into%2520distinct%2520feature%2520spaces.%2520The%2520proposed%250Amethod%2520extracts%2520feature%2520vectors%2520at%2520different%2520stages%252C%2520enabling%2520the%2520model%2520to%250Acapture%2520both%2520global%2520and%2520abstract%2520features.%2520By%2520utilizing%2520these%2520diverse%2520feature%250Aspaces%252C%2520our%2520model%2520enhances%2520its%2520performance.%2520Moreover%252C%2520employing%2520a%250Aself-attention%2520mechanism%2520improves%2520the%2520refinement%2520of%2520features%2520at%2520each%2520stage%252C%250Aleading%2520to%2520even%2520more%2520robust%2520representations%2520and%2520improved%2520overall%2520performance.%250AFurthermore%252C%2520assigning%2520learnable%2520weights%2520to%2520each%2520stage%2520significantly%2520improved%250Aperformance%2520and%2520results.%2520We%2520conducted%2520comprehensive%2520evaluations%2520on%2520the%250AMiniImageNet%2520and%2520FC100%2520datasets%252C%2520specifically%2520in%2520the%25205-way%25201-shot%2520and%25205-way%250A5-shot%2520scenarios.%2520Additionally%252C%2520we%2520performed%2520cross-domain%2520tasks%2520across%2520eight%250Abenchmark%2520datasets%252C%2520achieving%2520high%2520accuracy%2520in%2520the%2520testing%2520domains.%2520These%250Aevaluations%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520proposed%2520method%2520in%2520comparison%2520to%250Astate-of-the-art%2520approaches.%2520https%253A//github.com/FatemehAskari/MSENet%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07989v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Few-Shot%20Image%20Classification%20through%20Learnable%20Multi-Scale%0A%20%20Embedding%20and%20Attention%20Mechanisms&entry.906535625=Fatemeh%20Askari%20and%20Amirreza%20Fateh%20and%20Mohammad%20Reza%20Mohammadi&entry.1292438233=%20%20In%20the%20context%20of%20few-shot%20classification%2C%20the%20goal%20is%20to%20train%20a%20classifier%0Ausing%20a%20limited%20number%20of%20samples%20while%20maintaining%20satisfactory%20performance.%0AHowever%2C%20traditional%20metric-based%20methods%20exhibit%20certain%20limitations%20in%0Aachieving%20this%20objective.%20These%20methods%20typically%20rely%20on%20a%20single%20distance%0Avalue%20between%20the%20query%20feature%20and%20support%20feature%2C%20thereby%20overlooking%20the%0Acontribution%20of%20shallow%20features.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%0Anovel%20approach%20in%20this%20paper.%20Our%20approach%20involves%20utilizing%20a%20multi-output%0Aembedding%20network%20that%20maps%20samples%20into%20distinct%20feature%20spaces.%20The%20proposed%0Amethod%20extracts%20feature%20vectors%20at%20different%20stages%2C%20enabling%20the%20model%20to%0Acapture%20both%20global%20and%20abstract%20features.%20By%20utilizing%20these%20diverse%20feature%0Aspaces%2C%20our%20model%20enhances%20its%20performance.%20Moreover%2C%20employing%20a%0Aself-attention%20mechanism%20improves%20the%20refinement%20of%20features%20at%20each%20stage%2C%0Aleading%20to%20even%20more%20robust%20representations%20and%20improved%20overall%20performance.%0AFurthermore%2C%20assigning%20learnable%20weights%20to%20each%20stage%20significantly%20improved%0Aperformance%20and%20results.%20We%20conducted%20comprehensive%20evaluations%20on%20the%0AMiniImageNet%20and%20FC100%20datasets%2C%20specifically%20in%20the%205-way%201-shot%20and%205-way%0A5-shot%20scenarios.%20Additionally%2C%20we%20performed%20cross-domain%20tasks%20across%20eight%0Abenchmark%20datasets%2C%20achieving%20high%20accuracy%20in%20the%20testing%20domains.%20These%0Aevaluations%20demonstrate%20the%20efficacy%20of%20our%20proposed%20method%20in%20comparison%20to%0Astate-of-the-art%20approaches.%20https%3A//github.com/FatemehAskari/MSENet%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07989v2&entry.124074799=Read"},
{"title": "aiXcoder-7B: A Lightweight and Effective Large Language Model for Code\n  Processing", "author": "Siyuan Jiang and Jia Li and He Zong and Huanyu Liu and Hao Zhu and Shukai Hu and Erlu Li and Jiazheng Ding and Yu Han and Wei Ning and Gen Wang and Yihong Dong and Kechi Zhang and Ge Li", "abstract": "  Large Language Models (LLMs) have been widely used in code completion, and\nresearchers are focusing on scaling up LLMs to improve their accuracy. However,\nlarger LLMs have lower inference efficiency, affecting developers' experience\nand productivity. In this paper, we propose a lightweight and effective LLM for\ncode completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B\nachieves higher code completion accuracy while having smaller scales (i.e., 7\nbillion parameters). We attribute the superiority of aiXcoder-7B to three key\nfactors: (1) Multi-objective training. We employ three training objectives, one\nof which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers\nthe syntax structures in code and effectively improves the performance of LLMs\nfor code. (2) Diverse data sampling strategies. They consider inter-file\nrelationships and enhance the capability of LLMs in understanding cross-file\ncontexts. (3) Extensive high-quality data. We establish a rigorous data\ncollection pipeline and consume a total of 1.2 trillion unique tokens for\ntraining aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a\nbroad distribution of code. We evaluate aiXcoder-7B in five popular code\ncompletion benchmarks and a new benchmark collected by this paper. The results\nshow that aiXcoder-7B outperforms the latest six LLMs with similar sizes and\neven surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B),\npositioning aiXcoder-7B as a lightweight and effective LLM for academia and\nindustry. Finally, we summarize three valuable insights for helping\npractitioners train the next generations of LLMs for code. aiXcoder-7B has been\nopen-souced and gained significant attention. Until January 2025, aiXcoder-7B\nhas received 2,226 GitHub Stars.\n", "link": "http://arxiv.org/abs/2410.13187v3", "date": "2025-01-16", "relevancy": 2.6357, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20aiXcoder-7B%3A%20A%20Lightweight%20and%20Effective%20Large%20Language%20Model%20for%20Code%0A%20%20Processing&body=Title%3A%20aiXcoder-7B%3A%20A%20Lightweight%20and%20Effective%20Large%20Language%20Model%20for%20Code%0A%20%20Processing%0AAuthor%3A%20Siyuan%20Jiang%20and%20Jia%20Li%20and%20He%20Zong%20and%20Huanyu%20Liu%20and%20Hao%20Zhu%20and%20Shukai%20Hu%20and%20Erlu%20Li%20and%20Jiazheng%20Ding%20and%20Yu%20Han%20and%20Wei%20Ning%20and%20Gen%20Wang%20and%20Yihong%20Dong%20and%20Kechi%20Zhang%20and%20Ge%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20widely%20used%20in%20code%20completion%2C%20and%0Aresearchers%20are%20focusing%20on%20scaling%20up%20LLMs%20to%20improve%20their%20accuracy.%20However%2C%0Alarger%20LLMs%20have%20lower%20inference%20efficiency%2C%20affecting%20developers%27%20experience%0Aand%20productivity.%20In%20this%20paper%2C%20we%20propose%20a%20lightweight%20and%20effective%20LLM%20for%0Acode%20completion%20named%20aiXcoder-7B.%20Compared%20to%20existing%20LLMs%2C%20aiXcoder-7B%0Aachieves%20higher%20code%20completion%20accuracy%20while%20having%20smaller%20scales%20%28i.e.%2C%207%0Abillion%20parameters%29.%20We%20attribute%20the%20superiority%20of%20aiXcoder-7B%20to%20three%20key%0Afactors%3A%20%281%29%20Multi-objective%20training.%20We%20employ%20three%20training%20objectives%2C%20one%0Aof%20which%20is%20our%20proposed%20Structured%20Fill-In-the-Middle%20%28SFIM%29.%20SFIM%20considers%0Athe%20syntax%20structures%20in%20code%20and%20effectively%20improves%20the%20performance%20of%20LLMs%0Afor%20code.%20%282%29%20Diverse%20data%20sampling%20strategies.%20They%20consider%20inter-file%0Arelationships%20and%20enhance%20the%20capability%20of%20LLMs%20in%20understanding%20cross-file%0Acontexts.%20%283%29%20Extensive%20high-quality%20data.%20We%20establish%20a%20rigorous%20data%0Acollection%20pipeline%20and%20consume%20a%20total%20of%201.2%20trillion%20unique%20tokens%20for%0Atraining%20aiXcoder-7B.%20This%20vast%20volume%20of%20data%20enables%20aiXcoder-7B%20to%20learn%20a%0Abroad%20distribution%20of%20code.%20We%20evaluate%20aiXcoder-7B%20in%20five%20popular%20code%0Acompletion%20benchmarks%20and%20a%20new%20benchmark%20collected%20by%20this%20paper.%20The%20results%0Ashow%20that%20aiXcoder-7B%20outperforms%20the%20latest%20six%20LLMs%20with%20similar%20sizes%20and%0Aeven%20surpasses%20four%20larger%20LLMs%20%28e.g.%2C%20StarCoder2-15B%20and%20CodeLlama-34B%29%2C%0Apositioning%20aiXcoder-7B%20as%20a%20lightweight%20and%20effective%20LLM%20for%20academia%20and%0Aindustry.%20Finally%2C%20we%20summarize%20three%20valuable%20insights%20for%20helping%0Apractitioners%20train%20the%20next%20generations%20of%20LLMs%20for%20code.%20aiXcoder-7B%20has%20been%0Aopen-souced%20and%20gained%20significant%20attention.%20Until%20January%202025%2C%20aiXcoder-7B%0Ahas%20received%202%2C226%20GitHub%20Stars.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13187v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DaiXcoder-7B%253A%2520A%2520Lightweight%2520and%2520Effective%2520Large%2520Language%2520Model%2520for%2520Code%250A%2520%2520Processing%26entry.906535625%3DSiyuan%2520Jiang%2520and%2520Jia%2520Li%2520and%2520He%2520Zong%2520and%2520Huanyu%2520Liu%2520and%2520Hao%2520Zhu%2520and%2520Shukai%2520Hu%2520and%2520Erlu%2520Li%2520and%2520Jiazheng%2520Ding%2520and%2520Yu%2520Han%2520and%2520Wei%2520Ning%2520and%2520Gen%2520Wang%2520and%2520Yihong%2520Dong%2520and%2520Kechi%2520Zhang%2520and%2520Ge%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520widely%2520used%2520in%2520code%2520completion%252C%2520and%250Aresearchers%2520are%2520focusing%2520on%2520scaling%2520up%2520LLMs%2520to%2520improve%2520their%2520accuracy.%2520However%252C%250Alarger%2520LLMs%2520have%2520lower%2520inference%2520efficiency%252C%2520affecting%2520developers%2527%2520experience%250Aand%2520productivity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520lightweight%2520and%2520effective%2520LLM%2520for%250Acode%2520completion%2520named%2520aiXcoder-7B.%2520Compared%2520to%2520existing%2520LLMs%252C%2520aiXcoder-7B%250Aachieves%2520higher%2520code%2520completion%2520accuracy%2520while%2520having%2520smaller%2520scales%2520%2528i.e.%252C%25207%250Abillion%2520parameters%2529.%2520We%2520attribute%2520the%2520superiority%2520of%2520aiXcoder-7B%2520to%2520three%2520key%250Afactors%253A%2520%25281%2529%2520Multi-objective%2520training.%2520We%2520employ%2520three%2520training%2520objectives%252C%2520one%250Aof%2520which%2520is%2520our%2520proposed%2520Structured%2520Fill-In-the-Middle%2520%2528SFIM%2529.%2520SFIM%2520considers%250Athe%2520syntax%2520structures%2520in%2520code%2520and%2520effectively%2520improves%2520the%2520performance%2520of%2520LLMs%250Afor%2520code.%2520%25282%2529%2520Diverse%2520data%2520sampling%2520strategies.%2520They%2520consider%2520inter-file%250Arelationships%2520and%2520enhance%2520the%2520capability%2520of%2520LLMs%2520in%2520understanding%2520cross-file%250Acontexts.%2520%25283%2529%2520Extensive%2520high-quality%2520data.%2520We%2520establish%2520a%2520rigorous%2520data%250Acollection%2520pipeline%2520and%2520consume%2520a%2520total%2520of%25201.2%2520trillion%2520unique%2520tokens%2520for%250Atraining%2520aiXcoder-7B.%2520This%2520vast%2520volume%2520of%2520data%2520enables%2520aiXcoder-7B%2520to%2520learn%2520a%250Abroad%2520distribution%2520of%2520code.%2520We%2520evaluate%2520aiXcoder-7B%2520in%2520five%2520popular%2520code%250Acompletion%2520benchmarks%2520and%2520a%2520new%2520benchmark%2520collected%2520by%2520this%2520paper.%2520The%2520results%250Ashow%2520that%2520aiXcoder-7B%2520outperforms%2520the%2520latest%2520six%2520LLMs%2520with%2520similar%2520sizes%2520and%250Aeven%2520surpasses%2520four%2520larger%2520LLMs%2520%2528e.g.%252C%2520StarCoder2-15B%2520and%2520CodeLlama-34B%2529%252C%250Apositioning%2520aiXcoder-7B%2520as%2520a%2520lightweight%2520and%2520effective%2520LLM%2520for%2520academia%2520and%250Aindustry.%2520Finally%252C%2520we%2520summarize%2520three%2520valuable%2520insights%2520for%2520helping%250Apractitioners%2520train%2520the%2520next%2520generations%2520of%2520LLMs%2520for%2520code.%2520aiXcoder-7B%2520has%2520been%250Aopen-souced%2520and%2520gained%2520significant%2520attention.%2520Until%2520January%25202025%252C%2520aiXcoder-7B%250Ahas%2520received%25202%252C226%2520GitHub%2520Stars.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13187v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=aiXcoder-7B%3A%20A%20Lightweight%20and%20Effective%20Large%20Language%20Model%20for%20Code%0A%20%20Processing&entry.906535625=Siyuan%20Jiang%20and%20Jia%20Li%20and%20He%20Zong%20and%20Huanyu%20Liu%20and%20Hao%20Zhu%20and%20Shukai%20Hu%20and%20Erlu%20Li%20and%20Jiazheng%20Ding%20and%20Yu%20Han%20and%20Wei%20Ning%20and%20Gen%20Wang%20and%20Yihong%20Dong%20and%20Kechi%20Zhang%20and%20Ge%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20widely%20used%20in%20code%20completion%2C%20and%0Aresearchers%20are%20focusing%20on%20scaling%20up%20LLMs%20to%20improve%20their%20accuracy.%20However%2C%0Alarger%20LLMs%20have%20lower%20inference%20efficiency%2C%20affecting%20developers%27%20experience%0Aand%20productivity.%20In%20this%20paper%2C%20we%20propose%20a%20lightweight%20and%20effective%20LLM%20for%0Acode%20completion%20named%20aiXcoder-7B.%20Compared%20to%20existing%20LLMs%2C%20aiXcoder-7B%0Aachieves%20higher%20code%20completion%20accuracy%20while%20having%20smaller%20scales%20%28i.e.%2C%207%0Abillion%20parameters%29.%20We%20attribute%20the%20superiority%20of%20aiXcoder-7B%20to%20three%20key%0Afactors%3A%20%281%29%20Multi-objective%20training.%20We%20employ%20three%20training%20objectives%2C%20one%0Aof%20which%20is%20our%20proposed%20Structured%20Fill-In-the-Middle%20%28SFIM%29.%20SFIM%20considers%0Athe%20syntax%20structures%20in%20code%20and%20effectively%20improves%20the%20performance%20of%20LLMs%0Afor%20code.%20%282%29%20Diverse%20data%20sampling%20strategies.%20They%20consider%20inter-file%0Arelationships%20and%20enhance%20the%20capability%20of%20LLMs%20in%20understanding%20cross-file%0Acontexts.%20%283%29%20Extensive%20high-quality%20data.%20We%20establish%20a%20rigorous%20data%0Acollection%20pipeline%20and%20consume%20a%20total%20of%201.2%20trillion%20unique%20tokens%20for%0Atraining%20aiXcoder-7B.%20This%20vast%20volume%20of%20data%20enables%20aiXcoder-7B%20to%20learn%20a%0Abroad%20distribution%20of%20code.%20We%20evaluate%20aiXcoder-7B%20in%20five%20popular%20code%0Acompletion%20benchmarks%20and%20a%20new%20benchmark%20collected%20by%20this%20paper.%20The%20results%0Ashow%20that%20aiXcoder-7B%20outperforms%20the%20latest%20six%20LLMs%20with%20similar%20sizes%20and%0Aeven%20surpasses%20four%20larger%20LLMs%20%28e.g.%2C%20StarCoder2-15B%20and%20CodeLlama-34B%29%2C%0Apositioning%20aiXcoder-7B%20as%20a%20lightweight%20and%20effective%20LLM%20for%20academia%20and%0Aindustry.%20Finally%2C%20we%20summarize%20three%20valuable%20insights%20for%20helping%0Apractitioners%20train%20the%20next%20generations%20of%20LLMs%20for%20code.%20aiXcoder-7B%20has%20been%0Aopen-souced%20and%20gained%20significant%20attention.%20Until%20January%202025%2C%20aiXcoder-7B%0Ahas%20received%202%2C226%20GitHub%20Stars.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13187v3&entry.124074799=Read"},
{"title": "Practical Continual Forgetting for Pre-trained Vision Models", "author": "Hongbo Zhao and Fei Zhu and Bolin Ni and Feng Zhu and Gaofeng Meng and Zhaoxiang Zhang", "abstract": "  For privacy and security concerns, the need to erase unwanted information\nfrom pre-trained vision models is becoming evident nowadays. In real-world\nscenarios, erasure requests originate at any time from both users and model\nowners, and these requests usually form a sequence. Therefore, under such a\nsetting, selective information is expected to be continuously removed from a\npre-trained model while maintaining the rest. We define this problem as\ncontinual forgetting and identify three key challenges. (i) For unwanted\nknowledge, efficient and effective deleting is crucial. (ii) For remaining\nknowledge, the impact brought by the forgetting procedure should be minimal.\n(iii) In real-world scenarios, the training samples may be scarce or partially\nmissing during the process of forgetting. To address them, we first propose\nGroup Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA\nmodules to fine-tune the FFN layers in Transformer blocks for each forgetting\ntask independently, and towards (ii), a simple group sparse regularization is\nadopted, enabling automatic selection of specific LoRA groups and zeroing out\nthe others. To further extend GS-LoRA to more practical scenarios, we\nincorporate prototype information as additional supervision and introduce a\nmore practical approach, GS-LoRA++. For each forgotten class, we move the\nlogits away from its original prototype. For the remaining classes, we pull the\nlogits closer to their respective prototypes. We conduct extensive experiments\non face recognition, object detection and image classification and demonstrate\nthat our method manages to forget specific classes with minimal impact on other\nclasses. Codes have been released on https://github.com/bjzhb666/GS-LoRA.\n", "link": "http://arxiv.org/abs/2501.09705v1", "date": "2025-01-16", "relevancy": 2.6207, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5515}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Practical%20Continual%20Forgetting%20for%20Pre-trained%20Vision%20Models&body=Title%3A%20Practical%20Continual%20Forgetting%20for%20Pre-trained%20Vision%20Models%0AAuthor%3A%20Hongbo%20Zhao%20and%20Fei%20Zhu%20and%20Bolin%20Ni%20and%20Feng%20Zhu%20and%20Gaofeng%20Meng%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20For%20privacy%20and%20security%20concerns%2C%20the%20need%20to%20erase%20unwanted%20information%0Afrom%20pre-trained%20vision%20models%20is%20becoming%20evident%20nowadays.%20In%20real-world%0Ascenarios%2C%20erasure%20requests%20originate%20at%20any%20time%20from%20both%20users%20and%20model%0Aowners%2C%20and%20these%20requests%20usually%20form%20a%20sequence.%20Therefore%2C%20under%20such%20a%0Asetting%2C%20selective%20information%20is%20expected%20to%20be%20continuously%20removed%20from%20a%0Apre-trained%20model%20while%20maintaining%20the%20rest.%20We%20define%20this%20problem%20as%0Acontinual%20forgetting%20and%20identify%20three%20key%20challenges.%20%28i%29%20For%20unwanted%0Aknowledge%2C%20efficient%20and%20effective%20deleting%20is%20crucial.%20%28ii%29%20For%20remaining%0Aknowledge%2C%20the%20impact%20brought%20by%20the%20forgetting%20procedure%20should%20be%20minimal.%0A%28iii%29%20In%20real-world%20scenarios%2C%20the%20training%20samples%20may%20be%20scarce%20or%20partially%0Amissing%20during%20the%20process%20of%20forgetting.%20To%20address%20them%2C%20we%20first%20propose%0AGroup%20Sparse%20LoRA%20%28GS-LoRA%29.%20Specifically%2C%20towards%20%28i%29%2C%20we%20introduce%20LoRA%0Amodules%20to%20fine-tune%20the%20FFN%20layers%20in%20Transformer%20blocks%20for%20each%20forgetting%0Atask%20independently%2C%20and%20towards%20%28ii%29%2C%20a%20simple%20group%20sparse%20regularization%20is%0Aadopted%2C%20enabling%20automatic%20selection%20of%20specific%20LoRA%20groups%20and%20zeroing%20out%0Athe%20others.%20To%20further%20extend%20GS-LoRA%20to%20more%20practical%20scenarios%2C%20we%0Aincorporate%20prototype%20information%20as%20additional%20supervision%20and%20introduce%20a%0Amore%20practical%20approach%2C%20GS-LoRA%2B%2B.%20For%20each%20forgotten%20class%2C%20we%20move%20the%0Alogits%20away%20from%20its%20original%20prototype.%20For%20the%20remaining%20classes%2C%20we%20pull%20the%0Alogits%20closer%20to%20their%20respective%20prototypes.%20We%20conduct%20extensive%20experiments%0Aon%20face%20recognition%2C%20object%20detection%20and%20image%20classification%20and%20demonstrate%0Athat%20our%20method%20manages%20to%20forget%20specific%20classes%20with%20minimal%20impact%20on%20other%0Aclasses.%20Codes%20have%20been%20released%20on%20https%3A//github.com/bjzhb666/GS-LoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPractical%2520Continual%2520Forgetting%2520for%2520Pre-trained%2520Vision%2520Models%26entry.906535625%3DHongbo%2520Zhao%2520and%2520Fei%2520Zhu%2520and%2520Bolin%2520Ni%2520and%2520Feng%2520Zhu%2520and%2520Gaofeng%2520Meng%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520For%2520privacy%2520and%2520security%2520concerns%252C%2520the%2520need%2520to%2520erase%2520unwanted%2520information%250Afrom%2520pre-trained%2520vision%2520models%2520is%2520becoming%2520evident%2520nowadays.%2520In%2520real-world%250Ascenarios%252C%2520erasure%2520requests%2520originate%2520at%2520any%2520time%2520from%2520both%2520users%2520and%2520model%250Aowners%252C%2520and%2520these%2520requests%2520usually%2520form%2520a%2520sequence.%2520Therefore%252C%2520under%2520such%2520a%250Asetting%252C%2520selective%2520information%2520is%2520expected%2520to%2520be%2520continuously%2520removed%2520from%2520a%250Apre-trained%2520model%2520while%2520maintaining%2520the%2520rest.%2520We%2520define%2520this%2520problem%2520as%250Acontinual%2520forgetting%2520and%2520identify%2520three%2520key%2520challenges.%2520%2528i%2529%2520For%2520unwanted%250Aknowledge%252C%2520efficient%2520and%2520effective%2520deleting%2520is%2520crucial.%2520%2528ii%2529%2520For%2520remaining%250Aknowledge%252C%2520the%2520impact%2520brought%2520by%2520the%2520forgetting%2520procedure%2520should%2520be%2520minimal.%250A%2528iii%2529%2520In%2520real-world%2520scenarios%252C%2520the%2520training%2520samples%2520may%2520be%2520scarce%2520or%2520partially%250Amissing%2520during%2520the%2520process%2520of%2520forgetting.%2520To%2520address%2520them%252C%2520we%2520first%2520propose%250AGroup%2520Sparse%2520LoRA%2520%2528GS-LoRA%2529.%2520Specifically%252C%2520towards%2520%2528i%2529%252C%2520we%2520introduce%2520LoRA%250Amodules%2520to%2520fine-tune%2520the%2520FFN%2520layers%2520in%2520Transformer%2520blocks%2520for%2520each%2520forgetting%250Atask%2520independently%252C%2520and%2520towards%2520%2528ii%2529%252C%2520a%2520simple%2520group%2520sparse%2520regularization%2520is%250Aadopted%252C%2520enabling%2520automatic%2520selection%2520of%2520specific%2520LoRA%2520groups%2520and%2520zeroing%2520out%250Athe%2520others.%2520To%2520further%2520extend%2520GS-LoRA%2520to%2520more%2520practical%2520scenarios%252C%2520we%250Aincorporate%2520prototype%2520information%2520as%2520additional%2520supervision%2520and%2520introduce%2520a%250Amore%2520practical%2520approach%252C%2520GS-LoRA%252B%252B.%2520For%2520each%2520forgotten%2520class%252C%2520we%2520move%2520the%250Alogits%2520away%2520from%2520its%2520original%2520prototype.%2520For%2520the%2520remaining%2520classes%252C%2520we%2520pull%2520the%250Alogits%2520closer%2520to%2520their%2520respective%2520prototypes.%2520We%2520conduct%2520extensive%2520experiments%250Aon%2520face%2520recognition%252C%2520object%2520detection%2520and%2520image%2520classification%2520and%2520demonstrate%250Athat%2520our%2520method%2520manages%2520to%2520forget%2520specific%2520classes%2520with%2520minimal%2520impact%2520on%2520other%250Aclasses.%2520Codes%2520have%2520been%2520released%2520on%2520https%253A//github.com/bjzhb666/GS-LoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Practical%20Continual%20Forgetting%20for%20Pre-trained%20Vision%20Models&entry.906535625=Hongbo%20Zhao%20and%20Fei%20Zhu%20and%20Bolin%20Ni%20and%20Feng%20Zhu%20and%20Gaofeng%20Meng%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20For%20privacy%20and%20security%20concerns%2C%20the%20need%20to%20erase%20unwanted%20information%0Afrom%20pre-trained%20vision%20models%20is%20becoming%20evident%20nowadays.%20In%20real-world%0Ascenarios%2C%20erasure%20requests%20originate%20at%20any%20time%20from%20both%20users%20and%20model%0Aowners%2C%20and%20these%20requests%20usually%20form%20a%20sequence.%20Therefore%2C%20under%20such%20a%0Asetting%2C%20selective%20information%20is%20expected%20to%20be%20continuously%20removed%20from%20a%0Apre-trained%20model%20while%20maintaining%20the%20rest.%20We%20define%20this%20problem%20as%0Acontinual%20forgetting%20and%20identify%20three%20key%20challenges.%20%28i%29%20For%20unwanted%0Aknowledge%2C%20efficient%20and%20effective%20deleting%20is%20crucial.%20%28ii%29%20For%20remaining%0Aknowledge%2C%20the%20impact%20brought%20by%20the%20forgetting%20procedure%20should%20be%20minimal.%0A%28iii%29%20In%20real-world%20scenarios%2C%20the%20training%20samples%20may%20be%20scarce%20or%20partially%0Amissing%20during%20the%20process%20of%20forgetting.%20To%20address%20them%2C%20we%20first%20propose%0AGroup%20Sparse%20LoRA%20%28GS-LoRA%29.%20Specifically%2C%20towards%20%28i%29%2C%20we%20introduce%20LoRA%0Amodules%20to%20fine-tune%20the%20FFN%20layers%20in%20Transformer%20blocks%20for%20each%20forgetting%0Atask%20independently%2C%20and%20towards%20%28ii%29%2C%20a%20simple%20group%20sparse%20regularization%20is%0Aadopted%2C%20enabling%20automatic%20selection%20of%20specific%20LoRA%20groups%20and%20zeroing%20out%0Athe%20others.%20To%20further%20extend%20GS-LoRA%20to%20more%20practical%20scenarios%2C%20we%0Aincorporate%20prototype%20information%20as%20additional%20supervision%20and%20introduce%20a%0Amore%20practical%20approach%2C%20GS-LoRA%2B%2B.%20For%20each%20forgotten%20class%2C%20we%20move%20the%0Alogits%20away%20from%20its%20original%20prototype.%20For%20the%20remaining%20classes%2C%20we%20pull%20the%0Alogits%20closer%20to%20their%20respective%20prototypes.%20We%20conduct%20extensive%20experiments%0Aon%20face%20recognition%2C%20object%20detection%20and%20image%20classification%20and%20demonstrate%0Athat%20our%20method%20manages%20to%20forget%20specific%20classes%20with%20minimal%20impact%20on%20other%0Aclasses.%20Codes%20have%20been%20released%20on%20https%3A//github.com/bjzhb666/GS-LoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09705v1&entry.124074799=Read"},
{"title": "Super-class guided Transformer for Zero-Shot Attribute Classification", "author": "Sehyung Kim and Chanhyeong Yang and Jihwan Park and Taehoon Song and Hyunwoo J. Kim", "abstract": "  Attribute classification is crucial for identifying specific characteristics\nwithin image regions. Vision-Language Models (VLMs) have been effective in\nzero-shot tasks by leveraging their general knowledge from large-scale\ndatasets. Recent studies demonstrate that transformer-based models with\nclass-wise queries can effectively address zero-shot multi-label\nclassification. However, poor utilization of the relationship between seen and\nunseen attributes makes the model lack generalizability. Additionally,\nattribute classification generally involves many attributes, making maintaining\nthe model's scalability difficult. To address these issues, we propose\nSuper-class guided transFormer (SugaFormer), a novel framework that leverages\nsuper-classes to enhance scalability and generalizability for zero-shot\nattribute classification. SugaFormer employs Super-class Query Initialization\n(SQI) to reduce the number of queries, utilizing common semantic information\nfrom super-classes, and incorporates Multi-context Decoding (MD) to handle\ndiverse visual cues. To strengthen generalizability, we introduce two knowledge\ntransfer strategies that utilize VLMs. During training, Super-class guided\nConsistency Regularization (SCR) aligns model's features with VLMs using\nsuper-class guided prompts, and during inference, Zero-shot Retrieval-based\nScore Enhancement (ZRSE) refines predictions for unseen attributes. Extensive\nexperiments demonstrate that SugaFormer achieves state-of-the-art performance\nacross three widely-used attribute classification benchmarks under zero-shot,\nand cross-dataset transfer settings. Our code is available at\nhttps://github.com/mlvlab/SugaFormer.\n", "link": "http://arxiv.org/abs/2501.05728v2", "date": "2025-01-16", "relevancy": 2.5689, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.536}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5034}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Super-class%20guided%20Transformer%20for%20Zero-Shot%20Attribute%20Classification&body=Title%3A%20Super-class%20guided%20Transformer%20for%20Zero-Shot%20Attribute%20Classification%0AAuthor%3A%20Sehyung%20Kim%20and%20Chanhyeong%20Yang%20and%20Jihwan%20Park%20and%20Taehoon%20Song%20and%20Hyunwoo%20J.%20Kim%0AAbstract%3A%20%20%20Attribute%20classification%20is%20crucial%20for%20identifying%20specific%20characteristics%0Awithin%20image%20regions.%20Vision-Language%20Models%20%28VLMs%29%20have%20been%20effective%20in%0Azero-shot%20tasks%20by%20leveraging%20their%20general%20knowledge%20from%20large-scale%0Adatasets.%20Recent%20studies%20demonstrate%20that%20transformer-based%20models%20with%0Aclass-wise%20queries%20can%20effectively%20address%20zero-shot%20multi-label%0Aclassification.%20However%2C%20poor%20utilization%20of%20the%20relationship%20between%20seen%20and%0Aunseen%20attributes%20makes%20the%20model%20lack%20generalizability.%20Additionally%2C%0Aattribute%20classification%20generally%20involves%20many%20attributes%2C%20making%20maintaining%0Athe%20model%27s%20scalability%20difficult.%20To%20address%20these%20issues%2C%20we%20propose%0ASuper-class%20guided%20transFormer%20%28SugaFormer%29%2C%20a%20novel%20framework%20that%20leverages%0Asuper-classes%20to%20enhance%20scalability%20and%20generalizability%20for%20zero-shot%0Aattribute%20classification.%20SugaFormer%20employs%20Super-class%20Query%20Initialization%0A%28SQI%29%20to%20reduce%20the%20number%20of%20queries%2C%20utilizing%20common%20semantic%20information%0Afrom%20super-classes%2C%20and%20incorporates%20Multi-context%20Decoding%20%28MD%29%20to%20handle%0Adiverse%20visual%20cues.%20To%20strengthen%20generalizability%2C%20we%20introduce%20two%20knowledge%0Atransfer%20strategies%20that%20utilize%20VLMs.%20During%20training%2C%20Super-class%20guided%0AConsistency%20Regularization%20%28SCR%29%20aligns%20model%27s%20features%20with%20VLMs%20using%0Asuper-class%20guided%20prompts%2C%20and%20during%20inference%2C%20Zero-shot%20Retrieval-based%0AScore%20Enhancement%20%28ZRSE%29%20refines%20predictions%20for%20unseen%20attributes.%20Extensive%0Aexperiments%20demonstrate%20that%20SugaFormer%20achieves%20state-of-the-art%20performance%0Aacross%20three%20widely-used%20attribute%20classification%20benchmarks%20under%20zero-shot%2C%0Aand%20cross-dataset%20transfer%20settings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mlvlab/SugaFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05728v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuper-class%2520guided%2520Transformer%2520for%2520Zero-Shot%2520Attribute%2520Classification%26entry.906535625%3DSehyung%2520Kim%2520and%2520Chanhyeong%2520Yang%2520and%2520Jihwan%2520Park%2520and%2520Taehoon%2520Song%2520and%2520Hyunwoo%2520J.%2520Kim%26entry.1292438233%3D%2520%2520Attribute%2520classification%2520is%2520crucial%2520for%2520identifying%2520specific%2520characteristics%250Awithin%2520image%2520regions.%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520been%2520effective%2520in%250Azero-shot%2520tasks%2520by%2520leveraging%2520their%2520general%2520knowledge%2520from%2520large-scale%250Adatasets.%2520Recent%2520studies%2520demonstrate%2520that%2520transformer-based%2520models%2520with%250Aclass-wise%2520queries%2520can%2520effectively%2520address%2520zero-shot%2520multi-label%250Aclassification.%2520However%252C%2520poor%2520utilization%2520of%2520the%2520relationship%2520between%2520seen%2520and%250Aunseen%2520attributes%2520makes%2520the%2520model%2520lack%2520generalizability.%2520Additionally%252C%250Aattribute%2520classification%2520generally%2520involves%2520many%2520attributes%252C%2520making%2520maintaining%250Athe%2520model%2527s%2520scalability%2520difficult.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250ASuper-class%2520guided%2520transFormer%2520%2528SugaFormer%2529%252C%2520a%2520novel%2520framework%2520that%2520leverages%250Asuper-classes%2520to%2520enhance%2520scalability%2520and%2520generalizability%2520for%2520zero-shot%250Aattribute%2520classification.%2520SugaFormer%2520employs%2520Super-class%2520Query%2520Initialization%250A%2528SQI%2529%2520to%2520reduce%2520the%2520number%2520of%2520queries%252C%2520utilizing%2520common%2520semantic%2520information%250Afrom%2520super-classes%252C%2520and%2520incorporates%2520Multi-context%2520Decoding%2520%2528MD%2529%2520to%2520handle%250Adiverse%2520visual%2520cues.%2520To%2520strengthen%2520generalizability%252C%2520we%2520introduce%2520two%2520knowledge%250Atransfer%2520strategies%2520that%2520utilize%2520VLMs.%2520During%2520training%252C%2520Super-class%2520guided%250AConsistency%2520Regularization%2520%2528SCR%2529%2520aligns%2520model%2527s%2520features%2520with%2520VLMs%2520using%250Asuper-class%2520guided%2520prompts%252C%2520and%2520during%2520inference%252C%2520Zero-shot%2520Retrieval-based%250AScore%2520Enhancement%2520%2528ZRSE%2529%2520refines%2520predictions%2520for%2520unseen%2520attributes.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520SugaFormer%2520achieves%2520state-of-the-art%2520performance%250Aacross%2520three%2520widely-used%2520attribute%2520classification%2520benchmarks%2520under%2520zero-shot%252C%250Aand%2520cross-dataset%2520transfer%2520settings.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mlvlab/SugaFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05728v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Super-class%20guided%20Transformer%20for%20Zero-Shot%20Attribute%20Classification&entry.906535625=Sehyung%20Kim%20and%20Chanhyeong%20Yang%20and%20Jihwan%20Park%20and%20Taehoon%20Song%20and%20Hyunwoo%20J.%20Kim&entry.1292438233=%20%20Attribute%20classification%20is%20crucial%20for%20identifying%20specific%20characteristics%0Awithin%20image%20regions.%20Vision-Language%20Models%20%28VLMs%29%20have%20been%20effective%20in%0Azero-shot%20tasks%20by%20leveraging%20their%20general%20knowledge%20from%20large-scale%0Adatasets.%20Recent%20studies%20demonstrate%20that%20transformer-based%20models%20with%0Aclass-wise%20queries%20can%20effectively%20address%20zero-shot%20multi-label%0Aclassification.%20However%2C%20poor%20utilization%20of%20the%20relationship%20between%20seen%20and%0Aunseen%20attributes%20makes%20the%20model%20lack%20generalizability.%20Additionally%2C%0Aattribute%20classification%20generally%20involves%20many%20attributes%2C%20making%20maintaining%0Athe%20model%27s%20scalability%20difficult.%20To%20address%20these%20issues%2C%20we%20propose%0ASuper-class%20guided%20transFormer%20%28SugaFormer%29%2C%20a%20novel%20framework%20that%20leverages%0Asuper-classes%20to%20enhance%20scalability%20and%20generalizability%20for%20zero-shot%0Aattribute%20classification.%20SugaFormer%20employs%20Super-class%20Query%20Initialization%0A%28SQI%29%20to%20reduce%20the%20number%20of%20queries%2C%20utilizing%20common%20semantic%20information%0Afrom%20super-classes%2C%20and%20incorporates%20Multi-context%20Decoding%20%28MD%29%20to%20handle%0Adiverse%20visual%20cues.%20To%20strengthen%20generalizability%2C%20we%20introduce%20two%20knowledge%0Atransfer%20strategies%20that%20utilize%20VLMs.%20During%20training%2C%20Super-class%20guided%0AConsistency%20Regularization%20%28SCR%29%20aligns%20model%27s%20features%20with%20VLMs%20using%0Asuper-class%20guided%20prompts%2C%20and%20during%20inference%2C%20Zero-shot%20Retrieval-based%0AScore%20Enhancement%20%28ZRSE%29%20refines%20predictions%20for%20unseen%20attributes.%20Extensive%0Aexperiments%20demonstrate%20that%20SugaFormer%20achieves%20state-of-the-art%20performance%0Aacross%20three%20widely-used%20attribute%20classification%20benchmarks%20under%20zero-shot%2C%0Aand%20cross-dataset%20transfer%20settings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mlvlab/SugaFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05728v2&entry.124074799=Read"},
{"title": "Sparsity-Aware Distributed Learning for Gaussian Processes with Linear\n  Multiple Kernel", "author": "Richard Cornelius Suwandi and Zhidi Lin and Feng Yin and Zhiguo Wang and Sergios Theodoridis", "abstract": "  Gaussian processes (GPs) stand as crucial tools in machine learning and\nsignal processing, with their effectiveness hinging on kernel design and\nhyper-parameter optimization. This paper presents a novel GP linear multiple\nkernel (LMK) and a generic sparsity-aware distributed learning framework to\noptimize the hyper-parameters. The newly proposed grid spectral mixture product\n(GSMP) kernel is tailored for multi-dimensional data, effectively reducing the\nnumber of hyper-parameters while maintaining good approximation capability. We\nfurther demonstrate that the associated hyper-parameter optimization of this\nkernel yields sparse solutions. To exploit the inherent sparsity of the\nsolutions, we introduce the Sparse LInear Multiple Kernel Learning (SLIM-KL)\nframework. The framework incorporates a quantized alternating direction method\nof multipliers (ADMM) scheme for collaborative learning among multiple agents,\nwhere the local optimization problem is solved using a distributed successive\nconvex approximation (DSCA) algorithm. SLIM-KL effectively manages large-scale\nhyper-parameter optimization for the proposed kernel, simultaneously ensuring\ndata privacy and minimizing communication costs. Theoretical analysis\nestablishes convergence guarantees for the learning framework, while\nexperiments on diverse datasets demonstrate the superior prediction performance\nand efficiency of our proposed methods.\n", "link": "http://arxiv.org/abs/2309.08201v3", "date": "2025-01-16", "relevancy": 2.5616, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5152}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5148}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparsity-Aware%20Distributed%20Learning%20for%20Gaussian%20Processes%20with%20Linear%0A%20%20Multiple%20Kernel&body=Title%3A%20Sparsity-Aware%20Distributed%20Learning%20for%20Gaussian%20Processes%20with%20Linear%0A%20%20Multiple%20Kernel%0AAuthor%3A%20Richard%20Cornelius%20Suwandi%20and%20Zhidi%20Lin%20and%20Feng%20Yin%20and%20Zhiguo%20Wang%20and%20Sergios%20Theodoridis%0AAbstract%3A%20%20%20Gaussian%20processes%20%28GPs%29%20stand%20as%20crucial%20tools%20in%20machine%20learning%20and%0Asignal%20processing%2C%20with%20their%20effectiveness%20hinging%20on%20kernel%20design%20and%0Ahyper-parameter%20optimization.%20This%20paper%20presents%20a%20novel%20GP%20linear%20multiple%0Akernel%20%28LMK%29%20and%20a%20generic%20sparsity-aware%20distributed%20learning%20framework%20to%0Aoptimize%20the%20hyper-parameters.%20The%20newly%20proposed%20grid%20spectral%20mixture%20product%0A%28GSMP%29%20kernel%20is%20tailored%20for%20multi-dimensional%20data%2C%20effectively%20reducing%20the%0Anumber%20of%20hyper-parameters%20while%20maintaining%20good%20approximation%20capability.%20We%0Afurther%20demonstrate%20that%20the%20associated%20hyper-parameter%20optimization%20of%20this%0Akernel%20yields%20sparse%20solutions.%20To%20exploit%20the%20inherent%20sparsity%20of%20the%0Asolutions%2C%20we%20introduce%20the%20Sparse%20LInear%20Multiple%20Kernel%20Learning%20%28SLIM-KL%29%0Aframework.%20The%20framework%20incorporates%20a%20quantized%20alternating%20direction%20method%0Aof%20multipliers%20%28ADMM%29%20scheme%20for%20collaborative%20learning%20among%20multiple%20agents%2C%0Awhere%20the%20local%20optimization%20problem%20is%20solved%20using%20a%20distributed%20successive%0Aconvex%20approximation%20%28DSCA%29%20algorithm.%20SLIM-KL%20effectively%20manages%20large-scale%0Ahyper-parameter%20optimization%20for%20the%20proposed%20kernel%2C%20simultaneously%20ensuring%0Adata%20privacy%20and%20minimizing%20communication%20costs.%20Theoretical%20analysis%0Aestablishes%20convergence%20guarantees%20for%20the%20learning%20framework%2C%20while%0Aexperiments%20on%20diverse%20datasets%20demonstrate%20the%20superior%20prediction%20performance%0Aand%20efficiency%20of%20our%20proposed%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08201v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsity-Aware%2520Distributed%2520Learning%2520for%2520Gaussian%2520Processes%2520with%2520Linear%250A%2520%2520Multiple%2520Kernel%26entry.906535625%3DRichard%2520Cornelius%2520Suwandi%2520and%2520Zhidi%2520Lin%2520and%2520Feng%2520Yin%2520and%2520Zhiguo%2520Wang%2520and%2520Sergios%2520Theodoridis%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520%2528GPs%2529%2520stand%2520as%2520crucial%2520tools%2520in%2520machine%2520learning%2520and%250Asignal%2520processing%252C%2520with%2520their%2520effectiveness%2520hinging%2520on%2520kernel%2520design%2520and%250Ahyper-parameter%2520optimization.%2520This%2520paper%2520presents%2520a%2520novel%2520GP%2520linear%2520multiple%250Akernel%2520%2528LMK%2529%2520and%2520a%2520generic%2520sparsity-aware%2520distributed%2520learning%2520framework%2520to%250Aoptimize%2520the%2520hyper-parameters.%2520The%2520newly%2520proposed%2520grid%2520spectral%2520mixture%2520product%250A%2528GSMP%2529%2520kernel%2520is%2520tailored%2520for%2520multi-dimensional%2520data%252C%2520effectively%2520reducing%2520the%250Anumber%2520of%2520hyper-parameters%2520while%2520maintaining%2520good%2520approximation%2520capability.%2520We%250Afurther%2520demonstrate%2520that%2520the%2520associated%2520hyper-parameter%2520optimization%2520of%2520this%250Akernel%2520yields%2520sparse%2520solutions.%2520To%2520exploit%2520the%2520inherent%2520sparsity%2520of%2520the%250Asolutions%252C%2520we%2520introduce%2520the%2520Sparse%2520LInear%2520Multiple%2520Kernel%2520Learning%2520%2528SLIM-KL%2529%250Aframework.%2520The%2520framework%2520incorporates%2520a%2520quantized%2520alternating%2520direction%2520method%250Aof%2520multipliers%2520%2528ADMM%2529%2520scheme%2520for%2520collaborative%2520learning%2520among%2520multiple%2520agents%252C%250Awhere%2520the%2520local%2520optimization%2520problem%2520is%2520solved%2520using%2520a%2520distributed%2520successive%250Aconvex%2520approximation%2520%2528DSCA%2529%2520algorithm.%2520SLIM-KL%2520effectively%2520manages%2520large-scale%250Ahyper-parameter%2520optimization%2520for%2520the%2520proposed%2520kernel%252C%2520simultaneously%2520ensuring%250Adata%2520privacy%2520and%2520minimizing%2520communication%2520costs.%2520Theoretical%2520analysis%250Aestablishes%2520convergence%2520guarantees%2520for%2520the%2520learning%2520framework%252C%2520while%250Aexperiments%2520on%2520diverse%2520datasets%2520demonstrate%2520the%2520superior%2520prediction%2520performance%250Aand%2520efficiency%2520of%2520our%2520proposed%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08201v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparsity-Aware%20Distributed%20Learning%20for%20Gaussian%20Processes%20with%20Linear%0A%20%20Multiple%20Kernel&entry.906535625=Richard%20Cornelius%20Suwandi%20and%20Zhidi%20Lin%20and%20Feng%20Yin%20and%20Zhiguo%20Wang%20and%20Sergios%20Theodoridis&entry.1292438233=%20%20Gaussian%20processes%20%28GPs%29%20stand%20as%20crucial%20tools%20in%20machine%20learning%20and%0Asignal%20processing%2C%20with%20their%20effectiveness%20hinging%20on%20kernel%20design%20and%0Ahyper-parameter%20optimization.%20This%20paper%20presents%20a%20novel%20GP%20linear%20multiple%0Akernel%20%28LMK%29%20and%20a%20generic%20sparsity-aware%20distributed%20learning%20framework%20to%0Aoptimize%20the%20hyper-parameters.%20The%20newly%20proposed%20grid%20spectral%20mixture%20product%0A%28GSMP%29%20kernel%20is%20tailored%20for%20multi-dimensional%20data%2C%20effectively%20reducing%20the%0Anumber%20of%20hyper-parameters%20while%20maintaining%20good%20approximation%20capability.%20We%0Afurther%20demonstrate%20that%20the%20associated%20hyper-parameter%20optimization%20of%20this%0Akernel%20yields%20sparse%20solutions.%20To%20exploit%20the%20inherent%20sparsity%20of%20the%0Asolutions%2C%20we%20introduce%20the%20Sparse%20LInear%20Multiple%20Kernel%20Learning%20%28SLIM-KL%29%0Aframework.%20The%20framework%20incorporates%20a%20quantized%20alternating%20direction%20method%0Aof%20multipliers%20%28ADMM%29%20scheme%20for%20collaborative%20learning%20among%20multiple%20agents%2C%0Awhere%20the%20local%20optimization%20problem%20is%20solved%20using%20a%20distributed%20successive%0Aconvex%20approximation%20%28DSCA%29%20algorithm.%20SLIM-KL%20effectively%20manages%20large-scale%0Ahyper-parameter%20optimization%20for%20the%20proposed%20kernel%2C%20simultaneously%20ensuring%0Adata%20privacy%20and%20minimizing%20communication%20costs.%20Theoretical%20analysis%0Aestablishes%20convergence%20guarantees%20for%20the%20learning%20framework%2C%20while%0Aexperiments%20on%20diverse%20datasets%20demonstrate%20the%20superior%20prediction%20performance%0Aand%20efficiency%20of%20our%20proposed%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08201v3&entry.124074799=Read"},
{"title": "VanGogh: A Unified Multimodal Diffusion-based Framework for Video\n  Colorization", "author": "Zixun Fang and Zhiheng Liu and Kai Zhu and Yu Liu and Ka Leong Cheng and Wei Zhai and Yang Cao and Zheng-Jun Zha", "abstract": "  Video colorization aims to transform grayscale videos into vivid color\nrepresentations while maintaining temporal consistency and structural\nintegrity. Existing video colorization methods often suffer from color bleeding\nand lack comprehensive control, particularly under complex motion or diverse\nsemantic cues. To this end, we introduce VanGogh, a unified multimodal\ndiffusion-based framework for video colorization. VanGogh tackles these\nchallenges using a Dual Qformer to align and fuse features from multiple\nmodalities, complemented by a depth-guided generation process and an optical\nflow loss, which help reduce color overflow. Additionally, a color injection\nstrategy and luma channel replacement are implemented to improve generalization\nand mitigate flickering artifacts. Thanks to this design, users can exercise\nboth global and local control over the generation process, resulting in\nhigher-quality colorized videos. Extensive qualitative and quantitative\nevaluations, and user studies, demonstrate that VanGogh achieves superior\ntemporal consistency and color fidelity.Project page:\nhttps://becauseimbatman0.github.io/VanGogh.\n", "link": "http://arxiv.org/abs/2501.09499v1", "date": "2025-01-16", "relevancy": 2.5441, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6625}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6469}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VanGogh%3A%20A%20Unified%20Multimodal%20Diffusion-based%20Framework%20for%20Video%0A%20%20Colorization&body=Title%3A%20VanGogh%3A%20A%20Unified%20Multimodal%20Diffusion-based%20Framework%20for%20Video%0A%20%20Colorization%0AAuthor%3A%20Zixun%20Fang%20and%20Zhiheng%20Liu%20and%20Kai%20Zhu%20and%20Yu%20Liu%20and%20Ka%20Leong%20Cheng%20and%20Wei%20Zhai%20and%20Yang%20Cao%20and%20Zheng-Jun%20Zha%0AAbstract%3A%20%20%20Video%20colorization%20aims%20to%20transform%20grayscale%20videos%20into%20vivid%20color%0Arepresentations%20while%20maintaining%20temporal%20consistency%20and%20structural%0Aintegrity.%20Existing%20video%20colorization%20methods%20often%20suffer%20from%20color%20bleeding%0Aand%20lack%20comprehensive%20control%2C%20particularly%20under%20complex%20motion%20or%20diverse%0Asemantic%20cues.%20To%20this%20end%2C%20we%20introduce%20VanGogh%2C%20a%20unified%20multimodal%0Adiffusion-based%20framework%20for%20video%20colorization.%20VanGogh%20tackles%20these%0Achallenges%20using%20a%20Dual%20Qformer%20to%20align%20and%20fuse%20features%20from%20multiple%0Amodalities%2C%20complemented%20by%20a%20depth-guided%20generation%20process%20and%20an%20optical%0Aflow%20loss%2C%20which%20help%20reduce%20color%20overflow.%20Additionally%2C%20a%20color%20injection%0Astrategy%20and%20luma%20channel%20replacement%20are%20implemented%20to%20improve%20generalization%0Aand%20mitigate%20flickering%20artifacts.%20Thanks%20to%20this%20design%2C%20users%20can%20exercise%0Aboth%20global%20and%20local%20control%20over%20the%20generation%20process%2C%20resulting%20in%0Ahigher-quality%20colorized%20videos.%20Extensive%20qualitative%20and%20quantitative%0Aevaluations%2C%20and%20user%20studies%2C%20demonstrate%20that%20VanGogh%20achieves%20superior%0Atemporal%20consistency%20and%20color%20fidelity.Project%20page%3A%0Ahttps%3A//becauseimbatman0.github.io/VanGogh.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVanGogh%253A%2520A%2520Unified%2520Multimodal%2520Diffusion-based%2520Framework%2520for%2520Video%250A%2520%2520Colorization%26entry.906535625%3DZixun%2520Fang%2520and%2520Zhiheng%2520Liu%2520and%2520Kai%2520Zhu%2520and%2520Yu%2520Liu%2520and%2520Ka%2520Leong%2520Cheng%2520and%2520Wei%2520Zhai%2520and%2520Yang%2520Cao%2520and%2520Zheng-Jun%2520Zha%26entry.1292438233%3D%2520%2520Video%2520colorization%2520aims%2520to%2520transform%2520grayscale%2520videos%2520into%2520vivid%2520color%250Arepresentations%2520while%2520maintaining%2520temporal%2520consistency%2520and%2520structural%250Aintegrity.%2520Existing%2520video%2520colorization%2520methods%2520often%2520suffer%2520from%2520color%2520bleeding%250Aand%2520lack%2520comprehensive%2520control%252C%2520particularly%2520under%2520complex%2520motion%2520or%2520diverse%250Asemantic%2520cues.%2520To%2520this%2520end%252C%2520we%2520introduce%2520VanGogh%252C%2520a%2520unified%2520multimodal%250Adiffusion-based%2520framework%2520for%2520video%2520colorization.%2520VanGogh%2520tackles%2520these%250Achallenges%2520using%2520a%2520Dual%2520Qformer%2520to%2520align%2520and%2520fuse%2520features%2520from%2520multiple%250Amodalities%252C%2520complemented%2520by%2520a%2520depth-guided%2520generation%2520process%2520and%2520an%2520optical%250Aflow%2520loss%252C%2520which%2520help%2520reduce%2520color%2520overflow.%2520Additionally%252C%2520a%2520color%2520injection%250Astrategy%2520and%2520luma%2520channel%2520replacement%2520are%2520implemented%2520to%2520improve%2520generalization%250Aand%2520mitigate%2520flickering%2520artifacts.%2520Thanks%2520to%2520this%2520design%252C%2520users%2520can%2520exercise%250Aboth%2520global%2520and%2520local%2520control%2520over%2520the%2520generation%2520process%252C%2520resulting%2520in%250Ahigher-quality%2520colorized%2520videos.%2520Extensive%2520qualitative%2520and%2520quantitative%250Aevaluations%252C%2520and%2520user%2520studies%252C%2520demonstrate%2520that%2520VanGogh%2520achieves%2520superior%250Atemporal%2520consistency%2520and%2520color%2520fidelity.Project%2520page%253A%250Ahttps%253A//becauseimbatman0.github.io/VanGogh.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VanGogh%3A%20A%20Unified%20Multimodal%20Diffusion-based%20Framework%20for%20Video%0A%20%20Colorization&entry.906535625=Zixun%20Fang%20and%20Zhiheng%20Liu%20and%20Kai%20Zhu%20and%20Yu%20Liu%20and%20Ka%20Leong%20Cheng%20and%20Wei%20Zhai%20and%20Yang%20Cao%20and%20Zheng-Jun%20Zha&entry.1292438233=%20%20Video%20colorization%20aims%20to%20transform%20grayscale%20videos%20into%20vivid%20color%0Arepresentations%20while%20maintaining%20temporal%20consistency%20and%20structural%0Aintegrity.%20Existing%20video%20colorization%20methods%20often%20suffer%20from%20color%20bleeding%0Aand%20lack%20comprehensive%20control%2C%20particularly%20under%20complex%20motion%20or%20diverse%0Asemantic%20cues.%20To%20this%20end%2C%20we%20introduce%20VanGogh%2C%20a%20unified%20multimodal%0Adiffusion-based%20framework%20for%20video%20colorization.%20VanGogh%20tackles%20these%0Achallenges%20using%20a%20Dual%20Qformer%20to%20align%20and%20fuse%20features%20from%20multiple%0Amodalities%2C%20complemented%20by%20a%20depth-guided%20generation%20process%20and%20an%20optical%0Aflow%20loss%2C%20which%20help%20reduce%20color%20overflow.%20Additionally%2C%20a%20color%20injection%0Astrategy%20and%20luma%20channel%20replacement%20are%20implemented%20to%20improve%20generalization%0Aand%20mitigate%20flickering%20artifacts.%20Thanks%20to%20this%20design%2C%20users%20can%20exercise%0Aboth%20global%20and%20local%20control%20over%20the%20generation%20process%2C%20resulting%20in%0Ahigher-quality%20colorized%20videos.%20Extensive%20qualitative%20and%20quantitative%0Aevaluations%2C%20and%20user%20studies%2C%20demonstrate%20that%20VanGogh%20achieves%20superior%0Atemporal%20consistency%20and%20color%20fidelity.Project%20page%3A%0Ahttps%3A//becauseimbatman0.github.io/VanGogh.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09499v1&entry.124074799=Read"},
{"title": "A Simple Aerial Detection Baseline of Multimodal Language Models", "author": "Qingyun Li and Yushi Chen and Xinya Shu and Dong Chen and Xin He and Yi Yu and Xue Yang", "abstract": "  The multimodal language models (MLMs) based on generative pre-trained\nTransformer are considered powerful candidates for unifying various domains and\ntasks. MLMs developed for remote sensing (RS) have demonstrated outstanding\nperformance in multiple tasks, such as visual question answering and visual\ngrounding. In addition to visual grounding that detects specific objects\ncorresponded to given instruction, aerial detection, which detects all objects\nof multiple categories, is also a valuable and challenging task for RS\nfoundation models. However, aerial detection has not been explored by existing\nRS MLMs because the autoregressive prediction mechanism of MLMs differs\nsignificantly from the detection outputs. In this paper, we present a simple\nbaseline for applying MLMs to aerial detection for the first time, named\nLMMRotate. Specifically, we first introduce a normalization method to transform\ndetection outputs into textual outputs to be compatible with the MLM framework.\nThen, we propose a evaluation method, which ensures a fair comparison between\nMLMs and conventional object detection models. We construct the baseline by\nfine-tuning open-source general-purpose MLMs and achieve impressive detection\nperformance comparable to conventional detector. We hope that this baseline\nwill serve as a reference for future MLM development, enabling more\ncomprehensive capabilities for understanding RS images. Code is available at\nhttps://github.com/Li-Qingyun/mllm-mmrotate.\n", "link": "http://arxiv.org/abs/2501.09720v1", "date": "2025-01-16", "relevancy": 2.5324, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5067}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5067}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Aerial%20Detection%20Baseline%20of%20Multimodal%20Language%20Models&body=Title%3A%20A%20Simple%20Aerial%20Detection%20Baseline%20of%20Multimodal%20Language%20Models%0AAuthor%3A%20Qingyun%20Li%20and%20Yushi%20Chen%20and%20Xinya%20Shu%20and%20Dong%20Chen%20and%20Xin%20He%20and%20Yi%20Yu%20and%20Xue%20Yang%0AAbstract%3A%20%20%20The%20multimodal%20language%20models%20%28MLMs%29%20based%20on%20generative%20pre-trained%0ATransformer%20are%20considered%20powerful%20candidates%20for%20unifying%20various%20domains%20and%0Atasks.%20MLMs%20developed%20for%20remote%20sensing%20%28RS%29%20have%20demonstrated%20outstanding%0Aperformance%20in%20multiple%20tasks%2C%20such%20as%20visual%20question%20answering%20and%20visual%0Agrounding.%20In%20addition%20to%20visual%20grounding%20that%20detects%20specific%20objects%0Acorresponded%20to%20given%20instruction%2C%20aerial%20detection%2C%20which%20detects%20all%20objects%0Aof%20multiple%20categories%2C%20is%20also%20a%20valuable%20and%20challenging%20task%20for%20RS%0Afoundation%20models.%20However%2C%20aerial%20detection%20has%20not%20been%20explored%20by%20existing%0ARS%20MLMs%20because%20the%20autoregressive%20prediction%20mechanism%20of%20MLMs%20differs%0Asignificantly%20from%20the%20detection%20outputs.%20In%20this%20paper%2C%20we%20present%20a%20simple%0Abaseline%20for%20applying%20MLMs%20to%20aerial%20detection%20for%20the%20first%20time%2C%20named%0ALMMRotate.%20Specifically%2C%20we%20first%20introduce%20a%20normalization%20method%20to%20transform%0Adetection%20outputs%20into%20textual%20outputs%20to%20be%20compatible%20with%20the%20MLM%20framework.%0AThen%2C%20we%20propose%20a%20evaluation%20method%2C%20which%20ensures%20a%20fair%20comparison%20between%0AMLMs%20and%20conventional%20object%20detection%20models.%20We%20construct%20the%20baseline%20by%0Afine-tuning%20open-source%20general-purpose%20MLMs%20and%20achieve%20impressive%20detection%0Aperformance%20comparable%20to%20conventional%20detector.%20We%20hope%20that%20this%20baseline%0Awill%20serve%20as%20a%20reference%20for%20future%20MLM%20development%2C%20enabling%20more%0Acomprehensive%20capabilities%20for%20understanding%20RS%20images.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Li-Qingyun/mllm-mmrotate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Aerial%2520Detection%2520Baseline%2520of%2520Multimodal%2520Language%2520Models%26entry.906535625%3DQingyun%2520Li%2520and%2520Yushi%2520Chen%2520and%2520Xinya%2520Shu%2520and%2520Dong%2520Chen%2520and%2520Xin%2520He%2520and%2520Yi%2520Yu%2520and%2520Xue%2520Yang%26entry.1292438233%3D%2520%2520The%2520multimodal%2520language%2520models%2520%2528MLMs%2529%2520based%2520on%2520generative%2520pre-trained%250ATransformer%2520are%2520considered%2520powerful%2520candidates%2520for%2520unifying%2520various%2520domains%2520and%250Atasks.%2520MLMs%2520developed%2520for%2520remote%2520sensing%2520%2528RS%2529%2520have%2520demonstrated%2520outstanding%250Aperformance%2520in%2520multiple%2520tasks%252C%2520such%2520as%2520visual%2520question%2520answering%2520and%2520visual%250Agrounding.%2520In%2520addition%2520to%2520visual%2520grounding%2520that%2520detects%2520specific%2520objects%250Acorresponded%2520to%2520given%2520instruction%252C%2520aerial%2520detection%252C%2520which%2520detects%2520all%2520objects%250Aof%2520multiple%2520categories%252C%2520is%2520also%2520a%2520valuable%2520and%2520challenging%2520task%2520for%2520RS%250Afoundation%2520models.%2520However%252C%2520aerial%2520detection%2520has%2520not%2520been%2520explored%2520by%2520existing%250ARS%2520MLMs%2520because%2520the%2520autoregressive%2520prediction%2520mechanism%2520of%2520MLMs%2520differs%250Asignificantly%2520from%2520the%2520detection%2520outputs.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520simple%250Abaseline%2520for%2520applying%2520MLMs%2520to%2520aerial%2520detection%2520for%2520the%2520first%2520time%252C%2520named%250ALMMRotate.%2520Specifically%252C%2520we%2520first%2520introduce%2520a%2520normalization%2520method%2520to%2520transform%250Adetection%2520outputs%2520into%2520textual%2520outputs%2520to%2520be%2520compatible%2520with%2520the%2520MLM%2520framework.%250AThen%252C%2520we%2520propose%2520a%2520evaluation%2520method%252C%2520which%2520ensures%2520a%2520fair%2520comparison%2520between%250AMLMs%2520and%2520conventional%2520object%2520detection%2520models.%2520We%2520construct%2520the%2520baseline%2520by%250Afine-tuning%2520open-source%2520general-purpose%2520MLMs%2520and%2520achieve%2520impressive%2520detection%250Aperformance%2520comparable%2520to%2520conventional%2520detector.%2520We%2520hope%2520that%2520this%2520baseline%250Awill%2520serve%2520as%2520a%2520reference%2520for%2520future%2520MLM%2520development%252C%2520enabling%2520more%250Acomprehensive%2520capabilities%2520for%2520understanding%2520RS%2520images.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Li-Qingyun/mllm-mmrotate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Aerial%20Detection%20Baseline%20of%20Multimodal%20Language%20Models&entry.906535625=Qingyun%20Li%20and%20Yushi%20Chen%20and%20Xinya%20Shu%20and%20Dong%20Chen%20and%20Xin%20He%20and%20Yi%20Yu%20and%20Xue%20Yang&entry.1292438233=%20%20The%20multimodal%20language%20models%20%28MLMs%29%20based%20on%20generative%20pre-trained%0ATransformer%20are%20considered%20powerful%20candidates%20for%20unifying%20various%20domains%20and%0Atasks.%20MLMs%20developed%20for%20remote%20sensing%20%28RS%29%20have%20demonstrated%20outstanding%0Aperformance%20in%20multiple%20tasks%2C%20such%20as%20visual%20question%20answering%20and%20visual%0Agrounding.%20In%20addition%20to%20visual%20grounding%20that%20detects%20specific%20objects%0Acorresponded%20to%20given%20instruction%2C%20aerial%20detection%2C%20which%20detects%20all%20objects%0Aof%20multiple%20categories%2C%20is%20also%20a%20valuable%20and%20challenging%20task%20for%20RS%0Afoundation%20models.%20However%2C%20aerial%20detection%20has%20not%20been%20explored%20by%20existing%0ARS%20MLMs%20because%20the%20autoregressive%20prediction%20mechanism%20of%20MLMs%20differs%0Asignificantly%20from%20the%20detection%20outputs.%20In%20this%20paper%2C%20we%20present%20a%20simple%0Abaseline%20for%20applying%20MLMs%20to%20aerial%20detection%20for%20the%20first%20time%2C%20named%0ALMMRotate.%20Specifically%2C%20we%20first%20introduce%20a%20normalization%20method%20to%20transform%0Adetection%20outputs%20into%20textual%20outputs%20to%20be%20compatible%20with%20the%20MLM%20framework.%0AThen%2C%20we%20propose%20a%20evaluation%20method%2C%20which%20ensures%20a%20fair%20comparison%20between%0AMLMs%20and%20conventional%20object%20detection%20models.%20We%20construct%20the%20baseline%20by%0Afine-tuning%20open-source%20general-purpose%20MLMs%20and%20achieve%20impressive%20detection%0Aperformance%20comparable%20to%20conventional%20detector.%20We%20hope%20that%20this%20baseline%0Awill%20serve%20as%20a%20reference%20for%20future%20MLM%20development%2C%20enabling%20more%0Acomprehensive%20capabilities%20for%20understanding%20RS%20images.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Li-Qingyun/mllm-mmrotate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09720v1&entry.124074799=Read"},
{"title": "Unified Face Matching and Physical-Digital Spoofing Attack Detection", "author": "Arun Kunwar and Ajita Rattani", "abstract": "  Face recognition technology has dramatically transformed the landscape of\nsecurity, surveillance, and authentication systems, offering a user-friendly\nand non-invasive biometric solution. However, despite its significant\nadvantages, face recognition systems face increasing threats from physical and\ndigital spoofing attacks. Current research typically treats face recognition\nand attack detection as distinct classification challenges. This approach\nnecessitates the implementation of separate models for each task, leading to\nconsiderable computational complexity, particularly on devices with limited\nresources. Such inefficiencies can stifle scalability and hinder performance.\nIn response to these challenges, this paper introduces an innovative unified\nmodel designed for face recognition and detection of physical and digital\nattacks. By leveraging the advanced Swin Transformer backbone and incorporating\nHiLo attention in a convolutional neural network framework, we address unified\nface recognition and spoof attack detection more effectively. Moreover, we\nintroduce augmentation techniques that replicate the traits of physical and\ndigital spoofing cues, significantly enhancing our model robustness. Through\ncomprehensive experimental evaluation across various datasets, we showcase the\neffectiveness of our model in unified face recognition and spoof detection.\nAdditionally, we confirm its resilience against unseen physical and digital\nspoofing attacks, underscoring its potential for real-world applications.\n", "link": "http://arxiv.org/abs/2501.09635v1", "date": "2025-01-16", "relevancy": 2.4798, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.511}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4901}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Face%20Matching%20and%20Physical-Digital%20Spoofing%20Attack%20Detection&body=Title%3A%20Unified%20Face%20Matching%20and%20Physical-Digital%20Spoofing%20Attack%20Detection%0AAuthor%3A%20Arun%20Kunwar%20and%20Ajita%20Rattani%0AAbstract%3A%20%20%20Face%20recognition%20technology%20has%20dramatically%20transformed%20the%20landscape%20of%0Asecurity%2C%20surveillance%2C%20and%20authentication%20systems%2C%20offering%20a%20user-friendly%0Aand%20non-invasive%20biometric%20solution.%20However%2C%20despite%20its%20significant%0Aadvantages%2C%20face%20recognition%20systems%20face%20increasing%20threats%20from%20physical%20and%0Adigital%20spoofing%20attacks.%20Current%20research%20typically%20treats%20face%20recognition%0Aand%20attack%20detection%20as%20distinct%20classification%20challenges.%20This%20approach%0Anecessitates%20the%20implementation%20of%20separate%20models%20for%20each%20task%2C%20leading%20to%0Aconsiderable%20computational%20complexity%2C%20particularly%20on%20devices%20with%20limited%0Aresources.%20Such%20inefficiencies%20can%20stifle%20scalability%20and%20hinder%20performance.%0AIn%20response%20to%20these%20challenges%2C%20this%20paper%20introduces%20an%20innovative%20unified%0Amodel%20designed%20for%20face%20recognition%20and%20detection%20of%20physical%20and%20digital%0Aattacks.%20By%20leveraging%20the%20advanced%20Swin%20Transformer%20backbone%20and%20incorporating%0AHiLo%20attention%20in%20a%20convolutional%20neural%20network%20framework%2C%20we%20address%20unified%0Aface%20recognition%20and%20spoof%20attack%20detection%20more%20effectively.%20Moreover%2C%20we%0Aintroduce%20augmentation%20techniques%20that%20replicate%20the%20traits%20of%20physical%20and%0Adigital%20spoofing%20cues%2C%20significantly%20enhancing%20our%20model%20robustness.%20Through%0Acomprehensive%20experimental%20evaluation%20across%20various%20datasets%2C%20we%20showcase%20the%0Aeffectiveness%20of%20our%20model%20in%20unified%20face%20recognition%20and%20spoof%20detection.%0AAdditionally%2C%20we%20confirm%20its%20resilience%20against%20unseen%20physical%20and%20digital%0Aspoofing%20attacks%2C%20underscoring%20its%20potential%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Face%2520Matching%2520and%2520Physical-Digital%2520Spoofing%2520Attack%2520Detection%26entry.906535625%3DArun%2520Kunwar%2520and%2520Ajita%2520Rattani%26entry.1292438233%3D%2520%2520Face%2520recognition%2520technology%2520has%2520dramatically%2520transformed%2520the%2520landscape%2520of%250Asecurity%252C%2520surveillance%252C%2520and%2520authentication%2520systems%252C%2520offering%2520a%2520user-friendly%250Aand%2520non-invasive%2520biometric%2520solution.%2520However%252C%2520despite%2520its%2520significant%250Aadvantages%252C%2520face%2520recognition%2520systems%2520face%2520increasing%2520threats%2520from%2520physical%2520and%250Adigital%2520spoofing%2520attacks.%2520Current%2520research%2520typically%2520treats%2520face%2520recognition%250Aand%2520attack%2520detection%2520as%2520distinct%2520classification%2520challenges.%2520This%2520approach%250Anecessitates%2520the%2520implementation%2520of%2520separate%2520models%2520for%2520each%2520task%252C%2520leading%2520to%250Aconsiderable%2520computational%2520complexity%252C%2520particularly%2520on%2520devices%2520with%2520limited%250Aresources.%2520Such%2520inefficiencies%2520can%2520stifle%2520scalability%2520and%2520hinder%2520performance.%250AIn%2520response%2520to%2520these%2520challenges%252C%2520this%2520paper%2520introduces%2520an%2520innovative%2520unified%250Amodel%2520designed%2520for%2520face%2520recognition%2520and%2520detection%2520of%2520physical%2520and%2520digital%250Aattacks.%2520By%2520leveraging%2520the%2520advanced%2520Swin%2520Transformer%2520backbone%2520and%2520incorporating%250AHiLo%2520attention%2520in%2520a%2520convolutional%2520neural%2520network%2520framework%252C%2520we%2520address%2520unified%250Aface%2520recognition%2520and%2520spoof%2520attack%2520detection%2520more%2520effectively.%2520Moreover%252C%2520we%250Aintroduce%2520augmentation%2520techniques%2520that%2520replicate%2520the%2520traits%2520of%2520physical%2520and%250Adigital%2520spoofing%2520cues%252C%2520significantly%2520enhancing%2520our%2520model%2520robustness.%2520Through%250Acomprehensive%2520experimental%2520evaluation%2520across%2520various%2520datasets%252C%2520we%2520showcase%2520the%250Aeffectiveness%2520of%2520our%2520model%2520in%2520unified%2520face%2520recognition%2520and%2520spoof%2520detection.%250AAdditionally%252C%2520we%2520confirm%2520its%2520resilience%2520against%2520unseen%2520physical%2520and%2520digital%250Aspoofing%2520attacks%252C%2520underscoring%2520its%2520potential%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Face%20Matching%20and%20Physical-Digital%20Spoofing%20Attack%20Detection&entry.906535625=Arun%20Kunwar%20and%20Ajita%20Rattani&entry.1292438233=%20%20Face%20recognition%20technology%20has%20dramatically%20transformed%20the%20landscape%20of%0Asecurity%2C%20surveillance%2C%20and%20authentication%20systems%2C%20offering%20a%20user-friendly%0Aand%20non-invasive%20biometric%20solution.%20However%2C%20despite%20its%20significant%0Aadvantages%2C%20face%20recognition%20systems%20face%20increasing%20threats%20from%20physical%20and%0Adigital%20spoofing%20attacks.%20Current%20research%20typically%20treats%20face%20recognition%0Aand%20attack%20detection%20as%20distinct%20classification%20challenges.%20This%20approach%0Anecessitates%20the%20implementation%20of%20separate%20models%20for%20each%20task%2C%20leading%20to%0Aconsiderable%20computational%20complexity%2C%20particularly%20on%20devices%20with%20limited%0Aresources.%20Such%20inefficiencies%20can%20stifle%20scalability%20and%20hinder%20performance.%0AIn%20response%20to%20these%20challenges%2C%20this%20paper%20introduces%20an%20innovative%20unified%0Amodel%20designed%20for%20face%20recognition%20and%20detection%20of%20physical%20and%20digital%0Aattacks.%20By%20leveraging%20the%20advanced%20Swin%20Transformer%20backbone%20and%20incorporating%0AHiLo%20attention%20in%20a%20convolutional%20neural%20network%20framework%2C%20we%20address%20unified%0Aface%20recognition%20and%20spoof%20attack%20detection%20more%20effectively.%20Moreover%2C%20we%0Aintroduce%20augmentation%20techniques%20that%20replicate%20the%20traits%20of%20physical%20and%0Adigital%20spoofing%20cues%2C%20significantly%20enhancing%20our%20model%20robustness.%20Through%0Acomprehensive%20experimental%20evaluation%20across%20various%20datasets%2C%20we%20showcase%20the%0Aeffectiveness%20of%20our%20model%20in%20unified%20face%20recognition%20and%20spoof%20detection.%0AAdditionally%2C%20we%20confirm%20its%20resilience%20against%20unseen%20physical%20and%20digital%0Aspoofing%20attacks%2C%20underscoring%20its%20potential%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09635v1&entry.124074799=Read"},
{"title": "AudioBERT: Audio Knowledge Augmented Language Model", "author": "Hyunjong Ok and Suho Yoo and Jaeho Lee", "abstract": "  Recent studies have identified that language models, pretrained on text-only\ndatasets, often lack elementary visual knowledge, \\textit{e.g.,} colors of\neveryday objects. Motivated by this observation, we ask whether a similar\nshortcoming exists in terms of the \\textit{auditory} knowledge. To answer this\nquestion, we construct a new dataset called AuditoryBench, which consists of\ntwo novel tasks for evaluating auditory knowledge. Based on our analysis using\nthe benchmark, we find that language models also suffer from a severe lack of\nauditory knowledge. To address this limitation, we propose AudioBERT, a novel\nmethod to augment the auditory knowledge of BERT through a retrieval-based\napproach. First, we detect auditory knowledge spans in prompts to query our\nretrieval model efficiently. Then, we inject audio knowledge into BERT and\nswitch on low-rank adaptation for effective adaptation when audio knowledge is\nrequired. Our experiments demonstrate that AudioBERT is quite effective,\nachieving superior performance on the AuditoryBench. The dataset and code are\navailable at \\bulurl{https://github.com/HJ-Ok/AudioBERT}.\n", "link": "http://arxiv.org/abs/2409.08199v2", "date": "2025-01-16", "relevancy": 2.4506, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AudioBERT%3A%20Audio%20Knowledge%20Augmented%20Language%20Model&body=Title%3A%20AudioBERT%3A%20Audio%20Knowledge%20Augmented%20Language%20Model%0AAuthor%3A%20Hyunjong%20Ok%20and%20Suho%20Yoo%20and%20Jaeho%20Lee%0AAbstract%3A%20%20%20Recent%20studies%20have%20identified%20that%20language%20models%2C%20pretrained%20on%20text-only%0Adatasets%2C%20often%20lack%20elementary%20visual%20knowledge%2C%20%5Ctextit%7Be.g.%2C%7D%20colors%20of%0Aeveryday%20objects.%20Motivated%20by%20this%20observation%2C%20we%20ask%20whether%20a%20similar%0Ashortcoming%20exists%20in%20terms%20of%20the%20%5Ctextit%7Bauditory%7D%20knowledge.%20To%20answer%20this%0Aquestion%2C%20we%20construct%20a%20new%20dataset%20called%20AuditoryBench%2C%20which%20consists%20of%0Atwo%20novel%20tasks%20for%20evaluating%20auditory%20knowledge.%20Based%20on%20our%20analysis%20using%0Athe%20benchmark%2C%20we%20find%20that%20language%20models%20also%20suffer%20from%20a%20severe%20lack%20of%0Aauditory%20knowledge.%20To%20address%20this%20limitation%2C%20we%20propose%20AudioBERT%2C%20a%20novel%0Amethod%20to%20augment%20the%20auditory%20knowledge%20of%20BERT%20through%20a%20retrieval-based%0Aapproach.%20First%2C%20we%20detect%20auditory%20knowledge%20spans%20in%20prompts%20to%20query%20our%0Aretrieval%20model%20efficiently.%20Then%2C%20we%20inject%20audio%20knowledge%20into%20BERT%20and%0Aswitch%20on%20low-rank%20adaptation%20for%20effective%20adaptation%20when%20audio%20knowledge%20is%0Arequired.%20Our%20experiments%20demonstrate%20that%20AudioBERT%20is%20quite%20effective%2C%0Aachieving%20superior%20performance%20on%20the%20AuditoryBench.%20The%20dataset%20and%20code%20are%0Aavailable%20at%20%5Cbulurl%7Bhttps%3A//github.com/HJ-Ok/AudioBERT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08199v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudioBERT%253A%2520Audio%2520Knowledge%2520Augmented%2520Language%2520Model%26entry.906535625%3DHyunjong%2520Ok%2520and%2520Suho%2520Yoo%2520and%2520Jaeho%2520Lee%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520identified%2520that%2520language%2520models%252C%2520pretrained%2520on%2520text-only%250Adatasets%252C%2520often%2520lack%2520elementary%2520visual%2520knowledge%252C%2520%255Ctextit%257Be.g.%252C%257D%2520colors%2520of%250Aeveryday%2520objects.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520ask%2520whether%2520a%2520similar%250Ashortcoming%2520exists%2520in%2520terms%2520of%2520the%2520%255Ctextit%257Bauditory%257D%2520knowledge.%2520To%2520answer%2520this%250Aquestion%252C%2520we%2520construct%2520a%2520new%2520dataset%2520called%2520AuditoryBench%252C%2520which%2520consists%2520of%250Atwo%2520novel%2520tasks%2520for%2520evaluating%2520auditory%2520knowledge.%2520Based%2520on%2520our%2520analysis%2520using%250Athe%2520benchmark%252C%2520we%2520find%2520that%2520language%2520models%2520also%2520suffer%2520from%2520a%2520severe%2520lack%2520of%250Aauditory%2520knowledge.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520AudioBERT%252C%2520a%2520novel%250Amethod%2520to%2520augment%2520the%2520auditory%2520knowledge%2520of%2520BERT%2520through%2520a%2520retrieval-based%250Aapproach.%2520First%252C%2520we%2520detect%2520auditory%2520knowledge%2520spans%2520in%2520prompts%2520to%2520query%2520our%250Aretrieval%2520model%2520efficiently.%2520Then%252C%2520we%2520inject%2520audio%2520knowledge%2520into%2520BERT%2520and%250Aswitch%2520on%2520low-rank%2520adaptation%2520for%2520effective%2520adaptation%2520when%2520audio%2520knowledge%2520is%250Arequired.%2520Our%2520experiments%2520demonstrate%2520that%2520AudioBERT%2520is%2520quite%2520effective%252C%250Aachieving%2520superior%2520performance%2520on%2520the%2520AuditoryBench.%2520The%2520dataset%2520and%2520code%2520are%250Aavailable%2520at%2520%255Cbulurl%257Bhttps%253A//github.com/HJ-Ok/AudioBERT%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08199v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AudioBERT%3A%20Audio%20Knowledge%20Augmented%20Language%20Model&entry.906535625=Hyunjong%20Ok%20and%20Suho%20Yoo%20and%20Jaeho%20Lee&entry.1292438233=%20%20Recent%20studies%20have%20identified%20that%20language%20models%2C%20pretrained%20on%20text-only%0Adatasets%2C%20often%20lack%20elementary%20visual%20knowledge%2C%20%5Ctextit%7Be.g.%2C%7D%20colors%20of%0Aeveryday%20objects.%20Motivated%20by%20this%20observation%2C%20we%20ask%20whether%20a%20similar%0Ashortcoming%20exists%20in%20terms%20of%20the%20%5Ctextit%7Bauditory%7D%20knowledge.%20To%20answer%20this%0Aquestion%2C%20we%20construct%20a%20new%20dataset%20called%20AuditoryBench%2C%20which%20consists%20of%0Atwo%20novel%20tasks%20for%20evaluating%20auditory%20knowledge.%20Based%20on%20our%20analysis%20using%0Athe%20benchmark%2C%20we%20find%20that%20language%20models%20also%20suffer%20from%20a%20severe%20lack%20of%0Aauditory%20knowledge.%20To%20address%20this%20limitation%2C%20we%20propose%20AudioBERT%2C%20a%20novel%0Amethod%20to%20augment%20the%20auditory%20knowledge%20of%20BERT%20through%20a%20retrieval-based%0Aapproach.%20First%2C%20we%20detect%20auditory%20knowledge%20spans%20in%20prompts%20to%20query%20our%0Aretrieval%20model%20efficiently.%20Then%2C%20we%20inject%20audio%20knowledge%20into%20BERT%20and%0Aswitch%20on%20low-rank%20adaptation%20for%20effective%20adaptation%20when%20audio%20knowledge%20is%0Arequired.%20Our%20experiments%20demonstrate%20that%20AudioBERT%20is%20quite%20effective%2C%0Aachieving%20superior%20performance%20on%20the%20AuditoryBench.%20The%20dataset%20and%20code%20are%0Aavailable%20at%20%5Cbulurl%7Bhttps%3A//github.com/HJ-Ok/AudioBERT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08199v2&entry.124074799=Read"},
{"title": "Cueless EEG imagined speech for subject identification: dataset and\n  benchmarks", "author": "Ali Derakhshesh and Zahra Dehghanian and Reza Ebrahimpour and Hamid R. Rabiee", "abstract": "  Electroencephalogram (EEG) signals have emerged as a promising modality for\nbiometric identification. While previous studies have explored the use of\nimagined speech with semantically meaningful words for subject identification,\nmost have relied on additional visual or auditory cues. In this study, we\nintroduce a cueless EEG-based imagined speech paradigm, where subjects imagine\nthe pronunciation of semantically meaningful words without any external cues.\nThis innovative approach addresses the limitations of prior methods by\nrequiring subjects to select and imagine words from a predefined list\nnaturally. The dataset comprises over 4,350 trials from 11 subjects across five\nsessions. We assess a variety of classification methods, including traditional\nmachine learning techniques such as Support Vector Machines (SVM) and XGBoost,\nas well as time-series foundation models and deep learning architectures\nspecifically designed for EEG classification, such as EEG Conformer and Shallow\nConvNet. A session-based hold-out validation strategy was employed to ensure\nreliable evaluation and prevent data leakage. Our results demonstrate\noutstanding classification accuracy, reaching 97.93%. These findings highlight\nthe potential of cueless EEG paradigms for secure and reliable subject\nidentification in real-world applications, such as brain-computer interfaces\n(BCIs).\n", "link": "http://arxiv.org/abs/2501.09700v1", "date": "2025-01-16", "relevancy": 2.4282, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4717}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cueless%20EEG%20imagined%20speech%20for%20subject%20identification%3A%20dataset%20and%0A%20%20benchmarks&body=Title%3A%20Cueless%20EEG%20imagined%20speech%20for%20subject%20identification%3A%20dataset%20and%0A%20%20benchmarks%0AAuthor%3A%20Ali%20Derakhshesh%20and%20Zahra%20Dehghanian%20and%20Reza%20Ebrahimpour%20and%20Hamid%20R.%20Rabiee%0AAbstract%3A%20%20%20Electroencephalogram%20%28EEG%29%20signals%20have%20emerged%20as%20a%20promising%20modality%20for%0Abiometric%20identification.%20While%20previous%20studies%20have%20explored%20the%20use%20of%0Aimagined%20speech%20with%20semantically%20meaningful%20words%20for%20subject%20identification%2C%0Amost%20have%20relied%20on%20additional%20visual%20or%20auditory%20cues.%20In%20this%20study%2C%20we%0Aintroduce%20a%20cueless%20EEG-based%20imagined%20speech%20paradigm%2C%20where%20subjects%20imagine%0Athe%20pronunciation%20of%20semantically%20meaningful%20words%20without%20any%20external%20cues.%0AThis%20innovative%20approach%20addresses%20the%20limitations%20of%20prior%20methods%20by%0Arequiring%20subjects%20to%20select%20and%20imagine%20words%20from%20a%20predefined%20list%0Anaturally.%20The%20dataset%20comprises%20over%204%2C350%20trials%20from%2011%20subjects%20across%20five%0Asessions.%20We%20assess%20a%20variety%20of%20classification%20methods%2C%20including%20traditional%0Amachine%20learning%20techniques%20such%20as%20Support%20Vector%20Machines%20%28SVM%29%20and%20XGBoost%2C%0Aas%20well%20as%20time-series%20foundation%20models%20and%20deep%20learning%20architectures%0Aspecifically%20designed%20for%20EEG%20classification%2C%20such%20as%20EEG%20Conformer%20and%20Shallow%0AConvNet.%20A%20session-based%20hold-out%20validation%20strategy%20was%20employed%20to%20ensure%0Areliable%20evaluation%20and%20prevent%20data%20leakage.%20Our%20results%20demonstrate%0Aoutstanding%20classification%20accuracy%2C%20reaching%2097.93%25.%20These%20findings%20highlight%0Athe%20potential%20of%20cueless%20EEG%20paradigms%20for%20secure%20and%20reliable%20subject%0Aidentification%20in%20real-world%20applications%2C%20such%20as%20brain-computer%20interfaces%0A%28BCIs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCueless%2520EEG%2520imagined%2520speech%2520for%2520subject%2520identification%253A%2520dataset%2520and%250A%2520%2520benchmarks%26entry.906535625%3DAli%2520Derakhshesh%2520and%2520Zahra%2520Dehghanian%2520and%2520Reza%2520Ebrahimpour%2520and%2520Hamid%2520R.%2520Rabiee%26entry.1292438233%3D%2520%2520Electroencephalogram%2520%2528EEG%2529%2520signals%2520have%2520emerged%2520as%2520a%2520promising%2520modality%2520for%250Abiometric%2520identification.%2520While%2520previous%2520studies%2520have%2520explored%2520the%2520use%2520of%250Aimagined%2520speech%2520with%2520semantically%2520meaningful%2520words%2520for%2520subject%2520identification%252C%250Amost%2520have%2520relied%2520on%2520additional%2520visual%2520or%2520auditory%2520cues.%2520In%2520this%2520study%252C%2520we%250Aintroduce%2520a%2520cueless%2520EEG-based%2520imagined%2520speech%2520paradigm%252C%2520where%2520subjects%2520imagine%250Athe%2520pronunciation%2520of%2520semantically%2520meaningful%2520words%2520without%2520any%2520external%2520cues.%250AThis%2520innovative%2520approach%2520addresses%2520the%2520limitations%2520of%2520prior%2520methods%2520by%250Arequiring%2520subjects%2520to%2520select%2520and%2520imagine%2520words%2520from%2520a%2520predefined%2520list%250Anaturally.%2520The%2520dataset%2520comprises%2520over%25204%252C350%2520trials%2520from%252011%2520subjects%2520across%2520five%250Asessions.%2520We%2520assess%2520a%2520variety%2520of%2520classification%2520methods%252C%2520including%2520traditional%250Amachine%2520learning%2520techniques%2520such%2520as%2520Support%2520Vector%2520Machines%2520%2528SVM%2529%2520and%2520XGBoost%252C%250Aas%2520well%2520as%2520time-series%2520foundation%2520models%2520and%2520deep%2520learning%2520architectures%250Aspecifically%2520designed%2520for%2520EEG%2520classification%252C%2520such%2520as%2520EEG%2520Conformer%2520and%2520Shallow%250AConvNet.%2520A%2520session-based%2520hold-out%2520validation%2520strategy%2520was%2520employed%2520to%2520ensure%250Areliable%2520evaluation%2520and%2520prevent%2520data%2520leakage.%2520Our%2520results%2520demonstrate%250Aoutstanding%2520classification%2520accuracy%252C%2520reaching%252097.93%2525.%2520These%2520findings%2520highlight%250Athe%2520potential%2520of%2520cueless%2520EEG%2520paradigms%2520for%2520secure%2520and%2520reliable%2520subject%250Aidentification%2520in%2520real-world%2520applications%252C%2520such%2520as%2520brain-computer%2520interfaces%250A%2528BCIs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cueless%20EEG%20imagined%20speech%20for%20subject%20identification%3A%20dataset%20and%0A%20%20benchmarks&entry.906535625=Ali%20Derakhshesh%20and%20Zahra%20Dehghanian%20and%20Reza%20Ebrahimpour%20and%20Hamid%20R.%20Rabiee&entry.1292438233=%20%20Electroencephalogram%20%28EEG%29%20signals%20have%20emerged%20as%20a%20promising%20modality%20for%0Abiometric%20identification.%20While%20previous%20studies%20have%20explored%20the%20use%20of%0Aimagined%20speech%20with%20semantically%20meaningful%20words%20for%20subject%20identification%2C%0Amost%20have%20relied%20on%20additional%20visual%20or%20auditory%20cues.%20In%20this%20study%2C%20we%0Aintroduce%20a%20cueless%20EEG-based%20imagined%20speech%20paradigm%2C%20where%20subjects%20imagine%0Athe%20pronunciation%20of%20semantically%20meaningful%20words%20without%20any%20external%20cues.%0AThis%20innovative%20approach%20addresses%20the%20limitations%20of%20prior%20methods%20by%0Arequiring%20subjects%20to%20select%20and%20imagine%20words%20from%20a%20predefined%20list%0Anaturally.%20The%20dataset%20comprises%20over%204%2C350%20trials%20from%2011%20subjects%20across%20five%0Asessions.%20We%20assess%20a%20variety%20of%20classification%20methods%2C%20including%20traditional%0Amachine%20learning%20techniques%20such%20as%20Support%20Vector%20Machines%20%28SVM%29%20and%20XGBoost%2C%0Aas%20well%20as%20time-series%20foundation%20models%20and%20deep%20learning%20architectures%0Aspecifically%20designed%20for%20EEG%20classification%2C%20such%20as%20EEG%20Conformer%20and%20Shallow%0AConvNet.%20A%20session-based%20hold-out%20validation%20strategy%20was%20employed%20to%20ensure%0Areliable%20evaluation%20and%20prevent%20data%20leakage.%20Our%20results%20demonstrate%0Aoutstanding%20classification%20accuracy%2C%20reaching%2097.93%25.%20These%20findings%20highlight%0Athe%20potential%20of%20cueless%20EEG%20paradigms%20for%20secure%20and%20reliable%20subject%0Aidentification%20in%20real-world%20applications%2C%20such%20as%20brain-computer%20interfaces%0A%28BCIs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09700v1&entry.124074799=Read"},
{"title": "Higher-Order Topological Directionality and Directed Simplicial Neural\n  Networks", "author": "Manuel Lecha and Andrea Cavallo and Francesca Dominici and Elvin Isufi and Claudio Battiloro", "abstract": "  Topological Deep Learning (TDL) has emerged as a paradigm to process and\nlearn from signals defined on higher-order combinatorial topological spaces,\nsuch as simplicial or cell complexes. Although many complex systems have an\nasymmetric relational structure, most TDL models forcibly symmetrize these\nrelationships. In this paper, we first introduce a novel notion of higher-order\ndirectionality and we then design Directed Simplicial Neural Networks\n(Dir-SNNs) based on it. Dir-SNNs are message-passing networks operating on\ndirected simplicial complexes able to leverage directed and possibly asymmetric\ninteractions among the simplices. To our knowledge, this is the first TDL model\nusing a notion of higher-order directionality. We theoretically and empirically\nprove that Dir-SNNs are more expressive than their directed graph counterpart\nin distinguishing isomorphic directed graphs. Experiments on a synthetic source\nlocalization task demonstrate that Dir-SNNs outperform undirected SNNs when the\nunderlying complex is directed, and perform comparably when the underlying\ncomplex is undirected.\n", "link": "http://arxiv.org/abs/2409.08389v3", "date": "2025-01-16", "relevancy": 2.4085, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4813}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-Order%20Topological%20Directionality%20and%20Directed%20Simplicial%20Neural%0A%20%20Networks&body=Title%3A%20Higher-Order%20Topological%20Directionality%20and%20Directed%20Simplicial%20Neural%0A%20%20Networks%0AAuthor%3A%20Manuel%20Lecha%20and%20Andrea%20Cavallo%20and%20Francesca%20Dominici%20and%20Elvin%20Isufi%20and%20Claudio%20Battiloro%0AAbstract%3A%20%20%20Topological%20Deep%20Learning%20%28TDL%29%20has%20emerged%20as%20a%20paradigm%20to%20process%20and%0Alearn%20from%20signals%20defined%20on%20higher-order%20combinatorial%20topological%20spaces%2C%0Asuch%20as%20simplicial%20or%20cell%20complexes.%20Although%20many%20complex%20systems%20have%20an%0Aasymmetric%20relational%20structure%2C%20most%20TDL%20models%20forcibly%20symmetrize%20these%0Arelationships.%20In%20this%20paper%2C%20we%20first%20introduce%20a%20novel%20notion%20of%20higher-order%0Adirectionality%20and%20we%20then%20design%20Directed%20Simplicial%20Neural%20Networks%0A%28Dir-SNNs%29%20based%20on%20it.%20Dir-SNNs%20are%20message-passing%20networks%20operating%20on%0Adirected%20simplicial%20complexes%20able%20to%20leverage%20directed%20and%20possibly%20asymmetric%0Ainteractions%20among%20the%20simplices.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20TDL%20model%0Ausing%20a%20notion%20of%20higher-order%20directionality.%20We%20theoretically%20and%20empirically%0Aprove%20that%20Dir-SNNs%20are%20more%20expressive%20than%20their%20directed%20graph%20counterpart%0Ain%20distinguishing%20isomorphic%20directed%20graphs.%20Experiments%20on%20a%20synthetic%20source%0Alocalization%20task%20demonstrate%20that%20Dir-SNNs%20outperform%20undirected%20SNNs%20when%20the%0Aunderlying%20complex%20is%20directed%2C%20and%20perform%20comparably%20when%20the%20underlying%0Acomplex%20is%20undirected.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08389v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-Order%2520Topological%2520Directionality%2520and%2520Directed%2520Simplicial%2520Neural%250A%2520%2520Networks%26entry.906535625%3DManuel%2520Lecha%2520and%2520Andrea%2520Cavallo%2520and%2520Francesca%2520Dominici%2520and%2520Elvin%2520Isufi%2520and%2520Claudio%2520Battiloro%26entry.1292438233%3D%2520%2520Topological%2520Deep%2520Learning%2520%2528TDL%2529%2520has%2520emerged%2520as%2520a%2520paradigm%2520to%2520process%2520and%250Alearn%2520from%2520signals%2520defined%2520on%2520higher-order%2520combinatorial%2520topological%2520spaces%252C%250Asuch%2520as%2520simplicial%2520or%2520cell%2520complexes.%2520Although%2520many%2520complex%2520systems%2520have%2520an%250Aasymmetric%2520relational%2520structure%252C%2520most%2520TDL%2520models%2520forcibly%2520symmetrize%2520these%250Arelationships.%2520In%2520this%2520paper%252C%2520we%2520first%2520introduce%2520a%2520novel%2520notion%2520of%2520higher-order%250Adirectionality%2520and%2520we%2520then%2520design%2520Directed%2520Simplicial%2520Neural%2520Networks%250A%2528Dir-SNNs%2529%2520based%2520on%2520it.%2520Dir-SNNs%2520are%2520message-passing%2520networks%2520operating%2520on%250Adirected%2520simplicial%2520complexes%2520able%2520to%2520leverage%2520directed%2520and%2520possibly%2520asymmetric%250Ainteractions%2520among%2520the%2520simplices.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520TDL%2520model%250Ausing%2520a%2520notion%2520of%2520higher-order%2520directionality.%2520We%2520theoretically%2520and%2520empirically%250Aprove%2520that%2520Dir-SNNs%2520are%2520more%2520expressive%2520than%2520their%2520directed%2520graph%2520counterpart%250Ain%2520distinguishing%2520isomorphic%2520directed%2520graphs.%2520Experiments%2520on%2520a%2520synthetic%2520source%250Alocalization%2520task%2520demonstrate%2520that%2520Dir-SNNs%2520outperform%2520undirected%2520SNNs%2520when%2520the%250Aunderlying%2520complex%2520is%2520directed%252C%2520and%2520perform%2520comparably%2520when%2520the%2520underlying%250Acomplex%2520is%2520undirected.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08389v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-Order%20Topological%20Directionality%20and%20Directed%20Simplicial%20Neural%0A%20%20Networks&entry.906535625=Manuel%20Lecha%20and%20Andrea%20Cavallo%20and%20Francesca%20Dominici%20and%20Elvin%20Isufi%20and%20Claudio%20Battiloro&entry.1292438233=%20%20Topological%20Deep%20Learning%20%28TDL%29%20has%20emerged%20as%20a%20paradigm%20to%20process%20and%0Alearn%20from%20signals%20defined%20on%20higher-order%20combinatorial%20topological%20spaces%2C%0Asuch%20as%20simplicial%20or%20cell%20complexes.%20Although%20many%20complex%20systems%20have%20an%0Aasymmetric%20relational%20structure%2C%20most%20TDL%20models%20forcibly%20symmetrize%20these%0Arelationships.%20In%20this%20paper%2C%20we%20first%20introduce%20a%20novel%20notion%20of%20higher-order%0Adirectionality%20and%20we%20then%20design%20Directed%20Simplicial%20Neural%20Networks%0A%28Dir-SNNs%29%20based%20on%20it.%20Dir-SNNs%20are%20message-passing%20networks%20operating%20on%0Adirected%20simplicial%20complexes%20able%20to%20leverage%20directed%20and%20possibly%20asymmetric%0Ainteractions%20among%20the%20simplices.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20TDL%20model%0Ausing%20a%20notion%20of%20higher-order%20directionality.%20We%20theoretically%20and%20empirically%0Aprove%20that%20Dir-SNNs%20are%20more%20expressive%20than%20their%20directed%20graph%20counterpart%0Ain%20distinguishing%20isomorphic%20directed%20graphs.%20Experiments%20on%20a%20synthetic%20source%0Alocalization%20task%20demonstrate%20that%20Dir-SNNs%20outperform%20undirected%20SNNs%20when%20the%0Aunderlying%20complex%20is%20directed%2C%20and%20perform%20comparably%20when%20the%20underlying%0Acomplex%20is%20undirected.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08389v3&entry.124074799=Read"},
{"title": "Global SLAM in Visual-Inertial Systems with 5G Time-of-Arrival\n  Integration", "author": "Meisam Kabiri and Holger Voos", "abstract": "  This paper presents a novel approach that integrates 5G Time of Arrival (ToA)\nmeasurements into ORB-SLAM3 to enable global localization and enhance mapping\ncapabilities for indoor drone navigation. We extend ORB-SLAM3's optimization\npipeline to jointly process ToA data from 5G base stations alongside visual and\ninertial measurements while estimating system biases. This integration\ntransforms the inherently local SLAM estimates into globally referenced\ntrajectories and effectively resolves scale ambiguity in monocular\nconfigurations. Our method is evaluated using five real-world indoor datasets\ncollected with RGB-D cameras and inertial measurement units (IMUs),\ncomplemented by simulated 5G ToA measurements at 28 GHz and 78 GHz frequencies\nusing MATLAB and QuaDRiGa. Extensive experiments across four SLAM\nconfigurations (RGB-D, RGB-D-Inertial, Monocular, and Monocular-Inertial)\ndemonstrate that ToA integration enables consistent global positioning across\nall modes while significantly improving local accuracy in minimal sensor\nsetups. Notably, ToA-enhanced monocular SLAM achieves superior local accuracy\n(6.3 cm average) compared to the RGB-D baseline (11.5 cm), and enables reliable\noperation of monocular-inertial SLAM in scenarios where the baseline system\nfails completely. While ToA integration offers limited local accuracy\nimprovements for sensor-rich configurations like RGB-D SLAM, it consistently\nenables robust global localization.\n", "link": "http://arxiv.org/abs/2412.12406v3", "date": "2025-01-16", "relevancy": 2.3745, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6276}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20SLAM%20in%20Visual-Inertial%20Systems%20with%205G%20Time-of-Arrival%0A%20%20Integration&body=Title%3A%20Global%20SLAM%20in%20Visual-Inertial%20Systems%20with%205G%20Time-of-Arrival%0A%20%20Integration%0AAuthor%3A%20Meisam%20Kabiri%20and%20Holger%20Voos%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20that%20integrates%205G%20Time%20of%20Arrival%20%28ToA%29%0Ameasurements%20into%20ORB-SLAM3%20to%20enable%20global%20localization%20and%20enhance%20mapping%0Acapabilities%20for%20indoor%20drone%20navigation.%20We%20extend%20ORB-SLAM3%27s%20optimization%0Apipeline%20to%20jointly%20process%20ToA%20data%20from%205G%20base%20stations%20alongside%20visual%20and%0Ainertial%20measurements%20while%20estimating%20system%20biases.%20This%20integration%0Atransforms%20the%20inherently%20local%20SLAM%20estimates%20into%20globally%20referenced%0Atrajectories%20and%20effectively%20resolves%20scale%20ambiguity%20in%20monocular%0Aconfigurations.%20Our%20method%20is%20evaluated%20using%20five%20real-world%20indoor%20datasets%0Acollected%20with%20RGB-D%20cameras%20and%20inertial%20measurement%20units%20%28IMUs%29%2C%0Acomplemented%20by%20simulated%205G%20ToA%20measurements%20at%2028%20GHz%20and%2078%20GHz%20frequencies%0Ausing%20MATLAB%20and%20QuaDRiGa.%20Extensive%20experiments%20across%20four%20SLAM%0Aconfigurations%20%28RGB-D%2C%20RGB-D-Inertial%2C%20Monocular%2C%20and%20Monocular-Inertial%29%0Ademonstrate%20that%20ToA%20integration%20enables%20consistent%20global%20positioning%20across%0Aall%20modes%20while%20significantly%20improving%20local%20accuracy%20in%20minimal%20sensor%0Asetups.%20Notably%2C%20ToA-enhanced%20monocular%20SLAM%20achieves%20superior%20local%20accuracy%0A%286.3%20cm%20average%29%20compared%20to%20the%20RGB-D%20baseline%20%2811.5%20cm%29%2C%20and%20enables%20reliable%0Aoperation%20of%20monocular-inertial%20SLAM%20in%20scenarios%20where%20the%20baseline%20system%0Afails%20completely.%20While%20ToA%20integration%20offers%20limited%20local%20accuracy%0Aimprovements%20for%20sensor-rich%20configurations%20like%20RGB-D%20SLAM%2C%20it%20consistently%0Aenables%20robust%20global%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12406v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520SLAM%2520in%2520Visual-Inertial%2520Systems%2520with%25205G%2520Time-of-Arrival%250A%2520%2520Integration%26entry.906535625%3DMeisam%2520Kabiri%2520and%2520Holger%2520Voos%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520that%2520integrates%25205G%2520Time%2520of%2520Arrival%2520%2528ToA%2529%250Ameasurements%2520into%2520ORB-SLAM3%2520to%2520enable%2520global%2520localization%2520and%2520enhance%2520mapping%250Acapabilities%2520for%2520indoor%2520drone%2520navigation.%2520We%2520extend%2520ORB-SLAM3%2527s%2520optimization%250Apipeline%2520to%2520jointly%2520process%2520ToA%2520data%2520from%25205G%2520base%2520stations%2520alongside%2520visual%2520and%250Ainertial%2520measurements%2520while%2520estimating%2520system%2520biases.%2520This%2520integration%250Atransforms%2520the%2520inherently%2520local%2520SLAM%2520estimates%2520into%2520globally%2520referenced%250Atrajectories%2520and%2520effectively%2520resolves%2520scale%2520ambiguity%2520in%2520monocular%250Aconfigurations.%2520Our%2520method%2520is%2520evaluated%2520using%2520five%2520real-world%2520indoor%2520datasets%250Acollected%2520with%2520RGB-D%2520cameras%2520and%2520inertial%2520measurement%2520units%2520%2528IMUs%2529%252C%250Acomplemented%2520by%2520simulated%25205G%2520ToA%2520measurements%2520at%252028%2520GHz%2520and%252078%2520GHz%2520frequencies%250Ausing%2520MATLAB%2520and%2520QuaDRiGa.%2520Extensive%2520experiments%2520across%2520four%2520SLAM%250Aconfigurations%2520%2528RGB-D%252C%2520RGB-D-Inertial%252C%2520Monocular%252C%2520and%2520Monocular-Inertial%2529%250Ademonstrate%2520that%2520ToA%2520integration%2520enables%2520consistent%2520global%2520positioning%2520across%250Aall%2520modes%2520while%2520significantly%2520improving%2520local%2520accuracy%2520in%2520minimal%2520sensor%250Asetups.%2520Notably%252C%2520ToA-enhanced%2520monocular%2520SLAM%2520achieves%2520superior%2520local%2520accuracy%250A%25286.3%2520cm%2520average%2529%2520compared%2520to%2520the%2520RGB-D%2520baseline%2520%252811.5%2520cm%2529%252C%2520and%2520enables%2520reliable%250Aoperation%2520of%2520monocular-inertial%2520SLAM%2520in%2520scenarios%2520where%2520the%2520baseline%2520system%250Afails%2520completely.%2520While%2520ToA%2520integration%2520offers%2520limited%2520local%2520accuracy%250Aimprovements%2520for%2520sensor-rich%2520configurations%2520like%2520RGB-D%2520SLAM%252C%2520it%2520consistently%250Aenables%2520robust%2520global%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12406v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20SLAM%20in%20Visual-Inertial%20Systems%20with%205G%20Time-of-Arrival%0A%20%20Integration&entry.906535625=Meisam%20Kabiri%20and%20Holger%20Voos&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20that%20integrates%205G%20Time%20of%20Arrival%20%28ToA%29%0Ameasurements%20into%20ORB-SLAM3%20to%20enable%20global%20localization%20and%20enhance%20mapping%0Acapabilities%20for%20indoor%20drone%20navigation.%20We%20extend%20ORB-SLAM3%27s%20optimization%0Apipeline%20to%20jointly%20process%20ToA%20data%20from%205G%20base%20stations%20alongside%20visual%20and%0Ainertial%20measurements%20while%20estimating%20system%20biases.%20This%20integration%0Atransforms%20the%20inherently%20local%20SLAM%20estimates%20into%20globally%20referenced%0Atrajectories%20and%20effectively%20resolves%20scale%20ambiguity%20in%20monocular%0Aconfigurations.%20Our%20method%20is%20evaluated%20using%20five%20real-world%20indoor%20datasets%0Acollected%20with%20RGB-D%20cameras%20and%20inertial%20measurement%20units%20%28IMUs%29%2C%0Acomplemented%20by%20simulated%205G%20ToA%20measurements%20at%2028%20GHz%20and%2078%20GHz%20frequencies%0Ausing%20MATLAB%20and%20QuaDRiGa.%20Extensive%20experiments%20across%20four%20SLAM%0Aconfigurations%20%28RGB-D%2C%20RGB-D-Inertial%2C%20Monocular%2C%20and%20Monocular-Inertial%29%0Ademonstrate%20that%20ToA%20integration%20enables%20consistent%20global%20positioning%20across%0Aall%20modes%20while%20significantly%20improving%20local%20accuracy%20in%20minimal%20sensor%0Asetups.%20Notably%2C%20ToA-enhanced%20monocular%20SLAM%20achieves%20superior%20local%20accuracy%0A%286.3%20cm%20average%29%20compared%20to%20the%20RGB-D%20baseline%20%2811.5%20cm%29%2C%20and%20enables%20reliable%0Aoperation%20of%20monocular-inertial%20SLAM%20in%20scenarios%20where%20the%20baseline%20system%0Afails%20completely.%20While%20ToA%20integration%20offers%20limited%20local%20accuracy%0Aimprovements%20for%20sensor-rich%20configurations%20like%20RGB-D%20SLAM%2C%20it%20consistently%0Aenables%20robust%20global%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12406v3&entry.124074799=Read"},
{"title": "CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation", "author": "Hwan Heo and Jangyeong Kim and Seongyeong Lee and Jeong A Wi and Junyoung Choi and Sangjun Ahn", "abstract": "  The synthesis of high-quality 3D assets from textual or visual inputs has\nbecome a central objective in modern generative modeling. Despite the\nproliferation of 3D generation algorithms, they frequently grapple with\nchallenges such as multi-view inconsistency, slow generation times, low\nfidelity, and surface reconstruction problems. While some studies have\naddressed some of these issues, a comprehensive solution remains elusive. In\nthis paper, we introduce \\textbf{CaPa}, a carve-and-paint framework that\ngenerates high-fidelity 3D assets efficiently. CaPa employs a two-stage\nprocess, decoupling geometry generation from texture synthesis. Initially, a 3D\nlatent diffusion model generates geometry guided by multi-view inputs, ensuring\nstructural consistency across perspectives. Subsequently, leveraging a novel,\nmodel-agnostic Spatially Decoupled Attention, the framework synthesizes\nhigh-resolution textures (up to 4K) for a given geometry. Furthermore, we\npropose a 3D-aware occlusion inpainting algorithm that fills untextured\nregions, resulting in cohesive results across the entire model. This pipeline\ngenerates high-quality 3D assets in less than 30 seconds, providing\nready-to-use outputs for commercial applications. Experimental results\ndemonstrate that CaPa excels in both texture fidelity and geometric stability,\nestablishing a new standard for practical, scalable 3D asset generation.\n", "link": "http://arxiv.org/abs/2501.09433v1", "date": "2025-01-16", "relevancy": 2.3694, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5968}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5968}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaPa%3A%20Carve-n-Paint%20Synthesis%20for%20Efficient%204K%20Textured%20Mesh%20Generation&body=Title%3A%20CaPa%3A%20Carve-n-Paint%20Synthesis%20for%20Efficient%204K%20Textured%20Mesh%20Generation%0AAuthor%3A%20Hwan%20Heo%20and%20Jangyeong%20Kim%20and%20Seongyeong%20Lee%20and%20Jeong%20A%20Wi%20and%20Junyoung%20Choi%20and%20Sangjun%20Ahn%0AAbstract%3A%20%20%20The%20synthesis%20of%20high-quality%203D%20assets%20from%20textual%20or%20visual%20inputs%20has%0Abecome%20a%20central%20objective%20in%20modern%20generative%20modeling.%20Despite%20the%0Aproliferation%20of%203D%20generation%20algorithms%2C%20they%20frequently%20grapple%20with%0Achallenges%20such%20as%20multi-view%20inconsistency%2C%20slow%20generation%20times%2C%20low%0Afidelity%2C%20and%20surface%20reconstruction%20problems.%20While%20some%20studies%20have%0Aaddressed%20some%20of%20these%20issues%2C%20a%20comprehensive%20solution%20remains%20elusive.%20In%0Athis%20paper%2C%20we%20introduce%20%5Ctextbf%7BCaPa%7D%2C%20a%20carve-and-paint%20framework%20that%0Agenerates%20high-fidelity%203D%20assets%20efficiently.%20CaPa%20employs%20a%20two-stage%0Aprocess%2C%20decoupling%20geometry%20generation%20from%20texture%20synthesis.%20Initially%2C%20a%203D%0Alatent%20diffusion%20model%20generates%20geometry%20guided%20by%20multi-view%20inputs%2C%20ensuring%0Astructural%20consistency%20across%20perspectives.%20Subsequently%2C%20leveraging%20a%20novel%2C%0Amodel-agnostic%20Spatially%20Decoupled%20Attention%2C%20the%20framework%20synthesizes%0Ahigh-resolution%20textures%20%28up%20to%204K%29%20for%20a%20given%20geometry.%20Furthermore%2C%20we%0Apropose%20a%203D-aware%20occlusion%20inpainting%20algorithm%20that%20fills%20untextured%0Aregions%2C%20resulting%20in%20cohesive%20results%20across%20the%20entire%20model.%20This%20pipeline%0Agenerates%20high-quality%203D%20assets%20in%20less%20than%2030%20seconds%2C%20providing%0Aready-to-use%20outputs%20for%20commercial%20applications.%20Experimental%20results%0Ademonstrate%20that%20CaPa%20excels%20in%20both%20texture%20fidelity%20and%20geometric%20stability%2C%0Aestablishing%20a%20new%20standard%20for%20practical%2C%20scalable%203D%20asset%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaPa%253A%2520Carve-n-Paint%2520Synthesis%2520for%2520Efficient%25204K%2520Textured%2520Mesh%2520Generation%26entry.906535625%3DHwan%2520Heo%2520and%2520Jangyeong%2520Kim%2520and%2520Seongyeong%2520Lee%2520and%2520Jeong%2520A%2520Wi%2520and%2520Junyoung%2520Choi%2520and%2520Sangjun%2520Ahn%26entry.1292438233%3D%2520%2520The%2520synthesis%2520of%2520high-quality%25203D%2520assets%2520from%2520textual%2520or%2520visual%2520inputs%2520has%250Abecome%2520a%2520central%2520objective%2520in%2520modern%2520generative%2520modeling.%2520Despite%2520the%250Aproliferation%2520of%25203D%2520generation%2520algorithms%252C%2520they%2520frequently%2520grapple%2520with%250Achallenges%2520such%2520as%2520multi-view%2520inconsistency%252C%2520slow%2520generation%2520times%252C%2520low%250Afidelity%252C%2520and%2520surface%2520reconstruction%2520problems.%2520While%2520some%2520studies%2520have%250Aaddressed%2520some%2520of%2520these%2520issues%252C%2520a%2520comprehensive%2520solution%2520remains%2520elusive.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520%255Ctextbf%257BCaPa%257D%252C%2520a%2520carve-and-paint%2520framework%2520that%250Agenerates%2520high-fidelity%25203D%2520assets%2520efficiently.%2520CaPa%2520employs%2520a%2520two-stage%250Aprocess%252C%2520decoupling%2520geometry%2520generation%2520from%2520texture%2520synthesis.%2520Initially%252C%2520a%25203D%250Alatent%2520diffusion%2520model%2520generates%2520geometry%2520guided%2520by%2520multi-view%2520inputs%252C%2520ensuring%250Astructural%2520consistency%2520across%2520perspectives.%2520Subsequently%252C%2520leveraging%2520a%2520novel%252C%250Amodel-agnostic%2520Spatially%2520Decoupled%2520Attention%252C%2520the%2520framework%2520synthesizes%250Ahigh-resolution%2520textures%2520%2528up%2520to%25204K%2529%2520for%2520a%2520given%2520geometry.%2520Furthermore%252C%2520we%250Apropose%2520a%25203D-aware%2520occlusion%2520inpainting%2520algorithm%2520that%2520fills%2520untextured%250Aregions%252C%2520resulting%2520in%2520cohesive%2520results%2520across%2520the%2520entire%2520model.%2520This%2520pipeline%250Agenerates%2520high-quality%25203D%2520assets%2520in%2520less%2520than%252030%2520seconds%252C%2520providing%250Aready-to-use%2520outputs%2520for%2520commercial%2520applications.%2520Experimental%2520results%250Ademonstrate%2520that%2520CaPa%2520excels%2520in%2520both%2520texture%2520fidelity%2520and%2520geometric%2520stability%252C%250Aestablishing%2520a%2520new%2520standard%2520for%2520practical%252C%2520scalable%25203D%2520asset%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaPa%3A%20Carve-n-Paint%20Synthesis%20for%20Efficient%204K%20Textured%20Mesh%20Generation&entry.906535625=Hwan%20Heo%20and%20Jangyeong%20Kim%20and%20Seongyeong%20Lee%20and%20Jeong%20A%20Wi%20and%20Junyoung%20Choi%20and%20Sangjun%20Ahn&entry.1292438233=%20%20The%20synthesis%20of%20high-quality%203D%20assets%20from%20textual%20or%20visual%20inputs%20has%0Abecome%20a%20central%20objective%20in%20modern%20generative%20modeling.%20Despite%20the%0Aproliferation%20of%203D%20generation%20algorithms%2C%20they%20frequently%20grapple%20with%0Achallenges%20such%20as%20multi-view%20inconsistency%2C%20slow%20generation%20times%2C%20low%0Afidelity%2C%20and%20surface%20reconstruction%20problems.%20While%20some%20studies%20have%0Aaddressed%20some%20of%20these%20issues%2C%20a%20comprehensive%20solution%20remains%20elusive.%20In%0Athis%20paper%2C%20we%20introduce%20%5Ctextbf%7BCaPa%7D%2C%20a%20carve-and-paint%20framework%20that%0Agenerates%20high-fidelity%203D%20assets%20efficiently.%20CaPa%20employs%20a%20two-stage%0Aprocess%2C%20decoupling%20geometry%20generation%20from%20texture%20synthesis.%20Initially%2C%20a%203D%0Alatent%20diffusion%20model%20generates%20geometry%20guided%20by%20multi-view%20inputs%2C%20ensuring%0Astructural%20consistency%20across%20perspectives.%20Subsequently%2C%20leveraging%20a%20novel%2C%0Amodel-agnostic%20Spatially%20Decoupled%20Attention%2C%20the%20framework%20synthesizes%0Ahigh-resolution%20textures%20%28up%20to%204K%29%20for%20a%20given%20geometry.%20Furthermore%2C%20we%0Apropose%20a%203D-aware%20occlusion%20inpainting%20algorithm%20that%20fills%20untextured%0Aregions%2C%20resulting%20in%20cohesive%20results%20across%20the%20entire%20model.%20This%20pipeline%0Agenerates%20high-quality%203D%20assets%20in%20less%20than%2030%20seconds%2C%20providing%0Aready-to-use%20outputs%20for%20commercial%20applications.%20Experimental%20results%0Ademonstrate%20that%20CaPa%20excels%20in%20both%20texture%20fidelity%20and%20geometric%20stability%2C%0Aestablishing%20a%20new%20standard%20for%20practical%2C%20scalable%203D%20asset%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09433v1&entry.124074799=Read"},
{"title": "WMamba: Wavelet-based Mamba for Face Forgery Detection", "author": "Siran Peng and Tianshuo Zhang and Li Gao and Xiangyu Zhu and Haoyuan Zhang and Kai Pang and Zhen Lei", "abstract": "  With the rapid advancement of deepfake generation technologies, the demand\nfor robust and accurate face forgery detection algorithms has become\nincreasingly critical. Recent studies have demonstrated that wavelet analysis\ncan uncover subtle forgery artifacts that remain imperceptible in the spatial\ndomain. Wavelets effectively capture important facial contours, which are often\nslender, fine-grained, and global in nature. However, existing wavelet-based\napproaches fail to fully leverage these unique characteristics, resulting in\nsub-optimal feature extraction and limited generalizability. To address this\nchallenge, we introduce WMamba, a novel wavelet-based feature extractor built\nupon the Mamba architecture. WMamba maximizes the utility of wavelet\ninformation through two key innovations. First, we propose Dynamic Contour\nConvolution (DCConv), which employs specially crafted deformable kernels to\nadaptively model slender facial contours. Second, by leveraging the Mamba\narchitecture, our method captures long-range spatial relationships with linear\ncomputational complexity. This efficiency allows for the extraction of\nfine-grained, global forgery artifacts from small image patches. Extensive\nexperimental results show that WMamba achieves state-of-the-art (SOTA)\nperformance, highlighting its effectiveness and superiority in face forgery\ndetection.\n", "link": "http://arxiv.org/abs/2501.09617v1", "date": "2025-01-16", "relevancy": 2.3575, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4837}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4735}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WMamba%3A%20Wavelet-based%20Mamba%20for%20Face%20Forgery%20Detection&body=Title%3A%20WMamba%3A%20Wavelet-based%20Mamba%20for%20Face%20Forgery%20Detection%0AAuthor%3A%20Siran%20Peng%20and%20Tianshuo%20Zhang%20and%20Li%20Gao%20and%20Xiangyu%20Zhu%20and%20Haoyuan%20Zhang%20and%20Kai%20Pang%20and%20Zhen%20Lei%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20deepfake%20generation%20technologies%2C%20the%20demand%0Afor%20robust%20and%20accurate%20face%20forgery%20detection%20algorithms%20has%20become%0Aincreasingly%20critical.%20Recent%20studies%20have%20demonstrated%20that%20wavelet%20analysis%0Acan%20uncover%20subtle%20forgery%20artifacts%20that%20remain%20imperceptible%20in%20the%20spatial%0Adomain.%20Wavelets%20effectively%20capture%20important%20facial%20contours%2C%20which%20are%20often%0Aslender%2C%20fine-grained%2C%20and%20global%20in%20nature.%20However%2C%20existing%20wavelet-based%0Aapproaches%20fail%20to%20fully%20leverage%20these%20unique%20characteristics%2C%20resulting%20in%0Asub-optimal%20feature%20extraction%20and%20limited%20generalizability.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20WMamba%2C%20a%20novel%20wavelet-based%20feature%20extractor%20built%0Aupon%20the%20Mamba%20architecture.%20WMamba%20maximizes%20the%20utility%20of%20wavelet%0Ainformation%20through%20two%20key%20innovations.%20First%2C%20we%20propose%20Dynamic%20Contour%0AConvolution%20%28DCConv%29%2C%20which%20employs%20specially%20crafted%20deformable%20kernels%20to%0Aadaptively%20model%20slender%20facial%20contours.%20Second%2C%20by%20leveraging%20the%20Mamba%0Aarchitecture%2C%20our%20method%20captures%20long-range%20spatial%20relationships%20with%20linear%0Acomputational%20complexity.%20This%20efficiency%20allows%20for%20the%20extraction%20of%0Afine-grained%2C%20global%20forgery%20artifacts%20from%20small%20image%20patches.%20Extensive%0Aexperimental%20results%20show%20that%20WMamba%20achieves%20state-of-the-art%20%28SOTA%29%0Aperformance%2C%20highlighting%20its%20effectiveness%20and%20superiority%20in%20face%20forgery%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWMamba%253A%2520Wavelet-based%2520Mamba%2520for%2520Face%2520Forgery%2520Detection%26entry.906535625%3DSiran%2520Peng%2520and%2520Tianshuo%2520Zhang%2520and%2520Li%2520Gao%2520and%2520Xiangyu%2520Zhu%2520and%2520Haoyuan%2520Zhang%2520and%2520Kai%2520Pang%2520and%2520Zhen%2520Lei%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520deepfake%2520generation%2520technologies%252C%2520the%2520demand%250Afor%2520robust%2520and%2520accurate%2520face%2520forgery%2520detection%2520algorithms%2520has%2520become%250Aincreasingly%2520critical.%2520Recent%2520studies%2520have%2520demonstrated%2520that%2520wavelet%2520analysis%250Acan%2520uncover%2520subtle%2520forgery%2520artifacts%2520that%2520remain%2520imperceptible%2520in%2520the%2520spatial%250Adomain.%2520Wavelets%2520effectively%2520capture%2520important%2520facial%2520contours%252C%2520which%2520are%2520often%250Aslender%252C%2520fine-grained%252C%2520and%2520global%2520in%2520nature.%2520However%252C%2520existing%2520wavelet-based%250Aapproaches%2520fail%2520to%2520fully%2520leverage%2520these%2520unique%2520characteristics%252C%2520resulting%2520in%250Asub-optimal%2520feature%2520extraction%2520and%2520limited%2520generalizability.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520introduce%2520WMamba%252C%2520a%2520novel%2520wavelet-based%2520feature%2520extractor%2520built%250Aupon%2520the%2520Mamba%2520architecture.%2520WMamba%2520maximizes%2520the%2520utility%2520of%2520wavelet%250Ainformation%2520through%2520two%2520key%2520innovations.%2520First%252C%2520we%2520propose%2520Dynamic%2520Contour%250AConvolution%2520%2528DCConv%2529%252C%2520which%2520employs%2520specially%2520crafted%2520deformable%2520kernels%2520to%250Aadaptively%2520model%2520slender%2520facial%2520contours.%2520Second%252C%2520by%2520leveraging%2520the%2520Mamba%250Aarchitecture%252C%2520our%2520method%2520captures%2520long-range%2520spatial%2520relationships%2520with%2520linear%250Acomputational%2520complexity.%2520This%2520efficiency%2520allows%2520for%2520the%2520extraction%2520of%250Afine-grained%252C%2520global%2520forgery%2520artifacts%2520from%2520small%2520image%2520patches.%2520Extensive%250Aexperimental%2520results%2520show%2520that%2520WMamba%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%250Aperformance%252C%2520highlighting%2520its%2520effectiveness%2520and%2520superiority%2520in%2520face%2520forgery%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WMamba%3A%20Wavelet-based%20Mamba%20for%20Face%20Forgery%20Detection&entry.906535625=Siran%20Peng%20and%20Tianshuo%20Zhang%20and%20Li%20Gao%20and%20Xiangyu%20Zhu%20and%20Haoyuan%20Zhang%20and%20Kai%20Pang%20and%20Zhen%20Lei&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20deepfake%20generation%20technologies%2C%20the%20demand%0Afor%20robust%20and%20accurate%20face%20forgery%20detection%20algorithms%20has%20become%0Aincreasingly%20critical.%20Recent%20studies%20have%20demonstrated%20that%20wavelet%20analysis%0Acan%20uncover%20subtle%20forgery%20artifacts%20that%20remain%20imperceptible%20in%20the%20spatial%0Adomain.%20Wavelets%20effectively%20capture%20important%20facial%20contours%2C%20which%20are%20often%0Aslender%2C%20fine-grained%2C%20and%20global%20in%20nature.%20However%2C%20existing%20wavelet-based%0Aapproaches%20fail%20to%20fully%20leverage%20these%20unique%20characteristics%2C%20resulting%20in%0Asub-optimal%20feature%20extraction%20and%20limited%20generalizability.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20WMamba%2C%20a%20novel%20wavelet-based%20feature%20extractor%20built%0Aupon%20the%20Mamba%20architecture.%20WMamba%20maximizes%20the%20utility%20of%20wavelet%0Ainformation%20through%20two%20key%20innovations.%20First%2C%20we%20propose%20Dynamic%20Contour%0AConvolution%20%28DCConv%29%2C%20which%20employs%20specially%20crafted%20deformable%20kernels%20to%0Aadaptively%20model%20slender%20facial%20contours.%20Second%2C%20by%20leveraging%20the%20Mamba%0Aarchitecture%2C%20our%20method%20captures%20long-range%20spatial%20relationships%20with%20linear%0Acomputational%20complexity.%20This%20efficiency%20allows%20for%20the%20extraction%20of%0Afine-grained%2C%20global%20forgery%20artifacts%20from%20small%20image%20patches.%20Extensive%0Aexperimental%20results%20show%20that%20WMamba%20achieves%20state-of-the-art%20%28SOTA%29%0Aperformance%2C%20highlighting%20its%20effectiveness%20and%20superiority%20in%20face%20forgery%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09617v1&entry.124074799=Read"},
{"title": "Learnings from Scaling Visual Tokenizers for Reconstruction and\n  Generation", "author": "Philippe Hansen-Estruch and David Yan and Ching-Yao Chung and Orr Zohar and Jialiang Wang and Tingbo Hou and Tao Xu and Sriram Vishwanath and Peter Vajda and Xinlei Chen", "abstract": "  Visual tokenization via auto-encoding empowers state-of-the-art image and\nvideo generative models by compressing pixels into a latent space. Although\nscaling Transformer-based generators has been central to recent advances, the\ntokenizer component itself is rarely scaled, leaving open questions about how\nauto-encoder design choices influence both its objective of reconstruction and\ndownstream generative performance. Our work aims to conduct an exploration of\nscaling in auto-encoders to fill in this blank. To facilitate this exploration,\nwe replace the typical convolutional backbone with an enhanced Vision\nTransformer architecture for Tokenization (ViTok). We train ViTok on\nlarge-scale image and video datasets far exceeding ImageNet-1K, removing data\nconstraints on tokenizer scaling. We first study how scaling the auto-encoder\nbottleneck affects both reconstruction and generation -- and find that while it\nis highly correlated with reconstruction, its relationship with generation is\nmore complex. We next explored the effect of separately scaling the\nauto-encoders' encoder and decoder on reconstruction and generation\nperformance. Crucially, we find that scaling the encoder yields minimal gains\nfor either reconstruction or generation, while scaling the decoder boosts\nreconstruction but the benefits for generation are mixed. Building on our\nexploration, we design ViTok as a lightweight auto-encoder that achieves\ncompetitive performance with state-of-the-art auto-encoders on ImageNet-1K and\nCOCO reconstruction tasks (256p and 512p) while outperforming existing\nauto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x\nfewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates\ncompetitive performance on image generation for ImageNet-1K and sets new\nstate-of-the-art benchmarks for class-conditional video generation on UCF-101.\n", "link": "http://arxiv.org/abs/2501.09755v1", "date": "2025-01-16", "relevancy": 2.348, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6305}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5945}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learnings%20from%20Scaling%20Visual%20Tokenizers%20for%20Reconstruction%20and%0A%20%20Generation&body=Title%3A%20Learnings%20from%20Scaling%20Visual%20Tokenizers%20for%20Reconstruction%20and%0A%20%20Generation%0AAuthor%3A%20Philippe%20Hansen-Estruch%20and%20David%20Yan%20and%20Ching-Yao%20Chung%20and%20Orr%20Zohar%20and%20Jialiang%20Wang%20and%20Tingbo%20Hou%20and%20Tao%20Xu%20and%20Sriram%20Vishwanath%20and%20Peter%20Vajda%20and%20Xinlei%20Chen%0AAbstract%3A%20%20%20Visual%20tokenization%20via%20auto-encoding%20empowers%20state-of-the-art%20image%20and%0Avideo%20generative%20models%20by%20compressing%20pixels%20into%20a%20latent%20space.%20Although%0Ascaling%20Transformer-based%20generators%20has%20been%20central%20to%20recent%20advances%2C%20the%0Atokenizer%20component%20itself%20is%20rarely%20scaled%2C%20leaving%20open%20questions%20about%20how%0Aauto-encoder%20design%20choices%20influence%20both%20its%20objective%20of%20reconstruction%20and%0Adownstream%20generative%20performance.%20Our%20work%20aims%20to%20conduct%20an%20exploration%20of%0Ascaling%20in%20auto-encoders%20to%20fill%20in%20this%20blank.%20To%20facilitate%20this%20exploration%2C%0Awe%20replace%20the%20typical%20convolutional%20backbone%20with%20an%20enhanced%20Vision%0ATransformer%20architecture%20for%20Tokenization%20%28ViTok%29.%20We%20train%20ViTok%20on%0Alarge-scale%20image%20and%20video%20datasets%20far%20exceeding%20ImageNet-1K%2C%20removing%20data%0Aconstraints%20on%20tokenizer%20scaling.%20We%20first%20study%20how%20scaling%20the%20auto-encoder%0Abottleneck%20affects%20both%20reconstruction%20and%20generation%20--%20and%20find%20that%20while%20it%0Ais%20highly%20correlated%20with%20reconstruction%2C%20its%20relationship%20with%20generation%20is%0Amore%20complex.%20We%20next%20explored%20the%20effect%20of%20separately%20scaling%20the%0Aauto-encoders%27%20encoder%20and%20decoder%20on%20reconstruction%20and%20generation%0Aperformance.%20Crucially%2C%20we%20find%20that%20scaling%20the%20encoder%20yields%20minimal%20gains%0Afor%20either%20reconstruction%20or%20generation%2C%20while%20scaling%20the%20decoder%20boosts%0Areconstruction%20but%20the%20benefits%20for%20generation%20are%20mixed.%20Building%20on%20our%0Aexploration%2C%20we%20design%20ViTok%20as%20a%20lightweight%20auto-encoder%20that%20achieves%0Acompetitive%20performance%20with%20state-of-the-art%20auto-encoders%20on%20ImageNet-1K%20and%0ACOCO%20reconstruction%20tasks%20%28256p%20and%20512p%29%20while%20outperforming%20existing%0Aauto-encoders%20on%2016-frame%20128p%20video%20reconstruction%20for%20UCF-101%2C%20all%20with%202-5x%0Afewer%20FLOPs.%20When%20integrated%20with%20Diffusion%20Transformers%2C%20ViTok%20demonstrates%0Acompetitive%20performance%20on%20image%20generation%20for%20ImageNet-1K%20and%20sets%20new%0Astate-of-the-art%20benchmarks%20for%20class-conditional%20video%20generation%20on%20UCF-101.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnings%2520from%2520Scaling%2520Visual%2520Tokenizers%2520for%2520Reconstruction%2520and%250A%2520%2520Generation%26entry.906535625%3DPhilippe%2520Hansen-Estruch%2520and%2520David%2520Yan%2520and%2520Ching-Yao%2520Chung%2520and%2520Orr%2520Zohar%2520and%2520Jialiang%2520Wang%2520and%2520Tingbo%2520Hou%2520and%2520Tao%2520Xu%2520and%2520Sriram%2520Vishwanath%2520and%2520Peter%2520Vajda%2520and%2520Xinlei%2520Chen%26entry.1292438233%3D%2520%2520Visual%2520tokenization%2520via%2520auto-encoding%2520empowers%2520state-of-the-art%2520image%2520and%250Avideo%2520generative%2520models%2520by%2520compressing%2520pixels%2520into%2520a%2520latent%2520space.%2520Although%250Ascaling%2520Transformer-based%2520generators%2520has%2520been%2520central%2520to%2520recent%2520advances%252C%2520the%250Atokenizer%2520component%2520itself%2520is%2520rarely%2520scaled%252C%2520leaving%2520open%2520questions%2520about%2520how%250Aauto-encoder%2520design%2520choices%2520influence%2520both%2520its%2520objective%2520of%2520reconstruction%2520and%250Adownstream%2520generative%2520performance.%2520Our%2520work%2520aims%2520to%2520conduct%2520an%2520exploration%2520of%250Ascaling%2520in%2520auto-encoders%2520to%2520fill%2520in%2520this%2520blank.%2520To%2520facilitate%2520this%2520exploration%252C%250Awe%2520replace%2520the%2520typical%2520convolutional%2520backbone%2520with%2520an%2520enhanced%2520Vision%250ATransformer%2520architecture%2520for%2520Tokenization%2520%2528ViTok%2529.%2520We%2520train%2520ViTok%2520on%250Alarge-scale%2520image%2520and%2520video%2520datasets%2520far%2520exceeding%2520ImageNet-1K%252C%2520removing%2520data%250Aconstraints%2520on%2520tokenizer%2520scaling.%2520We%2520first%2520study%2520how%2520scaling%2520the%2520auto-encoder%250Abottleneck%2520affects%2520both%2520reconstruction%2520and%2520generation%2520--%2520and%2520find%2520that%2520while%2520it%250Ais%2520highly%2520correlated%2520with%2520reconstruction%252C%2520its%2520relationship%2520with%2520generation%2520is%250Amore%2520complex.%2520We%2520next%2520explored%2520the%2520effect%2520of%2520separately%2520scaling%2520the%250Aauto-encoders%2527%2520encoder%2520and%2520decoder%2520on%2520reconstruction%2520and%2520generation%250Aperformance.%2520Crucially%252C%2520we%2520find%2520that%2520scaling%2520the%2520encoder%2520yields%2520minimal%2520gains%250Afor%2520either%2520reconstruction%2520or%2520generation%252C%2520while%2520scaling%2520the%2520decoder%2520boosts%250Areconstruction%2520but%2520the%2520benefits%2520for%2520generation%2520are%2520mixed.%2520Building%2520on%2520our%250Aexploration%252C%2520we%2520design%2520ViTok%2520as%2520a%2520lightweight%2520auto-encoder%2520that%2520achieves%250Acompetitive%2520performance%2520with%2520state-of-the-art%2520auto-encoders%2520on%2520ImageNet-1K%2520and%250ACOCO%2520reconstruction%2520tasks%2520%2528256p%2520and%2520512p%2529%2520while%2520outperforming%2520existing%250Aauto-encoders%2520on%252016-frame%2520128p%2520video%2520reconstruction%2520for%2520UCF-101%252C%2520all%2520with%25202-5x%250Afewer%2520FLOPs.%2520When%2520integrated%2520with%2520Diffusion%2520Transformers%252C%2520ViTok%2520demonstrates%250Acompetitive%2520performance%2520on%2520image%2520generation%2520for%2520ImageNet-1K%2520and%2520sets%2520new%250Astate-of-the-art%2520benchmarks%2520for%2520class-conditional%2520video%2520generation%2520on%2520UCF-101.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnings%20from%20Scaling%20Visual%20Tokenizers%20for%20Reconstruction%20and%0A%20%20Generation&entry.906535625=Philippe%20Hansen-Estruch%20and%20David%20Yan%20and%20Ching-Yao%20Chung%20and%20Orr%20Zohar%20and%20Jialiang%20Wang%20and%20Tingbo%20Hou%20and%20Tao%20Xu%20and%20Sriram%20Vishwanath%20and%20Peter%20Vajda%20and%20Xinlei%20Chen&entry.1292438233=%20%20Visual%20tokenization%20via%20auto-encoding%20empowers%20state-of-the-art%20image%20and%0Avideo%20generative%20models%20by%20compressing%20pixels%20into%20a%20latent%20space.%20Although%0Ascaling%20Transformer-based%20generators%20has%20been%20central%20to%20recent%20advances%2C%20the%0Atokenizer%20component%20itself%20is%20rarely%20scaled%2C%20leaving%20open%20questions%20about%20how%0Aauto-encoder%20design%20choices%20influence%20both%20its%20objective%20of%20reconstruction%20and%0Adownstream%20generative%20performance.%20Our%20work%20aims%20to%20conduct%20an%20exploration%20of%0Ascaling%20in%20auto-encoders%20to%20fill%20in%20this%20blank.%20To%20facilitate%20this%20exploration%2C%0Awe%20replace%20the%20typical%20convolutional%20backbone%20with%20an%20enhanced%20Vision%0ATransformer%20architecture%20for%20Tokenization%20%28ViTok%29.%20We%20train%20ViTok%20on%0Alarge-scale%20image%20and%20video%20datasets%20far%20exceeding%20ImageNet-1K%2C%20removing%20data%0Aconstraints%20on%20tokenizer%20scaling.%20We%20first%20study%20how%20scaling%20the%20auto-encoder%0Abottleneck%20affects%20both%20reconstruction%20and%20generation%20--%20and%20find%20that%20while%20it%0Ais%20highly%20correlated%20with%20reconstruction%2C%20its%20relationship%20with%20generation%20is%0Amore%20complex.%20We%20next%20explored%20the%20effect%20of%20separately%20scaling%20the%0Aauto-encoders%27%20encoder%20and%20decoder%20on%20reconstruction%20and%20generation%0Aperformance.%20Crucially%2C%20we%20find%20that%20scaling%20the%20encoder%20yields%20minimal%20gains%0Afor%20either%20reconstruction%20or%20generation%2C%20while%20scaling%20the%20decoder%20boosts%0Areconstruction%20but%20the%20benefits%20for%20generation%20are%20mixed.%20Building%20on%20our%0Aexploration%2C%20we%20design%20ViTok%20as%20a%20lightweight%20auto-encoder%20that%20achieves%0Acompetitive%20performance%20with%20state-of-the-art%20auto-encoders%20on%20ImageNet-1K%20and%0ACOCO%20reconstruction%20tasks%20%28256p%20and%20512p%29%20while%20outperforming%20existing%0Aauto-encoders%20on%2016-frame%20128p%20video%20reconstruction%20for%20UCF-101%2C%20all%20with%202-5x%0Afewer%20FLOPs.%20When%20integrated%20with%20Diffusion%20Transformers%2C%20ViTok%20demonstrates%0Acompetitive%20performance%20on%20image%20generation%20for%20ImageNet-1K%20and%20sets%20new%0Astate-of-the-art%20benchmarks%20for%20class-conditional%20video%20generation%20on%20UCF-101.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09755v1&entry.124074799=Read"},
{"title": "AnyStory: Towards Unified Single and Multiple Subject Personalization in\n  Text-to-Image Generation", "author": "Junjie He and Yuxiang Tuo and Binghui Chen and Chongyang Zhong and Yifeng Geng and Liefeng Bo", "abstract": "  Recently, large-scale generative models have demonstrated outstanding\ntext-to-image generation capabilities. However, generating high-fidelity\npersonalized images with specific subjects still presents challenges,\nespecially in cases involving multiple subjects. In this paper, we propose\nAnyStory, a unified approach for personalized subject generation. AnyStory not\nonly achieves high-fidelity personalization for single subjects, but also for\nmultiple subjects, without sacrificing subject fidelity. Specifically, AnyStory\nmodels the subject personalization problem in an \"encode-then-route\" manner. In\nthe encoding step, AnyStory utilizes a universal and powerful image encoder,\ni.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve\nhigh-fidelity encoding of subject features. In the routing step, AnyStory\nutilizes a decoupled instance-aware subject router to accurately perceive and\npredict the potential location of the corresponding subject in the latent\nspace, and guide the injection of subject conditions. Detailed experimental\nresults demonstrate the excellent performance of our method in retaining\nsubject details, aligning text descriptions, and personalizing for multiple\nsubjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .\n", "link": "http://arxiv.org/abs/2501.09503v1", "date": "2025-01-16", "relevancy": 2.3298, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6231}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5984}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyStory%3A%20Towards%20Unified%20Single%20and%20Multiple%20Subject%20Personalization%20in%0A%20%20Text-to-Image%20Generation&body=Title%3A%20AnyStory%3A%20Towards%20Unified%20Single%20and%20Multiple%20Subject%20Personalization%20in%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Junjie%20He%20and%20Yuxiang%20Tuo%20and%20Binghui%20Chen%20and%20Chongyang%20Zhong%20and%20Yifeng%20Geng%20and%20Liefeng%20Bo%0AAbstract%3A%20%20%20Recently%2C%20large-scale%20generative%20models%20have%20demonstrated%20outstanding%0Atext-to-image%20generation%20capabilities.%20However%2C%20generating%20high-fidelity%0Apersonalized%20images%20with%20specific%20subjects%20still%20presents%20challenges%2C%0Aespecially%20in%20cases%20involving%20multiple%20subjects.%20In%20this%20paper%2C%20we%20propose%0AAnyStory%2C%20a%20unified%20approach%20for%20personalized%20subject%20generation.%20AnyStory%20not%0Aonly%20achieves%20high-fidelity%20personalization%20for%20single%20subjects%2C%20but%20also%20for%0Amultiple%20subjects%2C%20without%20sacrificing%20subject%20fidelity.%20Specifically%2C%20AnyStory%0Amodels%20the%20subject%20personalization%20problem%20in%20an%20%22encode-then-route%22%20manner.%20In%0Athe%20encoding%20step%2C%20AnyStory%20utilizes%20a%20universal%20and%20powerful%20image%20encoder%2C%0Ai.e.%2C%20ReferenceNet%2C%20in%20conjunction%20with%20CLIP%20vision%20encoder%20to%20achieve%0Ahigh-fidelity%20encoding%20of%20subject%20features.%20In%20the%20routing%20step%2C%20AnyStory%0Autilizes%20a%20decoupled%20instance-aware%20subject%20router%20to%20accurately%20perceive%20and%0Apredict%20the%20potential%20location%20of%20the%20corresponding%20subject%20in%20the%20latent%0Aspace%2C%20and%20guide%20the%20injection%20of%20subject%20conditions.%20Detailed%20experimental%0Aresults%20demonstrate%20the%20excellent%20performance%20of%20our%20method%20in%20retaining%0Asubject%20details%2C%20aligning%20text%20descriptions%2C%20and%20personalizing%20for%20multiple%0Asubjects.%20The%20project%20page%20is%20at%20https%3A//aigcdesigngroup.github.io/AnyStory/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyStory%253A%2520Towards%2520Unified%2520Single%2520and%2520Multiple%2520Subject%2520Personalization%2520in%250A%2520%2520Text-to-Image%2520Generation%26entry.906535625%3DJunjie%2520He%2520and%2520Yuxiang%2520Tuo%2520and%2520Binghui%2520Chen%2520and%2520Chongyang%2520Zhong%2520and%2520Yifeng%2520Geng%2520and%2520Liefeng%2520Bo%26entry.1292438233%3D%2520%2520Recently%252C%2520large-scale%2520generative%2520models%2520have%2520demonstrated%2520outstanding%250Atext-to-image%2520generation%2520capabilities.%2520However%252C%2520generating%2520high-fidelity%250Apersonalized%2520images%2520with%2520specific%2520subjects%2520still%2520presents%2520challenges%252C%250Aespecially%2520in%2520cases%2520involving%2520multiple%2520subjects.%2520In%2520this%2520paper%252C%2520we%2520propose%250AAnyStory%252C%2520a%2520unified%2520approach%2520for%2520personalized%2520subject%2520generation.%2520AnyStory%2520not%250Aonly%2520achieves%2520high-fidelity%2520personalization%2520for%2520single%2520subjects%252C%2520but%2520also%2520for%250Amultiple%2520subjects%252C%2520without%2520sacrificing%2520subject%2520fidelity.%2520Specifically%252C%2520AnyStory%250Amodels%2520the%2520subject%2520personalization%2520problem%2520in%2520an%2520%2522encode-then-route%2522%2520manner.%2520In%250Athe%2520encoding%2520step%252C%2520AnyStory%2520utilizes%2520a%2520universal%2520and%2520powerful%2520image%2520encoder%252C%250Ai.e.%252C%2520ReferenceNet%252C%2520in%2520conjunction%2520with%2520CLIP%2520vision%2520encoder%2520to%2520achieve%250Ahigh-fidelity%2520encoding%2520of%2520subject%2520features.%2520In%2520the%2520routing%2520step%252C%2520AnyStory%250Autilizes%2520a%2520decoupled%2520instance-aware%2520subject%2520router%2520to%2520accurately%2520perceive%2520and%250Apredict%2520the%2520potential%2520location%2520of%2520the%2520corresponding%2520subject%2520in%2520the%2520latent%250Aspace%252C%2520and%2520guide%2520the%2520injection%2520of%2520subject%2520conditions.%2520Detailed%2520experimental%250Aresults%2520demonstrate%2520the%2520excellent%2520performance%2520of%2520our%2520method%2520in%2520retaining%250Asubject%2520details%252C%2520aligning%2520text%2520descriptions%252C%2520and%2520personalizing%2520for%2520multiple%250Asubjects.%2520The%2520project%2520page%2520is%2520at%2520https%253A//aigcdesigngroup.github.io/AnyStory/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyStory%3A%20Towards%20Unified%20Single%20and%20Multiple%20Subject%20Personalization%20in%0A%20%20Text-to-Image%20Generation&entry.906535625=Junjie%20He%20and%20Yuxiang%20Tuo%20and%20Binghui%20Chen%20and%20Chongyang%20Zhong%20and%20Yifeng%20Geng%20and%20Liefeng%20Bo&entry.1292438233=%20%20Recently%2C%20large-scale%20generative%20models%20have%20demonstrated%20outstanding%0Atext-to-image%20generation%20capabilities.%20However%2C%20generating%20high-fidelity%0Apersonalized%20images%20with%20specific%20subjects%20still%20presents%20challenges%2C%0Aespecially%20in%20cases%20involving%20multiple%20subjects.%20In%20this%20paper%2C%20we%20propose%0AAnyStory%2C%20a%20unified%20approach%20for%20personalized%20subject%20generation.%20AnyStory%20not%0Aonly%20achieves%20high-fidelity%20personalization%20for%20single%20subjects%2C%20but%20also%20for%0Amultiple%20subjects%2C%20without%20sacrificing%20subject%20fidelity.%20Specifically%2C%20AnyStory%0Amodels%20the%20subject%20personalization%20problem%20in%20an%20%22encode-then-route%22%20manner.%20In%0Athe%20encoding%20step%2C%20AnyStory%20utilizes%20a%20universal%20and%20powerful%20image%20encoder%2C%0Ai.e.%2C%20ReferenceNet%2C%20in%20conjunction%20with%20CLIP%20vision%20encoder%20to%20achieve%0Ahigh-fidelity%20encoding%20of%20subject%20features.%20In%20the%20routing%20step%2C%20AnyStory%0Autilizes%20a%20decoupled%20instance-aware%20subject%20router%20to%20accurately%20perceive%20and%0Apredict%20the%20potential%20location%20of%20the%20corresponding%20subject%20in%20the%20latent%0Aspace%2C%20and%20guide%20the%20injection%20of%20subject%20conditions.%20Detailed%20experimental%0Aresults%20demonstrate%20the%20excellent%20performance%20of%20our%20method%20in%20retaining%0Asubject%20details%2C%20aligning%20text%20descriptions%2C%20and%20personalizing%20for%20multiple%0Asubjects.%20The%20project%20page%20is%20at%20https%3A//aigcdesigngroup.github.io/AnyStory/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09503v1&entry.124074799=Read"},
{"title": "Comparison of Various SLAM Systems for Mobile Robot in an Indoor\n  Environment", "author": "Maksim Filipenko and Ilya Afanasyev", "abstract": "  This article presents a comparative analysis of a mobile robot trajectories\ncomputed by various ROS-based SLAM systems. For this reason we developed a\nprototype of a mobile robot with common sensors: 2D lidar, a monocular and ZED\nstereo cameras. Then we conducted experiments in a typical office environment\nand collected data from all sensors, running all tested SLAM systems based on\nthe acquired dataset. We studied the following SLAM systems: (a) 2D\nlidar-based: GMapping, Hector SLAM, Cartographer; (b) monocular camera-based:\nLarge Scale Direct monocular SLAM (LSD SLAM), ORB SLAM, Direct Sparse Odometry\n(DSO); and (c) stereo camera-based: ZEDfu, Real-Time Appearance-Based Mapping\n(RTAB map), ORB SLAM, Stereo Parallel Tracking and Mapping (S-PTAM). Since all\nSLAM methods were tested on the same dataset we compared results for different\nSLAM systems with appropriate metrics, demonstrating encouraging results for\nlidar-based Cartographer SLAM, Monocular ORB SLAM and Stereo RTAB Map methods.\n", "link": "http://arxiv.org/abs/2501.09490v1", "date": "2025-01-16", "relevancy": 2.2978, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5811}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5764}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparison%20of%20Various%20SLAM%20Systems%20for%20Mobile%20Robot%20in%20an%20Indoor%0A%20%20Environment&body=Title%3A%20Comparison%20of%20Various%20SLAM%20Systems%20for%20Mobile%20Robot%20in%20an%20Indoor%0A%20%20Environment%0AAuthor%3A%20Maksim%20Filipenko%20and%20Ilya%20Afanasyev%0AAbstract%3A%20%20%20This%20article%20presents%20a%20comparative%20analysis%20of%20a%20mobile%20robot%20trajectories%0Acomputed%20by%20various%20ROS-based%20SLAM%20systems.%20For%20this%20reason%20we%20developed%20a%0Aprototype%20of%20a%20mobile%20robot%20with%20common%20sensors%3A%202D%20lidar%2C%20a%20monocular%20and%20ZED%0Astereo%20cameras.%20Then%20we%20conducted%20experiments%20in%20a%20typical%20office%20environment%0Aand%20collected%20data%20from%20all%20sensors%2C%20running%20all%20tested%20SLAM%20systems%20based%20on%0Athe%20acquired%20dataset.%20We%20studied%20the%20following%20SLAM%20systems%3A%20%28a%29%202D%0Alidar-based%3A%20GMapping%2C%20Hector%20SLAM%2C%20Cartographer%3B%20%28b%29%20monocular%20camera-based%3A%0ALarge%20Scale%20Direct%20monocular%20SLAM%20%28LSD%20SLAM%29%2C%20ORB%20SLAM%2C%20Direct%20Sparse%20Odometry%0A%28DSO%29%3B%20and%20%28c%29%20stereo%20camera-based%3A%20ZEDfu%2C%20Real-Time%20Appearance-Based%20Mapping%0A%28RTAB%20map%29%2C%20ORB%20SLAM%2C%20Stereo%20Parallel%20Tracking%20and%20Mapping%20%28S-PTAM%29.%20Since%20all%0ASLAM%20methods%20were%20tested%20on%20the%20same%20dataset%20we%20compared%20results%20for%20different%0ASLAM%20systems%20with%20appropriate%20metrics%2C%20demonstrating%20encouraging%20results%20for%0Alidar-based%20Cartographer%20SLAM%2C%20Monocular%20ORB%20SLAM%20and%20Stereo%20RTAB%20Map%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparison%2520of%2520Various%2520SLAM%2520Systems%2520for%2520Mobile%2520Robot%2520in%2520an%2520Indoor%250A%2520%2520Environment%26entry.906535625%3DMaksim%2520Filipenko%2520and%2520Ilya%2520Afanasyev%26entry.1292438233%3D%2520%2520This%2520article%2520presents%2520a%2520comparative%2520analysis%2520of%2520a%2520mobile%2520robot%2520trajectories%250Acomputed%2520by%2520various%2520ROS-based%2520SLAM%2520systems.%2520For%2520this%2520reason%2520we%2520developed%2520a%250Aprototype%2520of%2520a%2520mobile%2520robot%2520with%2520common%2520sensors%253A%25202D%2520lidar%252C%2520a%2520monocular%2520and%2520ZED%250Astereo%2520cameras.%2520Then%2520we%2520conducted%2520experiments%2520in%2520a%2520typical%2520office%2520environment%250Aand%2520collected%2520data%2520from%2520all%2520sensors%252C%2520running%2520all%2520tested%2520SLAM%2520systems%2520based%2520on%250Athe%2520acquired%2520dataset.%2520We%2520studied%2520the%2520following%2520SLAM%2520systems%253A%2520%2528a%2529%25202D%250Alidar-based%253A%2520GMapping%252C%2520Hector%2520SLAM%252C%2520Cartographer%253B%2520%2528b%2529%2520monocular%2520camera-based%253A%250ALarge%2520Scale%2520Direct%2520monocular%2520SLAM%2520%2528LSD%2520SLAM%2529%252C%2520ORB%2520SLAM%252C%2520Direct%2520Sparse%2520Odometry%250A%2528DSO%2529%253B%2520and%2520%2528c%2529%2520stereo%2520camera-based%253A%2520ZEDfu%252C%2520Real-Time%2520Appearance-Based%2520Mapping%250A%2528RTAB%2520map%2529%252C%2520ORB%2520SLAM%252C%2520Stereo%2520Parallel%2520Tracking%2520and%2520Mapping%2520%2528S-PTAM%2529.%2520Since%2520all%250ASLAM%2520methods%2520were%2520tested%2520on%2520the%2520same%2520dataset%2520we%2520compared%2520results%2520for%2520different%250ASLAM%2520systems%2520with%2520appropriate%2520metrics%252C%2520demonstrating%2520encouraging%2520results%2520for%250Alidar-based%2520Cartographer%2520SLAM%252C%2520Monocular%2520ORB%2520SLAM%2520and%2520Stereo%2520RTAB%2520Map%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparison%20of%20Various%20SLAM%20Systems%20for%20Mobile%20Robot%20in%20an%20Indoor%0A%20%20Environment&entry.906535625=Maksim%20Filipenko%20and%20Ilya%20Afanasyev&entry.1292438233=%20%20This%20article%20presents%20a%20comparative%20analysis%20of%20a%20mobile%20robot%20trajectories%0Acomputed%20by%20various%20ROS-based%20SLAM%20systems.%20For%20this%20reason%20we%20developed%20a%0Aprototype%20of%20a%20mobile%20robot%20with%20common%20sensors%3A%202D%20lidar%2C%20a%20monocular%20and%20ZED%0Astereo%20cameras.%20Then%20we%20conducted%20experiments%20in%20a%20typical%20office%20environment%0Aand%20collected%20data%20from%20all%20sensors%2C%20running%20all%20tested%20SLAM%20systems%20based%20on%0Athe%20acquired%20dataset.%20We%20studied%20the%20following%20SLAM%20systems%3A%20%28a%29%202D%0Alidar-based%3A%20GMapping%2C%20Hector%20SLAM%2C%20Cartographer%3B%20%28b%29%20monocular%20camera-based%3A%0ALarge%20Scale%20Direct%20monocular%20SLAM%20%28LSD%20SLAM%29%2C%20ORB%20SLAM%2C%20Direct%20Sparse%20Odometry%0A%28DSO%29%3B%20and%20%28c%29%20stereo%20camera-based%3A%20ZEDfu%2C%20Real-Time%20Appearance-Based%20Mapping%0A%28RTAB%20map%29%2C%20ORB%20SLAM%2C%20Stereo%20Parallel%20Tracking%20and%20Mapping%20%28S-PTAM%29.%20Since%20all%0ASLAM%20methods%20were%20tested%20on%20the%20same%20dataset%20we%20compared%20results%20for%20different%0ASLAM%20systems%20with%20appropriate%20metrics%2C%20demonstrating%20encouraging%20results%20for%0Alidar-based%20Cartographer%20SLAM%2C%20Monocular%20ORB%20SLAM%20and%20Stereo%20RTAB%20Map%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09490v1&entry.124074799=Read"},
{"title": "A Near-optimal Algorithm for Learning Margin Halfspaces with Massart\n  Noise", "author": "Ilias Diakonikolas and Nikos Zarifis", "abstract": "  We study the problem of PAC learning $\\gamma$-margin halfspaces in the\npresence of Massart noise. Without computational considerations, the sample\ncomplexity of this learning problem is known to be\n$\\widetilde{\\Theta}(1/(\\gamma^2 \\epsilon))$. Prior computationally efficient\nalgorithms for the problem incur sample complexity $\\tilde{O}(1/(\\gamma^4\n\\epsilon^3))$ and achieve 0-1 error of $\\eta+\\epsilon$, where $\\eta<1/2$ is the\nupper bound on the noise rate. Recent work gave evidence of an\ninformation-computation tradeoff, suggesting that a quadratic dependence on\n$1/\\epsilon$ is required for computationally efficient algorithms. Our main\nresult is a computationally efficient learner with sample complexity\n$\\widetilde{\\Theta}(1/(\\gamma^2 \\epsilon^2))$, nearly matching this lower\nbound. In addition, our algorithm is simple and practical, relying on online\nSGD on a carefully selected sequence of convex losses.\n", "link": "http://arxiv.org/abs/2501.09691v1", "date": "2025-01-16", "relevancy": 2.2959, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4666}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4585}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Near-optimal%20Algorithm%20for%20Learning%20Margin%20Halfspaces%20with%20Massart%0A%20%20Noise&body=Title%3A%20A%20Near-optimal%20Algorithm%20for%20Learning%20Margin%20Halfspaces%20with%20Massart%0A%20%20Noise%0AAuthor%3A%20Ilias%20Diakonikolas%20and%20Nikos%20Zarifis%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20PAC%20learning%20%24%5Cgamma%24-margin%20halfspaces%20in%20the%0Apresence%20of%20Massart%20noise.%20Without%20computational%20considerations%2C%20the%20sample%0Acomplexity%20of%20this%20learning%20problem%20is%20known%20to%20be%0A%24%5Cwidetilde%7B%5CTheta%7D%281/%28%5Cgamma%5E2%20%5Cepsilon%29%29%24.%20Prior%20computationally%20efficient%0Aalgorithms%20for%20the%20problem%20incur%20sample%20complexity%20%24%5Ctilde%7BO%7D%281/%28%5Cgamma%5E4%0A%5Cepsilon%5E3%29%29%24%20and%20achieve%200-1%20error%20of%20%24%5Ceta%2B%5Cepsilon%24%2C%20where%20%24%5Ceta%3C1/2%24%20is%20the%0Aupper%20bound%20on%20the%20noise%20rate.%20Recent%20work%20gave%20evidence%20of%20an%0Ainformation-computation%20tradeoff%2C%20suggesting%20that%20a%20quadratic%20dependence%20on%0A%241/%5Cepsilon%24%20is%20required%20for%20computationally%20efficient%20algorithms.%20Our%20main%0Aresult%20is%20a%20computationally%20efficient%20learner%20with%20sample%20complexity%0A%24%5Cwidetilde%7B%5CTheta%7D%281/%28%5Cgamma%5E2%20%5Cepsilon%5E2%29%29%24%2C%20nearly%20matching%20this%20lower%0Abound.%20In%20addition%2C%20our%20algorithm%20is%20simple%20and%20practical%2C%20relying%20on%20online%0ASGD%20on%20a%20carefully%20selected%20sequence%20of%20convex%20losses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Near-optimal%2520Algorithm%2520for%2520Learning%2520Margin%2520Halfspaces%2520with%2520Massart%250A%2520%2520Noise%26entry.906535625%3DIlias%2520Diakonikolas%2520and%2520Nikos%2520Zarifis%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520PAC%2520learning%2520%2524%255Cgamma%2524-margin%2520halfspaces%2520in%2520the%250Apresence%2520of%2520Massart%2520noise.%2520Without%2520computational%2520considerations%252C%2520the%2520sample%250Acomplexity%2520of%2520this%2520learning%2520problem%2520is%2520known%2520to%2520be%250A%2524%255Cwidetilde%257B%255CTheta%257D%25281/%2528%255Cgamma%255E2%2520%255Cepsilon%2529%2529%2524.%2520Prior%2520computationally%2520efficient%250Aalgorithms%2520for%2520the%2520problem%2520incur%2520sample%2520complexity%2520%2524%255Ctilde%257BO%257D%25281/%2528%255Cgamma%255E4%250A%255Cepsilon%255E3%2529%2529%2524%2520and%2520achieve%25200-1%2520error%2520of%2520%2524%255Ceta%252B%255Cepsilon%2524%252C%2520where%2520%2524%255Ceta%253C1/2%2524%2520is%2520the%250Aupper%2520bound%2520on%2520the%2520noise%2520rate.%2520Recent%2520work%2520gave%2520evidence%2520of%2520an%250Ainformation-computation%2520tradeoff%252C%2520suggesting%2520that%2520a%2520quadratic%2520dependence%2520on%250A%25241/%255Cepsilon%2524%2520is%2520required%2520for%2520computationally%2520efficient%2520algorithms.%2520Our%2520main%250Aresult%2520is%2520a%2520computationally%2520efficient%2520learner%2520with%2520sample%2520complexity%250A%2524%255Cwidetilde%257B%255CTheta%257D%25281/%2528%255Cgamma%255E2%2520%255Cepsilon%255E2%2529%2529%2524%252C%2520nearly%2520matching%2520this%2520lower%250Abound.%2520In%2520addition%252C%2520our%2520algorithm%2520is%2520simple%2520and%2520practical%252C%2520relying%2520on%2520online%250ASGD%2520on%2520a%2520carefully%2520selected%2520sequence%2520of%2520convex%2520losses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Near-optimal%20Algorithm%20for%20Learning%20Margin%20Halfspaces%20with%20Massart%0A%20%20Noise&entry.906535625=Ilias%20Diakonikolas%20and%20Nikos%20Zarifis&entry.1292438233=%20%20We%20study%20the%20problem%20of%20PAC%20learning%20%24%5Cgamma%24-margin%20halfspaces%20in%20the%0Apresence%20of%20Massart%20noise.%20Without%20computational%20considerations%2C%20the%20sample%0Acomplexity%20of%20this%20learning%20problem%20is%20known%20to%20be%0A%24%5Cwidetilde%7B%5CTheta%7D%281/%28%5Cgamma%5E2%20%5Cepsilon%29%29%24.%20Prior%20computationally%20efficient%0Aalgorithms%20for%20the%20problem%20incur%20sample%20complexity%20%24%5Ctilde%7BO%7D%281/%28%5Cgamma%5E4%0A%5Cepsilon%5E3%29%29%24%20and%20achieve%200-1%20error%20of%20%24%5Ceta%2B%5Cepsilon%24%2C%20where%20%24%5Ceta%3C1/2%24%20is%20the%0Aupper%20bound%20on%20the%20noise%20rate.%20Recent%20work%20gave%20evidence%20of%20an%0Ainformation-computation%20tradeoff%2C%20suggesting%20that%20a%20quadratic%20dependence%20on%0A%241/%5Cepsilon%24%20is%20required%20for%20computationally%20efficient%20algorithms.%20Our%20main%0Aresult%20is%20a%20computationally%20efficient%20learner%20with%20sample%20complexity%0A%24%5Cwidetilde%7B%5CTheta%7D%281/%28%5Cgamma%5E2%20%5Cepsilon%5E2%29%29%24%2C%20nearly%20matching%20this%20lower%0Abound.%20In%20addition%2C%20our%20algorithm%20is%20simple%20and%20practical%2C%20relying%20on%20online%0ASGD%20on%20a%20carefully%20selected%20sequence%20of%20convex%20losses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09691v1&entry.124074799=Read"},
{"title": "VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image\n  Segmentation and Classification", "author": "Zelong Liu and Andrew Tieu and Nikhil Patel and Georgios Soultanidis and Louisa Deyer and Ying Wang and Sean Huver and Alexander Zhou and Yunhao Mei and Zahi A. Fayad and Timothy Deyer and Xueyan Mei", "abstract": "  Artificial Intelligence (AI) has the potential to revolutionize diagnosis and\nsegmentation in medical imaging. However, development and clinical\nimplementation face multiple challenges including limited data availability,\nlack of generalizability, and the necessity to incorporate multi-modal data\neffectively. A foundation model, which is a large-scale pre-trained AI model,\noffers a versatile base that can be adapted to a variety of specific tasks and\ncontexts. Here, we present VIsualization and Segmentation Masked AutoEncoder\n(VIS-MAE), novel model weights specifically designed for medical imaging.\nSpecifically, VIS-MAE is trained on a dataset of 2.5 million unlabeled images\nfrom various modalities (CT, MR, PET,X-rays, and ultrasound), using\nself-supervised learning techniques. It is then adapted to classification and\nsegmentation tasks using explicit labels. VIS-MAE has high label efficiency,\noutperforming several benchmark models in both in-domain and out-of-domain\napplications. In addition, VIS-MAE has improved label efficiency as it can\nachieve similar performance to other models with a reduced amount of labeled\ntraining data (50% or 80%) compared to other pre-trained weights. VIS-MAE\nrepresents a significant advancement in medical imaging AI, offering a\ngeneralizable and robust solution for improving segmentation and classification\ntasks while reducing the data annotation workload. The source code of this work\nis available at https://github.com/lzl199704/VIS-MAE.\n", "link": "http://arxiv.org/abs/2402.01034v2", "date": "2025-01-16", "relevancy": 2.2847, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5939}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5684}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIS-MAE%3A%20An%20Efficient%20Self-supervised%20Learning%20Approach%20on%20Medical%20Image%0A%20%20Segmentation%20and%20Classification&body=Title%3A%20VIS-MAE%3A%20An%20Efficient%20Self-supervised%20Learning%20Approach%20on%20Medical%20Image%0A%20%20Segmentation%20and%20Classification%0AAuthor%3A%20Zelong%20Liu%20and%20Andrew%20Tieu%20and%20Nikhil%20Patel%20and%20Georgios%20Soultanidis%20and%20Louisa%20Deyer%20and%20Ying%20Wang%20and%20Sean%20Huver%20and%20Alexander%20Zhou%20and%20Yunhao%20Mei%20and%20Zahi%20A.%20Fayad%20and%20Timothy%20Deyer%20and%20Xueyan%20Mei%0AAbstract%3A%20%20%20Artificial%20Intelligence%20%28AI%29%20has%20the%20potential%20to%20revolutionize%20diagnosis%20and%0Asegmentation%20in%20medical%20imaging.%20However%2C%20development%20and%20clinical%0Aimplementation%20face%20multiple%20challenges%20including%20limited%20data%20availability%2C%0Alack%20of%20generalizability%2C%20and%20the%20necessity%20to%20incorporate%20multi-modal%20data%0Aeffectively.%20A%20foundation%20model%2C%20which%20is%20a%20large-scale%20pre-trained%20AI%20model%2C%0Aoffers%20a%20versatile%20base%20that%20can%20be%20adapted%20to%20a%20variety%20of%20specific%20tasks%20and%0Acontexts.%20Here%2C%20we%20present%20VIsualization%20and%20Segmentation%20Masked%20AutoEncoder%0A%28VIS-MAE%29%2C%20novel%20model%20weights%20specifically%20designed%20for%20medical%20imaging.%0ASpecifically%2C%20VIS-MAE%20is%20trained%20on%20a%20dataset%20of%202.5%20million%20unlabeled%20images%0Afrom%20various%20modalities%20%28CT%2C%20MR%2C%20PET%2CX-rays%2C%20and%20ultrasound%29%2C%20using%0Aself-supervised%20learning%20techniques.%20It%20is%20then%20adapted%20to%20classification%20and%0Asegmentation%20tasks%20using%20explicit%20labels.%20VIS-MAE%20has%20high%20label%20efficiency%2C%0Aoutperforming%20several%20benchmark%20models%20in%20both%20in-domain%20and%20out-of-domain%0Aapplications.%20In%20addition%2C%20VIS-MAE%20has%20improved%20label%20efficiency%20as%20it%20can%0Aachieve%20similar%20performance%20to%20other%20models%20with%20a%20reduced%20amount%20of%20labeled%0Atraining%20data%20%2850%25%20or%2080%25%29%20compared%20to%20other%20pre-trained%20weights.%20VIS-MAE%0Arepresents%20a%20significant%20advancement%20in%20medical%20imaging%20AI%2C%20offering%20a%0Ageneralizable%20and%20robust%20solution%20for%20improving%20segmentation%20and%20classification%0Atasks%20while%20reducing%20the%20data%20annotation%20workload.%20The%20source%20code%20of%20this%20work%0Ais%20available%20at%20https%3A//github.com/lzl199704/VIS-MAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01034v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIS-MAE%253A%2520An%2520Efficient%2520Self-supervised%2520Learning%2520Approach%2520on%2520Medical%2520Image%250A%2520%2520Segmentation%2520and%2520Classification%26entry.906535625%3DZelong%2520Liu%2520and%2520Andrew%2520Tieu%2520and%2520Nikhil%2520Patel%2520and%2520Georgios%2520Soultanidis%2520and%2520Louisa%2520Deyer%2520and%2520Ying%2520Wang%2520and%2520Sean%2520Huver%2520and%2520Alexander%2520Zhou%2520and%2520Yunhao%2520Mei%2520and%2520Zahi%2520A.%2520Fayad%2520and%2520Timothy%2520Deyer%2520and%2520Xueyan%2520Mei%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520%2528AI%2529%2520has%2520the%2520potential%2520to%2520revolutionize%2520diagnosis%2520and%250Asegmentation%2520in%2520medical%2520imaging.%2520However%252C%2520development%2520and%2520clinical%250Aimplementation%2520face%2520multiple%2520challenges%2520including%2520limited%2520data%2520availability%252C%250Alack%2520of%2520generalizability%252C%2520and%2520the%2520necessity%2520to%2520incorporate%2520multi-modal%2520data%250Aeffectively.%2520A%2520foundation%2520model%252C%2520which%2520is%2520a%2520large-scale%2520pre-trained%2520AI%2520model%252C%250Aoffers%2520a%2520versatile%2520base%2520that%2520can%2520be%2520adapted%2520to%2520a%2520variety%2520of%2520specific%2520tasks%2520and%250Acontexts.%2520Here%252C%2520we%2520present%2520VIsualization%2520and%2520Segmentation%2520Masked%2520AutoEncoder%250A%2528VIS-MAE%2529%252C%2520novel%2520model%2520weights%2520specifically%2520designed%2520for%2520medical%2520imaging.%250ASpecifically%252C%2520VIS-MAE%2520is%2520trained%2520on%2520a%2520dataset%2520of%25202.5%2520million%2520unlabeled%2520images%250Afrom%2520various%2520modalities%2520%2528CT%252C%2520MR%252C%2520PET%252CX-rays%252C%2520and%2520ultrasound%2529%252C%2520using%250Aself-supervised%2520learning%2520techniques.%2520It%2520is%2520then%2520adapted%2520to%2520classification%2520and%250Asegmentation%2520tasks%2520using%2520explicit%2520labels.%2520VIS-MAE%2520has%2520high%2520label%2520efficiency%252C%250Aoutperforming%2520several%2520benchmark%2520models%2520in%2520both%2520in-domain%2520and%2520out-of-domain%250Aapplications.%2520In%2520addition%252C%2520VIS-MAE%2520has%2520improved%2520label%2520efficiency%2520as%2520it%2520can%250Aachieve%2520similar%2520performance%2520to%2520other%2520models%2520with%2520a%2520reduced%2520amount%2520of%2520labeled%250Atraining%2520data%2520%252850%2525%2520or%252080%2525%2529%2520compared%2520to%2520other%2520pre-trained%2520weights.%2520VIS-MAE%250Arepresents%2520a%2520significant%2520advancement%2520in%2520medical%2520imaging%2520AI%252C%2520offering%2520a%250Ageneralizable%2520and%2520robust%2520solution%2520for%2520improving%2520segmentation%2520and%2520classification%250Atasks%2520while%2520reducing%2520the%2520data%2520annotation%2520workload.%2520The%2520source%2520code%2520of%2520this%2520work%250Ais%2520available%2520at%2520https%253A//github.com/lzl199704/VIS-MAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01034v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIS-MAE%3A%20An%20Efficient%20Self-supervised%20Learning%20Approach%20on%20Medical%20Image%0A%20%20Segmentation%20and%20Classification&entry.906535625=Zelong%20Liu%20and%20Andrew%20Tieu%20and%20Nikhil%20Patel%20and%20Georgios%20Soultanidis%20and%20Louisa%20Deyer%20and%20Ying%20Wang%20and%20Sean%20Huver%20and%20Alexander%20Zhou%20and%20Yunhao%20Mei%20and%20Zahi%20A.%20Fayad%20and%20Timothy%20Deyer%20and%20Xueyan%20Mei&entry.1292438233=%20%20Artificial%20Intelligence%20%28AI%29%20has%20the%20potential%20to%20revolutionize%20diagnosis%20and%0Asegmentation%20in%20medical%20imaging.%20However%2C%20development%20and%20clinical%0Aimplementation%20face%20multiple%20challenges%20including%20limited%20data%20availability%2C%0Alack%20of%20generalizability%2C%20and%20the%20necessity%20to%20incorporate%20multi-modal%20data%0Aeffectively.%20A%20foundation%20model%2C%20which%20is%20a%20large-scale%20pre-trained%20AI%20model%2C%0Aoffers%20a%20versatile%20base%20that%20can%20be%20adapted%20to%20a%20variety%20of%20specific%20tasks%20and%0Acontexts.%20Here%2C%20we%20present%20VIsualization%20and%20Segmentation%20Masked%20AutoEncoder%0A%28VIS-MAE%29%2C%20novel%20model%20weights%20specifically%20designed%20for%20medical%20imaging.%0ASpecifically%2C%20VIS-MAE%20is%20trained%20on%20a%20dataset%20of%202.5%20million%20unlabeled%20images%0Afrom%20various%20modalities%20%28CT%2C%20MR%2C%20PET%2CX-rays%2C%20and%20ultrasound%29%2C%20using%0Aself-supervised%20learning%20techniques.%20It%20is%20then%20adapted%20to%20classification%20and%0Asegmentation%20tasks%20using%20explicit%20labels.%20VIS-MAE%20has%20high%20label%20efficiency%2C%0Aoutperforming%20several%20benchmark%20models%20in%20both%20in-domain%20and%20out-of-domain%0Aapplications.%20In%20addition%2C%20VIS-MAE%20has%20improved%20label%20efficiency%20as%20it%20can%0Aachieve%20similar%20performance%20to%20other%20models%20with%20a%20reduced%20amount%20of%20labeled%0Atraining%20data%20%2850%25%20or%2080%25%29%20compared%20to%20other%20pre-trained%20weights.%20VIS-MAE%0Arepresents%20a%20significant%20advancement%20in%20medical%20imaging%20AI%2C%20offering%20a%0Ageneralizable%20and%20robust%20solution%20for%20improving%20segmentation%20and%20classification%0Atasks%20while%20reducing%20the%20data%20annotation%20workload.%20The%20source%20code%20of%20this%20work%0Ais%20available%20at%20https%3A//github.com/lzl199704/VIS-MAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01034v2&entry.124074799=Read"},
{"title": "MatrixNet: Learning over symmetry groups using learned group\n  representations", "author": "Lucas Laird and Circe Hsu and Asilata Bapat and Robin Walters", "abstract": "  Group theory has been used in machine learning to provide a theoretically\ngrounded approach for incorporating known symmetry transformations in tasks\nfrom robotics to protein modeling. In these applications, equivariant neural\nnetworks use known symmetry groups with predefined representations to learn\nover geometric input data. We propose MatrixNet, a neural network architecture\nthat learns matrix representations of group element inputs instead of using\npredefined representations. MatrixNet achieves higher sample efficiency and\ngeneralization over several standard baselines in prediction tasks over the\nseveral finite groups and the Artin braid group. We also show that MatrixNet\nrespects group relations allowing generalization to group elements of greater\nword length than in the training set.\n", "link": "http://arxiv.org/abs/2501.09571v1", "date": "2025-01-16", "relevancy": 2.2687, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4771}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4426}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MatrixNet%3A%20Learning%20over%20symmetry%20groups%20using%20learned%20group%0A%20%20representations&body=Title%3A%20MatrixNet%3A%20Learning%20over%20symmetry%20groups%20using%20learned%20group%0A%20%20representations%0AAuthor%3A%20Lucas%20Laird%20and%20Circe%20Hsu%20and%20Asilata%20Bapat%20and%20Robin%20Walters%0AAbstract%3A%20%20%20Group%20theory%20has%20been%20used%20in%20machine%20learning%20to%20provide%20a%20theoretically%0Agrounded%20approach%20for%20incorporating%20known%20symmetry%20transformations%20in%20tasks%0Afrom%20robotics%20to%20protein%20modeling.%20In%20these%20applications%2C%20equivariant%20neural%0Anetworks%20use%20known%20symmetry%20groups%20with%20predefined%20representations%20to%20learn%0Aover%20geometric%20input%20data.%20We%20propose%20MatrixNet%2C%20a%20neural%20network%20architecture%0Athat%20learns%20matrix%20representations%20of%20group%20element%20inputs%20instead%20of%20using%0Apredefined%20representations.%20MatrixNet%20achieves%20higher%20sample%20efficiency%20and%0Ageneralization%20over%20several%20standard%20baselines%20in%20prediction%20tasks%20over%20the%0Aseveral%20finite%20groups%20and%20the%20Artin%20braid%20group.%20We%20also%20show%20that%20MatrixNet%0Arespects%20group%20relations%20allowing%20generalization%20to%20group%20elements%20of%20greater%0Aword%20length%20than%20in%20the%20training%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatrixNet%253A%2520Learning%2520over%2520symmetry%2520groups%2520using%2520learned%2520group%250A%2520%2520representations%26entry.906535625%3DLucas%2520Laird%2520and%2520Circe%2520Hsu%2520and%2520Asilata%2520Bapat%2520and%2520Robin%2520Walters%26entry.1292438233%3D%2520%2520Group%2520theory%2520has%2520been%2520used%2520in%2520machine%2520learning%2520to%2520provide%2520a%2520theoretically%250Agrounded%2520approach%2520for%2520incorporating%2520known%2520symmetry%2520transformations%2520in%2520tasks%250Afrom%2520robotics%2520to%2520protein%2520modeling.%2520In%2520these%2520applications%252C%2520equivariant%2520neural%250Anetworks%2520use%2520known%2520symmetry%2520groups%2520with%2520predefined%2520representations%2520to%2520learn%250Aover%2520geometric%2520input%2520data.%2520We%2520propose%2520MatrixNet%252C%2520a%2520neural%2520network%2520architecture%250Athat%2520learns%2520matrix%2520representations%2520of%2520group%2520element%2520inputs%2520instead%2520of%2520using%250Apredefined%2520representations.%2520MatrixNet%2520achieves%2520higher%2520sample%2520efficiency%2520and%250Ageneralization%2520over%2520several%2520standard%2520baselines%2520in%2520prediction%2520tasks%2520over%2520the%250Aseveral%2520finite%2520groups%2520and%2520the%2520Artin%2520braid%2520group.%2520We%2520also%2520show%2520that%2520MatrixNet%250Arespects%2520group%2520relations%2520allowing%2520generalization%2520to%2520group%2520elements%2520of%2520greater%250Aword%2520length%2520than%2520in%2520the%2520training%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MatrixNet%3A%20Learning%20over%20symmetry%20groups%20using%20learned%20group%0A%20%20representations&entry.906535625=Lucas%20Laird%20and%20Circe%20Hsu%20and%20Asilata%20Bapat%20and%20Robin%20Walters&entry.1292438233=%20%20Group%20theory%20has%20been%20used%20in%20machine%20learning%20to%20provide%20a%20theoretically%0Agrounded%20approach%20for%20incorporating%20known%20symmetry%20transformations%20in%20tasks%0Afrom%20robotics%20to%20protein%20modeling.%20In%20these%20applications%2C%20equivariant%20neural%0Anetworks%20use%20known%20symmetry%20groups%20with%20predefined%20representations%20to%20learn%0Aover%20geometric%20input%20data.%20We%20propose%20MatrixNet%2C%20a%20neural%20network%20architecture%0Athat%20learns%20matrix%20representations%20of%20group%20element%20inputs%20instead%20of%20using%0Apredefined%20representations.%20MatrixNet%20achieves%20higher%20sample%20efficiency%20and%0Ageneralization%20over%20several%20standard%20baselines%20in%20prediction%20tasks%20over%20the%0Aseveral%20finite%20groups%20and%20the%20Artin%20braid%20group.%20We%20also%20show%20that%20MatrixNet%0Arespects%20group%20relations%20allowing%20generalization%20to%20group%20elements%20of%20greater%0Aword%20length%20than%20in%20the%20training%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09571v1&entry.124074799=Read"},
{"title": "FutureDepth: Learning to Predict the Future Improves Video Depth\n  Estimation", "author": "Rajeev Yasarla and Manish Kumar Singh and Hong Cai and Yunxiao Shi and Jisoo Jeong and Yinhao Zhu and Shizhong Han and Risheek Garrepalli and Fatih Porikli", "abstract": "  In this paper, we propose a novel video depth estimation approach,\nFutureDepth, which enables the model to implicitly leverage multi-frame and\nmotion cues to improve depth estimation by making it learn to predict the\nfuture at training. More specifically, we propose a future prediction network,\nF-Net, which takes the features of multiple consecutive frames and is trained\nto predict multi-frame features one time step ahead iteratively. In this way,\nF-Net learns the underlying motion and correspondence information, and we\nincorporate its features into the depth decoding process. Additionally, to\nenrich the learning of multiframe correspondence cues, we further leverage a\nreconstruction network, R-Net, which is trained via adaptively masked\nauto-encoding of multiframe feature volumes. At inference time, both F-Net and\nR-Net are used to produce queries to work with the depth decoder, as well as a\nfinal refinement network. Through extensive experiments on several benchmarks,\ni.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and\nopen-domain scenarios, we show that FutureDepth significantly improves upon\nbaseline models, outperforms existing video depth estimation methods, and sets\nnew state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more\nefficient than existing SOTA video depth estimation models and has similar\nlatencies when comparing to monocular models\n", "link": "http://arxiv.org/abs/2403.12953v2", "date": "2025-01-16", "relevancy": 2.2417, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FutureDepth%3A%20Learning%20to%20Predict%20the%20Future%20Improves%20Video%20Depth%0A%20%20Estimation&body=Title%3A%20FutureDepth%3A%20Learning%20to%20Predict%20the%20Future%20Improves%20Video%20Depth%0A%20%20Estimation%0AAuthor%3A%20Rajeev%20Yasarla%20and%20Manish%20Kumar%20Singh%20and%20Hong%20Cai%20and%20Yunxiao%20Shi%20and%20Jisoo%20Jeong%20and%20Yinhao%20Zhu%20and%20Shizhong%20Han%20and%20Risheek%20Garrepalli%20and%20Fatih%20Porikli%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20video%20depth%20estimation%20approach%2C%0AFutureDepth%2C%20which%20enables%20the%20model%20to%20implicitly%20leverage%20multi-frame%20and%0Amotion%20cues%20to%20improve%20depth%20estimation%20by%20making%20it%20learn%20to%20predict%20the%0Afuture%20at%20training.%20More%20specifically%2C%20we%20propose%20a%20future%20prediction%20network%2C%0AF-Net%2C%20which%20takes%20the%20features%20of%20multiple%20consecutive%20frames%20and%20is%20trained%0Ato%20predict%20multi-frame%20features%20one%20time%20step%20ahead%20iteratively.%20In%20this%20way%2C%0AF-Net%20learns%20the%20underlying%20motion%20and%20correspondence%20information%2C%20and%20we%0Aincorporate%20its%20features%20into%20the%20depth%20decoding%20process.%20Additionally%2C%20to%0Aenrich%20the%20learning%20of%20multiframe%20correspondence%20cues%2C%20we%20further%20leverage%20a%0Areconstruction%20network%2C%20R-Net%2C%20which%20is%20trained%20via%20adaptively%20masked%0Aauto-encoding%20of%20multiframe%20feature%20volumes.%20At%20inference%20time%2C%20both%20F-Net%20and%0AR-Net%20are%20used%20to%20produce%20queries%20to%20work%20with%20the%20depth%20decoder%2C%20as%20well%20as%20a%0Afinal%20refinement%20network.%20Through%20extensive%20experiments%20on%20several%20benchmarks%2C%0Ai.e.%2C%20NYUDv2%2C%20KITTI%2C%20DDAD%2C%20and%20Sintel%2C%20which%20cover%20indoor%2C%20driving%2C%20and%0Aopen-domain%20scenarios%2C%20we%20show%20that%20FutureDepth%20significantly%20improves%20upon%0Abaseline%20models%2C%20outperforms%20existing%20video%20depth%20estimation%20methods%2C%20and%20sets%0Anew%20state-of-the-art%20%28SOTA%29%20accuracy.%20Furthermore%2C%20FutureDepth%20is%20more%0Aefficient%20than%20existing%20SOTA%20video%20depth%20estimation%20models%20and%20has%20similar%0Alatencies%20when%20comparing%20to%20monocular%20models%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFutureDepth%253A%2520Learning%2520to%2520Predict%2520the%2520Future%2520Improves%2520Video%2520Depth%250A%2520%2520Estimation%26entry.906535625%3DRajeev%2520Yasarla%2520and%2520Manish%2520Kumar%2520Singh%2520and%2520Hong%2520Cai%2520and%2520Yunxiao%2520Shi%2520and%2520Jisoo%2520Jeong%2520and%2520Yinhao%2520Zhu%2520and%2520Shizhong%2520Han%2520and%2520Risheek%2520Garrepalli%2520and%2520Fatih%2520Porikli%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520video%2520depth%2520estimation%2520approach%252C%250AFutureDepth%252C%2520which%2520enables%2520the%2520model%2520to%2520implicitly%2520leverage%2520multi-frame%2520and%250Amotion%2520cues%2520to%2520improve%2520depth%2520estimation%2520by%2520making%2520it%2520learn%2520to%2520predict%2520the%250Afuture%2520at%2520training.%2520More%2520specifically%252C%2520we%2520propose%2520a%2520future%2520prediction%2520network%252C%250AF-Net%252C%2520which%2520takes%2520the%2520features%2520of%2520multiple%2520consecutive%2520frames%2520and%2520is%2520trained%250Ato%2520predict%2520multi-frame%2520features%2520one%2520time%2520step%2520ahead%2520iteratively.%2520In%2520this%2520way%252C%250AF-Net%2520learns%2520the%2520underlying%2520motion%2520and%2520correspondence%2520information%252C%2520and%2520we%250Aincorporate%2520its%2520features%2520into%2520the%2520depth%2520decoding%2520process.%2520Additionally%252C%2520to%250Aenrich%2520the%2520learning%2520of%2520multiframe%2520correspondence%2520cues%252C%2520we%2520further%2520leverage%2520a%250Areconstruction%2520network%252C%2520R-Net%252C%2520which%2520is%2520trained%2520via%2520adaptively%2520masked%250Aauto-encoding%2520of%2520multiframe%2520feature%2520volumes.%2520At%2520inference%2520time%252C%2520both%2520F-Net%2520and%250AR-Net%2520are%2520used%2520to%2520produce%2520queries%2520to%2520work%2520with%2520the%2520depth%2520decoder%252C%2520as%2520well%2520as%2520a%250Afinal%2520refinement%2520network.%2520Through%2520extensive%2520experiments%2520on%2520several%2520benchmarks%252C%250Ai.e.%252C%2520NYUDv2%252C%2520KITTI%252C%2520DDAD%252C%2520and%2520Sintel%252C%2520which%2520cover%2520indoor%252C%2520driving%252C%2520and%250Aopen-domain%2520scenarios%252C%2520we%2520show%2520that%2520FutureDepth%2520significantly%2520improves%2520upon%250Abaseline%2520models%252C%2520outperforms%2520existing%2520video%2520depth%2520estimation%2520methods%252C%2520and%2520sets%250Anew%2520state-of-the-art%2520%2528SOTA%2529%2520accuracy.%2520Furthermore%252C%2520FutureDepth%2520is%2520more%250Aefficient%2520than%2520existing%2520SOTA%2520video%2520depth%2520estimation%2520models%2520and%2520has%2520similar%250Alatencies%2520when%2520comparing%2520to%2520monocular%2520models%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FutureDepth%3A%20Learning%20to%20Predict%20the%20Future%20Improves%20Video%20Depth%0A%20%20Estimation&entry.906535625=Rajeev%20Yasarla%20and%20Manish%20Kumar%20Singh%20and%20Hong%20Cai%20and%20Yunxiao%20Shi%20and%20Jisoo%20Jeong%20and%20Yinhao%20Zhu%20and%20Shizhong%20Han%20and%20Risheek%20Garrepalli%20and%20Fatih%20Porikli&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20video%20depth%20estimation%20approach%2C%0AFutureDepth%2C%20which%20enables%20the%20model%20to%20implicitly%20leverage%20multi-frame%20and%0Amotion%20cues%20to%20improve%20depth%20estimation%20by%20making%20it%20learn%20to%20predict%20the%0Afuture%20at%20training.%20More%20specifically%2C%20we%20propose%20a%20future%20prediction%20network%2C%0AF-Net%2C%20which%20takes%20the%20features%20of%20multiple%20consecutive%20frames%20and%20is%20trained%0Ato%20predict%20multi-frame%20features%20one%20time%20step%20ahead%20iteratively.%20In%20this%20way%2C%0AF-Net%20learns%20the%20underlying%20motion%20and%20correspondence%20information%2C%20and%20we%0Aincorporate%20its%20features%20into%20the%20depth%20decoding%20process.%20Additionally%2C%20to%0Aenrich%20the%20learning%20of%20multiframe%20correspondence%20cues%2C%20we%20further%20leverage%20a%0Areconstruction%20network%2C%20R-Net%2C%20which%20is%20trained%20via%20adaptively%20masked%0Aauto-encoding%20of%20multiframe%20feature%20volumes.%20At%20inference%20time%2C%20both%20F-Net%20and%0AR-Net%20are%20used%20to%20produce%20queries%20to%20work%20with%20the%20depth%20decoder%2C%20as%20well%20as%20a%0Afinal%20refinement%20network.%20Through%20extensive%20experiments%20on%20several%20benchmarks%2C%0Ai.e.%2C%20NYUDv2%2C%20KITTI%2C%20DDAD%2C%20and%20Sintel%2C%20which%20cover%20indoor%2C%20driving%2C%20and%0Aopen-domain%20scenarios%2C%20we%20show%20that%20FutureDepth%20significantly%20improves%20upon%0Abaseline%20models%2C%20outperforms%20existing%20video%20depth%20estimation%20methods%2C%20and%20sets%0Anew%20state-of-the-art%20%28SOTA%29%20accuracy.%20Furthermore%2C%20FutureDepth%20is%20more%0Aefficient%20than%20existing%20SOTA%20video%20depth%20estimation%20models%20and%20has%20similar%0Alatencies%20when%20comparing%20to%20monocular%20models%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12953v2&entry.124074799=Read"},
{"title": "Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling\n  for Multimodal Emotion Analysis", "author": "Qize Yang and Detao Bai and Yi-Xing Peng and Xihan Wei", "abstract": "  Understanding emotions accurately is essential for fields like human-computer\ninteraction. Due to the complexity of emotions and their multi-modal nature\n(e.g., emotions are influenced by facial expressions and audio), researchers\nhave turned to using multi-modal models to understand human emotions rather\nthan single-modality. However, current video multi-modal large language models\n(MLLMs) encounter difficulties in effectively integrating audio and identifying\nsubtle facial micro-expressions. Furthermore, the lack of detailed emotion\nanalysis datasets also limits the development of multimodal emotion analysis.\nTo address these issues, we introduce a self-reviewed dataset and a\nhuman-reviewed dataset, comprising 24,137 coarse-grained samples and 3,500\nmanually annotated samples with detailed emotion annotations, respectively.\nThese datasets allow models to learn from diverse scenarios and better\ngeneralize to real-world applications. Moreover, in addition to the audio\nmodeling, we propose to explicitly integrate facial encoding models into the\nexisting advanced Video MLLM, enabling the MLLM to effectively unify audio and\nthe subtle facial cues for emotion understanding. By aligning these features\nwithin a unified space and employing instruction tuning in our proposed\ndatasets, our Omni-Emotion achieves state-of-the-art performance in both\nemotion recognition and reasoning tasks.\n", "link": "http://arxiv.org/abs/2501.09502v1", "date": "2025-01-16", "relevancy": 2.2383, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5609}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-Emotion%3A%20Extending%20Video%20MLLM%20with%20Detailed%20Face%20and%20Audio%20Modeling%0A%20%20for%20Multimodal%20Emotion%20Analysis&body=Title%3A%20Omni-Emotion%3A%20Extending%20Video%20MLLM%20with%20Detailed%20Face%20and%20Audio%20Modeling%0A%20%20for%20Multimodal%20Emotion%20Analysis%0AAuthor%3A%20Qize%20Yang%20and%20Detao%20Bai%20and%20Yi-Xing%20Peng%20and%20Xihan%20Wei%0AAbstract%3A%20%20%20Understanding%20emotions%20accurately%20is%20essential%20for%20fields%20like%20human-computer%0Ainteraction.%20Due%20to%20the%20complexity%20of%20emotions%20and%20their%20multi-modal%20nature%0A%28e.g.%2C%20emotions%20are%20influenced%20by%20facial%20expressions%20and%20audio%29%2C%20researchers%0Ahave%20turned%20to%20using%20multi-modal%20models%20to%20understand%20human%20emotions%20rather%0Athan%20single-modality.%20However%2C%20current%20video%20multi-modal%20large%20language%20models%0A%28MLLMs%29%20encounter%20difficulties%20in%20effectively%20integrating%20audio%20and%20identifying%0Asubtle%20facial%20micro-expressions.%20Furthermore%2C%20the%20lack%20of%20detailed%20emotion%0Aanalysis%20datasets%20also%20limits%20the%20development%20of%20multimodal%20emotion%20analysis.%0ATo%20address%20these%20issues%2C%20we%20introduce%20a%20self-reviewed%20dataset%20and%20a%0Ahuman-reviewed%20dataset%2C%20comprising%2024%2C137%20coarse-grained%20samples%20and%203%2C500%0Amanually%20annotated%20samples%20with%20detailed%20emotion%20annotations%2C%20respectively.%0AThese%20datasets%20allow%20models%20to%20learn%20from%20diverse%20scenarios%20and%20better%0Ageneralize%20to%20real-world%20applications.%20Moreover%2C%20in%20addition%20to%20the%20audio%0Amodeling%2C%20we%20propose%20to%20explicitly%20integrate%20facial%20encoding%20models%20into%20the%0Aexisting%20advanced%20Video%20MLLM%2C%20enabling%20the%20MLLM%20to%20effectively%20unify%20audio%20and%0Athe%20subtle%20facial%20cues%20for%20emotion%20understanding.%20By%20aligning%20these%20features%0Awithin%20a%20unified%20space%20and%20employing%20instruction%20tuning%20in%20our%20proposed%0Adatasets%2C%20our%20Omni-Emotion%20achieves%20state-of-the-art%20performance%20in%20both%0Aemotion%20recognition%20and%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-Emotion%253A%2520Extending%2520Video%2520MLLM%2520with%2520Detailed%2520Face%2520and%2520Audio%2520Modeling%250A%2520%2520for%2520Multimodal%2520Emotion%2520Analysis%26entry.906535625%3DQize%2520Yang%2520and%2520Detao%2520Bai%2520and%2520Yi-Xing%2520Peng%2520and%2520Xihan%2520Wei%26entry.1292438233%3D%2520%2520Understanding%2520emotions%2520accurately%2520is%2520essential%2520for%2520fields%2520like%2520human-computer%250Ainteraction.%2520Due%2520to%2520the%2520complexity%2520of%2520emotions%2520and%2520their%2520multi-modal%2520nature%250A%2528e.g.%252C%2520emotions%2520are%2520influenced%2520by%2520facial%2520expressions%2520and%2520audio%2529%252C%2520researchers%250Ahave%2520turned%2520to%2520using%2520multi-modal%2520models%2520to%2520understand%2520human%2520emotions%2520rather%250Athan%2520single-modality.%2520However%252C%2520current%2520video%2520multi-modal%2520large%2520language%2520models%250A%2528MLLMs%2529%2520encounter%2520difficulties%2520in%2520effectively%2520integrating%2520audio%2520and%2520identifying%250Asubtle%2520facial%2520micro-expressions.%2520Furthermore%252C%2520the%2520lack%2520of%2520detailed%2520emotion%250Aanalysis%2520datasets%2520also%2520limits%2520the%2520development%2520of%2520multimodal%2520emotion%2520analysis.%250ATo%2520address%2520these%2520issues%252C%2520we%2520introduce%2520a%2520self-reviewed%2520dataset%2520and%2520a%250Ahuman-reviewed%2520dataset%252C%2520comprising%252024%252C137%2520coarse-grained%2520samples%2520and%25203%252C500%250Amanually%2520annotated%2520samples%2520with%2520detailed%2520emotion%2520annotations%252C%2520respectively.%250AThese%2520datasets%2520allow%2520models%2520to%2520learn%2520from%2520diverse%2520scenarios%2520and%2520better%250Ageneralize%2520to%2520real-world%2520applications.%2520Moreover%252C%2520in%2520addition%2520to%2520the%2520audio%250Amodeling%252C%2520we%2520propose%2520to%2520explicitly%2520integrate%2520facial%2520encoding%2520models%2520into%2520the%250Aexisting%2520advanced%2520Video%2520MLLM%252C%2520enabling%2520the%2520MLLM%2520to%2520effectively%2520unify%2520audio%2520and%250Athe%2520subtle%2520facial%2520cues%2520for%2520emotion%2520understanding.%2520By%2520aligning%2520these%2520features%250Awithin%2520a%2520unified%2520space%2520and%2520employing%2520instruction%2520tuning%2520in%2520our%2520proposed%250Adatasets%252C%2520our%2520Omni-Emotion%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%250Aemotion%2520recognition%2520and%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-Emotion%3A%20Extending%20Video%20MLLM%20with%20Detailed%20Face%20and%20Audio%20Modeling%0A%20%20for%20Multimodal%20Emotion%20Analysis&entry.906535625=Qize%20Yang%20and%20Detao%20Bai%20and%20Yi-Xing%20Peng%20and%20Xihan%20Wei&entry.1292438233=%20%20Understanding%20emotions%20accurately%20is%20essential%20for%20fields%20like%20human-computer%0Ainteraction.%20Due%20to%20the%20complexity%20of%20emotions%20and%20their%20multi-modal%20nature%0A%28e.g.%2C%20emotions%20are%20influenced%20by%20facial%20expressions%20and%20audio%29%2C%20researchers%0Ahave%20turned%20to%20using%20multi-modal%20models%20to%20understand%20human%20emotions%20rather%0Athan%20single-modality.%20However%2C%20current%20video%20multi-modal%20large%20language%20models%0A%28MLLMs%29%20encounter%20difficulties%20in%20effectively%20integrating%20audio%20and%20identifying%0Asubtle%20facial%20micro-expressions.%20Furthermore%2C%20the%20lack%20of%20detailed%20emotion%0Aanalysis%20datasets%20also%20limits%20the%20development%20of%20multimodal%20emotion%20analysis.%0ATo%20address%20these%20issues%2C%20we%20introduce%20a%20self-reviewed%20dataset%20and%20a%0Ahuman-reviewed%20dataset%2C%20comprising%2024%2C137%20coarse-grained%20samples%20and%203%2C500%0Amanually%20annotated%20samples%20with%20detailed%20emotion%20annotations%2C%20respectively.%0AThese%20datasets%20allow%20models%20to%20learn%20from%20diverse%20scenarios%20and%20better%0Ageneralize%20to%20real-world%20applications.%20Moreover%2C%20in%20addition%20to%20the%20audio%0Amodeling%2C%20we%20propose%20to%20explicitly%20integrate%20facial%20encoding%20models%20into%20the%0Aexisting%20advanced%20Video%20MLLM%2C%20enabling%20the%20MLLM%20to%20effectively%20unify%20audio%20and%0Athe%20subtle%20facial%20cues%20for%20emotion%20understanding.%20By%20aligning%20these%20features%0Awithin%20a%20unified%20space%20and%20employing%20instruction%20tuning%20in%20our%20proposed%0Adatasets%2C%20our%20Omni-Emotion%20achieves%20state-of-the-art%20performance%20in%20both%0Aemotion%20recognition%20and%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09502v1&entry.124074799=Read"},
{"title": "Safe Control and Learning Using the Generalized Action Governor", "author": "Nan Li and Yutong Li and Ilya Kolmanovsky and Anouck Girard and H. Eric Tseng and Dimitar Filev", "abstract": "  This article introduces a general framework for safe control and learning\nbased on the generalized action governor (AG). The AG is a supervisory scheme\nfor augmenting a nominal closed-loop system with the ability of strictly\nhandling prescribed safety constraints. In the first part of this article, we\npresent a generalized AG methodology and analyze its key properties in a\ngeneral setting. Then, we introduce tailored AG design approaches derived from\nthe generalized methodology for linear and discrete systems. Afterward, we\ndiscuss the application of the generalized AG to facilitate safe online\nlearning, which aims at safely evolving control parameters using real-time data\nto enhance control performance in uncertain systems. We present two safe\nlearning algorithms based on, respectively, reinforcement learning and\ndata-driven Koopman operator-based control integrated with the generalized AG\nto exemplify this application. Finally, we illustrate the developments with a\nnumerical example.\n", "link": "http://arxiv.org/abs/2211.12628v2", "date": "2025-01-16", "relevancy": 2.237, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.643}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5216}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Control%20and%20Learning%20Using%20the%20Generalized%20Action%20Governor&body=Title%3A%20Safe%20Control%20and%20Learning%20Using%20the%20Generalized%20Action%20Governor%0AAuthor%3A%20Nan%20Li%20and%20Yutong%20Li%20and%20Ilya%20Kolmanovsky%20and%20Anouck%20Girard%20and%20H.%20Eric%20Tseng%20and%20Dimitar%20Filev%0AAbstract%3A%20%20%20This%20article%20introduces%20a%20general%20framework%20for%20safe%20control%20and%20learning%0Abased%20on%20the%20generalized%20action%20governor%20%28AG%29.%20The%20AG%20is%20a%20supervisory%20scheme%0Afor%20augmenting%20a%20nominal%20closed-loop%20system%20with%20the%20ability%20of%20strictly%0Ahandling%20prescribed%20safety%20constraints.%20In%20the%20first%20part%20of%20this%20article%2C%20we%0Apresent%20a%20generalized%20AG%20methodology%20and%20analyze%20its%20key%20properties%20in%20a%0Ageneral%20setting.%20Then%2C%20we%20introduce%20tailored%20AG%20design%20approaches%20derived%20from%0Athe%20generalized%20methodology%20for%20linear%20and%20discrete%20systems.%20Afterward%2C%20we%0Adiscuss%20the%20application%20of%20the%20generalized%20AG%20to%20facilitate%20safe%20online%0Alearning%2C%20which%20aims%20at%20safely%20evolving%20control%20parameters%20using%20real-time%20data%0Ato%20enhance%20control%20performance%20in%20uncertain%20systems.%20We%20present%20two%20safe%0Alearning%20algorithms%20based%20on%2C%20respectively%2C%20reinforcement%20learning%20and%0Adata-driven%20Koopman%20operator-based%20control%20integrated%20with%20the%20generalized%20AG%0Ato%20exemplify%20this%20application.%20Finally%2C%20we%20illustrate%20the%20developments%20with%20a%0Anumerical%20example.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.12628v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Control%2520and%2520Learning%2520Using%2520the%2520Generalized%2520Action%2520Governor%26entry.906535625%3DNan%2520Li%2520and%2520Yutong%2520Li%2520and%2520Ilya%2520Kolmanovsky%2520and%2520Anouck%2520Girard%2520and%2520H.%2520Eric%2520Tseng%2520and%2520Dimitar%2520Filev%26entry.1292438233%3D%2520%2520This%2520article%2520introduces%2520a%2520general%2520framework%2520for%2520safe%2520control%2520and%2520learning%250Abased%2520on%2520the%2520generalized%2520action%2520governor%2520%2528AG%2529.%2520The%2520AG%2520is%2520a%2520supervisory%2520scheme%250Afor%2520augmenting%2520a%2520nominal%2520closed-loop%2520system%2520with%2520the%2520ability%2520of%2520strictly%250Ahandling%2520prescribed%2520safety%2520constraints.%2520In%2520the%2520first%2520part%2520of%2520this%2520article%252C%2520we%250Apresent%2520a%2520generalized%2520AG%2520methodology%2520and%2520analyze%2520its%2520key%2520properties%2520in%2520a%250Ageneral%2520setting.%2520Then%252C%2520we%2520introduce%2520tailored%2520AG%2520design%2520approaches%2520derived%2520from%250Athe%2520generalized%2520methodology%2520for%2520linear%2520and%2520discrete%2520systems.%2520Afterward%252C%2520we%250Adiscuss%2520the%2520application%2520of%2520the%2520generalized%2520AG%2520to%2520facilitate%2520safe%2520online%250Alearning%252C%2520which%2520aims%2520at%2520safely%2520evolving%2520control%2520parameters%2520using%2520real-time%2520data%250Ato%2520enhance%2520control%2520performance%2520in%2520uncertain%2520systems.%2520We%2520present%2520two%2520safe%250Alearning%2520algorithms%2520based%2520on%252C%2520respectively%252C%2520reinforcement%2520learning%2520and%250Adata-driven%2520Koopman%2520operator-based%2520control%2520integrated%2520with%2520the%2520generalized%2520AG%250Ato%2520exemplify%2520this%2520application.%2520Finally%252C%2520we%2520illustrate%2520the%2520developments%2520with%2520a%250Anumerical%2520example.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.12628v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Control%20and%20Learning%20Using%20the%20Generalized%20Action%20Governor&entry.906535625=Nan%20Li%20and%20Yutong%20Li%20and%20Ilya%20Kolmanovsky%20and%20Anouck%20Girard%20and%20H.%20Eric%20Tseng%20and%20Dimitar%20Filev&entry.1292438233=%20%20This%20article%20introduces%20a%20general%20framework%20for%20safe%20control%20and%20learning%0Abased%20on%20the%20generalized%20action%20governor%20%28AG%29.%20The%20AG%20is%20a%20supervisory%20scheme%0Afor%20augmenting%20a%20nominal%20closed-loop%20system%20with%20the%20ability%20of%20strictly%0Ahandling%20prescribed%20safety%20constraints.%20In%20the%20first%20part%20of%20this%20article%2C%20we%0Apresent%20a%20generalized%20AG%20methodology%20and%20analyze%20its%20key%20properties%20in%20a%0Ageneral%20setting.%20Then%2C%20we%20introduce%20tailored%20AG%20design%20approaches%20derived%20from%0Athe%20generalized%20methodology%20for%20linear%20and%20discrete%20systems.%20Afterward%2C%20we%0Adiscuss%20the%20application%20of%20the%20generalized%20AG%20to%20facilitate%20safe%20online%0Alearning%2C%20which%20aims%20at%20safely%20evolving%20control%20parameters%20using%20real-time%20data%0Ato%20enhance%20control%20performance%20in%20uncertain%20systems.%20We%20present%20two%20safe%0Alearning%20algorithms%20based%20on%2C%20respectively%2C%20reinforcement%20learning%20and%0Adata-driven%20Koopman%20operator-based%20control%20integrated%20with%20the%20generalized%20AG%0Ato%20exemplify%20this%20application.%20Finally%2C%20we%20illustrate%20the%20developments%20with%20a%0Anumerical%20example.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.12628v2&entry.124074799=Read"},
{"title": "DriveLM: Driving with Graph Visual Question Answering", "author": "Chonghao Sima and Katrin Renz and Kashyap Chitta and Li Chen and Hanxue Zhang and Chengen Xie and Jens Bei\u00dfwenger and Ping Luo and Andreas Geiger and Hongyang Li", "abstract": "  We study how vision-language models (VLMs) trained on web-scale data can be\nintegrated into end-to-end driving systems to boost generalization and enable\ninteractivity with human users. While recent approaches adapt VLMs to driving\nvia single-round visual question answering (VQA), human drivers reason about\ndecisions in multiple steps. Starting from the localization of key objects,\nhumans estimate object interactions before taking actions. The key insight is\nthat with our proposed task, Graph VQA, where we model graph-structured\nreasoning through perception, prediction and planning question-answer pairs, we\nobtain a suitable proxy task to mimic the human reasoning process. We\ninstantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose\na VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA\nand end-to-end driving. The experiments demonstrate that Graph VQA provides a\nsimple, principled framework for reasoning about a driving scene, and\nDriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent\nbaseline performs end-to-end autonomous driving competitively in comparison to\nstate-of-the-art driving-specific architectures. Notably, its benefits are\npronounced when it is evaluated zero-shot on unseen objects or sensor\nconfigurations. We hope this work can be the starting point to shed new light\non how to apply VLMs for autonomous driving. To facilitate future research, all\ncode, data, and models are available to the public.\n", "link": "http://arxiv.org/abs/2312.14150v3", "date": "2025-01-16", "relevancy": 2.234, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5636}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5636}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveLM%3A%20Driving%20with%20Graph%20Visual%20Question%20Answering&body=Title%3A%20DriveLM%3A%20Driving%20with%20Graph%20Visual%20Question%20Answering%0AAuthor%3A%20Chonghao%20Sima%20and%20Katrin%20Renz%20and%20Kashyap%20Chitta%20and%20Li%20Chen%20and%20Hanxue%20Zhang%20and%20Chengen%20Xie%20and%20Jens%20Bei%C3%9Fwenger%20and%20Ping%20Luo%20and%20Andreas%20Geiger%20and%20Hongyang%20Li%0AAbstract%3A%20%20%20We%20study%20how%20vision-language%20models%20%28VLMs%29%20trained%20on%20web-scale%20data%20can%20be%0Aintegrated%20into%20end-to-end%20driving%20systems%20to%20boost%20generalization%20and%20enable%0Ainteractivity%20with%20human%20users.%20While%20recent%20approaches%20adapt%20VLMs%20to%20driving%0Avia%20single-round%20visual%20question%20answering%20%28VQA%29%2C%20human%20drivers%20reason%20about%0Adecisions%20in%20multiple%20steps.%20Starting%20from%20the%20localization%20of%20key%20objects%2C%0Ahumans%20estimate%20object%20interactions%20before%20taking%20actions.%20The%20key%20insight%20is%0Athat%20with%20our%20proposed%20task%2C%20Graph%20VQA%2C%20where%20we%20model%20graph-structured%0Areasoning%20through%20perception%2C%20prediction%20and%20planning%20question-answer%20pairs%2C%20we%0Aobtain%20a%20suitable%20proxy%20task%20to%20mimic%20the%20human%20reasoning%20process.%20We%0Ainstantiate%20datasets%20%28DriveLM-Data%29%20built%20upon%20nuScenes%20and%20CARLA%2C%20and%20propose%0Aa%20VLM-based%20baseline%20approach%20%28DriveLM-Agent%29%20for%20jointly%20performing%20Graph%20VQA%0Aand%20end-to-end%20driving.%20The%20experiments%20demonstrate%20that%20Graph%20VQA%20provides%20a%0Asimple%2C%20principled%20framework%20for%20reasoning%20about%20a%20driving%20scene%2C%20and%0ADriveLM-Data%20provides%20a%20challenging%20benchmark%20for%20this%20task.%20Our%20DriveLM-Agent%0Abaseline%20performs%20end-to-end%20autonomous%20driving%20competitively%20in%20comparison%20to%0Astate-of-the-art%20driving-specific%20architectures.%20Notably%2C%20its%20benefits%20are%0Apronounced%20when%20it%20is%20evaluated%20zero-shot%20on%20unseen%20objects%20or%20sensor%0Aconfigurations.%20We%20hope%20this%20work%20can%20be%20the%20starting%20point%20to%20shed%20new%20light%0Aon%20how%20to%20apply%20VLMs%20for%20autonomous%20driving.%20To%20facilitate%20future%20research%2C%20all%0Acode%2C%20data%2C%20and%20models%20are%20available%20to%20the%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14150v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveLM%253A%2520Driving%2520with%2520Graph%2520Visual%2520Question%2520Answering%26entry.906535625%3DChonghao%2520Sima%2520and%2520Katrin%2520Renz%2520and%2520Kashyap%2520Chitta%2520and%2520Li%2520Chen%2520and%2520Hanxue%2520Zhang%2520and%2520Chengen%2520Xie%2520and%2520Jens%2520Bei%25C3%259Fwenger%2520and%2520Ping%2520Luo%2520and%2520Andreas%2520Geiger%2520and%2520Hongyang%2520Li%26entry.1292438233%3D%2520%2520We%2520study%2520how%2520vision-language%2520models%2520%2528VLMs%2529%2520trained%2520on%2520web-scale%2520data%2520can%2520be%250Aintegrated%2520into%2520end-to-end%2520driving%2520systems%2520to%2520boost%2520generalization%2520and%2520enable%250Ainteractivity%2520with%2520human%2520users.%2520While%2520recent%2520approaches%2520adapt%2520VLMs%2520to%2520driving%250Avia%2520single-round%2520visual%2520question%2520answering%2520%2528VQA%2529%252C%2520human%2520drivers%2520reason%2520about%250Adecisions%2520in%2520multiple%2520steps.%2520Starting%2520from%2520the%2520localization%2520of%2520key%2520objects%252C%250Ahumans%2520estimate%2520object%2520interactions%2520before%2520taking%2520actions.%2520The%2520key%2520insight%2520is%250Athat%2520with%2520our%2520proposed%2520task%252C%2520Graph%2520VQA%252C%2520where%2520we%2520model%2520graph-structured%250Areasoning%2520through%2520perception%252C%2520prediction%2520and%2520planning%2520question-answer%2520pairs%252C%2520we%250Aobtain%2520a%2520suitable%2520proxy%2520task%2520to%2520mimic%2520the%2520human%2520reasoning%2520process.%2520We%250Ainstantiate%2520datasets%2520%2528DriveLM-Data%2529%2520built%2520upon%2520nuScenes%2520and%2520CARLA%252C%2520and%2520propose%250Aa%2520VLM-based%2520baseline%2520approach%2520%2528DriveLM-Agent%2529%2520for%2520jointly%2520performing%2520Graph%2520VQA%250Aand%2520end-to-end%2520driving.%2520The%2520experiments%2520demonstrate%2520that%2520Graph%2520VQA%2520provides%2520a%250Asimple%252C%2520principled%2520framework%2520for%2520reasoning%2520about%2520a%2520driving%2520scene%252C%2520and%250ADriveLM-Data%2520provides%2520a%2520challenging%2520benchmark%2520for%2520this%2520task.%2520Our%2520DriveLM-Agent%250Abaseline%2520performs%2520end-to-end%2520autonomous%2520driving%2520competitively%2520in%2520comparison%2520to%250Astate-of-the-art%2520driving-specific%2520architectures.%2520Notably%252C%2520its%2520benefits%2520are%250Apronounced%2520when%2520it%2520is%2520evaluated%2520zero-shot%2520on%2520unseen%2520objects%2520or%2520sensor%250Aconfigurations.%2520We%2520hope%2520this%2520work%2520can%2520be%2520the%2520starting%2520point%2520to%2520shed%2520new%2520light%250Aon%2520how%2520to%2520apply%2520VLMs%2520for%2520autonomous%2520driving.%2520To%2520facilitate%2520future%2520research%252C%2520all%250Acode%252C%2520data%252C%2520and%2520models%2520are%2520available%2520to%2520the%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14150v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveLM%3A%20Driving%20with%20Graph%20Visual%20Question%20Answering&entry.906535625=Chonghao%20Sima%20and%20Katrin%20Renz%20and%20Kashyap%20Chitta%20and%20Li%20Chen%20and%20Hanxue%20Zhang%20and%20Chengen%20Xie%20and%20Jens%20Bei%C3%9Fwenger%20and%20Ping%20Luo%20and%20Andreas%20Geiger%20and%20Hongyang%20Li&entry.1292438233=%20%20We%20study%20how%20vision-language%20models%20%28VLMs%29%20trained%20on%20web-scale%20data%20can%20be%0Aintegrated%20into%20end-to-end%20driving%20systems%20to%20boost%20generalization%20and%20enable%0Ainteractivity%20with%20human%20users.%20While%20recent%20approaches%20adapt%20VLMs%20to%20driving%0Avia%20single-round%20visual%20question%20answering%20%28VQA%29%2C%20human%20drivers%20reason%20about%0Adecisions%20in%20multiple%20steps.%20Starting%20from%20the%20localization%20of%20key%20objects%2C%0Ahumans%20estimate%20object%20interactions%20before%20taking%20actions.%20The%20key%20insight%20is%0Athat%20with%20our%20proposed%20task%2C%20Graph%20VQA%2C%20where%20we%20model%20graph-structured%0Areasoning%20through%20perception%2C%20prediction%20and%20planning%20question-answer%20pairs%2C%20we%0Aobtain%20a%20suitable%20proxy%20task%20to%20mimic%20the%20human%20reasoning%20process.%20We%0Ainstantiate%20datasets%20%28DriveLM-Data%29%20built%20upon%20nuScenes%20and%20CARLA%2C%20and%20propose%0Aa%20VLM-based%20baseline%20approach%20%28DriveLM-Agent%29%20for%20jointly%20performing%20Graph%20VQA%0Aand%20end-to-end%20driving.%20The%20experiments%20demonstrate%20that%20Graph%20VQA%20provides%20a%0Asimple%2C%20principled%20framework%20for%20reasoning%20about%20a%20driving%20scene%2C%20and%0ADriveLM-Data%20provides%20a%20challenging%20benchmark%20for%20this%20task.%20Our%20DriveLM-Agent%0Abaseline%20performs%20end-to-end%20autonomous%20driving%20competitively%20in%20comparison%20to%0Astate-of-the-art%20driving-specific%20architectures.%20Notably%2C%20its%20benefits%20are%0Apronounced%20when%20it%20is%20evaluated%20zero-shot%20on%20unseen%20objects%20or%20sensor%0Aconfigurations.%20We%20hope%20this%20work%20can%20be%20the%20starting%20point%20to%20shed%20new%20light%0Aon%20how%20to%20apply%20VLMs%20for%20autonomous%20driving.%20To%20facilitate%20future%20research%2C%20all%0Acode%2C%20data%2C%20and%20models%20are%20available%20to%20the%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14150v3&entry.124074799=Read"},
{"title": "Towards Spectral Convergence of Locally Linear Embedding on Manifolds\n  with Boundary", "author": "Andrew Lyons", "abstract": "  We study the eigenvalues and eigenfunctions of a differential operator that\ngoverns the asymptotic behavior of the unsupervised learning algorithm known as\nLocally Linear Embedding when a large data set is sampled from an interval or\ndisc. In particular, the differential operator is of second order, mixed-type,\nand degenerates near the boundary. We show that a natural regularity condition\non the eigenfunctions imposes a consistent boundary condition and use the\nFrobenius method to estimate pointwise behavior. We then determine the limiting\nsequence of eigenvalues analytically and compare them to numerical predictions.\nFinally, we propose a variational framework for determining eigenvalues on\nother compact manifolds.\n", "link": "http://arxiv.org/abs/2501.09572v1", "date": "2025-01-16", "relevancy": 2.2271, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4538}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4441}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Spectral%20Convergence%20of%20Locally%20Linear%20Embedding%20on%20Manifolds%0A%20%20with%20Boundary&body=Title%3A%20Towards%20Spectral%20Convergence%20of%20Locally%20Linear%20Embedding%20on%20Manifolds%0A%20%20with%20Boundary%0AAuthor%3A%20Andrew%20Lyons%0AAbstract%3A%20%20%20We%20study%20the%20eigenvalues%20and%20eigenfunctions%20of%20a%20differential%20operator%20that%0Agoverns%20the%20asymptotic%20behavior%20of%20the%20unsupervised%20learning%20algorithm%20known%20as%0ALocally%20Linear%20Embedding%20when%20a%20large%20data%20set%20is%20sampled%20from%20an%20interval%20or%0Adisc.%20In%20particular%2C%20the%20differential%20operator%20is%20of%20second%20order%2C%20mixed-type%2C%0Aand%20degenerates%20near%20the%20boundary.%20We%20show%20that%20a%20natural%20regularity%20condition%0Aon%20the%20eigenfunctions%20imposes%20a%20consistent%20boundary%20condition%20and%20use%20the%0AFrobenius%20method%20to%20estimate%20pointwise%20behavior.%20We%20then%20determine%20the%20limiting%0Asequence%20of%20eigenvalues%20analytically%20and%20compare%20them%20to%20numerical%20predictions.%0AFinally%2C%20we%20propose%20a%20variational%20framework%20for%20determining%20eigenvalues%20on%0Aother%20compact%20manifolds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09572v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Spectral%2520Convergence%2520of%2520Locally%2520Linear%2520Embedding%2520on%2520Manifolds%250A%2520%2520with%2520Boundary%26entry.906535625%3DAndrew%2520Lyons%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520eigenvalues%2520and%2520eigenfunctions%2520of%2520a%2520differential%2520operator%2520that%250Agoverns%2520the%2520asymptotic%2520behavior%2520of%2520the%2520unsupervised%2520learning%2520algorithm%2520known%2520as%250ALocally%2520Linear%2520Embedding%2520when%2520a%2520large%2520data%2520set%2520is%2520sampled%2520from%2520an%2520interval%2520or%250Adisc.%2520In%2520particular%252C%2520the%2520differential%2520operator%2520is%2520of%2520second%2520order%252C%2520mixed-type%252C%250Aand%2520degenerates%2520near%2520the%2520boundary.%2520We%2520show%2520that%2520a%2520natural%2520regularity%2520condition%250Aon%2520the%2520eigenfunctions%2520imposes%2520a%2520consistent%2520boundary%2520condition%2520and%2520use%2520the%250AFrobenius%2520method%2520to%2520estimate%2520pointwise%2520behavior.%2520We%2520then%2520determine%2520the%2520limiting%250Asequence%2520of%2520eigenvalues%2520analytically%2520and%2520compare%2520them%2520to%2520numerical%2520predictions.%250AFinally%252C%2520we%2520propose%2520a%2520variational%2520framework%2520for%2520determining%2520eigenvalues%2520on%250Aother%2520compact%2520manifolds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09572v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Spectral%20Convergence%20of%20Locally%20Linear%20Embedding%20on%20Manifolds%0A%20%20with%20Boundary&entry.906535625=Andrew%20Lyons&entry.1292438233=%20%20We%20study%20the%20eigenvalues%20and%20eigenfunctions%20of%20a%20differential%20operator%20that%0Agoverns%20the%20asymptotic%20behavior%20of%20the%20unsupervised%20learning%20algorithm%20known%20as%0ALocally%20Linear%20Embedding%20when%20a%20large%20data%20set%20is%20sampled%20from%20an%20interval%20or%0Adisc.%20In%20particular%2C%20the%20differential%20operator%20is%20of%20second%20order%2C%20mixed-type%2C%0Aand%20degenerates%20near%20the%20boundary.%20We%20show%20that%20a%20natural%20regularity%20condition%0Aon%20the%20eigenfunctions%20imposes%20a%20consistent%20boundary%20condition%20and%20use%20the%0AFrobenius%20method%20to%20estimate%20pointwise%20behavior.%20We%20then%20determine%20the%20limiting%0Asequence%20of%20eigenvalues%20analytically%20and%20compare%20them%20to%20numerical%20predictions.%0AFinally%2C%20we%20propose%20a%20variational%20framework%20for%20determining%20eigenvalues%20on%0Aother%20compact%20manifolds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09572v1&entry.124074799=Read"},
{"title": "ComplexVAD: Detecting Interaction Anomalies in Video", "author": "Furkan Mumcu and Michael J. Jones and Yasin Yilmaz and Anoop Cherian", "abstract": "  Existing video anomaly detection datasets are inadequate for representing\ncomplex anomalies that occur due to the interactions between objects. The\nabsence of complex anomalies in previous video anomaly detection datasets\naffects research by shifting the focus onto simple anomalies. To address this\nproblem, we introduce a new large-scale dataset: ComplexVAD. In addition, we\npropose a novel method to detect complex anomalies via modeling the\ninteractions between objects using a scene graph with spatio-temporal\nattributes. With our proposed method and two other state-of-the-art video\nanomaly detection methods, we obtain baseline scores on ComplexVAD and\ndemonstrate that our new method outperforms existing works.\n", "link": "http://arxiv.org/abs/2501.09733v1", "date": "2025-01-16", "relevancy": 2.2254, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5608}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ComplexVAD%3A%20Detecting%20Interaction%20Anomalies%20in%20Video&body=Title%3A%20ComplexVAD%3A%20Detecting%20Interaction%20Anomalies%20in%20Video%0AAuthor%3A%20Furkan%20Mumcu%20and%20Michael%20J.%20Jones%20and%20Yasin%20Yilmaz%20and%20Anoop%20Cherian%0AAbstract%3A%20%20%20Existing%20video%20anomaly%20detection%20datasets%20are%20inadequate%20for%20representing%0Acomplex%20anomalies%20that%20occur%20due%20to%20the%20interactions%20between%20objects.%20The%0Aabsence%20of%20complex%20anomalies%20in%20previous%20video%20anomaly%20detection%20datasets%0Aaffects%20research%20by%20shifting%20the%20focus%20onto%20simple%20anomalies.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20a%20new%20large-scale%20dataset%3A%20ComplexVAD.%20In%20addition%2C%20we%0Apropose%20a%20novel%20method%20to%20detect%20complex%20anomalies%20via%20modeling%20the%0Ainteractions%20between%20objects%20using%20a%20scene%20graph%20with%20spatio-temporal%0Aattributes.%20With%20our%20proposed%20method%20and%20two%20other%20state-of-the-art%20video%0Aanomaly%20detection%20methods%2C%20we%20obtain%20baseline%20scores%20on%20ComplexVAD%20and%0Ademonstrate%20that%20our%20new%20method%20outperforms%20existing%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplexVAD%253A%2520Detecting%2520Interaction%2520Anomalies%2520in%2520Video%26entry.906535625%3DFurkan%2520Mumcu%2520and%2520Michael%2520J.%2520Jones%2520and%2520Yasin%2520Yilmaz%2520and%2520Anoop%2520Cherian%26entry.1292438233%3D%2520%2520Existing%2520video%2520anomaly%2520detection%2520datasets%2520are%2520inadequate%2520for%2520representing%250Acomplex%2520anomalies%2520that%2520occur%2520due%2520to%2520the%2520interactions%2520between%2520objects.%2520The%250Aabsence%2520of%2520complex%2520anomalies%2520in%2520previous%2520video%2520anomaly%2520detection%2520datasets%250Aaffects%2520research%2520by%2520shifting%2520the%2520focus%2520onto%2520simple%2520anomalies.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520introduce%2520a%2520new%2520large-scale%2520dataset%253A%2520ComplexVAD.%2520In%2520addition%252C%2520we%250Apropose%2520a%2520novel%2520method%2520to%2520detect%2520complex%2520anomalies%2520via%2520modeling%2520the%250Ainteractions%2520between%2520objects%2520using%2520a%2520scene%2520graph%2520with%2520spatio-temporal%250Aattributes.%2520With%2520our%2520proposed%2520method%2520and%2520two%2520other%2520state-of-the-art%2520video%250Aanomaly%2520detection%2520methods%252C%2520we%2520obtain%2520baseline%2520scores%2520on%2520ComplexVAD%2520and%250Ademonstrate%2520that%2520our%2520new%2520method%2520outperforms%2520existing%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ComplexVAD%3A%20Detecting%20Interaction%20Anomalies%20in%20Video&entry.906535625=Furkan%20Mumcu%20and%20Michael%20J.%20Jones%20and%20Yasin%20Yilmaz%20and%20Anoop%20Cherian&entry.1292438233=%20%20Existing%20video%20anomaly%20detection%20datasets%20are%20inadequate%20for%20representing%0Acomplex%20anomalies%20that%20occur%20due%20to%20the%20interactions%20between%20objects.%20The%0Aabsence%20of%20complex%20anomalies%20in%20previous%20video%20anomaly%20detection%20datasets%0Aaffects%20research%20by%20shifting%20the%20focus%20onto%20simple%20anomalies.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20a%20new%20large-scale%20dataset%3A%20ComplexVAD.%20In%20addition%2C%20we%0Apropose%20a%20novel%20method%20to%20detect%20complex%20anomalies%20via%20modeling%20the%0Ainteractions%20between%20objects%20using%20a%20scene%20graph%20with%20spatio-temporal%0Aattributes.%20With%20our%20proposed%20method%20and%20two%20other%20state-of-the-art%20video%0Aanomaly%20detection%20methods%2C%20we%20obtain%20baseline%20scores%20on%20ComplexVAD%20and%0Ademonstrate%20that%20our%20new%20method%20outperforms%20existing%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09733v1&entry.124074799=Read"},
{"title": "The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating\n  Large Language Models", "author": "Jonathan Katzy and Razvan Mihai Popescu and Arie van Deursen and Maliheh Izadi", "abstract": "  The recent rise in the popularity of large language models has spurred the\ndevelopment of extensive code datasets needed to train them. This has left\nlimited code available for collection and use in the downstream investigation\nof specific behaviors, or evaluation of large language models without suffering\nfrom data contamination. To address this problem, we release The Heap, a large\nmultilingual dataset covering 57 programming languages that has been\ndeduplicated with respect to other open datasets of code, enabling researchers\nto conduct fair evaluations of large language models without significant data\ncleaning overhead.\n", "link": "http://arxiv.org/abs/2501.09653v1", "date": "2025-01-16", "relevancy": 2.2126, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4563}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Heap%3A%20A%20Contamination-Free%20Multilingual%20Code%20Dataset%20for%20Evaluating%0A%20%20Large%20Language%20Models&body=Title%3A%20The%20Heap%3A%20A%20Contamination-Free%20Multilingual%20Code%20Dataset%20for%20Evaluating%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Jonathan%20Katzy%20and%20Razvan%20Mihai%20Popescu%20and%20Arie%20van%20Deursen%20and%20Maliheh%20Izadi%0AAbstract%3A%20%20%20The%20recent%20rise%20in%20the%20popularity%20of%20large%20language%20models%20has%20spurred%20the%0Adevelopment%20of%20extensive%20code%20datasets%20needed%20to%20train%20them.%20This%20has%20left%0Alimited%20code%20available%20for%20collection%20and%20use%20in%20the%20downstream%20investigation%0Aof%20specific%20behaviors%2C%20or%20evaluation%20of%20large%20language%20models%20without%20suffering%0Afrom%20data%20contamination.%20To%20address%20this%20problem%2C%20we%20release%20The%20Heap%2C%20a%20large%0Amultilingual%20dataset%20covering%2057%20programming%20languages%20that%20has%20been%0Adeduplicated%20with%20respect%20to%20other%20open%20datasets%20of%20code%2C%20enabling%20researchers%0Ato%20conduct%20fair%20evaluations%20of%20large%20language%20models%20without%20significant%20data%0Acleaning%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Heap%253A%2520A%2520Contamination-Free%2520Multilingual%2520Code%2520Dataset%2520for%2520Evaluating%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DJonathan%2520Katzy%2520and%2520Razvan%2520Mihai%2520Popescu%2520and%2520Arie%2520van%2520Deursen%2520and%2520Maliheh%2520Izadi%26entry.1292438233%3D%2520%2520The%2520recent%2520rise%2520in%2520the%2520popularity%2520of%2520large%2520language%2520models%2520has%2520spurred%2520the%250Adevelopment%2520of%2520extensive%2520code%2520datasets%2520needed%2520to%2520train%2520them.%2520This%2520has%2520left%250Alimited%2520code%2520available%2520for%2520collection%2520and%2520use%2520in%2520the%2520downstream%2520investigation%250Aof%2520specific%2520behaviors%252C%2520or%2520evaluation%2520of%2520large%2520language%2520models%2520without%2520suffering%250Afrom%2520data%2520contamination.%2520To%2520address%2520this%2520problem%252C%2520we%2520release%2520The%2520Heap%252C%2520a%2520large%250Amultilingual%2520dataset%2520covering%252057%2520programming%2520languages%2520that%2520has%2520been%250Adeduplicated%2520with%2520respect%2520to%2520other%2520open%2520datasets%2520of%2520code%252C%2520enabling%2520researchers%250Ato%2520conduct%2520fair%2520evaluations%2520of%2520large%2520language%2520models%2520without%2520significant%2520data%250Acleaning%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Heap%3A%20A%20Contamination-Free%20Multilingual%20Code%20Dataset%20for%20Evaluating%0A%20%20Large%20Language%20Models&entry.906535625=Jonathan%20Katzy%20and%20Razvan%20Mihai%20Popescu%20and%20Arie%20van%20Deursen%20and%20Maliheh%20Izadi&entry.1292438233=%20%20The%20recent%20rise%20in%20the%20popularity%20of%20large%20language%20models%20has%20spurred%20the%0Adevelopment%20of%20extensive%20code%20datasets%20needed%20to%20train%20them.%20This%20has%20left%0Alimited%20code%20available%20for%20collection%20and%20use%20in%20the%20downstream%20investigation%0Aof%20specific%20behaviors%2C%20or%20evaluation%20of%20large%20language%20models%20without%20suffering%0Afrom%20data%20contamination.%20To%20address%20this%20problem%2C%20we%20release%20The%20Heap%2C%20a%20large%0Amultilingual%20dataset%20covering%2057%20programming%20languages%20that%20has%20been%0Adeduplicated%20with%20respect%20to%20other%20open%20datasets%20of%20code%2C%20enabling%20researchers%0Ato%20conduct%20fair%20evaluations%20of%20large%20language%20models%20without%20significant%20data%0Acleaning%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09653v1&entry.124074799=Read"},
{"title": "Text-driven Adaptation of Foundation Models for Few-shot Surgical\n  Workflow Analysis", "author": "Tingxuan Chen and Kun Yuan and Vinkle Srivastav and Nassir Navab and Nicolas Padoy", "abstract": "  Purpose: Surgical workflow analysis is crucial for improving surgical\nefficiency and safety. However, previous studies rely heavily on large-scale\nannotated datasets, posing challenges in cost, scalability, and reliance on\nexpert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven\nAdaptation), designed to handle various surgical workflow analysis tasks with\nminimal paired image-label data.\n  Methods: Our approach has two key components. First, Few-shot selection-based\nmodality alignment selects a small subset of images and aligns their embeddings\nwith text embeddings from the downstream task, bridging the modality gap.\nSecond, Text-driven adaptation leverages only text data to train a decoder,\neliminating the need for paired image-text data. This decoder is then applied\nto aligned image embeddings, enabling image-related tasks without explicit\nimage-text pairs.\n  Results: We evaluate our approach to generative tasks (image captioning) and\ndiscriminative tasks (triplet recognition and phase recognition). Results show\nthat Surg-FTDA outperforms baselines and generalizes well across downstream\ntasks.\n  Conclusion: We propose a text-driven adaptation approach that mitigates the\nmodality gap and handles multiple downstream tasks in surgical workflow\nanalysis, with minimal reliance on large annotated datasets. The code and\ndataset will be released in https://github.com/TingxuanSix/Surg-FTDA.\n", "link": "http://arxiv.org/abs/2501.09555v1", "date": "2025-01-16", "relevancy": 2.2125, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.56}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5507}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-driven%20Adaptation%20of%20Foundation%20Models%20for%20Few-shot%20Surgical%0A%20%20Workflow%20Analysis&body=Title%3A%20Text-driven%20Adaptation%20of%20Foundation%20Models%20for%20Few-shot%20Surgical%0A%20%20Workflow%20Analysis%0AAuthor%3A%20Tingxuan%20Chen%20and%20Kun%20Yuan%20and%20Vinkle%20Srivastav%20and%20Nassir%20Navab%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20Purpose%3A%20Surgical%20workflow%20analysis%20is%20crucial%20for%20improving%20surgical%0Aefficiency%20and%20safety.%20However%2C%20previous%20studies%20rely%20heavily%20on%20large-scale%0Aannotated%20datasets%2C%20posing%20challenges%20in%20cost%2C%20scalability%2C%20and%20reliance%20on%0Aexpert%20annotations.%20To%20address%20this%2C%20we%20propose%20Surg-FTDA%20%28Few-shot%20Text-driven%0AAdaptation%29%2C%20designed%20to%20handle%20various%20surgical%20workflow%20analysis%20tasks%20with%0Aminimal%20paired%20image-label%20data.%0A%20%20Methods%3A%20Our%20approach%20has%20two%20key%20components.%20First%2C%20Few-shot%20selection-based%0Amodality%20alignment%20selects%20a%20small%20subset%20of%20images%20and%20aligns%20their%20embeddings%0Awith%20text%20embeddings%20from%20the%20downstream%20task%2C%20bridging%20the%20modality%20gap.%0ASecond%2C%20Text-driven%20adaptation%20leverages%20only%20text%20data%20to%20train%20a%20decoder%2C%0Aeliminating%20the%20need%20for%20paired%20image-text%20data.%20This%20decoder%20is%20then%20applied%0Ato%20aligned%20image%20embeddings%2C%20enabling%20image-related%20tasks%20without%20explicit%0Aimage-text%20pairs.%0A%20%20Results%3A%20We%20evaluate%20our%20approach%20to%20generative%20tasks%20%28image%20captioning%29%20and%0Adiscriminative%20tasks%20%28triplet%20recognition%20and%20phase%20recognition%29.%20Results%20show%0Athat%20Surg-FTDA%20outperforms%20baselines%20and%20generalizes%20well%20across%20downstream%0Atasks.%0A%20%20Conclusion%3A%20We%20propose%20a%20text-driven%20adaptation%20approach%20that%20mitigates%20the%0Amodality%20gap%20and%20handles%20multiple%20downstream%20tasks%20in%20surgical%20workflow%0Aanalysis%2C%20with%20minimal%20reliance%20on%20large%20annotated%20datasets.%20The%20code%20and%0Adataset%20will%20be%20released%20in%20https%3A//github.com/TingxuanSix/Surg-FTDA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-driven%2520Adaptation%2520of%2520Foundation%2520Models%2520for%2520Few-shot%2520Surgical%250A%2520%2520Workflow%2520Analysis%26entry.906535625%3DTingxuan%2520Chen%2520and%2520Kun%2520Yuan%2520and%2520Vinkle%2520Srivastav%2520and%2520Nassir%2520Navab%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520Purpose%253A%2520Surgical%2520workflow%2520analysis%2520is%2520crucial%2520for%2520improving%2520surgical%250Aefficiency%2520and%2520safety.%2520However%252C%2520previous%2520studies%2520rely%2520heavily%2520on%2520large-scale%250Aannotated%2520datasets%252C%2520posing%2520challenges%2520in%2520cost%252C%2520scalability%252C%2520and%2520reliance%2520on%250Aexpert%2520annotations.%2520To%2520address%2520this%252C%2520we%2520propose%2520Surg-FTDA%2520%2528Few-shot%2520Text-driven%250AAdaptation%2529%252C%2520designed%2520to%2520handle%2520various%2520surgical%2520workflow%2520analysis%2520tasks%2520with%250Aminimal%2520paired%2520image-label%2520data.%250A%2520%2520Methods%253A%2520Our%2520approach%2520has%2520two%2520key%2520components.%2520First%252C%2520Few-shot%2520selection-based%250Amodality%2520alignment%2520selects%2520a%2520small%2520subset%2520of%2520images%2520and%2520aligns%2520their%2520embeddings%250Awith%2520text%2520embeddings%2520from%2520the%2520downstream%2520task%252C%2520bridging%2520the%2520modality%2520gap.%250ASecond%252C%2520Text-driven%2520adaptation%2520leverages%2520only%2520text%2520data%2520to%2520train%2520a%2520decoder%252C%250Aeliminating%2520the%2520need%2520for%2520paired%2520image-text%2520data.%2520This%2520decoder%2520is%2520then%2520applied%250Ato%2520aligned%2520image%2520embeddings%252C%2520enabling%2520image-related%2520tasks%2520without%2520explicit%250Aimage-text%2520pairs.%250A%2520%2520Results%253A%2520We%2520evaluate%2520our%2520approach%2520to%2520generative%2520tasks%2520%2528image%2520captioning%2529%2520and%250Adiscriminative%2520tasks%2520%2528triplet%2520recognition%2520and%2520phase%2520recognition%2529.%2520Results%2520show%250Athat%2520Surg-FTDA%2520outperforms%2520baselines%2520and%2520generalizes%2520well%2520across%2520downstream%250Atasks.%250A%2520%2520Conclusion%253A%2520We%2520propose%2520a%2520text-driven%2520adaptation%2520approach%2520that%2520mitigates%2520the%250Amodality%2520gap%2520and%2520handles%2520multiple%2520downstream%2520tasks%2520in%2520surgical%2520workflow%250Aanalysis%252C%2520with%2520minimal%2520reliance%2520on%2520large%2520annotated%2520datasets.%2520The%2520code%2520and%250Adataset%2520will%2520be%2520released%2520in%2520https%253A//github.com/TingxuanSix/Surg-FTDA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-driven%20Adaptation%20of%20Foundation%20Models%20for%20Few-shot%20Surgical%0A%20%20Workflow%20Analysis&entry.906535625=Tingxuan%20Chen%20and%20Kun%20Yuan%20and%20Vinkle%20Srivastav%20and%20Nassir%20Navab%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Purpose%3A%20Surgical%20workflow%20analysis%20is%20crucial%20for%20improving%20surgical%0Aefficiency%20and%20safety.%20However%2C%20previous%20studies%20rely%20heavily%20on%20large-scale%0Aannotated%20datasets%2C%20posing%20challenges%20in%20cost%2C%20scalability%2C%20and%20reliance%20on%0Aexpert%20annotations.%20To%20address%20this%2C%20we%20propose%20Surg-FTDA%20%28Few-shot%20Text-driven%0AAdaptation%29%2C%20designed%20to%20handle%20various%20surgical%20workflow%20analysis%20tasks%20with%0Aminimal%20paired%20image-label%20data.%0A%20%20Methods%3A%20Our%20approach%20has%20two%20key%20components.%20First%2C%20Few-shot%20selection-based%0Amodality%20alignment%20selects%20a%20small%20subset%20of%20images%20and%20aligns%20their%20embeddings%0Awith%20text%20embeddings%20from%20the%20downstream%20task%2C%20bridging%20the%20modality%20gap.%0ASecond%2C%20Text-driven%20adaptation%20leverages%20only%20text%20data%20to%20train%20a%20decoder%2C%0Aeliminating%20the%20need%20for%20paired%20image-text%20data.%20This%20decoder%20is%20then%20applied%0Ato%20aligned%20image%20embeddings%2C%20enabling%20image-related%20tasks%20without%20explicit%0Aimage-text%20pairs.%0A%20%20Results%3A%20We%20evaluate%20our%20approach%20to%20generative%20tasks%20%28image%20captioning%29%20and%0Adiscriminative%20tasks%20%28triplet%20recognition%20and%20phase%20recognition%29.%20Results%20show%0Athat%20Surg-FTDA%20outperforms%20baselines%20and%20generalizes%20well%20across%20downstream%0Atasks.%0A%20%20Conclusion%3A%20We%20propose%20a%20text-driven%20adaptation%20approach%20that%20mitigates%20the%0Amodality%20gap%20and%20handles%20multiple%20downstream%20tasks%20in%20surgical%20workflow%0Aanalysis%2C%20with%20minimal%20reliance%20on%20large%20annotated%20datasets.%20The%20code%20and%0Adataset%20will%20be%20released%20in%20https%3A//github.com/TingxuanSix/Surg-FTDA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09555v1&entry.124074799=Read"},
{"title": "Metric Learning with Progressive Self-Distillation for Audio-Visual\n  Embedding Learning", "author": "Donghuo Zeng and Kazushi Ikeda", "abstract": "  Metric learning projects samples into an embedded space, where similarities\nand dissimilarities are quantified based on their learned representations.\nHowever, existing methods often rely on label-guided representation learning,\nwhere representations of different modalities, such as audio and visual data,\nare aligned based on annotated labels. This approach tends to underutilize\nlatent complex features and potential relationships inherent in the\ndistributions of audio and visual data that are not directly tied to the\nlabels, resulting in suboptimal performance in audio-visual embedding learning.\nTo address this issue, we propose a novel architecture that integrates\ncross-modal triplet loss with progressive self-distillation. Our method\nenhances representation learning by leveraging inherent distributions and\ndynamically refining soft audio-visual alignments -- probabilistic alignments\nbetween audio and visual data that capture the inherent relationships beyond\nexplicit labels. Specifically, the model distills audio-visual\ndistribution-based knowledge from annotated labels in a subset of each batch.\nThis self-distilled knowledge is used t\n", "link": "http://arxiv.org/abs/2501.09608v1", "date": "2025-01-16", "relevancy": 2.2, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.554}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5536}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metric%20Learning%20with%20Progressive%20Self-Distillation%20for%20Audio-Visual%0A%20%20Embedding%20Learning&body=Title%3A%20Metric%20Learning%20with%20Progressive%20Self-Distillation%20for%20Audio-Visual%0A%20%20Embedding%20Learning%0AAuthor%3A%20Donghuo%20Zeng%20and%20Kazushi%20Ikeda%0AAbstract%3A%20%20%20Metric%20learning%20projects%20samples%20into%20an%20embedded%20space%2C%20where%20similarities%0Aand%20dissimilarities%20are%20quantified%20based%20on%20their%20learned%20representations.%0AHowever%2C%20existing%20methods%20often%20rely%20on%20label-guided%20representation%20learning%2C%0Awhere%20representations%20of%20different%20modalities%2C%20such%20as%20audio%20and%20visual%20data%2C%0Aare%20aligned%20based%20on%20annotated%20labels.%20This%20approach%20tends%20to%20underutilize%0Alatent%20complex%20features%20and%20potential%20relationships%20inherent%20in%20the%0Adistributions%20of%20audio%20and%20visual%20data%20that%20are%20not%20directly%20tied%20to%20the%0Alabels%2C%20resulting%20in%20suboptimal%20performance%20in%20audio-visual%20embedding%20learning.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20architecture%20that%20integrates%0Across-modal%20triplet%20loss%20with%20progressive%20self-distillation.%20Our%20method%0Aenhances%20representation%20learning%20by%20leveraging%20inherent%20distributions%20and%0Adynamically%20refining%20soft%20audio-visual%20alignments%20--%20probabilistic%20alignments%0Abetween%20audio%20and%20visual%20data%20that%20capture%20the%20inherent%20relationships%20beyond%0Aexplicit%20labels.%20Specifically%2C%20the%20model%20distills%20audio-visual%0Adistribution-based%20knowledge%20from%20annotated%20labels%20in%20a%20subset%20of%20each%20batch.%0AThis%20self-distilled%20knowledge%20is%20used%20t%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetric%2520Learning%2520with%2520Progressive%2520Self-Distillation%2520for%2520Audio-Visual%250A%2520%2520Embedding%2520Learning%26entry.906535625%3DDonghuo%2520Zeng%2520and%2520Kazushi%2520Ikeda%26entry.1292438233%3D%2520%2520Metric%2520learning%2520projects%2520samples%2520into%2520an%2520embedded%2520space%252C%2520where%2520similarities%250Aand%2520dissimilarities%2520are%2520quantified%2520based%2520on%2520their%2520learned%2520representations.%250AHowever%252C%2520existing%2520methods%2520often%2520rely%2520on%2520label-guided%2520representation%2520learning%252C%250Awhere%2520representations%2520of%2520different%2520modalities%252C%2520such%2520as%2520audio%2520and%2520visual%2520data%252C%250Aare%2520aligned%2520based%2520on%2520annotated%2520labels.%2520This%2520approach%2520tends%2520to%2520underutilize%250Alatent%2520complex%2520features%2520and%2520potential%2520relationships%2520inherent%2520in%2520the%250Adistributions%2520of%2520audio%2520and%2520visual%2520data%2520that%2520are%2520not%2520directly%2520tied%2520to%2520the%250Alabels%252C%2520resulting%2520in%2520suboptimal%2520performance%2520in%2520audio-visual%2520embedding%2520learning.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520architecture%2520that%2520integrates%250Across-modal%2520triplet%2520loss%2520with%2520progressive%2520self-distillation.%2520Our%2520method%250Aenhances%2520representation%2520learning%2520by%2520leveraging%2520inherent%2520distributions%2520and%250Adynamically%2520refining%2520soft%2520audio-visual%2520alignments%2520--%2520probabilistic%2520alignments%250Abetween%2520audio%2520and%2520visual%2520data%2520that%2520capture%2520the%2520inherent%2520relationships%2520beyond%250Aexplicit%2520labels.%2520Specifically%252C%2520the%2520model%2520distills%2520audio-visual%250Adistribution-based%2520knowledge%2520from%2520annotated%2520labels%2520in%2520a%2520subset%2520of%2520each%2520batch.%250AThis%2520self-distilled%2520knowledge%2520is%2520used%2520t%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metric%20Learning%20with%20Progressive%20Self-Distillation%20for%20Audio-Visual%0A%20%20Embedding%20Learning&entry.906535625=Donghuo%20Zeng%20and%20Kazushi%20Ikeda&entry.1292438233=%20%20Metric%20learning%20projects%20samples%20into%20an%20embedded%20space%2C%20where%20similarities%0Aand%20dissimilarities%20are%20quantified%20based%20on%20their%20learned%20representations.%0AHowever%2C%20existing%20methods%20often%20rely%20on%20label-guided%20representation%20learning%2C%0Awhere%20representations%20of%20different%20modalities%2C%20such%20as%20audio%20and%20visual%20data%2C%0Aare%20aligned%20based%20on%20annotated%20labels.%20This%20approach%20tends%20to%20underutilize%0Alatent%20complex%20features%20and%20potential%20relationships%20inherent%20in%20the%0Adistributions%20of%20audio%20and%20visual%20data%20that%20are%20not%20directly%20tied%20to%20the%0Alabels%2C%20resulting%20in%20suboptimal%20performance%20in%20audio-visual%20embedding%20learning.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20architecture%20that%20integrates%0Across-modal%20triplet%20loss%20with%20progressive%20self-distillation.%20Our%20method%0Aenhances%20representation%20learning%20by%20leveraging%20inherent%20distributions%20and%0Adynamically%20refining%20soft%20audio-visual%20alignments%20--%20probabilistic%20alignments%0Abetween%20audio%20and%20visual%20data%20that%20capture%20the%20inherent%20relationships%20beyond%0Aexplicit%20labels.%20Specifically%2C%20the%20model%20distills%20audio-visual%0Adistribution-based%20knowledge%20from%20annotated%20labels%20in%20a%20subset%20of%20each%20batch.%0AThis%20self-distilled%20knowledge%20is%20used%20t%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09608v1&entry.124074799=Read"},
{"title": "Distilling Multi-modal Large Language Models for Autonomous Driving", "author": "Deepti Hegde and Rajeev Yasarla and Hong Cai and Shizhong Han and Apratim Bhattacharyya and Shweta Mahajan and Litian Liu and Risheek Garrepalli and Vishal M. Patel and Fatih Porikli", "abstract": "  Autonomous driving demands safe motion planning, especially in critical\n\"long-tail\" scenarios. Recent end-to-end autonomous driving systems leverage\nlarge language models (LLMs) as planners to improve generalizability to rare\nevents. However, using LLMs at test time introduces high computational costs.\nTo address this, we propose DiMA, an end-to-end autonomous driving system that\nmaintains the efficiency of an LLM-free (or vision-based) planner while\nleveraging the world knowledge of an LLM. DiMA distills the information from a\nmulti-modal LLM to a vision-based end-to-end planner through a set of specially\ndesigned surrogate tasks. Under a joint training strategy, a scene encoder\ncommon to both networks produces structured representations that are\nsemantically grounded as well as aligned to the final planning objective.\nNotably, the LLM is optional at inference, enabling robust planning without\ncompromising on efficiency. Training with DiMA results in a 37% reduction in\nthe L2 trajectory error and an 80% reduction in the collision rate of the\nvision-based planner, as well as a 44% trajectory error reduction in longtail\nscenarios. DiMA also achieves state-of-the-art performance on the nuScenes\nplanning benchmark.\n", "link": "http://arxiv.org/abs/2501.09757v1", "date": "2025-01-16", "relevancy": 2.1976, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5539}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20Multi-modal%20Large%20Language%20Models%20for%20Autonomous%20Driving&body=Title%3A%20Distilling%20Multi-modal%20Large%20Language%20Models%20for%20Autonomous%20Driving%0AAuthor%3A%20Deepti%20Hegde%20and%20Rajeev%20Yasarla%20and%20Hong%20Cai%20and%20Shizhong%20Han%20and%20Apratim%20Bhattacharyya%20and%20Shweta%20Mahajan%20and%20Litian%20Liu%20and%20Risheek%20Garrepalli%20and%20Vishal%20M.%20Patel%20and%20Fatih%20Porikli%0AAbstract%3A%20%20%20Autonomous%20driving%20demands%20safe%20motion%20planning%2C%20especially%20in%20critical%0A%22long-tail%22%20scenarios.%20Recent%20end-to-end%20autonomous%20driving%20systems%20leverage%0Alarge%20language%20models%20%28LLMs%29%20as%20planners%20to%20improve%20generalizability%20to%20rare%0Aevents.%20However%2C%20using%20LLMs%20at%20test%20time%20introduces%20high%20computational%20costs.%0ATo%20address%20this%2C%20we%20propose%20DiMA%2C%20an%20end-to-end%20autonomous%20driving%20system%20that%0Amaintains%20the%20efficiency%20of%20an%20LLM-free%20%28or%20vision-based%29%20planner%20while%0Aleveraging%20the%20world%20knowledge%20of%20an%20LLM.%20DiMA%20distills%20the%20information%20from%20a%0Amulti-modal%20LLM%20to%20a%20vision-based%20end-to-end%20planner%20through%20a%20set%20of%20specially%0Adesigned%20surrogate%20tasks.%20Under%20a%20joint%20training%20strategy%2C%20a%20scene%20encoder%0Acommon%20to%20both%20networks%20produces%20structured%20representations%20that%20are%0Asemantically%20grounded%20as%20well%20as%20aligned%20to%20the%20final%20planning%20objective.%0ANotably%2C%20the%20LLM%20is%20optional%20at%20inference%2C%20enabling%20robust%20planning%20without%0Acompromising%20on%20efficiency.%20Training%20with%20DiMA%20results%20in%20a%2037%25%20reduction%20in%0Athe%20L2%20trajectory%20error%20and%20an%2080%25%20reduction%20in%20the%20collision%20rate%20of%20the%0Avision-based%20planner%2C%20as%20well%20as%20a%2044%25%20trajectory%20error%20reduction%20in%20longtail%0Ascenarios.%20DiMA%20also%20achieves%20state-of-the-art%20performance%20on%20the%20nuScenes%0Aplanning%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520Multi-modal%2520Large%2520Language%2520Models%2520for%2520Autonomous%2520Driving%26entry.906535625%3DDeepti%2520Hegde%2520and%2520Rajeev%2520Yasarla%2520and%2520Hong%2520Cai%2520and%2520Shizhong%2520Han%2520and%2520Apratim%2520Bhattacharyya%2520and%2520Shweta%2520Mahajan%2520and%2520Litian%2520Liu%2520and%2520Risheek%2520Garrepalli%2520and%2520Vishal%2520M.%2520Patel%2520and%2520Fatih%2520Porikli%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520demands%2520safe%2520motion%2520planning%252C%2520especially%2520in%2520critical%250A%2522long-tail%2522%2520scenarios.%2520Recent%2520end-to-end%2520autonomous%2520driving%2520systems%2520leverage%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520as%2520planners%2520to%2520improve%2520generalizability%2520to%2520rare%250Aevents.%2520However%252C%2520using%2520LLMs%2520at%2520test%2520time%2520introduces%2520high%2520computational%2520costs.%250ATo%2520address%2520this%252C%2520we%2520propose%2520DiMA%252C%2520an%2520end-to-end%2520autonomous%2520driving%2520system%2520that%250Amaintains%2520the%2520efficiency%2520of%2520an%2520LLM-free%2520%2528or%2520vision-based%2529%2520planner%2520while%250Aleveraging%2520the%2520world%2520knowledge%2520of%2520an%2520LLM.%2520DiMA%2520distills%2520the%2520information%2520from%2520a%250Amulti-modal%2520LLM%2520to%2520a%2520vision-based%2520end-to-end%2520planner%2520through%2520a%2520set%2520of%2520specially%250Adesigned%2520surrogate%2520tasks.%2520Under%2520a%2520joint%2520training%2520strategy%252C%2520a%2520scene%2520encoder%250Acommon%2520to%2520both%2520networks%2520produces%2520structured%2520representations%2520that%2520are%250Asemantically%2520grounded%2520as%2520well%2520as%2520aligned%2520to%2520the%2520final%2520planning%2520objective.%250ANotably%252C%2520the%2520LLM%2520is%2520optional%2520at%2520inference%252C%2520enabling%2520robust%2520planning%2520without%250Acompromising%2520on%2520efficiency.%2520Training%2520with%2520DiMA%2520results%2520in%2520a%252037%2525%2520reduction%2520in%250Athe%2520L2%2520trajectory%2520error%2520and%2520an%252080%2525%2520reduction%2520in%2520the%2520collision%2520rate%2520of%2520the%250Avision-based%2520planner%252C%2520as%2520well%2520as%2520a%252044%2525%2520trajectory%2520error%2520reduction%2520in%2520longtail%250Ascenarios.%2520DiMA%2520also%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520nuScenes%250Aplanning%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20Multi-modal%20Large%20Language%20Models%20for%20Autonomous%20Driving&entry.906535625=Deepti%20Hegde%20and%20Rajeev%20Yasarla%20and%20Hong%20Cai%20and%20Shizhong%20Han%20and%20Apratim%20Bhattacharyya%20and%20Shweta%20Mahajan%20and%20Litian%20Liu%20and%20Risheek%20Garrepalli%20and%20Vishal%20M.%20Patel%20and%20Fatih%20Porikli&entry.1292438233=%20%20Autonomous%20driving%20demands%20safe%20motion%20planning%2C%20especially%20in%20critical%0A%22long-tail%22%20scenarios.%20Recent%20end-to-end%20autonomous%20driving%20systems%20leverage%0Alarge%20language%20models%20%28LLMs%29%20as%20planners%20to%20improve%20generalizability%20to%20rare%0Aevents.%20However%2C%20using%20LLMs%20at%20test%20time%20introduces%20high%20computational%20costs.%0ATo%20address%20this%2C%20we%20propose%20DiMA%2C%20an%20end-to-end%20autonomous%20driving%20system%20that%0Amaintains%20the%20efficiency%20of%20an%20LLM-free%20%28or%20vision-based%29%20planner%20while%0Aleveraging%20the%20world%20knowledge%20of%20an%20LLM.%20DiMA%20distills%20the%20information%20from%20a%0Amulti-modal%20LLM%20to%20a%20vision-based%20end-to-end%20planner%20through%20a%20set%20of%20specially%0Adesigned%20surrogate%20tasks.%20Under%20a%20joint%20training%20strategy%2C%20a%20scene%20encoder%0Acommon%20to%20both%20networks%20produces%20structured%20representations%20that%20are%0Asemantically%20grounded%20as%20well%20as%20aligned%20to%20the%20final%20planning%20objective.%0ANotably%2C%20the%20LLM%20is%20optional%20at%20inference%2C%20enabling%20robust%20planning%20without%0Acompromising%20on%20efficiency.%20Training%20with%20DiMA%20results%20in%20a%2037%25%20reduction%20in%0Athe%20L2%20trajectory%20error%20and%20an%2080%25%20reduction%20in%20the%20collision%20rate%20of%20the%0Avision-based%20planner%2C%20as%20well%20as%20a%2044%25%20trajectory%20error%20reduction%20in%20longtail%0Ascenarios.%20DiMA%20also%20achieves%20state-of-the-art%20performance%20on%20the%20nuScenes%0Aplanning%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09757v1&entry.124074799=Read"},
{"title": "Local Anti-Concentration Class: Logarithmic Regret for Greedy Linear\n  Contextual Bandit", "author": "Seok-Jin Kim and Min-hwan Oh", "abstract": "  We study the performance guarantees of exploration-free greedy algorithms for\nthe linear contextual bandit problem. We introduce a novel condition, named the\n\\textit{Local Anti-Concentration} (LAC) condition, which enables a greedy\nbandit algorithm to achieve provable efficiency. We show that the LAC condition\nis satisfied by a broad class of distributions, including Gaussian,\nexponential, uniform, Cauchy, and Student's~$t$ distributions, along with other\nexponential family distributions and their truncated variants. This\nsignificantly expands the class of distributions under which greedy algorithms\ncan perform efficiently. Under our proposed LAC condition, we prove that the\ncumulative expected regret of the greedy algorithm for the linear contextual\nbandit is bounded by $O(\\operatorname{poly} \\log T)$. Our results establish the\nwidest range of distributions known to date that allow a sublinear regret bound\nfor greedy algorithms, further achieving a sharp poly-logarithmic regret.\n", "link": "http://arxiv.org/abs/2411.12878v2", "date": "2025-01-16", "relevancy": 2.1646, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4501}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4425}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Anti-Concentration%20Class%3A%20Logarithmic%20Regret%20for%20Greedy%20Linear%0A%20%20Contextual%20Bandit&body=Title%3A%20Local%20Anti-Concentration%20Class%3A%20Logarithmic%20Regret%20for%20Greedy%20Linear%0A%20%20Contextual%20Bandit%0AAuthor%3A%20Seok-Jin%20Kim%20and%20Min-hwan%20Oh%0AAbstract%3A%20%20%20We%20study%20the%20performance%20guarantees%20of%20exploration-free%20greedy%20algorithms%20for%0Athe%20linear%20contextual%20bandit%20problem.%20We%20introduce%20a%20novel%20condition%2C%20named%20the%0A%5Ctextit%7BLocal%20Anti-Concentration%7D%20%28LAC%29%20condition%2C%20which%20enables%20a%20greedy%0Abandit%20algorithm%20to%20achieve%20provable%20efficiency.%20We%20show%20that%20the%20LAC%20condition%0Ais%20satisfied%20by%20a%20broad%20class%20of%20distributions%2C%20including%20Gaussian%2C%0Aexponential%2C%20uniform%2C%20Cauchy%2C%20and%20Student%27s~%24t%24%20distributions%2C%20along%20with%20other%0Aexponential%20family%20distributions%20and%20their%20truncated%20variants.%20This%0Asignificantly%20expands%20the%20class%20of%20distributions%20under%20which%20greedy%20algorithms%0Acan%20perform%20efficiently.%20Under%20our%20proposed%20LAC%20condition%2C%20we%20prove%20that%20the%0Acumulative%20expected%20regret%20of%20the%20greedy%20algorithm%20for%20the%20linear%20contextual%0Abandit%20is%20bounded%20by%20%24O%28%5Coperatorname%7Bpoly%7D%20%5Clog%20T%29%24.%20Our%20results%20establish%20the%0Awidest%20range%20of%20distributions%20known%20to%20date%20that%20allow%20a%20sublinear%20regret%20bound%0Afor%20greedy%20algorithms%2C%20further%20achieving%20a%20sharp%20poly-logarithmic%20regret.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12878v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Anti-Concentration%2520Class%253A%2520Logarithmic%2520Regret%2520for%2520Greedy%2520Linear%250A%2520%2520Contextual%2520Bandit%26entry.906535625%3DSeok-Jin%2520Kim%2520and%2520Min-hwan%2520Oh%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520performance%2520guarantees%2520of%2520exploration-free%2520greedy%2520algorithms%2520for%250Athe%2520linear%2520contextual%2520bandit%2520problem.%2520We%2520introduce%2520a%2520novel%2520condition%252C%2520named%2520the%250A%255Ctextit%257BLocal%2520Anti-Concentration%257D%2520%2528LAC%2529%2520condition%252C%2520which%2520enables%2520a%2520greedy%250Abandit%2520algorithm%2520to%2520achieve%2520provable%2520efficiency.%2520We%2520show%2520that%2520the%2520LAC%2520condition%250Ais%2520satisfied%2520by%2520a%2520broad%2520class%2520of%2520distributions%252C%2520including%2520Gaussian%252C%250Aexponential%252C%2520uniform%252C%2520Cauchy%252C%2520and%2520Student%2527s~%2524t%2524%2520distributions%252C%2520along%2520with%2520other%250Aexponential%2520family%2520distributions%2520and%2520their%2520truncated%2520variants.%2520This%250Asignificantly%2520expands%2520the%2520class%2520of%2520distributions%2520under%2520which%2520greedy%2520algorithms%250Acan%2520perform%2520efficiently.%2520Under%2520our%2520proposed%2520LAC%2520condition%252C%2520we%2520prove%2520that%2520the%250Acumulative%2520expected%2520regret%2520of%2520the%2520greedy%2520algorithm%2520for%2520the%2520linear%2520contextual%250Abandit%2520is%2520bounded%2520by%2520%2524O%2528%255Coperatorname%257Bpoly%257D%2520%255Clog%2520T%2529%2524.%2520Our%2520results%2520establish%2520the%250Awidest%2520range%2520of%2520distributions%2520known%2520to%2520date%2520that%2520allow%2520a%2520sublinear%2520regret%2520bound%250Afor%2520greedy%2520algorithms%252C%2520further%2520achieving%2520a%2520sharp%2520poly-logarithmic%2520regret.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12878v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Anti-Concentration%20Class%3A%20Logarithmic%20Regret%20for%20Greedy%20Linear%0A%20%20Contextual%20Bandit&entry.906535625=Seok-Jin%20Kim%20and%20Min-hwan%20Oh&entry.1292438233=%20%20We%20study%20the%20performance%20guarantees%20of%20exploration-free%20greedy%20algorithms%20for%0Athe%20linear%20contextual%20bandit%20problem.%20We%20introduce%20a%20novel%20condition%2C%20named%20the%0A%5Ctextit%7BLocal%20Anti-Concentration%7D%20%28LAC%29%20condition%2C%20which%20enables%20a%20greedy%0Abandit%20algorithm%20to%20achieve%20provable%20efficiency.%20We%20show%20that%20the%20LAC%20condition%0Ais%20satisfied%20by%20a%20broad%20class%20of%20distributions%2C%20including%20Gaussian%2C%0Aexponential%2C%20uniform%2C%20Cauchy%2C%20and%20Student%27s~%24t%24%20distributions%2C%20along%20with%20other%0Aexponential%20family%20distributions%20and%20their%20truncated%20variants.%20This%0Asignificantly%20expands%20the%20class%20of%20distributions%20under%20which%20greedy%20algorithms%0Acan%20perform%20efficiently.%20Under%20our%20proposed%20LAC%20condition%2C%20we%20prove%20that%20the%0Acumulative%20expected%20regret%20of%20the%20greedy%20algorithm%20for%20the%20linear%20contextual%0Abandit%20is%20bounded%20by%20%24O%28%5Coperatorname%7Bpoly%7D%20%5Clog%20T%29%24.%20Our%20results%20establish%20the%0Awidest%20range%20of%20distributions%20known%20to%20date%20that%20allow%20a%20sublinear%20regret%20bound%0Afor%20greedy%20algorithms%2C%20further%20achieving%20a%20sharp%20poly-logarithmic%20regret.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12878v2&entry.124074799=Read"},
{"title": "Vulnerability-Aware Spatio-Temporal Learning for Generalizable and\n  Interpretable Deepfake Video Detection", "author": "Dat Nguyen and Marcella Astrid and Anis Kacem and Enjie Ghorbel and Djamila Aouada", "abstract": "  Detecting deepfake videos is highly challenging due to the complex\nintertwined spatial and temporal artifacts in forged sequences. Most recent\napproaches rely on binary classifiers trained on both real and fake data.\nHowever, such methods may struggle to focus on important artifacts, which can\nhinder their generalization capability. Additionally, these models often lack\ninterpretability, making it difficult to understand how predictions are made.\nTo address these issues, we propose FakeSTormer, offering two key\ncontributions. First, we introduce a multi-task learning framework with\nadditional spatial and temporal branches that enable the model to focus on\nsubtle spatio-temporal artifacts. These branches also provide interpretability\nby highlighting video regions that may contain artifacts. Second, we propose a\nvideo-level data synthesis algorithm that generates pseudo-fake videos with\nsubtle artifacts, providing the model with high-quality samples and ground\ntruth data for our spatial and temporal branches. Extensive experiments on\nseveral challenging benchmarks demonstrate the competitiveness of our approach\ncompared to recent state-of-the-art methods. The code is available at\nhttps://github.com/10Ring/FakeSTormer.\n", "link": "http://arxiv.org/abs/2501.01184v2", "date": "2025-01-16", "relevancy": 2.1402, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5377}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5365}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vulnerability-Aware%20Spatio-Temporal%20Learning%20for%20Generalizable%20and%0A%20%20Interpretable%20Deepfake%20Video%20Detection&body=Title%3A%20Vulnerability-Aware%20Spatio-Temporal%20Learning%20for%20Generalizable%20and%0A%20%20Interpretable%20Deepfake%20Video%20Detection%0AAuthor%3A%20Dat%20Nguyen%20and%20Marcella%20Astrid%20and%20Anis%20Kacem%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20Detecting%20deepfake%20videos%20is%20highly%20challenging%20due%20to%20the%20complex%0Aintertwined%20spatial%20and%20temporal%20artifacts%20in%20forged%20sequences.%20Most%20recent%0Aapproaches%20rely%20on%20binary%20classifiers%20trained%20on%20both%20real%20and%20fake%20data.%0AHowever%2C%20such%20methods%20may%20struggle%20to%20focus%20on%20important%20artifacts%2C%20which%20can%0Ahinder%20their%20generalization%20capability.%20Additionally%2C%20these%20models%20often%20lack%0Ainterpretability%2C%20making%20it%20difficult%20to%20understand%20how%20predictions%20are%20made.%0ATo%20address%20these%20issues%2C%20we%20propose%20FakeSTormer%2C%20offering%20two%20key%0Acontributions.%20First%2C%20we%20introduce%20a%20multi-task%20learning%20framework%20with%0Aadditional%20spatial%20and%20temporal%20branches%20that%20enable%20the%20model%20to%20focus%20on%0Asubtle%20spatio-temporal%20artifacts.%20These%20branches%20also%20provide%20interpretability%0Aby%20highlighting%20video%20regions%20that%20may%20contain%20artifacts.%20Second%2C%20we%20propose%20a%0Avideo-level%20data%20synthesis%20algorithm%20that%20generates%20pseudo-fake%20videos%20with%0Asubtle%20artifacts%2C%20providing%20the%20model%20with%20high-quality%20samples%20and%20ground%0Atruth%20data%20for%20our%20spatial%20and%20temporal%20branches.%20Extensive%20experiments%20on%0Aseveral%20challenging%20benchmarks%20demonstrate%20the%20competitiveness%20of%20our%20approach%0Acompared%20to%20recent%20state-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/10Ring/FakeSTormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01184v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVulnerability-Aware%2520Spatio-Temporal%2520Learning%2520for%2520Generalizable%2520and%250A%2520%2520Interpretable%2520Deepfake%2520Video%2520Detection%26entry.906535625%3DDat%2520Nguyen%2520and%2520Marcella%2520Astrid%2520and%2520Anis%2520Kacem%2520and%2520Enjie%2520Ghorbel%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520Detecting%2520deepfake%2520videos%2520is%2520highly%2520challenging%2520due%2520to%2520the%2520complex%250Aintertwined%2520spatial%2520and%2520temporal%2520artifacts%2520in%2520forged%2520sequences.%2520Most%2520recent%250Aapproaches%2520rely%2520on%2520binary%2520classifiers%2520trained%2520on%2520both%2520real%2520and%2520fake%2520data.%250AHowever%252C%2520such%2520methods%2520may%2520struggle%2520to%2520focus%2520on%2520important%2520artifacts%252C%2520which%2520can%250Ahinder%2520their%2520generalization%2520capability.%2520Additionally%252C%2520these%2520models%2520often%2520lack%250Ainterpretability%252C%2520making%2520it%2520difficult%2520to%2520understand%2520how%2520predictions%2520are%2520made.%250ATo%2520address%2520these%2520issues%252C%2520we%2520propose%2520FakeSTormer%252C%2520offering%2520two%2520key%250Acontributions.%2520First%252C%2520we%2520introduce%2520a%2520multi-task%2520learning%2520framework%2520with%250Aadditional%2520spatial%2520and%2520temporal%2520branches%2520that%2520enable%2520the%2520model%2520to%2520focus%2520on%250Asubtle%2520spatio-temporal%2520artifacts.%2520These%2520branches%2520also%2520provide%2520interpretability%250Aby%2520highlighting%2520video%2520regions%2520that%2520may%2520contain%2520artifacts.%2520Second%252C%2520we%2520propose%2520a%250Avideo-level%2520data%2520synthesis%2520algorithm%2520that%2520generates%2520pseudo-fake%2520videos%2520with%250Asubtle%2520artifacts%252C%2520providing%2520the%2520model%2520with%2520high-quality%2520samples%2520and%2520ground%250Atruth%2520data%2520for%2520our%2520spatial%2520and%2520temporal%2520branches.%2520Extensive%2520experiments%2520on%250Aseveral%2520challenging%2520benchmarks%2520demonstrate%2520the%2520competitiveness%2520of%2520our%2520approach%250Acompared%2520to%2520recent%2520state-of-the-art%2520methods.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/10Ring/FakeSTormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01184v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vulnerability-Aware%20Spatio-Temporal%20Learning%20for%20Generalizable%20and%0A%20%20Interpretable%20Deepfake%20Video%20Detection&entry.906535625=Dat%20Nguyen%20and%20Marcella%20Astrid%20and%20Anis%20Kacem%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada&entry.1292438233=%20%20Detecting%20deepfake%20videos%20is%20highly%20challenging%20due%20to%20the%20complex%0Aintertwined%20spatial%20and%20temporal%20artifacts%20in%20forged%20sequences.%20Most%20recent%0Aapproaches%20rely%20on%20binary%20classifiers%20trained%20on%20both%20real%20and%20fake%20data.%0AHowever%2C%20such%20methods%20may%20struggle%20to%20focus%20on%20important%20artifacts%2C%20which%20can%0Ahinder%20their%20generalization%20capability.%20Additionally%2C%20these%20models%20often%20lack%0Ainterpretability%2C%20making%20it%20difficult%20to%20understand%20how%20predictions%20are%20made.%0ATo%20address%20these%20issues%2C%20we%20propose%20FakeSTormer%2C%20offering%20two%20key%0Acontributions.%20First%2C%20we%20introduce%20a%20multi-task%20learning%20framework%20with%0Aadditional%20spatial%20and%20temporal%20branches%20that%20enable%20the%20model%20to%20focus%20on%0Asubtle%20spatio-temporal%20artifacts.%20These%20branches%20also%20provide%20interpretability%0Aby%20highlighting%20video%20regions%20that%20may%20contain%20artifacts.%20Second%2C%20we%20propose%20a%0Avideo-level%20data%20synthesis%20algorithm%20that%20generates%20pseudo-fake%20videos%20with%0Asubtle%20artifacts%2C%20providing%20the%20model%20with%20high-quality%20samples%20and%20ground%0Atruth%20data%20for%20our%20spatial%20and%20temporal%20branches.%20Extensive%20experiments%20on%0Aseveral%20challenging%20benchmarks%20demonstrate%20the%20competitiveness%20of%20our%20approach%0Acompared%20to%20recent%20state-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/10Ring/FakeSTormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01184v2&entry.124074799=Read"},
{"title": "A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems\n  using Disparity Maps", "author": "Ariel Larey and Eyal Rond and Omer Achrack", "abstract": "  Face recognition technologies are increasingly used in various applications,\nyet they are vulnerable to face spoofing attacks. These spoofing attacks often\ninvolve unique 3D structures, such as printed papers or mobile device screens.\nAlthough stereo-depth cameras can detect such attacks effectively, their\nhigh-cost limits their widespread adoption. Conversely, two-sensor systems\nwithout extrinsic calibration offer a cost-effective alternative but are unable\nto calculate depth using stereo techniques. In this work, we propose a method\nto overcome this challenge by leveraging facial attributes to derive disparity\ninformation and estimate relative depth for anti-spoofing purposes, using\nnon-calibrated systems. We introduce a multi-modal anti-spoofing model, coined\nDisparity Model, that incorporates created disparity maps as a third modality\nalongside the two original sensor modalities. We demonstrate the effectiveness\nof the Disparity Model in countering various spoof attacks using a\ncomprehensive dataset collected from the Intel RealSense ID Solution F455. Our\nmethod outperformed existing methods in the literature, achieving an Equal\nError Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False\nPositive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the\nerrors of the best comparison method, respectively. Additionally, we introduce\na model ensemble that addresses 3D spoof attacks as well, achieving an EER of\n2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a\nstate-of-the-art solution for the challenging task of anti-spoofing in\nnon-calibrated systems that lack depth information.\n", "link": "http://arxiv.org/abs/2410.24031v3", "date": "2025-01-16", "relevancy": 2.1358, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5423}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.533}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Modal%20Approach%20for%20Face%20Anti-Spoofing%20in%20Non-Calibrated%20Systems%0A%20%20using%20Disparity%20Maps&body=Title%3A%20A%20Multi-Modal%20Approach%20for%20Face%20Anti-Spoofing%20in%20Non-Calibrated%20Systems%0A%20%20using%20Disparity%20Maps%0AAuthor%3A%20Ariel%20Larey%20and%20Eyal%20Rond%20and%20Omer%20Achrack%0AAbstract%3A%20%20%20Face%20recognition%20technologies%20are%20increasingly%20used%20in%20various%20applications%2C%0Ayet%20they%20are%20vulnerable%20to%20face%20spoofing%20attacks.%20These%20spoofing%20attacks%20often%0Ainvolve%20unique%203D%20structures%2C%20such%20as%20printed%20papers%20or%20mobile%20device%20screens.%0AAlthough%20stereo-depth%20cameras%20can%20detect%20such%20attacks%20effectively%2C%20their%0Ahigh-cost%20limits%20their%20widespread%20adoption.%20Conversely%2C%20two-sensor%20systems%0Awithout%20extrinsic%20calibration%20offer%20a%20cost-effective%20alternative%20but%20are%20unable%0Ato%20calculate%20depth%20using%20stereo%20techniques.%20In%20this%20work%2C%20we%20propose%20a%20method%0Ato%20overcome%20this%20challenge%20by%20leveraging%20facial%20attributes%20to%20derive%20disparity%0Ainformation%20and%20estimate%20relative%20depth%20for%20anti-spoofing%20purposes%2C%20using%0Anon-calibrated%20systems.%20We%20introduce%20a%20multi-modal%20anti-spoofing%20model%2C%20coined%0ADisparity%20Model%2C%20that%20incorporates%20created%20disparity%20maps%20as%20a%20third%20modality%0Aalongside%20the%20two%20original%20sensor%20modalities.%20We%20demonstrate%20the%20effectiveness%0Aof%20the%20Disparity%20Model%20in%20countering%20various%20spoof%20attacks%20using%20a%0Acomprehensive%20dataset%20collected%20from%20the%20Intel%20RealSense%20ID%20Solution%20F455.%20Our%0Amethod%20outperformed%20existing%20methods%20in%20the%20literature%2C%20achieving%20an%20Equal%0AError%20Rate%20%28EER%29%20of%201.71%25%20and%20a%20False%20Negative%20Rate%20%28FNR%29%20of%202.77%25%20at%20a%20False%0APositive%20Rate%20%28FPR%29%20of%201%25.%20These%20errors%20are%20lower%20by%202.45%25%20and%207.94%25%20than%20the%0Aerrors%20of%20the%20best%20comparison%20method%2C%20respectively.%20Additionally%2C%20we%20introduce%0Aa%20model%20ensemble%20that%20addresses%203D%20spoof%20attacks%20as%20well%2C%20achieving%20an%20EER%20of%0A2.04%25%20and%20an%20FNR%20of%203.83%25%20at%20an%20FPR%20of%201%25.%20Overall%2C%20our%20work%20provides%20a%0Astate-of-the-art%20solution%20for%20the%20challenging%20task%20of%20anti-spoofing%20in%0Anon-calibrated%20systems%20that%20lack%20depth%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24031v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Modal%2520Approach%2520for%2520Face%2520Anti-Spoofing%2520in%2520Non-Calibrated%2520Systems%250A%2520%2520using%2520Disparity%2520Maps%26entry.906535625%3DAriel%2520Larey%2520and%2520Eyal%2520Rond%2520and%2520Omer%2520Achrack%26entry.1292438233%3D%2520%2520Face%2520recognition%2520technologies%2520are%2520increasingly%2520used%2520in%2520various%2520applications%252C%250Ayet%2520they%2520are%2520vulnerable%2520to%2520face%2520spoofing%2520attacks.%2520These%2520spoofing%2520attacks%2520often%250Ainvolve%2520unique%25203D%2520structures%252C%2520such%2520as%2520printed%2520papers%2520or%2520mobile%2520device%2520screens.%250AAlthough%2520stereo-depth%2520cameras%2520can%2520detect%2520such%2520attacks%2520effectively%252C%2520their%250Ahigh-cost%2520limits%2520their%2520widespread%2520adoption.%2520Conversely%252C%2520two-sensor%2520systems%250Awithout%2520extrinsic%2520calibration%2520offer%2520a%2520cost-effective%2520alternative%2520but%2520are%2520unable%250Ato%2520calculate%2520depth%2520using%2520stereo%2520techniques.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520method%250Ato%2520overcome%2520this%2520challenge%2520by%2520leveraging%2520facial%2520attributes%2520to%2520derive%2520disparity%250Ainformation%2520and%2520estimate%2520relative%2520depth%2520for%2520anti-spoofing%2520purposes%252C%2520using%250Anon-calibrated%2520systems.%2520We%2520introduce%2520a%2520multi-modal%2520anti-spoofing%2520model%252C%2520coined%250ADisparity%2520Model%252C%2520that%2520incorporates%2520created%2520disparity%2520maps%2520as%2520a%2520third%2520modality%250Aalongside%2520the%2520two%2520original%2520sensor%2520modalities.%2520We%2520demonstrate%2520the%2520effectiveness%250Aof%2520the%2520Disparity%2520Model%2520in%2520countering%2520various%2520spoof%2520attacks%2520using%2520a%250Acomprehensive%2520dataset%2520collected%2520from%2520the%2520Intel%2520RealSense%2520ID%2520Solution%2520F455.%2520Our%250Amethod%2520outperformed%2520existing%2520methods%2520in%2520the%2520literature%252C%2520achieving%2520an%2520Equal%250AError%2520Rate%2520%2528EER%2529%2520of%25201.71%2525%2520and%2520a%2520False%2520Negative%2520Rate%2520%2528FNR%2529%2520of%25202.77%2525%2520at%2520a%2520False%250APositive%2520Rate%2520%2528FPR%2529%2520of%25201%2525.%2520These%2520errors%2520are%2520lower%2520by%25202.45%2525%2520and%25207.94%2525%2520than%2520the%250Aerrors%2520of%2520the%2520best%2520comparison%2520method%252C%2520respectively.%2520Additionally%252C%2520we%2520introduce%250Aa%2520model%2520ensemble%2520that%2520addresses%25203D%2520spoof%2520attacks%2520as%2520well%252C%2520achieving%2520an%2520EER%2520of%250A2.04%2525%2520and%2520an%2520FNR%2520of%25203.83%2525%2520at%2520an%2520FPR%2520of%25201%2525.%2520Overall%252C%2520our%2520work%2520provides%2520a%250Astate-of-the-art%2520solution%2520for%2520the%2520challenging%2520task%2520of%2520anti-spoofing%2520in%250Anon-calibrated%2520systems%2520that%2520lack%2520depth%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24031v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Modal%20Approach%20for%20Face%20Anti-Spoofing%20in%20Non-Calibrated%20Systems%0A%20%20using%20Disparity%20Maps&entry.906535625=Ariel%20Larey%20and%20Eyal%20Rond%20and%20Omer%20Achrack&entry.1292438233=%20%20Face%20recognition%20technologies%20are%20increasingly%20used%20in%20various%20applications%2C%0Ayet%20they%20are%20vulnerable%20to%20face%20spoofing%20attacks.%20These%20spoofing%20attacks%20often%0Ainvolve%20unique%203D%20structures%2C%20such%20as%20printed%20papers%20or%20mobile%20device%20screens.%0AAlthough%20stereo-depth%20cameras%20can%20detect%20such%20attacks%20effectively%2C%20their%0Ahigh-cost%20limits%20their%20widespread%20adoption.%20Conversely%2C%20two-sensor%20systems%0Awithout%20extrinsic%20calibration%20offer%20a%20cost-effective%20alternative%20but%20are%20unable%0Ato%20calculate%20depth%20using%20stereo%20techniques.%20In%20this%20work%2C%20we%20propose%20a%20method%0Ato%20overcome%20this%20challenge%20by%20leveraging%20facial%20attributes%20to%20derive%20disparity%0Ainformation%20and%20estimate%20relative%20depth%20for%20anti-spoofing%20purposes%2C%20using%0Anon-calibrated%20systems.%20We%20introduce%20a%20multi-modal%20anti-spoofing%20model%2C%20coined%0ADisparity%20Model%2C%20that%20incorporates%20created%20disparity%20maps%20as%20a%20third%20modality%0Aalongside%20the%20two%20original%20sensor%20modalities.%20We%20demonstrate%20the%20effectiveness%0Aof%20the%20Disparity%20Model%20in%20countering%20various%20spoof%20attacks%20using%20a%0Acomprehensive%20dataset%20collected%20from%20the%20Intel%20RealSense%20ID%20Solution%20F455.%20Our%0Amethod%20outperformed%20existing%20methods%20in%20the%20literature%2C%20achieving%20an%20Equal%0AError%20Rate%20%28EER%29%20of%201.71%25%20and%20a%20False%20Negative%20Rate%20%28FNR%29%20of%202.77%25%20at%20a%20False%0APositive%20Rate%20%28FPR%29%20of%201%25.%20These%20errors%20are%20lower%20by%202.45%25%20and%207.94%25%20than%20the%0Aerrors%20of%20the%20best%20comparison%20method%2C%20respectively.%20Additionally%2C%20we%20introduce%0Aa%20model%20ensemble%20that%20addresses%203D%20spoof%20attacks%20as%20well%2C%20achieving%20an%20EER%20of%0A2.04%25%20and%20an%20FNR%20of%203.83%25%20at%20an%20FPR%20of%201%25.%20Overall%2C%20our%20work%20provides%20a%0Astate-of-the-art%20solution%20for%20the%20challenging%20task%20of%20anti-spoofing%20in%0Anon-calibrated%20systems%20that%20lack%20depth%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24031v3&entry.124074799=Read"},
{"title": "FLOL: Fast Baselines for Real-World Low-Light Enhancement", "author": "Juan C. Benito and Daniel Feijoo and Alvaro Garcia and Marcos V. Conde", "abstract": "  Low-Light Image Enhancement (LLIE) is a key task in computational photography\nand imaging. The problem of enhancing images captured during night or in dark\nenvironments has been well-studied in the image signal processing literature.\nHowever, current deep learning-based solutions struggle with efficiency and\nrobustness in real-world scenarios (e.g. scenes with noise, saturated pixels,\nbad illumination). We propose a lightweight neural network that combines image\nprocessing in the frequency and spatial domains. Our method, FLOL+, is one of\nthe fastest models for this task, achieving state-of-the-art results on popular\nreal scenes datasets such as LOL and LSRW. Moreover, we are able to process\n1080p images under 12ms. Code and models at https://github.com/cidautai/FLOL\n", "link": "http://arxiv.org/abs/2501.09718v1", "date": "2025-01-16", "relevancy": 2.1337, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5751}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5292}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLOL%3A%20Fast%20Baselines%20for%20Real-World%20Low-Light%20Enhancement&body=Title%3A%20FLOL%3A%20Fast%20Baselines%20for%20Real-World%20Low-Light%20Enhancement%0AAuthor%3A%20Juan%20C.%20Benito%20and%20Daniel%20Feijoo%20and%20Alvaro%20Garcia%20and%20Marcos%20V.%20Conde%0AAbstract%3A%20%20%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20is%20a%20key%20task%20in%20computational%20photography%0Aand%20imaging.%20The%20problem%20of%20enhancing%20images%20captured%20during%20night%20or%20in%20dark%0Aenvironments%20has%20been%20well-studied%20in%20the%20image%20signal%20processing%20literature.%0AHowever%2C%20current%20deep%20learning-based%20solutions%20struggle%20with%20efficiency%20and%0Arobustness%20in%20real-world%20scenarios%20%28e.g.%20scenes%20with%20noise%2C%20saturated%20pixels%2C%0Abad%20illumination%29.%20We%20propose%20a%20lightweight%20neural%20network%20that%20combines%20image%0Aprocessing%20in%20the%20frequency%20and%20spatial%20domains.%20Our%20method%2C%20FLOL%2B%2C%20is%20one%20of%0Athe%20fastest%20models%20for%20this%20task%2C%20achieving%20state-of-the-art%20results%20on%20popular%0Areal%20scenes%20datasets%20such%20as%20LOL%20and%20LSRW.%20Moreover%2C%20we%20are%20able%20to%20process%0A1080p%20images%20under%2012ms.%20Code%20and%20models%20at%20https%3A//github.com/cidautai/FLOL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLOL%253A%2520Fast%2520Baselines%2520for%2520Real-World%2520Low-Light%2520Enhancement%26entry.906535625%3DJuan%2520C.%2520Benito%2520and%2520Daniel%2520Feijoo%2520and%2520Alvaro%2520Garcia%2520and%2520Marcos%2520V.%2520Conde%26entry.1292438233%3D%2520%2520Low-Light%2520Image%2520Enhancement%2520%2528LLIE%2529%2520is%2520a%2520key%2520task%2520in%2520computational%2520photography%250Aand%2520imaging.%2520The%2520problem%2520of%2520enhancing%2520images%2520captured%2520during%2520night%2520or%2520in%2520dark%250Aenvironments%2520has%2520been%2520well-studied%2520in%2520the%2520image%2520signal%2520processing%2520literature.%250AHowever%252C%2520current%2520deep%2520learning-based%2520solutions%2520struggle%2520with%2520efficiency%2520and%250Arobustness%2520in%2520real-world%2520scenarios%2520%2528e.g.%2520scenes%2520with%2520noise%252C%2520saturated%2520pixels%252C%250Abad%2520illumination%2529.%2520We%2520propose%2520a%2520lightweight%2520neural%2520network%2520that%2520combines%2520image%250Aprocessing%2520in%2520the%2520frequency%2520and%2520spatial%2520domains.%2520Our%2520method%252C%2520FLOL%252B%252C%2520is%2520one%2520of%250Athe%2520fastest%2520models%2520for%2520this%2520task%252C%2520achieving%2520state-of-the-art%2520results%2520on%2520popular%250Areal%2520scenes%2520datasets%2520such%2520as%2520LOL%2520and%2520LSRW.%2520Moreover%252C%2520we%2520are%2520able%2520to%2520process%250A1080p%2520images%2520under%252012ms.%2520Code%2520and%2520models%2520at%2520https%253A//github.com/cidautai/FLOL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLOL%3A%20Fast%20Baselines%20for%20Real-World%20Low-Light%20Enhancement&entry.906535625=Juan%20C.%20Benito%20and%20Daniel%20Feijoo%20and%20Alvaro%20Garcia%20and%20Marcos%20V.%20Conde&entry.1292438233=%20%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20is%20a%20key%20task%20in%20computational%20photography%0Aand%20imaging.%20The%20problem%20of%20enhancing%20images%20captured%20during%20night%20or%20in%20dark%0Aenvironments%20has%20been%20well-studied%20in%20the%20image%20signal%20processing%20literature.%0AHowever%2C%20current%20deep%20learning-based%20solutions%20struggle%20with%20efficiency%20and%0Arobustness%20in%20real-world%20scenarios%20%28e.g.%20scenes%20with%20noise%2C%20saturated%20pixels%2C%0Abad%20illumination%29.%20We%20propose%20a%20lightweight%20neural%20network%20that%20combines%20image%0Aprocessing%20in%20the%20frequency%20and%20spatial%20domains.%20Our%20method%2C%20FLOL%2B%2C%20is%20one%20of%0Athe%20fastest%20models%20for%20this%20task%2C%20achieving%20state-of-the-art%20results%20on%20popular%0Areal%20scenes%20datasets%20such%20as%20LOL%20and%20LSRW.%20Moreover%2C%20we%20are%20able%20to%20process%0A1080p%20images%20under%2012ms.%20Code%20and%20models%20at%20https%3A//github.com/cidautai/FLOL%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09718v1&entry.124074799=Read"},
{"title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\n  Large Language Models", "author": "Fengli Xu and Qianyue Hao and Zefang Zong and Jingwei Wang and Yunke Zhang and Jingyi Wang and Xiaochong Lan and Jiahui Gong and Tianjian Ouyang and Fanjin Meng and Chenyang Shao and Yuwei Yan and Qinglong Yang and Yiwen Song and Sijian Ren and Xinyuan Hu and Yu Li and Jie Feng and Chen Gao and Yong Li", "abstract": "  Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.\n", "link": "http://arxiv.org/abs/2501.09686v1", "date": "2025-01-16", "relevancy": 2.1231, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5431}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Large%20Reasoning%20Models%3A%20A%20Survey%20of%20Reinforced%20Reasoning%20with%0A%20%20Large%20Language%20Models&body=Title%3A%20Towards%20Large%20Reasoning%20Models%3A%20A%20Survey%20of%20Reinforced%20Reasoning%20with%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Fengli%20Xu%20and%20Qianyue%20Hao%20and%20Zefang%20Zong%20and%20Jingwei%20Wang%20and%20Yunke%20Zhang%20and%20Jingyi%20Wang%20and%20Xiaochong%20Lan%20and%20Jiahui%20Gong%20and%20Tianjian%20Ouyang%20and%20Fanjin%20Meng%20and%20Chenyang%20Shao%20and%20Yuwei%20Yan%20and%20Qinglong%20Yang%20and%20Yiwen%20Song%20and%20Sijian%20Ren%20and%20Xinyuan%20Hu%20and%20Yu%20Li%20and%20Jie%20Feng%20and%20Chen%20Gao%20and%20Yong%20Li%0AAbstract%3A%20%20%20Language%20has%20long%20been%20conceived%20as%20an%20essential%20tool%20for%20human%20reasoning.%0AThe%20breakthrough%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20sparked%20significant%0Aresearch%20interest%20in%20leveraging%20these%20models%20to%20tackle%20complex%20reasoning%20tasks.%0AResearchers%20have%20moved%20beyond%20simple%20autoregressive%20token%20generation%20by%0Aintroducing%20the%20concept%20of%20%22thought%22%20--%20a%20sequence%20of%20tokens%20representing%0Aintermediate%20steps%20in%20the%20reasoning%20process.%20This%20innovative%20paradigm%20enables%0ALLMs%27%20to%20mimic%20complex%20human%20reasoning%20processes%2C%20such%20as%20tree%20search%20and%0Areflective%20thinking.%20Recently%2C%20an%20emerging%20trend%20of%20learning%20to%20reason%20has%0Aapplied%20reinforcement%20learning%20%28RL%29%20to%20train%20LLMs%20to%20master%20reasoning%0Aprocesses.%20This%20approach%20enables%20the%20automatic%20generation%20of%20high-quality%0Areasoning%20trajectories%20through%20trial-and-error%20search%20algorithms%2C%20significantly%0Aexpanding%20LLMs%27%20reasoning%20capacity%20by%20providing%20substantially%20more%20training%0Adata.%20Furthermore%2C%20recent%20studies%20demonstrate%20that%20encouraging%20LLMs%20to%20%22think%22%0Awith%20more%20tokens%20during%20test-time%20inference%20can%20further%20significantly%20boost%0Areasoning%20accuracy.%20Therefore%2C%20the%20train-time%20and%20test-time%20scaling%20combined%20to%0Ashow%20a%20new%20research%20frontier%20--%20a%20path%20toward%20Large%20Reasoning%20Model.%20The%0Aintroduction%20of%20OpenAI%27s%20o1%20series%20marks%20a%20significant%20milestone%20in%20this%0Aresearch%20direction.%20In%20this%20survey%2C%20we%20present%20a%20comprehensive%20review%20of%20recent%0Aprogress%20in%20LLM%20reasoning.%20We%20begin%20by%20introducing%20the%20foundational%20background%0Aof%20LLMs%20and%20then%20explore%20the%20key%20technical%20components%20driving%20the%20development%0Aof%20large%20reasoning%20models%2C%20with%20a%20focus%20on%20automated%20data%20construction%2C%0Alearning-to-reason%20techniques%2C%20and%20test-time%20scaling.%20We%20also%20analyze%20popular%0Aopen-source%20projects%20at%20building%20large%20reasoning%20models%2C%20and%20conclude%20with%20open%0Achallenges%20and%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Large%2520Reasoning%2520Models%253A%2520A%2520Survey%2520of%2520Reinforced%2520Reasoning%2520with%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DFengli%2520Xu%2520and%2520Qianyue%2520Hao%2520and%2520Zefang%2520Zong%2520and%2520Jingwei%2520Wang%2520and%2520Yunke%2520Zhang%2520and%2520Jingyi%2520Wang%2520and%2520Xiaochong%2520Lan%2520and%2520Jiahui%2520Gong%2520and%2520Tianjian%2520Ouyang%2520and%2520Fanjin%2520Meng%2520and%2520Chenyang%2520Shao%2520and%2520Yuwei%2520Yan%2520and%2520Qinglong%2520Yang%2520and%2520Yiwen%2520Song%2520and%2520Sijian%2520Ren%2520and%2520Xinyuan%2520Hu%2520and%2520Yu%2520Li%2520and%2520Jie%2520Feng%2520and%2520Chen%2520Gao%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520Language%2520has%2520long%2520been%2520conceived%2520as%2520an%2520essential%2520tool%2520for%2520human%2520reasoning.%250AThe%2520breakthrough%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520sparked%2520significant%250Aresearch%2520interest%2520in%2520leveraging%2520these%2520models%2520to%2520tackle%2520complex%2520reasoning%2520tasks.%250AResearchers%2520have%2520moved%2520beyond%2520simple%2520autoregressive%2520token%2520generation%2520by%250Aintroducing%2520the%2520concept%2520of%2520%2522thought%2522%2520--%2520a%2520sequence%2520of%2520tokens%2520representing%250Aintermediate%2520steps%2520in%2520the%2520reasoning%2520process.%2520This%2520innovative%2520paradigm%2520enables%250ALLMs%2527%2520to%2520mimic%2520complex%2520human%2520reasoning%2520processes%252C%2520such%2520as%2520tree%2520search%2520and%250Areflective%2520thinking.%2520Recently%252C%2520an%2520emerging%2520trend%2520of%2520learning%2520to%2520reason%2520has%250Aapplied%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520train%2520LLMs%2520to%2520master%2520reasoning%250Aprocesses.%2520This%2520approach%2520enables%2520the%2520automatic%2520generation%2520of%2520high-quality%250Areasoning%2520trajectories%2520through%2520trial-and-error%2520search%2520algorithms%252C%2520significantly%250Aexpanding%2520LLMs%2527%2520reasoning%2520capacity%2520by%2520providing%2520substantially%2520more%2520training%250Adata.%2520Furthermore%252C%2520recent%2520studies%2520demonstrate%2520that%2520encouraging%2520LLMs%2520to%2520%2522think%2522%250Awith%2520more%2520tokens%2520during%2520test-time%2520inference%2520can%2520further%2520significantly%2520boost%250Areasoning%2520accuracy.%2520Therefore%252C%2520the%2520train-time%2520and%2520test-time%2520scaling%2520combined%2520to%250Ashow%2520a%2520new%2520research%2520frontier%2520--%2520a%2520path%2520toward%2520Large%2520Reasoning%2520Model.%2520The%250Aintroduction%2520of%2520OpenAI%2527s%2520o1%2520series%2520marks%2520a%2520significant%2520milestone%2520in%2520this%250Aresearch%2520direction.%2520In%2520this%2520survey%252C%2520we%2520present%2520a%2520comprehensive%2520review%2520of%2520recent%250Aprogress%2520in%2520LLM%2520reasoning.%2520We%2520begin%2520by%2520introducing%2520the%2520foundational%2520background%250Aof%2520LLMs%2520and%2520then%2520explore%2520the%2520key%2520technical%2520components%2520driving%2520the%2520development%250Aof%2520large%2520reasoning%2520models%252C%2520with%2520a%2520focus%2520on%2520automated%2520data%2520construction%252C%250Alearning-to-reason%2520techniques%252C%2520and%2520test-time%2520scaling.%2520We%2520also%2520analyze%2520popular%250Aopen-source%2520projects%2520at%2520building%2520large%2520reasoning%2520models%252C%2520and%2520conclude%2520with%2520open%250Achallenges%2520and%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Large%20Reasoning%20Models%3A%20A%20Survey%20of%20Reinforced%20Reasoning%20with%0A%20%20Large%20Language%20Models&entry.906535625=Fengli%20Xu%20and%20Qianyue%20Hao%20and%20Zefang%20Zong%20and%20Jingwei%20Wang%20and%20Yunke%20Zhang%20and%20Jingyi%20Wang%20and%20Xiaochong%20Lan%20and%20Jiahui%20Gong%20and%20Tianjian%20Ouyang%20and%20Fanjin%20Meng%20and%20Chenyang%20Shao%20and%20Yuwei%20Yan%20and%20Qinglong%20Yang%20and%20Yiwen%20Song%20and%20Sijian%20Ren%20and%20Xinyuan%20Hu%20and%20Yu%20Li%20and%20Jie%20Feng%20and%20Chen%20Gao%20and%20Yong%20Li&entry.1292438233=%20%20Language%20has%20long%20been%20conceived%20as%20an%20essential%20tool%20for%20human%20reasoning.%0AThe%20breakthrough%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20sparked%20significant%0Aresearch%20interest%20in%20leveraging%20these%20models%20to%20tackle%20complex%20reasoning%20tasks.%0AResearchers%20have%20moved%20beyond%20simple%20autoregressive%20token%20generation%20by%0Aintroducing%20the%20concept%20of%20%22thought%22%20--%20a%20sequence%20of%20tokens%20representing%0Aintermediate%20steps%20in%20the%20reasoning%20process.%20This%20innovative%20paradigm%20enables%0ALLMs%27%20to%20mimic%20complex%20human%20reasoning%20processes%2C%20such%20as%20tree%20search%20and%0Areflective%20thinking.%20Recently%2C%20an%20emerging%20trend%20of%20learning%20to%20reason%20has%0Aapplied%20reinforcement%20learning%20%28RL%29%20to%20train%20LLMs%20to%20master%20reasoning%0Aprocesses.%20This%20approach%20enables%20the%20automatic%20generation%20of%20high-quality%0Areasoning%20trajectories%20through%20trial-and-error%20search%20algorithms%2C%20significantly%0Aexpanding%20LLMs%27%20reasoning%20capacity%20by%20providing%20substantially%20more%20training%0Adata.%20Furthermore%2C%20recent%20studies%20demonstrate%20that%20encouraging%20LLMs%20to%20%22think%22%0Awith%20more%20tokens%20during%20test-time%20inference%20can%20further%20significantly%20boost%0Areasoning%20accuracy.%20Therefore%2C%20the%20train-time%20and%20test-time%20scaling%20combined%20to%0Ashow%20a%20new%20research%20frontier%20--%20a%20path%20toward%20Large%20Reasoning%20Model.%20The%0Aintroduction%20of%20OpenAI%27s%20o1%20series%20marks%20a%20significant%20milestone%20in%20this%0Aresearch%20direction.%20In%20this%20survey%2C%20we%20present%20a%20comprehensive%20review%20of%20recent%0Aprogress%20in%20LLM%20reasoning.%20We%20begin%20by%20introducing%20the%20foundational%20background%0Aof%20LLMs%20and%20then%20explore%20the%20key%20technical%20components%20driving%20the%20development%0Aof%20large%20reasoning%20models%2C%20with%20a%20focus%20on%20automated%20data%20construction%2C%0Alearning-to-reason%20techniques%2C%20and%20test-time%20scaling.%20We%20also%20analyze%20popular%0Aopen-source%20projects%20at%20building%20large%20reasoning%20models%2C%20and%20conclude%20with%20open%0Achallenges%20and%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09686v1&entry.124074799=Read"},
{"title": "RE-POSE: Synergizing Reinforcement Learning-Based Partitioning and\n  Offloading for Edge Object Detection", "author": "Jianrui Shi and Yong Zhao and Zeyang Cui and Xiaoming Shen and Minhang Zeng and Xiaojie Liu", "abstract": "  Object detection plays a crucial role in smart video analysis, with\napplications ranging from autonomous driving and security to smart cities.\nHowever, achieving real-time object detection on edge devices presents\nsignificant challenges due to their limited computational resources and the\nhigh demands of deep neural network (DNN)-based detection models, particularly\nwhen processing high-resolution video. Conventional strategies, such as input\ndown-sampling and network up-scaling, often compromise detection accuracy for\nfaster performance or lead to higher inference latency. To address these\nissues, this paper introduces RE-POSE, a Reinforcement Learning (RL)-Driven\nPartitioning and Edge Offloading framework designed to optimize the\naccuracy-latency trade-off in resource-constrained edge environments. Our\napproach features an RL-Based Dynamic Clustering Algorithm (RL-DCA) that\npartitions video frames into non-uniform blocks based on object distribution\nand the computational characteristics of DNNs. Furthermore, a parallel edge\noffloading scheme is implemented to distribute these blocks across multiple\nedge servers for concurrent processing. Experimental evaluations show that\nRE-POSE significantly enhances detection accuracy and reduces inference\nlatency, surpassing existing methods.\n", "link": "http://arxiv.org/abs/2501.09465v1", "date": "2025-01-16", "relevancy": 2.1076, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5329}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5259}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RE-POSE%3A%20Synergizing%20Reinforcement%20Learning-Based%20Partitioning%20and%0A%20%20Offloading%20for%20Edge%20Object%20Detection&body=Title%3A%20RE-POSE%3A%20Synergizing%20Reinforcement%20Learning-Based%20Partitioning%20and%0A%20%20Offloading%20for%20Edge%20Object%20Detection%0AAuthor%3A%20Jianrui%20Shi%20and%20Yong%20Zhao%20and%20Zeyang%20Cui%20and%20Xiaoming%20Shen%20and%20Minhang%20Zeng%20and%20Xiaojie%20Liu%0AAbstract%3A%20%20%20Object%20detection%20plays%20a%20crucial%20role%20in%20smart%20video%20analysis%2C%20with%0Aapplications%20ranging%20from%20autonomous%20driving%20and%20security%20to%20smart%20cities.%0AHowever%2C%20achieving%20real-time%20object%20detection%20on%20edge%20devices%20presents%0Asignificant%20challenges%20due%20to%20their%20limited%20computational%20resources%20and%20the%0Ahigh%20demands%20of%20deep%20neural%20network%20%28DNN%29-based%20detection%20models%2C%20particularly%0Awhen%20processing%20high-resolution%20video.%20Conventional%20strategies%2C%20such%20as%20input%0Adown-sampling%20and%20network%20up-scaling%2C%20often%20compromise%20detection%20accuracy%20for%0Afaster%20performance%20or%20lead%20to%20higher%20inference%20latency.%20To%20address%20these%0Aissues%2C%20this%20paper%20introduces%20RE-POSE%2C%20a%20Reinforcement%20Learning%20%28RL%29-Driven%0APartitioning%20and%20Edge%20Offloading%20framework%20designed%20to%20optimize%20the%0Aaccuracy-latency%20trade-off%20in%20resource-constrained%20edge%20environments.%20Our%0Aapproach%20features%20an%20RL-Based%20Dynamic%20Clustering%20Algorithm%20%28RL-DCA%29%20that%0Apartitions%20video%20frames%20into%20non-uniform%20blocks%20based%20on%20object%20distribution%0Aand%20the%20computational%20characteristics%20of%20DNNs.%20Furthermore%2C%20a%20parallel%20edge%0Aoffloading%20scheme%20is%20implemented%20to%20distribute%20these%20blocks%20across%20multiple%0Aedge%20servers%20for%20concurrent%20processing.%20Experimental%20evaluations%20show%20that%0ARE-POSE%20significantly%20enhances%20detection%20accuracy%20and%20reduces%20inference%0Alatency%2C%20surpassing%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRE-POSE%253A%2520Synergizing%2520Reinforcement%2520Learning-Based%2520Partitioning%2520and%250A%2520%2520Offloading%2520for%2520Edge%2520Object%2520Detection%26entry.906535625%3DJianrui%2520Shi%2520and%2520Yong%2520Zhao%2520and%2520Zeyang%2520Cui%2520and%2520Xiaoming%2520Shen%2520and%2520Minhang%2520Zeng%2520and%2520Xiaojie%2520Liu%26entry.1292438233%3D%2520%2520Object%2520detection%2520plays%2520a%2520crucial%2520role%2520in%2520smart%2520video%2520analysis%252C%2520with%250Aapplications%2520ranging%2520from%2520autonomous%2520driving%2520and%2520security%2520to%2520smart%2520cities.%250AHowever%252C%2520achieving%2520real-time%2520object%2520detection%2520on%2520edge%2520devices%2520presents%250Asignificant%2520challenges%2520due%2520to%2520their%2520limited%2520computational%2520resources%2520and%2520the%250Ahigh%2520demands%2520of%2520deep%2520neural%2520network%2520%2528DNN%2529-based%2520detection%2520models%252C%2520particularly%250Awhen%2520processing%2520high-resolution%2520video.%2520Conventional%2520strategies%252C%2520such%2520as%2520input%250Adown-sampling%2520and%2520network%2520up-scaling%252C%2520often%2520compromise%2520detection%2520accuracy%2520for%250Afaster%2520performance%2520or%2520lead%2520to%2520higher%2520inference%2520latency.%2520To%2520address%2520these%250Aissues%252C%2520this%2520paper%2520introduces%2520RE-POSE%252C%2520a%2520Reinforcement%2520Learning%2520%2528RL%2529-Driven%250APartitioning%2520and%2520Edge%2520Offloading%2520framework%2520designed%2520to%2520optimize%2520the%250Aaccuracy-latency%2520trade-off%2520in%2520resource-constrained%2520edge%2520environments.%2520Our%250Aapproach%2520features%2520an%2520RL-Based%2520Dynamic%2520Clustering%2520Algorithm%2520%2528RL-DCA%2529%2520that%250Apartitions%2520video%2520frames%2520into%2520non-uniform%2520blocks%2520based%2520on%2520object%2520distribution%250Aand%2520the%2520computational%2520characteristics%2520of%2520DNNs.%2520Furthermore%252C%2520a%2520parallel%2520edge%250Aoffloading%2520scheme%2520is%2520implemented%2520to%2520distribute%2520these%2520blocks%2520across%2520multiple%250Aedge%2520servers%2520for%2520concurrent%2520processing.%2520Experimental%2520evaluations%2520show%2520that%250ARE-POSE%2520significantly%2520enhances%2520detection%2520accuracy%2520and%2520reduces%2520inference%250Alatency%252C%2520surpassing%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RE-POSE%3A%20Synergizing%20Reinforcement%20Learning-Based%20Partitioning%20and%0A%20%20Offloading%20for%20Edge%20Object%20Detection&entry.906535625=Jianrui%20Shi%20and%20Yong%20Zhao%20and%20Zeyang%20Cui%20and%20Xiaoming%20Shen%20and%20Minhang%20Zeng%20and%20Xiaojie%20Liu&entry.1292438233=%20%20Object%20detection%20plays%20a%20crucial%20role%20in%20smart%20video%20analysis%2C%20with%0Aapplications%20ranging%20from%20autonomous%20driving%20and%20security%20to%20smart%20cities.%0AHowever%2C%20achieving%20real-time%20object%20detection%20on%20edge%20devices%20presents%0Asignificant%20challenges%20due%20to%20their%20limited%20computational%20resources%20and%20the%0Ahigh%20demands%20of%20deep%20neural%20network%20%28DNN%29-based%20detection%20models%2C%20particularly%0Awhen%20processing%20high-resolution%20video.%20Conventional%20strategies%2C%20such%20as%20input%0Adown-sampling%20and%20network%20up-scaling%2C%20often%20compromise%20detection%20accuracy%20for%0Afaster%20performance%20or%20lead%20to%20higher%20inference%20latency.%20To%20address%20these%0Aissues%2C%20this%20paper%20introduces%20RE-POSE%2C%20a%20Reinforcement%20Learning%20%28RL%29-Driven%0APartitioning%20and%20Edge%20Offloading%20framework%20designed%20to%20optimize%20the%0Aaccuracy-latency%20trade-off%20in%20resource-constrained%20edge%20environments.%20Our%0Aapproach%20features%20an%20RL-Based%20Dynamic%20Clustering%20Algorithm%20%28RL-DCA%29%20that%0Apartitions%20video%20frames%20into%20non-uniform%20blocks%20based%20on%20object%20distribution%0Aand%20the%20computational%20characteristics%20of%20DNNs.%20Furthermore%2C%20a%20parallel%20edge%0Aoffloading%20scheme%20is%20implemented%20to%20distribute%20these%20blocks%20across%20multiple%0Aedge%20servers%20for%20concurrent%20processing.%20Experimental%20evaluations%20show%20that%0ARE-POSE%20significantly%20enhances%20detection%20accuracy%20and%20reduces%20inference%0Alatency%2C%20surpassing%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09465v1&entry.124074799=Read"},
{"title": "MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning", "author": "Tieyuan Chen and Huabin Liu and Yi Wang and Yihang Chen and Tianyao He and Chaofan Gan and Huanyu He and Weiyao Lin", "abstract": "  Video causal reasoning aims to achieve a high-level understanding of videos\nfrom a causal perspective. However, it exhibits limitations in its scope,\nprimarily executed in a question-answering paradigm and focusing on brief video\nsegments containing isolated events and basic causal relations, lacking\ncomprehensive and structured causality analysis for videos with multiple\ninterconnected events. To fill this gap, we introduce a new task and dataset,\nMulti-Event Causal Discovery (MECD). It aims to uncover the causal relations\nbetween events distributed chronologically across long videos. Given visual\nsegments and textual descriptions of events, MECD identifies the causal\nassociations between these events to derive a comprehensive and structured\nevent-level video causal graph explaining why and how the result event\noccurred. To address the challenges of MECD, we devise a novel framework\ninspired by the Granger Causality method, incorporating an efficient mask-based\nevent prediction model to perform an Event Granger Test. It estimates causality\nby comparing the predicted result event when premise events are masked versus\nunmasked. Furthermore, we integrate causal inference techniques such as\nfront-door adjustment and counterfactual inference to mitigate challenges in\nMECD like causality confounding and illusory causality. Additionally, context\nchain reasoning is introduced to conduct more robust and generalized reasoning.\nExperiments validate the effectiveness of our framework in reasoning complete\ncausal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,\nrespectively. Further experiments demonstrate that causal relation graphs can\nalso contribute to downstream video understanding tasks such as video question\nanswering and video event prediction.\n", "link": "http://arxiv.org/abs/2501.07227v2", "date": "2025-01-16", "relevancy": 2.0959, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5346}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MECD%2B%3A%20Unlocking%20Event-Level%20Causal%20Graph%20Discovery%20for%20Video%20Reasoning&body=Title%3A%20MECD%2B%3A%20Unlocking%20Event-Level%20Causal%20Graph%20Discovery%20for%20Video%20Reasoning%0AAuthor%3A%20Tieyuan%20Chen%20and%20Huabin%20Liu%20and%20Yi%20Wang%20and%20Yihang%20Chen%20and%20Tianyao%20He%20and%20Chaofan%20Gan%20and%20Huanyu%20He%20and%20Weiyao%20Lin%0AAbstract%3A%20%20%20Video%20causal%20reasoning%20aims%20to%20achieve%20a%20high-level%20understanding%20of%20videos%0Afrom%20a%20causal%20perspective.%20However%2C%20it%20exhibits%20limitations%20in%20its%20scope%2C%0Aprimarily%20executed%20in%20a%20question-answering%20paradigm%20and%20focusing%20on%20brief%20video%0Asegments%20containing%20isolated%20events%20and%20basic%20causal%20relations%2C%20lacking%0Acomprehensive%20and%20structured%20causality%20analysis%20for%20videos%20with%20multiple%0Ainterconnected%20events.%20To%20fill%20this%20gap%2C%20we%20introduce%20a%20new%20task%20and%20dataset%2C%0AMulti-Event%20Causal%20Discovery%20%28MECD%29.%20It%20aims%20to%20uncover%20the%20causal%20relations%0Abetween%20events%20distributed%20chronologically%20across%20long%20videos.%20Given%20visual%0Asegments%20and%20textual%20descriptions%20of%20events%2C%20MECD%20identifies%20the%20causal%0Aassociations%20between%20these%20events%20to%20derive%20a%20comprehensive%20and%20structured%0Aevent-level%20video%20causal%20graph%20explaining%20why%20and%20how%20the%20result%20event%0Aoccurred.%20To%20address%20the%20challenges%20of%20MECD%2C%20we%20devise%20a%20novel%20framework%0Ainspired%20by%20the%20Granger%20Causality%20method%2C%20incorporating%20an%20efficient%20mask-based%0Aevent%20prediction%20model%20to%20perform%20an%20Event%20Granger%20Test.%20It%20estimates%20causality%0Aby%20comparing%20the%20predicted%20result%20event%20when%20premise%20events%20are%20masked%20versus%0Aunmasked.%20Furthermore%2C%20we%20integrate%20causal%20inference%20techniques%20such%20as%0Afront-door%20adjustment%20and%20counterfactual%20inference%20to%20mitigate%20challenges%20in%0AMECD%20like%20causality%20confounding%20and%20illusory%20causality.%20Additionally%2C%20context%0Achain%20reasoning%20is%20introduced%20to%20conduct%20more%20robust%20and%20generalized%20reasoning.%0AExperiments%20validate%20the%20effectiveness%20of%20our%20framework%20in%20reasoning%20complete%0Acausal%20relations%2C%20outperforming%20GPT-4o%20and%20VideoChat2%20by%205.77%25%20and%202.70%25%2C%0Arespectively.%20Further%20experiments%20demonstrate%20that%20causal%20relation%20graphs%20can%0Aalso%20contribute%20to%20downstream%20video%20understanding%20tasks%20such%20as%20video%20question%0Aanswering%20and%20video%20event%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07227v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMECD%252B%253A%2520Unlocking%2520Event-Level%2520Causal%2520Graph%2520Discovery%2520for%2520Video%2520Reasoning%26entry.906535625%3DTieyuan%2520Chen%2520and%2520Huabin%2520Liu%2520and%2520Yi%2520Wang%2520and%2520Yihang%2520Chen%2520and%2520Tianyao%2520He%2520and%2520Chaofan%2520Gan%2520and%2520Huanyu%2520He%2520and%2520Weiyao%2520Lin%26entry.1292438233%3D%2520%2520Video%2520causal%2520reasoning%2520aims%2520to%2520achieve%2520a%2520high-level%2520understanding%2520of%2520videos%250Afrom%2520a%2520causal%2520perspective.%2520However%252C%2520it%2520exhibits%2520limitations%2520in%2520its%2520scope%252C%250Aprimarily%2520executed%2520in%2520a%2520question-answering%2520paradigm%2520and%2520focusing%2520on%2520brief%2520video%250Asegments%2520containing%2520isolated%2520events%2520and%2520basic%2520causal%2520relations%252C%2520lacking%250Acomprehensive%2520and%2520structured%2520causality%2520analysis%2520for%2520videos%2520with%2520multiple%250Ainterconnected%2520events.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520a%2520new%2520task%2520and%2520dataset%252C%250AMulti-Event%2520Causal%2520Discovery%2520%2528MECD%2529.%2520It%2520aims%2520to%2520uncover%2520the%2520causal%2520relations%250Abetween%2520events%2520distributed%2520chronologically%2520across%2520long%2520videos.%2520Given%2520visual%250Asegments%2520and%2520textual%2520descriptions%2520of%2520events%252C%2520MECD%2520identifies%2520the%2520causal%250Aassociations%2520between%2520these%2520events%2520to%2520derive%2520a%2520comprehensive%2520and%2520structured%250Aevent-level%2520video%2520causal%2520graph%2520explaining%2520why%2520and%2520how%2520the%2520result%2520event%250Aoccurred.%2520To%2520address%2520the%2520challenges%2520of%2520MECD%252C%2520we%2520devise%2520a%2520novel%2520framework%250Ainspired%2520by%2520the%2520Granger%2520Causality%2520method%252C%2520incorporating%2520an%2520efficient%2520mask-based%250Aevent%2520prediction%2520model%2520to%2520perform%2520an%2520Event%2520Granger%2520Test.%2520It%2520estimates%2520causality%250Aby%2520comparing%2520the%2520predicted%2520result%2520event%2520when%2520premise%2520events%2520are%2520masked%2520versus%250Aunmasked.%2520Furthermore%252C%2520we%2520integrate%2520causal%2520inference%2520techniques%2520such%2520as%250Afront-door%2520adjustment%2520and%2520counterfactual%2520inference%2520to%2520mitigate%2520challenges%2520in%250AMECD%2520like%2520causality%2520confounding%2520and%2520illusory%2520causality.%2520Additionally%252C%2520context%250Achain%2520reasoning%2520is%2520introduced%2520to%2520conduct%2520more%2520robust%2520and%2520generalized%2520reasoning.%250AExperiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520framework%2520in%2520reasoning%2520complete%250Acausal%2520relations%252C%2520outperforming%2520GPT-4o%2520and%2520VideoChat2%2520by%25205.77%2525%2520and%25202.70%2525%252C%250Arespectively.%2520Further%2520experiments%2520demonstrate%2520that%2520causal%2520relation%2520graphs%2520can%250Aalso%2520contribute%2520to%2520downstream%2520video%2520understanding%2520tasks%2520such%2520as%2520video%2520question%250Aanswering%2520and%2520video%2520event%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07227v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MECD%2B%3A%20Unlocking%20Event-Level%20Causal%20Graph%20Discovery%20for%20Video%20Reasoning&entry.906535625=Tieyuan%20Chen%20and%20Huabin%20Liu%20and%20Yi%20Wang%20and%20Yihang%20Chen%20and%20Tianyao%20He%20and%20Chaofan%20Gan%20and%20Huanyu%20He%20and%20Weiyao%20Lin&entry.1292438233=%20%20Video%20causal%20reasoning%20aims%20to%20achieve%20a%20high-level%20understanding%20of%20videos%0Afrom%20a%20causal%20perspective.%20However%2C%20it%20exhibits%20limitations%20in%20its%20scope%2C%0Aprimarily%20executed%20in%20a%20question-answering%20paradigm%20and%20focusing%20on%20brief%20video%0Asegments%20containing%20isolated%20events%20and%20basic%20causal%20relations%2C%20lacking%0Acomprehensive%20and%20structured%20causality%20analysis%20for%20videos%20with%20multiple%0Ainterconnected%20events.%20To%20fill%20this%20gap%2C%20we%20introduce%20a%20new%20task%20and%20dataset%2C%0AMulti-Event%20Causal%20Discovery%20%28MECD%29.%20It%20aims%20to%20uncover%20the%20causal%20relations%0Abetween%20events%20distributed%20chronologically%20across%20long%20videos.%20Given%20visual%0Asegments%20and%20textual%20descriptions%20of%20events%2C%20MECD%20identifies%20the%20causal%0Aassociations%20between%20these%20events%20to%20derive%20a%20comprehensive%20and%20structured%0Aevent-level%20video%20causal%20graph%20explaining%20why%20and%20how%20the%20result%20event%0Aoccurred.%20To%20address%20the%20challenges%20of%20MECD%2C%20we%20devise%20a%20novel%20framework%0Ainspired%20by%20the%20Granger%20Causality%20method%2C%20incorporating%20an%20efficient%20mask-based%0Aevent%20prediction%20model%20to%20perform%20an%20Event%20Granger%20Test.%20It%20estimates%20causality%0Aby%20comparing%20the%20predicted%20result%20event%20when%20premise%20events%20are%20masked%20versus%0Aunmasked.%20Furthermore%2C%20we%20integrate%20causal%20inference%20techniques%20such%20as%0Afront-door%20adjustment%20and%20counterfactual%20inference%20to%20mitigate%20challenges%20in%0AMECD%20like%20causality%20confounding%20and%20illusory%20causality.%20Additionally%2C%20context%0Achain%20reasoning%20is%20introduced%20to%20conduct%20more%20robust%20and%20generalized%20reasoning.%0AExperiments%20validate%20the%20effectiveness%20of%20our%20framework%20in%20reasoning%20complete%0Acausal%20relations%2C%20outperforming%20GPT-4o%20and%20VideoChat2%20by%205.77%25%20and%202.70%25%2C%0Arespectively.%20Further%20experiments%20demonstrate%20that%20causal%20relation%20graphs%20can%0Aalso%20contribute%20to%20downstream%20video%20understanding%20tasks%20such%20as%20video%20question%0Aanswering%20and%20video%20event%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07227v2&entry.124074799=Read"},
{"title": "STROOBnet Optimization via GPU-Accelerated Proximal Recurrence\n  Strategies", "author": "Ted Edward Holmberg and Mahdi Abdelguerfi and Elias Ioup", "abstract": "  Spatiotemporal networks' observational capabilities are crucial for accurate\ndata gathering and informed decisions across multiple sectors. This study\nfocuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network\n(STROOBnet), linking observational nodes (e.g., surveillance cameras) to events\nwithin defined geographical regions, enabling efficient monitoring. Using data\nfrom Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New\nOrleans, where RTCC combats rising crime amidst reduced police presence, we\naddress the network's initial observational imbalances. Aiming for uniform\nobservational efficacy, we propose the Proximal Recurrence approach. It\noutperformed traditional clustering methods like k-means and DBSCAN by offering\nholistic event frequency and spatial consideration, enhancing observational\ncoverage.\n", "link": "http://arxiv.org/abs/2404.14388v3", "date": "2025-01-16", "relevancy": 2.0929, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5513}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5051}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STROOBnet%20Optimization%20via%20GPU-Accelerated%20Proximal%20Recurrence%0A%20%20Strategies&body=Title%3A%20STROOBnet%20Optimization%20via%20GPU-Accelerated%20Proximal%20Recurrence%0A%20%20Strategies%0AAuthor%3A%20Ted%20Edward%20Holmberg%20and%20Mahdi%20Abdelguerfi%20and%20Elias%20Ioup%0AAbstract%3A%20%20%20Spatiotemporal%20networks%27%20observational%20capabilities%20are%20crucial%20for%20accurate%0Adata%20gathering%20and%20informed%20decisions%20across%20multiple%20sectors.%20This%20study%0Afocuses%20on%20the%20Spatiotemporal%20Ranged%20Observer-Observable%20Bipartite%20Network%0A%28STROOBnet%29%2C%20linking%20observational%20nodes%20%28e.g.%2C%20surveillance%20cameras%29%20to%20events%0Awithin%20defined%20geographical%20regions%2C%20enabling%20efficient%20monitoring.%20Using%20data%0Afrom%20Real-Time%20Crime%20Camera%20%28RTCC%29%20systems%20and%20Calls%20for%20Service%20%28CFS%29%20in%20New%0AOrleans%2C%20where%20RTCC%20combats%20rising%20crime%20amidst%20reduced%20police%20presence%2C%20we%0Aaddress%20the%20network%27s%20initial%20observational%20imbalances.%20Aiming%20for%20uniform%0Aobservational%20efficacy%2C%20we%20propose%20the%20Proximal%20Recurrence%20approach.%20It%0Aoutperformed%20traditional%20clustering%20methods%20like%20k-means%20and%20DBSCAN%20by%20offering%0Aholistic%20event%20frequency%20and%20spatial%20consideration%2C%20enhancing%20observational%0Acoverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14388v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTROOBnet%2520Optimization%2520via%2520GPU-Accelerated%2520Proximal%2520Recurrence%250A%2520%2520Strategies%26entry.906535625%3DTed%2520Edward%2520Holmberg%2520and%2520Mahdi%2520Abdelguerfi%2520and%2520Elias%2520Ioup%26entry.1292438233%3D%2520%2520Spatiotemporal%2520networks%2527%2520observational%2520capabilities%2520are%2520crucial%2520for%2520accurate%250Adata%2520gathering%2520and%2520informed%2520decisions%2520across%2520multiple%2520sectors.%2520This%2520study%250Afocuses%2520on%2520the%2520Spatiotemporal%2520Ranged%2520Observer-Observable%2520Bipartite%2520Network%250A%2528STROOBnet%2529%252C%2520linking%2520observational%2520nodes%2520%2528e.g.%252C%2520surveillance%2520cameras%2529%2520to%2520events%250Awithin%2520defined%2520geographical%2520regions%252C%2520enabling%2520efficient%2520monitoring.%2520Using%2520data%250Afrom%2520Real-Time%2520Crime%2520Camera%2520%2528RTCC%2529%2520systems%2520and%2520Calls%2520for%2520Service%2520%2528CFS%2529%2520in%2520New%250AOrleans%252C%2520where%2520RTCC%2520combats%2520rising%2520crime%2520amidst%2520reduced%2520police%2520presence%252C%2520we%250Aaddress%2520the%2520network%2527s%2520initial%2520observational%2520imbalances.%2520Aiming%2520for%2520uniform%250Aobservational%2520efficacy%252C%2520we%2520propose%2520the%2520Proximal%2520Recurrence%2520approach.%2520It%250Aoutperformed%2520traditional%2520clustering%2520methods%2520like%2520k-means%2520and%2520DBSCAN%2520by%2520offering%250Aholistic%2520event%2520frequency%2520and%2520spatial%2520consideration%252C%2520enhancing%2520observational%250Acoverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14388v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STROOBnet%20Optimization%20via%20GPU-Accelerated%20Proximal%20Recurrence%0A%20%20Strategies&entry.906535625=Ted%20Edward%20Holmberg%20and%20Mahdi%20Abdelguerfi%20and%20Elias%20Ioup&entry.1292438233=%20%20Spatiotemporal%20networks%27%20observational%20capabilities%20are%20crucial%20for%20accurate%0Adata%20gathering%20and%20informed%20decisions%20across%20multiple%20sectors.%20This%20study%0Afocuses%20on%20the%20Spatiotemporal%20Ranged%20Observer-Observable%20Bipartite%20Network%0A%28STROOBnet%29%2C%20linking%20observational%20nodes%20%28e.g.%2C%20surveillance%20cameras%29%20to%20events%0Awithin%20defined%20geographical%20regions%2C%20enabling%20efficient%20monitoring.%20Using%20data%0Afrom%20Real-Time%20Crime%20Camera%20%28RTCC%29%20systems%20and%20Calls%20for%20Service%20%28CFS%29%20in%20New%0AOrleans%2C%20where%20RTCC%20combats%20rising%20crime%20amidst%20reduced%20police%20presence%2C%20we%0Aaddress%20the%20network%27s%20initial%20observational%20imbalances.%20Aiming%20for%20uniform%0Aobservational%20efficacy%2C%20we%20propose%20the%20Proximal%20Recurrence%20approach.%20It%0Aoutperformed%20traditional%20clustering%20methods%20like%20k-means%20and%20DBSCAN%20by%20offering%0Aholistic%20event%20frequency%20and%20spatial%20consideration%2C%20enhancing%20observational%0Acoverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14388v3&entry.124074799=Read"},
{"title": "Teaching Wav2Vec2 the Language of the Brain", "author": "Tobias Fiedler and Leon Hermann and Florian M\u00fcller and Sarel Cohen and Peter Chin and Tobias Friedrich and Eilon Vaadia", "abstract": "  The decoding of continuously spoken speech from neuronal activity has the\npotential to become an important clinical solution for paralyzed patients. Deep\nLearning Brain Computer Interfaces (BCIs) have recently successfully mapped\nneuronal activity to text contents in subjects who attempted to formulate\nspeech. However, only small BCI datasets are available. In contrast, labeled\ndata and pre-trained models for the closely related task of speech recognition\nfrom audio are widely available. One such model is Wav2Vec2 which has been\ntrained in a self-supervised fashion to create meaningful representations of\nspeech audio data. In this study, we show that patterns learned by Wav2Vec2 are\ntransferable to brain data. Specifically, we replace its audio feature\nextractor with an untrained Brain Feature Extractor (BFE) model. We then\nexecute full fine-tuning with pre-trained weights for Wav2Vec2, training ''from\nscratch'' without pre-trained weights as well as freezing a pre-trained\nWav2Vec2 and training only the BFE each for 45 different BFE architectures.\nAcross these experiments, the best run is from full fine-tuning with\npre-trained weights, achieving a Character Error Rate (CER) of 18.54\\%,\noutperforming the best training from scratch run by 20.46\\% and that of frozen\nWav2Vec2 training by 15.92\\% percentage points. These results indicate that\nknowledge transfer from audio speech recognition to brain decoding is possible\nand significantly improves brain decoding performance for the same\narchitectures. Related source code is available at\nhttps://github.com/tfiedlerdev/Wav2Vec2ForBrain.\n", "link": "http://arxiv.org/abs/2501.09459v1", "date": "2025-01-16", "relevancy": 2.0744, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teaching%20Wav2Vec2%20the%20Language%20of%20the%20Brain&body=Title%3A%20Teaching%20Wav2Vec2%20the%20Language%20of%20the%20Brain%0AAuthor%3A%20Tobias%20Fiedler%20and%20Leon%20Hermann%20and%20Florian%20M%C3%BCller%20and%20Sarel%20Cohen%20and%20Peter%20Chin%20and%20Tobias%20Friedrich%20and%20Eilon%20Vaadia%0AAbstract%3A%20%20%20The%20decoding%20of%20continuously%20spoken%20speech%20from%20neuronal%20activity%20has%20the%0Apotential%20to%20become%20an%20important%20clinical%20solution%20for%20paralyzed%20patients.%20Deep%0ALearning%20Brain%20Computer%20Interfaces%20%28BCIs%29%20have%20recently%20successfully%20mapped%0Aneuronal%20activity%20to%20text%20contents%20in%20subjects%20who%20attempted%20to%20formulate%0Aspeech.%20However%2C%20only%20small%20BCI%20datasets%20are%20available.%20In%20contrast%2C%20labeled%0Adata%20and%20pre-trained%20models%20for%20the%20closely%20related%20task%20of%20speech%20recognition%0Afrom%20audio%20are%20widely%20available.%20One%20such%20model%20is%20Wav2Vec2%20which%20has%20been%0Atrained%20in%20a%20self-supervised%20fashion%20to%20create%20meaningful%20representations%20of%0Aspeech%20audio%20data.%20In%20this%20study%2C%20we%20show%20that%20patterns%20learned%20by%20Wav2Vec2%20are%0Atransferable%20to%20brain%20data.%20Specifically%2C%20we%20replace%20its%20audio%20feature%0Aextractor%20with%20an%20untrained%20Brain%20Feature%20Extractor%20%28BFE%29%20model.%20We%20then%0Aexecute%20full%20fine-tuning%20with%20pre-trained%20weights%20for%20Wav2Vec2%2C%20training%20%27%27from%0Ascratch%27%27%20without%20pre-trained%20weights%20as%20well%20as%20freezing%20a%20pre-trained%0AWav2Vec2%20and%20training%20only%20the%20BFE%20each%20for%2045%20different%20BFE%20architectures.%0AAcross%20these%20experiments%2C%20the%20best%20run%20is%20from%20full%20fine-tuning%20with%0Apre-trained%20weights%2C%20achieving%20a%20Character%20Error%20Rate%20%28CER%29%20of%2018.54%5C%25%2C%0Aoutperforming%20the%20best%20training%20from%20scratch%20run%20by%2020.46%5C%25%20and%20that%20of%20frozen%0AWav2Vec2%20training%20by%2015.92%5C%25%20percentage%20points.%20These%20results%20indicate%20that%0Aknowledge%20transfer%20from%20audio%20speech%20recognition%20to%20brain%20decoding%20is%20possible%0Aand%20significantly%20improves%20brain%20decoding%20performance%20for%20the%20same%0Aarchitectures.%20Related%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/tfiedlerdev/Wav2Vec2ForBrain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeaching%2520Wav2Vec2%2520the%2520Language%2520of%2520the%2520Brain%26entry.906535625%3DTobias%2520Fiedler%2520and%2520Leon%2520Hermann%2520and%2520Florian%2520M%25C3%25BCller%2520and%2520Sarel%2520Cohen%2520and%2520Peter%2520Chin%2520and%2520Tobias%2520Friedrich%2520and%2520Eilon%2520Vaadia%26entry.1292438233%3D%2520%2520The%2520decoding%2520of%2520continuously%2520spoken%2520speech%2520from%2520neuronal%2520activity%2520has%2520the%250Apotential%2520to%2520become%2520an%2520important%2520clinical%2520solution%2520for%2520paralyzed%2520patients.%2520Deep%250ALearning%2520Brain%2520Computer%2520Interfaces%2520%2528BCIs%2529%2520have%2520recently%2520successfully%2520mapped%250Aneuronal%2520activity%2520to%2520text%2520contents%2520in%2520subjects%2520who%2520attempted%2520to%2520formulate%250Aspeech.%2520However%252C%2520only%2520small%2520BCI%2520datasets%2520are%2520available.%2520In%2520contrast%252C%2520labeled%250Adata%2520and%2520pre-trained%2520models%2520for%2520the%2520closely%2520related%2520task%2520of%2520speech%2520recognition%250Afrom%2520audio%2520are%2520widely%2520available.%2520One%2520such%2520model%2520is%2520Wav2Vec2%2520which%2520has%2520been%250Atrained%2520in%2520a%2520self-supervised%2520fashion%2520to%2520create%2520meaningful%2520representations%2520of%250Aspeech%2520audio%2520data.%2520In%2520this%2520study%252C%2520we%2520show%2520that%2520patterns%2520learned%2520by%2520Wav2Vec2%2520are%250Atransferable%2520to%2520brain%2520data.%2520Specifically%252C%2520we%2520replace%2520its%2520audio%2520feature%250Aextractor%2520with%2520an%2520untrained%2520Brain%2520Feature%2520Extractor%2520%2528BFE%2529%2520model.%2520We%2520then%250Aexecute%2520full%2520fine-tuning%2520with%2520pre-trained%2520weights%2520for%2520Wav2Vec2%252C%2520training%2520%2527%2527from%250Ascratch%2527%2527%2520without%2520pre-trained%2520weights%2520as%2520well%2520as%2520freezing%2520a%2520pre-trained%250AWav2Vec2%2520and%2520training%2520only%2520the%2520BFE%2520each%2520for%252045%2520different%2520BFE%2520architectures.%250AAcross%2520these%2520experiments%252C%2520the%2520best%2520run%2520is%2520from%2520full%2520fine-tuning%2520with%250Apre-trained%2520weights%252C%2520achieving%2520a%2520Character%2520Error%2520Rate%2520%2528CER%2529%2520of%252018.54%255C%2525%252C%250Aoutperforming%2520the%2520best%2520training%2520from%2520scratch%2520run%2520by%252020.46%255C%2525%2520and%2520that%2520of%2520frozen%250AWav2Vec2%2520training%2520by%252015.92%255C%2525%2520percentage%2520points.%2520These%2520results%2520indicate%2520that%250Aknowledge%2520transfer%2520from%2520audio%2520speech%2520recognition%2520to%2520brain%2520decoding%2520is%2520possible%250Aand%2520significantly%2520improves%2520brain%2520decoding%2520performance%2520for%2520the%2520same%250Aarchitectures.%2520Related%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/tfiedlerdev/Wav2Vec2ForBrain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaching%20Wav2Vec2%20the%20Language%20of%20the%20Brain&entry.906535625=Tobias%20Fiedler%20and%20Leon%20Hermann%20and%20Florian%20M%C3%BCller%20and%20Sarel%20Cohen%20and%20Peter%20Chin%20and%20Tobias%20Friedrich%20and%20Eilon%20Vaadia&entry.1292438233=%20%20The%20decoding%20of%20continuously%20spoken%20speech%20from%20neuronal%20activity%20has%20the%0Apotential%20to%20become%20an%20important%20clinical%20solution%20for%20paralyzed%20patients.%20Deep%0ALearning%20Brain%20Computer%20Interfaces%20%28BCIs%29%20have%20recently%20successfully%20mapped%0Aneuronal%20activity%20to%20text%20contents%20in%20subjects%20who%20attempted%20to%20formulate%0Aspeech.%20However%2C%20only%20small%20BCI%20datasets%20are%20available.%20In%20contrast%2C%20labeled%0Adata%20and%20pre-trained%20models%20for%20the%20closely%20related%20task%20of%20speech%20recognition%0Afrom%20audio%20are%20widely%20available.%20One%20such%20model%20is%20Wav2Vec2%20which%20has%20been%0Atrained%20in%20a%20self-supervised%20fashion%20to%20create%20meaningful%20representations%20of%0Aspeech%20audio%20data.%20In%20this%20study%2C%20we%20show%20that%20patterns%20learned%20by%20Wav2Vec2%20are%0Atransferable%20to%20brain%20data.%20Specifically%2C%20we%20replace%20its%20audio%20feature%0Aextractor%20with%20an%20untrained%20Brain%20Feature%20Extractor%20%28BFE%29%20model.%20We%20then%0Aexecute%20full%20fine-tuning%20with%20pre-trained%20weights%20for%20Wav2Vec2%2C%20training%20%27%27from%0Ascratch%27%27%20without%20pre-trained%20weights%20as%20well%20as%20freezing%20a%20pre-trained%0AWav2Vec2%20and%20training%20only%20the%20BFE%20each%20for%2045%20different%20BFE%20architectures.%0AAcross%20these%20experiments%2C%20the%20best%20run%20is%20from%20full%20fine-tuning%20with%0Apre-trained%20weights%2C%20achieving%20a%20Character%20Error%20Rate%20%28CER%29%20of%2018.54%5C%25%2C%0Aoutperforming%20the%20best%20training%20from%20scratch%20run%20by%2020.46%5C%25%20and%20that%20of%20frozen%0AWav2Vec2%20training%20by%2015.92%5C%25%20percentage%20points.%20These%20results%20indicate%20that%0Aknowledge%20transfer%20from%20audio%20speech%20recognition%20to%20brain%20decoding%20is%20possible%0Aand%20significantly%20improves%20brain%20decoding%20performance%20for%20the%20same%0Aarchitectures.%20Related%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/tfiedlerdev/Wav2Vec2ForBrain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09459v1&entry.124074799=Read"},
{"title": "Multi-task deep-learning for sleep event detection and stage\n  classification", "author": "Adriana Anido-Alonso and Diego Alvarez-Estevez", "abstract": "  Polysomnographic sleep analysis is the standard clinical method to accurately\ndiagnose and treat sleep disorders. It is an intricate process which involves\nthe manual identification, classification, and location of multiple sleep event\npatterns. This is complex, for which identification of different types of\nevents involves focusing on different subsets of signals, resulting on an\niterative time-consuming process entailing several visual analysis passes. In\nthis paper we propose a multi-task deep-learning approach for the simultaneous\ndetection of sleep events and hypnogram construction in one single pass. Taking\nas reference state-of-the-art methodology for object-detection in the field of\nComputer Vision, we reformulate the problem for the analysis of multi-variate\ntime sequences, and more specifically for pattern detection in the sleep\nanalysis scenario. We investigate the performance of the resulting method in\nidentifying different assembly combinations of EEG arousals, respiratory events\n(apneas and hypopneas) and sleep stages, also considering different input\nsignal montage configurations. Furthermore, we evaluate our approach using two\nindependent datasets, assessing true-generalization effects involving local and\nexternal validation scenarios. Based on our results, we analyze and discuss our\nmethod's capabilities and its potential wide-range applicability across\ndifferent settings and datasets.\n", "link": "http://arxiv.org/abs/2501.09519v1", "date": "2025-01-16", "relevancy": 2.0686, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5555}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5049}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-task%20deep-learning%20for%20sleep%20event%20detection%20and%20stage%0A%20%20classification&body=Title%3A%20Multi-task%20deep-learning%20for%20sleep%20event%20detection%20and%20stage%0A%20%20classification%0AAuthor%3A%20Adriana%20Anido-Alonso%20and%20Diego%20Alvarez-Estevez%0AAbstract%3A%20%20%20Polysomnographic%20sleep%20analysis%20is%20the%20standard%20clinical%20method%20to%20accurately%0Adiagnose%20and%20treat%20sleep%20disorders.%20It%20is%20an%20intricate%20process%20which%20involves%0Athe%20manual%20identification%2C%20classification%2C%20and%20location%20of%20multiple%20sleep%20event%0Apatterns.%20This%20is%20complex%2C%20for%20which%20identification%20of%20different%20types%20of%0Aevents%20involves%20focusing%20on%20different%20subsets%20of%20signals%2C%20resulting%20on%20an%0Aiterative%20time-consuming%20process%20entailing%20several%20visual%20analysis%20passes.%20In%0Athis%20paper%20we%20propose%20a%20multi-task%20deep-learning%20approach%20for%20the%20simultaneous%0Adetection%20of%20sleep%20events%20and%20hypnogram%20construction%20in%20one%20single%20pass.%20Taking%0Aas%20reference%20state-of-the-art%20methodology%20for%20object-detection%20in%20the%20field%20of%0AComputer%20Vision%2C%20we%20reformulate%20the%20problem%20for%20the%20analysis%20of%20multi-variate%0Atime%20sequences%2C%20and%20more%20specifically%20for%20pattern%20detection%20in%20the%20sleep%0Aanalysis%20scenario.%20We%20investigate%20the%20performance%20of%20the%20resulting%20method%20in%0Aidentifying%20different%20assembly%20combinations%20of%20EEG%20arousals%2C%20respiratory%20events%0A%28apneas%20and%20hypopneas%29%20and%20sleep%20stages%2C%20also%20considering%20different%20input%0Asignal%20montage%20configurations.%20Furthermore%2C%20we%20evaluate%20our%20approach%20using%20two%0Aindependent%20datasets%2C%20assessing%20true-generalization%20effects%20involving%20local%20and%0Aexternal%20validation%20scenarios.%20Based%20on%20our%20results%2C%20we%20analyze%20and%20discuss%20our%0Amethod%27s%20capabilities%20and%20its%20potential%20wide-range%20applicability%20across%0Adifferent%20settings%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-task%2520deep-learning%2520for%2520sleep%2520event%2520detection%2520and%2520stage%250A%2520%2520classification%26entry.906535625%3DAdriana%2520Anido-Alonso%2520and%2520Diego%2520Alvarez-Estevez%26entry.1292438233%3D%2520%2520Polysomnographic%2520sleep%2520analysis%2520is%2520the%2520standard%2520clinical%2520method%2520to%2520accurately%250Adiagnose%2520and%2520treat%2520sleep%2520disorders.%2520It%2520is%2520an%2520intricate%2520process%2520which%2520involves%250Athe%2520manual%2520identification%252C%2520classification%252C%2520and%2520location%2520of%2520multiple%2520sleep%2520event%250Apatterns.%2520This%2520is%2520complex%252C%2520for%2520which%2520identification%2520of%2520different%2520types%2520of%250Aevents%2520involves%2520focusing%2520on%2520different%2520subsets%2520of%2520signals%252C%2520resulting%2520on%2520an%250Aiterative%2520time-consuming%2520process%2520entailing%2520several%2520visual%2520analysis%2520passes.%2520In%250Athis%2520paper%2520we%2520propose%2520a%2520multi-task%2520deep-learning%2520approach%2520for%2520the%2520simultaneous%250Adetection%2520of%2520sleep%2520events%2520and%2520hypnogram%2520construction%2520in%2520one%2520single%2520pass.%2520Taking%250Aas%2520reference%2520state-of-the-art%2520methodology%2520for%2520object-detection%2520in%2520the%2520field%2520of%250AComputer%2520Vision%252C%2520we%2520reformulate%2520the%2520problem%2520for%2520the%2520analysis%2520of%2520multi-variate%250Atime%2520sequences%252C%2520and%2520more%2520specifically%2520for%2520pattern%2520detection%2520in%2520the%2520sleep%250Aanalysis%2520scenario.%2520We%2520investigate%2520the%2520performance%2520of%2520the%2520resulting%2520method%2520in%250Aidentifying%2520different%2520assembly%2520combinations%2520of%2520EEG%2520arousals%252C%2520respiratory%2520events%250A%2528apneas%2520and%2520hypopneas%2529%2520and%2520sleep%2520stages%252C%2520also%2520considering%2520different%2520input%250Asignal%2520montage%2520configurations.%2520Furthermore%252C%2520we%2520evaluate%2520our%2520approach%2520using%2520two%250Aindependent%2520datasets%252C%2520assessing%2520true-generalization%2520effects%2520involving%2520local%2520and%250Aexternal%2520validation%2520scenarios.%2520Based%2520on%2520our%2520results%252C%2520we%2520analyze%2520and%2520discuss%2520our%250Amethod%2527s%2520capabilities%2520and%2520its%2520potential%2520wide-range%2520applicability%2520across%250Adifferent%2520settings%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-task%20deep-learning%20for%20sleep%20event%20detection%20and%20stage%0A%20%20classification&entry.906535625=Adriana%20Anido-Alonso%20and%20Diego%20Alvarez-Estevez&entry.1292438233=%20%20Polysomnographic%20sleep%20analysis%20is%20the%20standard%20clinical%20method%20to%20accurately%0Adiagnose%20and%20treat%20sleep%20disorders.%20It%20is%20an%20intricate%20process%20which%20involves%0Athe%20manual%20identification%2C%20classification%2C%20and%20location%20of%20multiple%20sleep%20event%0Apatterns.%20This%20is%20complex%2C%20for%20which%20identification%20of%20different%20types%20of%0Aevents%20involves%20focusing%20on%20different%20subsets%20of%20signals%2C%20resulting%20on%20an%0Aiterative%20time-consuming%20process%20entailing%20several%20visual%20analysis%20passes.%20In%0Athis%20paper%20we%20propose%20a%20multi-task%20deep-learning%20approach%20for%20the%20simultaneous%0Adetection%20of%20sleep%20events%20and%20hypnogram%20construction%20in%20one%20single%20pass.%20Taking%0Aas%20reference%20state-of-the-art%20methodology%20for%20object-detection%20in%20the%20field%20of%0AComputer%20Vision%2C%20we%20reformulate%20the%20problem%20for%20the%20analysis%20of%20multi-variate%0Atime%20sequences%2C%20and%20more%20specifically%20for%20pattern%20detection%20in%20the%20sleep%0Aanalysis%20scenario.%20We%20investigate%20the%20performance%20of%20the%20resulting%20method%20in%0Aidentifying%20different%20assembly%20combinations%20of%20EEG%20arousals%2C%20respiratory%20events%0A%28apneas%20and%20hypopneas%29%20and%20sleep%20stages%2C%20also%20considering%20different%20input%0Asignal%20montage%20configurations.%20Furthermore%2C%20we%20evaluate%20our%20approach%20using%20two%0Aindependent%20datasets%2C%20assessing%20true-generalization%20effects%20involving%20local%20and%0Aexternal%20validation%20scenarios.%20Based%20on%20our%20results%2C%20we%20analyze%20and%20discuss%20our%0Amethod%27s%20capabilities%20and%20its%20potential%20wide-range%20applicability%20across%0Adifferent%20settings%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09519v1&entry.124074799=Read"},
{"title": "A Comprehensive Survey of Foundation Models in Medicine", "author": "Wasif Khan and Seowung Leem and Kyle B. See and Joshua K. Wong and Shaoting Zhang and Ruogu Fang", "abstract": "  Foundation models (FMs) are large-scale deep learning models trained on\nmassive datasets, often using self-supervised learning techniques. These models\nserve as a versatile base for a wide range of downstream tasks, including those\nin medicine and healthcare. FMs have demonstrated remarkable success across\nmultiple healthcare domains. However, existing surveys in this field do not\ncomprehensively cover all areas where FMs have made significant strides. In\nthis survey, we present a comprehensive review of FMs in medicine, focusing on\ntheir evolution, learning strategies, flagship models, applications, and\nassociated challenges. We examine how prominent FMs, such as the BERT and GPT\nfamilies, are transforming various aspects of healthcare, including clinical\nlarge language models, medical image analysis, and omics research.\nAdditionally, we provide a detailed taxonomy of FM-enabled healthcare\napplications, spanning clinical natural language processing, medical computer\nvision, graph learning, and other biology- and omics- related tasks. Despite\nthe transformative potentials of FMs, they also pose unique challenges. This\nsurvey delves into these challenges and highlights open research questions and\nlessons learned to guide researchers and practitioners. Our goal is to provide\nvaluable insights into the capabilities of FMs in health, facilitating\nresponsible deployment and mitigating associated risks.\n", "link": "http://arxiv.org/abs/2406.10729v3", "date": "2025-01-16", "relevancy": 2.0629, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20of%20Foundation%20Models%20in%20Medicine&body=Title%3A%20A%20Comprehensive%20Survey%20of%20Foundation%20Models%20in%20Medicine%0AAuthor%3A%20Wasif%20Khan%20and%20Seowung%20Leem%20and%20Kyle%20B.%20See%20and%20Joshua%20K.%20Wong%20and%20Shaoting%20Zhang%20and%20Ruogu%20Fang%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20are%20large-scale%20deep%20learning%20models%20trained%20on%0Amassive%20datasets%2C%20often%20using%20self-supervised%20learning%20techniques.%20These%20models%0Aserve%20as%20a%20versatile%20base%20for%20a%20wide%20range%20of%20downstream%20tasks%2C%20including%20those%0Ain%20medicine%20and%20healthcare.%20FMs%20have%20demonstrated%20remarkable%20success%20across%0Amultiple%20healthcare%20domains.%20However%2C%20existing%20surveys%20in%20this%20field%20do%20not%0Acomprehensively%20cover%20all%20areas%20where%20FMs%20have%20made%20significant%20strides.%20In%0Athis%20survey%2C%20we%20present%20a%20comprehensive%20review%20of%20FMs%20in%20medicine%2C%20focusing%20on%0Atheir%20evolution%2C%20learning%20strategies%2C%20flagship%20models%2C%20applications%2C%20and%0Aassociated%20challenges.%20We%20examine%20how%20prominent%20FMs%2C%20such%20as%20the%20BERT%20and%20GPT%0Afamilies%2C%20are%20transforming%20various%20aspects%20of%20healthcare%2C%20including%20clinical%0Alarge%20language%20models%2C%20medical%20image%20analysis%2C%20and%20omics%20research.%0AAdditionally%2C%20we%20provide%20a%20detailed%20taxonomy%20of%20FM-enabled%20healthcare%0Aapplications%2C%20spanning%20clinical%20natural%20language%20processing%2C%20medical%20computer%0Avision%2C%20graph%20learning%2C%20and%20other%20biology-%20and%20omics-%20related%20tasks.%20Despite%0Athe%20transformative%20potentials%20of%20FMs%2C%20they%20also%20pose%20unique%20challenges.%20This%0Asurvey%20delves%20into%20these%20challenges%20and%20highlights%20open%20research%20questions%20and%0Alessons%20learned%20to%20guide%20researchers%20and%20practitioners.%20Our%20goal%20is%20to%20provide%0Avaluable%20insights%20into%20the%20capabilities%20of%20FMs%20in%20health%2C%20facilitating%0Aresponsible%20deployment%20and%20mitigating%20associated%20risks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10729v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Survey%2520of%2520Foundation%2520Models%2520in%2520Medicine%26entry.906535625%3DWasif%2520Khan%2520and%2520Seowung%2520Leem%2520and%2520Kyle%2520B.%2520See%2520and%2520Joshua%2520K.%2520Wong%2520and%2520Shaoting%2520Zhang%2520and%2520Ruogu%2520Fang%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520are%2520large-scale%2520deep%2520learning%2520models%2520trained%2520on%250Amassive%2520datasets%252C%2520often%2520using%2520self-supervised%2520learning%2520techniques.%2520These%2520models%250Aserve%2520as%2520a%2520versatile%2520base%2520for%2520a%2520wide%2520range%2520of%2520downstream%2520tasks%252C%2520including%2520those%250Ain%2520medicine%2520and%2520healthcare.%2520FMs%2520have%2520demonstrated%2520remarkable%2520success%2520across%250Amultiple%2520healthcare%2520domains.%2520However%252C%2520existing%2520surveys%2520in%2520this%2520field%2520do%2520not%250Acomprehensively%2520cover%2520all%2520areas%2520where%2520FMs%2520have%2520made%2520significant%2520strides.%2520In%250Athis%2520survey%252C%2520we%2520present%2520a%2520comprehensive%2520review%2520of%2520FMs%2520in%2520medicine%252C%2520focusing%2520on%250Atheir%2520evolution%252C%2520learning%2520strategies%252C%2520flagship%2520models%252C%2520applications%252C%2520and%250Aassociated%2520challenges.%2520We%2520examine%2520how%2520prominent%2520FMs%252C%2520such%2520as%2520the%2520BERT%2520and%2520GPT%250Afamilies%252C%2520are%2520transforming%2520various%2520aspects%2520of%2520healthcare%252C%2520including%2520clinical%250Alarge%2520language%2520models%252C%2520medical%2520image%2520analysis%252C%2520and%2520omics%2520research.%250AAdditionally%252C%2520we%2520provide%2520a%2520detailed%2520taxonomy%2520of%2520FM-enabled%2520healthcare%250Aapplications%252C%2520spanning%2520clinical%2520natural%2520language%2520processing%252C%2520medical%2520computer%250Avision%252C%2520graph%2520learning%252C%2520and%2520other%2520biology-%2520and%2520omics-%2520related%2520tasks.%2520Despite%250Athe%2520transformative%2520potentials%2520of%2520FMs%252C%2520they%2520also%2520pose%2520unique%2520challenges.%2520This%250Asurvey%2520delves%2520into%2520these%2520challenges%2520and%2520highlights%2520open%2520research%2520questions%2520and%250Alessons%2520learned%2520to%2520guide%2520researchers%2520and%2520practitioners.%2520Our%2520goal%2520is%2520to%2520provide%250Avaluable%2520insights%2520into%2520the%2520capabilities%2520of%2520FMs%2520in%2520health%252C%2520facilitating%250Aresponsible%2520deployment%2520and%2520mitigating%2520associated%2520risks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10729v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20of%20Foundation%20Models%20in%20Medicine&entry.906535625=Wasif%20Khan%20and%20Seowung%20Leem%20and%20Kyle%20B.%20See%20and%20Joshua%20K.%20Wong%20and%20Shaoting%20Zhang%20and%20Ruogu%20Fang&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20are%20large-scale%20deep%20learning%20models%20trained%20on%0Amassive%20datasets%2C%20often%20using%20self-supervised%20learning%20techniques.%20These%20models%0Aserve%20as%20a%20versatile%20base%20for%20a%20wide%20range%20of%20downstream%20tasks%2C%20including%20those%0Ain%20medicine%20and%20healthcare.%20FMs%20have%20demonstrated%20remarkable%20success%20across%0Amultiple%20healthcare%20domains.%20However%2C%20existing%20surveys%20in%20this%20field%20do%20not%0Acomprehensively%20cover%20all%20areas%20where%20FMs%20have%20made%20significant%20strides.%20In%0Athis%20survey%2C%20we%20present%20a%20comprehensive%20review%20of%20FMs%20in%20medicine%2C%20focusing%20on%0Atheir%20evolution%2C%20learning%20strategies%2C%20flagship%20models%2C%20applications%2C%20and%0Aassociated%20challenges.%20We%20examine%20how%20prominent%20FMs%2C%20such%20as%20the%20BERT%20and%20GPT%0Afamilies%2C%20are%20transforming%20various%20aspects%20of%20healthcare%2C%20including%20clinical%0Alarge%20language%20models%2C%20medical%20image%20analysis%2C%20and%20omics%20research.%0AAdditionally%2C%20we%20provide%20a%20detailed%20taxonomy%20of%20FM-enabled%20healthcare%0Aapplications%2C%20spanning%20clinical%20natural%20language%20processing%2C%20medical%20computer%0Avision%2C%20graph%20learning%2C%20and%20other%20biology-%20and%20omics-%20related%20tasks.%20Despite%0Athe%20transformative%20potentials%20of%20FMs%2C%20they%20also%20pose%20unique%20challenges.%20This%0Asurvey%20delves%20into%20these%20challenges%20and%20highlights%20open%20research%20questions%20and%0Alessons%20learned%20to%20guide%20researchers%20and%20practitioners.%20Our%20goal%20is%20to%20provide%0Avaluable%20insights%20into%20the%20capabilities%20of%20FMs%20in%20health%2C%20facilitating%0Aresponsible%20deployment%20and%20mitigating%20associated%20risks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10729v3&entry.124074799=Read"},
{"title": "Mitigating Hallucinations in Large Vision-Language Models via DPO:\n  On-Policy Data Hold the Key", "author": "Zhihe Yang and Xufang Luo and Dongqi Han and Yunjian Xu and Dongsheng Li", "abstract": "  Hallucination remains a major challenge for Large Vision-Language Models\n(LVLMs). Direct Preference Optimization (DPO) has gained increasing attention\nas a simple solution to hallucination issues. It directly learns from\nconstructed preference pairs that reflect the severity of hallucinations in\nresponses to the same prompt and image. Nonetheless, different data\nconstruction methods in existing works bring notable performance variations. We\nidentify a crucial factor here: outcomes are largely contingent on whether the\nconstructed data aligns on-policy w.r.t the initial (reference) policy of DPO.\nTheoretical analysis suggests that learning from off-policy data is impeded by\nthe presence of KL-divergence between the updated policy and the reference\npolicy. From the perspective of dataset distribution, we systematically\nsummarize the inherent flaws in existing algorithms that employ DPO to address\nhallucination issues. To alleviate the problems, we propose On-Policy Alignment\n(OPA)-DPO framework, which uniquely leverages expert feedback to correct\nhallucinated responses and aligns both the original and expert-revised\nresponses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO\nachieves an additional reduction in the hallucination rate of LLaVA-1.5-7B:\n13.26% on the AMBER benchmark and 5.39% on the Object-Hal benchmark, compared\nto the previous SOTA algorithm trained with 16k samples.\n", "link": "http://arxiv.org/abs/2501.09695v1", "date": "2025-01-16", "relevancy": 2.059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5166}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5166}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Hallucinations%20in%20Large%20Vision-Language%20Models%20via%20DPO%3A%0A%20%20On-Policy%20Data%20Hold%20the%20Key&body=Title%3A%20Mitigating%20Hallucinations%20in%20Large%20Vision-Language%20Models%20via%20DPO%3A%0A%20%20On-Policy%20Data%20Hold%20the%20Key%0AAuthor%3A%20Zhihe%20Yang%20and%20Xufang%20Luo%20and%20Dongqi%20Han%20and%20Yunjian%20Xu%20and%20Dongsheng%20Li%0AAbstract%3A%20%20%20Hallucination%20remains%20a%20major%20challenge%20for%20Large%20Vision-Language%20Models%0A%28LVLMs%29.%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20gained%20increasing%20attention%0Aas%20a%20simple%20solution%20to%20hallucination%20issues.%20It%20directly%20learns%20from%0Aconstructed%20preference%20pairs%20that%20reflect%20the%20severity%20of%20hallucinations%20in%0Aresponses%20to%20the%20same%20prompt%20and%20image.%20Nonetheless%2C%20different%20data%0Aconstruction%20methods%20in%20existing%20works%20bring%20notable%20performance%20variations.%20We%0Aidentify%20a%20crucial%20factor%20here%3A%20outcomes%20are%20largely%20contingent%20on%20whether%20the%0Aconstructed%20data%20aligns%20on-policy%20w.r.t%20the%20initial%20%28reference%29%20policy%20of%20DPO.%0ATheoretical%20analysis%20suggests%20that%20learning%20from%20off-policy%20data%20is%20impeded%20by%0Athe%20presence%20of%20KL-divergence%20between%20the%20updated%20policy%20and%20the%20reference%0Apolicy.%20From%20the%20perspective%20of%20dataset%20distribution%2C%20we%20systematically%0Asummarize%20the%20inherent%20flaws%20in%20existing%20algorithms%20that%20employ%20DPO%20to%20address%0Ahallucination%20issues.%20To%20alleviate%20the%20problems%2C%20we%20propose%20On-Policy%20Alignment%0A%28OPA%29-DPO%20framework%2C%20which%20uniquely%20leverages%20expert%20feedback%20to%20correct%0Ahallucinated%20responses%20and%20aligns%20both%20the%20original%20and%20expert-revised%0Aresponses%20in%20an%20on-policy%20manner.%20Notably%2C%20with%20only%204.8k%20data%2C%20OPA-DPO%0Aachieves%20an%20additional%20reduction%20in%20the%20hallucination%20rate%20of%20LLaVA-1.5-7B%3A%0A13.26%25%20on%20the%20AMBER%20benchmark%20and%205.39%25%20on%20the%20Object-Hal%20benchmark%2C%20compared%0Ato%20the%20previous%20SOTA%20algorithm%20trained%20with%2016k%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Hallucinations%2520in%2520Large%2520Vision-Language%2520Models%2520via%2520DPO%253A%250A%2520%2520On-Policy%2520Data%2520Hold%2520the%2520Key%26entry.906535625%3DZhihe%2520Yang%2520and%2520Xufang%2520Luo%2520and%2520Dongqi%2520Han%2520and%2520Yunjian%2520Xu%2520and%2520Dongsheng%2520Li%26entry.1292438233%3D%2520%2520Hallucination%2520remains%2520a%2520major%2520challenge%2520for%2520Large%2520Vision-Language%2520Models%250A%2528LVLMs%2529.%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520has%2520gained%2520increasing%2520attention%250Aas%2520a%2520simple%2520solution%2520to%2520hallucination%2520issues.%2520It%2520directly%2520learns%2520from%250Aconstructed%2520preference%2520pairs%2520that%2520reflect%2520the%2520severity%2520of%2520hallucinations%2520in%250Aresponses%2520to%2520the%2520same%2520prompt%2520and%2520image.%2520Nonetheless%252C%2520different%2520data%250Aconstruction%2520methods%2520in%2520existing%2520works%2520bring%2520notable%2520performance%2520variations.%2520We%250Aidentify%2520a%2520crucial%2520factor%2520here%253A%2520outcomes%2520are%2520largely%2520contingent%2520on%2520whether%2520the%250Aconstructed%2520data%2520aligns%2520on-policy%2520w.r.t%2520the%2520initial%2520%2528reference%2529%2520policy%2520of%2520DPO.%250ATheoretical%2520analysis%2520suggests%2520that%2520learning%2520from%2520off-policy%2520data%2520is%2520impeded%2520by%250Athe%2520presence%2520of%2520KL-divergence%2520between%2520the%2520updated%2520policy%2520and%2520the%2520reference%250Apolicy.%2520From%2520the%2520perspective%2520of%2520dataset%2520distribution%252C%2520we%2520systematically%250Asummarize%2520the%2520inherent%2520flaws%2520in%2520existing%2520algorithms%2520that%2520employ%2520DPO%2520to%2520address%250Ahallucination%2520issues.%2520To%2520alleviate%2520the%2520problems%252C%2520we%2520propose%2520On-Policy%2520Alignment%250A%2528OPA%2529-DPO%2520framework%252C%2520which%2520uniquely%2520leverages%2520expert%2520feedback%2520to%2520correct%250Ahallucinated%2520responses%2520and%2520aligns%2520both%2520the%2520original%2520and%2520expert-revised%250Aresponses%2520in%2520an%2520on-policy%2520manner.%2520Notably%252C%2520with%2520only%25204.8k%2520data%252C%2520OPA-DPO%250Aachieves%2520an%2520additional%2520reduction%2520in%2520the%2520hallucination%2520rate%2520of%2520LLaVA-1.5-7B%253A%250A13.26%2525%2520on%2520the%2520AMBER%2520benchmark%2520and%25205.39%2525%2520on%2520the%2520Object-Hal%2520benchmark%252C%2520compared%250Ato%2520the%2520previous%2520SOTA%2520algorithm%2520trained%2520with%252016k%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Hallucinations%20in%20Large%20Vision-Language%20Models%20via%20DPO%3A%0A%20%20On-Policy%20Data%20Hold%20the%20Key&entry.906535625=Zhihe%20Yang%20and%20Xufang%20Luo%20and%20Dongqi%20Han%20and%20Yunjian%20Xu%20and%20Dongsheng%20Li&entry.1292438233=%20%20Hallucination%20remains%20a%20major%20challenge%20for%20Large%20Vision-Language%20Models%0A%28LVLMs%29.%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20gained%20increasing%20attention%0Aas%20a%20simple%20solution%20to%20hallucination%20issues.%20It%20directly%20learns%20from%0Aconstructed%20preference%20pairs%20that%20reflect%20the%20severity%20of%20hallucinations%20in%0Aresponses%20to%20the%20same%20prompt%20and%20image.%20Nonetheless%2C%20different%20data%0Aconstruction%20methods%20in%20existing%20works%20bring%20notable%20performance%20variations.%20We%0Aidentify%20a%20crucial%20factor%20here%3A%20outcomes%20are%20largely%20contingent%20on%20whether%20the%0Aconstructed%20data%20aligns%20on-policy%20w.r.t%20the%20initial%20%28reference%29%20policy%20of%20DPO.%0ATheoretical%20analysis%20suggests%20that%20learning%20from%20off-policy%20data%20is%20impeded%20by%0Athe%20presence%20of%20KL-divergence%20between%20the%20updated%20policy%20and%20the%20reference%0Apolicy.%20From%20the%20perspective%20of%20dataset%20distribution%2C%20we%20systematically%0Asummarize%20the%20inherent%20flaws%20in%20existing%20algorithms%20that%20employ%20DPO%20to%20address%0Ahallucination%20issues.%20To%20alleviate%20the%20problems%2C%20we%20propose%20On-Policy%20Alignment%0A%28OPA%29-DPO%20framework%2C%20which%20uniquely%20leverages%20expert%20feedback%20to%20correct%0Ahallucinated%20responses%20and%20aligns%20both%20the%20original%20and%20expert-revised%0Aresponses%20in%20an%20on-policy%20manner.%20Notably%2C%20with%20only%204.8k%20data%2C%20OPA-DPO%0Aachieves%20an%20additional%20reduction%20in%20the%20hallucination%20rate%20of%20LLaVA-1.5-7B%3A%0A13.26%25%20on%20the%20AMBER%20benchmark%20and%205.39%25%20on%20the%20Object-Hal%20benchmark%2C%20compared%0Ato%20the%20previous%20SOTA%20algorithm%20trained%20with%2016k%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09695v1&entry.124074799=Read"},
{"title": "AeroHaptix: A Wearable Vibrotactile Feedback System for Enhancing\n  Collision Avoidance in UAV Teleoperation", "author": "Bingjian Huang and Zhecheng Wang and Qilong Cheng and Siyi Ren and Hanfeng Cai and Antonio Alvarez Valdivia and Karthik Mahadevan and Daniel Wigdor", "abstract": "  Haptic feedback enhances collision avoidance by providing directional\nobstacle information to operators during unmanned aerial vehicle (UAV)\nteleoperation. However, such feedback is often rendered via haptic joysticks,\nwhich are unfamiliar to UAV operators and limited to single-direction force\nfeedback. Additionally, the direct coupling between the input device and the\nfeedback method diminishes operators' sense of control and induces oscillatory\nmovements. To overcome these limitations, we propose AeroHaptix, a wearable\nhaptic feedback system that uses spatial vibrations to simultaneously\ncommunicate multiple obstacle directions to operators, without interfering with\ntheir input control. The layout of vibrotactile actuators was optimized via a\nperceptual study to eliminate perceptual biases and achieve uniform spatial\ncoverage. A novel rendering algorithm, MultiCBF, extended control barrier\nfunctions to support multi-directional feedback. Our system evaluation showed\nthat compared to a no-feedback condition, AeroHaptix effectively reduced the\nnumber of collisions and input disagreement. Furthermore, operators reported\nthat AeroHaptix was more helpful than force feedback, with improved situational\nawareness and comparable workload.\n", "link": "http://arxiv.org/abs/2407.12105v3", "date": "2025-01-16", "relevancy": 2.0562, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5448}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5126}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AeroHaptix%3A%20A%20Wearable%20Vibrotactile%20Feedback%20System%20for%20Enhancing%0A%20%20Collision%20Avoidance%20in%20UAV%20Teleoperation&body=Title%3A%20AeroHaptix%3A%20A%20Wearable%20Vibrotactile%20Feedback%20System%20for%20Enhancing%0A%20%20Collision%20Avoidance%20in%20UAV%20Teleoperation%0AAuthor%3A%20Bingjian%20Huang%20and%20Zhecheng%20Wang%20and%20Qilong%20Cheng%20and%20Siyi%20Ren%20and%20Hanfeng%20Cai%20and%20Antonio%20Alvarez%20Valdivia%20and%20Karthik%20Mahadevan%20and%20Daniel%20Wigdor%0AAbstract%3A%20%20%20Haptic%20feedback%20enhances%20collision%20avoidance%20by%20providing%20directional%0Aobstacle%20information%20to%20operators%20during%20unmanned%20aerial%20vehicle%20%28UAV%29%0Ateleoperation.%20However%2C%20such%20feedback%20is%20often%20rendered%20via%20haptic%20joysticks%2C%0Awhich%20are%20unfamiliar%20to%20UAV%20operators%20and%20limited%20to%20single-direction%20force%0Afeedback.%20Additionally%2C%20the%20direct%20coupling%20between%20the%20input%20device%20and%20the%0Afeedback%20method%20diminishes%20operators%27%20sense%20of%20control%20and%20induces%20oscillatory%0Amovements.%20To%20overcome%20these%20limitations%2C%20we%20propose%20AeroHaptix%2C%20a%20wearable%0Ahaptic%20feedback%20system%20that%20uses%20spatial%20vibrations%20to%20simultaneously%0Acommunicate%20multiple%20obstacle%20directions%20to%20operators%2C%20without%20interfering%20with%0Atheir%20input%20control.%20The%20layout%20of%20vibrotactile%20actuators%20was%20optimized%20via%20a%0Aperceptual%20study%20to%20eliminate%20perceptual%20biases%20and%20achieve%20uniform%20spatial%0Acoverage.%20A%20novel%20rendering%20algorithm%2C%20MultiCBF%2C%20extended%20control%20barrier%0Afunctions%20to%20support%20multi-directional%20feedback.%20Our%20system%20evaluation%20showed%0Athat%20compared%20to%20a%20no-feedback%20condition%2C%20AeroHaptix%20effectively%20reduced%20the%0Anumber%20of%20collisions%20and%20input%20disagreement.%20Furthermore%2C%20operators%20reported%0Athat%20AeroHaptix%20was%20more%20helpful%20than%20force%20feedback%2C%20with%20improved%20situational%0Aawareness%20and%20comparable%20workload.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12105v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAeroHaptix%253A%2520A%2520Wearable%2520Vibrotactile%2520Feedback%2520System%2520for%2520Enhancing%250A%2520%2520Collision%2520Avoidance%2520in%2520UAV%2520Teleoperation%26entry.906535625%3DBingjian%2520Huang%2520and%2520Zhecheng%2520Wang%2520and%2520Qilong%2520Cheng%2520and%2520Siyi%2520Ren%2520and%2520Hanfeng%2520Cai%2520and%2520Antonio%2520Alvarez%2520Valdivia%2520and%2520Karthik%2520Mahadevan%2520and%2520Daniel%2520Wigdor%26entry.1292438233%3D%2520%2520Haptic%2520feedback%2520enhances%2520collision%2520avoidance%2520by%2520providing%2520directional%250Aobstacle%2520information%2520to%2520operators%2520during%2520unmanned%2520aerial%2520vehicle%2520%2528UAV%2529%250Ateleoperation.%2520However%252C%2520such%2520feedback%2520is%2520often%2520rendered%2520via%2520haptic%2520joysticks%252C%250Awhich%2520are%2520unfamiliar%2520to%2520UAV%2520operators%2520and%2520limited%2520to%2520single-direction%2520force%250Afeedback.%2520Additionally%252C%2520the%2520direct%2520coupling%2520between%2520the%2520input%2520device%2520and%2520the%250Afeedback%2520method%2520diminishes%2520operators%2527%2520sense%2520of%2520control%2520and%2520induces%2520oscillatory%250Amovements.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520AeroHaptix%252C%2520a%2520wearable%250Ahaptic%2520feedback%2520system%2520that%2520uses%2520spatial%2520vibrations%2520to%2520simultaneously%250Acommunicate%2520multiple%2520obstacle%2520directions%2520to%2520operators%252C%2520without%2520interfering%2520with%250Atheir%2520input%2520control.%2520The%2520layout%2520of%2520vibrotactile%2520actuators%2520was%2520optimized%2520via%2520a%250Aperceptual%2520study%2520to%2520eliminate%2520perceptual%2520biases%2520and%2520achieve%2520uniform%2520spatial%250Acoverage.%2520A%2520novel%2520rendering%2520algorithm%252C%2520MultiCBF%252C%2520extended%2520control%2520barrier%250Afunctions%2520to%2520support%2520multi-directional%2520feedback.%2520Our%2520system%2520evaluation%2520showed%250Athat%2520compared%2520to%2520a%2520no-feedback%2520condition%252C%2520AeroHaptix%2520effectively%2520reduced%2520the%250Anumber%2520of%2520collisions%2520and%2520input%2520disagreement.%2520Furthermore%252C%2520operators%2520reported%250Athat%2520AeroHaptix%2520was%2520more%2520helpful%2520than%2520force%2520feedback%252C%2520with%2520improved%2520situational%250Aawareness%2520and%2520comparable%2520workload.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12105v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AeroHaptix%3A%20A%20Wearable%20Vibrotactile%20Feedback%20System%20for%20Enhancing%0A%20%20Collision%20Avoidance%20in%20UAV%20Teleoperation&entry.906535625=Bingjian%20Huang%20and%20Zhecheng%20Wang%20and%20Qilong%20Cheng%20and%20Siyi%20Ren%20and%20Hanfeng%20Cai%20and%20Antonio%20Alvarez%20Valdivia%20and%20Karthik%20Mahadevan%20and%20Daniel%20Wigdor&entry.1292438233=%20%20Haptic%20feedback%20enhances%20collision%20avoidance%20by%20providing%20directional%0Aobstacle%20information%20to%20operators%20during%20unmanned%20aerial%20vehicle%20%28UAV%29%0Ateleoperation.%20However%2C%20such%20feedback%20is%20often%20rendered%20via%20haptic%20joysticks%2C%0Awhich%20are%20unfamiliar%20to%20UAV%20operators%20and%20limited%20to%20single-direction%20force%0Afeedback.%20Additionally%2C%20the%20direct%20coupling%20between%20the%20input%20device%20and%20the%0Afeedback%20method%20diminishes%20operators%27%20sense%20of%20control%20and%20induces%20oscillatory%0Amovements.%20To%20overcome%20these%20limitations%2C%20we%20propose%20AeroHaptix%2C%20a%20wearable%0Ahaptic%20feedback%20system%20that%20uses%20spatial%20vibrations%20to%20simultaneously%0Acommunicate%20multiple%20obstacle%20directions%20to%20operators%2C%20without%20interfering%20with%0Atheir%20input%20control.%20The%20layout%20of%20vibrotactile%20actuators%20was%20optimized%20via%20a%0Aperceptual%20study%20to%20eliminate%20perceptual%20biases%20and%20achieve%20uniform%20spatial%0Acoverage.%20A%20novel%20rendering%20algorithm%2C%20MultiCBF%2C%20extended%20control%20barrier%0Afunctions%20to%20support%20multi-directional%20feedback.%20Our%20system%20evaluation%20showed%0Athat%20compared%20to%20a%20no-feedback%20condition%2C%20AeroHaptix%20effectively%20reduced%20the%0Anumber%20of%20collisions%20and%20input%20disagreement.%20Furthermore%2C%20operators%20reported%0Athat%20AeroHaptix%20was%20more%20helpful%20than%20force%20feedback%2C%20with%20improved%20situational%0Aawareness%20and%20comparable%20workload.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12105v3&entry.124074799=Read"},
{"title": "Towards an End-to-End (E2E) Adversarial Learning and Application in the\n  Physical World", "author": "Dudi Biton and Jacob Shams and Satoru Koda and Asaf Shabtai and Yuval Elovici and Ben Nassi", "abstract": "  The traditional learning process of patch-based adversarial attacks,\nconducted in the digital domain and then applied in the physical domain (e.g.,\nvia printed stickers), may suffer from reduced performance due to adversarial\npatches' limited transferability from the digital domain to the physical\ndomain. Given that previous studies have considered using projectors to apply\nadversarial attacks, we raise the following question: can adversarial learning\n(i.e., patch generation) be performed entirely in the physical domain with a\nprojector? In this work, we propose the Physical-domain Adversarial Patch\nLearning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework\nthat converts adversarial learning from the digital domain to the physical\ndomain using a projector. We evaluate PAPLA across multiple scenarios,\nincluding controlled laboratory settings and realistic outdoor environments,\ndemonstrating its ability to ensure attack success compared to conventional\ndigital learning-physical application (DL-PA) methods. We also analyze the\nimpact of environmental factors, such as projection surface color, projector\nstrength, ambient light, distance, and angle of the target object relative to\nthe camera, on the effectiveness of projected patches. Finally, we demonstrate\nthe feasibility of the attack against a parked car and a stop sign in a\nreal-world outdoor environment. Our results show that under specific\nconditions, E2E adversarial learning in the physical domain eliminates the\ntransferability issue and ensures evasion by object detectors. Finally, we\nprovide insights into the challenges and opportunities of applying adversarial\nlearning in the physical domain and explain where such an approach is more\neffective than using a sticker.\n", "link": "http://arxiv.org/abs/2501.08258v2", "date": "2025-01-16", "relevancy": 2.0527, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5234}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5174}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20an%20End-to-End%20%28E2E%29%20Adversarial%20Learning%20and%20Application%20in%20the%0A%20%20Physical%20World&body=Title%3A%20Towards%20an%20End-to-End%20%28E2E%29%20Adversarial%20Learning%20and%20Application%20in%20the%0A%20%20Physical%20World%0AAuthor%3A%20Dudi%20Biton%20and%20Jacob%20Shams%20and%20Satoru%20Koda%20and%20Asaf%20Shabtai%20and%20Yuval%20Elovici%20and%20Ben%20Nassi%0AAbstract%3A%20%20%20The%20traditional%20learning%20process%20of%20patch-based%20adversarial%20attacks%2C%0Aconducted%20in%20the%20digital%20domain%20and%20then%20applied%20in%20the%20physical%20domain%20%28e.g.%2C%0Avia%20printed%20stickers%29%2C%20may%20suffer%20from%20reduced%20performance%20due%20to%20adversarial%0Apatches%27%20limited%20transferability%20from%20the%20digital%20domain%20to%20the%20physical%0Adomain.%20Given%20that%20previous%20studies%20have%20considered%20using%20projectors%20to%20apply%0Aadversarial%20attacks%2C%20we%20raise%20the%20following%20question%3A%20can%20adversarial%20learning%0A%28i.e.%2C%20patch%20generation%29%20be%20performed%20entirely%20in%20the%20physical%20domain%20with%20a%0Aprojector%3F%20In%20this%20work%2C%20we%20propose%20the%20Physical-domain%20Adversarial%20Patch%0ALearning%20Augmentation%20%28PAPLA%29%20framework%2C%20a%20novel%20end-to-end%20%28E2E%29%20framework%0Athat%20converts%20adversarial%20learning%20from%20the%20digital%20domain%20to%20the%20physical%0Adomain%20using%20a%20projector.%20We%20evaluate%20PAPLA%20across%20multiple%20scenarios%2C%0Aincluding%20controlled%20laboratory%20settings%20and%20realistic%20outdoor%20environments%2C%0Ademonstrating%20its%20ability%20to%20ensure%20attack%20success%20compared%20to%20conventional%0Adigital%20learning-physical%20application%20%28DL-PA%29%20methods.%20We%20also%20analyze%20the%0Aimpact%20of%20environmental%20factors%2C%20such%20as%20projection%20surface%20color%2C%20projector%0Astrength%2C%20ambient%20light%2C%20distance%2C%20and%20angle%20of%20the%20target%20object%20relative%20to%0Athe%20camera%2C%20on%20the%20effectiveness%20of%20projected%20patches.%20Finally%2C%20we%20demonstrate%0Athe%20feasibility%20of%20the%20attack%20against%20a%20parked%20car%20and%20a%20stop%20sign%20in%20a%0Areal-world%20outdoor%20environment.%20Our%20results%20show%20that%20under%20specific%0Aconditions%2C%20E2E%20adversarial%20learning%20in%20the%20physical%20domain%20eliminates%20the%0Atransferability%20issue%20and%20ensures%20evasion%20by%20object%20detectors.%20Finally%2C%20we%0Aprovide%20insights%20into%20the%20challenges%20and%20opportunities%20of%20applying%20adversarial%0Alearning%20in%20the%20physical%20domain%20and%20explain%20where%20such%20an%20approach%20is%20more%0Aeffective%20than%20using%20a%20sticker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08258v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520an%2520End-to-End%2520%2528E2E%2529%2520Adversarial%2520Learning%2520and%2520Application%2520in%2520the%250A%2520%2520Physical%2520World%26entry.906535625%3DDudi%2520Biton%2520and%2520Jacob%2520Shams%2520and%2520Satoru%2520Koda%2520and%2520Asaf%2520Shabtai%2520and%2520Yuval%2520Elovici%2520and%2520Ben%2520Nassi%26entry.1292438233%3D%2520%2520The%2520traditional%2520learning%2520process%2520of%2520patch-based%2520adversarial%2520attacks%252C%250Aconducted%2520in%2520the%2520digital%2520domain%2520and%2520then%2520applied%2520in%2520the%2520physical%2520domain%2520%2528e.g.%252C%250Avia%2520printed%2520stickers%2529%252C%2520may%2520suffer%2520from%2520reduced%2520performance%2520due%2520to%2520adversarial%250Apatches%2527%2520limited%2520transferability%2520from%2520the%2520digital%2520domain%2520to%2520the%2520physical%250Adomain.%2520Given%2520that%2520previous%2520studies%2520have%2520considered%2520using%2520projectors%2520to%2520apply%250Aadversarial%2520attacks%252C%2520we%2520raise%2520the%2520following%2520question%253A%2520can%2520adversarial%2520learning%250A%2528i.e.%252C%2520patch%2520generation%2529%2520be%2520performed%2520entirely%2520in%2520the%2520physical%2520domain%2520with%2520a%250Aprojector%253F%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Physical-domain%2520Adversarial%2520Patch%250ALearning%2520Augmentation%2520%2528PAPLA%2529%2520framework%252C%2520a%2520novel%2520end-to-end%2520%2528E2E%2529%2520framework%250Athat%2520converts%2520adversarial%2520learning%2520from%2520the%2520digital%2520domain%2520to%2520the%2520physical%250Adomain%2520using%2520a%2520projector.%2520We%2520evaluate%2520PAPLA%2520across%2520multiple%2520scenarios%252C%250Aincluding%2520controlled%2520laboratory%2520settings%2520and%2520realistic%2520outdoor%2520environments%252C%250Ademonstrating%2520its%2520ability%2520to%2520ensure%2520attack%2520success%2520compared%2520to%2520conventional%250Adigital%2520learning-physical%2520application%2520%2528DL-PA%2529%2520methods.%2520We%2520also%2520analyze%2520the%250Aimpact%2520of%2520environmental%2520factors%252C%2520such%2520as%2520projection%2520surface%2520color%252C%2520projector%250Astrength%252C%2520ambient%2520light%252C%2520distance%252C%2520and%2520angle%2520of%2520the%2520target%2520object%2520relative%2520to%250Athe%2520camera%252C%2520on%2520the%2520effectiveness%2520of%2520projected%2520patches.%2520Finally%252C%2520we%2520demonstrate%250Athe%2520feasibility%2520of%2520the%2520attack%2520against%2520a%2520parked%2520car%2520and%2520a%2520stop%2520sign%2520in%2520a%250Areal-world%2520outdoor%2520environment.%2520Our%2520results%2520show%2520that%2520under%2520specific%250Aconditions%252C%2520E2E%2520adversarial%2520learning%2520in%2520the%2520physical%2520domain%2520eliminates%2520the%250Atransferability%2520issue%2520and%2520ensures%2520evasion%2520by%2520object%2520detectors.%2520Finally%252C%2520we%250Aprovide%2520insights%2520into%2520the%2520challenges%2520and%2520opportunities%2520of%2520applying%2520adversarial%250Alearning%2520in%2520the%2520physical%2520domain%2520and%2520explain%2520where%2520such%2520an%2520approach%2520is%2520more%250Aeffective%2520than%2520using%2520a%2520sticker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08258v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20an%20End-to-End%20%28E2E%29%20Adversarial%20Learning%20and%20Application%20in%20the%0A%20%20Physical%20World&entry.906535625=Dudi%20Biton%20and%20Jacob%20Shams%20and%20Satoru%20Koda%20and%20Asaf%20Shabtai%20and%20Yuval%20Elovici%20and%20Ben%20Nassi&entry.1292438233=%20%20The%20traditional%20learning%20process%20of%20patch-based%20adversarial%20attacks%2C%0Aconducted%20in%20the%20digital%20domain%20and%20then%20applied%20in%20the%20physical%20domain%20%28e.g.%2C%0Avia%20printed%20stickers%29%2C%20may%20suffer%20from%20reduced%20performance%20due%20to%20adversarial%0Apatches%27%20limited%20transferability%20from%20the%20digital%20domain%20to%20the%20physical%0Adomain.%20Given%20that%20previous%20studies%20have%20considered%20using%20projectors%20to%20apply%0Aadversarial%20attacks%2C%20we%20raise%20the%20following%20question%3A%20can%20adversarial%20learning%0A%28i.e.%2C%20patch%20generation%29%20be%20performed%20entirely%20in%20the%20physical%20domain%20with%20a%0Aprojector%3F%20In%20this%20work%2C%20we%20propose%20the%20Physical-domain%20Adversarial%20Patch%0ALearning%20Augmentation%20%28PAPLA%29%20framework%2C%20a%20novel%20end-to-end%20%28E2E%29%20framework%0Athat%20converts%20adversarial%20learning%20from%20the%20digital%20domain%20to%20the%20physical%0Adomain%20using%20a%20projector.%20We%20evaluate%20PAPLA%20across%20multiple%20scenarios%2C%0Aincluding%20controlled%20laboratory%20settings%20and%20realistic%20outdoor%20environments%2C%0Ademonstrating%20its%20ability%20to%20ensure%20attack%20success%20compared%20to%20conventional%0Adigital%20learning-physical%20application%20%28DL-PA%29%20methods.%20We%20also%20analyze%20the%0Aimpact%20of%20environmental%20factors%2C%20such%20as%20projection%20surface%20color%2C%20projector%0Astrength%2C%20ambient%20light%2C%20distance%2C%20and%20angle%20of%20the%20target%20object%20relative%20to%0Athe%20camera%2C%20on%20the%20effectiveness%20of%20projected%20patches.%20Finally%2C%20we%20demonstrate%0Athe%20feasibility%20of%20the%20attack%20against%20a%20parked%20car%20and%20a%20stop%20sign%20in%20a%0Areal-world%20outdoor%20environment.%20Our%20results%20show%20that%20under%20specific%0Aconditions%2C%20E2E%20adversarial%20learning%20in%20the%20physical%20domain%20eliminates%20the%0Atransferability%20issue%20and%20ensures%20evasion%20by%20object%20detectors.%20Finally%2C%20we%0Aprovide%20insights%20into%20the%20challenges%20and%20opportunities%20of%20applying%20adversarial%0Alearning%20in%20the%20physical%20domain%20and%20explain%20where%20such%20an%20approach%20is%20more%0Aeffective%20than%20using%20a%20sticker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08258v2&entry.124074799=Read"},
{"title": "An Adaptive Collocation Point Strategy For Physics Informed Neural\n  Networks via the QR Discrete Empirical Interpolation Method", "author": "Adrian Celaya and David Fuentes and Beatrice Riviere", "abstract": "  Physics-informed neural networks (PINNs) have gained significant attention\nfor solving forward and inverse problems related to partial differential\nequations (PDEs). While advancements in loss functions and network\narchitectures have improved PINN accuracy, the impact of collocation point\nsampling on their performance remains underexplored. Fixed sampling methods,\nsuch as uniform random sampling and equispaced grids, can fail to capture\ncritical regions with high solution gradients, limiting their effectiveness for\ncomplex PDEs. Adaptive methods, inspired by adaptive mesh refinement from\ntraditional numerical methods, address this by dynamically updating collocation\npoints during training but may overlook residual dynamics between updates,\npotentially losing valuable information. To overcome this limitation, we\npropose an adaptive collocation point selection strategy utilizing the QR\nDiscrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling\ntechnique for efficiently approximating nonlinear functions. Our results on\nbenchmark PDEs, including the wave, Allen-Cahn, and Burgers' equations,\ndemonstrate that our QR-DEIM-based approach improves PINN accuracy compared to\nexisting methods, offering a promising direction for adaptive collocation point\nstrategies.\n", "link": "http://arxiv.org/abs/2501.07700v2", "date": "2025-01-16", "relevancy": 2.0521, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.52}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5108}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adaptive%20Collocation%20Point%20Strategy%20For%20Physics%20Informed%20Neural%0A%20%20Networks%20via%20the%20QR%20Discrete%20Empirical%20Interpolation%20Method&body=Title%3A%20An%20Adaptive%20Collocation%20Point%20Strategy%20For%20Physics%20Informed%20Neural%0A%20%20Networks%20via%20the%20QR%20Discrete%20Empirical%20Interpolation%20Method%0AAuthor%3A%20Adrian%20Celaya%20and%20David%20Fuentes%20and%20Beatrice%20Riviere%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20have%20gained%20significant%20attention%0Afor%20solving%20forward%20and%20inverse%20problems%20related%20to%20partial%20differential%0Aequations%20%28PDEs%29.%20While%20advancements%20in%20loss%20functions%20and%20network%0Aarchitectures%20have%20improved%20PINN%20accuracy%2C%20the%20impact%20of%20collocation%20point%0Asampling%20on%20their%20performance%20remains%20underexplored.%20Fixed%20sampling%20methods%2C%0Asuch%20as%20uniform%20random%20sampling%20and%20equispaced%20grids%2C%20can%20fail%20to%20capture%0Acritical%20regions%20with%20high%20solution%20gradients%2C%20limiting%20their%20effectiveness%20for%0Acomplex%20PDEs.%20Adaptive%20methods%2C%20inspired%20by%20adaptive%20mesh%20refinement%20from%0Atraditional%20numerical%20methods%2C%20address%20this%20by%20dynamically%20updating%20collocation%0Apoints%20during%20training%20but%20may%20overlook%20residual%20dynamics%20between%20updates%2C%0Apotentially%20losing%20valuable%20information.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20an%20adaptive%20collocation%20point%20selection%20strategy%20utilizing%20the%20QR%0ADiscrete%20Empirical%20Interpolation%20Method%20%28QR-DEIM%29%2C%20a%20reduced-order%20modeling%0Atechnique%20for%20efficiently%20approximating%20nonlinear%20functions.%20Our%20results%20on%0Abenchmark%20PDEs%2C%20including%20the%20wave%2C%20Allen-Cahn%2C%20and%20Burgers%27%20equations%2C%0Ademonstrate%20that%20our%20QR-DEIM-based%20approach%20improves%20PINN%20accuracy%20compared%20to%0Aexisting%20methods%2C%20offering%20a%20promising%20direction%20for%20adaptive%20collocation%20point%0Astrategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adaptive%2520Collocation%2520Point%2520Strategy%2520For%2520Physics%2520Informed%2520Neural%250A%2520%2520Networks%2520via%2520the%2520QR%2520Discrete%2520Empirical%2520Interpolation%2520Method%26entry.906535625%3DAdrian%2520Celaya%2520and%2520David%2520Fuentes%2520and%2520Beatrice%2520Riviere%26entry.1292438233%3D%2520%2520Physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520have%2520gained%2520significant%2520attention%250Afor%2520solving%2520forward%2520and%2520inverse%2520problems%2520related%2520to%2520partial%2520differential%250Aequations%2520%2528PDEs%2529.%2520While%2520advancements%2520in%2520loss%2520functions%2520and%2520network%250Aarchitectures%2520have%2520improved%2520PINN%2520accuracy%252C%2520the%2520impact%2520of%2520collocation%2520point%250Asampling%2520on%2520their%2520performance%2520remains%2520underexplored.%2520Fixed%2520sampling%2520methods%252C%250Asuch%2520as%2520uniform%2520random%2520sampling%2520and%2520equispaced%2520grids%252C%2520can%2520fail%2520to%2520capture%250Acritical%2520regions%2520with%2520high%2520solution%2520gradients%252C%2520limiting%2520their%2520effectiveness%2520for%250Acomplex%2520PDEs.%2520Adaptive%2520methods%252C%2520inspired%2520by%2520adaptive%2520mesh%2520refinement%2520from%250Atraditional%2520numerical%2520methods%252C%2520address%2520this%2520by%2520dynamically%2520updating%2520collocation%250Apoints%2520during%2520training%2520but%2520may%2520overlook%2520residual%2520dynamics%2520between%2520updates%252C%250Apotentially%2520losing%2520valuable%2520information.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520an%2520adaptive%2520collocation%2520point%2520selection%2520strategy%2520utilizing%2520the%2520QR%250ADiscrete%2520Empirical%2520Interpolation%2520Method%2520%2528QR-DEIM%2529%252C%2520a%2520reduced-order%2520modeling%250Atechnique%2520for%2520efficiently%2520approximating%2520nonlinear%2520functions.%2520Our%2520results%2520on%250Abenchmark%2520PDEs%252C%2520including%2520the%2520wave%252C%2520Allen-Cahn%252C%2520and%2520Burgers%2527%2520equations%252C%250Ademonstrate%2520that%2520our%2520QR-DEIM-based%2520approach%2520improves%2520PINN%2520accuracy%2520compared%2520to%250Aexisting%2520methods%252C%2520offering%2520a%2520promising%2520direction%2520for%2520adaptive%2520collocation%2520point%250Astrategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adaptive%20Collocation%20Point%20Strategy%20For%20Physics%20Informed%20Neural%0A%20%20Networks%20via%20the%20QR%20Discrete%20Empirical%20Interpolation%20Method&entry.906535625=Adrian%20Celaya%20and%20David%20Fuentes%20and%20Beatrice%20Riviere&entry.1292438233=%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20have%20gained%20significant%20attention%0Afor%20solving%20forward%20and%20inverse%20problems%20related%20to%20partial%20differential%0Aequations%20%28PDEs%29.%20While%20advancements%20in%20loss%20functions%20and%20network%0Aarchitectures%20have%20improved%20PINN%20accuracy%2C%20the%20impact%20of%20collocation%20point%0Asampling%20on%20their%20performance%20remains%20underexplored.%20Fixed%20sampling%20methods%2C%0Asuch%20as%20uniform%20random%20sampling%20and%20equispaced%20grids%2C%20can%20fail%20to%20capture%0Acritical%20regions%20with%20high%20solution%20gradients%2C%20limiting%20their%20effectiveness%20for%0Acomplex%20PDEs.%20Adaptive%20methods%2C%20inspired%20by%20adaptive%20mesh%20refinement%20from%0Atraditional%20numerical%20methods%2C%20address%20this%20by%20dynamically%20updating%20collocation%0Apoints%20during%20training%20but%20may%20overlook%20residual%20dynamics%20between%20updates%2C%0Apotentially%20losing%20valuable%20information.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20an%20adaptive%20collocation%20point%20selection%20strategy%20utilizing%20the%20QR%0ADiscrete%20Empirical%20Interpolation%20Method%20%28QR-DEIM%29%2C%20a%20reduced-order%20modeling%0Atechnique%20for%20efficiently%20approximating%20nonlinear%20functions.%20Our%20results%20on%0Abenchmark%20PDEs%2C%20including%20the%20wave%2C%20Allen-Cahn%2C%20and%20Burgers%27%20equations%2C%0Ademonstrate%20that%20our%20QR-DEIM-based%20approach%20improves%20PINN%20accuracy%20compared%20to%0Aexisting%20methods%2C%20offering%20a%20promising%20direction%20for%20adaptive%20collocation%20point%0Astrategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07700v2&entry.124074799=Read"},
{"title": "MOGNET: A Mux-residual quantized Network leveraging Online-Generated\n  weights", "author": "Van Thien Nguyen and William Guicquero and Gilles Sicard", "abstract": "  This paper presents a compact model architecture called MOGNET, compatible\nwith a resource-limited hardware. MOGNET uses a streamlined Convolutional\nfactorization block based on a combination of 2 point-wise (1x1) convolutions\nwith a group-wise convolution in-between. To further limit the overall model\nsize and reduce the on-chip required memory, the second point-wise\nconvolution's parameters are on-line generated by a Cellular Automaton\nstructure. In addition, MOGNET enables the use of low-precision weights and\nactivations, by taking advantage of a Multiplexer mechanism with a proper\nBitshift rescaling for integrating residual paths without increasing the\nhardware-related complexity. To efficiently train this model we also introduce\na novel weight ternarization method favoring the balance between quantized\nlevels. Experimental results show that given tiny memory budget (sub-2Mb),\nMOGNET can achieve higher accuracy with a clear gap up to 1% at a similar or\neven lower model size compared to recent state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2501.09531v1", "date": "2025-01-16", "relevancy": 2.0455, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5254}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.507}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOGNET%3A%20A%20Mux-residual%20quantized%20Network%20leveraging%20Online-Generated%0A%20%20weights&body=Title%3A%20MOGNET%3A%20A%20Mux-residual%20quantized%20Network%20leveraging%20Online-Generated%0A%20%20weights%0AAuthor%3A%20Van%20Thien%20Nguyen%20and%20William%20Guicquero%20and%20Gilles%20Sicard%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20compact%20model%20architecture%20called%20MOGNET%2C%20compatible%0Awith%20a%20resource-limited%20hardware.%20MOGNET%20uses%20a%20streamlined%20Convolutional%0Afactorization%20block%20based%20on%20a%20combination%20of%202%20point-wise%20%281x1%29%20convolutions%0Awith%20a%20group-wise%20convolution%20in-between.%20To%20further%20limit%20the%20overall%20model%0Asize%20and%20reduce%20the%20on-chip%20required%20memory%2C%20the%20second%20point-wise%0Aconvolution%27s%20parameters%20are%20on-line%20generated%20by%20a%20Cellular%20Automaton%0Astructure.%20In%20addition%2C%20MOGNET%20enables%20the%20use%20of%20low-precision%20weights%20and%0Aactivations%2C%20by%20taking%20advantage%20of%20a%20Multiplexer%20mechanism%20with%20a%20proper%0ABitshift%20rescaling%20for%20integrating%20residual%20paths%20without%20increasing%20the%0Ahardware-related%20complexity.%20To%20efficiently%20train%20this%20model%20we%20also%20introduce%0Aa%20novel%20weight%20ternarization%20method%20favoring%20the%20balance%20between%20quantized%0Alevels.%20Experimental%20results%20show%20that%20given%20tiny%20memory%20budget%20%28sub-2Mb%29%2C%0AMOGNET%20can%20achieve%20higher%20accuracy%20with%20a%20clear%20gap%20up%20to%201%25%20at%20a%20similar%20or%0Aeven%20lower%20model%20size%20compared%20to%20recent%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOGNET%253A%2520A%2520Mux-residual%2520quantized%2520Network%2520leveraging%2520Online-Generated%250A%2520%2520weights%26entry.906535625%3DVan%2520Thien%2520Nguyen%2520and%2520William%2520Guicquero%2520and%2520Gilles%2520Sicard%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520compact%2520model%2520architecture%2520called%2520MOGNET%252C%2520compatible%250Awith%2520a%2520resource-limited%2520hardware.%2520MOGNET%2520uses%2520a%2520streamlined%2520Convolutional%250Afactorization%2520block%2520based%2520on%2520a%2520combination%2520of%25202%2520point-wise%2520%25281x1%2529%2520convolutions%250Awith%2520a%2520group-wise%2520convolution%2520in-between.%2520To%2520further%2520limit%2520the%2520overall%2520model%250Asize%2520and%2520reduce%2520the%2520on-chip%2520required%2520memory%252C%2520the%2520second%2520point-wise%250Aconvolution%2527s%2520parameters%2520are%2520on-line%2520generated%2520by%2520a%2520Cellular%2520Automaton%250Astructure.%2520In%2520addition%252C%2520MOGNET%2520enables%2520the%2520use%2520of%2520low-precision%2520weights%2520and%250Aactivations%252C%2520by%2520taking%2520advantage%2520of%2520a%2520Multiplexer%2520mechanism%2520with%2520a%2520proper%250ABitshift%2520rescaling%2520for%2520integrating%2520residual%2520paths%2520without%2520increasing%2520the%250Ahardware-related%2520complexity.%2520To%2520efficiently%2520train%2520this%2520model%2520we%2520also%2520introduce%250Aa%2520novel%2520weight%2520ternarization%2520method%2520favoring%2520the%2520balance%2520between%2520quantized%250Alevels.%2520Experimental%2520results%2520show%2520that%2520given%2520tiny%2520memory%2520budget%2520%2528sub-2Mb%2529%252C%250AMOGNET%2520can%2520achieve%2520higher%2520accuracy%2520with%2520a%2520clear%2520gap%2520up%2520to%25201%2525%2520at%2520a%2520similar%2520or%250Aeven%2520lower%2520model%2520size%2520compared%2520to%2520recent%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOGNET%3A%20A%20Mux-residual%20quantized%20Network%20leveraging%20Online-Generated%0A%20%20weights&entry.906535625=Van%20Thien%20Nguyen%20and%20William%20Guicquero%20and%20Gilles%20Sicard&entry.1292438233=%20%20This%20paper%20presents%20a%20compact%20model%20architecture%20called%20MOGNET%2C%20compatible%0Awith%20a%20resource-limited%20hardware.%20MOGNET%20uses%20a%20streamlined%20Convolutional%0Afactorization%20block%20based%20on%20a%20combination%20of%202%20point-wise%20%281x1%29%20convolutions%0Awith%20a%20group-wise%20convolution%20in-between.%20To%20further%20limit%20the%20overall%20model%0Asize%20and%20reduce%20the%20on-chip%20required%20memory%2C%20the%20second%20point-wise%0Aconvolution%27s%20parameters%20are%20on-line%20generated%20by%20a%20Cellular%20Automaton%0Astructure.%20In%20addition%2C%20MOGNET%20enables%20the%20use%20of%20low-precision%20weights%20and%0Aactivations%2C%20by%20taking%20advantage%20of%20a%20Multiplexer%20mechanism%20with%20a%20proper%0ABitshift%20rescaling%20for%20integrating%20residual%20paths%20without%20increasing%20the%0Ahardware-related%20complexity.%20To%20efficiently%20train%20this%20model%20we%20also%20introduce%0Aa%20novel%20weight%20ternarization%20method%20favoring%20the%20balance%20between%20quantized%0Alevels.%20Experimental%20results%20show%20that%20given%20tiny%20memory%20budget%20%28sub-2Mb%29%2C%0AMOGNET%20can%20achieve%20higher%20accuracy%20with%20a%20clear%20gap%20up%20to%201%25%20at%20a%20similar%20or%0Aeven%20lower%20model%20size%20compared%20to%20recent%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09531v1&entry.124074799=Read"},
{"title": "Using Machine Learning to Discover Parsimonious and\n  Physically-Interpretable Representations of Catchment-Scale Rainfall-Runoff\n  Dynamics", "author": "Yuan-Heng Wang and Hoshin V. Gupta", "abstract": "  Despite the excellent real-world predictive performance of modern machine\nlearning (ML) methods, many scientists remain hesitant to discard traditional\nphysical-conceptual (PC) approaches due mainly to their relative\ninterpretability, which contributes to credibility during decision-making. In\nthis context, a currently underexplored aspect of ML is how to develop\nminimally-optimal representations that can facilitate better insight regarding\nsystem functioning. Regardless of how this is achieved, it is arguably true\nthat parsimonious representations better support the advancement of scientific\nunderstanding. Our own view is that ML-based modeling of geoscientific systems\nshould be based in the use of computational units that are fundamentally\ninterpretable by design.\n  This paper continues our exploration of how the strengths of ML can be\nexploited in the service of better understanding via scientific investigation.\nHere, we use the Mass Conserving Perceptron (MCP) as the fundamental\ncomputational unit in a generic network architecture consisting of nodes\narranged in series and parallel to explore several generic and important issues\nrelated to the use of observational data for constructing input-state-output\nmodels of dynamical systems. In the context of lumped catchment modeling, we\nshow that physical interpretability and excellent predictive performance can\nboth be achieved using a relatively parsimonious distributed-state\nmultiple-flow-path network with context-dependent gating and information\nsharing across the nodes, suggesting that MCP-based modeling can play a\nsignificant role in application of ML to geoscientific investigation.\n", "link": "http://arxiv.org/abs/2412.04845v2", "date": "2025-01-16", "relevancy": 2.0428, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5569}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5168}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Machine%20Learning%20to%20Discover%20Parsimonious%20and%0A%20%20Physically-Interpretable%20Representations%20of%20Catchment-Scale%20Rainfall-Runoff%0A%20%20Dynamics&body=Title%3A%20Using%20Machine%20Learning%20to%20Discover%20Parsimonious%20and%0A%20%20Physically-Interpretable%20Representations%20of%20Catchment-Scale%20Rainfall-Runoff%0A%20%20Dynamics%0AAuthor%3A%20Yuan-Heng%20Wang%20and%20Hoshin%20V.%20Gupta%0AAbstract%3A%20%20%20Despite%20the%20excellent%20real-world%20predictive%20performance%20of%20modern%20machine%0Alearning%20%28ML%29%20methods%2C%20many%20scientists%20remain%20hesitant%20to%20discard%20traditional%0Aphysical-conceptual%20%28PC%29%20approaches%20due%20mainly%20to%20their%20relative%0Ainterpretability%2C%20which%20contributes%20to%20credibility%20during%20decision-making.%20In%0Athis%20context%2C%20a%20currently%20underexplored%20aspect%20of%20ML%20is%20how%20to%20develop%0Aminimally-optimal%20representations%20that%20can%20facilitate%20better%20insight%20regarding%0Asystem%20functioning.%20Regardless%20of%20how%20this%20is%20achieved%2C%20it%20is%20arguably%20true%0Athat%20parsimonious%20representations%20better%20support%20the%20advancement%20of%20scientific%0Aunderstanding.%20Our%20own%20view%20is%20that%20ML-based%20modeling%20of%20geoscientific%20systems%0Ashould%20be%20based%20in%20the%20use%20of%20computational%20units%20that%20are%20fundamentally%0Ainterpretable%20by%20design.%0A%20%20This%20paper%20continues%20our%20exploration%20of%20how%20the%20strengths%20of%20ML%20can%20be%0Aexploited%20in%20the%20service%20of%20better%20understanding%20via%20scientific%20investigation.%0AHere%2C%20we%20use%20the%20Mass%20Conserving%20Perceptron%20%28MCP%29%20as%20the%20fundamental%0Acomputational%20unit%20in%20a%20generic%20network%20architecture%20consisting%20of%20nodes%0Aarranged%20in%20series%20and%20parallel%20to%20explore%20several%20generic%20and%20important%20issues%0Arelated%20to%20the%20use%20of%20observational%20data%20for%20constructing%20input-state-output%0Amodels%20of%20dynamical%20systems.%20In%20the%20context%20of%20lumped%20catchment%20modeling%2C%20we%0Ashow%20that%20physical%20interpretability%20and%20excellent%20predictive%20performance%20can%0Aboth%20be%20achieved%20using%20a%20relatively%20parsimonious%20distributed-state%0Amultiple-flow-path%20network%20with%20context-dependent%20gating%20and%20information%0Asharing%20across%20the%20nodes%2C%20suggesting%20that%20MCP-based%20modeling%20can%20play%20a%0Asignificant%20role%20in%20application%20of%20ML%20to%20geoscientific%20investigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04845v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Machine%2520Learning%2520to%2520Discover%2520Parsimonious%2520and%250A%2520%2520Physically-Interpretable%2520Representations%2520of%2520Catchment-Scale%2520Rainfall-Runoff%250A%2520%2520Dynamics%26entry.906535625%3DYuan-Heng%2520Wang%2520and%2520Hoshin%2520V.%2520Gupta%26entry.1292438233%3D%2520%2520Despite%2520the%2520excellent%2520real-world%2520predictive%2520performance%2520of%2520modern%2520machine%250Alearning%2520%2528ML%2529%2520methods%252C%2520many%2520scientists%2520remain%2520hesitant%2520to%2520discard%2520traditional%250Aphysical-conceptual%2520%2528PC%2529%2520approaches%2520due%2520mainly%2520to%2520their%2520relative%250Ainterpretability%252C%2520which%2520contributes%2520to%2520credibility%2520during%2520decision-making.%2520In%250Athis%2520context%252C%2520a%2520currently%2520underexplored%2520aspect%2520of%2520ML%2520is%2520how%2520to%2520develop%250Aminimally-optimal%2520representations%2520that%2520can%2520facilitate%2520better%2520insight%2520regarding%250Asystem%2520functioning.%2520Regardless%2520of%2520how%2520this%2520is%2520achieved%252C%2520it%2520is%2520arguably%2520true%250Athat%2520parsimonious%2520representations%2520better%2520support%2520the%2520advancement%2520of%2520scientific%250Aunderstanding.%2520Our%2520own%2520view%2520is%2520that%2520ML-based%2520modeling%2520of%2520geoscientific%2520systems%250Ashould%2520be%2520based%2520in%2520the%2520use%2520of%2520computational%2520units%2520that%2520are%2520fundamentally%250Ainterpretable%2520by%2520design.%250A%2520%2520This%2520paper%2520continues%2520our%2520exploration%2520of%2520how%2520the%2520strengths%2520of%2520ML%2520can%2520be%250Aexploited%2520in%2520the%2520service%2520of%2520better%2520understanding%2520via%2520scientific%2520investigation.%250AHere%252C%2520we%2520use%2520the%2520Mass%2520Conserving%2520Perceptron%2520%2528MCP%2529%2520as%2520the%2520fundamental%250Acomputational%2520unit%2520in%2520a%2520generic%2520network%2520architecture%2520consisting%2520of%2520nodes%250Aarranged%2520in%2520series%2520and%2520parallel%2520to%2520explore%2520several%2520generic%2520and%2520important%2520issues%250Arelated%2520to%2520the%2520use%2520of%2520observational%2520data%2520for%2520constructing%2520input-state-output%250Amodels%2520of%2520dynamical%2520systems.%2520In%2520the%2520context%2520of%2520lumped%2520catchment%2520modeling%252C%2520we%250Ashow%2520that%2520physical%2520interpretability%2520and%2520excellent%2520predictive%2520performance%2520can%250Aboth%2520be%2520achieved%2520using%2520a%2520relatively%2520parsimonious%2520distributed-state%250Amultiple-flow-path%2520network%2520with%2520context-dependent%2520gating%2520and%2520information%250Asharing%2520across%2520the%2520nodes%252C%2520suggesting%2520that%2520MCP-based%2520modeling%2520can%2520play%2520a%250Asignificant%2520role%2520in%2520application%2520of%2520ML%2520to%2520geoscientific%2520investigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04845v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Machine%20Learning%20to%20Discover%20Parsimonious%20and%0A%20%20Physically-Interpretable%20Representations%20of%20Catchment-Scale%20Rainfall-Runoff%0A%20%20Dynamics&entry.906535625=Yuan-Heng%20Wang%20and%20Hoshin%20V.%20Gupta&entry.1292438233=%20%20Despite%20the%20excellent%20real-world%20predictive%20performance%20of%20modern%20machine%0Alearning%20%28ML%29%20methods%2C%20many%20scientists%20remain%20hesitant%20to%20discard%20traditional%0Aphysical-conceptual%20%28PC%29%20approaches%20due%20mainly%20to%20their%20relative%0Ainterpretability%2C%20which%20contributes%20to%20credibility%20during%20decision-making.%20In%0Athis%20context%2C%20a%20currently%20underexplored%20aspect%20of%20ML%20is%20how%20to%20develop%0Aminimally-optimal%20representations%20that%20can%20facilitate%20better%20insight%20regarding%0Asystem%20functioning.%20Regardless%20of%20how%20this%20is%20achieved%2C%20it%20is%20arguably%20true%0Athat%20parsimonious%20representations%20better%20support%20the%20advancement%20of%20scientific%0Aunderstanding.%20Our%20own%20view%20is%20that%20ML-based%20modeling%20of%20geoscientific%20systems%0Ashould%20be%20based%20in%20the%20use%20of%20computational%20units%20that%20are%20fundamentally%0Ainterpretable%20by%20design.%0A%20%20This%20paper%20continues%20our%20exploration%20of%20how%20the%20strengths%20of%20ML%20can%20be%0Aexploited%20in%20the%20service%20of%20better%20understanding%20via%20scientific%20investigation.%0AHere%2C%20we%20use%20the%20Mass%20Conserving%20Perceptron%20%28MCP%29%20as%20the%20fundamental%0Acomputational%20unit%20in%20a%20generic%20network%20architecture%20consisting%20of%20nodes%0Aarranged%20in%20series%20and%20parallel%20to%20explore%20several%20generic%20and%20important%20issues%0Arelated%20to%20the%20use%20of%20observational%20data%20for%20constructing%20input-state-output%0Amodels%20of%20dynamical%20systems.%20In%20the%20context%20of%20lumped%20catchment%20modeling%2C%20we%0Ashow%20that%20physical%20interpretability%20and%20excellent%20predictive%20performance%20can%0Aboth%20be%20achieved%20using%20a%20relatively%20parsimonious%20distributed-state%0Amultiple-flow-path%20network%20with%20context-dependent%20gating%20and%20information%0Asharing%20across%20the%20nodes%2C%20suggesting%20that%20MCP-based%20modeling%20can%20play%20a%0Asignificant%20role%20in%20application%20of%20ML%20to%20geoscientific%20investigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04845v2&entry.124074799=Read"},
{"title": "SRE-Conv: Symmetric Rotation Equivariant Convolution for Biomedical\n  Image Classification", "author": "Yuexi Du and Jiazhen Zhang and Tal Zeevi and Nicha C. Dvornek and John A. Onofrey", "abstract": "  Convolutional neural networks (CNNs) are essential tools for computer vision\ntasks, but they lack traditionally desired properties of extracted features\nthat could further improve model performance, e.g., rotational equivariance.\nSuch properties are ubiquitous in biomedical images, which often lack explicit\norientation. While current work largely relies on data augmentation or explicit\nmodules to capture orientation information, this comes at the expense of\nincreased training costs or ineffective approximations of the desired\nequivariance. To overcome these challenges, we propose a novel and efficient\nimplementation of the Symmetric Rotation-Equivariant (SRE) Convolution\n(SRE-Conv) kernel, designed to learn rotation-invariant features while\nsimultaneously compressing the model size. The SRE-Conv kernel can easily be\nincorporated into any CNN backbone. We validate the ability of a deep SRE-CNN\nto capture equivariance to rotation using the public MedMNISTv2 dataset (16\ntotal tasks). SRE-Conv-CNN demonstrated improved rotated image classification\nperformance accuracy on all 16 test datasets in both 2D and 3D images, all\nwhile increasing efficiency with fewer parameters and reduced memory footprint.\nThe code is available at https://github.com/XYPB/SRE-Conv.\n", "link": "http://arxiv.org/abs/2501.09753v1", "date": "2025-01-16", "relevancy": 2.0189, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5153}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5062}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRE-Conv%3A%20Symmetric%20Rotation%20Equivariant%20Convolution%20for%20Biomedical%0A%20%20Image%20Classification&body=Title%3A%20SRE-Conv%3A%20Symmetric%20Rotation%20Equivariant%20Convolution%20for%20Biomedical%0A%20%20Image%20Classification%0AAuthor%3A%20Yuexi%20Du%20and%20Jiazhen%20Zhang%20and%20Tal%20Zeevi%20and%20Nicha%20C.%20Dvornek%20and%20John%20A.%20Onofrey%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20essential%20tools%20for%20computer%20vision%0Atasks%2C%20but%20they%20lack%20traditionally%20desired%20properties%20of%20extracted%20features%0Athat%20could%20further%20improve%20model%20performance%2C%20e.g.%2C%20rotational%20equivariance.%0ASuch%20properties%20are%20ubiquitous%20in%20biomedical%20images%2C%20which%20often%20lack%20explicit%0Aorientation.%20While%20current%20work%20largely%20relies%20on%20data%20augmentation%20or%20explicit%0Amodules%20to%20capture%20orientation%20information%2C%20this%20comes%20at%20the%20expense%20of%0Aincreased%20training%20costs%20or%20ineffective%20approximations%20of%20the%20desired%0Aequivariance.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%20novel%20and%20efficient%0Aimplementation%20of%20the%20Symmetric%20Rotation-Equivariant%20%28SRE%29%20Convolution%0A%28SRE-Conv%29%20kernel%2C%20designed%20to%20learn%20rotation-invariant%20features%20while%0Asimultaneously%20compressing%20the%20model%20size.%20The%20SRE-Conv%20kernel%20can%20easily%20be%0Aincorporated%20into%20any%20CNN%20backbone.%20We%20validate%20the%20ability%20of%20a%20deep%20SRE-CNN%0Ato%20capture%20equivariance%20to%20rotation%20using%20the%20public%20MedMNISTv2%20dataset%20%2816%0Atotal%20tasks%29.%20SRE-Conv-CNN%20demonstrated%20improved%20rotated%20image%20classification%0Aperformance%20accuracy%20on%20all%2016%20test%20datasets%20in%20both%202D%20and%203D%20images%2C%20all%0Awhile%20increasing%20efficiency%20with%20fewer%20parameters%20and%20reduced%20memory%20footprint.%0AThe%20code%20is%20available%20at%20https%3A//github.com/XYPB/SRE-Conv.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRE-Conv%253A%2520Symmetric%2520Rotation%2520Equivariant%2520Convolution%2520for%2520Biomedical%250A%2520%2520Image%2520Classification%26entry.906535625%3DYuexi%2520Du%2520and%2520Jiazhen%2520Zhang%2520and%2520Tal%2520Zeevi%2520and%2520Nicha%2520C.%2520Dvornek%2520and%2520John%2520A.%2520Onofrey%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520are%2520essential%2520tools%2520for%2520computer%2520vision%250Atasks%252C%2520but%2520they%2520lack%2520traditionally%2520desired%2520properties%2520of%2520extracted%2520features%250Athat%2520could%2520further%2520improve%2520model%2520performance%252C%2520e.g.%252C%2520rotational%2520equivariance.%250ASuch%2520properties%2520are%2520ubiquitous%2520in%2520biomedical%2520images%252C%2520which%2520often%2520lack%2520explicit%250Aorientation.%2520While%2520current%2520work%2520largely%2520relies%2520on%2520data%2520augmentation%2520or%2520explicit%250Amodules%2520to%2520capture%2520orientation%2520information%252C%2520this%2520comes%2520at%2520the%2520expense%2520of%250Aincreased%2520training%2520costs%2520or%2520ineffective%2520approximations%2520of%2520the%2520desired%250Aequivariance.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520and%2520efficient%250Aimplementation%2520of%2520the%2520Symmetric%2520Rotation-Equivariant%2520%2528SRE%2529%2520Convolution%250A%2528SRE-Conv%2529%2520kernel%252C%2520designed%2520to%2520learn%2520rotation-invariant%2520features%2520while%250Asimultaneously%2520compressing%2520the%2520model%2520size.%2520The%2520SRE-Conv%2520kernel%2520can%2520easily%2520be%250Aincorporated%2520into%2520any%2520CNN%2520backbone.%2520We%2520validate%2520the%2520ability%2520of%2520a%2520deep%2520SRE-CNN%250Ato%2520capture%2520equivariance%2520to%2520rotation%2520using%2520the%2520public%2520MedMNISTv2%2520dataset%2520%252816%250Atotal%2520tasks%2529.%2520SRE-Conv-CNN%2520demonstrated%2520improved%2520rotated%2520image%2520classification%250Aperformance%2520accuracy%2520on%2520all%252016%2520test%2520datasets%2520in%2520both%25202D%2520and%25203D%2520images%252C%2520all%250Awhile%2520increasing%2520efficiency%2520with%2520fewer%2520parameters%2520and%2520reduced%2520memory%2520footprint.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/XYPB/SRE-Conv.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRE-Conv%3A%20Symmetric%20Rotation%20Equivariant%20Convolution%20for%20Biomedical%0A%20%20Image%20Classification&entry.906535625=Yuexi%20Du%20and%20Jiazhen%20Zhang%20and%20Tal%20Zeevi%20and%20Nicha%20C.%20Dvornek%20and%20John%20A.%20Onofrey&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20essential%20tools%20for%20computer%20vision%0Atasks%2C%20but%20they%20lack%20traditionally%20desired%20properties%20of%20extracted%20features%0Athat%20could%20further%20improve%20model%20performance%2C%20e.g.%2C%20rotational%20equivariance.%0ASuch%20properties%20are%20ubiquitous%20in%20biomedical%20images%2C%20which%20often%20lack%20explicit%0Aorientation.%20While%20current%20work%20largely%20relies%20on%20data%20augmentation%20or%20explicit%0Amodules%20to%20capture%20orientation%20information%2C%20this%20comes%20at%20the%20expense%20of%0Aincreased%20training%20costs%20or%20ineffective%20approximations%20of%20the%20desired%0Aequivariance.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%20novel%20and%20efficient%0Aimplementation%20of%20the%20Symmetric%20Rotation-Equivariant%20%28SRE%29%20Convolution%0A%28SRE-Conv%29%20kernel%2C%20designed%20to%20learn%20rotation-invariant%20features%20while%0Asimultaneously%20compressing%20the%20model%20size.%20The%20SRE-Conv%20kernel%20can%20easily%20be%0Aincorporated%20into%20any%20CNN%20backbone.%20We%20validate%20the%20ability%20of%20a%20deep%20SRE-CNN%0Ato%20capture%20equivariance%20to%20rotation%20using%20the%20public%20MedMNISTv2%20dataset%20%2816%0Atotal%20tasks%29.%20SRE-Conv-CNN%20demonstrated%20improved%20rotated%20image%20classification%0Aperformance%20accuracy%20on%20all%2016%20test%20datasets%20in%20both%202D%20and%203D%20images%2C%20all%0Awhile%20increasing%20efficiency%20with%20fewer%20parameters%20and%20reduced%20memory%20footprint.%0AThe%20code%20is%20available%20at%20https%3A//github.com/XYPB/SRE-Conv.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09753v1&entry.124074799=Read"},
{"title": "Model Predictive Path Integral Docking of Fully Actuated Surface Vessel", "author": "Akash Vijayakumar and Atmanand M A and Abhilash Somayajula", "abstract": "  Autonomous docking remains one of the most challenging maneuvers in marine\nrobotics, requiring precise control and robust perception in confined spaces.\nThis paper presents a novel approach integrating Model Predictive Path\nIntegral(MPPI) control with real-time LiDAR-based dock detection for autonomous\nsurface vessel docking. Our framework uniquely combines probabilistic\ntrajectory optimization with a multiobjective cost function that simultaneously\nconsiders docking precision, safety constraints, and motion efficiency. The\nMPPI controller generates optimal trajectories by intelligently sampling\ncontrol sequences and evaluating their costs based on dynamic clearance\nrequirements, orientation alignment, and target position objectives. We\nintroduce an adaptive dock detection pipeline that processes LiDAR point clouds\nto extract critical geometric features, enabling real-time updates of docking\nparameters. The proposed method is extensively validated in a physics-based\nsimulation environment that incorporates realistic sensor noise, vessel\ndynamics, and environmental constraints. Results demonstrate successful docking\nfrom various initial positions while maintaining safe clearances and smooth\nmotion characteristics.\n", "link": "http://arxiv.org/abs/2501.09668v1", "date": "2025-01-16", "relevancy": 2.0173, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5681}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5002}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Predictive%20Path%20Integral%20Docking%20of%20Fully%20Actuated%20Surface%20Vessel&body=Title%3A%20Model%20Predictive%20Path%20Integral%20Docking%20of%20Fully%20Actuated%20Surface%20Vessel%0AAuthor%3A%20Akash%20Vijayakumar%20and%20Atmanand%20M%20A%20and%20Abhilash%20Somayajula%0AAbstract%3A%20%20%20Autonomous%20docking%20remains%20one%20of%20the%20most%20challenging%20maneuvers%20in%20marine%0Arobotics%2C%20requiring%20precise%20control%20and%20robust%20perception%20in%20confined%20spaces.%0AThis%20paper%20presents%20a%20novel%20approach%20integrating%20Model%20Predictive%20Path%0AIntegral%28MPPI%29%20control%20with%20real-time%20LiDAR-based%20dock%20detection%20for%20autonomous%0Asurface%20vessel%20docking.%20Our%20framework%20uniquely%20combines%20probabilistic%0Atrajectory%20optimization%20with%20a%20multiobjective%20cost%20function%20that%20simultaneously%0Aconsiders%20docking%20precision%2C%20safety%20constraints%2C%20and%20motion%20efficiency.%20The%0AMPPI%20controller%20generates%20optimal%20trajectories%20by%20intelligently%20sampling%0Acontrol%20sequences%20and%20evaluating%20their%20costs%20based%20on%20dynamic%20clearance%0Arequirements%2C%20orientation%20alignment%2C%20and%20target%20position%20objectives.%20We%0Aintroduce%20an%20adaptive%20dock%20detection%20pipeline%20that%20processes%20LiDAR%20point%20clouds%0Ato%20extract%20critical%20geometric%20features%2C%20enabling%20real-time%20updates%20of%20docking%0Aparameters.%20The%20proposed%20method%20is%20extensively%20validated%20in%20a%20physics-based%0Asimulation%20environment%20that%20incorporates%20realistic%20sensor%20noise%2C%20vessel%0Adynamics%2C%20and%20environmental%20constraints.%20Results%20demonstrate%20successful%20docking%0Afrom%20various%20initial%20positions%20while%20maintaining%20safe%20clearances%20and%20smooth%0Amotion%20characteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Predictive%2520Path%2520Integral%2520Docking%2520of%2520Fully%2520Actuated%2520Surface%2520Vessel%26entry.906535625%3DAkash%2520Vijayakumar%2520and%2520Atmanand%2520M%2520A%2520and%2520Abhilash%2520Somayajula%26entry.1292438233%3D%2520%2520Autonomous%2520docking%2520remains%2520one%2520of%2520the%2520most%2520challenging%2520maneuvers%2520in%2520marine%250Arobotics%252C%2520requiring%2520precise%2520control%2520and%2520robust%2520perception%2520in%2520confined%2520spaces.%250AThis%2520paper%2520presents%2520a%2520novel%2520approach%2520integrating%2520Model%2520Predictive%2520Path%250AIntegral%2528MPPI%2529%2520control%2520with%2520real-time%2520LiDAR-based%2520dock%2520detection%2520for%2520autonomous%250Asurface%2520vessel%2520docking.%2520Our%2520framework%2520uniquely%2520combines%2520probabilistic%250Atrajectory%2520optimization%2520with%2520a%2520multiobjective%2520cost%2520function%2520that%2520simultaneously%250Aconsiders%2520docking%2520precision%252C%2520safety%2520constraints%252C%2520and%2520motion%2520efficiency.%2520The%250AMPPI%2520controller%2520generates%2520optimal%2520trajectories%2520by%2520intelligently%2520sampling%250Acontrol%2520sequences%2520and%2520evaluating%2520their%2520costs%2520based%2520on%2520dynamic%2520clearance%250Arequirements%252C%2520orientation%2520alignment%252C%2520and%2520target%2520position%2520objectives.%2520We%250Aintroduce%2520an%2520adaptive%2520dock%2520detection%2520pipeline%2520that%2520processes%2520LiDAR%2520point%2520clouds%250Ato%2520extract%2520critical%2520geometric%2520features%252C%2520enabling%2520real-time%2520updates%2520of%2520docking%250Aparameters.%2520The%2520proposed%2520method%2520is%2520extensively%2520validated%2520in%2520a%2520physics-based%250Asimulation%2520environment%2520that%2520incorporates%2520realistic%2520sensor%2520noise%252C%2520vessel%250Adynamics%252C%2520and%2520environmental%2520constraints.%2520Results%2520demonstrate%2520successful%2520docking%250Afrom%2520various%2520initial%2520positions%2520while%2520maintaining%2520safe%2520clearances%2520and%2520smooth%250Amotion%2520characteristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Predictive%20Path%20Integral%20Docking%20of%20Fully%20Actuated%20Surface%20Vessel&entry.906535625=Akash%20Vijayakumar%20and%20Atmanand%20M%20A%20and%20Abhilash%20Somayajula&entry.1292438233=%20%20Autonomous%20docking%20remains%20one%20of%20the%20most%20challenging%20maneuvers%20in%20marine%0Arobotics%2C%20requiring%20precise%20control%20and%20robust%20perception%20in%20confined%20spaces.%0AThis%20paper%20presents%20a%20novel%20approach%20integrating%20Model%20Predictive%20Path%0AIntegral%28MPPI%29%20control%20with%20real-time%20LiDAR-based%20dock%20detection%20for%20autonomous%0Asurface%20vessel%20docking.%20Our%20framework%20uniquely%20combines%20probabilistic%0Atrajectory%20optimization%20with%20a%20multiobjective%20cost%20function%20that%20simultaneously%0Aconsiders%20docking%20precision%2C%20safety%20constraints%2C%20and%20motion%20efficiency.%20The%0AMPPI%20controller%20generates%20optimal%20trajectories%20by%20intelligently%20sampling%0Acontrol%20sequences%20and%20evaluating%20their%20costs%20based%20on%20dynamic%20clearance%0Arequirements%2C%20orientation%20alignment%2C%20and%20target%20position%20objectives.%20We%0Aintroduce%20an%20adaptive%20dock%20detection%20pipeline%20that%20processes%20LiDAR%20point%20clouds%0Ato%20extract%20critical%20geometric%20features%2C%20enabling%20real-time%20updates%20of%20docking%0Aparameters.%20The%20proposed%20method%20is%20extensively%20validated%20in%20a%20physics-based%0Asimulation%20environment%20that%20incorporates%20realistic%20sensor%20noise%2C%20vessel%0Adynamics%2C%20and%20environmental%20constraints.%20Results%20demonstrate%20successful%20docking%0Afrom%20various%20initial%20positions%20while%20maintaining%20safe%20clearances%20and%20smooth%0Amotion%20characteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09668v1&entry.124074799=Read"},
{"title": "Aligning Brain Activity with Advanced Transformer Models: Exploring the\n  Role of Punctuation in Semantic Processing", "author": "Zenon Lamprou and Frank Polick and Yashar Moshfeghi", "abstract": "  This research examines the congruence between neural activity and advanced\ntransformer models, emphasizing the semantic significance of punctuation in\ntext understanding. Utilizing an innovative approach originally proposed by\nToneva and Wehbe, we evaluate four advanced transformer models RoBERTa,\nDistiliBERT, ALBERT, and ELECTRA against neural activity data. Our findings\nindicate that RoBERTa exhibits the closest alignment with neural activity,\nsurpassing BERT in accuracy. Furthermore, we investigate the impact of\npunctuation removal on model performance and neural alignment, revealing that\nBERT's accuracy enhances in the absence of punctuation. This study contributes\nto the comprehension of how neural networks represent language and the\ninfluence of punctuation on semantic processing within the human brain.\n", "link": "http://arxiv.org/abs/2501.06278v2", "date": "2025-01-16", "relevancy": 2.0122, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Brain%20Activity%20with%20Advanced%20Transformer%20Models%3A%20Exploring%20the%0A%20%20Role%20of%20Punctuation%20in%20Semantic%20Processing&body=Title%3A%20Aligning%20Brain%20Activity%20with%20Advanced%20Transformer%20Models%3A%20Exploring%20the%0A%20%20Role%20of%20Punctuation%20in%20Semantic%20Processing%0AAuthor%3A%20Zenon%20Lamprou%20and%20Frank%20Polick%20and%20Yashar%20Moshfeghi%0AAbstract%3A%20%20%20This%20research%20examines%20the%20congruence%20between%20neural%20activity%20and%20advanced%0Atransformer%20models%2C%20emphasizing%20the%20semantic%20significance%20of%20punctuation%20in%0Atext%20understanding.%20Utilizing%20an%20innovative%20approach%20originally%20proposed%20by%0AToneva%20and%20Wehbe%2C%20we%20evaluate%20four%20advanced%20transformer%20models%20RoBERTa%2C%0ADistiliBERT%2C%20ALBERT%2C%20and%20ELECTRA%20against%20neural%20activity%20data.%20Our%20findings%0Aindicate%20that%20RoBERTa%20exhibits%20the%20closest%20alignment%20with%20neural%20activity%2C%0Asurpassing%20BERT%20in%20accuracy.%20Furthermore%2C%20we%20investigate%20the%20impact%20of%0Apunctuation%20removal%20on%20model%20performance%20and%20neural%20alignment%2C%20revealing%20that%0ABERT%27s%20accuracy%20enhances%20in%20the%20absence%20of%20punctuation.%20This%20study%20contributes%0Ato%20the%20comprehension%20of%20how%20neural%20networks%20represent%20language%20and%20the%0Ainfluence%20of%20punctuation%20on%20semantic%20processing%20within%20the%20human%20brain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Brain%2520Activity%2520with%2520Advanced%2520Transformer%2520Models%253A%2520Exploring%2520the%250A%2520%2520Role%2520of%2520Punctuation%2520in%2520Semantic%2520Processing%26entry.906535625%3DZenon%2520Lamprou%2520and%2520Frank%2520Polick%2520and%2520Yashar%2520Moshfeghi%26entry.1292438233%3D%2520%2520This%2520research%2520examines%2520the%2520congruence%2520between%2520neural%2520activity%2520and%2520advanced%250Atransformer%2520models%252C%2520emphasizing%2520the%2520semantic%2520significance%2520of%2520punctuation%2520in%250Atext%2520understanding.%2520Utilizing%2520an%2520innovative%2520approach%2520originally%2520proposed%2520by%250AToneva%2520and%2520Wehbe%252C%2520we%2520evaluate%2520four%2520advanced%2520transformer%2520models%2520RoBERTa%252C%250ADistiliBERT%252C%2520ALBERT%252C%2520and%2520ELECTRA%2520against%2520neural%2520activity%2520data.%2520Our%2520findings%250Aindicate%2520that%2520RoBERTa%2520exhibits%2520the%2520closest%2520alignment%2520with%2520neural%2520activity%252C%250Asurpassing%2520BERT%2520in%2520accuracy.%2520Furthermore%252C%2520we%2520investigate%2520the%2520impact%2520of%250Apunctuation%2520removal%2520on%2520model%2520performance%2520and%2520neural%2520alignment%252C%2520revealing%2520that%250ABERT%2527s%2520accuracy%2520enhances%2520in%2520the%2520absence%2520of%2520punctuation.%2520This%2520study%2520contributes%250Ato%2520the%2520comprehension%2520of%2520how%2520neural%2520networks%2520represent%2520language%2520and%2520the%250Ainfluence%2520of%2520punctuation%2520on%2520semantic%2520processing%2520within%2520the%2520human%2520brain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Brain%20Activity%20with%20Advanced%20Transformer%20Models%3A%20Exploring%20the%0A%20%20Role%20of%20Punctuation%20in%20Semantic%20Processing&entry.906535625=Zenon%20Lamprou%20and%20Frank%20Polick%20and%20Yashar%20Moshfeghi&entry.1292438233=%20%20This%20research%20examines%20the%20congruence%20between%20neural%20activity%20and%20advanced%0Atransformer%20models%2C%20emphasizing%20the%20semantic%20significance%20of%20punctuation%20in%0Atext%20understanding.%20Utilizing%20an%20innovative%20approach%20originally%20proposed%20by%0AToneva%20and%20Wehbe%2C%20we%20evaluate%20four%20advanced%20transformer%20models%20RoBERTa%2C%0ADistiliBERT%2C%20ALBERT%2C%20and%20ELECTRA%20against%20neural%20activity%20data.%20Our%20findings%0Aindicate%20that%20RoBERTa%20exhibits%20the%20closest%20alignment%20with%20neural%20activity%2C%0Asurpassing%20BERT%20in%20accuracy.%20Furthermore%2C%20we%20investigate%20the%20impact%20of%0Apunctuation%20removal%20on%20model%20performance%20and%20neural%20alignment%2C%20revealing%20that%0ABERT%27s%20accuracy%20enhances%20in%20the%20absence%20of%20punctuation.%20This%20study%20contributes%0Ato%20the%20comprehension%20of%20how%20neural%20networks%20represent%20language%20and%20the%0Ainfluence%20of%20punctuation%20on%20semantic%20processing%20within%20the%20human%20brain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06278v2&entry.124074799=Read"},
{"title": "On the uncertainty principle of neural networks", "author": "Jun-Jie Zhang and Dong-Xiao Zhang and Jian-Nan Chen and Long-Gang Pang and Deyu Meng", "abstract": "  In this study, we explore the inherent trade-off between accuracy and\nrobustness in neural networks, drawing an analogy to the uncertainty principle\nin quantum mechanics. We propose that neural networks are subject to an\nuncertainty relation, which manifests as a fundamental limitation in their\nability to simultaneously achieve high accuracy and robustness against\nadversarial attacks. Through mathematical proofs and empirical evidence, we\ndemonstrate that this trade-off is a natural consequence of the sharp\nboundaries formed between different class concepts during training. Our\nfindings reveal that the complementarity principle, a cornerstone of quantum\nphysics, applies to neural networks, imposing fundamental limits on their\ncapabilities in simultaneous learning of conjugate features. Meanwhile, our\nwork suggests that achieving human-level intelligence through a single network\narchitecture or massive datasets alone may be inherently limited. Our work\nprovides new insights into the theoretical foundations of neural network\nvulnerability and opens up avenues for designing more robust neural network\narchitectures.\n", "link": "http://arxiv.org/abs/2205.01493v4", "date": "2025-01-16", "relevancy": 2.0081, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5215}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4899}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20uncertainty%20principle%20of%20neural%20networks&body=Title%3A%20On%20the%20uncertainty%20principle%20of%20neural%20networks%0AAuthor%3A%20Jun-Jie%20Zhang%20and%20Dong-Xiao%20Zhang%20and%20Jian-Nan%20Chen%20and%20Long-Gang%20Pang%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20explore%20the%20inherent%20trade-off%20between%20accuracy%20and%0Arobustness%20in%20neural%20networks%2C%20drawing%20an%20analogy%20to%20the%20uncertainty%20principle%0Ain%20quantum%20mechanics.%20We%20propose%20that%20neural%20networks%20are%20subject%20to%20an%0Auncertainty%20relation%2C%20which%20manifests%20as%20a%20fundamental%20limitation%20in%20their%0Aability%20to%20simultaneously%20achieve%20high%20accuracy%20and%20robustness%20against%0Aadversarial%20attacks.%20Through%20mathematical%20proofs%20and%20empirical%20evidence%2C%20we%0Ademonstrate%20that%20this%20trade-off%20is%20a%20natural%20consequence%20of%20the%20sharp%0Aboundaries%20formed%20between%20different%20class%20concepts%20during%20training.%20Our%0Afindings%20reveal%20that%20the%20complementarity%20principle%2C%20a%20cornerstone%20of%20quantum%0Aphysics%2C%20applies%20to%20neural%20networks%2C%20imposing%20fundamental%20limits%20on%20their%0Acapabilities%20in%20simultaneous%20learning%20of%20conjugate%20features.%20Meanwhile%2C%20our%0Awork%20suggests%20that%20achieving%20human-level%20intelligence%20through%20a%20single%20network%0Aarchitecture%20or%20massive%20datasets%20alone%20may%20be%20inherently%20limited.%20Our%20work%0Aprovides%20new%20insights%20into%20the%20theoretical%20foundations%20of%20neural%20network%0Avulnerability%20and%20opens%20up%20avenues%20for%20designing%20more%20robust%20neural%20network%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.01493v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520uncertainty%2520principle%2520of%2520neural%2520networks%26entry.906535625%3DJun-Jie%2520Zhang%2520and%2520Dong-Xiao%2520Zhang%2520and%2520Jian-Nan%2520Chen%2520and%2520Long-Gang%2520Pang%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520explore%2520the%2520inherent%2520trade-off%2520between%2520accuracy%2520and%250Arobustness%2520in%2520neural%2520networks%252C%2520drawing%2520an%2520analogy%2520to%2520the%2520uncertainty%2520principle%250Ain%2520quantum%2520mechanics.%2520We%2520propose%2520that%2520neural%2520networks%2520are%2520subject%2520to%2520an%250Auncertainty%2520relation%252C%2520which%2520manifests%2520as%2520a%2520fundamental%2520limitation%2520in%2520their%250Aability%2520to%2520simultaneously%2520achieve%2520high%2520accuracy%2520and%2520robustness%2520against%250Aadversarial%2520attacks.%2520Through%2520mathematical%2520proofs%2520and%2520empirical%2520evidence%252C%2520we%250Ademonstrate%2520that%2520this%2520trade-off%2520is%2520a%2520natural%2520consequence%2520of%2520the%2520sharp%250Aboundaries%2520formed%2520between%2520different%2520class%2520concepts%2520during%2520training.%2520Our%250Afindings%2520reveal%2520that%2520the%2520complementarity%2520principle%252C%2520a%2520cornerstone%2520of%2520quantum%250Aphysics%252C%2520applies%2520to%2520neural%2520networks%252C%2520imposing%2520fundamental%2520limits%2520on%2520their%250Acapabilities%2520in%2520simultaneous%2520learning%2520of%2520conjugate%2520features.%2520Meanwhile%252C%2520our%250Awork%2520suggests%2520that%2520achieving%2520human-level%2520intelligence%2520through%2520a%2520single%2520network%250Aarchitecture%2520or%2520massive%2520datasets%2520alone%2520may%2520be%2520inherently%2520limited.%2520Our%2520work%250Aprovides%2520new%2520insights%2520into%2520the%2520theoretical%2520foundations%2520of%2520neural%2520network%250Avulnerability%2520and%2520opens%2520up%2520avenues%2520for%2520designing%2520more%2520robust%2520neural%2520network%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.01493v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20uncertainty%20principle%20of%20neural%20networks&entry.906535625=Jun-Jie%20Zhang%20and%20Dong-Xiao%20Zhang%20and%20Jian-Nan%20Chen%20and%20Long-Gang%20Pang%20and%20Deyu%20Meng&entry.1292438233=%20%20In%20this%20study%2C%20we%20explore%20the%20inherent%20trade-off%20between%20accuracy%20and%0Arobustness%20in%20neural%20networks%2C%20drawing%20an%20analogy%20to%20the%20uncertainty%20principle%0Ain%20quantum%20mechanics.%20We%20propose%20that%20neural%20networks%20are%20subject%20to%20an%0Auncertainty%20relation%2C%20which%20manifests%20as%20a%20fundamental%20limitation%20in%20their%0Aability%20to%20simultaneously%20achieve%20high%20accuracy%20and%20robustness%20against%0Aadversarial%20attacks.%20Through%20mathematical%20proofs%20and%20empirical%20evidence%2C%20we%0Ademonstrate%20that%20this%20trade-off%20is%20a%20natural%20consequence%20of%20the%20sharp%0Aboundaries%20formed%20between%20different%20class%20concepts%20during%20training.%20Our%0Afindings%20reveal%20that%20the%20complementarity%20principle%2C%20a%20cornerstone%20of%20quantum%0Aphysics%2C%20applies%20to%20neural%20networks%2C%20imposing%20fundamental%20limits%20on%20their%0Acapabilities%20in%20simultaneous%20learning%20of%20conjugate%20features.%20Meanwhile%2C%20our%0Awork%20suggests%20that%20achieving%20human-level%20intelligence%20through%20a%20single%20network%0Aarchitecture%20or%20massive%20datasets%20alone%20may%20be%20inherently%20limited.%20Our%20work%0Aprovides%20new%20insights%20into%20the%20theoretical%20foundations%20of%20neural%20network%0Avulnerability%20and%20opens%20up%20avenues%20for%20designing%20more%20robust%20neural%20network%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.01493v4&entry.124074799=Read"},
{"title": "Empowering Large Language Models in Wireless Communication: A Novel\n  Dataset and Fine-Tuning Framework", "author": "Yushen Lin and Ruichen Zhang and Wenqi Huang and Kaidi Wang and Zhiguo Ding and Daniel K. C. So and Dusit Niyato", "abstract": "  In this work, we develop a specialized dataset aimed at enhancing the\nevaluation and fine-tuning of large language models (LLMs) specifically for\nwireless communication applications. The dataset includes a diverse set of\nmulti-hop questions, including true/false and multiple-choice types, spanning\nvarying difficulty levels from easy to hard. By utilizing advanced language\nmodels for entity extraction and question generation, rigorous data curation\nprocesses are employed to maintain high quality and relevance. Additionally, we\nintroduce a Pointwise V-Information (PVI) based fine-tuning method, providing a\ndetailed theoretical analysis and justification for its use in quantifying the\ninformation content of training data with 2.24\\% and 1.31\\% performance boost\nfor different models compared to baselines, respectively. To demonstrate the\neffectiveness of the fine-tuned models with the proposed methodologies on\npractical tasks, we also consider different tasks, including summarizing\noptimization problems from technical papers and solving the mathematical\nproblems related to non-orthogonal multiple access (NOMA), which are generated\nby using the proposed multi-agent framework. Simulation results show\nsignificant performance gain in summarization tasks with 20.9\\% in the ROUGE-L\nmetrics. We also study the scaling laws of fine-tuning LLMs and the challenges\nLLMs face in the field of wireless communications, offering insights into their\nadaptation to wireless communication tasks. This dataset and fine-tuning\nmethodology aim to enhance the training and evaluation of LLMs, contributing to\nadvancements in LLMs for wireless communication research and applications.\n", "link": "http://arxiv.org/abs/2501.09631v1", "date": "2025-01-16", "relevancy": 2.0054, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Large%20Language%20Models%20in%20Wireless%20Communication%3A%20A%20Novel%0A%20%20Dataset%20and%20Fine-Tuning%20Framework&body=Title%3A%20Empowering%20Large%20Language%20Models%20in%20Wireless%20Communication%3A%20A%20Novel%0A%20%20Dataset%20and%20Fine-Tuning%20Framework%0AAuthor%3A%20Yushen%20Lin%20and%20Ruichen%20Zhang%20and%20Wenqi%20Huang%20and%20Kaidi%20Wang%20and%20Zhiguo%20Ding%20and%20Daniel%20K.%20C.%20So%20and%20Dusit%20Niyato%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20develop%20a%20specialized%20dataset%20aimed%20at%20enhancing%20the%0Aevaluation%20and%20fine-tuning%20of%20large%20language%20models%20%28LLMs%29%20specifically%20for%0Awireless%20communication%20applications.%20The%20dataset%20includes%20a%20diverse%20set%20of%0Amulti-hop%20questions%2C%20including%20true/false%20and%20multiple-choice%20types%2C%20spanning%0Avarying%20difficulty%20levels%20from%20easy%20to%20hard.%20By%20utilizing%20advanced%20language%0Amodels%20for%20entity%20extraction%20and%20question%20generation%2C%20rigorous%20data%20curation%0Aprocesses%20are%20employed%20to%20maintain%20high%20quality%20and%20relevance.%20Additionally%2C%20we%0Aintroduce%20a%20Pointwise%20V-Information%20%28PVI%29%20based%20fine-tuning%20method%2C%20providing%20a%0Adetailed%20theoretical%20analysis%20and%20justification%20for%20its%20use%20in%20quantifying%20the%0Ainformation%20content%20of%20training%20data%20with%202.24%5C%25%20and%201.31%5C%25%20performance%20boost%0Afor%20different%20models%20compared%20to%20baselines%2C%20respectively.%20To%20demonstrate%20the%0Aeffectiveness%20of%20the%20fine-tuned%20models%20with%20the%20proposed%20methodologies%20on%0Apractical%20tasks%2C%20we%20also%20consider%20different%20tasks%2C%20including%20summarizing%0Aoptimization%20problems%20from%20technical%20papers%20and%20solving%20the%20mathematical%0Aproblems%20related%20to%20non-orthogonal%20multiple%20access%20%28NOMA%29%2C%20which%20are%20generated%0Aby%20using%20the%20proposed%20multi-agent%20framework.%20Simulation%20results%20show%0Asignificant%20performance%20gain%20in%20summarization%20tasks%20with%2020.9%5C%25%20in%20the%20ROUGE-L%0Ametrics.%20We%20also%20study%20the%20scaling%20laws%20of%20fine-tuning%20LLMs%20and%20the%20challenges%0ALLMs%20face%20in%20the%20field%20of%20wireless%20communications%2C%20offering%20insights%20into%20their%0Aadaptation%20to%20wireless%20communication%20tasks.%20This%20dataset%20and%20fine-tuning%0Amethodology%20aim%20to%20enhance%20the%20training%20and%20evaluation%20of%20LLMs%2C%20contributing%20to%0Aadvancements%20in%20LLMs%20for%20wireless%20communication%20research%20and%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Large%2520Language%2520Models%2520in%2520Wireless%2520Communication%253A%2520A%2520Novel%250A%2520%2520Dataset%2520and%2520Fine-Tuning%2520Framework%26entry.906535625%3DYushen%2520Lin%2520and%2520Ruichen%2520Zhang%2520and%2520Wenqi%2520Huang%2520and%2520Kaidi%2520Wang%2520and%2520Zhiguo%2520Ding%2520and%2520Daniel%2520K.%2520C.%2520So%2520and%2520Dusit%2520Niyato%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520develop%2520a%2520specialized%2520dataset%2520aimed%2520at%2520enhancing%2520the%250Aevaluation%2520and%2520fine-tuning%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520specifically%2520for%250Awireless%2520communication%2520applications.%2520The%2520dataset%2520includes%2520a%2520diverse%2520set%2520of%250Amulti-hop%2520questions%252C%2520including%2520true/false%2520and%2520multiple-choice%2520types%252C%2520spanning%250Avarying%2520difficulty%2520levels%2520from%2520easy%2520to%2520hard.%2520By%2520utilizing%2520advanced%2520language%250Amodels%2520for%2520entity%2520extraction%2520and%2520question%2520generation%252C%2520rigorous%2520data%2520curation%250Aprocesses%2520are%2520employed%2520to%2520maintain%2520high%2520quality%2520and%2520relevance.%2520Additionally%252C%2520we%250Aintroduce%2520a%2520Pointwise%2520V-Information%2520%2528PVI%2529%2520based%2520fine-tuning%2520method%252C%2520providing%2520a%250Adetailed%2520theoretical%2520analysis%2520and%2520justification%2520for%2520its%2520use%2520in%2520quantifying%2520the%250Ainformation%2520content%2520of%2520training%2520data%2520with%25202.24%255C%2525%2520and%25201.31%255C%2525%2520performance%2520boost%250Afor%2520different%2520models%2520compared%2520to%2520baselines%252C%2520respectively.%2520To%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520fine-tuned%2520models%2520with%2520the%2520proposed%2520methodologies%2520on%250Apractical%2520tasks%252C%2520we%2520also%2520consider%2520different%2520tasks%252C%2520including%2520summarizing%250Aoptimization%2520problems%2520from%2520technical%2520papers%2520and%2520solving%2520the%2520mathematical%250Aproblems%2520related%2520to%2520non-orthogonal%2520multiple%2520access%2520%2528NOMA%2529%252C%2520which%2520are%2520generated%250Aby%2520using%2520the%2520proposed%2520multi-agent%2520framework.%2520Simulation%2520results%2520show%250Asignificant%2520performance%2520gain%2520in%2520summarization%2520tasks%2520with%252020.9%255C%2525%2520in%2520the%2520ROUGE-L%250Ametrics.%2520We%2520also%2520study%2520the%2520scaling%2520laws%2520of%2520fine-tuning%2520LLMs%2520and%2520the%2520challenges%250ALLMs%2520face%2520in%2520the%2520field%2520of%2520wireless%2520communications%252C%2520offering%2520insights%2520into%2520their%250Aadaptation%2520to%2520wireless%2520communication%2520tasks.%2520This%2520dataset%2520and%2520fine-tuning%250Amethodology%2520aim%2520to%2520enhance%2520the%2520training%2520and%2520evaluation%2520of%2520LLMs%252C%2520contributing%2520to%250Aadvancements%2520in%2520LLMs%2520for%2520wireless%2520communication%2520research%2520and%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Large%20Language%20Models%20in%20Wireless%20Communication%3A%20A%20Novel%0A%20%20Dataset%20and%20Fine-Tuning%20Framework&entry.906535625=Yushen%20Lin%20and%20Ruichen%20Zhang%20and%20Wenqi%20Huang%20and%20Kaidi%20Wang%20and%20Zhiguo%20Ding%20and%20Daniel%20K.%20C.%20So%20and%20Dusit%20Niyato&entry.1292438233=%20%20In%20this%20work%2C%20we%20develop%20a%20specialized%20dataset%20aimed%20at%20enhancing%20the%0Aevaluation%20and%20fine-tuning%20of%20large%20language%20models%20%28LLMs%29%20specifically%20for%0Awireless%20communication%20applications.%20The%20dataset%20includes%20a%20diverse%20set%20of%0Amulti-hop%20questions%2C%20including%20true/false%20and%20multiple-choice%20types%2C%20spanning%0Avarying%20difficulty%20levels%20from%20easy%20to%20hard.%20By%20utilizing%20advanced%20language%0Amodels%20for%20entity%20extraction%20and%20question%20generation%2C%20rigorous%20data%20curation%0Aprocesses%20are%20employed%20to%20maintain%20high%20quality%20and%20relevance.%20Additionally%2C%20we%0Aintroduce%20a%20Pointwise%20V-Information%20%28PVI%29%20based%20fine-tuning%20method%2C%20providing%20a%0Adetailed%20theoretical%20analysis%20and%20justification%20for%20its%20use%20in%20quantifying%20the%0Ainformation%20content%20of%20training%20data%20with%202.24%5C%25%20and%201.31%5C%25%20performance%20boost%0Afor%20different%20models%20compared%20to%20baselines%2C%20respectively.%20To%20demonstrate%20the%0Aeffectiveness%20of%20the%20fine-tuned%20models%20with%20the%20proposed%20methodologies%20on%0Apractical%20tasks%2C%20we%20also%20consider%20different%20tasks%2C%20including%20summarizing%0Aoptimization%20problems%20from%20technical%20papers%20and%20solving%20the%20mathematical%0Aproblems%20related%20to%20non-orthogonal%20multiple%20access%20%28NOMA%29%2C%20which%20are%20generated%0Aby%20using%20the%20proposed%20multi-agent%20framework.%20Simulation%20results%20show%0Asignificant%20performance%20gain%20in%20summarization%20tasks%20with%2020.9%5C%25%20in%20the%20ROUGE-L%0Ametrics.%20We%20also%20study%20the%20scaling%20laws%20of%20fine-tuning%20LLMs%20and%20the%20challenges%0ALLMs%20face%20in%20the%20field%20of%20wireless%20communications%2C%20offering%20insights%20into%20their%0Aadaptation%20to%20wireless%20communication%20tasks.%20This%20dataset%20and%20fine-tuning%0Amethodology%20aim%20to%20enhance%20the%20training%20and%20evaluation%20of%20LLMs%2C%20contributing%20to%0Aadvancements%20in%20LLMs%20for%20wireless%20communication%20research%20and%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09631v1&entry.124074799=Read"},
{"title": "Solving the unsolvable: Translating case law in Hong Kong", "author": "King-kui Sin and Xi Xuan and Chunyu Kit and Clara Ho-yan Chan and Honic Ho-kin Ip", "abstract": "  This paper addresses the challenges translating case law under Hong Kong's\nbilingual legal system. It highlights the initial success of translating all\nwritten statutes into Chinese before the 1997 handover, a task mandated by the\nBasic Law. The effort involved significant collaboration among legal,\nlinguistic, and translation experts, resulting in a comprehensive and\nculturally appropriate bilingual legal system. However, translating case law\nremains a significant challenge due to the sheer volume and continuous growth\nof judicial decisions. The paper critiques the governments and judiciarys\nsporadic and uncoordinated efforts to translate case law, contrasting it with\nthe thorough approach previously taken for statute translation. Although the\ngovernment acknowledges the importance of legal bilingualism, it lacks a\nsustainable strategy for translating case law. The Judiciarys position that\ntranslating all judgments is unnecessary, unrealistic, and not cost-effectiveis\nanalyzed and critiqued for its impact on legal transparency and public trust. A\nproposed solution involves leveraging machine translation technology through a\nhuman-machine interactive translation platform, which undergoes two major\ntransitions. Initially based on a neural model, the platform transitions to\nusing a large language model for improved translation accuracy. Furthermore, it\nevolves from a single-agent system to a multi-agent system, incorporating\nTranslator, Annotator, and Proofreader agents. This multi-agent approach,\nsupported by a grant, aims to facilitate efficient, high-quality translation of\njudicial judgments by integrating advanced artificial intelligence and\ncontinuous feedback mechanisms, thus better meeting the needs of a bilingual\nlegal system.\n", "link": "http://arxiv.org/abs/2501.09444v1", "date": "2025-01-16", "relevancy": 1.9972, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4026}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20the%20unsolvable%3A%20Translating%20case%20law%20in%20Hong%20Kong&body=Title%3A%20Solving%20the%20unsolvable%3A%20Translating%20case%20law%20in%20Hong%20Kong%0AAuthor%3A%20King-kui%20Sin%20and%20Xi%20Xuan%20and%20Chunyu%20Kit%20and%20Clara%20Ho-yan%20Chan%20and%20Honic%20Ho-kin%20Ip%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenges%20translating%20case%20law%20under%20Hong%20Kong%27s%0Abilingual%20legal%20system.%20It%20highlights%20the%20initial%20success%20of%20translating%20all%0Awritten%20statutes%20into%20Chinese%20before%20the%201997%20handover%2C%20a%20task%20mandated%20by%20the%0ABasic%20Law.%20The%20effort%20involved%20significant%20collaboration%20among%20legal%2C%0Alinguistic%2C%20and%20translation%20experts%2C%20resulting%20in%20a%20comprehensive%20and%0Aculturally%20appropriate%20bilingual%20legal%20system.%20However%2C%20translating%20case%20law%0Aremains%20a%20significant%20challenge%20due%20to%20the%20sheer%20volume%20and%20continuous%20growth%0Aof%20judicial%20decisions.%20The%20paper%20critiques%20the%20governments%20and%20judiciarys%0Asporadic%20and%20uncoordinated%20efforts%20to%20translate%20case%20law%2C%20contrasting%20it%20with%0Athe%20thorough%20approach%20previously%20taken%20for%20statute%20translation.%20Although%20the%0Agovernment%20acknowledges%20the%20importance%20of%20legal%20bilingualism%2C%20it%20lacks%20a%0Asustainable%20strategy%20for%20translating%20case%20law.%20The%20Judiciarys%20position%20that%0Atranslating%20all%20judgments%20is%20unnecessary%2C%20unrealistic%2C%20and%20not%20cost-effectiveis%0Aanalyzed%20and%20critiqued%20for%20its%20impact%20on%20legal%20transparency%20and%20public%20trust.%20A%0Aproposed%20solution%20involves%20leveraging%20machine%20translation%20technology%20through%20a%0Ahuman-machine%20interactive%20translation%20platform%2C%20which%20undergoes%20two%20major%0Atransitions.%20Initially%20based%20on%20a%20neural%20model%2C%20the%20platform%20transitions%20to%0Ausing%20a%20large%20language%20model%20for%20improved%20translation%20accuracy.%20Furthermore%2C%20it%0Aevolves%20from%20a%20single-agent%20system%20to%20a%20multi-agent%20system%2C%20incorporating%0ATranslator%2C%20Annotator%2C%20and%20Proofreader%20agents.%20This%20multi-agent%20approach%2C%0Asupported%20by%20a%20grant%2C%20aims%20to%20facilitate%20efficient%2C%20high-quality%20translation%20of%0Ajudicial%20judgments%20by%20integrating%20advanced%20artificial%20intelligence%20and%0Acontinuous%20feedback%20mechanisms%2C%20thus%20better%20meeting%20the%20needs%20of%20a%20bilingual%0Alegal%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520the%2520unsolvable%253A%2520Translating%2520case%2520law%2520in%2520Hong%2520Kong%26entry.906535625%3DKing-kui%2520Sin%2520and%2520Xi%2520Xuan%2520and%2520Chunyu%2520Kit%2520and%2520Clara%2520Ho-yan%2520Chan%2520and%2520Honic%2520Ho-kin%2520Ip%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenges%2520translating%2520case%2520law%2520under%2520Hong%2520Kong%2527s%250Abilingual%2520legal%2520system.%2520It%2520highlights%2520the%2520initial%2520success%2520of%2520translating%2520all%250Awritten%2520statutes%2520into%2520Chinese%2520before%2520the%25201997%2520handover%252C%2520a%2520task%2520mandated%2520by%2520the%250ABasic%2520Law.%2520The%2520effort%2520involved%2520significant%2520collaboration%2520among%2520legal%252C%250Alinguistic%252C%2520and%2520translation%2520experts%252C%2520resulting%2520in%2520a%2520comprehensive%2520and%250Aculturally%2520appropriate%2520bilingual%2520legal%2520system.%2520However%252C%2520translating%2520case%2520law%250Aremains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520sheer%2520volume%2520and%2520continuous%2520growth%250Aof%2520judicial%2520decisions.%2520The%2520paper%2520critiques%2520the%2520governments%2520and%2520judiciarys%250Asporadic%2520and%2520uncoordinated%2520efforts%2520to%2520translate%2520case%2520law%252C%2520contrasting%2520it%2520with%250Athe%2520thorough%2520approach%2520previously%2520taken%2520for%2520statute%2520translation.%2520Although%2520the%250Agovernment%2520acknowledges%2520the%2520importance%2520of%2520legal%2520bilingualism%252C%2520it%2520lacks%2520a%250Asustainable%2520strategy%2520for%2520translating%2520case%2520law.%2520The%2520Judiciarys%2520position%2520that%250Atranslating%2520all%2520judgments%2520is%2520unnecessary%252C%2520unrealistic%252C%2520and%2520not%2520cost-effectiveis%250Aanalyzed%2520and%2520critiqued%2520for%2520its%2520impact%2520on%2520legal%2520transparency%2520and%2520public%2520trust.%2520A%250Aproposed%2520solution%2520involves%2520leveraging%2520machine%2520translation%2520technology%2520through%2520a%250Ahuman-machine%2520interactive%2520translation%2520platform%252C%2520which%2520undergoes%2520two%2520major%250Atransitions.%2520Initially%2520based%2520on%2520a%2520neural%2520model%252C%2520the%2520platform%2520transitions%2520to%250Ausing%2520a%2520large%2520language%2520model%2520for%2520improved%2520translation%2520accuracy.%2520Furthermore%252C%2520it%250Aevolves%2520from%2520a%2520single-agent%2520system%2520to%2520a%2520multi-agent%2520system%252C%2520incorporating%250ATranslator%252C%2520Annotator%252C%2520and%2520Proofreader%2520agents.%2520This%2520multi-agent%2520approach%252C%250Asupported%2520by%2520a%2520grant%252C%2520aims%2520to%2520facilitate%2520efficient%252C%2520high-quality%2520translation%2520of%250Ajudicial%2520judgments%2520by%2520integrating%2520advanced%2520artificial%2520intelligence%2520and%250Acontinuous%2520feedback%2520mechanisms%252C%2520thus%2520better%2520meeting%2520the%2520needs%2520of%2520a%2520bilingual%250Alegal%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20the%20unsolvable%3A%20Translating%20case%20law%20in%20Hong%20Kong&entry.906535625=King-kui%20Sin%20and%20Xi%20Xuan%20and%20Chunyu%20Kit%20and%20Clara%20Ho-yan%20Chan%20and%20Honic%20Ho-kin%20Ip&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenges%20translating%20case%20law%20under%20Hong%20Kong%27s%0Abilingual%20legal%20system.%20It%20highlights%20the%20initial%20success%20of%20translating%20all%0Awritten%20statutes%20into%20Chinese%20before%20the%201997%20handover%2C%20a%20task%20mandated%20by%20the%0ABasic%20Law.%20The%20effort%20involved%20significant%20collaboration%20among%20legal%2C%0Alinguistic%2C%20and%20translation%20experts%2C%20resulting%20in%20a%20comprehensive%20and%0Aculturally%20appropriate%20bilingual%20legal%20system.%20However%2C%20translating%20case%20law%0Aremains%20a%20significant%20challenge%20due%20to%20the%20sheer%20volume%20and%20continuous%20growth%0Aof%20judicial%20decisions.%20The%20paper%20critiques%20the%20governments%20and%20judiciarys%0Asporadic%20and%20uncoordinated%20efforts%20to%20translate%20case%20law%2C%20contrasting%20it%20with%0Athe%20thorough%20approach%20previously%20taken%20for%20statute%20translation.%20Although%20the%0Agovernment%20acknowledges%20the%20importance%20of%20legal%20bilingualism%2C%20it%20lacks%20a%0Asustainable%20strategy%20for%20translating%20case%20law.%20The%20Judiciarys%20position%20that%0Atranslating%20all%20judgments%20is%20unnecessary%2C%20unrealistic%2C%20and%20not%20cost-effectiveis%0Aanalyzed%20and%20critiqued%20for%20its%20impact%20on%20legal%20transparency%20and%20public%20trust.%20A%0Aproposed%20solution%20involves%20leveraging%20machine%20translation%20technology%20through%20a%0Ahuman-machine%20interactive%20translation%20platform%2C%20which%20undergoes%20two%20major%0Atransitions.%20Initially%20based%20on%20a%20neural%20model%2C%20the%20platform%20transitions%20to%0Ausing%20a%20large%20language%20model%20for%20improved%20translation%20accuracy.%20Furthermore%2C%20it%0Aevolves%20from%20a%20single-agent%20system%20to%20a%20multi-agent%20system%2C%20incorporating%0ATranslator%2C%20Annotator%2C%20and%20Proofreader%20agents.%20This%20multi-agent%20approach%2C%0Asupported%20by%20a%20grant%2C%20aims%20to%20facilitate%20efficient%2C%20high-quality%20translation%20of%0Ajudicial%20judgments%20by%20integrating%20advanced%20artificial%20intelligence%20and%0Acontinuous%20feedback%20mechanisms%2C%20thus%20better%20meeting%20the%20needs%20of%20a%20bilingual%0Alegal%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09444v1&entry.124074799=Read"},
{"title": "Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian\n  Neural Networks", "author": "Bao Gia Doan and Afshar Shamsi and Xiao-Yu Guo and Arash Mohammadi and Hamid Alinejad-Rokny and Dino Sejdinovic and Damien Teney and Damith C. Ranasinghe and Ehsan Abbasnejad", "abstract": "  Computational complexity of Bayesian learning is impeding its adoption in\npractical, large-scale tasks. Despite demonstrations of significant merits such\nas improved robustness and resilience to unseen or out-of-distribution inputs\nover their non- Bayesian counterparts, their practical use has faded to near\ninsignificance. In this study, we introduce an innovative framework to mitigate\nthe computational burden of Bayesian neural networks (BNNs). Our approach\nfollows the principle of Bayesian techniques based on deep ensembles, but\nsignificantly reduces their cost via multiple low-rank perturbations of\nparameters arising from a pre-trained neural network. Both vanilla version of\nensembles as well as more sophisticated schemes such as Bayesian learning with\nStein Variational Gradient Descent (SVGD), previously deemed impractical for\nlarge models, can be seamlessly implemented within the proposed framework,\ncalled Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a\ndramatic reduction in the number of trainable parameters required to\napproximate a Bayesian posterior; and ii) it not only maintains, but in some\ninstances, surpasses the performance of conventional Bayesian learning methods\nand non-Bayesian baselines. Our results with large-scale tasks such as\nImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the\neffectiveness and versatility of Bella in building highly scalable and\npractical Bayesian deep models for real-world applications.\n", "link": "http://arxiv.org/abs/2407.20891v4", "date": "2025-01-16", "relevancy": 1.9925, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5311}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4963}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Low-Rank%20LeArning%20%28Bella%29%3A%20A%20Practical%20Approach%20to%20Bayesian%0A%20%20Neural%20Networks&body=Title%3A%20Bayesian%20Low-Rank%20LeArning%20%28Bella%29%3A%20A%20Practical%20Approach%20to%20Bayesian%0A%20%20Neural%20Networks%0AAuthor%3A%20Bao%20Gia%20Doan%20and%20Afshar%20Shamsi%20and%20Xiao-Yu%20Guo%20and%20Arash%20Mohammadi%20and%20Hamid%20Alinejad-Rokny%20and%20Dino%20Sejdinovic%20and%20Damien%20Teney%20and%20Damith%20C.%20Ranasinghe%20and%20Ehsan%20Abbasnejad%0AAbstract%3A%20%20%20Computational%20complexity%20of%20Bayesian%20learning%20is%20impeding%20its%20adoption%20in%0Apractical%2C%20large-scale%20tasks.%20Despite%20demonstrations%20of%20significant%20merits%20such%0Aas%20improved%20robustness%20and%20resilience%20to%20unseen%20or%20out-of-distribution%20inputs%0Aover%20their%20non-%20Bayesian%20counterparts%2C%20their%20practical%20use%20has%20faded%20to%20near%0Ainsignificance.%20In%20this%20study%2C%20we%20introduce%20an%20innovative%20framework%20to%20mitigate%0Athe%20computational%20burden%20of%20Bayesian%20neural%20networks%20%28BNNs%29.%20Our%20approach%0Afollows%20the%20principle%20of%20Bayesian%20techniques%20based%20on%20deep%20ensembles%2C%20but%0Asignificantly%20reduces%20their%20cost%20via%20multiple%20low-rank%20perturbations%20of%0Aparameters%20arising%20from%20a%20pre-trained%20neural%20network.%20Both%20vanilla%20version%20of%0Aensembles%20as%20well%20as%20more%20sophisticated%20schemes%20such%20as%20Bayesian%20learning%20with%0AStein%20Variational%20Gradient%20Descent%20%28SVGD%29%2C%20previously%20deemed%20impractical%20for%0Alarge%20models%2C%20can%20be%20seamlessly%20implemented%20within%20the%20proposed%20framework%2C%0Acalled%20Bayesian%20Low-Rank%20LeArning%20%28Bella%29.%20In%20a%20nutshell%2C%20i%29%20Bella%20achieves%20a%0Adramatic%20reduction%20in%20the%20number%20of%20trainable%20parameters%20required%20to%0Aapproximate%20a%20Bayesian%20posterior%3B%20and%20ii%29%20it%20not%20only%20maintains%2C%20but%20in%20some%0Ainstances%2C%20surpasses%20the%20performance%20of%20conventional%20Bayesian%20learning%20methods%0Aand%20non-Bayesian%20baselines.%20Our%20results%20with%20large-scale%20tasks%20such%20as%0AImageNet%2C%20CAMELYON17%2C%20DomainNet%2C%20VQA%20with%20CLIP%2C%20LLaVA%20demonstrate%20the%0Aeffectiveness%20and%20versatility%20of%20Bella%20in%20building%20highly%20scalable%20and%0Apractical%20Bayesian%20deep%20models%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20891v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Low-Rank%2520LeArning%2520%2528Bella%2529%253A%2520A%2520Practical%2520Approach%2520to%2520Bayesian%250A%2520%2520Neural%2520Networks%26entry.906535625%3DBao%2520Gia%2520Doan%2520and%2520Afshar%2520Shamsi%2520and%2520Xiao-Yu%2520Guo%2520and%2520Arash%2520Mohammadi%2520and%2520Hamid%2520Alinejad-Rokny%2520and%2520Dino%2520Sejdinovic%2520and%2520Damien%2520Teney%2520and%2520Damith%2520C.%2520Ranasinghe%2520and%2520Ehsan%2520Abbasnejad%26entry.1292438233%3D%2520%2520Computational%2520complexity%2520of%2520Bayesian%2520learning%2520is%2520impeding%2520its%2520adoption%2520in%250Apractical%252C%2520large-scale%2520tasks.%2520Despite%2520demonstrations%2520of%2520significant%2520merits%2520such%250Aas%2520improved%2520robustness%2520and%2520resilience%2520to%2520unseen%2520or%2520out-of-distribution%2520inputs%250Aover%2520their%2520non-%2520Bayesian%2520counterparts%252C%2520their%2520practical%2520use%2520has%2520faded%2520to%2520near%250Ainsignificance.%2520In%2520this%2520study%252C%2520we%2520introduce%2520an%2520innovative%2520framework%2520to%2520mitigate%250Athe%2520computational%2520burden%2520of%2520Bayesian%2520neural%2520networks%2520%2528BNNs%2529.%2520Our%2520approach%250Afollows%2520the%2520principle%2520of%2520Bayesian%2520techniques%2520based%2520on%2520deep%2520ensembles%252C%2520but%250Asignificantly%2520reduces%2520their%2520cost%2520via%2520multiple%2520low-rank%2520perturbations%2520of%250Aparameters%2520arising%2520from%2520a%2520pre-trained%2520neural%2520network.%2520Both%2520vanilla%2520version%2520of%250Aensembles%2520as%2520well%2520as%2520more%2520sophisticated%2520schemes%2520such%2520as%2520Bayesian%2520learning%2520with%250AStein%2520Variational%2520Gradient%2520Descent%2520%2528SVGD%2529%252C%2520previously%2520deemed%2520impractical%2520for%250Alarge%2520models%252C%2520can%2520be%2520seamlessly%2520implemented%2520within%2520the%2520proposed%2520framework%252C%250Acalled%2520Bayesian%2520Low-Rank%2520LeArning%2520%2528Bella%2529.%2520In%2520a%2520nutshell%252C%2520i%2529%2520Bella%2520achieves%2520a%250Adramatic%2520reduction%2520in%2520the%2520number%2520of%2520trainable%2520parameters%2520required%2520to%250Aapproximate%2520a%2520Bayesian%2520posterior%253B%2520and%2520ii%2529%2520it%2520not%2520only%2520maintains%252C%2520but%2520in%2520some%250Ainstances%252C%2520surpasses%2520the%2520performance%2520of%2520conventional%2520Bayesian%2520learning%2520methods%250Aand%2520non-Bayesian%2520baselines.%2520Our%2520results%2520with%2520large-scale%2520tasks%2520such%2520as%250AImageNet%252C%2520CAMELYON17%252C%2520DomainNet%252C%2520VQA%2520with%2520CLIP%252C%2520LLaVA%2520demonstrate%2520the%250Aeffectiveness%2520and%2520versatility%2520of%2520Bella%2520in%2520building%2520highly%2520scalable%2520and%250Apractical%2520Bayesian%2520deep%2520models%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20891v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Low-Rank%20LeArning%20%28Bella%29%3A%20A%20Practical%20Approach%20to%20Bayesian%0A%20%20Neural%20Networks&entry.906535625=Bao%20Gia%20Doan%20and%20Afshar%20Shamsi%20and%20Xiao-Yu%20Guo%20and%20Arash%20Mohammadi%20and%20Hamid%20Alinejad-Rokny%20and%20Dino%20Sejdinovic%20and%20Damien%20Teney%20and%20Damith%20C.%20Ranasinghe%20and%20Ehsan%20Abbasnejad&entry.1292438233=%20%20Computational%20complexity%20of%20Bayesian%20learning%20is%20impeding%20its%20adoption%20in%0Apractical%2C%20large-scale%20tasks.%20Despite%20demonstrations%20of%20significant%20merits%20such%0Aas%20improved%20robustness%20and%20resilience%20to%20unseen%20or%20out-of-distribution%20inputs%0Aover%20their%20non-%20Bayesian%20counterparts%2C%20their%20practical%20use%20has%20faded%20to%20near%0Ainsignificance.%20In%20this%20study%2C%20we%20introduce%20an%20innovative%20framework%20to%20mitigate%0Athe%20computational%20burden%20of%20Bayesian%20neural%20networks%20%28BNNs%29.%20Our%20approach%0Afollows%20the%20principle%20of%20Bayesian%20techniques%20based%20on%20deep%20ensembles%2C%20but%0Asignificantly%20reduces%20their%20cost%20via%20multiple%20low-rank%20perturbations%20of%0Aparameters%20arising%20from%20a%20pre-trained%20neural%20network.%20Both%20vanilla%20version%20of%0Aensembles%20as%20well%20as%20more%20sophisticated%20schemes%20such%20as%20Bayesian%20learning%20with%0AStein%20Variational%20Gradient%20Descent%20%28SVGD%29%2C%20previously%20deemed%20impractical%20for%0Alarge%20models%2C%20can%20be%20seamlessly%20implemented%20within%20the%20proposed%20framework%2C%0Acalled%20Bayesian%20Low-Rank%20LeArning%20%28Bella%29.%20In%20a%20nutshell%2C%20i%29%20Bella%20achieves%20a%0Adramatic%20reduction%20in%20the%20number%20of%20trainable%20parameters%20required%20to%0Aapproximate%20a%20Bayesian%20posterior%3B%20and%20ii%29%20it%20not%20only%20maintains%2C%20but%20in%20some%0Ainstances%2C%20surpasses%20the%20performance%20of%20conventional%20Bayesian%20learning%20methods%0Aand%20non-Bayesian%20baselines.%20Our%20results%20with%20large-scale%20tasks%20such%20as%0AImageNet%2C%20CAMELYON17%2C%20DomainNet%2C%20VQA%20with%20CLIP%2C%20LLaVA%20demonstrate%20the%0Aeffectiveness%20and%20versatility%20of%20Bella%20in%20building%20highly%20scalable%20and%0Apractical%20Bayesian%20deep%20models%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20891v4&entry.124074799=Read"},
{"title": "A Survey of Research in Large Language Models for Electronic Design\n  Automation", "author": "Jingyu Pan and Guanglei Zhou and Chen-Chia Chang and Isaac Jacobson and Jiang Hu and Yiran Chen", "abstract": "  Within the rapidly evolving domain of Electronic Design Automation (EDA),\nLarge Language Models (LLMs) have emerged as transformative technologies,\noffering unprecedented capabilities for optimizing and automating various\naspects of electronic design. This survey provides a comprehensive exploration\nof LLM applications in EDA, focusing on advancements in model architectures,\nthe implications of varying model sizes, and innovative customization\ntechniques that enable tailored analytical insights. By examining the\nintersection of LLM capabilities and EDA requirements, the paper highlights the\nsignificant impact these models have on extracting nuanced understandings from\ncomplex datasets. Furthermore, it addresses the challenges and opportunities in\nintegrating LLMs into EDA workflows, paving the way for future research and\napplication in this dynamic field. Through this detailed analysis, the survey\naims to offer valuable insights to professionals in the EDA industry, AI\nresearchers, and anyone interested in the convergence of advanced AI\ntechnologies and electronic design.\n", "link": "http://arxiv.org/abs/2501.09655v1", "date": "2025-01-16", "relevancy": 1.9681, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Research%20in%20Large%20Language%20Models%20for%20Electronic%20Design%0A%20%20Automation&body=Title%3A%20A%20Survey%20of%20Research%20in%20Large%20Language%20Models%20for%20Electronic%20Design%0A%20%20Automation%0AAuthor%3A%20Jingyu%20Pan%20and%20Guanglei%20Zhou%20and%20Chen-Chia%20Chang%20and%20Isaac%20Jacobson%20and%20Jiang%20Hu%20and%20Yiran%20Chen%0AAbstract%3A%20%20%20Within%20the%20rapidly%20evolving%20domain%20of%20Electronic%20Design%20Automation%20%28EDA%29%2C%0ALarge%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20transformative%20technologies%2C%0Aoffering%20unprecedented%20capabilities%20for%20optimizing%20and%20automating%20various%0Aaspects%20of%20electronic%20design.%20This%20survey%20provides%20a%20comprehensive%20exploration%0Aof%20LLM%20applications%20in%20EDA%2C%20focusing%20on%20advancements%20in%20model%20architectures%2C%0Athe%20implications%20of%20varying%20model%20sizes%2C%20and%20innovative%20customization%0Atechniques%20that%20enable%20tailored%20analytical%20insights.%20By%20examining%20the%0Aintersection%20of%20LLM%20capabilities%20and%20EDA%20requirements%2C%20the%20paper%20highlights%20the%0Asignificant%20impact%20these%20models%20have%20on%20extracting%20nuanced%20understandings%20from%0Acomplex%20datasets.%20Furthermore%2C%20it%20addresses%20the%20challenges%20and%20opportunities%20in%0Aintegrating%20LLMs%20into%20EDA%20workflows%2C%20paving%20the%20way%20for%20future%20research%20and%0Aapplication%20in%20this%20dynamic%20field.%20Through%20this%20detailed%20analysis%2C%20the%20survey%0Aaims%20to%20offer%20valuable%20insights%20to%20professionals%20in%20the%20EDA%20industry%2C%20AI%0Aresearchers%2C%20and%20anyone%20interested%20in%20the%20convergence%20of%20advanced%20AI%0Atechnologies%20and%20electronic%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Research%2520in%2520Large%2520Language%2520Models%2520for%2520Electronic%2520Design%250A%2520%2520Automation%26entry.906535625%3DJingyu%2520Pan%2520and%2520Guanglei%2520Zhou%2520and%2520Chen-Chia%2520Chang%2520and%2520Isaac%2520Jacobson%2520and%2520Jiang%2520Hu%2520and%2520Yiran%2520Chen%26entry.1292438233%3D%2520%2520Within%2520the%2520rapidly%2520evolving%2520domain%2520of%2520Electronic%2520Design%2520Automation%2520%2528EDA%2529%252C%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520transformative%2520technologies%252C%250Aoffering%2520unprecedented%2520capabilities%2520for%2520optimizing%2520and%2520automating%2520various%250Aaspects%2520of%2520electronic%2520design.%2520This%2520survey%2520provides%2520a%2520comprehensive%2520exploration%250Aof%2520LLM%2520applications%2520in%2520EDA%252C%2520focusing%2520on%2520advancements%2520in%2520model%2520architectures%252C%250Athe%2520implications%2520of%2520varying%2520model%2520sizes%252C%2520and%2520innovative%2520customization%250Atechniques%2520that%2520enable%2520tailored%2520analytical%2520insights.%2520By%2520examining%2520the%250Aintersection%2520of%2520LLM%2520capabilities%2520and%2520EDA%2520requirements%252C%2520the%2520paper%2520highlights%2520the%250Asignificant%2520impact%2520these%2520models%2520have%2520on%2520extracting%2520nuanced%2520understandings%2520from%250Acomplex%2520datasets.%2520Furthermore%252C%2520it%2520addresses%2520the%2520challenges%2520and%2520opportunities%2520in%250Aintegrating%2520LLMs%2520into%2520EDA%2520workflows%252C%2520paving%2520the%2520way%2520for%2520future%2520research%2520and%250Aapplication%2520in%2520this%2520dynamic%2520field.%2520Through%2520this%2520detailed%2520analysis%252C%2520the%2520survey%250Aaims%2520to%2520offer%2520valuable%2520insights%2520to%2520professionals%2520in%2520the%2520EDA%2520industry%252C%2520AI%250Aresearchers%252C%2520and%2520anyone%2520interested%2520in%2520the%2520convergence%2520of%2520advanced%2520AI%250Atechnologies%2520and%2520electronic%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Research%20in%20Large%20Language%20Models%20for%20Electronic%20Design%0A%20%20Automation&entry.906535625=Jingyu%20Pan%20and%20Guanglei%20Zhou%20and%20Chen-Chia%20Chang%20and%20Isaac%20Jacobson%20and%20Jiang%20Hu%20and%20Yiran%20Chen&entry.1292438233=%20%20Within%20the%20rapidly%20evolving%20domain%20of%20Electronic%20Design%20Automation%20%28EDA%29%2C%0ALarge%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20transformative%20technologies%2C%0Aoffering%20unprecedented%20capabilities%20for%20optimizing%20and%20automating%20various%0Aaspects%20of%20electronic%20design.%20This%20survey%20provides%20a%20comprehensive%20exploration%0Aof%20LLM%20applications%20in%20EDA%2C%20focusing%20on%20advancements%20in%20model%20architectures%2C%0Athe%20implications%20of%20varying%20model%20sizes%2C%20and%20innovative%20customization%0Atechniques%20that%20enable%20tailored%20analytical%20insights.%20By%20examining%20the%0Aintersection%20of%20LLM%20capabilities%20and%20EDA%20requirements%2C%20the%20paper%20highlights%20the%0Asignificant%20impact%20these%20models%20have%20on%20extracting%20nuanced%20understandings%20from%0Acomplex%20datasets.%20Furthermore%2C%20it%20addresses%20the%20challenges%20and%20opportunities%20in%0Aintegrating%20LLMs%20into%20EDA%20workflows%2C%20paving%20the%20way%20for%20future%20research%20and%0Aapplication%20in%20this%20dynamic%20field.%20Through%20this%20detailed%20analysis%2C%20the%20survey%0Aaims%20to%20offer%20valuable%20insights%20to%20professionals%20in%20the%20EDA%20industry%2C%20AI%0Aresearchers%2C%20and%20anyone%20interested%20in%20the%20convergence%20of%20advanced%20AI%0Atechnologies%20and%20electronic%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09655v1&entry.124074799=Read"},
{"title": "Exploring AI-based System Design for Pixel-level Protected Health\n  Information Detection in Medical Images", "author": "Tuan Truong and Ivo M. Baltruschat and Mark Klemens and Grit Werner and Matthias Lenga", "abstract": "  De-identification of medical images is a critical step to ensure privacy\nduring data sharing in research and clinical settings. The initial step in this\nprocess involves detecting Protected Health Information (PHI), which can be\nfound in image metadata or imprinted within image pixels. Despite the\nimportance of such systems, there has been limited evaluation of existing\nAI-based solutions, creating barriers to the development of reliable and robust\ntools. In this study, we present an AI-based pipeline for PHI detection,\ncomprising three key components: text detection, text extraction, and analysis\nof PHI content in medical images. By experimenting with exchanging roles of\nvision and language models within the pipeline, we evaluate the performance and\nrecommend the best setup for the PHI detection task.\n", "link": "http://arxiv.org/abs/2501.09552v1", "date": "2025-01-16", "relevancy": 1.9619, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5028}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5013}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20AI-based%20System%20Design%20for%20Pixel-level%20Protected%20Health%0A%20%20Information%20Detection%20in%20Medical%20Images&body=Title%3A%20Exploring%20AI-based%20System%20Design%20for%20Pixel-level%20Protected%20Health%0A%20%20Information%20Detection%20in%20Medical%20Images%0AAuthor%3A%20Tuan%20Truong%20and%20Ivo%20M.%20Baltruschat%20and%20Mark%20Klemens%20and%20Grit%20Werner%20and%20Matthias%20Lenga%0AAbstract%3A%20%20%20De-identification%20of%20medical%20images%20is%20a%20critical%20step%20to%20ensure%20privacy%0Aduring%20data%20sharing%20in%20research%20and%20clinical%20settings.%20The%20initial%20step%20in%20this%0Aprocess%20involves%20detecting%20Protected%20Health%20Information%20%28PHI%29%2C%20which%20can%20be%0Afound%20in%20image%20metadata%20or%20imprinted%20within%20image%20pixels.%20Despite%20the%0Aimportance%20of%20such%20systems%2C%20there%20has%20been%20limited%20evaluation%20of%20existing%0AAI-based%20solutions%2C%20creating%20barriers%20to%20the%20development%20of%20reliable%20and%20robust%0Atools.%20In%20this%20study%2C%20we%20present%20an%20AI-based%20pipeline%20for%20PHI%20detection%2C%0Acomprising%20three%20key%20components%3A%20text%20detection%2C%20text%20extraction%2C%20and%20analysis%0Aof%20PHI%20content%20in%20medical%20images.%20By%20experimenting%20with%20exchanging%20roles%20of%0Avision%20and%20language%20models%20within%20the%20pipeline%2C%20we%20evaluate%20the%20performance%20and%0Arecommend%20the%20best%20setup%20for%20the%20PHI%20detection%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520AI-based%2520System%2520Design%2520for%2520Pixel-level%2520Protected%2520Health%250A%2520%2520Information%2520Detection%2520in%2520Medical%2520Images%26entry.906535625%3DTuan%2520Truong%2520and%2520Ivo%2520M.%2520Baltruschat%2520and%2520Mark%2520Klemens%2520and%2520Grit%2520Werner%2520and%2520Matthias%2520Lenga%26entry.1292438233%3D%2520%2520De-identification%2520of%2520medical%2520images%2520is%2520a%2520critical%2520step%2520to%2520ensure%2520privacy%250Aduring%2520data%2520sharing%2520in%2520research%2520and%2520clinical%2520settings.%2520The%2520initial%2520step%2520in%2520this%250Aprocess%2520involves%2520detecting%2520Protected%2520Health%2520Information%2520%2528PHI%2529%252C%2520which%2520can%2520be%250Afound%2520in%2520image%2520metadata%2520or%2520imprinted%2520within%2520image%2520pixels.%2520Despite%2520the%250Aimportance%2520of%2520such%2520systems%252C%2520there%2520has%2520been%2520limited%2520evaluation%2520of%2520existing%250AAI-based%2520solutions%252C%2520creating%2520barriers%2520to%2520the%2520development%2520of%2520reliable%2520and%2520robust%250Atools.%2520In%2520this%2520study%252C%2520we%2520present%2520an%2520AI-based%2520pipeline%2520for%2520PHI%2520detection%252C%250Acomprising%2520three%2520key%2520components%253A%2520text%2520detection%252C%2520text%2520extraction%252C%2520and%2520analysis%250Aof%2520PHI%2520content%2520in%2520medical%2520images.%2520By%2520experimenting%2520with%2520exchanging%2520roles%2520of%250Avision%2520and%2520language%2520models%2520within%2520the%2520pipeline%252C%2520we%2520evaluate%2520the%2520performance%2520and%250Arecommend%2520the%2520best%2520setup%2520for%2520the%2520PHI%2520detection%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20AI-based%20System%20Design%20for%20Pixel-level%20Protected%20Health%0A%20%20Information%20Detection%20in%20Medical%20Images&entry.906535625=Tuan%20Truong%20and%20Ivo%20M.%20Baltruschat%20and%20Mark%20Klemens%20and%20Grit%20Werner%20and%20Matthias%20Lenga&entry.1292438233=%20%20De-identification%20of%20medical%20images%20is%20a%20critical%20step%20to%20ensure%20privacy%0Aduring%20data%20sharing%20in%20research%20and%20clinical%20settings.%20The%20initial%20step%20in%20this%0Aprocess%20involves%20detecting%20Protected%20Health%20Information%20%28PHI%29%2C%20which%20can%20be%0Afound%20in%20image%20metadata%20or%20imprinted%20within%20image%20pixels.%20Despite%20the%0Aimportance%20of%20such%20systems%2C%20there%20has%20been%20limited%20evaluation%20of%20existing%0AAI-based%20solutions%2C%20creating%20barriers%20to%20the%20development%20of%20reliable%20and%20robust%0Atools.%20In%20this%20study%2C%20we%20present%20an%20AI-based%20pipeline%20for%20PHI%20detection%2C%0Acomprising%20three%20key%20components%3A%20text%20detection%2C%20text%20extraction%2C%20and%20analysis%0Aof%20PHI%20content%20in%20medical%20images.%20By%20experimenting%20with%20exchanging%20roles%20of%0Avision%20and%20language%20models%20within%20the%20pipeline%2C%20we%20evaluate%20the%20performance%20and%0Arecommend%20the%20best%20setup%20for%20the%20PHI%20detection%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09552v1&entry.124074799=Read"},
{"title": "Overshoot: Taking advantage of future gradients in momentum-based\n  stochastic optimization", "author": "Jakub Kopal and Michal Gregor and Santiago de Leon-Martinez and Jakub Simko", "abstract": "  Overshoot is a novel, momentum-based stochastic gradient descent optimization\nmethod designed to enhance performance beyond standard and Nesterov's momentum.\nIn conventional momentum methods, gradients from previous steps are aggregated\nwith the gradient at current model weights before taking a step and updating\nthe model. Rather than calculating gradient at the current model weights,\nOvershoot calculates the gradient at model weights shifted in the direction of\nthe current momentum. This sacrifices the immediate benefit of using the\ngradient w.r.t. the exact model weights now, in favor of evaluating at a point,\nwhich will likely be more relevant for future updates. We show that\nincorporating this principle into momentum-based optimizers (SGD with momentum\nand Adam) results in faster convergence (saving on average at least 15% of\nsteps). Overshoot consistently outperforms both standard and Nesterov's\nmomentum across a wide range of tasks and integrates into popular\nmomentum-based optimizers with zero memory and small computational overhead.\n", "link": "http://arxiv.org/abs/2501.09556v1", "date": "2025-01-16", "relevancy": 1.9493, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4981}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4828}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overshoot%3A%20Taking%20advantage%20of%20future%20gradients%20in%20momentum-based%0A%20%20stochastic%20optimization&body=Title%3A%20Overshoot%3A%20Taking%20advantage%20of%20future%20gradients%20in%20momentum-based%0A%20%20stochastic%20optimization%0AAuthor%3A%20Jakub%20Kopal%20and%20Michal%20Gregor%20and%20Santiago%20de%20Leon-Martinez%20and%20Jakub%20Simko%0AAbstract%3A%20%20%20Overshoot%20is%20a%20novel%2C%20momentum-based%20stochastic%20gradient%20descent%20optimization%0Amethod%20designed%20to%20enhance%20performance%20beyond%20standard%20and%20Nesterov%27s%20momentum.%0AIn%20conventional%20momentum%20methods%2C%20gradients%20from%20previous%20steps%20are%20aggregated%0Awith%20the%20gradient%20at%20current%20model%20weights%20before%20taking%20a%20step%20and%20updating%0Athe%20model.%20Rather%20than%20calculating%20gradient%20at%20the%20current%20model%20weights%2C%0AOvershoot%20calculates%20the%20gradient%20at%20model%20weights%20shifted%20in%20the%20direction%20of%0Athe%20current%20momentum.%20This%20sacrifices%20the%20immediate%20benefit%20of%20using%20the%0Agradient%20w.r.t.%20the%20exact%20model%20weights%20now%2C%20in%20favor%20of%20evaluating%20at%20a%20point%2C%0Awhich%20will%20likely%20be%20more%20relevant%20for%20future%20updates.%20We%20show%20that%0Aincorporating%20this%20principle%20into%20momentum-based%20optimizers%20%28SGD%20with%20momentum%0Aand%20Adam%29%20results%20in%20faster%20convergence%20%28saving%20on%20average%20at%20least%2015%25%20of%0Asteps%29.%20Overshoot%20consistently%20outperforms%20both%20standard%20and%20Nesterov%27s%0Amomentum%20across%20a%20wide%20range%20of%20tasks%20and%20integrates%20into%20popular%0Amomentum-based%20optimizers%20with%20zero%20memory%20and%20small%20computational%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvershoot%253A%2520Taking%2520advantage%2520of%2520future%2520gradients%2520in%2520momentum-based%250A%2520%2520stochastic%2520optimization%26entry.906535625%3DJakub%2520Kopal%2520and%2520Michal%2520Gregor%2520and%2520Santiago%2520de%2520Leon-Martinez%2520and%2520Jakub%2520Simko%26entry.1292438233%3D%2520%2520Overshoot%2520is%2520a%2520novel%252C%2520momentum-based%2520stochastic%2520gradient%2520descent%2520optimization%250Amethod%2520designed%2520to%2520enhance%2520performance%2520beyond%2520standard%2520and%2520Nesterov%2527s%2520momentum.%250AIn%2520conventional%2520momentum%2520methods%252C%2520gradients%2520from%2520previous%2520steps%2520are%2520aggregated%250Awith%2520the%2520gradient%2520at%2520current%2520model%2520weights%2520before%2520taking%2520a%2520step%2520and%2520updating%250Athe%2520model.%2520Rather%2520than%2520calculating%2520gradient%2520at%2520the%2520current%2520model%2520weights%252C%250AOvershoot%2520calculates%2520the%2520gradient%2520at%2520model%2520weights%2520shifted%2520in%2520the%2520direction%2520of%250Athe%2520current%2520momentum.%2520This%2520sacrifices%2520the%2520immediate%2520benefit%2520of%2520using%2520the%250Agradient%2520w.r.t.%2520the%2520exact%2520model%2520weights%2520now%252C%2520in%2520favor%2520of%2520evaluating%2520at%2520a%2520point%252C%250Awhich%2520will%2520likely%2520be%2520more%2520relevant%2520for%2520future%2520updates.%2520We%2520show%2520that%250Aincorporating%2520this%2520principle%2520into%2520momentum-based%2520optimizers%2520%2528SGD%2520with%2520momentum%250Aand%2520Adam%2529%2520results%2520in%2520faster%2520convergence%2520%2528saving%2520on%2520average%2520at%2520least%252015%2525%2520of%250Asteps%2529.%2520Overshoot%2520consistently%2520outperforms%2520both%2520standard%2520and%2520Nesterov%2527s%250Amomentum%2520across%2520a%2520wide%2520range%2520of%2520tasks%2520and%2520integrates%2520into%2520popular%250Amomentum-based%2520optimizers%2520with%2520zero%2520memory%2520and%2520small%2520computational%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overshoot%3A%20Taking%20advantage%20of%20future%20gradients%20in%20momentum-based%0A%20%20stochastic%20optimization&entry.906535625=Jakub%20Kopal%20and%20Michal%20Gregor%20and%20Santiago%20de%20Leon-Martinez%20and%20Jakub%20Simko&entry.1292438233=%20%20Overshoot%20is%20a%20novel%2C%20momentum-based%20stochastic%20gradient%20descent%20optimization%0Amethod%20designed%20to%20enhance%20performance%20beyond%20standard%20and%20Nesterov%27s%20momentum.%0AIn%20conventional%20momentum%20methods%2C%20gradients%20from%20previous%20steps%20are%20aggregated%0Awith%20the%20gradient%20at%20current%20model%20weights%20before%20taking%20a%20step%20and%20updating%0Athe%20model.%20Rather%20than%20calculating%20gradient%20at%20the%20current%20model%20weights%2C%0AOvershoot%20calculates%20the%20gradient%20at%20model%20weights%20shifted%20in%20the%20direction%20of%0Athe%20current%20momentum.%20This%20sacrifices%20the%20immediate%20benefit%20of%20using%20the%0Agradient%20w.r.t.%20the%20exact%20model%20weights%20now%2C%20in%20favor%20of%20evaluating%20at%20a%20point%2C%0Awhich%20will%20likely%20be%20more%20relevant%20for%20future%20updates.%20We%20show%20that%0Aincorporating%20this%20principle%20into%20momentum-based%20optimizers%20%28SGD%20with%20momentum%0Aand%20Adam%29%20results%20in%20faster%20convergence%20%28saving%20on%20average%20at%20least%2015%25%20of%0Asteps%29.%20Overshoot%20consistently%20outperforms%20both%20standard%20and%20Nesterov%27s%0Amomentum%20across%20a%20wide%20range%20of%20tasks%20and%20integrates%20into%20popular%0Amomentum-based%20optimizers%20with%20zero%20memory%20and%20small%20computational%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09556v1&entry.124074799=Read"},
{"title": "Class Incremental Fault Diagnosis under Limited Fault Data via\n  Supervised Contrastive Knowledge Distillation", "author": "Hanrong Zhang and Yifei Yao and Zixuan Wang and Jiayuan Su and Mengxuan Li and Peng Peng and Hongwei Wang", "abstract": "  Class-incremental fault diagnosis requires a model to adapt to new fault\nclasses while retaining previous knowledge. However, limited research exists\nfor imbalanced and long-tailed data. Extracting discriminative features from\nfew-shot fault data is challenging, and adding new fault classes often demands\ncostly model retraining. Moreover, incremental training of existing methods\nrisks catastrophic forgetting, and severe class imbalance can bias the model's\ndecisions toward normal classes. To tackle these issues, we introduce a\nSupervised Contrastive knowledge distiLlation for class Incremental Fault\nDiagnosis (SCLIFD) framework proposing supervised contrastive knowledge\ndistillation for improved representation learning capability and less\nforgetting, a novel prioritized exemplar selection method for sample replay to\nalleviate catastrophic forgetting, and the Random Forest Classifier to address\nthe class imbalance. Extensive experimentation on simulated and real-world\nindustrial datasets across various imbalance ratios demonstrates the\nsuperiority of SCLIFD over existing approaches. Our code can be found at\nhttps://github.com/Zhang-Henry/SCLIFD_TII.\n", "link": "http://arxiv.org/abs/2501.09525v1", "date": "2025-01-16", "relevancy": 1.9479, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4933}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.487}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Class%20Incremental%20Fault%20Diagnosis%20under%20Limited%20Fault%20Data%20via%0A%20%20Supervised%20Contrastive%20Knowledge%20Distillation&body=Title%3A%20Class%20Incremental%20Fault%20Diagnosis%20under%20Limited%20Fault%20Data%20via%0A%20%20Supervised%20Contrastive%20Knowledge%20Distillation%0AAuthor%3A%20Hanrong%20Zhang%20and%20Yifei%20Yao%20and%20Zixuan%20Wang%20and%20Jiayuan%20Su%20and%20Mengxuan%20Li%20and%20Peng%20Peng%20and%20Hongwei%20Wang%0AAbstract%3A%20%20%20Class-incremental%20fault%20diagnosis%20requires%20a%20model%20to%20adapt%20to%20new%20fault%0Aclasses%20while%20retaining%20previous%20knowledge.%20However%2C%20limited%20research%20exists%0Afor%20imbalanced%20and%20long-tailed%20data.%20Extracting%20discriminative%20features%20from%0Afew-shot%20fault%20data%20is%20challenging%2C%20and%20adding%20new%20fault%20classes%20often%20demands%0Acostly%20model%20retraining.%20Moreover%2C%20incremental%20training%20of%20existing%20methods%0Arisks%20catastrophic%20forgetting%2C%20and%20severe%20class%20imbalance%20can%20bias%20the%20model%27s%0Adecisions%20toward%20normal%20classes.%20To%20tackle%20these%20issues%2C%20we%20introduce%20a%0ASupervised%20Contrastive%20knowledge%20distiLlation%20for%20class%20Incremental%20Fault%0ADiagnosis%20%28SCLIFD%29%20framework%20proposing%20supervised%20contrastive%20knowledge%0Adistillation%20for%20improved%20representation%20learning%20capability%20and%20less%0Aforgetting%2C%20a%20novel%20prioritized%20exemplar%20selection%20method%20for%20sample%20replay%20to%0Aalleviate%20catastrophic%20forgetting%2C%20and%20the%20Random%20Forest%20Classifier%20to%20address%0Athe%20class%20imbalance.%20Extensive%20experimentation%20on%20simulated%20and%20real-world%0Aindustrial%20datasets%20across%20various%20imbalance%20ratios%20demonstrates%20the%0Asuperiority%20of%20SCLIFD%20over%20existing%20approaches.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/Zhang-Henry/SCLIFD_TII.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClass%2520Incremental%2520Fault%2520Diagnosis%2520under%2520Limited%2520Fault%2520Data%2520via%250A%2520%2520Supervised%2520Contrastive%2520Knowledge%2520Distillation%26entry.906535625%3DHanrong%2520Zhang%2520and%2520Yifei%2520Yao%2520and%2520Zixuan%2520Wang%2520and%2520Jiayuan%2520Su%2520and%2520Mengxuan%2520Li%2520and%2520Peng%2520Peng%2520and%2520Hongwei%2520Wang%26entry.1292438233%3D%2520%2520Class-incremental%2520fault%2520diagnosis%2520requires%2520a%2520model%2520to%2520adapt%2520to%2520new%2520fault%250Aclasses%2520while%2520retaining%2520previous%2520knowledge.%2520However%252C%2520limited%2520research%2520exists%250Afor%2520imbalanced%2520and%2520long-tailed%2520data.%2520Extracting%2520discriminative%2520features%2520from%250Afew-shot%2520fault%2520data%2520is%2520challenging%252C%2520and%2520adding%2520new%2520fault%2520classes%2520often%2520demands%250Acostly%2520model%2520retraining.%2520Moreover%252C%2520incremental%2520training%2520of%2520existing%2520methods%250Arisks%2520catastrophic%2520forgetting%252C%2520and%2520severe%2520class%2520imbalance%2520can%2520bias%2520the%2520model%2527s%250Adecisions%2520toward%2520normal%2520classes.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520introduce%2520a%250ASupervised%2520Contrastive%2520knowledge%2520distiLlation%2520for%2520class%2520Incremental%2520Fault%250ADiagnosis%2520%2528SCLIFD%2529%2520framework%2520proposing%2520supervised%2520contrastive%2520knowledge%250Adistillation%2520for%2520improved%2520representation%2520learning%2520capability%2520and%2520less%250Aforgetting%252C%2520a%2520novel%2520prioritized%2520exemplar%2520selection%2520method%2520for%2520sample%2520replay%2520to%250Aalleviate%2520catastrophic%2520forgetting%252C%2520and%2520the%2520Random%2520Forest%2520Classifier%2520to%2520address%250Athe%2520class%2520imbalance.%2520Extensive%2520experimentation%2520on%2520simulated%2520and%2520real-world%250Aindustrial%2520datasets%2520across%2520various%2520imbalance%2520ratios%2520demonstrates%2520the%250Asuperiority%2520of%2520SCLIFD%2520over%2520existing%2520approaches.%2520Our%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/Zhang-Henry/SCLIFD_TII.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class%20Incremental%20Fault%20Diagnosis%20under%20Limited%20Fault%20Data%20via%0A%20%20Supervised%20Contrastive%20Knowledge%20Distillation&entry.906535625=Hanrong%20Zhang%20and%20Yifei%20Yao%20and%20Zixuan%20Wang%20and%20Jiayuan%20Su%20and%20Mengxuan%20Li%20and%20Peng%20Peng%20and%20Hongwei%20Wang&entry.1292438233=%20%20Class-incremental%20fault%20diagnosis%20requires%20a%20model%20to%20adapt%20to%20new%20fault%0Aclasses%20while%20retaining%20previous%20knowledge.%20However%2C%20limited%20research%20exists%0Afor%20imbalanced%20and%20long-tailed%20data.%20Extracting%20discriminative%20features%20from%0Afew-shot%20fault%20data%20is%20challenging%2C%20and%20adding%20new%20fault%20classes%20often%20demands%0Acostly%20model%20retraining.%20Moreover%2C%20incremental%20training%20of%20existing%20methods%0Arisks%20catastrophic%20forgetting%2C%20and%20severe%20class%20imbalance%20can%20bias%20the%20model%27s%0Adecisions%20toward%20normal%20classes.%20To%20tackle%20these%20issues%2C%20we%20introduce%20a%0ASupervised%20Contrastive%20knowledge%20distiLlation%20for%20class%20Incremental%20Fault%0ADiagnosis%20%28SCLIFD%29%20framework%20proposing%20supervised%20contrastive%20knowledge%0Adistillation%20for%20improved%20representation%20learning%20capability%20and%20less%0Aforgetting%2C%20a%20novel%20prioritized%20exemplar%20selection%20method%20for%20sample%20replay%20to%0Aalleviate%20catastrophic%20forgetting%2C%20and%20the%20Random%20Forest%20Classifier%20to%20address%0Athe%20class%20imbalance.%20Extensive%20experimentation%20on%20simulated%20and%20real-world%0Aindustrial%20datasets%20across%20various%20imbalance%20ratios%20demonstrates%20the%0Asuperiority%20of%20SCLIFD%20over%20existing%20approaches.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/Zhang-Henry/SCLIFD_TII.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09525v1&entry.124074799=Read"},
{"title": "ERGNN: Spectral Graph Neural Network With Explicitly-Optimized Rational\n  Graph Filters", "author": "Guoming Li and Jian Yang and Shangsong Liang", "abstract": "  Approximation-based spectral graph neural networks, which construct graph\nfilters with function approximation, have shown substantial performance in\ngraph learning tasks. Despite their great success, existing works primarily\nemploy polynomial approximation to construct the filters, whereas another\nsuperior option, namely ration approximation, remains underexplored. Although a\nhandful of prior works have attempted to deploy the rational approximation,\ntheir implementations often involve intensive computational demands or still\nresort to polynomial approximations, hindering full potential of the rational\ngraph filters. To address the issues, this paper introduces ERGNN, a novel\nspectral GNN with explicitly-optimized rational filter. ERGNN adopts a unique\ntwo-step framework that sequentially applies the numerator filter and the\ndenominator filter to the input signals, thus streamlining the model paradigm\nwhile enabling explicit optimization of both numerator and denominator of the\nrational filter. Extensive experiments validate the superiority of ERGNN over\nstate-of-the-art methods, establishing it as a practical solution for deploying\nrational-based GNNs.\n", "link": "http://arxiv.org/abs/2412.19106v2", "date": "2025-01-16", "relevancy": 1.9443, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.491}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4874}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ERGNN%3A%20Spectral%20Graph%20Neural%20Network%20With%20Explicitly-Optimized%20Rational%0A%20%20Graph%20Filters&body=Title%3A%20ERGNN%3A%20Spectral%20Graph%20Neural%20Network%20With%20Explicitly-Optimized%20Rational%0A%20%20Graph%20Filters%0AAuthor%3A%20Guoming%20Li%20and%20Jian%20Yang%20and%20Shangsong%20Liang%0AAbstract%3A%20%20%20Approximation-based%20spectral%20graph%20neural%20networks%2C%20which%20construct%20graph%0Afilters%20with%20function%20approximation%2C%20have%20shown%20substantial%20performance%20in%0Agraph%20learning%20tasks.%20Despite%20their%20great%20success%2C%20existing%20works%20primarily%0Aemploy%20polynomial%20approximation%20to%20construct%20the%20filters%2C%20whereas%20another%0Asuperior%20option%2C%20namely%20ration%20approximation%2C%20remains%20underexplored.%20Although%20a%0Ahandful%20of%20prior%20works%20have%20attempted%20to%20deploy%20the%20rational%20approximation%2C%0Atheir%20implementations%20often%20involve%20intensive%20computational%20demands%20or%20still%0Aresort%20to%20polynomial%20approximations%2C%20hindering%20full%20potential%20of%20the%20rational%0Agraph%20filters.%20To%20address%20the%20issues%2C%20this%20paper%20introduces%20ERGNN%2C%20a%20novel%0Aspectral%20GNN%20with%20explicitly-optimized%20rational%20filter.%20ERGNN%20adopts%20a%20unique%0Atwo-step%20framework%20that%20sequentially%20applies%20the%20numerator%20filter%20and%20the%0Adenominator%20filter%20to%20the%20input%20signals%2C%20thus%20streamlining%20the%20model%20paradigm%0Awhile%20enabling%20explicit%20optimization%20of%20both%20numerator%20and%20denominator%20of%20the%0Arational%20filter.%20Extensive%20experiments%20validate%20the%20superiority%20of%20ERGNN%20over%0Astate-of-the-art%20methods%2C%20establishing%20it%20as%20a%20practical%20solution%20for%20deploying%0Arational-based%20GNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19106v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DERGNN%253A%2520Spectral%2520Graph%2520Neural%2520Network%2520With%2520Explicitly-Optimized%2520Rational%250A%2520%2520Graph%2520Filters%26entry.906535625%3DGuoming%2520Li%2520and%2520Jian%2520Yang%2520and%2520Shangsong%2520Liang%26entry.1292438233%3D%2520%2520Approximation-based%2520spectral%2520graph%2520neural%2520networks%252C%2520which%2520construct%2520graph%250Afilters%2520with%2520function%2520approximation%252C%2520have%2520shown%2520substantial%2520performance%2520in%250Agraph%2520learning%2520tasks.%2520Despite%2520their%2520great%2520success%252C%2520existing%2520works%2520primarily%250Aemploy%2520polynomial%2520approximation%2520to%2520construct%2520the%2520filters%252C%2520whereas%2520another%250Asuperior%2520option%252C%2520namely%2520ration%2520approximation%252C%2520remains%2520underexplored.%2520Although%2520a%250Ahandful%2520of%2520prior%2520works%2520have%2520attempted%2520to%2520deploy%2520the%2520rational%2520approximation%252C%250Atheir%2520implementations%2520often%2520involve%2520intensive%2520computational%2520demands%2520or%2520still%250Aresort%2520to%2520polynomial%2520approximations%252C%2520hindering%2520full%2520potential%2520of%2520the%2520rational%250Agraph%2520filters.%2520To%2520address%2520the%2520issues%252C%2520this%2520paper%2520introduces%2520ERGNN%252C%2520a%2520novel%250Aspectral%2520GNN%2520with%2520explicitly-optimized%2520rational%2520filter.%2520ERGNN%2520adopts%2520a%2520unique%250Atwo-step%2520framework%2520that%2520sequentially%2520applies%2520the%2520numerator%2520filter%2520and%2520the%250Adenominator%2520filter%2520to%2520the%2520input%2520signals%252C%2520thus%2520streamlining%2520the%2520model%2520paradigm%250Awhile%2520enabling%2520explicit%2520optimization%2520of%2520both%2520numerator%2520and%2520denominator%2520of%2520the%250Arational%2520filter.%2520Extensive%2520experiments%2520validate%2520the%2520superiority%2520of%2520ERGNN%2520over%250Astate-of-the-art%2520methods%252C%2520establishing%2520it%2520as%2520a%2520practical%2520solution%2520for%2520deploying%250Arational-based%2520GNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19106v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ERGNN%3A%20Spectral%20Graph%20Neural%20Network%20With%20Explicitly-Optimized%20Rational%0A%20%20Graph%20Filters&entry.906535625=Guoming%20Li%20and%20Jian%20Yang%20and%20Shangsong%20Liang&entry.1292438233=%20%20Approximation-based%20spectral%20graph%20neural%20networks%2C%20which%20construct%20graph%0Afilters%20with%20function%20approximation%2C%20have%20shown%20substantial%20performance%20in%0Agraph%20learning%20tasks.%20Despite%20their%20great%20success%2C%20existing%20works%20primarily%0Aemploy%20polynomial%20approximation%20to%20construct%20the%20filters%2C%20whereas%20another%0Asuperior%20option%2C%20namely%20ration%20approximation%2C%20remains%20underexplored.%20Although%20a%0Ahandful%20of%20prior%20works%20have%20attempted%20to%20deploy%20the%20rational%20approximation%2C%0Atheir%20implementations%20often%20involve%20intensive%20computational%20demands%20or%20still%0Aresort%20to%20polynomial%20approximations%2C%20hindering%20full%20potential%20of%20the%20rational%0Agraph%20filters.%20To%20address%20the%20issues%2C%20this%20paper%20introduces%20ERGNN%2C%20a%20novel%0Aspectral%20GNN%20with%20explicitly-optimized%20rational%20filter.%20ERGNN%20adopts%20a%20unique%0Atwo-step%20framework%20that%20sequentially%20applies%20the%20numerator%20filter%20and%20the%0Adenominator%20filter%20to%20the%20input%20signals%2C%20thus%20streamlining%20the%20model%20paradigm%0Awhile%20enabling%20explicit%20optimization%20of%20both%20numerator%20and%20denominator%20of%20the%0Arational%20filter.%20Extensive%20experiments%20validate%20the%20superiority%20of%20ERGNN%20over%0Astate-of-the-art%20methods%2C%20establishing%20it%20as%20a%20practical%20solution%20for%20deploying%0Arational-based%20GNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19106v2&entry.124074799=Read"},
{"title": "Generating particle physics Lagrangians with transformers", "author": "Yong Sheng Koay and Rikard Enberg and Stefano Moretti and Eliel Camargo-Molina", "abstract": "  In physics, Lagrangians provide a systematic way to describe laws governing\nphysical systems. In the context of particle physics, they encode the\ninteractions and behavior of the fundamental building blocks of our universe.\nBy treating Lagrangians as complex, rule-based constructs similar to linguistic\nexpressions, we trained a transformer model -- proven to be effective in\nnatural language tasks -- to predict the Lagrangian corresponding to a given\nlist of particles. We report on the transformer's performance in constructing\nLagrangians respecting the Standard Model $\\mathrm{SU}(3)\\times\n\\mathrm{SU}(2)\\times \\mathrm{U}(1)$ gauge symmetries. The resulting model is\nshown to achieve high accuracies (over 90\\%) with Lagrangians up to six matter\nfields, with the capacity to generalize beyond the training distribution,\nalbeit within architectural constraints. We show through an analysis of input\nembeddings that the model has internalized concepts such as group\nrepresentations and conjugation operations as it learned to generate\nLagrangians. We make the model and training datasets available to the\ncommunity. An interactive demonstration can be found at:\n\\url{https://huggingface.co/spaces/JoseEliel/generate-lagrangians}.\n", "link": "http://arxiv.org/abs/2501.09729v1", "date": "2025-01-16", "relevancy": 1.9422, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5237}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4908}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20particle%20physics%20Lagrangians%20with%20transformers&body=Title%3A%20Generating%20particle%20physics%20Lagrangians%20with%20transformers%0AAuthor%3A%20Yong%20Sheng%20Koay%20and%20Rikard%20Enberg%20and%20Stefano%20Moretti%20and%20Eliel%20Camargo-Molina%0AAbstract%3A%20%20%20In%20physics%2C%20Lagrangians%20provide%20a%20systematic%20way%20to%20describe%20laws%20governing%0Aphysical%20systems.%20In%20the%20context%20of%20particle%20physics%2C%20they%20encode%20the%0Ainteractions%20and%20behavior%20of%20the%20fundamental%20building%20blocks%20of%20our%20universe.%0ABy%20treating%20Lagrangians%20as%20complex%2C%20rule-based%20constructs%20similar%20to%20linguistic%0Aexpressions%2C%20we%20trained%20a%20transformer%20model%20--%20proven%20to%20be%20effective%20in%0Anatural%20language%20tasks%20--%20to%20predict%20the%20Lagrangian%20corresponding%20to%20a%20given%0Alist%20of%20particles.%20We%20report%20on%20the%20transformer%27s%20performance%20in%20constructing%0ALagrangians%20respecting%20the%20Standard%20Model%20%24%5Cmathrm%7BSU%7D%283%29%5Ctimes%0A%5Cmathrm%7BSU%7D%282%29%5Ctimes%20%5Cmathrm%7BU%7D%281%29%24%20gauge%20symmetries.%20The%20resulting%20model%20is%0Ashown%20to%20achieve%20high%20accuracies%20%28over%2090%5C%25%29%20with%20Lagrangians%20up%20to%20six%20matter%0Afields%2C%20with%20the%20capacity%20to%20generalize%20beyond%20the%20training%20distribution%2C%0Aalbeit%20within%20architectural%20constraints.%20We%20show%20through%20an%20analysis%20of%20input%0Aembeddings%20that%20the%20model%20has%20internalized%20concepts%20such%20as%20group%0Arepresentations%20and%20conjugation%20operations%20as%20it%20learned%20to%20generate%0ALagrangians.%20We%20make%20the%20model%20and%20training%20datasets%20available%20to%20the%0Acommunity.%20An%20interactive%20demonstration%20can%20be%20found%20at%3A%0A%5Curl%7Bhttps%3A//huggingface.co/spaces/JoseEliel/generate-lagrangians%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520particle%2520physics%2520Lagrangians%2520with%2520transformers%26entry.906535625%3DYong%2520Sheng%2520Koay%2520and%2520Rikard%2520Enberg%2520and%2520Stefano%2520Moretti%2520and%2520Eliel%2520Camargo-Molina%26entry.1292438233%3D%2520%2520In%2520physics%252C%2520Lagrangians%2520provide%2520a%2520systematic%2520way%2520to%2520describe%2520laws%2520governing%250Aphysical%2520systems.%2520In%2520the%2520context%2520of%2520particle%2520physics%252C%2520they%2520encode%2520the%250Ainteractions%2520and%2520behavior%2520of%2520the%2520fundamental%2520building%2520blocks%2520of%2520our%2520universe.%250ABy%2520treating%2520Lagrangians%2520as%2520complex%252C%2520rule-based%2520constructs%2520similar%2520to%2520linguistic%250Aexpressions%252C%2520we%2520trained%2520a%2520transformer%2520model%2520--%2520proven%2520to%2520be%2520effective%2520in%250Anatural%2520language%2520tasks%2520--%2520to%2520predict%2520the%2520Lagrangian%2520corresponding%2520to%2520a%2520given%250Alist%2520of%2520particles.%2520We%2520report%2520on%2520the%2520transformer%2527s%2520performance%2520in%2520constructing%250ALagrangians%2520respecting%2520the%2520Standard%2520Model%2520%2524%255Cmathrm%257BSU%257D%25283%2529%255Ctimes%250A%255Cmathrm%257BSU%257D%25282%2529%255Ctimes%2520%255Cmathrm%257BU%257D%25281%2529%2524%2520gauge%2520symmetries.%2520The%2520resulting%2520model%2520is%250Ashown%2520to%2520achieve%2520high%2520accuracies%2520%2528over%252090%255C%2525%2529%2520with%2520Lagrangians%2520up%2520to%2520six%2520matter%250Afields%252C%2520with%2520the%2520capacity%2520to%2520generalize%2520beyond%2520the%2520training%2520distribution%252C%250Aalbeit%2520within%2520architectural%2520constraints.%2520We%2520show%2520through%2520an%2520analysis%2520of%2520input%250Aembeddings%2520that%2520the%2520model%2520has%2520internalized%2520concepts%2520such%2520as%2520group%250Arepresentations%2520and%2520conjugation%2520operations%2520as%2520it%2520learned%2520to%2520generate%250ALagrangians.%2520We%2520make%2520the%2520model%2520and%2520training%2520datasets%2520available%2520to%2520the%250Acommunity.%2520An%2520interactive%2520demonstration%2520can%2520be%2520found%2520at%253A%250A%255Curl%257Bhttps%253A//huggingface.co/spaces/JoseEliel/generate-lagrangians%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20particle%20physics%20Lagrangians%20with%20transformers&entry.906535625=Yong%20Sheng%20Koay%20and%20Rikard%20Enberg%20and%20Stefano%20Moretti%20and%20Eliel%20Camargo-Molina&entry.1292438233=%20%20In%20physics%2C%20Lagrangians%20provide%20a%20systematic%20way%20to%20describe%20laws%20governing%0Aphysical%20systems.%20In%20the%20context%20of%20particle%20physics%2C%20they%20encode%20the%0Ainteractions%20and%20behavior%20of%20the%20fundamental%20building%20blocks%20of%20our%20universe.%0ABy%20treating%20Lagrangians%20as%20complex%2C%20rule-based%20constructs%20similar%20to%20linguistic%0Aexpressions%2C%20we%20trained%20a%20transformer%20model%20--%20proven%20to%20be%20effective%20in%0Anatural%20language%20tasks%20--%20to%20predict%20the%20Lagrangian%20corresponding%20to%20a%20given%0Alist%20of%20particles.%20We%20report%20on%20the%20transformer%27s%20performance%20in%20constructing%0ALagrangians%20respecting%20the%20Standard%20Model%20%24%5Cmathrm%7BSU%7D%283%29%5Ctimes%0A%5Cmathrm%7BSU%7D%282%29%5Ctimes%20%5Cmathrm%7BU%7D%281%29%24%20gauge%20symmetries.%20The%20resulting%20model%20is%0Ashown%20to%20achieve%20high%20accuracies%20%28over%2090%5C%25%29%20with%20Lagrangians%20up%20to%20six%20matter%0Afields%2C%20with%20the%20capacity%20to%20generalize%20beyond%20the%20training%20distribution%2C%0Aalbeit%20within%20architectural%20constraints.%20We%20show%20through%20an%20analysis%20of%20input%0Aembeddings%20that%20the%20model%20has%20internalized%20concepts%20such%20as%20group%0Arepresentations%20and%20conjugation%20operations%20as%20it%20learned%20to%20generate%0ALagrangians.%20We%20make%20the%20model%20and%20training%20datasets%20available%20to%20the%0Acommunity.%20An%20interactive%20demonstration%20can%20be%20found%20at%3A%0A%5Curl%7Bhttps%3A//huggingface.co/spaces/JoseEliel/generate-lagrangians%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09729v1&entry.124074799=Read"},
{"title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising\n  Steps", "author": "Nanye Ma and Shangyuan Tong and Haolin Jia and Hexiang Hu and Yu-Chuan Su and Mingda Zhang and Xuan Yang and Yandong Li and Tommi Jaakkola and Xuhui Jia and Saining Xie", "abstract": "  Generative models have made significant impacts across various domains,\nlargely due to their ability to scale during training by increasing data,\ncomputational resources, and model size, a phenomenon characterized by the\nscaling laws. Recent research has begun to explore inference-time scaling\nbehavior in Large Language Models (LLMs), revealing how performance can further\nimprove with additional computation during inference. Unlike LLMs, diffusion\nmodels inherently possess the flexibility to adjust inference-time computation\nvia the number of denoising steps, although the performance gains typically\nflatten after a few dozen. In this work, we explore the inference-time scaling\nbehavior of diffusion models beyond increasing denoising steps and investigate\nhow the generation performance can further improve with increased computation.\nSpecifically, we consider a search problem aimed at identifying better noises\nfor the diffusion sampling process. We structure the design space along two\naxes: the verifiers used to provide feedback, and the algorithms used to find\nbetter noise candidates. Through extensive experiments on class-conditioned and\ntext-conditioned image generation benchmarks, our findings reveal that\nincreasing inference-time compute leads to substantial improvements in the\nquality of samples generated by diffusion models, and with the complicated\nnature of images, combinations of the components in the framework can be\nspecifically chosen to conform with different application scenario.\n", "link": "http://arxiv.org/abs/2501.09732v1", "date": "2025-01-16", "relevancy": 1.939, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6829}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6717}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference-Time%20Scaling%20for%20Diffusion%20Models%20beyond%20Scaling%20Denoising%0A%20%20Steps&body=Title%3A%20Inference-Time%20Scaling%20for%20Diffusion%20Models%20beyond%20Scaling%20Denoising%0A%20%20Steps%0AAuthor%3A%20Nanye%20Ma%20and%20Shangyuan%20Tong%20and%20Haolin%20Jia%20and%20Hexiang%20Hu%20and%20Yu-Chuan%20Su%20and%20Mingda%20Zhang%20and%20Xuan%20Yang%20and%20Yandong%20Li%20and%20Tommi%20Jaakkola%20and%20Xuhui%20Jia%20and%20Saining%20Xie%0AAbstract%3A%20%20%20Generative%20models%20have%20made%20significant%20impacts%20across%20various%20domains%2C%0Alargely%20due%20to%20their%20ability%20to%20scale%20during%20training%20by%20increasing%20data%2C%0Acomputational%20resources%2C%20and%20model%20size%2C%20a%20phenomenon%20characterized%20by%20the%0Ascaling%20laws.%20Recent%20research%20has%20begun%20to%20explore%20inference-time%20scaling%0Abehavior%20in%20Large%20Language%20Models%20%28LLMs%29%2C%20revealing%20how%20performance%20can%20further%0Aimprove%20with%20additional%20computation%20during%20inference.%20Unlike%20LLMs%2C%20diffusion%0Amodels%20inherently%20possess%20the%20flexibility%20to%20adjust%20inference-time%20computation%0Avia%20the%20number%20of%20denoising%20steps%2C%20although%20the%20performance%20gains%20typically%0Aflatten%20after%20a%20few%20dozen.%20In%20this%20work%2C%20we%20explore%20the%20inference-time%20scaling%0Abehavior%20of%20diffusion%20models%20beyond%20increasing%20denoising%20steps%20and%20investigate%0Ahow%20the%20generation%20performance%20can%20further%20improve%20with%20increased%20computation.%0ASpecifically%2C%20we%20consider%20a%20search%20problem%20aimed%20at%20identifying%20better%20noises%0Afor%20the%20diffusion%20sampling%20process.%20We%20structure%20the%20design%20space%20along%20two%0Aaxes%3A%20the%20verifiers%20used%20to%20provide%20feedback%2C%20and%20the%20algorithms%20used%20to%20find%0Abetter%20noise%20candidates.%20Through%20extensive%20experiments%20on%20class-conditioned%20and%0Atext-conditioned%20image%20generation%20benchmarks%2C%20our%20findings%20reveal%20that%0Aincreasing%20inference-time%20compute%20leads%20to%20substantial%20improvements%20in%20the%0Aquality%20of%20samples%20generated%20by%20diffusion%20models%2C%20and%20with%20the%20complicated%0Anature%20of%20images%2C%20combinations%20of%20the%20components%20in%20the%20framework%20can%20be%0Aspecifically%20chosen%20to%20conform%20with%20different%20application%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference-Time%2520Scaling%2520for%2520Diffusion%2520Models%2520beyond%2520Scaling%2520Denoising%250A%2520%2520Steps%26entry.906535625%3DNanye%2520Ma%2520and%2520Shangyuan%2520Tong%2520and%2520Haolin%2520Jia%2520and%2520Hexiang%2520Hu%2520and%2520Yu-Chuan%2520Su%2520and%2520Mingda%2520Zhang%2520and%2520Xuan%2520Yang%2520and%2520Yandong%2520Li%2520and%2520Tommi%2520Jaakkola%2520and%2520Xuhui%2520Jia%2520and%2520Saining%2520Xie%26entry.1292438233%3D%2520%2520Generative%2520models%2520have%2520made%2520significant%2520impacts%2520across%2520various%2520domains%252C%250Alargely%2520due%2520to%2520their%2520ability%2520to%2520scale%2520during%2520training%2520by%2520increasing%2520data%252C%250Acomputational%2520resources%252C%2520and%2520model%2520size%252C%2520a%2520phenomenon%2520characterized%2520by%2520the%250Ascaling%2520laws.%2520Recent%2520research%2520has%2520begun%2520to%2520explore%2520inference-time%2520scaling%250Abehavior%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520revealing%2520how%2520performance%2520can%2520further%250Aimprove%2520with%2520additional%2520computation%2520during%2520inference.%2520Unlike%2520LLMs%252C%2520diffusion%250Amodels%2520inherently%2520possess%2520the%2520flexibility%2520to%2520adjust%2520inference-time%2520computation%250Avia%2520the%2520number%2520of%2520denoising%2520steps%252C%2520although%2520the%2520performance%2520gains%2520typically%250Aflatten%2520after%2520a%2520few%2520dozen.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520inference-time%2520scaling%250Abehavior%2520of%2520diffusion%2520models%2520beyond%2520increasing%2520denoising%2520steps%2520and%2520investigate%250Ahow%2520the%2520generation%2520performance%2520can%2520further%2520improve%2520with%2520increased%2520computation.%250ASpecifically%252C%2520we%2520consider%2520a%2520search%2520problem%2520aimed%2520at%2520identifying%2520better%2520noises%250Afor%2520the%2520diffusion%2520sampling%2520process.%2520We%2520structure%2520the%2520design%2520space%2520along%2520two%250Aaxes%253A%2520the%2520verifiers%2520used%2520to%2520provide%2520feedback%252C%2520and%2520the%2520algorithms%2520used%2520to%2520find%250Abetter%2520noise%2520candidates.%2520Through%2520extensive%2520experiments%2520on%2520class-conditioned%2520and%250Atext-conditioned%2520image%2520generation%2520benchmarks%252C%2520our%2520findings%2520reveal%2520that%250Aincreasing%2520inference-time%2520compute%2520leads%2520to%2520substantial%2520improvements%2520in%2520the%250Aquality%2520of%2520samples%2520generated%2520by%2520diffusion%2520models%252C%2520and%2520with%2520the%2520complicated%250Anature%2520of%2520images%252C%2520combinations%2520of%2520the%2520components%2520in%2520the%2520framework%2520can%2520be%250Aspecifically%2520chosen%2520to%2520conform%2520with%2520different%2520application%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference-Time%20Scaling%20for%20Diffusion%20Models%20beyond%20Scaling%20Denoising%0A%20%20Steps&entry.906535625=Nanye%20Ma%20and%20Shangyuan%20Tong%20and%20Haolin%20Jia%20and%20Hexiang%20Hu%20and%20Yu-Chuan%20Su%20and%20Mingda%20Zhang%20and%20Xuan%20Yang%20and%20Yandong%20Li%20and%20Tommi%20Jaakkola%20and%20Xuhui%20Jia%20and%20Saining%20Xie&entry.1292438233=%20%20Generative%20models%20have%20made%20significant%20impacts%20across%20various%20domains%2C%0Alargely%20due%20to%20their%20ability%20to%20scale%20during%20training%20by%20increasing%20data%2C%0Acomputational%20resources%2C%20and%20model%20size%2C%20a%20phenomenon%20characterized%20by%20the%0Ascaling%20laws.%20Recent%20research%20has%20begun%20to%20explore%20inference-time%20scaling%0Abehavior%20in%20Large%20Language%20Models%20%28LLMs%29%2C%20revealing%20how%20performance%20can%20further%0Aimprove%20with%20additional%20computation%20during%20inference.%20Unlike%20LLMs%2C%20diffusion%0Amodels%20inherently%20possess%20the%20flexibility%20to%20adjust%20inference-time%20computation%0Avia%20the%20number%20of%20denoising%20steps%2C%20although%20the%20performance%20gains%20typically%0Aflatten%20after%20a%20few%20dozen.%20In%20this%20work%2C%20we%20explore%20the%20inference-time%20scaling%0Abehavior%20of%20diffusion%20models%20beyond%20increasing%20denoising%20steps%20and%20investigate%0Ahow%20the%20generation%20performance%20can%20further%20improve%20with%20increased%20computation.%0ASpecifically%2C%20we%20consider%20a%20search%20problem%20aimed%20at%20identifying%20better%20noises%0Afor%20the%20diffusion%20sampling%20process.%20We%20structure%20the%20design%20space%20along%20two%0Aaxes%3A%20the%20verifiers%20used%20to%20provide%20feedback%2C%20and%20the%20algorithms%20used%20to%20find%0Abetter%20noise%20candidates.%20Through%20extensive%20experiments%20on%20class-conditioned%20and%0Atext-conditioned%20image%20generation%20benchmarks%2C%20our%20findings%20reveal%20that%0Aincreasing%20inference-time%20compute%20leads%20to%20substantial%20improvements%20in%20the%0Aquality%20of%20samples%20generated%20by%20diffusion%20models%2C%20and%20with%20the%20complicated%0Anature%20of%20images%2C%20combinations%20of%20the%20components%20in%20the%20framework%20can%20be%0Aspecifically%20chosen%20to%20conform%20with%20different%20application%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09732v1&entry.124074799=Read"},
{"title": "Confidence Estimation for Error Detection in Text-to-SQL Systems", "author": "Oleg Somov and Elena Tutubalina", "abstract": "  Text-to-SQL enables users to interact with databases through natural\nlanguage, simplifying the retrieval and synthesis of information. Despite the\nsuccess of large language models (LLMs) in converting natural language\nquestions into SQL queries, their broader adoption is limited by two main\nchallenges: achieving robust generalization across diverse queries and ensuring\ninterpretative confidence in their predictions. To tackle these issues, our\nresearch investigates the integration of selective classifiers into Text-to-SQL\nsystems. We analyse the trade-off between coverage and risk using entropy based\nconfidence estimation with selective classifiers and assess its impact on the\noverall performance of Text-to-SQL models. Additionally, we explore the models'\ninitial calibration and improve it with calibration techniques for better model\nalignment between confidence and accuracy. Our experimental results show that\nencoder-decoder T5 is better calibrated than in-context-learning GPT 4 and\ndecoder-only Llama 3, thus the designated external entropy-based selective\nclassifier has better performance. The study also reveal that, in terms of\nerror detection, selective classifier with a higher probability detects errors\nassociated with irrelevant questions rather than incorrect query generations.\n", "link": "http://arxiv.org/abs/2501.09527v1", "date": "2025-01-16", "relevancy": 1.9316, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.497}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence%20Estimation%20for%20Error%20Detection%20in%20Text-to-SQL%20Systems&body=Title%3A%20Confidence%20Estimation%20for%20Error%20Detection%20in%20Text-to-SQL%20Systems%0AAuthor%3A%20Oleg%20Somov%20and%20Elena%20Tutubalina%0AAbstract%3A%20%20%20Text-to-SQL%20enables%20users%20to%20interact%20with%20databases%20through%20natural%0Alanguage%2C%20simplifying%20the%20retrieval%20and%20synthesis%20of%20information.%20Despite%20the%0Asuccess%20of%20large%20language%20models%20%28LLMs%29%20in%20converting%20natural%20language%0Aquestions%20into%20SQL%20queries%2C%20their%20broader%20adoption%20is%20limited%20by%20two%20main%0Achallenges%3A%20achieving%20robust%20generalization%20across%20diverse%20queries%20and%20ensuring%0Ainterpretative%20confidence%20in%20their%20predictions.%20To%20tackle%20these%20issues%2C%20our%0Aresearch%20investigates%20the%20integration%20of%20selective%20classifiers%20into%20Text-to-SQL%0Asystems.%20We%20analyse%20the%20trade-off%20between%20coverage%20and%20risk%20using%20entropy%20based%0Aconfidence%20estimation%20with%20selective%20classifiers%20and%20assess%20its%20impact%20on%20the%0Aoverall%20performance%20of%20Text-to-SQL%20models.%20Additionally%2C%20we%20explore%20the%20models%27%0Ainitial%20calibration%20and%20improve%20it%20with%20calibration%20techniques%20for%20better%20model%0Aalignment%20between%20confidence%20and%20accuracy.%20Our%20experimental%20results%20show%20that%0Aencoder-decoder%20T5%20is%20better%20calibrated%20than%20in-context-learning%20GPT%204%20and%0Adecoder-only%20Llama%203%2C%20thus%20the%20designated%20external%20entropy-based%20selective%0Aclassifier%20has%20better%20performance.%20The%20study%20also%20reveal%20that%2C%20in%20terms%20of%0Aerror%20detection%2C%20selective%20classifier%20with%20a%20higher%20probability%20detects%20errors%0Aassociated%20with%20irrelevant%20questions%20rather%20than%20incorrect%20query%20generations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence%2520Estimation%2520for%2520Error%2520Detection%2520in%2520Text-to-SQL%2520Systems%26entry.906535625%3DOleg%2520Somov%2520and%2520Elena%2520Tutubalina%26entry.1292438233%3D%2520%2520Text-to-SQL%2520enables%2520users%2520to%2520interact%2520with%2520databases%2520through%2520natural%250Alanguage%252C%2520simplifying%2520the%2520retrieval%2520and%2520synthesis%2520of%2520information.%2520Despite%2520the%250Asuccess%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520converting%2520natural%2520language%250Aquestions%2520into%2520SQL%2520queries%252C%2520their%2520broader%2520adoption%2520is%2520limited%2520by%2520two%2520main%250Achallenges%253A%2520achieving%2520robust%2520generalization%2520across%2520diverse%2520queries%2520and%2520ensuring%250Ainterpretative%2520confidence%2520in%2520their%2520predictions.%2520To%2520tackle%2520these%2520issues%252C%2520our%250Aresearch%2520investigates%2520the%2520integration%2520of%2520selective%2520classifiers%2520into%2520Text-to-SQL%250Asystems.%2520We%2520analyse%2520the%2520trade-off%2520between%2520coverage%2520and%2520risk%2520using%2520entropy%2520based%250Aconfidence%2520estimation%2520with%2520selective%2520classifiers%2520and%2520assess%2520its%2520impact%2520on%2520the%250Aoverall%2520performance%2520of%2520Text-to-SQL%2520models.%2520Additionally%252C%2520we%2520explore%2520the%2520models%2527%250Ainitial%2520calibration%2520and%2520improve%2520it%2520with%2520calibration%2520techniques%2520for%2520better%2520model%250Aalignment%2520between%2520confidence%2520and%2520accuracy.%2520Our%2520experimental%2520results%2520show%2520that%250Aencoder-decoder%2520T5%2520is%2520better%2520calibrated%2520than%2520in-context-learning%2520GPT%25204%2520and%250Adecoder-only%2520Llama%25203%252C%2520thus%2520the%2520designated%2520external%2520entropy-based%2520selective%250Aclassifier%2520has%2520better%2520performance.%2520The%2520study%2520also%2520reveal%2520that%252C%2520in%2520terms%2520of%250Aerror%2520detection%252C%2520selective%2520classifier%2520with%2520a%2520higher%2520probability%2520detects%2520errors%250Aassociated%2520with%2520irrelevant%2520questions%2520rather%2520than%2520incorrect%2520query%2520generations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence%20Estimation%20for%20Error%20Detection%20in%20Text-to-SQL%20Systems&entry.906535625=Oleg%20Somov%20and%20Elena%20Tutubalina&entry.1292438233=%20%20Text-to-SQL%20enables%20users%20to%20interact%20with%20databases%20through%20natural%0Alanguage%2C%20simplifying%20the%20retrieval%20and%20synthesis%20of%20information.%20Despite%20the%0Asuccess%20of%20large%20language%20models%20%28LLMs%29%20in%20converting%20natural%20language%0Aquestions%20into%20SQL%20queries%2C%20their%20broader%20adoption%20is%20limited%20by%20two%20main%0Achallenges%3A%20achieving%20robust%20generalization%20across%20diverse%20queries%20and%20ensuring%0Ainterpretative%20confidence%20in%20their%20predictions.%20To%20tackle%20these%20issues%2C%20our%0Aresearch%20investigates%20the%20integration%20of%20selective%20classifiers%20into%20Text-to-SQL%0Asystems.%20We%20analyse%20the%20trade-off%20between%20coverage%20and%20risk%20using%20entropy%20based%0Aconfidence%20estimation%20with%20selective%20classifiers%20and%20assess%20its%20impact%20on%20the%0Aoverall%20performance%20of%20Text-to-SQL%20models.%20Additionally%2C%20we%20explore%20the%20models%27%0Ainitial%20calibration%20and%20improve%20it%20with%20calibration%20techniques%20for%20better%20model%0Aalignment%20between%20confidence%20and%20accuracy.%20Our%20experimental%20results%20show%20that%0Aencoder-decoder%20T5%20is%20better%20calibrated%20than%20in-context-learning%20GPT%204%20and%0Adecoder-only%20Llama%203%2C%20thus%20the%20designated%20external%20entropy-based%20selective%0Aclassifier%20has%20better%20performance.%20The%20study%20also%20reveal%20that%2C%20in%20terms%20of%0Aerror%20detection%2C%20selective%20classifier%20with%20a%20higher%20probability%20detects%20errors%0Aassociated%20with%20irrelevant%20questions%20rather%20than%20incorrect%20query%20generations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09527v1&entry.124074799=Read"},
{"title": "Reducing the Sensitivity of Neural Physics Simulators to Mesh Topology\n  via Pretraining", "author": "Nathan Vaska and Justin Goodwin and Robin Walters and Rajmonda S. Caceres", "abstract": "  Meshes are used to represent complex objects in high fidelity physics\nsimulators across a variety of domains, such as radar sensing and aerodynamics.\nThere is growing interest in using neural networks to accelerate physics\nsimulations, and also a growing body of work on applying neural networks\ndirectly to irregular mesh data. Since multiple mesh topologies can represent\nthe same object, mesh augmentation is typically required to handle topological\nvariation when training neural networks. Due to the sensitivity of physics\nsimulators to small changes in mesh shape, it is challenging to use these\naugmentations when training neural network-based physics simulators. In this\nwork, we show that variations in mesh topology can significantly reduce the\nperformance of neural network simulators. We evaluate whether pretraining can\nbe used to address this issue, and find that employing an established\nautoencoder pretraining technique with graph embedding models reduces the\nsensitivity of neural network simulators to variations in mesh topology.\nFinally, we highlight future research directions that may further reduce neural\nsimulator sensitivity to mesh topology.\n", "link": "http://arxiv.org/abs/2501.09597v1", "date": "2025-01-16", "relevancy": 1.9293, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5276}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4653}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reducing%20the%20Sensitivity%20of%20Neural%20Physics%20Simulators%20to%20Mesh%20Topology%0A%20%20via%20Pretraining&body=Title%3A%20Reducing%20the%20Sensitivity%20of%20Neural%20Physics%20Simulators%20to%20Mesh%20Topology%0A%20%20via%20Pretraining%0AAuthor%3A%20Nathan%20Vaska%20and%20Justin%20Goodwin%20and%20Robin%20Walters%20and%20Rajmonda%20S.%20Caceres%0AAbstract%3A%20%20%20Meshes%20are%20used%20to%20represent%20complex%20objects%20in%20high%20fidelity%20physics%0Asimulators%20across%20a%20variety%20of%20domains%2C%20such%20as%20radar%20sensing%20and%20aerodynamics.%0AThere%20is%20growing%20interest%20in%20using%20neural%20networks%20to%20accelerate%20physics%0Asimulations%2C%20and%20also%20a%20growing%20body%20of%20work%20on%20applying%20neural%20networks%0Adirectly%20to%20irregular%20mesh%20data.%20Since%20multiple%20mesh%20topologies%20can%20represent%0Athe%20same%20object%2C%20mesh%20augmentation%20is%20typically%20required%20to%20handle%20topological%0Avariation%20when%20training%20neural%20networks.%20Due%20to%20the%20sensitivity%20of%20physics%0Asimulators%20to%20small%20changes%20in%20mesh%20shape%2C%20it%20is%20challenging%20to%20use%20these%0Aaugmentations%20when%20training%20neural%20network-based%20physics%20simulators.%20In%20this%0Awork%2C%20we%20show%20that%20variations%20in%20mesh%20topology%20can%20significantly%20reduce%20the%0Aperformance%20of%20neural%20network%20simulators.%20We%20evaluate%20whether%20pretraining%20can%0Abe%20used%20to%20address%20this%20issue%2C%20and%20find%20that%20employing%20an%20established%0Aautoencoder%20pretraining%20technique%20with%20graph%20embedding%20models%20reduces%20the%0Asensitivity%20of%20neural%20network%20simulators%20to%20variations%20in%20mesh%20topology.%0AFinally%2C%20we%20highlight%20future%20research%20directions%20that%20may%20further%20reduce%20neural%0Asimulator%20sensitivity%20to%20mesh%20topology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReducing%2520the%2520Sensitivity%2520of%2520Neural%2520Physics%2520Simulators%2520to%2520Mesh%2520Topology%250A%2520%2520via%2520Pretraining%26entry.906535625%3DNathan%2520Vaska%2520and%2520Justin%2520Goodwin%2520and%2520Robin%2520Walters%2520and%2520Rajmonda%2520S.%2520Caceres%26entry.1292438233%3D%2520%2520Meshes%2520are%2520used%2520to%2520represent%2520complex%2520objects%2520in%2520high%2520fidelity%2520physics%250Asimulators%2520across%2520a%2520variety%2520of%2520domains%252C%2520such%2520as%2520radar%2520sensing%2520and%2520aerodynamics.%250AThere%2520is%2520growing%2520interest%2520in%2520using%2520neural%2520networks%2520to%2520accelerate%2520physics%250Asimulations%252C%2520and%2520also%2520a%2520growing%2520body%2520of%2520work%2520on%2520applying%2520neural%2520networks%250Adirectly%2520to%2520irregular%2520mesh%2520data.%2520Since%2520multiple%2520mesh%2520topologies%2520can%2520represent%250Athe%2520same%2520object%252C%2520mesh%2520augmentation%2520is%2520typically%2520required%2520to%2520handle%2520topological%250Avariation%2520when%2520training%2520neural%2520networks.%2520Due%2520to%2520the%2520sensitivity%2520of%2520physics%250Asimulators%2520to%2520small%2520changes%2520in%2520mesh%2520shape%252C%2520it%2520is%2520challenging%2520to%2520use%2520these%250Aaugmentations%2520when%2520training%2520neural%2520network-based%2520physics%2520simulators.%2520In%2520this%250Awork%252C%2520we%2520show%2520that%2520variations%2520in%2520mesh%2520topology%2520can%2520significantly%2520reduce%2520the%250Aperformance%2520of%2520neural%2520network%2520simulators.%2520We%2520evaluate%2520whether%2520pretraining%2520can%250Abe%2520used%2520to%2520address%2520this%2520issue%252C%2520and%2520find%2520that%2520employing%2520an%2520established%250Aautoencoder%2520pretraining%2520technique%2520with%2520graph%2520embedding%2520models%2520reduces%2520the%250Asensitivity%2520of%2520neural%2520network%2520simulators%2520to%2520variations%2520in%2520mesh%2520topology.%250AFinally%252C%2520we%2520highlight%2520future%2520research%2520directions%2520that%2520may%2520further%2520reduce%2520neural%250Asimulator%2520sensitivity%2520to%2520mesh%2520topology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20the%20Sensitivity%20of%20Neural%20Physics%20Simulators%20to%20Mesh%20Topology%0A%20%20via%20Pretraining&entry.906535625=Nathan%20Vaska%20and%20Justin%20Goodwin%20and%20Robin%20Walters%20and%20Rajmonda%20S.%20Caceres&entry.1292438233=%20%20Meshes%20are%20used%20to%20represent%20complex%20objects%20in%20high%20fidelity%20physics%0Asimulators%20across%20a%20variety%20of%20domains%2C%20such%20as%20radar%20sensing%20and%20aerodynamics.%0AThere%20is%20growing%20interest%20in%20using%20neural%20networks%20to%20accelerate%20physics%0Asimulations%2C%20and%20also%20a%20growing%20body%20of%20work%20on%20applying%20neural%20networks%0Adirectly%20to%20irregular%20mesh%20data.%20Since%20multiple%20mesh%20topologies%20can%20represent%0Athe%20same%20object%2C%20mesh%20augmentation%20is%20typically%20required%20to%20handle%20topological%0Avariation%20when%20training%20neural%20networks.%20Due%20to%20the%20sensitivity%20of%20physics%0Asimulators%20to%20small%20changes%20in%20mesh%20shape%2C%20it%20is%20challenging%20to%20use%20these%0Aaugmentations%20when%20training%20neural%20network-based%20physics%20simulators.%20In%20this%0Awork%2C%20we%20show%20that%20variations%20in%20mesh%20topology%20can%20significantly%20reduce%20the%0Aperformance%20of%20neural%20network%20simulators.%20We%20evaluate%20whether%20pretraining%20can%0Abe%20used%20to%20address%20this%20issue%2C%20and%20find%20that%20employing%20an%20established%0Aautoencoder%20pretraining%20technique%20with%20graph%20embedding%20models%20reduces%20the%0Asensitivity%20of%20neural%20network%20simulators%20to%20variations%20in%20mesh%20topology.%0AFinally%2C%20we%20highlight%20future%20research%20directions%20that%20may%20further%20reduce%20neural%0Asimulator%20sensitivity%20to%20mesh%20topology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09597v1&entry.124074799=Read"},
{"title": "A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and\n  Mitigation Strategy", "author": "Huandong Wang and Wenjie Fu and Yingzhou Tang and Zhilong Chen and Yuxi Huang and Jinghua Piao and Chen Gao and Fengli Xu and Tao Jiang and Yong Li", "abstract": "  While large language models (LLMs) present significant potential for\nsupporting numerous real-world applications and delivering positive social\nimpacts, they still face significant challenges in terms of the inherent risk\nof privacy leakage, hallucinated outputs, and value misalignment, and can be\nmaliciously used for generating toxic content and unethical purposes after been\njailbroken. Therefore, in this survey, we present a comprehensive review of\nrecent advancements aimed at mitigating these issues, organized across the four\nphases of LLM development and usage: data collecting and pre-training,\nfine-tuning and alignment, prompting and reasoning, and post-processing and\nauditing. We elaborate on the recent advances for enhancing the performance of\nLLMs in terms of privacy protection, hallucination reduction, value alignment,\ntoxicity elimination, and jailbreak defenses. In contrast to previous surveys\nthat focus on a single dimension of responsible LLMs, this survey presents a\nunified framework that encompasses these diverse dimensions, providing a\ncomprehensive view of enhancing LLMs to better serve real-world applications.\n", "link": "http://arxiv.org/abs/2501.09431v1", "date": "2025-01-16", "relevancy": 1.9211, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5076}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4786}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Responsible%20LLMs%3A%20Inherent%20Risk%2C%20Malicious%20Use%2C%20and%0A%20%20Mitigation%20Strategy&body=Title%3A%20A%20Survey%20on%20Responsible%20LLMs%3A%20Inherent%20Risk%2C%20Malicious%20Use%2C%20and%0A%20%20Mitigation%20Strategy%0AAuthor%3A%20Huandong%20Wang%20and%20Wenjie%20Fu%20and%20Yingzhou%20Tang%20and%20Zhilong%20Chen%20and%20Yuxi%20Huang%20and%20Jinghua%20Piao%20and%20Chen%20Gao%20and%20Fengli%20Xu%20and%20Tao%20Jiang%20and%20Yong%20Li%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20present%20significant%20potential%20for%0Asupporting%20numerous%20real-world%20applications%20and%20delivering%20positive%20social%0Aimpacts%2C%20they%20still%20face%20significant%20challenges%20in%20terms%20of%20the%20inherent%20risk%0Aof%20privacy%20leakage%2C%20hallucinated%20outputs%2C%20and%20value%20misalignment%2C%20and%20can%20be%0Amaliciously%20used%20for%20generating%20toxic%20content%20and%20unethical%20purposes%20after%20been%0Ajailbroken.%20Therefore%2C%20in%20this%20survey%2C%20we%20present%20a%20comprehensive%20review%20of%0Arecent%20advancements%20aimed%20at%20mitigating%20these%20issues%2C%20organized%20across%20the%20four%0Aphases%20of%20LLM%20development%20and%20usage%3A%20data%20collecting%20and%20pre-training%2C%0Afine-tuning%20and%20alignment%2C%20prompting%20and%20reasoning%2C%20and%20post-processing%20and%0Aauditing.%20We%20elaborate%20on%20the%20recent%20advances%20for%20enhancing%20the%20performance%20of%0ALLMs%20in%20terms%20of%20privacy%20protection%2C%20hallucination%20reduction%2C%20value%20alignment%2C%0Atoxicity%20elimination%2C%20and%20jailbreak%20defenses.%20In%20contrast%20to%20previous%20surveys%0Athat%20focus%20on%20a%20single%20dimension%20of%20responsible%20LLMs%2C%20this%20survey%20presents%20a%0Aunified%20framework%20that%20encompasses%20these%20diverse%20dimensions%2C%20providing%20a%0Acomprehensive%20view%20of%20enhancing%20LLMs%20to%20better%20serve%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Responsible%2520LLMs%253A%2520Inherent%2520Risk%252C%2520Malicious%2520Use%252C%2520and%250A%2520%2520Mitigation%2520Strategy%26entry.906535625%3DHuandong%2520Wang%2520and%2520Wenjie%2520Fu%2520and%2520Yingzhou%2520Tang%2520and%2520Zhilong%2520Chen%2520and%2520Yuxi%2520Huang%2520and%2520Jinghua%2520Piao%2520and%2520Chen%2520Gao%2520and%2520Fengli%2520Xu%2520and%2520Tao%2520Jiang%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520present%2520significant%2520potential%2520for%250Asupporting%2520numerous%2520real-world%2520applications%2520and%2520delivering%2520positive%2520social%250Aimpacts%252C%2520they%2520still%2520face%2520significant%2520challenges%2520in%2520terms%2520of%2520the%2520inherent%2520risk%250Aof%2520privacy%2520leakage%252C%2520hallucinated%2520outputs%252C%2520and%2520value%2520misalignment%252C%2520and%2520can%2520be%250Amaliciously%2520used%2520for%2520generating%2520toxic%2520content%2520and%2520unethical%2520purposes%2520after%2520been%250Ajailbroken.%2520Therefore%252C%2520in%2520this%2520survey%252C%2520we%2520present%2520a%2520comprehensive%2520review%2520of%250Arecent%2520advancements%2520aimed%2520at%2520mitigating%2520these%2520issues%252C%2520organized%2520across%2520the%2520four%250Aphases%2520of%2520LLM%2520development%2520and%2520usage%253A%2520data%2520collecting%2520and%2520pre-training%252C%250Afine-tuning%2520and%2520alignment%252C%2520prompting%2520and%2520reasoning%252C%2520and%2520post-processing%2520and%250Aauditing.%2520We%2520elaborate%2520on%2520the%2520recent%2520advances%2520for%2520enhancing%2520the%2520performance%2520of%250ALLMs%2520in%2520terms%2520of%2520privacy%2520protection%252C%2520hallucination%2520reduction%252C%2520value%2520alignment%252C%250Atoxicity%2520elimination%252C%2520and%2520jailbreak%2520defenses.%2520In%2520contrast%2520to%2520previous%2520surveys%250Athat%2520focus%2520on%2520a%2520single%2520dimension%2520of%2520responsible%2520LLMs%252C%2520this%2520survey%2520presents%2520a%250Aunified%2520framework%2520that%2520encompasses%2520these%2520diverse%2520dimensions%252C%2520providing%2520a%250Acomprehensive%2520view%2520of%2520enhancing%2520LLMs%2520to%2520better%2520serve%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Responsible%20LLMs%3A%20Inherent%20Risk%2C%20Malicious%20Use%2C%20and%0A%20%20Mitigation%20Strategy&entry.906535625=Huandong%20Wang%20and%20Wenjie%20Fu%20and%20Yingzhou%20Tang%20and%20Zhilong%20Chen%20and%20Yuxi%20Huang%20and%20Jinghua%20Piao%20and%20Chen%20Gao%20and%20Fengli%20Xu%20and%20Tao%20Jiang%20and%20Yong%20Li&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20present%20significant%20potential%20for%0Asupporting%20numerous%20real-world%20applications%20and%20delivering%20positive%20social%0Aimpacts%2C%20they%20still%20face%20significant%20challenges%20in%20terms%20of%20the%20inherent%20risk%0Aof%20privacy%20leakage%2C%20hallucinated%20outputs%2C%20and%20value%20misalignment%2C%20and%20can%20be%0Amaliciously%20used%20for%20generating%20toxic%20content%20and%20unethical%20purposes%20after%20been%0Ajailbroken.%20Therefore%2C%20in%20this%20survey%2C%20we%20present%20a%20comprehensive%20review%20of%0Arecent%20advancements%20aimed%20at%20mitigating%20these%20issues%2C%20organized%20across%20the%20four%0Aphases%20of%20LLM%20development%20and%20usage%3A%20data%20collecting%20and%20pre-training%2C%0Afine-tuning%20and%20alignment%2C%20prompting%20and%20reasoning%2C%20and%20post-processing%20and%0Aauditing.%20We%20elaborate%20on%20the%20recent%20advances%20for%20enhancing%20the%20performance%20of%0ALLMs%20in%20terms%20of%20privacy%20protection%2C%20hallucination%20reduction%2C%20value%20alignment%2C%0Atoxicity%20elimination%2C%20and%20jailbreak%20defenses.%20In%20contrast%20to%20previous%20surveys%0Athat%20focus%20on%20a%20single%20dimension%20of%20responsible%20LLMs%2C%20this%20survey%20presents%20a%0Aunified%20framework%20that%20encompasses%20these%20diverse%20dimensions%2C%20providing%20a%0Acomprehensive%20view%20of%20enhancing%20LLMs%20to%20better%20serve%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09431v1&entry.124074799=Read"},
{"title": "PIER: A Novel Metric for Evaluating What Matters in Code-Switching", "author": "Enes Yavuz Ugan and Ngoc-Quan Pham and Leonard B\u00e4rmann and Alex Waibel", "abstract": "  Code-switching, the alternation of languages within a single discourse,\npresents a significant challenge for Automatic Speech Recognition. Despite the\nunique nature of the task, performance is commonly measured with established\nmetrics such as Word-Error-Rate (WER). However, in this paper, we question\nwhether these general metrics accurately assess performance on code-switching.\nSpecifically, using both Connectionist-Temporal-Classification and\nEncoder-Decoder models, we show fine-tuning on non-code-switched data from both\nmatrix and embedded language improves classical metrics on code-switching test\nsets, although actual code-switched words worsen (as expected). Therefore, we\npropose Point-of-Interest Error Rate (PIER), a variant of WER that focuses only\non specific words of interest. We instantiate PIER on code-switched utterances\nand show that this more accurately describes the code-switching performance,\nshowing huge room for improvement in future work. This focused evaluation\nallows for a more precise assessment of model performance, particularly in\nchallenging aspects such as inter-word and intra-word code-switching.\n", "link": "http://arxiv.org/abs/2501.09512v1", "date": "2025-01-16", "relevancy": 1.9082, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4854}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4854}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIER%3A%20A%20Novel%20Metric%20for%20Evaluating%20What%20Matters%20in%20Code-Switching&body=Title%3A%20PIER%3A%20A%20Novel%20Metric%20for%20Evaluating%20What%20Matters%20in%20Code-Switching%0AAuthor%3A%20Enes%20Yavuz%20Ugan%20and%20Ngoc-Quan%20Pham%20and%20Leonard%20B%C3%A4rmann%20and%20Alex%20Waibel%0AAbstract%3A%20%20%20Code-switching%2C%20the%20alternation%20of%20languages%20within%20a%20single%20discourse%2C%0Apresents%20a%20significant%20challenge%20for%20Automatic%20Speech%20Recognition.%20Despite%20the%0Aunique%20nature%20of%20the%20task%2C%20performance%20is%20commonly%20measured%20with%20established%0Ametrics%20such%20as%20Word-Error-Rate%20%28WER%29.%20However%2C%20in%20this%20paper%2C%20we%20question%0Awhether%20these%20general%20metrics%20accurately%20assess%20performance%20on%20code-switching.%0ASpecifically%2C%20using%20both%20Connectionist-Temporal-Classification%20and%0AEncoder-Decoder%20models%2C%20we%20show%20fine-tuning%20on%20non-code-switched%20data%20from%20both%0Amatrix%20and%20embedded%20language%20improves%20classical%20metrics%20on%20code-switching%20test%0Asets%2C%20although%20actual%20code-switched%20words%20worsen%20%28as%20expected%29.%20Therefore%2C%20we%0Apropose%20Point-of-Interest%20Error%20Rate%20%28PIER%29%2C%20a%20variant%20of%20WER%20that%20focuses%20only%0Aon%20specific%20words%20of%20interest.%20We%20instantiate%20PIER%20on%20code-switched%20utterances%0Aand%20show%20that%20this%20more%20accurately%20describes%20the%20code-switching%20performance%2C%0Ashowing%20huge%20room%20for%20improvement%20in%20future%20work.%20This%20focused%20evaluation%0Aallows%20for%20a%20more%20precise%20assessment%20of%20model%20performance%2C%20particularly%20in%0Achallenging%20aspects%20such%20as%20inter-word%20and%20intra-word%20code-switching.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIER%253A%2520A%2520Novel%2520Metric%2520for%2520Evaluating%2520What%2520Matters%2520in%2520Code-Switching%26entry.906535625%3DEnes%2520Yavuz%2520Ugan%2520and%2520Ngoc-Quan%2520Pham%2520and%2520Leonard%2520B%25C3%25A4rmann%2520and%2520Alex%2520Waibel%26entry.1292438233%3D%2520%2520Code-switching%252C%2520the%2520alternation%2520of%2520languages%2520within%2520a%2520single%2520discourse%252C%250Apresents%2520a%2520significant%2520challenge%2520for%2520Automatic%2520Speech%2520Recognition.%2520Despite%2520the%250Aunique%2520nature%2520of%2520the%2520task%252C%2520performance%2520is%2520commonly%2520measured%2520with%2520established%250Ametrics%2520such%2520as%2520Word-Error-Rate%2520%2528WER%2529.%2520However%252C%2520in%2520this%2520paper%252C%2520we%2520question%250Awhether%2520these%2520general%2520metrics%2520accurately%2520assess%2520performance%2520on%2520code-switching.%250ASpecifically%252C%2520using%2520both%2520Connectionist-Temporal-Classification%2520and%250AEncoder-Decoder%2520models%252C%2520we%2520show%2520fine-tuning%2520on%2520non-code-switched%2520data%2520from%2520both%250Amatrix%2520and%2520embedded%2520language%2520improves%2520classical%2520metrics%2520on%2520code-switching%2520test%250Asets%252C%2520although%2520actual%2520code-switched%2520words%2520worsen%2520%2528as%2520expected%2529.%2520Therefore%252C%2520we%250Apropose%2520Point-of-Interest%2520Error%2520Rate%2520%2528PIER%2529%252C%2520a%2520variant%2520of%2520WER%2520that%2520focuses%2520only%250Aon%2520specific%2520words%2520of%2520interest.%2520We%2520instantiate%2520PIER%2520on%2520code-switched%2520utterances%250Aand%2520show%2520that%2520this%2520more%2520accurately%2520describes%2520the%2520code-switching%2520performance%252C%250Ashowing%2520huge%2520room%2520for%2520improvement%2520in%2520future%2520work.%2520This%2520focused%2520evaluation%250Aallows%2520for%2520a%2520more%2520precise%2520assessment%2520of%2520model%2520performance%252C%2520particularly%2520in%250Achallenging%2520aspects%2520such%2520as%2520inter-word%2520and%2520intra-word%2520code-switching.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIER%3A%20A%20Novel%20Metric%20for%20Evaluating%20What%20Matters%20in%20Code-Switching&entry.906535625=Enes%20Yavuz%20Ugan%20and%20Ngoc-Quan%20Pham%20and%20Leonard%20B%C3%A4rmann%20and%20Alex%20Waibel&entry.1292438233=%20%20Code-switching%2C%20the%20alternation%20of%20languages%20within%20a%20single%20discourse%2C%0Apresents%20a%20significant%20challenge%20for%20Automatic%20Speech%20Recognition.%20Despite%20the%0Aunique%20nature%20of%20the%20task%2C%20performance%20is%20commonly%20measured%20with%20established%0Ametrics%20such%20as%20Word-Error-Rate%20%28WER%29.%20However%2C%20in%20this%20paper%2C%20we%20question%0Awhether%20these%20general%20metrics%20accurately%20assess%20performance%20on%20code-switching.%0ASpecifically%2C%20using%20both%20Connectionist-Temporal-Classification%20and%0AEncoder-Decoder%20models%2C%20we%20show%20fine-tuning%20on%20non-code-switched%20data%20from%20both%0Amatrix%20and%20embedded%20language%20improves%20classical%20metrics%20on%20code-switching%20test%0Asets%2C%20although%20actual%20code-switched%20words%20worsen%20%28as%20expected%29.%20Therefore%2C%20we%0Apropose%20Point-of-Interest%20Error%20Rate%20%28PIER%29%2C%20a%20variant%20of%20WER%20that%20focuses%20only%0Aon%20specific%20words%20of%20interest.%20We%20instantiate%20PIER%20on%20code-switched%20utterances%0Aand%20show%20that%20this%20more%20accurately%20describes%20the%20code-switching%20performance%2C%0Ashowing%20huge%20room%20for%20improvement%20in%20future%20work.%20This%20focused%20evaluation%0Aallows%20for%20a%20more%20precise%20assessment%20of%20model%20performance%2C%20particularly%20in%0Achallenging%20aspects%20such%20as%20inter-word%20and%20intra-word%20code-switching.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09512v1&entry.124074799=Read"},
{"title": "Sines, Transient, Noise Neural Modeling of Piano Notes", "author": "Riccardo Simionato and Stefano Fasciani", "abstract": "  This paper introduces a novel method for emulating piano sounds. We propose\nto exploit the sines, transient, and noise decomposition to design a\ndifferentiable spectral modeling synthesizer replicating piano notes. Three\nsub-modules learn these components from piano recordings and generate the\ncorresponding harmonic, transient, and noise signals. Splitting the emulation\ninto three independently trainable models reduces the modeling tasks'\ncomplexity. The quasi-harmonic content is produced using a differentiable\nsinusoidal model guided by physics-derived formulas, whose parameters are\nautomatically estimated from audio recordings. The noise sub-module uses a\nlearnable time-varying filter, and the transients are generated using a deep\nconvolutional network. From singular notes, we emulate the coupling between\ndifferent keys in trichords with a convolutional-based network. Results show\nthe model matches the partial distribution of the target while predicting the\nenergy in the higher part of the spectrum presents more challenges. The energy\ndistribution in the spectra of the transient and noise components is accurate\noverall. While the model is more computationally and memory efficient,\nperceptual tests reveal limitations in accurately modeling the attack phase of\nnotes. Despite this, it generally achieves perceptual accuracy in emulating\nsingle notes and trichords.\n", "link": "http://arxiv.org/abs/2409.06513v2", "date": "2025-01-16", "relevancy": 1.9067, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4924}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4712}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sines%2C%20Transient%2C%20Noise%20Neural%20Modeling%20of%20Piano%20Notes&body=Title%3A%20Sines%2C%20Transient%2C%20Noise%20Neural%20Modeling%20of%20Piano%20Notes%0AAuthor%3A%20Riccardo%20Simionato%20and%20Stefano%20Fasciani%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20method%20for%20emulating%20piano%20sounds.%20We%20propose%0Ato%20exploit%20the%20sines%2C%20transient%2C%20and%20noise%20decomposition%20to%20design%20a%0Adifferentiable%20spectral%20modeling%20synthesizer%20replicating%20piano%20notes.%20Three%0Asub-modules%20learn%20these%20components%20from%20piano%20recordings%20and%20generate%20the%0Acorresponding%20harmonic%2C%20transient%2C%20and%20noise%20signals.%20Splitting%20the%20emulation%0Ainto%20three%20independently%20trainable%20models%20reduces%20the%20modeling%20tasks%27%0Acomplexity.%20The%20quasi-harmonic%20content%20is%20produced%20using%20a%20differentiable%0Asinusoidal%20model%20guided%20by%20physics-derived%20formulas%2C%20whose%20parameters%20are%0Aautomatically%20estimated%20from%20audio%20recordings.%20The%20noise%20sub-module%20uses%20a%0Alearnable%20time-varying%20filter%2C%20and%20the%20transients%20are%20generated%20using%20a%20deep%0Aconvolutional%20network.%20From%20singular%20notes%2C%20we%20emulate%20the%20coupling%20between%0Adifferent%20keys%20in%20trichords%20with%20a%20convolutional-based%20network.%20Results%20show%0Athe%20model%20matches%20the%20partial%20distribution%20of%20the%20target%20while%20predicting%20the%0Aenergy%20in%20the%20higher%20part%20of%20the%20spectrum%20presents%20more%20challenges.%20The%20energy%0Adistribution%20in%20the%20spectra%20of%20the%20transient%20and%20noise%20components%20is%20accurate%0Aoverall.%20While%20the%20model%20is%20more%20computationally%20and%20memory%20efficient%2C%0Aperceptual%20tests%20reveal%20limitations%20in%20accurately%20modeling%20the%20attack%20phase%20of%0Anotes.%20Despite%20this%2C%20it%20generally%20achieves%20perceptual%20accuracy%20in%20emulating%0Asingle%20notes%20and%20trichords.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06513v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSines%252C%2520Transient%252C%2520Noise%2520Neural%2520Modeling%2520of%2520Piano%2520Notes%26entry.906535625%3DRiccardo%2520Simionato%2520and%2520Stefano%2520Fasciani%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520method%2520for%2520emulating%2520piano%2520sounds.%2520We%2520propose%250Ato%2520exploit%2520the%2520sines%252C%2520transient%252C%2520and%2520noise%2520decomposition%2520to%2520design%2520a%250Adifferentiable%2520spectral%2520modeling%2520synthesizer%2520replicating%2520piano%2520notes.%2520Three%250Asub-modules%2520learn%2520these%2520components%2520from%2520piano%2520recordings%2520and%2520generate%2520the%250Acorresponding%2520harmonic%252C%2520transient%252C%2520and%2520noise%2520signals.%2520Splitting%2520the%2520emulation%250Ainto%2520three%2520independently%2520trainable%2520models%2520reduces%2520the%2520modeling%2520tasks%2527%250Acomplexity.%2520The%2520quasi-harmonic%2520content%2520is%2520produced%2520using%2520a%2520differentiable%250Asinusoidal%2520model%2520guided%2520by%2520physics-derived%2520formulas%252C%2520whose%2520parameters%2520are%250Aautomatically%2520estimated%2520from%2520audio%2520recordings.%2520The%2520noise%2520sub-module%2520uses%2520a%250Alearnable%2520time-varying%2520filter%252C%2520and%2520the%2520transients%2520are%2520generated%2520using%2520a%2520deep%250Aconvolutional%2520network.%2520From%2520singular%2520notes%252C%2520we%2520emulate%2520the%2520coupling%2520between%250Adifferent%2520keys%2520in%2520trichords%2520with%2520a%2520convolutional-based%2520network.%2520Results%2520show%250Athe%2520model%2520matches%2520the%2520partial%2520distribution%2520of%2520the%2520target%2520while%2520predicting%2520the%250Aenergy%2520in%2520the%2520higher%2520part%2520of%2520the%2520spectrum%2520presents%2520more%2520challenges.%2520The%2520energy%250Adistribution%2520in%2520the%2520spectra%2520of%2520the%2520transient%2520and%2520noise%2520components%2520is%2520accurate%250Aoverall.%2520While%2520the%2520model%2520is%2520more%2520computationally%2520and%2520memory%2520efficient%252C%250Aperceptual%2520tests%2520reveal%2520limitations%2520in%2520accurately%2520modeling%2520the%2520attack%2520phase%2520of%250Anotes.%2520Despite%2520this%252C%2520it%2520generally%2520achieves%2520perceptual%2520accuracy%2520in%2520emulating%250Asingle%2520notes%2520and%2520trichords.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06513v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sines%2C%20Transient%2C%20Noise%20Neural%20Modeling%20of%20Piano%20Notes&entry.906535625=Riccardo%20Simionato%20and%20Stefano%20Fasciani&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20method%20for%20emulating%20piano%20sounds.%20We%20propose%0Ato%20exploit%20the%20sines%2C%20transient%2C%20and%20noise%20decomposition%20to%20design%20a%0Adifferentiable%20spectral%20modeling%20synthesizer%20replicating%20piano%20notes.%20Three%0Asub-modules%20learn%20these%20components%20from%20piano%20recordings%20and%20generate%20the%0Acorresponding%20harmonic%2C%20transient%2C%20and%20noise%20signals.%20Splitting%20the%20emulation%0Ainto%20three%20independently%20trainable%20models%20reduces%20the%20modeling%20tasks%27%0Acomplexity.%20The%20quasi-harmonic%20content%20is%20produced%20using%20a%20differentiable%0Asinusoidal%20model%20guided%20by%20physics-derived%20formulas%2C%20whose%20parameters%20are%0Aautomatically%20estimated%20from%20audio%20recordings.%20The%20noise%20sub-module%20uses%20a%0Alearnable%20time-varying%20filter%2C%20and%20the%20transients%20are%20generated%20using%20a%20deep%0Aconvolutional%20network.%20From%20singular%20notes%2C%20we%20emulate%20the%20coupling%20between%0Adifferent%20keys%20in%20trichords%20with%20a%20convolutional-based%20network.%20Results%20show%0Athe%20model%20matches%20the%20partial%20distribution%20of%20the%20target%20while%20predicting%20the%0Aenergy%20in%20the%20higher%20part%20of%20the%20spectrum%20presents%20more%20challenges.%20The%20energy%0Adistribution%20in%20the%20spectra%20of%20the%20transient%20and%20noise%20components%20is%20accurate%0Aoverall.%20While%20the%20model%20is%20more%20computationally%20and%20memory%20efficient%2C%0Aperceptual%20tests%20reveal%20limitations%20in%20accurately%20modeling%20the%20attack%20phase%20of%0Anotes.%20Despite%20this%2C%20it%20generally%20achieves%20perceptual%20accuracy%20in%20emulating%0Asingle%20notes%20and%20trichords.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06513v2&entry.124074799=Read"},
{"title": "CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through\n  Category-Bounding", "author": "Johannes Kirmayr and Lukas Stappen and Phillip Schneider and Florian Matthes and Elisabeth Andr\u00e9", "abstract": "  In today's assistant landscape, personalisation enhances interactions,\nfosters long-term relationships, and deepens engagement. However, many systems\nstruggle with retaining user preferences, leading to repetitive user requests\nand disengagement. Furthermore, the unregulated and opaque extraction of user\npreferences in industry applications raises significant concerns about privacy\nand trust, especially in regions with stringent regulations like Europe. In\nresponse to these challenges, we propose a long-term memory system for voice\nassistants, structured around predefined categories. This approach leverages\nLarge Language Models to efficiently extract, store, and retrieve preferences\nwithin these categories, ensuring both personalisation and transparency. We\nalso introduce a synthetic multi-turn, multi-session conversation dataset\n(CarMem), grounded in real industry data, tailored to an in-car voice assistant\nsetting. Benchmarked on the dataset, our system achieves an F1-score of .78 to\n.95 in preference extraction, depending on category granularity. Our\nmaintenance strategy reduces redundant preferences by 95% and contradictory\nones by 92%, while the accuracy of optimal retrieval is at .87. Collectively,\nthe results demonstrate the system's suitability for industrial applications.\n", "link": "http://arxiv.org/abs/2501.09645v1", "date": "2025-01-16", "relevancy": 1.9057, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4922}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4672}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CarMem%3A%20Enhancing%20Long-Term%20Memory%20in%20LLM%20Voice%20Assistants%20through%0A%20%20Category-Bounding&body=Title%3A%20CarMem%3A%20Enhancing%20Long-Term%20Memory%20in%20LLM%20Voice%20Assistants%20through%0A%20%20Category-Bounding%0AAuthor%3A%20Johannes%20Kirmayr%20and%20Lukas%20Stappen%20and%20Phillip%20Schneider%20and%20Florian%20Matthes%20and%20Elisabeth%20Andr%C3%A9%0AAbstract%3A%20%20%20In%20today%27s%20assistant%20landscape%2C%20personalisation%20enhances%20interactions%2C%0Afosters%20long-term%20relationships%2C%20and%20deepens%20engagement.%20However%2C%20many%20systems%0Astruggle%20with%20retaining%20user%20preferences%2C%20leading%20to%20repetitive%20user%20requests%0Aand%20disengagement.%20Furthermore%2C%20the%20unregulated%20and%20opaque%20extraction%20of%20user%0Apreferences%20in%20industry%20applications%20raises%20significant%20concerns%20about%20privacy%0Aand%20trust%2C%20especially%20in%20regions%20with%20stringent%20regulations%20like%20Europe.%20In%0Aresponse%20to%20these%20challenges%2C%20we%20propose%20a%20long-term%20memory%20system%20for%20voice%0Aassistants%2C%20structured%20around%20predefined%20categories.%20This%20approach%20leverages%0ALarge%20Language%20Models%20to%20efficiently%20extract%2C%20store%2C%20and%20retrieve%20preferences%0Awithin%20these%20categories%2C%20ensuring%20both%20personalisation%20and%20transparency.%20We%0Aalso%20introduce%20a%20synthetic%20multi-turn%2C%20multi-session%20conversation%20dataset%0A%28CarMem%29%2C%20grounded%20in%20real%20industry%20data%2C%20tailored%20to%20an%20in-car%20voice%20assistant%0Asetting.%20Benchmarked%20on%20the%20dataset%2C%20our%20system%20achieves%20an%20F1-score%20of%20.78%20to%0A.95%20in%20preference%20extraction%2C%20depending%20on%20category%20granularity.%20Our%0Amaintenance%20strategy%20reduces%20redundant%20preferences%20by%2095%25%20and%20contradictory%0Aones%20by%2092%25%2C%20while%20the%20accuracy%20of%20optimal%20retrieval%20is%20at%20.87.%20Collectively%2C%0Athe%20results%20demonstrate%20the%20system%27s%20suitability%20for%20industrial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCarMem%253A%2520Enhancing%2520Long-Term%2520Memory%2520in%2520LLM%2520Voice%2520Assistants%2520through%250A%2520%2520Category-Bounding%26entry.906535625%3DJohannes%2520Kirmayr%2520and%2520Lukas%2520Stappen%2520and%2520Phillip%2520Schneider%2520and%2520Florian%2520Matthes%2520and%2520Elisabeth%2520Andr%25C3%25A9%26entry.1292438233%3D%2520%2520In%2520today%2527s%2520assistant%2520landscape%252C%2520personalisation%2520enhances%2520interactions%252C%250Afosters%2520long-term%2520relationships%252C%2520and%2520deepens%2520engagement.%2520However%252C%2520many%2520systems%250Astruggle%2520with%2520retaining%2520user%2520preferences%252C%2520leading%2520to%2520repetitive%2520user%2520requests%250Aand%2520disengagement.%2520Furthermore%252C%2520the%2520unregulated%2520and%2520opaque%2520extraction%2520of%2520user%250Apreferences%2520in%2520industry%2520applications%2520raises%2520significant%2520concerns%2520about%2520privacy%250Aand%2520trust%252C%2520especially%2520in%2520regions%2520with%2520stringent%2520regulations%2520like%2520Europe.%2520In%250Aresponse%2520to%2520these%2520challenges%252C%2520we%2520propose%2520a%2520long-term%2520memory%2520system%2520for%2520voice%250Aassistants%252C%2520structured%2520around%2520predefined%2520categories.%2520This%2520approach%2520leverages%250ALarge%2520Language%2520Models%2520to%2520efficiently%2520extract%252C%2520store%252C%2520and%2520retrieve%2520preferences%250Awithin%2520these%2520categories%252C%2520ensuring%2520both%2520personalisation%2520and%2520transparency.%2520We%250Aalso%2520introduce%2520a%2520synthetic%2520multi-turn%252C%2520multi-session%2520conversation%2520dataset%250A%2528CarMem%2529%252C%2520grounded%2520in%2520real%2520industry%2520data%252C%2520tailored%2520to%2520an%2520in-car%2520voice%2520assistant%250Asetting.%2520Benchmarked%2520on%2520the%2520dataset%252C%2520our%2520system%2520achieves%2520an%2520F1-score%2520of%2520.78%2520to%250A.95%2520in%2520preference%2520extraction%252C%2520depending%2520on%2520category%2520granularity.%2520Our%250Amaintenance%2520strategy%2520reduces%2520redundant%2520preferences%2520by%252095%2525%2520and%2520contradictory%250Aones%2520by%252092%2525%252C%2520while%2520the%2520accuracy%2520of%2520optimal%2520retrieval%2520is%2520at%2520.87.%2520Collectively%252C%250Athe%2520results%2520demonstrate%2520the%2520system%2527s%2520suitability%2520for%2520industrial%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CarMem%3A%20Enhancing%20Long-Term%20Memory%20in%20LLM%20Voice%20Assistants%20through%0A%20%20Category-Bounding&entry.906535625=Johannes%20Kirmayr%20and%20Lukas%20Stappen%20and%20Phillip%20Schneider%20and%20Florian%20Matthes%20and%20Elisabeth%20Andr%C3%A9&entry.1292438233=%20%20In%20today%27s%20assistant%20landscape%2C%20personalisation%20enhances%20interactions%2C%0Afosters%20long-term%20relationships%2C%20and%20deepens%20engagement.%20However%2C%20many%20systems%0Astruggle%20with%20retaining%20user%20preferences%2C%20leading%20to%20repetitive%20user%20requests%0Aand%20disengagement.%20Furthermore%2C%20the%20unregulated%20and%20opaque%20extraction%20of%20user%0Apreferences%20in%20industry%20applications%20raises%20significant%20concerns%20about%20privacy%0Aand%20trust%2C%20especially%20in%20regions%20with%20stringent%20regulations%20like%20Europe.%20In%0Aresponse%20to%20these%20challenges%2C%20we%20propose%20a%20long-term%20memory%20system%20for%20voice%0Aassistants%2C%20structured%20around%20predefined%20categories.%20This%20approach%20leverages%0ALarge%20Language%20Models%20to%20efficiently%20extract%2C%20store%2C%20and%20retrieve%20preferences%0Awithin%20these%20categories%2C%20ensuring%20both%20personalisation%20and%20transparency.%20We%0Aalso%20introduce%20a%20synthetic%20multi-turn%2C%20multi-session%20conversation%20dataset%0A%28CarMem%29%2C%20grounded%20in%20real%20industry%20data%2C%20tailored%20to%20an%20in-car%20voice%20assistant%0Asetting.%20Benchmarked%20on%20the%20dataset%2C%20our%20system%20achieves%20an%20F1-score%20of%20.78%20to%0A.95%20in%20preference%20extraction%2C%20depending%20on%20category%20granularity.%20Our%0Amaintenance%20strategy%20reduces%20redundant%20preferences%20by%2095%25%20and%20contradictory%0Aones%20by%2092%25%2C%20while%20the%20accuracy%20of%20optimal%20retrieval%20is%20at%20.87.%20Collectively%2C%0Athe%20results%20demonstrate%20the%20system%27s%20suitability%20for%20industrial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09645v1&entry.124074799=Read"},
{"title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation\n  Systems", "author": "Robert Friel and Masha Belyi and Atindriyo Sanyal", "abstract": "  Retrieval-Augmented Generation (RAG) has become a standard architectural\npattern for incorporating domain-specific knowledge into user-facing chat\napplications powered by Large Language Models (LLMs). RAG systems are\ncharacterized by (1) a document retriever that queries a domain-specific corpus\nfor context information relevant to an input query, and (2) an LLM that\ngenerates a response based on the provided query and context. However,\ncomprehensive evaluation of RAG systems remains a challenge due to the lack of\nunified evaluation criteria and annotated datasets. In response, we introduce\nRAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k\nexamples. It covers five unique industry-specific domains and various RAG task\ntypes. RAGBench examples are sourced from industry corpora such as user\nmanuals, making it particularly relevant for industry applications. Further, we\nformalize the TRACe evaluation framework: a set of explainable and actionable\nRAG evaluation metrics applicable across all RAG domains. We release the\nlabeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.\nRAGBench explainable labels facilitate holistic evaluation of RAG systems,\nenabling actionable feedback for continuous improvement of production\napplications. Thorough extensive benchmarking, we find that LLM-based RAG\nevaluation methods struggle to compete with a finetuned RoBERTa model on the\nRAG evaluation task. We identify areas where existing approaches fall short and\npropose the adoption of RAGBench with TRACe towards advancing the state of RAG\nevaluation systems.\n", "link": "http://arxiv.org/abs/2407.11005v2", "date": "2025-01-16", "relevancy": 1.9057, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4941}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4747}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAGBench%3A%20Explainable%20Benchmark%20for%20Retrieval-Augmented%20Generation%0A%20%20Systems&body=Title%3A%20RAGBench%3A%20Explainable%20Benchmark%20for%20Retrieval-Augmented%20Generation%0A%20%20Systems%0AAuthor%3A%20Robert%20Friel%20and%20Masha%20Belyi%20and%20Atindriyo%20Sanyal%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20become%20a%20standard%20architectural%0Apattern%20for%20incorporating%20domain-specific%20knowledge%20into%20user-facing%20chat%0Aapplications%20powered%20by%20Large%20Language%20Models%20%28LLMs%29.%20RAG%20systems%20are%0Acharacterized%20by%20%281%29%20a%20document%20retriever%20that%20queries%20a%20domain-specific%20corpus%0Afor%20context%20information%20relevant%20to%20an%20input%20query%2C%20and%20%282%29%20an%20LLM%20that%0Agenerates%20a%20response%20based%20on%20the%20provided%20query%20and%20context.%20However%2C%0Acomprehensive%20evaluation%20of%20RAG%20systems%20remains%20a%20challenge%20due%20to%20the%20lack%20of%0Aunified%20evaluation%20criteria%20and%20annotated%20datasets.%20In%20response%2C%20we%20introduce%0ARAGBench%3A%20the%20first%20comprehensive%2C%20large-scale%20RAG%20benchmark%20dataset%20of%20100k%0Aexamples.%20It%20covers%20five%20unique%20industry-specific%20domains%20and%20various%20RAG%20task%0Atypes.%20RAGBench%20examples%20are%20sourced%20from%20industry%20corpora%20such%20as%20user%0Amanuals%2C%20making%20it%20particularly%20relevant%20for%20industry%20applications.%20Further%2C%20we%0Aformalize%20the%20TRACe%20evaluation%20framework%3A%20a%20set%20of%20explainable%20and%20actionable%0ARAG%20evaluation%20metrics%20applicable%20across%20all%20RAG%20domains.%20We%20release%20the%0Alabeled%20dataset%20at%20https%3A//huggingface.co/datasets/rungalileo/ragbench.%0ARAGBench%20explainable%20labels%20facilitate%20holistic%20evaluation%20of%20RAG%20systems%2C%0Aenabling%20actionable%20feedback%20for%20continuous%20improvement%20of%20production%0Aapplications.%20Thorough%20extensive%20benchmarking%2C%20we%20find%20that%20LLM-based%20RAG%0Aevaluation%20methods%20struggle%20to%20compete%20with%20a%20finetuned%20RoBERTa%20model%20on%20the%0ARAG%20evaluation%20task.%20We%20identify%20areas%20where%20existing%20approaches%20fall%20short%20and%0Apropose%20the%20adoption%20of%20RAGBench%20with%20TRACe%20towards%20advancing%20the%20state%20of%20RAG%0Aevaluation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11005v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAGBench%253A%2520Explainable%2520Benchmark%2520for%2520Retrieval-Augmented%2520Generation%250A%2520%2520Systems%26entry.906535625%3DRobert%2520Friel%2520and%2520Masha%2520Belyi%2520and%2520Atindriyo%2520Sanyal%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520has%2520become%2520a%2520standard%2520architectural%250Apattern%2520for%2520incorporating%2520domain-specific%2520knowledge%2520into%2520user-facing%2520chat%250Aapplications%2520powered%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520RAG%2520systems%2520are%250Acharacterized%2520by%2520%25281%2529%2520a%2520document%2520retriever%2520that%2520queries%2520a%2520domain-specific%2520corpus%250Afor%2520context%2520information%2520relevant%2520to%2520an%2520input%2520query%252C%2520and%2520%25282%2529%2520an%2520LLM%2520that%250Agenerates%2520a%2520response%2520based%2520on%2520the%2520provided%2520query%2520and%2520context.%2520However%252C%250Acomprehensive%2520evaluation%2520of%2520RAG%2520systems%2520remains%2520a%2520challenge%2520due%2520to%2520the%2520lack%2520of%250Aunified%2520evaluation%2520criteria%2520and%2520annotated%2520datasets.%2520In%2520response%252C%2520we%2520introduce%250ARAGBench%253A%2520the%2520first%2520comprehensive%252C%2520large-scale%2520RAG%2520benchmark%2520dataset%2520of%2520100k%250Aexamples.%2520It%2520covers%2520five%2520unique%2520industry-specific%2520domains%2520and%2520various%2520RAG%2520task%250Atypes.%2520RAGBench%2520examples%2520are%2520sourced%2520from%2520industry%2520corpora%2520such%2520as%2520user%250Amanuals%252C%2520making%2520it%2520particularly%2520relevant%2520for%2520industry%2520applications.%2520Further%252C%2520we%250Aformalize%2520the%2520TRACe%2520evaluation%2520framework%253A%2520a%2520set%2520of%2520explainable%2520and%2520actionable%250ARAG%2520evaluation%2520metrics%2520applicable%2520across%2520all%2520RAG%2520domains.%2520We%2520release%2520the%250Alabeled%2520dataset%2520at%2520https%253A//huggingface.co/datasets/rungalileo/ragbench.%250ARAGBench%2520explainable%2520labels%2520facilitate%2520holistic%2520evaluation%2520of%2520RAG%2520systems%252C%250Aenabling%2520actionable%2520feedback%2520for%2520continuous%2520improvement%2520of%2520production%250Aapplications.%2520Thorough%2520extensive%2520benchmarking%252C%2520we%2520find%2520that%2520LLM-based%2520RAG%250Aevaluation%2520methods%2520struggle%2520to%2520compete%2520with%2520a%2520finetuned%2520RoBERTa%2520model%2520on%2520the%250ARAG%2520evaluation%2520task.%2520We%2520identify%2520areas%2520where%2520existing%2520approaches%2520fall%2520short%2520and%250Apropose%2520the%2520adoption%2520of%2520RAGBench%2520with%2520TRACe%2520towards%2520advancing%2520the%2520state%2520of%2520RAG%250Aevaluation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11005v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAGBench%3A%20Explainable%20Benchmark%20for%20Retrieval-Augmented%20Generation%0A%20%20Systems&entry.906535625=Robert%20Friel%20and%20Masha%20Belyi%20and%20Atindriyo%20Sanyal&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20become%20a%20standard%20architectural%0Apattern%20for%20incorporating%20domain-specific%20knowledge%20into%20user-facing%20chat%0Aapplications%20powered%20by%20Large%20Language%20Models%20%28LLMs%29.%20RAG%20systems%20are%0Acharacterized%20by%20%281%29%20a%20document%20retriever%20that%20queries%20a%20domain-specific%20corpus%0Afor%20context%20information%20relevant%20to%20an%20input%20query%2C%20and%20%282%29%20an%20LLM%20that%0Agenerates%20a%20response%20based%20on%20the%20provided%20query%20and%20context.%20However%2C%0Acomprehensive%20evaluation%20of%20RAG%20systems%20remains%20a%20challenge%20due%20to%20the%20lack%20of%0Aunified%20evaluation%20criteria%20and%20annotated%20datasets.%20In%20response%2C%20we%20introduce%0ARAGBench%3A%20the%20first%20comprehensive%2C%20large-scale%20RAG%20benchmark%20dataset%20of%20100k%0Aexamples.%20It%20covers%20five%20unique%20industry-specific%20domains%20and%20various%20RAG%20task%0Atypes.%20RAGBench%20examples%20are%20sourced%20from%20industry%20corpora%20such%20as%20user%0Amanuals%2C%20making%20it%20particularly%20relevant%20for%20industry%20applications.%20Further%2C%20we%0Aformalize%20the%20TRACe%20evaluation%20framework%3A%20a%20set%20of%20explainable%20and%20actionable%0ARAG%20evaluation%20metrics%20applicable%20across%20all%20RAG%20domains.%20We%20release%20the%0Alabeled%20dataset%20at%20https%3A//huggingface.co/datasets/rungalileo/ragbench.%0ARAGBench%20explainable%20labels%20facilitate%20holistic%20evaluation%20of%20RAG%20systems%2C%0Aenabling%20actionable%20feedback%20for%20continuous%20improvement%20of%20production%0Aapplications.%20Thorough%20extensive%20benchmarking%2C%20we%20find%20that%20LLM-based%20RAG%0Aevaluation%20methods%20struggle%20to%20compete%20with%20a%20finetuned%20RoBERTa%20model%20on%20the%0ARAG%20evaluation%20task.%20We%20identify%20areas%20where%20existing%20approaches%20fall%20short%20and%0Apropose%20the%20adoption%20of%20RAGBench%20with%20TRACe%20towards%20advancing%20the%20state%20of%20RAG%0Aevaluation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11005v2&entry.124074799=Read"},
{"title": "CoNav Chair: Design of a ROS-based Smart Wheelchair for Shared Control\n  Navigation in the Built Environment", "author": "Yifan Xu and Qianwei Wang and Jordan Lillie and Vineet Kamat and Carol Menassa", "abstract": "  With the number of people with disabilities (PWD) increasing worldwide each\nyear, the demand for mobility support to enable independent living and social\nintegration is also growing. Wheelchairs commonly support the mobility of PWD\nin both indoor and outdoor environments. However, current powered wheelchairs\n(PWC) often fail to meet the needs of PWD, who may find it difficult to operate\nthem. Furthermore, existing research on robotic wheelchairs typically focuses\neither on full autonomy or enhanced manual control, which can lead to reduced\nefficiency and user trust. To address these issues, this paper proposes a Robot\nOperating System (ROS)-based smart wheelchair, called CoNav Chair, that\nincorporates a shared control navigation algorithm and obstacle avoidance to\nsupport PWD while fostering efficiency and trust between the robot and the\nuser. Our design consists of hardware and software components. Experimental\nresults conducted in a typical indoor social environment demonstrate the\nperformance and effectiveness of the smart wheelchair hardware and software\ndesign. This integrated design promotes trust and autonomy, which are crucial\nfor the acceptance of assistive mobility technologies in the built environment.\n", "link": "http://arxiv.org/abs/2501.09680v1", "date": "2025-01-16", "relevancy": 1.8946, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.7309}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5444}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoNav%20Chair%3A%20Design%20of%20a%20ROS-based%20Smart%20Wheelchair%20for%20Shared%20Control%0A%20%20Navigation%20in%20the%20Built%20Environment&body=Title%3A%20CoNav%20Chair%3A%20Design%20of%20a%20ROS-based%20Smart%20Wheelchair%20for%20Shared%20Control%0A%20%20Navigation%20in%20the%20Built%20Environment%0AAuthor%3A%20Yifan%20Xu%20and%20Qianwei%20Wang%20and%20Jordan%20Lillie%20and%20Vineet%20Kamat%20and%20Carol%20Menassa%0AAbstract%3A%20%20%20With%20the%20number%20of%20people%20with%20disabilities%20%28PWD%29%20increasing%20worldwide%20each%0Ayear%2C%20the%20demand%20for%20mobility%20support%20to%20enable%20independent%20living%20and%20social%0Aintegration%20is%20also%20growing.%20Wheelchairs%20commonly%20support%20the%20mobility%20of%20PWD%0Ain%20both%20indoor%20and%20outdoor%20environments.%20However%2C%20current%20powered%20wheelchairs%0A%28PWC%29%20often%20fail%20to%20meet%20the%20needs%20of%20PWD%2C%20who%20may%20find%20it%20difficult%20to%20operate%0Athem.%20Furthermore%2C%20existing%20research%20on%20robotic%20wheelchairs%20typically%20focuses%0Aeither%20on%20full%20autonomy%20or%20enhanced%20manual%20control%2C%20which%20can%20lead%20to%20reduced%0Aefficiency%20and%20user%20trust.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%20a%20Robot%0AOperating%20System%20%28ROS%29-based%20smart%20wheelchair%2C%20called%20CoNav%20Chair%2C%20that%0Aincorporates%20a%20shared%20control%20navigation%20algorithm%20and%20obstacle%20avoidance%20to%0Asupport%20PWD%20while%20fostering%20efficiency%20and%20trust%20between%20the%20robot%20and%20the%0Auser.%20Our%20design%20consists%20of%20hardware%20and%20software%20components.%20Experimental%0Aresults%20conducted%20in%20a%20typical%20indoor%20social%20environment%20demonstrate%20the%0Aperformance%20and%20effectiveness%20of%20the%20smart%20wheelchair%20hardware%20and%20software%0Adesign.%20This%20integrated%20design%20promotes%20trust%20and%20autonomy%2C%20which%20are%20crucial%0Afor%20the%20acceptance%20of%20assistive%20mobility%20technologies%20in%20the%20built%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoNav%2520Chair%253A%2520Design%2520of%2520a%2520ROS-based%2520Smart%2520Wheelchair%2520for%2520Shared%2520Control%250A%2520%2520Navigation%2520in%2520the%2520Built%2520Environment%26entry.906535625%3DYifan%2520Xu%2520and%2520Qianwei%2520Wang%2520and%2520Jordan%2520Lillie%2520and%2520Vineet%2520Kamat%2520and%2520Carol%2520Menassa%26entry.1292438233%3D%2520%2520With%2520the%2520number%2520of%2520people%2520with%2520disabilities%2520%2528PWD%2529%2520increasing%2520worldwide%2520each%250Ayear%252C%2520the%2520demand%2520for%2520mobility%2520support%2520to%2520enable%2520independent%2520living%2520and%2520social%250Aintegration%2520is%2520also%2520growing.%2520Wheelchairs%2520commonly%2520support%2520the%2520mobility%2520of%2520PWD%250Ain%2520both%2520indoor%2520and%2520outdoor%2520environments.%2520However%252C%2520current%2520powered%2520wheelchairs%250A%2528PWC%2529%2520often%2520fail%2520to%2520meet%2520the%2520needs%2520of%2520PWD%252C%2520who%2520may%2520find%2520it%2520difficult%2520to%2520operate%250Athem.%2520Furthermore%252C%2520existing%2520research%2520on%2520robotic%2520wheelchairs%2520typically%2520focuses%250Aeither%2520on%2520full%2520autonomy%2520or%2520enhanced%2520manual%2520control%252C%2520which%2520can%2520lead%2520to%2520reduced%250Aefficiency%2520and%2520user%2520trust.%2520To%2520address%2520these%2520issues%252C%2520this%2520paper%2520proposes%2520a%2520Robot%250AOperating%2520System%2520%2528ROS%2529-based%2520smart%2520wheelchair%252C%2520called%2520CoNav%2520Chair%252C%2520that%250Aincorporates%2520a%2520shared%2520control%2520navigation%2520algorithm%2520and%2520obstacle%2520avoidance%2520to%250Asupport%2520PWD%2520while%2520fostering%2520efficiency%2520and%2520trust%2520between%2520the%2520robot%2520and%2520the%250Auser.%2520Our%2520design%2520consists%2520of%2520hardware%2520and%2520software%2520components.%2520Experimental%250Aresults%2520conducted%2520in%2520a%2520typical%2520indoor%2520social%2520environment%2520demonstrate%2520the%250Aperformance%2520and%2520effectiveness%2520of%2520the%2520smart%2520wheelchair%2520hardware%2520and%2520software%250Adesign.%2520This%2520integrated%2520design%2520promotes%2520trust%2520and%2520autonomy%252C%2520which%2520are%2520crucial%250Afor%2520the%2520acceptance%2520of%2520assistive%2520mobility%2520technologies%2520in%2520the%2520built%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoNav%20Chair%3A%20Design%20of%20a%20ROS-based%20Smart%20Wheelchair%20for%20Shared%20Control%0A%20%20Navigation%20in%20the%20Built%20Environment&entry.906535625=Yifan%20Xu%20and%20Qianwei%20Wang%20and%20Jordan%20Lillie%20and%20Vineet%20Kamat%20and%20Carol%20Menassa&entry.1292438233=%20%20With%20the%20number%20of%20people%20with%20disabilities%20%28PWD%29%20increasing%20worldwide%20each%0Ayear%2C%20the%20demand%20for%20mobility%20support%20to%20enable%20independent%20living%20and%20social%0Aintegration%20is%20also%20growing.%20Wheelchairs%20commonly%20support%20the%20mobility%20of%20PWD%0Ain%20both%20indoor%20and%20outdoor%20environments.%20However%2C%20current%20powered%20wheelchairs%0A%28PWC%29%20often%20fail%20to%20meet%20the%20needs%20of%20PWD%2C%20who%20may%20find%20it%20difficult%20to%20operate%0Athem.%20Furthermore%2C%20existing%20research%20on%20robotic%20wheelchairs%20typically%20focuses%0Aeither%20on%20full%20autonomy%20or%20enhanced%20manual%20control%2C%20which%20can%20lead%20to%20reduced%0Aefficiency%20and%20user%20trust.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%20a%20Robot%0AOperating%20System%20%28ROS%29-based%20smart%20wheelchair%2C%20called%20CoNav%20Chair%2C%20that%0Aincorporates%20a%20shared%20control%20navigation%20algorithm%20and%20obstacle%20avoidance%20to%0Asupport%20PWD%20while%20fostering%20efficiency%20and%20trust%20between%20the%20robot%20and%20the%0Auser.%20Our%20design%20consists%20of%20hardware%20and%20software%20components.%20Experimental%0Aresults%20conducted%20in%20a%20typical%20indoor%20social%20environment%20demonstrate%20the%0Aperformance%20and%20effectiveness%20of%20the%20smart%20wheelchair%20hardware%20and%20software%0Adesign.%20This%20integrated%20design%20promotes%20trust%20and%20autonomy%2C%20which%20are%20crucial%0Afor%20the%20acceptance%20of%20assistive%20mobility%20technologies%20in%20the%20built%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09680v1&entry.124074799=Read"},
{"title": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design", "author": "Zhi Zheng and Zhuoliang Xie and Zhenkun Wang and Bryan Hooi", "abstract": "  Handcrafting heuristics for solving complex planning tasks (e.g., NP-hard\ncombinatorial optimization (CO) problems) is a common practice but requires\nextensive domain knowledge. Recently, Large Language Model (LLM)-based\nautomatic heuristics design (AHD) methods have shown promise in generating\nhigh-quality heuristics without manual intervention. Existing LLM-based AHD\nmethods employ a population to maintain a fixed number of top-performing\nLLM-generated heuristics and introduce evolutionary computation (EC) to enhance\nthe population iteratively. However, the population-based procedure brings\ngreedy properties, often resulting in convergence to local optima. Instead, to\nmore comprehensively explore the space of heuristics, we propose using Monte\nCarlo Tree Search (MCTS) for LLM-based heuristic evolution while preserving all\nLLM-generated heuristics in a tree structure. With a novel thought-alignment\nprocess and an exploration-decay technique, the proposed MCTS-AHD method\ndelivers significantly higher-quality heuristics on various complex tasks. Our\ncode is available at https://github.com/zz1358m/MCTS-AHD-master.\n", "link": "http://arxiv.org/abs/2501.08603v2", "date": "2025-01-16", "relevancy": 1.8803, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5468}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4594}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monte%20Carlo%20Tree%20Search%20for%20Comprehensive%20Exploration%20in%20LLM-Based%0A%20%20Automatic%20Heuristic%20Design&body=Title%3A%20Monte%20Carlo%20Tree%20Search%20for%20Comprehensive%20Exploration%20in%20LLM-Based%0A%20%20Automatic%20Heuristic%20Design%0AAuthor%3A%20Zhi%20Zheng%20and%20Zhuoliang%20Xie%20and%20Zhenkun%20Wang%20and%20Bryan%20Hooi%0AAbstract%3A%20%20%20Handcrafting%20heuristics%20for%20solving%20complex%20planning%20tasks%20%28e.g.%2C%20NP-hard%0Acombinatorial%20optimization%20%28CO%29%20problems%29%20is%20a%20common%20practice%20but%20requires%0Aextensive%20domain%20knowledge.%20Recently%2C%20Large%20Language%20Model%20%28LLM%29-based%0Aautomatic%20heuristics%20design%20%28AHD%29%20methods%20have%20shown%20promise%20in%20generating%0Ahigh-quality%20heuristics%20without%20manual%20intervention.%20Existing%20LLM-based%20AHD%0Amethods%20employ%20a%20population%20to%20maintain%20a%20fixed%20number%20of%20top-performing%0ALLM-generated%20heuristics%20and%20introduce%20evolutionary%20computation%20%28EC%29%20to%20enhance%0Athe%20population%20iteratively.%20However%2C%20the%20population-based%20procedure%20brings%0Agreedy%20properties%2C%20often%20resulting%20in%20convergence%20to%20local%20optima.%20Instead%2C%20to%0Amore%20comprehensively%20explore%20the%20space%20of%20heuristics%2C%20we%20propose%20using%20Monte%0ACarlo%20Tree%20Search%20%28MCTS%29%20for%20LLM-based%20heuristic%20evolution%20while%20preserving%20all%0ALLM-generated%20heuristics%20in%20a%20tree%20structure.%20With%20a%20novel%20thought-alignment%0Aprocess%20and%20an%20exploration-decay%20technique%2C%20the%20proposed%20MCTS-AHD%20method%0Adelivers%20significantly%20higher-quality%20heuristics%20on%20various%20complex%20tasks.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/zz1358m/MCTS-AHD-master.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08603v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonte%2520Carlo%2520Tree%2520Search%2520for%2520Comprehensive%2520Exploration%2520in%2520LLM-Based%250A%2520%2520Automatic%2520Heuristic%2520Design%26entry.906535625%3DZhi%2520Zheng%2520and%2520Zhuoliang%2520Xie%2520and%2520Zhenkun%2520Wang%2520and%2520Bryan%2520Hooi%26entry.1292438233%3D%2520%2520Handcrafting%2520heuristics%2520for%2520solving%2520complex%2520planning%2520tasks%2520%2528e.g.%252C%2520NP-hard%250Acombinatorial%2520optimization%2520%2528CO%2529%2520problems%2529%2520is%2520a%2520common%2520practice%2520but%2520requires%250Aextensive%2520domain%2520knowledge.%2520Recently%252C%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%250Aautomatic%2520heuristics%2520design%2520%2528AHD%2529%2520methods%2520have%2520shown%2520promise%2520in%2520generating%250Ahigh-quality%2520heuristics%2520without%2520manual%2520intervention.%2520Existing%2520LLM-based%2520AHD%250Amethods%2520employ%2520a%2520population%2520to%2520maintain%2520a%2520fixed%2520number%2520of%2520top-performing%250ALLM-generated%2520heuristics%2520and%2520introduce%2520evolutionary%2520computation%2520%2528EC%2529%2520to%2520enhance%250Athe%2520population%2520iteratively.%2520However%252C%2520the%2520population-based%2520procedure%2520brings%250Agreedy%2520properties%252C%2520often%2520resulting%2520in%2520convergence%2520to%2520local%2520optima.%2520Instead%252C%2520to%250Amore%2520comprehensively%2520explore%2520the%2520space%2520of%2520heuristics%252C%2520we%2520propose%2520using%2520Monte%250ACarlo%2520Tree%2520Search%2520%2528MCTS%2529%2520for%2520LLM-based%2520heuristic%2520evolution%2520while%2520preserving%2520all%250ALLM-generated%2520heuristics%2520in%2520a%2520tree%2520structure.%2520With%2520a%2520novel%2520thought-alignment%250Aprocess%2520and%2520an%2520exploration-decay%2520technique%252C%2520the%2520proposed%2520MCTS-AHD%2520method%250Adelivers%2520significantly%2520higher-quality%2520heuristics%2520on%2520various%2520complex%2520tasks.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/zz1358m/MCTS-AHD-master.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08603v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monte%20Carlo%20Tree%20Search%20for%20Comprehensive%20Exploration%20in%20LLM-Based%0A%20%20Automatic%20Heuristic%20Design&entry.906535625=Zhi%20Zheng%20and%20Zhuoliang%20Xie%20and%20Zhenkun%20Wang%20and%20Bryan%20Hooi&entry.1292438233=%20%20Handcrafting%20heuristics%20for%20solving%20complex%20planning%20tasks%20%28e.g.%2C%20NP-hard%0Acombinatorial%20optimization%20%28CO%29%20problems%29%20is%20a%20common%20practice%20but%20requires%0Aextensive%20domain%20knowledge.%20Recently%2C%20Large%20Language%20Model%20%28LLM%29-based%0Aautomatic%20heuristics%20design%20%28AHD%29%20methods%20have%20shown%20promise%20in%20generating%0Ahigh-quality%20heuristics%20without%20manual%20intervention.%20Existing%20LLM-based%20AHD%0Amethods%20employ%20a%20population%20to%20maintain%20a%20fixed%20number%20of%20top-performing%0ALLM-generated%20heuristics%20and%20introduce%20evolutionary%20computation%20%28EC%29%20to%20enhance%0Athe%20population%20iteratively.%20However%2C%20the%20population-based%20procedure%20brings%0Agreedy%20properties%2C%20often%20resulting%20in%20convergence%20to%20local%20optima.%20Instead%2C%20to%0Amore%20comprehensively%20explore%20the%20space%20of%20heuristics%2C%20we%20propose%20using%20Monte%0ACarlo%20Tree%20Search%20%28MCTS%29%20for%20LLM-based%20heuristic%20evolution%20while%20preserving%20all%0ALLM-generated%20heuristics%20in%20a%20tree%20structure.%20With%20a%20novel%20thought-alignment%0Aprocess%20and%20an%20exploration-decay%20technique%2C%20the%20proposed%20MCTS-AHD%20method%0Adelivers%20significantly%20higher-quality%20heuristics%20on%20various%20complex%20tasks.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/zz1358m/MCTS-AHD-master.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08603v2&entry.124074799=Read"},
{"title": "SynthLight: Portrait Relighting with Diffusion Model by Learning to\n  Re-render Synthetic Faces", "author": "Sumit Chaturvedi and Mengwei Ren and Yannick Hold-Geoffroy and Jingyuan Liu and Julie Dorsey and Zhixin Shu", "abstract": "  We introduce SynthLight, a diffusion model for portrait relighting. Our\napproach frames image relighting as a re-rendering problem, where pixels are\ntransformed in response to changes in environmental lighting conditions. Using\na physically-based rendering engine, we synthesize a dataset to simulate this\nlighting-conditioned transformation with 3D head assets under varying lighting.\nWe propose two training and inference strategies to bridge the gap between the\nsynthetic and real image domains: (1) multi-task training that takes advantage\nof real human portraits without lighting labels; (2) an inference time\ndiffusion sampling procedure based on classifier-free guidance that leverages\nthe input portrait to better preserve details. Our method generalizes to\ndiverse real photographs and produces realistic illumination effects, including\nspecular highlights and cast shadows, while preserving the subject's identity.\nOur quantitative experiments on Light Stage data demonstrate results comparable\nto state-of-the-art relighting methods. Our qualitative results on in-the-wild\nimages showcase rich and unprecedented illumination effects. Project Page:\n\\url{https://vrroom.github.io/synthlight/}\n", "link": "http://arxiv.org/abs/2501.09756v1", "date": "2025-01-16", "relevancy": 1.8783, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6574}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5873}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynthLight%3A%20Portrait%20Relighting%20with%20Diffusion%20Model%20by%20Learning%20to%0A%20%20Re-render%20Synthetic%20Faces&body=Title%3A%20SynthLight%3A%20Portrait%20Relighting%20with%20Diffusion%20Model%20by%20Learning%20to%0A%20%20Re-render%20Synthetic%20Faces%0AAuthor%3A%20Sumit%20Chaturvedi%20and%20Mengwei%20Ren%20and%20Yannick%20Hold-Geoffroy%20and%20Jingyuan%20Liu%20and%20Julie%20Dorsey%20and%20Zhixin%20Shu%0AAbstract%3A%20%20%20We%20introduce%20SynthLight%2C%20a%20diffusion%20model%20for%20portrait%20relighting.%20Our%0Aapproach%20frames%20image%20relighting%20as%20a%20re-rendering%20problem%2C%20where%20pixels%20are%0Atransformed%20in%20response%20to%20changes%20in%20environmental%20lighting%20conditions.%20Using%0Aa%20physically-based%20rendering%20engine%2C%20we%20synthesize%20a%20dataset%20to%20simulate%20this%0Alighting-conditioned%20transformation%20with%203D%20head%20assets%20under%20varying%20lighting.%0AWe%20propose%20two%20training%20and%20inference%20strategies%20to%20bridge%20the%20gap%20between%20the%0Asynthetic%20and%20real%20image%20domains%3A%20%281%29%20multi-task%20training%20that%20takes%20advantage%0Aof%20real%20human%20portraits%20without%20lighting%20labels%3B%20%282%29%20an%20inference%20time%0Adiffusion%20sampling%20procedure%20based%20on%20classifier-free%20guidance%20that%20leverages%0Athe%20input%20portrait%20to%20better%20preserve%20details.%20Our%20method%20generalizes%20to%0Adiverse%20real%20photographs%20and%20produces%20realistic%20illumination%20effects%2C%20including%0Aspecular%20highlights%20and%20cast%20shadows%2C%20while%20preserving%20the%20subject%27s%20identity.%0AOur%20quantitative%20experiments%20on%20Light%20Stage%20data%20demonstrate%20results%20comparable%0Ato%20state-of-the-art%20relighting%20methods.%20Our%20qualitative%20results%20on%20in-the-wild%0Aimages%20showcase%20rich%20and%20unprecedented%20illumination%20effects.%20Project%20Page%3A%0A%5Curl%7Bhttps%3A//vrroom.github.io/synthlight/%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthLight%253A%2520Portrait%2520Relighting%2520with%2520Diffusion%2520Model%2520by%2520Learning%2520to%250A%2520%2520Re-render%2520Synthetic%2520Faces%26entry.906535625%3DSumit%2520Chaturvedi%2520and%2520Mengwei%2520Ren%2520and%2520Yannick%2520Hold-Geoffroy%2520and%2520Jingyuan%2520Liu%2520and%2520Julie%2520Dorsey%2520and%2520Zhixin%2520Shu%26entry.1292438233%3D%2520%2520We%2520introduce%2520SynthLight%252C%2520a%2520diffusion%2520model%2520for%2520portrait%2520relighting.%2520Our%250Aapproach%2520frames%2520image%2520relighting%2520as%2520a%2520re-rendering%2520problem%252C%2520where%2520pixels%2520are%250Atransformed%2520in%2520response%2520to%2520changes%2520in%2520environmental%2520lighting%2520conditions.%2520Using%250Aa%2520physically-based%2520rendering%2520engine%252C%2520we%2520synthesize%2520a%2520dataset%2520to%2520simulate%2520this%250Alighting-conditioned%2520transformation%2520with%25203D%2520head%2520assets%2520under%2520varying%2520lighting.%250AWe%2520propose%2520two%2520training%2520and%2520inference%2520strategies%2520to%2520bridge%2520the%2520gap%2520between%2520the%250Asynthetic%2520and%2520real%2520image%2520domains%253A%2520%25281%2529%2520multi-task%2520training%2520that%2520takes%2520advantage%250Aof%2520real%2520human%2520portraits%2520without%2520lighting%2520labels%253B%2520%25282%2529%2520an%2520inference%2520time%250Adiffusion%2520sampling%2520procedure%2520based%2520on%2520classifier-free%2520guidance%2520that%2520leverages%250Athe%2520input%2520portrait%2520to%2520better%2520preserve%2520details.%2520Our%2520method%2520generalizes%2520to%250Adiverse%2520real%2520photographs%2520and%2520produces%2520realistic%2520illumination%2520effects%252C%2520including%250Aspecular%2520highlights%2520and%2520cast%2520shadows%252C%2520while%2520preserving%2520the%2520subject%2527s%2520identity.%250AOur%2520quantitative%2520experiments%2520on%2520Light%2520Stage%2520data%2520demonstrate%2520results%2520comparable%250Ato%2520state-of-the-art%2520relighting%2520methods.%2520Our%2520qualitative%2520results%2520on%2520in-the-wild%250Aimages%2520showcase%2520rich%2520and%2520unprecedented%2520illumination%2520effects.%2520Project%2520Page%253A%250A%255Curl%257Bhttps%253A//vrroom.github.io/synthlight/%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynthLight%3A%20Portrait%20Relighting%20with%20Diffusion%20Model%20by%20Learning%20to%0A%20%20Re-render%20Synthetic%20Faces&entry.906535625=Sumit%20Chaturvedi%20and%20Mengwei%20Ren%20and%20Yannick%20Hold-Geoffroy%20and%20Jingyuan%20Liu%20and%20Julie%20Dorsey%20and%20Zhixin%20Shu&entry.1292438233=%20%20We%20introduce%20SynthLight%2C%20a%20diffusion%20model%20for%20portrait%20relighting.%20Our%0Aapproach%20frames%20image%20relighting%20as%20a%20re-rendering%20problem%2C%20where%20pixels%20are%0Atransformed%20in%20response%20to%20changes%20in%20environmental%20lighting%20conditions.%20Using%0Aa%20physically-based%20rendering%20engine%2C%20we%20synthesize%20a%20dataset%20to%20simulate%20this%0Alighting-conditioned%20transformation%20with%203D%20head%20assets%20under%20varying%20lighting.%0AWe%20propose%20two%20training%20and%20inference%20strategies%20to%20bridge%20the%20gap%20between%20the%0Asynthetic%20and%20real%20image%20domains%3A%20%281%29%20multi-task%20training%20that%20takes%20advantage%0Aof%20real%20human%20portraits%20without%20lighting%20labels%3B%20%282%29%20an%20inference%20time%0Adiffusion%20sampling%20procedure%20based%20on%20classifier-free%20guidance%20that%20leverages%0Athe%20input%20portrait%20to%20better%20preserve%20details.%20Our%20method%20generalizes%20to%0Adiverse%20real%20photographs%20and%20produces%20realistic%20illumination%20effects%2C%20including%0Aspecular%20highlights%20and%20cast%20shadows%2C%20while%20preserving%20the%20subject%27s%20identity.%0AOur%20quantitative%20experiments%20on%20Light%20Stage%20data%20demonstrate%20results%20comparable%0Ato%20state-of-the-art%20relighting%20methods.%20Our%20qualitative%20results%20on%20in-the-wild%0Aimages%20showcase%20rich%20and%20unprecedented%20illumination%20effects.%20Project%20Page%3A%0A%5Curl%7Bhttps%3A//vrroom.github.io/synthlight/%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09756v1&entry.124074799=Read"},
{"title": "Fokker-Planck to Callan-Symanzik: evolution of weight matrices under\n  training", "author": "Wei Bu and Uri Kol and Ziming Liu", "abstract": "  The dynamical evolution of a neural network during training has been an\nincredibly fascinating subject of study. First principal derivation of generic\nevolution of variables in statistical physics systems has proved useful when\nused to describe training dynamics conceptually, which in practice means\nnumerically solving equations such as Fokker-Planck equation. Simulating entire\nnetworks inevitably runs into the curse of dimensionality. In this paper, we\nutilize Fokker-Planck to simulate the probability density evolution of\nindividual weight matrices in the bottleneck layers of a simple\n2-bottleneck-layered auto-encoder and compare the theoretical evolutions\nagainst the empirical ones by examining the output data distributions. We also\nderive physically relevant partial differential equations such as\nCallan-Symanzik and Kardar-Parisi-Zhang equations from the dynamical equation\nwe have.\n", "link": "http://arxiv.org/abs/2501.09659v1", "date": "2025-01-16", "relevancy": 1.484, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4986}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4957}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fokker-Planck%20to%20Callan-Symanzik%3A%20evolution%20of%20weight%20matrices%20under%0A%20%20training&body=Title%3A%20Fokker-Planck%20to%20Callan-Symanzik%3A%20evolution%20of%20weight%20matrices%20under%0A%20%20training%0AAuthor%3A%20Wei%20Bu%20and%20Uri%20Kol%20and%20Ziming%20Liu%0AAbstract%3A%20%20%20The%20dynamical%20evolution%20of%20a%20neural%20network%20during%20training%20has%20been%20an%0Aincredibly%20fascinating%20subject%20of%20study.%20First%20principal%20derivation%20of%20generic%0Aevolution%20of%20variables%20in%20statistical%20physics%20systems%20has%20proved%20useful%20when%0Aused%20to%20describe%20training%20dynamics%20conceptually%2C%20which%20in%20practice%20means%0Anumerically%20solving%20equations%20such%20as%20Fokker-Planck%20equation.%20Simulating%20entire%0Anetworks%20inevitably%20runs%20into%20the%20curse%20of%20dimensionality.%20In%20this%20paper%2C%20we%0Autilize%20Fokker-Planck%20to%20simulate%20the%20probability%20density%20evolution%20of%0Aindividual%20weight%20matrices%20in%20the%20bottleneck%20layers%20of%20a%20simple%0A2-bottleneck-layered%20auto-encoder%20and%20compare%20the%20theoretical%20evolutions%0Aagainst%20the%20empirical%20ones%20by%20examining%20the%20output%20data%20distributions.%20We%20also%0Aderive%20physically%20relevant%20partial%20differential%20equations%20such%20as%0ACallan-Symanzik%20and%20Kardar-Parisi-Zhang%20equations%20from%20the%20dynamical%20equation%0Awe%20have.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFokker-Planck%2520to%2520Callan-Symanzik%253A%2520evolution%2520of%2520weight%2520matrices%2520under%250A%2520%2520training%26entry.906535625%3DWei%2520Bu%2520and%2520Uri%2520Kol%2520and%2520Ziming%2520Liu%26entry.1292438233%3D%2520%2520The%2520dynamical%2520evolution%2520of%2520a%2520neural%2520network%2520during%2520training%2520has%2520been%2520an%250Aincredibly%2520fascinating%2520subject%2520of%2520study.%2520First%2520principal%2520derivation%2520of%2520generic%250Aevolution%2520of%2520variables%2520in%2520statistical%2520physics%2520systems%2520has%2520proved%2520useful%2520when%250Aused%2520to%2520describe%2520training%2520dynamics%2520conceptually%252C%2520which%2520in%2520practice%2520means%250Anumerically%2520solving%2520equations%2520such%2520as%2520Fokker-Planck%2520equation.%2520Simulating%2520entire%250Anetworks%2520inevitably%2520runs%2520into%2520the%2520curse%2520of%2520dimensionality.%2520In%2520this%2520paper%252C%2520we%250Autilize%2520Fokker-Planck%2520to%2520simulate%2520the%2520probability%2520density%2520evolution%2520of%250Aindividual%2520weight%2520matrices%2520in%2520the%2520bottleneck%2520layers%2520of%2520a%2520simple%250A2-bottleneck-layered%2520auto-encoder%2520and%2520compare%2520the%2520theoretical%2520evolutions%250Aagainst%2520the%2520empirical%2520ones%2520by%2520examining%2520the%2520output%2520data%2520distributions.%2520We%2520also%250Aderive%2520physically%2520relevant%2520partial%2520differential%2520equations%2520such%2520as%250ACallan-Symanzik%2520and%2520Kardar-Parisi-Zhang%2520equations%2520from%2520the%2520dynamical%2520equation%250Awe%2520have.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fokker-Planck%20to%20Callan-Symanzik%3A%20evolution%20of%20weight%20matrices%20under%0A%20%20training&entry.906535625=Wei%20Bu%20and%20Uri%20Kol%20and%20Ziming%20Liu&entry.1292438233=%20%20The%20dynamical%20evolution%20of%20a%20neural%20network%20during%20training%20has%20been%20an%0Aincredibly%20fascinating%20subject%20of%20study.%20First%20principal%20derivation%20of%20generic%0Aevolution%20of%20variables%20in%20statistical%20physics%20systems%20has%20proved%20useful%20when%0Aused%20to%20describe%20training%20dynamics%20conceptually%2C%20which%20in%20practice%20means%0Anumerically%20solving%20equations%20such%20as%20Fokker-Planck%20equation.%20Simulating%20entire%0Anetworks%20inevitably%20runs%20into%20the%20curse%20of%20dimensionality.%20In%20this%20paper%2C%20we%0Autilize%20Fokker-Planck%20to%20simulate%20the%20probability%20density%20evolution%20of%0Aindividual%20weight%20matrices%20in%20the%20bottleneck%20layers%20of%20a%20simple%0A2-bottleneck-layered%20auto-encoder%20and%20compare%20the%20theoretical%20evolutions%0Aagainst%20the%20empirical%20ones%20by%20examining%20the%20output%20data%20distributions.%20We%20also%0Aderive%20physically%20relevant%20partial%20differential%20equations%20such%20as%0ACallan-Symanzik%20and%20Kardar-Parisi-Zhang%20equations%20from%20the%20dynamical%20equation%0Awe%20have.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09659v1&entry.124074799=Read"},
{"title": "The Goofus & Gallant Story Corpus for Practical Value Alignment", "author": "Md Sultan Al Nahian and Tasmia Tasrin and Spencer Frazier and Mark Riedl and Brent Harrison", "abstract": "  Values or principles are key elements of human society that influence people\nto behave and function according to an accepted standard set of social rules to\nmaintain social order. As AI systems are becoming ubiquitous in human society,\nit is a major concern that they could violate these norms or values and\npotentially cause harm. Thus, to prevent intentional or unintentional harm, AI\nsystems are expected to take actions that align with these principles. Training\nsystems to exhibit this type of behavior is difficult and often requires a\nspecialized dataset. This work presents a multi-modal dataset illustrating\nnormative and non-normative behavior in real-life situations described through\nnatural language and artistic images. This training set contains curated sets\nof images that are designed to teach young children about social principles. We\nargue that this is an ideal dataset to use for training socially normative\nagents given this fact.\n", "link": "http://arxiv.org/abs/2501.09707v1", "date": "2025-01-16", "relevancy": 1.4251, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4786}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4743}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Goofus%20%26%20Gallant%20Story%20Corpus%20for%20Practical%20Value%20Alignment&body=Title%3A%20The%20Goofus%20%26%20Gallant%20Story%20Corpus%20for%20Practical%20Value%20Alignment%0AAuthor%3A%20Md%20Sultan%20Al%20Nahian%20and%20Tasmia%20Tasrin%20and%20Spencer%20Frazier%20and%20Mark%20Riedl%20and%20Brent%20Harrison%0AAbstract%3A%20%20%20Values%20or%20principles%20are%20key%20elements%20of%20human%20society%20that%20influence%20people%0Ato%20behave%20and%20function%20according%20to%20an%20accepted%20standard%20set%20of%20social%20rules%20to%0Amaintain%20social%20order.%20As%20AI%20systems%20are%20becoming%20ubiquitous%20in%20human%20society%2C%0Ait%20is%20a%20major%20concern%20that%20they%20could%20violate%20these%20norms%20or%20values%20and%0Apotentially%20cause%20harm.%20Thus%2C%20to%20prevent%20intentional%20or%20unintentional%20harm%2C%20AI%0Asystems%20are%20expected%20to%20take%20actions%20that%20align%20with%20these%20principles.%20Training%0Asystems%20to%20exhibit%20this%20type%20of%20behavior%20is%20difficult%20and%20often%20requires%20a%0Aspecialized%20dataset.%20This%20work%20presents%20a%20multi-modal%20dataset%20illustrating%0Anormative%20and%20non-normative%20behavior%20in%20real-life%20situations%20described%20through%0Anatural%20language%20and%20artistic%20images.%20This%20training%20set%20contains%20curated%20sets%0Aof%20images%20that%20are%20designed%20to%20teach%20young%20children%20about%20social%20principles.%20We%0Aargue%20that%20this%20is%20an%20ideal%20dataset%20to%20use%20for%20training%20socially%20normative%0Aagents%20given%20this%20fact.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Goofus%2520%2526%2520Gallant%2520Story%2520Corpus%2520for%2520Practical%2520Value%2520Alignment%26entry.906535625%3DMd%2520Sultan%2520Al%2520Nahian%2520and%2520Tasmia%2520Tasrin%2520and%2520Spencer%2520Frazier%2520and%2520Mark%2520Riedl%2520and%2520Brent%2520Harrison%26entry.1292438233%3D%2520%2520Values%2520or%2520principles%2520are%2520key%2520elements%2520of%2520human%2520society%2520that%2520influence%2520people%250Ato%2520behave%2520and%2520function%2520according%2520to%2520an%2520accepted%2520standard%2520set%2520of%2520social%2520rules%2520to%250Amaintain%2520social%2520order.%2520As%2520AI%2520systems%2520are%2520becoming%2520ubiquitous%2520in%2520human%2520society%252C%250Ait%2520is%2520a%2520major%2520concern%2520that%2520they%2520could%2520violate%2520these%2520norms%2520or%2520values%2520and%250Apotentially%2520cause%2520harm.%2520Thus%252C%2520to%2520prevent%2520intentional%2520or%2520unintentional%2520harm%252C%2520AI%250Asystems%2520are%2520expected%2520to%2520take%2520actions%2520that%2520align%2520with%2520these%2520principles.%2520Training%250Asystems%2520to%2520exhibit%2520this%2520type%2520of%2520behavior%2520is%2520difficult%2520and%2520often%2520requires%2520a%250Aspecialized%2520dataset.%2520This%2520work%2520presents%2520a%2520multi-modal%2520dataset%2520illustrating%250Anormative%2520and%2520non-normative%2520behavior%2520in%2520real-life%2520situations%2520described%2520through%250Anatural%2520language%2520and%2520artistic%2520images.%2520This%2520training%2520set%2520contains%2520curated%2520sets%250Aof%2520images%2520that%2520are%2520designed%2520to%2520teach%2520young%2520children%2520about%2520social%2520principles.%2520We%250Aargue%2520that%2520this%2520is%2520an%2520ideal%2520dataset%2520to%2520use%2520for%2520training%2520socially%2520normative%250Aagents%2520given%2520this%2520fact.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Goofus%20%26%20Gallant%20Story%20Corpus%20for%20Practical%20Value%20Alignment&entry.906535625=Md%20Sultan%20Al%20Nahian%20and%20Tasmia%20Tasrin%20and%20Spencer%20Frazier%20and%20Mark%20Riedl%20and%20Brent%20Harrison&entry.1292438233=%20%20Values%20or%20principles%20are%20key%20elements%20of%20human%20society%20that%20influence%20people%0Ato%20behave%20and%20function%20according%20to%20an%20accepted%20standard%20set%20of%20social%20rules%20to%0Amaintain%20social%20order.%20As%20AI%20systems%20are%20becoming%20ubiquitous%20in%20human%20society%2C%0Ait%20is%20a%20major%20concern%20that%20they%20could%20violate%20these%20norms%20or%20values%20and%0Apotentially%20cause%20harm.%20Thus%2C%20to%20prevent%20intentional%20or%20unintentional%20harm%2C%20AI%0Asystems%20are%20expected%20to%20take%20actions%20that%20align%20with%20these%20principles.%20Training%0Asystems%20to%20exhibit%20this%20type%20of%20behavior%20is%20difficult%20and%20often%20requires%20a%0Aspecialized%20dataset.%20This%20work%20presents%20a%20multi-modal%20dataset%20illustrating%0Anormative%20and%20non-normative%20behavior%20in%20real-life%20situations%20described%20through%0Anatural%20language%20and%20artistic%20images.%20This%20training%20set%20contains%20curated%20sets%0Aof%20images%20that%20are%20designed%20to%20teach%20young%20children%20about%20social%20principles.%20We%0Aargue%20that%20this%20is%20an%20ideal%20dataset%20to%20use%20for%20training%20socially%20normative%0Aagents%20given%20this%20fact.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09707v1&entry.124074799=Read"},
{"title": "MonoSOWA: Scalable monocular 3D Object detector Without human\n  Annotations", "author": "Jan Skvrna and Lukas Neumann", "abstract": "  Detecting the three-dimensional position and orientation of objects using a\nsingle RGB camera is a foundational task in computer vision with many important\napplications. Traditionally, 3D object detection methods are trained in a\nfully-supervised setup, requiring vast amounts of human annotations, which are\nlaborious, costly, and do not scale well with the ever-increasing amounts of\ndata being captured.\n  In this paper, we present the first method to train 3D object detectors for\nmonocular RGB cameras without domain-specific human annotations, thus making\norders of magnitude more data available for training. Thanks to newly proposed\nCanonical Object Space, the method can not only exploit data across a variety\nof datasets and camera setups to train a single 3D detector, but unlike\nprevious work it also works out of the box in previously unseen camera setups.\nAll this is crucial for practical applications, where the data and cameras are\nextremely heterogeneous.\n  The method is evaluated on two standard autonomous driving datasets, where it\noutperforms previous works, which, unlike our method, still rely on 2D human\nannotations.\n", "link": "http://arxiv.org/abs/2501.09481v1", "date": "2025-01-16", "relevancy": 1.7343, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.583}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5792}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoSOWA%3A%20Scalable%20monocular%203D%20Object%20detector%20Without%20human%0A%20%20Annotations&body=Title%3A%20MonoSOWA%3A%20Scalable%20monocular%203D%20Object%20detector%20Without%20human%0A%20%20Annotations%0AAuthor%3A%20Jan%20Skvrna%20and%20Lukas%20Neumann%0AAbstract%3A%20%20%20Detecting%20the%20three-dimensional%20position%20and%20orientation%20of%20objects%20using%20a%0Asingle%20RGB%20camera%20is%20a%20foundational%20task%20in%20computer%20vision%20with%20many%20important%0Aapplications.%20Traditionally%2C%203D%20object%20detection%20methods%20are%20trained%20in%20a%0Afully-supervised%20setup%2C%20requiring%20vast%20amounts%20of%20human%20annotations%2C%20which%20are%0Alaborious%2C%20costly%2C%20and%20do%20not%20scale%20well%20with%20the%20ever-increasing%20amounts%20of%0Adata%20being%20captured.%0A%20%20In%20this%20paper%2C%20we%20present%20the%20first%20method%20to%20train%203D%20object%20detectors%20for%0Amonocular%20RGB%20cameras%20without%20domain-specific%20human%20annotations%2C%20thus%20making%0Aorders%20of%20magnitude%20more%20data%20available%20for%20training.%20Thanks%20to%20newly%20proposed%0ACanonical%20Object%20Space%2C%20the%20method%20can%20not%20only%20exploit%20data%20across%20a%20variety%0Aof%20datasets%20and%20camera%20setups%20to%20train%20a%20single%203D%20detector%2C%20but%20unlike%0Aprevious%20work%20it%20also%20works%20out%20of%20the%20box%20in%20previously%20unseen%20camera%20setups.%0AAll%20this%20is%20crucial%20for%20practical%20applications%2C%20where%20the%20data%20and%20cameras%20are%0Aextremely%20heterogeneous.%0A%20%20The%20method%20is%20evaluated%20on%20two%20standard%20autonomous%20driving%20datasets%2C%20where%20it%0Aoutperforms%20previous%20works%2C%20which%2C%20unlike%20our%20method%2C%20still%20rely%20on%202D%20human%0Aannotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoSOWA%253A%2520Scalable%2520monocular%25203D%2520Object%2520detector%2520Without%2520human%250A%2520%2520Annotations%26entry.906535625%3DJan%2520Skvrna%2520and%2520Lukas%2520Neumann%26entry.1292438233%3D%2520%2520Detecting%2520the%2520three-dimensional%2520position%2520and%2520orientation%2520of%2520objects%2520using%2520a%250Asingle%2520RGB%2520camera%2520is%2520a%2520foundational%2520task%2520in%2520computer%2520vision%2520with%2520many%2520important%250Aapplications.%2520Traditionally%252C%25203D%2520object%2520detection%2520methods%2520are%2520trained%2520in%2520a%250Afully-supervised%2520setup%252C%2520requiring%2520vast%2520amounts%2520of%2520human%2520annotations%252C%2520which%2520are%250Alaborious%252C%2520costly%252C%2520and%2520do%2520not%2520scale%2520well%2520with%2520the%2520ever-increasing%2520amounts%2520of%250Adata%2520being%2520captured.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520method%2520to%2520train%25203D%2520object%2520detectors%2520for%250Amonocular%2520RGB%2520cameras%2520without%2520domain-specific%2520human%2520annotations%252C%2520thus%2520making%250Aorders%2520of%2520magnitude%2520more%2520data%2520available%2520for%2520training.%2520Thanks%2520to%2520newly%2520proposed%250ACanonical%2520Object%2520Space%252C%2520the%2520method%2520can%2520not%2520only%2520exploit%2520data%2520across%2520a%2520variety%250Aof%2520datasets%2520and%2520camera%2520setups%2520to%2520train%2520a%2520single%25203D%2520detector%252C%2520but%2520unlike%250Aprevious%2520work%2520it%2520also%2520works%2520out%2520of%2520the%2520box%2520in%2520previously%2520unseen%2520camera%2520setups.%250AAll%2520this%2520is%2520crucial%2520for%2520practical%2520applications%252C%2520where%2520the%2520data%2520and%2520cameras%2520are%250Aextremely%2520heterogeneous.%250A%2520%2520The%2520method%2520is%2520evaluated%2520on%2520two%2520standard%2520autonomous%2520driving%2520datasets%252C%2520where%2520it%250Aoutperforms%2520previous%2520works%252C%2520which%252C%2520unlike%2520our%2520method%252C%2520still%2520rely%2520on%25202D%2520human%250Aannotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoSOWA%3A%20Scalable%20monocular%203D%20Object%20detector%20Without%20human%0A%20%20Annotations&entry.906535625=Jan%20Skvrna%20and%20Lukas%20Neumann&entry.1292438233=%20%20Detecting%20the%20three-dimensional%20position%20and%20orientation%20of%20objects%20using%20a%0Asingle%20RGB%20camera%20is%20a%20foundational%20task%20in%20computer%20vision%20with%20many%20important%0Aapplications.%20Traditionally%2C%203D%20object%20detection%20methods%20are%20trained%20in%20a%0Afully-supervised%20setup%2C%20requiring%20vast%20amounts%20of%20human%20annotations%2C%20which%20are%0Alaborious%2C%20costly%2C%20and%20do%20not%20scale%20well%20with%20the%20ever-increasing%20amounts%20of%0Adata%20being%20captured.%0A%20%20In%20this%20paper%2C%20we%20present%20the%20first%20method%20to%20train%203D%20object%20detectors%20for%0Amonocular%20RGB%20cameras%20without%20domain-specific%20human%20annotations%2C%20thus%20making%0Aorders%20of%20magnitude%20more%20data%20available%20for%20training.%20Thanks%20to%20newly%20proposed%0ACanonical%20Object%20Space%2C%20the%20method%20can%20not%20only%20exploit%20data%20across%20a%20variety%0Aof%20datasets%20and%20camera%20setups%20to%20train%20a%20single%203D%20detector%2C%20but%20unlike%0Aprevious%20work%20it%20also%20works%20out%20of%20the%20box%20in%20previously%20unseen%20camera%20setups.%0AAll%20this%20is%20crucial%20for%20practical%20applications%2C%20where%20the%20data%20and%20cameras%20are%0Aextremely%20heterogeneous.%0A%20%20The%20method%20is%20evaluated%20on%20two%20standard%20autonomous%20driving%20datasets%2C%20where%20it%0Aoutperforms%20previous%20works%2C%20which%2C%20unlike%20our%20method%2C%20still%20rely%20on%202D%20human%0Aannotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09481v1&entry.124074799=Read"},
{"title": "Reward-Guided Controlled Generation for Inference-Time Alignment in\n  Diffusion Models: Tutorial and Review", "author": "Masatoshi Uehara and Yulai Zhao and Chenyu Wang and Xiner Li and Aviv Regev and Sergey Levine and Tommaso Biancalani", "abstract": "  This tutorial provides an in-depth guide on inference-time guidance and\nalignment methods for optimizing downstream reward functions in diffusion\nmodels. While diffusion models are renowned for their generative modeling\ncapabilities, practical applications in fields such as biology often require\nsample generation that maximizes specific metrics (e.g., stability, affinity in\nproteins, closeness to target structures). In these scenarios, diffusion models\ncan be adapted not only to generate realistic samples but also to explicitly\nmaximize desired measures at inference time without fine-tuning. This tutorial\nexplores the foundational aspects of such inference-time algorithms. We review\nthese methods from a unified perspective, demonstrating that current techniques\n-- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling,\nand classifier guidance -- aim to approximate soft optimal denoising processes\n(a.k.a. policies in RL) that combine pre-trained denoising processes with value\nfunctions serving as look-ahead functions that predict from intermediate states\nto terminal rewards. Within this framework, we present several novel algorithms\nnot yet covered in the literature. Furthermore, we discuss (1) fine-tuning\nmethods combined with inference-time techniques, (2) inference-time algorithms\nbased on search algorithms such as Monte Carlo tree search, which have received\nlimited attention in current research, and (3) connections between\ninference-time algorithms in language models and diffusion models. The code of\nthis tutorial on protein design is available at\nhttps://github.com/masa-ue/AlignInversePro\n", "link": "http://arxiv.org/abs/2501.09685v1", "date": "2025-01-16", "relevancy": 1.6144, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.55}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5366}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward-Guided%20Controlled%20Generation%20for%20Inference-Time%20Alignment%20in%0A%20%20Diffusion%20Models%3A%20Tutorial%20and%20Review&body=Title%3A%20Reward-Guided%20Controlled%20Generation%20for%20Inference-Time%20Alignment%20in%0A%20%20Diffusion%20Models%3A%20Tutorial%20and%20Review%0AAuthor%3A%20Masatoshi%20Uehara%20and%20Yulai%20Zhao%20and%20Chenyu%20Wang%20and%20Xiner%20Li%20and%20Aviv%20Regev%20and%20Sergey%20Levine%20and%20Tommaso%20Biancalani%0AAbstract%3A%20%20%20This%20tutorial%20provides%20an%20in-depth%20guide%20on%20inference-time%20guidance%20and%0Aalignment%20methods%20for%20optimizing%20downstream%20reward%20functions%20in%20diffusion%0Amodels.%20While%20diffusion%20models%20are%20renowned%20for%20their%20generative%20modeling%0Acapabilities%2C%20practical%20applications%20in%20fields%20such%20as%20biology%20often%20require%0Asample%20generation%20that%20maximizes%20specific%20metrics%20%28e.g.%2C%20stability%2C%20affinity%20in%0Aproteins%2C%20closeness%20to%20target%20structures%29.%20In%20these%20scenarios%2C%20diffusion%20models%0Acan%20be%20adapted%20not%20only%20to%20generate%20realistic%20samples%20but%20also%20to%20explicitly%0Amaximize%20desired%20measures%20at%20inference%20time%20without%20fine-tuning.%20This%20tutorial%0Aexplores%20the%20foundational%20aspects%20of%20such%20inference-time%20algorithms.%20We%20review%0Athese%20methods%20from%20a%20unified%20perspective%2C%20demonstrating%20that%20current%20techniques%0A--%20such%20as%20Sequential%20Monte%20Carlo%20%28SMC%29-based%20guidance%2C%20value-based%20sampling%2C%0Aand%20classifier%20guidance%20--%20aim%20to%20approximate%20soft%20optimal%20denoising%20processes%0A%28a.k.a.%20policies%20in%20RL%29%20that%20combine%20pre-trained%20denoising%20processes%20with%20value%0Afunctions%20serving%20as%20look-ahead%20functions%20that%20predict%20from%20intermediate%20states%0Ato%20terminal%20rewards.%20Within%20this%20framework%2C%20we%20present%20several%20novel%20algorithms%0Anot%20yet%20covered%20in%20the%20literature.%20Furthermore%2C%20we%20discuss%20%281%29%20fine-tuning%0Amethods%20combined%20with%20inference-time%20techniques%2C%20%282%29%20inference-time%20algorithms%0Abased%20on%20search%20algorithms%20such%20as%20Monte%20Carlo%20tree%20search%2C%20which%20have%20received%0Alimited%20attention%20in%20current%20research%2C%20and%20%283%29%20connections%20between%0Ainference-time%20algorithms%20in%20language%20models%20and%20diffusion%20models.%20The%20code%20of%0Athis%20tutorial%20on%20protein%20design%20is%20available%20at%0Ahttps%3A//github.com/masa-ue/AlignInversePro%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward-Guided%2520Controlled%2520Generation%2520for%2520Inference-Time%2520Alignment%2520in%250A%2520%2520Diffusion%2520Models%253A%2520Tutorial%2520and%2520Review%26entry.906535625%3DMasatoshi%2520Uehara%2520and%2520Yulai%2520Zhao%2520and%2520Chenyu%2520Wang%2520and%2520Xiner%2520Li%2520and%2520Aviv%2520Regev%2520and%2520Sergey%2520Levine%2520and%2520Tommaso%2520Biancalani%26entry.1292438233%3D%2520%2520This%2520tutorial%2520provides%2520an%2520in-depth%2520guide%2520on%2520inference-time%2520guidance%2520and%250Aalignment%2520methods%2520for%2520optimizing%2520downstream%2520reward%2520functions%2520in%2520diffusion%250Amodels.%2520While%2520diffusion%2520models%2520are%2520renowned%2520for%2520their%2520generative%2520modeling%250Acapabilities%252C%2520practical%2520applications%2520in%2520fields%2520such%2520as%2520biology%2520often%2520require%250Asample%2520generation%2520that%2520maximizes%2520specific%2520metrics%2520%2528e.g.%252C%2520stability%252C%2520affinity%2520in%250Aproteins%252C%2520closeness%2520to%2520target%2520structures%2529.%2520In%2520these%2520scenarios%252C%2520diffusion%2520models%250Acan%2520be%2520adapted%2520not%2520only%2520to%2520generate%2520realistic%2520samples%2520but%2520also%2520to%2520explicitly%250Amaximize%2520desired%2520measures%2520at%2520inference%2520time%2520without%2520fine-tuning.%2520This%2520tutorial%250Aexplores%2520the%2520foundational%2520aspects%2520of%2520such%2520inference-time%2520algorithms.%2520We%2520review%250Athese%2520methods%2520from%2520a%2520unified%2520perspective%252C%2520demonstrating%2520that%2520current%2520techniques%250A--%2520such%2520as%2520Sequential%2520Monte%2520Carlo%2520%2528SMC%2529-based%2520guidance%252C%2520value-based%2520sampling%252C%250Aand%2520classifier%2520guidance%2520--%2520aim%2520to%2520approximate%2520soft%2520optimal%2520denoising%2520processes%250A%2528a.k.a.%2520policies%2520in%2520RL%2529%2520that%2520combine%2520pre-trained%2520denoising%2520processes%2520with%2520value%250Afunctions%2520serving%2520as%2520look-ahead%2520functions%2520that%2520predict%2520from%2520intermediate%2520states%250Ato%2520terminal%2520rewards.%2520Within%2520this%2520framework%252C%2520we%2520present%2520several%2520novel%2520algorithms%250Anot%2520yet%2520covered%2520in%2520the%2520literature.%2520Furthermore%252C%2520we%2520discuss%2520%25281%2529%2520fine-tuning%250Amethods%2520combined%2520with%2520inference-time%2520techniques%252C%2520%25282%2529%2520inference-time%2520algorithms%250Abased%2520on%2520search%2520algorithms%2520such%2520as%2520Monte%2520Carlo%2520tree%2520search%252C%2520which%2520have%2520received%250Alimited%2520attention%2520in%2520current%2520research%252C%2520and%2520%25283%2529%2520connections%2520between%250Ainference-time%2520algorithms%2520in%2520language%2520models%2520and%2520diffusion%2520models.%2520The%2520code%2520of%250Athis%2520tutorial%2520on%2520protein%2520design%2520is%2520available%2520at%250Ahttps%253A//github.com/masa-ue/AlignInversePro%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward-Guided%20Controlled%20Generation%20for%20Inference-Time%20Alignment%20in%0A%20%20Diffusion%20Models%3A%20Tutorial%20and%20Review&entry.906535625=Masatoshi%20Uehara%20and%20Yulai%20Zhao%20and%20Chenyu%20Wang%20and%20Xiner%20Li%20and%20Aviv%20Regev%20and%20Sergey%20Levine%20and%20Tommaso%20Biancalani&entry.1292438233=%20%20This%20tutorial%20provides%20an%20in-depth%20guide%20on%20inference-time%20guidance%20and%0Aalignment%20methods%20for%20optimizing%20downstream%20reward%20functions%20in%20diffusion%0Amodels.%20While%20diffusion%20models%20are%20renowned%20for%20their%20generative%20modeling%0Acapabilities%2C%20practical%20applications%20in%20fields%20such%20as%20biology%20often%20require%0Asample%20generation%20that%20maximizes%20specific%20metrics%20%28e.g.%2C%20stability%2C%20affinity%20in%0Aproteins%2C%20closeness%20to%20target%20structures%29.%20In%20these%20scenarios%2C%20diffusion%20models%0Acan%20be%20adapted%20not%20only%20to%20generate%20realistic%20samples%20but%20also%20to%20explicitly%0Amaximize%20desired%20measures%20at%20inference%20time%20without%20fine-tuning.%20This%20tutorial%0Aexplores%20the%20foundational%20aspects%20of%20such%20inference-time%20algorithms.%20We%20review%0Athese%20methods%20from%20a%20unified%20perspective%2C%20demonstrating%20that%20current%20techniques%0A--%20such%20as%20Sequential%20Monte%20Carlo%20%28SMC%29-based%20guidance%2C%20value-based%20sampling%2C%0Aand%20classifier%20guidance%20--%20aim%20to%20approximate%20soft%20optimal%20denoising%20processes%0A%28a.k.a.%20policies%20in%20RL%29%20that%20combine%20pre-trained%20denoising%20processes%20with%20value%0Afunctions%20serving%20as%20look-ahead%20functions%20that%20predict%20from%20intermediate%20states%0Ato%20terminal%20rewards.%20Within%20this%20framework%2C%20we%20present%20several%20novel%20algorithms%0Anot%20yet%20covered%20in%20the%20literature.%20Furthermore%2C%20we%20discuss%20%281%29%20fine-tuning%0Amethods%20combined%20with%20inference-time%20techniques%2C%20%282%29%20inference-time%20algorithms%0Abased%20on%20search%20algorithms%20such%20as%20Monte%20Carlo%20tree%20search%2C%20which%20have%20received%0Alimited%20attention%20in%20current%20research%2C%20and%20%283%29%20connections%20between%0Ainference-time%20algorithms%20in%20language%20models%20and%20diffusion%20models.%20The%20code%20of%0Athis%20tutorial%20on%20protein%20design%20is%20available%20at%0Ahttps%3A//github.com/masa-ue/AlignInversePro%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09685v1&entry.124074799=Read"},
{"title": "Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in\n  Reproducing Kernel Hilbert Spaces", "author": "Viktor Stein and Sebastian Neumayer and Nicolaj Rux and Gabriele Steidl", "abstract": "  Commonly used $f$-divergences of measures, e.g., the Kullback-Leibler\ndivergence, are subject to limitations regarding the support of the involved\nmeasures. A remedy is regularizing the $f$-divergence by a squared maximum mean\ndiscrepancy (MMD) associated with a characteristic kernel $K$. We use the\nkernel mean embedding to show that this regularization can be rewritten as the\nMoreau envelope of some function on the associated reproducing kernel Hilbert\nspace. Then, we exploit well-known results on Moreau envelopes in Hilbert\nspaces to analyze the MMD-regularized $f$-divergences, particularly their\ngradients. Subsequently, we use our findings to analyze Wasserstein gradient\nflows of MMD-regularized $f$-divergences. We provide proof-of-the-concept\nnumerical examples for flows starting from empirical measures. Here, we cover\n$f$-divergences with infinite and finite recession constants. Lastly, we extend\nour results to the tight variational formulation of $f$-divergences and\nnumerically compare the resulting flows.\n", "link": "http://arxiv.org/abs/2402.04613v3", "date": "2025-01-16", "relevancy": 1.6937, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4455}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.428}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.41}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wasserstein%20Gradient%20Flows%20for%20Moreau%20Envelopes%20of%20f-Divergences%20in%0A%20%20Reproducing%20Kernel%20Hilbert%20Spaces&body=Title%3A%20Wasserstein%20Gradient%20Flows%20for%20Moreau%20Envelopes%20of%20f-Divergences%20in%0A%20%20Reproducing%20Kernel%20Hilbert%20Spaces%0AAuthor%3A%20Viktor%20Stein%20and%20Sebastian%20Neumayer%20and%20Nicolaj%20Rux%20and%20Gabriele%20Steidl%0AAbstract%3A%20%20%20Commonly%20used%20%24f%24-divergences%20of%20measures%2C%20e.g.%2C%20the%20Kullback-Leibler%0Adivergence%2C%20are%20subject%20to%20limitations%20regarding%20the%20support%20of%20the%20involved%0Ameasures.%20A%20remedy%20is%20regularizing%20the%20%24f%24-divergence%20by%20a%20squared%20maximum%20mean%0Adiscrepancy%20%28MMD%29%20associated%20with%20a%20characteristic%20kernel%20%24K%24.%20We%20use%20the%0Akernel%20mean%20embedding%20to%20show%20that%20this%20regularization%20can%20be%20rewritten%20as%20the%0AMoreau%20envelope%20of%20some%20function%20on%20the%20associated%20reproducing%20kernel%20Hilbert%0Aspace.%20Then%2C%20we%20exploit%20well-known%20results%20on%20Moreau%20envelopes%20in%20Hilbert%0Aspaces%20to%20analyze%20the%20MMD-regularized%20%24f%24-divergences%2C%20particularly%20their%0Agradients.%20Subsequently%2C%20we%20use%20our%20findings%20to%20analyze%20Wasserstein%20gradient%0Aflows%20of%20MMD-regularized%20%24f%24-divergences.%20We%20provide%20proof-of-the-concept%0Anumerical%20examples%20for%20flows%20starting%20from%20empirical%20measures.%20Here%2C%20we%20cover%0A%24f%24-divergences%20with%20infinite%20and%20finite%20recession%20constants.%20Lastly%2C%20we%20extend%0Aour%20results%20to%20the%20tight%20variational%20formulation%20of%20%24f%24-divergences%20and%0Anumerically%20compare%20the%20resulting%20flows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04613v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWasserstein%2520Gradient%2520Flows%2520for%2520Moreau%2520Envelopes%2520of%2520f-Divergences%2520in%250A%2520%2520Reproducing%2520Kernel%2520Hilbert%2520Spaces%26entry.906535625%3DViktor%2520Stein%2520and%2520Sebastian%2520Neumayer%2520and%2520Nicolaj%2520Rux%2520and%2520Gabriele%2520Steidl%26entry.1292438233%3D%2520%2520Commonly%2520used%2520%2524f%2524-divergences%2520of%2520measures%252C%2520e.g.%252C%2520the%2520Kullback-Leibler%250Adivergence%252C%2520are%2520subject%2520to%2520limitations%2520regarding%2520the%2520support%2520of%2520the%2520involved%250Ameasures.%2520A%2520remedy%2520is%2520regularizing%2520the%2520%2524f%2524-divergence%2520by%2520a%2520squared%2520maximum%2520mean%250Adiscrepancy%2520%2528MMD%2529%2520associated%2520with%2520a%2520characteristic%2520kernel%2520%2524K%2524.%2520We%2520use%2520the%250Akernel%2520mean%2520embedding%2520to%2520show%2520that%2520this%2520regularization%2520can%2520be%2520rewritten%2520as%2520the%250AMoreau%2520envelope%2520of%2520some%2520function%2520on%2520the%2520associated%2520reproducing%2520kernel%2520Hilbert%250Aspace.%2520Then%252C%2520we%2520exploit%2520well-known%2520results%2520on%2520Moreau%2520envelopes%2520in%2520Hilbert%250Aspaces%2520to%2520analyze%2520the%2520MMD-regularized%2520%2524f%2524-divergences%252C%2520particularly%2520their%250Agradients.%2520Subsequently%252C%2520we%2520use%2520our%2520findings%2520to%2520analyze%2520Wasserstein%2520gradient%250Aflows%2520of%2520MMD-regularized%2520%2524f%2524-divergences.%2520We%2520provide%2520proof-of-the-concept%250Anumerical%2520examples%2520for%2520flows%2520starting%2520from%2520empirical%2520measures.%2520Here%252C%2520we%2520cover%250A%2524f%2524-divergences%2520with%2520infinite%2520and%2520finite%2520recession%2520constants.%2520Lastly%252C%2520we%2520extend%250Aour%2520results%2520to%2520the%2520tight%2520variational%2520formulation%2520of%2520%2524f%2524-divergences%2520and%250Anumerically%2520compare%2520the%2520resulting%2520flows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04613v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wasserstein%20Gradient%20Flows%20for%20Moreau%20Envelopes%20of%20f-Divergences%20in%0A%20%20Reproducing%20Kernel%20Hilbert%20Spaces&entry.906535625=Viktor%20Stein%20and%20Sebastian%20Neumayer%20and%20Nicolaj%20Rux%20and%20Gabriele%20Steidl&entry.1292438233=%20%20Commonly%20used%20%24f%24-divergences%20of%20measures%2C%20e.g.%2C%20the%20Kullback-Leibler%0Adivergence%2C%20are%20subject%20to%20limitations%20regarding%20the%20support%20of%20the%20involved%0Ameasures.%20A%20remedy%20is%20regularizing%20the%20%24f%24-divergence%20by%20a%20squared%20maximum%20mean%0Adiscrepancy%20%28MMD%29%20associated%20with%20a%20characteristic%20kernel%20%24K%24.%20We%20use%20the%0Akernel%20mean%20embedding%20to%20show%20that%20this%20regularization%20can%20be%20rewritten%20as%20the%0AMoreau%20envelope%20of%20some%20function%20on%20the%20associated%20reproducing%20kernel%20Hilbert%0Aspace.%20Then%2C%20we%20exploit%20well-known%20results%20on%20Moreau%20envelopes%20in%20Hilbert%0Aspaces%20to%20analyze%20the%20MMD-regularized%20%24f%24-divergences%2C%20particularly%20their%0Agradients.%20Subsequently%2C%20we%20use%20our%20findings%20to%20analyze%20Wasserstein%20gradient%0Aflows%20of%20MMD-regularized%20%24f%24-divergences.%20We%20provide%20proof-of-the-concept%0Anumerical%20examples%20for%20flows%20starting%20from%20empirical%20measures.%20Here%2C%20we%20cover%0A%24f%24-divergences%20with%20infinite%20and%20finite%20recession%20constants.%20Lastly%2C%20we%20extend%0Aour%20results%20to%20the%20tight%20variational%20formulation%20of%20%24f%24-divergences%20and%0Anumerically%20compare%20the%20resulting%20flows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04613v3&entry.124074799=Read"},
{"title": "Authenticated Delegation and Authorized AI Agents", "author": "Tobin South and Samuele Marro and Thomas Hardjono and Robert Mahari and Cedric Deslandes Whitney and Dazza Greenwood and Alan Chan and Alex Pentland", "abstract": "  The rapid deployment of autonomous AI agents creates urgent challenges around\nauthorization, accountability, and access control in digital spaces. New\nstandards are needed to know whom AI agents act on behalf of and guide their\nuse appropriately, protecting online spaces while unlocking the value of task\ndelegation to autonomous agents. We introduce a novel framework for\nauthenticated, authorized, and auditable delegation of authority to AI agents,\nwhere human users can securely delegate and restrict the permissions and scope\nof agents while maintaining clear chains of accountability. This framework\nbuilds on existing identification and access management protocols, extending\nOAuth 2.0 and OpenID Connect with agent-specific credentials and metadata,\nmaintaining compatibility with established authentication and web\ninfrastructure. Further, we propose a framework for translating flexible,\nnatural language permissions into auditable access control configurations,\nenabling robust scoping of AI agent capabilities across diverse interaction\nmodalities. Taken together, this practical approach facilitates immediate\ndeployment of AI agents while addressing key security and accountability\nconcerns, working toward ensuring agentic AI systems perform only appropriate\nactions and providing a tool for digital service providers to enable AI agent\ninteractions without risking harm from scalable interaction.\n", "link": "http://arxiv.org/abs/2501.09674v1", "date": "2025-01-16", "relevancy": 1.4437, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4975}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4615}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Authenticated%20Delegation%20and%20Authorized%20AI%20Agents&body=Title%3A%20Authenticated%20Delegation%20and%20Authorized%20AI%20Agents%0AAuthor%3A%20Tobin%20South%20and%20Samuele%20Marro%20and%20Thomas%20Hardjono%20and%20Robert%20Mahari%20and%20Cedric%20Deslandes%20Whitney%20and%20Dazza%20Greenwood%20and%20Alan%20Chan%20and%20Alex%20Pentland%0AAbstract%3A%20%20%20The%20rapid%20deployment%20of%20autonomous%20AI%20agents%20creates%20urgent%20challenges%20around%0Aauthorization%2C%20accountability%2C%20and%20access%20control%20in%20digital%20spaces.%20New%0Astandards%20are%20needed%20to%20know%20whom%20AI%20agents%20act%20on%20behalf%20of%20and%20guide%20their%0Ause%20appropriately%2C%20protecting%20online%20spaces%20while%20unlocking%20the%20value%20of%20task%0Adelegation%20to%20autonomous%20agents.%20We%20introduce%20a%20novel%20framework%20for%0Aauthenticated%2C%20authorized%2C%20and%20auditable%20delegation%20of%20authority%20to%20AI%20agents%2C%0Awhere%20human%20users%20can%20securely%20delegate%20and%20restrict%20the%20permissions%20and%20scope%0Aof%20agents%20while%20maintaining%20clear%20chains%20of%20accountability.%20This%20framework%0Abuilds%20on%20existing%20identification%20and%20access%20management%20protocols%2C%20extending%0AOAuth%202.0%20and%20OpenID%20Connect%20with%20agent-specific%20credentials%20and%20metadata%2C%0Amaintaining%20compatibility%20with%20established%20authentication%20and%20web%0Ainfrastructure.%20Further%2C%20we%20propose%20a%20framework%20for%20translating%20flexible%2C%0Anatural%20language%20permissions%20into%20auditable%20access%20control%20configurations%2C%0Aenabling%20robust%20scoping%20of%20AI%20agent%20capabilities%20across%20diverse%20interaction%0Amodalities.%20Taken%20together%2C%20this%20practical%20approach%20facilitates%20immediate%0Adeployment%20of%20AI%20agents%20while%20addressing%20key%20security%20and%20accountability%0Aconcerns%2C%20working%20toward%20ensuring%20agentic%20AI%20systems%20perform%20only%20appropriate%0Aactions%20and%20providing%20a%20tool%20for%20digital%20service%20providers%20to%20enable%20AI%20agent%0Ainteractions%20without%20risking%20harm%20from%20scalable%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuthenticated%2520Delegation%2520and%2520Authorized%2520AI%2520Agents%26entry.906535625%3DTobin%2520South%2520and%2520Samuele%2520Marro%2520and%2520Thomas%2520Hardjono%2520and%2520Robert%2520Mahari%2520and%2520Cedric%2520Deslandes%2520Whitney%2520and%2520Dazza%2520Greenwood%2520and%2520Alan%2520Chan%2520and%2520Alex%2520Pentland%26entry.1292438233%3D%2520%2520The%2520rapid%2520deployment%2520of%2520autonomous%2520AI%2520agents%2520creates%2520urgent%2520challenges%2520around%250Aauthorization%252C%2520accountability%252C%2520and%2520access%2520control%2520in%2520digital%2520spaces.%2520New%250Astandards%2520are%2520needed%2520to%2520know%2520whom%2520AI%2520agents%2520act%2520on%2520behalf%2520of%2520and%2520guide%2520their%250Ause%2520appropriately%252C%2520protecting%2520online%2520spaces%2520while%2520unlocking%2520the%2520value%2520of%2520task%250Adelegation%2520to%2520autonomous%2520agents.%2520We%2520introduce%2520a%2520novel%2520framework%2520for%250Aauthenticated%252C%2520authorized%252C%2520and%2520auditable%2520delegation%2520of%2520authority%2520to%2520AI%2520agents%252C%250Awhere%2520human%2520users%2520can%2520securely%2520delegate%2520and%2520restrict%2520the%2520permissions%2520and%2520scope%250Aof%2520agents%2520while%2520maintaining%2520clear%2520chains%2520of%2520accountability.%2520This%2520framework%250Abuilds%2520on%2520existing%2520identification%2520and%2520access%2520management%2520protocols%252C%2520extending%250AOAuth%25202.0%2520and%2520OpenID%2520Connect%2520with%2520agent-specific%2520credentials%2520and%2520metadata%252C%250Amaintaining%2520compatibility%2520with%2520established%2520authentication%2520and%2520web%250Ainfrastructure.%2520Further%252C%2520we%2520propose%2520a%2520framework%2520for%2520translating%2520flexible%252C%250Anatural%2520language%2520permissions%2520into%2520auditable%2520access%2520control%2520configurations%252C%250Aenabling%2520robust%2520scoping%2520of%2520AI%2520agent%2520capabilities%2520across%2520diverse%2520interaction%250Amodalities.%2520Taken%2520together%252C%2520this%2520practical%2520approach%2520facilitates%2520immediate%250Adeployment%2520of%2520AI%2520agents%2520while%2520addressing%2520key%2520security%2520and%2520accountability%250Aconcerns%252C%2520working%2520toward%2520ensuring%2520agentic%2520AI%2520systems%2520perform%2520only%2520appropriate%250Aactions%2520and%2520providing%2520a%2520tool%2520for%2520digital%2520service%2520providers%2520to%2520enable%2520AI%2520agent%250Ainteractions%2520without%2520risking%2520harm%2520from%2520scalable%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Authenticated%20Delegation%20and%20Authorized%20AI%20Agents&entry.906535625=Tobin%20South%20and%20Samuele%20Marro%20and%20Thomas%20Hardjono%20and%20Robert%20Mahari%20and%20Cedric%20Deslandes%20Whitney%20and%20Dazza%20Greenwood%20and%20Alan%20Chan%20and%20Alex%20Pentland&entry.1292438233=%20%20The%20rapid%20deployment%20of%20autonomous%20AI%20agents%20creates%20urgent%20challenges%20around%0Aauthorization%2C%20accountability%2C%20and%20access%20control%20in%20digital%20spaces.%20New%0Astandards%20are%20needed%20to%20know%20whom%20AI%20agents%20act%20on%20behalf%20of%20and%20guide%20their%0Ause%20appropriately%2C%20protecting%20online%20spaces%20while%20unlocking%20the%20value%20of%20task%0Adelegation%20to%20autonomous%20agents.%20We%20introduce%20a%20novel%20framework%20for%0Aauthenticated%2C%20authorized%2C%20and%20auditable%20delegation%20of%20authority%20to%20AI%20agents%2C%0Awhere%20human%20users%20can%20securely%20delegate%20and%20restrict%20the%20permissions%20and%20scope%0Aof%20agents%20while%20maintaining%20clear%20chains%20of%20accountability.%20This%20framework%0Abuilds%20on%20existing%20identification%20and%20access%20management%20protocols%2C%20extending%0AOAuth%202.0%20and%20OpenID%20Connect%20with%20agent-specific%20credentials%20and%20metadata%2C%0Amaintaining%20compatibility%20with%20established%20authentication%20and%20web%0Ainfrastructure.%20Further%2C%20we%20propose%20a%20framework%20for%20translating%20flexible%2C%0Anatural%20language%20permissions%20into%20auditable%20access%20control%20configurations%2C%0Aenabling%20robust%20scoping%20of%20AI%20agent%20capabilities%20across%20diverse%20interaction%0Amodalities.%20Taken%20together%2C%20this%20practical%20approach%20facilitates%20immediate%0Adeployment%20of%20AI%20agents%20while%20addressing%20key%20security%20and%20accountability%0Aconcerns%2C%20working%20toward%20ensuring%20agentic%20AI%20systems%20perform%20only%20appropriate%0Aactions%20and%20providing%20a%20tool%20for%20digital%20service%20providers%20to%20enable%20AI%20agent%0Ainteractions%20without%20risking%20harm%20from%20scalable%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09674v1&entry.124074799=Read"},
{"title": "Dataset-Free Weight-Initialization on Restricted Boltzmann Machine", "author": "Muneki Yasuda and Ryosuke Maeno and Chako Takahashi", "abstract": "  In feed-forward neural networks, dataset-free weight-initialization methods\nsuch as LeCun, Xavier (or Glorot), and He initializations have been developed.\nThese methods randomly determine the initial values of weight parameters based\non specific distributions (e.g., Gaussian or uniform distributions) without\nusing training datasets. To the best of the authors' knowledge, such a\ndataset-free weight-initialization method is yet to be developed for restricted\nBoltzmann machines (RBMs), which are probabilistic neural networks consisting\nof two layers. In this study, we derive a dataset-free weight-initialization\nmethod for Bernoulli--Bernoulli RBMs based on statistical mechanical analysis.\nIn the proposed weight-initialization method, the weight parameters are drawn\nfrom a Gaussian distribution with zero mean. The standard deviation of the\nGaussian distribution is optimized based on our hypothesis that a standard\ndeviation providing a larger layer correlation (LC) between the two layers\nimproves the learning efficiency. The expression of the LC is derived based on\na statistical mechanical analysis. The optimal value of the standard deviation\ncorresponds to the maximum point of the LC. The proposed weight-initialization\nmethod is identical to Xavier initialization in a specific case (i.e., when the\nsizes of the two layers are the same, the random variables of the layers are\n$\\{-1,1\\}$-binary, and all bias parameters are zero). The validity of the\nproposed weight-initialization method is demonstrated in numerical experiments\nusing a toy and real-world datasets.\n", "link": "http://arxiv.org/abs/2409.07708v3", "date": "2025-01-16", "relevancy": 1.7293, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4392}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4373}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dataset-Free%20Weight-Initialization%20on%20Restricted%20Boltzmann%20Machine&body=Title%3A%20Dataset-Free%20Weight-Initialization%20on%20Restricted%20Boltzmann%20Machine%0AAuthor%3A%20Muneki%20Yasuda%20and%20Ryosuke%20Maeno%20and%20Chako%20Takahashi%0AAbstract%3A%20%20%20In%20feed-forward%20neural%20networks%2C%20dataset-free%20weight-initialization%20methods%0Asuch%20as%20LeCun%2C%20Xavier%20%28or%20Glorot%29%2C%20and%20He%20initializations%20have%20been%20developed.%0AThese%20methods%20randomly%20determine%20the%20initial%20values%20of%20weight%20parameters%20based%0Aon%20specific%20distributions%20%28e.g.%2C%20Gaussian%20or%20uniform%20distributions%29%20without%0Ausing%20training%20datasets.%20To%20the%20best%20of%20the%20authors%27%20knowledge%2C%20such%20a%0Adataset-free%20weight-initialization%20method%20is%20yet%20to%20be%20developed%20for%20restricted%0ABoltzmann%20machines%20%28RBMs%29%2C%20which%20are%20probabilistic%20neural%20networks%20consisting%0Aof%20two%20layers.%20In%20this%20study%2C%20we%20derive%20a%20dataset-free%20weight-initialization%0Amethod%20for%20Bernoulli--Bernoulli%20RBMs%20based%20on%20statistical%20mechanical%20analysis.%0AIn%20the%20proposed%20weight-initialization%20method%2C%20the%20weight%20parameters%20are%20drawn%0Afrom%20a%20Gaussian%20distribution%20with%20zero%20mean.%20The%20standard%20deviation%20of%20the%0AGaussian%20distribution%20is%20optimized%20based%20on%20our%20hypothesis%20that%20a%20standard%0Adeviation%20providing%20a%20larger%20layer%20correlation%20%28LC%29%20between%20the%20two%20layers%0Aimproves%20the%20learning%20efficiency.%20The%20expression%20of%20the%20LC%20is%20derived%20based%20on%0Aa%20statistical%20mechanical%20analysis.%20The%20optimal%20value%20of%20the%20standard%20deviation%0Acorresponds%20to%20the%20maximum%20point%20of%20the%20LC.%20The%20proposed%20weight-initialization%0Amethod%20is%20identical%20to%20Xavier%20initialization%20in%20a%20specific%20case%20%28i.e.%2C%20when%20the%0Asizes%20of%20the%20two%20layers%20are%20the%20same%2C%20the%20random%20variables%20of%20the%20layers%20are%0A%24%5C%7B-1%2C1%5C%7D%24-binary%2C%20and%20all%20bias%20parameters%20are%20zero%29.%20The%20validity%20of%20the%0Aproposed%20weight-initialization%20method%20is%20demonstrated%20in%20numerical%20experiments%0Ausing%20a%20toy%20and%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07708v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataset-Free%2520Weight-Initialization%2520on%2520Restricted%2520Boltzmann%2520Machine%26entry.906535625%3DMuneki%2520Yasuda%2520and%2520Ryosuke%2520Maeno%2520and%2520Chako%2520Takahashi%26entry.1292438233%3D%2520%2520In%2520feed-forward%2520neural%2520networks%252C%2520dataset-free%2520weight-initialization%2520methods%250Asuch%2520as%2520LeCun%252C%2520Xavier%2520%2528or%2520Glorot%2529%252C%2520and%2520He%2520initializations%2520have%2520been%2520developed.%250AThese%2520methods%2520randomly%2520determine%2520the%2520initial%2520values%2520of%2520weight%2520parameters%2520based%250Aon%2520specific%2520distributions%2520%2528e.g.%252C%2520Gaussian%2520or%2520uniform%2520distributions%2529%2520without%250Ausing%2520training%2520datasets.%2520To%2520the%2520best%2520of%2520the%2520authors%2527%2520knowledge%252C%2520such%2520a%250Adataset-free%2520weight-initialization%2520method%2520is%2520yet%2520to%2520be%2520developed%2520for%2520restricted%250ABoltzmann%2520machines%2520%2528RBMs%2529%252C%2520which%2520are%2520probabilistic%2520neural%2520networks%2520consisting%250Aof%2520two%2520layers.%2520In%2520this%2520study%252C%2520we%2520derive%2520a%2520dataset-free%2520weight-initialization%250Amethod%2520for%2520Bernoulli--Bernoulli%2520RBMs%2520based%2520on%2520statistical%2520mechanical%2520analysis.%250AIn%2520the%2520proposed%2520weight-initialization%2520method%252C%2520the%2520weight%2520parameters%2520are%2520drawn%250Afrom%2520a%2520Gaussian%2520distribution%2520with%2520zero%2520mean.%2520The%2520standard%2520deviation%2520of%2520the%250AGaussian%2520distribution%2520is%2520optimized%2520based%2520on%2520our%2520hypothesis%2520that%2520a%2520standard%250Adeviation%2520providing%2520a%2520larger%2520layer%2520correlation%2520%2528LC%2529%2520between%2520the%2520two%2520layers%250Aimproves%2520the%2520learning%2520efficiency.%2520The%2520expression%2520of%2520the%2520LC%2520is%2520derived%2520based%2520on%250Aa%2520statistical%2520mechanical%2520analysis.%2520The%2520optimal%2520value%2520of%2520the%2520standard%2520deviation%250Acorresponds%2520to%2520the%2520maximum%2520point%2520of%2520the%2520LC.%2520The%2520proposed%2520weight-initialization%250Amethod%2520is%2520identical%2520to%2520Xavier%2520initialization%2520in%2520a%2520specific%2520case%2520%2528i.e.%252C%2520when%2520the%250Asizes%2520of%2520the%2520two%2520layers%2520are%2520the%2520same%252C%2520the%2520random%2520variables%2520of%2520the%2520layers%2520are%250A%2524%255C%257B-1%252C1%255C%257D%2524-binary%252C%2520and%2520all%2520bias%2520parameters%2520are%2520zero%2529.%2520The%2520validity%2520of%2520the%250Aproposed%2520weight-initialization%2520method%2520is%2520demonstrated%2520in%2520numerical%2520experiments%250Ausing%2520a%2520toy%2520and%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07708v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dataset-Free%20Weight-Initialization%20on%20Restricted%20Boltzmann%20Machine&entry.906535625=Muneki%20Yasuda%20and%20Ryosuke%20Maeno%20and%20Chako%20Takahashi&entry.1292438233=%20%20In%20feed-forward%20neural%20networks%2C%20dataset-free%20weight-initialization%20methods%0Asuch%20as%20LeCun%2C%20Xavier%20%28or%20Glorot%29%2C%20and%20He%20initializations%20have%20been%20developed.%0AThese%20methods%20randomly%20determine%20the%20initial%20values%20of%20weight%20parameters%20based%0Aon%20specific%20distributions%20%28e.g.%2C%20Gaussian%20or%20uniform%20distributions%29%20without%0Ausing%20training%20datasets.%20To%20the%20best%20of%20the%20authors%27%20knowledge%2C%20such%20a%0Adataset-free%20weight-initialization%20method%20is%20yet%20to%20be%20developed%20for%20restricted%0ABoltzmann%20machines%20%28RBMs%29%2C%20which%20are%20probabilistic%20neural%20networks%20consisting%0Aof%20two%20layers.%20In%20this%20study%2C%20we%20derive%20a%20dataset-free%20weight-initialization%0Amethod%20for%20Bernoulli--Bernoulli%20RBMs%20based%20on%20statistical%20mechanical%20analysis.%0AIn%20the%20proposed%20weight-initialization%20method%2C%20the%20weight%20parameters%20are%20drawn%0Afrom%20a%20Gaussian%20distribution%20with%20zero%20mean.%20The%20standard%20deviation%20of%20the%0AGaussian%20distribution%20is%20optimized%20based%20on%20our%20hypothesis%20that%20a%20standard%0Adeviation%20providing%20a%20larger%20layer%20correlation%20%28LC%29%20between%20the%20two%20layers%0Aimproves%20the%20learning%20efficiency.%20The%20expression%20of%20the%20LC%20is%20derived%20based%20on%0Aa%20statistical%20mechanical%20analysis.%20The%20optimal%20value%20of%20the%20standard%20deviation%0Acorresponds%20to%20the%20maximum%20point%20of%20the%20LC.%20The%20proposed%20weight-initialization%0Amethod%20is%20identical%20to%20Xavier%20initialization%20in%20a%20specific%20case%20%28i.e.%2C%20when%20the%0Asizes%20of%20the%20two%20layers%20are%20the%20same%2C%20the%20random%20variables%20of%20the%20layers%20are%0A%24%5C%7B-1%2C1%5C%7D%24-binary%2C%20and%20all%20bias%20parameters%20are%20zero%29.%20The%20validity%20of%20the%0Aproposed%20weight-initialization%20method%20is%20demonstrated%20in%20numerical%20experiments%0Ausing%20a%20toy%20and%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07708v3&entry.124074799=Read"},
{"title": "Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents", "author": "Elizaveta Tennant and Stephen Hailes and Mirco Musolesi", "abstract": "  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.\n", "link": "http://arxiv.org/abs/2403.04202v7", "date": "2025-01-16", "relevancy": 1.4524, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4883}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4857}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamics%20of%20Moral%20Behavior%20in%20Heterogeneous%20Populations%20of%20Learning%0A%20%20Agents&body=Title%3A%20Dynamics%20of%20Moral%20Behavior%20in%20Heterogeneous%20Populations%20of%20Learning%0A%20%20Agents%0AAuthor%3A%20Elizaveta%20Tennant%20and%20Stephen%20Hailes%20and%20Mirco%20Musolesi%0AAbstract%3A%20%20%20Growing%20concerns%20about%20safety%20and%20alignment%20of%20AI%20systems%20highlight%20the%0Aimportance%20of%20embedding%20moral%20capabilities%20in%20artificial%20agents%3A%20a%20promising%0Asolution%20is%20the%20use%20of%20learning%20from%20experience%2C%20i.e.%2C%20Reinforcement%20Learning.%0AIn%20multi-agent%20%28social%29%20environments%2C%20complex%20population-level%20phenomena%20may%0Aemerge%20from%20interactions%20between%20individual%20learning%20agents.%20Many%20of%20the%0Aexisting%20studies%20rely%20on%20simulated%20social%20dilemma%20environments%20to%20study%20the%0Ainteractions%20of%20independent%20learning%20agents%3B%20however%2C%20they%20tend%20to%20ignore%20the%0Amoral%20heterogeneity%20that%20is%20likely%20to%20be%20present%20in%20societies%20of%20agents%20in%0Apractice.%20For%20example%2C%20at%20different%20points%20in%20time%20a%20single%20learning%20agent%20may%0Aface%20opponents%20who%20are%20consequentialist%20%28i.e.%2C%20focused%20on%20maximizing%20outcomes%0Aover%20time%29%2C%20norm-based%20%28i.e.%2C%20conforming%20to%20specific%20norms%29%2C%20or%20virtue-based%0A%28i.e.%2C%20considering%20a%20combination%20of%20different%20virtues%29.%20The%20extent%20to%20which%0Aagents%27%20co-development%20may%20be%20impacted%20by%20such%20moral%20heterogeneity%20in%0Apopulations%20is%20not%20well%20understood.%20In%20this%20paper%2C%20we%20present%20a%20study%20of%20the%0Alearning%20dynamics%20of%20morally%20heterogeneous%20populations%20interacting%20in%20a%20social%0Adilemma%20setting.%20Using%20an%20Iterated%20Prisoner%27s%20Dilemma%20environment%20with%20a%0Apartner%20selection%20mechanism%2C%20we%20investigate%20the%20extent%20to%20which%20the%20prevalence%0Aof%20diverse%20moral%20agents%20in%20populations%20affects%20individual%20agents%27%20learning%0Abehaviors%20and%20emergent%20population-level%20outcomes.%20We%20observe%20several%20types%20of%0Anon-trivial%20interactions%20between%20pro-social%20and%20anti-social%20agents%2C%20and%20find%0Athat%20certain%20types%20of%20moral%20agents%20are%20able%20to%20steer%20selfish%20agents%20towards%0Amore%20cooperative%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04202v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamics%2520of%2520Moral%2520Behavior%2520in%2520Heterogeneous%2520Populations%2520of%2520Learning%250A%2520%2520Agents%26entry.906535625%3DElizaveta%2520Tennant%2520and%2520Stephen%2520Hailes%2520and%2520Mirco%2520Musolesi%26entry.1292438233%3D%2520%2520Growing%2520concerns%2520about%2520safety%2520and%2520alignment%2520of%2520AI%2520systems%2520highlight%2520the%250Aimportance%2520of%2520embedding%2520moral%2520capabilities%2520in%2520artificial%2520agents%253A%2520a%2520promising%250Asolution%2520is%2520the%2520use%2520of%2520learning%2520from%2520experience%252C%2520i.e.%252C%2520Reinforcement%2520Learning.%250AIn%2520multi-agent%2520%2528social%2529%2520environments%252C%2520complex%2520population-level%2520phenomena%2520may%250Aemerge%2520from%2520interactions%2520between%2520individual%2520learning%2520agents.%2520Many%2520of%2520the%250Aexisting%2520studies%2520rely%2520on%2520simulated%2520social%2520dilemma%2520environments%2520to%2520study%2520the%250Ainteractions%2520of%2520independent%2520learning%2520agents%253B%2520however%252C%2520they%2520tend%2520to%2520ignore%2520the%250Amoral%2520heterogeneity%2520that%2520is%2520likely%2520to%2520be%2520present%2520in%2520societies%2520of%2520agents%2520in%250Apractice.%2520For%2520example%252C%2520at%2520different%2520points%2520in%2520time%2520a%2520single%2520learning%2520agent%2520may%250Aface%2520opponents%2520who%2520are%2520consequentialist%2520%2528i.e.%252C%2520focused%2520on%2520maximizing%2520outcomes%250Aover%2520time%2529%252C%2520norm-based%2520%2528i.e.%252C%2520conforming%2520to%2520specific%2520norms%2529%252C%2520or%2520virtue-based%250A%2528i.e.%252C%2520considering%2520a%2520combination%2520of%2520different%2520virtues%2529.%2520The%2520extent%2520to%2520which%250Aagents%2527%2520co-development%2520may%2520be%2520impacted%2520by%2520such%2520moral%2520heterogeneity%2520in%250Apopulations%2520is%2520not%2520well%2520understood.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520study%2520of%2520the%250Alearning%2520dynamics%2520of%2520morally%2520heterogeneous%2520populations%2520interacting%2520in%2520a%2520social%250Adilemma%2520setting.%2520Using%2520an%2520Iterated%2520Prisoner%2527s%2520Dilemma%2520environment%2520with%2520a%250Apartner%2520selection%2520mechanism%252C%2520we%2520investigate%2520the%2520extent%2520to%2520which%2520the%2520prevalence%250Aof%2520diverse%2520moral%2520agents%2520in%2520populations%2520affects%2520individual%2520agents%2527%2520learning%250Abehaviors%2520and%2520emergent%2520population-level%2520outcomes.%2520We%2520observe%2520several%2520types%2520of%250Anon-trivial%2520interactions%2520between%2520pro-social%2520and%2520anti-social%2520agents%252C%2520and%2520find%250Athat%2520certain%2520types%2520of%2520moral%2520agents%2520are%2520able%2520to%2520steer%2520selfish%2520agents%2520towards%250Amore%2520cooperative%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04202v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamics%20of%20Moral%20Behavior%20in%20Heterogeneous%20Populations%20of%20Learning%0A%20%20Agents&entry.906535625=Elizaveta%20Tennant%20and%20Stephen%20Hailes%20and%20Mirco%20Musolesi&entry.1292438233=%20%20Growing%20concerns%20about%20safety%20and%20alignment%20of%20AI%20systems%20highlight%20the%0Aimportance%20of%20embedding%20moral%20capabilities%20in%20artificial%20agents%3A%20a%20promising%0Asolution%20is%20the%20use%20of%20learning%20from%20experience%2C%20i.e.%2C%20Reinforcement%20Learning.%0AIn%20multi-agent%20%28social%29%20environments%2C%20complex%20population-level%20phenomena%20may%0Aemerge%20from%20interactions%20between%20individual%20learning%20agents.%20Many%20of%20the%0Aexisting%20studies%20rely%20on%20simulated%20social%20dilemma%20environments%20to%20study%20the%0Ainteractions%20of%20independent%20learning%20agents%3B%20however%2C%20they%20tend%20to%20ignore%20the%0Amoral%20heterogeneity%20that%20is%20likely%20to%20be%20present%20in%20societies%20of%20agents%20in%0Apractice.%20For%20example%2C%20at%20different%20points%20in%20time%20a%20single%20learning%20agent%20may%0Aface%20opponents%20who%20are%20consequentialist%20%28i.e.%2C%20focused%20on%20maximizing%20outcomes%0Aover%20time%29%2C%20norm-based%20%28i.e.%2C%20conforming%20to%20specific%20norms%29%2C%20or%20virtue-based%0A%28i.e.%2C%20considering%20a%20combination%20of%20different%20virtues%29.%20The%20extent%20to%20which%0Aagents%27%20co-development%20may%20be%20impacted%20by%20such%20moral%20heterogeneity%20in%0Apopulations%20is%20not%20well%20understood.%20In%20this%20paper%2C%20we%20present%20a%20study%20of%20the%0Alearning%20dynamics%20of%20morally%20heterogeneous%20populations%20interacting%20in%20a%20social%0Adilemma%20setting.%20Using%20an%20Iterated%20Prisoner%27s%20Dilemma%20environment%20with%20a%0Apartner%20selection%20mechanism%2C%20we%20investigate%20the%20extent%20to%20which%20the%20prevalence%0Aof%20diverse%20moral%20agents%20in%20populations%20affects%20individual%20agents%27%20learning%0Abehaviors%20and%20emergent%20population-level%20outcomes.%20We%20observe%20several%20types%20of%0Anon-trivial%20interactions%20between%20pro-social%20and%20anti-social%20agents%2C%20and%20find%0Athat%20certain%20types%20of%20moral%20agents%20are%20able%20to%20steer%20selfish%20agents%20towards%0Amore%20cooperative%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04202v7&entry.124074799=Read"},
{"title": "On the Relation between Optical Aperture and Automotive Object Detection", "author": "Ofer Bar-Shalom and Tzvi Philipp and Eran Kishon", "abstract": "  We explore the impact of aperture size and shape on automotive camera systems\nfor deep-learning-based tasks like traffic sign recognition and light state\ndetection. A method is proposed to simulate optical effects using the point\nspread function (PSF), enhancing realism and reducing the domain gap between\nsynthetic and real-world images. Computer-generated scenes are refined with\nthis technique to model optical distortions and improve simulation accuracy.\n", "link": "http://arxiv.org/abs/2501.09456v1", "date": "2025-01-16", "relevancy": 1.4804, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5039}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4933}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Relation%20between%20Optical%20Aperture%20and%20Automotive%20Object%20Detection&body=Title%3A%20On%20the%20Relation%20between%20Optical%20Aperture%20and%20Automotive%20Object%20Detection%0AAuthor%3A%20Ofer%20Bar-Shalom%20and%20Tzvi%20Philipp%20and%20Eran%20Kishon%0AAbstract%3A%20%20%20We%20explore%20the%20impact%20of%20aperture%20size%20and%20shape%20on%20automotive%20camera%20systems%0Afor%20deep-learning-based%20tasks%20like%20traffic%20sign%20recognition%20and%20light%20state%0Adetection.%20A%20method%20is%20proposed%20to%20simulate%20optical%20effects%20using%20the%20point%0Aspread%20function%20%28PSF%29%2C%20enhancing%20realism%20and%20reducing%20the%20domain%20gap%20between%0Asynthetic%20and%20real-world%20images.%20Computer-generated%20scenes%20are%20refined%20with%0Athis%20technique%20to%20model%20optical%20distortions%20and%20improve%20simulation%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Relation%2520between%2520Optical%2520Aperture%2520and%2520Automotive%2520Object%2520Detection%26entry.906535625%3DOfer%2520Bar-Shalom%2520and%2520Tzvi%2520Philipp%2520and%2520Eran%2520Kishon%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520impact%2520of%2520aperture%2520size%2520and%2520shape%2520on%2520automotive%2520camera%2520systems%250Afor%2520deep-learning-based%2520tasks%2520like%2520traffic%2520sign%2520recognition%2520and%2520light%2520state%250Adetection.%2520A%2520method%2520is%2520proposed%2520to%2520simulate%2520optical%2520effects%2520using%2520the%2520point%250Aspread%2520function%2520%2528PSF%2529%252C%2520enhancing%2520realism%2520and%2520reducing%2520the%2520domain%2520gap%2520between%250Asynthetic%2520and%2520real-world%2520images.%2520Computer-generated%2520scenes%2520are%2520refined%2520with%250Athis%2520technique%2520to%2520model%2520optical%2520distortions%2520and%2520improve%2520simulation%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Relation%20between%20Optical%20Aperture%20and%20Automotive%20Object%20Detection&entry.906535625=Ofer%20Bar-Shalom%20and%20Tzvi%20Philipp%20and%20Eran%20Kishon&entry.1292438233=%20%20We%20explore%20the%20impact%20of%20aperture%20size%20and%20shape%20on%20automotive%20camera%20systems%0Afor%20deep-learning-based%20tasks%20like%20traffic%20sign%20recognition%20and%20light%20state%0Adetection.%20A%20method%20is%20proposed%20to%20simulate%20optical%20effects%20using%20the%20point%0Aspread%20function%20%28PSF%29%2C%20enhancing%20realism%20and%20reducing%20the%20domain%20gap%20between%0Asynthetic%20and%20real-world%20images.%20Computer-generated%20scenes%20are%20refined%20with%0Athis%20technique%20to%20model%20optical%20distortions%20and%20improve%20simulation%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09456v1&entry.124074799=Read"},
{"title": "Frechet Music Distance: A Metric For Generative Symbolic Music\n  Evaluation", "author": "Jan Retkowski and Jakub St\u0119pniak and Mateusz Modrzejewski", "abstract": "  In this paper we introduce the Frechet Music Distance (FMD), a novel\nevaluation metric for generative symbolic music models, inspired by the Frechet\nInception Distance (FID) in computer vision and Frechet Audio Distance (FAD) in\ngenerative audio. FMD calculates the distance between distributions of\nreference and generated symbolic music embeddings, capturing abstract musical\nfeatures. We validate FMD across several datasets and models. Results indicate\nthat FMD effectively differentiates model quality, providing a domain-specific\nmetric for evaluating symbolic music generation, and establishing a\nreproducible standard for future research in symbolic music modeling.\n", "link": "http://arxiv.org/abs/2412.07948v2", "date": "2025-01-16", "relevancy": 1.8124, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4591}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4585}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frechet%20Music%20Distance%3A%20A%20Metric%20For%20Generative%20Symbolic%20Music%0A%20%20Evaluation&body=Title%3A%20Frechet%20Music%20Distance%3A%20A%20Metric%20For%20Generative%20Symbolic%20Music%0A%20%20Evaluation%0AAuthor%3A%20Jan%20Retkowski%20and%20Jakub%20St%C4%99pniak%20and%20Mateusz%20Modrzejewski%0AAbstract%3A%20%20%20In%20this%20paper%20we%20introduce%20the%20Frechet%20Music%20Distance%20%28FMD%29%2C%20a%20novel%0Aevaluation%20metric%20for%20generative%20symbolic%20music%20models%2C%20inspired%20by%20the%20Frechet%0AInception%20Distance%20%28FID%29%20in%20computer%20vision%20and%20Frechet%20Audio%20Distance%20%28FAD%29%20in%0Agenerative%20audio.%20FMD%20calculates%20the%20distance%20between%20distributions%20of%0Areference%20and%20generated%20symbolic%20music%20embeddings%2C%20capturing%20abstract%20musical%0Afeatures.%20We%20validate%20FMD%20across%20several%20datasets%20and%20models.%20Results%20indicate%0Athat%20FMD%20effectively%20differentiates%20model%20quality%2C%20providing%20a%20domain-specific%0Ametric%20for%20evaluating%20symbolic%20music%20generation%2C%20and%20establishing%20a%0Areproducible%20standard%20for%20future%20research%20in%20symbolic%20music%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07948v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrechet%2520Music%2520Distance%253A%2520A%2520Metric%2520For%2520Generative%2520Symbolic%2520Music%250A%2520%2520Evaluation%26entry.906535625%3DJan%2520Retkowski%2520and%2520Jakub%2520St%25C4%2599pniak%2520and%2520Mateusz%2520Modrzejewski%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520introduce%2520the%2520Frechet%2520Music%2520Distance%2520%2528FMD%2529%252C%2520a%2520novel%250Aevaluation%2520metric%2520for%2520generative%2520symbolic%2520music%2520models%252C%2520inspired%2520by%2520the%2520Frechet%250AInception%2520Distance%2520%2528FID%2529%2520in%2520computer%2520vision%2520and%2520Frechet%2520Audio%2520Distance%2520%2528FAD%2529%2520in%250Agenerative%2520audio.%2520FMD%2520calculates%2520the%2520distance%2520between%2520distributions%2520of%250Areference%2520and%2520generated%2520symbolic%2520music%2520embeddings%252C%2520capturing%2520abstract%2520musical%250Afeatures.%2520We%2520validate%2520FMD%2520across%2520several%2520datasets%2520and%2520models.%2520Results%2520indicate%250Athat%2520FMD%2520effectively%2520differentiates%2520model%2520quality%252C%2520providing%2520a%2520domain-specific%250Ametric%2520for%2520evaluating%2520symbolic%2520music%2520generation%252C%2520and%2520establishing%2520a%250Areproducible%2520standard%2520for%2520future%2520research%2520in%2520symbolic%2520music%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07948v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frechet%20Music%20Distance%3A%20A%20Metric%20For%20Generative%20Symbolic%20Music%0A%20%20Evaluation&entry.906535625=Jan%20Retkowski%20and%20Jakub%20St%C4%99pniak%20and%20Mateusz%20Modrzejewski&entry.1292438233=%20%20In%20this%20paper%20we%20introduce%20the%20Frechet%20Music%20Distance%20%28FMD%29%2C%20a%20novel%0Aevaluation%20metric%20for%20generative%20symbolic%20music%20models%2C%20inspired%20by%20the%20Frechet%0AInception%20Distance%20%28FID%29%20in%20computer%20vision%20and%20Frechet%20Audio%20Distance%20%28FAD%29%20in%0Agenerative%20audio.%20FMD%20calculates%20the%20distance%20between%20distributions%20of%0Areference%20and%20generated%20symbolic%20music%20embeddings%2C%20capturing%20abstract%20musical%0Afeatures.%20We%20validate%20FMD%20across%20several%20datasets%20and%20models.%20Results%20indicate%0Athat%20FMD%20effectively%20differentiates%20model%20quality%2C%20providing%20a%20domain-specific%0Ametric%20for%20evaluating%20symbolic%20music%20generation%2C%20and%20establishing%20a%0Areproducible%20standard%20for%20future%20research%20in%20symbolic%20music%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07948v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


