<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250813.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing,\n  and Generation", "author": "Shuting He and Peilin Ji and Yitong Yang and Changshuo Wang and Jiayi Ji and Yinglin Wang and Henghui Ding", "abstract": "  3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative\nto Neural Radiance Fields (NeRF) for 3D scene representation, offering\nhigh-fidelity photorealistic rendering with real-time performance. Beyond novel\nview synthesis, the explicit and compact nature of 3DGS enables a wide range of\ndownstream applications that require geometric and semantic understanding. This\nsurvey provides a comprehensive overview of recent progress in 3DGS\napplications. It first introduces 2D foundation models that support semantic\nunderstanding and control in 3DGS applications, followed by a review of\nNeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS\napplications into segmentation, editing, generation, and other functional\ntasks. For each, we summarize representative methods, supervision strategies,\nand learning paradigms, highlighting shared design principles and emerging\ntrends. Commonly used datasets and evaluation protocols are also summarized,\nalong with comparative analyses of recent methods across public benchmarks. To\nsupport ongoing research and development, a continually updated repository of\npapers, code, and resources is maintained at\nhttps://github.com/heshuting555/Awesome-3DGS-Applications.\n", "link": "http://arxiv.org/abs/2508.09977v1", "date": "2025-08-13", "relevancy": 3.4332, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7128}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6752}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%203D%20Gaussian%20Splatting%20Applications%3A%20Segmentation%2C%20Editing%2C%0A%20%20and%20Generation&body=Title%3A%20A%20Survey%20on%203D%20Gaussian%20Splatting%20Applications%3A%20Segmentation%2C%20Editing%2C%0A%20%20and%20Generation%0AAuthor%3A%20Shuting%20He%20and%20Peilin%20Ji%20and%20Yitong%20Yang%20and%20Changshuo%20Wang%20and%20Jiayi%20Ji%20and%20Yinglin%20Wang%20and%20Henghui%20Ding%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20emerged%20as%20a%20powerful%20alternative%0Ato%20Neural%20Radiance%20Fields%20%28NeRF%29%20for%203D%20scene%20representation%2C%20offering%0Ahigh-fidelity%20photorealistic%20rendering%20with%20real-time%20performance.%20Beyond%20novel%0Aview%20synthesis%2C%20the%20explicit%20and%20compact%20nature%20of%203DGS%20enables%20a%20wide%20range%20of%0Adownstream%20applications%20that%20require%20geometric%20and%20semantic%20understanding.%20This%0Asurvey%20provides%20a%20comprehensive%20overview%20of%20recent%20progress%20in%203DGS%0Aapplications.%20It%20first%20introduces%202D%20foundation%20models%20that%20support%20semantic%0Aunderstanding%20and%20control%20in%203DGS%20applications%2C%20followed%20by%20a%20review%20of%0ANeRF-based%20methods%20that%20inform%20their%203DGS%20counterparts.%20We%20then%20categorize%203DGS%0Aapplications%20into%20segmentation%2C%20editing%2C%20generation%2C%20and%20other%20functional%0Atasks.%20For%20each%2C%20we%20summarize%20representative%20methods%2C%20supervision%20strategies%2C%0Aand%20learning%20paradigms%2C%20highlighting%20shared%20design%20principles%20and%20emerging%0Atrends.%20Commonly%20used%20datasets%20and%20evaluation%20protocols%20are%20also%20summarized%2C%0Aalong%20with%20comparative%20analyses%20of%20recent%20methods%20across%20public%20benchmarks.%20To%0Asupport%20ongoing%20research%20and%20development%2C%20a%20continually%20updated%20repository%20of%0Apapers%2C%20code%2C%20and%20resources%20is%20maintained%20at%0Ahttps%3A//github.com/heshuting555/Awesome-3DGS-Applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%25203D%2520Gaussian%2520Splatting%2520Applications%253A%2520Segmentation%252C%2520Editing%252C%250A%2520%2520and%2520Generation%26entry.906535625%3DShuting%2520He%2520and%2520Peilin%2520Ji%2520and%2520Yitong%2520Yang%2520and%2520Changshuo%2520Wang%2520and%2520Jiayi%2520Ji%2520and%2520Yinglin%2520Wang%2520and%2520Henghui%2520Ding%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520emerged%2520as%2520a%2520powerful%2520alternative%250Ato%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520for%25203D%2520scene%2520representation%252C%2520offering%250Ahigh-fidelity%2520photorealistic%2520rendering%2520with%2520real-time%2520performance.%2520Beyond%2520novel%250Aview%2520synthesis%252C%2520the%2520explicit%2520and%2520compact%2520nature%2520of%25203DGS%2520enables%2520a%2520wide%2520range%2520of%250Adownstream%2520applications%2520that%2520require%2520geometric%2520and%2520semantic%2520understanding.%2520This%250Asurvey%2520provides%2520a%2520comprehensive%2520overview%2520of%2520recent%2520progress%2520in%25203DGS%250Aapplications.%2520It%2520first%2520introduces%25202D%2520foundation%2520models%2520that%2520support%2520semantic%250Aunderstanding%2520and%2520control%2520in%25203DGS%2520applications%252C%2520followed%2520by%2520a%2520review%2520of%250ANeRF-based%2520methods%2520that%2520inform%2520their%25203DGS%2520counterparts.%2520We%2520then%2520categorize%25203DGS%250Aapplications%2520into%2520segmentation%252C%2520editing%252C%2520generation%252C%2520and%2520other%2520functional%250Atasks.%2520For%2520each%252C%2520we%2520summarize%2520representative%2520methods%252C%2520supervision%2520strategies%252C%250Aand%2520learning%2520paradigms%252C%2520highlighting%2520shared%2520design%2520principles%2520and%2520emerging%250Atrends.%2520Commonly%2520used%2520datasets%2520and%2520evaluation%2520protocols%2520are%2520also%2520summarized%252C%250Aalong%2520with%2520comparative%2520analyses%2520of%2520recent%2520methods%2520across%2520public%2520benchmarks.%2520To%250Asupport%2520ongoing%2520research%2520and%2520development%252C%2520a%2520continually%2520updated%2520repository%2520of%250Apapers%252C%2520code%252C%2520and%2520resources%2520is%2520maintained%2520at%250Ahttps%253A//github.com/heshuting555/Awesome-3DGS-Applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%203D%20Gaussian%20Splatting%20Applications%3A%20Segmentation%2C%20Editing%2C%0A%20%20and%20Generation&entry.906535625=Shuting%20He%20and%20Peilin%20Ji%20and%20Yitong%20Yang%20and%20Changshuo%20Wang%20and%20Jiayi%20Ji%20and%20Yinglin%20Wang%20and%20Henghui%20Ding&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20emerged%20as%20a%20powerful%20alternative%0Ato%20Neural%20Radiance%20Fields%20%28NeRF%29%20for%203D%20scene%20representation%2C%20offering%0Ahigh-fidelity%20photorealistic%20rendering%20with%20real-time%20performance.%20Beyond%20novel%0Aview%20synthesis%2C%20the%20explicit%20and%20compact%20nature%20of%203DGS%20enables%20a%20wide%20range%20of%0Adownstream%20applications%20that%20require%20geometric%20and%20semantic%20understanding.%20This%0Asurvey%20provides%20a%20comprehensive%20overview%20of%20recent%20progress%20in%203DGS%0Aapplications.%20It%20first%20introduces%202D%20foundation%20models%20that%20support%20semantic%0Aunderstanding%20and%20control%20in%203DGS%20applications%2C%20followed%20by%20a%20review%20of%0ANeRF-based%20methods%20that%20inform%20their%203DGS%20counterparts.%20We%20then%20categorize%203DGS%0Aapplications%20into%20segmentation%2C%20editing%2C%20generation%2C%20and%20other%20functional%0Atasks.%20For%20each%2C%20we%20summarize%20representative%20methods%2C%20supervision%20strategies%2C%0Aand%20learning%20paradigms%2C%20highlighting%20shared%20design%20principles%20and%20emerging%0Atrends.%20Commonly%20used%20datasets%20and%20evaluation%20protocols%20are%20also%20summarized%2C%0Aalong%20with%20comparative%20analyses%20of%20recent%20methods%20across%20public%20benchmarks.%20To%0Asupport%20ongoing%20research%20and%20development%2C%20a%20continually%20updated%20repository%20of%0Apapers%2C%20code%2C%20and%20resources%20is%20maintained%20at%0Ahttps%3A//github.com/heshuting555/Awesome-3DGS-Applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09977v1&entry.124074799=Read"},
{"title": "Multi-view Normal and Distance Guidance Gaussian Splatting for Surface\n  Reconstruction", "author": "Bo Jia and Yanan Guo and Ying Chang and Benkui Zhang and Ying Xie and Kangning Du and Lin Cao", "abstract": "  3D Gaussian Splatting (3DGS) achieves remarkable results in the field of\nsurface reconstruction. However, when Gaussian normal vectors are aligned\nwithin the single-view projection plane, while the geometry appears reasonable\nin the current view, biases may emerge upon switching to nearby views. To\naddress the distance and global matching challenges in multi-view scenes, we\ndesign multi-view normal and distance-guided Gaussian splatting. This method\nachieves geometric depth unification and high-accuracy reconstruction by\nconstraining nearby depth maps and aligning 3D normals. Specifically, for the\nreconstruction of small indoor and outdoor scenes, we propose a multi-view\ndistance reprojection regularization module that achieves multi-view Gaussian\nalignment by computing the distance loss between two nearby views and the same\nGaussian surface. Additionally, we develop a multi-view normal enhancement\nmodule, which ensures consistency across views by matching the normals of pixel\npoints in nearby views and calculating the loss. Extensive experimental results\ndemonstrate that our method outperforms the baseline in both quantitative and\nqualitative evaluations, significantly enhancing the surface reconstruction\ncapability of 3DGS. Our code will be made publicly available at\n(https://github.com/Bistu3DV/MND-GS/).\n", "link": "http://arxiv.org/abs/2508.07701v2", "date": "2025-08-13", "relevancy": 3.4145, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7016}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6764}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Normal%20and%20Distance%20Guidance%20Gaussian%20Splatting%20for%20Surface%0A%20%20Reconstruction&body=Title%3A%20Multi-view%20Normal%20and%20Distance%20Guidance%20Gaussian%20Splatting%20for%20Surface%0A%20%20Reconstruction%0AAuthor%3A%20Bo%20Jia%20and%20Yanan%20Guo%20and%20Ying%20Chang%20and%20Benkui%20Zhang%20and%20Ying%20Xie%20and%20Kangning%20Du%20and%20Lin%20Cao%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20achieves%20remarkable%20results%20in%20the%20field%20of%0Asurface%20reconstruction.%20However%2C%20when%20Gaussian%20normal%20vectors%20are%20aligned%0Awithin%20the%20single-view%20projection%20plane%2C%20while%20the%20geometry%20appears%20reasonable%0Ain%20the%20current%20view%2C%20biases%20may%20emerge%20upon%20switching%20to%20nearby%20views.%20To%0Aaddress%20the%20distance%20and%20global%20matching%20challenges%20in%20multi-view%20scenes%2C%20we%0Adesign%20multi-view%20normal%20and%20distance-guided%20Gaussian%20splatting.%20This%20method%0Aachieves%20geometric%20depth%20unification%20and%20high-accuracy%20reconstruction%20by%0Aconstraining%20nearby%20depth%20maps%20and%20aligning%203D%20normals.%20Specifically%2C%20for%20the%0Areconstruction%20of%20small%20indoor%20and%20outdoor%20scenes%2C%20we%20propose%20a%20multi-view%0Adistance%20reprojection%20regularization%20module%20that%20achieves%20multi-view%20Gaussian%0Aalignment%20by%20computing%20the%20distance%20loss%20between%20two%20nearby%20views%20and%20the%20same%0AGaussian%20surface.%20Additionally%2C%20we%20develop%20a%20multi-view%20normal%20enhancement%0Amodule%2C%20which%20ensures%20consistency%20across%20views%20by%20matching%20the%20normals%20of%20pixel%0Apoints%20in%20nearby%20views%20and%20calculating%20the%20loss.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20method%20outperforms%20the%20baseline%20in%20both%20quantitative%20and%0Aqualitative%20evaluations%2C%20significantly%20enhancing%20the%20surface%20reconstruction%0Acapability%20of%203DGS.%20Our%20code%20will%20be%20made%20publicly%20available%20at%0A%28https%3A//github.com/Bistu3DV/MND-GS/%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520Normal%2520and%2520Distance%2520Guidance%2520Gaussian%2520Splatting%2520for%2520Surface%250A%2520%2520Reconstruction%26entry.906535625%3DBo%2520Jia%2520and%2520Yanan%2520Guo%2520and%2520Ying%2520Chang%2520and%2520Benkui%2520Zhang%2520and%2520Ying%2520Xie%2520and%2520Kangning%2520Du%2520and%2520Lin%2520Cao%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520achieves%2520remarkable%2520results%2520in%2520the%2520field%2520of%250Asurface%2520reconstruction.%2520However%252C%2520when%2520Gaussian%2520normal%2520vectors%2520are%2520aligned%250Awithin%2520the%2520single-view%2520projection%2520plane%252C%2520while%2520the%2520geometry%2520appears%2520reasonable%250Ain%2520the%2520current%2520view%252C%2520biases%2520may%2520emerge%2520upon%2520switching%2520to%2520nearby%2520views.%2520To%250Aaddress%2520the%2520distance%2520and%2520global%2520matching%2520challenges%2520in%2520multi-view%2520scenes%252C%2520we%250Adesign%2520multi-view%2520normal%2520and%2520distance-guided%2520Gaussian%2520splatting.%2520This%2520method%250Aachieves%2520geometric%2520depth%2520unification%2520and%2520high-accuracy%2520reconstruction%2520by%250Aconstraining%2520nearby%2520depth%2520maps%2520and%2520aligning%25203D%2520normals.%2520Specifically%252C%2520for%2520the%250Areconstruction%2520of%2520small%2520indoor%2520and%2520outdoor%2520scenes%252C%2520we%2520propose%2520a%2520multi-view%250Adistance%2520reprojection%2520regularization%2520module%2520that%2520achieves%2520multi-view%2520Gaussian%250Aalignment%2520by%2520computing%2520the%2520distance%2520loss%2520between%2520two%2520nearby%2520views%2520and%2520the%2520same%250AGaussian%2520surface.%2520Additionally%252C%2520we%2520develop%2520a%2520multi-view%2520normal%2520enhancement%250Amodule%252C%2520which%2520ensures%2520consistency%2520across%2520views%2520by%2520matching%2520the%2520normals%2520of%2520pixel%250Apoints%2520in%2520nearby%2520views%2520and%2520calculating%2520the%2520loss.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520the%2520baseline%2520in%2520both%2520quantitative%2520and%250Aqualitative%2520evaluations%252C%2520significantly%2520enhancing%2520the%2520surface%2520reconstruction%250Acapability%2520of%25203DGS.%2520Our%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%250A%2528https%253A//github.com/Bistu3DV/MND-GS/%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Normal%20and%20Distance%20Guidance%20Gaussian%20Splatting%20for%20Surface%0A%20%20Reconstruction&entry.906535625=Bo%20Jia%20and%20Yanan%20Guo%20and%20Ying%20Chang%20and%20Benkui%20Zhang%20and%20Ying%20Xie%20and%20Kangning%20Du%20and%20Lin%20Cao&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20achieves%20remarkable%20results%20in%20the%20field%20of%0Asurface%20reconstruction.%20However%2C%20when%20Gaussian%20normal%20vectors%20are%20aligned%0Awithin%20the%20single-view%20projection%20plane%2C%20while%20the%20geometry%20appears%20reasonable%0Ain%20the%20current%20view%2C%20biases%20may%20emerge%20upon%20switching%20to%20nearby%20views.%20To%0Aaddress%20the%20distance%20and%20global%20matching%20challenges%20in%20multi-view%20scenes%2C%20we%0Adesign%20multi-view%20normal%20and%20distance-guided%20Gaussian%20splatting.%20This%20method%0Aachieves%20geometric%20depth%20unification%20and%20high-accuracy%20reconstruction%20by%0Aconstraining%20nearby%20depth%20maps%20and%20aligning%203D%20normals.%20Specifically%2C%20for%20the%0Areconstruction%20of%20small%20indoor%20and%20outdoor%20scenes%2C%20we%20propose%20a%20multi-view%0Adistance%20reprojection%20regularization%20module%20that%20achieves%20multi-view%20Gaussian%0Aalignment%20by%20computing%20the%20distance%20loss%20between%20two%20nearby%20views%20and%20the%20same%0AGaussian%20surface.%20Additionally%2C%20we%20develop%20a%20multi-view%20normal%20enhancement%0Amodule%2C%20which%20ensures%20consistency%20across%20views%20by%20matching%20the%20normals%20of%20pixel%0Apoints%20in%20nearby%20views%20and%20calculating%20the%20loss.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20method%20outperforms%20the%20baseline%20in%20both%20quantitative%20and%0Aqualitative%20evaluations%2C%20significantly%20enhancing%20the%20surface%20reconstruction%0Acapability%20of%203DGS.%20Our%20code%20will%20be%20made%20publicly%20available%20at%0A%28https%3A//github.com/Bistu3DV/MND-GS/%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07701v2&entry.124074799=Read"},
{"title": "HumanGenesis: Agent-Based Geometric and Generative Modeling for\n  Synthetic Human Dynamics", "author": "Weiqi Li and Zehao Zhang and Liang Lin and Guangrun Wang", "abstract": "  \\textbf{Synthetic human dynamics} aims to generate photorealistic videos of\nhuman subjects performing expressive, intention-driven motions. However,\ncurrent approaches face two core challenges: (1) \\emph{geometric inconsistency}\nand \\emph{coarse reconstruction}, due to limited 3D modeling and detail\npreservation; and (2) \\emph{motion generalization limitations} and \\emph{scene\ninharmonization}, stemming from weak generative capabilities. To address these,\nwe present \\textbf{HumanGenesis}, a framework that integrates geometric and\ngenerative modeling through four collaborative agents: (1)\n\\textbf{Reconstructor} builds 3D-consistent human-scene representations from\nmonocular video using 3D Gaussian Splatting and deformation decomposition. (2)\n\\textbf{Critique Agent} enhances reconstruction fidelity by identifying and\nrefining poor regions via multi-round MLLM-based reflection. (3) \\textbf{Pose\nGuider} enables motion generalization by generating expressive pose sequences\nusing time-aware parametric encoders. (4) \\textbf{Video Harmonizer} synthesizes\nphotorealistic, coherent video via a hybrid rendering pipeline with diffusion,\nrefining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis\nachieves state-of-the-art performance on tasks including text-guided synthesis,\nvideo reenactment, and novel-pose generalization, significantly improving\nexpressiveness, geometric fidelity, and scene integration.\n", "link": "http://arxiv.org/abs/2508.09858v1", "date": "2025-08-13", "relevancy": 3.3852, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7158}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6726}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanGenesis%3A%20Agent-Based%20Geometric%20and%20Generative%20Modeling%20for%0A%20%20Synthetic%20Human%20Dynamics&body=Title%3A%20HumanGenesis%3A%20Agent-Based%20Geometric%20and%20Generative%20Modeling%20for%0A%20%20Synthetic%20Human%20Dynamics%0AAuthor%3A%20Weiqi%20Li%20and%20Zehao%20Zhang%20and%20Liang%20Lin%20and%20Guangrun%20Wang%0AAbstract%3A%20%20%20%5Ctextbf%7BSynthetic%20human%20dynamics%7D%20aims%20to%20generate%20photorealistic%20videos%20of%0Ahuman%20subjects%20performing%20expressive%2C%20intention-driven%20motions.%20However%2C%0Acurrent%20approaches%20face%20two%20core%20challenges%3A%20%281%29%20%5Cemph%7Bgeometric%20inconsistency%7D%0Aand%20%5Cemph%7Bcoarse%20reconstruction%7D%2C%20due%20to%20limited%203D%20modeling%20and%20detail%0Apreservation%3B%20and%20%282%29%20%5Cemph%7Bmotion%20generalization%20limitations%7D%20and%20%5Cemph%7Bscene%0Ainharmonization%7D%2C%20stemming%20from%20weak%20generative%20capabilities.%20To%20address%20these%2C%0Awe%20present%20%5Ctextbf%7BHumanGenesis%7D%2C%20a%20framework%20that%20integrates%20geometric%20and%0Agenerative%20modeling%20through%20four%20collaborative%20agents%3A%20%281%29%0A%5Ctextbf%7BReconstructor%7D%20builds%203D-consistent%20human-scene%20representations%20from%0Amonocular%20video%20using%203D%20Gaussian%20Splatting%20and%20deformation%20decomposition.%20%282%29%0A%5Ctextbf%7BCritique%20Agent%7D%20enhances%20reconstruction%20fidelity%20by%20identifying%20and%0Arefining%20poor%20regions%20via%20multi-round%20MLLM-based%20reflection.%20%283%29%20%5Ctextbf%7BPose%0AGuider%7D%20enables%20motion%20generalization%20by%20generating%20expressive%20pose%20sequences%0Ausing%20time-aware%20parametric%20encoders.%20%284%29%20%5Ctextbf%7BVideo%20Harmonizer%7D%20synthesizes%0Aphotorealistic%2C%20coherent%20video%20via%20a%20hybrid%20rendering%20pipeline%20with%20diffusion%2C%0Arefining%20the%20Reconstructor%20through%20a%20Back-to-4D%20feedback%20loop.%20HumanGenesis%0Aachieves%20state-of-the-art%20performance%20on%20tasks%20including%20text-guided%20synthesis%2C%0Avideo%20reenactment%2C%20and%20novel-pose%20generalization%2C%20significantly%20improving%0Aexpressiveness%2C%20geometric%20fidelity%2C%20and%20scene%20integration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanGenesis%253A%2520Agent-Based%2520Geometric%2520and%2520Generative%2520Modeling%2520for%250A%2520%2520Synthetic%2520Human%2520Dynamics%26entry.906535625%3DWeiqi%2520Li%2520and%2520Zehao%2520Zhang%2520and%2520Liang%2520Lin%2520and%2520Guangrun%2520Wang%26entry.1292438233%3D%2520%2520%255Ctextbf%257BSynthetic%2520human%2520dynamics%257D%2520aims%2520to%2520generate%2520photorealistic%2520videos%2520of%250Ahuman%2520subjects%2520performing%2520expressive%252C%2520intention-driven%2520motions.%2520However%252C%250Acurrent%2520approaches%2520face%2520two%2520core%2520challenges%253A%2520%25281%2529%2520%255Cemph%257Bgeometric%2520inconsistency%257D%250Aand%2520%255Cemph%257Bcoarse%2520reconstruction%257D%252C%2520due%2520to%2520limited%25203D%2520modeling%2520and%2520detail%250Apreservation%253B%2520and%2520%25282%2529%2520%255Cemph%257Bmotion%2520generalization%2520limitations%257D%2520and%2520%255Cemph%257Bscene%250Ainharmonization%257D%252C%2520stemming%2520from%2520weak%2520generative%2520capabilities.%2520To%2520address%2520these%252C%250Awe%2520present%2520%255Ctextbf%257BHumanGenesis%257D%252C%2520a%2520framework%2520that%2520integrates%2520geometric%2520and%250Agenerative%2520modeling%2520through%2520four%2520collaborative%2520agents%253A%2520%25281%2529%250A%255Ctextbf%257BReconstructor%257D%2520builds%25203D-consistent%2520human-scene%2520representations%2520from%250Amonocular%2520video%2520using%25203D%2520Gaussian%2520Splatting%2520and%2520deformation%2520decomposition.%2520%25282%2529%250A%255Ctextbf%257BCritique%2520Agent%257D%2520enhances%2520reconstruction%2520fidelity%2520by%2520identifying%2520and%250Arefining%2520poor%2520regions%2520via%2520multi-round%2520MLLM-based%2520reflection.%2520%25283%2529%2520%255Ctextbf%257BPose%250AGuider%257D%2520enables%2520motion%2520generalization%2520by%2520generating%2520expressive%2520pose%2520sequences%250Ausing%2520time-aware%2520parametric%2520encoders.%2520%25284%2529%2520%255Ctextbf%257BVideo%2520Harmonizer%257D%2520synthesizes%250Aphotorealistic%252C%2520coherent%2520video%2520via%2520a%2520hybrid%2520rendering%2520pipeline%2520with%2520diffusion%252C%250Arefining%2520the%2520Reconstructor%2520through%2520a%2520Back-to-4D%2520feedback%2520loop.%2520HumanGenesis%250Aachieves%2520state-of-the-art%2520performance%2520on%2520tasks%2520including%2520text-guided%2520synthesis%252C%250Avideo%2520reenactment%252C%2520and%2520novel-pose%2520generalization%252C%2520significantly%2520improving%250Aexpressiveness%252C%2520geometric%2520fidelity%252C%2520and%2520scene%2520integration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanGenesis%3A%20Agent-Based%20Geometric%20and%20Generative%20Modeling%20for%0A%20%20Synthetic%20Human%20Dynamics&entry.906535625=Weiqi%20Li%20and%20Zehao%20Zhang%20and%20Liang%20Lin%20and%20Guangrun%20Wang&entry.1292438233=%20%20%5Ctextbf%7BSynthetic%20human%20dynamics%7D%20aims%20to%20generate%20photorealistic%20videos%20of%0Ahuman%20subjects%20performing%20expressive%2C%20intention-driven%20motions.%20However%2C%0Acurrent%20approaches%20face%20two%20core%20challenges%3A%20%281%29%20%5Cemph%7Bgeometric%20inconsistency%7D%0Aand%20%5Cemph%7Bcoarse%20reconstruction%7D%2C%20due%20to%20limited%203D%20modeling%20and%20detail%0Apreservation%3B%20and%20%282%29%20%5Cemph%7Bmotion%20generalization%20limitations%7D%20and%20%5Cemph%7Bscene%0Ainharmonization%7D%2C%20stemming%20from%20weak%20generative%20capabilities.%20To%20address%20these%2C%0Awe%20present%20%5Ctextbf%7BHumanGenesis%7D%2C%20a%20framework%20that%20integrates%20geometric%20and%0Agenerative%20modeling%20through%20four%20collaborative%20agents%3A%20%281%29%0A%5Ctextbf%7BReconstructor%7D%20builds%203D-consistent%20human-scene%20representations%20from%0Amonocular%20video%20using%203D%20Gaussian%20Splatting%20and%20deformation%20decomposition.%20%282%29%0A%5Ctextbf%7BCritique%20Agent%7D%20enhances%20reconstruction%20fidelity%20by%20identifying%20and%0Arefining%20poor%20regions%20via%20multi-round%20MLLM-based%20reflection.%20%283%29%20%5Ctextbf%7BPose%0AGuider%7D%20enables%20motion%20generalization%20by%20generating%20expressive%20pose%20sequences%0Ausing%20time-aware%20parametric%20encoders.%20%284%29%20%5Ctextbf%7BVideo%20Harmonizer%7D%20synthesizes%0Aphotorealistic%2C%20coherent%20video%20via%20a%20hybrid%20rendering%20pipeline%20with%20diffusion%2C%0Arefining%20the%20Reconstructor%20through%20a%20Back-to-4D%20feedback%20loop.%20HumanGenesis%0Aachieves%20state-of-the-art%20performance%20on%20tasks%20including%20text-guided%20synthesis%2C%0Avideo%20reenactment%2C%20and%20novel-pose%20generalization%2C%20significantly%20improving%0Aexpressiveness%2C%20geometric%20fidelity%2C%20and%20scene%20integration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09858v1&entry.124074799=Read"},
{"title": "PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations\n  from a Single Image", "author": "Geonhee Sim and Gyeongsik Moon", "abstract": "  Two major approaches exist for creating animatable human avatars. The first,\na 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a\nsingle person, achieving personalization through a disentangled identity\nrepresentation. However, modeling pose-driven deformations, such as non-rigid\ncloth deformations, requires numerous pose-rich videos, which are costly and\nimpractical to capture in daily life. The second, a diffusion-based approach,\nlearns pose-driven deformations from large-scale in-the-wild videos but\nstruggles with identity preservation and pose-dependent identity entanglement.\nWe present PERSONA, a framework that combines the strengths of both approaches\nto obtain a personalized 3D human avatar with pose-driven deformations from a\nsingle image. PERSONA leverages a diffusion-based approach to generate\npose-rich videos from the input image and optimizes a 3D avatar based on them.\nTo ensure high authenticity and sharp renderings across diverse poses, we\nintroduce balanced sampling and geometry-weighted optimization. Balanced\nsampling oversamples the input image to mitigate identity shifts in\ndiffusion-generated training videos. Geometry-weighted optimization prioritizes\ngeometry constraints over image loss, preserving rendering quality in diverse\nposes.\n", "link": "http://arxiv.org/abs/2508.09973v1", "date": "2025-08-13", "relevancy": 3.3699, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6985}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6655}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PERSONA%3A%20Personalized%20Whole-Body%203D%20Avatar%20with%20Pose-Driven%20Deformations%0A%20%20from%20a%20Single%20Image&body=Title%3A%20PERSONA%3A%20Personalized%20Whole-Body%203D%20Avatar%20with%20Pose-Driven%20Deformations%0A%20%20from%20a%20Single%20Image%0AAuthor%3A%20Geonhee%20Sim%20and%20Gyeongsik%20Moon%0AAbstract%3A%20%20%20Two%20major%20approaches%20exist%20for%20creating%20animatable%20human%20avatars.%20The%20first%2C%0Aa%203D-based%20approach%2C%20optimizes%20a%20NeRF-%20or%203DGS-based%20avatar%20from%20videos%20of%20a%0Asingle%20person%2C%20achieving%20personalization%20through%20a%20disentangled%20identity%0Arepresentation.%20However%2C%20modeling%20pose-driven%20deformations%2C%20such%20as%20non-rigid%0Acloth%20deformations%2C%20requires%20numerous%20pose-rich%20videos%2C%20which%20are%20costly%20and%0Aimpractical%20to%20capture%20in%20daily%20life.%20The%20second%2C%20a%20diffusion-based%20approach%2C%0Alearns%20pose-driven%20deformations%20from%20large-scale%20in-the-wild%20videos%20but%0Astruggles%20with%20identity%20preservation%20and%20pose-dependent%20identity%20entanglement.%0AWe%20present%20PERSONA%2C%20a%20framework%20that%20combines%20the%20strengths%20of%20both%20approaches%0Ato%20obtain%20a%20personalized%203D%20human%20avatar%20with%20pose-driven%20deformations%20from%20a%0Asingle%20image.%20PERSONA%20leverages%20a%20diffusion-based%20approach%20to%20generate%0Apose-rich%20videos%20from%20the%20input%20image%20and%20optimizes%20a%203D%20avatar%20based%20on%20them.%0ATo%20ensure%20high%20authenticity%20and%20sharp%20renderings%20across%20diverse%20poses%2C%20we%0Aintroduce%20balanced%20sampling%20and%20geometry-weighted%20optimization.%20Balanced%0Asampling%20oversamples%20the%20input%20image%20to%20mitigate%20identity%20shifts%20in%0Adiffusion-generated%20training%20videos.%20Geometry-weighted%20optimization%20prioritizes%0Ageometry%20constraints%20over%20image%20loss%2C%20preserving%20rendering%20quality%20in%20diverse%0Aposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPERSONA%253A%2520Personalized%2520Whole-Body%25203D%2520Avatar%2520with%2520Pose-Driven%2520Deformations%250A%2520%2520from%2520a%2520Single%2520Image%26entry.906535625%3DGeonhee%2520Sim%2520and%2520Gyeongsik%2520Moon%26entry.1292438233%3D%2520%2520Two%2520major%2520approaches%2520exist%2520for%2520creating%2520animatable%2520human%2520avatars.%2520The%2520first%252C%250Aa%25203D-based%2520approach%252C%2520optimizes%2520a%2520NeRF-%2520or%25203DGS-based%2520avatar%2520from%2520videos%2520of%2520a%250Asingle%2520person%252C%2520achieving%2520personalization%2520through%2520a%2520disentangled%2520identity%250Arepresentation.%2520However%252C%2520modeling%2520pose-driven%2520deformations%252C%2520such%2520as%2520non-rigid%250Acloth%2520deformations%252C%2520requires%2520numerous%2520pose-rich%2520videos%252C%2520which%2520are%2520costly%2520and%250Aimpractical%2520to%2520capture%2520in%2520daily%2520life.%2520The%2520second%252C%2520a%2520diffusion-based%2520approach%252C%250Alearns%2520pose-driven%2520deformations%2520from%2520large-scale%2520in-the-wild%2520videos%2520but%250Astruggles%2520with%2520identity%2520preservation%2520and%2520pose-dependent%2520identity%2520entanglement.%250AWe%2520present%2520PERSONA%252C%2520a%2520framework%2520that%2520combines%2520the%2520strengths%2520of%2520both%2520approaches%250Ato%2520obtain%2520a%2520personalized%25203D%2520human%2520avatar%2520with%2520pose-driven%2520deformations%2520from%2520a%250Asingle%2520image.%2520PERSONA%2520leverages%2520a%2520diffusion-based%2520approach%2520to%2520generate%250Apose-rich%2520videos%2520from%2520the%2520input%2520image%2520and%2520optimizes%2520a%25203D%2520avatar%2520based%2520on%2520them.%250ATo%2520ensure%2520high%2520authenticity%2520and%2520sharp%2520renderings%2520across%2520diverse%2520poses%252C%2520we%250Aintroduce%2520balanced%2520sampling%2520and%2520geometry-weighted%2520optimization.%2520Balanced%250Asampling%2520oversamples%2520the%2520input%2520image%2520to%2520mitigate%2520identity%2520shifts%2520in%250Adiffusion-generated%2520training%2520videos.%2520Geometry-weighted%2520optimization%2520prioritizes%250Ageometry%2520constraints%2520over%2520image%2520loss%252C%2520preserving%2520rendering%2520quality%2520in%2520diverse%250Aposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PERSONA%3A%20Personalized%20Whole-Body%203D%20Avatar%20with%20Pose-Driven%20Deformations%0A%20%20from%20a%20Single%20Image&entry.906535625=Geonhee%20Sim%20and%20Gyeongsik%20Moon&entry.1292438233=%20%20Two%20major%20approaches%20exist%20for%20creating%20animatable%20human%20avatars.%20The%20first%2C%0Aa%203D-based%20approach%2C%20optimizes%20a%20NeRF-%20or%203DGS-based%20avatar%20from%20videos%20of%20a%0Asingle%20person%2C%20achieving%20personalization%20through%20a%20disentangled%20identity%0Arepresentation.%20However%2C%20modeling%20pose-driven%20deformations%2C%20such%20as%20non-rigid%0Acloth%20deformations%2C%20requires%20numerous%20pose-rich%20videos%2C%20which%20are%20costly%20and%0Aimpractical%20to%20capture%20in%20daily%20life.%20The%20second%2C%20a%20diffusion-based%20approach%2C%0Alearns%20pose-driven%20deformations%20from%20large-scale%20in-the-wild%20videos%20but%0Astruggles%20with%20identity%20preservation%20and%20pose-dependent%20identity%20entanglement.%0AWe%20present%20PERSONA%2C%20a%20framework%20that%20combines%20the%20strengths%20of%20both%20approaches%0Ato%20obtain%20a%20personalized%203D%20human%20avatar%20with%20pose-driven%20deformations%20from%20a%0Asingle%20image.%20PERSONA%20leverages%20a%20diffusion-based%20approach%20to%20generate%0Apose-rich%20videos%20from%20the%20input%20image%20and%20optimizes%20a%203D%20avatar%20based%20on%20them.%0ATo%20ensure%20high%20authenticity%20and%20sharp%20renderings%20across%20diverse%20poses%2C%20we%0Aintroduce%20balanced%20sampling%20and%20geometry-weighted%20optimization.%20Balanced%0Asampling%20oversamples%20the%20input%20image%20to%20mitigate%20identity%20shifts%20in%0Adiffusion-generated%20training%20videos.%20Geometry-weighted%20optimization%20prioritizes%0Ageometry%20constraints%20over%20image%20loss%2C%20preserving%20rendering%20quality%20in%20diverse%0Aposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09973v1&entry.124074799=Read"},
{"title": "E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event\n  Cameras", "author": "Chaoran Feng and Zhenyu Tang and Wangbo Yu and Yatian Pang and Yian Zhao and Jianbin Zhao and Li Yuan and Yonghong Tian", "abstract": "  Novel view synthesis and 4D reconstruction techniques predominantly rely on\nRGB cameras, thereby inheriting inherent limitations such as the dependence on\nadequate lighting, susceptibility to motion blur, and a limited dynamic range.\nEvent cameras, offering advantages of low power, high temporal resolution and\nhigh dynamic range, have brought a new perspective to addressing the scene\nreconstruction challenges in high-speed motion and low-light scenes. To this\nend, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting\napproach, for novel view synthesis from multi-view event streams with\nfast-moving cameras. Specifically, we introduce an event-based initialization\nscheme to ensure stable training and propose event-adaptive slicing splatting\nfor time-aware reconstruction. Additionally, we employ intensity importance\npruning to eliminate floating artifacts and enhance 3D consistency, while\nincorporating an adaptive contrast threshold for more precise optimization. We\ndesign a synthetic multi-view camera setup with six moving event cameras\nsurrounding the object in a 360-degree configuration and provide a benchmark\nmulti-view event stream dataset that captures challenging motion scenarios. Our\napproach outperforms both event-only and event-RGB fusion baselines and paves\nthe way for the exploration of multi-view event-based reconstruction as a novel\napproach for rapid scene capture.\n", "link": "http://arxiv.org/abs/2508.09912v1", "date": "2025-08-13", "relevancy": 3.3246, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7105}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6574}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E-4DGS%3A%20High-Fidelity%20Dynamic%20Reconstruction%20from%20the%20Multi-view%20Event%0A%20%20Cameras&body=Title%3A%20E-4DGS%3A%20High-Fidelity%20Dynamic%20Reconstruction%20from%20the%20Multi-view%20Event%0A%20%20Cameras%0AAuthor%3A%20Chaoran%20Feng%20and%20Zhenyu%20Tang%20and%20Wangbo%20Yu%20and%20Yatian%20Pang%20and%20Yian%20Zhao%20and%20Jianbin%20Zhao%20and%20Li%20Yuan%20and%20Yonghong%20Tian%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20and%204D%20reconstruction%20techniques%20predominantly%20rely%20on%0ARGB%20cameras%2C%20thereby%20inheriting%20inherent%20limitations%20such%20as%20the%20dependence%20on%0Aadequate%20lighting%2C%20susceptibility%20to%20motion%20blur%2C%20and%20a%20limited%20dynamic%20range.%0AEvent%20cameras%2C%20offering%20advantages%20of%20low%20power%2C%20high%20temporal%20resolution%20and%0Ahigh%20dynamic%20range%2C%20have%20brought%20a%20new%20perspective%20to%20addressing%20the%20scene%0Areconstruction%20challenges%20in%20high-speed%20motion%20and%20low-light%20scenes.%20To%20this%0Aend%2C%20we%20propose%20E-4DGS%2C%20the%20first%20event-driven%20dynamic%20Gaussian%20Splatting%0Aapproach%2C%20for%20novel%20view%20synthesis%20from%20multi-view%20event%20streams%20with%0Afast-moving%20cameras.%20Specifically%2C%20we%20introduce%20an%20event-based%20initialization%0Ascheme%20to%20ensure%20stable%20training%20and%20propose%20event-adaptive%20slicing%20splatting%0Afor%20time-aware%20reconstruction.%20Additionally%2C%20we%20employ%20intensity%20importance%0Apruning%20to%20eliminate%20floating%20artifacts%20and%20enhance%203D%20consistency%2C%20while%0Aincorporating%20an%20adaptive%20contrast%20threshold%20for%20more%20precise%20optimization.%20We%0Adesign%20a%20synthetic%20multi-view%20camera%20setup%20with%20six%20moving%20event%20cameras%0Asurrounding%20the%20object%20in%20a%20360-degree%20configuration%20and%20provide%20a%20benchmark%0Amulti-view%20event%20stream%20dataset%20that%20captures%20challenging%20motion%20scenarios.%20Our%0Aapproach%20outperforms%20both%20event-only%20and%20event-RGB%20fusion%20baselines%20and%20paves%0Athe%20way%20for%20the%20exploration%20of%20multi-view%20event-based%20reconstruction%20as%20a%20novel%0Aapproach%20for%20rapid%20scene%20capture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE-4DGS%253A%2520High-Fidelity%2520Dynamic%2520Reconstruction%2520from%2520the%2520Multi-view%2520Event%250A%2520%2520Cameras%26entry.906535625%3DChaoran%2520Feng%2520and%2520Zhenyu%2520Tang%2520and%2520Wangbo%2520Yu%2520and%2520Yatian%2520Pang%2520and%2520Yian%2520Zhao%2520and%2520Jianbin%2520Zhao%2520and%2520Li%2520Yuan%2520and%2520Yonghong%2520Tian%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520and%25204D%2520reconstruction%2520techniques%2520predominantly%2520rely%2520on%250ARGB%2520cameras%252C%2520thereby%2520inheriting%2520inherent%2520limitations%2520such%2520as%2520the%2520dependence%2520on%250Aadequate%2520lighting%252C%2520susceptibility%2520to%2520motion%2520blur%252C%2520and%2520a%2520limited%2520dynamic%2520range.%250AEvent%2520cameras%252C%2520offering%2520advantages%2520of%2520low%2520power%252C%2520high%2520temporal%2520resolution%2520and%250Ahigh%2520dynamic%2520range%252C%2520have%2520brought%2520a%2520new%2520perspective%2520to%2520addressing%2520the%2520scene%250Areconstruction%2520challenges%2520in%2520high-speed%2520motion%2520and%2520low-light%2520scenes.%2520To%2520this%250Aend%252C%2520we%2520propose%2520E-4DGS%252C%2520the%2520first%2520event-driven%2520dynamic%2520Gaussian%2520Splatting%250Aapproach%252C%2520for%2520novel%2520view%2520synthesis%2520from%2520multi-view%2520event%2520streams%2520with%250Afast-moving%2520cameras.%2520Specifically%252C%2520we%2520introduce%2520an%2520event-based%2520initialization%250Ascheme%2520to%2520ensure%2520stable%2520training%2520and%2520propose%2520event-adaptive%2520slicing%2520splatting%250Afor%2520time-aware%2520reconstruction.%2520Additionally%252C%2520we%2520employ%2520intensity%2520importance%250Apruning%2520to%2520eliminate%2520floating%2520artifacts%2520and%2520enhance%25203D%2520consistency%252C%2520while%250Aincorporating%2520an%2520adaptive%2520contrast%2520threshold%2520for%2520more%2520precise%2520optimization.%2520We%250Adesign%2520a%2520synthetic%2520multi-view%2520camera%2520setup%2520with%2520six%2520moving%2520event%2520cameras%250Asurrounding%2520the%2520object%2520in%2520a%2520360-degree%2520configuration%2520and%2520provide%2520a%2520benchmark%250Amulti-view%2520event%2520stream%2520dataset%2520that%2520captures%2520challenging%2520motion%2520scenarios.%2520Our%250Aapproach%2520outperforms%2520both%2520event-only%2520and%2520event-RGB%2520fusion%2520baselines%2520and%2520paves%250Athe%2520way%2520for%2520the%2520exploration%2520of%2520multi-view%2520event-based%2520reconstruction%2520as%2520a%2520novel%250Aapproach%2520for%2520rapid%2520scene%2520capture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E-4DGS%3A%20High-Fidelity%20Dynamic%20Reconstruction%20from%20the%20Multi-view%20Event%0A%20%20Cameras&entry.906535625=Chaoran%20Feng%20and%20Zhenyu%20Tang%20and%20Wangbo%20Yu%20and%20Yatian%20Pang%20and%20Yian%20Zhao%20and%20Jianbin%20Zhao%20and%20Li%20Yuan%20and%20Yonghong%20Tian&entry.1292438233=%20%20Novel%20view%20synthesis%20and%204D%20reconstruction%20techniques%20predominantly%20rely%20on%0ARGB%20cameras%2C%20thereby%20inheriting%20inherent%20limitations%20such%20as%20the%20dependence%20on%0Aadequate%20lighting%2C%20susceptibility%20to%20motion%20blur%2C%20and%20a%20limited%20dynamic%20range.%0AEvent%20cameras%2C%20offering%20advantages%20of%20low%20power%2C%20high%20temporal%20resolution%20and%0Ahigh%20dynamic%20range%2C%20have%20brought%20a%20new%20perspective%20to%20addressing%20the%20scene%0Areconstruction%20challenges%20in%20high-speed%20motion%20and%20low-light%20scenes.%20To%20this%0Aend%2C%20we%20propose%20E-4DGS%2C%20the%20first%20event-driven%20dynamic%20Gaussian%20Splatting%0Aapproach%2C%20for%20novel%20view%20synthesis%20from%20multi-view%20event%20streams%20with%0Afast-moving%20cameras.%20Specifically%2C%20we%20introduce%20an%20event-based%20initialization%0Ascheme%20to%20ensure%20stable%20training%20and%20propose%20event-adaptive%20slicing%20splatting%0Afor%20time-aware%20reconstruction.%20Additionally%2C%20we%20employ%20intensity%20importance%0Apruning%20to%20eliminate%20floating%20artifacts%20and%20enhance%203D%20consistency%2C%20while%0Aincorporating%20an%20adaptive%20contrast%20threshold%20for%20more%20precise%20optimization.%20We%0Adesign%20a%20synthetic%20multi-view%20camera%20setup%20with%20six%20moving%20event%20cameras%0Asurrounding%20the%20object%20in%20a%20360-degree%20configuration%20and%20provide%20a%20benchmark%0Amulti-view%20event%20stream%20dataset%20that%20captures%20challenging%20motion%20scenarios.%20Our%0Aapproach%20outperforms%20both%20event-only%20and%20event-RGB%20fusion%20baselines%20and%20paves%0Athe%20way%20for%20the%20exploration%20of%20multi-view%20event-based%20reconstruction%20as%20a%20novel%0Aapproach%20for%20rapid%20scene%20capture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09912v1&entry.124074799=Read"},
{"title": "RayletDF: Raylet Distance Fields for Generalizable 3D Surface\n  Reconstruction from Point Clouds or Gaussians", "author": "Shenxing Wei and Jinxi Li and Yafei Yang and Siyuan Zhou and Bo Yang", "abstract": "  In this paper, we present a generalizable method for 3D surface\nreconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from\nRGB images. Unlike existing coordinate-based methods which are often\ncomputationally intensive when rendering explicit surfaces, our proposed\nmethod, named RayletDF, introduces a new technique called raylet distance\nfield, which aims to directly predict surface points from query rays. Our\npipeline consists of three key modules: a raylet feature extractor, a raylet\ndistance field predictor, and a multi-raylet blender. These components work\ntogether to extract fine-grained local geometric features, predict raylet\ndistances, and aggregate multiple predictions to reconstruct precise surface\npoints. We extensively evaluate our method on multiple public real-world\ndatasets, demonstrating superior performance in surface reconstruction from\npoint clouds or 3D Gaussians. Most notably, our method achieves exceptional\ngeneralization ability, successfully recovering 3D surfaces in a single-forward\npass across unseen datasets in testing.\n", "link": "http://arxiv.org/abs/2508.09830v1", "date": "2025-08-13", "relevancy": 3.1822, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6795}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6247}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RayletDF%3A%20Raylet%20Distance%20Fields%20for%20Generalizable%203D%20Surface%0A%20%20Reconstruction%20from%20Point%20Clouds%20or%20Gaussians&body=Title%3A%20RayletDF%3A%20Raylet%20Distance%20Fields%20for%20Generalizable%203D%20Surface%0A%20%20Reconstruction%20from%20Point%20Clouds%20or%20Gaussians%0AAuthor%3A%20Shenxing%20Wei%20and%20Jinxi%20Li%20and%20Yafei%20Yang%20and%20Siyuan%20Zhou%20and%20Bo%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20generalizable%20method%20for%203D%20surface%0Areconstruction%20from%20raw%20point%20clouds%20or%20pre-estimated%203D%20Gaussians%20by%203DGS%20from%0ARGB%20images.%20Unlike%20existing%20coordinate-based%20methods%20which%20are%20often%0Acomputationally%20intensive%20when%20rendering%20explicit%20surfaces%2C%20our%20proposed%0Amethod%2C%20named%20RayletDF%2C%20introduces%20a%20new%20technique%20called%20raylet%20distance%0Afield%2C%20which%20aims%20to%20directly%20predict%20surface%20points%20from%20query%20rays.%20Our%0Apipeline%20consists%20of%20three%20key%20modules%3A%20a%20raylet%20feature%20extractor%2C%20a%20raylet%0Adistance%20field%20predictor%2C%20and%20a%20multi-raylet%20blender.%20These%20components%20work%0Atogether%20to%20extract%20fine-grained%20local%20geometric%20features%2C%20predict%20raylet%0Adistances%2C%20and%20aggregate%20multiple%20predictions%20to%20reconstruct%20precise%20surface%0Apoints.%20We%20extensively%20evaluate%20our%20method%20on%20multiple%20public%20real-world%0Adatasets%2C%20demonstrating%20superior%20performance%20in%20surface%20reconstruction%20from%0Apoint%20clouds%20or%203D%20Gaussians.%20Most%20notably%2C%20our%20method%20achieves%20exceptional%0Ageneralization%20ability%2C%20successfully%20recovering%203D%20surfaces%20in%20a%20single-forward%0Apass%20across%20unseen%20datasets%20in%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRayletDF%253A%2520Raylet%2520Distance%2520Fields%2520for%2520Generalizable%25203D%2520Surface%250A%2520%2520Reconstruction%2520from%2520Point%2520Clouds%2520or%2520Gaussians%26entry.906535625%3DShenxing%2520Wei%2520and%2520Jinxi%2520Li%2520and%2520Yafei%2520Yang%2520and%2520Siyuan%2520Zhou%2520and%2520Bo%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520generalizable%2520method%2520for%25203D%2520surface%250Areconstruction%2520from%2520raw%2520point%2520clouds%2520or%2520pre-estimated%25203D%2520Gaussians%2520by%25203DGS%2520from%250ARGB%2520images.%2520Unlike%2520existing%2520coordinate-based%2520methods%2520which%2520are%2520often%250Acomputationally%2520intensive%2520when%2520rendering%2520explicit%2520surfaces%252C%2520our%2520proposed%250Amethod%252C%2520named%2520RayletDF%252C%2520introduces%2520a%2520new%2520technique%2520called%2520raylet%2520distance%250Afield%252C%2520which%2520aims%2520to%2520directly%2520predict%2520surface%2520points%2520from%2520query%2520rays.%2520Our%250Apipeline%2520consists%2520of%2520three%2520key%2520modules%253A%2520a%2520raylet%2520feature%2520extractor%252C%2520a%2520raylet%250Adistance%2520field%2520predictor%252C%2520and%2520a%2520multi-raylet%2520blender.%2520These%2520components%2520work%250Atogether%2520to%2520extract%2520fine-grained%2520local%2520geometric%2520features%252C%2520predict%2520raylet%250Adistances%252C%2520and%2520aggregate%2520multiple%2520predictions%2520to%2520reconstruct%2520precise%2520surface%250Apoints.%2520We%2520extensively%2520evaluate%2520our%2520method%2520on%2520multiple%2520public%2520real-world%250Adatasets%252C%2520demonstrating%2520superior%2520performance%2520in%2520surface%2520reconstruction%2520from%250Apoint%2520clouds%2520or%25203D%2520Gaussians.%2520Most%2520notably%252C%2520our%2520method%2520achieves%2520exceptional%250Ageneralization%2520ability%252C%2520successfully%2520recovering%25203D%2520surfaces%2520in%2520a%2520single-forward%250Apass%2520across%2520unseen%2520datasets%2520in%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RayletDF%3A%20Raylet%20Distance%20Fields%20for%20Generalizable%203D%20Surface%0A%20%20Reconstruction%20from%20Point%20Clouds%20or%20Gaussians&entry.906535625=Shenxing%20Wei%20and%20Jinxi%20Li%20and%20Yafei%20Yang%20and%20Siyuan%20Zhou%20and%20Bo%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20generalizable%20method%20for%203D%20surface%0Areconstruction%20from%20raw%20point%20clouds%20or%20pre-estimated%203D%20Gaussians%20by%203DGS%20from%0ARGB%20images.%20Unlike%20existing%20coordinate-based%20methods%20which%20are%20often%0Acomputationally%20intensive%20when%20rendering%20explicit%20surfaces%2C%20our%20proposed%0Amethod%2C%20named%20RayletDF%2C%20introduces%20a%20new%20technique%20called%20raylet%20distance%0Afield%2C%20which%20aims%20to%20directly%20predict%20surface%20points%20from%20query%20rays.%20Our%0Apipeline%20consists%20of%20three%20key%20modules%3A%20a%20raylet%20feature%20extractor%2C%20a%20raylet%0Adistance%20field%20predictor%2C%20and%20a%20multi-raylet%20blender.%20These%20components%20work%0Atogether%20to%20extract%20fine-grained%20local%20geometric%20features%2C%20predict%20raylet%0Adistances%2C%20and%20aggregate%20multiple%20predictions%20to%20reconstruct%20precise%20surface%0Apoints.%20We%20extensively%20evaluate%20our%20method%20on%20multiple%20public%20real-world%0Adatasets%2C%20demonstrating%20superior%20performance%20in%20surface%20reconstruction%20from%0Apoint%20clouds%20or%203D%20Gaussians.%20Most%20notably%2C%20our%20method%20achieves%20exceptional%0Ageneralization%20ability%2C%20successfully%20recovering%203D%20surfaces%20in%20a%20single-forward%0Apass%20across%20unseen%20datasets%20in%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09830v1&entry.124074799=Read"},
{"title": "TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos", "author": "Jinxi Li and Ziyang Song and Bo Yang", "abstract": "  In this paper, we aim to model 3D scene geometry, appearance, and physical\ninformation just from dynamic multi-view videos in the absence of any human\nlabels. By leveraging physics-informed losses as soft constraints or\nintegrating simple physics models into neural nets, existing works often fail\nto learn complex motion physics, or doing so requires additional labels such as\nobject types or masks. We propose a new framework named TRACE to model the\nmotion physics of complex dynamic 3D scenes. The key novelty of our method is\nthat, by formulating each 3D point as a rigid particle with size and\norientation in space, we directly learn a translation rotation dynamics system\nfor each particle, explicitly estimating a complete set of physical parameters\nto govern the particle's motion over time. Extensive experiments on three\nexisting dynamic datasets and one newly created challenging synthetic datasets\ndemonstrate the extraordinary performance of our method over baselines in the\ntask of future frame extrapolation. A nice property of our framework is that\nmultiple objects or parts can be easily segmented just by clustering the\nlearned physical parameters.\n", "link": "http://arxiv.org/abs/2508.09811v1", "date": "2025-08-13", "relevancy": 3.1295, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6414}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6201}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRACE%3A%20Learning%203D%20Gaussian%20Physical%20Dynamics%20from%20Multi-view%20Videos&body=Title%3A%20TRACE%3A%20Learning%203D%20Gaussian%20Physical%20Dynamics%20from%20Multi-view%20Videos%0AAuthor%3A%20Jinxi%20Li%20and%20Ziyang%20Song%20and%20Bo%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20aim%20to%20model%203D%20scene%20geometry%2C%20appearance%2C%20and%20physical%0Ainformation%20just%20from%20dynamic%20multi-view%20videos%20in%20the%20absence%20of%20any%20human%0Alabels.%20By%20leveraging%20physics-informed%20losses%20as%20soft%20constraints%20or%0Aintegrating%20simple%20physics%20models%20into%20neural%20nets%2C%20existing%20works%20often%20fail%0Ato%20learn%20complex%20motion%20physics%2C%20or%20doing%20so%20requires%20additional%20labels%20such%20as%0Aobject%20types%20or%20masks.%20We%20propose%20a%20new%20framework%20named%20TRACE%20to%20model%20the%0Amotion%20physics%20of%20complex%20dynamic%203D%20scenes.%20The%20key%20novelty%20of%20our%20method%20is%0Athat%2C%20by%20formulating%20each%203D%20point%20as%20a%20rigid%20particle%20with%20size%20and%0Aorientation%20in%20space%2C%20we%20directly%20learn%20a%20translation%20rotation%20dynamics%20system%0Afor%20each%20particle%2C%20explicitly%20estimating%20a%20complete%20set%20of%20physical%20parameters%0Ato%20govern%20the%20particle%27s%20motion%20over%20time.%20Extensive%20experiments%20on%20three%0Aexisting%20dynamic%20datasets%20and%20one%20newly%20created%20challenging%20synthetic%20datasets%0Ademonstrate%20the%20extraordinary%20performance%20of%20our%20method%20over%20baselines%20in%20the%0Atask%20of%20future%20frame%20extrapolation.%20A%20nice%20property%20of%20our%20framework%20is%20that%0Amultiple%20objects%20or%20parts%20can%20be%20easily%20segmented%20just%20by%20clustering%20the%0Alearned%20physical%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRACE%253A%2520Learning%25203D%2520Gaussian%2520Physical%2520Dynamics%2520from%2520Multi-view%2520Videos%26entry.906535625%3DJinxi%2520Li%2520and%2520Ziyang%2520Song%2520and%2520Bo%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520model%25203D%2520scene%2520geometry%252C%2520appearance%252C%2520and%2520physical%250Ainformation%2520just%2520from%2520dynamic%2520multi-view%2520videos%2520in%2520the%2520absence%2520of%2520any%2520human%250Alabels.%2520By%2520leveraging%2520physics-informed%2520losses%2520as%2520soft%2520constraints%2520or%250Aintegrating%2520simple%2520physics%2520models%2520into%2520neural%2520nets%252C%2520existing%2520works%2520often%2520fail%250Ato%2520learn%2520complex%2520motion%2520physics%252C%2520or%2520doing%2520so%2520requires%2520additional%2520labels%2520such%2520as%250Aobject%2520types%2520or%2520masks.%2520We%2520propose%2520a%2520new%2520framework%2520named%2520TRACE%2520to%2520model%2520the%250Amotion%2520physics%2520of%2520complex%2520dynamic%25203D%2520scenes.%2520The%2520key%2520novelty%2520of%2520our%2520method%2520is%250Athat%252C%2520by%2520formulating%2520each%25203D%2520point%2520as%2520a%2520rigid%2520particle%2520with%2520size%2520and%250Aorientation%2520in%2520space%252C%2520we%2520directly%2520learn%2520a%2520translation%2520rotation%2520dynamics%2520system%250Afor%2520each%2520particle%252C%2520explicitly%2520estimating%2520a%2520complete%2520set%2520of%2520physical%2520parameters%250Ato%2520govern%2520the%2520particle%2527s%2520motion%2520over%2520time.%2520Extensive%2520experiments%2520on%2520three%250Aexisting%2520dynamic%2520datasets%2520and%2520one%2520newly%2520created%2520challenging%2520synthetic%2520datasets%250Ademonstrate%2520the%2520extraordinary%2520performance%2520of%2520our%2520method%2520over%2520baselines%2520in%2520the%250Atask%2520of%2520future%2520frame%2520extrapolation.%2520A%2520nice%2520property%2520of%2520our%2520framework%2520is%2520that%250Amultiple%2520objects%2520or%2520parts%2520can%2520be%2520easily%2520segmented%2520just%2520by%2520clustering%2520the%250Alearned%2520physical%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRACE%3A%20Learning%203D%20Gaussian%20Physical%20Dynamics%20from%20Multi-view%20Videos&entry.906535625=Jinxi%20Li%20and%20Ziyang%20Song%20and%20Bo%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20aim%20to%20model%203D%20scene%20geometry%2C%20appearance%2C%20and%20physical%0Ainformation%20just%20from%20dynamic%20multi-view%20videos%20in%20the%20absence%20of%20any%20human%0Alabels.%20By%20leveraging%20physics-informed%20losses%20as%20soft%20constraints%20or%0Aintegrating%20simple%20physics%20models%20into%20neural%20nets%2C%20existing%20works%20often%20fail%0Ato%20learn%20complex%20motion%20physics%2C%20or%20doing%20so%20requires%20additional%20labels%20such%20as%0Aobject%20types%20or%20masks.%20We%20propose%20a%20new%20framework%20named%20TRACE%20to%20model%20the%0Amotion%20physics%20of%20complex%20dynamic%203D%20scenes.%20The%20key%20novelty%20of%20our%20method%20is%0Athat%2C%20by%20formulating%20each%203D%20point%20as%20a%20rigid%20particle%20with%20size%20and%0Aorientation%20in%20space%2C%20we%20directly%20learn%20a%20translation%20rotation%20dynamics%20system%0Afor%20each%20particle%2C%20explicitly%20estimating%20a%20complete%20set%20of%20physical%20parameters%0Ato%20govern%20the%20particle%27s%20motion%20over%20time.%20Extensive%20experiments%20on%20three%0Aexisting%20dynamic%20datasets%20and%20one%20newly%20created%20challenging%20synthetic%20datasets%0Ademonstrate%20the%20extraordinary%20performance%20of%20our%20method%20over%20baselines%20in%20the%0Atask%20of%20future%20frame%20extrapolation.%20A%20nice%20property%20of%20our%20framework%20is%20that%0Amultiple%20objects%20or%20parts%20can%20be%20easily%20segmented%20just%20by%20clustering%20the%0Alearned%20physical%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09811v1&entry.124074799=Read"},
{"title": "LIA-X: Interpretable Latent Portrait Animator", "author": "Yaohui Wang and Di Yang and Xinyuan Chen and Francois Bremond and Yu Qiao and Antitza Dantcheva", "abstract": "  We introduce LIA-X, a novel interpretable portrait animator designed to\ntransfer facial dynamics from a driving video to a source portrait with\nfine-grained control. LIA-X is an autoencoder that models motion transfer as a\nlinear navigation of motion codes in latent space. Crucially, it incorporates a\nnovel Sparse Motion Dictionary that enables the model to disentangle facial\ndynamics into interpretable factors. Deviating from previous 'warp-render'\napproaches, the interpretability of the Sparse Motion Dictionary allows LIA-X\nto support a highly controllable 'edit-warp-render' strategy, enabling precise\nmanipulation of fine-grained facial semantics in the source portrait. This\nhelps to narrow initial differences with the driving video in terms of pose and\nexpression. Moreover, we demonstrate the scalability of LIA-X by successfully\ntraining a large-scale model with approximately 1 billion parameters on\nextensive datasets. Experimental results show that our proposed method\noutperforms previous approaches in both self-reenactment and cross-reenactment\ntasks across several benchmarks. Additionally, the interpretable and\ncontrollable nature of LIA-X supports practical applications such as\nfine-grained, user-guided image and video editing, as well as 3D-aware portrait\nvideo manipulation.\n", "link": "http://arxiv.org/abs/2508.09959v1", "date": "2025-08-13", "relevancy": 3.0111, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6197}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6126}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIA-X%3A%20Interpretable%20Latent%20Portrait%20Animator&body=Title%3A%20LIA-X%3A%20Interpretable%20Latent%20Portrait%20Animator%0AAuthor%3A%20Yaohui%20Wang%20and%20Di%20Yang%20and%20Xinyuan%20Chen%20and%20Francois%20Bremond%20and%20Yu%20Qiao%20and%20Antitza%20Dantcheva%0AAbstract%3A%20%20%20We%20introduce%20LIA-X%2C%20a%20novel%20interpretable%20portrait%20animator%20designed%20to%0Atransfer%20facial%20dynamics%20from%20a%20driving%20video%20to%20a%20source%20portrait%20with%0Afine-grained%20control.%20LIA-X%20is%20an%20autoencoder%20that%20models%20motion%20transfer%20as%20a%0Alinear%20navigation%20of%20motion%20codes%20in%20latent%20space.%20Crucially%2C%20it%20incorporates%20a%0Anovel%20Sparse%20Motion%20Dictionary%20that%20enables%20the%20model%20to%20disentangle%20facial%0Adynamics%20into%20interpretable%20factors.%20Deviating%20from%20previous%20%27warp-render%27%0Aapproaches%2C%20the%20interpretability%20of%20the%20Sparse%20Motion%20Dictionary%20allows%20LIA-X%0Ato%20support%20a%20highly%20controllable%20%27edit-warp-render%27%20strategy%2C%20enabling%20precise%0Amanipulation%20of%20fine-grained%20facial%20semantics%20in%20the%20source%20portrait.%20This%0Ahelps%20to%20narrow%20initial%20differences%20with%20the%20driving%20video%20in%20terms%20of%20pose%20and%0Aexpression.%20Moreover%2C%20we%20demonstrate%20the%20scalability%20of%20LIA-X%20by%20successfully%0Atraining%20a%20large-scale%20model%20with%20approximately%201%20billion%20parameters%20on%0Aextensive%20datasets.%20Experimental%20results%20show%20that%20our%20proposed%20method%0Aoutperforms%20previous%20approaches%20in%20both%20self-reenactment%20and%20cross-reenactment%0Atasks%20across%20several%20benchmarks.%20Additionally%2C%20the%20interpretable%20and%0Acontrollable%20nature%20of%20LIA-X%20supports%20practical%20applications%20such%20as%0Afine-grained%2C%20user-guided%20image%20and%20video%20editing%2C%20as%20well%20as%203D-aware%20portrait%0Avideo%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIA-X%253A%2520Interpretable%2520Latent%2520Portrait%2520Animator%26entry.906535625%3DYaohui%2520Wang%2520and%2520Di%2520Yang%2520and%2520Xinyuan%2520Chen%2520and%2520Francois%2520Bremond%2520and%2520Yu%2520Qiao%2520and%2520Antitza%2520Dantcheva%26entry.1292438233%3D%2520%2520We%2520introduce%2520LIA-X%252C%2520a%2520novel%2520interpretable%2520portrait%2520animator%2520designed%2520to%250Atransfer%2520facial%2520dynamics%2520from%2520a%2520driving%2520video%2520to%2520a%2520source%2520portrait%2520with%250Afine-grained%2520control.%2520LIA-X%2520is%2520an%2520autoencoder%2520that%2520models%2520motion%2520transfer%2520as%2520a%250Alinear%2520navigation%2520of%2520motion%2520codes%2520in%2520latent%2520space.%2520Crucially%252C%2520it%2520incorporates%2520a%250Anovel%2520Sparse%2520Motion%2520Dictionary%2520that%2520enables%2520the%2520model%2520to%2520disentangle%2520facial%250Adynamics%2520into%2520interpretable%2520factors.%2520Deviating%2520from%2520previous%2520%2527warp-render%2527%250Aapproaches%252C%2520the%2520interpretability%2520of%2520the%2520Sparse%2520Motion%2520Dictionary%2520allows%2520LIA-X%250Ato%2520support%2520a%2520highly%2520controllable%2520%2527edit-warp-render%2527%2520strategy%252C%2520enabling%2520precise%250Amanipulation%2520of%2520fine-grained%2520facial%2520semantics%2520in%2520the%2520source%2520portrait.%2520This%250Ahelps%2520to%2520narrow%2520initial%2520differences%2520with%2520the%2520driving%2520video%2520in%2520terms%2520of%2520pose%2520and%250Aexpression.%2520Moreover%252C%2520we%2520demonstrate%2520the%2520scalability%2520of%2520LIA-X%2520by%2520successfully%250Atraining%2520a%2520large-scale%2520model%2520with%2520approximately%25201%2520billion%2520parameters%2520on%250Aextensive%2520datasets.%2520Experimental%2520results%2520show%2520that%2520our%2520proposed%2520method%250Aoutperforms%2520previous%2520approaches%2520in%2520both%2520self-reenactment%2520and%2520cross-reenactment%250Atasks%2520across%2520several%2520benchmarks.%2520Additionally%252C%2520the%2520interpretable%2520and%250Acontrollable%2520nature%2520of%2520LIA-X%2520supports%2520practical%2520applications%2520such%2520as%250Afine-grained%252C%2520user-guided%2520image%2520and%2520video%2520editing%252C%2520as%2520well%2520as%25203D-aware%2520portrait%250Avideo%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIA-X%3A%20Interpretable%20Latent%20Portrait%20Animator&entry.906535625=Yaohui%20Wang%20and%20Di%20Yang%20and%20Xinyuan%20Chen%20and%20Francois%20Bremond%20and%20Yu%20Qiao%20and%20Antitza%20Dantcheva&entry.1292438233=%20%20We%20introduce%20LIA-X%2C%20a%20novel%20interpretable%20portrait%20animator%20designed%20to%0Atransfer%20facial%20dynamics%20from%20a%20driving%20video%20to%20a%20source%20portrait%20with%0Afine-grained%20control.%20LIA-X%20is%20an%20autoencoder%20that%20models%20motion%20transfer%20as%20a%0Alinear%20navigation%20of%20motion%20codes%20in%20latent%20space.%20Crucially%2C%20it%20incorporates%20a%0Anovel%20Sparse%20Motion%20Dictionary%20that%20enables%20the%20model%20to%20disentangle%20facial%0Adynamics%20into%20interpretable%20factors.%20Deviating%20from%20previous%20%27warp-render%27%0Aapproaches%2C%20the%20interpretability%20of%20the%20Sparse%20Motion%20Dictionary%20allows%20LIA-X%0Ato%20support%20a%20highly%20controllable%20%27edit-warp-render%27%20strategy%2C%20enabling%20precise%0Amanipulation%20of%20fine-grained%20facial%20semantics%20in%20the%20source%20portrait.%20This%0Ahelps%20to%20narrow%20initial%20differences%20with%20the%20driving%20video%20in%20terms%20of%20pose%20and%0Aexpression.%20Moreover%2C%20we%20demonstrate%20the%20scalability%20of%20LIA-X%20by%20successfully%0Atraining%20a%20large-scale%20model%20with%20approximately%201%20billion%20parameters%20on%0Aextensive%20datasets.%20Experimental%20results%20show%20that%20our%20proposed%20method%0Aoutperforms%20previous%20approaches%20in%20both%20self-reenactment%20and%20cross-reenactment%0Atasks%20across%20several%20benchmarks.%20Additionally%2C%20the%20interpretable%20and%0Acontrollable%20nature%20of%20LIA-X%20supports%20practical%20applications%20such%20as%0Afine-grained%2C%20user-guided%20image%20and%20video%20editing%2C%20as%20well%20as%203D-aware%20portrait%0Avideo%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09959v1&entry.124074799=Read"},
{"title": "Yan: Foundational Interactive Video Generation", "author": "Deheng Ye and Fangyun Zhou and Jiacheng Lv and Jianqi Ma and Jun Zhang and Junyan Lv and Junyou Li and Minwen Deng and Mingyu Yang and Qiang Fu and Wei Yang and Wenkai Lv and Yangbin Yu and Yewen Wang and Yonghang Guan and Zhihao Hu and Zhongbin Fang and Zhongqian Sun", "abstract": "  We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.\n", "link": "http://arxiv.org/abs/2508.08601v2", "date": "2025-08-13", "relevancy": 3.01, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6106}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5981}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Yan%3A%20Foundational%20Interactive%20Video%20Generation&body=Title%3A%20Yan%3A%20Foundational%20Interactive%20Video%20Generation%0AAuthor%3A%20Deheng%20Ye%20and%20Fangyun%20Zhou%20and%20Jiacheng%20Lv%20and%20Jianqi%20Ma%20and%20Jun%20Zhang%20and%20Junyan%20Lv%20and%20Junyou%20Li%20and%20Minwen%20Deng%20and%20Mingyu%20Yang%20and%20Qiang%20Fu%20and%20Wei%20Yang%20and%20Wenkai%20Lv%20and%20Yangbin%20Yu%20and%20Yewen%20Wang%20and%20Yonghang%20Guan%20and%20Zhihao%20Hu%20and%20Zhongbin%20Fang%20and%20Zhongqian%20Sun%0AAbstract%3A%20%20%20We%20present%20Yan%2C%20a%20foundational%20framework%20for%20interactive%20video%20generation%2C%0Acovering%20the%20entire%20pipeline%20from%20simulation%20and%20generation%20to%20editing.%0ASpecifically%2C%20Yan%20comprises%20three%20core%20modules.%20AAA-level%20Simulation%3A%20We%20design%0Aa%20highly-compressed%2C%20low-latency%203D-VAE%20coupled%20with%20a%20KV-cache-based%0Ashift-window%20denoising%20inference%20process%2C%20achieving%20real-time%201080P/60FPS%0Ainteractive%20simulation.%20Multi-Modal%20Generation%3A%20We%20introduce%20a%20hierarchical%0Aautoregressive%20caption%20method%20that%20injects%20game-specific%20knowledge%20into%0Aopen-domain%20multi-modal%20video%20diffusion%20models%20%28VDMs%29%2C%20then%20transforming%20the%0AVDM%20into%20a%20frame-wise%2C%20action-controllable%2C%20real-time%20infinite%20interactive%0Avideo%20generator.%20Notably%2C%20when%20the%20textual%20and%20visual%20prompts%20are%20sourced%20from%0Adifferent%20domains%2C%20the%20model%20demonstrates%20strong%20generalization%2C%20allowing%20it%20to%0Ablend%20and%20compose%20the%20style%20and%20mechanics%20across%20domains%20flexibly%20according%20to%0Auser%20prompts.%20Multi-Granularity%20Editing%3A%20We%20propose%20a%20hybrid%20model%20that%0Aexplicitly%20disentangles%20interactive%20mechanics%20simulation%20from%20visual%20rendering%2C%0Aenabling%20multi-granularity%20video%20content%20editing%20during%20interaction%20through%0Atext.%20Collectively%2C%20Yan%20offers%20an%20integration%20of%20these%20modules%2C%20pushing%0Ainteractive%20video%20generation%20beyond%20isolated%20capabilities%20toward%20a%0Acomprehensive%20AI-driven%20interactive%20creation%20paradigm%2C%20paving%20the%20way%20for%20the%0Anext%20generation%20of%20creative%20tools%2C%20media%2C%20and%20entertainment.%20The%20project%20page%0Ais%3A%20https%3A//greatx3.github.io/Yan/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08601v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYan%253A%2520Foundational%2520Interactive%2520Video%2520Generation%26entry.906535625%3DDeheng%2520Ye%2520and%2520Fangyun%2520Zhou%2520and%2520Jiacheng%2520Lv%2520and%2520Jianqi%2520Ma%2520and%2520Jun%2520Zhang%2520and%2520Junyan%2520Lv%2520and%2520Junyou%2520Li%2520and%2520Minwen%2520Deng%2520and%2520Mingyu%2520Yang%2520and%2520Qiang%2520Fu%2520and%2520Wei%2520Yang%2520and%2520Wenkai%2520Lv%2520and%2520Yangbin%2520Yu%2520and%2520Yewen%2520Wang%2520and%2520Yonghang%2520Guan%2520and%2520Zhihao%2520Hu%2520and%2520Zhongbin%2520Fang%2520and%2520Zhongqian%2520Sun%26entry.1292438233%3D%2520%2520We%2520present%2520Yan%252C%2520a%2520foundational%2520framework%2520for%2520interactive%2520video%2520generation%252C%250Acovering%2520the%2520entire%2520pipeline%2520from%2520simulation%2520and%2520generation%2520to%2520editing.%250ASpecifically%252C%2520Yan%2520comprises%2520three%2520core%2520modules.%2520AAA-level%2520Simulation%253A%2520We%2520design%250Aa%2520highly-compressed%252C%2520low-latency%25203D-VAE%2520coupled%2520with%2520a%2520KV-cache-based%250Ashift-window%2520denoising%2520inference%2520process%252C%2520achieving%2520real-time%25201080P/60FPS%250Ainteractive%2520simulation.%2520Multi-Modal%2520Generation%253A%2520We%2520introduce%2520a%2520hierarchical%250Aautoregressive%2520caption%2520method%2520that%2520injects%2520game-specific%2520knowledge%2520into%250Aopen-domain%2520multi-modal%2520video%2520diffusion%2520models%2520%2528VDMs%2529%252C%2520then%2520transforming%2520the%250AVDM%2520into%2520a%2520frame-wise%252C%2520action-controllable%252C%2520real-time%2520infinite%2520interactive%250Avideo%2520generator.%2520Notably%252C%2520when%2520the%2520textual%2520and%2520visual%2520prompts%2520are%2520sourced%2520from%250Adifferent%2520domains%252C%2520the%2520model%2520demonstrates%2520strong%2520generalization%252C%2520allowing%2520it%2520to%250Ablend%2520and%2520compose%2520the%2520style%2520and%2520mechanics%2520across%2520domains%2520flexibly%2520according%2520to%250Auser%2520prompts.%2520Multi-Granularity%2520Editing%253A%2520We%2520propose%2520a%2520hybrid%2520model%2520that%250Aexplicitly%2520disentangles%2520interactive%2520mechanics%2520simulation%2520from%2520visual%2520rendering%252C%250Aenabling%2520multi-granularity%2520video%2520content%2520editing%2520during%2520interaction%2520through%250Atext.%2520Collectively%252C%2520Yan%2520offers%2520an%2520integration%2520of%2520these%2520modules%252C%2520pushing%250Ainteractive%2520video%2520generation%2520beyond%2520isolated%2520capabilities%2520toward%2520a%250Acomprehensive%2520AI-driven%2520interactive%2520creation%2520paradigm%252C%2520paving%2520the%2520way%2520for%2520the%250Anext%2520generation%2520of%2520creative%2520tools%252C%2520media%252C%2520and%2520entertainment.%2520The%2520project%2520page%250Ais%253A%2520https%253A//greatx3.github.io/Yan/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08601v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Yan%3A%20Foundational%20Interactive%20Video%20Generation&entry.906535625=Deheng%20Ye%20and%20Fangyun%20Zhou%20and%20Jiacheng%20Lv%20and%20Jianqi%20Ma%20and%20Jun%20Zhang%20and%20Junyan%20Lv%20and%20Junyou%20Li%20and%20Minwen%20Deng%20and%20Mingyu%20Yang%20and%20Qiang%20Fu%20and%20Wei%20Yang%20and%20Wenkai%20Lv%20and%20Yangbin%20Yu%20and%20Yewen%20Wang%20and%20Yonghang%20Guan%20and%20Zhihao%20Hu%20and%20Zhongbin%20Fang%20and%20Zhongqian%20Sun&entry.1292438233=%20%20We%20present%20Yan%2C%20a%20foundational%20framework%20for%20interactive%20video%20generation%2C%0Acovering%20the%20entire%20pipeline%20from%20simulation%20and%20generation%20to%20editing.%0ASpecifically%2C%20Yan%20comprises%20three%20core%20modules.%20AAA-level%20Simulation%3A%20We%20design%0Aa%20highly-compressed%2C%20low-latency%203D-VAE%20coupled%20with%20a%20KV-cache-based%0Ashift-window%20denoising%20inference%20process%2C%20achieving%20real-time%201080P/60FPS%0Ainteractive%20simulation.%20Multi-Modal%20Generation%3A%20We%20introduce%20a%20hierarchical%0Aautoregressive%20caption%20method%20that%20injects%20game-specific%20knowledge%20into%0Aopen-domain%20multi-modal%20video%20diffusion%20models%20%28VDMs%29%2C%20then%20transforming%20the%0AVDM%20into%20a%20frame-wise%2C%20action-controllable%2C%20real-time%20infinite%20interactive%0Avideo%20generator.%20Notably%2C%20when%20the%20textual%20and%20visual%20prompts%20are%20sourced%20from%0Adifferent%20domains%2C%20the%20model%20demonstrates%20strong%20generalization%2C%20allowing%20it%20to%0Ablend%20and%20compose%20the%20style%20and%20mechanics%20across%20domains%20flexibly%20according%20to%0Auser%20prompts.%20Multi-Granularity%20Editing%3A%20We%20propose%20a%20hybrid%20model%20that%0Aexplicitly%20disentangles%20interactive%20mechanics%20simulation%20from%20visual%20rendering%2C%0Aenabling%20multi-granularity%20video%20content%20editing%20during%20interaction%20through%0Atext.%20Collectively%2C%20Yan%20offers%20an%20integration%20of%20these%20modules%2C%20pushing%0Ainteractive%20video%20generation%20beyond%20isolated%20capabilities%20toward%20a%0Acomprehensive%20AI-driven%20interactive%20creation%20paradigm%2C%20paving%20the%20way%20for%20the%0Anext%20generation%20of%20creative%20tools%2C%20media%2C%20and%20entertainment.%20The%20project%20page%0Ais%3A%20https%3A//greatx3.github.io/Yan/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08601v2&entry.124074799=Read"},
{"title": "GLM-4.1V-Thinking and GLM-4.5V: Towards Versatile Multimodal Reasoning\n  with Scalable Reinforcement Learning", "author": "GLM-V Team and  : and Wenyi Hong and Wenmeng Yu and Xiaotao Gu and Guo Wang and Guobing Gan and Haomiao Tang and Jiale Cheng and Ji Qi and Junhui Ji and Lihang Pan and Shuaiqi Duan and Weihan Wang and Yan Wang and Yean Cheng and Zehai He and Zhe Su and Zhen Yang and Ziyang Pan and Aohan Zeng and Baoxu Wang and Bin Chen and Boyan Shi and Changyu Pang and Chenhui Zhang and Da Yin and Fan Yang and Guoqing Chen and Jiazheng Xu and Jiale Zhu and Jiali Chen and Jing Chen and Jinhao Chen and Jinghao Lin and Jinjiang Wang and Junjie Chen and Leqi Lei and Letian Gong and Leyi Pan and Mingdao Liu and Mingzhi Zhang and Qinkai Zheng and Sheng Yang and Shi Zhong and Shiyu Huang and Shuyuan Zhao and Siyan Xue and Shangqin Tu and Shengbiao Meng and Tianshu Zhang and Tianwei Luo and Tianxiang Hao and Wenkai Li and Wei Jia and Xiao Liu and Xiaohan Zhang and Xin Lyu and Xuancheng Huang and Yanling Wang and Yadong Xue and Yanfeng Wang and Yanzi Wang and Yifan An and Yifan Du and Yiming Shi and Yiheng Huang and Yilin Niu and Yuan Wang and Yuanchang Yue and Yuchen Li and Yutao Zhang and Yuting Wang and Yu Wang and Yuxuan Zhang and Zhanxiao Du and Zhenyu Hou and Zhao Xue and Zhengxiao Du and Zihan Wang and Peng Zhang and Debing Liu and Bin Xu and Juanzi Li and Minlie Huang and Yuxiao Dong and Jie Tang", "abstract": "  We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models\n(VLMs) designed to advance general-purpose multimodal understanding and\nreasoning. In this report, we share our key findings in the development of the\nreasoning-centric training framework. We first develop a capable vision\nfoundation model with significant potential through large-scale pre-training,\nwhich arguably sets the upper bound for the final performance. We then propose\nReinforcement Learning with Curriculum Sampling (RLCS) to unlock the full\npotential of the model, leading to comprehensive capability enhancement across\na diverse range of tasks, including STEM problem solving, video understanding,\ncontent recognition, coding, grounding, GUI-based agents, and long document\ninterpretation. In a comprehensive evaluation across 42 public benchmarks,\nGLM-4.5V achieves state-of-the-art performance on nearly all tasks among\nopen-source models of similar size, and demonstrates competitive or even\nsuperior results compared to closed-source models such as Gemini-2.5-Flash on\nchallenging tasks including Coding and GUI Agents. Meanwhile, the smaller\nGLM-4.1V-9B-Thinking remains highly competitive-achieving superior results to\nthe much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source both\nGLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information are\nreleased at https://github.com/zai-org/GLM-V.\n", "link": "http://arxiv.org/abs/2507.01006v3", "date": "2025-08-13", "relevancy": 2.9605, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6071}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLM-4.1V-Thinking%20and%20GLM-4.5V%3A%20Towards%20Versatile%20Multimodal%20Reasoning%0A%20%20with%20Scalable%20Reinforcement%20Learning&body=Title%3A%20GLM-4.1V-Thinking%20and%20GLM-4.5V%3A%20Towards%20Versatile%20Multimodal%20Reasoning%0A%20%20with%20Scalable%20Reinforcement%20Learning%0AAuthor%3A%20GLM-V%20Team%20and%20%20%3A%20and%20Wenyi%20Hong%20and%20Wenmeng%20Yu%20and%20Xiaotao%20Gu%20and%20Guo%20Wang%20and%20Guobing%20Gan%20and%20Haomiao%20Tang%20and%20Jiale%20Cheng%20and%20Ji%20Qi%20and%20Junhui%20Ji%20and%20Lihang%20Pan%20and%20Shuaiqi%20Duan%20and%20Weihan%20Wang%20and%20Yan%20Wang%20and%20Yean%20Cheng%20and%20Zehai%20He%20and%20Zhe%20Su%20and%20Zhen%20Yang%20and%20Ziyang%20Pan%20and%20Aohan%20Zeng%20and%20Baoxu%20Wang%20and%20Bin%20Chen%20and%20Boyan%20Shi%20and%20Changyu%20Pang%20and%20Chenhui%20Zhang%20and%20Da%20Yin%20and%20Fan%20Yang%20and%20Guoqing%20Chen%20and%20Jiazheng%20Xu%20and%20Jiale%20Zhu%20and%20Jiali%20Chen%20and%20Jing%20Chen%20and%20Jinhao%20Chen%20and%20Jinghao%20Lin%20and%20Jinjiang%20Wang%20and%20Junjie%20Chen%20and%20Leqi%20Lei%20and%20Letian%20Gong%20and%20Leyi%20Pan%20and%20Mingdao%20Liu%20and%20Mingzhi%20Zhang%20and%20Qinkai%20Zheng%20and%20Sheng%20Yang%20and%20Shi%20Zhong%20and%20Shiyu%20Huang%20and%20Shuyuan%20Zhao%20and%20Siyan%20Xue%20and%20Shangqin%20Tu%20and%20Shengbiao%20Meng%20and%20Tianshu%20Zhang%20and%20Tianwei%20Luo%20and%20Tianxiang%20Hao%20and%20Wenkai%20Li%20and%20Wei%20Jia%20and%20Xiao%20Liu%20and%20Xiaohan%20Zhang%20and%20Xin%20Lyu%20and%20Xuancheng%20Huang%20and%20Yanling%20Wang%20and%20Yadong%20Xue%20and%20Yanfeng%20Wang%20and%20Yanzi%20Wang%20and%20Yifan%20An%20and%20Yifan%20Du%20and%20Yiming%20Shi%20and%20Yiheng%20Huang%20and%20Yilin%20Niu%20and%20Yuan%20Wang%20and%20Yuanchang%20Yue%20and%20Yuchen%20Li%20and%20Yutao%20Zhang%20and%20Yuting%20Wang%20and%20Yu%20Wang%20and%20Yuxuan%20Zhang%20and%20Zhanxiao%20Du%20and%20Zhenyu%20Hou%20and%20Zhao%20Xue%20and%20Zhengxiao%20Du%20and%20Zihan%20Wang%20and%20Peng%20Zhang%20and%20Debing%20Liu%20and%20Bin%20Xu%20and%20Juanzi%20Li%20and%20Minlie%20Huang%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%0AAbstract%3A%20%20%20We%20present%20GLM-4.1V-Thinking%20and%20GLM-4.5V%2C%20a%20family%20of%20vision-language%20models%0A%28VLMs%29%20designed%20to%20advance%20general-purpose%20multimodal%20understanding%20and%0Areasoning.%20In%20this%20report%2C%20we%20share%20our%20key%20findings%20in%20the%20development%20of%20the%0Areasoning-centric%20training%20framework.%20We%20first%20develop%20a%20capable%20vision%0Afoundation%20model%20with%20significant%20potential%20through%20large-scale%20pre-training%2C%0Awhich%20arguably%20sets%20the%20upper%20bound%20for%20the%20final%20performance.%20We%20then%20propose%0AReinforcement%20Learning%20with%20Curriculum%20Sampling%20%28RLCS%29%20to%20unlock%20the%20full%0Apotential%20of%20the%20model%2C%20leading%20to%20comprehensive%20capability%20enhancement%20across%0Aa%20diverse%20range%20of%20tasks%2C%20including%20STEM%20problem%20solving%2C%20video%20understanding%2C%0Acontent%20recognition%2C%20coding%2C%20grounding%2C%20GUI-based%20agents%2C%20and%20long%20document%0Ainterpretation.%20In%20a%20comprehensive%20evaluation%20across%2042%20public%20benchmarks%2C%0AGLM-4.5V%20achieves%20state-of-the-art%20performance%20on%20nearly%20all%20tasks%20among%0Aopen-source%20models%20of%20similar%20size%2C%20and%20demonstrates%20competitive%20or%20even%0Asuperior%20results%20compared%20to%20closed-source%20models%20such%20as%20Gemini-2.5-Flash%20on%0Achallenging%20tasks%20including%20Coding%20and%20GUI%20Agents.%20Meanwhile%2C%20the%20smaller%0AGLM-4.1V-9B-Thinking%20remains%20highly%20competitive-achieving%20superior%20results%20to%0Athe%20much%20larger%20Qwen2.5-VL-72B%20on%2029%20benchmarks.%20We%20open-source%20both%0AGLM-4.1V-9B-Thinking%20and%20GLM-4.5V.%20Code%2C%20models%20and%20more%20information%20are%0Areleased%20at%20https%3A//github.com/zai-org/GLM-V.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01006v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLM-4.1V-Thinking%2520and%2520GLM-4.5V%253A%2520Towards%2520Versatile%2520Multimodal%2520Reasoning%250A%2520%2520with%2520Scalable%2520Reinforcement%2520Learning%26entry.906535625%3DGLM-V%2520Team%2520and%2520%2520%253A%2520and%2520Wenyi%2520Hong%2520and%2520Wenmeng%2520Yu%2520and%2520Xiaotao%2520Gu%2520and%2520Guo%2520Wang%2520and%2520Guobing%2520Gan%2520and%2520Haomiao%2520Tang%2520and%2520Jiale%2520Cheng%2520and%2520Ji%2520Qi%2520and%2520Junhui%2520Ji%2520and%2520Lihang%2520Pan%2520and%2520Shuaiqi%2520Duan%2520and%2520Weihan%2520Wang%2520and%2520Yan%2520Wang%2520and%2520Yean%2520Cheng%2520and%2520Zehai%2520He%2520and%2520Zhe%2520Su%2520and%2520Zhen%2520Yang%2520and%2520Ziyang%2520Pan%2520and%2520Aohan%2520Zeng%2520and%2520Baoxu%2520Wang%2520and%2520Bin%2520Chen%2520and%2520Boyan%2520Shi%2520and%2520Changyu%2520Pang%2520and%2520Chenhui%2520Zhang%2520and%2520Da%2520Yin%2520and%2520Fan%2520Yang%2520and%2520Guoqing%2520Chen%2520and%2520Jiazheng%2520Xu%2520and%2520Jiale%2520Zhu%2520and%2520Jiali%2520Chen%2520and%2520Jing%2520Chen%2520and%2520Jinhao%2520Chen%2520and%2520Jinghao%2520Lin%2520and%2520Jinjiang%2520Wang%2520and%2520Junjie%2520Chen%2520and%2520Leqi%2520Lei%2520and%2520Letian%2520Gong%2520and%2520Leyi%2520Pan%2520and%2520Mingdao%2520Liu%2520and%2520Mingzhi%2520Zhang%2520and%2520Qinkai%2520Zheng%2520and%2520Sheng%2520Yang%2520and%2520Shi%2520Zhong%2520and%2520Shiyu%2520Huang%2520and%2520Shuyuan%2520Zhao%2520and%2520Siyan%2520Xue%2520and%2520Shangqin%2520Tu%2520and%2520Shengbiao%2520Meng%2520and%2520Tianshu%2520Zhang%2520and%2520Tianwei%2520Luo%2520and%2520Tianxiang%2520Hao%2520and%2520Wenkai%2520Li%2520and%2520Wei%2520Jia%2520and%2520Xiao%2520Liu%2520and%2520Xiaohan%2520Zhang%2520and%2520Xin%2520Lyu%2520and%2520Xuancheng%2520Huang%2520and%2520Yanling%2520Wang%2520and%2520Yadong%2520Xue%2520and%2520Yanfeng%2520Wang%2520and%2520Yanzi%2520Wang%2520and%2520Yifan%2520An%2520and%2520Yifan%2520Du%2520and%2520Yiming%2520Shi%2520and%2520Yiheng%2520Huang%2520and%2520Yilin%2520Niu%2520and%2520Yuan%2520Wang%2520and%2520Yuanchang%2520Yue%2520and%2520Yuchen%2520Li%2520and%2520Yutao%2520Zhang%2520and%2520Yuting%2520Wang%2520and%2520Yu%2520Wang%2520and%2520Yuxuan%2520Zhang%2520and%2520Zhanxiao%2520Du%2520and%2520Zhenyu%2520Hou%2520and%2520Zhao%2520Xue%2520and%2520Zhengxiao%2520Du%2520and%2520Zihan%2520Wang%2520and%2520Peng%2520Zhang%2520and%2520Debing%2520Liu%2520and%2520Bin%2520Xu%2520and%2520Juanzi%2520Li%2520and%2520Minlie%2520Huang%2520and%2520Yuxiao%2520Dong%2520and%2520Jie%2520Tang%26entry.1292438233%3D%2520%2520We%2520present%2520GLM-4.1V-Thinking%2520and%2520GLM-4.5V%252C%2520a%2520family%2520of%2520vision-language%2520models%250A%2528VLMs%2529%2520designed%2520to%2520advance%2520general-purpose%2520multimodal%2520understanding%2520and%250Areasoning.%2520In%2520this%2520report%252C%2520we%2520share%2520our%2520key%2520findings%2520in%2520the%2520development%2520of%2520the%250Areasoning-centric%2520training%2520framework.%2520We%2520first%2520develop%2520a%2520capable%2520vision%250Afoundation%2520model%2520with%2520significant%2520potential%2520through%2520large-scale%2520pre-training%252C%250Awhich%2520arguably%2520sets%2520the%2520upper%2520bound%2520for%2520the%2520final%2520performance.%2520We%2520then%2520propose%250AReinforcement%2520Learning%2520with%2520Curriculum%2520Sampling%2520%2528RLCS%2529%2520to%2520unlock%2520the%2520full%250Apotential%2520of%2520the%2520model%252C%2520leading%2520to%2520comprehensive%2520capability%2520enhancement%2520across%250Aa%2520diverse%2520range%2520of%2520tasks%252C%2520including%2520STEM%2520problem%2520solving%252C%2520video%2520understanding%252C%250Acontent%2520recognition%252C%2520coding%252C%2520grounding%252C%2520GUI-based%2520agents%252C%2520and%2520long%2520document%250Ainterpretation.%2520In%2520a%2520comprehensive%2520evaluation%2520across%252042%2520public%2520benchmarks%252C%250AGLM-4.5V%2520achieves%2520state-of-the-art%2520performance%2520on%2520nearly%2520all%2520tasks%2520among%250Aopen-source%2520models%2520of%2520similar%2520size%252C%2520and%2520demonstrates%2520competitive%2520or%2520even%250Asuperior%2520results%2520compared%2520to%2520closed-source%2520models%2520such%2520as%2520Gemini-2.5-Flash%2520on%250Achallenging%2520tasks%2520including%2520Coding%2520and%2520GUI%2520Agents.%2520Meanwhile%252C%2520the%2520smaller%250AGLM-4.1V-9B-Thinking%2520remains%2520highly%2520competitive-achieving%2520superior%2520results%2520to%250Athe%2520much%2520larger%2520Qwen2.5-VL-72B%2520on%252029%2520benchmarks.%2520We%2520open-source%2520both%250AGLM-4.1V-9B-Thinking%2520and%2520GLM-4.5V.%2520Code%252C%2520models%2520and%2520more%2520information%2520are%250Areleased%2520at%2520https%253A//github.com/zai-org/GLM-V.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01006v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLM-4.1V-Thinking%20and%20GLM-4.5V%3A%20Towards%20Versatile%20Multimodal%20Reasoning%0A%20%20with%20Scalable%20Reinforcement%20Learning&entry.906535625=GLM-V%20Team%20and%20%20%3A%20and%20Wenyi%20Hong%20and%20Wenmeng%20Yu%20and%20Xiaotao%20Gu%20and%20Guo%20Wang%20and%20Guobing%20Gan%20and%20Haomiao%20Tang%20and%20Jiale%20Cheng%20and%20Ji%20Qi%20and%20Junhui%20Ji%20and%20Lihang%20Pan%20and%20Shuaiqi%20Duan%20and%20Weihan%20Wang%20and%20Yan%20Wang%20and%20Yean%20Cheng%20and%20Zehai%20He%20and%20Zhe%20Su%20and%20Zhen%20Yang%20and%20Ziyang%20Pan%20and%20Aohan%20Zeng%20and%20Baoxu%20Wang%20and%20Bin%20Chen%20and%20Boyan%20Shi%20and%20Changyu%20Pang%20and%20Chenhui%20Zhang%20and%20Da%20Yin%20and%20Fan%20Yang%20and%20Guoqing%20Chen%20and%20Jiazheng%20Xu%20and%20Jiale%20Zhu%20and%20Jiali%20Chen%20and%20Jing%20Chen%20and%20Jinhao%20Chen%20and%20Jinghao%20Lin%20and%20Jinjiang%20Wang%20and%20Junjie%20Chen%20and%20Leqi%20Lei%20and%20Letian%20Gong%20and%20Leyi%20Pan%20and%20Mingdao%20Liu%20and%20Mingzhi%20Zhang%20and%20Qinkai%20Zheng%20and%20Sheng%20Yang%20and%20Shi%20Zhong%20and%20Shiyu%20Huang%20and%20Shuyuan%20Zhao%20and%20Siyan%20Xue%20and%20Shangqin%20Tu%20and%20Shengbiao%20Meng%20and%20Tianshu%20Zhang%20and%20Tianwei%20Luo%20and%20Tianxiang%20Hao%20and%20Wenkai%20Li%20and%20Wei%20Jia%20and%20Xiao%20Liu%20and%20Xiaohan%20Zhang%20and%20Xin%20Lyu%20and%20Xuancheng%20Huang%20and%20Yanling%20Wang%20and%20Yadong%20Xue%20and%20Yanfeng%20Wang%20and%20Yanzi%20Wang%20and%20Yifan%20An%20and%20Yifan%20Du%20and%20Yiming%20Shi%20and%20Yiheng%20Huang%20and%20Yilin%20Niu%20and%20Yuan%20Wang%20and%20Yuanchang%20Yue%20and%20Yuchen%20Li%20and%20Yutao%20Zhang%20and%20Yuting%20Wang%20and%20Yu%20Wang%20and%20Yuxuan%20Zhang%20and%20Zhanxiao%20Du%20and%20Zhenyu%20Hou%20and%20Zhao%20Xue%20and%20Zhengxiao%20Du%20and%20Zihan%20Wang%20and%20Peng%20Zhang%20and%20Debing%20Liu%20and%20Bin%20Xu%20and%20Juanzi%20Li%20and%20Minlie%20Huang%20and%20Yuxiao%20Dong%20and%20Jie%20Tang&entry.1292438233=%20%20We%20present%20GLM-4.1V-Thinking%20and%20GLM-4.5V%2C%20a%20family%20of%20vision-language%20models%0A%28VLMs%29%20designed%20to%20advance%20general-purpose%20multimodal%20understanding%20and%0Areasoning.%20In%20this%20report%2C%20we%20share%20our%20key%20findings%20in%20the%20development%20of%20the%0Areasoning-centric%20training%20framework.%20We%20first%20develop%20a%20capable%20vision%0Afoundation%20model%20with%20significant%20potential%20through%20large-scale%20pre-training%2C%0Awhich%20arguably%20sets%20the%20upper%20bound%20for%20the%20final%20performance.%20We%20then%20propose%0AReinforcement%20Learning%20with%20Curriculum%20Sampling%20%28RLCS%29%20to%20unlock%20the%20full%0Apotential%20of%20the%20model%2C%20leading%20to%20comprehensive%20capability%20enhancement%20across%0Aa%20diverse%20range%20of%20tasks%2C%20including%20STEM%20problem%20solving%2C%20video%20understanding%2C%0Acontent%20recognition%2C%20coding%2C%20grounding%2C%20GUI-based%20agents%2C%20and%20long%20document%0Ainterpretation.%20In%20a%20comprehensive%20evaluation%20across%2042%20public%20benchmarks%2C%0AGLM-4.5V%20achieves%20state-of-the-art%20performance%20on%20nearly%20all%20tasks%20among%0Aopen-source%20models%20of%20similar%20size%2C%20and%20demonstrates%20competitive%20or%20even%0Asuperior%20results%20compared%20to%20closed-source%20models%20such%20as%20Gemini-2.5-Flash%20on%0Achallenging%20tasks%20including%20Coding%20and%20GUI%20Agents.%20Meanwhile%2C%20the%20smaller%0AGLM-4.1V-9B-Thinking%20remains%20highly%20competitive-achieving%20superior%20results%20to%0Athe%20much%20larger%20Qwen2.5-VL-72B%20on%2029%20benchmarks.%20We%20open-source%20both%0AGLM-4.1V-9B-Thinking%20and%20GLM-4.5V.%20Code%2C%20models%20and%20more%20information%20are%0Areleased%20at%20https%3A//github.com/zai-org/GLM-V.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01006v3&entry.124074799=Read"},
{"title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and\n  Coding Models", "author": "Lingjie Jiang and Shaohan Huang and Xun Wu and Yixia Li and Dongdong Zhang and Furu Wei", "abstract": "  Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.\n", "link": "http://arxiv.org/abs/2508.09945v1", "date": "2025-08-13", "relevancy": 2.9597, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6099}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisCodex%3A%20Unified%20Multimodal%20Code%20Generation%20via%20Merging%20Vision%20and%0A%20%20Coding%20Models&body=Title%3A%20VisCodex%3A%20Unified%20Multimodal%20Code%20Generation%20via%20Merging%20Vision%20and%0A%20%20Coding%20Models%0AAuthor%3A%20Lingjie%20Jiang%20and%20Shaohan%20Huang%20and%20Xun%20Wu%20and%20Yixia%20Li%20and%20Dongdong%20Zhang%20and%20Furu%20Wei%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20significantly%20advanced%20the%0Aintegration%20of%20visual%20and%20textual%20understanding.%20However%2C%20their%20ability%20to%0Agenerate%20code%20from%20multimodal%20inputs%20remains%20limited.%20In%20this%20work%2C%20we%0Aintroduce%20VisCodex%2C%20a%20unified%20framework%20that%20seamlessly%20merges%20vision%20and%0Acoding%20language%20models%20to%20empower%20MLLMs%20with%20strong%20multimodal%20code%20generation%0Aabilities.%20Leveraging%20a%20task%20vector-based%20model%20merging%20technique%2C%20we%20integrate%0Aa%20state-of-the-art%20coding%20LLM%20into%20a%20strong%20vision-language%20backbone%2C%20while%0Apreserving%20both%20visual%20comprehension%20and%20advanced%20coding%20skills.%20To%20support%0Atraining%20and%20evaluation%2C%20we%20introduce%20the%20Multimodal%20Coding%20Dataset%20%28MCD%29%2C%20a%0Alarge-scale%20and%20diverse%20collection%20of%20598k%20samples%2C%20including%20high-quality%20HTML%0Acode%2C%20chart%20image-code%20pairs%2C%20image-augmented%20StackOverflow%20QA%2C%20and%20algorithmic%0Aproblems.%20Furthermore%2C%20we%20propose%20InfiBench-V%2C%20a%20novel%20and%20challenging%0Abenchmark%20specifically%20designed%20to%20assess%20models%20on%20visually-rich%2C%20real-world%0Aprogramming%20questions%20that%20demand%20a%20nuanced%20understanding%20of%20both%20textual%20and%0Avisual%20contexts.%20Extensive%20experiments%20show%20that%20VisCodex%20achieves%0Astate-of-the-art%20performance%20among%20open-source%20MLLMs%20and%20approaches%20proprietary%0Amodels%20like%20GPT-4o%2C%20highlighting%20the%20effectiveness%20of%20our%20model%20merging%0Astrategy%20and%20new%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisCodex%253A%2520Unified%2520Multimodal%2520Code%2520Generation%2520via%2520Merging%2520Vision%2520and%250A%2520%2520Coding%2520Models%26entry.906535625%3DLingjie%2520Jiang%2520and%2520Shaohan%2520Huang%2520and%2520Xun%2520Wu%2520and%2520Yixia%2520Li%2520and%2520Dongdong%2520Zhang%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520significantly%2520advanced%2520the%250Aintegration%2520of%2520visual%2520and%2520textual%2520understanding.%2520However%252C%2520their%2520ability%2520to%250Agenerate%2520code%2520from%2520multimodal%2520inputs%2520remains%2520limited.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520VisCodex%252C%2520a%2520unified%2520framework%2520that%2520seamlessly%2520merges%2520vision%2520and%250Acoding%2520language%2520models%2520to%2520empower%2520MLLMs%2520with%2520strong%2520multimodal%2520code%2520generation%250Aabilities.%2520Leveraging%2520a%2520task%2520vector-based%2520model%2520merging%2520technique%252C%2520we%2520integrate%250Aa%2520state-of-the-art%2520coding%2520LLM%2520into%2520a%2520strong%2520vision-language%2520backbone%252C%2520while%250Apreserving%2520both%2520visual%2520comprehension%2520and%2520advanced%2520coding%2520skills.%2520To%2520support%250Atraining%2520and%2520evaluation%252C%2520we%2520introduce%2520the%2520Multimodal%2520Coding%2520Dataset%2520%2528MCD%2529%252C%2520a%250Alarge-scale%2520and%2520diverse%2520collection%2520of%2520598k%2520samples%252C%2520including%2520high-quality%2520HTML%250Acode%252C%2520chart%2520image-code%2520pairs%252C%2520image-augmented%2520StackOverflow%2520QA%252C%2520and%2520algorithmic%250Aproblems.%2520Furthermore%252C%2520we%2520propose%2520InfiBench-V%252C%2520a%2520novel%2520and%2520challenging%250Abenchmark%2520specifically%2520designed%2520to%2520assess%2520models%2520on%2520visually-rich%252C%2520real-world%250Aprogramming%2520questions%2520that%2520demand%2520a%2520nuanced%2520understanding%2520of%2520both%2520textual%2520and%250Avisual%2520contexts.%2520Extensive%2520experiments%2520show%2520that%2520VisCodex%2520achieves%250Astate-of-the-art%2520performance%2520among%2520open-source%2520MLLMs%2520and%2520approaches%2520proprietary%250Amodels%2520like%2520GPT-4o%252C%2520highlighting%2520the%2520effectiveness%2520of%2520our%2520model%2520merging%250Astrategy%2520and%2520new%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisCodex%3A%20Unified%20Multimodal%20Code%20Generation%20via%20Merging%20Vision%20and%0A%20%20Coding%20Models&entry.906535625=Lingjie%20Jiang%20and%20Shaohan%20Huang%20and%20Xun%20Wu%20and%20Yixia%20Li%20and%20Dongdong%20Zhang%20and%20Furu%20Wei&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20significantly%20advanced%20the%0Aintegration%20of%20visual%20and%20textual%20understanding.%20However%2C%20their%20ability%20to%0Agenerate%20code%20from%20multimodal%20inputs%20remains%20limited.%20In%20this%20work%2C%20we%0Aintroduce%20VisCodex%2C%20a%20unified%20framework%20that%20seamlessly%20merges%20vision%20and%0Acoding%20language%20models%20to%20empower%20MLLMs%20with%20strong%20multimodal%20code%20generation%0Aabilities.%20Leveraging%20a%20task%20vector-based%20model%20merging%20technique%2C%20we%20integrate%0Aa%20state-of-the-art%20coding%20LLM%20into%20a%20strong%20vision-language%20backbone%2C%20while%0Apreserving%20both%20visual%20comprehension%20and%20advanced%20coding%20skills.%20To%20support%0Atraining%20and%20evaluation%2C%20we%20introduce%20the%20Multimodal%20Coding%20Dataset%20%28MCD%29%2C%20a%0Alarge-scale%20and%20diverse%20collection%20of%20598k%20samples%2C%20including%20high-quality%20HTML%0Acode%2C%20chart%20image-code%20pairs%2C%20image-augmented%20StackOverflow%20QA%2C%20and%20algorithmic%0Aproblems.%20Furthermore%2C%20we%20propose%20InfiBench-V%2C%20a%20novel%20and%20challenging%0Abenchmark%20specifically%20designed%20to%20assess%20models%20on%20visually-rich%2C%20real-world%0Aprogramming%20questions%20that%20demand%20a%20nuanced%20understanding%20of%20both%20textual%20and%0Avisual%20contexts.%20Extensive%20experiments%20show%20that%20VisCodex%20achieves%0Astate-of-the-art%20performance%20among%20open-source%20MLLMs%20and%20approaches%20proprietary%0Amodels%20like%20GPT-4o%2C%20highlighting%20the%20effectiveness%20of%20our%20model%20merging%0Astrategy%20and%20new%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09945v1&entry.124074799=Read"},
{"title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning", "author": "Dake Bu and Wei Huang and Andi Han and Atsushi Nitanda and Taiji Suzuki and Qingfu Zhang and Hau-San Wong", "abstract": "  Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings.\n", "link": "http://arxiv.org/abs/2411.02199v5", "date": "2025-08-13", "relevancy": 2.9519, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5913}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provably%20Transformers%20Harness%20Multi-Concept%20Word%20Semantics%20for%20Efficient%0A%20%20In-Context%20Learning&body=Title%3A%20Provably%20Transformers%20Harness%20Multi-Concept%20Word%20Semantics%20for%20Efficient%0A%20%20In-Context%20Learning%0AAuthor%3A%20Dake%20Bu%20and%20Wei%20Huang%20and%20Andi%20Han%20and%20Atsushi%20Nitanda%20and%20Taiji%20Suzuki%20and%20Qingfu%20Zhang%20and%20Hau-San%20Wong%0AAbstract%3A%20%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20have%20displayed%20remarkable%0Acreative%20prowess%20and%20emergence%20capabilities.%20Existing%20empirical%20studies%20have%0Arevealed%20a%20strong%20connection%20between%20these%20LLMs%27%20impressive%20emergence%20abilities%0Aand%20their%20in-context%20learning%20%28ICL%29%20capacity%2C%20allowing%20them%20to%20solve%20new%20tasks%0Ausing%20only%20task-specific%20prompts%20without%20further%20fine-tuning.%20On%20the%20other%0Ahand%2C%20existing%20empirical%20and%20theoretical%20studies%20also%20show%20that%20there%20is%20a%0Alinear%20regularity%20of%20the%20multi-concept%20encoded%20semantic%20representation%20behind%0Atransformer-based%20LLMs.%20However%2C%20existing%20theoretical%20work%20fail%20to%20build%20up%20an%0Aunderstanding%20of%20the%20connection%20between%20this%20regularity%20and%20the%20innovative%0Apower%20of%20ICL.%20Additionally%2C%20prior%20work%20often%20focuses%20on%20simplified%2C%20unrealistic%0Ascenarios%20involving%20linear%20transformers%20or%20unrealistic%20loss%20functions%2C%20and%20they%0Aachieve%20only%20linear%20or%20sub-linear%20convergence%20rates.%20In%20contrast%2C%20this%20work%0Aprovides%20a%20fine-grained%20mathematical%20analysis%20to%20show%20how%20transformers%20leverage%0Athe%20multi-concept%20semantics%20of%20words%20to%20enable%20powerful%20ICL%20and%20excellent%0Aout-of-distribution%20ICL%20abilities%2C%20offering%20insights%20into%20how%20transformers%0Ainnovate%20solutions%20for%20certain%20unseen%20tasks%20encoded%20with%20multiple%20cross-concept%0Asemantics.%20Inspired%20by%20empirical%20studies%20on%20the%20linear%20latent%20geometry%20of%20LLMs%2C%0Athe%20analysis%20is%20based%20on%20a%20concept-based%20low-noise%20sparse%20coding%20prompt%20model.%0ALeveraging%20advanced%20techniques%2C%20this%20work%20showcases%20the%20exponential%200-1%20loss%0Aconvergence%20over%20the%20highly%20non-convex%20training%20dynamics%2C%20which%20pioneeringly%0Aincorporates%20the%20challenges%20of%20softmax%20self-attention%2C%20ReLU-activated%20MLPs%2C%20and%0Across-entropy%20loss.%20Empirical%20simulations%20corroborate%20the%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02199v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvably%2520Transformers%2520Harness%2520Multi-Concept%2520Word%2520Semantics%2520for%2520Efficient%250A%2520%2520In-Context%2520Learning%26entry.906535625%3DDake%2520Bu%2520and%2520Wei%2520Huang%2520and%2520Andi%2520Han%2520and%2520Atsushi%2520Nitanda%2520and%2520Taiji%2520Suzuki%2520and%2520Qingfu%2520Zhang%2520and%2520Hau-San%2520Wong%26entry.1292438233%3D%2520%2520Transformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520displayed%2520remarkable%250Acreative%2520prowess%2520and%2520emergence%2520capabilities.%2520Existing%2520empirical%2520studies%2520have%250Arevealed%2520a%2520strong%2520connection%2520between%2520these%2520LLMs%2527%2520impressive%2520emergence%2520abilities%250Aand%2520their%2520in-context%2520learning%2520%2528ICL%2529%2520capacity%252C%2520allowing%2520them%2520to%2520solve%2520new%2520tasks%250Ausing%2520only%2520task-specific%2520prompts%2520without%2520further%2520fine-tuning.%2520On%2520the%2520other%250Ahand%252C%2520existing%2520empirical%2520and%2520theoretical%2520studies%2520also%2520show%2520that%2520there%2520is%2520a%250Alinear%2520regularity%2520of%2520the%2520multi-concept%2520encoded%2520semantic%2520representation%2520behind%250Atransformer-based%2520LLMs.%2520However%252C%2520existing%2520theoretical%2520work%2520fail%2520to%2520build%2520up%2520an%250Aunderstanding%2520of%2520the%2520connection%2520between%2520this%2520regularity%2520and%2520the%2520innovative%250Apower%2520of%2520ICL.%2520Additionally%252C%2520prior%2520work%2520often%2520focuses%2520on%2520simplified%252C%2520unrealistic%250Ascenarios%2520involving%2520linear%2520transformers%2520or%2520unrealistic%2520loss%2520functions%252C%2520and%2520they%250Aachieve%2520only%2520linear%2520or%2520sub-linear%2520convergence%2520rates.%2520In%2520contrast%252C%2520this%2520work%250Aprovides%2520a%2520fine-grained%2520mathematical%2520analysis%2520to%2520show%2520how%2520transformers%2520leverage%250Athe%2520multi-concept%2520semantics%2520of%2520words%2520to%2520enable%2520powerful%2520ICL%2520and%2520excellent%250Aout-of-distribution%2520ICL%2520abilities%252C%2520offering%2520insights%2520into%2520how%2520transformers%250Ainnovate%2520solutions%2520for%2520certain%2520unseen%2520tasks%2520encoded%2520with%2520multiple%2520cross-concept%250Asemantics.%2520Inspired%2520by%2520empirical%2520studies%2520on%2520the%2520linear%2520latent%2520geometry%2520of%2520LLMs%252C%250Athe%2520analysis%2520is%2520based%2520on%2520a%2520concept-based%2520low-noise%2520sparse%2520coding%2520prompt%2520model.%250ALeveraging%2520advanced%2520techniques%252C%2520this%2520work%2520showcases%2520the%2520exponential%25200-1%2520loss%250Aconvergence%2520over%2520the%2520highly%2520non-convex%2520training%2520dynamics%252C%2520which%2520pioneeringly%250Aincorporates%2520the%2520challenges%2520of%2520softmax%2520self-attention%252C%2520ReLU-activated%2520MLPs%252C%2520and%250Across-entropy%2520loss.%2520Empirical%2520simulations%2520corroborate%2520the%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02199v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provably%20Transformers%20Harness%20Multi-Concept%20Word%20Semantics%20for%20Efficient%0A%20%20In-Context%20Learning&entry.906535625=Dake%20Bu%20and%20Wei%20Huang%20and%20Andi%20Han%20and%20Atsushi%20Nitanda%20and%20Taiji%20Suzuki%20and%20Qingfu%20Zhang%20and%20Hau-San%20Wong&entry.1292438233=%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20have%20displayed%20remarkable%0Acreative%20prowess%20and%20emergence%20capabilities.%20Existing%20empirical%20studies%20have%0Arevealed%20a%20strong%20connection%20between%20these%20LLMs%27%20impressive%20emergence%20abilities%0Aand%20their%20in-context%20learning%20%28ICL%29%20capacity%2C%20allowing%20them%20to%20solve%20new%20tasks%0Ausing%20only%20task-specific%20prompts%20without%20further%20fine-tuning.%20On%20the%20other%0Ahand%2C%20existing%20empirical%20and%20theoretical%20studies%20also%20show%20that%20there%20is%20a%0Alinear%20regularity%20of%20the%20multi-concept%20encoded%20semantic%20representation%20behind%0Atransformer-based%20LLMs.%20However%2C%20existing%20theoretical%20work%20fail%20to%20build%20up%20an%0Aunderstanding%20of%20the%20connection%20between%20this%20regularity%20and%20the%20innovative%0Apower%20of%20ICL.%20Additionally%2C%20prior%20work%20often%20focuses%20on%20simplified%2C%20unrealistic%0Ascenarios%20involving%20linear%20transformers%20or%20unrealistic%20loss%20functions%2C%20and%20they%0Aachieve%20only%20linear%20or%20sub-linear%20convergence%20rates.%20In%20contrast%2C%20this%20work%0Aprovides%20a%20fine-grained%20mathematical%20analysis%20to%20show%20how%20transformers%20leverage%0Athe%20multi-concept%20semantics%20of%20words%20to%20enable%20powerful%20ICL%20and%20excellent%0Aout-of-distribution%20ICL%20abilities%2C%20offering%20insights%20into%20how%20transformers%0Ainnovate%20solutions%20for%20certain%20unseen%20tasks%20encoded%20with%20multiple%20cross-concept%0Asemantics.%20Inspired%20by%20empirical%20studies%20on%20the%20linear%20latent%20geometry%20of%20LLMs%2C%0Athe%20analysis%20is%20based%20on%20a%20concept-based%20low-noise%20sparse%20coding%20prompt%20model.%0ALeveraging%20advanced%20techniques%2C%20this%20work%20showcases%20the%20exponential%200-1%20loss%0Aconvergence%20over%20the%20highly%20non-convex%20training%20dynamics%2C%20which%20pioneeringly%0Aincorporates%20the%20challenges%20of%20softmax%20self-attention%2C%20ReLU-activated%20MLPs%2C%20and%0Across-entropy%20loss.%20Empirical%20simulations%20corroborate%20the%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02199v5&entry.124074799=Read"},
{"title": "GeoVLA: Empowering 3D Representations in Vision-Language-Action Models", "author": "Lin Sun and Bin Xie and Yingfei Liu and Hao Shi and Tiancai Wang and Jiale Cao", "abstract": "  Vision-Language-Action (VLA) models have emerged as a promising approach for\nenabling robots to follow language instructions and predict corresponding\nactions. However, current VLA models mainly rely on 2D visual inputs,\nneglecting the rich geometric information in the 3D physical world, which\nlimits their spatial awareness and adaptability. In this paper, we present\nGeoVLA, a novel VLA framework that effectively integrates 3D information to\nadvance robotic manipulation. It uses a vision-language model (VLM) to process\nimages and language instructions,extracting fused vision-language embeddings.\nIn parallel, it converts depth maps into point clouds and employs a customized\npoint encoder, called Point Embedding Network, to generate 3D geometric\nembeddings independently. These produced embeddings are then concatenated and\nprocessed by our proposed spatial-aware action expert, called 3D-enhanced\nAction Expert, which combines information from different sensor modalities to\nproduce precise action sequences. Through extensive experiments in both\nsimulation and real-world environments, GeoVLA demonstrates superior\nperformance and robustness. It achieves state-of-the-art results in the LIBERO\nand ManiSkill2 simulation benchmarks and shows remarkable robustness in\nreal-world tasks requiring height adaptability, scale awareness and viewpoint\ninvariance.\n", "link": "http://arxiv.org/abs/2508.09071v2", "date": "2025-08-13", "relevancy": 2.9475, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6075}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoVLA%3A%20Empowering%203D%20Representations%20in%20Vision-Language-Action%20Models&body=Title%3A%20GeoVLA%3A%20Empowering%203D%20Representations%20in%20Vision-Language-Action%20Models%0AAuthor%3A%20Lin%20Sun%20and%20Bin%20Xie%20and%20Yingfei%20Liu%20and%20Hao%20Shi%20and%20Tiancai%20Wang%20and%20Jiale%20Cao%0AAbstract%3A%20%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20a%20promising%20approach%20for%0Aenabling%20robots%20to%20follow%20language%20instructions%20and%20predict%20corresponding%0Aactions.%20However%2C%20current%20VLA%20models%20mainly%20rely%20on%202D%20visual%20inputs%2C%0Aneglecting%20the%20rich%20geometric%20information%20in%20the%203D%20physical%20world%2C%20which%0Alimits%20their%20spatial%20awareness%20and%20adaptability.%20In%20this%20paper%2C%20we%20present%0AGeoVLA%2C%20a%20novel%20VLA%20framework%20that%20effectively%20integrates%203D%20information%20to%0Aadvance%20robotic%20manipulation.%20It%20uses%20a%20vision-language%20model%20%28VLM%29%20to%20process%0Aimages%20and%20language%20instructions%2Cextracting%20fused%20vision-language%20embeddings.%0AIn%20parallel%2C%20it%20converts%20depth%20maps%20into%20point%20clouds%20and%20employs%20a%20customized%0Apoint%20encoder%2C%20called%20Point%20Embedding%20Network%2C%20to%20generate%203D%20geometric%0Aembeddings%20independently.%20These%20produced%20embeddings%20are%20then%20concatenated%20and%0Aprocessed%20by%20our%20proposed%20spatial-aware%20action%20expert%2C%20called%203D-enhanced%0AAction%20Expert%2C%20which%20combines%20information%20from%20different%20sensor%20modalities%20to%0Aproduce%20precise%20action%20sequences.%20Through%20extensive%20experiments%20in%20both%0Asimulation%20and%20real-world%20environments%2C%20GeoVLA%20demonstrates%20superior%0Aperformance%20and%20robustness.%20It%20achieves%20state-of-the-art%20results%20in%20the%20LIBERO%0Aand%20ManiSkill2%20simulation%20benchmarks%20and%20shows%20remarkable%20robustness%20in%0Areal-world%20tasks%20requiring%20height%20adaptability%2C%20scale%20awareness%20and%20viewpoint%0Ainvariance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09071v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoVLA%253A%2520Empowering%25203D%2520Representations%2520in%2520Vision-Language-Action%2520Models%26entry.906535625%3DLin%2520Sun%2520and%2520Bin%2520Xie%2520and%2520Yingfei%2520Liu%2520and%2520Hao%2520Shi%2520and%2520Tiancai%2520Wang%2520and%2520Jiale%2520Cao%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520for%250Aenabling%2520robots%2520to%2520follow%2520language%2520instructions%2520and%2520predict%2520corresponding%250Aactions.%2520However%252C%2520current%2520VLA%2520models%2520mainly%2520rely%2520on%25202D%2520visual%2520inputs%252C%250Aneglecting%2520the%2520rich%2520geometric%2520information%2520in%2520the%25203D%2520physical%2520world%252C%2520which%250Alimits%2520their%2520spatial%2520awareness%2520and%2520adaptability.%2520In%2520this%2520paper%252C%2520we%2520present%250AGeoVLA%252C%2520a%2520novel%2520VLA%2520framework%2520that%2520effectively%2520integrates%25203D%2520information%2520to%250Aadvance%2520robotic%2520manipulation.%2520It%2520uses%2520a%2520vision-language%2520model%2520%2528VLM%2529%2520to%2520process%250Aimages%2520and%2520language%2520instructions%252Cextracting%2520fused%2520vision-language%2520embeddings.%250AIn%2520parallel%252C%2520it%2520converts%2520depth%2520maps%2520into%2520point%2520clouds%2520and%2520employs%2520a%2520customized%250Apoint%2520encoder%252C%2520called%2520Point%2520Embedding%2520Network%252C%2520to%2520generate%25203D%2520geometric%250Aembeddings%2520independently.%2520These%2520produced%2520embeddings%2520are%2520then%2520concatenated%2520and%250Aprocessed%2520by%2520our%2520proposed%2520spatial-aware%2520action%2520expert%252C%2520called%25203D-enhanced%250AAction%2520Expert%252C%2520which%2520combines%2520information%2520from%2520different%2520sensor%2520modalities%2520to%250Aproduce%2520precise%2520action%2520sequences.%2520Through%2520extensive%2520experiments%2520in%2520both%250Asimulation%2520and%2520real-world%2520environments%252C%2520GeoVLA%2520demonstrates%2520superior%250Aperformance%2520and%2520robustness.%2520It%2520achieves%2520state-of-the-art%2520results%2520in%2520the%2520LIBERO%250Aand%2520ManiSkill2%2520simulation%2520benchmarks%2520and%2520shows%2520remarkable%2520robustness%2520in%250Areal-world%2520tasks%2520requiring%2520height%2520adaptability%252C%2520scale%2520awareness%2520and%2520viewpoint%250Ainvariance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09071v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoVLA%3A%20Empowering%203D%20Representations%20in%20Vision-Language-Action%20Models&entry.906535625=Lin%20Sun%20and%20Bin%20Xie%20and%20Yingfei%20Liu%20and%20Hao%20Shi%20and%20Tiancai%20Wang%20and%20Jiale%20Cao&entry.1292438233=%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20a%20promising%20approach%20for%0Aenabling%20robots%20to%20follow%20language%20instructions%20and%20predict%20corresponding%0Aactions.%20However%2C%20current%20VLA%20models%20mainly%20rely%20on%202D%20visual%20inputs%2C%0Aneglecting%20the%20rich%20geometric%20information%20in%20the%203D%20physical%20world%2C%20which%0Alimits%20their%20spatial%20awareness%20and%20adaptability.%20In%20this%20paper%2C%20we%20present%0AGeoVLA%2C%20a%20novel%20VLA%20framework%20that%20effectively%20integrates%203D%20information%20to%0Aadvance%20robotic%20manipulation.%20It%20uses%20a%20vision-language%20model%20%28VLM%29%20to%20process%0Aimages%20and%20language%20instructions%2Cextracting%20fused%20vision-language%20embeddings.%0AIn%20parallel%2C%20it%20converts%20depth%20maps%20into%20point%20clouds%20and%20employs%20a%20customized%0Apoint%20encoder%2C%20called%20Point%20Embedding%20Network%2C%20to%20generate%203D%20geometric%0Aembeddings%20independently.%20These%20produced%20embeddings%20are%20then%20concatenated%20and%0Aprocessed%20by%20our%20proposed%20spatial-aware%20action%20expert%2C%20called%203D-enhanced%0AAction%20Expert%2C%20which%20combines%20information%20from%20different%20sensor%20modalities%20to%0Aproduce%20precise%20action%20sequences.%20Through%20extensive%20experiments%20in%20both%0Asimulation%20and%20real-world%20environments%2C%20GeoVLA%20demonstrates%20superior%0Aperformance%20and%20robustness.%20It%20achieves%20state-of-the-art%20results%20in%20the%20LIBERO%0Aand%20ManiSkill2%20simulation%20benchmarks%20and%20shows%20remarkable%20robustness%20in%0Areal-world%20tasks%20requiring%20height%20adaptability%2C%20scale%20awareness%20and%20viewpoint%0Ainvariance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09071v2&entry.124074799=Read"},
{"title": "LLMC+: Benchmarking Vision-Language Model Compression with a\n  Plug-and-play Toolkit", "author": "Chengtao Lv and Bilang Zhang and Yang Yong and Ruihao Gong and Yushi Huang and Shiqiao Gu and Jiajun Wu and Yumeng Shi and Jinyang Guo and Wenya Wang", "abstract": "  Large Vision-Language Models (VLMs) exhibit impressive multi-modal\ncapabilities but suffer from prohibitive computational and memory demands, due\nto their long visual token sequences and massive parameter sizes. To address\nthese issues, recent works have proposed training-free compression methods.\nHowever, existing efforts often suffer from three major limitations: (1)\nCurrent approaches do not decompose techniques into comparable modules,\nhindering fair evaluation across spatial and temporal redundancy. (2)\nEvaluation confined to simple single-turn tasks, failing to reflect performance\nin realistic scenarios. (3) Isolated use of individual compression techniques,\nwithout exploring their joint potential. To overcome these gaps, we introduce\nLLMC+, a comprehensive VLM compression benchmark with a versatile,\nplug-and-play toolkit. LLMC+ supports over 20 algorithms across five\nrepresentative VLM families and enables systematic study of token-level and\nmodel-level compression. Our benchmark reveals that: (1) Spatial and temporal\nredundancies demand distinct technical strategies. (2) Token reduction methods\ndegrade significantly in multi-turn dialogue and detail-sensitive tasks. (3)\nCombining token and model compression achieves extreme compression with minimal\nperformance loss. We believe LLMC+ will facilitate fair evaluation and inspire\nfuture research in efficient VLM. Our code is available at\nhttps://github.com/ModelTC/LightCompress.\n", "link": "http://arxiv.org/abs/2508.09981v1", "date": "2025-08-13", "relevancy": 2.9086, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5972}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMC%2B%3A%20Benchmarking%20Vision-Language%20Model%20Compression%20with%20a%0A%20%20Plug-and-play%20Toolkit&body=Title%3A%20LLMC%2B%3A%20Benchmarking%20Vision-Language%20Model%20Compression%20with%20a%0A%20%20Plug-and-play%20Toolkit%0AAuthor%3A%20Chengtao%20Lv%20and%20Bilang%20Zhang%20and%20Yang%20Yong%20and%20Ruihao%20Gong%20and%20Yushi%20Huang%20and%20Shiqiao%20Gu%20and%20Jiajun%20Wu%20and%20Yumeng%20Shi%20and%20Jinyang%20Guo%20and%20Wenya%20Wang%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20exhibit%20impressive%20multi-modal%0Acapabilities%20but%20suffer%20from%20prohibitive%20computational%20and%20memory%20demands%2C%20due%0Ato%20their%20long%20visual%20token%20sequences%20and%20massive%20parameter%20sizes.%20To%20address%0Athese%20issues%2C%20recent%20works%20have%20proposed%20training-free%20compression%20methods.%0AHowever%2C%20existing%20efforts%20often%20suffer%20from%20three%20major%20limitations%3A%20%281%29%0ACurrent%20approaches%20do%20not%20decompose%20techniques%20into%20comparable%20modules%2C%0Ahindering%20fair%20evaluation%20across%20spatial%20and%20temporal%20redundancy.%20%282%29%0AEvaluation%20confined%20to%20simple%20single-turn%20tasks%2C%20failing%20to%20reflect%20performance%0Ain%20realistic%20scenarios.%20%283%29%20Isolated%20use%20of%20individual%20compression%20techniques%2C%0Awithout%20exploring%20their%20joint%20potential.%20To%20overcome%20these%20gaps%2C%20we%20introduce%0ALLMC%2B%2C%20a%20comprehensive%20VLM%20compression%20benchmark%20with%20a%20versatile%2C%0Aplug-and-play%20toolkit.%20LLMC%2B%20supports%20over%2020%20algorithms%20across%20five%0Arepresentative%20VLM%20families%20and%20enables%20systematic%20study%20of%20token-level%20and%0Amodel-level%20compression.%20Our%20benchmark%20reveals%20that%3A%20%281%29%20Spatial%20and%20temporal%0Aredundancies%20demand%20distinct%20technical%20strategies.%20%282%29%20Token%20reduction%20methods%0Adegrade%20significantly%20in%20multi-turn%20dialogue%20and%20detail-sensitive%20tasks.%20%283%29%0ACombining%20token%20and%20model%20compression%20achieves%20extreme%20compression%20with%20minimal%0Aperformance%20loss.%20We%20believe%20LLMC%2B%20will%20facilitate%20fair%20evaluation%20and%20inspire%0Afuture%20research%20in%20efficient%20VLM.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ModelTC/LightCompress.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMC%252B%253A%2520Benchmarking%2520Vision-Language%2520Model%2520Compression%2520with%2520a%250A%2520%2520Plug-and-play%2520Toolkit%26entry.906535625%3DChengtao%2520Lv%2520and%2520Bilang%2520Zhang%2520and%2520Yang%2520Yong%2520and%2520Ruihao%2520Gong%2520and%2520Yushi%2520Huang%2520and%2520Shiqiao%2520Gu%2520and%2520Jiajun%2520Wu%2520and%2520Yumeng%2520Shi%2520and%2520Jinyang%2520Guo%2520and%2520Wenya%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520exhibit%2520impressive%2520multi-modal%250Acapabilities%2520but%2520suffer%2520from%2520prohibitive%2520computational%2520and%2520memory%2520demands%252C%2520due%250Ato%2520their%2520long%2520visual%2520token%2520sequences%2520and%2520massive%2520parameter%2520sizes.%2520To%2520address%250Athese%2520issues%252C%2520recent%2520works%2520have%2520proposed%2520training-free%2520compression%2520methods.%250AHowever%252C%2520existing%2520efforts%2520often%2520suffer%2520from%2520three%2520major%2520limitations%253A%2520%25281%2529%250ACurrent%2520approaches%2520do%2520not%2520decompose%2520techniques%2520into%2520comparable%2520modules%252C%250Ahindering%2520fair%2520evaluation%2520across%2520spatial%2520and%2520temporal%2520redundancy.%2520%25282%2529%250AEvaluation%2520confined%2520to%2520simple%2520single-turn%2520tasks%252C%2520failing%2520to%2520reflect%2520performance%250Ain%2520realistic%2520scenarios.%2520%25283%2529%2520Isolated%2520use%2520of%2520individual%2520compression%2520techniques%252C%250Awithout%2520exploring%2520their%2520joint%2520potential.%2520To%2520overcome%2520these%2520gaps%252C%2520we%2520introduce%250ALLMC%252B%252C%2520a%2520comprehensive%2520VLM%2520compression%2520benchmark%2520with%2520a%2520versatile%252C%250Aplug-and-play%2520toolkit.%2520LLMC%252B%2520supports%2520over%252020%2520algorithms%2520across%2520five%250Arepresentative%2520VLM%2520families%2520and%2520enables%2520systematic%2520study%2520of%2520token-level%2520and%250Amodel-level%2520compression.%2520Our%2520benchmark%2520reveals%2520that%253A%2520%25281%2529%2520Spatial%2520and%2520temporal%250Aredundancies%2520demand%2520distinct%2520technical%2520strategies.%2520%25282%2529%2520Token%2520reduction%2520methods%250Adegrade%2520significantly%2520in%2520multi-turn%2520dialogue%2520and%2520detail-sensitive%2520tasks.%2520%25283%2529%250ACombining%2520token%2520and%2520model%2520compression%2520achieves%2520extreme%2520compression%2520with%2520minimal%250Aperformance%2520loss.%2520We%2520believe%2520LLMC%252B%2520will%2520facilitate%2520fair%2520evaluation%2520and%2520inspire%250Afuture%2520research%2520in%2520efficient%2520VLM.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ModelTC/LightCompress.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMC%2B%3A%20Benchmarking%20Vision-Language%20Model%20Compression%20with%20a%0A%20%20Plug-and-play%20Toolkit&entry.906535625=Chengtao%20Lv%20and%20Bilang%20Zhang%20and%20Yang%20Yong%20and%20Ruihao%20Gong%20and%20Yushi%20Huang%20and%20Shiqiao%20Gu%20and%20Jiajun%20Wu%20and%20Yumeng%20Shi%20and%20Jinyang%20Guo%20and%20Wenya%20Wang&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20exhibit%20impressive%20multi-modal%0Acapabilities%20but%20suffer%20from%20prohibitive%20computational%20and%20memory%20demands%2C%20due%0Ato%20their%20long%20visual%20token%20sequences%20and%20massive%20parameter%20sizes.%20To%20address%0Athese%20issues%2C%20recent%20works%20have%20proposed%20training-free%20compression%20methods.%0AHowever%2C%20existing%20efforts%20often%20suffer%20from%20three%20major%20limitations%3A%20%281%29%0ACurrent%20approaches%20do%20not%20decompose%20techniques%20into%20comparable%20modules%2C%0Ahindering%20fair%20evaluation%20across%20spatial%20and%20temporal%20redundancy.%20%282%29%0AEvaluation%20confined%20to%20simple%20single-turn%20tasks%2C%20failing%20to%20reflect%20performance%0Ain%20realistic%20scenarios.%20%283%29%20Isolated%20use%20of%20individual%20compression%20techniques%2C%0Awithout%20exploring%20their%20joint%20potential.%20To%20overcome%20these%20gaps%2C%20we%20introduce%0ALLMC%2B%2C%20a%20comprehensive%20VLM%20compression%20benchmark%20with%20a%20versatile%2C%0Aplug-and-play%20toolkit.%20LLMC%2B%20supports%20over%2020%20algorithms%20across%20five%0Arepresentative%20VLM%20families%20and%20enables%20systematic%20study%20of%20token-level%20and%0Amodel-level%20compression.%20Our%20benchmark%20reveals%20that%3A%20%281%29%20Spatial%20and%20temporal%0Aredundancies%20demand%20distinct%20technical%20strategies.%20%282%29%20Token%20reduction%20methods%0Adegrade%20significantly%20in%20multi-turn%20dialogue%20and%20detail-sensitive%20tasks.%20%283%29%0ACombining%20token%20and%20model%20compression%20achieves%20extreme%20compression%20with%20minimal%0Aperformance%20loss.%20We%20believe%20LLMC%2B%20will%20facilitate%20fair%20evaluation%20and%20inspire%0Afuture%20research%20in%20efficient%20VLM.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ModelTC/LightCompress.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09981v1&entry.124074799=Read"},
{"title": "See the Forest and the Trees: A Synergistic Reasoning Framework for\n  Knowledge-Based Visual Question Answering", "author": "Junjie Wang and Yunhan Tang and Yijie Wang and Zhihao Yuan and Huan Wang and Yangfan He and Bin Li", "abstract": "  Multimodal Large Language Models (MLLMs) have pushed the frontiers of\nKnowledge-Based Visual Question Answering (KBVQA), yet their reasoning is\nfundamentally bottlenecked by a reliance on uni-dimensional evidence. This\n\"seeing only the trees, but not the forest\" approach prevents robust,\nmulti-faceted understanding. Inspired by the principle of seeing both the\nforest and trees, we propose Synergos-VQA, a novel synergistic reasoning\nframework. At its core, Synergos-VQA concurrently generates and fuses three\ncomplementary evidence streams at inference time: (1) Holistic Evidence to\nperceive the entire scene (the \"forest\"), (2) Structural Evidence from a\nprototype-driven module to identify key objects (the \"trees\"), and (3) Causal\nEvidence from a counterfactual probe to ensure the reasoning is robustly\ngrounded. By synergistically fusing this multi-faceted evidence, our framework\nachieves a more comprehensive and reliable reasoning process. Extensive\nexperiments show that Synergos-VQA decisively establishes a new\nstate-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.\nFurthermore, our approach demonstrates strong plug-and-play capabilities,\nsignificantly boosting various open-source MLLMs and proving that superior\nmethodological design can outperform sheer model scale.\n", "link": "http://arxiv.org/abs/2507.17659v3", "date": "2025-08-13", "relevancy": 2.894, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5919}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See%20the%20Forest%20and%20the%20Trees%3A%20A%20Synergistic%20Reasoning%20Framework%20for%0A%20%20Knowledge-Based%20Visual%20Question%20Answering&body=Title%3A%20See%20the%20Forest%20and%20the%20Trees%3A%20A%20Synergistic%20Reasoning%20Framework%20for%0A%20%20Knowledge-Based%20Visual%20Question%20Answering%0AAuthor%3A%20Junjie%20Wang%20and%20Yunhan%20Tang%20and%20Yijie%20Wang%20and%20Zhihao%20Yuan%20and%20Huan%20Wang%20and%20Yangfan%20He%20and%20Bin%20Li%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20pushed%20the%20frontiers%20of%0AKnowledge-Based%20Visual%20Question%20Answering%20%28KBVQA%29%2C%20yet%20their%20reasoning%20is%0Afundamentally%20bottlenecked%20by%20a%20reliance%20on%20uni-dimensional%20evidence.%20This%0A%22seeing%20only%20the%20trees%2C%20but%20not%20the%20forest%22%20approach%20prevents%20robust%2C%0Amulti-faceted%20understanding.%20Inspired%20by%20the%20principle%20of%20seeing%20both%20the%0Aforest%20and%20trees%2C%20we%20propose%20Synergos-VQA%2C%20a%20novel%20synergistic%20reasoning%0Aframework.%20At%20its%20core%2C%20Synergos-VQA%20concurrently%20generates%20and%20fuses%20three%0Acomplementary%20evidence%20streams%20at%20inference%20time%3A%20%281%29%20Holistic%20Evidence%20to%0Aperceive%20the%20entire%20scene%20%28the%20%22forest%22%29%2C%20%282%29%20Structural%20Evidence%20from%20a%0Aprototype-driven%20module%20to%20identify%20key%20objects%20%28the%20%22trees%22%29%2C%20and%20%283%29%20Causal%0AEvidence%20from%20a%20counterfactual%20probe%20to%20ensure%20the%20reasoning%20is%20robustly%0Agrounded.%20By%20synergistically%20fusing%20this%20multi-faceted%20evidence%2C%20our%20framework%0Aachieves%20a%20more%20comprehensive%20and%20reliable%20reasoning%20process.%20Extensive%0Aexperiments%20show%20that%20Synergos-VQA%20decisively%20establishes%20a%20new%0Astate-of-the-art%20on%20three%20challenging%20benchmarks%2C%20including%20OK-VQA%20and%20A-OKVQA.%0AFurthermore%2C%20our%20approach%20demonstrates%20strong%20plug-and-play%20capabilities%2C%0Asignificantly%20boosting%20various%20open-source%20MLLMs%20and%20proving%20that%20superior%0Amethodological%20design%20can%20outperform%20sheer%20model%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17659v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee%2520the%2520Forest%2520and%2520the%2520Trees%253A%2520A%2520Synergistic%2520Reasoning%2520Framework%2520for%250A%2520%2520Knowledge-Based%2520Visual%2520Question%2520Answering%26entry.906535625%3DJunjie%2520Wang%2520and%2520Yunhan%2520Tang%2520and%2520Yijie%2520Wang%2520and%2520Zhihao%2520Yuan%2520and%2520Huan%2520Wang%2520and%2520Yangfan%2520He%2520and%2520Bin%2520Li%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520pushed%2520the%2520frontiers%2520of%250AKnowledge-Based%2520Visual%2520Question%2520Answering%2520%2528KBVQA%2529%252C%2520yet%2520their%2520reasoning%2520is%250Afundamentally%2520bottlenecked%2520by%2520a%2520reliance%2520on%2520uni-dimensional%2520evidence.%2520This%250A%2522seeing%2520only%2520the%2520trees%252C%2520but%2520not%2520the%2520forest%2522%2520approach%2520prevents%2520robust%252C%250Amulti-faceted%2520understanding.%2520Inspired%2520by%2520the%2520principle%2520of%2520seeing%2520both%2520the%250Aforest%2520and%2520trees%252C%2520we%2520propose%2520Synergos-VQA%252C%2520a%2520novel%2520synergistic%2520reasoning%250Aframework.%2520At%2520its%2520core%252C%2520Synergos-VQA%2520concurrently%2520generates%2520and%2520fuses%2520three%250Acomplementary%2520evidence%2520streams%2520at%2520inference%2520time%253A%2520%25281%2529%2520Holistic%2520Evidence%2520to%250Aperceive%2520the%2520entire%2520scene%2520%2528the%2520%2522forest%2522%2529%252C%2520%25282%2529%2520Structural%2520Evidence%2520from%2520a%250Aprototype-driven%2520module%2520to%2520identify%2520key%2520objects%2520%2528the%2520%2522trees%2522%2529%252C%2520and%2520%25283%2529%2520Causal%250AEvidence%2520from%2520a%2520counterfactual%2520probe%2520to%2520ensure%2520the%2520reasoning%2520is%2520robustly%250Agrounded.%2520By%2520synergistically%2520fusing%2520this%2520multi-faceted%2520evidence%252C%2520our%2520framework%250Aachieves%2520a%2520more%2520comprehensive%2520and%2520reliable%2520reasoning%2520process.%2520Extensive%250Aexperiments%2520show%2520that%2520Synergos-VQA%2520decisively%2520establishes%2520a%2520new%250Astate-of-the-art%2520on%2520three%2520challenging%2520benchmarks%252C%2520including%2520OK-VQA%2520and%2520A-OKVQA.%250AFurthermore%252C%2520our%2520approach%2520demonstrates%2520strong%2520plug-and-play%2520capabilities%252C%250Asignificantly%2520boosting%2520various%2520open-source%2520MLLMs%2520and%2520proving%2520that%2520superior%250Amethodological%2520design%2520can%2520outperform%2520sheer%2520model%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17659v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See%20the%20Forest%20and%20the%20Trees%3A%20A%20Synergistic%20Reasoning%20Framework%20for%0A%20%20Knowledge-Based%20Visual%20Question%20Answering&entry.906535625=Junjie%20Wang%20and%20Yunhan%20Tang%20and%20Yijie%20Wang%20and%20Zhihao%20Yuan%20and%20Huan%20Wang%20and%20Yangfan%20He%20and%20Bin%20Li&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20pushed%20the%20frontiers%20of%0AKnowledge-Based%20Visual%20Question%20Answering%20%28KBVQA%29%2C%20yet%20their%20reasoning%20is%0Afundamentally%20bottlenecked%20by%20a%20reliance%20on%20uni-dimensional%20evidence.%20This%0A%22seeing%20only%20the%20trees%2C%20but%20not%20the%20forest%22%20approach%20prevents%20robust%2C%0Amulti-faceted%20understanding.%20Inspired%20by%20the%20principle%20of%20seeing%20both%20the%0Aforest%20and%20trees%2C%20we%20propose%20Synergos-VQA%2C%20a%20novel%20synergistic%20reasoning%0Aframework.%20At%20its%20core%2C%20Synergos-VQA%20concurrently%20generates%20and%20fuses%20three%0Acomplementary%20evidence%20streams%20at%20inference%20time%3A%20%281%29%20Holistic%20Evidence%20to%0Aperceive%20the%20entire%20scene%20%28the%20%22forest%22%29%2C%20%282%29%20Structural%20Evidence%20from%20a%0Aprototype-driven%20module%20to%20identify%20key%20objects%20%28the%20%22trees%22%29%2C%20and%20%283%29%20Causal%0AEvidence%20from%20a%20counterfactual%20probe%20to%20ensure%20the%20reasoning%20is%20robustly%0Agrounded.%20By%20synergistically%20fusing%20this%20multi-faceted%20evidence%2C%20our%20framework%0Aachieves%20a%20more%20comprehensive%20and%20reliable%20reasoning%20process.%20Extensive%0Aexperiments%20show%20that%20Synergos-VQA%20decisively%20establishes%20a%20new%0Astate-of-the-art%20on%20three%20challenging%20benchmarks%2C%20including%20OK-VQA%20and%20A-OKVQA.%0AFurthermore%2C%20our%20approach%20demonstrates%20strong%20plug-and-play%20capabilities%2C%0Asignificantly%20boosting%20various%20open-source%20MLLMs%20and%20proving%20that%20superior%0Amethodological%20design%20can%20outperform%20sheer%20model%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17659v3&entry.124074799=Read"},
{"title": "SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models\n  in Compositional Spatial Intelligence", "author": "Ziyang Gong and Wenhao Li and Oliver Ma and Songyuan Li and Jiayi Ji and Xue Yang and Gen Luo and Junchi Yan and Rongrong Ji", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved remarkable progress in\nvarious multimodal tasks. To pursue higher intelligence in space, MLLMs require\nintegrating multiple atomic spatial capabilities to handle complex and dynamic\ntasks. However, existing benchmarks struggle to comprehensively evaluate the\nspatial intelligence of common MLLMs from the atomic level to the compositional\nlevel. To fill this gap, we present SpaCE-10, a comprehensive benchmark for\ncompositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial\ncapabilities, which are combined to form 8 compositional capabilities. Based on\nthese definitions, we propose a novel hierarchical annotation pipeline to\ngenerate high-quality and diverse question-answer (QA) pairs. With over 150+\nhours of human expert effort, we obtain over 5k QA pairs for 811 real indoor\nscenes in SpaCE-10, which covers various evaluation settings like point cloud\ninput and multi-choice QA. We conduct an extensive evaluation of common MLLMs\non SpaCE-10 and find that even the most advanced MLLM still lags behind humans\nby large margins. Through our careful study, we also draw several significant\nfindings that benefit the MLLM community. For example, we reveal that the\nshortcoming of counting capability greatly limits the compositional spatial\ncapabilities of existing MLLMs. The evaluation code and benchmark datasets are\navailable at https://github.com/Cuzyoung/SpaCE-10.\n", "link": "http://arxiv.org/abs/2506.07966v3", "date": "2025-08-13", "relevancy": 2.8675, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5883}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5883}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaCE-10%3A%20A%20Comprehensive%20Benchmark%20for%20Multimodal%20Large%20Language%20Models%0A%20%20in%20Compositional%20Spatial%20Intelligence&body=Title%3A%20SpaCE-10%3A%20A%20Comprehensive%20Benchmark%20for%20Multimodal%20Large%20Language%20Models%0A%20%20in%20Compositional%20Spatial%20Intelligence%0AAuthor%3A%20Ziyang%20Gong%20and%20Wenhao%20Li%20and%20Oliver%20Ma%20and%20Songyuan%20Li%20and%20Jiayi%20Ji%20and%20Xue%20Yang%20and%20Gen%20Luo%20and%20Junchi%20Yan%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20progress%20in%0Avarious%20multimodal%20tasks.%20To%20pursue%20higher%20intelligence%20in%20space%2C%20MLLMs%20require%0Aintegrating%20multiple%20atomic%20spatial%20capabilities%20to%20handle%20complex%20and%20dynamic%0Atasks.%20However%2C%20existing%20benchmarks%20struggle%20to%20comprehensively%20evaluate%20the%0Aspatial%20intelligence%20of%20common%20MLLMs%20from%20the%20atomic%20level%20to%20the%20compositional%0Alevel.%20To%20fill%20this%20gap%2C%20we%20present%20SpaCE-10%2C%20a%20comprehensive%20benchmark%20for%0Acompositional%20spatial%20evaluations.%20In%20SpaCE-10%2C%20we%20define%2010%20atomic%20spatial%0Acapabilities%2C%20which%20are%20combined%20to%20form%208%20compositional%20capabilities.%20Based%20on%0Athese%20definitions%2C%20we%20propose%20a%20novel%20hierarchical%20annotation%20pipeline%20to%0Agenerate%20high-quality%20and%20diverse%20question-answer%20%28QA%29%20pairs.%20With%20over%20150%2B%0Ahours%20of%20human%20expert%20effort%2C%20we%20obtain%20over%205k%20QA%20pairs%20for%20811%20real%20indoor%0Ascenes%20in%20SpaCE-10%2C%20which%20covers%20various%20evaluation%20settings%20like%20point%20cloud%0Ainput%20and%20multi-choice%20QA.%20We%20conduct%20an%20extensive%20evaluation%20of%20common%20MLLMs%0Aon%20SpaCE-10%20and%20find%20that%20even%20the%20most%20advanced%20MLLM%20still%20lags%20behind%20humans%0Aby%20large%20margins.%20Through%20our%20careful%20study%2C%20we%20also%20draw%20several%20significant%0Afindings%20that%20benefit%20the%20MLLM%20community.%20For%20example%2C%20we%20reveal%20that%20the%0Ashortcoming%20of%20counting%20capability%20greatly%20limits%20the%20compositional%20spatial%0Acapabilities%20of%20existing%20MLLMs.%20The%20evaluation%20code%20and%20benchmark%20datasets%20are%0Aavailable%20at%20https%3A//github.com/Cuzyoung/SpaCE-10.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07966v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaCE-10%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Multimodal%2520Large%2520Language%2520Models%250A%2520%2520in%2520Compositional%2520Spatial%2520Intelligence%26entry.906535625%3DZiyang%2520Gong%2520and%2520Wenhao%2520Li%2520and%2520Oliver%2520Ma%2520and%2520Songyuan%2520Li%2520and%2520Jiayi%2520Ji%2520and%2520Xue%2520Yang%2520and%2520Gen%2520Luo%2520and%2520Junchi%2520Yan%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520in%250Avarious%2520multimodal%2520tasks.%2520To%2520pursue%2520higher%2520intelligence%2520in%2520space%252C%2520MLLMs%2520require%250Aintegrating%2520multiple%2520atomic%2520spatial%2520capabilities%2520to%2520handle%2520complex%2520and%2520dynamic%250Atasks.%2520However%252C%2520existing%2520benchmarks%2520struggle%2520to%2520comprehensively%2520evaluate%2520the%250Aspatial%2520intelligence%2520of%2520common%2520MLLMs%2520from%2520the%2520atomic%2520level%2520to%2520the%2520compositional%250Alevel.%2520To%2520fill%2520this%2520gap%252C%2520we%2520present%2520SpaCE-10%252C%2520a%2520comprehensive%2520benchmark%2520for%250Acompositional%2520spatial%2520evaluations.%2520In%2520SpaCE-10%252C%2520we%2520define%252010%2520atomic%2520spatial%250Acapabilities%252C%2520which%2520are%2520combined%2520to%2520form%25208%2520compositional%2520capabilities.%2520Based%2520on%250Athese%2520definitions%252C%2520we%2520propose%2520a%2520novel%2520hierarchical%2520annotation%2520pipeline%2520to%250Agenerate%2520high-quality%2520and%2520diverse%2520question-answer%2520%2528QA%2529%2520pairs.%2520With%2520over%2520150%252B%250Ahours%2520of%2520human%2520expert%2520effort%252C%2520we%2520obtain%2520over%25205k%2520QA%2520pairs%2520for%2520811%2520real%2520indoor%250Ascenes%2520in%2520SpaCE-10%252C%2520which%2520covers%2520various%2520evaluation%2520settings%2520like%2520point%2520cloud%250Ainput%2520and%2520multi-choice%2520QA.%2520We%2520conduct%2520an%2520extensive%2520evaluation%2520of%2520common%2520MLLMs%250Aon%2520SpaCE-10%2520and%2520find%2520that%2520even%2520the%2520most%2520advanced%2520MLLM%2520still%2520lags%2520behind%2520humans%250Aby%2520large%2520margins.%2520Through%2520our%2520careful%2520study%252C%2520we%2520also%2520draw%2520several%2520significant%250Afindings%2520that%2520benefit%2520the%2520MLLM%2520community.%2520For%2520example%252C%2520we%2520reveal%2520that%2520the%250Ashortcoming%2520of%2520counting%2520capability%2520greatly%2520limits%2520the%2520compositional%2520spatial%250Acapabilities%2520of%2520existing%2520MLLMs.%2520The%2520evaluation%2520code%2520and%2520benchmark%2520datasets%2520are%250Aavailable%2520at%2520https%253A//github.com/Cuzyoung/SpaCE-10.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07966v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaCE-10%3A%20A%20Comprehensive%20Benchmark%20for%20Multimodal%20Large%20Language%20Models%0A%20%20in%20Compositional%20Spatial%20Intelligence&entry.906535625=Ziyang%20Gong%20and%20Wenhao%20Li%20and%20Oliver%20Ma%20and%20Songyuan%20Li%20and%20Jiayi%20Ji%20and%20Xue%20Yang%20and%20Gen%20Luo%20and%20Junchi%20Yan%20and%20Rongrong%20Ji&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20progress%20in%0Avarious%20multimodal%20tasks.%20To%20pursue%20higher%20intelligence%20in%20space%2C%20MLLMs%20require%0Aintegrating%20multiple%20atomic%20spatial%20capabilities%20to%20handle%20complex%20and%20dynamic%0Atasks.%20However%2C%20existing%20benchmarks%20struggle%20to%20comprehensively%20evaluate%20the%0Aspatial%20intelligence%20of%20common%20MLLMs%20from%20the%20atomic%20level%20to%20the%20compositional%0Alevel.%20To%20fill%20this%20gap%2C%20we%20present%20SpaCE-10%2C%20a%20comprehensive%20benchmark%20for%0Acompositional%20spatial%20evaluations.%20In%20SpaCE-10%2C%20we%20define%2010%20atomic%20spatial%0Acapabilities%2C%20which%20are%20combined%20to%20form%208%20compositional%20capabilities.%20Based%20on%0Athese%20definitions%2C%20we%20propose%20a%20novel%20hierarchical%20annotation%20pipeline%20to%0Agenerate%20high-quality%20and%20diverse%20question-answer%20%28QA%29%20pairs.%20With%20over%20150%2B%0Ahours%20of%20human%20expert%20effort%2C%20we%20obtain%20over%205k%20QA%20pairs%20for%20811%20real%20indoor%0Ascenes%20in%20SpaCE-10%2C%20which%20covers%20various%20evaluation%20settings%20like%20point%20cloud%0Ainput%20and%20multi-choice%20QA.%20We%20conduct%20an%20extensive%20evaluation%20of%20common%20MLLMs%0Aon%20SpaCE-10%20and%20find%20that%20even%20the%20most%20advanced%20MLLM%20still%20lags%20behind%20humans%0Aby%20large%20margins.%20Through%20our%20careful%20study%2C%20we%20also%20draw%20several%20significant%0Afindings%20that%20benefit%20the%20MLLM%20community.%20For%20example%2C%20we%20reveal%20that%20the%0Ashortcoming%20of%20counting%20capability%20greatly%20limits%20the%20compositional%20spatial%0Acapabilities%20of%20existing%20MLLMs.%20The%20evaluation%20code%20and%20benchmark%20datasets%20are%0Aavailable%20at%20https%3A//github.com/Cuzyoung/SpaCE-10.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07966v3&entry.124074799=Read"},
{"title": "BridgeDepth: Bridging Monocular and Stereo Reasoning with Latent\n  Alignment", "author": "Tongfan Guan and Jiaxin Guo and Chen Wang and Yun-Hui Liu", "abstract": "  Monocular and stereo depth estimation offer complementary strengths:\nmonocular methods capture rich contextual priors but lack geometric precision,\nwhile stereo approaches leverage epipolar geometry yet struggle with\nambiguities such as reflective or textureless surfaces. Despite post-hoc\nsynergies, these paradigms remain largely disjoint in practice. We introduce a\nunified framework that bridges both through iterative bidirectional alignment\nof their latent representations. At its core, a novel cross-attentive alignment\nmechanism dynamically synchronizes monocular contextual cues with stereo\nhypothesis representations during stereo reasoning. This mutual alignment\nresolves stereo ambiguities (e.g., specular surfaces) by injecting monocular\nstructure priors while refining monocular depth with stereo geometry within a\nsingle network. Extensive experiments demonstrate state-of-the-art results:\n\\textbf{it reduces zero-shot generalization error by $\\!>\\!40\\%$ on Middlebury\nand ETH3D}, while addressing longstanding failures on transparent and\nreflective surfaces. By harmonizing multi-view geometry with monocular context,\nour approach enables robust 3D perception that transcends modality-specific\nlimitations. Codes available at https://github.com/aeolusguan/BridgeDepth.\n", "link": "http://arxiv.org/abs/2508.04611v2", "date": "2025-08-13", "relevancy": 2.8635, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5765}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BridgeDepth%3A%20Bridging%20Monocular%20and%20Stereo%20Reasoning%20with%20Latent%0A%20%20Alignment&body=Title%3A%20BridgeDepth%3A%20Bridging%20Monocular%20and%20Stereo%20Reasoning%20with%20Latent%0A%20%20Alignment%0AAuthor%3A%20Tongfan%20Guan%20and%20Jiaxin%20Guo%20and%20Chen%20Wang%20and%20Yun-Hui%20Liu%0AAbstract%3A%20%20%20Monocular%20and%20stereo%20depth%20estimation%20offer%20complementary%20strengths%3A%0Amonocular%20methods%20capture%20rich%20contextual%20priors%20but%20lack%20geometric%20precision%2C%0Awhile%20stereo%20approaches%20leverage%20epipolar%20geometry%20yet%20struggle%20with%0Aambiguities%20such%20as%20reflective%20or%20textureless%20surfaces.%20Despite%20post-hoc%0Asynergies%2C%20these%20paradigms%20remain%20largely%20disjoint%20in%20practice.%20We%20introduce%20a%0Aunified%20framework%20that%20bridges%20both%20through%20iterative%20bidirectional%20alignment%0Aof%20their%20latent%20representations.%20At%20its%20core%2C%20a%20novel%20cross-attentive%20alignment%0Amechanism%20dynamically%20synchronizes%20monocular%20contextual%20cues%20with%20stereo%0Ahypothesis%20representations%20during%20stereo%20reasoning.%20This%20mutual%20alignment%0Aresolves%20stereo%20ambiguities%20%28e.g.%2C%20specular%20surfaces%29%20by%20injecting%20monocular%0Astructure%20priors%20while%20refining%20monocular%20depth%20with%20stereo%20geometry%20within%20a%0Asingle%20network.%20Extensive%20experiments%20demonstrate%20state-of-the-art%20results%3A%0A%5Ctextbf%7Bit%20reduces%20zero-shot%20generalization%20error%20by%20%24%5C%21%3E%5C%2140%5C%25%24%20on%20Middlebury%0Aand%20ETH3D%7D%2C%20while%20addressing%20longstanding%20failures%20on%20transparent%20and%0Areflective%20surfaces.%20By%20harmonizing%20multi-view%20geometry%20with%20monocular%20context%2C%0Aour%20approach%20enables%20robust%203D%20perception%20that%20transcends%20modality-specific%0Alimitations.%20Codes%20available%20at%20https%3A//github.com/aeolusguan/BridgeDepth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04611v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridgeDepth%253A%2520Bridging%2520Monocular%2520and%2520Stereo%2520Reasoning%2520with%2520Latent%250A%2520%2520Alignment%26entry.906535625%3DTongfan%2520Guan%2520and%2520Jiaxin%2520Guo%2520and%2520Chen%2520Wang%2520and%2520Yun-Hui%2520Liu%26entry.1292438233%3D%2520%2520Monocular%2520and%2520stereo%2520depth%2520estimation%2520offer%2520complementary%2520strengths%253A%250Amonocular%2520methods%2520capture%2520rich%2520contextual%2520priors%2520but%2520lack%2520geometric%2520precision%252C%250Awhile%2520stereo%2520approaches%2520leverage%2520epipolar%2520geometry%2520yet%2520struggle%2520with%250Aambiguities%2520such%2520as%2520reflective%2520or%2520textureless%2520surfaces.%2520Despite%2520post-hoc%250Asynergies%252C%2520these%2520paradigms%2520remain%2520largely%2520disjoint%2520in%2520practice.%2520We%2520introduce%2520a%250Aunified%2520framework%2520that%2520bridges%2520both%2520through%2520iterative%2520bidirectional%2520alignment%250Aof%2520their%2520latent%2520representations.%2520At%2520its%2520core%252C%2520a%2520novel%2520cross-attentive%2520alignment%250Amechanism%2520dynamically%2520synchronizes%2520monocular%2520contextual%2520cues%2520with%2520stereo%250Ahypothesis%2520representations%2520during%2520stereo%2520reasoning.%2520This%2520mutual%2520alignment%250Aresolves%2520stereo%2520ambiguities%2520%2528e.g.%252C%2520specular%2520surfaces%2529%2520by%2520injecting%2520monocular%250Astructure%2520priors%2520while%2520refining%2520monocular%2520depth%2520with%2520stereo%2520geometry%2520within%2520a%250Asingle%2520network.%2520Extensive%2520experiments%2520demonstrate%2520state-of-the-art%2520results%253A%250A%255Ctextbf%257Bit%2520reduces%2520zero-shot%2520generalization%2520error%2520by%2520%2524%255C%2521%253E%255C%252140%255C%2525%2524%2520on%2520Middlebury%250Aand%2520ETH3D%257D%252C%2520while%2520addressing%2520longstanding%2520failures%2520on%2520transparent%2520and%250Areflective%2520surfaces.%2520By%2520harmonizing%2520multi-view%2520geometry%2520with%2520monocular%2520context%252C%250Aour%2520approach%2520enables%2520robust%25203D%2520perception%2520that%2520transcends%2520modality-specific%250Alimitations.%2520Codes%2520available%2520at%2520https%253A//github.com/aeolusguan/BridgeDepth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04611v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BridgeDepth%3A%20Bridging%20Monocular%20and%20Stereo%20Reasoning%20with%20Latent%0A%20%20Alignment&entry.906535625=Tongfan%20Guan%20and%20Jiaxin%20Guo%20and%20Chen%20Wang%20and%20Yun-Hui%20Liu&entry.1292438233=%20%20Monocular%20and%20stereo%20depth%20estimation%20offer%20complementary%20strengths%3A%0Amonocular%20methods%20capture%20rich%20contextual%20priors%20but%20lack%20geometric%20precision%2C%0Awhile%20stereo%20approaches%20leverage%20epipolar%20geometry%20yet%20struggle%20with%0Aambiguities%20such%20as%20reflective%20or%20textureless%20surfaces.%20Despite%20post-hoc%0Asynergies%2C%20these%20paradigms%20remain%20largely%20disjoint%20in%20practice.%20We%20introduce%20a%0Aunified%20framework%20that%20bridges%20both%20through%20iterative%20bidirectional%20alignment%0Aof%20their%20latent%20representations.%20At%20its%20core%2C%20a%20novel%20cross-attentive%20alignment%0Amechanism%20dynamically%20synchronizes%20monocular%20contextual%20cues%20with%20stereo%0Ahypothesis%20representations%20during%20stereo%20reasoning.%20This%20mutual%20alignment%0Aresolves%20stereo%20ambiguities%20%28e.g.%2C%20specular%20surfaces%29%20by%20injecting%20monocular%0Astructure%20priors%20while%20refining%20monocular%20depth%20with%20stereo%20geometry%20within%20a%0Asingle%20network.%20Extensive%20experiments%20demonstrate%20state-of-the-art%20results%3A%0A%5Ctextbf%7Bit%20reduces%20zero-shot%20generalization%20error%20by%20%24%5C%21%3E%5C%2140%5C%25%24%20on%20Middlebury%0Aand%20ETH3D%7D%2C%20while%20addressing%20longstanding%20failures%20on%20transparent%20and%0Areflective%20surfaces.%20By%20harmonizing%20multi-view%20geometry%20with%20monocular%20context%2C%0Aour%20approach%20enables%20robust%203D%20perception%20that%20transcends%20modality-specific%0Alimitations.%20Codes%20available%20at%20https%3A//github.com/aeolusguan/BridgeDepth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04611v2&entry.124074799=Read"},
{"title": "Human-Aligned Procedural Level Generation Reinforcement Learning via\n  Text-Level-Sketch Shared Representation", "author": "In-Chang Baek and Seoyoung Lee and Sung-Hyun Kim and Geumhwan Hwang and KyungJoong Kim", "abstract": "  Human-aligned AI is a critical component of co-creativity, as it enables\nmodels to accurately interpret human intent and generate controllable outputs\nthat align with design goals in collaborative content creation. This direction\nis especially relevant in procedural content generation via reinforcement\nlearning (PCGRL), which is intended to serve as a tool for human designers.\nHowever, existing systems often fall short of exhibiting human-centered\nbehavior, limiting the practical utility of AI-driven generation tools in\nreal-world design workflows. In this paper, we propose VIPCGRL\n(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that\nincorporates three modalities-text, level, and sketches-to extend control\nmodality and enhance human-likeness. We introduce a shared embedding space\ntrained via quadruple contrastive learning across modalities and human-AI\nstyles, and align the policy using an auxiliary reward based on embedding\nsimilarity. Experimental results show that VIPCGRL outperforms existing\nbaselines in human-likeness, as validated by both quantitative metrics and\nhuman evaluations. The code and dataset will be available upon publication.\n", "link": "http://arxiv.org/abs/2508.09860v1", "date": "2025-08-13", "relevancy": 2.8582, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5811}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.575}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Aligned%20Procedural%20Level%20Generation%20Reinforcement%20Learning%20via%0A%20%20Text-Level-Sketch%20Shared%20Representation&body=Title%3A%20Human-Aligned%20Procedural%20Level%20Generation%20Reinforcement%20Learning%20via%0A%20%20Text-Level-Sketch%20Shared%20Representation%0AAuthor%3A%20In-Chang%20Baek%20and%20Seoyoung%20Lee%20and%20Sung-Hyun%20Kim%20and%20Geumhwan%20Hwang%20and%20KyungJoong%20Kim%0AAbstract%3A%20%20%20Human-aligned%20AI%20is%20a%20critical%20component%20of%20co-creativity%2C%20as%20it%20enables%0Amodels%20to%20accurately%20interpret%20human%20intent%20and%20generate%20controllable%20outputs%0Athat%20align%20with%20design%20goals%20in%20collaborative%20content%20creation.%20This%20direction%0Ais%20especially%20relevant%20in%20procedural%20content%20generation%20via%20reinforcement%0Alearning%20%28PCGRL%29%2C%20which%20is%20intended%20to%20serve%20as%20a%20tool%20for%20human%20designers.%0AHowever%2C%20existing%20systems%20often%20fall%20short%20of%20exhibiting%20human-centered%0Abehavior%2C%20limiting%20the%20practical%20utility%20of%20AI-driven%20generation%20tools%20in%0Areal-world%20design%20workflows.%20In%20this%20paper%2C%20we%20propose%20VIPCGRL%0A%28Vision-Instruction%20PCGRL%29%2C%20a%20novel%20deep%20reinforcement%20learning%20framework%20that%0Aincorporates%20three%20modalities-text%2C%20level%2C%20and%20sketches-to%20extend%20control%0Amodality%20and%20enhance%20human-likeness.%20We%20introduce%20a%20shared%20embedding%20space%0Atrained%20via%20quadruple%20contrastive%20learning%20across%20modalities%20and%20human-AI%0Astyles%2C%20and%20align%20the%20policy%20using%20an%20auxiliary%20reward%20based%20on%20embedding%0Asimilarity.%20Experimental%20results%20show%20that%20VIPCGRL%20outperforms%20existing%0Abaselines%20in%20human-likeness%2C%20as%20validated%20by%20both%20quantitative%20metrics%20and%0Ahuman%20evaluations.%20The%20code%20and%20dataset%20will%20be%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Aligned%2520Procedural%2520Level%2520Generation%2520Reinforcement%2520Learning%2520via%250A%2520%2520Text-Level-Sketch%2520Shared%2520Representation%26entry.906535625%3DIn-Chang%2520Baek%2520and%2520Seoyoung%2520Lee%2520and%2520Sung-Hyun%2520Kim%2520and%2520Geumhwan%2520Hwang%2520and%2520KyungJoong%2520Kim%26entry.1292438233%3D%2520%2520Human-aligned%2520AI%2520is%2520a%2520critical%2520component%2520of%2520co-creativity%252C%2520as%2520it%2520enables%250Amodels%2520to%2520accurately%2520interpret%2520human%2520intent%2520and%2520generate%2520controllable%2520outputs%250Athat%2520align%2520with%2520design%2520goals%2520in%2520collaborative%2520content%2520creation.%2520This%2520direction%250Ais%2520especially%2520relevant%2520in%2520procedural%2520content%2520generation%2520via%2520reinforcement%250Alearning%2520%2528PCGRL%2529%252C%2520which%2520is%2520intended%2520to%2520serve%2520as%2520a%2520tool%2520for%2520human%2520designers.%250AHowever%252C%2520existing%2520systems%2520often%2520fall%2520short%2520of%2520exhibiting%2520human-centered%250Abehavior%252C%2520limiting%2520the%2520practical%2520utility%2520of%2520AI-driven%2520generation%2520tools%2520in%250Areal-world%2520design%2520workflows.%2520In%2520this%2520paper%252C%2520we%2520propose%2520VIPCGRL%250A%2528Vision-Instruction%2520PCGRL%2529%252C%2520a%2520novel%2520deep%2520reinforcement%2520learning%2520framework%2520that%250Aincorporates%2520three%2520modalities-text%252C%2520level%252C%2520and%2520sketches-to%2520extend%2520control%250Amodality%2520and%2520enhance%2520human-likeness.%2520We%2520introduce%2520a%2520shared%2520embedding%2520space%250Atrained%2520via%2520quadruple%2520contrastive%2520learning%2520across%2520modalities%2520and%2520human-AI%250Astyles%252C%2520and%2520align%2520the%2520policy%2520using%2520an%2520auxiliary%2520reward%2520based%2520on%2520embedding%250Asimilarity.%2520Experimental%2520results%2520show%2520that%2520VIPCGRL%2520outperforms%2520existing%250Abaselines%2520in%2520human-likeness%252C%2520as%2520validated%2520by%2520both%2520quantitative%2520metrics%2520and%250Ahuman%2520evaluations.%2520The%2520code%2520and%2520dataset%2520will%2520be%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Aligned%20Procedural%20Level%20Generation%20Reinforcement%20Learning%20via%0A%20%20Text-Level-Sketch%20Shared%20Representation&entry.906535625=In-Chang%20Baek%20and%20Seoyoung%20Lee%20and%20Sung-Hyun%20Kim%20and%20Geumhwan%20Hwang%20and%20KyungJoong%20Kim&entry.1292438233=%20%20Human-aligned%20AI%20is%20a%20critical%20component%20of%20co-creativity%2C%20as%20it%20enables%0Amodels%20to%20accurately%20interpret%20human%20intent%20and%20generate%20controllable%20outputs%0Athat%20align%20with%20design%20goals%20in%20collaborative%20content%20creation.%20This%20direction%0Ais%20especially%20relevant%20in%20procedural%20content%20generation%20via%20reinforcement%0Alearning%20%28PCGRL%29%2C%20which%20is%20intended%20to%20serve%20as%20a%20tool%20for%20human%20designers.%0AHowever%2C%20existing%20systems%20often%20fall%20short%20of%20exhibiting%20human-centered%0Abehavior%2C%20limiting%20the%20practical%20utility%20of%20AI-driven%20generation%20tools%20in%0Areal-world%20design%20workflows.%20In%20this%20paper%2C%20we%20propose%20VIPCGRL%0A%28Vision-Instruction%20PCGRL%29%2C%20a%20novel%20deep%20reinforcement%20learning%20framework%20that%0Aincorporates%20three%20modalities-text%2C%20level%2C%20and%20sketches-to%20extend%20control%0Amodality%20and%20enhance%20human-likeness.%20We%20introduce%20a%20shared%20embedding%20space%0Atrained%20via%20quadruple%20contrastive%20learning%20across%20modalities%20and%20human-AI%0Astyles%2C%20and%20align%20the%20policy%20using%20an%20auxiliary%20reward%20based%20on%20embedding%0Asimilarity.%20Experimental%20results%20show%20that%20VIPCGRL%20outperforms%20existing%0Abaselines%20in%20human-likeness%2C%20as%20validated%20by%20both%20quantitative%20metrics%20and%0Ahuman%20evaluations.%20The%20code%20and%20dataset%20will%20be%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09860v1&entry.124074799=Read"},
{"title": "Evolution of Low-Level and Texture Human-CLIP Alignment", "author": "Pablo Hern\u00e1ndez-C\u00e1mara and Jose Manuel Ja\u00e9n-Lorites and Jorge Vila-Tom\u00e1s and Jesus Malo and Valero Laparra", "abstract": "  During the training of multi-modal models like CLIP, we observed an\nintriguing phenomenon: the correlation with low-level human image quality\nassessments peaks in the early epochs before gradually declining. This study\ninvestigates this observation and seeks to understand its causes through two\nkey factors: shape-texture bias alignment and classification accuracy drop\nunder noise. Our findings suggest that CLIP initially learn low-level visual\nfeatures, enhancing its alignment with low-level human perception but also\nincreasing its sensitivity to noise and its texture bias. As training\nprogresses, the model shifts toward more abstract shape-based representations,\nimproving noise robustness but reducing alignment with low-level human\nperception. These results suggest that these factors shared an underlying\nlearning mechanism and provide new insights into optimizing the trade-off\nbetween perceptual alignment and robustness in vision-language models.\n", "link": "http://arxiv.org/abs/2508.09814v1", "date": "2025-08-13", "relevancy": 2.8561, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolution%20of%20Low-Level%20and%20Texture%20Human-CLIP%20Alignment&body=Title%3A%20Evolution%20of%20Low-Level%20and%20Texture%20Human-CLIP%20Alignment%0AAuthor%3A%20Pablo%20Hern%C3%A1ndez-C%C3%A1mara%20and%20Jose%20Manuel%20Ja%C3%A9n-Lorites%20and%20Jorge%20Vila-Tom%C3%A1s%20and%20Jesus%20Malo%20and%20Valero%20Laparra%0AAbstract%3A%20%20%20During%20the%20training%20of%20multi-modal%20models%20like%20CLIP%2C%20we%20observed%20an%0Aintriguing%20phenomenon%3A%20the%20correlation%20with%20low-level%20human%20image%20quality%0Aassessments%20peaks%20in%20the%20early%20epochs%20before%20gradually%20declining.%20This%20study%0Ainvestigates%20this%20observation%20and%20seeks%20to%20understand%20its%20causes%20through%20two%0Akey%20factors%3A%20shape-texture%20bias%20alignment%20and%20classification%20accuracy%20drop%0Aunder%20noise.%20Our%20findings%20suggest%20that%20CLIP%20initially%20learn%20low-level%20visual%0Afeatures%2C%20enhancing%20its%20alignment%20with%20low-level%20human%20perception%20but%20also%0Aincreasing%20its%20sensitivity%20to%20noise%20and%20its%20texture%20bias.%20As%20training%0Aprogresses%2C%20the%20model%20shifts%20toward%20more%20abstract%20shape-based%20representations%2C%0Aimproving%20noise%20robustness%20but%20reducing%20alignment%20with%20low-level%20human%0Aperception.%20These%20results%20suggest%20that%20these%20factors%20shared%20an%20underlying%0Alearning%20mechanism%20and%20provide%20new%20insights%20into%20optimizing%20the%20trade-off%0Abetween%20perceptual%20alignment%20and%20robustness%20in%20vision-language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolution%2520of%2520Low-Level%2520and%2520Texture%2520Human-CLIP%2520Alignment%26entry.906535625%3DPablo%2520Hern%25C3%25A1ndez-C%25C3%25A1mara%2520and%2520Jose%2520Manuel%2520Ja%25C3%25A9n-Lorites%2520and%2520Jorge%2520Vila-Tom%25C3%25A1s%2520and%2520Jesus%2520Malo%2520and%2520Valero%2520Laparra%26entry.1292438233%3D%2520%2520During%2520the%2520training%2520of%2520multi-modal%2520models%2520like%2520CLIP%252C%2520we%2520observed%2520an%250Aintriguing%2520phenomenon%253A%2520the%2520correlation%2520with%2520low-level%2520human%2520image%2520quality%250Aassessments%2520peaks%2520in%2520the%2520early%2520epochs%2520before%2520gradually%2520declining.%2520This%2520study%250Ainvestigates%2520this%2520observation%2520and%2520seeks%2520to%2520understand%2520its%2520causes%2520through%2520two%250Akey%2520factors%253A%2520shape-texture%2520bias%2520alignment%2520and%2520classification%2520accuracy%2520drop%250Aunder%2520noise.%2520Our%2520findings%2520suggest%2520that%2520CLIP%2520initially%2520learn%2520low-level%2520visual%250Afeatures%252C%2520enhancing%2520its%2520alignment%2520with%2520low-level%2520human%2520perception%2520but%2520also%250Aincreasing%2520its%2520sensitivity%2520to%2520noise%2520and%2520its%2520texture%2520bias.%2520As%2520training%250Aprogresses%252C%2520the%2520model%2520shifts%2520toward%2520more%2520abstract%2520shape-based%2520representations%252C%250Aimproving%2520noise%2520robustness%2520but%2520reducing%2520alignment%2520with%2520low-level%2520human%250Aperception.%2520These%2520results%2520suggest%2520that%2520these%2520factors%2520shared%2520an%2520underlying%250Alearning%2520mechanism%2520and%2520provide%2520new%2520insights%2520into%2520optimizing%2520the%2520trade-off%250Abetween%2520perceptual%2520alignment%2520and%2520robustness%2520in%2520vision-language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolution%20of%20Low-Level%20and%20Texture%20Human-CLIP%20Alignment&entry.906535625=Pablo%20Hern%C3%A1ndez-C%C3%A1mara%20and%20Jose%20Manuel%20Ja%C3%A9n-Lorites%20and%20Jorge%20Vila-Tom%C3%A1s%20and%20Jesus%20Malo%20and%20Valero%20Laparra&entry.1292438233=%20%20During%20the%20training%20of%20multi-modal%20models%20like%20CLIP%2C%20we%20observed%20an%0Aintriguing%20phenomenon%3A%20the%20correlation%20with%20low-level%20human%20image%20quality%0Aassessments%20peaks%20in%20the%20early%20epochs%20before%20gradually%20declining.%20This%20study%0Ainvestigates%20this%20observation%20and%20seeks%20to%20understand%20its%20causes%20through%20two%0Akey%20factors%3A%20shape-texture%20bias%20alignment%20and%20classification%20accuracy%20drop%0Aunder%20noise.%20Our%20findings%20suggest%20that%20CLIP%20initially%20learn%20low-level%20visual%0Afeatures%2C%20enhancing%20its%20alignment%20with%20low-level%20human%20perception%20but%20also%0Aincreasing%20its%20sensitivity%20to%20noise%20and%20its%20texture%20bias.%20As%20training%0Aprogresses%2C%20the%20model%20shifts%20toward%20more%20abstract%20shape-based%20representations%2C%0Aimproving%20noise%20robustness%20but%20reducing%20alignment%20with%20low-level%20human%0Aperception.%20These%20results%20suggest%20that%20these%20factors%20shared%20an%20underlying%0Alearning%20mechanism%20and%20provide%20new%20insights%20into%20optimizing%20the%20trade-off%0Abetween%20perceptual%20alignment%20and%20robustness%20in%20vision-language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09814v1&entry.124074799=Read"},
{"title": "ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT\n  3D Images", "author": "Jan Phillipp Albrecht and Jose R. A. Godinho and Christina H\u00fcbers and Deborah Schmidt", "abstract": "  X-ray computed tomography (CT) is the main 3D technique for imaging the\ninternal microstructures of materials. Quantitative analysis of the\nmicrostructures is usually achieved by applying a sequence of steps that are\nimplemented to the entire 3D image. This is challenged by various imaging\nartifacts inherent from the technique, e.g., beam hardening and partial volume.\nConsequently, the analysis requires users to make a number of decisions to\nsegment and classify the microstructures based on the voxel gray-values. In\nthis context, a software tool, here called ARI3D, is proposed to interactively\nanalyze regions in three-dimensional X-ray CT images, assisting users through\nthe various steps of a protocol designed to classify and quantify objects\nwithin regions of a three-dimensional image. ARI3D aims to 1) Improve phase\nidentification; 2) Account for partial volume effect; 3) Increase the detection\nlimit and accuracy of object quantification; and 4) Harmonize quantitative 3D\nanalysis that can be implemented in different fields of science.\n", "link": "http://arxiv.org/abs/2508.09849v1", "date": "2025-08-13", "relevancy": 2.843, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6119}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARI3D%3A%20A%20Software%20for%20Interactive%20Quantification%20of%20Regions%20in%20X-Ray%20CT%0A%20%203D%20Images&body=Title%3A%20ARI3D%3A%20A%20Software%20for%20Interactive%20Quantification%20of%20Regions%20in%20X-Ray%20CT%0A%20%203D%20Images%0AAuthor%3A%20Jan%20Phillipp%20Albrecht%20and%20Jose%20R.%20A.%20Godinho%20and%20Christina%20H%C3%BCbers%20and%20Deborah%20Schmidt%0AAbstract%3A%20%20%20X-ray%20computed%20tomography%20%28CT%29%20is%20the%20main%203D%20technique%20for%20imaging%20the%0Ainternal%20microstructures%20of%20materials.%20Quantitative%20analysis%20of%20the%0Amicrostructures%20is%20usually%20achieved%20by%20applying%20a%20sequence%20of%20steps%20that%20are%0Aimplemented%20to%20the%20entire%203D%20image.%20This%20is%20challenged%20by%20various%20imaging%0Aartifacts%20inherent%20from%20the%20technique%2C%20e.g.%2C%20beam%20hardening%20and%20partial%20volume.%0AConsequently%2C%20the%20analysis%20requires%20users%20to%20make%20a%20number%20of%20decisions%20to%0Asegment%20and%20classify%20the%20microstructures%20based%20on%20the%20voxel%20gray-values.%20In%0Athis%20context%2C%20a%20software%20tool%2C%20here%20called%20ARI3D%2C%20is%20proposed%20to%20interactively%0Aanalyze%20regions%20in%20three-dimensional%20X-ray%20CT%20images%2C%20assisting%20users%20through%0Athe%20various%20steps%20of%20a%20protocol%20designed%20to%20classify%20and%20quantify%20objects%0Awithin%20regions%20of%20a%20three-dimensional%20image.%20ARI3D%20aims%20to%201%29%20Improve%20phase%0Aidentification%3B%202%29%20Account%20for%20partial%20volume%20effect%3B%203%29%20Increase%20the%20detection%0Alimit%20and%20accuracy%20of%20object%20quantification%3B%20and%204%29%20Harmonize%20quantitative%203D%0Aanalysis%20that%20can%20be%20implemented%20in%20different%20fields%20of%20science.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARI3D%253A%2520A%2520Software%2520for%2520Interactive%2520Quantification%2520of%2520Regions%2520in%2520X-Ray%2520CT%250A%2520%25203D%2520Images%26entry.906535625%3DJan%2520Phillipp%2520Albrecht%2520and%2520Jose%2520R.%2520A.%2520Godinho%2520and%2520Christina%2520H%25C3%25BCbers%2520and%2520Deborah%2520Schmidt%26entry.1292438233%3D%2520%2520X-ray%2520computed%2520tomography%2520%2528CT%2529%2520is%2520the%2520main%25203D%2520technique%2520for%2520imaging%2520the%250Ainternal%2520microstructures%2520of%2520materials.%2520Quantitative%2520analysis%2520of%2520the%250Amicrostructures%2520is%2520usually%2520achieved%2520by%2520applying%2520a%2520sequence%2520of%2520steps%2520that%2520are%250Aimplemented%2520to%2520the%2520entire%25203D%2520image.%2520This%2520is%2520challenged%2520by%2520various%2520imaging%250Aartifacts%2520inherent%2520from%2520the%2520technique%252C%2520e.g.%252C%2520beam%2520hardening%2520and%2520partial%2520volume.%250AConsequently%252C%2520the%2520analysis%2520requires%2520users%2520to%2520make%2520a%2520number%2520of%2520decisions%2520to%250Asegment%2520and%2520classify%2520the%2520microstructures%2520based%2520on%2520the%2520voxel%2520gray-values.%2520In%250Athis%2520context%252C%2520a%2520software%2520tool%252C%2520here%2520called%2520ARI3D%252C%2520is%2520proposed%2520to%2520interactively%250Aanalyze%2520regions%2520in%2520three-dimensional%2520X-ray%2520CT%2520images%252C%2520assisting%2520users%2520through%250Athe%2520various%2520steps%2520of%2520a%2520protocol%2520designed%2520to%2520classify%2520and%2520quantify%2520objects%250Awithin%2520regions%2520of%2520a%2520three-dimensional%2520image.%2520ARI3D%2520aims%2520to%25201%2529%2520Improve%2520phase%250Aidentification%253B%25202%2529%2520Account%2520for%2520partial%2520volume%2520effect%253B%25203%2529%2520Increase%2520the%2520detection%250Alimit%2520and%2520accuracy%2520of%2520object%2520quantification%253B%2520and%25204%2529%2520Harmonize%2520quantitative%25203D%250Aanalysis%2520that%2520can%2520be%2520implemented%2520in%2520different%2520fields%2520of%2520science.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARI3D%3A%20A%20Software%20for%20Interactive%20Quantification%20of%20Regions%20in%20X-Ray%20CT%0A%20%203D%20Images&entry.906535625=Jan%20Phillipp%20Albrecht%20and%20Jose%20R.%20A.%20Godinho%20and%20Christina%20H%C3%BCbers%20and%20Deborah%20Schmidt&entry.1292438233=%20%20X-ray%20computed%20tomography%20%28CT%29%20is%20the%20main%203D%20technique%20for%20imaging%20the%0Ainternal%20microstructures%20of%20materials.%20Quantitative%20analysis%20of%20the%0Amicrostructures%20is%20usually%20achieved%20by%20applying%20a%20sequence%20of%20steps%20that%20are%0Aimplemented%20to%20the%20entire%203D%20image.%20This%20is%20challenged%20by%20various%20imaging%0Aartifacts%20inherent%20from%20the%20technique%2C%20e.g.%2C%20beam%20hardening%20and%20partial%20volume.%0AConsequently%2C%20the%20analysis%20requires%20users%20to%20make%20a%20number%20of%20decisions%20to%0Asegment%20and%20classify%20the%20microstructures%20based%20on%20the%20voxel%20gray-values.%20In%0Athis%20context%2C%20a%20software%20tool%2C%20here%20called%20ARI3D%2C%20is%20proposed%20to%20interactively%0Aanalyze%20regions%20in%20three-dimensional%20X-ray%20CT%20images%2C%20assisting%20users%20through%0Athe%20various%20steps%20of%20a%20protocol%20designed%20to%20classify%20and%20quantify%20objects%0Awithin%20regions%20of%20a%20three-dimensional%20image.%20ARI3D%20aims%20to%201%29%20Improve%20phase%0Aidentification%3B%202%29%20Account%20for%20partial%20volume%20effect%3B%203%29%20Increase%20the%20detection%0Alimit%20and%20accuracy%20of%20object%20quantification%3B%20and%204%29%20Harmonize%20quantitative%203D%0Aanalysis%20that%20can%20be%20implemented%20in%20different%20fields%20of%20science.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09849v1&entry.124074799=Read"},
{"title": "MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision\n  Language Models", "author": "Dianyi Wang and Siyuan Wang and Zejun Li and Yikun Wang and Yitong Li and Duyu Tang and Xiaoyu Shen and Xuanjing Huang and Zhongyu Wei", "abstract": "  Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross multi-modal tasks by scaling model size and training data. However,\nthese dense LVLMs incur significant computational costs and motivate the\nexploration of sparse Mixture of Experts (MoE) architectures. While MoE improve\nparameter efficiency, effectively applying MoE to simultaneously model\nmodality-specific features and cross-modal associations in LVLMs remains\nchallenging. In this work, we propose to incorporate Mixture of Intra- and\nInter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is\nguided by its modality, directing tokens to their respective intra-modality\nexperts as well as a shared pool of inter-modality experts, enabling the model\nto jointly learn rich intra-modal features and cross-modal interactions. We\nfurther introduce an effective and straightforward two-stage training strategy,\nwhich facilitates the direct activation of both MoE and multi-modal\ncapabilities. Extensive experiments across different data scales and LLM\nbackbone demonstrate the effectiveness, efficiency and generality of our\napproach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters\nmatch or even surpass the performance of existing advanced open-source MoE-LLMs\nbased multi-modal models that involve more activated parameters. The code is\navailable at https://github.com/AlenjandroWang/MoIIE.\n", "link": "http://arxiv.org/abs/2508.09779v1", "date": "2025-08-13", "relevancy": 2.8033, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoIIE%3A%20Mixture%20of%20Intra-%20and%20Inter-Modality%20Experts%20for%20Large%20Vision%0A%20%20Language%20Models&body=Title%3A%20MoIIE%3A%20Mixture%20of%20Intra-%20and%20Inter-Modality%20Experts%20for%20Large%20Vision%0A%20%20Language%20Models%0AAuthor%3A%20Dianyi%20Wang%20and%20Siyuan%20Wang%20and%20Zejun%20Li%20and%20Yikun%20Wang%20and%20Yitong%20Li%20and%20Duyu%20Tang%20and%20Xiaoyu%20Shen%20and%20Xuanjing%20Huang%20and%20Zhongyu%20Wei%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%20performance%0Aacross%20multi-modal%20tasks%20by%20scaling%20model%20size%20and%20training%20data.%20However%2C%0Athese%20dense%20LVLMs%20incur%20significant%20computational%20costs%20and%20motivate%20the%0Aexploration%20of%20sparse%20Mixture%20of%20Experts%20%28MoE%29%20architectures.%20While%20MoE%20improve%0Aparameter%20efficiency%2C%20effectively%20applying%20MoE%20to%20simultaneously%20model%0Amodality-specific%20features%20and%20cross-modal%20associations%20in%20LVLMs%20remains%0Achallenging.%20In%20this%20work%2C%20we%20propose%20to%20incorporate%20Mixture%20of%20Intra-%20and%0AInter-Modality%20Experts%20%28MoIIE%29%20to%20LVLMs.%20For%20each%20token%2C%20expert%20routing%20is%0Aguided%20by%20its%20modality%2C%20directing%20tokens%20to%20their%20respective%20intra-modality%0Aexperts%20as%20well%20as%20a%20shared%20pool%20of%20inter-modality%20experts%2C%20enabling%20the%20model%0Ato%20jointly%20learn%20rich%20intra-modal%20features%20and%20cross-modal%20interactions.%20We%0Afurther%20introduce%20an%20effective%20and%20straightforward%20two-stage%20training%20strategy%2C%0Awhich%20facilitates%20the%20direct%20activation%20of%20both%20MoE%20and%20multi-modal%0Acapabilities.%20Extensive%20experiments%20across%20different%20data%20scales%20and%20LLM%0Abackbone%20demonstrate%20the%20effectiveness%2C%20efficiency%20and%20generality%20of%20our%0Aapproach.%20Notably%2C%20our%20MoIIE%20models%20with%205.5B%20and%2011.3B%20activated%20parameters%0Amatch%20or%20even%20surpass%20the%20performance%20of%20existing%20advanced%20open-source%20MoE-LLMs%0Abased%20multi-modal%20models%20that%20involve%20more%20activated%20parameters.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/AlenjandroWang/MoIIE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoIIE%253A%2520Mixture%2520of%2520Intra-%2520and%2520Inter-Modality%2520Experts%2520for%2520Large%2520Vision%250A%2520%2520Language%2520Models%26entry.906535625%3DDianyi%2520Wang%2520and%2520Siyuan%2520Wang%2520and%2520Zejun%2520Li%2520and%2520Yikun%2520Wang%2520and%2520Yitong%2520Li%2520and%2520Duyu%2520Tang%2520and%2520Xiaoyu%2520Shen%2520and%2520Xuanjing%2520Huang%2520and%2520Zhongyu%2520Wei%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%250Aacross%2520multi-modal%2520tasks%2520by%2520scaling%2520model%2520size%2520and%2520training%2520data.%2520However%252C%250Athese%2520dense%2520LVLMs%2520incur%2520significant%2520computational%2520costs%2520and%2520motivate%2520the%250Aexploration%2520of%2520sparse%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520architectures.%2520While%2520MoE%2520improve%250Aparameter%2520efficiency%252C%2520effectively%2520applying%2520MoE%2520to%2520simultaneously%2520model%250Amodality-specific%2520features%2520and%2520cross-modal%2520associations%2520in%2520LVLMs%2520remains%250Achallenging.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520incorporate%2520Mixture%2520of%2520Intra-%2520and%250AInter-Modality%2520Experts%2520%2528MoIIE%2529%2520to%2520LVLMs.%2520For%2520each%2520token%252C%2520expert%2520routing%2520is%250Aguided%2520by%2520its%2520modality%252C%2520directing%2520tokens%2520to%2520their%2520respective%2520intra-modality%250Aexperts%2520as%2520well%2520as%2520a%2520shared%2520pool%2520of%2520inter-modality%2520experts%252C%2520enabling%2520the%2520model%250Ato%2520jointly%2520learn%2520rich%2520intra-modal%2520features%2520and%2520cross-modal%2520interactions.%2520We%250Afurther%2520introduce%2520an%2520effective%2520and%2520straightforward%2520two-stage%2520training%2520strategy%252C%250Awhich%2520facilitates%2520the%2520direct%2520activation%2520of%2520both%2520MoE%2520and%2520multi-modal%250Acapabilities.%2520Extensive%2520experiments%2520across%2520different%2520data%2520scales%2520and%2520LLM%250Abackbone%2520demonstrate%2520the%2520effectiveness%252C%2520efficiency%2520and%2520generality%2520of%2520our%250Aapproach.%2520Notably%252C%2520our%2520MoIIE%2520models%2520with%25205.5B%2520and%252011.3B%2520activated%2520parameters%250Amatch%2520or%2520even%2520surpass%2520the%2520performance%2520of%2520existing%2520advanced%2520open-source%2520MoE-LLMs%250Abased%2520multi-modal%2520models%2520that%2520involve%2520more%2520activated%2520parameters.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/AlenjandroWang/MoIIE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoIIE%3A%20Mixture%20of%20Intra-%20and%20Inter-Modality%20Experts%20for%20Large%20Vision%0A%20%20Language%20Models&entry.906535625=Dianyi%20Wang%20and%20Siyuan%20Wang%20and%20Zejun%20Li%20and%20Yikun%20Wang%20and%20Yitong%20Li%20and%20Duyu%20Tang%20and%20Xiaoyu%20Shen%20and%20Xuanjing%20Huang%20and%20Zhongyu%20Wei&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%20performance%0Aacross%20multi-modal%20tasks%20by%20scaling%20model%20size%20and%20training%20data.%20However%2C%0Athese%20dense%20LVLMs%20incur%20significant%20computational%20costs%20and%20motivate%20the%0Aexploration%20of%20sparse%20Mixture%20of%20Experts%20%28MoE%29%20architectures.%20While%20MoE%20improve%0Aparameter%20efficiency%2C%20effectively%20applying%20MoE%20to%20simultaneously%20model%0Amodality-specific%20features%20and%20cross-modal%20associations%20in%20LVLMs%20remains%0Achallenging.%20In%20this%20work%2C%20we%20propose%20to%20incorporate%20Mixture%20of%20Intra-%20and%0AInter-Modality%20Experts%20%28MoIIE%29%20to%20LVLMs.%20For%20each%20token%2C%20expert%20routing%20is%0Aguided%20by%20its%20modality%2C%20directing%20tokens%20to%20their%20respective%20intra-modality%0Aexperts%20as%20well%20as%20a%20shared%20pool%20of%20inter-modality%20experts%2C%20enabling%20the%20model%0Ato%20jointly%20learn%20rich%20intra-modal%20features%20and%20cross-modal%20interactions.%20We%0Afurther%20introduce%20an%20effective%20and%20straightforward%20two-stage%20training%20strategy%2C%0Awhich%20facilitates%20the%20direct%20activation%20of%20both%20MoE%20and%20multi-modal%0Acapabilities.%20Extensive%20experiments%20across%20different%20data%20scales%20and%20LLM%0Abackbone%20demonstrate%20the%20effectiveness%2C%20efficiency%20and%20generality%20of%20our%0Aapproach.%20Notably%2C%20our%20MoIIE%20models%20with%205.5B%20and%2011.3B%20activated%20parameters%0Amatch%20or%20even%20surpass%20the%20performance%20of%20existing%20advanced%20open-source%20MoE-LLMs%0Abased%20multi-modal%20models%20that%20involve%20more%20activated%20parameters.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/AlenjandroWang/MoIIE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09779v1&entry.124074799=Read"},
{"title": "Towards flexible perception with visual memory", "author": "Robert Geirhos and Priyank Jaini and Austin Stone and Sourabh Medapati and Xi Yi and George Toderici and Abhijit Ogale and Jonathon Shlens", "abstract": "  Training a neural network is a monolithic endeavor, akin to carving knowledge\ninto stone: once the process is completed, editing the knowledge in a network\nis hard, since all information is distributed across the network's weights. We\nhere explore a simple, compelling alternative by marrying the representational\npower of deep neural networks with the flexibility of a database. Decomposing\nthe task of image classification into image similarity (from a pre-trained\nembedding) and search (via fast nearest neighbor retrieval from a knowledge\ndatabase), we build on well-established components to construct a simple and\nflexible visual memory that has the following key capabilities: (1.) The\nability to flexibly add data across scales: from individual samples all the way\nto entire classes and billion-scale data; (2.) The ability to remove data\nthrough unlearning and memory pruning; (3.) An interpretable decision-mechanism\non which we can intervene to control its behavior. Taken together, these\ncapabilities comprehensively demonstrate the benefits of an explicit visual\nmemory. We hope that it might contribute to a conversation on how knowledge\nshould be represented in deep vision models -- beyond carving it in \"stone\"\nweights.\n", "link": "http://arxiv.org/abs/2408.08172v3", "date": "2025-08-13", "relevancy": 2.7737, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20flexible%20perception%20with%20visual%20memory&body=Title%3A%20Towards%20flexible%20perception%20with%20visual%20memory%0AAuthor%3A%20Robert%20Geirhos%20and%20Priyank%20Jaini%20and%20Austin%20Stone%20and%20Sourabh%20Medapati%20and%20Xi%20Yi%20and%20George%20Toderici%20and%20Abhijit%20Ogale%20and%20Jonathon%20Shlens%0AAbstract%3A%20%20%20Training%20a%20neural%20network%20is%20a%20monolithic%20endeavor%2C%20akin%20to%20carving%20knowledge%0Ainto%20stone%3A%20once%20the%20process%20is%20completed%2C%20editing%20the%20knowledge%20in%20a%20network%0Ais%20hard%2C%20since%20all%20information%20is%20distributed%20across%20the%20network%27s%20weights.%20We%0Ahere%20explore%20a%20simple%2C%20compelling%20alternative%20by%20marrying%20the%20representational%0Apower%20of%20deep%20neural%20networks%20with%20the%20flexibility%20of%20a%20database.%20Decomposing%0Athe%20task%20of%20image%20classification%20into%20image%20similarity%20%28from%20a%20pre-trained%0Aembedding%29%20and%20search%20%28via%20fast%20nearest%20neighbor%20retrieval%20from%20a%20knowledge%0Adatabase%29%2C%20we%20build%20on%20well-established%20components%20to%20construct%20a%20simple%20and%0Aflexible%20visual%20memory%20that%20has%20the%20following%20key%20capabilities%3A%20%281.%29%20The%0Aability%20to%20flexibly%20add%20data%20across%20scales%3A%20from%20individual%20samples%20all%20the%20way%0Ato%20entire%20classes%20and%20billion-scale%20data%3B%20%282.%29%20The%20ability%20to%20remove%20data%0Athrough%20unlearning%20and%20memory%20pruning%3B%20%283.%29%20An%20interpretable%20decision-mechanism%0Aon%20which%20we%20can%20intervene%20to%20control%20its%20behavior.%20Taken%20together%2C%20these%0Acapabilities%20comprehensively%20demonstrate%20the%20benefits%20of%20an%20explicit%20visual%0Amemory.%20We%20hope%20that%20it%20might%20contribute%20to%20a%20conversation%20on%20how%20knowledge%0Ashould%20be%20represented%20in%20deep%20vision%20models%20--%20beyond%20carving%20it%20in%20%22stone%22%0Aweights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08172v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520flexible%2520perception%2520with%2520visual%2520memory%26entry.906535625%3DRobert%2520Geirhos%2520and%2520Priyank%2520Jaini%2520and%2520Austin%2520Stone%2520and%2520Sourabh%2520Medapati%2520and%2520Xi%2520Yi%2520and%2520George%2520Toderici%2520and%2520Abhijit%2520Ogale%2520and%2520Jonathon%2520Shlens%26entry.1292438233%3D%2520%2520Training%2520a%2520neural%2520network%2520is%2520a%2520monolithic%2520endeavor%252C%2520akin%2520to%2520carving%2520knowledge%250Ainto%2520stone%253A%2520once%2520the%2520process%2520is%2520completed%252C%2520editing%2520the%2520knowledge%2520in%2520a%2520network%250Ais%2520hard%252C%2520since%2520all%2520information%2520is%2520distributed%2520across%2520the%2520network%2527s%2520weights.%2520We%250Ahere%2520explore%2520a%2520simple%252C%2520compelling%2520alternative%2520by%2520marrying%2520the%2520representational%250Apower%2520of%2520deep%2520neural%2520networks%2520with%2520the%2520flexibility%2520of%2520a%2520database.%2520Decomposing%250Athe%2520task%2520of%2520image%2520classification%2520into%2520image%2520similarity%2520%2528from%2520a%2520pre-trained%250Aembedding%2529%2520and%2520search%2520%2528via%2520fast%2520nearest%2520neighbor%2520retrieval%2520from%2520a%2520knowledge%250Adatabase%2529%252C%2520we%2520build%2520on%2520well-established%2520components%2520to%2520construct%2520a%2520simple%2520and%250Aflexible%2520visual%2520memory%2520that%2520has%2520the%2520following%2520key%2520capabilities%253A%2520%25281.%2529%2520The%250Aability%2520to%2520flexibly%2520add%2520data%2520across%2520scales%253A%2520from%2520individual%2520samples%2520all%2520the%2520way%250Ato%2520entire%2520classes%2520and%2520billion-scale%2520data%253B%2520%25282.%2529%2520The%2520ability%2520to%2520remove%2520data%250Athrough%2520unlearning%2520and%2520memory%2520pruning%253B%2520%25283.%2529%2520An%2520interpretable%2520decision-mechanism%250Aon%2520which%2520we%2520can%2520intervene%2520to%2520control%2520its%2520behavior.%2520Taken%2520together%252C%2520these%250Acapabilities%2520comprehensively%2520demonstrate%2520the%2520benefits%2520of%2520an%2520explicit%2520visual%250Amemory.%2520We%2520hope%2520that%2520it%2520might%2520contribute%2520to%2520a%2520conversation%2520on%2520how%2520knowledge%250Ashould%2520be%2520represented%2520in%2520deep%2520vision%2520models%2520--%2520beyond%2520carving%2520it%2520in%2520%2522stone%2522%250Aweights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08172v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20flexible%20perception%20with%20visual%20memory&entry.906535625=Robert%20Geirhos%20and%20Priyank%20Jaini%20and%20Austin%20Stone%20and%20Sourabh%20Medapati%20and%20Xi%20Yi%20and%20George%20Toderici%20and%20Abhijit%20Ogale%20and%20Jonathon%20Shlens&entry.1292438233=%20%20Training%20a%20neural%20network%20is%20a%20monolithic%20endeavor%2C%20akin%20to%20carving%20knowledge%0Ainto%20stone%3A%20once%20the%20process%20is%20completed%2C%20editing%20the%20knowledge%20in%20a%20network%0Ais%20hard%2C%20since%20all%20information%20is%20distributed%20across%20the%20network%27s%20weights.%20We%0Ahere%20explore%20a%20simple%2C%20compelling%20alternative%20by%20marrying%20the%20representational%0Apower%20of%20deep%20neural%20networks%20with%20the%20flexibility%20of%20a%20database.%20Decomposing%0Athe%20task%20of%20image%20classification%20into%20image%20similarity%20%28from%20a%20pre-trained%0Aembedding%29%20and%20search%20%28via%20fast%20nearest%20neighbor%20retrieval%20from%20a%20knowledge%0Adatabase%29%2C%20we%20build%20on%20well-established%20components%20to%20construct%20a%20simple%20and%0Aflexible%20visual%20memory%20that%20has%20the%20following%20key%20capabilities%3A%20%281.%29%20The%0Aability%20to%20flexibly%20add%20data%20across%20scales%3A%20from%20individual%20samples%20all%20the%20way%0Ato%20entire%20classes%20and%20billion-scale%20data%3B%20%282.%29%20The%20ability%20to%20remove%20data%0Athrough%20unlearning%20and%20memory%20pruning%3B%20%283.%29%20An%20interpretable%20decision-mechanism%0Aon%20which%20we%20can%20intervene%20to%20control%20its%20behavior.%20Taken%20together%2C%20these%0Acapabilities%20comprehensively%20demonstrate%20the%20benefits%20of%20an%20explicit%20visual%0Amemory.%20We%20hope%20that%20it%20might%20contribute%20to%20a%20conversation%20on%20how%20knowledge%0Ashould%20be%20represented%20in%20deep%20vision%20models%20--%20beyond%20carving%20it%20in%20%22stone%22%0Aweights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08172v3&entry.124074799=Read"},
{"title": "Explaining Caption-Image Interactions in CLIP Models with Second-Order\n  Attributions", "author": "Lucas M\u00f6ller and Pascal Tilli and Ngoc Thang Vu and Sebastian Pad\u00f3", "abstract": "  Dual encoder architectures like Clip models map two types of inputs into a\nshared embedding space and predict similarities between them. Despite their\nwide application, it is, however, not understood how these models compare their\ntwo inputs. Common first-order feature-attribution methods explain importances\nof individual features and can, thus, only provide limited insights into dual\nencoders, whose predictions depend on interactions between features. In this\npaper, we first derive a second-order method enabling the attribution of\npredictions by any differentiable dual encoder onto feature-interactions\nbetween its inputs. Second, we apply our method to Clip models and show that\nthey learn fine-grained correspondences between parts of captions and regions\nin images. They match objects across input modes and also account for\nmismatches. This intrinsic visual-linguistic grounding ability, however, varies\nheavily between object classes, exhibits pronounced out-of-domain effects and\nwe can identify individual errors as well as systematic failure categories.\nCode is publicly available: https://github.com/lucasmllr/exCLIP\n", "link": "http://arxiv.org/abs/2408.14153v4", "date": "2025-08-13", "relevancy": 2.7704, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20Caption-Image%20Interactions%20in%20CLIP%20Models%20with%20Second-Order%0A%20%20Attributions&body=Title%3A%20Explaining%20Caption-Image%20Interactions%20in%20CLIP%20Models%20with%20Second-Order%0A%20%20Attributions%0AAuthor%3A%20Lucas%20M%C3%B6ller%20and%20Pascal%20Tilli%20and%20Ngoc%20Thang%20Vu%20and%20Sebastian%20Pad%C3%B3%0AAbstract%3A%20%20%20Dual%20encoder%20architectures%20like%20Clip%20models%20map%20two%20types%20of%20inputs%20into%20a%0Ashared%20embedding%20space%20and%20predict%20similarities%20between%20them.%20Despite%20their%0Awide%20application%2C%20it%20is%2C%20however%2C%20not%20understood%20how%20these%20models%20compare%20their%0Atwo%20inputs.%20Common%20first-order%20feature-attribution%20methods%20explain%20importances%0Aof%20individual%20features%20and%20can%2C%20thus%2C%20only%20provide%20limited%20insights%20into%20dual%0Aencoders%2C%20whose%20predictions%20depend%20on%20interactions%20between%20features.%20In%20this%0Apaper%2C%20we%20first%20derive%20a%20second-order%20method%20enabling%20the%20attribution%20of%0Apredictions%20by%20any%20differentiable%20dual%20encoder%20onto%20feature-interactions%0Abetween%20its%20inputs.%20Second%2C%20we%20apply%20our%20method%20to%20Clip%20models%20and%20show%20that%0Athey%20learn%20fine-grained%20correspondences%20between%20parts%20of%20captions%20and%20regions%0Ain%20images.%20They%20match%20objects%20across%20input%20modes%20and%20also%20account%20for%0Amismatches.%20This%20intrinsic%20visual-linguistic%20grounding%20ability%2C%20however%2C%20varies%0Aheavily%20between%20object%20classes%2C%20exhibits%20pronounced%20out-of-domain%20effects%20and%0Awe%20can%20identify%20individual%20errors%20as%20well%20as%20systematic%20failure%20categories.%0ACode%20is%20publicly%20available%3A%20https%3A//github.com/lucasmllr/exCLIP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14153v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520Caption-Image%2520Interactions%2520in%2520CLIP%2520Models%2520with%2520Second-Order%250A%2520%2520Attributions%26entry.906535625%3DLucas%2520M%25C3%25B6ller%2520and%2520Pascal%2520Tilli%2520and%2520Ngoc%2520Thang%2520Vu%2520and%2520Sebastian%2520Pad%25C3%25B3%26entry.1292438233%3D%2520%2520Dual%2520encoder%2520architectures%2520like%2520Clip%2520models%2520map%2520two%2520types%2520of%2520inputs%2520into%2520a%250Ashared%2520embedding%2520space%2520and%2520predict%2520similarities%2520between%2520them.%2520Despite%2520their%250Awide%2520application%252C%2520it%2520is%252C%2520however%252C%2520not%2520understood%2520how%2520these%2520models%2520compare%2520their%250Atwo%2520inputs.%2520Common%2520first-order%2520feature-attribution%2520methods%2520explain%2520importances%250Aof%2520individual%2520features%2520and%2520can%252C%2520thus%252C%2520only%2520provide%2520limited%2520insights%2520into%2520dual%250Aencoders%252C%2520whose%2520predictions%2520depend%2520on%2520interactions%2520between%2520features.%2520In%2520this%250Apaper%252C%2520we%2520first%2520derive%2520a%2520second-order%2520method%2520enabling%2520the%2520attribution%2520of%250Apredictions%2520by%2520any%2520differentiable%2520dual%2520encoder%2520onto%2520feature-interactions%250Abetween%2520its%2520inputs.%2520Second%252C%2520we%2520apply%2520our%2520method%2520to%2520Clip%2520models%2520and%2520show%2520that%250Athey%2520learn%2520fine-grained%2520correspondences%2520between%2520parts%2520of%2520captions%2520and%2520regions%250Ain%2520images.%2520They%2520match%2520objects%2520across%2520input%2520modes%2520and%2520also%2520account%2520for%250Amismatches.%2520This%2520intrinsic%2520visual-linguistic%2520grounding%2520ability%252C%2520however%252C%2520varies%250Aheavily%2520between%2520object%2520classes%252C%2520exhibits%2520pronounced%2520out-of-domain%2520effects%2520and%250Awe%2520can%2520identify%2520individual%2520errors%2520as%2520well%2520as%2520systematic%2520failure%2520categories.%250ACode%2520is%2520publicly%2520available%253A%2520https%253A//github.com/lucasmllr/exCLIP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14153v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20Caption-Image%20Interactions%20in%20CLIP%20Models%20with%20Second-Order%0A%20%20Attributions&entry.906535625=Lucas%20M%C3%B6ller%20and%20Pascal%20Tilli%20and%20Ngoc%20Thang%20Vu%20and%20Sebastian%20Pad%C3%B3&entry.1292438233=%20%20Dual%20encoder%20architectures%20like%20Clip%20models%20map%20two%20types%20of%20inputs%20into%20a%0Ashared%20embedding%20space%20and%20predict%20similarities%20between%20them.%20Despite%20their%0Awide%20application%2C%20it%20is%2C%20however%2C%20not%20understood%20how%20these%20models%20compare%20their%0Atwo%20inputs.%20Common%20first-order%20feature-attribution%20methods%20explain%20importances%0Aof%20individual%20features%20and%20can%2C%20thus%2C%20only%20provide%20limited%20insights%20into%20dual%0Aencoders%2C%20whose%20predictions%20depend%20on%20interactions%20between%20features.%20In%20this%0Apaper%2C%20we%20first%20derive%20a%20second-order%20method%20enabling%20the%20attribution%20of%0Apredictions%20by%20any%20differentiable%20dual%20encoder%20onto%20feature-interactions%0Abetween%20its%20inputs.%20Second%2C%20we%20apply%20our%20method%20to%20Clip%20models%20and%20show%20that%0Athey%20learn%20fine-grained%20correspondences%20between%20parts%20of%20captions%20and%20regions%0Ain%20images.%20They%20match%20objects%20across%20input%20modes%20and%20also%20account%20for%0Amismatches.%20This%20intrinsic%20visual-linguistic%20grounding%20ability%2C%20however%2C%20varies%0Aheavily%20between%20object%20classes%2C%20exhibits%20pronounced%20out-of-domain%20effects%20and%0Awe%20can%20identify%20individual%20errors%20as%20well%20as%20systematic%20failure%20categories.%0ACode%20is%20publicly%20available%3A%20https%3A//github.com/lucasmllr/exCLIP%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14153v4&entry.124074799=Read"},
{"title": "Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting\n  CLIP in MERC", "author": "Guanyu Hu and Dimitrios Kollias and Xinyu Yang", "abstract": "  Multimodal Emotion Recognition in Conversations remains a challenging task\ndue to the complex interplay of textual, acoustic and visual signals. While\nrecent models have improved performance via advanced fusion strategies, they\noften lack psychologically meaningful priors to guide multimodal alignment. In\nthis paper, we revisit the use of CLIP and propose a novel Visual Emotion\nGuided Anchoring (VEGA) mechanism that introduces class-level visual semantics\ninto the fusion and classification process. Distinct from prior work that\nprimarily utilizes CLIP's textual encoder, our approach leverages its image\nencoder to construct emotion-specific visual anchors based on facial exemplars.\nThese anchors guide unimodal and multimodal features toward a perceptually\ngrounded and psychologically aligned representation space, drawing inspiration\nfrom cognitive theories (prototypical emotion categories and multisensory\nintegration). A stochastic anchor sampling strategy further enhances robustness\nby balancing semantic stability and intra-class diversity. Integrated into a\ndual-branch architecture with self-distillation, our VEGA-augmented model\nachieves sota performance on IEMOCAP and MELD. Code is available at:\nhttps://github.com/dkollias/VEGA.\n", "link": "http://arxiv.org/abs/2508.06564v2", "date": "2025-08-13", "relevancy": 2.7596, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20Emotion%20Recognition%20with%20Visual%20Prototypes%3A%20VEGA%20--%20Revisiting%0A%20%20CLIP%20in%20MERC&body=Title%3A%20Grounding%20Emotion%20Recognition%20with%20Visual%20Prototypes%3A%20VEGA%20--%20Revisiting%0A%20%20CLIP%20in%20MERC%0AAuthor%3A%20Guanyu%20Hu%20and%20Dimitrios%20Kollias%20and%20Xinyu%20Yang%0AAbstract%3A%20%20%20Multimodal%20Emotion%20Recognition%20in%20Conversations%20remains%20a%20challenging%20task%0Adue%20to%20the%20complex%20interplay%20of%20textual%2C%20acoustic%20and%20visual%20signals.%20While%0Arecent%20models%20have%20improved%20performance%20via%20advanced%20fusion%20strategies%2C%20they%0Aoften%20lack%20psychologically%20meaningful%20priors%20to%20guide%20multimodal%20alignment.%20In%0Athis%20paper%2C%20we%20revisit%20the%20use%20of%20CLIP%20and%20propose%20a%20novel%20Visual%20Emotion%0AGuided%20Anchoring%20%28VEGA%29%20mechanism%20that%20introduces%20class-level%20visual%20semantics%0Ainto%20the%20fusion%20and%20classification%20process.%20Distinct%20from%20prior%20work%20that%0Aprimarily%20utilizes%20CLIP%27s%20textual%20encoder%2C%20our%20approach%20leverages%20its%20image%0Aencoder%20to%20construct%20emotion-specific%20visual%20anchors%20based%20on%20facial%20exemplars.%0AThese%20anchors%20guide%20unimodal%20and%20multimodal%20features%20toward%20a%20perceptually%0Agrounded%20and%20psychologically%20aligned%20representation%20space%2C%20drawing%20inspiration%0Afrom%20cognitive%20theories%20%28prototypical%20emotion%20categories%20and%20multisensory%0Aintegration%29.%20A%20stochastic%20anchor%20sampling%20strategy%20further%20enhances%20robustness%0Aby%20balancing%20semantic%20stability%20and%20intra-class%20diversity.%20Integrated%20into%20a%0Adual-branch%20architecture%20with%20self-distillation%2C%20our%20VEGA-augmented%20model%0Aachieves%20sota%20performance%20on%20IEMOCAP%20and%20MELD.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/dkollias/VEGA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06564v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520Emotion%2520Recognition%2520with%2520Visual%2520Prototypes%253A%2520VEGA%2520--%2520Revisiting%250A%2520%2520CLIP%2520in%2520MERC%26entry.906535625%3DGuanyu%2520Hu%2520and%2520Dimitrios%2520Kollias%2520and%2520Xinyu%2520Yang%26entry.1292438233%3D%2520%2520Multimodal%2520Emotion%2520Recognition%2520in%2520Conversations%2520remains%2520a%2520challenging%2520task%250Adue%2520to%2520the%2520complex%2520interplay%2520of%2520textual%252C%2520acoustic%2520and%2520visual%2520signals.%2520While%250Arecent%2520models%2520have%2520improved%2520performance%2520via%2520advanced%2520fusion%2520strategies%252C%2520they%250Aoften%2520lack%2520psychologically%2520meaningful%2520priors%2520to%2520guide%2520multimodal%2520alignment.%2520In%250Athis%2520paper%252C%2520we%2520revisit%2520the%2520use%2520of%2520CLIP%2520and%2520propose%2520a%2520novel%2520Visual%2520Emotion%250AGuided%2520Anchoring%2520%2528VEGA%2529%2520mechanism%2520that%2520introduces%2520class-level%2520visual%2520semantics%250Ainto%2520the%2520fusion%2520and%2520classification%2520process.%2520Distinct%2520from%2520prior%2520work%2520that%250Aprimarily%2520utilizes%2520CLIP%2527s%2520textual%2520encoder%252C%2520our%2520approach%2520leverages%2520its%2520image%250Aencoder%2520to%2520construct%2520emotion-specific%2520visual%2520anchors%2520based%2520on%2520facial%2520exemplars.%250AThese%2520anchors%2520guide%2520unimodal%2520and%2520multimodal%2520features%2520toward%2520a%2520perceptually%250Agrounded%2520and%2520psychologically%2520aligned%2520representation%2520space%252C%2520drawing%2520inspiration%250Afrom%2520cognitive%2520theories%2520%2528prototypical%2520emotion%2520categories%2520and%2520multisensory%250Aintegration%2529.%2520A%2520stochastic%2520anchor%2520sampling%2520strategy%2520further%2520enhances%2520robustness%250Aby%2520balancing%2520semantic%2520stability%2520and%2520intra-class%2520diversity.%2520Integrated%2520into%2520a%250Adual-branch%2520architecture%2520with%2520self-distillation%252C%2520our%2520VEGA-augmented%2520model%250Aachieves%2520sota%2520performance%2520on%2520IEMOCAP%2520and%2520MELD.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/dkollias/VEGA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06564v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20Emotion%20Recognition%20with%20Visual%20Prototypes%3A%20VEGA%20--%20Revisiting%0A%20%20CLIP%20in%20MERC&entry.906535625=Guanyu%20Hu%20and%20Dimitrios%20Kollias%20and%20Xinyu%20Yang&entry.1292438233=%20%20Multimodal%20Emotion%20Recognition%20in%20Conversations%20remains%20a%20challenging%20task%0Adue%20to%20the%20complex%20interplay%20of%20textual%2C%20acoustic%20and%20visual%20signals.%20While%0Arecent%20models%20have%20improved%20performance%20via%20advanced%20fusion%20strategies%2C%20they%0Aoften%20lack%20psychologically%20meaningful%20priors%20to%20guide%20multimodal%20alignment.%20In%0Athis%20paper%2C%20we%20revisit%20the%20use%20of%20CLIP%20and%20propose%20a%20novel%20Visual%20Emotion%0AGuided%20Anchoring%20%28VEGA%29%20mechanism%20that%20introduces%20class-level%20visual%20semantics%0Ainto%20the%20fusion%20and%20classification%20process.%20Distinct%20from%20prior%20work%20that%0Aprimarily%20utilizes%20CLIP%27s%20textual%20encoder%2C%20our%20approach%20leverages%20its%20image%0Aencoder%20to%20construct%20emotion-specific%20visual%20anchors%20based%20on%20facial%20exemplars.%0AThese%20anchors%20guide%20unimodal%20and%20multimodal%20features%20toward%20a%20perceptually%0Agrounded%20and%20psychologically%20aligned%20representation%20space%2C%20drawing%20inspiration%0Afrom%20cognitive%20theories%20%28prototypical%20emotion%20categories%20and%20multisensory%0Aintegration%29.%20A%20stochastic%20anchor%20sampling%20strategy%20further%20enhances%20robustness%0Aby%20balancing%20semantic%20stability%20and%20intra-class%20diversity.%20Integrated%20into%20a%0Adual-branch%20architecture%20with%20self-distillation%2C%20our%20VEGA-augmented%20model%0Aachieves%20sota%20performance%20on%20IEMOCAP%20and%20MELD.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/dkollias/VEGA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06564v2&entry.124074799=Read"},
{"title": "Do Vision Transformers See Like Humans? Evaluating their Perceptual\n  Alignment", "author": "Pablo Hern\u00e1ndez-C\u00e1mara and Jose Manuel Ja\u00e9n-Lorites and Jorge Vila-Tom\u00e1s and Valero Laparra and Jesus Malo", "abstract": "  Vision Transformers (ViTs) achieve remarkable performance in image\nrecognition tasks, yet their alignment with human perception remains largely\nunexplored. This study systematically analyzes how model size, dataset size,\ndata augmentation and regularization impact ViT perceptual alignment with human\njudgments on the TID2013 dataset. Our findings confirm that larger models\nexhibit lower perceptual alignment, consistent with previous works. Increasing\ndataset diversity has a minimal impact, but exposing models to the same images\nmore times reduces alignment. Stronger data augmentation and regularization\nfurther decrease alignment, especially in models exposed to repeated training\ncycles. These results highlight a trade-off between model complexity, training\nstrategies, and alignment with human perception, raising important\nconsiderations for applications requiring human-like visual understanding.\n", "link": "http://arxiv.org/abs/2508.09850v1", "date": "2025-08-13", "relevancy": 2.7546, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Vision%20Transformers%20See%20Like%20Humans%3F%20Evaluating%20their%20Perceptual%0A%20%20Alignment&body=Title%3A%20Do%20Vision%20Transformers%20See%20Like%20Humans%3F%20Evaluating%20their%20Perceptual%0A%20%20Alignment%0AAuthor%3A%20Pablo%20Hern%C3%A1ndez-C%C3%A1mara%20and%20Jose%20Manuel%20Ja%C3%A9n-Lorites%20and%20Jorge%20Vila-Tom%C3%A1s%20and%20Valero%20Laparra%20and%20Jesus%20Malo%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20achieve%20remarkable%20performance%20in%20image%0Arecognition%20tasks%2C%20yet%20their%20alignment%20with%20human%20perception%20remains%20largely%0Aunexplored.%20This%20study%20systematically%20analyzes%20how%20model%20size%2C%20dataset%20size%2C%0Adata%20augmentation%20and%20regularization%20impact%20ViT%20perceptual%20alignment%20with%20human%0Ajudgments%20on%20the%20TID2013%20dataset.%20Our%20findings%20confirm%20that%20larger%20models%0Aexhibit%20lower%20perceptual%20alignment%2C%20consistent%20with%20previous%20works.%20Increasing%0Adataset%20diversity%20has%20a%20minimal%20impact%2C%20but%20exposing%20models%20to%20the%20same%20images%0Amore%20times%20reduces%20alignment.%20Stronger%20data%20augmentation%20and%20regularization%0Afurther%20decrease%20alignment%2C%20especially%20in%20models%20exposed%20to%20repeated%20training%0Acycles.%20These%20results%20highlight%20a%20trade-off%20between%20model%20complexity%2C%20training%0Astrategies%2C%20and%20alignment%20with%20human%20perception%2C%20raising%20important%0Aconsiderations%20for%20applications%20requiring%20human-like%20visual%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Vision%2520Transformers%2520See%2520Like%2520Humans%253F%2520Evaluating%2520their%2520Perceptual%250A%2520%2520Alignment%26entry.906535625%3DPablo%2520Hern%25C3%25A1ndez-C%25C3%25A1mara%2520and%2520Jose%2520Manuel%2520Ja%25C3%25A9n-Lorites%2520and%2520Jorge%2520Vila-Tom%25C3%25A1s%2520and%2520Valero%2520Laparra%2520and%2520Jesus%2520Malo%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520achieve%2520remarkable%2520performance%2520in%2520image%250Arecognition%2520tasks%252C%2520yet%2520their%2520alignment%2520with%2520human%2520perception%2520remains%2520largely%250Aunexplored.%2520This%2520study%2520systematically%2520analyzes%2520how%2520model%2520size%252C%2520dataset%2520size%252C%250Adata%2520augmentation%2520and%2520regularization%2520impact%2520ViT%2520perceptual%2520alignment%2520with%2520human%250Ajudgments%2520on%2520the%2520TID2013%2520dataset.%2520Our%2520findings%2520confirm%2520that%2520larger%2520models%250Aexhibit%2520lower%2520perceptual%2520alignment%252C%2520consistent%2520with%2520previous%2520works.%2520Increasing%250Adataset%2520diversity%2520has%2520a%2520minimal%2520impact%252C%2520but%2520exposing%2520models%2520to%2520the%2520same%2520images%250Amore%2520times%2520reduces%2520alignment.%2520Stronger%2520data%2520augmentation%2520and%2520regularization%250Afurther%2520decrease%2520alignment%252C%2520especially%2520in%2520models%2520exposed%2520to%2520repeated%2520training%250Acycles.%2520These%2520results%2520highlight%2520a%2520trade-off%2520between%2520model%2520complexity%252C%2520training%250Astrategies%252C%2520and%2520alignment%2520with%2520human%2520perception%252C%2520raising%2520important%250Aconsiderations%2520for%2520applications%2520requiring%2520human-like%2520visual%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Vision%20Transformers%20See%20Like%20Humans%3F%20Evaluating%20their%20Perceptual%0A%20%20Alignment&entry.906535625=Pablo%20Hern%C3%A1ndez-C%C3%A1mara%20and%20Jose%20Manuel%20Ja%C3%A9n-Lorites%20and%20Jorge%20Vila-Tom%C3%A1s%20and%20Valero%20Laparra%20and%20Jesus%20Malo&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20achieve%20remarkable%20performance%20in%20image%0Arecognition%20tasks%2C%20yet%20their%20alignment%20with%20human%20perception%20remains%20largely%0Aunexplored.%20This%20study%20systematically%20analyzes%20how%20model%20size%2C%20dataset%20size%2C%0Adata%20augmentation%20and%20regularization%20impact%20ViT%20perceptual%20alignment%20with%20human%0Ajudgments%20on%20the%20TID2013%20dataset.%20Our%20findings%20confirm%20that%20larger%20models%0Aexhibit%20lower%20perceptual%20alignment%2C%20consistent%20with%20previous%20works.%20Increasing%0Adataset%20diversity%20has%20a%20minimal%20impact%2C%20but%20exposing%20models%20to%20the%20same%20images%0Amore%20times%20reduces%20alignment.%20Stronger%20data%20augmentation%20and%20regularization%0Afurther%20decrease%20alignment%2C%20especially%20in%20models%20exposed%20to%20repeated%20training%0Acycles.%20These%20results%20highlight%20a%20trade-off%20between%20model%20complexity%2C%20training%0Astrategies%2C%20and%20alignment%20with%20human%20perception%2C%20raising%20important%0Aconsiderations%20for%20applications%20requiring%20human-like%20visual%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09850v1&entry.124074799=Read"},
{"title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language\n  Models", "author": "Jiaqi Cao and Jiarui Wang and Rubin Wei and Qipeng Guo and Kai Chen and Bowen Zhou and Zhouhan Lin", "abstract": "  Large Language Models (LLMs) have shown strong abilities in general language\ntasks, yet adapting them to specific domains remains a challenge. Current\nmethod like Domain Adaptive Pretraining (DAPT) requires costly full-parameter\ntraining and suffers from catastrophic forgetting. Meanwhile,\nRetrieval-Augmented Generation (RAG) introduces substantial inference latency\ndue to expensive nearest-neighbor searches and longer context. This paper\nintroduces Memory Decoder, a plug-and-play pretrained memory that enables\nefficient domain adaptation without changing the original model's parameters.\nMemory Decoder employs a small transformer decoder that learns to imitate the\nbehavior of an external non-parametric retriever. Once trained, Memory Decoder\ncan be seamlessly integrated with any pretrained language model that shares the\nsame tokenizer, requiring no model-specific modifications. Experimental results\ndemonstrate that Memory Decoder enables effective adaptation of various Qwen\nand Llama models to three distinct specialized domains: biomedicine, finance,\nand law, reducing perplexity by an average of 6.17 points. Overall, Memory\nDecoder introduces a novel paradigm centered on a specially pretrained memory\ncomponent designed for domain-specific adaptation. This memory architecture can\nbe integrated in a plug-and-play manner, consistently enhancing performance\nacross multiple models within the target domain.\n", "link": "http://arxiv.org/abs/2508.09874v1", "date": "2025-08-13", "relevancy": 2.7082, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5641}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20Decoder%3A%20A%20Pretrained%2C%20Plug-and-Play%20Memory%20for%20Large%20Language%0A%20%20Models&body=Title%3A%20Memory%20Decoder%3A%20A%20Pretrained%2C%20Plug-and-Play%20Memory%20for%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Jiaqi%20Cao%20and%20Jiarui%20Wang%20and%20Rubin%20Wei%20and%20Qipeng%20Guo%20and%20Kai%20Chen%20and%20Bowen%20Zhou%20and%20Zhouhan%20Lin%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20strong%20abilities%20in%20general%20language%0Atasks%2C%20yet%20adapting%20them%20to%20specific%20domains%20remains%20a%20challenge.%20Current%0Amethod%20like%20Domain%20Adaptive%20Pretraining%20%28DAPT%29%20requires%20costly%20full-parameter%0Atraining%20and%20suffers%20from%20catastrophic%20forgetting.%20Meanwhile%2C%0ARetrieval-Augmented%20Generation%20%28RAG%29%20introduces%20substantial%20inference%20latency%0Adue%20to%20expensive%20nearest-neighbor%20searches%20and%20longer%20context.%20This%20paper%0Aintroduces%20Memory%20Decoder%2C%20a%20plug-and-play%20pretrained%20memory%20that%20enables%0Aefficient%20domain%20adaptation%20without%20changing%20the%20original%20model%27s%20parameters.%0AMemory%20Decoder%20employs%20a%20small%20transformer%20decoder%20that%20learns%20to%20imitate%20the%0Abehavior%20of%20an%20external%20non-parametric%20retriever.%20Once%20trained%2C%20Memory%20Decoder%0Acan%20be%20seamlessly%20integrated%20with%20any%20pretrained%20language%20model%20that%20shares%20the%0Asame%20tokenizer%2C%20requiring%20no%20model-specific%20modifications.%20Experimental%20results%0Ademonstrate%20that%20Memory%20Decoder%20enables%20effective%20adaptation%20of%20various%20Qwen%0Aand%20Llama%20models%20to%20three%20distinct%20specialized%20domains%3A%20biomedicine%2C%20finance%2C%0Aand%20law%2C%20reducing%20perplexity%20by%20an%20average%20of%206.17%20points.%20Overall%2C%20Memory%0ADecoder%20introduces%20a%20novel%20paradigm%20centered%20on%20a%20specially%20pretrained%20memory%0Acomponent%20designed%20for%20domain-specific%20adaptation.%20This%20memory%20architecture%20can%0Abe%20integrated%20in%20a%20plug-and-play%20manner%2C%20consistently%20enhancing%20performance%0Aacross%20multiple%20models%20within%20the%20target%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520Decoder%253A%2520A%2520Pretrained%252C%2520Plug-and-Play%2520Memory%2520for%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DJiaqi%2520Cao%2520and%2520Jiarui%2520Wang%2520and%2520Rubin%2520Wei%2520and%2520Qipeng%2520Guo%2520and%2520Kai%2520Chen%2520and%2520Bowen%2520Zhou%2520and%2520Zhouhan%2520Lin%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520strong%2520abilities%2520in%2520general%2520language%250Atasks%252C%2520yet%2520adapting%2520them%2520to%2520specific%2520domains%2520remains%2520a%2520challenge.%2520Current%250Amethod%2520like%2520Domain%2520Adaptive%2520Pretraining%2520%2528DAPT%2529%2520requires%2520costly%2520full-parameter%250Atraining%2520and%2520suffers%2520from%2520catastrophic%2520forgetting.%2520Meanwhile%252C%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520introduces%2520substantial%2520inference%2520latency%250Adue%2520to%2520expensive%2520nearest-neighbor%2520searches%2520and%2520longer%2520context.%2520This%2520paper%250Aintroduces%2520Memory%2520Decoder%252C%2520a%2520plug-and-play%2520pretrained%2520memory%2520that%2520enables%250Aefficient%2520domain%2520adaptation%2520without%2520changing%2520the%2520original%2520model%2527s%2520parameters.%250AMemory%2520Decoder%2520employs%2520a%2520small%2520transformer%2520decoder%2520that%2520learns%2520to%2520imitate%2520the%250Abehavior%2520of%2520an%2520external%2520non-parametric%2520retriever.%2520Once%2520trained%252C%2520Memory%2520Decoder%250Acan%2520be%2520seamlessly%2520integrated%2520with%2520any%2520pretrained%2520language%2520model%2520that%2520shares%2520the%250Asame%2520tokenizer%252C%2520requiring%2520no%2520model-specific%2520modifications.%2520Experimental%2520results%250Ademonstrate%2520that%2520Memory%2520Decoder%2520enables%2520effective%2520adaptation%2520of%2520various%2520Qwen%250Aand%2520Llama%2520models%2520to%2520three%2520distinct%2520specialized%2520domains%253A%2520biomedicine%252C%2520finance%252C%250Aand%2520law%252C%2520reducing%2520perplexity%2520by%2520an%2520average%2520of%25206.17%2520points.%2520Overall%252C%2520Memory%250ADecoder%2520introduces%2520a%2520novel%2520paradigm%2520centered%2520on%2520a%2520specially%2520pretrained%2520memory%250Acomponent%2520designed%2520for%2520domain-specific%2520adaptation.%2520This%2520memory%2520architecture%2520can%250Abe%2520integrated%2520in%2520a%2520plug-and-play%2520manner%252C%2520consistently%2520enhancing%2520performance%250Aacross%2520multiple%2520models%2520within%2520the%2520target%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20Decoder%3A%20A%20Pretrained%2C%20Plug-and-Play%20Memory%20for%20Large%20Language%0A%20%20Models&entry.906535625=Jiaqi%20Cao%20and%20Jiarui%20Wang%20and%20Rubin%20Wei%20and%20Qipeng%20Guo%20and%20Kai%20Chen%20and%20Bowen%20Zhou%20and%20Zhouhan%20Lin&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20strong%20abilities%20in%20general%20language%0Atasks%2C%20yet%20adapting%20them%20to%20specific%20domains%20remains%20a%20challenge.%20Current%0Amethod%20like%20Domain%20Adaptive%20Pretraining%20%28DAPT%29%20requires%20costly%20full-parameter%0Atraining%20and%20suffers%20from%20catastrophic%20forgetting.%20Meanwhile%2C%0ARetrieval-Augmented%20Generation%20%28RAG%29%20introduces%20substantial%20inference%20latency%0Adue%20to%20expensive%20nearest-neighbor%20searches%20and%20longer%20context.%20This%20paper%0Aintroduces%20Memory%20Decoder%2C%20a%20plug-and-play%20pretrained%20memory%20that%20enables%0Aefficient%20domain%20adaptation%20without%20changing%20the%20original%20model%27s%20parameters.%0AMemory%20Decoder%20employs%20a%20small%20transformer%20decoder%20that%20learns%20to%20imitate%20the%0Abehavior%20of%20an%20external%20non-parametric%20retriever.%20Once%20trained%2C%20Memory%20Decoder%0Acan%20be%20seamlessly%20integrated%20with%20any%20pretrained%20language%20model%20that%20shares%20the%0Asame%20tokenizer%2C%20requiring%20no%20model-specific%20modifications.%20Experimental%20results%0Ademonstrate%20that%20Memory%20Decoder%20enables%20effective%20adaptation%20of%20various%20Qwen%0Aand%20Llama%20models%20to%20three%20distinct%20specialized%20domains%3A%20biomedicine%2C%20finance%2C%0Aand%20law%2C%20reducing%20perplexity%20by%20an%20average%20of%206.17%20points.%20Overall%2C%20Memory%0ADecoder%20introduces%20a%20novel%20paradigm%20centered%20on%20a%20specially%20pretrained%20memory%0Acomponent%20designed%20for%20domain-specific%20adaptation.%20This%20memory%20architecture%20can%0Abe%20integrated%20in%20a%20plug-and-play%20manner%2C%20consistently%20enhancing%20performance%0Aacross%20multiple%20models%20within%20the%20target%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09874v1&entry.124074799=Read"},
{"title": "CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography\n  Quality Assessment", "author": "Bo Wang and De-Xing Huang and Xiao-Hu Zhou and Mei-Jiang Gui and Nu-Fang Xiao and Jian-Long Hao and Ming-Yuan Liu and Zeng-Guang Hou", "abstract": "  Synthetic X-ray angiographies generated by modern generative models hold\ngreat potential to reduce the use of contrast agents in vascular interventional\nprocedures. However, low-quality synthetic angiographies can significantly\nincrease procedural risk, underscoring the need for reliable image quality\nassessment (IQA) methods. Existing IQA models, however, fail to leverage\nauxiliary images as references during evaluation and lack fine-grained,\ntask-specific metrics necessary for clinical relevance. To address these\nlimitations, this paper proposes CAS-IQA, a vision-language model (VLM)-based\nframework that predicts fine-grained quality scores by effectively\nincorporating auxiliary information from related images. In the absence of\nangiography datasets, CAS-3K is constructed, comprising 3,565 synthetic\nangiographies along with score annotations. To ensure clinically meaningful\nassessment, three task-specific evaluation metrics are defined. Furthermore, a\nMulti-path featUre fuSion and rouTing (MUST) module is designed to enhance\nimage representations by adaptively fusing and routing visual tokens to\nmetric-specific branches. Extensive experiments on the CAS-3K dataset\ndemonstrate that CAS-IQA significantly outperforms state-of-the-art IQA methods\nby a considerable margin.\n", "link": "http://arxiv.org/abs/2505.17619v2", "date": "2025-08-13", "relevancy": 2.6715, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAS-IQA%3A%20Teaching%20Vision-Language%20Models%20for%20Synthetic%20Angiography%0A%20%20Quality%20Assessment&body=Title%3A%20CAS-IQA%3A%20Teaching%20Vision-Language%20Models%20for%20Synthetic%20Angiography%0A%20%20Quality%20Assessment%0AAuthor%3A%20Bo%20Wang%20and%20De-Xing%20Huang%20and%20Xiao-Hu%20Zhou%20and%20Mei-Jiang%20Gui%20and%20Nu-Fang%20Xiao%20and%20Jian-Long%20Hao%20and%20Ming-Yuan%20Liu%20and%20Zeng-Guang%20Hou%0AAbstract%3A%20%20%20Synthetic%20X-ray%20angiographies%20generated%20by%20modern%20generative%20models%20hold%0Agreat%20potential%20to%20reduce%20the%20use%20of%20contrast%20agents%20in%20vascular%20interventional%0Aprocedures.%20However%2C%20low-quality%20synthetic%20angiographies%20can%20significantly%0Aincrease%20procedural%20risk%2C%20underscoring%20the%20need%20for%20reliable%20image%20quality%0Aassessment%20%28IQA%29%20methods.%20Existing%20IQA%20models%2C%20however%2C%20fail%20to%20leverage%0Aauxiliary%20images%20as%20references%20during%20evaluation%20and%20lack%20fine-grained%2C%0Atask-specific%20metrics%20necessary%20for%20clinical%20relevance.%20To%20address%20these%0Alimitations%2C%20this%20paper%20proposes%20CAS-IQA%2C%20a%20vision-language%20model%20%28VLM%29-based%0Aframework%20that%20predicts%20fine-grained%20quality%20scores%20by%20effectively%0Aincorporating%20auxiliary%20information%20from%20related%20images.%20In%20the%20absence%20of%0Aangiography%20datasets%2C%20CAS-3K%20is%20constructed%2C%20comprising%203%2C565%20synthetic%0Aangiographies%20along%20with%20score%20annotations.%20To%20ensure%20clinically%20meaningful%0Aassessment%2C%20three%20task-specific%20evaluation%20metrics%20are%20defined.%20Furthermore%2C%20a%0AMulti-path%20featUre%20fuSion%20and%20rouTing%20%28MUST%29%20module%20is%20designed%20to%20enhance%0Aimage%20representations%20by%20adaptively%20fusing%20and%20routing%20visual%20tokens%20to%0Ametric-specific%20branches.%20Extensive%20experiments%20on%20the%20CAS-3K%20dataset%0Ademonstrate%20that%20CAS-IQA%20significantly%20outperforms%20state-of-the-art%20IQA%20methods%0Aby%20a%20considerable%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17619v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAS-IQA%253A%2520Teaching%2520Vision-Language%2520Models%2520for%2520Synthetic%2520Angiography%250A%2520%2520Quality%2520Assessment%26entry.906535625%3DBo%2520Wang%2520and%2520De-Xing%2520Huang%2520and%2520Xiao-Hu%2520Zhou%2520and%2520Mei-Jiang%2520Gui%2520and%2520Nu-Fang%2520Xiao%2520and%2520Jian-Long%2520Hao%2520and%2520Ming-Yuan%2520Liu%2520and%2520Zeng-Guang%2520Hou%26entry.1292438233%3D%2520%2520Synthetic%2520X-ray%2520angiographies%2520generated%2520by%2520modern%2520generative%2520models%2520hold%250Agreat%2520potential%2520to%2520reduce%2520the%2520use%2520of%2520contrast%2520agents%2520in%2520vascular%2520interventional%250Aprocedures.%2520However%252C%2520low-quality%2520synthetic%2520angiographies%2520can%2520significantly%250Aincrease%2520procedural%2520risk%252C%2520underscoring%2520the%2520need%2520for%2520reliable%2520image%2520quality%250Aassessment%2520%2528IQA%2529%2520methods.%2520Existing%2520IQA%2520models%252C%2520however%252C%2520fail%2520to%2520leverage%250Aauxiliary%2520images%2520as%2520references%2520during%2520evaluation%2520and%2520lack%2520fine-grained%252C%250Atask-specific%2520metrics%2520necessary%2520for%2520clinical%2520relevance.%2520To%2520address%2520these%250Alimitations%252C%2520this%2520paper%2520proposes%2520CAS-IQA%252C%2520a%2520vision-language%2520model%2520%2528VLM%2529-based%250Aframework%2520that%2520predicts%2520fine-grained%2520quality%2520scores%2520by%2520effectively%250Aincorporating%2520auxiliary%2520information%2520from%2520related%2520images.%2520In%2520the%2520absence%2520of%250Aangiography%2520datasets%252C%2520CAS-3K%2520is%2520constructed%252C%2520comprising%25203%252C565%2520synthetic%250Aangiographies%2520along%2520with%2520score%2520annotations.%2520To%2520ensure%2520clinically%2520meaningful%250Aassessment%252C%2520three%2520task-specific%2520evaluation%2520metrics%2520are%2520defined.%2520Furthermore%252C%2520a%250AMulti-path%2520featUre%2520fuSion%2520and%2520rouTing%2520%2528MUST%2529%2520module%2520is%2520designed%2520to%2520enhance%250Aimage%2520representations%2520by%2520adaptively%2520fusing%2520and%2520routing%2520visual%2520tokens%2520to%250Ametric-specific%2520branches.%2520Extensive%2520experiments%2520on%2520the%2520CAS-3K%2520dataset%250Ademonstrate%2520that%2520CAS-IQA%2520significantly%2520outperforms%2520state-of-the-art%2520IQA%2520methods%250Aby%2520a%2520considerable%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17619v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAS-IQA%3A%20Teaching%20Vision-Language%20Models%20for%20Synthetic%20Angiography%0A%20%20Quality%20Assessment&entry.906535625=Bo%20Wang%20and%20De-Xing%20Huang%20and%20Xiao-Hu%20Zhou%20and%20Mei-Jiang%20Gui%20and%20Nu-Fang%20Xiao%20and%20Jian-Long%20Hao%20and%20Ming-Yuan%20Liu%20and%20Zeng-Guang%20Hou&entry.1292438233=%20%20Synthetic%20X-ray%20angiographies%20generated%20by%20modern%20generative%20models%20hold%0Agreat%20potential%20to%20reduce%20the%20use%20of%20contrast%20agents%20in%20vascular%20interventional%0Aprocedures.%20However%2C%20low-quality%20synthetic%20angiographies%20can%20significantly%0Aincrease%20procedural%20risk%2C%20underscoring%20the%20need%20for%20reliable%20image%20quality%0Aassessment%20%28IQA%29%20methods.%20Existing%20IQA%20models%2C%20however%2C%20fail%20to%20leverage%0Aauxiliary%20images%20as%20references%20during%20evaluation%20and%20lack%20fine-grained%2C%0Atask-specific%20metrics%20necessary%20for%20clinical%20relevance.%20To%20address%20these%0Alimitations%2C%20this%20paper%20proposes%20CAS-IQA%2C%20a%20vision-language%20model%20%28VLM%29-based%0Aframework%20that%20predicts%20fine-grained%20quality%20scores%20by%20effectively%0Aincorporating%20auxiliary%20information%20from%20related%20images.%20In%20the%20absence%20of%0Aangiography%20datasets%2C%20CAS-3K%20is%20constructed%2C%20comprising%203%2C565%20synthetic%0Aangiographies%20along%20with%20score%20annotations.%20To%20ensure%20clinically%20meaningful%0Aassessment%2C%20three%20task-specific%20evaluation%20metrics%20are%20defined.%20Furthermore%2C%20a%0AMulti-path%20featUre%20fuSion%20and%20rouTing%20%28MUST%29%20module%20is%20designed%20to%20enhance%0Aimage%20representations%20by%20adaptively%20fusing%20and%20routing%20visual%20tokens%20to%0Ametric-specific%20branches.%20Extensive%20experiments%20on%20the%20CAS-3K%20dataset%0Ademonstrate%20that%20CAS-IQA%20significantly%20outperforms%20state-of-the-art%20IQA%20methods%0Aby%20a%20considerable%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17619v2&entry.124074799=Read"},
{"title": "Dynamic Mixture-of-Experts for Incremental Graph Learning", "author": "Lecheng Kong and Theodore Vasiloudis and Seongjun Yun and Han Xie and Xiang Song", "abstract": "  Graph incremental learning is a learning paradigm that aims to adapt trained\nmodels to continuously incremented graphs and data over time without the need\nfor retraining on the full dataset. However, regular graph machine learning\nmethods suffer from catastrophic forgetting when applied to incremental\nlearning settings, where previously learned knowledge is overridden by new\nknowledge. Previous approaches have tried to address this by treating the\npreviously trained model as an inseparable unit and using techniques to\nmaintain old behaviors while learning new knowledge. These approaches, however,\ndo not account for the fact that previously acquired knowledge at different\ntimestamps contributes differently to learning new tasks. Some prior patterns\ncan be transferred to help learn new data, while others may deviate from the\nnew data distribution and be detrimental. To address this, we propose a dynamic\nmixture-of-experts (DyMoE) approach for incremental learning. Specifically, a\nDyMoE GNN layer adds new expert networks specialized in modeling the incoming\ndata blocks. We design a customized regularization loss that utilizes data\nsequence information so existing experts can maintain their ability to solve\nold tasks while helping the new expert learn the new data effectively. As the\nnumber of data blocks grows over time, the computational cost of the full\nmixture-of-experts (MoE) model increases. To address this, we introduce a\nsparse MoE approach, where only the top-$k$ most relevant experts make\npredictions, significantly reducing the computation time. Our model achieved\n4.92\\% relative accuracy increase compared to the best baselines on class\nincremental learning, showing the model's exceptional power.\n", "link": "http://arxiv.org/abs/2508.09974v1", "date": "2025-08-13", "relevancy": 2.6473, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6117}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.493}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Mixture-of-Experts%20for%20Incremental%20Graph%20Learning&body=Title%3A%20Dynamic%20Mixture-of-Experts%20for%20Incremental%20Graph%20Learning%0AAuthor%3A%20Lecheng%20Kong%20and%20Theodore%20Vasiloudis%20and%20Seongjun%20Yun%20and%20Han%20Xie%20and%20Xiang%20Song%0AAbstract%3A%20%20%20Graph%20incremental%20learning%20is%20a%20learning%20paradigm%20that%20aims%20to%20adapt%20trained%0Amodels%20to%20continuously%20incremented%20graphs%20and%20data%20over%20time%20without%20the%20need%0Afor%20retraining%20on%20the%20full%20dataset.%20However%2C%20regular%20graph%20machine%20learning%0Amethods%20suffer%20from%20catastrophic%20forgetting%20when%20applied%20to%20incremental%0Alearning%20settings%2C%20where%20previously%20learned%20knowledge%20is%20overridden%20by%20new%0Aknowledge.%20Previous%20approaches%20have%20tried%20to%20address%20this%20by%20treating%20the%0Apreviously%20trained%20model%20as%20an%20inseparable%20unit%20and%20using%20techniques%20to%0Amaintain%20old%20behaviors%20while%20learning%20new%20knowledge.%20These%20approaches%2C%20however%2C%0Ado%20not%20account%20for%20the%20fact%20that%20previously%20acquired%20knowledge%20at%20different%0Atimestamps%20contributes%20differently%20to%20learning%20new%20tasks.%20Some%20prior%20patterns%0Acan%20be%20transferred%20to%20help%20learn%20new%20data%2C%20while%20others%20may%20deviate%20from%20the%0Anew%20data%20distribution%20and%20be%20detrimental.%20To%20address%20this%2C%20we%20propose%20a%20dynamic%0Amixture-of-experts%20%28DyMoE%29%20approach%20for%20incremental%20learning.%20Specifically%2C%20a%0ADyMoE%20GNN%20layer%20adds%20new%20expert%20networks%20specialized%20in%20modeling%20the%20incoming%0Adata%20blocks.%20We%20design%20a%20customized%20regularization%20loss%20that%20utilizes%20data%0Asequence%20information%20so%20existing%20experts%20can%20maintain%20their%20ability%20to%20solve%0Aold%20tasks%20while%20helping%20the%20new%20expert%20learn%20the%20new%20data%20effectively.%20As%20the%0Anumber%20of%20data%20blocks%20grows%20over%20time%2C%20the%20computational%20cost%20of%20the%20full%0Amixture-of-experts%20%28MoE%29%20model%20increases.%20To%20address%20this%2C%20we%20introduce%20a%0Asparse%20MoE%20approach%2C%20where%20only%20the%20top-%24k%24%20most%20relevant%20experts%20make%0Apredictions%2C%20significantly%20reducing%20the%20computation%20time.%20Our%20model%20achieved%0A4.92%5C%25%20relative%20accuracy%20increase%20compared%20to%20the%20best%20baselines%20on%20class%0Aincremental%20learning%2C%20showing%20the%20model%27s%20exceptional%20power.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Mixture-of-Experts%2520for%2520Incremental%2520Graph%2520Learning%26entry.906535625%3DLecheng%2520Kong%2520and%2520Theodore%2520Vasiloudis%2520and%2520Seongjun%2520Yun%2520and%2520Han%2520Xie%2520and%2520Xiang%2520Song%26entry.1292438233%3D%2520%2520Graph%2520incremental%2520learning%2520is%2520a%2520learning%2520paradigm%2520that%2520aims%2520to%2520adapt%2520trained%250Amodels%2520to%2520continuously%2520incremented%2520graphs%2520and%2520data%2520over%2520time%2520without%2520the%2520need%250Afor%2520retraining%2520on%2520the%2520full%2520dataset.%2520However%252C%2520regular%2520graph%2520machine%2520learning%250Amethods%2520suffer%2520from%2520catastrophic%2520forgetting%2520when%2520applied%2520to%2520incremental%250Alearning%2520settings%252C%2520where%2520previously%2520learned%2520knowledge%2520is%2520overridden%2520by%2520new%250Aknowledge.%2520Previous%2520approaches%2520have%2520tried%2520to%2520address%2520this%2520by%2520treating%2520the%250Apreviously%2520trained%2520model%2520as%2520an%2520inseparable%2520unit%2520and%2520using%2520techniques%2520to%250Amaintain%2520old%2520behaviors%2520while%2520learning%2520new%2520knowledge.%2520These%2520approaches%252C%2520however%252C%250Ado%2520not%2520account%2520for%2520the%2520fact%2520that%2520previously%2520acquired%2520knowledge%2520at%2520different%250Atimestamps%2520contributes%2520differently%2520to%2520learning%2520new%2520tasks.%2520Some%2520prior%2520patterns%250Acan%2520be%2520transferred%2520to%2520help%2520learn%2520new%2520data%252C%2520while%2520others%2520may%2520deviate%2520from%2520the%250Anew%2520data%2520distribution%2520and%2520be%2520detrimental.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520dynamic%250Amixture-of-experts%2520%2528DyMoE%2529%2520approach%2520for%2520incremental%2520learning.%2520Specifically%252C%2520a%250ADyMoE%2520GNN%2520layer%2520adds%2520new%2520expert%2520networks%2520specialized%2520in%2520modeling%2520the%2520incoming%250Adata%2520blocks.%2520We%2520design%2520a%2520customized%2520regularization%2520loss%2520that%2520utilizes%2520data%250Asequence%2520information%2520so%2520existing%2520experts%2520can%2520maintain%2520their%2520ability%2520to%2520solve%250Aold%2520tasks%2520while%2520helping%2520the%2520new%2520expert%2520learn%2520the%2520new%2520data%2520effectively.%2520As%2520the%250Anumber%2520of%2520data%2520blocks%2520grows%2520over%2520time%252C%2520the%2520computational%2520cost%2520of%2520the%2520full%250Amixture-of-experts%2520%2528MoE%2529%2520model%2520increases.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%250Asparse%2520MoE%2520approach%252C%2520where%2520only%2520the%2520top-%2524k%2524%2520most%2520relevant%2520experts%2520make%250Apredictions%252C%2520significantly%2520reducing%2520the%2520computation%2520time.%2520Our%2520model%2520achieved%250A4.92%255C%2525%2520relative%2520accuracy%2520increase%2520compared%2520to%2520the%2520best%2520baselines%2520on%2520class%250Aincremental%2520learning%252C%2520showing%2520the%2520model%2527s%2520exceptional%2520power.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Mixture-of-Experts%20for%20Incremental%20Graph%20Learning&entry.906535625=Lecheng%20Kong%20and%20Theodore%20Vasiloudis%20and%20Seongjun%20Yun%20and%20Han%20Xie%20and%20Xiang%20Song&entry.1292438233=%20%20Graph%20incremental%20learning%20is%20a%20learning%20paradigm%20that%20aims%20to%20adapt%20trained%0Amodels%20to%20continuously%20incremented%20graphs%20and%20data%20over%20time%20without%20the%20need%0Afor%20retraining%20on%20the%20full%20dataset.%20However%2C%20regular%20graph%20machine%20learning%0Amethods%20suffer%20from%20catastrophic%20forgetting%20when%20applied%20to%20incremental%0Alearning%20settings%2C%20where%20previously%20learned%20knowledge%20is%20overridden%20by%20new%0Aknowledge.%20Previous%20approaches%20have%20tried%20to%20address%20this%20by%20treating%20the%0Apreviously%20trained%20model%20as%20an%20inseparable%20unit%20and%20using%20techniques%20to%0Amaintain%20old%20behaviors%20while%20learning%20new%20knowledge.%20These%20approaches%2C%20however%2C%0Ado%20not%20account%20for%20the%20fact%20that%20previously%20acquired%20knowledge%20at%20different%0Atimestamps%20contributes%20differently%20to%20learning%20new%20tasks.%20Some%20prior%20patterns%0Acan%20be%20transferred%20to%20help%20learn%20new%20data%2C%20while%20others%20may%20deviate%20from%20the%0Anew%20data%20distribution%20and%20be%20detrimental.%20To%20address%20this%2C%20we%20propose%20a%20dynamic%0Amixture-of-experts%20%28DyMoE%29%20approach%20for%20incremental%20learning.%20Specifically%2C%20a%0ADyMoE%20GNN%20layer%20adds%20new%20expert%20networks%20specialized%20in%20modeling%20the%20incoming%0Adata%20blocks.%20We%20design%20a%20customized%20regularization%20loss%20that%20utilizes%20data%0Asequence%20information%20so%20existing%20experts%20can%20maintain%20their%20ability%20to%20solve%0Aold%20tasks%20while%20helping%20the%20new%20expert%20learn%20the%20new%20data%20effectively.%20As%20the%0Anumber%20of%20data%20blocks%20grows%20over%20time%2C%20the%20computational%20cost%20of%20the%20full%0Amixture-of-experts%20%28MoE%29%20model%20increases.%20To%20address%20this%2C%20we%20introduce%20a%0Asparse%20MoE%20approach%2C%20where%20only%20the%20top-%24k%24%20most%20relevant%20experts%20make%0Apredictions%2C%20significantly%20reducing%20the%20computation%20time.%20Our%20model%20achieved%0A4.92%5C%25%20relative%20accuracy%20increase%20compared%20to%20the%20best%20baselines%20on%20class%0Aincremental%20learning%2C%20showing%20the%20model%27s%20exceptional%20power.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09974v1&entry.124074799=Read"},
{"title": "Automated Segmentation of Coronal Brain Tissue Slabs for 3D\n  Neuropathology", "author": "Jonathan Williams Ramirez and Dina Zemlyanker and Lucas Deden-Binder and Rogeny Herisse and Erendira Garcia Pallares and Karthik Gopinath and Harshvardhan Gazula and Christopher Mount and Liana N. Kozanno and Michael S. Marshall and Theresa R. Connors and Matthew P. Frosch and Mark Montine and Derek H. Oakley and Christine L. Mac Donald and C. Dirk Keene and Bradley T. Hyman and Juan Eugenio Iglesias", "abstract": "  Advances in image registration and machine learning have recently enabled\nvolumetric analysis of \\emph{postmortem} brain tissue from conventional\nphotographs of coronal slabs, which are routinely collected in brain banks and\nneuropathology laboratories worldwide. One caveat of this methodology is the\nrequirement of segmentation of the tissue from photographs, which currently\nrequires costly manual intervention. In this article, we present a deep\nlearning model to automate this process. The automatic segmentation tool relies\non a U-Net architecture that was trained with a combination of\n\\textit{(i)}1,414 manually segmented images of both fixed and fresh tissue,\nfrom specimens with varying diagnoses, photographed at two different sites; and\n\\textit{(ii)}~2,000 synthetic images with randomized contrast and corresponding\nmasks generated from MRI scans for improved generalizability to unseen\nphotographic setups. Automated model predictions on a subset of photographs not\nseen in training were analyzed to estimate performance compared to manual\nlabels -- including both inter- and intra-rater variability. Our model achieved\na median Dice score over 0.98, mean surface distance under 0.4~mm, and 95\\%\nHausdorff distance under 1.60~mm, which approaches inter-/intra-rater levels.\nOur tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.\n", "link": "http://arxiv.org/abs/2508.09805v1", "date": "2025-08-13", "relevancy": 2.6242, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5375}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.521}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Segmentation%20of%20Coronal%20Brain%20Tissue%20Slabs%20for%203D%0A%20%20Neuropathology&body=Title%3A%20Automated%20Segmentation%20of%20Coronal%20Brain%20Tissue%20Slabs%20for%203D%0A%20%20Neuropathology%0AAuthor%3A%20Jonathan%20Williams%20Ramirez%20and%20Dina%20Zemlyanker%20and%20Lucas%20Deden-Binder%20and%20Rogeny%20Herisse%20and%20Erendira%20Garcia%20Pallares%20and%20Karthik%20Gopinath%20and%20Harshvardhan%20Gazula%20and%20Christopher%20Mount%20and%20Liana%20N.%20Kozanno%20and%20Michael%20S.%20Marshall%20and%20Theresa%20R.%20Connors%20and%20Matthew%20P.%20Frosch%20and%20Mark%20Montine%20and%20Derek%20H.%20Oakley%20and%20Christine%20L.%20Mac%20Donald%20and%20C.%20Dirk%20Keene%20and%20Bradley%20T.%20Hyman%20and%20Juan%20Eugenio%20Iglesias%0AAbstract%3A%20%20%20Advances%20in%20image%20registration%20and%20machine%20learning%20have%20recently%20enabled%0Avolumetric%20analysis%20of%20%5Cemph%7Bpostmortem%7D%20brain%20tissue%20from%20conventional%0Aphotographs%20of%20coronal%20slabs%2C%20which%20are%20routinely%20collected%20in%20brain%20banks%20and%0Aneuropathology%20laboratories%20worldwide.%20One%20caveat%20of%20this%20methodology%20is%20the%0Arequirement%20of%20segmentation%20of%20the%20tissue%20from%20photographs%2C%20which%20currently%0Arequires%20costly%20manual%20intervention.%20In%20this%20article%2C%20we%20present%20a%20deep%0Alearning%20model%20to%20automate%20this%20process.%20The%20automatic%20segmentation%20tool%20relies%0Aon%20a%20U-Net%20architecture%20that%20was%20trained%20with%20a%20combination%20of%0A%5Ctextit%7B%28i%29%7D1%2C414%20manually%20segmented%20images%20of%20both%20fixed%20and%20fresh%20tissue%2C%0Afrom%20specimens%20with%20varying%20diagnoses%2C%20photographed%20at%20two%20different%20sites%3B%20and%0A%5Ctextit%7B%28ii%29%7D~2%2C000%20synthetic%20images%20with%20randomized%20contrast%20and%20corresponding%0Amasks%20generated%20from%20MRI%20scans%20for%20improved%20generalizability%20to%20unseen%0Aphotographic%20setups.%20Automated%20model%20predictions%20on%20a%20subset%20of%20photographs%20not%0Aseen%20in%20training%20were%20analyzed%20to%20estimate%20performance%20compared%20to%20manual%0Alabels%20--%20including%20both%20inter-%20and%20intra-rater%20variability.%20Our%20model%20achieved%0Aa%20median%20Dice%20score%20over%200.98%2C%20mean%20surface%20distance%20under%200.4~mm%2C%20and%2095%5C%25%0AHausdorff%20distance%20under%201.60~mm%2C%20which%20approaches%20inter-/intra-rater%20levels.%0AOur%20tool%20is%20publicly%20available%20at%20surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Segmentation%2520of%2520Coronal%2520Brain%2520Tissue%2520Slabs%2520for%25203D%250A%2520%2520Neuropathology%26entry.906535625%3DJonathan%2520Williams%2520Ramirez%2520and%2520Dina%2520Zemlyanker%2520and%2520Lucas%2520Deden-Binder%2520and%2520Rogeny%2520Herisse%2520and%2520Erendira%2520Garcia%2520Pallares%2520and%2520Karthik%2520Gopinath%2520and%2520Harshvardhan%2520Gazula%2520and%2520Christopher%2520Mount%2520and%2520Liana%2520N.%2520Kozanno%2520and%2520Michael%2520S.%2520Marshall%2520and%2520Theresa%2520R.%2520Connors%2520and%2520Matthew%2520P.%2520Frosch%2520and%2520Mark%2520Montine%2520and%2520Derek%2520H.%2520Oakley%2520and%2520Christine%2520L.%2520Mac%2520Donald%2520and%2520C.%2520Dirk%2520Keene%2520and%2520Bradley%2520T.%2520Hyman%2520and%2520Juan%2520Eugenio%2520Iglesias%26entry.1292438233%3D%2520%2520Advances%2520in%2520image%2520registration%2520and%2520machine%2520learning%2520have%2520recently%2520enabled%250Avolumetric%2520analysis%2520of%2520%255Cemph%257Bpostmortem%257D%2520brain%2520tissue%2520from%2520conventional%250Aphotographs%2520of%2520coronal%2520slabs%252C%2520which%2520are%2520routinely%2520collected%2520in%2520brain%2520banks%2520and%250Aneuropathology%2520laboratories%2520worldwide.%2520One%2520caveat%2520of%2520this%2520methodology%2520is%2520the%250Arequirement%2520of%2520segmentation%2520of%2520the%2520tissue%2520from%2520photographs%252C%2520which%2520currently%250Arequires%2520costly%2520manual%2520intervention.%2520In%2520this%2520article%252C%2520we%2520present%2520a%2520deep%250Alearning%2520model%2520to%2520automate%2520this%2520process.%2520The%2520automatic%2520segmentation%2520tool%2520relies%250Aon%2520a%2520U-Net%2520architecture%2520that%2520was%2520trained%2520with%2520a%2520combination%2520of%250A%255Ctextit%257B%2528i%2529%257D1%252C414%2520manually%2520segmented%2520images%2520of%2520both%2520fixed%2520and%2520fresh%2520tissue%252C%250Afrom%2520specimens%2520with%2520varying%2520diagnoses%252C%2520photographed%2520at%2520two%2520different%2520sites%253B%2520and%250A%255Ctextit%257B%2528ii%2529%257D~2%252C000%2520synthetic%2520images%2520with%2520randomized%2520contrast%2520and%2520corresponding%250Amasks%2520generated%2520from%2520MRI%2520scans%2520for%2520improved%2520generalizability%2520to%2520unseen%250Aphotographic%2520setups.%2520Automated%2520model%2520predictions%2520on%2520a%2520subset%2520of%2520photographs%2520not%250Aseen%2520in%2520training%2520were%2520analyzed%2520to%2520estimate%2520performance%2520compared%2520to%2520manual%250Alabels%2520--%2520including%2520both%2520inter-%2520and%2520intra-rater%2520variability.%2520Our%2520model%2520achieved%250Aa%2520median%2520Dice%2520score%2520over%25200.98%252C%2520mean%2520surface%2520distance%2520under%25200.4~mm%252C%2520and%252095%255C%2525%250AHausdorff%2520distance%2520under%25201.60~mm%252C%2520which%2520approaches%2520inter-/intra-rater%2520levels.%250AOur%2520tool%2520is%2520publicly%2520available%2520at%2520surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Segmentation%20of%20Coronal%20Brain%20Tissue%20Slabs%20for%203D%0A%20%20Neuropathology&entry.906535625=Jonathan%20Williams%20Ramirez%20and%20Dina%20Zemlyanker%20and%20Lucas%20Deden-Binder%20and%20Rogeny%20Herisse%20and%20Erendira%20Garcia%20Pallares%20and%20Karthik%20Gopinath%20and%20Harshvardhan%20Gazula%20and%20Christopher%20Mount%20and%20Liana%20N.%20Kozanno%20and%20Michael%20S.%20Marshall%20and%20Theresa%20R.%20Connors%20and%20Matthew%20P.%20Frosch%20and%20Mark%20Montine%20and%20Derek%20H.%20Oakley%20and%20Christine%20L.%20Mac%20Donald%20and%20C.%20Dirk%20Keene%20and%20Bradley%20T.%20Hyman%20and%20Juan%20Eugenio%20Iglesias&entry.1292438233=%20%20Advances%20in%20image%20registration%20and%20machine%20learning%20have%20recently%20enabled%0Avolumetric%20analysis%20of%20%5Cemph%7Bpostmortem%7D%20brain%20tissue%20from%20conventional%0Aphotographs%20of%20coronal%20slabs%2C%20which%20are%20routinely%20collected%20in%20brain%20banks%20and%0Aneuropathology%20laboratories%20worldwide.%20One%20caveat%20of%20this%20methodology%20is%20the%0Arequirement%20of%20segmentation%20of%20the%20tissue%20from%20photographs%2C%20which%20currently%0Arequires%20costly%20manual%20intervention.%20In%20this%20article%2C%20we%20present%20a%20deep%0Alearning%20model%20to%20automate%20this%20process.%20The%20automatic%20segmentation%20tool%20relies%0Aon%20a%20U-Net%20architecture%20that%20was%20trained%20with%20a%20combination%20of%0A%5Ctextit%7B%28i%29%7D1%2C414%20manually%20segmented%20images%20of%20both%20fixed%20and%20fresh%20tissue%2C%0Afrom%20specimens%20with%20varying%20diagnoses%2C%20photographed%20at%20two%20different%20sites%3B%20and%0A%5Ctextit%7B%28ii%29%7D~2%2C000%20synthetic%20images%20with%20randomized%20contrast%20and%20corresponding%0Amasks%20generated%20from%20MRI%20scans%20for%20improved%20generalizability%20to%20unseen%0Aphotographic%20setups.%20Automated%20model%20predictions%20on%20a%20subset%20of%20photographs%20not%0Aseen%20in%20training%20were%20analyzed%20to%20estimate%20performance%20compared%20to%20manual%0Alabels%20--%20including%20both%20inter-%20and%20intra-rater%20variability.%20Our%20model%20achieved%0Aa%20median%20Dice%20score%20over%200.98%2C%20mean%20surface%20distance%20under%200.4~mm%2C%20and%2095%5C%25%0AHausdorff%20distance%20under%201.60~mm%2C%20which%20approaches%20inter-/intra-rater%20levels.%0AOur%20tool%20is%20publicly%20available%20at%20surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09805v1&entry.124074799=Read"},
{"title": "Specialised or Generic? Tokenization Choices for Radiology Language\n  Models", "author": "Hermione Warr and Wentian Xu and Harry Anthony and Yasin Ibrahim and Daniel McGowan and Konstantinos Kamnitsas", "abstract": "  The vocabulary used by language models (LM) - defined by the tokenizer -\nplays a key role in text generation quality. However, its impact remains\nunder-explored in radiology. In this work, we address this gap by\nsystematically comparing general, medical, and domain-specific tokenizers on\nthe task of radiology report summarisation across three imaging modalities. We\nalso investigate scenarios with and without LM pre-training on PubMed\nabstracts. Our findings demonstrate that medical and domain-specific\nvocabularies outperformed widely used natural language alternatives when models\nare trained from scratch. Pre-training partially mitigates performance\ndifferences between tokenizers, whilst the domain-specific tokenizers achieve\nthe most favourable results. Domain-specific tokenizers also reduce memory\nrequirements due to smaller vocabularies and shorter sequences. These results\ndemonstrate that adapting the vocabulary of LMs to the clinical domain provides\npractical benefits, including improved performance and reduced computational\ndemands, making such models more accessible and effective for both research and\nreal-world healthcare settings.\n", "link": "http://arxiv.org/abs/2508.09952v1", "date": "2025-08-13", "relevancy": 2.6146, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Specialised%20or%20Generic%3F%20Tokenization%20Choices%20for%20Radiology%20Language%0A%20%20Models&body=Title%3A%20Specialised%20or%20Generic%3F%20Tokenization%20Choices%20for%20Radiology%20Language%0A%20%20Models%0AAuthor%3A%20Hermione%20Warr%20and%20Wentian%20Xu%20and%20Harry%20Anthony%20and%20Yasin%20Ibrahim%20and%20Daniel%20McGowan%20and%20Konstantinos%20Kamnitsas%0AAbstract%3A%20%20%20The%20vocabulary%20used%20by%20language%20models%20%28LM%29%20-%20defined%20by%20the%20tokenizer%20-%0Aplays%20a%20key%20role%20in%20text%20generation%20quality.%20However%2C%20its%20impact%20remains%0Aunder-explored%20in%20radiology.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%0Asystematically%20comparing%20general%2C%20medical%2C%20and%20domain-specific%20tokenizers%20on%0Athe%20task%20of%20radiology%20report%20summarisation%20across%20three%20imaging%20modalities.%20We%0Aalso%20investigate%20scenarios%20with%20and%20without%20LM%20pre-training%20on%20PubMed%0Aabstracts.%20Our%20findings%20demonstrate%20that%20medical%20and%20domain-specific%0Avocabularies%20outperformed%20widely%20used%20natural%20language%20alternatives%20when%20models%0Aare%20trained%20from%20scratch.%20Pre-training%20partially%20mitigates%20performance%0Adifferences%20between%20tokenizers%2C%20whilst%20the%20domain-specific%20tokenizers%20achieve%0Athe%20most%20favourable%20results.%20Domain-specific%20tokenizers%20also%20reduce%20memory%0Arequirements%20due%20to%20smaller%20vocabularies%20and%20shorter%20sequences.%20These%20results%0Ademonstrate%20that%20adapting%20the%20vocabulary%20of%20LMs%20to%20the%20clinical%20domain%20provides%0Apractical%20benefits%2C%20including%20improved%20performance%20and%20reduced%20computational%0Ademands%2C%20making%20such%20models%20more%20accessible%20and%20effective%20for%20both%20research%20and%0Areal-world%20healthcare%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecialised%2520or%2520Generic%253F%2520Tokenization%2520Choices%2520for%2520Radiology%2520Language%250A%2520%2520Models%26entry.906535625%3DHermione%2520Warr%2520and%2520Wentian%2520Xu%2520and%2520Harry%2520Anthony%2520and%2520Yasin%2520Ibrahim%2520and%2520Daniel%2520McGowan%2520and%2520Konstantinos%2520Kamnitsas%26entry.1292438233%3D%2520%2520The%2520vocabulary%2520used%2520by%2520language%2520models%2520%2528LM%2529%2520-%2520defined%2520by%2520the%2520tokenizer%2520-%250Aplays%2520a%2520key%2520role%2520in%2520text%2520generation%2520quality.%2520However%252C%2520its%2520impact%2520remains%250Aunder-explored%2520in%2520radiology.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%250Asystematically%2520comparing%2520general%252C%2520medical%252C%2520and%2520domain-specific%2520tokenizers%2520on%250Athe%2520task%2520of%2520radiology%2520report%2520summarisation%2520across%2520three%2520imaging%2520modalities.%2520We%250Aalso%2520investigate%2520scenarios%2520with%2520and%2520without%2520LM%2520pre-training%2520on%2520PubMed%250Aabstracts.%2520Our%2520findings%2520demonstrate%2520that%2520medical%2520and%2520domain-specific%250Avocabularies%2520outperformed%2520widely%2520used%2520natural%2520language%2520alternatives%2520when%2520models%250Aare%2520trained%2520from%2520scratch.%2520Pre-training%2520partially%2520mitigates%2520performance%250Adifferences%2520between%2520tokenizers%252C%2520whilst%2520the%2520domain-specific%2520tokenizers%2520achieve%250Athe%2520most%2520favourable%2520results.%2520Domain-specific%2520tokenizers%2520also%2520reduce%2520memory%250Arequirements%2520due%2520to%2520smaller%2520vocabularies%2520and%2520shorter%2520sequences.%2520These%2520results%250Ademonstrate%2520that%2520adapting%2520the%2520vocabulary%2520of%2520LMs%2520to%2520the%2520clinical%2520domain%2520provides%250Apractical%2520benefits%252C%2520including%2520improved%2520performance%2520and%2520reduced%2520computational%250Ademands%252C%2520making%2520such%2520models%2520more%2520accessible%2520and%2520effective%2520for%2520both%2520research%2520and%250Areal-world%2520healthcare%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Specialised%20or%20Generic%3F%20Tokenization%20Choices%20for%20Radiology%20Language%0A%20%20Models&entry.906535625=Hermione%20Warr%20and%20Wentian%20Xu%20and%20Harry%20Anthony%20and%20Yasin%20Ibrahim%20and%20Daniel%20McGowan%20and%20Konstantinos%20Kamnitsas&entry.1292438233=%20%20The%20vocabulary%20used%20by%20language%20models%20%28LM%29%20-%20defined%20by%20the%20tokenizer%20-%0Aplays%20a%20key%20role%20in%20text%20generation%20quality.%20However%2C%20its%20impact%20remains%0Aunder-explored%20in%20radiology.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%0Asystematically%20comparing%20general%2C%20medical%2C%20and%20domain-specific%20tokenizers%20on%0Athe%20task%20of%20radiology%20report%20summarisation%20across%20three%20imaging%20modalities.%20We%0Aalso%20investigate%20scenarios%20with%20and%20without%20LM%20pre-training%20on%20PubMed%0Aabstracts.%20Our%20findings%20demonstrate%20that%20medical%20and%20domain-specific%0Avocabularies%20outperformed%20widely%20used%20natural%20language%20alternatives%20when%20models%0Aare%20trained%20from%20scratch.%20Pre-training%20partially%20mitigates%20performance%0Adifferences%20between%20tokenizers%2C%20whilst%20the%20domain-specific%20tokenizers%20achieve%0Athe%20most%20favourable%20results.%20Domain-specific%20tokenizers%20also%20reduce%20memory%0Arequirements%20due%20to%20smaller%20vocabularies%20and%20shorter%20sequences.%20These%20results%0Ademonstrate%20that%20adapting%20the%20vocabulary%20of%20LMs%20to%20the%20clinical%20domain%20provides%0Apractical%20benefits%2C%20including%20improved%20performance%20and%20reduced%20computational%0Ademands%2C%20making%20such%20models%20more%20accessible%20and%20effective%20for%20both%20research%20and%0Areal-world%20healthcare%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09952v1&entry.124074799=Read"},
{"title": "Provable In-Context Vector Arithmetic via Retrieving Task Concepts", "author": "Dake Bu and Wei Huang and Andi Han and Atsushi Nitanda and Qingfu Zhang and Hau-San Wong and Taiji Suzuki", "abstract": "  In-context learning (ICL) has garnered significant attention for its ability\nto grasp functions/tasks from demonstrations. Recent studies suggest the\npresence of a latent task/function vector in LLMs during ICL. Merullo et al.\n(2024) showed that LLMs leverage this vector alongside the residual stream for\nWord2Vec-like vector arithmetic, solving factual-recall ICL tasks.\nAdditionally, recent work empirically highlighted the key role of\nQuestion-Answer data in enhancing factual-recall capabilities. Despite these\ninsights, a theoretical explanation remains elusive. To move one step forward,\nwe propose a theoretical framework building on empirically grounded\nhierarchical concept modeling. We develop an optimization theory, showing how\nnonlinear residual transformers trained via gradient descent on cross-entropy\nloss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss\nconvergence and show the strong generalization, including robustness to concept\nrecombination and distribution shifts. These results elucidate the advantages\nof transformers over static embedding predecessors. Empirical simulations\ncorroborate our theoretical insights.\n", "link": "http://arxiv.org/abs/2508.09820v1", "date": "2025-08-13", "relevancy": 2.6007, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20In-Context%20Vector%20Arithmetic%20via%20Retrieving%20Task%20Concepts&body=Title%3A%20Provable%20In-Context%20Vector%20Arithmetic%20via%20Retrieving%20Task%20Concepts%0AAuthor%3A%20Dake%20Bu%20and%20Wei%20Huang%20and%20Andi%20Han%20and%20Atsushi%20Nitanda%20and%20Qingfu%20Zhang%20and%20Hau-San%20Wong%20and%20Taiji%20Suzuki%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20has%20garnered%20significant%20attention%20for%20its%20ability%0Ato%20grasp%20functions/tasks%20from%20demonstrations.%20Recent%20studies%20suggest%20the%0Apresence%20of%20a%20latent%20task/function%20vector%20in%20LLMs%20during%20ICL.%20Merullo%20et%20al.%0A%282024%29%20showed%20that%20LLMs%20leverage%20this%20vector%20alongside%20the%20residual%20stream%20for%0AWord2Vec-like%20vector%20arithmetic%2C%20solving%20factual-recall%20ICL%20tasks.%0AAdditionally%2C%20recent%20work%20empirically%20highlighted%20the%20key%20role%20of%0AQuestion-Answer%20data%20in%20enhancing%20factual-recall%20capabilities.%20Despite%20these%0Ainsights%2C%20a%20theoretical%20explanation%20remains%20elusive.%20To%20move%20one%20step%20forward%2C%0Awe%20propose%20a%20theoretical%20framework%20building%20on%20empirically%20grounded%0Ahierarchical%20concept%20modeling.%20We%20develop%20an%20optimization%20theory%2C%20showing%20how%0Anonlinear%20residual%20transformers%20trained%20via%20gradient%20descent%20on%20cross-entropy%0Aloss%20perform%20factual-recall%20ICL%20tasks%20via%20vector%20arithmetic.%20We%20prove%200-1%20loss%0Aconvergence%20and%20show%20the%20strong%20generalization%2C%20including%20robustness%20to%20concept%0Arecombination%20and%20distribution%20shifts.%20These%20results%20elucidate%20the%20advantages%0Aof%20transformers%20over%20static%20embedding%20predecessors.%20Empirical%20simulations%0Acorroborate%20our%20theoretical%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520In-Context%2520Vector%2520Arithmetic%2520via%2520Retrieving%2520Task%2520Concepts%26entry.906535625%3DDake%2520Bu%2520and%2520Wei%2520Huang%2520and%2520Andi%2520Han%2520and%2520Atsushi%2520Nitanda%2520and%2520Qingfu%2520Zhang%2520and%2520Hau-San%2520Wong%2520and%2520Taiji%2520Suzuki%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520has%2520garnered%2520significant%2520attention%2520for%2520its%2520ability%250Ato%2520grasp%2520functions/tasks%2520from%2520demonstrations.%2520Recent%2520studies%2520suggest%2520the%250Apresence%2520of%2520a%2520latent%2520task/function%2520vector%2520in%2520LLMs%2520during%2520ICL.%2520Merullo%2520et%2520al.%250A%25282024%2529%2520showed%2520that%2520LLMs%2520leverage%2520this%2520vector%2520alongside%2520the%2520residual%2520stream%2520for%250AWord2Vec-like%2520vector%2520arithmetic%252C%2520solving%2520factual-recall%2520ICL%2520tasks.%250AAdditionally%252C%2520recent%2520work%2520empirically%2520highlighted%2520the%2520key%2520role%2520of%250AQuestion-Answer%2520data%2520in%2520enhancing%2520factual-recall%2520capabilities.%2520Despite%2520these%250Ainsights%252C%2520a%2520theoretical%2520explanation%2520remains%2520elusive.%2520To%2520move%2520one%2520step%2520forward%252C%250Awe%2520propose%2520a%2520theoretical%2520framework%2520building%2520on%2520empirically%2520grounded%250Ahierarchical%2520concept%2520modeling.%2520We%2520develop%2520an%2520optimization%2520theory%252C%2520showing%2520how%250Anonlinear%2520residual%2520transformers%2520trained%2520via%2520gradient%2520descent%2520on%2520cross-entropy%250Aloss%2520perform%2520factual-recall%2520ICL%2520tasks%2520via%2520vector%2520arithmetic.%2520We%2520prove%25200-1%2520loss%250Aconvergence%2520and%2520show%2520the%2520strong%2520generalization%252C%2520including%2520robustness%2520to%2520concept%250Arecombination%2520and%2520distribution%2520shifts.%2520These%2520results%2520elucidate%2520the%2520advantages%250Aof%2520transformers%2520over%2520static%2520embedding%2520predecessors.%2520Empirical%2520simulations%250Acorroborate%2520our%2520theoretical%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20In-Context%20Vector%20Arithmetic%20via%20Retrieving%20Task%20Concepts&entry.906535625=Dake%20Bu%20and%20Wei%20Huang%20and%20Andi%20Han%20and%20Atsushi%20Nitanda%20and%20Qingfu%20Zhang%20and%20Hau-San%20Wong%20and%20Taiji%20Suzuki&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20has%20garnered%20significant%20attention%20for%20its%20ability%0Ato%20grasp%20functions/tasks%20from%20demonstrations.%20Recent%20studies%20suggest%20the%0Apresence%20of%20a%20latent%20task/function%20vector%20in%20LLMs%20during%20ICL.%20Merullo%20et%20al.%0A%282024%29%20showed%20that%20LLMs%20leverage%20this%20vector%20alongside%20the%20residual%20stream%20for%0AWord2Vec-like%20vector%20arithmetic%2C%20solving%20factual-recall%20ICL%20tasks.%0AAdditionally%2C%20recent%20work%20empirically%20highlighted%20the%20key%20role%20of%0AQuestion-Answer%20data%20in%20enhancing%20factual-recall%20capabilities.%20Despite%20these%0Ainsights%2C%20a%20theoretical%20explanation%20remains%20elusive.%20To%20move%20one%20step%20forward%2C%0Awe%20propose%20a%20theoretical%20framework%20building%20on%20empirically%20grounded%0Ahierarchical%20concept%20modeling.%20We%20develop%20an%20optimization%20theory%2C%20showing%20how%0Anonlinear%20residual%20transformers%20trained%20via%20gradient%20descent%20on%20cross-entropy%0Aloss%20perform%20factual-recall%20ICL%20tasks%20via%20vector%20arithmetic.%20We%20prove%200-1%20loss%0Aconvergence%20and%20show%20the%20strong%20generalization%2C%20including%20robustness%20to%20concept%0Arecombination%20and%20distribution%20shifts.%20These%20results%20elucidate%20the%20advantages%0Aof%20transformers%20over%20static%20embedding%20predecessors.%20Empirical%20simulations%0Acorroborate%20our%20theoretical%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09820v1&entry.124074799=Read"},
{"title": "RAGAR: Retrieval Augmented Personalized Image Generation Guided by\n  Recommendation", "author": "Run Ling and Wenji Wang and Yuting Liu and Guibing Guo and Haowei Liu and Jian Lu and Quanwei Zhang and Yexing Xu and Shuo Lu and Yun Wang and Yihua Shao and Zhanjie Zhang and Ao Ma and Linying Jiang and Xingwei Wang", "abstract": "  Personalized image generation is crucial for improving the user experience,\nas it renders reference images into preferred ones according to user visual\npreferences. Although effective, existing methods face two main issues. First,\nexisting methods treat all items in the user historical sequence equally when\nextracting user preferences, overlooking the varying semantic similarities\nbetween historical items and the reference item. Disproportionately high\nweights for low-similarity items distort users' visual preferences for the\nreference item. Second, existing methods heavily rely on consistency between\ngenerated and reference images to optimize the generation, which leads to\nunderfitting user preferences and hinders personalization. To address these\nissues, we propose Retrieval Augment Personalized Image GenerAtion guided by\nRecommendation (RAGAR). Our approach uses a retrieval mechanism to assign\ndifferent weights to historical items according to their similarities to the\nreference item, thereby extracting more refined users' visual preferences for\nthe reference item. Then we introduce a novel rank task based on the\nmulti-modal ranking model to optimize the personalization of the generated\nimages instead of forcing depend on consistency. Extensive experiments and\nhuman evaluations on three real-world datasets demonstrate that RAGAR achieves\nsignificant improvements in both personalization and semantic metrics compared\nto five baselines.\n", "link": "http://arxiv.org/abs/2505.01657v2", "date": "2025-08-13", "relevancy": 2.5867, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5263}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5133}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAGAR%3A%20Retrieval%20Augmented%20Personalized%20Image%20Generation%20Guided%20by%0A%20%20Recommendation&body=Title%3A%20RAGAR%3A%20Retrieval%20Augmented%20Personalized%20Image%20Generation%20Guided%20by%0A%20%20Recommendation%0AAuthor%3A%20Run%20Ling%20and%20Wenji%20Wang%20and%20Yuting%20Liu%20and%20Guibing%20Guo%20and%20Haowei%20Liu%20and%20Jian%20Lu%20and%20Quanwei%20Zhang%20and%20Yexing%20Xu%20and%20Shuo%20Lu%20and%20Yun%20Wang%20and%20Yihua%20Shao%20and%20Zhanjie%20Zhang%20and%20Ao%20Ma%20and%20Linying%20Jiang%20and%20Xingwei%20Wang%0AAbstract%3A%20%20%20Personalized%20image%20generation%20is%20crucial%20for%20improving%20the%20user%20experience%2C%0Aas%20it%20renders%20reference%20images%20into%20preferred%20ones%20according%20to%20user%20visual%0Apreferences.%20Although%20effective%2C%20existing%20methods%20face%20two%20main%20issues.%20First%2C%0Aexisting%20methods%20treat%20all%20items%20in%20the%20user%20historical%20sequence%20equally%20when%0Aextracting%20user%20preferences%2C%20overlooking%20the%20varying%20semantic%20similarities%0Abetween%20historical%20items%20and%20the%20reference%20item.%20Disproportionately%20high%0Aweights%20for%20low-similarity%20items%20distort%20users%27%20visual%20preferences%20for%20the%0Areference%20item.%20Second%2C%20existing%20methods%20heavily%20rely%20on%20consistency%20between%0Agenerated%20and%20reference%20images%20to%20optimize%20the%20generation%2C%20which%20leads%20to%0Aunderfitting%20user%20preferences%20and%20hinders%20personalization.%20To%20address%20these%0Aissues%2C%20we%20propose%20Retrieval%20Augment%20Personalized%20Image%20GenerAtion%20guided%20by%0ARecommendation%20%28RAGAR%29.%20Our%20approach%20uses%20a%20retrieval%20mechanism%20to%20assign%0Adifferent%20weights%20to%20historical%20items%20according%20to%20their%20similarities%20to%20the%0Areference%20item%2C%20thereby%20extracting%20more%20refined%20users%27%20visual%20preferences%20for%0Athe%20reference%20item.%20Then%20we%20introduce%20a%20novel%20rank%20task%20based%20on%20the%0Amulti-modal%20ranking%20model%20to%20optimize%20the%20personalization%20of%20the%20generated%0Aimages%20instead%20of%20forcing%20depend%20on%20consistency.%20Extensive%20experiments%20and%0Ahuman%20evaluations%20on%20three%20real-world%20datasets%20demonstrate%20that%20RAGAR%20achieves%0Asignificant%20improvements%20in%20both%20personalization%20and%20semantic%20metrics%20compared%0Ato%20five%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01657v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAGAR%253A%2520Retrieval%2520Augmented%2520Personalized%2520Image%2520Generation%2520Guided%2520by%250A%2520%2520Recommendation%26entry.906535625%3DRun%2520Ling%2520and%2520Wenji%2520Wang%2520and%2520Yuting%2520Liu%2520and%2520Guibing%2520Guo%2520and%2520Haowei%2520Liu%2520and%2520Jian%2520Lu%2520and%2520Quanwei%2520Zhang%2520and%2520Yexing%2520Xu%2520and%2520Shuo%2520Lu%2520and%2520Yun%2520Wang%2520and%2520Yihua%2520Shao%2520and%2520Zhanjie%2520Zhang%2520and%2520Ao%2520Ma%2520and%2520Linying%2520Jiang%2520and%2520Xingwei%2520Wang%26entry.1292438233%3D%2520%2520Personalized%2520image%2520generation%2520is%2520crucial%2520for%2520improving%2520the%2520user%2520experience%252C%250Aas%2520it%2520renders%2520reference%2520images%2520into%2520preferred%2520ones%2520according%2520to%2520user%2520visual%250Apreferences.%2520Although%2520effective%252C%2520existing%2520methods%2520face%2520two%2520main%2520issues.%2520First%252C%250Aexisting%2520methods%2520treat%2520all%2520items%2520in%2520the%2520user%2520historical%2520sequence%2520equally%2520when%250Aextracting%2520user%2520preferences%252C%2520overlooking%2520the%2520varying%2520semantic%2520similarities%250Abetween%2520historical%2520items%2520and%2520the%2520reference%2520item.%2520Disproportionately%2520high%250Aweights%2520for%2520low-similarity%2520items%2520distort%2520users%2527%2520visual%2520preferences%2520for%2520the%250Areference%2520item.%2520Second%252C%2520existing%2520methods%2520heavily%2520rely%2520on%2520consistency%2520between%250Agenerated%2520and%2520reference%2520images%2520to%2520optimize%2520the%2520generation%252C%2520which%2520leads%2520to%250Aunderfitting%2520user%2520preferences%2520and%2520hinders%2520personalization.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520Retrieval%2520Augment%2520Personalized%2520Image%2520GenerAtion%2520guided%2520by%250ARecommendation%2520%2528RAGAR%2529.%2520Our%2520approach%2520uses%2520a%2520retrieval%2520mechanism%2520to%2520assign%250Adifferent%2520weights%2520to%2520historical%2520items%2520according%2520to%2520their%2520similarities%2520to%2520the%250Areference%2520item%252C%2520thereby%2520extracting%2520more%2520refined%2520users%2527%2520visual%2520preferences%2520for%250Athe%2520reference%2520item.%2520Then%2520we%2520introduce%2520a%2520novel%2520rank%2520task%2520based%2520on%2520the%250Amulti-modal%2520ranking%2520model%2520to%2520optimize%2520the%2520personalization%2520of%2520the%2520generated%250Aimages%2520instead%2520of%2520forcing%2520depend%2520on%2520consistency.%2520Extensive%2520experiments%2520and%250Ahuman%2520evaluations%2520on%2520three%2520real-world%2520datasets%2520demonstrate%2520that%2520RAGAR%2520achieves%250Asignificant%2520improvements%2520in%2520both%2520personalization%2520and%2520semantic%2520metrics%2520compared%250Ato%2520five%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01657v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAGAR%3A%20Retrieval%20Augmented%20Personalized%20Image%20Generation%20Guided%20by%0A%20%20Recommendation&entry.906535625=Run%20Ling%20and%20Wenji%20Wang%20and%20Yuting%20Liu%20and%20Guibing%20Guo%20and%20Haowei%20Liu%20and%20Jian%20Lu%20and%20Quanwei%20Zhang%20and%20Yexing%20Xu%20and%20Shuo%20Lu%20and%20Yun%20Wang%20and%20Yihua%20Shao%20and%20Zhanjie%20Zhang%20and%20Ao%20Ma%20and%20Linying%20Jiang%20and%20Xingwei%20Wang&entry.1292438233=%20%20Personalized%20image%20generation%20is%20crucial%20for%20improving%20the%20user%20experience%2C%0Aas%20it%20renders%20reference%20images%20into%20preferred%20ones%20according%20to%20user%20visual%0Apreferences.%20Although%20effective%2C%20existing%20methods%20face%20two%20main%20issues.%20First%2C%0Aexisting%20methods%20treat%20all%20items%20in%20the%20user%20historical%20sequence%20equally%20when%0Aextracting%20user%20preferences%2C%20overlooking%20the%20varying%20semantic%20similarities%0Abetween%20historical%20items%20and%20the%20reference%20item.%20Disproportionately%20high%0Aweights%20for%20low-similarity%20items%20distort%20users%27%20visual%20preferences%20for%20the%0Areference%20item.%20Second%2C%20existing%20methods%20heavily%20rely%20on%20consistency%20between%0Agenerated%20and%20reference%20images%20to%20optimize%20the%20generation%2C%20which%20leads%20to%0Aunderfitting%20user%20preferences%20and%20hinders%20personalization.%20To%20address%20these%0Aissues%2C%20we%20propose%20Retrieval%20Augment%20Personalized%20Image%20GenerAtion%20guided%20by%0ARecommendation%20%28RAGAR%29.%20Our%20approach%20uses%20a%20retrieval%20mechanism%20to%20assign%0Adifferent%20weights%20to%20historical%20items%20according%20to%20their%20similarities%20to%20the%0Areference%20item%2C%20thereby%20extracting%20more%20refined%20users%27%20visual%20preferences%20for%0Athe%20reference%20item.%20Then%20we%20introduce%20a%20novel%20rank%20task%20based%20on%20the%0Amulti-modal%20ranking%20model%20to%20optimize%20the%20personalization%20of%20the%20generated%0Aimages%20instead%20of%20forcing%20depend%20on%20consistency.%20Extensive%20experiments%20and%0Ahuman%20evaluations%20on%20three%20real-world%20datasets%20demonstrate%20that%20RAGAR%20achieves%0Asignificant%20improvements%20in%20both%20personalization%20and%20semantic%20metrics%20compared%0Ato%20five%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01657v2&entry.124074799=Read"},
{"title": "KonfAI: A Modular and Fully Configurable Framework for Deep Learning in\n  Medical Imaging", "author": "Valentin Boussot and Jean-Louis Dillenseger", "abstract": "  KonfAI is a modular, extensible, and fully configurable deep learning\nframework specifically designed for medical imaging tasks. It enables users to\ndefine complete training, inference, and evaluation workflows through\nstructured YAML configuration files, without modifying the underlying code.\nThis declarative approach enhances reproducibility, transparency, and\nexperimental traceability while reducing development time. Beyond the\ncapabilities of standard pipelines, KonfAI provides native abstractions for\nadvanced strategies including patch-based learning, test-time augmentation,\nmodel ensembling, and direct access to intermediate feature representations for\ndeep supervision. It also supports complex multi-model training setups such as\ngenerative adversarial architectures. Thanks to its modular and extensible\narchitecture, KonfAI can easily accommodate custom models, loss functions, and\ndata processing components. The framework has been successfully applied to\nsegmentation, registration, and image synthesis tasks, and has contributed to\ntop-ranking results in several international medical imaging challenges. KonfAI\nis open source and available at\n\\href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}.\n", "link": "http://arxiv.org/abs/2508.09823v1", "date": "2025-08-13", "relevancy": 2.5743, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KonfAI%3A%20A%20Modular%20and%20Fully%20Configurable%20Framework%20for%20Deep%20Learning%20in%0A%20%20Medical%20Imaging&body=Title%3A%20KonfAI%3A%20A%20Modular%20and%20Fully%20Configurable%20Framework%20for%20Deep%20Learning%20in%0A%20%20Medical%20Imaging%0AAuthor%3A%20Valentin%20Boussot%20and%20Jean-Louis%20Dillenseger%0AAbstract%3A%20%20%20KonfAI%20is%20a%20modular%2C%20extensible%2C%20and%20fully%20configurable%20deep%20learning%0Aframework%20specifically%20designed%20for%20medical%20imaging%20tasks.%20It%20enables%20users%20to%0Adefine%20complete%20training%2C%20inference%2C%20and%20evaluation%20workflows%20through%0Astructured%20YAML%20configuration%20files%2C%20without%20modifying%20the%20underlying%20code.%0AThis%20declarative%20approach%20enhances%20reproducibility%2C%20transparency%2C%20and%0Aexperimental%20traceability%20while%20reducing%20development%20time.%20Beyond%20the%0Acapabilities%20of%20standard%20pipelines%2C%20KonfAI%20provides%20native%20abstractions%20for%0Aadvanced%20strategies%20including%20patch-based%20learning%2C%20test-time%20augmentation%2C%0Amodel%20ensembling%2C%20and%20direct%20access%20to%20intermediate%20feature%20representations%20for%0Adeep%20supervision.%20It%20also%20supports%20complex%20multi-model%20training%20setups%20such%20as%0Agenerative%20adversarial%20architectures.%20Thanks%20to%20its%20modular%20and%20extensible%0Aarchitecture%2C%20KonfAI%20can%20easily%20accommodate%20custom%20models%2C%20loss%20functions%2C%20and%0Adata%20processing%20components.%20The%20framework%20has%20been%20successfully%20applied%20to%0Asegmentation%2C%20registration%2C%20and%20image%20synthesis%20tasks%2C%20and%20has%20contributed%20to%0Atop-ranking%20results%20in%20several%20international%20medical%20imaging%20challenges.%20KonfAI%0Ais%20open%20source%20and%20available%20at%0A%5Chref%7Bhttps%3A//github.com/vboussot/KonfAI%7D%7Bhttps%3A//github.com/vboussot/KonfAI%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKonfAI%253A%2520A%2520Modular%2520and%2520Fully%2520Configurable%2520Framework%2520for%2520Deep%2520Learning%2520in%250A%2520%2520Medical%2520Imaging%26entry.906535625%3DValentin%2520Boussot%2520and%2520Jean-Louis%2520Dillenseger%26entry.1292438233%3D%2520%2520KonfAI%2520is%2520a%2520modular%252C%2520extensible%252C%2520and%2520fully%2520configurable%2520deep%2520learning%250Aframework%2520specifically%2520designed%2520for%2520medical%2520imaging%2520tasks.%2520It%2520enables%2520users%2520to%250Adefine%2520complete%2520training%252C%2520inference%252C%2520and%2520evaluation%2520workflows%2520through%250Astructured%2520YAML%2520configuration%2520files%252C%2520without%2520modifying%2520the%2520underlying%2520code.%250AThis%2520declarative%2520approach%2520enhances%2520reproducibility%252C%2520transparency%252C%2520and%250Aexperimental%2520traceability%2520while%2520reducing%2520development%2520time.%2520Beyond%2520the%250Acapabilities%2520of%2520standard%2520pipelines%252C%2520KonfAI%2520provides%2520native%2520abstractions%2520for%250Aadvanced%2520strategies%2520including%2520patch-based%2520learning%252C%2520test-time%2520augmentation%252C%250Amodel%2520ensembling%252C%2520and%2520direct%2520access%2520to%2520intermediate%2520feature%2520representations%2520for%250Adeep%2520supervision.%2520It%2520also%2520supports%2520complex%2520multi-model%2520training%2520setups%2520such%2520as%250Agenerative%2520adversarial%2520architectures.%2520Thanks%2520to%2520its%2520modular%2520and%2520extensible%250Aarchitecture%252C%2520KonfAI%2520can%2520easily%2520accommodate%2520custom%2520models%252C%2520loss%2520functions%252C%2520and%250Adata%2520processing%2520components.%2520The%2520framework%2520has%2520been%2520successfully%2520applied%2520to%250Asegmentation%252C%2520registration%252C%2520and%2520image%2520synthesis%2520tasks%252C%2520and%2520has%2520contributed%2520to%250Atop-ranking%2520results%2520in%2520several%2520international%2520medical%2520imaging%2520challenges.%2520KonfAI%250Ais%2520open%2520source%2520and%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/vboussot/KonfAI%257D%257Bhttps%253A//github.com/vboussot/KonfAI%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KonfAI%3A%20A%20Modular%20and%20Fully%20Configurable%20Framework%20for%20Deep%20Learning%20in%0A%20%20Medical%20Imaging&entry.906535625=Valentin%20Boussot%20and%20Jean-Louis%20Dillenseger&entry.1292438233=%20%20KonfAI%20is%20a%20modular%2C%20extensible%2C%20and%20fully%20configurable%20deep%20learning%0Aframework%20specifically%20designed%20for%20medical%20imaging%20tasks.%20It%20enables%20users%20to%0Adefine%20complete%20training%2C%20inference%2C%20and%20evaluation%20workflows%20through%0Astructured%20YAML%20configuration%20files%2C%20without%20modifying%20the%20underlying%20code.%0AThis%20declarative%20approach%20enhances%20reproducibility%2C%20transparency%2C%20and%0Aexperimental%20traceability%20while%20reducing%20development%20time.%20Beyond%20the%0Acapabilities%20of%20standard%20pipelines%2C%20KonfAI%20provides%20native%20abstractions%20for%0Aadvanced%20strategies%20including%20patch-based%20learning%2C%20test-time%20augmentation%2C%0Amodel%20ensembling%2C%20and%20direct%20access%20to%20intermediate%20feature%20representations%20for%0Adeep%20supervision.%20It%20also%20supports%20complex%20multi-model%20training%20setups%20such%20as%0Agenerative%20adversarial%20architectures.%20Thanks%20to%20its%20modular%20and%20extensible%0Aarchitecture%2C%20KonfAI%20can%20easily%20accommodate%20custom%20models%2C%20loss%20functions%2C%20and%0Adata%20processing%20components.%20The%20framework%20has%20been%20successfully%20applied%20to%0Asegmentation%2C%20registration%2C%20and%20image%20synthesis%20tasks%2C%20and%20has%20contributed%20to%0Atop-ranking%20results%20in%20several%20international%20medical%20imaging%20challenges.%20KonfAI%0Ais%20open%20source%20and%20available%20at%0A%5Chref%7Bhttps%3A//github.com/vboussot/KonfAI%7D%7Bhttps%3A//github.com/vboussot/KonfAI%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09823v1&entry.124074799=Read"},
{"title": "Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal\n  Decoupled Finetuning", "author": "Yue Ma and Yulong Liu and Qiyuan Zhu and Ayden Yang and Kunyu Feng and Xinhua Zhang and Zhifeng Li and Sirui Han and Chenyang Qi and Qifeng Chen", "abstract": "  Recently, breakthroughs in the video diffusion transformer have shown\nremarkable capabilities in diverse motion generations. As for the\nmotion-transfer task, current methods mainly use two-stage Low-Rank Adaptations\n(LoRAs) finetuning to obtain better performance. However, existing\nadaptation-based motion transfer still suffers from motion inconsistency and\ntuning inefficiency when applied to large video diffusion transformers. Naive\ntwo-stage LoRA tuning struggles to maintain motion consistency between\ngenerated and input videos due to the inherent spatial-temporal coupling in the\n3D attention operator. Additionally, they require time-consuming fine-tuning\nprocesses in both stages. To tackle these issues, we propose\nFollow-Your-Motion, an efficient two-stage video motion transfer framework that\nfinetunes a powerful video diffusion transformer to synthesize complex motion.\nSpecifically, we propose a spatial-temporal decoupled LoRA to decouple the\nattention architecture for spatial appearance and temporal motion processing.\nDuring the second training stage, we design the sparse motion sampling and\nadaptive RoPE to accelerate the tuning speed. To address the lack of a\nbenchmark for this field, we introduce MotionBench, a comprehensive benchmark\ncomprising diverse motion, including creative camera motion, single object\nmotion, multiple object motion, and complex human motion. We show extensive\nevaluations on MotionBench to verify the superiority of Follow-Your-Motion.\n", "link": "http://arxiv.org/abs/2506.05207v2", "date": "2025-08-13", "relevancy": 2.5689, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6972}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6327}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Follow-Your-Motion%3A%20Video%20Motion%20Transfer%20via%20Efficient%20Spatial-Temporal%0A%20%20Decoupled%20Finetuning&body=Title%3A%20Follow-Your-Motion%3A%20Video%20Motion%20Transfer%20via%20Efficient%20Spatial-Temporal%0A%20%20Decoupled%20Finetuning%0AAuthor%3A%20Yue%20Ma%20and%20Yulong%20Liu%20and%20Qiyuan%20Zhu%20and%20Ayden%20Yang%20and%20Kunyu%20Feng%20and%20Xinhua%20Zhang%20and%20Zhifeng%20Li%20and%20Sirui%20Han%20and%20Chenyang%20Qi%20and%20Qifeng%20Chen%0AAbstract%3A%20%20%20Recently%2C%20breakthroughs%20in%20the%20video%20diffusion%20transformer%20have%20shown%0Aremarkable%20capabilities%20in%20diverse%20motion%20generations.%20As%20for%20the%0Amotion-transfer%20task%2C%20current%20methods%20mainly%20use%20two-stage%20Low-Rank%20Adaptations%0A%28LoRAs%29%20finetuning%20to%20obtain%20better%20performance.%20However%2C%20existing%0Aadaptation-based%20motion%20transfer%20still%20suffers%20from%20motion%20inconsistency%20and%0Atuning%20inefficiency%20when%20applied%20to%20large%20video%20diffusion%20transformers.%20Naive%0Atwo-stage%20LoRA%20tuning%20struggles%20to%20maintain%20motion%20consistency%20between%0Agenerated%20and%20input%20videos%20due%20to%20the%20inherent%20spatial-temporal%20coupling%20in%20the%0A3D%20attention%20operator.%20Additionally%2C%20they%20require%20time-consuming%20fine-tuning%0Aprocesses%20in%20both%20stages.%20To%20tackle%20these%20issues%2C%20we%20propose%0AFollow-Your-Motion%2C%20an%20efficient%20two-stage%20video%20motion%20transfer%20framework%20that%0Afinetunes%20a%20powerful%20video%20diffusion%20transformer%20to%20synthesize%20complex%20motion.%0ASpecifically%2C%20we%20propose%20a%20spatial-temporal%20decoupled%20LoRA%20to%20decouple%20the%0Aattention%20architecture%20for%20spatial%20appearance%20and%20temporal%20motion%20processing.%0ADuring%20the%20second%20training%20stage%2C%20we%20design%20the%20sparse%20motion%20sampling%20and%0Aadaptive%20RoPE%20to%20accelerate%20the%20tuning%20speed.%20To%20address%20the%20lack%20of%20a%0Abenchmark%20for%20this%20field%2C%20we%20introduce%20MotionBench%2C%20a%20comprehensive%20benchmark%0Acomprising%20diverse%20motion%2C%20including%20creative%20camera%20motion%2C%20single%20object%0Amotion%2C%20multiple%20object%20motion%2C%20and%20complex%20human%20motion.%20We%20show%20extensive%0Aevaluations%20on%20MotionBench%20to%20verify%20the%20superiority%20of%20Follow-Your-Motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05207v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFollow-Your-Motion%253A%2520Video%2520Motion%2520Transfer%2520via%2520Efficient%2520Spatial-Temporal%250A%2520%2520Decoupled%2520Finetuning%26entry.906535625%3DYue%2520Ma%2520and%2520Yulong%2520Liu%2520and%2520Qiyuan%2520Zhu%2520and%2520Ayden%2520Yang%2520and%2520Kunyu%2520Feng%2520and%2520Xinhua%2520Zhang%2520and%2520Zhifeng%2520Li%2520and%2520Sirui%2520Han%2520and%2520Chenyang%2520Qi%2520and%2520Qifeng%2520Chen%26entry.1292438233%3D%2520%2520Recently%252C%2520breakthroughs%2520in%2520the%2520video%2520diffusion%2520transformer%2520have%2520shown%250Aremarkable%2520capabilities%2520in%2520diverse%2520motion%2520generations.%2520As%2520for%2520the%250Amotion-transfer%2520task%252C%2520current%2520methods%2520mainly%2520use%2520two-stage%2520Low-Rank%2520Adaptations%250A%2528LoRAs%2529%2520finetuning%2520to%2520obtain%2520better%2520performance.%2520However%252C%2520existing%250Aadaptation-based%2520motion%2520transfer%2520still%2520suffers%2520from%2520motion%2520inconsistency%2520and%250Atuning%2520inefficiency%2520when%2520applied%2520to%2520large%2520video%2520diffusion%2520transformers.%2520Naive%250Atwo-stage%2520LoRA%2520tuning%2520struggles%2520to%2520maintain%2520motion%2520consistency%2520between%250Agenerated%2520and%2520input%2520videos%2520due%2520to%2520the%2520inherent%2520spatial-temporal%2520coupling%2520in%2520the%250A3D%2520attention%2520operator.%2520Additionally%252C%2520they%2520require%2520time-consuming%2520fine-tuning%250Aprocesses%2520in%2520both%2520stages.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%250AFollow-Your-Motion%252C%2520an%2520efficient%2520two-stage%2520video%2520motion%2520transfer%2520framework%2520that%250Afinetunes%2520a%2520powerful%2520video%2520diffusion%2520transformer%2520to%2520synthesize%2520complex%2520motion.%250ASpecifically%252C%2520we%2520propose%2520a%2520spatial-temporal%2520decoupled%2520LoRA%2520to%2520decouple%2520the%250Aattention%2520architecture%2520for%2520spatial%2520appearance%2520and%2520temporal%2520motion%2520processing.%250ADuring%2520the%2520second%2520training%2520stage%252C%2520we%2520design%2520the%2520sparse%2520motion%2520sampling%2520and%250Aadaptive%2520RoPE%2520to%2520accelerate%2520the%2520tuning%2520speed.%2520To%2520address%2520the%2520lack%2520of%2520a%250Abenchmark%2520for%2520this%2520field%252C%2520we%2520introduce%2520MotionBench%252C%2520a%2520comprehensive%2520benchmark%250Acomprising%2520diverse%2520motion%252C%2520including%2520creative%2520camera%2520motion%252C%2520single%2520object%250Amotion%252C%2520multiple%2520object%2520motion%252C%2520and%2520complex%2520human%2520motion.%2520We%2520show%2520extensive%250Aevaluations%2520on%2520MotionBench%2520to%2520verify%2520the%2520superiority%2520of%2520Follow-Your-Motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05207v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Follow-Your-Motion%3A%20Video%20Motion%20Transfer%20via%20Efficient%20Spatial-Temporal%0A%20%20Decoupled%20Finetuning&entry.906535625=Yue%20Ma%20and%20Yulong%20Liu%20and%20Qiyuan%20Zhu%20and%20Ayden%20Yang%20and%20Kunyu%20Feng%20and%20Xinhua%20Zhang%20and%20Zhifeng%20Li%20and%20Sirui%20Han%20and%20Chenyang%20Qi%20and%20Qifeng%20Chen&entry.1292438233=%20%20Recently%2C%20breakthroughs%20in%20the%20video%20diffusion%20transformer%20have%20shown%0Aremarkable%20capabilities%20in%20diverse%20motion%20generations.%20As%20for%20the%0Amotion-transfer%20task%2C%20current%20methods%20mainly%20use%20two-stage%20Low-Rank%20Adaptations%0A%28LoRAs%29%20finetuning%20to%20obtain%20better%20performance.%20However%2C%20existing%0Aadaptation-based%20motion%20transfer%20still%20suffers%20from%20motion%20inconsistency%20and%0Atuning%20inefficiency%20when%20applied%20to%20large%20video%20diffusion%20transformers.%20Naive%0Atwo-stage%20LoRA%20tuning%20struggles%20to%20maintain%20motion%20consistency%20between%0Agenerated%20and%20input%20videos%20due%20to%20the%20inherent%20spatial-temporal%20coupling%20in%20the%0A3D%20attention%20operator.%20Additionally%2C%20they%20require%20time-consuming%20fine-tuning%0Aprocesses%20in%20both%20stages.%20To%20tackle%20these%20issues%2C%20we%20propose%0AFollow-Your-Motion%2C%20an%20efficient%20two-stage%20video%20motion%20transfer%20framework%20that%0Afinetunes%20a%20powerful%20video%20diffusion%20transformer%20to%20synthesize%20complex%20motion.%0ASpecifically%2C%20we%20propose%20a%20spatial-temporal%20decoupled%20LoRA%20to%20decouple%20the%0Aattention%20architecture%20for%20spatial%20appearance%20and%20temporal%20motion%20processing.%0ADuring%20the%20second%20training%20stage%2C%20we%20design%20the%20sparse%20motion%20sampling%20and%0Aadaptive%20RoPE%20to%20accelerate%20the%20tuning%20speed.%20To%20address%20the%20lack%20of%20a%0Abenchmark%20for%20this%20field%2C%20we%20introduce%20MotionBench%2C%20a%20comprehensive%20benchmark%0Acomprising%20diverse%20motion%2C%20including%20creative%20camera%20motion%2C%20single%20object%0Amotion%2C%20multiple%20object%20motion%2C%20and%20complex%20human%20motion.%20We%20show%20extensive%0Aevaluations%20on%20MotionBench%20to%20verify%20the%20superiority%20of%20Follow-Your-Motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05207v2&entry.124074799=Read"},
{"title": "Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs\n  Performance", "author": "Abdelrahman A. Ali and Aya E. Fouda and Radwa J. Hanafy and Mohammed E. Fouda", "abstract": "  Mental health disorders are increasingly prevalent worldwide, creating an\nurgent need for innovative tools to support early diagnosis and intervention.\nThis study explores the potential of Large Language Models (LLMs) in multimodal\nmental health diagnostics, specifically for detecting depression and Post\nTraumatic Stress Disorder through text and audio modalities. Using the E-DAIC\ndataset, we compare text and audio modalities to investigate whether LLMs can\nperform equally well or better with audio inputs. We further examine the\nintegration of both modalities to determine if this can enhance diagnostic\naccuracy, which generally results in improved performance metrics. Our analysis\nspecifically utilizes custom-formulated metrics; Modal Superiority Score and\nDisagreement Resolvement Score to evaluate how combined modalities influence\nmodel performance. The Gemini 1.5 Pro model achieves the highest scores in\nbinary depression classification when using the combined modality, with an F1\nscore of 0.67 and a Balanced Accuracy (BA) of 77.4%, assessed across the full\ndataset. These results represent an increase of 3.1% over its performance with\nthe text modality and 2.7% over the audio modality, highlighting the\neffectiveness of integrating modalities to enhance diagnostic accuracy.\nNotably, all results are obtained in zero-shot inferring, highlighting the\nrobustness of the models without requiring task-specific fine-tuning. To\nexplore the impact of different configurations on model performance, we conduct\nbinary, severity, and multiclass tasks using both zero-shot and few-shot\nprompts, examining the effects of prompt variations on performance. The results\nreveal that models such as Gemini 1.5 Pro in text and audio modalities, and\nGPT-4o mini in the text modality, often surpass other models in balanced\naccuracy and F1 scores across multiple tasks.\n", "link": "http://arxiv.org/abs/2412.10417v2", "date": "2025-08-13", "relevancy": 2.5496, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Audio%20and%20Text%20Modalities%20in%20Mental%20Health%3A%20A%20Study%20of%20LLMs%0A%20%20Performance&body=Title%3A%20Leveraging%20Audio%20and%20Text%20Modalities%20in%20Mental%20Health%3A%20A%20Study%20of%20LLMs%0A%20%20Performance%0AAuthor%3A%20Abdelrahman%20A.%20Ali%20and%20Aya%20E.%20Fouda%20and%20Radwa%20J.%20Hanafy%20and%20Mohammed%20E.%20Fouda%0AAbstract%3A%20%20%20Mental%20health%20disorders%20are%20increasingly%20prevalent%20worldwide%2C%20creating%20an%0Aurgent%20need%20for%20innovative%20tools%20to%20support%20early%20diagnosis%20and%20intervention.%0AThis%20study%20explores%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20multimodal%0Amental%20health%20diagnostics%2C%20specifically%20for%20detecting%20depression%20and%20Post%0ATraumatic%20Stress%20Disorder%20through%20text%20and%20audio%20modalities.%20Using%20the%20E-DAIC%0Adataset%2C%20we%20compare%20text%20and%20audio%20modalities%20to%20investigate%20whether%20LLMs%20can%0Aperform%20equally%20well%20or%20better%20with%20audio%20inputs.%20We%20further%20examine%20the%0Aintegration%20of%20both%20modalities%20to%20determine%20if%20this%20can%20enhance%20diagnostic%0Aaccuracy%2C%20which%20generally%20results%20in%20improved%20performance%20metrics.%20Our%20analysis%0Aspecifically%20utilizes%20custom-formulated%20metrics%3B%20Modal%20Superiority%20Score%20and%0ADisagreement%20Resolvement%20Score%20to%20evaluate%20how%20combined%20modalities%20influence%0Amodel%20performance.%20The%20Gemini%201.5%20Pro%20model%20achieves%20the%20highest%20scores%20in%0Abinary%20depression%20classification%20when%20using%20the%20combined%20modality%2C%20with%20an%20F1%0Ascore%20of%200.67%20and%20a%20Balanced%20Accuracy%20%28BA%29%20of%2077.4%25%2C%20assessed%20across%20the%20full%0Adataset.%20These%20results%20represent%20an%20increase%20of%203.1%25%20over%20its%20performance%20with%0Athe%20text%20modality%20and%202.7%25%20over%20the%20audio%20modality%2C%20highlighting%20the%0Aeffectiveness%20of%20integrating%20modalities%20to%20enhance%20diagnostic%20accuracy.%0ANotably%2C%20all%20results%20are%20obtained%20in%20zero-shot%20inferring%2C%20highlighting%20the%0Arobustness%20of%20the%20models%20without%20requiring%20task-specific%20fine-tuning.%20To%0Aexplore%20the%20impact%20of%20different%20configurations%20on%20model%20performance%2C%20we%20conduct%0Abinary%2C%20severity%2C%20and%20multiclass%20tasks%20using%20both%20zero-shot%20and%20few-shot%0Aprompts%2C%20examining%20the%20effects%20of%20prompt%20variations%20on%20performance.%20The%20results%0Areveal%20that%20models%20such%20as%20Gemini%201.5%20Pro%20in%20text%20and%20audio%20modalities%2C%20and%0AGPT-4o%20mini%20in%20the%20text%20modality%2C%20often%20surpass%20other%20models%20in%20balanced%0Aaccuracy%20and%20F1%20scores%20across%20multiple%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Audio%2520and%2520Text%2520Modalities%2520in%2520Mental%2520Health%253A%2520A%2520Study%2520of%2520LLMs%250A%2520%2520Performance%26entry.906535625%3DAbdelrahman%2520A.%2520Ali%2520and%2520Aya%2520E.%2520Fouda%2520and%2520Radwa%2520J.%2520Hanafy%2520and%2520Mohammed%2520E.%2520Fouda%26entry.1292438233%3D%2520%2520Mental%2520health%2520disorders%2520are%2520increasingly%2520prevalent%2520worldwide%252C%2520creating%2520an%250Aurgent%2520need%2520for%2520innovative%2520tools%2520to%2520support%2520early%2520diagnosis%2520and%2520intervention.%250AThis%2520study%2520explores%2520the%2520potential%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520multimodal%250Amental%2520health%2520diagnostics%252C%2520specifically%2520for%2520detecting%2520depression%2520and%2520Post%250ATraumatic%2520Stress%2520Disorder%2520through%2520text%2520and%2520audio%2520modalities.%2520Using%2520the%2520E-DAIC%250Adataset%252C%2520we%2520compare%2520text%2520and%2520audio%2520modalities%2520to%2520investigate%2520whether%2520LLMs%2520can%250Aperform%2520equally%2520well%2520or%2520better%2520with%2520audio%2520inputs.%2520We%2520further%2520examine%2520the%250Aintegration%2520of%2520both%2520modalities%2520to%2520determine%2520if%2520this%2520can%2520enhance%2520diagnostic%250Aaccuracy%252C%2520which%2520generally%2520results%2520in%2520improved%2520performance%2520metrics.%2520Our%2520analysis%250Aspecifically%2520utilizes%2520custom-formulated%2520metrics%253B%2520Modal%2520Superiority%2520Score%2520and%250ADisagreement%2520Resolvement%2520Score%2520to%2520evaluate%2520how%2520combined%2520modalities%2520influence%250Amodel%2520performance.%2520The%2520Gemini%25201.5%2520Pro%2520model%2520achieves%2520the%2520highest%2520scores%2520in%250Abinary%2520depression%2520classification%2520when%2520using%2520the%2520combined%2520modality%252C%2520with%2520an%2520F1%250Ascore%2520of%25200.67%2520and%2520a%2520Balanced%2520Accuracy%2520%2528BA%2529%2520of%252077.4%2525%252C%2520assessed%2520across%2520the%2520full%250Adataset.%2520These%2520results%2520represent%2520an%2520increase%2520of%25203.1%2525%2520over%2520its%2520performance%2520with%250Athe%2520text%2520modality%2520and%25202.7%2525%2520over%2520the%2520audio%2520modality%252C%2520highlighting%2520the%250Aeffectiveness%2520of%2520integrating%2520modalities%2520to%2520enhance%2520diagnostic%2520accuracy.%250ANotably%252C%2520all%2520results%2520are%2520obtained%2520in%2520zero-shot%2520inferring%252C%2520highlighting%2520the%250Arobustness%2520of%2520the%2520models%2520without%2520requiring%2520task-specific%2520fine-tuning.%2520To%250Aexplore%2520the%2520impact%2520of%2520different%2520configurations%2520on%2520model%2520performance%252C%2520we%2520conduct%250Abinary%252C%2520severity%252C%2520and%2520multiclass%2520tasks%2520using%2520both%2520zero-shot%2520and%2520few-shot%250Aprompts%252C%2520examining%2520the%2520effects%2520of%2520prompt%2520variations%2520on%2520performance.%2520The%2520results%250Areveal%2520that%2520models%2520such%2520as%2520Gemini%25201.5%2520Pro%2520in%2520text%2520and%2520audio%2520modalities%252C%2520and%250AGPT-4o%2520mini%2520in%2520the%2520text%2520modality%252C%2520often%2520surpass%2520other%2520models%2520in%2520balanced%250Aaccuracy%2520and%2520F1%2520scores%2520across%2520multiple%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Audio%20and%20Text%20Modalities%20in%20Mental%20Health%3A%20A%20Study%20of%20LLMs%0A%20%20Performance&entry.906535625=Abdelrahman%20A.%20Ali%20and%20Aya%20E.%20Fouda%20and%20Radwa%20J.%20Hanafy%20and%20Mohammed%20E.%20Fouda&entry.1292438233=%20%20Mental%20health%20disorders%20are%20increasingly%20prevalent%20worldwide%2C%20creating%20an%0Aurgent%20need%20for%20innovative%20tools%20to%20support%20early%20diagnosis%20and%20intervention.%0AThis%20study%20explores%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20multimodal%0Amental%20health%20diagnostics%2C%20specifically%20for%20detecting%20depression%20and%20Post%0ATraumatic%20Stress%20Disorder%20through%20text%20and%20audio%20modalities.%20Using%20the%20E-DAIC%0Adataset%2C%20we%20compare%20text%20and%20audio%20modalities%20to%20investigate%20whether%20LLMs%20can%0Aperform%20equally%20well%20or%20better%20with%20audio%20inputs.%20We%20further%20examine%20the%0Aintegration%20of%20both%20modalities%20to%20determine%20if%20this%20can%20enhance%20diagnostic%0Aaccuracy%2C%20which%20generally%20results%20in%20improved%20performance%20metrics.%20Our%20analysis%0Aspecifically%20utilizes%20custom-formulated%20metrics%3B%20Modal%20Superiority%20Score%20and%0ADisagreement%20Resolvement%20Score%20to%20evaluate%20how%20combined%20modalities%20influence%0Amodel%20performance.%20The%20Gemini%201.5%20Pro%20model%20achieves%20the%20highest%20scores%20in%0Abinary%20depression%20classification%20when%20using%20the%20combined%20modality%2C%20with%20an%20F1%0Ascore%20of%200.67%20and%20a%20Balanced%20Accuracy%20%28BA%29%20of%2077.4%25%2C%20assessed%20across%20the%20full%0Adataset.%20These%20results%20represent%20an%20increase%20of%203.1%25%20over%20its%20performance%20with%0Athe%20text%20modality%20and%202.7%25%20over%20the%20audio%20modality%2C%20highlighting%20the%0Aeffectiveness%20of%20integrating%20modalities%20to%20enhance%20diagnostic%20accuracy.%0ANotably%2C%20all%20results%20are%20obtained%20in%20zero-shot%20inferring%2C%20highlighting%20the%0Arobustness%20of%20the%20models%20without%20requiring%20task-specific%20fine-tuning.%20To%0Aexplore%20the%20impact%20of%20different%20configurations%20on%20model%20performance%2C%20we%20conduct%0Abinary%2C%20severity%2C%20and%20multiclass%20tasks%20using%20both%20zero-shot%20and%20few-shot%0Aprompts%2C%20examining%20the%20effects%20of%20prompt%20variations%20on%20performance.%20The%20results%0Areveal%20that%20models%20such%20as%20Gemini%201.5%20Pro%20in%20text%20and%20audio%20modalities%2C%20and%0AGPT-4o%20mini%20in%20the%20text%20modality%2C%20often%20surpass%20other%20models%20in%20balanced%0Aaccuracy%20and%20F1%20scores%20across%20multiple%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10417v2&entry.124074799=Read"},
{"title": "Generative Active Adaptation for Drifting and Imbalanced Network\n  Intrusion Detection", "author": "Ragini Gupta and Shinan Liu and Ruixiao Zhang and Xinyue Hu and Xiaoyang Wang and Hadjer Benkraouda and Pranav Kommaraju and Nick Feamster and Klara Nahrstedt", "abstract": "  Machine learning has shown promise in network intrusion detection systems,\nyet its performance often degrades due to concept drift and imbalanced data.\nThese challenges are compounded by the labor-intensive process of labeling\nnetwork traffic, especially when dealing with evolving and rare attack types,\nwhich makes preparing the right data for adaptation difficult. To address these\nissues, we propose a generative active adaptation framework that minimizes\nlabeling effort while enhancing model robustness. Our approach employs\ndensity-aware dataset prior selection to identify the most informative samples\nfor annotation, and leverages deep generative models to conditionally\nsynthesize diverse samples, thereby augmenting the training set and mitigating\nthe effects of concept drift. We evaluate our end-to-end framework \\NetGuard on\nboth simulated IDS data and a real-world ISP dataset, demonstrating significant\nimprovements in intrusion detection performance. Our method boosts the overall\nF1-score from 0.60 (without adaptation) to 0.86. Rare attacks such as\nInfiltration, Web Attack, and FTP-BruteForce, which originally achieved F1\nscores of 0.001, 0.04, and 0.00, improve to 0.30, 0.50, and 0.71, respectively,\nwith generative active adaptation in the CIC-IDS 2018 dataset. Our framework\neffectively enhances rare attack detection while reducing labeling costs,\nmaking it a scalable and practical solution for intrusion detection.\n", "link": "http://arxiv.org/abs/2503.03022v2", "date": "2025-08-13", "relevancy": 2.5467, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5173}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5079}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Active%20Adaptation%20for%20Drifting%20and%20Imbalanced%20Network%0A%20%20Intrusion%20Detection&body=Title%3A%20Generative%20Active%20Adaptation%20for%20Drifting%20and%20Imbalanced%20Network%0A%20%20Intrusion%20Detection%0AAuthor%3A%20Ragini%20Gupta%20and%20Shinan%20Liu%20and%20Ruixiao%20Zhang%20and%20Xinyue%20Hu%20and%20Xiaoyang%20Wang%20and%20Hadjer%20Benkraouda%20and%20Pranav%20Kommaraju%20and%20Nick%20Feamster%20and%20Klara%20Nahrstedt%0AAbstract%3A%20%20%20Machine%20learning%20has%20shown%20promise%20in%20network%20intrusion%20detection%20systems%2C%0Ayet%20its%20performance%20often%20degrades%20due%20to%20concept%20drift%20and%20imbalanced%20data.%0AThese%20challenges%20are%20compounded%20by%20the%20labor-intensive%20process%20of%20labeling%0Anetwork%20traffic%2C%20especially%20when%20dealing%20with%20evolving%20and%20rare%20attack%20types%2C%0Awhich%20makes%20preparing%20the%20right%20data%20for%20adaptation%20difficult.%20To%20address%20these%0Aissues%2C%20we%20propose%20a%20generative%20active%20adaptation%20framework%20that%20minimizes%0Alabeling%20effort%20while%20enhancing%20model%20robustness.%20Our%20approach%20employs%0Adensity-aware%20dataset%20prior%20selection%20to%20identify%20the%20most%20informative%20samples%0Afor%20annotation%2C%20and%20leverages%20deep%20generative%20models%20to%20conditionally%0Asynthesize%20diverse%20samples%2C%20thereby%20augmenting%20the%20training%20set%20and%20mitigating%0Athe%20effects%20of%20concept%20drift.%20We%20evaluate%20our%20end-to-end%20framework%20%5CNetGuard%20on%0Aboth%20simulated%20IDS%20data%20and%20a%20real-world%20ISP%20dataset%2C%20demonstrating%20significant%0Aimprovements%20in%20intrusion%20detection%20performance.%20Our%20method%20boosts%20the%20overall%0AF1-score%20from%200.60%20%28without%20adaptation%29%20to%200.86.%20Rare%20attacks%20such%20as%0AInfiltration%2C%20Web%20Attack%2C%20and%20FTP-BruteForce%2C%20which%20originally%20achieved%20F1%0Ascores%20of%200.001%2C%200.04%2C%20and%200.00%2C%20improve%20to%200.30%2C%200.50%2C%20and%200.71%2C%20respectively%2C%0Awith%20generative%20active%20adaptation%20in%20the%20CIC-IDS%202018%20dataset.%20Our%20framework%0Aeffectively%20enhances%20rare%20attack%20detection%20while%20reducing%20labeling%20costs%2C%0Amaking%20it%20a%20scalable%20and%20practical%20solution%20for%20intrusion%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03022v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Active%2520Adaptation%2520for%2520Drifting%2520and%2520Imbalanced%2520Network%250A%2520%2520Intrusion%2520Detection%26entry.906535625%3DRagini%2520Gupta%2520and%2520Shinan%2520Liu%2520and%2520Ruixiao%2520Zhang%2520and%2520Xinyue%2520Hu%2520and%2520Xiaoyang%2520Wang%2520and%2520Hadjer%2520Benkraouda%2520and%2520Pranav%2520Kommaraju%2520and%2520Nick%2520Feamster%2520and%2520Klara%2520Nahrstedt%26entry.1292438233%3D%2520%2520Machine%2520learning%2520has%2520shown%2520promise%2520in%2520network%2520intrusion%2520detection%2520systems%252C%250Ayet%2520its%2520performance%2520often%2520degrades%2520due%2520to%2520concept%2520drift%2520and%2520imbalanced%2520data.%250AThese%2520challenges%2520are%2520compounded%2520by%2520the%2520labor-intensive%2520process%2520of%2520labeling%250Anetwork%2520traffic%252C%2520especially%2520when%2520dealing%2520with%2520evolving%2520and%2520rare%2520attack%2520types%252C%250Awhich%2520makes%2520preparing%2520the%2520right%2520data%2520for%2520adaptation%2520difficult.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520a%2520generative%2520active%2520adaptation%2520framework%2520that%2520minimizes%250Alabeling%2520effort%2520while%2520enhancing%2520model%2520robustness.%2520Our%2520approach%2520employs%250Adensity-aware%2520dataset%2520prior%2520selection%2520to%2520identify%2520the%2520most%2520informative%2520samples%250Afor%2520annotation%252C%2520and%2520leverages%2520deep%2520generative%2520models%2520to%2520conditionally%250Asynthesize%2520diverse%2520samples%252C%2520thereby%2520augmenting%2520the%2520training%2520set%2520and%2520mitigating%250Athe%2520effects%2520of%2520concept%2520drift.%2520We%2520evaluate%2520our%2520end-to-end%2520framework%2520%255CNetGuard%2520on%250Aboth%2520simulated%2520IDS%2520data%2520and%2520a%2520real-world%2520ISP%2520dataset%252C%2520demonstrating%2520significant%250Aimprovements%2520in%2520intrusion%2520detection%2520performance.%2520Our%2520method%2520boosts%2520the%2520overall%250AF1-score%2520from%25200.60%2520%2528without%2520adaptation%2529%2520to%25200.86.%2520Rare%2520attacks%2520such%2520as%250AInfiltration%252C%2520Web%2520Attack%252C%2520and%2520FTP-BruteForce%252C%2520which%2520originally%2520achieved%2520F1%250Ascores%2520of%25200.001%252C%25200.04%252C%2520and%25200.00%252C%2520improve%2520to%25200.30%252C%25200.50%252C%2520and%25200.71%252C%2520respectively%252C%250Awith%2520generative%2520active%2520adaptation%2520in%2520the%2520CIC-IDS%25202018%2520dataset.%2520Our%2520framework%250Aeffectively%2520enhances%2520rare%2520attack%2520detection%2520while%2520reducing%2520labeling%2520costs%252C%250Amaking%2520it%2520a%2520scalable%2520and%2520practical%2520solution%2520for%2520intrusion%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03022v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Active%20Adaptation%20for%20Drifting%20and%20Imbalanced%20Network%0A%20%20Intrusion%20Detection&entry.906535625=Ragini%20Gupta%20and%20Shinan%20Liu%20and%20Ruixiao%20Zhang%20and%20Xinyue%20Hu%20and%20Xiaoyang%20Wang%20and%20Hadjer%20Benkraouda%20and%20Pranav%20Kommaraju%20and%20Nick%20Feamster%20and%20Klara%20Nahrstedt&entry.1292438233=%20%20Machine%20learning%20has%20shown%20promise%20in%20network%20intrusion%20detection%20systems%2C%0Ayet%20its%20performance%20often%20degrades%20due%20to%20concept%20drift%20and%20imbalanced%20data.%0AThese%20challenges%20are%20compounded%20by%20the%20labor-intensive%20process%20of%20labeling%0Anetwork%20traffic%2C%20especially%20when%20dealing%20with%20evolving%20and%20rare%20attack%20types%2C%0Awhich%20makes%20preparing%20the%20right%20data%20for%20adaptation%20difficult.%20To%20address%20these%0Aissues%2C%20we%20propose%20a%20generative%20active%20adaptation%20framework%20that%20minimizes%0Alabeling%20effort%20while%20enhancing%20model%20robustness.%20Our%20approach%20employs%0Adensity-aware%20dataset%20prior%20selection%20to%20identify%20the%20most%20informative%20samples%0Afor%20annotation%2C%20and%20leverages%20deep%20generative%20models%20to%20conditionally%0Asynthesize%20diverse%20samples%2C%20thereby%20augmenting%20the%20training%20set%20and%20mitigating%0Athe%20effects%20of%20concept%20drift.%20We%20evaluate%20our%20end-to-end%20framework%20%5CNetGuard%20on%0Aboth%20simulated%20IDS%20data%20and%20a%20real-world%20ISP%20dataset%2C%20demonstrating%20significant%0Aimprovements%20in%20intrusion%20detection%20performance.%20Our%20method%20boosts%20the%20overall%0AF1-score%20from%200.60%20%28without%20adaptation%29%20to%200.86.%20Rare%20attacks%20such%20as%0AInfiltration%2C%20Web%20Attack%2C%20and%20FTP-BruteForce%2C%20which%20originally%20achieved%20F1%0Ascores%20of%200.001%2C%200.04%2C%20and%200.00%2C%20improve%20to%200.30%2C%200.50%2C%20and%200.71%2C%20respectively%2C%0Awith%20generative%20active%20adaptation%20in%20the%20CIC-IDS%202018%20dataset.%20Our%20framework%0Aeffectively%20enhances%20rare%20attack%20detection%20while%20reducing%20labeling%20costs%2C%0Amaking%20it%20a%20scalable%20and%20practical%20solution%20for%20intrusion%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03022v2&entry.124074799=Read"},
{"title": "Towards Affordance-Aware Robotic Dexterous Grasping with Human-like\n  Priors", "author": "Haoyu Zhao and Linghao Zhuang and Xingyue Zhao and Cheng Zeng and Haoran Xu and Yuming Jiang and Jun Cen and Kexiang Wang and Jiayan Guo and Siteng Huang and Xin Li and Deli Zhao and Hua Zou", "abstract": "  A dexterous hand capable of generalizable grasping objects is fundamental for\nthe development of general-purpose embodied AI. However, previous methods focus\nnarrowly on low-level grasp stability metrics, neglecting affordance-aware\npositioning and human-like poses which are crucial for downstream manipulation.\nTo address these limitations, we propose AffordDex, a novel framework with\ntwo-stage training that learns a universal grasping policy with an inherent\nunderstanding of both motion priors and object affordances. In the first stage,\na trajectory imitator is pre-trained on a large corpus of human hand motions to\ninstill a strong prior for natural movement. In the second stage, a residual\nmodule is trained to adapt these general human-like motions to specific object\ninstances. This refinement is critically guided by two components: our Negative\nAffordance-aware Segmentation (NAA) module, which identifies functionally\ninappropriate contact regions, and a privileged teacher-student distillation\nprocess that ensures the final vision-based policy is highly successful.\nExtensive experiments demonstrate that AffordDex not only achieves universal\ndexterous grasping but also remains remarkably human-like in posture and\nfunctionally appropriate in contact location. As a result, AffordDex\nsignificantly outperforms state-of-the-art baselines across seen objects,\nunseen instances, and even entirely novel categories.\n", "link": "http://arxiv.org/abs/2508.08896v2", "date": "2025-08-13", "relevancy": 2.5372, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6917}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5954}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Affordance-Aware%20Robotic%20Dexterous%20Grasping%20with%20Human-like%0A%20%20Priors&body=Title%3A%20Towards%20Affordance-Aware%20Robotic%20Dexterous%20Grasping%20with%20Human-like%0A%20%20Priors%0AAuthor%3A%20Haoyu%20Zhao%20and%20Linghao%20Zhuang%20and%20Xingyue%20Zhao%20and%20Cheng%20Zeng%20and%20Haoran%20Xu%20and%20Yuming%20Jiang%20and%20Jun%20Cen%20and%20Kexiang%20Wang%20and%20Jiayan%20Guo%20and%20Siteng%20Huang%20and%20Xin%20Li%20and%20Deli%20Zhao%20and%20Hua%20Zou%0AAbstract%3A%20%20%20A%20dexterous%20hand%20capable%20of%20generalizable%20grasping%20objects%20is%20fundamental%20for%0Athe%20development%20of%20general-purpose%20embodied%20AI.%20However%2C%20previous%20methods%20focus%0Anarrowly%20on%20low-level%20grasp%20stability%20metrics%2C%20neglecting%20affordance-aware%0Apositioning%20and%20human-like%20poses%20which%20are%20crucial%20for%20downstream%20manipulation.%0ATo%20address%20these%20limitations%2C%20we%20propose%20AffordDex%2C%20a%20novel%20framework%20with%0Atwo-stage%20training%20that%20learns%20a%20universal%20grasping%20policy%20with%20an%20inherent%0Aunderstanding%20of%20both%20motion%20priors%20and%20object%20affordances.%20In%20the%20first%20stage%2C%0Aa%20trajectory%20imitator%20is%20pre-trained%20on%20a%20large%20corpus%20of%20human%20hand%20motions%20to%0Ainstill%20a%20strong%20prior%20for%20natural%20movement.%20In%20the%20second%20stage%2C%20a%20residual%0Amodule%20is%20trained%20to%20adapt%20these%20general%20human-like%20motions%20to%20specific%20object%0Ainstances.%20This%20refinement%20is%20critically%20guided%20by%20two%20components%3A%20our%20Negative%0AAffordance-aware%20Segmentation%20%28NAA%29%20module%2C%20which%20identifies%20functionally%0Ainappropriate%20contact%20regions%2C%20and%20a%20privileged%20teacher-student%20distillation%0Aprocess%20that%20ensures%20the%20final%20vision-based%20policy%20is%20highly%20successful.%0AExtensive%20experiments%20demonstrate%20that%20AffordDex%20not%20only%20achieves%20universal%0Adexterous%20grasping%20but%20also%20remains%20remarkably%20human-like%20in%20posture%20and%0Afunctionally%20appropriate%20in%20contact%20location.%20As%20a%20result%2C%20AffordDex%0Asignificantly%20outperforms%20state-of-the-art%20baselines%20across%20seen%20objects%2C%0Aunseen%20instances%2C%20and%20even%20entirely%20novel%20categories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08896v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Affordance-Aware%2520Robotic%2520Dexterous%2520Grasping%2520with%2520Human-like%250A%2520%2520Priors%26entry.906535625%3DHaoyu%2520Zhao%2520and%2520Linghao%2520Zhuang%2520and%2520Xingyue%2520Zhao%2520and%2520Cheng%2520Zeng%2520and%2520Haoran%2520Xu%2520and%2520Yuming%2520Jiang%2520and%2520Jun%2520Cen%2520and%2520Kexiang%2520Wang%2520and%2520Jiayan%2520Guo%2520and%2520Siteng%2520Huang%2520and%2520Xin%2520Li%2520and%2520Deli%2520Zhao%2520and%2520Hua%2520Zou%26entry.1292438233%3D%2520%2520A%2520dexterous%2520hand%2520capable%2520of%2520generalizable%2520grasping%2520objects%2520is%2520fundamental%2520for%250Athe%2520development%2520of%2520general-purpose%2520embodied%2520AI.%2520However%252C%2520previous%2520methods%2520focus%250Anarrowly%2520on%2520low-level%2520grasp%2520stability%2520metrics%252C%2520neglecting%2520affordance-aware%250Apositioning%2520and%2520human-like%2520poses%2520which%2520are%2520crucial%2520for%2520downstream%2520manipulation.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520AffordDex%252C%2520a%2520novel%2520framework%2520with%250Atwo-stage%2520training%2520that%2520learns%2520a%2520universal%2520grasping%2520policy%2520with%2520an%2520inherent%250Aunderstanding%2520of%2520both%2520motion%2520priors%2520and%2520object%2520affordances.%2520In%2520the%2520first%2520stage%252C%250Aa%2520trajectory%2520imitator%2520is%2520pre-trained%2520on%2520a%2520large%2520corpus%2520of%2520human%2520hand%2520motions%2520to%250Ainstill%2520a%2520strong%2520prior%2520for%2520natural%2520movement.%2520In%2520the%2520second%2520stage%252C%2520a%2520residual%250Amodule%2520is%2520trained%2520to%2520adapt%2520these%2520general%2520human-like%2520motions%2520to%2520specific%2520object%250Ainstances.%2520This%2520refinement%2520is%2520critically%2520guided%2520by%2520two%2520components%253A%2520our%2520Negative%250AAffordance-aware%2520Segmentation%2520%2528NAA%2529%2520module%252C%2520which%2520identifies%2520functionally%250Ainappropriate%2520contact%2520regions%252C%2520and%2520a%2520privileged%2520teacher-student%2520distillation%250Aprocess%2520that%2520ensures%2520the%2520final%2520vision-based%2520policy%2520is%2520highly%2520successful.%250AExtensive%2520experiments%2520demonstrate%2520that%2520AffordDex%2520not%2520only%2520achieves%2520universal%250Adexterous%2520grasping%2520but%2520also%2520remains%2520remarkably%2520human-like%2520in%2520posture%2520and%250Afunctionally%2520appropriate%2520in%2520contact%2520location.%2520As%2520a%2520result%252C%2520AffordDex%250Asignificantly%2520outperforms%2520state-of-the-art%2520baselines%2520across%2520seen%2520objects%252C%250Aunseen%2520instances%252C%2520and%2520even%2520entirely%2520novel%2520categories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08896v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Affordance-Aware%20Robotic%20Dexterous%20Grasping%20with%20Human-like%0A%20%20Priors&entry.906535625=Haoyu%20Zhao%20and%20Linghao%20Zhuang%20and%20Xingyue%20Zhao%20and%20Cheng%20Zeng%20and%20Haoran%20Xu%20and%20Yuming%20Jiang%20and%20Jun%20Cen%20and%20Kexiang%20Wang%20and%20Jiayan%20Guo%20and%20Siteng%20Huang%20and%20Xin%20Li%20and%20Deli%20Zhao%20and%20Hua%20Zou&entry.1292438233=%20%20A%20dexterous%20hand%20capable%20of%20generalizable%20grasping%20objects%20is%20fundamental%20for%0Athe%20development%20of%20general-purpose%20embodied%20AI.%20However%2C%20previous%20methods%20focus%0Anarrowly%20on%20low-level%20grasp%20stability%20metrics%2C%20neglecting%20affordance-aware%0Apositioning%20and%20human-like%20poses%20which%20are%20crucial%20for%20downstream%20manipulation.%0ATo%20address%20these%20limitations%2C%20we%20propose%20AffordDex%2C%20a%20novel%20framework%20with%0Atwo-stage%20training%20that%20learns%20a%20universal%20grasping%20policy%20with%20an%20inherent%0Aunderstanding%20of%20both%20motion%20priors%20and%20object%20affordances.%20In%20the%20first%20stage%2C%0Aa%20trajectory%20imitator%20is%20pre-trained%20on%20a%20large%20corpus%20of%20human%20hand%20motions%20to%0Ainstill%20a%20strong%20prior%20for%20natural%20movement.%20In%20the%20second%20stage%2C%20a%20residual%0Amodule%20is%20trained%20to%20adapt%20these%20general%20human-like%20motions%20to%20specific%20object%0Ainstances.%20This%20refinement%20is%20critically%20guided%20by%20two%20components%3A%20our%20Negative%0AAffordance-aware%20Segmentation%20%28NAA%29%20module%2C%20which%20identifies%20functionally%0Ainappropriate%20contact%20regions%2C%20and%20a%20privileged%20teacher-student%20distillation%0Aprocess%20that%20ensures%20the%20final%20vision-based%20policy%20is%20highly%20successful.%0AExtensive%20experiments%20demonstrate%20that%20AffordDex%20not%20only%20achieves%20universal%0Adexterous%20grasping%20but%20also%20remains%20remarkably%20human-like%20in%20posture%20and%0Afunctionally%20appropriate%20in%20contact%20location.%20As%20a%20result%2C%20AffordDex%0Asignificantly%20outperforms%20state-of-the-art%20baselines%20across%20seen%20objects%2C%0Aunseen%20instances%2C%20and%20even%20entirely%20novel%20categories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08896v2&entry.124074799=Read"},
{"title": "Are you Struggling? Dataset and Baselines for Struggle Determination in\n  Assembly Videos", "author": "Shijia Feng and Michael Wray and Brian Sullivan and Youngkyoon Jang and Casimir Ludwig and Iain Gilchrist and Walterio Mayol-Cuevas", "abstract": "  Determining when people are struggling allows for a finer-grained\nunderstanding of actions that complements conventional action classification\nand error detection. Struggle detection, as defined in this paper, is a\ndistinct and important task that can be identified without explicit step or\nactivity knowledge. We introduce the first struggle dataset with three\nreal-world problem-solving activities that are labelled by both expert and\ncrowd-source annotators. Video segments were scored w.r.t. their level of\nstruggle using a forced choice 4-point scale. This dataset contains 5.1 hours\nof video from 73 participants. We conducted a series of experiments to identify\nthe most suitable modelling approaches for struggle determination.\nAdditionally, we compared various deep learning models, establishing baseline\nresults for struggle classification, struggle regression, and struggle label\ndistribution learning. Our results indicate that struggle detection in video\ncan achieve up to $88.24\\%$ accuracy in binary classification, while detecting\nthe level of struggle in a four-way classification setting performs lower, with\nan overall accuracy of $52.45\\%$. Our work is motivated toward a more\ncomprehensive understanding of action in video and potentially the improvement\nof assistive systems that analyse struggle and can better support users during\nmanual activities.\n", "link": "http://arxiv.org/abs/2402.11057v5", "date": "2025-08-13", "relevancy": 2.5302, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20you%20Struggling%3F%20Dataset%20and%20Baselines%20for%20Struggle%20Determination%20in%0A%20%20Assembly%20Videos&body=Title%3A%20Are%20you%20Struggling%3F%20Dataset%20and%20Baselines%20for%20Struggle%20Determination%20in%0A%20%20Assembly%20Videos%0AAuthor%3A%20Shijia%20Feng%20and%20Michael%20Wray%20and%20Brian%20Sullivan%20and%20Youngkyoon%20Jang%20and%20Casimir%20Ludwig%20and%20Iain%20Gilchrist%20and%20Walterio%20Mayol-Cuevas%0AAbstract%3A%20%20%20Determining%20when%20people%20are%20struggling%20allows%20for%20a%20finer-grained%0Aunderstanding%20of%20actions%20that%20complements%20conventional%20action%20classification%0Aand%20error%20detection.%20Struggle%20detection%2C%20as%20defined%20in%20this%20paper%2C%20is%20a%0Adistinct%20and%20important%20task%20that%20can%20be%20identified%20without%20explicit%20step%20or%0Aactivity%20knowledge.%20We%20introduce%20the%20first%20struggle%20dataset%20with%20three%0Areal-world%20problem-solving%20activities%20that%20are%20labelled%20by%20both%20expert%20and%0Acrowd-source%20annotators.%20Video%20segments%20were%20scored%20w.r.t.%20their%20level%20of%0Astruggle%20using%20a%20forced%20choice%204-point%20scale.%20This%20dataset%20contains%205.1%20hours%0Aof%20video%20from%2073%20participants.%20We%20conducted%20a%20series%20of%20experiments%20to%20identify%0Athe%20most%20suitable%20modelling%20approaches%20for%20struggle%20determination.%0AAdditionally%2C%20we%20compared%20various%20deep%20learning%20models%2C%20establishing%20baseline%0Aresults%20for%20struggle%20classification%2C%20struggle%20regression%2C%20and%20struggle%20label%0Adistribution%20learning.%20Our%20results%20indicate%20that%20struggle%20detection%20in%20video%0Acan%20achieve%20up%20to%20%2488.24%5C%25%24%20accuracy%20in%20binary%20classification%2C%20while%20detecting%0Athe%20level%20of%20struggle%20in%20a%20four-way%20classification%20setting%20performs%20lower%2C%20with%0Aan%20overall%20accuracy%20of%20%2452.45%5C%25%24.%20Our%20work%20is%20motivated%20toward%20a%20more%0Acomprehensive%20understanding%20of%20action%20in%20video%20and%20potentially%20the%20improvement%0Aof%20assistive%20systems%20that%20analyse%20struggle%20and%20can%20better%20support%20users%20during%0Amanual%20activities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11057v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520you%2520Struggling%253F%2520Dataset%2520and%2520Baselines%2520for%2520Struggle%2520Determination%2520in%250A%2520%2520Assembly%2520Videos%26entry.906535625%3DShijia%2520Feng%2520and%2520Michael%2520Wray%2520and%2520Brian%2520Sullivan%2520and%2520Youngkyoon%2520Jang%2520and%2520Casimir%2520Ludwig%2520and%2520Iain%2520Gilchrist%2520and%2520Walterio%2520Mayol-Cuevas%26entry.1292438233%3D%2520%2520Determining%2520when%2520people%2520are%2520struggling%2520allows%2520for%2520a%2520finer-grained%250Aunderstanding%2520of%2520actions%2520that%2520complements%2520conventional%2520action%2520classification%250Aand%2520error%2520detection.%2520Struggle%2520detection%252C%2520as%2520defined%2520in%2520this%2520paper%252C%2520is%2520a%250Adistinct%2520and%2520important%2520task%2520that%2520can%2520be%2520identified%2520without%2520explicit%2520step%2520or%250Aactivity%2520knowledge.%2520We%2520introduce%2520the%2520first%2520struggle%2520dataset%2520with%2520three%250Areal-world%2520problem-solving%2520activities%2520that%2520are%2520labelled%2520by%2520both%2520expert%2520and%250Acrowd-source%2520annotators.%2520Video%2520segments%2520were%2520scored%2520w.r.t.%2520their%2520level%2520of%250Astruggle%2520using%2520a%2520forced%2520choice%25204-point%2520scale.%2520This%2520dataset%2520contains%25205.1%2520hours%250Aof%2520video%2520from%252073%2520participants.%2520We%2520conducted%2520a%2520series%2520of%2520experiments%2520to%2520identify%250Athe%2520most%2520suitable%2520modelling%2520approaches%2520for%2520struggle%2520determination.%250AAdditionally%252C%2520we%2520compared%2520various%2520deep%2520learning%2520models%252C%2520establishing%2520baseline%250Aresults%2520for%2520struggle%2520classification%252C%2520struggle%2520regression%252C%2520and%2520struggle%2520label%250Adistribution%2520learning.%2520Our%2520results%2520indicate%2520that%2520struggle%2520detection%2520in%2520video%250Acan%2520achieve%2520up%2520to%2520%252488.24%255C%2525%2524%2520accuracy%2520in%2520binary%2520classification%252C%2520while%2520detecting%250Athe%2520level%2520of%2520struggle%2520in%2520a%2520four-way%2520classification%2520setting%2520performs%2520lower%252C%2520with%250Aan%2520overall%2520accuracy%2520of%2520%252452.45%255C%2525%2524.%2520Our%2520work%2520is%2520motivated%2520toward%2520a%2520more%250Acomprehensive%2520understanding%2520of%2520action%2520in%2520video%2520and%2520potentially%2520the%2520improvement%250Aof%2520assistive%2520systems%2520that%2520analyse%2520struggle%2520and%2520can%2520better%2520support%2520users%2520during%250Amanual%2520activities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11057v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20you%20Struggling%3F%20Dataset%20and%20Baselines%20for%20Struggle%20Determination%20in%0A%20%20Assembly%20Videos&entry.906535625=Shijia%20Feng%20and%20Michael%20Wray%20and%20Brian%20Sullivan%20and%20Youngkyoon%20Jang%20and%20Casimir%20Ludwig%20and%20Iain%20Gilchrist%20and%20Walterio%20Mayol-Cuevas&entry.1292438233=%20%20Determining%20when%20people%20are%20struggling%20allows%20for%20a%20finer-grained%0Aunderstanding%20of%20actions%20that%20complements%20conventional%20action%20classification%0Aand%20error%20detection.%20Struggle%20detection%2C%20as%20defined%20in%20this%20paper%2C%20is%20a%0Adistinct%20and%20important%20task%20that%20can%20be%20identified%20without%20explicit%20step%20or%0Aactivity%20knowledge.%20We%20introduce%20the%20first%20struggle%20dataset%20with%20three%0Areal-world%20problem-solving%20activities%20that%20are%20labelled%20by%20both%20expert%20and%0Acrowd-source%20annotators.%20Video%20segments%20were%20scored%20w.r.t.%20their%20level%20of%0Astruggle%20using%20a%20forced%20choice%204-point%20scale.%20This%20dataset%20contains%205.1%20hours%0Aof%20video%20from%2073%20participants.%20We%20conducted%20a%20series%20of%20experiments%20to%20identify%0Athe%20most%20suitable%20modelling%20approaches%20for%20struggle%20determination.%0AAdditionally%2C%20we%20compared%20various%20deep%20learning%20models%2C%20establishing%20baseline%0Aresults%20for%20struggle%20classification%2C%20struggle%20regression%2C%20and%20struggle%20label%0Adistribution%20learning.%20Our%20results%20indicate%20that%20struggle%20detection%20in%20video%0Acan%20achieve%20up%20to%20%2488.24%5C%25%24%20accuracy%20in%20binary%20classification%2C%20while%20detecting%0Athe%20level%20of%20struggle%20in%20a%20four-way%20classification%20setting%20performs%20lower%2C%20with%0Aan%20overall%20accuracy%20of%20%2452.45%5C%25%24.%20Our%20work%20is%20motivated%20toward%20a%20more%0Acomprehensive%20understanding%20of%20action%20in%20video%20and%20potentially%20the%20improvement%0Aof%20assistive%20systems%20that%20analyse%20struggle%20and%20can%20better%20support%20users%20during%0Amanual%20activities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11057v5&entry.124074799=Read"},
{"title": "CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce\n  High-Resolution Time-Varying Data", "author": "Chongke Bi and Xin Gao and Jiangkang Deng and Guan Li and Jun Han", "abstract": "  Large-scale scientific simulations require significant resources to generate\nhigh-resolution time-varying data (TVD). While super-resolution is an efficient\npost-processing strategy to reduce costs, existing methods rely on a large\namount of HR training data, limiting their applicability to diverse simulation\nscenarios. To address this constraint, we proposed CD-TVD, a novel framework\nthat combines contrastive learning and an improved diffusion-based\nsuper-resolution model to achieve accurate 3D super-resolution from limited\ntime-step high-resolution data. During pre-training on historical simulation\ndata, the contrastive encoder and diffusion superresolution modules learn\ndegradation patterns and detailed features of high-resolution and\nlow-resolution samples. In the training phase, the improved diffusion model\nwith a local attention mechanism is fine-tuned using only one newly generated\nhigh-resolution timestep, leveraging the degradation knowledge learned by the\nencoder. This design minimizes the reliance on large-scale high-resolution\ndatasets while maintaining the capability to recover fine-grained details.\nExperimental results on fluid and atmospheric simulation datasets confirm that\nCD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a\nsignificant advancement in data augmentation for large-scale scientific\nsimulations. The code is available at\nhttps://github.com/Xin-Gao-private/CD-TVD.\n", "link": "http://arxiv.org/abs/2508.08173v2", "date": "2025-08-13", "relevancy": 2.5225, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6327}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6327}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.62}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CD-TVD%3A%20Contrastive%20Diffusion%20for%203D%20Super-Resolution%20with%20Scarce%0A%20%20High-Resolution%20Time-Varying%20Data&body=Title%3A%20CD-TVD%3A%20Contrastive%20Diffusion%20for%203D%20Super-Resolution%20with%20Scarce%0A%20%20High-Resolution%20Time-Varying%20Data%0AAuthor%3A%20Chongke%20Bi%20and%20Xin%20Gao%20and%20Jiangkang%20Deng%20and%20Guan%20Li%20and%20Jun%20Han%0AAbstract%3A%20%20%20Large-scale%20scientific%20simulations%20require%20significant%20resources%20to%20generate%0Ahigh-resolution%20time-varying%20data%20%28TVD%29.%20While%20super-resolution%20is%20an%20efficient%0Apost-processing%20strategy%20to%20reduce%20costs%2C%20existing%20methods%20rely%20on%20a%20large%0Aamount%20of%20HR%20training%20data%2C%20limiting%20their%20applicability%20to%20diverse%20simulation%0Ascenarios.%20To%20address%20this%20constraint%2C%20we%20proposed%20CD-TVD%2C%20a%20novel%20framework%0Athat%20combines%20contrastive%20learning%20and%20an%20improved%20diffusion-based%0Asuper-resolution%20model%20to%20achieve%20accurate%203D%20super-resolution%20from%20limited%0Atime-step%20high-resolution%20data.%20During%20pre-training%20on%20historical%20simulation%0Adata%2C%20the%20contrastive%20encoder%20and%20diffusion%20superresolution%20modules%20learn%0Adegradation%20patterns%20and%20detailed%20features%20of%20high-resolution%20and%0Alow-resolution%20samples.%20In%20the%20training%20phase%2C%20the%20improved%20diffusion%20model%0Awith%20a%20local%20attention%20mechanism%20is%20fine-tuned%20using%20only%20one%20newly%20generated%0Ahigh-resolution%20timestep%2C%20leveraging%20the%20degradation%20knowledge%20learned%20by%20the%0Aencoder.%20This%20design%20minimizes%20the%20reliance%20on%20large-scale%20high-resolution%0Adatasets%20while%20maintaining%20the%20capability%20to%20recover%20fine-grained%20details.%0AExperimental%20results%20on%20fluid%20and%20atmospheric%20simulation%20datasets%20confirm%20that%0ACD-TVD%20delivers%20accurate%20and%20resource-efficient%203D%20super-resolution%2C%20marking%20a%0Asignificant%20advancement%20in%20data%20augmentation%20for%20large-scale%20scientific%0Asimulations.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Xin-Gao-private/CD-TVD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08173v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCD-TVD%253A%2520Contrastive%2520Diffusion%2520for%25203D%2520Super-Resolution%2520with%2520Scarce%250A%2520%2520High-Resolution%2520Time-Varying%2520Data%26entry.906535625%3DChongke%2520Bi%2520and%2520Xin%2520Gao%2520and%2520Jiangkang%2520Deng%2520and%2520Guan%2520Li%2520and%2520Jun%2520Han%26entry.1292438233%3D%2520%2520Large-scale%2520scientific%2520simulations%2520require%2520significant%2520resources%2520to%2520generate%250Ahigh-resolution%2520time-varying%2520data%2520%2528TVD%2529.%2520While%2520super-resolution%2520is%2520an%2520efficient%250Apost-processing%2520strategy%2520to%2520reduce%2520costs%252C%2520existing%2520methods%2520rely%2520on%2520a%2520large%250Aamount%2520of%2520HR%2520training%2520data%252C%2520limiting%2520their%2520applicability%2520to%2520diverse%2520simulation%250Ascenarios.%2520To%2520address%2520this%2520constraint%252C%2520we%2520proposed%2520CD-TVD%252C%2520a%2520novel%2520framework%250Athat%2520combines%2520contrastive%2520learning%2520and%2520an%2520improved%2520diffusion-based%250Asuper-resolution%2520model%2520to%2520achieve%2520accurate%25203D%2520super-resolution%2520from%2520limited%250Atime-step%2520high-resolution%2520data.%2520During%2520pre-training%2520on%2520historical%2520simulation%250Adata%252C%2520the%2520contrastive%2520encoder%2520and%2520diffusion%2520superresolution%2520modules%2520learn%250Adegradation%2520patterns%2520and%2520detailed%2520features%2520of%2520high-resolution%2520and%250Alow-resolution%2520samples.%2520In%2520the%2520training%2520phase%252C%2520the%2520improved%2520diffusion%2520model%250Awith%2520a%2520local%2520attention%2520mechanism%2520is%2520fine-tuned%2520using%2520only%2520one%2520newly%2520generated%250Ahigh-resolution%2520timestep%252C%2520leveraging%2520the%2520degradation%2520knowledge%2520learned%2520by%2520the%250Aencoder.%2520This%2520design%2520minimizes%2520the%2520reliance%2520on%2520large-scale%2520high-resolution%250Adatasets%2520while%2520maintaining%2520the%2520capability%2520to%2520recover%2520fine-grained%2520details.%250AExperimental%2520results%2520on%2520fluid%2520and%2520atmospheric%2520simulation%2520datasets%2520confirm%2520that%250ACD-TVD%2520delivers%2520accurate%2520and%2520resource-efficient%25203D%2520super-resolution%252C%2520marking%2520a%250Asignificant%2520advancement%2520in%2520data%2520augmentation%2520for%2520large-scale%2520scientific%250Asimulations.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Xin-Gao-private/CD-TVD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08173v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CD-TVD%3A%20Contrastive%20Diffusion%20for%203D%20Super-Resolution%20with%20Scarce%0A%20%20High-Resolution%20Time-Varying%20Data&entry.906535625=Chongke%20Bi%20and%20Xin%20Gao%20and%20Jiangkang%20Deng%20and%20Guan%20Li%20and%20Jun%20Han&entry.1292438233=%20%20Large-scale%20scientific%20simulations%20require%20significant%20resources%20to%20generate%0Ahigh-resolution%20time-varying%20data%20%28TVD%29.%20While%20super-resolution%20is%20an%20efficient%0Apost-processing%20strategy%20to%20reduce%20costs%2C%20existing%20methods%20rely%20on%20a%20large%0Aamount%20of%20HR%20training%20data%2C%20limiting%20their%20applicability%20to%20diverse%20simulation%0Ascenarios.%20To%20address%20this%20constraint%2C%20we%20proposed%20CD-TVD%2C%20a%20novel%20framework%0Athat%20combines%20contrastive%20learning%20and%20an%20improved%20diffusion-based%0Asuper-resolution%20model%20to%20achieve%20accurate%203D%20super-resolution%20from%20limited%0Atime-step%20high-resolution%20data.%20During%20pre-training%20on%20historical%20simulation%0Adata%2C%20the%20contrastive%20encoder%20and%20diffusion%20superresolution%20modules%20learn%0Adegradation%20patterns%20and%20detailed%20features%20of%20high-resolution%20and%0Alow-resolution%20samples.%20In%20the%20training%20phase%2C%20the%20improved%20diffusion%20model%0Awith%20a%20local%20attention%20mechanism%20is%20fine-tuned%20using%20only%20one%20newly%20generated%0Ahigh-resolution%20timestep%2C%20leveraging%20the%20degradation%20knowledge%20learned%20by%20the%0Aencoder.%20This%20design%20minimizes%20the%20reliance%20on%20large-scale%20high-resolution%0Adatasets%20while%20maintaining%20the%20capability%20to%20recover%20fine-grained%20details.%0AExperimental%20results%20on%20fluid%20and%20atmospheric%20simulation%20datasets%20confirm%20that%0ACD-TVD%20delivers%20accurate%20and%20resource-efficient%203D%20super-resolution%2C%20marking%20a%0Asignificant%20advancement%20in%20data%20augmentation%20for%20large-scale%20scientific%0Asimulations.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Xin-Gao-private/CD-TVD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08173v2&entry.124074799=Read"},
{"title": "MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image\n  Classification", "author": "Tianqi Xiang and Yi Li and Qixiang Zhang and Xiaomeng Li", "abstract": "  Recent advances in histopathology vision-language foundation models (VLFMs)\nhave shown promise in addressing data scarcity for whole slide image (WSI)\nclassification via zero-shot adaptation. However, these methods remain\noutperformed by conventional multiple instance learning (MIL) approaches\ntrained on large datasets, motivating recent efforts to enhance VLFM-based WSI\nclassification through fewshot learning paradigms. While existing few-shot\nmethods improve diagnostic accuracy with limited annotations, their reliance on\nconventional classifier designs introduces critical vulnerabilities to data\nscarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)\ncomprising two core components: (1) a meta-learner that automatically optimizes\na classifier configuration from a mixture of candidate classifiers and (2) a\nclassifier bank housing diverse candidate classifiers to enable a holistic\npathological interpretation. Extensive experiments demonstrate that MOC\noutperforms prior arts in multiple few-shot benchmarks. Notably, on the\nTCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art\nfew-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,\noffering a critical advancement for clinical deployments where diagnostic\ntraining data is severely limited. Code is available at\nhttps://github.com/xmed-lab/MOC.\n", "link": "http://arxiv.org/abs/2508.09967v1", "date": "2025-08-13", "relevancy": 2.519, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5437}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4897}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOC%3A%20Meta-Optimized%20Classifier%20for%20Few-Shot%20Whole%20Slide%20Image%0A%20%20Classification&body=Title%3A%20MOC%3A%20Meta-Optimized%20Classifier%20for%20Few-Shot%20Whole%20Slide%20Image%0A%20%20Classification%0AAuthor%3A%20Tianqi%20Xiang%20and%20Yi%20Li%20and%20Qixiang%20Zhang%20and%20Xiaomeng%20Li%0AAbstract%3A%20%20%20Recent%20advances%20in%20histopathology%20vision-language%20foundation%20models%20%28VLFMs%29%0Ahave%20shown%20promise%20in%20addressing%20data%20scarcity%20for%20whole%20slide%20image%20%28WSI%29%0Aclassification%20via%20zero-shot%20adaptation.%20However%2C%20these%20methods%20remain%0Aoutperformed%20by%20conventional%20multiple%20instance%20learning%20%28MIL%29%20approaches%0Atrained%20on%20large%20datasets%2C%20motivating%20recent%20efforts%20to%20enhance%20VLFM-based%20WSI%0Aclassification%20through%20fewshot%20learning%20paradigms.%20While%20existing%20few-shot%0Amethods%20improve%20diagnostic%20accuracy%20with%20limited%20annotations%2C%20their%20reliance%20on%0Aconventional%20classifier%20designs%20introduces%20critical%20vulnerabilities%20to%20data%0Ascarcity.%20To%20address%20this%20problem%2C%20we%20propose%20a%20Meta-Optimized%20Classifier%20%28MOC%29%0Acomprising%20two%20core%20components%3A%20%281%29%20a%20meta-learner%20that%20automatically%20optimizes%0Aa%20classifier%20configuration%20from%20a%20mixture%20of%20candidate%20classifiers%20and%20%282%29%20a%0Aclassifier%20bank%20housing%20diverse%20candidate%20classifiers%20to%20enable%20a%20holistic%0Apathological%20interpretation.%20Extensive%20experiments%20demonstrate%20that%20MOC%0Aoutperforms%20prior%20arts%20in%20multiple%20few-shot%20benchmarks.%20Notably%2C%20on%20the%0ATCGA-NSCLC%20benchmark%2C%20MOC%20improves%20AUC%20by%2010.4%25%20over%20the%20state-of-the-art%0Afew-shot%20VLFM-based%20methods%2C%20with%20gains%20up%20to%2026.25%25%20under%201-shot%20conditions%2C%0Aoffering%20a%20critical%20advancement%20for%20clinical%20deployments%20where%20diagnostic%0Atraining%20data%20is%20severely%20limited.%20Code%20is%20available%20at%0Ahttps%3A//github.com/xmed-lab/MOC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOC%253A%2520Meta-Optimized%2520Classifier%2520for%2520Few-Shot%2520Whole%2520Slide%2520Image%250A%2520%2520Classification%26entry.906535625%3DTianqi%2520Xiang%2520and%2520Yi%2520Li%2520and%2520Qixiang%2520Zhang%2520and%2520Xiaomeng%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520histopathology%2520vision-language%2520foundation%2520models%2520%2528VLFMs%2529%250Ahave%2520shown%2520promise%2520in%2520addressing%2520data%2520scarcity%2520for%2520whole%2520slide%2520image%2520%2528WSI%2529%250Aclassification%2520via%2520zero-shot%2520adaptation.%2520However%252C%2520these%2520methods%2520remain%250Aoutperformed%2520by%2520conventional%2520multiple%2520instance%2520learning%2520%2528MIL%2529%2520approaches%250Atrained%2520on%2520large%2520datasets%252C%2520motivating%2520recent%2520efforts%2520to%2520enhance%2520VLFM-based%2520WSI%250Aclassification%2520through%2520fewshot%2520learning%2520paradigms.%2520While%2520existing%2520few-shot%250Amethods%2520improve%2520diagnostic%2520accuracy%2520with%2520limited%2520annotations%252C%2520their%2520reliance%2520on%250Aconventional%2520classifier%2520designs%2520introduces%2520critical%2520vulnerabilities%2520to%2520data%250Ascarcity.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520Meta-Optimized%2520Classifier%2520%2528MOC%2529%250Acomprising%2520two%2520core%2520components%253A%2520%25281%2529%2520a%2520meta-learner%2520that%2520automatically%2520optimizes%250Aa%2520classifier%2520configuration%2520from%2520a%2520mixture%2520of%2520candidate%2520classifiers%2520and%2520%25282%2529%2520a%250Aclassifier%2520bank%2520housing%2520diverse%2520candidate%2520classifiers%2520to%2520enable%2520a%2520holistic%250Apathological%2520interpretation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MOC%250Aoutperforms%2520prior%2520arts%2520in%2520multiple%2520few-shot%2520benchmarks.%2520Notably%252C%2520on%2520the%250ATCGA-NSCLC%2520benchmark%252C%2520MOC%2520improves%2520AUC%2520by%252010.4%2525%2520over%2520the%2520state-of-the-art%250Afew-shot%2520VLFM-based%2520methods%252C%2520with%2520gains%2520up%2520to%252026.25%2525%2520under%25201-shot%2520conditions%252C%250Aoffering%2520a%2520critical%2520advancement%2520for%2520clinical%2520deployments%2520where%2520diagnostic%250Atraining%2520data%2520is%2520severely%2520limited.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/xmed-lab/MOC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOC%3A%20Meta-Optimized%20Classifier%20for%20Few-Shot%20Whole%20Slide%20Image%0A%20%20Classification&entry.906535625=Tianqi%20Xiang%20and%20Yi%20Li%20and%20Qixiang%20Zhang%20and%20Xiaomeng%20Li&entry.1292438233=%20%20Recent%20advances%20in%20histopathology%20vision-language%20foundation%20models%20%28VLFMs%29%0Ahave%20shown%20promise%20in%20addressing%20data%20scarcity%20for%20whole%20slide%20image%20%28WSI%29%0Aclassification%20via%20zero-shot%20adaptation.%20However%2C%20these%20methods%20remain%0Aoutperformed%20by%20conventional%20multiple%20instance%20learning%20%28MIL%29%20approaches%0Atrained%20on%20large%20datasets%2C%20motivating%20recent%20efforts%20to%20enhance%20VLFM-based%20WSI%0Aclassification%20through%20fewshot%20learning%20paradigms.%20While%20existing%20few-shot%0Amethods%20improve%20diagnostic%20accuracy%20with%20limited%20annotations%2C%20their%20reliance%20on%0Aconventional%20classifier%20designs%20introduces%20critical%20vulnerabilities%20to%20data%0Ascarcity.%20To%20address%20this%20problem%2C%20we%20propose%20a%20Meta-Optimized%20Classifier%20%28MOC%29%0Acomprising%20two%20core%20components%3A%20%281%29%20a%20meta-learner%20that%20automatically%20optimizes%0Aa%20classifier%20configuration%20from%20a%20mixture%20of%20candidate%20classifiers%20and%20%282%29%20a%0Aclassifier%20bank%20housing%20diverse%20candidate%20classifiers%20to%20enable%20a%20holistic%0Apathological%20interpretation.%20Extensive%20experiments%20demonstrate%20that%20MOC%0Aoutperforms%20prior%20arts%20in%20multiple%20few-shot%20benchmarks.%20Notably%2C%20on%20the%0ATCGA-NSCLC%20benchmark%2C%20MOC%20improves%20AUC%20by%2010.4%25%20over%20the%20state-of-the-art%0Afew-shot%20VLFM-based%20methods%2C%20with%20gains%20up%20to%2026.25%25%20under%201-shot%20conditions%2C%0Aoffering%20a%20critical%20advancement%20for%20clinical%20deployments%20where%20diagnostic%0Atraining%20data%20is%20severely%20limited.%20Code%20is%20available%20at%0Ahttps%3A//github.com/xmed-lab/MOC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09967v1&entry.124074799=Read"},
{"title": "Exploring the Potential of Large Language Models in Fine-Grained Review\n  Comment Classification", "author": "Linh Nguyen and Chunhua Liu and Hong Yi Lin and Patanamon Thongtanunam", "abstract": "  Code review is a crucial practice in software development. As code review\nnowadays is lightweight, various issues can be identified, and sometimes, they\ncan be trivial. Research has investigated automated approaches to classify\nreview comments to gauge the effectiveness of code reviews. However, previous\nstudies have primarily relied on supervised machine learning, which requires\nextensive manual annotation to train the models effectively. To address this\nlimitation, we explore the potential of using Large Language Models (LLMs) to\nclassify code review comments. We assess the performance of LLMs to classify 17\ncategories of code review comments. Our results show that LLMs can classify\ncode review comments, outperforming the state-of-the-art approach using a\ntrained deep learning model. In particular, LLMs achieve better accuracy in\nclassifying the five most useful categories, which the state-of-the-art\napproach struggles with due to low training examples. Rather than relying\nsolely on a specific small training data distribution, our results show that\nLLMs provide balanced performance across high- and low-frequency categories.\nThese results suggest that the LLMs could offer a scalable solution for code\nreview analytics to improve the effectiveness of the code review process.\n", "link": "http://arxiv.org/abs/2508.09832v1", "date": "2025-08-13", "relevancy": 2.5155, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Potential%20of%20Large%20Language%20Models%20in%20Fine-Grained%20Review%0A%20%20Comment%20Classification&body=Title%3A%20Exploring%20the%20Potential%20of%20Large%20Language%20Models%20in%20Fine-Grained%20Review%0A%20%20Comment%20Classification%0AAuthor%3A%20Linh%20Nguyen%20and%20Chunhua%20Liu%20and%20Hong%20Yi%20Lin%20and%20Patanamon%20Thongtanunam%0AAbstract%3A%20%20%20Code%20review%20is%20a%20crucial%20practice%20in%20software%20development.%20As%20code%20review%0Anowadays%20is%20lightweight%2C%20various%20issues%20can%20be%20identified%2C%20and%20sometimes%2C%20they%0Acan%20be%20trivial.%20Research%20has%20investigated%20automated%20approaches%20to%20classify%0Areview%20comments%20to%20gauge%20the%20effectiveness%20of%20code%20reviews.%20However%2C%20previous%0Astudies%20have%20primarily%20relied%20on%20supervised%20machine%20learning%2C%20which%20requires%0Aextensive%20manual%20annotation%20to%20train%20the%20models%20effectively.%20To%20address%20this%0Alimitation%2C%20we%20explore%20the%20potential%20of%20using%20Large%20Language%20Models%20%28LLMs%29%20to%0Aclassify%20code%20review%20comments.%20We%20assess%20the%20performance%20of%20LLMs%20to%20classify%2017%0Acategories%20of%20code%20review%20comments.%20Our%20results%20show%20that%20LLMs%20can%20classify%0Acode%20review%20comments%2C%20outperforming%20the%20state-of-the-art%20approach%20using%20a%0Atrained%20deep%20learning%20model.%20In%20particular%2C%20LLMs%20achieve%20better%20accuracy%20in%0Aclassifying%20the%20five%20most%20useful%20categories%2C%20which%20the%20state-of-the-art%0Aapproach%20struggles%20with%20due%20to%20low%20training%20examples.%20Rather%20than%20relying%0Asolely%20on%20a%20specific%20small%20training%20data%20distribution%2C%20our%20results%20show%20that%0ALLMs%20provide%20balanced%20performance%20across%20high-%20and%20low-frequency%20categories.%0AThese%20results%20suggest%20that%20the%20LLMs%20could%20offer%20a%20scalable%20solution%20for%20code%0Areview%20analytics%20to%20improve%20the%20effectiveness%20of%20the%20code%20review%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Potential%2520of%2520Large%2520Language%2520Models%2520in%2520Fine-Grained%2520Review%250A%2520%2520Comment%2520Classification%26entry.906535625%3DLinh%2520Nguyen%2520and%2520Chunhua%2520Liu%2520and%2520Hong%2520Yi%2520Lin%2520and%2520Patanamon%2520Thongtanunam%26entry.1292438233%3D%2520%2520Code%2520review%2520is%2520a%2520crucial%2520practice%2520in%2520software%2520development.%2520As%2520code%2520review%250Anowadays%2520is%2520lightweight%252C%2520various%2520issues%2520can%2520be%2520identified%252C%2520and%2520sometimes%252C%2520they%250Acan%2520be%2520trivial.%2520Research%2520has%2520investigated%2520automated%2520approaches%2520to%2520classify%250Areview%2520comments%2520to%2520gauge%2520the%2520effectiveness%2520of%2520code%2520reviews.%2520However%252C%2520previous%250Astudies%2520have%2520primarily%2520relied%2520on%2520supervised%2520machine%2520learning%252C%2520which%2520requires%250Aextensive%2520manual%2520annotation%2520to%2520train%2520the%2520models%2520effectively.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520explore%2520the%2520potential%2520of%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%250Aclassify%2520code%2520review%2520comments.%2520We%2520assess%2520the%2520performance%2520of%2520LLMs%2520to%2520classify%252017%250Acategories%2520of%2520code%2520review%2520comments.%2520Our%2520results%2520show%2520that%2520LLMs%2520can%2520classify%250Acode%2520review%2520comments%252C%2520outperforming%2520the%2520state-of-the-art%2520approach%2520using%2520a%250Atrained%2520deep%2520learning%2520model.%2520In%2520particular%252C%2520LLMs%2520achieve%2520better%2520accuracy%2520in%250Aclassifying%2520the%2520five%2520most%2520useful%2520categories%252C%2520which%2520the%2520state-of-the-art%250Aapproach%2520struggles%2520with%2520due%2520to%2520low%2520training%2520examples.%2520Rather%2520than%2520relying%250Asolely%2520on%2520a%2520specific%2520small%2520training%2520data%2520distribution%252C%2520our%2520results%2520show%2520that%250ALLMs%2520provide%2520balanced%2520performance%2520across%2520high-%2520and%2520low-frequency%2520categories.%250AThese%2520results%2520suggest%2520that%2520the%2520LLMs%2520could%2520offer%2520a%2520scalable%2520solution%2520for%2520code%250Areview%2520analytics%2520to%2520improve%2520the%2520effectiveness%2520of%2520the%2520code%2520review%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Potential%20of%20Large%20Language%20Models%20in%20Fine-Grained%20Review%0A%20%20Comment%20Classification&entry.906535625=Linh%20Nguyen%20and%20Chunhua%20Liu%20and%20Hong%20Yi%20Lin%20and%20Patanamon%20Thongtanunam&entry.1292438233=%20%20Code%20review%20is%20a%20crucial%20practice%20in%20software%20development.%20As%20code%20review%0Anowadays%20is%20lightweight%2C%20various%20issues%20can%20be%20identified%2C%20and%20sometimes%2C%20they%0Acan%20be%20trivial.%20Research%20has%20investigated%20automated%20approaches%20to%20classify%0Areview%20comments%20to%20gauge%20the%20effectiveness%20of%20code%20reviews.%20However%2C%20previous%0Astudies%20have%20primarily%20relied%20on%20supervised%20machine%20learning%2C%20which%20requires%0Aextensive%20manual%20annotation%20to%20train%20the%20models%20effectively.%20To%20address%20this%0Alimitation%2C%20we%20explore%20the%20potential%20of%20using%20Large%20Language%20Models%20%28LLMs%29%20to%0Aclassify%20code%20review%20comments.%20We%20assess%20the%20performance%20of%20LLMs%20to%20classify%2017%0Acategories%20of%20code%20review%20comments.%20Our%20results%20show%20that%20LLMs%20can%20classify%0Acode%20review%20comments%2C%20outperforming%20the%20state-of-the-art%20approach%20using%20a%0Atrained%20deep%20learning%20model.%20In%20particular%2C%20LLMs%20achieve%20better%20accuracy%20in%0Aclassifying%20the%20five%20most%20useful%20categories%2C%20which%20the%20state-of-the-art%0Aapproach%20struggles%20with%20due%20to%20low%20training%20examples.%20Rather%20than%20relying%0Asolely%20on%20a%20specific%20small%20training%20data%20distribution%2C%20our%20results%20show%20that%0ALLMs%20provide%20balanced%20performance%20across%20high-%20and%20low-frequency%20categories.%0AThese%20results%20suggest%20that%20the%20LLMs%20could%20offer%20a%20scalable%20solution%20for%20code%0Areview%20analytics%20to%20improve%20the%20effectiveness%20of%20the%20code%20review%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09832v1&entry.124074799=Read"},
{"title": "AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using\n  Diffusion Models", "author": "Tom\u00e1s de la Sotta and Jos\u00e9 M. Saavedra and H\u00e9ctor Henr\u00edquez and Violeta Chang and Aline Xavier", "abstract": "  Low-dose CT (LDCT) protocols reduce radiation exposure but increase image\nnoise, compromising diagnostic confidence. Diffusion-based generative models\nhave shown promise for LDCT denoising by learning image priors and performing\niterative refinement. In this work, we introduce AST-n, an accelerated\ninference framework that initiates reverse diffusion from intermediate noise\nlevels, and integrate high-order ODE solvers within conditioned models to\nfurther reduce sampling steps. We evaluate two acceleration paradigms--AST-n\nsampling and standard scheduling with high-order solvers -- on the Low Dose CT\nGrand Challenge dataset, covering head, abdominal, and chest scans at 10-25 %\nof standard dose. Conditioned models using only 25 steps (AST-25) achieve peak\nsignal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM)\nabove 0.95, closely matching standard baselines while cutting inference time\nfrom ~16 seg to under 1 seg per slice. Unconditional sampling suffers\nsubstantial quality loss, underscoring the necessity of conditioning. We also\nassess DDIM inversion, which yields marginal PSNR gains at the cost of doubling\ninference time, limiting its clinical practicality. Our results demonstrate\nthat AST-n with high-order samplers enables rapid LDCT reconstruction without\nsignificant loss of image fidelity, advancing the feasibility of\ndiffusion-based methods in clinical workflows.\n", "link": "http://arxiv.org/abs/2508.09943v1", "date": "2025-08-13", "relevancy": 2.4955, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6778}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6131}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AST-n%3A%20A%20Fast%20Sampling%20Approach%20for%20Low-Dose%20CT%20Reconstruction%20using%0A%20%20Diffusion%20Models&body=Title%3A%20AST-n%3A%20A%20Fast%20Sampling%20Approach%20for%20Low-Dose%20CT%20Reconstruction%20using%0A%20%20Diffusion%20Models%0AAuthor%3A%20Tom%C3%A1s%20de%20la%20Sotta%20and%20Jos%C3%A9%20M.%20Saavedra%20and%20H%C3%A9ctor%20Henr%C3%ADquez%20and%20Violeta%20Chang%20and%20Aline%20Xavier%0AAbstract%3A%20%20%20Low-dose%20CT%20%28LDCT%29%20protocols%20reduce%20radiation%20exposure%20but%20increase%20image%0Anoise%2C%20compromising%20diagnostic%20confidence.%20Diffusion-based%20generative%20models%0Ahave%20shown%20promise%20for%20LDCT%20denoising%20by%20learning%20image%20priors%20and%20performing%0Aiterative%20refinement.%20In%20this%20work%2C%20we%20introduce%20AST-n%2C%20an%20accelerated%0Ainference%20framework%20that%20initiates%20reverse%20diffusion%20from%20intermediate%20noise%0Alevels%2C%20and%20integrate%20high-order%20ODE%20solvers%20within%20conditioned%20models%20to%0Afurther%20reduce%20sampling%20steps.%20We%20evaluate%20two%20acceleration%20paradigms--AST-n%0Asampling%20and%20standard%20scheduling%20with%20high-order%20solvers%20--%20on%20the%20Low%20Dose%20CT%0AGrand%20Challenge%20dataset%2C%20covering%20head%2C%20abdominal%2C%20and%20chest%20scans%20at%2010-25%20%25%0Aof%20standard%20dose.%20Conditioned%20models%20using%20only%2025%20steps%20%28AST-25%29%20achieve%20peak%0Asignal-to-noise%20ratio%20%28PSNR%29%20above%2038%20dB%20and%20structural%20similarity%20index%20%28SSIM%29%0Aabove%200.95%2C%20closely%20matching%20standard%20baselines%20while%20cutting%20inference%20time%0Afrom%20~16%20seg%20to%20under%201%20seg%20per%20slice.%20Unconditional%20sampling%20suffers%0Asubstantial%20quality%20loss%2C%20underscoring%20the%20necessity%20of%20conditioning.%20We%20also%0Aassess%20DDIM%20inversion%2C%20which%20yields%20marginal%20PSNR%20gains%20at%20the%20cost%20of%20doubling%0Ainference%20time%2C%20limiting%20its%20clinical%20practicality.%20Our%20results%20demonstrate%0Athat%20AST-n%20with%20high-order%20samplers%20enables%20rapid%20LDCT%20reconstruction%20without%0Asignificant%20loss%20of%20image%20fidelity%2C%20advancing%20the%20feasibility%20of%0Adiffusion-based%20methods%20in%20clinical%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAST-n%253A%2520A%2520Fast%2520Sampling%2520Approach%2520for%2520Low-Dose%2520CT%2520Reconstruction%2520using%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DTom%25C3%25A1s%2520de%2520la%2520Sotta%2520and%2520Jos%25C3%25A9%2520M.%2520Saavedra%2520and%2520H%25C3%25A9ctor%2520Henr%25C3%25ADquez%2520and%2520Violeta%2520Chang%2520and%2520Aline%2520Xavier%26entry.1292438233%3D%2520%2520Low-dose%2520CT%2520%2528LDCT%2529%2520protocols%2520reduce%2520radiation%2520exposure%2520but%2520increase%2520image%250Anoise%252C%2520compromising%2520diagnostic%2520confidence.%2520Diffusion-based%2520generative%2520models%250Ahave%2520shown%2520promise%2520for%2520LDCT%2520denoising%2520by%2520learning%2520image%2520priors%2520and%2520performing%250Aiterative%2520refinement.%2520In%2520this%2520work%252C%2520we%2520introduce%2520AST-n%252C%2520an%2520accelerated%250Ainference%2520framework%2520that%2520initiates%2520reverse%2520diffusion%2520from%2520intermediate%2520noise%250Alevels%252C%2520and%2520integrate%2520high-order%2520ODE%2520solvers%2520within%2520conditioned%2520models%2520to%250Afurther%2520reduce%2520sampling%2520steps.%2520We%2520evaluate%2520two%2520acceleration%2520paradigms--AST-n%250Asampling%2520and%2520standard%2520scheduling%2520with%2520high-order%2520solvers%2520--%2520on%2520the%2520Low%2520Dose%2520CT%250AGrand%2520Challenge%2520dataset%252C%2520covering%2520head%252C%2520abdominal%252C%2520and%2520chest%2520scans%2520at%252010-25%2520%2525%250Aof%2520standard%2520dose.%2520Conditioned%2520models%2520using%2520only%252025%2520steps%2520%2528AST-25%2529%2520achieve%2520peak%250Asignal-to-noise%2520ratio%2520%2528PSNR%2529%2520above%252038%2520dB%2520and%2520structural%2520similarity%2520index%2520%2528SSIM%2529%250Aabove%25200.95%252C%2520closely%2520matching%2520standard%2520baselines%2520while%2520cutting%2520inference%2520time%250Afrom%2520~16%2520seg%2520to%2520under%25201%2520seg%2520per%2520slice.%2520Unconditional%2520sampling%2520suffers%250Asubstantial%2520quality%2520loss%252C%2520underscoring%2520the%2520necessity%2520of%2520conditioning.%2520We%2520also%250Aassess%2520DDIM%2520inversion%252C%2520which%2520yields%2520marginal%2520PSNR%2520gains%2520at%2520the%2520cost%2520of%2520doubling%250Ainference%2520time%252C%2520limiting%2520its%2520clinical%2520practicality.%2520Our%2520results%2520demonstrate%250Athat%2520AST-n%2520with%2520high-order%2520samplers%2520enables%2520rapid%2520LDCT%2520reconstruction%2520without%250Asignificant%2520loss%2520of%2520image%2520fidelity%252C%2520advancing%2520the%2520feasibility%2520of%250Adiffusion-based%2520methods%2520in%2520clinical%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AST-n%3A%20A%20Fast%20Sampling%20Approach%20for%20Low-Dose%20CT%20Reconstruction%20using%0A%20%20Diffusion%20Models&entry.906535625=Tom%C3%A1s%20de%20la%20Sotta%20and%20Jos%C3%A9%20M.%20Saavedra%20and%20H%C3%A9ctor%20Henr%C3%ADquez%20and%20Violeta%20Chang%20and%20Aline%20Xavier&entry.1292438233=%20%20Low-dose%20CT%20%28LDCT%29%20protocols%20reduce%20radiation%20exposure%20but%20increase%20image%0Anoise%2C%20compromising%20diagnostic%20confidence.%20Diffusion-based%20generative%20models%0Ahave%20shown%20promise%20for%20LDCT%20denoising%20by%20learning%20image%20priors%20and%20performing%0Aiterative%20refinement.%20In%20this%20work%2C%20we%20introduce%20AST-n%2C%20an%20accelerated%0Ainference%20framework%20that%20initiates%20reverse%20diffusion%20from%20intermediate%20noise%0Alevels%2C%20and%20integrate%20high-order%20ODE%20solvers%20within%20conditioned%20models%20to%0Afurther%20reduce%20sampling%20steps.%20We%20evaluate%20two%20acceleration%20paradigms--AST-n%0Asampling%20and%20standard%20scheduling%20with%20high-order%20solvers%20--%20on%20the%20Low%20Dose%20CT%0AGrand%20Challenge%20dataset%2C%20covering%20head%2C%20abdominal%2C%20and%20chest%20scans%20at%2010-25%20%25%0Aof%20standard%20dose.%20Conditioned%20models%20using%20only%2025%20steps%20%28AST-25%29%20achieve%20peak%0Asignal-to-noise%20ratio%20%28PSNR%29%20above%2038%20dB%20and%20structural%20similarity%20index%20%28SSIM%29%0Aabove%200.95%2C%20closely%20matching%20standard%20baselines%20while%20cutting%20inference%20time%0Afrom%20~16%20seg%20to%20under%201%20seg%20per%20slice.%20Unconditional%20sampling%20suffers%0Asubstantial%20quality%20loss%2C%20underscoring%20the%20necessity%20of%20conditioning.%20We%20also%0Aassess%20DDIM%20inversion%2C%20which%20yields%20marginal%20PSNR%20gains%20at%20the%20cost%20of%20doubling%0Ainference%20time%2C%20limiting%20its%20clinical%20practicality.%20Our%20results%20demonstrate%0Athat%20AST-n%20with%20high-order%20samplers%20enables%20rapid%20LDCT%20reconstruction%20without%0Asignificant%20loss%20of%20image%20fidelity%2C%20advancing%20the%20feasibility%20of%0Adiffusion-based%20methods%20in%20clinical%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09943v1&entry.124074799=Read"},
{"title": "Pretrained Reversible Generation as Unsupervised Visual Representation\n  Learning", "author": "Rongkun Xue and Jinouwen Zhang and Yazhe Niu and Dazhong Shen and Bingqi Ma and Yu Liu and Jing Yang", "abstract": "  Recent generative models based on score matching and flow matching have\nsignificantly advanced generation tasks, but their potential in discriminative\ntasks remains underexplored. Previous approaches, such as generative\nclassifiers, have not fully leveraged the capabilities of these models for\ndiscriminative tasks due to their intricate designs. We propose Pretrained\nReversible Generation (PRG), which extracts unsupervised representations by\nreversing the generative process of a pretrained continuous generation model.\nPRG effectively reuses unsupervised generative models, leveraging their high\ncapacity to serve as robust and generalizable feature extractors for downstream\ntasks. This framework enables the flexible selection of feature hierarchies\ntailored to specific downstream tasks. Our method consistently outperforms\nprior approaches across multiple benchmarks, achieving state-of-the-art\nperformance among generative model based methods, including 78% top-1 accuracy\non ImageNet at a resolution of 64*64. Extensive ablation studies, including\nout-of-distribution evaluations, further validate the effectiveness of our\napproach.PRG is available at https://github.com/opendilab/PRG.\n", "link": "http://arxiv.org/abs/2412.01787v6", "date": "2025-08-13", "relevancy": 2.454, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6221}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6088}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pretrained%20Reversible%20Generation%20as%20Unsupervised%20Visual%20Representation%0A%20%20Learning&body=Title%3A%20Pretrained%20Reversible%20Generation%20as%20Unsupervised%20Visual%20Representation%0A%20%20Learning%0AAuthor%3A%20Rongkun%20Xue%20and%20Jinouwen%20Zhang%20and%20Yazhe%20Niu%20and%20Dazhong%20Shen%20and%20Bingqi%20Ma%20and%20Yu%20Liu%20and%20Jing%20Yang%0AAbstract%3A%20%20%20Recent%20generative%20models%20based%20on%20score%20matching%20and%20flow%20matching%20have%0Asignificantly%20advanced%20generation%20tasks%2C%20but%20their%20potential%20in%20discriminative%0Atasks%20remains%20underexplored.%20Previous%20approaches%2C%20such%20as%20generative%0Aclassifiers%2C%20have%20not%20fully%20leveraged%20the%20capabilities%20of%20these%20models%20for%0Adiscriminative%20tasks%20due%20to%20their%20intricate%20designs.%20We%20propose%20Pretrained%0AReversible%20Generation%20%28PRG%29%2C%20which%20extracts%20unsupervised%20representations%20by%0Areversing%20the%20generative%20process%20of%20a%20pretrained%20continuous%20generation%20model.%0APRG%20effectively%20reuses%20unsupervised%20generative%20models%2C%20leveraging%20their%20high%0Acapacity%20to%20serve%20as%20robust%20and%20generalizable%20feature%20extractors%20for%20downstream%0Atasks.%20This%20framework%20enables%20the%20flexible%20selection%20of%20feature%20hierarchies%0Atailored%20to%20specific%20downstream%20tasks.%20Our%20method%20consistently%20outperforms%0Aprior%20approaches%20across%20multiple%20benchmarks%2C%20achieving%20state-of-the-art%0Aperformance%20among%20generative%20model%20based%20methods%2C%20including%2078%25%20top-1%20accuracy%0Aon%20ImageNet%20at%20a%20resolution%20of%2064%2A64.%20Extensive%20ablation%20studies%2C%20including%0Aout-of-distribution%20evaluations%2C%20further%20validate%20the%20effectiveness%20of%20our%0Aapproach.PRG%20is%20available%20at%20https%3A//github.com/opendilab/PRG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01787v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPretrained%2520Reversible%2520Generation%2520as%2520Unsupervised%2520Visual%2520Representation%250A%2520%2520Learning%26entry.906535625%3DRongkun%2520Xue%2520and%2520Jinouwen%2520Zhang%2520and%2520Yazhe%2520Niu%2520and%2520Dazhong%2520Shen%2520and%2520Bingqi%2520Ma%2520and%2520Yu%2520Liu%2520and%2520Jing%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520generative%2520models%2520based%2520on%2520score%2520matching%2520and%2520flow%2520matching%2520have%250Asignificantly%2520advanced%2520generation%2520tasks%252C%2520but%2520their%2520potential%2520in%2520discriminative%250Atasks%2520remains%2520underexplored.%2520Previous%2520approaches%252C%2520such%2520as%2520generative%250Aclassifiers%252C%2520have%2520not%2520fully%2520leveraged%2520the%2520capabilities%2520of%2520these%2520models%2520for%250Adiscriminative%2520tasks%2520due%2520to%2520their%2520intricate%2520designs.%2520We%2520propose%2520Pretrained%250AReversible%2520Generation%2520%2528PRG%2529%252C%2520which%2520extracts%2520unsupervised%2520representations%2520by%250Areversing%2520the%2520generative%2520process%2520of%2520a%2520pretrained%2520continuous%2520generation%2520model.%250APRG%2520effectively%2520reuses%2520unsupervised%2520generative%2520models%252C%2520leveraging%2520their%2520high%250Acapacity%2520to%2520serve%2520as%2520robust%2520and%2520generalizable%2520feature%2520extractors%2520for%2520downstream%250Atasks.%2520This%2520framework%2520enables%2520the%2520flexible%2520selection%2520of%2520feature%2520hierarchies%250Atailored%2520to%2520specific%2520downstream%2520tasks.%2520Our%2520method%2520consistently%2520outperforms%250Aprior%2520approaches%2520across%2520multiple%2520benchmarks%252C%2520achieving%2520state-of-the-art%250Aperformance%2520among%2520generative%2520model%2520based%2520methods%252C%2520including%252078%2525%2520top-1%2520accuracy%250Aon%2520ImageNet%2520at%2520a%2520resolution%2520of%252064%252A64.%2520Extensive%2520ablation%2520studies%252C%2520including%250Aout-of-distribution%2520evaluations%252C%2520further%2520validate%2520the%2520effectiveness%2520of%2520our%250Aapproach.PRG%2520is%2520available%2520at%2520https%253A//github.com/opendilab/PRG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01787v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretrained%20Reversible%20Generation%20as%20Unsupervised%20Visual%20Representation%0A%20%20Learning&entry.906535625=Rongkun%20Xue%20and%20Jinouwen%20Zhang%20and%20Yazhe%20Niu%20and%20Dazhong%20Shen%20and%20Bingqi%20Ma%20and%20Yu%20Liu%20and%20Jing%20Yang&entry.1292438233=%20%20Recent%20generative%20models%20based%20on%20score%20matching%20and%20flow%20matching%20have%0Asignificantly%20advanced%20generation%20tasks%2C%20but%20their%20potential%20in%20discriminative%0Atasks%20remains%20underexplored.%20Previous%20approaches%2C%20such%20as%20generative%0Aclassifiers%2C%20have%20not%20fully%20leveraged%20the%20capabilities%20of%20these%20models%20for%0Adiscriminative%20tasks%20due%20to%20their%20intricate%20designs.%20We%20propose%20Pretrained%0AReversible%20Generation%20%28PRG%29%2C%20which%20extracts%20unsupervised%20representations%20by%0Areversing%20the%20generative%20process%20of%20a%20pretrained%20continuous%20generation%20model.%0APRG%20effectively%20reuses%20unsupervised%20generative%20models%2C%20leveraging%20their%20high%0Acapacity%20to%20serve%20as%20robust%20and%20generalizable%20feature%20extractors%20for%20downstream%0Atasks.%20This%20framework%20enables%20the%20flexible%20selection%20of%20feature%20hierarchies%0Atailored%20to%20specific%20downstream%20tasks.%20Our%20method%20consistently%20outperforms%0Aprior%20approaches%20across%20multiple%20benchmarks%2C%20achieving%20state-of-the-art%0Aperformance%20among%20generative%20model%20based%20methods%2C%20including%2078%25%20top-1%20accuracy%0Aon%20ImageNet%20at%20a%20resolution%20of%2064%2A64.%20Extensive%20ablation%20studies%2C%20including%0Aout-of-distribution%20evaluations%2C%20further%20validate%20the%20effectiveness%20of%20our%0Aapproach.PRG%20is%20available%20at%20https%3A//github.com/opendilab/PRG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01787v6&entry.124074799=Read"},
{"title": "LM-MCVT: A Lightweight Multi-modal Multi-view Convolutional-Vision\n  Transformer Approach for 3D Object Recognition", "author": "Songsong Xiong and Hamidreza Kasaei", "abstract": "  In human-centered environments such as restaurants, homes, and warehouses,\nrobots often face challenges in accurately recognizing 3D objects. These\nchallenges stem from the complexity and variability of these environments,\nincluding diverse object shapes. In this paper, we propose a novel Lightweight\nMulti-modal Multi-view Convolutional-Vision Transformer network (LM-MCVT) to\nenhance 3D object recognition in robotic applications. Our approach leverages\nthe Globally Entropy-based Embeddings Fusion (GEEF) method to integrate\nmulti-views efficiently. The LM-MCVT architecture incorporates pre- and\nmid-level convolutional encoders and local and global transformers to enhance\nfeature extraction and recognition accuracy. We evaluate our method on the\nsynthetic ModelNet40 dataset and achieve a recognition accuracy of 95.6% using\na four-view setup, surpassing existing state-of-the-art methods. To further\nvalidate its effectiveness, we conduct 5-fold cross-validation on the\nreal-world OmniObject3D dataset using the same configuration. Results\nconsistently show superior performance, demonstrating the method's robustness\nin 3D object recognition across synthetic and real-world 3D data.\n", "link": "http://arxiv.org/abs/2504.19256v3", "date": "2025-08-13", "relevancy": 2.453, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6325}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6184}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LM-MCVT%3A%20A%20Lightweight%20Multi-modal%20Multi-view%20Convolutional-Vision%0A%20%20Transformer%20Approach%20for%203D%20Object%20Recognition&body=Title%3A%20LM-MCVT%3A%20A%20Lightweight%20Multi-modal%20Multi-view%20Convolutional-Vision%0A%20%20Transformer%20Approach%20for%203D%20Object%20Recognition%0AAuthor%3A%20Songsong%20Xiong%20and%20Hamidreza%20Kasaei%0AAbstract%3A%20%20%20In%20human-centered%20environments%20such%20as%20restaurants%2C%20homes%2C%20and%20warehouses%2C%0Arobots%20often%20face%20challenges%20in%20accurately%20recognizing%203D%20objects.%20These%0Achallenges%20stem%20from%20the%20complexity%20and%20variability%20of%20these%20environments%2C%0Aincluding%20diverse%20object%20shapes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Lightweight%0AMulti-modal%20Multi-view%20Convolutional-Vision%20Transformer%20network%20%28LM-MCVT%29%20to%0Aenhance%203D%20object%20recognition%20in%20robotic%20applications.%20Our%20approach%20leverages%0Athe%20Globally%20Entropy-based%20Embeddings%20Fusion%20%28GEEF%29%20method%20to%20integrate%0Amulti-views%20efficiently.%20The%20LM-MCVT%20architecture%20incorporates%20pre-%20and%0Amid-level%20convolutional%20encoders%20and%20local%20and%20global%20transformers%20to%20enhance%0Afeature%20extraction%20and%20recognition%20accuracy.%20We%20evaluate%20our%20method%20on%20the%0Asynthetic%20ModelNet40%20dataset%20and%20achieve%20a%20recognition%20accuracy%20of%2095.6%25%20using%0Aa%20four-view%20setup%2C%20surpassing%20existing%20state-of-the-art%20methods.%20To%20further%0Avalidate%20its%20effectiveness%2C%20we%20conduct%205-fold%20cross-validation%20on%20the%0Areal-world%20OmniObject3D%20dataset%20using%20the%20same%20configuration.%20Results%0Aconsistently%20show%20superior%20performance%2C%20demonstrating%20the%20method%27s%20robustness%0Ain%203D%20object%20recognition%20across%20synthetic%20and%20real-world%203D%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19256v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLM-MCVT%253A%2520A%2520Lightweight%2520Multi-modal%2520Multi-view%2520Convolutional-Vision%250A%2520%2520Transformer%2520Approach%2520for%25203D%2520Object%2520Recognition%26entry.906535625%3DSongsong%2520Xiong%2520and%2520Hamidreza%2520Kasaei%26entry.1292438233%3D%2520%2520In%2520human-centered%2520environments%2520such%2520as%2520restaurants%252C%2520homes%252C%2520and%2520warehouses%252C%250Arobots%2520often%2520face%2520challenges%2520in%2520accurately%2520recognizing%25203D%2520objects.%2520These%250Achallenges%2520stem%2520from%2520the%2520complexity%2520and%2520variability%2520of%2520these%2520environments%252C%250Aincluding%2520diverse%2520object%2520shapes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Lightweight%250AMulti-modal%2520Multi-view%2520Convolutional-Vision%2520Transformer%2520network%2520%2528LM-MCVT%2529%2520to%250Aenhance%25203D%2520object%2520recognition%2520in%2520robotic%2520applications.%2520Our%2520approach%2520leverages%250Athe%2520Globally%2520Entropy-based%2520Embeddings%2520Fusion%2520%2528GEEF%2529%2520method%2520to%2520integrate%250Amulti-views%2520efficiently.%2520The%2520LM-MCVT%2520architecture%2520incorporates%2520pre-%2520and%250Amid-level%2520convolutional%2520encoders%2520and%2520local%2520and%2520global%2520transformers%2520to%2520enhance%250Afeature%2520extraction%2520and%2520recognition%2520accuracy.%2520We%2520evaluate%2520our%2520method%2520on%2520the%250Asynthetic%2520ModelNet40%2520dataset%2520and%2520achieve%2520a%2520recognition%2520accuracy%2520of%252095.6%2525%2520using%250Aa%2520four-view%2520setup%252C%2520surpassing%2520existing%2520state-of-the-art%2520methods.%2520To%2520further%250Avalidate%2520its%2520effectiveness%252C%2520we%2520conduct%25205-fold%2520cross-validation%2520on%2520the%250Areal-world%2520OmniObject3D%2520dataset%2520using%2520the%2520same%2520configuration.%2520Results%250Aconsistently%2520show%2520superior%2520performance%252C%2520demonstrating%2520the%2520method%2527s%2520robustness%250Ain%25203D%2520object%2520recognition%2520across%2520synthetic%2520and%2520real-world%25203D%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19256v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LM-MCVT%3A%20A%20Lightweight%20Multi-modal%20Multi-view%20Convolutional-Vision%0A%20%20Transformer%20Approach%20for%203D%20Object%20Recognition&entry.906535625=Songsong%20Xiong%20and%20Hamidreza%20Kasaei&entry.1292438233=%20%20In%20human-centered%20environments%20such%20as%20restaurants%2C%20homes%2C%20and%20warehouses%2C%0Arobots%20often%20face%20challenges%20in%20accurately%20recognizing%203D%20objects.%20These%0Achallenges%20stem%20from%20the%20complexity%20and%20variability%20of%20these%20environments%2C%0Aincluding%20diverse%20object%20shapes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Lightweight%0AMulti-modal%20Multi-view%20Convolutional-Vision%20Transformer%20network%20%28LM-MCVT%29%20to%0Aenhance%203D%20object%20recognition%20in%20robotic%20applications.%20Our%20approach%20leverages%0Athe%20Globally%20Entropy-based%20Embeddings%20Fusion%20%28GEEF%29%20method%20to%20integrate%0Amulti-views%20efficiently.%20The%20LM-MCVT%20architecture%20incorporates%20pre-%20and%0Amid-level%20convolutional%20encoders%20and%20local%20and%20global%20transformers%20to%20enhance%0Afeature%20extraction%20and%20recognition%20accuracy.%20We%20evaluate%20our%20method%20on%20the%0Asynthetic%20ModelNet40%20dataset%20and%20achieve%20a%20recognition%20accuracy%20of%2095.6%25%20using%0Aa%20four-view%20setup%2C%20surpassing%20existing%20state-of-the-art%20methods.%20To%20further%0Avalidate%20its%20effectiveness%2C%20we%20conduct%205-fold%20cross-validation%20on%20the%0Areal-world%20OmniObject3D%20dataset%20using%20the%20same%20configuration.%20Results%0Aconsistently%20show%20superior%20performance%2C%20demonstrating%20the%20method%27s%20robustness%0Ain%203D%20object%20recognition%20across%20synthetic%20and%20real-world%203D%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19256v3&entry.124074799=Read"},
{"title": "ViMoNet: A Multimodal Vision-Language Framework for Human Behavior\n  Understanding from Motion and Video", "author": "Rajan Das Gupta and Md Yeasin Rahat and Nafiz Fahad and Abir Ahmed and Liew Tze Hui", "abstract": "  This study investigates how large language models (LLMs) can be used to\nunderstand human behavior using motion and video data. We think that mixing\nboth types is essential to completely capture the nuanced movements and\nmeanings of human actions, in contrast to recent models that simply concentrate\non motion data or films. To address this, we provide ViMoNet, a straightforward\nyet effective framework for comprehending, characterizing, and deducing human\naction. ViMoNet employs a joint training strategy that leverages the advantages\nof two data types: detailed motion-text data, which is more exact, and generic\nvideo-text data, which is more comprehensive but less detailed. This aids in\nthe model's acquisition of rich data regarding time and space in human\nbehavior. Additionally, we provide a brand new dataset named VIMOS that\ncontains a variety of films, motion sequences, instructions, and subtitles. We\ndeveloped ViMoNet-Bench, a standardized benchmark with carefully labeled\nsamples, to evaluate how well models understand human behavior. Our tests show\nthat ViMoNet outperforms existing methods in caption generation, motion\nunderstanding, and behavior interpretation.\n", "link": "http://arxiv.org/abs/2508.09818v1", "date": "2025-08-13", "relevancy": 2.4484, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6174}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViMoNet%3A%20A%20Multimodal%20Vision-Language%20Framework%20for%20Human%20Behavior%0A%20%20Understanding%20from%20Motion%20and%20Video&body=Title%3A%20ViMoNet%3A%20A%20Multimodal%20Vision-Language%20Framework%20for%20Human%20Behavior%0A%20%20Understanding%20from%20Motion%20and%20Video%0AAuthor%3A%20Rajan%20Das%20Gupta%20and%20Md%20Yeasin%20Rahat%20and%20Nafiz%20Fahad%20and%20Abir%20Ahmed%20and%20Liew%20Tze%20Hui%0AAbstract%3A%20%20%20This%20study%20investigates%20how%20large%20language%20models%20%28LLMs%29%20can%20be%20used%20to%0Aunderstand%20human%20behavior%20using%20motion%20and%20video%20data.%20We%20think%20that%20mixing%0Aboth%20types%20is%20essential%20to%20completely%20capture%20the%20nuanced%20movements%20and%0Ameanings%20of%20human%20actions%2C%20in%20contrast%20to%20recent%20models%20that%20simply%20concentrate%0Aon%20motion%20data%20or%20films.%20To%20address%20this%2C%20we%20provide%20ViMoNet%2C%20a%20straightforward%0Ayet%20effective%20framework%20for%20comprehending%2C%20characterizing%2C%20and%20deducing%20human%0Aaction.%20ViMoNet%20employs%20a%20joint%20training%20strategy%20that%20leverages%20the%20advantages%0Aof%20two%20data%20types%3A%20detailed%20motion-text%20data%2C%20which%20is%20more%20exact%2C%20and%20generic%0Avideo-text%20data%2C%20which%20is%20more%20comprehensive%20but%20less%20detailed.%20This%20aids%20in%0Athe%20model%27s%20acquisition%20of%20rich%20data%20regarding%20time%20and%20space%20in%20human%0Abehavior.%20Additionally%2C%20we%20provide%20a%20brand%20new%20dataset%20named%20VIMOS%20that%0Acontains%20a%20variety%20of%20films%2C%20motion%20sequences%2C%20instructions%2C%20and%20subtitles.%20We%0Adeveloped%20ViMoNet-Bench%2C%20a%20standardized%20benchmark%20with%20carefully%20labeled%0Asamples%2C%20to%20evaluate%20how%20well%20models%20understand%20human%20behavior.%20Our%20tests%20show%0Athat%20ViMoNet%20outperforms%20existing%20methods%20in%20caption%20generation%2C%20motion%0Aunderstanding%2C%20and%20behavior%20interpretation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViMoNet%253A%2520A%2520Multimodal%2520Vision-Language%2520Framework%2520for%2520Human%2520Behavior%250A%2520%2520Understanding%2520from%2520Motion%2520and%2520Video%26entry.906535625%3DRajan%2520Das%2520Gupta%2520and%2520Md%2520Yeasin%2520Rahat%2520and%2520Nafiz%2520Fahad%2520and%2520Abir%2520Ahmed%2520and%2520Liew%2520Tze%2520Hui%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520how%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%2520be%2520used%2520to%250Aunderstand%2520human%2520behavior%2520using%2520motion%2520and%2520video%2520data.%2520We%2520think%2520that%2520mixing%250Aboth%2520types%2520is%2520essential%2520to%2520completely%2520capture%2520the%2520nuanced%2520movements%2520and%250Ameanings%2520of%2520human%2520actions%252C%2520in%2520contrast%2520to%2520recent%2520models%2520that%2520simply%2520concentrate%250Aon%2520motion%2520data%2520or%2520films.%2520To%2520address%2520this%252C%2520we%2520provide%2520ViMoNet%252C%2520a%2520straightforward%250Ayet%2520effective%2520framework%2520for%2520comprehending%252C%2520characterizing%252C%2520and%2520deducing%2520human%250Aaction.%2520ViMoNet%2520employs%2520a%2520joint%2520training%2520strategy%2520that%2520leverages%2520the%2520advantages%250Aof%2520two%2520data%2520types%253A%2520detailed%2520motion-text%2520data%252C%2520which%2520is%2520more%2520exact%252C%2520and%2520generic%250Avideo-text%2520data%252C%2520which%2520is%2520more%2520comprehensive%2520but%2520less%2520detailed.%2520This%2520aids%2520in%250Athe%2520model%2527s%2520acquisition%2520of%2520rich%2520data%2520regarding%2520time%2520and%2520space%2520in%2520human%250Abehavior.%2520Additionally%252C%2520we%2520provide%2520a%2520brand%2520new%2520dataset%2520named%2520VIMOS%2520that%250Acontains%2520a%2520variety%2520of%2520films%252C%2520motion%2520sequences%252C%2520instructions%252C%2520and%2520subtitles.%2520We%250Adeveloped%2520ViMoNet-Bench%252C%2520a%2520standardized%2520benchmark%2520with%2520carefully%2520labeled%250Asamples%252C%2520to%2520evaluate%2520how%2520well%2520models%2520understand%2520human%2520behavior.%2520Our%2520tests%2520show%250Athat%2520ViMoNet%2520outperforms%2520existing%2520methods%2520in%2520caption%2520generation%252C%2520motion%250Aunderstanding%252C%2520and%2520behavior%2520interpretation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViMoNet%3A%20A%20Multimodal%20Vision-Language%20Framework%20for%20Human%20Behavior%0A%20%20Understanding%20from%20Motion%20and%20Video&entry.906535625=Rajan%20Das%20Gupta%20and%20Md%20Yeasin%20Rahat%20and%20Nafiz%20Fahad%20and%20Abir%20Ahmed%20and%20Liew%20Tze%20Hui&entry.1292438233=%20%20This%20study%20investigates%20how%20large%20language%20models%20%28LLMs%29%20can%20be%20used%20to%0Aunderstand%20human%20behavior%20using%20motion%20and%20video%20data.%20We%20think%20that%20mixing%0Aboth%20types%20is%20essential%20to%20completely%20capture%20the%20nuanced%20movements%20and%0Ameanings%20of%20human%20actions%2C%20in%20contrast%20to%20recent%20models%20that%20simply%20concentrate%0Aon%20motion%20data%20or%20films.%20To%20address%20this%2C%20we%20provide%20ViMoNet%2C%20a%20straightforward%0Ayet%20effective%20framework%20for%20comprehending%2C%20characterizing%2C%20and%20deducing%20human%0Aaction.%20ViMoNet%20employs%20a%20joint%20training%20strategy%20that%20leverages%20the%20advantages%0Aof%20two%20data%20types%3A%20detailed%20motion-text%20data%2C%20which%20is%20more%20exact%2C%20and%20generic%0Avideo-text%20data%2C%20which%20is%20more%20comprehensive%20but%20less%20detailed.%20This%20aids%20in%0Athe%20model%27s%20acquisition%20of%20rich%20data%20regarding%20time%20and%20space%20in%20human%0Abehavior.%20Additionally%2C%20we%20provide%20a%20brand%20new%20dataset%20named%20VIMOS%20that%0Acontains%20a%20variety%20of%20films%2C%20motion%20sequences%2C%20instructions%2C%20and%20subtitles.%20We%0Adeveloped%20ViMoNet-Bench%2C%20a%20standardized%20benchmark%20with%20carefully%20labeled%0Asamples%2C%20to%20evaluate%20how%20well%20models%20understand%20human%20behavior.%20Our%20tests%20show%0Athat%20ViMoNet%20outperforms%20existing%20methods%20in%20caption%20generation%2C%20motion%0Aunderstanding%2C%20and%20behavior%20interpretation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09818v1&entry.124074799=Read"},
{"title": "Conformal Prediction of Classifiers with Many Classes based on Noisy\n  Labels", "author": "Coby Penso and Jacob Goldberger and Ethan Fetaya", "abstract": "  Conformal Prediction (CP) controls the prediction uncertainty of\nclassification systems by producing a small prediction set, ensuring a\npredetermined probability that the true class lies within this set. This is\ncommonly done by defining a score, based on the model predictions, and setting\na threshold on this score using a validation set. In this study, we address the\nproblem of CP calibration when we only have access to a calibration set with\nnoisy labels. We show how we can estimate the noise-free conformal threshold\nbased on the noisy labeled data. We derive a finite sample coverage guarantee\nfor uniform noise that remains effective even in tasks with a large number of\nclasses. We dub our approach Noise-Aware Conformal Prediction (NACP). We\nillustrate the performance of the proposed results on several standard image\nclassification datasets with a large number of classes.\n", "link": "http://arxiv.org/abs/2501.12749v2", "date": "2025-08-13", "relevancy": 2.4424, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4915}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4907}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Prediction%20of%20Classifiers%20with%20Many%20Classes%20based%20on%20Noisy%0A%20%20Labels&body=Title%3A%20Conformal%20Prediction%20of%20Classifiers%20with%20Many%20Classes%20based%20on%20Noisy%0A%20%20Labels%0AAuthor%3A%20Coby%20Penso%20and%20Jacob%20Goldberger%20and%20Ethan%20Fetaya%0AAbstract%3A%20%20%20Conformal%20Prediction%20%28CP%29%20controls%20the%20prediction%20uncertainty%20of%0Aclassification%20systems%20by%20producing%20a%20small%20prediction%20set%2C%20ensuring%20a%0Apredetermined%20probability%20that%20the%20true%20class%20lies%20within%20this%20set.%20This%20is%0Acommonly%20done%20by%20defining%20a%20score%2C%20based%20on%20the%20model%20predictions%2C%20and%20setting%0Aa%20threshold%20on%20this%20score%20using%20a%20validation%20set.%20In%20this%20study%2C%20we%20address%20the%0Aproblem%20of%20CP%20calibration%20when%20we%20only%20have%20access%20to%20a%20calibration%20set%20with%0Anoisy%20labels.%20We%20show%20how%20we%20can%20estimate%20the%20noise-free%20conformal%20threshold%0Abased%20on%20the%20noisy%20labeled%20data.%20We%20derive%20a%20finite%20sample%20coverage%20guarantee%0Afor%20uniform%20noise%20that%20remains%20effective%20even%20in%20tasks%20with%20a%20large%20number%20of%0Aclasses.%20We%20dub%20our%20approach%20Noise-Aware%20Conformal%20Prediction%20%28NACP%29.%20We%0Aillustrate%20the%20performance%20of%20the%20proposed%20results%20on%20several%20standard%20image%0Aclassification%20datasets%20with%20a%20large%20number%20of%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12749v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Prediction%2520of%2520Classifiers%2520with%2520Many%2520Classes%2520based%2520on%2520Noisy%250A%2520%2520Labels%26entry.906535625%3DCoby%2520Penso%2520and%2520Jacob%2520Goldberger%2520and%2520Ethan%2520Fetaya%26entry.1292438233%3D%2520%2520Conformal%2520Prediction%2520%2528CP%2529%2520controls%2520the%2520prediction%2520uncertainty%2520of%250Aclassification%2520systems%2520by%2520producing%2520a%2520small%2520prediction%2520set%252C%2520ensuring%2520a%250Apredetermined%2520probability%2520that%2520the%2520true%2520class%2520lies%2520within%2520this%2520set.%2520This%2520is%250Acommonly%2520done%2520by%2520defining%2520a%2520score%252C%2520based%2520on%2520the%2520model%2520predictions%252C%2520and%2520setting%250Aa%2520threshold%2520on%2520this%2520score%2520using%2520a%2520validation%2520set.%2520In%2520this%2520study%252C%2520we%2520address%2520the%250Aproblem%2520of%2520CP%2520calibration%2520when%2520we%2520only%2520have%2520access%2520to%2520a%2520calibration%2520set%2520with%250Anoisy%2520labels.%2520We%2520show%2520how%2520we%2520can%2520estimate%2520the%2520noise-free%2520conformal%2520threshold%250Abased%2520on%2520the%2520noisy%2520labeled%2520data.%2520We%2520derive%2520a%2520finite%2520sample%2520coverage%2520guarantee%250Afor%2520uniform%2520noise%2520that%2520remains%2520effective%2520even%2520in%2520tasks%2520with%2520a%2520large%2520number%2520of%250Aclasses.%2520We%2520dub%2520our%2520approach%2520Noise-Aware%2520Conformal%2520Prediction%2520%2528NACP%2529.%2520We%250Aillustrate%2520the%2520performance%2520of%2520the%2520proposed%2520results%2520on%2520several%2520standard%2520image%250Aclassification%2520datasets%2520with%2520a%2520large%2520number%2520of%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12749v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Prediction%20of%20Classifiers%20with%20Many%20Classes%20based%20on%20Noisy%0A%20%20Labels&entry.906535625=Coby%20Penso%20and%20Jacob%20Goldberger%20and%20Ethan%20Fetaya&entry.1292438233=%20%20Conformal%20Prediction%20%28CP%29%20controls%20the%20prediction%20uncertainty%20of%0Aclassification%20systems%20by%20producing%20a%20small%20prediction%20set%2C%20ensuring%20a%0Apredetermined%20probability%20that%20the%20true%20class%20lies%20within%20this%20set.%20This%20is%0Acommonly%20done%20by%20defining%20a%20score%2C%20based%20on%20the%20model%20predictions%2C%20and%20setting%0Aa%20threshold%20on%20this%20score%20using%20a%20validation%20set.%20In%20this%20study%2C%20we%20address%20the%0Aproblem%20of%20CP%20calibration%20when%20we%20only%20have%20access%20to%20a%20calibration%20set%20with%0Anoisy%20labels.%20We%20show%20how%20we%20can%20estimate%20the%20noise-free%20conformal%20threshold%0Abased%20on%20the%20noisy%20labeled%20data.%20We%20derive%20a%20finite%20sample%20coverage%20guarantee%0Afor%20uniform%20noise%20that%20remains%20effective%20even%20in%20tasks%20with%20a%20large%20number%20of%0Aclasses.%20We%20dub%20our%20approach%20Noise-Aware%20Conformal%20Prediction%20%28NACP%29.%20We%0Aillustrate%20the%20performance%20of%20the%20proposed%20results%20on%20several%20standard%20image%0Aclassification%20datasets%20with%20a%20large%20number%20of%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12749v2&entry.124074799=Read"},
{"title": "REACT: Real-time Efficient Attribute Clustering and Transfer for\n  Updatable 3D Scene Graph", "author": "Phuoc Nguyen and Francesco Verdoja and Ville Kyrki", "abstract": "  Modern-day autonomous robots need high-level map representations to perform\nsophisticated tasks. Recently, 3D scene graphs (3DSGs) have emerged as a\npromising alternative to traditional grid maps, blending efficient memory use\nand rich feature representation. However, most efforts to apply them have been\nlimited to static worlds. This work introduces REACT, a framework that\nefficiently performs real-time attribute clustering and transfer to relocalize\nobject nodes in a 3DSG. REACT employs a novel method for comparing object\ninstances using an embedding model trained on triplet loss, facilitating\ninstance clustering and matching. Experimental results demonstrate that REACT\nis able to relocalize objects while maintaining computational efficiency. The\nREACT framework's source code will be available as an open-source project,\npromoting further advancements in reusable and updatable 3DSGs.\n", "link": "http://arxiv.org/abs/2503.03412v2", "date": "2025-08-13", "relevancy": 2.3936, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6172}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6077}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REACT%3A%20Real-time%20Efficient%20Attribute%20Clustering%20and%20Transfer%20for%0A%20%20Updatable%203D%20Scene%20Graph&body=Title%3A%20REACT%3A%20Real-time%20Efficient%20Attribute%20Clustering%20and%20Transfer%20for%0A%20%20Updatable%203D%20Scene%20Graph%0AAuthor%3A%20Phuoc%20Nguyen%20and%20Francesco%20Verdoja%20and%20Ville%20Kyrki%0AAbstract%3A%20%20%20Modern-day%20autonomous%20robots%20need%20high-level%20map%20representations%20to%20perform%0Asophisticated%20tasks.%20Recently%2C%203D%20scene%20graphs%20%283DSGs%29%20have%20emerged%20as%20a%0Apromising%20alternative%20to%20traditional%20grid%20maps%2C%20blending%20efficient%20memory%20use%0Aand%20rich%20feature%20representation.%20However%2C%20most%20efforts%20to%20apply%20them%20have%20been%0Alimited%20to%20static%20worlds.%20This%20work%20introduces%20REACT%2C%20a%20framework%20that%0Aefficiently%20performs%20real-time%20attribute%20clustering%20and%20transfer%20to%20relocalize%0Aobject%20nodes%20in%20a%203DSG.%20REACT%20employs%20a%20novel%20method%20for%20comparing%20object%0Ainstances%20using%20an%20embedding%20model%20trained%20on%20triplet%20loss%2C%20facilitating%0Ainstance%20clustering%20and%20matching.%20Experimental%20results%20demonstrate%20that%20REACT%0Ais%20able%20to%20relocalize%20objects%20while%20maintaining%20computational%20efficiency.%20The%0AREACT%20framework%27s%20source%20code%20will%20be%20available%20as%20an%20open-source%20project%2C%0Apromoting%20further%20advancements%20in%20reusable%20and%20updatable%203DSGs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03412v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREACT%253A%2520Real-time%2520Efficient%2520Attribute%2520Clustering%2520and%2520Transfer%2520for%250A%2520%2520Updatable%25203D%2520Scene%2520Graph%26entry.906535625%3DPhuoc%2520Nguyen%2520and%2520Francesco%2520Verdoja%2520and%2520Ville%2520Kyrki%26entry.1292438233%3D%2520%2520Modern-day%2520autonomous%2520robots%2520need%2520high-level%2520map%2520representations%2520to%2520perform%250Asophisticated%2520tasks.%2520Recently%252C%25203D%2520scene%2520graphs%2520%25283DSGs%2529%2520have%2520emerged%2520as%2520a%250Apromising%2520alternative%2520to%2520traditional%2520grid%2520maps%252C%2520blending%2520efficient%2520memory%2520use%250Aand%2520rich%2520feature%2520representation.%2520However%252C%2520most%2520efforts%2520to%2520apply%2520them%2520have%2520been%250Alimited%2520to%2520static%2520worlds.%2520This%2520work%2520introduces%2520REACT%252C%2520a%2520framework%2520that%250Aefficiently%2520performs%2520real-time%2520attribute%2520clustering%2520and%2520transfer%2520to%2520relocalize%250Aobject%2520nodes%2520in%2520a%25203DSG.%2520REACT%2520employs%2520a%2520novel%2520method%2520for%2520comparing%2520object%250Ainstances%2520using%2520an%2520embedding%2520model%2520trained%2520on%2520triplet%2520loss%252C%2520facilitating%250Ainstance%2520clustering%2520and%2520matching.%2520Experimental%2520results%2520demonstrate%2520that%2520REACT%250Ais%2520able%2520to%2520relocalize%2520objects%2520while%2520maintaining%2520computational%2520efficiency.%2520The%250AREACT%2520framework%2527s%2520source%2520code%2520will%2520be%2520available%2520as%2520an%2520open-source%2520project%252C%250Apromoting%2520further%2520advancements%2520in%2520reusable%2520and%2520updatable%25203DSGs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03412v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REACT%3A%20Real-time%20Efficient%20Attribute%20Clustering%20and%20Transfer%20for%0A%20%20Updatable%203D%20Scene%20Graph&entry.906535625=Phuoc%20Nguyen%20and%20Francesco%20Verdoja%20and%20Ville%20Kyrki&entry.1292438233=%20%20Modern-day%20autonomous%20robots%20need%20high-level%20map%20representations%20to%20perform%0Asophisticated%20tasks.%20Recently%2C%203D%20scene%20graphs%20%283DSGs%29%20have%20emerged%20as%20a%0Apromising%20alternative%20to%20traditional%20grid%20maps%2C%20blending%20efficient%20memory%20use%0Aand%20rich%20feature%20representation.%20However%2C%20most%20efforts%20to%20apply%20them%20have%20been%0Alimited%20to%20static%20worlds.%20This%20work%20introduces%20REACT%2C%20a%20framework%20that%0Aefficiently%20performs%20real-time%20attribute%20clustering%20and%20transfer%20to%20relocalize%0Aobject%20nodes%20in%20a%203DSG.%20REACT%20employs%20a%20novel%20method%20for%20comparing%20object%0Ainstances%20using%20an%20embedding%20model%20trained%20on%20triplet%20loss%2C%20facilitating%0Ainstance%20clustering%20and%20matching.%20Experimental%20results%20demonstrate%20that%20REACT%0Ais%20able%20to%20relocalize%20objects%20while%20maintaining%20computational%20efficiency.%20The%0AREACT%20framework%27s%20source%20code%20will%20be%20available%20as%20an%20open-source%20project%2C%0Apromoting%20further%20advancements%20in%20reusable%20and%20updatable%203DSGs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03412v2&entry.124074799=Read"},
{"title": "MetaFold: Language-Guided Multi-Category Garment Folding Framework via\n  Trajectory Generation and Foundation Model", "author": "Haonan Chen and Junxiao Li and Ruihai Wu and Yiwei Liu and Yiwen Hou and Zhixuan Xu and Jingxiang Guo and Chongkai Gao and Zhenyu Wei and Shensi Xu and Jiaqi Huang and Lin Shao", "abstract": "  Garment folding is a common yet challenging task in robotic manipulation. The\ndeformability of garments leads to a vast state space and complex dynamics,\nwhich complicates precise and fine-grained manipulation. Previous approaches\noften rely on predefined key points or demonstrations, limiting their\ngeneralization across diverse garment categories. This paper presents a\nframework, MetaFold, that disentangles task planning from action prediction,\nlearning each independently to enhance model generalization. It employs\nlanguage-guided point cloud trajectory generation for task planning and a\nlow-level foundation model for action prediction. This structure facilitates\nmulti-category learning, enabling the model to adapt flexibly to various user\ninstructions and folding tasks. Experimental results demonstrate the\nsuperiority of our proposed framework. Supplementary materials are available on\nour website: https://meta-fold.github.io/.\n", "link": "http://arxiv.org/abs/2503.08372v2", "date": "2025-08-13", "relevancy": 2.3884, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6393}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5682}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaFold%3A%20Language-Guided%20Multi-Category%20Garment%20Folding%20Framework%20via%0A%20%20Trajectory%20Generation%20and%20Foundation%20Model&body=Title%3A%20MetaFold%3A%20Language-Guided%20Multi-Category%20Garment%20Folding%20Framework%20via%0A%20%20Trajectory%20Generation%20and%20Foundation%20Model%0AAuthor%3A%20Haonan%20Chen%20and%20Junxiao%20Li%20and%20Ruihai%20Wu%20and%20Yiwei%20Liu%20and%20Yiwen%20Hou%20and%20Zhixuan%20Xu%20and%20Jingxiang%20Guo%20and%20Chongkai%20Gao%20and%20Zhenyu%20Wei%20and%20Shensi%20Xu%20and%20Jiaqi%20Huang%20and%20Lin%20Shao%0AAbstract%3A%20%20%20Garment%20folding%20is%20a%20common%20yet%20challenging%20task%20in%20robotic%20manipulation.%20The%0Adeformability%20of%20garments%20leads%20to%20a%20vast%20state%20space%20and%20complex%20dynamics%2C%0Awhich%20complicates%20precise%20and%20fine-grained%20manipulation.%20Previous%20approaches%0Aoften%20rely%20on%20predefined%20key%20points%20or%20demonstrations%2C%20limiting%20their%0Ageneralization%20across%20diverse%20garment%20categories.%20This%20paper%20presents%20a%0Aframework%2C%20MetaFold%2C%20that%20disentangles%20task%20planning%20from%20action%20prediction%2C%0Alearning%20each%20independently%20to%20enhance%20model%20generalization.%20It%20employs%0Alanguage-guided%20point%20cloud%20trajectory%20generation%20for%20task%20planning%20and%20a%0Alow-level%20foundation%20model%20for%20action%20prediction.%20This%20structure%20facilitates%0Amulti-category%20learning%2C%20enabling%20the%20model%20to%20adapt%20flexibly%20to%20various%20user%0Ainstructions%20and%20folding%20tasks.%20Experimental%20results%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20framework.%20Supplementary%20materials%20are%20available%20on%0Aour%20website%3A%20https%3A//meta-fold.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08372v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaFold%253A%2520Language-Guided%2520Multi-Category%2520Garment%2520Folding%2520Framework%2520via%250A%2520%2520Trajectory%2520Generation%2520and%2520Foundation%2520Model%26entry.906535625%3DHaonan%2520Chen%2520and%2520Junxiao%2520Li%2520and%2520Ruihai%2520Wu%2520and%2520Yiwei%2520Liu%2520and%2520Yiwen%2520Hou%2520and%2520Zhixuan%2520Xu%2520and%2520Jingxiang%2520Guo%2520and%2520Chongkai%2520Gao%2520and%2520Zhenyu%2520Wei%2520and%2520Shensi%2520Xu%2520and%2520Jiaqi%2520Huang%2520and%2520Lin%2520Shao%26entry.1292438233%3D%2520%2520Garment%2520folding%2520is%2520a%2520common%2520yet%2520challenging%2520task%2520in%2520robotic%2520manipulation.%2520The%250Adeformability%2520of%2520garments%2520leads%2520to%2520a%2520vast%2520state%2520space%2520and%2520complex%2520dynamics%252C%250Awhich%2520complicates%2520precise%2520and%2520fine-grained%2520manipulation.%2520Previous%2520approaches%250Aoften%2520rely%2520on%2520predefined%2520key%2520points%2520or%2520demonstrations%252C%2520limiting%2520their%250Ageneralization%2520across%2520diverse%2520garment%2520categories.%2520This%2520paper%2520presents%2520a%250Aframework%252C%2520MetaFold%252C%2520that%2520disentangles%2520task%2520planning%2520from%2520action%2520prediction%252C%250Alearning%2520each%2520independently%2520to%2520enhance%2520model%2520generalization.%2520It%2520employs%250Alanguage-guided%2520point%2520cloud%2520trajectory%2520generation%2520for%2520task%2520planning%2520and%2520a%250Alow-level%2520foundation%2520model%2520for%2520action%2520prediction.%2520This%2520structure%2520facilitates%250Amulti-category%2520learning%252C%2520enabling%2520the%2520model%2520to%2520adapt%2520flexibly%2520to%2520various%2520user%250Ainstructions%2520and%2520folding%2520tasks.%2520Experimental%2520results%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520proposed%2520framework.%2520Supplementary%2520materials%2520are%2520available%2520on%250Aour%2520website%253A%2520https%253A//meta-fold.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08372v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaFold%3A%20Language-Guided%20Multi-Category%20Garment%20Folding%20Framework%20via%0A%20%20Trajectory%20Generation%20and%20Foundation%20Model&entry.906535625=Haonan%20Chen%20and%20Junxiao%20Li%20and%20Ruihai%20Wu%20and%20Yiwei%20Liu%20and%20Yiwen%20Hou%20and%20Zhixuan%20Xu%20and%20Jingxiang%20Guo%20and%20Chongkai%20Gao%20and%20Zhenyu%20Wei%20and%20Shensi%20Xu%20and%20Jiaqi%20Huang%20and%20Lin%20Shao&entry.1292438233=%20%20Garment%20folding%20is%20a%20common%20yet%20challenging%20task%20in%20robotic%20manipulation.%20The%0Adeformability%20of%20garments%20leads%20to%20a%20vast%20state%20space%20and%20complex%20dynamics%2C%0Awhich%20complicates%20precise%20and%20fine-grained%20manipulation.%20Previous%20approaches%0Aoften%20rely%20on%20predefined%20key%20points%20or%20demonstrations%2C%20limiting%20their%0Ageneralization%20across%20diverse%20garment%20categories.%20This%20paper%20presents%20a%0Aframework%2C%20MetaFold%2C%20that%20disentangles%20task%20planning%20from%20action%20prediction%2C%0Alearning%20each%20independently%20to%20enhance%20model%20generalization.%20It%20employs%0Alanguage-guided%20point%20cloud%20trajectory%20generation%20for%20task%20planning%20and%20a%0Alow-level%20foundation%20model%20for%20action%20prediction.%20This%20structure%20facilitates%0Amulti-category%20learning%2C%20enabling%20the%20model%20to%20adapt%20flexibly%20to%20various%20user%0Ainstructions%20and%20folding%20tasks.%20Experimental%20results%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20framework.%20Supplementary%20materials%20are%20available%20on%0Aour%20website%3A%20https%3A//meta-fold.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08372v2&entry.124074799=Read"},
{"title": "Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter\n  Estimation for Wheeled Humanoid Locomanipulation", "author": "Donghoon Baek and Amartya Purushottam and Jason J. Choi and Joao Ramos", "abstract": "  This paper presents an object-aware whole-body bilateral teleoperation\nframework for wheeled humanoid loco-manipulation. This framework combines\nwhole-body bilateral teleoperation with an online multi-stage object inertial\nparameter estimation module, which is the core technical contribution of this\nwork. The multi-stage process sequentially integrates a vision-based object\nsize estimator, an initial parameter guess generated by a large vision-language\nmodel (VLM), and a decoupled hierarchical sampling strategy. The visual size\nestimate and VLM prior offer a strong initial guess of the object's inertial\nparameters, significantly reducing the search space for sampling-based\nrefinement and improving the overall estimation speed. A hierarchical strategy\nfirst estimates mass and center of mass, then infers inertia from object size\nto ensure physically feasible parameters, while a decoupled multi-hypothesis\nscheme enhances robustness to VLM prior errors. Our estimator operates in\nparallel with high-fidelity simulation and hardware, enabling real-time online\nupdates. The estimated parameters are then used to update the wheeled\nhumanoid's equilibrium point, allowing the operator to focus more on locomotion\nand manipulation. This integration improves the haptic force feedback for\ndynamic synchronization, enabling more dynamic whole-body teleoperation. By\ncompensating for object dynamics using the estimated parameters, the framework\nalso improves manipulation tracking while preserving compliant behavior. We\nvalidate the system on a customized wheeled humanoid with a robotic gripper and\nhuman-machine interface, demonstrating real-time execution of lifting,\ndelivering, and releasing tasks with a payload weighing approximately one-third\nof the robot's body weight.\n", "link": "http://arxiv.org/abs/2508.09846v1", "date": "2025-08-13", "relevancy": 2.3693, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6323}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6289}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whole-Body%20Bilateral%20Teleoperation%20with%20Multi-Stage%20Object%20Parameter%0A%20%20Estimation%20for%20Wheeled%20Humanoid%20Locomanipulation&body=Title%3A%20Whole-Body%20Bilateral%20Teleoperation%20with%20Multi-Stage%20Object%20Parameter%0A%20%20Estimation%20for%20Wheeled%20Humanoid%20Locomanipulation%0AAuthor%3A%20Donghoon%20Baek%20and%20Amartya%20Purushottam%20and%20Jason%20J.%20Choi%20and%20Joao%20Ramos%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20object-aware%20whole-body%20bilateral%20teleoperation%0Aframework%20for%20wheeled%20humanoid%20loco-manipulation.%20This%20framework%20combines%0Awhole-body%20bilateral%20teleoperation%20with%20an%20online%20multi-stage%20object%20inertial%0Aparameter%20estimation%20module%2C%20which%20is%20the%20core%20technical%20contribution%20of%20this%0Awork.%20The%20multi-stage%20process%20sequentially%20integrates%20a%20vision-based%20object%0Asize%20estimator%2C%20an%20initial%20parameter%20guess%20generated%20by%20a%20large%20vision-language%0Amodel%20%28VLM%29%2C%20and%20a%20decoupled%20hierarchical%20sampling%20strategy.%20The%20visual%20size%0Aestimate%20and%20VLM%20prior%20offer%20a%20strong%20initial%20guess%20of%20the%20object%27s%20inertial%0Aparameters%2C%20significantly%20reducing%20the%20search%20space%20for%20sampling-based%0Arefinement%20and%20improving%20the%20overall%20estimation%20speed.%20A%20hierarchical%20strategy%0Afirst%20estimates%20mass%20and%20center%20of%20mass%2C%20then%20infers%20inertia%20from%20object%20size%0Ato%20ensure%20physically%20feasible%20parameters%2C%20while%20a%20decoupled%20multi-hypothesis%0Ascheme%20enhances%20robustness%20to%20VLM%20prior%20errors.%20Our%20estimator%20operates%20in%0Aparallel%20with%20high-fidelity%20simulation%20and%20hardware%2C%20enabling%20real-time%20online%0Aupdates.%20The%20estimated%20parameters%20are%20then%20used%20to%20update%20the%20wheeled%0Ahumanoid%27s%20equilibrium%20point%2C%20allowing%20the%20operator%20to%20focus%20more%20on%20locomotion%0Aand%20manipulation.%20This%20integration%20improves%20the%20haptic%20force%20feedback%20for%0Adynamic%20synchronization%2C%20enabling%20more%20dynamic%20whole-body%20teleoperation.%20By%0Acompensating%20for%20object%20dynamics%20using%20the%20estimated%20parameters%2C%20the%20framework%0Aalso%20improves%20manipulation%20tracking%20while%20preserving%20compliant%20behavior.%20We%0Avalidate%20the%20system%20on%20a%20customized%20wheeled%20humanoid%20with%20a%20robotic%20gripper%20and%0Ahuman-machine%20interface%2C%20demonstrating%20real-time%20execution%20of%20lifting%2C%0Adelivering%2C%20and%20releasing%20tasks%20with%20a%20payload%20weighing%20approximately%20one-third%0Aof%20the%20robot%27s%20body%20weight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhole-Body%2520Bilateral%2520Teleoperation%2520with%2520Multi-Stage%2520Object%2520Parameter%250A%2520%2520Estimation%2520for%2520Wheeled%2520Humanoid%2520Locomanipulation%26entry.906535625%3DDonghoon%2520Baek%2520and%2520Amartya%2520Purushottam%2520and%2520Jason%2520J.%2520Choi%2520and%2520Joao%2520Ramos%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520object-aware%2520whole-body%2520bilateral%2520teleoperation%250Aframework%2520for%2520wheeled%2520humanoid%2520loco-manipulation.%2520This%2520framework%2520combines%250Awhole-body%2520bilateral%2520teleoperation%2520with%2520an%2520online%2520multi-stage%2520object%2520inertial%250Aparameter%2520estimation%2520module%252C%2520which%2520is%2520the%2520core%2520technical%2520contribution%2520of%2520this%250Awork.%2520The%2520multi-stage%2520process%2520sequentially%2520integrates%2520a%2520vision-based%2520object%250Asize%2520estimator%252C%2520an%2520initial%2520parameter%2520guess%2520generated%2520by%2520a%2520large%2520vision-language%250Amodel%2520%2528VLM%2529%252C%2520and%2520a%2520decoupled%2520hierarchical%2520sampling%2520strategy.%2520The%2520visual%2520size%250Aestimate%2520and%2520VLM%2520prior%2520offer%2520a%2520strong%2520initial%2520guess%2520of%2520the%2520object%2527s%2520inertial%250Aparameters%252C%2520significantly%2520reducing%2520the%2520search%2520space%2520for%2520sampling-based%250Arefinement%2520and%2520improving%2520the%2520overall%2520estimation%2520speed.%2520A%2520hierarchical%2520strategy%250Afirst%2520estimates%2520mass%2520and%2520center%2520of%2520mass%252C%2520then%2520infers%2520inertia%2520from%2520object%2520size%250Ato%2520ensure%2520physically%2520feasible%2520parameters%252C%2520while%2520a%2520decoupled%2520multi-hypothesis%250Ascheme%2520enhances%2520robustness%2520to%2520VLM%2520prior%2520errors.%2520Our%2520estimator%2520operates%2520in%250Aparallel%2520with%2520high-fidelity%2520simulation%2520and%2520hardware%252C%2520enabling%2520real-time%2520online%250Aupdates.%2520The%2520estimated%2520parameters%2520are%2520then%2520used%2520to%2520update%2520the%2520wheeled%250Ahumanoid%2527s%2520equilibrium%2520point%252C%2520allowing%2520the%2520operator%2520to%2520focus%2520more%2520on%2520locomotion%250Aand%2520manipulation.%2520This%2520integration%2520improves%2520the%2520haptic%2520force%2520feedback%2520for%250Adynamic%2520synchronization%252C%2520enabling%2520more%2520dynamic%2520whole-body%2520teleoperation.%2520By%250Acompensating%2520for%2520object%2520dynamics%2520using%2520the%2520estimated%2520parameters%252C%2520the%2520framework%250Aalso%2520improves%2520manipulation%2520tracking%2520while%2520preserving%2520compliant%2520behavior.%2520We%250Avalidate%2520the%2520system%2520on%2520a%2520customized%2520wheeled%2520humanoid%2520with%2520a%2520robotic%2520gripper%2520and%250Ahuman-machine%2520interface%252C%2520demonstrating%2520real-time%2520execution%2520of%2520lifting%252C%250Adelivering%252C%2520and%2520releasing%2520tasks%2520with%2520a%2520payload%2520weighing%2520approximately%2520one-third%250Aof%2520the%2520robot%2527s%2520body%2520weight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whole-Body%20Bilateral%20Teleoperation%20with%20Multi-Stage%20Object%20Parameter%0A%20%20Estimation%20for%20Wheeled%20Humanoid%20Locomanipulation&entry.906535625=Donghoon%20Baek%20and%20Amartya%20Purushottam%20and%20Jason%20J.%20Choi%20and%20Joao%20Ramos&entry.1292438233=%20%20This%20paper%20presents%20an%20object-aware%20whole-body%20bilateral%20teleoperation%0Aframework%20for%20wheeled%20humanoid%20loco-manipulation.%20This%20framework%20combines%0Awhole-body%20bilateral%20teleoperation%20with%20an%20online%20multi-stage%20object%20inertial%0Aparameter%20estimation%20module%2C%20which%20is%20the%20core%20technical%20contribution%20of%20this%0Awork.%20The%20multi-stage%20process%20sequentially%20integrates%20a%20vision-based%20object%0Asize%20estimator%2C%20an%20initial%20parameter%20guess%20generated%20by%20a%20large%20vision-language%0Amodel%20%28VLM%29%2C%20and%20a%20decoupled%20hierarchical%20sampling%20strategy.%20The%20visual%20size%0Aestimate%20and%20VLM%20prior%20offer%20a%20strong%20initial%20guess%20of%20the%20object%27s%20inertial%0Aparameters%2C%20significantly%20reducing%20the%20search%20space%20for%20sampling-based%0Arefinement%20and%20improving%20the%20overall%20estimation%20speed.%20A%20hierarchical%20strategy%0Afirst%20estimates%20mass%20and%20center%20of%20mass%2C%20then%20infers%20inertia%20from%20object%20size%0Ato%20ensure%20physically%20feasible%20parameters%2C%20while%20a%20decoupled%20multi-hypothesis%0Ascheme%20enhances%20robustness%20to%20VLM%20prior%20errors.%20Our%20estimator%20operates%20in%0Aparallel%20with%20high-fidelity%20simulation%20and%20hardware%2C%20enabling%20real-time%20online%0Aupdates.%20The%20estimated%20parameters%20are%20then%20used%20to%20update%20the%20wheeled%0Ahumanoid%27s%20equilibrium%20point%2C%20allowing%20the%20operator%20to%20focus%20more%20on%20locomotion%0Aand%20manipulation.%20This%20integration%20improves%20the%20haptic%20force%20feedback%20for%0Adynamic%20synchronization%2C%20enabling%20more%20dynamic%20whole-body%20teleoperation.%20By%0Acompensating%20for%20object%20dynamics%20using%20the%20estimated%20parameters%2C%20the%20framework%0Aalso%20improves%20manipulation%20tracking%20while%20preserving%20compliant%20behavior.%20We%0Avalidate%20the%20system%20on%20a%20customized%20wheeled%20humanoid%20with%20a%20robotic%20gripper%20and%0Ahuman-machine%20interface%2C%20demonstrating%20real-time%20execution%20of%20lifting%2C%0Adelivering%2C%20and%20releasing%20tasks%20with%20a%20payload%20weighing%20approximately%20one-third%0Aof%20the%20robot%27s%20body%20weight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09846v1&entry.124074799=Read"},
{"title": "GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid\n  Imitation", "author": "Yifei Yao and Chengyuan Luo and Jiaheng Du and Wentao He and Jun-Guo Lu", "abstract": "  The creation of human-like humanoid robots is hindered by a fundamental\nfragmentation: data processing and learning algorithms are rarely universal\nacross different robot morphologies. This paper introduces the Generalized\nBehavior Cloning (GBC) framework, a comprehensive and unified solution designed\nto solve this end-to-end challenge. GBC establishes a complete pathway from\nhuman motion to robot action through three synergistic innovations. First, an\nadaptive data pipeline leverages a differentiable IK network to automatically\nretarget any human MoCap data to any humanoid. Building on this foundation, our\nnovel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust,\nhigh-fidelity imitation policies. To complete the ecosystem, the entire\nframework is delivered as an efficient, open-source platform based on Isaac\nLab, empowering the community to deploy the full workflow via simple\nconfiguration scripts. We validate the power and generality of GBC by training\npolicies on multiple heterogeneous humanoids, demonstrating excellent\nperformance and transfer to novel motions. This work establishes the first\npractical and unified pathway for creating truly generalized humanoid\ncontrollers.\n", "link": "http://arxiv.org/abs/2508.09960v1", "date": "2025-08-13", "relevancy": 2.3636, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6051}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5863}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GBC%3A%20Generalized%20Behavior-Cloning%20Framework%20for%20Whole-Body%20Humanoid%0A%20%20Imitation&body=Title%3A%20GBC%3A%20Generalized%20Behavior-Cloning%20Framework%20for%20Whole-Body%20Humanoid%0A%20%20Imitation%0AAuthor%3A%20Yifei%20Yao%20and%20Chengyuan%20Luo%20and%20Jiaheng%20Du%20and%20Wentao%20He%20and%20Jun-Guo%20Lu%0AAbstract%3A%20%20%20The%20creation%20of%20human-like%20humanoid%20robots%20is%20hindered%20by%20a%20fundamental%0Afragmentation%3A%20data%20processing%20and%20learning%20algorithms%20are%20rarely%20universal%0Aacross%20different%20robot%20morphologies.%20This%20paper%20introduces%20the%20Generalized%0ABehavior%20Cloning%20%28GBC%29%20framework%2C%20a%20comprehensive%20and%20unified%20solution%20designed%0Ato%20solve%20this%20end-to-end%20challenge.%20GBC%20establishes%20a%20complete%20pathway%20from%0Ahuman%20motion%20to%20robot%20action%20through%20three%20synergistic%20innovations.%20First%2C%20an%0Aadaptive%20data%20pipeline%20leverages%20a%20differentiable%20IK%20network%20to%20automatically%0Aretarget%20any%20human%20MoCap%20data%20to%20any%20humanoid.%20Building%20on%20this%20foundation%2C%20our%0Anovel%20DAgger-MMPPO%20algorithm%20with%20its%20MMTransformer%20architecture%20learns%20robust%2C%0Ahigh-fidelity%20imitation%20policies.%20To%20complete%20the%20ecosystem%2C%20the%20entire%0Aframework%20is%20delivered%20as%20an%20efficient%2C%20open-source%20platform%20based%20on%20Isaac%0ALab%2C%20empowering%20the%20community%20to%20deploy%20the%20full%20workflow%20via%20simple%0Aconfiguration%20scripts.%20We%20validate%20the%20power%20and%20generality%20of%20GBC%20by%20training%0Apolicies%20on%20multiple%20heterogeneous%20humanoids%2C%20demonstrating%20excellent%0Aperformance%20and%20transfer%20to%20novel%20motions.%20This%20work%20establishes%20the%20first%0Apractical%20and%20unified%20pathway%20for%20creating%20truly%20generalized%20humanoid%0Acontrollers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGBC%253A%2520Generalized%2520Behavior-Cloning%2520Framework%2520for%2520Whole-Body%2520Humanoid%250A%2520%2520Imitation%26entry.906535625%3DYifei%2520Yao%2520and%2520Chengyuan%2520Luo%2520and%2520Jiaheng%2520Du%2520and%2520Wentao%2520He%2520and%2520Jun-Guo%2520Lu%26entry.1292438233%3D%2520%2520The%2520creation%2520of%2520human-like%2520humanoid%2520robots%2520is%2520hindered%2520by%2520a%2520fundamental%250Afragmentation%253A%2520data%2520processing%2520and%2520learning%2520algorithms%2520are%2520rarely%2520universal%250Aacross%2520different%2520robot%2520morphologies.%2520This%2520paper%2520introduces%2520the%2520Generalized%250ABehavior%2520Cloning%2520%2528GBC%2529%2520framework%252C%2520a%2520comprehensive%2520and%2520unified%2520solution%2520designed%250Ato%2520solve%2520this%2520end-to-end%2520challenge.%2520GBC%2520establishes%2520a%2520complete%2520pathway%2520from%250Ahuman%2520motion%2520to%2520robot%2520action%2520through%2520three%2520synergistic%2520innovations.%2520First%252C%2520an%250Aadaptive%2520data%2520pipeline%2520leverages%2520a%2520differentiable%2520IK%2520network%2520to%2520automatically%250Aretarget%2520any%2520human%2520MoCap%2520data%2520to%2520any%2520humanoid.%2520Building%2520on%2520this%2520foundation%252C%2520our%250Anovel%2520DAgger-MMPPO%2520algorithm%2520with%2520its%2520MMTransformer%2520architecture%2520learns%2520robust%252C%250Ahigh-fidelity%2520imitation%2520policies.%2520To%2520complete%2520the%2520ecosystem%252C%2520the%2520entire%250Aframework%2520is%2520delivered%2520as%2520an%2520efficient%252C%2520open-source%2520platform%2520based%2520on%2520Isaac%250ALab%252C%2520empowering%2520the%2520community%2520to%2520deploy%2520the%2520full%2520workflow%2520via%2520simple%250Aconfiguration%2520scripts.%2520We%2520validate%2520the%2520power%2520and%2520generality%2520of%2520GBC%2520by%2520training%250Apolicies%2520on%2520multiple%2520heterogeneous%2520humanoids%252C%2520demonstrating%2520excellent%250Aperformance%2520and%2520transfer%2520to%2520novel%2520motions.%2520This%2520work%2520establishes%2520the%2520first%250Apractical%2520and%2520unified%2520pathway%2520for%2520creating%2520truly%2520generalized%2520humanoid%250Acontrollers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GBC%3A%20Generalized%20Behavior-Cloning%20Framework%20for%20Whole-Body%20Humanoid%0A%20%20Imitation&entry.906535625=Yifei%20Yao%20and%20Chengyuan%20Luo%20and%20Jiaheng%20Du%20and%20Wentao%20He%20and%20Jun-Guo%20Lu&entry.1292438233=%20%20The%20creation%20of%20human-like%20humanoid%20robots%20is%20hindered%20by%20a%20fundamental%0Afragmentation%3A%20data%20processing%20and%20learning%20algorithms%20are%20rarely%20universal%0Aacross%20different%20robot%20morphologies.%20This%20paper%20introduces%20the%20Generalized%0ABehavior%20Cloning%20%28GBC%29%20framework%2C%20a%20comprehensive%20and%20unified%20solution%20designed%0Ato%20solve%20this%20end-to-end%20challenge.%20GBC%20establishes%20a%20complete%20pathway%20from%0Ahuman%20motion%20to%20robot%20action%20through%20three%20synergistic%20innovations.%20First%2C%20an%0Aadaptive%20data%20pipeline%20leverages%20a%20differentiable%20IK%20network%20to%20automatically%0Aretarget%20any%20human%20MoCap%20data%20to%20any%20humanoid.%20Building%20on%20this%20foundation%2C%20our%0Anovel%20DAgger-MMPPO%20algorithm%20with%20its%20MMTransformer%20architecture%20learns%20robust%2C%0Ahigh-fidelity%20imitation%20policies.%20To%20complete%20the%20ecosystem%2C%20the%20entire%0Aframework%20is%20delivered%20as%20an%20efficient%2C%20open-source%20platform%20based%20on%20Isaac%0ALab%2C%20empowering%20the%20community%20to%20deploy%20the%20full%20workflow%20via%20simple%0Aconfiguration%20scripts.%20We%20validate%20the%20power%20and%20generality%20of%20GBC%20by%20training%0Apolicies%20on%20multiple%20heterogeneous%20humanoids%2C%20demonstrating%20excellent%0Aperformance%20and%20transfer%20to%20novel%20motions.%20This%20work%20establishes%20the%20first%0Apractical%20and%20unified%20pathway%20for%20creating%20truly%20generalized%20humanoid%0Acontrollers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09960v1&entry.124074799=Read"},
{"title": "Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes", "author": "Yuekun Wu and Yik Lung Pang and Andrea Cavallaro and Changjae Oh", "abstract": "  Human-robot teaming (HRT) systems often rely on large-scale datasets of human\nand robot interactions, especially for close-proximity collaboration tasks such\nas human-robot handovers. Learning robot manipulation policies from raw,\nreal-world image data requires a large number of robot-action trials in the\nphysical environment. Although simulation training offers a cost-effective\nalternative, the visual domain gap between simulation and robot workspace\nremains a major limitation. We introduce a method for training HRT policies,\nfocusing on human-to-robot handovers, solely from RGB images without the need\nfor real-robot training or real-robot data collection. The goal is to enable\nthe robot to reliably receive objects from a human with stable grasping while\navoiding collisions with the human hand. The proposed policy learner leverages\nsparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes\nto generate robot demonstrations containing image-action pairs captured with a\ncamera mounted on the robot gripper. As a result, the simulated camera pose\nchanges in the reconstructed scene can be directly translated into gripper pose\nchanges. Experiments in both Gaussian Splatting reconstructed scene and\nreal-world human-to-robot handover experiments demonstrate that our method\nserves as a new and effective representation for the human-to-robot handover\ntask, contributing to more seamless and robust HRT.\n", "link": "http://arxiv.org/abs/2508.09855v1", "date": "2025-08-13", "relevancy": 2.3597, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5919}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5909}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Human-Robot%20Teaming%3A%20Learning%20Handover%20Behaviors%20from%203D%20Scenes&body=Title%3A%20Toward%20Human-Robot%20Teaming%3A%20Learning%20Handover%20Behaviors%20from%203D%20Scenes%0AAuthor%3A%20Yuekun%20Wu%20and%20Yik%20Lung%20Pang%20and%20Andrea%20Cavallaro%20and%20Changjae%20Oh%0AAbstract%3A%20%20%20Human-robot%20teaming%20%28HRT%29%20systems%20often%20rely%20on%20large-scale%20datasets%20of%20human%0Aand%20robot%20interactions%2C%20especially%20for%20close-proximity%20collaboration%20tasks%20such%0Aas%20human-robot%20handovers.%20Learning%20robot%20manipulation%20policies%20from%20raw%2C%0Areal-world%20image%20data%20requires%20a%20large%20number%20of%20robot-action%20trials%20in%20the%0Aphysical%20environment.%20Although%20simulation%20training%20offers%20a%20cost-effective%0Aalternative%2C%20the%20visual%20domain%20gap%20between%20simulation%20and%20robot%20workspace%0Aremains%20a%20major%20limitation.%20We%20introduce%20a%20method%20for%20training%20HRT%20policies%2C%0Afocusing%20on%20human-to-robot%20handovers%2C%20solely%20from%20RGB%20images%20without%20the%20need%0Afor%20real-robot%20training%20or%20real-robot%20data%20collection.%20The%20goal%20is%20to%20enable%0Athe%20robot%20to%20reliably%20receive%20objects%20from%20a%20human%20with%20stable%20grasping%20while%0Aavoiding%20collisions%20with%20the%20human%20hand.%20The%20proposed%20policy%20learner%20leverages%0Asparse-view%20Gaussian%20Splatting%20reconstruction%20of%20human-to-robot%20handover%20scenes%0Ato%20generate%20robot%20demonstrations%20containing%20image-action%20pairs%20captured%20with%20a%0Acamera%20mounted%20on%20the%20robot%20gripper.%20As%20a%20result%2C%20the%20simulated%20camera%20pose%0Achanges%20in%20the%20reconstructed%20scene%20can%20be%20directly%20translated%20into%20gripper%20pose%0Achanges.%20Experiments%20in%20both%20Gaussian%20Splatting%20reconstructed%20scene%20and%0Areal-world%20human-to-robot%20handover%20experiments%20demonstrate%20that%20our%20method%0Aserves%20as%20a%20new%20and%20effective%20representation%20for%20the%20human-to-robot%20handover%0Atask%2C%20contributing%20to%20more%20seamless%20and%20robust%20HRT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Human-Robot%2520Teaming%253A%2520Learning%2520Handover%2520Behaviors%2520from%25203D%2520Scenes%26entry.906535625%3DYuekun%2520Wu%2520and%2520Yik%2520Lung%2520Pang%2520and%2520Andrea%2520Cavallaro%2520and%2520Changjae%2520Oh%26entry.1292438233%3D%2520%2520Human-robot%2520teaming%2520%2528HRT%2529%2520systems%2520often%2520rely%2520on%2520large-scale%2520datasets%2520of%2520human%250Aand%2520robot%2520interactions%252C%2520especially%2520for%2520close-proximity%2520collaboration%2520tasks%2520such%250Aas%2520human-robot%2520handovers.%2520Learning%2520robot%2520manipulation%2520policies%2520from%2520raw%252C%250Areal-world%2520image%2520data%2520requires%2520a%2520large%2520number%2520of%2520robot-action%2520trials%2520in%2520the%250Aphysical%2520environment.%2520Although%2520simulation%2520training%2520offers%2520a%2520cost-effective%250Aalternative%252C%2520the%2520visual%2520domain%2520gap%2520between%2520simulation%2520and%2520robot%2520workspace%250Aremains%2520a%2520major%2520limitation.%2520We%2520introduce%2520a%2520method%2520for%2520training%2520HRT%2520policies%252C%250Afocusing%2520on%2520human-to-robot%2520handovers%252C%2520solely%2520from%2520RGB%2520images%2520without%2520the%2520need%250Afor%2520real-robot%2520training%2520or%2520real-robot%2520data%2520collection.%2520The%2520goal%2520is%2520to%2520enable%250Athe%2520robot%2520to%2520reliably%2520receive%2520objects%2520from%2520a%2520human%2520with%2520stable%2520grasping%2520while%250Aavoiding%2520collisions%2520with%2520the%2520human%2520hand.%2520The%2520proposed%2520policy%2520learner%2520leverages%250Asparse-view%2520Gaussian%2520Splatting%2520reconstruction%2520of%2520human-to-robot%2520handover%2520scenes%250Ato%2520generate%2520robot%2520demonstrations%2520containing%2520image-action%2520pairs%2520captured%2520with%2520a%250Acamera%2520mounted%2520on%2520the%2520robot%2520gripper.%2520As%2520a%2520result%252C%2520the%2520simulated%2520camera%2520pose%250Achanges%2520in%2520the%2520reconstructed%2520scene%2520can%2520be%2520directly%2520translated%2520into%2520gripper%2520pose%250Achanges.%2520Experiments%2520in%2520both%2520Gaussian%2520Splatting%2520reconstructed%2520scene%2520and%250Areal-world%2520human-to-robot%2520handover%2520experiments%2520demonstrate%2520that%2520our%2520method%250Aserves%2520as%2520a%2520new%2520and%2520effective%2520representation%2520for%2520the%2520human-to-robot%2520handover%250Atask%252C%2520contributing%2520to%2520more%2520seamless%2520and%2520robust%2520HRT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Human-Robot%20Teaming%3A%20Learning%20Handover%20Behaviors%20from%203D%20Scenes&entry.906535625=Yuekun%20Wu%20and%20Yik%20Lung%20Pang%20and%20Andrea%20Cavallaro%20and%20Changjae%20Oh&entry.1292438233=%20%20Human-robot%20teaming%20%28HRT%29%20systems%20often%20rely%20on%20large-scale%20datasets%20of%20human%0Aand%20robot%20interactions%2C%20especially%20for%20close-proximity%20collaboration%20tasks%20such%0Aas%20human-robot%20handovers.%20Learning%20robot%20manipulation%20policies%20from%20raw%2C%0Areal-world%20image%20data%20requires%20a%20large%20number%20of%20robot-action%20trials%20in%20the%0Aphysical%20environment.%20Although%20simulation%20training%20offers%20a%20cost-effective%0Aalternative%2C%20the%20visual%20domain%20gap%20between%20simulation%20and%20robot%20workspace%0Aremains%20a%20major%20limitation.%20We%20introduce%20a%20method%20for%20training%20HRT%20policies%2C%0Afocusing%20on%20human-to-robot%20handovers%2C%20solely%20from%20RGB%20images%20without%20the%20need%0Afor%20real-robot%20training%20or%20real-robot%20data%20collection.%20The%20goal%20is%20to%20enable%0Athe%20robot%20to%20reliably%20receive%20objects%20from%20a%20human%20with%20stable%20grasping%20while%0Aavoiding%20collisions%20with%20the%20human%20hand.%20The%20proposed%20policy%20learner%20leverages%0Asparse-view%20Gaussian%20Splatting%20reconstruction%20of%20human-to-robot%20handover%20scenes%0Ato%20generate%20robot%20demonstrations%20containing%20image-action%20pairs%20captured%20with%20a%0Acamera%20mounted%20on%20the%20robot%20gripper.%20As%20a%20result%2C%20the%20simulated%20camera%20pose%0Achanges%20in%20the%20reconstructed%20scene%20can%20be%20directly%20translated%20into%20gripper%20pose%0Achanges.%20Experiments%20in%20both%20Gaussian%20Splatting%20reconstructed%20scene%20and%0Areal-world%20human-to-robot%20handover%20experiments%20demonstrate%20that%20our%20method%0Aserves%20as%20a%20new%20and%20effective%20representation%20for%20the%20human-to-robot%20handover%0Atask%2C%20contributing%20to%20more%20seamless%20and%20robust%20HRT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09855v1&entry.124074799=Read"},
{"title": "Generative Modeling with Multi-Instance Reward Learning for E-commerce\n  Creative Optimization", "author": "Qiaolei Gu and Yu Li and DingYi Zeng and Lu Wang and Ming Pang and Changping Peng and Zhangang Lin and Ching Law and Jingping Shao", "abstract": "  In e-commerce advertising, selecting the most compelling combination of\ncreative elements -- such as titles, images, and highlights -- is critical for\ncapturing user attention and driving conversions. However, existing methods\noften evaluate creative components individually, failing to navigate the\nexponentially large search space of possible combinations. To address this\nchallenge, we propose a novel framework named GenCO that integrates generative\nmodeling with multi-instance reward learning. Our unified two-stage\narchitecture first employs a generative model to efficiently produce a diverse\nset of creative combinations. This generative process is optimized with\nreinforcement learning, enabling the model to effectively explore and refine\nits selections. Next, to overcome the challenge of sparse user feedback, a\nmulti-instance learning model attributes combination-level rewards, such as\nclicks, to the individual creative elements. This allows the reward model to\nprovide a more accurate feedback signal, which in turn guides the generative\nmodel toward creating more effective combinations. Deployed on a leading\ne-commerce platform, our approach has significantly increased advertising\nrevenue, demonstrating its practical value. Additionally, we are releasing a\nlarge-scale industrial dataset to facilitate further research in this important\ndomain.\n", "link": "http://arxiv.org/abs/2508.09730v1", "date": "2025-08-13", "relevancy": 2.3177, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5932}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5926}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Modeling%20with%20Multi-Instance%20Reward%20Learning%20for%20E-commerce%0A%20%20Creative%20Optimization&body=Title%3A%20Generative%20Modeling%20with%20Multi-Instance%20Reward%20Learning%20for%20E-commerce%0A%20%20Creative%20Optimization%0AAuthor%3A%20Qiaolei%20Gu%20and%20Yu%20Li%20and%20DingYi%20Zeng%20and%20Lu%20Wang%20and%20Ming%20Pang%20and%20Changping%20Peng%20and%20Zhangang%20Lin%20and%20Ching%20Law%20and%20Jingping%20Shao%0AAbstract%3A%20%20%20In%20e-commerce%20advertising%2C%20selecting%20the%20most%20compelling%20combination%20of%0Acreative%20elements%20--%20such%20as%20titles%2C%20images%2C%20and%20highlights%20--%20is%20critical%20for%0Acapturing%20user%20attention%20and%20driving%20conversions.%20However%2C%20existing%20methods%0Aoften%20evaluate%20creative%20components%20individually%2C%20failing%20to%20navigate%20the%0Aexponentially%20large%20search%20space%20of%20possible%20combinations.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20novel%20framework%20named%20GenCO%20that%20integrates%20generative%0Amodeling%20with%20multi-instance%20reward%20learning.%20Our%20unified%20two-stage%0Aarchitecture%20first%20employs%20a%20generative%20model%20to%20efficiently%20produce%20a%20diverse%0Aset%20of%20creative%20combinations.%20This%20generative%20process%20is%20optimized%20with%0Areinforcement%20learning%2C%20enabling%20the%20model%20to%20effectively%20explore%20and%20refine%0Aits%20selections.%20Next%2C%20to%20overcome%20the%20challenge%20of%20sparse%20user%20feedback%2C%20a%0Amulti-instance%20learning%20model%20attributes%20combination-level%20rewards%2C%20such%20as%0Aclicks%2C%20to%20the%20individual%20creative%20elements.%20This%20allows%20the%20reward%20model%20to%0Aprovide%20a%20more%20accurate%20feedback%20signal%2C%20which%20in%20turn%20guides%20the%20generative%0Amodel%20toward%20creating%20more%20effective%20combinations.%20Deployed%20on%20a%20leading%0Ae-commerce%20platform%2C%20our%20approach%20has%20significantly%20increased%20advertising%0Arevenue%2C%20demonstrating%20its%20practical%20value.%20Additionally%2C%20we%20are%20releasing%20a%0Alarge-scale%20industrial%20dataset%20to%20facilitate%20further%20research%20in%20this%20important%0Adomain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Modeling%2520with%2520Multi-Instance%2520Reward%2520Learning%2520for%2520E-commerce%250A%2520%2520Creative%2520Optimization%26entry.906535625%3DQiaolei%2520Gu%2520and%2520Yu%2520Li%2520and%2520DingYi%2520Zeng%2520and%2520Lu%2520Wang%2520and%2520Ming%2520Pang%2520and%2520Changping%2520Peng%2520and%2520Zhangang%2520Lin%2520and%2520Ching%2520Law%2520and%2520Jingping%2520Shao%26entry.1292438233%3D%2520%2520In%2520e-commerce%2520advertising%252C%2520selecting%2520the%2520most%2520compelling%2520combination%2520of%250Acreative%2520elements%2520--%2520such%2520as%2520titles%252C%2520images%252C%2520and%2520highlights%2520--%2520is%2520critical%2520for%250Acapturing%2520user%2520attention%2520and%2520driving%2520conversions.%2520However%252C%2520existing%2520methods%250Aoften%2520evaluate%2520creative%2520components%2520individually%252C%2520failing%2520to%2520navigate%2520the%250Aexponentially%2520large%2520search%2520space%2520of%2520possible%2520combinations.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520a%2520novel%2520framework%2520named%2520GenCO%2520that%2520integrates%2520generative%250Amodeling%2520with%2520multi-instance%2520reward%2520learning.%2520Our%2520unified%2520two-stage%250Aarchitecture%2520first%2520employs%2520a%2520generative%2520model%2520to%2520efficiently%2520produce%2520a%2520diverse%250Aset%2520of%2520creative%2520combinations.%2520This%2520generative%2520process%2520is%2520optimized%2520with%250Areinforcement%2520learning%252C%2520enabling%2520the%2520model%2520to%2520effectively%2520explore%2520and%2520refine%250Aits%2520selections.%2520Next%252C%2520to%2520overcome%2520the%2520challenge%2520of%2520sparse%2520user%2520feedback%252C%2520a%250Amulti-instance%2520learning%2520model%2520attributes%2520combination-level%2520rewards%252C%2520such%2520as%250Aclicks%252C%2520to%2520the%2520individual%2520creative%2520elements.%2520This%2520allows%2520the%2520reward%2520model%2520to%250Aprovide%2520a%2520more%2520accurate%2520feedback%2520signal%252C%2520which%2520in%2520turn%2520guides%2520the%2520generative%250Amodel%2520toward%2520creating%2520more%2520effective%2520combinations.%2520Deployed%2520on%2520a%2520leading%250Ae-commerce%2520platform%252C%2520our%2520approach%2520has%2520significantly%2520increased%2520advertising%250Arevenue%252C%2520demonstrating%2520its%2520practical%2520value.%2520Additionally%252C%2520we%2520are%2520releasing%2520a%250Alarge-scale%2520industrial%2520dataset%2520to%2520facilitate%2520further%2520research%2520in%2520this%2520important%250Adomain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Modeling%20with%20Multi-Instance%20Reward%20Learning%20for%20E-commerce%0A%20%20Creative%20Optimization&entry.906535625=Qiaolei%20Gu%20and%20Yu%20Li%20and%20DingYi%20Zeng%20and%20Lu%20Wang%20and%20Ming%20Pang%20and%20Changping%20Peng%20and%20Zhangang%20Lin%20and%20Ching%20Law%20and%20Jingping%20Shao&entry.1292438233=%20%20In%20e-commerce%20advertising%2C%20selecting%20the%20most%20compelling%20combination%20of%0Acreative%20elements%20--%20such%20as%20titles%2C%20images%2C%20and%20highlights%20--%20is%20critical%20for%0Acapturing%20user%20attention%20and%20driving%20conversions.%20However%2C%20existing%20methods%0Aoften%20evaluate%20creative%20components%20individually%2C%20failing%20to%20navigate%20the%0Aexponentially%20large%20search%20space%20of%20possible%20combinations.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20novel%20framework%20named%20GenCO%20that%20integrates%20generative%0Amodeling%20with%20multi-instance%20reward%20learning.%20Our%20unified%20two-stage%0Aarchitecture%20first%20employs%20a%20generative%20model%20to%20efficiently%20produce%20a%20diverse%0Aset%20of%20creative%20combinations.%20This%20generative%20process%20is%20optimized%20with%0Areinforcement%20learning%2C%20enabling%20the%20model%20to%20effectively%20explore%20and%20refine%0Aits%20selections.%20Next%2C%20to%20overcome%20the%20challenge%20of%20sparse%20user%20feedback%2C%20a%0Amulti-instance%20learning%20model%20attributes%20combination-level%20rewards%2C%20such%20as%0Aclicks%2C%20to%20the%20individual%20creative%20elements.%20This%20allows%20the%20reward%20model%20to%0Aprovide%20a%20more%20accurate%20feedback%20signal%2C%20which%20in%20turn%20guides%20the%20generative%0Amodel%20toward%20creating%20more%20effective%20combinations.%20Deployed%20on%20a%20leading%0Ae-commerce%20platform%2C%20our%20approach%20has%20significantly%20increased%20advertising%0Arevenue%2C%20demonstrating%20its%20practical%20value.%20Additionally%2C%20we%20are%20releasing%20a%0Alarge-scale%20industrial%20dataset%20to%20facilitate%20further%20research%20in%20this%20important%0Adomain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09730v1&entry.124074799=Read"},
{"title": "COME: Dual Structure-Semantic Learning with Collaborative MoE for\n  Universal Lesion Detection Across Heterogeneous Ultrasound Datasets", "author": "Lingyu Chen and Yawen Zeng and Yue Wang and Peng Wan and Guo-chen Ning and Hongen Liao and Daoqiang Zhang and Fang Chen", "abstract": "  Conventional single-dataset training often fails with new data distributions,\nespecially in ultrasound (US) image analysis due to limited data, acoustic\nshadows, and speckle noise. Therefore, constructing a universal framework for\nmulti-heterogeneous US datasets is imperative. However, a key challenge arises:\nhow to effectively mitigate inter-dataset interference while preserving\ndataset-specific discriminative features for robust downstream task? Previous\napproaches utilize either a single source-specific decoder or a domain\nadaptation strategy, but these methods experienced a decline in performance\nwhen applied to other domains. Considering this, we propose a Universal\nCollaborative Mixture of Heterogeneous Source-Specific Experts (COME).\nSpecifically, COME establishes dual structure-semantic shared experts that\ncreate a universal representation space and then collaborate with\nsource-specific experts to extract discriminative features through providing\ncomplementary features. This design enables robust generalization by leveraging\ncross-datasets experience distributions and providing universal US priors for\nsmall-batch or unseen data scenarios. Extensive experiments under three\nevaluation modes (single-dataset, intra-organ, and inter-organ integration\ndatasets) demonstrate COME's superiority, achieving significant mean AP\nimprovements over state-of-the-art methods. Our project is available at:\nhttps://universalcome.github.io/UniversalCOME/.\n", "link": "http://arxiv.org/abs/2508.09886v1", "date": "2025-08-13", "relevancy": 2.3089, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5949}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COME%3A%20Dual%20Structure-Semantic%20Learning%20with%20Collaborative%20MoE%20for%0A%20%20Universal%20Lesion%20Detection%20Across%20Heterogeneous%20Ultrasound%20Datasets&body=Title%3A%20COME%3A%20Dual%20Structure-Semantic%20Learning%20with%20Collaborative%20MoE%20for%0A%20%20Universal%20Lesion%20Detection%20Across%20Heterogeneous%20Ultrasound%20Datasets%0AAuthor%3A%20Lingyu%20Chen%20and%20Yawen%20Zeng%20and%20Yue%20Wang%20and%20Peng%20Wan%20and%20Guo-chen%20Ning%20and%20Hongen%20Liao%20and%20Daoqiang%20Zhang%20and%20Fang%20Chen%0AAbstract%3A%20%20%20Conventional%20single-dataset%20training%20often%20fails%20with%20new%20data%20distributions%2C%0Aespecially%20in%20ultrasound%20%28US%29%20image%20analysis%20due%20to%20limited%20data%2C%20acoustic%0Ashadows%2C%20and%20speckle%20noise.%20Therefore%2C%20constructing%20a%20universal%20framework%20for%0Amulti-heterogeneous%20US%20datasets%20is%20imperative.%20However%2C%20a%20key%20challenge%20arises%3A%0Ahow%20to%20effectively%20mitigate%20inter-dataset%20interference%20while%20preserving%0Adataset-specific%20discriminative%20features%20for%20robust%20downstream%20task%3F%20Previous%0Aapproaches%20utilize%20either%20a%20single%20source-specific%20decoder%20or%20a%20domain%0Aadaptation%20strategy%2C%20but%20these%20methods%20experienced%20a%20decline%20in%20performance%0Awhen%20applied%20to%20other%20domains.%20Considering%20this%2C%20we%20propose%20a%20Universal%0ACollaborative%20Mixture%20of%20Heterogeneous%20Source-Specific%20Experts%20%28COME%29.%0ASpecifically%2C%20COME%20establishes%20dual%20structure-semantic%20shared%20experts%20that%0Acreate%20a%20universal%20representation%20space%20and%20then%20collaborate%20with%0Asource-specific%20experts%20to%20extract%20discriminative%20features%20through%20providing%0Acomplementary%20features.%20This%20design%20enables%20robust%20generalization%20by%20leveraging%0Across-datasets%20experience%20distributions%20and%20providing%20universal%20US%20priors%20for%0Asmall-batch%20or%20unseen%20data%20scenarios.%20Extensive%20experiments%20under%20three%0Aevaluation%20modes%20%28single-dataset%2C%20intra-organ%2C%20and%20inter-organ%20integration%0Adatasets%29%20demonstrate%20COME%27s%20superiority%2C%20achieving%20significant%20mean%20AP%0Aimprovements%20over%20state-of-the-art%20methods.%20Our%20project%20is%20available%20at%3A%0Ahttps%3A//universalcome.github.io/UniversalCOME/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOME%253A%2520Dual%2520Structure-Semantic%2520Learning%2520with%2520Collaborative%2520MoE%2520for%250A%2520%2520Universal%2520Lesion%2520Detection%2520Across%2520Heterogeneous%2520Ultrasound%2520Datasets%26entry.906535625%3DLingyu%2520Chen%2520and%2520Yawen%2520Zeng%2520and%2520Yue%2520Wang%2520and%2520Peng%2520Wan%2520and%2520Guo-chen%2520Ning%2520and%2520Hongen%2520Liao%2520and%2520Daoqiang%2520Zhang%2520and%2520Fang%2520Chen%26entry.1292438233%3D%2520%2520Conventional%2520single-dataset%2520training%2520often%2520fails%2520with%2520new%2520data%2520distributions%252C%250Aespecially%2520in%2520ultrasound%2520%2528US%2529%2520image%2520analysis%2520due%2520to%2520limited%2520data%252C%2520acoustic%250Ashadows%252C%2520and%2520speckle%2520noise.%2520Therefore%252C%2520constructing%2520a%2520universal%2520framework%2520for%250Amulti-heterogeneous%2520US%2520datasets%2520is%2520imperative.%2520However%252C%2520a%2520key%2520challenge%2520arises%253A%250Ahow%2520to%2520effectively%2520mitigate%2520inter-dataset%2520interference%2520while%2520preserving%250Adataset-specific%2520discriminative%2520features%2520for%2520robust%2520downstream%2520task%253F%2520Previous%250Aapproaches%2520utilize%2520either%2520a%2520single%2520source-specific%2520decoder%2520or%2520a%2520domain%250Aadaptation%2520strategy%252C%2520but%2520these%2520methods%2520experienced%2520a%2520decline%2520in%2520performance%250Awhen%2520applied%2520to%2520other%2520domains.%2520Considering%2520this%252C%2520we%2520propose%2520a%2520Universal%250ACollaborative%2520Mixture%2520of%2520Heterogeneous%2520Source-Specific%2520Experts%2520%2528COME%2529.%250ASpecifically%252C%2520COME%2520establishes%2520dual%2520structure-semantic%2520shared%2520experts%2520that%250Acreate%2520a%2520universal%2520representation%2520space%2520and%2520then%2520collaborate%2520with%250Asource-specific%2520experts%2520to%2520extract%2520discriminative%2520features%2520through%2520providing%250Acomplementary%2520features.%2520This%2520design%2520enables%2520robust%2520generalization%2520by%2520leveraging%250Across-datasets%2520experience%2520distributions%2520and%2520providing%2520universal%2520US%2520priors%2520for%250Asmall-batch%2520or%2520unseen%2520data%2520scenarios.%2520Extensive%2520experiments%2520under%2520three%250Aevaluation%2520modes%2520%2528single-dataset%252C%2520intra-organ%252C%2520and%2520inter-organ%2520integration%250Adatasets%2529%2520demonstrate%2520COME%2527s%2520superiority%252C%2520achieving%2520significant%2520mean%2520AP%250Aimprovements%2520over%2520state-of-the-art%2520methods.%2520Our%2520project%2520is%2520available%2520at%253A%250Ahttps%253A//universalcome.github.io/UniversalCOME/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COME%3A%20Dual%20Structure-Semantic%20Learning%20with%20Collaborative%20MoE%20for%0A%20%20Universal%20Lesion%20Detection%20Across%20Heterogeneous%20Ultrasound%20Datasets&entry.906535625=Lingyu%20Chen%20and%20Yawen%20Zeng%20and%20Yue%20Wang%20and%20Peng%20Wan%20and%20Guo-chen%20Ning%20and%20Hongen%20Liao%20and%20Daoqiang%20Zhang%20and%20Fang%20Chen&entry.1292438233=%20%20Conventional%20single-dataset%20training%20often%20fails%20with%20new%20data%20distributions%2C%0Aespecially%20in%20ultrasound%20%28US%29%20image%20analysis%20due%20to%20limited%20data%2C%20acoustic%0Ashadows%2C%20and%20speckle%20noise.%20Therefore%2C%20constructing%20a%20universal%20framework%20for%0Amulti-heterogeneous%20US%20datasets%20is%20imperative.%20However%2C%20a%20key%20challenge%20arises%3A%0Ahow%20to%20effectively%20mitigate%20inter-dataset%20interference%20while%20preserving%0Adataset-specific%20discriminative%20features%20for%20robust%20downstream%20task%3F%20Previous%0Aapproaches%20utilize%20either%20a%20single%20source-specific%20decoder%20or%20a%20domain%0Aadaptation%20strategy%2C%20but%20these%20methods%20experienced%20a%20decline%20in%20performance%0Awhen%20applied%20to%20other%20domains.%20Considering%20this%2C%20we%20propose%20a%20Universal%0ACollaborative%20Mixture%20of%20Heterogeneous%20Source-Specific%20Experts%20%28COME%29.%0ASpecifically%2C%20COME%20establishes%20dual%20structure-semantic%20shared%20experts%20that%0Acreate%20a%20universal%20representation%20space%20and%20then%20collaborate%20with%0Asource-specific%20experts%20to%20extract%20discriminative%20features%20through%20providing%0Acomplementary%20features.%20This%20design%20enables%20robust%20generalization%20by%20leveraging%0Across-datasets%20experience%20distributions%20and%20providing%20universal%20US%20priors%20for%0Asmall-batch%20or%20unseen%20data%20scenarios.%20Extensive%20experiments%20under%20three%0Aevaluation%20modes%20%28single-dataset%2C%20intra-organ%2C%20and%20inter-organ%20integration%0Adatasets%29%20demonstrate%20COME%27s%20superiority%2C%20achieving%20significant%20mean%20AP%0Aimprovements%20over%20state-of-the-art%20methods.%20Our%20project%20is%20available%20at%3A%0Ahttps%3A//universalcome.github.io/UniversalCOME/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09886v1&entry.124074799=Read"},
{"title": "Residual Reservoir Memory Networks", "author": "Matteo Pinna and Andrea Ceni and Claudio Gallicchio", "abstract": "  We introduce a novel class of untrained Recurrent Neural Networks (RNNs)\nwithin the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory\nNetworks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear\nreservoir, where the latter is based on residual orthogonal connections along\nthe temporal dimension for enhanced long-term propagation of the input. The\nresulting reservoir state dynamics are studied through the lens of linear\nstability analysis, and we investigate diverse configurations for the temporal\nresidual connections. The proposed approach is empirically assessed on\ntime-series and pixel-level 1-D classification tasks. Our experimental results\nhighlight the advantages of the proposed approach over other conventional RC\nmodels.\n", "link": "http://arxiv.org/abs/2508.09925v1", "date": "2025-08-13", "relevancy": 2.3083, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4745}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4568}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Reservoir%20Memory%20Networks&body=Title%3A%20Residual%20Reservoir%20Memory%20Networks%0AAuthor%3A%20Matteo%20Pinna%20and%20Andrea%20Ceni%20and%20Claudio%20Gallicchio%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20class%20of%20untrained%20Recurrent%20Neural%20Networks%20%28RNNs%29%0Awithin%20the%20Reservoir%20Computing%20%28RC%29%20paradigm%2C%20called%20Residual%20Reservoir%20Memory%0ANetworks%20%28ResRMNs%29.%20ResRMN%20combines%20a%20linear%20memory%20reservoir%20with%20a%20non-linear%0Areservoir%2C%20where%20the%20latter%20is%20based%20on%20residual%20orthogonal%20connections%20along%0Athe%20temporal%20dimension%20for%20enhanced%20long-term%20propagation%20of%20the%20input.%20The%0Aresulting%20reservoir%20state%20dynamics%20are%20studied%20through%20the%20lens%20of%20linear%0Astability%20analysis%2C%20and%20we%20investigate%20diverse%20configurations%20for%20the%20temporal%0Aresidual%20connections.%20The%20proposed%20approach%20is%20empirically%20assessed%20on%0Atime-series%20and%20pixel-level%201-D%20classification%20tasks.%20Our%20experimental%20results%0Ahighlight%20the%20advantages%20of%20the%20proposed%20approach%20over%20other%20conventional%20RC%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Reservoir%2520Memory%2520Networks%26entry.906535625%3DMatteo%2520Pinna%2520and%2520Andrea%2520Ceni%2520and%2520Claudio%2520Gallicchio%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520class%2520of%2520untrained%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529%250Awithin%2520the%2520Reservoir%2520Computing%2520%2528RC%2529%2520paradigm%252C%2520called%2520Residual%2520Reservoir%2520Memory%250ANetworks%2520%2528ResRMNs%2529.%2520ResRMN%2520combines%2520a%2520linear%2520memory%2520reservoir%2520with%2520a%2520non-linear%250Areservoir%252C%2520where%2520the%2520latter%2520is%2520based%2520on%2520residual%2520orthogonal%2520connections%2520along%250Athe%2520temporal%2520dimension%2520for%2520enhanced%2520long-term%2520propagation%2520of%2520the%2520input.%2520The%250Aresulting%2520reservoir%2520state%2520dynamics%2520are%2520studied%2520through%2520the%2520lens%2520of%2520linear%250Astability%2520analysis%252C%2520and%2520we%2520investigate%2520diverse%2520configurations%2520for%2520the%2520temporal%250Aresidual%2520connections.%2520The%2520proposed%2520approach%2520is%2520empirically%2520assessed%2520on%250Atime-series%2520and%2520pixel-level%25201-D%2520classification%2520tasks.%2520Our%2520experimental%2520results%250Ahighlight%2520the%2520advantages%2520of%2520the%2520proposed%2520approach%2520over%2520other%2520conventional%2520RC%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Reservoir%20Memory%20Networks&entry.906535625=Matteo%20Pinna%20and%20Andrea%20Ceni%20and%20Claudio%20Gallicchio&entry.1292438233=%20%20We%20introduce%20a%20novel%20class%20of%20untrained%20Recurrent%20Neural%20Networks%20%28RNNs%29%0Awithin%20the%20Reservoir%20Computing%20%28RC%29%20paradigm%2C%20called%20Residual%20Reservoir%20Memory%0ANetworks%20%28ResRMNs%29.%20ResRMN%20combines%20a%20linear%20memory%20reservoir%20with%20a%20non-linear%0Areservoir%2C%20where%20the%20latter%20is%20based%20on%20residual%20orthogonal%20connections%20along%0Athe%20temporal%20dimension%20for%20enhanced%20long-term%20propagation%20of%20the%20input.%20The%0Aresulting%20reservoir%20state%20dynamics%20are%20studied%20through%20the%20lens%20of%20linear%0Astability%20analysis%2C%20and%20we%20investigate%20diverse%20configurations%20for%20the%20temporal%0Aresidual%20connections.%20The%20proposed%20approach%20is%20empirically%20assessed%20on%0Atime-series%20and%20pixel-level%201-D%20classification%20tasks.%20Our%20experimental%20results%0Ahighlight%20the%20advantages%20of%20the%20proposed%20approach%20over%20other%20conventional%20RC%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09925v1&entry.124074799=Read"},
{"title": "Transferable Model-agnostic Vision-Language Model Adaptation for\n  Efficient Weak-to-Strong Generalization", "author": "Jihwan Park and Taehoon song and Sanghyeok Lee and Miso Choi and Hyunwoo J. Kim", "abstract": "  Vision-Language Models (VLMs) have been widely used in various visual\nrecognition tasks due to their remarkable generalization capabilities. As these\nmodels grow in size and complexity, fine-tuning becomes costly, emphasizing the\nneed to reuse adaptation knowledge from 'weaker' models to efficiently enhance\n'stronger' ones. However, existing adaptation transfer methods exhibit limited\ntransferability across models due to their model-specific design and high\ncomputational demands. To tackle this, we propose Transferable Model-agnostic\nadapter (TransMiter), a light-weight adapter that improves vision-language\nmodels 'without backpropagation'. TransMiter captures the knowledge gap between\npre-trained and fine-tuned VLMs, in an 'unsupervised' manner. Once trained,\nthis knowledge can be seamlessly transferred across different models without\nthe need for backpropagation. Moreover, TransMiter consists of only a few\nlayers, inducing a negligible additional inference cost. Notably, supplementing\nthe process with a few labeled data further yields additional performance gain,\noften surpassing a fine-tuned stronger model, with a marginal training cost.\nExperimental results and analyses demonstrate that TransMiter effectively and\nefficiently transfers adaptation knowledge while preserving generalization\nabilities across VLMs of different sizes and architectures in visual\nrecognition tasks.\n", "link": "http://arxiv.org/abs/2508.08604v2", "date": "2025-08-13", "relevancy": 2.2624, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6223}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5381}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferable%20Model-agnostic%20Vision-Language%20Model%20Adaptation%20for%0A%20%20Efficient%20Weak-to-Strong%20Generalization&body=Title%3A%20Transferable%20Model-agnostic%20Vision-Language%20Model%20Adaptation%20for%0A%20%20Efficient%20Weak-to-Strong%20Generalization%0AAuthor%3A%20Jihwan%20Park%20and%20Taehoon%20song%20and%20Sanghyeok%20Lee%20and%20Miso%20Choi%20and%20Hyunwoo%20J.%20Kim%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20been%20widely%20used%20in%20various%20visual%0Arecognition%20tasks%20due%20to%20their%20remarkable%20generalization%20capabilities.%20As%20these%0Amodels%20grow%20in%20size%20and%20complexity%2C%20fine-tuning%20becomes%20costly%2C%20emphasizing%20the%0Aneed%20to%20reuse%20adaptation%20knowledge%20from%20%27weaker%27%20models%20to%20efficiently%20enhance%0A%27stronger%27%20ones.%20However%2C%20existing%20adaptation%20transfer%20methods%20exhibit%20limited%0Atransferability%20across%20models%20due%20to%20their%20model-specific%20design%20and%20high%0Acomputational%20demands.%20To%20tackle%20this%2C%20we%20propose%20Transferable%20Model-agnostic%0Aadapter%20%28TransMiter%29%2C%20a%20light-weight%20adapter%20that%20improves%20vision-language%0Amodels%20%27without%20backpropagation%27.%20TransMiter%20captures%20the%20knowledge%20gap%20between%0Apre-trained%20and%20fine-tuned%20VLMs%2C%20in%20an%20%27unsupervised%27%20manner.%20Once%20trained%2C%0Athis%20knowledge%20can%20be%20seamlessly%20transferred%20across%20different%20models%20without%0Athe%20need%20for%20backpropagation.%20Moreover%2C%20TransMiter%20consists%20of%20only%20a%20few%0Alayers%2C%20inducing%20a%20negligible%20additional%20inference%20cost.%20Notably%2C%20supplementing%0Athe%20process%20with%20a%20few%20labeled%20data%20further%20yields%20additional%20performance%20gain%2C%0Aoften%20surpassing%20a%20fine-tuned%20stronger%20model%2C%20with%20a%20marginal%20training%20cost.%0AExperimental%20results%20and%20analyses%20demonstrate%20that%20TransMiter%20effectively%20and%0Aefficiently%20transfers%20adaptation%20knowledge%20while%20preserving%20generalization%0Aabilities%20across%20VLMs%20of%20different%20sizes%20and%20architectures%20in%20visual%0Arecognition%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08604v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferable%2520Model-agnostic%2520Vision-Language%2520Model%2520Adaptation%2520for%250A%2520%2520Efficient%2520Weak-to-Strong%2520Generalization%26entry.906535625%3DJihwan%2520Park%2520and%2520Taehoon%2520song%2520and%2520Sanghyeok%2520Lee%2520and%2520Miso%2520Choi%2520and%2520Hyunwoo%2520J.%2520Kim%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520been%2520widely%2520used%2520in%2520various%2520visual%250Arecognition%2520tasks%2520due%2520to%2520their%2520remarkable%2520generalization%2520capabilities.%2520As%2520these%250Amodels%2520grow%2520in%2520size%2520and%2520complexity%252C%2520fine-tuning%2520becomes%2520costly%252C%2520emphasizing%2520the%250Aneed%2520to%2520reuse%2520adaptation%2520knowledge%2520from%2520%2527weaker%2527%2520models%2520to%2520efficiently%2520enhance%250A%2527stronger%2527%2520ones.%2520However%252C%2520existing%2520adaptation%2520transfer%2520methods%2520exhibit%2520limited%250Atransferability%2520across%2520models%2520due%2520to%2520their%2520model-specific%2520design%2520and%2520high%250Acomputational%2520demands.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520Transferable%2520Model-agnostic%250Aadapter%2520%2528TransMiter%2529%252C%2520a%2520light-weight%2520adapter%2520that%2520improves%2520vision-language%250Amodels%2520%2527without%2520backpropagation%2527.%2520TransMiter%2520captures%2520the%2520knowledge%2520gap%2520between%250Apre-trained%2520and%2520fine-tuned%2520VLMs%252C%2520in%2520an%2520%2527unsupervised%2527%2520manner.%2520Once%2520trained%252C%250Athis%2520knowledge%2520can%2520be%2520seamlessly%2520transferred%2520across%2520different%2520models%2520without%250Athe%2520need%2520for%2520backpropagation.%2520Moreover%252C%2520TransMiter%2520consists%2520of%2520only%2520a%2520few%250Alayers%252C%2520inducing%2520a%2520negligible%2520additional%2520inference%2520cost.%2520Notably%252C%2520supplementing%250Athe%2520process%2520with%2520a%2520few%2520labeled%2520data%2520further%2520yields%2520additional%2520performance%2520gain%252C%250Aoften%2520surpassing%2520a%2520fine-tuned%2520stronger%2520model%252C%2520with%2520a%2520marginal%2520training%2520cost.%250AExperimental%2520results%2520and%2520analyses%2520demonstrate%2520that%2520TransMiter%2520effectively%2520and%250Aefficiently%2520transfers%2520adaptation%2520knowledge%2520while%2520preserving%2520generalization%250Aabilities%2520across%2520VLMs%2520of%2520different%2520sizes%2520and%2520architectures%2520in%2520visual%250Arecognition%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08604v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferable%20Model-agnostic%20Vision-Language%20Model%20Adaptation%20for%0A%20%20Efficient%20Weak-to-Strong%20Generalization&entry.906535625=Jihwan%20Park%20and%20Taehoon%20song%20and%20Sanghyeok%20Lee%20and%20Miso%20Choi%20and%20Hyunwoo%20J.%20Kim&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20been%20widely%20used%20in%20various%20visual%0Arecognition%20tasks%20due%20to%20their%20remarkable%20generalization%20capabilities.%20As%20these%0Amodels%20grow%20in%20size%20and%20complexity%2C%20fine-tuning%20becomes%20costly%2C%20emphasizing%20the%0Aneed%20to%20reuse%20adaptation%20knowledge%20from%20%27weaker%27%20models%20to%20efficiently%20enhance%0A%27stronger%27%20ones.%20However%2C%20existing%20adaptation%20transfer%20methods%20exhibit%20limited%0Atransferability%20across%20models%20due%20to%20their%20model-specific%20design%20and%20high%0Acomputational%20demands.%20To%20tackle%20this%2C%20we%20propose%20Transferable%20Model-agnostic%0Aadapter%20%28TransMiter%29%2C%20a%20light-weight%20adapter%20that%20improves%20vision-language%0Amodels%20%27without%20backpropagation%27.%20TransMiter%20captures%20the%20knowledge%20gap%20between%0Apre-trained%20and%20fine-tuned%20VLMs%2C%20in%20an%20%27unsupervised%27%20manner.%20Once%20trained%2C%0Athis%20knowledge%20can%20be%20seamlessly%20transferred%20across%20different%20models%20without%0Athe%20need%20for%20backpropagation.%20Moreover%2C%20TransMiter%20consists%20of%20only%20a%20few%0Alayers%2C%20inducing%20a%20negligible%20additional%20inference%20cost.%20Notably%2C%20supplementing%0Athe%20process%20with%20a%20few%20labeled%20data%20further%20yields%20additional%20performance%20gain%2C%0Aoften%20surpassing%20a%20fine-tuned%20stronger%20model%2C%20with%20a%20marginal%20training%20cost.%0AExperimental%20results%20and%20analyses%20demonstrate%20that%20TransMiter%20effectively%20and%0Aefficiently%20transfers%20adaptation%20knowledge%20while%20preserving%20generalization%0Aabilities%20across%20VLMs%20of%20different%20sizes%20and%20architectures%20in%20visual%0Arecognition%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08604v2&entry.124074799=Read"},
{"title": "Describe What You See with Multimodal Large Language Models to Enhance\n  Video Recommendations", "author": "Marco De Nadai and Andreas Damianou and Mounia Lalmas", "abstract": "  Existing video recommender systems rely primarily on user-defined metadata or\non low-level visual and acoustic signals extracted by specialised encoders.\nThese low-level features describe what appears on the screen but miss deeper\nsemantics such as intent, humour, and world knowledge that make clips resonate\nwith viewers. For example, is a 30-second clip simply a singer on a rooftop, or\nan ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such\ndistinctions are critical to personalised recommendations yet remain invisible\nto traditional encoding pipelines. In this paper, we introduce a simple,\nrecommendation system-agnostic zero-finetuning framework that injects\nhigh-level semantics into the recommendation pipeline by prompting an\noff-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip\ninto a rich natural-language description (e.g. \"a superhero parody with\nslapstick fights and orchestral stabs\"), bridging the gap between raw content\nand user intent. We use MLLM output with a state-of-the-art text encoder and\nfeed it into standard collaborative, content-based, and generative\nrecommenders. On the MicroLens-100K dataset, which emulates user interactions\nwith TikTok-style videos, our framework consistently surpasses conventional\nvideo, audio, and metadata features in five representative models. Our findings\nhighlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to\nbuild more intent-aware video recommenders.\n", "link": "http://arxiv.org/abs/2508.09789v1", "date": "2025-08-13", "relevancy": 2.2595, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5701}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Describe%20What%20You%20See%20with%20Multimodal%20Large%20Language%20Models%20to%20Enhance%0A%20%20Video%20Recommendations&body=Title%3A%20Describe%20What%20You%20See%20with%20Multimodal%20Large%20Language%20Models%20to%20Enhance%0A%20%20Video%20Recommendations%0AAuthor%3A%20Marco%20De%20Nadai%20and%20Andreas%20Damianou%20and%20Mounia%20Lalmas%0AAbstract%3A%20%20%20Existing%20video%20recommender%20systems%20rely%20primarily%20on%20user-defined%20metadata%20or%0Aon%20low-level%20visual%20and%20acoustic%20signals%20extracted%20by%20specialised%20encoders.%0AThese%20low-level%20features%20describe%20what%20appears%20on%20the%20screen%20but%20miss%20deeper%0Asemantics%20such%20as%20intent%2C%20humour%2C%20and%20world%20knowledge%20that%20make%20clips%20resonate%0Awith%20viewers.%20For%20example%2C%20is%20a%2030-second%20clip%20simply%20a%20singer%20on%20a%20rooftop%2C%20or%0Aan%20ironic%20parody%20filmed%20amid%20the%20fairy%20chimneys%20of%20Cappadocia%2C%20Turkey%3F%20Such%0Adistinctions%20are%20critical%20to%20personalised%20recommendations%20yet%20remain%20invisible%0Ato%20traditional%20encoding%20pipelines.%20In%20this%20paper%2C%20we%20introduce%20a%20simple%2C%0Arecommendation%20system-agnostic%20zero-finetuning%20framework%20that%20injects%0Ahigh-level%20semantics%20into%20the%20recommendation%20pipeline%20by%20prompting%20an%0Aoff-the-shelf%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20to%20summarise%20each%20clip%0Ainto%20a%20rich%20natural-language%20description%20%28e.g.%20%22a%20superhero%20parody%20with%0Aslapstick%20fights%20and%20orchestral%20stabs%22%29%2C%20bridging%20the%20gap%20between%20raw%20content%0Aand%20user%20intent.%20We%20use%20MLLM%20output%20with%20a%20state-of-the-art%20text%20encoder%20and%0Afeed%20it%20into%20standard%20collaborative%2C%20content-based%2C%20and%20generative%0Arecommenders.%20On%20the%20MicroLens-100K%20dataset%2C%20which%20emulates%20user%20interactions%0Awith%20TikTok-style%20videos%2C%20our%20framework%20consistently%20surpasses%20conventional%0Avideo%2C%20audio%2C%20and%20metadata%20features%20in%20five%20representative%20models.%20Our%20findings%0Ahighlight%20the%20promise%20of%20leveraging%20MLLMs%20as%20on-the-fly%20knowledge%20extractors%20to%0Abuild%20more%20intent-aware%20video%20recommenders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDescribe%2520What%2520You%2520See%2520with%2520Multimodal%2520Large%2520Language%2520Models%2520to%2520Enhance%250A%2520%2520Video%2520Recommendations%26entry.906535625%3DMarco%2520De%2520Nadai%2520and%2520Andreas%2520Damianou%2520and%2520Mounia%2520Lalmas%26entry.1292438233%3D%2520%2520Existing%2520video%2520recommender%2520systems%2520rely%2520primarily%2520on%2520user-defined%2520metadata%2520or%250Aon%2520low-level%2520visual%2520and%2520acoustic%2520signals%2520extracted%2520by%2520specialised%2520encoders.%250AThese%2520low-level%2520features%2520describe%2520what%2520appears%2520on%2520the%2520screen%2520but%2520miss%2520deeper%250Asemantics%2520such%2520as%2520intent%252C%2520humour%252C%2520and%2520world%2520knowledge%2520that%2520make%2520clips%2520resonate%250Awith%2520viewers.%2520For%2520example%252C%2520is%2520a%252030-second%2520clip%2520simply%2520a%2520singer%2520on%2520a%2520rooftop%252C%2520or%250Aan%2520ironic%2520parody%2520filmed%2520amid%2520the%2520fairy%2520chimneys%2520of%2520Cappadocia%252C%2520Turkey%253F%2520Such%250Adistinctions%2520are%2520critical%2520to%2520personalised%2520recommendations%2520yet%2520remain%2520invisible%250Ato%2520traditional%2520encoding%2520pipelines.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520simple%252C%250Arecommendation%2520system-agnostic%2520zero-finetuning%2520framework%2520that%2520injects%250Ahigh-level%2520semantics%2520into%2520the%2520recommendation%2520pipeline%2520by%2520prompting%2520an%250Aoff-the-shelf%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520to%2520summarise%2520each%2520clip%250Ainto%2520a%2520rich%2520natural-language%2520description%2520%2528e.g.%2520%2522a%2520superhero%2520parody%2520with%250Aslapstick%2520fights%2520and%2520orchestral%2520stabs%2522%2529%252C%2520bridging%2520the%2520gap%2520between%2520raw%2520content%250Aand%2520user%2520intent.%2520We%2520use%2520MLLM%2520output%2520with%2520a%2520state-of-the-art%2520text%2520encoder%2520and%250Afeed%2520it%2520into%2520standard%2520collaborative%252C%2520content-based%252C%2520and%2520generative%250Arecommenders.%2520On%2520the%2520MicroLens-100K%2520dataset%252C%2520which%2520emulates%2520user%2520interactions%250Awith%2520TikTok-style%2520videos%252C%2520our%2520framework%2520consistently%2520surpasses%2520conventional%250Avideo%252C%2520audio%252C%2520and%2520metadata%2520features%2520in%2520five%2520representative%2520models.%2520Our%2520findings%250Ahighlight%2520the%2520promise%2520of%2520leveraging%2520MLLMs%2520as%2520on-the-fly%2520knowledge%2520extractors%2520to%250Abuild%2520more%2520intent-aware%2520video%2520recommenders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Describe%20What%20You%20See%20with%20Multimodal%20Large%20Language%20Models%20to%20Enhance%0A%20%20Video%20Recommendations&entry.906535625=Marco%20De%20Nadai%20and%20Andreas%20Damianou%20and%20Mounia%20Lalmas&entry.1292438233=%20%20Existing%20video%20recommender%20systems%20rely%20primarily%20on%20user-defined%20metadata%20or%0Aon%20low-level%20visual%20and%20acoustic%20signals%20extracted%20by%20specialised%20encoders.%0AThese%20low-level%20features%20describe%20what%20appears%20on%20the%20screen%20but%20miss%20deeper%0Asemantics%20such%20as%20intent%2C%20humour%2C%20and%20world%20knowledge%20that%20make%20clips%20resonate%0Awith%20viewers.%20For%20example%2C%20is%20a%2030-second%20clip%20simply%20a%20singer%20on%20a%20rooftop%2C%20or%0Aan%20ironic%20parody%20filmed%20amid%20the%20fairy%20chimneys%20of%20Cappadocia%2C%20Turkey%3F%20Such%0Adistinctions%20are%20critical%20to%20personalised%20recommendations%20yet%20remain%20invisible%0Ato%20traditional%20encoding%20pipelines.%20In%20this%20paper%2C%20we%20introduce%20a%20simple%2C%0Arecommendation%20system-agnostic%20zero-finetuning%20framework%20that%20injects%0Ahigh-level%20semantics%20into%20the%20recommendation%20pipeline%20by%20prompting%20an%0Aoff-the-shelf%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20to%20summarise%20each%20clip%0Ainto%20a%20rich%20natural-language%20description%20%28e.g.%20%22a%20superhero%20parody%20with%0Aslapstick%20fights%20and%20orchestral%20stabs%22%29%2C%20bridging%20the%20gap%20between%20raw%20content%0Aand%20user%20intent.%20We%20use%20MLLM%20output%20with%20a%20state-of-the-art%20text%20encoder%20and%0Afeed%20it%20into%20standard%20collaborative%2C%20content-based%2C%20and%20generative%0Arecommenders.%20On%20the%20MicroLens-100K%20dataset%2C%20which%20emulates%20user%20interactions%0Awith%20TikTok-style%20videos%2C%20our%20framework%20consistently%20surpasses%20conventional%0Avideo%2C%20audio%2C%20and%20metadata%20features%20in%20five%20representative%20models.%20Our%20findings%0Ahighlight%20the%20promise%20of%20leveraging%20MLLMs%20as%20on-the-fly%20knowledge%20extractors%20to%0Abuild%20more%20intent-aware%20video%20recommenders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09789v1&entry.124074799=Read"},
{"title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with\n  Long-Term Memory", "author": "Lin Long and Yichen He and Wentao Ye and Yiyuan Pan and Yuan Lin and Hang Li and Junbo Zhao and Wei Li", "abstract": "  We introduce M3-Agent, a novel multimodal agent framework equipped with\nlong-term memory. Like humans, M3-Agent can process real-time visual and\nauditory inputs to build and update its long-term memory. Beyond episodic\nmemory, it also develops semantic memory, enabling it to accumulate world\nknowledge over time. Its memory is organized in an entity-centric, multimodal\nformat, allowing deeper and more consistent understanding of the environment.\nGiven an instruction, M3-Agent autonomously performs multi-turn, iterative\nreasoning and retrieves relevant information from memory to accomplish the\ntask. To evaluate memory effectiveness and memory-based reasoning in multimodal\nagents, we develop M3-Bench, a new long-video question answering benchmark.\nM3-Bench comprises 100 newly recorded real-world videos captured from a robot's\nperspective (M3-Bench-robot) and 929 web-sourced videos across diverse\nscenarios (M3-Bench-web). We annotate question-answer pairs designed to test\nkey capabilities essential for agent applications, such as human understanding,\ngeneral knowledge extraction, and cross-modal reasoning. Experimental results\nshow that M3-Agent, trained via reinforcement learning, outperforms the\nstrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,\nachieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web\nand VideoMME-long, respectively. Our work advances the multimodal agents toward\nmore human-like long-term memory and provides insights into their practical\ndesign. Model, code and data are available at\nhttps://github.com/bytedance-seed/m3-agent\n", "link": "http://arxiv.org/abs/2508.09736v1", "date": "2025-08-13", "relevancy": 2.2456, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%2C%20Listening%2C%20Remembering%2C%20and%20Reasoning%3A%20A%20Multimodal%20Agent%20with%0A%20%20Long-Term%20Memory&body=Title%3A%20Seeing%2C%20Listening%2C%20Remembering%2C%20and%20Reasoning%3A%20A%20Multimodal%20Agent%20with%0A%20%20Long-Term%20Memory%0AAuthor%3A%20Lin%20Long%20and%20Yichen%20He%20and%20Wentao%20Ye%20and%20Yiyuan%20Pan%20and%20Yuan%20Lin%20and%20Hang%20Li%20and%20Junbo%20Zhao%20and%20Wei%20Li%0AAbstract%3A%20%20%20We%20introduce%20M3-Agent%2C%20a%20novel%20multimodal%20agent%20framework%20equipped%20with%0Along-term%20memory.%20Like%20humans%2C%20M3-Agent%20can%20process%20real-time%20visual%20and%0Aauditory%20inputs%20to%20build%20and%20update%20its%20long-term%20memory.%20Beyond%20episodic%0Amemory%2C%20it%20also%20develops%20semantic%20memory%2C%20enabling%20it%20to%20accumulate%20world%0Aknowledge%20over%20time.%20Its%20memory%20is%20organized%20in%20an%20entity-centric%2C%20multimodal%0Aformat%2C%20allowing%20deeper%20and%20more%20consistent%20understanding%20of%20the%20environment.%0AGiven%20an%20instruction%2C%20M3-Agent%20autonomously%20performs%20multi-turn%2C%20iterative%0Areasoning%20and%20retrieves%20relevant%20information%20from%20memory%20to%20accomplish%20the%0Atask.%20To%20evaluate%20memory%20effectiveness%20and%20memory-based%20reasoning%20in%20multimodal%0Aagents%2C%20we%20develop%20M3-Bench%2C%20a%20new%20long-video%20question%20answering%20benchmark.%0AM3-Bench%20comprises%20100%20newly%20recorded%20real-world%20videos%20captured%20from%20a%20robot%27s%0Aperspective%20%28M3-Bench-robot%29%20and%20929%20web-sourced%20videos%20across%20diverse%0Ascenarios%20%28M3-Bench-web%29.%20We%20annotate%20question-answer%20pairs%20designed%20to%20test%0Akey%20capabilities%20essential%20for%20agent%20applications%2C%20such%20as%20human%20understanding%2C%0Ageneral%20knowledge%20extraction%2C%20and%20cross-modal%20reasoning.%20Experimental%20results%0Ashow%20that%20M3-Agent%2C%20trained%20via%20reinforcement%20learning%2C%20outperforms%20the%0Astrongest%20baseline%2C%20a%20prompting%20agent%20using%20Gemini-1.5-pro%20and%20GPT-4o%2C%0Aachieving%206.7%25%2C%207.7%25%2C%20and%205.3%25%20higher%20accuracy%20on%20M3-Bench-robot%2C%20M3-Bench-web%0Aand%20VideoMME-long%2C%20respectively.%20Our%20work%20advances%20the%20multimodal%20agents%20toward%0Amore%20human-like%20long-term%20memory%20and%20provides%20insights%20into%20their%20practical%0Adesign.%20Model%2C%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/bytedance-seed/m3-agent%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%252C%2520Listening%252C%2520Remembering%252C%2520and%2520Reasoning%253A%2520A%2520Multimodal%2520Agent%2520with%250A%2520%2520Long-Term%2520Memory%26entry.906535625%3DLin%2520Long%2520and%2520Yichen%2520He%2520and%2520Wentao%2520Ye%2520and%2520Yiyuan%2520Pan%2520and%2520Yuan%2520Lin%2520and%2520Hang%2520Li%2520and%2520Junbo%2520Zhao%2520and%2520Wei%2520Li%26entry.1292438233%3D%2520%2520We%2520introduce%2520M3-Agent%252C%2520a%2520novel%2520multimodal%2520agent%2520framework%2520equipped%2520with%250Along-term%2520memory.%2520Like%2520humans%252C%2520M3-Agent%2520can%2520process%2520real-time%2520visual%2520and%250Aauditory%2520inputs%2520to%2520build%2520and%2520update%2520its%2520long-term%2520memory.%2520Beyond%2520episodic%250Amemory%252C%2520it%2520also%2520develops%2520semantic%2520memory%252C%2520enabling%2520it%2520to%2520accumulate%2520world%250Aknowledge%2520over%2520time.%2520Its%2520memory%2520is%2520organized%2520in%2520an%2520entity-centric%252C%2520multimodal%250Aformat%252C%2520allowing%2520deeper%2520and%2520more%2520consistent%2520understanding%2520of%2520the%2520environment.%250AGiven%2520an%2520instruction%252C%2520M3-Agent%2520autonomously%2520performs%2520multi-turn%252C%2520iterative%250Areasoning%2520and%2520retrieves%2520relevant%2520information%2520from%2520memory%2520to%2520accomplish%2520the%250Atask.%2520To%2520evaluate%2520memory%2520effectiveness%2520and%2520memory-based%2520reasoning%2520in%2520multimodal%250Aagents%252C%2520we%2520develop%2520M3-Bench%252C%2520a%2520new%2520long-video%2520question%2520answering%2520benchmark.%250AM3-Bench%2520comprises%2520100%2520newly%2520recorded%2520real-world%2520videos%2520captured%2520from%2520a%2520robot%2527s%250Aperspective%2520%2528M3-Bench-robot%2529%2520and%2520929%2520web-sourced%2520videos%2520across%2520diverse%250Ascenarios%2520%2528M3-Bench-web%2529.%2520We%2520annotate%2520question-answer%2520pairs%2520designed%2520to%2520test%250Akey%2520capabilities%2520essential%2520for%2520agent%2520applications%252C%2520such%2520as%2520human%2520understanding%252C%250Ageneral%2520knowledge%2520extraction%252C%2520and%2520cross-modal%2520reasoning.%2520Experimental%2520results%250Ashow%2520that%2520M3-Agent%252C%2520trained%2520via%2520reinforcement%2520learning%252C%2520outperforms%2520the%250Astrongest%2520baseline%252C%2520a%2520prompting%2520agent%2520using%2520Gemini-1.5-pro%2520and%2520GPT-4o%252C%250Aachieving%25206.7%2525%252C%25207.7%2525%252C%2520and%25205.3%2525%2520higher%2520accuracy%2520on%2520M3-Bench-robot%252C%2520M3-Bench-web%250Aand%2520VideoMME-long%252C%2520respectively.%2520Our%2520work%2520advances%2520the%2520multimodal%2520agents%2520toward%250Amore%2520human-like%2520long-term%2520memory%2520and%2520provides%2520insights%2520into%2520their%2520practical%250Adesign.%2520Model%252C%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/bytedance-seed/m3-agent%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%2C%20Listening%2C%20Remembering%2C%20and%20Reasoning%3A%20A%20Multimodal%20Agent%20with%0A%20%20Long-Term%20Memory&entry.906535625=Lin%20Long%20and%20Yichen%20He%20and%20Wentao%20Ye%20and%20Yiyuan%20Pan%20and%20Yuan%20Lin%20and%20Hang%20Li%20and%20Junbo%20Zhao%20and%20Wei%20Li&entry.1292438233=%20%20We%20introduce%20M3-Agent%2C%20a%20novel%20multimodal%20agent%20framework%20equipped%20with%0Along-term%20memory.%20Like%20humans%2C%20M3-Agent%20can%20process%20real-time%20visual%20and%0Aauditory%20inputs%20to%20build%20and%20update%20its%20long-term%20memory.%20Beyond%20episodic%0Amemory%2C%20it%20also%20develops%20semantic%20memory%2C%20enabling%20it%20to%20accumulate%20world%0Aknowledge%20over%20time.%20Its%20memory%20is%20organized%20in%20an%20entity-centric%2C%20multimodal%0Aformat%2C%20allowing%20deeper%20and%20more%20consistent%20understanding%20of%20the%20environment.%0AGiven%20an%20instruction%2C%20M3-Agent%20autonomously%20performs%20multi-turn%2C%20iterative%0Areasoning%20and%20retrieves%20relevant%20information%20from%20memory%20to%20accomplish%20the%0Atask.%20To%20evaluate%20memory%20effectiveness%20and%20memory-based%20reasoning%20in%20multimodal%0Aagents%2C%20we%20develop%20M3-Bench%2C%20a%20new%20long-video%20question%20answering%20benchmark.%0AM3-Bench%20comprises%20100%20newly%20recorded%20real-world%20videos%20captured%20from%20a%20robot%27s%0Aperspective%20%28M3-Bench-robot%29%20and%20929%20web-sourced%20videos%20across%20diverse%0Ascenarios%20%28M3-Bench-web%29.%20We%20annotate%20question-answer%20pairs%20designed%20to%20test%0Akey%20capabilities%20essential%20for%20agent%20applications%2C%20such%20as%20human%20understanding%2C%0Ageneral%20knowledge%20extraction%2C%20and%20cross-modal%20reasoning.%20Experimental%20results%0Ashow%20that%20M3-Agent%2C%20trained%20via%20reinforcement%20learning%2C%20outperforms%20the%0Astrongest%20baseline%2C%20a%20prompting%20agent%20using%20Gemini-1.5-pro%20and%20GPT-4o%2C%0Aachieving%206.7%25%2C%207.7%25%2C%20and%205.3%25%20higher%20accuracy%20on%20M3-Bench-robot%2C%20M3-Bench-web%0Aand%20VideoMME-long%2C%20respectively.%20Our%20work%20advances%20the%20multimodal%20agents%20toward%0Amore%20human-like%20long-term%20memory%20and%20provides%20insights%20into%20their%20practical%0Adesign.%20Model%2C%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/bytedance-seed/m3-agent%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09736v1&entry.124074799=Read"},
{"title": "Collision-Free Bearing-Driven Formation Tracking for Euler-Lagrange\n  Systems", "author": "Haoshu Cheng and Martin Guay and Shimin Wang and Yunhong Che", "abstract": "  In this paper, we investigate the problem of tracking formations driven by\nbearings for heterogeneous Euler-Lagrange systems with parametric uncertainty\nin the presence of multiple moving leaders. To estimate the leaders' velocities\nand accelerations, we first design a distributed observer for the leader\nsystem, utilizing a bearing-based localization condition in place of the\nconventional connectivity assumption. This observer, coupled with an adaptive\nmechanism, enables the synthesis of a novel distributed control law that guides\nthe formation towards the target formation, without requiring prior knowledge\nof the system parameters. Furthermore, we establish a sufficient condition,\ndependent on the initial formation configuration, that ensures collision\navoidance throughout the formation evolution. The effectiveness of the proposed\napproach is demonstrated through a numerical example.\n", "link": "http://arxiv.org/abs/2508.09908v1", "date": "2025-08-13", "relevancy": 2.2414, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4688}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4436}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collision-Free%20Bearing-Driven%20Formation%20Tracking%20for%20Euler-Lagrange%0A%20%20Systems&body=Title%3A%20Collision-Free%20Bearing-Driven%20Formation%20Tracking%20for%20Euler-Lagrange%0A%20%20Systems%0AAuthor%3A%20Haoshu%20Cheng%20and%20Martin%20Guay%20and%20Shimin%20Wang%20and%20Yunhong%20Che%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20problem%20of%20tracking%20formations%20driven%20by%0Abearings%20for%20heterogeneous%20Euler-Lagrange%20systems%20with%20parametric%20uncertainty%0Ain%20the%20presence%20of%20multiple%20moving%20leaders.%20To%20estimate%20the%20leaders%27%20velocities%0Aand%20accelerations%2C%20we%20first%20design%20a%20distributed%20observer%20for%20the%20leader%0Asystem%2C%20utilizing%20a%20bearing-based%20localization%20condition%20in%20place%20of%20the%0Aconventional%20connectivity%20assumption.%20This%20observer%2C%20coupled%20with%20an%20adaptive%0Amechanism%2C%20enables%20the%20synthesis%20of%20a%20novel%20distributed%20control%20law%20that%20guides%0Athe%20formation%20towards%20the%20target%20formation%2C%20without%20requiring%20prior%20knowledge%0Aof%20the%20system%20parameters.%20Furthermore%2C%20we%20establish%20a%20sufficient%20condition%2C%0Adependent%20on%20the%20initial%20formation%20configuration%2C%20that%20ensures%20collision%0Aavoidance%20throughout%20the%20formation%20evolution.%20The%20effectiveness%20of%20the%20proposed%0Aapproach%20is%20demonstrated%20through%20a%20numerical%20example.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollision-Free%2520Bearing-Driven%2520Formation%2520Tracking%2520for%2520Euler-Lagrange%250A%2520%2520Systems%26entry.906535625%3DHaoshu%2520Cheng%2520and%2520Martin%2520Guay%2520and%2520Shimin%2520Wang%2520and%2520Yunhong%2520Che%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520problem%2520of%2520tracking%2520formations%2520driven%2520by%250Abearings%2520for%2520heterogeneous%2520Euler-Lagrange%2520systems%2520with%2520parametric%2520uncertainty%250Ain%2520the%2520presence%2520of%2520multiple%2520moving%2520leaders.%2520To%2520estimate%2520the%2520leaders%2527%2520velocities%250Aand%2520accelerations%252C%2520we%2520first%2520design%2520a%2520distributed%2520observer%2520for%2520the%2520leader%250Asystem%252C%2520utilizing%2520a%2520bearing-based%2520localization%2520condition%2520in%2520place%2520of%2520the%250Aconventional%2520connectivity%2520assumption.%2520This%2520observer%252C%2520coupled%2520with%2520an%2520adaptive%250Amechanism%252C%2520enables%2520the%2520synthesis%2520of%2520a%2520novel%2520distributed%2520control%2520law%2520that%2520guides%250Athe%2520formation%2520towards%2520the%2520target%2520formation%252C%2520without%2520requiring%2520prior%2520knowledge%250Aof%2520the%2520system%2520parameters.%2520Furthermore%252C%2520we%2520establish%2520a%2520sufficient%2520condition%252C%250Adependent%2520on%2520the%2520initial%2520formation%2520configuration%252C%2520that%2520ensures%2520collision%250Aavoidance%2520throughout%2520the%2520formation%2520evolution.%2520The%2520effectiveness%2520of%2520the%2520proposed%250Aapproach%2520is%2520demonstrated%2520through%2520a%2520numerical%2520example.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collision-Free%20Bearing-Driven%20Formation%20Tracking%20for%20Euler-Lagrange%0A%20%20Systems&entry.906535625=Haoshu%20Cheng%20and%20Martin%20Guay%20and%20Shimin%20Wang%20and%20Yunhong%20Che&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20problem%20of%20tracking%20formations%20driven%20by%0Abearings%20for%20heterogeneous%20Euler-Lagrange%20systems%20with%20parametric%20uncertainty%0Ain%20the%20presence%20of%20multiple%20moving%20leaders.%20To%20estimate%20the%20leaders%27%20velocities%0Aand%20accelerations%2C%20we%20first%20design%20a%20distributed%20observer%20for%20the%20leader%0Asystem%2C%20utilizing%20a%20bearing-based%20localization%20condition%20in%20place%20of%20the%0Aconventional%20connectivity%20assumption.%20This%20observer%2C%20coupled%20with%20an%20adaptive%0Amechanism%2C%20enables%20the%20synthesis%20of%20a%20novel%20distributed%20control%20law%20that%20guides%0Athe%20formation%20towards%20the%20target%20formation%2C%20without%20requiring%20prior%20knowledge%0Aof%20the%20system%20parameters.%20Furthermore%2C%20we%20establish%20a%20sufficient%20condition%2C%0Adependent%20on%20the%20initial%20formation%20configuration%2C%20that%20ensures%20collision%0Aavoidance%20throughout%20the%20formation%20evolution.%20The%20effectiveness%20of%20the%20proposed%0Aapproach%20is%20demonstrated%20through%20a%20numerical%20example.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09908v1&entry.124074799=Read"},
{"title": "ViewDelta: Scaling Scene Change Detection through Text-Conditioning", "author": "Subin Varghese and Joshua Gao and Vedhus Hoskere", "abstract": "  We introduce a generalized framework for Scene Change Detection (SCD) that\naddresses the core ambiguity of distinguishing \"relevant\" from \"nuisance\"\nchanges, enabling effective joint training of a single model across diverse\ndomains and applications. Existing methods struggle to generalize due to\ndifferences in dataset labeling, where changes such as vegetation growth or\nlane marking alterations may be labeled as relevant in one dataset and\nirrelevant in another. To resolve this ambiguity, we propose ViewDelta, a text\nconditioned change detection framework that uses natural language prompts to\ndefine relevant changes precisely, such as a single attribute, a specific set\nof classes, or all observable differences. To facilitate training in this\nparadigm, we release the Conditional Change Segmentation dataset (CSeg), the\nfirst large-scale synthetic dataset for text conditioned SCD, consisting of\nover 500,000 image pairs with more than 300,000 unique textual prompts\ndescribing relevant changes. Experiments demonstrate that a single ViewDelta\nmodel trained jointly on CSeg, SYSU-CD, PSCD, VL-CMU-CD, and their unaligned\nvariants achieves performance competitive with or superior to dataset specific\nmodels, highlighting text conditioning as a powerful approach for generalizable\nSCD. Our code and dataset are available at\nhttps://joshuakgao.github.io/viewdelta/.\n", "link": "http://arxiv.org/abs/2412.07612v3", "date": "2025-08-13", "relevancy": 2.2413, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5701}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViewDelta%3A%20Scaling%20Scene%20Change%20Detection%20through%20Text-Conditioning&body=Title%3A%20ViewDelta%3A%20Scaling%20Scene%20Change%20Detection%20through%20Text-Conditioning%0AAuthor%3A%20Subin%20Varghese%20and%20Joshua%20Gao%20and%20Vedhus%20Hoskere%0AAbstract%3A%20%20%20We%20introduce%20a%20generalized%20framework%20for%20Scene%20Change%20Detection%20%28SCD%29%20that%0Aaddresses%20the%20core%20ambiguity%20of%20distinguishing%20%22relevant%22%20from%20%22nuisance%22%0Achanges%2C%20enabling%20effective%20joint%20training%20of%20a%20single%20model%20across%20diverse%0Adomains%20and%20applications.%20Existing%20methods%20struggle%20to%20generalize%20due%20to%0Adifferences%20in%20dataset%20labeling%2C%20where%20changes%20such%20as%20vegetation%20growth%20or%0Alane%20marking%20alterations%20may%20be%20labeled%20as%20relevant%20in%20one%20dataset%20and%0Airrelevant%20in%20another.%20To%20resolve%20this%20ambiguity%2C%20we%20propose%20ViewDelta%2C%20a%20text%0Aconditioned%20change%20detection%20framework%20that%20uses%20natural%20language%20prompts%20to%0Adefine%20relevant%20changes%20precisely%2C%20such%20as%20a%20single%20attribute%2C%20a%20specific%20set%0Aof%20classes%2C%20or%20all%20observable%20differences.%20To%20facilitate%20training%20in%20this%0Aparadigm%2C%20we%20release%20the%20Conditional%20Change%20Segmentation%20dataset%20%28CSeg%29%2C%20the%0Afirst%20large-scale%20synthetic%20dataset%20for%20text%20conditioned%20SCD%2C%20consisting%20of%0Aover%20500%2C000%20image%20pairs%20with%20more%20than%20300%2C000%20unique%20textual%20prompts%0Adescribing%20relevant%20changes.%20Experiments%20demonstrate%20that%20a%20single%20ViewDelta%0Amodel%20trained%20jointly%20on%20CSeg%2C%20SYSU-CD%2C%20PSCD%2C%20VL-CMU-CD%2C%20and%20their%20unaligned%0Avariants%20achieves%20performance%20competitive%20with%20or%20superior%20to%20dataset%20specific%0Amodels%2C%20highlighting%20text%20conditioning%20as%20a%20powerful%20approach%20for%20generalizable%0ASCD.%20Our%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//joshuakgao.github.io/viewdelta/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07612v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViewDelta%253A%2520Scaling%2520Scene%2520Change%2520Detection%2520through%2520Text-Conditioning%26entry.906535625%3DSubin%2520Varghese%2520and%2520Joshua%2520Gao%2520and%2520Vedhus%2520Hoskere%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520generalized%2520framework%2520for%2520Scene%2520Change%2520Detection%2520%2528SCD%2529%2520that%250Aaddresses%2520the%2520core%2520ambiguity%2520of%2520distinguishing%2520%2522relevant%2522%2520from%2520%2522nuisance%2522%250Achanges%252C%2520enabling%2520effective%2520joint%2520training%2520of%2520a%2520single%2520model%2520across%2520diverse%250Adomains%2520and%2520applications.%2520Existing%2520methods%2520struggle%2520to%2520generalize%2520due%2520to%250Adifferences%2520in%2520dataset%2520labeling%252C%2520where%2520changes%2520such%2520as%2520vegetation%2520growth%2520or%250Alane%2520marking%2520alterations%2520may%2520be%2520labeled%2520as%2520relevant%2520in%2520one%2520dataset%2520and%250Airrelevant%2520in%2520another.%2520To%2520resolve%2520this%2520ambiguity%252C%2520we%2520propose%2520ViewDelta%252C%2520a%2520text%250Aconditioned%2520change%2520detection%2520framework%2520that%2520uses%2520natural%2520language%2520prompts%2520to%250Adefine%2520relevant%2520changes%2520precisely%252C%2520such%2520as%2520a%2520single%2520attribute%252C%2520a%2520specific%2520set%250Aof%2520classes%252C%2520or%2520all%2520observable%2520differences.%2520To%2520facilitate%2520training%2520in%2520this%250Aparadigm%252C%2520we%2520release%2520the%2520Conditional%2520Change%2520Segmentation%2520dataset%2520%2528CSeg%2529%252C%2520the%250Afirst%2520large-scale%2520synthetic%2520dataset%2520for%2520text%2520conditioned%2520SCD%252C%2520consisting%2520of%250Aover%2520500%252C000%2520image%2520pairs%2520with%2520more%2520than%2520300%252C000%2520unique%2520textual%2520prompts%250Adescribing%2520relevant%2520changes.%2520Experiments%2520demonstrate%2520that%2520a%2520single%2520ViewDelta%250Amodel%2520trained%2520jointly%2520on%2520CSeg%252C%2520SYSU-CD%252C%2520PSCD%252C%2520VL-CMU-CD%252C%2520and%2520their%2520unaligned%250Avariants%2520achieves%2520performance%2520competitive%2520with%2520or%2520superior%2520to%2520dataset%2520specific%250Amodels%252C%2520highlighting%2520text%2520conditioning%2520as%2520a%2520powerful%2520approach%2520for%2520generalizable%250ASCD.%2520Our%2520code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//joshuakgao.github.io/viewdelta/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07612v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViewDelta%3A%20Scaling%20Scene%20Change%20Detection%20through%20Text-Conditioning&entry.906535625=Subin%20Varghese%20and%20Joshua%20Gao%20and%20Vedhus%20Hoskere&entry.1292438233=%20%20We%20introduce%20a%20generalized%20framework%20for%20Scene%20Change%20Detection%20%28SCD%29%20that%0Aaddresses%20the%20core%20ambiguity%20of%20distinguishing%20%22relevant%22%20from%20%22nuisance%22%0Achanges%2C%20enabling%20effective%20joint%20training%20of%20a%20single%20model%20across%20diverse%0Adomains%20and%20applications.%20Existing%20methods%20struggle%20to%20generalize%20due%20to%0Adifferences%20in%20dataset%20labeling%2C%20where%20changes%20such%20as%20vegetation%20growth%20or%0Alane%20marking%20alterations%20may%20be%20labeled%20as%20relevant%20in%20one%20dataset%20and%0Airrelevant%20in%20another.%20To%20resolve%20this%20ambiguity%2C%20we%20propose%20ViewDelta%2C%20a%20text%0Aconditioned%20change%20detection%20framework%20that%20uses%20natural%20language%20prompts%20to%0Adefine%20relevant%20changes%20precisely%2C%20such%20as%20a%20single%20attribute%2C%20a%20specific%20set%0Aof%20classes%2C%20or%20all%20observable%20differences.%20To%20facilitate%20training%20in%20this%0Aparadigm%2C%20we%20release%20the%20Conditional%20Change%20Segmentation%20dataset%20%28CSeg%29%2C%20the%0Afirst%20large-scale%20synthetic%20dataset%20for%20text%20conditioned%20SCD%2C%20consisting%20of%0Aover%20500%2C000%20image%20pairs%20with%20more%20than%20300%2C000%20unique%20textual%20prompts%0Adescribing%20relevant%20changes.%20Experiments%20demonstrate%20that%20a%20single%20ViewDelta%0Amodel%20trained%20jointly%20on%20CSeg%2C%20SYSU-CD%2C%20PSCD%2C%20VL-CMU-CD%2C%20and%20their%20unaligned%0Avariants%20achieves%20performance%20competitive%20with%20or%20superior%20to%20dataset%20specific%0Amodels%2C%20highlighting%20text%20conditioning%20as%20a%20powerful%20approach%20for%20generalizable%0ASCD.%20Our%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//joshuakgao.github.io/viewdelta/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07612v3&entry.124074799=Read"},
{"title": "MIND: A Noise-Adaptive Denoising Framework for Medical Images\n  Integrating Multi-Scale Transformer", "author": "Tao Tang and Chengxu Yang", "abstract": "  The core role of medical images in disease diagnosis makes their quality\ndirectly affect the accuracy of clinical judgment. However, due to factors such\nas low-dose scanning, equipment limitations and imaging artifacts, medical\nimages are often accompanied by non-uniform noise interference, which seriously\naffects structure recognition and lesion detection. This paper proposes a\nmedical image adaptive denoising model (MI-ND) that integrates multi-scale\nconvolutional and Transformer architecture, introduces a noise level estimator\n(NLE) and a noise adaptive attention module (NAAB), and realizes\nchannel-spatial attention regulation and cross-modal feature fusion driven by\nnoise perception. Systematic testing is carried out on multimodal public\ndatasets. Experiments show that this method significantly outperforms the\ncomparative methods in image quality indicators such as PSNR, SSIM, and LPIPS,\nand improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing\nstrong prac-tical value and promotional potential. The model has outstanding\nbenefits in structural recovery, diagnostic sensitivity, and cross-modal\nrobustness, and provides an effective solution for medical image enhancement\nand AI-assisted diagnosis and treatment.\n", "link": "http://arxiv.org/abs/2508.07817v2", "date": "2025-08-13", "relevancy": 2.2368, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5821}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5813}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIND%3A%20A%20Noise-Adaptive%20Denoising%20Framework%20for%20Medical%20Images%0A%20%20Integrating%20Multi-Scale%20Transformer&body=Title%3A%20MIND%3A%20A%20Noise-Adaptive%20Denoising%20Framework%20for%20Medical%20Images%0A%20%20Integrating%20Multi-Scale%20Transformer%0AAuthor%3A%20Tao%20Tang%20and%20Chengxu%20Yang%0AAbstract%3A%20%20%20The%20core%20role%20of%20medical%20images%20in%20disease%20diagnosis%20makes%20their%20quality%0Adirectly%20affect%20the%20accuracy%20of%20clinical%20judgment.%20However%2C%20due%20to%20factors%20such%0Aas%20low-dose%20scanning%2C%20equipment%20limitations%20and%20imaging%20artifacts%2C%20medical%0Aimages%20are%20often%20accompanied%20by%20non-uniform%20noise%20interference%2C%20which%20seriously%0Aaffects%20structure%20recognition%20and%20lesion%20detection.%20This%20paper%20proposes%20a%0Amedical%20image%20adaptive%20denoising%20model%20%28MI-ND%29%20that%20integrates%20multi-scale%0Aconvolutional%20and%20Transformer%20architecture%2C%20introduces%20a%20noise%20level%20estimator%0A%28NLE%29%20and%20a%20noise%20adaptive%20attention%20module%20%28NAAB%29%2C%20and%20realizes%0Achannel-spatial%20attention%20regulation%20and%20cross-modal%20feature%20fusion%20driven%20by%0Anoise%20perception.%20Systematic%20testing%20is%20carried%20out%20on%20multimodal%20public%0Adatasets.%20Experiments%20show%20that%20this%20method%20significantly%20outperforms%20the%0Acomparative%20methods%20in%20image%20quality%20indicators%20such%20as%20PSNR%2C%20SSIM%2C%20and%20LPIPS%2C%0Aand%20improves%20the%20F1%20score%20and%20ROC-AUC%20in%20downstream%20diagnostic%20tasks%2C%20showing%0Astrong%20prac-tical%20value%20and%20promotional%20potential.%20The%20model%20has%20outstanding%0Abenefits%20in%20structural%20recovery%2C%20diagnostic%20sensitivity%2C%20and%20cross-modal%0Arobustness%2C%20and%20provides%20an%20effective%20solution%20for%20medical%20image%20enhancement%0Aand%20AI-assisted%20diagnosis%20and%20treatment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07817v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIND%253A%2520A%2520Noise-Adaptive%2520Denoising%2520Framework%2520for%2520Medical%2520Images%250A%2520%2520Integrating%2520Multi-Scale%2520Transformer%26entry.906535625%3DTao%2520Tang%2520and%2520Chengxu%2520Yang%26entry.1292438233%3D%2520%2520The%2520core%2520role%2520of%2520medical%2520images%2520in%2520disease%2520diagnosis%2520makes%2520their%2520quality%250Adirectly%2520affect%2520the%2520accuracy%2520of%2520clinical%2520judgment.%2520However%252C%2520due%2520to%2520factors%2520such%250Aas%2520low-dose%2520scanning%252C%2520equipment%2520limitations%2520and%2520imaging%2520artifacts%252C%2520medical%250Aimages%2520are%2520often%2520accompanied%2520by%2520non-uniform%2520noise%2520interference%252C%2520which%2520seriously%250Aaffects%2520structure%2520recognition%2520and%2520lesion%2520detection.%2520This%2520paper%2520proposes%2520a%250Amedical%2520image%2520adaptive%2520denoising%2520model%2520%2528MI-ND%2529%2520that%2520integrates%2520multi-scale%250Aconvolutional%2520and%2520Transformer%2520architecture%252C%2520introduces%2520a%2520noise%2520level%2520estimator%250A%2528NLE%2529%2520and%2520a%2520noise%2520adaptive%2520attention%2520module%2520%2528NAAB%2529%252C%2520and%2520realizes%250Achannel-spatial%2520attention%2520regulation%2520and%2520cross-modal%2520feature%2520fusion%2520driven%2520by%250Anoise%2520perception.%2520Systematic%2520testing%2520is%2520carried%2520out%2520on%2520multimodal%2520public%250Adatasets.%2520Experiments%2520show%2520that%2520this%2520method%2520significantly%2520outperforms%2520the%250Acomparative%2520methods%2520in%2520image%2520quality%2520indicators%2520such%2520as%2520PSNR%252C%2520SSIM%252C%2520and%2520LPIPS%252C%250Aand%2520improves%2520the%2520F1%2520score%2520and%2520ROC-AUC%2520in%2520downstream%2520diagnostic%2520tasks%252C%2520showing%250Astrong%2520prac-tical%2520value%2520and%2520promotional%2520potential.%2520The%2520model%2520has%2520outstanding%250Abenefits%2520in%2520structural%2520recovery%252C%2520diagnostic%2520sensitivity%252C%2520and%2520cross-modal%250Arobustness%252C%2520and%2520provides%2520an%2520effective%2520solution%2520for%2520medical%2520image%2520enhancement%250Aand%2520AI-assisted%2520diagnosis%2520and%2520treatment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07817v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIND%3A%20A%20Noise-Adaptive%20Denoising%20Framework%20for%20Medical%20Images%0A%20%20Integrating%20Multi-Scale%20Transformer&entry.906535625=Tao%20Tang%20and%20Chengxu%20Yang&entry.1292438233=%20%20The%20core%20role%20of%20medical%20images%20in%20disease%20diagnosis%20makes%20their%20quality%0Adirectly%20affect%20the%20accuracy%20of%20clinical%20judgment.%20However%2C%20due%20to%20factors%20such%0Aas%20low-dose%20scanning%2C%20equipment%20limitations%20and%20imaging%20artifacts%2C%20medical%0Aimages%20are%20often%20accompanied%20by%20non-uniform%20noise%20interference%2C%20which%20seriously%0Aaffects%20structure%20recognition%20and%20lesion%20detection.%20This%20paper%20proposes%20a%0Amedical%20image%20adaptive%20denoising%20model%20%28MI-ND%29%20that%20integrates%20multi-scale%0Aconvolutional%20and%20Transformer%20architecture%2C%20introduces%20a%20noise%20level%20estimator%0A%28NLE%29%20and%20a%20noise%20adaptive%20attention%20module%20%28NAAB%29%2C%20and%20realizes%0Achannel-spatial%20attention%20regulation%20and%20cross-modal%20feature%20fusion%20driven%20by%0Anoise%20perception.%20Systematic%20testing%20is%20carried%20out%20on%20multimodal%20public%0Adatasets.%20Experiments%20show%20that%20this%20method%20significantly%20outperforms%20the%0Acomparative%20methods%20in%20image%20quality%20indicators%20such%20as%20PSNR%2C%20SSIM%2C%20and%20LPIPS%2C%0Aand%20improves%20the%20F1%20score%20and%20ROC-AUC%20in%20downstream%20diagnostic%20tasks%2C%20showing%0Astrong%20prac-tical%20value%20and%20promotional%20potential.%20The%20model%20has%20outstanding%0Abenefits%20in%20structural%20recovery%2C%20diagnostic%20sensitivity%2C%20and%20cross-modal%0Arobustness%2C%20and%20provides%20an%20effective%20solution%20for%20medical%20image%20enhancement%0Aand%20AI-assisted%20diagnosis%20and%20treatment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07817v2&entry.124074799=Read"},
{"title": "Multi-Target Backdoor Attacks Against Speaker Recognition", "author": "Alexandrine Fortier and Sonal Joshi and Thomas Thebaud and Jesus Villalba Lopez and Najim Dehak and Patrick Cardinal", "abstract": "  In this work, we propose a multi-target backdoor attack against speaker\nidentification using position-independent clicking sounds as triggers. Unlike\nprevious single-target approaches, our method targets up to 50 speakers\nsimultaneously, achieving success rates of up to 95.04%. To simulate more\nrealistic attack conditions, we vary the signal-to-noise ratio between speech\nand trigger, demonstrating a trade-off between stealth and effectiveness. We\nfurther extend the attack to the speaker verification task by selecting the\nmost similar training speaker - based on cosine similarity - as a proxy target.\nThe attack is most effective when target and enrolled speaker pairs are highly\nsimilar, reaching success rates of up to 90% in such cases.\n", "link": "http://arxiv.org/abs/2508.08559v2", "date": "2025-08-13", "relevancy": 2.2317, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4699}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.44}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Target%20Backdoor%20Attacks%20Against%20Speaker%20Recognition&body=Title%3A%20Multi-Target%20Backdoor%20Attacks%20Against%20Speaker%20Recognition%0AAuthor%3A%20Alexandrine%20Fortier%20and%20Sonal%20Joshi%20and%20Thomas%20Thebaud%20and%20Jesus%20Villalba%20Lopez%20and%20Najim%20Dehak%20and%20Patrick%20Cardinal%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20multi-target%20backdoor%20attack%20against%20speaker%0Aidentification%20using%20position-independent%20clicking%20sounds%20as%20triggers.%20Unlike%0Aprevious%20single-target%20approaches%2C%20our%20method%20targets%20up%20to%2050%20speakers%0Asimultaneously%2C%20achieving%20success%20rates%20of%20up%20to%2095.04%25.%20To%20simulate%20more%0Arealistic%20attack%20conditions%2C%20we%20vary%20the%20signal-to-noise%20ratio%20between%20speech%0Aand%20trigger%2C%20demonstrating%20a%20trade-off%20between%20stealth%20and%20effectiveness.%20We%0Afurther%20extend%20the%20attack%20to%20the%20speaker%20verification%20task%20by%20selecting%20the%0Amost%20similar%20training%20speaker%20-%20based%20on%20cosine%20similarity%20-%20as%20a%20proxy%20target.%0AThe%20attack%20is%20most%20effective%20when%20target%20and%20enrolled%20speaker%20pairs%20are%20highly%0Asimilar%2C%20reaching%20success%20rates%20of%20up%20to%2090%25%20in%20such%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08559v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Target%2520Backdoor%2520Attacks%2520Against%2520Speaker%2520Recognition%26entry.906535625%3DAlexandrine%2520Fortier%2520and%2520Sonal%2520Joshi%2520and%2520Thomas%2520Thebaud%2520and%2520Jesus%2520Villalba%2520Lopez%2520and%2520Najim%2520Dehak%2520and%2520Patrick%2520Cardinal%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520multi-target%2520backdoor%2520attack%2520against%2520speaker%250Aidentification%2520using%2520position-independent%2520clicking%2520sounds%2520as%2520triggers.%2520Unlike%250Aprevious%2520single-target%2520approaches%252C%2520our%2520method%2520targets%2520up%2520to%252050%2520speakers%250Asimultaneously%252C%2520achieving%2520success%2520rates%2520of%2520up%2520to%252095.04%2525.%2520To%2520simulate%2520more%250Arealistic%2520attack%2520conditions%252C%2520we%2520vary%2520the%2520signal-to-noise%2520ratio%2520between%2520speech%250Aand%2520trigger%252C%2520demonstrating%2520a%2520trade-off%2520between%2520stealth%2520and%2520effectiveness.%2520We%250Afurther%2520extend%2520the%2520attack%2520to%2520the%2520speaker%2520verification%2520task%2520by%2520selecting%2520the%250Amost%2520similar%2520training%2520speaker%2520-%2520based%2520on%2520cosine%2520similarity%2520-%2520as%2520a%2520proxy%2520target.%250AThe%2520attack%2520is%2520most%2520effective%2520when%2520target%2520and%2520enrolled%2520speaker%2520pairs%2520are%2520highly%250Asimilar%252C%2520reaching%2520success%2520rates%2520of%2520up%2520to%252090%2525%2520in%2520such%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08559v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Target%20Backdoor%20Attacks%20Against%20Speaker%20Recognition&entry.906535625=Alexandrine%20Fortier%20and%20Sonal%20Joshi%20and%20Thomas%20Thebaud%20and%20Jesus%20Villalba%20Lopez%20and%20Najim%20Dehak%20and%20Patrick%20Cardinal&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20multi-target%20backdoor%20attack%20against%20speaker%0Aidentification%20using%20position-independent%20clicking%20sounds%20as%20triggers.%20Unlike%0Aprevious%20single-target%20approaches%2C%20our%20method%20targets%20up%20to%2050%20speakers%0Asimultaneously%2C%20achieving%20success%20rates%20of%20up%20to%2095.04%25.%20To%20simulate%20more%0Arealistic%20attack%20conditions%2C%20we%20vary%20the%20signal-to-noise%20ratio%20between%20speech%0Aand%20trigger%2C%20demonstrating%20a%20trade-off%20between%20stealth%20and%20effectiveness.%20We%0Afurther%20extend%20the%20attack%20to%20the%20speaker%20verification%20task%20by%20selecting%20the%0Amost%20similar%20training%20speaker%20-%20based%20on%20cosine%20similarity%20-%20as%20a%20proxy%20target.%0AThe%20attack%20is%20most%20effective%20when%20target%20and%20enrolled%20speaker%20pairs%20are%20highly%0Asimilar%2C%20reaching%20success%20rates%20of%20up%20to%2090%25%20in%20such%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08559v2&entry.124074799=Read"},
{"title": "GenAI Confessions: Black-box Membership Inference for Generative Image\n  Models", "author": "Matyas Bohacek and Hany Farid", "abstract": "  From a simple text prompt, generative-AI image models can create stunningly\nrealistic and creative images bounded, it seems, by only our imagination. These\nmodels have achieved this remarkable feat thanks, in part, to the ingestion of\nbillions of images collected from nearly every corner of the internet. Many\ncreators have understandably expressed concern over how their intellectual\nproperty has been ingested without their permission or a mechanism to opt out\nof training. As a result, questions of fair use and copyright infringement have\nquickly emerged. We describe a method that allows us to determine if a model\nwas trained on a specific image or set of images. This method is\ncomputationally efficient and assumes no explicit knowledge of the model\narchitecture or weights (so-called black-box membership inference). We\nanticipate that this method will be crucial for auditing existing models and,\nlooking ahead, ensuring the fairer development and deployment of generative AI\nmodels.\n", "link": "http://arxiv.org/abs/2501.06399v2", "date": "2025-08-13", "relevancy": 2.2302, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5928}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5768}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenAI%20Confessions%3A%20Black-box%20Membership%20Inference%20for%20Generative%20Image%0A%20%20Models&body=Title%3A%20GenAI%20Confessions%3A%20Black-box%20Membership%20Inference%20for%20Generative%20Image%0A%20%20Models%0AAuthor%3A%20Matyas%20Bohacek%20and%20Hany%20Farid%0AAbstract%3A%20%20%20From%20a%20simple%20text%20prompt%2C%20generative-AI%20image%20models%20can%20create%20stunningly%0Arealistic%20and%20creative%20images%20bounded%2C%20it%20seems%2C%20by%20only%20our%20imagination.%20These%0Amodels%20have%20achieved%20this%20remarkable%20feat%20thanks%2C%20in%20part%2C%20to%20the%20ingestion%20of%0Abillions%20of%20images%20collected%20from%20nearly%20every%20corner%20of%20the%20internet.%20Many%0Acreators%20have%20understandably%20expressed%20concern%20over%20how%20their%20intellectual%0Aproperty%20has%20been%20ingested%20without%20their%20permission%20or%20a%20mechanism%20to%20opt%20out%0Aof%20training.%20As%20a%20result%2C%20questions%20of%20fair%20use%20and%20copyright%20infringement%20have%0Aquickly%20emerged.%20We%20describe%20a%20method%20that%20allows%20us%20to%20determine%20if%20a%20model%0Awas%20trained%20on%20a%20specific%20image%20or%20set%20of%20images.%20This%20method%20is%0Acomputationally%20efficient%20and%20assumes%20no%20explicit%20knowledge%20of%20the%20model%0Aarchitecture%20or%20weights%20%28so-called%20black-box%20membership%20inference%29.%20We%0Aanticipate%20that%20this%20method%20will%20be%20crucial%20for%20auditing%20existing%20models%20and%2C%0Alooking%20ahead%2C%20ensuring%20the%20fairer%20development%20and%20deployment%20of%20generative%20AI%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06399v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenAI%2520Confessions%253A%2520Black-box%2520Membership%2520Inference%2520for%2520Generative%2520Image%250A%2520%2520Models%26entry.906535625%3DMatyas%2520Bohacek%2520and%2520Hany%2520Farid%26entry.1292438233%3D%2520%2520From%2520a%2520simple%2520text%2520prompt%252C%2520generative-AI%2520image%2520models%2520can%2520create%2520stunningly%250Arealistic%2520and%2520creative%2520images%2520bounded%252C%2520it%2520seems%252C%2520by%2520only%2520our%2520imagination.%2520These%250Amodels%2520have%2520achieved%2520this%2520remarkable%2520feat%2520thanks%252C%2520in%2520part%252C%2520to%2520the%2520ingestion%2520of%250Abillions%2520of%2520images%2520collected%2520from%2520nearly%2520every%2520corner%2520of%2520the%2520internet.%2520Many%250Acreators%2520have%2520understandably%2520expressed%2520concern%2520over%2520how%2520their%2520intellectual%250Aproperty%2520has%2520been%2520ingested%2520without%2520their%2520permission%2520or%2520a%2520mechanism%2520to%2520opt%2520out%250Aof%2520training.%2520As%2520a%2520result%252C%2520questions%2520of%2520fair%2520use%2520and%2520copyright%2520infringement%2520have%250Aquickly%2520emerged.%2520We%2520describe%2520a%2520method%2520that%2520allows%2520us%2520to%2520determine%2520if%2520a%2520model%250Awas%2520trained%2520on%2520a%2520specific%2520image%2520or%2520set%2520of%2520images.%2520This%2520method%2520is%250Acomputationally%2520efficient%2520and%2520assumes%2520no%2520explicit%2520knowledge%2520of%2520the%2520model%250Aarchitecture%2520or%2520weights%2520%2528so-called%2520black-box%2520membership%2520inference%2529.%2520We%250Aanticipate%2520that%2520this%2520method%2520will%2520be%2520crucial%2520for%2520auditing%2520existing%2520models%2520and%252C%250Alooking%2520ahead%252C%2520ensuring%2520the%2520fairer%2520development%2520and%2520deployment%2520of%2520generative%2520AI%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06399v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenAI%20Confessions%3A%20Black-box%20Membership%20Inference%20for%20Generative%20Image%0A%20%20Models&entry.906535625=Matyas%20Bohacek%20and%20Hany%20Farid&entry.1292438233=%20%20From%20a%20simple%20text%20prompt%2C%20generative-AI%20image%20models%20can%20create%20stunningly%0Arealistic%20and%20creative%20images%20bounded%2C%20it%20seems%2C%20by%20only%20our%20imagination.%20These%0Amodels%20have%20achieved%20this%20remarkable%20feat%20thanks%2C%20in%20part%2C%20to%20the%20ingestion%20of%0Abillions%20of%20images%20collected%20from%20nearly%20every%20corner%20of%20the%20internet.%20Many%0Acreators%20have%20understandably%20expressed%20concern%20over%20how%20their%20intellectual%0Aproperty%20has%20been%20ingested%20without%20their%20permission%20or%20a%20mechanism%20to%20opt%20out%0Aof%20training.%20As%20a%20result%2C%20questions%20of%20fair%20use%20and%20copyright%20infringement%20have%0Aquickly%20emerged.%20We%20describe%20a%20method%20that%20allows%20us%20to%20determine%20if%20a%20model%0Awas%20trained%20on%20a%20specific%20image%20or%20set%20of%20images.%20This%20method%20is%0Acomputationally%20efficient%20and%20assumes%20no%20explicit%20knowledge%20of%20the%20model%0Aarchitecture%20or%20weights%20%28so-called%20black-box%20membership%20inference%29.%20We%0Aanticipate%20that%20this%20method%20will%20be%20crucial%20for%20auditing%20existing%20models%20and%2C%0Alooking%20ahead%2C%20ensuring%20the%20fairer%20development%20and%20deployment%20of%20generative%20AI%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06399v2&entry.124074799=Read"},
{"title": "GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph\n  Generation", "author": "Yitong Luo and Islem Rekik", "abstract": "  Brain connectomes, representing neural connectivity as graphs, are crucial\nfor understanding brain organization but costly and time-consuming to acquire,\nmotivating generative approaches. Recent advances in graph generative modeling\noffer a data-driven alternative, enabling synthetic connectome generation and\nreducing dependence on large neuroimaging datasets. However, current models\nface key limitations: (i) compressing the whole graph into a single latent code\n(e.g., VGAEs) blurs fine-grained local motifs; (ii) relying on rich node\nattributes rarely available in connectomes reduces reconstruction quality;\n(iii) edge-centric models emphasize topology but overlook accurate edge-weight\nprediction, harming quantitative fidelity; and (iv) computationally expensive\ndesigns (e.g., edge-conditioned convolutions) impose high memory demands,\nlimiting scalability. We propose GraphTreeGen (GTG), a subtree-centric\ngenerative framework for efficient, accurate connectome synthesis. GTG\ndecomposes each connectome into entropy-guided k-hop trees capturing\ninformative local structure, encoded by a shared GCN. A bipartite\nmessage-passing layer fuses subtree embeddings with global node features, while\na dual-branch decoder jointly predicts edge existence and weights to\nreconstruct the adjacency matrix. GTG outperforms state-of-the-art baselines in\nself-supervised tasks and remains competitive in supervised settings,\ndelivering higher structural fidelity and more precise weights with far less\nmemory. Its modular design enables extensions to connectome super-resolution\nand cross-modality synthesis. Code: https://github.com/basiralab/GTG/\n", "link": "http://arxiv.org/abs/2508.09710v1", "date": "2025-08-13", "relevancy": 2.2254, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5591}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5566}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphTreeGen%3A%20Subtree-Centric%20Approach%20to%20Efficient%20and%20Supervised%20Graph%0A%20%20Generation&body=Title%3A%20GraphTreeGen%3A%20Subtree-Centric%20Approach%20to%20Efficient%20and%20Supervised%20Graph%0A%20%20Generation%0AAuthor%3A%20Yitong%20Luo%20and%20Islem%20Rekik%0AAbstract%3A%20%20%20Brain%20connectomes%2C%20representing%20neural%20connectivity%20as%20graphs%2C%20are%20crucial%0Afor%20understanding%20brain%20organization%20but%20costly%20and%20time-consuming%20to%20acquire%2C%0Amotivating%20generative%20approaches.%20Recent%20advances%20in%20graph%20generative%20modeling%0Aoffer%20a%20data-driven%20alternative%2C%20enabling%20synthetic%20connectome%20generation%20and%0Areducing%20dependence%20on%20large%20neuroimaging%20datasets.%20However%2C%20current%20models%0Aface%20key%20limitations%3A%20%28i%29%20compressing%20the%20whole%20graph%20into%20a%20single%20latent%20code%0A%28e.g.%2C%20VGAEs%29%20blurs%20fine-grained%20local%20motifs%3B%20%28ii%29%20relying%20on%20rich%20node%0Aattributes%20rarely%20available%20in%20connectomes%20reduces%20reconstruction%20quality%3B%0A%28iii%29%20edge-centric%20models%20emphasize%20topology%20but%20overlook%20accurate%20edge-weight%0Aprediction%2C%20harming%20quantitative%20fidelity%3B%20and%20%28iv%29%20computationally%20expensive%0Adesigns%20%28e.g.%2C%20edge-conditioned%20convolutions%29%20impose%20high%20memory%20demands%2C%0Alimiting%20scalability.%20We%20propose%20GraphTreeGen%20%28GTG%29%2C%20a%20subtree-centric%0Agenerative%20framework%20for%20efficient%2C%20accurate%20connectome%20synthesis.%20GTG%0Adecomposes%20each%20connectome%20into%20entropy-guided%20k-hop%20trees%20capturing%0Ainformative%20local%20structure%2C%20encoded%20by%20a%20shared%20GCN.%20A%20bipartite%0Amessage-passing%20layer%20fuses%20subtree%20embeddings%20with%20global%20node%20features%2C%20while%0Aa%20dual-branch%20decoder%20jointly%20predicts%20edge%20existence%20and%20weights%20to%0Areconstruct%20the%20adjacency%20matrix.%20GTG%20outperforms%20state-of-the-art%20baselines%20in%0Aself-supervised%20tasks%20and%20remains%20competitive%20in%20supervised%20settings%2C%0Adelivering%20higher%20structural%20fidelity%20and%20more%20precise%20weights%20with%20far%20less%0Amemory.%20Its%20modular%20design%20enables%20extensions%20to%20connectome%20super-resolution%0Aand%20cross-modality%20synthesis.%20Code%3A%20https%3A//github.com/basiralab/GTG/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphTreeGen%253A%2520Subtree-Centric%2520Approach%2520to%2520Efficient%2520and%2520Supervised%2520Graph%250A%2520%2520Generation%26entry.906535625%3DYitong%2520Luo%2520and%2520Islem%2520Rekik%26entry.1292438233%3D%2520%2520Brain%2520connectomes%252C%2520representing%2520neural%2520connectivity%2520as%2520graphs%252C%2520are%2520crucial%250Afor%2520understanding%2520brain%2520organization%2520but%2520costly%2520and%2520time-consuming%2520to%2520acquire%252C%250Amotivating%2520generative%2520approaches.%2520Recent%2520advances%2520in%2520graph%2520generative%2520modeling%250Aoffer%2520a%2520data-driven%2520alternative%252C%2520enabling%2520synthetic%2520connectome%2520generation%2520and%250Areducing%2520dependence%2520on%2520large%2520neuroimaging%2520datasets.%2520However%252C%2520current%2520models%250Aface%2520key%2520limitations%253A%2520%2528i%2529%2520compressing%2520the%2520whole%2520graph%2520into%2520a%2520single%2520latent%2520code%250A%2528e.g.%252C%2520VGAEs%2529%2520blurs%2520fine-grained%2520local%2520motifs%253B%2520%2528ii%2529%2520relying%2520on%2520rich%2520node%250Aattributes%2520rarely%2520available%2520in%2520connectomes%2520reduces%2520reconstruction%2520quality%253B%250A%2528iii%2529%2520edge-centric%2520models%2520emphasize%2520topology%2520but%2520overlook%2520accurate%2520edge-weight%250Aprediction%252C%2520harming%2520quantitative%2520fidelity%253B%2520and%2520%2528iv%2529%2520computationally%2520expensive%250Adesigns%2520%2528e.g.%252C%2520edge-conditioned%2520convolutions%2529%2520impose%2520high%2520memory%2520demands%252C%250Alimiting%2520scalability.%2520We%2520propose%2520GraphTreeGen%2520%2528GTG%2529%252C%2520a%2520subtree-centric%250Agenerative%2520framework%2520for%2520efficient%252C%2520accurate%2520connectome%2520synthesis.%2520GTG%250Adecomposes%2520each%2520connectome%2520into%2520entropy-guided%2520k-hop%2520trees%2520capturing%250Ainformative%2520local%2520structure%252C%2520encoded%2520by%2520a%2520shared%2520GCN.%2520A%2520bipartite%250Amessage-passing%2520layer%2520fuses%2520subtree%2520embeddings%2520with%2520global%2520node%2520features%252C%2520while%250Aa%2520dual-branch%2520decoder%2520jointly%2520predicts%2520edge%2520existence%2520and%2520weights%2520to%250Areconstruct%2520the%2520adjacency%2520matrix.%2520GTG%2520outperforms%2520state-of-the-art%2520baselines%2520in%250Aself-supervised%2520tasks%2520and%2520remains%2520competitive%2520in%2520supervised%2520settings%252C%250Adelivering%2520higher%2520structural%2520fidelity%2520and%2520more%2520precise%2520weights%2520with%2520far%2520less%250Amemory.%2520Its%2520modular%2520design%2520enables%2520extensions%2520to%2520connectome%2520super-resolution%250Aand%2520cross-modality%2520synthesis.%2520Code%253A%2520https%253A//github.com/basiralab/GTG/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphTreeGen%3A%20Subtree-Centric%20Approach%20to%20Efficient%20and%20Supervised%20Graph%0A%20%20Generation&entry.906535625=Yitong%20Luo%20and%20Islem%20Rekik&entry.1292438233=%20%20Brain%20connectomes%2C%20representing%20neural%20connectivity%20as%20graphs%2C%20are%20crucial%0Afor%20understanding%20brain%20organization%20but%20costly%20and%20time-consuming%20to%20acquire%2C%0Amotivating%20generative%20approaches.%20Recent%20advances%20in%20graph%20generative%20modeling%0Aoffer%20a%20data-driven%20alternative%2C%20enabling%20synthetic%20connectome%20generation%20and%0Areducing%20dependence%20on%20large%20neuroimaging%20datasets.%20However%2C%20current%20models%0Aface%20key%20limitations%3A%20%28i%29%20compressing%20the%20whole%20graph%20into%20a%20single%20latent%20code%0A%28e.g.%2C%20VGAEs%29%20blurs%20fine-grained%20local%20motifs%3B%20%28ii%29%20relying%20on%20rich%20node%0Aattributes%20rarely%20available%20in%20connectomes%20reduces%20reconstruction%20quality%3B%0A%28iii%29%20edge-centric%20models%20emphasize%20topology%20but%20overlook%20accurate%20edge-weight%0Aprediction%2C%20harming%20quantitative%20fidelity%3B%20and%20%28iv%29%20computationally%20expensive%0Adesigns%20%28e.g.%2C%20edge-conditioned%20convolutions%29%20impose%20high%20memory%20demands%2C%0Alimiting%20scalability.%20We%20propose%20GraphTreeGen%20%28GTG%29%2C%20a%20subtree-centric%0Agenerative%20framework%20for%20efficient%2C%20accurate%20connectome%20synthesis.%20GTG%0Adecomposes%20each%20connectome%20into%20entropy-guided%20k-hop%20trees%20capturing%0Ainformative%20local%20structure%2C%20encoded%20by%20a%20shared%20GCN.%20A%20bipartite%0Amessage-passing%20layer%20fuses%20subtree%20embeddings%20with%20global%20node%20features%2C%20while%0Aa%20dual-branch%20decoder%20jointly%20predicts%20edge%20existence%20and%20weights%20to%0Areconstruct%20the%20adjacency%20matrix.%20GTG%20outperforms%20state-of-the-art%20baselines%20in%0Aself-supervised%20tasks%20and%20remains%20competitive%20in%20supervised%20settings%2C%0Adelivering%20higher%20structural%20fidelity%20and%20more%20precise%20weights%20with%20far%20less%0Amemory.%20Its%20modular%20design%20enables%20extensions%20to%20connectome%20super-resolution%0Aand%20cross-modality%20synthesis.%20Code%3A%20https%3A//github.com/basiralab/GTG/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09710v1&entry.124074799=Read"},
{"title": "DRWKV: Focusing on Object Edges for Low-Light Image Enhancement", "author": "Xuecheng Bai and Yuxiang Wang and Boyu Hu and Qinyuan Jie and Chuanzhi Xu and Hongru Xiao and Kechen Li and Vera Chung", "abstract": "  Low-light image enhancement remains a challenging task, particularly in\npreserving object edge continuity and fine structural details under extreme\nillumination degradation. In this paper, we propose a novel model, DRWKV\n(Detailed Receptance Weighted Key Value), which integrates our proposed Global\nEdge Retinex (GER) theory, enabling effective decoupling of illumination and\nedge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV\nAttention, a spiral-scanning mechanism that captures spatial edge continuity\nand models irregular structures more effectively. Thirdly, we design the\nBilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align\nluminance and chrominance features, improving visual naturalness and mitigating\nartifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV\nachieves leading performance in PSNR, SSIM, and NIQE while maintaining low\ncomputational complexity. Furthermore, DRWKV enhances downstream performance in\nlow-light multi-object tracking tasks, validating its generalization\ncapabilities.\n", "link": "http://arxiv.org/abs/2507.18594v2", "date": "2025-08-13", "relevancy": 2.2146, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5566}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5532}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRWKV%3A%20Focusing%20on%20Object%20Edges%20for%20Low-Light%20Image%20Enhancement&body=Title%3A%20DRWKV%3A%20Focusing%20on%20Object%20Edges%20for%20Low-Light%20Image%20Enhancement%0AAuthor%3A%20Xuecheng%20Bai%20and%20Yuxiang%20Wang%20and%20Boyu%20Hu%20and%20Qinyuan%20Jie%20and%20Chuanzhi%20Xu%20and%20Hongru%20Xiao%20and%20Kechen%20Li%20and%20Vera%20Chung%0AAbstract%3A%20%20%20Low-light%20image%20enhancement%20remains%20a%20challenging%20task%2C%20particularly%20in%0Apreserving%20object%20edge%20continuity%20and%20fine%20structural%20details%20under%20extreme%0Aillumination%20degradation.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20model%2C%20DRWKV%0A%28Detailed%20Receptance%20Weighted%20Key%20Value%29%2C%20which%20integrates%20our%20proposed%20Global%0AEdge%20Retinex%20%28GER%29%20theory%2C%20enabling%20effective%20decoupling%20of%20illumination%20and%0Aedge%20structures%20for%20enhanced%20edge%20fidelity.%20Secondly%2C%20we%20introduce%20Evolving%20WKV%0AAttention%2C%20a%20spiral-scanning%20mechanism%20that%20captures%20spatial%20edge%20continuity%0Aand%20models%20irregular%20structures%20more%20effectively.%20Thirdly%2C%20we%20design%20the%0ABilateral%20Spectrum%20Aligner%20%28Bi-SAB%29%20and%20a%20tailored%20MS2-Loss%20to%20jointly%20align%0Aluminance%20and%20chrominance%20features%2C%20improving%20visual%20naturalness%20and%20mitigating%0Aartifacts.%20Extensive%20experiments%20on%20five%20LLIE%20benchmarks%20demonstrate%20that%20DRWKV%0Aachieves%20leading%20performance%20in%20PSNR%2C%20SSIM%2C%20and%20NIQE%20while%20maintaining%20low%0Acomputational%20complexity.%20Furthermore%2C%20DRWKV%20enhances%20downstream%20performance%20in%0Alow-light%20multi-object%20tracking%20tasks%2C%20validating%20its%20generalization%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18594v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRWKV%253A%2520Focusing%2520on%2520Object%2520Edges%2520for%2520Low-Light%2520Image%2520Enhancement%26entry.906535625%3DXuecheng%2520Bai%2520and%2520Yuxiang%2520Wang%2520and%2520Boyu%2520Hu%2520and%2520Qinyuan%2520Jie%2520and%2520Chuanzhi%2520Xu%2520and%2520Hongru%2520Xiao%2520and%2520Kechen%2520Li%2520and%2520Vera%2520Chung%26entry.1292438233%3D%2520%2520Low-light%2520image%2520enhancement%2520remains%2520a%2520challenging%2520task%252C%2520particularly%2520in%250Apreserving%2520object%2520edge%2520continuity%2520and%2520fine%2520structural%2520details%2520under%2520extreme%250Aillumination%2520degradation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520model%252C%2520DRWKV%250A%2528Detailed%2520Receptance%2520Weighted%2520Key%2520Value%2529%252C%2520which%2520integrates%2520our%2520proposed%2520Global%250AEdge%2520Retinex%2520%2528GER%2529%2520theory%252C%2520enabling%2520effective%2520decoupling%2520of%2520illumination%2520and%250Aedge%2520structures%2520for%2520enhanced%2520edge%2520fidelity.%2520Secondly%252C%2520we%2520introduce%2520Evolving%2520WKV%250AAttention%252C%2520a%2520spiral-scanning%2520mechanism%2520that%2520captures%2520spatial%2520edge%2520continuity%250Aand%2520models%2520irregular%2520structures%2520more%2520effectively.%2520Thirdly%252C%2520we%2520design%2520the%250ABilateral%2520Spectrum%2520Aligner%2520%2528Bi-SAB%2529%2520and%2520a%2520tailored%2520MS2-Loss%2520to%2520jointly%2520align%250Aluminance%2520and%2520chrominance%2520features%252C%2520improving%2520visual%2520naturalness%2520and%2520mitigating%250Aartifacts.%2520Extensive%2520experiments%2520on%2520five%2520LLIE%2520benchmarks%2520demonstrate%2520that%2520DRWKV%250Aachieves%2520leading%2520performance%2520in%2520PSNR%252C%2520SSIM%252C%2520and%2520NIQE%2520while%2520maintaining%2520low%250Acomputational%2520complexity.%2520Furthermore%252C%2520DRWKV%2520enhances%2520downstream%2520performance%2520in%250Alow-light%2520multi-object%2520tracking%2520tasks%252C%2520validating%2520its%2520generalization%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18594v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRWKV%3A%20Focusing%20on%20Object%20Edges%20for%20Low-Light%20Image%20Enhancement&entry.906535625=Xuecheng%20Bai%20and%20Yuxiang%20Wang%20and%20Boyu%20Hu%20and%20Qinyuan%20Jie%20and%20Chuanzhi%20Xu%20and%20Hongru%20Xiao%20and%20Kechen%20Li%20and%20Vera%20Chung&entry.1292438233=%20%20Low-light%20image%20enhancement%20remains%20a%20challenging%20task%2C%20particularly%20in%0Apreserving%20object%20edge%20continuity%20and%20fine%20structural%20details%20under%20extreme%0Aillumination%20degradation.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20model%2C%20DRWKV%0A%28Detailed%20Receptance%20Weighted%20Key%20Value%29%2C%20which%20integrates%20our%20proposed%20Global%0AEdge%20Retinex%20%28GER%29%20theory%2C%20enabling%20effective%20decoupling%20of%20illumination%20and%0Aedge%20structures%20for%20enhanced%20edge%20fidelity.%20Secondly%2C%20we%20introduce%20Evolving%20WKV%0AAttention%2C%20a%20spiral-scanning%20mechanism%20that%20captures%20spatial%20edge%20continuity%0Aand%20models%20irregular%20structures%20more%20effectively.%20Thirdly%2C%20we%20design%20the%0ABilateral%20Spectrum%20Aligner%20%28Bi-SAB%29%20and%20a%20tailored%20MS2-Loss%20to%20jointly%20align%0Aluminance%20and%20chrominance%20features%2C%20improving%20visual%20naturalness%20and%20mitigating%0Aartifacts.%20Extensive%20experiments%20on%20five%20LLIE%20benchmarks%20demonstrate%20that%20DRWKV%0Aachieves%20leading%20performance%20in%20PSNR%2C%20SSIM%2C%20and%20NIQE%20while%20maintaining%20low%0Acomputational%20complexity.%20Furthermore%2C%20DRWKV%20enhances%20downstream%20performance%20in%0Alow-light%20multi-object%20tracking%20tasks%2C%20validating%20its%20generalization%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18594v2&entry.124074799=Read"},
{"title": "Perceptual Reality Transformer: Neural Architectures for Simulating\n  Neurological Perception Conditions", "author": "Baihan Lin", "abstract": "  Neurological conditions affecting visual perception create profound\nexperiential divides between affected individuals and their caregivers,\nfamilies, and medical professionals. We present the Perceptual Reality\nTransformer, a comprehensive framework employing six distinct neural\narchitectures to simulate eight neurological perception conditions with\nscientifically-grounded visual transformations. Our system learns mappings from\nnatural images to condition-specific perceptual states, enabling others to\nexperience approximations of simultanagnosia, prosopagnosia, ADHD attention\ndeficits, visual agnosia, depression-related changes, anxiety tunnel vision,\nand Alzheimer's memory effects. Through systematic evaluation across ImageNet\nand CIFAR-10 datasets, we demonstrate that Vision Transformer architectures\nachieve optimal performance, outperforming traditional CNN and generative\napproaches. Our work establishes the first systematic benchmark for\nneurological perception simulation, contributes novel condition-specific\nperturbation functions grounded in clinical literature, and provides\nquantitative metrics for evaluating simulation fidelity. The framework has\nimmediate applications in medical education, empathy training, and assistive\ntechnology development, while advancing our fundamental understanding of how\nneural networks can model atypical human perception.\n", "link": "http://arxiv.org/abs/2508.09852v1", "date": "2025-08-13", "relevancy": 2.2118, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perceptual%20Reality%20Transformer%3A%20Neural%20Architectures%20for%20Simulating%0A%20%20Neurological%20Perception%20Conditions&body=Title%3A%20Perceptual%20Reality%20Transformer%3A%20Neural%20Architectures%20for%20Simulating%0A%20%20Neurological%20Perception%20Conditions%0AAuthor%3A%20Baihan%20Lin%0AAbstract%3A%20%20%20Neurological%20conditions%20affecting%20visual%20perception%20create%20profound%0Aexperiential%20divides%20between%20affected%20individuals%20and%20their%20caregivers%2C%0Afamilies%2C%20and%20medical%20professionals.%20We%20present%20the%20Perceptual%20Reality%0ATransformer%2C%20a%20comprehensive%20framework%20employing%20six%20distinct%20neural%0Aarchitectures%20to%20simulate%20eight%20neurological%20perception%20conditions%20with%0Ascientifically-grounded%20visual%20transformations.%20Our%20system%20learns%20mappings%20from%0Anatural%20images%20to%20condition-specific%20perceptual%20states%2C%20enabling%20others%20to%0Aexperience%20approximations%20of%20simultanagnosia%2C%20prosopagnosia%2C%20ADHD%20attention%0Adeficits%2C%20visual%20agnosia%2C%20depression-related%20changes%2C%20anxiety%20tunnel%20vision%2C%0Aand%20Alzheimer%27s%20memory%20effects.%20Through%20systematic%20evaluation%20across%20ImageNet%0Aand%20CIFAR-10%20datasets%2C%20we%20demonstrate%20that%20Vision%20Transformer%20architectures%0Aachieve%20optimal%20performance%2C%20outperforming%20traditional%20CNN%20and%20generative%0Aapproaches.%20Our%20work%20establishes%20the%20first%20systematic%20benchmark%20for%0Aneurological%20perception%20simulation%2C%20contributes%20novel%20condition-specific%0Aperturbation%20functions%20grounded%20in%20clinical%20literature%2C%20and%20provides%0Aquantitative%20metrics%20for%20evaluating%20simulation%20fidelity.%20The%20framework%20has%0Aimmediate%20applications%20in%20medical%20education%2C%20empathy%20training%2C%20and%20assistive%0Atechnology%20development%2C%20while%20advancing%20our%20fundamental%20understanding%20of%20how%0Aneural%20networks%20can%20model%20atypical%20human%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceptual%2520Reality%2520Transformer%253A%2520Neural%2520Architectures%2520for%2520Simulating%250A%2520%2520Neurological%2520Perception%2520Conditions%26entry.906535625%3DBaihan%2520Lin%26entry.1292438233%3D%2520%2520Neurological%2520conditions%2520affecting%2520visual%2520perception%2520create%2520profound%250Aexperiential%2520divides%2520between%2520affected%2520individuals%2520and%2520their%2520caregivers%252C%250Afamilies%252C%2520and%2520medical%2520professionals.%2520We%2520present%2520the%2520Perceptual%2520Reality%250ATransformer%252C%2520a%2520comprehensive%2520framework%2520employing%2520six%2520distinct%2520neural%250Aarchitectures%2520to%2520simulate%2520eight%2520neurological%2520perception%2520conditions%2520with%250Ascientifically-grounded%2520visual%2520transformations.%2520Our%2520system%2520learns%2520mappings%2520from%250Anatural%2520images%2520to%2520condition-specific%2520perceptual%2520states%252C%2520enabling%2520others%2520to%250Aexperience%2520approximations%2520of%2520simultanagnosia%252C%2520prosopagnosia%252C%2520ADHD%2520attention%250Adeficits%252C%2520visual%2520agnosia%252C%2520depression-related%2520changes%252C%2520anxiety%2520tunnel%2520vision%252C%250Aand%2520Alzheimer%2527s%2520memory%2520effects.%2520Through%2520systematic%2520evaluation%2520across%2520ImageNet%250Aand%2520CIFAR-10%2520datasets%252C%2520we%2520demonstrate%2520that%2520Vision%2520Transformer%2520architectures%250Aachieve%2520optimal%2520performance%252C%2520outperforming%2520traditional%2520CNN%2520and%2520generative%250Aapproaches.%2520Our%2520work%2520establishes%2520the%2520first%2520systematic%2520benchmark%2520for%250Aneurological%2520perception%2520simulation%252C%2520contributes%2520novel%2520condition-specific%250Aperturbation%2520functions%2520grounded%2520in%2520clinical%2520literature%252C%2520and%2520provides%250Aquantitative%2520metrics%2520for%2520evaluating%2520simulation%2520fidelity.%2520The%2520framework%2520has%250Aimmediate%2520applications%2520in%2520medical%2520education%252C%2520empathy%2520training%252C%2520and%2520assistive%250Atechnology%2520development%252C%2520while%2520advancing%2520our%2520fundamental%2520understanding%2520of%2520how%250Aneural%2520networks%2520can%2520model%2520atypical%2520human%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perceptual%20Reality%20Transformer%3A%20Neural%20Architectures%20for%20Simulating%0A%20%20Neurological%20Perception%20Conditions&entry.906535625=Baihan%20Lin&entry.1292438233=%20%20Neurological%20conditions%20affecting%20visual%20perception%20create%20profound%0Aexperiential%20divides%20between%20affected%20individuals%20and%20their%20caregivers%2C%0Afamilies%2C%20and%20medical%20professionals.%20We%20present%20the%20Perceptual%20Reality%0ATransformer%2C%20a%20comprehensive%20framework%20employing%20six%20distinct%20neural%0Aarchitectures%20to%20simulate%20eight%20neurological%20perception%20conditions%20with%0Ascientifically-grounded%20visual%20transformations.%20Our%20system%20learns%20mappings%20from%0Anatural%20images%20to%20condition-specific%20perceptual%20states%2C%20enabling%20others%20to%0Aexperience%20approximations%20of%20simultanagnosia%2C%20prosopagnosia%2C%20ADHD%20attention%0Adeficits%2C%20visual%20agnosia%2C%20depression-related%20changes%2C%20anxiety%20tunnel%20vision%2C%0Aand%20Alzheimer%27s%20memory%20effects.%20Through%20systematic%20evaluation%20across%20ImageNet%0Aand%20CIFAR-10%20datasets%2C%20we%20demonstrate%20that%20Vision%20Transformer%20architectures%0Aachieve%20optimal%20performance%2C%20outperforming%20traditional%20CNN%20and%20generative%0Aapproaches.%20Our%20work%20establishes%20the%20first%20systematic%20benchmark%20for%0Aneurological%20perception%20simulation%2C%20contributes%20novel%20condition-specific%0Aperturbation%20functions%20grounded%20in%20clinical%20literature%2C%20and%20provides%0Aquantitative%20metrics%20for%20evaluating%20simulation%20fidelity.%20The%20framework%20has%0Aimmediate%20applications%20in%20medical%20education%2C%20empathy%20training%2C%20and%20assistive%0Atechnology%20development%2C%20while%20advancing%20our%20fundamental%20understanding%20of%20how%0Aneural%20networks%20can%20model%20atypical%20human%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09852v1&entry.124074799=Read"},
{"title": "Calibrated Self-supervised Vision Transformers Improve Intracranial\n  Arterial Calcification Segmentation from Clinical CT Head Scans", "author": "Benjamin Jin and Grant Mair and Joanna M. Wardlaw and Maria del C. Vald\u00e9s Hern\u00e1ndez", "abstract": "  Vision Transformers (ViTs) have gained significant popularity in the natural\nimage domain but have been less successful in 3D medical image segmentation.\nNevertheless, 3D ViTs are particularly interesting for large medical imaging\nvolumes due to their efficient self-supervised training within the masked\nautoencoder (MAE) framework, which enables the use of imaging data without the\nneed for expensive manual annotations. Intracranial arterial calcification\n(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to\nneurovascular diseases such as stroke and dementia, and automated IAC\nquantification could enable their large-scale risk assessment. We pre-train\nViTs with MAE and fine-tune them for IAC segmentation for the first time. To\ndevelop our models, we use highly heterogeneous data from a large clinical\ntrial, the third International Stroke Trial (IST-3). We evaluate key aspects of\nMAE pre-trained ViTs in IAC segmentation, and analyse the clinical\nimplications. We show: 1) our calibrated self-supervised ViT beats a strong\nsupervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial\nfor ViTs for IAC segmentation and interpolation upsampling with regular\nconvolutions is preferable to transposed convolutions for ViT-based models, and\n3) our ViTs increase robustness to higher slice thicknesses and improve risk\ngroup classification in a clinical scenario by 46%. Our code is available\nonline.\n", "link": "http://arxiv.org/abs/2507.01744v2", "date": "2025-08-13", "relevancy": 2.2099, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5609}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5518}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrated%20Self-supervised%20Vision%20Transformers%20Improve%20Intracranial%0A%20%20Arterial%20Calcification%20Segmentation%20from%20Clinical%20CT%20Head%20Scans&body=Title%3A%20Calibrated%20Self-supervised%20Vision%20Transformers%20Improve%20Intracranial%0A%20%20Arterial%20Calcification%20Segmentation%20from%20Clinical%20CT%20Head%20Scans%0AAuthor%3A%20Benjamin%20Jin%20and%20Grant%20Mair%20and%20Joanna%20M.%20Wardlaw%20and%20Maria%20del%20C.%20Vald%C3%A9s%20Hern%C3%A1ndez%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20gained%20significant%20popularity%20in%20the%20natural%0Aimage%20domain%20but%20have%20been%20less%20successful%20in%203D%20medical%20image%20segmentation.%0ANevertheless%2C%203D%20ViTs%20are%20particularly%20interesting%20for%20large%20medical%20imaging%0Avolumes%20due%20to%20their%20efficient%20self-supervised%20training%20within%20the%20masked%0Aautoencoder%20%28MAE%29%20framework%2C%20which%20enables%20the%20use%20of%20imaging%20data%20without%20the%0Aneed%20for%20expensive%20manual%20annotations.%20Intracranial%20arterial%20calcification%0A%28IAC%29%20is%20an%20imaging%20biomarker%20visible%20on%20routinely%20acquired%20CT%20scans%20linked%20to%0Aneurovascular%20diseases%20such%20as%20stroke%20and%20dementia%2C%20and%20automated%20IAC%0Aquantification%20could%20enable%20their%20large-scale%20risk%20assessment.%20We%20pre-train%0AViTs%20with%20MAE%20and%20fine-tune%20them%20for%20IAC%20segmentation%20for%20the%20first%20time.%20To%0Adevelop%20our%20models%2C%20we%20use%20highly%20heterogeneous%20data%20from%20a%20large%20clinical%0Atrial%2C%20the%20third%20International%20Stroke%20Trial%20%28IST-3%29.%20We%20evaluate%20key%20aspects%20of%0AMAE%20pre-trained%20ViTs%20in%20IAC%20segmentation%2C%20and%20analyse%20the%20clinical%0Aimplications.%20We%20show%3A%201%29%20our%20calibrated%20self-supervised%20ViT%20beats%20a%20strong%0Asupervised%20nnU-Net%20baseline%20by%203.2%20Dice%20points%2C%202%29%20low%20patch%20sizes%20are%20crucial%0Afor%20ViTs%20for%20IAC%20segmentation%20and%20interpolation%20upsampling%20with%20regular%0Aconvolutions%20is%20preferable%20to%20transposed%20convolutions%20for%20ViT-based%20models%2C%20and%0A3%29%20our%20ViTs%20increase%20robustness%20to%20higher%20slice%20thicknesses%20and%20improve%20risk%0Agroup%20classification%20in%20a%20clinical%20scenario%20by%2046%25.%20Our%20code%20is%20available%0Aonline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01744v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrated%2520Self-supervised%2520Vision%2520Transformers%2520Improve%2520Intracranial%250A%2520%2520Arterial%2520Calcification%2520Segmentation%2520from%2520Clinical%2520CT%2520Head%2520Scans%26entry.906535625%3DBenjamin%2520Jin%2520and%2520Grant%2520Mair%2520and%2520Joanna%2520M.%2520Wardlaw%2520and%2520Maria%2520del%2520C.%2520Vald%25C3%25A9s%2520Hern%25C3%25A1ndez%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520gained%2520significant%2520popularity%2520in%2520the%2520natural%250Aimage%2520domain%2520but%2520have%2520been%2520less%2520successful%2520in%25203D%2520medical%2520image%2520segmentation.%250ANevertheless%252C%25203D%2520ViTs%2520are%2520particularly%2520interesting%2520for%2520large%2520medical%2520imaging%250Avolumes%2520due%2520to%2520their%2520efficient%2520self-supervised%2520training%2520within%2520the%2520masked%250Aautoencoder%2520%2528MAE%2529%2520framework%252C%2520which%2520enables%2520the%2520use%2520of%2520imaging%2520data%2520without%2520the%250Aneed%2520for%2520expensive%2520manual%2520annotations.%2520Intracranial%2520arterial%2520calcification%250A%2528IAC%2529%2520is%2520an%2520imaging%2520biomarker%2520visible%2520on%2520routinely%2520acquired%2520CT%2520scans%2520linked%2520to%250Aneurovascular%2520diseases%2520such%2520as%2520stroke%2520and%2520dementia%252C%2520and%2520automated%2520IAC%250Aquantification%2520could%2520enable%2520their%2520large-scale%2520risk%2520assessment.%2520We%2520pre-train%250AViTs%2520with%2520MAE%2520and%2520fine-tune%2520them%2520for%2520IAC%2520segmentation%2520for%2520the%2520first%2520time.%2520To%250Adevelop%2520our%2520models%252C%2520we%2520use%2520highly%2520heterogeneous%2520data%2520from%2520a%2520large%2520clinical%250Atrial%252C%2520the%2520third%2520International%2520Stroke%2520Trial%2520%2528IST-3%2529.%2520We%2520evaluate%2520key%2520aspects%2520of%250AMAE%2520pre-trained%2520ViTs%2520in%2520IAC%2520segmentation%252C%2520and%2520analyse%2520the%2520clinical%250Aimplications.%2520We%2520show%253A%25201%2529%2520our%2520calibrated%2520self-supervised%2520ViT%2520beats%2520a%2520strong%250Asupervised%2520nnU-Net%2520baseline%2520by%25203.2%2520Dice%2520points%252C%25202%2529%2520low%2520patch%2520sizes%2520are%2520crucial%250Afor%2520ViTs%2520for%2520IAC%2520segmentation%2520and%2520interpolation%2520upsampling%2520with%2520regular%250Aconvolutions%2520is%2520preferable%2520to%2520transposed%2520convolutions%2520for%2520ViT-based%2520models%252C%2520and%250A3%2529%2520our%2520ViTs%2520increase%2520robustness%2520to%2520higher%2520slice%2520thicknesses%2520and%2520improve%2520risk%250Agroup%2520classification%2520in%2520a%2520clinical%2520scenario%2520by%252046%2525.%2520Our%2520code%2520is%2520available%250Aonline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01744v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrated%20Self-supervised%20Vision%20Transformers%20Improve%20Intracranial%0A%20%20Arterial%20Calcification%20Segmentation%20from%20Clinical%20CT%20Head%20Scans&entry.906535625=Benjamin%20Jin%20and%20Grant%20Mair%20and%20Joanna%20M.%20Wardlaw%20and%20Maria%20del%20C.%20Vald%C3%A9s%20Hern%C3%A1ndez&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20gained%20significant%20popularity%20in%20the%20natural%0Aimage%20domain%20but%20have%20been%20less%20successful%20in%203D%20medical%20image%20segmentation.%0ANevertheless%2C%203D%20ViTs%20are%20particularly%20interesting%20for%20large%20medical%20imaging%0Avolumes%20due%20to%20their%20efficient%20self-supervised%20training%20within%20the%20masked%0Aautoencoder%20%28MAE%29%20framework%2C%20which%20enables%20the%20use%20of%20imaging%20data%20without%20the%0Aneed%20for%20expensive%20manual%20annotations.%20Intracranial%20arterial%20calcification%0A%28IAC%29%20is%20an%20imaging%20biomarker%20visible%20on%20routinely%20acquired%20CT%20scans%20linked%20to%0Aneurovascular%20diseases%20such%20as%20stroke%20and%20dementia%2C%20and%20automated%20IAC%0Aquantification%20could%20enable%20their%20large-scale%20risk%20assessment.%20We%20pre-train%0AViTs%20with%20MAE%20and%20fine-tune%20them%20for%20IAC%20segmentation%20for%20the%20first%20time.%20To%0Adevelop%20our%20models%2C%20we%20use%20highly%20heterogeneous%20data%20from%20a%20large%20clinical%0Atrial%2C%20the%20third%20International%20Stroke%20Trial%20%28IST-3%29.%20We%20evaluate%20key%20aspects%20of%0AMAE%20pre-trained%20ViTs%20in%20IAC%20segmentation%2C%20and%20analyse%20the%20clinical%0Aimplications.%20We%20show%3A%201%29%20our%20calibrated%20self-supervised%20ViT%20beats%20a%20strong%0Asupervised%20nnU-Net%20baseline%20by%203.2%20Dice%20points%2C%202%29%20low%20patch%20sizes%20are%20crucial%0Afor%20ViTs%20for%20IAC%20segmentation%20and%20interpolation%20upsampling%20with%20regular%0Aconvolutions%20is%20preferable%20to%20transposed%20convolutions%20for%20ViT-based%20models%2C%20and%0A3%29%20our%20ViTs%20increase%20robustness%20to%20higher%20slice%20thicknesses%20and%20improve%20risk%0Agroup%20classification%20in%20a%20clinical%20scenario%20by%2046%25.%20Our%20code%20is%20available%0Aonline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01744v2&entry.124074799=Read"},
{"title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and\n  Reasoning over Long Contexts", "author": "Mo Yu and Tsz Ting Chung and Chulun Zhou and Tong Li and Rui Lu and Jiangnan Li and Liyan Xu and Haoshu Lu and Ning Zhang and Jing Li and Jie Zhou", "abstract": "  We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning.\n", "link": "http://arxiv.org/abs/2508.09848v1", "date": "2025-08-13", "relevancy": 2.2074, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.572}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRELUDE%3A%20A%20Benchmark%20Designed%20to%20Require%20Global%20Comprehension%20and%0A%20%20Reasoning%20over%20Long%20Contexts&body=Title%3A%20PRELUDE%3A%20A%20Benchmark%20Designed%20to%20Require%20Global%20Comprehension%20and%0A%20%20Reasoning%20over%20Long%20Contexts%0AAuthor%3A%20Mo%20Yu%20and%20Tsz%20Ting%20Chung%20and%20Chulun%20Zhou%20and%20Tong%20Li%20and%20Rui%20Lu%20and%20Jiangnan%20Li%20and%20Liyan%20Xu%20and%20Haoshu%20Lu%20and%20Ning%20Zhang%20and%20Jing%20Li%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20We%20introduce%20PRELUDE%2C%20a%20benchmark%20for%20evaluating%20long-context%20understanding%0Athrough%20the%20task%20of%20determining%20whether%20a%20character%27s%20prequel%20story%20is%0Aconsistent%20with%20the%20canonical%20narrative%20of%20the%20original%20book.%20Our%20task%20poses%20a%0Astronger%20demand%20for%20global%20comprehension%20and%20deep%20reasoning%20than%20existing%0Abenchmarks%20--%20as%20the%20prequels%20are%20not%20part%20of%20the%20original%20story%2C%20assessing%0Atheir%20plausibility%20typically%20requires%20searching%20and%20integrating%20information%0Athat%20is%20only%20indirectly%20related.%20Empirically%2C%2088%25%20of%20instances%20require%20evidence%0Afrom%20multiple%20parts%20of%20the%20narrative.%20Experimental%20results%20highlight%20the%0Achallenge%20of%20our%20task%3A%20in-context%20learning%2C%20RAG%20and%20in-domain%20training%20with%0Astate-of-the-art%20LLMs%2C%20and%20commercial%20DeepResearch%20services%2C%20lag%20behind%20humans%0Aby%20%3E15%25.%20A%20further%20human%20study%20reveals%20that%20models%20often%20produce%20correct%0Aanswers%20with%20flawed%20reasoning%2C%20leading%20to%20an%20over%2030%25%20gap%20in%20reasoning%20accuracy%0Acompared%20to%20humans.%20These%20findings%20underscore%20the%20substantial%20room%20for%0Aimprovement%20in%20long-context%20understanding%20and%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRELUDE%253A%2520A%2520Benchmark%2520Designed%2520to%2520Require%2520Global%2520Comprehension%2520and%250A%2520%2520Reasoning%2520over%2520Long%2520Contexts%26entry.906535625%3DMo%2520Yu%2520and%2520Tsz%2520Ting%2520Chung%2520and%2520Chulun%2520Zhou%2520and%2520Tong%2520Li%2520and%2520Rui%2520Lu%2520and%2520Jiangnan%2520Li%2520and%2520Liyan%2520Xu%2520and%2520Haoshu%2520Lu%2520and%2520Ning%2520Zhang%2520and%2520Jing%2520Li%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520We%2520introduce%2520PRELUDE%252C%2520a%2520benchmark%2520for%2520evaluating%2520long-context%2520understanding%250Athrough%2520the%2520task%2520of%2520determining%2520whether%2520a%2520character%2527s%2520prequel%2520story%2520is%250Aconsistent%2520with%2520the%2520canonical%2520narrative%2520of%2520the%2520original%2520book.%2520Our%2520task%2520poses%2520a%250Astronger%2520demand%2520for%2520global%2520comprehension%2520and%2520deep%2520reasoning%2520than%2520existing%250Abenchmarks%2520--%2520as%2520the%2520prequels%2520are%2520not%2520part%2520of%2520the%2520original%2520story%252C%2520assessing%250Atheir%2520plausibility%2520typically%2520requires%2520searching%2520and%2520integrating%2520information%250Athat%2520is%2520only%2520indirectly%2520related.%2520Empirically%252C%252088%2525%2520of%2520instances%2520require%2520evidence%250Afrom%2520multiple%2520parts%2520of%2520the%2520narrative.%2520Experimental%2520results%2520highlight%2520the%250Achallenge%2520of%2520our%2520task%253A%2520in-context%2520learning%252C%2520RAG%2520and%2520in-domain%2520training%2520with%250Astate-of-the-art%2520LLMs%252C%2520and%2520commercial%2520DeepResearch%2520services%252C%2520lag%2520behind%2520humans%250Aby%2520%253E15%2525.%2520A%2520further%2520human%2520study%2520reveals%2520that%2520models%2520often%2520produce%2520correct%250Aanswers%2520with%2520flawed%2520reasoning%252C%2520leading%2520to%2520an%2520over%252030%2525%2520gap%2520in%2520reasoning%2520accuracy%250Acompared%2520to%2520humans.%2520These%2520findings%2520underscore%2520the%2520substantial%2520room%2520for%250Aimprovement%2520in%2520long-context%2520understanding%2520and%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRELUDE%3A%20A%20Benchmark%20Designed%20to%20Require%20Global%20Comprehension%20and%0A%20%20Reasoning%20over%20Long%20Contexts&entry.906535625=Mo%20Yu%20and%20Tsz%20Ting%20Chung%20and%20Chulun%20Zhou%20and%20Tong%20Li%20and%20Rui%20Lu%20and%20Jiangnan%20Li%20and%20Liyan%20Xu%20and%20Haoshu%20Lu%20and%20Ning%20Zhang%20and%20Jing%20Li%20and%20Jie%20Zhou&entry.1292438233=%20%20We%20introduce%20PRELUDE%2C%20a%20benchmark%20for%20evaluating%20long-context%20understanding%0Athrough%20the%20task%20of%20determining%20whether%20a%20character%27s%20prequel%20story%20is%0Aconsistent%20with%20the%20canonical%20narrative%20of%20the%20original%20book.%20Our%20task%20poses%20a%0Astronger%20demand%20for%20global%20comprehension%20and%20deep%20reasoning%20than%20existing%0Abenchmarks%20--%20as%20the%20prequels%20are%20not%20part%20of%20the%20original%20story%2C%20assessing%0Atheir%20plausibility%20typically%20requires%20searching%20and%20integrating%20information%0Athat%20is%20only%20indirectly%20related.%20Empirically%2C%2088%25%20of%20instances%20require%20evidence%0Afrom%20multiple%20parts%20of%20the%20narrative.%20Experimental%20results%20highlight%20the%0Achallenge%20of%20our%20task%3A%20in-context%20learning%2C%20RAG%20and%20in-domain%20training%20with%0Astate-of-the-art%20LLMs%2C%20and%20commercial%20DeepResearch%20services%2C%20lag%20behind%20humans%0Aby%20%3E15%25.%20A%20further%20human%20study%20reveals%20that%20models%20often%20produce%20correct%0Aanswers%20with%20flawed%20reasoning%2C%20leading%20to%20an%20over%2030%25%20gap%20in%20reasoning%20accuracy%0Acompared%20to%20humans.%20These%20findings%20underscore%20the%20substantial%20room%20for%0Aimprovement%20in%20long-context%20understanding%20and%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09848v1&entry.124074799=Read"},
{"title": "OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video\n  VAE Train Better", "author": "Yupeng Zhou and Zhen Li and Ziheng Ouyang and Yuming Chen and Ruoyi Du and Daquan Zhou and Bin Fu and Yihao Liu and Peng Gao and Ming-Ming Cheng and Qibin Hou", "abstract": "  Encoding videos into discrete tokens could align with text tokens to\nfacilitate concise and unified multi-modal LLMs, yet introducing significant\nspatiotemporal compression compared to continuous video representation.\nPrevious discrete video VAEs experienced unstable training, long training time,\nand degraded reconstruction quality. Given the easier training and superior\nperformance of continuous VAEs, an intuitive idea is to enhance discrete video\nVAEs by leveraging continuous VAEs. After rethinking the intrinsic link between\ndiscrete and continuous representations, we found that FSQ could effectively\npreserve pre-trained continuous VAE priors compared to other quantization\nmethods. By leveraging continuous VAE priors, it converges several times faster\nthan training from scratch and achieves superior performance at convergence.\nMeanwhile, two structural improvements are proposed. First, inspired by how\ncontinuous VAEs enhance reconstruction via enlarged latent dimensions, we\nintroduce a multi-token quantization mechanism, which achieves nearly a 1 dB\nimprovement in PSNR without compromising the token compression ratio. Second,\nto tackle reconstruction challenges in high-compression video VAEs, we\nstrengthen first-frame reconstruction, enabling the causal VAE to leverage this\ninformation in subsequent frames and markedly improving the performance of 4 x\n16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous\noptimization scheme that unifies the two paradigms and, for the first time,\nachieves competitive performance on both continuous and discrete\nrepresentations within a single network. We name our method OneVAE to reflect\nthis connection.\n", "link": "http://arxiv.org/abs/2508.09857v1", "date": "2025-08-13", "relevancy": 2.2, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5509}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5507}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneVAE%3A%20Joint%20Discrete%20and%20Continuous%20Optimization%20Helps%20Discrete%20Video%0A%20%20VAE%20Train%20Better&body=Title%3A%20OneVAE%3A%20Joint%20Discrete%20and%20Continuous%20Optimization%20Helps%20Discrete%20Video%0A%20%20VAE%20Train%20Better%0AAuthor%3A%20Yupeng%20Zhou%20and%20Zhen%20Li%20and%20Ziheng%20Ouyang%20and%20Yuming%20Chen%20and%20Ruoyi%20Du%20and%20Daquan%20Zhou%20and%20Bin%20Fu%20and%20Yihao%20Liu%20and%20Peng%20Gao%20and%20Ming-Ming%20Cheng%20and%20Qibin%20Hou%0AAbstract%3A%20%20%20Encoding%20videos%20into%20discrete%20tokens%20could%20align%20with%20text%20tokens%20to%0Afacilitate%20concise%20and%20unified%20multi-modal%20LLMs%2C%20yet%20introducing%20significant%0Aspatiotemporal%20compression%20compared%20to%20continuous%20video%20representation.%0APrevious%20discrete%20video%20VAEs%20experienced%20unstable%20training%2C%20long%20training%20time%2C%0Aand%20degraded%20reconstruction%20quality.%20Given%20the%20easier%20training%20and%20superior%0Aperformance%20of%20continuous%20VAEs%2C%20an%20intuitive%20idea%20is%20to%20enhance%20discrete%20video%0AVAEs%20by%20leveraging%20continuous%20VAEs.%20After%20rethinking%20the%20intrinsic%20link%20between%0Adiscrete%20and%20continuous%20representations%2C%20we%20found%20that%20FSQ%20could%20effectively%0Apreserve%20pre-trained%20continuous%20VAE%20priors%20compared%20to%20other%20quantization%0Amethods.%20By%20leveraging%20continuous%20VAE%20priors%2C%20it%20converges%20several%20times%20faster%0Athan%20training%20from%20scratch%20and%20achieves%20superior%20performance%20at%20convergence.%0AMeanwhile%2C%20two%20structural%20improvements%20are%20proposed.%20First%2C%20inspired%20by%20how%0Acontinuous%20VAEs%20enhance%20reconstruction%20via%20enlarged%20latent%20dimensions%2C%20we%0Aintroduce%20a%20multi-token%20quantization%20mechanism%2C%20which%20achieves%20nearly%20a%201%20dB%0Aimprovement%20in%20PSNR%20without%20compromising%20the%20token%20compression%20ratio.%20Second%2C%0Ato%20tackle%20reconstruction%20challenges%20in%20high-compression%20video%20VAEs%2C%20we%0Astrengthen%20first-frame%20reconstruction%2C%20enabling%20the%20causal%20VAE%20to%20leverage%20this%0Ainformation%20in%20subsequent%20frames%20and%20markedly%20improving%20the%20performance%20of%204%20x%0A16%20x%2016%20discrete%20VAEs.%20Furthermore%2C%20we%20propose%20a%20joint%20discrete-continuous%0Aoptimization%20scheme%20that%20unifies%20the%20two%20paradigms%20and%2C%20for%20the%20first%20time%2C%0Aachieves%20competitive%20performance%20on%20both%20continuous%20and%20discrete%0Arepresentations%20within%20a%20single%20network.%20We%20name%20our%20method%20OneVAE%20to%20reflect%0Athis%20connection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneVAE%253A%2520Joint%2520Discrete%2520and%2520Continuous%2520Optimization%2520Helps%2520Discrete%2520Video%250A%2520%2520VAE%2520Train%2520Better%26entry.906535625%3DYupeng%2520Zhou%2520and%2520Zhen%2520Li%2520and%2520Ziheng%2520Ouyang%2520and%2520Yuming%2520Chen%2520and%2520Ruoyi%2520Du%2520and%2520Daquan%2520Zhou%2520and%2520Bin%2520Fu%2520and%2520Yihao%2520Liu%2520and%2520Peng%2520Gao%2520and%2520Ming-Ming%2520Cheng%2520and%2520Qibin%2520Hou%26entry.1292438233%3D%2520%2520Encoding%2520videos%2520into%2520discrete%2520tokens%2520could%2520align%2520with%2520text%2520tokens%2520to%250Afacilitate%2520concise%2520and%2520unified%2520multi-modal%2520LLMs%252C%2520yet%2520introducing%2520significant%250Aspatiotemporal%2520compression%2520compared%2520to%2520continuous%2520video%2520representation.%250APrevious%2520discrete%2520video%2520VAEs%2520experienced%2520unstable%2520training%252C%2520long%2520training%2520time%252C%250Aand%2520degraded%2520reconstruction%2520quality.%2520Given%2520the%2520easier%2520training%2520and%2520superior%250Aperformance%2520of%2520continuous%2520VAEs%252C%2520an%2520intuitive%2520idea%2520is%2520to%2520enhance%2520discrete%2520video%250AVAEs%2520by%2520leveraging%2520continuous%2520VAEs.%2520After%2520rethinking%2520the%2520intrinsic%2520link%2520between%250Adiscrete%2520and%2520continuous%2520representations%252C%2520we%2520found%2520that%2520FSQ%2520could%2520effectively%250Apreserve%2520pre-trained%2520continuous%2520VAE%2520priors%2520compared%2520to%2520other%2520quantization%250Amethods.%2520By%2520leveraging%2520continuous%2520VAE%2520priors%252C%2520it%2520converges%2520several%2520times%2520faster%250Athan%2520training%2520from%2520scratch%2520and%2520achieves%2520superior%2520performance%2520at%2520convergence.%250AMeanwhile%252C%2520two%2520structural%2520improvements%2520are%2520proposed.%2520First%252C%2520inspired%2520by%2520how%250Acontinuous%2520VAEs%2520enhance%2520reconstruction%2520via%2520enlarged%2520latent%2520dimensions%252C%2520we%250Aintroduce%2520a%2520multi-token%2520quantization%2520mechanism%252C%2520which%2520achieves%2520nearly%2520a%25201%2520dB%250Aimprovement%2520in%2520PSNR%2520without%2520compromising%2520the%2520token%2520compression%2520ratio.%2520Second%252C%250Ato%2520tackle%2520reconstruction%2520challenges%2520in%2520high-compression%2520video%2520VAEs%252C%2520we%250Astrengthen%2520first-frame%2520reconstruction%252C%2520enabling%2520the%2520causal%2520VAE%2520to%2520leverage%2520this%250Ainformation%2520in%2520subsequent%2520frames%2520and%2520markedly%2520improving%2520the%2520performance%2520of%25204%2520x%250A16%2520x%252016%2520discrete%2520VAEs.%2520Furthermore%252C%2520we%2520propose%2520a%2520joint%2520discrete-continuous%250Aoptimization%2520scheme%2520that%2520unifies%2520the%2520two%2520paradigms%2520and%252C%2520for%2520the%2520first%2520time%252C%250Aachieves%2520competitive%2520performance%2520on%2520both%2520continuous%2520and%2520discrete%250Arepresentations%2520within%2520a%2520single%2520network.%2520We%2520name%2520our%2520method%2520OneVAE%2520to%2520reflect%250Athis%2520connection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneVAE%3A%20Joint%20Discrete%20and%20Continuous%20Optimization%20Helps%20Discrete%20Video%0A%20%20VAE%20Train%20Better&entry.906535625=Yupeng%20Zhou%20and%20Zhen%20Li%20and%20Ziheng%20Ouyang%20and%20Yuming%20Chen%20and%20Ruoyi%20Du%20and%20Daquan%20Zhou%20and%20Bin%20Fu%20and%20Yihao%20Liu%20and%20Peng%20Gao%20and%20Ming-Ming%20Cheng%20and%20Qibin%20Hou&entry.1292438233=%20%20Encoding%20videos%20into%20discrete%20tokens%20could%20align%20with%20text%20tokens%20to%0Afacilitate%20concise%20and%20unified%20multi-modal%20LLMs%2C%20yet%20introducing%20significant%0Aspatiotemporal%20compression%20compared%20to%20continuous%20video%20representation.%0APrevious%20discrete%20video%20VAEs%20experienced%20unstable%20training%2C%20long%20training%20time%2C%0Aand%20degraded%20reconstruction%20quality.%20Given%20the%20easier%20training%20and%20superior%0Aperformance%20of%20continuous%20VAEs%2C%20an%20intuitive%20idea%20is%20to%20enhance%20discrete%20video%0AVAEs%20by%20leveraging%20continuous%20VAEs.%20After%20rethinking%20the%20intrinsic%20link%20between%0Adiscrete%20and%20continuous%20representations%2C%20we%20found%20that%20FSQ%20could%20effectively%0Apreserve%20pre-trained%20continuous%20VAE%20priors%20compared%20to%20other%20quantization%0Amethods.%20By%20leveraging%20continuous%20VAE%20priors%2C%20it%20converges%20several%20times%20faster%0Athan%20training%20from%20scratch%20and%20achieves%20superior%20performance%20at%20convergence.%0AMeanwhile%2C%20two%20structural%20improvements%20are%20proposed.%20First%2C%20inspired%20by%20how%0Acontinuous%20VAEs%20enhance%20reconstruction%20via%20enlarged%20latent%20dimensions%2C%20we%0Aintroduce%20a%20multi-token%20quantization%20mechanism%2C%20which%20achieves%20nearly%20a%201%20dB%0Aimprovement%20in%20PSNR%20without%20compromising%20the%20token%20compression%20ratio.%20Second%2C%0Ato%20tackle%20reconstruction%20challenges%20in%20high-compression%20video%20VAEs%2C%20we%0Astrengthen%20first-frame%20reconstruction%2C%20enabling%20the%20causal%20VAE%20to%20leverage%20this%0Ainformation%20in%20subsequent%20frames%20and%20markedly%20improving%20the%20performance%20of%204%20x%0A16%20x%2016%20discrete%20VAEs.%20Furthermore%2C%20we%20propose%20a%20joint%20discrete-continuous%0Aoptimization%20scheme%20that%20unifies%20the%20two%20paradigms%20and%2C%20for%20the%20first%20time%2C%0Aachieves%20competitive%20performance%20on%20both%20continuous%20and%20discrete%0Arepresentations%20within%20a%20single%20network.%20We%20name%20our%20method%20OneVAE%20to%20reflect%0Athis%20connection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09857v1&entry.124074799=Read"},
{"title": "Speed Always Wins: A Survey on Efficient Architectures for Large\n  Language Models", "author": "Weigao Sun and Jiaxi Hu and Yucheng Zhou and Jusen Du and Disen Lan and Kexin Wang and Tong Zhu and Xiaoye Qu and Yu Zhang and Xiaoyu Mo and Daizong Liu and Yuxuan Liang and Wenliang Chen and Guoqi Li and Yu Cheng", "abstract": "  Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.\n", "link": "http://arxiv.org/abs/2508.09834v1", "date": "2025-08-13", "relevancy": 2.1999, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5643}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speed%20Always%20Wins%3A%20A%20Survey%20on%20Efficient%20Architectures%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20Speed%20Always%20Wins%3A%20A%20Survey%20on%20Efficient%20Architectures%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Weigao%20Sun%20and%20Jiaxi%20Hu%20and%20Yucheng%20Zhou%20and%20Jusen%20Du%20and%20Disen%20Lan%20and%20Kexin%20Wang%20and%20Tong%20Zhu%20and%20Xiaoye%20Qu%20and%20Yu%20Zhang%20and%20Xiaoyu%20Mo%20and%20Daizong%20Liu%20and%20Yuxuan%20Liang%20and%20Wenliang%20Chen%20and%20Guoqi%20Li%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20delivered%20impressive%20results%20in%20language%0Aunderstanding%2C%20generation%2C%20reasoning%2C%20and%20pushes%20the%20ability%20boundary%20of%0Amultimodal%20models.%20Transformer%20models%2C%20as%20the%20foundation%20of%20modern%20LLMs%2C%20offer%0Aa%20strong%20baseline%20with%20excellent%20scaling%20properties.%20However%2C%20the%20traditional%0Atransformer%20architecture%20requires%20substantial%20computations%20and%20poses%0Asignificant%20obstacles%20for%20large-scale%20training%20and%20practical%20deployment.%20In%0Athis%20survey%2C%20we%20offer%20a%20systematic%20examination%20of%20innovative%20LLM%20architectures%0Athat%20address%20the%20inherent%20limitations%20of%20transformers%20and%20boost%20the%20efficiency.%0AStarting%20from%20language%20modeling%2C%20this%20survey%20covers%20the%20background%20and%0Atechnical%20details%20of%20linear%20and%20sparse%20sequence%20modeling%20methods%2C%20efficient%0Afull%20attention%20variants%2C%20sparse%20mixture-of-experts%2C%20hybrid%20model%20architectures%0Aincorporating%20the%20above%20techniques%2C%20and%20emerging%20diffusion%20LLMs.%20Additionally%2C%0Awe%20discuss%20applications%20of%20these%20techniques%20to%20other%20modalities%20and%20consider%0Atheir%20wider%20implications%20for%20developing%20scalable%2C%20resource-aware%20foundation%0Amodels.%20By%20grouping%20recent%20studies%20into%20the%20above%20category%2C%20this%20survey%0Apresents%20a%20blueprint%20of%20modern%20efficient%20LLM%20architectures%2C%20and%20we%20hope%20this%0Acould%20help%20motivate%20future%20research%20toward%20more%20efficient%2C%20versatile%20AI%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeed%2520Always%2520Wins%253A%2520A%2520Survey%2520on%2520Efficient%2520Architectures%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DWeigao%2520Sun%2520and%2520Jiaxi%2520Hu%2520and%2520Yucheng%2520Zhou%2520and%2520Jusen%2520Du%2520and%2520Disen%2520Lan%2520and%2520Kexin%2520Wang%2520and%2520Tong%2520Zhu%2520and%2520Xiaoye%2520Qu%2520and%2520Yu%2520Zhang%2520and%2520Xiaoyu%2520Mo%2520and%2520Daizong%2520Liu%2520and%2520Yuxuan%2520Liang%2520and%2520Wenliang%2520Chen%2520and%2520Guoqi%2520Li%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520delivered%2520impressive%2520results%2520in%2520language%250Aunderstanding%252C%2520generation%252C%2520reasoning%252C%2520and%2520pushes%2520the%2520ability%2520boundary%2520of%250Amultimodal%2520models.%2520Transformer%2520models%252C%2520as%2520the%2520foundation%2520of%2520modern%2520LLMs%252C%2520offer%250Aa%2520strong%2520baseline%2520with%2520excellent%2520scaling%2520properties.%2520However%252C%2520the%2520traditional%250Atransformer%2520architecture%2520requires%2520substantial%2520computations%2520and%2520poses%250Asignificant%2520obstacles%2520for%2520large-scale%2520training%2520and%2520practical%2520deployment.%2520In%250Athis%2520survey%252C%2520we%2520offer%2520a%2520systematic%2520examination%2520of%2520innovative%2520LLM%2520architectures%250Athat%2520address%2520the%2520inherent%2520limitations%2520of%2520transformers%2520and%2520boost%2520the%2520efficiency.%250AStarting%2520from%2520language%2520modeling%252C%2520this%2520survey%2520covers%2520the%2520background%2520and%250Atechnical%2520details%2520of%2520linear%2520and%2520sparse%2520sequence%2520modeling%2520methods%252C%2520efficient%250Afull%2520attention%2520variants%252C%2520sparse%2520mixture-of-experts%252C%2520hybrid%2520model%2520architectures%250Aincorporating%2520the%2520above%2520techniques%252C%2520and%2520emerging%2520diffusion%2520LLMs.%2520Additionally%252C%250Awe%2520discuss%2520applications%2520of%2520these%2520techniques%2520to%2520other%2520modalities%2520and%2520consider%250Atheir%2520wider%2520implications%2520for%2520developing%2520scalable%252C%2520resource-aware%2520foundation%250Amodels.%2520By%2520grouping%2520recent%2520studies%2520into%2520the%2520above%2520category%252C%2520this%2520survey%250Apresents%2520a%2520blueprint%2520of%2520modern%2520efficient%2520LLM%2520architectures%252C%2520and%2520we%2520hope%2520this%250Acould%2520help%2520motivate%2520future%2520research%2520toward%2520more%2520efficient%252C%2520versatile%2520AI%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speed%20Always%20Wins%3A%20A%20Survey%20on%20Efficient%20Architectures%20for%20Large%0A%20%20Language%20Models&entry.906535625=Weigao%20Sun%20and%20Jiaxi%20Hu%20and%20Yucheng%20Zhou%20and%20Jusen%20Du%20and%20Disen%20Lan%20and%20Kexin%20Wang%20and%20Tong%20Zhu%20and%20Xiaoye%20Qu%20and%20Yu%20Zhang%20and%20Xiaoyu%20Mo%20and%20Daizong%20Liu%20and%20Yuxuan%20Liang%20and%20Wenliang%20Chen%20and%20Guoqi%20Li%20and%20Yu%20Cheng&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20delivered%20impressive%20results%20in%20language%0Aunderstanding%2C%20generation%2C%20reasoning%2C%20and%20pushes%20the%20ability%20boundary%20of%0Amultimodal%20models.%20Transformer%20models%2C%20as%20the%20foundation%20of%20modern%20LLMs%2C%20offer%0Aa%20strong%20baseline%20with%20excellent%20scaling%20properties.%20However%2C%20the%20traditional%0Atransformer%20architecture%20requires%20substantial%20computations%20and%20poses%0Asignificant%20obstacles%20for%20large-scale%20training%20and%20practical%20deployment.%20In%0Athis%20survey%2C%20we%20offer%20a%20systematic%20examination%20of%20innovative%20LLM%20architectures%0Athat%20address%20the%20inherent%20limitations%20of%20transformers%20and%20boost%20the%20efficiency.%0AStarting%20from%20language%20modeling%2C%20this%20survey%20covers%20the%20background%20and%0Atechnical%20details%20of%20linear%20and%20sparse%20sequence%20modeling%20methods%2C%20efficient%0Afull%20attention%20variants%2C%20sparse%20mixture-of-experts%2C%20hybrid%20model%20architectures%0Aincorporating%20the%20above%20techniques%2C%20and%20emerging%20diffusion%20LLMs.%20Additionally%2C%0Awe%20discuss%20applications%20of%20these%20techniques%20to%20other%20modalities%20and%20consider%0Atheir%20wider%20implications%20for%20developing%20scalable%2C%20resource-aware%20foundation%0Amodels.%20By%20grouping%20recent%20studies%20into%20the%20above%20category%2C%20this%20survey%0Apresents%20a%20blueprint%20of%20modern%20efficient%20LLM%20architectures%2C%20and%20we%20hope%20this%0Acould%20help%20motivate%20future%20research%20toward%20more%20efficient%2C%20versatile%20AI%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09834v1&entry.124074799=Read"},
{"title": "Multi-Step Reasoning with Large Language Models, a Survey", "author": "Aske Plaat and Annie Wong and Suzan Verberne and Joost Broekens and Niki van Stein and Thomas Back", "abstract": "  Language models with billions of parameters exhibit in-context learning\nabilities, enabling few-shot learning on tasks that the model was not\nspecifically trained for. Traditional models achieve breakthrough performance\non language tasks, but do not perform well on basic reasoning benchmarks.\nHowever, a new in-context learning approach, Chain-of-thought, has demonstrated\nstrong multi-step reasoning abilities on these benchmarks.\n  The research on LLM reasoning abilities started with the question whether\nLLMs can solve grade school math word problems, and has expanded to other tasks\nin the past few years. This paper reviews the field of multi-step reasoning\nwith LLMs. We propose a taxonomy that identifies different ways to generate,\nevaluate, and control multi-step reasoning. We provide an in-depth coverage of\ncore approaches and open problems, and we propose a research agenda for the\nnear future.\n  We find that multi-step reasoning approaches have progressed beyond math word\nproblems, and can now successfully solve challenges in logic, combinatorial\ngames, and robotics, sometimes by first generating code that is then executed\nby external tools. Many studies in multi-step methods are using reinforcement\nlearning for finetuning, external optimization loops, in context reinforcement\nlearning, and self-reflection.\n", "link": "http://arxiv.org/abs/2407.11511v2", "date": "2025-08-13", "relevancy": 2.196, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Step%20Reasoning%20with%20Large%20Language%20Models%2C%20a%20Survey&body=Title%3A%20Multi-Step%20Reasoning%20with%20Large%20Language%20Models%2C%20a%20Survey%0AAuthor%3A%20Aske%20Plaat%20and%20Annie%20Wong%20and%20Suzan%20Verberne%20and%20Joost%20Broekens%20and%20Niki%20van%20Stein%20and%20Thomas%20Back%0AAbstract%3A%20%20%20Language%20models%20with%20billions%20of%20parameters%20exhibit%20in-context%20learning%0Aabilities%2C%20enabling%20few-shot%20learning%20on%20tasks%20that%20the%20model%20was%20not%0Aspecifically%20trained%20for.%20Traditional%20models%20achieve%20breakthrough%20performance%0Aon%20language%20tasks%2C%20but%20do%20not%20perform%20well%20on%20basic%20reasoning%20benchmarks.%0AHowever%2C%20a%20new%20in-context%20learning%20approach%2C%20Chain-of-thought%2C%20has%20demonstrated%0Astrong%20multi-step%20reasoning%20abilities%20on%20these%20benchmarks.%0A%20%20The%20research%20on%20LLM%20reasoning%20abilities%20started%20with%20the%20question%20whether%0ALLMs%20can%20solve%20grade%20school%20math%20word%20problems%2C%20and%20has%20expanded%20to%20other%20tasks%0Ain%20the%20past%20few%20years.%20This%20paper%20reviews%20the%20field%20of%20multi-step%20reasoning%0Awith%20LLMs.%20We%20propose%20a%20taxonomy%20that%20identifies%20different%20ways%20to%20generate%2C%0Aevaluate%2C%20and%20control%20multi-step%20reasoning.%20We%20provide%20an%20in-depth%20coverage%20of%0Acore%20approaches%20and%20open%20problems%2C%20and%20we%20propose%20a%20research%20agenda%20for%20the%0Anear%20future.%0A%20%20We%20find%20that%20multi-step%20reasoning%20approaches%20have%20progressed%20beyond%20math%20word%0Aproblems%2C%20and%20can%20now%20successfully%20solve%20challenges%20in%20logic%2C%20combinatorial%0Agames%2C%20and%20robotics%2C%20sometimes%20by%20first%20generating%20code%20that%20is%20then%20executed%0Aby%20external%20tools.%20Many%20studies%20in%20multi-step%20methods%20are%20using%20reinforcement%0Alearning%20for%20finetuning%2C%20external%20optimization%20loops%2C%20in%20context%20reinforcement%0Alearning%2C%20and%20self-reflection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Step%2520Reasoning%2520with%2520Large%2520Language%2520Models%252C%2520a%2520Survey%26entry.906535625%3DAske%2520Plaat%2520and%2520Annie%2520Wong%2520and%2520Suzan%2520Verberne%2520and%2520Joost%2520Broekens%2520and%2520Niki%2520van%2520Stein%2520and%2520Thomas%2520Back%26entry.1292438233%3D%2520%2520Language%2520models%2520with%2520billions%2520of%2520parameters%2520exhibit%2520in-context%2520learning%250Aabilities%252C%2520enabling%2520few-shot%2520learning%2520on%2520tasks%2520that%2520the%2520model%2520was%2520not%250Aspecifically%2520trained%2520for.%2520Traditional%2520models%2520achieve%2520breakthrough%2520performance%250Aon%2520language%2520tasks%252C%2520but%2520do%2520not%2520perform%2520well%2520on%2520basic%2520reasoning%2520benchmarks.%250AHowever%252C%2520a%2520new%2520in-context%2520learning%2520approach%252C%2520Chain-of-thought%252C%2520has%2520demonstrated%250Astrong%2520multi-step%2520reasoning%2520abilities%2520on%2520these%2520benchmarks.%250A%2520%2520The%2520research%2520on%2520LLM%2520reasoning%2520abilities%2520started%2520with%2520the%2520question%2520whether%250ALLMs%2520can%2520solve%2520grade%2520school%2520math%2520word%2520problems%252C%2520and%2520has%2520expanded%2520to%2520other%2520tasks%250Ain%2520the%2520past%2520few%2520years.%2520This%2520paper%2520reviews%2520the%2520field%2520of%2520multi-step%2520reasoning%250Awith%2520LLMs.%2520We%2520propose%2520a%2520taxonomy%2520that%2520identifies%2520different%2520ways%2520to%2520generate%252C%250Aevaluate%252C%2520and%2520control%2520multi-step%2520reasoning.%2520We%2520provide%2520an%2520in-depth%2520coverage%2520of%250Acore%2520approaches%2520and%2520open%2520problems%252C%2520and%2520we%2520propose%2520a%2520research%2520agenda%2520for%2520the%250Anear%2520future.%250A%2520%2520We%2520find%2520that%2520multi-step%2520reasoning%2520approaches%2520have%2520progressed%2520beyond%2520math%2520word%250Aproblems%252C%2520and%2520can%2520now%2520successfully%2520solve%2520challenges%2520in%2520logic%252C%2520combinatorial%250Agames%252C%2520and%2520robotics%252C%2520sometimes%2520by%2520first%2520generating%2520code%2520that%2520is%2520then%2520executed%250Aby%2520external%2520tools.%2520Many%2520studies%2520in%2520multi-step%2520methods%2520are%2520using%2520reinforcement%250Alearning%2520for%2520finetuning%252C%2520external%2520optimization%2520loops%252C%2520in%2520context%2520reinforcement%250Alearning%252C%2520and%2520self-reflection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Step%20Reasoning%20with%20Large%20Language%20Models%2C%20a%20Survey&entry.906535625=Aske%20Plaat%20and%20Annie%20Wong%20and%20Suzan%20Verberne%20and%20Joost%20Broekens%20and%20Niki%20van%20Stein%20and%20Thomas%20Back&entry.1292438233=%20%20Language%20models%20with%20billions%20of%20parameters%20exhibit%20in-context%20learning%0Aabilities%2C%20enabling%20few-shot%20learning%20on%20tasks%20that%20the%20model%20was%20not%0Aspecifically%20trained%20for.%20Traditional%20models%20achieve%20breakthrough%20performance%0Aon%20language%20tasks%2C%20but%20do%20not%20perform%20well%20on%20basic%20reasoning%20benchmarks.%0AHowever%2C%20a%20new%20in-context%20learning%20approach%2C%20Chain-of-thought%2C%20has%20demonstrated%0Astrong%20multi-step%20reasoning%20abilities%20on%20these%20benchmarks.%0A%20%20The%20research%20on%20LLM%20reasoning%20abilities%20started%20with%20the%20question%20whether%0ALLMs%20can%20solve%20grade%20school%20math%20word%20problems%2C%20and%20has%20expanded%20to%20other%20tasks%0Ain%20the%20past%20few%20years.%20This%20paper%20reviews%20the%20field%20of%20multi-step%20reasoning%0Awith%20LLMs.%20We%20propose%20a%20taxonomy%20that%20identifies%20different%20ways%20to%20generate%2C%0Aevaluate%2C%20and%20control%20multi-step%20reasoning.%20We%20provide%20an%20in-depth%20coverage%20of%0Acore%20approaches%20and%20open%20problems%2C%20and%20we%20propose%20a%20research%20agenda%20for%20the%0Anear%20future.%0A%20%20We%20find%20that%20multi-step%20reasoning%20approaches%20have%20progressed%20beyond%20math%20word%0Aproblems%2C%20and%20can%20now%20successfully%20solve%20challenges%20in%20logic%2C%20combinatorial%0Agames%2C%20and%20robotics%2C%20sometimes%20by%20first%20generating%20code%20that%20is%20then%20executed%0Aby%20external%20tools.%20Many%20studies%20in%20multi-step%20methods%20are%20using%20reinforcement%0Alearning%20for%20finetuning%2C%20external%20optimization%20loops%2C%20in%20context%20reinforcement%0Alearning%2C%20and%20self-reflection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11511v2&entry.124074799=Read"},
{"title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models", "author": "Meng Cao and Pengfei Hu and Yingyao Wang and Jihao Gu and Haoran Tang and Haoze Zhao and Chen Wang and Jiahua Dong and Wangbo Yu and Ge Zhang and Jun Song and Xiang Li and Bo Zheng and Ian Reid and Xiaodan Liang", "abstract": "  Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in videos remains a critical unsolved challenge. To address this gap,\nwe introduce Video SimpleQA, the first comprehensive benchmark tailored for\nfactuality evaluation in video contexts. Our work differs from existing video\nbenchmarks through the following key features: 1) Knowledge required: demanding\nintegration of external knowledge beyond the video's explicit narrative; 2)\nMulti-hop fact-seeking question: Each question involves multiple explicit facts\nand requires strict factual grounding without hypothetical or subjective\ninferences. We also include per-hop single-fact-based sub-QAs alongside final\nQAs to enable fine-grained, stepby-step evaluation; 3) Short-form definitive\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat with minimal scoring variance; 4) Temporal grounded required: Requiring\nanswers to rely on one or more temporal segments in videos, rather than single\nframes. We extensively evaluate 33 state-of-the-art LVLMs and summarize key\nfindings as follows: 1) Current LVLMs exhibit notable deficiencies in factual\nadherence, with the best-performing model o3 merely achieving an F-score of\n66.3%; 2) Most LVLMs are overconfident in what they generate, with self-stated\nconfidence exceeding actual accuracy; 3) Retrieval-augmented generation\ndemonstrates consistent improvements at the cost of additional inference time\noverhead; 4) Multi-hop QA demonstrates substantially degraded performance\ncompared to single-hop sub-QAs, with first-hop object or event recognition\nemerging as the primary bottleneck. We position Video SimpleQA as the\ncornerstone benchmark for video factuality assessment, aiming to steer LVLM\ndevelopment toward verifiable grounding in real-world contexts.\n", "link": "http://arxiv.org/abs/2503.18923v2", "date": "2025-08-13", "relevancy": 2.1818, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20SimpleQA%3A%20Towards%20Factuality%20Evaluation%20in%20Large%20Video%20Language%0A%20%20Models&body=Title%3A%20Video%20SimpleQA%3A%20Towards%20Factuality%20Evaluation%20in%20Large%20Video%20Language%0A%20%20Models%0AAuthor%3A%20Meng%20Cao%20and%20Pengfei%20Hu%20and%20Yingyao%20Wang%20and%20Jihao%20Gu%20and%20Haoran%20Tang%20and%20Haoze%20Zhao%20and%20Chen%20Wang%20and%20Jiahua%20Dong%20and%20Wangbo%20Yu%20and%20Ge%20Zhang%20and%20Jun%20Song%20and%20Xiang%20Li%20and%20Bo%20Zheng%20and%20Ian%20Reid%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Video%20Language%20Models%20%28LVLMs%29%20have%20highlighted%0Atheir%20potential%20for%20multi-modal%20understanding%2C%20yet%20evaluating%20their%20factual%0Agrounding%20in%20videos%20remains%20a%20critical%20unsolved%20challenge.%20To%20address%20this%20gap%2C%0Awe%20introduce%20Video%20SimpleQA%2C%20the%20first%20comprehensive%20benchmark%20tailored%20for%0Afactuality%20evaluation%20in%20video%20contexts.%20Our%20work%20differs%20from%20existing%20video%0Abenchmarks%20through%20the%20following%20key%20features%3A%201%29%20Knowledge%20required%3A%20demanding%0Aintegration%20of%20external%20knowledge%20beyond%20the%20video%27s%20explicit%20narrative%3B%202%29%0AMulti-hop%20fact-seeking%20question%3A%20Each%20question%20involves%20multiple%20explicit%20facts%0Aand%20requires%20strict%20factual%20grounding%20without%20hypothetical%20or%20subjective%0Ainferences.%20We%20also%20include%20per-hop%20single-fact-based%20sub-QAs%20alongside%20final%0AQAs%20to%20enable%20fine-grained%2C%20stepby-step%20evaluation%3B%203%29%20Short-form%20definitive%0Aanswer%3A%20Answers%20are%20crafted%20as%20unambiguous%20and%20definitively%20correct%20in%20a%20short%0Aformat%20with%20minimal%20scoring%20variance%3B%204%29%20Temporal%20grounded%20required%3A%20Requiring%0Aanswers%20to%20rely%20on%20one%20or%20more%20temporal%20segments%20in%20videos%2C%20rather%20than%20single%0Aframes.%20We%20extensively%20evaluate%2033%20state-of-the-art%20LVLMs%20and%20summarize%20key%0Afindings%20as%20follows%3A%201%29%20Current%20LVLMs%20exhibit%20notable%20deficiencies%20in%20factual%0Aadherence%2C%20with%20the%20best-performing%20model%20o3%20merely%20achieving%20an%20F-score%20of%0A66.3%25%3B%202%29%20Most%20LVLMs%20are%20overconfident%20in%20what%20they%20generate%2C%20with%20self-stated%0Aconfidence%20exceeding%20actual%20accuracy%3B%203%29%20Retrieval-augmented%20generation%0Ademonstrates%20consistent%20improvements%20at%20the%20cost%20of%20additional%20inference%20time%0Aoverhead%3B%204%29%20Multi-hop%20QA%20demonstrates%20substantially%20degraded%20performance%0Acompared%20to%20single-hop%20sub-QAs%2C%20with%20first-hop%20object%20or%20event%20recognition%0Aemerging%20as%20the%20primary%20bottleneck.%20We%20position%20Video%20SimpleQA%20as%20the%0Acornerstone%20benchmark%20for%20video%20factuality%20assessment%2C%20aiming%20to%20steer%20LVLM%0Adevelopment%20toward%20verifiable%20grounding%20in%20real-world%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18923v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520SimpleQA%253A%2520Towards%2520Factuality%2520Evaluation%2520in%2520Large%2520Video%2520Language%250A%2520%2520Models%26entry.906535625%3DMeng%2520Cao%2520and%2520Pengfei%2520Hu%2520and%2520Yingyao%2520Wang%2520and%2520Jihao%2520Gu%2520and%2520Haoran%2520Tang%2520and%2520Haoze%2520Zhao%2520and%2520Chen%2520Wang%2520and%2520Jiahua%2520Dong%2520and%2520Wangbo%2520Yu%2520and%2520Ge%2520Zhang%2520and%2520Jun%2520Song%2520and%2520Xiang%2520Li%2520and%2520Bo%2520Zheng%2520and%2520Ian%2520Reid%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Video%2520Language%2520Models%2520%2528LVLMs%2529%2520have%2520highlighted%250Atheir%2520potential%2520for%2520multi-modal%2520understanding%252C%2520yet%2520evaluating%2520their%2520factual%250Agrounding%2520in%2520videos%2520remains%2520a%2520critical%2520unsolved%2520challenge.%2520To%2520address%2520this%2520gap%252C%250Awe%2520introduce%2520Video%2520SimpleQA%252C%2520the%2520first%2520comprehensive%2520benchmark%2520tailored%2520for%250Afactuality%2520evaluation%2520in%2520video%2520contexts.%2520Our%2520work%2520differs%2520from%2520existing%2520video%250Abenchmarks%2520through%2520the%2520following%2520key%2520features%253A%25201%2529%2520Knowledge%2520required%253A%2520demanding%250Aintegration%2520of%2520external%2520knowledge%2520beyond%2520the%2520video%2527s%2520explicit%2520narrative%253B%25202%2529%250AMulti-hop%2520fact-seeking%2520question%253A%2520Each%2520question%2520involves%2520multiple%2520explicit%2520facts%250Aand%2520requires%2520strict%2520factual%2520grounding%2520without%2520hypothetical%2520or%2520subjective%250Ainferences.%2520We%2520also%2520include%2520per-hop%2520single-fact-based%2520sub-QAs%2520alongside%2520final%250AQAs%2520to%2520enable%2520fine-grained%252C%2520stepby-step%2520evaluation%253B%25203%2529%2520Short-form%2520definitive%250Aanswer%253A%2520Answers%2520are%2520crafted%2520as%2520unambiguous%2520and%2520definitively%2520correct%2520in%2520a%2520short%250Aformat%2520with%2520minimal%2520scoring%2520variance%253B%25204%2529%2520Temporal%2520grounded%2520required%253A%2520Requiring%250Aanswers%2520to%2520rely%2520on%2520one%2520or%2520more%2520temporal%2520segments%2520in%2520videos%252C%2520rather%2520than%2520single%250Aframes.%2520We%2520extensively%2520evaluate%252033%2520state-of-the-art%2520LVLMs%2520and%2520summarize%2520key%250Afindings%2520as%2520follows%253A%25201%2529%2520Current%2520LVLMs%2520exhibit%2520notable%2520deficiencies%2520in%2520factual%250Aadherence%252C%2520with%2520the%2520best-performing%2520model%2520o3%2520merely%2520achieving%2520an%2520F-score%2520of%250A66.3%2525%253B%25202%2529%2520Most%2520LVLMs%2520are%2520overconfident%2520in%2520what%2520they%2520generate%252C%2520with%2520self-stated%250Aconfidence%2520exceeding%2520actual%2520accuracy%253B%25203%2529%2520Retrieval-augmented%2520generation%250Ademonstrates%2520consistent%2520improvements%2520at%2520the%2520cost%2520of%2520additional%2520inference%2520time%250Aoverhead%253B%25204%2529%2520Multi-hop%2520QA%2520demonstrates%2520substantially%2520degraded%2520performance%250Acompared%2520to%2520single-hop%2520sub-QAs%252C%2520with%2520first-hop%2520object%2520or%2520event%2520recognition%250Aemerging%2520as%2520the%2520primary%2520bottleneck.%2520We%2520position%2520Video%2520SimpleQA%2520as%2520the%250Acornerstone%2520benchmark%2520for%2520video%2520factuality%2520assessment%252C%2520aiming%2520to%2520steer%2520LVLM%250Adevelopment%2520toward%2520verifiable%2520grounding%2520in%2520real-world%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18923v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20SimpleQA%3A%20Towards%20Factuality%20Evaluation%20in%20Large%20Video%20Language%0A%20%20Models&entry.906535625=Meng%20Cao%20and%20Pengfei%20Hu%20and%20Yingyao%20Wang%20and%20Jihao%20Gu%20and%20Haoran%20Tang%20and%20Haoze%20Zhao%20and%20Chen%20Wang%20and%20Jiahua%20Dong%20and%20Wangbo%20Yu%20and%20Ge%20Zhang%20and%20Jun%20Song%20and%20Xiang%20Li%20and%20Bo%20Zheng%20and%20Ian%20Reid%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Video%20Language%20Models%20%28LVLMs%29%20have%20highlighted%0Atheir%20potential%20for%20multi-modal%20understanding%2C%20yet%20evaluating%20their%20factual%0Agrounding%20in%20videos%20remains%20a%20critical%20unsolved%20challenge.%20To%20address%20this%20gap%2C%0Awe%20introduce%20Video%20SimpleQA%2C%20the%20first%20comprehensive%20benchmark%20tailored%20for%0Afactuality%20evaluation%20in%20video%20contexts.%20Our%20work%20differs%20from%20existing%20video%0Abenchmarks%20through%20the%20following%20key%20features%3A%201%29%20Knowledge%20required%3A%20demanding%0Aintegration%20of%20external%20knowledge%20beyond%20the%20video%27s%20explicit%20narrative%3B%202%29%0AMulti-hop%20fact-seeking%20question%3A%20Each%20question%20involves%20multiple%20explicit%20facts%0Aand%20requires%20strict%20factual%20grounding%20without%20hypothetical%20or%20subjective%0Ainferences.%20We%20also%20include%20per-hop%20single-fact-based%20sub-QAs%20alongside%20final%0AQAs%20to%20enable%20fine-grained%2C%20stepby-step%20evaluation%3B%203%29%20Short-form%20definitive%0Aanswer%3A%20Answers%20are%20crafted%20as%20unambiguous%20and%20definitively%20correct%20in%20a%20short%0Aformat%20with%20minimal%20scoring%20variance%3B%204%29%20Temporal%20grounded%20required%3A%20Requiring%0Aanswers%20to%20rely%20on%20one%20or%20more%20temporal%20segments%20in%20videos%2C%20rather%20than%20single%0Aframes.%20We%20extensively%20evaluate%2033%20state-of-the-art%20LVLMs%20and%20summarize%20key%0Afindings%20as%20follows%3A%201%29%20Current%20LVLMs%20exhibit%20notable%20deficiencies%20in%20factual%0Aadherence%2C%20with%20the%20best-performing%20model%20o3%20merely%20achieving%20an%20F-score%20of%0A66.3%25%3B%202%29%20Most%20LVLMs%20are%20overconfident%20in%20what%20they%20generate%2C%20with%20self-stated%0Aconfidence%20exceeding%20actual%20accuracy%3B%203%29%20Retrieval-augmented%20generation%0Ademonstrates%20consistent%20improvements%20at%20the%20cost%20of%20additional%20inference%20time%0Aoverhead%3B%204%29%20Multi-hop%20QA%20demonstrates%20substantially%20degraded%20performance%0Acompared%20to%20single-hop%20sub-QAs%2C%20with%20first-hop%20object%20or%20event%20recognition%0Aemerging%20as%20the%20primary%20bottleneck.%20We%20position%20Video%20SimpleQA%20as%20the%0Acornerstone%20benchmark%20for%20video%20factuality%20assessment%2C%20aiming%20to%20steer%20LVLM%0Adevelopment%20toward%20verifiable%20grounding%20in%20real-world%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18923v2&entry.124074799=Read"},
{"title": "ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with\n  Probability Map Guided Multi-Format Feature Fusion", "author": "Bing Zhu and Zixin He and Weiyi Xiong and Guanhua Ding and Tao Huang and Wei Xiang", "abstract": "  Millimeter wave (mmWave) radar is a non-intrusive privacy and relatively\nconvenient and inexpensive device, which has been demonstrated to be applicable\nin place of RGB cameras in human indoor pose estimation tasks. However, mmWave\nradar relies on the collection of reflected signals from the target, and the\nradar signals containing information is difficult to be fully applied. This has\nbeen a long-standing hindrance to the improvement of pose estimation accuracy.\nTo address this major challenge, this paper introduces a probability map guided\nmulti-format feature fusion model, ProbRadarM3F. This is a novel radar feature\nextraction framework using a traditional FFT method in parallel with a\nprobability map based positional encoding method. ProbRadarM3F fuses the\ntraditional heatmap features and the positional features, then effectively\nachieves the estimation of 14 keypoints of the human body. Experimental\nevaluation on the HuPR dataset proves the effectiveness of the model proposed\nin this paper, outperforming other methods experimented on this dataset with an\nAP of 69.9 %. The emphasis of our study is focusing on the position information\nthat is not exploited before in radar singal. This provides direction to\ninvestigate other potential non-redundant information from mmWave rader.\n", "link": "http://arxiv.org/abs/2405.05164v5", "date": "2025-08-13", "relevancy": 2.1812, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5852}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5478}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProbRadarM3F%3A%20mmWave%20Radar%20based%20Human%20Skeletal%20Pose%20Estimation%20with%0A%20%20Probability%20Map%20Guided%20Multi-Format%20Feature%20Fusion&body=Title%3A%20ProbRadarM3F%3A%20mmWave%20Radar%20based%20Human%20Skeletal%20Pose%20Estimation%20with%0A%20%20Probability%20Map%20Guided%20Multi-Format%20Feature%20Fusion%0AAuthor%3A%20Bing%20Zhu%20and%20Zixin%20He%20and%20Weiyi%20Xiong%20and%20Guanhua%20Ding%20and%20Tao%20Huang%20and%20Wei%20Xiang%0AAbstract%3A%20%20%20Millimeter%20wave%20%28mmWave%29%20radar%20is%20a%20non-intrusive%20privacy%20and%20relatively%0Aconvenient%20and%20inexpensive%20device%2C%20which%20has%20been%20demonstrated%20to%20be%20applicable%0Ain%20place%20of%20RGB%20cameras%20in%20human%20indoor%20pose%20estimation%20tasks.%20However%2C%20mmWave%0Aradar%20relies%20on%20the%20collection%20of%20reflected%20signals%20from%20the%20target%2C%20and%20the%0Aradar%20signals%20containing%20information%20is%20difficult%20to%20be%20fully%20applied.%20This%20has%0Abeen%20a%20long-standing%20hindrance%20to%20the%20improvement%20of%20pose%20estimation%20accuracy.%0ATo%20address%20this%20major%20challenge%2C%20this%20paper%20introduces%20a%20probability%20map%20guided%0Amulti-format%20feature%20fusion%20model%2C%20ProbRadarM3F.%20This%20is%20a%20novel%20radar%20feature%0Aextraction%20framework%20using%20a%20traditional%20FFT%20method%20in%20parallel%20with%20a%0Aprobability%20map%20based%20positional%20encoding%20method.%20ProbRadarM3F%20fuses%20the%0Atraditional%20heatmap%20features%20and%20the%20positional%20features%2C%20then%20effectively%0Aachieves%20the%20estimation%20of%2014%20keypoints%20of%20the%20human%20body.%20Experimental%0Aevaluation%20on%20the%20HuPR%20dataset%20proves%20the%20effectiveness%20of%20the%20model%20proposed%0Ain%20this%20paper%2C%20outperforming%20other%20methods%20experimented%20on%20this%20dataset%20with%20an%0AAP%20of%2069.9%20%25.%20The%20emphasis%20of%20our%20study%20is%20focusing%20on%20the%20position%20information%0Athat%20is%20not%20exploited%20before%20in%20radar%20singal.%20This%20provides%20direction%20to%0Ainvestigate%20other%20potential%20non-redundant%20information%20from%20mmWave%20rader.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05164v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbRadarM3F%253A%2520mmWave%2520Radar%2520based%2520Human%2520Skeletal%2520Pose%2520Estimation%2520with%250A%2520%2520Probability%2520Map%2520Guided%2520Multi-Format%2520Feature%2520Fusion%26entry.906535625%3DBing%2520Zhu%2520and%2520Zixin%2520He%2520and%2520Weiyi%2520Xiong%2520and%2520Guanhua%2520Ding%2520and%2520Tao%2520Huang%2520and%2520Wei%2520Xiang%26entry.1292438233%3D%2520%2520Millimeter%2520wave%2520%2528mmWave%2529%2520radar%2520is%2520a%2520non-intrusive%2520privacy%2520and%2520relatively%250Aconvenient%2520and%2520inexpensive%2520device%252C%2520which%2520has%2520been%2520demonstrated%2520to%2520be%2520applicable%250Ain%2520place%2520of%2520RGB%2520cameras%2520in%2520human%2520indoor%2520pose%2520estimation%2520tasks.%2520However%252C%2520mmWave%250Aradar%2520relies%2520on%2520the%2520collection%2520of%2520reflected%2520signals%2520from%2520the%2520target%252C%2520and%2520the%250Aradar%2520signals%2520containing%2520information%2520is%2520difficult%2520to%2520be%2520fully%2520applied.%2520This%2520has%250Abeen%2520a%2520long-standing%2520hindrance%2520to%2520the%2520improvement%2520of%2520pose%2520estimation%2520accuracy.%250ATo%2520address%2520this%2520major%2520challenge%252C%2520this%2520paper%2520introduces%2520a%2520probability%2520map%2520guided%250Amulti-format%2520feature%2520fusion%2520model%252C%2520ProbRadarM3F.%2520This%2520is%2520a%2520novel%2520radar%2520feature%250Aextraction%2520framework%2520using%2520a%2520traditional%2520FFT%2520method%2520in%2520parallel%2520with%2520a%250Aprobability%2520map%2520based%2520positional%2520encoding%2520method.%2520ProbRadarM3F%2520fuses%2520the%250Atraditional%2520heatmap%2520features%2520and%2520the%2520positional%2520features%252C%2520then%2520effectively%250Aachieves%2520the%2520estimation%2520of%252014%2520keypoints%2520of%2520the%2520human%2520body.%2520Experimental%250Aevaluation%2520on%2520the%2520HuPR%2520dataset%2520proves%2520the%2520effectiveness%2520of%2520the%2520model%2520proposed%250Ain%2520this%2520paper%252C%2520outperforming%2520other%2520methods%2520experimented%2520on%2520this%2520dataset%2520with%2520an%250AAP%2520of%252069.9%2520%2525.%2520The%2520emphasis%2520of%2520our%2520study%2520is%2520focusing%2520on%2520the%2520position%2520information%250Athat%2520is%2520not%2520exploited%2520before%2520in%2520radar%2520singal.%2520This%2520provides%2520direction%2520to%250Ainvestigate%2520other%2520potential%2520non-redundant%2520information%2520from%2520mmWave%2520rader.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05164v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProbRadarM3F%3A%20mmWave%20Radar%20based%20Human%20Skeletal%20Pose%20Estimation%20with%0A%20%20Probability%20Map%20Guided%20Multi-Format%20Feature%20Fusion&entry.906535625=Bing%20Zhu%20and%20Zixin%20He%20and%20Weiyi%20Xiong%20and%20Guanhua%20Ding%20and%20Tao%20Huang%20and%20Wei%20Xiang&entry.1292438233=%20%20Millimeter%20wave%20%28mmWave%29%20radar%20is%20a%20non-intrusive%20privacy%20and%20relatively%0Aconvenient%20and%20inexpensive%20device%2C%20which%20has%20been%20demonstrated%20to%20be%20applicable%0Ain%20place%20of%20RGB%20cameras%20in%20human%20indoor%20pose%20estimation%20tasks.%20However%2C%20mmWave%0Aradar%20relies%20on%20the%20collection%20of%20reflected%20signals%20from%20the%20target%2C%20and%20the%0Aradar%20signals%20containing%20information%20is%20difficult%20to%20be%20fully%20applied.%20This%20has%0Abeen%20a%20long-standing%20hindrance%20to%20the%20improvement%20of%20pose%20estimation%20accuracy.%0ATo%20address%20this%20major%20challenge%2C%20this%20paper%20introduces%20a%20probability%20map%20guided%0Amulti-format%20feature%20fusion%20model%2C%20ProbRadarM3F.%20This%20is%20a%20novel%20radar%20feature%0Aextraction%20framework%20using%20a%20traditional%20FFT%20method%20in%20parallel%20with%20a%0Aprobability%20map%20based%20positional%20encoding%20method.%20ProbRadarM3F%20fuses%20the%0Atraditional%20heatmap%20features%20and%20the%20positional%20features%2C%20then%20effectively%0Aachieves%20the%20estimation%20of%2014%20keypoints%20of%20the%20human%20body.%20Experimental%0Aevaluation%20on%20the%20HuPR%20dataset%20proves%20the%20effectiveness%20of%20the%20model%20proposed%0Ain%20this%20paper%2C%20outperforming%20other%20methods%20experimented%20on%20this%20dataset%20with%20an%0AAP%20of%2069.9%20%25.%20The%20emphasis%20of%20our%20study%20is%20focusing%20on%20the%20position%20information%0Athat%20is%20not%20exploited%20before%20in%20radar%20singal.%20This%20provides%20direction%20to%0Ainvestigate%20other%20potential%20non-redundant%20information%20from%20mmWave%20rader.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05164v5&entry.124074799=Read"},
{"title": "Hierarchical Graph Attention Network for No-Reference Omnidirectional\n  Image Quality Assessment", "author": "Hao Yang and Xu Zhang and Jiaqi Ma and Linwei Zhu and Yun Zhang and Huan Zhang", "abstract": "  Current Omnidirectional Image Quality Assessment (OIQA) methods struggle to\nevaluate locally non-uniform distortions due to inadequate modeling of spatial\nvariations in quality and ineffective feature representation capturing both\nlocal details and global context. To address this, we propose a graph neural\nnetwork-based OIQA framework that explicitly models structural relationships\nbetween viewports to enhance perception of spatial distortion non-uniformity.\nOur approach employs Fibonacci sphere sampling to generate viewports with\nwell-structured topology, representing each as a graph node. Multi-stage\nfeature extraction networks then derive high-dimensional node representation.\nTo holistically capture spatial dependencies, we integrate a Graph Attention\nNetwork (GAT) modeling fine-grained local distortion variations among adjacent\nviewports, and a graph transformer capturing long-range quality interactions\nacross distant regions. Extensive experiments on two large-scale OIQA databases\nwith complex spatial distortions demonstrate that our method significantly\noutperforms existing approaches, confirming its effectiveness and strong\ngeneralization capability.\n", "link": "http://arxiv.org/abs/2508.09843v1", "date": "2025-08-13", "relevancy": 2.1794, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5493}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5442}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Graph%20Attention%20Network%20for%20No-Reference%20Omnidirectional%0A%20%20Image%20Quality%20Assessment&body=Title%3A%20Hierarchical%20Graph%20Attention%20Network%20for%20No-Reference%20Omnidirectional%0A%20%20Image%20Quality%20Assessment%0AAuthor%3A%20Hao%20Yang%20and%20Xu%20Zhang%20and%20Jiaqi%20Ma%20and%20Linwei%20Zhu%20and%20Yun%20Zhang%20and%20Huan%20Zhang%0AAbstract%3A%20%20%20Current%20Omnidirectional%20Image%20Quality%20Assessment%20%28OIQA%29%20methods%20struggle%20to%0Aevaluate%20locally%20non-uniform%20distortions%20due%20to%20inadequate%20modeling%20of%20spatial%0Avariations%20in%20quality%20and%20ineffective%20feature%20representation%20capturing%20both%0Alocal%20details%20and%20global%20context.%20To%20address%20this%2C%20we%20propose%20a%20graph%20neural%0Anetwork-based%20OIQA%20framework%20that%20explicitly%20models%20structural%20relationships%0Abetween%20viewports%20to%20enhance%20perception%20of%20spatial%20distortion%20non-uniformity.%0AOur%20approach%20employs%20Fibonacci%20sphere%20sampling%20to%20generate%20viewports%20with%0Awell-structured%20topology%2C%20representing%20each%20as%20a%20graph%20node.%20Multi-stage%0Afeature%20extraction%20networks%20then%20derive%20high-dimensional%20node%20representation.%0ATo%20holistically%20capture%20spatial%20dependencies%2C%20we%20integrate%20a%20Graph%20Attention%0ANetwork%20%28GAT%29%20modeling%20fine-grained%20local%20distortion%20variations%20among%20adjacent%0Aviewports%2C%20and%20a%20graph%20transformer%20capturing%20long-range%20quality%20interactions%0Aacross%20distant%20regions.%20Extensive%20experiments%20on%20two%20large-scale%20OIQA%20databases%0Awith%20complex%20spatial%20distortions%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20existing%20approaches%2C%20confirming%20its%20effectiveness%20and%20strong%0Ageneralization%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Graph%2520Attention%2520Network%2520for%2520No-Reference%2520Omnidirectional%250A%2520%2520Image%2520Quality%2520Assessment%26entry.906535625%3DHao%2520Yang%2520and%2520Xu%2520Zhang%2520and%2520Jiaqi%2520Ma%2520and%2520Linwei%2520Zhu%2520and%2520Yun%2520Zhang%2520and%2520Huan%2520Zhang%26entry.1292438233%3D%2520%2520Current%2520Omnidirectional%2520Image%2520Quality%2520Assessment%2520%2528OIQA%2529%2520methods%2520struggle%2520to%250Aevaluate%2520locally%2520non-uniform%2520distortions%2520due%2520to%2520inadequate%2520modeling%2520of%2520spatial%250Avariations%2520in%2520quality%2520and%2520ineffective%2520feature%2520representation%2520capturing%2520both%250Alocal%2520details%2520and%2520global%2520context.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520graph%2520neural%250Anetwork-based%2520OIQA%2520framework%2520that%2520explicitly%2520models%2520structural%2520relationships%250Abetween%2520viewports%2520to%2520enhance%2520perception%2520of%2520spatial%2520distortion%2520non-uniformity.%250AOur%2520approach%2520employs%2520Fibonacci%2520sphere%2520sampling%2520to%2520generate%2520viewports%2520with%250Awell-structured%2520topology%252C%2520representing%2520each%2520as%2520a%2520graph%2520node.%2520Multi-stage%250Afeature%2520extraction%2520networks%2520then%2520derive%2520high-dimensional%2520node%2520representation.%250ATo%2520holistically%2520capture%2520spatial%2520dependencies%252C%2520we%2520integrate%2520a%2520Graph%2520Attention%250ANetwork%2520%2528GAT%2529%2520modeling%2520fine-grained%2520local%2520distortion%2520variations%2520among%2520adjacent%250Aviewports%252C%2520and%2520a%2520graph%2520transformer%2520capturing%2520long-range%2520quality%2520interactions%250Aacross%2520distant%2520regions.%2520Extensive%2520experiments%2520on%2520two%2520large-scale%2520OIQA%2520databases%250Awith%2520complex%2520spatial%2520distortions%2520demonstrate%2520that%2520our%2520method%2520significantly%250Aoutperforms%2520existing%2520approaches%252C%2520confirming%2520its%2520effectiveness%2520and%2520strong%250Ageneralization%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Graph%20Attention%20Network%20for%20No-Reference%20Omnidirectional%0A%20%20Image%20Quality%20Assessment&entry.906535625=Hao%20Yang%20and%20Xu%20Zhang%20and%20Jiaqi%20Ma%20and%20Linwei%20Zhu%20and%20Yun%20Zhang%20and%20Huan%20Zhang&entry.1292438233=%20%20Current%20Omnidirectional%20Image%20Quality%20Assessment%20%28OIQA%29%20methods%20struggle%20to%0Aevaluate%20locally%20non-uniform%20distortions%20due%20to%20inadequate%20modeling%20of%20spatial%0Avariations%20in%20quality%20and%20ineffective%20feature%20representation%20capturing%20both%0Alocal%20details%20and%20global%20context.%20To%20address%20this%2C%20we%20propose%20a%20graph%20neural%0Anetwork-based%20OIQA%20framework%20that%20explicitly%20models%20structural%20relationships%0Abetween%20viewports%20to%20enhance%20perception%20of%20spatial%20distortion%20non-uniformity.%0AOur%20approach%20employs%20Fibonacci%20sphere%20sampling%20to%20generate%20viewports%20with%0Awell-structured%20topology%2C%20representing%20each%20as%20a%20graph%20node.%20Multi-stage%0Afeature%20extraction%20networks%20then%20derive%20high-dimensional%20node%20representation.%0ATo%20holistically%20capture%20spatial%20dependencies%2C%20we%20integrate%20a%20Graph%20Attention%0ANetwork%20%28GAT%29%20modeling%20fine-grained%20local%20distortion%20variations%20among%20adjacent%0Aviewports%2C%20and%20a%20graph%20transformer%20capturing%20long-range%20quality%20interactions%0Aacross%20distant%20regions.%20Extensive%20experiments%20on%20two%20large-scale%20OIQA%20databases%0Awith%20complex%20spatial%20distortions%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20existing%20approaches%2C%20confirming%20its%20effectiveness%20and%20strong%0Ageneralization%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09843v1&entry.124074799=Read"},
{"title": "Towards Comprehensive Cellular Characterisation of H&E slides", "author": "Benjamin Adjadj and Pierre-Antoine Bannier and Guillaume Horent and Sebastien Mandela and Aurore Lyon and Kathryn Schutte and Ulysse Marteau and Valentin Gaury and Laura Dumont and Thomas Mathieu and Reda Belbahri and Beno\u00eet Schmauch and Eric Durand and Katharina Von Loga and Lucie Gillet", "abstract": "  Cell detection, segmentation and classification are essential for analyzing\ntumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing\nmethods suffer from poor performance on understudied cell types (rare or not\npresent in public datasets) and limited cross-domain generalization. To address\nthese shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell\nanalysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei\ncovering 13 cell types. In external validation across 4 independent cohorts,\nHistoPLUS outperforms current state-of-the-art models in detection quality by\n5.2% and overall F1 classification score by 23.7%, while using 5x fewer\nparameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types\nand brings significant improvements on 8 of 13 cell types. Moreover, we show\nthat HistoPLUS robustly transfers to two oncology indications unseen during\ntraining. To support broader TME biomarker research, we release the model\nweights and inference code at https://github.com/owkin/histoplus/.\n", "link": "http://arxiv.org/abs/2508.09926v1", "date": "2025-08-13", "relevancy": 2.1546, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4247}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Comprehensive%20Cellular%20Characterisation%20of%20H%26E%20slides&body=Title%3A%20Towards%20Comprehensive%20Cellular%20Characterisation%20of%20H%26E%20slides%0AAuthor%3A%20Benjamin%20Adjadj%20and%20Pierre-Antoine%20Bannier%20and%20Guillaume%20Horent%20and%20Sebastien%20Mandela%20and%20Aurore%20Lyon%20and%20Kathryn%20Schutte%20and%20Ulysse%20Marteau%20and%20Valentin%20Gaury%20and%20Laura%20Dumont%20and%20Thomas%20Mathieu%20and%20Reda%20Belbahri%20and%20Beno%C3%AEt%20Schmauch%20and%20Eric%20Durand%20and%20Katharina%20Von%20Loga%20and%20Lucie%20Gillet%0AAbstract%3A%20%20%20Cell%20detection%2C%20segmentation%20and%20classification%20are%20essential%20for%20analyzing%0Atumor%20microenvironments%20%28TME%29%20on%20hematoxylin%20and%20eosin%20%28H%26E%29%20slides.%20Existing%0Amethods%20suffer%20from%20poor%20performance%20on%20understudied%20cell%20types%20%28rare%20or%20not%0Apresent%20in%20public%20datasets%29%20and%20limited%20cross-domain%20generalization.%20To%20address%0Athese%20shortcomings%2C%20we%20introduce%20HistoPLUS%2C%20a%20state-of-the-art%20model%20for%20cell%0Aanalysis%2C%20trained%20on%20a%20novel%20curated%20pan-cancer%20dataset%20of%20108%2C722%20nuclei%0Acovering%2013%20cell%20types.%20In%20external%20validation%20across%204%20independent%20cohorts%2C%0AHistoPLUS%20outperforms%20current%20state-of-the-art%20models%20in%20detection%20quality%20by%0A5.2%25%20and%20overall%20F1%20classification%20score%20by%2023.7%25%2C%20while%20using%205x%20fewer%0Aparameters.%20Notably%2C%20HistoPLUS%20unlocks%20the%20study%20of%207%20understudied%20cell%20types%0Aand%20brings%20significant%20improvements%20on%208%20of%2013%20cell%20types.%20Moreover%2C%20we%20show%0Athat%20HistoPLUS%20robustly%20transfers%20to%20two%20oncology%20indications%20unseen%20during%0Atraining.%20To%20support%20broader%20TME%20biomarker%20research%2C%20we%20release%20the%20model%0Aweights%20and%20inference%20code%20at%20https%3A//github.com/owkin/histoplus/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Comprehensive%2520Cellular%2520Characterisation%2520of%2520H%2526E%2520slides%26entry.906535625%3DBenjamin%2520Adjadj%2520and%2520Pierre-Antoine%2520Bannier%2520and%2520Guillaume%2520Horent%2520and%2520Sebastien%2520Mandela%2520and%2520Aurore%2520Lyon%2520and%2520Kathryn%2520Schutte%2520and%2520Ulysse%2520Marteau%2520and%2520Valentin%2520Gaury%2520and%2520Laura%2520Dumont%2520and%2520Thomas%2520Mathieu%2520and%2520Reda%2520Belbahri%2520and%2520Beno%25C3%25AEt%2520Schmauch%2520and%2520Eric%2520Durand%2520and%2520Katharina%2520Von%2520Loga%2520and%2520Lucie%2520Gillet%26entry.1292438233%3D%2520%2520Cell%2520detection%252C%2520segmentation%2520and%2520classification%2520are%2520essential%2520for%2520analyzing%250Atumor%2520microenvironments%2520%2528TME%2529%2520on%2520hematoxylin%2520and%2520eosin%2520%2528H%2526E%2529%2520slides.%2520Existing%250Amethods%2520suffer%2520from%2520poor%2520performance%2520on%2520understudied%2520cell%2520types%2520%2528rare%2520or%2520not%250Apresent%2520in%2520public%2520datasets%2529%2520and%2520limited%2520cross-domain%2520generalization.%2520To%2520address%250Athese%2520shortcomings%252C%2520we%2520introduce%2520HistoPLUS%252C%2520a%2520state-of-the-art%2520model%2520for%2520cell%250Aanalysis%252C%2520trained%2520on%2520a%2520novel%2520curated%2520pan-cancer%2520dataset%2520of%2520108%252C722%2520nuclei%250Acovering%252013%2520cell%2520types.%2520In%2520external%2520validation%2520across%25204%2520independent%2520cohorts%252C%250AHistoPLUS%2520outperforms%2520current%2520state-of-the-art%2520models%2520in%2520detection%2520quality%2520by%250A5.2%2525%2520and%2520overall%2520F1%2520classification%2520score%2520by%252023.7%2525%252C%2520while%2520using%25205x%2520fewer%250Aparameters.%2520Notably%252C%2520HistoPLUS%2520unlocks%2520the%2520study%2520of%25207%2520understudied%2520cell%2520types%250Aand%2520brings%2520significant%2520improvements%2520on%25208%2520of%252013%2520cell%2520types.%2520Moreover%252C%2520we%2520show%250Athat%2520HistoPLUS%2520robustly%2520transfers%2520to%2520two%2520oncology%2520indications%2520unseen%2520during%250Atraining.%2520To%2520support%2520broader%2520TME%2520biomarker%2520research%252C%2520we%2520release%2520the%2520model%250Aweights%2520and%2520inference%2520code%2520at%2520https%253A//github.com/owkin/histoplus/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Comprehensive%20Cellular%20Characterisation%20of%20H%26E%20slides&entry.906535625=Benjamin%20Adjadj%20and%20Pierre-Antoine%20Bannier%20and%20Guillaume%20Horent%20and%20Sebastien%20Mandela%20and%20Aurore%20Lyon%20and%20Kathryn%20Schutte%20and%20Ulysse%20Marteau%20and%20Valentin%20Gaury%20and%20Laura%20Dumont%20and%20Thomas%20Mathieu%20and%20Reda%20Belbahri%20and%20Beno%C3%AEt%20Schmauch%20and%20Eric%20Durand%20and%20Katharina%20Von%20Loga%20and%20Lucie%20Gillet&entry.1292438233=%20%20Cell%20detection%2C%20segmentation%20and%20classification%20are%20essential%20for%20analyzing%0Atumor%20microenvironments%20%28TME%29%20on%20hematoxylin%20and%20eosin%20%28H%26E%29%20slides.%20Existing%0Amethods%20suffer%20from%20poor%20performance%20on%20understudied%20cell%20types%20%28rare%20or%20not%0Apresent%20in%20public%20datasets%29%20and%20limited%20cross-domain%20generalization.%20To%20address%0Athese%20shortcomings%2C%20we%20introduce%20HistoPLUS%2C%20a%20state-of-the-art%20model%20for%20cell%0Aanalysis%2C%20trained%20on%20a%20novel%20curated%20pan-cancer%20dataset%20of%20108%2C722%20nuclei%0Acovering%2013%20cell%20types.%20In%20external%20validation%20across%204%20independent%20cohorts%2C%0AHistoPLUS%20outperforms%20current%20state-of-the-art%20models%20in%20detection%20quality%20by%0A5.2%25%20and%20overall%20F1%20classification%20score%20by%2023.7%25%2C%20while%20using%205x%20fewer%0Aparameters.%20Notably%2C%20HistoPLUS%20unlocks%20the%20study%20of%207%20understudied%20cell%20types%0Aand%20brings%20significant%20improvements%20on%208%20of%2013%20cell%20types.%20Moreover%2C%20we%20show%0Athat%20HistoPLUS%20robustly%20transfers%20to%20two%20oncology%20indications%20unseen%20during%0Atraining.%20To%20support%20broader%20TME%20biomarker%20research%2C%20we%20release%20the%20model%0Aweights%20and%20inference%20code%20at%20https%3A//github.com/owkin/histoplus/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09926v1&entry.124074799=Read"},
{"title": "Revisiting 3D Medical Scribble Supervision: Benchmarking Beyond Cardiac\n  Segmentation", "author": "Karol Gotkowski and Klaus H. Maier-Hein and Fabian Isensee", "abstract": "  Scribble supervision has emerged as a promising approach for reducing\nannotation costs in medical 3D segmentation by leveraging sparse annotations\ninstead of voxel-wise labels. While existing methods report strong performance,\na closer analysis reveals that the majority of research is confined to the\ncardiac domain, predominantly using ACDC and MSCMR datasets. This\nover-specialization has resulted in severe overfitting, misleading claims of\nperformance improvements, and a lack of generalization across broader\nsegmentation tasks. In this work, we formulate a set of key requirements for\npractical scribble supervision and introduce ScribbleBench, a comprehensive\nbenchmark spanning over seven diverse medical imaging datasets, to\nsystematically evaluate the fulfillment of these requirements. Consequently, we\nuncover a general failure of methods to generalize across tasks and that many\nwidely used novelties degrade performance outside of the cardiac domain,\nwhereas simpler overlooked approaches achieve superior generalization. Finally,\nwe raise awareness for a strong yet overlooked baseline, nnU-Net coupled with a\npartial loss, which consistently outperforms specialized methods across a\ndiverse range of tasks. By identifying fundamental limitations in existing\nresearch and establishing a new benchmark-driven evaluation standard, this work\naims to steer scribble supervision toward more practical, robust, and\ngeneralizable methodologies for medical image segmentation.\n", "link": "http://arxiv.org/abs/2403.12834v2", "date": "2025-08-13", "relevancy": 2.1435, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5381}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%203D%20Medical%20Scribble%20Supervision%3A%20Benchmarking%20Beyond%20Cardiac%0A%20%20Segmentation&body=Title%3A%20Revisiting%203D%20Medical%20Scribble%20Supervision%3A%20Benchmarking%20Beyond%20Cardiac%0A%20%20Segmentation%0AAuthor%3A%20Karol%20Gotkowski%20and%20Klaus%20H.%20Maier-Hein%20and%20Fabian%20Isensee%0AAbstract%3A%20%20%20Scribble%20supervision%20has%20emerged%20as%20a%20promising%20approach%20for%20reducing%0Aannotation%20costs%20in%20medical%203D%20segmentation%20by%20leveraging%20sparse%20annotations%0Ainstead%20of%20voxel-wise%20labels.%20While%20existing%20methods%20report%20strong%20performance%2C%0Aa%20closer%20analysis%20reveals%20that%20the%20majority%20of%20research%20is%20confined%20to%20the%0Acardiac%20domain%2C%20predominantly%20using%20ACDC%20and%20MSCMR%20datasets.%20This%0Aover-specialization%20has%20resulted%20in%20severe%20overfitting%2C%20misleading%20claims%20of%0Aperformance%20improvements%2C%20and%20a%20lack%20of%20generalization%20across%20broader%0Asegmentation%20tasks.%20In%20this%20work%2C%20we%20formulate%20a%20set%20of%20key%20requirements%20for%0Apractical%20scribble%20supervision%20and%20introduce%20ScribbleBench%2C%20a%20comprehensive%0Abenchmark%20spanning%20over%20seven%20diverse%20medical%20imaging%20datasets%2C%20to%0Asystematically%20evaluate%20the%20fulfillment%20of%20these%20requirements.%20Consequently%2C%20we%0Auncover%20a%20general%20failure%20of%20methods%20to%20generalize%20across%20tasks%20and%20that%20many%0Awidely%20used%20novelties%20degrade%20performance%20outside%20of%20the%20cardiac%20domain%2C%0Awhereas%20simpler%20overlooked%20approaches%20achieve%20superior%20generalization.%20Finally%2C%0Awe%20raise%20awareness%20for%20a%20strong%20yet%20overlooked%20baseline%2C%20nnU-Net%20coupled%20with%20a%0Apartial%20loss%2C%20which%20consistently%20outperforms%20specialized%20methods%20across%20a%0Adiverse%20range%20of%20tasks.%20By%20identifying%20fundamental%20limitations%20in%20existing%0Aresearch%20and%20establishing%20a%20new%20benchmark-driven%20evaluation%20standard%2C%20this%20work%0Aaims%20to%20steer%20scribble%20supervision%20toward%20more%20practical%2C%20robust%2C%20and%0Ageneralizable%20methodologies%20for%20medical%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12834v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%25203D%2520Medical%2520Scribble%2520Supervision%253A%2520Benchmarking%2520Beyond%2520Cardiac%250A%2520%2520Segmentation%26entry.906535625%3DKarol%2520Gotkowski%2520and%2520Klaus%2520H.%2520Maier-Hein%2520and%2520Fabian%2520Isensee%26entry.1292438233%3D%2520%2520Scribble%2520supervision%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520reducing%250Aannotation%2520costs%2520in%2520medical%25203D%2520segmentation%2520by%2520leveraging%2520sparse%2520annotations%250Ainstead%2520of%2520voxel-wise%2520labels.%2520While%2520existing%2520methods%2520report%2520strong%2520performance%252C%250Aa%2520closer%2520analysis%2520reveals%2520that%2520the%2520majority%2520of%2520research%2520is%2520confined%2520to%2520the%250Acardiac%2520domain%252C%2520predominantly%2520using%2520ACDC%2520and%2520MSCMR%2520datasets.%2520This%250Aover-specialization%2520has%2520resulted%2520in%2520severe%2520overfitting%252C%2520misleading%2520claims%2520of%250Aperformance%2520improvements%252C%2520and%2520a%2520lack%2520of%2520generalization%2520across%2520broader%250Asegmentation%2520tasks.%2520In%2520this%2520work%252C%2520we%2520formulate%2520a%2520set%2520of%2520key%2520requirements%2520for%250Apractical%2520scribble%2520supervision%2520and%2520introduce%2520ScribbleBench%252C%2520a%2520comprehensive%250Abenchmark%2520spanning%2520over%2520seven%2520diverse%2520medical%2520imaging%2520datasets%252C%2520to%250Asystematically%2520evaluate%2520the%2520fulfillment%2520of%2520these%2520requirements.%2520Consequently%252C%2520we%250Auncover%2520a%2520general%2520failure%2520of%2520methods%2520to%2520generalize%2520across%2520tasks%2520and%2520that%2520many%250Awidely%2520used%2520novelties%2520degrade%2520performance%2520outside%2520of%2520the%2520cardiac%2520domain%252C%250Awhereas%2520simpler%2520overlooked%2520approaches%2520achieve%2520superior%2520generalization.%2520Finally%252C%250Awe%2520raise%2520awareness%2520for%2520a%2520strong%2520yet%2520overlooked%2520baseline%252C%2520nnU-Net%2520coupled%2520with%2520a%250Apartial%2520loss%252C%2520which%2520consistently%2520outperforms%2520specialized%2520methods%2520across%2520a%250Adiverse%2520range%2520of%2520tasks.%2520By%2520identifying%2520fundamental%2520limitations%2520in%2520existing%250Aresearch%2520and%2520establishing%2520a%2520new%2520benchmark-driven%2520evaluation%2520standard%252C%2520this%2520work%250Aaims%2520to%2520steer%2520scribble%2520supervision%2520toward%2520more%2520practical%252C%2520robust%252C%2520and%250Ageneralizable%2520methodologies%2520for%2520medical%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12834v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%203D%20Medical%20Scribble%20Supervision%3A%20Benchmarking%20Beyond%20Cardiac%0A%20%20Segmentation&entry.906535625=Karol%20Gotkowski%20and%20Klaus%20H.%20Maier-Hein%20and%20Fabian%20Isensee&entry.1292438233=%20%20Scribble%20supervision%20has%20emerged%20as%20a%20promising%20approach%20for%20reducing%0Aannotation%20costs%20in%20medical%203D%20segmentation%20by%20leveraging%20sparse%20annotations%0Ainstead%20of%20voxel-wise%20labels.%20While%20existing%20methods%20report%20strong%20performance%2C%0Aa%20closer%20analysis%20reveals%20that%20the%20majority%20of%20research%20is%20confined%20to%20the%0Acardiac%20domain%2C%20predominantly%20using%20ACDC%20and%20MSCMR%20datasets.%20This%0Aover-specialization%20has%20resulted%20in%20severe%20overfitting%2C%20misleading%20claims%20of%0Aperformance%20improvements%2C%20and%20a%20lack%20of%20generalization%20across%20broader%0Asegmentation%20tasks.%20In%20this%20work%2C%20we%20formulate%20a%20set%20of%20key%20requirements%20for%0Apractical%20scribble%20supervision%20and%20introduce%20ScribbleBench%2C%20a%20comprehensive%0Abenchmark%20spanning%20over%20seven%20diverse%20medical%20imaging%20datasets%2C%20to%0Asystematically%20evaluate%20the%20fulfillment%20of%20these%20requirements.%20Consequently%2C%20we%0Auncover%20a%20general%20failure%20of%20methods%20to%20generalize%20across%20tasks%20and%20that%20many%0Awidely%20used%20novelties%20degrade%20performance%20outside%20of%20the%20cardiac%20domain%2C%0Awhereas%20simpler%20overlooked%20approaches%20achieve%20superior%20generalization.%20Finally%2C%0Awe%20raise%20awareness%20for%20a%20strong%20yet%20overlooked%20baseline%2C%20nnU-Net%20coupled%20with%20a%0Apartial%20loss%2C%20which%20consistently%20outperforms%20specialized%20methods%20across%20a%0Adiverse%20range%20of%20tasks.%20By%20identifying%20fundamental%20limitations%20in%20existing%0Aresearch%20and%20establishing%20a%20new%20benchmark-driven%20evaluation%20standard%2C%20this%20work%0Aaims%20to%20steer%20scribble%20supervision%20toward%20more%20practical%2C%20robust%2C%20and%0Ageneralizable%20methodologies%20for%20medical%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12834v2&entry.124074799=Read"},
{"title": "Debiased Fine-Tuning for Vision-language Models by Prompt Regularization", "author": "Beier Zhu and Yulei Niu and Saeil Lee and Minhoe Hur and Hanwang Zhang", "abstract": "  We present a new paradigm for fine-tuning large-scale visionlanguage\npre-trained models on downstream task, dubbed Prompt Regularization (ProReg).\nDifferent from traditional fine-tuning which easily overfits to the downstream\ntask data, ProReg uses the prediction by prompting the pretrained model to\nregularize the fine-tuning. The motivation is: by prompting the large model \"a\nphoto of a [CLASS]\", the fil-lin answer is only dependent on the pretraining\nencyclopedic knowledge while independent of the task data distribution, which\nis usually biased. Specifically, given a training sample prediction during\nfine-tuning, we first calculate its KullbackLeibler loss of the prompt\nprediction and Cross-Entropy loss of the ground-truth label, and then combine\nthem with a proposed sample-wise adaptive trade-off weight, which automatically\nadjusts the transfer between the pretrained and downstream domains. On various\nout-of-distribution benchmarks, we show the consistently strong performance of\nProReg compared with conventional fine-tuning, zero-shot prompt, prompt tuning,\nand other state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2301.12429v3", "date": "2025-08-13", "relevancy": 2.133, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Debiased%20Fine-Tuning%20for%20Vision-language%20Models%20by%20Prompt%20Regularization&body=Title%3A%20Debiased%20Fine-Tuning%20for%20Vision-language%20Models%20by%20Prompt%20Regularization%0AAuthor%3A%20Beier%20Zhu%20and%20Yulei%20Niu%20and%20Saeil%20Lee%20and%20Minhoe%20Hur%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20We%20present%20a%20new%20paradigm%20for%20fine-tuning%20large-scale%20visionlanguage%0Apre-trained%20models%20on%20downstream%20task%2C%20dubbed%20Prompt%20Regularization%20%28ProReg%29.%0ADifferent%20from%20traditional%20fine-tuning%20which%20easily%20overfits%20to%20the%20downstream%0Atask%20data%2C%20ProReg%20uses%20the%20prediction%20by%20prompting%20the%20pretrained%20model%20to%0Aregularize%20the%20fine-tuning.%20The%20motivation%20is%3A%20by%20prompting%20the%20large%20model%20%22a%0Aphoto%20of%20a%20%5BCLASS%5D%22%2C%20the%20fil-lin%20answer%20is%20only%20dependent%20on%20the%20pretraining%0Aencyclopedic%20knowledge%20while%20independent%20of%20the%20task%20data%20distribution%2C%20which%0Ais%20usually%20biased.%20Specifically%2C%20given%20a%20training%20sample%20prediction%20during%0Afine-tuning%2C%20we%20first%20calculate%20its%20KullbackLeibler%20loss%20of%20the%20prompt%0Aprediction%20and%20Cross-Entropy%20loss%20of%20the%20ground-truth%20label%2C%20and%20then%20combine%0Athem%20with%20a%20proposed%20sample-wise%20adaptive%20trade-off%20weight%2C%20which%20automatically%0Aadjusts%20the%20transfer%20between%20the%20pretrained%20and%20downstream%20domains.%20On%20various%0Aout-of-distribution%20benchmarks%2C%20we%20show%20the%20consistently%20strong%20performance%20of%0AProReg%20compared%20with%20conventional%20fine-tuning%2C%20zero-shot%20prompt%2C%20prompt%20tuning%2C%0Aand%20other%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.12429v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDebiased%2520Fine-Tuning%2520for%2520Vision-language%2520Models%2520by%2520Prompt%2520Regularization%26entry.906535625%3DBeier%2520Zhu%2520and%2520Yulei%2520Niu%2520and%2520Saeil%2520Lee%2520and%2520Minhoe%2520Hur%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520paradigm%2520for%2520fine-tuning%2520large-scale%2520visionlanguage%250Apre-trained%2520models%2520on%2520downstream%2520task%252C%2520dubbed%2520Prompt%2520Regularization%2520%2528ProReg%2529.%250ADifferent%2520from%2520traditional%2520fine-tuning%2520which%2520easily%2520overfits%2520to%2520the%2520downstream%250Atask%2520data%252C%2520ProReg%2520uses%2520the%2520prediction%2520by%2520prompting%2520the%2520pretrained%2520model%2520to%250Aregularize%2520the%2520fine-tuning.%2520The%2520motivation%2520is%253A%2520by%2520prompting%2520the%2520large%2520model%2520%2522a%250Aphoto%2520of%2520a%2520%255BCLASS%255D%2522%252C%2520the%2520fil-lin%2520answer%2520is%2520only%2520dependent%2520on%2520the%2520pretraining%250Aencyclopedic%2520knowledge%2520while%2520independent%2520of%2520the%2520task%2520data%2520distribution%252C%2520which%250Ais%2520usually%2520biased.%2520Specifically%252C%2520given%2520a%2520training%2520sample%2520prediction%2520during%250Afine-tuning%252C%2520we%2520first%2520calculate%2520its%2520KullbackLeibler%2520loss%2520of%2520the%2520prompt%250Aprediction%2520and%2520Cross-Entropy%2520loss%2520of%2520the%2520ground-truth%2520label%252C%2520and%2520then%2520combine%250Athem%2520with%2520a%2520proposed%2520sample-wise%2520adaptive%2520trade-off%2520weight%252C%2520which%2520automatically%250Aadjusts%2520the%2520transfer%2520between%2520the%2520pretrained%2520and%2520downstream%2520domains.%2520On%2520various%250Aout-of-distribution%2520benchmarks%252C%2520we%2520show%2520the%2520consistently%2520strong%2520performance%2520of%250AProReg%2520compared%2520with%2520conventional%2520fine-tuning%252C%2520zero-shot%2520prompt%252C%2520prompt%2520tuning%252C%250Aand%2520other%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.12429v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Debiased%20Fine-Tuning%20for%20Vision-language%20Models%20by%20Prompt%20Regularization&entry.906535625=Beier%20Zhu%20and%20Yulei%20Niu%20and%20Saeil%20Lee%20and%20Minhoe%20Hur%20and%20Hanwang%20Zhang&entry.1292438233=%20%20We%20present%20a%20new%20paradigm%20for%20fine-tuning%20large-scale%20visionlanguage%0Apre-trained%20models%20on%20downstream%20task%2C%20dubbed%20Prompt%20Regularization%20%28ProReg%29.%0ADifferent%20from%20traditional%20fine-tuning%20which%20easily%20overfits%20to%20the%20downstream%0Atask%20data%2C%20ProReg%20uses%20the%20prediction%20by%20prompting%20the%20pretrained%20model%20to%0Aregularize%20the%20fine-tuning.%20The%20motivation%20is%3A%20by%20prompting%20the%20large%20model%20%22a%0Aphoto%20of%20a%20%5BCLASS%5D%22%2C%20the%20fil-lin%20answer%20is%20only%20dependent%20on%20the%20pretraining%0Aencyclopedic%20knowledge%20while%20independent%20of%20the%20task%20data%20distribution%2C%20which%0Ais%20usually%20biased.%20Specifically%2C%20given%20a%20training%20sample%20prediction%20during%0Afine-tuning%2C%20we%20first%20calculate%20its%20KullbackLeibler%20loss%20of%20the%20prompt%0Aprediction%20and%20Cross-Entropy%20loss%20of%20the%20ground-truth%20label%2C%20and%20then%20combine%0Athem%20with%20a%20proposed%20sample-wise%20adaptive%20trade-off%20weight%2C%20which%20automatically%0Aadjusts%20the%20transfer%20between%20the%20pretrained%20and%20downstream%20domains.%20On%20various%0Aout-of-distribution%20benchmarks%2C%20we%20show%20the%20consistently%20strong%20performance%20of%0AProReg%20compared%20with%20conventional%20fine-tuning%2C%20zero-shot%20prompt%2C%20prompt%20tuning%2C%0Aand%20other%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.12429v3&entry.124074799=Read"},
{"title": "MGDFIS: Multi-scale Global-detail Feature Integration Strategy for Small\n  Object Detection", "author": "Yuxiang Wang and Xuecheng Bai and Boyu Hu and Chuanzhi Xu and Haodong Chen and Vera Chung and Tingxue Li and Xiaoming Chen", "abstract": "  Small object detection in UAV imagery is crucial for applications such as\nsearch-and-rescue, traffic monitoring, and environmental surveillance, but it\nis hampered by tiny object size, low signal-to-noise ratios, and limited\nfeature extraction. Existing multi-scale fusion methods help, but add\ncomputational burden and blur fine details, making small object detection in\ncluttered scenes difficult. To overcome these challenges, we propose the\nMulti-scale Global-detail Feature Integration Strategy (MGDFIS), a unified\nfusion framework that tightly couples global context with local detail to boost\ndetection performance while maintaining efficiency. MGDFIS comprises three\nsynergistic modules: the FusionLock-TSS Attention Module, which marries\ntoken-statistics self-attention with DynamicTanh normalization to highlight\nspectral and spatial cues at minimal cost; the Global-detail Integration\nModule, which fuses multi-scale context via directional convolution and\nparallel attention while preserving subtle shape and texture variations; and\nthe Dynamic Pixel Attention Module, which generates pixel-wise weighting maps\nto rebalance uneven foreground and background distributions and sharpen\nresponses to true object regions. Extensive experiments on the VisDrone\nbenchmark demonstrate that MGDFIS consistently outperforms state-of-the-art\nmethods across diverse backbone architectures and detection frameworks,\nachieving superior precision and recall with low inference time. By striking an\noptimal balance between accuracy and resource usage, MGDFIS provides a\npractical solution for small-object detection on resource-constrained UAV\nplatforms.\n", "link": "http://arxiv.org/abs/2506.12697v2", "date": "2025-08-13", "relevancy": 2.128, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5395}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5328}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGDFIS%3A%20Multi-scale%20Global-detail%20Feature%20Integration%20Strategy%20for%20Small%0A%20%20Object%20Detection&body=Title%3A%20MGDFIS%3A%20Multi-scale%20Global-detail%20Feature%20Integration%20Strategy%20for%20Small%0A%20%20Object%20Detection%0AAuthor%3A%20Yuxiang%20Wang%20and%20Xuecheng%20Bai%20and%20Boyu%20Hu%20and%20Chuanzhi%20Xu%20and%20Haodong%20Chen%20and%20Vera%20Chung%20and%20Tingxue%20Li%20and%20Xiaoming%20Chen%0AAbstract%3A%20%20%20Small%20object%20detection%20in%20UAV%20imagery%20is%20crucial%20for%20applications%20such%20as%0Asearch-and-rescue%2C%20traffic%20monitoring%2C%20and%20environmental%20surveillance%2C%20but%20it%0Ais%20hampered%20by%20tiny%20object%20size%2C%20low%20signal-to-noise%20ratios%2C%20and%20limited%0Afeature%20extraction.%20Existing%20multi-scale%20fusion%20methods%20help%2C%20but%20add%0Acomputational%20burden%20and%20blur%20fine%20details%2C%20making%20small%20object%20detection%20in%0Acluttered%20scenes%20difficult.%20To%20overcome%20these%20challenges%2C%20we%20propose%20the%0AMulti-scale%20Global-detail%20Feature%20Integration%20Strategy%20%28MGDFIS%29%2C%20a%20unified%0Afusion%20framework%20that%20tightly%20couples%20global%20context%20with%20local%20detail%20to%20boost%0Adetection%20performance%20while%20maintaining%20efficiency.%20MGDFIS%20comprises%20three%0Asynergistic%20modules%3A%20the%20FusionLock-TSS%20Attention%20Module%2C%20which%20marries%0Atoken-statistics%20self-attention%20with%20DynamicTanh%20normalization%20to%20highlight%0Aspectral%20and%20spatial%20cues%20at%20minimal%20cost%3B%20the%20Global-detail%20Integration%0AModule%2C%20which%20fuses%20multi-scale%20context%20via%20directional%20convolution%20and%0Aparallel%20attention%20while%20preserving%20subtle%20shape%20and%20texture%20variations%3B%20and%0Athe%20Dynamic%20Pixel%20Attention%20Module%2C%20which%20generates%20pixel-wise%20weighting%20maps%0Ato%20rebalance%20uneven%20foreground%20and%20background%20distributions%20and%20sharpen%0Aresponses%20to%20true%20object%20regions.%20Extensive%20experiments%20on%20the%20VisDrone%0Abenchmark%20demonstrate%20that%20MGDFIS%20consistently%20outperforms%20state-of-the-art%0Amethods%20across%20diverse%20backbone%20architectures%20and%20detection%20frameworks%2C%0Aachieving%20superior%20precision%20and%20recall%20with%20low%20inference%20time.%20By%20striking%20an%0Aoptimal%20balance%20between%20accuracy%20and%20resource%20usage%2C%20MGDFIS%20provides%20a%0Apractical%20solution%20for%20small-object%20detection%20on%20resource-constrained%20UAV%0Aplatforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGDFIS%253A%2520Multi-scale%2520Global-detail%2520Feature%2520Integration%2520Strategy%2520for%2520Small%250A%2520%2520Object%2520Detection%26entry.906535625%3DYuxiang%2520Wang%2520and%2520Xuecheng%2520Bai%2520and%2520Boyu%2520Hu%2520and%2520Chuanzhi%2520Xu%2520and%2520Haodong%2520Chen%2520and%2520Vera%2520Chung%2520and%2520Tingxue%2520Li%2520and%2520Xiaoming%2520Chen%26entry.1292438233%3D%2520%2520Small%2520object%2520detection%2520in%2520UAV%2520imagery%2520is%2520crucial%2520for%2520applications%2520such%2520as%250Asearch-and-rescue%252C%2520traffic%2520monitoring%252C%2520and%2520environmental%2520surveillance%252C%2520but%2520it%250Ais%2520hampered%2520by%2520tiny%2520object%2520size%252C%2520low%2520signal-to-noise%2520ratios%252C%2520and%2520limited%250Afeature%2520extraction.%2520Existing%2520multi-scale%2520fusion%2520methods%2520help%252C%2520but%2520add%250Acomputational%2520burden%2520and%2520blur%2520fine%2520details%252C%2520making%2520small%2520object%2520detection%2520in%250Acluttered%2520scenes%2520difficult.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520the%250AMulti-scale%2520Global-detail%2520Feature%2520Integration%2520Strategy%2520%2528MGDFIS%2529%252C%2520a%2520unified%250Afusion%2520framework%2520that%2520tightly%2520couples%2520global%2520context%2520with%2520local%2520detail%2520to%2520boost%250Adetection%2520performance%2520while%2520maintaining%2520efficiency.%2520MGDFIS%2520comprises%2520three%250Asynergistic%2520modules%253A%2520the%2520FusionLock-TSS%2520Attention%2520Module%252C%2520which%2520marries%250Atoken-statistics%2520self-attention%2520with%2520DynamicTanh%2520normalization%2520to%2520highlight%250Aspectral%2520and%2520spatial%2520cues%2520at%2520minimal%2520cost%253B%2520the%2520Global-detail%2520Integration%250AModule%252C%2520which%2520fuses%2520multi-scale%2520context%2520via%2520directional%2520convolution%2520and%250Aparallel%2520attention%2520while%2520preserving%2520subtle%2520shape%2520and%2520texture%2520variations%253B%2520and%250Athe%2520Dynamic%2520Pixel%2520Attention%2520Module%252C%2520which%2520generates%2520pixel-wise%2520weighting%2520maps%250Ato%2520rebalance%2520uneven%2520foreground%2520and%2520background%2520distributions%2520and%2520sharpen%250Aresponses%2520to%2520true%2520object%2520regions.%2520Extensive%2520experiments%2520on%2520the%2520VisDrone%250Abenchmark%2520demonstrate%2520that%2520MGDFIS%2520consistently%2520outperforms%2520state-of-the-art%250Amethods%2520across%2520diverse%2520backbone%2520architectures%2520and%2520detection%2520frameworks%252C%250Aachieving%2520superior%2520precision%2520and%2520recall%2520with%2520low%2520inference%2520time.%2520By%2520striking%2520an%250Aoptimal%2520balance%2520between%2520accuracy%2520and%2520resource%2520usage%252C%2520MGDFIS%2520provides%2520a%250Apractical%2520solution%2520for%2520small-object%2520detection%2520on%2520resource-constrained%2520UAV%250Aplatforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGDFIS%3A%20Multi-scale%20Global-detail%20Feature%20Integration%20Strategy%20for%20Small%0A%20%20Object%20Detection&entry.906535625=Yuxiang%20Wang%20and%20Xuecheng%20Bai%20and%20Boyu%20Hu%20and%20Chuanzhi%20Xu%20and%20Haodong%20Chen%20and%20Vera%20Chung%20and%20Tingxue%20Li%20and%20Xiaoming%20Chen&entry.1292438233=%20%20Small%20object%20detection%20in%20UAV%20imagery%20is%20crucial%20for%20applications%20such%20as%0Asearch-and-rescue%2C%20traffic%20monitoring%2C%20and%20environmental%20surveillance%2C%20but%20it%0Ais%20hampered%20by%20tiny%20object%20size%2C%20low%20signal-to-noise%20ratios%2C%20and%20limited%0Afeature%20extraction.%20Existing%20multi-scale%20fusion%20methods%20help%2C%20but%20add%0Acomputational%20burden%20and%20blur%20fine%20details%2C%20making%20small%20object%20detection%20in%0Acluttered%20scenes%20difficult.%20To%20overcome%20these%20challenges%2C%20we%20propose%20the%0AMulti-scale%20Global-detail%20Feature%20Integration%20Strategy%20%28MGDFIS%29%2C%20a%20unified%0Afusion%20framework%20that%20tightly%20couples%20global%20context%20with%20local%20detail%20to%20boost%0Adetection%20performance%20while%20maintaining%20efficiency.%20MGDFIS%20comprises%20three%0Asynergistic%20modules%3A%20the%20FusionLock-TSS%20Attention%20Module%2C%20which%20marries%0Atoken-statistics%20self-attention%20with%20DynamicTanh%20normalization%20to%20highlight%0Aspectral%20and%20spatial%20cues%20at%20minimal%20cost%3B%20the%20Global-detail%20Integration%0AModule%2C%20which%20fuses%20multi-scale%20context%20via%20directional%20convolution%20and%0Aparallel%20attention%20while%20preserving%20subtle%20shape%20and%20texture%20variations%3B%20and%0Athe%20Dynamic%20Pixel%20Attention%20Module%2C%20which%20generates%20pixel-wise%20weighting%20maps%0Ato%20rebalance%20uneven%20foreground%20and%20background%20distributions%20and%20sharpen%0Aresponses%20to%20true%20object%20regions.%20Extensive%20experiments%20on%20the%20VisDrone%0Abenchmark%20demonstrate%20that%20MGDFIS%20consistently%20outperforms%20state-of-the-art%0Amethods%20across%20diverse%20backbone%20architectures%20and%20detection%20frameworks%2C%0Aachieving%20superior%20precision%20and%20recall%20with%20low%20inference%20time.%20By%20striking%20an%0Aoptimal%20balance%20between%20accuracy%20and%20resource%20usage%2C%20MGDFIS%20provides%20a%0Apractical%20solution%20for%20small-object%20detection%20on%20resource-constrained%20UAV%0Aplatforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12697v2&entry.124074799=Read"},
{"title": "NEUBORN: The Neurodevelopmental Evolution framework Using BiOmechanical\n  RemodelliNg", "author": "Nashira Baena and Mariana da Silva and Irina Grigorescu and Aakash Saboo and Saga Masui and Jaques-Donald Tournier and Emma C. Robinson", "abstract": "  Understanding individual cortical development is essential for identifying\ndeviations linked to neurodevelopmental disorders. However, current normative\nmodelling frameworks struggle to capture fine-scale anatomical details due to\ntheir reliance on modelling data within a population-average reference space.\nHere, we present a novel framework for learning individual growth trajectories\nfrom biomechanically constrained, longitudinal, diffeomorphic image\nregistration, implemented via a hierarchical network architecture. Trained on\nneonatal MRI data from the Developing Human Connectome Project, the method\nimproves the biological plausibility of warps, generating growth trajectories\nthat better follow population-level trends while generating smoother warps,\nwith fewer negative Jacobians, relative to state-of-the-art baselines. The\nresulting subject-specific deformations provide interpretable, biologically\ngrounded mappings of development. This framework opens new possibilities for\npredictive modeling of brain maturation and early identification of\nmalformations of cortical development.\n", "link": "http://arxiv.org/abs/2508.09757v1", "date": "2025-08-13", "relevancy": 2.1279, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5523}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5205}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NEUBORN%3A%20The%20Neurodevelopmental%20Evolution%20framework%20Using%20BiOmechanical%0A%20%20RemodelliNg&body=Title%3A%20NEUBORN%3A%20The%20Neurodevelopmental%20Evolution%20framework%20Using%20BiOmechanical%0A%20%20RemodelliNg%0AAuthor%3A%20Nashira%20Baena%20and%20Mariana%20da%20Silva%20and%20Irina%20Grigorescu%20and%20Aakash%20Saboo%20and%20Saga%20Masui%20and%20Jaques-Donald%20Tournier%20and%20Emma%20C.%20Robinson%0AAbstract%3A%20%20%20Understanding%20individual%20cortical%20development%20is%20essential%20for%20identifying%0Adeviations%20linked%20to%20neurodevelopmental%20disorders.%20However%2C%20current%20normative%0Amodelling%20frameworks%20struggle%20to%20capture%20fine-scale%20anatomical%20details%20due%20to%0Atheir%20reliance%20on%20modelling%20data%20within%20a%20population-average%20reference%20space.%0AHere%2C%20we%20present%20a%20novel%20framework%20for%20learning%20individual%20growth%20trajectories%0Afrom%20biomechanically%20constrained%2C%20longitudinal%2C%20diffeomorphic%20image%0Aregistration%2C%20implemented%20via%20a%20hierarchical%20network%20architecture.%20Trained%20on%0Aneonatal%20MRI%20data%20from%20the%20Developing%20Human%20Connectome%20Project%2C%20the%20method%0Aimproves%20the%20biological%20plausibility%20of%20warps%2C%20generating%20growth%20trajectories%0Athat%20better%20follow%20population-level%20trends%20while%20generating%20smoother%20warps%2C%0Awith%20fewer%20negative%20Jacobians%2C%20relative%20to%20state-of-the-art%20baselines.%20The%0Aresulting%20subject-specific%20deformations%20provide%20interpretable%2C%20biologically%0Agrounded%20mappings%20of%20development.%20This%20framework%20opens%20new%20possibilities%20for%0Apredictive%20modeling%20of%20brain%20maturation%20and%20early%20identification%20of%0Amalformations%20of%20cortical%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNEUBORN%253A%2520The%2520Neurodevelopmental%2520Evolution%2520framework%2520Using%2520BiOmechanical%250A%2520%2520RemodelliNg%26entry.906535625%3DNashira%2520Baena%2520and%2520Mariana%2520da%2520Silva%2520and%2520Irina%2520Grigorescu%2520and%2520Aakash%2520Saboo%2520and%2520Saga%2520Masui%2520and%2520Jaques-Donald%2520Tournier%2520and%2520Emma%2520C.%2520Robinson%26entry.1292438233%3D%2520%2520Understanding%2520individual%2520cortical%2520development%2520is%2520essential%2520for%2520identifying%250Adeviations%2520linked%2520to%2520neurodevelopmental%2520disorders.%2520However%252C%2520current%2520normative%250Amodelling%2520frameworks%2520struggle%2520to%2520capture%2520fine-scale%2520anatomical%2520details%2520due%2520to%250Atheir%2520reliance%2520on%2520modelling%2520data%2520within%2520a%2520population-average%2520reference%2520space.%250AHere%252C%2520we%2520present%2520a%2520novel%2520framework%2520for%2520learning%2520individual%2520growth%2520trajectories%250Afrom%2520biomechanically%2520constrained%252C%2520longitudinal%252C%2520diffeomorphic%2520image%250Aregistration%252C%2520implemented%2520via%2520a%2520hierarchical%2520network%2520architecture.%2520Trained%2520on%250Aneonatal%2520MRI%2520data%2520from%2520the%2520Developing%2520Human%2520Connectome%2520Project%252C%2520the%2520method%250Aimproves%2520the%2520biological%2520plausibility%2520of%2520warps%252C%2520generating%2520growth%2520trajectories%250Athat%2520better%2520follow%2520population-level%2520trends%2520while%2520generating%2520smoother%2520warps%252C%250Awith%2520fewer%2520negative%2520Jacobians%252C%2520relative%2520to%2520state-of-the-art%2520baselines.%2520The%250Aresulting%2520subject-specific%2520deformations%2520provide%2520interpretable%252C%2520biologically%250Agrounded%2520mappings%2520of%2520development.%2520This%2520framework%2520opens%2520new%2520possibilities%2520for%250Apredictive%2520modeling%2520of%2520brain%2520maturation%2520and%2520early%2520identification%2520of%250Amalformations%2520of%2520cortical%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NEUBORN%3A%20The%20Neurodevelopmental%20Evolution%20framework%20Using%20BiOmechanical%0A%20%20RemodelliNg&entry.906535625=Nashira%20Baena%20and%20Mariana%20da%20Silva%20and%20Irina%20Grigorescu%20and%20Aakash%20Saboo%20and%20Saga%20Masui%20and%20Jaques-Donald%20Tournier%20and%20Emma%20C.%20Robinson&entry.1292438233=%20%20Understanding%20individual%20cortical%20development%20is%20essential%20for%20identifying%0Adeviations%20linked%20to%20neurodevelopmental%20disorders.%20However%2C%20current%20normative%0Amodelling%20frameworks%20struggle%20to%20capture%20fine-scale%20anatomical%20details%20due%20to%0Atheir%20reliance%20on%20modelling%20data%20within%20a%20population-average%20reference%20space.%0AHere%2C%20we%20present%20a%20novel%20framework%20for%20learning%20individual%20growth%20trajectories%0Afrom%20biomechanically%20constrained%2C%20longitudinal%2C%20diffeomorphic%20image%0Aregistration%2C%20implemented%20via%20a%20hierarchical%20network%20architecture.%20Trained%20on%0Aneonatal%20MRI%20data%20from%20the%20Developing%20Human%20Connectome%20Project%2C%20the%20method%0Aimproves%20the%20biological%20plausibility%20of%20warps%2C%20generating%20growth%20trajectories%0Athat%20better%20follow%20population-level%20trends%20while%20generating%20smoother%20warps%2C%0Awith%20fewer%20negative%20Jacobians%2C%20relative%20to%20state-of-the-art%20baselines.%20The%0Aresulting%20subject-specific%20deformations%20provide%20interpretable%2C%20biologically%0Agrounded%20mappings%20of%20development.%20This%20framework%20opens%20new%20possibilities%20for%0Apredictive%20modeling%20of%20brain%20maturation%20and%20early%20identification%20of%0Amalformations%20of%20cortical%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09757v1&entry.124074799=Read"},
{"title": "HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge\n  Transfer in Neural Networks", "author": "Yanick Chistian Tchenko and Felix Mohr and Hicham Hadj Abdelkader and Hedi Tabia", "abstract": "  A prevailing trend in neural network research suggests that model performance\nimproves with increasing depth and capacity - often at the cost of\nintegrability and efficiency. In this paper, we propose a strategy to optimize\nsmall, deployable models by enhancing their capabilities through structured\nknowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a\nbiologically inspired framework for modular and selective transfer of\ntask-relevant features from a larger, pretrained parent network to a smaller\nchild model. Unlike standard knowledge distillation, which enforces uniform\nimitation of teacher outputs, HKT draws inspiration from biological inheritance\nmechanisms - such as memory RNA transfer in planarians - to guide a multi-stage\nprocess of feature transfer. Neural network blocks are treated as functional\ncarriers, and knowledge is transmitted through three biologically motivated\ncomponents: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention\n(GA) mechanism governs the integration of inherited and native representations,\nensuring both alignment and selectivity. We evaluate HKT across diverse vision\ntasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10),\nand semantic segmentation (LiTS), demonstrating that it significantly improves\nchild model performance while preserving its compactness. The results show that\nHKT consistently outperforms conventional distillation approaches, offering a\ngeneral-purpose, interpretable, and scalable solution for deploying\nhigh-performance neural networks in resource-constrained environments.\n", "link": "http://arxiv.org/abs/2508.09743v1", "date": "2025-08-13", "relevancy": 2.1271, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5356}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HKT%3A%20A%20Biologically%20Inspired%20Framework%20for%20Modular%20Hereditary%20Knowledge%0A%20%20Transfer%20in%20Neural%20Networks&body=Title%3A%20HKT%3A%20A%20Biologically%20Inspired%20Framework%20for%20Modular%20Hereditary%20Knowledge%0A%20%20Transfer%20in%20Neural%20Networks%0AAuthor%3A%20Yanick%20Chistian%20Tchenko%20and%20Felix%20Mohr%20and%20Hicham%20Hadj%20Abdelkader%20and%20Hedi%20Tabia%0AAbstract%3A%20%20%20A%20prevailing%20trend%20in%20neural%20network%20research%20suggests%20that%20model%20performance%0Aimproves%20with%20increasing%20depth%20and%20capacity%20-%20often%20at%20the%20cost%20of%0Aintegrability%20and%20efficiency.%20In%20this%20paper%2C%20we%20propose%20a%20strategy%20to%20optimize%0Asmall%2C%20deployable%20models%20by%20enhancing%20their%20capabilities%20through%20structured%0Aknowledge%20inheritance.%20We%20introduce%20Hereditary%20Knowledge%20Transfer%20%28HKT%29%2C%20a%0Abiologically%20inspired%20framework%20for%20modular%20and%20selective%20transfer%20of%0Atask-relevant%20features%20from%20a%20larger%2C%20pretrained%20parent%20network%20to%20a%20smaller%0Achild%20model.%20Unlike%20standard%20knowledge%20distillation%2C%20which%20enforces%20uniform%0Aimitation%20of%20teacher%20outputs%2C%20HKT%20draws%20inspiration%20from%20biological%20inheritance%0Amechanisms%20-%20such%20as%20memory%20RNA%20transfer%20in%20planarians%20-%20to%20guide%20a%20multi-stage%0Aprocess%20of%20feature%20transfer.%20Neural%20network%20blocks%20are%20treated%20as%20functional%0Acarriers%2C%20and%20knowledge%20is%20transmitted%20through%20three%20biologically%20motivated%0Acomponents%3A%20Extraction%2C%20Transfer%2C%20and%20Mixture%20%28ETM%29.%20A%20novel%20Genetic%20Attention%0A%28GA%29%20mechanism%20governs%20the%20integration%20of%20inherited%20and%20native%20representations%2C%0Aensuring%20both%20alignment%20and%20selectivity.%20We%20evaluate%20HKT%20across%20diverse%20vision%0Atasks%2C%20including%20optical%20flow%20%28Sintel%2C%20KITTI%29%2C%20image%20classification%20%28CIFAR-10%29%2C%0Aand%20semantic%20segmentation%20%28LiTS%29%2C%20demonstrating%20that%20it%20significantly%20improves%0Achild%20model%20performance%20while%20preserving%20its%20compactness.%20The%20results%20show%20that%0AHKT%20consistently%20outperforms%20conventional%20distillation%20approaches%2C%20offering%20a%0Ageneral-purpose%2C%20interpretable%2C%20and%20scalable%20solution%20for%20deploying%0Ahigh-performance%20neural%20networks%20in%20resource-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHKT%253A%2520A%2520Biologically%2520Inspired%2520Framework%2520for%2520Modular%2520Hereditary%2520Knowledge%250A%2520%2520Transfer%2520in%2520Neural%2520Networks%26entry.906535625%3DYanick%2520Chistian%2520Tchenko%2520and%2520Felix%2520Mohr%2520and%2520Hicham%2520Hadj%2520Abdelkader%2520and%2520Hedi%2520Tabia%26entry.1292438233%3D%2520%2520A%2520prevailing%2520trend%2520in%2520neural%2520network%2520research%2520suggests%2520that%2520model%2520performance%250Aimproves%2520with%2520increasing%2520depth%2520and%2520capacity%2520-%2520often%2520at%2520the%2520cost%2520of%250Aintegrability%2520and%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520strategy%2520to%2520optimize%250Asmall%252C%2520deployable%2520models%2520by%2520enhancing%2520their%2520capabilities%2520through%2520structured%250Aknowledge%2520inheritance.%2520We%2520introduce%2520Hereditary%2520Knowledge%2520Transfer%2520%2528HKT%2529%252C%2520a%250Abiologically%2520inspired%2520framework%2520for%2520modular%2520and%2520selective%2520transfer%2520of%250Atask-relevant%2520features%2520from%2520a%2520larger%252C%2520pretrained%2520parent%2520network%2520to%2520a%2520smaller%250Achild%2520model.%2520Unlike%2520standard%2520knowledge%2520distillation%252C%2520which%2520enforces%2520uniform%250Aimitation%2520of%2520teacher%2520outputs%252C%2520HKT%2520draws%2520inspiration%2520from%2520biological%2520inheritance%250Amechanisms%2520-%2520such%2520as%2520memory%2520RNA%2520transfer%2520in%2520planarians%2520-%2520to%2520guide%2520a%2520multi-stage%250Aprocess%2520of%2520feature%2520transfer.%2520Neural%2520network%2520blocks%2520are%2520treated%2520as%2520functional%250Acarriers%252C%2520and%2520knowledge%2520is%2520transmitted%2520through%2520three%2520biologically%2520motivated%250Acomponents%253A%2520Extraction%252C%2520Transfer%252C%2520and%2520Mixture%2520%2528ETM%2529.%2520A%2520novel%2520Genetic%2520Attention%250A%2528GA%2529%2520mechanism%2520governs%2520the%2520integration%2520of%2520inherited%2520and%2520native%2520representations%252C%250Aensuring%2520both%2520alignment%2520and%2520selectivity.%2520We%2520evaluate%2520HKT%2520across%2520diverse%2520vision%250Atasks%252C%2520including%2520optical%2520flow%2520%2528Sintel%252C%2520KITTI%2529%252C%2520image%2520classification%2520%2528CIFAR-10%2529%252C%250Aand%2520semantic%2520segmentation%2520%2528LiTS%2529%252C%2520demonstrating%2520that%2520it%2520significantly%2520improves%250Achild%2520model%2520performance%2520while%2520preserving%2520its%2520compactness.%2520The%2520results%2520show%2520that%250AHKT%2520consistently%2520outperforms%2520conventional%2520distillation%2520approaches%252C%2520offering%2520a%250Ageneral-purpose%252C%2520interpretable%252C%2520and%2520scalable%2520solution%2520for%2520deploying%250Ahigh-performance%2520neural%2520networks%2520in%2520resource-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HKT%3A%20A%20Biologically%20Inspired%20Framework%20for%20Modular%20Hereditary%20Knowledge%0A%20%20Transfer%20in%20Neural%20Networks&entry.906535625=Yanick%20Chistian%20Tchenko%20and%20Felix%20Mohr%20and%20Hicham%20Hadj%20Abdelkader%20and%20Hedi%20Tabia&entry.1292438233=%20%20A%20prevailing%20trend%20in%20neural%20network%20research%20suggests%20that%20model%20performance%0Aimproves%20with%20increasing%20depth%20and%20capacity%20-%20often%20at%20the%20cost%20of%0Aintegrability%20and%20efficiency.%20In%20this%20paper%2C%20we%20propose%20a%20strategy%20to%20optimize%0Asmall%2C%20deployable%20models%20by%20enhancing%20their%20capabilities%20through%20structured%0Aknowledge%20inheritance.%20We%20introduce%20Hereditary%20Knowledge%20Transfer%20%28HKT%29%2C%20a%0Abiologically%20inspired%20framework%20for%20modular%20and%20selective%20transfer%20of%0Atask-relevant%20features%20from%20a%20larger%2C%20pretrained%20parent%20network%20to%20a%20smaller%0Achild%20model.%20Unlike%20standard%20knowledge%20distillation%2C%20which%20enforces%20uniform%0Aimitation%20of%20teacher%20outputs%2C%20HKT%20draws%20inspiration%20from%20biological%20inheritance%0Amechanisms%20-%20such%20as%20memory%20RNA%20transfer%20in%20planarians%20-%20to%20guide%20a%20multi-stage%0Aprocess%20of%20feature%20transfer.%20Neural%20network%20blocks%20are%20treated%20as%20functional%0Acarriers%2C%20and%20knowledge%20is%20transmitted%20through%20three%20biologically%20motivated%0Acomponents%3A%20Extraction%2C%20Transfer%2C%20and%20Mixture%20%28ETM%29.%20A%20novel%20Genetic%20Attention%0A%28GA%29%20mechanism%20governs%20the%20integration%20of%20inherited%20and%20native%20representations%2C%0Aensuring%20both%20alignment%20and%20selectivity.%20We%20evaluate%20HKT%20across%20diverse%20vision%0Atasks%2C%20including%20optical%20flow%20%28Sintel%2C%20KITTI%29%2C%20image%20classification%20%28CIFAR-10%29%2C%0Aand%20semantic%20segmentation%20%28LiTS%29%2C%20demonstrating%20that%20it%20significantly%20improves%0Achild%20model%20performance%20while%20preserving%20its%20compactness.%20The%20results%20show%20that%0AHKT%20consistently%20outperforms%20conventional%20distillation%20approaches%2C%20offering%20a%0Ageneral-purpose%2C%20interpretable%2C%20and%20scalable%20solution%20for%20deploying%0Ahigh-performance%20neural%20networks%20in%20resource-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09743v1&entry.124074799=Read"},
{"title": "Combinative Matching for Geometric Shape Assembly", "author": "Nahyuk Lee and Juhong Min and Junhong Lee and Chunghyun Park and Minsu Cho", "abstract": "  This paper introduces a new shape-matching methodology, combinative matching,\nto combine interlocking parts for geometric shape assembly. Previous methods\nfor geometric assembly typically rely on aligning parts by finding identical\nsurfaces between the parts as in conventional shape matching and registration.\nIn contrast, we explicitly model two distinct properties of interlocking\nshapes: 'identical surface shape' and 'opposite volume occupancy.' Our method\nthus learns to establish correspondences across regions where their surface\nshapes appear identical but their volumes occupy the inverted space to each\nother. To facilitate this process, we also learn to align regions in rotation\nby estimating their shape orientations via equivariant neural networks. The\nproposed approach significantly reduces local ambiguities in matching and\nallows a robust combination of parts in assembly. Experimental results on\ngeometric assembly benchmarks demonstrate the efficacy of our method,\nconsistently outperforming the state of the art. Project page:\nhttps://nahyuklee.github.io/cmnet.\n", "link": "http://arxiv.org/abs/2508.09780v1", "date": "2025-08-13", "relevancy": 2.121, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5442}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5253}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combinative%20Matching%20for%20Geometric%20Shape%20Assembly&body=Title%3A%20Combinative%20Matching%20for%20Geometric%20Shape%20Assembly%0AAuthor%3A%20Nahyuk%20Lee%20and%20Juhong%20Min%20and%20Junhong%20Lee%20and%20Chunghyun%20Park%20and%20Minsu%20Cho%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20new%20shape-matching%20methodology%2C%20combinative%20matching%2C%0Ato%20combine%20interlocking%20parts%20for%20geometric%20shape%20assembly.%20Previous%20methods%0Afor%20geometric%20assembly%20typically%20rely%20on%20aligning%20parts%20by%20finding%20identical%0Asurfaces%20between%20the%20parts%20as%20in%20conventional%20shape%20matching%20and%20registration.%0AIn%20contrast%2C%20we%20explicitly%20model%20two%20distinct%20properties%20of%20interlocking%0Ashapes%3A%20%27identical%20surface%20shape%27%20and%20%27opposite%20volume%20occupancy.%27%20Our%20method%0Athus%20learns%20to%20establish%20correspondences%20across%20regions%20where%20their%20surface%0Ashapes%20appear%20identical%20but%20their%20volumes%20occupy%20the%20inverted%20space%20to%20each%0Aother.%20To%20facilitate%20this%20process%2C%20we%20also%20learn%20to%20align%20regions%20in%20rotation%0Aby%20estimating%20their%20shape%20orientations%20via%20equivariant%20neural%20networks.%20The%0Aproposed%20approach%20significantly%20reduces%20local%20ambiguities%20in%20matching%20and%0Aallows%20a%20robust%20combination%20of%20parts%20in%20assembly.%20Experimental%20results%20on%0Ageometric%20assembly%20benchmarks%20demonstrate%20the%20efficacy%20of%20our%20method%2C%0Aconsistently%20outperforming%20the%20state%20of%20the%20art.%20Project%20page%3A%0Ahttps%3A//nahyuklee.github.io/cmnet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombinative%2520Matching%2520for%2520Geometric%2520Shape%2520Assembly%26entry.906535625%3DNahyuk%2520Lee%2520and%2520Juhong%2520Min%2520and%2520Junhong%2520Lee%2520and%2520Chunghyun%2520Park%2520and%2520Minsu%2520Cho%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520new%2520shape-matching%2520methodology%252C%2520combinative%2520matching%252C%250Ato%2520combine%2520interlocking%2520parts%2520for%2520geometric%2520shape%2520assembly.%2520Previous%2520methods%250Afor%2520geometric%2520assembly%2520typically%2520rely%2520on%2520aligning%2520parts%2520by%2520finding%2520identical%250Asurfaces%2520between%2520the%2520parts%2520as%2520in%2520conventional%2520shape%2520matching%2520and%2520registration.%250AIn%2520contrast%252C%2520we%2520explicitly%2520model%2520two%2520distinct%2520properties%2520of%2520interlocking%250Ashapes%253A%2520%2527identical%2520surface%2520shape%2527%2520and%2520%2527opposite%2520volume%2520occupancy.%2527%2520Our%2520method%250Athus%2520learns%2520to%2520establish%2520correspondences%2520across%2520regions%2520where%2520their%2520surface%250Ashapes%2520appear%2520identical%2520but%2520their%2520volumes%2520occupy%2520the%2520inverted%2520space%2520to%2520each%250Aother.%2520To%2520facilitate%2520this%2520process%252C%2520we%2520also%2520learn%2520to%2520align%2520regions%2520in%2520rotation%250Aby%2520estimating%2520their%2520shape%2520orientations%2520via%2520equivariant%2520neural%2520networks.%2520The%250Aproposed%2520approach%2520significantly%2520reduces%2520local%2520ambiguities%2520in%2520matching%2520and%250Aallows%2520a%2520robust%2520combination%2520of%2520parts%2520in%2520assembly.%2520Experimental%2520results%2520on%250Ageometric%2520assembly%2520benchmarks%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520method%252C%250Aconsistently%2520outperforming%2520the%2520state%2520of%2520the%2520art.%2520Project%2520page%253A%250Ahttps%253A//nahyuklee.github.io/cmnet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combinative%20Matching%20for%20Geometric%20Shape%20Assembly&entry.906535625=Nahyuk%20Lee%20and%20Juhong%20Min%20and%20Junhong%20Lee%20and%20Chunghyun%20Park%20and%20Minsu%20Cho&entry.1292438233=%20%20This%20paper%20introduces%20a%20new%20shape-matching%20methodology%2C%20combinative%20matching%2C%0Ato%20combine%20interlocking%20parts%20for%20geometric%20shape%20assembly.%20Previous%20methods%0Afor%20geometric%20assembly%20typically%20rely%20on%20aligning%20parts%20by%20finding%20identical%0Asurfaces%20between%20the%20parts%20as%20in%20conventional%20shape%20matching%20and%20registration.%0AIn%20contrast%2C%20we%20explicitly%20model%20two%20distinct%20properties%20of%20interlocking%0Ashapes%3A%20%27identical%20surface%20shape%27%20and%20%27opposite%20volume%20occupancy.%27%20Our%20method%0Athus%20learns%20to%20establish%20correspondences%20across%20regions%20where%20their%20surface%0Ashapes%20appear%20identical%20but%20their%20volumes%20occupy%20the%20inverted%20space%20to%20each%0Aother.%20To%20facilitate%20this%20process%2C%20we%20also%20learn%20to%20align%20regions%20in%20rotation%0Aby%20estimating%20their%20shape%20orientations%20via%20equivariant%20neural%20networks.%20The%0Aproposed%20approach%20significantly%20reduces%20local%20ambiguities%20in%20matching%20and%0Aallows%20a%20robust%20combination%20of%20parts%20in%20assembly.%20Experimental%20results%20on%0Ageometric%20assembly%20benchmarks%20demonstrate%20the%20efficacy%20of%20our%20method%2C%0Aconsistently%20outperforming%20the%20state%20of%20the%20art.%20Project%20page%3A%0Ahttps%3A//nahyuklee.github.io/cmnet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09780v1&entry.124074799=Read"},
{"title": "A Comprehensive Evaluation framework of Alignment Techniques for LLMs", "author": "Muneeza Azmat and Momin Abbas and Maysa Malfiza Garcia de Macedo and Marcelo Carpinette Grave and Luan Soares de Souza and Tiago Machado and Rogerio A de Paula and Raya Horesh and Yixin Chen and Heloisa Caroline de Souza Pereira Candello and Rebecka Nordenlow and Aminat Adebiyi", "abstract": "  As Large Language Models (LLMs) become increasingly integrated into\nreal-world applications, ensuring their outputs align with human values and\nsafety standards has become critical. The field has developed diverse alignment\napproaches including traditional fine-tuning methods (RLHF, instruction\ntuning), post-hoc correction systems, and inference-time interventions, each\nwith distinct advantages and limitations. However, the lack of unified\nevaluation frameworks makes it difficult to systematically compare these\nparadigms and guide deployment decisions. This paper introduces a\nmulti-dimensional evaluation of alignment techniques for LLMs, a comprehensive\nevaluation framework that provides a systematic comparison across all major\nalignment paradigms. Our framework assesses methods along four key dimensions:\nalignment detection, alignment quality, computational efficiency, and\nrobustness. Through experiments across diverse base models and alignment\nstrategies, we demonstrate the utility of our framework in identifying\nstrengths and limitations of current state-of-the-art models, providing\nvaluable insights for future research directions.\n", "link": "http://arxiv.org/abs/2508.09937v1", "date": "2025-08-13", "relevancy": 2.1127, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5306}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5306}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Evaluation%20framework%20of%20Alignment%20Techniques%20for%20LLMs&body=Title%3A%20A%20Comprehensive%20Evaluation%20framework%20of%20Alignment%20Techniques%20for%20LLMs%0AAuthor%3A%20Muneeza%20Azmat%20and%20Momin%20Abbas%20and%20Maysa%20Malfiza%20Garcia%20de%20Macedo%20and%20Marcelo%20Carpinette%20Grave%20and%20Luan%20Soares%20de%20Souza%20and%20Tiago%20Machado%20and%20Rogerio%20A%20de%20Paula%20and%20Raya%20Horesh%20and%20Yixin%20Chen%20and%20Heloisa%20Caroline%20de%20Souza%20Pereira%20Candello%20and%20Rebecka%20Nordenlow%20and%20Aminat%20Adebiyi%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20increasingly%20integrated%20into%0Areal-world%20applications%2C%20ensuring%20their%20outputs%20align%20with%20human%20values%20and%0Asafety%20standards%20has%20become%20critical.%20The%20field%20has%20developed%20diverse%20alignment%0Aapproaches%20including%20traditional%20fine-tuning%20methods%20%28RLHF%2C%20instruction%0Atuning%29%2C%20post-hoc%20correction%20systems%2C%20and%20inference-time%20interventions%2C%20each%0Awith%20distinct%20advantages%20and%20limitations.%20However%2C%20the%20lack%20of%20unified%0Aevaluation%20frameworks%20makes%20it%20difficult%20to%20systematically%20compare%20these%0Aparadigms%20and%20guide%20deployment%20decisions.%20This%20paper%20introduces%20a%0Amulti-dimensional%20evaluation%20of%20alignment%20techniques%20for%20LLMs%2C%20a%20comprehensive%0Aevaluation%20framework%20that%20provides%20a%20systematic%20comparison%20across%20all%20major%0Aalignment%20paradigms.%20Our%20framework%20assesses%20methods%20along%20four%20key%20dimensions%3A%0Aalignment%20detection%2C%20alignment%20quality%2C%20computational%20efficiency%2C%20and%0Arobustness.%20Through%20experiments%20across%20diverse%20base%20models%20and%20alignment%0Astrategies%2C%20we%20demonstrate%20the%20utility%20of%20our%20framework%20in%20identifying%0Astrengths%20and%20limitations%20of%20current%20state-of-the-art%20models%2C%20providing%0Avaluable%20insights%20for%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Evaluation%2520framework%2520of%2520Alignment%2520Techniques%2520for%2520LLMs%26entry.906535625%3DMuneeza%2520Azmat%2520and%2520Momin%2520Abbas%2520and%2520Maysa%2520Malfiza%2520Garcia%2520de%2520Macedo%2520and%2520Marcelo%2520Carpinette%2520Grave%2520and%2520Luan%2520Soares%2520de%2520Souza%2520and%2520Tiago%2520Machado%2520and%2520Rogerio%2520A%2520de%2520Paula%2520and%2520Raya%2520Horesh%2520and%2520Yixin%2520Chen%2520and%2520Heloisa%2520Caroline%2520de%2520Souza%2520Pereira%2520Candello%2520and%2520Rebecka%2520Nordenlow%2520and%2520Aminat%2520Adebiyi%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520become%2520increasingly%2520integrated%2520into%250Areal-world%2520applications%252C%2520ensuring%2520their%2520outputs%2520align%2520with%2520human%2520values%2520and%250Asafety%2520standards%2520has%2520become%2520critical.%2520The%2520field%2520has%2520developed%2520diverse%2520alignment%250Aapproaches%2520including%2520traditional%2520fine-tuning%2520methods%2520%2528RLHF%252C%2520instruction%250Atuning%2529%252C%2520post-hoc%2520correction%2520systems%252C%2520and%2520inference-time%2520interventions%252C%2520each%250Awith%2520distinct%2520advantages%2520and%2520limitations.%2520However%252C%2520the%2520lack%2520of%2520unified%250Aevaluation%2520frameworks%2520makes%2520it%2520difficult%2520to%2520systematically%2520compare%2520these%250Aparadigms%2520and%2520guide%2520deployment%2520decisions.%2520This%2520paper%2520introduces%2520a%250Amulti-dimensional%2520evaluation%2520of%2520alignment%2520techniques%2520for%2520LLMs%252C%2520a%2520comprehensive%250Aevaluation%2520framework%2520that%2520provides%2520a%2520systematic%2520comparison%2520across%2520all%2520major%250Aalignment%2520paradigms.%2520Our%2520framework%2520assesses%2520methods%2520along%2520four%2520key%2520dimensions%253A%250Aalignment%2520detection%252C%2520alignment%2520quality%252C%2520computational%2520efficiency%252C%2520and%250Arobustness.%2520Through%2520experiments%2520across%2520diverse%2520base%2520models%2520and%2520alignment%250Astrategies%252C%2520we%2520demonstrate%2520the%2520utility%2520of%2520our%2520framework%2520in%2520identifying%250Astrengths%2520and%2520limitations%2520of%2520current%2520state-of-the-art%2520models%252C%2520providing%250Avaluable%2520insights%2520for%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Evaluation%20framework%20of%20Alignment%20Techniques%20for%20LLMs&entry.906535625=Muneeza%20Azmat%20and%20Momin%20Abbas%20and%20Maysa%20Malfiza%20Garcia%20de%20Macedo%20and%20Marcelo%20Carpinette%20Grave%20and%20Luan%20Soares%20de%20Souza%20and%20Tiago%20Machado%20and%20Rogerio%20A%20de%20Paula%20and%20Raya%20Horesh%20and%20Yixin%20Chen%20and%20Heloisa%20Caroline%20de%20Souza%20Pereira%20Candello%20and%20Rebecka%20Nordenlow%20and%20Aminat%20Adebiyi&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20increasingly%20integrated%20into%0Areal-world%20applications%2C%20ensuring%20their%20outputs%20align%20with%20human%20values%20and%0Asafety%20standards%20has%20become%20critical.%20The%20field%20has%20developed%20diverse%20alignment%0Aapproaches%20including%20traditional%20fine-tuning%20methods%20%28RLHF%2C%20instruction%0Atuning%29%2C%20post-hoc%20correction%20systems%2C%20and%20inference-time%20interventions%2C%20each%0Awith%20distinct%20advantages%20and%20limitations.%20However%2C%20the%20lack%20of%20unified%0Aevaluation%20frameworks%20makes%20it%20difficult%20to%20systematically%20compare%20these%0Aparadigms%20and%20guide%20deployment%20decisions.%20This%20paper%20introduces%20a%0Amulti-dimensional%20evaluation%20of%20alignment%20techniques%20for%20LLMs%2C%20a%20comprehensive%0Aevaluation%20framework%20that%20provides%20a%20systematic%20comparison%20across%20all%20major%0Aalignment%20paradigms.%20Our%20framework%20assesses%20methods%20along%20four%20key%20dimensions%3A%0Aalignment%20detection%2C%20alignment%20quality%2C%20computational%20efficiency%2C%20and%0Arobustness.%20Through%20experiments%20across%20diverse%20base%20models%20and%20alignment%0Astrategies%2C%20we%20demonstrate%20the%20utility%20of%20our%20framework%20in%20identifying%0Astrengths%20and%20limitations%20of%20current%20state-of-the-art%20models%2C%20providing%0Avaluable%20insights%20for%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09937v1&entry.124074799=Read"},
{"title": "T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement\n  Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and\n  Diagnosis", "author": "Xiaojiao Xiao and Jianfeng Zhao and Qinmin Vivian Hu and Guanghui Wang", "abstract": "  Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of\nliver cancer, significantly improving the classification of the lesion and\npatient outcomes. However, traditional MRI faces challenges including risks\nfrom contrast agent (CA) administration, time-consuming manual assessment, and\nlimited annotated datasets. To address these limitations, we propose a\nTime-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for\nsynthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from\nnon-contrast MRI (NCMRI). T-CACE introduces three core innovations: a\nconditional token encoding (CTE) mechanism that unifies anatomical priors and\ntemporal phase information into latent representations; and a dynamic\ntime-aware attention mask (DTAM) that adaptively modulates inter-phase\ninformation flow using a Gaussian-decayed attention mechanism, ensuring smooth\nand physiologically plausible transitions across phases. Furthermore, a\nconstraint for temporal classification consistency (TCC) aligns the lesion\nclassification output with the evolution of the physiological signal, further\nenhancing diagnostic reliability. Extensive experiments on two independent\nliver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods\nin image synthesis, segmentation, and lesion classification. This framework\noffers a clinically relevant and efficient alternative to traditional\ncontrast-enhanced imaging, improving safety, diagnostic efficiency, and\nreliability for the assessment of liver lesion. The implementation of T-CACE is\npublicly available at: https://github.com/xiaojiao929/T-CACE.\n", "link": "http://arxiv.org/abs/2508.09919v1", "date": "2025-08-13", "relevancy": 2.1051, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5375}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5222}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-CACE%3A%20A%20Time-Conditioned%20Autoregressive%20Contrast%20Enhancement%0A%20%20Multi-Task%20Framework%20for%20Contrast-Free%20Liver%20MRI%20Synthesis%2C%20Segmentation%2C%20and%0A%20%20Diagnosis&body=Title%3A%20T-CACE%3A%20A%20Time-Conditioned%20Autoregressive%20Contrast%20Enhancement%0A%20%20Multi-Task%20Framework%20for%20Contrast-Free%20Liver%20MRI%20Synthesis%2C%20Segmentation%2C%20and%0A%20%20Diagnosis%0AAuthor%3A%20Xiaojiao%20Xiao%20and%20Jianfeng%20Zhao%20and%20Qinmin%20Vivian%20Hu%20and%20Guanghui%20Wang%0AAbstract%3A%20%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20a%20leading%20modality%20for%20the%20diagnosis%20of%0Aliver%20cancer%2C%20significantly%20improving%20the%20classification%20of%20the%20lesion%20and%0Apatient%20outcomes.%20However%2C%20traditional%20MRI%20faces%20challenges%20including%20risks%0Afrom%20contrast%20agent%20%28CA%29%20administration%2C%20time-consuming%20manual%20assessment%2C%20and%0Alimited%20annotated%20datasets.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0ATime-Conditioned%20Autoregressive%20Contrast%20Enhancement%20%28T-CACE%29%20framework%20for%0Asynthesizing%20multi-phase%20contrast-enhanced%20MRI%20%28CEMRI%29%20directly%20from%0Anon-contrast%20MRI%20%28NCMRI%29.%20T-CACE%20introduces%20three%20core%20innovations%3A%20a%0Aconditional%20token%20encoding%20%28CTE%29%20mechanism%20that%20unifies%20anatomical%20priors%20and%0Atemporal%20phase%20information%20into%20latent%20representations%3B%20and%20a%20dynamic%0Atime-aware%20attention%20mask%20%28DTAM%29%20that%20adaptively%20modulates%20inter-phase%0Ainformation%20flow%20using%20a%20Gaussian-decayed%20attention%20mechanism%2C%20ensuring%20smooth%0Aand%20physiologically%20plausible%20transitions%20across%20phases.%20Furthermore%2C%20a%0Aconstraint%20for%20temporal%20classification%20consistency%20%28TCC%29%20aligns%20the%20lesion%0Aclassification%20output%20with%20the%20evolution%20of%20the%20physiological%20signal%2C%20further%0Aenhancing%20diagnostic%20reliability.%20Extensive%20experiments%20on%20two%20independent%0Aliver%20MRI%20datasets%20demonstrate%20that%20T-CACE%20outperforms%20state-of-the-art%20methods%0Ain%20image%20synthesis%2C%20segmentation%2C%20and%20lesion%20classification.%20This%20framework%0Aoffers%20a%20clinically%20relevant%20and%20efficient%20alternative%20to%20traditional%0Acontrast-enhanced%20imaging%2C%20improving%20safety%2C%20diagnostic%20efficiency%2C%20and%0Areliability%20for%20the%20assessment%20of%20liver%20lesion.%20The%20implementation%20of%20T-CACE%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/xiaojiao929/T-CACE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-CACE%253A%2520A%2520Time-Conditioned%2520Autoregressive%2520Contrast%2520Enhancement%250A%2520%2520Multi-Task%2520Framework%2520for%2520Contrast-Free%2520Liver%2520MRI%2520Synthesis%252C%2520Segmentation%252C%2520and%250A%2520%2520Diagnosis%26entry.906535625%3DXiaojiao%2520Xiao%2520and%2520Jianfeng%2520Zhao%2520and%2520Qinmin%2520Vivian%2520Hu%2520and%2520Guanghui%2520Wang%26entry.1292438233%3D%2520%2520Magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520a%2520leading%2520modality%2520for%2520the%2520diagnosis%2520of%250Aliver%2520cancer%252C%2520significantly%2520improving%2520the%2520classification%2520of%2520the%2520lesion%2520and%250Apatient%2520outcomes.%2520However%252C%2520traditional%2520MRI%2520faces%2520challenges%2520including%2520risks%250Afrom%2520contrast%2520agent%2520%2528CA%2529%2520administration%252C%2520time-consuming%2520manual%2520assessment%252C%2520and%250Alimited%2520annotated%2520datasets.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250ATime-Conditioned%2520Autoregressive%2520Contrast%2520Enhancement%2520%2528T-CACE%2529%2520framework%2520for%250Asynthesizing%2520multi-phase%2520contrast-enhanced%2520MRI%2520%2528CEMRI%2529%2520directly%2520from%250Anon-contrast%2520MRI%2520%2528NCMRI%2529.%2520T-CACE%2520introduces%2520three%2520core%2520innovations%253A%2520a%250Aconditional%2520token%2520encoding%2520%2528CTE%2529%2520mechanism%2520that%2520unifies%2520anatomical%2520priors%2520and%250Atemporal%2520phase%2520information%2520into%2520latent%2520representations%253B%2520and%2520a%2520dynamic%250Atime-aware%2520attention%2520mask%2520%2528DTAM%2529%2520that%2520adaptively%2520modulates%2520inter-phase%250Ainformation%2520flow%2520using%2520a%2520Gaussian-decayed%2520attention%2520mechanism%252C%2520ensuring%2520smooth%250Aand%2520physiologically%2520plausible%2520transitions%2520across%2520phases.%2520Furthermore%252C%2520a%250Aconstraint%2520for%2520temporal%2520classification%2520consistency%2520%2528TCC%2529%2520aligns%2520the%2520lesion%250Aclassification%2520output%2520with%2520the%2520evolution%2520of%2520the%2520physiological%2520signal%252C%2520further%250Aenhancing%2520diagnostic%2520reliability.%2520Extensive%2520experiments%2520on%2520two%2520independent%250Aliver%2520MRI%2520datasets%2520demonstrate%2520that%2520T-CACE%2520outperforms%2520state-of-the-art%2520methods%250Ain%2520image%2520synthesis%252C%2520segmentation%252C%2520and%2520lesion%2520classification.%2520This%2520framework%250Aoffers%2520a%2520clinically%2520relevant%2520and%2520efficient%2520alternative%2520to%2520traditional%250Acontrast-enhanced%2520imaging%252C%2520improving%2520safety%252C%2520diagnostic%2520efficiency%252C%2520and%250Areliability%2520for%2520the%2520assessment%2520of%2520liver%2520lesion.%2520The%2520implementation%2520of%2520T-CACE%2520is%250Apublicly%2520available%2520at%253A%2520https%253A//github.com/xiaojiao929/T-CACE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-CACE%3A%20A%20Time-Conditioned%20Autoregressive%20Contrast%20Enhancement%0A%20%20Multi-Task%20Framework%20for%20Contrast-Free%20Liver%20MRI%20Synthesis%2C%20Segmentation%2C%20and%0A%20%20Diagnosis&entry.906535625=Xiaojiao%20Xiao%20and%20Jianfeng%20Zhao%20and%20Qinmin%20Vivian%20Hu%20and%20Guanghui%20Wang&entry.1292438233=%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20a%20leading%20modality%20for%20the%20diagnosis%20of%0Aliver%20cancer%2C%20significantly%20improving%20the%20classification%20of%20the%20lesion%20and%0Apatient%20outcomes.%20However%2C%20traditional%20MRI%20faces%20challenges%20including%20risks%0Afrom%20contrast%20agent%20%28CA%29%20administration%2C%20time-consuming%20manual%20assessment%2C%20and%0Alimited%20annotated%20datasets.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0ATime-Conditioned%20Autoregressive%20Contrast%20Enhancement%20%28T-CACE%29%20framework%20for%0Asynthesizing%20multi-phase%20contrast-enhanced%20MRI%20%28CEMRI%29%20directly%20from%0Anon-contrast%20MRI%20%28NCMRI%29.%20T-CACE%20introduces%20three%20core%20innovations%3A%20a%0Aconditional%20token%20encoding%20%28CTE%29%20mechanism%20that%20unifies%20anatomical%20priors%20and%0Atemporal%20phase%20information%20into%20latent%20representations%3B%20and%20a%20dynamic%0Atime-aware%20attention%20mask%20%28DTAM%29%20that%20adaptively%20modulates%20inter-phase%0Ainformation%20flow%20using%20a%20Gaussian-decayed%20attention%20mechanism%2C%20ensuring%20smooth%0Aand%20physiologically%20plausible%20transitions%20across%20phases.%20Furthermore%2C%20a%0Aconstraint%20for%20temporal%20classification%20consistency%20%28TCC%29%20aligns%20the%20lesion%0Aclassification%20output%20with%20the%20evolution%20of%20the%20physiological%20signal%2C%20further%0Aenhancing%20diagnostic%20reliability.%20Extensive%20experiments%20on%20two%20independent%0Aliver%20MRI%20datasets%20demonstrate%20that%20T-CACE%20outperforms%20state-of-the-art%20methods%0Ain%20image%20synthesis%2C%20segmentation%2C%20and%20lesion%20classification.%20This%20framework%0Aoffers%20a%20clinically%20relevant%20and%20efficient%20alternative%20to%20traditional%0Acontrast-enhanced%20imaging%2C%20improving%20safety%2C%20diagnostic%20efficiency%2C%20and%0Areliability%20for%20the%20assessment%20of%20liver%20lesion.%20The%20implementation%20of%20T-CACE%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/xiaojiao929/T-CACE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09919v1&entry.124074799=Read"},
{"title": "A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist\n  Human Non-Steady Locomotion", "author": "Xiaowei Tan and Weizhong Jiang and Bi Zhang and Wanxin Chen and Yiwen Zhao and Ning Li and Lianqing Liu and Xingang Zhao", "abstract": "  Exoskeletons have been shown to effectively assist humans during steady\nlocomotion. However, their effects on non-steady locomotion, characterized by\nnonlinear phase progression within a gait cycle, remain insufficiently\nexplored, particularly across diverse activities. This work presents a shank\nangle-based control system that enables the exoskeleton to maintain real-time\ncoordination with human gait, even under phase perturbations, while dynamically\nshaping assistance profiles to match the biological ankle moment patterns\nacross walking, running, stair negotiation tasks. The control system consists\nof an assistance profile online generation method and a model-based feedforward\ncontrol method. The assistance profile is formulated as a dual-Gaussian model\nwith the shank angle as the independent variable. Leveraging only IMU\nmeasurements, the model parameters are updated online each stride to adapt to\ninter- and intra-individual biomechanical variability. The profile tracking\ncontrol employs a human-exoskeleton kinematics and stiffness model as a\nfeedforward component, reducing reliance on historical control data due to the\nlack of clear and consistent periodicity in non-steady locomotion. Three\nexperiments were conducted using a lightweight soft exoskeleton with multiple\nsubjects. The results validated the effectiveness of each individual method,\ndemonstrated the robustness of the control system against gait perturbations\nacross various activities, and revealed positive biomechanical and\nphysiological responses of human users to the exoskeleton's mechanical\nassistance.\n", "link": "http://arxiv.org/abs/2508.09876v1", "date": "2025-08-13", "relevancy": 2.1014, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5384}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5238}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Shank%20Angle-Based%20Control%20System%20Enables%20Soft%20Exoskeleton%20to%20Assist%0A%20%20Human%20Non-Steady%20Locomotion&body=Title%3A%20A%20Shank%20Angle-Based%20Control%20System%20Enables%20Soft%20Exoskeleton%20to%20Assist%0A%20%20Human%20Non-Steady%20Locomotion%0AAuthor%3A%20Xiaowei%20Tan%20and%20Weizhong%20Jiang%20and%20Bi%20Zhang%20and%20Wanxin%20Chen%20and%20Yiwen%20Zhao%20and%20Ning%20Li%20and%20Lianqing%20Liu%20and%20Xingang%20Zhao%0AAbstract%3A%20%20%20Exoskeletons%20have%20been%20shown%20to%20effectively%20assist%20humans%20during%20steady%0Alocomotion.%20However%2C%20their%20effects%20on%20non-steady%20locomotion%2C%20characterized%20by%0Anonlinear%20phase%20progression%20within%20a%20gait%20cycle%2C%20remain%20insufficiently%0Aexplored%2C%20particularly%20across%20diverse%20activities.%20This%20work%20presents%20a%20shank%0Aangle-based%20control%20system%20that%20enables%20the%20exoskeleton%20to%20maintain%20real-time%0Acoordination%20with%20human%20gait%2C%20even%20under%20phase%20perturbations%2C%20while%20dynamically%0Ashaping%20assistance%20profiles%20to%20match%20the%20biological%20ankle%20moment%20patterns%0Aacross%20walking%2C%20running%2C%20stair%20negotiation%20tasks.%20The%20control%20system%20consists%0Aof%20an%20assistance%20profile%20online%20generation%20method%20and%20a%20model-based%20feedforward%0Acontrol%20method.%20The%20assistance%20profile%20is%20formulated%20as%20a%20dual-Gaussian%20model%0Awith%20the%20shank%20angle%20as%20the%20independent%20variable.%20Leveraging%20only%20IMU%0Ameasurements%2C%20the%20model%20parameters%20are%20updated%20online%20each%20stride%20to%20adapt%20to%0Ainter-%20and%20intra-individual%20biomechanical%20variability.%20The%20profile%20tracking%0Acontrol%20employs%20a%20human-exoskeleton%20kinematics%20and%20stiffness%20model%20as%20a%0Afeedforward%20component%2C%20reducing%20reliance%20on%20historical%20control%20data%20due%20to%20the%0Alack%20of%20clear%20and%20consistent%20periodicity%20in%20non-steady%20locomotion.%20Three%0Aexperiments%20were%20conducted%20using%20a%20lightweight%20soft%20exoskeleton%20with%20multiple%0Asubjects.%20The%20results%20validated%20the%20effectiveness%20of%20each%20individual%20method%2C%0Ademonstrated%20the%20robustness%20of%20the%20control%20system%20against%20gait%20perturbations%0Aacross%20various%20activities%2C%20and%20revealed%20positive%20biomechanical%20and%0Aphysiological%20responses%20of%20human%20users%20to%20the%20exoskeleton%27s%20mechanical%0Aassistance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Shank%2520Angle-Based%2520Control%2520System%2520Enables%2520Soft%2520Exoskeleton%2520to%2520Assist%250A%2520%2520Human%2520Non-Steady%2520Locomotion%26entry.906535625%3DXiaowei%2520Tan%2520and%2520Weizhong%2520Jiang%2520and%2520Bi%2520Zhang%2520and%2520Wanxin%2520Chen%2520and%2520Yiwen%2520Zhao%2520and%2520Ning%2520Li%2520and%2520Lianqing%2520Liu%2520and%2520Xingang%2520Zhao%26entry.1292438233%3D%2520%2520Exoskeletons%2520have%2520been%2520shown%2520to%2520effectively%2520assist%2520humans%2520during%2520steady%250Alocomotion.%2520However%252C%2520their%2520effects%2520on%2520non-steady%2520locomotion%252C%2520characterized%2520by%250Anonlinear%2520phase%2520progression%2520within%2520a%2520gait%2520cycle%252C%2520remain%2520insufficiently%250Aexplored%252C%2520particularly%2520across%2520diverse%2520activities.%2520This%2520work%2520presents%2520a%2520shank%250Aangle-based%2520control%2520system%2520that%2520enables%2520the%2520exoskeleton%2520to%2520maintain%2520real-time%250Acoordination%2520with%2520human%2520gait%252C%2520even%2520under%2520phase%2520perturbations%252C%2520while%2520dynamically%250Ashaping%2520assistance%2520profiles%2520to%2520match%2520the%2520biological%2520ankle%2520moment%2520patterns%250Aacross%2520walking%252C%2520running%252C%2520stair%2520negotiation%2520tasks.%2520The%2520control%2520system%2520consists%250Aof%2520an%2520assistance%2520profile%2520online%2520generation%2520method%2520and%2520a%2520model-based%2520feedforward%250Acontrol%2520method.%2520The%2520assistance%2520profile%2520is%2520formulated%2520as%2520a%2520dual-Gaussian%2520model%250Awith%2520the%2520shank%2520angle%2520as%2520the%2520independent%2520variable.%2520Leveraging%2520only%2520IMU%250Ameasurements%252C%2520the%2520model%2520parameters%2520are%2520updated%2520online%2520each%2520stride%2520to%2520adapt%2520to%250Ainter-%2520and%2520intra-individual%2520biomechanical%2520variability.%2520The%2520profile%2520tracking%250Acontrol%2520employs%2520a%2520human-exoskeleton%2520kinematics%2520and%2520stiffness%2520model%2520as%2520a%250Afeedforward%2520component%252C%2520reducing%2520reliance%2520on%2520historical%2520control%2520data%2520due%2520to%2520the%250Alack%2520of%2520clear%2520and%2520consistent%2520periodicity%2520in%2520non-steady%2520locomotion.%2520Three%250Aexperiments%2520were%2520conducted%2520using%2520a%2520lightweight%2520soft%2520exoskeleton%2520with%2520multiple%250Asubjects.%2520The%2520results%2520validated%2520the%2520effectiveness%2520of%2520each%2520individual%2520method%252C%250Ademonstrated%2520the%2520robustness%2520of%2520the%2520control%2520system%2520against%2520gait%2520perturbations%250Aacross%2520various%2520activities%252C%2520and%2520revealed%2520positive%2520biomechanical%2520and%250Aphysiological%2520responses%2520of%2520human%2520users%2520to%2520the%2520exoskeleton%2527s%2520mechanical%250Aassistance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Shank%20Angle-Based%20Control%20System%20Enables%20Soft%20Exoskeleton%20to%20Assist%0A%20%20Human%20Non-Steady%20Locomotion&entry.906535625=Xiaowei%20Tan%20and%20Weizhong%20Jiang%20and%20Bi%20Zhang%20and%20Wanxin%20Chen%20and%20Yiwen%20Zhao%20and%20Ning%20Li%20and%20Lianqing%20Liu%20and%20Xingang%20Zhao&entry.1292438233=%20%20Exoskeletons%20have%20been%20shown%20to%20effectively%20assist%20humans%20during%20steady%0Alocomotion.%20However%2C%20their%20effects%20on%20non-steady%20locomotion%2C%20characterized%20by%0Anonlinear%20phase%20progression%20within%20a%20gait%20cycle%2C%20remain%20insufficiently%0Aexplored%2C%20particularly%20across%20diverse%20activities.%20This%20work%20presents%20a%20shank%0Aangle-based%20control%20system%20that%20enables%20the%20exoskeleton%20to%20maintain%20real-time%0Acoordination%20with%20human%20gait%2C%20even%20under%20phase%20perturbations%2C%20while%20dynamically%0Ashaping%20assistance%20profiles%20to%20match%20the%20biological%20ankle%20moment%20patterns%0Aacross%20walking%2C%20running%2C%20stair%20negotiation%20tasks.%20The%20control%20system%20consists%0Aof%20an%20assistance%20profile%20online%20generation%20method%20and%20a%20model-based%20feedforward%0Acontrol%20method.%20The%20assistance%20profile%20is%20formulated%20as%20a%20dual-Gaussian%20model%0Awith%20the%20shank%20angle%20as%20the%20independent%20variable.%20Leveraging%20only%20IMU%0Ameasurements%2C%20the%20model%20parameters%20are%20updated%20online%20each%20stride%20to%20adapt%20to%0Ainter-%20and%20intra-individual%20biomechanical%20variability.%20The%20profile%20tracking%0Acontrol%20employs%20a%20human-exoskeleton%20kinematics%20and%20stiffness%20model%20as%20a%0Afeedforward%20component%2C%20reducing%20reliance%20on%20historical%20control%20data%20due%20to%20the%0Alack%20of%20clear%20and%20consistent%20periodicity%20in%20non-steady%20locomotion.%20Three%0Aexperiments%20were%20conducted%20using%20a%20lightweight%20soft%20exoskeleton%20with%20multiple%0Asubjects.%20The%20results%20validated%20the%20effectiveness%20of%20each%20individual%20method%2C%0Ademonstrated%20the%20robustness%20of%20the%20control%20system%20against%20gait%20perturbations%0Aacross%20various%20activities%2C%20and%20revealed%20positive%20biomechanical%20and%0Aphysiological%20responses%20of%20human%20users%20to%20the%20exoskeleton%27s%20mechanical%0Aassistance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09876v1&entry.124074799=Read"},
{"title": "Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?", "author": "Vittorio Pippi and Konstantina Nikolaidou and Silvia Cascianelli and George Retsinas and Giorgos Sfikas and Rita Cucchiara and Marcus Liwicki", "abstract": "  The digitization of historical manuscripts presents significant challenges\nfor Handwritten Text Recognition (HTR) systems, particularly when dealing with\nsmall, author-specific collections that diverge from the training data\ndistributions. Handwritten Text Generation (HTG) techniques, which generate\nsynthetic data tailored to specific handwriting styles, offer a promising\nsolution to address these challenges. However, the effectiveness of various HTG\nmodels in enhancing HTR performance, especially in low-resource transcription\nsettings, has not been thoroughly evaluated. In this work, we systematically\ncompare three state-of-the-art styled HTG models (representing the generative\nadversarial, diffusion, and autoregressive paradigms for HTG) to assess their\nimpact on HTR fine-tuning. We analyze how visual and linguistic characteristics\nof synthetic data influence fine-tuning outcomes and provide quantitative\nguidelines for selecting the most effective HTG model. The results of our\nanalysis provide insights into the current capabilities of HTG methods and\nhighlight key areas for further improvement in their application to\nlow-resource HTR.\n", "link": "http://arxiv.org/abs/2508.09936v1", "date": "2025-08-13", "relevancy": 2.0953, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5434}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.531}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quo%20Vadis%20Handwritten%20Text%20Generation%20for%20Handwritten%20Text%20Recognition%3F&body=Title%3A%20Quo%20Vadis%20Handwritten%20Text%20Generation%20for%20Handwritten%20Text%20Recognition%3F%0AAuthor%3A%20Vittorio%20Pippi%20and%20Konstantina%20Nikolaidou%20and%20Silvia%20Cascianelli%20and%20George%20Retsinas%20and%20Giorgos%20Sfikas%20and%20Rita%20Cucchiara%20and%20Marcus%20Liwicki%0AAbstract%3A%20%20%20The%20digitization%20of%20historical%20manuscripts%20presents%20significant%20challenges%0Afor%20Handwritten%20Text%20Recognition%20%28HTR%29%20systems%2C%20particularly%20when%20dealing%20with%0Asmall%2C%20author-specific%20collections%20that%20diverge%20from%20the%20training%20data%0Adistributions.%20Handwritten%20Text%20Generation%20%28HTG%29%20techniques%2C%20which%20generate%0Asynthetic%20data%20tailored%20to%20specific%20handwriting%20styles%2C%20offer%20a%20promising%0Asolution%20to%20address%20these%20challenges.%20However%2C%20the%20effectiveness%20of%20various%20HTG%0Amodels%20in%20enhancing%20HTR%20performance%2C%20especially%20in%20low-resource%20transcription%0Asettings%2C%20has%20not%20been%20thoroughly%20evaluated.%20In%20this%20work%2C%20we%20systematically%0Acompare%20three%20state-of-the-art%20styled%20HTG%20models%20%28representing%20the%20generative%0Aadversarial%2C%20diffusion%2C%20and%20autoregressive%20paradigms%20for%20HTG%29%20to%20assess%20their%0Aimpact%20on%20HTR%20fine-tuning.%20We%20analyze%20how%20visual%20and%20linguistic%20characteristics%0Aof%20synthetic%20data%20influence%20fine-tuning%20outcomes%20and%20provide%20quantitative%0Aguidelines%20for%20selecting%20the%20most%20effective%20HTG%20model.%20The%20results%20of%20our%0Aanalysis%20provide%20insights%20into%20the%20current%20capabilities%20of%20HTG%20methods%20and%0Ahighlight%20key%20areas%20for%20further%20improvement%20in%20their%20application%20to%0Alow-resource%20HTR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuo%2520Vadis%2520Handwritten%2520Text%2520Generation%2520for%2520Handwritten%2520Text%2520Recognition%253F%26entry.906535625%3DVittorio%2520Pippi%2520and%2520Konstantina%2520Nikolaidou%2520and%2520Silvia%2520Cascianelli%2520and%2520George%2520Retsinas%2520and%2520Giorgos%2520Sfikas%2520and%2520Rita%2520Cucchiara%2520and%2520Marcus%2520Liwicki%26entry.1292438233%3D%2520%2520The%2520digitization%2520of%2520historical%2520manuscripts%2520presents%2520significant%2520challenges%250Afor%2520Handwritten%2520Text%2520Recognition%2520%2528HTR%2529%2520systems%252C%2520particularly%2520when%2520dealing%2520with%250Asmall%252C%2520author-specific%2520collections%2520that%2520diverge%2520from%2520the%2520training%2520data%250Adistributions.%2520Handwritten%2520Text%2520Generation%2520%2528HTG%2529%2520techniques%252C%2520which%2520generate%250Asynthetic%2520data%2520tailored%2520to%2520specific%2520handwriting%2520styles%252C%2520offer%2520a%2520promising%250Asolution%2520to%2520address%2520these%2520challenges.%2520However%252C%2520the%2520effectiveness%2520of%2520various%2520HTG%250Amodels%2520in%2520enhancing%2520HTR%2520performance%252C%2520especially%2520in%2520low-resource%2520transcription%250Asettings%252C%2520has%2520not%2520been%2520thoroughly%2520evaluated.%2520In%2520this%2520work%252C%2520we%2520systematically%250Acompare%2520three%2520state-of-the-art%2520styled%2520HTG%2520models%2520%2528representing%2520the%2520generative%250Aadversarial%252C%2520diffusion%252C%2520and%2520autoregressive%2520paradigms%2520for%2520HTG%2529%2520to%2520assess%2520their%250Aimpact%2520on%2520HTR%2520fine-tuning.%2520We%2520analyze%2520how%2520visual%2520and%2520linguistic%2520characteristics%250Aof%2520synthetic%2520data%2520influence%2520fine-tuning%2520outcomes%2520and%2520provide%2520quantitative%250Aguidelines%2520for%2520selecting%2520the%2520most%2520effective%2520HTG%2520model.%2520The%2520results%2520of%2520our%250Aanalysis%2520provide%2520insights%2520into%2520the%2520current%2520capabilities%2520of%2520HTG%2520methods%2520and%250Ahighlight%2520key%2520areas%2520for%2520further%2520improvement%2520in%2520their%2520application%2520to%250Alow-resource%2520HTR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quo%20Vadis%20Handwritten%20Text%20Generation%20for%20Handwritten%20Text%20Recognition%3F&entry.906535625=Vittorio%20Pippi%20and%20Konstantina%20Nikolaidou%20and%20Silvia%20Cascianelli%20and%20George%20Retsinas%20and%20Giorgos%20Sfikas%20and%20Rita%20Cucchiara%20and%20Marcus%20Liwicki&entry.1292438233=%20%20The%20digitization%20of%20historical%20manuscripts%20presents%20significant%20challenges%0Afor%20Handwritten%20Text%20Recognition%20%28HTR%29%20systems%2C%20particularly%20when%20dealing%20with%0Asmall%2C%20author-specific%20collections%20that%20diverge%20from%20the%20training%20data%0Adistributions.%20Handwritten%20Text%20Generation%20%28HTG%29%20techniques%2C%20which%20generate%0Asynthetic%20data%20tailored%20to%20specific%20handwriting%20styles%2C%20offer%20a%20promising%0Asolution%20to%20address%20these%20challenges.%20However%2C%20the%20effectiveness%20of%20various%20HTG%0Amodels%20in%20enhancing%20HTR%20performance%2C%20especially%20in%20low-resource%20transcription%0Asettings%2C%20has%20not%20been%20thoroughly%20evaluated.%20In%20this%20work%2C%20we%20systematically%0Acompare%20three%20state-of-the-art%20styled%20HTG%20models%20%28representing%20the%20generative%0Aadversarial%2C%20diffusion%2C%20and%20autoregressive%20paradigms%20for%20HTG%29%20to%20assess%20their%0Aimpact%20on%20HTR%20fine-tuning.%20We%20analyze%20how%20visual%20and%20linguistic%20characteristics%0Aof%20synthetic%20data%20influence%20fine-tuning%20outcomes%20and%20provide%20quantitative%0Aguidelines%20for%20selecting%20the%20most%20effective%20HTG%20model.%20The%20results%20of%20our%0Aanalysis%20provide%20insights%20into%20the%20current%20capabilities%20of%20HTG%20methods%20and%0Ahighlight%20key%20areas%20for%20further%20improvement%20in%20their%20application%20to%0Alow-resource%20HTR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09936v1&entry.124074799=Read"},
{"title": "Poaching Hotspot Identification Using Satellite Imagery", "author": "Aryan Pandhi and Shrey Baid and Sanjali Jha", "abstract": "  Elephant Poaching in African countries has been a decade-old problem. So much\nso that African Forest Elephants are now listed as an endangered species, and\nAfrican Savannah Elephants as critically endangered by the IUCN (International\nUnion for Conservation of Nature). [1] Elephants are hunted primarily for their\nivory tusks which caused many elephants to be born tuskless as a genetic\nmodification for survival. [2] Data gathered by recent studies shows that\nthough poaching methods remain the same, the poaching grounds are rather\ndynamic. Poachers have shifted to areas with less ranger patrols and several\nother factors like watering holes, seasons, altitude etc. cause constant shifts\nin poaching hotspot locations. [3] After a period of low poaching from\n2000-2014, poaching numbers in African countries are now on the rise again --\nWWF (World Wildlife Foundation) says there are 20,000 elephants poached\nannually [4]. In African countries, anti-poaching efforts are concentrated near\ntowns, while a majority of poaching occurs in the deserted regions. All of\nthese factors result in the need for a Computer Vision Model to identify\npoaching hotspots through locating the geographic indicators of favorable\npoaching regions. A CV model eliminates the need to manually track poachers and\naccount for the environmental factors to deploy resources and its combination\nwith satellite imagery allows us to survey large areas without disturbing local\nspecies or cross border aviation restrictions.\n", "link": "http://arxiv.org/abs/2508.09812v1", "date": "2025-08-13", "relevancy": 2.0933, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4547}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Poaching%20Hotspot%20Identification%20Using%20Satellite%20Imagery&body=Title%3A%20Poaching%20Hotspot%20Identification%20Using%20Satellite%20Imagery%0AAuthor%3A%20Aryan%20Pandhi%20and%20Shrey%20Baid%20and%20Sanjali%20Jha%0AAbstract%3A%20%20%20Elephant%20Poaching%20in%20African%20countries%20has%20been%20a%20decade-old%20problem.%20So%20much%0Aso%20that%20African%20Forest%20Elephants%20are%20now%20listed%20as%20an%20endangered%20species%2C%20and%0AAfrican%20Savannah%20Elephants%20as%20critically%20endangered%20by%20the%20IUCN%20%28International%0AUnion%20for%20Conservation%20of%20Nature%29.%20%5B1%5D%20Elephants%20are%20hunted%20primarily%20for%20their%0Aivory%20tusks%20which%20caused%20many%20elephants%20to%20be%20born%20tuskless%20as%20a%20genetic%0Amodification%20for%20survival.%20%5B2%5D%20Data%20gathered%20by%20recent%20studies%20shows%20that%0Athough%20poaching%20methods%20remain%20the%20same%2C%20the%20poaching%20grounds%20are%20rather%0Adynamic.%20Poachers%20have%20shifted%20to%20areas%20with%20less%20ranger%20patrols%20and%20several%0Aother%20factors%20like%20watering%20holes%2C%20seasons%2C%20altitude%20etc.%20cause%20constant%20shifts%0Ain%20poaching%20hotspot%20locations.%20%5B3%5D%20After%20a%20period%20of%20low%20poaching%20from%0A2000-2014%2C%20poaching%20numbers%20in%20African%20countries%20are%20now%20on%20the%20rise%20again%20--%0AWWF%20%28World%20Wildlife%20Foundation%29%20says%20there%20are%2020%2C000%20elephants%20poached%0Aannually%20%5B4%5D.%20In%20African%20countries%2C%20anti-poaching%20efforts%20are%20concentrated%20near%0Atowns%2C%20while%20a%20majority%20of%20poaching%20occurs%20in%20the%20deserted%20regions.%20All%20of%0Athese%20factors%20result%20in%20the%20need%20for%20a%20Computer%20Vision%20Model%20to%20identify%0Apoaching%20hotspots%20through%20locating%20the%20geographic%20indicators%20of%20favorable%0Apoaching%20regions.%20A%20CV%20model%20eliminates%20the%20need%20to%20manually%20track%20poachers%20and%0Aaccount%20for%20the%20environmental%20factors%20to%20deploy%20resources%20and%20its%20combination%0Awith%20satellite%20imagery%20allows%20us%20to%20survey%20large%20areas%20without%20disturbing%20local%0Aspecies%20or%20cross%20border%20aviation%20restrictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoaching%2520Hotspot%2520Identification%2520Using%2520Satellite%2520Imagery%26entry.906535625%3DAryan%2520Pandhi%2520and%2520Shrey%2520Baid%2520and%2520Sanjali%2520Jha%26entry.1292438233%3D%2520%2520Elephant%2520Poaching%2520in%2520African%2520countries%2520has%2520been%2520a%2520decade-old%2520problem.%2520So%2520much%250Aso%2520that%2520African%2520Forest%2520Elephants%2520are%2520now%2520listed%2520as%2520an%2520endangered%2520species%252C%2520and%250AAfrican%2520Savannah%2520Elephants%2520as%2520critically%2520endangered%2520by%2520the%2520IUCN%2520%2528International%250AUnion%2520for%2520Conservation%2520of%2520Nature%2529.%2520%255B1%255D%2520Elephants%2520are%2520hunted%2520primarily%2520for%2520their%250Aivory%2520tusks%2520which%2520caused%2520many%2520elephants%2520to%2520be%2520born%2520tuskless%2520as%2520a%2520genetic%250Amodification%2520for%2520survival.%2520%255B2%255D%2520Data%2520gathered%2520by%2520recent%2520studies%2520shows%2520that%250Athough%2520poaching%2520methods%2520remain%2520the%2520same%252C%2520the%2520poaching%2520grounds%2520are%2520rather%250Adynamic.%2520Poachers%2520have%2520shifted%2520to%2520areas%2520with%2520less%2520ranger%2520patrols%2520and%2520several%250Aother%2520factors%2520like%2520watering%2520holes%252C%2520seasons%252C%2520altitude%2520etc.%2520cause%2520constant%2520shifts%250Ain%2520poaching%2520hotspot%2520locations.%2520%255B3%255D%2520After%2520a%2520period%2520of%2520low%2520poaching%2520from%250A2000-2014%252C%2520poaching%2520numbers%2520in%2520African%2520countries%2520are%2520now%2520on%2520the%2520rise%2520again%2520--%250AWWF%2520%2528World%2520Wildlife%2520Foundation%2529%2520says%2520there%2520are%252020%252C000%2520elephants%2520poached%250Aannually%2520%255B4%255D.%2520In%2520African%2520countries%252C%2520anti-poaching%2520efforts%2520are%2520concentrated%2520near%250Atowns%252C%2520while%2520a%2520majority%2520of%2520poaching%2520occurs%2520in%2520the%2520deserted%2520regions.%2520All%2520of%250Athese%2520factors%2520result%2520in%2520the%2520need%2520for%2520a%2520Computer%2520Vision%2520Model%2520to%2520identify%250Apoaching%2520hotspots%2520through%2520locating%2520the%2520geographic%2520indicators%2520of%2520favorable%250Apoaching%2520regions.%2520A%2520CV%2520model%2520eliminates%2520the%2520need%2520to%2520manually%2520track%2520poachers%2520and%250Aaccount%2520for%2520the%2520environmental%2520factors%2520to%2520deploy%2520resources%2520and%2520its%2520combination%250Awith%2520satellite%2520imagery%2520allows%2520us%2520to%2520survey%2520large%2520areas%2520without%2520disturbing%2520local%250Aspecies%2520or%2520cross%2520border%2520aviation%2520restrictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poaching%20Hotspot%20Identification%20Using%20Satellite%20Imagery&entry.906535625=Aryan%20Pandhi%20and%20Shrey%20Baid%20and%20Sanjali%20Jha&entry.1292438233=%20%20Elephant%20Poaching%20in%20African%20countries%20has%20been%20a%20decade-old%20problem.%20So%20much%0Aso%20that%20African%20Forest%20Elephants%20are%20now%20listed%20as%20an%20endangered%20species%2C%20and%0AAfrican%20Savannah%20Elephants%20as%20critically%20endangered%20by%20the%20IUCN%20%28International%0AUnion%20for%20Conservation%20of%20Nature%29.%20%5B1%5D%20Elephants%20are%20hunted%20primarily%20for%20their%0Aivory%20tusks%20which%20caused%20many%20elephants%20to%20be%20born%20tuskless%20as%20a%20genetic%0Amodification%20for%20survival.%20%5B2%5D%20Data%20gathered%20by%20recent%20studies%20shows%20that%0Athough%20poaching%20methods%20remain%20the%20same%2C%20the%20poaching%20grounds%20are%20rather%0Adynamic.%20Poachers%20have%20shifted%20to%20areas%20with%20less%20ranger%20patrols%20and%20several%0Aother%20factors%20like%20watering%20holes%2C%20seasons%2C%20altitude%20etc.%20cause%20constant%20shifts%0Ain%20poaching%20hotspot%20locations.%20%5B3%5D%20After%20a%20period%20of%20low%20poaching%20from%0A2000-2014%2C%20poaching%20numbers%20in%20African%20countries%20are%20now%20on%20the%20rise%20again%20--%0AWWF%20%28World%20Wildlife%20Foundation%29%20says%20there%20are%2020%2C000%20elephants%20poached%0Aannually%20%5B4%5D.%20In%20African%20countries%2C%20anti-poaching%20efforts%20are%20concentrated%20near%0Atowns%2C%20while%20a%20majority%20of%20poaching%20occurs%20in%20the%20deserted%20regions.%20All%20of%0Athese%20factors%20result%20in%20the%20need%20for%20a%20Computer%20Vision%20Model%20to%20identify%0Apoaching%20hotspots%20through%20locating%20the%20geographic%20indicators%20of%20favorable%0Apoaching%20regions.%20A%20CV%20model%20eliminates%20the%20need%20to%20manually%20track%20poachers%20and%0Aaccount%20for%20the%20environmental%20factors%20to%20deploy%20resources%20and%20its%20combination%0Awith%20satellite%20imagery%20allows%20us%20to%20survey%20large%20areas%20without%20disturbing%20local%0Aspecies%20or%20cross%20border%20aviation%20restrictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09812v1&entry.124074799=Read"},
{"title": "Beyond Na\u00efve Prompting: Strategies for Improved Zero-shot\n  Context-aided Forecasting with LLMs", "author": "Arjun Ashok and Andrew Robert Williams and Vincent Zhihao Zheng and Irina Rish and Nicolas Chapados and \u00c9tienne Marcotte and Valentina Zantedeschi and Alexandre Drouin", "abstract": "  Forecasting in real-world settings requires models to integrate not only\nhistorical data but also relevant contextual information, often available in\ntextual form. While recent work has shown that large language models (LLMs) can\nbe effective context-aided forecasters via na\\\"ive direct prompting, their full\npotential remains underexplored. We address this gap with 4 strategies,\nproviding new insights into the zero-shot capabilities of LLMs in this setting.\nReDP improves interpretability by eliciting explicit reasoning traces, allowing\nus to assess the model's reasoning over the context independently from its\nforecast accuracy. CorDP leverages LLMs solely to refine existing forecasts\nwith context, enhancing their applicability in real-world forecasting\npipelines. IC-DP proposes embedding historical examples of context-aided\nforecasting tasks in the prompt, substantially improving accuracy even for the\nlargest models. Finally, RouteDP optimizes resource efficiency by using LLMs to\nestimate task difficulty, and routing the most challenging tasks to larger\nmodels. Evaluated on different kinds of context-aided forecasting tasks from\nthe CiK benchmark, our strategies demonstrate distinct benefits over na\\\"ive\nprompting across LLMs of different sizes and families. These results open the\ndoor to further simple yet effective improvements in LLM-based context-aided\nforecasting.\n", "link": "http://arxiv.org/abs/2508.09904v1", "date": "2025-08-13", "relevancy": 2.0901, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5281}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5281}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Na%C3%AFve%20Prompting%3A%20Strategies%20for%20Improved%20Zero-shot%0A%20%20Context-aided%20Forecasting%20with%20LLMs&body=Title%3A%20Beyond%20Na%C3%AFve%20Prompting%3A%20Strategies%20for%20Improved%20Zero-shot%0A%20%20Context-aided%20Forecasting%20with%20LLMs%0AAuthor%3A%20Arjun%20Ashok%20and%20Andrew%20Robert%20Williams%20and%20Vincent%20Zhihao%20Zheng%20and%20Irina%20Rish%20and%20Nicolas%20Chapados%20and%20%C3%89tienne%20Marcotte%20and%20Valentina%20Zantedeschi%20and%20Alexandre%20Drouin%0AAbstract%3A%20%20%20Forecasting%20in%20real-world%20settings%20requires%20models%20to%20integrate%20not%20only%0Ahistorical%20data%20but%20also%20relevant%20contextual%20information%2C%20often%20available%20in%0Atextual%20form.%20While%20recent%20work%20has%20shown%20that%20large%20language%20models%20%28LLMs%29%20can%0Abe%20effective%20context-aided%20forecasters%20via%20na%5C%22ive%20direct%20prompting%2C%20their%20full%0Apotential%20remains%20underexplored.%20We%20address%20this%20gap%20with%204%20strategies%2C%0Aproviding%20new%20insights%20into%20the%20zero-shot%20capabilities%20of%20LLMs%20in%20this%20setting.%0AReDP%20improves%20interpretability%20by%20eliciting%20explicit%20reasoning%20traces%2C%20allowing%0Aus%20to%20assess%20the%20model%27s%20reasoning%20over%20the%20context%20independently%20from%20its%0Aforecast%20accuracy.%20CorDP%20leverages%20LLMs%20solely%20to%20refine%20existing%20forecasts%0Awith%20context%2C%20enhancing%20their%20applicability%20in%20real-world%20forecasting%0Apipelines.%20IC-DP%20proposes%20embedding%20historical%20examples%20of%20context-aided%0Aforecasting%20tasks%20in%20the%20prompt%2C%20substantially%20improving%20accuracy%20even%20for%20the%0Alargest%20models.%20Finally%2C%20RouteDP%20optimizes%20resource%20efficiency%20by%20using%20LLMs%20to%0Aestimate%20task%20difficulty%2C%20and%20routing%20the%20most%20challenging%20tasks%20to%20larger%0Amodels.%20Evaluated%20on%20different%20kinds%20of%20context-aided%20forecasting%20tasks%20from%0Athe%20CiK%20benchmark%2C%20our%20strategies%20demonstrate%20distinct%20benefits%20over%20na%5C%22ive%0Aprompting%20across%20LLMs%20of%20different%20sizes%20and%20families.%20These%20results%20open%20the%0Adoor%20to%20further%20simple%20yet%20effective%20improvements%20in%20LLM-based%20context-aided%0Aforecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Na%25C3%25AFve%2520Prompting%253A%2520Strategies%2520for%2520Improved%2520Zero-shot%250A%2520%2520Context-aided%2520Forecasting%2520with%2520LLMs%26entry.906535625%3DArjun%2520Ashok%2520and%2520Andrew%2520Robert%2520Williams%2520and%2520Vincent%2520Zhihao%2520Zheng%2520and%2520Irina%2520Rish%2520and%2520Nicolas%2520Chapados%2520and%2520%25C3%2589tienne%2520Marcotte%2520and%2520Valentina%2520Zantedeschi%2520and%2520Alexandre%2520Drouin%26entry.1292438233%3D%2520%2520Forecasting%2520in%2520real-world%2520settings%2520requires%2520models%2520to%2520integrate%2520not%2520only%250Ahistorical%2520data%2520but%2520also%2520relevant%2520contextual%2520information%252C%2520often%2520available%2520in%250Atextual%2520form.%2520While%2520recent%2520work%2520has%2520shown%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%250Abe%2520effective%2520context-aided%2520forecasters%2520via%2520na%255C%2522ive%2520direct%2520prompting%252C%2520their%2520full%250Apotential%2520remains%2520underexplored.%2520We%2520address%2520this%2520gap%2520with%25204%2520strategies%252C%250Aproviding%2520new%2520insights%2520into%2520the%2520zero-shot%2520capabilities%2520of%2520LLMs%2520in%2520this%2520setting.%250AReDP%2520improves%2520interpretability%2520by%2520eliciting%2520explicit%2520reasoning%2520traces%252C%2520allowing%250Aus%2520to%2520assess%2520the%2520model%2527s%2520reasoning%2520over%2520the%2520context%2520independently%2520from%2520its%250Aforecast%2520accuracy.%2520CorDP%2520leverages%2520LLMs%2520solely%2520to%2520refine%2520existing%2520forecasts%250Awith%2520context%252C%2520enhancing%2520their%2520applicability%2520in%2520real-world%2520forecasting%250Apipelines.%2520IC-DP%2520proposes%2520embedding%2520historical%2520examples%2520of%2520context-aided%250Aforecasting%2520tasks%2520in%2520the%2520prompt%252C%2520substantially%2520improving%2520accuracy%2520even%2520for%2520the%250Alargest%2520models.%2520Finally%252C%2520RouteDP%2520optimizes%2520resource%2520efficiency%2520by%2520using%2520LLMs%2520to%250Aestimate%2520task%2520difficulty%252C%2520and%2520routing%2520the%2520most%2520challenging%2520tasks%2520to%2520larger%250Amodels.%2520Evaluated%2520on%2520different%2520kinds%2520of%2520context-aided%2520forecasting%2520tasks%2520from%250Athe%2520CiK%2520benchmark%252C%2520our%2520strategies%2520demonstrate%2520distinct%2520benefits%2520over%2520na%255C%2522ive%250Aprompting%2520across%2520LLMs%2520of%2520different%2520sizes%2520and%2520families.%2520These%2520results%2520open%2520the%250Adoor%2520to%2520further%2520simple%2520yet%2520effective%2520improvements%2520in%2520LLM-based%2520context-aided%250Aforecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Na%C3%AFve%20Prompting%3A%20Strategies%20for%20Improved%20Zero-shot%0A%20%20Context-aided%20Forecasting%20with%20LLMs&entry.906535625=Arjun%20Ashok%20and%20Andrew%20Robert%20Williams%20and%20Vincent%20Zhihao%20Zheng%20and%20Irina%20Rish%20and%20Nicolas%20Chapados%20and%20%C3%89tienne%20Marcotte%20and%20Valentina%20Zantedeschi%20and%20Alexandre%20Drouin&entry.1292438233=%20%20Forecasting%20in%20real-world%20settings%20requires%20models%20to%20integrate%20not%20only%0Ahistorical%20data%20but%20also%20relevant%20contextual%20information%2C%20often%20available%20in%0Atextual%20form.%20While%20recent%20work%20has%20shown%20that%20large%20language%20models%20%28LLMs%29%20can%0Abe%20effective%20context-aided%20forecasters%20via%20na%5C%22ive%20direct%20prompting%2C%20their%20full%0Apotential%20remains%20underexplored.%20We%20address%20this%20gap%20with%204%20strategies%2C%0Aproviding%20new%20insights%20into%20the%20zero-shot%20capabilities%20of%20LLMs%20in%20this%20setting.%0AReDP%20improves%20interpretability%20by%20eliciting%20explicit%20reasoning%20traces%2C%20allowing%0Aus%20to%20assess%20the%20model%27s%20reasoning%20over%20the%20context%20independently%20from%20its%0Aforecast%20accuracy.%20CorDP%20leverages%20LLMs%20solely%20to%20refine%20existing%20forecasts%0Awith%20context%2C%20enhancing%20their%20applicability%20in%20real-world%20forecasting%0Apipelines.%20IC-DP%20proposes%20embedding%20historical%20examples%20of%20context-aided%0Aforecasting%20tasks%20in%20the%20prompt%2C%20substantially%20improving%20accuracy%20even%20for%20the%0Alargest%20models.%20Finally%2C%20RouteDP%20optimizes%20resource%20efficiency%20by%20using%20LLMs%20to%0Aestimate%20task%20difficulty%2C%20and%20routing%20the%20most%20challenging%20tasks%20to%20larger%0Amodels.%20Evaluated%20on%20different%20kinds%20of%20context-aided%20forecasting%20tasks%20from%0Athe%20CiK%20benchmark%2C%20our%20strategies%20demonstrate%20distinct%20benefits%20over%20na%5C%22ive%0Aprompting%20across%20LLMs%20of%20different%20sizes%20and%20families.%20These%20results%20open%20the%0Adoor%20to%20further%20simple%20yet%20effective%20improvements%20in%20LLM-based%20context-aided%0Aforecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09904v1&entry.124074799=Read"},
{"title": "Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models", "author": "Anish Narain and Ritam Majumdar and Nikita Narayanan and Dominic Marshall and Sonali Parbhoo", "abstract": "  Large, publicly available clinical datasets have emerged as a novel resource\nfor understanding disease heterogeneity and to explore personalization of\ntherapy. These datasets are derived from data not originally collected for\nresearch purposes and, as a result, are often incomplete and lack critical\nlabels. Many AI tools have been developed to retrospectively label these\ndatasets, such as by performing disease classification; however, they often\nsuffer from limited interpretability. Previous work has attempted to explain\npredictions using Concept Bottleneck Models (CBMs), which learn interpretable\nconcepts that map to higher-level clinical ideas, facilitating human\nevaluation. However, these models often experience performance limitations when\nthe concepts fail to adequately explain or characterize the task. We use the\nidentification of Acute Respiratory Distress Syndrome (ARDS) as a challenging\ntest case to demonstrate the value of incorporating contextual information from\nclinical notes to improve CBM performance. Our approach leverages a Large\nLanguage Model (LLM) to process clinical notes and generate additional\nconcepts, resulting in a 10% performance gain over existing methods.\nAdditionally, it facilitates the learning of more comprehensive concepts,\nthereby reducing the risk of information leakage and reliance on spurious\nshortcuts, thus improving the characterization of ARDS.\n", "link": "http://arxiv.org/abs/2508.09719v1", "date": "2025-08-13", "relevancy": 2.0762, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20ARDS%20Diagnosis%20Through%20Context-Aware%20Concept%20Bottleneck%20Models&body=Title%3A%20Improving%20ARDS%20Diagnosis%20Through%20Context-Aware%20Concept%20Bottleneck%20Models%0AAuthor%3A%20Anish%20Narain%20and%20Ritam%20Majumdar%20and%20Nikita%20Narayanan%20and%20Dominic%20Marshall%20and%20Sonali%20Parbhoo%0AAbstract%3A%20%20%20Large%2C%20publicly%20available%20clinical%20datasets%20have%20emerged%20as%20a%20novel%20resource%0Afor%20understanding%20disease%20heterogeneity%20and%20to%20explore%20personalization%20of%0Atherapy.%20These%20datasets%20are%20derived%20from%20data%20not%20originally%20collected%20for%0Aresearch%20purposes%20and%2C%20as%20a%20result%2C%20are%20often%20incomplete%20and%20lack%20critical%0Alabels.%20Many%20AI%20tools%20have%20been%20developed%20to%20retrospectively%20label%20these%0Adatasets%2C%20such%20as%20by%20performing%20disease%20classification%3B%20however%2C%20they%20often%0Asuffer%20from%20limited%20interpretability.%20Previous%20work%20has%20attempted%20to%20explain%0Apredictions%20using%20Concept%20Bottleneck%20Models%20%28CBMs%29%2C%20which%20learn%20interpretable%0Aconcepts%20that%20map%20to%20higher-level%20clinical%20ideas%2C%20facilitating%20human%0Aevaluation.%20However%2C%20these%20models%20often%20experience%20performance%20limitations%20when%0Athe%20concepts%20fail%20to%20adequately%20explain%20or%20characterize%20the%20task.%20We%20use%20the%0Aidentification%20of%20Acute%20Respiratory%20Distress%20Syndrome%20%28ARDS%29%20as%20a%20challenging%0Atest%20case%20to%20demonstrate%20the%20value%20of%20incorporating%20contextual%20information%20from%0Aclinical%20notes%20to%20improve%20CBM%20performance.%20Our%20approach%20leverages%20a%20Large%0ALanguage%20Model%20%28LLM%29%20to%20process%20clinical%20notes%20and%20generate%20additional%0Aconcepts%2C%20resulting%20in%20a%2010%25%20performance%20gain%20over%20existing%20methods.%0AAdditionally%2C%20it%20facilitates%20the%20learning%20of%20more%20comprehensive%20concepts%2C%0Athereby%20reducing%20the%20risk%20of%20information%20leakage%20and%20reliance%20on%20spurious%0Ashortcuts%2C%20thus%20improving%20the%20characterization%20of%20ARDS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520ARDS%2520Diagnosis%2520Through%2520Context-Aware%2520Concept%2520Bottleneck%2520Models%26entry.906535625%3DAnish%2520Narain%2520and%2520Ritam%2520Majumdar%2520and%2520Nikita%2520Narayanan%2520and%2520Dominic%2520Marshall%2520and%2520Sonali%2520Parbhoo%26entry.1292438233%3D%2520%2520Large%252C%2520publicly%2520available%2520clinical%2520datasets%2520have%2520emerged%2520as%2520a%2520novel%2520resource%250Afor%2520understanding%2520disease%2520heterogeneity%2520and%2520to%2520explore%2520personalization%2520of%250Atherapy.%2520These%2520datasets%2520are%2520derived%2520from%2520data%2520not%2520originally%2520collected%2520for%250Aresearch%2520purposes%2520and%252C%2520as%2520a%2520result%252C%2520are%2520often%2520incomplete%2520and%2520lack%2520critical%250Alabels.%2520Many%2520AI%2520tools%2520have%2520been%2520developed%2520to%2520retrospectively%2520label%2520these%250Adatasets%252C%2520such%2520as%2520by%2520performing%2520disease%2520classification%253B%2520however%252C%2520they%2520often%250Asuffer%2520from%2520limited%2520interpretability.%2520Previous%2520work%2520has%2520attempted%2520to%2520explain%250Apredictions%2520using%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%252C%2520which%2520learn%2520interpretable%250Aconcepts%2520that%2520map%2520to%2520higher-level%2520clinical%2520ideas%252C%2520facilitating%2520human%250Aevaluation.%2520However%252C%2520these%2520models%2520often%2520experience%2520performance%2520limitations%2520when%250Athe%2520concepts%2520fail%2520to%2520adequately%2520explain%2520or%2520characterize%2520the%2520task.%2520We%2520use%2520the%250Aidentification%2520of%2520Acute%2520Respiratory%2520Distress%2520Syndrome%2520%2528ARDS%2529%2520as%2520a%2520challenging%250Atest%2520case%2520to%2520demonstrate%2520the%2520value%2520of%2520incorporating%2520contextual%2520information%2520from%250Aclinical%2520notes%2520to%2520improve%2520CBM%2520performance.%2520Our%2520approach%2520leverages%2520a%2520Large%250ALanguage%2520Model%2520%2528LLM%2529%2520to%2520process%2520clinical%2520notes%2520and%2520generate%2520additional%250Aconcepts%252C%2520resulting%2520in%2520a%252010%2525%2520performance%2520gain%2520over%2520existing%2520methods.%250AAdditionally%252C%2520it%2520facilitates%2520the%2520learning%2520of%2520more%2520comprehensive%2520concepts%252C%250Athereby%2520reducing%2520the%2520risk%2520of%2520information%2520leakage%2520and%2520reliance%2520on%2520spurious%250Ashortcuts%252C%2520thus%2520improving%2520the%2520characterization%2520of%2520ARDS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20ARDS%20Diagnosis%20Through%20Context-Aware%20Concept%20Bottleneck%20Models&entry.906535625=Anish%20Narain%20and%20Ritam%20Majumdar%20and%20Nikita%20Narayanan%20and%20Dominic%20Marshall%20and%20Sonali%20Parbhoo&entry.1292438233=%20%20Large%2C%20publicly%20available%20clinical%20datasets%20have%20emerged%20as%20a%20novel%20resource%0Afor%20understanding%20disease%20heterogeneity%20and%20to%20explore%20personalization%20of%0Atherapy.%20These%20datasets%20are%20derived%20from%20data%20not%20originally%20collected%20for%0Aresearch%20purposes%20and%2C%20as%20a%20result%2C%20are%20often%20incomplete%20and%20lack%20critical%0Alabels.%20Many%20AI%20tools%20have%20been%20developed%20to%20retrospectively%20label%20these%0Adatasets%2C%20such%20as%20by%20performing%20disease%20classification%3B%20however%2C%20they%20often%0Asuffer%20from%20limited%20interpretability.%20Previous%20work%20has%20attempted%20to%20explain%0Apredictions%20using%20Concept%20Bottleneck%20Models%20%28CBMs%29%2C%20which%20learn%20interpretable%0Aconcepts%20that%20map%20to%20higher-level%20clinical%20ideas%2C%20facilitating%20human%0Aevaluation.%20However%2C%20these%20models%20often%20experience%20performance%20limitations%20when%0Athe%20concepts%20fail%20to%20adequately%20explain%20or%20characterize%20the%20task.%20We%20use%20the%0Aidentification%20of%20Acute%20Respiratory%20Distress%20Syndrome%20%28ARDS%29%20as%20a%20challenging%0Atest%20case%20to%20demonstrate%20the%20value%20of%20incorporating%20contextual%20information%20from%0Aclinical%20notes%20to%20improve%20CBM%20performance.%20Our%20approach%20leverages%20a%20Large%0ALanguage%20Model%20%28LLM%29%20to%20process%20clinical%20notes%20and%20generate%20additional%0Aconcepts%2C%20resulting%20in%20a%2010%25%20performance%20gain%20over%20existing%20methods.%0AAdditionally%2C%20it%20facilitates%20the%20learning%20of%20more%20comprehensive%20concepts%2C%0Athereby%20reducing%20the%20risk%20of%20information%20leakage%20and%20reliance%20on%20spurious%0Ashortcuts%2C%20thus%20improving%20the%20characterization%20of%20ARDS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09719v1&entry.124074799=Read"},
{"title": "MUJICA: Reforming SISR Models for PBR Material Super-Resolution via\n  Cross-Map Attention", "author": "Xin Du and Maoyuan Xu and Zhi Ying", "abstract": "  Physically Based Rendering (PBR) materials are typically characterized by\nmultiple 2D texture maps such as basecolor, normal, metallic, and roughness\nwhich encode spatially-varying bi-directional reflectance distribution function\n(SVBRDF) parameters to model surface reflectance properties and microfacet\ninteractions. Upscaling SVBRDF material is valuable for modern 3D graphics\napplications. However, existing Single Image Super-Resolution (SISR) methods\nstruggle with cross-map inconsistency, inadequate modeling of modality-specific\nfeatures, and limited generalization due to data distribution shifts. In this\nwork, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention\n(MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based\nSISR models for PBR material super-resolution. MUJICA is seamlessly attached\nafter the pre-trained and frozen SISR backbone. It leverages cross-map\nattention to fuse features while preserving remarkable reconstruction ability\nof the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and\nHMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map\nconsistency. Experiments demonstrate that MUJICA enables efficient training\neven with limited resources and delivers state-of-the-art performance on PBR\nmaterial datasets.\n", "link": "http://arxiv.org/abs/2508.09802v1", "date": "2025-08-13", "relevancy": 2.0749, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.532}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5183}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MUJICA%3A%20Reforming%20SISR%20Models%20for%20PBR%20Material%20Super-Resolution%20via%0A%20%20Cross-Map%20Attention&body=Title%3A%20MUJICA%3A%20Reforming%20SISR%20Models%20for%20PBR%20Material%20Super-Resolution%20via%0A%20%20Cross-Map%20Attention%0AAuthor%3A%20Xin%20Du%20and%20Maoyuan%20Xu%20and%20Zhi%20Ying%0AAbstract%3A%20%20%20Physically%20Based%20Rendering%20%28PBR%29%20materials%20are%20typically%20characterized%20by%0Amultiple%202D%20texture%20maps%20such%20as%20basecolor%2C%20normal%2C%20metallic%2C%20and%20roughness%0Awhich%20encode%20spatially-varying%20bi-directional%20reflectance%20distribution%20function%0A%28SVBRDF%29%20parameters%20to%20model%20surface%20reflectance%20properties%20and%20microfacet%0Ainteractions.%20Upscaling%20SVBRDF%20material%20is%20valuable%20for%20modern%203D%20graphics%0Aapplications.%20However%2C%20existing%20Single%20Image%20Super-Resolution%20%28SISR%29%20methods%0Astruggle%20with%20cross-map%20inconsistency%2C%20inadequate%20modeling%20of%20modality-specific%0Afeatures%2C%20and%20limited%20generalization%20due%20to%20data%20distribution%20shifts.%20In%20this%0Awork%2C%20we%20propose%20Multi-modal%20Upscaling%20Joint%20Inference%20via%20Cross-map%20Attention%0A%28MUJICA%29%2C%20a%20flexible%20adapter%20that%20reforms%20pre-trained%20Swin-transformer-based%0ASISR%20models%20for%20PBR%20material%20super-resolution.%20MUJICA%20is%20seamlessly%20attached%0Aafter%20the%20pre-trained%20and%20frozen%20SISR%20backbone.%20It%20leverages%20cross-map%0Aattention%20to%20fuse%20features%20while%20preserving%20remarkable%20reconstruction%20ability%0Aof%20the%20pre-trained%20SISR%20model.%20Applied%20to%20SISR%20models%20such%20as%20SwinIR%2C%20DRCT%2C%20and%0AHMANet%2C%20MUJICA%20improves%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20scores%20while%20preserving%20cross-map%0Aconsistency.%20Experiments%20demonstrate%20that%20MUJICA%20enables%20efficient%20training%0Aeven%20with%20limited%20resources%20and%20delivers%20state-of-the-art%20performance%20on%20PBR%0Amaterial%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMUJICA%253A%2520Reforming%2520SISR%2520Models%2520for%2520PBR%2520Material%2520Super-Resolution%2520via%250A%2520%2520Cross-Map%2520Attention%26entry.906535625%3DXin%2520Du%2520and%2520Maoyuan%2520Xu%2520and%2520Zhi%2520Ying%26entry.1292438233%3D%2520%2520Physically%2520Based%2520Rendering%2520%2528PBR%2529%2520materials%2520are%2520typically%2520characterized%2520by%250Amultiple%25202D%2520texture%2520maps%2520such%2520as%2520basecolor%252C%2520normal%252C%2520metallic%252C%2520and%2520roughness%250Awhich%2520encode%2520spatially-varying%2520bi-directional%2520reflectance%2520distribution%2520function%250A%2528SVBRDF%2529%2520parameters%2520to%2520model%2520surface%2520reflectance%2520properties%2520and%2520microfacet%250Ainteractions.%2520Upscaling%2520SVBRDF%2520material%2520is%2520valuable%2520for%2520modern%25203D%2520graphics%250Aapplications.%2520However%252C%2520existing%2520Single%2520Image%2520Super-Resolution%2520%2528SISR%2529%2520methods%250Astruggle%2520with%2520cross-map%2520inconsistency%252C%2520inadequate%2520modeling%2520of%2520modality-specific%250Afeatures%252C%2520and%2520limited%2520generalization%2520due%2520to%2520data%2520distribution%2520shifts.%2520In%2520this%250Awork%252C%2520we%2520propose%2520Multi-modal%2520Upscaling%2520Joint%2520Inference%2520via%2520Cross-map%2520Attention%250A%2528MUJICA%2529%252C%2520a%2520flexible%2520adapter%2520that%2520reforms%2520pre-trained%2520Swin-transformer-based%250ASISR%2520models%2520for%2520PBR%2520material%2520super-resolution.%2520MUJICA%2520is%2520seamlessly%2520attached%250Aafter%2520the%2520pre-trained%2520and%2520frozen%2520SISR%2520backbone.%2520It%2520leverages%2520cross-map%250Aattention%2520to%2520fuse%2520features%2520while%2520preserving%2520remarkable%2520reconstruction%2520ability%250Aof%2520the%2520pre-trained%2520SISR%2520model.%2520Applied%2520to%2520SISR%2520models%2520such%2520as%2520SwinIR%252C%2520DRCT%252C%2520and%250AHMANet%252C%2520MUJICA%2520improves%2520PSNR%252C%2520SSIM%252C%2520and%2520LPIPS%2520scores%2520while%2520preserving%2520cross-map%250Aconsistency.%2520Experiments%2520demonstrate%2520that%2520MUJICA%2520enables%2520efficient%2520training%250Aeven%2520with%2520limited%2520resources%2520and%2520delivers%2520state-of-the-art%2520performance%2520on%2520PBR%250Amaterial%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MUJICA%3A%20Reforming%20SISR%20Models%20for%20PBR%20Material%20Super-Resolution%20via%0A%20%20Cross-Map%20Attention&entry.906535625=Xin%20Du%20and%20Maoyuan%20Xu%20and%20Zhi%20Ying&entry.1292438233=%20%20Physically%20Based%20Rendering%20%28PBR%29%20materials%20are%20typically%20characterized%20by%0Amultiple%202D%20texture%20maps%20such%20as%20basecolor%2C%20normal%2C%20metallic%2C%20and%20roughness%0Awhich%20encode%20spatially-varying%20bi-directional%20reflectance%20distribution%20function%0A%28SVBRDF%29%20parameters%20to%20model%20surface%20reflectance%20properties%20and%20microfacet%0Ainteractions.%20Upscaling%20SVBRDF%20material%20is%20valuable%20for%20modern%203D%20graphics%0Aapplications.%20However%2C%20existing%20Single%20Image%20Super-Resolution%20%28SISR%29%20methods%0Astruggle%20with%20cross-map%20inconsistency%2C%20inadequate%20modeling%20of%20modality-specific%0Afeatures%2C%20and%20limited%20generalization%20due%20to%20data%20distribution%20shifts.%20In%20this%0Awork%2C%20we%20propose%20Multi-modal%20Upscaling%20Joint%20Inference%20via%20Cross-map%20Attention%0A%28MUJICA%29%2C%20a%20flexible%20adapter%20that%20reforms%20pre-trained%20Swin-transformer-based%0ASISR%20models%20for%20PBR%20material%20super-resolution.%20MUJICA%20is%20seamlessly%20attached%0Aafter%20the%20pre-trained%20and%20frozen%20SISR%20backbone.%20It%20leverages%20cross-map%0Aattention%20to%20fuse%20features%20while%20preserving%20remarkable%20reconstruction%20ability%0Aof%20the%20pre-trained%20SISR%20model.%20Applied%20to%20SISR%20models%20such%20as%20SwinIR%2C%20DRCT%2C%20and%0AHMANet%2C%20MUJICA%20improves%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20scores%20while%20preserving%20cross-map%0Aconsistency.%20Experiments%20demonstrate%20that%20MUJICA%20enables%20efficient%20training%0Aeven%20with%20limited%20resources%20and%20delivers%20state-of-the-art%20performance%20on%20PBR%0Amaterial%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09802v1&entry.124074799=Read"},
{"title": "Retrieval-Augmented Decision Transformer: External Memory for In-context\n  RL", "author": "Thomas Schmied and Fabian Paischer and Vihang Patil and Markus Hofmarcher and Razvan Pascanu and Sepp Hochreiter", "abstract": "  In-context learning (ICL) is the ability of a model to learn a new task by\nobserving a few exemplars in its context. While prevalent in NLP, this\ncapability has recently also been observed in Reinforcement Learning (RL)\nsettings. Prior in-context RL methods, however, require entire episodes in the\nagent's context. Given that complex environments typically lead to long\nepisodes with sparse rewards, these methods are constrained to simple\nenvironments with short episodes. To address these challenges, we introduce\nRetrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external\nmemory mechanism to store past experiences from which it retrieves only\nsub-trajectories relevant for the current situation. The retrieval component in\nRA-DT does not require training and can be entirely domain-agnostic. We\nevaluate the capabilities of RA-DT on grid-world environments, robotics\nsimulations, and procedurally-generated video games. On grid-worlds, RA-DT\noutperforms baselines, while using only a fraction of their context length.\nFurthermore, we illuminate the limitations of current in-context RL methods on\ncomplex environments and discuss future directions. To facilitate future\nresearch, we release datasets for four of the considered environments.\n", "link": "http://arxiv.org/abs/2410.07071v3", "date": "2025-08-13", "relevancy": 2.0674, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5641}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5082}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval-Augmented%20Decision%20Transformer%3A%20External%20Memory%20for%20In-context%0A%20%20RL&body=Title%3A%20Retrieval-Augmented%20Decision%20Transformer%3A%20External%20Memory%20for%20In-context%0A%20%20RL%0AAuthor%3A%20Thomas%20Schmied%20and%20Fabian%20Paischer%20and%20Vihang%20Patil%20and%20Markus%20Hofmarcher%20and%20Razvan%20Pascanu%20and%20Sepp%20Hochreiter%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20is%20the%20ability%20of%20a%20model%20to%20learn%20a%20new%20task%20by%0Aobserving%20a%20few%20exemplars%20in%20its%20context.%20While%20prevalent%20in%20NLP%2C%20this%0Acapability%20has%20recently%20also%20been%20observed%20in%20Reinforcement%20Learning%20%28RL%29%0Asettings.%20Prior%20in-context%20RL%20methods%2C%20however%2C%20require%20entire%20episodes%20in%20the%0Aagent%27s%20context.%20Given%20that%20complex%20environments%20typically%20lead%20to%20long%0Aepisodes%20with%20sparse%20rewards%2C%20these%20methods%20are%20constrained%20to%20simple%0Aenvironments%20with%20short%20episodes.%20To%20address%20these%20challenges%2C%20we%20introduce%0ARetrieval-Augmented%20Decision%20Transformer%20%28RA-DT%29.%20RA-DT%20employs%20an%20external%0Amemory%20mechanism%20to%20store%20past%20experiences%20from%20which%20it%20retrieves%20only%0Asub-trajectories%20relevant%20for%20the%20current%20situation.%20The%20retrieval%20component%20in%0ARA-DT%20does%20not%20require%20training%20and%20can%20be%20entirely%20domain-agnostic.%20We%0Aevaluate%20the%20capabilities%20of%20RA-DT%20on%20grid-world%20environments%2C%20robotics%0Asimulations%2C%20and%20procedurally-generated%20video%20games.%20On%20grid-worlds%2C%20RA-DT%0Aoutperforms%20baselines%2C%20while%20using%20only%20a%20fraction%20of%20their%20context%20length.%0AFurthermore%2C%20we%20illuminate%20the%20limitations%20of%20current%20in-context%20RL%20methods%20on%0Acomplex%20environments%20and%20discuss%20future%20directions.%20To%20facilitate%20future%0Aresearch%2C%20we%20release%20datasets%20for%20four%20of%20the%20considered%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07071v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval-Augmented%2520Decision%2520Transformer%253A%2520External%2520Memory%2520for%2520In-context%250A%2520%2520RL%26entry.906535625%3DThomas%2520Schmied%2520and%2520Fabian%2520Paischer%2520and%2520Vihang%2520Patil%2520and%2520Markus%2520Hofmarcher%2520and%2520Razvan%2520Pascanu%2520and%2520Sepp%2520Hochreiter%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520is%2520the%2520ability%2520of%2520a%2520model%2520to%2520learn%2520a%2520new%2520task%2520by%250Aobserving%2520a%2520few%2520exemplars%2520in%2520its%2520context.%2520While%2520prevalent%2520in%2520NLP%252C%2520this%250Acapability%2520has%2520recently%2520also%2520been%2520observed%2520in%2520Reinforcement%2520Learning%2520%2528RL%2529%250Asettings.%2520Prior%2520in-context%2520RL%2520methods%252C%2520however%252C%2520require%2520entire%2520episodes%2520in%2520the%250Aagent%2527s%2520context.%2520Given%2520that%2520complex%2520environments%2520typically%2520lead%2520to%2520long%250Aepisodes%2520with%2520sparse%2520rewards%252C%2520these%2520methods%2520are%2520constrained%2520to%2520simple%250Aenvironments%2520with%2520short%2520episodes.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250ARetrieval-Augmented%2520Decision%2520Transformer%2520%2528RA-DT%2529.%2520RA-DT%2520employs%2520an%2520external%250Amemory%2520mechanism%2520to%2520store%2520past%2520experiences%2520from%2520which%2520it%2520retrieves%2520only%250Asub-trajectories%2520relevant%2520for%2520the%2520current%2520situation.%2520The%2520retrieval%2520component%2520in%250ARA-DT%2520does%2520not%2520require%2520training%2520and%2520can%2520be%2520entirely%2520domain-agnostic.%2520We%250Aevaluate%2520the%2520capabilities%2520of%2520RA-DT%2520on%2520grid-world%2520environments%252C%2520robotics%250Asimulations%252C%2520and%2520procedurally-generated%2520video%2520games.%2520On%2520grid-worlds%252C%2520RA-DT%250Aoutperforms%2520baselines%252C%2520while%2520using%2520only%2520a%2520fraction%2520of%2520their%2520context%2520length.%250AFurthermore%252C%2520we%2520illuminate%2520the%2520limitations%2520of%2520current%2520in-context%2520RL%2520methods%2520on%250Acomplex%2520environments%2520and%2520discuss%2520future%2520directions.%2520To%2520facilitate%2520future%250Aresearch%252C%2520we%2520release%2520datasets%2520for%2520four%2520of%2520the%2520considered%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07071v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval-Augmented%20Decision%20Transformer%3A%20External%20Memory%20for%20In-context%0A%20%20RL&entry.906535625=Thomas%20Schmied%20and%20Fabian%20Paischer%20and%20Vihang%20Patil%20and%20Markus%20Hofmarcher%20and%20Razvan%20Pascanu%20and%20Sepp%20Hochreiter&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20is%20the%20ability%20of%20a%20model%20to%20learn%20a%20new%20task%20by%0Aobserving%20a%20few%20exemplars%20in%20its%20context.%20While%20prevalent%20in%20NLP%2C%20this%0Acapability%20has%20recently%20also%20been%20observed%20in%20Reinforcement%20Learning%20%28RL%29%0Asettings.%20Prior%20in-context%20RL%20methods%2C%20however%2C%20require%20entire%20episodes%20in%20the%0Aagent%27s%20context.%20Given%20that%20complex%20environments%20typically%20lead%20to%20long%0Aepisodes%20with%20sparse%20rewards%2C%20these%20methods%20are%20constrained%20to%20simple%0Aenvironments%20with%20short%20episodes.%20To%20address%20these%20challenges%2C%20we%20introduce%0ARetrieval-Augmented%20Decision%20Transformer%20%28RA-DT%29.%20RA-DT%20employs%20an%20external%0Amemory%20mechanism%20to%20store%20past%20experiences%20from%20which%20it%20retrieves%20only%0Asub-trajectories%20relevant%20for%20the%20current%20situation.%20The%20retrieval%20component%20in%0ARA-DT%20does%20not%20require%20training%20and%20can%20be%20entirely%20domain-agnostic.%20We%0Aevaluate%20the%20capabilities%20of%20RA-DT%20on%20grid-world%20environments%2C%20robotics%0Asimulations%2C%20and%20procedurally-generated%20video%20games.%20On%20grid-worlds%2C%20RA-DT%0Aoutperforms%20baselines%2C%20while%20using%20only%20a%20fraction%20of%20their%20context%20length.%0AFurthermore%2C%20we%20illuminate%20the%20limitations%20of%20current%20in-context%20RL%20methods%20on%0Acomplex%20environments%20and%20discuss%20future%20directions.%20To%20facilitate%20future%0Aresearch%2C%20we%20release%20datasets%20for%20four%20of%20the%20considered%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07071v3&entry.124074799=Read"},
{"title": "A Survey on Parallel Text Generation: From Parallel Decoding to\n  Diffusion Language Models", "author": "Lingzhe Zhang and Liancheng Fang and Chiming Duan and Minghua He and Leyi Pan and Pei Xiao and Shiyu Huang and Yunpeng Zhai and Xuming Hu and Philip S. Yu and Aiwei Liu", "abstract": "  As text generation has become a core capability of modern Large Language\nModels (LLMs), it underpins a wide range of downstream applications. However,\nmost existing LLMs rely on autoregressive (AR) generation, producing one token\nat a time based on previously generated context-resulting in limited generation\nspeed due to the inherently sequential nature of the process. To address this\nchallenge, an increasing number of researchers have begun exploring parallel\ntext generation-a broad class of techniques aimed at breaking the\ntoken-by-token generation bottleneck and improving inference efficiency.\nDespite growing interest, there remains a lack of comprehensive analysis on\nwhat specific techniques constitute parallel text generation and how they\nimprove inference performance. To bridge this gap, we present a systematic\nsurvey of parallel text generation methods. We categorize existing approaches\ninto AR-based and Non-AR-based paradigms, and provide a detailed examination of\nthe core techniques within each category. Following this taxonomy, we assess\ntheir theoretical trade-offs in terms of speed, quality, and efficiency, and\nexamine their potential for combination and comparison with alternative\nacceleration strategies. Finally, based on our findings, we highlight recent\nadvancements, identify open challenges, and outline promising directions for\nfuture research in parallel text generation. We have also created a GitHub\nrepository for indexing relevant papers and open resources available at\nhttps://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.\n", "link": "http://arxiv.org/abs/2508.08712v2", "date": "2025-08-13", "relevancy": 2.062, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5436}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5282}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Parallel%20Text%20Generation%3A%20From%20Parallel%20Decoding%20to%0A%20%20Diffusion%20Language%20Models&body=Title%3A%20A%20Survey%20on%20Parallel%20Text%20Generation%3A%20From%20Parallel%20Decoding%20to%0A%20%20Diffusion%20Language%20Models%0AAuthor%3A%20Lingzhe%20Zhang%20and%20Liancheng%20Fang%20and%20Chiming%20Duan%20and%20Minghua%20He%20and%20Leyi%20Pan%20and%20Pei%20Xiao%20and%20Shiyu%20Huang%20and%20Yunpeng%20Zhai%20and%20Xuming%20Hu%20and%20Philip%20S.%20Yu%20and%20Aiwei%20Liu%0AAbstract%3A%20%20%20As%20text%20generation%20has%20become%20a%20core%20capability%20of%20modern%20Large%20Language%0AModels%20%28LLMs%29%2C%20it%20underpins%20a%20wide%20range%20of%20downstream%20applications.%20However%2C%0Amost%20existing%20LLMs%20rely%20on%20autoregressive%20%28AR%29%20generation%2C%20producing%20one%20token%0Aat%20a%20time%20based%20on%20previously%20generated%20context-resulting%20in%20limited%20generation%0Aspeed%20due%20to%20the%20inherently%20sequential%20nature%20of%20the%20process.%20To%20address%20this%0Achallenge%2C%20an%20increasing%20number%20of%20researchers%20have%20begun%20exploring%20parallel%0Atext%20generation-a%20broad%20class%20of%20techniques%20aimed%20at%20breaking%20the%0Atoken-by-token%20generation%20bottleneck%20and%20improving%20inference%20efficiency.%0ADespite%20growing%20interest%2C%20there%20remains%20a%20lack%20of%20comprehensive%20analysis%20on%0Awhat%20specific%20techniques%20constitute%20parallel%20text%20generation%20and%20how%20they%0Aimprove%20inference%20performance.%20To%20bridge%20this%20gap%2C%20we%20present%20a%20systematic%0Asurvey%20of%20parallel%20text%20generation%20methods.%20We%20categorize%20existing%20approaches%0Ainto%20AR-based%20and%20Non-AR-based%20paradigms%2C%20and%20provide%20a%20detailed%20examination%20of%0Athe%20core%20techniques%20within%20each%20category.%20Following%20this%20taxonomy%2C%20we%20assess%0Atheir%20theoretical%20trade-offs%20in%20terms%20of%20speed%2C%20quality%2C%20and%20efficiency%2C%20and%0Aexamine%20their%20potential%20for%20combination%20and%20comparison%20with%20alternative%0Aacceleration%20strategies.%20Finally%2C%20based%20on%20our%20findings%2C%20we%20highlight%20recent%0Aadvancements%2C%20identify%20open%20challenges%2C%20and%20outline%20promising%20directions%20for%0Afuture%20research%20in%20parallel%20text%20generation.%20We%20have%20also%20created%20a%20GitHub%0Arepository%20for%20indexing%20relevant%20papers%20and%20open%20resources%20available%20at%0Ahttps%3A//github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08712v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Parallel%2520Text%2520Generation%253A%2520From%2520Parallel%2520Decoding%2520to%250A%2520%2520Diffusion%2520Language%2520Models%26entry.906535625%3DLingzhe%2520Zhang%2520and%2520Liancheng%2520Fang%2520and%2520Chiming%2520Duan%2520and%2520Minghua%2520He%2520and%2520Leyi%2520Pan%2520and%2520Pei%2520Xiao%2520and%2520Shiyu%2520Huang%2520and%2520Yunpeng%2520Zhai%2520and%2520Xuming%2520Hu%2520and%2520Philip%2520S.%2520Yu%2520and%2520Aiwei%2520Liu%26entry.1292438233%3D%2520%2520As%2520text%2520generation%2520has%2520become%2520a%2520core%2520capability%2520of%2520modern%2520Large%2520Language%250AModels%2520%2528LLMs%2529%252C%2520it%2520underpins%2520a%2520wide%2520range%2520of%2520downstream%2520applications.%2520However%252C%250Amost%2520existing%2520LLMs%2520rely%2520on%2520autoregressive%2520%2528AR%2529%2520generation%252C%2520producing%2520one%2520token%250Aat%2520a%2520time%2520based%2520on%2520previously%2520generated%2520context-resulting%2520in%2520limited%2520generation%250Aspeed%2520due%2520to%2520the%2520inherently%2520sequential%2520nature%2520of%2520the%2520process.%2520To%2520address%2520this%250Achallenge%252C%2520an%2520increasing%2520number%2520of%2520researchers%2520have%2520begun%2520exploring%2520parallel%250Atext%2520generation-a%2520broad%2520class%2520of%2520techniques%2520aimed%2520at%2520breaking%2520the%250Atoken-by-token%2520generation%2520bottleneck%2520and%2520improving%2520inference%2520efficiency.%250ADespite%2520growing%2520interest%252C%2520there%2520remains%2520a%2520lack%2520of%2520comprehensive%2520analysis%2520on%250Awhat%2520specific%2520techniques%2520constitute%2520parallel%2520text%2520generation%2520and%2520how%2520they%250Aimprove%2520inference%2520performance.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520a%2520systematic%250Asurvey%2520of%2520parallel%2520text%2520generation%2520methods.%2520We%2520categorize%2520existing%2520approaches%250Ainto%2520AR-based%2520and%2520Non-AR-based%2520paradigms%252C%2520and%2520provide%2520a%2520detailed%2520examination%2520of%250Athe%2520core%2520techniques%2520within%2520each%2520category.%2520Following%2520this%2520taxonomy%252C%2520we%2520assess%250Atheir%2520theoretical%2520trade-offs%2520in%2520terms%2520of%2520speed%252C%2520quality%252C%2520and%2520efficiency%252C%2520and%250Aexamine%2520their%2520potential%2520for%2520combination%2520and%2520comparison%2520with%2520alternative%250Aacceleration%2520strategies.%2520Finally%252C%2520based%2520on%2520our%2520findings%252C%2520we%2520highlight%2520recent%250Aadvancements%252C%2520identify%2520open%2520challenges%252C%2520and%2520outline%2520promising%2520directions%2520for%250Afuture%2520research%2520in%2520parallel%2520text%2520generation.%2520We%2520have%2520also%2520created%2520a%2520GitHub%250Arepository%2520for%2520indexing%2520relevant%2520papers%2520and%2520open%2520resources%2520available%2520at%250Ahttps%253A//github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08712v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Parallel%20Text%20Generation%3A%20From%20Parallel%20Decoding%20to%0A%20%20Diffusion%20Language%20Models&entry.906535625=Lingzhe%20Zhang%20and%20Liancheng%20Fang%20and%20Chiming%20Duan%20and%20Minghua%20He%20and%20Leyi%20Pan%20and%20Pei%20Xiao%20and%20Shiyu%20Huang%20and%20Yunpeng%20Zhai%20and%20Xuming%20Hu%20and%20Philip%20S.%20Yu%20and%20Aiwei%20Liu&entry.1292438233=%20%20As%20text%20generation%20has%20become%20a%20core%20capability%20of%20modern%20Large%20Language%0AModels%20%28LLMs%29%2C%20it%20underpins%20a%20wide%20range%20of%20downstream%20applications.%20However%2C%0Amost%20existing%20LLMs%20rely%20on%20autoregressive%20%28AR%29%20generation%2C%20producing%20one%20token%0Aat%20a%20time%20based%20on%20previously%20generated%20context-resulting%20in%20limited%20generation%0Aspeed%20due%20to%20the%20inherently%20sequential%20nature%20of%20the%20process.%20To%20address%20this%0Achallenge%2C%20an%20increasing%20number%20of%20researchers%20have%20begun%20exploring%20parallel%0Atext%20generation-a%20broad%20class%20of%20techniques%20aimed%20at%20breaking%20the%0Atoken-by-token%20generation%20bottleneck%20and%20improving%20inference%20efficiency.%0ADespite%20growing%20interest%2C%20there%20remains%20a%20lack%20of%20comprehensive%20analysis%20on%0Awhat%20specific%20techniques%20constitute%20parallel%20text%20generation%20and%20how%20they%0Aimprove%20inference%20performance.%20To%20bridge%20this%20gap%2C%20we%20present%20a%20systematic%0Asurvey%20of%20parallel%20text%20generation%20methods.%20We%20categorize%20existing%20approaches%0Ainto%20AR-based%20and%20Non-AR-based%20paradigms%2C%20and%20provide%20a%20detailed%20examination%20of%0Athe%20core%20techniques%20within%20each%20category.%20Following%20this%20taxonomy%2C%20we%20assess%0Atheir%20theoretical%20trade-offs%20in%20terms%20of%20speed%2C%20quality%2C%20and%20efficiency%2C%20and%0Aexamine%20their%20potential%20for%20combination%20and%20comparison%20with%20alternative%0Aacceleration%20strategies.%20Finally%2C%20based%20on%20our%20findings%2C%20we%20highlight%20recent%0Aadvancements%2C%20identify%20open%20challenges%2C%20and%20outline%20promising%20directions%20for%0Afuture%20research%20in%20parallel%20text%20generation.%20We%20have%20also%20created%20a%20GitHub%0Arepository%20for%20indexing%20relevant%20papers%20and%20open%20resources%20available%20at%0Ahttps%3A//github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08712v2&entry.124074799=Read"},
{"title": "Cryo-em images are intrinsically low dimensional", "author": "Luke Evans and Octavian-Vlad Murad and Lars Dingeldein and Pilar Cossio and Roberto Covino and Marina Meila", "abstract": "  Simulation-based inference provides a powerful framework for cryo-electron\nmicroscopy, employing neural networks in methods like CryoSBI to infer\nbiomolecular conformations via learned latent representations. This latent\nspace represents a rich opportunity, encoding valuable information about the\nphysical system and the inference process. Harnessing this potential hinges on\nunderstanding the underlying geometric structure of these representations. We\ninvestigate this structure by applying manifold learning techniques to CryoSBI\nrepresentations of hemagglutinin (simulated and experimental). We reveal that\nthese high-dimensional data inherently populate low-dimensional, smooth\nmanifolds, with simulated data effectively covering the experimental\ncounterpart. By characterizing the manifold's geometry using Diffusion Maps and\nidentifying its principal axes of variation via coordinate interpretation\nmethods, we establish a direct link between the latent structure and key\nphysical parameters. Discovering this intrinsic low-dimensionality and\ninterpretable geometric organization not only validates the CryoSBI approach\nbut enables us to learn more from the data structure and provides opportunities\nfor improving future inference strategies by exploiting this revealed manifold\ngeometry.\n", "link": "http://arxiv.org/abs/2504.11249v2", "date": "2025-08-13", "relevancy": 1.9101, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4812}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4778}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cryo-em%20images%20are%20intrinsically%20low%20dimensional&body=Title%3A%20Cryo-em%20images%20are%20intrinsically%20low%20dimensional%0AAuthor%3A%20Luke%20Evans%20and%20Octavian-Vlad%20Murad%20and%20Lars%20Dingeldein%20and%20Pilar%20Cossio%20and%20Roberto%20Covino%20and%20Marina%20Meila%0AAbstract%3A%20%20%20Simulation-based%20inference%20provides%20a%20powerful%20framework%20for%20cryo-electron%0Amicroscopy%2C%20employing%20neural%20networks%20in%20methods%20like%20CryoSBI%20to%20infer%0Abiomolecular%20conformations%20via%20learned%20latent%20representations.%20This%20latent%0Aspace%20represents%20a%20rich%20opportunity%2C%20encoding%20valuable%20information%20about%20the%0Aphysical%20system%20and%20the%20inference%20process.%20Harnessing%20this%20potential%20hinges%20on%0Aunderstanding%20the%20underlying%20geometric%20structure%20of%20these%20representations.%20We%0Ainvestigate%20this%20structure%20by%20applying%20manifold%20learning%20techniques%20to%20CryoSBI%0Arepresentations%20of%20hemagglutinin%20%28simulated%20and%20experimental%29.%20We%20reveal%20that%0Athese%20high-dimensional%20data%20inherently%20populate%20low-dimensional%2C%20smooth%0Amanifolds%2C%20with%20simulated%20data%20effectively%20covering%20the%20experimental%0Acounterpart.%20By%20characterizing%20the%20manifold%27s%20geometry%20using%20Diffusion%20Maps%20and%0Aidentifying%20its%20principal%20axes%20of%20variation%20via%20coordinate%20interpretation%0Amethods%2C%20we%20establish%20a%20direct%20link%20between%20the%20latent%20structure%20and%20key%0Aphysical%20parameters.%20Discovering%20this%20intrinsic%20low-dimensionality%20and%0Ainterpretable%20geometric%20organization%20not%20only%20validates%20the%20CryoSBI%20approach%0Abut%20enables%20us%20to%20learn%20more%20from%20the%20data%20structure%20and%20provides%20opportunities%0Afor%20improving%20future%20inference%20strategies%20by%20exploiting%20this%20revealed%20manifold%0Ageometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11249v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCryo-em%2520images%2520are%2520intrinsically%2520low%2520dimensional%26entry.906535625%3DLuke%2520Evans%2520and%2520Octavian-Vlad%2520Murad%2520and%2520Lars%2520Dingeldein%2520and%2520Pilar%2520Cossio%2520and%2520Roberto%2520Covino%2520and%2520Marina%2520Meila%26entry.1292438233%3D%2520%2520Simulation-based%2520inference%2520provides%2520a%2520powerful%2520framework%2520for%2520cryo-electron%250Amicroscopy%252C%2520employing%2520neural%2520networks%2520in%2520methods%2520like%2520CryoSBI%2520to%2520infer%250Abiomolecular%2520conformations%2520via%2520learned%2520latent%2520representations.%2520This%2520latent%250Aspace%2520represents%2520a%2520rich%2520opportunity%252C%2520encoding%2520valuable%2520information%2520about%2520the%250Aphysical%2520system%2520and%2520the%2520inference%2520process.%2520Harnessing%2520this%2520potential%2520hinges%2520on%250Aunderstanding%2520the%2520underlying%2520geometric%2520structure%2520of%2520these%2520representations.%2520We%250Ainvestigate%2520this%2520structure%2520by%2520applying%2520manifold%2520learning%2520techniques%2520to%2520CryoSBI%250Arepresentations%2520of%2520hemagglutinin%2520%2528simulated%2520and%2520experimental%2529.%2520We%2520reveal%2520that%250Athese%2520high-dimensional%2520data%2520inherently%2520populate%2520low-dimensional%252C%2520smooth%250Amanifolds%252C%2520with%2520simulated%2520data%2520effectively%2520covering%2520the%2520experimental%250Acounterpart.%2520By%2520characterizing%2520the%2520manifold%2527s%2520geometry%2520using%2520Diffusion%2520Maps%2520and%250Aidentifying%2520its%2520principal%2520axes%2520of%2520variation%2520via%2520coordinate%2520interpretation%250Amethods%252C%2520we%2520establish%2520a%2520direct%2520link%2520between%2520the%2520latent%2520structure%2520and%2520key%250Aphysical%2520parameters.%2520Discovering%2520this%2520intrinsic%2520low-dimensionality%2520and%250Ainterpretable%2520geometric%2520organization%2520not%2520only%2520validates%2520the%2520CryoSBI%2520approach%250Abut%2520enables%2520us%2520to%2520learn%2520more%2520from%2520the%2520data%2520structure%2520and%2520provides%2520opportunities%250Afor%2520improving%2520future%2520inference%2520strategies%2520by%2520exploiting%2520this%2520revealed%2520manifold%250Ageometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11249v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cryo-em%20images%20are%20intrinsically%20low%20dimensional&entry.906535625=Luke%20Evans%20and%20Octavian-Vlad%20Murad%20and%20Lars%20Dingeldein%20and%20Pilar%20Cossio%20and%20Roberto%20Covino%20and%20Marina%20Meila&entry.1292438233=%20%20Simulation-based%20inference%20provides%20a%20powerful%20framework%20for%20cryo-electron%0Amicroscopy%2C%20employing%20neural%20networks%20in%20methods%20like%20CryoSBI%20to%20infer%0Abiomolecular%20conformations%20via%20learned%20latent%20representations.%20This%20latent%0Aspace%20represents%20a%20rich%20opportunity%2C%20encoding%20valuable%20information%20about%20the%0Aphysical%20system%20and%20the%20inference%20process.%20Harnessing%20this%20potential%20hinges%20on%0Aunderstanding%20the%20underlying%20geometric%20structure%20of%20these%20representations.%20We%0Ainvestigate%20this%20structure%20by%20applying%20manifold%20learning%20techniques%20to%20CryoSBI%0Arepresentations%20of%20hemagglutinin%20%28simulated%20and%20experimental%29.%20We%20reveal%20that%0Athese%20high-dimensional%20data%20inherently%20populate%20low-dimensional%2C%20smooth%0Amanifolds%2C%20with%20simulated%20data%20effectively%20covering%20the%20experimental%0Acounterpart.%20By%20characterizing%20the%20manifold%27s%20geometry%20using%20Diffusion%20Maps%20and%0Aidentifying%20its%20principal%20axes%20of%20variation%20via%20coordinate%20interpretation%0Amethods%2C%20we%20establish%20a%20direct%20link%20between%20the%20latent%20structure%20and%20key%0Aphysical%20parameters.%20Discovering%20this%20intrinsic%20low-dimensionality%20and%0Ainterpretable%20geometric%20organization%20not%20only%20validates%20the%20CryoSBI%20approach%0Abut%20enables%20us%20to%20learn%20more%20from%20the%20data%20structure%20and%20provides%20opportunities%0Afor%20improving%20future%20inference%20strategies%20by%20exploiting%20this%20revealed%20manifold%0Ageometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11249v2&entry.124074799=Read"},
{"title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression", "author": "Payman Behnam and Yaosheng Fu and Ritchie Zhao and Po-An Tsai and Zhiding Yu and Alexey Tumanov", "abstract": "  Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV.\n", "link": "http://arxiv.org/abs/2502.14051v3", "date": "2025-08-13", "relevancy": 1.8614, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4843}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RocketKV%3A%20Accelerating%20Long-Context%20LLM%20Inference%20via%20Two-Stage%20KV%20Cache%0A%20%20Compression&body=Title%3A%20RocketKV%3A%20Accelerating%20Long-Context%20LLM%20Inference%20via%20Two-Stage%20KV%20Cache%0A%20%20Compression%0AAuthor%3A%20Payman%20Behnam%20and%20Yaosheng%20Fu%20and%20Ritchie%20Zhao%20and%20Po-An%20Tsai%20and%20Zhiding%20Yu%20and%20Alexey%20Tumanov%0AAbstract%3A%20%20%20Transformer-based%20Large%20Language%20Models%20rely%20critically%20on%20the%20KV%20cache%20to%0Aefficiently%20handle%20extended%20contexts%20during%20the%20decode%20phase.%20Yet%2C%20the%20size%20of%0Athe%20KV%20cache%20grows%20proportionally%20with%20the%20input%20length%2C%20burdening%20both%20memory%0Abandwidth%20and%20capacity%20as%20decoding%20progresses.%20To%20address%20this%20challenge%2C%20we%0Apresent%20RocketKV%2C%20a%20training-free%20KV%20cache%20compression%20strategy%20containing%20two%0Aconsecutive%20stages.%20In%20the%20first%20stage%2C%20it%20performs%20coarse-grain%20permanent%20KV%0Acache%20eviction%20on%20the%20input%20sequence%20tokens.%20In%20the%20second%20stage%2C%20it%20adopts%20a%0Ahybrid%20sparse%20attention%20method%20to%20conduct%20fine-grain%20top-k%20sparse%20attention%2C%0Aapproximating%20the%20attention%20scores%20by%20leveraging%20both%20head%20and%20sequence%0Adimensionality%20reductions.%20We%20show%20that%20RocketKV%20provides%20a%20compression%20ratio%0Aof%20up%20to%20400%24%5Ctimes%24%2C%20end-to-end%20speedup%20of%20up%20to%203.7%24%5Ctimes%24%20as%20well%20as%20peak%0Amemory%20reduction%20of%20up%20to%2032.6%25%20in%20the%20decode%20phase%20on%20an%20NVIDIA%20A100%20GPU%0Acompared%20to%20the%20full%20KV%20cache%20baseline%2C%20while%20achieving%20negligible%20accuracy%0Aloss%20on%20a%20variety%20of%20long-context%20tasks.%20We%20also%20propose%20a%20variant%20of%20RocketKV%0Afor%20multi-turn%20scenarios%2C%20which%20consistently%20outperforms%20other%20existing%20methods%0Aand%20achieves%20accuracy%20nearly%20on%20par%20with%20an%20oracle%20top-k%20attention%20scheme.%20The%0Asource%20code%20is%20available%20here%3A%20https%3A//github.com/NVlabs/RocketKV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14051v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRocketKV%253A%2520Accelerating%2520Long-Context%2520LLM%2520Inference%2520via%2520Two-Stage%2520KV%2520Cache%250A%2520%2520Compression%26entry.906535625%3DPayman%2520Behnam%2520and%2520Yaosheng%2520Fu%2520and%2520Ritchie%2520Zhao%2520and%2520Po-An%2520Tsai%2520and%2520Zhiding%2520Yu%2520and%2520Alexey%2520Tumanov%26entry.1292438233%3D%2520%2520Transformer-based%2520Large%2520Language%2520Models%2520rely%2520critically%2520on%2520the%2520KV%2520cache%2520to%250Aefficiently%2520handle%2520extended%2520contexts%2520during%2520the%2520decode%2520phase.%2520Yet%252C%2520the%2520size%2520of%250Athe%2520KV%2520cache%2520grows%2520proportionally%2520with%2520the%2520input%2520length%252C%2520burdening%2520both%2520memory%250Abandwidth%2520and%2520capacity%2520as%2520decoding%2520progresses.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apresent%2520RocketKV%252C%2520a%2520training-free%2520KV%2520cache%2520compression%2520strategy%2520containing%2520two%250Aconsecutive%2520stages.%2520In%2520the%2520first%2520stage%252C%2520it%2520performs%2520coarse-grain%2520permanent%2520KV%250Acache%2520eviction%2520on%2520the%2520input%2520sequence%2520tokens.%2520In%2520the%2520second%2520stage%252C%2520it%2520adopts%2520a%250Ahybrid%2520sparse%2520attention%2520method%2520to%2520conduct%2520fine-grain%2520top-k%2520sparse%2520attention%252C%250Aapproximating%2520the%2520attention%2520scores%2520by%2520leveraging%2520both%2520head%2520and%2520sequence%250Adimensionality%2520reductions.%2520We%2520show%2520that%2520RocketKV%2520provides%2520a%2520compression%2520ratio%250Aof%2520up%2520to%2520400%2524%255Ctimes%2524%252C%2520end-to-end%2520speedup%2520of%2520up%2520to%25203.7%2524%255Ctimes%2524%2520as%2520well%2520as%2520peak%250Amemory%2520reduction%2520of%2520up%2520to%252032.6%2525%2520in%2520the%2520decode%2520phase%2520on%2520an%2520NVIDIA%2520A100%2520GPU%250Acompared%2520to%2520the%2520full%2520KV%2520cache%2520baseline%252C%2520while%2520achieving%2520negligible%2520accuracy%250Aloss%2520on%2520a%2520variety%2520of%2520long-context%2520tasks.%2520We%2520also%2520propose%2520a%2520variant%2520of%2520RocketKV%250Afor%2520multi-turn%2520scenarios%252C%2520which%2520consistently%2520outperforms%2520other%2520existing%2520methods%250Aand%2520achieves%2520accuracy%2520nearly%2520on%2520par%2520with%2520an%2520oracle%2520top-k%2520attention%2520scheme.%2520The%250Asource%2520code%2520is%2520available%2520here%253A%2520https%253A//github.com/NVlabs/RocketKV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14051v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RocketKV%3A%20Accelerating%20Long-Context%20LLM%20Inference%20via%20Two-Stage%20KV%20Cache%0A%20%20Compression&entry.906535625=Payman%20Behnam%20and%20Yaosheng%20Fu%20and%20Ritchie%20Zhao%20and%20Po-An%20Tsai%20and%20Zhiding%20Yu%20and%20Alexey%20Tumanov&entry.1292438233=%20%20Transformer-based%20Large%20Language%20Models%20rely%20critically%20on%20the%20KV%20cache%20to%0Aefficiently%20handle%20extended%20contexts%20during%20the%20decode%20phase.%20Yet%2C%20the%20size%20of%0Athe%20KV%20cache%20grows%20proportionally%20with%20the%20input%20length%2C%20burdening%20both%20memory%0Abandwidth%20and%20capacity%20as%20decoding%20progresses.%20To%20address%20this%20challenge%2C%20we%0Apresent%20RocketKV%2C%20a%20training-free%20KV%20cache%20compression%20strategy%20containing%20two%0Aconsecutive%20stages.%20In%20the%20first%20stage%2C%20it%20performs%20coarse-grain%20permanent%20KV%0Acache%20eviction%20on%20the%20input%20sequence%20tokens.%20In%20the%20second%20stage%2C%20it%20adopts%20a%0Ahybrid%20sparse%20attention%20method%20to%20conduct%20fine-grain%20top-k%20sparse%20attention%2C%0Aapproximating%20the%20attention%20scores%20by%20leveraging%20both%20head%20and%20sequence%0Adimensionality%20reductions.%20We%20show%20that%20RocketKV%20provides%20a%20compression%20ratio%0Aof%20up%20to%20400%24%5Ctimes%24%2C%20end-to-end%20speedup%20of%20up%20to%203.7%24%5Ctimes%24%20as%20well%20as%20peak%0Amemory%20reduction%20of%20up%20to%2032.6%25%20in%20the%20decode%20phase%20on%20an%20NVIDIA%20A100%20GPU%0Acompared%20to%20the%20full%20KV%20cache%20baseline%2C%20while%20achieving%20negligible%20accuracy%0Aloss%20on%20a%20variety%20of%20long-context%20tasks.%20We%20also%20propose%20a%20variant%20of%20RocketKV%0Afor%20multi-turn%20scenarios%2C%20which%20consistently%20outperforms%20other%20existing%20methods%0Aand%20achieves%20accuracy%20nearly%20on%20par%20with%20an%20oracle%20top-k%20attention%20scheme.%20The%0Asource%20code%20is%20available%20here%3A%20https%3A//github.com/NVlabs/RocketKV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14051v3&entry.124074799=Read"},
{"title": "FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via\n  Reinforcement Learning", "author": "Dongcheng Cao and Jin Zhou and Xian Wang and Shuo Li", "abstract": "  Agile flight for the quadrotor cable-suspended payload system is a formidable\nchallenge due to its underactuated, highly nonlinear, and hybrid dynamics.\nTraditional optimization-based methods often struggle with high computational\ncosts and the complexities of cable mode transitions, limiting their real-time\napplicability and maneuverability exploitation. In this letter, we present\nFLARE, a reinforcement learning (RL) framework that directly learns agile\nnavigation policy from high-fidelity simulation. Our method is validated across\nthree designed challenging scenarios, notably outperforming a state-of-the-art\noptimization-based approach by a 3x speedup during gate traversal maneuvers.\nFurthermore, the learned policies achieve successful zero-shot sim-to-real\ntransfer, demonstrating remarkable agility and safety in real-world\nexperiments, running in real time on an onboard computer.\n", "link": "http://arxiv.org/abs/2508.09797v1", "date": "2025-08-13", "relevancy": 1.5126, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5248}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5034}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLARE%3A%20Agile%20Flights%20for%20Quadrotor%20Cable-Suspended%20Payload%20System%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20FLARE%3A%20Agile%20Flights%20for%20Quadrotor%20Cable-Suspended%20Payload%20System%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Dongcheng%20Cao%20and%20Jin%20Zhou%20and%20Xian%20Wang%20and%20Shuo%20Li%0AAbstract%3A%20%20%20Agile%20flight%20for%20the%20quadrotor%20cable-suspended%20payload%20system%20is%20a%20formidable%0Achallenge%20due%20to%20its%20underactuated%2C%20highly%20nonlinear%2C%20and%20hybrid%20dynamics.%0ATraditional%20optimization-based%20methods%20often%20struggle%20with%20high%20computational%0Acosts%20and%20the%20complexities%20of%20cable%20mode%20transitions%2C%20limiting%20their%20real-time%0Aapplicability%20and%20maneuverability%20exploitation.%20In%20this%20letter%2C%20we%20present%0AFLARE%2C%20a%20reinforcement%20learning%20%28RL%29%20framework%20that%20directly%20learns%20agile%0Anavigation%20policy%20from%20high-fidelity%20simulation.%20Our%20method%20is%20validated%20across%0Athree%20designed%20challenging%20scenarios%2C%20notably%20outperforming%20a%20state-of-the-art%0Aoptimization-based%20approach%20by%20a%203x%20speedup%20during%20gate%20traversal%20maneuvers.%0AFurthermore%2C%20the%20learned%20policies%20achieve%20successful%20zero-shot%20sim-to-real%0Atransfer%2C%20demonstrating%20remarkable%20agility%20and%20safety%20in%20real-world%0Aexperiments%2C%20running%20in%20real%20time%20on%20an%20onboard%20computer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLARE%253A%2520Agile%2520Flights%2520for%2520Quadrotor%2520Cable-Suspended%2520Payload%2520System%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DDongcheng%2520Cao%2520and%2520Jin%2520Zhou%2520and%2520Xian%2520Wang%2520and%2520Shuo%2520Li%26entry.1292438233%3D%2520%2520Agile%2520flight%2520for%2520the%2520quadrotor%2520cable-suspended%2520payload%2520system%2520is%2520a%2520formidable%250Achallenge%2520due%2520to%2520its%2520underactuated%252C%2520highly%2520nonlinear%252C%2520and%2520hybrid%2520dynamics.%250ATraditional%2520optimization-based%2520methods%2520often%2520struggle%2520with%2520high%2520computational%250Acosts%2520and%2520the%2520complexities%2520of%2520cable%2520mode%2520transitions%252C%2520limiting%2520their%2520real-time%250Aapplicability%2520and%2520maneuverability%2520exploitation.%2520In%2520this%2520letter%252C%2520we%2520present%250AFLARE%252C%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520framework%2520that%2520directly%2520learns%2520agile%250Anavigation%2520policy%2520from%2520high-fidelity%2520simulation.%2520Our%2520method%2520is%2520validated%2520across%250Athree%2520designed%2520challenging%2520scenarios%252C%2520notably%2520outperforming%2520a%2520state-of-the-art%250Aoptimization-based%2520approach%2520by%2520a%25203x%2520speedup%2520during%2520gate%2520traversal%2520maneuvers.%250AFurthermore%252C%2520the%2520learned%2520policies%2520achieve%2520successful%2520zero-shot%2520sim-to-real%250Atransfer%252C%2520demonstrating%2520remarkable%2520agility%2520and%2520safety%2520in%2520real-world%250Aexperiments%252C%2520running%2520in%2520real%2520time%2520on%2520an%2520onboard%2520computer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLARE%3A%20Agile%20Flights%20for%20Quadrotor%20Cable-Suspended%20Payload%20System%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Dongcheng%20Cao%20and%20Jin%20Zhou%20and%20Xian%20Wang%20and%20Shuo%20Li&entry.1292438233=%20%20Agile%20flight%20for%20the%20quadrotor%20cable-suspended%20payload%20system%20is%20a%20formidable%0Achallenge%20due%20to%20its%20underactuated%2C%20highly%20nonlinear%2C%20and%20hybrid%20dynamics.%0ATraditional%20optimization-based%20methods%20often%20struggle%20with%20high%20computational%0Acosts%20and%20the%20complexities%20of%20cable%20mode%20transitions%2C%20limiting%20their%20real-time%0Aapplicability%20and%20maneuverability%20exploitation.%20In%20this%20letter%2C%20we%20present%0AFLARE%2C%20a%20reinforcement%20learning%20%28RL%29%20framework%20that%20directly%20learns%20agile%0Anavigation%20policy%20from%20high-fidelity%20simulation.%20Our%20method%20is%20validated%20across%0Athree%20designed%20challenging%20scenarios%2C%20notably%20outperforming%20a%20state-of-the-art%0Aoptimization-based%20approach%20by%20a%203x%20speedup%20during%20gate%20traversal%20maneuvers.%0AFurthermore%2C%20the%20learned%20policies%20achieve%20successful%20zero-shot%20sim-to-real%0Atransfer%2C%20demonstrating%20remarkable%20agility%20and%20safety%20in%20real-world%0Aexperiments%2C%20running%20in%20real%20time%20on%20an%20onboard%20computer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09797v1&entry.124074799=Read"},
{"title": "SpectralEarth: Training Hyperspectral Foundation Models at Scale", "author": "Nassim Ait Ali Braham and Conrad M Albrecht and Julien Mairal and Jocelyn Chanussot and Yi Wang and Xiao Xiang Zhu", "abstract": "  Foundation models have triggered a paradigm shift in computer vision and are\nincreasingly being adopted in remote sensing, particularly for multispectral\nimagery. Yet, their potential in hyperspectral imaging (HSI) remains untapped\ndue to the absence of comprehensive and globally representative hyperspectral\ndatasets. To close this gap, we introduce SpectralEarth, a large-scale\nmultitemporal dataset designed to pretrain hyperspectral foundation models\nleveraging data from the environmental mapping and analysis program (EnMAP).\nSpectralEarth comprises 538 974 image patches covering 415 153 unique locations\nfrom 11 636 globally distributed EnMAP scenes spanning two years of archive. In\naddition, 17.5% of these locations include multiple timestamps, enabling\nmultitemporal HSI analysis. Utilizing state-of-the-art self-supervised learning\nalgorithms, we pretrain a series of foundation models on SpectralEarth,\nintegrating a spectral adapter into classical vision backbones to accommodate\nthe unique characteristics of HSI. In tandem, we construct nine downstream\ndatasets for land-cover, crop-type mapping, and tree-species classification,\nproviding benchmarks for model evaluation. Experimental results support the\nversatility of our models and their generalizability across different tasks and\nsensors. We also highlight computational efficiency during model fine-tuning.\n", "link": "http://arxiv.org/abs/2408.08447v2", "date": "2025-08-13", "relevancy": 2.0533, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5173}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5173}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpectralEarth%3A%20Training%20Hyperspectral%20Foundation%20Models%20at%20Scale&body=Title%3A%20SpectralEarth%3A%20Training%20Hyperspectral%20Foundation%20Models%20at%20Scale%0AAuthor%3A%20Nassim%20Ait%20Ali%20Braham%20and%20Conrad%20M%20Albrecht%20and%20Julien%20Mairal%20and%20Jocelyn%20Chanussot%20and%20Yi%20Wang%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20%20%20Foundation%20models%20have%20triggered%20a%20paradigm%20shift%20in%20computer%20vision%20and%20are%0Aincreasingly%20being%20adopted%20in%20remote%20sensing%2C%20particularly%20for%20multispectral%0Aimagery.%20Yet%2C%20their%20potential%20in%20hyperspectral%20imaging%20%28HSI%29%20remains%20untapped%0Adue%20to%20the%20absence%20of%20comprehensive%20and%20globally%20representative%20hyperspectral%0Adatasets.%20To%20close%20this%20gap%2C%20we%20introduce%20SpectralEarth%2C%20a%20large-scale%0Amultitemporal%20dataset%20designed%20to%20pretrain%20hyperspectral%20foundation%20models%0Aleveraging%20data%20from%20the%20environmental%20mapping%20and%20analysis%20program%20%28EnMAP%29.%0ASpectralEarth%20comprises%20538%20974%20image%20patches%20covering%20415%20153%20unique%20locations%0Afrom%2011%20636%20globally%20distributed%20EnMAP%20scenes%20spanning%20two%20years%20of%20archive.%20In%0Aaddition%2C%2017.5%25%20of%20these%20locations%20include%20multiple%20timestamps%2C%20enabling%0Amultitemporal%20HSI%20analysis.%20Utilizing%20state-of-the-art%20self-supervised%20learning%0Aalgorithms%2C%20we%20pretrain%20a%20series%20of%20foundation%20models%20on%20SpectralEarth%2C%0Aintegrating%20a%20spectral%20adapter%20into%20classical%20vision%20backbones%20to%20accommodate%0Athe%20unique%20characteristics%20of%20HSI.%20In%20tandem%2C%20we%20construct%20nine%20downstream%0Adatasets%20for%20land-cover%2C%20crop-type%20mapping%2C%20and%20tree-species%20classification%2C%0Aproviding%20benchmarks%20for%20model%20evaluation.%20Experimental%20results%20support%20the%0Aversatility%20of%20our%20models%20and%20their%20generalizability%20across%20different%20tasks%20and%0Asensors.%20We%20also%20highlight%20computational%20efficiency%20during%20model%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08447v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectralEarth%253A%2520Training%2520Hyperspectral%2520Foundation%2520Models%2520at%2520Scale%26entry.906535625%3DNassim%2520Ait%2520Ali%2520Braham%2520and%2520Conrad%2520M%2520Albrecht%2520and%2520Julien%2520Mairal%2520and%2520Jocelyn%2520Chanussot%2520and%2520Yi%2520Wang%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520triggered%2520a%2520paradigm%2520shift%2520in%2520computer%2520vision%2520and%2520are%250Aincreasingly%2520being%2520adopted%2520in%2520remote%2520sensing%252C%2520particularly%2520for%2520multispectral%250Aimagery.%2520Yet%252C%2520their%2520potential%2520in%2520hyperspectral%2520imaging%2520%2528HSI%2529%2520remains%2520untapped%250Adue%2520to%2520the%2520absence%2520of%2520comprehensive%2520and%2520globally%2520representative%2520hyperspectral%250Adatasets.%2520To%2520close%2520this%2520gap%252C%2520we%2520introduce%2520SpectralEarth%252C%2520a%2520large-scale%250Amultitemporal%2520dataset%2520designed%2520to%2520pretrain%2520hyperspectral%2520foundation%2520models%250Aleveraging%2520data%2520from%2520the%2520environmental%2520mapping%2520and%2520analysis%2520program%2520%2528EnMAP%2529.%250ASpectralEarth%2520comprises%2520538%2520974%2520image%2520patches%2520covering%2520415%2520153%2520unique%2520locations%250Afrom%252011%2520636%2520globally%2520distributed%2520EnMAP%2520scenes%2520spanning%2520two%2520years%2520of%2520archive.%2520In%250Aaddition%252C%252017.5%2525%2520of%2520these%2520locations%2520include%2520multiple%2520timestamps%252C%2520enabling%250Amultitemporal%2520HSI%2520analysis.%2520Utilizing%2520state-of-the-art%2520self-supervised%2520learning%250Aalgorithms%252C%2520we%2520pretrain%2520a%2520series%2520of%2520foundation%2520models%2520on%2520SpectralEarth%252C%250Aintegrating%2520a%2520spectral%2520adapter%2520into%2520classical%2520vision%2520backbones%2520to%2520accommodate%250Athe%2520unique%2520characteristics%2520of%2520HSI.%2520In%2520tandem%252C%2520we%2520construct%2520nine%2520downstream%250Adatasets%2520for%2520land-cover%252C%2520crop-type%2520mapping%252C%2520and%2520tree-species%2520classification%252C%250Aproviding%2520benchmarks%2520for%2520model%2520evaluation.%2520Experimental%2520results%2520support%2520the%250Aversatility%2520of%2520our%2520models%2520and%2520their%2520generalizability%2520across%2520different%2520tasks%2520and%250Asensors.%2520We%2520also%2520highlight%2520computational%2520efficiency%2520during%2520model%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08447v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpectralEarth%3A%20Training%20Hyperspectral%20Foundation%20Models%20at%20Scale&entry.906535625=Nassim%20Ait%20Ali%20Braham%20and%20Conrad%20M%20Albrecht%20and%20Julien%20Mairal%20and%20Jocelyn%20Chanussot%20and%20Yi%20Wang%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%20Foundation%20models%20have%20triggered%20a%20paradigm%20shift%20in%20computer%20vision%20and%20are%0Aincreasingly%20being%20adopted%20in%20remote%20sensing%2C%20particularly%20for%20multispectral%0Aimagery.%20Yet%2C%20their%20potential%20in%20hyperspectral%20imaging%20%28HSI%29%20remains%20untapped%0Adue%20to%20the%20absence%20of%20comprehensive%20and%20globally%20representative%20hyperspectral%0Adatasets.%20To%20close%20this%20gap%2C%20we%20introduce%20SpectralEarth%2C%20a%20large-scale%0Amultitemporal%20dataset%20designed%20to%20pretrain%20hyperspectral%20foundation%20models%0Aleveraging%20data%20from%20the%20environmental%20mapping%20and%20analysis%20program%20%28EnMAP%29.%0ASpectralEarth%20comprises%20538%20974%20image%20patches%20covering%20415%20153%20unique%20locations%0Afrom%2011%20636%20globally%20distributed%20EnMAP%20scenes%20spanning%20two%20years%20of%20archive.%20In%0Aaddition%2C%2017.5%25%20of%20these%20locations%20include%20multiple%20timestamps%2C%20enabling%0Amultitemporal%20HSI%20analysis.%20Utilizing%20state-of-the-art%20self-supervised%20learning%0Aalgorithms%2C%20we%20pretrain%20a%20series%20of%20foundation%20models%20on%20SpectralEarth%2C%0Aintegrating%20a%20spectral%20adapter%20into%20classical%20vision%20backbones%20to%20accommodate%0Athe%20unique%20characteristics%20of%20HSI.%20In%20tandem%2C%20we%20construct%20nine%20downstream%0Adatasets%20for%20land-cover%2C%20crop-type%20mapping%2C%20and%20tree-species%20classification%2C%0Aproviding%20benchmarks%20for%20model%20evaluation.%20Experimental%20results%20support%20the%0Aversatility%20of%20our%20models%20and%20their%20generalizability%20across%20different%20tasks%20and%0Asensors.%20We%20also%20highlight%20computational%20efficiency%20during%20model%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08447v2&entry.124074799=Read"},
{"title": "BeyondMimic: From Motion Tracking to Versatile Humanoid Control via\n  Guided Diffusion", "author": "Qiayuan Liao and Takara E. Truong and Xiaoyu Huang and Guy Tevet and Koushil Sreenath and C. Karen Liu", "abstract": "  Learning skills from human motions offers a promising path toward\ngeneralizable policies for versatile humanoid whole-body control, yet two key\ncornerstones are missing: (1) a high-quality motion tracking framework that\nfaithfully transforms large-scale kinematic references into robust and\nextremely dynamic motions on real hardware, and (2) a distillation approach\nthat can effectively learn these motion primitives and compose them to solve\ndownstream tasks. We address these gaps with BeyondMimic, a real-world\nframework to learn from human motions for versatile and naturalistic humanoid\ncontrol via guided diffusion. Our framework provides a motion tracking pipeline\ncapable of challenging skills such as jumping spins, sprinting, and cartwheels\nwith state-of-the-art motion quality. Moving beyond simply mimicking existing\nmotions, we further introduce a unified diffusion policy that enables zero-shot\ntask-specific control at test time using simple cost functions. Deployed on\nhardware, BeyondMimic performs diverse tasks at test time, including waypoint\nnavigation, joystick teleoperation, and obstacle avoidance, bridging\nsim-to-real motion tracking and flexible synthesis of human motion primitives\nfor whole-body control. https://beyondmimic.github.io/.\n", "link": "http://arxiv.org/abs/2508.08241v3", "date": "2025-08-13", "relevancy": 2.0088, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7291}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6126}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BeyondMimic%3A%20From%20Motion%20Tracking%20to%20Versatile%20Humanoid%20Control%20via%0A%20%20Guided%20Diffusion&body=Title%3A%20BeyondMimic%3A%20From%20Motion%20Tracking%20to%20Versatile%20Humanoid%20Control%20via%0A%20%20Guided%20Diffusion%0AAuthor%3A%20Qiayuan%20Liao%20and%20Takara%20E.%20Truong%20and%20Xiaoyu%20Huang%20and%20Guy%20Tevet%20and%20Koushil%20Sreenath%20and%20C.%20Karen%20Liu%0AAbstract%3A%20%20%20Learning%20skills%20from%20human%20motions%20offers%20a%20promising%20path%20toward%0Ageneralizable%20policies%20for%20versatile%20humanoid%20whole-body%20control%2C%20yet%20two%20key%0Acornerstones%20are%20missing%3A%20%281%29%20a%20high-quality%20motion%20tracking%20framework%20that%0Afaithfully%20transforms%20large-scale%20kinematic%20references%20into%20robust%20and%0Aextremely%20dynamic%20motions%20on%20real%20hardware%2C%20and%20%282%29%20a%20distillation%20approach%0Athat%20can%20effectively%20learn%20these%20motion%20primitives%20and%20compose%20them%20to%20solve%0Adownstream%20tasks.%20We%20address%20these%20gaps%20with%20BeyondMimic%2C%20a%20real-world%0Aframework%20to%20learn%20from%20human%20motions%20for%20versatile%20and%20naturalistic%20humanoid%0Acontrol%20via%20guided%20diffusion.%20Our%20framework%20provides%20a%20motion%20tracking%20pipeline%0Acapable%20of%20challenging%20skills%20such%20as%20jumping%20spins%2C%20sprinting%2C%20and%20cartwheels%0Awith%20state-of-the-art%20motion%20quality.%20Moving%20beyond%20simply%20mimicking%20existing%0Amotions%2C%20we%20further%20introduce%20a%20unified%20diffusion%20policy%20that%20enables%20zero-shot%0Atask-specific%20control%20at%20test%20time%20using%20simple%20cost%20functions.%20Deployed%20on%0Ahardware%2C%20BeyondMimic%20performs%20diverse%20tasks%20at%20test%20time%2C%20including%20waypoint%0Anavigation%2C%20joystick%20teleoperation%2C%20and%20obstacle%20avoidance%2C%20bridging%0Asim-to-real%20motion%20tracking%20and%20flexible%20synthesis%20of%20human%20motion%20primitives%0Afor%20whole-body%20control.%20https%3A//beyondmimic.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08241v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyondMimic%253A%2520From%2520Motion%2520Tracking%2520to%2520Versatile%2520Humanoid%2520Control%2520via%250A%2520%2520Guided%2520Diffusion%26entry.906535625%3DQiayuan%2520Liao%2520and%2520Takara%2520E.%2520Truong%2520and%2520Xiaoyu%2520Huang%2520and%2520Guy%2520Tevet%2520and%2520Koushil%2520Sreenath%2520and%2520C.%2520Karen%2520Liu%26entry.1292438233%3D%2520%2520Learning%2520skills%2520from%2520human%2520motions%2520offers%2520a%2520promising%2520path%2520toward%250Ageneralizable%2520policies%2520for%2520versatile%2520humanoid%2520whole-body%2520control%252C%2520yet%2520two%2520key%250Acornerstones%2520are%2520missing%253A%2520%25281%2529%2520a%2520high-quality%2520motion%2520tracking%2520framework%2520that%250Afaithfully%2520transforms%2520large-scale%2520kinematic%2520references%2520into%2520robust%2520and%250Aextremely%2520dynamic%2520motions%2520on%2520real%2520hardware%252C%2520and%2520%25282%2529%2520a%2520distillation%2520approach%250Athat%2520can%2520effectively%2520learn%2520these%2520motion%2520primitives%2520and%2520compose%2520them%2520to%2520solve%250Adownstream%2520tasks.%2520We%2520address%2520these%2520gaps%2520with%2520BeyondMimic%252C%2520a%2520real-world%250Aframework%2520to%2520learn%2520from%2520human%2520motions%2520for%2520versatile%2520and%2520naturalistic%2520humanoid%250Acontrol%2520via%2520guided%2520diffusion.%2520Our%2520framework%2520provides%2520a%2520motion%2520tracking%2520pipeline%250Acapable%2520of%2520challenging%2520skills%2520such%2520as%2520jumping%2520spins%252C%2520sprinting%252C%2520and%2520cartwheels%250Awith%2520state-of-the-art%2520motion%2520quality.%2520Moving%2520beyond%2520simply%2520mimicking%2520existing%250Amotions%252C%2520we%2520further%2520introduce%2520a%2520unified%2520diffusion%2520policy%2520that%2520enables%2520zero-shot%250Atask-specific%2520control%2520at%2520test%2520time%2520using%2520simple%2520cost%2520functions.%2520Deployed%2520on%250Ahardware%252C%2520BeyondMimic%2520performs%2520diverse%2520tasks%2520at%2520test%2520time%252C%2520including%2520waypoint%250Anavigation%252C%2520joystick%2520teleoperation%252C%2520and%2520obstacle%2520avoidance%252C%2520bridging%250Asim-to-real%2520motion%2520tracking%2520and%2520flexible%2520synthesis%2520of%2520human%2520motion%2520primitives%250Afor%2520whole-body%2520control.%2520https%253A//beyondmimic.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08241v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BeyondMimic%3A%20From%20Motion%20Tracking%20to%20Versatile%20Humanoid%20Control%20via%0A%20%20Guided%20Diffusion&entry.906535625=Qiayuan%20Liao%20and%20Takara%20E.%20Truong%20and%20Xiaoyu%20Huang%20and%20Guy%20Tevet%20and%20Koushil%20Sreenath%20and%20C.%20Karen%20Liu&entry.1292438233=%20%20Learning%20skills%20from%20human%20motions%20offers%20a%20promising%20path%20toward%0Ageneralizable%20policies%20for%20versatile%20humanoid%20whole-body%20control%2C%20yet%20two%20key%0Acornerstones%20are%20missing%3A%20%281%29%20a%20high-quality%20motion%20tracking%20framework%20that%0Afaithfully%20transforms%20large-scale%20kinematic%20references%20into%20robust%20and%0Aextremely%20dynamic%20motions%20on%20real%20hardware%2C%20and%20%282%29%20a%20distillation%20approach%0Athat%20can%20effectively%20learn%20these%20motion%20primitives%20and%20compose%20them%20to%20solve%0Adownstream%20tasks.%20We%20address%20these%20gaps%20with%20BeyondMimic%2C%20a%20real-world%0Aframework%20to%20learn%20from%20human%20motions%20for%20versatile%20and%20naturalistic%20humanoid%0Acontrol%20via%20guided%20diffusion.%20Our%20framework%20provides%20a%20motion%20tracking%20pipeline%0Acapable%20of%20challenging%20skills%20such%20as%20jumping%20spins%2C%20sprinting%2C%20and%20cartwheels%0Awith%20state-of-the-art%20motion%20quality.%20Moving%20beyond%20simply%20mimicking%20existing%0Amotions%2C%20we%20further%20introduce%20a%20unified%20diffusion%20policy%20that%20enables%20zero-shot%0Atask-specific%20control%20at%20test%20time%20using%20simple%20cost%20functions.%20Deployed%20on%0Ahardware%2C%20BeyondMimic%20performs%20diverse%20tasks%20at%20test%20time%2C%20including%20waypoint%0Anavigation%2C%20joystick%20teleoperation%2C%20and%20obstacle%20avoidance%2C%20bridging%0Asim-to-real%20motion%20tracking%20and%20flexible%20synthesis%20of%20human%20motion%20primitives%0Afor%20whole-body%20control.%20https%3A//beyondmimic.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08241v3&entry.124074799=Read"},
{"title": "Prototype Training with Dual Pseudo-Inverse and Optimized Hidden\n  Activations", "author": "Mauro Tucci", "abstract": "  We present Proto-PINV+H, a fast training paradigm that combines closed-form\nweight computation with gradient-based optimisation of a small set of synthetic\ninputs, soft labels, and-crucially-hidden activations. At each iteration we\nrecompute all weight matrices in closed form via two (or more)\nridge-regularised pseudo-inverse solves, while updating only the prototypes\nwith Adam. The trainable degrees of freedom are thus shifted from weight space\nto data/activation space. On MNIST (60k train, 10k test) and Fashion-MNIST (60k\ntrain, 10k test), our method reaches 97.8% and 89.3% test accuracy on the\nofficial 10k test sets, respectively, in 3.9s--4.5s using approximately 130k\ntrainable parameters and only 250 epochs on an RTX 5060 (16GB). We provide a\nmulti-layer extension (optimised activations at each hidden stage), learnable\nridge parameters, optional PCA/PLS projections, and theory linking the\ncondition number of prototype matrices to generalisation. The approach yields\nfavourable accuracy--speed--size trade-offs against ELM, random-feature ridge,\nand shallow MLPs trained by back-propagation.\n", "link": "http://arxiv.org/abs/2508.09787v1", "date": "2025-08-13", "relevancy": 2.0039, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5164}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5012}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototype%20Training%20with%20Dual%20Pseudo-Inverse%20and%20Optimized%20Hidden%0A%20%20Activations&body=Title%3A%20Prototype%20Training%20with%20Dual%20Pseudo-Inverse%20and%20Optimized%20Hidden%0A%20%20Activations%0AAuthor%3A%20Mauro%20Tucci%0AAbstract%3A%20%20%20We%20present%20Proto-PINV%2BH%2C%20a%20fast%20training%20paradigm%20that%20combines%20closed-form%0Aweight%20computation%20with%20gradient-based%20optimisation%20of%20a%20small%20set%20of%20synthetic%0Ainputs%2C%20soft%20labels%2C%20and-crucially-hidden%20activations.%20At%20each%20iteration%20we%0Arecompute%20all%20weight%20matrices%20in%20closed%20form%20via%20two%20%28or%20more%29%0Aridge-regularised%20pseudo-inverse%20solves%2C%20while%20updating%20only%20the%20prototypes%0Awith%20Adam.%20The%20trainable%20degrees%20of%20freedom%20are%20thus%20shifted%20from%20weight%20space%0Ato%20data/activation%20space.%20On%20MNIST%20%2860k%20train%2C%2010k%20test%29%20and%20Fashion-MNIST%20%2860k%0Atrain%2C%2010k%20test%29%2C%20our%20method%20reaches%2097.8%25%20and%2089.3%25%20test%20accuracy%20on%20the%0Aofficial%2010k%20test%20sets%2C%20respectively%2C%20in%203.9s--4.5s%20using%20approximately%20130k%0Atrainable%20parameters%20and%20only%20250%20epochs%20on%20an%20RTX%205060%20%2816GB%29.%20We%20provide%20a%0Amulti-layer%20extension%20%28optimised%20activations%20at%20each%20hidden%20stage%29%2C%20learnable%0Aridge%20parameters%2C%20optional%20PCA/PLS%20projections%2C%20and%20theory%20linking%20the%0Acondition%20number%20of%20prototype%20matrices%20to%20generalisation.%20The%20approach%20yields%0Afavourable%20accuracy--speed--size%20trade-offs%20against%20ELM%2C%20random-feature%20ridge%2C%0Aand%20shallow%20MLPs%20trained%20by%20back-propagation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototype%2520Training%2520with%2520Dual%2520Pseudo-Inverse%2520and%2520Optimized%2520Hidden%250A%2520%2520Activations%26entry.906535625%3DMauro%2520Tucci%26entry.1292438233%3D%2520%2520We%2520present%2520Proto-PINV%252BH%252C%2520a%2520fast%2520training%2520paradigm%2520that%2520combines%2520closed-form%250Aweight%2520computation%2520with%2520gradient-based%2520optimisation%2520of%2520a%2520small%2520set%2520of%2520synthetic%250Ainputs%252C%2520soft%2520labels%252C%2520and-crucially-hidden%2520activations.%2520At%2520each%2520iteration%2520we%250Arecompute%2520all%2520weight%2520matrices%2520in%2520closed%2520form%2520via%2520two%2520%2528or%2520more%2529%250Aridge-regularised%2520pseudo-inverse%2520solves%252C%2520while%2520updating%2520only%2520the%2520prototypes%250Awith%2520Adam.%2520The%2520trainable%2520degrees%2520of%2520freedom%2520are%2520thus%2520shifted%2520from%2520weight%2520space%250Ato%2520data/activation%2520space.%2520On%2520MNIST%2520%252860k%2520train%252C%252010k%2520test%2529%2520and%2520Fashion-MNIST%2520%252860k%250Atrain%252C%252010k%2520test%2529%252C%2520our%2520method%2520reaches%252097.8%2525%2520and%252089.3%2525%2520test%2520accuracy%2520on%2520the%250Aofficial%252010k%2520test%2520sets%252C%2520respectively%252C%2520in%25203.9s--4.5s%2520using%2520approximately%2520130k%250Atrainable%2520parameters%2520and%2520only%2520250%2520epochs%2520on%2520an%2520RTX%25205060%2520%252816GB%2529.%2520We%2520provide%2520a%250Amulti-layer%2520extension%2520%2528optimised%2520activations%2520at%2520each%2520hidden%2520stage%2529%252C%2520learnable%250Aridge%2520parameters%252C%2520optional%2520PCA/PLS%2520projections%252C%2520and%2520theory%2520linking%2520the%250Acondition%2520number%2520of%2520prototype%2520matrices%2520to%2520generalisation.%2520The%2520approach%2520yields%250Afavourable%2520accuracy--speed--size%2520trade-offs%2520against%2520ELM%252C%2520random-feature%2520ridge%252C%250Aand%2520shallow%2520MLPs%2520trained%2520by%2520back-propagation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototype%20Training%20with%20Dual%20Pseudo-Inverse%20and%20Optimized%20Hidden%0A%20%20Activations&entry.906535625=Mauro%20Tucci&entry.1292438233=%20%20We%20present%20Proto-PINV%2BH%2C%20a%20fast%20training%20paradigm%20that%20combines%20closed-form%0Aweight%20computation%20with%20gradient-based%20optimisation%20of%20a%20small%20set%20of%20synthetic%0Ainputs%2C%20soft%20labels%2C%20and-crucially-hidden%20activations.%20At%20each%20iteration%20we%0Arecompute%20all%20weight%20matrices%20in%20closed%20form%20via%20two%20%28or%20more%29%0Aridge-regularised%20pseudo-inverse%20solves%2C%20while%20updating%20only%20the%20prototypes%0Awith%20Adam.%20The%20trainable%20degrees%20of%20freedom%20are%20thus%20shifted%20from%20weight%20space%0Ato%20data/activation%20space.%20On%20MNIST%20%2860k%20train%2C%2010k%20test%29%20and%20Fashion-MNIST%20%2860k%0Atrain%2C%2010k%20test%29%2C%20our%20method%20reaches%2097.8%25%20and%2089.3%25%20test%20accuracy%20on%20the%0Aofficial%2010k%20test%20sets%2C%20respectively%2C%20in%203.9s--4.5s%20using%20approximately%20130k%0Atrainable%20parameters%20and%20only%20250%20epochs%20on%20an%20RTX%205060%20%2816GB%29.%20We%20provide%20a%0Amulti-layer%20extension%20%28optimised%20activations%20at%20each%20hidden%20stage%29%2C%20learnable%0Aridge%20parameters%2C%20optional%20PCA/PLS%20projections%2C%20and%20theory%20linking%20the%0Acondition%20number%20of%20prototype%20matrices%20to%20generalisation.%20The%20approach%20yields%0Afavourable%20accuracy--speed--size%20trade-offs%20against%20ELM%2C%20random-feature%20ridge%2C%0Aand%20shallow%20MLPs%20trained%20by%20back-propagation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09787v1&entry.124074799=Read"},
{"title": "Beyond Scaling Law: A Data-Efficient Distillation Framework for\n  Reasoning", "author": "Xiaojun Wu and Xiaoguang Jiang and Huiyang Li and Jucai Zhai and Dengfeng Liu and Qiaobo Hao and Huang Liu and Zhiguo Yang and Ji Xie and Ninglun Gu and Jin Yang and Kailai Zhang and Yelun Bao and Jun Wang", "abstract": "  Large language models (LLMs) demonstrate remarkable reasoning capabilities in\ntasks such as algorithmic coding and mathematical problem-solving. Recent\nmethods have improved reasoning through expanded corpus and multistage training\ncombining reinforcement learning and supervised fine-tuning. Although some\nmethods suggest that small but targeted dataset can incentivize reasoning via\nonly distillation, a reasoning scaling laws is still taking shape, increasing\ncomputational costs. To address this, we propose a data-efficient distillation\nframework (DED) that optimizes the Pareto frontier of reasoning distillation.\nInspired by the on-policy learning and diverse roll-out strategies of\nreinforcement learning, the key idea of our approach is threefold: (1) We\nidentify that benchmark scores alone do not determine an effective teacher\nmodel. Through comprehensive comparisons of leading reasoning LLMs, we develop\na method to select an optimal teacher model. (2) While scaling distillation can\nenhance reasoning, it often degrades out-of-domain performance. A carefully\ncurated, smaller corpus achieves a balanced trade-off between in-domain and\nout-of-domain capabilities. (3) Diverse reasoning trajectories encourage the\nstudent model to develop robust reasoning skills. We validate our method\nthrough evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and\ncode generation (LiveCodeBench), achieving state-of-the-art results with only\n0.8k carefully curated examples, bypassing the need for extensive scaling. Our\nsystematic analysis demonstrates that DED outperforms existing methods by\nconsidering factors beyond superficial hardness, token length, or teacher model\ncapability. This work offers a practical and efficient pathway to advanced\nreasoning while preserving general capabilities.\n", "link": "http://arxiv.org/abs/2508.09883v1", "date": "2025-08-13", "relevancy": 1.4892, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4939}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Scaling%20Law%3A%20A%20Data-Efficient%20Distillation%20Framework%20for%0A%20%20Reasoning&body=Title%3A%20Beyond%20Scaling%20Law%3A%20A%20Data-Efficient%20Distillation%20Framework%20for%0A%20%20Reasoning%0AAuthor%3A%20Xiaojun%20Wu%20and%20Xiaoguang%20Jiang%20and%20Huiyang%20Li%20and%20Jucai%20Zhai%20and%20Dengfeng%20Liu%20and%20Qiaobo%20Hao%20and%20Huang%20Liu%20and%20Zhiguo%20Yang%20and%20Ji%20Xie%20and%20Ninglun%20Gu%20and%20Jin%20Yang%20and%20Kailai%20Zhang%20and%20Yelun%20Bao%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20remarkable%20reasoning%20capabilities%20in%0Atasks%20such%20as%20algorithmic%20coding%20and%20mathematical%20problem-solving.%20Recent%0Amethods%20have%20improved%20reasoning%20through%20expanded%20corpus%20and%20multistage%20training%0Acombining%20reinforcement%20learning%20and%20supervised%20fine-tuning.%20Although%20some%0Amethods%20suggest%20that%20small%20but%20targeted%20dataset%20can%20incentivize%20reasoning%20via%0Aonly%20distillation%2C%20a%20reasoning%20scaling%20laws%20is%20still%20taking%20shape%2C%20increasing%0Acomputational%20costs.%20To%20address%20this%2C%20we%20propose%20a%20data-efficient%20distillation%0Aframework%20%28DED%29%20that%20optimizes%20the%20Pareto%20frontier%20of%20reasoning%20distillation.%0AInspired%20by%20the%20on-policy%20learning%20and%20diverse%20roll-out%20strategies%20of%0Areinforcement%20learning%2C%20the%20key%20idea%20of%20our%20approach%20is%20threefold%3A%20%281%29%20We%0Aidentify%20that%20benchmark%20scores%20alone%20do%20not%20determine%20an%20effective%20teacher%0Amodel.%20Through%20comprehensive%20comparisons%20of%20leading%20reasoning%20LLMs%2C%20we%20develop%0Aa%20method%20to%20select%20an%20optimal%20teacher%20model.%20%282%29%20While%20scaling%20distillation%20can%0Aenhance%20reasoning%2C%20it%20often%20degrades%20out-of-domain%20performance.%20A%20carefully%0Acurated%2C%20smaller%20corpus%20achieves%20a%20balanced%20trade-off%20between%20in-domain%20and%0Aout-of-domain%20capabilities.%20%283%29%20Diverse%20reasoning%20trajectories%20encourage%20the%0Astudent%20model%20to%20develop%20robust%20reasoning%20skills.%20We%20validate%20our%20method%0Athrough%20evaluations%20on%20mathematical%20reasoning%20%28AIME%202024/2025%2C%20MATH-500%29%20and%0Acode%20generation%20%28LiveCodeBench%29%2C%20achieving%20state-of-the-art%20results%20with%20only%0A0.8k%20carefully%20curated%20examples%2C%20bypassing%20the%20need%20for%20extensive%20scaling.%20Our%0Asystematic%20analysis%20demonstrates%20that%20DED%20outperforms%20existing%20methods%20by%0Aconsidering%20factors%20beyond%20superficial%20hardness%2C%20token%20length%2C%20or%20teacher%20model%0Acapability.%20This%20work%20offers%20a%20practical%20and%20efficient%20pathway%20to%20advanced%0Areasoning%20while%20preserving%20general%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Scaling%2520Law%253A%2520A%2520Data-Efficient%2520Distillation%2520Framework%2520for%250A%2520%2520Reasoning%26entry.906535625%3DXiaojun%2520Wu%2520and%2520Xiaoguang%2520Jiang%2520and%2520Huiyang%2520Li%2520and%2520Jucai%2520Zhai%2520and%2520Dengfeng%2520Liu%2520and%2520Qiaobo%2520Hao%2520and%2520Huang%2520Liu%2520and%2520Zhiguo%2520Yang%2520and%2520Ji%2520Xie%2520and%2520Ninglun%2520Gu%2520and%2520Jin%2520Yang%2520and%2520Kailai%2520Zhang%2520and%2520Yelun%2520Bao%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520remarkable%2520reasoning%2520capabilities%2520in%250Atasks%2520such%2520as%2520algorithmic%2520coding%2520and%2520mathematical%2520problem-solving.%2520Recent%250Amethods%2520have%2520improved%2520reasoning%2520through%2520expanded%2520corpus%2520and%2520multistage%2520training%250Acombining%2520reinforcement%2520learning%2520and%2520supervised%2520fine-tuning.%2520Although%2520some%250Amethods%2520suggest%2520that%2520small%2520but%2520targeted%2520dataset%2520can%2520incentivize%2520reasoning%2520via%250Aonly%2520distillation%252C%2520a%2520reasoning%2520scaling%2520laws%2520is%2520still%2520taking%2520shape%252C%2520increasing%250Acomputational%2520costs.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520data-efficient%2520distillation%250Aframework%2520%2528DED%2529%2520that%2520optimizes%2520the%2520Pareto%2520frontier%2520of%2520reasoning%2520distillation.%250AInspired%2520by%2520the%2520on-policy%2520learning%2520and%2520diverse%2520roll-out%2520strategies%2520of%250Areinforcement%2520learning%252C%2520the%2520key%2520idea%2520of%2520our%2520approach%2520is%2520threefold%253A%2520%25281%2529%2520We%250Aidentify%2520that%2520benchmark%2520scores%2520alone%2520do%2520not%2520determine%2520an%2520effective%2520teacher%250Amodel.%2520Through%2520comprehensive%2520comparisons%2520of%2520leading%2520reasoning%2520LLMs%252C%2520we%2520develop%250Aa%2520method%2520to%2520select%2520an%2520optimal%2520teacher%2520model.%2520%25282%2529%2520While%2520scaling%2520distillation%2520can%250Aenhance%2520reasoning%252C%2520it%2520often%2520degrades%2520out-of-domain%2520performance.%2520A%2520carefully%250Acurated%252C%2520smaller%2520corpus%2520achieves%2520a%2520balanced%2520trade-off%2520between%2520in-domain%2520and%250Aout-of-domain%2520capabilities.%2520%25283%2529%2520Diverse%2520reasoning%2520trajectories%2520encourage%2520the%250Astudent%2520model%2520to%2520develop%2520robust%2520reasoning%2520skills.%2520We%2520validate%2520our%2520method%250Athrough%2520evaluations%2520on%2520mathematical%2520reasoning%2520%2528AIME%25202024/2025%252C%2520MATH-500%2529%2520and%250Acode%2520generation%2520%2528LiveCodeBench%2529%252C%2520achieving%2520state-of-the-art%2520results%2520with%2520only%250A0.8k%2520carefully%2520curated%2520examples%252C%2520bypassing%2520the%2520need%2520for%2520extensive%2520scaling.%2520Our%250Asystematic%2520analysis%2520demonstrates%2520that%2520DED%2520outperforms%2520existing%2520methods%2520by%250Aconsidering%2520factors%2520beyond%2520superficial%2520hardness%252C%2520token%2520length%252C%2520or%2520teacher%2520model%250Acapability.%2520This%2520work%2520offers%2520a%2520practical%2520and%2520efficient%2520pathway%2520to%2520advanced%250Areasoning%2520while%2520preserving%2520general%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Scaling%20Law%3A%20A%20Data-Efficient%20Distillation%20Framework%20for%0A%20%20Reasoning&entry.906535625=Xiaojun%20Wu%20and%20Xiaoguang%20Jiang%20and%20Huiyang%20Li%20and%20Jucai%20Zhai%20and%20Dengfeng%20Liu%20and%20Qiaobo%20Hao%20and%20Huang%20Liu%20and%20Zhiguo%20Yang%20and%20Ji%20Xie%20and%20Ninglun%20Gu%20and%20Jin%20Yang%20and%20Kailai%20Zhang%20and%20Yelun%20Bao%20and%20Jun%20Wang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20remarkable%20reasoning%20capabilities%20in%0Atasks%20such%20as%20algorithmic%20coding%20and%20mathematical%20problem-solving.%20Recent%0Amethods%20have%20improved%20reasoning%20through%20expanded%20corpus%20and%20multistage%20training%0Acombining%20reinforcement%20learning%20and%20supervised%20fine-tuning.%20Although%20some%0Amethods%20suggest%20that%20small%20but%20targeted%20dataset%20can%20incentivize%20reasoning%20via%0Aonly%20distillation%2C%20a%20reasoning%20scaling%20laws%20is%20still%20taking%20shape%2C%20increasing%0Acomputational%20costs.%20To%20address%20this%2C%20we%20propose%20a%20data-efficient%20distillation%0Aframework%20%28DED%29%20that%20optimizes%20the%20Pareto%20frontier%20of%20reasoning%20distillation.%0AInspired%20by%20the%20on-policy%20learning%20and%20diverse%20roll-out%20strategies%20of%0Areinforcement%20learning%2C%20the%20key%20idea%20of%20our%20approach%20is%20threefold%3A%20%281%29%20We%0Aidentify%20that%20benchmark%20scores%20alone%20do%20not%20determine%20an%20effective%20teacher%0Amodel.%20Through%20comprehensive%20comparisons%20of%20leading%20reasoning%20LLMs%2C%20we%20develop%0Aa%20method%20to%20select%20an%20optimal%20teacher%20model.%20%282%29%20While%20scaling%20distillation%20can%0Aenhance%20reasoning%2C%20it%20often%20degrades%20out-of-domain%20performance.%20A%20carefully%0Acurated%2C%20smaller%20corpus%20achieves%20a%20balanced%20trade-off%20between%20in-domain%20and%0Aout-of-domain%20capabilities.%20%283%29%20Diverse%20reasoning%20trajectories%20encourage%20the%0Astudent%20model%20to%20develop%20robust%20reasoning%20skills.%20We%20validate%20our%20method%0Athrough%20evaluations%20on%20mathematical%20reasoning%20%28AIME%202024/2025%2C%20MATH-500%29%20and%0Acode%20generation%20%28LiveCodeBench%29%2C%20achieving%20state-of-the-art%20results%20with%20only%0A0.8k%20carefully%20curated%20examples%2C%20bypassing%20the%20need%20for%20extensive%20scaling.%20Our%0Asystematic%20analysis%20demonstrates%20that%20DED%20outperforms%20existing%20methods%20by%0Aconsidering%20factors%20beyond%20superficial%20hardness%2C%20token%20length%2C%20or%20teacher%20model%0Acapability.%20This%20work%20offers%20a%20practical%20and%20efficient%20pathway%20to%20advanced%0Areasoning%20while%20preserving%20general%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09883v1&entry.124074799=Read"},
{"title": "Leveraging Reviewer Experience in Code Review Comment Generation", "author": "Hong Yi Lin and Patanamon Thongtanunam and Christoph Treude and Michael W. Godfrey and Chunhua Liu and Wachiraphan Charoenwet", "abstract": "  Modern code review is a ubiquitous software quality assurance process aimed\nat identifying potential issues within newly written code. Despite its\neffectiveness, the process demands large amounts of effort from the human\nreviewers involved. To help alleviate this workload, researchers have trained\ndeep learning models to imitate human reviewers in providing natural language\ncode reviews. Formally, this task is known as code review comment generation.\nPrior work has demonstrated improvements in this task by leveraging machine\nlearning techniques and neural models, such as transfer learning and the\ntransformer architecture. However, the quality of the model generated reviews\nremain sub-optimal due to the quality of the open-source code review data used\nin model training. This is in part due to the data obtained from open-source\nprojects where code reviews are conducted in a public forum, and reviewers\npossess varying levels of software development experience, potentially\naffecting the quality of their feedback. To accommodate for this variation, we\npropose a suite of experience-aware training methods that utilise the\nreviewers' past authoring and reviewing experiences as signals for review\nquality. Specifically, we propose experience-aware loss functions (ELF), which\nuse the reviewers' authoring and reviewing ownership of a project as weights in\nthe model's loss function. Through this method, experienced reviewers' code\nreviews yield larger influence over the model's behaviour. Compared to the SOTA\nmodel, ELF was able to generate higher quality reviews in terms of accuracy,\ninformativeness, and comment types generated. The key contribution of this work\nis the demonstration of how traditional software engineering concepts such as\nreviewer experience can be integrated into the design of AI-based automated\ncode review models.\n", "link": "http://arxiv.org/abs/2409.10959v2", "date": "2025-08-13", "relevancy": 1.8703, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Reviewer%20Experience%20in%20Code%20Review%20Comment%20Generation&body=Title%3A%20Leveraging%20Reviewer%20Experience%20in%20Code%20Review%20Comment%20Generation%0AAuthor%3A%20Hong%20Yi%20Lin%20and%20Patanamon%20Thongtanunam%20and%20Christoph%20Treude%20and%20Michael%20W.%20Godfrey%20and%20Chunhua%20Liu%20and%20Wachiraphan%20Charoenwet%0AAbstract%3A%20%20%20Modern%20code%20review%20is%20a%20ubiquitous%20software%20quality%20assurance%20process%20aimed%0Aat%20identifying%20potential%20issues%20within%20newly%20written%20code.%20Despite%20its%0Aeffectiveness%2C%20the%20process%20demands%20large%20amounts%20of%20effort%20from%20the%20human%0Areviewers%20involved.%20To%20help%20alleviate%20this%20workload%2C%20researchers%20have%20trained%0Adeep%20learning%20models%20to%20imitate%20human%20reviewers%20in%20providing%20natural%20language%0Acode%20reviews.%20Formally%2C%20this%20task%20is%20known%20as%20code%20review%20comment%20generation.%0APrior%20work%20has%20demonstrated%20improvements%20in%20this%20task%20by%20leveraging%20machine%0Alearning%20techniques%20and%20neural%20models%2C%20such%20as%20transfer%20learning%20and%20the%0Atransformer%20architecture.%20However%2C%20the%20quality%20of%20the%20model%20generated%20reviews%0Aremain%20sub-optimal%20due%20to%20the%20quality%20of%20the%20open-source%20code%20review%20data%20used%0Ain%20model%20training.%20This%20is%20in%20part%20due%20to%20the%20data%20obtained%20from%20open-source%0Aprojects%20where%20code%20reviews%20are%20conducted%20in%20a%20public%20forum%2C%20and%20reviewers%0Apossess%20varying%20levels%20of%20software%20development%20experience%2C%20potentially%0Aaffecting%20the%20quality%20of%20their%20feedback.%20To%20accommodate%20for%20this%20variation%2C%20we%0Apropose%20a%20suite%20of%20experience-aware%20training%20methods%20that%20utilise%20the%0Areviewers%27%20past%20authoring%20and%20reviewing%20experiences%20as%20signals%20for%20review%0Aquality.%20Specifically%2C%20we%20propose%20experience-aware%20loss%20functions%20%28ELF%29%2C%20which%0Ause%20the%20reviewers%27%20authoring%20and%20reviewing%20ownership%20of%20a%20project%20as%20weights%20in%0Athe%20model%27s%20loss%20function.%20Through%20this%20method%2C%20experienced%20reviewers%27%20code%0Areviews%20yield%20larger%20influence%20over%20the%20model%27s%20behaviour.%20Compared%20to%20the%20SOTA%0Amodel%2C%20ELF%20was%20able%20to%20generate%20higher%20quality%20reviews%20in%20terms%20of%20accuracy%2C%0Ainformativeness%2C%20and%20comment%20types%20generated.%20The%20key%20contribution%20of%20this%20work%0Ais%20the%20demonstration%20of%20how%20traditional%20software%20engineering%20concepts%20such%20as%0Areviewer%20experience%20can%20be%20integrated%20into%20the%20design%20of%20AI-based%20automated%0Acode%20review%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Reviewer%2520Experience%2520in%2520Code%2520Review%2520Comment%2520Generation%26entry.906535625%3DHong%2520Yi%2520Lin%2520and%2520Patanamon%2520Thongtanunam%2520and%2520Christoph%2520Treude%2520and%2520Michael%2520W.%2520Godfrey%2520and%2520Chunhua%2520Liu%2520and%2520Wachiraphan%2520Charoenwet%26entry.1292438233%3D%2520%2520Modern%2520code%2520review%2520is%2520a%2520ubiquitous%2520software%2520quality%2520assurance%2520process%2520aimed%250Aat%2520identifying%2520potential%2520issues%2520within%2520newly%2520written%2520code.%2520Despite%2520its%250Aeffectiveness%252C%2520the%2520process%2520demands%2520large%2520amounts%2520of%2520effort%2520from%2520the%2520human%250Areviewers%2520involved.%2520To%2520help%2520alleviate%2520this%2520workload%252C%2520researchers%2520have%2520trained%250Adeep%2520learning%2520models%2520to%2520imitate%2520human%2520reviewers%2520in%2520providing%2520natural%2520language%250Acode%2520reviews.%2520Formally%252C%2520this%2520task%2520is%2520known%2520as%2520code%2520review%2520comment%2520generation.%250APrior%2520work%2520has%2520demonstrated%2520improvements%2520in%2520this%2520task%2520by%2520leveraging%2520machine%250Alearning%2520techniques%2520and%2520neural%2520models%252C%2520such%2520as%2520transfer%2520learning%2520and%2520the%250Atransformer%2520architecture.%2520However%252C%2520the%2520quality%2520of%2520the%2520model%2520generated%2520reviews%250Aremain%2520sub-optimal%2520due%2520to%2520the%2520quality%2520of%2520the%2520open-source%2520code%2520review%2520data%2520used%250Ain%2520model%2520training.%2520This%2520is%2520in%2520part%2520due%2520to%2520the%2520data%2520obtained%2520from%2520open-source%250Aprojects%2520where%2520code%2520reviews%2520are%2520conducted%2520in%2520a%2520public%2520forum%252C%2520and%2520reviewers%250Apossess%2520varying%2520levels%2520of%2520software%2520development%2520experience%252C%2520potentially%250Aaffecting%2520the%2520quality%2520of%2520their%2520feedback.%2520To%2520accommodate%2520for%2520this%2520variation%252C%2520we%250Apropose%2520a%2520suite%2520of%2520experience-aware%2520training%2520methods%2520that%2520utilise%2520the%250Areviewers%2527%2520past%2520authoring%2520and%2520reviewing%2520experiences%2520as%2520signals%2520for%2520review%250Aquality.%2520Specifically%252C%2520we%2520propose%2520experience-aware%2520loss%2520functions%2520%2528ELF%2529%252C%2520which%250Ause%2520the%2520reviewers%2527%2520authoring%2520and%2520reviewing%2520ownership%2520of%2520a%2520project%2520as%2520weights%2520in%250Athe%2520model%2527s%2520loss%2520function.%2520Through%2520this%2520method%252C%2520experienced%2520reviewers%2527%2520code%250Areviews%2520yield%2520larger%2520influence%2520over%2520the%2520model%2527s%2520behaviour.%2520Compared%2520to%2520the%2520SOTA%250Amodel%252C%2520ELF%2520was%2520able%2520to%2520generate%2520higher%2520quality%2520reviews%2520in%2520terms%2520of%2520accuracy%252C%250Ainformativeness%252C%2520and%2520comment%2520types%2520generated.%2520The%2520key%2520contribution%2520of%2520this%2520work%250Ais%2520the%2520demonstration%2520of%2520how%2520traditional%2520software%2520engineering%2520concepts%2520such%2520as%250Areviewer%2520experience%2520can%2520be%2520integrated%2520into%2520the%2520design%2520of%2520AI-based%2520automated%250Acode%2520review%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Reviewer%20Experience%20in%20Code%20Review%20Comment%20Generation&entry.906535625=Hong%20Yi%20Lin%20and%20Patanamon%20Thongtanunam%20and%20Christoph%20Treude%20and%20Michael%20W.%20Godfrey%20and%20Chunhua%20Liu%20and%20Wachiraphan%20Charoenwet&entry.1292438233=%20%20Modern%20code%20review%20is%20a%20ubiquitous%20software%20quality%20assurance%20process%20aimed%0Aat%20identifying%20potential%20issues%20within%20newly%20written%20code.%20Despite%20its%0Aeffectiveness%2C%20the%20process%20demands%20large%20amounts%20of%20effort%20from%20the%20human%0Areviewers%20involved.%20To%20help%20alleviate%20this%20workload%2C%20researchers%20have%20trained%0Adeep%20learning%20models%20to%20imitate%20human%20reviewers%20in%20providing%20natural%20language%0Acode%20reviews.%20Formally%2C%20this%20task%20is%20known%20as%20code%20review%20comment%20generation.%0APrior%20work%20has%20demonstrated%20improvements%20in%20this%20task%20by%20leveraging%20machine%0Alearning%20techniques%20and%20neural%20models%2C%20such%20as%20transfer%20learning%20and%20the%0Atransformer%20architecture.%20However%2C%20the%20quality%20of%20the%20model%20generated%20reviews%0Aremain%20sub-optimal%20due%20to%20the%20quality%20of%20the%20open-source%20code%20review%20data%20used%0Ain%20model%20training.%20This%20is%20in%20part%20due%20to%20the%20data%20obtained%20from%20open-source%0Aprojects%20where%20code%20reviews%20are%20conducted%20in%20a%20public%20forum%2C%20and%20reviewers%0Apossess%20varying%20levels%20of%20software%20development%20experience%2C%20potentially%0Aaffecting%20the%20quality%20of%20their%20feedback.%20To%20accommodate%20for%20this%20variation%2C%20we%0Apropose%20a%20suite%20of%20experience-aware%20training%20methods%20that%20utilise%20the%0Areviewers%27%20past%20authoring%20and%20reviewing%20experiences%20as%20signals%20for%20review%0Aquality.%20Specifically%2C%20we%20propose%20experience-aware%20loss%20functions%20%28ELF%29%2C%20which%0Ause%20the%20reviewers%27%20authoring%20and%20reviewing%20ownership%20of%20a%20project%20as%20weights%20in%0Athe%20model%27s%20loss%20function.%20Through%20this%20method%2C%20experienced%20reviewers%27%20code%0Areviews%20yield%20larger%20influence%20over%20the%20model%27s%20behaviour.%20Compared%20to%20the%20SOTA%0Amodel%2C%20ELF%20was%20able%20to%20generate%20higher%20quality%20reviews%20in%20terms%20of%20accuracy%2C%0Ainformativeness%2C%20and%20comment%20types%20generated.%20The%20key%20contribution%20of%20this%20work%0Ais%20the%20demonstration%20of%20how%20traditional%20software%20engineering%20concepts%20such%20as%0Areviewer%20experience%20can%20be%20integrated%20into%20the%20design%20of%20AI-based%20automated%0Acode%20review%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10959v2&entry.124074799=Read"},
{"title": "January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation\n  Suite for Multimodal Food Analysis", "author": "Amir Hosseinian and Ashkan Dehghani Zahedani and Umer Mansoor and Noosheen Hashemi and Mark Woodward", "abstract": "  Progress in AI for automated nutritional analysis is critically hampered by\nthe lack of standardized evaluation methodologies and high-quality, real-world\nbenchmark datasets. To address this, we introduce three primary contributions.\nFirst, we present the January Food Benchmark (JFB), a publicly available\ncollection of 1,000 food images with human-validated annotations. Second, we\ndetail a comprehensive benchmarking framework, including robust metrics and a\nnovel, application-oriented overall score designed to assess model performance\nholistically. Third, we provide baseline results from both general-purpose\nVision-Language Models (VLMs) and our own specialized model,\njanuary/food-vision-v1. Our evaluation demonstrates that the specialized model\nachieves an Overall Score of 86.2, a 12.1-point improvement over the\nbest-performing general-purpose configuration. This work offers the research\ncommunity a valuable new evaluation dataset and a rigorous framework to guide\nand benchmark future developments in automated nutritional analysis.\n", "link": "http://arxiv.org/abs/2508.09966v1", "date": "2025-08-13", "relevancy": 1.9473, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4876}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20January%20Food%20Benchmark%20%28JFB%29%3A%20A%20Public%20Benchmark%20Dataset%20and%20Evaluation%0A%20%20Suite%20for%20Multimodal%20Food%20Analysis&body=Title%3A%20January%20Food%20Benchmark%20%28JFB%29%3A%20A%20Public%20Benchmark%20Dataset%20and%20Evaluation%0A%20%20Suite%20for%20Multimodal%20Food%20Analysis%0AAuthor%3A%20Amir%20Hosseinian%20and%20Ashkan%20Dehghani%20Zahedani%20and%20Umer%20Mansoor%20and%20Noosheen%20Hashemi%20and%20Mark%20Woodward%0AAbstract%3A%20%20%20Progress%20in%20AI%20for%20automated%20nutritional%20analysis%20is%20critically%20hampered%20by%0Athe%20lack%20of%20standardized%20evaluation%20methodologies%20and%20high-quality%2C%20real-world%0Abenchmark%20datasets.%20To%20address%20this%2C%20we%20introduce%20three%20primary%20contributions.%0AFirst%2C%20we%20present%20the%20January%20Food%20Benchmark%20%28JFB%29%2C%20a%20publicly%20available%0Acollection%20of%201%2C000%20food%20images%20with%20human-validated%20annotations.%20Second%2C%20we%0Adetail%20a%20comprehensive%20benchmarking%20framework%2C%20including%20robust%20metrics%20and%20a%0Anovel%2C%20application-oriented%20overall%20score%20designed%20to%20assess%20model%20performance%0Aholistically.%20Third%2C%20we%20provide%20baseline%20results%20from%20both%20general-purpose%0AVision-Language%20Models%20%28VLMs%29%20and%20our%20own%20specialized%20model%2C%0Ajanuary/food-vision-v1.%20Our%20evaluation%20demonstrates%20that%20the%20specialized%20model%0Aachieves%20an%20Overall%20Score%20of%2086.2%2C%20a%2012.1-point%20improvement%20over%20the%0Abest-performing%20general-purpose%20configuration.%20This%20work%20offers%20the%20research%0Acommunity%20a%20valuable%20new%20evaluation%20dataset%20and%20a%20rigorous%20framework%20to%20guide%0Aand%20benchmark%20future%20developments%20in%20automated%20nutritional%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJanuary%2520Food%2520Benchmark%2520%2528JFB%2529%253A%2520A%2520Public%2520Benchmark%2520Dataset%2520and%2520Evaluation%250A%2520%2520Suite%2520for%2520Multimodal%2520Food%2520Analysis%26entry.906535625%3DAmir%2520Hosseinian%2520and%2520Ashkan%2520Dehghani%2520Zahedani%2520and%2520Umer%2520Mansoor%2520and%2520Noosheen%2520Hashemi%2520and%2520Mark%2520Woodward%26entry.1292438233%3D%2520%2520Progress%2520in%2520AI%2520for%2520automated%2520nutritional%2520analysis%2520is%2520critically%2520hampered%2520by%250Athe%2520lack%2520of%2520standardized%2520evaluation%2520methodologies%2520and%2520high-quality%252C%2520real-world%250Abenchmark%2520datasets.%2520To%2520address%2520this%252C%2520we%2520introduce%2520three%2520primary%2520contributions.%250AFirst%252C%2520we%2520present%2520the%2520January%2520Food%2520Benchmark%2520%2528JFB%2529%252C%2520a%2520publicly%2520available%250Acollection%2520of%25201%252C000%2520food%2520images%2520with%2520human-validated%2520annotations.%2520Second%252C%2520we%250Adetail%2520a%2520comprehensive%2520benchmarking%2520framework%252C%2520including%2520robust%2520metrics%2520and%2520a%250Anovel%252C%2520application-oriented%2520overall%2520score%2520designed%2520to%2520assess%2520model%2520performance%250Aholistically.%2520Third%252C%2520we%2520provide%2520baseline%2520results%2520from%2520both%2520general-purpose%250AVision-Language%2520Models%2520%2528VLMs%2529%2520and%2520our%2520own%2520specialized%2520model%252C%250Ajanuary/food-vision-v1.%2520Our%2520evaluation%2520demonstrates%2520that%2520the%2520specialized%2520model%250Aachieves%2520an%2520Overall%2520Score%2520of%252086.2%252C%2520a%252012.1-point%2520improvement%2520over%2520the%250Abest-performing%2520general-purpose%2520configuration.%2520This%2520work%2520offers%2520the%2520research%250Acommunity%2520a%2520valuable%2520new%2520evaluation%2520dataset%2520and%2520a%2520rigorous%2520framework%2520to%2520guide%250Aand%2520benchmark%2520future%2520developments%2520in%2520automated%2520nutritional%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=January%20Food%20Benchmark%20%28JFB%29%3A%20A%20Public%20Benchmark%20Dataset%20and%20Evaluation%0A%20%20Suite%20for%20Multimodal%20Food%20Analysis&entry.906535625=Amir%20Hosseinian%20and%20Ashkan%20Dehghani%20Zahedani%20and%20Umer%20Mansoor%20and%20Noosheen%20Hashemi%20and%20Mark%20Woodward&entry.1292438233=%20%20Progress%20in%20AI%20for%20automated%20nutritional%20analysis%20is%20critically%20hampered%20by%0Athe%20lack%20of%20standardized%20evaluation%20methodologies%20and%20high-quality%2C%20real-world%0Abenchmark%20datasets.%20To%20address%20this%2C%20we%20introduce%20three%20primary%20contributions.%0AFirst%2C%20we%20present%20the%20January%20Food%20Benchmark%20%28JFB%29%2C%20a%20publicly%20available%0Acollection%20of%201%2C000%20food%20images%20with%20human-validated%20annotations.%20Second%2C%20we%0Adetail%20a%20comprehensive%20benchmarking%20framework%2C%20including%20robust%20metrics%20and%20a%0Anovel%2C%20application-oriented%20overall%20score%20designed%20to%20assess%20model%20performance%0Aholistically.%20Third%2C%20we%20provide%20baseline%20results%20from%20both%20general-purpose%0AVision-Language%20Models%20%28VLMs%29%20and%20our%20own%20specialized%20model%2C%0Ajanuary/food-vision-v1.%20Our%20evaluation%20demonstrates%20that%20the%20specialized%20model%0Aachieves%20an%20Overall%20Score%20of%2086.2%2C%20a%2012.1-point%20improvement%20over%20the%0Abest-performing%20general-purpose%20configuration.%20This%20work%20offers%20the%20research%0Acommunity%20a%20valuable%20new%20evaluation%20dataset%20and%20a%20rigorous%20framework%20to%20guide%0Aand%20benchmark%20future%20developments%20in%20automated%20nutritional%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09966v1&entry.124074799=Read"},
{"title": "The Importance of Being Lazy: Scaling Limits of Continual Learning", "author": "Jacopo Graldi and Alessandro Breccia and Giulia Lanzillotta and Thomas Hofmann and Lorenzo Noci", "abstract": "  Despite recent efforts, neural networks still struggle to learn in\nnon-stationary environments, and our understanding of catastrophic forgetting\n(CF) is far from complete. In this work, we perform a systematic study on the\nimpact of model scale and the degree of feature learning in continual learning.\nWe reconcile existing contradictory observations on scale in the literature, by\ndifferentiating between lazy and rich training regimes through a variable\nparameterization of the architecture. We show that increasing model width is\nonly beneficial when it reduces the amount of feature learning, yielding more\nlaziness. Using the framework of dynamical mean field theory, we then study the\ninfinite width dynamics of the model in the feature learning regime and\ncharacterize CF, extending prior theoretical results limited to the lazy\nregime. We study the intricate relationship between feature learning, task\nnon-stationarity, and forgetting, finding that high feature learning is only\nbeneficial with highly similar tasks. We identify a transition modulated by\ntask similarity where the model exits an effectively lazy regime with low\nforgetting to enter a rich regime with significant forgetting. Finally, our\nfindings reveal that neural networks achieve optimal performance at a critical\nlevel of feature learning, which depends on task non-stationarity and transfers\nacross model scales. This work provides a unified perspective on the role of\nscale and feature learning in continual learning.\n", "link": "http://arxiv.org/abs/2506.16884v2", "date": "2025-08-13", "relevancy": 1.9791, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5266}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4938}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Importance%20of%20Being%20Lazy%3A%20Scaling%20Limits%20of%20Continual%20Learning&body=Title%3A%20The%20Importance%20of%20Being%20Lazy%3A%20Scaling%20Limits%20of%20Continual%20Learning%0AAuthor%3A%20Jacopo%20Graldi%20and%20Alessandro%20Breccia%20and%20Giulia%20Lanzillotta%20and%20Thomas%20Hofmann%20and%20Lorenzo%20Noci%0AAbstract%3A%20%20%20Despite%20recent%20efforts%2C%20neural%20networks%20still%20struggle%20to%20learn%20in%0Anon-stationary%20environments%2C%20and%20our%20understanding%20of%20catastrophic%20forgetting%0A%28CF%29%20is%20far%20from%20complete.%20In%20this%20work%2C%20we%20perform%20a%20systematic%20study%20on%20the%0Aimpact%20of%20model%20scale%20and%20the%20degree%20of%20feature%20learning%20in%20continual%20learning.%0AWe%20reconcile%20existing%20contradictory%20observations%20on%20scale%20in%20the%20literature%2C%20by%0Adifferentiating%20between%20lazy%20and%20rich%20training%20regimes%20through%20a%20variable%0Aparameterization%20of%20the%20architecture.%20We%20show%20that%20increasing%20model%20width%20is%0Aonly%20beneficial%20when%20it%20reduces%20the%20amount%20of%20feature%20learning%2C%20yielding%20more%0Alaziness.%20Using%20the%20framework%20of%20dynamical%20mean%20field%20theory%2C%20we%20then%20study%20the%0Ainfinite%20width%20dynamics%20of%20the%20model%20in%20the%20feature%20learning%20regime%20and%0Acharacterize%20CF%2C%20extending%20prior%20theoretical%20results%20limited%20to%20the%20lazy%0Aregime.%20We%20study%20the%20intricate%20relationship%20between%20feature%20learning%2C%20task%0Anon-stationarity%2C%20and%20forgetting%2C%20finding%20that%20high%20feature%20learning%20is%20only%0Abeneficial%20with%20highly%20similar%20tasks.%20We%20identify%20a%20transition%20modulated%20by%0Atask%20similarity%20where%20the%20model%20exits%20an%20effectively%20lazy%20regime%20with%20low%0Aforgetting%20to%20enter%20a%20rich%20regime%20with%20significant%20forgetting.%20Finally%2C%20our%0Afindings%20reveal%20that%20neural%20networks%20achieve%20optimal%20performance%20at%20a%20critical%0Alevel%20of%20feature%20learning%2C%20which%20depends%20on%20task%20non-stationarity%20and%20transfers%0Aacross%20model%20scales.%20This%20work%20provides%20a%20unified%20perspective%20on%20the%20role%20of%0Ascale%20and%20feature%20learning%20in%20continual%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.16884v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Importance%2520of%2520Being%2520Lazy%253A%2520Scaling%2520Limits%2520of%2520Continual%2520Learning%26entry.906535625%3DJacopo%2520Graldi%2520and%2520Alessandro%2520Breccia%2520and%2520Giulia%2520Lanzillotta%2520and%2520Thomas%2520Hofmann%2520and%2520Lorenzo%2520Noci%26entry.1292438233%3D%2520%2520Despite%2520recent%2520efforts%252C%2520neural%2520networks%2520still%2520struggle%2520to%2520learn%2520in%250Anon-stationary%2520environments%252C%2520and%2520our%2520understanding%2520of%2520catastrophic%2520forgetting%250A%2528CF%2529%2520is%2520far%2520from%2520complete.%2520In%2520this%2520work%252C%2520we%2520perform%2520a%2520systematic%2520study%2520on%2520the%250Aimpact%2520of%2520model%2520scale%2520and%2520the%2520degree%2520of%2520feature%2520learning%2520in%2520continual%2520learning.%250AWe%2520reconcile%2520existing%2520contradictory%2520observations%2520on%2520scale%2520in%2520the%2520literature%252C%2520by%250Adifferentiating%2520between%2520lazy%2520and%2520rich%2520training%2520regimes%2520through%2520a%2520variable%250Aparameterization%2520of%2520the%2520architecture.%2520We%2520show%2520that%2520increasing%2520model%2520width%2520is%250Aonly%2520beneficial%2520when%2520it%2520reduces%2520the%2520amount%2520of%2520feature%2520learning%252C%2520yielding%2520more%250Alaziness.%2520Using%2520the%2520framework%2520of%2520dynamical%2520mean%2520field%2520theory%252C%2520we%2520then%2520study%2520the%250Ainfinite%2520width%2520dynamics%2520of%2520the%2520model%2520in%2520the%2520feature%2520learning%2520regime%2520and%250Acharacterize%2520CF%252C%2520extending%2520prior%2520theoretical%2520results%2520limited%2520to%2520the%2520lazy%250Aregime.%2520We%2520study%2520the%2520intricate%2520relationship%2520between%2520feature%2520learning%252C%2520task%250Anon-stationarity%252C%2520and%2520forgetting%252C%2520finding%2520that%2520high%2520feature%2520learning%2520is%2520only%250Abeneficial%2520with%2520highly%2520similar%2520tasks.%2520We%2520identify%2520a%2520transition%2520modulated%2520by%250Atask%2520similarity%2520where%2520the%2520model%2520exits%2520an%2520effectively%2520lazy%2520regime%2520with%2520low%250Aforgetting%2520to%2520enter%2520a%2520rich%2520regime%2520with%2520significant%2520forgetting.%2520Finally%252C%2520our%250Afindings%2520reveal%2520that%2520neural%2520networks%2520achieve%2520optimal%2520performance%2520at%2520a%2520critical%250Alevel%2520of%2520feature%2520learning%252C%2520which%2520depends%2520on%2520task%2520non-stationarity%2520and%2520transfers%250Aacross%2520model%2520scales.%2520This%2520work%2520provides%2520a%2520unified%2520perspective%2520on%2520the%2520role%2520of%250Ascale%2520and%2520feature%2520learning%2520in%2520continual%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16884v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Importance%20of%20Being%20Lazy%3A%20Scaling%20Limits%20of%20Continual%20Learning&entry.906535625=Jacopo%20Graldi%20and%20Alessandro%20Breccia%20and%20Giulia%20Lanzillotta%20and%20Thomas%20Hofmann%20and%20Lorenzo%20Noci&entry.1292438233=%20%20Despite%20recent%20efforts%2C%20neural%20networks%20still%20struggle%20to%20learn%20in%0Anon-stationary%20environments%2C%20and%20our%20understanding%20of%20catastrophic%20forgetting%0A%28CF%29%20is%20far%20from%20complete.%20In%20this%20work%2C%20we%20perform%20a%20systematic%20study%20on%20the%0Aimpact%20of%20model%20scale%20and%20the%20degree%20of%20feature%20learning%20in%20continual%20learning.%0AWe%20reconcile%20existing%20contradictory%20observations%20on%20scale%20in%20the%20literature%2C%20by%0Adifferentiating%20between%20lazy%20and%20rich%20training%20regimes%20through%20a%20variable%0Aparameterization%20of%20the%20architecture.%20We%20show%20that%20increasing%20model%20width%20is%0Aonly%20beneficial%20when%20it%20reduces%20the%20amount%20of%20feature%20learning%2C%20yielding%20more%0Alaziness.%20Using%20the%20framework%20of%20dynamical%20mean%20field%20theory%2C%20we%20then%20study%20the%0Ainfinite%20width%20dynamics%20of%20the%20model%20in%20the%20feature%20learning%20regime%20and%0Acharacterize%20CF%2C%20extending%20prior%20theoretical%20results%20limited%20to%20the%20lazy%0Aregime.%20We%20study%20the%20intricate%20relationship%20between%20feature%20learning%2C%20task%0Anon-stationarity%2C%20and%20forgetting%2C%20finding%20that%20high%20feature%20learning%20is%20only%0Abeneficial%20with%20highly%20similar%20tasks.%20We%20identify%20a%20transition%20modulated%20by%0Atask%20similarity%20where%20the%20model%20exits%20an%20effectively%20lazy%20regime%20with%20low%0Aforgetting%20to%20enter%20a%20rich%20regime%20with%20significant%20forgetting.%20Finally%2C%20our%0Afindings%20reveal%20that%20neural%20networks%20achieve%20optimal%20performance%20at%20a%20critical%0Alevel%20of%20feature%20learning%2C%20which%20depends%20on%20task%20non-stationarity%20and%20transfers%0Aacross%20model%20scales.%20This%20work%20provides%20a%20unified%20perspective%20on%20the%20role%20of%0Ascale%20and%20feature%20learning%20in%20continual%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.16884v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


