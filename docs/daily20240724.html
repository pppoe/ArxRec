<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240723.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Surfel-based Gaussian Inverse Rendering for Fast and Relightable Dynamic\n  Human Reconstruction from Monocular Video", "author": "Yiqun Zhao and Chenming Wu and Binbin Huang and Yihao Zhi and Chen Zhao and Jingdong Wang and Shenghua Gao", "abstract": "  Efficient and accurate reconstruction of a relightable, dynamic clothed human\navatar from a monocular video is crucial for the entertainment industry. This\npaper introduces the Surfel-based Gaussian Inverse Avatar (SGIA) method, which\nintroduces efficient training and rendering for relightable dynamic human\nreconstruction. SGIA advances previous Gaussian Avatar methods by\ncomprehensively modeling Physically-Based Rendering (PBR) properties for\nclothed human avatars, allowing for the manipulation of avatars into novel\nposes under diverse lighting conditions. Specifically, our approach integrates\npre-integration and image-based lighting for fast light calculations that\nsurpass the performance of existing implicit-based techniques. To address\nchallenges related to material lighting disentanglement and accurate geometry\nreconstruction, we propose an innovative occlusion approximation strategy and a\nprogressive training approach. Extensive experiments demonstrate that SGIA not\nonly achieves highly accurate physical properties but also significantly\nenhances the realistic relighting of dynamic human avatars, providing a\nsubstantial speed advantage. We exhibit more results in our project page:\nhttps://GS-IA.github.io.\n", "link": "http://arxiv.org/abs/2407.15212v2", "date": "2024-07-23", "relevancy": 3.3682, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6774}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6774}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surfel-based%20Gaussian%20Inverse%20Rendering%20for%20Fast%20and%20Relightable%20Dynamic%0A%20%20Human%20Reconstruction%20from%20Monocular%20Video&body=Title%3A%20Surfel-based%20Gaussian%20Inverse%20Rendering%20for%20Fast%20and%20Relightable%20Dynamic%0A%20%20Human%20Reconstruction%20from%20Monocular%20Video%0AAuthor%3A%20Yiqun%20Zhao%20and%20Chenming%20Wu%20and%20Binbin%20Huang%20and%20Yihao%20Zhi%20and%20Chen%20Zhao%20and%20Jingdong%20Wang%20and%20Shenghua%20Gao%0AAbstract%3A%20%20%20Efficient%20and%20accurate%20reconstruction%20of%20a%20relightable%2C%20dynamic%20clothed%20human%0Aavatar%20from%20a%20monocular%20video%20is%20crucial%20for%20the%20entertainment%20industry.%20This%0Apaper%20introduces%20the%20Surfel-based%20Gaussian%20Inverse%20Avatar%20%28SGIA%29%20method%2C%20which%0Aintroduces%20efficient%20training%20and%20rendering%20for%20relightable%20dynamic%20human%0Areconstruction.%20SGIA%20advances%20previous%20Gaussian%20Avatar%20methods%20by%0Acomprehensively%20modeling%20Physically-Based%20Rendering%20%28PBR%29%20properties%20for%0Aclothed%20human%20avatars%2C%20allowing%20for%20the%20manipulation%20of%20avatars%20into%20novel%0Aposes%20under%20diverse%20lighting%20conditions.%20Specifically%2C%20our%20approach%20integrates%0Apre-integration%20and%20image-based%20lighting%20for%20fast%20light%20calculations%20that%0Asurpass%20the%20performance%20of%20existing%20implicit-based%20techniques.%20To%20address%0Achallenges%20related%20to%20material%20lighting%20disentanglement%20and%20accurate%20geometry%0Areconstruction%2C%20we%20propose%20an%20innovative%20occlusion%20approximation%20strategy%20and%20a%0Aprogressive%20training%20approach.%20Extensive%20experiments%20demonstrate%20that%20SGIA%20not%0Aonly%20achieves%20highly%20accurate%20physical%20properties%20but%20also%20significantly%0Aenhances%20the%20realistic%20relighting%20of%20dynamic%20human%20avatars%2C%20providing%20a%0Asubstantial%20speed%20advantage.%20We%20exhibit%20more%20results%20in%20our%20project%20page%3A%0Ahttps%3A//GS-IA.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15212v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurfel-based%2520Gaussian%2520Inverse%2520Rendering%2520for%2520Fast%2520and%2520Relightable%2520Dynamic%250A%2520%2520Human%2520Reconstruction%2520from%2520Monocular%2520Video%26entry.906535625%3DYiqun%2520Zhao%2520and%2520Chenming%2520Wu%2520and%2520Binbin%2520Huang%2520and%2520Yihao%2520Zhi%2520and%2520Chen%2520Zhao%2520and%2520Jingdong%2520Wang%2520and%2520Shenghua%2520Gao%26entry.1292438233%3D%2520%2520Efficient%2520and%2520accurate%2520reconstruction%2520of%2520a%2520relightable%252C%2520dynamic%2520clothed%2520human%250Aavatar%2520from%2520a%2520monocular%2520video%2520is%2520crucial%2520for%2520the%2520entertainment%2520industry.%2520This%250Apaper%2520introduces%2520the%2520Surfel-based%2520Gaussian%2520Inverse%2520Avatar%2520%2528SGIA%2529%2520method%252C%2520which%250Aintroduces%2520efficient%2520training%2520and%2520rendering%2520for%2520relightable%2520dynamic%2520human%250Areconstruction.%2520SGIA%2520advances%2520previous%2520Gaussian%2520Avatar%2520methods%2520by%250Acomprehensively%2520modeling%2520Physically-Based%2520Rendering%2520%2528PBR%2529%2520properties%2520for%250Aclothed%2520human%2520avatars%252C%2520allowing%2520for%2520the%2520manipulation%2520of%2520avatars%2520into%2520novel%250Aposes%2520under%2520diverse%2520lighting%2520conditions.%2520Specifically%252C%2520our%2520approach%2520integrates%250Apre-integration%2520and%2520image-based%2520lighting%2520for%2520fast%2520light%2520calculations%2520that%250Asurpass%2520the%2520performance%2520of%2520existing%2520implicit-based%2520techniques.%2520To%2520address%250Achallenges%2520related%2520to%2520material%2520lighting%2520disentanglement%2520and%2520accurate%2520geometry%250Areconstruction%252C%2520we%2520propose%2520an%2520innovative%2520occlusion%2520approximation%2520strategy%2520and%2520a%250Aprogressive%2520training%2520approach.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SGIA%2520not%250Aonly%2520achieves%2520highly%2520accurate%2520physical%2520properties%2520but%2520also%2520significantly%250Aenhances%2520the%2520realistic%2520relighting%2520of%2520dynamic%2520human%2520avatars%252C%2520providing%2520a%250Asubstantial%2520speed%2520advantage.%2520We%2520exhibit%2520more%2520results%2520in%2520our%2520project%2520page%253A%250Ahttps%253A//GS-IA.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15212v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surfel-based%20Gaussian%20Inverse%20Rendering%20for%20Fast%20and%20Relightable%20Dynamic%0A%20%20Human%20Reconstruction%20from%20Monocular%20Video&entry.906535625=Yiqun%20Zhao%20and%20Chenming%20Wu%20and%20Binbin%20Huang%20and%20Yihao%20Zhi%20and%20Chen%20Zhao%20and%20Jingdong%20Wang%20and%20Shenghua%20Gao&entry.1292438233=%20%20Efficient%20and%20accurate%20reconstruction%20of%20a%20relightable%2C%20dynamic%20clothed%20human%0Aavatar%20from%20a%20monocular%20video%20is%20crucial%20for%20the%20entertainment%20industry.%20This%0Apaper%20introduces%20the%20Surfel-based%20Gaussian%20Inverse%20Avatar%20%28SGIA%29%20method%2C%20which%0Aintroduces%20efficient%20training%20and%20rendering%20for%20relightable%20dynamic%20human%0Areconstruction.%20SGIA%20advances%20previous%20Gaussian%20Avatar%20methods%20by%0Acomprehensively%20modeling%20Physically-Based%20Rendering%20%28PBR%29%20properties%20for%0Aclothed%20human%20avatars%2C%20allowing%20for%20the%20manipulation%20of%20avatars%20into%20novel%0Aposes%20under%20diverse%20lighting%20conditions.%20Specifically%2C%20our%20approach%20integrates%0Apre-integration%20and%20image-based%20lighting%20for%20fast%20light%20calculations%20that%0Asurpass%20the%20performance%20of%20existing%20implicit-based%20techniques.%20To%20address%0Achallenges%20related%20to%20material%20lighting%20disentanglement%20and%20accurate%20geometry%0Areconstruction%2C%20we%20propose%20an%20innovative%20occlusion%20approximation%20strategy%20and%20a%0Aprogressive%20training%20approach.%20Extensive%20experiments%20demonstrate%20that%20SGIA%20not%0Aonly%20achieves%20highly%20accurate%20physical%20properties%20but%20also%20significantly%0Aenhances%20the%20realistic%20relighting%20of%20dynamic%20human%20avatars%2C%20providing%20a%0Asubstantial%20speed%20advantage.%20We%20exhibit%20more%20results%20in%20our%20project%20page%3A%0Ahttps%3A//GS-IA.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15212v2&entry.124074799=Read"},
{"title": "HDRSplat: Gaussian Splatting for High Dynamic Range 3D Scene\n  Reconstruction from Raw Images", "author": "Shreyas Singh and Aryan Garg and Kaushik Mitra", "abstract": "  The recent advent of 3D Gaussian Splatting (3DGS) has revolutionized the 3D\nscene reconstruction space enabling high-fidelity novel view synthesis in\nreal-time. However, with the exception of RawNeRF, all prior 3DGS and\nNeRF-based methods rely on 8-bit tone-mapped Low Dynamic Range (LDR) images for\nscene reconstruction. Such methods struggle to achieve accurate reconstructions\nin scenes that require a higher dynamic range. Examples include scenes captured\nin nighttime or poorly lit indoor spaces having a low signal-to-noise ratio, as\nwell as daylight scenes with shadow regions exhibiting extreme contrast. Our\nproposed method HDRSplat tailors 3DGS to train directly on 14-bit linear raw\nimages in near darkness which preserves the scenes' full dynamic range and\ncontent. Our key contributions are two-fold: Firstly, we propose a linear HDR\nspace-suited loss that effectively extracts scene information from noisy dark\nregions and nearly saturated bright regions simultaneously, while also handling\nview-dependent colors without increasing the degree of spherical harmonics.\nSecondly, through careful rasterization tuning, we implicitly overcome the\nheavy reliance and sensitivity of 3DGS on point cloud initialization. This is\ncritical for accurate reconstruction in regions of low texture, high depth of\nfield, and low illumination. HDRSplat is the fastest method to date that does\n14-bit (HDR) 3D scene reconstruction in $\\le$15 minutes/scene ($\\sim$30x faster\nthan prior state-of-the-art RawNeRF). It also boasts the fastest inference\nspeed at $\\ge$120fps. We further demonstrate the applicability of our HDR scene\nreconstruction by showcasing various applications like synthetic defocus, dense\ndepth map extraction, and post-capture control of exposure, tone-mapping and\nview-point.\n", "link": "http://arxiv.org/abs/2407.16503v1", "date": "2024-07-23", "relevancy": 3.1952, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7777}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5947}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HDRSplat%3A%20Gaussian%20Splatting%20for%20High%20Dynamic%20Range%203D%20Scene%0A%20%20Reconstruction%20from%20Raw%20Images&body=Title%3A%20HDRSplat%3A%20Gaussian%20Splatting%20for%20High%20Dynamic%20Range%203D%20Scene%0A%20%20Reconstruction%20from%20Raw%20Images%0AAuthor%3A%20Shreyas%20Singh%20and%20Aryan%20Garg%20and%20Kaushik%20Mitra%0AAbstract%3A%20%20%20The%20recent%20advent%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20revolutionized%20the%203D%0Ascene%20reconstruction%20space%20enabling%20high-fidelity%20novel%20view%20synthesis%20in%0Areal-time.%20However%2C%20with%20the%20exception%20of%20RawNeRF%2C%20all%20prior%203DGS%20and%0ANeRF-based%20methods%20rely%20on%208-bit%20tone-mapped%20Low%20Dynamic%20Range%20%28LDR%29%20images%20for%0Ascene%20reconstruction.%20Such%20methods%20struggle%20to%20achieve%20accurate%20reconstructions%0Ain%20scenes%20that%20require%20a%20higher%20dynamic%20range.%20Examples%20include%20scenes%20captured%0Ain%20nighttime%20or%20poorly%20lit%20indoor%20spaces%20having%20a%20low%20signal-to-noise%20ratio%2C%20as%0Awell%20as%20daylight%20scenes%20with%20shadow%20regions%20exhibiting%20extreme%20contrast.%20Our%0Aproposed%20method%20HDRSplat%20tailors%203DGS%20to%20train%20directly%20on%2014-bit%20linear%20raw%0Aimages%20in%20near%20darkness%20which%20preserves%20the%20scenes%27%20full%20dynamic%20range%20and%0Acontent.%20Our%20key%20contributions%20are%20two-fold%3A%20Firstly%2C%20we%20propose%20a%20linear%20HDR%0Aspace-suited%20loss%20that%20effectively%20extracts%20scene%20information%20from%20noisy%20dark%0Aregions%20and%20nearly%20saturated%20bright%20regions%20simultaneously%2C%20while%20also%20handling%0Aview-dependent%20colors%20without%20increasing%20the%20degree%20of%20spherical%20harmonics.%0ASecondly%2C%20through%20careful%20rasterization%20tuning%2C%20we%20implicitly%20overcome%20the%0Aheavy%20reliance%20and%20sensitivity%20of%203DGS%20on%20point%20cloud%20initialization.%20This%20is%0Acritical%20for%20accurate%20reconstruction%20in%20regions%20of%20low%20texture%2C%20high%20depth%20of%0Afield%2C%20and%20low%20illumination.%20HDRSplat%20is%20the%20fastest%20method%20to%20date%20that%20does%0A14-bit%20%28HDR%29%203D%20scene%20reconstruction%20in%20%24%5Cle%2415%20minutes/scene%20%28%24%5Csim%2430x%20faster%0Athan%20prior%20state-of-the-art%20RawNeRF%29.%20It%20also%20boasts%20the%20fastest%20inference%0Aspeed%20at%20%24%5Cge%24120fps.%20We%20further%20demonstrate%20the%20applicability%20of%20our%20HDR%20scene%0Areconstruction%20by%20showcasing%20various%20applications%20like%20synthetic%20defocus%2C%20dense%0Adepth%20map%20extraction%2C%20and%20post-capture%20control%20of%20exposure%2C%20tone-mapping%20and%0Aview-point.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHDRSplat%253A%2520Gaussian%2520Splatting%2520for%2520High%2520Dynamic%2520Range%25203D%2520Scene%250A%2520%2520Reconstruction%2520from%2520Raw%2520Images%26entry.906535625%3DShreyas%2520Singh%2520and%2520Aryan%2520Garg%2520and%2520Kaushik%2520Mitra%26entry.1292438233%3D%2520%2520The%2520recent%2520advent%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520revolutionized%2520the%25203D%250Ascene%2520reconstruction%2520space%2520enabling%2520high-fidelity%2520novel%2520view%2520synthesis%2520in%250Areal-time.%2520However%252C%2520with%2520the%2520exception%2520of%2520RawNeRF%252C%2520all%2520prior%25203DGS%2520and%250ANeRF-based%2520methods%2520rely%2520on%25208-bit%2520tone-mapped%2520Low%2520Dynamic%2520Range%2520%2528LDR%2529%2520images%2520for%250Ascene%2520reconstruction.%2520Such%2520methods%2520struggle%2520to%2520achieve%2520accurate%2520reconstructions%250Ain%2520scenes%2520that%2520require%2520a%2520higher%2520dynamic%2520range.%2520Examples%2520include%2520scenes%2520captured%250Ain%2520nighttime%2520or%2520poorly%2520lit%2520indoor%2520spaces%2520having%2520a%2520low%2520signal-to-noise%2520ratio%252C%2520as%250Awell%2520as%2520daylight%2520scenes%2520with%2520shadow%2520regions%2520exhibiting%2520extreme%2520contrast.%2520Our%250Aproposed%2520method%2520HDRSplat%2520tailors%25203DGS%2520to%2520train%2520directly%2520on%252014-bit%2520linear%2520raw%250Aimages%2520in%2520near%2520darkness%2520which%2520preserves%2520the%2520scenes%2527%2520full%2520dynamic%2520range%2520and%250Acontent.%2520Our%2520key%2520contributions%2520are%2520two-fold%253A%2520Firstly%252C%2520we%2520propose%2520a%2520linear%2520HDR%250Aspace-suited%2520loss%2520that%2520effectively%2520extracts%2520scene%2520information%2520from%2520noisy%2520dark%250Aregions%2520and%2520nearly%2520saturated%2520bright%2520regions%2520simultaneously%252C%2520while%2520also%2520handling%250Aview-dependent%2520colors%2520without%2520increasing%2520the%2520degree%2520of%2520spherical%2520harmonics.%250ASecondly%252C%2520through%2520careful%2520rasterization%2520tuning%252C%2520we%2520implicitly%2520overcome%2520the%250Aheavy%2520reliance%2520and%2520sensitivity%2520of%25203DGS%2520on%2520point%2520cloud%2520initialization.%2520This%2520is%250Acritical%2520for%2520accurate%2520reconstruction%2520in%2520regions%2520of%2520low%2520texture%252C%2520high%2520depth%2520of%250Afield%252C%2520and%2520low%2520illumination.%2520HDRSplat%2520is%2520the%2520fastest%2520method%2520to%2520date%2520that%2520does%250A14-bit%2520%2528HDR%2529%25203D%2520scene%2520reconstruction%2520in%2520%2524%255Cle%252415%2520minutes/scene%2520%2528%2524%255Csim%252430x%2520faster%250Athan%2520prior%2520state-of-the-art%2520RawNeRF%2529.%2520It%2520also%2520boasts%2520the%2520fastest%2520inference%250Aspeed%2520at%2520%2524%255Cge%2524120fps.%2520We%2520further%2520demonstrate%2520the%2520applicability%2520of%2520our%2520HDR%2520scene%250Areconstruction%2520by%2520showcasing%2520various%2520applications%2520like%2520synthetic%2520defocus%252C%2520dense%250Adepth%2520map%2520extraction%252C%2520and%2520post-capture%2520control%2520of%2520exposure%252C%2520tone-mapping%2520and%250Aview-point.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HDRSplat%3A%20Gaussian%20Splatting%20for%20High%20Dynamic%20Range%203D%20Scene%0A%20%20Reconstruction%20from%20Raw%20Images&entry.906535625=Shreyas%20Singh%20and%20Aryan%20Garg%20and%20Kaushik%20Mitra&entry.1292438233=%20%20The%20recent%20advent%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20revolutionized%20the%203D%0Ascene%20reconstruction%20space%20enabling%20high-fidelity%20novel%20view%20synthesis%20in%0Areal-time.%20However%2C%20with%20the%20exception%20of%20RawNeRF%2C%20all%20prior%203DGS%20and%0ANeRF-based%20methods%20rely%20on%208-bit%20tone-mapped%20Low%20Dynamic%20Range%20%28LDR%29%20images%20for%0Ascene%20reconstruction.%20Such%20methods%20struggle%20to%20achieve%20accurate%20reconstructions%0Ain%20scenes%20that%20require%20a%20higher%20dynamic%20range.%20Examples%20include%20scenes%20captured%0Ain%20nighttime%20or%20poorly%20lit%20indoor%20spaces%20having%20a%20low%20signal-to-noise%20ratio%2C%20as%0Awell%20as%20daylight%20scenes%20with%20shadow%20regions%20exhibiting%20extreme%20contrast.%20Our%0Aproposed%20method%20HDRSplat%20tailors%203DGS%20to%20train%20directly%20on%2014-bit%20linear%20raw%0Aimages%20in%20near%20darkness%20which%20preserves%20the%20scenes%27%20full%20dynamic%20range%20and%0Acontent.%20Our%20key%20contributions%20are%20two-fold%3A%20Firstly%2C%20we%20propose%20a%20linear%20HDR%0Aspace-suited%20loss%20that%20effectively%20extracts%20scene%20information%20from%20noisy%20dark%0Aregions%20and%20nearly%20saturated%20bright%20regions%20simultaneously%2C%20while%20also%20handling%0Aview-dependent%20colors%20without%20increasing%20the%20degree%20of%20spherical%20harmonics.%0ASecondly%2C%20through%20careful%20rasterization%20tuning%2C%20we%20implicitly%20overcome%20the%0Aheavy%20reliance%20and%20sensitivity%20of%203DGS%20on%20point%20cloud%20initialization.%20This%20is%0Acritical%20for%20accurate%20reconstruction%20in%20regions%20of%20low%20texture%2C%20high%20depth%20of%0Afield%2C%20and%20low%20illumination.%20HDRSplat%20is%20the%20fastest%20method%20to%20date%20that%20does%0A14-bit%20%28HDR%29%203D%20scene%20reconstruction%20in%20%24%5Cle%2415%20minutes/scene%20%28%24%5Csim%2430x%20faster%0Athan%20prior%20state-of-the-art%20RawNeRF%29.%20It%20also%20boasts%20the%20fastest%20inference%0Aspeed%20at%20%24%5Cge%24120fps.%20We%20further%20demonstrate%20the%20applicability%20of%20our%20HDR%20scene%0Areconstruction%20by%20showcasing%20various%20applications%20like%20synthetic%20defocus%2C%20dense%0Adepth%20map%20extraction%2C%20and%20post-capture%20control%20of%20exposure%2C%20tone-mapping%20and%0Aview-point.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16503v1&entry.124074799=Read"},
{"title": "Learning to Generate Conditional Tri-plane for 3D-aware Expression\n  Controllable Portrait Animation", "author": "Taekyung Ki and Dongchan Min and Gyeongsu Chae", "abstract": "  In this paper, we present Export3D, a one-shot 3D-aware portrait animation\nmethod that is able to control the facial expression and camera view of a given\nportrait image. To achieve this, we introduce a tri-plane generator with an\neffective expression conditioning method, which directly generates a tri-plane\nof 3D prior by transferring the expression parameter of 3DMM into the source\nimage. The tri-plane is then decoded into the image of different view through a\ndifferentiable volume rendering. Existing portrait animation methods heavily\nrely on image warping to transfer the expression in the motion space,\nchallenging on disentanglement of appearance and expression. In contrast, we\npropose a contrastive pre-training framework for appearance-free expression\nparameter, eliminating undesirable appearance swap when transferring a\ncross-identity expression. Extensive experiments show that our pre-training\nframework can learn the appearance-free expression representation hidden in\n3DMM, and our model can generate 3D-aware expression controllable portrait\nimages without appearance swap in the cross-identity manner.\n", "link": "http://arxiv.org/abs/2404.00636v3", "date": "2024-07-23", "relevancy": 3.0711, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6245}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6245}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Generate%20Conditional%20Tri-plane%20for%203D-aware%20Expression%0A%20%20Controllable%20Portrait%20Animation&body=Title%3A%20Learning%20to%20Generate%20Conditional%20Tri-plane%20for%203D-aware%20Expression%0A%20%20Controllable%20Portrait%20Animation%0AAuthor%3A%20Taekyung%20Ki%20and%20Dongchan%20Min%20and%20Gyeongsu%20Chae%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20Export3D%2C%20a%20one-shot%203D-aware%20portrait%20animation%0Amethod%20that%20is%20able%20to%20control%20the%20facial%20expression%20and%20camera%20view%20of%20a%20given%0Aportrait%20image.%20To%20achieve%20this%2C%20we%20introduce%20a%20tri-plane%20generator%20with%20an%0Aeffective%20expression%20conditioning%20method%2C%20which%20directly%20generates%20a%20tri-plane%0Aof%203D%20prior%20by%20transferring%20the%20expression%20parameter%20of%203DMM%20into%20the%20source%0Aimage.%20The%20tri-plane%20is%20then%20decoded%20into%20the%20image%20of%20different%20view%20through%20a%0Adifferentiable%20volume%20rendering.%20Existing%20portrait%20animation%20methods%20heavily%0Arely%20on%20image%20warping%20to%20transfer%20the%20expression%20in%20the%20motion%20space%2C%0Achallenging%20on%20disentanglement%20of%20appearance%20and%20expression.%20In%20contrast%2C%20we%0Apropose%20a%20contrastive%20pre-training%20framework%20for%20appearance-free%20expression%0Aparameter%2C%20eliminating%20undesirable%20appearance%20swap%20when%20transferring%20a%0Across-identity%20expression.%20Extensive%20experiments%20show%20that%20our%20pre-training%0Aframework%20can%20learn%20the%20appearance-free%20expression%20representation%20hidden%20in%0A3DMM%2C%20and%20our%20model%20can%20generate%203D-aware%20expression%20controllable%20portrait%0Aimages%20without%20appearance%20swap%20in%20the%20cross-identity%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00636v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Generate%2520Conditional%2520Tri-plane%2520for%25203D-aware%2520Expression%250A%2520%2520Controllable%2520Portrait%2520Animation%26entry.906535625%3DTaekyung%2520Ki%2520and%2520Dongchan%2520Min%2520and%2520Gyeongsu%2520Chae%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520Export3D%252C%2520a%2520one-shot%25203D-aware%2520portrait%2520animation%250Amethod%2520that%2520is%2520able%2520to%2520control%2520the%2520facial%2520expression%2520and%2520camera%2520view%2520of%2520a%2520given%250Aportrait%2520image.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520a%2520tri-plane%2520generator%2520with%2520an%250Aeffective%2520expression%2520conditioning%2520method%252C%2520which%2520directly%2520generates%2520a%2520tri-plane%250Aof%25203D%2520prior%2520by%2520transferring%2520the%2520expression%2520parameter%2520of%25203DMM%2520into%2520the%2520source%250Aimage.%2520The%2520tri-plane%2520is%2520then%2520decoded%2520into%2520the%2520image%2520of%2520different%2520view%2520through%2520a%250Adifferentiable%2520volume%2520rendering.%2520Existing%2520portrait%2520animation%2520methods%2520heavily%250Arely%2520on%2520image%2520warping%2520to%2520transfer%2520the%2520expression%2520in%2520the%2520motion%2520space%252C%250Achallenging%2520on%2520disentanglement%2520of%2520appearance%2520and%2520expression.%2520In%2520contrast%252C%2520we%250Apropose%2520a%2520contrastive%2520pre-training%2520framework%2520for%2520appearance-free%2520expression%250Aparameter%252C%2520eliminating%2520undesirable%2520appearance%2520swap%2520when%2520transferring%2520a%250Across-identity%2520expression.%2520Extensive%2520experiments%2520show%2520that%2520our%2520pre-training%250Aframework%2520can%2520learn%2520the%2520appearance-free%2520expression%2520representation%2520hidden%2520in%250A3DMM%252C%2520and%2520our%2520model%2520can%2520generate%25203D-aware%2520expression%2520controllable%2520portrait%250Aimages%2520without%2520appearance%2520swap%2520in%2520the%2520cross-identity%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00636v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Generate%20Conditional%20Tri-plane%20for%203D-aware%20Expression%0A%20%20Controllable%20Portrait%20Animation&entry.906535625=Taekyung%20Ki%20and%20Dongchan%20Min%20and%20Gyeongsu%20Chae&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20Export3D%2C%20a%20one-shot%203D-aware%20portrait%20animation%0Amethod%20that%20is%20able%20to%20control%20the%20facial%20expression%20and%20camera%20view%20of%20a%20given%0Aportrait%20image.%20To%20achieve%20this%2C%20we%20introduce%20a%20tri-plane%20generator%20with%20an%0Aeffective%20expression%20conditioning%20method%2C%20which%20directly%20generates%20a%20tri-plane%0Aof%203D%20prior%20by%20transferring%20the%20expression%20parameter%20of%203DMM%20into%20the%20source%0Aimage.%20The%20tri-plane%20is%20then%20decoded%20into%20the%20image%20of%20different%20view%20through%20a%0Adifferentiable%20volume%20rendering.%20Existing%20portrait%20animation%20methods%20heavily%0Arely%20on%20image%20warping%20to%20transfer%20the%20expression%20in%20the%20motion%20space%2C%0Achallenging%20on%20disentanglement%20of%20appearance%20and%20expression.%20In%20contrast%2C%20we%0Apropose%20a%20contrastive%20pre-training%20framework%20for%20appearance-free%20expression%0Aparameter%2C%20eliminating%20undesirable%20appearance%20swap%20when%20transferring%20a%0Across-identity%20expression.%20Extensive%20experiments%20show%20that%20our%20pre-training%0Aframework%20can%20learn%20the%20appearance-free%20expression%20representation%20hidden%20in%0A3DMM%2C%20and%20our%20model%20can%20generate%203D-aware%20expression%20controllable%20portrait%0Aimages%20without%20appearance%20swap%20in%20the%20cross-identity%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00636v3&entry.124074799=Read"},
{"title": "Beyond the Contact: Discovering Comprehensive Affordance for 3D Objects\n  from Pre-trained 2D Diffusion Models", "author": "Hyeonwoo Kim and Sookwan Han and Patrick Kwon and Hanbyul Joo", "abstract": "  Understanding the inherent human knowledge in interacting with a given\nenvironment (e.g., affordance) is essential for improving AI to better assist\nhumans. While existing approaches primarily focus on human-object contacts\nduring interactions, such affordance representation cannot fully address other\nimportant aspects of human-object interactions (HOIs), i.e., patterns of\nrelative positions and orientations. In this paper, we introduce a novel\naffordance representation, named Comprehensive Affordance (ComA). Given a 3D\nobject mesh, ComA models the distribution of relative orientation and proximity\nof vertices in interacting human meshes, capturing plausible patterns of\ncontact, relative orientations, and spatial relationships. To construct the\ndistribution, we present a novel pipeline that synthesizes diverse and\nrealistic 3D HOI samples given any 3D object mesh. The pipeline leverages a\npre-trained 2D inpainting diffusion model to generate HOI images from object\nrenderings and lifts them into 3D. To avoid the generation of false\naffordances, we propose a new inpainting framework, Adaptive Mask Inpainting.\nSince ComA is built on synthetic samples, it can extend to any object in an\nunbounded manner. Through extensive experiments, we demonstrate that ComA\noutperforms competitors that rely on human annotations in modeling\ncontact-based affordance. Importantly, we also showcase the potential of ComA\nto reconstruct human-object interactions in 3D through an optimization\nframework, highlighting its advantage in incorporating both contact and\nnon-contact properties.\n", "link": "http://arxiv.org/abs/2401.12978v3", "date": "2024-07-23", "relevancy": 2.9566, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5981}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5981}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Contact%3A%20Discovering%20Comprehensive%20Affordance%20for%203D%20Objects%0A%20%20from%20Pre-trained%202D%20Diffusion%20Models&body=Title%3A%20Beyond%20the%20Contact%3A%20Discovering%20Comprehensive%20Affordance%20for%203D%20Objects%0A%20%20from%20Pre-trained%202D%20Diffusion%20Models%0AAuthor%3A%20Hyeonwoo%20Kim%20and%20Sookwan%20Han%20and%20Patrick%20Kwon%20and%20Hanbyul%20Joo%0AAbstract%3A%20%20%20Understanding%20the%20inherent%20human%20knowledge%20in%20interacting%20with%20a%20given%0Aenvironment%20%28e.g.%2C%20affordance%29%20is%20essential%20for%20improving%20AI%20to%20better%20assist%0Ahumans.%20While%20existing%20approaches%20primarily%20focus%20on%20human-object%20contacts%0Aduring%20interactions%2C%20such%20affordance%20representation%20cannot%20fully%20address%20other%0Aimportant%20aspects%20of%20human-object%20interactions%20%28HOIs%29%2C%20i.e.%2C%20patterns%20of%0Arelative%20positions%20and%20orientations.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Aaffordance%20representation%2C%20named%20Comprehensive%20Affordance%20%28ComA%29.%20Given%20a%203D%0Aobject%20mesh%2C%20ComA%20models%20the%20distribution%20of%20relative%20orientation%20and%20proximity%0Aof%20vertices%20in%20interacting%20human%20meshes%2C%20capturing%20plausible%20patterns%20of%0Acontact%2C%20relative%20orientations%2C%20and%20spatial%20relationships.%20To%20construct%20the%0Adistribution%2C%20we%20present%20a%20novel%20pipeline%20that%20synthesizes%20diverse%20and%0Arealistic%203D%20HOI%20samples%20given%20any%203D%20object%20mesh.%20The%20pipeline%20leverages%20a%0Apre-trained%202D%20inpainting%20diffusion%20model%20to%20generate%20HOI%20images%20from%20object%0Arenderings%20and%20lifts%20them%20into%203D.%20To%20avoid%20the%20generation%20of%20false%0Aaffordances%2C%20we%20propose%20a%20new%20inpainting%20framework%2C%20Adaptive%20Mask%20Inpainting.%0ASince%20ComA%20is%20built%20on%20synthetic%20samples%2C%20it%20can%20extend%20to%20any%20object%20in%20an%0Aunbounded%20manner.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20ComA%0Aoutperforms%20competitors%20that%20rely%20on%20human%20annotations%20in%20modeling%0Acontact-based%20affordance.%20Importantly%2C%20we%20also%20showcase%20the%20potential%20of%20ComA%0Ato%20reconstruct%20human-object%20interactions%20in%203D%20through%20an%20optimization%0Aframework%2C%20highlighting%20its%20advantage%20in%20incorporating%20both%20contact%20and%0Anon-contact%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12978v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Contact%253A%2520Discovering%2520Comprehensive%2520Affordance%2520for%25203D%2520Objects%250A%2520%2520from%2520Pre-trained%25202D%2520Diffusion%2520Models%26entry.906535625%3DHyeonwoo%2520Kim%2520and%2520Sookwan%2520Han%2520and%2520Patrick%2520Kwon%2520and%2520Hanbyul%2520Joo%26entry.1292438233%3D%2520%2520Understanding%2520the%2520inherent%2520human%2520knowledge%2520in%2520interacting%2520with%2520a%2520given%250Aenvironment%2520%2528e.g.%252C%2520affordance%2529%2520is%2520essential%2520for%2520improving%2520AI%2520to%2520better%2520assist%250Ahumans.%2520While%2520existing%2520approaches%2520primarily%2520focus%2520on%2520human-object%2520contacts%250Aduring%2520interactions%252C%2520such%2520affordance%2520representation%2520cannot%2520fully%2520address%2520other%250Aimportant%2520aspects%2520of%2520human-object%2520interactions%2520%2528HOIs%2529%252C%2520i.e.%252C%2520patterns%2520of%250Arelative%2520positions%2520and%2520orientations.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250Aaffordance%2520representation%252C%2520named%2520Comprehensive%2520Affordance%2520%2528ComA%2529.%2520Given%2520a%25203D%250Aobject%2520mesh%252C%2520ComA%2520models%2520the%2520distribution%2520of%2520relative%2520orientation%2520and%2520proximity%250Aof%2520vertices%2520in%2520interacting%2520human%2520meshes%252C%2520capturing%2520plausible%2520patterns%2520of%250Acontact%252C%2520relative%2520orientations%252C%2520and%2520spatial%2520relationships.%2520To%2520construct%2520the%250Adistribution%252C%2520we%2520present%2520a%2520novel%2520pipeline%2520that%2520synthesizes%2520diverse%2520and%250Arealistic%25203D%2520HOI%2520samples%2520given%2520any%25203D%2520object%2520mesh.%2520The%2520pipeline%2520leverages%2520a%250Apre-trained%25202D%2520inpainting%2520diffusion%2520model%2520to%2520generate%2520HOI%2520images%2520from%2520object%250Arenderings%2520and%2520lifts%2520them%2520into%25203D.%2520To%2520avoid%2520the%2520generation%2520of%2520false%250Aaffordances%252C%2520we%2520propose%2520a%2520new%2520inpainting%2520framework%252C%2520Adaptive%2520Mask%2520Inpainting.%250ASince%2520ComA%2520is%2520built%2520on%2520synthetic%2520samples%252C%2520it%2520can%2520extend%2520to%2520any%2520object%2520in%2520an%250Aunbounded%2520manner.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520ComA%250Aoutperforms%2520competitors%2520that%2520rely%2520on%2520human%2520annotations%2520in%2520modeling%250Acontact-based%2520affordance.%2520Importantly%252C%2520we%2520also%2520showcase%2520the%2520potential%2520of%2520ComA%250Ato%2520reconstruct%2520human-object%2520interactions%2520in%25203D%2520through%2520an%2520optimization%250Aframework%252C%2520highlighting%2520its%2520advantage%2520in%2520incorporating%2520both%2520contact%2520and%250Anon-contact%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12978v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Contact%3A%20Discovering%20Comprehensive%20Affordance%20for%203D%20Objects%0A%20%20from%20Pre-trained%202D%20Diffusion%20Models&entry.906535625=Hyeonwoo%20Kim%20and%20Sookwan%20Han%20and%20Patrick%20Kwon%20and%20Hanbyul%20Joo&entry.1292438233=%20%20Understanding%20the%20inherent%20human%20knowledge%20in%20interacting%20with%20a%20given%0Aenvironment%20%28e.g.%2C%20affordance%29%20is%20essential%20for%20improving%20AI%20to%20better%20assist%0Ahumans.%20While%20existing%20approaches%20primarily%20focus%20on%20human-object%20contacts%0Aduring%20interactions%2C%20such%20affordance%20representation%20cannot%20fully%20address%20other%0Aimportant%20aspects%20of%20human-object%20interactions%20%28HOIs%29%2C%20i.e.%2C%20patterns%20of%0Arelative%20positions%20and%20orientations.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Aaffordance%20representation%2C%20named%20Comprehensive%20Affordance%20%28ComA%29.%20Given%20a%203D%0Aobject%20mesh%2C%20ComA%20models%20the%20distribution%20of%20relative%20orientation%20and%20proximity%0Aof%20vertices%20in%20interacting%20human%20meshes%2C%20capturing%20plausible%20patterns%20of%0Acontact%2C%20relative%20orientations%2C%20and%20spatial%20relationships.%20To%20construct%20the%0Adistribution%2C%20we%20present%20a%20novel%20pipeline%20that%20synthesizes%20diverse%20and%0Arealistic%203D%20HOI%20samples%20given%20any%203D%20object%20mesh.%20The%20pipeline%20leverages%20a%0Apre-trained%202D%20inpainting%20diffusion%20model%20to%20generate%20HOI%20images%20from%20object%0Arenderings%20and%20lifts%20them%20into%203D.%20To%20avoid%20the%20generation%20of%20false%0Aaffordances%2C%20we%20propose%20a%20new%20inpainting%20framework%2C%20Adaptive%20Mask%20Inpainting.%0ASince%20ComA%20is%20built%20on%20synthetic%20samples%2C%20it%20can%20extend%20to%20any%20object%20in%20an%0Aunbounded%20manner.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20ComA%0Aoutperforms%20competitors%20that%20rely%20on%20human%20annotations%20in%20modeling%0Acontact-based%20affordance.%20Importantly%2C%20we%20also%20showcase%20the%20potential%20of%20ComA%0Ato%20reconstruct%20human-object%20interactions%20in%203D%20through%20an%20optimization%0Aframework%2C%20highlighting%20its%20advantage%20in%20incorporating%20both%20contact%20and%0Anon-contact%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12978v3&entry.124074799=Read"},
{"title": "COMO: Compact Mapping and Odometry", "author": "Eric Dexheimer and Andrew J. Davison", "abstract": "  We present COMO, a real-time monocular mapping and odometry system that\nencodes dense geometry via a compact set of 3D anchor points. Decoding anchor\npoint projections into dense geometry via per-keyframe depth covariance\nfunctions guarantees that depth maps are joined together at visible anchor\npoints. The representation enables joint optimization of camera poses and dense\ngeometry, intrinsic 3D consistency, and efficient second-order inference. To\nmaintain a compact yet expressive map, we introduce a frontend that leverages\nthe covariance function for tracking and initializing potentially visually\nindistinct 3D points across frames. Altogether, we introduce a real-time system\ncapable of estimating accurate poses and consistent geometry.\n", "link": "http://arxiv.org/abs/2404.03531v2", "date": "2024-07-23", "relevancy": 2.9561, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6151}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5838}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COMO%3A%20Compact%20Mapping%20and%20Odometry&body=Title%3A%20COMO%3A%20Compact%20Mapping%20and%20Odometry%0AAuthor%3A%20Eric%20Dexheimer%20and%20Andrew%20J.%20Davison%0AAbstract%3A%20%20%20We%20present%20COMO%2C%20a%20real-time%20monocular%20mapping%20and%20odometry%20system%20that%0Aencodes%20dense%20geometry%20via%20a%20compact%20set%20of%203D%20anchor%20points.%20Decoding%20anchor%0Apoint%20projections%20into%20dense%20geometry%20via%20per-keyframe%20depth%20covariance%0Afunctions%20guarantees%20that%20depth%20maps%20are%20joined%20together%20at%20visible%20anchor%0Apoints.%20The%20representation%20enables%20joint%20optimization%20of%20camera%20poses%20and%20dense%0Ageometry%2C%20intrinsic%203D%20consistency%2C%20and%20efficient%20second-order%20inference.%20To%0Amaintain%20a%20compact%20yet%20expressive%20map%2C%20we%20introduce%20a%20frontend%20that%20leverages%0Athe%20covariance%20function%20for%20tracking%20and%20initializing%20potentially%20visually%0Aindistinct%203D%20points%20across%20frames.%20Altogether%2C%20we%20introduce%20a%20real-time%20system%0Acapable%20of%20estimating%20accurate%20poses%20and%20consistent%20geometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03531v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOMO%253A%2520Compact%2520Mapping%2520and%2520Odometry%26entry.906535625%3DEric%2520Dexheimer%2520and%2520Andrew%2520J.%2520Davison%26entry.1292438233%3D%2520%2520We%2520present%2520COMO%252C%2520a%2520real-time%2520monocular%2520mapping%2520and%2520odometry%2520system%2520that%250Aencodes%2520dense%2520geometry%2520via%2520a%2520compact%2520set%2520of%25203D%2520anchor%2520points.%2520Decoding%2520anchor%250Apoint%2520projections%2520into%2520dense%2520geometry%2520via%2520per-keyframe%2520depth%2520covariance%250Afunctions%2520guarantees%2520that%2520depth%2520maps%2520are%2520joined%2520together%2520at%2520visible%2520anchor%250Apoints.%2520The%2520representation%2520enables%2520joint%2520optimization%2520of%2520camera%2520poses%2520and%2520dense%250Ageometry%252C%2520intrinsic%25203D%2520consistency%252C%2520and%2520efficient%2520second-order%2520inference.%2520To%250Amaintain%2520a%2520compact%2520yet%2520expressive%2520map%252C%2520we%2520introduce%2520a%2520frontend%2520that%2520leverages%250Athe%2520covariance%2520function%2520for%2520tracking%2520and%2520initializing%2520potentially%2520visually%250Aindistinct%25203D%2520points%2520across%2520frames.%2520Altogether%252C%2520we%2520introduce%2520a%2520real-time%2520system%250Acapable%2520of%2520estimating%2520accurate%2520poses%2520and%2520consistent%2520geometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03531v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMO%3A%20Compact%20Mapping%20and%20Odometry&entry.906535625=Eric%20Dexheimer%20and%20Andrew%20J.%20Davison&entry.1292438233=%20%20We%20present%20COMO%2C%20a%20real-time%20monocular%20mapping%20and%20odometry%20system%20that%0Aencodes%20dense%20geometry%20via%20a%20compact%20set%20of%203D%20anchor%20points.%20Decoding%20anchor%0Apoint%20projections%20into%20dense%20geometry%20via%20per-keyframe%20depth%20covariance%0Afunctions%20guarantees%20that%20depth%20maps%20are%20joined%20together%20at%20visible%20anchor%0Apoints.%20The%20representation%20enables%20joint%20optimization%20of%20camera%20poses%20and%20dense%0Ageometry%2C%20intrinsic%203D%20consistency%2C%20and%20efficient%20second-order%20inference.%20To%0Amaintain%20a%20compact%20yet%20expressive%20map%2C%20we%20introduce%20a%20frontend%20that%20leverages%0Athe%20covariance%20function%20for%20tracking%20and%20initializing%20potentially%20visually%0Aindistinct%203D%20points%20across%20frames.%20Altogether%2C%20we%20introduce%20a%20real-time%20system%0Acapable%20of%20estimating%20accurate%20poses%20and%20consistent%20geometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03531v2&entry.124074799=Read"},
{"title": "RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF", "author": "Sibi Catley-Chandar and Richard Shaw and Gregory Slabaugh and Eduardo Perez-Pellitero", "abstract": "  Recent advances in neural rendering have enabled highly photorealistic 3D\nscene reconstruction and novel view synthesis. Despite this progress, current\nstate-of-the-art methods struggle to reconstruct high frequency detail, due to\nfactors such as a low-frequency bias of radiance fields and inaccurate camera\ncalibration. One approach to mitigate this issue is to enhance images\npost-rendering. 2D enhancers can be pre-trained to recover some detail but are\nagnostic to scene geometry and do not easily generalize to new distributions of\nimage degradation. Conversely, existing 3D enhancers are able to transfer\ndetail from nearby training images in a generalizable manner, but suffer from\ninaccurate camera calibration and can propagate errors from the geometry into\nrendered images. We propose a neural rendering enhancer, RoGUENeRF, which\nexploits the best of both paradigms. Our method is pre-trained to learn a\ngeneral enhancer while also leveraging information from nearby training images\nvia robust 3D alignment and geometry-aware fusion. Our approach restores\nhigh-frequency textures while maintaining geometric consistency and is also\nrobust to inaccurate camera calibration. We show that RoGUENeRF substantially\nenhances the rendering quality of a wide range of neural rendering baselines,\ne.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the\nreal world 360v2 dataset.\n", "link": "http://arxiv.org/abs/2403.11909v2", "date": "2024-07-23", "relevancy": 2.8805, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5988}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5715}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoGUENeRF%3A%20A%20Robust%20Geometry-Consistent%20Universal%20Enhancer%20for%20NeRF&body=Title%3A%20RoGUENeRF%3A%20A%20Robust%20Geometry-Consistent%20Universal%20Enhancer%20for%20NeRF%0AAuthor%3A%20Sibi%20Catley-Chandar%20and%20Richard%20Shaw%20and%20Gregory%20Slabaugh%20and%20Eduardo%20Perez-Pellitero%0AAbstract%3A%20%20%20Recent%20advances%20in%20neural%20rendering%20have%20enabled%20highly%20photorealistic%203D%0Ascene%20reconstruction%20and%20novel%20view%20synthesis.%20Despite%20this%20progress%2C%20current%0Astate-of-the-art%20methods%20struggle%20to%20reconstruct%20high%20frequency%20detail%2C%20due%20to%0Afactors%20such%20as%20a%20low-frequency%20bias%20of%20radiance%20fields%20and%20inaccurate%20camera%0Acalibration.%20One%20approach%20to%20mitigate%20this%20issue%20is%20to%20enhance%20images%0Apost-rendering.%202D%20enhancers%20can%20be%20pre-trained%20to%20recover%20some%20detail%20but%20are%0Aagnostic%20to%20scene%20geometry%20and%20do%20not%20easily%20generalize%20to%20new%20distributions%20of%0Aimage%20degradation.%20Conversely%2C%20existing%203D%20enhancers%20are%20able%20to%20transfer%0Adetail%20from%20nearby%20training%20images%20in%20a%20generalizable%20manner%2C%20but%20suffer%20from%0Ainaccurate%20camera%20calibration%20and%20can%20propagate%20errors%20from%20the%20geometry%20into%0Arendered%20images.%20We%20propose%20a%20neural%20rendering%20enhancer%2C%20RoGUENeRF%2C%20which%0Aexploits%20the%20best%20of%20both%20paradigms.%20Our%20method%20is%20pre-trained%20to%20learn%20a%0Ageneral%20enhancer%20while%20also%20leveraging%20information%20from%20nearby%20training%20images%0Avia%20robust%203D%20alignment%20and%20geometry-aware%20fusion.%20Our%20approach%20restores%0Ahigh-frequency%20textures%20while%20maintaining%20geometric%20consistency%20and%20is%20also%0Arobust%20to%20inaccurate%20camera%20calibration.%20We%20show%20that%20RoGUENeRF%20substantially%0Aenhances%20the%20rendering%20quality%20of%20a%20wide%20range%20of%20neural%20rendering%20baselines%2C%0Ae.g.%20improving%20the%20PSNR%20of%20MipNeRF360%20by%200.63dB%20and%20Nerfacto%20by%201.34dB%20on%20the%0Areal%20world%20360v2%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11909v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoGUENeRF%253A%2520A%2520Robust%2520Geometry-Consistent%2520Universal%2520Enhancer%2520for%2520NeRF%26entry.906535625%3DSibi%2520Catley-Chandar%2520and%2520Richard%2520Shaw%2520and%2520Gregory%2520Slabaugh%2520and%2520Eduardo%2520Perez-Pellitero%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520neural%2520rendering%2520have%2520enabled%2520highly%2520photorealistic%25203D%250Ascene%2520reconstruction%2520and%2520novel%2520view%2520synthesis.%2520Despite%2520this%2520progress%252C%2520current%250Astate-of-the-art%2520methods%2520struggle%2520to%2520reconstruct%2520high%2520frequency%2520detail%252C%2520due%2520to%250Afactors%2520such%2520as%2520a%2520low-frequency%2520bias%2520of%2520radiance%2520fields%2520and%2520inaccurate%2520camera%250Acalibration.%2520One%2520approach%2520to%2520mitigate%2520this%2520issue%2520is%2520to%2520enhance%2520images%250Apost-rendering.%25202D%2520enhancers%2520can%2520be%2520pre-trained%2520to%2520recover%2520some%2520detail%2520but%2520are%250Aagnostic%2520to%2520scene%2520geometry%2520and%2520do%2520not%2520easily%2520generalize%2520to%2520new%2520distributions%2520of%250Aimage%2520degradation.%2520Conversely%252C%2520existing%25203D%2520enhancers%2520are%2520able%2520to%2520transfer%250Adetail%2520from%2520nearby%2520training%2520images%2520in%2520a%2520generalizable%2520manner%252C%2520but%2520suffer%2520from%250Ainaccurate%2520camera%2520calibration%2520and%2520can%2520propagate%2520errors%2520from%2520the%2520geometry%2520into%250Arendered%2520images.%2520We%2520propose%2520a%2520neural%2520rendering%2520enhancer%252C%2520RoGUENeRF%252C%2520which%250Aexploits%2520the%2520best%2520of%2520both%2520paradigms.%2520Our%2520method%2520is%2520pre-trained%2520to%2520learn%2520a%250Ageneral%2520enhancer%2520while%2520also%2520leveraging%2520information%2520from%2520nearby%2520training%2520images%250Avia%2520robust%25203D%2520alignment%2520and%2520geometry-aware%2520fusion.%2520Our%2520approach%2520restores%250Ahigh-frequency%2520textures%2520while%2520maintaining%2520geometric%2520consistency%2520and%2520is%2520also%250Arobust%2520to%2520inaccurate%2520camera%2520calibration.%2520We%2520show%2520that%2520RoGUENeRF%2520substantially%250Aenhances%2520the%2520rendering%2520quality%2520of%2520a%2520wide%2520range%2520of%2520neural%2520rendering%2520baselines%252C%250Ae.g.%2520improving%2520the%2520PSNR%2520of%2520MipNeRF360%2520by%25200.63dB%2520and%2520Nerfacto%2520by%25201.34dB%2520on%2520the%250Areal%2520world%2520360v2%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11909v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoGUENeRF%3A%20A%20Robust%20Geometry-Consistent%20Universal%20Enhancer%20for%20NeRF&entry.906535625=Sibi%20Catley-Chandar%20and%20Richard%20Shaw%20and%20Gregory%20Slabaugh%20and%20Eduardo%20Perez-Pellitero&entry.1292438233=%20%20Recent%20advances%20in%20neural%20rendering%20have%20enabled%20highly%20photorealistic%203D%0Ascene%20reconstruction%20and%20novel%20view%20synthesis.%20Despite%20this%20progress%2C%20current%0Astate-of-the-art%20methods%20struggle%20to%20reconstruct%20high%20frequency%20detail%2C%20due%20to%0Afactors%20such%20as%20a%20low-frequency%20bias%20of%20radiance%20fields%20and%20inaccurate%20camera%0Acalibration.%20One%20approach%20to%20mitigate%20this%20issue%20is%20to%20enhance%20images%0Apost-rendering.%202D%20enhancers%20can%20be%20pre-trained%20to%20recover%20some%20detail%20but%20are%0Aagnostic%20to%20scene%20geometry%20and%20do%20not%20easily%20generalize%20to%20new%20distributions%20of%0Aimage%20degradation.%20Conversely%2C%20existing%203D%20enhancers%20are%20able%20to%20transfer%0Adetail%20from%20nearby%20training%20images%20in%20a%20generalizable%20manner%2C%20but%20suffer%20from%0Ainaccurate%20camera%20calibration%20and%20can%20propagate%20errors%20from%20the%20geometry%20into%0Arendered%20images.%20We%20propose%20a%20neural%20rendering%20enhancer%2C%20RoGUENeRF%2C%20which%0Aexploits%20the%20best%20of%20both%20paradigms.%20Our%20method%20is%20pre-trained%20to%20learn%20a%0Ageneral%20enhancer%20while%20also%20leveraging%20information%20from%20nearby%20training%20images%0Avia%20robust%203D%20alignment%20and%20geometry-aware%20fusion.%20Our%20approach%20restores%0Ahigh-frequency%20textures%20while%20maintaining%20geometric%20consistency%20and%20is%20also%0Arobust%20to%20inaccurate%20camera%20calibration.%20We%20show%20that%20RoGUENeRF%20substantially%0Aenhances%20the%20rendering%20quality%20of%20a%20wide%20range%20of%20neural%20rendering%20baselines%2C%0Ae.g.%20improving%20the%20PSNR%20of%20MipNeRF360%20by%200.63dB%20and%20Nerfacto%20by%201.34dB%20on%20the%0Areal%20world%20360v2%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11909v2&entry.124074799=Read"},
{"title": "Imperfect Vision Encoders: Efficient and Robust Tuning for\n  Vision-Language Models", "author": "Aristeidis Panos and Rahaf Aljundi and Daniel Olmeda Reino and Richard E Turner", "abstract": "  Vision language models (VLMs) demonstrate impressive capabilities in visual\nquestion answering and image captioning, acting as a crucial link between\nvisual and language models. However, existing open-source VLMs heavily rely on\npretrained and frozen vision encoders (such as CLIP). Despite CLIP's robustness\nacross diverse domains, it still exhibits non-negligible image understanding\nerrors. These errors propagate to the VLM responses, resulting in sub-optimal\nperformance. In our work, we propose an efficient and robust method for\nupdating vision encoders within VLMs. Our approach selectively and locally\nupdates encoders, leading to substantial performance improvements on data where\nprevious mistakes occurred, while maintaining overall robustness. Furthermore,\nwe demonstrate the effectiveness of our method during continual few-shot\nupdates. Theoretical grounding, generality, and computational efficiency\ncharacterize our approach.\n", "link": "http://arxiv.org/abs/2407.16526v1", "date": "2024-07-23", "relevancy": 2.8159, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5695}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.565}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imperfect%20Vision%20Encoders%3A%20Efficient%20and%20Robust%20Tuning%20for%0A%20%20Vision-Language%20Models&body=Title%3A%20Imperfect%20Vision%20Encoders%3A%20Efficient%20and%20Robust%20Tuning%20for%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Aristeidis%20Panos%20and%20Rahaf%20Aljundi%20and%20Daniel%20Olmeda%20Reino%20and%20Richard%20E%20Turner%0AAbstract%3A%20%20%20Vision%20language%20models%20%28VLMs%29%20demonstrate%20impressive%20capabilities%20in%20visual%0Aquestion%20answering%20and%20image%20captioning%2C%20acting%20as%20a%20crucial%20link%20between%0Avisual%20and%20language%20models.%20However%2C%20existing%20open-source%20VLMs%20heavily%20rely%20on%0Apretrained%20and%20frozen%20vision%20encoders%20%28such%20as%20CLIP%29.%20Despite%20CLIP%27s%20robustness%0Aacross%20diverse%20domains%2C%20it%20still%20exhibits%20non-negligible%20image%20understanding%0Aerrors.%20These%20errors%20propagate%20to%20the%20VLM%20responses%2C%20resulting%20in%20sub-optimal%0Aperformance.%20In%20our%20work%2C%20we%20propose%20an%20efficient%20and%20robust%20method%20for%0Aupdating%20vision%20encoders%20within%20VLMs.%20Our%20approach%20selectively%20and%20locally%0Aupdates%20encoders%2C%20leading%20to%20substantial%20performance%20improvements%20on%20data%20where%0Aprevious%20mistakes%20occurred%2C%20while%20maintaining%20overall%20robustness.%20Furthermore%2C%0Awe%20demonstrate%20the%20effectiveness%20of%20our%20method%20during%20continual%20few-shot%0Aupdates.%20Theoretical%20grounding%2C%20generality%2C%20and%20computational%20efficiency%0Acharacterize%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImperfect%2520Vision%2520Encoders%253A%2520Efficient%2520and%2520Robust%2520Tuning%2520for%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DAristeidis%2520Panos%2520and%2520Rahaf%2520Aljundi%2520and%2520Daniel%2520Olmeda%2520Reino%2520and%2520Richard%2520E%2520Turner%26entry.1292438233%3D%2520%2520Vision%2520language%2520models%2520%2528VLMs%2529%2520demonstrate%2520impressive%2520capabilities%2520in%2520visual%250Aquestion%2520answering%2520and%2520image%2520captioning%252C%2520acting%2520as%2520a%2520crucial%2520link%2520between%250Avisual%2520and%2520language%2520models.%2520However%252C%2520existing%2520open-source%2520VLMs%2520heavily%2520rely%2520on%250Apretrained%2520and%2520frozen%2520vision%2520encoders%2520%2528such%2520as%2520CLIP%2529.%2520Despite%2520CLIP%2527s%2520robustness%250Aacross%2520diverse%2520domains%252C%2520it%2520still%2520exhibits%2520non-negligible%2520image%2520understanding%250Aerrors.%2520These%2520errors%2520propagate%2520to%2520the%2520VLM%2520responses%252C%2520resulting%2520in%2520sub-optimal%250Aperformance.%2520In%2520our%2520work%252C%2520we%2520propose%2520an%2520efficient%2520and%2520robust%2520method%2520for%250Aupdating%2520vision%2520encoders%2520within%2520VLMs.%2520Our%2520approach%2520selectively%2520and%2520locally%250Aupdates%2520encoders%252C%2520leading%2520to%2520substantial%2520performance%2520improvements%2520on%2520data%2520where%250Aprevious%2520mistakes%2520occurred%252C%2520while%2520maintaining%2520overall%2520robustness.%2520Furthermore%252C%250Awe%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520during%2520continual%2520few-shot%250Aupdates.%2520Theoretical%2520grounding%252C%2520generality%252C%2520and%2520computational%2520efficiency%250Acharacterize%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imperfect%20Vision%20Encoders%3A%20Efficient%20and%20Robust%20Tuning%20for%0A%20%20Vision-Language%20Models&entry.906535625=Aristeidis%20Panos%20and%20Rahaf%20Aljundi%20and%20Daniel%20Olmeda%20Reino%20and%20Richard%20E%20Turner&entry.1292438233=%20%20Vision%20language%20models%20%28VLMs%29%20demonstrate%20impressive%20capabilities%20in%20visual%0Aquestion%20answering%20and%20image%20captioning%2C%20acting%20as%20a%20crucial%20link%20between%0Avisual%20and%20language%20models.%20However%2C%20existing%20open-source%20VLMs%20heavily%20rely%20on%0Apretrained%20and%20frozen%20vision%20encoders%20%28such%20as%20CLIP%29.%20Despite%20CLIP%27s%20robustness%0Aacross%20diverse%20domains%2C%20it%20still%20exhibits%20non-negligible%20image%20understanding%0Aerrors.%20These%20errors%20propagate%20to%20the%20VLM%20responses%2C%20resulting%20in%20sub-optimal%0Aperformance.%20In%20our%20work%2C%20we%20propose%20an%20efficient%20and%20robust%20method%20for%0Aupdating%20vision%20encoders%20within%20VLMs.%20Our%20approach%20selectively%20and%20locally%0Aupdates%20encoders%2C%20leading%20to%20substantial%20performance%20improvements%20on%20data%20where%0Aprevious%20mistakes%20occurred%2C%20while%20maintaining%20overall%20robustness.%20Furthermore%2C%0Awe%20demonstrate%20the%20effectiveness%20of%20our%20method%20during%20continual%20few-shot%0Aupdates.%20Theoretical%20grounding%2C%20generality%2C%20and%20computational%20efficiency%0Acharacterize%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16526v1&entry.124074799=Read"},
{"title": "ToDER: Towards Colonoscopy Depth Estimation and Reconstruction with\n  Geometry Constraint Adaptation", "author": "Zhenhua Wu and Yanlin Jin and Liangdong Qiu and Xiaoguang Han and Xiang Wan and Guanbin Li", "abstract": "  Visualizing colonoscopy is crucial for medical auxiliary diagnosis to prevent\nundetected polyps in areas that are not fully observed. Traditional\nfeature-based and depth-based reconstruction approaches usually end up with\nundesirable results due to incorrect point matching or imprecise depth\nestimation in realistic colonoscopy videos. Modern deep-based methods often\nrequire a sufficient number of ground truth samples, which are generally hard\nto obtain in optical colonoscopy. To address this issue, self-supervised and\ndomain adaptation methods have been explored. However, these methods neglect\ngeometry constraints and exhibit lower accuracy in predicting detailed depth.\nWe thus propose a novel reconstruction pipeline with a bi-directional\nadaptation architecture named ToDER to get precise depth estimations.\nFurthermore, we carefully design a TNet module in our adaptation architecture\nto yield geometry constraints and obtain better depth quality. Estimated depth\nis finally utilized to reconstruct a reliable colon model for visualization.\nExperimental results demonstrate that our approach can precisely predict depth\nmaps in both realistic and synthetic colonoscopy videos compared with other\nself-supervised and domain adaptation methods. Our method on realistic\ncolonoscopy also shows the great potential for visualizing unobserved regions\nand preventing misdiagnoses.\n", "link": "http://arxiv.org/abs/2407.16508v1", "date": "2024-07-23", "relevancy": 2.7549, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5673}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5515}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToDER%3A%20Towards%20Colonoscopy%20Depth%20Estimation%20and%20Reconstruction%20with%0A%20%20Geometry%20Constraint%20Adaptation&body=Title%3A%20ToDER%3A%20Towards%20Colonoscopy%20Depth%20Estimation%20and%20Reconstruction%20with%0A%20%20Geometry%20Constraint%20Adaptation%0AAuthor%3A%20Zhenhua%20Wu%20and%20Yanlin%20Jin%20and%20Liangdong%20Qiu%20and%20Xiaoguang%20Han%20and%20Xiang%20Wan%20and%20Guanbin%20Li%0AAbstract%3A%20%20%20Visualizing%20colonoscopy%20is%20crucial%20for%20medical%20auxiliary%20diagnosis%20to%20prevent%0Aundetected%20polyps%20in%20areas%20that%20are%20not%20fully%20observed.%20Traditional%0Afeature-based%20and%20depth-based%20reconstruction%20approaches%20usually%20end%20up%20with%0Aundesirable%20results%20due%20to%20incorrect%20point%20matching%20or%20imprecise%20depth%0Aestimation%20in%20realistic%20colonoscopy%20videos.%20Modern%20deep-based%20methods%20often%0Arequire%20a%20sufficient%20number%20of%20ground%20truth%20samples%2C%20which%20are%20generally%20hard%0Ato%20obtain%20in%20optical%20colonoscopy.%20To%20address%20this%20issue%2C%20self-supervised%20and%0Adomain%20adaptation%20methods%20have%20been%20explored.%20However%2C%20these%20methods%20neglect%0Ageometry%20constraints%20and%20exhibit%20lower%20accuracy%20in%20predicting%20detailed%20depth.%0AWe%20thus%20propose%20a%20novel%20reconstruction%20pipeline%20with%20a%20bi-directional%0Aadaptation%20architecture%20named%20ToDER%20to%20get%20precise%20depth%20estimations.%0AFurthermore%2C%20we%20carefully%20design%20a%20TNet%20module%20in%20our%20adaptation%20architecture%0Ato%20yield%20geometry%20constraints%20and%20obtain%20better%20depth%20quality.%20Estimated%20depth%0Ais%20finally%20utilized%20to%20reconstruct%20a%20reliable%20colon%20model%20for%20visualization.%0AExperimental%20results%20demonstrate%20that%20our%20approach%20can%20precisely%20predict%20depth%0Amaps%20in%20both%20realistic%20and%20synthetic%20colonoscopy%20videos%20compared%20with%20other%0Aself-supervised%20and%20domain%20adaptation%20methods.%20Our%20method%20on%20realistic%0Acolonoscopy%20also%20shows%20the%20great%20potential%20for%20visualizing%20unobserved%20regions%0Aand%20preventing%20misdiagnoses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToDER%253A%2520Towards%2520Colonoscopy%2520Depth%2520Estimation%2520and%2520Reconstruction%2520with%250A%2520%2520Geometry%2520Constraint%2520Adaptation%26entry.906535625%3DZhenhua%2520Wu%2520and%2520Yanlin%2520Jin%2520and%2520Liangdong%2520Qiu%2520and%2520Xiaoguang%2520Han%2520and%2520Xiang%2520Wan%2520and%2520Guanbin%2520Li%26entry.1292438233%3D%2520%2520Visualizing%2520colonoscopy%2520is%2520crucial%2520for%2520medical%2520auxiliary%2520diagnosis%2520to%2520prevent%250Aundetected%2520polyps%2520in%2520areas%2520that%2520are%2520not%2520fully%2520observed.%2520Traditional%250Afeature-based%2520and%2520depth-based%2520reconstruction%2520approaches%2520usually%2520end%2520up%2520with%250Aundesirable%2520results%2520due%2520to%2520incorrect%2520point%2520matching%2520or%2520imprecise%2520depth%250Aestimation%2520in%2520realistic%2520colonoscopy%2520videos.%2520Modern%2520deep-based%2520methods%2520often%250Arequire%2520a%2520sufficient%2520number%2520of%2520ground%2520truth%2520samples%252C%2520which%2520are%2520generally%2520hard%250Ato%2520obtain%2520in%2520optical%2520colonoscopy.%2520To%2520address%2520this%2520issue%252C%2520self-supervised%2520and%250Adomain%2520adaptation%2520methods%2520have%2520been%2520explored.%2520However%252C%2520these%2520methods%2520neglect%250Ageometry%2520constraints%2520and%2520exhibit%2520lower%2520accuracy%2520in%2520predicting%2520detailed%2520depth.%250AWe%2520thus%2520propose%2520a%2520novel%2520reconstruction%2520pipeline%2520with%2520a%2520bi-directional%250Aadaptation%2520architecture%2520named%2520ToDER%2520to%2520get%2520precise%2520depth%2520estimations.%250AFurthermore%252C%2520we%2520carefully%2520design%2520a%2520TNet%2520module%2520in%2520our%2520adaptation%2520architecture%250Ato%2520yield%2520geometry%2520constraints%2520and%2520obtain%2520better%2520depth%2520quality.%2520Estimated%2520depth%250Ais%2520finally%2520utilized%2520to%2520reconstruct%2520a%2520reliable%2520colon%2520model%2520for%2520visualization.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520can%2520precisely%2520predict%2520depth%250Amaps%2520in%2520both%2520realistic%2520and%2520synthetic%2520colonoscopy%2520videos%2520compared%2520with%2520other%250Aself-supervised%2520and%2520domain%2520adaptation%2520methods.%2520Our%2520method%2520on%2520realistic%250Acolonoscopy%2520also%2520shows%2520the%2520great%2520potential%2520for%2520visualizing%2520unobserved%2520regions%250Aand%2520preventing%2520misdiagnoses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToDER%3A%20Towards%20Colonoscopy%20Depth%20Estimation%20and%20Reconstruction%20with%0A%20%20Geometry%20Constraint%20Adaptation&entry.906535625=Zhenhua%20Wu%20and%20Yanlin%20Jin%20and%20Liangdong%20Qiu%20and%20Xiaoguang%20Han%20and%20Xiang%20Wan%20and%20Guanbin%20Li&entry.1292438233=%20%20Visualizing%20colonoscopy%20is%20crucial%20for%20medical%20auxiliary%20diagnosis%20to%20prevent%0Aundetected%20polyps%20in%20areas%20that%20are%20not%20fully%20observed.%20Traditional%0Afeature-based%20and%20depth-based%20reconstruction%20approaches%20usually%20end%20up%20with%0Aundesirable%20results%20due%20to%20incorrect%20point%20matching%20or%20imprecise%20depth%0Aestimation%20in%20realistic%20colonoscopy%20videos.%20Modern%20deep-based%20methods%20often%0Arequire%20a%20sufficient%20number%20of%20ground%20truth%20samples%2C%20which%20are%20generally%20hard%0Ato%20obtain%20in%20optical%20colonoscopy.%20To%20address%20this%20issue%2C%20self-supervised%20and%0Adomain%20adaptation%20methods%20have%20been%20explored.%20However%2C%20these%20methods%20neglect%0Ageometry%20constraints%20and%20exhibit%20lower%20accuracy%20in%20predicting%20detailed%20depth.%0AWe%20thus%20propose%20a%20novel%20reconstruction%20pipeline%20with%20a%20bi-directional%0Aadaptation%20architecture%20named%20ToDER%20to%20get%20precise%20depth%20estimations.%0AFurthermore%2C%20we%20carefully%20design%20a%20TNet%20module%20in%20our%20adaptation%20architecture%0Ato%20yield%20geometry%20constraints%20and%20obtain%20better%20depth%20quality.%20Estimated%20depth%0Ais%20finally%20utilized%20to%20reconstruct%20a%20reliable%20colon%20model%20for%20visualization.%0AExperimental%20results%20demonstrate%20that%20our%20approach%20can%20precisely%20predict%20depth%0Amaps%20in%20both%20realistic%20and%20synthetic%20colonoscopy%20videos%20compared%20with%20other%0Aself-supervised%20and%20domain%20adaptation%20methods.%20Our%20method%20on%20realistic%0Acolonoscopy%20also%20shows%20the%20great%20potential%20for%20visualizing%20unobserved%20regions%0Aand%20preventing%20misdiagnoses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16508v1&entry.124074799=Read"},
{"title": "Strike a Balance in Continual Panoptic Segmentation", "author": "Jinpeng Chen and Runmin Cong and Yuxuan Luo and Horace Ho Shing Ip and Sam Kwong", "abstract": "  This study explores the emerging area of continual panoptic segmentation,\nhighlighting three key balances. First, we introduce past-class backtrace\ndistillation to balance the stability of existing knowledge with the\nadaptability to new information. This technique retraces the features\nassociated with past classes based on the final label assignment results,\nperforming knowledge distillation targeting these specific features from the\nprevious model while allowing other features to flexibly adapt to new\ninformation. Additionally, we introduce a class-proportional memory strategy,\nwhich aligns the class distribution in the replay sample set with that of the\nhistorical training data. This strategy maintains a balanced class\nrepresentation during replay, enhancing the utility of the limited-capacity\nreplay sample set in recalling prior classes. Moreover, recognizing that replay\nsamples are annotated only for the classes of their original step, we devise\nbalanced anti-misguidance losses, which combat the impact of incomplete\nannotations without incurring classification bias. Building upon these\ninnovations, we present a new method named Balanced Continual Panoptic\nSegmentation (BalConpas). Our evaluation on the challenging ADE20K dataset\ndemonstrates its superior performance compared to existing state-of-the-art\nmethods. The official code is available at\nhttps://github.com/jinpeng0528/BalConpas.\n", "link": "http://arxiv.org/abs/2407.16354v1", "date": "2024-07-23", "relevancy": 2.7416, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.575}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5642}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strike%20a%20Balance%20in%20Continual%20Panoptic%20Segmentation&body=Title%3A%20Strike%20a%20Balance%20in%20Continual%20Panoptic%20Segmentation%0AAuthor%3A%20Jinpeng%20Chen%20and%20Runmin%20Cong%20and%20Yuxuan%20Luo%20and%20Horace%20Ho%20Shing%20Ip%20and%20Sam%20Kwong%0AAbstract%3A%20%20%20This%20study%20explores%20the%20emerging%20area%20of%20continual%20panoptic%20segmentation%2C%0Ahighlighting%20three%20key%20balances.%20First%2C%20we%20introduce%20past-class%20backtrace%0Adistillation%20to%20balance%20the%20stability%20of%20existing%20knowledge%20with%20the%0Aadaptability%20to%20new%20information.%20This%20technique%20retraces%20the%20features%0Aassociated%20with%20past%20classes%20based%20on%20the%20final%20label%20assignment%20results%2C%0Aperforming%20knowledge%20distillation%20targeting%20these%20specific%20features%20from%20the%0Aprevious%20model%20while%20allowing%20other%20features%20to%20flexibly%20adapt%20to%20new%0Ainformation.%20Additionally%2C%20we%20introduce%20a%20class-proportional%20memory%20strategy%2C%0Awhich%20aligns%20the%20class%20distribution%20in%20the%20replay%20sample%20set%20with%20that%20of%20the%0Ahistorical%20training%20data.%20This%20strategy%20maintains%20a%20balanced%20class%0Arepresentation%20during%20replay%2C%20enhancing%20the%20utility%20of%20the%20limited-capacity%0Areplay%20sample%20set%20in%20recalling%20prior%20classes.%20Moreover%2C%20recognizing%20that%20replay%0Asamples%20are%20annotated%20only%20for%20the%20classes%20of%20their%20original%20step%2C%20we%20devise%0Abalanced%20anti-misguidance%20losses%2C%20which%20combat%20the%20impact%20of%20incomplete%0Aannotations%20without%20incurring%20classification%20bias.%20Building%20upon%20these%0Ainnovations%2C%20we%20present%20a%20new%20method%20named%20Balanced%20Continual%20Panoptic%0ASegmentation%20%28BalConpas%29.%20Our%20evaluation%20on%20the%20challenging%20ADE20K%20dataset%0Ademonstrates%20its%20superior%20performance%20compared%20to%20existing%20state-of-the-art%0Amethods.%20The%20official%20code%20is%20available%20at%0Ahttps%3A//github.com/jinpeng0528/BalConpas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrike%2520a%2520Balance%2520in%2520Continual%2520Panoptic%2520Segmentation%26entry.906535625%3DJinpeng%2520Chen%2520and%2520Runmin%2520Cong%2520and%2520Yuxuan%2520Luo%2520and%2520Horace%2520Ho%2520Shing%2520Ip%2520and%2520Sam%2520Kwong%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520emerging%2520area%2520of%2520continual%2520panoptic%2520segmentation%252C%250Ahighlighting%2520three%2520key%2520balances.%2520First%252C%2520we%2520introduce%2520past-class%2520backtrace%250Adistillation%2520to%2520balance%2520the%2520stability%2520of%2520existing%2520knowledge%2520with%2520the%250Aadaptability%2520to%2520new%2520information.%2520This%2520technique%2520retraces%2520the%2520features%250Aassociated%2520with%2520past%2520classes%2520based%2520on%2520the%2520final%2520label%2520assignment%2520results%252C%250Aperforming%2520knowledge%2520distillation%2520targeting%2520these%2520specific%2520features%2520from%2520the%250Aprevious%2520model%2520while%2520allowing%2520other%2520features%2520to%2520flexibly%2520adapt%2520to%2520new%250Ainformation.%2520Additionally%252C%2520we%2520introduce%2520a%2520class-proportional%2520memory%2520strategy%252C%250Awhich%2520aligns%2520the%2520class%2520distribution%2520in%2520the%2520replay%2520sample%2520set%2520with%2520that%2520of%2520the%250Ahistorical%2520training%2520data.%2520This%2520strategy%2520maintains%2520a%2520balanced%2520class%250Arepresentation%2520during%2520replay%252C%2520enhancing%2520the%2520utility%2520of%2520the%2520limited-capacity%250Areplay%2520sample%2520set%2520in%2520recalling%2520prior%2520classes.%2520Moreover%252C%2520recognizing%2520that%2520replay%250Asamples%2520are%2520annotated%2520only%2520for%2520the%2520classes%2520of%2520their%2520original%2520step%252C%2520we%2520devise%250Abalanced%2520anti-misguidance%2520losses%252C%2520which%2520combat%2520the%2520impact%2520of%2520incomplete%250Aannotations%2520without%2520incurring%2520classification%2520bias.%2520Building%2520upon%2520these%250Ainnovations%252C%2520we%2520present%2520a%2520new%2520method%2520named%2520Balanced%2520Continual%2520Panoptic%250ASegmentation%2520%2528BalConpas%2529.%2520Our%2520evaluation%2520on%2520the%2520challenging%2520ADE20K%2520dataset%250Ademonstrates%2520its%2520superior%2520performance%2520compared%2520to%2520existing%2520state-of-the-art%250Amethods.%2520The%2520official%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/jinpeng0528/BalConpas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strike%20a%20Balance%20in%20Continual%20Panoptic%20Segmentation&entry.906535625=Jinpeng%20Chen%20and%20Runmin%20Cong%20and%20Yuxuan%20Luo%20and%20Horace%20Ho%20Shing%20Ip%20and%20Sam%20Kwong&entry.1292438233=%20%20This%20study%20explores%20the%20emerging%20area%20of%20continual%20panoptic%20segmentation%2C%0Ahighlighting%20three%20key%20balances.%20First%2C%20we%20introduce%20past-class%20backtrace%0Adistillation%20to%20balance%20the%20stability%20of%20existing%20knowledge%20with%20the%0Aadaptability%20to%20new%20information.%20This%20technique%20retraces%20the%20features%0Aassociated%20with%20past%20classes%20based%20on%20the%20final%20label%20assignment%20results%2C%0Aperforming%20knowledge%20distillation%20targeting%20these%20specific%20features%20from%20the%0Aprevious%20model%20while%20allowing%20other%20features%20to%20flexibly%20adapt%20to%20new%0Ainformation.%20Additionally%2C%20we%20introduce%20a%20class-proportional%20memory%20strategy%2C%0Awhich%20aligns%20the%20class%20distribution%20in%20the%20replay%20sample%20set%20with%20that%20of%20the%0Ahistorical%20training%20data.%20This%20strategy%20maintains%20a%20balanced%20class%0Arepresentation%20during%20replay%2C%20enhancing%20the%20utility%20of%20the%20limited-capacity%0Areplay%20sample%20set%20in%20recalling%20prior%20classes.%20Moreover%2C%20recognizing%20that%20replay%0Asamples%20are%20annotated%20only%20for%20the%20classes%20of%20their%20original%20step%2C%20we%20devise%0Abalanced%20anti-misguidance%20losses%2C%20which%20combat%20the%20impact%20of%20incomplete%0Aannotations%20without%20incurring%20classification%20bias.%20Building%20upon%20these%0Ainnovations%2C%20we%20present%20a%20new%20method%20named%20Balanced%20Continual%20Panoptic%0ASegmentation%20%28BalConpas%29.%20Our%20evaluation%20on%20the%20challenging%20ADE20K%20dataset%0Ademonstrates%20its%20superior%20performance%20compared%20to%20existing%20state-of-the-art%0Amethods.%20The%20official%20code%20is%20available%20at%0Ahttps%3A//github.com/jinpeng0528/BalConpas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16354v1&entry.124074799=Read"},
{"title": "Timeliness-Fidelity Tradeoff in 3D Scene Representations", "author": "Xiangmin Xu and Zhen Meng and Yichi Zhang and Changyang She and Philip G. Zhao", "abstract": "  Real-time three-dimensional (3D) scene representations serve as one of the\nbuilding blocks that bolster various innovative applications, e.g., digital\nmanufacturing, Virtual/Augmented/Extended/Mixed Reality (VR/AR/XR/MR), and the\nmetaverse. Despite substantial efforts that have been made to real-time\ncommunications and computing, real-time 3D scene representations remain a\nchallenging task. This paper investigates the tradeoff between timeliness and\nfidelity in real-time 3D scene representations. Specifically, we establish a\nframework to evaluate the impact of communication delay on the tradeoff, where\nthe real-world scenario is monitored by multiple cameras that communicate with\nan edge server. To improve fidelity for 3D scene representations, we propose to\nuse a single-step Proximal Policy Optimization (PPO) method that leverages the\nAge of Information (AoI) to decide if the received image needs to be involved\nin 3D scene representations and rendering. We test our framework and the\nproposed approach with different well-known 3D scene representation methods.\nSimulation results reveal that real-time 3D scene representation can be\nsensitively affected by communication delay, and our proposed method can\nachieve optimal 3D scene representation results.\n", "link": "http://arxiv.org/abs/2407.16575v1", "date": "2024-07-23", "relevancy": 2.7393, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5559}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5559}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Timeliness-Fidelity%20Tradeoff%20in%203D%20Scene%20Representations&body=Title%3A%20Timeliness-Fidelity%20Tradeoff%20in%203D%20Scene%20Representations%0AAuthor%3A%20Xiangmin%20Xu%20and%20Zhen%20Meng%20and%20Yichi%20Zhang%20and%20Changyang%20She%20and%20Philip%20G.%20Zhao%0AAbstract%3A%20%20%20Real-time%20three-dimensional%20%283D%29%20scene%20representations%20serve%20as%20one%20of%20the%0Abuilding%20blocks%20that%20bolster%20various%20innovative%20applications%2C%20e.g.%2C%20digital%0Amanufacturing%2C%20Virtual/Augmented/Extended/Mixed%20Reality%20%28VR/AR/XR/MR%29%2C%20and%20the%0Ametaverse.%20Despite%20substantial%20efforts%20that%20have%20been%20made%20to%20real-time%0Acommunications%20and%20computing%2C%20real-time%203D%20scene%20representations%20remain%20a%0Achallenging%20task.%20This%20paper%20investigates%20the%20tradeoff%20between%20timeliness%20and%0Afidelity%20in%20real-time%203D%20scene%20representations.%20Specifically%2C%20we%20establish%20a%0Aframework%20to%20evaluate%20the%20impact%20of%20communication%20delay%20on%20the%20tradeoff%2C%20where%0Athe%20real-world%20scenario%20is%20monitored%20by%20multiple%20cameras%20that%20communicate%20with%0Aan%20edge%20server.%20To%20improve%20fidelity%20for%203D%20scene%20representations%2C%20we%20propose%20to%0Ause%20a%20single-step%20Proximal%20Policy%20Optimization%20%28PPO%29%20method%20that%20leverages%20the%0AAge%20of%20Information%20%28AoI%29%20to%20decide%20if%20the%20received%20image%20needs%20to%20be%20involved%0Ain%203D%20scene%20representations%20and%20rendering.%20We%20test%20our%20framework%20and%20the%0Aproposed%20approach%20with%20different%20well-known%203D%20scene%20representation%20methods.%0ASimulation%20results%20reveal%20that%20real-time%203D%20scene%20representation%20can%20be%0Asensitively%20affected%20by%20communication%20delay%2C%20and%20our%20proposed%20method%20can%0Aachieve%20optimal%203D%20scene%20representation%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeliness-Fidelity%2520Tradeoff%2520in%25203D%2520Scene%2520Representations%26entry.906535625%3DXiangmin%2520Xu%2520and%2520Zhen%2520Meng%2520and%2520Yichi%2520Zhang%2520and%2520Changyang%2520She%2520and%2520Philip%2520G.%2520Zhao%26entry.1292438233%3D%2520%2520Real-time%2520three-dimensional%2520%25283D%2529%2520scene%2520representations%2520serve%2520as%2520one%2520of%2520the%250Abuilding%2520blocks%2520that%2520bolster%2520various%2520innovative%2520applications%252C%2520e.g.%252C%2520digital%250Amanufacturing%252C%2520Virtual/Augmented/Extended/Mixed%2520Reality%2520%2528VR/AR/XR/MR%2529%252C%2520and%2520the%250Ametaverse.%2520Despite%2520substantial%2520efforts%2520that%2520have%2520been%2520made%2520to%2520real-time%250Acommunications%2520and%2520computing%252C%2520real-time%25203D%2520scene%2520representations%2520remain%2520a%250Achallenging%2520task.%2520This%2520paper%2520investigates%2520the%2520tradeoff%2520between%2520timeliness%2520and%250Afidelity%2520in%2520real-time%25203D%2520scene%2520representations.%2520Specifically%252C%2520we%2520establish%2520a%250Aframework%2520to%2520evaluate%2520the%2520impact%2520of%2520communication%2520delay%2520on%2520the%2520tradeoff%252C%2520where%250Athe%2520real-world%2520scenario%2520is%2520monitored%2520by%2520multiple%2520cameras%2520that%2520communicate%2520with%250Aan%2520edge%2520server.%2520To%2520improve%2520fidelity%2520for%25203D%2520scene%2520representations%252C%2520we%2520propose%2520to%250Ause%2520a%2520single-step%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520method%2520that%2520leverages%2520the%250AAge%2520of%2520Information%2520%2528AoI%2529%2520to%2520decide%2520if%2520the%2520received%2520image%2520needs%2520to%2520be%2520involved%250Ain%25203D%2520scene%2520representations%2520and%2520rendering.%2520We%2520test%2520our%2520framework%2520and%2520the%250Aproposed%2520approach%2520with%2520different%2520well-known%25203D%2520scene%2520representation%2520methods.%250ASimulation%2520results%2520reveal%2520that%2520real-time%25203D%2520scene%2520representation%2520can%2520be%250Asensitively%2520affected%2520by%2520communication%2520delay%252C%2520and%2520our%2520proposed%2520method%2520can%250Aachieve%2520optimal%25203D%2520scene%2520representation%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Timeliness-Fidelity%20Tradeoff%20in%203D%20Scene%20Representations&entry.906535625=Xiangmin%20Xu%20and%20Zhen%20Meng%20and%20Yichi%20Zhang%20and%20Changyang%20She%20and%20Philip%20G.%20Zhao&entry.1292438233=%20%20Real-time%20three-dimensional%20%283D%29%20scene%20representations%20serve%20as%20one%20of%20the%0Abuilding%20blocks%20that%20bolster%20various%20innovative%20applications%2C%20e.g.%2C%20digital%0Amanufacturing%2C%20Virtual/Augmented/Extended/Mixed%20Reality%20%28VR/AR/XR/MR%29%2C%20and%20the%0Ametaverse.%20Despite%20substantial%20efforts%20that%20have%20been%20made%20to%20real-time%0Acommunications%20and%20computing%2C%20real-time%203D%20scene%20representations%20remain%20a%0Achallenging%20task.%20This%20paper%20investigates%20the%20tradeoff%20between%20timeliness%20and%0Afidelity%20in%20real-time%203D%20scene%20representations.%20Specifically%2C%20we%20establish%20a%0Aframework%20to%20evaluate%20the%20impact%20of%20communication%20delay%20on%20the%20tradeoff%2C%20where%0Athe%20real-world%20scenario%20is%20monitored%20by%20multiple%20cameras%20that%20communicate%20with%0Aan%20edge%20server.%20To%20improve%20fidelity%20for%203D%20scene%20representations%2C%20we%20propose%20to%0Ause%20a%20single-step%20Proximal%20Policy%20Optimization%20%28PPO%29%20method%20that%20leverages%20the%0AAge%20of%20Information%20%28AoI%29%20to%20decide%20if%20the%20received%20image%20needs%20to%20be%20involved%0Ain%203D%20scene%20representations%20and%20rendering.%20We%20test%20our%20framework%20and%20the%0Aproposed%20approach%20with%20different%20well-known%203D%20scene%20representation%20methods.%0ASimulation%20results%20reveal%20that%20real-time%203D%20scene%20representation%20can%20be%0Asensitively%20affected%20by%20communication%20delay%2C%20and%20our%20proposed%20method%20can%0Aachieve%20optimal%203D%20scene%20representation%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16575v1&entry.124074799=Read"},
{"title": "Unsupervised End-to-End Training with a Self-Defined Target", "author": "Dongshu Liu and J\u00e9r\u00e9mie Laydevant and Adrien Pontlevy and Damien Querlioz and Julie Grollier", "abstract": "  Designing algorithms for versatile AI hardware that can learn on the edge\nusing both labeled and unlabeled data is challenging. Deep end-to-end training\nmethods incorporating phases of self-supervised and supervised learning are\naccurate and adaptable to input data but self-supervised learning requires even\nmore computational and memory resources than supervised learning, too high for\ncurrent embedded hardware. Conversely, unsupervised layer-by-layer training,\nsuch as Hebbian learning, is more compatible with existing hardware but does\nnot integrate well with supervised learning. To address this, we propose a\nmethod enabling networks or hardware designed for end-to-end supervised\nlearning to also perform high-performance unsupervised learning by adding two\nsimple elements to the output layer: Winner-Take-All (WTA) selectivity and\nhomeostasis regularization. These mechanisms introduce a \"self-defined target\"\nfor unlabeled data, allowing purely unsupervised training for both\nfully-connected and convolutional layers using backpropagation or equilibrium\npropagation on datasets like MNIST (up to 99.2%), Fashion-MNIST (up to 90.3%),\nand SVHN (up to 81.5%). We extend this method to semi-supervised learning,\nadjusting targets based on data type, achieving 96.6% accuracy with only 600\nlabeled MNIST samples in a multi-layer perceptron. Our results show that this\napproach can effectively enable networks and hardware initially dedicated to\nsupervised learning to also perform unsupervised learning, adapting to varying\navailability of labeled data.\n", "link": "http://arxiv.org/abs/2403.12116v2", "date": "2024-07-23", "relevancy": 2.7363, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5798}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5408}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20End-to-End%20Training%20with%20a%20Self-Defined%20Target&body=Title%3A%20Unsupervised%20End-to-End%20Training%20with%20a%20Self-Defined%20Target%0AAuthor%3A%20Dongshu%20Liu%20and%20J%C3%A9r%C3%A9mie%20Laydevant%20and%20Adrien%20Pontlevy%20and%20Damien%20Querlioz%20and%20Julie%20Grollier%0AAbstract%3A%20%20%20Designing%20algorithms%20for%20versatile%20AI%20hardware%20that%20can%20learn%20on%20the%20edge%0Ausing%20both%20labeled%20and%20unlabeled%20data%20is%20challenging.%20Deep%20end-to-end%20training%0Amethods%20incorporating%20phases%20of%20self-supervised%20and%20supervised%20learning%20are%0Aaccurate%20and%20adaptable%20to%20input%20data%20but%20self-supervised%20learning%20requires%20even%0Amore%20computational%20and%20memory%20resources%20than%20supervised%20learning%2C%20too%20high%20for%0Acurrent%20embedded%20hardware.%20Conversely%2C%20unsupervised%20layer-by-layer%20training%2C%0Asuch%20as%20Hebbian%20learning%2C%20is%20more%20compatible%20with%20existing%20hardware%20but%20does%0Anot%20integrate%20well%20with%20supervised%20learning.%20To%20address%20this%2C%20we%20propose%20a%0Amethod%20enabling%20networks%20or%20hardware%20designed%20for%20end-to-end%20supervised%0Alearning%20to%20also%20perform%20high-performance%20unsupervised%20learning%20by%20adding%20two%0Asimple%20elements%20to%20the%20output%20layer%3A%20Winner-Take-All%20%28WTA%29%20selectivity%20and%0Ahomeostasis%20regularization.%20These%20mechanisms%20introduce%20a%20%22self-defined%20target%22%0Afor%20unlabeled%20data%2C%20allowing%20purely%20unsupervised%20training%20for%20both%0Afully-connected%20and%20convolutional%20layers%20using%20backpropagation%20or%20equilibrium%0Apropagation%20on%20datasets%20like%20MNIST%20%28up%20to%2099.2%25%29%2C%20Fashion-MNIST%20%28up%20to%2090.3%25%29%2C%0Aand%20SVHN%20%28up%20to%2081.5%25%29.%20We%20extend%20this%20method%20to%20semi-supervised%20learning%2C%0Aadjusting%20targets%20based%20on%20data%20type%2C%20achieving%2096.6%25%20accuracy%20with%20only%20600%0Alabeled%20MNIST%20samples%20in%20a%20multi-layer%20perceptron.%20Our%20results%20show%20that%20this%0Aapproach%20can%20effectively%20enable%20networks%20and%20hardware%20initially%20dedicated%20to%0Asupervised%20learning%20to%20also%20perform%20unsupervised%20learning%2C%20adapting%20to%20varying%0Aavailability%20of%20labeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12116v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520End-to-End%2520Training%2520with%2520a%2520Self-Defined%2520Target%26entry.906535625%3DDongshu%2520Liu%2520and%2520J%25C3%25A9r%25C3%25A9mie%2520Laydevant%2520and%2520Adrien%2520Pontlevy%2520and%2520Damien%2520Querlioz%2520and%2520Julie%2520Grollier%26entry.1292438233%3D%2520%2520Designing%2520algorithms%2520for%2520versatile%2520AI%2520hardware%2520that%2520can%2520learn%2520on%2520the%2520edge%250Ausing%2520both%2520labeled%2520and%2520unlabeled%2520data%2520is%2520challenging.%2520Deep%2520end-to-end%2520training%250Amethods%2520incorporating%2520phases%2520of%2520self-supervised%2520and%2520supervised%2520learning%2520are%250Aaccurate%2520and%2520adaptable%2520to%2520input%2520data%2520but%2520self-supervised%2520learning%2520requires%2520even%250Amore%2520computational%2520and%2520memory%2520resources%2520than%2520supervised%2520learning%252C%2520too%2520high%2520for%250Acurrent%2520embedded%2520hardware.%2520Conversely%252C%2520unsupervised%2520layer-by-layer%2520training%252C%250Asuch%2520as%2520Hebbian%2520learning%252C%2520is%2520more%2520compatible%2520with%2520existing%2520hardware%2520but%2520does%250Anot%2520integrate%2520well%2520with%2520supervised%2520learning.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Amethod%2520enabling%2520networks%2520or%2520hardware%2520designed%2520for%2520end-to-end%2520supervised%250Alearning%2520to%2520also%2520perform%2520high-performance%2520unsupervised%2520learning%2520by%2520adding%2520two%250Asimple%2520elements%2520to%2520the%2520output%2520layer%253A%2520Winner-Take-All%2520%2528WTA%2529%2520selectivity%2520and%250Ahomeostasis%2520regularization.%2520These%2520mechanisms%2520introduce%2520a%2520%2522self-defined%2520target%2522%250Afor%2520unlabeled%2520data%252C%2520allowing%2520purely%2520unsupervised%2520training%2520for%2520both%250Afully-connected%2520and%2520convolutional%2520layers%2520using%2520backpropagation%2520or%2520equilibrium%250Apropagation%2520on%2520datasets%2520like%2520MNIST%2520%2528up%2520to%252099.2%2525%2529%252C%2520Fashion-MNIST%2520%2528up%2520to%252090.3%2525%2529%252C%250Aand%2520SVHN%2520%2528up%2520to%252081.5%2525%2529.%2520We%2520extend%2520this%2520method%2520to%2520semi-supervised%2520learning%252C%250Aadjusting%2520targets%2520based%2520on%2520data%2520type%252C%2520achieving%252096.6%2525%2520accuracy%2520with%2520only%2520600%250Alabeled%2520MNIST%2520samples%2520in%2520a%2520multi-layer%2520perceptron.%2520Our%2520results%2520show%2520that%2520this%250Aapproach%2520can%2520effectively%2520enable%2520networks%2520and%2520hardware%2520initially%2520dedicated%2520to%250Asupervised%2520learning%2520to%2520also%2520perform%2520unsupervised%2520learning%252C%2520adapting%2520to%2520varying%250Aavailability%2520of%2520labeled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12116v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20End-to-End%20Training%20with%20a%20Self-Defined%20Target&entry.906535625=Dongshu%20Liu%20and%20J%C3%A9r%C3%A9mie%20Laydevant%20and%20Adrien%20Pontlevy%20and%20Damien%20Querlioz%20and%20Julie%20Grollier&entry.1292438233=%20%20Designing%20algorithms%20for%20versatile%20AI%20hardware%20that%20can%20learn%20on%20the%20edge%0Ausing%20both%20labeled%20and%20unlabeled%20data%20is%20challenging.%20Deep%20end-to-end%20training%0Amethods%20incorporating%20phases%20of%20self-supervised%20and%20supervised%20learning%20are%0Aaccurate%20and%20adaptable%20to%20input%20data%20but%20self-supervised%20learning%20requires%20even%0Amore%20computational%20and%20memory%20resources%20than%20supervised%20learning%2C%20too%20high%20for%0Acurrent%20embedded%20hardware.%20Conversely%2C%20unsupervised%20layer-by-layer%20training%2C%0Asuch%20as%20Hebbian%20learning%2C%20is%20more%20compatible%20with%20existing%20hardware%20but%20does%0Anot%20integrate%20well%20with%20supervised%20learning.%20To%20address%20this%2C%20we%20propose%20a%0Amethod%20enabling%20networks%20or%20hardware%20designed%20for%20end-to-end%20supervised%0Alearning%20to%20also%20perform%20high-performance%20unsupervised%20learning%20by%20adding%20two%0Asimple%20elements%20to%20the%20output%20layer%3A%20Winner-Take-All%20%28WTA%29%20selectivity%20and%0Ahomeostasis%20regularization.%20These%20mechanisms%20introduce%20a%20%22self-defined%20target%22%0Afor%20unlabeled%20data%2C%20allowing%20purely%20unsupervised%20training%20for%20both%0Afully-connected%20and%20convolutional%20layers%20using%20backpropagation%20or%20equilibrium%0Apropagation%20on%20datasets%20like%20MNIST%20%28up%20to%2099.2%25%29%2C%20Fashion-MNIST%20%28up%20to%2090.3%25%29%2C%0Aand%20SVHN%20%28up%20to%2081.5%25%29.%20We%20extend%20this%20method%20to%20semi-supervised%20learning%2C%0Aadjusting%20targets%20based%20on%20data%20type%2C%20achieving%2096.6%25%20accuracy%20with%20only%20600%0Alabeled%20MNIST%20samples%20in%20a%20multi-layer%20perceptron.%20Our%20results%20show%20that%20this%0Aapproach%20can%20effectively%20enable%20networks%20and%20hardware%20initially%20dedicated%20to%0Asupervised%20learning%20to%20also%20perform%20unsupervised%20learning%2C%20adapting%20to%20varying%0Aavailability%20of%20labeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12116v2&entry.124074799=Read"},
{"title": "Is 3D Convolution with 5D Tensors Really Necessary for Video Analysis?", "author": "Habib Hajimolahoseini and Walid Ahmed and Austin Wen and Yang Liu", "abstract": "  In this paper, we present a comprehensive study and propose several novel\ntechniques for implementing 3D convolutional blocks using 2D and/or 1D\nconvolutions with only 4D and/or 3D tensors. Our motivation is that 3D\nconvolutions with 5D tensors are computationally very expensive and they may\nnot be supported by some of the edge devices used in real-time applications\nsuch as robots. The existing approaches mitigate this by splitting the 3D\nkernels into spatial and temporal domains, but they still use 3D convolutions\nwith 5D tensors in their implementations. We resolve this issue by introducing\nsome appropriate 4D/3D tensor reshaping as well as new combination techniques\nfor spatial and temporal splits. The proposed implementation methods show\nsignificant improvement both in terms of efficiency and accuracy. The\nexperimental results confirm that the proposed spatio-temporal processing\nstructure outperforms the original model in terms of speed and accuracy using\nonly 4D tensors with fewer parameters.\n", "link": "http://arxiv.org/abs/2407.16514v1", "date": "2024-07-23", "relevancy": 2.6406, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5358}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5358}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%203D%20Convolution%20with%205D%20Tensors%20Really%20Necessary%20for%20Video%20Analysis%3F&body=Title%3A%20Is%203D%20Convolution%20with%205D%20Tensors%20Really%20Necessary%20for%20Video%20Analysis%3F%0AAuthor%3A%20Habib%20Hajimolahoseini%20and%20Walid%20Ahmed%20and%20Austin%20Wen%20and%20Yang%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20study%20and%20propose%20several%20novel%0Atechniques%20for%20implementing%203D%20convolutional%20blocks%20using%202D%20and/or%201D%0Aconvolutions%20with%20only%204D%20and/or%203D%20tensors.%20Our%20motivation%20is%20that%203D%0Aconvolutions%20with%205D%20tensors%20are%20computationally%20very%20expensive%20and%20they%20may%0Anot%20be%20supported%20by%20some%20of%20the%20edge%20devices%20used%20in%20real-time%20applications%0Asuch%20as%20robots.%20The%20existing%20approaches%20mitigate%20this%20by%20splitting%20the%203D%0Akernels%20into%20spatial%20and%20temporal%20domains%2C%20but%20they%20still%20use%203D%20convolutions%0Awith%205D%20tensors%20in%20their%20implementations.%20We%20resolve%20this%20issue%20by%20introducing%0Asome%20appropriate%204D/3D%20tensor%20reshaping%20as%20well%20as%20new%20combination%20techniques%0Afor%20spatial%20and%20temporal%20splits.%20The%20proposed%20implementation%20methods%20show%0Asignificant%20improvement%20both%20in%20terms%20of%20efficiency%20and%20accuracy.%20The%0Aexperimental%20results%20confirm%20that%20the%20proposed%20spatio-temporal%20processing%0Astructure%20outperforms%20the%20original%20model%20in%20terms%20of%20speed%20and%20accuracy%20using%0Aonly%204D%20tensors%20with%20fewer%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%25203D%2520Convolution%2520with%25205D%2520Tensors%2520Really%2520Necessary%2520for%2520Video%2520Analysis%253F%26entry.906535625%3DHabib%2520Hajimolahoseini%2520and%2520Walid%2520Ahmed%2520and%2520Austin%2520Wen%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520study%2520and%2520propose%2520several%2520novel%250Atechniques%2520for%2520implementing%25203D%2520convolutional%2520blocks%2520using%25202D%2520and/or%25201D%250Aconvolutions%2520with%2520only%25204D%2520and/or%25203D%2520tensors.%2520Our%2520motivation%2520is%2520that%25203D%250Aconvolutions%2520with%25205D%2520tensors%2520are%2520computationally%2520very%2520expensive%2520and%2520they%2520may%250Anot%2520be%2520supported%2520by%2520some%2520of%2520the%2520edge%2520devices%2520used%2520in%2520real-time%2520applications%250Asuch%2520as%2520robots.%2520The%2520existing%2520approaches%2520mitigate%2520this%2520by%2520splitting%2520the%25203D%250Akernels%2520into%2520spatial%2520and%2520temporal%2520domains%252C%2520but%2520they%2520still%2520use%25203D%2520convolutions%250Awith%25205D%2520tensors%2520in%2520their%2520implementations.%2520We%2520resolve%2520this%2520issue%2520by%2520introducing%250Asome%2520appropriate%25204D/3D%2520tensor%2520reshaping%2520as%2520well%2520as%2520new%2520combination%2520techniques%250Afor%2520spatial%2520and%2520temporal%2520splits.%2520The%2520proposed%2520implementation%2520methods%2520show%250Asignificant%2520improvement%2520both%2520in%2520terms%2520of%2520efficiency%2520and%2520accuracy.%2520The%250Aexperimental%2520results%2520confirm%2520that%2520the%2520proposed%2520spatio-temporal%2520processing%250Astructure%2520outperforms%2520the%2520original%2520model%2520in%2520terms%2520of%2520speed%2520and%2520accuracy%2520using%250Aonly%25204D%2520tensors%2520with%2520fewer%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%203D%20Convolution%20with%205D%20Tensors%20Really%20Necessary%20for%20Video%20Analysis%3F&entry.906535625=Habib%20Hajimolahoseini%20and%20Walid%20Ahmed%20and%20Austin%20Wen%20and%20Yang%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20study%20and%20propose%20several%20novel%0Atechniques%20for%20implementing%203D%20convolutional%20blocks%20using%202D%20and/or%201D%0Aconvolutions%20with%20only%204D%20and/or%203D%20tensors.%20Our%20motivation%20is%20that%203D%0Aconvolutions%20with%205D%20tensors%20are%20computationally%20very%20expensive%20and%20they%20may%0Anot%20be%20supported%20by%20some%20of%20the%20edge%20devices%20used%20in%20real-time%20applications%0Asuch%20as%20robots.%20The%20existing%20approaches%20mitigate%20this%20by%20splitting%20the%203D%0Akernels%20into%20spatial%20and%20temporal%20domains%2C%20but%20they%20still%20use%203D%20convolutions%0Awith%205D%20tensors%20in%20their%20implementations.%20We%20resolve%20this%20issue%20by%20introducing%0Asome%20appropriate%204D/3D%20tensor%20reshaping%20as%20well%20as%20new%20combination%20techniques%0Afor%20spatial%20and%20temporal%20splits.%20The%20proposed%20implementation%20methods%20show%0Asignificant%20improvement%20both%20in%20terms%20of%20efficiency%20and%20accuracy.%20The%0Aexperimental%20results%20confirm%20that%20the%20proposed%20spatio-temporal%20processing%0Astructure%20outperforms%20the%20original%20model%20in%20terms%20of%20speed%20and%20accuracy%20using%0Aonly%204D%20tensors%20with%20fewer%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16514v1&entry.124074799=Read"},
{"title": "GenRec: A Flexible Data Generator for Recommendations", "author": "Erica Coppolillo and Simone Mungari and Ettore Ritacco and Giuseppe Manco", "abstract": "  The scarcity of realistic datasets poses a significant challenge in\nbenchmarking recommender systems and social network analysis methods and\ntechniques. A common and effective solution is to generate synthetic data that\nsimulates realistic interactions. However, although various methods have been\nproposed, the existing literature still lacks generators that are fully\nadaptable and allow easy manipulation of the underlying data distributions and\nstructural properties. To address this issue, the present work introduces\nGenRec, a novel framework for generating synthetic user-item interactions that\nexhibit realistic and well-known properties observed in recommendation\nscenarios. The framework is based on a stochastic generative process based on\nlatent factor modeling. Here, the latent factors can be exploited to yield\nlong-tailed preference distributions, and at the same time they characterize\nsubpopulations of users and topic-based item clusters. Notably, the proposed\nframework is highly flexible and offers a wide range of hyper-parameters for\ncustomizing the generation of user-item interactions. The code used to perform\nthe experiments is publicly available at\nhttps://anonymous.4open.science/r/GenRec-DED3.\n", "link": "http://arxiv.org/abs/2407.16594v1", "date": "2024-07-23", "relevancy": 2.6336, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5653}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5085}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenRec%3A%20A%20Flexible%20Data%20Generator%20for%20Recommendations&body=Title%3A%20GenRec%3A%20A%20Flexible%20Data%20Generator%20for%20Recommendations%0AAuthor%3A%20Erica%20Coppolillo%20and%20Simone%20Mungari%20and%20Ettore%20Ritacco%20and%20Giuseppe%20Manco%0AAbstract%3A%20%20%20The%20scarcity%20of%20realistic%20datasets%20poses%20a%20significant%20challenge%20in%0Abenchmarking%20recommender%20systems%20and%20social%20network%20analysis%20methods%20and%0Atechniques.%20A%20common%20and%20effective%20solution%20is%20to%20generate%20synthetic%20data%20that%0Asimulates%20realistic%20interactions.%20However%2C%20although%20various%20methods%20have%20been%0Aproposed%2C%20the%20existing%20literature%20still%20lacks%20generators%20that%20are%20fully%0Aadaptable%20and%20allow%20easy%20manipulation%20of%20the%20underlying%20data%20distributions%20and%0Astructural%20properties.%20To%20address%20this%20issue%2C%20the%20present%20work%20introduces%0AGenRec%2C%20a%20novel%20framework%20for%20generating%20synthetic%20user-item%20interactions%20that%0Aexhibit%20realistic%20and%20well-known%20properties%20observed%20in%20recommendation%0Ascenarios.%20The%20framework%20is%20based%20on%20a%20stochastic%20generative%20process%20based%20on%0Alatent%20factor%20modeling.%20Here%2C%20the%20latent%20factors%20can%20be%20exploited%20to%20yield%0Along-tailed%20preference%20distributions%2C%20and%20at%20the%20same%20time%20they%20characterize%0Asubpopulations%20of%20users%20and%20topic-based%20item%20clusters.%20Notably%2C%20the%20proposed%0Aframework%20is%20highly%20flexible%20and%20offers%20a%20wide%20range%20of%20hyper-parameters%20for%0Acustomizing%20the%20generation%20of%20user-item%20interactions.%20The%20code%20used%20to%20perform%0Athe%20experiments%20is%20publicly%20available%20at%0Ahttps%3A//anonymous.4open.science/r/GenRec-DED3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenRec%253A%2520A%2520Flexible%2520Data%2520Generator%2520for%2520Recommendations%26entry.906535625%3DErica%2520Coppolillo%2520and%2520Simone%2520Mungari%2520and%2520Ettore%2520Ritacco%2520and%2520Giuseppe%2520Manco%26entry.1292438233%3D%2520%2520The%2520scarcity%2520of%2520realistic%2520datasets%2520poses%2520a%2520significant%2520challenge%2520in%250Abenchmarking%2520recommender%2520systems%2520and%2520social%2520network%2520analysis%2520methods%2520and%250Atechniques.%2520A%2520common%2520and%2520effective%2520solution%2520is%2520to%2520generate%2520synthetic%2520data%2520that%250Asimulates%2520realistic%2520interactions.%2520However%252C%2520although%2520various%2520methods%2520have%2520been%250Aproposed%252C%2520the%2520existing%2520literature%2520still%2520lacks%2520generators%2520that%2520are%2520fully%250Aadaptable%2520and%2520allow%2520easy%2520manipulation%2520of%2520the%2520underlying%2520data%2520distributions%2520and%250Astructural%2520properties.%2520To%2520address%2520this%2520issue%252C%2520the%2520present%2520work%2520introduces%250AGenRec%252C%2520a%2520novel%2520framework%2520for%2520generating%2520synthetic%2520user-item%2520interactions%2520that%250Aexhibit%2520realistic%2520and%2520well-known%2520properties%2520observed%2520in%2520recommendation%250Ascenarios.%2520The%2520framework%2520is%2520based%2520on%2520a%2520stochastic%2520generative%2520process%2520based%2520on%250Alatent%2520factor%2520modeling.%2520Here%252C%2520the%2520latent%2520factors%2520can%2520be%2520exploited%2520to%2520yield%250Along-tailed%2520preference%2520distributions%252C%2520and%2520at%2520the%2520same%2520time%2520they%2520characterize%250Asubpopulations%2520of%2520users%2520and%2520topic-based%2520item%2520clusters.%2520Notably%252C%2520the%2520proposed%250Aframework%2520is%2520highly%2520flexible%2520and%2520offers%2520a%2520wide%2520range%2520of%2520hyper-parameters%2520for%250Acustomizing%2520the%2520generation%2520of%2520user-item%2520interactions.%2520The%2520code%2520used%2520to%2520perform%250Athe%2520experiments%2520is%2520publicly%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/GenRec-DED3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenRec%3A%20A%20Flexible%20Data%20Generator%20for%20Recommendations&entry.906535625=Erica%20Coppolillo%20and%20Simone%20Mungari%20and%20Ettore%20Ritacco%20and%20Giuseppe%20Manco&entry.1292438233=%20%20The%20scarcity%20of%20realistic%20datasets%20poses%20a%20significant%20challenge%20in%0Abenchmarking%20recommender%20systems%20and%20social%20network%20analysis%20methods%20and%0Atechniques.%20A%20common%20and%20effective%20solution%20is%20to%20generate%20synthetic%20data%20that%0Asimulates%20realistic%20interactions.%20However%2C%20although%20various%20methods%20have%20been%0Aproposed%2C%20the%20existing%20literature%20still%20lacks%20generators%20that%20are%20fully%0Aadaptable%20and%20allow%20easy%20manipulation%20of%20the%20underlying%20data%20distributions%20and%0Astructural%20properties.%20To%20address%20this%20issue%2C%20the%20present%20work%20introduces%0AGenRec%2C%20a%20novel%20framework%20for%20generating%20synthetic%20user-item%20interactions%20that%0Aexhibit%20realistic%20and%20well-known%20properties%20observed%20in%20recommendation%0Ascenarios.%20The%20framework%20is%20based%20on%20a%20stochastic%20generative%20process%20based%20on%0Alatent%20factor%20modeling.%20Here%2C%20the%20latent%20factors%20can%20be%20exploited%20to%20yield%0Along-tailed%20preference%20distributions%2C%20and%20at%20the%20same%20time%20they%20characterize%0Asubpopulations%20of%20users%20and%20topic-based%20item%20clusters.%20Notably%2C%20the%20proposed%0Aframework%20is%20highly%20flexible%20and%20offers%20a%20wide%20range%20of%20hyper-parameters%20for%0Acustomizing%20the%20generation%20of%20user-item%20interactions.%20The%20code%20used%20to%20perform%0Athe%20experiments%20is%20publicly%20available%20at%0Ahttps%3A//anonymous.4open.science/r/GenRec-DED3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16594v1&entry.124074799=Read"},
{"title": "Coarse-to-Fine Proposal Refinement Framework for Audio Temporal Forgery\n  Detection and Localization", "author": "Junyan Wu and Wei Lu and Xiangyang Luo and Rui Yang and Qian Wang and Xiaochun Cao", "abstract": "  Recently, a novel form of audio partial forgery has posed challenges to its\nforensics, requiring advanced countermeasures to detect subtle forgery\nmanipulations within long-duration audio. However, existing countermeasures\nstill serve a classification purpose and fail to perform meaningful analysis of\nthe start and end timestamps of partial forgery segments. To address this\nchallenge, we introduce a novel coarse-to-fine proposal refinement framework\n(CFPRF) that incorporates a frame-level detection network (FDN) and a proposal\nrefinement network (PRN) for audio temporal forgery detection and localization.\nSpecifically, the FDN aims to mine informative inconsistency cues between real\nand fake frames to obtain discriminative features that are beneficial for\nroughly indicating forgery regions. The PRN is responsible for predicting\nconfidence scores and regression offsets to refine the coarse-grained proposals\nderived from the FDN. To learn robust discriminative features, we devise a\ndifference-aware feature learning (DAFL) module guided by contrastive\nrepresentation learning to enlarge the sensitive differences between different\nframes induced by minor manipulations. We further design a boundary-aware\nfeature enhancement (BAFE) module to capture the contextual information of\nmultiple transition boundaries and guide the interaction between boundary\ninformation and temporal features via a cross-attention mechanism. Extensive\nexperiments show that our CFPRF achieves state-of-the-art performance on\nvarious datasets, including LAV-DF, ASVS2019PS, and HAD.\n", "link": "http://arxiv.org/abs/2407.16554v1", "date": "2024-07-23", "relevancy": 2.6082, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5397}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5274}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coarse-to-Fine%20Proposal%20Refinement%20Framework%20for%20Audio%20Temporal%20Forgery%0A%20%20Detection%20and%20Localization&body=Title%3A%20Coarse-to-Fine%20Proposal%20Refinement%20Framework%20for%20Audio%20Temporal%20Forgery%0A%20%20Detection%20and%20Localization%0AAuthor%3A%20Junyan%20Wu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo%20and%20Rui%20Yang%20and%20Qian%20Wang%20and%20Xiaochun%20Cao%0AAbstract%3A%20%20%20Recently%2C%20a%20novel%20form%20of%20audio%20partial%20forgery%20has%20posed%20challenges%20to%20its%0Aforensics%2C%20requiring%20advanced%20countermeasures%20to%20detect%20subtle%20forgery%0Amanipulations%20within%20long-duration%20audio.%20However%2C%20existing%20countermeasures%0Astill%20serve%20a%20classification%20purpose%20and%20fail%20to%20perform%20meaningful%20analysis%20of%0Athe%20start%20and%20end%20timestamps%20of%20partial%20forgery%20segments.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20a%20novel%20coarse-to-fine%20proposal%20refinement%20framework%0A%28CFPRF%29%20that%20incorporates%20a%20frame-level%20detection%20network%20%28FDN%29%20and%20a%20proposal%0Arefinement%20network%20%28PRN%29%20for%20audio%20temporal%20forgery%20detection%20and%20localization.%0ASpecifically%2C%20the%20FDN%20aims%20to%20mine%20informative%20inconsistency%20cues%20between%20real%0Aand%20fake%20frames%20to%20obtain%20discriminative%20features%20that%20are%20beneficial%20for%0Aroughly%20indicating%20forgery%20regions.%20The%20PRN%20is%20responsible%20for%20predicting%0Aconfidence%20scores%20and%20regression%20offsets%20to%20refine%20the%20coarse-grained%20proposals%0Aderived%20from%20the%20FDN.%20To%20learn%20robust%20discriminative%20features%2C%20we%20devise%20a%0Adifference-aware%20feature%20learning%20%28DAFL%29%20module%20guided%20by%20contrastive%0Arepresentation%20learning%20to%20enlarge%20the%20sensitive%20differences%20between%20different%0Aframes%20induced%20by%20minor%20manipulations.%20We%20further%20design%20a%20boundary-aware%0Afeature%20enhancement%20%28BAFE%29%20module%20to%20capture%20the%20contextual%20information%20of%0Amultiple%20transition%20boundaries%20and%20guide%20the%20interaction%20between%20boundary%0Ainformation%20and%20temporal%20features%20via%20a%20cross-attention%20mechanism.%20Extensive%0Aexperiments%20show%20that%20our%20CFPRF%20achieves%20state-of-the-art%20performance%20on%0Avarious%20datasets%2C%20including%20LAV-DF%2C%20ASVS2019PS%2C%20and%20HAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoarse-to-Fine%2520Proposal%2520Refinement%2520Framework%2520for%2520Audio%2520Temporal%2520Forgery%250A%2520%2520Detection%2520and%2520Localization%26entry.906535625%3DJunyan%2520Wu%2520and%2520Wei%2520Lu%2520and%2520Xiangyang%2520Luo%2520and%2520Rui%2520Yang%2520and%2520Qian%2520Wang%2520and%2520Xiaochun%2520Cao%26entry.1292438233%3D%2520%2520Recently%252C%2520a%2520novel%2520form%2520of%2520audio%2520partial%2520forgery%2520has%2520posed%2520challenges%2520to%2520its%250Aforensics%252C%2520requiring%2520advanced%2520countermeasures%2520to%2520detect%2520subtle%2520forgery%250Amanipulations%2520within%2520long-duration%2520audio.%2520However%252C%2520existing%2520countermeasures%250Astill%2520serve%2520a%2520classification%2520purpose%2520and%2520fail%2520to%2520perform%2520meaningful%2520analysis%2520of%250Athe%2520start%2520and%2520end%2520timestamps%2520of%2520partial%2520forgery%2520segments.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520introduce%2520a%2520novel%2520coarse-to-fine%2520proposal%2520refinement%2520framework%250A%2528CFPRF%2529%2520that%2520incorporates%2520a%2520frame-level%2520detection%2520network%2520%2528FDN%2529%2520and%2520a%2520proposal%250Arefinement%2520network%2520%2528PRN%2529%2520for%2520audio%2520temporal%2520forgery%2520detection%2520and%2520localization.%250ASpecifically%252C%2520the%2520FDN%2520aims%2520to%2520mine%2520informative%2520inconsistency%2520cues%2520between%2520real%250Aand%2520fake%2520frames%2520to%2520obtain%2520discriminative%2520features%2520that%2520are%2520beneficial%2520for%250Aroughly%2520indicating%2520forgery%2520regions.%2520The%2520PRN%2520is%2520responsible%2520for%2520predicting%250Aconfidence%2520scores%2520and%2520regression%2520offsets%2520to%2520refine%2520the%2520coarse-grained%2520proposals%250Aderived%2520from%2520the%2520FDN.%2520To%2520learn%2520robust%2520discriminative%2520features%252C%2520we%2520devise%2520a%250Adifference-aware%2520feature%2520learning%2520%2528DAFL%2529%2520module%2520guided%2520by%2520contrastive%250Arepresentation%2520learning%2520to%2520enlarge%2520the%2520sensitive%2520differences%2520between%2520different%250Aframes%2520induced%2520by%2520minor%2520manipulations.%2520We%2520further%2520design%2520a%2520boundary-aware%250Afeature%2520enhancement%2520%2528BAFE%2529%2520module%2520to%2520capture%2520the%2520contextual%2520information%2520of%250Amultiple%2520transition%2520boundaries%2520and%2520guide%2520the%2520interaction%2520between%2520boundary%250Ainformation%2520and%2520temporal%2520features%2520via%2520a%2520cross-attention%2520mechanism.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520CFPRF%2520achieves%2520state-of-the-art%2520performance%2520on%250Avarious%2520datasets%252C%2520including%2520LAV-DF%252C%2520ASVS2019PS%252C%2520and%2520HAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coarse-to-Fine%20Proposal%20Refinement%20Framework%20for%20Audio%20Temporal%20Forgery%0A%20%20Detection%20and%20Localization&entry.906535625=Junyan%20Wu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo%20and%20Rui%20Yang%20and%20Qian%20Wang%20and%20Xiaochun%20Cao&entry.1292438233=%20%20Recently%2C%20a%20novel%20form%20of%20audio%20partial%20forgery%20has%20posed%20challenges%20to%20its%0Aforensics%2C%20requiring%20advanced%20countermeasures%20to%20detect%20subtle%20forgery%0Amanipulations%20within%20long-duration%20audio.%20However%2C%20existing%20countermeasures%0Astill%20serve%20a%20classification%20purpose%20and%20fail%20to%20perform%20meaningful%20analysis%20of%0Athe%20start%20and%20end%20timestamps%20of%20partial%20forgery%20segments.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20a%20novel%20coarse-to-fine%20proposal%20refinement%20framework%0A%28CFPRF%29%20that%20incorporates%20a%20frame-level%20detection%20network%20%28FDN%29%20and%20a%20proposal%0Arefinement%20network%20%28PRN%29%20for%20audio%20temporal%20forgery%20detection%20and%20localization.%0ASpecifically%2C%20the%20FDN%20aims%20to%20mine%20informative%20inconsistency%20cues%20between%20real%0Aand%20fake%20frames%20to%20obtain%20discriminative%20features%20that%20are%20beneficial%20for%0Aroughly%20indicating%20forgery%20regions.%20The%20PRN%20is%20responsible%20for%20predicting%0Aconfidence%20scores%20and%20regression%20offsets%20to%20refine%20the%20coarse-grained%20proposals%0Aderived%20from%20the%20FDN.%20To%20learn%20robust%20discriminative%20features%2C%20we%20devise%20a%0Adifference-aware%20feature%20learning%20%28DAFL%29%20module%20guided%20by%20contrastive%0Arepresentation%20learning%20to%20enlarge%20the%20sensitive%20differences%20between%20different%0Aframes%20induced%20by%20minor%20manipulations.%20We%20further%20design%20a%20boundary-aware%0Afeature%20enhancement%20%28BAFE%29%20module%20to%20capture%20the%20contextual%20information%20of%0Amultiple%20transition%20boundaries%20and%20guide%20the%20interaction%20between%20boundary%0Ainformation%20and%20temporal%20features%20via%20a%20cross-attention%20mechanism.%20Extensive%0Aexperiments%20show%20that%20our%20CFPRF%20achieves%20state-of-the-art%20performance%20on%0Avarious%20datasets%2C%20including%20LAV-DF%2C%20ASVS2019PS%2C%20and%20HAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16554v1&entry.124074799=Read"},
{"title": "Gradient-Regularized Out-of-Distribution Detection", "author": "Sina Sharifi and Taha Entesari and Bardia Safaei and Vishal M. Patel and Mahyar Fazlyab", "abstract": "  One of the challenges for neural networks in real-life applications is the\noverconfident errors these models make when the data is not from the original\ntraining distribution.\n  Addressing this issue is known as Out-of-Distribution (OOD) detection.\n  Many state-of-the-art OOD methods employ an auxiliary dataset as a surrogate\nfor OOD data during training to achieve improved performance.\n  However, these methods fail to fully exploit the local information embedded\nin the auxiliary dataset.\n  In this work, we propose the idea of leveraging the information embedded in\nthe gradient of the loss function during training to enable the network to not\nonly learn a desired OOD score for each sample but also to exhibit similar\nbehavior in a local neighborhood around each sample.\n  We also develop a novel energy-based sampling method to allow the network to\nbe exposed to more informative OOD samples during the training phase. This is\nespecially important when the auxiliary dataset is large. We demonstrate the\neffectiveness of our method through extensive experiments on several OOD\nbenchmarks, improving the existing state-of-the-art FPR95 by 4% on our ImageNet\nexperiment.\n  We further provide a theoretical analysis through the lens of certified\nrobustness and Lipschitz analysis to showcase the theoretical foundation of our\nwork. Our code is available at https://github.com/o4lc/Greg-OOD.\n", "link": "http://arxiv.org/abs/2404.12368v3", "date": "2024-07-23", "relevancy": 2.6031, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5563}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5029}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-Regularized%20Out-of-Distribution%20Detection&body=Title%3A%20Gradient-Regularized%20Out-of-Distribution%20Detection%0AAuthor%3A%20Sina%20Sharifi%20and%20Taha%20Entesari%20and%20Bardia%20Safaei%20and%20Vishal%20M.%20Patel%20and%20Mahyar%20Fazlyab%0AAbstract%3A%20%20%20One%20of%20the%20challenges%20for%20neural%20networks%20in%20real-life%20applications%20is%20the%0Aoverconfident%20errors%20these%20models%20make%20when%20the%20data%20is%20not%20from%20the%20original%0Atraining%20distribution.%0A%20%20Addressing%20this%20issue%20is%20known%20as%20Out-of-Distribution%20%28OOD%29%20detection.%0A%20%20Many%20state-of-the-art%20OOD%20methods%20employ%20an%20auxiliary%20dataset%20as%20a%20surrogate%0Afor%20OOD%20data%20during%20training%20to%20achieve%20improved%20performance.%0A%20%20However%2C%20these%20methods%20fail%20to%20fully%20exploit%20the%20local%20information%20embedded%0Ain%20the%20auxiliary%20dataset.%0A%20%20In%20this%20work%2C%20we%20propose%20the%20idea%20of%20leveraging%20the%20information%20embedded%20in%0Athe%20gradient%20of%20the%20loss%20function%20during%20training%20to%20enable%20the%20network%20to%20not%0Aonly%20learn%20a%20desired%20OOD%20score%20for%20each%20sample%20but%20also%20to%20exhibit%20similar%0Abehavior%20in%20a%20local%20neighborhood%20around%20each%20sample.%0A%20%20We%20also%20develop%20a%20novel%20energy-based%20sampling%20method%20to%20allow%20the%20network%20to%0Abe%20exposed%20to%20more%20informative%20OOD%20samples%20during%20the%20training%20phase.%20This%20is%0Aespecially%20important%20when%20the%20auxiliary%20dataset%20is%20large.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20through%20extensive%20experiments%20on%20several%20OOD%0Abenchmarks%2C%20improving%20the%20existing%20state-of-the-art%20FPR95%20by%204%25%20on%20our%20ImageNet%0Aexperiment.%0A%20%20We%20further%20provide%20a%20theoretical%20analysis%20through%20the%20lens%20of%20certified%0Arobustness%20and%20Lipschitz%20analysis%20to%20showcase%20the%20theoretical%20foundation%20of%20our%0Awork.%20Our%20code%20is%20available%20at%20https%3A//github.com/o4lc/Greg-OOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12368v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-Regularized%2520Out-of-Distribution%2520Detection%26entry.906535625%3DSina%2520Sharifi%2520and%2520Taha%2520Entesari%2520and%2520Bardia%2520Safaei%2520and%2520Vishal%2520M.%2520Patel%2520and%2520Mahyar%2520Fazlyab%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520challenges%2520for%2520neural%2520networks%2520in%2520real-life%2520applications%2520is%2520the%250Aoverconfident%2520errors%2520these%2520models%2520make%2520when%2520the%2520data%2520is%2520not%2520from%2520the%2520original%250Atraining%2520distribution.%250A%2520%2520Addressing%2520this%2520issue%2520is%2520known%2520as%2520Out-of-Distribution%2520%2528OOD%2529%2520detection.%250A%2520%2520Many%2520state-of-the-art%2520OOD%2520methods%2520employ%2520an%2520auxiliary%2520dataset%2520as%2520a%2520surrogate%250Afor%2520OOD%2520data%2520during%2520training%2520to%2520achieve%2520improved%2520performance.%250A%2520%2520However%252C%2520these%2520methods%2520fail%2520to%2520fully%2520exploit%2520the%2520local%2520information%2520embedded%250Ain%2520the%2520auxiliary%2520dataset.%250A%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520idea%2520of%2520leveraging%2520the%2520information%2520embedded%2520in%250Athe%2520gradient%2520of%2520the%2520loss%2520function%2520during%2520training%2520to%2520enable%2520the%2520network%2520to%2520not%250Aonly%2520learn%2520a%2520desired%2520OOD%2520score%2520for%2520each%2520sample%2520but%2520also%2520to%2520exhibit%2520similar%250Abehavior%2520in%2520a%2520local%2520neighborhood%2520around%2520each%2520sample.%250A%2520%2520We%2520also%2520develop%2520a%2520novel%2520energy-based%2520sampling%2520method%2520to%2520allow%2520the%2520network%2520to%250Abe%2520exposed%2520to%2520more%2520informative%2520OOD%2520samples%2520during%2520the%2520training%2520phase.%2520This%2520is%250Aespecially%2520important%2520when%2520the%2520auxiliary%2520dataset%2520is%2520large.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520through%2520extensive%2520experiments%2520on%2520several%2520OOD%250Abenchmarks%252C%2520improving%2520the%2520existing%2520state-of-the-art%2520FPR95%2520by%25204%2525%2520on%2520our%2520ImageNet%250Aexperiment.%250A%2520%2520We%2520further%2520provide%2520a%2520theoretical%2520analysis%2520through%2520the%2520lens%2520of%2520certified%250Arobustness%2520and%2520Lipschitz%2520analysis%2520to%2520showcase%2520the%2520theoretical%2520foundation%2520of%2520our%250Awork.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/o4lc/Greg-OOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12368v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Regularized%20Out-of-Distribution%20Detection&entry.906535625=Sina%20Sharifi%20and%20Taha%20Entesari%20and%20Bardia%20Safaei%20and%20Vishal%20M.%20Patel%20and%20Mahyar%20Fazlyab&entry.1292438233=%20%20One%20of%20the%20challenges%20for%20neural%20networks%20in%20real-life%20applications%20is%20the%0Aoverconfident%20errors%20these%20models%20make%20when%20the%20data%20is%20not%20from%20the%20original%0Atraining%20distribution.%0A%20%20Addressing%20this%20issue%20is%20known%20as%20Out-of-Distribution%20%28OOD%29%20detection.%0A%20%20Many%20state-of-the-art%20OOD%20methods%20employ%20an%20auxiliary%20dataset%20as%20a%20surrogate%0Afor%20OOD%20data%20during%20training%20to%20achieve%20improved%20performance.%0A%20%20However%2C%20these%20methods%20fail%20to%20fully%20exploit%20the%20local%20information%20embedded%0Ain%20the%20auxiliary%20dataset.%0A%20%20In%20this%20work%2C%20we%20propose%20the%20idea%20of%20leveraging%20the%20information%20embedded%20in%0Athe%20gradient%20of%20the%20loss%20function%20during%20training%20to%20enable%20the%20network%20to%20not%0Aonly%20learn%20a%20desired%20OOD%20score%20for%20each%20sample%20but%20also%20to%20exhibit%20similar%0Abehavior%20in%20a%20local%20neighborhood%20around%20each%20sample.%0A%20%20We%20also%20develop%20a%20novel%20energy-based%20sampling%20method%20to%20allow%20the%20network%20to%0Abe%20exposed%20to%20more%20informative%20OOD%20samples%20during%20the%20training%20phase.%20This%20is%0Aespecially%20important%20when%20the%20auxiliary%20dataset%20is%20large.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20through%20extensive%20experiments%20on%20several%20OOD%0Abenchmarks%2C%20improving%20the%20existing%20state-of-the-art%20FPR95%20by%204%25%20on%20our%20ImageNet%0Aexperiment.%0A%20%20We%20further%20provide%20a%20theoretical%20analysis%20through%20the%20lens%20of%20certified%0Arobustness%20and%20Lipschitz%20analysis%20to%20showcase%20the%20theoretical%20foundation%20of%20our%0Awork.%20Our%20code%20is%20available%20at%20https%3A//github.com/o4lc/Greg-OOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12368v3&entry.124074799=Read"},
{"title": "PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects", "author": "Junyi Li and Junfeng Wu and Weizhi Zhao and Song Bai and Xiang Bai", "abstract": "  We present PartGLEE, a part-level foundation model for locating and\nidentifying both objects and parts in images. Through a unified framework,\nPartGLEE accomplishes detection, segmentation, and grounding of instances at\nany granularity in the open world scenario. Specifically, we propose a Q-Former\nto construct the hierarchical relationship between objects and parts, parsing\nevery object into corresponding semantic parts. By incorporating a large amount\nof object-level data, the hierarchical relationships can be extended, enabling\nPartGLEE to recognize a rich variety of parts. We conduct comprehensive studies\nto validate the effectiveness of our method, PartGLEE achieves the\nstate-of-the-art performance across various part-level tasks and obtain\ncompetitive results on object-level tasks. The proposed PartGLEE significantly\nenhances hierarchical modeling capabilities and part-level perception over our\nprevious GLEE model. Further analysis indicates that the hierarchical cognitive\nability of PartGLEE is able to facilitate a detailed comprehension in images\nfor mLLMs. The model and code will be released at\nhttps://provencestar.github.io/PartGLEE-Vision/ .\n", "link": "http://arxiv.org/abs/2407.16696v1", "date": "2024-07-23", "relevancy": 2.5862, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5294}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PartGLEE%3A%20A%20Foundation%20Model%20for%20Recognizing%20and%20Parsing%20Any%20Objects&body=Title%3A%20PartGLEE%3A%20A%20Foundation%20Model%20for%20Recognizing%20and%20Parsing%20Any%20Objects%0AAuthor%3A%20Junyi%20Li%20and%20Junfeng%20Wu%20and%20Weizhi%20Zhao%20and%20Song%20Bai%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20We%20present%20PartGLEE%2C%20a%20part-level%20foundation%20model%20for%20locating%20and%0Aidentifying%20both%20objects%20and%20parts%20in%20images.%20Through%20a%20unified%20framework%2C%0APartGLEE%20accomplishes%20detection%2C%20segmentation%2C%20and%20grounding%20of%20instances%20at%0Aany%20granularity%20in%20the%20open%20world%20scenario.%20Specifically%2C%20we%20propose%20a%20Q-Former%0Ato%20construct%20the%20hierarchical%20relationship%20between%20objects%20and%20parts%2C%20parsing%0Aevery%20object%20into%20corresponding%20semantic%20parts.%20By%20incorporating%20a%20large%20amount%0Aof%20object-level%20data%2C%20the%20hierarchical%20relationships%20can%20be%20extended%2C%20enabling%0APartGLEE%20to%20recognize%20a%20rich%20variety%20of%20parts.%20We%20conduct%20comprehensive%20studies%0Ato%20validate%20the%20effectiveness%20of%20our%20method%2C%20PartGLEE%20achieves%20the%0Astate-of-the-art%20performance%20across%20various%20part-level%20tasks%20and%20obtain%0Acompetitive%20results%20on%20object-level%20tasks.%20The%20proposed%20PartGLEE%20significantly%0Aenhances%20hierarchical%20modeling%20capabilities%20and%20part-level%20perception%20over%20our%0Aprevious%20GLEE%20model.%20Further%20analysis%20indicates%20that%20the%20hierarchical%20cognitive%0Aability%20of%20PartGLEE%20is%20able%20to%20facilitate%20a%20detailed%20comprehension%20in%20images%0Afor%20mLLMs.%20The%20model%20and%20code%20will%20be%20released%20at%0Ahttps%3A//provencestar.github.io/PartGLEE-Vision/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartGLEE%253A%2520A%2520Foundation%2520Model%2520for%2520Recognizing%2520and%2520Parsing%2520Any%2520Objects%26entry.906535625%3DJunyi%2520Li%2520and%2520Junfeng%2520Wu%2520and%2520Weizhi%2520Zhao%2520and%2520Song%2520Bai%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520We%2520present%2520PartGLEE%252C%2520a%2520part-level%2520foundation%2520model%2520for%2520locating%2520and%250Aidentifying%2520both%2520objects%2520and%2520parts%2520in%2520images.%2520Through%2520a%2520unified%2520framework%252C%250APartGLEE%2520accomplishes%2520detection%252C%2520segmentation%252C%2520and%2520grounding%2520of%2520instances%2520at%250Aany%2520granularity%2520in%2520the%2520open%2520world%2520scenario.%2520Specifically%252C%2520we%2520propose%2520a%2520Q-Former%250Ato%2520construct%2520the%2520hierarchical%2520relationship%2520between%2520objects%2520and%2520parts%252C%2520parsing%250Aevery%2520object%2520into%2520corresponding%2520semantic%2520parts.%2520By%2520incorporating%2520a%2520large%2520amount%250Aof%2520object-level%2520data%252C%2520the%2520hierarchical%2520relationships%2520can%2520be%2520extended%252C%2520enabling%250APartGLEE%2520to%2520recognize%2520a%2520rich%2520variety%2520of%2520parts.%2520We%2520conduct%2520comprehensive%2520studies%250Ato%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520PartGLEE%2520achieves%2520the%250Astate-of-the-art%2520performance%2520across%2520various%2520part-level%2520tasks%2520and%2520obtain%250Acompetitive%2520results%2520on%2520object-level%2520tasks.%2520The%2520proposed%2520PartGLEE%2520significantly%250Aenhances%2520hierarchical%2520modeling%2520capabilities%2520and%2520part-level%2520perception%2520over%2520our%250Aprevious%2520GLEE%2520model.%2520Further%2520analysis%2520indicates%2520that%2520the%2520hierarchical%2520cognitive%250Aability%2520of%2520PartGLEE%2520is%2520able%2520to%2520facilitate%2520a%2520detailed%2520comprehension%2520in%2520images%250Afor%2520mLLMs.%2520The%2520model%2520and%2520code%2520will%2520be%2520released%2520at%250Ahttps%253A//provencestar.github.io/PartGLEE-Vision/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartGLEE%3A%20A%20Foundation%20Model%20for%20Recognizing%20and%20Parsing%20Any%20Objects&entry.906535625=Junyi%20Li%20and%20Junfeng%20Wu%20and%20Weizhi%20Zhao%20and%20Song%20Bai%20and%20Xiang%20Bai&entry.1292438233=%20%20We%20present%20PartGLEE%2C%20a%20part-level%20foundation%20model%20for%20locating%20and%0Aidentifying%20both%20objects%20and%20parts%20in%20images.%20Through%20a%20unified%20framework%2C%0APartGLEE%20accomplishes%20detection%2C%20segmentation%2C%20and%20grounding%20of%20instances%20at%0Aany%20granularity%20in%20the%20open%20world%20scenario.%20Specifically%2C%20we%20propose%20a%20Q-Former%0Ato%20construct%20the%20hierarchical%20relationship%20between%20objects%20and%20parts%2C%20parsing%0Aevery%20object%20into%20corresponding%20semantic%20parts.%20By%20incorporating%20a%20large%20amount%0Aof%20object-level%20data%2C%20the%20hierarchical%20relationships%20can%20be%20extended%2C%20enabling%0APartGLEE%20to%20recognize%20a%20rich%20variety%20of%20parts.%20We%20conduct%20comprehensive%20studies%0Ato%20validate%20the%20effectiveness%20of%20our%20method%2C%20PartGLEE%20achieves%20the%0Astate-of-the-art%20performance%20across%20various%20part-level%20tasks%20and%20obtain%0Acompetitive%20results%20on%20object-level%20tasks.%20The%20proposed%20PartGLEE%20significantly%0Aenhances%20hierarchical%20modeling%20capabilities%20and%20part-level%20perception%20over%20our%0Aprevious%20GLEE%20model.%20Further%20analysis%20indicates%20that%20the%20hierarchical%20cognitive%0Aability%20of%20PartGLEE%20is%20able%20to%20facilitate%20a%20detailed%20comprehension%20in%20images%0Afor%20mLLMs.%20The%20model%20and%20code%20will%20be%20released%20at%0Ahttps%3A//provencestar.github.io/PartGLEE-Vision/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16696v1&entry.124074799=Read"},
{"title": "Implementing engrams from a machine learning perspective: the relevance\n  of a latent space", "author": "J Marco de Lucas", "abstract": "  In our previous work, we proposed that engrams in the brain could be\nbiologically implemented as autoencoders over recurrent neural networks. These\nautoencoders would comprise basic excitatory/inhibitory motifs, with credit\nassignment deriving from a simple homeostatic criterion. This brief note\nexamines the relevance of the latent space in these autoencoders. We consider\nthe relationship between the dimensionality of these autoencoders and the\ncomplexity of the information being encoded. We discuss how observed\ndifferences between species in their connectome could be linked to their\ncognitive capacities. Finally, we link this analysis with a basic but often\noverlooked fact: human cognition is likely limited by our own brain structure.\nHowever, this limitation does not apply to machine learning systems, and we\nshould be aware of the need to learn how to exploit this augmented vision of\nthe nature.\n", "link": "http://arxiv.org/abs/2407.16616v1", "date": "2024-07-23", "relevancy": 2.5741, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5277}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5104}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implementing%20engrams%20from%20a%20machine%20learning%20perspective%3A%20the%20relevance%0A%20%20of%20a%20latent%20space&body=Title%3A%20Implementing%20engrams%20from%20a%20machine%20learning%20perspective%3A%20the%20relevance%0A%20%20of%20a%20latent%20space%0AAuthor%3A%20J%20Marco%20de%20Lucas%0AAbstract%3A%20%20%20In%20our%20previous%20work%2C%20we%20proposed%20that%20engrams%20in%20the%20brain%20could%20be%0Abiologically%20implemented%20as%20autoencoders%20over%20recurrent%20neural%20networks.%20These%0Aautoencoders%20would%20comprise%20basic%20excitatory/inhibitory%20motifs%2C%20with%20credit%0Aassignment%20deriving%20from%20a%20simple%20homeostatic%20criterion.%20This%20brief%20note%0Aexamines%20the%20relevance%20of%20the%20latent%20space%20in%20these%20autoencoders.%20We%20consider%0Athe%20relationship%20between%20the%20dimensionality%20of%20these%20autoencoders%20and%20the%0Acomplexity%20of%20the%20information%20being%20encoded.%20We%20discuss%20how%20observed%0Adifferences%20between%20species%20in%20their%20connectome%20could%20be%20linked%20to%20their%0Acognitive%20capacities.%20Finally%2C%20we%20link%20this%20analysis%20with%20a%20basic%20but%20often%0Aoverlooked%20fact%3A%20human%20cognition%20is%20likely%20limited%20by%20our%20own%20brain%20structure.%0AHowever%2C%20this%20limitation%20does%20not%20apply%20to%20machine%20learning%20systems%2C%20and%20we%0Ashould%20be%20aware%20of%20the%20need%20to%20learn%20how%20to%20exploit%20this%20augmented%20vision%20of%0Athe%20nature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplementing%2520engrams%2520from%2520a%2520machine%2520learning%2520perspective%253A%2520the%2520relevance%250A%2520%2520of%2520a%2520latent%2520space%26entry.906535625%3DJ%2520Marco%2520de%2520Lucas%26entry.1292438233%3D%2520%2520In%2520our%2520previous%2520work%252C%2520we%2520proposed%2520that%2520engrams%2520in%2520the%2520brain%2520could%2520be%250Abiologically%2520implemented%2520as%2520autoencoders%2520over%2520recurrent%2520neural%2520networks.%2520These%250Aautoencoders%2520would%2520comprise%2520basic%2520excitatory/inhibitory%2520motifs%252C%2520with%2520credit%250Aassignment%2520deriving%2520from%2520a%2520simple%2520homeostatic%2520criterion.%2520This%2520brief%2520note%250Aexamines%2520the%2520relevance%2520of%2520the%2520latent%2520space%2520in%2520these%2520autoencoders.%2520We%2520consider%250Athe%2520relationship%2520between%2520the%2520dimensionality%2520of%2520these%2520autoencoders%2520and%2520the%250Acomplexity%2520of%2520the%2520information%2520being%2520encoded.%2520We%2520discuss%2520how%2520observed%250Adifferences%2520between%2520species%2520in%2520their%2520connectome%2520could%2520be%2520linked%2520to%2520their%250Acognitive%2520capacities.%2520Finally%252C%2520we%2520link%2520this%2520analysis%2520with%2520a%2520basic%2520but%2520often%250Aoverlooked%2520fact%253A%2520human%2520cognition%2520is%2520likely%2520limited%2520by%2520our%2520own%2520brain%2520structure.%250AHowever%252C%2520this%2520limitation%2520does%2520not%2520apply%2520to%2520machine%2520learning%2520systems%252C%2520and%2520we%250Ashould%2520be%2520aware%2520of%2520the%2520need%2520to%2520learn%2520how%2520to%2520exploit%2520this%2520augmented%2520vision%2520of%250Athe%2520nature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implementing%20engrams%20from%20a%20machine%20learning%20perspective%3A%20the%20relevance%0A%20%20of%20a%20latent%20space&entry.906535625=J%20Marco%20de%20Lucas&entry.1292438233=%20%20In%20our%20previous%20work%2C%20we%20proposed%20that%20engrams%20in%20the%20brain%20could%20be%0Abiologically%20implemented%20as%20autoencoders%20over%20recurrent%20neural%20networks.%20These%0Aautoencoders%20would%20comprise%20basic%20excitatory/inhibitory%20motifs%2C%20with%20credit%0Aassignment%20deriving%20from%20a%20simple%20homeostatic%20criterion.%20This%20brief%20note%0Aexamines%20the%20relevance%20of%20the%20latent%20space%20in%20these%20autoencoders.%20We%20consider%0Athe%20relationship%20between%20the%20dimensionality%20of%20these%20autoencoders%20and%20the%0Acomplexity%20of%20the%20information%20being%20encoded.%20We%20discuss%20how%20observed%0Adifferences%20between%20species%20in%20their%20connectome%20could%20be%20linked%20to%20their%0Acognitive%20capacities.%20Finally%2C%20we%20link%20this%20analysis%20with%20a%20basic%20but%20often%0Aoverlooked%20fact%3A%20human%20cognition%20is%20likely%20limited%20by%20our%20own%20brain%20structure.%0AHowever%2C%20this%20limitation%20does%20not%20apply%20to%20machine%20learning%20systems%2C%20and%20we%0Ashould%20be%20aware%20of%20the%20need%20to%20learn%20how%20to%20exploit%20this%20augmented%20vision%20of%0Athe%20nature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16616v1&entry.124074799=Read"},
{"title": "Diffusion Models for Monocular Depth Estimation: Overcoming Challenging\n  Conditions", "author": "Fabio Tosi and Pierluigi Zama Ramirez and Matteo Poggi", "abstract": "  We present a novel approach designed to address the complexities posed by\nchallenging, out-of-distribution data in the single-image depth estimation\ntask. Starting with images that facilitate depth prediction due to the absence\nof unfavorable factors, we systematically generate new, user-defined scenes\nwith a comprehensive set of challenges and associated depth information. This\nis achieved by leveraging cutting-edge text-to-image diffusion models with\ndepth-aware control, known for synthesizing high-quality image content from\ntextual prompts while preserving the coherence of 3D structure between\ngenerated and source imagery. Subsequent fine-tuning of any monocular depth\nnetwork is carried out through a self-distillation protocol that takes into\naccount images generated using our strategy and its own depth predictions on\nsimple, unchallenging scenes. Experiments on benchmarks tailored for our\npurposes demonstrate the effectiveness and versatility of our proposal.\n", "link": "http://arxiv.org/abs/2407.16698v1", "date": "2024-07-23", "relevancy": 2.5576, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.677}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6319}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Models%20for%20Monocular%20Depth%20Estimation%3A%20Overcoming%20Challenging%0A%20%20Conditions&body=Title%3A%20Diffusion%20Models%20for%20Monocular%20Depth%20Estimation%3A%20Overcoming%20Challenging%0A%20%20Conditions%0AAuthor%3A%20Fabio%20Tosi%20and%20Pierluigi%20Zama%20Ramirez%20and%20Matteo%20Poggi%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20designed%20to%20address%20the%20complexities%20posed%20by%0Achallenging%2C%20out-of-distribution%20data%20in%20the%20single-image%20depth%20estimation%0Atask.%20Starting%20with%20images%20that%20facilitate%20depth%20prediction%20due%20to%20the%20absence%0Aof%20unfavorable%20factors%2C%20we%20systematically%20generate%20new%2C%20user-defined%20scenes%0Awith%20a%20comprehensive%20set%20of%20challenges%20and%20associated%20depth%20information.%20This%0Ais%20achieved%20by%20leveraging%20cutting-edge%20text-to-image%20diffusion%20models%20with%0Adepth-aware%20control%2C%20known%20for%20synthesizing%20high-quality%20image%20content%20from%0Atextual%20prompts%20while%20preserving%20the%20coherence%20of%203D%20structure%20between%0Agenerated%20and%20source%20imagery.%20Subsequent%20fine-tuning%20of%20any%20monocular%20depth%0Anetwork%20is%20carried%20out%20through%20a%20self-distillation%20protocol%20that%20takes%20into%0Aaccount%20images%20generated%20using%20our%20strategy%20and%20its%20own%20depth%20predictions%20on%0Asimple%2C%20unchallenging%20scenes.%20Experiments%20on%20benchmarks%20tailored%20for%20our%0Apurposes%20demonstrate%20the%20effectiveness%20and%20versatility%20of%20our%20proposal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Models%2520for%2520Monocular%2520Depth%2520Estimation%253A%2520Overcoming%2520Challenging%250A%2520%2520Conditions%26entry.906535625%3DFabio%2520Tosi%2520and%2520Pierluigi%2520Zama%2520Ramirez%2520and%2520Matteo%2520Poggi%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520designed%2520to%2520address%2520the%2520complexities%2520posed%2520by%250Achallenging%252C%2520out-of-distribution%2520data%2520in%2520the%2520single-image%2520depth%2520estimation%250Atask.%2520Starting%2520with%2520images%2520that%2520facilitate%2520depth%2520prediction%2520due%2520to%2520the%2520absence%250Aof%2520unfavorable%2520factors%252C%2520we%2520systematically%2520generate%2520new%252C%2520user-defined%2520scenes%250Awith%2520a%2520comprehensive%2520set%2520of%2520challenges%2520and%2520associated%2520depth%2520information.%2520This%250Ais%2520achieved%2520by%2520leveraging%2520cutting-edge%2520text-to-image%2520diffusion%2520models%2520with%250Adepth-aware%2520control%252C%2520known%2520for%2520synthesizing%2520high-quality%2520image%2520content%2520from%250Atextual%2520prompts%2520while%2520preserving%2520the%2520coherence%2520of%25203D%2520structure%2520between%250Agenerated%2520and%2520source%2520imagery.%2520Subsequent%2520fine-tuning%2520of%2520any%2520monocular%2520depth%250Anetwork%2520is%2520carried%2520out%2520through%2520a%2520self-distillation%2520protocol%2520that%2520takes%2520into%250Aaccount%2520images%2520generated%2520using%2520our%2520strategy%2520and%2520its%2520own%2520depth%2520predictions%2520on%250Asimple%252C%2520unchallenging%2520scenes.%2520Experiments%2520on%2520benchmarks%2520tailored%2520for%2520our%250Apurposes%2520demonstrate%2520the%2520effectiveness%2520and%2520versatility%2520of%2520our%2520proposal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Models%20for%20Monocular%20Depth%20Estimation%3A%20Overcoming%20Challenging%0A%20%20Conditions&entry.906535625=Fabio%20Tosi%20and%20Pierluigi%20Zama%20Ramirez%20and%20Matteo%20Poggi&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20designed%20to%20address%20the%20complexities%20posed%20by%0Achallenging%2C%20out-of-distribution%20data%20in%20the%20single-image%20depth%20estimation%0Atask.%20Starting%20with%20images%20that%20facilitate%20depth%20prediction%20due%20to%20the%20absence%0Aof%20unfavorable%20factors%2C%20we%20systematically%20generate%20new%2C%20user-defined%20scenes%0Awith%20a%20comprehensive%20set%20of%20challenges%20and%20associated%20depth%20information.%20This%0Ais%20achieved%20by%20leveraging%20cutting-edge%20text-to-image%20diffusion%20models%20with%0Adepth-aware%20control%2C%20known%20for%20synthesizing%20high-quality%20image%20content%20from%0Atextual%20prompts%20while%20preserving%20the%20coherence%20of%203D%20structure%20between%0Agenerated%20and%20source%20imagery.%20Subsequent%20fine-tuning%20of%20any%20monocular%20depth%0Anetwork%20is%20carried%20out%20through%20a%20self-distillation%20protocol%20that%20takes%20into%0Aaccount%20images%20generated%20using%20our%20strategy%20and%20its%20own%20depth%20predictions%20on%0Asimple%2C%20unchallenging%20scenes.%20Experiments%20on%20benchmarks%20tailored%20for%20our%0Apurposes%20demonstrate%20the%20effectiveness%20and%20versatility%20of%20our%20proposal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16698v1&entry.124074799=Read"},
{"title": "Deformable Convolution Based Road Scene Semantic Segmentation of Fisheye\n  Images in Autonomous Driving", "author": "Anam Manzoor and Aryan Singh and Ganesh Sistu and Reenu Mohandas and Eoin Grua and Anthony Scanlan and Ciar\u00e1n Eising", "abstract": "  This study investigates the effectiveness of modern Deformable Convolutional\nNeural Networks (DCNNs) for semantic segmentation tasks, particularly in\nautonomous driving scenarios with fisheye images. These images, providing a\nwide field of view, pose unique challenges for extracting spatial and geometric\ninformation due to dynamic changes in object attributes. Our experiments focus\non segmenting the WoodScape fisheye image dataset into ten distinct classes,\nassessing the Deformable Networks' ability to capture intricate spatial\nrelationships and improve segmentation accuracy. Additionally, we explore\ndifferent loss functions to address class imbalance issues and compare the\nperformance of conventional CNN architectures with Deformable Convolution-based\nCNNs, including Vanilla U-Net and Residual U-Net architectures. The significant\nimprovement in mIoU score resulting from integrating Deformable CNNs\ndemonstrates their effectiveness in handling the geometric distortions present\nin fisheye imagery, exceeding the performance of traditional CNN architectures.\nThis underscores the significant role of Deformable convolution in enhancing\nsemantic segmentation performance for fisheye imagery.\n", "link": "http://arxiv.org/abs/2407.16647v1", "date": "2024-07-23", "relevancy": 2.5407, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5392}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4935}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deformable%20Convolution%20Based%20Road%20Scene%20Semantic%20Segmentation%20of%20Fisheye%0A%20%20Images%20in%20Autonomous%20Driving&body=Title%3A%20Deformable%20Convolution%20Based%20Road%20Scene%20Semantic%20Segmentation%20of%20Fisheye%0A%20%20Images%20in%20Autonomous%20Driving%0AAuthor%3A%20Anam%20Manzoor%20and%20Aryan%20Singh%20and%20Ganesh%20Sistu%20and%20Reenu%20Mohandas%20and%20Eoin%20Grua%20and%20Anthony%20Scanlan%20and%20Ciar%C3%A1n%20Eising%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20effectiveness%20of%20modern%20Deformable%20Convolutional%0ANeural%20Networks%20%28DCNNs%29%20for%20semantic%20segmentation%20tasks%2C%20particularly%20in%0Aautonomous%20driving%20scenarios%20with%20fisheye%20images.%20These%20images%2C%20providing%20a%0Awide%20field%20of%20view%2C%20pose%20unique%20challenges%20for%20extracting%20spatial%20and%20geometric%0Ainformation%20due%20to%20dynamic%20changes%20in%20object%20attributes.%20Our%20experiments%20focus%0Aon%20segmenting%20the%20WoodScape%20fisheye%20image%20dataset%20into%20ten%20distinct%20classes%2C%0Aassessing%20the%20Deformable%20Networks%27%20ability%20to%20capture%20intricate%20spatial%0Arelationships%20and%20improve%20segmentation%20accuracy.%20Additionally%2C%20we%20explore%0Adifferent%20loss%20functions%20to%20address%20class%20imbalance%20issues%20and%20compare%20the%0Aperformance%20of%20conventional%20CNN%20architectures%20with%20Deformable%20Convolution-based%0ACNNs%2C%20including%20Vanilla%20U-Net%20and%20Residual%20U-Net%20architectures.%20The%20significant%0Aimprovement%20in%20mIoU%20score%20resulting%20from%20integrating%20Deformable%20CNNs%0Ademonstrates%20their%20effectiveness%20in%20handling%20the%20geometric%20distortions%20present%0Ain%20fisheye%20imagery%2C%20exceeding%20the%20performance%20of%20traditional%20CNN%20architectures.%0AThis%20underscores%20the%20significant%20role%20of%20Deformable%20convolution%20in%20enhancing%0Asemantic%20segmentation%20performance%20for%20fisheye%20imagery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeformable%2520Convolution%2520Based%2520Road%2520Scene%2520Semantic%2520Segmentation%2520of%2520Fisheye%250A%2520%2520Images%2520in%2520Autonomous%2520Driving%26entry.906535625%3DAnam%2520Manzoor%2520and%2520Aryan%2520Singh%2520and%2520Ganesh%2520Sistu%2520and%2520Reenu%2520Mohandas%2520and%2520Eoin%2520Grua%2520and%2520Anthony%2520Scanlan%2520and%2520Ciar%25C3%25A1n%2520Eising%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520effectiveness%2520of%2520modern%2520Deformable%2520Convolutional%250ANeural%2520Networks%2520%2528DCNNs%2529%2520for%2520semantic%2520segmentation%2520tasks%252C%2520particularly%2520in%250Aautonomous%2520driving%2520scenarios%2520with%2520fisheye%2520images.%2520These%2520images%252C%2520providing%2520a%250Awide%2520field%2520of%2520view%252C%2520pose%2520unique%2520challenges%2520for%2520extracting%2520spatial%2520and%2520geometric%250Ainformation%2520due%2520to%2520dynamic%2520changes%2520in%2520object%2520attributes.%2520Our%2520experiments%2520focus%250Aon%2520segmenting%2520the%2520WoodScape%2520fisheye%2520image%2520dataset%2520into%2520ten%2520distinct%2520classes%252C%250Aassessing%2520the%2520Deformable%2520Networks%2527%2520ability%2520to%2520capture%2520intricate%2520spatial%250Arelationships%2520and%2520improve%2520segmentation%2520accuracy.%2520Additionally%252C%2520we%2520explore%250Adifferent%2520loss%2520functions%2520to%2520address%2520class%2520imbalance%2520issues%2520and%2520compare%2520the%250Aperformance%2520of%2520conventional%2520CNN%2520architectures%2520with%2520Deformable%2520Convolution-based%250ACNNs%252C%2520including%2520Vanilla%2520U-Net%2520and%2520Residual%2520U-Net%2520architectures.%2520The%2520significant%250Aimprovement%2520in%2520mIoU%2520score%2520resulting%2520from%2520integrating%2520Deformable%2520CNNs%250Ademonstrates%2520their%2520effectiveness%2520in%2520handling%2520the%2520geometric%2520distortions%2520present%250Ain%2520fisheye%2520imagery%252C%2520exceeding%2520the%2520performance%2520of%2520traditional%2520CNN%2520architectures.%250AThis%2520underscores%2520the%2520significant%2520role%2520of%2520Deformable%2520convolution%2520in%2520enhancing%250Asemantic%2520segmentation%2520performance%2520for%2520fisheye%2520imagery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deformable%20Convolution%20Based%20Road%20Scene%20Semantic%20Segmentation%20of%20Fisheye%0A%20%20Images%20in%20Autonomous%20Driving&entry.906535625=Anam%20Manzoor%20and%20Aryan%20Singh%20and%20Ganesh%20Sistu%20and%20Reenu%20Mohandas%20and%20Eoin%20Grua%20and%20Anthony%20Scanlan%20and%20Ciar%C3%A1n%20Eising&entry.1292438233=%20%20This%20study%20investigates%20the%20effectiveness%20of%20modern%20Deformable%20Convolutional%0ANeural%20Networks%20%28DCNNs%29%20for%20semantic%20segmentation%20tasks%2C%20particularly%20in%0Aautonomous%20driving%20scenarios%20with%20fisheye%20images.%20These%20images%2C%20providing%20a%0Awide%20field%20of%20view%2C%20pose%20unique%20challenges%20for%20extracting%20spatial%20and%20geometric%0Ainformation%20due%20to%20dynamic%20changes%20in%20object%20attributes.%20Our%20experiments%20focus%0Aon%20segmenting%20the%20WoodScape%20fisheye%20image%20dataset%20into%20ten%20distinct%20classes%2C%0Aassessing%20the%20Deformable%20Networks%27%20ability%20to%20capture%20intricate%20spatial%0Arelationships%20and%20improve%20segmentation%20accuracy.%20Additionally%2C%20we%20explore%0Adifferent%20loss%20functions%20to%20address%20class%20imbalance%20issues%20and%20compare%20the%0Aperformance%20of%20conventional%20CNN%20architectures%20with%20Deformable%20Convolution-based%0ACNNs%2C%20including%20Vanilla%20U-Net%20and%20Residual%20U-Net%20architectures.%20The%20significant%0Aimprovement%20in%20mIoU%20score%20resulting%20from%20integrating%20Deformable%20CNNs%0Ademonstrates%20their%20effectiveness%20in%20handling%20the%20geometric%20distortions%20present%0Ain%20fisheye%20imagery%2C%20exceeding%20the%20performance%20of%20traditional%20CNN%20architectures.%0AThis%20underscores%20the%20significant%20role%20of%20Deformable%20convolution%20in%20enhancing%0Asemantic%20segmentation%20performance%20for%20fisheye%20imagery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16647v1&entry.124074799=Read"},
{"title": "First-order ANIL provably learns representations despite\n  overparametrization", "author": "O\u011fuz Kaan Y\u00fcksel and Etienne Boursier and Nicolas Flammarion", "abstract": "  Due to its empirical success in few-shot classification and reinforcement\nlearning, meta-learning has recently received significant interest.\nMeta-learning methods leverage data from previous tasks to learn a new task in\na sample-efficient manner. In particular, model-agnostic methods look for\ninitialization points from which gradient descent quickly adapts to any new\ntask. Although it has been empirically suggested that such methods perform well\nby learning shared representations during pretraining, there is limited\ntheoretical evidence of such behavior. More importantly, it has not been shown\nthat these methods still learn a shared structure, despite architectural\nmisspecifications. In this direction, this work shows, in the limit of an\ninfinite number of tasks, that first-order ANIL with a linear two-layer network\narchitecture successfully learns linear shared representations. This result\neven holds with overparametrization; having a width larger than the dimension\nof the shared representations results in an asymptotically low-rank solution.\nThe learned solution then yields a good adaptation performance on any new task\nafter a single gradient step. Overall, this illustrates how well model-agnostic\nmethods such as first-order ANIL can learn shared representations.\n", "link": "http://arxiv.org/abs/2303.01335v3", "date": "2024-07-23", "relevancy": 2.5272, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5106}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5077}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20First-order%20ANIL%20provably%20learns%20representations%20despite%0A%20%20overparametrization&body=Title%3A%20First-order%20ANIL%20provably%20learns%20representations%20despite%0A%20%20overparametrization%0AAuthor%3A%20O%C4%9Fuz%20Kaan%20Y%C3%BCksel%20and%20Etienne%20Boursier%20and%20Nicolas%20Flammarion%0AAbstract%3A%20%20%20Due%20to%20its%20empirical%20success%20in%20few-shot%20classification%20and%20reinforcement%0Alearning%2C%20meta-learning%20has%20recently%20received%20significant%20interest.%0AMeta-learning%20methods%20leverage%20data%20from%20previous%20tasks%20to%20learn%20a%20new%20task%20in%0Aa%20sample-efficient%20manner.%20In%20particular%2C%20model-agnostic%20methods%20look%20for%0Ainitialization%20points%20from%20which%20gradient%20descent%20quickly%20adapts%20to%20any%20new%0Atask.%20Although%20it%20has%20been%20empirically%20suggested%20that%20such%20methods%20perform%20well%0Aby%20learning%20shared%20representations%20during%20pretraining%2C%20there%20is%20limited%0Atheoretical%20evidence%20of%20such%20behavior.%20More%20importantly%2C%20it%20has%20not%20been%20shown%0Athat%20these%20methods%20still%20learn%20a%20shared%20structure%2C%20despite%20architectural%0Amisspecifications.%20In%20this%20direction%2C%20this%20work%20shows%2C%20in%20the%20limit%20of%20an%0Ainfinite%20number%20of%20tasks%2C%20that%20first-order%20ANIL%20with%20a%20linear%20two-layer%20network%0Aarchitecture%20successfully%20learns%20linear%20shared%20representations.%20This%20result%0Aeven%20holds%20with%20overparametrization%3B%20having%20a%20width%20larger%20than%20the%20dimension%0Aof%20the%20shared%20representations%20results%20in%20an%20asymptotically%20low-rank%20solution.%0AThe%20learned%20solution%20then%20yields%20a%20good%20adaptation%20performance%20on%20any%20new%20task%0Aafter%20a%20single%20gradient%20step.%20Overall%2C%20this%20illustrates%20how%20well%20model-agnostic%0Amethods%20such%20as%20first-order%20ANIL%20can%20learn%20shared%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.01335v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFirst-order%2520ANIL%2520provably%2520learns%2520representations%2520despite%250A%2520%2520overparametrization%26entry.906535625%3DO%25C4%259Fuz%2520Kaan%2520Y%25C3%25BCksel%2520and%2520Etienne%2520Boursier%2520and%2520Nicolas%2520Flammarion%26entry.1292438233%3D%2520%2520Due%2520to%2520its%2520empirical%2520success%2520in%2520few-shot%2520classification%2520and%2520reinforcement%250Alearning%252C%2520meta-learning%2520has%2520recently%2520received%2520significant%2520interest.%250AMeta-learning%2520methods%2520leverage%2520data%2520from%2520previous%2520tasks%2520to%2520learn%2520a%2520new%2520task%2520in%250Aa%2520sample-efficient%2520manner.%2520In%2520particular%252C%2520model-agnostic%2520methods%2520look%2520for%250Ainitialization%2520points%2520from%2520which%2520gradient%2520descent%2520quickly%2520adapts%2520to%2520any%2520new%250Atask.%2520Although%2520it%2520has%2520been%2520empirically%2520suggested%2520that%2520such%2520methods%2520perform%2520well%250Aby%2520learning%2520shared%2520representations%2520during%2520pretraining%252C%2520there%2520is%2520limited%250Atheoretical%2520evidence%2520of%2520such%2520behavior.%2520More%2520importantly%252C%2520it%2520has%2520not%2520been%2520shown%250Athat%2520these%2520methods%2520still%2520learn%2520a%2520shared%2520structure%252C%2520despite%2520architectural%250Amisspecifications.%2520In%2520this%2520direction%252C%2520this%2520work%2520shows%252C%2520in%2520the%2520limit%2520of%2520an%250Ainfinite%2520number%2520of%2520tasks%252C%2520that%2520first-order%2520ANIL%2520with%2520a%2520linear%2520two-layer%2520network%250Aarchitecture%2520successfully%2520learns%2520linear%2520shared%2520representations.%2520This%2520result%250Aeven%2520holds%2520with%2520overparametrization%253B%2520having%2520a%2520width%2520larger%2520than%2520the%2520dimension%250Aof%2520the%2520shared%2520representations%2520results%2520in%2520an%2520asymptotically%2520low-rank%2520solution.%250AThe%2520learned%2520solution%2520then%2520yields%2520a%2520good%2520adaptation%2520performance%2520on%2520any%2520new%2520task%250Aafter%2520a%2520single%2520gradient%2520step.%2520Overall%252C%2520this%2520illustrates%2520how%2520well%2520model-agnostic%250Amethods%2520such%2520as%2520first-order%2520ANIL%2520can%2520learn%2520shared%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.01335v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=First-order%20ANIL%20provably%20learns%20representations%20despite%0A%20%20overparametrization&entry.906535625=O%C4%9Fuz%20Kaan%20Y%C3%BCksel%20and%20Etienne%20Boursier%20and%20Nicolas%20Flammarion&entry.1292438233=%20%20Due%20to%20its%20empirical%20success%20in%20few-shot%20classification%20and%20reinforcement%0Alearning%2C%20meta-learning%20has%20recently%20received%20significant%20interest.%0AMeta-learning%20methods%20leverage%20data%20from%20previous%20tasks%20to%20learn%20a%20new%20task%20in%0Aa%20sample-efficient%20manner.%20In%20particular%2C%20model-agnostic%20methods%20look%20for%0Ainitialization%20points%20from%20which%20gradient%20descent%20quickly%20adapts%20to%20any%20new%0Atask.%20Although%20it%20has%20been%20empirically%20suggested%20that%20such%20methods%20perform%20well%0Aby%20learning%20shared%20representations%20during%20pretraining%2C%20there%20is%20limited%0Atheoretical%20evidence%20of%20such%20behavior.%20More%20importantly%2C%20it%20has%20not%20been%20shown%0Athat%20these%20methods%20still%20learn%20a%20shared%20structure%2C%20despite%20architectural%0Amisspecifications.%20In%20this%20direction%2C%20this%20work%20shows%2C%20in%20the%20limit%20of%20an%0Ainfinite%20number%20of%20tasks%2C%20that%20first-order%20ANIL%20with%20a%20linear%20two-layer%20network%0Aarchitecture%20successfully%20learns%20linear%20shared%20representations.%20This%20result%0Aeven%20holds%20with%20overparametrization%3B%20having%20a%20width%20larger%20than%20the%20dimension%0Aof%20the%20shared%20representations%20results%20in%20an%20asymptotically%20low-rank%20solution.%0AThe%20learned%20solution%20then%20yields%20a%20good%20adaptation%20performance%20on%20any%20new%20task%0Aafter%20a%20single%20gradient%20step.%20Overall%2C%20this%20illustrates%20how%20well%20model-agnostic%0Amethods%20such%20as%20first-order%20ANIL%20can%20learn%20shared%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.01335v3&entry.124074799=Read"},
{"title": "A Framework for Pupil Tracking with Event Cameras", "author": "Khadija Iddrisu and Waseem Shariff and Suzanne Little", "abstract": "  Saccades are extremely rapid movements of both eyes that occur\nsimultaneously, typically observed when an individual shifts their focus from\none object to another. These movements are among the swiftest produced by\nhumans and possess the potential to achieve velocities greater than that of\nblinks. The peak angular speed of the eye during a saccade can reach as high as\n700{\\deg}/s in humans, especially during larger saccades that cover a visual\nangle of 25{\\deg}. Previous research has demonstrated encouraging outcomes in\ncomprehending neurological conditions through the study of saccades. A\nnecessary step in saccade detection involves accurately identifying the precise\nlocation of the pupil within the eye, from which additional information such as\ngaze angles can be inferred. Conventional frame-based cameras often struggle\nwith the high temporal precision necessary for tracking very fast movements,\nresulting in motion blur and latency issues. Event cameras, on the other hand,\noffer a promising alternative by recording changes in the visual scene\nasynchronously and providing high temporal resolution and low latency. By\nbridging the gap between traditional computer vision and event-based vision, we\npresent events as frames that can be readily utilized by standard deep learning\nalgorithms. This approach harnesses YOLOv8, a state-of-the-art object detection\ntechnology, to process these frames for pupil tracking using the publicly\naccessible Ev-Eye dataset. Experimental results demonstrate the framework's\neffectiveness, highlighting its potential applications in neuroscience,\nophthalmology, and human-computer interaction.\n", "link": "http://arxiv.org/abs/2407.16665v1", "date": "2024-07-23", "relevancy": 2.5154, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5101}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5079}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Pupil%20Tracking%20with%20Event%20Cameras&body=Title%3A%20A%20Framework%20for%20Pupil%20Tracking%20with%20Event%20Cameras%0AAuthor%3A%20Khadija%20Iddrisu%20and%20Waseem%20Shariff%20and%20Suzanne%20Little%0AAbstract%3A%20%20%20Saccades%20are%20extremely%20rapid%20movements%20of%20both%20eyes%20that%20occur%0Asimultaneously%2C%20typically%20observed%20when%20an%20individual%20shifts%20their%20focus%20from%0Aone%20object%20to%20another.%20These%20movements%20are%20among%20the%20swiftest%20produced%20by%0Ahumans%20and%20possess%20the%20potential%20to%20achieve%20velocities%20greater%20than%20that%20of%0Ablinks.%20The%20peak%20angular%20speed%20of%20the%20eye%20during%20a%20saccade%20can%20reach%20as%20high%20as%0A700%7B%5Cdeg%7D/s%20in%20humans%2C%20especially%20during%20larger%20saccades%20that%20cover%20a%20visual%0Aangle%20of%2025%7B%5Cdeg%7D.%20Previous%20research%20has%20demonstrated%20encouraging%20outcomes%20in%0Acomprehending%20neurological%20conditions%20through%20the%20study%20of%20saccades.%20A%0Anecessary%20step%20in%20saccade%20detection%20involves%20accurately%20identifying%20the%20precise%0Alocation%20of%20the%20pupil%20within%20the%20eye%2C%20from%20which%20additional%20information%20such%20as%0Agaze%20angles%20can%20be%20inferred.%20Conventional%20frame-based%20cameras%20often%20struggle%0Awith%20the%20high%20temporal%20precision%20necessary%20for%20tracking%20very%20fast%20movements%2C%0Aresulting%20in%20motion%20blur%20and%20latency%20issues.%20Event%20cameras%2C%20on%20the%20other%20hand%2C%0Aoffer%20a%20promising%20alternative%20by%20recording%20changes%20in%20the%20visual%20scene%0Aasynchronously%20and%20providing%20high%20temporal%20resolution%20and%20low%20latency.%20By%0Abridging%20the%20gap%20between%20traditional%20computer%20vision%20and%20event-based%20vision%2C%20we%0Apresent%20events%20as%20frames%20that%20can%20be%20readily%20utilized%20by%20standard%20deep%20learning%0Aalgorithms.%20This%20approach%20harnesses%20YOLOv8%2C%20a%20state-of-the-art%20object%20detection%0Atechnology%2C%20to%20process%20these%20frames%20for%20pupil%20tracking%20using%20the%20publicly%0Aaccessible%20Ev-Eye%20dataset.%20Experimental%20results%20demonstrate%20the%20framework%27s%0Aeffectiveness%2C%20highlighting%20its%20potential%20applications%20in%20neuroscience%2C%0Aophthalmology%2C%20and%20human-computer%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Pupil%2520Tracking%2520with%2520Event%2520Cameras%26entry.906535625%3DKhadija%2520Iddrisu%2520and%2520Waseem%2520Shariff%2520and%2520Suzanne%2520Little%26entry.1292438233%3D%2520%2520Saccades%2520are%2520extremely%2520rapid%2520movements%2520of%2520both%2520eyes%2520that%2520occur%250Asimultaneously%252C%2520typically%2520observed%2520when%2520an%2520individual%2520shifts%2520their%2520focus%2520from%250Aone%2520object%2520to%2520another.%2520These%2520movements%2520are%2520among%2520the%2520swiftest%2520produced%2520by%250Ahumans%2520and%2520possess%2520the%2520potential%2520to%2520achieve%2520velocities%2520greater%2520than%2520that%2520of%250Ablinks.%2520The%2520peak%2520angular%2520speed%2520of%2520the%2520eye%2520during%2520a%2520saccade%2520can%2520reach%2520as%2520high%2520as%250A700%257B%255Cdeg%257D/s%2520in%2520humans%252C%2520especially%2520during%2520larger%2520saccades%2520that%2520cover%2520a%2520visual%250Aangle%2520of%252025%257B%255Cdeg%257D.%2520Previous%2520research%2520has%2520demonstrated%2520encouraging%2520outcomes%2520in%250Acomprehending%2520neurological%2520conditions%2520through%2520the%2520study%2520of%2520saccades.%2520A%250Anecessary%2520step%2520in%2520saccade%2520detection%2520involves%2520accurately%2520identifying%2520the%2520precise%250Alocation%2520of%2520the%2520pupil%2520within%2520the%2520eye%252C%2520from%2520which%2520additional%2520information%2520such%2520as%250Agaze%2520angles%2520can%2520be%2520inferred.%2520Conventional%2520frame-based%2520cameras%2520often%2520struggle%250Awith%2520the%2520high%2520temporal%2520precision%2520necessary%2520for%2520tracking%2520very%2520fast%2520movements%252C%250Aresulting%2520in%2520motion%2520blur%2520and%2520latency%2520issues.%2520Event%2520cameras%252C%2520on%2520the%2520other%2520hand%252C%250Aoffer%2520a%2520promising%2520alternative%2520by%2520recording%2520changes%2520in%2520the%2520visual%2520scene%250Aasynchronously%2520and%2520providing%2520high%2520temporal%2520resolution%2520and%2520low%2520latency.%2520By%250Abridging%2520the%2520gap%2520between%2520traditional%2520computer%2520vision%2520and%2520event-based%2520vision%252C%2520we%250Apresent%2520events%2520as%2520frames%2520that%2520can%2520be%2520readily%2520utilized%2520by%2520standard%2520deep%2520learning%250Aalgorithms.%2520This%2520approach%2520harnesses%2520YOLOv8%252C%2520a%2520state-of-the-art%2520object%2520detection%250Atechnology%252C%2520to%2520process%2520these%2520frames%2520for%2520pupil%2520tracking%2520using%2520the%2520publicly%250Aaccessible%2520Ev-Eye%2520dataset.%2520Experimental%2520results%2520demonstrate%2520the%2520framework%2527s%250Aeffectiveness%252C%2520highlighting%2520its%2520potential%2520applications%2520in%2520neuroscience%252C%250Aophthalmology%252C%2520and%2520human-computer%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Pupil%20Tracking%20with%20Event%20Cameras&entry.906535625=Khadija%20Iddrisu%20and%20Waseem%20Shariff%20and%20Suzanne%20Little&entry.1292438233=%20%20Saccades%20are%20extremely%20rapid%20movements%20of%20both%20eyes%20that%20occur%0Asimultaneously%2C%20typically%20observed%20when%20an%20individual%20shifts%20their%20focus%20from%0Aone%20object%20to%20another.%20These%20movements%20are%20among%20the%20swiftest%20produced%20by%0Ahumans%20and%20possess%20the%20potential%20to%20achieve%20velocities%20greater%20than%20that%20of%0Ablinks.%20The%20peak%20angular%20speed%20of%20the%20eye%20during%20a%20saccade%20can%20reach%20as%20high%20as%0A700%7B%5Cdeg%7D/s%20in%20humans%2C%20especially%20during%20larger%20saccades%20that%20cover%20a%20visual%0Aangle%20of%2025%7B%5Cdeg%7D.%20Previous%20research%20has%20demonstrated%20encouraging%20outcomes%20in%0Acomprehending%20neurological%20conditions%20through%20the%20study%20of%20saccades.%20A%0Anecessary%20step%20in%20saccade%20detection%20involves%20accurately%20identifying%20the%20precise%0Alocation%20of%20the%20pupil%20within%20the%20eye%2C%20from%20which%20additional%20information%20such%20as%0Agaze%20angles%20can%20be%20inferred.%20Conventional%20frame-based%20cameras%20often%20struggle%0Awith%20the%20high%20temporal%20precision%20necessary%20for%20tracking%20very%20fast%20movements%2C%0Aresulting%20in%20motion%20blur%20and%20latency%20issues.%20Event%20cameras%2C%20on%20the%20other%20hand%2C%0Aoffer%20a%20promising%20alternative%20by%20recording%20changes%20in%20the%20visual%20scene%0Aasynchronously%20and%20providing%20high%20temporal%20resolution%20and%20low%20latency.%20By%0Abridging%20the%20gap%20between%20traditional%20computer%20vision%20and%20event-based%20vision%2C%20we%0Apresent%20events%20as%20frames%20that%20can%20be%20readily%20utilized%20by%20standard%20deep%20learning%0Aalgorithms.%20This%20approach%20harnesses%20YOLOv8%2C%20a%20state-of-the-art%20object%20detection%0Atechnology%2C%20to%20process%20these%20frames%20for%20pupil%20tracking%20using%20the%20publicly%0Aaccessible%20Ev-Eye%20dataset.%20Experimental%20results%20demonstrate%20the%20framework%27s%0Aeffectiveness%2C%20highlighting%20its%20potential%20applications%20in%20neuroscience%2C%0Aophthalmology%2C%20and%20human-computer%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16665v1&entry.124074799=Read"},
{"title": "A Geometry-Aware Algorithm to Learn Hierarchical Embeddings in\n  Hyperbolic Space", "author": "Zhangyu Wang and Lantian Xu and Zhifeng Kong and Weilong Wang and Xuyu Peng and Enyang Zheng", "abstract": "  Hyperbolic embeddings are a class of representation learning methods that\noffer competitive performances when data can be abstracted as a tree-like\ngraph. However, in practice, learning hyperbolic embeddings of hierarchical\ndata is difficult due to the different geometry between hyperbolic space and\nthe Euclidean space. To address such difficulties, we first categorize three\nkinds of illness that harm the performance of the embeddings. Then, we develop\na geometry-aware algorithm using a dilation operation and a transitive closure\nregularization to tackle these illnesses. We empirically validate these\ntechniques and present a theoretical analysis of the mechanism behind the\ndilation operation. Experiments on synthetic and real-world datasets reveal\nsuperior performances of our algorithm.\n", "link": "http://arxiv.org/abs/2407.16641v1", "date": "2024-07-23", "relevancy": 2.4832, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5235}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.484}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Geometry-Aware%20Algorithm%20to%20Learn%20Hierarchical%20Embeddings%20in%0A%20%20Hyperbolic%20Space&body=Title%3A%20A%20Geometry-Aware%20Algorithm%20to%20Learn%20Hierarchical%20Embeddings%20in%0A%20%20Hyperbolic%20Space%0AAuthor%3A%20Zhangyu%20Wang%20and%20Lantian%20Xu%20and%20Zhifeng%20Kong%20and%20Weilong%20Wang%20and%20Xuyu%20Peng%20and%20Enyang%20Zheng%0AAbstract%3A%20%20%20Hyperbolic%20embeddings%20are%20a%20class%20of%20representation%20learning%20methods%20that%0Aoffer%20competitive%20performances%20when%20data%20can%20be%20abstracted%20as%20a%20tree-like%0Agraph.%20However%2C%20in%20practice%2C%20learning%20hyperbolic%20embeddings%20of%20hierarchical%0Adata%20is%20difficult%20due%20to%20the%20different%20geometry%20between%20hyperbolic%20space%20and%0Athe%20Euclidean%20space.%20To%20address%20such%20difficulties%2C%20we%20first%20categorize%20three%0Akinds%20of%20illness%20that%20harm%20the%20performance%20of%20the%20embeddings.%20Then%2C%20we%20develop%0Aa%20geometry-aware%20algorithm%20using%20a%20dilation%20operation%20and%20a%20transitive%20closure%0Aregularization%20to%20tackle%20these%20illnesses.%20We%20empirically%20validate%20these%0Atechniques%20and%20present%20a%20theoretical%20analysis%20of%20the%20mechanism%20behind%20the%0Adilation%20operation.%20Experiments%20on%20synthetic%20and%20real-world%20datasets%20reveal%0Asuperior%20performances%20of%20our%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Geometry-Aware%2520Algorithm%2520to%2520Learn%2520Hierarchical%2520Embeddings%2520in%250A%2520%2520Hyperbolic%2520Space%26entry.906535625%3DZhangyu%2520Wang%2520and%2520Lantian%2520Xu%2520and%2520Zhifeng%2520Kong%2520and%2520Weilong%2520Wang%2520and%2520Xuyu%2520Peng%2520and%2520Enyang%2520Zheng%26entry.1292438233%3D%2520%2520Hyperbolic%2520embeddings%2520are%2520a%2520class%2520of%2520representation%2520learning%2520methods%2520that%250Aoffer%2520competitive%2520performances%2520when%2520data%2520can%2520be%2520abstracted%2520as%2520a%2520tree-like%250Agraph.%2520However%252C%2520in%2520practice%252C%2520learning%2520hyperbolic%2520embeddings%2520of%2520hierarchical%250Adata%2520is%2520difficult%2520due%2520to%2520the%2520different%2520geometry%2520between%2520hyperbolic%2520space%2520and%250Athe%2520Euclidean%2520space.%2520To%2520address%2520such%2520difficulties%252C%2520we%2520first%2520categorize%2520three%250Akinds%2520of%2520illness%2520that%2520harm%2520the%2520performance%2520of%2520the%2520embeddings.%2520Then%252C%2520we%2520develop%250Aa%2520geometry-aware%2520algorithm%2520using%2520a%2520dilation%2520operation%2520and%2520a%2520transitive%2520closure%250Aregularization%2520to%2520tackle%2520these%2520illnesses.%2520We%2520empirically%2520validate%2520these%250Atechniques%2520and%2520present%2520a%2520theoretical%2520analysis%2520of%2520the%2520mechanism%2520behind%2520the%250Adilation%2520operation.%2520Experiments%2520on%2520synthetic%2520and%2520real-world%2520datasets%2520reveal%250Asuperior%2520performances%2520of%2520our%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Geometry-Aware%20Algorithm%20to%20Learn%20Hierarchical%20Embeddings%20in%0A%20%20Hyperbolic%20Space&entry.906535625=Zhangyu%20Wang%20and%20Lantian%20Xu%20and%20Zhifeng%20Kong%20and%20Weilong%20Wang%20and%20Xuyu%20Peng%20and%20Enyang%20Zheng&entry.1292438233=%20%20Hyperbolic%20embeddings%20are%20a%20class%20of%20representation%20learning%20methods%20that%0Aoffer%20competitive%20performances%20when%20data%20can%20be%20abstracted%20as%20a%20tree-like%0Agraph.%20However%2C%20in%20practice%2C%20learning%20hyperbolic%20embeddings%20of%20hierarchical%0Adata%20is%20difficult%20due%20to%20the%20different%20geometry%20between%20hyperbolic%20space%20and%0Athe%20Euclidean%20space.%20To%20address%20such%20difficulties%2C%20we%20first%20categorize%20three%0Akinds%20of%20illness%20that%20harm%20the%20performance%20of%20the%20embeddings.%20Then%2C%20we%20develop%0Aa%20geometry-aware%20algorithm%20using%20a%20dilation%20operation%20and%20a%20transitive%20closure%0Aregularization%20to%20tackle%20these%20illnesses.%20We%20empirically%20validate%20these%0Atechniques%20and%20present%20a%20theoretical%20analysis%20of%20the%20mechanism%20behind%20the%0Adilation%20operation.%20Experiments%20on%20synthetic%20and%20real-world%20datasets%20reveal%0Asuperior%20performances%20of%20our%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16641v1&entry.124074799=Read"},
{"title": "AbdomenAtlas: A Large-Scale, Detailed-Annotated, & Multi-Center Dataset\n  for Efficient Transfer Learning and Open Algorithmic Benchmarking", "author": "Wenxuan Li and Chongyu Qu and Xiaoxi Chen and Pedro R. A. S. Bassi and Yijia Shi and Yuxiang Lai and Qian Yu and Huimin Xue and Yixiong Chen and Xiaorui Lin and Yutong Tang and Yining Cao and Haoqi Han and Zheyuan Zhang and Jiawei Liu and Tiezheng Zhang and Yujiu Ma and Jincheng Wang and Guang Zhang and Alan Yuille and Zongwei Zhou", "abstract": "  We introduce the largest abdominal CT dataset (termed AbdomenAtlas) of 20,460\nthree-dimensional CT volumes sourced from 112 hospitals across diverse\npopulations, geographies, and facilities. AbdomenAtlas provides 673K\nhigh-quality masks of anatomical structures in the abdominal region annotated\nby a team of 10 radiologists with the help of AI algorithms. We start by having\nexpert radiologists manually annotate 22 anatomical structures in 5,246 CT\nvolumes. Following this, a semi-automatic annotation procedure is performed for\nthe remaining CT volumes, where radiologists revise the annotations predicted\nby AI, and in turn, AI improves its predictions by learning from revised\nannotations. Such a large-scale, detailed-annotated, and multi-center dataset\nis needed for two reasons. Firstly, AbdomenAtlas provides important resources\nfor AI development at scale, branded as large pre-trained models, which can\nalleviate the annotation workload of expert radiologists to transfer to broader\nclinical applications. Secondly, AbdomenAtlas establishes a large-scale\nbenchmark for evaluating AI algorithms -- the more data we use to test the\nalgorithms, the better we can guarantee reliable performance in complex\nclinical scenarios. An ISBI & MICCAI challenge named BodyMaps: Towards 3D Atlas\nof Human Body was launched using a subset of our AbdomenAtlas, aiming to\nstimulate AI innovation and to benchmark segmentation accuracy, inference\nefficiency, and domain generalizability. We hope our AbdomenAtlas can set the\nstage for larger-scale clinical trials and offer exceptional opportunities to\npractitioners in the medical imaging community. Codes, models, and datasets are\navailable at https://www.zongweiz.com/dataset\n", "link": "http://arxiv.org/abs/2407.16697v1", "date": "2024-07-23", "relevancy": 2.4822, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5019}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4937}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AbdomenAtlas%3A%20A%20Large-Scale%2C%20Detailed-Annotated%2C%20%26%20Multi-Center%20Dataset%0A%20%20for%20Efficient%20Transfer%20Learning%20and%20Open%20Algorithmic%20Benchmarking&body=Title%3A%20AbdomenAtlas%3A%20A%20Large-Scale%2C%20Detailed-Annotated%2C%20%26%20Multi-Center%20Dataset%0A%20%20for%20Efficient%20Transfer%20Learning%20and%20Open%20Algorithmic%20Benchmarking%0AAuthor%3A%20Wenxuan%20Li%20and%20Chongyu%20Qu%20and%20Xiaoxi%20Chen%20and%20Pedro%20R.%20A.%20S.%20Bassi%20and%20Yijia%20Shi%20and%20Yuxiang%20Lai%20and%20Qian%20Yu%20and%20Huimin%20Xue%20and%20Yixiong%20Chen%20and%20Xiaorui%20Lin%20and%20Yutong%20Tang%20and%20Yining%20Cao%20and%20Haoqi%20Han%20and%20Zheyuan%20Zhang%20and%20Jiawei%20Liu%20and%20Tiezheng%20Zhang%20and%20Yujiu%20Ma%20and%20Jincheng%20Wang%20and%20Guang%20Zhang%20and%20Alan%20Yuille%20and%20Zongwei%20Zhou%0AAbstract%3A%20%20%20We%20introduce%20the%20largest%20abdominal%20CT%20dataset%20%28termed%20AbdomenAtlas%29%20of%2020%2C460%0Athree-dimensional%20CT%20volumes%20sourced%20from%20112%20hospitals%20across%20diverse%0Apopulations%2C%20geographies%2C%20and%20facilities.%20AbdomenAtlas%20provides%20673K%0Ahigh-quality%20masks%20of%20anatomical%20structures%20in%20the%20abdominal%20region%20annotated%0Aby%20a%20team%20of%2010%20radiologists%20with%20the%20help%20of%20AI%20algorithms.%20We%20start%20by%20having%0Aexpert%20radiologists%20manually%20annotate%2022%20anatomical%20structures%20in%205%2C246%20CT%0Avolumes.%20Following%20this%2C%20a%20semi-automatic%20annotation%20procedure%20is%20performed%20for%0Athe%20remaining%20CT%20volumes%2C%20where%20radiologists%20revise%20the%20annotations%20predicted%0Aby%20AI%2C%20and%20in%20turn%2C%20AI%20improves%20its%20predictions%20by%20learning%20from%20revised%0Aannotations.%20Such%20a%20large-scale%2C%20detailed-annotated%2C%20and%20multi-center%20dataset%0Ais%20needed%20for%20two%20reasons.%20Firstly%2C%20AbdomenAtlas%20provides%20important%20resources%0Afor%20AI%20development%20at%20scale%2C%20branded%20as%20large%20pre-trained%20models%2C%20which%20can%0Aalleviate%20the%20annotation%20workload%20of%20expert%20radiologists%20to%20transfer%20to%20broader%0Aclinical%20applications.%20Secondly%2C%20AbdomenAtlas%20establishes%20a%20large-scale%0Abenchmark%20for%20evaluating%20AI%20algorithms%20--%20the%20more%20data%20we%20use%20to%20test%20the%0Aalgorithms%2C%20the%20better%20we%20can%20guarantee%20reliable%20performance%20in%20complex%0Aclinical%20scenarios.%20An%20ISBI%20%26%20MICCAI%20challenge%20named%20BodyMaps%3A%20Towards%203D%20Atlas%0Aof%20Human%20Body%20was%20launched%20using%20a%20subset%20of%20our%20AbdomenAtlas%2C%20aiming%20to%0Astimulate%20AI%20innovation%20and%20to%20benchmark%20segmentation%20accuracy%2C%20inference%0Aefficiency%2C%20and%20domain%20generalizability.%20We%20hope%20our%20AbdomenAtlas%20can%20set%20the%0Astage%20for%20larger-scale%20clinical%20trials%20and%20offer%20exceptional%20opportunities%20to%0Apractitioners%20in%20the%20medical%20imaging%20community.%20Codes%2C%20models%2C%20and%20datasets%20are%0Aavailable%20at%20https%3A//www.zongweiz.com/dataset%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbdomenAtlas%253A%2520A%2520Large-Scale%252C%2520Detailed-Annotated%252C%2520%2526%2520Multi-Center%2520Dataset%250A%2520%2520for%2520Efficient%2520Transfer%2520Learning%2520and%2520Open%2520Algorithmic%2520Benchmarking%26entry.906535625%3DWenxuan%2520Li%2520and%2520Chongyu%2520Qu%2520and%2520Xiaoxi%2520Chen%2520and%2520Pedro%2520R.%2520A.%2520S.%2520Bassi%2520and%2520Yijia%2520Shi%2520and%2520Yuxiang%2520Lai%2520and%2520Qian%2520Yu%2520and%2520Huimin%2520Xue%2520and%2520Yixiong%2520Chen%2520and%2520Xiaorui%2520Lin%2520and%2520Yutong%2520Tang%2520and%2520Yining%2520Cao%2520and%2520Haoqi%2520Han%2520and%2520Zheyuan%2520Zhang%2520and%2520Jiawei%2520Liu%2520and%2520Tiezheng%2520Zhang%2520and%2520Yujiu%2520Ma%2520and%2520Jincheng%2520Wang%2520and%2520Guang%2520Zhang%2520and%2520Alan%2520Yuille%2520and%2520Zongwei%2520Zhou%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520largest%2520abdominal%2520CT%2520dataset%2520%2528termed%2520AbdomenAtlas%2529%2520of%252020%252C460%250Athree-dimensional%2520CT%2520volumes%2520sourced%2520from%2520112%2520hospitals%2520across%2520diverse%250Apopulations%252C%2520geographies%252C%2520and%2520facilities.%2520AbdomenAtlas%2520provides%2520673K%250Ahigh-quality%2520masks%2520of%2520anatomical%2520structures%2520in%2520the%2520abdominal%2520region%2520annotated%250Aby%2520a%2520team%2520of%252010%2520radiologists%2520with%2520the%2520help%2520of%2520AI%2520algorithms.%2520We%2520start%2520by%2520having%250Aexpert%2520radiologists%2520manually%2520annotate%252022%2520anatomical%2520structures%2520in%25205%252C246%2520CT%250Avolumes.%2520Following%2520this%252C%2520a%2520semi-automatic%2520annotation%2520procedure%2520is%2520performed%2520for%250Athe%2520remaining%2520CT%2520volumes%252C%2520where%2520radiologists%2520revise%2520the%2520annotations%2520predicted%250Aby%2520AI%252C%2520and%2520in%2520turn%252C%2520AI%2520improves%2520its%2520predictions%2520by%2520learning%2520from%2520revised%250Aannotations.%2520Such%2520a%2520large-scale%252C%2520detailed-annotated%252C%2520and%2520multi-center%2520dataset%250Ais%2520needed%2520for%2520two%2520reasons.%2520Firstly%252C%2520AbdomenAtlas%2520provides%2520important%2520resources%250Afor%2520AI%2520development%2520at%2520scale%252C%2520branded%2520as%2520large%2520pre-trained%2520models%252C%2520which%2520can%250Aalleviate%2520the%2520annotation%2520workload%2520of%2520expert%2520radiologists%2520to%2520transfer%2520to%2520broader%250Aclinical%2520applications.%2520Secondly%252C%2520AbdomenAtlas%2520establishes%2520a%2520large-scale%250Abenchmark%2520for%2520evaluating%2520AI%2520algorithms%2520--%2520the%2520more%2520data%2520we%2520use%2520to%2520test%2520the%250Aalgorithms%252C%2520the%2520better%2520we%2520can%2520guarantee%2520reliable%2520performance%2520in%2520complex%250Aclinical%2520scenarios.%2520An%2520ISBI%2520%2526%2520MICCAI%2520challenge%2520named%2520BodyMaps%253A%2520Towards%25203D%2520Atlas%250Aof%2520Human%2520Body%2520was%2520launched%2520using%2520a%2520subset%2520of%2520our%2520AbdomenAtlas%252C%2520aiming%2520to%250Astimulate%2520AI%2520innovation%2520and%2520to%2520benchmark%2520segmentation%2520accuracy%252C%2520inference%250Aefficiency%252C%2520and%2520domain%2520generalizability.%2520We%2520hope%2520our%2520AbdomenAtlas%2520can%2520set%2520the%250Astage%2520for%2520larger-scale%2520clinical%2520trials%2520and%2520offer%2520exceptional%2520opportunities%2520to%250Apractitioners%2520in%2520the%2520medical%2520imaging%2520community.%2520Codes%252C%2520models%252C%2520and%2520datasets%2520are%250Aavailable%2520at%2520https%253A//www.zongweiz.com/dataset%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AbdomenAtlas%3A%20A%20Large-Scale%2C%20Detailed-Annotated%2C%20%26%20Multi-Center%20Dataset%0A%20%20for%20Efficient%20Transfer%20Learning%20and%20Open%20Algorithmic%20Benchmarking&entry.906535625=Wenxuan%20Li%20and%20Chongyu%20Qu%20and%20Xiaoxi%20Chen%20and%20Pedro%20R.%20A.%20S.%20Bassi%20and%20Yijia%20Shi%20and%20Yuxiang%20Lai%20and%20Qian%20Yu%20and%20Huimin%20Xue%20and%20Yixiong%20Chen%20and%20Xiaorui%20Lin%20and%20Yutong%20Tang%20and%20Yining%20Cao%20and%20Haoqi%20Han%20and%20Zheyuan%20Zhang%20and%20Jiawei%20Liu%20and%20Tiezheng%20Zhang%20and%20Yujiu%20Ma%20and%20Jincheng%20Wang%20and%20Guang%20Zhang%20and%20Alan%20Yuille%20and%20Zongwei%20Zhou&entry.1292438233=%20%20We%20introduce%20the%20largest%20abdominal%20CT%20dataset%20%28termed%20AbdomenAtlas%29%20of%2020%2C460%0Athree-dimensional%20CT%20volumes%20sourced%20from%20112%20hospitals%20across%20diverse%0Apopulations%2C%20geographies%2C%20and%20facilities.%20AbdomenAtlas%20provides%20673K%0Ahigh-quality%20masks%20of%20anatomical%20structures%20in%20the%20abdominal%20region%20annotated%0Aby%20a%20team%20of%2010%20radiologists%20with%20the%20help%20of%20AI%20algorithms.%20We%20start%20by%20having%0Aexpert%20radiologists%20manually%20annotate%2022%20anatomical%20structures%20in%205%2C246%20CT%0Avolumes.%20Following%20this%2C%20a%20semi-automatic%20annotation%20procedure%20is%20performed%20for%0Athe%20remaining%20CT%20volumes%2C%20where%20radiologists%20revise%20the%20annotations%20predicted%0Aby%20AI%2C%20and%20in%20turn%2C%20AI%20improves%20its%20predictions%20by%20learning%20from%20revised%0Aannotations.%20Such%20a%20large-scale%2C%20detailed-annotated%2C%20and%20multi-center%20dataset%0Ais%20needed%20for%20two%20reasons.%20Firstly%2C%20AbdomenAtlas%20provides%20important%20resources%0Afor%20AI%20development%20at%20scale%2C%20branded%20as%20large%20pre-trained%20models%2C%20which%20can%0Aalleviate%20the%20annotation%20workload%20of%20expert%20radiologists%20to%20transfer%20to%20broader%0Aclinical%20applications.%20Secondly%2C%20AbdomenAtlas%20establishes%20a%20large-scale%0Abenchmark%20for%20evaluating%20AI%20algorithms%20--%20the%20more%20data%20we%20use%20to%20test%20the%0Aalgorithms%2C%20the%20better%20we%20can%20guarantee%20reliable%20performance%20in%20complex%0Aclinical%20scenarios.%20An%20ISBI%20%26%20MICCAI%20challenge%20named%20BodyMaps%3A%20Towards%203D%20Atlas%0Aof%20Human%20Body%20was%20launched%20using%20a%20subset%20of%20our%20AbdomenAtlas%2C%20aiming%20to%0Astimulate%20AI%20innovation%20and%20to%20benchmark%20segmentation%20accuracy%2C%20inference%0Aefficiency%2C%20and%20domain%20generalizability.%20We%20hope%20our%20AbdomenAtlas%20can%20set%20the%0Astage%20for%20larger-scale%20clinical%20trials%20and%20offer%20exceptional%20opportunities%20to%0Apractitioners%20in%20the%20medical%20imaging%20community.%20Codes%2C%20models%2C%20and%20datasets%20are%0Aavailable%20at%20https%3A//www.zongweiz.com/dataset%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16697v1&entry.124074799=Read"},
{"title": "Enhancing GNNs Performance on Combinatorial Optimization by Recurrent\n  Feature Update", "author": "Daria Pugacheva and Andrei Ermakov and Igor Lyskov and Ilya Makarov and Yuriy Zotov", "abstract": "  Combinatorial optimization (CO) problems are crucial in various scientific\nand industrial applications. Recently, researchers have proposed using\nunsupervised Graph Neural Networks (GNNs) to address NP-hard combinatorial\noptimization problems, which can be reformulated as Quadratic Unconstrained\nBinary Optimization (QUBO) problems. GNNs have demonstrated high performance\nwith nearly linear scalability and significantly outperformed classic\nheuristic-based algorithms in terms of computational efficiency on large-scale\nproblems. However, when utilizing standard node features, GNNs tend to get\ntrapped to suboptimal local minima of the energy landscape, resulting in low\nquality solutions. We introduce a novel algorithm, denoted hereafter as\nQRF-GNN, leveraging the power of GNNs to efficiently solve CO problems with\nQUBO formulation. It relies on unsupervised learning by minimizing the loss\nfunction derived from QUBO relaxation. The proposed key components of the\narchitecture include the recurrent use of intermediate GNN predictions,\nparallel convolutional layers and combination of static node features as input.\nAltogether, it helps to adapt the intermediate solution candidate to minimize\nQUBO-based loss function, taking into account not only static graph features,\nbut also intermediate predictions treated as dynamic, i.e. iteratively changing\nrecurrent features. The performance of the proposed algorithm has been\nevaluated on the canonical benchmark datasets for maximum cut, graph coloring\nand maximum independent set problems. Results of experiments show that QRF-GNN\ndrastically surpasses existing learning-based approaches and is comparable to\nthe state-of-the-art conventional heuristics, improving their scalability on\nlarge instances.\n", "link": "http://arxiv.org/abs/2407.16468v1", "date": "2024-07-23", "relevancy": 2.481, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5121}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5023}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20GNNs%20Performance%20on%20Combinatorial%20Optimization%20by%20Recurrent%0A%20%20Feature%20Update&body=Title%3A%20Enhancing%20GNNs%20Performance%20on%20Combinatorial%20Optimization%20by%20Recurrent%0A%20%20Feature%20Update%0AAuthor%3A%20Daria%20Pugacheva%20and%20Andrei%20Ermakov%20and%20Igor%20Lyskov%20and%20Ilya%20Makarov%20and%20Yuriy%20Zotov%0AAbstract%3A%20%20%20Combinatorial%20optimization%20%28CO%29%20problems%20are%20crucial%20in%20various%20scientific%0Aand%20industrial%20applications.%20Recently%2C%20researchers%20have%20proposed%20using%0Aunsupervised%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20address%20NP-hard%20combinatorial%0Aoptimization%20problems%2C%20which%20can%20be%20reformulated%20as%20Quadratic%20Unconstrained%0ABinary%20Optimization%20%28QUBO%29%20problems.%20GNNs%20have%20demonstrated%20high%20performance%0Awith%20nearly%20linear%20scalability%20and%20significantly%20outperformed%20classic%0Aheuristic-based%20algorithms%20in%20terms%20of%20computational%20efficiency%20on%20large-scale%0Aproblems.%20However%2C%20when%20utilizing%20standard%20node%20features%2C%20GNNs%20tend%20to%20get%0Atrapped%20to%20suboptimal%20local%20minima%20of%20the%20energy%20landscape%2C%20resulting%20in%20low%0Aquality%20solutions.%20We%20introduce%20a%20novel%20algorithm%2C%20denoted%20hereafter%20as%0AQRF-GNN%2C%20leveraging%20the%20power%20of%20GNNs%20to%20efficiently%20solve%20CO%20problems%20with%0AQUBO%20formulation.%20It%20relies%20on%20unsupervised%20learning%20by%20minimizing%20the%20loss%0Afunction%20derived%20from%20QUBO%20relaxation.%20The%20proposed%20key%20components%20of%20the%0Aarchitecture%20include%20the%20recurrent%20use%20of%20intermediate%20GNN%20predictions%2C%0Aparallel%20convolutional%20layers%20and%20combination%20of%20static%20node%20features%20as%20input.%0AAltogether%2C%20it%20helps%20to%20adapt%20the%20intermediate%20solution%20candidate%20to%20minimize%0AQUBO-based%20loss%20function%2C%20taking%20into%20account%20not%20only%20static%20graph%20features%2C%0Abut%20also%20intermediate%20predictions%20treated%20as%20dynamic%2C%20i.e.%20iteratively%20changing%0Arecurrent%20features.%20The%20performance%20of%20the%20proposed%20algorithm%20has%20been%0Aevaluated%20on%20the%20canonical%20benchmark%20datasets%20for%20maximum%20cut%2C%20graph%20coloring%0Aand%20maximum%20independent%20set%20problems.%20Results%20of%20experiments%20show%20that%20QRF-GNN%0Adrastically%20surpasses%20existing%20learning-based%20approaches%20and%20is%20comparable%20to%0Athe%20state-of-the-art%20conventional%20heuristics%2C%20improving%20their%20scalability%20on%0Alarge%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520GNNs%2520Performance%2520on%2520Combinatorial%2520Optimization%2520by%2520Recurrent%250A%2520%2520Feature%2520Update%26entry.906535625%3DDaria%2520Pugacheva%2520and%2520Andrei%2520Ermakov%2520and%2520Igor%2520Lyskov%2520and%2520Ilya%2520Makarov%2520and%2520Yuriy%2520Zotov%26entry.1292438233%3D%2520%2520Combinatorial%2520optimization%2520%2528CO%2529%2520problems%2520are%2520crucial%2520in%2520various%2520scientific%250Aand%2520industrial%2520applications.%2520Recently%252C%2520researchers%2520have%2520proposed%2520using%250Aunsupervised%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520to%2520address%2520NP-hard%2520combinatorial%250Aoptimization%2520problems%252C%2520which%2520can%2520be%2520reformulated%2520as%2520Quadratic%2520Unconstrained%250ABinary%2520Optimization%2520%2528QUBO%2529%2520problems.%2520GNNs%2520have%2520demonstrated%2520high%2520performance%250Awith%2520nearly%2520linear%2520scalability%2520and%2520significantly%2520outperformed%2520classic%250Aheuristic-based%2520algorithms%2520in%2520terms%2520of%2520computational%2520efficiency%2520on%2520large-scale%250Aproblems.%2520However%252C%2520when%2520utilizing%2520standard%2520node%2520features%252C%2520GNNs%2520tend%2520to%2520get%250Atrapped%2520to%2520suboptimal%2520local%2520minima%2520of%2520the%2520energy%2520landscape%252C%2520resulting%2520in%2520low%250Aquality%2520solutions.%2520We%2520introduce%2520a%2520novel%2520algorithm%252C%2520denoted%2520hereafter%2520as%250AQRF-GNN%252C%2520leveraging%2520the%2520power%2520of%2520GNNs%2520to%2520efficiently%2520solve%2520CO%2520problems%2520with%250AQUBO%2520formulation.%2520It%2520relies%2520on%2520unsupervised%2520learning%2520by%2520minimizing%2520the%2520loss%250Afunction%2520derived%2520from%2520QUBO%2520relaxation.%2520The%2520proposed%2520key%2520components%2520of%2520the%250Aarchitecture%2520include%2520the%2520recurrent%2520use%2520of%2520intermediate%2520GNN%2520predictions%252C%250Aparallel%2520convolutional%2520layers%2520and%2520combination%2520of%2520static%2520node%2520features%2520as%2520input.%250AAltogether%252C%2520it%2520helps%2520to%2520adapt%2520the%2520intermediate%2520solution%2520candidate%2520to%2520minimize%250AQUBO-based%2520loss%2520function%252C%2520taking%2520into%2520account%2520not%2520only%2520static%2520graph%2520features%252C%250Abut%2520also%2520intermediate%2520predictions%2520treated%2520as%2520dynamic%252C%2520i.e.%2520iteratively%2520changing%250Arecurrent%2520features.%2520The%2520performance%2520of%2520the%2520proposed%2520algorithm%2520has%2520been%250Aevaluated%2520on%2520the%2520canonical%2520benchmark%2520datasets%2520for%2520maximum%2520cut%252C%2520graph%2520coloring%250Aand%2520maximum%2520independent%2520set%2520problems.%2520Results%2520of%2520experiments%2520show%2520that%2520QRF-GNN%250Adrastically%2520surpasses%2520existing%2520learning-based%2520approaches%2520and%2520is%2520comparable%2520to%250Athe%2520state-of-the-art%2520conventional%2520heuristics%252C%2520improving%2520their%2520scalability%2520on%250Alarge%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20GNNs%20Performance%20on%20Combinatorial%20Optimization%20by%20Recurrent%0A%20%20Feature%20Update&entry.906535625=Daria%20Pugacheva%20and%20Andrei%20Ermakov%20and%20Igor%20Lyskov%20and%20Ilya%20Makarov%20and%20Yuriy%20Zotov&entry.1292438233=%20%20Combinatorial%20optimization%20%28CO%29%20problems%20are%20crucial%20in%20various%20scientific%0Aand%20industrial%20applications.%20Recently%2C%20researchers%20have%20proposed%20using%0Aunsupervised%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20address%20NP-hard%20combinatorial%0Aoptimization%20problems%2C%20which%20can%20be%20reformulated%20as%20Quadratic%20Unconstrained%0ABinary%20Optimization%20%28QUBO%29%20problems.%20GNNs%20have%20demonstrated%20high%20performance%0Awith%20nearly%20linear%20scalability%20and%20significantly%20outperformed%20classic%0Aheuristic-based%20algorithms%20in%20terms%20of%20computational%20efficiency%20on%20large-scale%0Aproblems.%20However%2C%20when%20utilizing%20standard%20node%20features%2C%20GNNs%20tend%20to%20get%0Atrapped%20to%20suboptimal%20local%20minima%20of%20the%20energy%20landscape%2C%20resulting%20in%20low%0Aquality%20solutions.%20We%20introduce%20a%20novel%20algorithm%2C%20denoted%20hereafter%20as%0AQRF-GNN%2C%20leveraging%20the%20power%20of%20GNNs%20to%20efficiently%20solve%20CO%20problems%20with%0AQUBO%20formulation.%20It%20relies%20on%20unsupervised%20learning%20by%20minimizing%20the%20loss%0Afunction%20derived%20from%20QUBO%20relaxation.%20The%20proposed%20key%20components%20of%20the%0Aarchitecture%20include%20the%20recurrent%20use%20of%20intermediate%20GNN%20predictions%2C%0Aparallel%20convolutional%20layers%20and%20combination%20of%20static%20node%20features%20as%20input.%0AAltogether%2C%20it%20helps%20to%20adapt%20the%20intermediate%20solution%20candidate%20to%20minimize%0AQUBO-based%20loss%20function%2C%20taking%20into%20account%20not%20only%20static%20graph%20features%2C%0Abut%20also%20intermediate%20predictions%20treated%20as%20dynamic%2C%20i.e.%20iteratively%20changing%0Arecurrent%20features.%20The%20performance%20of%20the%20proposed%20algorithm%20has%20been%0Aevaluated%20on%20the%20canonical%20benchmark%20datasets%20for%20maximum%20cut%2C%20graph%20coloring%0Aand%20maximum%20independent%20set%20problems.%20Results%20of%20experiments%20show%20that%20QRF-GNN%0Adrastically%20surpasses%20existing%20learning-based%20approaches%20and%20is%20comparable%20to%0Athe%20state-of-the-art%20conventional%20heuristics%2C%20improving%20their%20scalability%20on%0Alarge%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16468v1&entry.124074799=Read"},
{"title": "On ADMM in Heterogeneous Federated Learning: Personalization,\n  Robustness, and Fairness", "author": "Shengkun Zhu and Jinshan Zeng and Sheng Wang and Yuan Sun and Xiaodong Li and Yuan Yao and Zhiyong Peng", "abstract": "  Statistical heterogeneity is a root cause of tension among accuracy,\nfairness, and robustness of federated learning (FL), and is key in paving a\npath forward. Personalized FL (PFL) is an approach that aims to reduce the\nimpact of statistical heterogeneity by developing personalized models for\nindividual users, while also inherently providing benefits in terms of fairness\nand robustness. However, existing PFL frameworks focus on improving the\nperformance of personalized models while neglecting the global model. Moreover,\nthese frameworks achieve sublinear convergence rates and rely on strong\nassumptions. In this paper, we propose FLAME, an optimization framework by\nutilizing the alternating direction method of multipliers (ADMM) to train\npersonalized and global models. We propose a model selection strategy to\nimprove performance in situations where clients have different types of\nheterogeneous data. Our theoretical analysis establishes the global convergence\nand two kinds of convergence rates for FLAME under mild assumptions. We\ntheoretically demonstrate that FLAME is more robust and fair than the\nstate-of-the-art methods on a class of linear problems. Our experimental\nfindings show that FLAME outperforms state-of-the-art methods in convergence\nand accuracy, and it achieves higher test accuracy under various attacks and\nperforms more uniformly across clients.\n", "link": "http://arxiv.org/abs/2407.16397v1", "date": "2024-07-23", "relevancy": 2.4538, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5014}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4893}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20ADMM%20in%20Heterogeneous%20Federated%20Learning%3A%20Personalization%2C%0A%20%20Robustness%2C%20and%20Fairness&body=Title%3A%20On%20ADMM%20in%20Heterogeneous%20Federated%20Learning%3A%20Personalization%2C%0A%20%20Robustness%2C%20and%20Fairness%0AAuthor%3A%20Shengkun%20Zhu%20and%20Jinshan%20Zeng%20and%20Sheng%20Wang%20and%20Yuan%20Sun%20and%20Xiaodong%20Li%20and%20Yuan%20Yao%20and%20Zhiyong%20Peng%0AAbstract%3A%20%20%20Statistical%20heterogeneity%20is%20a%20root%20cause%20of%20tension%20among%20accuracy%2C%0Afairness%2C%20and%20robustness%20of%20federated%20learning%20%28FL%29%2C%20and%20is%20key%20in%20paving%20a%0Apath%20forward.%20Personalized%20FL%20%28PFL%29%20is%20an%20approach%20that%20aims%20to%20reduce%20the%0Aimpact%20of%20statistical%20heterogeneity%20by%20developing%20personalized%20models%20for%0Aindividual%20users%2C%20while%20also%20inherently%20providing%20benefits%20in%20terms%20of%20fairness%0Aand%20robustness.%20However%2C%20existing%20PFL%20frameworks%20focus%20on%20improving%20the%0Aperformance%20of%20personalized%20models%20while%20neglecting%20the%20global%20model.%20Moreover%2C%0Athese%20frameworks%20achieve%20sublinear%20convergence%20rates%20and%20rely%20on%20strong%0Aassumptions.%20In%20this%20paper%2C%20we%20propose%20FLAME%2C%20an%20optimization%20framework%20by%0Autilizing%20the%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20to%20train%0Apersonalized%20and%20global%20models.%20We%20propose%20a%20model%20selection%20strategy%20to%0Aimprove%20performance%20in%20situations%20where%20clients%20have%20different%20types%20of%0Aheterogeneous%20data.%20Our%20theoretical%20analysis%20establishes%20the%20global%20convergence%0Aand%20two%20kinds%20of%20convergence%20rates%20for%20FLAME%20under%20mild%20assumptions.%20We%0Atheoretically%20demonstrate%20that%20FLAME%20is%20more%20robust%20and%20fair%20than%20the%0Astate-of-the-art%20methods%20on%20a%20class%20of%20linear%20problems.%20Our%20experimental%0Afindings%20show%20that%20FLAME%20outperforms%20state-of-the-art%20methods%20in%20convergence%0Aand%20accuracy%2C%20and%20it%20achieves%20higher%20test%20accuracy%20under%20various%20attacks%20and%0Aperforms%20more%20uniformly%20across%20clients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520ADMM%2520in%2520Heterogeneous%2520Federated%2520Learning%253A%2520Personalization%252C%250A%2520%2520Robustness%252C%2520and%2520Fairness%26entry.906535625%3DShengkun%2520Zhu%2520and%2520Jinshan%2520Zeng%2520and%2520Sheng%2520Wang%2520and%2520Yuan%2520Sun%2520and%2520Xiaodong%2520Li%2520and%2520Yuan%2520Yao%2520and%2520Zhiyong%2520Peng%26entry.1292438233%3D%2520%2520Statistical%2520heterogeneity%2520is%2520a%2520root%2520cause%2520of%2520tension%2520among%2520accuracy%252C%250Afairness%252C%2520and%2520robustness%2520of%2520federated%2520learning%2520%2528FL%2529%252C%2520and%2520is%2520key%2520in%2520paving%2520a%250Apath%2520forward.%2520Personalized%2520FL%2520%2528PFL%2529%2520is%2520an%2520approach%2520that%2520aims%2520to%2520reduce%2520the%250Aimpact%2520of%2520statistical%2520heterogeneity%2520by%2520developing%2520personalized%2520models%2520for%250Aindividual%2520users%252C%2520while%2520also%2520inherently%2520providing%2520benefits%2520in%2520terms%2520of%2520fairness%250Aand%2520robustness.%2520However%252C%2520existing%2520PFL%2520frameworks%2520focus%2520on%2520improving%2520the%250Aperformance%2520of%2520personalized%2520models%2520while%2520neglecting%2520the%2520global%2520model.%2520Moreover%252C%250Athese%2520frameworks%2520achieve%2520sublinear%2520convergence%2520rates%2520and%2520rely%2520on%2520strong%250Aassumptions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520FLAME%252C%2520an%2520optimization%2520framework%2520by%250Autilizing%2520the%2520alternating%2520direction%2520method%2520of%2520multipliers%2520%2528ADMM%2529%2520to%2520train%250Apersonalized%2520and%2520global%2520models.%2520We%2520propose%2520a%2520model%2520selection%2520strategy%2520to%250Aimprove%2520performance%2520in%2520situations%2520where%2520clients%2520have%2520different%2520types%2520of%250Aheterogeneous%2520data.%2520Our%2520theoretical%2520analysis%2520establishes%2520the%2520global%2520convergence%250Aand%2520two%2520kinds%2520of%2520convergence%2520rates%2520for%2520FLAME%2520under%2520mild%2520assumptions.%2520We%250Atheoretically%2520demonstrate%2520that%2520FLAME%2520is%2520more%2520robust%2520and%2520fair%2520than%2520the%250Astate-of-the-art%2520methods%2520on%2520a%2520class%2520of%2520linear%2520problems.%2520Our%2520experimental%250Afindings%2520show%2520that%2520FLAME%2520outperforms%2520state-of-the-art%2520methods%2520in%2520convergence%250Aand%2520accuracy%252C%2520and%2520it%2520achieves%2520higher%2520test%2520accuracy%2520under%2520various%2520attacks%2520and%250Aperforms%2520more%2520uniformly%2520across%2520clients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20ADMM%20in%20Heterogeneous%20Federated%20Learning%3A%20Personalization%2C%0A%20%20Robustness%2C%20and%20Fairness&entry.906535625=Shengkun%20Zhu%20and%20Jinshan%20Zeng%20and%20Sheng%20Wang%20and%20Yuan%20Sun%20and%20Xiaodong%20Li%20and%20Yuan%20Yao%20and%20Zhiyong%20Peng&entry.1292438233=%20%20Statistical%20heterogeneity%20is%20a%20root%20cause%20of%20tension%20among%20accuracy%2C%0Afairness%2C%20and%20robustness%20of%20federated%20learning%20%28FL%29%2C%20and%20is%20key%20in%20paving%20a%0Apath%20forward.%20Personalized%20FL%20%28PFL%29%20is%20an%20approach%20that%20aims%20to%20reduce%20the%0Aimpact%20of%20statistical%20heterogeneity%20by%20developing%20personalized%20models%20for%0Aindividual%20users%2C%20while%20also%20inherently%20providing%20benefits%20in%20terms%20of%20fairness%0Aand%20robustness.%20However%2C%20existing%20PFL%20frameworks%20focus%20on%20improving%20the%0Aperformance%20of%20personalized%20models%20while%20neglecting%20the%20global%20model.%20Moreover%2C%0Athese%20frameworks%20achieve%20sublinear%20convergence%20rates%20and%20rely%20on%20strong%0Aassumptions.%20In%20this%20paper%2C%20we%20propose%20FLAME%2C%20an%20optimization%20framework%20by%0Autilizing%20the%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20to%20train%0Apersonalized%20and%20global%20models.%20We%20propose%20a%20model%20selection%20strategy%20to%0Aimprove%20performance%20in%20situations%20where%20clients%20have%20different%20types%20of%0Aheterogeneous%20data.%20Our%20theoretical%20analysis%20establishes%20the%20global%20convergence%0Aand%20two%20kinds%20of%20convergence%20rates%20for%20FLAME%20under%20mild%20assumptions.%20We%0Atheoretically%20demonstrate%20that%20FLAME%20is%20more%20robust%20and%20fair%20than%20the%0Astate-of-the-art%20methods%20on%20a%20class%20of%20linear%20problems.%20Our%20experimental%0Afindings%20show%20that%20FLAME%20outperforms%20state-of-the-art%20methods%20in%20convergence%0Aand%20accuracy%2C%20and%20it%20achieves%20higher%20test%20accuracy%20under%20various%20attacks%20and%0Aperforms%20more%20uniformly%20across%20clients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16397v1&entry.124074799=Read"},
{"title": "Local vs Global continual learning", "author": "Giulia Lanzillotta and Sidak Pal Singh and Benjamin F. Grewe and Thomas Hofmann", "abstract": "  Continual learning is the problem of integrating new information in a model\nwhile retaining the knowledge acquired in the past. Despite the tangible\nimprovements achieved in recent years, the problem of continual learning is\nstill an open one. A better understanding of the mechanisms behind the\nsuccesses and failures of existing continual learning algorithms can unlock the\ndevelopment of new successful strategies. In this work, we view continual\nlearning from the perspective of the multi-task loss approximation, and we\ncompare two alternative strategies, namely local and global approximations. We\nclassify existing continual learning algorithms based on the approximation\nused, and we assess the practical effects of this distinction in common\ncontinual learning settings.Additionally, we study optimal continual learning\nobjectives in the case of local polynomial approximations and we provide\nexamples of existing algorithms implementing the optimal objectives\n", "link": "http://arxiv.org/abs/2407.16611v1", "date": "2024-07-23", "relevancy": 2.4395, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5117}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4949}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20vs%20Global%20continual%20learning&body=Title%3A%20Local%20vs%20Global%20continual%20learning%0AAuthor%3A%20Giulia%20Lanzillotta%20and%20Sidak%20Pal%20Singh%20and%20Benjamin%20F.%20Grewe%20and%20Thomas%20Hofmann%0AAbstract%3A%20%20%20Continual%20learning%20is%20the%20problem%20of%20integrating%20new%20information%20in%20a%20model%0Awhile%20retaining%20the%20knowledge%20acquired%20in%20the%20past.%20Despite%20the%20tangible%0Aimprovements%20achieved%20in%20recent%20years%2C%20the%20problem%20of%20continual%20learning%20is%0Astill%20an%20open%20one.%20A%20better%20understanding%20of%20the%20mechanisms%20behind%20the%0Asuccesses%20and%20failures%20of%20existing%20continual%20learning%20algorithms%20can%20unlock%20the%0Adevelopment%20of%20new%20successful%20strategies.%20In%20this%20work%2C%20we%20view%20continual%0Alearning%20from%20the%20perspective%20of%20the%20multi-task%20loss%20approximation%2C%20and%20we%0Acompare%20two%20alternative%20strategies%2C%20namely%20local%20and%20global%20approximations.%20We%0Aclassify%20existing%20continual%20learning%20algorithms%20based%20on%20the%20approximation%0Aused%2C%20and%20we%20assess%20the%20practical%20effects%20of%20this%20distinction%20in%20common%0Acontinual%20learning%20settings.Additionally%2C%20we%20study%20optimal%20continual%20learning%0Aobjectives%20in%20the%20case%20of%20local%20polynomial%20approximations%20and%20we%20provide%0Aexamples%20of%20existing%20algorithms%20implementing%20the%20optimal%20objectives%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520vs%2520Global%2520continual%2520learning%26entry.906535625%3DGiulia%2520Lanzillotta%2520and%2520Sidak%2520Pal%2520Singh%2520and%2520Benjamin%2520F.%2520Grewe%2520and%2520Thomas%2520Hofmann%26entry.1292438233%3D%2520%2520Continual%2520learning%2520is%2520the%2520problem%2520of%2520integrating%2520new%2520information%2520in%2520a%2520model%250Awhile%2520retaining%2520the%2520knowledge%2520acquired%2520in%2520the%2520past.%2520Despite%2520the%2520tangible%250Aimprovements%2520achieved%2520in%2520recent%2520years%252C%2520the%2520problem%2520of%2520continual%2520learning%2520is%250Astill%2520an%2520open%2520one.%2520A%2520better%2520understanding%2520of%2520the%2520mechanisms%2520behind%2520the%250Asuccesses%2520and%2520failures%2520of%2520existing%2520continual%2520learning%2520algorithms%2520can%2520unlock%2520the%250Adevelopment%2520of%2520new%2520successful%2520strategies.%2520In%2520this%2520work%252C%2520we%2520view%2520continual%250Alearning%2520from%2520the%2520perspective%2520of%2520the%2520multi-task%2520loss%2520approximation%252C%2520and%2520we%250Acompare%2520two%2520alternative%2520strategies%252C%2520namely%2520local%2520and%2520global%2520approximations.%2520We%250Aclassify%2520existing%2520continual%2520learning%2520algorithms%2520based%2520on%2520the%2520approximation%250Aused%252C%2520and%2520we%2520assess%2520the%2520practical%2520effects%2520of%2520this%2520distinction%2520in%2520common%250Acontinual%2520learning%2520settings.Additionally%252C%2520we%2520study%2520optimal%2520continual%2520learning%250Aobjectives%2520in%2520the%2520case%2520of%2520local%2520polynomial%2520approximations%2520and%2520we%2520provide%250Aexamples%2520of%2520existing%2520algorithms%2520implementing%2520the%2520optimal%2520objectives%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20vs%20Global%20continual%20learning&entry.906535625=Giulia%20Lanzillotta%20and%20Sidak%20Pal%20Singh%20and%20Benjamin%20F.%20Grewe%20and%20Thomas%20Hofmann&entry.1292438233=%20%20Continual%20learning%20is%20the%20problem%20of%20integrating%20new%20information%20in%20a%20model%0Awhile%20retaining%20the%20knowledge%20acquired%20in%20the%20past.%20Despite%20the%20tangible%0Aimprovements%20achieved%20in%20recent%20years%2C%20the%20problem%20of%20continual%20learning%20is%0Astill%20an%20open%20one.%20A%20better%20understanding%20of%20the%20mechanisms%20behind%20the%0Asuccesses%20and%20failures%20of%20existing%20continual%20learning%20algorithms%20can%20unlock%20the%0Adevelopment%20of%20new%20successful%20strategies.%20In%20this%20work%2C%20we%20view%20continual%0Alearning%20from%20the%20perspective%20of%20the%20multi-task%20loss%20approximation%2C%20and%20we%0Acompare%20two%20alternative%20strategies%2C%20namely%20local%20and%20global%20approximations.%20We%0Aclassify%20existing%20continual%20learning%20algorithms%20based%20on%20the%20approximation%0Aused%2C%20and%20we%20assess%20the%20practical%20effects%20of%20this%20distinction%20in%20common%0Acontinual%20learning%20settings.Additionally%2C%20we%20study%20optimal%20continual%20learning%0Aobjectives%20in%20the%20case%20of%20local%20polynomial%20approximations%20and%20we%20provide%0Aexamples%20of%20existing%20algorithms%20implementing%20the%20optimal%20objectives%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16611v1&entry.124074799=Read"},
{"title": "Data Mixture Inference: What do BPE Tokenizers Reveal about their\n  Training Data?", "author": "Jonathan Hayase and Alisa Liu and Yejin Choi and Sewoong Oh and Noah A. Smith", "abstract": "  The pretraining data of today's strongest language models is opaque. In\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation -- byte-pair encoding (BPE) tokenizers, used by the vast majority\nof modern language models. Our key insight is that the ordered list of merge\nrules learned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data: the first merge is the most common byte pair,\nthe second is the most common pair after merging the first token, and so on.\nGiven a tokenizer's merge list along with data samples for each category of\ninterest, we formulate a linear program that solves for the proportion of each\ncategory in the tokenizer's training set. Importantly, to the extent to which\ntokenizer training data is representative of the pretraining data, we\nindirectly learn about the pretraining data. In controlled experiments, we show\nthat our attack recovers mixture ratios with high precision for tokenizers\ntrained on known mixtures of natural languages, programming languages, and data\nsources. We then apply our approach to off-the-shelf tokenizers released with\nrecent LMs. We confirm much publicly disclosed information about these models,\nand also make several new inferences: GPT-4o's tokenizer is much more\nmultilingual than its predecessors, training on 39% non-English data; Llama3\nextends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and\nClaude's tokenizers are trained on predominantly code (~60%). We hope our work\nsheds light on current design practices for pretraining data, and inspires\ncontinued research into data mixture inference for LMs.\n", "link": "http://arxiv.org/abs/2407.16607v1", "date": "2024-07-23", "relevancy": 2.4337, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4922}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4875}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Mixture%20Inference%3A%20What%20do%20BPE%20Tokenizers%20Reveal%20about%20their%0A%20%20Training%20Data%3F&body=Title%3A%20Data%20Mixture%20Inference%3A%20What%20do%20BPE%20Tokenizers%20Reveal%20about%20their%0A%20%20Training%20Data%3F%0AAuthor%3A%20Jonathan%20Hayase%20and%20Alisa%20Liu%20and%20Yejin%20Choi%20and%20Sewoong%20Oh%20and%20Noah%20A.%20Smith%0AAbstract%3A%20%20%20The%20pretraining%20data%20of%20today%27s%20strongest%20language%20models%20is%20opaque.%20In%0Aparticular%2C%20little%20is%20known%20about%20the%20proportions%20of%20various%20domains%20or%0Alanguages%20represented.%20In%20this%20work%2C%20we%20tackle%20a%20task%20which%20we%20call%20data%0Amixture%20inference%2C%20which%20aims%20to%20uncover%20the%20distributional%20make-up%20of%20training%0Adata.%20We%20introduce%20a%20novel%20attack%20based%20on%20a%20previously%20overlooked%20source%20of%0Ainformation%20--%20byte-pair%20encoding%20%28BPE%29%20tokenizers%2C%20used%20by%20the%20vast%20majority%0Aof%20modern%20language%20models.%20Our%20key%20insight%20is%20that%20the%20ordered%20list%20of%20merge%0Arules%20learned%20by%20a%20BPE%20tokenizer%20naturally%20reveals%20information%20about%20the%20token%0Afrequencies%20in%20its%20training%20data%3A%20the%20first%20merge%20is%20the%20most%20common%20byte%20pair%2C%0Athe%20second%20is%20the%20most%20common%20pair%20after%20merging%20the%20first%20token%2C%20and%20so%20on.%0AGiven%20a%20tokenizer%27s%20merge%20list%20along%20with%20data%20samples%20for%20each%20category%20of%0Ainterest%2C%20we%20formulate%20a%20linear%20program%20that%20solves%20for%20the%20proportion%20of%20each%0Acategory%20in%20the%20tokenizer%27s%20training%20set.%20Importantly%2C%20to%20the%20extent%20to%20which%0Atokenizer%20training%20data%20is%20representative%20of%20the%20pretraining%20data%2C%20we%0Aindirectly%20learn%20about%20the%20pretraining%20data.%20In%20controlled%20experiments%2C%20we%20show%0Athat%20our%20attack%20recovers%20mixture%20ratios%20with%20high%20precision%20for%20tokenizers%0Atrained%20on%20known%20mixtures%20of%20natural%20languages%2C%20programming%20languages%2C%20and%20data%0Asources.%20We%20then%20apply%20our%20approach%20to%20off-the-shelf%20tokenizers%20released%20with%0Arecent%20LMs.%20We%20confirm%20much%20publicly%20disclosed%20information%20about%20these%20models%2C%0Aand%20also%20make%20several%20new%20inferences%3A%20GPT-4o%27s%20tokenizer%20is%20much%20more%0Amultilingual%20than%20its%20predecessors%2C%20training%20on%2039%25%20non-English%20data%3B%20Llama3%0Aextends%20GPT-3.5%27s%20tokenizer%20primarily%20for%20multilingual%20%2848%25%29%20use%3B%20GPT-3.5%27s%20and%0AClaude%27s%20tokenizers%20are%20trained%20on%20predominantly%20code%20%28~60%25%29.%20We%20hope%20our%20work%0Asheds%20light%20on%20current%20design%20practices%20for%20pretraining%20data%2C%20and%20inspires%0Acontinued%20research%20into%20data%20mixture%20inference%20for%20LMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Mixture%2520Inference%253A%2520What%2520do%2520BPE%2520Tokenizers%2520Reveal%2520about%2520their%250A%2520%2520Training%2520Data%253F%26entry.906535625%3DJonathan%2520Hayase%2520and%2520Alisa%2520Liu%2520and%2520Yejin%2520Choi%2520and%2520Sewoong%2520Oh%2520and%2520Noah%2520A.%2520Smith%26entry.1292438233%3D%2520%2520The%2520pretraining%2520data%2520of%2520today%2527s%2520strongest%2520language%2520models%2520is%2520opaque.%2520In%250Aparticular%252C%2520little%2520is%2520known%2520about%2520the%2520proportions%2520of%2520various%2520domains%2520or%250Alanguages%2520represented.%2520In%2520this%2520work%252C%2520we%2520tackle%2520a%2520task%2520which%2520we%2520call%2520data%250Amixture%2520inference%252C%2520which%2520aims%2520to%2520uncover%2520the%2520distributional%2520make-up%2520of%2520training%250Adata.%2520We%2520introduce%2520a%2520novel%2520attack%2520based%2520on%2520a%2520previously%2520overlooked%2520source%2520of%250Ainformation%2520--%2520byte-pair%2520encoding%2520%2528BPE%2529%2520tokenizers%252C%2520used%2520by%2520the%2520vast%2520majority%250Aof%2520modern%2520language%2520models.%2520Our%2520key%2520insight%2520is%2520that%2520the%2520ordered%2520list%2520of%2520merge%250Arules%2520learned%2520by%2520a%2520BPE%2520tokenizer%2520naturally%2520reveals%2520information%2520about%2520the%2520token%250Afrequencies%2520in%2520its%2520training%2520data%253A%2520the%2520first%2520merge%2520is%2520the%2520most%2520common%2520byte%2520pair%252C%250Athe%2520second%2520is%2520the%2520most%2520common%2520pair%2520after%2520merging%2520the%2520first%2520token%252C%2520and%2520so%2520on.%250AGiven%2520a%2520tokenizer%2527s%2520merge%2520list%2520along%2520with%2520data%2520samples%2520for%2520each%2520category%2520of%250Ainterest%252C%2520we%2520formulate%2520a%2520linear%2520program%2520that%2520solves%2520for%2520the%2520proportion%2520of%2520each%250Acategory%2520in%2520the%2520tokenizer%2527s%2520training%2520set.%2520Importantly%252C%2520to%2520the%2520extent%2520to%2520which%250Atokenizer%2520training%2520data%2520is%2520representative%2520of%2520the%2520pretraining%2520data%252C%2520we%250Aindirectly%2520learn%2520about%2520the%2520pretraining%2520data.%2520In%2520controlled%2520experiments%252C%2520we%2520show%250Athat%2520our%2520attack%2520recovers%2520mixture%2520ratios%2520with%2520high%2520precision%2520for%2520tokenizers%250Atrained%2520on%2520known%2520mixtures%2520of%2520natural%2520languages%252C%2520programming%2520languages%252C%2520and%2520data%250Asources.%2520We%2520then%2520apply%2520our%2520approach%2520to%2520off-the-shelf%2520tokenizers%2520released%2520with%250Arecent%2520LMs.%2520We%2520confirm%2520much%2520publicly%2520disclosed%2520information%2520about%2520these%2520models%252C%250Aand%2520also%2520make%2520several%2520new%2520inferences%253A%2520GPT-4o%2527s%2520tokenizer%2520is%2520much%2520more%250Amultilingual%2520than%2520its%2520predecessors%252C%2520training%2520on%252039%2525%2520non-English%2520data%253B%2520Llama3%250Aextends%2520GPT-3.5%2527s%2520tokenizer%2520primarily%2520for%2520multilingual%2520%252848%2525%2529%2520use%253B%2520GPT-3.5%2527s%2520and%250AClaude%2527s%2520tokenizers%2520are%2520trained%2520on%2520predominantly%2520code%2520%2528~60%2525%2529.%2520We%2520hope%2520our%2520work%250Asheds%2520light%2520on%2520current%2520design%2520practices%2520for%2520pretraining%2520data%252C%2520and%2520inspires%250Acontinued%2520research%2520into%2520data%2520mixture%2520inference%2520for%2520LMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Mixture%20Inference%3A%20What%20do%20BPE%20Tokenizers%20Reveal%20about%20their%0A%20%20Training%20Data%3F&entry.906535625=Jonathan%20Hayase%20and%20Alisa%20Liu%20and%20Yejin%20Choi%20and%20Sewoong%20Oh%20and%20Noah%20A.%20Smith&entry.1292438233=%20%20The%20pretraining%20data%20of%20today%27s%20strongest%20language%20models%20is%20opaque.%20In%0Aparticular%2C%20little%20is%20known%20about%20the%20proportions%20of%20various%20domains%20or%0Alanguages%20represented.%20In%20this%20work%2C%20we%20tackle%20a%20task%20which%20we%20call%20data%0Amixture%20inference%2C%20which%20aims%20to%20uncover%20the%20distributional%20make-up%20of%20training%0Adata.%20We%20introduce%20a%20novel%20attack%20based%20on%20a%20previously%20overlooked%20source%20of%0Ainformation%20--%20byte-pair%20encoding%20%28BPE%29%20tokenizers%2C%20used%20by%20the%20vast%20majority%0Aof%20modern%20language%20models.%20Our%20key%20insight%20is%20that%20the%20ordered%20list%20of%20merge%0Arules%20learned%20by%20a%20BPE%20tokenizer%20naturally%20reveals%20information%20about%20the%20token%0Afrequencies%20in%20its%20training%20data%3A%20the%20first%20merge%20is%20the%20most%20common%20byte%20pair%2C%0Athe%20second%20is%20the%20most%20common%20pair%20after%20merging%20the%20first%20token%2C%20and%20so%20on.%0AGiven%20a%20tokenizer%27s%20merge%20list%20along%20with%20data%20samples%20for%20each%20category%20of%0Ainterest%2C%20we%20formulate%20a%20linear%20program%20that%20solves%20for%20the%20proportion%20of%20each%0Acategory%20in%20the%20tokenizer%27s%20training%20set.%20Importantly%2C%20to%20the%20extent%20to%20which%0Atokenizer%20training%20data%20is%20representative%20of%20the%20pretraining%20data%2C%20we%0Aindirectly%20learn%20about%20the%20pretraining%20data.%20In%20controlled%20experiments%2C%20we%20show%0Athat%20our%20attack%20recovers%20mixture%20ratios%20with%20high%20precision%20for%20tokenizers%0Atrained%20on%20known%20mixtures%20of%20natural%20languages%2C%20programming%20languages%2C%20and%20data%0Asources.%20We%20then%20apply%20our%20approach%20to%20off-the-shelf%20tokenizers%20released%20with%0Arecent%20LMs.%20We%20confirm%20much%20publicly%20disclosed%20information%20about%20these%20models%2C%0Aand%20also%20make%20several%20new%20inferences%3A%20GPT-4o%27s%20tokenizer%20is%20much%20more%0Amultilingual%20than%20its%20predecessors%2C%20training%20on%2039%25%20non-English%20data%3B%20Llama3%0Aextends%20GPT-3.5%27s%20tokenizer%20primarily%20for%20multilingual%20%2848%25%29%20use%3B%20GPT-3.5%27s%20and%0AClaude%27s%20tokenizers%20are%20trained%20on%20predominantly%20code%20%28~60%25%29.%20We%20hope%20our%20work%0Asheds%20light%20on%20current%20design%20practices%20for%20pretraining%20data%2C%20and%20inspires%0Acontinued%20research%20into%20data%20mixture%20inference%20for%20LMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16607v1&entry.124074799=Read"},
{"title": "Automatic Equalization for Individual Instrument Tracks Using\n  Convolutional Neural Networks", "author": "Florian Mockenhaupt and Joscha Simon Rieber and Shahan Nercessian", "abstract": "  We propose a novel approach for the automatic equalization of individual\nmusical instrument tracks. Our method begins by identifying the instrument\npresent within a source recording in order to choose its corresponding ideal\nspectrum as a target. Next, the spectral difference between the recording and\nthe target is calculated, and accordingly, an equalizer matching model is used\nto predict settings for a parametric equalizer. To this end, we build upon a\ndifferentiable parametric equalizer matching neural network, demonstrating\nimprovements relative to previously established state-of-the-art. Unlike past\napproaches, we show how our system naturally allows real-world audio data to be\nleveraged during the training of our matching model, effectively generating\nsuitably produced training targets in an automated manner mirroring conditions\nat inference time. Consequently, we illustrate how fine-tuning our matching\nmodel on such examples considerably improves parametric equalizer matching\nperformance in real-world scenarios, decreasing mean absolute error by 24%\nrelative to methods relying solely on random parameter sampling techniques as a\nself-supervised learning strategy. We perform listening tests, and demonstrate\nthat our proposed automatic equalization solution subjectively enhances the\ntonal characteristics for recordings of common instrument types.\n", "link": "http://arxiv.org/abs/2407.16691v1", "date": "2024-07-23", "relevancy": 2.4164, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5019}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4832}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Equalization%20for%20Individual%20Instrument%20Tracks%20Using%0A%20%20Convolutional%20Neural%20Networks&body=Title%3A%20Automatic%20Equalization%20for%20Individual%20Instrument%20Tracks%20Using%0A%20%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Florian%20Mockenhaupt%20and%20Joscha%20Simon%20Rieber%20and%20Shahan%20Nercessian%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20approach%20for%20the%20automatic%20equalization%20of%20individual%0Amusical%20instrument%20tracks.%20Our%20method%20begins%20by%20identifying%20the%20instrument%0Apresent%20within%20a%20source%20recording%20in%20order%20to%20choose%20its%20corresponding%20ideal%0Aspectrum%20as%20a%20target.%20Next%2C%20the%20spectral%20difference%20between%20the%20recording%20and%0Athe%20target%20is%20calculated%2C%20and%20accordingly%2C%20an%20equalizer%20matching%20model%20is%20used%0Ato%20predict%20settings%20for%20a%20parametric%20equalizer.%20To%20this%20end%2C%20we%20build%20upon%20a%0Adifferentiable%20parametric%20equalizer%20matching%20neural%20network%2C%20demonstrating%0Aimprovements%20relative%20to%20previously%20established%20state-of-the-art.%20Unlike%20past%0Aapproaches%2C%20we%20show%20how%20our%20system%20naturally%20allows%20real-world%20audio%20data%20to%20be%0Aleveraged%20during%20the%20training%20of%20our%20matching%20model%2C%20effectively%20generating%0Asuitably%20produced%20training%20targets%20in%20an%20automated%20manner%20mirroring%20conditions%0Aat%20inference%20time.%20Consequently%2C%20we%20illustrate%20how%20fine-tuning%20our%20matching%0Amodel%20on%20such%20examples%20considerably%20improves%20parametric%20equalizer%20matching%0Aperformance%20in%20real-world%20scenarios%2C%20decreasing%20mean%20absolute%20error%20by%2024%25%0Arelative%20to%20methods%20relying%20solely%20on%20random%20parameter%20sampling%20techniques%20as%20a%0Aself-supervised%20learning%20strategy.%20We%20perform%20listening%20tests%2C%20and%20demonstrate%0Athat%20our%20proposed%20automatic%20equalization%20solution%20subjectively%20enhances%20the%0Atonal%20characteristics%20for%20recordings%20of%20common%20instrument%20types.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Equalization%2520for%2520Individual%2520Instrument%2520Tracks%2520Using%250A%2520%2520Convolutional%2520Neural%2520Networks%26entry.906535625%3DFlorian%2520Mockenhaupt%2520and%2520Joscha%2520Simon%2520Rieber%2520and%2520Shahan%2520Nercessian%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520approach%2520for%2520the%2520automatic%2520equalization%2520of%2520individual%250Amusical%2520instrument%2520tracks.%2520Our%2520method%2520begins%2520by%2520identifying%2520the%2520instrument%250Apresent%2520within%2520a%2520source%2520recording%2520in%2520order%2520to%2520choose%2520its%2520corresponding%2520ideal%250Aspectrum%2520as%2520a%2520target.%2520Next%252C%2520the%2520spectral%2520difference%2520between%2520the%2520recording%2520and%250Athe%2520target%2520is%2520calculated%252C%2520and%2520accordingly%252C%2520an%2520equalizer%2520matching%2520model%2520is%2520used%250Ato%2520predict%2520settings%2520for%2520a%2520parametric%2520equalizer.%2520To%2520this%2520end%252C%2520we%2520build%2520upon%2520a%250Adifferentiable%2520parametric%2520equalizer%2520matching%2520neural%2520network%252C%2520demonstrating%250Aimprovements%2520relative%2520to%2520previously%2520established%2520state-of-the-art.%2520Unlike%2520past%250Aapproaches%252C%2520we%2520show%2520how%2520our%2520system%2520naturally%2520allows%2520real-world%2520audio%2520data%2520to%2520be%250Aleveraged%2520during%2520the%2520training%2520of%2520our%2520matching%2520model%252C%2520effectively%2520generating%250Asuitably%2520produced%2520training%2520targets%2520in%2520an%2520automated%2520manner%2520mirroring%2520conditions%250Aat%2520inference%2520time.%2520Consequently%252C%2520we%2520illustrate%2520how%2520fine-tuning%2520our%2520matching%250Amodel%2520on%2520such%2520examples%2520considerably%2520improves%2520parametric%2520equalizer%2520matching%250Aperformance%2520in%2520real-world%2520scenarios%252C%2520decreasing%2520mean%2520absolute%2520error%2520by%252024%2525%250Arelative%2520to%2520methods%2520relying%2520solely%2520on%2520random%2520parameter%2520sampling%2520techniques%2520as%2520a%250Aself-supervised%2520learning%2520strategy.%2520We%2520perform%2520listening%2520tests%252C%2520and%2520demonstrate%250Athat%2520our%2520proposed%2520automatic%2520equalization%2520solution%2520subjectively%2520enhances%2520the%250Atonal%2520characteristics%2520for%2520recordings%2520of%2520common%2520instrument%2520types.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Equalization%20for%20Individual%20Instrument%20Tracks%20Using%0A%20%20Convolutional%20Neural%20Networks&entry.906535625=Florian%20Mockenhaupt%20and%20Joscha%20Simon%20Rieber%20and%20Shahan%20Nercessian&entry.1292438233=%20%20We%20propose%20a%20novel%20approach%20for%20the%20automatic%20equalization%20of%20individual%0Amusical%20instrument%20tracks.%20Our%20method%20begins%20by%20identifying%20the%20instrument%0Apresent%20within%20a%20source%20recording%20in%20order%20to%20choose%20its%20corresponding%20ideal%0Aspectrum%20as%20a%20target.%20Next%2C%20the%20spectral%20difference%20between%20the%20recording%20and%0Athe%20target%20is%20calculated%2C%20and%20accordingly%2C%20an%20equalizer%20matching%20model%20is%20used%0Ato%20predict%20settings%20for%20a%20parametric%20equalizer.%20To%20this%20end%2C%20we%20build%20upon%20a%0Adifferentiable%20parametric%20equalizer%20matching%20neural%20network%2C%20demonstrating%0Aimprovements%20relative%20to%20previously%20established%20state-of-the-art.%20Unlike%20past%0Aapproaches%2C%20we%20show%20how%20our%20system%20naturally%20allows%20real-world%20audio%20data%20to%20be%0Aleveraged%20during%20the%20training%20of%20our%20matching%20model%2C%20effectively%20generating%0Asuitably%20produced%20training%20targets%20in%20an%20automated%20manner%20mirroring%20conditions%0Aat%20inference%20time.%20Consequently%2C%20we%20illustrate%20how%20fine-tuning%20our%20matching%0Amodel%20on%20such%20examples%20considerably%20improves%20parametric%20equalizer%20matching%0Aperformance%20in%20real-world%20scenarios%2C%20decreasing%20mean%20absolute%20error%20by%2024%25%0Arelative%20to%20methods%20relying%20solely%20on%20random%20parameter%20sampling%20techniques%20as%20a%0Aself-supervised%20learning%20strategy.%20We%20perform%20listening%20tests%2C%20and%20demonstrate%0Athat%20our%20proposed%20automatic%20equalization%20solution%20subjectively%20enhances%20the%0Atonal%20characteristics%20for%20recordings%20of%20common%20instrument%20types.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16691v1&entry.124074799=Read"},
{"title": "WING: Wheel-Inertial Neural Odometry with Ground Manifold Constraints", "author": "Chenxing Jiang and Kunyi Zhang and Sheng Yang and Shaojie Shen and Chao Xu and Fei Gao", "abstract": "  In this paper, we propose an interoceptive-only odometry system for ground\nrobots with neural network processing and soft constraints based on the\nassumption of a globally continuous ground manifold. Exteroceptive sensors such\nas cameras, GPS and LiDAR may encounter difficulties in scenarios with poor\nillumination, indoor environments, dusty areas and straight tunnels. Therefore,\nimproving the pose estimation accuracy only using interoceptive sensors is\nimportant to enhance the reliability of navigation system even in degrading\nscenarios mentioned above. However, interoceptive sensors like IMU and wheel\nencoders suffer from large drift due to noisy measurements. To overcome these\nchallenges, the proposed system trains deep neural networks to correct the\nmeasurements from IMU and wheel encoders, while considering their uncertainty.\nMoreover, because ground robots can only travel on the ground, we model the\nground surface as a globally continuous manifold using a dual cubic B-spline\nmanifold to further improve the estimation accuracy by this soft constraint. A\nnovel space-based sliding-window filtering framework is proposed to fully\nexploit the $C^2$ continuity of ground manifold soft constraints and fuse all\nthe information from raw measurements and neural networks in a yaw-independent\nattitude convention. Extensive experiments demonstrate that our proposed\napproach can outperform state-of-the-art learning-based interoceptive-only\nodometry methods.\n", "link": "http://arxiv.org/abs/2407.10101v2", "date": "2024-07-23", "relevancy": 2.4107, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6404}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5988}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WING%3A%20Wheel-Inertial%20Neural%20Odometry%20with%20Ground%20Manifold%20Constraints&body=Title%3A%20WING%3A%20Wheel-Inertial%20Neural%20Odometry%20with%20Ground%20Manifold%20Constraints%0AAuthor%3A%20Chenxing%20Jiang%20and%20Kunyi%20Zhang%20and%20Sheng%20Yang%20and%20Shaojie%20Shen%20and%20Chao%20Xu%20and%20Fei%20Gao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20an%20interoceptive-only%20odometry%20system%20for%20ground%0Arobots%20with%20neural%20network%20processing%20and%20soft%20constraints%20based%20on%20the%0Aassumption%20of%20a%20globally%20continuous%20ground%20manifold.%20Exteroceptive%20sensors%20such%0Aas%20cameras%2C%20GPS%20and%20LiDAR%20may%20encounter%20difficulties%20in%20scenarios%20with%20poor%0Aillumination%2C%20indoor%20environments%2C%20dusty%20areas%20and%20straight%20tunnels.%20Therefore%2C%0Aimproving%20the%20pose%20estimation%20accuracy%20only%20using%20interoceptive%20sensors%20is%0Aimportant%20to%20enhance%20the%20reliability%20of%20navigation%20system%20even%20in%20degrading%0Ascenarios%20mentioned%20above.%20However%2C%20interoceptive%20sensors%20like%20IMU%20and%20wheel%0Aencoders%20suffer%20from%20large%20drift%20due%20to%20noisy%20measurements.%20To%20overcome%20these%0Achallenges%2C%20the%20proposed%20system%20trains%20deep%20neural%20networks%20to%20correct%20the%0Ameasurements%20from%20IMU%20and%20wheel%20encoders%2C%20while%20considering%20their%20uncertainty.%0AMoreover%2C%20because%20ground%20robots%20can%20only%20travel%20on%20the%20ground%2C%20we%20model%20the%0Aground%20surface%20as%20a%20globally%20continuous%20manifold%20using%20a%20dual%20cubic%20B-spline%0Amanifold%20to%20further%20improve%20the%20estimation%20accuracy%20by%20this%20soft%20constraint.%20A%0Anovel%20space-based%20sliding-window%20filtering%20framework%20is%20proposed%20to%20fully%0Aexploit%20the%20%24C%5E2%24%20continuity%20of%20ground%20manifold%20soft%20constraints%20and%20fuse%20all%0Athe%20information%20from%20raw%20measurements%20and%20neural%20networks%20in%20a%20yaw-independent%0Aattitude%20convention.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%0Aapproach%20can%20outperform%20state-of-the-art%20learning-based%20interoceptive-only%0Aodometry%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWING%253A%2520Wheel-Inertial%2520Neural%2520Odometry%2520with%2520Ground%2520Manifold%2520Constraints%26entry.906535625%3DChenxing%2520Jiang%2520and%2520Kunyi%2520Zhang%2520and%2520Sheng%2520Yang%2520and%2520Shaojie%2520Shen%2520and%2520Chao%2520Xu%2520and%2520Fei%2520Gao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520interoceptive-only%2520odometry%2520system%2520for%2520ground%250Arobots%2520with%2520neural%2520network%2520processing%2520and%2520soft%2520constraints%2520based%2520on%2520the%250Aassumption%2520of%2520a%2520globally%2520continuous%2520ground%2520manifold.%2520Exteroceptive%2520sensors%2520such%250Aas%2520cameras%252C%2520GPS%2520and%2520LiDAR%2520may%2520encounter%2520difficulties%2520in%2520scenarios%2520with%2520poor%250Aillumination%252C%2520indoor%2520environments%252C%2520dusty%2520areas%2520and%2520straight%2520tunnels.%2520Therefore%252C%250Aimproving%2520the%2520pose%2520estimation%2520accuracy%2520only%2520using%2520interoceptive%2520sensors%2520is%250Aimportant%2520to%2520enhance%2520the%2520reliability%2520of%2520navigation%2520system%2520even%2520in%2520degrading%250Ascenarios%2520mentioned%2520above.%2520However%252C%2520interoceptive%2520sensors%2520like%2520IMU%2520and%2520wheel%250Aencoders%2520suffer%2520from%2520large%2520drift%2520due%2520to%2520noisy%2520measurements.%2520To%2520overcome%2520these%250Achallenges%252C%2520the%2520proposed%2520system%2520trains%2520deep%2520neural%2520networks%2520to%2520correct%2520the%250Ameasurements%2520from%2520IMU%2520and%2520wheel%2520encoders%252C%2520while%2520considering%2520their%2520uncertainty.%250AMoreover%252C%2520because%2520ground%2520robots%2520can%2520only%2520travel%2520on%2520the%2520ground%252C%2520we%2520model%2520the%250Aground%2520surface%2520as%2520a%2520globally%2520continuous%2520manifold%2520using%2520a%2520dual%2520cubic%2520B-spline%250Amanifold%2520to%2520further%2520improve%2520the%2520estimation%2520accuracy%2520by%2520this%2520soft%2520constraint.%2520A%250Anovel%2520space-based%2520sliding-window%2520filtering%2520framework%2520is%2520proposed%2520to%2520fully%250Aexploit%2520the%2520%2524C%255E2%2524%2520continuity%2520of%2520ground%2520manifold%2520soft%2520constraints%2520and%2520fuse%2520all%250Athe%2520information%2520from%2520raw%2520measurements%2520and%2520neural%2520networks%2520in%2520a%2520yaw-independent%250Aattitude%2520convention.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%250Aapproach%2520can%2520outperform%2520state-of-the-art%2520learning-based%2520interoceptive-only%250Aodometry%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WING%3A%20Wheel-Inertial%20Neural%20Odometry%20with%20Ground%20Manifold%20Constraints&entry.906535625=Chenxing%20Jiang%20and%20Kunyi%20Zhang%20and%20Sheng%20Yang%20and%20Shaojie%20Shen%20and%20Chao%20Xu%20and%20Fei%20Gao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20an%20interoceptive-only%20odometry%20system%20for%20ground%0Arobots%20with%20neural%20network%20processing%20and%20soft%20constraints%20based%20on%20the%0Aassumption%20of%20a%20globally%20continuous%20ground%20manifold.%20Exteroceptive%20sensors%20such%0Aas%20cameras%2C%20GPS%20and%20LiDAR%20may%20encounter%20difficulties%20in%20scenarios%20with%20poor%0Aillumination%2C%20indoor%20environments%2C%20dusty%20areas%20and%20straight%20tunnels.%20Therefore%2C%0Aimproving%20the%20pose%20estimation%20accuracy%20only%20using%20interoceptive%20sensors%20is%0Aimportant%20to%20enhance%20the%20reliability%20of%20navigation%20system%20even%20in%20degrading%0Ascenarios%20mentioned%20above.%20However%2C%20interoceptive%20sensors%20like%20IMU%20and%20wheel%0Aencoders%20suffer%20from%20large%20drift%20due%20to%20noisy%20measurements.%20To%20overcome%20these%0Achallenges%2C%20the%20proposed%20system%20trains%20deep%20neural%20networks%20to%20correct%20the%0Ameasurements%20from%20IMU%20and%20wheel%20encoders%2C%20while%20considering%20their%20uncertainty.%0AMoreover%2C%20because%20ground%20robots%20can%20only%20travel%20on%20the%20ground%2C%20we%20model%20the%0Aground%20surface%20as%20a%20globally%20continuous%20manifold%20using%20a%20dual%20cubic%20B-spline%0Amanifold%20to%20further%20improve%20the%20estimation%20accuracy%20by%20this%20soft%20constraint.%20A%0Anovel%20space-based%20sliding-window%20filtering%20framework%20is%20proposed%20to%20fully%0Aexploit%20the%20%24C%5E2%24%20continuity%20of%20ground%20manifold%20soft%20constraints%20and%20fuse%20all%0Athe%20information%20from%20raw%20measurements%20and%20neural%20networks%20in%20a%20yaw-independent%0Aattitude%20convention.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%0Aapproach%20can%20outperform%20state-of-the-art%20learning-based%20interoceptive-only%0Aodometry%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10101v2&entry.124074799=Read"},
{"title": "MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence", "author": "Canyu Zhao and Mingyu Liu and Wen Wang and Jianlong Yuan and Hao Chen and Bo Zhang and Chunhua Shen", "abstract": "  Recent advancements in video generation have primarily leveraged diffusion\nmodels for short-duration content. However, these approaches often fall short\nin modeling complex narratives and maintaining character consistency over\nextended periods, which is essential for long-form video production like\nmovies. We propose MovieDreamer, a novel hierarchical framework that integrates\nthe strengths of autoregressive models with diffusion-based rendering to\npioneer long-duration video generation with intricate plot progressions and\nhigh visual fidelity. Our approach utilizes autoregressive models for global\nnarrative coherence, predicting sequences of visual tokens that are\nsubsequently transformed into high-quality video frames through diffusion\nrendering. This method is akin to traditional movie production processes, where\ncomplex stories are factorized down into manageable scene capturing. Further,\nwe employ a multimodal script that enriches scene descriptions with detailed\ncharacter information and visual style, enhancing continuity and character\nidentity across scenes. We present extensive experiments across various movie\ngenres, demonstrating that our approach not only achieves superior visual and\nnarrative quality but also effectively extends the duration of generated\ncontent significantly beyond current capabilities. Homepage:\nhttps://aim-uofa.github.io/MovieDreamer/.\n", "link": "http://arxiv.org/abs/2407.16655v1", "date": "2024-07-23", "relevancy": 2.4094, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6272}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6202}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MovieDreamer%3A%20Hierarchical%20Generation%20for%20Coherent%20Long%20Visual%20Sequence&body=Title%3A%20MovieDreamer%3A%20Hierarchical%20Generation%20for%20Coherent%20Long%20Visual%20Sequence%0AAuthor%3A%20Canyu%20Zhao%20and%20Mingyu%20Liu%20and%20Wen%20Wang%20and%20Jianlong%20Yuan%20and%20Hao%20Chen%20and%20Bo%20Zhang%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20video%20generation%20have%20primarily%20leveraged%20diffusion%0Amodels%20for%20short-duration%20content.%20However%2C%20these%20approaches%20often%20fall%20short%0Ain%20modeling%20complex%20narratives%20and%20maintaining%20character%20consistency%20over%0Aextended%20periods%2C%20which%20is%20essential%20for%20long-form%20video%20production%20like%0Amovies.%20We%20propose%20MovieDreamer%2C%20a%20novel%20hierarchical%20framework%20that%20integrates%0Athe%20strengths%20of%20autoregressive%20models%20with%20diffusion-based%20rendering%20to%0Apioneer%20long-duration%20video%20generation%20with%20intricate%20plot%20progressions%20and%0Ahigh%20visual%20fidelity.%20Our%20approach%20utilizes%20autoregressive%20models%20for%20global%0Anarrative%20coherence%2C%20predicting%20sequences%20of%20visual%20tokens%20that%20are%0Asubsequently%20transformed%20into%20high-quality%20video%20frames%20through%20diffusion%0Arendering.%20This%20method%20is%20akin%20to%20traditional%20movie%20production%20processes%2C%20where%0Acomplex%20stories%20are%20factorized%20down%20into%20manageable%20scene%20capturing.%20Further%2C%0Awe%20employ%20a%20multimodal%20script%20that%20enriches%20scene%20descriptions%20with%20detailed%0Acharacter%20information%20and%20visual%20style%2C%20enhancing%20continuity%20and%20character%0Aidentity%20across%20scenes.%20We%20present%20extensive%20experiments%20across%20various%20movie%0Agenres%2C%20demonstrating%20that%20our%20approach%20not%20only%20achieves%20superior%20visual%20and%0Anarrative%20quality%20but%20also%20effectively%20extends%20the%20duration%20of%20generated%0Acontent%20significantly%20beyond%20current%20capabilities.%20Homepage%3A%0Ahttps%3A//aim-uofa.github.io/MovieDreamer/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMovieDreamer%253A%2520Hierarchical%2520Generation%2520for%2520Coherent%2520Long%2520Visual%2520Sequence%26entry.906535625%3DCanyu%2520Zhao%2520and%2520Mingyu%2520Liu%2520and%2520Wen%2520Wang%2520and%2520Jianlong%2520Yuan%2520and%2520Hao%2520Chen%2520and%2520Bo%2520Zhang%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520video%2520generation%2520have%2520primarily%2520leveraged%2520diffusion%250Amodels%2520for%2520short-duration%2520content.%2520However%252C%2520these%2520approaches%2520often%2520fall%2520short%250Ain%2520modeling%2520complex%2520narratives%2520and%2520maintaining%2520character%2520consistency%2520over%250Aextended%2520periods%252C%2520which%2520is%2520essential%2520for%2520long-form%2520video%2520production%2520like%250Amovies.%2520We%2520propose%2520MovieDreamer%252C%2520a%2520novel%2520hierarchical%2520framework%2520that%2520integrates%250Athe%2520strengths%2520of%2520autoregressive%2520models%2520with%2520diffusion-based%2520rendering%2520to%250Apioneer%2520long-duration%2520video%2520generation%2520with%2520intricate%2520plot%2520progressions%2520and%250Ahigh%2520visual%2520fidelity.%2520Our%2520approach%2520utilizes%2520autoregressive%2520models%2520for%2520global%250Anarrative%2520coherence%252C%2520predicting%2520sequences%2520of%2520visual%2520tokens%2520that%2520are%250Asubsequently%2520transformed%2520into%2520high-quality%2520video%2520frames%2520through%2520diffusion%250Arendering.%2520This%2520method%2520is%2520akin%2520to%2520traditional%2520movie%2520production%2520processes%252C%2520where%250Acomplex%2520stories%2520are%2520factorized%2520down%2520into%2520manageable%2520scene%2520capturing.%2520Further%252C%250Awe%2520employ%2520a%2520multimodal%2520script%2520that%2520enriches%2520scene%2520descriptions%2520with%2520detailed%250Acharacter%2520information%2520and%2520visual%2520style%252C%2520enhancing%2520continuity%2520and%2520character%250Aidentity%2520across%2520scenes.%2520We%2520present%2520extensive%2520experiments%2520across%2520various%2520movie%250Agenres%252C%2520demonstrating%2520that%2520our%2520approach%2520not%2520only%2520achieves%2520superior%2520visual%2520and%250Anarrative%2520quality%2520but%2520also%2520effectively%2520extends%2520the%2520duration%2520of%2520generated%250Acontent%2520significantly%2520beyond%2520current%2520capabilities.%2520Homepage%253A%250Ahttps%253A//aim-uofa.github.io/MovieDreamer/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MovieDreamer%3A%20Hierarchical%20Generation%20for%20Coherent%20Long%20Visual%20Sequence&entry.906535625=Canyu%20Zhao%20and%20Mingyu%20Liu%20and%20Wen%20Wang%20and%20Jianlong%20Yuan%20and%20Hao%20Chen%20and%20Bo%20Zhang%20and%20Chunhua%20Shen&entry.1292438233=%20%20Recent%20advancements%20in%20video%20generation%20have%20primarily%20leveraged%20diffusion%0Amodels%20for%20short-duration%20content.%20However%2C%20these%20approaches%20often%20fall%20short%0Ain%20modeling%20complex%20narratives%20and%20maintaining%20character%20consistency%20over%0Aextended%20periods%2C%20which%20is%20essential%20for%20long-form%20video%20production%20like%0Amovies.%20We%20propose%20MovieDreamer%2C%20a%20novel%20hierarchical%20framework%20that%20integrates%0Athe%20strengths%20of%20autoregressive%20models%20with%20diffusion-based%20rendering%20to%0Apioneer%20long-duration%20video%20generation%20with%20intricate%20plot%20progressions%20and%0Ahigh%20visual%20fidelity.%20Our%20approach%20utilizes%20autoregressive%20models%20for%20global%0Anarrative%20coherence%2C%20predicting%20sequences%20of%20visual%20tokens%20that%20are%0Asubsequently%20transformed%20into%20high-quality%20video%20frames%20through%20diffusion%0Arendering.%20This%20method%20is%20akin%20to%20traditional%20movie%20production%20processes%2C%20where%0Acomplex%20stories%20are%20factorized%20down%20into%20manageable%20scene%20capturing.%20Further%2C%0Awe%20employ%20a%20multimodal%20script%20that%20enriches%20scene%20descriptions%20with%20detailed%0Acharacter%20information%20and%20visual%20style%2C%20enhancing%20continuity%20and%20character%0Aidentity%20across%20scenes.%20We%20present%20extensive%20experiments%20across%20various%20movie%0Agenres%2C%20demonstrating%20that%20our%20approach%20not%20only%20achieves%20superior%20visual%20and%0Anarrative%20quality%20but%20also%20effectively%20extends%20the%20duration%20of%20generated%0Acontent%20significantly%20beyond%20current%20capabilities.%20Homepage%3A%0Ahttps%3A//aim-uofa.github.io/MovieDreamer/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16655v1&entry.124074799=Read"},
{"title": "Laplacian Segmentation Networks Improve Epistemic Uncertainty\n  Quantification", "author": "Kilian Zepf and Selma Wanna and Marco Miani and Juston Moore and Jes Frellsen and S\u00f8ren Hauberg and Frederik Warburg and Aasa Feragen", "abstract": "  Image segmentation relies heavily on neural networks which are known to be\noverconfident, especially when making predictions on out-of-distribution (OOD)\nimages. This is a common scenario in the medical domain due to variations in\nequipment, acquisition sites, or image corruptions. This work addresses the\nchallenge of OOD detection by proposing Laplacian Segmentation Networks (LSN):\nmethods which jointly model epistemic (model) and aleatoric (data) uncertainty\nfor OOD detection. In doing so, we propose the first Laplace approximation of\nthe weight posterior that scales to large neural networks with skip connections\nthat have high-dimensional outputs. We demonstrate on three datasets that the\nLSN-modeled parameter distributions, in combination with suitable uncertainty\nmeasures, gives superior OOD detection.\n", "link": "http://arxiv.org/abs/2303.13123v2", "date": "2024-07-23", "relevancy": 2.387, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6309}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6235}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Laplacian%20Segmentation%20Networks%20Improve%20Epistemic%20Uncertainty%0A%20%20Quantification&body=Title%3A%20Laplacian%20Segmentation%20Networks%20Improve%20Epistemic%20Uncertainty%0A%20%20Quantification%0AAuthor%3A%20Kilian%20Zepf%20and%20Selma%20Wanna%20and%20Marco%20Miani%20and%20Juston%20Moore%20and%20Jes%20Frellsen%20and%20S%C3%B8ren%20Hauberg%20and%20Frederik%20Warburg%20and%20Aasa%20Feragen%0AAbstract%3A%20%20%20Image%20segmentation%20relies%20heavily%20on%20neural%20networks%20which%20are%20known%20to%20be%0Aoverconfident%2C%20especially%20when%20making%20predictions%20on%20out-of-distribution%20%28OOD%29%0Aimages.%20This%20is%20a%20common%20scenario%20in%20the%20medical%20domain%20due%20to%20variations%20in%0Aequipment%2C%20acquisition%20sites%2C%20or%20image%20corruptions.%20This%20work%20addresses%20the%0Achallenge%20of%20OOD%20detection%20by%20proposing%20Laplacian%20Segmentation%20Networks%20%28LSN%29%3A%0Amethods%20which%20jointly%20model%20epistemic%20%28model%29%20and%20aleatoric%20%28data%29%20uncertainty%0Afor%20OOD%20detection.%20In%20doing%20so%2C%20we%20propose%20the%20first%20Laplace%20approximation%20of%0Athe%20weight%20posterior%20that%20scales%20to%20large%20neural%20networks%20with%20skip%20connections%0Athat%20have%20high-dimensional%20outputs.%20We%20demonstrate%20on%20three%20datasets%20that%20the%0ALSN-modeled%20parameter%20distributions%2C%20in%20combination%20with%20suitable%20uncertainty%0Ameasures%2C%20gives%20superior%20OOD%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.13123v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaplacian%2520Segmentation%2520Networks%2520Improve%2520Epistemic%2520Uncertainty%250A%2520%2520Quantification%26entry.906535625%3DKilian%2520Zepf%2520and%2520Selma%2520Wanna%2520and%2520Marco%2520Miani%2520and%2520Juston%2520Moore%2520and%2520Jes%2520Frellsen%2520and%2520S%25C3%25B8ren%2520Hauberg%2520and%2520Frederik%2520Warburg%2520and%2520Aasa%2520Feragen%26entry.1292438233%3D%2520%2520Image%2520segmentation%2520relies%2520heavily%2520on%2520neural%2520networks%2520which%2520are%2520known%2520to%2520be%250Aoverconfident%252C%2520especially%2520when%2520making%2520predictions%2520on%2520out-of-distribution%2520%2528OOD%2529%250Aimages.%2520This%2520is%2520a%2520common%2520scenario%2520in%2520the%2520medical%2520domain%2520due%2520to%2520variations%2520in%250Aequipment%252C%2520acquisition%2520sites%252C%2520or%2520image%2520corruptions.%2520This%2520work%2520addresses%2520the%250Achallenge%2520of%2520OOD%2520detection%2520by%2520proposing%2520Laplacian%2520Segmentation%2520Networks%2520%2528LSN%2529%253A%250Amethods%2520which%2520jointly%2520model%2520epistemic%2520%2528model%2529%2520and%2520aleatoric%2520%2528data%2529%2520uncertainty%250Afor%2520OOD%2520detection.%2520In%2520doing%2520so%252C%2520we%2520propose%2520the%2520first%2520Laplace%2520approximation%2520of%250Athe%2520weight%2520posterior%2520that%2520scales%2520to%2520large%2520neural%2520networks%2520with%2520skip%2520connections%250Athat%2520have%2520high-dimensional%2520outputs.%2520We%2520demonstrate%2520on%2520three%2520datasets%2520that%2520the%250ALSN-modeled%2520parameter%2520distributions%252C%2520in%2520combination%2520with%2520suitable%2520uncertainty%250Ameasures%252C%2520gives%2520superior%2520OOD%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.13123v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Laplacian%20Segmentation%20Networks%20Improve%20Epistemic%20Uncertainty%0A%20%20Quantification&entry.906535625=Kilian%20Zepf%20and%20Selma%20Wanna%20and%20Marco%20Miani%20and%20Juston%20Moore%20and%20Jes%20Frellsen%20and%20S%C3%B8ren%20Hauberg%20and%20Frederik%20Warburg%20and%20Aasa%20Feragen&entry.1292438233=%20%20Image%20segmentation%20relies%20heavily%20on%20neural%20networks%20which%20are%20known%20to%20be%0Aoverconfident%2C%20especially%20when%20making%20predictions%20on%20out-of-distribution%20%28OOD%29%0Aimages.%20This%20is%20a%20common%20scenario%20in%20the%20medical%20domain%20due%20to%20variations%20in%0Aequipment%2C%20acquisition%20sites%2C%20or%20image%20corruptions.%20This%20work%20addresses%20the%0Achallenge%20of%20OOD%20detection%20by%20proposing%20Laplacian%20Segmentation%20Networks%20%28LSN%29%3A%0Amethods%20which%20jointly%20model%20epistemic%20%28model%29%20and%20aleatoric%20%28data%29%20uncertainty%0Afor%20OOD%20detection.%20In%20doing%20so%2C%20we%20propose%20the%20first%20Laplace%20approximation%20of%0Athe%20weight%20posterior%20that%20scales%20to%20large%20neural%20networks%20with%20skip%20connections%0Athat%20have%20high-dimensional%20outputs.%20We%20demonstrate%20on%20three%20datasets%20that%20the%0ALSN-modeled%20parameter%20distributions%2C%20in%20combination%20with%20suitable%20uncertainty%0Ameasures%2C%20gives%20superior%20OOD%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.13123v2&entry.124074799=Read"},
{"title": "ITINERA: Integrating Spatial Optimization with Large Language Models for\n  Open-domain Urban Itinerary Planning", "author": "Yihong Tang and Zhaokai Wang and Ao Qu and Yihao Yan and Zhaofeng Wu and Dingyi Zhuang and Jushi Kai and Kebing Hou and Xiaotong Guo and Jinhua Zhao and Zhan Zhao and Wei Ma", "abstract": "  Citywalk, a recently popular form of urban travel, requires genuine\npersonalization and understanding of fine-grained requests compared to\ntraditional itinerary planning. In this paper, we introduce the novel task of\nOpen-domain Urban Itinerary Planning (OUIP), which generates personalized urban\nitineraries from user requests in natural language. We then present ITINERA, an\nOUIP system that integrates spatial optimization with large language models to\nprovide customized urban itineraries based on user needs. This involves\ndecomposing user requests, selecting candidate points of interest (POIs),\nordering the POIs based on cluster-aware spatial optimization, and generating\nthe itinerary. Experiments on real-world datasets and the performance of the\ndeployed system demonstrate our system's capacity to deliver personalized and\nspatially coherent itineraries compared to current solutions.\n", "link": "http://arxiv.org/abs/2402.07204v3", "date": "2024-07-23", "relevancy": 2.3763, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5098}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4645}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ITINERA%3A%20Integrating%20Spatial%20Optimization%20with%20Large%20Language%20Models%20for%0A%20%20Open-domain%20Urban%20Itinerary%20Planning&body=Title%3A%20ITINERA%3A%20Integrating%20Spatial%20Optimization%20with%20Large%20Language%20Models%20for%0A%20%20Open-domain%20Urban%20Itinerary%20Planning%0AAuthor%3A%20Yihong%20Tang%20and%20Zhaokai%20Wang%20and%20Ao%20Qu%20and%20Yihao%20Yan%20and%20Zhaofeng%20Wu%20and%20Dingyi%20Zhuang%20and%20Jushi%20Kai%20and%20Kebing%20Hou%20and%20Xiaotong%20Guo%20and%20Jinhua%20Zhao%20and%20Zhan%20Zhao%20and%20Wei%20Ma%0AAbstract%3A%20%20%20Citywalk%2C%20a%20recently%20popular%20form%20of%20urban%20travel%2C%20requires%20genuine%0Apersonalization%20and%20understanding%20of%20fine-grained%20requests%20compared%20to%0Atraditional%20itinerary%20planning.%20In%20this%20paper%2C%20we%20introduce%20the%20novel%20task%20of%0AOpen-domain%20Urban%20Itinerary%20Planning%20%28OUIP%29%2C%20which%20generates%20personalized%20urban%0Aitineraries%20from%20user%20requests%20in%20natural%20language.%20We%20then%20present%20ITINERA%2C%20an%0AOUIP%20system%20that%20integrates%20spatial%20optimization%20with%20large%20language%20models%20to%0Aprovide%20customized%20urban%20itineraries%20based%20on%20user%20needs.%20This%20involves%0Adecomposing%20user%20requests%2C%20selecting%20candidate%20points%20of%20interest%20%28POIs%29%2C%0Aordering%20the%20POIs%20based%20on%20cluster-aware%20spatial%20optimization%2C%20and%20generating%0Athe%20itinerary.%20Experiments%20on%20real-world%20datasets%20and%20the%20performance%20of%20the%0Adeployed%20system%20demonstrate%20our%20system%27s%20capacity%20to%20deliver%20personalized%20and%0Aspatially%20coherent%20itineraries%20compared%20to%20current%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07204v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DITINERA%253A%2520Integrating%2520Spatial%2520Optimization%2520with%2520Large%2520Language%2520Models%2520for%250A%2520%2520Open-domain%2520Urban%2520Itinerary%2520Planning%26entry.906535625%3DYihong%2520Tang%2520and%2520Zhaokai%2520Wang%2520and%2520Ao%2520Qu%2520and%2520Yihao%2520Yan%2520and%2520Zhaofeng%2520Wu%2520and%2520Dingyi%2520Zhuang%2520and%2520Jushi%2520Kai%2520and%2520Kebing%2520Hou%2520and%2520Xiaotong%2520Guo%2520and%2520Jinhua%2520Zhao%2520and%2520Zhan%2520Zhao%2520and%2520Wei%2520Ma%26entry.1292438233%3D%2520%2520Citywalk%252C%2520a%2520recently%2520popular%2520form%2520of%2520urban%2520travel%252C%2520requires%2520genuine%250Apersonalization%2520and%2520understanding%2520of%2520fine-grained%2520requests%2520compared%2520to%250Atraditional%2520itinerary%2520planning.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520novel%2520task%2520of%250AOpen-domain%2520Urban%2520Itinerary%2520Planning%2520%2528OUIP%2529%252C%2520which%2520generates%2520personalized%2520urban%250Aitineraries%2520from%2520user%2520requests%2520in%2520natural%2520language.%2520We%2520then%2520present%2520ITINERA%252C%2520an%250AOUIP%2520system%2520that%2520integrates%2520spatial%2520optimization%2520with%2520large%2520language%2520models%2520to%250Aprovide%2520customized%2520urban%2520itineraries%2520based%2520on%2520user%2520needs.%2520This%2520involves%250Adecomposing%2520user%2520requests%252C%2520selecting%2520candidate%2520points%2520of%2520interest%2520%2528POIs%2529%252C%250Aordering%2520the%2520POIs%2520based%2520on%2520cluster-aware%2520spatial%2520optimization%252C%2520and%2520generating%250Athe%2520itinerary.%2520Experiments%2520on%2520real-world%2520datasets%2520and%2520the%2520performance%2520of%2520the%250Adeployed%2520system%2520demonstrate%2520our%2520system%2527s%2520capacity%2520to%2520deliver%2520personalized%2520and%250Aspatially%2520coherent%2520itineraries%2520compared%2520to%2520current%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07204v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ITINERA%3A%20Integrating%20Spatial%20Optimization%20with%20Large%20Language%20Models%20for%0A%20%20Open-domain%20Urban%20Itinerary%20Planning&entry.906535625=Yihong%20Tang%20and%20Zhaokai%20Wang%20and%20Ao%20Qu%20and%20Yihao%20Yan%20and%20Zhaofeng%20Wu%20and%20Dingyi%20Zhuang%20and%20Jushi%20Kai%20and%20Kebing%20Hou%20and%20Xiaotong%20Guo%20and%20Jinhua%20Zhao%20and%20Zhan%20Zhao%20and%20Wei%20Ma&entry.1292438233=%20%20Citywalk%2C%20a%20recently%20popular%20form%20of%20urban%20travel%2C%20requires%20genuine%0Apersonalization%20and%20understanding%20of%20fine-grained%20requests%20compared%20to%0Atraditional%20itinerary%20planning.%20In%20this%20paper%2C%20we%20introduce%20the%20novel%20task%20of%0AOpen-domain%20Urban%20Itinerary%20Planning%20%28OUIP%29%2C%20which%20generates%20personalized%20urban%0Aitineraries%20from%20user%20requests%20in%20natural%20language.%20We%20then%20present%20ITINERA%2C%20an%0AOUIP%20system%20that%20integrates%20spatial%20optimization%20with%20large%20language%20models%20to%0Aprovide%20customized%20urban%20itineraries%20based%20on%20user%20needs.%20This%20involves%0Adecomposing%20user%20requests%2C%20selecting%20candidate%20points%20of%20interest%20%28POIs%29%2C%0Aordering%20the%20POIs%20based%20on%20cluster-aware%20spatial%20optimization%2C%20and%20generating%0Athe%20itinerary.%20Experiments%20on%20real-world%20datasets%20and%20the%20performance%20of%20the%0Adeployed%20system%20demonstrate%20our%20system%27s%20capacity%20to%20deliver%20personalized%20and%0Aspatially%20coherent%20itineraries%20compared%20to%20current%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07204v3&entry.124074799=Read"},
{"title": "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning\n  Instruction Using Language Model", "author": "Wenqi Zhang and Zhenglin Cheng and Yuanyu He and Mengna Wang and Yongliang Shen and Zeqi Tan and Guiyang Hou and Mingqian He and Yanna Ma and Weiming Lu and Yueting Zhuang", "abstract": "  Although most current large multimodal models (LMMs) can already understand\nphotos of natural scenes and portraits, their understanding of abstract images,\ne.g., charts, maps, or layouts, and visual reasoning capabilities remains quite\nrudimentary. They often struggle with simple daily tasks, such as reading time\nfrom a clock, understanding a flowchart, or planning a route using a road map.\nIn light of this, we design a multi-modal self-instruct, utilizing large\nlanguage models and their code capabilities to synthesize massive abstract\nimages and visual reasoning instructions across daily scenarios. Our strategy\neffortlessly creates a multimodal benchmark with 11,193 instructions for eight\nvisual scenarios: charts, tables, simulated maps, dashboards, flowcharts,\nrelation graphs, floor plans, and visual puzzles. \\textbf{This benchmark,\nconstructed with simple lines and geometric elements, exposes the shortcomings\nof most advanced LMMs} like Claude-3.5-Sonnet and GPT-4o in abstract image\nunderstanding, spatial relations reasoning, and visual element induction.\nBesides, to verify the quality of our synthetic data, we fine-tune an LMM using\n62,476 synthetic chart, table and road map instructions. The results\ndemonstrate improved chart understanding and map navigation performance, and\nalso demonstrate potential benefits for other visual reasoning tasks. Our code\nis available at: \\url{https://github.com/zwq2018/Multi-modal-Self-instruct}.\n", "link": "http://arxiv.org/abs/2407.07053v3", "date": "2024-07-23", "relevancy": 2.3753, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.601}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5972}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Self-Instruct%3A%20Synthetic%20Abstract%20Image%20and%20Visual%20Reasoning%0A%20%20Instruction%20Using%20Language%20Model&body=Title%3A%20Multimodal%20Self-Instruct%3A%20Synthetic%20Abstract%20Image%20and%20Visual%20Reasoning%0A%20%20Instruction%20Using%20Language%20Model%0AAuthor%3A%20Wenqi%20Zhang%20and%20Zhenglin%20Cheng%20and%20Yuanyu%20He%20and%20Mengna%20Wang%20and%20Yongliang%20Shen%20and%20Zeqi%20Tan%20and%20Guiyang%20Hou%20and%20Mingqian%20He%20and%20Yanna%20Ma%20and%20Weiming%20Lu%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Although%20most%20current%20large%20multimodal%20models%20%28LMMs%29%20can%20already%20understand%0Aphotos%20of%20natural%20scenes%20and%20portraits%2C%20their%20understanding%20of%20abstract%20images%2C%0Ae.g.%2C%20charts%2C%20maps%2C%20or%20layouts%2C%20and%20visual%20reasoning%20capabilities%20remains%20quite%0Arudimentary.%20They%20often%20struggle%20with%20simple%20daily%20tasks%2C%20such%20as%20reading%20time%0Afrom%20a%20clock%2C%20understanding%20a%20flowchart%2C%20or%20planning%20a%20route%20using%20a%20road%20map.%0AIn%20light%20of%20this%2C%20we%20design%20a%20multi-modal%20self-instruct%2C%20utilizing%20large%0Alanguage%20models%20and%20their%20code%20capabilities%20to%20synthesize%20massive%20abstract%0Aimages%20and%20visual%20reasoning%20instructions%20across%20daily%20scenarios.%20Our%20strategy%0Aeffortlessly%20creates%20a%20multimodal%20benchmark%20with%2011%2C193%20instructions%20for%20eight%0Avisual%20scenarios%3A%20charts%2C%20tables%2C%20simulated%20maps%2C%20dashboards%2C%20flowcharts%2C%0Arelation%20graphs%2C%20floor%20plans%2C%20and%20visual%20puzzles.%20%5Ctextbf%7BThis%20benchmark%2C%0Aconstructed%20with%20simple%20lines%20and%20geometric%20elements%2C%20exposes%20the%20shortcomings%0Aof%20most%20advanced%20LMMs%7D%20like%20Claude-3.5-Sonnet%20and%20GPT-4o%20in%20abstract%20image%0Aunderstanding%2C%20spatial%20relations%20reasoning%2C%20and%20visual%20element%20induction.%0ABesides%2C%20to%20verify%20the%20quality%20of%20our%20synthetic%20data%2C%20we%20fine-tune%20an%20LMM%20using%0A62%2C476%20synthetic%20chart%2C%20table%20and%20road%20map%20instructions.%20The%20results%0Ademonstrate%20improved%20chart%20understanding%20and%20map%20navigation%20performance%2C%20and%0Aalso%20demonstrate%20potential%20benefits%20for%20other%20visual%20reasoning%20tasks.%20Our%20code%0Ais%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/zwq2018/Multi-modal-Self-instruct%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07053v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Self-Instruct%253A%2520Synthetic%2520Abstract%2520Image%2520and%2520Visual%2520Reasoning%250A%2520%2520Instruction%2520Using%2520Language%2520Model%26entry.906535625%3DWenqi%2520Zhang%2520and%2520Zhenglin%2520Cheng%2520and%2520Yuanyu%2520He%2520and%2520Mengna%2520Wang%2520and%2520Yongliang%2520Shen%2520and%2520Zeqi%2520Tan%2520and%2520Guiyang%2520Hou%2520and%2520Mingqian%2520He%2520and%2520Yanna%2520Ma%2520and%2520Weiming%2520Lu%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Although%2520most%2520current%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520can%2520already%2520understand%250Aphotos%2520of%2520natural%2520scenes%2520and%2520portraits%252C%2520their%2520understanding%2520of%2520abstract%2520images%252C%250Ae.g.%252C%2520charts%252C%2520maps%252C%2520or%2520layouts%252C%2520and%2520visual%2520reasoning%2520capabilities%2520remains%2520quite%250Arudimentary.%2520They%2520often%2520struggle%2520with%2520simple%2520daily%2520tasks%252C%2520such%2520as%2520reading%2520time%250Afrom%2520a%2520clock%252C%2520understanding%2520a%2520flowchart%252C%2520or%2520planning%2520a%2520route%2520using%2520a%2520road%2520map.%250AIn%2520light%2520of%2520this%252C%2520we%2520design%2520a%2520multi-modal%2520self-instruct%252C%2520utilizing%2520large%250Alanguage%2520models%2520and%2520their%2520code%2520capabilities%2520to%2520synthesize%2520massive%2520abstract%250Aimages%2520and%2520visual%2520reasoning%2520instructions%2520across%2520daily%2520scenarios.%2520Our%2520strategy%250Aeffortlessly%2520creates%2520a%2520multimodal%2520benchmark%2520with%252011%252C193%2520instructions%2520for%2520eight%250Avisual%2520scenarios%253A%2520charts%252C%2520tables%252C%2520simulated%2520maps%252C%2520dashboards%252C%2520flowcharts%252C%250Arelation%2520graphs%252C%2520floor%2520plans%252C%2520and%2520visual%2520puzzles.%2520%255Ctextbf%257BThis%2520benchmark%252C%250Aconstructed%2520with%2520simple%2520lines%2520and%2520geometric%2520elements%252C%2520exposes%2520the%2520shortcomings%250Aof%2520most%2520advanced%2520LMMs%257D%2520like%2520Claude-3.5-Sonnet%2520and%2520GPT-4o%2520in%2520abstract%2520image%250Aunderstanding%252C%2520spatial%2520relations%2520reasoning%252C%2520and%2520visual%2520element%2520induction.%250ABesides%252C%2520to%2520verify%2520the%2520quality%2520of%2520our%2520synthetic%2520data%252C%2520we%2520fine-tune%2520an%2520LMM%2520using%250A62%252C476%2520synthetic%2520chart%252C%2520table%2520and%2520road%2520map%2520instructions.%2520The%2520results%250Ademonstrate%2520improved%2520chart%2520understanding%2520and%2520map%2520navigation%2520performance%252C%2520and%250Aalso%2520demonstrate%2520potential%2520benefits%2520for%2520other%2520visual%2520reasoning%2520tasks.%2520Our%2520code%250Ais%2520available%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/zwq2018/Multi-modal-Self-instruct%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07053v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Self-Instruct%3A%20Synthetic%20Abstract%20Image%20and%20Visual%20Reasoning%0A%20%20Instruction%20Using%20Language%20Model&entry.906535625=Wenqi%20Zhang%20and%20Zhenglin%20Cheng%20and%20Yuanyu%20He%20and%20Mengna%20Wang%20and%20Yongliang%20Shen%20and%20Zeqi%20Tan%20and%20Guiyang%20Hou%20and%20Mingqian%20He%20and%20Yanna%20Ma%20and%20Weiming%20Lu%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Although%20most%20current%20large%20multimodal%20models%20%28LMMs%29%20can%20already%20understand%0Aphotos%20of%20natural%20scenes%20and%20portraits%2C%20their%20understanding%20of%20abstract%20images%2C%0Ae.g.%2C%20charts%2C%20maps%2C%20or%20layouts%2C%20and%20visual%20reasoning%20capabilities%20remains%20quite%0Arudimentary.%20They%20often%20struggle%20with%20simple%20daily%20tasks%2C%20such%20as%20reading%20time%0Afrom%20a%20clock%2C%20understanding%20a%20flowchart%2C%20or%20planning%20a%20route%20using%20a%20road%20map.%0AIn%20light%20of%20this%2C%20we%20design%20a%20multi-modal%20self-instruct%2C%20utilizing%20large%0Alanguage%20models%20and%20their%20code%20capabilities%20to%20synthesize%20massive%20abstract%0Aimages%20and%20visual%20reasoning%20instructions%20across%20daily%20scenarios.%20Our%20strategy%0Aeffortlessly%20creates%20a%20multimodal%20benchmark%20with%2011%2C193%20instructions%20for%20eight%0Avisual%20scenarios%3A%20charts%2C%20tables%2C%20simulated%20maps%2C%20dashboards%2C%20flowcharts%2C%0Arelation%20graphs%2C%20floor%20plans%2C%20and%20visual%20puzzles.%20%5Ctextbf%7BThis%20benchmark%2C%0Aconstructed%20with%20simple%20lines%20and%20geometric%20elements%2C%20exposes%20the%20shortcomings%0Aof%20most%20advanced%20LMMs%7D%20like%20Claude-3.5-Sonnet%20and%20GPT-4o%20in%20abstract%20image%0Aunderstanding%2C%20spatial%20relations%20reasoning%2C%20and%20visual%20element%20induction.%0ABesides%2C%20to%20verify%20the%20quality%20of%20our%20synthetic%20data%2C%20we%20fine-tune%20an%20LMM%20using%0A62%2C476%20synthetic%20chart%2C%20table%20and%20road%20map%20instructions.%20The%20results%0Ademonstrate%20improved%20chart%20understanding%20and%20map%20navigation%20performance%2C%20and%0Aalso%20demonstrate%20potential%20benefits%20for%20other%20visual%20reasoning%20tasks.%20Our%20code%0Ais%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/zwq2018/Multi-modal-Self-instruct%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07053v3&entry.124074799=Read"},
{"title": "DHGS: Decoupled Hybrid Gaussian Splatting for Driving Scene", "author": "Xi Shi and Lingli Chen and Peng Wei and Xi Wu and Tian Jiang and Yonggang Luo and Lecheng Xie", "abstract": "  Existing Gaussian splatting methods struggle to achieve satisfactory novel\nview synthesis in driving scenes due to the lack of crafty design and geometric\nconstraints of related elements. This paper introduces a novel method called\nDecoupled Hybrid Gaussian Splatting (DHGS), which aims at promoting the\nrendering quality of novel view synthesis for driving scenes. The novelty of\nthis work lies in the decoupled and hybrid pixel-level blender for road and\nnon-road layers, without conventional unified differentiable rendering logic\nfor the entire scene, meanwhile maintaining consistent and continuous\nsuperimposition through the proposed depth-ordered rendering strategy. Beyond\nthat, an implicit road representation comprised of Signed Distance Field (SDF)\nis trained to supervise the road surface with subtle geometric attributes.\nAccompanied by the use of auxiliary transmittance loss and consistency loss,\nnovel images with imperceptible boundary and elevated fidelity are ultimately\nobtained. Substantial experiments on Waymo dataset prove that DHGS outperforms\nthe state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2407.16600v1", "date": "2024-07-23", "relevancy": 2.3543, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6156}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5873}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DHGS%3A%20Decoupled%20Hybrid%20Gaussian%20Splatting%20for%20Driving%20Scene&body=Title%3A%20DHGS%3A%20Decoupled%20Hybrid%20Gaussian%20Splatting%20for%20Driving%20Scene%0AAuthor%3A%20Xi%20Shi%20and%20Lingli%20Chen%20and%20Peng%20Wei%20and%20Xi%20Wu%20and%20Tian%20Jiang%20and%20Yonggang%20Luo%20and%20Lecheng%20Xie%0AAbstract%3A%20%20%20Existing%20Gaussian%20splatting%20methods%20struggle%20to%20achieve%20satisfactory%20novel%0Aview%20synthesis%20in%20driving%20scenes%20due%20to%20the%20lack%20of%20crafty%20design%20and%20geometric%0Aconstraints%20of%20related%20elements.%20This%20paper%20introduces%20a%20novel%20method%20called%0ADecoupled%20Hybrid%20Gaussian%20Splatting%20%28DHGS%29%2C%20which%20aims%20at%20promoting%20the%0Arendering%20quality%20of%20novel%20view%20synthesis%20for%20driving%20scenes.%20The%20novelty%20of%0Athis%20work%20lies%20in%20the%20decoupled%20and%20hybrid%20pixel-level%20blender%20for%20road%20and%0Anon-road%20layers%2C%20without%20conventional%20unified%20differentiable%20rendering%20logic%0Afor%20the%20entire%20scene%2C%20meanwhile%20maintaining%20consistent%20and%20continuous%0Asuperimposition%20through%20the%20proposed%20depth-ordered%20rendering%20strategy.%20Beyond%0Athat%2C%20an%20implicit%20road%20representation%20comprised%20of%20Signed%20Distance%20Field%20%28SDF%29%0Ais%20trained%20to%20supervise%20the%20road%20surface%20with%20subtle%20geometric%20attributes.%0AAccompanied%20by%20the%20use%20of%20auxiliary%20transmittance%20loss%20and%20consistency%20loss%2C%0Anovel%20images%20with%20imperceptible%20boundary%20and%20elevated%20fidelity%20are%20ultimately%0Aobtained.%20Substantial%20experiments%20on%20Waymo%20dataset%20prove%20that%20DHGS%20outperforms%0Athe%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDHGS%253A%2520Decoupled%2520Hybrid%2520Gaussian%2520Splatting%2520for%2520Driving%2520Scene%26entry.906535625%3DXi%2520Shi%2520and%2520Lingli%2520Chen%2520and%2520Peng%2520Wei%2520and%2520Xi%2520Wu%2520and%2520Tian%2520Jiang%2520and%2520Yonggang%2520Luo%2520and%2520Lecheng%2520Xie%26entry.1292438233%3D%2520%2520Existing%2520Gaussian%2520splatting%2520methods%2520struggle%2520to%2520achieve%2520satisfactory%2520novel%250Aview%2520synthesis%2520in%2520driving%2520scenes%2520due%2520to%2520the%2520lack%2520of%2520crafty%2520design%2520and%2520geometric%250Aconstraints%2520of%2520related%2520elements.%2520This%2520paper%2520introduces%2520a%2520novel%2520method%2520called%250ADecoupled%2520Hybrid%2520Gaussian%2520Splatting%2520%2528DHGS%2529%252C%2520which%2520aims%2520at%2520promoting%2520the%250Arendering%2520quality%2520of%2520novel%2520view%2520synthesis%2520for%2520driving%2520scenes.%2520The%2520novelty%2520of%250Athis%2520work%2520lies%2520in%2520the%2520decoupled%2520and%2520hybrid%2520pixel-level%2520blender%2520for%2520road%2520and%250Anon-road%2520layers%252C%2520without%2520conventional%2520unified%2520differentiable%2520rendering%2520logic%250Afor%2520the%2520entire%2520scene%252C%2520meanwhile%2520maintaining%2520consistent%2520and%2520continuous%250Asuperimposition%2520through%2520the%2520proposed%2520depth-ordered%2520rendering%2520strategy.%2520Beyond%250Athat%252C%2520an%2520implicit%2520road%2520representation%2520comprised%2520of%2520Signed%2520Distance%2520Field%2520%2528SDF%2529%250Ais%2520trained%2520to%2520supervise%2520the%2520road%2520surface%2520with%2520subtle%2520geometric%2520attributes.%250AAccompanied%2520by%2520the%2520use%2520of%2520auxiliary%2520transmittance%2520loss%2520and%2520consistency%2520loss%252C%250Anovel%2520images%2520with%2520imperceptible%2520boundary%2520and%2520elevated%2520fidelity%2520are%2520ultimately%250Aobtained.%2520Substantial%2520experiments%2520on%2520Waymo%2520dataset%2520prove%2520that%2520DHGS%2520outperforms%250Athe%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DHGS%3A%20Decoupled%20Hybrid%20Gaussian%20Splatting%20for%20Driving%20Scene&entry.906535625=Xi%20Shi%20and%20Lingli%20Chen%20and%20Peng%20Wei%20and%20Xi%20Wu%20and%20Tian%20Jiang%20and%20Yonggang%20Luo%20and%20Lecheng%20Xie&entry.1292438233=%20%20Existing%20Gaussian%20splatting%20methods%20struggle%20to%20achieve%20satisfactory%20novel%0Aview%20synthesis%20in%20driving%20scenes%20due%20to%20the%20lack%20of%20crafty%20design%20and%20geometric%0Aconstraints%20of%20related%20elements.%20This%20paper%20introduces%20a%20novel%20method%20called%0ADecoupled%20Hybrid%20Gaussian%20Splatting%20%28DHGS%29%2C%20which%20aims%20at%20promoting%20the%0Arendering%20quality%20of%20novel%20view%20synthesis%20for%20driving%20scenes.%20The%20novelty%20of%0Athis%20work%20lies%20in%20the%20decoupled%20and%20hybrid%20pixel-level%20blender%20for%20road%20and%0Anon-road%20layers%2C%20without%20conventional%20unified%20differentiable%20rendering%20logic%0Afor%20the%20entire%20scene%2C%20meanwhile%20maintaining%20consistent%20and%20continuous%0Asuperimposition%20through%20the%20proposed%20depth-ordered%20rendering%20strategy.%20Beyond%0Athat%2C%20an%20implicit%20road%20representation%20comprised%20of%20Signed%20Distance%20Field%20%28SDF%29%0Ais%20trained%20to%20supervise%20the%20road%20surface%20with%20subtle%20geometric%20attributes.%0AAccompanied%20by%20the%20use%20of%20auxiliary%20transmittance%20loss%20and%20consistency%20loss%2C%0Anovel%20images%20with%20imperceptible%20boundary%20and%20elevated%20fidelity%20are%20ultimately%0Aobtained.%20Substantial%20experiments%20on%20Waymo%20dataset%20prove%20that%20DHGS%20outperforms%0Athe%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16600v1&entry.124074799=Read"},
{"title": "Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition", "author": "Jinfu Liu and Chen Chen and Mengyuan Liu", "abstract": "  Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.\n", "link": "http://arxiv.org/abs/2407.15706v2", "date": "2024-07-23", "relevancy": 2.34, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6207}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5714}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modality%20Co-Learning%20for%20Efficient%20Skeleton-based%20Action%0A%20%20Recognition&body=Title%3A%20Multi-Modality%20Co-Learning%20for%20Efficient%20Skeleton-based%20Action%0A%20%20Recognition%0AAuthor%3A%20Jinfu%20Liu%20and%20Chen%20Chen%20and%20Mengyuan%20Liu%0AAbstract%3A%20%20%20Skeleton-based%20action%20recognition%20has%20garnered%20significant%20attention%20due%20to%0Athe%20utilization%20of%20concise%20and%20resilient%20skeletons.%20Nevertheless%2C%20the%20absence%0Aof%20detailed%20body%20information%20in%20skeletons%20restricts%20performance%2C%20while%20other%0Amultimodal%20methods%20require%20substantial%20inference%20resources%20and%20are%20inefficient%0Awhen%20using%20multimodal%20data%20during%20both%20training%20and%20inference%20stages.%20To%0Aaddress%20this%20and%20fully%20harness%20the%20complementary%20multimodal%20features%2C%20we%0Apropose%20a%20novel%20multi-modality%20co-learning%20%28MMCL%29%20framework%20by%20leveraging%20the%0Amultimodal%20large%20language%20models%20%28LLMs%29%20as%20auxiliary%20networks%20for%20efficient%0Askeleton-based%20action%20recognition%2C%20which%20engages%20in%20multi-modality%20co-learning%0Aduring%20the%20training%20stage%20and%20keeps%20efficiency%20by%20employing%20only%20concise%0Askeletons%20in%20inference.%20Our%20MMCL%20framework%20primarily%20consists%20of%20two%20modules.%0AFirst%2C%20the%20Feature%20Alignment%20Module%20%28FAM%29%20extracts%20rich%20RGB%20features%20from%20video%0Aframes%20and%20aligns%20them%20with%20global%20skeleton%20features%20via%20contrastive%20learning.%0ASecond%2C%20the%20Feature%20Refinement%20Module%20%28FRM%29%20uses%20RGB%20images%20with%20temporal%0Ainformation%20and%20text%20instruction%20to%20generate%20instructive%20features%20based%20on%20the%0Apowerful%20generalization%20of%20multimodal%20LLMs.%20These%20instructive%20text%20features%0Awill%20further%20refine%20the%20classification%20scores%20and%20the%20refined%20scores%20will%0Aenhance%20the%20model%27s%20robustness%20and%20generalization%20in%20a%20manner%20similar%20to%20soft%0Alabels.%20Extensive%20experiments%20on%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%20and%20Northwestern-UCLA%0Abenchmarks%20consistently%20verify%20the%20effectiveness%20of%20our%20MMCL%2C%20which%20outperforms%0Athe%20existing%20skeleton-based%20action%20recognition%20methods.%20Meanwhile%2C%20experiments%0Aon%20UTD-MHAD%20and%20SYSU-Action%20datasets%20demonstrate%20the%20commendable%20generalization%0Aof%20our%20MMCL%20in%20zero-shot%20and%20domain-adaptive%20action%20recognition.%20Our%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/liujf69/MMCL-Action.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15706v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modality%2520Co-Learning%2520for%2520Efficient%2520Skeleton-based%2520Action%250A%2520%2520Recognition%26entry.906535625%3DJinfu%2520Liu%2520and%2520Chen%2520Chen%2520and%2520Mengyuan%2520Liu%26entry.1292438233%3D%2520%2520Skeleton-based%2520action%2520recognition%2520has%2520garnered%2520significant%2520attention%2520due%2520to%250Athe%2520utilization%2520of%2520concise%2520and%2520resilient%2520skeletons.%2520Nevertheless%252C%2520the%2520absence%250Aof%2520detailed%2520body%2520information%2520in%2520skeletons%2520restricts%2520performance%252C%2520while%2520other%250Amultimodal%2520methods%2520require%2520substantial%2520inference%2520resources%2520and%2520are%2520inefficient%250Awhen%2520using%2520multimodal%2520data%2520during%2520both%2520training%2520and%2520inference%2520stages.%2520To%250Aaddress%2520this%2520and%2520fully%2520harness%2520the%2520complementary%2520multimodal%2520features%252C%2520we%250Apropose%2520a%2520novel%2520multi-modality%2520co-learning%2520%2528MMCL%2529%2520framework%2520by%2520leveraging%2520the%250Amultimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520auxiliary%2520networks%2520for%2520efficient%250Askeleton-based%2520action%2520recognition%252C%2520which%2520engages%2520in%2520multi-modality%2520co-learning%250Aduring%2520the%2520training%2520stage%2520and%2520keeps%2520efficiency%2520by%2520employing%2520only%2520concise%250Askeletons%2520in%2520inference.%2520Our%2520MMCL%2520framework%2520primarily%2520consists%2520of%2520two%2520modules.%250AFirst%252C%2520the%2520Feature%2520Alignment%2520Module%2520%2528FAM%2529%2520extracts%2520rich%2520RGB%2520features%2520from%2520video%250Aframes%2520and%2520aligns%2520them%2520with%2520global%2520skeleton%2520features%2520via%2520contrastive%2520learning.%250ASecond%252C%2520the%2520Feature%2520Refinement%2520Module%2520%2528FRM%2529%2520uses%2520RGB%2520images%2520with%2520temporal%250Ainformation%2520and%2520text%2520instruction%2520to%2520generate%2520instructive%2520features%2520based%2520on%2520the%250Apowerful%2520generalization%2520of%2520multimodal%2520LLMs.%2520These%2520instructive%2520text%2520features%250Awill%2520further%2520refine%2520the%2520classification%2520scores%2520and%2520the%2520refined%2520scores%2520will%250Aenhance%2520the%2520model%2527s%2520robustness%2520and%2520generalization%2520in%2520a%2520manner%2520similar%2520to%2520soft%250Alabels.%2520Extensive%2520experiments%2520on%2520NTU%2520RGB%252BD%252C%2520NTU%2520RGB%252BD%2520120%2520and%2520Northwestern-UCLA%250Abenchmarks%2520consistently%2520verify%2520the%2520effectiveness%2520of%2520our%2520MMCL%252C%2520which%2520outperforms%250Athe%2520existing%2520skeleton-based%2520action%2520recognition%2520methods.%2520Meanwhile%252C%2520experiments%250Aon%2520UTD-MHAD%2520and%2520SYSU-Action%2520datasets%2520demonstrate%2520the%2520commendable%2520generalization%250Aof%2520our%2520MMCL%2520in%2520zero-shot%2520and%2520domain-adaptive%2520action%2520recognition.%2520Our%2520code%2520is%250Apublicly%2520available%2520at%253A%2520https%253A//github.com/liujf69/MMCL-Action.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15706v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modality%20Co-Learning%20for%20Efficient%20Skeleton-based%20Action%0A%20%20Recognition&entry.906535625=Jinfu%20Liu%20and%20Chen%20Chen%20and%20Mengyuan%20Liu&entry.1292438233=%20%20Skeleton-based%20action%20recognition%20has%20garnered%20significant%20attention%20due%20to%0Athe%20utilization%20of%20concise%20and%20resilient%20skeletons.%20Nevertheless%2C%20the%20absence%0Aof%20detailed%20body%20information%20in%20skeletons%20restricts%20performance%2C%20while%20other%0Amultimodal%20methods%20require%20substantial%20inference%20resources%20and%20are%20inefficient%0Awhen%20using%20multimodal%20data%20during%20both%20training%20and%20inference%20stages.%20To%0Aaddress%20this%20and%20fully%20harness%20the%20complementary%20multimodal%20features%2C%20we%0Apropose%20a%20novel%20multi-modality%20co-learning%20%28MMCL%29%20framework%20by%20leveraging%20the%0Amultimodal%20large%20language%20models%20%28LLMs%29%20as%20auxiliary%20networks%20for%20efficient%0Askeleton-based%20action%20recognition%2C%20which%20engages%20in%20multi-modality%20co-learning%0Aduring%20the%20training%20stage%20and%20keeps%20efficiency%20by%20employing%20only%20concise%0Askeletons%20in%20inference.%20Our%20MMCL%20framework%20primarily%20consists%20of%20two%20modules.%0AFirst%2C%20the%20Feature%20Alignment%20Module%20%28FAM%29%20extracts%20rich%20RGB%20features%20from%20video%0Aframes%20and%20aligns%20them%20with%20global%20skeleton%20features%20via%20contrastive%20learning.%0ASecond%2C%20the%20Feature%20Refinement%20Module%20%28FRM%29%20uses%20RGB%20images%20with%20temporal%0Ainformation%20and%20text%20instruction%20to%20generate%20instructive%20features%20based%20on%20the%0Apowerful%20generalization%20of%20multimodal%20LLMs.%20These%20instructive%20text%20features%0Awill%20further%20refine%20the%20classification%20scores%20and%20the%20refined%20scores%20will%0Aenhance%20the%20model%27s%20robustness%20and%20generalization%20in%20a%20manner%20similar%20to%20soft%0Alabels.%20Extensive%20experiments%20on%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%20and%20Northwestern-UCLA%0Abenchmarks%20consistently%20verify%20the%20effectiveness%20of%20our%20MMCL%2C%20which%20outperforms%0Athe%20existing%20skeleton-based%20action%20recognition%20methods.%20Meanwhile%2C%20experiments%0Aon%20UTD-MHAD%20and%20SYSU-Action%20datasets%20demonstrate%20the%20commendable%20generalization%0Aof%20our%20MMCL%20in%20zero-shot%20and%20domain-adaptive%20action%20recognition.%20Our%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/liujf69/MMCL-Action.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15706v2&entry.124074799=Read"},
{"title": "MapsTP: HD Map Images Based Multimodal Trajectory Prediction for\n  Automated Vehicles", "author": "Sushil Sharma and Arindam Das and Ganesh Sistu and Mark Halton and Ciar\u00e1n Eising", "abstract": "  Predicting ego vehicle trajectories remains a critical challenge, especially\nin urban and dense areas due to the unpredictable behaviours of other vehicles\nand pedestrians. Multimodal trajectory prediction enhances decision-making by\nconsidering multiple possible future trajectories based on diverse sources of\nenvironmental data. In this approach, we leverage ResNet-50 to extract image\nfeatures from high-definition map data and use IMU sensor data to calculate\nspeed, acceleration, and yaw rate. A temporal probabilistic network is employed\nto compute potential trajectories, selecting the most accurate and highly\nprobable trajectory paths. This method integrates HD map data to improve the\nrobustness and reliability of trajectory predictions for autonomous vehicles.\n", "link": "http://arxiv.org/abs/2407.05811v2", "date": "2024-07-23", "relevancy": 2.3382, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6148}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6051}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapsTP%3A%20HD%20Map%20Images%20Based%20Multimodal%20Trajectory%20Prediction%20for%0A%20%20Automated%20Vehicles&body=Title%3A%20MapsTP%3A%20HD%20Map%20Images%20Based%20Multimodal%20Trajectory%20Prediction%20for%0A%20%20Automated%20Vehicles%0AAuthor%3A%20Sushil%20Sharma%20and%20Arindam%20Das%20and%20Ganesh%20Sistu%20and%20Mark%20Halton%20and%20Ciar%C3%A1n%20Eising%0AAbstract%3A%20%20%20Predicting%20ego%20vehicle%20trajectories%20remains%20a%20critical%20challenge%2C%20especially%0Ain%20urban%20and%20dense%20areas%20due%20to%20the%20unpredictable%20behaviours%20of%20other%20vehicles%0Aand%20pedestrians.%20Multimodal%20trajectory%20prediction%20enhances%20decision-making%20by%0Aconsidering%20multiple%20possible%20future%20trajectories%20based%20on%20diverse%20sources%20of%0Aenvironmental%20data.%20In%20this%20approach%2C%20we%20leverage%20ResNet-50%20to%20extract%20image%0Afeatures%20from%20high-definition%20map%20data%20and%20use%20IMU%20sensor%20data%20to%20calculate%0Aspeed%2C%20acceleration%2C%20and%20yaw%20rate.%20A%20temporal%20probabilistic%20network%20is%20employed%0Ato%20compute%20potential%20trajectories%2C%20selecting%20the%20most%20accurate%20and%20highly%0Aprobable%20trajectory%20paths.%20This%20method%20integrates%20HD%20map%20data%20to%20improve%20the%0Arobustness%20and%20reliability%20of%20trajectory%20predictions%20for%20autonomous%20vehicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05811v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapsTP%253A%2520HD%2520Map%2520Images%2520Based%2520Multimodal%2520Trajectory%2520Prediction%2520for%250A%2520%2520Automated%2520Vehicles%26entry.906535625%3DSushil%2520Sharma%2520and%2520Arindam%2520Das%2520and%2520Ganesh%2520Sistu%2520and%2520Mark%2520Halton%2520and%2520Ciar%25C3%25A1n%2520Eising%26entry.1292438233%3D%2520%2520Predicting%2520ego%2520vehicle%2520trajectories%2520remains%2520a%2520critical%2520challenge%252C%2520especially%250Ain%2520urban%2520and%2520dense%2520areas%2520due%2520to%2520the%2520unpredictable%2520behaviours%2520of%2520other%2520vehicles%250Aand%2520pedestrians.%2520Multimodal%2520trajectory%2520prediction%2520enhances%2520decision-making%2520by%250Aconsidering%2520multiple%2520possible%2520future%2520trajectories%2520based%2520on%2520diverse%2520sources%2520of%250Aenvironmental%2520data.%2520In%2520this%2520approach%252C%2520we%2520leverage%2520ResNet-50%2520to%2520extract%2520image%250Afeatures%2520from%2520high-definition%2520map%2520data%2520and%2520use%2520IMU%2520sensor%2520data%2520to%2520calculate%250Aspeed%252C%2520acceleration%252C%2520and%2520yaw%2520rate.%2520A%2520temporal%2520probabilistic%2520network%2520is%2520employed%250Ato%2520compute%2520potential%2520trajectories%252C%2520selecting%2520the%2520most%2520accurate%2520and%2520highly%250Aprobable%2520trajectory%2520paths.%2520This%2520method%2520integrates%2520HD%2520map%2520data%2520to%2520improve%2520the%250Arobustness%2520and%2520reliability%2520of%2520trajectory%2520predictions%2520for%2520autonomous%2520vehicles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05811v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapsTP%3A%20HD%20Map%20Images%20Based%20Multimodal%20Trajectory%20Prediction%20for%0A%20%20Automated%20Vehicles&entry.906535625=Sushil%20Sharma%20and%20Arindam%20Das%20and%20Ganesh%20Sistu%20and%20Mark%20Halton%20and%20Ciar%C3%A1n%20Eising&entry.1292438233=%20%20Predicting%20ego%20vehicle%20trajectories%20remains%20a%20critical%20challenge%2C%20especially%0Ain%20urban%20and%20dense%20areas%20due%20to%20the%20unpredictable%20behaviours%20of%20other%20vehicles%0Aand%20pedestrians.%20Multimodal%20trajectory%20prediction%20enhances%20decision-making%20by%0Aconsidering%20multiple%20possible%20future%20trajectories%20based%20on%20diverse%20sources%20of%0Aenvironmental%20data.%20In%20this%20approach%2C%20we%20leverage%20ResNet-50%20to%20extract%20image%0Afeatures%20from%20high-definition%20map%20data%20and%20use%20IMU%20sensor%20data%20to%20calculate%0Aspeed%2C%20acceleration%2C%20and%20yaw%20rate.%20A%20temporal%20probabilistic%20network%20is%20employed%0Ato%20compute%20potential%20trajectories%2C%20selecting%20the%20most%20accurate%20and%20highly%0Aprobable%20trajectory%20paths.%20This%20method%20integrates%20HD%20map%20data%20to%20improve%20the%0Arobustness%20and%20reliability%20of%20trajectory%20predictions%20for%20autonomous%20vehicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05811v2&entry.124074799=Read"},
{"title": "Velocity Driven Vision: Asynchronous Sensor Fusion Birds Eye View Models\n  for Autonomous Vehicles", "author": "Seamie Hayes and Sushil Sharma and Ciar\u00e1n Eising", "abstract": "  Fusing different sensor modalities can be a difficult task, particularly if\nthey are asynchronous. Asynchronisation may arise due to long processing times\nor improper synchronisation during calibration, and there must exist a way to\nstill utilise this previous information for the purpose of safe driving, and\nobject detection in ego vehicle/ multi-agent trajectory prediction.\nDifficulties arise in the fact that the sensor modalities have captured\ninformation at different times and also at different positions in space.\nTherefore, they are not spatially nor temporally aligned. This paper will\ninvestigate the challenge of radar and LiDAR sensors being asynchronous\nrelative to the camera sensors, for various time latencies. The spatial\nalignment will be resolved before lifting into BEV space via the transformation\nof the radar/LiDAR point clouds into the new ego frame coordinate system. Only\nafter this can we concatenate the radar/LiDAR point cloud and lifted camera\nfeatures. Temporal alignment will be remedied for radar data only, we will\nimplement a novel method of inferring the future radar point positions using\nthe velocity information. Our approach to resolving the issue of sensor\nasynchrony yields promising results. We demonstrate velocity information can\ndrastically improve IoU for asynchronous datasets, as for a time latency of 360\nmilliseconds (ms), IoU improves from 49.54 to 53.63. Additionally, for a time\nlatency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR\n(C+L) model by 0.18 IoU. This is an advancement in utilising the\noften-neglected radar sensor modality, which is less favoured than LiDAR for\nautonomous driving purposes.\n", "link": "http://arxiv.org/abs/2407.16636v1", "date": "2024-07-23", "relevancy": 2.3315, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5944}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5862}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Velocity%20Driven%20Vision%3A%20Asynchronous%20Sensor%20Fusion%20Birds%20Eye%20View%20Models%0A%20%20for%20Autonomous%20Vehicles&body=Title%3A%20Velocity%20Driven%20Vision%3A%20Asynchronous%20Sensor%20Fusion%20Birds%20Eye%20View%20Models%0A%20%20for%20Autonomous%20Vehicles%0AAuthor%3A%20Seamie%20Hayes%20and%20Sushil%20Sharma%20and%20Ciar%C3%A1n%20Eising%0AAbstract%3A%20%20%20Fusing%20different%20sensor%20modalities%20can%20be%20a%20difficult%20task%2C%20particularly%20if%0Athey%20are%20asynchronous.%20Asynchronisation%20may%20arise%20due%20to%20long%20processing%20times%0Aor%20improper%20synchronisation%20during%20calibration%2C%20and%20there%20must%20exist%20a%20way%20to%0Astill%20utilise%20this%20previous%20information%20for%20the%20purpose%20of%20safe%20driving%2C%20and%0Aobject%20detection%20in%20ego%20vehicle/%20multi-agent%20trajectory%20prediction.%0ADifficulties%20arise%20in%20the%20fact%20that%20the%20sensor%20modalities%20have%20captured%0Ainformation%20at%20different%20times%20and%20also%20at%20different%20positions%20in%20space.%0ATherefore%2C%20they%20are%20not%20spatially%20nor%20temporally%20aligned.%20This%20paper%20will%0Ainvestigate%20the%20challenge%20of%20radar%20and%20LiDAR%20sensors%20being%20asynchronous%0Arelative%20to%20the%20camera%20sensors%2C%20for%20various%20time%20latencies.%20The%20spatial%0Aalignment%20will%20be%20resolved%20before%20lifting%20into%20BEV%20space%20via%20the%20transformation%0Aof%20the%20radar/LiDAR%20point%20clouds%20into%20the%20new%20ego%20frame%20coordinate%20system.%20Only%0Aafter%20this%20can%20we%20concatenate%20the%20radar/LiDAR%20point%20cloud%20and%20lifted%20camera%0Afeatures.%20Temporal%20alignment%20will%20be%20remedied%20for%20radar%20data%20only%2C%20we%20will%0Aimplement%20a%20novel%20method%20of%20inferring%20the%20future%20radar%20point%20positions%20using%0Athe%20velocity%20information.%20Our%20approach%20to%20resolving%20the%20issue%20of%20sensor%0Aasynchrony%20yields%20promising%20results.%20We%20demonstrate%20velocity%20information%20can%0Adrastically%20improve%20IoU%20for%20asynchronous%20datasets%2C%20as%20for%20a%20time%20latency%20of%20360%0Amilliseconds%20%28ms%29%2C%20IoU%20improves%20from%2049.54%20to%2053.63.%20Additionally%2C%20for%20a%20time%0Alatency%20of%20550ms%2C%20the%20camera%2Bradar%20%28C%2BR%29%20model%20outperforms%20the%20camera%2BLiDAR%0A%28C%2BL%29%20model%20by%200.18%20IoU.%20This%20is%20an%20advancement%20in%20utilising%20the%0Aoften-neglected%20radar%20sensor%20modality%2C%20which%20is%20less%20favoured%20than%20LiDAR%20for%0Aautonomous%20driving%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVelocity%2520Driven%2520Vision%253A%2520Asynchronous%2520Sensor%2520Fusion%2520Birds%2520Eye%2520View%2520Models%250A%2520%2520for%2520Autonomous%2520Vehicles%26entry.906535625%3DSeamie%2520Hayes%2520and%2520Sushil%2520Sharma%2520and%2520Ciar%25C3%25A1n%2520Eising%26entry.1292438233%3D%2520%2520Fusing%2520different%2520sensor%2520modalities%2520can%2520be%2520a%2520difficult%2520task%252C%2520particularly%2520if%250Athey%2520are%2520asynchronous.%2520Asynchronisation%2520may%2520arise%2520due%2520to%2520long%2520processing%2520times%250Aor%2520improper%2520synchronisation%2520during%2520calibration%252C%2520and%2520there%2520must%2520exist%2520a%2520way%2520to%250Astill%2520utilise%2520this%2520previous%2520information%2520for%2520the%2520purpose%2520of%2520safe%2520driving%252C%2520and%250Aobject%2520detection%2520in%2520ego%2520vehicle/%2520multi-agent%2520trajectory%2520prediction.%250ADifficulties%2520arise%2520in%2520the%2520fact%2520that%2520the%2520sensor%2520modalities%2520have%2520captured%250Ainformation%2520at%2520different%2520times%2520and%2520also%2520at%2520different%2520positions%2520in%2520space.%250ATherefore%252C%2520they%2520are%2520not%2520spatially%2520nor%2520temporally%2520aligned.%2520This%2520paper%2520will%250Ainvestigate%2520the%2520challenge%2520of%2520radar%2520and%2520LiDAR%2520sensors%2520being%2520asynchronous%250Arelative%2520to%2520the%2520camera%2520sensors%252C%2520for%2520various%2520time%2520latencies.%2520The%2520spatial%250Aalignment%2520will%2520be%2520resolved%2520before%2520lifting%2520into%2520BEV%2520space%2520via%2520the%2520transformation%250Aof%2520the%2520radar/LiDAR%2520point%2520clouds%2520into%2520the%2520new%2520ego%2520frame%2520coordinate%2520system.%2520Only%250Aafter%2520this%2520can%2520we%2520concatenate%2520the%2520radar/LiDAR%2520point%2520cloud%2520and%2520lifted%2520camera%250Afeatures.%2520Temporal%2520alignment%2520will%2520be%2520remedied%2520for%2520radar%2520data%2520only%252C%2520we%2520will%250Aimplement%2520a%2520novel%2520method%2520of%2520inferring%2520the%2520future%2520radar%2520point%2520positions%2520using%250Athe%2520velocity%2520information.%2520Our%2520approach%2520to%2520resolving%2520the%2520issue%2520of%2520sensor%250Aasynchrony%2520yields%2520promising%2520results.%2520We%2520demonstrate%2520velocity%2520information%2520can%250Adrastically%2520improve%2520IoU%2520for%2520asynchronous%2520datasets%252C%2520as%2520for%2520a%2520time%2520latency%2520of%2520360%250Amilliseconds%2520%2528ms%2529%252C%2520IoU%2520improves%2520from%252049.54%2520to%252053.63.%2520Additionally%252C%2520for%2520a%2520time%250Alatency%2520of%2520550ms%252C%2520the%2520camera%252Bradar%2520%2528C%252BR%2529%2520model%2520outperforms%2520the%2520camera%252BLiDAR%250A%2528C%252BL%2529%2520model%2520by%25200.18%2520IoU.%2520This%2520is%2520an%2520advancement%2520in%2520utilising%2520the%250Aoften-neglected%2520radar%2520sensor%2520modality%252C%2520which%2520is%2520less%2520favoured%2520than%2520LiDAR%2520for%250Aautonomous%2520driving%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Velocity%20Driven%20Vision%3A%20Asynchronous%20Sensor%20Fusion%20Birds%20Eye%20View%20Models%0A%20%20for%20Autonomous%20Vehicles&entry.906535625=Seamie%20Hayes%20and%20Sushil%20Sharma%20and%20Ciar%C3%A1n%20Eising&entry.1292438233=%20%20Fusing%20different%20sensor%20modalities%20can%20be%20a%20difficult%20task%2C%20particularly%20if%0Athey%20are%20asynchronous.%20Asynchronisation%20may%20arise%20due%20to%20long%20processing%20times%0Aor%20improper%20synchronisation%20during%20calibration%2C%20and%20there%20must%20exist%20a%20way%20to%0Astill%20utilise%20this%20previous%20information%20for%20the%20purpose%20of%20safe%20driving%2C%20and%0Aobject%20detection%20in%20ego%20vehicle/%20multi-agent%20trajectory%20prediction.%0ADifficulties%20arise%20in%20the%20fact%20that%20the%20sensor%20modalities%20have%20captured%0Ainformation%20at%20different%20times%20and%20also%20at%20different%20positions%20in%20space.%0ATherefore%2C%20they%20are%20not%20spatially%20nor%20temporally%20aligned.%20This%20paper%20will%0Ainvestigate%20the%20challenge%20of%20radar%20and%20LiDAR%20sensors%20being%20asynchronous%0Arelative%20to%20the%20camera%20sensors%2C%20for%20various%20time%20latencies.%20The%20spatial%0Aalignment%20will%20be%20resolved%20before%20lifting%20into%20BEV%20space%20via%20the%20transformation%0Aof%20the%20radar/LiDAR%20point%20clouds%20into%20the%20new%20ego%20frame%20coordinate%20system.%20Only%0Aafter%20this%20can%20we%20concatenate%20the%20radar/LiDAR%20point%20cloud%20and%20lifted%20camera%0Afeatures.%20Temporal%20alignment%20will%20be%20remedied%20for%20radar%20data%20only%2C%20we%20will%0Aimplement%20a%20novel%20method%20of%20inferring%20the%20future%20radar%20point%20positions%20using%0Athe%20velocity%20information.%20Our%20approach%20to%20resolving%20the%20issue%20of%20sensor%0Aasynchrony%20yields%20promising%20results.%20We%20demonstrate%20velocity%20information%20can%0Adrastically%20improve%20IoU%20for%20asynchronous%20datasets%2C%20as%20for%20a%20time%20latency%20of%20360%0Amilliseconds%20%28ms%29%2C%20IoU%20improves%20from%2049.54%20to%2053.63.%20Additionally%2C%20for%20a%20time%0Alatency%20of%20550ms%2C%20the%20camera%2Bradar%20%28C%2BR%29%20model%20outperforms%20the%20camera%2BLiDAR%0A%28C%2BL%29%20model%20by%200.18%20IoU.%20This%20is%20an%20advancement%20in%20utilising%20the%0Aoften-neglected%20radar%20sensor%20modality%2C%20which%20is%20less%20favoured%20than%20LiDAR%20for%0Aautonomous%20driving%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16636v1&entry.124074799=Read"},
{"title": "On Differentially Private 3D Medical Image Synthesis with Controllable\n  Latent Diffusion Models", "author": "Deniz Daum and Richard Osuala and Anneliese Riess and Georgios Kaissis and Julia A. Schnabel and Maxime Di Folco", "abstract": "  Generally, the small size of public medical imaging datasets coupled with\nstringent privacy concerns, hampers the advancement of data-hungry deep\nlearning models in medical imaging. This study addresses these challenges for\n3D cardiac MRI images in the short-axis view. We propose Latent Diffusion\nModels that generate synthetic images conditioned on medical attributes, while\nensuring patient privacy through differentially private model training. To our\nknowledge, this is the first work to apply and quantify differential privacy in\n3D medical image generation. We pre-train our models on public data and\nfinetune them with differential privacy on the UK Biobank dataset. Our\nexperiments reveal that pre-training significantly improves model performance,\nachieving a Fr\\'echet Inception Distance (FID) of 26.77 at $\\epsilon=10$,\ncompared to 92.52 for models without pre-training. Additionally, we explore the\ntrade-off between privacy constraints and image quality, investigating how\ntighter privacy budgets affect output controllability and may lead to degraded\nperformance. Our results demonstrate that proper consideration during training\nwith differential privacy can substantially improve the quality of synthetic\ncardiac MRI images, but there are still notable challenges in achieving\nconsistent medical realism.\n", "link": "http://arxiv.org/abs/2407.16405v1", "date": "2024-07-23", "relevancy": 2.2942, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6288}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5625}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Differentially%20Private%203D%20Medical%20Image%20Synthesis%20with%20Controllable%0A%20%20Latent%20Diffusion%20Models&body=Title%3A%20On%20Differentially%20Private%203D%20Medical%20Image%20Synthesis%20with%20Controllable%0A%20%20Latent%20Diffusion%20Models%0AAuthor%3A%20Deniz%20Daum%20and%20Richard%20Osuala%20and%20Anneliese%20Riess%20and%20Georgios%20Kaissis%20and%20Julia%20A.%20Schnabel%20and%20Maxime%20Di%20Folco%0AAbstract%3A%20%20%20Generally%2C%20the%20small%20size%20of%20public%20medical%20imaging%20datasets%20coupled%20with%0Astringent%20privacy%20concerns%2C%20hampers%20the%20advancement%20of%20data-hungry%20deep%0Alearning%20models%20in%20medical%20imaging.%20This%20study%20addresses%20these%20challenges%20for%0A3D%20cardiac%20MRI%20images%20in%20the%20short-axis%20view.%20We%20propose%20Latent%20Diffusion%0AModels%20that%20generate%20synthetic%20images%20conditioned%20on%20medical%20attributes%2C%20while%0Aensuring%20patient%20privacy%20through%20differentially%20private%20model%20training.%20To%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20to%20apply%20and%20quantify%20differential%20privacy%20in%0A3D%20medical%20image%20generation.%20We%20pre-train%20our%20models%20on%20public%20data%20and%0Afinetune%20them%20with%20differential%20privacy%20on%20the%20UK%20Biobank%20dataset.%20Our%0Aexperiments%20reveal%20that%20pre-training%20significantly%20improves%20model%20performance%2C%0Aachieving%20a%20Fr%5C%27echet%20Inception%20Distance%20%28FID%29%20of%2026.77%20at%20%24%5Cepsilon%3D10%24%2C%0Acompared%20to%2092.52%20for%20models%20without%20pre-training.%20Additionally%2C%20we%20explore%20the%0Atrade-off%20between%20privacy%20constraints%20and%20image%20quality%2C%20investigating%20how%0Atighter%20privacy%20budgets%20affect%20output%20controllability%20and%20may%20lead%20to%20degraded%0Aperformance.%20Our%20results%20demonstrate%20that%20proper%20consideration%20during%20training%0Awith%20differential%20privacy%20can%20substantially%20improve%20the%20quality%20of%20synthetic%0Acardiac%20MRI%20images%2C%20but%20there%20are%20still%20notable%20challenges%20in%20achieving%0Aconsistent%20medical%20realism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Differentially%2520Private%25203D%2520Medical%2520Image%2520Synthesis%2520with%2520Controllable%250A%2520%2520Latent%2520Diffusion%2520Models%26entry.906535625%3DDeniz%2520Daum%2520and%2520Richard%2520Osuala%2520and%2520Anneliese%2520Riess%2520and%2520Georgios%2520Kaissis%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Maxime%2520Di%2520Folco%26entry.1292438233%3D%2520%2520Generally%252C%2520the%2520small%2520size%2520of%2520public%2520medical%2520imaging%2520datasets%2520coupled%2520with%250Astringent%2520privacy%2520concerns%252C%2520hampers%2520the%2520advancement%2520of%2520data-hungry%2520deep%250Alearning%2520models%2520in%2520medical%2520imaging.%2520This%2520study%2520addresses%2520these%2520challenges%2520for%250A3D%2520cardiac%2520MRI%2520images%2520in%2520the%2520short-axis%2520view.%2520We%2520propose%2520Latent%2520Diffusion%250AModels%2520that%2520generate%2520synthetic%2520images%2520conditioned%2520on%2520medical%2520attributes%252C%2520while%250Aensuring%2520patient%2520privacy%2520through%2520differentially%2520private%2520model%2520training.%2520To%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520apply%2520and%2520quantify%2520differential%2520privacy%2520in%250A3D%2520medical%2520image%2520generation.%2520We%2520pre-train%2520our%2520models%2520on%2520public%2520data%2520and%250Afinetune%2520them%2520with%2520differential%2520privacy%2520on%2520the%2520UK%2520Biobank%2520dataset.%2520Our%250Aexperiments%2520reveal%2520that%2520pre-training%2520significantly%2520improves%2520model%2520performance%252C%250Aachieving%2520a%2520Fr%255C%2527echet%2520Inception%2520Distance%2520%2528FID%2529%2520of%252026.77%2520at%2520%2524%255Cepsilon%253D10%2524%252C%250Acompared%2520to%252092.52%2520for%2520models%2520without%2520pre-training.%2520Additionally%252C%2520we%2520explore%2520the%250Atrade-off%2520between%2520privacy%2520constraints%2520and%2520image%2520quality%252C%2520investigating%2520how%250Atighter%2520privacy%2520budgets%2520affect%2520output%2520controllability%2520and%2520may%2520lead%2520to%2520degraded%250Aperformance.%2520Our%2520results%2520demonstrate%2520that%2520proper%2520consideration%2520during%2520training%250Awith%2520differential%2520privacy%2520can%2520substantially%2520improve%2520the%2520quality%2520of%2520synthetic%250Acardiac%2520MRI%2520images%252C%2520but%2520there%2520are%2520still%2520notable%2520challenges%2520in%2520achieving%250Aconsistent%2520medical%2520realism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Differentially%20Private%203D%20Medical%20Image%20Synthesis%20with%20Controllable%0A%20%20Latent%20Diffusion%20Models&entry.906535625=Deniz%20Daum%20and%20Richard%20Osuala%20and%20Anneliese%20Riess%20and%20Georgios%20Kaissis%20and%20Julia%20A.%20Schnabel%20and%20Maxime%20Di%20Folco&entry.1292438233=%20%20Generally%2C%20the%20small%20size%20of%20public%20medical%20imaging%20datasets%20coupled%20with%0Astringent%20privacy%20concerns%2C%20hampers%20the%20advancement%20of%20data-hungry%20deep%0Alearning%20models%20in%20medical%20imaging.%20This%20study%20addresses%20these%20challenges%20for%0A3D%20cardiac%20MRI%20images%20in%20the%20short-axis%20view.%20We%20propose%20Latent%20Diffusion%0AModels%20that%20generate%20synthetic%20images%20conditioned%20on%20medical%20attributes%2C%20while%0Aensuring%20patient%20privacy%20through%20differentially%20private%20model%20training.%20To%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20to%20apply%20and%20quantify%20differential%20privacy%20in%0A3D%20medical%20image%20generation.%20We%20pre-train%20our%20models%20on%20public%20data%20and%0Afinetune%20them%20with%20differential%20privacy%20on%20the%20UK%20Biobank%20dataset.%20Our%0Aexperiments%20reveal%20that%20pre-training%20significantly%20improves%20model%20performance%2C%0Aachieving%20a%20Fr%5C%27echet%20Inception%20Distance%20%28FID%29%20of%2026.77%20at%20%24%5Cepsilon%3D10%24%2C%0Acompared%20to%2092.52%20for%20models%20without%20pre-training.%20Additionally%2C%20we%20explore%20the%0Atrade-off%20between%20privacy%20constraints%20and%20image%20quality%2C%20investigating%20how%0Atighter%20privacy%20budgets%20affect%20output%20controllability%20and%20may%20lead%20to%20degraded%0Aperformance.%20Our%20results%20demonstrate%20that%20proper%20consideration%20during%20training%0Awith%20differential%20privacy%20can%20substantially%20improve%20the%20quality%20of%20synthetic%0Acardiac%20MRI%20images%2C%20but%20there%20are%20still%20notable%20challenges%20in%20achieving%0Aconsistent%20medical%20realism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16405v1&entry.124074799=Read"},
{"title": "FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient\n  Descent", "author": "Cameron Smith and David Charatan and Ayush Tewari and Vincent Sitzmann", "abstract": "  This paper introduces FlowMap, an end-to-end differentiable method that\nsolves for precise camera poses, camera intrinsics, and per-frame dense depth\nof a video sequence. Our method performs per-video gradient-descent\nminimization of a simple least-squares objective that compares the optical flow\ninduced by depth, intrinsics, and poses against correspondences obtained via\noff-the-shelf optical flow and point tracking. Alongside the use of point\ntracks to encourage long-term geometric consistency, we introduce\ndifferentiable re-parameterizations of depth, intrinsics, and pose that are\namenable to first-order optimization. We empirically show that camera\nparameters and dense depth recovered by our method enable photo-realistic novel\nview synthesis on 360-degree trajectories using Gaussian Splatting. Our method\nnot only far outperforms prior gradient-descent based bundle adjustment\nmethods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM\nmethod, on the downstream task of 360-degree novel view synthesis (even though\nour method is purely gradient-descent based, fully differentiable, and presents\na complete departure from conventional SfM).\n", "link": "http://arxiv.org/abs/2404.15259v3", "date": "2024-07-23", "relevancy": 2.2869, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5807}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5678}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlowMap%3A%20High-Quality%20Camera%20Poses%2C%20Intrinsics%2C%20and%20Depth%20via%20Gradient%0A%20%20Descent&body=Title%3A%20FlowMap%3A%20High-Quality%20Camera%20Poses%2C%20Intrinsics%2C%20and%20Depth%20via%20Gradient%0A%20%20Descent%0AAuthor%3A%20Cameron%20Smith%20and%20David%20Charatan%20and%20Ayush%20Tewari%20and%20Vincent%20Sitzmann%0AAbstract%3A%20%20%20This%20paper%20introduces%20FlowMap%2C%20an%20end-to-end%20differentiable%20method%20that%0Asolves%20for%20precise%20camera%20poses%2C%20camera%20intrinsics%2C%20and%20per-frame%20dense%20depth%0Aof%20a%20video%20sequence.%20Our%20method%20performs%20per-video%20gradient-descent%0Aminimization%20of%20a%20simple%20least-squares%20objective%20that%20compares%20the%20optical%20flow%0Ainduced%20by%20depth%2C%20intrinsics%2C%20and%20poses%20against%20correspondences%20obtained%20via%0Aoff-the-shelf%20optical%20flow%20and%20point%20tracking.%20Alongside%20the%20use%20of%20point%0Atracks%20to%20encourage%20long-term%20geometric%20consistency%2C%20we%20introduce%0Adifferentiable%20re-parameterizations%20of%20depth%2C%20intrinsics%2C%20and%20pose%20that%20are%0Aamenable%20to%20first-order%20optimization.%20We%20empirically%20show%20that%20camera%0Aparameters%20and%20dense%20depth%20recovered%20by%20our%20method%20enable%20photo-realistic%20novel%0Aview%20synthesis%20on%20360-degree%20trajectories%20using%20Gaussian%20Splatting.%20Our%20method%0Anot%20only%20far%20outperforms%20prior%20gradient-descent%20based%20bundle%20adjustment%0Amethods%2C%20but%20surprisingly%20performs%20on%20par%20with%20COLMAP%2C%20the%20state-of-the-art%20SfM%0Amethod%2C%20on%20the%20downstream%20task%20of%20360-degree%20novel%20view%20synthesis%20%28even%20though%0Aour%20method%20is%20purely%20gradient-descent%20based%2C%20fully%20differentiable%2C%20and%20presents%0Aa%20complete%20departure%20from%20conventional%20SfM%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15259v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlowMap%253A%2520High-Quality%2520Camera%2520Poses%252C%2520Intrinsics%252C%2520and%2520Depth%2520via%2520Gradient%250A%2520%2520Descent%26entry.906535625%3DCameron%2520Smith%2520and%2520David%2520Charatan%2520and%2520Ayush%2520Tewari%2520and%2520Vincent%2520Sitzmann%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520FlowMap%252C%2520an%2520end-to-end%2520differentiable%2520method%2520that%250Asolves%2520for%2520precise%2520camera%2520poses%252C%2520camera%2520intrinsics%252C%2520and%2520per-frame%2520dense%2520depth%250Aof%2520a%2520video%2520sequence.%2520Our%2520method%2520performs%2520per-video%2520gradient-descent%250Aminimization%2520of%2520a%2520simple%2520least-squares%2520objective%2520that%2520compares%2520the%2520optical%2520flow%250Ainduced%2520by%2520depth%252C%2520intrinsics%252C%2520and%2520poses%2520against%2520correspondences%2520obtained%2520via%250Aoff-the-shelf%2520optical%2520flow%2520and%2520point%2520tracking.%2520Alongside%2520the%2520use%2520of%2520point%250Atracks%2520to%2520encourage%2520long-term%2520geometric%2520consistency%252C%2520we%2520introduce%250Adifferentiable%2520re-parameterizations%2520of%2520depth%252C%2520intrinsics%252C%2520and%2520pose%2520that%2520are%250Aamenable%2520to%2520first-order%2520optimization.%2520We%2520empirically%2520show%2520that%2520camera%250Aparameters%2520and%2520dense%2520depth%2520recovered%2520by%2520our%2520method%2520enable%2520photo-realistic%2520novel%250Aview%2520synthesis%2520on%2520360-degree%2520trajectories%2520using%2520Gaussian%2520Splatting.%2520Our%2520method%250Anot%2520only%2520far%2520outperforms%2520prior%2520gradient-descent%2520based%2520bundle%2520adjustment%250Amethods%252C%2520but%2520surprisingly%2520performs%2520on%2520par%2520with%2520COLMAP%252C%2520the%2520state-of-the-art%2520SfM%250Amethod%252C%2520on%2520the%2520downstream%2520task%2520of%2520360-degree%2520novel%2520view%2520synthesis%2520%2528even%2520though%250Aour%2520method%2520is%2520purely%2520gradient-descent%2520based%252C%2520fully%2520differentiable%252C%2520and%2520presents%250Aa%2520complete%2520departure%2520from%2520conventional%2520SfM%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15259v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlowMap%3A%20High-Quality%20Camera%20Poses%2C%20Intrinsics%2C%20and%20Depth%20via%20Gradient%0A%20%20Descent&entry.906535625=Cameron%20Smith%20and%20David%20Charatan%20and%20Ayush%20Tewari%20and%20Vincent%20Sitzmann&entry.1292438233=%20%20This%20paper%20introduces%20FlowMap%2C%20an%20end-to-end%20differentiable%20method%20that%0Asolves%20for%20precise%20camera%20poses%2C%20camera%20intrinsics%2C%20and%20per-frame%20dense%20depth%0Aof%20a%20video%20sequence.%20Our%20method%20performs%20per-video%20gradient-descent%0Aminimization%20of%20a%20simple%20least-squares%20objective%20that%20compares%20the%20optical%20flow%0Ainduced%20by%20depth%2C%20intrinsics%2C%20and%20poses%20against%20correspondences%20obtained%20via%0Aoff-the-shelf%20optical%20flow%20and%20point%20tracking.%20Alongside%20the%20use%20of%20point%0Atracks%20to%20encourage%20long-term%20geometric%20consistency%2C%20we%20introduce%0Adifferentiable%20re-parameterizations%20of%20depth%2C%20intrinsics%2C%20and%20pose%20that%20are%0Aamenable%20to%20first-order%20optimization.%20We%20empirically%20show%20that%20camera%0Aparameters%20and%20dense%20depth%20recovered%20by%20our%20method%20enable%20photo-realistic%20novel%0Aview%20synthesis%20on%20360-degree%20trajectories%20using%20Gaussian%20Splatting.%20Our%20method%0Anot%20only%20far%20outperforms%20prior%20gradient-descent%20based%20bundle%20adjustment%0Amethods%2C%20but%20surprisingly%20performs%20on%20par%20with%20COLMAP%2C%20the%20state-of-the-art%20SfM%0Amethod%2C%20on%20the%20downstream%20task%20of%20360-degree%20novel%20view%20synthesis%20%28even%20though%0Aour%20method%20is%20purely%20gradient-descent%20based%2C%20fully%20differentiable%2C%20and%20presents%0Aa%20complete%20departure%20from%20conventional%20SfM%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15259v3&entry.124074799=Read"},
{"title": "Disentangling spatio-temporal knowledge for weakly supervised object\n  detection and segmentation in surgical video", "author": "Guiqiu Liao and Matjaz Jogan and Sai Koushik and Eric Eaton and Daniel A. Hashimoto", "abstract": "  Weakly supervised video object segmentation (WSVOS) enables the\nidentification of segmentation maps without requiring an extensive training\ndataset of object masks, relying instead on coarse video labels indicating\nobject presence. Current state-of-the-art methods either require multiple\nindependent stages of processing that employ motion cues or, in the case of\nend-to-end trainable networks, lack in segmentation accuracy, in part due to\nthe difficulty of learning segmentation maps from videos with transient object\npresence. This limits the application of WSVOS for semantic annotation of\nsurgical videos where multiple surgical tools frequently move in and out of the\nfield of view, a problem that is more difficult than typically encountered in\nWSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks\n(VDST-Net), a framework to disentangle spatiotemporal information using\nsemi-decoupled knowledge distillation to predict high-quality class activation\nmaps (CAMs). A teacher network designed to resolve temporal conflicts when\nspecifics about object location and timing in the video are not provided works\nwith a student network that integrates information over time by leveraging\ntemporal dependencies. We demonstrate the efficacy of our framework on a public\nreference dataset and on a more challenging surgical video dataset where\nobjects are, on average, present in less than 60\\% of annotated frames. Our\nmethod outperforms state-of-the-art techniques and generates superior\nsegmentation masks under video-level weak supervision.\n", "link": "http://arxiv.org/abs/2407.15794v2", "date": "2024-07-23", "relevancy": 2.2852, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6026}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20spatio-temporal%20knowledge%20for%20weakly%20supervised%20object%0A%20%20detection%20and%20segmentation%20in%20surgical%20video&body=Title%3A%20Disentangling%20spatio-temporal%20knowledge%20for%20weakly%20supervised%20object%0A%20%20detection%20and%20segmentation%20in%20surgical%20video%0AAuthor%3A%20Guiqiu%20Liao%20and%20Matjaz%20Jogan%20and%20Sai%20Koushik%20and%20Eric%20Eaton%20and%20Daniel%20A.%20Hashimoto%0AAbstract%3A%20%20%20Weakly%20supervised%20video%20object%20segmentation%20%28WSVOS%29%20enables%20the%0Aidentification%20of%20segmentation%20maps%20without%20requiring%20an%20extensive%20training%0Adataset%20of%20object%20masks%2C%20relying%20instead%20on%20coarse%20video%20labels%20indicating%0Aobject%20presence.%20Current%20state-of-the-art%20methods%20either%20require%20multiple%0Aindependent%20stages%20of%20processing%20that%20employ%20motion%20cues%20or%2C%20in%20the%20case%20of%0Aend-to-end%20trainable%20networks%2C%20lack%20in%20segmentation%20accuracy%2C%20in%20part%20due%20to%0Athe%20difficulty%20of%20learning%20segmentation%20maps%20from%20videos%20with%20transient%20object%0Apresence.%20This%20limits%20the%20application%20of%20WSVOS%20for%20semantic%20annotation%20of%0Asurgical%20videos%20where%20multiple%20surgical%20tools%20frequently%20move%20in%20and%20out%20of%20the%0Afield%20of%20view%2C%20a%20problem%20that%20is%20more%20difficult%20than%20typically%20encountered%20in%0AWSVOS.%20This%20paper%20introduces%20Video%20Spatio-Temporal%20Disentanglement%20Networks%0A%28VDST-Net%29%2C%20a%20framework%20to%20disentangle%20spatiotemporal%20information%20using%0Asemi-decoupled%20knowledge%20distillation%20to%20predict%20high-quality%20class%20activation%0Amaps%20%28CAMs%29.%20A%20teacher%20network%20designed%20to%20resolve%20temporal%20conflicts%20when%0Aspecifics%20about%20object%20location%20and%20timing%20in%20the%20video%20are%20not%20provided%20works%0Awith%20a%20student%20network%20that%20integrates%20information%20over%20time%20by%20leveraging%0Atemporal%20dependencies.%20We%20demonstrate%20the%20efficacy%20of%20our%20framework%20on%20a%20public%0Areference%20dataset%20and%20on%20a%20more%20challenging%20surgical%20video%20dataset%20where%0Aobjects%20are%2C%20on%20average%2C%20present%20in%20less%20than%2060%5C%25%20of%20annotated%20frames.%20Our%0Amethod%20outperforms%20state-of-the-art%20techniques%20and%20generates%20superior%0Asegmentation%20masks%20under%20video-level%20weak%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15794v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520spatio-temporal%2520knowledge%2520for%2520weakly%2520supervised%2520object%250A%2520%2520detection%2520and%2520segmentation%2520in%2520surgical%2520video%26entry.906535625%3DGuiqiu%2520Liao%2520and%2520Matjaz%2520Jogan%2520and%2520Sai%2520Koushik%2520and%2520Eric%2520Eaton%2520and%2520Daniel%2520A.%2520Hashimoto%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520video%2520object%2520segmentation%2520%2528WSVOS%2529%2520enables%2520the%250Aidentification%2520of%2520segmentation%2520maps%2520without%2520requiring%2520an%2520extensive%2520training%250Adataset%2520of%2520object%2520masks%252C%2520relying%2520instead%2520on%2520coarse%2520video%2520labels%2520indicating%250Aobject%2520presence.%2520Current%2520state-of-the-art%2520methods%2520either%2520require%2520multiple%250Aindependent%2520stages%2520of%2520processing%2520that%2520employ%2520motion%2520cues%2520or%252C%2520in%2520the%2520case%2520of%250Aend-to-end%2520trainable%2520networks%252C%2520lack%2520in%2520segmentation%2520accuracy%252C%2520in%2520part%2520due%2520to%250Athe%2520difficulty%2520of%2520learning%2520segmentation%2520maps%2520from%2520videos%2520with%2520transient%2520object%250Apresence.%2520This%2520limits%2520the%2520application%2520of%2520WSVOS%2520for%2520semantic%2520annotation%2520of%250Asurgical%2520videos%2520where%2520multiple%2520surgical%2520tools%2520frequently%2520move%2520in%2520and%2520out%2520of%2520the%250Afield%2520of%2520view%252C%2520a%2520problem%2520that%2520is%2520more%2520difficult%2520than%2520typically%2520encountered%2520in%250AWSVOS.%2520This%2520paper%2520introduces%2520Video%2520Spatio-Temporal%2520Disentanglement%2520Networks%250A%2528VDST-Net%2529%252C%2520a%2520framework%2520to%2520disentangle%2520spatiotemporal%2520information%2520using%250Asemi-decoupled%2520knowledge%2520distillation%2520to%2520predict%2520high-quality%2520class%2520activation%250Amaps%2520%2528CAMs%2529.%2520A%2520teacher%2520network%2520designed%2520to%2520resolve%2520temporal%2520conflicts%2520when%250Aspecifics%2520about%2520object%2520location%2520and%2520timing%2520in%2520the%2520video%2520are%2520not%2520provided%2520works%250Awith%2520a%2520student%2520network%2520that%2520integrates%2520information%2520over%2520time%2520by%2520leveraging%250Atemporal%2520dependencies.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520framework%2520on%2520a%2520public%250Areference%2520dataset%2520and%2520on%2520a%2520more%2520challenging%2520surgical%2520video%2520dataset%2520where%250Aobjects%2520are%252C%2520on%2520average%252C%2520present%2520in%2520less%2520than%252060%255C%2525%2520of%2520annotated%2520frames.%2520Our%250Amethod%2520outperforms%2520state-of-the-art%2520techniques%2520and%2520generates%2520superior%250Asegmentation%2520masks%2520under%2520video-level%2520weak%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15794v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20spatio-temporal%20knowledge%20for%20weakly%20supervised%20object%0A%20%20detection%20and%20segmentation%20in%20surgical%20video&entry.906535625=Guiqiu%20Liao%20and%20Matjaz%20Jogan%20and%20Sai%20Koushik%20and%20Eric%20Eaton%20and%20Daniel%20A.%20Hashimoto&entry.1292438233=%20%20Weakly%20supervised%20video%20object%20segmentation%20%28WSVOS%29%20enables%20the%0Aidentification%20of%20segmentation%20maps%20without%20requiring%20an%20extensive%20training%0Adataset%20of%20object%20masks%2C%20relying%20instead%20on%20coarse%20video%20labels%20indicating%0Aobject%20presence.%20Current%20state-of-the-art%20methods%20either%20require%20multiple%0Aindependent%20stages%20of%20processing%20that%20employ%20motion%20cues%20or%2C%20in%20the%20case%20of%0Aend-to-end%20trainable%20networks%2C%20lack%20in%20segmentation%20accuracy%2C%20in%20part%20due%20to%0Athe%20difficulty%20of%20learning%20segmentation%20maps%20from%20videos%20with%20transient%20object%0Apresence.%20This%20limits%20the%20application%20of%20WSVOS%20for%20semantic%20annotation%20of%0Asurgical%20videos%20where%20multiple%20surgical%20tools%20frequently%20move%20in%20and%20out%20of%20the%0Afield%20of%20view%2C%20a%20problem%20that%20is%20more%20difficult%20than%20typically%20encountered%20in%0AWSVOS.%20This%20paper%20introduces%20Video%20Spatio-Temporal%20Disentanglement%20Networks%0A%28VDST-Net%29%2C%20a%20framework%20to%20disentangle%20spatiotemporal%20information%20using%0Asemi-decoupled%20knowledge%20distillation%20to%20predict%20high-quality%20class%20activation%0Amaps%20%28CAMs%29.%20A%20teacher%20network%20designed%20to%20resolve%20temporal%20conflicts%20when%0Aspecifics%20about%20object%20location%20and%20timing%20in%20the%20video%20are%20not%20provided%20works%0Awith%20a%20student%20network%20that%20integrates%20information%20over%20time%20by%20leveraging%0Atemporal%20dependencies.%20We%20demonstrate%20the%20efficacy%20of%20our%20framework%20on%20a%20public%0Areference%20dataset%20and%20on%20a%20more%20challenging%20surgical%20video%20dataset%20where%0Aobjects%20are%2C%20on%20average%2C%20present%20in%20less%20than%2060%5C%25%20of%20annotated%20frames.%20Our%0Amethod%20outperforms%20state-of-the-art%20techniques%20and%20generates%20superior%0Asegmentation%20masks%20under%20video-level%20weak%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15794v2&entry.124074799=Read"},
{"title": "FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression\n  Recognition with AdaptERs", "author": "Haodong Chen and Haojian Huang and Junhao Dong and Mingzhe Zheng and Dian Shao", "abstract": "  Dynamic Facial Expression Recognition (DFER) is crucial for understanding\nhuman behavior. However, current methods exhibit limited performance mainly due\nto the scarcity of high-quality data, the insufficient utilization of facial\ndynamics, and the ambiguity of expression semantics, etc. To this end, we\npropose a novel framework, named Multi-modal Fine-grained CLIP for Dynamic\nFacial Expression Recognition with AdaptERs (FineCLIPER), incorporating the\nfollowing novel designs: 1) To better distinguish between similar facial\nexpressions, we extend the class labels to textual descriptions from both\npositive and negative aspects, and obtain supervision by calculating the\ncross-modal similarity based on the CLIP model; 2) Our FineCLIPER adopts a\nhierarchical manner to effectively mine useful cues from DFE videos.\nSpecifically, besides directly embedding video frames as input (low semantic\nlevel), we propose to extract the face segmentation masks and landmarks based\non each frame (middle semantic level) and utilize the Multi-modal Large\nLanguage Model (MLLM) to further generate detailed descriptions of facial\nchanges across frames with designed prompts (high semantic level).\nAdditionally, we also adopt Parameter-Efficient Fine-Tuning (PEFT) to enable\nefficient adaptation of large pre-trained models (i.e., CLIP) for this task.\nOur FineCLIPER achieves SOTA performance on the DFEW, FERV39k, and MAFW\ndatasets in both supervised and zero-shot settings with few tunable parameters.\nProject Page: https://haroldchen19.github.io/FineCLIPER-Page/\n", "link": "http://arxiv.org/abs/2407.02157v2", "date": "2024-07-23", "relevancy": 2.2666, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6144}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5361}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FineCLIPER%3A%20Multi-modal%20Fine-grained%20CLIP%20for%20Dynamic%20Facial%20Expression%0A%20%20Recognition%20with%20AdaptERs&body=Title%3A%20FineCLIPER%3A%20Multi-modal%20Fine-grained%20CLIP%20for%20Dynamic%20Facial%20Expression%0A%20%20Recognition%20with%20AdaptERs%0AAuthor%3A%20Haodong%20Chen%20and%20Haojian%20Huang%20and%20Junhao%20Dong%20and%20Mingzhe%20Zheng%20and%20Dian%20Shao%0AAbstract%3A%20%20%20Dynamic%20Facial%20Expression%20Recognition%20%28DFER%29%20is%20crucial%20for%20understanding%0Ahuman%20behavior.%20However%2C%20current%20methods%20exhibit%20limited%20performance%20mainly%20due%0Ato%20the%20scarcity%20of%20high-quality%20data%2C%20the%20insufficient%20utilization%20of%20facial%0Adynamics%2C%20and%20the%20ambiguity%20of%20expression%20semantics%2C%20etc.%20To%20this%20end%2C%20we%0Apropose%20a%20novel%20framework%2C%20named%20Multi-modal%20Fine-grained%20CLIP%20for%20Dynamic%0AFacial%20Expression%20Recognition%20with%20AdaptERs%20%28FineCLIPER%29%2C%20incorporating%20the%0Afollowing%20novel%20designs%3A%201%29%20To%20better%20distinguish%20between%20similar%20facial%0Aexpressions%2C%20we%20extend%20the%20class%20labels%20to%20textual%20descriptions%20from%20both%0Apositive%20and%20negative%20aspects%2C%20and%20obtain%20supervision%20by%20calculating%20the%0Across-modal%20similarity%20based%20on%20the%20CLIP%20model%3B%202%29%20Our%20FineCLIPER%20adopts%20a%0Ahierarchical%20manner%20to%20effectively%20mine%20useful%20cues%20from%20DFE%20videos.%0ASpecifically%2C%20besides%20directly%20embedding%20video%20frames%20as%20input%20%28low%20semantic%0Alevel%29%2C%20we%20propose%20to%20extract%20the%20face%20segmentation%20masks%20and%20landmarks%20based%0Aon%20each%20frame%20%28middle%20semantic%20level%29%20and%20utilize%20the%20Multi-modal%20Large%0ALanguage%20Model%20%28MLLM%29%20to%20further%20generate%20detailed%20descriptions%20of%20facial%0Achanges%20across%20frames%20with%20designed%20prompts%20%28high%20semantic%20level%29.%0AAdditionally%2C%20we%20also%20adopt%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20to%20enable%0Aefficient%20adaptation%20of%20large%20pre-trained%20models%20%28i.e.%2C%20CLIP%29%20for%20this%20task.%0AOur%20FineCLIPER%20achieves%20SOTA%20performance%20on%20the%20DFEW%2C%20FERV39k%2C%20and%20MAFW%0Adatasets%20in%20both%20supervised%20and%20zero-shot%20settings%20with%20few%20tunable%20parameters.%0AProject%20Page%3A%20https%3A//haroldchen19.github.io/FineCLIPER-Page/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFineCLIPER%253A%2520Multi-modal%2520Fine-grained%2520CLIP%2520for%2520Dynamic%2520Facial%2520Expression%250A%2520%2520Recognition%2520with%2520AdaptERs%26entry.906535625%3DHaodong%2520Chen%2520and%2520Haojian%2520Huang%2520and%2520Junhao%2520Dong%2520and%2520Mingzhe%2520Zheng%2520and%2520Dian%2520Shao%26entry.1292438233%3D%2520%2520Dynamic%2520Facial%2520Expression%2520Recognition%2520%2528DFER%2529%2520is%2520crucial%2520for%2520understanding%250Ahuman%2520behavior.%2520However%252C%2520current%2520methods%2520exhibit%2520limited%2520performance%2520mainly%2520due%250Ato%2520the%2520scarcity%2520of%2520high-quality%2520data%252C%2520the%2520insufficient%2520utilization%2520of%2520facial%250Adynamics%252C%2520and%2520the%2520ambiguity%2520of%2520expression%2520semantics%252C%2520etc.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520novel%2520framework%252C%2520named%2520Multi-modal%2520Fine-grained%2520CLIP%2520for%2520Dynamic%250AFacial%2520Expression%2520Recognition%2520with%2520AdaptERs%2520%2528FineCLIPER%2529%252C%2520incorporating%2520the%250Afollowing%2520novel%2520designs%253A%25201%2529%2520To%2520better%2520distinguish%2520between%2520similar%2520facial%250Aexpressions%252C%2520we%2520extend%2520the%2520class%2520labels%2520to%2520textual%2520descriptions%2520from%2520both%250Apositive%2520and%2520negative%2520aspects%252C%2520and%2520obtain%2520supervision%2520by%2520calculating%2520the%250Across-modal%2520similarity%2520based%2520on%2520the%2520CLIP%2520model%253B%25202%2529%2520Our%2520FineCLIPER%2520adopts%2520a%250Ahierarchical%2520manner%2520to%2520effectively%2520mine%2520useful%2520cues%2520from%2520DFE%2520videos.%250ASpecifically%252C%2520besides%2520directly%2520embedding%2520video%2520frames%2520as%2520input%2520%2528low%2520semantic%250Alevel%2529%252C%2520we%2520propose%2520to%2520extract%2520the%2520face%2520segmentation%2520masks%2520and%2520landmarks%2520based%250Aon%2520each%2520frame%2520%2528middle%2520semantic%2520level%2529%2520and%2520utilize%2520the%2520Multi-modal%2520Large%250ALanguage%2520Model%2520%2528MLLM%2529%2520to%2520further%2520generate%2520detailed%2520descriptions%2520of%2520facial%250Achanges%2520across%2520frames%2520with%2520designed%2520prompts%2520%2528high%2520semantic%2520level%2529.%250AAdditionally%252C%2520we%2520also%2520adopt%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520to%2520enable%250Aefficient%2520adaptation%2520of%2520large%2520pre-trained%2520models%2520%2528i.e.%252C%2520CLIP%2529%2520for%2520this%2520task.%250AOur%2520FineCLIPER%2520achieves%2520SOTA%2520performance%2520on%2520the%2520DFEW%252C%2520FERV39k%252C%2520and%2520MAFW%250Adatasets%2520in%2520both%2520supervised%2520and%2520zero-shot%2520settings%2520with%2520few%2520tunable%2520parameters.%250AProject%2520Page%253A%2520https%253A//haroldchen19.github.io/FineCLIPER-Page/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FineCLIPER%3A%20Multi-modal%20Fine-grained%20CLIP%20for%20Dynamic%20Facial%20Expression%0A%20%20Recognition%20with%20AdaptERs&entry.906535625=Haodong%20Chen%20and%20Haojian%20Huang%20and%20Junhao%20Dong%20and%20Mingzhe%20Zheng%20and%20Dian%20Shao&entry.1292438233=%20%20Dynamic%20Facial%20Expression%20Recognition%20%28DFER%29%20is%20crucial%20for%20understanding%0Ahuman%20behavior.%20However%2C%20current%20methods%20exhibit%20limited%20performance%20mainly%20due%0Ato%20the%20scarcity%20of%20high-quality%20data%2C%20the%20insufficient%20utilization%20of%20facial%0Adynamics%2C%20and%20the%20ambiguity%20of%20expression%20semantics%2C%20etc.%20To%20this%20end%2C%20we%0Apropose%20a%20novel%20framework%2C%20named%20Multi-modal%20Fine-grained%20CLIP%20for%20Dynamic%0AFacial%20Expression%20Recognition%20with%20AdaptERs%20%28FineCLIPER%29%2C%20incorporating%20the%0Afollowing%20novel%20designs%3A%201%29%20To%20better%20distinguish%20between%20similar%20facial%0Aexpressions%2C%20we%20extend%20the%20class%20labels%20to%20textual%20descriptions%20from%20both%0Apositive%20and%20negative%20aspects%2C%20and%20obtain%20supervision%20by%20calculating%20the%0Across-modal%20similarity%20based%20on%20the%20CLIP%20model%3B%202%29%20Our%20FineCLIPER%20adopts%20a%0Ahierarchical%20manner%20to%20effectively%20mine%20useful%20cues%20from%20DFE%20videos.%0ASpecifically%2C%20besides%20directly%20embedding%20video%20frames%20as%20input%20%28low%20semantic%0Alevel%29%2C%20we%20propose%20to%20extract%20the%20face%20segmentation%20masks%20and%20landmarks%20based%0Aon%20each%20frame%20%28middle%20semantic%20level%29%20and%20utilize%20the%20Multi-modal%20Large%0ALanguage%20Model%20%28MLLM%29%20to%20further%20generate%20detailed%20descriptions%20of%20facial%0Achanges%20across%20frames%20with%20designed%20prompts%20%28high%20semantic%20level%29.%0AAdditionally%2C%20we%20also%20adopt%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20to%20enable%0Aefficient%20adaptation%20of%20large%20pre-trained%20models%20%28i.e.%2C%20CLIP%29%20for%20this%20task.%0AOur%20FineCLIPER%20achieves%20SOTA%20performance%20on%20the%20DFEW%2C%20FERV39k%2C%20and%20MAFW%0Adatasets%20in%20both%20supervised%20and%20zero-shot%20settings%20with%20few%20tunable%20parameters.%0AProject%20Page%3A%20https%3A//haroldchen19.github.io/FineCLIPER-Page/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02157v2&entry.124074799=Read"},
{"title": "Molecular Topological Profile (MOLTOP) -- Simple and Strong Baseline for\n  Molecular Graph Classification", "author": "Jakub Adamczyk and Wojciech Czech", "abstract": "  We revisit the effectiveness of topological descriptors for molecular graph\nclassification and design a simple, yet strong baseline. We demonstrate that a\nsimple approach to feature engineering - employing histogram aggregation of\nedge descriptors and one-hot encoding for atomic numbers and bond types - when\ncombined with a Random Forest classifier, can establish a strong baseline for\nGraph Neural Networks (GNNs). The novel algorithm, Molecular Topological\nProfile (MOLTOP), integrates Edge Betweenness Centrality, Adjusted Rand Index\nand SCAN Structural Similarity score. This approach proves to be remarkably\ncompetitive when compared to modern GNNs, while also being simple, fast,\nlow-variance and hyperparameter-free. Our approach is rigorously tested on\nMoleculeNet datasets using fair evaluation protocol provided by Open Graph\nBenchmark. We additionally show out-of-domain generation capabilities on\npeptide classification task from Long Range Graph Benchmark. The evaluations\nacross eleven benchmark datasets reveal MOLTOP's strong discriminative\ncapabilities, surpassing the $1$-WL test and even $3$-WL test for some classes\nof graphs. Our conclusion is that descriptor-based baselines, such as the one\nwe propose, are still crucial for accurately assessing advancements in the GNN\ndomain.\n", "link": "http://arxiv.org/abs/2407.12136v3", "date": "2024-07-23", "relevancy": 2.2664, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4574}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.453}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Molecular%20Topological%20Profile%20%28MOLTOP%29%20--%20Simple%20and%20Strong%20Baseline%20for%0A%20%20Molecular%20Graph%20Classification&body=Title%3A%20Molecular%20Topological%20Profile%20%28MOLTOP%29%20--%20Simple%20and%20Strong%20Baseline%20for%0A%20%20Molecular%20Graph%20Classification%0AAuthor%3A%20Jakub%20Adamczyk%20and%20Wojciech%20Czech%0AAbstract%3A%20%20%20We%20revisit%20the%20effectiveness%20of%20topological%20descriptors%20for%20molecular%20graph%0Aclassification%20and%20design%20a%20simple%2C%20yet%20strong%20baseline.%20We%20demonstrate%20that%20a%0Asimple%20approach%20to%20feature%20engineering%20-%20employing%20histogram%20aggregation%20of%0Aedge%20descriptors%20and%20one-hot%20encoding%20for%20atomic%20numbers%20and%20bond%20types%20-%20when%0Acombined%20with%20a%20Random%20Forest%20classifier%2C%20can%20establish%20a%20strong%20baseline%20for%0AGraph%20Neural%20Networks%20%28GNNs%29.%20The%20novel%20algorithm%2C%20Molecular%20Topological%0AProfile%20%28MOLTOP%29%2C%20integrates%20Edge%20Betweenness%20Centrality%2C%20Adjusted%20Rand%20Index%0Aand%20SCAN%20Structural%20Similarity%20score.%20This%20approach%20proves%20to%20be%20remarkably%0Acompetitive%20when%20compared%20to%20modern%20GNNs%2C%20while%20also%20being%20simple%2C%20fast%2C%0Alow-variance%20and%20hyperparameter-free.%20Our%20approach%20is%20rigorously%20tested%20on%0AMoleculeNet%20datasets%20using%20fair%20evaluation%20protocol%20provided%20by%20Open%20Graph%0ABenchmark.%20We%20additionally%20show%20out-of-domain%20generation%20capabilities%20on%0Apeptide%20classification%20task%20from%20Long%20Range%20Graph%20Benchmark.%20The%20evaluations%0Aacross%20eleven%20benchmark%20datasets%20reveal%20MOLTOP%27s%20strong%20discriminative%0Acapabilities%2C%20surpassing%20the%20%241%24-WL%20test%20and%20even%20%243%24-WL%20test%20for%20some%20classes%0Aof%20graphs.%20Our%20conclusion%20is%20that%20descriptor-based%20baselines%2C%20such%20as%20the%20one%0Awe%20propose%2C%20are%20still%20crucial%20for%20accurately%20assessing%20advancements%20in%20the%20GNN%0Adomain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12136v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolecular%2520Topological%2520Profile%2520%2528MOLTOP%2529%2520--%2520Simple%2520and%2520Strong%2520Baseline%2520for%250A%2520%2520Molecular%2520Graph%2520Classification%26entry.906535625%3DJakub%2520Adamczyk%2520and%2520Wojciech%2520Czech%26entry.1292438233%3D%2520%2520We%2520revisit%2520the%2520effectiveness%2520of%2520topological%2520descriptors%2520for%2520molecular%2520graph%250Aclassification%2520and%2520design%2520a%2520simple%252C%2520yet%2520strong%2520baseline.%2520We%2520demonstrate%2520that%2520a%250Asimple%2520approach%2520to%2520feature%2520engineering%2520-%2520employing%2520histogram%2520aggregation%2520of%250Aedge%2520descriptors%2520and%2520one-hot%2520encoding%2520for%2520atomic%2520numbers%2520and%2520bond%2520types%2520-%2520when%250Acombined%2520with%2520a%2520Random%2520Forest%2520classifier%252C%2520can%2520establish%2520a%2520strong%2520baseline%2520for%250AGraph%2520Neural%2520Networks%2520%2528GNNs%2529.%2520The%2520novel%2520algorithm%252C%2520Molecular%2520Topological%250AProfile%2520%2528MOLTOP%2529%252C%2520integrates%2520Edge%2520Betweenness%2520Centrality%252C%2520Adjusted%2520Rand%2520Index%250Aand%2520SCAN%2520Structural%2520Similarity%2520score.%2520This%2520approach%2520proves%2520to%2520be%2520remarkably%250Acompetitive%2520when%2520compared%2520to%2520modern%2520GNNs%252C%2520while%2520also%2520being%2520simple%252C%2520fast%252C%250Alow-variance%2520and%2520hyperparameter-free.%2520Our%2520approach%2520is%2520rigorously%2520tested%2520on%250AMoleculeNet%2520datasets%2520using%2520fair%2520evaluation%2520protocol%2520provided%2520by%2520Open%2520Graph%250ABenchmark.%2520We%2520additionally%2520show%2520out-of-domain%2520generation%2520capabilities%2520on%250Apeptide%2520classification%2520task%2520from%2520Long%2520Range%2520Graph%2520Benchmark.%2520The%2520evaluations%250Aacross%2520eleven%2520benchmark%2520datasets%2520reveal%2520MOLTOP%2527s%2520strong%2520discriminative%250Acapabilities%252C%2520surpassing%2520the%2520%25241%2524-WL%2520test%2520and%2520even%2520%25243%2524-WL%2520test%2520for%2520some%2520classes%250Aof%2520graphs.%2520Our%2520conclusion%2520is%2520that%2520descriptor-based%2520baselines%252C%2520such%2520as%2520the%2520one%250Awe%2520propose%252C%2520are%2520still%2520crucial%2520for%2520accurately%2520assessing%2520advancements%2520in%2520the%2520GNN%250Adomain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12136v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Molecular%20Topological%20Profile%20%28MOLTOP%29%20--%20Simple%20and%20Strong%20Baseline%20for%0A%20%20Molecular%20Graph%20Classification&entry.906535625=Jakub%20Adamczyk%20and%20Wojciech%20Czech&entry.1292438233=%20%20We%20revisit%20the%20effectiveness%20of%20topological%20descriptors%20for%20molecular%20graph%0Aclassification%20and%20design%20a%20simple%2C%20yet%20strong%20baseline.%20We%20demonstrate%20that%20a%0Asimple%20approach%20to%20feature%20engineering%20-%20employing%20histogram%20aggregation%20of%0Aedge%20descriptors%20and%20one-hot%20encoding%20for%20atomic%20numbers%20and%20bond%20types%20-%20when%0Acombined%20with%20a%20Random%20Forest%20classifier%2C%20can%20establish%20a%20strong%20baseline%20for%0AGraph%20Neural%20Networks%20%28GNNs%29.%20The%20novel%20algorithm%2C%20Molecular%20Topological%0AProfile%20%28MOLTOP%29%2C%20integrates%20Edge%20Betweenness%20Centrality%2C%20Adjusted%20Rand%20Index%0Aand%20SCAN%20Structural%20Similarity%20score.%20This%20approach%20proves%20to%20be%20remarkably%0Acompetitive%20when%20compared%20to%20modern%20GNNs%2C%20while%20also%20being%20simple%2C%20fast%2C%0Alow-variance%20and%20hyperparameter-free.%20Our%20approach%20is%20rigorously%20tested%20on%0AMoleculeNet%20datasets%20using%20fair%20evaluation%20protocol%20provided%20by%20Open%20Graph%0ABenchmark.%20We%20additionally%20show%20out-of-domain%20generation%20capabilities%20on%0Apeptide%20classification%20task%20from%20Long%20Range%20Graph%20Benchmark.%20The%20evaluations%0Aacross%20eleven%20benchmark%20datasets%20reveal%20MOLTOP%27s%20strong%20discriminative%0Acapabilities%2C%20surpassing%20the%20%241%24-WL%20test%20and%20even%20%243%24-WL%20test%20for%20some%20classes%0Aof%20graphs.%20Our%20conclusion%20is%20that%20descriptor-based%20baselines%2C%20such%20as%20the%20one%0Awe%20propose%2C%20are%20still%20crucial%20for%20accurately%20assessing%20advancements%20in%20the%20GNN%0Adomain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12136v3&entry.124074799=Read"},
{"title": "RogueGPT: dis-ethical tuning transforms ChatGPT4 into a Rogue AI in 158\n  Words", "author": "Alessio Buscemi and Daniele Proverbio", "abstract": "  The ethical implications and potentials for misuse of Generative Artificial\nIntelligence are increasingly worrying topics. This paper explores how easily\nthe default ethical guardrails of ChatGPT, using its latest customization\nfeatures, can be bypassed by simple prompts and fine-tuning, that can be\neffortlessly accessed by the broad public. This malevolently altered version of\nChatGPT, nicknamed \"RogueGPT\", responded with worrying behaviours, beyond those\ntriggered by jailbreak prompts. We conduct an empirical study of RogueGPT\nresponses, assessing its flexibility in answering questions pertaining to what\nshould be disallowed usage. Our findings raise significant concerns about the\nmodel's knowledge about topics like illegal drug production, torture methods\nand terrorism. The ease of driving ChatGPT astray, coupled with its global\naccessibility, highlights severe issues regarding the data quality used for\ntraining the foundational model and the implementation of ethical safeguards.\nWe thus underline the responsibilities and dangers of user-driven\nmodifications, and the broader effects that these may have on the design of\nsafeguarding and ethical modules implemented by AI programmers.\n", "link": "http://arxiv.org/abs/2407.15009v2", "date": "2024-07-23", "relevancy": 2.2451, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4742}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4551}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RogueGPT%3A%20dis-ethical%20tuning%20transforms%20ChatGPT4%20into%20a%20Rogue%20AI%20in%20158%0A%20%20Words&body=Title%3A%20RogueGPT%3A%20dis-ethical%20tuning%20transforms%20ChatGPT4%20into%20a%20Rogue%20AI%20in%20158%0A%20%20Words%0AAuthor%3A%20Alessio%20Buscemi%20and%20Daniele%20Proverbio%0AAbstract%3A%20%20%20The%20ethical%20implications%20and%20potentials%20for%20misuse%20of%20Generative%20Artificial%0AIntelligence%20are%20increasingly%20worrying%20topics.%20This%20paper%20explores%20how%20easily%0Athe%20default%20ethical%20guardrails%20of%20ChatGPT%2C%20using%20its%20latest%20customization%0Afeatures%2C%20can%20be%20bypassed%20by%20simple%20prompts%20and%20fine-tuning%2C%20that%20can%20be%0Aeffortlessly%20accessed%20by%20the%20broad%20public.%20This%20malevolently%20altered%20version%20of%0AChatGPT%2C%20nicknamed%20%22RogueGPT%22%2C%20responded%20with%20worrying%20behaviours%2C%20beyond%20those%0Atriggered%20by%20jailbreak%20prompts.%20We%20conduct%20an%20empirical%20study%20of%20RogueGPT%0Aresponses%2C%20assessing%20its%20flexibility%20in%20answering%20questions%20pertaining%20to%20what%0Ashould%20be%20disallowed%20usage.%20Our%20findings%20raise%20significant%20concerns%20about%20the%0Amodel%27s%20knowledge%20about%20topics%20like%20illegal%20drug%20production%2C%20torture%20methods%0Aand%20terrorism.%20The%20ease%20of%20driving%20ChatGPT%20astray%2C%20coupled%20with%20its%20global%0Aaccessibility%2C%20highlights%20severe%20issues%20regarding%20the%20data%20quality%20used%20for%0Atraining%20the%20foundational%20model%20and%20the%20implementation%20of%20ethical%20safeguards.%0AWe%20thus%20underline%20the%20responsibilities%20and%20dangers%20of%20user-driven%0Amodifications%2C%20and%20the%20broader%20effects%20that%20these%20may%20have%20on%20the%20design%20of%0Asafeguarding%20and%20ethical%20modules%20implemented%20by%20AI%20programmers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15009v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRogueGPT%253A%2520dis-ethical%2520tuning%2520transforms%2520ChatGPT4%2520into%2520a%2520Rogue%2520AI%2520in%2520158%250A%2520%2520Words%26entry.906535625%3DAlessio%2520Buscemi%2520and%2520Daniele%2520Proverbio%26entry.1292438233%3D%2520%2520The%2520ethical%2520implications%2520and%2520potentials%2520for%2520misuse%2520of%2520Generative%2520Artificial%250AIntelligence%2520are%2520increasingly%2520worrying%2520topics.%2520This%2520paper%2520explores%2520how%2520easily%250Athe%2520default%2520ethical%2520guardrails%2520of%2520ChatGPT%252C%2520using%2520its%2520latest%2520customization%250Afeatures%252C%2520can%2520be%2520bypassed%2520by%2520simple%2520prompts%2520and%2520fine-tuning%252C%2520that%2520can%2520be%250Aeffortlessly%2520accessed%2520by%2520the%2520broad%2520public.%2520This%2520malevolently%2520altered%2520version%2520of%250AChatGPT%252C%2520nicknamed%2520%2522RogueGPT%2522%252C%2520responded%2520with%2520worrying%2520behaviours%252C%2520beyond%2520those%250Atriggered%2520by%2520jailbreak%2520prompts.%2520We%2520conduct%2520an%2520empirical%2520study%2520of%2520RogueGPT%250Aresponses%252C%2520assessing%2520its%2520flexibility%2520in%2520answering%2520questions%2520pertaining%2520to%2520what%250Ashould%2520be%2520disallowed%2520usage.%2520Our%2520findings%2520raise%2520significant%2520concerns%2520about%2520the%250Amodel%2527s%2520knowledge%2520about%2520topics%2520like%2520illegal%2520drug%2520production%252C%2520torture%2520methods%250Aand%2520terrorism.%2520The%2520ease%2520of%2520driving%2520ChatGPT%2520astray%252C%2520coupled%2520with%2520its%2520global%250Aaccessibility%252C%2520highlights%2520severe%2520issues%2520regarding%2520the%2520data%2520quality%2520used%2520for%250Atraining%2520the%2520foundational%2520model%2520and%2520the%2520implementation%2520of%2520ethical%2520safeguards.%250AWe%2520thus%2520underline%2520the%2520responsibilities%2520and%2520dangers%2520of%2520user-driven%250Amodifications%252C%2520and%2520the%2520broader%2520effects%2520that%2520these%2520may%2520have%2520on%2520the%2520design%2520of%250Asafeguarding%2520and%2520ethical%2520modules%2520implemented%2520by%2520AI%2520programmers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15009v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RogueGPT%3A%20dis-ethical%20tuning%20transforms%20ChatGPT4%20into%20a%20Rogue%20AI%20in%20158%0A%20%20Words&entry.906535625=Alessio%20Buscemi%20and%20Daniele%20Proverbio&entry.1292438233=%20%20The%20ethical%20implications%20and%20potentials%20for%20misuse%20of%20Generative%20Artificial%0AIntelligence%20are%20increasingly%20worrying%20topics.%20This%20paper%20explores%20how%20easily%0Athe%20default%20ethical%20guardrails%20of%20ChatGPT%2C%20using%20its%20latest%20customization%0Afeatures%2C%20can%20be%20bypassed%20by%20simple%20prompts%20and%20fine-tuning%2C%20that%20can%20be%0Aeffortlessly%20accessed%20by%20the%20broad%20public.%20This%20malevolently%20altered%20version%20of%0AChatGPT%2C%20nicknamed%20%22RogueGPT%22%2C%20responded%20with%20worrying%20behaviours%2C%20beyond%20those%0Atriggered%20by%20jailbreak%20prompts.%20We%20conduct%20an%20empirical%20study%20of%20RogueGPT%0Aresponses%2C%20assessing%20its%20flexibility%20in%20answering%20questions%20pertaining%20to%20what%0Ashould%20be%20disallowed%20usage.%20Our%20findings%20raise%20significant%20concerns%20about%20the%0Amodel%27s%20knowledge%20about%20topics%20like%20illegal%20drug%20production%2C%20torture%20methods%0Aand%20terrorism.%20The%20ease%20of%20driving%20ChatGPT%20astray%2C%20coupled%20with%20its%20global%0Aaccessibility%2C%20highlights%20severe%20issues%20regarding%20the%20data%20quality%20used%20for%0Atraining%20the%20foundational%20model%20and%20the%20implementation%20of%20ethical%20safeguards.%0AWe%20thus%20underline%20the%20responsibilities%20and%20dangers%20of%20user-driven%0Amodifications%2C%20and%20the%20broader%20effects%20that%20these%20may%20have%20on%20the%20design%20of%0Asafeguarding%20and%20ethical%20modules%20implemented%20by%20AI%20programmers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15009v2&entry.124074799=Read"},
{"title": "Generative Motion Stylization of Cross-structure Characters within\n  Canonical Motion Space", "author": "Jiaxu Zhang and Xin Chen and Gang Yu and Zhigang Tu", "abstract": "  Stylized motion breathes life into characters. However, the fixed skeleton\nstructure and style representation hinder existing data-driven motion synthesis\nmethods from generating stylized motion for various characters. In this work,\nwe propose a generative motion stylization pipeline, named MotionS, for\nsynthesizing diverse and stylized motion on cross-structure characters using\ncross-modality style prompts. Our key insight is to embed motion style into a\ncross-modality latent space and perceive the cross-structure skeleton\ntopologies, allowing for motion stylization within a canonical motion space.\nSpecifically, the large-scale Contrastive-Language-Image-Pre-training (CLIP)\nmodel is leveraged to construct the cross-modality latent space, enabling\nflexible style representation within it. Additionally, two topology-encoded\ntokens are learned to capture the canonical and specific skeleton topologies,\nfacilitating cross-structure topology shifting. Subsequently, the\ntopology-shifted stylization diffusion is designed to generate motion content\nfor the particular skeleton and stylize it in the shifted canonical motion\nspace using multi-modality style descriptions. Through an extensive set of\nexamples, we demonstrate the flexibility and generalizability of our pipeline\nacross various characters and style descriptions. Qualitative and quantitative\ncomparisons show the superiority of our pipeline over state-of-the-arts,\nconsistently delivering high-quality stylized motion across a broad spectrum of\nskeletal structures.\n", "link": "http://arxiv.org/abs/2403.11469v2", "date": "2024-07-23", "relevancy": 2.2384, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5801}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5605}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Motion%20Stylization%20of%20Cross-structure%20Characters%20within%0A%20%20Canonical%20Motion%20Space&body=Title%3A%20Generative%20Motion%20Stylization%20of%20Cross-structure%20Characters%20within%0A%20%20Canonical%20Motion%20Space%0AAuthor%3A%20Jiaxu%20Zhang%20and%20Xin%20Chen%20and%20Gang%20Yu%20and%20Zhigang%20Tu%0AAbstract%3A%20%20%20Stylized%20motion%20breathes%20life%20into%20characters.%20However%2C%20the%20fixed%20skeleton%0Astructure%20and%20style%20representation%20hinder%20existing%20data-driven%20motion%20synthesis%0Amethods%20from%20generating%20stylized%20motion%20for%20various%20characters.%20In%20this%20work%2C%0Awe%20propose%20a%20generative%20motion%20stylization%20pipeline%2C%20named%20MotionS%2C%20for%0Asynthesizing%20diverse%20and%20stylized%20motion%20on%20cross-structure%20characters%20using%0Across-modality%20style%20prompts.%20Our%20key%20insight%20is%20to%20embed%20motion%20style%20into%20a%0Across-modality%20latent%20space%20and%20perceive%20the%20cross-structure%20skeleton%0Atopologies%2C%20allowing%20for%20motion%20stylization%20within%20a%20canonical%20motion%20space.%0ASpecifically%2C%20the%20large-scale%20Contrastive-Language-Image-Pre-training%20%28CLIP%29%0Amodel%20is%20leveraged%20to%20construct%20the%20cross-modality%20latent%20space%2C%20enabling%0Aflexible%20style%20representation%20within%20it.%20Additionally%2C%20two%20topology-encoded%0Atokens%20are%20learned%20to%20capture%20the%20canonical%20and%20specific%20skeleton%20topologies%2C%0Afacilitating%20cross-structure%20topology%20shifting.%20Subsequently%2C%20the%0Atopology-shifted%20stylization%20diffusion%20is%20designed%20to%20generate%20motion%20content%0Afor%20the%20particular%20skeleton%20and%20stylize%20it%20in%20the%20shifted%20canonical%20motion%0Aspace%20using%20multi-modality%20style%20descriptions.%20Through%20an%20extensive%20set%20of%0Aexamples%2C%20we%20demonstrate%20the%20flexibility%20and%20generalizability%20of%20our%20pipeline%0Aacross%20various%20characters%20and%20style%20descriptions.%20Qualitative%20and%20quantitative%0Acomparisons%20show%20the%20superiority%20of%20our%20pipeline%20over%20state-of-the-arts%2C%0Aconsistently%20delivering%20high-quality%20stylized%20motion%20across%20a%20broad%20spectrum%20of%0Askeletal%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11469v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Motion%2520Stylization%2520of%2520Cross-structure%2520Characters%2520within%250A%2520%2520Canonical%2520Motion%2520Space%26entry.906535625%3DJiaxu%2520Zhang%2520and%2520Xin%2520Chen%2520and%2520Gang%2520Yu%2520and%2520Zhigang%2520Tu%26entry.1292438233%3D%2520%2520Stylized%2520motion%2520breathes%2520life%2520into%2520characters.%2520However%252C%2520the%2520fixed%2520skeleton%250Astructure%2520and%2520style%2520representation%2520hinder%2520existing%2520data-driven%2520motion%2520synthesis%250Amethods%2520from%2520generating%2520stylized%2520motion%2520for%2520various%2520characters.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520generative%2520motion%2520stylization%2520pipeline%252C%2520named%2520MotionS%252C%2520for%250Asynthesizing%2520diverse%2520and%2520stylized%2520motion%2520on%2520cross-structure%2520characters%2520using%250Across-modality%2520style%2520prompts.%2520Our%2520key%2520insight%2520is%2520to%2520embed%2520motion%2520style%2520into%2520a%250Across-modality%2520latent%2520space%2520and%2520perceive%2520the%2520cross-structure%2520skeleton%250Atopologies%252C%2520allowing%2520for%2520motion%2520stylization%2520within%2520a%2520canonical%2520motion%2520space.%250ASpecifically%252C%2520the%2520large-scale%2520Contrastive-Language-Image-Pre-training%2520%2528CLIP%2529%250Amodel%2520is%2520leveraged%2520to%2520construct%2520the%2520cross-modality%2520latent%2520space%252C%2520enabling%250Aflexible%2520style%2520representation%2520within%2520it.%2520Additionally%252C%2520two%2520topology-encoded%250Atokens%2520are%2520learned%2520to%2520capture%2520the%2520canonical%2520and%2520specific%2520skeleton%2520topologies%252C%250Afacilitating%2520cross-structure%2520topology%2520shifting.%2520Subsequently%252C%2520the%250Atopology-shifted%2520stylization%2520diffusion%2520is%2520designed%2520to%2520generate%2520motion%2520content%250Afor%2520the%2520particular%2520skeleton%2520and%2520stylize%2520it%2520in%2520the%2520shifted%2520canonical%2520motion%250Aspace%2520using%2520multi-modality%2520style%2520descriptions.%2520Through%2520an%2520extensive%2520set%2520of%250Aexamples%252C%2520we%2520demonstrate%2520the%2520flexibility%2520and%2520generalizability%2520of%2520our%2520pipeline%250Aacross%2520various%2520characters%2520and%2520style%2520descriptions.%2520Qualitative%2520and%2520quantitative%250Acomparisons%2520show%2520the%2520superiority%2520of%2520our%2520pipeline%2520over%2520state-of-the-arts%252C%250Aconsistently%2520delivering%2520high-quality%2520stylized%2520motion%2520across%2520a%2520broad%2520spectrum%2520of%250Askeletal%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11469v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Motion%20Stylization%20of%20Cross-structure%20Characters%20within%0A%20%20Canonical%20Motion%20Space&entry.906535625=Jiaxu%20Zhang%20and%20Xin%20Chen%20and%20Gang%20Yu%20and%20Zhigang%20Tu&entry.1292438233=%20%20Stylized%20motion%20breathes%20life%20into%20characters.%20However%2C%20the%20fixed%20skeleton%0Astructure%20and%20style%20representation%20hinder%20existing%20data-driven%20motion%20synthesis%0Amethods%20from%20generating%20stylized%20motion%20for%20various%20characters.%20In%20this%20work%2C%0Awe%20propose%20a%20generative%20motion%20stylization%20pipeline%2C%20named%20MotionS%2C%20for%0Asynthesizing%20diverse%20and%20stylized%20motion%20on%20cross-structure%20characters%20using%0Across-modality%20style%20prompts.%20Our%20key%20insight%20is%20to%20embed%20motion%20style%20into%20a%0Across-modality%20latent%20space%20and%20perceive%20the%20cross-structure%20skeleton%0Atopologies%2C%20allowing%20for%20motion%20stylization%20within%20a%20canonical%20motion%20space.%0ASpecifically%2C%20the%20large-scale%20Contrastive-Language-Image-Pre-training%20%28CLIP%29%0Amodel%20is%20leveraged%20to%20construct%20the%20cross-modality%20latent%20space%2C%20enabling%0Aflexible%20style%20representation%20within%20it.%20Additionally%2C%20two%20topology-encoded%0Atokens%20are%20learned%20to%20capture%20the%20canonical%20and%20specific%20skeleton%20topologies%2C%0Afacilitating%20cross-structure%20topology%20shifting.%20Subsequently%2C%20the%0Atopology-shifted%20stylization%20diffusion%20is%20designed%20to%20generate%20motion%20content%0Afor%20the%20particular%20skeleton%20and%20stylize%20it%20in%20the%20shifted%20canonical%20motion%0Aspace%20using%20multi-modality%20style%20descriptions.%20Through%20an%20extensive%20set%20of%0Aexamples%2C%20we%20demonstrate%20the%20flexibility%20and%20generalizability%20of%20our%20pipeline%0Aacross%20various%20characters%20and%20style%20descriptions.%20Qualitative%20and%20quantitative%0Acomparisons%20show%20the%20superiority%20of%20our%20pipeline%20over%20state-of-the-arts%2C%0Aconsistently%20delivering%20high-quality%20stylized%20motion%20across%20a%20broad%20spectrum%20of%0Askeletal%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11469v2&entry.124074799=Read"},
{"title": "Event-Based Motion Magnification", "author": "Yutian Chen and Shi Guo and Fangzheng Yu and Feng Zhang and Jinwei Gu and Tianfan Xue", "abstract": "  Detecting and magnifying imperceptible high-frequency motions in real-world\nscenarios has substantial implications for industrial and medical applications.\nThese motions are characterized by small amplitudes and high frequencies.\nTraditional motion magnification methods rely on costly high-speed cameras or\nactive light sources, which limit the scope of their applications. In this\nwork, we propose a dual-camera system consisting of an event camera and a\nconventional RGB camera for video motion magnification, providing\ntemporally-dense information from the event stream and spatially-dense data\nfrom the RGB images. This innovative combination enables a broad and\ncost-effective amplification of high-frequency motions. By revisiting the\nphysical camera model, we observe that estimating motion direction and\nmagnitude necessitates the integration of event streams with additional image\nfeatures. On this basis, we propose a novel deep network tailored for\nevent-based motion magnification. Our approach utilizes the Second-order\nRecurrent Propagation module to proficiently interpolate multiple frames while\naddressing artifacts and distortions induced by magnified motions.\nAdditionally, we employ a temporal filter to distinguish between noise and\nuseful signals, thus minimizing the impact of noise. We also introduced the\nfirst event-based motion magnification dataset, which includes a synthetic\nsubset and a real-captured subset for training and benchmarking. Through\nextensive experiments in magnifying small-amplitude, high-frequency motions, we\ndemonstrate the effectiveness and accuracy of our dual-camera system and\nnetwork, offering a cost-effective and flexible solution for motion detection\nand magnification.\n", "link": "http://arxiv.org/abs/2402.11957v2", "date": "2024-07-23", "relevancy": 2.2381, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5716}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5524}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-Based%20Motion%20Magnification&body=Title%3A%20Event-Based%20Motion%20Magnification%0AAuthor%3A%20Yutian%20Chen%20and%20Shi%20Guo%20and%20Fangzheng%20Yu%20and%20Feng%20Zhang%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%0AAbstract%3A%20%20%20Detecting%20and%20magnifying%20imperceptible%20high-frequency%20motions%20in%20real-world%0Ascenarios%20has%20substantial%20implications%20for%20industrial%20and%20medical%20applications.%0AThese%20motions%20are%20characterized%20by%20small%20amplitudes%20and%20high%20frequencies.%0ATraditional%20motion%20magnification%20methods%20rely%20on%20costly%20high-speed%20cameras%20or%0Aactive%20light%20sources%2C%20which%20limit%20the%20scope%20of%20their%20applications.%20In%20this%0Awork%2C%20we%20propose%20a%20dual-camera%20system%20consisting%20of%20an%20event%20camera%20and%20a%0Aconventional%20RGB%20camera%20for%20video%20motion%20magnification%2C%20providing%0Atemporally-dense%20information%20from%20the%20event%20stream%20and%20spatially-dense%20data%0Afrom%20the%20RGB%20images.%20This%20innovative%20combination%20enables%20a%20broad%20and%0Acost-effective%20amplification%20of%20high-frequency%20motions.%20By%20revisiting%20the%0Aphysical%20camera%20model%2C%20we%20observe%20that%20estimating%20motion%20direction%20and%0Amagnitude%20necessitates%20the%20integration%20of%20event%20streams%20with%20additional%20image%0Afeatures.%20On%20this%20basis%2C%20we%20propose%20a%20novel%20deep%20network%20tailored%20for%0Aevent-based%20motion%20magnification.%20Our%20approach%20utilizes%20the%20Second-order%0ARecurrent%20Propagation%20module%20to%20proficiently%20interpolate%20multiple%20frames%20while%0Aaddressing%20artifacts%20and%20distortions%20induced%20by%20magnified%20motions.%0AAdditionally%2C%20we%20employ%20a%20temporal%20filter%20to%20distinguish%20between%20noise%20and%0Auseful%20signals%2C%20thus%20minimizing%20the%20impact%20of%20noise.%20We%20also%20introduced%20the%0Afirst%20event-based%20motion%20magnification%20dataset%2C%20which%20includes%20a%20synthetic%0Asubset%20and%20a%20real-captured%20subset%20for%20training%20and%20benchmarking.%20Through%0Aextensive%20experiments%20in%20magnifying%20small-amplitude%2C%20high-frequency%20motions%2C%20we%0Ademonstrate%20the%20effectiveness%20and%20accuracy%20of%20our%20dual-camera%20system%20and%0Anetwork%2C%20offering%20a%20cost-effective%20and%20flexible%20solution%20for%20motion%20detection%0Aand%20magnification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11957v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-Based%2520Motion%2520Magnification%26entry.906535625%3DYutian%2520Chen%2520and%2520Shi%2520Guo%2520and%2520Fangzheng%2520Yu%2520and%2520Feng%2520Zhang%2520and%2520Jinwei%2520Gu%2520and%2520Tianfan%2520Xue%26entry.1292438233%3D%2520%2520Detecting%2520and%2520magnifying%2520imperceptible%2520high-frequency%2520motions%2520in%2520real-world%250Ascenarios%2520has%2520substantial%2520implications%2520for%2520industrial%2520and%2520medical%2520applications.%250AThese%2520motions%2520are%2520characterized%2520by%2520small%2520amplitudes%2520and%2520high%2520frequencies.%250ATraditional%2520motion%2520magnification%2520methods%2520rely%2520on%2520costly%2520high-speed%2520cameras%2520or%250Aactive%2520light%2520sources%252C%2520which%2520limit%2520the%2520scope%2520of%2520their%2520applications.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520dual-camera%2520system%2520consisting%2520of%2520an%2520event%2520camera%2520and%2520a%250Aconventional%2520RGB%2520camera%2520for%2520video%2520motion%2520magnification%252C%2520providing%250Atemporally-dense%2520information%2520from%2520the%2520event%2520stream%2520and%2520spatially-dense%2520data%250Afrom%2520the%2520RGB%2520images.%2520This%2520innovative%2520combination%2520enables%2520a%2520broad%2520and%250Acost-effective%2520amplification%2520of%2520high-frequency%2520motions.%2520By%2520revisiting%2520the%250Aphysical%2520camera%2520model%252C%2520we%2520observe%2520that%2520estimating%2520motion%2520direction%2520and%250Amagnitude%2520necessitates%2520the%2520integration%2520of%2520event%2520streams%2520with%2520additional%2520image%250Afeatures.%2520On%2520this%2520basis%252C%2520we%2520propose%2520a%2520novel%2520deep%2520network%2520tailored%2520for%250Aevent-based%2520motion%2520magnification.%2520Our%2520approach%2520utilizes%2520the%2520Second-order%250ARecurrent%2520Propagation%2520module%2520to%2520proficiently%2520interpolate%2520multiple%2520frames%2520while%250Aaddressing%2520artifacts%2520and%2520distortions%2520induced%2520by%2520magnified%2520motions.%250AAdditionally%252C%2520we%2520employ%2520a%2520temporal%2520filter%2520to%2520distinguish%2520between%2520noise%2520and%250Auseful%2520signals%252C%2520thus%2520minimizing%2520the%2520impact%2520of%2520noise.%2520We%2520also%2520introduced%2520the%250Afirst%2520event-based%2520motion%2520magnification%2520dataset%252C%2520which%2520includes%2520a%2520synthetic%250Asubset%2520and%2520a%2520real-captured%2520subset%2520for%2520training%2520and%2520benchmarking.%2520Through%250Aextensive%2520experiments%2520in%2520magnifying%2520small-amplitude%252C%2520high-frequency%2520motions%252C%2520we%250Ademonstrate%2520the%2520effectiveness%2520and%2520accuracy%2520of%2520our%2520dual-camera%2520system%2520and%250Anetwork%252C%2520offering%2520a%2520cost-effective%2520and%2520flexible%2520solution%2520for%2520motion%2520detection%250Aand%2520magnification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11957v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-Based%20Motion%20Magnification&entry.906535625=Yutian%20Chen%20and%20Shi%20Guo%20and%20Fangzheng%20Yu%20and%20Feng%20Zhang%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue&entry.1292438233=%20%20Detecting%20and%20magnifying%20imperceptible%20high-frequency%20motions%20in%20real-world%0Ascenarios%20has%20substantial%20implications%20for%20industrial%20and%20medical%20applications.%0AThese%20motions%20are%20characterized%20by%20small%20amplitudes%20and%20high%20frequencies.%0ATraditional%20motion%20magnification%20methods%20rely%20on%20costly%20high-speed%20cameras%20or%0Aactive%20light%20sources%2C%20which%20limit%20the%20scope%20of%20their%20applications.%20In%20this%0Awork%2C%20we%20propose%20a%20dual-camera%20system%20consisting%20of%20an%20event%20camera%20and%20a%0Aconventional%20RGB%20camera%20for%20video%20motion%20magnification%2C%20providing%0Atemporally-dense%20information%20from%20the%20event%20stream%20and%20spatially-dense%20data%0Afrom%20the%20RGB%20images.%20This%20innovative%20combination%20enables%20a%20broad%20and%0Acost-effective%20amplification%20of%20high-frequency%20motions.%20By%20revisiting%20the%0Aphysical%20camera%20model%2C%20we%20observe%20that%20estimating%20motion%20direction%20and%0Amagnitude%20necessitates%20the%20integration%20of%20event%20streams%20with%20additional%20image%0Afeatures.%20On%20this%20basis%2C%20we%20propose%20a%20novel%20deep%20network%20tailored%20for%0Aevent-based%20motion%20magnification.%20Our%20approach%20utilizes%20the%20Second-order%0ARecurrent%20Propagation%20module%20to%20proficiently%20interpolate%20multiple%20frames%20while%0Aaddressing%20artifacts%20and%20distortions%20induced%20by%20magnified%20motions.%0AAdditionally%2C%20we%20employ%20a%20temporal%20filter%20to%20distinguish%20between%20noise%20and%0Auseful%20signals%2C%20thus%20minimizing%20the%20impact%20of%20noise.%20We%20also%20introduced%20the%0Afirst%20event-based%20motion%20magnification%20dataset%2C%20which%20includes%20a%20synthetic%0Asubset%20and%20a%20real-captured%20subset%20for%20training%20and%20benchmarking.%20Through%0Aextensive%20experiments%20in%20magnifying%20small-amplitude%2C%20high-frequency%20motions%2C%20we%0Ademonstrate%20the%20effectiveness%20and%20accuracy%20of%20our%20dual-camera%20system%20and%0Anetwork%2C%20offering%20a%20cost-effective%20and%20flexible%20solution%20for%20motion%20detection%0Aand%20magnification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11957v2&entry.124074799=Read"},
{"title": "QPT V2: Masked Image Modeling Advances Visual Scoring", "author": "Qizhi Xie and Kun Yuan and Yunpeng Qu and Mingda Wu and Ming Sun and Chao Zhou and Jihong Zhu", "abstract": "  Quality assessment and aesthetics assessment aim to evaluate the perceived\nquality and aesthetics of visual content. Current learning-based methods suffer\ngreatly from the scarcity of labeled data and usually perform sub-optimally in\nterms of generalization. Although masked image modeling (MIM) has achieved\nnoteworthy advancements across various high-level tasks (e.g., classification,\ndetection etc.). In this work, we take on a novel perspective to investigate\nits capabilities in terms of quality- and aesthetics-awareness. To this end, we\npropose Quality- and aesthetics-aware pretraining (QPT V2), the first\npretraining framework based on MIM that offers a unified solution to quality\nand aesthetics assessment. To perceive the high-level semantics and\nfine-grained details, pretraining data is curated. To comprehensively encompass\nquality- and aesthetics-related factors, degradation is introduced. To capture\nmulti-scale quality and aesthetic information, model structure is modified.\nExtensive experimental results on 11 downstream benchmarks clearly show the\nsuperior performance of QPT V2 in comparison with current state-of-the-art\napproaches and other pretraining paradigms. Code and models will be released at\n\\url{https://github.com/KeiChiTse/QPT-V2}.\n", "link": "http://arxiv.org/abs/2407.16541v1", "date": "2024-07-23", "relevancy": 2.2195, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5837}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5344}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QPT%20V2%3A%20Masked%20Image%20Modeling%20Advances%20Visual%20Scoring&body=Title%3A%20QPT%20V2%3A%20Masked%20Image%20Modeling%20Advances%20Visual%20Scoring%0AAuthor%3A%20Qizhi%20Xie%20and%20Kun%20Yuan%20and%20Yunpeng%20Qu%20and%20Mingda%20Wu%20and%20Ming%20Sun%20and%20Chao%20Zhou%20and%20Jihong%20Zhu%0AAbstract%3A%20%20%20Quality%20assessment%20and%20aesthetics%20assessment%20aim%20to%20evaluate%20the%20perceived%0Aquality%20and%20aesthetics%20of%20visual%20content.%20Current%20learning-based%20methods%20suffer%0Agreatly%20from%20the%20scarcity%20of%20labeled%20data%20and%20usually%20perform%20sub-optimally%20in%0Aterms%20of%20generalization.%20Although%20masked%20image%20modeling%20%28MIM%29%20has%20achieved%0Anoteworthy%20advancements%20across%20various%20high-level%20tasks%20%28e.g.%2C%20classification%2C%0Adetection%20etc.%29.%20In%20this%20work%2C%20we%20take%20on%20a%20novel%20perspective%20to%20investigate%0Aits%20capabilities%20in%20terms%20of%20quality-%20and%20aesthetics-awareness.%20To%20this%20end%2C%20we%0Apropose%20Quality-%20and%20aesthetics-aware%20pretraining%20%28QPT%20V2%29%2C%20the%20first%0Apretraining%20framework%20based%20on%20MIM%20that%20offers%20a%20unified%20solution%20to%20quality%0Aand%20aesthetics%20assessment.%20To%20perceive%20the%20high-level%20semantics%20and%0Afine-grained%20details%2C%20pretraining%20data%20is%20curated.%20To%20comprehensively%20encompass%0Aquality-%20and%20aesthetics-related%20factors%2C%20degradation%20is%20introduced.%20To%20capture%0Amulti-scale%20quality%20and%20aesthetic%20information%2C%20model%20structure%20is%20modified.%0AExtensive%20experimental%20results%20on%2011%20downstream%20benchmarks%20clearly%20show%20the%0Asuperior%20performance%20of%20QPT%20V2%20in%20comparison%20with%20current%20state-of-the-art%0Aapproaches%20and%20other%20pretraining%20paradigms.%20Code%20and%20models%20will%20be%20released%20at%0A%5Curl%7Bhttps%3A//github.com/KeiChiTse/QPT-V2%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQPT%2520V2%253A%2520Masked%2520Image%2520Modeling%2520Advances%2520Visual%2520Scoring%26entry.906535625%3DQizhi%2520Xie%2520and%2520Kun%2520Yuan%2520and%2520Yunpeng%2520Qu%2520and%2520Mingda%2520Wu%2520and%2520Ming%2520Sun%2520and%2520Chao%2520Zhou%2520and%2520Jihong%2520Zhu%26entry.1292438233%3D%2520%2520Quality%2520assessment%2520and%2520aesthetics%2520assessment%2520aim%2520to%2520evaluate%2520the%2520perceived%250Aquality%2520and%2520aesthetics%2520of%2520visual%2520content.%2520Current%2520learning-based%2520methods%2520suffer%250Agreatly%2520from%2520the%2520scarcity%2520of%2520labeled%2520data%2520and%2520usually%2520perform%2520sub-optimally%2520in%250Aterms%2520of%2520generalization.%2520Although%2520masked%2520image%2520modeling%2520%2528MIM%2529%2520has%2520achieved%250Anoteworthy%2520advancements%2520across%2520various%2520high-level%2520tasks%2520%2528e.g.%252C%2520classification%252C%250Adetection%2520etc.%2529.%2520In%2520this%2520work%252C%2520we%2520take%2520on%2520a%2520novel%2520perspective%2520to%2520investigate%250Aits%2520capabilities%2520in%2520terms%2520of%2520quality-%2520and%2520aesthetics-awareness.%2520To%2520this%2520end%252C%2520we%250Apropose%2520Quality-%2520and%2520aesthetics-aware%2520pretraining%2520%2528QPT%2520V2%2529%252C%2520the%2520first%250Apretraining%2520framework%2520based%2520on%2520MIM%2520that%2520offers%2520a%2520unified%2520solution%2520to%2520quality%250Aand%2520aesthetics%2520assessment.%2520To%2520perceive%2520the%2520high-level%2520semantics%2520and%250Afine-grained%2520details%252C%2520pretraining%2520data%2520is%2520curated.%2520To%2520comprehensively%2520encompass%250Aquality-%2520and%2520aesthetics-related%2520factors%252C%2520degradation%2520is%2520introduced.%2520To%2520capture%250Amulti-scale%2520quality%2520and%2520aesthetic%2520information%252C%2520model%2520structure%2520is%2520modified.%250AExtensive%2520experimental%2520results%2520on%252011%2520downstream%2520benchmarks%2520clearly%2520show%2520the%250Asuperior%2520performance%2520of%2520QPT%2520V2%2520in%2520comparison%2520with%2520current%2520state-of-the-art%250Aapproaches%2520and%2520other%2520pretraining%2520paradigms.%2520Code%2520and%2520models%2520will%2520be%2520released%2520at%250A%255Curl%257Bhttps%253A//github.com/KeiChiTse/QPT-V2%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QPT%20V2%3A%20Masked%20Image%20Modeling%20Advances%20Visual%20Scoring&entry.906535625=Qizhi%20Xie%20and%20Kun%20Yuan%20and%20Yunpeng%20Qu%20and%20Mingda%20Wu%20and%20Ming%20Sun%20and%20Chao%20Zhou%20and%20Jihong%20Zhu&entry.1292438233=%20%20Quality%20assessment%20and%20aesthetics%20assessment%20aim%20to%20evaluate%20the%20perceived%0Aquality%20and%20aesthetics%20of%20visual%20content.%20Current%20learning-based%20methods%20suffer%0Agreatly%20from%20the%20scarcity%20of%20labeled%20data%20and%20usually%20perform%20sub-optimally%20in%0Aterms%20of%20generalization.%20Although%20masked%20image%20modeling%20%28MIM%29%20has%20achieved%0Anoteworthy%20advancements%20across%20various%20high-level%20tasks%20%28e.g.%2C%20classification%2C%0Adetection%20etc.%29.%20In%20this%20work%2C%20we%20take%20on%20a%20novel%20perspective%20to%20investigate%0Aits%20capabilities%20in%20terms%20of%20quality-%20and%20aesthetics-awareness.%20To%20this%20end%2C%20we%0Apropose%20Quality-%20and%20aesthetics-aware%20pretraining%20%28QPT%20V2%29%2C%20the%20first%0Apretraining%20framework%20based%20on%20MIM%20that%20offers%20a%20unified%20solution%20to%20quality%0Aand%20aesthetics%20assessment.%20To%20perceive%20the%20high-level%20semantics%20and%0Afine-grained%20details%2C%20pretraining%20data%20is%20curated.%20To%20comprehensively%20encompass%0Aquality-%20and%20aesthetics-related%20factors%2C%20degradation%20is%20introduced.%20To%20capture%0Amulti-scale%20quality%20and%20aesthetic%20information%2C%20model%20structure%20is%20modified.%0AExtensive%20experimental%20results%20on%2011%20downstream%20benchmarks%20clearly%20show%20the%0Asuperior%20performance%20of%20QPT%20V2%20in%20comparison%20with%20current%20state-of-the-art%0Aapproaches%20and%20other%20pretraining%20paradigms.%20Code%20and%20models%20will%20be%20released%20at%0A%5Curl%7Bhttps%3A//github.com/KeiChiTse/QPT-V2%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16541v1&entry.124074799=Read"},
{"title": "From Category to Scenery: An End-to-End Framework for Multi-Person\n  Human-Object Interaction Recognition in Videos", "author": "Tanqiu Qiao and Ruochen Li and Frederick W. B. Li and Hubert P. H. Shum", "abstract": "  Video-based Human-Object Interaction (HOI) recognition explores the intricate\ndynamics between humans and objects, which are essential for a comprehensive\nunderstanding of human behavior and intentions. While previous work has made\nsignificant strides, effectively integrating geometric and visual features to\nmodel dynamic relationships between humans and objects in a graph framework\nremains a challenge. In this work, we propose a novel end-to-end category to\nscenery framework, CATS, starting by generating geometric features for various\ncategories through graphs respectively, then fusing them with corresponding\nvisual features. Subsequently, we construct a scenery interactive graph with\nthese enhanced geometric-visual features as nodes to learn the relationships\namong human and object categories. This methodological advance facilitates a\ndeeper, more structured comprehension of interactions, bridging\ncategory-specific insights with broad scenery dynamics. Our method demonstrates\nstate-of-the-art performance on two pivotal HOI benchmarks, including the\nMPHOI-72 dataset for multi-person HOIs and the single-person HOI CAD-120\ndataset.\n", "link": "http://arxiv.org/abs/2407.00917v2", "date": "2024-07-23", "relevancy": 2.2192, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5734}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5511}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Category%20to%20Scenery%3A%20An%20End-to-End%20Framework%20for%20Multi-Person%0A%20%20Human-Object%20Interaction%20Recognition%20in%20Videos&body=Title%3A%20From%20Category%20to%20Scenery%3A%20An%20End-to-End%20Framework%20for%20Multi-Person%0A%20%20Human-Object%20Interaction%20Recognition%20in%20Videos%0AAuthor%3A%20Tanqiu%20Qiao%20and%20Ruochen%20Li%20and%20Frederick%20W.%20B.%20Li%20and%20Hubert%20P.%20H.%20Shum%0AAbstract%3A%20%20%20Video-based%20Human-Object%20Interaction%20%28HOI%29%20recognition%20explores%20the%20intricate%0Adynamics%20between%20humans%20and%20objects%2C%20which%20are%20essential%20for%20a%20comprehensive%0Aunderstanding%20of%20human%20behavior%20and%20intentions.%20While%20previous%20work%20has%20made%0Asignificant%20strides%2C%20effectively%20integrating%20geometric%20and%20visual%20features%20to%0Amodel%20dynamic%20relationships%20between%20humans%20and%20objects%20in%20a%20graph%20framework%0Aremains%20a%20challenge.%20In%20this%20work%2C%20we%20propose%20a%20novel%20end-to-end%20category%20to%0Ascenery%20framework%2C%20CATS%2C%20starting%20by%20generating%20geometric%20features%20for%20various%0Acategories%20through%20graphs%20respectively%2C%20then%20fusing%20them%20with%20corresponding%0Avisual%20features.%20Subsequently%2C%20we%20construct%20a%20scenery%20interactive%20graph%20with%0Athese%20enhanced%20geometric-visual%20features%20as%20nodes%20to%20learn%20the%20relationships%0Aamong%20human%20and%20object%20categories.%20This%20methodological%20advance%20facilitates%20a%0Adeeper%2C%20more%20structured%20comprehension%20of%20interactions%2C%20bridging%0Acategory-specific%20insights%20with%20broad%20scenery%20dynamics.%20Our%20method%20demonstrates%0Astate-of-the-art%20performance%20on%20two%20pivotal%20HOI%20benchmarks%2C%20including%20the%0AMPHOI-72%20dataset%20for%20multi-person%20HOIs%20and%20the%20single-person%20HOI%20CAD-120%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Category%2520to%2520Scenery%253A%2520An%2520End-to-End%2520Framework%2520for%2520Multi-Person%250A%2520%2520Human-Object%2520Interaction%2520Recognition%2520in%2520Videos%26entry.906535625%3DTanqiu%2520Qiao%2520and%2520Ruochen%2520Li%2520and%2520Frederick%2520W.%2520B.%2520Li%2520and%2520Hubert%2520P.%2520H.%2520Shum%26entry.1292438233%3D%2520%2520Video-based%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520recognition%2520explores%2520the%2520intricate%250Adynamics%2520between%2520humans%2520and%2520objects%252C%2520which%2520are%2520essential%2520for%2520a%2520comprehensive%250Aunderstanding%2520of%2520human%2520behavior%2520and%2520intentions.%2520While%2520previous%2520work%2520has%2520made%250Asignificant%2520strides%252C%2520effectively%2520integrating%2520geometric%2520and%2520visual%2520features%2520to%250Amodel%2520dynamic%2520relationships%2520between%2520humans%2520and%2520objects%2520in%2520a%2520graph%2520framework%250Aremains%2520a%2520challenge.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520end-to-end%2520category%2520to%250Ascenery%2520framework%252C%2520CATS%252C%2520starting%2520by%2520generating%2520geometric%2520features%2520for%2520various%250Acategories%2520through%2520graphs%2520respectively%252C%2520then%2520fusing%2520them%2520with%2520corresponding%250Avisual%2520features.%2520Subsequently%252C%2520we%2520construct%2520a%2520scenery%2520interactive%2520graph%2520with%250Athese%2520enhanced%2520geometric-visual%2520features%2520as%2520nodes%2520to%2520learn%2520the%2520relationships%250Aamong%2520human%2520and%2520object%2520categories.%2520This%2520methodological%2520advance%2520facilitates%2520a%250Adeeper%252C%2520more%2520structured%2520comprehension%2520of%2520interactions%252C%2520bridging%250Acategory-specific%2520insights%2520with%2520broad%2520scenery%2520dynamics.%2520Our%2520method%2520demonstrates%250Astate-of-the-art%2520performance%2520on%2520two%2520pivotal%2520HOI%2520benchmarks%252C%2520including%2520the%250AMPHOI-72%2520dataset%2520for%2520multi-person%2520HOIs%2520and%2520the%2520single-person%2520HOI%2520CAD-120%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Category%20to%20Scenery%3A%20An%20End-to-End%20Framework%20for%20Multi-Person%0A%20%20Human-Object%20Interaction%20Recognition%20in%20Videos&entry.906535625=Tanqiu%20Qiao%20and%20Ruochen%20Li%20and%20Frederick%20W.%20B.%20Li%20and%20Hubert%20P.%20H.%20Shum&entry.1292438233=%20%20Video-based%20Human-Object%20Interaction%20%28HOI%29%20recognition%20explores%20the%20intricate%0Adynamics%20between%20humans%20and%20objects%2C%20which%20are%20essential%20for%20a%20comprehensive%0Aunderstanding%20of%20human%20behavior%20and%20intentions.%20While%20previous%20work%20has%20made%0Asignificant%20strides%2C%20effectively%20integrating%20geometric%20and%20visual%20features%20to%0Amodel%20dynamic%20relationships%20between%20humans%20and%20objects%20in%20a%20graph%20framework%0Aremains%20a%20challenge.%20In%20this%20work%2C%20we%20propose%20a%20novel%20end-to-end%20category%20to%0Ascenery%20framework%2C%20CATS%2C%20starting%20by%20generating%20geometric%20features%20for%20various%0Acategories%20through%20graphs%20respectively%2C%20then%20fusing%20them%20with%20corresponding%0Avisual%20features.%20Subsequently%2C%20we%20construct%20a%20scenery%20interactive%20graph%20with%0Athese%20enhanced%20geometric-visual%20features%20as%20nodes%20to%20learn%20the%20relationships%0Aamong%20human%20and%20object%20categories.%20This%20methodological%20advance%20facilitates%20a%0Adeeper%2C%20more%20structured%20comprehension%20of%20interactions%2C%20bridging%0Acategory-specific%20insights%20with%20broad%20scenery%20dynamics.%20Our%20method%20demonstrates%0Astate-of-the-art%20performance%20on%20two%20pivotal%20HOI%20benchmarks%2C%20including%20the%0AMPHOI-72%20dataset%20for%20multi-person%20HOIs%20and%20the%20single-person%20HOI%20CAD-120%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00917v2&entry.124074799=Read"},
{"title": "Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal\n  Models for Video Question Answering", "author": "Haibo Wang and Chenghang Lai and Yixuan Sun and Weifeng Ge", "abstract": "  Video Question Answering (VideoQA) aims to answer natural language questions\nbased on the information observed in videos. Despite the recent success of\nLarge Multimodal Models (LMMs) in image-language understanding and reasoning,\nthey deal with VideoQA insufficiently, by simply taking uniformly sampled\nframes as visual inputs, which ignores question-relevant visual clues.\nMoreover, there are no human annotations for question-critical timestamps in\nexisting VideoQA datasets. In light of this, we propose a novel weakly\nsupervised framework to enforce the LMMs to reason out the answers with\nquestion-critical moments as visual inputs. Specifically, we first fuse the\nquestion and answer pairs as event descriptions to find multiple keyframes as\ntarget moments and pseudo-labels, with the visual-language alignment capability\nof the CLIP models. With these pseudo-labeled keyframes as additionally weak\nsupervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG)\nmodule. GCG learns multiple Gaussian functions to characterize the temporal\nstructure of the video, and sample question-critical frames as positive moments\nto be the visual inputs of LMMs. Extensive experiments on several benchmarks\nverify the effectiveness of our framework, and we achieve substantial\nimprovements compared to previous state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2401.10711v4", "date": "2024-07-23", "relevancy": 2.1974, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5545}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5517}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20Gaussian%20Contrastive%20Grounding%20with%20Large%20Multimodal%0A%20%20Models%20for%20Video%20Question%20Answering&body=Title%3A%20Weakly%20Supervised%20Gaussian%20Contrastive%20Grounding%20with%20Large%20Multimodal%0A%20%20Models%20for%20Video%20Question%20Answering%0AAuthor%3A%20Haibo%20Wang%20and%20Chenghang%20Lai%20and%20Yixuan%20Sun%20and%20Weifeng%20Ge%0AAbstract%3A%20%20%20Video%20Question%20Answering%20%28VideoQA%29%20aims%20to%20answer%20natural%20language%20questions%0Abased%20on%20the%20information%20observed%20in%20videos.%20Despite%20the%20recent%20success%20of%0ALarge%20Multimodal%20Models%20%28LMMs%29%20in%20image-language%20understanding%20and%20reasoning%2C%0Athey%20deal%20with%20VideoQA%20insufficiently%2C%20by%20simply%20taking%20uniformly%20sampled%0Aframes%20as%20visual%20inputs%2C%20which%20ignores%20question-relevant%20visual%20clues.%0AMoreover%2C%20there%20are%20no%20human%20annotations%20for%20question-critical%20timestamps%20in%0Aexisting%20VideoQA%20datasets.%20In%20light%20of%20this%2C%20we%20propose%20a%20novel%20weakly%0Asupervised%20framework%20to%20enforce%20the%20LMMs%20to%20reason%20out%20the%20answers%20with%0Aquestion-critical%20moments%20as%20visual%20inputs.%20Specifically%2C%20we%20first%20fuse%20the%0Aquestion%20and%20answer%20pairs%20as%20event%20descriptions%20to%20find%20multiple%20keyframes%20as%0Atarget%20moments%20and%20pseudo-labels%2C%20with%20the%20visual-language%20alignment%20capability%0Aof%20the%20CLIP%20models.%20With%20these%20pseudo-labeled%20keyframes%20as%20additionally%20weak%0Asupervision%2C%20we%20devise%20a%20lightweight%20Gaussian-based%20Contrastive%20Grounding%20%28GCG%29%0Amodule.%20GCG%20learns%20multiple%20Gaussian%20functions%20to%20characterize%20the%20temporal%0Astructure%20of%20the%20video%2C%20and%20sample%20question-critical%20frames%20as%20positive%20moments%0Ato%20be%20the%20visual%20inputs%20of%20LMMs.%20Extensive%20experiments%20on%20several%20benchmarks%0Averify%20the%20effectiveness%20of%20our%20framework%2C%20and%20we%20achieve%20substantial%0Aimprovements%20compared%20to%20previous%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10711v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520Gaussian%2520Contrastive%2520Grounding%2520with%2520Large%2520Multimodal%250A%2520%2520Models%2520for%2520Video%2520Question%2520Answering%26entry.906535625%3DHaibo%2520Wang%2520and%2520Chenghang%2520Lai%2520and%2520Yixuan%2520Sun%2520and%2520Weifeng%2520Ge%26entry.1292438233%3D%2520%2520Video%2520Question%2520Answering%2520%2528VideoQA%2529%2520aims%2520to%2520answer%2520natural%2520language%2520questions%250Abased%2520on%2520the%2520information%2520observed%2520in%2520videos.%2520Despite%2520the%2520recent%2520success%2520of%250ALarge%2520Multimodal%2520Models%2520%2528LMMs%2529%2520in%2520image-language%2520understanding%2520and%2520reasoning%252C%250Athey%2520deal%2520with%2520VideoQA%2520insufficiently%252C%2520by%2520simply%2520taking%2520uniformly%2520sampled%250Aframes%2520as%2520visual%2520inputs%252C%2520which%2520ignores%2520question-relevant%2520visual%2520clues.%250AMoreover%252C%2520there%2520are%2520no%2520human%2520annotations%2520for%2520question-critical%2520timestamps%2520in%250Aexisting%2520VideoQA%2520datasets.%2520In%2520light%2520of%2520this%252C%2520we%2520propose%2520a%2520novel%2520weakly%250Asupervised%2520framework%2520to%2520enforce%2520the%2520LMMs%2520to%2520reason%2520out%2520the%2520answers%2520with%250Aquestion-critical%2520moments%2520as%2520visual%2520inputs.%2520Specifically%252C%2520we%2520first%2520fuse%2520the%250Aquestion%2520and%2520answer%2520pairs%2520as%2520event%2520descriptions%2520to%2520find%2520multiple%2520keyframes%2520as%250Atarget%2520moments%2520and%2520pseudo-labels%252C%2520with%2520the%2520visual-language%2520alignment%2520capability%250Aof%2520the%2520CLIP%2520models.%2520With%2520these%2520pseudo-labeled%2520keyframes%2520as%2520additionally%2520weak%250Asupervision%252C%2520we%2520devise%2520a%2520lightweight%2520Gaussian-based%2520Contrastive%2520Grounding%2520%2528GCG%2529%250Amodule.%2520GCG%2520learns%2520multiple%2520Gaussian%2520functions%2520to%2520characterize%2520the%2520temporal%250Astructure%2520of%2520the%2520video%252C%2520and%2520sample%2520question-critical%2520frames%2520as%2520positive%2520moments%250Ato%2520be%2520the%2520visual%2520inputs%2520of%2520LMMs.%2520Extensive%2520experiments%2520on%2520several%2520benchmarks%250Averify%2520the%2520effectiveness%2520of%2520our%2520framework%252C%2520and%2520we%2520achieve%2520substantial%250Aimprovements%2520compared%2520to%2520previous%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10711v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20Gaussian%20Contrastive%20Grounding%20with%20Large%20Multimodal%0A%20%20Models%20for%20Video%20Question%20Answering&entry.906535625=Haibo%20Wang%20and%20Chenghang%20Lai%20and%20Yixuan%20Sun%20and%20Weifeng%20Ge&entry.1292438233=%20%20Video%20Question%20Answering%20%28VideoQA%29%20aims%20to%20answer%20natural%20language%20questions%0Abased%20on%20the%20information%20observed%20in%20videos.%20Despite%20the%20recent%20success%20of%0ALarge%20Multimodal%20Models%20%28LMMs%29%20in%20image-language%20understanding%20and%20reasoning%2C%0Athey%20deal%20with%20VideoQA%20insufficiently%2C%20by%20simply%20taking%20uniformly%20sampled%0Aframes%20as%20visual%20inputs%2C%20which%20ignores%20question-relevant%20visual%20clues.%0AMoreover%2C%20there%20are%20no%20human%20annotations%20for%20question-critical%20timestamps%20in%0Aexisting%20VideoQA%20datasets.%20In%20light%20of%20this%2C%20we%20propose%20a%20novel%20weakly%0Asupervised%20framework%20to%20enforce%20the%20LMMs%20to%20reason%20out%20the%20answers%20with%0Aquestion-critical%20moments%20as%20visual%20inputs.%20Specifically%2C%20we%20first%20fuse%20the%0Aquestion%20and%20answer%20pairs%20as%20event%20descriptions%20to%20find%20multiple%20keyframes%20as%0Atarget%20moments%20and%20pseudo-labels%2C%20with%20the%20visual-language%20alignment%20capability%0Aof%20the%20CLIP%20models.%20With%20these%20pseudo-labeled%20keyframes%20as%20additionally%20weak%0Asupervision%2C%20we%20devise%20a%20lightweight%20Gaussian-based%20Contrastive%20Grounding%20%28GCG%29%0Amodule.%20GCG%20learns%20multiple%20Gaussian%20functions%20to%20characterize%20the%20temporal%0Astructure%20of%20the%20video%2C%20and%20sample%20question-critical%20frames%20as%20positive%20moments%0Ato%20be%20the%20visual%20inputs%20of%20LMMs.%20Extensive%20experiments%20on%20several%20benchmarks%0Averify%20the%20effectiveness%20of%20our%20framework%2C%20and%20we%20achieve%20substantial%0Aimprovements%20compared%20to%20previous%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10711v4&entry.124074799=Read"},
{"title": "Dynamic Retraining-Updating Mean Teacher for Source-Free Object\n  Detection", "author": "Trinh Le Ba Khanh and Huy-Hung Nguyen and Long Hoang Pham and Duong Nguyen-Ngoc Tran and Jae Wook Jeon", "abstract": "  In object detection, unsupervised domain adaptation (UDA) aims to transfer\nknowledge from a labeled source domain to an unlabeled target domain. However,\nUDA's reliance on labeled source data restricts its adaptability in\nprivacy-related scenarios. This study focuses on source-free object detection\n(SFOD), which adapts a source-trained detector to an unlabeled target domain\nwithout using labeled source data. Recent advancements in self-training,\nparticularly with the Mean Teacher (MT) framework, show promise for SFOD\ndeployment. However, the absence of source supervision significantly\ncompromises the stability of these approaches. We identify two primary issues,\n(1) uncontrollable degradation of the teacher model due to inopportune updates\nfrom the student model, and (2) the student model's tendency to replicate\nerrors from incorrect pseudo labels, leading to it being trapped in a local\noptimum. Both factors contribute to a detrimental circular dependency,\nresulting in rapid performance degradation in recent self-training frameworks.\nTo tackle these challenges, we propose the Dynamic Retraining-Updating (DRU)\nmechanism, which actively manages the student training and teacher updating\nprocesses to achieve co-evolutionary training. Additionally, we introduce\nHistorical Student Loss to mitigate the influence of incorrect pseudo labels.\nOur method achieves state-of-the-art performance in the SFOD setting on\nmultiple domain adaptation benchmarks, comparable to or even surpassing\nadvanced UDA methods. The code will be released at\nhttps://github.com/lbktrinh/DRU\n", "link": "http://arxiv.org/abs/2407.16497v1", "date": "2024-07-23", "relevancy": 2.194, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5663}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.561}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Retraining-Updating%20Mean%20Teacher%20for%20Source-Free%20Object%0A%20%20Detection&body=Title%3A%20Dynamic%20Retraining-Updating%20Mean%20Teacher%20for%20Source-Free%20Object%0A%20%20Detection%0AAuthor%3A%20Trinh%20Le%20Ba%20Khanh%20and%20Huy-Hung%20Nguyen%20and%20Long%20Hoang%20Pham%20and%20Duong%20Nguyen-Ngoc%20Tran%20and%20Jae%20Wook%20Jeon%0AAbstract%3A%20%20%20In%20object%20detection%2C%20unsupervised%20domain%20adaptation%20%28UDA%29%20aims%20to%20transfer%0Aknowledge%20from%20a%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20domain.%20However%2C%0AUDA%27s%20reliance%20on%20labeled%20source%20data%20restricts%20its%20adaptability%20in%0Aprivacy-related%20scenarios.%20This%20study%20focuses%20on%20source-free%20object%20detection%0A%28SFOD%29%2C%20which%20adapts%20a%20source-trained%20detector%20to%20an%20unlabeled%20target%20domain%0Awithout%20using%20labeled%20source%20data.%20Recent%20advancements%20in%20self-training%2C%0Aparticularly%20with%20the%20Mean%20Teacher%20%28MT%29%20framework%2C%20show%20promise%20for%20SFOD%0Adeployment.%20However%2C%20the%20absence%20of%20source%20supervision%20significantly%0Acompromises%20the%20stability%20of%20these%20approaches.%20We%20identify%20two%20primary%20issues%2C%0A%281%29%20uncontrollable%20degradation%20of%20the%20teacher%20model%20due%20to%20inopportune%20updates%0Afrom%20the%20student%20model%2C%20and%20%282%29%20the%20student%20model%27s%20tendency%20to%20replicate%0Aerrors%20from%20incorrect%20pseudo%20labels%2C%20leading%20to%20it%20being%20trapped%20in%20a%20local%0Aoptimum.%20Both%20factors%20contribute%20to%20a%20detrimental%20circular%20dependency%2C%0Aresulting%20in%20rapid%20performance%20degradation%20in%20recent%20self-training%20frameworks.%0ATo%20tackle%20these%20challenges%2C%20we%20propose%20the%20Dynamic%20Retraining-Updating%20%28DRU%29%0Amechanism%2C%20which%20actively%20manages%20the%20student%20training%20and%20teacher%20updating%0Aprocesses%20to%20achieve%20co-evolutionary%20training.%20Additionally%2C%20we%20introduce%0AHistorical%20Student%20Loss%20to%20mitigate%20the%20influence%20of%20incorrect%20pseudo%20labels.%0AOur%20method%20achieves%20state-of-the-art%20performance%20in%20the%20SFOD%20setting%20on%0Amultiple%20domain%20adaptation%20benchmarks%2C%20comparable%20to%20or%20even%20surpassing%0Aadvanced%20UDA%20methods.%20The%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/lbktrinh/DRU%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Retraining-Updating%2520Mean%2520Teacher%2520for%2520Source-Free%2520Object%250A%2520%2520Detection%26entry.906535625%3DTrinh%2520Le%2520Ba%2520Khanh%2520and%2520Huy-Hung%2520Nguyen%2520and%2520Long%2520Hoang%2520Pham%2520and%2520Duong%2520Nguyen-Ngoc%2520Tran%2520and%2520Jae%2520Wook%2520Jeon%26entry.1292438233%3D%2520%2520In%2520object%2520detection%252C%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520aims%2520to%2520transfer%250Aknowledge%2520from%2520a%2520labeled%2520source%2520domain%2520to%2520an%2520unlabeled%2520target%2520domain.%2520However%252C%250AUDA%2527s%2520reliance%2520on%2520labeled%2520source%2520data%2520restricts%2520its%2520adaptability%2520in%250Aprivacy-related%2520scenarios.%2520This%2520study%2520focuses%2520on%2520source-free%2520object%2520detection%250A%2528SFOD%2529%252C%2520which%2520adapts%2520a%2520source-trained%2520detector%2520to%2520an%2520unlabeled%2520target%2520domain%250Awithout%2520using%2520labeled%2520source%2520data.%2520Recent%2520advancements%2520in%2520self-training%252C%250Aparticularly%2520with%2520the%2520Mean%2520Teacher%2520%2528MT%2529%2520framework%252C%2520show%2520promise%2520for%2520SFOD%250Adeployment.%2520However%252C%2520the%2520absence%2520of%2520source%2520supervision%2520significantly%250Acompromises%2520the%2520stability%2520of%2520these%2520approaches.%2520We%2520identify%2520two%2520primary%2520issues%252C%250A%25281%2529%2520uncontrollable%2520degradation%2520of%2520the%2520teacher%2520model%2520due%2520to%2520inopportune%2520updates%250Afrom%2520the%2520student%2520model%252C%2520and%2520%25282%2529%2520the%2520student%2520model%2527s%2520tendency%2520to%2520replicate%250Aerrors%2520from%2520incorrect%2520pseudo%2520labels%252C%2520leading%2520to%2520it%2520being%2520trapped%2520in%2520a%2520local%250Aoptimum.%2520Both%2520factors%2520contribute%2520to%2520a%2520detrimental%2520circular%2520dependency%252C%250Aresulting%2520in%2520rapid%2520performance%2520degradation%2520in%2520recent%2520self-training%2520frameworks.%250ATo%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520the%2520Dynamic%2520Retraining-Updating%2520%2528DRU%2529%250Amechanism%252C%2520which%2520actively%2520manages%2520the%2520student%2520training%2520and%2520teacher%2520updating%250Aprocesses%2520to%2520achieve%2520co-evolutionary%2520training.%2520Additionally%252C%2520we%2520introduce%250AHistorical%2520Student%2520Loss%2520to%2520mitigate%2520the%2520influence%2520of%2520incorrect%2520pseudo%2520labels.%250AOur%2520method%2520achieves%2520state-of-the-art%2520performance%2520in%2520the%2520SFOD%2520setting%2520on%250Amultiple%2520domain%2520adaptation%2520benchmarks%252C%2520comparable%2520to%2520or%2520even%2520surpassing%250Aadvanced%2520UDA%2520methods.%2520The%2520code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/lbktrinh/DRU%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Retraining-Updating%20Mean%20Teacher%20for%20Source-Free%20Object%0A%20%20Detection&entry.906535625=Trinh%20Le%20Ba%20Khanh%20and%20Huy-Hung%20Nguyen%20and%20Long%20Hoang%20Pham%20and%20Duong%20Nguyen-Ngoc%20Tran%20and%20Jae%20Wook%20Jeon&entry.1292438233=%20%20In%20object%20detection%2C%20unsupervised%20domain%20adaptation%20%28UDA%29%20aims%20to%20transfer%0Aknowledge%20from%20a%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20domain.%20However%2C%0AUDA%27s%20reliance%20on%20labeled%20source%20data%20restricts%20its%20adaptability%20in%0Aprivacy-related%20scenarios.%20This%20study%20focuses%20on%20source-free%20object%20detection%0A%28SFOD%29%2C%20which%20adapts%20a%20source-trained%20detector%20to%20an%20unlabeled%20target%20domain%0Awithout%20using%20labeled%20source%20data.%20Recent%20advancements%20in%20self-training%2C%0Aparticularly%20with%20the%20Mean%20Teacher%20%28MT%29%20framework%2C%20show%20promise%20for%20SFOD%0Adeployment.%20However%2C%20the%20absence%20of%20source%20supervision%20significantly%0Acompromises%20the%20stability%20of%20these%20approaches.%20We%20identify%20two%20primary%20issues%2C%0A%281%29%20uncontrollable%20degradation%20of%20the%20teacher%20model%20due%20to%20inopportune%20updates%0Afrom%20the%20student%20model%2C%20and%20%282%29%20the%20student%20model%27s%20tendency%20to%20replicate%0Aerrors%20from%20incorrect%20pseudo%20labels%2C%20leading%20to%20it%20being%20trapped%20in%20a%20local%0Aoptimum.%20Both%20factors%20contribute%20to%20a%20detrimental%20circular%20dependency%2C%0Aresulting%20in%20rapid%20performance%20degradation%20in%20recent%20self-training%20frameworks.%0ATo%20tackle%20these%20challenges%2C%20we%20propose%20the%20Dynamic%20Retraining-Updating%20%28DRU%29%0Amechanism%2C%20which%20actively%20manages%20the%20student%20training%20and%20teacher%20updating%0Aprocesses%20to%20achieve%20co-evolutionary%20training.%20Additionally%2C%20we%20introduce%0AHistorical%20Student%20Loss%20to%20mitigate%20the%20influence%20of%20incorrect%20pseudo%20labels.%0AOur%20method%20achieves%20state-of-the-art%20performance%20in%20the%20SFOD%20setting%20on%0Amultiple%20domain%20adaptation%20benchmarks%2C%20comparable%20to%20or%20even%20surpassing%0Aadvanced%20UDA%20methods.%20The%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/lbktrinh/DRU%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16497v1&entry.124074799=Read"},
{"title": "Deep Learning Assisted Inertial Dead Reckoning and Fusion", "author": "Dror Hurwitz and Nadav Cohen and Itzik Klein", "abstract": "  The interest in mobile platforms across a variety of applications has\nincreased significantly in recent years. One of the reasons is the ability to\nachieve accurate navigation by using low-cost sensors. To this end, inertial\nsensors are fused with global navigation satellite systems (GNSS) signals. GNSS\noutages during platform operation can result in pure inertial navigation,\ncausing the navigation solution to drift. In such situations, periodic\ntrajectories with dedicated algorithms were suggested to mitigate the drift.\nWith periodic dynamics, inertial deep learning approaches can capture the\nmotion more accurately and provide accurate dead-reckoning for drones and\nmobile robots. In this paper, we propose approaches to extend deep\nlearning-assisted inertial sensing and fusion capabilities during periodic\nmotion. We begin by demonstrating that fusion between GNSS and inertial sensors\nin periodic trajectories achieves better accuracy compared to straight-line\ntrajectories. Next, we propose an empowered network architecture to accurately\nregress the change in distance of the platform. Utilizing this network, we\ndrive a hybrid approach for a neural-inertial fusion filter. Finally, we\nutilize this approach for situations when GNSS is available and show its\nbenefits. A dataset of 337 minutes of data collected from inertial sensors\nmounted on a mobile robot and a quadrotor is used to evaluate our approaches.\n", "link": "http://arxiv.org/abs/2407.16387v1", "date": "2024-07-23", "relevancy": 2.1929, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5637}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5501}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Assisted%20Inertial%20Dead%20Reckoning%20and%20Fusion&body=Title%3A%20Deep%20Learning%20Assisted%20Inertial%20Dead%20Reckoning%20and%20Fusion%0AAuthor%3A%20Dror%20Hurwitz%20and%20Nadav%20Cohen%20and%20Itzik%20Klein%0AAbstract%3A%20%20%20The%20interest%20in%20mobile%20platforms%20across%20a%20variety%20of%20applications%20has%0Aincreased%20significantly%20in%20recent%20years.%20One%20of%20the%20reasons%20is%20the%20ability%20to%0Aachieve%20accurate%20navigation%20by%20using%20low-cost%20sensors.%20To%20this%20end%2C%20inertial%0Asensors%20are%20fused%20with%20global%20navigation%20satellite%20systems%20%28GNSS%29%20signals.%20GNSS%0Aoutages%20during%20platform%20operation%20can%20result%20in%20pure%20inertial%20navigation%2C%0Acausing%20the%20navigation%20solution%20to%20drift.%20In%20such%20situations%2C%20periodic%0Atrajectories%20with%20dedicated%20algorithms%20were%20suggested%20to%20mitigate%20the%20drift.%0AWith%20periodic%20dynamics%2C%20inertial%20deep%20learning%20approaches%20can%20capture%20the%0Amotion%20more%20accurately%20and%20provide%20accurate%20dead-reckoning%20for%20drones%20and%0Amobile%20robots.%20In%20this%20paper%2C%20we%20propose%20approaches%20to%20extend%20deep%0Alearning-assisted%20inertial%20sensing%20and%20fusion%20capabilities%20during%20periodic%0Amotion.%20We%20begin%20by%20demonstrating%20that%20fusion%20between%20GNSS%20and%20inertial%20sensors%0Ain%20periodic%20trajectories%20achieves%20better%20accuracy%20compared%20to%20straight-line%0Atrajectories.%20Next%2C%20we%20propose%20an%20empowered%20network%20architecture%20to%20accurately%0Aregress%20the%20change%20in%20distance%20of%20the%20platform.%20Utilizing%20this%20network%2C%20we%0Adrive%20a%20hybrid%20approach%20for%20a%20neural-inertial%20fusion%20filter.%20Finally%2C%20we%0Autilize%20this%20approach%20for%20situations%20when%20GNSS%20is%20available%20and%20show%20its%0Abenefits.%20A%20dataset%20of%20337%20minutes%20of%20data%20collected%20from%20inertial%20sensors%0Amounted%20on%20a%20mobile%20robot%20and%20a%20quadrotor%20is%20used%20to%20evaluate%20our%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Assisted%2520Inertial%2520Dead%2520Reckoning%2520and%2520Fusion%26entry.906535625%3DDror%2520Hurwitz%2520and%2520Nadav%2520Cohen%2520and%2520Itzik%2520Klein%26entry.1292438233%3D%2520%2520The%2520interest%2520in%2520mobile%2520platforms%2520across%2520a%2520variety%2520of%2520applications%2520has%250Aincreased%2520significantly%2520in%2520recent%2520years.%2520One%2520of%2520the%2520reasons%2520is%2520the%2520ability%2520to%250Aachieve%2520accurate%2520navigation%2520by%2520using%2520low-cost%2520sensors.%2520To%2520this%2520end%252C%2520inertial%250Asensors%2520are%2520fused%2520with%2520global%2520navigation%2520satellite%2520systems%2520%2528GNSS%2529%2520signals.%2520GNSS%250Aoutages%2520during%2520platform%2520operation%2520can%2520result%2520in%2520pure%2520inertial%2520navigation%252C%250Acausing%2520the%2520navigation%2520solution%2520to%2520drift.%2520In%2520such%2520situations%252C%2520periodic%250Atrajectories%2520with%2520dedicated%2520algorithms%2520were%2520suggested%2520to%2520mitigate%2520the%2520drift.%250AWith%2520periodic%2520dynamics%252C%2520inertial%2520deep%2520learning%2520approaches%2520can%2520capture%2520the%250Amotion%2520more%2520accurately%2520and%2520provide%2520accurate%2520dead-reckoning%2520for%2520drones%2520and%250Amobile%2520robots.%2520In%2520this%2520paper%252C%2520we%2520propose%2520approaches%2520to%2520extend%2520deep%250Alearning-assisted%2520inertial%2520sensing%2520and%2520fusion%2520capabilities%2520during%2520periodic%250Amotion.%2520We%2520begin%2520by%2520demonstrating%2520that%2520fusion%2520between%2520GNSS%2520and%2520inertial%2520sensors%250Ain%2520periodic%2520trajectories%2520achieves%2520better%2520accuracy%2520compared%2520to%2520straight-line%250Atrajectories.%2520Next%252C%2520we%2520propose%2520an%2520empowered%2520network%2520architecture%2520to%2520accurately%250Aregress%2520the%2520change%2520in%2520distance%2520of%2520the%2520platform.%2520Utilizing%2520this%2520network%252C%2520we%250Adrive%2520a%2520hybrid%2520approach%2520for%2520a%2520neural-inertial%2520fusion%2520filter.%2520Finally%252C%2520we%250Autilize%2520this%2520approach%2520for%2520situations%2520when%2520GNSS%2520is%2520available%2520and%2520show%2520its%250Abenefits.%2520A%2520dataset%2520of%2520337%2520minutes%2520of%2520data%2520collected%2520from%2520inertial%2520sensors%250Amounted%2520on%2520a%2520mobile%2520robot%2520and%2520a%2520quadrotor%2520is%2520used%2520to%2520evaluate%2520our%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Assisted%20Inertial%20Dead%20Reckoning%20and%20Fusion&entry.906535625=Dror%20Hurwitz%20and%20Nadav%20Cohen%20and%20Itzik%20Klein&entry.1292438233=%20%20The%20interest%20in%20mobile%20platforms%20across%20a%20variety%20of%20applications%20has%0Aincreased%20significantly%20in%20recent%20years.%20One%20of%20the%20reasons%20is%20the%20ability%20to%0Aachieve%20accurate%20navigation%20by%20using%20low-cost%20sensors.%20To%20this%20end%2C%20inertial%0Asensors%20are%20fused%20with%20global%20navigation%20satellite%20systems%20%28GNSS%29%20signals.%20GNSS%0Aoutages%20during%20platform%20operation%20can%20result%20in%20pure%20inertial%20navigation%2C%0Acausing%20the%20navigation%20solution%20to%20drift.%20In%20such%20situations%2C%20periodic%0Atrajectories%20with%20dedicated%20algorithms%20were%20suggested%20to%20mitigate%20the%20drift.%0AWith%20periodic%20dynamics%2C%20inertial%20deep%20learning%20approaches%20can%20capture%20the%0Amotion%20more%20accurately%20and%20provide%20accurate%20dead-reckoning%20for%20drones%20and%0Amobile%20robots.%20In%20this%20paper%2C%20we%20propose%20approaches%20to%20extend%20deep%0Alearning-assisted%20inertial%20sensing%20and%20fusion%20capabilities%20during%20periodic%0Amotion.%20We%20begin%20by%20demonstrating%20that%20fusion%20between%20GNSS%20and%20inertial%20sensors%0Ain%20periodic%20trajectories%20achieves%20better%20accuracy%20compared%20to%20straight-line%0Atrajectories.%20Next%2C%20we%20propose%20an%20empowered%20network%20architecture%20to%20accurately%0Aregress%20the%20change%20in%20distance%20of%20the%20platform.%20Utilizing%20this%20network%2C%20we%0Adrive%20a%20hybrid%20approach%20for%20a%20neural-inertial%20fusion%20filter.%20Finally%2C%20we%0Autilize%20this%20approach%20for%20situations%20when%20GNSS%20is%20available%20and%20show%20its%0Abenefits.%20A%20dataset%20of%20337%20minutes%20of%20data%20collected%20from%20inertial%20sensors%0Amounted%20on%20a%20mobile%20robot%20and%20a%20quadrotor%20is%20used%20to%20evaluate%20our%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16387v1&entry.124074799=Read"},
{"title": "Navigating Uncertainty in Medical Image Segmentation", "author": "Kilian Zepf and Jes Frellsen and Aasa Feragen", "abstract": "  We address the selection and evaluation of uncertain segmentation methods in\nmedical imaging and present two case studies: prostate segmentation,\nillustrating that for minimal annotator variation simple deterministic models\ncan suffice, and lung lesion segmentation, highlighting the limitations of the\nGeneralized Energy Distance (GED) in model selection. Our findings lead to\nguidelines for accurately choosing and developing uncertain segmentation\nmodels, that integrate aleatoric and epistemic components. These guidelines are\ndesigned to aid researchers and practitioners in better developing, selecting,\nand evaluating uncertain segmentation methods, thereby facilitating enhanced\nadoption and effective application of segmentation uncertainty in practice.\n", "link": "http://arxiv.org/abs/2407.16367v1", "date": "2024-07-23", "relevancy": 2.1927, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6046}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5548}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20Uncertainty%20in%20Medical%20Image%20Segmentation&body=Title%3A%20Navigating%20Uncertainty%20in%20Medical%20Image%20Segmentation%0AAuthor%3A%20Kilian%20Zepf%20and%20Jes%20Frellsen%20and%20Aasa%20Feragen%0AAbstract%3A%20%20%20We%20address%20the%20selection%20and%20evaluation%20of%20uncertain%20segmentation%20methods%20in%0Amedical%20imaging%20and%20present%20two%20case%20studies%3A%20prostate%20segmentation%2C%0Aillustrating%20that%20for%20minimal%20annotator%20variation%20simple%20deterministic%20models%0Acan%20suffice%2C%20and%20lung%20lesion%20segmentation%2C%20highlighting%20the%20limitations%20of%20the%0AGeneralized%20Energy%20Distance%20%28GED%29%20in%20model%20selection.%20Our%20findings%20lead%20to%0Aguidelines%20for%20accurately%20choosing%20and%20developing%20uncertain%20segmentation%0Amodels%2C%20that%20integrate%20aleatoric%20and%20epistemic%20components.%20These%20guidelines%20are%0Adesigned%20to%20aid%20researchers%20and%20practitioners%20in%20better%20developing%2C%20selecting%2C%0Aand%20evaluating%20uncertain%20segmentation%20methods%2C%20thereby%20facilitating%20enhanced%0Aadoption%20and%20effective%20application%20of%20segmentation%20uncertainty%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520Uncertainty%2520in%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DKilian%2520Zepf%2520and%2520Jes%2520Frellsen%2520and%2520Aasa%2520Feragen%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520selection%2520and%2520evaluation%2520of%2520uncertain%2520segmentation%2520methods%2520in%250Amedical%2520imaging%2520and%2520present%2520two%2520case%2520studies%253A%2520prostate%2520segmentation%252C%250Aillustrating%2520that%2520for%2520minimal%2520annotator%2520variation%2520simple%2520deterministic%2520models%250Acan%2520suffice%252C%2520and%2520lung%2520lesion%2520segmentation%252C%2520highlighting%2520the%2520limitations%2520of%2520the%250AGeneralized%2520Energy%2520Distance%2520%2528GED%2529%2520in%2520model%2520selection.%2520Our%2520findings%2520lead%2520to%250Aguidelines%2520for%2520accurately%2520choosing%2520and%2520developing%2520uncertain%2520segmentation%250Amodels%252C%2520that%2520integrate%2520aleatoric%2520and%2520epistemic%2520components.%2520These%2520guidelines%2520are%250Adesigned%2520to%2520aid%2520researchers%2520and%2520practitioners%2520in%2520better%2520developing%252C%2520selecting%252C%250Aand%2520evaluating%2520uncertain%2520segmentation%2520methods%252C%2520thereby%2520facilitating%2520enhanced%250Aadoption%2520and%2520effective%2520application%2520of%2520segmentation%2520uncertainty%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20Uncertainty%20in%20Medical%20Image%20Segmentation&entry.906535625=Kilian%20Zepf%20and%20Jes%20Frellsen%20and%20Aasa%20Feragen&entry.1292438233=%20%20We%20address%20the%20selection%20and%20evaluation%20of%20uncertain%20segmentation%20methods%20in%0Amedical%20imaging%20and%20present%20two%20case%20studies%3A%20prostate%20segmentation%2C%0Aillustrating%20that%20for%20minimal%20annotator%20variation%20simple%20deterministic%20models%0Acan%20suffice%2C%20and%20lung%20lesion%20segmentation%2C%20highlighting%20the%20limitations%20of%20the%0AGeneralized%20Energy%20Distance%20%28GED%29%20in%20model%20selection.%20Our%20findings%20lead%20to%0Aguidelines%20for%20accurately%20choosing%20and%20developing%20uncertain%20segmentation%0Amodels%2C%20that%20integrate%20aleatoric%20and%20epistemic%20components.%20These%20guidelines%20are%0Adesigned%20to%20aid%20researchers%20and%20practitioners%20in%20better%20developing%2C%20selecting%2C%0Aand%20evaluating%20uncertain%20segmentation%20methods%2C%20thereby%20facilitating%20enhanced%0Aadoption%20and%20effective%20application%20of%20segmentation%20uncertainty%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16367v1&entry.124074799=Read"},
{"title": "RanDumb: A Simple Approach that Questions the Efficacy of Continual\n  Representation Learning", "author": "Ameya Prabhu and Shiven Sinha and Ponnurangam Kumaraguru and Philip H. S. Torr and Ozan Sener and Puneet K. Dokania", "abstract": "  Continual learning has primarily focused on the issue of catastrophic\nforgetting and the associated stability-plasticity tradeoffs. However, little\nattention has been paid to the efficacy of continually learned representations,\nas representations are learned alongside classifiers throughout the learning\nprocess. Our primary contribution is empirically demonstrating that existing\nonline continually trained deep networks produce inferior representations\ncompared to a simple pre-defined random transforms. Our approach embeds raw\npixels using a fixed random transform, approximating an RBF-Kernel initialized\nbefore any data is seen. We then train a simple linear classifier on top\nwithout storing any exemplars, processing one sample at a time in an online\ncontinual learning setting. This method, called RanDumb, significantly\noutperforms state-of-the-art continually learned representations across all\nstandard online continual learning benchmarks. Our study reveals the\nsignificant limitations of representation learning, particularly in\nlow-exemplar and online continual learning scenarios. Extending our\ninvestigation to popular exemplar-free scenarios with pretrained models, we\nfind that training only a linear classifier on top of pretrained\nrepresentations surpasses most continual fine-tuning and prompt-tuning\nstrategies. Overall, our investigation challenges the prevailing assumptions\nabout effective representation learning in online continual learning. Our code\nis available at://github.com/drimpossible/RanDumb.\n", "link": "http://arxiv.org/abs/2402.08823v2", "date": "2024-07-23", "relevancy": 2.1776, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5506}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5466}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RanDumb%3A%20A%20Simple%20Approach%20that%20Questions%20the%20Efficacy%20of%20Continual%0A%20%20Representation%20Learning&body=Title%3A%20RanDumb%3A%20A%20Simple%20Approach%20that%20Questions%20the%20Efficacy%20of%20Continual%0A%20%20Representation%20Learning%0AAuthor%3A%20Ameya%20Prabhu%20and%20Shiven%20Sinha%20and%20Ponnurangam%20Kumaraguru%20and%20Philip%20H.%20S.%20Torr%20and%20Ozan%20Sener%20and%20Puneet%20K.%20Dokania%0AAbstract%3A%20%20%20Continual%20learning%20has%20primarily%20focused%20on%20the%20issue%20of%20catastrophic%0Aforgetting%20and%20the%20associated%20stability-plasticity%20tradeoffs.%20However%2C%20little%0Aattention%20has%20been%20paid%20to%20the%20efficacy%20of%20continually%20learned%20representations%2C%0Aas%20representations%20are%20learned%20alongside%20classifiers%20throughout%20the%20learning%0Aprocess.%20Our%20primary%20contribution%20is%20empirically%20demonstrating%20that%20existing%0Aonline%20continually%20trained%20deep%20networks%20produce%20inferior%20representations%0Acompared%20to%20a%20simple%20pre-defined%20random%20transforms.%20Our%20approach%20embeds%20raw%0Apixels%20using%20a%20fixed%20random%20transform%2C%20approximating%20an%20RBF-Kernel%20initialized%0Abefore%20any%20data%20is%20seen.%20We%20then%20train%20a%20simple%20linear%20classifier%20on%20top%0Awithout%20storing%20any%20exemplars%2C%20processing%20one%20sample%20at%20a%20time%20in%20an%20online%0Acontinual%20learning%20setting.%20This%20method%2C%20called%20RanDumb%2C%20significantly%0Aoutperforms%20state-of-the-art%20continually%20learned%20representations%20across%20all%0Astandard%20online%20continual%20learning%20benchmarks.%20Our%20study%20reveals%20the%0Asignificant%20limitations%20of%20representation%20learning%2C%20particularly%20in%0Alow-exemplar%20and%20online%20continual%20learning%20scenarios.%20Extending%20our%0Ainvestigation%20to%20popular%20exemplar-free%20scenarios%20with%20pretrained%20models%2C%20we%0Afind%20that%20training%20only%20a%20linear%20classifier%20on%20top%20of%20pretrained%0Arepresentations%20surpasses%20most%20continual%20fine-tuning%20and%20prompt-tuning%0Astrategies.%20Overall%2C%20our%20investigation%20challenges%20the%20prevailing%20assumptions%0Aabout%20effective%20representation%20learning%20in%20online%20continual%20learning.%20Our%20code%0Ais%20available%20at%3A//github.com/drimpossible/RanDumb.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08823v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRanDumb%253A%2520A%2520Simple%2520Approach%2520that%2520Questions%2520the%2520Efficacy%2520of%2520Continual%250A%2520%2520Representation%2520Learning%26entry.906535625%3DAmeya%2520Prabhu%2520and%2520Shiven%2520Sinha%2520and%2520Ponnurangam%2520Kumaraguru%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Ozan%2520Sener%2520and%2520Puneet%2520K.%2520Dokania%26entry.1292438233%3D%2520%2520Continual%2520learning%2520has%2520primarily%2520focused%2520on%2520the%2520issue%2520of%2520catastrophic%250Aforgetting%2520and%2520the%2520associated%2520stability-plasticity%2520tradeoffs.%2520However%252C%2520little%250Aattention%2520has%2520been%2520paid%2520to%2520the%2520efficacy%2520of%2520continually%2520learned%2520representations%252C%250Aas%2520representations%2520are%2520learned%2520alongside%2520classifiers%2520throughout%2520the%2520learning%250Aprocess.%2520Our%2520primary%2520contribution%2520is%2520empirically%2520demonstrating%2520that%2520existing%250Aonline%2520continually%2520trained%2520deep%2520networks%2520produce%2520inferior%2520representations%250Acompared%2520to%2520a%2520simple%2520pre-defined%2520random%2520transforms.%2520Our%2520approach%2520embeds%2520raw%250Apixels%2520using%2520a%2520fixed%2520random%2520transform%252C%2520approximating%2520an%2520RBF-Kernel%2520initialized%250Abefore%2520any%2520data%2520is%2520seen.%2520We%2520then%2520train%2520a%2520simple%2520linear%2520classifier%2520on%2520top%250Awithout%2520storing%2520any%2520exemplars%252C%2520processing%2520one%2520sample%2520at%2520a%2520time%2520in%2520an%2520online%250Acontinual%2520learning%2520setting.%2520This%2520method%252C%2520called%2520RanDumb%252C%2520significantly%250Aoutperforms%2520state-of-the-art%2520continually%2520learned%2520representations%2520across%2520all%250Astandard%2520online%2520continual%2520learning%2520benchmarks.%2520Our%2520study%2520reveals%2520the%250Asignificant%2520limitations%2520of%2520representation%2520learning%252C%2520particularly%2520in%250Alow-exemplar%2520and%2520online%2520continual%2520learning%2520scenarios.%2520Extending%2520our%250Ainvestigation%2520to%2520popular%2520exemplar-free%2520scenarios%2520with%2520pretrained%2520models%252C%2520we%250Afind%2520that%2520training%2520only%2520a%2520linear%2520classifier%2520on%2520top%2520of%2520pretrained%250Arepresentations%2520surpasses%2520most%2520continual%2520fine-tuning%2520and%2520prompt-tuning%250Astrategies.%2520Overall%252C%2520our%2520investigation%2520challenges%2520the%2520prevailing%2520assumptions%250Aabout%2520effective%2520representation%2520learning%2520in%2520online%2520continual%2520learning.%2520Our%2520code%250Ais%2520available%2520at%253A//github.com/drimpossible/RanDumb.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08823v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RanDumb%3A%20A%20Simple%20Approach%20that%20Questions%20the%20Efficacy%20of%20Continual%0A%20%20Representation%20Learning&entry.906535625=Ameya%20Prabhu%20and%20Shiven%20Sinha%20and%20Ponnurangam%20Kumaraguru%20and%20Philip%20H.%20S.%20Torr%20and%20Ozan%20Sener%20and%20Puneet%20K.%20Dokania&entry.1292438233=%20%20Continual%20learning%20has%20primarily%20focused%20on%20the%20issue%20of%20catastrophic%0Aforgetting%20and%20the%20associated%20stability-plasticity%20tradeoffs.%20However%2C%20little%0Aattention%20has%20been%20paid%20to%20the%20efficacy%20of%20continually%20learned%20representations%2C%0Aas%20representations%20are%20learned%20alongside%20classifiers%20throughout%20the%20learning%0Aprocess.%20Our%20primary%20contribution%20is%20empirically%20demonstrating%20that%20existing%0Aonline%20continually%20trained%20deep%20networks%20produce%20inferior%20representations%0Acompared%20to%20a%20simple%20pre-defined%20random%20transforms.%20Our%20approach%20embeds%20raw%0Apixels%20using%20a%20fixed%20random%20transform%2C%20approximating%20an%20RBF-Kernel%20initialized%0Abefore%20any%20data%20is%20seen.%20We%20then%20train%20a%20simple%20linear%20classifier%20on%20top%0Awithout%20storing%20any%20exemplars%2C%20processing%20one%20sample%20at%20a%20time%20in%20an%20online%0Acontinual%20learning%20setting.%20This%20method%2C%20called%20RanDumb%2C%20significantly%0Aoutperforms%20state-of-the-art%20continually%20learned%20representations%20across%20all%0Astandard%20online%20continual%20learning%20benchmarks.%20Our%20study%20reveals%20the%0Asignificant%20limitations%20of%20representation%20learning%2C%20particularly%20in%0Alow-exemplar%20and%20online%20continual%20learning%20scenarios.%20Extending%20our%0Ainvestigation%20to%20popular%20exemplar-free%20scenarios%20with%20pretrained%20models%2C%20we%0Afind%20that%20training%20only%20a%20linear%20classifier%20on%20top%20of%20pretrained%0Arepresentations%20surpasses%20most%20continual%20fine-tuning%20and%20prompt-tuning%0Astrategies.%20Overall%2C%20our%20investigation%20challenges%20the%20prevailing%20assumptions%0Aabout%20effective%20representation%20learning%20in%20online%20continual%20learning.%20Our%20code%0Ais%20available%20at%3A//github.com/drimpossible/RanDumb.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08823v2&entry.124074799=Read"},
{"title": "ESOD: Efficient Small Object Detection on High-Resolution Images", "author": "Kai Liu and Zhihang Fu and Sheng Jin and Ze Chen and Fan Zhou and Rongxin Jiang and Yaowu Chen and Jieping Ye", "abstract": "  Enlarging input images is a straightforward and effective approach to promote\nsmall object detection. However, simple image enlargement is significantly\nexpensive on both computations and GPU memory. In fact, small objects are\nusually sparsely distributed and locally clustered. Therefore, massive feature\nextraction computations are wasted on the non-target background area of images.\nRecent works have tried to pick out target-containing regions using an extra\nnetwork and perform conventional object detection, but the newly introduced\ncomputation limits their final performance. In this paper, we propose to reuse\nthe detector's backbone to conduct feature-level object-seeking and\npatch-slicing, which can avoid redundant feature extraction and reduce the\ncomputation cost. Incorporating a sparse detection head, we are able to detect\nsmall objects on high-resolution inputs (e.g., 1080P or larger) for superior\nperformance. The resulting Efficient Small Object Detection (ESOD) approach is\na generic framework, which can be applied to both CNN- and ViT-based detectors\nto save the computation and GPU memory costs. Extensive experiments demonstrate\nthe efficacy and efficiency of our method. In particular, our method\nconsistently surpasses the SOTA detectors by a large margin (e.g., 8% gains on\nAP) on the representative VisDrone, UAVDT, and TinyPerson datasets. Code will\nbe made public soon.\n", "link": "http://arxiv.org/abs/2407.16424v1", "date": "2024-07-23", "relevancy": 2.1748, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5624}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5526}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ESOD%3A%20Efficient%20Small%20Object%20Detection%20on%20High-Resolution%20Images&body=Title%3A%20ESOD%3A%20Efficient%20Small%20Object%20Detection%20on%20High-Resolution%20Images%0AAuthor%3A%20Kai%20Liu%20and%20Zhihang%20Fu%20and%20Sheng%20Jin%20and%20Ze%20Chen%20and%20Fan%20Zhou%20and%20Rongxin%20Jiang%20and%20Yaowu%20Chen%20and%20Jieping%20Ye%0AAbstract%3A%20%20%20Enlarging%20input%20images%20is%20a%20straightforward%20and%20effective%20approach%20to%20promote%0Asmall%20object%20detection.%20However%2C%20simple%20image%20enlargement%20is%20significantly%0Aexpensive%20on%20both%20computations%20and%20GPU%20memory.%20In%20fact%2C%20small%20objects%20are%0Ausually%20sparsely%20distributed%20and%20locally%20clustered.%20Therefore%2C%20massive%20feature%0Aextraction%20computations%20are%20wasted%20on%20the%20non-target%20background%20area%20of%20images.%0ARecent%20works%20have%20tried%20to%20pick%20out%20target-containing%20regions%20using%20an%20extra%0Anetwork%20and%20perform%20conventional%20object%20detection%2C%20but%20the%20newly%20introduced%0Acomputation%20limits%20their%20final%20performance.%20In%20this%20paper%2C%20we%20propose%20to%20reuse%0Athe%20detector%27s%20backbone%20to%20conduct%20feature-level%20object-seeking%20and%0Apatch-slicing%2C%20which%20can%20avoid%20redundant%20feature%20extraction%20and%20reduce%20the%0Acomputation%20cost.%20Incorporating%20a%20sparse%20detection%20head%2C%20we%20are%20able%20to%20detect%0Asmall%20objects%20on%20high-resolution%20inputs%20%28e.g.%2C%201080P%20or%20larger%29%20for%20superior%0Aperformance.%20The%20resulting%20Efficient%20Small%20Object%20Detection%20%28ESOD%29%20approach%20is%0Aa%20generic%20framework%2C%20which%20can%20be%20applied%20to%20both%20CNN-%20and%20ViT-based%20detectors%0Ato%20save%20the%20computation%20and%20GPU%20memory%20costs.%20Extensive%20experiments%20demonstrate%0Athe%20efficacy%20and%20efficiency%20of%20our%20method.%20In%20particular%2C%20our%20method%0Aconsistently%20surpasses%20the%20SOTA%20detectors%20by%20a%20large%20margin%20%28e.g.%2C%208%25%20gains%20on%0AAP%29%20on%20the%20representative%20VisDrone%2C%20UAVDT%2C%20and%20TinyPerson%20datasets.%20Code%20will%0Abe%20made%20public%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DESOD%253A%2520Efficient%2520Small%2520Object%2520Detection%2520on%2520High-Resolution%2520Images%26entry.906535625%3DKai%2520Liu%2520and%2520Zhihang%2520Fu%2520and%2520Sheng%2520Jin%2520and%2520Ze%2520Chen%2520and%2520Fan%2520Zhou%2520and%2520Rongxin%2520Jiang%2520and%2520Yaowu%2520Chen%2520and%2520Jieping%2520Ye%26entry.1292438233%3D%2520%2520Enlarging%2520input%2520images%2520is%2520a%2520straightforward%2520and%2520effective%2520approach%2520to%2520promote%250Asmall%2520object%2520detection.%2520However%252C%2520simple%2520image%2520enlargement%2520is%2520significantly%250Aexpensive%2520on%2520both%2520computations%2520and%2520GPU%2520memory.%2520In%2520fact%252C%2520small%2520objects%2520are%250Ausually%2520sparsely%2520distributed%2520and%2520locally%2520clustered.%2520Therefore%252C%2520massive%2520feature%250Aextraction%2520computations%2520are%2520wasted%2520on%2520the%2520non-target%2520background%2520area%2520of%2520images.%250ARecent%2520works%2520have%2520tried%2520to%2520pick%2520out%2520target-containing%2520regions%2520using%2520an%2520extra%250Anetwork%2520and%2520perform%2520conventional%2520object%2520detection%252C%2520but%2520the%2520newly%2520introduced%250Acomputation%2520limits%2520their%2520final%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520reuse%250Athe%2520detector%2527s%2520backbone%2520to%2520conduct%2520feature-level%2520object-seeking%2520and%250Apatch-slicing%252C%2520which%2520can%2520avoid%2520redundant%2520feature%2520extraction%2520and%2520reduce%2520the%250Acomputation%2520cost.%2520Incorporating%2520a%2520sparse%2520detection%2520head%252C%2520we%2520are%2520able%2520to%2520detect%250Asmall%2520objects%2520on%2520high-resolution%2520inputs%2520%2528e.g.%252C%25201080P%2520or%2520larger%2529%2520for%2520superior%250Aperformance.%2520The%2520resulting%2520Efficient%2520Small%2520Object%2520Detection%2520%2528ESOD%2529%2520approach%2520is%250Aa%2520generic%2520framework%252C%2520which%2520can%2520be%2520applied%2520to%2520both%2520CNN-%2520and%2520ViT-based%2520detectors%250Ato%2520save%2520the%2520computation%2520and%2520GPU%2520memory%2520costs.%2520Extensive%2520experiments%2520demonstrate%250Athe%2520efficacy%2520and%2520efficiency%2520of%2520our%2520method.%2520In%2520particular%252C%2520our%2520method%250Aconsistently%2520surpasses%2520the%2520SOTA%2520detectors%2520by%2520a%2520large%2520margin%2520%2528e.g.%252C%25208%2525%2520gains%2520on%250AAP%2529%2520on%2520the%2520representative%2520VisDrone%252C%2520UAVDT%252C%2520and%2520TinyPerson%2520datasets.%2520Code%2520will%250Abe%2520made%2520public%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ESOD%3A%20Efficient%20Small%20Object%20Detection%20on%20High-Resolution%20Images&entry.906535625=Kai%20Liu%20and%20Zhihang%20Fu%20and%20Sheng%20Jin%20and%20Ze%20Chen%20and%20Fan%20Zhou%20and%20Rongxin%20Jiang%20and%20Yaowu%20Chen%20and%20Jieping%20Ye&entry.1292438233=%20%20Enlarging%20input%20images%20is%20a%20straightforward%20and%20effective%20approach%20to%20promote%0Asmall%20object%20detection.%20However%2C%20simple%20image%20enlargement%20is%20significantly%0Aexpensive%20on%20both%20computations%20and%20GPU%20memory.%20In%20fact%2C%20small%20objects%20are%0Ausually%20sparsely%20distributed%20and%20locally%20clustered.%20Therefore%2C%20massive%20feature%0Aextraction%20computations%20are%20wasted%20on%20the%20non-target%20background%20area%20of%20images.%0ARecent%20works%20have%20tried%20to%20pick%20out%20target-containing%20regions%20using%20an%20extra%0Anetwork%20and%20perform%20conventional%20object%20detection%2C%20but%20the%20newly%20introduced%0Acomputation%20limits%20their%20final%20performance.%20In%20this%20paper%2C%20we%20propose%20to%20reuse%0Athe%20detector%27s%20backbone%20to%20conduct%20feature-level%20object-seeking%20and%0Apatch-slicing%2C%20which%20can%20avoid%20redundant%20feature%20extraction%20and%20reduce%20the%0Acomputation%20cost.%20Incorporating%20a%20sparse%20detection%20head%2C%20we%20are%20able%20to%20detect%0Asmall%20objects%20on%20high-resolution%20inputs%20%28e.g.%2C%201080P%20or%20larger%29%20for%20superior%0Aperformance.%20The%20resulting%20Efficient%20Small%20Object%20Detection%20%28ESOD%29%20approach%20is%0Aa%20generic%20framework%2C%20which%20can%20be%20applied%20to%20both%20CNN-%20and%20ViT-based%20detectors%0Ato%20save%20the%20computation%20and%20GPU%20memory%20costs.%20Extensive%20experiments%20demonstrate%0Athe%20efficacy%20and%20efficiency%20of%20our%20method.%20In%20particular%2C%20our%20method%0Aconsistently%20surpasses%20the%20SOTA%20detectors%20by%20a%20large%20margin%20%28e.g.%2C%208%25%20gains%20on%0AAP%29%20on%20the%20representative%20VisDrone%2C%20UAVDT%2C%20and%20TinyPerson%20datasets.%20Code%20will%0Abe%20made%20public%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16424v1&entry.124074799=Read"},
{"title": "SEDS: Semantically Enhanced Dual-Stream Encoder for Sign Language\n  Retrieval", "author": "Longtao Jiang and Min Wang and Zecheng Li and Yao Fang and Wengang Zhou and Houqiang Li", "abstract": "  Different from traditional video retrieval, sign language retrieval is more\nbiased towards understanding the semantic information of human actions\ncontained in video clips. Previous works typically only encode RGB videos to\nobtain high-level semantic features, resulting in local action details drowned\nin a large amount of visual information redundancy. Furthermore, existing\nRGB-based sign retrieval works suffer from the huge memory cost of dense visual\ndata embedding in end-to-end training, and adopt offline RGB encoder instead,\nleading to suboptimal feature representation. To address these issues, we\npropose a novel sign language representation framework called Semantically\nEnhanced Dual-Stream Encoder (SEDS), which integrates Pose and RGB modalities\nto represent the local and global information of sign language videos.\nSpecifically, the Pose encoder embeds the coordinates of keypoints\ncorresponding to human joints, effectively capturing detailed action features.\nFor better context-aware fusion of two video modalities, we propose a Cross\nGloss Attention Fusion (CGAF) module to aggregate the adjacent clip features\nwith similar semantic information from intra-modality and inter-modality.\nMoreover, a Pose-RGB Fine-grained Matching Objective is developed to enhance\nthe aggregated fusion feature by contextual matching of fine-grained\ndual-stream features. Besides the offline RGB encoder, the whole framework only\ncontains learnable lightweight networks, which can be trained end-to-end.\nExtensive experiments demonstrate that our framework significantly outperforms\nstate-of-the-art methods on various datasets.\n", "link": "http://arxiv.org/abs/2407.16394v1", "date": "2024-07-23", "relevancy": 2.1739, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.549}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5426}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEDS%3A%20Semantically%20Enhanced%20Dual-Stream%20Encoder%20for%20Sign%20Language%0A%20%20Retrieval&body=Title%3A%20SEDS%3A%20Semantically%20Enhanced%20Dual-Stream%20Encoder%20for%20Sign%20Language%0A%20%20Retrieval%0AAuthor%3A%20Longtao%20Jiang%20and%20Min%20Wang%20and%20Zecheng%20Li%20and%20Yao%20Fang%20and%20Wengang%20Zhou%20and%20Houqiang%20Li%0AAbstract%3A%20%20%20Different%20from%20traditional%20video%20retrieval%2C%20sign%20language%20retrieval%20is%20more%0Abiased%20towards%20understanding%20the%20semantic%20information%20of%20human%20actions%0Acontained%20in%20video%20clips.%20Previous%20works%20typically%20only%20encode%20RGB%20videos%20to%0Aobtain%20high-level%20semantic%20features%2C%20resulting%20in%20local%20action%20details%20drowned%0Ain%20a%20large%20amount%20of%20visual%20information%20redundancy.%20Furthermore%2C%20existing%0ARGB-based%20sign%20retrieval%20works%20suffer%20from%20the%20huge%20memory%20cost%20of%20dense%20visual%0Adata%20embedding%20in%20end-to-end%20training%2C%20and%20adopt%20offline%20RGB%20encoder%20instead%2C%0Aleading%20to%20suboptimal%20feature%20representation.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20novel%20sign%20language%20representation%20framework%20called%20Semantically%0AEnhanced%20Dual-Stream%20Encoder%20%28SEDS%29%2C%20which%20integrates%20Pose%20and%20RGB%20modalities%0Ato%20represent%20the%20local%20and%20global%20information%20of%20sign%20language%20videos.%0ASpecifically%2C%20the%20Pose%20encoder%20embeds%20the%20coordinates%20of%20keypoints%0Acorresponding%20to%20human%20joints%2C%20effectively%20capturing%20detailed%20action%20features.%0AFor%20better%20context-aware%20fusion%20of%20two%20video%20modalities%2C%20we%20propose%20a%20Cross%0AGloss%20Attention%20Fusion%20%28CGAF%29%20module%20to%20aggregate%20the%20adjacent%20clip%20features%0Awith%20similar%20semantic%20information%20from%20intra-modality%20and%20inter-modality.%0AMoreover%2C%20a%20Pose-RGB%20Fine-grained%20Matching%20Objective%20is%20developed%20to%20enhance%0Athe%20aggregated%20fusion%20feature%20by%20contextual%20matching%20of%20fine-grained%0Adual-stream%20features.%20Besides%20the%20offline%20RGB%20encoder%2C%20the%20whole%20framework%20only%0Acontains%20learnable%20lightweight%20networks%2C%20which%20can%20be%20trained%20end-to-end.%0AExtensive%20experiments%20demonstrate%20that%20our%20framework%20significantly%20outperforms%0Astate-of-the-art%20methods%20on%20various%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEDS%253A%2520Semantically%2520Enhanced%2520Dual-Stream%2520Encoder%2520for%2520Sign%2520Language%250A%2520%2520Retrieval%26entry.906535625%3DLongtao%2520Jiang%2520and%2520Min%2520Wang%2520and%2520Zecheng%2520Li%2520and%2520Yao%2520Fang%2520and%2520Wengang%2520Zhou%2520and%2520Houqiang%2520Li%26entry.1292438233%3D%2520%2520Different%2520from%2520traditional%2520video%2520retrieval%252C%2520sign%2520language%2520retrieval%2520is%2520more%250Abiased%2520towards%2520understanding%2520the%2520semantic%2520information%2520of%2520human%2520actions%250Acontained%2520in%2520video%2520clips.%2520Previous%2520works%2520typically%2520only%2520encode%2520RGB%2520videos%2520to%250Aobtain%2520high-level%2520semantic%2520features%252C%2520resulting%2520in%2520local%2520action%2520details%2520drowned%250Ain%2520a%2520large%2520amount%2520of%2520visual%2520information%2520redundancy.%2520Furthermore%252C%2520existing%250ARGB-based%2520sign%2520retrieval%2520works%2520suffer%2520from%2520the%2520huge%2520memory%2520cost%2520of%2520dense%2520visual%250Adata%2520embedding%2520in%2520end-to-end%2520training%252C%2520and%2520adopt%2520offline%2520RGB%2520encoder%2520instead%252C%250Aleading%2520to%2520suboptimal%2520feature%2520representation.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520novel%2520sign%2520language%2520representation%2520framework%2520called%2520Semantically%250AEnhanced%2520Dual-Stream%2520Encoder%2520%2528SEDS%2529%252C%2520which%2520integrates%2520Pose%2520and%2520RGB%2520modalities%250Ato%2520represent%2520the%2520local%2520and%2520global%2520information%2520of%2520sign%2520language%2520videos.%250ASpecifically%252C%2520the%2520Pose%2520encoder%2520embeds%2520the%2520coordinates%2520of%2520keypoints%250Acorresponding%2520to%2520human%2520joints%252C%2520effectively%2520capturing%2520detailed%2520action%2520features.%250AFor%2520better%2520context-aware%2520fusion%2520of%2520two%2520video%2520modalities%252C%2520we%2520propose%2520a%2520Cross%250AGloss%2520Attention%2520Fusion%2520%2528CGAF%2529%2520module%2520to%2520aggregate%2520the%2520adjacent%2520clip%2520features%250Awith%2520similar%2520semantic%2520information%2520from%2520intra-modality%2520and%2520inter-modality.%250AMoreover%252C%2520a%2520Pose-RGB%2520Fine-grained%2520Matching%2520Objective%2520is%2520developed%2520to%2520enhance%250Athe%2520aggregated%2520fusion%2520feature%2520by%2520contextual%2520matching%2520of%2520fine-grained%250Adual-stream%2520features.%2520Besides%2520the%2520offline%2520RGB%2520encoder%252C%2520the%2520whole%2520framework%2520only%250Acontains%2520learnable%2520lightweight%2520networks%252C%2520which%2520can%2520be%2520trained%2520end-to-end.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520framework%2520significantly%2520outperforms%250Astate-of-the-art%2520methods%2520on%2520various%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEDS%3A%20Semantically%20Enhanced%20Dual-Stream%20Encoder%20for%20Sign%20Language%0A%20%20Retrieval&entry.906535625=Longtao%20Jiang%20and%20Min%20Wang%20and%20Zecheng%20Li%20and%20Yao%20Fang%20and%20Wengang%20Zhou%20and%20Houqiang%20Li&entry.1292438233=%20%20Different%20from%20traditional%20video%20retrieval%2C%20sign%20language%20retrieval%20is%20more%0Abiased%20towards%20understanding%20the%20semantic%20information%20of%20human%20actions%0Acontained%20in%20video%20clips.%20Previous%20works%20typically%20only%20encode%20RGB%20videos%20to%0Aobtain%20high-level%20semantic%20features%2C%20resulting%20in%20local%20action%20details%20drowned%0Ain%20a%20large%20amount%20of%20visual%20information%20redundancy.%20Furthermore%2C%20existing%0ARGB-based%20sign%20retrieval%20works%20suffer%20from%20the%20huge%20memory%20cost%20of%20dense%20visual%0Adata%20embedding%20in%20end-to-end%20training%2C%20and%20adopt%20offline%20RGB%20encoder%20instead%2C%0Aleading%20to%20suboptimal%20feature%20representation.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20novel%20sign%20language%20representation%20framework%20called%20Semantically%0AEnhanced%20Dual-Stream%20Encoder%20%28SEDS%29%2C%20which%20integrates%20Pose%20and%20RGB%20modalities%0Ato%20represent%20the%20local%20and%20global%20information%20of%20sign%20language%20videos.%0ASpecifically%2C%20the%20Pose%20encoder%20embeds%20the%20coordinates%20of%20keypoints%0Acorresponding%20to%20human%20joints%2C%20effectively%20capturing%20detailed%20action%20features.%0AFor%20better%20context-aware%20fusion%20of%20two%20video%20modalities%2C%20we%20propose%20a%20Cross%0AGloss%20Attention%20Fusion%20%28CGAF%29%20module%20to%20aggregate%20the%20adjacent%20clip%20features%0Awith%20similar%20semantic%20information%20from%20intra-modality%20and%20inter-modality.%0AMoreover%2C%20a%20Pose-RGB%20Fine-grained%20Matching%20Objective%20is%20developed%20to%20enhance%0Athe%20aggregated%20fusion%20feature%20by%20contextual%20matching%20of%20fine-grained%0Adual-stream%20features.%20Besides%20the%20offline%20RGB%20encoder%2C%20the%20whole%20framework%20only%0Acontains%20learnable%20lightweight%20networks%2C%20which%20can%20be%20trained%20end-to-end.%0AExtensive%20experiments%20demonstrate%20that%20our%20framework%20significantly%20outperforms%0Astate-of-the-art%20methods%20on%20various%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16394v1&entry.124074799=Read"},
{"title": "A Multitask Deep Learning Model for Classification and Regression of\n  Hyperspectral Images: Application to the large-scale dataset", "author": "Koushikey Chhapariya and Alexandre Benoit and Krishna Mohan Buddhiraju and Anil Kumar", "abstract": "  Multitask learning is a widely recognized technique in the field of computer\nvision and deep learning domain. However, it is still a research question in\nremote sensing, particularly for hyperspectral imaging. Moreover, most of the\nresearch in the remote sensing domain focuses on small and single-task-based\nannotated datasets, which limits the generalizability and scalability of the\ndeveloped models to more diverse and complex real-world scenarios. Thus, in\nthis study, we propose a multitask deep learning model designed to perform\nmultiple classification and regression tasks simultaneously on hyperspectral\nimages. We validated our approach on a large hyperspectral dataset called\nTAIGA, which contains 13 forest variables, including three categorical\nvariables and ten continuous variables with different biophysical parameters.\nWe design a sharing encoder and task-specific decoder network to streamline\nfeature learning while allowing each task-specific decoder to focus on the\nunique aspects of its respective task.\n  Additionally, a dense atrous pyramid pooling layer and attention network were\nintegrated to extract multi-scale contextual information and enable selective\ninformation processing by prioritizing task-specific features. Further, we\ncomputed multitask loss and optimized its parameters for the proposed framework\nto improve the model performance and efficiency across diverse tasks. A\ncomprehensive qualitative and quantitative analysis of the results shows that\nthe proposed method significantly outperforms other state-of-the-art methods.\nWe trained our model across 10 seeds/trials to ensure robustness. Our proposed\nmodel demonstrates higher mean performance while maintaining lower or\nequivalent variability. To make the work reproducible, the codes will be\navailable at\nhttps://github.com/Koushikey4596/Multitask-Deep-Learning-Model-for-Taiga-datatset.\n", "link": "http://arxiv.org/abs/2407.16384v1", "date": "2024-07-23", "relevancy": 2.1697, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5536}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5412}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multitask%20Deep%20Learning%20Model%20for%20Classification%20and%20Regression%20of%0A%20%20Hyperspectral%20Images%3A%20Application%20to%20the%20large-scale%20dataset&body=Title%3A%20A%20Multitask%20Deep%20Learning%20Model%20for%20Classification%20and%20Regression%20of%0A%20%20Hyperspectral%20Images%3A%20Application%20to%20the%20large-scale%20dataset%0AAuthor%3A%20Koushikey%20Chhapariya%20and%20Alexandre%20Benoit%20and%20Krishna%20Mohan%20Buddhiraju%20and%20Anil%20Kumar%0AAbstract%3A%20%20%20Multitask%20learning%20is%20a%20widely%20recognized%20technique%20in%20the%20field%20of%20computer%0Avision%20and%20deep%20learning%20domain.%20However%2C%20it%20is%20still%20a%20research%20question%20in%0Aremote%20sensing%2C%20particularly%20for%20hyperspectral%20imaging.%20Moreover%2C%20most%20of%20the%0Aresearch%20in%20the%20remote%20sensing%20domain%20focuses%20on%20small%20and%20single-task-based%0Aannotated%20datasets%2C%20which%20limits%20the%20generalizability%20and%20scalability%20of%20the%0Adeveloped%20models%20to%20more%20diverse%20and%20complex%20real-world%20scenarios.%20Thus%2C%20in%0Athis%20study%2C%20we%20propose%20a%20multitask%20deep%20learning%20model%20designed%20to%20perform%0Amultiple%20classification%20and%20regression%20tasks%20simultaneously%20on%20hyperspectral%0Aimages.%20We%20validated%20our%20approach%20on%20a%20large%20hyperspectral%20dataset%20called%0ATAIGA%2C%20which%20contains%2013%20forest%20variables%2C%20including%20three%20categorical%0Avariables%20and%20ten%20continuous%20variables%20with%20different%20biophysical%20parameters.%0AWe%20design%20a%20sharing%20encoder%20and%20task-specific%20decoder%20network%20to%20streamline%0Afeature%20learning%20while%20allowing%20each%20task-specific%20decoder%20to%20focus%20on%20the%0Aunique%20aspects%20of%20its%20respective%20task.%0A%20%20Additionally%2C%20a%20dense%20atrous%20pyramid%20pooling%20layer%20and%20attention%20network%20were%0Aintegrated%20to%20extract%20multi-scale%20contextual%20information%20and%20enable%20selective%0Ainformation%20processing%20by%20prioritizing%20task-specific%20features.%20Further%2C%20we%0Acomputed%20multitask%20loss%20and%20optimized%20its%20parameters%20for%20the%20proposed%20framework%0Ato%20improve%20the%20model%20performance%20and%20efficiency%20across%20diverse%20tasks.%20A%0Acomprehensive%20qualitative%20and%20quantitative%20analysis%20of%20the%20results%20shows%20that%0Athe%20proposed%20method%20significantly%20outperforms%20other%20state-of-the-art%20methods.%0AWe%20trained%20our%20model%20across%2010%20seeds/trials%20to%20ensure%20robustness.%20Our%20proposed%0Amodel%20demonstrates%20higher%20mean%20performance%20while%20maintaining%20lower%20or%0Aequivalent%20variability.%20To%20make%20the%20work%20reproducible%2C%20the%20codes%20will%20be%0Aavailable%20at%0Ahttps%3A//github.com/Koushikey4596/Multitask-Deep-Learning-Model-for-Taiga-datatset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multitask%2520Deep%2520Learning%2520Model%2520for%2520Classification%2520and%2520Regression%2520of%250A%2520%2520Hyperspectral%2520Images%253A%2520Application%2520to%2520the%2520large-scale%2520dataset%26entry.906535625%3DKoushikey%2520Chhapariya%2520and%2520Alexandre%2520Benoit%2520and%2520Krishna%2520Mohan%2520Buddhiraju%2520and%2520Anil%2520Kumar%26entry.1292438233%3D%2520%2520Multitask%2520learning%2520is%2520a%2520widely%2520recognized%2520technique%2520in%2520the%2520field%2520of%2520computer%250Avision%2520and%2520deep%2520learning%2520domain.%2520However%252C%2520it%2520is%2520still%2520a%2520research%2520question%2520in%250Aremote%2520sensing%252C%2520particularly%2520for%2520hyperspectral%2520imaging.%2520Moreover%252C%2520most%2520of%2520the%250Aresearch%2520in%2520the%2520remote%2520sensing%2520domain%2520focuses%2520on%2520small%2520and%2520single-task-based%250Aannotated%2520datasets%252C%2520which%2520limits%2520the%2520generalizability%2520and%2520scalability%2520of%2520the%250Adeveloped%2520models%2520to%2520more%2520diverse%2520and%2520complex%2520real-world%2520scenarios.%2520Thus%252C%2520in%250Athis%2520study%252C%2520we%2520propose%2520a%2520multitask%2520deep%2520learning%2520model%2520designed%2520to%2520perform%250Amultiple%2520classification%2520and%2520regression%2520tasks%2520simultaneously%2520on%2520hyperspectral%250Aimages.%2520We%2520validated%2520our%2520approach%2520on%2520a%2520large%2520hyperspectral%2520dataset%2520called%250ATAIGA%252C%2520which%2520contains%252013%2520forest%2520variables%252C%2520including%2520three%2520categorical%250Avariables%2520and%2520ten%2520continuous%2520variables%2520with%2520different%2520biophysical%2520parameters.%250AWe%2520design%2520a%2520sharing%2520encoder%2520and%2520task-specific%2520decoder%2520network%2520to%2520streamline%250Afeature%2520learning%2520while%2520allowing%2520each%2520task-specific%2520decoder%2520to%2520focus%2520on%2520the%250Aunique%2520aspects%2520of%2520its%2520respective%2520task.%250A%2520%2520Additionally%252C%2520a%2520dense%2520atrous%2520pyramid%2520pooling%2520layer%2520and%2520attention%2520network%2520were%250Aintegrated%2520to%2520extract%2520multi-scale%2520contextual%2520information%2520and%2520enable%2520selective%250Ainformation%2520processing%2520by%2520prioritizing%2520task-specific%2520features.%2520Further%252C%2520we%250Acomputed%2520multitask%2520loss%2520and%2520optimized%2520its%2520parameters%2520for%2520the%2520proposed%2520framework%250Ato%2520improve%2520the%2520model%2520performance%2520and%2520efficiency%2520across%2520diverse%2520tasks.%2520A%250Acomprehensive%2520qualitative%2520and%2520quantitative%2520analysis%2520of%2520the%2520results%2520shows%2520that%250Athe%2520proposed%2520method%2520significantly%2520outperforms%2520other%2520state-of-the-art%2520methods.%250AWe%2520trained%2520our%2520model%2520across%252010%2520seeds/trials%2520to%2520ensure%2520robustness.%2520Our%2520proposed%250Amodel%2520demonstrates%2520higher%2520mean%2520performance%2520while%2520maintaining%2520lower%2520or%250Aequivalent%2520variability.%2520To%2520make%2520the%2520work%2520reproducible%252C%2520the%2520codes%2520will%2520be%250Aavailable%2520at%250Ahttps%253A//github.com/Koushikey4596/Multitask-Deep-Learning-Model-for-Taiga-datatset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multitask%20Deep%20Learning%20Model%20for%20Classification%20and%20Regression%20of%0A%20%20Hyperspectral%20Images%3A%20Application%20to%20the%20large-scale%20dataset&entry.906535625=Koushikey%20Chhapariya%20and%20Alexandre%20Benoit%20and%20Krishna%20Mohan%20Buddhiraju%20and%20Anil%20Kumar&entry.1292438233=%20%20Multitask%20learning%20is%20a%20widely%20recognized%20technique%20in%20the%20field%20of%20computer%0Avision%20and%20deep%20learning%20domain.%20However%2C%20it%20is%20still%20a%20research%20question%20in%0Aremote%20sensing%2C%20particularly%20for%20hyperspectral%20imaging.%20Moreover%2C%20most%20of%20the%0Aresearch%20in%20the%20remote%20sensing%20domain%20focuses%20on%20small%20and%20single-task-based%0Aannotated%20datasets%2C%20which%20limits%20the%20generalizability%20and%20scalability%20of%20the%0Adeveloped%20models%20to%20more%20diverse%20and%20complex%20real-world%20scenarios.%20Thus%2C%20in%0Athis%20study%2C%20we%20propose%20a%20multitask%20deep%20learning%20model%20designed%20to%20perform%0Amultiple%20classification%20and%20regression%20tasks%20simultaneously%20on%20hyperspectral%0Aimages.%20We%20validated%20our%20approach%20on%20a%20large%20hyperspectral%20dataset%20called%0ATAIGA%2C%20which%20contains%2013%20forest%20variables%2C%20including%20three%20categorical%0Avariables%20and%20ten%20continuous%20variables%20with%20different%20biophysical%20parameters.%0AWe%20design%20a%20sharing%20encoder%20and%20task-specific%20decoder%20network%20to%20streamline%0Afeature%20learning%20while%20allowing%20each%20task-specific%20decoder%20to%20focus%20on%20the%0Aunique%20aspects%20of%20its%20respective%20task.%0A%20%20Additionally%2C%20a%20dense%20atrous%20pyramid%20pooling%20layer%20and%20attention%20network%20were%0Aintegrated%20to%20extract%20multi-scale%20contextual%20information%20and%20enable%20selective%0Ainformation%20processing%20by%20prioritizing%20task-specific%20features.%20Further%2C%20we%0Acomputed%20multitask%20loss%20and%20optimized%20its%20parameters%20for%20the%20proposed%20framework%0Ato%20improve%20the%20model%20performance%20and%20efficiency%20across%20diverse%20tasks.%20A%0Acomprehensive%20qualitative%20and%20quantitative%20analysis%20of%20the%20results%20shows%20that%0Athe%20proposed%20method%20significantly%20outperforms%20other%20state-of-the-art%20methods.%0AWe%20trained%20our%20model%20across%2010%20seeds/trials%20to%20ensure%20robustness.%20Our%20proposed%0Amodel%20demonstrates%20higher%20mean%20performance%20while%20maintaining%20lower%20or%0Aequivalent%20variability.%20To%20make%20the%20work%20reproducible%2C%20the%20codes%20will%20be%0Aavailable%20at%0Ahttps%3A//github.com/Koushikey4596/Multitask-Deep-Learning-Model-for-Taiga-datatset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16384v1&entry.124074799=Read"},
{"title": "End-to-End Video Question Answering with Frame Scoring Mechanisms and\n  Adaptive Sampling", "author": "Jianxin Liang and Xiaojun Meng and Yueqian Wang and Chang Liu and Qun Liu and Dongyan Zhao", "abstract": "  Video Question Answering (VideoQA) has emerged as a challenging frontier in\nthe field of multimedia processing, requiring intricate interactions between\nvisual and textual modalities. Simply uniformly sampling frames or\nindiscriminately aggregating frame-level visual features often falls short in\ncapturing the nuanced and relevant contexts of videos to well perform VideoQA.\nTo mitigate these issues, we propose VidF4, a novel VideoQA framework equipped\nwith tailored frame selection strategy for effective and efficient VideoQA. We\npropose three frame-scoring mechanisms that consider both question relevance\nand inter-frame similarity to evaluate the importance of each frame for a given\nquestion on the video. Furthermore, we design a differentiable adaptive frame\nsampling mechanism to facilitate end-to-end training for the frame selector and\nanswer generator. The experimental results across three widely adopted\nbenchmarks demonstrate that our model consistently outperforms existing VideoQA\nmethods, establishing a new SOTA across NExT-QA (+0.3%), STAR (+0.9%), and TVQA\n(+1.0%). Furthermore, through both quantitative and qualitative analyses, we\nvalidate the effectiveness of each design choice.\n", "link": "http://arxiv.org/abs/2407.15047v2", "date": "2024-07-23", "relevancy": 2.1539, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.564}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5512}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Video%20Question%20Answering%20with%20Frame%20Scoring%20Mechanisms%20and%0A%20%20Adaptive%20Sampling&body=Title%3A%20End-to-End%20Video%20Question%20Answering%20with%20Frame%20Scoring%20Mechanisms%20and%0A%20%20Adaptive%20Sampling%0AAuthor%3A%20Jianxin%20Liang%20and%20Xiaojun%20Meng%20and%20Yueqian%20Wang%20and%20Chang%20Liu%20and%20Qun%20Liu%20and%20Dongyan%20Zhao%0AAbstract%3A%20%20%20Video%20Question%20Answering%20%28VideoQA%29%20has%20emerged%20as%20a%20challenging%20frontier%20in%0Athe%20field%20of%20multimedia%20processing%2C%20requiring%20intricate%20interactions%20between%0Avisual%20and%20textual%20modalities.%20Simply%20uniformly%20sampling%20frames%20or%0Aindiscriminately%20aggregating%20frame-level%20visual%20features%20often%20falls%20short%20in%0Acapturing%20the%20nuanced%20and%20relevant%20contexts%20of%20videos%20to%20well%20perform%20VideoQA.%0ATo%20mitigate%20these%20issues%2C%20we%20propose%20VidF4%2C%20a%20novel%20VideoQA%20framework%20equipped%0Awith%20tailored%20frame%20selection%20strategy%20for%20effective%20and%20efficient%20VideoQA.%20We%0Apropose%20three%20frame-scoring%20mechanisms%20that%20consider%20both%20question%20relevance%0Aand%20inter-frame%20similarity%20to%20evaluate%20the%20importance%20of%20each%20frame%20for%20a%20given%0Aquestion%20on%20the%20video.%20Furthermore%2C%20we%20design%20a%20differentiable%20adaptive%20frame%0Asampling%20mechanism%20to%20facilitate%20end-to-end%20training%20for%20the%20frame%20selector%20and%0Aanswer%20generator.%20The%20experimental%20results%20across%20three%20widely%20adopted%0Abenchmarks%20demonstrate%20that%20our%20model%20consistently%20outperforms%20existing%20VideoQA%0Amethods%2C%20establishing%20a%20new%20SOTA%20across%20NExT-QA%20%28%2B0.3%25%29%2C%20STAR%20%28%2B0.9%25%29%2C%20and%20TVQA%0A%28%2B1.0%25%29.%20Furthermore%2C%20through%20both%20quantitative%20and%20qualitative%20analyses%2C%20we%0Avalidate%20the%20effectiveness%20of%20each%20design%20choice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Video%2520Question%2520Answering%2520with%2520Frame%2520Scoring%2520Mechanisms%2520and%250A%2520%2520Adaptive%2520Sampling%26entry.906535625%3DJianxin%2520Liang%2520and%2520Xiaojun%2520Meng%2520and%2520Yueqian%2520Wang%2520and%2520Chang%2520Liu%2520and%2520Qun%2520Liu%2520and%2520Dongyan%2520Zhao%26entry.1292438233%3D%2520%2520Video%2520Question%2520Answering%2520%2528VideoQA%2529%2520has%2520emerged%2520as%2520a%2520challenging%2520frontier%2520in%250Athe%2520field%2520of%2520multimedia%2520processing%252C%2520requiring%2520intricate%2520interactions%2520between%250Avisual%2520and%2520textual%2520modalities.%2520Simply%2520uniformly%2520sampling%2520frames%2520or%250Aindiscriminately%2520aggregating%2520frame-level%2520visual%2520features%2520often%2520falls%2520short%2520in%250Acapturing%2520the%2520nuanced%2520and%2520relevant%2520contexts%2520of%2520videos%2520to%2520well%2520perform%2520VideoQA.%250ATo%2520mitigate%2520these%2520issues%252C%2520we%2520propose%2520VidF4%252C%2520a%2520novel%2520VideoQA%2520framework%2520equipped%250Awith%2520tailored%2520frame%2520selection%2520strategy%2520for%2520effective%2520and%2520efficient%2520VideoQA.%2520We%250Apropose%2520three%2520frame-scoring%2520mechanisms%2520that%2520consider%2520both%2520question%2520relevance%250Aand%2520inter-frame%2520similarity%2520to%2520evaluate%2520the%2520importance%2520of%2520each%2520frame%2520for%2520a%2520given%250Aquestion%2520on%2520the%2520video.%2520Furthermore%252C%2520we%2520design%2520a%2520differentiable%2520adaptive%2520frame%250Asampling%2520mechanism%2520to%2520facilitate%2520end-to-end%2520training%2520for%2520the%2520frame%2520selector%2520and%250Aanswer%2520generator.%2520The%2520experimental%2520results%2520across%2520three%2520widely%2520adopted%250Abenchmarks%2520demonstrate%2520that%2520our%2520model%2520consistently%2520outperforms%2520existing%2520VideoQA%250Amethods%252C%2520establishing%2520a%2520new%2520SOTA%2520across%2520NExT-QA%2520%2528%252B0.3%2525%2529%252C%2520STAR%2520%2528%252B0.9%2525%2529%252C%2520and%2520TVQA%250A%2528%252B1.0%2525%2529.%2520Furthermore%252C%2520through%2520both%2520quantitative%2520and%2520qualitative%2520analyses%252C%2520we%250Avalidate%2520the%2520effectiveness%2520of%2520each%2520design%2520choice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Video%20Question%20Answering%20with%20Frame%20Scoring%20Mechanisms%20and%0A%20%20Adaptive%20Sampling&entry.906535625=Jianxin%20Liang%20and%20Xiaojun%20Meng%20and%20Yueqian%20Wang%20and%20Chang%20Liu%20and%20Qun%20Liu%20and%20Dongyan%20Zhao&entry.1292438233=%20%20Video%20Question%20Answering%20%28VideoQA%29%20has%20emerged%20as%20a%20challenging%20frontier%20in%0Athe%20field%20of%20multimedia%20processing%2C%20requiring%20intricate%20interactions%20between%0Avisual%20and%20textual%20modalities.%20Simply%20uniformly%20sampling%20frames%20or%0Aindiscriminately%20aggregating%20frame-level%20visual%20features%20often%20falls%20short%20in%0Acapturing%20the%20nuanced%20and%20relevant%20contexts%20of%20videos%20to%20well%20perform%20VideoQA.%0ATo%20mitigate%20these%20issues%2C%20we%20propose%20VidF4%2C%20a%20novel%20VideoQA%20framework%20equipped%0Awith%20tailored%20frame%20selection%20strategy%20for%20effective%20and%20efficient%20VideoQA.%20We%0Apropose%20three%20frame-scoring%20mechanisms%20that%20consider%20both%20question%20relevance%0Aand%20inter-frame%20similarity%20to%20evaluate%20the%20importance%20of%20each%20frame%20for%20a%20given%0Aquestion%20on%20the%20video.%20Furthermore%2C%20we%20design%20a%20differentiable%20adaptive%20frame%0Asampling%20mechanism%20to%20facilitate%20end-to-end%20training%20for%20the%20frame%20selector%20and%0Aanswer%20generator.%20The%20experimental%20results%20across%20three%20widely%20adopted%0Abenchmarks%20demonstrate%20that%20our%20model%20consistently%20outperforms%20existing%20VideoQA%0Amethods%2C%20establishing%20a%20new%20SOTA%20across%20NExT-QA%20%28%2B0.3%25%29%2C%20STAR%20%28%2B0.9%25%29%2C%20and%20TVQA%0A%28%2B1.0%25%29.%20Furthermore%2C%20through%20both%20quantitative%20and%20qualitative%20analyses%2C%20we%0Avalidate%20the%20effectiveness%20of%20each%20design%20choice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15047v2&entry.124074799=Read"},
{"title": "E(n) Equivariant Topological Neural Networks", "author": "Claudio Battiloro and Ege Karaismailo\u011flu and Mauricio Tec and George Dasoulas and Michelle Audirac and Francesca Dominici", "abstract": "  Graph neural networks excel at modeling pairwise interactions, but they\ncannot flexibly accommodate higher-order interactions and features. Topological\ndeep learning (TDL) has emerged recently as a promising tool for addressing\nthis issue. TDL enables the principled modeling of arbitrary multi-way,\nhierarchical higher-order interactions by operating on combinatorial\ntopological spaces, such as simplicial or cell complexes, instead of graphs.\nHowever, little is known about how to leverage geometric features such as\npositions and velocities for TDL. This paper introduces E(n)-Equivariant\nTopological Neural Networks (ETNNs), which are E(n)-equivariant message-passing\nnetworks operating on combinatorial complexes, formal objects unifying graphs,\nhypergraphs, simplicial, path, and cell complexes. ETNNs incorporate geometric\nnode features while respecting rotation and translation equivariance. Moreover,\nETNNs are natively ready for settings with heterogeneous interactions. We\nprovide a theoretical analysis to show the improved expressiveness of ETNNs\nover architectures for geometric graphs. We also show how several E(n)\nequivariant variants of TDL models can be directly derived from our framework.\nThe broad applicability of ETNNs is demonstrated through two tasks of vastly\ndifferent nature: i) molecular property prediction on the QM9 benchmark and ii)\nland-use regression for hyper-local estimation of air pollution with\nmulti-resolution irregular geospatial data. The experiment results indicate\nthat ETNNs are an effective tool for learning from diverse types of richly\nstructured data, highlighting the benefits of principled geometric inductive\nbias.\n", "link": "http://arxiv.org/abs/2405.15429v3", "date": "2024-07-23", "relevancy": 2.1537, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5797}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5161}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E%28n%29%20Equivariant%20Topological%20Neural%20Networks&body=Title%3A%20E%28n%29%20Equivariant%20Topological%20Neural%20Networks%0AAuthor%3A%20Claudio%20Battiloro%20and%20Ege%20Karaismailo%C4%9Flu%20and%20Mauricio%20Tec%20and%20George%20Dasoulas%20and%20Michelle%20Audirac%20and%20Francesca%20Dominici%0AAbstract%3A%20%20%20Graph%20neural%20networks%20excel%20at%20modeling%20pairwise%20interactions%2C%20but%20they%0Acannot%20flexibly%20accommodate%20higher-order%20interactions%20and%20features.%20Topological%0Adeep%20learning%20%28TDL%29%20has%20emerged%20recently%20as%20a%20promising%20tool%20for%20addressing%0Athis%20issue.%20TDL%20enables%20the%20principled%20modeling%20of%20arbitrary%20multi-way%2C%0Ahierarchical%20higher-order%20interactions%20by%20operating%20on%20combinatorial%0Atopological%20spaces%2C%20such%20as%20simplicial%20or%20cell%20complexes%2C%20instead%20of%20graphs.%0AHowever%2C%20little%20is%20known%20about%20how%20to%20leverage%20geometric%20features%20such%20as%0Apositions%20and%20velocities%20for%20TDL.%20This%20paper%20introduces%20E%28n%29-Equivariant%0ATopological%20Neural%20Networks%20%28ETNNs%29%2C%20which%20are%20E%28n%29-equivariant%20message-passing%0Anetworks%20operating%20on%20combinatorial%20complexes%2C%20formal%20objects%20unifying%20graphs%2C%0Ahypergraphs%2C%20simplicial%2C%20path%2C%20and%20cell%20complexes.%20ETNNs%20incorporate%20geometric%0Anode%20features%20while%20respecting%20rotation%20and%20translation%20equivariance.%20Moreover%2C%0AETNNs%20are%20natively%20ready%20for%20settings%20with%20heterogeneous%20interactions.%20We%0Aprovide%20a%20theoretical%20analysis%20to%20show%20the%20improved%20expressiveness%20of%20ETNNs%0Aover%20architectures%20for%20geometric%20graphs.%20We%20also%20show%20how%20several%20E%28n%29%0Aequivariant%20variants%20of%20TDL%20models%20can%20be%20directly%20derived%20from%20our%20framework.%0AThe%20broad%20applicability%20of%20ETNNs%20is%20demonstrated%20through%20two%20tasks%20of%20vastly%0Adifferent%20nature%3A%20i%29%20molecular%20property%20prediction%20on%20the%20QM9%20benchmark%20and%20ii%29%0Aland-use%20regression%20for%20hyper-local%20estimation%20of%20air%20pollution%20with%0Amulti-resolution%20irregular%20geospatial%20data.%20The%20experiment%20results%20indicate%0Athat%20ETNNs%20are%20an%20effective%20tool%20for%20learning%20from%20diverse%20types%20of%20richly%0Astructured%20data%2C%20highlighting%20the%20benefits%20of%20principled%20geometric%20inductive%0Abias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15429v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE%2528n%2529%2520Equivariant%2520Topological%2520Neural%2520Networks%26entry.906535625%3DClaudio%2520Battiloro%2520and%2520Ege%2520Karaismailo%25C4%259Flu%2520and%2520Mauricio%2520Tec%2520and%2520George%2520Dasoulas%2520and%2520Michelle%2520Audirac%2520and%2520Francesca%2520Dominici%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520excel%2520at%2520modeling%2520pairwise%2520interactions%252C%2520but%2520they%250Acannot%2520flexibly%2520accommodate%2520higher-order%2520interactions%2520and%2520features.%2520Topological%250Adeep%2520learning%2520%2528TDL%2529%2520has%2520emerged%2520recently%2520as%2520a%2520promising%2520tool%2520for%2520addressing%250Athis%2520issue.%2520TDL%2520enables%2520the%2520principled%2520modeling%2520of%2520arbitrary%2520multi-way%252C%250Ahierarchical%2520higher-order%2520interactions%2520by%2520operating%2520on%2520combinatorial%250Atopological%2520spaces%252C%2520such%2520as%2520simplicial%2520or%2520cell%2520complexes%252C%2520instead%2520of%2520graphs.%250AHowever%252C%2520little%2520is%2520known%2520about%2520how%2520to%2520leverage%2520geometric%2520features%2520such%2520as%250Apositions%2520and%2520velocities%2520for%2520TDL.%2520This%2520paper%2520introduces%2520E%2528n%2529-Equivariant%250ATopological%2520Neural%2520Networks%2520%2528ETNNs%2529%252C%2520which%2520are%2520E%2528n%2529-equivariant%2520message-passing%250Anetworks%2520operating%2520on%2520combinatorial%2520complexes%252C%2520formal%2520objects%2520unifying%2520graphs%252C%250Ahypergraphs%252C%2520simplicial%252C%2520path%252C%2520and%2520cell%2520complexes.%2520ETNNs%2520incorporate%2520geometric%250Anode%2520features%2520while%2520respecting%2520rotation%2520and%2520translation%2520equivariance.%2520Moreover%252C%250AETNNs%2520are%2520natively%2520ready%2520for%2520settings%2520with%2520heterogeneous%2520interactions.%2520We%250Aprovide%2520a%2520theoretical%2520analysis%2520to%2520show%2520the%2520improved%2520expressiveness%2520of%2520ETNNs%250Aover%2520architectures%2520for%2520geometric%2520graphs.%2520We%2520also%2520show%2520how%2520several%2520E%2528n%2529%250Aequivariant%2520variants%2520of%2520TDL%2520models%2520can%2520be%2520directly%2520derived%2520from%2520our%2520framework.%250AThe%2520broad%2520applicability%2520of%2520ETNNs%2520is%2520demonstrated%2520through%2520two%2520tasks%2520of%2520vastly%250Adifferent%2520nature%253A%2520i%2529%2520molecular%2520property%2520prediction%2520on%2520the%2520QM9%2520benchmark%2520and%2520ii%2529%250Aland-use%2520regression%2520for%2520hyper-local%2520estimation%2520of%2520air%2520pollution%2520with%250Amulti-resolution%2520irregular%2520geospatial%2520data.%2520The%2520experiment%2520results%2520indicate%250Athat%2520ETNNs%2520are%2520an%2520effective%2520tool%2520for%2520learning%2520from%2520diverse%2520types%2520of%2520richly%250Astructured%2520data%252C%2520highlighting%2520the%2520benefits%2520of%2520principled%2520geometric%2520inductive%250Abias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15429v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E%28n%29%20Equivariant%20Topological%20Neural%20Networks&entry.906535625=Claudio%20Battiloro%20and%20Ege%20Karaismailo%C4%9Flu%20and%20Mauricio%20Tec%20and%20George%20Dasoulas%20and%20Michelle%20Audirac%20and%20Francesca%20Dominici&entry.1292438233=%20%20Graph%20neural%20networks%20excel%20at%20modeling%20pairwise%20interactions%2C%20but%20they%0Acannot%20flexibly%20accommodate%20higher-order%20interactions%20and%20features.%20Topological%0Adeep%20learning%20%28TDL%29%20has%20emerged%20recently%20as%20a%20promising%20tool%20for%20addressing%0Athis%20issue.%20TDL%20enables%20the%20principled%20modeling%20of%20arbitrary%20multi-way%2C%0Ahierarchical%20higher-order%20interactions%20by%20operating%20on%20combinatorial%0Atopological%20spaces%2C%20such%20as%20simplicial%20or%20cell%20complexes%2C%20instead%20of%20graphs.%0AHowever%2C%20little%20is%20known%20about%20how%20to%20leverage%20geometric%20features%20such%20as%0Apositions%20and%20velocities%20for%20TDL.%20This%20paper%20introduces%20E%28n%29-Equivariant%0ATopological%20Neural%20Networks%20%28ETNNs%29%2C%20which%20are%20E%28n%29-equivariant%20message-passing%0Anetworks%20operating%20on%20combinatorial%20complexes%2C%20formal%20objects%20unifying%20graphs%2C%0Ahypergraphs%2C%20simplicial%2C%20path%2C%20and%20cell%20complexes.%20ETNNs%20incorporate%20geometric%0Anode%20features%20while%20respecting%20rotation%20and%20translation%20equivariance.%20Moreover%2C%0AETNNs%20are%20natively%20ready%20for%20settings%20with%20heterogeneous%20interactions.%20We%0Aprovide%20a%20theoretical%20analysis%20to%20show%20the%20improved%20expressiveness%20of%20ETNNs%0Aover%20architectures%20for%20geometric%20graphs.%20We%20also%20show%20how%20several%20E%28n%29%0Aequivariant%20variants%20of%20TDL%20models%20can%20be%20directly%20derived%20from%20our%20framework.%0AThe%20broad%20applicability%20of%20ETNNs%20is%20demonstrated%20through%20two%20tasks%20of%20vastly%0Adifferent%20nature%3A%20i%29%20molecular%20property%20prediction%20on%20the%20QM9%20benchmark%20and%20ii%29%0Aland-use%20regression%20for%20hyper-local%20estimation%20of%20air%20pollution%20with%0Amulti-resolution%20irregular%20geospatial%20data.%20The%20experiment%20results%20indicate%0Athat%20ETNNs%20are%20an%20effective%20tool%20for%20learning%20from%20diverse%20types%20of%20richly%0Astructured%20data%2C%20highlighting%20the%20benefits%20of%20principled%20geometric%20inductive%0Abias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15429v3&entry.124074799=Read"},
{"title": "Low Complexity Regularized Phase Retrieval", "author": "Jean-Jacques Godeme and Jalal Fadili", "abstract": "  In this paper, we study the phase retrieval problem in the situation where\nthe vector to be recovered has an a priori structure that can encoded into a\nregularization term. This regularizer is intended to promote solutions\nconforming to some notion of simplicity or low complexity. We investigate both\nnoiseless recovery and stability to noise and provide a very general and\nunified analysis framework that goes far beyond the sparse phase retrieval\nmostly considered in the literature. In the noiseless case we provide\nsufficient conditions under which exact recovery, up to global sign change, is\npossible. For Gaussian measurement maps, we also provide a sample complexity\nbound for exact recovery. This bound depends on the Gaussian width of the\ndescent cone at the soughtafter vector which is a geometric measure of the\ncomplexity of the latter. In the noisy case, we consider both the constrained\n(Mozorov) and penalized (Tikhonov) formulations. We provide sufficient\nconditions for stable recovery and prove linear convergence for sufficiently\nsmall noise. For Gaussian measurements, we again give a sample complexity bound\nfor linear convergence to hold with high probability. This bound scales\nlinearly in the intrinsic dimension of the sought-after vector but only\nlogarithmically in the ambient dimension.\n", "link": "http://arxiv.org/abs/2407.16413v1", "date": "2024-07-23", "relevancy": 2.1374, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4347}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4266}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low%20Complexity%20Regularized%20Phase%20Retrieval&body=Title%3A%20Low%20Complexity%20Regularized%20Phase%20Retrieval%0AAuthor%3A%20Jean-Jacques%20Godeme%20and%20Jalal%20Fadili%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20the%20phase%20retrieval%20problem%20in%20the%20situation%20where%0Athe%20vector%20to%20be%20recovered%20has%20an%20a%20priori%20structure%20that%20can%20encoded%20into%20a%0Aregularization%20term.%20This%20regularizer%20is%20intended%20to%20promote%20solutions%0Aconforming%20to%20some%20notion%20of%20simplicity%20or%20low%20complexity.%20We%20investigate%20both%0Anoiseless%20recovery%20and%20stability%20to%20noise%20and%20provide%20a%20very%20general%20and%0Aunified%20analysis%20framework%20that%20goes%20far%20beyond%20the%20sparse%20phase%20retrieval%0Amostly%20considered%20in%20the%20literature.%20In%20the%20noiseless%20case%20we%20provide%0Asufficient%20conditions%20under%20which%20exact%20recovery%2C%20up%20to%20global%20sign%20change%2C%20is%0Apossible.%20For%20Gaussian%20measurement%20maps%2C%20we%20also%20provide%20a%20sample%20complexity%0Abound%20for%20exact%20recovery.%20This%20bound%20depends%20on%20the%20Gaussian%20width%20of%20the%0Adescent%20cone%20at%20the%20soughtafter%20vector%20which%20is%20a%20geometric%20measure%20of%20the%0Acomplexity%20of%20the%20latter.%20In%20the%20noisy%20case%2C%20we%20consider%20both%20the%20constrained%0A%28Mozorov%29%20and%20penalized%20%28Tikhonov%29%20formulations.%20We%20provide%20sufficient%0Aconditions%20for%20stable%20recovery%20and%20prove%20linear%20convergence%20for%20sufficiently%0Asmall%20noise.%20For%20Gaussian%20measurements%2C%20we%20again%20give%20a%20sample%20complexity%20bound%0Afor%20linear%20convergence%20to%20hold%20with%20high%20probability.%20This%20bound%20scales%0Alinearly%20in%20the%20intrinsic%20dimension%20of%20the%20sought-after%20vector%20but%20only%0Alogarithmically%20in%20the%20ambient%20dimension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow%2520Complexity%2520Regularized%2520Phase%2520Retrieval%26entry.906535625%3DJean-Jacques%2520Godeme%2520and%2520Jalal%2520Fadili%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520phase%2520retrieval%2520problem%2520in%2520the%2520situation%2520where%250Athe%2520vector%2520to%2520be%2520recovered%2520has%2520an%2520a%2520priori%2520structure%2520that%2520can%2520encoded%2520into%2520a%250Aregularization%2520term.%2520This%2520regularizer%2520is%2520intended%2520to%2520promote%2520solutions%250Aconforming%2520to%2520some%2520notion%2520of%2520simplicity%2520or%2520low%2520complexity.%2520We%2520investigate%2520both%250Anoiseless%2520recovery%2520and%2520stability%2520to%2520noise%2520and%2520provide%2520a%2520very%2520general%2520and%250Aunified%2520analysis%2520framework%2520that%2520goes%2520far%2520beyond%2520the%2520sparse%2520phase%2520retrieval%250Amostly%2520considered%2520in%2520the%2520literature.%2520In%2520the%2520noiseless%2520case%2520we%2520provide%250Asufficient%2520conditions%2520under%2520which%2520exact%2520recovery%252C%2520up%2520to%2520global%2520sign%2520change%252C%2520is%250Apossible.%2520For%2520Gaussian%2520measurement%2520maps%252C%2520we%2520also%2520provide%2520a%2520sample%2520complexity%250Abound%2520for%2520exact%2520recovery.%2520This%2520bound%2520depends%2520on%2520the%2520Gaussian%2520width%2520of%2520the%250Adescent%2520cone%2520at%2520the%2520soughtafter%2520vector%2520which%2520is%2520a%2520geometric%2520measure%2520of%2520the%250Acomplexity%2520of%2520the%2520latter.%2520In%2520the%2520noisy%2520case%252C%2520we%2520consider%2520both%2520the%2520constrained%250A%2528Mozorov%2529%2520and%2520penalized%2520%2528Tikhonov%2529%2520formulations.%2520We%2520provide%2520sufficient%250Aconditions%2520for%2520stable%2520recovery%2520and%2520prove%2520linear%2520convergence%2520for%2520sufficiently%250Asmall%2520noise.%2520For%2520Gaussian%2520measurements%252C%2520we%2520again%2520give%2520a%2520sample%2520complexity%2520bound%250Afor%2520linear%2520convergence%2520to%2520hold%2520with%2520high%2520probability.%2520This%2520bound%2520scales%250Alinearly%2520in%2520the%2520intrinsic%2520dimension%2520of%2520the%2520sought-after%2520vector%2520but%2520only%250Alogarithmically%2520in%2520the%2520ambient%2520dimension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low%20Complexity%20Regularized%20Phase%20Retrieval&entry.906535625=Jean-Jacques%20Godeme%20and%20Jalal%20Fadili&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20the%20phase%20retrieval%20problem%20in%20the%20situation%20where%0Athe%20vector%20to%20be%20recovered%20has%20an%20a%20priori%20structure%20that%20can%20encoded%20into%20a%0Aregularization%20term.%20This%20regularizer%20is%20intended%20to%20promote%20solutions%0Aconforming%20to%20some%20notion%20of%20simplicity%20or%20low%20complexity.%20We%20investigate%20both%0Anoiseless%20recovery%20and%20stability%20to%20noise%20and%20provide%20a%20very%20general%20and%0Aunified%20analysis%20framework%20that%20goes%20far%20beyond%20the%20sparse%20phase%20retrieval%0Amostly%20considered%20in%20the%20literature.%20In%20the%20noiseless%20case%20we%20provide%0Asufficient%20conditions%20under%20which%20exact%20recovery%2C%20up%20to%20global%20sign%20change%2C%20is%0Apossible.%20For%20Gaussian%20measurement%20maps%2C%20we%20also%20provide%20a%20sample%20complexity%0Abound%20for%20exact%20recovery.%20This%20bound%20depends%20on%20the%20Gaussian%20width%20of%20the%0Adescent%20cone%20at%20the%20soughtafter%20vector%20which%20is%20a%20geometric%20measure%20of%20the%0Acomplexity%20of%20the%20latter.%20In%20the%20noisy%20case%2C%20we%20consider%20both%20the%20constrained%0A%28Mozorov%29%20and%20penalized%20%28Tikhonov%29%20formulations.%20We%20provide%20sufficient%0Aconditions%20for%20stable%20recovery%20and%20prove%20linear%20convergence%20for%20sufficiently%0Asmall%20noise.%20For%20Gaussian%20measurements%2C%20we%20again%20give%20a%20sample%20complexity%20bound%0Afor%20linear%20convergence%20to%20hold%20with%20high%20probability.%20This%20bound%20scales%0Alinearly%20in%20the%20intrinsic%20dimension%20of%20the%20sought-after%20vector%20but%20only%0Alogarithmically%20in%20the%20ambient%20dimension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16413v1&entry.124074799=Read"},
{"title": "Real-Time Interactions Between Human Controllers and Remote Devices in\n  Metaverse", "author": "Kan Chen and Zhen Meng and Xiangmin Xu and Changyang She and Philip G. Zhao", "abstract": "  Supporting real-time interactions between human controllers and remote\ndevices remains a challenging goal in the Metaverse due to the stringent\nrequirements on computing workload, communication throughput, and round-trip\nlatency. In this paper, we establish a novel framework for real-time\ninteractions through the virtual models in the Metaverse. Specifically, we\njointly predict the motion of the human controller for 1) proactive rendering\nin the Metaverse and 2) generating control commands to the real-world remote\ndevice in advance. The virtual model is decoupled into two components for\nrendering and control, respectively. To dynamically adjust the prediction\nhorizons for rendering and control, we develop a two-step human-in-the-loop\ncontinuous reinforcement learning approach and use an expert policy to improve\nthe training efficiency. An experimental prototype is built to verify our\nalgorithm with different communication latencies. Compared with the baseline\npolicy without prediction, our proposed method can reduce 1) the\nMotion-To-Photon (MTP) latency between human motion and rendering feedback and\n2) the root mean squared error (RMSE) between human motion and real-world\nremote devices significantly.\n", "link": "http://arxiv.org/abs/2407.16591v1", "date": "2024-07-23", "relevancy": 2.1365, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5624}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5484}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Interactions%20Between%20Human%20Controllers%20and%20Remote%20Devices%20in%0A%20%20Metaverse&body=Title%3A%20Real-Time%20Interactions%20Between%20Human%20Controllers%20and%20Remote%20Devices%20in%0A%20%20Metaverse%0AAuthor%3A%20Kan%20Chen%20and%20Zhen%20Meng%20and%20Xiangmin%20Xu%20and%20Changyang%20She%20and%20Philip%20G.%20Zhao%0AAbstract%3A%20%20%20Supporting%20real-time%20interactions%20between%20human%20controllers%20and%20remote%0Adevices%20remains%20a%20challenging%20goal%20in%20the%20Metaverse%20due%20to%20the%20stringent%0Arequirements%20on%20computing%20workload%2C%20communication%20throughput%2C%20and%20round-trip%0Alatency.%20In%20this%20paper%2C%20we%20establish%20a%20novel%20framework%20for%20real-time%0Ainteractions%20through%20the%20virtual%20models%20in%20the%20Metaverse.%20Specifically%2C%20we%0Ajointly%20predict%20the%20motion%20of%20the%20human%20controller%20for%201%29%20proactive%20rendering%0Ain%20the%20Metaverse%20and%202%29%20generating%20control%20commands%20to%20the%20real-world%20remote%0Adevice%20in%20advance.%20The%20virtual%20model%20is%20decoupled%20into%20two%20components%20for%0Arendering%20and%20control%2C%20respectively.%20To%20dynamically%20adjust%20the%20prediction%0Ahorizons%20for%20rendering%20and%20control%2C%20we%20develop%20a%20two-step%20human-in-the-loop%0Acontinuous%20reinforcement%20learning%20approach%20and%20use%20an%20expert%20policy%20to%20improve%0Athe%20training%20efficiency.%20An%20experimental%20prototype%20is%20built%20to%20verify%20our%0Aalgorithm%20with%20different%20communication%20latencies.%20Compared%20with%20the%20baseline%0Apolicy%20without%20prediction%2C%20our%20proposed%20method%20can%20reduce%201%29%20the%0AMotion-To-Photon%20%28MTP%29%20latency%20between%20human%20motion%20and%20rendering%20feedback%20and%0A2%29%20the%20root%20mean%20squared%20error%20%28RMSE%29%20between%20human%20motion%20and%20real-world%0Aremote%20devices%20significantly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Interactions%2520Between%2520Human%2520Controllers%2520and%2520Remote%2520Devices%2520in%250A%2520%2520Metaverse%26entry.906535625%3DKan%2520Chen%2520and%2520Zhen%2520Meng%2520and%2520Xiangmin%2520Xu%2520and%2520Changyang%2520She%2520and%2520Philip%2520G.%2520Zhao%26entry.1292438233%3D%2520%2520Supporting%2520real-time%2520interactions%2520between%2520human%2520controllers%2520and%2520remote%250Adevices%2520remains%2520a%2520challenging%2520goal%2520in%2520the%2520Metaverse%2520due%2520to%2520the%2520stringent%250Arequirements%2520on%2520computing%2520workload%252C%2520communication%2520throughput%252C%2520and%2520round-trip%250Alatency.%2520In%2520this%2520paper%252C%2520we%2520establish%2520a%2520novel%2520framework%2520for%2520real-time%250Ainteractions%2520through%2520the%2520virtual%2520models%2520in%2520the%2520Metaverse.%2520Specifically%252C%2520we%250Ajointly%2520predict%2520the%2520motion%2520of%2520the%2520human%2520controller%2520for%25201%2529%2520proactive%2520rendering%250Ain%2520the%2520Metaverse%2520and%25202%2529%2520generating%2520control%2520commands%2520to%2520the%2520real-world%2520remote%250Adevice%2520in%2520advance.%2520The%2520virtual%2520model%2520is%2520decoupled%2520into%2520two%2520components%2520for%250Arendering%2520and%2520control%252C%2520respectively.%2520To%2520dynamically%2520adjust%2520the%2520prediction%250Ahorizons%2520for%2520rendering%2520and%2520control%252C%2520we%2520develop%2520a%2520two-step%2520human-in-the-loop%250Acontinuous%2520reinforcement%2520learning%2520approach%2520and%2520use%2520an%2520expert%2520policy%2520to%2520improve%250Athe%2520training%2520efficiency.%2520An%2520experimental%2520prototype%2520is%2520built%2520to%2520verify%2520our%250Aalgorithm%2520with%2520different%2520communication%2520latencies.%2520Compared%2520with%2520the%2520baseline%250Apolicy%2520without%2520prediction%252C%2520our%2520proposed%2520method%2520can%2520reduce%25201%2529%2520the%250AMotion-To-Photon%2520%2528MTP%2529%2520latency%2520between%2520human%2520motion%2520and%2520rendering%2520feedback%2520and%250A2%2529%2520the%2520root%2520mean%2520squared%2520error%2520%2528RMSE%2529%2520between%2520human%2520motion%2520and%2520real-world%250Aremote%2520devices%2520significantly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Interactions%20Between%20Human%20Controllers%20and%20Remote%20Devices%20in%0A%20%20Metaverse&entry.906535625=Kan%20Chen%20and%20Zhen%20Meng%20and%20Xiangmin%20Xu%20and%20Changyang%20She%20and%20Philip%20G.%20Zhao&entry.1292438233=%20%20Supporting%20real-time%20interactions%20between%20human%20controllers%20and%20remote%0Adevices%20remains%20a%20challenging%20goal%20in%20the%20Metaverse%20due%20to%20the%20stringent%0Arequirements%20on%20computing%20workload%2C%20communication%20throughput%2C%20and%20round-trip%0Alatency.%20In%20this%20paper%2C%20we%20establish%20a%20novel%20framework%20for%20real-time%0Ainteractions%20through%20the%20virtual%20models%20in%20the%20Metaverse.%20Specifically%2C%20we%0Ajointly%20predict%20the%20motion%20of%20the%20human%20controller%20for%201%29%20proactive%20rendering%0Ain%20the%20Metaverse%20and%202%29%20generating%20control%20commands%20to%20the%20real-world%20remote%0Adevice%20in%20advance.%20The%20virtual%20model%20is%20decoupled%20into%20two%20components%20for%0Arendering%20and%20control%2C%20respectively.%20To%20dynamically%20adjust%20the%20prediction%0Ahorizons%20for%20rendering%20and%20control%2C%20we%20develop%20a%20two-step%20human-in-the-loop%0Acontinuous%20reinforcement%20learning%20approach%20and%20use%20an%20expert%20policy%20to%20improve%0Athe%20training%20efficiency.%20An%20experimental%20prototype%20is%20built%20to%20verify%20our%0Aalgorithm%20with%20different%20communication%20latencies.%20Compared%20with%20the%20baseline%0Apolicy%20without%20prediction%2C%20our%20proposed%20method%20can%20reduce%201%29%20the%0AMotion-To-Photon%20%28MTP%29%20latency%20between%20human%20motion%20and%20rendering%20feedback%20and%0A2%29%20the%20root%20mean%20squared%20error%20%28RMSE%29%20between%20human%20motion%20and%20real-world%0Aremote%20devices%20significantly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16591v1&entry.124074799=Read"},
{"title": "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with\n  Micro-Expression Dynamics in Video Dialogues", "author": "Liyun Zhang", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nmultimodal emotion recognition capabilities, integrating multimodal cues from\nvisual, acoustic, and linguistic contexts in the video to recognize human\nemotional states. However, existing methods ignore capturing local facial\nfeatures of temporal dynamics of micro-expressions and do not leverage the\ncontextual dependencies of the utterance-aware temporal segments in the video,\nthereby limiting their expected effectiveness to a certain extent. In this\nwork, we propose MicroEmo, a time-sensitive MLLM aimed at directing attention\nto the local facial micro-expression dynamics and the contextual dependencies\nof utterance-aware video clips. Our model incorporates two key architectural\ncontributions: (1) a global-local attention visual encoder that integrates\nglobal frame-level timestamp-bound image features with local facial features of\ntemporal dynamics of micro-expressions; (2) an utterance-aware video Q-Former\nthat captures multi-scale and contextual dependencies by generating visual\ntoken sequences for each utterance segment and for the entire video then\ncombining them. Preliminary qualitative experiments demonstrate that in a new\nExplainable Multimodal Emotion Recognition (EMER) task that exploits\nmulti-modal and multi-faceted clues to predict emotions in an open-vocabulary\n(OV) manner, MicroEmo demonstrates its effectiveness compared with the latest\nmethods.\n", "link": "http://arxiv.org/abs/2407.16552v1", "date": "2024-07-23", "relevancy": 2.1346, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.545}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MicroEmo%3A%20Time-Sensitive%20Multimodal%20Emotion%20Recognition%20with%0A%20%20Micro-Expression%20Dynamics%20in%20Video%20Dialogues&body=Title%3A%20MicroEmo%3A%20Time-Sensitive%20Multimodal%20Emotion%20Recognition%20with%0A%20%20Micro-Expression%20Dynamics%20in%20Video%20Dialogues%0AAuthor%3A%20Liyun%20Zhang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20remarkable%0Amultimodal%20emotion%20recognition%20capabilities%2C%20integrating%20multimodal%20cues%20from%0Avisual%2C%20acoustic%2C%20and%20linguistic%20contexts%20in%20the%20video%20to%20recognize%20human%0Aemotional%20states.%20However%2C%20existing%20methods%20ignore%20capturing%20local%20facial%0Afeatures%20of%20temporal%20dynamics%20of%20micro-expressions%20and%20do%20not%20leverage%20the%0Acontextual%20dependencies%20of%20the%20utterance-aware%20temporal%20segments%20in%20the%20video%2C%0Athereby%20limiting%20their%20expected%20effectiveness%20to%20a%20certain%20extent.%20In%20this%0Awork%2C%20we%20propose%20MicroEmo%2C%20a%20time-sensitive%20MLLM%20aimed%20at%20directing%20attention%0Ato%20the%20local%20facial%20micro-expression%20dynamics%20and%20the%20contextual%20dependencies%0Aof%20utterance-aware%20video%20clips.%20Our%20model%20incorporates%20two%20key%20architectural%0Acontributions%3A%20%281%29%20a%20global-local%20attention%20visual%20encoder%20that%20integrates%0Aglobal%20frame-level%20timestamp-bound%20image%20features%20with%20local%20facial%20features%20of%0Atemporal%20dynamics%20of%20micro-expressions%3B%20%282%29%20an%20utterance-aware%20video%20Q-Former%0Athat%20captures%20multi-scale%20and%20contextual%20dependencies%20by%20generating%20visual%0Atoken%20sequences%20for%20each%20utterance%20segment%20and%20for%20the%20entire%20video%20then%0Acombining%20them.%20Preliminary%20qualitative%20experiments%20demonstrate%20that%20in%20a%20new%0AExplainable%20Multimodal%20Emotion%20Recognition%20%28EMER%29%20task%20that%20exploits%0Amulti-modal%20and%20multi-faceted%20clues%20to%20predict%20emotions%20in%20an%20open-vocabulary%0A%28OV%29%20manner%2C%20MicroEmo%20demonstrates%20its%20effectiveness%20compared%20with%20the%20latest%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMicroEmo%253A%2520Time-Sensitive%2520Multimodal%2520Emotion%2520Recognition%2520with%250A%2520%2520Micro-Expression%2520Dynamics%2520in%2520Video%2520Dialogues%26entry.906535625%3DLiyun%2520Zhang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520remarkable%250Amultimodal%2520emotion%2520recognition%2520capabilities%252C%2520integrating%2520multimodal%2520cues%2520from%250Avisual%252C%2520acoustic%252C%2520and%2520linguistic%2520contexts%2520in%2520the%2520video%2520to%2520recognize%2520human%250Aemotional%2520states.%2520However%252C%2520existing%2520methods%2520ignore%2520capturing%2520local%2520facial%250Afeatures%2520of%2520temporal%2520dynamics%2520of%2520micro-expressions%2520and%2520do%2520not%2520leverage%2520the%250Acontextual%2520dependencies%2520of%2520the%2520utterance-aware%2520temporal%2520segments%2520in%2520the%2520video%252C%250Athereby%2520limiting%2520their%2520expected%2520effectiveness%2520to%2520a%2520certain%2520extent.%2520In%2520this%250Awork%252C%2520we%2520propose%2520MicroEmo%252C%2520a%2520time-sensitive%2520MLLM%2520aimed%2520at%2520directing%2520attention%250Ato%2520the%2520local%2520facial%2520micro-expression%2520dynamics%2520and%2520the%2520contextual%2520dependencies%250Aof%2520utterance-aware%2520video%2520clips.%2520Our%2520model%2520incorporates%2520two%2520key%2520architectural%250Acontributions%253A%2520%25281%2529%2520a%2520global-local%2520attention%2520visual%2520encoder%2520that%2520integrates%250Aglobal%2520frame-level%2520timestamp-bound%2520image%2520features%2520with%2520local%2520facial%2520features%2520of%250Atemporal%2520dynamics%2520of%2520micro-expressions%253B%2520%25282%2529%2520an%2520utterance-aware%2520video%2520Q-Former%250Athat%2520captures%2520multi-scale%2520and%2520contextual%2520dependencies%2520by%2520generating%2520visual%250Atoken%2520sequences%2520for%2520each%2520utterance%2520segment%2520and%2520for%2520the%2520entire%2520video%2520then%250Acombining%2520them.%2520Preliminary%2520qualitative%2520experiments%2520demonstrate%2520that%2520in%2520a%2520new%250AExplainable%2520Multimodal%2520Emotion%2520Recognition%2520%2528EMER%2529%2520task%2520that%2520exploits%250Amulti-modal%2520and%2520multi-faceted%2520clues%2520to%2520predict%2520emotions%2520in%2520an%2520open-vocabulary%250A%2528OV%2529%2520manner%252C%2520MicroEmo%2520demonstrates%2520its%2520effectiveness%2520compared%2520with%2520the%2520latest%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MicroEmo%3A%20Time-Sensitive%20Multimodal%20Emotion%20Recognition%20with%0A%20%20Micro-Expression%20Dynamics%20in%20Video%20Dialogues&entry.906535625=Liyun%20Zhang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20remarkable%0Amultimodal%20emotion%20recognition%20capabilities%2C%20integrating%20multimodal%20cues%20from%0Avisual%2C%20acoustic%2C%20and%20linguistic%20contexts%20in%20the%20video%20to%20recognize%20human%0Aemotional%20states.%20However%2C%20existing%20methods%20ignore%20capturing%20local%20facial%0Afeatures%20of%20temporal%20dynamics%20of%20micro-expressions%20and%20do%20not%20leverage%20the%0Acontextual%20dependencies%20of%20the%20utterance-aware%20temporal%20segments%20in%20the%20video%2C%0Athereby%20limiting%20their%20expected%20effectiveness%20to%20a%20certain%20extent.%20In%20this%0Awork%2C%20we%20propose%20MicroEmo%2C%20a%20time-sensitive%20MLLM%20aimed%20at%20directing%20attention%0Ato%20the%20local%20facial%20micro-expression%20dynamics%20and%20the%20contextual%20dependencies%0Aof%20utterance-aware%20video%20clips.%20Our%20model%20incorporates%20two%20key%20architectural%0Acontributions%3A%20%281%29%20a%20global-local%20attention%20visual%20encoder%20that%20integrates%0Aglobal%20frame-level%20timestamp-bound%20image%20features%20with%20local%20facial%20features%20of%0Atemporal%20dynamics%20of%20micro-expressions%3B%20%282%29%20an%20utterance-aware%20video%20Q-Former%0Athat%20captures%20multi-scale%20and%20contextual%20dependencies%20by%20generating%20visual%0Atoken%20sequences%20for%20each%20utterance%20segment%20and%20for%20the%20entire%20video%20then%0Acombining%20them.%20Preliminary%20qualitative%20experiments%20demonstrate%20that%20in%20a%20new%0AExplainable%20Multimodal%20Emotion%20Recognition%20%28EMER%29%20task%20that%20exploits%0Amulti-modal%20and%20multi-faceted%20clues%20to%20predict%20emotions%20in%20an%20open-vocabulary%0A%28OV%29%20manner%2C%20MicroEmo%20demonstrates%20its%20effectiveness%20compared%20with%20the%20latest%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16552v1&entry.124074799=Read"},
{"title": "CeCNN: Copula-enhanced convolutional neural networks in joint prediction\n  of refraction error and axial length based on ultra-widefield fundus images", "author": "Chong Zhong and Yang Li and Danjuan Yang and Meiyan Li and Xingyao Zhou and Bo Fu and Catherine C. Liu and A. H. Welsh", "abstract": "  The ultra-widefield (UWF) fundus image is an attractive 3D biomarker in\nAI-aided myopia screening because it provides much richer myopia-related\ninformation. Though axial length (AL) has been acknowledged to be highly\nrelated to the two key targets of myopia screening, Spherical Equivalence (SE)\nmeasuring and high myopia diagnosis, its prediction based on the UWF fundus\nimage is rarely considered. To save the high expense and time costs of\nmeasuring SE and AL, we propose the Copula-enhanced Convolutional Neural\nNetwork (CeCNN), a one-stop UWF-based ophthalmic AI framework to jointly\npredict SE, AL, and myopia status. The CeCNN formulates a multiresponse\nregression that relates multiple dependent discrete-continuous responses and\nthe image covariate, where the nonlinearity of the association is modeled by a\nbackbone CNN. To thoroughly describe the dependence structure among the\nresponses, we model and incorporate the conditional dependence among responses\nin a CNN through a new copula-likelihood loss. We provide statistical\ninterpretations of the conditional dependence among responses, and reveal that\nsuch dependence is beyond the dependence explained by the image covariate. We\nheuristically justify that the proposed loss can enhance the estimation\nefficiency of the CNN weights. We apply the CeCNN to the UWF dataset collected\nby us and demonstrate that the CeCNN sharply enhances the predictive capability\nof various backbone CNNs. Our study evidences the ophthalmology view that\nbesides SE, AL is also an important measure to myopia.\n", "link": "http://arxiv.org/abs/2311.03967v3", "date": "2024-07-23", "relevancy": 2.1283, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5571}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5146}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CeCNN%3A%20Copula-enhanced%20convolutional%20neural%20networks%20in%20joint%20prediction%0A%20%20of%20refraction%20error%20and%20axial%20length%20based%20on%20ultra-widefield%20fundus%20images&body=Title%3A%20CeCNN%3A%20Copula-enhanced%20convolutional%20neural%20networks%20in%20joint%20prediction%0A%20%20of%20refraction%20error%20and%20axial%20length%20based%20on%20ultra-widefield%20fundus%20images%0AAuthor%3A%20Chong%20Zhong%20and%20Yang%20Li%20and%20Danjuan%20Yang%20and%20Meiyan%20Li%20and%20Xingyao%20Zhou%20and%20Bo%20Fu%20and%20Catherine%20C.%20Liu%20and%20A.%20H.%20Welsh%0AAbstract%3A%20%20%20The%20ultra-widefield%20%28UWF%29%20fundus%20image%20is%20an%20attractive%203D%20biomarker%20in%0AAI-aided%20myopia%20screening%20because%20it%20provides%20much%20richer%20myopia-related%0Ainformation.%20Though%20axial%20length%20%28AL%29%20has%20been%20acknowledged%20to%20be%20highly%0Arelated%20to%20the%20two%20key%20targets%20of%20myopia%20screening%2C%20Spherical%20Equivalence%20%28SE%29%0Ameasuring%20and%20high%20myopia%20diagnosis%2C%20its%20prediction%20based%20on%20the%20UWF%20fundus%0Aimage%20is%20rarely%20considered.%20To%20save%20the%20high%20expense%20and%20time%20costs%20of%0Ameasuring%20SE%20and%20AL%2C%20we%20propose%20the%20Copula-enhanced%20Convolutional%20Neural%0ANetwork%20%28CeCNN%29%2C%20a%20one-stop%20UWF-based%20ophthalmic%20AI%20framework%20to%20jointly%0Apredict%20SE%2C%20AL%2C%20and%20myopia%20status.%20The%20CeCNN%20formulates%20a%20multiresponse%0Aregression%20that%20relates%20multiple%20dependent%20discrete-continuous%20responses%20and%0Athe%20image%20covariate%2C%20where%20the%20nonlinearity%20of%20the%20association%20is%20modeled%20by%20a%0Abackbone%20CNN.%20To%20thoroughly%20describe%20the%20dependence%20structure%20among%20the%0Aresponses%2C%20we%20model%20and%20incorporate%20the%20conditional%20dependence%20among%20responses%0Ain%20a%20CNN%20through%20a%20new%20copula-likelihood%20loss.%20We%20provide%20statistical%0Ainterpretations%20of%20the%20conditional%20dependence%20among%20responses%2C%20and%20reveal%20that%0Asuch%20dependence%20is%20beyond%20the%20dependence%20explained%20by%20the%20image%20covariate.%20We%0Aheuristically%20justify%20that%20the%20proposed%20loss%20can%20enhance%20the%20estimation%0Aefficiency%20of%20the%20CNN%20weights.%20We%20apply%20the%20CeCNN%20to%20the%20UWF%20dataset%20collected%0Aby%20us%20and%20demonstrate%20that%20the%20CeCNN%20sharply%20enhances%20the%20predictive%20capability%0Aof%20various%20backbone%20CNNs.%20Our%20study%20evidences%20the%20ophthalmology%20view%20that%0Abesides%20SE%2C%20AL%20is%20also%20an%20important%20measure%20to%20myopia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.03967v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCeCNN%253A%2520Copula-enhanced%2520convolutional%2520neural%2520networks%2520in%2520joint%2520prediction%250A%2520%2520of%2520refraction%2520error%2520and%2520axial%2520length%2520based%2520on%2520ultra-widefield%2520fundus%2520images%26entry.906535625%3DChong%2520Zhong%2520and%2520Yang%2520Li%2520and%2520Danjuan%2520Yang%2520and%2520Meiyan%2520Li%2520and%2520Xingyao%2520Zhou%2520and%2520Bo%2520Fu%2520and%2520Catherine%2520C.%2520Liu%2520and%2520A.%2520H.%2520Welsh%26entry.1292438233%3D%2520%2520The%2520ultra-widefield%2520%2528UWF%2529%2520fundus%2520image%2520is%2520an%2520attractive%25203D%2520biomarker%2520in%250AAI-aided%2520myopia%2520screening%2520because%2520it%2520provides%2520much%2520richer%2520myopia-related%250Ainformation.%2520Though%2520axial%2520length%2520%2528AL%2529%2520has%2520been%2520acknowledged%2520to%2520be%2520highly%250Arelated%2520to%2520the%2520two%2520key%2520targets%2520of%2520myopia%2520screening%252C%2520Spherical%2520Equivalence%2520%2528SE%2529%250Ameasuring%2520and%2520high%2520myopia%2520diagnosis%252C%2520its%2520prediction%2520based%2520on%2520the%2520UWF%2520fundus%250Aimage%2520is%2520rarely%2520considered.%2520To%2520save%2520the%2520high%2520expense%2520and%2520time%2520costs%2520of%250Ameasuring%2520SE%2520and%2520AL%252C%2520we%2520propose%2520the%2520Copula-enhanced%2520Convolutional%2520Neural%250ANetwork%2520%2528CeCNN%2529%252C%2520a%2520one-stop%2520UWF-based%2520ophthalmic%2520AI%2520framework%2520to%2520jointly%250Apredict%2520SE%252C%2520AL%252C%2520and%2520myopia%2520status.%2520The%2520CeCNN%2520formulates%2520a%2520multiresponse%250Aregression%2520that%2520relates%2520multiple%2520dependent%2520discrete-continuous%2520responses%2520and%250Athe%2520image%2520covariate%252C%2520where%2520the%2520nonlinearity%2520of%2520the%2520association%2520is%2520modeled%2520by%2520a%250Abackbone%2520CNN.%2520To%2520thoroughly%2520describe%2520the%2520dependence%2520structure%2520among%2520the%250Aresponses%252C%2520we%2520model%2520and%2520incorporate%2520the%2520conditional%2520dependence%2520among%2520responses%250Ain%2520a%2520CNN%2520through%2520a%2520new%2520copula-likelihood%2520loss.%2520We%2520provide%2520statistical%250Ainterpretations%2520of%2520the%2520conditional%2520dependence%2520among%2520responses%252C%2520and%2520reveal%2520that%250Asuch%2520dependence%2520is%2520beyond%2520the%2520dependence%2520explained%2520by%2520the%2520image%2520covariate.%2520We%250Aheuristically%2520justify%2520that%2520the%2520proposed%2520loss%2520can%2520enhance%2520the%2520estimation%250Aefficiency%2520of%2520the%2520CNN%2520weights.%2520We%2520apply%2520the%2520CeCNN%2520to%2520the%2520UWF%2520dataset%2520collected%250Aby%2520us%2520and%2520demonstrate%2520that%2520the%2520CeCNN%2520sharply%2520enhances%2520the%2520predictive%2520capability%250Aof%2520various%2520backbone%2520CNNs.%2520Our%2520study%2520evidences%2520the%2520ophthalmology%2520view%2520that%250Abesides%2520SE%252C%2520AL%2520is%2520also%2520an%2520important%2520measure%2520to%2520myopia.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.03967v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CeCNN%3A%20Copula-enhanced%20convolutional%20neural%20networks%20in%20joint%20prediction%0A%20%20of%20refraction%20error%20and%20axial%20length%20based%20on%20ultra-widefield%20fundus%20images&entry.906535625=Chong%20Zhong%20and%20Yang%20Li%20and%20Danjuan%20Yang%20and%20Meiyan%20Li%20and%20Xingyao%20Zhou%20and%20Bo%20Fu%20and%20Catherine%20C.%20Liu%20and%20A.%20H.%20Welsh&entry.1292438233=%20%20The%20ultra-widefield%20%28UWF%29%20fundus%20image%20is%20an%20attractive%203D%20biomarker%20in%0AAI-aided%20myopia%20screening%20because%20it%20provides%20much%20richer%20myopia-related%0Ainformation.%20Though%20axial%20length%20%28AL%29%20has%20been%20acknowledged%20to%20be%20highly%0Arelated%20to%20the%20two%20key%20targets%20of%20myopia%20screening%2C%20Spherical%20Equivalence%20%28SE%29%0Ameasuring%20and%20high%20myopia%20diagnosis%2C%20its%20prediction%20based%20on%20the%20UWF%20fundus%0Aimage%20is%20rarely%20considered.%20To%20save%20the%20high%20expense%20and%20time%20costs%20of%0Ameasuring%20SE%20and%20AL%2C%20we%20propose%20the%20Copula-enhanced%20Convolutional%20Neural%0ANetwork%20%28CeCNN%29%2C%20a%20one-stop%20UWF-based%20ophthalmic%20AI%20framework%20to%20jointly%0Apredict%20SE%2C%20AL%2C%20and%20myopia%20status.%20The%20CeCNN%20formulates%20a%20multiresponse%0Aregression%20that%20relates%20multiple%20dependent%20discrete-continuous%20responses%20and%0Athe%20image%20covariate%2C%20where%20the%20nonlinearity%20of%20the%20association%20is%20modeled%20by%20a%0Abackbone%20CNN.%20To%20thoroughly%20describe%20the%20dependence%20structure%20among%20the%0Aresponses%2C%20we%20model%20and%20incorporate%20the%20conditional%20dependence%20among%20responses%0Ain%20a%20CNN%20through%20a%20new%20copula-likelihood%20loss.%20We%20provide%20statistical%0Ainterpretations%20of%20the%20conditional%20dependence%20among%20responses%2C%20and%20reveal%20that%0Asuch%20dependence%20is%20beyond%20the%20dependence%20explained%20by%20the%20image%20covariate.%20We%0Aheuristically%20justify%20that%20the%20proposed%20loss%20can%20enhance%20the%20estimation%0Aefficiency%20of%20the%20CNN%20weights.%20We%20apply%20the%20CeCNN%20to%20the%20UWF%20dataset%20collected%0Aby%20us%20and%20demonstrate%20that%20the%20CeCNN%20sharply%20enhances%20the%20predictive%20capability%0Aof%20various%20backbone%20CNNs.%20Our%20study%20evidences%20the%20ophthalmology%20view%20that%0Abesides%20SE%2C%20AL%20is%20also%20an%20important%20measure%20to%20myopia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.03967v3&entry.124074799=Read"},
{"title": "Language Models Meet Anomaly Detection for Better Interpretability and\n  Generalizability", "author": "Jun Li and Su Hwan Kim and Philip M\u00fcller and Lina Felsner and Daniel Rueckert and Benedikt Wiestler and Julia A. Schnabel and Cosmin I. Bercea", "abstract": "  This research explores the integration of language models and unsupervised\nanomaly detection in medical imaging, addressing two key questions: (1) Can\nlanguage models enhance the interpretability of anomaly detection maps? and (2)\nCan anomaly maps improve the generalizability of language models in open-set\nanomaly detection tasks? To investigate these questions, we introduce a new\ndataset for multi-image visual question-answering on brain magnetic resonance\nimages encompassing multiple conditions. We propose KQ-Former (Knowledge\nQuerying Transformer), which is designed to optimally align visual and textual\ninformation in limited-sample contexts. Our model achieves a 60.81% accuracy on\nclosed questions, covering disease classification and severity across 15\ndifferent classes. For open questions, KQ-Former demonstrates a 70% improvement\nover the baseline with a BLEU-4 score of 0.41, and achieves the highest\nentailment ratios (up to 71.9%) and lowest contradiction ratios (down to 10.0%)\namong various natural language inference models. Furthermore, integrating\nanomaly maps results in an 18% accuracy increase in detecting open-set\nanomalies, thereby enhancing the language model's generalizability to\npreviously unseen medical conditions. The code and dataset are available at\nhttps://github.com/compai-lab/miccai-2024-junli?tab=readme-ov-file\n", "link": "http://arxiv.org/abs/2404.07622v2", "date": "2024-07-23", "relevancy": 2.125, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.566}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.533}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Models%20Meet%20Anomaly%20Detection%20for%20Better%20Interpretability%20and%0A%20%20Generalizability&body=Title%3A%20Language%20Models%20Meet%20Anomaly%20Detection%20for%20Better%20Interpretability%20and%0A%20%20Generalizability%0AAuthor%3A%20Jun%20Li%20and%20Su%20Hwan%20Kim%20and%20Philip%20M%C3%BCller%20and%20Lina%20Felsner%20and%20Daniel%20Rueckert%20and%20Benedikt%20Wiestler%20and%20Julia%20A.%20Schnabel%20and%20Cosmin%20I.%20Bercea%0AAbstract%3A%20%20%20This%20research%20explores%20the%20integration%20of%20language%20models%20and%20unsupervised%0Aanomaly%20detection%20in%20medical%20imaging%2C%20addressing%20two%20key%20questions%3A%20%281%29%20Can%0Alanguage%20models%20enhance%20the%20interpretability%20of%20anomaly%20detection%20maps%3F%20and%20%282%29%0ACan%20anomaly%20maps%20improve%20the%20generalizability%20of%20language%20models%20in%20open-set%0Aanomaly%20detection%20tasks%3F%20To%20investigate%20these%20questions%2C%20we%20introduce%20a%20new%0Adataset%20for%20multi-image%20visual%20question-answering%20on%20brain%20magnetic%20resonance%0Aimages%20encompassing%20multiple%20conditions.%20We%20propose%20KQ-Former%20%28Knowledge%0AQuerying%20Transformer%29%2C%20which%20is%20designed%20to%20optimally%20align%20visual%20and%20textual%0Ainformation%20in%20limited-sample%20contexts.%20Our%20model%20achieves%20a%2060.81%25%20accuracy%20on%0Aclosed%20questions%2C%20covering%20disease%20classification%20and%20severity%20across%2015%0Adifferent%20classes.%20For%20open%20questions%2C%20KQ-Former%20demonstrates%20a%2070%25%20improvement%0Aover%20the%20baseline%20with%20a%20BLEU-4%20score%20of%200.41%2C%20and%20achieves%20the%20highest%0Aentailment%20ratios%20%28up%20to%2071.9%25%29%20and%20lowest%20contradiction%20ratios%20%28down%20to%2010.0%25%29%0Aamong%20various%20natural%20language%20inference%20models.%20Furthermore%2C%20integrating%0Aanomaly%20maps%20results%20in%20an%2018%25%20accuracy%20increase%20in%20detecting%20open-set%0Aanomalies%2C%20thereby%20enhancing%20the%20language%20model%27s%20generalizability%20to%0Apreviously%20unseen%20medical%20conditions.%20The%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/compai-lab/miccai-2024-junli%3Ftab%3Dreadme-ov-file%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Models%2520Meet%2520Anomaly%2520Detection%2520for%2520Better%2520Interpretability%2520and%250A%2520%2520Generalizability%26entry.906535625%3DJun%2520Li%2520and%2520Su%2520Hwan%2520Kim%2520and%2520Philip%2520M%25C3%25BCller%2520and%2520Lina%2520Felsner%2520and%2520Daniel%2520Rueckert%2520and%2520Benedikt%2520Wiestler%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Cosmin%2520I.%2520Bercea%26entry.1292438233%3D%2520%2520This%2520research%2520explores%2520the%2520integration%2520of%2520language%2520models%2520and%2520unsupervised%250Aanomaly%2520detection%2520in%2520medical%2520imaging%252C%2520addressing%2520two%2520key%2520questions%253A%2520%25281%2529%2520Can%250Alanguage%2520models%2520enhance%2520the%2520interpretability%2520of%2520anomaly%2520detection%2520maps%253F%2520and%2520%25282%2529%250ACan%2520anomaly%2520maps%2520improve%2520the%2520generalizability%2520of%2520language%2520models%2520in%2520open-set%250Aanomaly%2520detection%2520tasks%253F%2520To%2520investigate%2520these%2520questions%252C%2520we%2520introduce%2520a%2520new%250Adataset%2520for%2520multi-image%2520visual%2520question-answering%2520on%2520brain%2520magnetic%2520resonance%250Aimages%2520encompassing%2520multiple%2520conditions.%2520We%2520propose%2520KQ-Former%2520%2528Knowledge%250AQuerying%2520Transformer%2529%252C%2520which%2520is%2520designed%2520to%2520optimally%2520align%2520visual%2520and%2520textual%250Ainformation%2520in%2520limited-sample%2520contexts.%2520Our%2520model%2520achieves%2520a%252060.81%2525%2520accuracy%2520on%250Aclosed%2520questions%252C%2520covering%2520disease%2520classification%2520and%2520severity%2520across%252015%250Adifferent%2520classes.%2520For%2520open%2520questions%252C%2520KQ-Former%2520demonstrates%2520a%252070%2525%2520improvement%250Aover%2520the%2520baseline%2520with%2520a%2520BLEU-4%2520score%2520of%25200.41%252C%2520and%2520achieves%2520the%2520highest%250Aentailment%2520ratios%2520%2528up%2520to%252071.9%2525%2529%2520and%2520lowest%2520contradiction%2520ratios%2520%2528down%2520to%252010.0%2525%2529%250Aamong%2520various%2520natural%2520language%2520inference%2520models.%2520Furthermore%252C%2520integrating%250Aanomaly%2520maps%2520results%2520in%2520an%252018%2525%2520accuracy%2520increase%2520in%2520detecting%2520open-set%250Aanomalies%252C%2520thereby%2520enhancing%2520the%2520language%2520model%2527s%2520generalizability%2520to%250Apreviously%2520unseen%2520medical%2520conditions.%2520The%2520code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/compai-lab/miccai-2024-junli%253Ftab%253Dreadme-ov-file%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20Meet%20Anomaly%20Detection%20for%20Better%20Interpretability%20and%0A%20%20Generalizability&entry.906535625=Jun%20Li%20and%20Su%20Hwan%20Kim%20and%20Philip%20M%C3%BCller%20and%20Lina%20Felsner%20and%20Daniel%20Rueckert%20and%20Benedikt%20Wiestler%20and%20Julia%20A.%20Schnabel%20and%20Cosmin%20I.%20Bercea&entry.1292438233=%20%20This%20research%20explores%20the%20integration%20of%20language%20models%20and%20unsupervised%0Aanomaly%20detection%20in%20medical%20imaging%2C%20addressing%20two%20key%20questions%3A%20%281%29%20Can%0Alanguage%20models%20enhance%20the%20interpretability%20of%20anomaly%20detection%20maps%3F%20and%20%282%29%0ACan%20anomaly%20maps%20improve%20the%20generalizability%20of%20language%20models%20in%20open-set%0Aanomaly%20detection%20tasks%3F%20To%20investigate%20these%20questions%2C%20we%20introduce%20a%20new%0Adataset%20for%20multi-image%20visual%20question-answering%20on%20brain%20magnetic%20resonance%0Aimages%20encompassing%20multiple%20conditions.%20We%20propose%20KQ-Former%20%28Knowledge%0AQuerying%20Transformer%29%2C%20which%20is%20designed%20to%20optimally%20align%20visual%20and%20textual%0Ainformation%20in%20limited-sample%20contexts.%20Our%20model%20achieves%20a%2060.81%25%20accuracy%20on%0Aclosed%20questions%2C%20covering%20disease%20classification%20and%20severity%20across%2015%0Adifferent%20classes.%20For%20open%20questions%2C%20KQ-Former%20demonstrates%20a%2070%25%20improvement%0Aover%20the%20baseline%20with%20a%20BLEU-4%20score%20of%200.41%2C%20and%20achieves%20the%20highest%0Aentailment%20ratios%20%28up%20to%2071.9%25%29%20and%20lowest%20contradiction%20ratios%20%28down%20to%2010.0%25%29%0Aamong%20various%20natural%20language%20inference%20models.%20Furthermore%2C%20integrating%0Aanomaly%20maps%20results%20in%20an%2018%25%20accuracy%20increase%20in%20detecting%20open-set%0Aanomalies%2C%20thereby%20enhancing%20the%20language%20model%27s%20generalizability%20to%0Apreviously%20unseen%20medical%20conditions.%20The%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/compai-lab/miccai-2024-junli%3Ftab%3Dreadme-ov-file%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07622v2&entry.124074799=Read"},
{"title": "Deep-Graph-Sprints: Accelerated Representation Learning in\n  Continuous-Time Dynamic Graphs", "author": "Ahmad Naser Eddin and Jacopo Bono and David Apar\u00edcio and Hugo Ferreira and Pedro Ribeiro and Pedro Bizarro", "abstract": "  Continuous-time dynamic graphs (CTDGs) are essential for modeling\ninterconnected, evolving systems. Traditional methods for extracting knowledge\nfrom these graphs often depend on feature engineering or deep learning. Feature\nengineering is limited by the manual and time-intensive nature of crafting\nfeatures, while deep learning approaches suffer from high inference latency,\nmaking them impractical for real-time applications. This paper introduces\nDeep-Graph-Sprints (DGS), a novel deep learning architecture designed for\nefficient representation learning on CTDGs with low-latency inference\nrequirements. We benchmark DGS against state-of-the-art feature engineering and\ngraph neural network methods using five diverse datasets. The results indicate\nthat DGS achieves competitive performance while improving inference speed up to\n12x compared to other deep learning approaches on our tested benchmarks. Our\nmethod effectively bridges the gap between deep representation learning and\nlow-latency application requirements for CTDGs.\n", "link": "http://arxiv.org/abs/2407.07712v2", "date": "2024-07-23", "relevancy": 2.1206, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.542}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5283}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep-Graph-Sprints%3A%20Accelerated%20Representation%20Learning%20in%0A%20%20Continuous-Time%20Dynamic%20Graphs&body=Title%3A%20Deep-Graph-Sprints%3A%20Accelerated%20Representation%20Learning%20in%0A%20%20Continuous-Time%20Dynamic%20Graphs%0AAuthor%3A%20Ahmad%20Naser%20Eddin%20and%20Jacopo%20Bono%20and%20David%20Apar%C3%ADcio%20and%20Hugo%20Ferreira%20and%20Pedro%20Ribeiro%20and%20Pedro%20Bizarro%0AAbstract%3A%20%20%20Continuous-time%20dynamic%20graphs%20%28CTDGs%29%20are%20essential%20for%20modeling%0Ainterconnected%2C%20evolving%20systems.%20Traditional%20methods%20for%20extracting%20knowledge%0Afrom%20these%20graphs%20often%20depend%20on%20feature%20engineering%20or%20deep%20learning.%20Feature%0Aengineering%20is%20limited%20by%20the%20manual%20and%20time-intensive%20nature%20of%20crafting%0Afeatures%2C%20while%20deep%20learning%20approaches%20suffer%20from%20high%20inference%20latency%2C%0Amaking%20them%20impractical%20for%20real-time%20applications.%20This%20paper%20introduces%0ADeep-Graph-Sprints%20%28DGS%29%2C%20a%20novel%20deep%20learning%20architecture%20designed%20for%0Aefficient%20representation%20learning%20on%20CTDGs%20with%20low-latency%20inference%0Arequirements.%20We%20benchmark%20DGS%20against%20state-of-the-art%20feature%20engineering%20and%0Agraph%20neural%20network%20methods%20using%20five%20diverse%20datasets.%20The%20results%20indicate%0Athat%20DGS%20achieves%20competitive%20performance%20while%20improving%20inference%20speed%20up%20to%0A12x%20compared%20to%20other%20deep%20learning%20approaches%20on%20our%20tested%20benchmarks.%20Our%0Amethod%20effectively%20bridges%20the%20gap%20between%20deep%20representation%20learning%20and%0Alow-latency%20application%20requirements%20for%20CTDGs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07712v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep-Graph-Sprints%253A%2520Accelerated%2520Representation%2520Learning%2520in%250A%2520%2520Continuous-Time%2520Dynamic%2520Graphs%26entry.906535625%3DAhmad%2520Naser%2520Eddin%2520and%2520Jacopo%2520Bono%2520and%2520David%2520Apar%25C3%25ADcio%2520and%2520Hugo%2520Ferreira%2520and%2520Pedro%2520Ribeiro%2520and%2520Pedro%2520Bizarro%26entry.1292438233%3D%2520%2520Continuous-time%2520dynamic%2520graphs%2520%2528CTDGs%2529%2520are%2520essential%2520for%2520modeling%250Ainterconnected%252C%2520evolving%2520systems.%2520Traditional%2520methods%2520for%2520extracting%2520knowledge%250Afrom%2520these%2520graphs%2520often%2520depend%2520on%2520feature%2520engineering%2520or%2520deep%2520learning.%2520Feature%250Aengineering%2520is%2520limited%2520by%2520the%2520manual%2520and%2520time-intensive%2520nature%2520of%2520crafting%250Afeatures%252C%2520while%2520deep%2520learning%2520approaches%2520suffer%2520from%2520high%2520inference%2520latency%252C%250Amaking%2520them%2520impractical%2520for%2520real-time%2520applications.%2520This%2520paper%2520introduces%250ADeep-Graph-Sprints%2520%2528DGS%2529%252C%2520a%2520novel%2520deep%2520learning%2520architecture%2520designed%2520for%250Aefficient%2520representation%2520learning%2520on%2520CTDGs%2520with%2520low-latency%2520inference%250Arequirements.%2520We%2520benchmark%2520DGS%2520against%2520state-of-the-art%2520feature%2520engineering%2520and%250Agraph%2520neural%2520network%2520methods%2520using%2520five%2520diverse%2520datasets.%2520The%2520results%2520indicate%250Athat%2520DGS%2520achieves%2520competitive%2520performance%2520while%2520improving%2520inference%2520speed%2520up%2520to%250A12x%2520compared%2520to%2520other%2520deep%2520learning%2520approaches%2520on%2520our%2520tested%2520benchmarks.%2520Our%250Amethod%2520effectively%2520bridges%2520the%2520gap%2520between%2520deep%2520representation%2520learning%2520and%250Alow-latency%2520application%2520requirements%2520for%2520CTDGs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07712v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep-Graph-Sprints%3A%20Accelerated%20Representation%20Learning%20in%0A%20%20Continuous-Time%20Dynamic%20Graphs&entry.906535625=Ahmad%20Naser%20Eddin%20and%20Jacopo%20Bono%20and%20David%20Apar%C3%ADcio%20and%20Hugo%20Ferreira%20and%20Pedro%20Ribeiro%20and%20Pedro%20Bizarro&entry.1292438233=%20%20Continuous-time%20dynamic%20graphs%20%28CTDGs%29%20are%20essential%20for%20modeling%0Ainterconnected%2C%20evolving%20systems.%20Traditional%20methods%20for%20extracting%20knowledge%0Afrom%20these%20graphs%20often%20depend%20on%20feature%20engineering%20or%20deep%20learning.%20Feature%0Aengineering%20is%20limited%20by%20the%20manual%20and%20time-intensive%20nature%20of%20crafting%0Afeatures%2C%20while%20deep%20learning%20approaches%20suffer%20from%20high%20inference%20latency%2C%0Amaking%20them%20impractical%20for%20real-time%20applications.%20This%20paper%20introduces%0ADeep-Graph-Sprints%20%28DGS%29%2C%20a%20novel%20deep%20learning%20architecture%20designed%20for%0Aefficient%20representation%20learning%20on%20CTDGs%20with%20low-latency%20inference%0Arequirements.%20We%20benchmark%20DGS%20against%20state-of-the-art%20feature%20engineering%20and%0Agraph%20neural%20network%20methods%20using%20five%20diverse%20datasets.%20The%20results%20indicate%0Athat%20DGS%20achieves%20competitive%20performance%20while%20improving%20inference%20speed%20up%20to%0A12x%20compared%20to%20other%20deep%20learning%20approaches%20on%20our%20tested%20benchmarks.%20Our%0Amethod%20effectively%20bridges%20the%20gap%20between%20deep%20representation%20learning%20and%0Alow-latency%20application%20requirements%20for%20CTDGs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07712v2&entry.124074799=Read"},
{"title": "Aggregated Attributions for Explanatory Analysis of 3D Segmentation\n  Models", "author": "Maciej Chrabaszcz and Hubert Baniecki and Piotr Komorowski and Szymon P\u0142otka and Przemyslaw Biecek", "abstract": "  Analysis of 3D segmentation models, especially in the context of medical\nimaging, is often limited to segmentation performance metrics that overlook the\ncrucial aspect of explainability and bias. Currently, effectively explaining\nthese models with saliency maps is challenging due to the high dimensions of\ninput images multiplied by the ever-growing number of segmented class labels.\nTo this end, we introduce Agg^2Exp, a methodology for aggregating fine-grained\nvoxel attributions of the segmentation model's predictions. Unlike classical\nexplanation methods that primarily focus on the local feature attribution,\nAgg^2Exp enables a more comprehensive global view on the importance of\npredicted segments in 3D images. Our benchmarking experiments show that\ngradient-based voxel attributions are more faithful to the model's predictions\nthan perturbation-based explanations. As a concrete use-case, we apply Agg^2Exp\nto discover knowledge acquired by the Swin UNEt TRansformer model trained on\nthe TotalSegmentator v2 dataset for segmenting anatomical structures in\ncomputed tomography medical images. Agg^2Exp facilitates the explanatory\nanalysis of large segmentation models beyond their predictive performance.\n", "link": "http://arxiv.org/abs/2407.16653v1", "date": "2024-07-23", "relevancy": 2.1147, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5586}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5313}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aggregated%20Attributions%20for%20Explanatory%20Analysis%20of%203D%20Segmentation%0A%20%20Models&body=Title%3A%20Aggregated%20Attributions%20for%20Explanatory%20Analysis%20of%203D%20Segmentation%0A%20%20Models%0AAuthor%3A%20Maciej%20Chrabaszcz%20and%20Hubert%20Baniecki%20and%20Piotr%20Komorowski%20and%20Szymon%20P%C5%82otka%20and%20Przemyslaw%20Biecek%0AAbstract%3A%20%20%20Analysis%20of%203D%20segmentation%20models%2C%20especially%20in%20the%20context%20of%20medical%0Aimaging%2C%20is%20often%20limited%20to%20segmentation%20performance%20metrics%20that%20overlook%20the%0Acrucial%20aspect%20of%20explainability%20and%20bias.%20Currently%2C%20effectively%20explaining%0Athese%20models%20with%20saliency%20maps%20is%20challenging%20due%20to%20the%20high%20dimensions%20of%0Ainput%20images%20multiplied%20by%20the%20ever-growing%20number%20of%20segmented%20class%20labels.%0ATo%20this%20end%2C%20we%20introduce%20Agg%5E2Exp%2C%20a%20methodology%20for%20aggregating%20fine-grained%0Avoxel%20attributions%20of%20the%20segmentation%20model%27s%20predictions.%20Unlike%20classical%0Aexplanation%20methods%20that%20primarily%20focus%20on%20the%20local%20feature%20attribution%2C%0AAgg%5E2Exp%20enables%20a%20more%20comprehensive%20global%20view%20on%20the%20importance%20of%0Apredicted%20segments%20in%203D%20images.%20Our%20benchmarking%20experiments%20show%20that%0Agradient-based%20voxel%20attributions%20are%20more%20faithful%20to%20the%20model%27s%20predictions%0Athan%20perturbation-based%20explanations.%20As%20a%20concrete%20use-case%2C%20we%20apply%20Agg%5E2Exp%0Ato%20discover%20knowledge%20acquired%20by%20the%20Swin%20UNEt%20TRansformer%20model%20trained%20on%0Athe%20TotalSegmentator%20v2%20dataset%20for%20segmenting%20anatomical%20structures%20in%0Acomputed%20tomography%20medical%20images.%20Agg%5E2Exp%20facilitates%20the%20explanatory%0Aanalysis%20of%20large%20segmentation%20models%20beyond%20their%20predictive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAggregated%2520Attributions%2520for%2520Explanatory%2520Analysis%2520of%25203D%2520Segmentation%250A%2520%2520Models%26entry.906535625%3DMaciej%2520Chrabaszcz%2520and%2520Hubert%2520Baniecki%2520and%2520Piotr%2520Komorowski%2520and%2520Szymon%2520P%25C5%2582otka%2520and%2520Przemyslaw%2520Biecek%26entry.1292438233%3D%2520%2520Analysis%2520of%25203D%2520segmentation%2520models%252C%2520especially%2520in%2520the%2520context%2520of%2520medical%250Aimaging%252C%2520is%2520often%2520limited%2520to%2520segmentation%2520performance%2520metrics%2520that%2520overlook%2520the%250Acrucial%2520aspect%2520of%2520explainability%2520and%2520bias.%2520Currently%252C%2520effectively%2520explaining%250Athese%2520models%2520with%2520saliency%2520maps%2520is%2520challenging%2520due%2520to%2520the%2520high%2520dimensions%2520of%250Ainput%2520images%2520multiplied%2520by%2520the%2520ever-growing%2520number%2520of%2520segmented%2520class%2520labels.%250ATo%2520this%2520end%252C%2520we%2520introduce%2520Agg%255E2Exp%252C%2520a%2520methodology%2520for%2520aggregating%2520fine-grained%250Avoxel%2520attributions%2520of%2520the%2520segmentation%2520model%2527s%2520predictions.%2520Unlike%2520classical%250Aexplanation%2520methods%2520that%2520primarily%2520focus%2520on%2520the%2520local%2520feature%2520attribution%252C%250AAgg%255E2Exp%2520enables%2520a%2520more%2520comprehensive%2520global%2520view%2520on%2520the%2520importance%2520of%250Apredicted%2520segments%2520in%25203D%2520images.%2520Our%2520benchmarking%2520experiments%2520show%2520that%250Agradient-based%2520voxel%2520attributions%2520are%2520more%2520faithful%2520to%2520the%2520model%2527s%2520predictions%250Athan%2520perturbation-based%2520explanations.%2520As%2520a%2520concrete%2520use-case%252C%2520we%2520apply%2520Agg%255E2Exp%250Ato%2520discover%2520knowledge%2520acquired%2520by%2520the%2520Swin%2520UNEt%2520TRansformer%2520model%2520trained%2520on%250Athe%2520TotalSegmentator%2520v2%2520dataset%2520for%2520segmenting%2520anatomical%2520structures%2520in%250Acomputed%2520tomography%2520medical%2520images.%2520Agg%255E2Exp%2520facilitates%2520the%2520explanatory%250Aanalysis%2520of%2520large%2520segmentation%2520models%2520beyond%2520their%2520predictive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aggregated%20Attributions%20for%20Explanatory%20Analysis%20of%203D%20Segmentation%0A%20%20Models&entry.906535625=Maciej%20Chrabaszcz%20and%20Hubert%20Baniecki%20and%20Piotr%20Komorowski%20and%20Szymon%20P%C5%82otka%20and%20Przemyslaw%20Biecek&entry.1292438233=%20%20Analysis%20of%203D%20segmentation%20models%2C%20especially%20in%20the%20context%20of%20medical%0Aimaging%2C%20is%20often%20limited%20to%20segmentation%20performance%20metrics%20that%20overlook%20the%0Acrucial%20aspect%20of%20explainability%20and%20bias.%20Currently%2C%20effectively%20explaining%0Athese%20models%20with%20saliency%20maps%20is%20challenging%20due%20to%20the%20high%20dimensions%20of%0Ainput%20images%20multiplied%20by%20the%20ever-growing%20number%20of%20segmented%20class%20labels.%0ATo%20this%20end%2C%20we%20introduce%20Agg%5E2Exp%2C%20a%20methodology%20for%20aggregating%20fine-grained%0Avoxel%20attributions%20of%20the%20segmentation%20model%27s%20predictions.%20Unlike%20classical%0Aexplanation%20methods%20that%20primarily%20focus%20on%20the%20local%20feature%20attribution%2C%0AAgg%5E2Exp%20enables%20a%20more%20comprehensive%20global%20view%20on%20the%20importance%20of%0Apredicted%20segments%20in%203D%20images.%20Our%20benchmarking%20experiments%20show%20that%0Agradient-based%20voxel%20attributions%20are%20more%20faithful%20to%20the%20model%27s%20predictions%0Athan%20perturbation-based%20explanations.%20As%20a%20concrete%20use-case%2C%20we%20apply%20Agg%5E2Exp%0Ato%20discover%20knowledge%20acquired%20by%20the%20Swin%20UNEt%20TRansformer%20model%20trained%20on%0Athe%20TotalSegmentator%20v2%20dataset%20for%20segmenting%20anatomical%20structures%20in%0Acomputed%20tomography%20medical%20images.%20Agg%5E2Exp%20facilitates%20the%20explanatory%0Aanalysis%20of%20large%20segmentation%20models%20beyond%20their%20predictive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16653v1&entry.124074799=Read"},
{"title": "Probing Perfection: The Relentless Art of Meddling for Pulmonary Airway\n  Segmentation from HRCT via a Human-AI Collaboration Based Active Learning\n  Method", "author": "Shiyi Wang and Yang Nan and Sheng Zhang and Federico Felder and Xiaodan Xing and Yingying Fang and Javier Del Ser and Simon L F Walsh and Guang Yang", "abstract": "  In pulmonary tracheal segmentation, the scarcity of annotated data is a\nprevalent issue in medical segmentation. Additionally, Deep Learning (DL)\nmethods face challenges: the opacity of 'black box' models and the need for\nperformance enhancement. Our Human-Computer Interaction (HCI) based models\n(RS_UNet, LC_UNet, UUNet, and WD_UNet) address these challenges by combining\ndiverse query strategies with various DL models. We train four HCI models and\nrepeat these steps: (1) Query Strategy: The HCI models select samples that\nprovide the most additional representative information when labeled in each\niteration and identify unlabeled samples with the greatest predictive disparity\nusing Wasserstein Distance, Least Confidence, Entropy Sampling, and Random\nSampling. (2) Central line correction: Selected samples are used for expert\ncorrection of system-generated tracheal central lines in each training round.\n(3) Update training dataset: Experts update the training dataset after each DL\nmodel's training epoch, enhancing the trustworthiness and performance of the\nmodels. (4) Model training: The HCI model is trained using the updated dataset\nand an enhanced UNet version. Experimental results confirm the effectiveness of\nthese HCI-based approaches, showing that WD-UNet, LC-UNet, UUNet, and RS-UNet\nachieve comparable or superior performance to state-of-the-art DL models.\nNotably, WD-UNet achieves this with only 15%-35% of the training data, reducing\nphysician annotation time by 65%-85%.\n", "link": "http://arxiv.org/abs/2407.03542v2", "date": "2024-07-23", "relevancy": 2.1135, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5843}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.521}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20Perfection%3A%20The%20Relentless%20Art%20of%20Meddling%20for%20Pulmonary%20Airway%0A%20%20Segmentation%20from%20HRCT%20via%20a%20Human-AI%20Collaboration%20Based%20Active%20Learning%0A%20%20Method&body=Title%3A%20Probing%20Perfection%3A%20The%20Relentless%20Art%20of%20Meddling%20for%20Pulmonary%20Airway%0A%20%20Segmentation%20from%20HRCT%20via%20a%20Human-AI%20Collaboration%20Based%20Active%20Learning%0A%20%20Method%0AAuthor%3A%20Shiyi%20Wang%20and%20Yang%20Nan%20and%20Sheng%20Zhang%20and%20Federico%20Felder%20and%20Xiaodan%20Xing%20and%20Yingying%20Fang%20and%20Javier%20Del%20Ser%20and%20Simon%20L%20F%20Walsh%20and%20Guang%20Yang%0AAbstract%3A%20%20%20In%20pulmonary%20tracheal%20segmentation%2C%20the%20scarcity%20of%20annotated%20data%20is%20a%0Aprevalent%20issue%20in%20medical%20segmentation.%20Additionally%2C%20Deep%20Learning%20%28DL%29%0Amethods%20face%20challenges%3A%20the%20opacity%20of%20%27black%20box%27%20models%20and%20the%20need%20for%0Aperformance%20enhancement.%20Our%20Human-Computer%20Interaction%20%28HCI%29%20based%20models%0A%28RS_UNet%2C%20LC_UNet%2C%20UUNet%2C%20and%20WD_UNet%29%20address%20these%20challenges%20by%20combining%0Adiverse%20query%20strategies%20with%20various%20DL%20models.%20We%20train%20four%20HCI%20models%20and%0Arepeat%20these%20steps%3A%20%281%29%20Query%20Strategy%3A%20The%20HCI%20models%20select%20samples%20that%0Aprovide%20the%20most%20additional%20representative%20information%20when%20labeled%20in%20each%0Aiteration%20and%20identify%20unlabeled%20samples%20with%20the%20greatest%20predictive%20disparity%0Ausing%20Wasserstein%20Distance%2C%20Least%20Confidence%2C%20Entropy%20Sampling%2C%20and%20Random%0ASampling.%20%282%29%20Central%20line%20correction%3A%20Selected%20samples%20are%20used%20for%20expert%0Acorrection%20of%20system-generated%20tracheal%20central%20lines%20in%20each%20training%20round.%0A%283%29%20Update%20training%20dataset%3A%20Experts%20update%20the%20training%20dataset%20after%20each%20DL%0Amodel%27s%20training%20epoch%2C%20enhancing%20the%20trustworthiness%20and%20performance%20of%20the%0Amodels.%20%284%29%20Model%20training%3A%20The%20HCI%20model%20is%20trained%20using%20the%20updated%20dataset%0Aand%20an%20enhanced%20UNet%20version.%20Experimental%20results%20confirm%20the%20effectiveness%20of%0Athese%20HCI-based%20approaches%2C%20showing%20that%20WD-UNet%2C%20LC-UNet%2C%20UUNet%2C%20and%20RS-UNet%0Aachieve%20comparable%20or%20superior%20performance%20to%20state-of-the-art%20DL%20models.%0ANotably%2C%20WD-UNet%20achieves%20this%20with%20only%2015%25-35%25%20of%20the%20training%20data%2C%20reducing%0Aphysician%20annotation%20time%20by%2065%25-85%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03542v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520Perfection%253A%2520The%2520Relentless%2520Art%2520of%2520Meddling%2520for%2520Pulmonary%2520Airway%250A%2520%2520Segmentation%2520from%2520HRCT%2520via%2520a%2520Human-AI%2520Collaboration%2520Based%2520Active%2520Learning%250A%2520%2520Method%26entry.906535625%3DShiyi%2520Wang%2520and%2520Yang%2520Nan%2520and%2520Sheng%2520Zhang%2520and%2520Federico%2520Felder%2520and%2520Xiaodan%2520Xing%2520and%2520Yingying%2520Fang%2520and%2520Javier%2520Del%2520Ser%2520and%2520Simon%2520L%2520F%2520Walsh%2520and%2520Guang%2520Yang%26entry.1292438233%3D%2520%2520In%2520pulmonary%2520tracheal%2520segmentation%252C%2520the%2520scarcity%2520of%2520annotated%2520data%2520is%2520a%250Aprevalent%2520issue%2520in%2520medical%2520segmentation.%2520Additionally%252C%2520Deep%2520Learning%2520%2528DL%2529%250Amethods%2520face%2520challenges%253A%2520the%2520opacity%2520of%2520%2527black%2520box%2527%2520models%2520and%2520the%2520need%2520for%250Aperformance%2520enhancement.%2520Our%2520Human-Computer%2520Interaction%2520%2528HCI%2529%2520based%2520models%250A%2528RS_UNet%252C%2520LC_UNet%252C%2520UUNet%252C%2520and%2520WD_UNet%2529%2520address%2520these%2520challenges%2520by%2520combining%250Adiverse%2520query%2520strategies%2520with%2520various%2520DL%2520models.%2520We%2520train%2520four%2520HCI%2520models%2520and%250Arepeat%2520these%2520steps%253A%2520%25281%2529%2520Query%2520Strategy%253A%2520The%2520HCI%2520models%2520select%2520samples%2520that%250Aprovide%2520the%2520most%2520additional%2520representative%2520information%2520when%2520labeled%2520in%2520each%250Aiteration%2520and%2520identify%2520unlabeled%2520samples%2520with%2520the%2520greatest%2520predictive%2520disparity%250Ausing%2520Wasserstein%2520Distance%252C%2520Least%2520Confidence%252C%2520Entropy%2520Sampling%252C%2520and%2520Random%250ASampling.%2520%25282%2529%2520Central%2520line%2520correction%253A%2520Selected%2520samples%2520are%2520used%2520for%2520expert%250Acorrection%2520of%2520system-generated%2520tracheal%2520central%2520lines%2520in%2520each%2520training%2520round.%250A%25283%2529%2520Update%2520training%2520dataset%253A%2520Experts%2520update%2520the%2520training%2520dataset%2520after%2520each%2520DL%250Amodel%2527s%2520training%2520epoch%252C%2520enhancing%2520the%2520trustworthiness%2520and%2520performance%2520of%2520the%250Amodels.%2520%25284%2529%2520Model%2520training%253A%2520The%2520HCI%2520model%2520is%2520trained%2520using%2520the%2520updated%2520dataset%250Aand%2520an%2520enhanced%2520UNet%2520version.%2520Experimental%2520results%2520confirm%2520the%2520effectiveness%2520of%250Athese%2520HCI-based%2520approaches%252C%2520showing%2520that%2520WD-UNet%252C%2520LC-UNet%252C%2520UUNet%252C%2520and%2520RS-UNet%250Aachieve%2520comparable%2520or%2520superior%2520performance%2520to%2520state-of-the-art%2520DL%2520models.%250ANotably%252C%2520WD-UNet%2520achieves%2520this%2520with%2520only%252015%2525-35%2525%2520of%2520the%2520training%2520data%252C%2520reducing%250Aphysician%2520annotation%2520time%2520by%252065%2525-85%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03542v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20Perfection%3A%20The%20Relentless%20Art%20of%20Meddling%20for%20Pulmonary%20Airway%0A%20%20Segmentation%20from%20HRCT%20via%20a%20Human-AI%20Collaboration%20Based%20Active%20Learning%0A%20%20Method&entry.906535625=Shiyi%20Wang%20and%20Yang%20Nan%20and%20Sheng%20Zhang%20and%20Federico%20Felder%20and%20Xiaodan%20Xing%20and%20Yingying%20Fang%20and%20Javier%20Del%20Ser%20and%20Simon%20L%20F%20Walsh%20and%20Guang%20Yang&entry.1292438233=%20%20In%20pulmonary%20tracheal%20segmentation%2C%20the%20scarcity%20of%20annotated%20data%20is%20a%0Aprevalent%20issue%20in%20medical%20segmentation.%20Additionally%2C%20Deep%20Learning%20%28DL%29%0Amethods%20face%20challenges%3A%20the%20opacity%20of%20%27black%20box%27%20models%20and%20the%20need%20for%0Aperformance%20enhancement.%20Our%20Human-Computer%20Interaction%20%28HCI%29%20based%20models%0A%28RS_UNet%2C%20LC_UNet%2C%20UUNet%2C%20and%20WD_UNet%29%20address%20these%20challenges%20by%20combining%0Adiverse%20query%20strategies%20with%20various%20DL%20models.%20We%20train%20four%20HCI%20models%20and%0Arepeat%20these%20steps%3A%20%281%29%20Query%20Strategy%3A%20The%20HCI%20models%20select%20samples%20that%0Aprovide%20the%20most%20additional%20representative%20information%20when%20labeled%20in%20each%0Aiteration%20and%20identify%20unlabeled%20samples%20with%20the%20greatest%20predictive%20disparity%0Ausing%20Wasserstein%20Distance%2C%20Least%20Confidence%2C%20Entropy%20Sampling%2C%20and%20Random%0ASampling.%20%282%29%20Central%20line%20correction%3A%20Selected%20samples%20are%20used%20for%20expert%0Acorrection%20of%20system-generated%20tracheal%20central%20lines%20in%20each%20training%20round.%0A%283%29%20Update%20training%20dataset%3A%20Experts%20update%20the%20training%20dataset%20after%20each%20DL%0Amodel%27s%20training%20epoch%2C%20enhancing%20the%20trustworthiness%20and%20performance%20of%20the%0Amodels.%20%284%29%20Model%20training%3A%20The%20HCI%20model%20is%20trained%20using%20the%20updated%20dataset%0Aand%20an%20enhanced%20UNet%20version.%20Experimental%20results%20confirm%20the%20effectiveness%20of%0Athese%20HCI-based%20approaches%2C%20showing%20that%20WD-UNet%2C%20LC-UNet%2C%20UUNet%2C%20and%20RS-UNet%0Aachieve%20comparable%20or%20superior%20performance%20to%20state-of-the-art%20DL%20models.%0ANotably%2C%20WD-UNet%20achieves%20this%20with%20only%2015%25-35%25%20of%20the%20training%20data%2C%20reducing%0Aphysician%20annotation%20time%20by%2065%25-85%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03542v2&entry.124074799=Read"},
{"title": "EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video\n  Retrieval", "author": "Thomas Hummel and Shyamgopal Karthik and Mariana-Iuliana Georgescu and Zeynep Akata", "abstract": "  In Composed Video Retrieval, a video and a textual description which modifies\nthe video content are provided as inputs to the model. The aim is to retrieve\nthe relevant video with the modified content from a database of videos. In this\nchallenging task, the first step is to acquire large-scale training datasets\nand collect high-quality benchmarks for evaluation. In this work, we introduce\nEgoCVR, a new evaluation benchmark for fine-grained Composed Video Retrieval\nusing large-scale egocentric video datasets. EgoCVR consists of 2,295 queries\nthat specifically focus on high-quality temporal video understanding. We find\nthat existing Composed Video Retrieval frameworks do not achieve the necessary\nhigh-quality temporal video understanding for this task. To address this\nshortcoming, we adapt a simple training-free method, propose a generic\nre-ranking framework for Composed Video Retrieval, and demonstrate that this\nachieves strong results on EgoCVR. Our code and benchmark are freely available\nat https://github.com/ExplainableML/EgoCVR.\n", "link": "http://arxiv.org/abs/2407.16658v1", "date": "2024-07-23", "relevancy": 2.0935, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5542}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5224}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoCVR%3A%20An%20Egocentric%20Benchmark%20for%20Fine-Grained%20Composed%20Video%0A%20%20Retrieval&body=Title%3A%20EgoCVR%3A%20An%20Egocentric%20Benchmark%20for%20Fine-Grained%20Composed%20Video%0A%20%20Retrieval%0AAuthor%3A%20Thomas%20Hummel%20and%20Shyamgopal%20Karthik%20and%20Mariana-Iuliana%20Georgescu%20and%20Zeynep%20Akata%0AAbstract%3A%20%20%20In%20Composed%20Video%20Retrieval%2C%20a%20video%20and%20a%20textual%20description%20which%20modifies%0Athe%20video%20content%20are%20provided%20as%20inputs%20to%20the%20model.%20The%20aim%20is%20to%20retrieve%0Athe%20relevant%20video%20with%20the%20modified%20content%20from%20a%20database%20of%20videos.%20In%20this%0Achallenging%20task%2C%20the%20first%20step%20is%20to%20acquire%20large-scale%20training%20datasets%0Aand%20collect%20high-quality%20benchmarks%20for%20evaluation.%20In%20this%20work%2C%20we%20introduce%0AEgoCVR%2C%20a%20new%20evaluation%20benchmark%20for%20fine-grained%20Composed%20Video%20Retrieval%0Ausing%20large-scale%20egocentric%20video%20datasets.%20EgoCVR%20consists%20of%202%2C295%20queries%0Athat%20specifically%20focus%20on%20high-quality%20temporal%20video%20understanding.%20We%20find%0Athat%20existing%20Composed%20Video%20Retrieval%20frameworks%20do%20not%20achieve%20the%20necessary%0Ahigh-quality%20temporal%20video%20understanding%20for%20this%20task.%20To%20address%20this%0Ashortcoming%2C%20we%20adapt%20a%20simple%20training-free%20method%2C%20propose%20a%20generic%0Are-ranking%20framework%20for%20Composed%20Video%20Retrieval%2C%20and%20demonstrate%20that%20this%0Aachieves%20strong%20results%20on%20EgoCVR.%20Our%20code%20and%20benchmark%20are%20freely%20available%0Aat%20https%3A//github.com/ExplainableML/EgoCVR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoCVR%253A%2520An%2520Egocentric%2520Benchmark%2520for%2520Fine-Grained%2520Composed%2520Video%250A%2520%2520Retrieval%26entry.906535625%3DThomas%2520Hummel%2520and%2520Shyamgopal%2520Karthik%2520and%2520Mariana-Iuliana%2520Georgescu%2520and%2520Zeynep%2520Akata%26entry.1292438233%3D%2520%2520In%2520Composed%2520Video%2520Retrieval%252C%2520a%2520video%2520and%2520a%2520textual%2520description%2520which%2520modifies%250Athe%2520video%2520content%2520are%2520provided%2520as%2520inputs%2520to%2520the%2520model.%2520The%2520aim%2520is%2520to%2520retrieve%250Athe%2520relevant%2520video%2520with%2520the%2520modified%2520content%2520from%2520a%2520database%2520of%2520videos.%2520In%2520this%250Achallenging%2520task%252C%2520the%2520first%2520step%2520is%2520to%2520acquire%2520large-scale%2520training%2520datasets%250Aand%2520collect%2520high-quality%2520benchmarks%2520for%2520evaluation.%2520In%2520this%2520work%252C%2520we%2520introduce%250AEgoCVR%252C%2520a%2520new%2520evaluation%2520benchmark%2520for%2520fine-grained%2520Composed%2520Video%2520Retrieval%250Ausing%2520large-scale%2520egocentric%2520video%2520datasets.%2520EgoCVR%2520consists%2520of%25202%252C295%2520queries%250Athat%2520specifically%2520focus%2520on%2520high-quality%2520temporal%2520video%2520understanding.%2520We%2520find%250Athat%2520existing%2520Composed%2520Video%2520Retrieval%2520frameworks%2520do%2520not%2520achieve%2520the%2520necessary%250Ahigh-quality%2520temporal%2520video%2520understanding%2520for%2520this%2520task.%2520To%2520address%2520this%250Ashortcoming%252C%2520we%2520adapt%2520a%2520simple%2520training-free%2520method%252C%2520propose%2520a%2520generic%250Are-ranking%2520framework%2520for%2520Composed%2520Video%2520Retrieval%252C%2520and%2520demonstrate%2520that%2520this%250Aachieves%2520strong%2520results%2520on%2520EgoCVR.%2520Our%2520code%2520and%2520benchmark%2520are%2520freely%2520available%250Aat%2520https%253A//github.com/ExplainableML/EgoCVR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoCVR%3A%20An%20Egocentric%20Benchmark%20for%20Fine-Grained%20Composed%20Video%0A%20%20Retrieval&entry.906535625=Thomas%20Hummel%20and%20Shyamgopal%20Karthik%20and%20Mariana-Iuliana%20Georgescu%20and%20Zeynep%20Akata&entry.1292438233=%20%20In%20Composed%20Video%20Retrieval%2C%20a%20video%20and%20a%20textual%20description%20which%20modifies%0Athe%20video%20content%20are%20provided%20as%20inputs%20to%20the%20model.%20The%20aim%20is%20to%20retrieve%0Athe%20relevant%20video%20with%20the%20modified%20content%20from%20a%20database%20of%20videos.%20In%20this%0Achallenging%20task%2C%20the%20first%20step%20is%20to%20acquire%20large-scale%20training%20datasets%0Aand%20collect%20high-quality%20benchmarks%20for%20evaluation.%20In%20this%20work%2C%20we%20introduce%0AEgoCVR%2C%20a%20new%20evaluation%20benchmark%20for%20fine-grained%20Composed%20Video%20Retrieval%0Ausing%20large-scale%20egocentric%20video%20datasets.%20EgoCVR%20consists%20of%202%2C295%20queries%0Athat%20specifically%20focus%20on%20high-quality%20temporal%20video%20understanding.%20We%20find%0Athat%20existing%20Composed%20Video%20Retrieval%20frameworks%20do%20not%20achieve%20the%20necessary%0Ahigh-quality%20temporal%20video%20understanding%20for%20this%20task.%20To%20address%20this%0Ashortcoming%2C%20we%20adapt%20a%20simple%20training-free%20method%2C%20propose%20a%20generic%0Are-ranking%20framework%20for%20Composed%20Video%20Retrieval%2C%20and%20demonstrate%20that%20this%0Aachieves%20strong%20results%20on%20EgoCVR.%20Our%20code%20and%20benchmark%20are%20freely%20available%0Aat%20https%3A//github.com/ExplainableML/EgoCVR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16658v1&entry.124074799=Read"},
{"title": "FakingRecipe: Detecting Fake News on Short Video Platforms from the\n  Perspective of Creative Process", "author": "Yuyan Bu and Qiang Sheng and Juan Cao and Peng Qi and Danding Wang and Jintao Li", "abstract": "  As short-form video-sharing platforms become a significant channel for news\nconsumption, fake news in short videos has emerged as a serious threat in the\nonline information ecosystem, making developing detection methods for this new\nscenario an urgent need. Compared with that in text and image formats, fake\nnews on short video platforms contains rich but heterogeneous information in\nvarious modalities, posing a challenge to effective feature utilization. Unlike\nexisting works mostly focusing on analyzing what is presented, we introduce a\nnovel perspective that considers how it might be created. Through the lens of\nthe creative process behind news video production, our empirical analysis\nuncovers the unique characteristics of fake news videos in material selection\nand editing. Based on the obtained insights, we design FakingRecipe, a creative\nprocess-aware model for detecting fake news short videos. It captures the fake\nnews preferences in material selection from sentimental and semantic aspects\nand considers the traits of material editing from spatial and temporal aspects.\nTo improve evaluation comprehensiveness, we first construct FakeTT, an English\ndataset for this task, and conduct experiments on both FakeTT and the existing\nChinese FakeSV dataset. The results show FakingRecipe's superiority in\ndetecting fake news on short video platforms.\n", "link": "http://arxiv.org/abs/2407.16670v1", "date": "2024-07-23", "relevancy": 2.0896, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.526}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5231}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FakingRecipe%3A%20Detecting%20Fake%20News%20on%20Short%20Video%20Platforms%20from%20the%0A%20%20Perspective%20of%20Creative%20Process&body=Title%3A%20FakingRecipe%3A%20Detecting%20Fake%20News%20on%20Short%20Video%20Platforms%20from%20the%0A%20%20Perspective%20of%20Creative%20Process%0AAuthor%3A%20Yuyan%20Bu%20and%20Qiang%20Sheng%20and%20Juan%20Cao%20and%20Peng%20Qi%20and%20Danding%20Wang%20and%20Jintao%20Li%0AAbstract%3A%20%20%20As%20short-form%20video-sharing%20platforms%20become%20a%20significant%20channel%20for%20news%0Aconsumption%2C%20fake%20news%20in%20short%20videos%20has%20emerged%20as%20a%20serious%20threat%20in%20the%0Aonline%20information%20ecosystem%2C%20making%20developing%20detection%20methods%20for%20this%20new%0Ascenario%20an%20urgent%20need.%20Compared%20with%20that%20in%20text%20and%20image%20formats%2C%20fake%0Anews%20on%20short%20video%20platforms%20contains%20rich%20but%20heterogeneous%20information%20in%0Avarious%20modalities%2C%20posing%20a%20challenge%20to%20effective%20feature%20utilization.%20Unlike%0Aexisting%20works%20mostly%20focusing%20on%20analyzing%20what%20is%20presented%2C%20we%20introduce%20a%0Anovel%20perspective%20that%20considers%20how%20it%20might%20be%20created.%20Through%20the%20lens%20of%0Athe%20creative%20process%20behind%20news%20video%20production%2C%20our%20empirical%20analysis%0Auncovers%20the%20unique%20characteristics%20of%20fake%20news%20videos%20in%20material%20selection%0Aand%20editing.%20Based%20on%20the%20obtained%20insights%2C%20we%20design%20FakingRecipe%2C%20a%20creative%0Aprocess-aware%20model%20for%20detecting%20fake%20news%20short%20videos.%20It%20captures%20the%20fake%0Anews%20preferences%20in%20material%20selection%20from%20sentimental%20and%20semantic%20aspects%0Aand%20considers%20the%20traits%20of%20material%20editing%20from%20spatial%20and%20temporal%20aspects.%0ATo%20improve%20evaluation%20comprehensiveness%2C%20we%20first%20construct%20FakeTT%2C%20an%20English%0Adataset%20for%20this%20task%2C%20and%20conduct%20experiments%20on%20both%20FakeTT%20and%20the%20existing%0AChinese%20FakeSV%20dataset.%20The%20results%20show%20FakingRecipe%27s%20superiority%20in%0Adetecting%20fake%20news%20on%20short%20video%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFakingRecipe%253A%2520Detecting%2520Fake%2520News%2520on%2520Short%2520Video%2520Platforms%2520from%2520the%250A%2520%2520Perspective%2520of%2520Creative%2520Process%26entry.906535625%3DYuyan%2520Bu%2520and%2520Qiang%2520Sheng%2520and%2520Juan%2520Cao%2520and%2520Peng%2520Qi%2520and%2520Danding%2520Wang%2520and%2520Jintao%2520Li%26entry.1292438233%3D%2520%2520As%2520short-form%2520video-sharing%2520platforms%2520become%2520a%2520significant%2520channel%2520for%2520news%250Aconsumption%252C%2520fake%2520news%2520in%2520short%2520videos%2520has%2520emerged%2520as%2520a%2520serious%2520threat%2520in%2520the%250Aonline%2520information%2520ecosystem%252C%2520making%2520developing%2520detection%2520methods%2520for%2520this%2520new%250Ascenario%2520an%2520urgent%2520need.%2520Compared%2520with%2520that%2520in%2520text%2520and%2520image%2520formats%252C%2520fake%250Anews%2520on%2520short%2520video%2520platforms%2520contains%2520rich%2520but%2520heterogeneous%2520information%2520in%250Avarious%2520modalities%252C%2520posing%2520a%2520challenge%2520to%2520effective%2520feature%2520utilization.%2520Unlike%250Aexisting%2520works%2520mostly%2520focusing%2520on%2520analyzing%2520what%2520is%2520presented%252C%2520we%2520introduce%2520a%250Anovel%2520perspective%2520that%2520considers%2520how%2520it%2520might%2520be%2520created.%2520Through%2520the%2520lens%2520of%250Athe%2520creative%2520process%2520behind%2520news%2520video%2520production%252C%2520our%2520empirical%2520analysis%250Auncovers%2520the%2520unique%2520characteristics%2520of%2520fake%2520news%2520videos%2520in%2520material%2520selection%250Aand%2520editing.%2520Based%2520on%2520the%2520obtained%2520insights%252C%2520we%2520design%2520FakingRecipe%252C%2520a%2520creative%250Aprocess-aware%2520model%2520for%2520detecting%2520fake%2520news%2520short%2520videos.%2520It%2520captures%2520the%2520fake%250Anews%2520preferences%2520in%2520material%2520selection%2520from%2520sentimental%2520and%2520semantic%2520aspects%250Aand%2520considers%2520the%2520traits%2520of%2520material%2520editing%2520from%2520spatial%2520and%2520temporal%2520aspects.%250ATo%2520improve%2520evaluation%2520comprehensiveness%252C%2520we%2520first%2520construct%2520FakeTT%252C%2520an%2520English%250Adataset%2520for%2520this%2520task%252C%2520and%2520conduct%2520experiments%2520on%2520both%2520FakeTT%2520and%2520the%2520existing%250AChinese%2520FakeSV%2520dataset.%2520The%2520results%2520show%2520FakingRecipe%2527s%2520superiority%2520in%250Adetecting%2520fake%2520news%2520on%2520short%2520video%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FakingRecipe%3A%20Detecting%20Fake%20News%20on%20Short%20Video%20Platforms%20from%20the%0A%20%20Perspective%20of%20Creative%20Process&entry.906535625=Yuyan%20Bu%20and%20Qiang%20Sheng%20and%20Juan%20Cao%20and%20Peng%20Qi%20and%20Danding%20Wang%20and%20Jintao%20Li&entry.1292438233=%20%20As%20short-form%20video-sharing%20platforms%20become%20a%20significant%20channel%20for%20news%0Aconsumption%2C%20fake%20news%20in%20short%20videos%20has%20emerged%20as%20a%20serious%20threat%20in%20the%0Aonline%20information%20ecosystem%2C%20making%20developing%20detection%20methods%20for%20this%20new%0Ascenario%20an%20urgent%20need.%20Compared%20with%20that%20in%20text%20and%20image%20formats%2C%20fake%0Anews%20on%20short%20video%20platforms%20contains%20rich%20but%20heterogeneous%20information%20in%0Avarious%20modalities%2C%20posing%20a%20challenge%20to%20effective%20feature%20utilization.%20Unlike%0Aexisting%20works%20mostly%20focusing%20on%20analyzing%20what%20is%20presented%2C%20we%20introduce%20a%0Anovel%20perspective%20that%20considers%20how%20it%20might%20be%20created.%20Through%20the%20lens%20of%0Athe%20creative%20process%20behind%20news%20video%20production%2C%20our%20empirical%20analysis%0Auncovers%20the%20unique%20characteristics%20of%20fake%20news%20videos%20in%20material%20selection%0Aand%20editing.%20Based%20on%20the%20obtained%20insights%2C%20we%20design%20FakingRecipe%2C%20a%20creative%0Aprocess-aware%20model%20for%20detecting%20fake%20news%20short%20videos.%20It%20captures%20the%20fake%0Anews%20preferences%20in%20material%20selection%20from%20sentimental%20and%20semantic%20aspects%0Aand%20considers%20the%20traits%20of%20material%20editing%20from%20spatial%20and%20temporal%20aspects.%0ATo%20improve%20evaluation%20comprehensiveness%2C%20we%20first%20construct%20FakeTT%2C%20an%20English%0Adataset%20for%20this%20task%2C%20and%20conduct%20experiments%20on%20both%20FakeTT%20and%20the%20existing%0AChinese%20FakeSV%20dataset.%20The%20results%20show%20FakingRecipe%27s%20superiority%20in%0Adetecting%20fake%20news%20on%20short%20video%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16670v1&entry.124074799=Read"},
{"title": "FCNR: Fast Compressive Neural Representation of Visualization Images", "author": "Yunfei Lu and Pengfei Gu and Chaoli Wang", "abstract": "  We present FCNR, a fast compressive neural representation for tens of\nthousands of visualization images under varying viewpoints and timesteps. The\nexisting NeRVI solution, albeit enjoying a high compression ratio, incurs slow\nspeeds in encoding and decoding. Built on the recent advances in stereo image\ncompression, FCNR assimilates stereo context modules and joint context transfer\nmodules to compress image pairs. Our solution significantly improves encoding\nand decoding speed while maintaining high reconstruction quality and satisfying\ncompression ratio. To demonstrate its effectiveness, we compare FCNR with\nstate-of-the-art neural compression methods, including E-NeRV, HNeRV, NeRVI,\nand ECSIC. The source code can be found at\nhttps://github.com/YunfeiLu0112/FCNR.\n", "link": "http://arxiv.org/abs/2407.16369v1", "date": "2024-07-23", "relevancy": 2.0829, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5277}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.517}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FCNR%3A%20Fast%20Compressive%20Neural%20Representation%20of%20Visualization%20Images&body=Title%3A%20FCNR%3A%20Fast%20Compressive%20Neural%20Representation%20of%20Visualization%20Images%0AAuthor%3A%20Yunfei%20Lu%20and%20Pengfei%20Gu%20and%20Chaoli%20Wang%0AAbstract%3A%20%20%20We%20present%20FCNR%2C%20a%20fast%20compressive%20neural%20representation%20for%20tens%20of%0Athousands%20of%20visualization%20images%20under%20varying%20viewpoints%20and%20timesteps.%20The%0Aexisting%20NeRVI%20solution%2C%20albeit%20enjoying%20a%20high%20compression%20ratio%2C%20incurs%20slow%0Aspeeds%20in%20encoding%20and%20decoding.%20Built%20on%20the%20recent%20advances%20in%20stereo%20image%0Acompression%2C%20FCNR%20assimilates%20stereo%20context%20modules%20and%20joint%20context%20transfer%0Amodules%20to%20compress%20image%20pairs.%20Our%20solution%20significantly%20improves%20encoding%0Aand%20decoding%20speed%20while%20maintaining%20high%20reconstruction%20quality%20and%20satisfying%0Acompression%20ratio.%20To%20demonstrate%20its%20effectiveness%2C%20we%20compare%20FCNR%20with%0Astate-of-the-art%20neural%20compression%20methods%2C%20including%20E-NeRV%2C%20HNeRV%2C%20NeRVI%2C%0Aand%20ECSIC.%20The%20source%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/YunfeiLu0112/FCNR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFCNR%253A%2520Fast%2520Compressive%2520Neural%2520Representation%2520of%2520Visualization%2520Images%26entry.906535625%3DYunfei%2520Lu%2520and%2520Pengfei%2520Gu%2520and%2520Chaoli%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520FCNR%252C%2520a%2520fast%2520compressive%2520neural%2520representation%2520for%2520tens%2520of%250Athousands%2520of%2520visualization%2520images%2520under%2520varying%2520viewpoints%2520and%2520timesteps.%2520The%250Aexisting%2520NeRVI%2520solution%252C%2520albeit%2520enjoying%2520a%2520high%2520compression%2520ratio%252C%2520incurs%2520slow%250Aspeeds%2520in%2520encoding%2520and%2520decoding.%2520Built%2520on%2520the%2520recent%2520advances%2520in%2520stereo%2520image%250Acompression%252C%2520FCNR%2520assimilates%2520stereo%2520context%2520modules%2520and%2520joint%2520context%2520transfer%250Amodules%2520to%2520compress%2520image%2520pairs.%2520Our%2520solution%2520significantly%2520improves%2520encoding%250Aand%2520decoding%2520speed%2520while%2520maintaining%2520high%2520reconstruction%2520quality%2520and%2520satisfying%250Acompression%2520ratio.%2520To%2520demonstrate%2520its%2520effectiveness%252C%2520we%2520compare%2520FCNR%2520with%250Astate-of-the-art%2520neural%2520compression%2520methods%252C%2520including%2520E-NeRV%252C%2520HNeRV%252C%2520NeRVI%252C%250Aand%2520ECSIC.%2520The%2520source%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/YunfeiLu0112/FCNR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FCNR%3A%20Fast%20Compressive%20Neural%20Representation%20of%20Visualization%20Images&entry.906535625=Yunfei%20Lu%20and%20Pengfei%20Gu%20and%20Chaoli%20Wang&entry.1292438233=%20%20We%20present%20FCNR%2C%20a%20fast%20compressive%20neural%20representation%20for%20tens%20of%0Athousands%20of%20visualization%20images%20under%20varying%20viewpoints%20and%20timesteps.%20The%0Aexisting%20NeRVI%20solution%2C%20albeit%20enjoying%20a%20high%20compression%20ratio%2C%20incurs%20slow%0Aspeeds%20in%20encoding%20and%20decoding.%20Built%20on%20the%20recent%20advances%20in%20stereo%20image%0Acompression%2C%20FCNR%20assimilates%20stereo%20context%20modules%20and%20joint%20context%20transfer%0Amodules%20to%20compress%20image%20pairs.%20Our%20solution%20significantly%20improves%20encoding%0Aand%20decoding%20speed%20while%20maintaining%20high%20reconstruction%20quality%20and%20satisfying%0Acompression%20ratio.%20To%20demonstrate%20its%20effectiveness%2C%20we%20compare%20FCNR%20with%0Astate-of-the-art%20neural%20compression%20methods%2C%20including%20E-NeRV%2C%20HNeRV%2C%20NeRVI%2C%0Aand%20ECSIC.%20The%20source%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/YunfeiLu0112/FCNR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16369v1&entry.124074799=Read"},
{"title": "Graph Neural Networks for Learning Equivariant Representations of Neural\n  Networks", "author": "Miltiadis Kofinas and Boris Knyazev and Yan Zhang and Yunlu Chen and Gertjan J. Burghouts and Efstratios Gavves and Cees G. M. Snoek and David W. Zhang", "abstract": "  Neural networks that process the parameters of other neural networks find\napplications in domains as diverse as classifying implicit neural\nrepresentations, generating neural network weights, and predicting\ngeneralization errors. However, existing approaches either overlook the\ninherent permutation symmetry in the neural network or rely on intricate\nweight-sharing patterns to achieve equivariance, while ignoring the impact of\nthe network architecture itself. In this work, we propose to represent neural\nnetworks as computational graphs of parameters, which allows us to harness\npowerful graph neural networks and transformers that preserve permutation\nsymmetry. Consequently, our approach enables a single model to encode neural\ncomputational graphs with diverse architectures. We showcase the effectiveness\nof our method on a wide range of tasks, including classification and editing of\nimplicit neural representations, predicting generalization performance, and\nlearning to optimize, while consistently outperforming state-of-the-art\nmethods. The source code is open-sourced at\nhttps://github.com/mkofinas/neural-graphs.\n", "link": "http://arxiv.org/abs/2403.12143v3", "date": "2024-07-23", "relevancy": 2.0778, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5461}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5341}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20for%20Learning%20Equivariant%20Representations%20of%20Neural%0A%20%20Networks&body=Title%3A%20Graph%20Neural%20Networks%20for%20Learning%20Equivariant%20Representations%20of%20Neural%0A%20%20Networks%0AAuthor%3A%20Miltiadis%20Kofinas%20and%20Boris%20Knyazev%20and%20Yan%20Zhang%20and%20Yunlu%20Chen%20and%20Gertjan%20J.%20Burghouts%20and%20Efstratios%20Gavves%20and%20Cees%20G.%20M.%20Snoek%20and%20David%20W.%20Zhang%0AAbstract%3A%20%20%20Neural%20networks%20that%20process%20the%20parameters%20of%20other%20neural%20networks%20find%0Aapplications%20in%20domains%20as%20diverse%20as%20classifying%20implicit%20neural%0Arepresentations%2C%20generating%20neural%20network%20weights%2C%20and%20predicting%0Ageneralization%20errors.%20However%2C%20existing%20approaches%20either%20overlook%20the%0Ainherent%20permutation%20symmetry%20in%20the%20neural%20network%20or%20rely%20on%20intricate%0Aweight-sharing%20patterns%20to%20achieve%20equivariance%2C%20while%20ignoring%20the%20impact%20of%0Athe%20network%20architecture%20itself.%20In%20this%20work%2C%20we%20propose%20to%20represent%20neural%0Anetworks%20as%20computational%20graphs%20of%20parameters%2C%20which%20allows%20us%20to%20harness%0Apowerful%20graph%20neural%20networks%20and%20transformers%20that%20preserve%20permutation%0Asymmetry.%20Consequently%2C%20our%20approach%20enables%20a%20single%20model%20to%20encode%20neural%0Acomputational%20graphs%20with%20diverse%20architectures.%20We%20showcase%20the%20effectiveness%0Aof%20our%20method%20on%20a%20wide%20range%20of%20tasks%2C%20including%20classification%20and%20editing%20of%0Aimplicit%20neural%20representations%2C%20predicting%20generalization%20performance%2C%20and%0Alearning%20to%20optimize%2C%20while%20consistently%20outperforming%20state-of-the-art%0Amethods.%20The%20source%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/mkofinas/neural-graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12143v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520for%2520Learning%2520Equivariant%2520Representations%2520of%2520Neural%250A%2520%2520Networks%26entry.906535625%3DMiltiadis%2520Kofinas%2520and%2520Boris%2520Knyazev%2520and%2520Yan%2520Zhang%2520and%2520Yunlu%2520Chen%2520and%2520Gertjan%2520J.%2520Burghouts%2520and%2520Efstratios%2520Gavves%2520and%2520Cees%2520G.%2520M.%2520Snoek%2520and%2520David%2520W.%2520Zhang%26entry.1292438233%3D%2520%2520Neural%2520networks%2520that%2520process%2520the%2520parameters%2520of%2520other%2520neural%2520networks%2520find%250Aapplications%2520in%2520domains%2520as%2520diverse%2520as%2520classifying%2520implicit%2520neural%250Arepresentations%252C%2520generating%2520neural%2520network%2520weights%252C%2520and%2520predicting%250Ageneralization%2520errors.%2520However%252C%2520existing%2520approaches%2520either%2520overlook%2520the%250Ainherent%2520permutation%2520symmetry%2520in%2520the%2520neural%2520network%2520or%2520rely%2520on%2520intricate%250Aweight-sharing%2520patterns%2520to%2520achieve%2520equivariance%252C%2520while%2520ignoring%2520the%2520impact%2520of%250Athe%2520network%2520architecture%2520itself.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520represent%2520neural%250Anetworks%2520as%2520computational%2520graphs%2520of%2520parameters%252C%2520which%2520allows%2520us%2520to%2520harness%250Apowerful%2520graph%2520neural%2520networks%2520and%2520transformers%2520that%2520preserve%2520permutation%250Asymmetry.%2520Consequently%252C%2520our%2520approach%2520enables%2520a%2520single%2520model%2520to%2520encode%2520neural%250Acomputational%2520graphs%2520with%2520diverse%2520architectures.%2520We%2520showcase%2520the%2520effectiveness%250Aof%2520our%2520method%2520on%2520a%2520wide%2520range%2520of%2520tasks%252C%2520including%2520classification%2520and%2520editing%2520of%250Aimplicit%2520neural%2520representations%252C%2520predicting%2520generalization%2520performance%252C%2520and%250Alearning%2520to%2520optimize%252C%2520while%2520consistently%2520outperforming%2520state-of-the-art%250Amethods.%2520The%2520source%2520code%2520is%2520open-sourced%2520at%250Ahttps%253A//github.com/mkofinas/neural-graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12143v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20for%20Learning%20Equivariant%20Representations%20of%20Neural%0A%20%20Networks&entry.906535625=Miltiadis%20Kofinas%20and%20Boris%20Knyazev%20and%20Yan%20Zhang%20and%20Yunlu%20Chen%20and%20Gertjan%20J.%20Burghouts%20and%20Efstratios%20Gavves%20and%20Cees%20G.%20M.%20Snoek%20and%20David%20W.%20Zhang&entry.1292438233=%20%20Neural%20networks%20that%20process%20the%20parameters%20of%20other%20neural%20networks%20find%0Aapplications%20in%20domains%20as%20diverse%20as%20classifying%20implicit%20neural%0Arepresentations%2C%20generating%20neural%20network%20weights%2C%20and%20predicting%0Ageneralization%20errors.%20However%2C%20existing%20approaches%20either%20overlook%20the%0Ainherent%20permutation%20symmetry%20in%20the%20neural%20network%20or%20rely%20on%20intricate%0Aweight-sharing%20patterns%20to%20achieve%20equivariance%2C%20while%20ignoring%20the%20impact%20of%0Athe%20network%20architecture%20itself.%20In%20this%20work%2C%20we%20propose%20to%20represent%20neural%0Anetworks%20as%20computational%20graphs%20of%20parameters%2C%20which%20allows%20us%20to%20harness%0Apowerful%20graph%20neural%20networks%20and%20transformers%20that%20preserve%20permutation%0Asymmetry.%20Consequently%2C%20our%20approach%20enables%20a%20single%20model%20to%20encode%20neural%0Acomputational%20graphs%20with%20diverse%20architectures.%20We%20showcase%20the%20effectiveness%0Aof%20our%20method%20on%20a%20wide%20range%20of%20tasks%2C%20including%20classification%20and%20editing%20of%0Aimplicit%20neural%20representations%2C%20predicting%20generalization%20performance%2C%20and%0Alearning%20to%20optimize%2C%20while%20consistently%20outperforming%20state-of-the-art%0Amethods.%20The%20source%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/mkofinas/neural-graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12143v3&entry.124074799=Read"},
{"title": "ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy", "author": "Kirill Vishniakov and Zhiqiang Shen and Zhuang Liu", "abstract": "  Modern computer vision offers a great variety of models to practitioners, and\nselecting a model from multiple options for specific applications can be\nchallenging. Conventionally, competing model architectures and training\nprotocols are compared by their classification accuracy on ImageNet. However,\nthis single metric does not fully capture performance nuances critical for\nspecialized tasks. In this work, we conduct an in-depth comparative analysis of\nmodel behaviors beyond ImageNet accuracy, for both ConvNet and Vision\nTransformer architectures, each across supervised and CLIP training paradigms.\nAlthough our selected models have similar ImageNet accuracies and compute\nrequirements, we find that they differ in many other aspects: types of\nmistakes, output calibration, transferability, and feature invariance, among\nothers. This diversity in model characteristics, not captured by traditional\nmetrics, highlights the need for more nuanced analysis when choosing among\ndifferent models. Our code is available at\nhttps://github.com/kirill-vish/Beyond-INet.\n", "link": "http://arxiv.org/abs/2311.09215v3", "date": "2024-07-23", "relevancy": 2.0724, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConvNet%20vs%20Transformer%2C%20Supervised%20vs%20CLIP%3A%20Beyond%20ImageNet%20Accuracy&body=Title%3A%20ConvNet%20vs%20Transformer%2C%20Supervised%20vs%20CLIP%3A%20Beyond%20ImageNet%20Accuracy%0AAuthor%3A%20Kirill%20Vishniakov%20and%20Zhiqiang%20Shen%20and%20Zhuang%20Liu%0AAbstract%3A%20%20%20Modern%20computer%20vision%20offers%20a%20great%20variety%20of%20models%20to%20practitioners%2C%20and%0Aselecting%20a%20model%20from%20multiple%20options%20for%20specific%20applications%20can%20be%0Achallenging.%20Conventionally%2C%20competing%20model%20architectures%20and%20training%0Aprotocols%20are%20compared%20by%20their%20classification%20accuracy%20on%20ImageNet.%20However%2C%0Athis%20single%20metric%20does%20not%20fully%20capture%20performance%20nuances%20critical%20for%0Aspecialized%20tasks.%20In%20this%20work%2C%20we%20conduct%20an%20in-depth%20comparative%20analysis%20of%0Amodel%20behaviors%20beyond%20ImageNet%20accuracy%2C%20for%20both%20ConvNet%20and%20Vision%0ATransformer%20architectures%2C%20each%20across%20supervised%20and%20CLIP%20training%20paradigms.%0AAlthough%20our%20selected%20models%20have%20similar%20ImageNet%20accuracies%20and%20compute%0Arequirements%2C%20we%20find%20that%20they%20differ%20in%20many%20other%20aspects%3A%20types%20of%0Amistakes%2C%20output%20calibration%2C%20transferability%2C%20and%20feature%20invariance%2C%20among%0Aothers.%20This%20diversity%20in%20model%20characteristics%2C%20not%20captured%20by%20traditional%0Ametrics%2C%20highlights%20the%20need%20for%20more%20nuanced%20analysis%20when%20choosing%20among%0Adifferent%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/kirill-vish/Beyond-INet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09215v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvNet%2520vs%2520Transformer%252C%2520Supervised%2520vs%2520CLIP%253A%2520Beyond%2520ImageNet%2520Accuracy%26entry.906535625%3DKirill%2520Vishniakov%2520and%2520Zhiqiang%2520Shen%2520and%2520Zhuang%2520Liu%26entry.1292438233%3D%2520%2520Modern%2520computer%2520vision%2520offers%2520a%2520great%2520variety%2520of%2520models%2520to%2520practitioners%252C%2520and%250Aselecting%2520a%2520model%2520from%2520multiple%2520options%2520for%2520specific%2520applications%2520can%2520be%250Achallenging.%2520Conventionally%252C%2520competing%2520model%2520architectures%2520and%2520training%250Aprotocols%2520are%2520compared%2520by%2520their%2520classification%2520accuracy%2520on%2520ImageNet.%2520However%252C%250Athis%2520single%2520metric%2520does%2520not%2520fully%2520capture%2520performance%2520nuances%2520critical%2520for%250Aspecialized%2520tasks.%2520In%2520this%2520work%252C%2520we%2520conduct%2520an%2520in-depth%2520comparative%2520analysis%2520of%250Amodel%2520behaviors%2520beyond%2520ImageNet%2520accuracy%252C%2520for%2520both%2520ConvNet%2520and%2520Vision%250ATransformer%2520architectures%252C%2520each%2520across%2520supervised%2520and%2520CLIP%2520training%2520paradigms.%250AAlthough%2520our%2520selected%2520models%2520have%2520similar%2520ImageNet%2520accuracies%2520and%2520compute%250Arequirements%252C%2520we%2520find%2520that%2520they%2520differ%2520in%2520many%2520other%2520aspects%253A%2520types%2520of%250Amistakes%252C%2520output%2520calibration%252C%2520transferability%252C%2520and%2520feature%2520invariance%252C%2520among%250Aothers.%2520This%2520diversity%2520in%2520model%2520characteristics%252C%2520not%2520captured%2520by%2520traditional%250Ametrics%252C%2520highlights%2520the%2520need%2520for%2520more%2520nuanced%2520analysis%2520when%2520choosing%2520among%250Adifferent%2520models.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/kirill-vish/Beyond-INet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.09215v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConvNet%20vs%20Transformer%2C%20Supervised%20vs%20CLIP%3A%20Beyond%20ImageNet%20Accuracy&entry.906535625=Kirill%20Vishniakov%20and%20Zhiqiang%20Shen%20and%20Zhuang%20Liu&entry.1292438233=%20%20Modern%20computer%20vision%20offers%20a%20great%20variety%20of%20models%20to%20practitioners%2C%20and%0Aselecting%20a%20model%20from%20multiple%20options%20for%20specific%20applications%20can%20be%0Achallenging.%20Conventionally%2C%20competing%20model%20architectures%20and%20training%0Aprotocols%20are%20compared%20by%20their%20classification%20accuracy%20on%20ImageNet.%20However%2C%0Athis%20single%20metric%20does%20not%20fully%20capture%20performance%20nuances%20critical%20for%0Aspecialized%20tasks.%20In%20this%20work%2C%20we%20conduct%20an%20in-depth%20comparative%20analysis%20of%0Amodel%20behaviors%20beyond%20ImageNet%20accuracy%2C%20for%20both%20ConvNet%20and%20Vision%0ATransformer%20architectures%2C%20each%20across%20supervised%20and%20CLIP%20training%20paradigms.%0AAlthough%20our%20selected%20models%20have%20similar%20ImageNet%20accuracies%20and%20compute%0Arequirements%2C%20we%20find%20that%20they%20differ%20in%20many%20other%20aspects%3A%20types%20of%0Amistakes%2C%20output%20calibration%2C%20transferability%2C%20and%20feature%20invariance%2C%20among%0Aothers.%20This%20diversity%20in%20model%20characteristics%2C%20not%20captured%20by%20traditional%0Ametrics%2C%20highlights%20the%20need%20for%20more%20nuanced%20analysis%20when%20choosing%20among%0Adifferent%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/kirill-vish/Beyond-INet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09215v3&entry.124074799=Read"},
{"title": "MedMNIST-C: Comprehensive benchmark and improved classifier robustness\n  by simulating realistic image corruptions", "author": "Francesco Di Salvo and Sebastian Doerrich and Christian Ledig", "abstract": "  The integration of neural-network-based systems into clinical practice is\nlimited by challenges related to domain generalization and robustness. The\ncomputer vision community established benchmarks such as ImageNet-C as a\nfundamental prerequisite to measure progress towards those challenges. Similar\ndatasets are largely absent in the medical imaging community which lacks a\ncomprehensive benchmark that spans across imaging modalities and applications.\nTo address this gap, we create and open-source MedMNIST-C, a benchmark dataset\nbased on the MedMNIST+ collection covering 12 datasets and 9 imaging\nmodalities. We simulate task and modality-specific image corruptions of varying\nseverity to comprehensively evaluate the robustness of established algorithms\nagainst real-world artifacts and distribution shifts. We further provide\nquantitative evidence that our simple-to-use artificial corruptions allow for\nhighly performant, lightweight data augmentation to enhance model robustness.\nUnlike traditional, generic augmentation strategies, our approach leverages\ndomain knowledge, exhibiting significantly higher robustness when compared to\nwidely adopted methods. By introducing MedMNIST-C and open-sourcing the\ncorresponding library allowing for targeted data augmentations, we contribute\nto the development of increasingly robust methods tailored to the challenges of\nmedical imaging. The code is available at\nhttps://github.com/francescodisalvo05/medmnistc-api .\n", "link": "http://arxiv.org/abs/2406.17536v3", "date": "2024-07-23", "relevancy": 2.0551, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5392}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5146}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedMNIST-C%3A%20Comprehensive%20benchmark%20and%20improved%20classifier%20robustness%0A%20%20by%20simulating%20realistic%20image%20corruptions&body=Title%3A%20MedMNIST-C%3A%20Comprehensive%20benchmark%20and%20improved%20classifier%20robustness%0A%20%20by%20simulating%20realistic%20image%20corruptions%0AAuthor%3A%20Francesco%20Di%20Salvo%20and%20Sebastian%20Doerrich%20and%20Christian%20Ledig%0AAbstract%3A%20%20%20The%20integration%20of%20neural-network-based%20systems%20into%20clinical%20practice%20is%0Alimited%20by%20challenges%20related%20to%20domain%20generalization%20and%20robustness.%20The%0Acomputer%20vision%20community%20established%20benchmarks%20such%20as%20ImageNet-C%20as%20a%0Afundamental%20prerequisite%20to%20measure%20progress%20towards%20those%20challenges.%20Similar%0Adatasets%20are%20largely%20absent%20in%20the%20medical%20imaging%20community%20which%20lacks%20a%0Acomprehensive%20benchmark%20that%20spans%20across%20imaging%20modalities%20and%20applications.%0ATo%20address%20this%20gap%2C%20we%20create%20and%20open-source%20MedMNIST-C%2C%20a%20benchmark%20dataset%0Abased%20on%20the%20MedMNIST%2B%20collection%20covering%2012%20datasets%20and%209%20imaging%0Amodalities.%20We%20simulate%20task%20and%20modality-specific%20image%20corruptions%20of%20varying%0Aseverity%20to%20comprehensively%20evaluate%20the%20robustness%20of%20established%20algorithms%0Aagainst%20real-world%20artifacts%20and%20distribution%20shifts.%20We%20further%20provide%0Aquantitative%20evidence%20that%20our%20simple-to-use%20artificial%20corruptions%20allow%20for%0Ahighly%20performant%2C%20lightweight%20data%20augmentation%20to%20enhance%20model%20robustness.%0AUnlike%20traditional%2C%20generic%20augmentation%20strategies%2C%20our%20approach%20leverages%0Adomain%20knowledge%2C%20exhibiting%20significantly%20higher%20robustness%20when%20compared%20to%0Awidely%20adopted%20methods.%20By%20introducing%20MedMNIST-C%20and%20open-sourcing%20the%0Acorresponding%20library%20allowing%20for%20targeted%20data%20augmentations%2C%20we%20contribute%0Ato%20the%20development%20of%20increasingly%20robust%20methods%20tailored%20to%20the%20challenges%20of%0Amedical%20imaging.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/francescodisalvo05/medmnistc-api%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17536v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedMNIST-C%253A%2520Comprehensive%2520benchmark%2520and%2520improved%2520classifier%2520robustness%250A%2520%2520by%2520simulating%2520realistic%2520image%2520corruptions%26entry.906535625%3DFrancesco%2520Di%2520Salvo%2520and%2520Sebastian%2520Doerrich%2520and%2520Christian%2520Ledig%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520neural-network-based%2520systems%2520into%2520clinical%2520practice%2520is%250Alimited%2520by%2520challenges%2520related%2520to%2520domain%2520generalization%2520and%2520robustness.%2520The%250Acomputer%2520vision%2520community%2520established%2520benchmarks%2520such%2520as%2520ImageNet-C%2520as%2520a%250Afundamental%2520prerequisite%2520to%2520measure%2520progress%2520towards%2520those%2520challenges.%2520Similar%250Adatasets%2520are%2520largely%2520absent%2520in%2520the%2520medical%2520imaging%2520community%2520which%2520lacks%2520a%250Acomprehensive%2520benchmark%2520that%2520spans%2520across%2520imaging%2520modalities%2520and%2520applications.%250ATo%2520address%2520this%2520gap%252C%2520we%2520create%2520and%2520open-source%2520MedMNIST-C%252C%2520a%2520benchmark%2520dataset%250Abased%2520on%2520the%2520MedMNIST%252B%2520collection%2520covering%252012%2520datasets%2520and%25209%2520imaging%250Amodalities.%2520We%2520simulate%2520task%2520and%2520modality-specific%2520image%2520corruptions%2520of%2520varying%250Aseverity%2520to%2520comprehensively%2520evaluate%2520the%2520robustness%2520of%2520established%2520algorithms%250Aagainst%2520real-world%2520artifacts%2520and%2520distribution%2520shifts.%2520We%2520further%2520provide%250Aquantitative%2520evidence%2520that%2520our%2520simple-to-use%2520artificial%2520corruptions%2520allow%2520for%250Ahighly%2520performant%252C%2520lightweight%2520data%2520augmentation%2520to%2520enhance%2520model%2520robustness.%250AUnlike%2520traditional%252C%2520generic%2520augmentation%2520strategies%252C%2520our%2520approach%2520leverages%250Adomain%2520knowledge%252C%2520exhibiting%2520significantly%2520higher%2520robustness%2520when%2520compared%2520to%250Awidely%2520adopted%2520methods.%2520By%2520introducing%2520MedMNIST-C%2520and%2520open-sourcing%2520the%250Acorresponding%2520library%2520allowing%2520for%2520targeted%2520data%2520augmentations%252C%2520we%2520contribute%250Ato%2520the%2520development%2520of%2520increasingly%2520robust%2520methods%2520tailored%2520to%2520the%2520challenges%2520of%250Amedical%2520imaging.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/francescodisalvo05/medmnistc-api%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17536v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedMNIST-C%3A%20Comprehensive%20benchmark%20and%20improved%20classifier%20robustness%0A%20%20by%20simulating%20realistic%20image%20corruptions&entry.906535625=Francesco%20Di%20Salvo%20and%20Sebastian%20Doerrich%20and%20Christian%20Ledig&entry.1292438233=%20%20The%20integration%20of%20neural-network-based%20systems%20into%20clinical%20practice%20is%0Alimited%20by%20challenges%20related%20to%20domain%20generalization%20and%20robustness.%20The%0Acomputer%20vision%20community%20established%20benchmarks%20such%20as%20ImageNet-C%20as%20a%0Afundamental%20prerequisite%20to%20measure%20progress%20towards%20those%20challenges.%20Similar%0Adatasets%20are%20largely%20absent%20in%20the%20medical%20imaging%20community%20which%20lacks%20a%0Acomprehensive%20benchmark%20that%20spans%20across%20imaging%20modalities%20and%20applications.%0ATo%20address%20this%20gap%2C%20we%20create%20and%20open-source%20MedMNIST-C%2C%20a%20benchmark%20dataset%0Abased%20on%20the%20MedMNIST%2B%20collection%20covering%2012%20datasets%20and%209%20imaging%0Amodalities.%20We%20simulate%20task%20and%20modality-specific%20image%20corruptions%20of%20varying%0Aseverity%20to%20comprehensively%20evaluate%20the%20robustness%20of%20established%20algorithms%0Aagainst%20real-world%20artifacts%20and%20distribution%20shifts.%20We%20further%20provide%0Aquantitative%20evidence%20that%20our%20simple-to-use%20artificial%20corruptions%20allow%20for%0Ahighly%20performant%2C%20lightweight%20data%20augmentation%20to%20enhance%20model%20robustness.%0AUnlike%20traditional%2C%20generic%20augmentation%20strategies%2C%20our%20approach%20leverages%0Adomain%20knowledge%2C%20exhibiting%20significantly%20higher%20robustness%20when%20compared%20to%0Awidely%20adopted%20methods.%20By%20introducing%20MedMNIST-C%20and%20open-sourcing%20the%0Acorresponding%20library%20allowing%20for%20targeted%20data%20augmentations%2C%20we%20contribute%0Ato%20the%20development%20of%20increasingly%20robust%20methods%20tailored%20to%20the%20challenges%20of%0Amedical%20imaging.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/francescodisalvo05/medmnistc-api%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17536v3&entry.124074799=Read"},
{"title": "BEV$^2$PR: BEV-Enhanced Visual Place Recognition with Structural Cues", "author": "Fudong Ge and Yiwei Zhang and Shuhan Shen and Yue Wang and Weiming Hu and Jin Gao", "abstract": "  In this paper, we propose a new image-based visual place recognition (VPR)\nframework by exploiting the structural cues in bird's-eye view (BEV) from a\nsingle monocular camera. The motivation arises from two key observations about\nplace recognition methods based on both appearance and structure: 1) For the\nmethods relying on LiDAR sensors, the integration of LiDAR in robotic systems\nhas led to increased expenses, while the alignment of data between different\nsensors is also a major challenge. 2) Other image-/camera-based methods,\ninvolving integrating RGB images and their derived variants (eg, pseudo depth\nimages, pseudo 3D point clouds), exhibit several limitations, such as the\nfailure to effectively exploit the explicit spatial relationships between\ndifferent objects. To tackle the above issues, we design a new BEV-enhanced VPR\nframework, namely BEV$^2$PR, generating a composite descriptor with both visual\ncues and spatial awareness based on a single camera. The key points lie in: 1)\nWe use BEV features as an explicit source of structural knowledge in\nconstructing global features. 2) The lower layers of the pre-trained backbone\nfrom BEV generation are shared for visual and structural streams in VPR,\nfacilitating the learning of fine-grained local features in the visual stream.\n3) The complementary visual and structural features can jointly enhance VPR\nperformance. Our BEV$^2$PR framework enables consistent performance\nimprovements over several popular aggregation modules for RGB global features.\nThe experiments on our collected VPR-NuScenes dataset demonstrate an absolute\ngain of 2.47% on Recall@1 for the strong Conv-AP baseline to achieve the best\nperformance in our setting, and notably, a 18.06% gain on the hard set. The\ncode and dataset will be available at https://github.com/FudongGe/BEV2PR.\n", "link": "http://arxiv.org/abs/2403.06600v2", "date": "2024-07-23", "relevancy": 2.0545, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5161}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5123}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEV%24%5E2%24PR%3A%20BEV-Enhanced%20Visual%20Place%20Recognition%20with%20Structural%20Cues&body=Title%3A%20BEV%24%5E2%24PR%3A%20BEV-Enhanced%20Visual%20Place%20Recognition%20with%20Structural%20Cues%0AAuthor%3A%20Fudong%20Ge%20and%20Yiwei%20Zhang%20and%20Shuhan%20Shen%20and%20Yue%20Wang%20and%20Weiming%20Hu%20and%20Jin%20Gao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20image-based%20visual%20place%20recognition%20%28VPR%29%0Aframework%20by%20exploiting%20the%20structural%20cues%20in%20bird%27s-eye%20view%20%28BEV%29%20from%20a%0Asingle%20monocular%20camera.%20The%20motivation%20arises%20from%20two%20key%20observations%20about%0Aplace%20recognition%20methods%20based%20on%20both%20appearance%20and%20structure%3A%201%29%20For%20the%0Amethods%20relying%20on%20LiDAR%20sensors%2C%20the%20integration%20of%20LiDAR%20in%20robotic%20systems%0Ahas%20led%20to%20increased%20expenses%2C%20while%20the%20alignment%20of%20data%20between%20different%0Asensors%20is%20also%20a%20major%20challenge.%202%29%20Other%20image-/camera-based%20methods%2C%0Ainvolving%20integrating%20RGB%20images%20and%20their%20derived%20variants%20%28eg%2C%20pseudo%20depth%0Aimages%2C%20pseudo%203D%20point%20clouds%29%2C%20exhibit%20several%20limitations%2C%20such%20as%20the%0Afailure%20to%20effectively%20exploit%20the%20explicit%20spatial%20relationships%20between%0Adifferent%20objects.%20To%20tackle%20the%20above%20issues%2C%20we%20design%20a%20new%20BEV-enhanced%20VPR%0Aframework%2C%20namely%20BEV%24%5E2%24PR%2C%20generating%20a%20composite%20descriptor%20with%20both%20visual%0Acues%20and%20spatial%20awareness%20based%20on%20a%20single%20camera.%20The%20key%20points%20lie%20in%3A%201%29%0AWe%20use%20BEV%20features%20as%20an%20explicit%20source%20of%20structural%20knowledge%20in%0Aconstructing%20global%20features.%202%29%20The%20lower%20layers%20of%20the%20pre-trained%20backbone%0Afrom%20BEV%20generation%20are%20shared%20for%20visual%20and%20structural%20streams%20in%20VPR%2C%0Afacilitating%20the%20learning%20of%20fine-grained%20local%20features%20in%20the%20visual%20stream.%0A3%29%20The%20complementary%20visual%20and%20structural%20features%20can%20jointly%20enhance%20VPR%0Aperformance.%20Our%20BEV%24%5E2%24PR%20framework%20enables%20consistent%20performance%0Aimprovements%20over%20several%20popular%20aggregation%20modules%20for%20RGB%20global%20features.%0AThe%20experiments%20on%20our%20collected%20VPR-NuScenes%20dataset%20demonstrate%20an%20absolute%0Again%20of%202.47%25%20on%20Recall%401%20for%20the%20strong%20Conv-AP%20baseline%20to%20achieve%20the%20best%0Aperformance%20in%20our%20setting%2C%20and%20notably%2C%20a%2018.06%25%20gain%20on%20the%20hard%20set.%20The%0Acode%20and%20dataset%20will%20be%20available%20at%20https%3A//github.com/FudongGe/BEV2PR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06600v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEV%2524%255E2%2524PR%253A%2520BEV-Enhanced%2520Visual%2520Place%2520Recognition%2520with%2520Structural%2520Cues%26entry.906535625%3DFudong%2520Ge%2520and%2520Yiwei%2520Zhang%2520and%2520Shuhan%2520Shen%2520and%2520Yue%2520Wang%2520and%2520Weiming%2520Hu%2520and%2520Jin%2520Gao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520image-based%2520visual%2520place%2520recognition%2520%2528VPR%2529%250Aframework%2520by%2520exploiting%2520the%2520structural%2520cues%2520in%2520bird%2527s-eye%2520view%2520%2528BEV%2529%2520from%2520a%250Asingle%2520monocular%2520camera.%2520The%2520motivation%2520arises%2520from%2520two%2520key%2520observations%2520about%250Aplace%2520recognition%2520methods%2520based%2520on%2520both%2520appearance%2520and%2520structure%253A%25201%2529%2520For%2520the%250Amethods%2520relying%2520on%2520LiDAR%2520sensors%252C%2520the%2520integration%2520of%2520LiDAR%2520in%2520robotic%2520systems%250Ahas%2520led%2520to%2520increased%2520expenses%252C%2520while%2520the%2520alignment%2520of%2520data%2520between%2520different%250Asensors%2520is%2520also%2520a%2520major%2520challenge.%25202%2529%2520Other%2520image-/camera-based%2520methods%252C%250Ainvolving%2520integrating%2520RGB%2520images%2520and%2520their%2520derived%2520variants%2520%2528eg%252C%2520pseudo%2520depth%250Aimages%252C%2520pseudo%25203D%2520point%2520clouds%2529%252C%2520exhibit%2520several%2520limitations%252C%2520such%2520as%2520the%250Afailure%2520to%2520effectively%2520exploit%2520the%2520explicit%2520spatial%2520relationships%2520between%250Adifferent%2520objects.%2520To%2520tackle%2520the%2520above%2520issues%252C%2520we%2520design%2520a%2520new%2520BEV-enhanced%2520VPR%250Aframework%252C%2520namely%2520BEV%2524%255E2%2524PR%252C%2520generating%2520a%2520composite%2520descriptor%2520with%2520both%2520visual%250Acues%2520and%2520spatial%2520awareness%2520based%2520on%2520a%2520single%2520camera.%2520The%2520key%2520points%2520lie%2520in%253A%25201%2529%250AWe%2520use%2520BEV%2520features%2520as%2520an%2520explicit%2520source%2520of%2520structural%2520knowledge%2520in%250Aconstructing%2520global%2520features.%25202%2529%2520The%2520lower%2520layers%2520of%2520the%2520pre-trained%2520backbone%250Afrom%2520BEV%2520generation%2520are%2520shared%2520for%2520visual%2520and%2520structural%2520streams%2520in%2520VPR%252C%250Afacilitating%2520the%2520learning%2520of%2520fine-grained%2520local%2520features%2520in%2520the%2520visual%2520stream.%250A3%2529%2520The%2520complementary%2520visual%2520and%2520structural%2520features%2520can%2520jointly%2520enhance%2520VPR%250Aperformance.%2520Our%2520BEV%2524%255E2%2524PR%2520framework%2520enables%2520consistent%2520performance%250Aimprovements%2520over%2520several%2520popular%2520aggregation%2520modules%2520for%2520RGB%2520global%2520features.%250AThe%2520experiments%2520on%2520our%2520collected%2520VPR-NuScenes%2520dataset%2520demonstrate%2520an%2520absolute%250Again%2520of%25202.47%2525%2520on%2520Recall%25401%2520for%2520the%2520strong%2520Conv-AP%2520baseline%2520to%2520achieve%2520the%2520best%250Aperformance%2520in%2520our%2520setting%252C%2520and%2520notably%252C%2520a%252018.06%2525%2520gain%2520on%2520the%2520hard%2520set.%2520The%250Acode%2520and%2520dataset%2520will%2520be%2520available%2520at%2520https%253A//github.com/FudongGe/BEV2PR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06600v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEV%24%5E2%24PR%3A%20BEV-Enhanced%20Visual%20Place%20Recognition%20with%20Structural%20Cues&entry.906535625=Fudong%20Ge%20and%20Yiwei%20Zhang%20and%20Shuhan%20Shen%20and%20Yue%20Wang%20and%20Weiming%20Hu%20and%20Jin%20Gao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20image-based%20visual%20place%20recognition%20%28VPR%29%0Aframework%20by%20exploiting%20the%20structural%20cues%20in%20bird%27s-eye%20view%20%28BEV%29%20from%20a%0Asingle%20monocular%20camera.%20The%20motivation%20arises%20from%20two%20key%20observations%20about%0Aplace%20recognition%20methods%20based%20on%20both%20appearance%20and%20structure%3A%201%29%20For%20the%0Amethods%20relying%20on%20LiDAR%20sensors%2C%20the%20integration%20of%20LiDAR%20in%20robotic%20systems%0Ahas%20led%20to%20increased%20expenses%2C%20while%20the%20alignment%20of%20data%20between%20different%0Asensors%20is%20also%20a%20major%20challenge.%202%29%20Other%20image-/camera-based%20methods%2C%0Ainvolving%20integrating%20RGB%20images%20and%20their%20derived%20variants%20%28eg%2C%20pseudo%20depth%0Aimages%2C%20pseudo%203D%20point%20clouds%29%2C%20exhibit%20several%20limitations%2C%20such%20as%20the%0Afailure%20to%20effectively%20exploit%20the%20explicit%20spatial%20relationships%20between%0Adifferent%20objects.%20To%20tackle%20the%20above%20issues%2C%20we%20design%20a%20new%20BEV-enhanced%20VPR%0Aframework%2C%20namely%20BEV%24%5E2%24PR%2C%20generating%20a%20composite%20descriptor%20with%20both%20visual%0Acues%20and%20spatial%20awareness%20based%20on%20a%20single%20camera.%20The%20key%20points%20lie%20in%3A%201%29%0AWe%20use%20BEV%20features%20as%20an%20explicit%20source%20of%20structural%20knowledge%20in%0Aconstructing%20global%20features.%202%29%20The%20lower%20layers%20of%20the%20pre-trained%20backbone%0Afrom%20BEV%20generation%20are%20shared%20for%20visual%20and%20structural%20streams%20in%20VPR%2C%0Afacilitating%20the%20learning%20of%20fine-grained%20local%20features%20in%20the%20visual%20stream.%0A3%29%20The%20complementary%20visual%20and%20structural%20features%20can%20jointly%20enhance%20VPR%0Aperformance.%20Our%20BEV%24%5E2%24PR%20framework%20enables%20consistent%20performance%0Aimprovements%20over%20several%20popular%20aggregation%20modules%20for%20RGB%20global%20features.%0AThe%20experiments%20on%20our%20collected%20VPR-NuScenes%20dataset%20demonstrate%20an%20absolute%0Again%20of%202.47%25%20on%20Recall%401%20for%20the%20strong%20Conv-AP%20baseline%20to%20achieve%20the%20best%0Aperformance%20in%20our%20setting%2C%20and%20notably%2C%20a%2018.06%25%20gain%20on%20the%20hard%20set.%20The%0Acode%20and%20dataset%20will%20be%20available%20at%20https%3A//github.com/FudongGe/BEV2PR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06600v2&entry.124074799=Read"},
{"title": "Rethinking Out-of-Distribution Detection on Imbalanced Data Distribution", "author": "Kai Liu and Zhihang Fu and Sheng Jin and Chao Chen and Ze Chen and Rongxin Jiang and Fan Zhou and Yaowu Chen and Jieping Ye", "abstract": "  Detecting and rejecting unknown out-of-distribution (OOD) samples is critical\nfor deployed neural networks to void unreliable predictions. In real-world\nscenarios, however, the efficacy of existing OOD detection methods is often\nimpeded by the inherent imbalance of in-distribution (ID) data, which causes\nsignificant performance decline. Through statistical observations, we have\nidentified two common challenges faced by different OOD detectors:\nmisidentifying tail class ID samples as OOD, while erroneously predicting OOD\nsamples as head class from ID. To explain this phenomenon, we introduce a\ngeneralized statistical framework, termed ImOOD, to formulate the OOD detection\nproblem on imbalanced data distribution. Consequently, the theoretical analysis\nreveals that there exists a class-aware bias item between balanced and\nimbalanced OOD detection, which contributes to the performance gap. Building\nupon this finding, we present a unified training-time regularization technique\nto mitigate the bias and boost imbalanced OOD detectors across architecture\ndesigns. Our theoretically grounded method translates into consistent\nimprovements on the representative CIFAR10-LT, CIFAR100-LT, and ImageNet-LT\nbenchmarks against several state-of-the-art OOD detection approaches. Code will\nbe made public soon.\n", "link": "http://arxiv.org/abs/2407.16430v1", "date": "2024-07-23", "relevancy": 2.0517, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5224}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5127}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Out-of-Distribution%20Detection%20on%20Imbalanced%20Data%20Distribution&body=Title%3A%20Rethinking%20Out-of-Distribution%20Detection%20on%20Imbalanced%20Data%20Distribution%0AAuthor%3A%20Kai%20Liu%20and%20Zhihang%20Fu%20and%20Sheng%20Jin%20and%20Chao%20Chen%20and%20Ze%20Chen%20and%20Rongxin%20Jiang%20and%20Fan%20Zhou%20and%20Yaowu%20Chen%20and%20Jieping%20Ye%0AAbstract%3A%20%20%20Detecting%20and%20rejecting%20unknown%20out-of-distribution%20%28OOD%29%20samples%20is%20critical%0Afor%20deployed%20neural%20networks%20to%20void%20unreliable%20predictions.%20In%20real-world%0Ascenarios%2C%20however%2C%20the%20efficacy%20of%20existing%20OOD%20detection%20methods%20is%20often%0Aimpeded%20by%20the%20inherent%20imbalance%20of%20in-distribution%20%28ID%29%20data%2C%20which%20causes%0Asignificant%20performance%20decline.%20Through%20statistical%20observations%2C%20we%20have%0Aidentified%20two%20common%20challenges%20faced%20by%20different%20OOD%20detectors%3A%0Amisidentifying%20tail%20class%20ID%20samples%20as%20OOD%2C%20while%20erroneously%20predicting%20OOD%0Asamples%20as%20head%20class%20from%20ID.%20To%20explain%20this%20phenomenon%2C%20we%20introduce%20a%0Ageneralized%20statistical%20framework%2C%20termed%20ImOOD%2C%20to%20formulate%20the%20OOD%20detection%0Aproblem%20on%20imbalanced%20data%20distribution.%20Consequently%2C%20the%20theoretical%20analysis%0Areveals%20that%20there%20exists%20a%20class-aware%20bias%20item%20between%20balanced%20and%0Aimbalanced%20OOD%20detection%2C%20which%20contributes%20to%20the%20performance%20gap.%20Building%0Aupon%20this%20finding%2C%20we%20present%20a%20unified%20training-time%20regularization%20technique%0Ato%20mitigate%20the%20bias%20and%20boost%20imbalanced%20OOD%20detectors%20across%20architecture%0Adesigns.%20Our%20theoretically%20grounded%20method%20translates%20into%20consistent%0Aimprovements%20on%20the%20representative%20CIFAR10-LT%2C%20CIFAR100-LT%2C%20and%20ImageNet-LT%0Abenchmarks%20against%20several%20state-of-the-art%20OOD%20detection%20approaches.%20Code%20will%0Abe%20made%20public%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Out-of-Distribution%2520Detection%2520on%2520Imbalanced%2520Data%2520Distribution%26entry.906535625%3DKai%2520Liu%2520and%2520Zhihang%2520Fu%2520and%2520Sheng%2520Jin%2520and%2520Chao%2520Chen%2520and%2520Ze%2520Chen%2520and%2520Rongxin%2520Jiang%2520and%2520Fan%2520Zhou%2520and%2520Yaowu%2520Chen%2520and%2520Jieping%2520Ye%26entry.1292438233%3D%2520%2520Detecting%2520and%2520rejecting%2520unknown%2520out-of-distribution%2520%2528OOD%2529%2520samples%2520is%2520critical%250Afor%2520deployed%2520neural%2520networks%2520to%2520void%2520unreliable%2520predictions.%2520In%2520real-world%250Ascenarios%252C%2520however%252C%2520the%2520efficacy%2520of%2520existing%2520OOD%2520detection%2520methods%2520is%2520often%250Aimpeded%2520by%2520the%2520inherent%2520imbalance%2520of%2520in-distribution%2520%2528ID%2529%2520data%252C%2520which%2520causes%250Asignificant%2520performance%2520decline.%2520Through%2520statistical%2520observations%252C%2520we%2520have%250Aidentified%2520two%2520common%2520challenges%2520faced%2520by%2520different%2520OOD%2520detectors%253A%250Amisidentifying%2520tail%2520class%2520ID%2520samples%2520as%2520OOD%252C%2520while%2520erroneously%2520predicting%2520OOD%250Asamples%2520as%2520head%2520class%2520from%2520ID.%2520To%2520explain%2520this%2520phenomenon%252C%2520we%2520introduce%2520a%250Ageneralized%2520statistical%2520framework%252C%2520termed%2520ImOOD%252C%2520to%2520formulate%2520the%2520OOD%2520detection%250Aproblem%2520on%2520imbalanced%2520data%2520distribution.%2520Consequently%252C%2520the%2520theoretical%2520analysis%250Areveals%2520that%2520there%2520exists%2520a%2520class-aware%2520bias%2520item%2520between%2520balanced%2520and%250Aimbalanced%2520OOD%2520detection%252C%2520which%2520contributes%2520to%2520the%2520performance%2520gap.%2520Building%250Aupon%2520this%2520finding%252C%2520we%2520present%2520a%2520unified%2520training-time%2520regularization%2520technique%250Ato%2520mitigate%2520the%2520bias%2520and%2520boost%2520imbalanced%2520OOD%2520detectors%2520across%2520architecture%250Adesigns.%2520Our%2520theoretically%2520grounded%2520method%2520translates%2520into%2520consistent%250Aimprovements%2520on%2520the%2520representative%2520CIFAR10-LT%252C%2520CIFAR100-LT%252C%2520and%2520ImageNet-LT%250Abenchmarks%2520against%2520several%2520state-of-the-art%2520OOD%2520detection%2520approaches.%2520Code%2520will%250Abe%2520made%2520public%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Out-of-Distribution%20Detection%20on%20Imbalanced%20Data%20Distribution&entry.906535625=Kai%20Liu%20and%20Zhihang%20Fu%20and%20Sheng%20Jin%20and%20Chao%20Chen%20and%20Ze%20Chen%20and%20Rongxin%20Jiang%20and%20Fan%20Zhou%20and%20Yaowu%20Chen%20and%20Jieping%20Ye&entry.1292438233=%20%20Detecting%20and%20rejecting%20unknown%20out-of-distribution%20%28OOD%29%20samples%20is%20critical%0Afor%20deployed%20neural%20networks%20to%20void%20unreliable%20predictions.%20In%20real-world%0Ascenarios%2C%20however%2C%20the%20efficacy%20of%20existing%20OOD%20detection%20methods%20is%20often%0Aimpeded%20by%20the%20inherent%20imbalance%20of%20in-distribution%20%28ID%29%20data%2C%20which%20causes%0Asignificant%20performance%20decline.%20Through%20statistical%20observations%2C%20we%20have%0Aidentified%20two%20common%20challenges%20faced%20by%20different%20OOD%20detectors%3A%0Amisidentifying%20tail%20class%20ID%20samples%20as%20OOD%2C%20while%20erroneously%20predicting%20OOD%0Asamples%20as%20head%20class%20from%20ID.%20To%20explain%20this%20phenomenon%2C%20we%20introduce%20a%0Ageneralized%20statistical%20framework%2C%20termed%20ImOOD%2C%20to%20formulate%20the%20OOD%20detection%0Aproblem%20on%20imbalanced%20data%20distribution.%20Consequently%2C%20the%20theoretical%20analysis%0Areveals%20that%20there%20exists%20a%20class-aware%20bias%20item%20between%20balanced%20and%0Aimbalanced%20OOD%20detection%2C%20which%20contributes%20to%20the%20performance%20gap.%20Building%0Aupon%20this%20finding%2C%20we%20present%20a%20unified%20training-time%20regularization%20technique%0Ato%20mitigate%20the%20bias%20and%20boost%20imbalanced%20OOD%20detectors%20across%20architecture%0Adesigns.%20Our%20theoretically%20grounded%20method%20translates%20into%20consistent%0Aimprovements%20on%20the%20representative%20CIFAR10-LT%2C%20CIFAR100-LT%2C%20and%20ImageNet-LT%0Abenchmarks%20against%20several%20state-of-the-art%20OOD%20detection%20approaches.%20Code%20will%0Abe%20made%20public%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16430v1&entry.124074799=Read"},
{"title": "A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data", "author": "Adrian Remonda and Nicklas Hansen and Ayoub Raji and Nicola Musiu and Marko Bertogna and Eduardo Veas and Xiaolong Wang", "abstract": "  Despite the availability of international prize-money competitions, scaled\nvehicles, and simulation environments, research on autonomous racing and the\ncontrol of sports cars operating close to the limit of handling has been\nlimited by the high costs of vehicle acquisition and management, as well as the\nlimited physics accuracy of open-source simulators. In this paper, we propose a\nracing simulation platform based on the simulator Assetto Corsa to test,\nvalidate, and benchmark autonomous driving algorithms, including reinforcement\nlearning (RL) and classical Model Predictive Control (MPC), in realistic and\nchallenging scenarios. Our contributions include the development of this\nsimulation platform, several state-of-the-art algorithms tailored to the racing\nenvironment, and a comprehensive dataset collected from human drivers.\nAdditionally, we evaluate algorithms in the offline RL setting. All the\nnecessary code (including environment and benchmarks), working examples,\ndatasets, and videos are publicly released and can be found at:\n\\url{https://assetto-corsa-gym.github.io}.\n", "link": "http://arxiv.org/abs/2407.16680v1", "date": "2024-07-23", "relevancy": 2.0221, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5493}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4971}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simulation%20Benchmark%20for%20Autonomous%20Racing%20with%20Large-Scale%20Human%20Data&body=Title%3A%20A%20Simulation%20Benchmark%20for%20Autonomous%20Racing%20with%20Large-Scale%20Human%20Data%0AAuthor%3A%20Adrian%20Remonda%20and%20Nicklas%20Hansen%20and%20Ayoub%20Raji%20and%20Nicola%20Musiu%20and%20Marko%20Bertogna%20and%20Eduardo%20Veas%20and%20Xiaolong%20Wang%0AAbstract%3A%20%20%20Despite%20the%20availability%20of%20international%20prize-money%20competitions%2C%20scaled%0Avehicles%2C%20and%20simulation%20environments%2C%20research%20on%20autonomous%20racing%20and%20the%0Acontrol%20of%20sports%20cars%20operating%20close%20to%20the%20limit%20of%20handling%20has%20been%0Alimited%20by%20the%20high%20costs%20of%20vehicle%20acquisition%20and%20management%2C%20as%20well%20as%20the%0Alimited%20physics%20accuracy%20of%20open-source%20simulators.%20In%20this%20paper%2C%20we%20propose%20a%0Aracing%20simulation%20platform%20based%20on%20the%20simulator%20Assetto%20Corsa%20to%20test%2C%0Avalidate%2C%20and%20benchmark%20autonomous%20driving%20algorithms%2C%20including%20reinforcement%0Alearning%20%28RL%29%20and%20classical%20Model%20Predictive%20Control%20%28MPC%29%2C%20in%20realistic%20and%0Achallenging%20scenarios.%20Our%20contributions%20include%20the%20development%20of%20this%0Asimulation%20platform%2C%20several%20state-of-the-art%20algorithms%20tailored%20to%20the%20racing%0Aenvironment%2C%20and%20a%20comprehensive%20dataset%20collected%20from%20human%20drivers.%0AAdditionally%2C%20we%20evaluate%20algorithms%20in%20the%20offline%20RL%20setting.%20All%20the%0Anecessary%20code%20%28including%20environment%20and%20benchmarks%29%2C%20working%20examples%2C%0Adatasets%2C%20and%20videos%20are%20publicly%20released%20and%20can%20be%20found%20at%3A%0A%5Curl%7Bhttps%3A//assetto-corsa-gym.github.io%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simulation%2520Benchmark%2520for%2520Autonomous%2520Racing%2520with%2520Large-Scale%2520Human%2520Data%26entry.906535625%3DAdrian%2520Remonda%2520and%2520Nicklas%2520Hansen%2520and%2520Ayoub%2520Raji%2520and%2520Nicola%2520Musiu%2520and%2520Marko%2520Bertogna%2520and%2520Eduardo%2520Veas%2520and%2520Xiaolong%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520the%2520availability%2520of%2520international%2520prize-money%2520competitions%252C%2520scaled%250Avehicles%252C%2520and%2520simulation%2520environments%252C%2520research%2520on%2520autonomous%2520racing%2520and%2520the%250Acontrol%2520of%2520sports%2520cars%2520operating%2520close%2520to%2520the%2520limit%2520of%2520handling%2520has%2520been%250Alimited%2520by%2520the%2520high%2520costs%2520of%2520vehicle%2520acquisition%2520and%2520management%252C%2520as%2520well%2520as%2520the%250Alimited%2520physics%2520accuracy%2520of%2520open-source%2520simulators.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Aracing%2520simulation%2520platform%2520based%2520on%2520the%2520simulator%2520Assetto%2520Corsa%2520to%2520test%252C%250Avalidate%252C%2520and%2520benchmark%2520autonomous%2520driving%2520algorithms%252C%2520including%2520reinforcement%250Alearning%2520%2528RL%2529%2520and%2520classical%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%252C%2520in%2520realistic%2520and%250Achallenging%2520scenarios.%2520Our%2520contributions%2520include%2520the%2520development%2520of%2520this%250Asimulation%2520platform%252C%2520several%2520state-of-the-art%2520algorithms%2520tailored%2520to%2520the%2520racing%250Aenvironment%252C%2520and%2520a%2520comprehensive%2520dataset%2520collected%2520from%2520human%2520drivers.%250AAdditionally%252C%2520we%2520evaluate%2520algorithms%2520in%2520the%2520offline%2520RL%2520setting.%2520All%2520the%250Anecessary%2520code%2520%2528including%2520environment%2520and%2520benchmarks%2529%252C%2520working%2520examples%252C%250Adatasets%252C%2520and%2520videos%2520are%2520publicly%2520released%2520and%2520can%2520be%2520found%2520at%253A%250A%255Curl%257Bhttps%253A//assetto-corsa-gym.github.io%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simulation%20Benchmark%20for%20Autonomous%20Racing%20with%20Large-Scale%20Human%20Data&entry.906535625=Adrian%20Remonda%20and%20Nicklas%20Hansen%20and%20Ayoub%20Raji%20and%20Nicola%20Musiu%20and%20Marko%20Bertogna%20and%20Eduardo%20Veas%20and%20Xiaolong%20Wang&entry.1292438233=%20%20Despite%20the%20availability%20of%20international%20prize-money%20competitions%2C%20scaled%0Avehicles%2C%20and%20simulation%20environments%2C%20research%20on%20autonomous%20racing%20and%20the%0Acontrol%20of%20sports%20cars%20operating%20close%20to%20the%20limit%20of%20handling%20has%20been%0Alimited%20by%20the%20high%20costs%20of%20vehicle%20acquisition%20and%20management%2C%20as%20well%20as%20the%0Alimited%20physics%20accuracy%20of%20open-source%20simulators.%20In%20this%20paper%2C%20we%20propose%20a%0Aracing%20simulation%20platform%20based%20on%20the%20simulator%20Assetto%20Corsa%20to%20test%2C%0Avalidate%2C%20and%20benchmark%20autonomous%20driving%20algorithms%2C%20including%20reinforcement%0Alearning%20%28RL%29%20and%20classical%20Model%20Predictive%20Control%20%28MPC%29%2C%20in%20realistic%20and%0Achallenging%20scenarios.%20Our%20contributions%20include%20the%20development%20of%20this%0Asimulation%20platform%2C%20several%20state-of-the-art%20algorithms%20tailored%20to%20the%20racing%0Aenvironment%2C%20and%20a%20comprehensive%20dataset%20collected%20from%20human%20drivers.%0AAdditionally%2C%20we%20evaluate%20algorithms%20in%20the%20offline%20RL%20setting.%20All%20the%0Anecessary%20code%20%28including%20environment%20and%20benchmarks%29%2C%20working%20examples%2C%0Adatasets%2C%20and%20videos%20are%20publicly%20released%20and%20can%20be%20found%20at%3A%0A%5Curl%7Bhttps%3A//assetto-corsa-gym.github.io%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16680v1&entry.124074799=Read"},
{"title": "A Task is Worth One Word: Learning with Task Prompts for High-Quality\n  Versatile Image Inpainting", "author": "Junhao Zhuang and Yanhong Zeng and Wenran Liu and Chun Yuan and Kai Chen", "abstract": "  Advancing image inpainting is challenging as it requires filling\nuser-specified regions for various intents, such as background filling and\nobject synthesis. Existing approaches focus on either context-aware filling or\nobject synthesis using text descriptions. However, achieving both tasks\nsimultaneously is challenging due to differing training strategies. To overcome\nthis challenge, we introduce PowerPaint, the first high-quality and versatile\ninpainting model that excels in multiple inpainting tasks. First, we introduce\nlearnable task prompts along with tailored fine-tuning strategies to guide the\nmodel's focus on different inpainting targets explicitly. This enables\nPowerPaint to accomplish various inpainting tasks by utilizing different task\nprompts, resulting in state-of-the-art performance. Second, we demonstrate the\nversatility of the task prompt in PowerPaint by showcasing its effectiveness as\na negative prompt for object removal. Moreover, we leverage prompt\ninterpolation techniques to enable controllable shape-guided object inpainting,\nenhancing the model's applicability in shape-guided applications. Finally, we\nconduct extensive experiments and applications to verify the effectiveness of\nPowerPaint. We release our codes and models on our project page:\nhttps://powerpaint.github.io/.\n", "link": "http://arxiv.org/abs/2312.03594v4", "date": "2024-07-23", "relevancy": 2.0143, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.506}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5028}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Task%20is%20Worth%20One%20Word%3A%20Learning%20with%20Task%20Prompts%20for%20High-Quality%0A%20%20Versatile%20Image%20Inpainting&body=Title%3A%20A%20Task%20is%20Worth%20One%20Word%3A%20Learning%20with%20Task%20Prompts%20for%20High-Quality%0A%20%20Versatile%20Image%20Inpainting%0AAuthor%3A%20Junhao%20Zhuang%20and%20Yanhong%20Zeng%20and%20Wenran%20Liu%20and%20Chun%20Yuan%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Advancing%20image%20inpainting%20is%20challenging%20as%20it%20requires%20filling%0Auser-specified%20regions%20for%20various%20intents%2C%20such%20as%20background%20filling%20and%0Aobject%20synthesis.%20Existing%20approaches%20focus%20on%20either%20context-aware%20filling%20or%0Aobject%20synthesis%20using%20text%20descriptions.%20However%2C%20achieving%20both%20tasks%0Asimultaneously%20is%20challenging%20due%20to%20differing%20training%20strategies.%20To%20overcome%0Athis%20challenge%2C%20we%20introduce%20PowerPaint%2C%20the%20first%20high-quality%20and%20versatile%0Ainpainting%20model%20that%20excels%20in%20multiple%20inpainting%20tasks.%20First%2C%20we%20introduce%0Alearnable%20task%20prompts%20along%20with%20tailored%20fine-tuning%20strategies%20to%20guide%20the%0Amodel%27s%20focus%20on%20different%20inpainting%20targets%20explicitly.%20This%20enables%0APowerPaint%20to%20accomplish%20various%20inpainting%20tasks%20by%20utilizing%20different%20task%0Aprompts%2C%20resulting%20in%20state-of-the-art%20performance.%20Second%2C%20we%20demonstrate%20the%0Aversatility%20of%20the%20task%20prompt%20in%20PowerPaint%20by%20showcasing%20its%20effectiveness%20as%0Aa%20negative%20prompt%20for%20object%20removal.%20Moreover%2C%20we%20leverage%20prompt%0Ainterpolation%20techniques%20to%20enable%20controllable%20shape-guided%20object%20inpainting%2C%0Aenhancing%20the%20model%27s%20applicability%20in%20shape-guided%20applications.%20Finally%2C%20we%0Aconduct%20extensive%20experiments%20and%20applications%20to%20verify%20the%20effectiveness%20of%0APowerPaint.%20We%20release%20our%20codes%20and%20models%20on%20our%20project%20page%3A%0Ahttps%3A//powerpaint.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03594v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Task%2520is%2520Worth%2520One%2520Word%253A%2520Learning%2520with%2520Task%2520Prompts%2520for%2520High-Quality%250A%2520%2520Versatile%2520Image%2520Inpainting%26entry.906535625%3DJunhao%2520Zhuang%2520and%2520Yanhong%2520Zeng%2520and%2520Wenran%2520Liu%2520and%2520Chun%2520Yuan%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Advancing%2520image%2520inpainting%2520is%2520challenging%2520as%2520it%2520requires%2520filling%250Auser-specified%2520regions%2520for%2520various%2520intents%252C%2520such%2520as%2520background%2520filling%2520and%250Aobject%2520synthesis.%2520Existing%2520approaches%2520focus%2520on%2520either%2520context-aware%2520filling%2520or%250Aobject%2520synthesis%2520using%2520text%2520descriptions.%2520However%252C%2520achieving%2520both%2520tasks%250Asimultaneously%2520is%2520challenging%2520due%2520to%2520differing%2520training%2520strategies.%2520To%2520overcome%250Athis%2520challenge%252C%2520we%2520introduce%2520PowerPaint%252C%2520the%2520first%2520high-quality%2520and%2520versatile%250Ainpainting%2520model%2520that%2520excels%2520in%2520multiple%2520inpainting%2520tasks.%2520First%252C%2520we%2520introduce%250Alearnable%2520task%2520prompts%2520along%2520with%2520tailored%2520fine-tuning%2520strategies%2520to%2520guide%2520the%250Amodel%2527s%2520focus%2520on%2520different%2520inpainting%2520targets%2520explicitly.%2520This%2520enables%250APowerPaint%2520to%2520accomplish%2520various%2520inpainting%2520tasks%2520by%2520utilizing%2520different%2520task%250Aprompts%252C%2520resulting%2520in%2520state-of-the-art%2520performance.%2520Second%252C%2520we%2520demonstrate%2520the%250Aversatility%2520of%2520the%2520task%2520prompt%2520in%2520PowerPaint%2520by%2520showcasing%2520its%2520effectiveness%2520as%250Aa%2520negative%2520prompt%2520for%2520object%2520removal.%2520Moreover%252C%2520we%2520leverage%2520prompt%250Ainterpolation%2520techniques%2520to%2520enable%2520controllable%2520shape-guided%2520object%2520inpainting%252C%250Aenhancing%2520the%2520model%2527s%2520applicability%2520in%2520shape-guided%2520applications.%2520Finally%252C%2520we%250Aconduct%2520extensive%2520experiments%2520and%2520applications%2520to%2520verify%2520the%2520effectiveness%2520of%250APowerPaint.%2520We%2520release%2520our%2520codes%2520and%2520models%2520on%2520our%2520project%2520page%253A%250Ahttps%253A//powerpaint.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03594v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Task%20is%20Worth%20One%20Word%3A%20Learning%20with%20Task%20Prompts%20for%20High-Quality%0A%20%20Versatile%20Image%20Inpainting&entry.906535625=Junhao%20Zhuang%20and%20Yanhong%20Zeng%20and%20Wenran%20Liu%20and%20Chun%20Yuan%20and%20Kai%20Chen&entry.1292438233=%20%20Advancing%20image%20inpainting%20is%20challenging%20as%20it%20requires%20filling%0Auser-specified%20regions%20for%20various%20intents%2C%20such%20as%20background%20filling%20and%0Aobject%20synthesis.%20Existing%20approaches%20focus%20on%20either%20context-aware%20filling%20or%0Aobject%20synthesis%20using%20text%20descriptions.%20However%2C%20achieving%20both%20tasks%0Asimultaneously%20is%20challenging%20due%20to%20differing%20training%20strategies.%20To%20overcome%0Athis%20challenge%2C%20we%20introduce%20PowerPaint%2C%20the%20first%20high-quality%20and%20versatile%0Ainpainting%20model%20that%20excels%20in%20multiple%20inpainting%20tasks.%20First%2C%20we%20introduce%0Alearnable%20task%20prompts%20along%20with%20tailored%20fine-tuning%20strategies%20to%20guide%20the%0Amodel%27s%20focus%20on%20different%20inpainting%20targets%20explicitly.%20This%20enables%0APowerPaint%20to%20accomplish%20various%20inpainting%20tasks%20by%20utilizing%20different%20task%0Aprompts%2C%20resulting%20in%20state-of-the-art%20performance.%20Second%2C%20we%20demonstrate%20the%0Aversatility%20of%20the%20task%20prompt%20in%20PowerPaint%20by%20showcasing%20its%20effectiveness%20as%0Aa%20negative%20prompt%20for%20object%20removal.%20Moreover%2C%20we%20leverage%20prompt%0Ainterpolation%20techniques%20to%20enable%20controllable%20shape-guided%20object%20inpainting%2C%0Aenhancing%20the%20model%27s%20applicability%20in%20shape-guided%20applications.%20Finally%2C%20we%0Aconduct%20extensive%20experiments%20and%20applications%20to%20verify%20the%20effectiveness%20of%0APowerPaint.%20We%20release%20our%20codes%20and%20models%20on%20our%20project%20page%3A%0Ahttps%3A//powerpaint.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03594v4&entry.124074799=Read"},
{"title": "COALA: A Practical and Vision-Centric Federated Learning Platform", "author": "Weiming Zhuang and Jian Xu and Chen Chen and Jingtao Li and Lingjuan Lyu", "abstract": "  We present COALA, a vision-centric Federated Learning (FL) platform, and a\nsuite of benchmarks for practical FL scenarios, which we categorize into three\nlevels: task, data, and model. At the task level, COALA extends support from\nsimple classification to 15 computer vision tasks, including object detection,\nsegmentation, pose estimation, and more. It also facilitates federated\nmultiple-task learning, allowing clients to tackle multiple tasks\nsimultaneously. At the data level, COALA goes beyond supervised FL to benchmark\nboth semi-supervised FL and unsupervised FL. It also benchmarks feature\ndistribution shifts other than commonly considered label distribution shifts.\nIn addition to dealing with static data, it supports federated continual\nlearning for continuously changing data in real-world scenarios. At the model\nlevel, COALA benchmarks FL with split models and different models in different\nclients. COALA platform offers three degrees of customization for these\npractical FL scenarios, including configuration customization, components\ncustomization, and workflow customization. We conduct systematic benchmarking\nexperiments for the practical FL scenarios and highlight potential\nopportunities for further advancements in FL. Codes are open sourced at\nhttps://github.com/SonyResearch/COALA.\n", "link": "http://arxiv.org/abs/2407.16560v1", "date": "2024-07-23", "relevancy": 2.0071, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5097}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.497}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COALA%3A%20A%20Practical%20and%20Vision-Centric%20Federated%20Learning%20Platform&body=Title%3A%20COALA%3A%20A%20Practical%20and%20Vision-Centric%20Federated%20Learning%20Platform%0AAuthor%3A%20Weiming%20Zhuang%20and%20Jian%20Xu%20and%20Chen%20Chen%20and%20Jingtao%20Li%20and%20Lingjuan%20Lyu%0AAbstract%3A%20%20%20We%20present%20COALA%2C%20a%20vision-centric%20Federated%20Learning%20%28FL%29%20platform%2C%20and%20a%0Asuite%20of%20benchmarks%20for%20practical%20FL%20scenarios%2C%20which%20we%20categorize%20into%20three%0Alevels%3A%20task%2C%20data%2C%20and%20model.%20At%20the%20task%20level%2C%20COALA%20extends%20support%20from%0Asimple%20classification%20to%2015%20computer%20vision%20tasks%2C%20including%20object%20detection%2C%0Asegmentation%2C%20pose%20estimation%2C%20and%20more.%20It%20also%20facilitates%20federated%0Amultiple-task%20learning%2C%20allowing%20clients%20to%20tackle%20multiple%20tasks%0Asimultaneously.%20At%20the%20data%20level%2C%20COALA%20goes%20beyond%20supervised%20FL%20to%20benchmark%0Aboth%20semi-supervised%20FL%20and%20unsupervised%20FL.%20It%20also%20benchmarks%20feature%0Adistribution%20shifts%20other%20than%20commonly%20considered%20label%20distribution%20shifts.%0AIn%20addition%20to%20dealing%20with%20static%20data%2C%20it%20supports%20federated%20continual%0Alearning%20for%20continuously%20changing%20data%20in%20real-world%20scenarios.%20At%20the%20model%0Alevel%2C%20COALA%20benchmarks%20FL%20with%20split%20models%20and%20different%20models%20in%20different%0Aclients.%20COALA%20platform%20offers%20three%20degrees%20of%20customization%20for%20these%0Apractical%20FL%20scenarios%2C%20including%20configuration%20customization%2C%20components%0Acustomization%2C%20and%20workflow%20customization.%20We%20conduct%20systematic%20benchmarking%0Aexperiments%20for%20the%20practical%20FL%20scenarios%20and%20highlight%20potential%0Aopportunities%20for%20further%20advancements%20in%20FL.%20Codes%20are%20open%20sourced%20at%0Ahttps%3A//github.com/SonyResearch/COALA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOALA%253A%2520A%2520Practical%2520and%2520Vision-Centric%2520Federated%2520Learning%2520Platform%26entry.906535625%3DWeiming%2520Zhuang%2520and%2520Jian%2520Xu%2520and%2520Chen%2520Chen%2520and%2520Jingtao%2520Li%2520and%2520Lingjuan%2520Lyu%26entry.1292438233%3D%2520%2520We%2520present%2520COALA%252C%2520a%2520vision-centric%2520Federated%2520Learning%2520%2528FL%2529%2520platform%252C%2520and%2520a%250Asuite%2520of%2520benchmarks%2520for%2520practical%2520FL%2520scenarios%252C%2520which%2520we%2520categorize%2520into%2520three%250Alevels%253A%2520task%252C%2520data%252C%2520and%2520model.%2520At%2520the%2520task%2520level%252C%2520COALA%2520extends%2520support%2520from%250Asimple%2520classification%2520to%252015%2520computer%2520vision%2520tasks%252C%2520including%2520object%2520detection%252C%250Asegmentation%252C%2520pose%2520estimation%252C%2520and%2520more.%2520It%2520also%2520facilitates%2520federated%250Amultiple-task%2520learning%252C%2520allowing%2520clients%2520to%2520tackle%2520multiple%2520tasks%250Asimultaneously.%2520At%2520the%2520data%2520level%252C%2520COALA%2520goes%2520beyond%2520supervised%2520FL%2520to%2520benchmark%250Aboth%2520semi-supervised%2520FL%2520and%2520unsupervised%2520FL.%2520It%2520also%2520benchmarks%2520feature%250Adistribution%2520shifts%2520other%2520than%2520commonly%2520considered%2520label%2520distribution%2520shifts.%250AIn%2520addition%2520to%2520dealing%2520with%2520static%2520data%252C%2520it%2520supports%2520federated%2520continual%250Alearning%2520for%2520continuously%2520changing%2520data%2520in%2520real-world%2520scenarios.%2520At%2520the%2520model%250Alevel%252C%2520COALA%2520benchmarks%2520FL%2520with%2520split%2520models%2520and%2520different%2520models%2520in%2520different%250Aclients.%2520COALA%2520platform%2520offers%2520three%2520degrees%2520of%2520customization%2520for%2520these%250Apractical%2520FL%2520scenarios%252C%2520including%2520configuration%2520customization%252C%2520components%250Acustomization%252C%2520and%2520workflow%2520customization.%2520We%2520conduct%2520systematic%2520benchmarking%250Aexperiments%2520for%2520the%2520practical%2520FL%2520scenarios%2520and%2520highlight%2520potential%250Aopportunities%2520for%2520further%2520advancements%2520in%2520FL.%2520Codes%2520are%2520open%2520sourced%2520at%250Ahttps%253A//github.com/SonyResearch/COALA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COALA%3A%20A%20Practical%20and%20Vision-Centric%20Federated%20Learning%20Platform&entry.906535625=Weiming%20Zhuang%20and%20Jian%20Xu%20and%20Chen%20Chen%20and%20Jingtao%20Li%20and%20Lingjuan%20Lyu&entry.1292438233=%20%20We%20present%20COALA%2C%20a%20vision-centric%20Federated%20Learning%20%28FL%29%20platform%2C%20and%20a%0Asuite%20of%20benchmarks%20for%20practical%20FL%20scenarios%2C%20which%20we%20categorize%20into%20three%0Alevels%3A%20task%2C%20data%2C%20and%20model.%20At%20the%20task%20level%2C%20COALA%20extends%20support%20from%0Asimple%20classification%20to%2015%20computer%20vision%20tasks%2C%20including%20object%20detection%2C%0Asegmentation%2C%20pose%20estimation%2C%20and%20more.%20It%20also%20facilitates%20federated%0Amultiple-task%20learning%2C%20allowing%20clients%20to%20tackle%20multiple%20tasks%0Asimultaneously.%20At%20the%20data%20level%2C%20COALA%20goes%20beyond%20supervised%20FL%20to%20benchmark%0Aboth%20semi-supervised%20FL%20and%20unsupervised%20FL.%20It%20also%20benchmarks%20feature%0Adistribution%20shifts%20other%20than%20commonly%20considered%20label%20distribution%20shifts.%0AIn%20addition%20to%20dealing%20with%20static%20data%2C%20it%20supports%20federated%20continual%0Alearning%20for%20continuously%20changing%20data%20in%20real-world%20scenarios.%20At%20the%20model%0Alevel%2C%20COALA%20benchmarks%20FL%20with%20split%20models%20and%20different%20models%20in%20different%0Aclients.%20COALA%20platform%20offers%20three%20degrees%20of%20customization%20for%20these%0Apractical%20FL%20scenarios%2C%20including%20configuration%20customization%2C%20components%0Acustomization%2C%20and%20workflow%20customization.%20We%20conduct%20systematic%20benchmarking%0Aexperiments%20for%20the%20practical%20FL%20scenarios%20and%20highlight%20potential%0Aopportunities%20for%20further%20advancements%20in%20FL.%20Codes%20are%20open%20sourced%20at%0Ahttps%3A//github.com/SonyResearch/COALA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16560v1&entry.124074799=Read"},
{"title": "SAM-CP: Marrying SAM with Composable Prompts for Versatile Segmentation", "author": "Pengfei Chen and Lingxi Xie and Xinyue Huo and Xuehui Yu and Xiaopeng Zhang and Yingfei Sun and Zhenjun Han and Qi Tian", "abstract": "  The Segment Anything model (SAM) has shown a generalized ability to group\nimage pixels into patches, but applying it to semantic-aware segmentation still\nfaces major challenges. This paper presents SAM-CP, a simple approach that\nestablishes two types of composable prompts beyond SAM and composes them for\nversatile segmentation. Specifically, given a set of classes (in texts) and a\nset of SAM patches, the Type-I prompt judges whether a SAM patch aligns with a\ntext label, and the Type-II prompt judges whether two SAM patches with the same\ntext label also belong to the same instance. To decrease the complexity in\ndealing with a large number of semantic classes and patches, we establish a\nunified framework that calculates the affinity between (semantic and instance)\nqueries and SAM patches and merges patches with high affinity to the query.\nExperiments show that SAM-CP achieves semantic, instance, and panoptic\nsegmentation in both open and closed domains. In particular, it achieves\nstate-of-the-art performance in open-vocabulary segmentation. Our research\noffers a novel and generalized methodology for equipping vision foundation\nmodels like SAM with multi-grained semantic perception abilities.\n", "link": "http://arxiv.org/abs/2407.16682v1", "date": "2024-07-23", "relevancy": 2.0009, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5265}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4902}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM-CP%3A%20Marrying%20SAM%20with%20Composable%20Prompts%20for%20Versatile%20Segmentation&body=Title%3A%20SAM-CP%3A%20Marrying%20SAM%20with%20Composable%20Prompts%20for%20Versatile%20Segmentation%0AAuthor%3A%20Pengfei%20Chen%20and%20Lingxi%20Xie%20and%20Xinyue%20Huo%20and%20Xuehui%20Yu%20and%20Xiaopeng%20Zhang%20and%20Yingfei%20Sun%20and%20Zhenjun%20Han%20and%20Qi%20Tian%0AAbstract%3A%20%20%20The%20Segment%20Anything%20model%20%28SAM%29%20has%20shown%20a%20generalized%20ability%20to%20group%0Aimage%20pixels%20into%20patches%2C%20but%20applying%20it%20to%20semantic-aware%20segmentation%20still%0Afaces%20major%20challenges.%20This%20paper%20presents%20SAM-CP%2C%20a%20simple%20approach%20that%0Aestablishes%20two%20types%20of%20composable%20prompts%20beyond%20SAM%20and%20composes%20them%20for%0Aversatile%20segmentation.%20Specifically%2C%20given%20a%20set%20of%20classes%20%28in%20texts%29%20and%20a%0Aset%20of%20SAM%20patches%2C%20the%20Type-I%20prompt%20judges%20whether%20a%20SAM%20patch%20aligns%20with%20a%0Atext%20label%2C%20and%20the%20Type-II%20prompt%20judges%20whether%20two%20SAM%20patches%20with%20the%20same%0Atext%20label%20also%20belong%20to%20the%20same%20instance.%20To%20decrease%20the%20complexity%20in%0Adealing%20with%20a%20large%20number%20of%20semantic%20classes%20and%20patches%2C%20we%20establish%20a%0Aunified%20framework%20that%20calculates%20the%20affinity%20between%20%28semantic%20and%20instance%29%0Aqueries%20and%20SAM%20patches%20and%20merges%20patches%20with%20high%20affinity%20to%20the%20query.%0AExperiments%20show%20that%20SAM-CP%20achieves%20semantic%2C%20instance%2C%20and%20panoptic%0Asegmentation%20in%20both%20open%20and%20closed%20domains.%20In%20particular%2C%20it%20achieves%0Astate-of-the-art%20performance%20in%20open-vocabulary%20segmentation.%20Our%20research%0Aoffers%20a%20novel%20and%20generalized%20methodology%20for%20equipping%20vision%20foundation%0Amodels%20like%20SAM%20with%20multi-grained%20semantic%20perception%20abilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM-CP%253A%2520Marrying%2520SAM%2520with%2520Composable%2520Prompts%2520for%2520Versatile%2520Segmentation%26entry.906535625%3DPengfei%2520Chen%2520and%2520Lingxi%2520Xie%2520and%2520Xinyue%2520Huo%2520and%2520Xuehui%2520Yu%2520and%2520Xiaopeng%2520Zhang%2520and%2520Yingfei%2520Sun%2520and%2520Zhenjun%2520Han%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520The%2520Segment%2520Anything%2520model%2520%2528SAM%2529%2520has%2520shown%2520a%2520generalized%2520ability%2520to%2520group%250Aimage%2520pixels%2520into%2520patches%252C%2520but%2520applying%2520it%2520to%2520semantic-aware%2520segmentation%2520still%250Afaces%2520major%2520challenges.%2520This%2520paper%2520presents%2520SAM-CP%252C%2520a%2520simple%2520approach%2520that%250Aestablishes%2520two%2520types%2520of%2520composable%2520prompts%2520beyond%2520SAM%2520and%2520composes%2520them%2520for%250Aversatile%2520segmentation.%2520Specifically%252C%2520given%2520a%2520set%2520of%2520classes%2520%2528in%2520texts%2529%2520and%2520a%250Aset%2520of%2520SAM%2520patches%252C%2520the%2520Type-I%2520prompt%2520judges%2520whether%2520a%2520SAM%2520patch%2520aligns%2520with%2520a%250Atext%2520label%252C%2520and%2520the%2520Type-II%2520prompt%2520judges%2520whether%2520two%2520SAM%2520patches%2520with%2520the%2520same%250Atext%2520label%2520also%2520belong%2520to%2520the%2520same%2520instance.%2520To%2520decrease%2520the%2520complexity%2520in%250Adealing%2520with%2520a%2520large%2520number%2520of%2520semantic%2520classes%2520and%2520patches%252C%2520we%2520establish%2520a%250Aunified%2520framework%2520that%2520calculates%2520the%2520affinity%2520between%2520%2528semantic%2520and%2520instance%2529%250Aqueries%2520and%2520SAM%2520patches%2520and%2520merges%2520patches%2520with%2520high%2520affinity%2520to%2520the%2520query.%250AExperiments%2520show%2520that%2520SAM-CP%2520achieves%2520semantic%252C%2520instance%252C%2520and%2520panoptic%250Asegmentation%2520in%2520both%2520open%2520and%2520closed%2520domains.%2520In%2520particular%252C%2520it%2520achieves%250Astate-of-the-art%2520performance%2520in%2520open-vocabulary%2520segmentation.%2520Our%2520research%250Aoffers%2520a%2520novel%2520and%2520generalized%2520methodology%2520for%2520equipping%2520vision%2520foundation%250Amodels%2520like%2520SAM%2520with%2520multi-grained%2520semantic%2520perception%2520abilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-CP%3A%20Marrying%20SAM%20with%20Composable%20Prompts%20for%20Versatile%20Segmentation&entry.906535625=Pengfei%20Chen%20and%20Lingxi%20Xie%20and%20Xinyue%20Huo%20and%20Xuehui%20Yu%20and%20Xiaopeng%20Zhang%20and%20Yingfei%20Sun%20and%20Zhenjun%20Han%20and%20Qi%20Tian&entry.1292438233=%20%20The%20Segment%20Anything%20model%20%28SAM%29%20has%20shown%20a%20generalized%20ability%20to%20group%0Aimage%20pixels%20into%20patches%2C%20but%20applying%20it%20to%20semantic-aware%20segmentation%20still%0Afaces%20major%20challenges.%20This%20paper%20presents%20SAM-CP%2C%20a%20simple%20approach%20that%0Aestablishes%20two%20types%20of%20composable%20prompts%20beyond%20SAM%20and%20composes%20them%20for%0Aversatile%20segmentation.%20Specifically%2C%20given%20a%20set%20of%20classes%20%28in%20texts%29%20and%20a%0Aset%20of%20SAM%20patches%2C%20the%20Type-I%20prompt%20judges%20whether%20a%20SAM%20patch%20aligns%20with%20a%0Atext%20label%2C%20and%20the%20Type-II%20prompt%20judges%20whether%20two%20SAM%20patches%20with%20the%20same%0Atext%20label%20also%20belong%20to%20the%20same%20instance.%20To%20decrease%20the%20complexity%20in%0Adealing%20with%20a%20large%20number%20of%20semantic%20classes%20and%20patches%2C%20we%20establish%20a%0Aunified%20framework%20that%20calculates%20the%20affinity%20between%20%28semantic%20and%20instance%29%0Aqueries%20and%20SAM%20patches%20and%20merges%20patches%20with%20high%20affinity%20to%20the%20query.%0AExperiments%20show%20that%20SAM-CP%20achieves%20semantic%2C%20instance%2C%20and%20panoptic%0Asegmentation%20in%20both%20open%20and%20closed%20domains.%20In%20particular%2C%20it%20achieves%0Astate-of-the-art%20performance%20in%20open-vocabulary%20segmentation.%20Our%20research%0Aoffers%20a%20novel%20and%20generalized%20methodology%20for%20equipping%20vision%20foundation%0Amodels%20like%20SAM%20with%20multi-grained%20semantic%20perception%20abilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16682v1&entry.124074799=Read"},
{"title": "Adapting Multilingual LLMs to Low-Resource Languages with Knowledge\n  Graphs via Adapters", "author": "Daniil Gurgurov and Mareike Hartmann and Simon Ostermann", "abstract": "  This paper explores the integration of graph knowledge from linguistic\nontologies into multilingual Large Language Models (LLMs) using adapters to\nimprove performance for low-resource languages (LRLs) in sentiment analysis\n(SA) and named entity recognition (NER). Building upon successful\nparameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we\npropose a similar approach for incorporating knowledge from multilingual\ngraphs, connecting concepts in various languages with each other through\nlinguistic relationships, into multilingual LLMs for LRLs. Specifically, we\nfocus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,\nUyghur, Tibetan, and Sinhala -- and employ language-specific adapters\nfine-tuned on data extracted from the language-specific section of ConceptNet,\naiming to enable knowledge transfer across the languages covered by the\nknowledge graph. We compare various fine-tuning objectives, including standard\nMasked Language Modeling (MLM), MLM with full-word masking, and MLM with\ntargeted masking, to analyse their effectiveness in learning and integrating\nthe extracted graph data. Through empirical evaluation on language-specific\ntasks, we assess how structured graph knowledge affects the performance of\nmultilingual LLMs for LRLs in SA and NER, providing insights into the potential\nbenefits of adapting language models for low-resource scenarios.\n", "link": "http://arxiv.org/abs/2407.01406v2", "date": "2024-07-23", "relevancy": 1.9995, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4715}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Multilingual%20LLMs%20to%20Low-Resource%20Languages%20with%20Knowledge%0A%20%20Graphs%20via%20Adapters&body=Title%3A%20Adapting%20Multilingual%20LLMs%20to%20Low-Resource%20Languages%20with%20Knowledge%0A%20%20Graphs%20via%20Adapters%0AAuthor%3A%20Daniil%20Gurgurov%20and%20Mareike%20Hartmann%20and%20Simon%20Ostermann%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20integration%20of%20graph%20knowledge%20from%20linguistic%0Aontologies%20into%20multilingual%20Large%20Language%20Models%20%28LLMs%29%20using%20adapters%20to%0Aimprove%20performance%20for%20low-resource%20languages%20%28LRLs%29%20in%20sentiment%20analysis%0A%28SA%29%20and%20named%20entity%20recognition%20%28NER%29.%20Building%20upon%20successful%0Aparameter-efficient%20fine-tuning%20techniques%2C%20such%20as%20K-ADAPTER%20and%20MAD-X%2C%20we%0Apropose%20a%20similar%20approach%20for%20incorporating%20knowledge%20from%20multilingual%0Agraphs%2C%20connecting%20concepts%20in%20various%20languages%20with%20each%20other%20through%0Alinguistic%20relationships%2C%20into%20multilingual%20LLMs%20for%20LRLs.%20Specifically%2C%20we%0Afocus%20on%20eight%20LRLs%20--%20Maltese%2C%20Bulgarian%2C%20Indonesian%2C%20Nepali%2C%20Javanese%2C%0AUyghur%2C%20Tibetan%2C%20and%20Sinhala%20--%20and%20employ%20language-specific%20adapters%0Afine-tuned%20on%20data%20extracted%20from%20the%20language-specific%20section%20of%20ConceptNet%2C%0Aaiming%20to%20enable%20knowledge%20transfer%20across%20the%20languages%20covered%20by%20the%0Aknowledge%20graph.%20We%20compare%20various%20fine-tuning%20objectives%2C%20including%20standard%0AMasked%20Language%20Modeling%20%28MLM%29%2C%20MLM%20with%20full-word%20masking%2C%20and%20MLM%20with%0Atargeted%20masking%2C%20to%20analyse%20their%20effectiveness%20in%20learning%20and%20integrating%0Athe%20extracted%20graph%20data.%20Through%20empirical%20evaluation%20on%20language-specific%0Atasks%2C%20we%20assess%20how%20structured%20graph%20knowledge%20affects%20the%20performance%20of%0Amultilingual%20LLMs%20for%20LRLs%20in%20SA%20and%20NER%2C%20providing%20insights%20into%20the%20potential%0Abenefits%20of%20adapting%20language%20models%20for%20low-resource%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01406v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Multilingual%2520LLMs%2520to%2520Low-Resource%2520Languages%2520with%2520Knowledge%250A%2520%2520Graphs%2520via%2520Adapters%26entry.906535625%3DDaniil%2520Gurgurov%2520and%2520Mareike%2520Hartmann%2520and%2520Simon%2520Ostermann%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520integration%2520of%2520graph%2520knowledge%2520from%2520linguistic%250Aontologies%2520into%2520multilingual%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520using%2520adapters%2520to%250Aimprove%2520performance%2520for%2520low-resource%2520languages%2520%2528LRLs%2529%2520in%2520sentiment%2520analysis%250A%2528SA%2529%2520and%2520named%2520entity%2520recognition%2520%2528NER%2529.%2520Building%2520upon%2520successful%250Aparameter-efficient%2520fine-tuning%2520techniques%252C%2520such%2520as%2520K-ADAPTER%2520and%2520MAD-X%252C%2520we%250Apropose%2520a%2520similar%2520approach%2520for%2520incorporating%2520knowledge%2520from%2520multilingual%250Agraphs%252C%2520connecting%2520concepts%2520in%2520various%2520languages%2520with%2520each%2520other%2520through%250Alinguistic%2520relationships%252C%2520into%2520multilingual%2520LLMs%2520for%2520LRLs.%2520Specifically%252C%2520we%250Afocus%2520on%2520eight%2520LRLs%2520--%2520Maltese%252C%2520Bulgarian%252C%2520Indonesian%252C%2520Nepali%252C%2520Javanese%252C%250AUyghur%252C%2520Tibetan%252C%2520and%2520Sinhala%2520--%2520and%2520employ%2520language-specific%2520adapters%250Afine-tuned%2520on%2520data%2520extracted%2520from%2520the%2520language-specific%2520section%2520of%2520ConceptNet%252C%250Aaiming%2520to%2520enable%2520knowledge%2520transfer%2520across%2520the%2520languages%2520covered%2520by%2520the%250Aknowledge%2520graph.%2520We%2520compare%2520various%2520fine-tuning%2520objectives%252C%2520including%2520standard%250AMasked%2520Language%2520Modeling%2520%2528MLM%2529%252C%2520MLM%2520with%2520full-word%2520masking%252C%2520and%2520MLM%2520with%250Atargeted%2520masking%252C%2520to%2520analyse%2520their%2520effectiveness%2520in%2520learning%2520and%2520integrating%250Athe%2520extracted%2520graph%2520data.%2520Through%2520empirical%2520evaluation%2520on%2520language-specific%250Atasks%252C%2520we%2520assess%2520how%2520structured%2520graph%2520knowledge%2520affects%2520the%2520performance%2520of%250Amultilingual%2520LLMs%2520for%2520LRLs%2520in%2520SA%2520and%2520NER%252C%2520providing%2520insights%2520into%2520the%2520potential%250Abenefits%2520of%2520adapting%2520language%2520models%2520for%2520low-resource%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01406v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Multilingual%20LLMs%20to%20Low-Resource%20Languages%20with%20Knowledge%0A%20%20Graphs%20via%20Adapters&entry.906535625=Daniil%20Gurgurov%20and%20Mareike%20Hartmann%20and%20Simon%20Ostermann&entry.1292438233=%20%20This%20paper%20explores%20the%20integration%20of%20graph%20knowledge%20from%20linguistic%0Aontologies%20into%20multilingual%20Large%20Language%20Models%20%28LLMs%29%20using%20adapters%20to%0Aimprove%20performance%20for%20low-resource%20languages%20%28LRLs%29%20in%20sentiment%20analysis%0A%28SA%29%20and%20named%20entity%20recognition%20%28NER%29.%20Building%20upon%20successful%0Aparameter-efficient%20fine-tuning%20techniques%2C%20such%20as%20K-ADAPTER%20and%20MAD-X%2C%20we%0Apropose%20a%20similar%20approach%20for%20incorporating%20knowledge%20from%20multilingual%0Agraphs%2C%20connecting%20concepts%20in%20various%20languages%20with%20each%20other%20through%0Alinguistic%20relationships%2C%20into%20multilingual%20LLMs%20for%20LRLs.%20Specifically%2C%20we%0Afocus%20on%20eight%20LRLs%20--%20Maltese%2C%20Bulgarian%2C%20Indonesian%2C%20Nepali%2C%20Javanese%2C%0AUyghur%2C%20Tibetan%2C%20and%20Sinhala%20--%20and%20employ%20language-specific%20adapters%0Afine-tuned%20on%20data%20extracted%20from%20the%20language-specific%20section%20of%20ConceptNet%2C%0Aaiming%20to%20enable%20knowledge%20transfer%20across%20the%20languages%20covered%20by%20the%0Aknowledge%20graph.%20We%20compare%20various%20fine-tuning%20objectives%2C%20including%20standard%0AMasked%20Language%20Modeling%20%28MLM%29%2C%20MLM%20with%20full-word%20masking%2C%20and%20MLM%20with%0Atargeted%20masking%2C%20to%20analyse%20their%20effectiveness%20in%20learning%20and%20integrating%0Athe%20extracted%20graph%20data.%20Through%20empirical%20evaluation%20on%20language-specific%0Atasks%2C%20we%20assess%20how%20structured%20graph%20knowledge%20affects%20the%20performance%20of%0Amultilingual%20LLMs%20for%20LRLs%20in%20SA%20and%20NER%2C%20providing%20insights%20into%20the%20potential%0Abenefits%20of%20adapting%20language%20models%20for%20low-resource%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01406v2&entry.124074799=Read"},
{"title": "A Comprehensive Review of Knowledge Distillation in Computer Vision", "author": "Gousia Habib and Tausifa jan Saleem and Sheikh Musa Kaleem and Tufail Rouf and Brejesh Lall", "abstract": "  Deep learning techniques have been demonstrated to surpass preceding\ncutting-edge machine learning techniques in recent years, with computer vision\nbeing one of the most prominent examples. However, deep learning models suffer\nfrom significant drawbacks when deployed in resource-constrained environments\ndue to their large model size and high complexity. Knowledge Distillation is\none of the prominent solutions to overcome this challenge. This review paper\nexamines the current state of research on knowledge distillation, a technique\nfor compressing complex models into smaller and simpler ones. The paper\nprovides an overview of the major principles and techniques associated with\nknowledge distillation and reviews the applications of knowledge distillation\nin the domain of computer vision. The review focuses on the benefits of\nknowledge distillation, as well as the problems that must be overcome to\nimprove its effectiveness.\n", "link": "http://arxiv.org/abs/2404.00936v4", "date": "2024-07-23", "relevancy": 1.9813, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5045}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4988}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Review%20of%20Knowledge%20Distillation%20in%20Computer%20Vision&body=Title%3A%20A%20Comprehensive%20Review%20of%20Knowledge%20Distillation%20in%20Computer%20Vision%0AAuthor%3A%20Gousia%20Habib%20and%20Tausifa%20jan%20Saleem%20and%20Sheikh%20Musa%20Kaleem%20and%20Tufail%20Rouf%20and%20Brejesh%20Lall%0AAbstract%3A%20%20%20Deep%20learning%20techniques%20have%20been%20demonstrated%20to%20surpass%20preceding%0Acutting-edge%20machine%20learning%20techniques%20in%20recent%20years%2C%20with%20computer%20vision%0Abeing%20one%20of%20the%20most%20prominent%20examples.%20However%2C%20deep%20learning%20models%20suffer%0Afrom%20significant%20drawbacks%20when%20deployed%20in%20resource-constrained%20environments%0Adue%20to%20their%20large%20model%20size%20and%20high%20complexity.%20Knowledge%20Distillation%20is%0Aone%20of%20the%20prominent%20solutions%20to%20overcome%20this%20challenge.%20This%20review%20paper%0Aexamines%20the%20current%20state%20of%20research%20on%20knowledge%20distillation%2C%20a%20technique%0Afor%20compressing%20complex%20models%20into%20smaller%20and%20simpler%20ones.%20The%20paper%0Aprovides%20an%20overview%20of%20the%20major%20principles%20and%20techniques%20associated%20with%0Aknowledge%20distillation%20and%20reviews%20the%20applications%20of%20knowledge%20distillation%0Ain%20the%20domain%20of%20computer%20vision.%20The%20review%20focuses%20on%20the%20benefits%20of%0Aknowledge%20distillation%2C%20as%20well%20as%20the%20problems%20that%20must%20be%20overcome%20to%0Aimprove%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00936v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Review%2520of%2520Knowledge%2520Distillation%2520in%2520Computer%2520Vision%26entry.906535625%3DGousia%2520Habib%2520and%2520Tausifa%2520jan%2520Saleem%2520and%2520Sheikh%2520Musa%2520Kaleem%2520and%2520Tufail%2520Rouf%2520and%2520Brejesh%2520Lall%26entry.1292438233%3D%2520%2520Deep%2520learning%2520techniques%2520have%2520been%2520demonstrated%2520to%2520surpass%2520preceding%250Acutting-edge%2520machine%2520learning%2520techniques%2520in%2520recent%2520years%252C%2520with%2520computer%2520vision%250Abeing%2520one%2520of%2520the%2520most%2520prominent%2520examples.%2520However%252C%2520deep%2520learning%2520models%2520suffer%250Afrom%2520significant%2520drawbacks%2520when%2520deployed%2520in%2520resource-constrained%2520environments%250Adue%2520to%2520their%2520large%2520model%2520size%2520and%2520high%2520complexity.%2520Knowledge%2520Distillation%2520is%250Aone%2520of%2520the%2520prominent%2520solutions%2520to%2520overcome%2520this%2520challenge.%2520This%2520review%2520paper%250Aexamines%2520the%2520current%2520state%2520of%2520research%2520on%2520knowledge%2520distillation%252C%2520a%2520technique%250Afor%2520compressing%2520complex%2520models%2520into%2520smaller%2520and%2520simpler%2520ones.%2520The%2520paper%250Aprovides%2520an%2520overview%2520of%2520the%2520major%2520principles%2520and%2520techniques%2520associated%2520with%250Aknowledge%2520distillation%2520and%2520reviews%2520the%2520applications%2520of%2520knowledge%2520distillation%250Ain%2520the%2520domain%2520of%2520computer%2520vision.%2520The%2520review%2520focuses%2520on%2520the%2520benefits%2520of%250Aknowledge%2520distillation%252C%2520as%2520well%2520as%2520the%2520problems%2520that%2520must%2520be%2520overcome%2520to%250Aimprove%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00936v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Review%20of%20Knowledge%20Distillation%20in%20Computer%20Vision&entry.906535625=Gousia%20Habib%20and%20Tausifa%20jan%20Saleem%20and%20Sheikh%20Musa%20Kaleem%20and%20Tufail%20Rouf%20and%20Brejesh%20Lall&entry.1292438233=%20%20Deep%20learning%20techniques%20have%20been%20demonstrated%20to%20surpass%20preceding%0Acutting-edge%20machine%20learning%20techniques%20in%20recent%20years%2C%20with%20computer%20vision%0Abeing%20one%20of%20the%20most%20prominent%20examples.%20However%2C%20deep%20learning%20models%20suffer%0Afrom%20significant%20drawbacks%20when%20deployed%20in%20resource-constrained%20environments%0Adue%20to%20their%20large%20model%20size%20and%20high%20complexity.%20Knowledge%20Distillation%20is%0Aone%20of%20the%20prominent%20solutions%20to%20overcome%20this%20challenge.%20This%20review%20paper%0Aexamines%20the%20current%20state%20of%20research%20on%20knowledge%20distillation%2C%20a%20technique%0Afor%20compressing%20complex%20models%20into%20smaller%20and%20simpler%20ones.%20The%20paper%0Aprovides%20an%20overview%20of%20the%20major%20principles%20and%20techniques%20associated%20with%0Aknowledge%20distillation%20and%20reviews%20the%20applications%20of%20knowledge%20distillation%0Ain%20the%20domain%20of%20computer%20vision.%20The%20review%20focuses%20on%20the%20benefits%20of%0Aknowledge%20distillation%2C%20as%20well%20as%20the%20problems%20that%20must%20be%20overcome%20to%0Aimprove%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00936v4&entry.124074799=Read"},
{"title": "AutoRG-Brain: Grounded Report Generation for Brain MRI", "author": "Jiayu Lei and Xiaoman Zhang and Chaoyi Wu and Lisong Dai and Ya Zhang and Yanyong Zhang and Yanfeng Wang and Weidi Xie and Yuehua Li", "abstract": "  Radiologists are tasked with interpreting a large number of images in a daily\nbase, with the responsibility of generating corresponding reports. This\ndemanding workload elevates the risk of human error, potentially leading to\ntreatment delays, increased healthcare costs, revenue loss, and operational\ninefficiencies. To address these challenges, we initiate a series of work on\ngrounded Automatic Report Generation (AutoRG), starting from the brain MRI\ninterpretation system, which supports the delineation of brain structures, the\nlocalization of anomalies, and the generation of well-organized findings. We\nmake contributions from the following aspects, first, on dataset construction,\nwe release a comprehensive dataset encompassing segmentation masks of anomaly\nregions and manually authored reports, termed as RadGenome-Brain MRI. This data\nresource is intended to catalyze ongoing research and development in the field\nof AI-assisted report generation systems. Second, on system design, we propose\nAutoRG-Brain, the first brain MRI report generation system with pixel-level\ngrounded visual clues. Third, for evaluation, we conduct quantitative\nassessments and human evaluations of brain structure segmentation, anomaly\nlocalization, and report generation tasks to provide evidence of its\nreliability and accuracy. This system has been integrated into real clinical\nscenarios, where radiologists were instructed to write reports based on our\ngenerated findings and anomaly segmentation masks. The results demonstrate that\nour system enhances the report-writing skills of junior doctors, aligning their\nperformance more closely with senior doctors, thereby boosting overall\nproductivity.\n", "link": "http://arxiv.org/abs/2407.16684v1", "date": "2024-07-23", "relevancy": 1.9783, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5156}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5096}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoRG-Brain%3A%20Grounded%20Report%20Generation%20for%20Brain%20MRI&body=Title%3A%20AutoRG-Brain%3A%20Grounded%20Report%20Generation%20for%20Brain%20MRI%0AAuthor%3A%20Jiayu%20Lei%20and%20Xiaoman%20Zhang%20and%20Chaoyi%20Wu%20and%20Lisong%20Dai%20and%20Ya%20Zhang%20and%20Yanyong%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie%20and%20Yuehua%20Li%0AAbstract%3A%20%20%20Radiologists%20are%20tasked%20with%20interpreting%20a%20large%20number%20of%20images%20in%20a%20daily%0Abase%2C%20with%20the%20responsibility%20of%20generating%20corresponding%20reports.%20This%0Ademanding%20workload%20elevates%20the%20risk%20of%20human%20error%2C%20potentially%20leading%20to%0Atreatment%20delays%2C%20increased%20healthcare%20costs%2C%20revenue%20loss%2C%20and%20operational%0Ainefficiencies.%20To%20address%20these%20challenges%2C%20we%20initiate%20a%20series%20of%20work%20on%0Agrounded%20Automatic%20Report%20Generation%20%28AutoRG%29%2C%20starting%20from%20the%20brain%20MRI%0Ainterpretation%20system%2C%20which%20supports%20the%20delineation%20of%20brain%20structures%2C%20the%0Alocalization%20of%20anomalies%2C%20and%20the%20generation%20of%20well-organized%20findings.%20We%0Amake%20contributions%20from%20the%20following%20aspects%2C%20first%2C%20on%20dataset%20construction%2C%0Awe%20release%20a%20comprehensive%20dataset%20encompassing%20segmentation%20masks%20of%20anomaly%0Aregions%20and%20manually%20authored%20reports%2C%20termed%20as%20RadGenome-Brain%20MRI.%20This%20data%0Aresource%20is%20intended%20to%20catalyze%20ongoing%20research%20and%20development%20in%20the%20field%0Aof%20AI-assisted%20report%20generation%20systems.%20Second%2C%20on%20system%20design%2C%20we%20propose%0AAutoRG-Brain%2C%20the%20first%20brain%20MRI%20report%20generation%20system%20with%20pixel-level%0Agrounded%20visual%20clues.%20Third%2C%20for%20evaluation%2C%20we%20conduct%20quantitative%0Aassessments%20and%20human%20evaluations%20of%20brain%20structure%20segmentation%2C%20anomaly%0Alocalization%2C%20and%20report%20generation%20tasks%20to%20provide%20evidence%20of%20its%0Areliability%20and%20accuracy.%20This%20system%20has%20been%20integrated%20into%20real%20clinical%0Ascenarios%2C%20where%20radiologists%20were%20instructed%20to%20write%20reports%20based%20on%20our%0Agenerated%20findings%20and%20anomaly%20segmentation%20masks.%20The%20results%20demonstrate%20that%0Aour%20system%20enhances%20the%20report-writing%20skills%20of%20junior%20doctors%2C%20aligning%20their%0Aperformance%20more%20closely%20with%20senior%20doctors%2C%20thereby%20boosting%20overall%0Aproductivity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoRG-Brain%253A%2520Grounded%2520Report%2520Generation%2520for%2520Brain%2520MRI%26entry.906535625%3DJiayu%2520Lei%2520and%2520Xiaoman%2520Zhang%2520and%2520Chaoyi%2520Wu%2520and%2520Lisong%2520Dai%2520and%2520Ya%2520Zhang%2520and%2520Yanyong%2520Zhang%2520and%2520Yanfeng%2520Wang%2520and%2520Weidi%2520Xie%2520and%2520Yuehua%2520Li%26entry.1292438233%3D%2520%2520Radiologists%2520are%2520tasked%2520with%2520interpreting%2520a%2520large%2520number%2520of%2520images%2520in%2520a%2520daily%250Abase%252C%2520with%2520the%2520responsibility%2520of%2520generating%2520corresponding%2520reports.%2520This%250Ademanding%2520workload%2520elevates%2520the%2520risk%2520of%2520human%2520error%252C%2520potentially%2520leading%2520to%250Atreatment%2520delays%252C%2520increased%2520healthcare%2520costs%252C%2520revenue%2520loss%252C%2520and%2520operational%250Ainefficiencies.%2520To%2520address%2520these%2520challenges%252C%2520we%2520initiate%2520a%2520series%2520of%2520work%2520on%250Agrounded%2520Automatic%2520Report%2520Generation%2520%2528AutoRG%2529%252C%2520starting%2520from%2520the%2520brain%2520MRI%250Ainterpretation%2520system%252C%2520which%2520supports%2520the%2520delineation%2520of%2520brain%2520structures%252C%2520the%250Alocalization%2520of%2520anomalies%252C%2520and%2520the%2520generation%2520of%2520well-organized%2520findings.%2520We%250Amake%2520contributions%2520from%2520the%2520following%2520aspects%252C%2520first%252C%2520on%2520dataset%2520construction%252C%250Awe%2520release%2520a%2520comprehensive%2520dataset%2520encompassing%2520segmentation%2520masks%2520of%2520anomaly%250Aregions%2520and%2520manually%2520authored%2520reports%252C%2520termed%2520as%2520RadGenome-Brain%2520MRI.%2520This%2520data%250Aresource%2520is%2520intended%2520to%2520catalyze%2520ongoing%2520research%2520and%2520development%2520in%2520the%2520field%250Aof%2520AI-assisted%2520report%2520generation%2520systems.%2520Second%252C%2520on%2520system%2520design%252C%2520we%2520propose%250AAutoRG-Brain%252C%2520the%2520first%2520brain%2520MRI%2520report%2520generation%2520system%2520with%2520pixel-level%250Agrounded%2520visual%2520clues.%2520Third%252C%2520for%2520evaluation%252C%2520we%2520conduct%2520quantitative%250Aassessments%2520and%2520human%2520evaluations%2520of%2520brain%2520structure%2520segmentation%252C%2520anomaly%250Alocalization%252C%2520and%2520report%2520generation%2520tasks%2520to%2520provide%2520evidence%2520of%2520its%250Areliability%2520and%2520accuracy.%2520This%2520system%2520has%2520been%2520integrated%2520into%2520real%2520clinical%250Ascenarios%252C%2520where%2520radiologists%2520were%2520instructed%2520to%2520write%2520reports%2520based%2520on%2520our%250Agenerated%2520findings%2520and%2520anomaly%2520segmentation%2520masks.%2520The%2520results%2520demonstrate%2520that%250Aour%2520system%2520enhances%2520the%2520report-writing%2520skills%2520of%2520junior%2520doctors%252C%2520aligning%2520their%250Aperformance%2520more%2520closely%2520with%2520senior%2520doctors%252C%2520thereby%2520boosting%2520overall%250Aproductivity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoRG-Brain%3A%20Grounded%20Report%20Generation%20for%20Brain%20MRI&entry.906535625=Jiayu%20Lei%20and%20Xiaoman%20Zhang%20and%20Chaoyi%20Wu%20and%20Lisong%20Dai%20and%20Ya%20Zhang%20and%20Yanyong%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie%20and%20Yuehua%20Li&entry.1292438233=%20%20Radiologists%20are%20tasked%20with%20interpreting%20a%20large%20number%20of%20images%20in%20a%20daily%0Abase%2C%20with%20the%20responsibility%20of%20generating%20corresponding%20reports.%20This%0Ademanding%20workload%20elevates%20the%20risk%20of%20human%20error%2C%20potentially%20leading%20to%0Atreatment%20delays%2C%20increased%20healthcare%20costs%2C%20revenue%20loss%2C%20and%20operational%0Ainefficiencies.%20To%20address%20these%20challenges%2C%20we%20initiate%20a%20series%20of%20work%20on%0Agrounded%20Automatic%20Report%20Generation%20%28AutoRG%29%2C%20starting%20from%20the%20brain%20MRI%0Ainterpretation%20system%2C%20which%20supports%20the%20delineation%20of%20brain%20structures%2C%20the%0Alocalization%20of%20anomalies%2C%20and%20the%20generation%20of%20well-organized%20findings.%20We%0Amake%20contributions%20from%20the%20following%20aspects%2C%20first%2C%20on%20dataset%20construction%2C%0Awe%20release%20a%20comprehensive%20dataset%20encompassing%20segmentation%20masks%20of%20anomaly%0Aregions%20and%20manually%20authored%20reports%2C%20termed%20as%20RadGenome-Brain%20MRI.%20This%20data%0Aresource%20is%20intended%20to%20catalyze%20ongoing%20research%20and%20development%20in%20the%20field%0Aof%20AI-assisted%20report%20generation%20systems.%20Second%2C%20on%20system%20design%2C%20we%20propose%0AAutoRG-Brain%2C%20the%20first%20brain%20MRI%20report%20generation%20system%20with%20pixel-level%0Agrounded%20visual%20clues.%20Third%2C%20for%20evaluation%2C%20we%20conduct%20quantitative%0Aassessments%20and%20human%20evaluations%20of%20brain%20structure%20segmentation%2C%20anomaly%0Alocalization%2C%20and%20report%20generation%20tasks%20to%20provide%20evidence%20of%20its%0Areliability%20and%20accuracy.%20This%20system%20has%20been%20integrated%20into%20real%20clinical%0Ascenarios%2C%20where%20radiologists%20were%20instructed%20to%20write%20reports%20based%20on%20our%0Agenerated%20findings%20and%20anomaly%20segmentation%20masks.%20The%20results%20demonstrate%20that%0Aour%20system%20enhances%20the%20report-writing%20skills%20of%20junior%20doctors%2C%20aligning%20their%0Aperformance%20more%20closely%20with%20senior%20doctors%2C%20thereby%20boosting%20overall%0Aproductivity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16684v1&entry.124074799=Read"},
{"title": "Knowledge-driven AI-generated data for accurate and interpretable breast\n  ultrasound diagnoses", "author": "Haojun Yu and Youcheng Li and Nan Zhang and Zihan Niu and Xuantong Gong and Yanwen Luo and Quanlin Wu and Wangyan Qin and Mengyuan Zhou and Jie Han and Jia Tao and Ziwei Zhao and Di Dai and Di He and Dong Wang and Binghui Tang and Ling Huo and Qingli Zhu and Yong Wang and Liwei Wang", "abstract": "  Data-driven deep learning models have shown great capabilities to assist\nradiologists in breast ultrasound (US) diagnoses. However, their effectiveness\nis limited by the long-tail distribution of training data, which leads to\ninaccuracies in rare cases. In this study, we address a long-standing challenge\nof improving the diagnostic model performance on rare cases using long-tailed\ndata. Specifically, we introduce a pipeline, TAILOR, that builds a\nknowledge-driven generative model to produce tailored synthetic data. The\ngenerative model, using 3,749 lesions as source data, can generate millions of\nbreast-US images, especially for error-prone rare cases. The generated data can\nbe further used to build a diagnostic model for accurate and interpretable\ndiagnoses. In the prospective external evaluation, our diagnostic model\noutperforms the average performance of nine radiologists by 33.5% in\nspecificity with the same sensitivity, improving their performance by providing\npredictions with an interpretable decision-making process. Moreover, on ductal\ncarcinoma in situ (DCIS), our diagnostic model outperforms all radiologists by\na large margin, with only 34 DCIS lesions in the source data. We believe that\nTAILOR can potentially be extended to various diseases and imaging modalities.\n", "link": "http://arxiv.org/abs/2407.16634v1", "date": "2024-07-23", "relevancy": 1.9694, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5118}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5042}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge-driven%20AI-generated%20data%20for%20accurate%20and%20interpretable%20breast%0A%20%20ultrasound%20diagnoses&body=Title%3A%20Knowledge-driven%20AI-generated%20data%20for%20accurate%20and%20interpretable%20breast%0A%20%20ultrasound%20diagnoses%0AAuthor%3A%20Haojun%20Yu%20and%20Youcheng%20Li%20and%20Nan%20Zhang%20and%20Zihan%20Niu%20and%20Xuantong%20Gong%20and%20Yanwen%20Luo%20and%20Quanlin%20Wu%20and%20Wangyan%20Qin%20and%20Mengyuan%20Zhou%20and%20Jie%20Han%20and%20Jia%20Tao%20and%20Ziwei%20Zhao%20and%20Di%20Dai%20and%20Di%20He%20and%20Dong%20Wang%20and%20Binghui%20Tang%20and%20Ling%20Huo%20and%20Qingli%20Zhu%20and%20Yong%20Wang%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20Data-driven%20deep%20learning%20models%20have%20shown%20great%20capabilities%20to%20assist%0Aradiologists%20in%20breast%20ultrasound%20%28US%29%20diagnoses.%20However%2C%20their%20effectiveness%0Ais%20limited%20by%20the%20long-tail%20distribution%20of%20training%20data%2C%20which%20leads%20to%0Ainaccuracies%20in%20rare%20cases.%20In%20this%20study%2C%20we%20address%20a%20long-standing%20challenge%0Aof%20improving%20the%20diagnostic%20model%20performance%20on%20rare%20cases%20using%20long-tailed%0Adata.%20Specifically%2C%20we%20introduce%20a%20pipeline%2C%20TAILOR%2C%20that%20builds%20a%0Aknowledge-driven%20generative%20model%20to%20produce%20tailored%20synthetic%20data.%20The%0Agenerative%20model%2C%20using%203%2C749%20lesions%20as%20source%20data%2C%20can%20generate%20millions%20of%0Abreast-US%20images%2C%20especially%20for%20error-prone%20rare%20cases.%20The%20generated%20data%20can%0Abe%20further%20used%20to%20build%20a%20diagnostic%20model%20for%20accurate%20and%20interpretable%0Adiagnoses.%20In%20the%20prospective%20external%20evaluation%2C%20our%20diagnostic%20model%0Aoutperforms%20the%20average%20performance%20of%20nine%20radiologists%20by%2033.5%25%20in%0Aspecificity%20with%20the%20same%20sensitivity%2C%20improving%20their%20performance%20by%20providing%0Apredictions%20with%20an%20interpretable%20decision-making%20process.%20Moreover%2C%20on%20ductal%0Acarcinoma%20in%20situ%20%28DCIS%29%2C%20our%20diagnostic%20model%20outperforms%20all%20radiologists%20by%0Aa%20large%20margin%2C%20with%20only%2034%20DCIS%20lesions%20in%20the%20source%20data.%20We%20believe%20that%0ATAILOR%20can%20potentially%20be%20extended%20to%20various%20diseases%20and%20imaging%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge-driven%2520AI-generated%2520data%2520for%2520accurate%2520and%2520interpretable%2520breast%250A%2520%2520ultrasound%2520diagnoses%26entry.906535625%3DHaojun%2520Yu%2520and%2520Youcheng%2520Li%2520and%2520Nan%2520Zhang%2520and%2520Zihan%2520Niu%2520and%2520Xuantong%2520Gong%2520and%2520Yanwen%2520Luo%2520and%2520Quanlin%2520Wu%2520and%2520Wangyan%2520Qin%2520and%2520Mengyuan%2520Zhou%2520and%2520Jie%2520Han%2520and%2520Jia%2520Tao%2520and%2520Ziwei%2520Zhao%2520and%2520Di%2520Dai%2520and%2520Di%2520He%2520and%2520Dong%2520Wang%2520and%2520Binghui%2520Tang%2520and%2520Ling%2520Huo%2520and%2520Qingli%2520Zhu%2520and%2520Yong%2520Wang%2520and%2520Liwei%2520Wang%26entry.1292438233%3D%2520%2520Data-driven%2520deep%2520learning%2520models%2520have%2520shown%2520great%2520capabilities%2520to%2520assist%250Aradiologists%2520in%2520breast%2520ultrasound%2520%2528US%2529%2520diagnoses.%2520However%252C%2520their%2520effectiveness%250Ais%2520limited%2520by%2520the%2520long-tail%2520distribution%2520of%2520training%2520data%252C%2520which%2520leads%2520to%250Ainaccuracies%2520in%2520rare%2520cases.%2520In%2520this%2520study%252C%2520we%2520address%2520a%2520long-standing%2520challenge%250Aof%2520improving%2520the%2520diagnostic%2520model%2520performance%2520on%2520rare%2520cases%2520using%2520long-tailed%250Adata.%2520Specifically%252C%2520we%2520introduce%2520a%2520pipeline%252C%2520TAILOR%252C%2520that%2520builds%2520a%250Aknowledge-driven%2520generative%2520model%2520to%2520produce%2520tailored%2520synthetic%2520data.%2520The%250Agenerative%2520model%252C%2520using%25203%252C749%2520lesions%2520as%2520source%2520data%252C%2520can%2520generate%2520millions%2520of%250Abreast-US%2520images%252C%2520especially%2520for%2520error-prone%2520rare%2520cases.%2520The%2520generated%2520data%2520can%250Abe%2520further%2520used%2520to%2520build%2520a%2520diagnostic%2520model%2520for%2520accurate%2520and%2520interpretable%250Adiagnoses.%2520In%2520the%2520prospective%2520external%2520evaluation%252C%2520our%2520diagnostic%2520model%250Aoutperforms%2520the%2520average%2520performance%2520of%2520nine%2520radiologists%2520by%252033.5%2525%2520in%250Aspecificity%2520with%2520the%2520same%2520sensitivity%252C%2520improving%2520their%2520performance%2520by%2520providing%250Apredictions%2520with%2520an%2520interpretable%2520decision-making%2520process.%2520Moreover%252C%2520on%2520ductal%250Acarcinoma%2520in%2520situ%2520%2528DCIS%2529%252C%2520our%2520diagnostic%2520model%2520outperforms%2520all%2520radiologists%2520by%250Aa%2520large%2520margin%252C%2520with%2520only%252034%2520DCIS%2520lesions%2520in%2520the%2520source%2520data.%2520We%2520believe%2520that%250ATAILOR%2520can%2520potentially%2520be%2520extended%2520to%2520various%2520diseases%2520and%2520imaging%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge-driven%20AI-generated%20data%20for%20accurate%20and%20interpretable%20breast%0A%20%20ultrasound%20diagnoses&entry.906535625=Haojun%20Yu%20and%20Youcheng%20Li%20and%20Nan%20Zhang%20and%20Zihan%20Niu%20and%20Xuantong%20Gong%20and%20Yanwen%20Luo%20and%20Quanlin%20Wu%20and%20Wangyan%20Qin%20and%20Mengyuan%20Zhou%20and%20Jie%20Han%20and%20Jia%20Tao%20and%20Ziwei%20Zhao%20and%20Di%20Dai%20and%20Di%20He%20and%20Dong%20Wang%20and%20Binghui%20Tang%20and%20Ling%20Huo%20and%20Qingli%20Zhu%20and%20Yong%20Wang%20and%20Liwei%20Wang&entry.1292438233=%20%20Data-driven%20deep%20learning%20models%20have%20shown%20great%20capabilities%20to%20assist%0Aradiologists%20in%20breast%20ultrasound%20%28US%29%20diagnoses.%20However%2C%20their%20effectiveness%0Ais%20limited%20by%20the%20long-tail%20distribution%20of%20training%20data%2C%20which%20leads%20to%0Ainaccuracies%20in%20rare%20cases.%20In%20this%20study%2C%20we%20address%20a%20long-standing%20challenge%0Aof%20improving%20the%20diagnostic%20model%20performance%20on%20rare%20cases%20using%20long-tailed%0Adata.%20Specifically%2C%20we%20introduce%20a%20pipeline%2C%20TAILOR%2C%20that%20builds%20a%0Aknowledge-driven%20generative%20model%20to%20produce%20tailored%20synthetic%20data.%20The%0Agenerative%20model%2C%20using%203%2C749%20lesions%20as%20source%20data%2C%20can%20generate%20millions%20of%0Abreast-US%20images%2C%20especially%20for%20error-prone%20rare%20cases.%20The%20generated%20data%20can%0Abe%20further%20used%20to%20build%20a%20diagnostic%20model%20for%20accurate%20and%20interpretable%0Adiagnoses.%20In%20the%20prospective%20external%20evaluation%2C%20our%20diagnostic%20model%0Aoutperforms%20the%20average%20performance%20of%20nine%20radiologists%20by%2033.5%25%20in%0Aspecificity%20with%20the%20same%20sensitivity%2C%20improving%20their%20performance%20by%20providing%0Apredictions%20with%20an%20interpretable%20decision-making%20process.%20Moreover%2C%20on%20ductal%0Acarcinoma%20in%20situ%20%28DCIS%29%2C%20our%20diagnostic%20model%20outperforms%20all%20radiologists%20by%0Aa%20large%20margin%2C%20with%20only%2034%20DCIS%20lesions%20in%20the%20source%20data.%20We%20believe%20that%0ATAILOR%20can%20potentially%20be%20extended%20to%20various%20diseases%20and%20imaging%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16634v1&entry.124074799=Read"},
{"title": "Deep learning empowered sensor fusion boosts infant movement\n  classification", "author": "Tomas Kulvicius and Dajie Zhang and Luise Poustka and Sven B\u00f6lte and Lennart Jahn and Sarah Fl\u00fcgge and Marc Kraft and Markus Zweckstetter and Karin Nielsen-Saines and Florentin W\u00f6rg\u00f6tter and Peter B Marschik", "abstract": "  There is a recent boom in the development of AI solutions to facilitate and\nenhance diagnostic procedures for established clinical tools. To assess the\nintegrity of the developing nervous system, the Prechtl general movement\nassessment (GMA) is recognized for its clinical value in diagnosing\nneurological impairments in early infancy. GMA has been increasingly augmented\nthrough machine learning approaches intending to scale-up its application,\ncircumvent costs in the training of human assessors and further standardize\nclassification of spontaneous motor patterns. Available deep learning tools,\nall of which are based on single sensor modalities, are however still\nconsiderably inferior to that of well-trained human assessors. These approaches\nare hardly comparable as all models are designed, trained and evaluated on\nproprietary/silo-data sets. With this study we propose a sensor fusion approach\nfor assessing fidgety movements (FMs) comparing three different sensor\nmodalities (pressure, inertial, and visual sensors). Various combinations and\ntwo sensor fusion approaches (late and early fusion) for infant movement\nclassification were tested to evaluate whether a multi-sensor system\noutperforms single modality assessments. The performance of the three-sensor\nfusion (classification accuracy of 94.5\\%) was significantly higher than that\nof any single modality evaluated, suggesting the sensor fusion approach is a\npromising avenue for automated classification of infant motor patterns. The\ndevelopment of a robust sensor fusion system may significantly enhance AI-based\nearly recognition of neurofunctions, ultimately facilitating automated early\ndetection of neurodevelopmental conditions.\n", "link": "http://arxiv.org/abs/2406.09014v4", "date": "2024-07-23", "relevancy": 1.9656, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5397}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4855}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20learning%20empowered%20sensor%20fusion%20boosts%20infant%20movement%0A%20%20classification&body=Title%3A%20Deep%20learning%20empowered%20sensor%20fusion%20boosts%20infant%20movement%0A%20%20classification%0AAuthor%3A%20Tomas%20Kulvicius%20and%20Dajie%20Zhang%20and%20Luise%20Poustka%20and%20Sven%20B%C3%B6lte%20and%20Lennart%20Jahn%20and%20Sarah%20Fl%C3%BCgge%20and%20Marc%20Kraft%20and%20Markus%20Zweckstetter%20and%20Karin%20Nielsen-Saines%20and%20Florentin%20W%C3%B6rg%C3%B6tter%20and%20Peter%20B%20Marschik%0AAbstract%3A%20%20%20There%20is%20a%20recent%20boom%20in%20the%20development%20of%20AI%20solutions%20to%20facilitate%20and%0Aenhance%20diagnostic%20procedures%20for%20established%20clinical%20tools.%20To%20assess%20the%0Aintegrity%20of%20the%20developing%20nervous%20system%2C%20the%20Prechtl%20general%20movement%0Aassessment%20%28GMA%29%20is%20recognized%20for%20its%20clinical%20value%20in%20diagnosing%0Aneurological%20impairments%20in%20early%20infancy.%20GMA%20has%20been%20increasingly%20augmented%0Athrough%20machine%20learning%20approaches%20intending%20to%20scale-up%20its%20application%2C%0Acircumvent%20costs%20in%20the%20training%20of%20human%20assessors%20and%20further%20standardize%0Aclassification%20of%20spontaneous%20motor%20patterns.%20Available%20deep%20learning%20tools%2C%0Aall%20of%20which%20are%20based%20on%20single%20sensor%20modalities%2C%20are%20however%20still%0Aconsiderably%20inferior%20to%20that%20of%20well-trained%20human%20assessors.%20These%20approaches%0Aare%20hardly%20comparable%20as%20all%20models%20are%20designed%2C%20trained%20and%20evaluated%20on%0Aproprietary/silo-data%20sets.%20With%20this%20study%20we%20propose%20a%20sensor%20fusion%20approach%0Afor%20assessing%20fidgety%20movements%20%28FMs%29%20comparing%20three%20different%20sensor%0Amodalities%20%28pressure%2C%20inertial%2C%20and%20visual%20sensors%29.%20Various%20combinations%20and%0Atwo%20sensor%20fusion%20approaches%20%28late%20and%20early%20fusion%29%20for%20infant%20movement%0Aclassification%20were%20tested%20to%20evaluate%20whether%20a%20multi-sensor%20system%0Aoutperforms%20single%20modality%20assessments.%20The%20performance%20of%20the%20three-sensor%0Afusion%20%28classification%20accuracy%20of%2094.5%5C%25%29%20was%20significantly%20higher%20than%20that%0Aof%20any%20single%20modality%20evaluated%2C%20suggesting%20the%20sensor%20fusion%20approach%20is%20a%0Apromising%20avenue%20for%20automated%20classification%20of%20infant%20motor%20patterns.%20The%0Adevelopment%20of%20a%20robust%20sensor%20fusion%20system%20may%20significantly%20enhance%20AI-based%0Aearly%20recognition%20of%20neurofunctions%2C%20ultimately%20facilitating%20automated%20early%0Adetection%20of%20neurodevelopmental%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09014v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520learning%2520empowered%2520sensor%2520fusion%2520boosts%2520infant%2520movement%250A%2520%2520classification%26entry.906535625%3DTomas%2520Kulvicius%2520and%2520Dajie%2520Zhang%2520and%2520Luise%2520Poustka%2520and%2520Sven%2520B%25C3%25B6lte%2520and%2520Lennart%2520Jahn%2520and%2520Sarah%2520Fl%25C3%25BCgge%2520and%2520Marc%2520Kraft%2520and%2520Markus%2520Zweckstetter%2520and%2520Karin%2520Nielsen-Saines%2520and%2520Florentin%2520W%25C3%25B6rg%25C3%25B6tter%2520and%2520Peter%2520B%2520Marschik%26entry.1292438233%3D%2520%2520There%2520is%2520a%2520recent%2520boom%2520in%2520the%2520development%2520of%2520AI%2520solutions%2520to%2520facilitate%2520and%250Aenhance%2520diagnostic%2520procedures%2520for%2520established%2520clinical%2520tools.%2520To%2520assess%2520the%250Aintegrity%2520of%2520the%2520developing%2520nervous%2520system%252C%2520the%2520Prechtl%2520general%2520movement%250Aassessment%2520%2528GMA%2529%2520is%2520recognized%2520for%2520its%2520clinical%2520value%2520in%2520diagnosing%250Aneurological%2520impairments%2520in%2520early%2520infancy.%2520GMA%2520has%2520been%2520increasingly%2520augmented%250Athrough%2520machine%2520learning%2520approaches%2520intending%2520to%2520scale-up%2520its%2520application%252C%250Acircumvent%2520costs%2520in%2520the%2520training%2520of%2520human%2520assessors%2520and%2520further%2520standardize%250Aclassification%2520of%2520spontaneous%2520motor%2520patterns.%2520Available%2520deep%2520learning%2520tools%252C%250Aall%2520of%2520which%2520are%2520based%2520on%2520single%2520sensor%2520modalities%252C%2520are%2520however%2520still%250Aconsiderably%2520inferior%2520to%2520that%2520of%2520well-trained%2520human%2520assessors.%2520These%2520approaches%250Aare%2520hardly%2520comparable%2520as%2520all%2520models%2520are%2520designed%252C%2520trained%2520and%2520evaluated%2520on%250Aproprietary/silo-data%2520sets.%2520With%2520this%2520study%2520we%2520propose%2520a%2520sensor%2520fusion%2520approach%250Afor%2520assessing%2520fidgety%2520movements%2520%2528FMs%2529%2520comparing%2520three%2520different%2520sensor%250Amodalities%2520%2528pressure%252C%2520inertial%252C%2520and%2520visual%2520sensors%2529.%2520Various%2520combinations%2520and%250Atwo%2520sensor%2520fusion%2520approaches%2520%2528late%2520and%2520early%2520fusion%2529%2520for%2520infant%2520movement%250Aclassification%2520were%2520tested%2520to%2520evaluate%2520whether%2520a%2520multi-sensor%2520system%250Aoutperforms%2520single%2520modality%2520assessments.%2520The%2520performance%2520of%2520the%2520three-sensor%250Afusion%2520%2528classification%2520accuracy%2520of%252094.5%255C%2525%2529%2520was%2520significantly%2520higher%2520than%2520that%250Aof%2520any%2520single%2520modality%2520evaluated%252C%2520suggesting%2520the%2520sensor%2520fusion%2520approach%2520is%2520a%250Apromising%2520avenue%2520for%2520automated%2520classification%2520of%2520infant%2520motor%2520patterns.%2520The%250Adevelopment%2520of%2520a%2520robust%2520sensor%2520fusion%2520system%2520may%2520significantly%2520enhance%2520AI-based%250Aearly%2520recognition%2520of%2520neurofunctions%252C%2520ultimately%2520facilitating%2520automated%2520early%250Adetection%2520of%2520neurodevelopmental%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09014v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20learning%20empowered%20sensor%20fusion%20boosts%20infant%20movement%0A%20%20classification&entry.906535625=Tomas%20Kulvicius%20and%20Dajie%20Zhang%20and%20Luise%20Poustka%20and%20Sven%20B%C3%B6lte%20and%20Lennart%20Jahn%20and%20Sarah%20Fl%C3%BCgge%20and%20Marc%20Kraft%20and%20Markus%20Zweckstetter%20and%20Karin%20Nielsen-Saines%20and%20Florentin%20W%C3%B6rg%C3%B6tter%20and%20Peter%20B%20Marschik&entry.1292438233=%20%20There%20is%20a%20recent%20boom%20in%20the%20development%20of%20AI%20solutions%20to%20facilitate%20and%0Aenhance%20diagnostic%20procedures%20for%20established%20clinical%20tools.%20To%20assess%20the%0Aintegrity%20of%20the%20developing%20nervous%20system%2C%20the%20Prechtl%20general%20movement%0Aassessment%20%28GMA%29%20is%20recognized%20for%20its%20clinical%20value%20in%20diagnosing%0Aneurological%20impairments%20in%20early%20infancy.%20GMA%20has%20been%20increasingly%20augmented%0Athrough%20machine%20learning%20approaches%20intending%20to%20scale-up%20its%20application%2C%0Acircumvent%20costs%20in%20the%20training%20of%20human%20assessors%20and%20further%20standardize%0Aclassification%20of%20spontaneous%20motor%20patterns.%20Available%20deep%20learning%20tools%2C%0Aall%20of%20which%20are%20based%20on%20single%20sensor%20modalities%2C%20are%20however%20still%0Aconsiderably%20inferior%20to%20that%20of%20well-trained%20human%20assessors.%20These%20approaches%0Aare%20hardly%20comparable%20as%20all%20models%20are%20designed%2C%20trained%20and%20evaluated%20on%0Aproprietary/silo-data%20sets.%20With%20this%20study%20we%20propose%20a%20sensor%20fusion%20approach%0Afor%20assessing%20fidgety%20movements%20%28FMs%29%20comparing%20three%20different%20sensor%0Amodalities%20%28pressure%2C%20inertial%2C%20and%20visual%20sensors%29.%20Various%20combinations%20and%0Atwo%20sensor%20fusion%20approaches%20%28late%20and%20early%20fusion%29%20for%20infant%20movement%0Aclassification%20were%20tested%20to%20evaluate%20whether%20a%20multi-sensor%20system%0Aoutperforms%20single%20modality%20assessments.%20The%20performance%20of%20the%20three-sensor%0Afusion%20%28classification%20accuracy%20of%2094.5%5C%25%29%20was%20significantly%20higher%20than%20that%0Aof%20any%20single%20modality%20evaluated%2C%20suggesting%20the%20sensor%20fusion%20approach%20is%20a%0Apromising%20avenue%20for%20automated%20classification%20of%20infant%20motor%20patterns.%20The%0Adevelopment%20of%20a%20robust%20sensor%20fusion%20system%20may%20significantly%20enhance%20AI-based%0Aearly%20recognition%20of%20neurofunctions%2C%20ultimately%20facilitating%20automated%20early%0Adetection%20of%20neurodevelopmental%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09014v4&entry.124074799=Read"},
{"title": "Defending Our Privacy With Backdoors", "author": "Dominik Hintersdorf and Lukas Struppek and Daniel Neider and Kristian Kersting", "abstract": "  The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information, such as names and faces of\nindividuals, from vision-language models by fine-tuning them for only a few\nminutes instead of re-training them from scratch. Specifically, by\nstrategically inserting backdoors into text encoders, we align the embeddings\nof sensitive phrases with those of neutral terms-\"a person\" instead of the\nperson's actual name. For image encoders, we map individuals' embeddings to be\nremoved from the model to a universal, anonymous embedding. The results of our\nextensive experimental evaluation demonstrate the effectiveness of our\nbackdoor-based defense on CLIP by assessing its performance using a specialized\nprivacy attack for zero-shot classifiers. Our approach provides a new\n\"dual-use\" perspective on backdoor attacks and presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.\n", "link": "http://arxiv.org/abs/2310.08320v4", "date": "2024-07-23", "relevancy": 1.9634, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4887}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defending%20Our%20Privacy%20With%20Backdoors&body=Title%3A%20Defending%20Our%20Privacy%20With%20Backdoors%0AAuthor%3A%20Dominik%20Hintersdorf%20and%20Lukas%20Struppek%20and%20Daniel%20Neider%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20The%20proliferation%20of%20large%20AI%20models%20trained%20on%20uncurated%2C%20often%20sensitive%0Aweb-scraped%20data%20has%20raised%20significant%20privacy%20concerns.%20One%20of%20the%20concerns%0Ais%20that%20adversaries%20can%20extract%20information%20about%20the%20training%20data%20using%0Aprivacy%20attacks.%20Unfortunately%2C%20the%20task%20of%20removing%20specific%20information%20from%0Athe%20models%20without%20sacrificing%20performance%20is%20not%20straightforward%20and%20has%0Aproven%20to%20be%20challenging.%20We%20propose%20a%20rather%20easy%20yet%20effective%20defense%20based%0Aon%20backdoor%20attacks%20to%20remove%20private%20information%2C%20such%20as%20names%20and%20faces%20of%0Aindividuals%2C%20from%20vision-language%20models%20by%20fine-tuning%20them%20for%20only%20a%20few%0Aminutes%20instead%20of%20re-training%20them%20from%20scratch.%20Specifically%2C%20by%0Astrategically%20inserting%20backdoors%20into%20text%20encoders%2C%20we%20align%20the%20embeddings%0Aof%20sensitive%20phrases%20with%20those%20of%20neutral%20terms-%22a%20person%22%20instead%20of%20the%0Aperson%27s%20actual%20name.%20For%20image%20encoders%2C%20we%20map%20individuals%27%20embeddings%20to%20be%0Aremoved%20from%20the%20model%20to%20a%20universal%2C%20anonymous%20embedding.%20The%20results%20of%20our%0Aextensive%20experimental%20evaluation%20demonstrate%20the%20effectiveness%20of%20our%0Abackdoor-based%20defense%20on%20CLIP%20by%20assessing%20its%20performance%20using%20a%20specialized%0Aprivacy%20attack%20for%20zero-shot%20classifiers.%20Our%20approach%20provides%20a%20new%0A%22dual-use%22%20perspective%20on%20backdoor%20attacks%20and%20presents%20a%20promising%20avenue%20to%0Aenhance%20the%20privacy%20of%20individuals%20within%20models%20trained%20on%20uncurated%0Aweb-scraped%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08320v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefending%2520Our%2520Privacy%2520With%2520Backdoors%26entry.906535625%3DDominik%2520Hintersdorf%2520and%2520Lukas%2520Struppek%2520and%2520Daniel%2520Neider%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520large%2520AI%2520models%2520trained%2520on%2520uncurated%252C%2520often%2520sensitive%250Aweb-scraped%2520data%2520has%2520raised%2520significant%2520privacy%2520concerns.%2520One%2520of%2520the%2520concerns%250Ais%2520that%2520adversaries%2520can%2520extract%2520information%2520about%2520the%2520training%2520data%2520using%250Aprivacy%2520attacks.%2520Unfortunately%252C%2520the%2520task%2520of%2520removing%2520specific%2520information%2520from%250Athe%2520models%2520without%2520sacrificing%2520performance%2520is%2520not%2520straightforward%2520and%2520has%250Aproven%2520to%2520be%2520challenging.%2520We%2520propose%2520a%2520rather%2520easy%2520yet%2520effective%2520defense%2520based%250Aon%2520backdoor%2520attacks%2520to%2520remove%2520private%2520information%252C%2520such%2520as%2520names%2520and%2520faces%2520of%250Aindividuals%252C%2520from%2520vision-language%2520models%2520by%2520fine-tuning%2520them%2520for%2520only%2520a%2520few%250Aminutes%2520instead%2520of%2520re-training%2520them%2520from%2520scratch.%2520Specifically%252C%2520by%250Astrategically%2520inserting%2520backdoors%2520into%2520text%2520encoders%252C%2520we%2520align%2520the%2520embeddings%250Aof%2520sensitive%2520phrases%2520with%2520those%2520of%2520neutral%2520terms-%2522a%2520person%2522%2520instead%2520of%2520the%250Aperson%2527s%2520actual%2520name.%2520For%2520image%2520encoders%252C%2520we%2520map%2520individuals%2527%2520embeddings%2520to%2520be%250Aremoved%2520from%2520the%2520model%2520to%2520a%2520universal%252C%2520anonymous%2520embedding.%2520The%2520results%2520of%2520our%250Aextensive%2520experimental%2520evaluation%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Abackdoor-based%2520defense%2520on%2520CLIP%2520by%2520assessing%2520its%2520performance%2520using%2520a%2520specialized%250Aprivacy%2520attack%2520for%2520zero-shot%2520classifiers.%2520Our%2520approach%2520provides%2520a%2520new%250A%2522dual-use%2522%2520perspective%2520on%2520backdoor%2520attacks%2520and%2520presents%2520a%2520promising%2520avenue%2520to%250Aenhance%2520the%2520privacy%2520of%2520individuals%2520within%2520models%2520trained%2520on%2520uncurated%250Aweb-scraped%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08320v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defending%20Our%20Privacy%20With%20Backdoors&entry.906535625=Dominik%20Hintersdorf%20and%20Lukas%20Struppek%20and%20Daniel%20Neider%20and%20Kristian%20Kersting&entry.1292438233=%20%20The%20proliferation%20of%20large%20AI%20models%20trained%20on%20uncurated%2C%20often%20sensitive%0Aweb-scraped%20data%20has%20raised%20significant%20privacy%20concerns.%20One%20of%20the%20concerns%0Ais%20that%20adversaries%20can%20extract%20information%20about%20the%20training%20data%20using%0Aprivacy%20attacks.%20Unfortunately%2C%20the%20task%20of%20removing%20specific%20information%20from%0Athe%20models%20without%20sacrificing%20performance%20is%20not%20straightforward%20and%20has%0Aproven%20to%20be%20challenging.%20We%20propose%20a%20rather%20easy%20yet%20effective%20defense%20based%0Aon%20backdoor%20attacks%20to%20remove%20private%20information%2C%20such%20as%20names%20and%20faces%20of%0Aindividuals%2C%20from%20vision-language%20models%20by%20fine-tuning%20them%20for%20only%20a%20few%0Aminutes%20instead%20of%20re-training%20them%20from%20scratch.%20Specifically%2C%20by%0Astrategically%20inserting%20backdoors%20into%20text%20encoders%2C%20we%20align%20the%20embeddings%0Aof%20sensitive%20phrases%20with%20those%20of%20neutral%20terms-%22a%20person%22%20instead%20of%20the%0Aperson%27s%20actual%20name.%20For%20image%20encoders%2C%20we%20map%20individuals%27%20embeddings%20to%20be%0Aremoved%20from%20the%20model%20to%20a%20universal%2C%20anonymous%20embedding.%20The%20results%20of%20our%0Aextensive%20experimental%20evaluation%20demonstrate%20the%20effectiveness%20of%20our%0Abackdoor-based%20defense%20on%20CLIP%20by%20assessing%20its%20performance%20using%20a%20specialized%0Aprivacy%20attack%20for%20zero-shot%20classifiers.%20Our%20approach%20provides%20a%20new%0A%22dual-use%22%20perspective%20on%20backdoor%20attacks%20and%20presents%20a%20promising%20avenue%20to%0Aenhance%20the%20privacy%20of%20individuals%20within%20models%20trained%20on%20uncurated%0Aweb-scraped%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08320v4&entry.124074799=Read"},
{"title": "PerAda: Parameter-Efficient Federated Learning Personalization with\n  Generalization Guarantees", "author": "Chulin Xie and De-An Huang and Wenda Chu and Daguang Xu and Chaowei Xiao and Bo Li and Anima Anandkumar", "abstract": "  Personalized Federated Learning (pFL) has emerged as a promising solution to\ntackle data heterogeneity across clients in FL. However, existing pFL methods\neither (1) introduce high communication and computation costs or (2) overfit to\nlocal data, which can be limited in scope, and are vulnerable to evolved test\nsamples with natural shifts. In this paper, we propose PerAda, a\nparameter-efficient pFL framework that reduces communication and computational\ncosts and exhibits superior generalization performance, especially under\ntest-time distribution shifts. PerAda reduces the costs by leveraging the power\nof pretrained models and only updates and communicates a small number of\nadditional parameters from adapters. PerAda has good generalization since it\nregularizes each client's personalized adapter with a global adapter, while the\nglobal adapter uses knowledge distillation to aggregate generalized information\nfrom all clients. Theoretically, we provide generalization bounds to explain\nwhy PerAda improves generalization, and we prove its convergence to stationary\npoints under non-convex settings. Empirically, PerAda demonstrates competitive\npersonalized performance (+4.85% on CheXpert) and enables better\nout-of-distribution generalization (+5.23% on CIFAR-10-C) on different datasets\nacross natural and medical domains compared with baselines, while only updating\n12.6% of parameters per model based on the adapter. Our code is available at\nhttps://github.com/NVlabs/PerAda.\n", "link": "http://arxiv.org/abs/2302.06637v3", "date": "2024-07-23", "relevancy": 1.9486, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.503}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.48}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PerAda%3A%20Parameter-Efficient%20Federated%20Learning%20Personalization%20with%0A%20%20Generalization%20Guarantees&body=Title%3A%20PerAda%3A%20Parameter-Efficient%20Federated%20Learning%20Personalization%20with%0A%20%20Generalization%20Guarantees%0AAuthor%3A%20Chulin%20Xie%20and%20De-An%20Huang%20and%20Wenda%20Chu%20and%20Daguang%20Xu%20and%20Chaowei%20Xiao%20and%20Bo%20Li%20and%20Anima%20Anandkumar%0AAbstract%3A%20%20%20Personalized%20Federated%20Learning%20%28pFL%29%20has%20emerged%20as%20a%20promising%20solution%20to%0Atackle%20data%20heterogeneity%20across%20clients%20in%20FL.%20However%2C%20existing%20pFL%20methods%0Aeither%20%281%29%20introduce%20high%20communication%20and%20computation%20costs%20or%20%282%29%20overfit%20to%0Alocal%20data%2C%20which%20can%20be%20limited%20in%20scope%2C%20and%20are%20vulnerable%20to%20evolved%20test%0Asamples%20with%20natural%20shifts.%20In%20this%20paper%2C%20we%20propose%20PerAda%2C%20a%0Aparameter-efficient%20pFL%20framework%20that%20reduces%20communication%20and%20computational%0Acosts%20and%20exhibits%20superior%20generalization%20performance%2C%20especially%20under%0Atest-time%20distribution%20shifts.%20PerAda%20reduces%20the%20costs%20by%20leveraging%20the%20power%0Aof%20pretrained%20models%20and%20only%20updates%20and%20communicates%20a%20small%20number%20of%0Aadditional%20parameters%20from%20adapters.%20PerAda%20has%20good%20generalization%20since%20it%0Aregularizes%20each%20client%27s%20personalized%20adapter%20with%20a%20global%20adapter%2C%20while%20the%0Aglobal%20adapter%20uses%20knowledge%20distillation%20to%20aggregate%20generalized%20information%0Afrom%20all%20clients.%20Theoretically%2C%20we%20provide%20generalization%20bounds%20to%20explain%0Awhy%20PerAda%20improves%20generalization%2C%20and%20we%20prove%20its%20convergence%20to%20stationary%0Apoints%20under%20non-convex%20settings.%20Empirically%2C%20PerAda%20demonstrates%20competitive%0Apersonalized%20performance%20%28%2B4.85%25%20on%20CheXpert%29%20and%20enables%20better%0Aout-of-distribution%20generalization%20%28%2B5.23%25%20on%20CIFAR-10-C%29%20on%20different%20datasets%0Aacross%20natural%20and%20medical%20domains%20compared%20with%20baselines%2C%20while%20only%20updating%0A12.6%25%20of%20parameters%20per%20model%20based%20on%20the%20adapter.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/NVlabs/PerAda.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.06637v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerAda%253A%2520Parameter-Efficient%2520Federated%2520Learning%2520Personalization%2520with%250A%2520%2520Generalization%2520Guarantees%26entry.906535625%3DChulin%2520Xie%2520and%2520De-An%2520Huang%2520and%2520Wenda%2520Chu%2520and%2520Daguang%2520Xu%2520and%2520Chaowei%2520Xiao%2520and%2520Bo%2520Li%2520and%2520Anima%2520Anandkumar%26entry.1292438233%3D%2520%2520Personalized%2520Federated%2520Learning%2520%2528pFL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520to%250Atackle%2520data%2520heterogeneity%2520across%2520clients%2520in%2520FL.%2520However%252C%2520existing%2520pFL%2520methods%250Aeither%2520%25281%2529%2520introduce%2520high%2520communication%2520and%2520computation%2520costs%2520or%2520%25282%2529%2520overfit%2520to%250Alocal%2520data%252C%2520which%2520can%2520be%2520limited%2520in%2520scope%252C%2520and%2520are%2520vulnerable%2520to%2520evolved%2520test%250Asamples%2520with%2520natural%2520shifts.%2520In%2520this%2520paper%252C%2520we%2520propose%2520PerAda%252C%2520a%250Aparameter-efficient%2520pFL%2520framework%2520that%2520reduces%2520communication%2520and%2520computational%250Acosts%2520and%2520exhibits%2520superior%2520generalization%2520performance%252C%2520especially%2520under%250Atest-time%2520distribution%2520shifts.%2520PerAda%2520reduces%2520the%2520costs%2520by%2520leveraging%2520the%2520power%250Aof%2520pretrained%2520models%2520and%2520only%2520updates%2520and%2520communicates%2520a%2520small%2520number%2520of%250Aadditional%2520parameters%2520from%2520adapters.%2520PerAda%2520has%2520good%2520generalization%2520since%2520it%250Aregularizes%2520each%2520client%2527s%2520personalized%2520adapter%2520with%2520a%2520global%2520adapter%252C%2520while%2520the%250Aglobal%2520adapter%2520uses%2520knowledge%2520distillation%2520to%2520aggregate%2520generalized%2520information%250Afrom%2520all%2520clients.%2520Theoretically%252C%2520we%2520provide%2520generalization%2520bounds%2520to%2520explain%250Awhy%2520PerAda%2520improves%2520generalization%252C%2520and%2520we%2520prove%2520its%2520convergence%2520to%2520stationary%250Apoints%2520under%2520non-convex%2520settings.%2520Empirically%252C%2520PerAda%2520demonstrates%2520competitive%250Apersonalized%2520performance%2520%2528%252B4.85%2525%2520on%2520CheXpert%2529%2520and%2520enables%2520better%250Aout-of-distribution%2520generalization%2520%2528%252B5.23%2525%2520on%2520CIFAR-10-C%2529%2520on%2520different%2520datasets%250Aacross%2520natural%2520and%2520medical%2520domains%2520compared%2520with%2520baselines%252C%2520while%2520only%2520updating%250A12.6%2525%2520of%2520parameters%2520per%2520model%2520based%2520on%2520the%2520adapter.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/NVlabs/PerAda.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.06637v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerAda%3A%20Parameter-Efficient%20Federated%20Learning%20Personalization%20with%0A%20%20Generalization%20Guarantees&entry.906535625=Chulin%20Xie%20and%20De-An%20Huang%20and%20Wenda%20Chu%20and%20Daguang%20Xu%20and%20Chaowei%20Xiao%20and%20Bo%20Li%20and%20Anima%20Anandkumar&entry.1292438233=%20%20Personalized%20Federated%20Learning%20%28pFL%29%20has%20emerged%20as%20a%20promising%20solution%20to%0Atackle%20data%20heterogeneity%20across%20clients%20in%20FL.%20However%2C%20existing%20pFL%20methods%0Aeither%20%281%29%20introduce%20high%20communication%20and%20computation%20costs%20or%20%282%29%20overfit%20to%0Alocal%20data%2C%20which%20can%20be%20limited%20in%20scope%2C%20and%20are%20vulnerable%20to%20evolved%20test%0Asamples%20with%20natural%20shifts.%20In%20this%20paper%2C%20we%20propose%20PerAda%2C%20a%0Aparameter-efficient%20pFL%20framework%20that%20reduces%20communication%20and%20computational%0Acosts%20and%20exhibits%20superior%20generalization%20performance%2C%20especially%20under%0Atest-time%20distribution%20shifts.%20PerAda%20reduces%20the%20costs%20by%20leveraging%20the%20power%0Aof%20pretrained%20models%20and%20only%20updates%20and%20communicates%20a%20small%20number%20of%0Aadditional%20parameters%20from%20adapters.%20PerAda%20has%20good%20generalization%20since%20it%0Aregularizes%20each%20client%27s%20personalized%20adapter%20with%20a%20global%20adapter%2C%20while%20the%0Aglobal%20adapter%20uses%20knowledge%20distillation%20to%20aggregate%20generalized%20information%0Afrom%20all%20clients.%20Theoretically%2C%20we%20provide%20generalization%20bounds%20to%20explain%0Awhy%20PerAda%20improves%20generalization%2C%20and%20we%20prove%20its%20convergence%20to%20stationary%0Apoints%20under%20non-convex%20settings.%20Empirically%2C%20PerAda%20demonstrates%20competitive%0Apersonalized%20performance%20%28%2B4.85%25%20on%20CheXpert%29%20and%20enables%20better%0Aout-of-distribution%20generalization%20%28%2B5.23%25%20on%20CIFAR-10-C%29%20on%20different%20datasets%0Aacross%20natural%20and%20medical%20domains%20compared%20with%20baselines%2C%20while%20only%20updating%0A12.6%25%20of%20parameters%20per%20model%20based%20on%20the%20adapter.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/NVlabs/PerAda.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.06637v3&entry.124074799=Read"},
{"title": "TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced\n  CTR Prediction at Kuaishou", "author": "Zihua Si and Lin Guan and ZhongXiang Sun and Xiaoxue Zang and Jing Lu and Yiqun Hui and Xingchao Cao and Zeyu Yang and Yichen Zheng and Dewei Leng and Kai Zheng and Chenbin Zhang and Yanan Niu and Yang Song and Kun Gai", "abstract": "  The significance of modeling long-term user interests for CTR prediction\ntasks in large-scale recommendation systems is progressively gaining attention\namong researchers and practitioners. Existing work, such as SIM and TWIN,\ntypically employs a two-stage approach to model long-term user behavior\nsequences for efficiency concerns. The first stage rapidly retrieves a subset\nof sequences related to the target item from a long sequence using a\nsearch-based mechanism namely the General Search Unit (GSU), while the second\nstage calculates the interest scores using the Exact Search Unit (ESU) on the\nretrieved results. Given the extensive length of user behavior sequences\nspanning the entire life cycle, potentially reaching up to 10^6 in scale, there\nis currently no effective solution for fully modeling such expansive user\ninterests. To overcome this issue, we introduced TWIN-V2, an enhancement of\nTWIN, where a divide-and-conquer approach is applied to compress life-cycle\nbehaviors and uncover more accurate and diverse user interests. Specifically, a\nhierarchical clustering method groups items with similar characteristics in\nlife-cycle behaviors into a single cluster during the offline phase. By\nlimiting the size of clusters, we can compress behavior sequences well beyond\nthe magnitude of 10^5 to a length manageable for online inference in GSU\nretrieval. Cluster-aware target attention extracts comprehensive and\nmulti-faceted long-term interests of users, thereby making the final\nrecommendation results more accurate and diverse. Extensive offline experiments\non a multi-billion-scale industrial dataset and online A/B tests have\ndemonstrated the effectiveness of TWIN-V2. Under an efficient deployment\nframework, TWIN-V2 has been successfully deployed to the primary traffic that\nserves hundreds of millions of daily active users at Kuaishou.\n", "link": "http://arxiv.org/abs/2407.16357v1", "date": "2024-07-23", "relevancy": 1.937, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4887}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4885}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TWIN%20V2%3A%20Scaling%20Ultra-Long%20User%20Behavior%20Sequence%20Modeling%20for%20Enhanced%0A%20%20CTR%20Prediction%20at%20Kuaishou&body=Title%3A%20TWIN%20V2%3A%20Scaling%20Ultra-Long%20User%20Behavior%20Sequence%20Modeling%20for%20Enhanced%0A%20%20CTR%20Prediction%20at%20Kuaishou%0AAuthor%3A%20Zihua%20Si%20and%20Lin%20Guan%20and%20ZhongXiang%20Sun%20and%20Xiaoxue%20Zang%20and%20Jing%20Lu%20and%20Yiqun%20Hui%20and%20Xingchao%20Cao%20and%20Zeyu%20Yang%20and%20Yichen%20Zheng%20and%20Dewei%20Leng%20and%20Kai%20Zheng%20and%20Chenbin%20Zhang%20and%20Yanan%20Niu%20and%20Yang%20Song%20and%20Kun%20Gai%0AAbstract%3A%20%20%20The%20significance%20of%20modeling%20long-term%20user%20interests%20for%20CTR%20prediction%0Atasks%20in%20large-scale%20recommendation%20systems%20is%20progressively%20gaining%20attention%0Aamong%20researchers%20and%20practitioners.%20Existing%20work%2C%20such%20as%20SIM%20and%20TWIN%2C%0Atypically%20employs%20a%20two-stage%20approach%20to%20model%20long-term%20user%20behavior%0Asequences%20for%20efficiency%20concerns.%20The%20first%20stage%20rapidly%20retrieves%20a%20subset%0Aof%20sequences%20related%20to%20the%20target%20item%20from%20a%20long%20sequence%20using%20a%0Asearch-based%20mechanism%20namely%20the%20General%20Search%20Unit%20%28GSU%29%2C%20while%20the%20second%0Astage%20calculates%20the%20interest%20scores%20using%20the%20Exact%20Search%20Unit%20%28ESU%29%20on%20the%0Aretrieved%20results.%20Given%20the%20extensive%20length%20of%20user%20behavior%20sequences%0Aspanning%20the%20entire%20life%20cycle%2C%20potentially%20reaching%20up%20to%2010%5E6%20in%20scale%2C%20there%0Ais%20currently%20no%20effective%20solution%20for%20fully%20modeling%20such%20expansive%20user%0Ainterests.%20To%20overcome%20this%20issue%2C%20we%20introduced%20TWIN-V2%2C%20an%20enhancement%20of%0ATWIN%2C%20where%20a%20divide-and-conquer%20approach%20is%20applied%20to%20compress%20life-cycle%0Abehaviors%20and%20uncover%20more%20accurate%20and%20diverse%20user%20interests.%20Specifically%2C%20a%0Ahierarchical%20clustering%20method%20groups%20items%20with%20similar%20characteristics%20in%0Alife-cycle%20behaviors%20into%20a%20single%20cluster%20during%20the%20offline%20phase.%20By%0Alimiting%20the%20size%20of%20clusters%2C%20we%20can%20compress%20behavior%20sequences%20well%20beyond%0Athe%20magnitude%20of%2010%5E5%20to%20a%20length%20manageable%20for%20online%20inference%20in%20GSU%0Aretrieval.%20Cluster-aware%20target%20attention%20extracts%20comprehensive%20and%0Amulti-faceted%20long-term%20interests%20of%20users%2C%20thereby%20making%20the%20final%0Arecommendation%20results%20more%20accurate%20and%20diverse.%20Extensive%20offline%20experiments%0Aon%20a%20multi-billion-scale%20industrial%20dataset%20and%20online%20A/B%20tests%20have%0Ademonstrated%20the%20effectiveness%20of%20TWIN-V2.%20Under%20an%20efficient%20deployment%0Aframework%2C%20TWIN-V2%20has%20been%20successfully%20deployed%20to%20the%20primary%20traffic%20that%0Aserves%20hundreds%20of%20millions%20of%20daily%20active%20users%20at%20Kuaishou.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTWIN%2520V2%253A%2520Scaling%2520Ultra-Long%2520User%2520Behavior%2520Sequence%2520Modeling%2520for%2520Enhanced%250A%2520%2520CTR%2520Prediction%2520at%2520Kuaishou%26entry.906535625%3DZihua%2520Si%2520and%2520Lin%2520Guan%2520and%2520ZhongXiang%2520Sun%2520and%2520Xiaoxue%2520Zang%2520and%2520Jing%2520Lu%2520and%2520Yiqun%2520Hui%2520and%2520Xingchao%2520Cao%2520and%2520Zeyu%2520Yang%2520and%2520Yichen%2520Zheng%2520and%2520Dewei%2520Leng%2520and%2520Kai%2520Zheng%2520and%2520Chenbin%2520Zhang%2520and%2520Yanan%2520Niu%2520and%2520Yang%2520Song%2520and%2520Kun%2520Gai%26entry.1292438233%3D%2520%2520The%2520significance%2520of%2520modeling%2520long-term%2520user%2520interests%2520for%2520CTR%2520prediction%250Atasks%2520in%2520large-scale%2520recommendation%2520systems%2520is%2520progressively%2520gaining%2520attention%250Aamong%2520researchers%2520and%2520practitioners.%2520Existing%2520work%252C%2520such%2520as%2520SIM%2520and%2520TWIN%252C%250Atypically%2520employs%2520a%2520two-stage%2520approach%2520to%2520model%2520long-term%2520user%2520behavior%250Asequences%2520for%2520efficiency%2520concerns.%2520The%2520first%2520stage%2520rapidly%2520retrieves%2520a%2520subset%250Aof%2520sequences%2520related%2520to%2520the%2520target%2520item%2520from%2520a%2520long%2520sequence%2520using%2520a%250Asearch-based%2520mechanism%2520namely%2520the%2520General%2520Search%2520Unit%2520%2528GSU%2529%252C%2520while%2520the%2520second%250Astage%2520calculates%2520the%2520interest%2520scores%2520using%2520the%2520Exact%2520Search%2520Unit%2520%2528ESU%2529%2520on%2520the%250Aretrieved%2520results.%2520Given%2520the%2520extensive%2520length%2520of%2520user%2520behavior%2520sequences%250Aspanning%2520the%2520entire%2520life%2520cycle%252C%2520potentially%2520reaching%2520up%2520to%252010%255E6%2520in%2520scale%252C%2520there%250Ais%2520currently%2520no%2520effective%2520solution%2520for%2520fully%2520modeling%2520such%2520expansive%2520user%250Ainterests.%2520To%2520overcome%2520this%2520issue%252C%2520we%2520introduced%2520TWIN-V2%252C%2520an%2520enhancement%2520of%250ATWIN%252C%2520where%2520a%2520divide-and-conquer%2520approach%2520is%2520applied%2520to%2520compress%2520life-cycle%250Abehaviors%2520and%2520uncover%2520more%2520accurate%2520and%2520diverse%2520user%2520interests.%2520Specifically%252C%2520a%250Ahierarchical%2520clustering%2520method%2520groups%2520items%2520with%2520similar%2520characteristics%2520in%250Alife-cycle%2520behaviors%2520into%2520a%2520single%2520cluster%2520during%2520the%2520offline%2520phase.%2520By%250Alimiting%2520the%2520size%2520of%2520clusters%252C%2520we%2520can%2520compress%2520behavior%2520sequences%2520well%2520beyond%250Athe%2520magnitude%2520of%252010%255E5%2520to%2520a%2520length%2520manageable%2520for%2520online%2520inference%2520in%2520GSU%250Aretrieval.%2520Cluster-aware%2520target%2520attention%2520extracts%2520comprehensive%2520and%250Amulti-faceted%2520long-term%2520interests%2520of%2520users%252C%2520thereby%2520making%2520the%2520final%250Arecommendation%2520results%2520more%2520accurate%2520and%2520diverse.%2520Extensive%2520offline%2520experiments%250Aon%2520a%2520multi-billion-scale%2520industrial%2520dataset%2520and%2520online%2520A/B%2520tests%2520have%250Ademonstrated%2520the%2520effectiveness%2520of%2520TWIN-V2.%2520Under%2520an%2520efficient%2520deployment%250Aframework%252C%2520TWIN-V2%2520has%2520been%2520successfully%2520deployed%2520to%2520the%2520primary%2520traffic%2520that%250Aserves%2520hundreds%2520of%2520millions%2520of%2520daily%2520active%2520users%2520at%2520Kuaishou.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TWIN%20V2%3A%20Scaling%20Ultra-Long%20User%20Behavior%20Sequence%20Modeling%20for%20Enhanced%0A%20%20CTR%20Prediction%20at%20Kuaishou&entry.906535625=Zihua%20Si%20and%20Lin%20Guan%20and%20ZhongXiang%20Sun%20and%20Xiaoxue%20Zang%20and%20Jing%20Lu%20and%20Yiqun%20Hui%20and%20Xingchao%20Cao%20and%20Zeyu%20Yang%20and%20Yichen%20Zheng%20and%20Dewei%20Leng%20and%20Kai%20Zheng%20and%20Chenbin%20Zhang%20and%20Yanan%20Niu%20and%20Yang%20Song%20and%20Kun%20Gai&entry.1292438233=%20%20The%20significance%20of%20modeling%20long-term%20user%20interests%20for%20CTR%20prediction%0Atasks%20in%20large-scale%20recommendation%20systems%20is%20progressively%20gaining%20attention%0Aamong%20researchers%20and%20practitioners.%20Existing%20work%2C%20such%20as%20SIM%20and%20TWIN%2C%0Atypically%20employs%20a%20two-stage%20approach%20to%20model%20long-term%20user%20behavior%0Asequences%20for%20efficiency%20concerns.%20The%20first%20stage%20rapidly%20retrieves%20a%20subset%0Aof%20sequences%20related%20to%20the%20target%20item%20from%20a%20long%20sequence%20using%20a%0Asearch-based%20mechanism%20namely%20the%20General%20Search%20Unit%20%28GSU%29%2C%20while%20the%20second%0Astage%20calculates%20the%20interest%20scores%20using%20the%20Exact%20Search%20Unit%20%28ESU%29%20on%20the%0Aretrieved%20results.%20Given%20the%20extensive%20length%20of%20user%20behavior%20sequences%0Aspanning%20the%20entire%20life%20cycle%2C%20potentially%20reaching%20up%20to%2010%5E6%20in%20scale%2C%20there%0Ais%20currently%20no%20effective%20solution%20for%20fully%20modeling%20such%20expansive%20user%0Ainterests.%20To%20overcome%20this%20issue%2C%20we%20introduced%20TWIN-V2%2C%20an%20enhancement%20of%0ATWIN%2C%20where%20a%20divide-and-conquer%20approach%20is%20applied%20to%20compress%20life-cycle%0Abehaviors%20and%20uncover%20more%20accurate%20and%20diverse%20user%20interests.%20Specifically%2C%20a%0Ahierarchical%20clustering%20method%20groups%20items%20with%20similar%20characteristics%20in%0Alife-cycle%20behaviors%20into%20a%20single%20cluster%20during%20the%20offline%20phase.%20By%0Alimiting%20the%20size%20of%20clusters%2C%20we%20can%20compress%20behavior%20sequences%20well%20beyond%0Athe%20magnitude%20of%2010%5E5%20to%20a%20length%20manageable%20for%20online%20inference%20in%20GSU%0Aretrieval.%20Cluster-aware%20target%20attention%20extracts%20comprehensive%20and%0Amulti-faceted%20long-term%20interests%20of%20users%2C%20thereby%20making%20the%20final%0Arecommendation%20results%20more%20accurate%20and%20diverse.%20Extensive%20offline%20experiments%0Aon%20a%20multi-billion-scale%20industrial%20dataset%20and%20online%20A/B%20tests%20have%0Ademonstrated%20the%20effectiveness%20of%20TWIN-V2.%20Under%20an%20efficient%20deployment%0Aframework%2C%20TWIN-V2%20has%20been%20successfully%20deployed%20to%20the%20primary%20traffic%20that%0Aserves%20hundreds%20of%20millions%20of%20daily%20active%20users%20at%20Kuaishou.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16357v1&entry.124074799=Read"},
{"title": "DreamVTON: Customizing 3D Virtual Try-on with Personalized Diffusion\n  Models", "author": "Zhenyu Xie and Haoye Dong and Yufei Gao and Zehua Ma and Xiaodan Liang", "abstract": "  Image-based 3D Virtual Try-ON (VTON) aims to sculpt the 3D human according to\nperson and clothes images, which is data-efficient (i.e., getting rid of\nexpensive 3D data) but challenging. Recent text-to-3D methods achieve\nremarkable improvement in high-fidelity 3D human generation, demonstrating its\npotential for 3D virtual try-on. Inspired by the impressive success of\npersonalized diffusion models (e.g., Dreambooth and LoRA) for 2D VTON, it is\nstraightforward to achieve 3D VTON by integrating the personalization technique\ninto the diffusion-based text-to-3D framework. However, employing the\npersonalized module in a pre-trained diffusion model (e.g., StableDiffusion\n(SD)) would degrade the model's capability for multi-view or multi-domain\nsynthesis, which is detrimental to the geometry and texture optimization guided\nby Score Distillation Sampling (SDS) loss. In this work, we propose a novel\ncustomizing 3D human try-on model, named \\textbf{DreamVTON}, to separately\noptimize the geometry and texture of the 3D human. Specifically, a personalized\nSD with multi-concept LoRA is proposed to provide the generative prior about\nthe specific person and clothes, while a Densepose-guided ControlNet is\nexploited to guarantee consistent prior about body pose across various camera\nviews. Besides, to avoid the inconsistent multi-view priors from the\npersonalized SD dominating the optimization, DreamVTON introduces a\ntemplate-based optimization mechanism, which employs mask templates for\ngeometry shape learning and normal/RGB templates for geometry/texture details\nlearning. Furthermore, for the geometry optimization phase, DreamVTON\nintegrates a normal-style LoRA into personalized SD to enhance normal map\ngenerative prior, facilitating smooth geometry modeling.\n", "link": "http://arxiv.org/abs/2407.16511v1", "date": "2024-07-23", "relevancy": 1.9319, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6579}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6487}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamVTON%3A%20Customizing%203D%20Virtual%20Try-on%20with%20Personalized%20Diffusion%0A%20%20Models&body=Title%3A%20DreamVTON%3A%20Customizing%203D%20Virtual%20Try-on%20with%20Personalized%20Diffusion%0A%20%20Models%0AAuthor%3A%20Zhenyu%20Xie%20and%20Haoye%20Dong%20and%20Yufei%20Gao%20and%20Zehua%20Ma%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Image-based%203D%20Virtual%20Try-ON%20%28VTON%29%20aims%20to%20sculpt%20the%203D%20human%20according%20to%0Aperson%20and%20clothes%20images%2C%20which%20is%20data-efficient%20%28i.e.%2C%20getting%20rid%20of%0Aexpensive%203D%20data%29%20but%20challenging.%20Recent%20text-to-3D%20methods%20achieve%0Aremarkable%20improvement%20in%20high-fidelity%203D%20human%20generation%2C%20demonstrating%20its%0Apotential%20for%203D%20virtual%20try-on.%20Inspired%20by%20the%20impressive%20success%20of%0Apersonalized%20diffusion%20models%20%28e.g.%2C%20Dreambooth%20and%20LoRA%29%20for%202D%20VTON%2C%20it%20is%0Astraightforward%20to%20achieve%203D%20VTON%20by%20integrating%20the%20personalization%20technique%0Ainto%20the%20diffusion-based%20text-to-3D%20framework.%20However%2C%20employing%20the%0Apersonalized%20module%20in%20a%20pre-trained%20diffusion%20model%20%28e.g.%2C%20StableDiffusion%0A%28SD%29%29%20would%20degrade%20the%20model%27s%20capability%20for%20multi-view%20or%20multi-domain%0Asynthesis%2C%20which%20is%20detrimental%20to%20the%20geometry%20and%20texture%20optimization%20guided%0Aby%20Score%20Distillation%20Sampling%20%28SDS%29%20loss.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Acustomizing%203D%20human%20try-on%20model%2C%20named%20%5Ctextbf%7BDreamVTON%7D%2C%20to%20separately%0Aoptimize%20the%20geometry%20and%20texture%20of%20the%203D%20human.%20Specifically%2C%20a%20personalized%0ASD%20with%20multi-concept%20LoRA%20is%20proposed%20to%20provide%20the%20generative%20prior%20about%0Athe%20specific%20person%20and%20clothes%2C%20while%20a%20Densepose-guided%20ControlNet%20is%0Aexploited%20to%20guarantee%20consistent%20prior%20about%20body%20pose%20across%20various%20camera%0Aviews.%20Besides%2C%20to%20avoid%20the%20inconsistent%20multi-view%20priors%20from%20the%0Apersonalized%20SD%20dominating%20the%20optimization%2C%20DreamVTON%20introduces%20a%0Atemplate-based%20optimization%20mechanism%2C%20which%20employs%20mask%20templates%20for%0Ageometry%20shape%20learning%20and%20normal/RGB%20templates%20for%20geometry/texture%20details%0Alearning.%20Furthermore%2C%20for%20the%20geometry%20optimization%20phase%2C%20DreamVTON%0Aintegrates%20a%20normal-style%20LoRA%20into%20personalized%20SD%20to%20enhance%20normal%20map%0Agenerative%20prior%2C%20facilitating%20smooth%20geometry%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamVTON%253A%2520Customizing%25203D%2520Virtual%2520Try-on%2520with%2520Personalized%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DZhenyu%2520Xie%2520and%2520Haoye%2520Dong%2520and%2520Yufei%2520Gao%2520and%2520Zehua%2520Ma%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Image-based%25203D%2520Virtual%2520Try-ON%2520%2528VTON%2529%2520aims%2520to%2520sculpt%2520the%25203D%2520human%2520according%2520to%250Aperson%2520and%2520clothes%2520images%252C%2520which%2520is%2520data-efficient%2520%2528i.e.%252C%2520getting%2520rid%2520of%250Aexpensive%25203D%2520data%2529%2520but%2520challenging.%2520Recent%2520text-to-3D%2520methods%2520achieve%250Aremarkable%2520improvement%2520in%2520high-fidelity%25203D%2520human%2520generation%252C%2520demonstrating%2520its%250Apotential%2520for%25203D%2520virtual%2520try-on.%2520Inspired%2520by%2520the%2520impressive%2520success%2520of%250Apersonalized%2520diffusion%2520models%2520%2528e.g.%252C%2520Dreambooth%2520and%2520LoRA%2529%2520for%25202D%2520VTON%252C%2520it%2520is%250Astraightforward%2520to%2520achieve%25203D%2520VTON%2520by%2520integrating%2520the%2520personalization%2520technique%250Ainto%2520the%2520diffusion-based%2520text-to-3D%2520framework.%2520However%252C%2520employing%2520the%250Apersonalized%2520module%2520in%2520a%2520pre-trained%2520diffusion%2520model%2520%2528e.g.%252C%2520StableDiffusion%250A%2528SD%2529%2529%2520would%2520degrade%2520the%2520model%2527s%2520capability%2520for%2520multi-view%2520or%2520multi-domain%250Asynthesis%252C%2520which%2520is%2520detrimental%2520to%2520the%2520geometry%2520and%2520texture%2520optimization%2520guided%250Aby%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520loss.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Acustomizing%25203D%2520human%2520try-on%2520model%252C%2520named%2520%255Ctextbf%257BDreamVTON%257D%252C%2520to%2520separately%250Aoptimize%2520the%2520geometry%2520and%2520texture%2520of%2520the%25203D%2520human.%2520Specifically%252C%2520a%2520personalized%250ASD%2520with%2520multi-concept%2520LoRA%2520is%2520proposed%2520to%2520provide%2520the%2520generative%2520prior%2520about%250Athe%2520specific%2520person%2520and%2520clothes%252C%2520while%2520a%2520Densepose-guided%2520ControlNet%2520is%250Aexploited%2520to%2520guarantee%2520consistent%2520prior%2520about%2520body%2520pose%2520across%2520various%2520camera%250Aviews.%2520Besides%252C%2520to%2520avoid%2520the%2520inconsistent%2520multi-view%2520priors%2520from%2520the%250Apersonalized%2520SD%2520dominating%2520the%2520optimization%252C%2520DreamVTON%2520introduces%2520a%250Atemplate-based%2520optimization%2520mechanism%252C%2520which%2520employs%2520mask%2520templates%2520for%250Ageometry%2520shape%2520learning%2520and%2520normal/RGB%2520templates%2520for%2520geometry/texture%2520details%250Alearning.%2520Furthermore%252C%2520for%2520the%2520geometry%2520optimization%2520phase%252C%2520DreamVTON%250Aintegrates%2520a%2520normal-style%2520LoRA%2520into%2520personalized%2520SD%2520to%2520enhance%2520normal%2520map%250Agenerative%2520prior%252C%2520facilitating%2520smooth%2520geometry%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamVTON%3A%20Customizing%203D%20Virtual%20Try-on%20with%20Personalized%20Diffusion%0A%20%20Models&entry.906535625=Zhenyu%20Xie%20and%20Haoye%20Dong%20and%20Yufei%20Gao%20and%20Zehua%20Ma%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Image-based%203D%20Virtual%20Try-ON%20%28VTON%29%20aims%20to%20sculpt%20the%203D%20human%20according%20to%0Aperson%20and%20clothes%20images%2C%20which%20is%20data-efficient%20%28i.e.%2C%20getting%20rid%20of%0Aexpensive%203D%20data%29%20but%20challenging.%20Recent%20text-to-3D%20methods%20achieve%0Aremarkable%20improvement%20in%20high-fidelity%203D%20human%20generation%2C%20demonstrating%20its%0Apotential%20for%203D%20virtual%20try-on.%20Inspired%20by%20the%20impressive%20success%20of%0Apersonalized%20diffusion%20models%20%28e.g.%2C%20Dreambooth%20and%20LoRA%29%20for%202D%20VTON%2C%20it%20is%0Astraightforward%20to%20achieve%203D%20VTON%20by%20integrating%20the%20personalization%20technique%0Ainto%20the%20diffusion-based%20text-to-3D%20framework.%20However%2C%20employing%20the%0Apersonalized%20module%20in%20a%20pre-trained%20diffusion%20model%20%28e.g.%2C%20StableDiffusion%0A%28SD%29%29%20would%20degrade%20the%20model%27s%20capability%20for%20multi-view%20or%20multi-domain%0Asynthesis%2C%20which%20is%20detrimental%20to%20the%20geometry%20and%20texture%20optimization%20guided%0Aby%20Score%20Distillation%20Sampling%20%28SDS%29%20loss.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Acustomizing%203D%20human%20try-on%20model%2C%20named%20%5Ctextbf%7BDreamVTON%7D%2C%20to%20separately%0Aoptimize%20the%20geometry%20and%20texture%20of%20the%203D%20human.%20Specifically%2C%20a%20personalized%0ASD%20with%20multi-concept%20LoRA%20is%20proposed%20to%20provide%20the%20generative%20prior%20about%0Athe%20specific%20person%20and%20clothes%2C%20while%20a%20Densepose-guided%20ControlNet%20is%0Aexploited%20to%20guarantee%20consistent%20prior%20about%20body%20pose%20across%20various%20camera%0Aviews.%20Besides%2C%20to%20avoid%20the%20inconsistent%20multi-view%20priors%20from%20the%0Apersonalized%20SD%20dominating%20the%20optimization%2C%20DreamVTON%20introduces%20a%0Atemplate-based%20optimization%20mechanism%2C%20which%20employs%20mask%20templates%20for%0Ageometry%20shape%20learning%20and%20normal/RGB%20templates%20for%20geometry/texture%20details%0Alearning.%20Furthermore%2C%20for%20the%20geometry%20optimization%20phase%2C%20DreamVTON%0Aintegrates%20a%20normal-style%20LoRA%20into%20personalized%20SD%20to%20enhance%20normal%20map%0Agenerative%20prior%2C%20facilitating%20smooth%20geometry%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16511v1&entry.124074799=Read"},
{"title": "Enhancing Encrypted Internet Traffic Classification Through Advanced\n  Data Augmentation Techniques", "author": "Yehonatan Zion and Porat Aharon and Ran Dubin and Amit Dvir and Chen Hajaj", "abstract": "  The increasing popularity of online services has made Internet Traffic\nClassification a critical field of study. However, the rapid development of\ninternet protocols and encryption limits usable data availability. This paper\naddresses the challenges of classifying encrypted internet traffic, focusing on\nthe scarcity of open-source datasets and limitations of existing ones. We\npropose two Data Augmentation (DA) techniques to synthetically generate data\nbased on real samples: Average augmentation and MTU augmentation. Both\naugmentations are aimed to improve the performance of the classifier, each from\na different perspective: The Average augmentation aims to increase dataset size\nby generating new synthetic samples, while the MTU augmentation enhances\nclassifier robustness to varying Maximum Transmission Units (MTUs). Our\nexperiments, conducted on two well-known academic datasets and a commercial\ndataset, demonstrate the effectiveness of these approaches in improving model\nperformance and mitigating constraints associated with limited and homogeneous\ndatasets. Our findings underscore the potential of data augmentation in\naddressing the challenges of modern internet traffic classification.\nSpecifically, we show that our augmentation techniques significantly enhance\nencrypted traffic classification models. This improvement can positively impact\nuser Quality of Experience (QoE) by more accurately classifying traffic as\nvideo streaming (e.g., YouTube) or chat (e.g., Google Chat). Additionally, it\ncan enhance Quality of Service (QoS) for file downloading activities (e.g.,\nGoogle Docs).\n", "link": "http://arxiv.org/abs/2407.16539v1", "date": "2024-07-23", "relevancy": 1.9297, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4852}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4848}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Encrypted%20Internet%20Traffic%20Classification%20Through%20Advanced%0A%20%20Data%20Augmentation%20Techniques&body=Title%3A%20Enhancing%20Encrypted%20Internet%20Traffic%20Classification%20Through%20Advanced%0A%20%20Data%20Augmentation%20Techniques%0AAuthor%3A%20Yehonatan%20Zion%20and%20Porat%20Aharon%20and%20Ran%20Dubin%20and%20Amit%20Dvir%20and%20Chen%20Hajaj%0AAbstract%3A%20%20%20The%20increasing%20popularity%20of%20online%20services%20has%20made%20Internet%20Traffic%0AClassification%20a%20critical%20field%20of%20study.%20However%2C%20the%20rapid%20development%20of%0Ainternet%20protocols%20and%20encryption%20limits%20usable%20data%20availability.%20This%20paper%0Aaddresses%20the%20challenges%20of%20classifying%20encrypted%20internet%20traffic%2C%20focusing%20on%0Athe%20scarcity%20of%20open-source%20datasets%20and%20limitations%20of%20existing%20ones.%20We%0Apropose%20two%20Data%20Augmentation%20%28DA%29%20techniques%20to%20synthetically%20generate%20data%0Abased%20on%20real%20samples%3A%20Average%20augmentation%20and%20MTU%20augmentation.%20Both%0Aaugmentations%20are%20aimed%20to%20improve%20the%20performance%20of%20the%20classifier%2C%20each%20from%0Aa%20different%20perspective%3A%20The%20Average%20augmentation%20aims%20to%20increase%20dataset%20size%0Aby%20generating%20new%20synthetic%20samples%2C%20while%20the%20MTU%20augmentation%20enhances%0Aclassifier%20robustness%20to%20varying%20Maximum%20Transmission%20Units%20%28MTUs%29.%20Our%0Aexperiments%2C%20conducted%20on%20two%20well-known%20academic%20datasets%20and%20a%20commercial%0Adataset%2C%20demonstrate%20the%20effectiveness%20of%20these%20approaches%20in%20improving%20model%0Aperformance%20and%20mitigating%20constraints%20associated%20with%20limited%20and%20homogeneous%0Adatasets.%20Our%20findings%20underscore%20the%20potential%20of%20data%20augmentation%20in%0Aaddressing%20the%20challenges%20of%20modern%20internet%20traffic%20classification.%0ASpecifically%2C%20we%20show%20that%20our%20augmentation%20techniques%20significantly%20enhance%0Aencrypted%20traffic%20classification%20models.%20This%20improvement%20can%20positively%20impact%0Auser%20Quality%20of%20Experience%20%28QoE%29%20by%20more%20accurately%20classifying%20traffic%20as%0Avideo%20streaming%20%28e.g.%2C%20YouTube%29%20or%20chat%20%28e.g.%2C%20Google%20Chat%29.%20Additionally%2C%20it%0Acan%20enhance%20Quality%20of%20Service%20%28QoS%29%20for%20file%20downloading%20activities%20%28e.g.%2C%0AGoogle%20Docs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Encrypted%2520Internet%2520Traffic%2520Classification%2520Through%2520Advanced%250A%2520%2520Data%2520Augmentation%2520Techniques%26entry.906535625%3DYehonatan%2520Zion%2520and%2520Porat%2520Aharon%2520and%2520Ran%2520Dubin%2520and%2520Amit%2520Dvir%2520and%2520Chen%2520Hajaj%26entry.1292438233%3D%2520%2520The%2520increasing%2520popularity%2520of%2520online%2520services%2520has%2520made%2520Internet%2520Traffic%250AClassification%2520a%2520critical%2520field%2520of%2520study.%2520However%252C%2520the%2520rapid%2520development%2520of%250Ainternet%2520protocols%2520and%2520encryption%2520limits%2520usable%2520data%2520availability.%2520This%2520paper%250Aaddresses%2520the%2520challenges%2520of%2520classifying%2520encrypted%2520internet%2520traffic%252C%2520focusing%2520on%250Athe%2520scarcity%2520of%2520open-source%2520datasets%2520and%2520limitations%2520of%2520existing%2520ones.%2520We%250Apropose%2520two%2520Data%2520Augmentation%2520%2528DA%2529%2520techniques%2520to%2520synthetically%2520generate%2520data%250Abased%2520on%2520real%2520samples%253A%2520Average%2520augmentation%2520and%2520MTU%2520augmentation.%2520Both%250Aaugmentations%2520are%2520aimed%2520to%2520improve%2520the%2520performance%2520of%2520the%2520classifier%252C%2520each%2520from%250Aa%2520different%2520perspective%253A%2520The%2520Average%2520augmentation%2520aims%2520to%2520increase%2520dataset%2520size%250Aby%2520generating%2520new%2520synthetic%2520samples%252C%2520while%2520the%2520MTU%2520augmentation%2520enhances%250Aclassifier%2520robustness%2520to%2520varying%2520Maximum%2520Transmission%2520Units%2520%2528MTUs%2529.%2520Our%250Aexperiments%252C%2520conducted%2520on%2520two%2520well-known%2520academic%2520datasets%2520and%2520a%2520commercial%250Adataset%252C%2520demonstrate%2520the%2520effectiveness%2520of%2520these%2520approaches%2520in%2520improving%2520model%250Aperformance%2520and%2520mitigating%2520constraints%2520associated%2520with%2520limited%2520and%2520homogeneous%250Adatasets.%2520Our%2520findings%2520underscore%2520the%2520potential%2520of%2520data%2520augmentation%2520in%250Aaddressing%2520the%2520challenges%2520of%2520modern%2520internet%2520traffic%2520classification.%250ASpecifically%252C%2520we%2520show%2520that%2520our%2520augmentation%2520techniques%2520significantly%2520enhance%250Aencrypted%2520traffic%2520classification%2520models.%2520This%2520improvement%2520can%2520positively%2520impact%250Auser%2520Quality%2520of%2520Experience%2520%2528QoE%2529%2520by%2520more%2520accurately%2520classifying%2520traffic%2520as%250Avideo%2520streaming%2520%2528e.g.%252C%2520YouTube%2529%2520or%2520chat%2520%2528e.g.%252C%2520Google%2520Chat%2529.%2520Additionally%252C%2520it%250Acan%2520enhance%2520Quality%2520of%2520Service%2520%2528QoS%2529%2520for%2520file%2520downloading%2520activities%2520%2528e.g.%252C%250AGoogle%2520Docs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Encrypted%20Internet%20Traffic%20Classification%20Through%20Advanced%0A%20%20Data%20Augmentation%20Techniques&entry.906535625=Yehonatan%20Zion%20and%20Porat%20Aharon%20and%20Ran%20Dubin%20and%20Amit%20Dvir%20and%20Chen%20Hajaj&entry.1292438233=%20%20The%20increasing%20popularity%20of%20online%20services%20has%20made%20Internet%20Traffic%0AClassification%20a%20critical%20field%20of%20study.%20However%2C%20the%20rapid%20development%20of%0Ainternet%20protocols%20and%20encryption%20limits%20usable%20data%20availability.%20This%20paper%0Aaddresses%20the%20challenges%20of%20classifying%20encrypted%20internet%20traffic%2C%20focusing%20on%0Athe%20scarcity%20of%20open-source%20datasets%20and%20limitations%20of%20existing%20ones.%20We%0Apropose%20two%20Data%20Augmentation%20%28DA%29%20techniques%20to%20synthetically%20generate%20data%0Abased%20on%20real%20samples%3A%20Average%20augmentation%20and%20MTU%20augmentation.%20Both%0Aaugmentations%20are%20aimed%20to%20improve%20the%20performance%20of%20the%20classifier%2C%20each%20from%0Aa%20different%20perspective%3A%20The%20Average%20augmentation%20aims%20to%20increase%20dataset%20size%0Aby%20generating%20new%20synthetic%20samples%2C%20while%20the%20MTU%20augmentation%20enhances%0Aclassifier%20robustness%20to%20varying%20Maximum%20Transmission%20Units%20%28MTUs%29.%20Our%0Aexperiments%2C%20conducted%20on%20two%20well-known%20academic%20datasets%20and%20a%20commercial%0Adataset%2C%20demonstrate%20the%20effectiveness%20of%20these%20approaches%20in%20improving%20model%0Aperformance%20and%20mitigating%20constraints%20associated%20with%20limited%20and%20homogeneous%0Adatasets.%20Our%20findings%20underscore%20the%20potential%20of%20data%20augmentation%20in%0Aaddressing%20the%20challenges%20of%20modern%20internet%20traffic%20classification.%0ASpecifically%2C%20we%20show%20that%20our%20augmentation%20techniques%20significantly%20enhance%0Aencrypted%20traffic%20classification%20models.%20This%20improvement%20can%20positively%20impact%0Auser%20Quality%20of%20Experience%20%28QoE%29%20by%20more%20accurately%20classifying%20traffic%20as%0Avideo%20streaming%20%28e.g.%2C%20YouTube%29%20or%20chat%20%28e.g.%2C%20Google%20Chat%29.%20Additionally%2C%20it%0Acan%20enhance%20Quality%20of%20Service%20%28QoS%29%20for%20file%20downloading%20activities%20%28e.g.%2C%0AGoogle%20Docs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16539v1&entry.124074799=Read"},
{"title": "Era Splitting: Invariant Learning for Decision Trees", "author": "Timothy DeLise", "abstract": "  Real-life machine learning problems exhibit distributional shifts in the data\nfrom one time to another or from one place to another. This behavior is beyond\nthe scope of the traditional empirical risk minimization paradigm, which\nassumes i.i.d. distribution of data over time and across locations. The\nemerging field of out-of-distribution (OOD) generalization addresses this\nreality with new theory and algorithms which incorporate \"environmental\", or\n\"era-wise\" information into the algorithms. So far, most research has been\nfocused on linear models and/or neural networks . In this research we develop\ntwo new splitting criteria for decision trees, which allow us to apply ideas\nfrom OOD generalization research to decision tree models, namely, gradient\nboosting decision trees (GBDTs). The new splitting criteria use era-wise\ninformation associated with the data to grow tree-based models that are optimal\nacross all disjoint eras in the data, instead of optimal over the entire data\nset pooled together, which is the default setting. In this paper, two new\nsplitting criteria are defined and analyzed theoretically. Effectiveness is\ntested on four experiments, ranging from simple, synthetic to complex,\nreal-world applications. In particular we cast the OOD domain-adaptation\nproblem in the context of financial markets, where the new models out-perform\nstate-of-the-art GBDT models on the Numerai data set. The new criteria are\nincorporated into the Scikit-Learn code base and made freely available online.\n", "link": "http://arxiv.org/abs/2309.14496v5", "date": "2024-07-23", "relevancy": 1.9246, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4979}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4748}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Era%20Splitting%3A%20Invariant%20Learning%20for%20Decision%20Trees&body=Title%3A%20Era%20Splitting%3A%20Invariant%20Learning%20for%20Decision%20Trees%0AAuthor%3A%20Timothy%20DeLise%0AAbstract%3A%20%20%20Real-life%20machine%20learning%20problems%20exhibit%20distributional%20shifts%20in%20the%20data%0Afrom%20one%20time%20to%20another%20or%20from%20one%20place%20to%20another.%20This%20behavior%20is%20beyond%0Athe%20scope%20of%20the%20traditional%20empirical%20risk%20minimization%20paradigm%2C%20which%0Aassumes%20i.i.d.%20distribution%20of%20data%20over%20time%20and%20across%20locations.%20The%0Aemerging%20field%20of%20out-of-distribution%20%28OOD%29%20generalization%20addresses%20this%0Areality%20with%20new%20theory%20and%20algorithms%20which%20incorporate%20%22environmental%22%2C%20or%0A%22era-wise%22%20information%20into%20the%20algorithms.%20So%20far%2C%20most%20research%20has%20been%0Afocused%20on%20linear%20models%20and/or%20neural%20networks%20.%20In%20this%20research%20we%20develop%0Atwo%20new%20splitting%20criteria%20for%20decision%20trees%2C%20which%20allow%20us%20to%20apply%20ideas%0Afrom%20OOD%20generalization%20research%20to%20decision%20tree%20models%2C%20namely%2C%20gradient%0Aboosting%20decision%20trees%20%28GBDTs%29.%20The%20new%20splitting%20criteria%20use%20era-wise%0Ainformation%20associated%20with%20the%20data%20to%20grow%20tree-based%20models%20that%20are%20optimal%0Aacross%20all%20disjoint%20eras%20in%20the%20data%2C%20instead%20of%20optimal%20over%20the%20entire%20data%0Aset%20pooled%20together%2C%20which%20is%20the%20default%20setting.%20In%20this%20paper%2C%20two%20new%0Asplitting%20criteria%20are%20defined%20and%20analyzed%20theoretically.%20Effectiveness%20is%0Atested%20on%20four%20experiments%2C%20ranging%20from%20simple%2C%20synthetic%20to%20complex%2C%0Areal-world%20applications.%20In%20particular%20we%20cast%20the%20OOD%20domain-adaptation%0Aproblem%20in%20the%20context%20of%20financial%20markets%2C%20where%20the%20new%20models%20out-perform%0Astate-of-the-art%20GBDT%20models%20on%20the%20Numerai%20data%20set.%20The%20new%20criteria%20are%0Aincorporated%20into%20the%20Scikit-Learn%20code%20base%20and%20made%20freely%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14496v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEra%2520Splitting%253A%2520Invariant%2520Learning%2520for%2520Decision%2520Trees%26entry.906535625%3DTimothy%2520DeLise%26entry.1292438233%3D%2520%2520Real-life%2520machine%2520learning%2520problems%2520exhibit%2520distributional%2520shifts%2520in%2520the%2520data%250Afrom%2520one%2520time%2520to%2520another%2520or%2520from%2520one%2520place%2520to%2520another.%2520This%2520behavior%2520is%2520beyond%250Athe%2520scope%2520of%2520the%2520traditional%2520empirical%2520risk%2520minimization%2520paradigm%252C%2520which%250Aassumes%2520i.i.d.%2520distribution%2520of%2520data%2520over%2520time%2520and%2520across%2520locations.%2520The%250Aemerging%2520field%2520of%2520out-of-distribution%2520%2528OOD%2529%2520generalization%2520addresses%2520this%250Areality%2520with%2520new%2520theory%2520and%2520algorithms%2520which%2520incorporate%2520%2522environmental%2522%252C%2520or%250A%2522era-wise%2522%2520information%2520into%2520the%2520algorithms.%2520So%2520far%252C%2520most%2520research%2520has%2520been%250Afocused%2520on%2520linear%2520models%2520and/or%2520neural%2520networks%2520.%2520In%2520this%2520research%2520we%2520develop%250Atwo%2520new%2520splitting%2520criteria%2520for%2520decision%2520trees%252C%2520which%2520allow%2520us%2520to%2520apply%2520ideas%250Afrom%2520OOD%2520generalization%2520research%2520to%2520decision%2520tree%2520models%252C%2520namely%252C%2520gradient%250Aboosting%2520decision%2520trees%2520%2528GBDTs%2529.%2520The%2520new%2520splitting%2520criteria%2520use%2520era-wise%250Ainformation%2520associated%2520with%2520the%2520data%2520to%2520grow%2520tree-based%2520models%2520that%2520are%2520optimal%250Aacross%2520all%2520disjoint%2520eras%2520in%2520the%2520data%252C%2520instead%2520of%2520optimal%2520over%2520the%2520entire%2520data%250Aset%2520pooled%2520together%252C%2520which%2520is%2520the%2520default%2520setting.%2520In%2520this%2520paper%252C%2520two%2520new%250Asplitting%2520criteria%2520are%2520defined%2520and%2520analyzed%2520theoretically.%2520Effectiveness%2520is%250Atested%2520on%2520four%2520experiments%252C%2520ranging%2520from%2520simple%252C%2520synthetic%2520to%2520complex%252C%250Areal-world%2520applications.%2520In%2520particular%2520we%2520cast%2520the%2520OOD%2520domain-adaptation%250Aproblem%2520in%2520the%2520context%2520of%2520financial%2520markets%252C%2520where%2520the%2520new%2520models%2520out-perform%250Astate-of-the-art%2520GBDT%2520models%2520on%2520the%2520Numerai%2520data%2520set.%2520The%2520new%2520criteria%2520are%250Aincorporated%2520into%2520the%2520Scikit-Learn%2520code%2520base%2520and%2520made%2520freely%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14496v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Era%20Splitting%3A%20Invariant%20Learning%20for%20Decision%20Trees&entry.906535625=Timothy%20DeLise&entry.1292438233=%20%20Real-life%20machine%20learning%20problems%20exhibit%20distributional%20shifts%20in%20the%20data%0Afrom%20one%20time%20to%20another%20or%20from%20one%20place%20to%20another.%20This%20behavior%20is%20beyond%0Athe%20scope%20of%20the%20traditional%20empirical%20risk%20minimization%20paradigm%2C%20which%0Aassumes%20i.i.d.%20distribution%20of%20data%20over%20time%20and%20across%20locations.%20The%0Aemerging%20field%20of%20out-of-distribution%20%28OOD%29%20generalization%20addresses%20this%0Areality%20with%20new%20theory%20and%20algorithms%20which%20incorporate%20%22environmental%22%2C%20or%0A%22era-wise%22%20information%20into%20the%20algorithms.%20So%20far%2C%20most%20research%20has%20been%0Afocused%20on%20linear%20models%20and/or%20neural%20networks%20.%20In%20this%20research%20we%20develop%0Atwo%20new%20splitting%20criteria%20for%20decision%20trees%2C%20which%20allow%20us%20to%20apply%20ideas%0Afrom%20OOD%20generalization%20research%20to%20decision%20tree%20models%2C%20namely%2C%20gradient%0Aboosting%20decision%20trees%20%28GBDTs%29.%20The%20new%20splitting%20criteria%20use%20era-wise%0Ainformation%20associated%20with%20the%20data%20to%20grow%20tree-based%20models%20that%20are%20optimal%0Aacross%20all%20disjoint%20eras%20in%20the%20data%2C%20instead%20of%20optimal%20over%20the%20entire%20data%0Aset%20pooled%20together%2C%20which%20is%20the%20default%20setting.%20In%20this%20paper%2C%20two%20new%0Asplitting%20criteria%20are%20defined%20and%20analyzed%20theoretically.%20Effectiveness%20is%0Atested%20on%20four%20experiments%2C%20ranging%20from%20simple%2C%20synthetic%20to%20complex%2C%0Areal-world%20applications.%20In%20particular%20we%20cast%20the%20OOD%20domain-adaptation%0Aproblem%20in%20the%20context%20of%20financial%20markets%2C%20where%20the%20new%20models%20out-perform%0Astate-of-the-art%20GBDT%20models%20on%20the%20Numerai%20data%20set.%20The%20new%20criteria%20are%0Aincorporated%20into%20the%20Scikit-Learn%20code%20base%20and%20made%20freely%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14496v5&entry.124074799=Read"},
{"title": "Interpretable Machine Learning for TabPFN", "author": "David Rundel and Julius Kobialka and Constantin von Crailsheim and Matthias Feurer and Thomas Nagler and David R\u00fcgamer", "abstract": "  The recently developed Prior-Data Fitted Networks (PFNs) have shown very\npromising results for applications in low-data regimes. The TabPFN model, a\nspecial case of PFNs for tabular data, is able to achieve state-of-the-art\nperformance on a variety of classification tasks while producing posterior\npredictive distributions in mere seconds by in-context learning without the\nneed for learning parameters or hyperparameter tuning. This makes TabPFN a very\nattractive option for a wide range of domain applications. However, a major\ndrawback of the method is its lack of interpretability. Therefore, we propose\nseveral adaptations of popular interpretability methods that we specifically\ndesign for TabPFN. By taking advantage of the unique properties of the model,\nour adaptations allow for more efficient computations than existing\nimplementations. In particular, we show how in-context learning facilitates the\nestimation of Shapley values by avoiding approximate retraining and enables the\nuse of Leave-One-Covariate-Out (LOCO) even when working with large-scale\nTransformers. In addition, we demonstrate how data valuation methods can be\nused to address scalability challenges of TabPFN. Our proposed methods are\nimplemented in a package tabpfn_iml and made available at\nhttps://github.com/david-rundel/tabpfn_iml.\n", "link": "http://arxiv.org/abs/2403.10923v2", "date": "2024-07-23", "relevancy": 1.9238, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4977}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4713}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Machine%20Learning%20for%20TabPFN&body=Title%3A%20Interpretable%20Machine%20Learning%20for%20TabPFN%0AAuthor%3A%20David%20Rundel%20and%20Julius%20Kobialka%20and%20Constantin%20von%20Crailsheim%20and%20Matthias%20Feurer%20and%20Thomas%20Nagler%20and%20David%20R%C3%BCgamer%0AAbstract%3A%20%20%20The%20recently%20developed%20Prior-Data%20Fitted%20Networks%20%28PFNs%29%20have%20shown%20very%0Apromising%20results%20for%20applications%20in%20low-data%20regimes.%20The%20TabPFN%20model%2C%20a%0Aspecial%20case%20of%20PFNs%20for%20tabular%20data%2C%20is%20able%20to%20achieve%20state-of-the-art%0Aperformance%20on%20a%20variety%20of%20classification%20tasks%20while%20producing%20posterior%0Apredictive%20distributions%20in%20mere%20seconds%20by%20in-context%20learning%20without%20the%0Aneed%20for%20learning%20parameters%20or%20hyperparameter%20tuning.%20This%20makes%20TabPFN%20a%20very%0Aattractive%20option%20for%20a%20wide%20range%20of%20domain%20applications.%20However%2C%20a%20major%0Adrawback%20of%20the%20method%20is%20its%20lack%20of%20interpretability.%20Therefore%2C%20we%20propose%0Aseveral%20adaptations%20of%20popular%20interpretability%20methods%20that%20we%20specifically%0Adesign%20for%20TabPFN.%20By%20taking%20advantage%20of%20the%20unique%20properties%20of%20the%20model%2C%0Aour%20adaptations%20allow%20for%20more%20efficient%20computations%20than%20existing%0Aimplementations.%20In%20particular%2C%20we%20show%20how%20in-context%20learning%20facilitates%20the%0Aestimation%20of%20Shapley%20values%20by%20avoiding%20approximate%20retraining%20and%20enables%20the%0Ause%20of%20Leave-One-Covariate-Out%20%28LOCO%29%20even%20when%20working%20with%20large-scale%0ATransformers.%20In%20addition%2C%20we%20demonstrate%20how%20data%20valuation%20methods%20can%20be%0Aused%20to%20address%20scalability%20challenges%20of%20TabPFN.%20Our%20proposed%20methods%20are%0Aimplemented%20in%20a%20package%20tabpfn_iml%20and%20made%20available%20at%0Ahttps%3A//github.com/david-rundel/tabpfn_iml.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10923v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Machine%2520Learning%2520for%2520TabPFN%26entry.906535625%3DDavid%2520Rundel%2520and%2520Julius%2520Kobialka%2520and%2520Constantin%2520von%2520Crailsheim%2520and%2520Matthias%2520Feurer%2520and%2520Thomas%2520Nagler%2520and%2520David%2520R%25C3%25BCgamer%26entry.1292438233%3D%2520%2520The%2520recently%2520developed%2520Prior-Data%2520Fitted%2520Networks%2520%2528PFNs%2529%2520have%2520shown%2520very%250Apromising%2520results%2520for%2520applications%2520in%2520low-data%2520regimes.%2520The%2520TabPFN%2520model%252C%2520a%250Aspecial%2520case%2520of%2520PFNs%2520for%2520tabular%2520data%252C%2520is%2520able%2520to%2520achieve%2520state-of-the-art%250Aperformance%2520on%2520a%2520variety%2520of%2520classification%2520tasks%2520while%2520producing%2520posterior%250Apredictive%2520distributions%2520in%2520mere%2520seconds%2520by%2520in-context%2520learning%2520without%2520the%250Aneed%2520for%2520learning%2520parameters%2520or%2520hyperparameter%2520tuning.%2520This%2520makes%2520TabPFN%2520a%2520very%250Aattractive%2520option%2520for%2520a%2520wide%2520range%2520of%2520domain%2520applications.%2520However%252C%2520a%2520major%250Adrawback%2520of%2520the%2520method%2520is%2520its%2520lack%2520of%2520interpretability.%2520Therefore%252C%2520we%2520propose%250Aseveral%2520adaptations%2520of%2520popular%2520interpretability%2520methods%2520that%2520we%2520specifically%250Adesign%2520for%2520TabPFN.%2520By%2520taking%2520advantage%2520of%2520the%2520unique%2520properties%2520of%2520the%2520model%252C%250Aour%2520adaptations%2520allow%2520for%2520more%2520efficient%2520computations%2520than%2520existing%250Aimplementations.%2520In%2520particular%252C%2520we%2520show%2520how%2520in-context%2520learning%2520facilitates%2520the%250Aestimation%2520of%2520Shapley%2520values%2520by%2520avoiding%2520approximate%2520retraining%2520and%2520enables%2520the%250Ause%2520of%2520Leave-One-Covariate-Out%2520%2528LOCO%2529%2520even%2520when%2520working%2520with%2520large-scale%250ATransformers.%2520In%2520addition%252C%2520we%2520demonstrate%2520how%2520data%2520valuation%2520methods%2520can%2520be%250Aused%2520to%2520address%2520scalability%2520challenges%2520of%2520TabPFN.%2520Our%2520proposed%2520methods%2520are%250Aimplemented%2520in%2520a%2520package%2520tabpfn_iml%2520and%2520made%2520available%2520at%250Ahttps%253A//github.com/david-rundel/tabpfn_iml.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10923v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Machine%20Learning%20for%20TabPFN&entry.906535625=David%20Rundel%20and%20Julius%20Kobialka%20and%20Constantin%20von%20Crailsheim%20and%20Matthias%20Feurer%20and%20Thomas%20Nagler%20and%20David%20R%C3%BCgamer&entry.1292438233=%20%20The%20recently%20developed%20Prior-Data%20Fitted%20Networks%20%28PFNs%29%20have%20shown%20very%0Apromising%20results%20for%20applications%20in%20low-data%20regimes.%20The%20TabPFN%20model%2C%20a%0Aspecial%20case%20of%20PFNs%20for%20tabular%20data%2C%20is%20able%20to%20achieve%20state-of-the-art%0Aperformance%20on%20a%20variety%20of%20classification%20tasks%20while%20producing%20posterior%0Apredictive%20distributions%20in%20mere%20seconds%20by%20in-context%20learning%20without%20the%0Aneed%20for%20learning%20parameters%20or%20hyperparameter%20tuning.%20This%20makes%20TabPFN%20a%20very%0Aattractive%20option%20for%20a%20wide%20range%20of%20domain%20applications.%20However%2C%20a%20major%0Adrawback%20of%20the%20method%20is%20its%20lack%20of%20interpretability.%20Therefore%2C%20we%20propose%0Aseveral%20adaptations%20of%20popular%20interpretability%20methods%20that%20we%20specifically%0Adesign%20for%20TabPFN.%20By%20taking%20advantage%20of%20the%20unique%20properties%20of%20the%20model%2C%0Aour%20adaptations%20allow%20for%20more%20efficient%20computations%20than%20existing%0Aimplementations.%20In%20particular%2C%20we%20show%20how%20in-context%20learning%20facilitates%20the%0Aestimation%20of%20Shapley%20values%20by%20avoiding%20approximate%20retraining%20and%20enables%20the%0Ause%20of%20Leave-One-Covariate-Out%20%28LOCO%29%20even%20when%20working%20with%20large-scale%0ATransformers.%20In%20addition%2C%20we%20demonstrate%20how%20data%20valuation%20methods%20can%20be%0Aused%20to%20address%20scalability%20challenges%20of%20TabPFN.%20Our%20proposed%20methods%20are%0Aimplemented%20in%20a%20package%20tabpfn_iml%20and%20made%20available%20at%0Ahttps%3A//github.com/david-rundel/tabpfn_iml.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10923v2&entry.124074799=Read"},
{"title": "Structured Inverse-Free Natural Gradient: Memory-Efficient &\n  Numerically-Stable KFAC", "author": "Wu Lin and Felix Dangel and Runa Eschenhagen and Kirill Neklyudov and Agustinus Kristiadi and Richard E. Turner and Alireza Makhzani", "abstract": "  Second-order methods such as KFAC can be useful for neural net training.\nHowever, they are often memory-inefficient since their preconditioning\nKronecker factors are dense, and numerically unstable in low precision as they\nrequire matrix inversion or decomposition. These limitations render such\nmethods unpopular for modern mixed-precision training. We address them by (i)\nformulating an inverse-free KFAC update and (ii) imposing structures in the\nKronecker factors, resulting in structured inverse-free natural gradient\ndescent (SINGD). On modern neural networks, we show that SINGD is\nmemory-efficient and numerically robust, in contrast to KFAC, and often\noutperforms AdamW even in half precision. Our work closes a gap between first-\nand second-order methods in modern low-precision training.\n", "link": "http://arxiv.org/abs/2312.05705v4", "date": "2024-07-23", "relevancy": 1.9134, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4805}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4793}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Inverse-Free%20Natural%20Gradient%3A%20Memory-Efficient%20%26%0A%20%20Numerically-Stable%20KFAC&body=Title%3A%20Structured%20Inverse-Free%20Natural%20Gradient%3A%20Memory-Efficient%20%26%0A%20%20Numerically-Stable%20KFAC%0AAuthor%3A%20Wu%20Lin%20and%20Felix%20Dangel%20and%20Runa%20Eschenhagen%20and%20Kirill%20Neklyudov%20and%20Agustinus%20Kristiadi%20and%20Richard%20E.%20Turner%20and%20Alireza%20Makhzani%0AAbstract%3A%20%20%20Second-order%20methods%20such%20as%20KFAC%20can%20be%20useful%20for%20neural%20net%20training.%0AHowever%2C%20they%20are%20often%20memory-inefficient%20since%20their%20preconditioning%0AKronecker%20factors%20are%20dense%2C%20and%20numerically%20unstable%20in%20low%20precision%20as%20they%0Arequire%20matrix%20inversion%20or%20decomposition.%20These%20limitations%20render%20such%0Amethods%20unpopular%20for%20modern%20mixed-precision%20training.%20We%20address%20them%20by%20%28i%29%0Aformulating%20an%20inverse-free%20KFAC%20update%20and%20%28ii%29%20imposing%20structures%20in%20the%0AKronecker%20factors%2C%20resulting%20in%20structured%20inverse-free%20natural%20gradient%0Adescent%20%28SINGD%29.%20On%20modern%20neural%20networks%2C%20we%20show%20that%20SINGD%20is%0Amemory-efficient%20and%20numerically%20robust%2C%20in%20contrast%20to%20KFAC%2C%20and%20often%0Aoutperforms%20AdamW%20even%20in%20half%20precision.%20Our%20work%20closes%20a%20gap%20between%20first-%0Aand%20second-order%20methods%20in%20modern%20low-precision%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05705v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Inverse-Free%2520Natural%2520Gradient%253A%2520Memory-Efficient%2520%2526%250A%2520%2520Numerically-Stable%2520KFAC%26entry.906535625%3DWu%2520Lin%2520and%2520Felix%2520Dangel%2520and%2520Runa%2520Eschenhagen%2520and%2520Kirill%2520Neklyudov%2520and%2520Agustinus%2520Kristiadi%2520and%2520Richard%2520E.%2520Turner%2520and%2520Alireza%2520Makhzani%26entry.1292438233%3D%2520%2520Second-order%2520methods%2520such%2520as%2520KFAC%2520can%2520be%2520useful%2520for%2520neural%2520net%2520training.%250AHowever%252C%2520they%2520are%2520often%2520memory-inefficient%2520since%2520their%2520preconditioning%250AKronecker%2520factors%2520are%2520dense%252C%2520and%2520numerically%2520unstable%2520in%2520low%2520precision%2520as%2520they%250Arequire%2520matrix%2520inversion%2520or%2520decomposition.%2520These%2520limitations%2520render%2520such%250Amethods%2520unpopular%2520for%2520modern%2520mixed-precision%2520training.%2520We%2520address%2520them%2520by%2520%2528i%2529%250Aformulating%2520an%2520inverse-free%2520KFAC%2520update%2520and%2520%2528ii%2529%2520imposing%2520structures%2520in%2520the%250AKronecker%2520factors%252C%2520resulting%2520in%2520structured%2520inverse-free%2520natural%2520gradient%250Adescent%2520%2528SINGD%2529.%2520On%2520modern%2520neural%2520networks%252C%2520we%2520show%2520that%2520SINGD%2520is%250Amemory-efficient%2520and%2520numerically%2520robust%252C%2520in%2520contrast%2520to%2520KFAC%252C%2520and%2520often%250Aoutperforms%2520AdamW%2520even%2520in%2520half%2520precision.%2520Our%2520work%2520closes%2520a%2520gap%2520between%2520first-%250Aand%2520second-order%2520methods%2520in%2520modern%2520low-precision%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05705v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Inverse-Free%20Natural%20Gradient%3A%20Memory-Efficient%20%26%0A%20%20Numerically-Stable%20KFAC&entry.906535625=Wu%20Lin%20and%20Felix%20Dangel%20and%20Runa%20Eschenhagen%20and%20Kirill%20Neklyudov%20and%20Agustinus%20Kristiadi%20and%20Richard%20E.%20Turner%20and%20Alireza%20Makhzani&entry.1292438233=%20%20Second-order%20methods%20such%20as%20KFAC%20can%20be%20useful%20for%20neural%20net%20training.%0AHowever%2C%20they%20are%20often%20memory-inefficient%20since%20their%20preconditioning%0AKronecker%20factors%20are%20dense%2C%20and%20numerically%20unstable%20in%20low%20precision%20as%20they%0Arequire%20matrix%20inversion%20or%20decomposition.%20These%20limitations%20render%20such%0Amethods%20unpopular%20for%20modern%20mixed-precision%20training.%20We%20address%20them%20by%20%28i%29%0Aformulating%20an%20inverse-free%20KFAC%20update%20and%20%28ii%29%20imposing%20structures%20in%20the%0AKronecker%20factors%2C%20resulting%20in%20structured%20inverse-free%20natural%20gradient%0Adescent%20%28SINGD%29.%20On%20modern%20neural%20networks%2C%20we%20show%20that%20SINGD%20is%0Amemory-efficient%20and%20numerically%20robust%2C%20in%20contrast%20to%20KFAC%2C%20and%20often%0Aoutperforms%20AdamW%20even%20in%20half%20precision.%20Our%20work%20closes%20a%20gap%20between%20first-%0Aand%20second-order%20methods%20in%20modern%20low-precision%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05705v4&entry.124074799=Read"},
{"title": "E-TSL: A Continuous Educational Turkish Sign Language Dataset with\n  Baseline Methods", "author": "\u015e\u00fckr\u00fc \u00d6zt\u00fcrk and Hacer Yalim Keles", "abstract": "  This study introduces the continuous Educational Turkish Sign Language\n(E-TSL) dataset, collected from online Turkish language lessons for 5th, 6th,\nand 8th grades. The dataset comprises 1,410 videos totaling nearly 24 hours and\nincludes performances from 11 signers. Turkish, an agglutinative language,\nposes unique challenges for sign language translation, particularly with a\nvocabulary where 64% are singleton words and 85% are rare words, appearing less\nthan five times. We developed two baseline models to address these challenges:\nthe Pose to Text Transformer (P2T-T) and the Graph Neural Network based\nTransformer (GNN-T) models. The GNN-T model achieved 19.13% BLEU-1 score and\n3.28% BLEU-4 score, presenting a significant challenge compared to existing\nbenchmarks. The P2T-T model, while demonstrating slightly lower performance in\nBLEU scores, achieved a higher ROUGE-L score of 22.09%. Additionally, we\nbenchmarked our model using the well-known PHOENIX-Weather 2014T dataset to\nvalidate our approach.\n", "link": "http://arxiv.org/abs/2405.02984v2", "date": "2024-07-23", "relevancy": 1.4675, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.506}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4741}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E-TSL%3A%20A%20Continuous%20Educational%20Turkish%20Sign%20Language%20Dataset%20with%0A%20%20Baseline%20Methods&body=Title%3A%20E-TSL%3A%20A%20Continuous%20Educational%20Turkish%20Sign%20Language%20Dataset%20with%0A%20%20Baseline%20Methods%0AAuthor%3A%20%C5%9E%C3%BCkr%C3%BC%20%C3%96zt%C3%BCrk%20and%20Hacer%20Yalim%20Keles%0AAbstract%3A%20%20%20This%20study%20introduces%20the%20continuous%20Educational%20Turkish%20Sign%20Language%0A%28E-TSL%29%20dataset%2C%20collected%20from%20online%20Turkish%20language%20lessons%20for%205th%2C%206th%2C%0Aand%208th%20grades.%20The%20dataset%20comprises%201%2C410%20videos%20totaling%20nearly%2024%20hours%20and%0Aincludes%20performances%20from%2011%20signers.%20Turkish%2C%20an%20agglutinative%20language%2C%0Aposes%20unique%20challenges%20for%20sign%20language%20translation%2C%20particularly%20with%20a%0Avocabulary%20where%2064%25%20are%20singleton%20words%20and%2085%25%20are%20rare%20words%2C%20appearing%20less%0Athan%20five%20times.%20We%20developed%20two%20baseline%20models%20to%20address%20these%20challenges%3A%0Athe%20Pose%20to%20Text%20Transformer%20%28P2T-T%29%20and%20the%20Graph%20Neural%20Network%20based%0ATransformer%20%28GNN-T%29%20models.%20The%20GNN-T%20model%20achieved%2019.13%25%20BLEU-1%20score%20and%0A3.28%25%20BLEU-4%20score%2C%20presenting%20a%20significant%20challenge%20compared%20to%20existing%0Abenchmarks.%20The%20P2T-T%20model%2C%20while%20demonstrating%20slightly%20lower%20performance%20in%0ABLEU%20scores%2C%20achieved%20a%20higher%20ROUGE-L%20score%20of%2022.09%25.%20Additionally%2C%20we%0Abenchmarked%20our%20model%20using%20the%20well-known%20PHOENIX-Weather%202014T%20dataset%20to%0Avalidate%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02984v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE-TSL%253A%2520A%2520Continuous%2520Educational%2520Turkish%2520Sign%2520Language%2520Dataset%2520with%250A%2520%2520Baseline%2520Methods%26entry.906535625%3D%25C5%259E%25C3%25BCkr%25C3%25BC%2520%25C3%2596zt%25C3%25BCrk%2520and%2520Hacer%2520Yalim%2520Keles%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520the%2520continuous%2520Educational%2520Turkish%2520Sign%2520Language%250A%2528E-TSL%2529%2520dataset%252C%2520collected%2520from%2520online%2520Turkish%2520language%2520lessons%2520for%25205th%252C%25206th%252C%250Aand%25208th%2520grades.%2520The%2520dataset%2520comprises%25201%252C410%2520videos%2520totaling%2520nearly%252024%2520hours%2520and%250Aincludes%2520performances%2520from%252011%2520signers.%2520Turkish%252C%2520an%2520agglutinative%2520language%252C%250Aposes%2520unique%2520challenges%2520for%2520sign%2520language%2520translation%252C%2520particularly%2520with%2520a%250Avocabulary%2520where%252064%2525%2520are%2520singleton%2520words%2520and%252085%2525%2520are%2520rare%2520words%252C%2520appearing%2520less%250Athan%2520five%2520times.%2520We%2520developed%2520two%2520baseline%2520models%2520to%2520address%2520these%2520challenges%253A%250Athe%2520Pose%2520to%2520Text%2520Transformer%2520%2528P2T-T%2529%2520and%2520the%2520Graph%2520Neural%2520Network%2520based%250ATransformer%2520%2528GNN-T%2529%2520models.%2520The%2520GNN-T%2520model%2520achieved%252019.13%2525%2520BLEU-1%2520score%2520and%250A3.28%2525%2520BLEU-4%2520score%252C%2520presenting%2520a%2520significant%2520challenge%2520compared%2520to%2520existing%250Abenchmarks.%2520The%2520P2T-T%2520model%252C%2520while%2520demonstrating%2520slightly%2520lower%2520performance%2520in%250ABLEU%2520scores%252C%2520achieved%2520a%2520higher%2520ROUGE-L%2520score%2520of%252022.09%2525.%2520Additionally%252C%2520we%250Abenchmarked%2520our%2520model%2520using%2520the%2520well-known%2520PHOENIX-Weather%25202014T%2520dataset%2520to%250Avalidate%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02984v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E-TSL%3A%20A%20Continuous%20Educational%20Turkish%20Sign%20Language%20Dataset%20with%0A%20%20Baseline%20Methods&entry.906535625=%C5%9E%C3%BCkr%C3%BC%20%C3%96zt%C3%BCrk%20and%20Hacer%20Yalim%20Keles&entry.1292438233=%20%20This%20study%20introduces%20the%20continuous%20Educational%20Turkish%20Sign%20Language%0A%28E-TSL%29%20dataset%2C%20collected%20from%20online%20Turkish%20language%20lessons%20for%205th%2C%206th%2C%0Aand%208th%20grades.%20The%20dataset%20comprises%201%2C410%20videos%20totaling%20nearly%2024%20hours%20and%0Aincludes%20performances%20from%2011%20signers.%20Turkish%2C%20an%20agglutinative%20language%2C%0Aposes%20unique%20challenges%20for%20sign%20language%20translation%2C%20particularly%20with%20a%0Avocabulary%20where%2064%25%20are%20singleton%20words%20and%2085%25%20are%20rare%20words%2C%20appearing%20less%0Athan%20five%20times.%20We%20developed%20two%20baseline%20models%20to%20address%20these%20challenges%3A%0Athe%20Pose%20to%20Text%20Transformer%20%28P2T-T%29%20and%20the%20Graph%20Neural%20Network%20based%0ATransformer%20%28GNN-T%29%20models.%20The%20GNN-T%20model%20achieved%2019.13%25%20BLEU-1%20score%20and%0A3.28%25%20BLEU-4%20score%2C%20presenting%20a%20significant%20challenge%20compared%20to%20existing%0Abenchmarks.%20The%20P2T-T%20model%2C%20while%20demonstrating%20slightly%20lower%20performance%20in%0ABLEU%20scores%2C%20achieved%20a%20higher%20ROUGE-L%20score%20of%2022.09%25.%20Additionally%2C%20we%0Abenchmarked%20our%20model%20using%20the%20well-known%20PHOENIX-Weather%202014T%20dataset%20to%0Avalidate%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02984v2&entry.124074799=Read"},
{"title": "Sample-Efficient Constrained Reinforcement Learning with General\n  Parameterization", "author": "Washim Uddin Mondal and Vaneet Aggarwal", "abstract": "  We consider a constrained Markov Decision Problem (CMDP) where the goal of an\nagent is to maximize the expected discounted sum of rewards over an infinite\nhorizon while ensuring that the expected discounted sum of costs exceeds a\ncertain threshold. Building on the idea of momentum-based acceleration, we\ndevelop the Primal-Dual Accelerated Natural Policy Gradient (PD-ANPG) algorithm\nthat guarantees an $\\epsilon$ global optimality gap and $\\epsilon$ constraint\nviolation with $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ sample complexity for\ngeneral parameterized policies. This improves the state-of-the-art sample\ncomplexity in general parameterized CMDPs by a factor of\n$\\mathcal{O}(\\epsilon^{-2})$ and achieves the theoretical lower bound.\n", "link": "http://arxiv.org/abs/2405.10624v2", "date": "2024-07-23", "relevancy": 1.3618, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4918}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4436}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample-Efficient%20Constrained%20Reinforcement%20Learning%20with%20General%0A%20%20Parameterization&body=Title%3A%20Sample-Efficient%20Constrained%20Reinforcement%20Learning%20with%20General%0A%20%20Parameterization%0AAuthor%3A%20Washim%20Uddin%20Mondal%20and%20Vaneet%20Aggarwal%0AAbstract%3A%20%20%20We%20consider%20a%20constrained%20Markov%20Decision%20Problem%20%28CMDP%29%20where%20the%20goal%20of%20an%0Aagent%20is%20to%20maximize%20the%20expected%20discounted%20sum%20of%20rewards%20over%20an%20infinite%0Ahorizon%20while%20ensuring%20that%20the%20expected%20discounted%20sum%20of%20costs%20exceeds%20a%0Acertain%20threshold.%20Building%20on%20the%20idea%20of%20momentum-based%20acceleration%2C%20we%0Adevelop%20the%20Primal-Dual%20Accelerated%20Natural%20Policy%20Gradient%20%28PD-ANPG%29%20algorithm%0Athat%20guarantees%20an%20%24%5Cepsilon%24%20global%20optimality%20gap%20and%20%24%5Cepsilon%24%20constraint%0Aviolation%20with%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-2%7D%29%24%20sample%20complexity%20for%0Ageneral%20parameterized%20policies.%20This%20improves%20the%20state-of-the-art%20sample%0Acomplexity%20in%20general%20parameterized%20CMDPs%20by%20a%20factor%20of%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-2%7D%29%24%20and%20achieves%20the%20theoretical%20lower%20bound.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10624v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample-Efficient%2520Constrained%2520Reinforcement%2520Learning%2520with%2520General%250A%2520%2520Parameterization%26entry.906535625%3DWashim%2520Uddin%2520Mondal%2520and%2520Vaneet%2520Aggarwal%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520constrained%2520Markov%2520Decision%2520Problem%2520%2528CMDP%2529%2520where%2520the%2520goal%2520of%2520an%250Aagent%2520is%2520to%2520maximize%2520the%2520expected%2520discounted%2520sum%2520of%2520rewards%2520over%2520an%2520infinite%250Ahorizon%2520while%2520ensuring%2520that%2520the%2520expected%2520discounted%2520sum%2520of%2520costs%2520exceeds%2520a%250Acertain%2520threshold.%2520Building%2520on%2520the%2520idea%2520of%2520momentum-based%2520acceleration%252C%2520we%250Adevelop%2520the%2520Primal-Dual%2520Accelerated%2520Natural%2520Policy%2520Gradient%2520%2528PD-ANPG%2529%2520algorithm%250Athat%2520guarantees%2520an%2520%2524%255Cepsilon%2524%2520global%2520optimality%2520gap%2520and%2520%2524%255Cepsilon%2524%2520constraint%250Aviolation%2520with%2520%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%2528%255Cepsilon%255E%257B-2%257D%2529%2524%2520sample%2520complexity%2520for%250Ageneral%2520parameterized%2520policies.%2520This%2520improves%2520the%2520state-of-the-art%2520sample%250Acomplexity%2520in%2520general%2520parameterized%2520CMDPs%2520by%2520a%2520factor%2520of%250A%2524%255Cmathcal%257BO%257D%2528%255Cepsilon%255E%257B-2%257D%2529%2524%2520and%2520achieves%2520the%2520theoretical%2520lower%2520bound.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10624v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample-Efficient%20Constrained%20Reinforcement%20Learning%20with%20General%0A%20%20Parameterization&entry.906535625=Washim%20Uddin%20Mondal%20and%20Vaneet%20Aggarwal&entry.1292438233=%20%20We%20consider%20a%20constrained%20Markov%20Decision%20Problem%20%28CMDP%29%20where%20the%20goal%20of%20an%0Aagent%20is%20to%20maximize%20the%20expected%20discounted%20sum%20of%20rewards%20over%20an%20infinite%0Ahorizon%20while%20ensuring%20that%20the%20expected%20discounted%20sum%20of%20costs%20exceeds%20a%0Acertain%20threshold.%20Building%20on%20the%20idea%20of%20momentum-based%20acceleration%2C%20we%0Adevelop%20the%20Primal-Dual%20Accelerated%20Natural%20Policy%20Gradient%20%28PD-ANPG%29%20algorithm%0Athat%20guarantees%20an%20%24%5Cepsilon%24%20global%20optimality%20gap%20and%20%24%5Cepsilon%24%20constraint%0Aviolation%20with%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-2%7D%29%24%20sample%20complexity%20for%0Ageneral%20parameterized%20policies.%20This%20improves%20the%20state-of-the-art%20sample%0Acomplexity%20in%20general%20parameterized%20CMDPs%20by%20a%20factor%20of%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-2%7D%29%24%20and%20achieves%20the%20theoretical%20lower%20bound.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10624v2&entry.124074799=Read"},
{"title": "Learning Unsigned Distance Functions from Multi-view Images with Volume\n  Rendering Priors", "author": "Wenyuan Zhang and Kanle Shi and Yu-Shen Liu and Zhizhong Han", "abstract": "  Unsigned distance functions (UDFs) have been a vital representation for open\nsurfaces. With different differentiable renderers, current methods are able to\ntrain neural networks to infer a UDF by minimizing the rendering errors on the\nUDF to the multi-view ground truth. However, these differentiable renderers are\nmainly handcrafted, which makes them either biased on ray-surface\nintersections, or sensitive to unsigned distance outliers, or not scalable to\nlarge scale scenes. To resolve these issues, we present a novel differentiable\nrenderer to infer UDFs more accurately. Instead of using handcrafted equations,\nour differentiable renderer is a neural network which is pre-trained in a\ndata-driven manner. It learns how to render unsigned distances into depth\nimages, leading to a prior knowledge, dubbed volume rendering priors. To infer\na UDF for an unseen scene from multiple RGB images, we generalize the learned\nvolume rendering priors to map inferred unsigned distances in alpha blending\nfor RGB image rendering. Our results show that the learned volume rendering\npriors are unbiased, robust, scalable, 3D aware, and more importantly, easy to\nlearn. We evaluate our method on both widely used benchmarks and real scenes,\nand report superior performance over the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2407.16396v1", "date": "2024-07-23", "relevancy": 1.1196, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5816}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5533}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Unsigned%20Distance%20Functions%20from%20Multi-view%20Images%20with%20Volume%0A%20%20Rendering%20Priors&body=Title%3A%20Learning%20Unsigned%20Distance%20Functions%20from%20Multi-view%20Images%20with%20Volume%0A%20%20Rendering%20Priors%0AAuthor%3A%20Wenyuan%20Zhang%20and%20Kanle%20Shi%20and%20Yu-Shen%20Liu%20and%20Zhizhong%20Han%0AAbstract%3A%20%20%20Unsigned%20distance%20functions%20%28UDFs%29%20have%20been%20a%20vital%20representation%20for%20open%0Asurfaces.%20With%20different%20differentiable%20renderers%2C%20current%20methods%20are%20able%20to%0Atrain%20neural%20networks%20to%20infer%20a%20UDF%20by%20minimizing%20the%20rendering%20errors%20on%20the%0AUDF%20to%20the%20multi-view%20ground%20truth.%20However%2C%20these%20differentiable%20renderers%20are%0Amainly%20handcrafted%2C%20which%20makes%20them%20either%20biased%20on%20ray-surface%0Aintersections%2C%20or%20sensitive%20to%20unsigned%20distance%20outliers%2C%20or%20not%20scalable%20to%0Alarge%20scale%20scenes.%20To%20resolve%20these%20issues%2C%20we%20present%20a%20novel%20differentiable%0Arenderer%20to%20infer%20UDFs%20more%20accurately.%20Instead%20of%20using%20handcrafted%20equations%2C%0Aour%20differentiable%20renderer%20is%20a%20neural%20network%20which%20is%20pre-trained%20in%20a%0Adata-driven%20manner.%20It%20learns%20how%20to%20render%20unsigned%20distances%20into%20depth%0Aimages%2C%20leading%20to%20a%20prior%20knowledge%2C%20dubbed%20volume%20rendering%20priors.%20To%20infer%0Aa%20UDF%20for%20an%20unseen%20scene%20from%20multiple%20RGB%20images%2C%20we%20generalize%20the%20learned%0Avolume%20rendering%20priors%20to%20map%20inferred%20unsigned%20distances%20in%20alpha%20blending%0Afor%20RGB%20image%20rendering.%20Our%20results%20show%20that%20the%20learned%20volume%20rendering%0Apriors%20are%20unbiased%2C%20robust%2C%20scalable%2C%203D%20aware%2C%20and%20more%20importantly%2C%20easy%20to%0Alearn.%20We%20evaluate%20our%20method%20on%20both%20widely%20used%20benchmarks%20and%20real%20scenes%2C%0Aand%20report%20superior%20performance%20over%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Unsigned%2520Distance%2520Functions%2520from%2520Multi-view%2520Images%2520with%2520Volume%250A%2520%2520Rendering%2520Priors%26entry.906535625%3DWenyuan%2520Zhang%2520and%2520Kanle%2520Shi%2520and%2520Yu-Shen%2520Liu%2520and%2520Zhizhong%2520Han%26entry.1292438233%3D%2520%2520Unsigned%2520distance%2520functions%2520%2528UDFs%2529%2520have%2520been%2520a%2520vital%2520representation%2520for%2520open%250Asurfaces.%2520With%2520different%2520differentiable%2520renderers%252C%2520current%2520methods%2520are%2520able%2520to%250Atrain%2520neural%2520networks%2520to%2520infer%2520a%2520UDF%2520by%2520minimizing%2520the%2520rendering%2520errors%2520on%2520the%250AUDF%2520to%2520the%2520multi-view%2520ground%2520truth.%2520However%252C%2520these%2520differentiable%2520renderers%2520are%250Amainly%2520handcrafted%252C%2520which%2520makes%2520them%2520either%2520biased%2520on%2520ray-surface%250Aintersections%252C%2520or%2520sensitive%2520to%2520unsigned%2520distance%2520outliers%252C%2520or%2520not%2520scalable%2520to%250Alarge%2520scale%2520scenes.%2520To%2520resolve%2520these%2520issues%252C%2520we%2520present%2520a%2520novel%2520differentiable%250Arenderer%2520to%2520infer%2520UDFs%2520more%2520accurately.%2520Instead%2520of%2520using%2520handcrafted%2520equations%252C%250Aour%2520differentiable%2520renderer%2520is%2520a%2520neural%2520network%2520which%2520is%2520pre-trained%2520in%2520a%250Adata-driven%2520manner.%2520It%2520learns%2520how%2520to%2520render%2520unsigned%2520distances%2520into%2520depth%250Aimages%252C%2520leading%2520to%2520a%2520prior%2520knowledge%252C%2520dubbed%2520volume%2520rendering%2520priors.%2520To%2520infer%250Aa%2520UDF%2520for%2520an%2520unseen%2520scene%2520from%2520multiple%2520RGB%2520images%252C%2520we%2520generalize%2520the%2520learned%250Avolume%2520rendering%2520priors%2520to%2520map%2520inferred%2520unsigned%2520distances%2520in%2520alpha%2520blending%250Afor%2520RGB%2520image%2520rendering.%2520Our%2520results%2520show%2520that%2520the%2520learned%2520volume%2520rendering%250Apriors%2520are%2520unbiased%252C%2520robust%252C%2520scalable%252C%25203D%2520aware%252C%2520and%2520more%2520importantly%252C%2520easy%2520to%250Alearn.%2520We%2520evaluate%2520our%2520method%2520on%2520both%2520widely%2520used%2520benchmarks%2520and%2520real%2520scenes%252C%250Aand%2520report%2520superior%2520performance%2520over%2520the%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Unsigned%20Distance%20Functions%20from%20Multi-view%20Images%20with%20Volume%0A%20%20Rendering%20Priors&entry.906535625=Wenyuan%20Zhang%20and%20Kanle%20Shi%20and%20Yu-Shen%20Liu%20and%20Zhizhong%20Han&entry.1292438233=%20%20Unsigned%20distance%20functions%20%28UDFs%29%20have%20been%20a%20vital%20representation%20for%20open%0Asurfaces.%20With%20different%20differentiable%20renderers%2C%20current%20methods%20are%20able%20to%0Atrain%20neural%20networks%20to%20infer%20a%20UDF%20by%20minimizing%20the%20rendering%20errors%20on%20the%0AUDF%20to%20the%20multi-view%20ground%20truth.%20However%2C%20these%20differentiable%20renderers%20are%0Amainly%20handcrafted%2C%20which%20makes%20them%20either%20biased%20on%20ray-surface%0Aintersections%2C%20or%20sensitive%20to%20unsigned%20distance%20outliers%2C%20or%20not%20scalable%20to%0Alarge%20scale%20scenes.%20To%20resolve%20these%20issues%2C%20we%20present%20a%20novel%20differentiable%0Arenderer%20to%20infer%20UDFs%20more%20accurately.%20Instead%20of%20using%20handcrafted%20equations%2C%0Aour%20differentiable%20renderer%20is%20a%20neural%20network%20which%20is%20pre-trained%20in%20a%0Adata-driven%20manner.%20It%20learns%20how%20to%20render%20unsigned%20distances%20into%20depth%0Aimages%2C%20leading%20to%20a%20prior%20knowledge%2C%20dubbed%20volume%20rendering%20priors.%20To%20infer%0Aa%20UDF%20for%20an%20unseen%20scene%20from%20multiple%20RGB%20images%2C%20we%20generalize%20the%20learned%0Avolume%20rendering%20priors%20to%20map%20inferred%20unsigned%20distances%20in%20alpha%20blending%0Afor%20RGB%20image%20rendering.%20Our%20results%20show%20that%20the%20learned%20volume%20rendering%0Apriors%20are%20unbiased%2C%20robust%2C%20scalable%2C%203D%20aware%2C%20and%20more%20importantly%2C%20easy%20to%0Alearn.%20We%20evaluate%20our%20method%20on%20both%20widely%20used%20benchmarks%20and%20real%20scenes%2C%0Aand%20report%20superior%20performance%20over%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16396v1&entry.124074799=Read"},
{"title": "Computable learning of natural hypothesis classes", "author": "Matthew Harrison-Trainor and Syed Akbari", "abstract": "  This paper is about the recent notion of computably probably approximately\ncorrect learning, which lies between the statistical learning theory where\nthere is no computational requirement on the learner and efficient PAC where\nthe learner must be polynomially bounded. Examples have recently been given of\nhypothesis classes which are PAC learnable but not computably PAC learnable,\nbut these hypothesis classes are unnatural or non-canonical in the sense that\nthey depend on a numbering of proofs, formulas, or programs. We use the\non-a-cone machinery from computability theory to prove that, under mild\nassumptions such as that the hypothesis class can be computably listable, any\nnatural hypothesis class which is learnable must be computably learnable. Thus\nthe counterexamples given previously are necessarily unnatural.\n", "link": "http://arxiv.org/abs/2407.16663v1", "date": "2024-07-23", "relevancy": 1.6808, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.45}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4328}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computable%20learning%20of%20natural%20hypothesis%20classes&body=Title%3A%20Computable%20learning%20of%20natural%20hypothesis%20classes%0AAuthor%3A%20Matthew%20Harrison-Trainor%20and%20Syed%20Akbari%0AAbstract%3A%20%20%20This%20paper%20is%20about%20the%20recent%20notion%20of%20computably%20probably%20approximately%0Acorrect%20learning%2C%20which%20lies%20between%20the%20statistical%20learning%20theory%20where%0Athere%20is%20no%20computational%20requirement%20on%20the%20learner%20and%20efficient%20PAC%20where%0Athe%20learner%20must%20be%20polynomially%20bounded.%20Examples%20have%20recently%20been%20given%20of%0Ahypothesis%20classes%20which%20are%20PAC%20learnable%20but%20not%20computably%20PAC%20learnable%2C%0Abut%20these%20hypothesis%20classes%20are%20unnatural%20or%20non-canonical%20in%20the%20sense%20that%0Athey%20depend%20on%20a%20numbering%20of%20proofs%2C%20formulas%2C%20or%20programs.%20We%20use%20the%0Aon-a-cone%20machinery%20from%20computability%20theory%20to%20prove%20that%2C%20under%20mild%0Aassumptions%20such%20as%20that%20the%20hypothesis%20class%20can%20be%20computably%20listable%2C%20any%0Anatural%20hypothesis%20class%20which%20is%20learnable%20must%20be%20computably%20learnable.%20Thus%0Athe%20counterexamples%20given%20previously%20are%20necessarily%20unnatural.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputable%2520learning%2520of%2520natural%2520hypothesis%2520classes%26entry.906535625%3DMatthew%2520Harrison-Trainor%2520and%2520Syed%2520Akbari%26entry.1292438233%3D%2520%2520This%2520paper%2520is%2520about%2520the%2520recent%2520notion%2520of%2520computably%2520probably%2520approximately%250Acorrect%2520learning%252C%2520which%2520lies%2520between%2520the%2520statistical%2520learning%2520theory%2520where%250Athere%2520is%2520no%2520computational%2520requirement%2520on%2520the%2520learner%2520and%2520efficient%2520PAC%2520where%250Athe%2520learner%2520must%2520be%2520polynomially%2520bounded.%2520Examples%2520have%2520recently%2520been%2520given%2520of%250Ahypothesis%2520classes%2520which%2520are%2520PAC%2520learnable%2520but%2520not%2520computably%2520PAC%2520learnable%252C%250Abut%2520these%2520hypothesis%2520classes%2520are%2520unnatural%2520or%2520non-canonical%2520in%2520the%2520sense%2520that%250Athey%2520depend%2520on%2520a%2520numbering%2520of%2520proofs%252C%2520formulas%252C%2520or%2520programs.%2520We%2520use%2520the%250Aon-a-cone%2520machinery%2520from%2520computability%2520theory%2520to%2520prove%2520that%252C%2520under%2520mild%250Aassumptions%2520such%2520as%2520that%2520the%2520hypothesis%2520class%2520can%2520be%2520computably%2520listable%252C%2520any%250Anatural%2520hypothesis%2520class%2520which%2520is%2520learnable%2520must%2520be%2520computably%2520learnable.%2520Thus%250Athe%2520counterexamples%2520given%2520previously%2520are%2520necessarily%2520unnatural.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computable%20learning%20of%20natural%20hypothesis%20classes&entry.906535625=Matthew%20Harrison-Trainor%20and%20Syed%20Akbari&entry.1292438233=%20%20This%20paper%20is%20about%20the%20recent%20notion%20of%20computably%20probably%20approximately%0Acorrect%20learning%2C%20which%20lies%20between%20the%20statistical%20learning%20theory%20where%0Athere%20is%20no%20computational%20requirement%20on%20the%20learner%20and%20efficient%20PAC%20where%0Athe%20learner%20must%20be%20polynomially%20bounded.%20Examples%20have%20recently%20been%20given%20of%0Ahypothesis%20classes%20which%20are%20PAC%20learnable%20but%20not%20computably%20PAC%20learnable%2C%0Abut%20these%20hypothesis%20classes%20are%20unnatural%20or%20non-canonical%20in%20the%20sense%20that%0Athey%20depend%20on%20a%20numbering%20of%20proofs%2C%20formulas%2C%20or%20programs.%20We%20use%20the%0Aon-a-cone%20machinery%20from%20computability%20theory%20to%20prove%20that%2C%20under%20mild%0Aassumptions%20such%20as%20that%20the%20hypothesis%20class%20can%20be%20computably%20listable%2C%20any%0Anatural%20hypothesis%20class%20which%20is%20learnable%20must%20be%20computably%20learnable.%20Thus%0Athe%20counterexamples%20given%20previously%20are%20necessarily%20unnatural.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16663v1&entry.124074799=Read"},
{"title": "Functional Acceleration for Policy Mirror Descent", "author": "Veronica Chelu and Doina Precup", "abstract": "  We apply functional acceleration to the Policy Mirror Descent (PMD) general\nfamily of algorithms, which cover a wide range of novel and fundamental methods\nin Reinforcement Learning (RL). Leveraging duality, we propose a momentum-based\nPMD update. By taking the functional route, our approach is independent of the\npolicy parametrization and applicable to large-scale optimization, covering\nprevious applications of momentum at the level of policy parameters as a\nspecial case. We theoretically analyze several properties of this approach and\ncomplement with a numerical ablation study, which serves to illustrate the\npolicy optimization dynamics on the value polytope, relative to different\nalgorithmic design choices in this space. We further characterize numerically\nseveral features of the problem setting relevant for functional acceleration,\nand lastly, we investigate the impact of approximation on their learning\nmechanics.\n", "link": "http://arxiv.org/abs/2407.16602v1", "date": "2024-07-23", "relevancy": 1.313, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4607}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4314}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Functional%20Acceleration%20for%20Policy%20Mirror%20Descent&body=Title%3A%20Functional%20Acceleration%20for%20Policy%20Mirror%20Descent%0AAuthor%3A%20Veronica%20Chelu%20and%20Doina%20Precup%0AAbstract%3A%20%20%20We%20apply%20functional%20acceleration%20to%20the%20Policy%20Mirror%20Descent%20%28PMD%29%20general%0Afamily%20of%20algorithms%2C%20which%20cover%20a%20wide%20range%20of%20novel%20and%20fundamental%20methods%0Ain%20Reinforcement%20Learning%20%28RL%29.%20Leveraging%20duality%2C%20we%20propose%20a%20momentum-based%0APMD%20update.%20By%20taking%20the%20functional%20route%2C%20our%20approach%20is%20independent%20of%20the%0Apolicy%20parametrization%20and%20applicable%20to%20large-scale%20optimization%2C%20covering%0Aprevious%20applications%20of%20momentum%20at%20the%20level%20of%20policy%20parameters%20as%20a%0Aspecial%20case.%20We%20theoretically%20analyze%20several%20properties%20of%20this%20approach%20and%0Acomplement%20with%20a%20numerical%20ablation%20study%2C%20which%20serves%20to%20illustrate%20the%0Apolicy%20optimization%20dynamics%20on%20the%20value%20polytope%2C%20relative%20to%20different%0Aalgorithmic%20design%20choices%20in%20this%20space.%20We%20further%20characterize%20numerically%0Aseveral%20features%20of%20the%20problem%20setting%20relevant%20for%20functional%20acceleration%2C%0Aand%20lastly%2C%20we%20investigate%20the%20impact%20of%20approximation%20on%20their%20learning%0Amechanics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunctional%2520Acceleration%2520for%2520Policy%2520Mirror%2520Descent%26entry.906535625%3DVeronica%2520Chelu%2520and%2520Doina%2520Precup%26entry.1292438233%3D%2520%2520We%2520apply%2520functional%2520acceleration%2520to%2520the%2520Policy%2520Mirror%2520Descent%2520%2528PMD%2529%2520general%250Afamily%2520of%2520algorithms%252C%2520which%2520cover%2520a%2520wide%2520range%2520of%2520novel%2520and%2520fundamental%2520methods%250Ain%2520Reinforcement%2520Learning%2520%2528RL%2529.%2520Leveraging%2520duality%252C%2520we%2520propose%2520a%2520momentum-based%250APMD%2520update.%2520By%2520taking%2520the%2520functional%2520route%252C%2520our%2520approach%2520is%2520independent%2520of%2520the%250Apolicy%2520parametrization%2520and%2520applicable%2520to%2520large-scale%2520optimization%252C%2520covering%250Aprevious%2520applications%2520of%2520momentum%2520at%2520the%2520level%2520of%2520policy%2520parameters%2520as%2520a%250Aspecial%2520case.%2520We%2520theoretically%2520analyze%2520several%2520properties%2520of%2520this%2520approach%2520and%250Acomplement%2520with%2520a%2520numerical%2520ablation%2520study%252C%2520which%2520serves%2520to%2520illustrate%2520the%250Apolicy%2520optimization%2520dynamics%2520on%2520the%2520value%2520polytope%252C%2520relative%2520to%2520different%250Aalgorithmic%2520design%2520choices%2520in%2520this%2520space.%2520We%2520further%2520characterize%2520numerically%250Aseveral%2520features%2520of%2520the%2520problem%2520setting%2520relevant%2520for%2520functional%2520acceleration%252C%250Aand%2520lastly%252C%2520we%2520investigate%2520the%2520impact%2520of%2520approximation%2520on%2520their%2520learning%250Amechanics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Functional%20Acceleration%20for%20Policy%20Mirror%20Descent&entry.906535625=Veronica%20Chelu%20and%20Doina%20Precup&entry.1292438233=%20%20We%20apply%20functional%20acceleration%20to%20the%20Policy%20Mirror%20Descent%20%28PMD%29%20general%0Afamily%20of%20algorithms%2C%20which%20cover%20a%20wide%20range%20of%20novel%20and%20fundamental%20methods%0Ain%20Reinforcement%20Learning%20%28RL%29.%20Leveraging%20duality%2C%20we%20propose%20a%20momentum-based%0APMD%20update.%20By%20taking%20the%20functional%20route%2C%20our%20approach%20is%20independent%20of%20the%0Apolicy%20parametrization%20and%20applicable%20to%20large-scale%20optimization%2C%20covering%0Aprevious%20applications%20of%20momentum%20at%20the%20level%20of%20policy%20parameters%20as%20a%0Aspecial%20case.%20We%20theoretically%20analyze%20several%20properties%20of%20this%20approach%20and%0Acomplement%20with%20a%20numerical%20ablation%20study%2C%20which%20serves%20to%20illustrate%20the%0Apolicy%20optimization%20dynamics%20on%20the%20value%20polytope%2C%20relative%20to%20different%0Aalgorithmic%20design%20choices%20in%20this%20space.%20We%20further%20characterize%20numerically%0Aseveral%20features%20of%20the%20problem%20setting%20relevant%20for%20functional%20acceleration%2C%0Aand%20lastly%2C%20we%20investigate%20the%20impact%20of%20approximation%20on%20their%20learning%0Amechanics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16602v1&entry.124074799=Read"},
{"title": "DC is all you need: describing ReLU from a signal processing standpoint", "author": "Christodoulos Kechris and Jonathan Dan and Jose Miranda and David Atienza", "abstract": "  Non-linear activation functions are crucial in Convolutional Neural Networks.\nHowever, until now they have not been well described in the frequency domain.\nIn this work, we study the spectral behavior of ReLU, a popular activation\nfunction. We use the ReLU's Taylor expansion to derive its frequency domain\nbehavior. We demonstrate that ReLU introduces higher frequency oscillations in\nthe signal and a constant DC component. Furthermore, we investigate the\nimportance of this DC component, where we demonstrate that it helps the model\nextract meaningful features related to the input frequency content. We\naccompany our theoretical derivations with experiments and real-world examples.\nFirst, we numerically validate our frequency response model. Then we observe\nReLU's spectral behavior on two example models and a real-world one. Finally,\nwe experimentally investigate the role of the DC component introduced by ReLU\nin the CNN's representations. Our results indicate that the DC helps to\nconverge to a weight configuration that is close to the initial random weights.\n", "link": "http://arxiv.org/abs/2407.16556v1", "date": "2024-07-23", "relevancy": 1.3259, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4753}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4369}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DC%20is%20all%20you%20need%3A%20describing%20ReLU%20from%20a%20signal%20processing%20standpoint&body=Title%3A%20DC%20is%20all%20you%20need%3A%20describing%20ReLU%20from%20a%20signal%20processing%20standpoint%0AAuthor%3A%20Christodoulos%20Kechris%20and%20Jonathan%20Dan%20and%20Jose%20Miranda%20and%20David%20Atienza%0AAbstract%3A%20%20%20Non-linear%20activation%20functions%20are%20crucial%20in%20Convolutional%20Neural%20Networks.%0AHowever%2C%20until%20now%20they%20have%20not%20been%20well%20described%20in%20the%20frequency%20domain.%0AIn%20this%20work%2C%20we%20study%20the%20spectral%20behavior%20of%20ReLU%2C%20a%20popular%20activation%0Afunction.%20We%20use%20the%20ReLU%27s%20Taylor%20expansion%20to%20derive%20its%20frequency%20domain%0Abehavior.%20We%20demonstrate%20that%20ReLU%20introduces%20higher%20frequency%20oscillations%20in%0Athe%20signal%20and%20a%20constant%20DC%20component.%20Furthermore%2C%20we%20investigate%20the%0Aimportance%20of%20this%20DC%20component%2C%20where%20we%20demonstrate%20that%20it%20helps%20the%20model%0Aextract%20meaningful%20features%20related%20to%20the%20input%20frequency%20content.%20We%0Aaccompany%20our%20theoretical%20derivations%20with%20experiments%20and%20real-world%20examples.%0AFirst%2C%20we%20numerically%20validate%20our%20frequency%20response%20model.%20Then%20we%20observe%0AReLU%27s%20spectral%20behavior%20on%20two%20example%20models%20and%20a%20real-world%20one.%20Finally%2C%0Awe%20experimentally%20investigate%20the%20role%20of%20the%20DC%20component%20introduced%20by%20ReLU%0Ain%20the%20CNN%27s%20representations.%20Our%20results%20indicate%20that%20the%20DC%20helps%20to%0Aconverge%20to%20a%20weight%20configuration%20that%20is%20close%20to%20the%20initial%20random%20weights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDC%2520is%2520all%2520you%2520need%253A%2520describing%2520ReLU%2520from%2520a%2520signal%2520processing%2520standpoint%26entry.906535625%3DChristodoulos%2520Kechris%2520and%2520Jonathan%2520Dan%2520and%2520Jose%2520Miranda%2520and%2520David%2520Atienza%26entry.1292438233%3D%2520%2520Non-linear%2520activation%2520functions%2520are%2520crucial%2520in%2520Convolutional%2520Neural%2520Networks.%250AHowever%252C%2520until%2520now%2520they%2520have%2520not%2520been%2520well%2520described%2520in%2520the%2520frequency%2520domain.%250AIn%2520this%2520work%252C%2520we%2520study%2520the%2520spectral%2520behavior%2520of%2520ReLU%252C%2520a%2520popular%2520activation%250Afunction.%2520We%2520use%2520the%2520ReLU%2527s%2520Taylor%2520expansion%2520to%2520derive%2520its%2520frequency%2520domain%250Abehavior.%2520We%2520demonstrate%2520that%2520ReLU%2520introduces%2520higher%2520frequency%2520oscillations%2520in%250Athe%2520signal%2520and%2520a%2520constant%2520DC%2520component.%2520Furthermore%252C%2520we%2520investigate%2520the%250Aimportance%2520of%2520this%2520DC%2520component%252C%2520where%2520we%2520demonstrate%2520that%2520it%2520helps%2520the%2520model%250Aextract%2520meaningful%2520features%2520related%2520to%2520the%2520input%2520frequency%2520content.%2520We%250Aaccompany%2520our%2520theoretical%2520derivations%2520with%2520experiments%2520and%2520real-world%2520examples.%250AFirst%252C%2520we%2520numerically%2520validate%2520our%2520frequency%2520response%2520model.%2520Then%2520we%2520observe%250AReLU%2527s%2520spectral%2520behavior%2520on%2520two%2520example%2520models%2520and%2520a%2520real-world%2520one.%2520Finally%252C%250Awe%2520experimentally%2520investigate%2520the%2520role%2520of%2520the%2520DC%2520component%2520introduced%2520by%2520ReLU%250Ain%2520the%2520CNN%2527s%2520representations.%2520Our%2520results%2520indicate%2520that%2520the%2520DC%2520helps%2520to%250Aconverge%2520to%2520a%2520weight%2520configuration%2520that%2520is%2520close%2520to%2520the%2520initial%2520random%2520weights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DC%20is%20all%20you%20need%3A%20describing%20ReLU%20from%20a%20signal%20processing%20standpoint&entry.906535625=Christodoulos%20Kechris%20and%20Jonathan%20Dan%20and%20Jose%20Miranda%20and%20David%20Atienza&entry.1292438233=%20%20Non-linear%20activation%20functions%20are%20crucial%20in%20Convolutional%20Neural%20Networks.%0AHowever%2C%20until%20now%20they%20have%20not%20been%20well%20described%20in%20the%20frequency%20domain.%0AIn%20this%20work%2C%20we%20study%20the%20spectral%20behavior%20of%20ReLU%2C%20a%20popular%20activation%0Afunction.%20We%20use%20the%20ReLU%27s%20Taylor%20expansion%20to%20derive%20its%20frequency%20domain%0Abehavior.%20We%20demonstrate%20that%20ReLU%20introduces%20higher%20frequency%20oscillations%20in%0Athe%20signal%20and%20a%20constant%20DC%20component.%20Furthermore%2C%20we%20investigate%20the%0Aimportance%20of%20this%20DC%20component%2C%20where%20we%20demonstrate%20that%20it%20helps%20the%20model%0Aextract%20meaningful%20features%20related%20to%20the%20input%20frequency%20content.%20We%0Aaccompany%20our%20theoretical%20derivations%20with%20experiments%20and%20real-world%20examples.%0AFirst%2C%20we%20numerically%20validate%20our%20frequency%20response%20model.%20Then%20we%20observe%0AReLU%27s%20spectral%20behavior%20on%20two%20example%20models%20and%20a%20real-world%20one.%20Finally%2C%0Awe%20experimentally%20investigate%20the%20role%20of%20the%20DC%20component%20introduced%20by%20ReLU%0Ain%20the%20CNN%27s%20representations.%20Our%20results%20indicate%20that%20the%20DC%20helps%20to%0Aconverge%20to%20a%20weight%20configuration%20that%20is%20close%20to%20the%20initial%20random%20weights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16556v1&entry.124074799=Read"},
{"title": "TimeInf: Time Series Data Contribution via Influence Functions", "author": "Yizi Zhang and Jingyan Shen and Xiaoxue Xiong and Yongchan Kwon", "abstract": "  Evaluating the contribution of individual data points to a model's prediction\nis critical for interpreting model predictions and improving model performance.\nExisting data contribution methods have been applied to various data types,\nincluding tabular data, images, and texts; however, their primary focus has\nbeen on i.i.d. settings. Despite the pressing need for principled approaches\ntailored to time series datasets, the problem of estimating data contribution\nin such settings remains unexplored, possibly due to challenges associated with\nhandling inherent temporal dependencies. This paper introduces TimeInf, a data\ncontribution estimation method for time-series datasets. TimeInf uses influence\nfunctions to attribute model predictions to individual time points while\npreserving temporal structures. Our extensive empirical results demonstrate\nthat TimeInf outperforms state-of-the-art methods in identifying harmful\nanomalies and helpful time points for forecasting. Additionally, TimeInf offers\nintuitive and interpretable attributions of data values, allowing us to easily\ndistinguish diverse anomaly patterns through visualizations.\n", "link": "http://arxiv.org/abs/2407.15247v2", "date": "2024-07-23", "relevancy": 1.2346, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4261}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4082}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeInf%3A%20Time%20Series%20Data%20Contribution%20via%20Influence%20Functions&body=Title%3A%20TimeInf%3A%20Time%20Series%20Data%20Contribution%20via%20Influence%20Functions%0AAuthor%3A%20Yizi%20Zhang%20and%20Jingyan%20Shen%20and%20Xiaoxue%20Xiong%20and%20Yongchan%20Kwon%0AAbstract%3A%20%20%20Evaluating%20the%20contribution%20of%20individual%20data%20points%20to%20a%20model%27s%20prediction%0Ais%20critical%20for%20interpreting%20model%20predictions%20and%20improving%20model%20performance.%0AExisting%20data%20contribution%20methods%20have%20been%20applied%20to%20various%20data%20types%2C%0Aincluding%20tabular%20data%2C%20images%2C%20and%20texts%3B%20however%2C%20their%20primary%20focus%20has%0Abeen%20on%20i.i.d.%20settings.%20Despite%20the%20pressing%20need%20for%20principled%20approaches%0Atailored%20to%20time%20series%20datasets%2C%20the%20problem%20of%20estimating%20data%20contribution%0Ain%20such%20settings%20remains%20unexplored%2C%20possibly%20due%20to%20challenges%20associated%20with%0Ahandling%20inherent%20temporal%20dependencies.%20This%20paper%20introduces%20TimeInf%2C%20a%20data%0Acontribution%20estimation%20method%20for%20time-series%20datasets.%20TimeInf%20uses%20influence%0Afunctions%20to%20attribute%20model%20predictions%20to%20individual%20time%20points%20while%0Apreserving%20temporal%20structures.%20Our%20extensive%20empirical%20results%20demonstrate%0Athat%20TimeInf%20outperforms%20state-of-the-art%20methods%20in%20identifying%20harmful%0Aanomalies%20and%20helpful%20time%20points%20for%20forecasting.%20Additionally%2C%20TimeInf%20offers%0Aintuitive%20and%20interpretable%20attributions%20of%20data%20values%2C%20allowing%20us%20to%20easily%0Adistinguish%20diverse%20anomaly%20patterns%20through%20visualizations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15247v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeInf%253A%2520Time%2520Series%2520Data%2520Contribution%2520via%2520Influence%2520Functions%26entry.906535625%3DYizi%2520Zhang%2520and%2520Jingyan%2520Shen%2520and%2520Xiaoxue%2520Xiong%2520and%2520Yongchan%2520Kwon%26entry.1292438233%3D%2520%2520Evaluating%2520the%2520contribution%2520of%2520individual%2520data%2520points%2520to%2520a%2520model%2527s%2520prediction%250Ais%2520critical%2520for%2520interpreting%2520model%2520predictions%2520and%2520improving%2520model%2520performance.%250AExisting%2520data%2520contribution%2520methods%2520have%2520been%2520applied%2520to%2520various%2520data%2520types%252C%250Aincluding%2520tabular%2520data%252C%2520images%252C%2520and%2520texts%253B%2520however%252C%2520their%2520primary%2520focus%2520has%250Abeen%2520on%2520i.i.d.%2520settings.%2520Despite%2520the%2520pressing%2520need%2520for%2520principled%2520approaches%250Atailored%2520to%2520time%2520series%2520datasets%252C%2520the%2520problem%2520of%2520estimating%2520data%2520contribution%250Ain%2520such%2520settings%2520remains%2520unexplored%252C%2520possibly%2520due%2520to%2520challenges%2520associated%2520with%250Ahandling%2520inherent%2520temporal%2520dependencies.%2520This%2520paper%2520introduces%2520TimeInf%252C%2520a%2520data%250Acontribution%2520estimation%2520method%2520for%2520time-series%2520datasets.%2520TimeInf%2520uses%2520influence%250Afunctions%2520to%2520attribute%2520model%2520predictions%2520to%2520individual%2520time%2520points%2520while%250Apreserving%2520temporal%2520structures.%2520Our%2520extensive%2520empirical%2520results%2520demonstrate%250Athat%2520TimeInf%2520outperforms%2520state-of-the-art%2520methods%2520in%2520identifying%2520harmful%250Aanomalies%2520and%2520helpful%2520time%2520points%2520for%2520forecasting.%2520Additionally%252C%2520TimeInf%2520offers%250Aintuitive%2520and%2520interpretable%2520attributions%2520of%2520data%2520values%252C%2520allowing%2520us%2520to%2520easily%250Adistinguish%2520diverse%2520anomaly%2520patterns%2520through%2520visualizations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15247v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeInf%3A%20Time%20Series%20Data%20Contribution%20via%20Influence%20Functions&entry.906535625=Yizi%20Zhang%20and%20Jingyan%20Shen%20and%20Xiaoxue%20Xiong%20and%20Yongchan%20Kwon&entry.1292438233=%20%20Evaluating%20the%20contribution%20of%20individual%20data%20points%20to%20a%20model%27s%20prediction%0Ais%20critical%20for%20interpreting%20model%20predictions%20and%20improving%20model%20performance.%0AExisting%20data%20contribution%20methods%20have%20been%20applied%20to%20various%20data%20types%2C%0Aincluding%20tabular%20data%2C%20images%2C%20and%20texts%3B%20however%2C%20their%20primary%20focus%20has%0Abeen%20on%20i.i.d.%20settings.%20Despite%20the%20pressing%20need%20for%20principled%20approaches%0Atailored%20to%20time%20series%20datasets%2C%20the%20problem%20of%20estimating%20data%20contribution%0Ain%20such%20settings%20remains%20unexplored%2C%20possibly%20due%20to%20challenges%20associated%20with%0Ahandling%20inherent%20temporal%20dependencies.%20This%20paper%20introduces%20TimeInf%2C%20a%20data%0Acontribution%20estimation%20method%20for%20time-series%20datasets.%20TimeInf%20uses%20influence%0Afunctions%20to%20attribute%20model%20predictions%20to%20individual%20time%20points%20while%0Apreserving%20temporal%20structures.%20Our%20extensive%20empirical%20results%20demonstrate%0Athat%20TimeInf%20outperforms%20state-of-the-art%20methods%20in%20identifying%20harmful%0Aanomalies%20and%20helpful%20time%20points%20for%20forecasting.%20Additionally%2C%20TimeInf%20offers%0Aintuitive%20and%20interpretable%20attributions%20of%20data%20values%2C%20allowing%20us%20to%20easily%0Adistinguish%20diverse%20anomaly%20patterns%20through%20visualizations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15247v2&entry.124074799=Read"},
{"title": "Advances in Land Surface Model-based Forecasting: A comparative study of\n  LSTM, Gradient Boosting, and Feedforward Neural Network Models as prognostic\n  state emulators", "author": "Marieke Wesselkamp and Matthew Chantry and Ewan Pinnington and Margarita Choulga and Souhail Boussetta and Maria Kalweit and Joschka Boedecker and Carsten F. Dormann and Florian Pappenberger and Gianpaolo Balsamo", "abstract": "  Most useful weather prediction for the public is near the surface. The\nprocesses that are most relevant for near-surface weather prediction are also\nthose that are most interactive and exhibit positive feedback or have key role\nin energy partitioning. Land surface models (LSMs) consider these processes\ntogether with surface heterogeneity and forecast water, carbon and energy\nfluxes, and coupled with an atmospheric model provide boundary and initial\nconditions. This numerical parametrization of atmospheric boundaries being\ncomputationally expensive, statistical surrogate models are increasingly used\nto accelerated progress in experimental research. We evaluated the efficiency\nof three surrogate models in speeding up experimental research by simulating\nland surface processes, which are integral to forecasting water, carbon, and\nenergy fluxes in coupled atmospheric models. Specifically, we compared the\nperformance of a Long-Short Term Memory (LSTM) encoder-decoder network, extreme\ngradient boosting, and a feed-forward neural network within a physics-informed\nmulti-objective framework. This framework emulates key states of the ECMWF's\nIntegrated Forecasting System (IFS) land surface scheme, ECLand, across\ncontinental and global scales. Our findings indicate that while all models on\naverage demonstrate high accuracy over the forecast period, the LSTM network\nexcels in continental long-range predictions when carefully tuned, the XGB\nscores consistently high across tasks and the MLP provides an excellent\nimplementation-time-accuracy trade-off. The runtime reduction achieved by the\nemulators in comparison to the full numerical models are significant, offering\na faster, yet reliable alternative for conducting numerical experiments on land\nsurfaces.\n", "link": "http://arxiv.org/abs/2407.16463v1", "date": "2024-07-23", "relevancy": 1.5033, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5091}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5044}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advances%20in%20Land%20Surface%20Model-based%20Forecasting%3A%20A%20comparative%20study%20of%0A%20%20LSTM%2C%20Gradient%20Boosting%2C%20and%20Feedforward%20Neural%20Network%20Models%20as%20prognostic%0A%20%20state%20emulators&body=Title%3A%20Advances%20in%20Land%20Surface%20Model-based%20Forecasting%3A%20A%20comparative%20study%20of%0A%20%20LSTM%2C%20Gradient%20Boosting%2C%20and%20Feedforward%20Neural%20Network%20Models%20as%20prognostic%0A%20%20state%20emulators%0AAuthor%3A%20Marieke%20Wesselkamp%20and%20Matthew%20Chantry%20and%20Ewan%20Pinnington%20and%20Margarita%20Choulga%20and%20Souhail%20Boussetta%20and%20Maria%20Kalweit%20and%20Joschka%20Boedecker%20and%20Carsten%20F.%20Dormann%20and%20Florian%20Pappenberger%20and%20Gianpaolo%20Balsamo%0AAbstract%3A%20%20%20Most%20useful%20weather%20prediction%20for%20the%20public%20is%20near%20the%20surface.%20The%0Aprocesses%20that%20are%20most%20relevant%20for%20near-surface%20weather%20prediction%20are%20also%0Athose%20that%20are%20most%20interactive%20and%20exhibit%20positive%20feedback%20or%20have%20key%20role%0Ain%20energy%20partitioning.%20Land%20surface%20models%20%28LSMs%29%20consider%20these%20processes%0Atogether%20with%20surface%20heterogeneity%20and%20forecast%20water%2C%20carbon%20and%20energy%0Afluxes%2C%20and%20coupled%20with%20an%20atmospheric%20model%20provide%20boundary%20and%20initial%0Aconditions.%20This%20numerical%20parametrization%20of%20atmospheric%20boundaries%20being%0Acomputationally%20expensive%2C%20statistical%20surrogate%20models%20are%20increasingly%20used%0Ato%20accelerated%20progress%20in%20experimental%20research.%20We%20evaluated%20the%20efficiency%0Aof%20three%20surrogate%20models%20in%20speeding%20up%20experimental%20research%20by%20simulating%0Aland%20surface%20processes%2C%20which%20are%20integral%20to%20forecasting%20water%2C%20carbon%2C%20and%0Aenergy%20fluxes%20in%20coupled%20atmospheric%20models.%20Specifically%2C%20we%20compared%20the%0Aperformance%20of%20a%20Long-Short%20Term%20Memory%20%28LSTM%29%20encoder-decoder%20network%2C%20extreme%0Agradient%20boosting%2C%20and%20a%20feed-forward%20neural%20network%20within%20a%20physics-informed%0Amulti-objective%20framework.%20This%20framework%20emulates%20key%20states%20of%20the%20ECMWF%27s%0AIntegrated%20Forecasting%20System%20%28IFS%29%20land%20surface%20scheme%2C%20ECLand%2C%20across%0Acontinental%20and%20global%20scales.%20Our%20findings%20indicate%20that%20while%20all%20models%20on%0Aaverage%20demonstrate%20high%20accuracy%20over%20the%20forecast%20period%2C%20the%20LSTM%20network%0Aexcels%20in%20continental%20long-range%20predictions%20when%20carefully%20tuned%2C%20the%20XGB%0Ascores%20consistently%20high%20across%20tasks%20and%20the%20MLP%20provides%20an%20excellent%0Aimplementation-time-accuracy%20trade-off.%20The%20runtime%20reduction%20achieved%20by%20the%0Aemulators%20in%20comparison%20to%20the%20full%20numerical%20models%20are%20significant%2C%20offering%0Aa%20faster%2C%20yet%20reliable%20alternative%20for%20conducting%20numerical%20experiments%20on%20land%0Asurfaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvances%2520in%2520Land%2520Surface%2520Model-based%2520Forecasting%253A%2520A%2520comparative%2520study%2520of%250A%2520%2520LSTM%252C%2520Gradient%2520Boosting%252C%2520and%2520Feedforward%2520Neural%2520Network%2520Models%2520as%2520prognostic%250A%2520%2520state%2520emulators%26entry.906535625%3DMarieke%2520Wesselkamp%2520and%2520Matthew%2520Chantry%2520and%2520Ewan%2520Pinnington%2520and%2520Margarita%2520Choulga%2520and%2520Souhail%2520Boussetta%2520and%2520Maria%2520Kalweit%2520and%2520Joschka%2520Boedecker%2520and%2520Carsten%2520F.%2520Dormann%2520and%2520Florian%2520Pappenberger%2520and%2520Gianpaolo%2520Balsamo%26entry.1292438233%3D%2520%2520Most%2520useful%2520weather%2520prediction%2520for%2520the%2520public%2520is%2520near%2520the%2520surface.%2520The%250Aprocesses%2520that%2520are%2520most%2520relevant%2520for%2520near-surface%2520weather%2520prediction%2520are%2520also%250Athose%2520that%2520are%2520most%2520interactive%2520and%2520exhibit%2520positive%2520feedback%2520or%2520have%2520key%2520role%250Ain%2520energy%2520partitioning.%2520Land%2520surface%2520models%2520%2528LSMs%2529%2520consider%2520these%2520processes%250Atogether%2520with%2520surface%2520heterogeneity%2520and%2520forecast%2520water%252C%2520carbon%2520and%2520energy%250Afluxes%252C%2520and%2520coupled%2520with%2520an%2520atmospheric%2520model%2520provide%2520boundary%2520and%2520initial%250Aconditions.%2520This%2520numerical%2520parametrization%2520of%2520atmospheric%2520boundaries%2520being%250Acomputationally%2520expensive%252C%2520statistical%2520surrogate%2520models%2520are%2520increasingly%2520used%250Ato%2520accelerated%2520progress%2520in%2520experimental%2520research.%2520We%2520evaluated%2520the%2520efficiency%250Aof%2520three%2520surrogate%2520models%2520in%2520speeding%2520up%2520experimental%2520research%2520by%2520simulating%250Aland%2520surface%2520processes%252C%2520which%2520are%2520integral%2520to%2520forecasting%2520water%252C%2520carbon%252C%2520and%250Aenergy%2520fluxes%2520in%2520coupled%2520atmospheric%2520models.%2520Specifically%252C%2520we%2520compared%2520the%250Aperformance%2520of%2520a%2520Long-Short%2520Term%2520Memory%2520%2528LSTM%2529%2520encoder-decoder%2520network%252C%2520extreme%250Agradient%2520boosting%252C%2520and%2520a%2520feed-forward%2520neural%2520network%2520within%2520a%2520physics-informed%250Amulti-objective%2520framework.%2520This%2520framework%2520emulates%2520key%2520states%2520of%2520the%2520ECMWF%2527s%250AIntegrated%2520Forecasting%2520System%2520%2528IFS%2529%2520land%2520surface%2520scheme%252C%2520ECLand%252C%2520across%250Acontinental%2520and%2520global%2520scales.%2520Our%2520findings%2520indicate%2520that%2520while%2520all%2520models%2520on%250Aaverage%2520demonstrate%2520high%2520accuracy%2520over%2520the%2520forecast%2520period%252C%2520the%2520LSTM%2520network%250Aexcels%2520in%2520continental%2520long-range%2520predictions%2520when%2520carefully%2520tuned%252C%2520the%2520XGB%250Ascores%2520consistently%2520high%2520across%2520tasks%2520and%2520the%2520MLP%2520provides%2520an%2520excellent%250Aimplementation-time-accuracy%2520trade-off.%2520The%2520runtime%2520reduction%2520achieved%2520by%2520the%250Aemulators%2520in%2520comparison%2520to%2520the%2520full%2520numerical%2520models%2520are%2520significant%252C%2520offering%250Aa%2520faster%252C%2520yet%2520reliable%2520alternative%2520for%2520conducting%2520numerical%2520experiments%2520on%2520land%250Asurfaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20in%20Land%20Surface%20Model-based%20Forecasting%3A%20A%20comparative%20study%20of%0A%20%20LSTM%2C%20Gradient%20Boosting%2C%20and%20Feedforward%20Neural%20Network%20Models%20as%20prognostic%0A%20%20state%20emulators&entry.906535625=Marieke%20Wesselkamp%20and%20Matthew%20Chantry%20and%20Ewan%20Pinnington%20and%20Margarita%20Choulga%20and%20Souhail%20Boussetta%20and%20Maria%20Kalweit%20and%20Joschka%20Boedecker%20and%20Carsten%20F.%20Dormann%20and%20Florian%20Pappenberger%20and%20Gianpaolo%20Balsamo&entry.1292438233=%20%20Most%20useful%20weather%20prediction%20for%20the%20public%20is%20near%20the%20surface.%20The%0Aprocesses%20that%20are%20most%20relevant%20for%20near-surface%20weather%20prediction%20are%20also%0Athose%20that%20are%20most%20interactive%20and%20exhibit%20positive%20feedback%20or%20have%20key%20role%0Ain%20energy%20partitioning.%20Land%20surface%20models%20%28LSMs%29%20consider%20these%20processes%0Atogether%20with%20surface%20heterogeneity%20and%20forecast%20water%2C%20carbon%20and%20energy%0Afluxes%2C%20and%20coupled%20with%20an%20atmospheric%20model%20provide%20boundary%20and%20initial%0Aconditions.%20This%20numerical%20parametrization%20of%20atmospheric%20boundaries%20being%0Acomputationally%20expensive%2C%20statistical%20surrogate%20models%20are%20increasingly%20used%0Ato%20accelerated%20progress%20in%20experimental%20research.%20We%20evaluated%20the%20efficiency%0Aof%20three%20surrogate%20models%20in%20speeding%20up%20experimental%20research%20by%20simulating%0Aland%20surface%20processes%2C%20which%20are%20integral%20to%20forecasting%20water%2C%20carbon%2C%20and%0Aenergy%20fluxes%20in%20coupled%20atmospheric%20models.%20Specifically%2C%20we%20compared%20the%0Aperformance%20of%20a%20Long-Short%20Term%20Memory%20%28LSTM%29%20encoder-decoder%20network%2C%20extreme%0Agradient%20boosting%2C%20and%20a%20feed-forward%20neural%20network%20within%20a%20physics-informed%0Amulti-objective%20framework.%20This%20framework%20emulates%20key%20states%20of%20the%20ECMWF%27s%0AIntegrated%20Forecasting%20System%20%28IFS%29%20land%20surface%20scheme%2C%20ECLand%2C%20across%0Acontinental%20and%20global%20scales.%20Our%20findings%20indicate%20that%20while%20all%20models%20on%0Aaverage%20demonstrate%20high%20accuracy%20over%20the%20forecast%20period%2C%20the%20LSTM%20network%0Aexcels%20in%20continental%20long-range%20predictions%20when%20carefully%20tuned%2C%20the%20XGB%0Ascores%20consistently%20high%20across%20tasks%20and%20the%20MLP%20provides%20an%20excellent%0Aimplementation-time-accuracy%20trade-off.%20The%20runtime%20reduction%20achieved%20by%20the%0Aemulators%20in%20comparison%20to%20the%20full%20numerical%20models%20are%20significant%2C%20offering%0Aa%20faster%2C%20yet%20reliable%20alternative%20for%20conducting%20numerical%20experiments%20on%20land%0Asurfaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16463v1&entry.124074799=Read"},
{"title": "Virtue Ethics For Ethically Tunable Robotic Assistants", "author": "Rajitha Ramanayake and Vivek Nallur", "abstract": "  The common consensus is that robots designed to work alongside or serve\nhumans must adhere to the ethical standards of their operational environment.\nTo achieve this, several methods based on established ethical theories have\nbeen suggested. Nonetheless, numerous empirical studies show that the ethical\nrequirements of the real world are very diverse and can change rapidly from\nregion to region. This eliminates the idea of a universal robot that can fit\ninto any ethical context. However, creating customised robots for each\ndeployment, using existing techniques is challenging. This paper presents a way\nto overcome this challenge by introducing a virtue ethics inspired\ncomputational method that enables character-based tuning of robots to\naccommodate the specific ethical needs of an environment. Using a simulated\nelder-care environment, we illustrate how tuning can be used to change the\nbehaviour of a robot that interacts with an elderly resident in an\nambient-assisted environment. Further, we assess the robot's responses by\nconsulting ethicists to identify potential shortcomings.\n", "link": "http://arxiv.org/abs/2407.16361v1", "date": "2024-07-23", "relevancy": 1.5234, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5254}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4981}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Virtue%20Ethics%20For%20Ethically%20Tunable%20Robotic%20Assistants&body=Title%3A%20Virtue%20Ethics%20For%20Ethically%20Tunable%20Robotic%20Assistants%0AAuthor%3A%20Rajitha%20Ramanayake%20and%20Vivek%20Nallur%0AAbstract%3A%20%20%20The%20common%20consensus%20is%20that%20robots%20designed%20to%20work%20alongside%20or%20serve%0Ahumans%20must%20adhere%20to%20the%20ethical%20standards%20of%20their%20operational%20environment.%0ATo%20achieve%20this%2C%20several%20methods%20based%20on%20established%20ethical%20theories%20have%0Abeen%20suggested.%20Nonetheless%2C%20numerous%20empirical%20studies%20show%20that%20the%20ethical%0Arequirements%20of%20the%20real%20world%20are%20very%20diverse%20and%20can%20change%20rapidly%20from%0Aregion%20to%20region.%20This%20eliminates%20the%20idea%20of%20a%20universal%20robot%20that%20can%20fit%0Ainto%20any%20ethical%20context.%20However%2C%20creating%20customised%20robots%20for%20each%0Adeployment%2C%20using%20existing%20techniques%20is%20challenging.%20This%20paper%20presents%20a%20way%0Ato%20overcome%20this%20challenge%20by%20introducing%20a%20virtue%20ethics%20inspired%0Acomputational%20method%20that%20enables%20character-based%20tuning%20of%20robots%20to%0Aaccommodate%20the%20specific%20ethical%20needs%20of%20an%20environment.%20Using%20a%20simulated%0Aelder-care%20environment%2C%20we%20illustrate%20how%20tuning%20can%20be%20used%20to%20change%20the%0Abehaviour%20of%20a%20robot%20that%20interacts%20with%20an%20elderly%20resident%20in%20an%0Aambient-assisted%20environment.%20Further%2C%20we%20assess%20the%20robot%27s%20responses%20by%0Aconsulting%20ethicists%20to%20identify%20potential%20shortcomings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirtue%2520Ethics%2520For%2520Ethically%2520Tunable%2520Robotic%2520Assistants%26entry.906535625%3DRajitha%2520Ramanayake%2520and%2520Vivek%2520Nallur%26entry.1292438233%3D%2520%2520The%2520common%2520consensus%2520is%2520that%2520robots%2520designed%2520to%2520work%2520alongside%2520or%2520serve%250Ahumans%2520must%2520adhere%2520to%2520the%2520ethical%2520standards%2520of%2520their%2520operational%2520environment.%250ATo%2520achieve%2520this%252C%2520several%2520methods%2520based%2520on%2520established%2520ethical%2520theories%2520have%250Abeen%2520suggested.%2520Nonetheless%252C%2520numerous%2520empirical%2520studies%2520show%2520that%2520the%2520ethical%250Arequirements%2520of%2520the%2520real%2520world%2520are%2520very%2520diverse%2520and%2520can%2520change%2520rapidly%2520from%250Aregion%2520to%2520region.%2520This%2520eliminates%2520the%2520idea%2520of%2520a%2520universal%2520robot%2520that%2520can%2520fit%250Ainto%2520any%2520ethical%2520context.%2520However%252C%2520creating%2520customised%2520robots%2520for%2520each%250Adeployment%252C%2520using%2520existing%2520techniques%2520is%2520challenging.%2520This%2520paper%2520presents%2520a%2520way%250Ato%2520overcome%2520this%2520challenge%2520by%2520introducing%2520a%2520virtue%2520ethics%2520inspired%250Acomputational%2520method%2520that%2520enables%2520character-based%2520tuning%2520of%2520robots%2520to%250Aaccommodate%2520the%2520specific%2520ethical%2520needs%2520of%2520an%2520environment.%2520Using%2520a%2520simulated%250Aelder-care%2520environment%252C%2520we%2520illustrate%2520how%2520tuning%2520can%2520be%2520used%2520to%2520change%2520the%250Abehaviour%2520of%2520a%2520robot%2520that%2520interacts%2520with%2520an%2520elderly%2520resident%2520in%2520an%250Aambient-assisted%2520environment.%2520Further%252C%2520we%2520assess%2520the%2520robot%2527s%2520responses%2520by%250Aconsulting%2520ethicists%2520to%2520identify%2520potential%2520shortcomings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Virtue%20Ethics%20For%20Ethically%20Tunable%20Robotic%20Assistants&entry.906535625=Rajitha%20Ramanayake%20and%20Vivek%20Nallur&entry.1292438233=%20%20The%20common%20consensus%20is%20that%20robots%20designed%20to%20work%20alongside%20or%20serve%0Ahumans%20must%20adhere%20to%20the%20ethical%20standards%20of%20their%20operational%20environment.%0ATo%20achieve%20this%2C%20several%20methods%20based%20on%20established%20ethical%20theories%20have%0Abeen%20suggested.%20Nonetheless%2C%20numerous%20empirical%20studies%20show%20that%20the%20ethical%0Arequirements%20of%20the%20real%20world%20are%20very%20diverse%20and%20can%20change%20rapidly%20from%0Aregion%20to%20region.%20This%20eliminates%20the%20idea%20of%20a%20universal%20robot%20that%20can%20fit%0Ainto%20any%20ethical%20context.%20However%2C%20creating%20customised%20robots%20for%20each%0Adeployment%2C%20using%20existing%20techniques%20is%20challenging.%20This%20paper%20presents%20a%20way%0Ato%20overcome%20this%20challenge%20by%20introducing%20a%20virtue%20ethics%20inspired%0Acomputational%20method%20that%20enables%20character-based%20tuning%20of%20robots%20to%0Aaccommodate%20the%20specific%20ethical%20needs%20of%20an%20environment.%20Using%20a%20simulated%0Aelder-care%20environment%2C%20we%20illustrate%20how%20tuning%20can%20be%20used%20to%20change%20the%0Abehaviour%20of%20a%20robot%20that%20interacts%20with%20an%20elderly%20resident%20in%20an%0Aambient-assisted%20environment.%20Further%2C%20we%20assess%20the%20robot%27s%20responses%20by%0Aconsulting%20ethicists%20to%20identify%20potential%20shortcomings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16361v1&entry.124074799=Read"},
{"title": "Anwendung von Causal-Discovery-Algorithmen zur Root-Cause-Analyse in der\n  Fahrzeugmontage", "author": "Lucas Possner and Lukas Bahr and Leonard Roehl and Christoph Wehner and Sophie Groeger", "abstract": "  Root Cause Analysis (RCA) is a quality management method that aims to\nsystematically investigate and identify the cause-and-effect relationships of\nproblems and their underlying causes. Traditional methods are based on the\nanalysis of problems by subject matter experts. In modern production processes,\nlarge amounts of data are collected. For this reason, increasingly\ncomputer-aided and data-driven methods are used for RCA. One of these methods\nare Causal Discovery Algorithms (CDA). This publication demonstrates the\napplication of CDA on data from the assembly of a leading automotive\nmanufacturer. The algorithms used learn the causal structure between the\ncharacteristics of the manufactured vehicles, the ergonomics and the temporal\nscope of the involved assembly processes, and quality-relevant product features\nbased on representative data. This publication compares various CDAs in terms\nof their suitability in the context of quality management. For this purpose,\nthe causal structures learned by the algorithms as well as their runtime are\ncompared. This publication provides a contribution to quality management and\ndemonstrates how CDAs can be used for RCA in assembly processes.\n", "link": "http://arxiv.org/abs/2407.16388v1", "date": "2024-07-23", "relevancy": 1.1116, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4033}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3668}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anwendung%20von%20Causal-Discovery-Algorithmen%20zur%20Root-Cause-Analyse%20in%20der%0A%20%20Fahrzeugmontage&body=Title%3A%20Anwendung%20von%20Causal-Discovery-Algorithmen%20zur%20Root-Cause-Analyse%20in%20der%0A%20%20Fahrzeugmontage%0AAuthor%3A%20Lucas%20Possner%20and%20Lukas%20Bahr%20and%20Leonard%20Roehl%20and%20Christoph%20Wehner%20and%20Sophie%20Groeger%0AAbstract%3A%20%20%20Root%20Cause%20Analysis%20%28RCA%29%20is%20a%20quality%20management%20method%20that%20aims%20to%0Asystematically%20investigate%20and%20identify%20the%20cause-and-effect%20relationships%20of%0Aproblems%20and%20their%20underlying%20causes.%20Traditional%20methods%20are%20based%20on%20the%0Aanalysis%20of%20problems%20by%20subject%20matter%20experts.%20In%20modern%20production%20processes%2C%0Alarge%20amounts%20of%20data%20are%20collected.%20For%20this%20reason%2C%20increasingly%0Acomputer-aided%20and%20data-driven%20methods%20are%20used%20for%20RCA.%20One%20of%20these%20methods%0Aare%20Causal%20Discovery%20Algorithms%20%28CDA%29.%20This%20publication%20demonstrates%20the%0Aapplication%20of%20CDA%20on%20data%20from%20the%20assembly%20of%20a%20leading%20automotive%0Amanufacturer.%20The%20algorithms%20used%20learn%20the%20causal%20structure%20between%20the%0Acharacteristics%20of%20the%20manufactured%20vehicles%2C%20the%20ergonomics%20and%20the%20temporal%0Ascope%20of%20the%20involved%20assembly%20processes%2C%20and%20quality-relevant%20product%20features%0Abased%20on%20representative%20data.%20This%20publication%20compares%20various%20CDAs%20in%20terms%0Aof%20their%20suitability%20in%20the%20context%20of%20quality%20management.%20For%20this%20purpose%2C%0Athe%20causal%20structures%20learned%20by%20the%20algorithms%20as%20well%20as%20their%20runtime%20are%0Acompared.%20This%20publication%20provides%20a%20contribution%20to%20quality%20management%20and%0Ademonstrates%20how%20CDAs%20can%20be%20used%20for%20RCA%20in%20assembly%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnwendung%2520von%2520Causal-Discovery-Algorithmen%2520zur%2520Root-Cause-Analyse%2520in%2520der%250A%2520%2520Fahrzeugmontage%26entry.906535625%3DLucas%2520Possner%2520and%2520Lukas%2520Bahr%2520and%2520Leonard%2520Roehl%2520and%2520Christoph%2520Wehner%2520and%2520Sophie%2520Groeger%26entry.1292438233%3D%2520%2520Root%2520Cause%2520Analysis%2520%2528RCA%2529%2520is%2520a%2520quality%2520management%2520method%2520that%2520aims%2520to%250Asystematically%2520investigate%2520and%2520identify%2520the%2520cause-and-effect%2520relationships%2520of%250Aproblems%2520and%2520their%2520underlying%2520causes.%2520Traditional%2520methods%2520are%2520based%2520on%2520the%250Aanalysis%2520of%2520problems%2520by%2520subject%2520matter%2520experts.%2520In%2520modern%2520production%2520processes%252C%250Alarge%2520amounts%2520of%2520data%2520are%2520collected.%2520For%2520this%2520reason%252C%2520increasingly%250Acomputer-aided%2520and%2520data-driven%2520methods%2520are%2520used%2520for%2520RCA.%2520One%2520of%2520these%2520methods%250Aare%2520Causal%2520Discovery%2520Algorithms%2520%2528CDA%2529.%2520This%2520publication%2520demonstrates%2520the%250Aapplication%2520of%2520CDA%2520on%2520data%2520from%2520the%2520assembly%2520of%2520a%2520leading%2520automotive%250Amanufacturer.%2520The%2520algorithms%2520used%2520learn%2520the%2520causal%2520structure%2520between%2520the%250Acharacteristics%2520of%2520the%2520manufactured%2520vehicles%252C%2520the%2520ergonomics%2520and%2520the%2520temporal%250Ascope%2520of%2520the%2520involved%2520assembly%2520processes%252C%2520and%2520quality-relevant%2520product%2520features%250Abased%2520on%2520representative%2520data.%2520This%2520publication%2520compares%2520various%2520CDAs%2520in%2520terms%250Aof%2520their%2520suitability%2520in%2520the%2520context%2520of%2520quality%2520management.%2520For%2520this%2520purpose%252C%250Athe%2520causal%2520structures%2520learned%2520by%2520the%2520algorithms%2520as%2520well%2520as%2520their%2520runtime%2520are%250Acompared.%2520This%2520publication%2520provides%2520a%2520contribution%2520to%2520quality%2520management%2520and%250Ademonstrates%2520how%2520CDAs%2520can%2520be%2520used%2520for%2520RCA%2520in%2520assembly%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anwendung%20von%20Causal-Discovery-Algorithmen%20zur%20Root-Cause-Analyse%20in%20der%0A%20%20Fahrzeugmontage&entry.906535625=Lucas%20Possner%20and%20Lukas%20Bahr%20and%20Leonard%20Roehl%20and%20Christoph%20Wehner%20and%20Sophie%20Groeger&entry.1292438233=%20%20Root%20Cause%20Analysis%20%28RCA%29%20is%20a%20quality%20management%20method%20that%20aims%20to%0Asystematically%20investigate%20and%20identify%20the%20cause-and-effect%20relationships%20of%0Aproblems%20and%20their%20underlying%20causes.%20Traditional%20methods%20are%20based%20on%20the%0Aanalysis%20of%20problems%20by%20subject%20matter%20experts.%20In%20modern%20production%20processes%2C%0Alarge%20amounts%20of%20data%20are%20collected.%20For%20this%20reason%2C%20increasingly%0Acomputer-aided%20and%20data-driven%20methods%20are%20used%20for%20RCA.%20One%20of%20these%20methods%0Aare%20Causal%20Discovery%20Algorithms%20%28CDA%29.%20This%20publication%20demonstrates%20the%0Aapplication%20of%20CDA%20on%20data%20from%20the%20assembly%20of%20a%20leading%20automotive%0Amanufacturer.%20The%20algorithms%20used%20learn%20the%20causal%20structure%20between%20the%0Acharacteristics%20of%20the%20manufactured%20vehicles%2C%20the%20ergonomics%20and%20the%20temporal%0Ascope%20of%20the%20involved%20assembly%20processes%2C%20and%20quality-relevant%20product%20features%0Abased%20on%20representative%20data.%20This%20publication%20compares%20various%20CDAs%20in%20terms%0Aof%20their%20suitability%20in%20the%20context%20of%20quality%20management.%20For%20this%20purpose%2C%0Athe%20causal%20structures%20learned%20by%20the%20algorithms%20as%20well%20as%20their%20runtime%20are%0Acompared.%20This%20publication%20provides%20a%20contribution%20to%20quality%20management%20and%0Ademonstrates%20how%20CDAs%20can%20be%20used%20for%20RCA%20in%20assembly%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16388v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


