<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251201.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Sketch-guided Cage-based 3D Gaussian Splatting Deformation", "author": "Tianhao Xie and Noam Aigerman and Eugene Belilovsky and Tiberiu Popa", "abstract": "3D Gaussian Splatting (GS) is one of the most promising novel 3D representations that has received great interest in computer graphics and computer vision. While various systems have introduced editing capabilities for 3D GS, such as those guided by text prompts, fine-grained control over deformation remains an open challenge. In this work, we present a novel sketch-guided 3D GS deformation system that allows users to intuitively modify the geometry of a 3D GS model by drawing a silhouette sketch from a single viewpoint. Our approach introduces a new deformation method that combines cage-based deformations with a variant of Neural Jacobian Fields, enabling precise, fine-grained control. Additionally, it leverages large-scale 2D diffusion priors and ControlNet to ensure the generated deformations are semantically plausible. Through a series of experiments, we demonstrate the effectiveness of our method and showcase its ability to animate static 3D GS models as one of its key applications.", "link": "http://arxiv.org/abs/2411.12168v3", "date": "2025-12-01", "relevancy": 3.3125, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6816}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6535}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch-guided%20Cage-based%203D%20Gaussian%20Splatting%20Deformation&body=Title%3A%20Sketch-guided%20Cage-based%203D%20Gaussian%20Splatting%20Deformation%0AAuthor%3A%20Tianhao%20Xie%20and%20Noam%20Aigerman%20and%20Eugene%20Belilovsky%20and%20Tiberiu%20Popa%0AAbstract%3A%203D%20Gaussian%20Splatting%20%28GS%29%20is%20one%20of%20the%20most%20promising%20novel%203D%20representations%20that%20has%20received%20great%20interest%20in%20computer%20graphics%20and%20computer%20vision.%20While%20various%20systems%20have%20introduced%20editing%20capabilities%20for%203D%20GS%2C%20such%20as%20those%20guided%20by%20text%20prompts%2C%20fine-grained%20control%20over%20deformation%20remains%20an%20open%20challenge.%20In%20this%20work%2C%20we%20present%20a%20novel%20sketch-guided%203D%20GS%20deformation%20system%20that%20allows%20users%20to%20intuitively%20modify%20the%20geometry%20of%20a%203D%20GS%20model%20by%20drawing%20a%20silhouette%20sketch%20from%20a%20single%20viewpoint.%20Our%20approach%20introduces%20a%20new%20deformation%20method%20that%20combines%20cage-based%20deformations%20with%20a%20variant%20of%20Neural%20Jacobian%20Fields%2C%20enabling%20precise%2C%20fine-grained%20control.%20Additionally%2C%20it%20leverages%20large-scale%202D%20diffusion%20priors%20and%20ControlNet%20to%20ensure%20the%20generated%20deformations%20are%20semantically%20plausible.%20Through%20a%20series%20of%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20method%20and%20showcase%20its%20ability%20to%20animate%20static%203D%20GS%20models%20as%20one%20of%20its%20key%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2411.12168v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch-guided%2520Cage-based%25203D%2520Gaussian%2520Splatting%2520Deformation%26entry.906535625%3DTianhao%2520Xie%2520and%2520Noam%2520Aigerman%2520and%2520Eugene%2520Belilovsky%2520and%2520Tiberiu%2520Popa%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%2528GS%2529%2520is%2520one%2520of%2520the%2520most%2520promising%2520novel%25203D%2520representations%2520that%2520has%2520received%2520great%2520interest%2520in%2520computer%2520graphics%2520and%2520computer%2520vision.%2520While%2520various%2520systems%2520have%2520introduced%2520editing%2520capabilities%2520for%25203D%2520GS%252C%2520such%2520as%2520those%2520guided%2520by%2520text%2520prompts%252C%2520fine-grained%2520control%2520over%2520deformation%2520remains%2520an%2520open%2520challenge.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520sketch-guided%25203D%2520GS%2520deformation%2520system%2520that%2520allows%2520users%2520to%2520intuitively%2520modify%2520the%2520geometry%2520of%2520a%25203D%2520GS%2520model%2520by%2520drawing%2520a%2520silhouette%2520sketch%2520from%2520a%2520single%2520viewpoint.%2520Our%2520approach%2520introduces%2520a%2520new%2520deformation%2520method%2520that%2520combines%2520cage-based%2520deformations%2520with%2520a%2520variant%2520of%2520Neural%2520Jacobian%2520Fields%252C%2520enabling%2520precise%252C%2520fine-grained%2520control.%2520Additionally%252C%2520it%2520leverages%2520large-scale%25202D%2520diffusion%2520priors%2520and%2520ControlNet%2520to%2520ensure%2520the%2520generated%2520deformations%2520are%2520semantically%2520plausible.%2520Through%2520a%2520series%2520of%2520experiments%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520and%2520showcase%2520its%2520ability%2520to%2520animate%2520static%25203D%2520GS%2520models%2520as%2520one%2520of%2520its%2520key%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12168v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch-guided%20Cage-based%203D%20Gaussian%20Splatting%20Deformation&entry.906535625=Tianhao%20Xie%20and%20Noam%20Aigerman%20and%20Eugene%20Belilovsky%20and%20Tiberiu%20Popa&entry.1292438233=3D%20Gaussian%20Splatting%20%28GS%29%20is%20one%20of%20the%20most%20promising%20novel%203D%20representations%20that%20has%20received%20great%20interest%20in%20computer%20graphics%20and%20computer%20vision.%20While%20various%20systems%20have%20introduced%20editing%20capabilities%20for%203D%20GS%2C%20such%20as%20those%20guided%20by%20text%20prompts%2C%20fine-grained%20control%20over%20deformation%20remains%20an%20open%20challenge.%20In%20this%20work%2C%20we%20present%20a%20novel%20sketch-guided%203D%20GS%20deformation%20system%20that%20allows%20users%20to%20intuitively%20modify%20the%20geometry%20of%20a%203D%20GS%20model%20by%20drawing%20a%20silhouette%20sketch%20from%20a%20single%20viewpoint.%20Our%20approach%20introduces%20a%20new%20deformation%20method%20that%20combines%20cage-based%20deformations%20with%20a%20variant%20of%20Neural%20Jacobian%20Fields%2C%20enabling%20precise%2C%20fine-grained%20control.%20Additionally%2C%20it%20leverages%20large-scale%202D%20diffusion%20priors%20and%20ControlNet%20to%20ensure%20the%20generated%20deformations%20are%20semantically%20plausible.%20Through%20a%20series%20of%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20method%20and%20showcase%20its%20ability%20to%20animate%20static%203D%20GS%20models%20as%20one%20of%20its%20key%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2411.12168v3&entry.124074799=Read"},
{"title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos", "author": "Xavier Thomas and Youngsun Lim and Ananya Srinivasan and Audrey Zheng and Deepti Ghadiyaram", "abstract": "Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.", "link": "http://arxiv.org/abs/2512.01803v1", "date": "2025-12-01", "relevancy": 3.2221, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6635}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6457}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Action%20Tell-Tales%3A%20Assessing%20Human%20Motion%20in%20Synthesized%20Videos&body=Title%3A%20Generative%20Action%20Tell-Tales%3A%20Assessing%20Human%20Motion%20in%20Synthesized%20Videos%0AAuthor%3A%20Xavier%20Thomas%20and%20Youngsun%20Lim%20and%20Ananya%20Srinivasan%20and%20Audrey%20Zheng%20and%20Deepti%20Ghadiyaram%0AAbstract%3A%20Despite%20rapid%20advances%20in%20video%20generative%20models%2C%20robust%20metrics%20for%20evaluating%20visual%20and%20temporal%20correctness%20of%20complex%20human%20actions%20remain%20elusive.%20Critically%2C%20existing%20pure-vision%20encoders%20and%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20strongly%20appearance-biased%2C%20lack%20temporal%20understanding%2C%20and%20thus%20struggle%20to%20discern%20intricate%20motion%20dynamics%20and%20anatomical%20implausibilities%20in%20generated%20videos.%20We%20tackle%20this%20gap%20by%20introducing%20a%20novel%20evaluation%20metric%20derived%20from%20a%20learned%20latent%20space%20of%20real-world%20human%20actions.%20Our%20method%20first%20captures%20the%20nuances%2C%20constraints%2C%20and%20temporal%20smoothness%20of%20real-world%20motion%20by%20fusing%20appearance-agnostic%20human%20skeletal%20geometry%20features%20with%20appearance-based%20features.%20We%20posit%20that%20this%20combined%20feature%20space%20provides%20a%20robust%20representation%20of%20action%20plausibility.%20Given%20a%20generated%20video%2C%20our%20metric%20quantifies%20its%20action%20quality%20by%20measuring%20the%20distance%20between%20its%20underlying%20representations%20and%20this%20learned%20real-world%20action%20distribution.%20For%20rigorous%20validation%2C%20we%20develop%20a%20new%20multi-faceted%20benchmark%20specifically%20designed%20to%20probe%20temporally%20challenging%20aspects%20of%20human%20action%20fidelity.%20Through%20extensive%20experiments%2C%20we%20show%20that%20our%20metric%20achieves%20substantial%20improvement%20of%20more%20than%2068%25%20compared%20to%20existing%20state-of-the-art%20methods%20on%20our%20benchmark%2C%20performs%20competitively%20on%20established%20external%20benchmarks%2C%20and%20has%20a%20stronger%20correlation%20with%20human%20perception.%20Our%20in-depth%20analysis%20reveals%20critical%20limitations%20in%20current%20video%20generative%20models%20and%20establishes%20a%20new%20standard%20for%20advanced%20research%20in%20video%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Action%2520Tell-Tales%253A%2520Assessing%2520Human%2520Motion%2520in%2520Synthesized%2520Videos%26entry.906535625%3DXavier%2520Thomas%2520and%2520Youngsun%2520Lim%2520and%2520Ananya%2520Srinivasan%2520and%2520Audrey%2520Zheng%2520and%2520Deepti%2520Ghadiyaram%26entry.1292438233%3DDespite%2520rapid%2520advances%2520in%2520video%2520generative%2520models%252C%2520robust%2520metrics%2520for%2520evaluating%2520visual%2520and%2520temporal%2520correctness%2520of%2520complex%2520human%2520actions%2520remain%2520elusive.%2520Critically%252C%2520existing%2520pure-vision%2520encoders%2520and%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520strongly%2520appearance-biased%252C%2520lack%2520temporal%2520understanding%252C%2520and%2520thus%2520struggle%2520to%2520discern%2520intricate%2520motion%2520dynamics%2520and%2520anatomical%2520implausibilities%2520in%2520generated%2520videos.%2520We%2520tackle%2520this%2520gap%2520by%2520introducing%2520a%2520novel%2520evaluation%2520metric%2520derived%2520from%2520a%2520learned%2520latent%2520space%2520of%2520real-world%2520human%2520actions.%2520Our%2520method%2520first%2520captures%2520the%2520nuances%252C%2520constraints%252C%2520and%2520temporal%2520smoothness%2520of%2520real-world%2520motion%2520by%2520fusing%2520appearance-agnostic%2520human%2520skeletal%2520geometry%2520features%2520with%2520appearance-based%2520features.%2520We%2520posit%2520that%2520this%2520combined%2520feature%2520space%2520provides%2520a%2520robust%2520representation%2520of%2520action%2520plausibility.%2520Given%2520a%2520generated%2520video%252C%2520our%2520metric%2520quantifies%2520its%2520action%2520quality%2520by%2520measuring%2520the%2520distance%2520between%2520its%2520underlying%2520representations%2520and%2520this%2520learned%2520real-world%2520action%2520distribution.%2520For%2520rigorous%2520validation%252C%2520we%2520develop%2520a%2520new%2520multi-faceted%2520benchmark%2520specifically%2520designed%2520to%2520probe%2520temporally%2520challenging%2520aspects%2520of%2520human%2520action%2520fidelity.%2520Through%2520extensive%2520experiments%252C%2520we%2520show%2520that%2520our%2520metric%2520achieves%2520substantial%2520improvement%2520of%2520more%2520than%252068%2525%2520compared%2520to%2520existing%2520state-of-the-art%2520methods%2520on%2520our%2520benchmark%252C%2520performs%2520competitively%2520on%2520established%2520external%2520benchmarks%252C%2520and%2520has%2520a%2520stronger%2520correlation%2520with%2520human%2520perception.%2520Our%2520in-depth%2520analysis%2520reveals%2520critical%2520limitations%2520in%2520current%2520video%2520generative%2520models%2520and%2520establishes%2520a%2520new%2520standard%2520for%2520advanced%2520research%2520in%2520video%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Action%20Tell-Tales%3A%20Assessing%20Human%20Motion%20in%20Synthesized%20Videos&entry.906535625=Xavier%20Thomas%20and%20Youngsun%20Lim%20and%20Ananya%20Srinivasan%20and%20Audrey%20Zheng%20and%20Deepti%20Ghadiyaram&entry.1292438233=Despite%20rapid%20advances%20in%20video%20generative%20models%2C%20robust%20metrics%20for%20evaluating%20visual%20and%20temporal%20correctness%20of%20complex%20human%20actions%20remain%20elusive.%20Critically%2C%20existing%20pure-vision%20encoders%20and%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20strongly%20appearance-biased%2C%20lack%20temporal%20understanding%2C%20and%20thus%20struggle%20to%20discern%20intricate%20motion%20dynamics%20and%20anatomical%20implausibilities%20in%20generated%20videos.%20We%20tackle%20this%20gap%20by%20introducing%20a%20novel%20evaluation%20metric%20derived%20from%20a%20learned%20latent%20space%20of%20real-world%20human%20actions.%20Our%20method%20first%20captures%20the%20nuances%2C%20constraints%2C%20and%20temporal%20smoothness%20of%20real-world%20motion%20by%20fusing%20appearance-agnostic%20human%20skeletal%20geometry%20features%20with%20appearance-based%20features.%20We%20posit%20that%20this%20combined%20feature%20space%20provides%20a%20robust%20representation%20of%20action%20plausibility.%20Given%20a%20generated%20video%2C%20our%20metric%20quantifies%20its%20action%20quality%20by%20measuring%20the%20distance%20between%20its%20underlying%20representations%20and%20this%20learned%20real-world%20action%20distribution.%20For%20rigorous%20validation%2C%20we%20develop%20a%20new%20multi-faceted%20benchmark%20specifically%20designed%20to%20probe%20temporally%20challenging%20aspects%20of%20human%20action%20fidelity.%20Through%20extensive%20experiments%2C%20we%20show%20that%20our%20metric%20achieves%20substantial%20improvement%20of%20more%20than%2068%25%20compared%20to%20existing%20state-of-the-art%20methods%20on%20our%20benchmark%2C%20performs%20competitively%20on%20established%20external%20benchmarks%2C%20and%20has%20a%20stronger%20correlation%20with%20human%20perception.%20Our%20in-depth%20analysis%20reveals%20critical%20limitations%20in%20current%20video%20generative%20models%20and%20establishes%20a%20new%20standard%20for%20advanced%20research%20in%20video%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.01803v1&entry.124074799=Read"},
{"title": "DenoiseGS: Gaussian Reconstruction Model for Burst Denoising", "author": "Yongsen Cheng and Yuanhao Cai and Yulun Zhang", "abstract": "Burst denoising methods are crucial for enhancing images captured on handheld devices, but they often struggle with large motion or suffer from prohibitive computational costs. In this paper, we propose DenoiseGS, the first framework to leverage the efficiency of 3D Gaussian Splatting for burst denoising. Our approach addresses two key challenges when applying feedforward Gaussian reconsturction model to noisy inputs: the degradation of Gaussian point clouds and the loss of fine details. To this end, we propose a Gaussian self-consistency (GSC) loss, which regularizes the geometry predicted from noisy inputs with high-quality Gaussian point clouds. These point clouds are generated from clean inputs by the same model that we are training, thereby alleviating potential bias or domain gaps. Additionally, we introduce a log-weighted frequency (LWF) loss to strengthen supervision within the spectral domain, effectively preserving fine-grained details. The LWF loss adaptively weights frequency discrepancies in a logarithmic manner, emphasizing challenging high-frequency details. Extensive experiments demonstrate that DenoiseGS significantly exceeds the state-of-the-art NeRF-based methods on both burst denoising and novel view synthesis under noisy conditions, while achieving 250$\\times$ faster inference speed. Code and models are released at https://github.com/yscheng04/DenoiseGS.", "link": "http://arxiv.org/abs/2511.22939v2", "date": "2025-12-01", "relevancy": 3.0984, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6493}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6064}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DenoiseGS%3A%20Gaussian%20Reconstruction%20Model%20for%20Burst%20Denoising&body=Title%3A%20DenoiseGS%3A%20Gaussian%20Reconstruction%20Model%20for%20Burst%20Denoising%0AAuthor%3A%20Yongsen%20Cheng%20and%20Yuanhao%20Cai%20and%20Yulun%20Zhang%0AAbstract%3A%20Burst%20denoising%20methods%20are%20crucial%20for%20enhancing%20images%20captured%20on%20handheld%20devices%2C%20but%20they%20often%20struggle%20with%20large%20motion%20or%20suffer%20from%20prohibitive%20computational%20costs.%20In%20this%20paper%2C%20we%20propose%20DenoiseGS%2C%20the%20first%20framework%20to%20leverage%20the%20efficiency%20of%203D%20Gaussian%20Splatting%20for%20burst%20denoising.%20Our%20approach%20addresses%20two%20key%20challenges%20when%20applying%20feedforward%20Gaussian%20reconsturction%20model%20to%20noisy%20inputs%3A%20the%20degradation%20of%20Gaussian%20point%20clouds%20and%20the%20loss%20of%20fine%20details.%20To%20this%20end%2C%20we%20propose%20a%20Gaussian%20self-consistency%20%28GSC%29%20loss%2C%20which%20regularizes%20the%20geometry%20predicted%20from%20noisy%20inputs%20with%20high-quality%20Gaussian%20point%20clouds.%20These%20point%20clouds%20are%20generated%20from%20clean%20inputs%20by%20the%20same%20model%20that%20we%20are%20training%2C%20thereby%20alleviating%20potential%20bias%20or%20domain%20gaps.%20Additionally%2C%20we%20introduce%20a%20log-weighted%20frequency%20%28LWF%29%20loss%20to%20strengthen%20supervision%20within%20the%20spectral%20domain%2C%20effectively%20preserving%20fine-grained%20details.%20The%20LWF%20loss%20adaptively%20weights%20frequency%20discrepancies%20in%20a%20logarithmic%20manner%2C%20emphasizing%20challenging%20high-frequency%20details.%20Extensive%20experiments%20demonstrate%20that%20DenoiseGS%20significantly%20exceeds%20the%20state-of-the-art%20NeRF-based%20methods%20on%20both%20burst%20denoising%20and%20novel%20view%20synthesis%20under%20noisy%20conditions%2C%20while%20achieving%20250%24%5Ctimes%24%20faster%20inference%20speed.%20Code%20and%20models%20are%20released%20at%20https%3A//github.com/yscheng04/DenoiseGS.%0ALink%3A%20http%3A//arxiv.org/abs/2511.22939v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoiseGS%253A%2520Gaussian%2520Reconstruction%2520Model%2520for%2520Burst%2520Denoising%26entry.906535625%3DYongsen%2520Cheng%2520and%2520Yuanhao%2520Cai%2520and%2520Yulun%2520Zhang%26entry.1292438233%3DBurst%2520denoising%2520methods%2520are%2520crucial%2520for%2520enhancing%2520images%2520captured%2520on%2520handheld%2520devices%252C%2520but%2520they%2520often%2520struggle%2520with%2520large%2520motion%2520or%2520suffer%2520from%2520prohibitive%2520computational%2520costs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DenoiseGS%252C%2520the%2520first%2520framework%2520to%2520leverage%2520the%2520efficiency%2520of%25203D%2520Gaussian%2520Splatting%2520for%2520burst%2520denoising.%2520Our%2520approach%2520addresses%2520two%2520key%2520challenges%2520when%2520applying%2520feedforward%2520Gaussian%2520reconsturction%2520model%2520to%2520noisy%2520inputs%253A%2520the%2520degradation%2520of%2520Gaussian%2520point%2520clouds%2520and%2520the%2520loss%2520of%2520fine%2520details.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520Gaussian%2520self-consistency%2520%2528GSC%2529%2520loss%252C%2520which%2520regularizes%2520the%2520geometry%2520predicted%2520from%2520noisy%2520inputs%2520with%2520high-quality%2520Gaussian%2520point%2520clouds.%2520These%2520point%2520clouds%2520are%2520generated%2520from%2520clean%2520inputs%2520by%2520the%2520same%2520model%2520that%2520we%2520are%2520training%252C%2520thereby%2520alleviating%2520potential%2520bias%2520or%2520domain%2520gaps.%2520Additionally%252C%2520we%2520introduce%2520a%2520log-weighted%2520frequency%2520%2528LWF%2529%2520loss%2520to%2520strengthen%2520supervision%2520within%2520the%2520spectral%2520domain%252C%2520effectively%2520preserving%2520fine-grained%2520details.%2520The%2520LWF%2520loss%2520adaptively%2520weights%2520frequency%2520discrepancies%2520in%2520a%2520logarithmic%2520manner%252C%2520emphasizing%2520challenging%2520high-frequency%2520details.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DenoiseGS%2520significantly%2520exceeds%2520the%2520state-of-the-art%2520NeRF-based%2520methods%2520on%2520both%2520burst%2520denoising%2520and%2520novel%2520view%2520synthesis%2520under%2520noisy%2520conditions%252C%2520while%2520achieving%2520250%2524%255Ctimes%2524%2520faster%2520inference%2520speed.%2520Code%2520and%2520models%2520are%2520released%2520at%2520https%253A//github.com/yscheng04/DenoiseGS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.22939v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DenoiseGS%3A%20Gaussian%20Reconstruction%20Model%20for%20Burst%20Denoising&entry.906535625=Yongsen%20Cheng%20and%20Yuanhao%20Cai%20and%20Yulun%20Zhang&entry.1292438233=Burst%20denoising%20methods%20are%20crucial%20for%20enhancing%20images%20captured%20on%20handheld%20devices%2C%20but%20they%20often%20struggle%20with%20large%20motion%20or%20suffer%20from%20prohibitive%20computational%20costs.%20In%20this%20paper%2C%20we%20propose%20DenoiseGS%2C%20the%20first%20framework%20to%20leverage%20the%20efficiency%20of%203D%20Gaussian%20Splatting%20for%20burst%20denoising.%20Our%20approach%20addresses%20two%20key%20challenges%20when%20applying%20feedforward%20Gaussian%20reconsturction%20model%20to%20noisy%20inputs%3A%20the%20degradation%20of%20Gaussian%20point%20clouds%20and%20the%20loss%20of%20fine%20details.%20To%20this%20end%2C%20we%20propose%20a%20Gaussian%20self-consistency%20%28GSC%29%20loss%2C%20which%20regularizes%20the%20geometry%20predicted%20from%20noisy%20inputs%20with%20high-quality%20Gaussian%20point%20clouds.%20These%20point%20clouds%20are%20generated%20from%20clean%20inputs%20by%20the%20same%20model%20that%20we%20are%20training%2C%20thereby%20alleviating%20potential%20bias%20or%20domain%20gaps.%20Additionally%2C%20we%20introduce%20a%20log-weighted%20frequency%20%28LWF%29%20loss%20to%20strengthen%20supervision%20within%20the%20spectral%20domain%2C%20effectively%20preserving%20fine-grained%20details.%20The%20LWF%20loss%20adaptively%20weights%20frequency%20discrepancies%20in%20a%20logarithmic%20manner%2C%20emphasizing%20challenging%20high-frequency%20details.%20Extensive%20experiments%20demonstrate%20that%20DenoiseGS%20significantly%20exceeds%20the%20state-of-the-art%20NeRF-based%20methods%20on%20both%20burst%20denoising%20and%20novel%20view%20synthesis%20under%20noisy%20conditions%2C%20while%20achieving%20250%24%5Ctimes%24%20faster%20inference%20speed.%20Code%20and%20models%20are%20released%20at%20https%3A//github.com/yscheng04/DenoiseGS.&entry.1838667208=http%3A//arxiv.org/abs/2511.22939v2&entry.124074799=Read"},
{"title": "SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation", "author": "Zisu Li and Hengye Lyu and Jiaxin Shi and Yufeng Zeng and Mingming Fan and Hanwang Zhang and Chen Liang", "abstract": "Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.", "link": "http://arxiv.org/abs/2512.01960v1", "date": "2025-12-01", "relevancy": 3.0743, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6458}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6112}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpriteHand%3A%20Real-Time%20Versatile%20Hand-Object%20Interaction%20with%20Autoregressive%20Video%20Generation&body=Title%3A%20SpriteHand%3A%20Real-Time%20Versatile%20Hand-Object%20Interaction%20with%20Autoregressive%20Video%20Generation%0AAuthor%3A%20Zisu%20Li%20and%20Hengye%20Lyu%20and%20Jiaxin%20Shi%20and%20Yufeng%20Zeng%20and%20Mingming%20Fan%20and%20Hanwang%20Zhang%20and%20Chen%20Liang%0AAbstract%3A%20Modeling%20and%20synthesizing%20complex%20hand-object%20interactions%20remains%20a%20significant%20challenge%2C%20even%20for%20state-of-the-art%20physics%20engines.%20Conventional%20simulation-based%20approaches%20rely%20on%20explicitly%20defined%20rigid%20object%20models%20and%20pre-scripted%20hand%20gestures%2C%20making%20them%20inadequate%20for%20capturing%20dynamic%20interactions%20with%20non-rigid%20or%20articulated%20entities%20such%20as%20deformable%20fabrics%2C%20elastic%20materials%2C%20hinge-based%20structures%2C%20furry%20surfaces%2C%20or%20even%20living%20creatures.%20In%20this%20paper%2C%20we%20present%20SpriteHand%2C%20an%20autoregressive%20video%20generation%20framework%20for%20real-time%20synthesis%20of%20versatile%20hand-object%20interaction%20videos%20across%20a%20wide%20range%20of%20object%20types%20and%20motion%20patterns.%20SpriteHand%20takes%20as%20input%20a%20static%20object%20image%20and%20a%20video%20stream%20in%20which%20the%20hands%20are%20imagined%20to%20interact%20with%20the%20virtual%20object%20embedded%20in%20a%20real-world%20scene%2C%20and%20generates%20corresponding%20hand-object%20interaction%20effects%20in%20real%20time.%20Our%20model%20employs%20a%20causal%20inference%20architecture%20for%20autoregressive%20generation%20and%20leverages%20a%20hybrid%20post-training%20approach%20to%20enhance%20visual%20realism%20and%20temporal%20coherence.%20Our%201.3B%20model%20supports%20real-time%20streaming%20generation%20at%20around%2018%20FPS%20and%20640x368%20resolution%2C%20with%20an%20approximate%20150%20ms%20latency%20on%20a%20single%20NVIDIA%20RTX%205090%20GPU%2C%20and%20more%20than%20a%20minute%20of%20continuous%20output.%20Experiments%20demonstrate%20superior%20visual%20quality%2C%20physical%20plausibility%2C%20and%20interaction%20fidelity%20compared%20to%20both%20generative%20and%20engine-based%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpriteHand%253A%2520Real-Time%2520Versatile%2520Hand-Object%2520Interaction%2520with%2520Autoregressive%2520Video%2520Generation%26entry.906535625%3DZisu%2520Li%2520and%2520Hengye%2520Lyu%2520and%2520Jiaxin%2520Shi%2520and%2520Yufeng%2520Zeng%2520and%2520Mingming%2520Fan%2520and%2520Hanwang%2520Zhang%2520and%2520Chen%2520Liang%26entry.1292438233%3DModeling%2520and%2520synthesizing%2520complex%2520hand-object%2520interactions%2520remains%2520a%2520significant%2520challenge%252C%2520even%2520for%2520state-of-the-art%2520physics%2520engines.%2520Conventional%2520simulation-based%2520approaches%2520rely%2520on%2520explicitly%2520defined%2520rigid%2520object%2520models%2520and%2520pre-scripted%2520hand%2520gestures%252C%2520making%2520them%2520inadequate%2520for%2520capturing%2520dynamic%2520interactions%2520with%2520non-rigid%2520or%2520articulated%2520entities%2520such%2520as%2520deformable%2520fabrics%252C%2520elastic%2520materials%252C%2520hinge-based%2520structures%252C%2520furry%2520surfaces%252C%2520or%2520even%2520living%2520creatures.%2520In%2520this%2520paper%252C%2520we%2520present%2520SpriteHand%252C%2520an%2520autoregressive%2520video%2520generation%2520framework%2520for%2520real-time%2520synthesis%2520of%2520versatile%2520hand-object%2520interaction%2520videos%2520across%2520a%2520wide%2520range%2520of%2520object%2520types%2520and%2520motion%2520patterns.%2520SpriteHand%2520takes%2520as%2520input%2520a%2520static%2520object%2520image%2520and%2520a%2520video%2520stream%2520in%2520which%2520the%2520hands%2520are%2520imagined%2520to%2520interact%2520with%2520the%2520virtual%2520object%2520embedded%2520in%2520a%2520real-world%2520scene%252C%2520and%2520generates%2520corresponding%2520hand-object%2520interaction%2520effects%2520in%2520real%2520time.%2520Our%2520model%2520employs%2520a%2520causal%2520inference%2520architecture%2520for%2520autoregressive%2520generation%2520and%2520leverages%2520a%2520hybrid%2520post-training%2520approach%2520to%2520enhance%2520visual%2520realism%2520and%2520temporal%2520coherence.%2520Our%25201.3B%2520model%2520supports%2520real-time%2520streaming%2520generation%2520at%2520around%252018%2520FPS%2520and%2520640x368%2520resolution%252C%2520with%2520an%2520approximate%2520150%2520ms%2520latency%2520on%2520a%2520single%2520NVIDIA%2520RTX%25205090%2520GPU%252C%2520and%2520more%2520than%2520a%2520minute%2520of%2520continuous%2520output.%2520Experiments%2520demonstrate%2520superior%2520visual%2520quality%252C%2520physical%2520plausibility%252C%2520and%2520interaction%2520fidelity%2520compared%2520to%2520both%2520generative%2520and%2520engine-based%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpriteHand%3A%20Real-Time%20Versatile%20Hand-Object%20Interaction%20with%20Autoregressive%20Video%20Generation&entry.906535625=Zisu%20Li%20and%20Hengye%20Lyu%20and%20Jiaxin%20Shi%20and%20Yufeng%20Zeng%20and%20Mingming%20Fan%20and%20Hanwang%20Zhang%20and%20Chen%20Liang&entry.1292438233=Modeling%20and%20synthesizing%20complex%20hand-object%20interactions%20remains%20a%20significant%20challenge%2C%20even%20for%20state-of-the-art%20physics%20engines.%20Conventional%20simulation-based%20approaches%20rely%20on%20explicitly%20defined%20rigid%20object%20models%20and%20pre-scripted%20hand%20gestures%2C%20making%20them%20inadequate%20for%20capturing%20dynamic%20interactions%20with%20non-rigid%20or%20articulated%20entities%20such%20as%20deformable%20fabrics%2C%20elastic%20materials%2C%20hinge-based%20structures%2C%20furry%20surfaces%2C%20or%20even%20living%20creatures.%20In%20this%20paper%2C%20we%20present%20SpriteHand%2C%20an%20autoregressive%20video%20generation%20framework%20for%20real-time%20synthesis%20of%20versatile%20hand-object%20interaction%20videos%20across%20a%20wide%20range%20of%20object%20types%20and%20motion%20patterns.%20SpriteHand%20takes%20as%20input%20a%20static%20object%20image%20and%20a%20video%20stream%20in%20which%20the%20hands%20are%20imagined%20to%20interact%20with%20the%20virtual%20object%20embedded%20in%20a%20real-world%20scene%2C%20and%20generates%20corresponding%20hand-object%20interaction%20effects%20in%20real%20time.%20Our%20model%20employs%20a%20causal%20inference%20architecture%20for%20autoregressive%20generation%20and%20leverages%20a%20hybrid%20post-training%20approach%20to%20enhance%20visual%20realism%20and%20temporal%20coherence.%20Our%201.3B%20model%20supports%20real-time%20streaming%20generation%20at%20around%2018%20FPS%20and%20640x368%20resolution%2C%20with%20an%20approximate%20150%20ms%20latency%20on%20a%20single%20NVIDIA%20RTX%205090%20GPU%2C%20and%20more%20than%20a%20minute%20of%20continuous%20output.%20Experiments%20demonstrate%20superior%20visual%20quality%2C%20physical%20plausibility%2C%20and%20interaction%20fidelity%20compared%20to%20both%20generative%20and%20engine-based%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2512.01960v1&entry.124074799=Read"},
{"title": "B2N3D: Progressive Learning from Binary to N-ary Relationships for 3D Object Grounding", "author": "Feng Xiao and Hongbin Xu and Hai Ci and Wenxiong Kang", "abstract": "Localizing 3D objects using natural language is essential for robotic scene understanding. The descriptions often involve multiple spatial relationships to distinguish similar objects, making 3D-language alignment difficult. Current methods only model relationships for pairwise objects, ignoring the global perceptual significance of n-ary combinations in multi-modal relational understanding. To address this, we propose a novel progressive relational learning framework for 3D object grounding. We extend relational learning from binary to n-ary to identify visual relations that match the referential description globally. Given the absence of specific annotations for referred objects in the training data, we design a grouped supervision loss to facilitate n-ary relational learning. In the scene graph created with n-ary relationships, we use a multi-modal network with hybrid attention mechanisms to further localize the target within the n-ary combinations. Experiments and ablation studies on the ReferIt3D and ScanRefer benchmarks demonstrate that our method outperforms the state-of-the-art, and proves the advantages of the n-ary relational perception in 3D localization.", "link": "http://arxiv.org/abs/2510.10194v2", "date": "2025-12-01", "relevancy": 3.0679, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20B2N3D%3A%20Progressive%20Learning%20from%20Binary%20to%20N-ary%20Relationships%20for%203D%20Object%20Grounding&body=Title%3A%20B2N3D%3A%20Progressive%20Learning%20from%20Binary%20to%20N-ary%20Relationships%20for%203D%20Object%20Grounding%0AAuthor%3A%20Feng%20Xiao%20and%20Hongbin%20Xu%20and%20Hai%20Ci%20and%20Wenxiong%20Kang%0AAbstract%3A%20Localizing%203D%20objects%20using%20natural%20language%20is%20essential%20for%20robotic%20scene%20understanding.%20The%20descriptions%20often%20involve%20multiple%20spatial%20relationships%20to%20distinguish%20similar%20objects%2C%20making%203D-language%20alignment%20difficult.%20Current%20methods%20only%20model%20relationships%20for%20pairwise%20objects%2C%20ignoring%20the%20global%20perceptual%20significance%20of%20n-ary%20combinations%20in%20multi-modal%20relational%20understanding.%20To%20address%20this%2C%20we%20propose%20a%20novel%20progressive%20relational%20learning%20framework%20for%203D%20object%20grounding.%20We%20extend%20relational%20learning%20from%20binary%20to%20n-ary%20to%20identify%20visual%20relations%20that%20match%20the%20referential%20description%20globally.%20Given%20the%20absence%20of%20specific%20annotations%20for%20referred%20objects%20in%20the%20training%20data%2C%20we%20design%20a%20grouped%20supervision%20loss%20to%20facilitate%20n-ary%20relational%20learning.%20In%20the%20scene%20graph%20created%20with%20n-ary%20relationships%2C%20we%20use%20a%20multi-modal%20network%20with%20hybrid%20attention%20mechanisms%20to%20further%20localize%20the%20target%20within%20the%20n-ary%20combinations.%20Experiments%20and%20ablation%20studies%20on%20the%20ReferIt3D%20and%20ScanRefer%20benchmarks%20demonstrate%20that%20our%20method%20outperforms%20the%20state-of-the-art%2C%20and%20proves%20the%20advantages%20of%20the%20n-ary%20relational%20perception%20in%203D%20localization.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10194v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DB2N3D%253A%2520Progressive%2520Learning%2520from%2520Binary%2520to%2520N-ary%2520Relationships%2520for%25203D%2520Object%2520Grounding%26entry.906535625%3DFeng%2520Xiao%2520and%2520Hongbin%2520Xu%2520and%2520Hai%2520Ci%2520and%2520Wenxiong%2520Kang%26entry.1292438233%3DLocalizing%25203D%2520objects%2520using%2520natural%2520language%2520is%2520essential%2520for%2520robotic%2520scene%2520understanding.%2520The%2520descriptions%2520often%2520involve%2520multiple%2520spatial%2520relationships%2520to%2520distinguish%2520similar%2520objects%252C%2520making%25203D-language%2520alignment%2520difficult.%2520Current%2520methods%2520only%2520model%2520relationships%2520for%2520pairwise%2520objects%252C%2520ignoring%2520the%2520global%2520perceptual%2520significance%2520of%2520n-ary%2520combinations%2520in%2520multi-modal%2520relational%2520understanding.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520progressive%2520relational%2520learning%2520framework%2520for%25203D%2520object%2520grounding.%2520We%2520extend%2520relational%2520learning%2520from%2520binary%2520to%2520n-ary%2520to%2520identify%2520visual%2520relations%2520that%2520match%2520the%2520referential%2520description%2520globally.%2520Given%2520the%2520absence%2520of%2520specific%2520annotations%2520for%2520referred%2520objects%2520in%2520the%2520training%2520data%252C%2520we%2520design%2520a%2520grouped%2520supervision%2520loss%2520to%2520facilitate%2520n-ary%2520relational%2520learning.%2520In%2520the%2520scene%2520graph%2520created%2520with%2520n-ary%2520relationships%252C%2520we%2520use%2520a%2520multi-modal%2520network%2520with%2520hybrid%2520attention%2520mechanisms%2520to%2520further%2520localize%2520the%2520target%2520within%2520the%2520n-ary%2520combinations.%2520Experiments%2520and%2520ablation%2520studies%2520on%2520the%2520ReferIt3D%2520and%2520ScanRefer%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%252C%2520and%2520proves%2520the%2520advantages%2520of%2520the%2520n-ary%2520relational%2520perception%2520in%25203D%2520localization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10194v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=B2N3D%3A%20Progressive%20Learning%20from%20Binary%20to%20N-ary%20Relationships%20for%203D%20Object%20Grounding&entry.906535625=Feng%20Xiao%20and%20Hongbin%20Xu%20and%20Hai%20Ci%20and%20Wenxiong%20Kang&entry.1292438233=Localizing%203D%20objects%20using%20natural%20language%20is%20essential%20for%20robotic%20scene%20understanding.%20The%20descriptions%20often%20involve%20multiple%20spatial%20relationships%20to%20distinguish%20similar%20objects%2C%20making%203D-language%20alignment%20difficult.%20Current%20methods%20only%20model%20relationships%20for%20pairwise%20objects%2C%20ignoring%20the%20global%20perceptual%20significance%20of%20n-ary%20combinations%20in%20multi-modal%20relational%20understanding.%20To%20address%20this%2C%20we%20propose%20a%20novel%20progressive%20relational%20learning%20framework%20for%203D%20object%20grounding.%20We%20extend%20relational%20learning%20from%20binary%20to%20n-ary%20to%20identify%20visual%20relations%20that%20match%20the%20referential%20description%20globally.%20Given%20the%20absence%20of%20specific%20annotations%20for%20referred%20objects%20in%20the%20training%20data%2C%20we%20design%20a%20grouped%20supervision%20loss%20to%20facilitate%20n-ary%20relational%20learning.%20In%20the%20scene%20graph%20created%20with%20n-ary%20relationships%2C%20we%20use%20a%20multi-modal%20network%20with%20hybrid%20attention%20mechanisms%20to%20further%20localize%20the%20target%20within%20the%20n-ary%20combinations.%20Experiments%20and%20ablation%20studies%20on%20the%20ReferIt3D%20and%20ScanRefer%20benchmarks%20demonstrate%20that%20our%20method%20outperforms%20the%20state-of-the-art%2C%20and%20proves%20the%20advantages%20of%20the%20n-ary%20relational%20perception%20in%203D%20localization.&entry.1838667208=http%3A//arxiv.org/abs/2510.10194v2&entry.124074799=Read"},
{"title": "MAMMA: Markerless & Automatic Multi-Person Motion Action Capture", "author": "Hanz Cuevas-Velasquez and Anastasios Yiannakidis and Soyong Shin and Giorgio Becherini and Markus H\u00f6schle and Joachim Tesch and Taylor Obersat and Tsvetelina Alexiadis and Eni Halilaj and Michael J. Black", "abstract": "We present MAMMA, a markerless motion-capture pipeline that accurately recovers SMPL-X parameters from multi-view video of two-person interaction sequences. Traditional motion-capture systems rely on physical markers. Although they offer high accuracy, their requirements of specialized hardware, manual marker placement, and extensive post-processing make them costly and time-consuming. Recent learning-based methods attempt to overcome these limitations, but most are designed for single-person capture, rely on sparse keypoints, or struggle with occlusions and physical interactions. In this work, we introduce a method that predicts dense 2D contact-aware surface landmarks conditioned on segmentation masks, enabling person-specific correspondence estimation even under heavy occlusion. We employ a novel architecture that exploits learnable queries for each landmark. We demonstrate that our approach can handle complex person--person interaction and offers greater accuracy than existing methods. To train our network, we construct a large, synthetic multi-view dataset combining human motions from diverse sources, including extreme poses, hand motions, and close interactions. Our dataset yields high-variability synthetic sequences with rich body contact and occlusion, and includes SMPL-X ground-truth annotations with dense 2D landmarks. The result is a system capable of capturing human motion without the need for markers. Our approach offers competitive reconstruction quality compared to commercial marker-based motion-capture solutions, without the extensive manual cleanup. Finally, we address the absence of common benchmarks for dense-landmark prediction and markerless motion capture by introducing two evaluation settings built from real multi-view sequences. We will release our dataset, benchmark, method, training code, and pre-trained model weights for research purposes.", "link": "http://arxiv.org/abs/2506.13040v3", "date": "2025-12-01", "relevancy": 3.0465, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.662}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5831}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAMMA%3A%20Markerless%20%26%20Automatic%20Multi-Person%20Motion%20Action%20Capture&body=Title%3A%20MAMMA%3A%20Markerless%20%26%20Automatic%20Multi-Person%20Motion%20Action%20Capture%0AAuthor%3A%20Hanz%20Cuevas-Velasquez%20and%20Anastasios%20Yiannakidis%20and%20Soyong%20Shin%20and%20Giorgio%20Becherini%20and%20Markus%20H%C3%B6schle%20and%20Joachim%20Tesch%20and%20Taylor%20Obersat%20and%20Tsvetelina%20Alexiadis%20and%20Eni%20Halilaj%20and%20Michael%20J.%20Black%0AAbstract%3A%20We%20present%20MAMMA%2C%20a%20markerless%20motion-capture%20pipeline%20that%20accurately%20recovers%20SMPL-X%20parameters%20from%20multi-view%20video%20of%20two-person%20interaction%20sequences.%20Traditional%20motion-capture%20systems%20rely%20on%20physical%20markers.%20Although%20they%20offer%20high%20accuracy%2C%20their%20requirements%20of%20specialized%20hardware%2C%20manual%20marker%20placement%2C%20and%20extensive%20post-processing%20make%20them%20costly%20and%20time-consuming.%20Recent%20learning-based%20methods%20attempt%20to%20overcome%20these%20limitations%2C%20but%20most%20are%20designed%20for%20single-person%20capture%2C%20rely%20on%20sparse%20keypoints%2C%20or%20struggle%20with%20occlusions%20and%20physical%20interactions.%20In%20this%20work%2C%20we%20introduce%20a%20method%20that%20predicts%20dense%202D%20contact-aware%20surface%20landmarks%20conditioned%20on%20segmentation%20masks%2C%20enabling%20person-specific%20correspondence%20estimation%20even%20under%20heavy%20occlusion.%20We%20employ%20a%20novel%20architecture%20that%20exploits%20learnable%20queries%20for%20each%20landmark.%20We%20demonstrate%20that%20our%20approach%20can%20handle%20complex%20person--person%20interaction%20and%20offers%20greater%20accuracy%20than%20existing%20methods.%20To%20train%20our%20network%2C%20we%20construct%20a%20large%2C%20synthetic%20multi-view%20dataset%20combining%20human%20motions%20from%20diverse%20sources%2C%20including%20extreme%20poses%2C%20hand%20motions%2C%20and%20close%20interactions.%20Our%20dataset%20yields%20high-variability%20synthetic%20sequences%20with%20rich%20body%20contact%20and%20occlusion%2C%20and%20includes%20SMPL-X%20ground-truth%20annotations%20with%20dense%202D%20landmarks.%20The%20result%20is%20a%20system%20capable%20of%20capturing%20human%20motion%20without%20the%20need%20for%20markers.%20Our%20approach%20offers%20competitive%20reconstruction%20quality%20compared%20to%20commercial%20marker-based%20motion-capture%20solutions%2C%20without%20the%20extensive%20manual%20cleanup.%20Finally%2C%20we%20address%20the%20absence%20of%20common%20benchmarks%20for%20dense-landmark%20prediction%20and%20markerless%20motion%20capture%20by%20introducing%20two%20evaluation%20settings%20built%20from%20real%20multi-view%20sequences.%20We%20will%20release%20our%20dataset%2C%20benchmark%2C%20method%2C%20training%20code%2C%20and%20pre-trained%20model%20weights%20for%20research%20purposes.%0ALink%3A%20http%3A//arxiv.org/abs/2506.13040v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAMMA%253A%2520Markerless%2520%2526%2520Automatic%2520Multi-Person%2520Motion%2520Action%2520Capture%26entry.906535625%3DHanz%2520Cuevas-Velasquez%2520and%2520Anastasios%2520Yiannakidis%2520and%2520Soyong%2520Shin%2520and%2520Giorgio%2520Becherini%2520and%2520Markus%2520H%25C3%25B6schle%2520and%2520Joachim%2520Tesch%2520and%2520Taylor%2520Obersat%2520and%2520Tsvetelina%2520Alexiadis%2520and%2520Eni%2520Halilaj%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3DWe%2520present%2520MAMMA%252C%2520a%2520markerless%2520motion-capture%2520pipeline%2520that%2520accurately%2520recovers%2520SMPL-X%2520parameters%2520from%2520multi-view%2520video%2520of%2520two-person%2520interaction%2520sequences.%2520Traditional%2520motion-capture%2520systems%2520rely%2520on%2520physical%2520markers.%2520Although%2520they%2520offer%2520high%2520accuracy%252C%2520their%2520requirements%2520of%2520specialized%2520hardware%252C%2520manual%2520marker%2520placement%252C%2520and%2520extensive%2520post-processing%2520make%2520them%2520costly%2520and%2520time-consuming.%2520Recent%2520learning-based%2520methods%2520attempt%2520to%2520overcome%2520these%2520limitations%252C%2520but%2520most%2520are%2520designed%2520for%2520single-person%2520capture%252C%2520rely%2520on%2520sparse%2520keypoints%252C%2520or%2520struggle%2520with%2520occlusions%2520and%2520physical%2520interactions.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520method%2520that%2520predicts%2520dense%25202D%2520contact-aware%2520surface%2520landmarks%2520conditioned%2520on%2520segmentation%2520masks%252C%2520enabling%2520person-specific%2520correspondence%2520estimation%2520even%2520under%2520heavy%2520occlusion.%2520We%2520employ%2520a%2520novel%2520architecture%2520that%2520exploits%2520learnable%2520queries%2520for%2520each%2520landmark.%2520We%2520demonstrate%2520that%2520our%2520approach%2520can%2520handle%2520complex%2520person--person%2520interaction%2520and%2520offers%2520greater%2520accuracy%2520than%2520existing%2520methods.%2520To%2520train%2520our%2520network%252C%2520we%2520construct%2520a%2520large%252C%2520synthetic%2520multi-view%2520dataset%2520combining%2520human%2520motions%2520from%2520diverse%2520sources%252C%2520including%2520extreme%2520poses%252C%2520hand%2520motions%252C%2520and%2520close%2520interactions.%2520Our%2520dataset%2520yields%2520high-variability%2520synthetic%2520sequences%2520with%2520rich%2520body%2520contact%2520and%2520occlusion%252C%2520and%2520includes%2520SMPL-X%2520ground-truth%2520annotations%2520with%2520dense%25202D%2520landmarks.%2520The%2520result%2520is%2520a%2520system%2520capable%2520of%2520capturing%2520human%2520motion%2520without%2520the%2520need%2520for%2520markers.%2520Our%2520approach%2520offers%2520competitive%2520reconstruction%2520quality%2520compared%2520to%2520commercial%2520marker-based%2520motion-capture%2520solutions%252C%2520without%2520the%2520extensive%2520manual%2520cleanup.%2520Finally%252C%2520we%2520address%2520the%2520absence%2520of%2520common%2520benchmarks%2520for%2520dense-landmark%2520prediction%2520and%2520markerless%2520motion%2520capture%2520by%2520introducing%2520two%2520evaluation%2520settings%2520built%2520from%2520real%2520multi-view%2520sequences.%2520We%2520will%2520release%2520our%2520dataset%252C%2520benchmark%252C%2520method%252C%2520training%2520code%252C%2520and%2520pre-trained%2520model%2520weights%2520for%2520research%2520purposes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13040v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAMMA%3A%20Markerless%20%26%20Automatic%20Multi-Person%20Motion%20Action%20Capture&entry.906535625=Hanz%20Cuevas-Velasquez%20and%20Anastasios%20Yiannakidis%20and%20Soyong%20Shin%20and%20Giorgio%20Becherini%20and%20Markus%20H%C3%B6schle%20and%20Joachim%20Tesch%20and%20Taylor%20Obersat%20and%20Tsvetelina%20Alexiadis%20and%20Eni%20Halilaj%20and%20Michael%20J.%20Black&entry.1292438233=We%20present%20MAMMA%2C%20a%20markerless%20motion-capture%20pipeline%20that%20accurately%20recovers%20SMPL-X%20parameters%20from%20multi-view%20video%20of%20two-person%20interaction%20sequences.%20Traditional%20motion-capture%20systems%20rely%20on%20physical%20markers.%20Although%20they%20offer%20high%20accuracy%2C%20their%20requirements%20of%20specialized%20hardware%2C%20manual%20marker%20placement%2C%20and%20extensive%20post-processing%20make%20them%20costly%20and%20time-consuming.%20Recent%20learning-based%20methods%20attempt%20to%20overcome%20these%20limitations%2C%20but%20most%20are%20designed%20for%20single-person%20capture%2C%20rely%20on%20sparse%20keypoints%2C%20or%20struggle%20with%20occlusions%20and%20physical%20interactions.%20In%20this%20work%2C%20we%20introduce%20a%20method%20that%20predicts%20dense%202D%20contact-aware%20surface%20landmarks%20conditioned%20on%20segmentation%20masks%2C%20enabling%20person-specific%20correspondence%20estimation%20even%20under%20heavy%20occlusion.%20We%20employ%20a%20novel%20architecture%20that%20exploits%20learnable%20queries%20for%20each%20landmark.%20We%20demonstrate%20that%20our%20approach%20can%20handle%20complex%20person--person%20interaction%20and%20offers%20greater%20accuracy%20than%20existing%20methods.%20To%20train%20our%20network%2C%20we%20construct%20a%20large%2C%20synthetic%20multi-view%20dataset%20combining%20human%20motions%20from%20diverse%20sources%2C%20including%20extreme%20poses%2C%20hand%20motions%2C%20and%20close%20interactions.%20Our%20dataset%20yields%20high-variability%20synthetic%20sequences%20with%20rich%20body%20contact%20and%20occlusion%2C%20and%20includes%20SMPL-X%20ground-truth%20annotations%20with%20dense%202D%20landmarks.%20The%20result%20is%20a%20system%20capable%20of%20capturing%20human%20motion%20without%20the%20need%20for%20markers.%20Our%20approach%20offers%20competitive%20reconstruction%20quality%20compared%20to%20commercial%20marker-based%20motion-capture%20solutions%2C%20without%20the%20extensive%20manual%20cleanup.%20Finally%2C%20we%20address%20the%20absence%20of%20common%20benchmarks%20for%20dense-landmark%20prediction%20and%20markerless%20motion%20capture%20by%20introducing%20two%20evaluation%20settings%20built%20from%20real%20multi-view%20sequences.%20We%20will%20release%20our%20dataset%2C%20benchmark%2C%20method%2C%20training%20code%2C%20and%20pre-trained%20model%20weights%20for%20research%20purposes.&entry.1838667208=http%3A//arxiv.org/abs/2506.13040v3&entry.124074799=Read"},
{"title": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention", "author": "Zipeng Wang and Dan Xu", "abstract": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.", "link": "http://arxiv.org/abs/2512.01540v1", "date": "2025-12-01", "relevancy": 3.0343, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6226}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6127}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlashVGGT%3A%20Efficient%20and%20Scalable%20Visual%20Geometry%20Transformers%20with%20Compressed%20Descriptor%20Attention&body=Title%3A%20FlashVGGT%3A%20Efficient%20and%20Scalable%20Visual%20Geometry%20Transformers%20with%20Compressed%20Descriptor%20Attention%0AAuthor%3A%20Zipeng%20Wang%20and%20Dan%20Xu%0AAbstract%3A%203D%20reconstruction%20from%20multi-view%20images%20is%20a%20core%20challenge%20in%20computer%20vision.%20Recently%2C%20feed-forward%20methods%20have%20emerged%20as%20efficient%20and%20robust%20alternatives%20to%20traditional%20per-scene%20optimization%20techniques.%20Among%20them%2C%20state-of-the-art%20models%20like%20the%20Visual%20Geometry%20Grounding%20Transformer%20%28VGGT%29%20leverage%20full%20self-attention%20over%20all%20image%20tokens%20to%20capture%20global%20relationships.%20However%2C%20this%20approach%20suffers%20from%20poor%20scalability%20due%20to%20the%20quadratic%20complexity%20of%20self-attention%20and%20the%20large%20number%20of%20tokens%20generated%20in%20long%20image%20sequences.%20In%20this%20work%2C%20we%20introduce%20FlashVGGT%2C%20an%20efficient%20alternative%20that%20addresses%20this%20bottleneck%20through%20a%20descriptor-based%20attention%20mechanism.%20Instead%20of%20applying%20dense%20global%20attention%20across%20all%20tokens%2C%20FlashVGGT%20compresses%20spatial%20information%20from%20each%20frame%20into%20a%20compact%20set%20of%20descriptor%20tokens.%20Global%20attention%20is%20then%20computed%20as%20cross-attention%20between%20the%20full%20set%20of%20image%20tokens%20and%20this%20smaller%20descriptor%20set%2C%20significantly%20reducing%20computational%20overhead.%20Moreover%2C%20the%20compactness%20of%20the%20descriptors%20enables%20online%20inference%20over%20long%20sequences%20via%20a%20chunk-recursive%20mechanism%20that%20reuses%20cached%20descriptors%20from%20previous%20chunks.%20Experimental%20results%20show%20that%20FlashVGGT%20achieves%20reconstruction%20accuracy%20competitive%20with%20VGGT%20while%20reducing%20inference%20time%20to%20just%209.3%25%20of%20VGGT%20for%201%2C000%20images%2C%20and%20scaling%20efficiently%20to%20sequences%20exceeding%203%2C000%20images.%20Our%20project%20page%20is%20available%20at%20https%3A//wzpscott.github.io/flashvggt_page/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlashVGGT%253A%2520Efficient%2520and%2520Scalable%2520Visual%2520Geometry%2520Transformers%2520with%2520Compressed%2520Descriptor%2520Attention%26entry.906535625%3DZipeng%2520Wang%2520and%2520Dan%2520Xu%26entry.1292438233%3D3D%2520reconstruction%2520from%2520multi-view%2520images%2520is%2520a%2520core%2520challenge%2520in%2520computer%2520vision.%2520Recently%252C%2520feed-forward%2520methods%2520have%2520emerged%2520as%2520efficient%2520and%2520robust%2520alternatives%2520to%2520traditional%2520per-scene%2520optimization%2520techniques.%2520Among%2520them%252C%2520state-of-the-art%2520models%2520like%2520the%2520Visual%2520Geometry%2520Grounding%2520Transformer%2520%2528VGGT%2529%2520leverage%2520full%2520self-attention%2520over%2520all%2520image%2520tokens%2520to%2520capture%2520global%2520relationships.%2520However%252C%2520this%2520approach%2520suffers%2520from%2520poor%2520scalability%2520due%2520to%2520the%2520quadratic%2520complexity%2520of%2520self-attention%2520and%2520the%2520large%2520number%2520of%2520tokens%2520generated%2520in%2520long%2520image%2520sequences.%2520In%2520this%2520work%252C%2520we%2520introduce%2520FlashVGGT%252C%2520an%2520efficient%2520alternative%2520that%2520addresses%2520this%2520bottleneck%2520through%2520a%2520descriptor-based%2520attention%2520mechanism.%2520Instead%2520of%2520applying%2520dense%2520global%2520attention%2520across%2520all%2520tokens%252C%2520FlashVGGT%2520compresses%2520spatial%2520information%2520from%2520each%2520frame%2520into%2520a%2520compact%2520set%2520of%2520descriptor%2520tokens.%2520Global%2520attention%2520is%2520then%2520computed%2520as%2520cross-attention%2520between%2520the%2520full%2520set%2520of%2520image%2520tokens%2520and%2520this%2520smaller%2520descriptor%2520set%252C%2520significantly%2520reducing%2520computational%2520overhead.%2520Moreover%252C%2520the%2520compactness%2520of%2520the%2520descriptors%2520enables%2520online%2520inference%2520over%2520long%2520sequences%2520via%2520a%2520chunk-recursive%2520mechanism%2520that%2520reuses%2520cached%2520descriptors%2520from%2520previous%2520chunks.%2520Experimental%2520results%2520show%2520that%2520FlashVGGT%2520achieves%2520reconstruction%2520accuracy%2520competitive%2520with%2520VGGT%2520while%2520reducing%2520inference%2520time%2520to%2520just%25209.3%2525%2520of%2520VGGT%2520for%25201%252C000%2520images%252C%2520and%2520scaling%2520efficiently%2520to%2520sequences%2520exceeding%25203%252C000%2520images.%2520Our%2520project%2520page%2520is%2520available%2520at%2520https%253A//wzpscott.github.io/flashvggt_page/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlashVGGT%3A%20Efficient%20and%20Scalable%20Visual%20Geometry%20Transformers%20with%20Compressed%20Descriptor%20Attention&entry.906535625=Zipeng%20Wang%20and%20Dan%20Xu&entry.1292438233=3D%20reconstruction%20from%20multi-view%20images%20is%20a%20core%20challenge%20in%20computer%20vision.%20Recently%2C%20feed-forward%20methods%20have%20emerged%20as%20efficient%20and%20robust%20alternatives%20to%20traditional%20per-scene%20optimization%20techniques.%20Among%20them%2C%20state-of-the-art%20models%20like%20the%20Visual%20Geometry%20Grounding%20Transformer%20%28VGGT%29%20leverage%20full%20self-attention%20over%20all%20image%20tokens%20to%20capture%20global%20relationships.%20However%2C%20this%20approach%20suffers%20from%20poor%20scalability%20due%20to%20the%20quadratic%20complexity%20of%20self-attention%20and%20the%20large%20number%20of%20tokens%20generated%20in%20long%20image%20sequences.%20In%20this%20work%2C%20we%20introduce%20FlashVGGT%2C%20an%20efficient%20alternative%20that%20addresses%20this%20bottleneck%20through%20a%20descriptor-based%20attention%20mechanism.%20Instead%20of%20applying%20dense%20global%20attention%20across%20all%20tokens%2C%20FlashVGGT%20compresses%20spatial%20information%20from%20each%20frame%20into%20a%20compact%20set%20of%20descriptor%20tokens.%20Global%20attention%20is%20then%20computed%20as%20cross-attention%20between%20the%20full%20set%20of%20image%20tokens%20and%20this%20smaller%20descriptor%20set%2C%20significantly%20reducing%20computational%20overhead.%20Moreover%2C%20the%20compactness%20of%20the%20descriptors%20enables%20online%20inference%20over%20long%20sequences%20via%20a%20chunk-recursive%20mechanism%20that%20reuses%20cached%20descriptors%20from%20previous%20chunks.%20Experimental%20results%20show%20that%20FlashVGGT%20achieves%20reconstruction%20accuracy%20competitive%20with%20VGGT%20while%20reducing%20inference%20time%20to%20just%209.3%25%20of%20VGGT%20for%201%2C000%20images%2C%20and%20scaling%20efficiently%20to%20sequences%20exceeding%203%2C000%20images.%20Our%20project%20page%20is%20available%20at%20https%3A//wzpscott.github.io/flashvggt_page/.&entry.1838667208=http%3A//arxiv.org/abs/2512.01540v1&entry.124074799=Read"},
{"title": "PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models", "author": "Zeqing Wang and Keze Wang and Lei Zhang", "abstract": "Driven by the growing capacity and training scale, Text-to-Video (T2V) generation models have recently achieved substantial progress in video quality, length, and instruction-following capability. However, whether these models can understand physics and generate physically plausible videos remains a question. While Vision-Language Models (VLMs) have been widely used as general-purpose evaluators in various applications, they struggle to identify the physically impossible content from generated videos. To investigate this issue, we construct a \\textbf{PID} (\\textbf{P}hysical \\textbf{I}mplausibility \\textbf{D}etection) dataset, which consists of a \\textit{test split} of 500 manually annotated videos and a \\textit{train split} of 2,588 paired videos, where each implausible video is generated by carefully rewriting the caption of its corresponding real-world video to induce T2V models producing physically implausible content. With the constructed dataset, we introduce a lightweight fine-tuning approach, enabling VLMs to not only detect physically implausible events but also generate textual explanations on the violated physical principles. Taking the fine-tuned VLM as a physical plausibility detector and explainer, namely \\textbf{PhyDetEx}, we benchmark a series of state-of-the-art T2V models to assess their adherence to physical laws. Our findings show that although recent T2V models have made notable progress toward generating physically plausible content, understanding and adhering to physical laws remains a challenging issue, especially for open-source models. Our dataset, training code, and checkpoints are available at \\href{https://github.com/Zeqing-Wang/PhyDetEx}{https://github.com/Zeqing-Wang/PhyDetEx}.", "link": "http://arxiv.org/abs/2512.01843v1", "date": "2025-12-01", "relevancy": 3.0043, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.614}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5959}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhyDetEx%3A%20Detecting%20and%20Explaining%20the%20Physical%20Plausibility%20of%20T2V%20Models&body=Title%3A%20PhyDetEx%3A%20Detecting%20and%20Explaining%20the%20Physical%20Plausibility%20of%20T2V%20Models%0AAuthor%3A%20Zeqing%20Wang%20and%20Keze%20Wang%20and%20Lei%20Zhang%0AAbstract%3A%20Driven%20by%20the%20growing%20capacity%20and%20training%20scale%2C%20Text-to-Video%20%28T2V%29%20generation%20models%20have%20recently%20achieved%20substantial%20progress%20in%20video%20quality%2C%20length%2C%20and%20instruction-following%20capability.%20However%2C%20whether%20these%20models%20can%20understand%20physics%20and%20generate%20physically%20plausible%20videos%20remains%20a%20question.%20While%20Vision-Language%20Models%20%28VLMs%29%20have%20been%20widely%20used%20as%20general-purpose%20evaluators%20in%20various%20applications%2C%20they%20struggle%20to%20identify%20the%20physically%20impossible%20content%20from%20generated%20videos.%20To%20investigate%20this%20issue%2C%20we%20construct%20a%20%5Ctextbf%7BPID%7D%20%28%5Ctextbf%7BP%7Dhysical%20%5Ctextbf%7BI%7Dmplausibility%20%5Ctextbf%7BD%7Detection%29%20dataset%2C%20which%20consists%20of%20a%20%5Ctextit%7Btest%20split%7D%20of%20500%20manually%20annotated%20videos%20and%20a%20%5Ctextit%7Btrain%20split%7D%20of%202%2C588%20paired%20videos%2C%20where%20each%20implausible%20video%20is%20generated%20by%20carefully%20rewriting%20the%20caption%20of%20its%20corresponding%20real-world%20video%20to%20induce%20T2V%20models%20producing%20physically%20implausible%20content.%20With%20the%20constructed%20dataset%2C%20we%20introduce%20a%20lightweight%20fine-tuning%20approach%2C%20enabling%20VLMs%20to%20not%20only%20detect%20physically%20implausible%20events%20but%20also%20generate%20textual%20explanations%20on%20the%20violated%20physical%20principles.%20Taking%20the%20fine-tuned%20VLM%20as%20a%20physical%20plausibility%20detector%20and%20explainer%2C%20namely%20%5Ctextbf%7BPhyDetEx%7D%2C%20we%20benchmark%20a%20series%20of%20state-of-the-art%20T2V%20models%20to%20assess%20their%20adherence%20to%20physical%20laws.%20Our%20findings%20show%20that%20although%20recent%20T2V%20models%20have%20made%20notable%20progress%20toward%20generating%20physically%20plausible%20content%2C%20understanding%20and%20adhering%20to%20physical%20laws%20remains%20a%20challenging%20issue%2C%20especially%20for%20open-source%20models.%20Our%20dataset%2C%20training%20code%2C%20and%20checkpoints%20are%20available%20at%20%5Chref%7Bhttps%3A//github.com/Zeqing-Wang/PhyDetEx%7D%7Bhttps%3A//github.com/Zeqing-Wang/PhyDetEx%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhyDetEx%253A%2520Detecting%2520and%2520Explaining%2520the%2520Physical%2520Plausibility%2520of%2520T2V%2520Models%26entry.906535625%3DZeqing%2520Wang%2520and%2520Keze%2520Wang%2520and%2520Lei%2520Zhang%26entry.1292438233%3DDriven%2520by%2520the%2520growing%2520capacity%2520and%2520training%2520scale%252C%2520Text-to-Video%2520%2528T2V%2529%2520generation%2520models%2520have%2520recently%2520achieved%2520substantial%2520progress%2520in%2520video%2520quality%252C%2520length%252C%2520and%2520instruction-following%2520capability.%2520However%252C%2520whether%2520these%2520models%2520can%2520understand%2520physics%2520and%2520generate%2520physically%2520plausible%2520videos%2520remains%2520a%2520question.%2520While%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520been%2520widely%2520used%2520as%2520general-purpose%2520evaluators%2520in%2520various%2520applications%252C%2520they%2520struggle%2520to%2520identify%2520the%2520physically%2520impossible%2520content%2520from%2520generated%2520videos.%2520To%2520investigate%2520this%2520issue%252C%2520we%2520construct%2520a%2520%255Ctextbf%257BPID%257D%2520%2528%255Ctextbf%257BP%257Dhysical%2520%255Ctextbf%257BI%257Dmplausibility%2520%255Ctextbf%257BD%257Detection%2529%2520dataset%252C%2520which%2520consists%2520of%2520a%2520%255Ctextit%257Btest%2520split%257D%2520of%2520500%2520manually%2520annotated%2520videos%2520and%2520a%2520%255Ctextit%257Btrain%2520split%257D%2520of%25202%252C588%2520paired%2520videos%252C%2520where%2520each%2520implausible%2520video%2520is%2520generated%2520by%2520carefully%2520rewriting%2520the%2520caption%2520of%2520its%2520corresponding%2520real-world%2520video%2520to%2520induce%2520T2V%2520models%2520producing%2520physically%2520implausible%2520content.%2520With%2520the%2520constructed%2520dataset%252C%2520we%2520introduce%2520a%2520lightweight%2520fine-tuning%2520approach%252C%2520enabling%2520VLMs%2520to%2520not%2520only%2520detect%2520physically%2520implausible%2520events%2520but%2520also%2520generate%2520textual%2520explanations%2520on%2520the%2520violated%2520physical%2520principles.%2520Taking%2520the%2520fine-tuned%2520VLM%2520as%2520a%2520physical%2520plausibility%2520detector%2520and%2520explainer%252C%2520namely%2520%255Ctextbf%257BPhyDetEx%257D%252C%2520we%2520benchmark%2520a%2520series%2520of%2520state-of-the-art%2520T2V%2520models%2520to%2520assess%2520their%2520adherence%2520to%2520physical%2520laws.%2520Our%2520findings%2520show%2520that%2520although%2520recent%2520T2V%2520models%2520have%2520made%2520notable%2520progress%2520toward%2520generating%2520physically%2520plausible%2520content%252C%2520understanding%2520and%2520adhering%2520to%2520physical%2520laws%2520remains%2520a%2520challenging%2520issue%252C%2520especially%2520for%2520open-source%2520models.%2520Our%2520dataset%252C%2520training%2520code%252C%2520and%2520checkpoints%2520are%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/Zeqing-Wang/PhyDetEx%257D%257Bhttps%253A//github.com/Zeqing-Wang/PhyDetEx%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhyDetEx%3A%20Detecting%20and%20Explaining%20the%20Physical%20Plausibility%20of%20T2V%20Models&entry.906535625=Zeqing%20Wang%20and%20Keze%20Wang%20and%20Lei%20Zhang&entry.1292438233=Driven%20by%20the%20growing%20capacity%20and%20training%20scale%2C%20Text-to-Video%20%28T2V%29%20generation%20models%20have%20recently%20achieved%20substantial%20progress%20in%20video%20quality%2C%20length%2C%20and%20instruction-following%20capability.%20However%2C%20whether%20these%20models%20can%20understand%20physics%20and%20generate%20physically%20plausible%20videos%20remains%20a%20question.%20While%20Vision-Language%20Models%20%28VLMs%29%20have%20been%20widely%20used%20as%20general-purpose%20evaluators%20in%20various%20applications%2C%20they%20struggle%20to%20identify%20the%20physically%20impossible%20content%20from%20generated%20videos.%20To%20investigate%20this%20issue%2C%20we%20construct%20a%20%5Ctextbf%7BPID%7D%20%28%5Ctextbf%7BP%7Dhysical%20%5Ctextbf%7BI%7Dmplausibility%20%5Ctextbf%7BD%7Detection%29%20dataset%2C%20which%20consists%20of%20a%20%5Ctextit%7Btest%20split%7D%20of%20500%20manually%20annotated%20videos%20and%20a%20%5Ctextit%7Btrain%20split%7D%20of%202%2C588%20paired%20videos%2C%20where%20each%20implausible%20video%20is%20generated%20by%20carefully%20rewriting%20the%20caption%20of%20its%20corresponding%20real-world%20video%20to%20induce%20T2V%20models%20producing%20physically%20implausible%20content.%20With%20the%20constructed%20dataset%2C%20we%20introduce%20a%20lightweight%20fine-tuning%20approach%2C%20enabling%20VLMs%20to%20not%20only%20detect%20physically%20implausible%20events%20but%20also%20generate%20textual%20explanations%20on%20the%20violated%20physical%20principles.%20Taking%20the%20fine-tuned%20VLM%20as%20a%20physical%20plausibility%20detector%20and%20explainer%2C%20namely%20%5Ctextbf%7BPhyDetEx%7D%2C%20we%20benchmark%20a%20series%20of%20state-of-the-art%20T2V%20models%20to%20assess%20their%20adherence%20to%20physical%20laws.%20Our%20findings%20show%20that%20although%20recent%20T2V%20models%20have%20made%20notable%20progress%20toward%20generating%20physically%20plausible%20content%2C%20understanding%20and%20adhering%20to%20physical%20laws%20remains%20a%20challenging%20issue%2C%20especially%20for%20open-source%20models.%20Our%20dataset%2C%20training%20code%2C%20and%20checkpoints%20are%20available%20at%20%5Chref%7Bhttps%3A//github.com/Zeqing-Wang/PhyDetEx%7D%7Bhttps%3A//github.com/Zeqing-Wang/PhyDetEx%7D.&entry.1838667208=http%3A//arxiv.org/abs/2512.01843v1&entry.124074799=Read"},
{"title": "VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis", "author": "Hafsa Billah", "abstract": "Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain.\n  This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.", "link": "http://arxiv.org/abs/2512.01769v1", "date": "2025-12-01", "relevancy": 2.9585, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6017}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6017}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoScoop%3A%20A%20Non-Traditional%20Domain-Independent%20Framework%20For%20Video%20Analysis&body=Title%3A%20VideoScoop%3A%20A%20Non-Traditional%20Domain-Independent%20Framework%20For%20Video%20Analysis%0AAuthor%3A%20Hafsa%20Billah%0AAbstract%3A%20Automatically%20understanding%20video%20contents%20is%20important%20for%20several%20applications%20in%20Civic%20Monitoring%20%28CM%29%2C%20general%20Surveillance%20%28SL%29%2C%20Assisted%20Living%20%28AL%29%2C%20etc.%20Decades%20of%20Image%20and%20Video%20Analysis%20%28IVA%29%20research%20have%20advanced%20tasks%20such%20as%20content%20extraction%20%28e.g.%2C%20object%20recognition%20and%20tracking%29.%20Identifying%20meaningful%20activities%20or%20situations%20%28e.g.%2C%20two%20objects%20coming%20closer%29%20remains%20difficult%20and%20cannot%20be%20achieved%20by%20content%20extraction%20alone.%20Currently%2C%20Video%20Situation%20Analysis%20%28VSA%29%20is%20done%20manually%20with%20a%20human%20in%20the%20loop%2C%20which%20is%20error-prone%20and%20labor-intensive%2C%20or%20through%20custom%20algorithms%20designed%20for%20specific%20video%20types%20or%20situations.%20These%20algorithms%20are%20not%20general-purpose%20and%20require%20a%20new%20algorithm/software%20for%20each%20new%20situation%20or%20video%20from%20a%20new%20domain.%0A%20%20This%20report%20proposes%20a%20general-purpose%20VSA%20framework%20that%20overcomes%20the%20above%20limitations.%20Video%20contents%20are%20extracted%20once%20using%20state-of-the-art%20Video%20Content%20Extraction%20technologies.%20They%20are%20represented%20using%20two%20alternative%20models%20--%20the%20extended%20relational%20model%20%28R%2B%2B%29%20and%20graph%20models.%20When%20represented%20using%20R%2B%2B%2C%20the%20extracted%20contents%20can%20be%20used%20as%20data%20streams%2C%20enabling%20Continuous%20Query%20Processing%20via%20the%20proposed%20Continuous%20Query%20Language%20for%20Video%20Analysis.%20The%20graph%20models%20complement%20this%20by%20enabling%20the%20detection%20of%20situations%20that%20are%20difficult%20or%20impossible%20to%20detect%20using%20the%20relational%20model%20alone.%20Existing%20graph%20algorithms%20and%20newly%20developed%20algorithms%20support%20a%20wide%20variety%20of%20situation%20detection.%20To%20support%20domain%20independence%2C%20primitive%20situation%20variants%20across%20domains%20are%20identified%20and%20expressed%20as%20parameterized%20templates.%20Extensive%20experiments%20were%20conducted%20across%20several%20interesting%20situations%20from%20three%20domains%20--%20AL%2C%20CM%2C%20and%20SL--%20to%20evaluate%20the%20accuracy%2C%20efficiency%2C%20and%20robustness%20of%20the%20proposed%20approach%20using%20a%20dataset%20of%20videos%20of%20varying%20lengths%20from%20these%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoScoop%253A%2520A%2520Non-Traditional%2520Domain-Independent%2520Framework%2520For%2520Video%2520Analysis%26entry.906535625%3DHafsa%2520Billah%26entry.1292438233%3DAutomatically%2520understanding%2520video%2520contents%2520is%2520important%2520for%2520several%2520applications%2520in%2520Civic%2520Monitoring%2520%2528CM%2529%252C%2520general%2520Surveillance%2520%2528SL%2529%252C%2520Assisted%2520Living%2520%2528AL%2529%252C%2520etc.%2520Decades%2520of%2520Image%2520and%2520Video%2520Analysis%2520%2528IVA%2529%2520research%2520have%2520advanced%2520tasks%2520such%2520as%2520content%2520extraction%2520%2528e.g.%252C%2520object%2520recognition%2520and%2520tracking%2529.%2520Identifying%2520meaningful%2520activities%2520or%2520situations%2520%2528e.g.%252C%2520two%2520objects%2520coming%2520closer%2529%2520remains%2520difficult%2520and%2520cannot%2520be%2520achieved%2520by%2520content%2520extraction%2520alone.%2520Currently%252C%2520Video%2520Situation%2520Analysis%2520%2528VSA%2529%2520is%2520done%2520manually%2520with%2520a%2520human%2520in%2520the%2520loop%252C%2520which%2520is%2520error-prone%2520and%2520labor-intensive%252C%2520or%2520through%2520custom%2520algorithms%2520designed%2520for%2520specific%2520video%2520types%2520or%2520situations.%2520These%2520algorithms%2520are%2520not%2520general-purpose%2520and%2520require%2520a%2520new%2520algorithm/software%2520for%2520each%2520new%2520situation%2520or%2520video%2520from%2520a%2520new%2520domain.%250A%2520%2520This%2520report%2520proposes%2520a%2520general-purpose%2520VSA%2520framework%2520that%2520overcomes%2520the%2520above%2520limitations.%2520Video%2520contents%2520are%2520extracted%2520once%2520using%2520state-of-the-art%2520Video%2520Content%2520Extraction%2520technologies.%2520They%2520are%2520represented%2520using%2520two%2520alternative%2520models%2520--%2520the%2520extended%2520relational%2520model%2520%2528R%252B%252B%2529%2520and%2520graph%2520models.%2520When%2520represented%2520using%2520R%252B%252B%252C%2520the%2520extracted%2520contents%2520can%2520be%2520used%2520as%2520data%2520streams%252C%2520enabling%2520Continuous%2520Query%2520Processing%2520via%2520the%2520proposed%2520Continuous%2520Query%2520Language%2520for%2520Video%2520Analysis.%2520The%2520graph%2520models%2520complement%2520this%2520by%2520enabling%2520the%2520detection%2520of%2520situations%2520that%2520are%2520difficult%2520or%2520impossible%2520to%2520detect%2520using%2520the%2520relational%2520model%2520alone.%2520Existing%2520graph%2520algorithms%2520and%2520newly%2520developed%2520algorithms%2520support%2520a%2520wide%2520variety%2520of%2520situation%2520detection.%2520To%2520support%2520domain%2520independence%252C%2520primitive%2520situation%2520variants%2520across%2520domains%2520are%2520identified%2520and%2520expressed%2520as%2520parameterized%2520templates.%2520Extensive%2520experiments%2520were%2520conducted%2520across%2520several%2520interesting%2520situations%2520from%2520three%2520domains%2520--%2520AL%252C%2520CM%252C%2520and%2520SL--%2520to%2520evaluate%2520the%2520accuracy%252C%2520efficiency%252C%2520and%2520robustness%2520of%2520the%2520proposed%2520approach%2520using%2520a%2520dataset%2520of%2520videos%2520of%2520varying%2520lengths%2520from%2520these%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoScoop%3A%20A%20Non-Traditional%20Domain-Independent%20Framework%20For%20Video%20Analysis&entry.906535625=Hafsa%20Billah&entry.1292438233=Automatically%20understanding%20video%20contents%20is%20important%20for%20several%20applications%20in%20Civic%20Monitoring%20%28CM%29%2C%20general%20Surveillance%20%28SL%29%2C%20Assisted%20Living%20%28AL%29%2C%20etc.%20Decades%20of%20Image%20and%20Video%20Analysis%20%28IVA%29%20research%20have%20advanced%20tasks%20such%20as%20content%20extraction%20%28e.g.%2C%20object%20recognition%20and%20tracking%29.%20Identifying%20meaningful%20activities%20or%20situations%20%28e.g.%2C%20two%20objects%20coming%20closer%29%20remains%20difficult%20and%20cannot%20be%20achieved%20by%20content%20extraction%20alone.%20Currently%2C%20Video%20Situation%20Analysis%20%28VSA%29%20is%20done%20manually%20with%20a%20human%20in%20the%20loop%2C%20which%20is%20error-prone%20and%20labor-intensive%2C%20or%20through%20custom%20algorithms%20designed%20for%20specific%20video%20types%20or%20situations.%20These%20algorithms%20are%20not%20general-purpose%20and%20require%20a%20new%20algorithm/software%20for%20each%20new%20situation%20or%20video%20from%20a%20new%20domain.%0A%20%20This%20report%20proposes%20a%20general-purpose%20VSA%20framework%20that%20overcomes%20the%20above%20limitations.%20Video%20contents%20are%20extracted%20once%20using%20state-of-the-art%20Video%20Content%20Extraction%20technologies.%20They%20are%20represented%20using%20two%20alternative%20models%20--%20the%20extended%20relational%20model%20%28R%2B%2B%29%20and%20graph%20models.%20When%20represented%20using%20R%2B%2B%2C%20the%20extracted%20contents%20can%20be%20used%20as%20data%20streams%2C%20enabling%20Continuous%20Query%20Processing%20via%20the%20proposed%20Continuous%20Query%20Language%20for%20Video%20Analysis.%20The%20graph%20models%20complement%20this%20by%20enabling%20the%20detection%20of%20situations%20that%20are%20difficult%20or%20impossible%20to%20detect%20using%20the%20relational%20model%20alone.%20Existing%20graph%20algorithms%20and%20newly%20developed%20algorithms%20support%20a%20wide%20variety%20of%20situation%20detection.%20To%20support%20domain%20independence%2C%20primitive%20situation%20variants%20across%20domains%20are%20identified%20and%20expressed%20as%20parameterized%20templates.%20Extensive%20experiments%20were%20conducted%20across%20several%20interesting%20situations%20from%20three%20domains%20--%20AL%2C%20CM%2C%20and%20SL--%20to%20evaluate%20the%20accuracy%2C%20efficiency%2C%20and%20robustness%20of%20the%20proposed%20approach%20using%20a%20dataset%20of%20videos%20of%20varying%20lengths%20from%20these%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2512.01769v1&entry.124074799=Read"},
{"title": "Continuous Perception Matters: Diagnosing Temporal Integration Failures in Multimodal Models", "author": "Zeyu Wang and Zhenzhen Weng and Serena Yeung-Levy", "abstract": "Continuous perception, the ability to integrate visual observations over time in a continuous stream fashion, is essential for robust real-world understanding, yet remains largely untested in current multimodal models. We introduce CP-Bench, a minimal and fully controlled benchmark designed to isolate this capability using an extremely simple task: counting identical cubes in a synthetic scene while the camera moves and only reveals subsets of objects at any moment. Despite the simplicity of the setting, we find that state-of-the-art open-source and commercial models, including Qwen-3-VL, InternVL3, GPT-5, and Gemini-3-Pro, fail dramatically. A static-camera control variant confirms that the failure arises not from object recognition but from an inability to accumulate evidence across time. Further experiments show that neither higher sampling FPS, perception- or spatial-enhanced models, nor finetuning with additional videos leads to meaningful cross-temporal generalization. Our results reveal a fundamental limitation in modern multimodal architectures and training paradigms. CP-Bench provides a simple yet powerful diagnostic tool and establishes a clean testbed for developing models capable of genuine time-consistent visual reasoning.", "link": "http://arxiv.org/abs/2408.07867v2", "date": "2025-12-01", "relevancy": 2.928, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5959}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Perception%20Matters%3A%20Diagnosing%20Temporal%20Integration%20Failures%20in%20Multimodal%20Models&body=Title%3A%20Continuous%20Perception%20Matters%3A%20Diagnosing%20Temporal%20Integration%20Failures%20in%20Multimodal%20Models%0AAuthor%3A%20Zeyu%20Wang%20and%20Zhenzhen%20Weng%20and%20Serena%20Yeung-Levy%0AAbstract%3A%20Continuous%20perception%2C%20the%20ability%20to%20integrate%20visual%20observations%20over%20time%20in%20a%20continuous%20stream%20fashion%2C%20is%20essential%20for%20robust%20real-world%20understanding%2C%20yet%20remains%20largely%20untested%20in%20current%20multimodal%20models.%20We%20introduce%20CP-Bench%2C%20a%20minimal%20and%20fully%20controlled%20benchmark%20designed%20to%20isolate%20this%20capability%20using%20an%20extremely%20simple%20task%3A%20counting%20identical%20cubes%20in%20a%20synthetic%20scene%20while%20the%20camera%20moves%20and%20only%20reveals%20subsets%20of%20objects%20at%20any%20moment.%20Despite%20the%20simplicity%20of%20the%20setting%2C%20we%20find%20that%20state-of-the-art%20open-source%20and%20commercial%20models%2C%20including%20Qwen-3-VL%2C%20InternVL3%2C%20GPT-5%2C%20and%20Gemini-3-Pro%2C%20fail%20dramatically.%20A%20static-camera%20control%20variant%20confirms%20that%20the%20failure%20arises%20not%20from%20object%20recognition%20but%20from%20an%20inability%20to%20accumulate%20evidence%20across%20time.%20Further%20experiments%20show%20that%20neither%20higher%20sampling%20FPS%2C%20perception-%20or%20spatial-enhanced%20models%2C%20nor%20finetuning%20with%20additional%20videos%20leads%20to%20meaningful%20cross-temporal%20generalization.%20Our%20results%20reveal%20a%20fundamental%20limitation%20in%20modern%20multimodal%20architectures%20and%20training%20paradigms.%20CP-Bench%20provides%20a%20simple%20yet%20powerful%20diagnostic%20tool%20and%20establishes%20a%20clean%20testbed%20for%20developing%20models%20capable%20of%20genuine%20time-consistent%20visual%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2408.07867v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Perception%2520Matters%253A%2520Diagnosing%2520Temporal%2520Integration%2520Failures%2520in%2520Multimodal%2520Models%26entry.906535625%3DZeyu%2520Wang%2520and%2520Zhenzhen%2520Weng%2520and%2520Serena%2520Yeung-Levy%26entry.1292438233%3DContinuous%2520perception%252C%2520the%2520ability%2520to%2520integrate%2520visual%2520observations%2520over%2520time%2520in%2520a%2520continuous%2520stream%2520fashion%252C%2520is%2520essential%2520for%2520robust%2520real-world%2520understanding%252C%2520yet%2520remains%2520largely%2520untested%2520in%2520current%2520multimodal%2520models.%2520We%2520introduce%2520CP-Bench%252C%2520a%2520minimal%2520and%2520fully%2520controlled%2520benchmark%2520designed%2520to%2520isolate%2520this%2520capability%2520using%2520an%2520extremely%2520simple%2520task%253A%2520counting%2520identical%2520cubes%2520in%2520a%2520synthetic%2520scene%2520while%2520the%2520camera%2520moves%2520and%2520only%2520reveals%2520subsets%2520of%2520objects%2520at%2520any%2520moment.%2520Despite%2520the%2520simplicity%2520of%2520the%2520setting%252C%2520we%2520find%2520that%2520state-of-the-art%2520open-source%2520and%2520commercial%2520models%252C%2520including%2520Qwen-3-VL%252C%2520InternVL3%252C%2520GPT-5%252C%2520and%2520Gemini-3-Pro%252C%2520fail%2520dramatically.%2520A%2520static-camera%2520control%2520variant%2520confirms%2520that%2520the%2520failure%2520arises%2520not%2520from%2520object%2520recognition%2520but%2520from%2520an%2520inability%2520to%2520accumulate%2520evidence%2520across%2520time.%2520Further%2520experiments%2520show%2520that%2520neither%2520higher%2520sampling%2520FPS%252C%2520perception-%2520or%2520spatial-enhanced%2520models%252C%2520nor%2520finetuning%2520with%2520additional%2520videos%2520leads%2520to%2520meaningful%2520cross-temporal%2520generalization.%2520Our%2520results%2520reveal%2520a%2520fundamental%2520limitation%2520in%2520modern%2520multimodal%2520architectures%2520and%2520training%2520paradigms.%2520CP-Bench%2520provides%2520a%2520simple%2520yet%2520powerful%2520diagnostic%2520tool%2520and%2520establishes%2520a%2520clean%2520testbed%2520for%2520developing%2520models%2520capable%2520of%2520genuine%2520time-consistent%2520visual%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07867v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Perception%20Matters%3A%20Diagnosing%20Temporal%20Integration%20Failures%20in%20Multimodal%20Models&entry.906535625=Zeyu%20Wang%20and%20Zhenzhen%20Weng%20and%20Serena%20Yeung-Levy&entry.1292438233=Continuous%20perception%2C%20the%20ability%20to%20integrate%20visual%20observations%20over%20time%20in%20a%20continuous%20stream%20fashion%2C%20is%20essential%20for%20robust%20real-world%20understanding%2C%20yet%20remains%20largely%20untested%20in%20current%20multimodal%20models.%20We%20introduce%20CP-Bench%2C%20a%20minimal%20and%20fully%20controlled%20benchmark%20designed%20to%20isolate%20this%20capability%20using%20an%20extremely%20simple%20task%3A%20counting%20identical%20cubes%20in%20a%20synthetic%20scene%20while%20the%20camera%20moves%20and%20only%20reveals%20subsets%20of%20objects%20at%20any%20moment.%20Despite%20the%20simplicity%20of%20the%20setting%2C%20we%20find%20that%20state-of-the-art%20open-source%20and%20commercial%20models%2C%20including%20Qwen-3-VL%2C%20InternVL3%2C%20GPT-5%2C%20and%20Gemini-3-Pro%2C%20fail%20dramatically.%20A%20static-camera%20control%20variant%20confirms%20that%20the%20failure%20arises%20not%20from%20object%20recognition%20but%20from%20an%20inability%20to%20accumulate%20evidence%20across%20time.%20Further%20experiments%20show%20that%20neither%20higher%20sampling%20FPS%2C%20perception-%20or%20spatial-enhanced%20models%2C%20nor%20finetuning%20with%20additional%20videos%20leads%20to%20meaningful%20cross-temporal%20generalization.%20Our%20results%20reveal%20a%20fundamental%20limitation%20in%20modern%20multimodal%20architectures%20and%20training%20paradigms.%20CP-Bench%20provides%20a%20simple%20yet%20powerful%20diagnostic%20tool%20and%20establishes%20a%20clean%20testbed%20for%20developing%20models%20capable%20of%20genuine%20time-consistent%20visual%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2408.07867v2&entry.124074799=Read"},
{"title": "TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks", "author": "Johannes Kiechle and Stefan M. Fischer and Daniel M. Lang and Cosmin I. Bercea and Matthew J. Nyflot and Lina Felsner and Julia A. Schnabel and Jan C. Peeken", "abstract": "The sharp rise in medical tomography examinations has created a demand for automated systems that can reliably extract informative features for downstream tasks such as tumor characterization. Although 3D volumes contain richer information than individual slices, effective 3D classification remains difficult: volumetric data encode complex spatial dependencies, and the scarcity of large-scale 3D datasets has constrained progress toward 3D foundation models. As a result, many recent approaches rely on 2D vision foundation models trained on natural images, repurposing them as feature extractors for medical scans with surprisingly strong performance. Despite their practical success, current methods that apply 2D foundation models to 3D scans via slice-based decomposition remain fundamentally limited. Standard slicing along axial, sagittal, and coronal planes often fails to capture the true spatial extent of a structure when its orientation does not align with these canonical views. More critically, most approaches aggregate slice features independently, ignoring the underlying 3D geometry and losing spatial coherence across slices. To overcome these limitations, we propose TomoGraphView, a novel framework that integrates omnidirectional volume slicing with spherical graph-based feature aggregation. Instead of restricting the model to axial, sagittal, or coronal planes, our method samples both canonical and non-canonical cross-sections generated from uniformly distributed points on a sphere enclosing the volume. We publicly share our accessible code base at http://github.com/compai-lab/2025-MedIA-kiechle and provide a user-friendly library for omnidirectional volume slicing at https://pypi.org/project/OmniSlicer.", "link": "http://arxiv.org/abs/2511.09605v2", "date": "2025-12-01", "relevancy": 2.927, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6001}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6001}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TomoGraphView%3A%203D%20Medical%20Image%20Classification%20with%20Omnidirectional%20Slice%20Representations%20and%20Graph%20Neural%20Networks&body=Title%3A%20TomoGraphView%3A%203D%20Medical%20Image%20Classification%20with%20Omnidirectional%20Slice%20Representations%20and%20Graph%20Neural%20Networks%0AAuthor%3A%20Johannes%20Kiechle%20and%20Stefan%20M.%20Fischer%20and%20Daniel%20M.%20Lang%20and%20Cosmin%20I.%20Bercea%20and%20Matthew%20J.%20Nyflot%20and%20Lina%20Felsner%20and%20Julia%20A.%20Schnabel%20and%20Jan%20C.%20Peeken%0AAbstract%3A%20The%20sharp%20rise%20in%20medical%20tomography%20examinations%20has%20created%20a%20demand%20for%20automated%20systems%20that%20can%20reliably%20extract%20informative%20features%20for%20downstream%20tasks%20such%20as%20tumor%20characterization.%20Although%203D%20volumes%20contain%20richer%20information%20than%20individual%20slices%2C%20effective%203D%20classification%20remains%20difficult%3A%20volumetric%20data%20encode%20complex%20spatial%20dependencies%2C%20and%20the%20scarcity%20of%20large-scale%203D%20datasets%20has%20constrained%20progress%20toward%203D%20foundation%20models.%20As%20a%20result%2C%20many%20recent%20approaches%20rely%20on%202D%20vision%20foundation%20models%20trained%20on%20natural%20images%2C%20repurposing%20them%20as%20feature%20extractors%20for%20medical%20scans%20with%20surprisingly%20strong%20performance.%20Despite%20their%20practical%20success%2C%20current%20methods%20that%20apply%202D%20foundation%20models%20to%203D%20scans%20via%20slice-based%20decomposition%20remain%20fundamentally%20limited.%20Standard%20slicing%20along%20axial%2C%20sagittal%2C%20and%20coronal%20planes%20often%20fails%20to%20capture%20the%20true%20spatial%20extent%20of%20a%20structure%20when%20its%20orientation%20does%20not%20align%20with%20these%20canonical%20views.%20More%20critically%2C%20most%20approaches%20aggregate%20slice%20features%20independently%2C%20ignoring%20the%20underlying%203D%20geometry%20and%20losing%20spatial%20coherence%20across%20slices.%20To%20overcome%20these%20limitations%2C%20we%20propose%20TomoGraphView%2C%20a%20novel%20framework%20that%20integrates%20omnidirectional%20volume%20slicing%20with%20spherical%20graph-based%20feature%20aggregation.%20Instead%20of%20restricting%20the%20model%20to%20axial%2C%20sagittal%2C%20or%20coronal%20planes%2C%20our%20method%20samples%20both%20canonical%20and%20non-canonical%20cross-sections%20generated%20from%20uniformly%20distributed%20points%20on%20a%20sphere%20enclosing%20the%20volume.%20We%20publicly%20share%20our%20accessible%20code%20base%20at%20http%3A//github.com/compai-lab/2025-MedIA-kiechle%20and%20provide%20a%20user-friendly%20library%20for%20omnidirectional%20volume%20slicing%20at%20https%3A//pypi.org/project/OmniSlicer.%0ALink%3A%20http%3A//arxiv.org/abs/2511.09605v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTomoGraphView%253A%25203D%2520Medical%2520Image%2520Classification%2520with%2520Omnidirectional%2520Slice%2520Representations%2520and%2520Graph%2520Neural%2520Networks%26entry.906535625%3DJohannes%2520Kiechle%2520and%2520Stefan%2520M.%2520Fischer%2520and%2520Daniel%2520M.%2520Lang%2520and%2520Cosmin%2520I.%2520Bercea%2520and%2520Matthew%2520J.%2520Nyflot%2520and%2520Lina%2520Felsner%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Jan%2520C.%2520Peeken%26entry.1292438233%3DThe%2520sharp%2520rise%2520in%2520medical%2520tomography%2520examinations%2520has%2520created%2520a%2520demand%2520for%2520automated%2520systems%2520that%2520can%2520reliably%2520extract%2520informative%2520features%2520for%2520downstream%2520tasks%2520such%2520as%2520tumor%2520characterization.%2520Although%25203D%2520volumes%2520contain%2520richer%2520information%2520than%2520individual%2520slices%252C%2520effective%25203D%2520classification%2520remains%2520difficult%253A%2520volumetric%2520data%2520encode%2520complex%2520spatial%2520dependencies%252C%2520and%2520the%2520scarcity%2520of%2520large-scale%25203D%2520datasets%2520has%2520constrained%2520progress%2520toward%25203D%2520foundation%2520models.%2520As%2520a%2520result%252C%2520many%2520recent%2520approaches%2520rely%2520on%25202D%2520vision%2520foundation%2520models%2520trained%2520on%2520natural%2520images%252C%2520repurposing%2520them%2520as%2520feature%2520extractors%2520for%2520medical%2520scans%2520with%2520surprisingly%2520strong%2520performance.%2520Despite%2520their%2520practical%2520success%252C%2520current%2520methods%2520that%2520apply%25202D%2520foundation%2520models%2520to%25203D%2520scans%2520via%2520slice-based%2520decomposition%2520remain%2520fundamentally%2520limited.%2520Standard%2520slicing%2520along%2520axial%252C%2520sagittal%252C%2520and%2520coronal%2520planes%2520often%2520fails%2520to%2520capture%2520the%2520true%2520spatial%2520extent%2520of%2520a%2520structure%2520when%2520its%2520orientation%2520does%2520not%2520align%2520with%2520these%2520canonical%2520views.%2520More%2520critically%252C%2520most%2520approaches%2520aggregate%2520slice%2520features%2520independently%252C%2520ignoring%2520the%2520underlying%25203D%2520geometry%2520and%2520losing%2520spatial%2520coherence%2520across%2520slices.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520TomoGraphView%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520omnidirectional%2520volume%2520slicing%2520with%2520spherical%2520graph-based%2520feature%2520aggregation.%2520Instead%2520of%2520restricting%2520the%2520model%2520to%2520axial%252C%2520sagittal%252C%2520or%2520coronal%2520planes%252C%2520our%2520method%2520samples%2520both%2520canonical%2520and%2520non-canonical%2520cross-sections%2520generated%2520from%2520uniformly%2520distributed%2520points%2520on%2520a%2520sphere%2520enclosing%2520the%2520volume.%2520We%2520publicly%2520share%2520our%2520accessible%2520code%2520base%2520at%2520http%253A//github.com/compai-lab/2025-MedIA-kiechle%2520and%2520provide%2520a%2520user-friendly%2520library%2520for%2520omnidirectional%2520volume%2520slicing%2520at%2520https%253A//pypi.org/project/OmniSlicer.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.09605v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TomoGraphView%3A%203D%20Medical%20Image%20Classification%20with%20Omnidirectional%20Slice%20Representations%20and%20Graph%20Neural%20Networks&entry.906535625=Johannes%20Kiechle%20and%20Stefan%20M.%20Fischer%20and%20Daniel%20M.%20Lang%20and%20Cosmin%20I.%20Bercea%20and%20Matthew%20J.%20Nyflot%20and%20Lina%20Felsner%20and%20Julia%20A.%20Schnabel%20and%20Jan%20C.%20Peeken&entry.1292438233=The%20sharp%20rise%20in%20medical%20tomography%20examinations%20has%20created%20a%20demand%20for%20automated%20systems%20that%20can%20reliably%20extract%20informative%20features%20for%20downstream%20tasks%20such%20as%20tumor%20characterization.%20Although%203D%20volumes%20contain%20richer%20information%20than%20individual%20slices%2C%20effective%203D%20classification%20remains%20difficult%3A%20volumetric%20data%20encode%20complex%20spatial%20dependencies%2C%20and%20the%20scarcity%20of%20large-scale%203D%20datasets%20has%20constrained%20progress%20toward%203D%20foundation%20models.%20As%20a%20result%2C%20many%20recent%20approaches%20rely%20on%202D%20vision%20foundation%20models%20trained%20on%20natural%20images%2C%20repurposing%20them%20as%20feature%20extractors%20for%20medical%20scans%20with%20surprisingly%20strong%20performance.%20Despite%20their%20practical%20success%2C%20current%20methods%20that%20apply%202D%20foundation%20models%20to%203D%20scans%20via%20slice-based%20decomposition%20remain%20fundamentally%20limited.%20Standard%20slicing%20along%20axial%2C%20sagittal%2C%20and%20coronal%20planes%20often%20fails%20to%20capture%20the%20true%20spatial%20extent%20of%20a%20structure%20when%20its%20orientation%20does%20not%20align%20with%20these%20canonical%20views.%20More%20critically%2C%20most%20approaches%20aggregate%20slice%20features%20independently%2C%20ignoring%20the%20underlying%203D%20geometry%20and%20losing%20spatial%20coherence%20across%20slices.%20To%20overcome%20these%20limitations%2C%20we%20propose%20TomoGraphView%2C%20a%20novel%20framework%20that%20integrates%20omnidirectional%20volume%20slicing%20with%20spherical%20graph-based%20feature%20aggregation.%20Instead%20of%20restricting%20the%20model%20to%20axial%2C%20sagittal%2C%20or%20coronal%20planes%2C%20our%20method%20samples%20both%20canonical%20and%20non-canonical%20cross-sections%20generated%20from%20uniformly%20distributed%20points%20on%20a%20sphere%20enclosing%20the%20volume.%20We%20publicly%20share%20our%20accessible%20code%20base%20at%20http%3A//github.com/compai-lab/2025-MedIA-kiechle%20and%20provide%20a%20user-friendly%20library%20for%20omnidirectional%20volume%20slicing%20at%20https%3A//pypi.org/project/OmniSlicer.&entry.1838667208=http%3A//arxiv.org/abs/2511.09605v2&entry.124074799=Read"},
{"title": "STORM: Segment, Track, and Object Re-Localization from a Single Image", "author": "Yu Deng and Teng Cao and Hikaru Shindo and Jiahong Xue and Quentin Delfosse and Kristian Kersting", "abstract": "Accurate 6D pose estimation and tracking are fundamental capabilities for physical AI systems such as robots. However, existing approaches typically require a pre-defined 3D model of the target and rely on a manually annotated segmentation mask in the first frame, which is labor-intensive and leads to reduced performance when faced with occlusions or rapid movement. To address these limitations, we propose STORM (Segment, Track, and Object Re-localization from a single iMage), an open-source robust real-time 6D pose estimation system that requires no manual annotation. STORM employs a novel three-stage pipeline combining vision-language understanding with feature matching: contextual object descriptions guide localization, self-cross-attention mechanisms identify candidate regions, and produce precise masks and 3D models for accurate pose estimation. Another key innovation is our automatic re-registration mechanism that detects tracking failures through feature similarity monitoring and recovers from severe occlusions or rapid motion. STORM achieves state-of-the-art accuracy on challenging industrial datasets featuring multi-object occlusions, high-speed motion, and varying illumination, while operating at real-time speeds without additional training. This annotation-free approach significantly reduces deployment overhead, providing a practical solution for modern applications, such as flexible manufacturing and intelligent quality control.", "link": "http://arxiv.org/abs/2511.09771v2", "date": "2025-12-01", "relevancy": 2.8979, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5883}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5845}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STORM%3A%20Segment%2C%20Track%2C%20and%20Object%20Re-Localization%20from%20a%20Single%20Image&body=Title%3A%20STORM%3A%20Segment%2C%20Track%2C%20and%20Object%20Re-Localization%20from%20a%20Single%20Image%0AAuthor%3A%20Yu%20Deng%20and%20Teng%20Cao%20and%20Hikaru%20Shindo%20and%20Jiahong%20Xue%20and%20Quentin%20Delfosse%20and%20Kristian%20Kersting%0AAbstract%3A%20Accurate%206D%20pose%20estimation%20and%20tracking%20are%20fundamental%20capabilities%20for%20physical%20AI%20systems%20such%20as%20robots.%20However%2C%20existing%20approaches%20typically%20require%20a%20pre-defined%203D%20model%20of%20the%20target%20and%20rely%20on%20a%20manually%20annotated%20segmentation%20mask%20in%20the%20first%20frame%2C%20which%20is%20labor-intensive%20and%20leads%20to%20reduced%20performance%20when%20faced%20with%20occlusions%20or%20rapid%20movement.%20To%20address%20these%20limitations%2C%20we%20propose%20STORM%20%28Segment%2C%20Track%2C%20and%20Object%20Re-localization%20from%20a%20single%20iMage%29%2C%20an%20open-source%20robust%20real-time%206D%20pose%20estimation%20system%20that%20requires%20no%20manual%20annotation.%20STORM%20employs%20a%20novel%20three-stage%20pipeline%20combining%20vision-language%20understanding%20with%20feature%20matching%3A%20contextual%20object%20descriptions%20guide%20localization%2C%20self-cross-attention%20mechanisms%20identify%20candidate%20regions%2C%20and%20produce%20precise%20masks%20and%203D%20models%20for%20accurate%20pose%20estimation.%20Another%20key%20innovation%20is%20our%20automatic%20re-registration%20mechanism%20that%20detects%20tracking%20failures%20through%20feature%20similarity%20monitoring%20and%20recovers%20from%20severe%20occlusions%20or%20rapid%20motion.%20STORM%20achieves%20state-of-the-art%20accuracy%20on%20challenging%20industrial%20datasets%20featuring%20multi-object%20occlusions%2C%20high-speed%20motion%2C%20and%20varying%20illumination%2C%20while%20operating%20at%20real-time%20speeds%20without%20additional%20training.%20This%20annotation-free%20approach%20significantly%20reduces%20deployment%20overhead%2C%20providing%20a%20practical%20solution%20for%20modern%20applications%2C%20such%20as%20flexible%20manufacturing%20and%20intelligent%20quality%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2511.09771v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTORM%253A%2520Segment%252C%2520Track%252C%2520and%2520Object%2520Re-Localization%2520from%2520a%2520Single%2520Image%26entry.906535625%3DYu%2520Deng%2520and%2520Teng%2520Cao%2520and%2520Hikaru%2520Shindo%2520and%2520Jiahong%2520Xue%2520and%2520Quentin%2520Delfosse%2520and%2520Kristian%2520Kersting%26entry.1292438233%3DAccurate%25206D%2520pose%2520estimation%2520and%2520tracking%2520are%2520fundamental%2520capabilities%2520for%2520physical%2520AI%2520systems%2520such%2520as%2520robots.%2520However%252C%2520existing%2520approaches%2520typically%2520require%2520a%2520pre-defined%25203D%2520model%2520of%2520the%2520target%2520and%2520rely%2520on%2520a%2520manually%2520annotated%2520segmentation%2520mask%2520in%2520the%2520first%2520frame%252C%2520which%2520is%2520labor-intensive%2520and%2520leads%2520to%2520reduced%2520performance%2520when%2520faced%2520with%2520occlusions%2520or%2520rapid%2520movement.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520STORM%2520%2528Segment%252C%2520Track%252C%2520and%2520Object%2520Re-localization%2520from%2520a%2520single%2520iMage%2529%252C%2520an%2520open-source%2520robust%2520real-time%25206D%2520pose%2520estimation%2520system%2520that%2520requires%2520no%2520manual%2520annotation.%2520STORM%2520employs%2520a%2520novel%2520three-stage%2520pipeline%2520combining%2520vision-language%2520understanding%2520with%2520feature%2520matching%253A%2520contextual%2520object%2520descriptions%2520guide%2520localization%252C%2520self-cross-attention%2520mechanisms%2520identify%2520candidate%2520regions%252C%2520and%2520produce%2520precise%2520masks%2520and%25203D%2520models%2520for%2520accurate%2520pose%2520estimation.%2520Another%2520key%2520innovation%2520is%2520our%2520automatic%2520re-registration%2520mechanism%2520that%2520detects%2520tracking%2520failures%2520through%2520feature%2520similarity%2520monitoring%2520and%2520recovers%2520from%2520severe%2520occlusions%2520or%2520rapid%2520motion.%2520STORM%2520achieves%2520state-of-the-art%2520accuracy%2520on%2520challenging%2520industrial%2520datasets%2520featuring%2520multi-object%2520occlusions%252C%2520high-speed%2520motion%252C%2520and%2520varying%2520illumination%252C%2520while%2520operating%2520at%2520real-time%2520speeds%2520without%2520additional%2520training.%2520This%2520annotation-free%2520approach%2520significantly%2520reduces%2520deployment%2520overhead%252C%2520providing%2520a%2520practical%2520solution%2520for%2520modern%2520applications%252C%2520such%2520as%2520flexible%2520manufacturing%2520and%2520intelligent%2520quality%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.09771v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STORM%3A%20Segment%2C%20Track%2C%20and%20Object%20Re-Localization%20from%20a%20Single%20Image&entry.906535625=Yu%20Deng%20and%20Teng%20Cao%20and%20Hikaru%20Shindo%20and%20Jiahong%20Xue%20and%20Quentin%20Delfosse%20and%20Kristian%20Kersting&entry.1292438233=Accurate%206D%20pose%20estimation%20and%20tracking%20are%20fundamental%20capabilities%20for%20physical%20AI%20systems%20such%20as%20robots.%20However%2C%20existing%20approaches%20typically%20require%20a%20pre-defined%203D%20model%20of%20the%20target%20and%20rely%20on%20a%20manually%20annotated%20segmentation%20mask%20in%20the%20first%20frame%2C%20which%20is%20labor-intensive%20and%20leads%20to%20reduced%20performance%20when%20faced%20with%20occlusions%20or%20rapid%20movement.%20To%20address%20these%20limitations%2C%20we%20propose%20STORM%20%28Segment%2C%20Track%2C%20and%20Object%20Re-localization%20from%20a%20single%20iMage%29%2C%20an%20open-source%20robust%20real-time%206D%20pose%20estimation%20system%20that%20requires%20no%20manual%20annotation.%20STORM%20employs%20a%20novel%20three-stage%20pipeline%20combining%20vision-language%20understanding%20with%20feature%20matching%3A%20contextual%20object%20descriptions%20guide%20localization%2C%20self-cross-attention%20mechanisms%20identify%20candidate%20regions%2C%20and%20produce%20precise%20masks%20and%203D%20models%20for%20accurate%20pose%20estimation.%20Another%20key%20innovation%20is%20our%20automatic%20re-registration%20mechanism%20that%20detects%20tracking%20failures%20through%20feature%20similarity%20monitoring%20and%20recovers%20from%20severe%20occlusions%20or%20rapid%20motion.%20STORM%20achieves%20state-of-the-art%20accuracy%20on%20challenging%20industrial%20datasets%20featuring%20multi-object%20occlusions%2C%20high-speed%20motion%2C%20and%20varying%20illumination%2C%20while%20operating%20at%20real-time%20speeds%20without%20additional%20training.%20This%20annotation-free%20approach%20significantly%20reduces%20deployment%20overhead%2C%20providing%20a%20practical%20solution%20for%20modern%20applications%2C%20such%20as%20flexible%20manufacturing%20and%20intelligent%20quality%20control.&entry.1838667208=http%3A//arxiv.org/abs/2511.09771v2&entry.124074799=Read"},
{"title": "ViT$^3$: Unlocking Test-Time Training in Vision", "author": "Dongchen Han and Yining Li and Tianyu Li and Zixuan Cao and Ziming Wang and Jun Song and Yu Cheng and Bo Zheng and Gao Huang", "abstract": "Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling. TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time. This reformulation opens a rich and flexible design space while achieving linear computational complexity. However, crafting a powerful visual TTT design remains challenging: fundamental choices for the inner module and inner training lack comprehensive understanding and practical guidelines. To bridge this critical gap, in this paper, we present a systematic empirical study of TTT designs for visual sequence modeling. From a series of experiments and analyses, we distill six practical insights that establish design principles for effective visual TTT and illuminate paths for future improvement. These findings culminate in the Vision Test-Time Training (ViT$^3$) model, a pure TTT architecture that achieves linear complexity and parallelizable computation. We evaluate ViT$^3$ across diverse visual tasks, including image classification, image generation, object detection, and semantic segmentation. Results show that ViT$^3$ consistently matches or outperforms advanced linear-complexity models (e.g., Mamba and linear attention variants) and effectively narrows the gap to highly optimized vision Transformers. We hope this study and the ViT$^3$ baseline can facilitate future work on visual TTT models. Code is available at https://github.com/LeapLabTHU/ViTTT.", "link": "http://arxiv.org/abs/2512.01643v1", "date": "2025-12-01", "relevancy": 2.8928, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViT%24%5E3%24%3A%20Unlocking%20Test-Time%20Training%20in%20Vision&body=Title%3A%20ViT%24%5E3%24%3A%20Unlocking%20Test-Time%20Training%20in%20Vision%0AAuthor%3A%20Dongchen%20Han%20and%20Yining%20Li%20and%20Tianyu%20Li%20and%20Zixuan%20Cao%20and%20Ziming%20Wang%20and%20Jun%20Song%20and%20Yu%20Cheng%20and%20Bo%20Zheng%20and%20Gao%20Huang%0AAbstract%3A%20Test-Time%20Training%20%28TTT%29%20has%20recently%20emerged%20as%20a%20promising%20direction%20for%20efficient%20sequence%20modeling.%20TTT%20reformulates%20attention%20operation%20as%20an%20online%20learning%20problem%2C%20constructing%20a%20compact%20inner%20model%20from%20key-value%20pairs%20at%20test%20time.%20This%20reformulation%20opens%20a%20rich%20and%20flexible%20design%20space%20while%20achieving%20linear%20computational%20complexity.%20However%2C%20crafting%20a%20powerful%20visual%20TTT%20design%20remains%20challenging%3A%20fundamental%20choices%20for%20the%20inner%20module%20and%20inner%20training%20lack%20comprehensive%20understanding%20and%20practical%20guidelines.%20To%20bridge%20this%20critical%20gap%2C%20in%20this%20paper%2C%20we%20present%20a%20systematic%20empirical%20study%20of%20TTT%20designs%20for%20visual%20sequence%20modeling.%20From%20a%20series%20of%20experiments%20and%20analyses%2C%20we%20distill%20six%20practical%20insights%20that%20establish%20design%20principles%20for%20effective%20visual%20TTT%20and%20illuminate%20paths%20for%20future%20improvement.%20These%20findings%20culminate%20in%20the%20Vision%20Test-Time%20Training%20%28ViT%24%5E3%24%29%20model%2C%20a%20pure%20TTT%20architecture%20that%20achieves%20linear%20complexity%20and%20parallelizable%20computation.%20We%20evaluate%20ViT%24%5E3%24%20across%20diverse%20visual%20tasks%2C%20including%20image%20classification%2C%20image%20generation%2C%20object%20detection%2C%20and%20semantic%20segmentation.%20Results%20show%20that%20ViT%24%5E3%24%20consistently%20matches%20or%20outperforms%20advanced%20linear-complexity%20models%20%28e.g.%2C%20Mamba%20and%20linear%20attention%20variants%29%20and%20effectively%20narrows%20the%20gap%20to%20highly%20optimized%20vision%20Transformers.%20We%20hope%20this%20study%20and%20the%20ViT%24%5E3%24%20baseline%20can%20facilitate%20future%20work%20on%20visual%20TTT%20models.%20Code%20is%20available%20at%20https%3A//github.com/LeapLabTHU/ViTTT.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViT%2524%255E3%2524%253A%2520Unlocking%2520Test-Time%2520Training%2520in%2520Vision%26entry.906535625%3DDongchen%2520Han%2520and%2520Yining%2520Li%2520and%2520Tianyu%2520Li%2520and%2520Zixuan%2520Cao%2520and%2520Ziming%2520Wang%2520and%2520Jun%2520Song%2520and%2520Yu%2520Cheng%2520and%2520Bo%2520Zheng%2520and%2520Gao%2520Huang%26entry.1292438233%3DTest-Time%2520Training%2520%2528TTT%2529%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520direction%2520for%2520efficient%2520sequence%2520modeling.%2520TTT%2520reformulates%2520attention%2520operation%2520as%2520an%2520online%2520learning%2520problem%252C%2520constructing%2520a%2520compact%2520inner%2520model%2520from%2520key-value%2520pairs%2520at%2520test%2520time.%2520This%2520reformulation%2520opens%2520a%2520rich%2520and%2520flexible%2520design%2520space%2520while%2520achieving%2520linear%2520computational%2520complexity.%2520However%252C%2520crafting%2520a%2520powerful%2520visual%2520TTT%2520design%2520remains%2520challenging%253A%2520fundamental%2520choices%2520for%2520the%2520inner%2520module%2520and%2520inner%2520training%2520lack%2520comprehensive%2520understanding%2520and%2520practical%2520guidelines.%2520To%2520bridge%2520this%2520critical%2520gap%252C%2520in%2520this%2520paper%252C%2520we%2520present%2520a%2520systematic%2520empirical%2520study%2520of%2520TTT%2520designs%2520for%2520visual%2520sequence%2520modeling.%2520From%2520a%2520series%2520of%2520experiments%2520and%2520analyses%252C%2520we%2520distill%2520six%2520practical%2520insights%2520that%2520establish%2520design%2520principles%2520for%2520effective%2520visual%2520TTT%2520and%2520illuminate%2520paths%2520for%2520future%2520improvement.%2520These%2520findings%2520culminate%2520in%2520the%2520Vision%2520Test-Time%2520Training%2520%2528ViT%2524%255E3%2524%2529%2520model%252C%2520a%2520pure%2520TTT%2520architecture%2520that%2520achieves%2520linear%2520complexity%2520and%2520parallelizable%2520computation.%2520We%2520evaluate%2520ViT%2524%255E3%2524%2520across%2520diverse%2520visual%2520tasks%252C%2520including%2520image%2520classification%252C%2520image%2520generation%252C%2520object%2520detection%252C%2520and%2520semantic%2520segmentation.%2520Results%2520show%2520that%2520ViT%2524%255E3%2524%2520consistently%2520matches%2520or%2520outperforms%2520advanced%2520linear-complexity%2520models%2520%2528e.g.%252C%2520Mamba%2520and%2520linear%2520attention%2520variants%2529%2520and%2520effectively%2520narrows%2520the%2520gap%2520to%2520highly%2520optimized%2520vision%2520Transformers.%2520We%2520hope%2520this%2520study%2520and%2520the%2520ViT%2524%255E3%2524%2520baseline%2520can%2520facilitate%2520future%2520work%2520on%2520visual%2520TTT%2520models.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/LeapLabTHU/ViTTT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViT%24%5E3%24%3A%20Unlocking%20Test-Time%20Training%20in%20Vision&entry.906535625=Dongchen%20Han%20and%20Yining%20Li%20and%20Tianyu%20Li%20and%20Zixuan%20Cao%20and%20Ziming%20Wang%20and%20Jun%20Song%20and%20Yu%20Cheng%20and%20Bo%20Zheng%20and%20Gao%20Huang&entry.1292438233=Test-Time%20Training%20%28TTT%29%20has%20recently%20emerged%20as%20a%20promising%20direction%20for%20efficient%20sequence%20modeling.%20TTT%20reformulates%20attention%20operation%20as%20an%20online%20learning%20problem%2C%20constructing%20a%20compact%20inner%20model%20from%20key-value%20pairs%20at%20test%20time.%20This%20reformulation%20opens%20a%20rich%20and%20flexible%20design%20space%20while%20achieving%20linear%20computational%20complexity.%20However%2C%20crafting%20a%20powerful%20visual%20TTT%20design%20remains%20challenging%3A%20fundamental%20choices%20for%20the%20inner%20module%20and%20inner%20training%20lack%20comprehensive%20understanding%20and%20practical%20guidelines.%20To%20bridge%20this%20critical%20gap%2C%20in%20this%20paper%2C%20we%20present%20a%20systematic%20empirical%20study%20of%20TTT%20designs%20for%20visual%20sequence%20modeling.%20From%20a%20series%20of%20experiments%20and%20analyses%2C%20we%20distill%20six%20practical%20insights%20that%20establish%20design%20principles%20for%20effective%20visual%20TTT%20and%20illuminate%20paths%20for%20future%20improvement.%20These%20findings%20culminate%20in%20the%20Vision%20Test-Time%20Training%20%28ViT%24%5E3%24%29%20model%2C%20a%20pure%20TTT%20architecture%20that%20achieves%20linear%20complexity%20and%20parallelizable%20computation.%20We%20evaluate%20ViT%24%5E3%24%20across%20diverse%20visual%20tasks%2C%20including%20image%20classification%2C%20image%20generation%2C%20object%20detection%2C%20and%20semantic%20segmentation.%20Results%20show%20that%20ViT%24%5E3%24%20consistently%20matches%20or%20outperforms%20advanced%20linear-complexity%20models%20%28e.g.%2C%20Mamba%20and%20linear%20attention%20variants%29%20and%20effectively%20narrows%20the%20gap%20to%20highly%20optimized%20vision%20Transformers.%20We%20hope%20this%20study%20and%20the%20ViT%24%5E3%24%20baseline%20can%20facilitate%20future%20work%20on%20visual%20TTT%20models.%20Code%20is%20available%20at%20https%3A//github.com/LeapLabTHU/ViTTT.&entry.1838667208=http%3A//arxiv.org/abs/2512.01643v1&entry.124074799=Read"},
{"title": "VIVAT: Virtuous Improving VAE Training through Artifact Mitigation", "author": "Lev Novitskiy and Viacheslav Vasilev and Maria Kovaleva and Vladimir Arkhipkin and Denis Dimitrov", "abstract": "Variational Autoencoders (VAEs) remain a cornerstone of generative computer vision, yet their training is often plagued by artifacts that degrade reconstruction and generation quality. This paper introduces VIVAT, a systematic approach to mitigating common artifacts in KL-VAE training without requiring radical architectural changes. We present a detailed taxonomy of five prevalent artifacts - color shift, grid patterns, blur, corner and droplet artifacts - and analyze their root causes. Through straightforward modifications, including adjustments to loss weights, padding strategies, and the integration of Spatially Conditional Normalization, we demonstrate significant improvements in VAE performance. Our method achieves state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across multiple benchmarks and enhances text-to-image generation quality, as evidenced by superior CLIP scores. By preserving the simplicity of the KL-VAE framework while addressing its practical challenges, VIVAT offers actionable insights for researchers and practitioners aiming to optimize VAE training.", "link": "http://arxiv.org/abs/2506.07863v2", "date": "2025-12-01", "relevancy": 2.8692, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6199}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5546}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIVAT%3A%20Virtuous%20Improving%20VAE%20Training%20through%20Artifact%20Mitigation&body=Title%3A%20VIVAT%3A%20Virtuous%20Improving%20VAE%20Training%20through%20Artifact%20Mitigation%0AAuthor%3A%20Lev%20Novitskiy%20and%20Viacheslav%20Vasilev%20and%20Maria%20Kovaleva%20and%20Vladimir%20Arkhipkin%20and%20Denis%20Dimitrov%0AAbstract%3A%20Variational%20Autoencoders%20%28VAEs%29%20remain%20a%20cornerstone%20of%20generative%20computer%20vision%2C%20yet%20their%20training%20is%20often%20plagued%20by%20artifacts%20that%20degrade%20reconstruction%20and%20generation%20quality.%20This%20paper%20introduces%20VIVAT%2C%20a%20systematic%20approach%20to%20mitigating%20common%20artifacts%20in%20KL-VAE%20training%20without%20requiring%20radical%20architectural%20changes.%20We%20present%20a%20detailed%20taxonomy%20of%20five%20prevalent%20artifacts%20-%20color%20shift%2C%20grid%20patterns%2C%20blur%2C%20corner%20and%20droplet%20artifacts%20-%20and%20analyze%20their%20root%20causes.%20Through%20straightforward%20modifications%2C%20including%20adjustments%20to%20loss%20weights%2C%20padding%20strategies%2C%20and%20the%20integration%20of%20Spatially%20Conditional%20Normalization%2C%20we%20demonstrate%20significant%20improvements%20in%20VAE%20performance.%20Our%20method%20achieves%20state-of-the-art%20results%20in%20image%20reconstruction%20metrics%20%28PSNR%20and%20SSIM%29%20across%20multiple%20benchmarks%20and%20enhances%20text-to-image%20generation%20quality%2C%20as%20evidenced%20by%20superior%20CLIP%20scores.%20By%20preserving%20the%20simplicity%20of%20the%20KL-VAE%20framework%20while%20addressing%20its%20practical%20challenges%2C%20VIVAT%20offers%20actionable%20insights%20for%20researchers%20and%20practitioners%20aiming%20to%20optimize%20VAE%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2506.07863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIVAT%253A%2520Virtuous%2520Improving%2520VAE%2520Training%2520through%2520Artifact%2520Mitigation%26entry.906535625%3DLev%2520Novitskiy%2520and%2520Viacheslav%2520Vasilev%2520and%2520Maria%2520Kovaleva%2520and%2520Vladimir%2520Arkhipkin%2520and%2520Denis%2520Dimitrov%26entry.1292438233%3DVariational%2520Autoencoders%2520%2528VAEs%2529%2520remain%2520a%2520cornerstone%2520of%2520generative%2520computer%2520vision%252C%2520yet%2520their%2520training%2520is%2520often%2520plagued%2520by%2520artifacts%2520that%2520degrade%2520reconstruction%2520and%2520generation%2520quality.%2520This%2520paper%2520introduces%2520VIVAT%252C%2520a%2520systematic%2520approach%2520to%2520mitigating%2520common%2520artifacts%2520in%2520KL-VAE%2520training%2520without%2520requiring%2520radical%2520architectural%2520changes.%2520We%2520present%2520a%2520detailed%2520taxonomy%2520of%2520five%2520prevalent%2520artifacts%2520-%2520color%2520shift%252C%2520grid%2520patterns%252C%2520blur%252C%2520corner%2520and%2520droplet%2520artifacts%2520-%2520and%2520analyze%2520their%2520root%2520causes.%2520Through%2520straightforward%2520modifications%252C%2520including%2520adjustments%2520to%2520loss%2520weights%252C%2520padding%2520strategies%252C%2520and%2520the%2520integration%2520of%2520Spatially%2520Conditional%2520Normalization%252C%2520we%2520demonstrate%2520significant%2520improvements%2520in%2520VAE%2520performance.%2520Our%2520method%2520achieves%2520state-of-the-art%2520results%2520in%2520image%2520reconstruction%2520metrics%2520%2528PSNR%2520and%2520SSIM%2529%2520across%2520multiple%2520benchmarks%2520and%2520enhances%2520text-to-image%2520generation%2520quality%252C%2520as%2520evidenced%2520by%2520superior%2520CLIP%2520scores.%2520By%2520preserving%2520the%2520simplicity%2520of%2520the%2520KL-VAE%2520framework%2520while%2520addressing%2520its%2520practical%2520challenges%252C%2520VIVAT%2520offers%2520actionable%2520insights%2520for%2520researchers%2520and%2520practitioners%2520aiming%2520to%2520optimize%2520VAE%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIVAT%3A%20Virtuous%20Improving%20VAE%20Training%20through%20Artifact%20Mitigation&entry.906535625=Lev%20Novitskiy%20and%20Viacheslav%20Vasilev%20and%20Maria%20Kovaleva%20and%20Vladimir%20Arkhipkin%20and%20Denis%20Dimitrov&entry.1292438233=Variational%20Autoencoders%20%28VAEs%29%20remain%20a%20cornerstone%20of%20generative%20computer%20vision%2C%20yet%20their%20training%20is%20often%20plagued%20by%20artifacts%20that%20degrade%20reconstruction%20and%20generation%20quality.%20This%20paper%20introduces%20VIVAT%2C%20a%20systematic%20approach%20to%20mitigating%20common%20artifacts%20in%20KL-VAE%20training%20without%20requiring%20radical%20architectural%20changes.%20We%20present%20a%20detailed%20taxonomy%20of%20five%20prevalent%20artifacts%20-%20color%20shift%2C%20grid%20patterns%2C%20blur%2C%20corner%20and%20droplet%20artifacts%20-%20and%20analyze%20their%20root%20causes.%20Through%20straightforward%20modifications%2C%20including%20adjustments%20to%20loss%20weights%2C%20padding%20strategies%2C%20and%20the%20integration%20of%20Spatially%20Conditional%20Normalization%2C%20we%20demonstrate%20significant%20improvements%20in%20VAE%20performance.%20Our%20method%20achieves%20state-of-the-art%20results%20in%20image%20reconstruction%20metrics%20%28PSNR%20and%20SSIM%29%20across%20multiple%20benchmarks%20and%20enhances%20text-to-image%20generation%20quality%2C%20as%20evidenced%20by%20superior%20CLIP%20scores.%20By%20preserving%20the%20simplicity%20of%20the%20KL-VAE%20framework%20while%20addressing%20its%20practical%20challenges%2C%20VIVAT%20offers%20actionable%20insights%20for%20researchers%20and%20practitioners%20aiming%20to%20optimize%20VAE%20training.&entry.1838667208=http%3A//arxiv.org/abs/2506.07863v2&entry.124074799=Read"},
{"title": "Multi-view diffusion geometry using intertwined diffusion trajectories", "author": "Gwendal Debaussart-Joniec and Argyris Kalogeratos", "abstract": "This paper introduces a comprehensive unified framework for constructing multi-view diffusion geometries through intertwined multi-view diffusion trajectories (MDTs), a class of inhomogeneous diffusion processes that iteratively combine the random walk operators of multiple data views. Each MDT defines a trajectory-dependent diffusion operator with a clear probabilistic and geometric interpretation, capturing over time the interplay between data views. Our formulation encompasses existing multi-view diffusion models, while providing new degrees of freedom for view interaction and fusion. We establish theoretical properties under mild assumptions, including ergodicity of both the point-wise operator and the process in itself. We also derive MDT-based diffusion distances, and associated embeddings via singular value decompositions. Finally, we propose various strategies for learning MDT operators within the defined operator space, guided by internal quality measures. Beyond enabling flexible model design, MDTs also offer a neutral baseline for evaluating diffusion-based approaches through comparison with randomly selected MDTs. Experiments show the practical impact of the MDT operators in a manifold learning and data clustering context.", "link": "http://arxiv.org/abs/2512.01484v1", "date": "2025-12-01", "relevancy": 2.8546, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5876}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5876}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20diffusion%20geometry%20using%20intertwined%20diffusion%20trajectories&body=Title%3A%20Multi-view%20diffusion%20geometry%20using%20intertwined%20diffusion%20trajectories%0AAuthor%3A%20Gwendal%20Debaussart-Joniec%20and%20Argyris%20Kalogeratos%0AAbstract%3A%20This%20paper%20introduces%20a%20comprehensive%20unified%20framework%20for%20constructing%20multi-view%20diffusion%20geometries%20through%20intertwined%20multi-view%20diffusion%20trajectories%20%28MDTs%29%2C%20a%20class%20of%20inhomogeneous%20diffusion%20processes%20that%20iteratively%20combine%20the%20random%20walk%20operators%20of%20multiple%20data%20views.%20Each%20MDT%20defines%20a%20trajectory-dependent%20diffusion%20operator%20with%20a%20clear%20probabilistic%20and%20geometric%20interpretation%2C%20capturing%20over%20time%20the%20interplay%20between%20data%20views.%20Our%20formulation%20encompasses%20existing%20multi-view%20diffusion%20models%2C%20while%20providing%20new%20degrees%20of%20freedom%20for%20view%20interaction%20and%20fusion.%20We%20establish%20theoretical%20properties%20under%20mild%20assumptions%2C%20including%20ergodicity%20of%20both%20the%20point-wise%20operator%20and%20the%20process%20in%20itself.%20We%20also%20derive%20MDT-based%20diffusion%20distances%2C%20and%20associated%20embeddings%20via%20singular%20value%20decompositions.%20Finally%2C%20we%20propose%20various%20strategies%20for%20learning%20MDT%20operators%20within%20the%20defined%20operator%20space%2C%20guided%20by%20internal%20quality%20measures.%20Beyond%20enabling%20flexible%20model%20design%2C%20MDTs%20also%20offer%20a%20neutral%20baseline%20for%20evaluating%20diffusion-based%20approaches%20through%20comparison%20with%20randomly%20selected%20MDTs.%20Experiments%20show%20the%20practical%20impact%20of%20the%20MDT%20operators%20in%20a%20manifold%20learning%20and%20data%20clustering%20context.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520diffusion%2520geometry%2520using%2520intertwined%2520diffusion%2520trajectories%26entry.906535625%3DGwendal%2520Debaussart-Joniec%2520and%2520Argyris%2520Kalogeratos%26entry.1292438233%3DThis%2520paper%2520introduces%2520a%2520comprehensive%2520unified%2520framework%2520for%2520constructing%2520multi-view%2520diffusion%2520geometries%2520through%2520intertwined%2520multi-view%2520diffusion%2520trajectories%2520%2528MDTs%2529%252C%2520a%2520class%2520of%2520inhomogeneous%2520diffusion%2520processes%2520that%2520iteratively%2520combine%2520the%2520random%2520walk%2520operators%2520of%2520multiple%2520data%2520views.%2520Each%2520MDT%2520defines%2520a%2520trajectory-dependent%2520diffusion%2520operator%2520with%2520a%2520clear%2520probabilistic%2520and%2520geometric%2520interpretation%252C%2520capturing%2520over%2520time%2520the%2520interplay%2520between%2520data%2520views.%2520Our%2520formulation%2520encompasses%2520existing%2520multi-view%2520diffusion%2520models%252C%2520while%2520providing%2520new%2520degrees%2520of%2520freedom%2520for%2520view%2520interaction%2520and%2520fusion.%2520We%2520establish%2520theoretical%2520properties%2520under%2520mild%2520assumptions%252C%2520including%2520ergodicity%2520of%2520both%2520the%2520point-wise%2520operator%2520and%2520the%2520process%2520in%2520itself.%2520We%2520also%2520derive%2520MDT-based%2520diffusion%2520distances%252C%2520and%2520associated%2520embeddings%2520via%2520singular%2520value%2520decompositions.%2520Finally%252C%2520we%2520propose%2520various%2520strategies%2520for%2520learning%2520MDT%2520operators%2520within%2520the%2520defined%2520operator%2520space%252C%2520guided%2520by%2520internal%2520quality%2520measures.%2520Beyond%2520enabling%2520flexible%2520model%2520design%252C%2520MDTs%2520also%2520offer%2520a%2520neutral%2520baseline%2520for%2520evaluating%2520diffusion-based%2520approaches%2520through%2520comparison%2520with%2520randomly%2520selected%2520MDTs.%2520Experiments%2520show%2520the%2520practical%2520impact%2520of%2520the%2520MDT%2520operators%2520in%2520a%2520manifold%2520learning%2520and%2520data%2520clustering%2520context.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20diffusion%20geometry%20using%20intertwined%20diffusion%20trajectories&entry.906535625=Gwendal%20Debaussart-Joniec%20and%20Argyris%20Kalogeratos&entry.1292438233=This%20paper%20introduces%20a%20comprehensive%20unified%20framework%20for%20constructing%20multi-view%20diffusion%20geometries%20through%20intertwined%20multi-view%20diffusion%20trajectories%20%28MDTs%29%2C%20a%20class%20of%20inhomogeneous%20diffusion%20processes%20that%20iteratively%20combine%20the%20random%20walk%20operators%20of%20multiple%20data%20views.%20Each%20MDT%20defines%20a%20trajectory-dependent%20diffusion%20operator%20with%20a%20clear%20probabilistic%20and%20geometric%20interpretation%2C%20capturing%20over%20time%20the%20interplay%20between%20data%20views.%20Our%20formulation%20encompasses%20existing%20multi-view%20diffusion%20models%2C%20while%20providing%20new%20degrees%20of%20freedom%20for%20view%20interaction%20and%20fusion.%20We%20establish%20theoretical%20properties%20under%20mild%20assumptions%2C%20including%20ergodicity%20of%20both%20the%20point-wise%20operator%20and%20the%20process%20in%20itself.%20We%20also%20derive%20MDT-based%20diffusion%20distances%2C%20and%20associated%20embeddings%20via%20singular%20value%20decompositions.%20Finally%2C%20we%20propose%20various%20strategies%20for%20learning%20MDT%20operators%20within%20the%20defined%20operator%20space%2C%20guided%20by%20internal%20quality%20measures.%20Beyond%20enabling%20flexible%20model%20design%2C%20MDTs%20also%20offer%20a%20neutral%20baseline%20for%20evaluating%20diffusion-based%20approaches%20through%20comparison%20with%20randomly%20selected%20MDTs.%20Experiments%20show%20the%20practical%20impact%20of%20the%20MDT%20operators%20in%20a%20manifold%20learning%20and%20data%20clustering%20context.&entry.1838667208=http%3A//arxiv.org/abs/2512.01484v1&entry.124074799=Read"},
{"title": "DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models", "author": "Patrick Kwon and Chen Chen", "abstract": "Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126.github.io/dreamingcomics/", "link": "http://arxiv.org/abs/2512.01686v1", "date": "2025-12-01", "relevancy": 2.8524, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5745}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5739}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamingComics%3A%20A%20Story%20Visualization%20Pipeline%20via%20Subject%20and%20Layout%20Customized%20Generation%20using%20Video%20Models&body=Title%3A%20DreamingComics%3A%20A%20Story%20Visualization%20Pipeline%20via%20Subject%20and%20Layout%20Customized%20Generation%20using%20Video%20Models%0AAuthor%3A%20Patrick%20Kwon%20and%20Chen%20Chen%0AAbstract%3A%20Current%20story%20visualization%20methods%20tend%20to%20position%20subjects%20solely%20by%20text%20and%20face%20challenges%20in%20maintaining%20artistic%20consistency.%20To%20address%20these%20limitations%2C%20we%20introduce%20DreamingComics%2C%20a%20layout-aware%20story%20visualization%20framework.%20We%20build%20upon%20a%20pretrained%20video%20diffusion-transformer%20%28DiT%29%20model%2C%20leveraging%20its%20spatiotemporal%20priors%20to%20enhance%20identity%20and%20style%20consistency.%20For%20layout-based%20position%20control%2C%20we%20propose%20RegionalRoPE%2C%20a%20region-aware%20positional%20encoding%20scheme%20that%20re-indexes%20embeddings%20based%20on%20the%20target%20layout.%20Additionally%2C%20we%20introduce%20a%20masked%20condition%20loss%20to%20further%20constrain%20each%20subject%27s%20visual%20features%20to%20their%20designated%20region.%20To%20infer%20layouts%20from%20natural%20language%20scripts%2C%20we%20integrate%20an%20LLM-based%20layout%20generator%20trained%20to%20produce%20comic-style%20layouts%2C%20enabling%20flexible%20and%20controllable%20layout%20conditioning.%20We%20present%20a%20comprehensive%20evaluation%20of%20our%20approach%2C%20showing%20a%2029.2%25%20increase%20in%20character%20consistency%20and%20a%2036.2%25%20increase%20in%20style%20similarity%20compared%20to%20previous%20methods%2C%20while%20displaying%20high%20spatial%20accuracy.%20Our%20project%20page%20is%20available%20at%20https%3A//yj7082126.github.io/dreamingcomics/%0ALink%3A%20http%3A//arxiv.org/abs/2512.01686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamingComics%253A%2520A%2520Story%2520Visualization%2520Pipeline%2520via%2520Subject%2520and%2520Layout%2520Customized%2520Generation%2520using%2520Video%2520Models%26entry.906535625%3DPatrick%2520Kwon%2520and%2520Chen%2520Chen%26entry.1292438233%3DCurrent%2520story%2520visualization%2520methods%2520tend%2520to%2520position%2520subjects%2520solely%2520by%2520text%2520and%2520face%2520challenges%2520in%2520maintaining%2520artistic%2520consistency.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520DreamingComics%252C%2520a%2520layout-aware%2520story%2520visualization%2520framework.%2520We%2520build%2520upon%2520a%2520pretrained%2520video%2520diffusion-transformer%2520%2528DiT%2529%2520model%252C%2520leveraging%2520its%2520spatiotemporal%2520priors%2520to%2520enhance%2520identity%2520and%2520style%2520consistency.%2520For%2520layout-based%2520position%2520control%252C%2520we%2520propose%2520RegionalRoPE%252C%2520a%2520region-aware%2520positional%2520encoding%2520scheme%2520that%2520re-indexes%2520embeddings%2520based%2520on%2520the%2520target%2520layout.%2520Additionally%252C%2520we%2520introduce%2520a%2520masked%2520condition%2520loss%2520to%2520further%2520constrain%2520each%2520subject%2527s%2520visual%2520features%2520to%2520their%2520designated%2520region.%2520To%2520infer%2520layouts%2520from%2520natural%2520language%2520scripts%252C%2520we%2520integrate%2520an%2520LLM-based%2520layout%2520generator%2520trained%2520to%2520produce%2520comic-style%2520layouts%252C%2520enabling%2520flexible%2520and%2520controllable%2520layout%2520conditioning.%2520We%2520present%2520a%2520comprehensive%2520evaluation%2520of%2520our%2520approach%252C%2520showing%2520a%252029.2%2525%2520increase%2520in%2520character%2520consistency%2520and%2520a%252036.2%2525%2520increase%2520in%2520style%2520similarity%2520compared%2520to%2520previous%2520methods%252C%2520while%2520displaying%2520high%2520spatial%2520accuracy.%2520Our%2520project%2520page%2520is%2520available%2520at%2520https%253A//yj7082126.github.io/dreamingcomics/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamingComics%3A%20A%20Story%20Visualization%20Pipeline%20via%20Subject%20and%20Layout%20Customized%20Generation%20using%20Video%20Models&entry.906535625=Patrick%20Kwon%20and%20Chen%20Chen&entry.1292438233=Current%20story%20visualization%20methods%20tend%20to%20position%20subjects%20solely%20by%20text%20and%20face%20challenges%20in%20maintaining%20artistic%20consistency.%20To%20address%20these%20limitations%2C%20we%20introduce%20DreamingComics%2C%20a%20layout-aware%20story%20visualization%20framework.%20We%20build%20upon%20a%20pretrained%20video%20diffusion-transformer%20%28DiT%29%20model%2C%20leveraging%20its%20spatiotemporal%20priors%20to%20enhance%20identity%20and%20style%20consistency.%20For%20layout-based%20position%20control%2C%20we%20propose%20RegionalRoPE%2C%20a%20region-aware%20positional%20encoding%20scheme%20that%20re-indexes%20embeddings%20based%20on%20the%20target%20layout.%20Additionally%2C%20we%20introduce%20a%20masked%20condition%20loss%20to%20further%20constrain%20each%20subject%27s%20visual%20features%20to%20their%20designated%20region.%20To%20infer%20layouts%20from%20natural%20language%20scripts%2C%20we%20integrate%20an%20LLM-based%20layout%20generator%20trained%20to%20produce%20comic-style%20layouts%2C%20enabling%20flexible%20and%20controllable%20layout%20conditioning.%20We%20present%20a%20comprehensive%20evaluation%20of%20our%20approach%2C%20showing%20a%2029.2%25%20increase%20in%20character%20consistency%20and%20a%2036.2%25%20increase%20in%20style%20similarity%20compared%20to%20previous%20methods%2C%20while%20displaying%20high%20spatial%20accuracy.%20Our%20project%20page%20is%20available%20at%20https%3A//yj7082126.github.io/dreamingcomics/&entry.1838667208=http%3A//arxiv.org/abs/2512.01686v1&entry.124074799=Read"},
{"title": "ELVIS: Enhance Low-Light for Video Instance Segmentation in the Dark", "author": "Joanne Lin and Ruirui Lin and Yini Li and David Bull and Nantheera Anantrasirichai", "abstract": "Video instance segmentation (VIS) for low-light content remains highly challenging for both humans and machines alike, due to adverse imaging conditions including noise, blur and low-contrast. The lack of large-scale annotated datasets and the limitations of current synthetic pipelines, particularly in modeling temporal degradations, further hinder progress. Moreover, existing VIS methods are not robust to the degradations found in low-light videos and, as a result, perform poorly even when finetuned on low-light data. In this paper, we introduce \\textbf{ELVIS} (\\textbf{E}nhance \\textbf{L}ow-light for \\textbf{V}ideo \\textbf{I}nstance \\textbf{S}egmentation), a novel framework that enables effective domain adaptation of state-of-the-art VIS models to low-light scenarios. ELVIS comprises an unsupervised synthetic low-light video pipeline that models both spatial and temporal degradations, a calibration-free degradation profile synthesis network (VDP-Net) and an enhancement decoder head that disentangles degradations from content features. ELVIS improves performances by up to \\textbf{+3.7AP} on the synthetic low-light YouTube-VIS 2019 dataset. Code will be released upon acceptance.", "link": "http://arxiv.org/abs/2512.01495v1", "date": "2025-12-01", "relevancy": 2.8069, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5638}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5637}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELVIS%3A%20Enhance%20Low-Light%20for%20Video%20Instance%20Segmentation%20in%20the%20Dark&body=Title%3A%20ELVIS%3A%20Enhance%20Low-Light%20for%20Video%20Instance%20Segmentation%20in%20the%20Dark%0AAuthor%3A%20Joanne%20Lin%20and%20Ruirui%20Lin%20and%20Yini%20Li%20and%20David%20Bull%20and%20Nantheera%20Anantrasirichai%0AAbstract%3A%20Video%20instance%20segmentation%20%28VIS%29%20for%20low-light%20content%20remains%20highly%20challenging%20for%20both%20humans%20and%20machines%20alike%2C%20due%20to%20adverse%20imaging%20conditions%20including%20noise%2C%20blur%20and%20low-contrast.%20The%20lack%20of%20large-scale%20annotated%20datasets%20and%20the%20limitations%20of%20current%20synthetic%20pipelines%2C%20particularly%20in%20modeling%20temporal%20degradations%2C%20further%20hinder%20progress.%20Moreover%2C%20existing%20VIS%20methods%20are%20not%20robust%20to%20the%20degradations%20found%20in%20low-light%20videos%20and%2C%20as%20a%20result%2C%20perform%20poorly%20even%20when%20finetuned%20on%20low-light%20data.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BELVIS%7D%20%28%5Ctextbf%7BE%7Dnhance%20%5Ctextbf%7BL%7Dow-light%20for%20%5Ctextbf%7BV%7Dideo%20%5Ctextbf%7BI%7Dnstance%20%5Ctextbf%7BS%7Degmentation%29%2C%20a%20novel%20framework%20that%20enables%20effective%20domain%20adaptation%20of%20state-of-the-art%20VIS%20models%20to%20low-light%20scenarios.%20ELVIS%20comprises%20an%20unsupervised%20synthetic%20low-light%20video%20pipeline%20that%20models%20both%20spatial%20and%20temporal%20degradations%2C%20a%20calibration-free%20degradation%20profile%20synthesis%20network%20%28VDP-Net%29%20and%20an%20enhancement%20decoder%20head%20that%20disentangles%20degradations%20from%20content%20features.%20ELVIS%20improves%20performances%20by%20up%20to%20%5Ctextbf%7B%2B3.7AP%7D%20on%20the%20synthetic%20low-light%20YouTube-VIS%202019%20dataset.%20Code%20will%20be%20released%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELVIS%253A%2520Enhance%2520Low-Light%2520for%2520Video%2520Instance%2520Segmentation%2520in%2520the%2520Dark%26entry.906535625%3DJoanne%2520Lin%2520and%2520Ruirui%2520Lin%2520and%2520Yini%2520Li%2520and%2520David%2520Bull%2520and%2520Nantheera%2520Anantrasirichai%26entry.1292438233%3DVideo%2520instance%2520segmentation%2520%2528VIS%2529%2520for%2520low-light%2520content%2520remains%2520highly%2520challenging%2520for%2520both%2520humans%2520and%2520machines%2520alike%252C%2520due%2520to%2520adverse%2520imaging%2520conditions%2520including%2520noise%252C%2520blur%2520and%2520low-contrast.%2520The%2520lack%2520of%2520large-scale%2520annotated%2520datasets%2520and%2520the%2520limitations%2520of%2520current%2520synthetic%2520pipelines%252C%2520particularly%2520in%2520modeling%2520temporal%2520degradations%252C%2520further%2520hinder%2520progress.%2520Moreover%252C%2520existing%2520VIS%2520methods%2520are%2520not%2520robust%2520to%2520the%2520degradations%2520found%2520in%2520low-light%2520videos%2520and%252C%2520as%2520a%2520result%252C%2520perform%2520poorly%2520even%2520when%2520finetuned%2520on%2520low-light%2520data.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520%255Ctextbf%257BELVIS%257D%2520%2528%255Ctextbf%257BE%257Dnhance%2520%255Ctextbf%257BL%257Dow-light%2520for%2520%255Ctextbf%257BV%257Dideo%2520%255Ctextbf%257BI%257Dnstance%2520%255Ctextbf%257BS%257Degmentation%2529%252C%2520a%2520novel%2520framework%2520that%2520enables%2520effective%2520domain%2520adaptation%2520of%2520state-of-the-art%2520VIS%2520models%2520to%2520low-light%2520scenarios.%2520ELVIS%2520comprises%2520an%2520unsupervised%2520synthetic%2520low-light%2520video%2520pipeline%2520that%2520models%2520both%2520spatial%2520and%2520temporal%2520degradations%252C%2520a%2520calibration-free%2520degradation%2520profile%2520synthesis%2520network%2520%2528VDP-Net%2529%2520and%2520an%2520enhancement%2520decoder%2520head%2520that%2520disentangles%2520degradations%2520from%2520content%2520features.%2520ELVIS%2520improves%2520performances%2520by%2520up%2520to%2520%255Ctextbf%257B%252B3.7AP%257D%2520on%2520the%2520synthetic%2520low-light%2520YouTube-VIS%25202019%2520dataset.%2520Code%2520will%2520be%2520released%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELVIS%3A%20Enhance%20Low-Light%20for%20Video%20Instance%20Segmentation%20in%20the%20Dark&entry.906535625=Joanne%20Lin%20and%20Ruirui%20Lin%20and%20Yini%20Li%20and%20David%20Bull%20and%20Nantheera%20Anantrasirichai&entry.1292438233=Video%20instance%20segmentation%20%28VIS%29%20for%20low-light%20content%20remains%20highly%20challenging%20for%20both%20humans%20and%20machines%20alike%2C%20due%20to%20adverse%20imaging%20conditions%20including%20noise%2C%20blur%20and%20low-contrast.%20The%20lack%20of%20large-scale%20annotated%20datasets%20and%20the%20limitations%20of%20current%20synthetic%20pipelines%2C%20particularly%20in%20modeling%20temporal%20degradations%2C%20further%20hinder%20progress.%20Moreover%2C%20existing%20VIS%20methods%20are%20not%20robust%20to%20the%20degradations%20found%20in%20low-light%20videos%20and%2C%20as%20a%20result%2C%20perform%20poorly%20even%20when%20finetuned%20on%20low-light%20data.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BELVIS%7D%20%28%5Ctextbf%7BE%7Dnhance%20%5Ctextbf%7BL%7Dow-light%20for%20%5Ctextbf%7BV%7Dideo%20%5Ctextbf%7BI%7Dnstance%20%5Ctextbf%7BS%7Degmentation%29%2C%20a%20novel%20framework%20that%20enables%20effective%20domain%20adaptation%20of%20state-of-the-art%20VIS%20models%20to%20low-light%20scenarios.%20ELVIS%20comprises%20an%20unsupervised%20synthetic%20low-light%20video%20pipeline%20that%20models%20both%20spatial%20and%20temporal%20degradations%2C%20a%20calibration-free%20degradation%20profile%20synthesis%20network%20%28VDP-Net%29%20and%20an%20enhancement%20decoder%20head%20that%20disentangles%20degradations%20from%20content%20features.%20ELVIS%20improves%20performances%20by%20up%20to%20%5Ctextbf%7B%2B3.7AP%7D%20on%20the%20synthetic%20low-light%20YouTube-VIS%202019%20dataset.%20Code%20will%20be%20released%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2512.01495v1&entry.124074799=Read"},
{"title": "Hierarchical Semi-Supervised Active Learning for Remote Sensing", "author": "Wei Huang and Zhitong Xiong and Chenying Liu and Xiao Xiang Zhu", "abstract": "The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be publicly available.", "link": "http://arxiv.org/abs/2511.18058v2", "date": "2025-12-01", "relevancy": 2.7936, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5645}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5619}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Semi-Supervised%20Active%20Learning%20for%20Remote%20Sensing&body=Title%3A%20Hierarchical%20Semi-Supervised%20Active%20Learning%20for%20Remote%20Sensing%0AAuthor%3A%20Wei%20Huang%20and%20Zhitong%20Xiong%20and%20Chenying%20Liu%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20The%20performance%20of%20deep%20learning%20models%20in%20remote%20sensing%20%28RS%29%20strongly%20depends%20on%20the%20availability%20of%20high-quality%20labeled%20data.%20However%2C%20collecting%20large-scale%20annotations%20is%20costly%20and%20time-consuming%2C%20while%20vast%20amounts%20of%20unlabeled%20imagery%20remain%20underutilized.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20Hierarchical%20Semi-Supervised%20Active%20Learning%20%28HSSAL%29%20framework%20that%20integrates%20semi-supervised%20learning%20%28SSL%29%20and%20a%20novel%20hierarchical%20active%20learning%20%28HAL%29%20in%20a%20closed%20iterative%20loop.%20In%20each%20iteration%2C%20SSL%20refines%20the%20model%20using%20both%20labeled%20data%20through%20supervised%20learning%20and%20unlabeled%20data%20via%20weak-to-strong%20self-training%2C%20improving%20feature%20representation%20and%20uncertainty%20estimation.%20Guided%20by%20the%20refined%20representations%20and%20uncertainty%20cues%20of%20unlabeled%20samples%2C%20HAL%20then%20conducts%20sample%20querying%20through%20a%20progressive%20clustering%20strategy%2C%20selecting%20the%20most%20informative%20instances%20that%20jointly%20satisfy%20the%20criteria%20of%20scalability%2C%20diversity%2C%20and%20uncertainty.%20This%20hierarchical%20process%20ensures%20both%20efficiency%20and%20representativeness%20in%20sample%20selection.%20Extensive%20experiments%20on%20three%20benchmark%20RS%20scene%20classification%20datasets%2C%20including%20UCM%2C%20AID%2C%20and%20NWPU-RESISC45%2C%20demonstrate%20that%20HSSAL%20consistently%20outperforms%20SSL-%20or%20AL-only%20baselines.%20Remarkably%2C%20with%20only%208%25%2C%204%25%2C%20and%202%25%20labeled%20training%20data%20on%20UCM%2C%20AID%2C%20and%20NWPU-RESISC45%2C%20respectively%2C%20HSSAL%20achieves%20over%2095%25%20of%20fully-supervised%20accuracy%2C%20highlighting%20its%20superior%20label%20efficiency%20through%20informativeness%20exploitation%20of%20unlabeled%20data.%20Our%20code%20will%20be%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18058v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Semi-Supervised%2520Active%2520Learning%2520for%2520Remote%2520Sensing%26entry.906535625%3DWei%2520Huang%2520and%2520Zhitong%2520Xiong%2520and%2520Chenying%2520Liu%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3DThe%2520performance%2520of%2520deep%2520learning%2520models%2520in%2520remote%2520sensing%2520%2528RS%2529%2520strongly%2520depends%2520on%2520the%2520availability%2520of%2520high-quality%2520labeled%2520data.%2520However%252C%2520collecting%2520large-scale%2520annotations%2520is%2520costly%2520and%2520time-consuming%252C%2520while%2520vast%2520amounts%2520of%2520unlabeled%2520imagery%2520remain%2520underutilized.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520Hierarchical%2520Semi-Supervised%2520Active%2520Learning%2520%2528HSSAL%2529%2520framework%2520that%2520integrates%2520semi-supervised%2520learning%2520%2528SSL%2529%2520and%2520a%2520novel%2520hierarchical%2520active%2520learning%2520%2528HAL%2529%2520in%2520a%2520closed%2520iterative%2520loop.%2520In%2520each%2520iteration%252C%2520SSL%2520refines%2520the%2520model%2520using%2520both%2520labeled%2520data%2520through%2520supervised%2520learning%2520and%2520unlabeled%2520data%2520via%2520weak-to-strong%2520self-training%252C%2520improving%2520feature%2520representation%2520and%2520uncertainty%2520estimation.%2520Guided%2520by%2520the%2520refined%2520representations%2520and%2520uncertainty%2520cues%2520of%2520unlabeled%2520samples%252C%2520HAL%2520then%2520conducts%2520sample%2520querying%2520through%2520a%2520progressive%2520clustering%2520strategy%252C%2520selecting%2520the%2520most%2520informative%2520instances%2520that%2520jointly%2520satisfy%2520the%2520criteria%2520of%2520scalability%252C%2520diversity%252C%2520and%2520uncertainty.%2520This%2520hierarchical%2520process%2520ensures%2520both%2520efficiency%2520and%2520representativeness%2520in%2520sample%2520selection.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%2520RS%2520scene%2520classification%2520datasets%252C%2520including%2520UCM%252C%2520AID%252C%2520and%2520NWPU-RESISC45%252C%2520demonstrate%2520that%2520HSSAL%2520consistently%2520outperforms%2520SSL-%2520or%2520AL-only%2520baselines.%2520Remarkably%252C%2520with%2520only%25208%2525%252C%25204%2525%252C%2520and%25202%2525%2520labeled%2520training%2520data%2520on%2520UCM%252C%2520AID%252C%2520and%2520NWPU-RESISC45%252C%2520respectively%252C%2520HSSAL%2520achieves%2520over%252095%2525%2520of%2520fully-supervised%2520accuracy%252C%2520highlighting%2520its%2520superior%2520label%2520efficiency%2520through%2520informativeness%2520exploitation%2520of%2520unlabeled%2520data.%2520Our%2520code%2520will%2520be%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18058v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Semi-Supervised%20Active%20Learning%20for%20Remote%20Sensing&entry.906535625=Wei%20Huang%20and%20Zhitong%20Xiong%20and%20Chenying%20Liu%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=The%20performance%20of%20deep%20learning%20models%20in%20remote%20sensing%20%28RS%29%20strongly%20depends%20on%20the%20availability%20of%20high-quality%20labeled%20data.%20However%2C%20collecting%20large-scale%20annotations%20is%20costly%20and%20time-consuming%2C%20while%20vast%20amounts%20of%20unlabeled%20imagery%20remain%20underutilized.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20Hierarchical%20Semi-Supervised%20Active%20Learning%20%28HSSAL%29%20framework%20that%20integrates%20semi-supervised%20learning%20%28SSL%29%20and%20a%20novel%20hierarchical%20active%20learning%20%28HAL%29%20in%20a%20closed%20iterative%20loop.%20In%20each%20iteration%2C%20SSL%20refines%20the%20model%20using%20both%20labeled%20data%20through%20supervised%20learning%20and%20unlabeled%20data%20via%20weak-to-strong%20self-training%2C%20improving%20feature%20representation%20and%20uncertainty%20estimation.%20Guided%20by%20the%20refined%20representations%20and%20uncertainty%20cues%20of%20unlabeled%20samples%2C%20HAL%20then%20conducts%20sample%20querying%20through%20a%20progressive%20clustering%20strategy%2C%20selecting%20the%20most%20informative%20instances%20that%20jointly%20satisfy%20the%20criteria%20of%20scalability%2C%20diversity%2C%20and%20uncertainty.%20This%20hierarchical%20process%20ensures%20both%20efficiency%20and%20representativeness%20in%20sample%20selection.%20Extensive%20experiments%20on%20three%20benchmark%20RS%20scene%20classification%20datasets%2C%20including%20UCM%2C%20AID%2C%20and%20NWPU-RESISC45%2C%20demonstrate%20that%20HSSAL%20consistently%20outperforms%20SSL-%20or%20AL-only%20baselines.%20Remarkably%2C%20with%20only%208%25%2C%204%25%2C%20and%202%25%20labeled%20training%20data%20on%20UCM%2C%20AID%2C%20and%20NWPU-RESISC45%2C%20respectively%2C%20HSSAL%20achieves%20over%2095%25%20of%20fully-supervised%20accuracy%2C%20highlighting%20its%20superior%20label%20efficiency%20through%20informativeness%20exploitation%20of%20unlabeled%20data.%20Our%20code%20will%20be%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2511.18058v2&entry.124074799=Read"},
{"title": "Connecting Neural Models Latent Geometries with Relative Geodesic Representations", "author": "Hanlin Yu and Berfin Inal and Georgios Arvanitidis and Soren Hauberg and Francesco Locatello and Marco Fumero", "abstract": "Neural models learn representations of high-dimensional data on low-dimensional manifolds. Multiple factors, including stochasticities in the training process, model architectures, and additional inductive biases, may induce different representations, even when learning the same task on the same data. However, it has recently been shown that when a latent structure is shared between distinct latent spaces, relative distances between representations can be preserved, up to distortions. Building on this idea, we demonstrate that exploiting the differential-geometric structure of latent spaces of neural models, it is possible to capture precisely the transformations between representational spaces trained on similar data distributions. Specifically, we assume that distinct neural models parametrize approximately the same underlying manifold, and introduce a representation based on the pullback metric that captures the intrinsic structure of the latent space, while scaling efficiently to large models. We validate experimentally our method on model stitching and retrieval tasks, covering autoencoders and vision foundation discriminative models, across diverse architectures, datasets, and pretraining schemes.", "link": "http://arxiv.org/abs/2506.01599v2", "date": "2025-12-01", "relevancy": 2.779, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5639}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5532}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Connecting%20Neural%20Models%20Latent%20Geometries%20with%20Relative%20Geodesic%20Representations&body=Title%3A%20Connecting%20Neural%20Models%20Latent%20Geometries%20with%20Relative%20Geodesic%20Representations%0AAuthor%3A%20Hanlin%20Yu%20and%20Berfin%20Inal%20and%20Georgios%20Arvanitidis%20and%20Soren%20Hauberg%20and%20Francesco%20Locatello%20and%20Marco%20Fumero%0AAbstract%3A%20Neural%20models%20learn%20representations%20of%20high-dimensional%20data%20on%20low-dimensional%20manifolds.%20Multiple%20factors%2C%20including%20stochasticities%20in%20the%20training%20process%2C%20model%20architectures%2C%20and%20additional%20inductive%20biases%2C%20may%20induce%20different%20representations%2C%20even%20when%20learning%20the%20same%20task%20on%20the%20same%20data.%20However%2C%20it%20has%20recently%20been%20shown%20that%20when%20a%20latent%20structure%20is%20shared%20between%20distinct%20latent%20spaces%2C%20relative%20distances%20between%20representations%20can%20be%20preserved%2C%20up%20to%20distortions.%20Building%20on%20this%20idea%2C%20we%20demonstrate%20that%20exploiting%20the%20differential-geometric%20structure%20of%20latent%20spaces%20of%20neural%20models%2C%20it%20is%20possible%20to%20capture%20precisely%20the%20transformations%20between%20representational%20spaces%20trained%20on%20similar%20data%20distributions.%20Specifically%2C%20we%20assume%20that%20distinct%20neural%20models%20parametrize%20approximately%20the%20same%20underlying%20manifold%2C%20and%20introduce%20a%20representation%20based%20on%20the%20pullback%20metric%20that%20captures%20the%20intrinsic%20structure%20of%20the%20latent%20space%2C%20while%20scaling%20efficiently%20to%20large%20models.%20We%20validate%20experimentally%20our%20method%20on%20model%20stitching%20and%20retrieval%20tasks%2C%20covering%20autoencoders%20and%20vision%20foundation%20discriminative%20models%2C%20across%20diverse%20architectures%2C%20datasets%2C%20and%20pretraining%20schemes.%0ALink%3A%20http%3A//arxiv.org/abs/2506.01599v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConnecting%2520Neural%2520Models%2520Latent%2520Geometries%2520with%2520Relative%2520Geodesic%2520Representations%26entry.906535625%3DHanlin%2520Yu%2520and%2520Berfin%2520Inal%2520and%2520Georgios%2520Arvanitidis%2520and%2520Soren%2520Hauberg%2520and%2520Francesco%2520Locatello%2520and%2520Marco%2520Fumero%26entry.1292438233%3DNeural%2520models%2520learn%2520representations%2520of%2520high-dimensional%2520data%2520on%2520low-dimensional%2520manifolds.%2520Multiple%2520factors%252C%2520including%2520stochasticities%2520in%2520the%2520training%2520process%252C%2520model%2520architectures%252C%2520and%2520additional%2520inductive%2520biases%252C%2520may%2520induce%2520different%2520representations%252C%2520even%2520when%2520learning%2520the%2520same%2520task%2520on%2520the%2520same%2520data.%2520However%252C%2520it%2520has%2520recently%2520been%2520shown%2520that%2520when%2520a%2520latent%2520structure%2520is%2520shared%2520between%2520distinct%2520latent%2520spaces%252C%2520relative%2520distances%2520between%2520representations%2520can%2520be%2520preserved%252C%2520up%2520to%2520distortions.%2520Building%2520on%2520this%2520idea%252C%2520we%2520demonstrate%2520that%2520exploiting%2520the%2520differential-geometric%2520structure%2520of%2520latent%2520spaces%2520of%2520neural%2520models%252C%2520it%2520is%2520possible%2520to%2520capture%2520precisely%2520the%2520transformations%2520between%2520representational%2520spaces%2520trained%2520on%2520similar%2520data%2520distributions.%2520Specifically%252C%2520we%2520assume%2520that%2520distinct%2520neural%2520models%2520parametrize%2520approximately%2520the%2520same%2520underlying%2520manifold%252C%2520and%2520introduce%2520a%2520representation%2520based%2520on%2520the%2520pullback%2520metric%2520that%2520captures%2520the%2520intrinsic%2520structure%2520of%2520the%2520latent%2520space%252C%2520while%2520scaling%2520efficiently%2520to%2520large%2520models.%2520We%2520validate%2520experimentally%2520our%2520method%2520on%2520model%2520stitching%2520and%2520retrieval%2520tasks%252C%2520covering%2520autoencoders%2520and%2520vision%2520foundation%2520discriminative%2520models%252C%2520across%2520diverse%2520architectures%252C%2520datasets%252C%2520and%2520pretraining%2520schemes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01599v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Connecting%20Neural%20Models%20Latent%20Geometries%20with%20Relative%20Geodesic%20Representations&entry.906535625=Hanlin%20Yu%20and%20Berfin%20Inal%20and%20Georgios%20Arvanitidis%20and%20Soren%20Hauberg%20and%20Francesco%20Locatello%20and%20Marco%20Fumero&entry.1292438233=Neural%20models%20learn%20representations%20of%20high-dimensional%20data%20on%20low-dimensional%20manifolds.%20Multiple%20factors%2C%20including%20stochasticities%20in%20the%20training%20process%2C%20model%20architectures%2C%20and%20additional%20inductive%20biases%2C%20may%20induce%20different%20representations%2C%20even%20when%20learning%20the%20same%20task%20on%20the%20same%20data.%20However%2C%20it%20has%20recently%20been%20shown%20that%20when%20a%20latent%20structure%20is%20shared%20between%20distinct%20latent%20spaces%2C%20relative%20distances%20between%20representations%20can%20be%20preserved%2C%20up%20to%20distortions.%20Building%20on%20this%20idea%2C%20we%20demonstrate%20that%20exploiting%20the%20differential-geometric%20structure%20of%20latent%20spaces%20of%20neural%20models%2C%20it%20is%20possible%20to%20capture%20precisely%20the%20transformations%20between%20representational%20spaces%20trained%20on%20similar%20data%20distributions.%20Specifically%2C%20we%20assume%20that%20distinct%20neural%20models%20parametrize%20approximately%20the%20same%20underlying%20manifold%2C%20and%20introduce%20a%20representation%20based%20on%20the%20pullback%20metric%20that%20captures%20the%20intrinsic%20structure%20of%20the%20latent%20space%2C%20while%20scaling%20efficiently%20to%20large%20models.%20We%20validate%20experimentally%20our%20method%20on%20model%20stitching%20and%20retrieval%20tasks%2C%20covering%20autoencoders%20and%20vision%20foundation%20discriminative%20models%2C%20across%20diverse%20architectures%2C%20datasets%2C%20and%20pretraining%20schemes.&entry.1838667208=http%3A//arxiv.org/abs/2506.01599v2&entry.124074799=Read"},
{"title": "Evaluating SAM2 for Video Semantic Segmentation", "author": "Syed Hesham Syed Ariff and Yun Liu and Guolei Sun and Jing Yang and Henghui Ding and Xue Geng and Xudong Jiang", "abstract": "The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.", "link": "http://arxiv.org/abs/2512.01774v1", "date": "2025-12-01", "relevancy": 2.7733, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20SAM2%20for%20Video%20Semantic%20Segmentation&body=Title%3A%20Evaluating%20SAM2%20for%20Video%20Semantic%20Segmentation%0AAuthor%3A%20Syed%20Hesham%20Syed%20Ariff%20and%20Yun%20Liu%20and%20Guolei%20Sun%20and%20Jing%20Yang%20and%20Henghui%20Ding%20and%20Xue%20Geng%20and%20Xudong%20Jiang%0AAbstract%3A%20The%20Segmentation%20Anything%20Model%202%20%28SAM2%29%20has%20proven%20to%20be%20a%20powerful%20foundation%20model%20for%20promptable%20visual%20object%20segmentation%20in%20both%20images%20and%20videos%2C%20capable%20of%20storing%20object-aware%20memories%20and%20transferring%20them%20temporally%20through%20memory%20blocks.%20While%20SAM2%20excels%20in%20video%20object%20segmentation%20by%20providing%20dense%20segmentation%20masks%20based%20on%20prompts%2C%20extending%20it%20to%20dense%20Video%20Semantic%20Segmentation%20%28VSS%29%20poses%20challenges%20due%20to%20the%20need%20for%20spatial%20accuracy%2C%20temporal%20consistency%2C%20and%20the%20ability%20to%20track%20multiple%20objects%20with%20complex%20boundaries%20and%20varying%20scales.%20This%20paper%20explores%20the%20extension%20of%20SAM2%20for%20VSS%2C%20focusing%20on%20two%20primary%20approaches%20and%20highlighting%20firsthand%20observations%20and%20common%20challenges%20faced%20during%20this%20process.%20The%20first%20approach%20involves%20using%20SAM2%20to%20extract%20unique%20objects%20as%20masks%20from%20a%20given%20image%2C%20with%20a%20segmentation%20network%20employed%20in%20parallel%20to%20generate%20and%20refine%20initial%20predictions.%20The%20second%20approach%20utilizes%20the%20predicted%20masks%20to%20extract%20unique%20feature%20vectors%2C%20which%20are%20then%20fed%20into%20a%20simple%20network%20for%20classification.%20The%20resulting%20classifications%20and%20masks%20are%20subsequently%20combined%20to%20produce%20the%20final%20segmentation.%20Our%20experiments%20suggest%20that%20leveraging%20SAM2%20enhances%20overall%20performance%20in%20VSS%2C%20primarily%20due%20to%20its%20precise%20predictions%20of%20object%20boundaries.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520SAM2%2520for%2520Video%2520Semantic%2520Segmentation%26entry.906535625%3DSyed%2520Hesham%2520Syed%2520Ariff%2520and%2520Yun%2520Liu%2520and%2520Guolei%2520Sun%2520and%2520Jing%2520Yang%2520and%2520Henghui%2520Ding%2520and%2520Xue%2520Geng%2520and%2520Xudong%2520Jiang%26entry.1292438233%3DThe%2520Segmentation%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520has%2520proven%2520to%2520be%2520a%2520powerful%2520foundation%2520model%2520for%2520promptable%2520visual%2520object%2520segmentation%2520in%2520both%2520images%2520and%2520videos%252C%2520capable%2520of%2520storing%2520object-aware%2520memories%2520and%2520transferring%2520them%2520temporally%2520through%2520memory%2520blocks.%2520While%2520SAM2%2520excels%2520in%2520video%2520object%2520segmentation%2520by%2520providing%2520dense%2520segmentation%2520masks%2520based%2520on%2520prompts%252C%2520extending%2520it%2520to%2520dense%2520Video%2520Semantic%2520Segmentation%2520%2528VSS%2529%2520poses%2520challenges%2520due%2520to%2520the%2520need%2520for%2520spatial%2520accuracy%252C%2520temporal%2520consistency%252C%2520and%2520the%2520ability%2520to%2520track%2520multiple%2520objects%2520with%2520complex%2520boundaries%2520and%2520varying%2520scales.%2520This%2520paper%2520explores%2520the%2520extension%2520of%2520SAM2%2520for%2520VSS%252C%2520focusing%2520on%2520two%2520primary%2520approaches%2520and%2520highlighting%2520firsthand%2520observations%2520and%2520common%2520challenges%2520faced%2520during%2520this%2520process.%2520The%2520first%2520approach%2520involves%2520using%2520SAM2%2520to%2520extract%2520unique%2520objects%2520as%2520masks%2520from%2520a%2520given%2520image%252C%2520with%2520a%2520segmentation%2520network%2520employed%2520in%2520parallel%2520to%2520generate%2520and%2520refine%2520initial%2520predictions.%2520The%2520second%2520approach%2520utilizes%2520the%2520predicted%2520masks%2520to%2520extract%2520unique%2520feature%2520vectors%252C%2520which%2520are%2520then%2520fed%2520into%2520a%2520simple%2520network%2520for%2520classification.%2520The%2520resulting%2520classifications%2520and%2520masks%2520are%2520subsequently%2520combined%2520to%2520produce%2520the%2520final%2520segmentation.%2520Our%2520experiments%2520suggest%2520that%2520leveraging%2520SAM2%2520enhances%2520overall%2520performance%2520in%2520VSS%252C%2520primarily%2520due%2520to%2520its%2520precise%2520predictions%2520of%2520object%2520boundaries.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20SAM2%20for%20Video%20Semantic%20Segmentation&entry.906535625=Syed%20Hesham%20Syed%20Ariff%20and%20Yun%20Liu%20and%20Guolei%20Sun%20and%20Jing%20Yang%20and%20Henghui%20Ding%20and%20Xue%20Geng%20and%20Xudong%20Jiang&entry.1292438233=The%20Segmentation%20Anything%20Model%202%20%28SAM2%29%20has%20proven%20to%20be%20a%20powerful%20foundation%20model%20for%20promptable%20visual%20object%20segmentation%20in%20both%20images%20and%20videos%2C%20capable%20of%20storing%20object-aware%20memories%20and%20transferring%20them%20temporally%20through%20memory%20blocks.%20While%20SAM2%20excels%20in%20video%20object%20segmentation%20by%20providing%20dense%20segmentation%20masks%20based%20on%20prompts%2C%20extending%20it%20to%20dense%20Video%20Semantic%20Segmentation%20%28VSS%29%20poses%20challenges%20due%20to%20the%20need%20for%20spatial%20accuracy%2C%20temporal%20consistency%2C%20and%20the%20ability%20to%20track%20multiple%20objects%20with%20complex%20boundaries%20and%20varying%20scales.%20This%20paper%20explores%20the%20extension%20of%20SAM2%20for%20VSS%2C%20focusing%20on%20two%20primary%20approaches%20and%20highlighting%20firsthand%20observations%20and%20common%20challenges%20faced%20during%20this%20process.%20The%20first%20approach%20involves%20using%20SAM2%20to%20extract%20unique%20objects%20as%20masks%20from%20a%20given%20image%2C%20with%20a%20segmentation%20network%20employed%20in%20parallel%20to%20generate%20and%20refine%20initial%20predictions.%20The%20second%20approach%20utilizes%20the%20predicted%20masks%20to%20extract%20unique%20feature%20vectors%2C%20which%20are%20then%20fed%20into%20a%20simple%20network%20for%20classification.%20The%20resulting%20classifications%20and%20masks%20are%20subsequently%20combined%20to%20produce%20the%20final%20segmentation.%20Our%20experiments%20suggest%20that%20leveraging%20SAM2%20enhances%20overall%20performance%20in%20VSS%2C%20primarily%20due%20to%20its%20precise%20predictions%20of%20object%20boundaries.&entry.1838667208=http%3A//arxiv.org/abs/2512.01774v1&entry.124074799=Read"},
{"title": "Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching", "author": "Yue Pan and Tao Sun and Liyuan Zhu and Lucas Nunes and Iro Armeni and Jens Behley and Cyrill Stachniss", "abstract": "Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.", "link": "http://arxiv.org/abs/2512.01850v1", "date": "2025-12-01", "relevancy": 2.7369, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5854}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5294}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Register%20Any%20Point%3A%20Scaling%203D%20Point%20Cloud%20Registration%20by%20Flow%20Matching&body=Title%3A%20Register%20Any%20Point%3A%20Scaling%203D%20Point%20Cloud%20Registration%20by%20Flow%20Matching%0AAuthor%3A%20Yue%20Pan%20and%20Tao%20Sun%20and%20Liyuan%20Zhu%20and%20Lucas%20Nunes%20and%20Iro%20Armeni%20and%20Jens%20Behley%20and%20Cyrill%20Stachniss%0AAbstract%3A%20Point%20cloud%20registration%20aligns%20multiple%20unposed%20point%20clouds%20into%20a%20common%20frame%2C%20and%20is%20a%20core%20step%20for%203D%20reconstruction%20and%20robot%20localization.%20In%20this%20work%2C%20we%20cast%20registration%20as%20conditional%20generation%3A%20a%20learned%20continuous%2C%20point-wise%20velocity%20field%20transports%20noisy%20points%20to%20a%20registered%20scene%2C%20from%20which%20the%20pose%20of%20each%20view%20is%20recovered.%20Unlike%20previous%20methods%20that%20conduct%20correspondence%20matching%20to%20estimate%20the%20transformation%20between%20a%20pair%20of%20point%20clouds%20and%20then%20optimize%20the%20pairwise%20transformations%20to%20realize%20multi-view%20registration%2C%20our%20model%20directly%20generates%20the%20registered%20point%20cloud.%20With%20a%20lightweight%20local%20feature%20extractor%20and%20test-time%20rigidity%20enforcement%2C%20our%20approach%20achieves%20state-of-the-art%20results%20on%20pairwise%20and%20multi-view%20registration%20benchmarks%2C%20particularly%20with%20low%20overlap%2C%20and%20generalizes%20across%20scales%20and%20sensor%20modalities.%20It%20further%20supports%20downstream%20tasks%20including%20relocalization%2C%20multi-robot%20SLAM%2C%20and%20multi-session%20map%20merging.%20Source%20code%20available%20at%3A%20https%3A//github.com/PRBonn/RAP.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegister%2520Any%2520Point%253A%2520Scaling%25203D%2520Point%2520Cloud%2520Registration%2520by%2520Flow%2520Matching%26entry.906535625%3DYue%2520Pan%2520and%2520Tao%2520Sun%2520and%2520Liyuan%2520Zhu%2520and%2520Lucas%2520Nunes%2520and%2520Iro%2520Armeni%2520and%2520Jens%2520Behley%2520and%2520Cyrill%2520Stachniss%26entry.1292438233%3DPoint%2520cloud%2520registration%2520aligns%2520multiple%2520unposed%2520point%2520clouds%2520into%2520a%2520common%2520frame%252C%2520and%2520is%2520a%2520core%2520step%2520for%25203D%2520reconstruction%2520and%2520robot%2520localization.%2520In%2520this%2520work%252C%2520we%2520cast%2520registration%2520as%2520conditional%2520generation%253A%2520a%2520learned%2520continuous%252C%2520point-wise%2520velocity%2520field%2520transports%2520noisy%2520points%2520to%2520a%2520registered%2520scene%252C%2520from%2520which%2520the%2520pose%2520of%2520each%2520view%2520is%2520recovered.%2520Unlike%2520previous%2520methods%2520that%2520conduct%2520correspondence%2520matching%2520to%2520estimate%2520the%2520transformation%2520between%2520a%2520pair%2520of%2520point%2520clouds%2520and%2520then%2520optimize%2520the%2520pairwise%2520transformations%2520to%2520realize%2520multi-view%2520registration%252C%2520our%2520model%2520directly%2520generates%2520the%2520registered%2520point%2520cloud.%2520With%2520a%2520lightweight%2520local%2520feature%2520extractor%2520and%2520test-time%2520rigidity%2520enforcement%252C%2520our%2520approach%2520achieves%2520state-of-the-art%2520results%2520on%2520pairwise%2520and%2520multi-view%2520registration%2520benchmarks%252C%2520particularly%2520with%2520low%2520overlap%252C%2520and%2520generalizes%2520across%2520scales%2520and%2520sensor%2520modalities.%2520It%2520further%2520supports%2520downstream%2520tasks%2520including%2520relocalization%252C%2520multi-robot%2520SLAM%252C%2520and%2520multi-session%2520map%2520merging.%2520Source%2520code%2520available%2520at%253A%2520https%253A//github.com/PRBonn/RAP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Register%20Any%20Point%3A%20Scaling%203D%20Point%20Cloud%20Registration%20by%20Flow%20Matching&entry.906535625=Yue%20Pan%20and%20Tao%20Sun%20and%20Liyuan%20Zhu%20and%20Lucas%20Nunes%20and%20Iro%20Armeni%20and%20Jens%20Behley%20and%20Cyrill%20Stachniss&entry.1292438233=Point%20cloud%20registration%20aligns%20multiple%20unposed%20point%20clouds%20into%20a%20common%20frame%2C%20and%20is%20a%20core%20step%20for%203D%20reconstruction%20and%20robot%20localization.%20In%20this%20work%2C%20we%20cast%20registration%20as%20conditional%20generation%3A%20a%20learned%20continuous%2C%20point-wise%20velocity%20field%20transports%20noisy%20points%20to%20a%20registered%20scene%2C%20from%20which%20the%20pose%20of%20each%20view%20is%20recovered.%20Unlike%20previous%20methods%20that%20conduct%20correspondence%20matching%20to%20estimate%20the%20transformation%20between%20a%20pair%20of%20point%20clouds%20and%20then%20optimize%20the%20pairwise%20transformations%20to%20realize%20multi-view%20registration%2C%20our%20model%20directly%20generates%20the%20registered%20point%20cloud.%20With%20a%20lightweight%20local%20feature%20extractor%20and%20test-time%20rigidity%20enforcement%2C%20our%20approach%20achieves%20state-of-the-art%20results%20on%20pairwise%20and%20multi-view%20registration%20benchmarks%2C%20particularly%20with%20low%20overlap%2C%20and%20generalizes%20across%20scales%20and%20sensor%20modalities.%20It%20further%20supports%20downstream%20tasks%20including%20relocalization%2C%20multi-robot%20SLAM%2C%20and%20multi-session%20map%20merging.%20Source%20code%20available%20at%3A%20https%3A//github.com/PRBonn/RAP.&entry.1838667208=http%3A//arxiv.org/abs/2512.01850v1&entry.124074799=Read"},
{"title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "author": "Zhiheng Liu and Weiming Ren and Haozhe Liu and Zijian Zhou and Shoufa Chen and Haonan Qiu and Xiaoke Huang and Zhaochong An and Fanny Yang and Aditya Patel and Viktar Atliha and Tony Ng and Xiao Han and Chuyan Zhu and Chenyang Zhang and Ding Liu and Juan-Manuel Perez-Rua and Sen He and J\u00fcrgen Schmidhuber and Wenhu Chen and Ping Luo and Wei Liu and Tao Xiang and Jonas Schult and Yuren Cong", "abstract": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.", "link": "http://arxiv.org/abs/2512.02014v1", "date": "2025-12-01", "relevancy": 2.7123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TUNA%3A%20Taming%20Unified%20Visual%20Representations%20for%20Native%20Unified%20Multimodal%20Models&body=Title%3A%20TUNA%3A%20Taming%20Unified%20Visual%20Representations%20for%20Native%20Unified%20Multimodal%20Models%0AAuthor%3A%20Zhiheng%20Liu%20and%20Weiming%20Ren%20and%20Haozhe%20Liu%20and%20Zijian%20Zhou%20and%20Shoufa%20Chen%20and%20Haonan%20Qiu%20and%20Xiaoke%20Huang%20and%20Zhaochong%20An%20and%20Fanny%20Yang%20and%20Aditya%20Patel%20and%20Viktar%20Atliha%20and%20Tony%20Ng%20and%20Xiao%20Han%20and%20Chuyan%20Zhu%20and%20Chenyang%20Zhang%20and%20Ding%20Liu%20and%20Juan-Manuel%20Perez-Rua%20and%20Sen%20He%20and%20J%C3%BCrgen%20Schmidhuber%20and%20Wenhu%20Chen%20and%20Ping%20Luo%20and%20Wei%20Liu%20and%20Tao%20Xiang%20and%20Jonas%20Schult%20and%20Yuren%20Cong%0AAbstract%3A%20Unified%20multimodal%20models%20%28UMMs%29%20aim%20to%20jointly%20perform%20multimodal%20understanding%20and%20generation%20within%20a%20single%20framework.%20We%20present%20TUNA%2C%20a%20native%20UMM%20that%20builds%20a%20unified%20continuous%20visual%20representation%20by%20cascading%20a%20VAE%20encoder%20with%20a%20representation%20encoder.%20This%20unified%20representation%20space%20allows%20end-to-end%20processing%20of%20images%20and%20videos%20for%20both%20understanding%20and%20generation%20tasks.%20Compared%20to%20prior%20UMMs%20with%20decoupled%20representations%2C%20TUNA%27s%20unified%20visual%20space%20avoids%20representation%20format%20mismatches%20introduced%20by%20separate%20encoders%2C%20outperforming%20decoupled%20alternatives%20in%20both%20understanding%20and%20generation.%20Moreover%2C%20we%20observe%20that%20stronger%20pretrained%20representation%20encoders%20consistently%20yield%20better%20performance%20across%20all%20multimodal%20tasks%2C%20highlighting%20the%20importance%20of%20the%20representation%20encoder.%20Finally%2C%20in%20this%20unified%20setting%2C%20jointly%20training%20on%20both%20understanding%20and%20generation%20data%20allows%20the%20two%20tasks%20to%20benefit%20from%20each%20other%20rather%20than%20interfere.%20Our%20extensive%20experiments%20on%20multimodal%20understanding%20and%20generation%20benchmarks%20show%20that%20TUNA%20achieves%20state-of-the-art%20results%20in%20image%20and%20video%20understanding%2C%20image%20and%20video%20generation%2C%20and%20image%20editing%2C%20demonstrating%20the%20effectiveness%20and%20scalability%20of%20its%20unified%20representation%20design.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTUNA%253A%2520Taming%2520Unified%2520Visual%2520Representations%2520for%2520Native%2520Unified%2520Multimodal%2520Models%26entry.906535625%3DZhiheng%2520Liu%2520and%2520Weiming%2520Ren%2520and%2520Haozhe%2520Liu%2520and%2520Zijian%2520Zhou%2520and%2520Shoufa%2520Chen%2520and%2520Haonan%2520Qiu%2520and%2520Xiaoke%2520Huang%2520and%2520Zhaochong%2520An%2520and%2520Fanny%2520Yang%2520and%2520Aditya%2520Patel%2520and%2520Viktar%2520Atliha%2520and%2520Tony%2520Ng%2520and%2520Xiao%2520Han%2520and%2520Chuyan%2520Zhu%2520and%2520Chenyang%2520Zhang%2520and%2520Ding%2520Liu%2520and%2520Juan-Manuel%2520Perez-Rua%2520and%2520Sen%2520He%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%2520and%2520Wenhu%2520Chen%2520and%2520Ping%2520Luo%2520and%2520Wei%2520Liu%2520and%2520Tao%2520Xiang%2520and%2520Jonas%2520Schult%2520and%2520Yuren%2520Cong%26entry.1292438233%3DUnified%2520multimodal%2520models%2520%2528UMMs%2529%2520aim%2520to%2520jointly%2520perform%2520multimodal%2520understanding%2520and%2520generation%2520within%2520a%2520single%2520framework.%2520We%2520present%2520TUNA%252C%2520a%2520native%2520UMM%2520that%2520builds%2520a%2520unified%2520continuous%2520visual%2520representation%2520by%2520cascading%2520a%2520VAE%2520encoder%2520with%2520a%2520representation%2520encoder.%2520This%2520unified%2520representation%2520space%2520allows%2520end-to-end%2520processing%2520of%2520images%2520and%2520videos%2520for%2520both%2520understanding%2520and%2520generation%2520tasks.%2520Compared%2520to%2520prior%2520UMMs%2520with%2520decoupled%2520representations%252C%2520TUNA%2527s%2520unified%2520visual%2520space%2520avoids%2520representation%2520format%2520mismatches%2520introduced%2520by%2520separate%2520encoders%252C%2520outperforming%2520decoupled%2520alternatives%2520in%2520both%2520understanding%2520and%2520generation.%2520Moreover%252C%2520we%2520observe%2520that%2520stronger%2520pretrained%2520representation%2520encoders%2520consistently%2520yield%2520better%2520performance%2520across%2520all%2520multimodal%2520tasks%252C%2520highlighting%2520the%2520importance%2520of%2520the%2520representation%2520encoder.%2520Finally%252C%2520in%2520this%2520unified%2520setting%252C%2520jointly%2520training%2520on%2520both%2520understanding%2520and%2520generation%2520data%2520allows%2520the%2520two%2520tasks%2520to%2520benefit%2520from%2520each%2520other%2520rather%2520than%2520interfere.%2520Our%2520extensive%2520experiments%2520on%2520multimodal%2520understanding%2520and%2520generation%2520benchmarks%2520show%2520that%2520TUNA%2520achieves%2520state-of-the-art%2520results%2520in%2520image%2520and%2520video%2520understanding%252C%2520image%2520and%2520video%2520generation%252C%2520and%2520image%2520editing%252C%2520demonstrating%2520the%2520effectiveness%2520and%2520scalability%2520of%2520its%2520unified%2520representation%2520design.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TUNA%3A%20Taming%20Unified%20Visual%20Representations%20for%20Native%20Unified%20Multimodal%20Models&entry.906535625=Zhiheng%20Liu%20and%20Weiming%20Ren%20and%20Haozhe%20Liu%20and%20Zijian%20Zhou%20and%20Shoufa%20Chen%20and%20Haonan%20Qiu%20and%20Xiaoke%20Huang%20and%20Zhaochong%20An%20and%20Fanny%20Yang%20and%20Aditya%20Patel%20and%20Viktar%20Atliha%20and%20Tony%20Ng%20and%20Xiao%20Han%20and%20Chuyan%20Zhu%20and%20Chenyang%20Zhang%20and%20Ding%20Liu%20and%20Juan-Manuel%20Perez-Rua%20and%20Sen%20He%20and%20J%C3%BCrgen%20Schmidhuber%20and%20Wenhu%20Chen%20and%20Ping%20Luo%20and%20Wei%20Liu%20and%20Tao%20Xiang%20and%20Jonas%20Schult%20and%20Yuren%20Cong&entry.1292438233=Unified%20multimodal%20models%20%28UMMs%29%20aim%20to%20jointly%20perform%20multimodal%20understanding%20and%20generation%20within%20a%20single%20framework.%20We%20present%20TUNA%2C%20a%20native%20UMM%20that%20builds%20a%20unified%20continuous%20visual%20representation%20by%20cascading%20a%20VAE%20encoder%20with%20a%20representation%20encoder.%20This%20unified%20representation%20space%20allows%20end-to-end%20processing%20of%20images%20and%20videos%20for%20both%20understanding%20and%20generation%20tasks.%20Compared%20to%20prior%20UMMs%20with%20decoupled%20representations%2C%20TUNA%27s%20unified%20visual%20space%20avoids%20representation%20format%20mismatches%20introduced%20by%20separate%20encoders%2C%20outperforming%20decoupled%20alternatives%20in%20both%20understanding%20and%20generation.%20Moreover%2C%20we%20observe%20that%20stronger%20pretrained%20representation%20encoders%20consistently%20yield%20better%20performance%20across%20all%20multimodal%20tasks%2C%20highlighting%20the%20importance%20of%20the%20representation%20encoder.%20Finally%2C%20in%20this%20unified%20setting%2C%20jointly%20training%20on%20both%20understanding%20and%20generation%20data%20allows%20the%20two%20tasks%20to%20benefit%20from%20each%20other%20rather%20than%20interfere.%20Our%20extensive%20experiments%20on%20multimodal%20understanding%20and%20generation%20benchmarks%20show%20that%20TUNA%20achieves%20state-of-the-art%20results%20in%20image%20and%20video%20understanding%2C%20image%20and%20video%20generation%2C%20and%20image%20editing%2C%20demonstrating%20the%20effectiveness%20and%20scalability%20of%20its%20unified%20representation%20design.&entry.1838667208=http%3A//arxiv.org/abs/2512.02014v1&entry.124074799=Read"},
{"title": "IGen: Scalable Data Generation for Robot Learning from Open-World Images", "author": "Chenghao Gu and Haolan Kang and Junchao Lin and Jinghe Wang and Duo Wu and Shuzhao Xie and Fanding Huang and Junchen Ge and Ziyang Gong and Letian Li and Hongying Zheng and Changwei Lv and Zhi Wang", "abstract": "The rise of generalist robotic policies has created an exponential demand for large-scale training data. However, on-robot data collection is labor-intensive and often limited to specific environments. In contrast, open-world images capture a vast diversity of real-world scenes that naturally align with robotic manipulation tasks, offering a promising avenue for low-cost, large-scale robot data acquisition. Despite this potential, the lack of associated robot actions hinders the practical use of open-world images for robot learning, leaving this rich visual resource largely unexploited. To bridge this gap, we propose IGen, a framework that scalably generates realistic visual observations and executable actions from open-world images. IGen first converts unstructured 2D pixels into structured 3D scene representations suitable for scene understanding and manipulation. It then leverages the reasoning capabilities of vision-language models to transform scene-specific task instructions into high-level plans and generate low-level actions as SE(3) end-effector pose sequences. From these poses, it synthesizes dynamic scene evolution and renders temporally coherent visual observations. Experiments validate the high quality of visuomotor data generated by IGen, and show that policies trained solely on IGen-synthesized data achieve performance comparable to those trained on real-world data. This highlights the potential of IGen to support scalable data generation from open-world images for generalist robotic policy training.", "link": "http://arxiv.org/abs/2512.01773v1", "date": "2025-12-01", "relevancy": 2.6875, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.7281}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6453}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IGen%3A%20Scalable%20Data%20Generation%20for%20Robot%20Learning%20from%20Open-World%20Images&body=Title%3A%20IGen%3A%20Scalable%20Data%20Generation%20for%20Robot%20Learning%20from%20Open-World%20Images%0AAuthor%3A%20Chenghao%20Gu%20and%20Haolan%20Kang%20and%20Junchao%20Lin%20and%20Jinghe%20Wang%20and%20Duo%20Wu%20and%20Shuzhao%20Xie%20and%20Fanding%20Huang%20and%20Junchen%20Ge%20and%20Ziyang%20Gong%20and%20Letian%20Li%20and%20Hongying%20Zheng%20and%20Changwei%20Lv%20and%20Zhi%20Wang%0AAbstract%3A%20The%20rise%20of%20generalist%20robotic%20policies%20has%20created%20an%20exponential%20demand%20for%20large-scale%20training%20data.%20However%2C%20on-robot%20data%20collection%20is%20labor-intensive%20and%20often%20limited%20to%20specific%20environments.%20In%20contrast%2C%20open-world%20images%20capture%20a%20vast%20diversity%20of%20real-world%20scenes%20that%20naturally%20align%20with%20robotic%20manipulation%20tasks%2C%20offering%20a%20promising%20avenue%20for%20low-cost%2C%20large-scale%20robot%20data%20acquisition.%20Despite%20this%20potential%2C%20the%20lack%20of%20associated%20robot%20actions%20hinders%20the%20practical%20use%20of%20open-world%20images%20for%20robot%20learning%2C%20leaving%20this%20rich%20visual%20resource%20largely%20unexploited.%20To%20bridge%20this%20gap%2C%20we%20propose%20IGen%2C%20a%20framework%20that%20scalably%20generates%20realistic%20visual%20observations%20and%20executable%20actions%20from%20open-world%20images.%20IGen%20first%20converts%20unstructured%202D%20pixels%20into%20structured%203D%20scene%20representations%20suitable%20for%20scene%20understanding%20and%20manipulation.%20It%20then%20leverages%20the%20reasoning%20capabilities%20of%20vision-language%20models%20to%20transform%20scene-specific%20task%20instructions%20into%20high-level%20plans%20and%20generate%20low-level%20actions%20as%20SE%283%29%20end-effector%20pose%20sequences.%20From%20these%20poses%2C%20it%20synthesizes%20dynamic%20scene%20evolution%20and%20renders%20temporally%20coherent%20visual%20observations.%20Experiments%20validate%20the%20high%20quality%20of%20visuomotor%20data%20generated%20by%20IGen%2C%20and%20show%20that%20policies%20trained%20solely%20on%20IGen-synthesized%20data%20achieve%20performance%20comparable%20to%20those%20trained%20on%20real-world%20data.%20This%20highlights%20the%20potential%20of%20IGen%20to%20support%20scalable%20data%20generation%20from%20open-world%20images%20for%20generalist%20robotic%20policy%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIGen%253A%2520Scalable%2520Data%2520Generation%2520for%2520Robot%2520Learning%2520from%2520Open-World%2520Images%26entry.906535625%3DChenghao%2520Gu%2520and%2520Haolan%2520Kang%2520and%2520Junchao%2520Lin%2520and%2520Jinghe%2520Wang%2520and%2520Duo%2520Wu%2520and%2520Shuzhao%2520Xie%2520and%2520Fanding%2520Huang%2520and%2520Junchen%2520Ge%2520and%2520Ziyang%2520Gong%2520and%2520Letian%2520Li%2520and%2520Hongying%2520Zheng%2520and%2520Changwei%2520Lv%2520and%2520Zhi%2520Wang%26entry.1292438233%3DThe%2520rise%2520of%2520generalist%2520robotic%2520policies%2520has%2520created%2520an%2520exponential%2520demand%2520for%2520large-scale%2520training%2520data.%2520However%252C%2520on-robot%2520data%2520collection%2520is%2520labor-intensive%2520and%2520often%2520limited%2520to%2520specific%2520environments.%2520In%2520contrast%252C%2520open-world%2520images%2520capture%2520a%2520vast%2520diversity%2520of%2520real-world%2520scenes%2520that%2520naturally%2520align%2520with%2520robotic%2520manipulation%2520tasks%252C%2520offering%2520a%2520promising%2520avenue%2520for%2520low-cost%252C%2520large-scale%2520robot%2520data%2520acquisition.%2520Despite%2520this%2520potential%252C%2520the%2520lack%2520of%2520associated%2520robot%2520actions%2520hinders%2520the%2520practical%2520use%2520of%2520open-world%2520images%2520for%2520robot%2520learning%252C%2520leaving%2520this%2520rich%2520visual%2520resource%2520largely%2520unexploited.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520IGen%252C%2520a%2520framework%2520that%2520scalably%2520generates%2520realistic%2520visual%2520observations%2520and%2520executable%2520actions%2520from%2520open-world%2520images.%2520IGen%2520first%2520converts%2520unstructured%25202D%2520pixels%2520into%2520structured%25203D%2520scene%2520representations%2520suitable%2520for%2520scene%2520understanding%2520and%2520manipulation.%2520It%2520then%2520leverages%2520the%2520reasoning%2520capabilities%2520of%2520vision-language%2520models%2520to%2520transform%2520scene-specific%2520task%2520instructions%2520into%2520high-level%2520plans%2520and%2520generate%2520low-level%2520actions%2520as%2520SE%25283%2529%2520end-effector%2520pose%2520sequences.%2520From%2520these%2520poses%252C%2520it%2520synthesizes%2520dynamic%2520scene%2520evolution%2520and%2520renders%2520temporally%2520coherent%2520visual%2520observations.%2520Experiments%2520validate%2520the%2520high%2520quality%2520of%2520visuomotor%2520data%2520generated%2520by%2520IGen%252C%2520and%2520show%2520that%2520policies%2520trained%2520solely%2520on%2520IGen-synthesized%2520data%2520achieve%2520performance%2520comparable%2520to%2520those%2520trained%2520on%2520real-world%2520data.%2520This%2520highlights%2520the%2520potential%2520of%2520IGen%2520to%2520support%2520scalable%2520data%2520generation%2520from%2520open-world%2520images%2520for%2520generalist%2520robotic%2520policy%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IGen%3A%20Scalable%20Data%20Generation%20for%20Robot%20Learning%20from%20Open-World%20Images&entry.906535625=Chenghao%20Gu%20and%20Haolan%20Kang%20and%20Junchao%20Lin%20and%20Jinghe%20Wang%20and%20Duo%20Wu%20and%20Shuzhao%20Xie%20and%20Fanding%20Huang%20and%20Junchen%20Ge%20and%20Ziyang%20Gong%20and%20Letian%20Li%20and%20Hongying%20Zheng%20and%20Changwei%20Lv%20and%20Zhi%20Wang&entry.1292438233=The%20rise%20of%20generalist%20robotic%20policies%20has%20created%20an%20exponential%20demand%20for%20large-scale%20training%20data.%20However%2C%20on-robot%20data%20collection%20is%20labor-intensive%20and%20often%20limited%20to%20specific%20environments.%20In%20contrast%2C%20open-world%20images%20capture%20a%20vast%20diversity%20of%20real-world%20scenes%20that%20naturally%20align%20with%20robotic%20manipulation%20tasks%2C%20offering%20a%20promising%20avenue%20for%20low-cost%2C%20large-scale%20robot%20data%20acquisition.%20Despite%20this%20potential%2C%20the%20lack%20of%20associated%20robot%20actions%20hinders%20the%20practical%20use%20of%20open-world%20images%20for%20robot%20learning%2C%20leaving%20this%20rich%20visual%20resource%20largely%20unexploited.%20To%20bridge%20this%20gap%2C%20we%20propose%20IGen%2C%20a%20framework%20that%20scalably%20generates%20realistic%20visual%20observations%20and%20executable%20actions%20from%20open-world%20images.%20IGen%20first%20converts%20unstructured%202D%20pixels%20into%20structured%203D%20scene%20representations%20suitable%20for%20scene%20understanding%20and%20manipulation.%20It%20then%20leverages%20the%20reasoning%20capabilities%20of%20vision-language%20models%20to%20transform%20scene-specific%20task%20instructions%20into%20high-level%20plans%20and%20generate%20low-level%20actions%20as%20SE%283%29%20end-effector%20pose%20sequences.%20From%20these%20poses%2C%20it%20synthesizes%20dynamic%20scene%20evolution%20and%20renders%20temporally%20coherent%20visual%20observations.%20Experiments%20validate%20the%20high%20quality%20of%20visuomotor%20data%20generated%20by%20IGen%2C%20and%20show%20that%20policies%20trained%20solely%20on%20IGen-synthesized%20data%20achieve%20performance%20comparable%20to%20those%20trained%20on%20real-world%20data.%20This%20highlights%20the%20potential%20of%20IGen%20to%20support%20scalable%20data%20generation%20from%20open-world%20images%20for%20generalist%20robotic%20policy%20training.&entry.1838667208=http%3A//arxiv.org/abs/2512.01773v1&entry.124074799=Read"},
{"title": "Learning Visual Affordance from Audio", "author": "Lidong Lu and Guo Chen and Zhu Wei and Yicheng Liu and Tong Lu", "abstract": "We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds. Unlike existing approaches that rely on textual instructions or demonstration videos, which often limited by ambiguity or occlusion, audio provides real-time, semantically rich, and visually independent cues for affordance grounding, enabling more intuitive understanding of interaction regions. To support this task, we construct the first AV-AG dataset, comprising a large collection of action sounds, object images, and pixel-level affordance annotations. The dataset also includes an unseen subset to evaluate zero-shot generalization. Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction. Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks. Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component. Code and dataset have been released on https://jscslld.github.io/AVAGFormer/.", "link": "http://arxiv.org/abs/2512.02005v1", "date": "2025-12-01", "relevancy": 2.6869, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5313}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Visual%20Affordance%20from%20Audio&body=Title%3A%20Learning%20Visual%20Affordance%20from%20Audio%0AAuthor%3A%20Lidong%20Lu%20and%20Guo%20Chen%20and%20Zhu%20Wei%20and%20Yicheng%20Liu%20and%20Tong%20Lu%0AAbstract%3A%20We%20introduce%20Audio-Visual%20Affordance%20Grounding%20%28AV-AG%29%2C%20a%20new%20task%20that%20segments%20object%20interaction%20regions%20from%20action%20sounds.%20Unlike%20existing%20approaches%20that%20rely%20on%20textual%20instructions%20or%20demonstration%20videos%2C%20which%20often%20limited%20by%20ambiguity%20or%20occlusion%2C%20audio%20provides%20real-time%2C%20semantically%20rich%2C%20and%20visually%20independent%20cues%20for%20affordance%20grounding%2C%20enabling%20more%20intuitive%20understanding%20of%20interaction%20regions.%20To%20support%20this%20task%2C%20we%20construct%20the%20first%20AV-AG%20dataset%2C%20comprising%20a%20large%20collection%20of%20action%20sounds%2C%20object%20images%2C%20and%20pixel-level%20affordance%20annotations.%20The%20dataset%20also%20includes%20an%20unseen%20subset%20to%20evaluate%20zero-shot%20generalization.%20Furthermore%2C%20we%20propose%20AVAGFormer%2C%20a%20model%20equipped%20with%20a%20semantic-conditioned%20cross-modal%20mixer%20and%20a%20dual-head%20decoder%20that%20effectively%20fuses%20audio%20and%20visual%20signals%20for%20mask%20prediction.%20Experiments%20show%20that%20AVAGFormer%20achieves%20state-of-the-art%20performance%20on%20AV-AG%2C%20surpassing%20baselines%20from%20related%20tasks.%20Comprehensive%20analyses%20highlight%20the%20distinctions%20between%20AV-AG%20and%20AVS%2C%20the%20benefits%20of%20end-to-end%20modeling%2C%20and%20the%20contribution%20of%20each%20component.%20Code%20and%20dataset%20have%20been%20released%20on%20https%3A//jscslld.github.io/AVAGFormer/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Visual%2520Affordance%2520from%2520Audio%26entry.906535625%3DLidong%2520Lu%2520and%2520Guo%2520Chen%2520and%2520Zhu%2520Wei%2520and%2520Yicheng%2520Liu%2520and%2520Tong%2520Lu%26entry.1292438233%3DWe%2520introduce%2520Audio-Visual%2520Affordance%2520Grounding%2520%2528AV-AG%2529%252C%2520a%2520new%2520task%2520that%2520segments%2520object%2520interaction%2520regions%2520from%2520action%2520sounds.%2520Unlike%2520existing%2520approaches%2520that%2520rely%2520on%2520textual%2520instructions%2520or%2520demonstration%2520videos%252C%2520which%2520often%2520limited%2520by%2520ambiguity%2520or%2520occlusion%252C%2520audio%2520provides%2520real-time%252C%2520semantically%2520rich%252C%2520and%2520visually%2520independent%2520cues%2520for%2520affordance%2520grounding%252C%2520enabling%2520more%2520intuitive%2520understanding%2520of%2520interaction%2520regions.%2520To%2520support%2520this%2520task%252C%2520we%2520construct%2520the%2520first%2520AV-AG%2520dataset%252C%2520comprising%2520a%2520large%2520collection%2520of%2520action%2520sounds%252C%2520object%2520images%252C%2520and%2520pixel-level%2520affordance%2520annotations.%2520The%2520dataset%2520also%2520includes%2520an%2520unseen%2520subset%2520to%2520evaluate%2520zero-shot%2520generalization.%2520Furthermore%252C%2520we%2520propose%2520AVAGFormer%252C%2520a%2520model%2520equipped%2520with%2520a%2520semantic-conditioned%2520cross-modal%2520mixer%2520and%2520a%2520dual-head%2520decoder%2520that%2520effectively%2520fuses%2520audio%2520and%2520visual%2520signals%2520for%2520mask%2520prediction.%2520Experiments%2520show%2520that%2520AVAGFormer%2520achieves%2520state-of-the-art%2520performance%2520on%2520AV-AG%252C%2520surpassing%2520baselines%2520from%2520related%2520tasks.%2520Comprehensive%2520analyses%2520highlight%2520the%2520distinctions%2520between%2520AV-AG%2520and%2520AVS%252C%2520the%2520benefits%2520of%2520end-to-end%2520modeling%252C%2520and%2520the%2520contribution%2520of%2520each%2520component.%2520Code%2520and%2520dataset%2520have%2520been%2520released%2520on%2520https%253A//jscslld.github.io/AVAGFormer/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Visual%20Affordance%20from%20Audio&entry.906535625=Lidong%20Lu%20and%20Guo%20Chen%20and%20Zhu%20Wei%20and%20Yicheng%20Liu%20and%20Tong%20Lu&entry.1292438233=We%20introduce%20Audio-Visual%20Affordance%20Grounding%20%28AV-AG%29%2C%20a%20new%20task%20that%20segments%20object%20interaction%20regions%20from%20action%20sounds.%20Unlike%20existing%20approaches%20that%20rely%20on%20textual%20instructions%20or%20demonstration%20videos%2C%20which%20often%20limited%20by%20ambiguity%20or%20occlusion%2C%20audio%20provides%20real-time%2C%20semantically%20rich%2C%20and%20visually%20independent%20cues%20for%20affordance%20grounding%2C%20enabling%20more%20intuitive%20understanding%20of%20interaction%20regions.%20To%20support%20this%20task%2C%20we%20construct%20the%20first%20AV-AG%20dataset%2C%20comprising%20a%20large%20collection%20of%20action%20sounds%2C%20object%20images%2C%20and%20pixel-level%20affordance%20annotations.%20The%20dataset%20also%20includes%20an%20unseen%20subset%20to%20evaluate%20zero-shot%20generalization.%20Furthermore%2C%20we%20propose%20AVAGFormer%2C%20a%20model%20equipped%20with%20a%20semantic-conditioned%20cross-modal%20mixer%20and%20a%20dual-head%20decoder%20that%20effectively%20fuses%20audio%20and%20visual%20signals%20for%20mask%20prediction.%20Experiments%20show%20that%20AVAGFormer%20achieves%20state-of-the-art%20performance%20on%20AV-AG%2C%20surpassing%20baselines%20from%20related%20tasks.%20Comprehensive%20analyses%20highlight%20the%20distinctions%20between%20AV-AG%20and%20AVS%2C%20the%20benefits%20of%20end-to-end%20modeling%2C%20and%20the%20contribution%20of%20each%20component.%20Code%20and%20dataset%20have%20been%20released%20on%20https%3A//jscslld.github.io/AVAGFormer/.&entry.1838667208=http%3A//arxiv.org/abs/2512.02005v1&entry.124074799=Read"},
{"title": "SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation", "author": "Xiuli Bi and Die Xiao and Junchao Fan and Bin Xiao", "abstract": "In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.", "link": "http://arxiv.org/abs/2512.01701v1", "date": "2025-12-01", "relevancy": 2.6822, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5643}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSR%3A%20Semantic%20and%20Spatial%20Rectification%20for%20CLIP-based%20Weakly%20Supervised%20Segmentation&body=Title%3A%20SSR%3A%20Semantic%20and%20Spatial%20Rectification%20for%20CLIP-based%20Weakly%20Supervised%20Segmentation%0AAuthor%3A%20Xiuli%20Bi%20and%20Die%20Xiao%20and%20Junchao%20Fan%20and%20Bin%20Xiao%0AAbstract%3A%20In%20recent%20years%2C%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20has%20been%20widely%20applied%20to%20Weakly%20Supervised%20Semantic%20Segmentation%20%28WSSS%29%20tasks%20due%20to%20its%20powerful%20cross-modal%20semantic%20understanding%20capabilities.%20This%20paper%20proposes%20a%20novel%20Semantic%20and%20Spatial%20Rectification%20%28SSR%29%20method%20to%20address%20the%20limitations%20of%20existing%20CLIP-based%20weakly%20supervised%20semantic%20segmentation%20approaches%3A%20over-activation%20in%20non-target%20foreground%20regions%20and%20background%20areas.%20Specifically%2C%20at%20the%20semantic%20level%2C%20the%20Cross-Modal%20Prototype%20Alignment%20%28CMPA%29%20establishes%20a%20contrastive%20learning%20mechanism%20to%20enforce%20feature%20space%20alignment%20across%20modalities%2C%20reducing%20inter-class%20overlap%20while%20enhancing%20semantic%20correlations%2C%20to%20rectify%20over-activation%20in%20non-target%20foreground%20regions%20effectively%3B%20at%20the%20spatial%20level%2C%20the%20Superpixel-Guided%20Correction%20%28SGC%29%20leverages%20superpixel-based%20spatial%20priors%20to%20precisely%20filter%20out%20interference%20from%20non-target%20regions%20during%20affinity%20propagation%2C%20significantly%20rectifying%20background%20over-activation.%20Extensive%20experiments%20on%20the%20PASCAL%20VOC%20and%20MS%20COCO%20datasets%20demonstrate%20that%20our%20method%20outperforms%20all%20single-stage%20approaches%2C%20as%20well%20as%20more%20complex%20multi-stage%20approaches%2C%20achieving%20mIoU%20scores%20of%2079.5%25%20and%2050.6%25%2C%20respectively.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSR%253A%2520Semantic%2520and%2520Spatial%2520Rectification%2520for%2520CLIP-based%2520Weakly%2520Supervised%2520Segmentation%26entry.906535625%3DXiuli%2520Bi%2520and%2520Die%2520Xiao%2520and%2520Junchao%2520Fan%2520and%2520Bin%2520Xiao%26entry.1292438233%3DIn%2520recent%2520years%252C%2520Contrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%2520has%2520been%2520widely%2520applied%2520to%2520Weakly%2520Supervised%2520Semantic%2520Segmentation%2520%2528WSSS%2529%2520tasks%2520due%2520to%2520its%2520powerful%2520cross-modal%2520semantic%2520understanding%2520capabilities.%2520This%2520paper%2520proposes%2520a%2520novel%2520Semantic%2520and%2520Spatial%2520Rectification%2520%2528SSR%2529%2520method%2520to%2520address%2520the%2520limitations%2520of%2520existing%2520CLIP-based%2520weakly%2520supervised%2520semantic%2520segmentation%2520approaches%253A%2520over-activation%2520in%2520non-target%2520foreground%2520regions%2520and%2520background%2520areas.%2520Specifically%252C%2520at%2520the%2520semantic%2520level%252C%2520the%2520Cross-Modal%2520Prototype%2520Alignment%2520%2528CMPA%2529%2520establishes%2520a%2520contrastive%2520learning%2520mechanism%2520to%2520enforce%2520feature%2520space%2520alignment%2520across%2520modalities%252C%2520reducing%2520inter-class%2520overlap%2520while%2520enhancing%2520semantic%2520correlations%252C%2520to%2520rectify%2520over-activation%2520in%2520non-target%2520foreground%2520regions%2520effectively%253B%2520at%2520the%2520spatial%2520level%252C%2520the%2520Superpixel-Guided%2520Correction%2520%2528SGC%2529%2520leverages%2520superpixel-based%2520spatial%2520priors%2520to%2520precisely%2520filter%2520out%2520interference%2520from%2520non-target%2520regions%2520during%2520affinity%2520propagation%252C%2520significantly%2520rectifying%2520background%2520over-activation.%2520Extensive%2520experiments%2520on%2520the%2520PASCAL%2520VOC%2520and%2520MS%2520COCO%2520datasets%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520all%2520single-stage%2520approaches%252C%2520as%2520well%2520as%2520more%2520complex%2520multi-stage%2520approaches%252C%2520achieving%2520mIoU%2520scores%2520of%252079.5%2525%2520and%252050.6%2525%252C%2520respectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSR%3A%20Semantic%20and%20Spatial%20Rectification%20for%20CLIP-based%20Weakly%20Supervised%20Segmentation&entry.906535625=Xiuli%20Bi%20and%20Die%20Xiao%20and%20Junchao%20Fan%20and%20Bin%20Xiao&entry.1292438233=In%20recent%20years%2C%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20has%20been%20widely%20applied%20to%20Weakly%20Supervised%20Semantic%20Segmentation%20%28WSSS%29%20tasks%20due%20to%20its%20powerful%20cross-modal%20semantic%20understanding%20capabilities.%20This%20paper%20proposes%20a%20novel%20Semantic%20and%20Spatial%20Rectification%20%28SSR%29%20method%20to%20address%20the%20limitations%20of%20existing%20CLIP-based%20weakly%20supervised%20semantic%20segmentation%20approaches%3A%20over-activation%20in%20non-target%20foreground%20regions%20and%20background%20areas.%20Specifically%2C%20at%20the%20semantic%20level%2C%20the%20Cross-Modal%20Prototype%20Alignment%20%28CMPA%29%20establishes%20a%20contrastive%20learning%20mechanism%20to%20enforce%20feature%20space%20alignment%20across%20modalities%2C%20reducing%20inter-class%20overlap%20while%20enhancing%20semantic%20correlations%2C%20to%20rectify%20over-activation%20in%20non-target%20foreground%20regions%20effectively%3B%20at%20the%20spatial%20level%2C%20the%20Superpixel-Guided%20Correction%20%28SGC%29%20leverages%20superpixel-based%20spatial%20priors%20to%20precisely%20filter%20out%20interference%20from%20non-target%20regions%20during%20affinity%20propagation%2C%20significantly%20rectifying%20background%20over-activation.%20Extensive%20experiments%20on%20the%20PASCAL%20VOC%20and%20MS%20COCO%20datasets%20demonstrate%20that%20our%20method%20outperforms%20all%20single-stage%20approaches%2C%20as%20well%20as%20more%20complex%20multi-stage%20approaches%2C%20achieving%20mIoU%20scores%20of%2079.5%25%20and%2050.6%25%2C%20respectively.&entry.1838667208=http%3A//arxiv.org/abs/2512.01701v1&entry.124074799=Read"},
{"title": "StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos", "author": "Daeun Lee and Subhojyoti Mukherjee and Branislav Kveton and Ryan A. Rossi and Viet Dac Lai and Seunghyun Yoon and Trung Bui and Franck Dernoncourt and Mohit Bansal", "abstract": "Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.", "link": "http://arxiv.org/abs/2512.01707v1", "date": "2025-12-01", "relevancy": 2.6739, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5445}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5352}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamGaze%3A%20Gaze-Guided%20Temporal%20Reasoning%20and%20Proactive%20Understanding%20in%20Streaming%20Videos&body=Title%3A%20StreamGaze%3A%20Gaze-Guided%20Temporal%20Reasoning%20and%20Proactive%20Understanding%20in%20Streaming%20Videos%0AAuthor%3A%20Daeun%20Lee%20and%20Subhojyoti%20Mukherjee%20and%20Branislav%20Kveton%20and%20Ryan%20A.%20Rossi%20and%20Viet%20Dac%20Lai%20and%20Seunghyun%20Yoon%20and%20Trung%20Bui%20and%20Franck%20Dernoncourt%20and%20Mohit%20Bansal%0AAbstract%3A%20Streaming%20video%20understanding%20requires%20models%20not%20only%20to%20process%20temporally%20incoming%20frames%2C%20but%20also%20to%20anticipate%20user%20intention%20for%20realistic%20applications%20like%20AR%20glasses.%20While%20prior%20streaming%20benchmarks%20evaluate%20temporal%20reasoning%2C%20none%20measure%20whether%20MLLMs%20can%20interpret%20or%20leverage%20human%20gaze%20signals%20within%20a%20streaming%20setting.%20To%20fill%20this%20gap%2C%20we%20introduce%20StreamGaze%2C%20the%20first%20benchmark%20designed%20to%20evaluate%20how%20effectively%20MLLMs%20use%20gaze%20for%20temporal%20and%20proactive%20reasoning%20in%20streaming%20videos.%20StreamGaze%20introduces%20gaze-guided%20past%2C%20present%2C%20and%20proactive%20tasks%20that%20comprehensively%20evaluate%20streaming%20video%20understanding.%20These%20tasks%20assess%20whether%20models%20can%20use%20real-time%20gaze%20to%20follow%20shifting%20attention%20and%20infer%20user%20intentions%20from%20only%20past%20and%20currently%20observed%20frames.%20To%20build%20StreamGaze%2C%20we%20develop%20a%20gaze-video%20QA%20generation%20pipeline%20that%20aligns%20egocentric%20videos%20with%20raw%20gaze%20trajectories%20via%20fixation%20extraction%2C%20region-specific%20visual%20prompting%2C%20and%20scanpath%20construction.%20This%20pipeline%20produces%20spatio-temporally%20grounded%20QA%20pairs%20that%20closely%20reflect%20human%20perceptual%20dynamics.%20Across%20all%20StreamGaze%20tasks%2C%20we%20observe%20substantial%20performance%20gaps%20between%20state-of-the-art%20MLLMs%20and%20human%20performance%2C%20revealing%20fundamental%20limitations%20in%20gaze-based%20temporal%20reasoning%2C%20intention%20modeling%2C%20and%20proactive%20prediction.%20We%20further%20provide%20detailed%20analyses%20of%20gaze-prompting%20strategies%2C%20reasoning%20behaviors%2C%20and%20task-specific%20failure%20modes%2C%20offering%20deeper%20insight%20into%20why%20current%20MLLMs%20struggle%20and%20what%20capabilities%20future%20models%20must%20develop.%20All%20data%20and%20code%20will%20be%20publicly%20released%20to%20support%20continued%20research%20in%20gaze-guided%20streaming%20video%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamGaze%253A%2520Gaze-Guided%2520Temporal%2520Reasoning%2520and%2520Proactive%2520Understanding%2520in%2520Streaming%2520Videos%26entry.906535625%3DDaeun%2520Lee%2520and%2520Subhojyoti%2520Mukherjee%2520and%2520Branislav%2520Kveton%2520and%2520Ryan%2520A.%2520Rossi%2520and%2520Viet%2520Dac%2520Lai%2520and%2520Seunghyun%2520Yoon%2520and%2520Trung%2520Bui%2520and%2520Franck%2520Dernoncourt%2520and%2520Mohit%2520Bansal%26entry.1292438233%3DStreaming%2520video%2520understanding%2520requires%2520models%2520not%2520only%2520to%2520process%2520temporally%2520incoming%2520frames%252C%2520but%2520also%2520to%2520anticipate%2520user%2520intention%2520for%2520realistic%2520applications%2520like%2520AR%2520glasses.%2520While%2520prior%2520streaming%2520benchmarks%2520evaluate%2520temporal%2520reasoning%252C%2520none%2520measure%2520whether%2520MLLMs%2520can%2520interpret%2520or%2520leverage%2520human%2520gaze%2520signals%2520within%2520a%2520streaming%2520setting.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520StreamGaze%252C%2520the%2520first%2520benchmark%2520designed%2520to%2520evaluate%2520how%2520effectively%2520MLLMs%2520use%2520gaze%2520for%2520temporal%2520and%2520proactive%2520reasoning%2520in%2520streaming%2520videos.%2520StreamGaze%2520introduces%2520gaze-guided%2520past%252C%2520present%252C%2520and%2520proactive%2520tasks%2520that%2520comprehensively%2520evaluate%2520streaming%2520video%2520understanding.%2520These%2520tasks%2520assess%2520whether%2520models%2520can%2520use%2520real-time%2520gaze%2520to%2520follow%2520shifting%2520attention%2520and%2520infer%2520user%2520intentions%2520from%2520only%2520past%2520and%2520currently%2520observed%2520frames.%2520To%2520build%2520StreamGaze%252C%2520we%2520develop%2520a%2520gaze-video%2520QA%2520generation%2520pipeline%2520that%2520aligns%2520egocentric%2520videos%2520with%2520raw%2520gaze%2520trajectories%2520via%2520fixation%2520extraction%252C%2520region-specific%2520visual%2520prompting%252C%2520and%2520scanpath%2520construction.%2520This%2520pipeline%2520produces%2520spatio-temporally%2520grounded%2520QA%2520pairs%2520that%2520closely%2520reflect%2520human%2520perceptual%2520dynamics.%2520Across%2520all%2520StreamGaze%2520tasks%252C%2520we%2520observe%2520substantial%2520performance%2520gaps%2520between%2520state-of-the-art%2520MLLMs%2520and%2520human%2520performance%252C%2520revealing%2520fundamental%2520limitations%2520in%2520gaze-based%2520temporal%2520reasoning%252C%2520intention%2520modeling%252C%2520and%2520proactive%2520prediction.%2520We%2520further%2520provide%2520detailed%2520analyses%2520of%2520gaze-prompting%2520strategies%252C%2520reasoning%2520behaviors%252C%2520and%2520task-specific%2520failure%2520modes%252C%2520offering%2520deeper%2520insight%2520into%2520why%2520current%2520MLLMs%2520struggle%2520and%2520what%2520capabilities%2520future%2520models%2520must%2520develop.%2520All%2520data%2520and%2520code%2520will%2520be%2520publicly%2520released%2520to%2520support%2520continued%2520research%2520in%2520gaze-guided%2520streaming%2520video%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamGaze%3A%20Gaze-Guided%20Temporal%20Reasoning%20and%20Proactive%20Understanding%20in%20Streaming%20Videos&entry.906535625=Daeun%20Lee%20and%20Subhojyoti%20Mukherjee%20and%20Branislav%20Kveton%20and%20Ryan%20A.%20Rossi%20and%20Viet%20Dac%20Lai%20and%20Seunghyun%20Yoon%20and%20Trung%20Bui%20and%20Franck%20Dernoncourt%20and%20Mohit%20Bansal&entry.1292438233=Streaming%20video%20understanding%20requires%20models%20not%20only%20to%20process%20temporally%20incoming%20frames%2C%20but%20also%20to%20anticipate%20user%20intention%20for%20realistic%20applications%20like%20AR%20glasses.%20While%20prior%20streaming%20benchmarks%20evaluate%20temporal%20reasoning%2C%20none%20measure%20whether%20MLLMs%20can%20interpret%20or%20leverage%20human%20gaze%20signals%20within%20a%20streaming%20setting.%20To%20fill%20this%20gap%2C%20we%20introduce%20StreamGaze%2C%20the%20first%20benchmark%20designed%20to%20evaluate%20how%20effectively%20MLLMs%20use%20gaze%20for%20temporal%20and%20proactive%20reasoning%20in%20streaming%20videos.%20StreamGaze%20introduces%20gaze-guided%20past%2C%20present%2C%20and%20proactive%20tasks%20that%20comprehensively%20evaluate%20streaming%20video%20understanding.%20These%20tasks%20assess%20whether%20models%20can%20use%20real-time%20gaze%20to%20follow%20shifting%20attention%20and%20infer%20user%20intentions%20from%20only%20past%20and%20currently%20observed%20frames.%20To%20build%20StreamGaze%2C%20we%20develop%20a%20gaze-video%20QA%20generation%20pipeline%20that%20aligns%20egocentric%20videos%20with%20raw%20gaze%20trajectories%20via%20fixation%20extraction%2C%20region-specific%20visual%20prompting%2C%20and%20scanpath%20construction.%20This%20pipeline%20produces%20spatio-temporally%20grounded%20QA%20pairs%20that%20closely%20reflect%20human%20perceptual%20dynamics.%20Across%20all%20StreamGaze%20tasks%2C%20we%20observe%20substantial%20performance%20gaps%20between%20state-of-the-art%20MLLMs%20and%20human%20performance%2C%20revealing%20fundamental%20limitations%20in%20gaze-based%20temporal%20reasoning%2C%20intention%20modeling%2C%20and%20proactive%20prediction.%20We%20further%20provide%20detailed%20analyses%20of%20gaze-prompting%20strategies%2C%20reasoning%20behaviors%2C%20and%20task-specific%20failure%20modes%2C%20offering%20deeper%20insight%20into%20why%20current%20MLLMs%20struggle%20and%20what%20capabilities%20future%20models%20must%20develop.%20All%20data%20and%20code%20will%20be%20publicly%20released%20to%20support%20continued%20research%20in%20gaze-guided%20streaming%20video%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2512.01707v1&entry.124074799=Read"},
{"title": "GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain tumour Segmentation on mp-MRI", "author": "Cecilia Diana-Albelda and Roberto Alcover-Couso and \u00c1lvaro Garc\u00eda-Mart\u00edn and Jesus Bescos and Marcos Escudero-Vi\u00f1olo", "abstract": "Gliomas are aggressive brain tumors that require accurate imaging-based diagnosis, with segmentation playing a critical role in evaluating morphology and treatment decisions. Manual delineation of gliomas is time-consuming and prone to variability, motivating the use of deep learning to improve consistency and alleviate clinical workload. However, existing methods often fail to fully exploit the information available in multi-parametric MRI (mp-MRI), particularly inter-slice contextual features, and typically require considerable computational resources while lacking robustness across tumor type variations. We present GBT-SAM, a parameter-efficient deep learning framework that adapts the Segment Anything Model (SAM), a large-scale vision model, to volumetric mp-MRI data. GBT-SAM reduces input complexity by selecting fewer than 2.6\\% of slices per scan while incorporating all four MRI modalities, preserving essential tumor-related information with minimal cost. Furthermore, our model is trained by a two-step fine-tuning strategy that incorporates a depth-aware module to capture inter-slice correlations and lightweight adaptation layers, resulting in just 6.5M trainable parameters, which is the lowest among SAM-based approaches. GBT-SAM achieves a Dice Score of 93.54 on the BraTS Adult Glioma dataset and demonstrates robust performance on Meningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. These results highlight GBT-SAM's potential as a computationally efficient and domain-robust framework for brain tumor segmentation using mp-MRI. Our code and models are available at https://github.com/vpulab/med-sam-brain .", "link": "http://arxiv.org/abs/2503.04325v4", "date": "2025-12-01", "relevancy": 2.6474, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5553}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5169}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GBT-SAM%3A%20A%20Parameter-Efficient%20Depth-Aware%20Model%20for%20Generalizable%20Brain%20tumour%20Segmentation%20on%20mp-MRI&body=Title%3A%20GBT-SAM%3A%20A%20Parameter-Efficient%20Depth-Aware%20Model%20for%20Generalizable%20Brain%20tumour%20Segmentation%20on%20mp-MRI%0AAuthor%3A%20Cecilia%20Diana-Albelda%20and%20Roberto%20Alcover-Couso%20and%20%C3%81lvaro%20Garc%C3%ADa-Mart%C3%ADn%20and%20Jesus%20Bescos%20and%20Marcos%20Escudero-Vi%C3%B1olo%0AAbstract%3A%20Gliomas%20are%20aggressive%20brain%20tumors%20that%20require%20accurate%20imaging-based%20diagnosis%2C%20with%20segmentation%20playing%20a%20critical%20role%20in%20evaluating%20morphology%20and%20treatment%20decisions.%20Manual%20delineation%20of%20gliomas%20is%20time-consuming%20and%20prone%20to%20variability%2C%20motivating%20the%20use%20of%20deep%20learning%20to%20improve%20consistency%20and%20alleviate%20clinical%20workload.%20However%2C%20existing%20methods%20often%20fail%20to%20fully%20exploit%20the%20information%20available%20in%20multi-parametric%20MRI%20%28mp-MRI%29%2C%20particularly%20inter-slice%20contextual%20features%2C%20and%20typically%20require%20considerable%20computational%20resources%20while%20lacking%20robustness%20across%20tumor%20type%20variations.%20We%20present%20GBT-SAM%2C%20a%20parameter-efficient%20deep%20learning%20framework%20that%20adapts%20the%20Segment%20Anything%20Model%20%28SAM%29%2C%20a%20large-scale%20vision%20model%2C%20to%20volumetric%20mp-MRI%20data.%20GBT-SAM%20reduces%20input%20complexity%20by%20selecting%20fewer%20than%202.6%5C%25%20of%20slices%20per%20scan%20while%20incorporating%20all%20four%20MRI%20modalities%2C%20preserving%20essential%20tumor-related%20information%20with%20minimal%20cost.%20Furthermore%2C%20our%20model%20is%20trained%20by%20a%20two-step%20fine-tuning%20strategy%20that%20incorporates%20a%20depth-aware%20module%20to%20capture%20inter-slice%20correlations%20and%20lightweight%20adaptation%20layers%2C%20resulting%20in%20just%206.5M%20trainable%20parameters%2C%20which%20is%20the%20lowest%20among%20SAM-based%20approaches.%20GBT-SAM%20achieves%20a%20Dice%20Score%20of%2093.54%20on%20the%20BraTS%20Adult%20Glioma%20dataset%20and%20demonstrates%20robust%20performance%20on%20Meningioma%2C%20Pediatric%20Glioma%2C%20and%20Sub-Saharan%20Glioma%20datasets.%20These%20results%20highlight%20GBT-SAM%27s%20potential%20as%20a%20computationally%20efficient%20and%20domain-robust%20framework%20for%20brain%20tumor%20segmentation%20using%20mp-MRI.%20Our%20code%20and%20models%20are%20available%20at%20https%3A//github.com/vpulab/med-sam-brain%20.%0ALink%3A%20http%3A//arxiv.org/abs/2503.04325v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGBT-SAM%253A%2520A%2520Parameter-Efficient%2520Depth-Aware%2520Model%2520for%2520Generalizable%2520Brain%2520tumour%2520Segmentation%2520on%2520mp-MRI%26entry.906535625%3DCecilia%2520Diana-Albelda%2520and%2520Roberto%2520Alcover-Couso%2520and%2520%25C3%2581lvaro%2520Garc%25C3%25ADa-Mart%25C3%25ADn%2520and%2520Jesus%2520Bescos%2520and%2520Marcos%2520Escudero-Vi%25C3%25B1olo%26entry.1292438233%3DGliomas%2520are%2520aggressive%2520brain%2520tumors%2520that%2520require%2520accurate%2520imaging-based%2520diagnosis%252C%2520with%2520segmentation%2520playing%2520a%2520critical%2520role%2520in%2520evaluating%2520morphology%2520and%2520treatment%2520decisions.%2520Manual%2520delineation%2520of%2520gliomas%2520is%2520time-consuming%2520and%2520prone%2520to%2520variability%252C%2520motivating%2520the%2520use%2520of%2520deep%2520learning%2520to%2520improve%2520consistency%2520and%2520alleviate%2520clinical%2520workload.%2520However%252C%2520existing%2520methods%2520often%2520fail%2520to%2520fully%2520exploit%2520the%2520information%2520available%2520in%2520multi-parametric%2520MRI%2520%2528mp-MRI%2529%252C%2520particularly%2520inter-slice%2520contextual%2520features%252C%2520and%2520typically%2520require%2520considerable%2520computational%2520resources%2520while%2520lacking%2520robustness%2520across%2520tumor%2520type%2520variations.%2520We%2520present%2520GBT-SAM%252C%2520a%2520parameter-efficient%2520deep%2520learning%2520framework%2520that%2520adapts%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520a%2520large-scale%2520vision%2520model%252C%2520to%2520volumetric%2520mp-MRI%2520data.%2520GBT-SAM%2520reduces%2520input%2520complexity%2520by%2520selecting%2520fewer%2520than%25202.6%255C%2525%2520of%2520slices%2520per%2520scan%2520while%2520incorporating%2520all%2520four%2520MRI%2520modalities%252C%2520preserving%2520essential%2520tumor-related%2520information%2520with%2520minimal%2520cost.%2520Furthermore%252C%2520our%2520model%2520is%2520trained%2520by%2520a%2520two-step%2520fine-tuning%2520strategy%2520that%2520incorporates%2520a%2520depth-aware%2520module%2520to%2520capture%2520inter-slice%2520correlations%2520and%2520lightweight%2520adaptation%2520layers%252C%2520resulting%2520in%2520just%25206.5M%2520trainable%2520parameters%252C%2520which%2520is%2520the%2520lowest%2520among%2520SAM-based%2520approaches.%2520GBT-SAM%2520achieves%2520a%2520Dice%2520Score%2520of%252093.54%2520on%2520the%2520BraTS%2520Adult%2520Glioma%2520dataset%2520and%2520demonstrates%2520robust%2520performance%2520on%2520Meningioma%252C%2520Pediatric%2520Glioma%252C%2520and%2520Sub-Saharan%2520Glioma%2520datasets.%2520These%2520results%2520highlight%2520GBT-SAM%2527s%2520potential%2520as%2520a%2520computationally%2520efficient%2520and%2520domain-robust%2520framework%2520for%2520brain%2520tumor%2520segmentation%2520using%2520mp-MRI.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/vpulab/med-sam-brain%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04325v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GBT-SAM%3A%20A%20Parameter-Efficient%20Depth-Aware%20Model%20for%20Generalizable%20Brain%20tumour%20Segmentation%20on%20mp-MRI&entry.906535625=Cecilia%20Diana-Albelda%20and%20Roberto%20Alcover-Couso%20and%20%C3%81lvaro%20Garc%C3%ADa-Mart%C3%ADn%20and%20Jesus%20Bescos%20and%20Marcos%20Escudero-Vi%C3%B1olo&entry.1292438233=Gliomas%20are%20aggressive%20brain%20tumors%20that%20require%20accurate%20imaging-based%20diagnosis%2C%20with%20segmentation%20playing%20a%20critical%20role%20in%20evaluating%20morphology%20and%20treatment%20decisions.%20Manual%20delineation%20of%20gliomas%20is%20time-consuming%20and%20prone%20to%20variability%2C%20motivating%20the%20use%20of%20deep%20learning%20to%20improve%20consistency%20and%20alleviate%20clinical%20workload.%20However%2C%20existing%20methods%20often%20fail%20to%20fully%20exploit%20the%20information%20available%20in%20multi-parametric%20MRI%20%28mp-MRI%29%2C%20particularly%20inter-slice%20contextual%20features%2C%20and%20typically%20require%20considerable%20computational%20resources%20while%20lacking%20robustness%20across%20tumor%20type%20variations.%20We%20present%20GBT-SAM%2C%20a%20parameter-efficient%20deep%20learning%20framework%20that%20adapts%20the%20Segment%20Anything%20Model%20%28SAM%29%2C%20a%20large-scale%20vision%20model%2C%20to%20volumetric%20mp-MRI%20data.%20GBT-SAM%20reduces%20input%20complexity%20by%20selecting%20fewer%20than%202.6%5C%25%20of%20slices%20per%20scan%20while%20incorporating%20all%20four%20MRI%20modalities%2C%20preserving%20essential%20tumor-related%20information%20with%20minimal%20cost.%20Furthermore%2C%20our%20model%20is%20trained%20by%20a%20two-step%20fine-tuning%20strategy%20that%20incorporates%20a%20depth-aware%20module%20to%20capture%20inter-slice%20correlations%20and%20lightweight%20adaptation%20layers%2C%20resulting%20in%20just%206.5M%20trainable%20parameters%2C%20which%20is%20the%20lowest%20among%20SAM-based%20approaches.%20GBT-SAM%20achieves%20a%20Dice%20Score%20of%2093.54%20on%20the%20BraTS%20Adult%20Glioma%20dataset%20and%20demonstrates%20robust%20performance%20on%20Meningioma%2C%20Pediatric%20Glioma%2C%20and%20Sub-Saharan%20Glioma%20datasets.%20These%20results%20highlight%20GBT-SAM%27s%20potential%20as%20a%20computationally%20efficient%20and%20domain-robust%20framework%20for%20brain%20tumor%20segmentation%20using%20mp-MRI.%20Our%20code%20and%20models%20are%20available%20at%20https%3A//github.com/vpulab/med-sam-brain%20.&entry.1838667208=http%3A//arxiv.org/abs/2503.04325v4&entry.124074799=Read"},
{"title": "Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation", "author": "Franz Thaler and Martin Urschler and Mateusz Kozinski and Matthias AF Gsell and Gernot Plank and Darko Stern", "abstract": "We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation. To this end, we aim for training a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training. We propose a novel method for promoting DG when training deep segmentation networks, which we call SRCSM. During training, our method diversifies the source domain through semantic-aware random convolution, where different regions of a source image are augmented differently, based on their annotation labels. At test-time, we complement the randomization of the training domain via mapping the intensity of target domain images, making them similar to source domain data. We perform a comprehensive evaluation on a variety of cross-modality and cross-center generalization settings for abdominal, whole-heart and prostate segmentation, where we outperform previous DG techniques in a vast majority of experiments. Additionally, we also investigate our method when training on whole-heart CT or MR data and testing on the diastolic and systolic phase of cine MR data captured with different scanner hardware, where we make a step towards closing the domain gap in this even more challenging setting. Overall, our evaluation shows that SRCSM can be considered a new state-of-the-art in DG for medical image segmentation and, moreover, even achieves a segmentation performance that matches the performance of the in-domain baseline in several settings.", "link": "http://arxiv.org/abs/2512.01510v1", "date": "2025-12-01", "relevancy": 2.6115, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5296}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5188}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic-aware%20Random%20Convolution%20and%20Source%20Matching%20for%20Domain%20Generalization%20in%20Medical%20Image%20Segmentation&body=Title%3A%20Semantic-aware%20Random%20Convolution%20and%20Source%20Matching%20for%20Domain%20Generalization%20in%20Medical%20Image%20Segmentation%0AAuthor%3A%20Franz%20Thaler%20and%20Martin%20Urschler%20and%20Mateusz%20Kozinski%20and%20Matthias%20AF%20Gsell%20and%20Gernot%20Plank%20and%20Darko%20Stern%0AAbstract%3A%20We%20tackle%20the%20challenging%20problem%20of%20single-source%20domain%20generalization%20%28DG%29%20for%20medical%20image%20segmentation.%20To%20this%20end%2C%20we%20aim%20for%20training%20a%20network%20on%20one%20domain%20%28e.g.%2C%20CT%29%20and%20directly%20apply%20it%20to%20a%20different%20domain%20%28e.g.%2C%20MR%29%20without%20adapting%20the%20model%20and%20without%20requiring%20images%20or%20annotations%20from%20the%20new%20domain%20during%20training.%20We%20propose%20a%20novel%20method%20for%20promoting%20DG%20when%20training%20deep%20segmentation%20networks%2C%20which%20we%20call%20SRCSM.%20During%20training%2C%20our%20method%20diversifies%20the%20source%20domain%20through%20semantic-aware%20random%20convolution%2C%20where%20different%20regions%20of%20a%20source%20image%20are%20augmented%20differently%2C%20based%20on%20their%20annotation%20labels.%20At%20test-time%2C%20we%20complement%20the%20randomization%20of%20the%20training%20domain%20via%20mapping%20the%20intensity%20of%20target%20domain%20images%2C%20making%20them%20similar%20to%20source%20domain%20data.%20We%20perform%20a%20comprehensive%20evaluation%20on%20a%20variety%20of%20cross-modality%20and%20cross-center%20generalization%20settings%20for%20abdominal%2C%20whole-heart%20and%20prostate%20segmentation%2C%20where%20we%20outperform%20previous%20DG%20techniques%20in%20a%20vast%20majority%20of%20experiments.%20Additionally%2C%20we%20also%20investigate%20our%20method%20when%20training%20on%20whole-heart%20CT%20or%20MR%20data%20and%20testing%20on%20the%20diastolic%20and%20systolic%20phase%20of%20cine%20MR%20data%20captured%20with%20different%20scanner%20hardware%2C%20where%20we%20make%20a%20step%20towards%20closing%20the%20domain%20gap%20in%20this%20even%20more%20challenging%20setting.%20Overall%2C%20our%20evaluation%20shows%20that%20SRCSM%20can%20be%20considered%20a%20new%20state-of-the-art%20in%20DG%20for%20medical%20image%20segmentation%20and%2C%20moreover%2C%20even%20achieves%20a%20segmentation%20performance%20that%20matches%20the%20performance%20of%20the%20in-domain%20baseline%20in%20several%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic-aware%2520Random%2520Convolution%2520and%2520Source%2520Matching%2520for%2520Domain%2520Generalization%2520in%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DFranz%2520Thaler%2520and%2520Martin%2520Urschler%2520and%2520Mateusz%2520Kozinski%2520and%2520Matthias%2520AF%2520Gsell%2520and%2520Gernot%2520Plank%2520and%2520Darko%2520Stern%26entry.1292438233%3DWe%2520tackle%2520the%2520challenging%2520problem%2520of%2520single-source%2520domain%2520generalization%2520%2528DG%2529%2520for%2520medical%2520image%2520segmentation.%2520To%2520this%2520end%252C%2520we%2520aim%2520for%2520training%2520a%2520network%2520on%2520one%2520domain%2520%2528e.g.%252C%2520CT%2529%2520and%2520directly%2520apply%2520it%2520to%2520a%2520different%2520domain%2520%2528e.g.%252C%2520MR%2529%2520without%2520adapting%2520the%2520model%2520and%2520without%2520requiring%2520images%2520or%2520annotations%2520from%2520the%2520new%2520domain%2520during%2520training.%2520We%2520propose%2520a%2520novel%2520method%2520for%2520promoting%2520DG%2520when%2520training%2520deep%2520segmentation%2520networks%252C%2520which%2520we%2520call%2520SRCSM.%2520During%2520training%252C%2520our%2520method%2520diversifies%2520the%2520source%2520domain%2520through%2520semantic-aware%2520random%2520convolution%252C%2520where%2520different%2520regions%2520of%2520a%2520source%2520image%2520are%2520augmented%2520differently%252C%2520based%2520on%2520their%2520annotation%2520labels.%2520At%2520test-time%252C%2520we%2520complement%2520the%2520randomization%2520of%2520the%2520training%2520domain%2520via%2520mapping%2520the%2520intensity%2520of%2520target%2520domain%2520images%252C%2520making%2520them%2520similar%2520to%2520source%2520domain%2520data.%2520We%2520perform%2520a%2520comprehensive%2520evaluation%2520on%2520a%2520variety%2520of%2520cross-modality%2520and%2520cross-center%2520generalization%2520settings%2520for%2520abdominal%252C%2520whole-heart%2520and%2520prostate%2520segmentation%252C%2520where%2520we%2520outperform%2520previous%2520DG%2520techniques%2520in%2520a%2520vast%2520majority%2520of%2520experiments.%2520Additionally%252C%2520we%2520also%2520investigate%2520our%2520method%2520when%2520training%2520on%2520whole-heart%2520CT%2520or%2520MR%2520data%2520and%2520testing%2520on%2520the%2520diastolic%2520and%2520systolic%2520phase%2520of%2520cine%2520MR%2520data%2520captured%2520with%2520different%2520scanner%2520hardware%252C%2520where%2520we%2520make%2520a%2520step%2520towards%2520closing%2520the%2520domain%2520gap%2520in%2520this%2520even%2520more%2520challenging%2520setting.%2520Overall%252C%2520our%2520evaluation%2520shows%2520that%2520SRCSM%2520can%2520be%2520considered%2520a%2520new%2520state-of-the-art%2520in%2520DG%2520for%2520medical%2520image%2520segmentation%2520and%252C%2520moreover%252C%2520even%2520achieves%2520a%2520segmentation%2520performance%2520that%2520matches%2520the%2520performance%2520of%2520the%2520in-domain%2520baseline%2520in%2520several%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-aware%20Random%20Convolution%20and%20Source%20Matching%20for%20Domain%20Generalization%20in%20Medical%20Image%20Segmentation&entry.906535625=Franz%20Thaler%20and%20Martin%20Urschler%20and%20Mateusz%20Kozinski%20and%20Matthias%20AF%20Gsell%20and%20Gernot%20Plank%20and%20Darko%20Stern&entry.1292438233=We%20tackle%20the%20challenging%20problem%20of%20single-source%20domain%20generalization%20%28DG%29%20for%20medical%20image%20segmentation.%20To%20this%20end%2C%20we%20aim%20for%20training%20a%20network%20on%20one%20domain%20%28e.g.%2C%20CT%29%20and%20directly%20apply%20it%20to%20a%20different%20domain%20%28e.g.%2C%20MR%29%20without%20adapting%20the%20model%20and%20without%20requiring%20images%20or%20annotations%20from%20the%20new%20domain%20during%20training.%20We%20propose%20a%20novel%20method%20for%20promoting%20DG%20when%20training%20deep%20segmentation%20networks%2C%20which%20we%20call%20SRCSM.%20During%20training%2C%20our%20method%20diversifies%20the%20source%20domain%20through%20semantic-aware%20random%20convolution%2C%20where%20different%20regions%20of%20a%20source%20image%20are%20augmented%20differently%2C%20based%20on%20their%20annotation%20labels.%20At%20test-time%2C%20we%20complement%20the%20randomization%20of%20the%20training%20domain%20via%20mapping%20the%20intensity%20of%20target%20domain%20images%2C%20making%20them%20similar%20to%20source%20domain%20data.%20We%20perform%20a%20comprehensive%20evaluation%20on%20a%20variety%20of%20cross-modality%20and%20cross-center%20generalization%20settings%20for%20abdominal%2C%20whole-heart%20and%20prostate%20segmentation%2C%20where%20we%20outperform%20previous%20DG%20techniques%20in%20a%20vast%20majority%20of%20experiments.%20Additionally%2C%20we%20also%20investigate%20our%20method%20when%20training%20on%20whole-heart%20CT%20or%20MR%20data%20and%20testing%20on%20the%20diastolic%20and%20systolic%20phase%20of%20cine%20MR%20data%20captured%20with%20different%20scanner%20hardware%2C%20where%20we%20make%20a%20step%20towards%20closing%20the%20domain%20gap%20in%20this%20even%20more%20challenging%20setting.%20Overall%2C%20our%20evaluation%20shows%20that%20SRCSM%20can%20be%20considered%20a%20new%20state-of-the-art%20in%20DG%20for%20medical%20image%20segmentation%20and%2C%20moreover%2C%20even%20achieves%20a%20segmentation%20performance%20that%20matches%20the%20performance%20of%20the%20in-domain%20baseline%20in%20several%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.01510v1&entry.124074799=Read"},
{"title": "Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models", "author": "Zhongyu Yang and Dannong Xu and Wei Pang and Yingfang Yuan", "abstract": "The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.", "link": "http://arxiv.org/abs/2512.01949v1", "date": "2025-12-01", "relevancy": 2.595, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Script%3A%20Graph-Structured%20and%20Query-Conditioned%20Semantic%20Token%20Pruning%20for%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Script%3A%20Graph-Structured%20and%20Query-Conditioned%20Semantic%20Token%20Pruning%20for%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Zhongyu%20Yang%20and%20Dannong%20Xu%20and%20Wei%20Pang%20and%20Yingfang%20Yuan%0AAbstract%3A%20The%20rapid%20growth%20of%20visual%20tokens%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20leads%20to%20excessive%20memory%20consumption%20and%20inference%20latency%2C%20especially%20when%20handling%20high-resolution%20images%20and%20videos.%20Token%20pruning%20is%20a%20technique%20used%20to%20mitigate%20this%20issue%20by%20removing%20redundancy%2C%20but%20existing%20methods%20often%20ignore%20relevance%20to%20the%20user%20query%20or%20suffer%20from%20the%20limitations%20of%20attention%20mechanisms%2C%20reducing%20their%20adaptability%20and%20effectiveness.%20To%20address%20these%20challenges%2C%20we%20propose%20Script%2C%20a%20plug-and-play%20pruning%20method%20that%20requires%20no%20retraining%20and%20generalizes%20across%20diverse%20MLLMs.%20Script%20comprises%20two%20modules%3A%20a%20graph-structured%20pruning%20module%20that%20removes%20visually%20redundant%20tokens%2C%20and%20a%20query-conditioned%20semantic%20pruning%20module%20that%20preserves%20query-relevant%20visual%20information.%20Together%2C%20they%20enhance%20performance%20on%20multimodal%20tasks.%20Experiments%20on%20fourteen%20benchmarks%20across%20image%20and%20video%20understanding%20tasks%20show%20that%20Script%20consistently%20achieves%20higher%20model%20efficiency%20and%20predictive%20accuracy%20compared%20to%20existing%20pruning%20methods.%20On%20LLaVA-NeXT-7B%2C%20it%20achieves%20up%20to%206.8x%20prefill%20speedup%20and%2010x%20FLOP%20reduction%2C%20while%20retaining%2096.88%25%20of%20the%20original%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScript%253A%2520Graph-Structured%2520and%2520Query-Conditioned%2520Semantic%2520Token%2520Pruning%2520for%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DZhongyu%2520Yang%2520and%2520Dannong%2520Xu%2520and%2520Wei%2520Pang%2520and%2520Yingfang%2520Yuan%26entry.1292438233%3DThe%2520rapid%2520growth%2520of%2520visual%2520tokens%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520leads%2520to%2520excessive%2520memory%2520consumption%2520and%2520inference%2520latency%252C%2520especially%2520when%2520handling%2520high-resolution%2520images%2520and%2520videos.%2520Token%2520pruning%2520is%2520a%2520technique%2520used%2520to%2520mitigate%2520this%2520issue%2520by%2520removing%2520redundancy%252C%2520but%2520existing%2520methods%2520often%2520ignore%2520relevance%2520to%2520the%2520user%2520query%2520or%2520suffer%2520from%2520the%2520limitations%2520of%2520attention%2520mechanisms%252C%2520reducing%2520their%2520adaptability%2520and%2520effectiveness.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Script%252C%2520a%2520plug-and-play%2520pruning%2520method%2520that%2520requires%2520no%2520retraining%2520and%2520generalizes%2520across%2520diverse%2520MLLMs.%2520Script%2520comprises%2520two%2520modules%253A%2520a%2520graph-structured%2520pruning%2520module%2520that%2520removes%2520visually%2520redundant%2520tokens%252C%2520and%2520a%2520query-conditioned%2520semantic%2520pruning%2520module%2520that%2520preserves%2520query-relevant%2520visual%2520information.%2520Together%252C%2520they%2520enhance%2520performance%2520on%2520multimodal%2520tasks.%2520Experiments%2520on%2520fourteen%2520benchmarks%2520across%2520image%2520and%2520video%2520understanding%2520tasks%2520show%2520that%2520Script%2520consistently%2520achieves%2520higher%2520model%2520efficiency%2520and%2520predictive%2520accuracy%2520compared%2520to%2520existing%2520pruning%2520methods.%2520On%2520LLaVA-NeXT-7B%252C%2520it%2520achieves%2520up%2520to%25206.8x%2520prefill%2520speedup%2520and%252010x%2520FLOP%2520reduction%252C%2520while%2520retaining%252096.88%2525%2520of%2520the%2520original%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Script%3A%20Graph-Structured%20and%20Query-Conditioned%20Semantic%20Token%20Pruning%20for%20Multimodal%20Large%20Language%20Models&entry.906535625=Zhongyu%20Yang%20and%20Dannong%20Xu%20and%20Wei%20Pang%20and%20Yingfang%20Yuan&entry.1292438233=The%20rapid%20growth%20of%20visual%20tokens%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20leads%20to%20excessive%20memory%20consumption%20and%20inference%20latency%2C%20especially%20when%20handling%20high-resolution%20images%20and%20videos.%20Token%20pruning%20is%20a%20technique%20used%20to%20mitigate%20this%20issue%20by%20removing%20redundancy%2C%20but%20existing%20methods%20often%20ignore%20relevance%20to%20the%20user%20query%20or%20suffer%20from%20the%20limitations%20of%20attention%20mechanisms%2C%20reducing%20their%20adaptability%20and%20effectiveness.%20To%20address%20these%20challenges%2C%20we%20propose%20Script%2C%20a%20plug-and-play%20pruning%20method%20that%20requires%20no%20retraining%20and%20generalizes%20across%20diverse%20MLLMs.%20Script%20comprises%20two%20modules%3A%20a%20graph-structured%20pruning%20module%20that%20removes%20visually%20redundant%20tokens%2C%20and%20a%20query-conditioned%20semantic%20pruning%20module%20that%20preserves%20query-relevant%20visual%20information.%20Together%2C%20they%20enhance%20performance%20on%20multimodal%20tasks.%20Experiments%20on%20fourteen%20benchmarks%20across%20image%20and%20video%20understanding%20tasks%20show%20that%20Script%20consistently%20achieves%20higher%20model%20efficiency%20and%20predictive%20accuracy%20compared%20to%20existing%20pruning%20methods.%20On%20LLaVA-NeXT-7B%2C%20it%20achieves%20up%20to%206.8x%20prefill%20speedup%20and%2010x%20FLOP%20reduction%2C%20while%20retaining%2096.88%25%20of%20the%20original%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.01949v1&entry.124074799=Read"},
{"title": "Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback", "author": "Aiden Yiliu Li and Bizhi Yu and Daoan Lei and Tianhe Ren and Shilong Liu", "abstract": "GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.", "link": "http://arxiv.org/abs/2512.01979v1", "date": "2025-12-01", "relevancy": 2.5847, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain-of-Ground%3A%20Improving%20GUI%20Grounding%20via%20Iterative%20Reasoning%20and%20Reference%20Feedback&body=Title%3A%20Chain-of-Ground%3A%20Improving%20GUI%20Grounding%20via%20Iterative%20Reasoning%20and%20Reference%20Feedback%0AAuthor%3A%20Aiden%20Yiliu%20Li%20and%20Bizhi%20Yu%20and%20Daoan%20Lei%20and%20Tianhe%20Ren%20and%20Shilong%20Liu%0AAbstract%3A%20GUI%20grounding%20aims%20to%20align%20natural%20language%20instructions%20with%20precise%20regions%20in%20complex%20user%20interfaces.%20Advanced%20multimodal%20large%20language%20models%20show%20strong%20ability%20in%20visual%20GUI%20grounding%20but%20still%20struggle%20with%20small%20or%20visually%20similar%20targets%20and%20ambiguity%20in%20real%20world%20layouts.%20These%20limitations%20arise%20from%20limited%20grounding%20capacity%20and%20from%20underuse%20of%20existing%20reasoning%20potential.%20We%20present%20Chain%20of%20Ground%20CoG%20a%20training%20free%20multi%20step%20grounding%20framework%20that%20uses%20multimodal%20large%20language%20models%20for%20iterative%20visual%20reasoning%20and%20refinement.%20Instead%20of%20direct%20prediction%20the%20model%20progressively%20reflects%20and%20adjusts%20its%20hypotheses%20leading%20to%20more%20accurate%20and%20interpretable%20localization.%20Our%20approach%20achieves%2068.4%20accuracy%20on%20the%20ScreenSpot%20Pro%20benchmark%20an%20improvement%20of%204.8%20points.%20To%20measure%20real%20world%20generalization%20we%20introduce%20TPanel%20UI%20a%20dataset%20of%20420%20labeled%20industrial%20control%20panels%20with%20visual%20distortions%20such%20as%20blur%20and%20masking.%20On%20TPanel%20UI%20Chain%20of%20Ground%20improves%20over%20the%20strong%20baseline%20Qwen3%20VL%20235B%20by%206.9%20points%20showing%20the%20effectiveness%20of%20multi%20step%20training%20free%20grounding%20across%20real%20world%20and%20digital%20interfaces.%20These%20results%20highlight%20a%20direction%20for%20unlocking%20grounding%20potential%20through%20structured%20iterative%20refinement%20instead%20of%20additional%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain-of-Ground%253A%2520Improving%2520GUI%2520Grounding%2520via%2520Iterative%2520Reasoning%2520and%2520Reference%2520Feedback%26entry.906535625%3DAiden%2520Yiliu%2520Li%2520and%2520Bizhi%2520Yu%2520and%2520Daoan%2520Lei%2520and%2520Tianhe%2520Ren%2520and%2520Shilong%2520Liu%26entry.1292438233%3DGUI%2520grounding%2520aims%2520to%2520align%2520natural%2520language%2520instructions%2520with%2520precise%2520regions%2520in%2520complex%2520user%2520interfaces.%2520Advanced%2520multimodal%2520large%2520language%2520models%2520show%2520strong%2520ability%2520in%2520visual%2520GUI%2520grounding%2520but%2520still%2520struggle%2520with%2520small%2520or%2520visually%2520similar%2520targets%2520and%2520ambiguity%2520in%2520real%2520world%2520layouts.%2520These%2520limitations%2520arise%2520from%2520limited%2520grounding%2520capacity%2520and%2520from%2520underuse%2520of%2520existing%2520reasoning%2520potential.%2520We%2520present%2520Chain%2520of%2520Ground%2520CoG%2520a%2520training%2520free%2520multi%2520step%2520grounding%2520framework%2520that%2520uses%2520multimodal%2520large%2520language%2520models%2520for%2520iterative%2520visual%2520reasoning%2520and%2520refinement.%2520Instead%2520of%2520direct%2520prediction%2520the%2520model%2520progressively%2520reflects%2520and%2520adjusts%2520its%2520hypotheses%2520leading%2520to%2520more%2520accurate%2520and%2520interpretable%2520localization.%2520Our%2520approach%2520achieves%252068.4%2520accuracy%2520on%2520the%2520ScreenSpot%2520Pro%2520benchmark%2520an%2520improvement%2520of%25204.8%2520points.%2520To%2520measure%2520real%2520world%2520generalization%2520we%2520introduce%2520TPanel%2520UI%2520a%2520dataset%2520of%2520420%2520labeled%2520industrial%2520control%2520panels%2520with%2520visual%2520distortions%2520such%2520as%2520blur%2520and%2520masking.%2520On%2520TPanel%2520UI%2520Chain%2520of%2520Ground%2520improves%2520over%2520the%2520strong%2520baseline%2520Qwen3%2520VL%2520235B%2520by%25206.9%2520points%2520showing%2520the%2520effectiveness%2520of%2520multi%2520step%2520training%2520free%2520grounding%2520across%2520real%2520world%2520and%2520digital%2520interfaces.%2520These%2520results%2520highlight%2520a%2520direction%2520for%2520unlocking%2520grounding%2520potential%2520through%2520structured%2520iterative%2520refinement%2520instead%2520of%2520additional%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain-of-Ground%3A%20Improving%20GUI%20Grounding%20via%20Iterative%20Reasoning%20and%20Reference%20Feedback&entry.906535625=Aiden%20Yiliu%20Li%20and%20Bizhi%20Yu%20and%20Daoan%20Lei%20and%20Tianhe%20Ren%20and%20Shilong%20Liu&entry.1292438233=GUI%20grounding%20aims%20to%20align%20natural%20language%20instructions%20with%20precise%20regions%20in%20complex%20user%20interfaces.%20Advanced%20multimodal%20large%20language%20models%20show%20strong%20ability%20in%20visual%20GUI%20grounding%20but%20still%20struggle%20with%20small%20or%20visually%20similar%20targets%20and%20ambiguity%20in%20real%20world%20layouts.%20These%20limitations%20arise%20from%20limited%20grounding%20capacity%20and%20from%20underuse%20of%20existing%20reasoning%20potential.%20We%20present%20Chain%20of%20Ground%20CoG%20a%20training%20free%20multi%20step%20grounding%20framework%20that%20uses%20multimodal%20large%20language%20models%20for%20iterative%20visual%20reasoning%20and%20refinement.%20Instead%20of%20direct%20prediction%20the%20model%20progressively%20reflects%20and%20adjusts%20its%20hypotheses%20leading%20to%20more%20accurate%20and%20interpretable%20localization.%20Our%20approach%20achieves%2068.4%20accuracy%20on%20the%20ScreenSpot%20Pro%20benchmark%20an%20improvement%20of%204.8%20points.%20To%20measure%20real%20world%20generalization%20we%20introduce%20TPanel%20UI%20a%20dataset%20of%20420%20labeled%20industrial%20control%20panels%20with%20visual%20distortions%20such%20as%20blur%20and%20masking.%20On%20TPanel%20UI%20Chain%20of%20Ground%20improves%20over%20the%20strong%20baseline%20Qwen3%20VL%20235B%20by%206.9%20points%20showing%20the%20effectiveness%20of%20multi%20step%20training%20free%20grounding%20across%20real%20world%20and%20digital%20interfaces.%20These%20results%20highlight%20a%20direction%20for%20unlocking%20grounding%20potential%20through%20structured%20iterative%20refinement%20instead%20of%20additional%20training.&entry.1838667208=http%3A//arxiv.org/abs/2512.01979v1&entry.124074799=Read"},
{"title": "Morphling: Fast, Fused, and Flexible GNN Training at Scale", "author": " Anubhab and Rupesh Nasre", "abstract": "Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. The results show that Morphling improves per-epoch training throughput by an average of 20X on CPUs and 19X on GPUs over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.", "link": "http://arxiv.org/abs/2512.01678v1", "date": "2025-12-01", "relevancy": 2.5724, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5477}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5017}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Morphling%3A%20Fast%2C%20Fused%2C%20and%20Flexible%20GNN%20Training%20at%20Scale&body=Title%3A%20Morphling%3A%20Fast%2C%20Fused%2C%20and%20Flexible%20GNN%20Training%20at%20Scale%0AAuthor%3A%20%20Anubhab%20and%20Rupesh%20Nasre%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20present%20a%20fundamental%20hardware%20challenge%20by%20fusing%20irregular%2C%20memory-bound%20graph%20traversals%20with%20regular%2C%20compute-intensive%20dense%20matrix%20operations.%20While%20frameworks%20such%20as%20PyTorch%20Geometric%20%28PyG%29%20and%20Deep%20Graph%20Library%20%28DGL%29%20prioritize%20high-level%20usability%2C%20they%20fail%20to%20address%20these%20divergent%20execution%20characteristics.%20As%20a%20result%2C%20they%20rely%20on%20generic%20kernels%20that%20suffer%20from%20poor%20cache%20locality%2C%20excessive%20memory%20movement%2C%20and%20substantial%20intermediate%20allocations.%20To%20address%20these%20limitations%2C%20we%20present%20Morphling%2C%20a%20domain-specific%20code%20synthesizer%20designed%20to%20bridge%20this%20gap.%20Morphling%20compiles%20high-level%20GNN%20specifications%20into%20portable%2C%20backend-specialized%20implementations%20targeting%20OpenMP%2C%20CUDA%2C%20and%20MPI.%20It%20achieves%20this%20by%20instantiating%20a%20library%20of%20optimized%2C%20architecture-aware%20primitives%20tailored%20to%20each%20execution%20environment.%20Morphling%20also%20incorporates%20a%20runtime%20sparsity-aware%20execution%20engine%20that%20dynamically%20selects%20dense%20or%20sparse%20execution%20paths%20using%20input%20feature%20statistics%2C%20reducing%20unnecessary%20computation%20on%20zero-valued%20entries.%20We%20evaluate%20Morphling%20on%20eleven%20real-world%20datasets%20spanning%20diverse%20graph%20structures%2C%20feature%20dimensionalities%2C%20and%20sparsity%20regimes.%20The%20results%20show%20that%20Morphling%20improves%20per-epoch%20training%20throughput%20by%20an%20average%20of%2020X%20on%20CPUs%20and%2019X%20on%20GPUs%20over%20PyG%20and%20DGL%2C%20with%20peak%20speedups%20reaching%2066X.%20Morphling%27s%20memory-efficient%20layouts%20further%20reduce%20peak%20memory%20consumption%20by%20up%20to%2015X%2C%20enabling%20large-scale%20GNN%20training%20on%20commodity%20hardware.%20These%20findings%20demonstrate%20that%20specialized%2C%20architecture-aware%20code%20synthesis%20provides%20an%20effective%20and%20scalable%20path%20toward%20high-performance%20GNN%20execution%20across%20diverse%20parallel%20and%20distributed%20platforms.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphling%253A%2520Fast%252C%2520Fused%252C%2520and%2520Flexible%2520GNN%2520Training%2520at%2520Scale%26entry.906535625%3D%2520Anubhab%2520and%2520Rupesh%2520Nasre%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520present%2520a%2520fundamental%2520hardware%2520challenge%2520by%2520fusing%2520irregular%252C%2520memory-bound%2520graph%2520traversals%2520with%2520regular%252C%2520compute-intensive%2520dense%2520matrix%2520operations.%2520While%2520frameworks%2520such%2520as%2520PyTorch%2520Geometric%2520%2528PyG%2529%2520and%2520Deep%2520Graph%2520Library%2520%2528DGL%2529%2520prioritize%2520high-level%2520usability%252C%2520they%2520fail%2520to%2520address%2520these%2520divergent%2520execution%2520characteristics.%2520As%2520a%2520result%252C%2520they%2520rely%2520on%2520generic%2520kernels%2520that%2520suffer%2520from%2520poor%2520cache%2520locality%252C%2520excessive%2520memory%2520movement%252C%2520and%2520substantial%2520intermediate%2520allocations.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520Morphling%252C%2520a%2520domain-specific%2520code%2520synthesizer%2520designed%2520to%2520bridge%2520this%2520gap.%2520Morphling%2520compiles%2520high-level%2520GNN%2520specifications%2520into%2520portable%252C%2520backend-specialized%2520implementations%2520targeting%2520OpenMP%252C%2520CUDA%252C%2520and%2520MPI.%2520It%2520achieves%2520this%2520by%2520instantiating%2520a%2520library%2520of%2520optimized%252C%2520architecture-aware%2520primitives%2520tailored%2520to%2520each%2520execution%2520environment.%2520Morphling%2520also%2520incorporates%2520a%2520runtime%2520sparsity-aware%2520execution%2520engine%2520that%2520dynamically%2520selects%2520dense%2520or%2520sparse%2520execution%2520paths%2520using%2520input%2520feature%2520statistics%252C%2520reducing%2520unnecessary%2520computation%2520on%2520zero-valued%2520entries.%2520We%2520evaluate%2520Morphling%2520on%2520eleven%2520real-world%2520datasets%2520spanning%2520diverse%2520graph%2520structures%252C%2520feature%2520dimensionalities%252C%2520and%2520sparsity%2520regimes.%2520The%2520results%2520show%2520that%2520Morphling%2520improves%2520per-epoch%2520training%2520throughput%2520by%2520an%2520average%2520of%252020X%2520on%2520CPUs%2520and%252019X%2520on%2520GPUs%2520over%2520PyG%2520and%2520DGL%252C%2520with%2520peak%2520speedups%2520reaching%252066X.%2520Morphling%2527s%2520memory-efficient%2520layouts%2520further%2520reduce%2520peak%2520memory%2520consumption%2520by%2520up%2520to%252015X%252C%2520enabling%2520large-scale%2520GNN%2520training%2520on%2520commodity%2520hardware.%2520These%2520findings%2520demonstrate%2520that%2520specialized%252C%2520architecture-aware%2520code%2520synthesis%2520provides%2520an%2520effective%2520and%2520scalable%2520path%2520toward%2520high-performance%2520GNN%2520execution%2520across%2520diverse%2520parallel%2520and%2520distributed%2520platforms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morphling%3A%20Fast%2C%20Fused%2C%20and%20Flexible%20GNN%20Training%20at%20Scale&entry.906535625=%20Anubhab%20and%20Rupesh%20Nasre&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20present%20a%20fundamental%20hardware%20challenge%20by%20fusing%20irregular%2C%20memory-bound%20graph%20traversals%20with%20regular%2C%20compute-intensive%20dense%20matrix%20operations.%20While%20frameworks%20such%20as%20PyTorch%20Geometric%20%28PyG%29%20and%20Deep%20Graph%20Library%20%28DGL%29%20prioritize%20high-level%20usability%2C%20they%20fail%20to%20address%20these%20divergent%20execution%20characteristics.%20As%20a%20result%2C%20they%20rely%20on%20generic%20kernels%20that%20suffer%20from%20poor%20cache%20locality%2C%20excessive%20memory%20movement%2C%20and%20substantial%20intermediate%20allocations.%20To%20address%20these%20limitations%2C%20we%20present%20Morphling%2C%20a%20domain-specific%20code%20synthesizer%20designed%20to%20bridge%20this%20gap.%20Morphling%20compiles%20high-level%20GNN%20specifications%20into%20portable%2C%20backend-specialized%20implementations%20targeting%20OpenMP%2C%20CUDA%2C%20and%20MPI.%20It%20achieves%20this%20by%20instantiating%20a%20library%20of%20optimized%2C%20architecture-aware%20primitives%20tailored%20to%20each%20execution%20environment.%20Morphling%20also%20incorporates%20a%20runtime%20sparsity-aware%20execution%20engine%20that%20dynamically%20selects%20dense%20or%20sparse%20execution%20paths%20using%20input%20feature%20statistics%2C%20reducing%20unnecessary%20computation%20on%20zero-valued%20entries.%20We%20evaluate%20Morphling%20on%20eleven%20real-world%20datasets%20spanning%20diverse%20graph%20structures%2C%20feature%20dimensionalities%2C%20and%20sparsity%20regimes.%20The%20results%20show%20that%20Morphling%20improves%20per-epoch%20training%20throughput%20by%20an%20average%20of%2020X%20on%20CPUs%20and%2019X%20on%20GPUs%20over%20PyG%20and%20DGL%2C%20with%20peak%20speedups%20reaching%2066X.%20Morphling%27s%20memory-efficient%20layouts%20further%20reduce%20peak%20memory%20consumption%20by%20up%20to%2015X%2C%20enabling%20large-scale%20GNN%20training%20on%20commodity%20hardware.%20These%20findings%20demonstrate%20that%20specialized%2C%20architecture-aware%20code%20synthesis%20provides%20an%20effective%20and%20scalable%20path%20toward%20high-performance%20GNN%20execution%20across%20diverse%20parallel%20and%20distributed%20platforms.&entry.1838667208=http%3A//arxiv.org/abs/2512.01678v1&entry.124074799=Read"},
{"title": "How Muon's Spectral Design Benefits Generalization: A Study on Imbalanced Data", "author": "Bhavya Vasudeva and Puneesh Deora and Yize Zhao and Vatsal Sharan and Christos Thrampoulidis", "abstract": "The growing adoption of spectrum-aware matrix-valued optimizers such as Muon and Shampoo in deep learning motivates a systematic study of their generalization properties and, in particular, when they might outperform competitive algorithms. We approach this question by introducing appropriate simplifying abstractions as follows: First, we use imbalanced data as a testbed. Second, we study the canonical form of such optimizers, which is Spectral Gradient Descent (SpecGD) -- each update step is $UV^T$ where $U\u03a3V^T$ is the truncated SVD of the gradient. Third, within this framework we identify a canonical setting for which we precisely quantify when SpecGD outperforms vanilla Euclidean GD. For a Gaussian mixture data model and both linear and bilinear models, we show that unlike GD, which prioritizes learning dominant principal components of the data first, SpecGD learns all principal components of the data at equal rates. We demonstrate how this translates to a growing gap in balanced accuracy favoring SpecGD early in training and further show that the gap remains consistent even when the GD counterpart uses adaptive step-sizes via normalization. By extending the analysis to deep linear models, we show that depth amplifies these effects. We empirically verify our theoretical findings on a variety of imbalanced datasets. Our experiments compare practical variants of spectral methods, like Muon and Shampoo, against their Euclidean counterparts and Adam. The results validate our findings that these spectral optimizers achieve superior generalization by promoting a more balanced learning of the data's underlying components.", "link": "http://arxiv.org/abs/2510.22980v2", "date": "2025-12-01", "relevancy": 2.5656, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5208}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5095}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Muon%27s%20Spectral%20Design%20Benefits%20Generalization%3A%20A%20Study%20on%20Imbalanced%20Data&body=Title%3A%20How%20Muon%27s%20Spectral%20Design%20Benefits%20Generalization%3A%20A%20Study%20on%20Imbalanced%20Data%0AAuthor%3A%20Bhavya%20Vasudeva%20and%20Puneesh%20Deora%20and%20Yize%20Zhao%20and%20Vatsal%20Sharan%20and%20Christos%20Thrampoulidis%0AAbstract%3A%20The%20growing%20adoption%20of%20spectrum-aware%20matrix-valued%20optimizers%20such%20as%20Muon%20and%20Shampoo%20in%20deep%20learning%20motivates%20a%20systematic%20study%20of%20their%20generalization%20properties%20and%2C%20in%20particular%2C%20when%20they%20might%20outperform%20competitive%20algorithms.%20We%20approach%20this%20question%20by%20introducing%20appropriate%20simplifying%20abstractions%20as%20follows%3A%20First%2C%20we%20use%20imbalanced%20data%20as%20a%20testbed.%20Second%2C%20we%20study%20the%20canonical%20form%20of%20such%20optimizers%2C%20which%20is%20Spectral%20Gradient%20Descent%20%28SpecGD%29%20--%20each%20update%20step%20is%20%24UV%5ET%24%20where%20%24U%CE%A3V%5ET%24%20is%20the%20truncated%20SVD%20of%20the%20gradient.%20Third%2C%20within%20this%20framework%20we%20identify%20a%20canonical%20setting%20for%20which%20we%20precisely%20quantify%20when%20SpecGD%20outperforms%20vanilla%20Euclidean%20GD.%20For%20a%20Gaussian%20mixture%20data%20model%20and%20both%20linear%20and%20bilinear%20models%2C%20we%20show%20that%20unlike%20GD%2C%20which%20prioritizes%20learning%20dominant%20principal%20components%20of%20the%20data%20first%2C%20SpecGD%20learns%20all%20principal%20components%20of%20the%20data%20at%20equal%20rates.%20We%20demonstrate%20how%20this%20translates%20to%20a%20growing%20gap%20in%20balanced%20accuracy%20favoring%20SpecGD%20early%20in%20training%20and%20further%20show%20that%20the%20gap%20remains%20consistent%20even%20when%20the%20GD%20counterpart%20uses%20adaptive%20step-sizes%20via%20normalization.%20By%20extending%20the%20analysis%20to%20deep%20linear%20models%2C%20we%20show%20that%20depth%20amplifies%20these%20effects.%20We%20empirically%20verify%20our%20theoretical%20findings%20on%20a%20variety%20of%20imbalanced%20datasets.%20Our%20experiments%20compare%20practical%20variants%20of%20spectral%20methods%2C%20like%20Muon%20and%20Shampoo%2C%20against%20their%20Euclidean%20counterparts%20and%20Adam.%20The%20results%20validate%20our%20findings%20that%20these%20spectral%20optimizers%20achieve%20superior%20generalization%20by%20promoting%20a%20more%20balanced%20learning%20of%20the%20data%27s%20underlying%20components.%0ALink%3A%20http%3A//arxiv.org/abs/2510.22980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Muon%2527s%2520Spectral%2520Design%2520Benefits%2520Generalization%253A%2520A%2520Study%2520on%2520Imbalanced%2520Data%26entry.906535625%3DBhavya%2520Vasudeva%2520and%2520Puneesh%2520Deora%2520and%2520Yize%2520Zhao%2520and%2520Vatsal%2520Sharan%2520and%2520Christos%2520Thrampoulidis%26entry.1292438233%3DThe%2520growing%2520adoption%2520of%2520spectrum-aware%2520matrix-valued%2520optimizers%2520such%2520as%2520Muon%2520and%2520Shampoo%2520in%2520deep%2520learning%2520motivates%2520a%2520systematic%2520study%2520of%2520their%2520generalization%2520properties%2520and%252C%2520in%2520particular%252C%2520when%2520they%2520might%2520outperform%2520competitive%2520algorithms.%2520We%2520approach%2520this%2520question%2520by%2520introducing%2520appropriate%2520simplifying%2520abstractions%2520as%2520follows%253A%2520First%252C%2520we%2520use%2520imbalanced%2520data%2520as%2520a%2520testbed.%2520Second%252C%2520we%2520study%2520the%2520canonical%2520form%2520of%2520such%2520optimizers%252C%2520which%2520is%2520Spectral%2520Gradient%2520Descent%2520%2528SpecGD%2529%2520--%2520each%2520update%2520step%2520is%2520%2524UV%255ET%2524%2520where%2520%2524U%25CE%25A3V%255ET%2524%2520is%2520the%2520truncated%2520SVD%2520of%2520the%2520gradient.%2520Third%252C%2520within%2520this%2520framework%2520we%2520identify%2520a%2520canonical%2520setting%2520for%2520which%2520we%2520precisely%2520quantify%2520when%2520SpecGD%2520outperforms%2520vanilla%2520Euclidean%2520GD.%2520For%2520a%2520Gaussian%2520mixture%2520data%2520model%2520and%2520both%2520linear%2520and%2520bilinear%2520models%252C%2520we%2520show%2520that%2520unlike%2520GD%252C%2520which%2520prioritizes%2520learning%2520dominant%2520principal%2520components%2520of%2520the%2520data%2520first%252C%2520SpecGD%2520learns%2520all%2520principal%2520components%2520of%2520the%2520data%2520at%2520equal%2520rates.%2520We%2520demonstrate%2520how%2520this%2520translates%2520to%2520a%2520growing%2520gap%2520in%2520balanced%2520accuracy%2520favoring%2520SpecGD%2520early%2520in%2520training%2520and%2520further%2520show%2520that%2520the%2520gap%2520remains%2520consistent%2520even%2520when%2520the%2520GD%2520counterpart%2520uses%2520adaptive%2520step-sizes%2520via%2520normalization.%2520By%2520extending%2520the%2520analysis%2520to%2520deep%2520linear%2520models%252C%2520we%2520show%2520that%2520depth%2520amplifies%2520these%2520effects.%2520We%2520empirically%2520verify%2520our%2520theoretical%2520findings%2520on%2520a%2520variety%2520of%2520imbalanced%2520datasets.%2520Our%2520experiments%2520compare%2520practical%2520variants%2520of%2520spectral%2520methods%252C%2520like%2520Muon%2520and%2520Shampoo%252C%2520against%2520their%2520Euclidean%2520counterparts%2520and%2520Adam.%2520The%2520results%2520validate%2520our%2520findings%2520that%2520these%2520spectral%2520optimizers%2520achieve%2520superior%2520generalization%2520by%2520promoting%2520a%2520more%2520balanced%2520learning%2520of%2520the%2520data%2527s%2520underlying%2520components.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Muon%27s%20Spectral%20Design%20Benefits%20Generalization%3A%20A%20Study%20on%20Imbalanced%20Data&entry.906535625=Bhavya%20Vasudeva%20and%20Puneesh%20Deora%20and%20Yize%20Zhao%20and%20Vatsal%20Sharan%20and%20Christos%20Thrampoulidis&entry.1292438233=The%20growing%20adoption%20of%20spectrum-aware%20matrix-valued%20optimizers%20such%20as%20Muon%20and%20Shampoo%20in%20deep%20learning%20motivates%20a%20systematic%20study%20of%20their%20generalization%20properties%20and%2C%20in%20particular%2C%20when%20they%20might%20outperform%20competitive%20algorithms.%20We%20approach%20this%20question%20by%20introducing%20appropriate%20simplifying%20abstractions%20as%20follows%3A%20First%2C%20we%20use%20imbalanced%20data%20as%20a%20testbed.%20Second%2C%20we%20study%20the%20canonical%20form%20of%20such%20optimizers%2C%20which%20is%20Spectral%20Gradient%20Descent%20%28SpecGD%29%20--%20each%20update%20step%20is%20%24UV%5ET%24%20where%20%24U%CE%A3V%5ET%24%20is%20the%20truncated%20SVD%20of%20the%20gradient.%20Third%2C%20within%20this%20framework%20we%20identify%20a%20canonical%20setting%20for%20which%20we%20precisely%20quantify%20when%20SpecGD%20outperforms%20vanilla%20Euclidean%20GD.%20For%20a%20Gaussian%20mixture%20data%20model%20and%20both%20linear%20and%20bilinear%20models%2C%20we%20show%20that%20unlike%20GD%2C%20which%20prioritizes%20learning%20dominant%20principal%20components%20of%20the%20data%20first%2C%20SpecGD%20learns%20all%20principal%20components%20of%20the%20data%20at%20equal%20rates.%20We%20demonstrate%20how%20this%20translates%20to%20a%20growing%20gap%20in%20balanced%20accuracy%20favoring%20SpecGD%20early%20in%20training%20and%20further%20show%20that%20the%20gap%20remains%20consistent%20even%20when%20the%20GD%20counterpart%20uses%20adaptive%20step-sizes%20via%20normalization.%20By%20extending%20the%20analysis%20to%20deep%20linear%20models%2C%20we%20show%20that%20depth%20amplifies%20these%20effects.%20We%20empirically%20verify%20our%20theoretical%20findings%20on%20a%20variety%20of%20imbalanced%20datasets.%20Our%20experiments%20compare%20practical%20variants%20of%20spectral%20methods%2C%20like%20Muon%20and%20Shampoo%2C%20against%20their%20Euclidean%20counterparts%20and%20Adam.%20The%20results%20validate%20our%20findings%20that%20these%20spectral%20optimizers%20achieve%20superior%20generalization%20by%20promoting%20a%20more%20balanced%20learning%20of%20the%20data%27s%20underlying%20components.&entry.1838667208=http%3A//arxiv.org/abs/2510.22980v2&entry.124074799=Read"},
{"title": "AlignSAE: Concept-Aligned Sparse Autoencoders", "author": "Minglai Yang and Xinyu Guo and Mihai Surdeanu and Liangming Pan", "abstract": "Large Language Models (LLMs) encode factual knowledge within hidden parametric spaces that are difficult to inspect or control. While Sparse Autoencoders (SAEs) can decompose hidden activations into more fine-grained, interpretable features, they often struggle to reliably align these features with human-defined concepts, resulting in entangled and distributed feature representations. To address this, we introduce AlignSAE, a method that aligns SAE features with a defined ontology through a \"pre-train, then post-train\" curriculum. After an initial unsupervised training phase, we apply supervised post-training to bind specific concepts to dedicated latent slots while preserving the remaining capacity for general reconstruction. This separation creates an interpretable interface where specific relations can be inspected and controlled without interference from unrelated features. Empirical results demonstrate that AlignSAE enables precise causal interventions, such as reliable \"concept swaps\", by targeting single, semantically aligned slots.", "link": "http://arxiv.org/abs/2512.02004v1", "date": "2025-12-01", "relevancy": 2.5636, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignSAE%3A%20Concept-Aligned%20Sparse%20Autoencoders&body=Title%3A%20AlignSAE%3A%20Concept-Aligned%20Sparse%20Autoencoders%0AAuthor%3A%20Minglai%20Yang%20and%20Xinyu%20Guo%20and%20Mihai%20Surdeanu%20and%20Liangming%20Pan%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20encode%20factual%20knowledge%20within%20hidden%20parametric%20spaces%20that%20are%20difficult%20to%20inspect%20or%20control.%20While%20Sparse%20Autoencoders%20%28SAEs%29%20can%20decompose%20hidden%20activations%20into%20more%20fine-grained%2C%20interpretable%20features%2C%20they%20often%20struggle%20to%20reliably%20align%20these%20features%20with%20human-defined%20concepts%2C%20resulting%20in%20entangled%20and%20distributed%20feature%20representations.%20To%20address%20this%2C%20we%20introduce%20AlignSAE%2C%20a%20method%20that%20aligns%20SAE%20features%20with%20a%20defined%20ontology%20through%20a%20%22pre-train%2C%20then%20post-train%22%20curriculum.%20After%20an%20initial%20unsupervised%20training%20phase%2C%20we%20apply%20supervised%20post-training%20to%20bind%20specific%20concepts%20to%20dedicated%20latent%20slots%20while%20preserving%20the%20remaining%20capacity%20for%20general%20reconstruction.%20This%20separation%20creates%20an%20interpretable%20interface%20where%20specific%20relations%20can%20be%20inspected%20and%20controlled%20without%20interference%20from%20unrelated%20features.%20Empirical%20results%20demonstrate%20that%20AlignSAE%20enables%20precise%20causal%20interventions%2C%20such%20as%20reliable%20%22concept%20swaps%22%2C%20by%20targeting%20single%2C%20semantically%20aligned%20slots.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignSAE%253A%2520Concept-Aligned%2520Sparse%2520Autoencoders%26entry.906535625%3DMinglai%2520Yang%2520and%2520Xinyu%2520Guo%2520and%2520Mihai%2520Surdeanu%2520and%2520Liangming%2520Pan%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520encode%2520factual%2520knowledge%2520within%2520hidden%2520parametric%2520spaces%2520that%2520are%2520difficult%2520to%2520inspect%2520or%2520control.%2520While%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520can%2520decompose%2520hidden%2520activations%2520into%2520more%2520fine-grained%252C%2520interpretable%2520features%252C%2520they%2520often%2520struggle%2520to%2520reliably%2520align%2520these%2520features%2520with%2520human-defined%2520concepts%252C%2520resulting%2520in%2520entangled%2520and%2520distributed%2520feature%2520representations.%2520To%2520address%2520this%252C%2520we%2520introduce%2520AlignSAE%252C%2520a%2520method%2520that%2520aligns%2520SAE%2520features%2520with%2520a%2520defined%2520ontology%2520through%2520a%2520%2522pre-train%252C%2520then%2520post-train%2522%2520curriculum.%2520After%2520an%2520initial%2520unsupervised%2520training%2520phase%252C%2520we%2520apply%2520supervised%2520post-training%2520to%2520bind%2520specific%2520concepts%2520to%2520dedicated%2520latent%2520slots%2520while%2520preserving%2520the%2520remaining%2520capacity%2520for%2520general%2520reconstruction.%2520This%2520separation%2520creates%2520an%2520interpretable%2520interface%2520where%2520specific%2520relations%2520can%2520be%2520inspected%2520and%2520controlled%2520without%2520interference%2520from%2520unrelated%2520features.%2520Empirical%2520results%2520demonstrate%2520that%2520AlignSAE%2520enables%2520precise%2520causal%2520interventions%252C%2520such%2520as%2520reliable%2520%2522concept%2520swaps%2522%252C%2520by%2520targeting%2520single%252C%2520semantically%2520aligned%2520slots.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignSAE%3A%20Concept-Aligned%20Sparse%20Autoencoders&entry.906535625=Minglai%20Yang%20and%20Xinyu%20Guo%20and%20Mihai%20Surdeanu%20and%20Liangming%20Pan&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20encode%20factual%20knowledge%20within%20hidden%20parametric%20spaces%20that%20are%20difficult%20to%20inspect%20or%20control.%20While%20Sparse%20Autoencoders%20%28SAEs%29%20can%20decompose%20hidden%20activations%20into%20more%20fine-grained%2C%20interpretable%20features%2C%20they%20often%20struggle%20to%20reliably%20align%20these%20features%20with%20human-defined%20concepts%2C%20resulting%20in%20entangled%20and%20distributed%20feature%20representations.%20To%20address%20this%2C%20we%20introduce%20AlignSAE%2C%20a%20method%20that%20aligns%20SAE%20features%20with%20a%20defined%20ontology%20through%20a%20%22pre-train%2C%20then%20post-train%22%20curriculum.%20After%20an%20initial%20unsupervised%20training%20phase%2C%20we%20apply%20supervised%20post-training%20to%20bind%20specific%20concepts%20to%20dedicated%20latent%20slots%20while%20preserving%20the%20remaining%20capacity%20for%20general%20reconstruction.%20This%20separation%20creates%20an%20interpretable%20interface%20where%20specific%20relations%20can%20be%20inspected%20and%20controlled%20without%20interference%20from%20unrelated%20features.%20Empirical%20results%20demonstrate%20that%20AlignSAE%20enables%20precise%20causal%20interventions%2C%20such%20as%20reliable%20%22concept%20swaps%22%2C%20by%20targeting%20single%2C%20semantically%20aligned%20slots.&entry.1838667208=http%3A//arxiv.org/abs/2512.02004v1&entry.124074799=Read"},
{"title": "DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation", "author": "Hongfei Zhang and Kanghao Chen and Zixin Zhang and Harold Haodong Chen and Yuanhuiyi Lyu and Yuqi Zhang and Shuai Yang and Kun Zhou and Yingcong Chen", "abstract": "This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl-page/", "link": "http://arxiv.org/abs/2511.23127v2", "date": "2025-12-01", "relevancy": 2.5572, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.725}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6281}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualCamCtrl%3A%20Dual-Branch%20Diffusion%20Model%20for%20Geometry-Aware%20Camera-Controlled%20Video%20Generation&body=Title%3A%20DualCamCtrl%3A%20Dual-Branch%20Diffusion%20Model%20for%20Geometry-Aware%20Camera-Controlled%20Video%20Generation%0AAuthor%3A%20Hongfei%20Zhang%20and%20Kanghao%20Chen%20and%20Zixin%20Zhang%20and%20Harold%20Haodong%20Chen%20and%20Yuanhuiyi%20Lyu%20and%20Yuqi%20Zhang%20and%20Shuai%20Yang%20and%20Kun%20Zhou%20and%20Yingcong%20Chen%0AAbstract%3A%20This%20paper%20presents%20DualCamCtrl%2C%20a%20novel%20end-to-end%20diffusion%20model%20for%20camera-controlled%20video%20generation.%20Recent%20works%20have%20advanced%20this%20field%20by%20representing%20camera%20poses%20as%20ray-based%20conditions%2C%20yet%20they%20often%20lack%20sufficient%20scene%20understanding%20and%20geometric%20awareness.%20DualCamCtrl%20specifically%20targets%20this%20limitation%20by%20introducing%20a%20dual-branch%20framework%20that%20mutually%20generates%20camera-consistent%20RGB%20and%20depth%20sequences.%20To%20harmonize%20these%20two%20modalities%2C%20we%20further%20propose%20the%20Semantic%20Guided%20Mutual%20Alignment%20%28SIGMA%29%20mechanism%2C%20which%20performs%20RGB-depth%20fusion%20in%20a%20semantics-guided%20and%20mutually%20reinforced%20manner.%20These%20designs%20collectively%20enable%20DualCamCtrl%20to%20better%20disentangle%20appearance%20and%20geometry%20modeling%2C%20generating%20videos%20that%20more%20faithfully%20adhere%20to%20the%20specified%20camera%20trajectories.%20Additionally%2C%20we%20analyze%20and%20reveal%20the%20distinct%20influence%20of%20depth%20and%20camera%20poses%20across%20denoising%20stages%20and%20further%20demonstrate%20that%20early%20and%20late%20stages%20play%20complementary%20roles%20in%20forming%20global%20structure%20and%20refining%20local%20details.%20Extensive%20experiments%20demonstrate%20that%20DualCamCtrl%20achieves%20more%20consistent%20camera-controlled%20video%20generation%2C%20with%20over%2040%5C%25%20reduction%20in%20camera%20motion%20errors%20compared%20with%20prior%20methods.%20Our%20project%20page%3A%20https%3A//soyouthinkyoucantell.github.io/dualcamctrl-page/%0ALink%3A%20http%3A//arxiv.org/abs/2511.23127v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualCamCtrl%253A%2520Dual-Branch%2520Diffusion%2520Model%2520for%2520Geometry-Aware%2520Camera-Controlled%2520Video%2520Generation%26entry.906535625%3DHongfei%2520Zhang%2520and%2520Kanghao%2520Chen%2520and%2520Zixin%2520Zhang%2520and%2520Harold%2520Haodong%2520Chen%2520and%2520Yuanhuiyi%2520Lyu%2520and%2520Yuqi%2520Zhang%2520and%2520Shuai%2520Yang%2520and%2520Kun%2520Zhou%2520and%2520Yingcong%2520Chen%26entry.1292438233%3DThis%2520paper%2520presents%2520DualCamCtrl%252C%2520a%2520novel%2520end-to-end%2520diffusion%2520model%2520for%2520camera-controlled%2520video%2520generation.%2520Recent%2520works%2520have%2520advanced%2520this%2520field%2520by%2520representing%2520camera%2520poses%2520as%2520ray-based%2520conditions%252C%2520yet%2520they%2520often%2520lack%2520sufficient%2520scene%2520understanding%2520and%2520geometric%2520awareness.%2520DualCamCtrl%2520specifically%2520targets%2520this%2520limitation%2520by%2520introducing%2520a%2520dual-branch%2520framework%2520that%2520mutually%2520generates%2520camera-consistent%2520RGB%2520and%2520depth%2520sequences.%2520To%2520harmonize%2520these%2520two%2520modalities%252C%2520we%2520further%2520propose%2520the%2520Semantic%2520Guided%2520Mutual%2520Alignment%2520%2528SIGMA%2529%2520mechanism%252C%2520which%2520performs%2520RGB-depth%2520fusion%2520in%2520a%2520semantics-guided%2520and%2520mutually%2520reinforced%2520manner.%2520These%2520designs%2520collectively%2520enable%2520DualCamCtrl%2520to%2520better%2520disentangle%2520appearance%2520and%2520geometry%2520modeling%252C%2520generating%2520videos%2520that%2520more%2520faithfully%2520adhere%2520to%2520the%2520specified%2520camera%2520trajectories.%2520Additionally%252C%2520we%2520analyze%2520and%2520reveal%2520the%2520distinct%2520influence%2520of%2520depth%2520and%2520camera%2520poses%2520across%2520denoising%2520stages%2520and%2520further%2520demonstrate%2520that%2520early%2520and%2520late%2520stages%2520play%2520complementary%2520roles%2520in%2520forming%2520global%2520structure%2520and%2520refining%2520local%2520details.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DualCamCtrl%2520achieves%2520more%2520consistent%2520camera-controlled%2520video%2520generation%252C%2520with%2520over%252040%255C%2525%2520reduction%2520in%2520camera%2520motion%2520errors%2520compared%2520with%2520prior%2520methods.%2520Our%2520project%2520page%253A%2520https%253A//soyouthinkyoucantell.github.io/dualcamctrl-page/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23127v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualCamCtrl%3A%20Dual-Branch%20Diffusion%20Model%20for%20Geometry-Aware%20Camera-Controlled%20Video%20Generation&entry.906535625=Hongfei%20Zhang%20and%20Kanghao%20Chen%20and%20Zixin%20Zhang%20and%20Harold%20Haodong%20Chen%20and%20Yuanhuiyi%20Lyu%20and%20Yuqi%20Zhang%20and%20Shuai%20Yang%20and%20Kun%20Zhou%20and%20Yingcong%20Chen&entry.1292438233=This%20paper%20presents%20DualCamCtrl%2C%20a%20novel%20end-to-end%20diffusion%20model%20for%20camera-controlled%20video%20generation.%20Recent%20works%20have%20advanced%20this%20field%20by%20representing%20camera%20poses%20as%20ray-based%20conditions%2C%20yet%20they%20often%20lack%20sufficient%20scene%20understanding%20and%20geometric%20awareness.%20DualCamCtrl%20specifically%20targets%20this%20limitation%20by%20introducing%20a%20dual-branch%20framework%20that%20mutually%20generates%20camera-consistent%20RGB%20and%20depth%20sequences.%20To%20harmonize%20these%20two%20modalities%2C%20we%20further%20propose%20the%20Semantic%20Guided%20Mutual%20Alignment%20%28SIGMA%29%20mechanism%2C%20which%20performs%20RGB-depth%20fusion%20in%20a%20semantics-guided%20and%20mutually%20reinforced%20manner.%20These%20designs%20collectively%20enable%20DualCamCtrl%20to%20better%20disentangle%20appearance%20and%20geometry%20modeling%2C%20generating%20videos%20that%20more%20faithfully%20adhere%20to%20the%20specified%20camera%20trajectories.%20Additionally%2C%20we%20analyze%20and%20reveal%20the%20distinct%20influence%20of%20depth%20and%20camera%20poses%20across%20denoising%20stages%20and%20further%20demonstrate%20that%20early%20and%20late%20stages%20play%20complementary%20roles%20in%20forming%20global%20structure%20and%20refining%20local%20details.%20Extensive%20experiments%20demonstrate%20that%20DualCamCtrl%20achieves%20more%20consistent%20camera-controlled%20video%20generation%2C%20with%20over%2040%5C%25%20reduction%20in%20camera%20motion%20errors%20compared%20with%20prior%20methods.%20Our%20project%20page%3A%20https%3A//soyouthinkyoucantell.github.io/dualcamctrl-page/&entry.1838667208=http%3A//arxiv.org/abs/2511.23127v2&entry.124074799=Read"},
{"title": "Beyond Scaffold: A Unified Spatio-Temporal Gradient Tracking Method", "author": "Yan Huang and Jinming Xu and Jiming Chen and Karl Henrik Johansson", "abstract": "In distributed and federated learning algorithms, communication overhead is often reduced by performing multiple local updates between communication rounds. However, due to data heterogeneity across nodes and the local gradient noise within each node, this strategy can lead to the drift of local models away from the global optimum. To address this issue, we revisit the well-known federated learning method Scaffold (Karimireddy et al., 2020) under a gradient tracking perspective, and propose a unified spatio-temporal gradient tracking algorithm, termed ST-GT, for distributed stochastic optimization over time-varying graphs. ST-GT tracks the global gradient across neighboring nodes to mitigate data heterogeneity, while maintaining a running average of local gradients to substantially suppress noise, with slightly more storage overhead. Without assuming bounded data heterogeneity, we prove that ST-GT attains a linear convergence rate for strongly convex problems and a sublinear rate for nonconvex cases. Notably, ST-GT achieves the first linear speed-up in communication complexity with respect to the number of local updates per round $\u03c4$ for the strongly-convex setting. Compared to traditional gradient tracking methods, ST-GT reduces the topology-dependent noise term from $\u03c3^2$ to $\u03c3^2/\u03c4$, where $\u03c3^2$ denotes the noise level, thereby improving communication efficiency.", "link": "http://arxiv.org/abs/2512.01732v1", "date": "2025-12-01", "relevancy": 2.5524, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.52}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5071}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Scaffold%3A%20A%20Unified%20Spatio-Temporal%20Gradient%20Tracking%20Method&body=Title%3A%20Beyond%20Scaffold%3A%20A%20Unified%20Spatio-Temporal%20Gradient%20Tracking%20Method%0AAuthor%3A%20Yan%20Huang%20and%20Jinming%20Xu%20and%20Jiming%20Chen%20and%20Karl%20Henrik%20Johansson%0AAbstract%3A%20In%20distributed%20and%20federated%20learning%20algorithms%2C%20communication%20overhead%20is%20often%20reduced%20by%20performing%20multiple%20local%20updates%20between%20communication%20rounds.%20However%2C%20due%20to%20data%20heterogeneity%20across%20nodes%20and%20the%20local%20gradient%20noise%20within%20each%20node%2C%20this%20strategy%20can%20lead%20to%20the%20drift%20of%20local%20models%20away%20from%20the%20global%20optimum.%20To%20address%20this%20issue%2C%20we%20revisit%20the%20well-known%20federated%20learning%20method%20Scaffold%20%28Karimireddy%20et%20al.%2C%202020%29%20under%20a%20gradient%20tracking%20perspective%2C%20and%20propose%20a%20unified%20spatio-temporal%20gradient%20tracking%20algorithm%2C%20termed%20ST-GT%2C%20for%20distributed%20stochastic%20optimization%20over%20time-varying%20graphs.%20ST-GT%20tracks%20the%20global%20gradient%20across%20neighboring%20nodes%20to%20mitigate%20data%20heterogeneity%2C%20while%20maintaining%20a%20running%20average%20of%20local%20gradients%20to%20substantially%20suppress%20noise%2C%20with%20slightly%20more%20storage%20overhead.%20Without%20assuming%20bounded%20data%20heterogeneity%2C%20we%20prove%20that%20ST-GT%20attains%20a%20linear%20convergence%20rate%20for%20strongly%20convex%20problems%20and%20a%20sublinear%20rate%20for%20nonconvex%20cases.%20Notably%2C%20ST-GT%20achieves%20the%20first%20linear%20speed-up%20in%20communication%20complexity%20with%20respect%20to%20the%20number%20of%20local%20updates%20per%20round%20%24%CF%84%24%20for%20the%20strongly-convex%20setting.%20Compared%20to%20traditional%20gradient%20tracking%20methods%2C%20ST-GT%20reduces%20the%20topology-dependent%20noise%20term%20from%20%24%CF%83%5E2%24%20to%20%24%CF%83%5E2/%CF%84%24%2C%20where%20%24%CF%83%5E2%24%20denotes%20the%20noise%20level%2C%20thereby%20improving%20communication%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Scaffold%253A%2520A%2520Unified%2520Spatio-Temporal%2520Gradient%2520Tracking%2520Method%26entry.906535625%3DYan%2520Huang%2520and%2520Jinming%2520Xu%2520and%2520Jiming%2520Chen%2520and%2520Karl%2520Henrik%2520Johansson%26entry.1292438233%3DIn%2520distributed%2520and%2520federated%2520learning%2520algorithms%252C%2520communication%2520overhead%2520is%2520often%2520reduced%2520by%2520performing%2520multiple%2520local%2520updates%2520between%2520communication%2520rounds.%2520However%252C%2520due%2520to%2520data%2520heterogeneity%2520across%2520nodes%2520and%2520the%2520local%2520gradient%2520noise%2520within%2520each%2520node%252C%2520this%2520strategy%2520can%2520lead%2520to%2520the%2520drift%2520of%2520local%2520models%2520away%2520from%2520the%2520global%2520optimum.%2520To%2520address%2520this%2520issue%252C%2520we%2520revisit%2520the%2520well-known%2520federated%2520learning%2520method%2520Scaffold%2520%2528Karimireddy%2520et%2520al.%252C%25202020%2529%2520under%2520a%2520gradient%2520tracking%2520perspective%252C%2520and%2520propose%2520a%2520unified%2520spatio-temporal%2520gradient%2520tracking%2520algorithm%252C%2520termed%2520ST-GT%252C%2520for%2520distributed%2520stochastic%2520optimization%2520over%2520time-varying%2520graphs.%2520ST-GT%2520tracks%2520the%2520global%2520gradient%2520across%2520neighboring%2520nodes%2520to%2520mitigate%2520data%2520heterogeneity%252C%2520while%2520maintaining%2520a%2520running%2520average%2520of%2520local%2520gradients%2520to%2520substantially%2520suppress%2520noise%252C%2520with%2520slightly%2520more%2520storage%2520overhead.%2520Without%2520assuming%2520bounded%2520data%2520heterogeneity%252C%2520we%2520prove%2520that%2520ST-GT%2520attains%2520a%2520linear%2520convergence%2520rate%2520for%2520strongly%2520convex%2520problems%2520and%2520a%2520sublinear%2520rate%2520for%2520nonconvex%2520cases.%2520Notably%252C%2520ST-GT%2520achieves%2520the%2520first%2520linear%2520speed-up%2520in%2520communication%2520complexity%2520with%2520respect%2520to%2520the%2520number%2520of%2520local%2520updates%2520per%2520round%2520%2524%25CF%2584%2524%2520for%2520the%2520strongly-convex%2520setting.%2520Compared%2520to%2520traditional%2520gradient%2520tracking%2520methods%252C%2520ST-GT%2520reduces%2520the%2520topology-dependent%2520noise%2520term%2520from%2520%2524%25CF%2583%255E2%2524%2520to%2520%2524%25CF%2583%255E2/%25CF%2584%2524%252C%2520where%2520%2524%25CF%2583%255E2%2524%2520denotes%2520the%2520noise%2520level%252C%2520thereby%2520improving%2520communication%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Scaffold%3A%20A%20Unified%20Spatio-Temporal%20Gradient%20Tracking%20Method&entry.906535625=Yan%20Huang%20and%20Jinming%20Xu%20and%20Jiming%20Chen%20and%20Karl%20Henrik%20Johansson&entry.1292438233=In%20distributed%20and%20federated%20learning%20algorithms%2C%20communication%20overhead%20is%20often%20reduced%20by%20performing%20multiple%20local%20updates%20between%20communication%20rounds.%20However%2C%20due%20to%20data%20heterogeneity%20across%20nodes%20and%20the%20local%20gradient%20noise%20within%20each%20node%2C%20this%20strategy%20can%20lead%20to%20the%20drift%20of%20local%20models%20away%20from%20the%20global%20optimum.%20To%20address%20this%20issue%2C%20we%20revisit%20the%20well-known%20federated%20learning%20method%20Scaffold%20%28Karimireddy%20et%20al.%2C%202020%29%20under%20a%20gradient%20tracking%20perspective%2C%20and%20propose%20a%20unified%20spatio-temporal%20gradient%20tracking%20algorithm%2C%20termed%20ST-GT%2C%20for%20distributed%20stochastic%20optimization%20over%20time-varying%20graphs.%20ST-GT%20tracks%20the%20global%20gradient%20across%20neighboring%20nodes%20to%20mitigate%20data%20heterogeneity%2C%20while%20maintaining%20a%20running%20average%20of%20local%20gradients%20to%20substantially%20suppress%20noise%2C%20with%20slightly%20more%20storage%20overhead.%20Without%20assuming%20bounded%20data%20heterogeneity%2C%20we%20prove%20that%20ST-GT%20attains%20a%20linear%20convergence%20rate%20for%20strongly%20convex%20problems%20and%20a%20sublinear%20rate%20for%20nonconvex%20cases.%20Notably%2C%20ST-GT%20achieves%20the%20first%20linear%20speed-up%20in%20communication%20complexity%20with%20respect%20to%20the%20number%20of%20local%20updates%20per%20round%20%24%CF%84%24%20for%20the%20strongly-convex%20setting.%20Compared%20to%20traditional%20gradient%20tracking%20methods%2C%20ST-GT%20reduces%20the%20topology-dependent%20noise%20term%20from%20%24%CF%83%5E2%24%20to%20%24%CF%83%5E2/%CF%84%24%2C%20where%20%24%CF%83%5E2%24%20denotes%20the%20noise%20level%2C%20thereby%20improving%20communication%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2512.01732v1&entry.124074799=Read"},
{"title": "Prediction of Distant Metastasis in Head and Neck Cancer Patients Using Tumor and Peritumoral Multi-Modal Deep Learning", "author": "Nuo Tong and Changhao Liu and Zizhao Tang and Feifan Sun and Yingping Li and Shuiping Gou and Mei Shi", "abstract": "Although the combined treatment of surgery, radiotherapy, chemotherapy, and emerging target therapy has significantly improved the outcomes of patients with head and neck cancer, distant metastasis remains the leading cause of treatment failure. In this study, we propose a deep learning-based multimodal framework integrating CT imaging, radiomics, and clinical data to predict metastasis risk in HNSCC. A total of 1497 patients were retrospectively analyzed. Tumor and organ masks were generated from pretreatment CT scans, from which a 3D Swin Transformer extracted deep imaging features, while 1562 radiomics features were reduced to 36 via correlation filtering and random forest selection. Clinical data (age, sex, smoking, and alcohol status) were encoded and fused with imaging features, and the multimodal representation was fed into a fully connected network for prediction. Five-fold cross-validation was used to assess performance via AUC, accuracy, sensitivity, and specificity. The multimodal model outperformed all single-modality baselines. The deep learning module alone achieved an AUC of 0.715, whereas multimodal fusion significantly improved performance (AUC = 0.803, ACC = 0.752, SEN = 0.730, SPE = 0.758). Stratified analyses confirmed good generalizability across tumor subtypes. Ablation experiments demonstrated complementary contributions from each modality, and the 3D Swin Transformer provided more robust representations than conventional architectures. This multimodal deep learning model enables accurate, non-invasive metastasis prediction in HNSCC and shows strong potential for individualized treatment planning.", "link": "http://arxiv.org/abs/2508.20469v2", "date": "2025-12-01", "relevancy": 2.5348, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5044}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction%20of%20Distant%20Metastasis%20in%20Head%20and%20Neck%20Cancer%20Patients%20Using%20Tumor%20and%20Peritumoral%20Multi-Modal%20Deep%20Learning&body=Title%3A%20Prediction%20of%20Distant%20Metastasis%20in%20Head%20and%20Neck%20Cancer%20Patients%20Using%20Tumor%20and%20Peritumoral%20Multi-Modal%20Deep%20Learning%0AAuthor%3A%20Nuo%20Tong%20and%20Changhao%20Liu%20and%20Zizhao%20Tang%20and%20Feifan%20Sun%20and%20Yingping%20Li%20and%20Shuiping%20Gou%20and%20Mei%20Shi%0AAbstract%3A%20Although%20the%20combined%20treatment%20of%20surgery%2C%20radiotherapy%2C%20chemotherapy%2C%20and%20emerging%20target%20therapy%20has%20significantly%20improved%20the%20outcomes%20of%20patients%20with%20head%20and%20neck%20cancer%2C%20distant%20metastasis%20remains%20the%20leading%20cause%20of%20treatment%20failure.%20In%20this%20study%2C%20we%20propose%20a%20deep%20learning-based%20multimodal%20framework%20integrating%20CT%20imaging%2C%20radiomics%2C%20and%20clinical%20data%20to%20predict%20metastasis%20risk%20in%20HNSCC.%20A%20total%20of%201497%20patients%20were%20retrospectively%20analyzed.%20Tumor%20and%20organ%20masks%20were%20generated%20from%20pretreatment%20CT%20scans%2C%20from%20which%20a%203D%20Swin%20Transformer%20extracted%20deep%20imaging%20features%2C%20while%201562%20radiomics%20features%20were%20reduced%20to%2036%20via%20correlation%20filtering%20and%20random%20forest%20selection.%20Clinical%20data%20%28age%2C%20sex%2C%20smoking%2C%20and%20alcohol%20status%29%20were%20encoded%20and%20fused%20with%20imaging%20features%2C%20and%20the%20multimodal%20representation%20was%20fed%20into%20a%20fully%20connected%20network%20for%20prediction.%20Five-fold%20cross-validation%20was%20used%20to%20assess%20performance%20via%20AUC%2C%20accuracy%2C%20sensitivity%2C%20and%20specificity.%20The%20multimodal%20model%20outperformed%20all%20single-modality%20baselines.%20The%20deep%20learning%20module%20alone%20achieved%20an%20AUC%20of%200.715%2C%20whereas%20multimodal%20fusion%20significantly%20improved%20performance%20%28AUC%20%3D%200.803%2C%20ACC%20%3D%200.752%2C%20SEN%20%3D%200.730%2C%20SPE%20%3D%200.758%29.%20Stratified%20analyses%20confirmed%20good%20generalizability%20across%20tumor%20subtypes.%20Ablation%20experiments%20demonstrated%20complementary%20contributions%20from%20each%20modality%2C%20and%20the%203D%20Swin%20Transformer%20provided%20more%20robust%20representations%20than%20conventional%20architectures.%20This%20multimodal%20deep%20learning%20model%20enables%20accurate%2C%20non-invasive%20metastasis%20prediction%20in%20HNSCC%20and%20shows%20strong%20potential%20for%20individualized%20treatment%20planning.%0ALink%3A%20http%3A//arxiv.org/abs/2508.20469v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction%2520of%2520Distant%2520Metastasis%2520in%2520Head%2520and%2520Neck%2520Cancer%2520Patients%2520Using%2520Tumor%2520and%2520Peritumoral%2520Multi-Modal%2520Deep%2520Learning%26entry.906535625%3DNuo%2520Tong%2520and%2520Changhao%2520Liu%2520and%2520Zizhao%2520Tang%2520and%2520Feifan%2520Sun%2520and%2520Yingping%2520Li%2520and%2520Shuiping%2520Gou%2520and%2520Mei%2520Shi%26entry.1292438233%3DAlthough%2520the%2520combined%2520treatment%2520of%2520surgery%252C%2520radiotherapy%252C%2520chemotherapy%252C%2520and%2520emerging%2520target%2520therapy%2520has%2520significantly%2520improved%2520the%2520outcomes%2520of%2520patients%2520with%2520head%2520and%2520neck%2520cancer%252C%2520distant%2520metastasis%2520remains%2520the%2520leading%2520cause%2520of%2520treatment%2520failure.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520deep%2520learning-based%2520multimodal%2520framework%2520integrating%2520CT%2520imaging%252C%2520radiomics%252C%2520and%2520clinical%2520data%2520to%2520predict%2520metastasis%2520risk%2520in%2520HNSCC.%2520A%2520total%2520of%25201497%2520patients%2520were%2520retrospectively%2520analyzed.%2520Tumor%2520and%2520organ%2520masks%2520were%2520generated%2520from%2520pretreatment%2520CT%2520scans%252C%2520from%2520which%2520a%25203D%2520Swin%2520Transformer%2520extracted%2520deep%2520imaging%2520features%252C%2520while%25201562%2520radiomics%2520features%2520were%2520reduced%2520to%252036%2520via%2520correlation%2520filtering%2520and%2520random%2520forest%2520selection.%2520Clinical%2520data%2520%2528age%252C%2520sex%252C%2520smoking%252C%2520and%2520alcohol%2520status%2529%2520were%2520encoded%2520and%2520fused%2520with%2520imaging%2520features%252C%2520and%2520the%2520multimodal%2520representation%2520was%2520fed%2520into%2520a%2520fully%2520connected%2520network%2520for%2520prediction.%2520Five-fold%2520cross-validation%2520was%2520used%2520to%2520assess%2520performance%2520via%2520AUC%252C%2520accuracy%252C%2520sensitivity%252C%2520and%2520specificity.%2520The%2520multimodal%2520model%2520outperformed%2520all%2520single-modality%2520baselines.%2520The%2520deep%2520learning%2520module%2520alone%2520achieved%2520an%2520AUC%2520of%25200.715%252C%2520whereas%2520multimodal%2520fusion%2520significantly%2520improved%2520performance%2520%2528AUC%2520%253D%25200.803%252C%2520ACC%2520%253D%25200.752%252C%2520SEN%2520%253D%25200.730%252C%2520SPE%2520%253D%25200.758%2529.%2520Stratified%2520analyses%2520confirmed%2520good%2520generalizability%2520across%2520tumor%2520subtypes.%2520Ablation%2520experiments%2520demonstrated%2520complementary%2520contributions%2520from%2520each%2520modality%252C%2520and%2520the%25203D%2520Swin%2520Transformer%2520provided%2520more%2520robust%2520representations%2520than%2520conventional%2520architectures.%2520This%2520multimodal%2520deep%2520learning%2520model%2520enables%2520accurate%252C%2520non-invasive%2520metastasis%2520prediction%2520in%2520HNSCC%2520and%2520shows%2520strong%2520potential%2520for%2520individualized%2520treatment%2520planning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20469v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction%20of%20Distant%20Metastasis%20in%20Head%20and%20Neck%20Cancer%20Patients%20Using%20Tumor%20and%20Peritumoral%20Multi-Modal%20Deep%20Learning&entry.906535625=Nuo%20Tong%20and%20Changhao%20Liu%20and%20Zizhao%20Tang%20and%20Feifan%20Sun%20and%20Yingping%20Li%20and%20Shuiping%20Gou%20and%20Mei%20Shi&entry.1292438233=Although%20the%20combined%20treatment%20of%20surgery%2C%20radiotherapy%2C%20chemotherapy%2C%20and%20emerging%20target%20therapy%20has%20significantly%20improved%20the%20outcomes%20of%20patients%20with%20head%20and%20neck%20cancer%2C%20distant%20metastasis%20remains%20the%20leading%20cause%20of%20treatment%20failure.%20In%20this%20study%2C%20we%20propose%20a%20deep%20learning-based%20multimodal%20framework%20integrating%20CT%20imaging%2C%20radiomics%2C%20and%20clinical%20data%20to%20predict%20metastasis%20risk%20in%20HNSCC.%20A%20total%20of%201497%20patients%20were%20retrospectively%20analyzed.%20Tumor%20and%20organ%20masks%20were%20generated%20from%20pretreatment%20CT%20scans%2C%20from%20which%20a%203D%20Swin%20Transformer%20extracted%20deep%20imaging%20features%2C%20while%201562%20radiomics%20features%20were%20reduced%20to%2036%20via%20correlation%20filtering%20and%20random%20forest%20selection.%20Clinical%20data%20%28age%2C%20sex%2C%20smoking%2C%20and%20alcohol%20status%29%20were%20encoded%20and%20fused%20with%20imaging%20features%2C%20and%20the%20multimodal%20representation%20was%20fed%20into%20a%20fully%20connected%20network%20for%20prediction.%20Five-fold%20cross-validation%20was%20used%20to%20assess%20performance%20via%20AUC%2C%20accuracy%2C%20sensitivity%2C%20and%20specificity.%20The%20multimodal%20model%20outperformed%20all%20single-modality%20baselines.%20The%20deep%20learning%20module%20alone%20achieved%20an%20AUC%20of%200.715%2C%20whereas%20multimodal%20fusion%20significantly%20improved%20performance%20%28AUC%20%3D%200.803%2C%20ACC%20%3D%200.752%2C%20SEN%20%3D%200.730%2C%20SPE%20%3D%200.758%29.%20Stratified%20analyses%20confirmed%20good%20generalizability%20across%20tumor%20subtypes.%20Ablation%20experiments%20demonstrated%20complementary%20contributions%20from%20each%20modality%2C%20and%20the%203D%20Swin%20Transformer%20provided%20more%20robust%20representations%20than%20conventional%20architectures.%20This%20multimodal%20deep%20learning%20model%20enables%20accurate%2C%20non-invasive%20metastasis%20prediction%20in%20HNSCC%20and%20shows%20strong%20potential%20for%20individualized%20treatment%20planning.&entry.1838667208=http%3A//arxiv.org/abs/2508.20469v2&entry.124074799=Read"},
{"title": "Scaling and context steer LLMs along the same computational path as the human brain", "author": "Jos\u00e9phine Raugel and St\u00e9phane d'Ascoli and J\u00e9r\u00e9my Rapin and Valentin Wyart and Jean-R\u00e9mi King", "abstract": "Recent studies suggest that the representations learned by large language models (LLMs) are partially aligned to those of the human brain. However, whether and why this alignment score arises from a similar sequence of computations remains elusive. In this study, we explore this question by examining temporally-resolved brain signals of participants listening to 10 hours of an audiobook. We study these neural dynamics jointly with a benchmark encompassing 22 LLMs varying in size and architecture type. Our analyses confirm that LLMs and the brain generate representations in a similar order: specifically, activations in the initial layers of LLMs tend to best align with early brain responses, while the deeper layers of LLMs tend to best align with later brain responses. This brain-LLM alignment is consistent across transformers and recurrent architectures. However, its emergence depends on both model size and context length. Overall, this study sheds light on the sequential nature of computations and the factors underlying the partial convergence between biological and artificial neural networks.", "link": "http://arxiv.org/abs/2512.01591v1", "date": "2025-12-01", "relevancy": 2.5322, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5167}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5167}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20and%20context%20steer%20LLMs%20along%20the%20same%20computational%20path%20as%20the%20human%20brain&body=Title%3A%20Scaling%20and%20context%20steer%20LLMs%20along%20the%20same%20computational%20path%20as%20the%20human%20brain%0AAuthor%3A%20Jos%C3%A9phine%20Raugel%20and%20St%C3%A9phane%20d%27Ascoli%20and%20J%C3%A9r%C3%A9my%20Rapin%20and%20Valentin%20Wyart%20and%20Jean-R%C3%A9mi%20King%0AAbstract%3A%20Recent%20studies%20suggest%20that%20the%20representations%20learned%20by%20large%20language%20models%20%28LLMs%29%20are%20partially%20aligned%20to%20those%20of%20the%20human%20brain.%20However%2C%20whether%20and%20why%20this%20alignment%20score%20arises%20from%20a%20similar%20sequence%20of%20computations%20remains%20elusive.%20In%20this%20study%2C%20we%20explore%20this%20question%20by%20examining%20temporally-resolved%20brain%20signals%20of%20participants%20listening%20to%2010%20hours%20of%20an%20audiobook.%20We%20study%20these%20neural%20dynamics%20jointly%20with%20a%20benchmark%20encompassing%2022%20LLMs%20varying%20in%20size%20and%20architecture%20type.%20Our%20analyses%20confirm%20that%20LLMs%20and%20the%20brain%20generate%20representations%20in%20a%20similar%20order%3A%20specifically%2C%20activations%20in%20the%20initial%20layers%20of%20LLMs%20tend%20to%20best%20align%20with%20early%20brain%20responses%2C%20while%20the%20deeper%20layers%20of%20LLMs%20tend%20to%20best%20align%20with%20later%20brain%20responses.%20This%20brain-LLM%20alignment%20is%20consistent%20across%20transformers%20and%20recurrent%20architectures.%20However%2C%20its%20emergence%20depends%20on%20both%20model%20size%20and%20context%20length.%20Overall%2C%20this%20study%20sheds%20light%20on%20the%20sequential%20nature%20of%20computations%20and%20the%20factors%20underlying%20the%20partial%20convergence%20between%20biological%20and%20artificial%20neural%20networks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520and%2520context%2520steer%2520LLMs%2520along%2520the%2520same%2520computational%2520path%2520as%2520the%2520human%2520brain%26entry.906535625%3DJos%25C3%25A9phine%2520Raugel%2520and%2520St%25C3%25A9phane%2520d%2527Ascoli%2520and%2520J%25C3%25A9r%25C3%25A9my%2520Rapin%2520and%2520Valentin%2520Wyart%2520and%2520Jean-R%25C3%25A9mi%2520King%26entry.1292438233%3DRecent%2520studies%2520suggest%2520that%2520the%2520representations%2520learned%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520partially%2520aligned%2520to%2520those%2520of%2520the%2520human%2520brain.%2520However%252C%2520whether%2520and%2520why%2520this%2520alignment%2520score%2520arises%2520from%2520a%2520similar%2520sequence%2520of%2520computations%2520remains%2520elusive.%2520In%2520this%2520study%252C%2520we%2520explore%2520this%2520question%2520by%2520examining%2520temporally-resolved%2520brain%2520signals%2520of%2520participants%2520listening%2520to%252010%2520hours%2520of%2520an%2520audiobook.%2520We%2520study%2520these%2520neural%2520dynamics%2520jointly%2520with%2520a%2520benchmark%2520encompassing%252022%2520LLMs%2520varying%2520in%2520size%2520and%2520architecture%2520type.%2520Our%2520analyses%2520confirm%2520that%2520LLMs%2520and%2520the%2520brain%2520generate%2520representations%2520in%2520a%2520similar%2520order%253A%2520specifically%252C%2520activations%2520in%2520the%2520initial%2520layers%2520of%2520LLMs%2520tend%2520to%2520best%2520align%2520with%2520early%2520brain%2520responses%252C%2520while%2520the%2520deeper%2520layers%2520of%2520LLMs%2520tend%2520to%2520best%2520align%2520with%2520later%2520brain%2520responses.%2520This%2520brain-LLM%2520alignment%2520is%2520consistent%2520across%2520transformers%2520and%2520recurrent%2520architectures.%2520However%252C%2520its%2520emergence%2520depends%2520on%2520both%2520model%2520size%2520and%2520context%2520length.%2520Overall%252C%2520this%2520study%2520sheds%2520light%2520on%2520the%2520sequential%2520nature%2520of%2520computations%2520and%2520the%2520factors%2520underlying%2520the%2520partial%2520convergence%2520between%2520biological%2520and%2520artificial%2520neural%2520networks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20and%20context%20steer%20LLMs%20along%20the%20same%20computational%20path%20as%20the%20human%20brain&entry.906535625=Jos%C3%A9phine%20Raugel%20and%20St%C3%A9phane%20d%27Ascoli%20and%20J%C3%A9r%C3%A9my%20Rapin%20and%20Valentin%20Wyart%20and%20Jean-R%C3%A9mi%20King&entry.1292438233=Recent%20studies%20suggest%20that%20the%20representations%20learned%20by%20large%20language%20models%20%28LLMs%29%20are%20partially%20aligned%20to%20those%20of%20the%20human%20brain.%20However%2C%20whether%20and%20why%20this%20alignment%20score%20arises%20from%20a%20similar%20sequence%20of%20computations%20remains%20elusive.%20In%20this%20study%2C%20we%20explore%20this%20question%20by%20examining%20temporally-resolved%20brain%20signals%20of%20participants%20listening%20to%2010%20hours%20of%20an%20audiobook.%20We%20study%20these%20neural%20dynamics%20jointly%20with%20a%20benchmark%20encompassing%2022%20LLMs%20varying%20in%20size%20and%20architecture%20type.%20Our%20analyses%20confirm%20that%20LLMs%20and%20the%20brain%20generate%20representations%20in%20a%20similar%20order%3A%20specifically%2C%20activations%20in%20the%20initial%20layers%20of%20LLMs%20tend%20to%20best%20align%20with%20early%20brain%20responses%2C%20while%20the%20deeper%20layers%20of%20LLMs%20tend%20to%20best%20align%20with%20later%20brain%20responses.%20This%20brain-LLM%20alignment%20is%20consistent%20across%20transformers%20and%20recurrent%20architectures.%20However%2C%20its%20emergence%20depends%20on%20both%20model%20size%20and%20context%20length.%20Overall%2C%20this%20study%20sheds%20light%20on%20the%20sequential%20nature%20of%20computations%20and%20the%20factors%20underlying%20the%20partial%20convergence%20between%20biological%20and%20artificial%20neural%20networks.&entry.1838667208=http%3A//arxiv.org/abs/2512.01591v1&entry.124074799=Read"},
{"title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models", "author": "Tenghui Li and Guoxu Zhou and Xuyang Zhao and Yuning Qiu and Qibin Zhao", "abstract": "As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two-stage framework that jointly decomposes the full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at https://github.com/tenghuilee/LRQK.", "link": "http://arxiv.org/abs/2510.23649v2", "date": "2025-12-01", "relevancy": 2.519, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Low%20Rank%20Attention%20for%20Long-Context%20Inference%20in%20Large%20Language%20Models&body=Title%3A%20Efficient%20Low%20Rank%20Attention%20for%20Long-Context%20Inference%20in%20Large%20Language%20Models%0AAuthor%3A%20Tenghui%20Li%20and%20Guoxu%20Zhou%20and%20Xuyang%20Zhao%20and%20Yuning%20Qiu%20and%20Qibin%20Zhao%0AAbstract%3A%20As%20the%20length%20of%20input%20text%20grows%2C%20the%20key-value%20%28KV%29%20cache%20in%20LLMs%20imposes%20prohibitive%20GPU%20memory%20costs%20and%20limits%20long-context%20inference%20on%20resource%20constrained%20devices.%20Existing%20approaches%2C%20such%20as%20KV%20quantization%20and%20pruning%2C%20reduce%20memory%20usage%20but%20suffer%20from%20numerical%20precision%20loss%20or%20suboptimal%20retention%20of%20key-value%20pairs.%20We%20introduce%20Low%20Rank%20Query%20and%20Key%20attention%20%28LRQK%29%2C%20a%20two-stage%20framework%20that%20jointly%20decomposes%20the%20full-precision%20query%20and%20key%20matrices%20into%20compact%20rank-%5C%28r%5C%29%20factors%20during%20the%20prefill%20stage%2C%20and%20then%20uses%20these%20low-dimensional%20projections%20to%20compute%20proxy%20attention%20scores%20in%20%5C%28%5Cmathcal%7BO%7D%28lr%29%5C%29%20time%20at%20each%20decode%20step.%20By%20selecting%20only%20the%20top-%5C%28k%5C%29%20tokens%20and%20a%20small%20fixed%20set%20of%20recent%20tokens%2C%20LRQK%20employs%20a%20mixed%20GPU-CPU%20cache%20with%20a%20hit-and-miss%20mechanism%20that%20transfers%20only%20missing%20full-precision%20KV%20pairs%2C%20thereby%20preserving%20exact%20attention%20outputs%20while%20reducing%20CPU-GPU%20data%20movement.%20Extensive%20experiments%20on%20the%20RULER%20and%20LongBench%20benchmarks%20with%20LLaMA-3-8B%20and%20Qwen2.5-7B%20demonstrate%20that%20LRQK%20matches%20or%20surpasses%20leading%20sparse-attention%20methods%20in%20long%20context%20settings%2C%20while%20delivering%20significant%20memory%20savings%20with%20minimal%20loss%20in%20accuracy.%20Our%20code%20is%20available%20at%20https%3A//github.com/tenghuilee/LRQK.%0ALink%3A%20http%3A//arxiv.org/abs/2510.23649v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Low%2520Rank%2520Attention%2520for%2520Long-Context%2520Inference%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DTenghui%2520Li%2520and%2520Guoxu%2520Zhou%2520and%2520Xuyang%2520Zhao%2520and%2520Yuning%2520Qiu%2520and%2520Qibin%2520Zhao%26entry.1292438233%3DAs%2520the%2520length%2520of%2520input%2520text%2520grows%252C%2520the%2520key-value%2520%2528KV%2529%2520cache%2520in%2520LLMs%2520imposes%2520prohibitive%2520GPU%2520memory%2520costs%2520and%2520limits%2520long-context%2520inference%2520on%2520resource%2520constrained%2520devices.%2520Existing%2520approaches%252C%2520such%2520as%2520KV%2520quantization%2520and%2520pruning%252C%2520reduce%2520memory%2520usage%2520but%2520suffer%2520from%2520numerical%2520precision%2520loss%2520or%2520suboptimal%2520retention%2520of%2520key-value%2520pairs.%2520We%2520introduce%2520Low%2520Rank%2520Query%2520and%2520Key%2520attention%2520%2528LRQK%2529%252C%2520a%2520two-stage%2520framework%2520that%2520jointly%2520decomposes%2520the%2520full-precision%2520query%2520and%2520key%2520matrices%2520into%2520compact%2520rank-%255C%2528r%255C%2529%2520factors%2520during%2520the%2520prefill%2520stage%252C%2520and%2520then%2520uses%2520these%2520low-dimensional%2520projections%2520to%2520compute%2520proxy%2520attention%2520scores%2520in%2520%255C%2528%255Cmathcal%257BO%257D%2528lr%2529%255C%2529%2520time%2520at%2520each%2520decode%2520step.%2520By%2520selecting%2520only%2520the%2520top-%255C%2528k%255C%2529%2520tokens%2520and%2520a%2520small%2520fixed%2520set%2520of%2520recent%2520tokens%252C%2520LRQK%2520employs%2520a%2520mixed%2520GPU-CPU%2520cache%2520with%2520a%2520hit-and-miss%2520mechanism%2520that%2520transfers%2520only%2520missing%2520full-precision%2520KV%2520pairs%252C%2520thereby%2520preserving%2520exact%2520attention%2520outputs%2520while%2520reducing%2520CPU-GPU%2520data%2520movement.%2520Extensive%2520experiments%2520on%2520the%2520RULER%2520and%2520LongBench%2520benchmarks%2520with%2520LLaMA-3-8B%2520and%2520Qwen2.5-7B%2520demonstrate%2520that%2520LRQK%2520matches%2520or%2520surpasses%2520leading%2520sparse-attention%2520methods%2520in%2520long%2520context%2520settings%252C%2520while%2520delivering%2520significant%2520memory%2520savings%2520with%2520minimal%2520loss%2520in%2520accuracy.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/tenghuilee/LRQK.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23649v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Low%20Rank%20Attention%20for%20Long-Context%20Inference%20in%20Large%20Language%20Models&entry.906535625=Tenghui%20Li%20and%20Guoxu%20Zhou%20and%20Xuyang%20Zhao%20and%20Yuning%20Qiu%20and%20Qibin%20Zhao&entry.1292438233=As%20the%20length%20of%20input%20text%20grows%2C%20the%20key-value%20%28KV%29%20cache%20in%20LLMs%20imposes%20prohibitive%20GPU%20memory%20costs%20and%20limits%20long-context%20inference%20on%20resource%20constrained%20devices.%20Existing%20approaches%2C%20such%20as%20KV%20quantization%20and%20pruning%2C%20reduce%20memory%20usage%20but%20suffer%20from%20numerical%20precision%20loss%20or%20suboptimal%20retention%20of%20key-value%20pairs.%20We%20introduce%20Low%20Rank%20Query%20and%20Key%20attention%20%28LRQK%29%2C%20a%20two-stage%20framework%20that%20jointly%20decomposes%20the%20full-precision%20query%20and%20key%20matrices%20into%20compact%20rank-%5C%28r%5C%29%20factors%20during%20the%20prefill%20stage%2C%20and%20then%20uses%20these%20low-dimensional%20projections%20to%20compute%20proxy%20attention%20scores%20in%20%5C%28%5Cmathcal%7BO%7D%28lr%29%5C%29%20time%20at%20each%20decode%20step.%20By%20selecting%20only%20the%20top-%5C%28k%5C%29%20tokens%20and%20a%20small%20fixed%20set%20of%20recent%20tokens%2C%20LRQK%20employs%20a%20mixed%20GPU-CPU%20cache%20with%20a%20hit-and-miss%20mechanism%20that%20transfers%20only%20missing%20full-precision%20KV%20pairs%2C%20thereby%20preserving%20exact%20attention%20outputs%20while%20reducing%20CPU-GPU%20data%20movement.%20Extensive%20experiments%20on%20the%20RULER%20and%20LongBench%20benchmarks%20with%20LLaMA-3-8B%20and%20Qwen2.5-7B%20demonstrate%20that%20LRQK%20matches%20or%20surpasses%20leading%20sparse-attention%20methods%20in%20long%20context%20settings%2C%20while%20delivering%20significant%20memory%20savings%20with%20minimal%20loss%20in%20accuracy.%20Our%20code%20is%20available%20at%20https%3A//github.com/tenghuilee/LRQK.&entry.1838667208=http%3A//arxiv.org/abs/2510.23649v2&entry.124074799=Read"},
{"title": "Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling", "author": "Meng Cao and Haokun Lin and Haoyuan Li and Haoran Tang and Rongtao Xu and Dong An and Xue Liu and Ian Reid and Xiaodan Liang", "abstract": "Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.", "link": "http://arxiv.org/abs/2512.01821v1", "date": "2025-12-01", "relevancy": 2.5111, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6317}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20through%20Imagination%3A%20Learning%20Scene%20Geometry%20via%20Implicit%20Spatial%20World%20Modeling&body=Title%3A%20Seeing%20through%20Imagination%3A%20Learning%20Scene%20Geometry%20via%20Implicit%20Spatial%20World%20Modeling%0AAuthor%3A%20Meng%20Cao%20and%20Haokun%20Lin%20and%20Haoyuan%20Li%20and%20Haoran%20Tang%20and%20Rongtao%20Xu%20and%20Dong%20An%20and%20Xue%20Liu%20and%20Ian%20Reid%20and%20Xiaodan%20Liang%0AAbstract%3A%20Spatial%20reasoning%2C%20the%20ability%20to%20understand%20and%20interpret%20the%203D%20structure%20of%20the%20world%2C%20is%20a%20critical%20yet%20underdeveloped%20capability%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Current%20methods%20predominantly%20rely%20on%20verbal%20descriptive%20tuning%2C%20which%20suffers%20from%20visual%20illiteracy%2C%20i.e.%2C%20they%20learn%20spatial%20concepts%20through%20textual%20symbols%20alone%2C%20devoid%20of%20connection%20to%20their%20visual%20manifestations.%20To%20bridge%20this%20gap%2C%20this%20paper%20introduces%20MILO%2C%20an%20Implicit%20spatIaL%20wOrld%20modeling%20paradigm%20that%20simulates%20human-like%20spatial%20imagination.%20MILO%20integrates%20a%20visual%20generator%20to%20provide%20geometry-aware%20feedback%2C%20thereby%20implicitly%20grounding%20the%20MLLM%27s%20symbolic%20reasoning%20in%20perceptual%20experience.%20Complementing%20this%20paradigm%2C%20we%20propose%20RePE%20%28Relative%20Positional%20Encoding%29%2C%20a%20novel%20encoding%20scheme%20that%20captures%20relative%20camera-pose%20transformations%2C%20offering%20superior%20performance%20over%20absolute%20coordinate%20systems.%20To%20support%20the%20training%2C%20we%20construct%20GeoGen%2C%20a%20large-scale%20Geometry-aware%20Generative%20dataset%20with%20approximately%202%2C241%20videos%20and%2067%2C827%20observation-action-outcome%20triplets.%20Experiments%20demonstrate%20that%20our%20approach%20significantly%20enhances%20spatial%20reasoning%20capabilities%20across%20multiple%20baselines%20and%20benchmarks%2C%20offering%20a%20more%20holistic%20understanding%20of%203D%20space.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520through%2520Imagination%253A%2520Learning%2520Scene%2520Geometry%2520via%2520Implicit%2520Spatial%2520World%2520Modeling%26entry.906535625%3DMeng%2520Cao%2520and%2520Haokun%2520Lin%2520and%2520Haoyuan%2520Li%2520and%2520Haoran%2520Tang%2520and%2520Rongtao%2520Xu%2520and%2520Dong%2520An%2520and%2520Xue%2520Liu%2520and%2520Ian%2520Reid%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3DSpatial%2520reasoning%252C%2520the%2520ability%2520to%2520understand%2520and%2520interpret%2520the%25203D%2520structure%2520of%2520the%2520world%252C%2520is%2520a%2520critical%2520yet%2520underdeveloped%2520capability%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520Current%2520methods%2520predominantly%2520rely%2520on%2520verbal%2520descriptive%2520tuning%252C%2520which%2520suffers%2520from%2520visual%2520illiteracy%252C%2520i.e.%252C%2520they%2520learn%2520spatial%2520concepts%2520through%2520textual%2520symbols%2520alone%252C%2520devoid%2520of%2520connection%2520to%2520their%2520visual%2520manifestations.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520paper%2520introduces%2520MILO%252C%2520an%2520Implicit%2520spatIaL%2520wOrld%2520modeling%2520paradigm%2520that%2520simulates%2520human-like%2520spatial%2520imagination.%2520MILO%2520integrates%2520a%2520visual%2520generator%2520to%2520provide%2520geometry-aware%2520feedback%252C%2520thereby%2520implicitly%2520grounding%2520the%2520MLLM%2527s%2520symbolic%2520reasoning%2520in%2520perceptual%2520experience.%2520Complementing%2520this%2520paradigm%252C%2520we%2520propose%2520RePE%2520%2528Relative%2520Positional%2520Encoding%2529%252C%2520a%2520novel%2520encoding%2520scheme%2520that%2520captures%2520relative%2520camera-pose%2520transformations%252C%2520offering%2520superior%2520performance%2520over%2520absolute%2520coordinate%2520systems.%2520To%2520support%2520the%2520training%252C%2520we%2520construct%2520GeoGen%252C%2520a%2520large-scale%2520Geometry-aware%2520Generative%2520dataset%2520with%2520approximately%25202%252C241%2520videos%2520and%252067%252C827%2520observation-action-outcome%2520triplets.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520enhances%2520spatial%2520reasoning%2520capabilities%2520across%2520multiple%2520baselines%2520and%2520benchmarks%252C%2520offering%2520a%2520more%2520holistic%2520understanding%2520of%25203D%2520space.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20through%20Imagination%3A%20Learning%20Scene%20Geometry%20via%20Implicit%20Spatial%20World%20Modeling&entry.906535625=Meng%20Cao%20and%20Haokun%20Lin%20and%20Haoyuan%20Li%20and%20Haoran%20Tang%20and%20Rongtao%20Xu%20and%20Dong%20An%20and%20Xue%20Liu%20and%20Ian%20Reid%20and%20Xiaodan%20Liang&entry.1292438233=Spatial%20reasoning%2C%20the%20ability%20to%20understand%20and%20interpret%20the%203D%20structure%20of%20the%20world%2C%20is%20a%20critical%20yet%20underdeveloped%20capability%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Current%20methods%20predominantly%20rely%20on%20verbal%20descriptive%20tuning%2C%20which%20suffers%20from%20visual%20illiteracy%2C%20i.e.%2C%20they%20learn%20spatial%20concepts%20through%20textual%20symbols%20alone%2C%20devoid%20of%20connection%20to%20their%20visual%20manifestations.%20To%20bridge%20this%20gap%2C%20this%20paper%20introduces%20MILO%2C%20an%20Implicit%20spatIaL%20wOrld%20modeling%20paradigm%20that%20simulates%20human-like%20spatial%20imagination.%20MILO%20integrates%20a%20visual%20generator%20to%20provide%20geometry-aware%20feedback%2C%20thereby%20implicitly%20grounding%20the%20MLLM%27s%20symbolic%20reasoning%20in%20perceptual%20experience.%20Complementing%20this%20paradigm%2C%20we%20propose%20RePE%20%28Relative%20Positional%20Encoding%29%2C%20a%20novel%20encoding%20scheme%20that%20captures%20relative%20camera-pose%20transformations%2C%20offering%20superior%20performance%20over%20absolute%20coordinate%20systems.%20To%20support%20the%20training%2C%20we%20construct%20GeoGen%2C%20a%20large-scale%20Geometry-aware%20Generative%20dataset%20with%20approximately%202%2C241%20videos%20and%2067%2C827%20observation-action-outcome%20triplets.%20Experiments%20demonstrate%20that%20our%20approach%20significantly%20enhances%20spatial%20reasoning%20capabilities%20across%20multiple%20baselines%20and%20benchmarks%2C%20offering%20a%20more%20holistic%20understanding%20of%203D%20space.&entry.1838667208=http%3A//arxiv.org/abs/2512.01821v1&entry.124074799=Read"},
{"title": "Rectifying LLM Thought from Lens of Optimization", "author": "Junnan Liu and Hongwei Liu and Songyang Zhang and Kai Chen", "abstract": "Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.", "link": "http://arxiv.org/abs/2512.01925v1", "date": "2025-12-01", "relevancy": 2.5024, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rectifying%20LLM%20Thought%20from%20Lens%20of%20Optimization&body=Title%3A%20Rectifying%20LLM%20Thought%20from%20Lens%20of%20Optimization%0AAuthor%3A%20Junnan%20Liu%20and%20Hongwei%20Liu%20and%20Songyang%20Zhang%20and%20Kai%20Chen%0AAbstract%3A%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20been%20driven%20by%20their%20emergent%20reasoning%20capabilities%2C%20particularly%20through%20long%20chain-of-thought%20%28CoT%29%20prompting%2C%20which%20enables%20thorough%20exploration%20and%20deliberation.%20Despite%20these%20advances%2C%20long-CoT%20LLMs%20often%20exhibit%20suboptimal%20reasoning%20behaviors%2C%20such%20as%20overthinking%20and%20excessively%20protracted%20reasoning%20chains%2C%20which%20can%20impair%20performance.%20In%20this%20paper%2C%20we%20analyze%20reasoning%20processes%20through%20an%20optimization%20lens%2C%20framing%20CoT%20as%20a%20gradient%20descent%20procedure%20where%20each%20reasoning%20step%20constitutes%20an%20update%20toward%20problem%20resolution.%20Building%20on%20this%20perspective%2C%20we%20introduce%20RePro%20%28Rectifying%20Process-level%20Reward%29%2C%20a%20novel%20approach%20to%20refine%20LLM%20reasoning%20during%20post-training.%20RePro%20defines%20a%20surrogate%20objective%20function%20to%20assess%20the%20optimization%20process%20underlying%20CoT%2C%20utilizing%20a%20dual%20scoring%20mechanism%20to%20quantify%20its%20intensity%20and%20stability.%20These%20scores%20are%20aggregated%20into%20a%20composite%20process-level%20reward%2C%20seamlessly%20integrated%20into%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20pipelines%20to%20optimize%20LLMs.%20Extensive%20experiments%20across%20multiple%20reinforcement%20learning%20algorithms%20and%20diverse%20LLMs%2C%20evaluated%20on%20benchmarks%20spanning%20mathematics%2C%20science%2C%20and%20coding%2C%20demonstrate%20that%20RePro%20consistently%20enhances%20reasoning%20performance%20and%20mitigates%20suboptimal%20reasoning%20behaviors.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRectifying%2520LLM%2520Thought%2520from%2520Lens%2520of%2520Optimization%26entry.906535625%3DJunnan%2520Liu%2520and%2520Hongwei%2520Liu%2520and%2520Songyang%2520Zhang%2520and%2520Kai%2520Chen%26entry.1292438233%3DRecent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520driven%2520by%2520their%2520emergent%2520reasoning%2520capabilities%252C%2520particularly%2520through%2520long%2520chain-of-thought%2520%2528CoT%2529%2520prompting%252C%2520which%2520enables%2520thorough%2520exploration%2520and%2520deliberation.%2520Despite%2520these%2520advances%252C%2520long-CoT%2520LLMs%2520often%2520exhibit%2520suboptimal%2520reasoning%2520behaviors%252C%2520such%2520as%2520overthinking%2520and%2520excessively%2520protracted%2520reasoning%2520chains%252C%2520which%2520can%2520impair%2520performance.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520reasoning%2520processes%2520through%2520an%2520optimization%2520lens%252C%2520framing%2520CoT%2520as%2520a%2520gradient%2520descent%2520procedure%2520where%2520each%2520reasoning%2520step%2520constitutes%2520an%2520update%2520toward%2520problem%2520resolution.%2520Building%2520on%2520this%2520perspective%252C%2520we%2520introduce%2520RePro%2520%2528Rectifying%2520Process-level%2520Reward%2529%252C%2520a%2520novel%2520approach%2520to%2520refine%2520LLM%2520reasoning%2520during%2520post-training.%2520RePro%2520defines%2520a%2520surrogate%2520objective%2520function%2520to%2520assess%2520the%2520optimization%2520process%2520underlying%2520CoT%252C%2520utilizing%2520a%2520dual%2520scoring%2520mechanism%2520to%2520quantify%2520its%2520intensity%2520and%2520stability.%2520These%2520scores%2520are%2520aggregated%2520into%2520a%2520composite%2520process-level%2520reward%252C%2520seamlessly%2520integrated%2520into%2520reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520pipelines%2520to%2520optimize%2520LLMs.%2520Extensive%2520experiments%2520across%2520multiple%2520reinforcement%2520learning%2520algorithms%2520and%2520diverse%2520LLMs%252C%2520evaluated%2520on%2520benchmarks%2520spanning%2520mathematics%252C%2520science%252C%2520and%2520coding%252C%2520demonstrate%2520that%2520RePro%2520consistently%2520enhances%2520reasoning%2520performance%2520and%2520mitigates%2520suboptimal%2520reasoning%2520behaviors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rectifying%20LLM%20Thought%20from%20Lens%20of%20Optimization&entry.906535625=Junnan%20Liu%20and%20Hongwei%20Liu%20and%20Songyang%20Zhang%20and%20Kai%20Chen&entry.1292438233=Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20been%20driven%20by%20their%20emergent%20reasoning%20capabilities%2C%20particularly%20through%20long%20chain-of-thought%20%28CoT%29%20prompting%2C%20which%20enables%20thorough%20exploration%20and%20deliberation.%20Despite%20these%20advances%2C%20long-CoT%20LLMs%20often%20exhibit%20suboptimal%20reasoning%20behaviors%2C%20such%20as%20overthinking%20and%20excessively%20protracted%20reasoning%20chains%2C%20which%20can%20impair%20performance.%20In%20this%20paper%2C%20we%20analyze%20reasoning%20processes%20through%20an%20optimization%20lens%2C%20framing%20CoT%20as%20a%20gradient%20descent%20procedure%20where%20each%20reasoning%20step%20constitutes%20an%20update%20toward%20problem%20resolution.%20Building%20on%20this%20perspective%2C%20we%20introduce%20RePro%20%28Rectifying%20Process-level%20Reward%29%2C%20a%20novel%20approach%20to%20refine%20LLM%20reasoning%20during%20post-training.%20RePro%20defines%20a%20surrogate%20objective%20function%20to%20assess%20the%20optimization%20process%20underlying%20CoT%2C%20utilizing%20a%20dual%20scoring%20mechanism%20to%20quantify%20its%20intensity%20and%20stability.%20These%20scores%20are%20aggregated%20into%20a%20composite%20process-level%20reward%2C%20seamlessly%20integrated%20into%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20pipelines%20to%20optimize%20LLMs.%20Extensive%20experiments%20across%20multiple%20reinforcement%20learning%20algorithms%20and%20diverse%20LLMs%2C%20evaluated%20on%20benchmarks%20spanning%20mathematics%2C%20science%2C%20and%20coding%2C%20demonstrate%20that%20RePro%20consistently%20enhances%20reasoning%20performance%20and%20mitigates%20suboptimal%20reasoning%20behaviors.&entry.1838667208=http%3A//arxiv.org/abs/2512.01925v1&entry.124074799=Read"},
{"title": "Fast Multi-view Consistent 3D Editing with Video Priors", "author": "Liyi Chen and Ruihuang Li and Guowen Zhang and Pengfei Wang and Lei Zhang", "abstract": "Text-driven 3D editing enables user-friendly 3D object or scene editing with text instructions. Due to the lack of multi-view consistency priors, existing methods typically resort to employing 2D generation or editing models to process each view individually, followed by iterative 2D-3D-2D updating. However, these methods are not only time-consuming but also prone to over-smoothed results because the different editing signals gathered from different views are averaged during the iterative process. In this paper, we propose generative Video Prior based 3D Editing (ViP3DE) to employ the temporal consistency priors from pre-trained video generation models for multi-view consistent 3D editing in a single forward pass. Our key insight is to condition the video generation model on a single edited view to generate other consistent edited views for 3D updating directly, thereby bypassing the iterative editing paradigm. Since 3D updating requires edited views to be paired with specific camera poses, we propose motion-preserved noise blending for the video model to generate edited views at predefined camera poses. In addition, we introduce geometry-aware denoising to further enhance multi-view consistency by integrating 3D geometric priors into video models. Extensive experiments demonstrate that our proposed ViP3DE can achieve high-quality 3D editing results even within a single forward pass, significantly outperforming existing methods in both editing quality and speed.", "link": "http://arxiv.org/abs/2511.23172v2", "date": "2025-12-01", "relevancy": 2.4934, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6285}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6223}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Multi-view%20Consistent%203D%20Editing%20with%20Video%20Priors&body=Title%3A%20Fast%20Multi-view%20Consistent%203D%20Editing%20with%20Video%20Priors%0AAuthor%3A%20Liyi%20Chen%20and%20Ruihuang%20Li%20and%20Guowen%20Zhang%20and%20Pengfei%20Wang%20and%20Lei%20Zhang%0AAbstract%3A%20Text-driven%203D%20editing%20enables%20user-friendly%203D%20object%20or%20scene%20editing%20with%20text%20instructions.%20Due%20to%20the%20lack%20of%20multi-view%20consistency%20priors%2C%20existing%20methods%20typically%20resort%20to%20employing%202D%20generation%20or%20editing%20models%20to%20process%20each%20view%20individually%2C%20followed%20by%20iterative%202D-3D-2D%20updating.%20However%2C%20these%20methods%20are%20not%20only%20time-consuming%20but%20also%20prone%20to%20over-smoothed%20results%20because%20the%20different%20editing%20signals%20gathered%20from%20different%20views%20are%20averaged%20during%20the%20iterative%20process.%20In%20this%20paper%2C%20we%20propose%20generative%20Video%20Prior%20based%203D%20Editing%20%28ViP3DE%29%20to%20employ%20the%20temporal%20consistency%20priors%20from%20pre-trained%20video%20generation%20models%20for%20multi-view%20consistent%203D%20editing%20in%20a%20single%20forward%20pass.%20Our%20key%20insight%20is%20to%20condition%20the%20video%20generation%20model%20on%20a%20single%20edited%20view%20to%20generate%20other%20consistent%20edited%20views%20for%203D%20updating%20directly%2C%20thereby%20bypassing%20the%20iterative%20editing%20paradigm.%20Since%203D%20updating%20requires%20edited%20views%20to%20be%20paired%20with%20specific%20camera%20poses%2C%20we%20propose%20motion-preserved%20noise%20blending%20for%20the%20video%20model%20to%20generate%20edited%20views%20at%20predefined%20camera%20poses.%20In%20addition%2C%20we%20introduce%20geometry-aware%20denoising%20to%20further%20enhance%20multi-view%20consistency%20by%20integrating%203D%20geometric%20priors%20into%20video%20models.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20ViP3DE%20can%20achieve%20high-quality%203D%20editing%20results%20even%20within%20a%20single%20forward%20pass%2C%20significantly%20outperforming%20existing%20methods%20in%20both%20editing%20quality%20and%20speed.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23172v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Multi-view%2520Consistent%25203D%2520Editing%2520with%2520Video%2520Priors%26entry.906535625%3DLiyi%2520Chen%2520and%2520Ruihuang%2520Li%2520and%2520Guowen%2520Zhang%2520and%2520Pengfei%2520Wang%2520and%2520Lei%2520Zhang%26entry.1292438233%3DText-driven%25203D%2520editing%2520enables%2520user-friendly%25203D%2520object%2520or%2520scene%2520editing%2520with%2520text%2520instructions.%2520Due%2520to%2520the%2520lack%2520of%2520multi-view%2520consistency%2520priors%252C%2520existing%2520methods%2520typically%2520resort%2520to%2520employing%25202D%2520generation%2520or%2520editing%2520models%2520to%2520process%2520each%2520view%2520individually%252C%2520followed%2520by%2520iterative%25202D-3D-2D%2520updating.%2520However%252C%2520these%2520methods%2520are%2520not%2520only%2520time-consuming%2520but%2520also%2520prone%2520to%2520over-smoothed%2520results%2520because%2520the%2520different%2520editing%2520signals%2520gathered%2520from%2520different%2520views%2520are%2520averaged%2520during%2520the%2520iterative%2520process.%2520In%2520this%2520paper%252C%2520we%2520propose%2520generative%2520Video%2520Prior%2520based%25203D%2520Editing%2520%2528ViP3DE%2529%2520to%2520employ%2520the%2520temporal%2520consistency%2520priors%2520from%2520pre-trained%2520video%2520generation%2520models%2520for%2520multi-view%2520consistent%25203D%2520editing%2520in%2520a%2520single%2520forward%2520pass.%2520Our%2520key%2520insight%2520is%2520to%2520condition%2520the%2520video%2520generation%2520model%2520on%2520a%2520single%2520edited%2520view%2520to%2520generate%2520other%2520consistent%2520edited%2520views%2520for%25203D%2520updating%2520directly%252C%2520thereby%2520bypassing%2520the%2520iterative%2520editing%2520paradigm.%2520Since%25203D%2520updating%2520requires%2520edited%2520views%2520to%2520be%2520paired%2520with%2520specific%2520camera%2520poses%252C%2520we%2520propose%2520motion-preserved%2520noise%2520blending%2520for%2520the%2520video%2520model%2520to%2520generate%2520edited%2520views%2520at%2520predefined%2520camera%2520poses.%2520In%2520addition%252C%2520we%2520introduce%2520geometry-aware%2520denoising%2520to%2520further%2520enhance%2520multi-view%2520consistency%2520by%2520integrating%25203D%2520geometric%2520priors%2520into%2520video%2520models.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520ViP3DE%2520can%2520achieve%2520high-quality%25203D%2520editing%2520results%2520even%2520within%2520a%2520single%2520forward%2520pass%252C%2520significantly%2520outperforming%2520existing%2520methods%2520in%2520both%2520editing%2520quality%2520and%2520speed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23172v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Multi-view%20Consistent%203D%20Editing%20with%20Video%20Priors&entry.906535625=Liyi%20Chen%20and%20Ruihuang%20Li%20and%20Guowen%20Zhang%20and%20Pengfei%20Wang%20and%20Lei%20Zhang&entry.1292438233=Text-driven%203D%20editing%20enables%20user-friendly%203D%20object%20or%20scene%20editing%20with%20text%20instructions.%20Due%20to%20the%20lack%20of%20multi-view%20consistency%20priors%2C%20existing%20methods%20typically%20resort%20to%20employing%202D%20generation%20or%20editing%20models%20to%20process%20each%20view%20individually%2C%20followed%20by%20iterative%202D-3D-2D%20updating.%20However%2C%20these%20methods%20are%20not%20only%20time-consuming%20but%20also%20prone%20to%20over-smoothed%20results%20because%20the%20different%20editing%20signals%20gathered%20from%20different%20views%20are%20averaged%20during%20the%20iterative%20process.%20In%20this%20paper%2C%20we%20propose%20generative%20Video%20Prior%20based%203D%20Editing%20%28ViP3DE%29%20to%20employ%20the%20temporal%20consistency%20priors%20from%20pre-trained%20video%20generation%20models%20for%20multi-view%20consistent%203D%20editing%20in%20a%20single%20forward%20pass.%20Our%20key%20insight%20is%20to%20condition%20the%20video%20generation%20model%20on%20a%20single%20edited%20view%20to%20generate%20other%20consistent%20edited%20views%20for%203D%20updating%20directly%2C%20thereby%20bypassing%20the%20iterative%20editing%20paradigm.%20Since%203D%20updating%20requires%20edited%20views%20to%20be%20paired%20with%20specific%20camera%20poses%2C%20we%20propose%20motion-preserved%20noise%20blending%20for%20the%20video%20model%20to%20generate%20edited%20views%20at%20predefined%20camera%20poses.%20In%20addition%2C%20we%20introduce%20geometry-aware%20denoising%20to%20further%20enhance%20multi-view%20consistency%20by%20integrating%203D%20geometric%20priors%20into%20video%20models.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20ViP3DE%20can%20achieve%20high-quality%203D%20editing%20results%20even%20within%20a%20single%20forward%20pass%2C%20significantly%20outperforming%20existing%20methods%20in%20both%20editing%20quality%20and%20speed.&entry.1838667208=http%3A//arxiv.org/abs/2511.23172v2&entry.124074799=Read"},
{"title": "Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs", "author": "Haifeng Wen and Hong Xing and Osvaldo Simeone", "abstract": "For modern artificial intelligence (AI) applications such as large language models (LLMs), the training paradigm has recently shifted to pre-training followed by fine-tuning. Furthermore, owing to dwindling open repositories of data and thanks to efforts to democratize access to AI models, pre-training is expected to increasingly migrate from the current centralized deployments to federated learning (FL) implementations. Meta-learning provides a general framework in which pre-training and fine-tuning can be formalized. Meta-learning-based personalized FL (meta-pFL) moves beyond basic personalization by targeting generalization to new agents and tasks. This paper studies the generalization performance of meta-pFL for a wireless setting in which the agents participating in the pre-training phase, i.e., meta-learning, are connected via a shared wireless channel to the server. Adopting over-the-air computing, we study the trade-off between generalization to new agents and tasks, on the one hand, and convergence, on the other hand. The trade-off arises from the fact that channel impairments may enhance generalization, while degrading convergence. Extensive numerical results validate the theory.", "link": "http://arxiv.org/abs/2406.11569v6", "date": "2025-12-01", "relevancy": 2.4849, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4816}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-Training%20and%20Personalized%20Fine-Tuning%20via%20Over-the-Air%20Federated%20Meta-Learning%3A%20Convergence-Generalization%20Trade-Offs&body=Title%3A%20Pre-Training%20and%20Personalized%20Fine-Tuning%20via%20Over-the-Air%20Federated%20Meta-Learning%3A%20Convergence-Generalization%20Trade-Offs%0AAuthor%3A%20Haifeng%20Wen%20and%20Hong%20Xing%20and%20Osvaldo%20Simeone%0AAbstract%3A%20For%20modern%20artificial%20intelligence%20%28AI%29%20applications%20such%20as%20large%20language%20models%20%28LLMs%29%2C%20the%20training%20paradigm%20has%20recently%20shifted%20to%20pre-training%20followed%20by%20fine-tuning.%20Furthermore%2C%20owing%20to%20dwindling%20open%20repositories%20of%20data%20and%20thanks%20to%20efforts%20to%20democratize%20access%20to%20AI%20models%2C%20pre-training%20is%20expected%20to%20increasingly%20migrate%20from%20the%20current%20centralized%20deployments%20to%20federated%20learning%20%28FL%29%20implementations.%20Meta-learning%20provides%20a%20general%20framework%20in%20which%20pre-training%20and%20fine-tuning%20can%20be%20formalized.%20Meta-learning-based%20personalized%20FL%20%28meta-pFL%29%20moves%20beyond%20basic%20personalization%20by%20targeting%20generalization%20to%20new%20agents%20and%20tasks.%20This%20paper%20studies%20the%20generalization%20performance%20of%20meta-pFL%20for%20a%20wireless%20setting%20in%20which%20the%20agents%20participating%20in%20the%20pre-training%20phase%2C%20i.e.%2C%20meta-learning%2C%20are%20connected%20via%20a%20shared%20wireless%20channel%20to%20the%20server.%20Adopting%20over-the-air%20computing%2C%20we%20study%20the%20trade-off%20between%20generalization%20to%20new%20agents%20and%20tasks%2C%20on%20the%20one%20hand%2C%20and%20convergence%2C%20on%20the%20other%20hand.%20The%20trade-off%20arises%20from%20the%20fact%20that%20channel%20impairments%20may%20enhance%20generalization%2C%20while%20degrading%20convergence.%20Extensive%20numerical%20results%20validate%20the%20theory.%0ALink%3A%20http%3A//arxiv.org/abs/2406.11569v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-Training%2520and%2520Personalized%2520Fine-Tuning%2520via%2520Over-the-Air%2520Federated%2520Meta-Learning%253A%2520Convergence-Generalization%2520Trade-Offs%26entry.906535625%3DHaifeng%2520Wen%2520and%2520Hong%2520Xing%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3DFor%2520modern%2520artificial%2520intelligence%2520%2528AI%2529%2520applications%2520such%2520as%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520the%2520training%2520paradigm%2520has%2520recently%2520shifted%2520to%2520pre-training%2520followed%2520by%2520fine-tuning.%2520Furthermore%252C%2520owing%2520to%2520dwindling%2520open%2520repositories%2520of%2520data%2520and%2520thanks%2520to%2520efforts%2520to%2520democratize%2520access%2520to%2520AI%2520models%252C%2520pre-training%2520is%2520expected%2520to%2520increasingly%2520migrate%2520from%2520the%2520current%2520centralized%2520deployments%2520to%2520federated%2520learning%2520%2528FL%2529%2520implementations.%2520Meta-learning%2520provides%2520a%2520general%2520framework%2520in%2520which%2520pre-training%2520and%2520fine-tuning%2520can%2520be%2520formalized.%2520Meta-learning-based%2520personalized%2520FL%2520%2528meta-pFL%2529%2520moves%2520beyond%2520basic%2520personalization%2520by%2520targeting%2520generalization%2520to%2520new%2520agents%2520and%2520tasks.%2520This%2520paper%2520studies%2520the%2520generalization%2520performance%2520of%2520meta-pFL%2520for%2520a%2520wireless%2520setting%2520in%2520which%2520the%2520agents%2520participating%2520in%2520the%2520pre-training%2520phase%252C%2520i.e.%252C%2520meta-learning%252C%2520are%2520connected%2520via%2520a%2520shared%2520wireless%2520channel%2520to%2520the%2520server.%2520Adopting%2520over-the-air%2520computing%252C%2520we%2520study%2520the%2520trade-off%2520between%2520generalization%2520to%2520new%2520agents%2520and%2520tasks%252C%2520on%2520the%2520one%2520hand%252C%2520and%2520convergence%252C%2520on%2520the%2520other%2520hand.%2520The%2520trade-off%2520arises%2520from%2520the%2520fact%2520that%2520channel%2520impairments%2520may%2520enhance%2520generalization%252C%2520while%2520degrading%2520convergence.%2520Extensive%2520numerical%2520results%2520validate%2520the%2520theory.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11569v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-Training%20and%20Personalized%20Fine-Tuning%20via%20Over-the-Air%20Federated%20Meta-Learning%3A%20Convergence-Generalization%20Trade-Offs&entry.906535625=Haifeng%20Wen%20and%20Hong%20Xing%20and%20Osvaldo%20Simeone&entry.1292438233=For%20modern%20artificial%20intelligence%20%28AI%29%20applications%20such%20as%20large%20language%20models%20%28LLMs%29%2C%20the%20training%20paradigm%20has%20recently%20shifted%20to%20pre-training%20followed%20by%20fine-tuning.%20Furthermore%2C%20owing%20to%20dwindling%20open%20repositories%20of%20data%20and%20thanks%20to%20efforts%20to%20democratize%20access%20to%20AI%20models%2C%20pre-training%20is%20expected%20to%20increasingly%20migrate%20from%20the%20current%20centralized%20deployments%20to%20federated%20learning%20%28FL%29%20implementations.%20Meta-learning%20provides%20a%20general%20framework%20in%20which%20pre-training%20and%20fine-tuning%20can%20be%20formalized.%20Meta-learning-based%20personalized%20FL%20%28meta-pFL%29%20moves%20beyond%20basic%20personalization%20by%20targeting%20generalization%20to%20new%20agents%20and%20tasks.%20This%20paper%20studies%20the%20generalization%20performance%20of%20meta-pFL%20for%20a%20wireless%20setting%20in%20which%20the%20agents%20participating%20in%20the%20pre-training%20phase%2C%20i.e.%2C%20meta-learning%2C%20are%20connected%20via%20a%20shared%20wireless%20channel%20to%20the%20server.%20Adopting%20over-the-air%20computing%2C%20we%20study%20the%20trade-off%20between%20generalization%20to%20new%20agents%20and%20tasks%2C%20on%20the%20one%20hand%2C%20and%20convergence%2C%20on%20the%20other%20hand.%20The%20trade-off%20arises%20from%20the%20fact%20that%20channel%20impairments%20may%20enhance%20generalization%2C%20while%20degrading%20convergence.%20Extensive%20numerical%20results%20validate%20the%20theory.&entry.1838667208=http%3A//arxiv.org/abs/2406.11569v6&entry.124074799=Read"},
{"title": "Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager", "author": "Fengfeng Li and Zhou Feng and Hongliang Wu and Hao Zhang and Han Tian and Peng Liu and Lixin Yuan", "abstract": "In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction. This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm. The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment. We implement this by employing a combined feature set of the one-dimensional Histogram of Oriented Gradients (HOG1D) and the original signal as the shape descriptor. Field test examples demonstrate that our method achieves precise alignment for images with complex textures, depth shifts, or local scaling. Furthermore, it provides a flexible framework for feature extension, allowing the integration of other descriptors tailored to specific geological features.", "link": "http://arxiv.org/abs/2512.01611v1", "date": "2025-12-01", "relevancy": 2.4756, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5286}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.515}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Matching%20Method%20Based%20on%20ShapeDTW%20for%20Oil-Based%20Mud%20Imager&body=Title%3A%20Depth%20Matching%20Method%20Based%20on%20ShapeDTW%20for%20Oil-Based%20Mud%20Imager%0AAuthor%3A%20Fengfeng%20Li%20and%20Zhou%20Feng%20and%20Hongliang%20Wu%20and%20Hao%20Zhang%20and%20Han%20Tian%20and%20Peng%20Liu%20and%20Lixin%20Yuan%0AAbstract%3A%20In%20well%20logging%20operations%20using%20the%20oil-based%20mud%20%28OBM%29%20microresistivity%20imager%2C%20which%20employs%20an%20interleaved%20design%20with%20upper%20and%20lower%20pad%20sets%2C%20depth%20misalignment%20issues%20persist%20between%20the%20pad%20images%20even%20after%20velocity%20correction.%20This%20paper%20presents%20a%20depth%20matching%20method%20for%20borehole%20images%20based%20on%20the%20Shape%20Dynamic%20Time%20Warping%20%28ShapeDTW%29%20algorithm.%20The%20method%20extracts%20local%20shape%20features%20to%20construct%20a%20morphologically%20sensitive%20distance%20matrix%2C%20better%20preserving%20structural%20similarity%20between%20sequences%20during%20alignment.%20We%20implement%20this%20by%20employing%20a%20combined%20feature%20set%20of%20the%20one-dimensional%20Histogram%20of%20Oriented%20Gradients%20%28HOG1D%29%20and%20the%20original%20signal%20as%20the%20shape%20descriptor.%20Field%20test%20examples%20demonstrate%20that%20our%20method%20achieves%20precise%20alignment%20for%20images%20with%20complex%20textures%2C%20depth%20shifts%2C%20or%20local%20scaling.%20Furthermore%2C%20it%20provides%20a%20flexible%20framework%20for%20feature%20extension%2C%20allowing%20the%20integration%20of%20other%20descriptors%20tailored%20to%20specific%20geological%20features.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Matching%2520Method%2520Based%2520on%2520ShapeDTW%2520for%2520Oil-Based%2520Mud%2520Imager%26entry.906535625%3DFengfeng%2520Li%2520and%2520Zhou%2520Feng%2520and%2520Hongliang%2520Wu%2520and%2520Hao%2520Zhang%2520and%2520Han%2520Tian%2520and%2520Peng%2520Liu%2520and%2520Lixin%2520Yuan%26entry.1292438233%3DIn%2520well%2520logging%2520operations%2520using%2520the%2520oil-based%2520mud%2520%2528OBM%2529%2520microresistivity%2520imager%252C%2520which%2520employs%2520an%2520interleaved%2520design%2520with%2520upper%2520and%2520lower%2520pad%2520sets%252C%2520depth%2520misalignment%2520issues%2520persist%2520between%2520the%2520pad%2520images%2520even%2520after%2520velocity%2520correction.%2520This%2520paper%2520presents%2520a%2520depth%2520matching%2520method%2520for%2520borehole%2520images%2520based%2520on%2520the%2520Shape%2520Dynamic%2520Time%2520Warping%2520%2528ShapeDTW%2529%2520algorithm.%2520The%2520method%2520extracts%2520local%2520shape%2520features%2520to%2520construct%2520a%2520morphologically%2520sensitive%2520distance%2520matrix%252C%2520better%2520preserving%2520structural%2520similarity%2520between%2520sequences%2520during%2520alignment.%2520We%2520implement%2520this%2520by%2520employing%2520a%2520combined%2520feature%2520set%2520of%2520the%2520one-dimensional%2520Histogram%2520of%2520Oriented%2520Gradients%2520%2528HOG1D%2529%2520and%2520the%2520original%2520signal%2520as%2520the%2520shape%2520descriptor.%2520Field%2520test%2520examples%2520demonstrate%2520that%2520our%2520method%2520achieves%2520precise%2520alignment%2520for%2520images%2520with%2520complex%2520textures%252C%2520depth%2520shifts%252C%2520or%2520local%2520scaling.%2520Furthermore%252C%2520it%2520provides%2520a%2520flexible%2520framework%2520for%2520feature%2520extension%252C%2520allowing%2520the%2520integration%2520of%2520other%2520descriptors%2520tailored%2520to%2520specific%2520geological%2520features.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Matching%20Method%20Based%20on%20ShapeDTW%20for%20Oil-Based%20Mud%20Imager&entry.906535625=Fengfeng%20Li%20and%20Zhou%20Feng%20and%20Hongliang%20Wu%20and%20Hao%20Zhang%20and%20Han%20Tian%20and%20Peng%20Liu%20and%20Lixin%20Yuan&entry.1292438233=In%20well%20logging%20operations%20using%20the%20oil-based%20mud%20%28OBM%29%20microresistivity%20imager%2C%20which%20employs%20an%20interleaved%20design%20with%20upper%20and%20lower%20pad%20sets%2C%20depth%20misalignment%20issues%20persist%20between%20the%20pad%20images%20even%20after%20velocity%20correction.%20This%20paper%20presents%20a%20depth%20matching%20method%20for%20borehole%20images%20based%20on%20the%20Shape%20Dynamic%20Time%20Warping%20%28ShapeDTW%29%20algorithm.%20The%20method%20extracts%20local%20shape%20features%20to%20construct%20a%20morphologically%20sensitive%20distance%20matrix%2C%20better%20preserving%20structural%20similarity%20between%20sequences%20during%20alignment.%20We%20implement%20this%20by%20employing%20a%20combined%20feature%20set%20of%20the%20one-dimensional%20Histogram%20of%20Oriented%20Gradients%20%28HOG1D%29%20and%20the%20original%20signal%20as%20the%20shape%20descriptor.%20Field%20test%20examples%20demonstrate%20that%20our%20method%20achieves%20precise%20alignment%20for%20images%20with%20complex%20textures%2C%20depth%20shifts%2C%20or%20local%20scaling.%20Furthermore%2C%20it%20provides%20a%20flexible%20framework%20for%20feature%20extension%2C%20allowing%20the%20integration%20of%20other%20descriptors%20tailored%20to%20specific%20geological%20features.&entry.1838667208=http%3A//arxiv.org/abs/2512.01611v1&entry.124074799=Read"},
{"title": "3EED: Ground Everything Everywhere in 3D", "author": "Rong Li and Yuhao Dong and Tianshuai Hu and Ao Liang and Youquan Liu and Dongyue Lu and Liang Pan and Lingdong Kong and Junwei Liang and Ziwei Liu", "abstract": "Visual grounding in 3D is the key for embodied agents to localize language-referred objects in open-world environments. However, existing benchmarks are limited to indoor focus, single-platform constraints, and small scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We provide over 128,000 objects and 22,000 validated referring expressions across diverse outdoor scenes -- 10x larger than existing datasets. We develop a scalable annotation pipeline combining vision-language model prompting with human verification to ensure high-quality spatial grounding. To support cross-platform learning, we propose platform-aware normalization and cross-modal alignment techniques, and establish benchmark protocols for in-domain and cross-platform evaluations. Our findings reveal significant performance gaps, highlighting the challenges and opportunities of generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released to advance future research in language-driven 3D embodied perception.", "link": "http://arxiv.org/abs/2511.01755v2", "date": "2025-12-01", "relevancy": 2.4736, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6237}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203EED%3A%20Ground%20Everything%20Everywhere%20in%203D&body=Title%3A%203EED%3A%20Ground%20Everything%20Everywhere%20in%203D%0AAuthor%3A%20Rong%20Li%20and%20Yuhao%20Dong%20and%20Tianshuai%20Hu%20and%20Ao%20Liang%20and%20Youquan%20Liu%20and%20Dongyue%20Lu%20and%20Liang%20Pan%20and%20Lingdong%20Kong%20and%20Junwei%20Liang%20and%20Ziwei%20Liu%0AAbstract%3A%20Visual%20grounding%20in%203D%20is%20the%20key%20for%20embodied%20agents%20to%20localize%20language-referred%20objects%20in%20open-world%20environments.%20However%2C%20existing%20benchmarks%20are%20limited%20to%20indoor%20focus%2C%20single-platform%20constraints%2C%20and%20small%20scale.%20We%20introduce%203EED%2C%20a%20multi-platform%2C%20multi-modal%203D%20grounding%20benchmark%20featuring%20RGB%20and%20LiDAR%20data%20from%20vehicle%2C%20drone%2C%20and%20quadruped%20platforms.%20We%20provide%20over%20128%2C000%20objects%20and%2022%2C000%20validated%20referring%20expressions%20across%20diverse%20outdoor%20scenes%20--%2010x%20larger%20than%20existing%20datasets.%20We%20develop%20a%20scalable%20annotation%20pipeline%20combining%20vision-language%20model%20prompting%20with%20human%20verification%20to%20ensure%20high-quality%20spatial%20grounding.%20To%20support%20cross-platform%20learning%2C%20we%20propose%20platform-aware%20normalization%20and%20cross-modal%20alignment%20techniques%2C%20and%20establish%20benchmark%20protocols%20for%20in-domain%20and%20cross-platform%20evaluations.%20Our%20findings%20reveal%20significant%20performance%20gaps%2C%20highlighting%20the%20challenges%20and%20opportunities%20of%20generalizable%203D%20grounding.%20The%203EED%20dataset%20and%20benchmark%20toolkit%20are%20released%20to%20advance%20future%20research%20in%20language-driven%203D%20embodied%20perception.%0ALink%3A%20http%3A//arxiv.org/abs/2511.01755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3EED%253A%2520Ground%2520Everything%2520Everywhere%2520in%25203D%26entry.906535625%3DRong%2520Li%2520and%2520Yuhao%2520Dong%2520and%2520Tianshuai%2520Hu%2520and%2520Ao%2520Liang%2520and%2520Youquan%2520Liu%2520and%2520Dongyue%2520Lu%2520and%2520Liang%2520Pan%2520and%2520Lingdong%2520Kong%2520and%2520Junwei%2520Liang%2520and%2520Ziwei%2520Liu%26entry.1292438233%3DVisual%2520grounding%2520in%25203D%2520is%2520the%2520key%2520for%2520embodied%2520agents%2520to%2520localize%2520language-referred%2520objects%2520in%2520open-world%2520environments.%2520However%252C%2520existing%2520benchmarks%2520are%2520limited%2520to%2520indoor%2520focus%252C%2520single-platform%2520constraints%252C%2520and%2520small%2520scale.%2520We%2520introduce%25203EED%252C%2520a%2520multi-platform%252C%2520multi-modal%25203D%2520grounding%2520benchmark%2520featuring%2520RGB%2520and%2520LiDAR%2520data%2520from%2520vehicle%252C%2520drone%252C%2520and%2520quadruped%2520platforms.%2520We%2520provide%2520over%2520128%252C000%2520objects%2520and%252022%252C000%2520validated%2520referring%2520expressions%2520across%2520diverse%2520outdoor%2520scenes%2520--%252010x%2520larger%2520than%2520existing%2520datasets.%2520We%2520develop%2520a%2520scalable%2520annotation%2520pipeline%2520combining%2520vision-language%2520model%2520prompting%2520with%2520human%2520verification%2520to%2520ensure%2520high-quality%2520spatial%2520grounding.%2520To%2520support%2520cross-platform%2520learning%252C%2520we%2520propose%2520platform-aware%2520normalization%2520and%2520cross-modal%2520alignment%2520techniques%252C%2520and%2520establish%2520benchmark%2520protocols%2520for%2520in-domain%2520and%2520cross-platform%2520evaluations.%2520Our%2520findings%2520reveal%2520significant%2520performance%2520gaps%252C%2520highlighting%2520the%2520challenges%2520and%2520opportunities%2520of%2520generalizable%25203D%2520grounding.%2520The%25203EED%2520dataset%2520and%2520benchmark%2520toolkit%2520are%2520released%2520to%2520advance%2520future%2520research%2520in%2520language-driven%25203D%2520embodied%2520perception.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3EED%3A%20Ground%20Everything%20Everywhere%20in%203D&entry.906535625=Rong%20Li%20and%20Yuhao%20Dong%20and%20Tianshuai%20Hu%20and%20Ao%20Liang%20and%20Youquan%20Liu%20and%20Dongyue%20Lu%20and%20Liang%20Pan%20and%20Lingdong%20Kong%20and%20Junwei%20Liang%20and%20Ziwei%20Liu&entry.1292438233=Visual%20grounding%20in%203D%20is%20the%20key%20for%20embodied%20agents%20to%20localize%20language-referred%20objects%20in%20open-world%20environments.%20However%2C%20existing%20benchmarks%20are%20limited%20to%20indoor%20focus%2C%20single-platform%20constraints%2C%20and%20small%20scale.%20We%20introduce%203EED%2C%20a%20multi-platform%2C%20multi-modal%203D%20grounding%20benchmark%20featuring%20RGB%20and%20LiDAR%20data%20from%20vehicle%2C%20drone%2C%20and%20quadruped%20platforms.%20We%20provide%20over%20128%2C000%20objects%20and%2022%2C000%20validated%20referring%20expressions%20across%20diverse%20outdoor%20scenes%20--%2010x%20larger%20than%20existing%20datasets.%20We%20develop%20a%20scalable%20annotation%20pipeline%20combining%20vision-language%20model%20prompting%20with%20human%20verification%20to%20ensure%20high-quality%20spatial%20grounding.%20To%20support%20cross-platform%20learning%2C%20we%20propose%20platform-aware%20normalization%20and%20cross-modal%20alignment%20techniques%2C%20and%20establish%20benchmark%20protocols%20for%20in-domain%20and%20cross-platform%20evaluations.%20Our%20findings%20reveal%20significant%20performance%20gaps%2C%20highlighting%20the%20challenges%20and%20opportunities%20of%20generalizable%203D%20grounding.%20The%203EED%20dataset%20and%20benchmark%20toolkit%20are%20released%20to%20advance%20future%20research%20in%20language-driven%203D%20embodied%20perception.&entry.1838667208=http%3A//arxiv.org/abs/2511.01755v2&entry.124074799=Read"},
{"title": "Robust Detection of Synthetic Tabular Data under Schema Variability", "author": "G. Charbel N. Kindji and Elisa Fromont and Lina Maria Rojas-Barahona and Tanguy Urvoy", "abstract": "The rise of powerful generative models has sparked concerns over data authenticity. While detection methods have been extensively developed for images and text, the case of tabular data, despite its ubiquity, has been largely overlooked. Yet, detecting synthetic tabular data is especially challenging due to its heterogeneous structure and unseen formats at test time. We address the underexplored task of detecting synthetic tabular data ''in the wild'', i.e. when the detector is deployed on tables with variable and previously unseen schemas. We introduce a novel datum-wise transformer architecture that significantly outperforms the only previously published baseline, improving both AUC and accuracy by 7 points. By incorporating a table-adaptation component, our model gains an additional 7 accuracy points, demonstrating enhanced robustness. This work provides the first strong evidence that detecting synthetic tabular data in real-world conditions is feasible, and demonstrates substantial improvements over previous approaches. Following acceptance of the paper, we are finalizing the administrative and licensing procedures necessary for releasing the source code. This extended version will be updated as soon as the release is complete.", "link": "http://arxiv.org/abs/2509.00092v2", "date": "2025-12-01", "relevancy": 2.469, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5057}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.496}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Detection%20of%20Synthetic%20Tabular%20Data%20under%20Schema%20Variability&body=Title%3A%20Robust%20Detection%20of%20Synthetic%20Tabular%20Data%20under%20Schema%20Variability%0AAuthor%3A%20G.%20Charbel%20N.%20Kindji%20and%20Elisa%20Fromont%20and%20Lina%20Maria%20Rojas-Barahona%20and%20Tanguy%20Urvoy%0AAbstract%3A%20The%20rise%20of%20powerful%20generative%20models%20has%20sparked%20concerns%20over%20data%20authenticity.%20While%20detection%20methods%20have%20been%20extensively%20developed%20for%20images%20and%20text%2C%20the%20case%20of%20tabular%20data%2C%20despite%20its%20ubiquity%2C%20has%20been%20largely%20overlooked.%20Yet%2C%20detecting%20synthetic%20tabular%20data%20is%20especially%20challenging%20due%20to%20its%20heterogeneous%20structure%20and%20unseen%20formats%20at%20test%20time.%20We%20address%20the%20underexplored%20task%20of%20detecting%20synthetic%20tabular%20data%20%27%27in%20the%20wild%27%27%2C%20i.e.%20when%20the%20detector%20is%20deployed%20on%20tables%20with%20variable%20and%20previously%20unseen%20schemas.%20We%20introduce%20a%20novel%20datum-wise%20transformer%20architecture%20that%20significantly%20outperforms%20the%20only%20previously%20published%20baseline%2C%20improving%20both%20AUC%20and%20accuracy%20by%207%20points.%20By%20incorporating%20a%20table-adaptation%20component%2C%20our%20model%20gains%20an%20additional%207%20accuracy%20points%2C%20demonstrating%20enhanced%20robustness.%20This%20work%20provides%20the%20first%20strong%20evidence%20that%20detecting%20synthetic%20tabular%20data%20in%20real-world%20conditions%20is%20feasible%2C%20and%20demonstrates%20substantial%20improvements%20over%20previous%20approaches.%20Following%20acceptance%20of%20the%20paper%2C%20we%20are%20finalizing%20the%20administrative%20and%20licensing%20procedures%20necessary%20for%20releasing%20the%20source%20code.%20This%20extended%20version%20will%20be%20updated%20as%20soon%20as%20the%20release%20is%20complete.%0ALink%3A%20http%3A//arxiv.org/abs/2509.00092v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Detection%2520of%2520Synthetic%2520Tabular%2520Data%2520under%2520Schema%2520Variability%26entry.906535625%3DG.%2520Charbel%2520N.%2520Kindji%2520and%2520Elisa%2520Fromont%2520and%2520Lina%2520Maria%2520Rojas-Barahona%2520and%2520Tanguy%2520Urvoy%26entry.1292438233%3DThe%2520rise%2520of%2520powerful%2520generative%2520models%2520has%2520sparked%2520concerns%2520over%2520data%2520authenticity.%2520While%2520detection%2520methods%2520have%2520been%2520extensively%2520developed%2520for%2520images%2520and%2520text%252C%2520the%2520case%2520of%2520tabular%2520data%252C%2520despite%2520its%2520ubiquity%252C%2520has%2520been%2520largely%2520overlooked.%2520Yet%252C%2520detecting%2520synthetic%2520tabular%2520data%2520is%2520especially%2520challenging%2520due%2520to%2520its%2520heterogeneous%2520structure%2520and%2520unseen%2520formats%2520at%2520test%2520time.%2520We%2520address%2520the%2520underexplored%2520task%2520of%2520detecting%2520synthetic%2520tabular%2520data%2520%2527%2527in%2520the%2520wild%2527%2527%252C%2520i.e.%2520when%2520the%2520detector%2520is%2520deployed%2520on%2520tables%2520with%2520variable%2520and%2520previously%2520unseen%2520schemas.%2520We%2520introduce%2520a%2520novel%2520datum-wise%2520transformer%2520architecture%2520that%2520significantly%2520outperforms%2520the%2520only%2520previously%2520published%2520baseline%252C%2520improving%2520both%2520AUC%2520and%2520accuracy%2520by%25207%2520points.%2520By%2520incorporating%2520a%2520table-adaptation%2520component%252C%2520our%2520model%2520gains%2520an%2520additional%25207%2520accuracy%2520points%252C%2520demonstrating%2520enhanced%2520robustness.%2520This%2520work%2520provides%2520the%2520first%2520strong%2520evidence%2520that%2520detecting%2520synthetic%2520tabular%2520data%2520in%2520real-world%2520conditions%2520is%2520feasible%252C%2520and%2520demonstrates%2520substantial%2520improvements%2520over%2520previous%2520approaches.%2520Following%2520acceptance%2520of%2520the%2520paper%252C%2520we%2520are%2520finalizing%2520the%2520administrative%2520and%2520licensing%2520procedures%2520necessary%2520for%2520releasing%2520the%2520source%2520code.%2520This%2520extended%2520version%2520will%2520be%2520updated%2520as%2520soon%2520as%2520the%2520release%2520is%2520complete.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00092v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Detection%20of%20Synthetic%20Tabular%20Data%20under%20Schema%20Variability&entry.906535625=G.%20Charbel%20N.%20Kindji%20and%20Elisa%20Fromont%20and%20Lina%20Maria%20Rojas-Barahona%20and%20Tanguy%20Urvoy&entry.1292438233=The%20rise%20of%20powerful%20generative%20models%20has%20sparked%20concerns%20over%20data%20authenticity.%20While%20detection%20methods%20have%20been%20extensively%20developed%20for%20images%20and%20text%2C%20the%20case%20of%20tabular%20data%2C%20despite%20its%20ubiquity%2C%20has%20been%20largely%20overlooked.%20Yet%2C%20detecting%20synthetic%20tabular%20data%20is%20especially%20challenging%20due%20to%20its%20heterogeneous%20structure%20and%20unseen%20formats%20at%20test%20time.%20We%20address%20the%20underexplored%20task%20of%20detecting%20synthetic%20tabular%20data%20%27%27in%20the%20wild%27%27%2C%20i.e.%20when%20the%20detector%20is%20deployed%20on%20tables%20with%20variable%20and%20previously%20unseen%20schemas.%20We%20introduce%20a%20novel%20datum-wise%20transformer%20architecture%20that%20significantly%20outperforms%20the%20only%20previously%20published%20baseline%2C%20improving%20both%20AUC%20and%20accuracy%20by%207%20points.%20By%20incorporating%20a%20table-adaptation%20component%2C%20our%20model%20gains%20an%20additional%207%20accuracy%20points%2C%20demonstrating%20enhanced%20robustness.%20This%20work%20provides%20the%20first%20strong%20evidence%20that%20detecting%20synthetic%20tabular%20data%20in%20real-world%20conditions%20is%20feasible%2C%20and%20demonstrates%20substantial%20improvements%20over%20previous%20approaches.%20Following%20acceptance%20of%20the%20paper%2C%20we%20are%20finalizing%20the%20administrative%20and%20licensing%20procedures%20necessary%20for%20releasing%20the%20source%20code.%20This%20extended%20version%20will%20be%20updated%20as%20soon%20as%20the%20release%20is%20complete.&entry.1838667208=http%3A//arxiv.org/abs/2509.00092v2&entry.124074799=Read"},
{"title": "IAEmu: Learning Galaxy Intrinsic Alignment Correlations", "author": "Sneh Pandya and Yuanyuan Yang and Nicholas Van Alfen and Jonathan Blazek and Robin Walters", "abstract": "The intrinsic alignments (IA) of galaxies, a key contaminant in weak lensing analyses, arise from correlations in galaxy shapes driven by tidal interactions and galaxy formation processes. Accurate IA modeling is essential for robust cosmological inference, but current approaches rely on perturbative methods that break down on nonlinear scales or on expensive simulations. We introduce IAEmu, a neural network-based emulator that predicts the galaxy position-position ($\u03be$), position-orientation ($\u03c9$), and orientation-orientation ($\u03b7$) correlation functions and their uncertainties using mock catalogs based on the halo occupation distribution (HOD) framework. Compared to simulations, IAEmu achieves ~3% average error for $\u03be$ and ~5% for $\u03c9$, while capturing the stochasticity of $\u03b7$ without overfitting. The emulator provides both aleatoric and epistemic uncertainties, helping identify regions where predictions may be less reliable. We also demonstrate generalization to non-HOD alignment signals by fitting to IllustrisTNG hydrodynamical simulation data. As a fully differentiable neural network, IAEmu enables $\\sim$10,000$\\times$ speed-ups in mapping HOD parameters to correlation functions on GPUs, compared to CPU-based simulations. This acceleration facilitates inverse modeling via gradient-based sampling, making IAEmu a powerful surrogate model for galaxy bias and IA studies with direct applications to Stage IV weak lensing surveys.", "link": "http://arxiv.org/abs/2504.05235v4", "date": "2025-12-01", "relevancy": 2.4689, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5335}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4751}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IAEmu%3A%20Learning%20Galaxy%20Intrinsic%20Alignment%20Correlations&body=Title%3A%20IAEmu%3A%20Learning%20Galaxy%20Intrinsic%20Alignment%20Correlations%0AAuthor%3A%20Sneh%20Pandya%20and%20Yuanyuan%20Yang%20and%20Nicholas%20Van%20Alfen%20and%20Jonathan%20Blazek%20and%20Robin%20Walters%0AAbstract%3A%20The%20intrinsic%20alignments%20%28IA%29%20of%20galaxies%2C%20a%20key%20contaminant%20in%20weak%20lensing%20analyses%2C%20arise%20from%20correlations%20in%20galaxy%20shapes%20driven%20by%20tidal%20interactions%20and%20galaxy%20formation%20processes.%20Accurate%20IA%20modeling%20is%20essential%20for%20robust%20cosmological%20inference%2C%20but%20current%20approaches%20rely%20on%20perturbative%20methods%20that%20break%20down%20on%20nonlinear%20scales%20or%20on%20expensive%20simulations.%20We%20introduce%20IAEmu%2C%20a%20neural%20network-based%20emulator%20that%20predicts%20the%20galaxy%20position-position%20%28%24%CE%BE%24%29%2C%20position-orientation%20%28%24%CF%89%24%29%2C%20and%20orientation-orientation%20%28%24%CE%B7%24%29%20correlation%20functions%20and%20their%20uncertainties%20using%20mock%20catalogs%20based%20on%20the%20halo%20occupation%20distribution%20%28HOD%29%20framework.%20Compared%20to%20simulations%2C%20IAEmu%20achieves%20~3%25%20average%20error%20for%20%24%CE%BE%24%20and%20~5%25%20for%20%24%CF%89%24%2C%20while%20capturing%20the%20stochasticity%20of%20%24%CE%B7%24%20without%20overfitting.%20The%20emulator%20provides%20both%20aleatoric%20and%20epistemic%20uncertainties%2C%20helping%20identify%20regions%20where%20predictions%20may%20be%20less%20reliable.%20We%20also%20demonstrate%20generalization%20to%20non-HOD%20alignment%20signals%20by%20fitting%20to%20IllustrisTNG%20hydrodynamical%20simulation%20data.%20As%20a%20fully%20differentiable%20neural%20network%2C%20IAEmu%20enables%20%24%5Csim%2410%2C000%24%5Ctimes%24%20speed-ups%20in%20mapping%20HOD%20parameters%20to%20correlation%20functions%20on%20GPUs%2C%20compared%20to%20CPU-based%20simulations.%20This%20acceleration%20facilitates%20inverse%20modeling%20via%20gradient-based%20sampling%2C%20making%20IAEmu%20a%20powerful%20surrogate%20model%20for%20galaxy%20bias%20and%20IA%20studies%20with%20direct%20applications%20to%20Stage%20IV%20weak%20lensing%20surveys.%0ALink%3A%20http%3A//arxiv.org/abs/2504.05235v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIAEmu%253A%2520Learning%2520Galaxy%2520Intrinsic%2520Alignment%2520Correlations%26entry.906535625%3DSneh%2520Pandya%2520and%2520Yuanyuan%2520Yang%2520and%2520Nicholas%2520Van%2520Alfen%2520and%2520Jonathan%2520Blazek%2520and%2520Robin%2520Walters%26entry.1292438233%3DThe%2520intrinsic%2520alignments%2520%2528IA%2529%2520of%2520galaxies%252C%2520a%2520key%2520contaminant%2520in%2520weak%2520lensing%2520analyses%252C%2520arise%2520from%2520correlations%2520in%2520galaxy%2520shapes%2520driven%2520by%2520tidal%2520interactions%2520and%2520galaxy%2520formation%2520processes.%2520Accurate%2520IA%2520modeling%2520is%2520essential%2520for%2520robust%2520cosmological%2520inference%252C%2520but%2520current%2520approaches%2520rely%2520on%2520perturbative%2520methods%2520that%2520break%2520down%2520on%2520nonlinear%2520scales%2520or%2520on%2520expensive%2520simulations.%2520We%2520introduce%2520IAEmu%252C%2520a%2520neural%2520network-based%2520emulator%2520that%2520predicts%2520the%2520galaxy%2520position-position%2520%2528%2524%25CE%25BE%2524%2529%252C%2520position-orientation%2520%2528%2524%25CF%2589%2524%2529%252C%2520and%2520orientation-orientation%2520%2528%2524%25CE%25B7%2524%2529%2520correlation%2520functions%2520and%2520their%2520uncertainties%2520using%2520mock%2520catalogs%2520based%2520on%2520the%2520halo%2520occupation%2520distribution%2520%2528HOD%2529%2520framework.%2520Compared%2520to%2520simulations%252C%2520IAEmu%2520achieves%2520~3%2525%2520average%2520error%2520for%2520%2524%25CE%25BE%2524%2520and%2520~5%2525%2520for%2520%2524%25CF%2589%2524%252C%2520while%2520capturing%2520the%2520stochasticity%2520of%2520%2524%25CE%25B7%2524%2520without%2520overfitting.%2520The%2520emulator%2520provides%2520both%2520aleatoric%2520and%2520epistemic%2520uncertainties%252C%2520helping%2520identify%2520regions%2520where%2520predictions%2520may%2520be%2520less%2520reliable.%2520We%2520also%2520demonstrate%2520generalization%2520to%2520non-HOD%2520alignment%2520signals%2520by%2520fitting%2520to%2520IllustrisTNG%2520hydrodynamical%2520simulation%2520data.%2520As%2520a%2520fully%2520differentiable%2520neural%2520network%252C%2520IAEmu%2520enables%2520%2524%255Csim%252410%252C000%2524%255Ctimes%2524%2520speed-ups%2520in%2520mapping%2520HOD%2520parameters%2520to%2520correlation%2520functions%2520on%2520GPUs%252C%2520compared%2520to%2520CPU-based%2520simulations.%2520This%2520acceleration%2520facilitates%2520inverse%2520modeling%2520via%2520gradient-based%2520sampling%252C%2520making%2520IAEmu%2520a%2520powerful%2520surrogate%2520model%2520for%2520galaxy%2520bias%2520and%2520IA%2520studies%2520with%2520direct%2520applications%2520to%2520Stage%2520IV%2520weak%2520lensing%2520surveys.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05235v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IAEmu%3A%20Learning%20Galaxy%20Intrinsic%20Alignment%20Correlations&entry.906535625=Sneh%20Pandya%20and%20Yuanyuan%20Yang%20and%20Nicholas%20Van%20Alfen%20and%20Jonathan%20Blazek%20and%20Robin%20Walters&entry.1292438233=The%20intrinsic%20alignments%20%28IA%29%20of%20galaxies%2C%20a%20key%20contaminant%20in%20weak%20lensing%20analyses%2C%20arise%20from%20correlations%20in%20galaxy%20shapes%20driven%20by%20tidal%20interactions%20and%20galaxy%20formation%20processes.%20Accurate%20IA%20modeling%20is%20essential%20for%20robust%20cosmological%20inference%2C%20but%20current%20approaches%20rely%20on%20perturbative%20methods%20that%20break%20down%20on%20nonlinear%20scales%20or%20on%20expensive%20simulations.%20We%20introduce%20IAEmu%2C%20a%20neural%20network-based%20emulator%20that%20predicts%20the%20galaxy%20position-position%20%28%24%CE%BE%24%29%2C%20position-orientation%20%28%24%CF%89%24%29%2C%20and%20orientation-orientation%20%28%24%CE%B7%24%29%20correlation%20functions%20and%20their%20uncertainties%20using%20mock%20catalogs%20based%20on%20the%20halo%20occupation%20distribution%20%28HOD%29%20framework.%20Compared%20to%20simulations%2C%20IAEmu%20achieves%20~3%25%20average%20error%20for%20%24%CE%BE%24%20and%20~5%25%20for%20%24%CF%89%24%2C%20while%20capturing%20the%20stochasticity%20of%20%24%CE%B7%24%20without%20overfitting.%20The%20emulator%20provides%20both%20aleatoric%20and%20epistemic%20uncertainties%2C%20helping%20identify%20regions%20where%20predictions%20may%20be%20less%20reliable.%20We%20also%20demonstrate%20generalization%20to%20non-HOD%20alignment%20signals%20by%20fitting%20to%20IllustrisTNG%20hydrodynamical%20simulation%20data.%20As%20a%20fully%20differentiable%20neural%20network%2C%20IAEmu%20enables%20%24%5Csim%2410%2C000%24%5Ctimes%24%20speed-ups%20in%20mapping%20HOD%20parameters%20to%20correlation%20functions%20on%20GPUs%2C%20compared%20to%20CPU-based%20simulations.%20This%20acceleration%20facilitates%20inverse%20modeling%20via%20gradient-based%20sampling%2C%20making%20IAEmu%20a%20powerful%20surrogate%20model%20for%20galaxy%20bias%20and%20IA%20studies%20with%20direct%20applications%20to%20Stage%20IV%20weak%20lensing%20surveys.&entry.1838667208=http%3A//arxiv.org/abs/2504.05235v4&entry.124074799=Read"},
{"title": "Cross-Lingual Interleaving for Speech Language Models", "author": "Adel Moumen and Guangzhi Sun and Philip C. Woodland", "abstract": "Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.", "link": "http://arxiv.org/abs/2512.01865v1", "date": "2025-12-01", "relevancy": 2.4675, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Lingual%20Interleaving%20for%20Speech%20Language%20Models&body=Title%3A%20Cross-Lingual%20Interleaving%20for%20Speech%20Language%20Models%0AAuthor%3A%20Adel%20Moumen%20and%20Guangzhi%20Sun%20and%20Philip%20C.%20Woodland%0AAbstract%3A%20Spoken%20Language%20Models%20%28SLMs%29%20aim%20to%20learn%20linguistic%20competence%20directly%20from%20speech%20using%20discrete%20units%2C%20widening%20access%20to%20Natural%20Language%20Processing%20%28NLP%29%20technologies%20for%20languages%20with%20limited%20written%20resources.%20However%2C%20progress%20has%20been%20largely%20English-centric%20due%20to%20scarce%20spoken%20evaluation%20benchmarks%20and%20training%20data%2C%20making%20cross-lingual%20learning%20difficult.%20We%20present%20a%20cross-lingual%20interleaving%20method%20that%20mixes%20speech%20tokens%20across%20languages%20without%20textual%20supervision.%20We%20also%20release%20an%20EN-FR%20training%20dataset%2C%20TinyStories%20%28~42k%20hours%29%2C%20together%20with%20EN-FR%20spoken%20StoryCloze%20and%20TopicCloze%20benchmarks%20for%20cross-lingual%20semantic%20evaluation%2C%20both%20synthetically%20generated%20using%20GPT-4.%20On%20360M%20and%201B%20SLMs%20under%20matched%20training-token%20budgets%2C%20interleaving%20improves%20monolingual%20semantic%20accuracy%2C%20enables%20robust%20cross-lingual%20continuation%2C%20and%20strengthens%20cross-lingual%20hidden-state%20alignment.%20Taken%20together%2C%20these%20results%20indicate%20that%20cross-lingual%20interleaving%20is%20a%20simple%2C%20scalable%20route%20to%20building%20multilingual%20SLMs%20that%20understand%20and%20converse%20across%20languages.%20All%20resources%20will%20be%20made%20open-source%20to%20support%20reproducibility.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Lingual%2520Interleaving%2520for%2520Speech%2520Language%2520Models%26entry.906535625%3DAdel%2520Moumen%2520and%2520Guangzhi%2520Sun%2520and%2520Philip%2520C.%2520Woodland%26entry.1292438233%3DSpoken%2520Language%2520Models%2520%2528SLMs%2529%2520aim%2520to%2520learn%2520linguistic%2520competence%2520directly%2520from%2520speech%2520using%2520discrete%2520units%252C%2520widening%2520access%2520to%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520technologies%2520for%2520languages%2520with%2520limited%2520written%2520resources.%2520However%252C%2520progress%2520has%2520been%2520largely%2520English-centric%2520due%2520to%2520scarce%2520spoken%2520evaluation%2520benchmarks%2520and%2520training%2520data%252C%2520making%2520cross-lingual%2520learning%2520difficult.%2520We%2520present%2520a%2520cross-lingual%2520interleaving%2520method%2520that%2520mixes%2520speech%2520tokens%2520across%2520languages%2520without%2520textual%2520supervision.%2520We%2520also%2520release%2520an%2520EN-FR%2520training%2520dataset%252C%2520TinyStories%2520%2528~42k%2520hours%2529%252C%2520together%2520with%2520EN-FR%2520spoken%2520StoryCloze%2520and%2520TopicCloze%2520benchmarks%2520for%2520cross-lingual%2520semantic%2520evaluation%252C%2520both%2520synthetically%2520generated%2520using%2520GPT-4.%2520On%2520360M%2520and%25201B%2520SLMs%2520under%2520matched%2520training-token%2520budgets%252C%2520interleaving%2520improves%2520monolingual%2520semantic%2520accuracy%252C%2520enables%2520robust%2520cross-lingual%2520continuation%252C%2520and%2520strengthens%2520cross-lingual%2520hidden-state%2520alignment.%2520Taken%2520together%252C%2520these%2520results%2520indicate%2520that%2520cross-lingual%2520interleaving%2520is%2520a%2520simple%252C%2520scalable%2520route%2520to%2520building%2520multilingual%2520SLMs%2520that%2520understand%2520and%2520converse%2520across%2520languages.%2520All%2520resources%2520will%2520be%2520made%2520open-source%2520to%2520support%2520reproducibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Lingual%20Interleaving%20for%20Speech%20Language%20Models&entry.906535625=Adel%20Moumen%20and%20Guangzhi%20Sun%20and%20Philip%20C.%20Woodland&entry.1292438233=Spoken%20Language%20Models%20%28SLMs%29%20aim%20to%20learn%20linguistic%20competence%20directly%20from%20speech%20using%20discrete%20units%2C%20widening%20access%20to%20Natural%20Language%20Processing%20%28NLP%29%20technologies%20for%20languages%20with%20limited%20written%20resources.%20However%2C%20progress%20has%20been%20largely%20English-centric%20due%20to%20scarce%20spoken%20evaluation%20benchmarks%20and%20training%20data%2C%20making%20cross-lingual%20learning%20difficult.%20We%20present%20a%20cross-lingual%20interleaving%20method%20that%20mixes%20speech%20tokens%20across%20languages%20without%20textual%20supervision.%20We%20also%20release%20an%20EN-FR%20training%20dataset%2C%20TinyStories%20%28~42k%20hours%29%2C%20together%20with%20EN-FR%20spoken%20StoryCloze%20and%20TopicCloze%20benchmarks%20for%20cross-lingual%20semantic%20evaluation%2C%20both%20synthetically%20generated%20using%20GPT-4.%20On%20360M%20and%201B%20SLMs%20under%20matched%20training-token%20budgets%2C%20interleaving%20improves%20monolingual%20semantic%20accuracy%2C%20enables%20robust%20cross-lingual%20continuation%2C%20and%20strengthens%20cross-lingual%20hidden-state%20alignment.%20Taken%20together%2C%20these%20results%20indicate%20that%20cross-lingual%20interleaving%20is%20a%20simple%2C%20scalable%20route%20to%20building%20multilingual%20SLMs%20that%20understand%20and%20converse%20across%20languages.%20All%20resources%20will%20be%20made%20open-source%20to%20support%20reproducibility.&entry.1838667208=http%3A//arxiv.org/abs/2512.01865v1&entry.124074799=Read"},
{"title": "Q2D2: A Geometry-Aware Audio Codec Leveraging Two-Dimensional Quantization", "author": "Tal Shuster and Eliya Nachmani", "abstract": "Recent neural audio codecs have achieved impressive reconstruction quality, typically relying on quantization methods such as Residual Vector Quantization (RVQ), Vector Quantization (VQ) and Finite Scalar Quantization (FSQ). However, these quantization techniques limit the geometric structure of the latent space, make it harder to capture correlations between features leading to inefficiency in representation learning, codebook utilization and token rate. In this paper we introduce Two Dimensional Quantization (Q2D2), a quantization scheme in which feature pairs are projected onto structured 2D grids such as hexagonal, rhombic, or rectangular tiling and quantized to the nearest grid values, yielding an implicit codebook defined by the product of grid levels, with codebook sizes comparable to conventional methods. Despite its simple geometric formulation, Q2D2 improves audio compression efficiency, with low token rates and high codebook utilization while maintaining state of the art reconstruction quality. Specifically, Q2D2 achieves competitive to superior performance in various objective and subjective reconstruction metrics, across extensive experiments in speech domain compared to state of the art models. Comprehensive ablation studies further confirm the effectiveness of our design choices.", "link": "http://arxiv.org/abs/2512.01537v1", "date": "2025-12-01", "relevancy": 2.462, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4938}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q2D2%3A%20A%20Geometry-Aware%20Audio%20Codec%20Leveraging%20Two-Dimensional%20Quantization&body=Title%3A%20Q2D2%3A%20A%20Geometry-Aware%20Audio%20Codec%20Leveraging%20Two-Dimensional%20Quantization%0AAuthor%3A%20Tal%20Shuster%20and%20Eliya%20Nachmani%0AAbstract%3A%20Recent%20neural%20audio%20codecs%20have%20achieved%20impressive%20reconstruction%20quality%2C%20typically%20relying%20on%20quantization%20methods%20such%20as%20Residual%20Vector%20Quantization%20%28RVQ%29%2C%20Vector%20Quantization%20%28VQ%29%20and%20Finite%20Scalar%20Quantization%20%28FSQ%29.%20However%2C%20these%20quantization%20techniques%20limit%20the%20geometric%20structure%20of%20the%20latent%20space%2C%20make%20it%20harder%20to%20capture%20correlations%20between%20features%20leading%20to%20inefficiency%20in%20representation%20learning%2C%20codebook%20utilization%20and%20token%20rate.%20In%20this%20paper%20we%20introduce%20Two%20Dimensional%20Quantization%20%28Q2D2%29%2C%20a%20quantization%20scheme%20in%20which%20feature%20pairs%20are%20projected%20onto%20structured%202D%20grids%20such%20as%20hexagonal%2C%20rhombic%2C%20or%20rectangular%20tiling%20and%20quantized%20to%20the%20nearest%20grid%20values%2C%20yielding%20an%20implicit%20codebook%20defined%20by%20the%20product%20of%20grid%20levels%2C%20with%20codebook%20sizes%20comparable%20to%20conventional%20methods.%20Despite%20its%20simple%20geometric%20formulation%2C%20Q2D2%20improves%20audio%20compression%20efficiency%2C%20with%20low%20token%20rates%20and%20high%20codebook%20utilization%20while%20maintaining%20state%20of%20the%20art%20reconstruction%20quality.%20Specifically%2C%20Q2D2%20achieves%20competitive%20to%20superior%20performance%20in%20various%20objective%20and%20subjective%20reconstruction%20metrics%2C%20across%20extensive%20experiments%20in%20speech%20domain%20compared%20to%20state%20of%20the%20art%20models.%20Comprehensive%20ablation%20studies%20further%20confirm%20the%20effectiveness%20of%20our%20design%20choices.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ2D2%253A%2520A%2520Geometry-Aware%2520Audio%2520Codec%2520Leveraging%2520Two-Dimensional%2520Quantization%26entry.906535625%3DTal%2520Shuster%2520and%2520Eliya%2520Nachmani%26entry.1292438233%3DRecent%2520neural%2520audio%2520codecs%2520have%2520achieved%2520impressive%2520reconstruction%2520quality%252C%2520typically%2520relying%2520on%2520quantization%2520methods%2520such%2520as%2520Residual%2520Vector%2520Quantization%2520%2528RVQ%2529%252C%2520Vector%2520Quantization%2520%2528VQ%2529%2520and%2520Finite%2520Scalar%2520Quantization%2520%2528FSQ%2529.%2520However%252C%2520these%2520quantization%2520techniques%2520limit%2520the%2520geometric%2520structure%2520of%2520the%2520latent%2520space%252C%2520make%2520it%2520harder%2520to%2520capture%2520correlations%2520between%2520features%2520leading%2520to%2520inefficiency%2520in%2520representation%2520learning%252C%2520codebook%2520utilization%2520and%2520token%2520rate.%2520In%2520this%2520paper%2520we%2520introduce%2520Two%2520Dimensional%2520Quantization%2520%2528Q2D2%2529%252C%2520a%2520quantization%2520scheme%2520in%2520which%2520feature%2520pairs%2520are%2520projected%2520onto%2520structured%25202D%2520grids%2520such%2520as%2520hexagonal%252C%2520rhombic%252C%2520or%2520rectangular%2520tiling%2520and%2520quantized%2520to%2520the%2520nearest%2520grid%2520values%252C%2520yielding%2520an%2520implicit%2520codebook%2520defined%2520by%2520the%2520product%2520of%2520grid%2520levels%252C%2520with%2520codebook%2520sizes%2520comparable%2520to%2520conventional%2520methods.%2520Despite%2520its%2520simple%2520geometric%2520formulation%252C%2520Q2D2%2520improves%2520audio%2520compression%2520efficiency%252C%2520with%2520low%2520token%2520rates%2520and%2520high%2520codebook%2520utilization%2520while%2520maintaining%2520state%2520of%2520the%2520art%2520reconstruction%2520quality.%2520Specifically%252C%2520Q2D2%2520achieves%2520competitive%2520to%2520superior%2520performance%2520in%2520various%2520objective%2520and%2520subjective%2520reconstruction%2520metrics%252C%2520across%2520extensive%2520experiments%2520in%2520speech%2520domain%2520compared%2520to%2520state%2520of%2520the%2520art%2520models.%2520Comprehensive%2520ablation%2520studies%2520further%2520confirm%2520the%2520effectiveness%2520of%2520our%2520design%2520choices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q2D2%3A%20A%20Geometry-Aware%20Audio%20Codec%20Leveraging%20Two-Dimensional%20Quantization&entry.906535625=Tal%20Shuster%20and%20Eliya%20Nachmani&entry.1292438233=Recent%20neural%20audio%20codecs%20have%20achieved%20impressive%20reconstruction%20quality%2C%20typically%20relying%20on%20quantization%20methods%20such%20as%20Residual%20Vector%20Quantization%20%28RVQ%29%2C%20Vector%20Quantization%20%28VQ%29%20and%20Finite%20Scalar%20Quantization%20%28FSQ%29.%20However%2C%20these%20quantization%20techniques%20limit%20the%20geometric%20structure%20of%20the%20latent%20space%2C%20make%20it%20harder%20to%20capture%20correlations%20between%20features%20leading%20to%20inefficiency%20in%20representation%20learning%2C%20codebook%20utilization%20and%20token%20rate.%20In%20this%20paper%20we%20introduce%20Two%20Dimensional%20Quantization%20%28Q2D2%29%2C%20a%20quantization%20scheme%20in%20which%20feature%20pairs%20are%20projected%20onto%20structured%202D%20grids%20such%20as%20hexagonal%2C%20rhombic%2C%20or%20rectangular%20tiling%20and%20quantized%20to%20the%20nearest%20grid%20values%2C%20yielding%20an%20implicit%20codebook%20defined%20by%20the%20product%20of%20grid%20levels%2C%20with%20codebook%20sizes%20comparable%20to%20conventional%20methods.%20Despite%20its%20simple%20geometric%20formulation%2C%20Q2D2%20improves%20audio%20compression%20efficiency%2C%20with%20low%20token%20rates%20and%20high%20codebook%20utilization%20while%20maintaining%20state%20of%20the%20art%20reconstruction%20quality.%20Specifically%2C%20Q2D2%20achieves%20competitive%20to%20superior%20performance%20in%20various%20objective%20and%20subjective%20reconstruction%20metrics%2C%20across%20extensive%20experiments%20in%20speech%20domain%20compared%20to%20state%20of%20the%20art%20models.%20Comprehensive%20ablation%20studies%20further%20confirm%20the%20effectiveness%20of%20our%20design%20choices.&entry.1838667208=http%3A//arxiv.org/abs/2512.01537v1&entry.124074799=Read"},
{"title": "MRI Super-Resolution with Deep Learning: A Comprehensive Survey", "author": "Mohammad Khateri and Serge Vasylechko and Morteza Ghahremani and Liam Timms and Deniz Kocanaogullari and Simon K. Warfield and Camilo Jaimes and Davood Karimi and Alejandra Sierra and Jussi Tohka and Sila Kurugol and Onur Afacan", "abstract": "High-resolution (HR) magnetic resonance imaging (MRI) is crucial for many clinical and research applications. However, achieving it remains costly and constrained by technical trade-offs and experimental limitations. Super-resolution (SR) presents a promising computational approach to overcome these challenges by generating HR images from more affordable low-resolution (LR) scans, potentially improving diagnostic accuracy and efficiency without requiring additional hardware. This survey reviews recent advances in MRI SR techniques, with a focus on deep learning (DL) approaches. It examines DL-based MRI SR methods from the perspectives of computer vision, computational imaging, inverse problems, and MR physics, covering theoretical foundations, architectural designs, learning strategies, benchmark datasets, and performance metrics. We propose a systematic taxonomy to categorize these methods and present an in-depth study of both established and emerging SR techniques applicable to MRI, considering unique challenges in clinical and research contexts. We also highlight open challenges and directions that the community needs to address. Additionally, we provide a collection of essential open-access resources, tools, and tutorials, available on our GitHub: https://github.com/mkhateri/Awesome-MRI-Super-Resolution.\n  IEEE keywords: MRI, Super-Resolution, Deep Learning, Computational Imaging, Inverse Problem, Survey.", "link": "http://arxiv.org/abs/2511.16854v2", "date": "2025-12-01", "relevancy": 2.4493, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4949}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4877}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRI%20Super-Resolution%20with%20Deep%20Learning%3A%20A%20Comprehensive%20Survey&body=Title%3A%20MRI%20Super-Resolution%20with%20Deep%20Learning%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Mohammad%20Khateri%20and%20Serge%20Vasylechko%20and%20Morteza%20Ghahremani%20and%20Liam%20Timms%20and%20Deniz%20Kocanaogullari%20and%20Simon%20K.%20Warfield%20and%20Camilo%20Jaimes%20and%20Davood%20Karimi%20and%20Alejandra%20Sierra%20and%20Jussi%20Tohka%20and%20Sila%20Kurugol%20and%20Onur%20Afacan%0AAbstract%3A%20High-resolution%20%28HR%29%20magnetic%20resonance%20imaging%20%28MRI%29%20is%20crucial%20for%20many%20clinical%20and%20research%20applications.%20However%2C%20achieving%20it%20remains%20costly%20and%20constrained%20by%20technical%20trade-offs%20and%20experimental%20limitations.%20Super-resolution%20%28SR%29%20presents%20a%20promising%20computational%20approach%20to%20overcome%20these%20challenges%20by%20generating%20HR%20images%20from%20more%20affordable%20low-resolution%20%28LR%29%20scans%2C%20potentially%20improving%20diagnostic%20accuracy%20and%20efficiency%20without%20requiring%20additional%20hardware.%20This%20survey%20reviews%20recent%20advances%20in%20MRI%20SR%20techniques%2C%20with%20a%20focus%20on%20deep%20learning%20%28DL%29%20approaches.%20It%20examines%20DL-based%20MRI%20SR%20methods%20from%20the%20perspectives%20of%20computer%20vision%2C%20computational%20imaging%2C%20inverse%20problems%2C%20and%20MR%20physics%2C%20covering%20theoretical%20foundations%2C%20architectural%20designs%2C%20learning%20strategies%2C%20benchmark%20datasets%2C%20and%20performance%20metrics.%20We%20propose%20a%20systematic%20taxonomy%20to%20categorize%20these%20methods%20and%20present%20an%20in-depth%20study%20of%20both%20established%20and%20emerging%20SR%20techniques%20applicable%20to%20MRI%2C%20considering%20unique%20challenges%20in%20clinical%20and%20research%20contexts.%20We%20also%20highlight%20open%20challenges%20and%20directions%20that%20the%20community%20needs%20to%20address.%20Additionally%2C%20we%20provide%20a%20collection%20of%20essential%20open-access%20resources%2C%20tools%2C%20and%20tutorials%2C%20available%20on%20our%20GitHub%3A%20https%3A//github.com/mkhateri/Awesome-MRI-Super-Resolution.%0A%20%20IEEE%20keywords%3A%20MRI%2C%20Super-Resolution%2C%20Deep%20Learning%2C%20Computational%20Imaging%2C%20Inverse%20Problem%2C%20Survey.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16854v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRI%2520Super-Resolution%2520with%2520Deep%2520Learning%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DMohammad%2520Khateri%2520and%2520Serge%2520Vasylechko%2520and%2520Morteza%2520Ghahremani%2520and%2520Liam%2520Timms%2520and%2520Deniz%2520Kocanaogullari%2520and%2520Simon%2520K.%2520Warfield%2520and%2520Camilo%2520Jaimes%2520and%2520Davood%2520Karimi%2520and%2520Alejandra%2520Sierra%2520and%2520Jussi%2520Tohka%2520and%2520Sila%2520Kurugol%2520and%2520Onur%2520Afacan%26entry.1292438233%3DHigh-resolution%2520%2528HR%2529%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520crucial%2520for%2520many%2520clinical%2520and%2520research%2520applications.%2520However%252C%2520achieving%2520it%2520remains%2520costly%2520and%2520constrained%2520by%2520technical%2520trade-offs%2520and%2520experimental%2520limitations.%2520Super-resolution%2520%2528SR%2529%2520presents%2520a%2520promising%2520computational%2520approach%2520to%2520overcome%2520these%2520challenges%2520by%2520generating%2520HR%2520images%2520from%2520more%2520affordable%2520low-resolution%2520%2528LR%2529%2520scans%252C%2520potentially%2520improving%2520diagnostic%2520accuracy%2520and%2520efficiency%2520without%2520requiring%2520additional%2520hardware.%2520This%2520survey%2520reviews%2520recent%2520advances%2520in%2520MRI%2520SR%2520techniques%252C%2520with%2520a%2520focus%2520on%2520deep%2520learning%2520%2528DL%2529%2520approaches.%2520It%2520examines%2520DL-based%2520MRI%2520SR%2520methods%2520from%2520the%2520perspectives%2520of%2520computer%2520vision%252C%2520computational%2520imaging%252C%2520inverse%2520problems%252C%2520and%2520MR%2520physics%252C%2520covering%2520theoretical%2520foundations%252C%2520architectural%2520designs%252C%2520learning%2520strategies%252C%2520benchmark%2520datasets%252C%2520and%2520performance%2520metrics.%2520We%2520propose%2520a%2520systematic%2520taxonomy%2520to%2520categorize%2520these%2520methods%2520and%2520present%2520an%2520in-depth%2520study%2520of%2520both%2520established%2520and%2520emerging%2520SR%2520techniques%2520applicable%2520to%2520MRI%252C%2520considering%2520unique%2520challenges%2520in%2520clinical%2520and%2520research%2520contexts.%2520We%2520also%2520highlight%2520open%2520challenges%2520and%2520directions%2520that%2520the%2520community%2520needs%2520to%2520address.%2520Additionally%252C%2520we%2520provide%2520a%2520collection%2520of%2520essential%2520open-access%2520resources%252C%2520tools%252C%2520and%2520tutorials%252C%2520available%2520on%2520our%2520GitHub%253A%2520https%253A//github.com/mkhateri/Awesome-MRI-Super-Resolution.%250A%2520%2520IEEE%2520keywords%253A%2520MRI%252C%2520Super-Resolution%252C%2520Deep%2520Learning%252C%2520Computational%2520Imaging%252C%2520Inverse%2520Problem%252C%2520Survey.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16854v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRI%20Super-Resolution%20with%20Deep%20Learning%3A%20A%20Comprehensive%20Survey&entry.906535625=Mohammad%20Khateri%20and%20Serge%20Vasylechko%20and%20Morteza%20Ghahremani%20and%20Liam%20Timms%20and%20Deniz%20Kocanaogullari%20and%20Simon%20K.%20Warfield%20and%20Camilo%20Jaimes%20and%20Davood%20Karimi%20and%20Alejandra%20Sierra%20and%20Jussi%20Tohka%20and%20Sila%20Kurugol%20and%20Onur%20Afacan&entry.1292438233=High-resolution%20%28HR%29%20magnetic%20resonance%20imaging%20%28MRI%29%20is%20crucial%20for%20many%20clinical%20and%20research%20applications.%20However%2C%20achieving%20it%20remains%20costly%20and%20constrained%20by%20technical%20trade-offs%20and%20experimental%20limitations.%20Super-resolution%20%28SR%29%20presents%20a%20promising%20computational%20approach%20to%20overcome%20these%20challenges%20by%20generating%20HR%20images%20from%20more%20affordable%20low-resolution%20%28LR%29%20scans%2C%20potentially%20improving%20diagnostic%20accuracy%20and%20efficiency%20without%20requiring%20additional%20hardware.%20This%20survey%20reviews%20recent%20advances%20in%20MRI%20SR%20techniques%2C%20with%20a%20focus%20on%20deep%20learning%20%28DL%29%20approaches.%20It%20examines%20DL-based%20MRI%20SR%20methods%20from%20the%20perspectives%20of%20computer%20vision%2C%20computational%20imaging%2C%20inverse%20problems%2C%20and%20MR%20physics%2C%20covering%20theoretical%20foundations%2C%20architectural%20designs%2C%20learning%20strategies%2C%20benchmark%20datasets%2C%20and%20performance%20metrics.%20We%20propose%20a%20systematic%20taxonomy%20to%20categorize%20these%20methods%20and%20present%20an%20in-depth%20study%20of%20both%20established%20and%20emerging%20SR%20techniques%20applicable%20to%20MRI%2C%20considering%20unique%20challenges%20in%20clinical%20and%20research%20contexts.%20We%20also%20highlight%20open%20challenges%20and%20directions%20that%20the%20community%20needs%20to%20address.%20Additionally%2C%20we%20provide%20a%20collection%20of%20essential%20open-access%20resources%2C%20tools%2C%20and%20tutorials%2C%20available%20on%20our%20GitHub%3A%20https%3A//github.com/mkhateri/Awesome-MRI-Super-Resolution.%0A%20%20IEEE%20keywords%3A%20MRI%2C%20Super-Resolution%2C%20Deep%20Learning%2C%20Computational%20Imaging%2C%20Inverse%20Problem%2C%20Survey.&entry.1838667208=http%3A//arxiv.org/abs/2511.16854v2&entry.124074799=Read"},
{"title": "KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM", "author": "Zaid Nasser and Mikhail Iumanov and Tianhao Li and Maxim Popov and Jaafar Mahmoud and Malik Mohrat and Ilya Obrubov and Ekaterina Derevyanka and Ivan Sosin and Sergey Kolyubin", "abstract": "We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.", "link": "http://arxiv.org/abs/2512.01889v1", "date": "2025-12-01", "relevancy": 2.442, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6219}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6097}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KM-ViPE%3A%20Online%20Tightly%20Coupled%20Vision-Language-Geometry%20Fusion%20for%20Open-Vocabulary%20Semantic%20SLAM&body=Title%3A%20KM-ViPE%3A%20Online%20Tightly%20Coupled%20Vision-Language-Geometry%20Fusion%20for%20Open-Vocabulary%20Semantic%20SLAM%0AAuthor%3A%20Zaid%20Nasser%20and%20Mikhail%20Iumanov%20and%20Tianhao%20Li%20and%20Maxim%20Popov%20and%20Jaafar%20Mahmoud%20and%20Malik%20Mohrat%20and%20Ilya%20Obrubov%20and%20Ekaterina%20Derevyanka%20and%20Ivan%20Sosin%20and%20Sergey%20Kolyubin%0AAbstract%3A%20We%20present%20KM-ViPE%20%28Knowledge%20Mapping%20Video%20Pose%20Engine%29%2C%20a%20real-time%20open-vocabulary%20SLAM%20framework%20for%20uncalibrated%20monocular%20cameras%20in%20dynamic%20environments.%20Unlike%20systems%20requiring%20depth%20sensors%20and%20offline%20calibration%2C%20KM-ViPE%20operates%20directly%20on%20raw%20RGB%20streams%2C%20making%20it%20ideal%20for%20ego-centric%20applications%20and%20harvesting%20internet-scale%20video%20data%20for%20training.%20KM-ViPE%20tightly%20couples%20DINO%20visual%20features%20with%20geometric%20constraints%20through%20a%20high-level%20features%20based%20adaptive%20robust%20kernel%20that%20handles%20both%20moving%20objects%20and%20movable%20static%20objects%20%28e.g.%2C%20moving%20furniture%20in%20ego-centric%20views%29.%20The%20system%20performs%20simultaneous%20online%20localization%20and%20open-vocabulary%20semantic%20mapping%20by%20fusing%20geometric%20and%20deep%20visual%20features%20aligned%20with%20language%20embeddings.%20Our%20results%20are%20competitive%20with%20state-of-the-art%20approaches%2C%20while%20existing%20solutions%20either%20operate%20offline%2C%20need%20depth%20data%20and/or%20odometry%20estimation%2C%20or%20lack%20dynamic%20scene%20robustness.%20KM-ViPE%20benefits%20from%20internet-scale%20training%20and%20uniquely%20combines%20online%20operation%2C%20uncalibrated%20monocular%20input%2C%20and%20robust%20handling%20of%20dynamic%20scenes%2C%20which%20makes%20it%20a%20good%20fit%20for%20autonomous%20robotics%20and%20AR/VR%20applications%20and%20advances%20practical%20spatial%20intelligence%20capabilities%20for%20embodied%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKM-ViPE%253A%2520Online%2520Tightly%2520Coupled%2520Vision-Language-Geometry%2520Fusion%2520for%2520Open-Vocabulary%2520Semantic%2520SLAM%26entry.906535625%3DZaid%2520Nasser%2520and%2520Mikhail%2520Iumanov%2520and%2520Tianhao%2520Li%2520and%2520Maxim%2520Popov%2520and%2520Jaafar%2520Mahmoud%2520and%2520Malik%2520Mohrat%2520and%2520Ilya%2520Obrubov%2520and%2520Ekaterina%2520Derevyanka%2520and%2520Ivan%2520Sosin%2520and%2520Sergey%2520Kolyubin%26entry.1292438233%3DWe%2520present%2520KM-ViPE%2520%2528Knowledge%2520Mapping%2520Video%2520Pose%2520Engine%2529%252C%2520a%2520real-time%2520open-vocabulary%2520SLAM%2520framework%2520for%2520uncalibrated%2520monocular%2520cameras%2520in%2520dynamic%2520environments.%2520Unlike%2520systems%2520requiring%2520depth%2520sensors%2520and%2520offline%2520calibration%252C%2520KM-ViPE%2520operates%2520directly%2520on%2520raw%2520RGB%2520streams%252C%2520making%2520it%2520ideal%2520for%2520ego-centric%2520applications%2520and%2520harvesting%2520internet-scale%2520video%2520data%2520for%2520training.%2520KM-ViPE%2520tightly%2520couples%2520DINO%2520visual%2520features%2520with%2520geometric%2520constraints%2520through%2520a%2520high-level%2520features%2520based%2520adaptive%2520robust%2520kernel%2520that%2520handles%2520both%2520moving%2520objects%2520and%2520movable%2520static%2520objects%2520%2528e.g.%252C%2520moving%2520furniture%2520in%2520ego-centric%2520views%2529.%2520The%2520system%2520performs%2520simultaneous%2520online%2520localization%2520and%2520open-vocabulary%2520semantic%2520mapping%2520by%2520fusing%2520geometric%2520and%2520deep%2520visual%2520features%2520aligned%2520with%2520language%2520embeddings.%2520Our%2520results%2520are%2520competitive%2520with%2520state-of-the-art%2520approaches%252C%2520while%2520existing%2520solutions%2520either%2520operate%2520offline%252C%2520need%2520depth%2520data%2520and/or%2520odometry%2520estimation%252C%2520or%2520lack%2520dynamic%2520scene%2520robustness.%2520KM-ViPE%2520benefits%2520from%2520internet-scale%2520training%2520and%2520uniquely%2520combines%2520online%2520operation%252C%2520uncalibrated%2520monocular%2520input%252C%2520and%2520robust%2520handling%2520of%2520dynamic%2520scenes%252C%2520which%2520makes%2520it%2520a%2520good%2520fit%2520for%2520autonomous%2520robotics%2520and%2520AR/VR%2520applications%2520and%2520advances%2520practical%2520spatial%2520intelligence%2520capabilities%2520for%2520embodied%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KM-ViPE%3A%20Online%20Tightly%20Coupled%20Vision-Language-Geometry%20Fusion%20for%20Open-Vocabulary%20Semantic%20SLAM&entry.906535625=Zaid%20Nasser%20and%20Mikhail%20Iumanov%20and%20Tianhao%20Li%20and%20Maxim%20Popov%20and%20Jaafar%20Mahmoud%20and%20Malik%20Mohrat%20and%20Ilya%20Obrubov%20and%20Ekaterina%20Derevyanka%20and%20Ivan%20Sosin%20and%20Sergey%20Kolyubin&entry.1292438233=We%20present%20KM-ViPE%20%28Knowledge%20Mapping%20Video%20Pose%20Engine%29%2C%20a%20real-time%20open-vocabulary%20SLAM%20framework%20for%20uncalibrated%20monocular%20cameras%20in%20dynamic%20environments.%20Unlike%20systems%20requiring%20depth%20sensors%20and%20offline%20calibration%2C%20KM-ViPE%20operates%20directly%20on%20raw%20RGB%20streams%2C%20making%20it%20ideal%20for%20ego-centric%20applications%20and%20harvesting%20internet-scale%20video%20data%20for%20training.%20KM-ViPE%20tightly%20couples%20DINO%20visual%20features%20with%20geometric%20constraints%20through%20a%20high-level%20features%20based%20adaptive%20robust%20kernel%20that%20handles%20both%20moving%20objects%20and%20movable%20static%20objects%20%28e.g.%2C%20moving%20furniture%20in%20ego-centric%20views%29.%20The%20system%20performs%20simultaneous%20online%20localization%20and%20open-vocabulary%20semantic%20mapping%20by%20fusing%20geometric%20and%20deep%20visual%20features%20aligned%20with%20language%20embeddings.%20Our%20results%20are%20competitive%20with%20state-of-the-art%20approaches%2C%20while%20existing%20solutions%20either%20operate%20offline%2C%20need%20depth%20data%20and/or%20odometry%20estimation%2C%20or%20lack%20dynamic%20scene%20robustness.%20KM-ViPE%20benefits%20from%20internet-scale%20training%20and%20uniquely%20combines%20online%20operation%2C%20uncalibrated%20monocular%20input%2C%20and%20robust%20handling%20of%20dynamic%20scenes%2C%20which%20makes%20it%20a%20good%20fit%20for%20autonomous%20robotics%20and%20AR/VR%20applications%20and%20advances%20practical%20spatial%20intelligence%20capabilities%20for%20embodied%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2512.01889v1&entry.124074799=Read"},
{"title": "GRASP: Guided Residual Adapters with Sample-wise Partitioning", "author": "Felix N\u00fctzel and Mischa Dombrowski and Bernhard Kainz", "abstract": "Recent advances in text-to-image diffusion models enable high-fidelity generation across diverse prompts. However, these models falter in long-tail settings, such as medical imaging, where rare pathologies comprise a small fraction of data. This results in mode collapse: tail-class outputs lack quality and diversity, undermining the goal of synthetic data augmentation for underrepresented conditions. We pinpoint gradient conflicts between frequent head and rare tail classes as the primary culprit, a factor unaddressed by existing sampling or conditioning methods that mainly steer inference without altering the learned distribution. To resolve this, we propose GRASP: Guided Residual Adapters with Sample-wise Partitioning. GRASP uses external priors to statically partition samples into clusters that minimize intra-group gradient clashes. It then fine-tunes pre-trained models by injecting cluster-specific residual adapters into transformer feedforward layers, bypassing learned gating for stability and efficiency. On the long-tail MIMIC-CXR-LT dataset, GRASP yields superior FID and diversity metrics, especially for rare classes, outperforming baselines like vanilla fine-tuning and Mixture of Experts variants. Downstream classification on NIH-CXR-LT improves considerably for tail labels. Generalization to ImageNet-LT confirms broad applicability. Our method is lightweight, scalable, and readily integrates with diffusion pipelines.", "link": "http://arxiv.org/abs/2512.01675v1", "date": "2025-12-01", "relevancy": 2.4416, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6281}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6114}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRASP%3A%20Guided%20Residual%20Adapters%20with%20Sample-wise%20Partitioning&body=Title%3A%20GRASP%3A%20Guided%20Residual%20Adapters%20with%20Sample-wise%20Partitioning%0AAuthor%3A%20Felix%20N%C3%BCtzel%20and%20Mischa%20Dombrowski%20and%20Bernhard%20Kainz%0AAbstract%3A%20Recent%20advances%20in%20text-to-image%20diffusion%20models%20enable%20high-fidelity%20generation%20across%20diverse%20prompts.%20However%2C%20these%20models%20falter%20in%20long-tail%20settings%2C%20such%20as%20medical%20imaging%2C%20where%20rare%20pathologies%20comprise%20a%20small%20fraction%20of%20data.%20This%20results%20in%20mode%20collapse%3A%20tail-class%20outputs%20lack%20quality%20and%20diversity%2C%20undermining%20the%20goal%20of%20synthetic%20data%20augmentation%20for%20underrepresented%20conditions.%20We%20pinpoint%20gradient%20conflicts%20between%20frequent%20head%20and%20rare%20tail%20classes%20as%20the%20primary%20culprit%2C%20a%20factor%20unaddressed%20by%20existing%20sampling%20or%20conditioning%20methods%20that%20mainly%20steer%20inference%20without%20altering%20the%20learned%20distribution.%20To%20resolve%20this%2C%20we%20propose%20GRASP%3A%20Guided%20Residual%20Adapters%20with%20Sample-wise%20Partitioning.%20GRASP%20uses%20external%20priors%20to%20statically%20partition%20samples%20into%20clusters%20that%20minimize%20intra-group%20gradient%20clashes.%20It%20then%20fine-tunes%20pre-trained%20models%20by%20injecting%20cluster-specific%20residual%20adapters%20into%20transformer%20feedforward%20layers%2C%20bypassing%20learned%20gating%20for%20stability%20and%20efficiency.%20On%20the%20long-tail%20MIMIC-CXR-LT%20dataset%2C%20GRASP%20yields%20superior%20FID%20and%20diversity%20metrics%2C%20especially%20for%20rare%20classes%2C%20outperforming%20baselines%20like%20vanilla%20fine-tuning%20and%20Mixture%20of%20Experts%20variants.%20Downstream%20classification%20on%20NIH-CXR-LT%20improves%20considerably%20for%20tail%20labels.%20Generalization%20to%20ImageNet-LT%20confirms%20broad%20applicability.%20Our%20method%20is%20lightweight%2C%20scalable%2C%20and%20readily%20integrates%20with%20diffusion%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRASP%253A%2520Guided%2520Residual%2520Adapters%2520with%2520Sample-wise%2520Partitioning%26entry.906535625%3DFelix%2520N%25C3%25BCtzel%2520and%2520Mischa%2520Dombrowski%2520and%2520Bernhard%2520Kainz%26entry.1292438233%3DRecent%2520advances%2520in%2520text-to-image%2520diffusion%2520models%2520enable%2520high-fidelity%2520generation%2520across%2520diverse%2520prompts.%2520However%252C%2520these%2520models%2520falter%2520in%2520long-tail%2520settings%252C%2520such%2520as%2520medical%2520imaging%252C%2520where%2520rare%2520pathologies%2520comprise%2520a%2520small%2520fraction%2520of%2520data.%2520This%2520results%2520in%2520mode%2520collapse%253A%2520tail-class%2520outputs%2520lack%2520quality%2520and%2520diversity%252C%2520undermining%2520the%2520goal%2520of%2520synthetic%2520data%2520augmentation%2520for%2520underrepresented%2520conditions.%2520We%2520pinpoint%2520gradient%2520conflicts%2520between%2520frequent%2520head%2520and%2520rare%2520tail%2520classes%2520as%2520the%2520primary%2520culprit%252C%2520a%2520factor%2520unaddressed%2520by%2520existing%2520sampling%2520or%2520conditioning%2520methods%2520that%2520mainly%2520steer%2520inference%2520without%2520altering%2520the%2520learned%2520distribution.%2520To%2520resolve%2520this%252C%2520we%2520propose%2520GRASP%253A%2520Guided%2520Residual%2520Adapters%2520with%2520Sample-wise%2520Partitioning.%2520GRASP%2520uses%2520external%2520priors%2520to%2520statically%2520partition%2520samples%2520into%2520clusters%2520that%2520minimize%2520intra-group%2520gradient%2520clashes.%2520It%2520then%2520fine-tunes%2520pre-trained%2520models%2520by%2520injecting%2520cluster-specific%2520residual%2520adapters%2520into%2520transformer%2520feedforward%2520layers%252C%2520bypassing%2520learned%2520gating%2520for%2520stability%2520and%2520efficiency.%2520On%2520the%2520long-tail%2520MIMIC-CXR-LT%2520dataset%252C%2520GRASP%2520yields%2520superior%2520FID%2520and%2520diversity%2520metrics%252C%2520especially%2520for%2520rare%2520classes%252C%2520outperforming%2520baselines%2520like%2520vanilla%2520fine-tuning%2520and%2520Mixture%2520of%2520Experts%2520variants.%2520Downstream%2520classification%2520on%2520NIH-CXR-LT%2520improves%2520considerably%2520for%2520tail%2520labels.%2520Generalization%2520to%2520ImageNet-LT%2520confirms%2520broad%2520applicability.%2520Our%2520method%2520is%2520lightweight%252C%2520scalable%252C%2520and%2520readily%2520integrates%2520with%2520diffusion%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRASP%3A%20Guided%20Residual%20Adapters%20with%20Sample-wise%20Partitioning&entry.906535625=Felix%20N%C3%BCtzel%20and%20Mischa%20Dombrowski%20and%20Bernhard%20Kainz&entry.1292438233=Recent%20advances%20in%20text-to-image%20diffusion%20models%20enable%20high-fidelity%20generation%20across%20diverse%20prompts.%20However%2C%20these%20models%20falter%20in%20long-tail%20settings%2C%20such%20as%20medical%20imaging%2C%20where%20rare%20pathologies%20comprise%20a%20small%20fraction%20of%20data.%20This%20results%20in%20mode%20collapse%3A%20tail-class%20outputs%20lack%20quality%20and%20diversity%2C%20undermining%20the%20goal%20of%20synthetic%20data%20augmentation%20for%20underrepresented%20conditions.%20We%20pinpoint%20gradient%20conflicts%20between%20frequent%20head%20and%20rare%20tail%20classes%20as%20the%20primary%20culprit%2C%20a%20factor%20unaddressed%20by%20existing%20sampling%20or%20conditioning%20methods%20that%20mainly%20steer%20inference%20without%20altering%20the%20learned%20distribution.%20To%20resolve%20this%2C%20we%20propose%20GRASP%3A%20Guided%20Residual%20Adapters%20with%20Sample-wise%20Partitioning.%20GRASP%20uses%20external%20priors%20to%20statically%20partition%20samples%20into%20clusters%20that%20minimize%20intra-group%20gradient%20clashes.%20It%20then%20fine-tunes%20pre-trained%20models%20by%20injecting%20cluster-specific%20residual%20adapters%20into%20transformer%20feedforward%20layers%2C%20bypassing%20learned%20gating%20for%20stability%20and%20efficiency.%20On%20the%20long-tail%20MIMIC-CXR-LT%20dataset%2C%20GRASP%20yields%20superior%20FID%20and%20diversity%20metrics%2C%20especially%20for%20rare%20classes%2C%20outperforming%20baselines%20like%20vanilla%20fine-tuning%20and%20Mixture%20of%20Experts%20variants.%20Downstream%20classification%20on%20NIH-CXR-LT%20improves%20considerably%20for%20tail%20labels.%20Generalization%20to%20ImageNet-LT%20confirms%20broad%20applicability.%20Our%20method%20is%20lightweight%2C%20scalable%2C%20and%20readily%20integrates%20with%20diffusion%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2512.01675v1&entry.124074799=Read"},
{"title": "RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions", "author": "Junran Peng and Yiheng Huang and Silei Shen and Zeji Wei and Jingwei Yang and Baojie Wang and Yonghao He and Chuanchen Luo and Man Zhang and Xucheng Yin and Wei Sui", "abstract": "In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.", "link": "http://arxiv.org/abs/2512.01582v1", "date": "2025-12-01", "relevancy": 2.4296, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6496}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5782}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoleMotion%3A%20A%20Large-Scale%20Dataset%20towards%20Robust%20Scene-Specific%20Role-Playing%20Motion%20Synthesis%20with%20Fine-grained%20Descriptions&body=Title%3A%20RoleMotion%3A%20A%20Large-Scale%20Dataset%20towards%20Robust%20Scene-Specific%20Role-Playing%20Motion%20Synthesis%20with%20Fine-grained%20Descriptions%0AAuthor%3A%20Junran%20Peng%20and%20Yiheng%20Huang%20and%20Silei%20Shen%20and%20Zeji%20Wei%20and%20Jingwei%20Yang%20and%20Baojie%20Wang%20and%20Yonghao%20He%20and%20Chuanchen%20Luo%20and%20Man%20Zhang%20and%20Xucheng%20Yin%20and%20Wei%20Sui%0AAbstract%3A%20In%20this%20paper%2C%20we%20introduce%20RoleMotion%2C%20a%20large-scale%20human%20motion%20dataset%20that%20encompasses%20a%20wealth%20of%20role-playing%20and%20functional%20motion%20data%20tailored%20to%20fit%20various%20specific%20scenes.%20Existing%20text%20datasets%20are%20mainly%20constructed%20decentrally%20as%20amalgamation%20of%20assorted%20subsets%20that%20their%20data%20are%20nonfunctional%20and%20isolated%20to%20work%20together%20to%20cover%20social%20activities%20in%20various%20scenes.%20Also%2C%20the%20quality%20of%20motion%20data%20is%20inconsistent%2C%20and%20textual%20annotation%20lacks%20fine-grained%20details%20in%20these%20datasets.%20In%20contrast%2C%20RoleMotion%20is%20meticulously%20designed%20and%20collected%20with%20a%20particular%20focus%20on%20scenes%20and%20roles.%20The%20dataset%20features%2025%20classic%20scenes%2C%20110%20functional%20roles%2C%20over%20500%20behaviors%2C%20and%2010296%20high-quality%20human%20motion%20sequences%20of%20body%20and%20hands%2C%20annotated%20with%2027831%20fine-grained%20text%20descriptions.%20We%20build%20an%20evaluator%20stronger%20than%20existing%20counterparts%2C%20prove%20its%20reliability%2C%20and%20evaluate%20various%20text-to-motion%20methods%20on%20our%20dataset.%20Finally%2C%20we%20explore%20the%20interplay%20of%20motion%20generation%20of%20body%20and%20hands.%20Experimental%20results%20demonstrate%20the%20high-quality%20and%20functionality%20of%20our%20dataset%20on%20text-driven%20whole-body%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoleMotion%253A%2520A%2520Large-Scale%2520Dataset%2520towards%2520Robust%2520Scene-Specific%2520Role-Playing%2520Motion%2520Synthesis%2520with%2520Fine-grained%2520Descriptions%26entry.906535625%3DJunran%2520Peng%2520and%2520Yiheng%2520Huang%2520and%2520Silei%2520Shen%2520and%2520Zeji%2520Wei%2520and%2520Jingwei%2520Yang%2520and%2520Baojie%2520Wang%2520and%2520Yonghao%2520He%2520and%2520Chuanchen%2520Luo%2520and%2520Man%2520Zhang%2520and%2520Xucheng%2520Yin%2520and%2520Wei%2520Sui%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520introduce%2520RoleMotion%252C%2520a%2520large-scale%2520human%2520motion%2520dataset%2520that%2520encompasses%2520a%2520wealth%2520of%2520role-playing%2520and%2520functional%2520motion%2520data%2520tailored%2520to%2520fit%2520various%2520specific%2520scenes.%2520Existing%2520text%2520datasets%2520are%2520mainly%2520constructed%2520decentrally%2520as%2520amalgamation%2520of%2520assorted%2520subsets%2520that%2520their%2520data%2520are%2520nonfunctional%2520and%2520isolated%2520to%2520work%2520together%2520to%2520cover%2520social%2520activities%2520in%2520various%2520scenes.%2520Also%252C%2520the%2520quality%2520of%2520motion%2520data%2520is%2520inconsistent%252C%2520and%2520textual%2520annotation%2520lacks%2520fine-grained%2520details%2520in%2520these%2520datasets.%2520In%2520contrast%252C%2520RoleMotion%2520is%2520meticulously%2520designed%2520and%2520collected%2520with%2520a%2520particular%2520focus%2520on%2520scenes%2520and%2520roles.%2520The%2520dataset%2520features%252025%2520classic%2520scenes%252C%2520110%2520functional%2520roles%252C%2520over%2520500%2520behaviors%252C%2520and%252010296%2520high-quality%2520human%2520motion%2520sequences%2520of%2520body%2520and%2520hands%252C%2520annotated%2520with%252027831%2520fine-grained%2520text%2520descriptions.%2520We%2520build%2520an%2520evaluator%2520stronger%2520than%2520existing%2520counterparts%252C%2520prove%2520its%2520reliability%252C%2520and%2520evaluate%2520various%2520text-to-motion%2520methods%2520on%2520our%2520dataset.%2520Finally%252C%2520we%2520explore%2520the%2520interplay%2520of%2520motion%2520generation%2520of%2520body%2520and%2520hands.%2520Experimental%2520results%2520demonstrate%2520the%2520high-quality%2520and%2520functionality%2520of%2520our%2520dataset%2520on%2520text-driven%2520whole-body%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoleMotion%3A%20A%20Large-Scale%20Dataset%20towards%20Robust%20Scene-Specific%20Role-Playing%20Motion%20Synthesis%20with%20Fine-grained%20Descriptions&entry.906535625=Junran%20Peng%20and%20Yiheng%20Huang%20and%20Silei%20Shen%20and%20Zeji%20Wei%20and%20Jingwei%20Yang%20and%20Baojie%20Wang%20and%20Yonghao%20He%20and%20Chuanchen%20Luo%20and%20Man%20Zhang%20and%20Xucheng%20Yin%20and%20Wei%20Sui&entry.1292438233=In%20this%20paper%2C%20we%20introduce%20RoleMotion%2C%20a%20large-scale%20human%20motion%20dataset%20that%20encompasses%20a%20wealth%20of%20role-playing%20and%20functional%20motion%20data%20tailored%20to%20fit%20various%20specific%20scenes.%20Existing%20text%20datasets%20are%20mainly%20constructed%20decentrally%20as%20amalgamation%20of%20assorted%20subsets%20that%20their%20data%20are%20nonfunctional%20and%20isolated%20to%20work%20together%20to%20cover%20social%20activities%20in%20various%20scenes.%20Also%2C%20the%20quality%20of%20motion%20data%20is%20inconsistent%2C%20and%20textual%20annotation%20lacks%20fine-grained%20details%20in%20these%20datasets.%20In%20contrast%2C%20RoleMotion%20is%20meticulously%20designed%20and%20collected%20with%20a%20particular%20focus%20on%20scenes%20and%20roles.%20The%20dataset%20features%2025%20classic%20scenes%2C%20110%20functional%20roles%2C%20over%20500%20behaviors%2C%20and%2010296%20high-quality%20human%20motion%20sequences%20of%20body%20and%20hands%2C%20annotated%20with%2027831%20fine-grained%20text%20descriptions.%20We%20build%20an%20evaluator%20stronger%20than%20existing%20counterparts%2C%20prove%20its%20reliability%2C%20and%20evaluate%20various%20text-to-motion%20methods%20on%20our%20dataset.%20Finally%2C%20we%20explore%20the%20interplay%20of%20motion%20generation%20of%20body%20and%20hands.%20Experimental%20results%20demonstrate%20the%20high-quality%20and%20functionality%20of%20our%20dataset%20on%20text-driven%20whole-body%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.01582v1&entry.124074799=Read"},
{"title": "Integrated YOLOP Perception and Lyapunov-based Control for Autonomous Mobile Robot Navigation on Track", "author": "Mo Chen", "abstract": "This work presents a real-time autonomous track navigation framework for nonholonomic differential-drive mobile robots by jointly integrating multi-task visual perception and a provably stable tracking controller. The perception pipeline reconstructs lane centerlines using 2D-to-3D camera projection, arc-length based uniform point resampling, and cubic polynomial fitting solved via robust QR least-squares optimization. The controller regulates robot linear and angular velocities through a Lyapunov-stability grounded design, ensuring bounded error dynamics and asymptotic convergence of position and heading deviations even in dynamic and partially perceived lane scenarios, without relying on HD prior maps or global satellite localization. Real-world experiments on embedded platforms verify system fidelity, real-time execution, trajectory smoothness, and closed-loop stability for reliable autonomous navigation.", "link": "http://arxiv.org/abs/2512.01608v1", "date": "2025-12-01", "relevancy": 2.4258, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6366}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5867}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrated%20YOLOP%20Perception%20and%20Lyapunov-based%20Control%20for%20Autonomous%20Mobile%20Robot%20Navigation%20on%20Track&body=Title%3A%20Integrated%20YOLOP%20Perception%20and%20Lyapunov-based%20Control%20for%20Autonomous%20Mobile%20Robot%20Navigation%20on%20Track%0AAuthor%3A%20Mo%20Chen%0AAbstract%3A%20This%20work%20presents%20a%20real-time%20autonomous%20track%20navigation%20framework%20for%20nonholonomic%20differential-drive%20mobile%20robots%20by%20jointly%20integrating%20multi-task%20visual%20perception%20and%20a%20provably%20stable%20tracking%20controller.%20The%20perception%20pipeline%20reconstructs%20lane%20centerlines%20using%202D-to-3D%20camera%20projection%2C%20arc-length%20based%20uniform%20point%20resampling%2C%20and%20cubic%20polynomial%20fitting%20solved%20via%20robust%20QR%20least-squares%20optimization.%20The%20controller%20regulates%20robot%20linear%20and%20angular%20velocities%20through%20a%20Lyapunov-stability%20grounded%20design%2C%20ensuring%20bounded%20error%20dynamics%20and%20asymptotic%20convergence%20of%20position%20and%20heading%20deviations%20even%20in%20dynamic%20and%20partially%20perceived%20lane%20scenarios%2C%20without%20relying%20on%20HD%20prior%20maps%20or%20global%20satellite%20localization.%20Real-world%20experiments%20on%20embedded%20platforms%20verify%20system%20fidelity%2C%20real-time%20execution%2C%20trajectory%20smoothness%2C%20and%20closed-loop%20stability%20for%20reliable%20autonomous%20navigation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrated%2520YOLOP%2520Perception%2520and%2520Lyapunov-based%2520Control%2520for%2520Autonomous%2520Mobile%2520Robot%2520Navigation%2520on%2520Track%26entry.906535625%3DMo%2520Chen%26entry.1292438233%3DThis%2520work%2520presents%2520a%2520real-time%2520autonomous%2520track%2520navigation%2520framework%2520for%2520nonholonomic%2520differential-drive%2520mobile%2520robots%2520by%2520jointly%2520integrating%2520multi-task%2520visual%2520perception%2520and%2520a%2520provably%2520stable%2520tracking%2520controller.%2520The%2520perception%2520pipeline%2520reconstructs%2520lane%2520centerlines%2520using%25202D-to-3D%2520camera%2520projection%252C%2520arc-length%2520based%2520uniform%2520point%2520resampling%252C%2520and%2520cubic%2520polynomial%2520fitting%2520solved%2520via%2520robust%2520QR%2520least-squares%2520optimization.%2520The%2520controller%2520regulates%2520robot%2520linear%2520and%2520angular%2520velocities%2520through%2520a%2520Lyapunov-stability%2520grounded%2520design%252C%2520ensuring%2520bounded%2520error%2520dynamics%2520and%2520asymptotic%2520convergence%2520of%2520position%2520and%2520heading%2520deviations%2520even%2520in%2520dynamic%2520and%2520partially%2520perceived%2520lane%2520scenarios%252C%2520without%2520relying%2520on%2520HD%2520prior%2520maps%2520or%2520global%2520satellite%2520localization.%2520Real-world%2520experiments%2520on%2520embedded%2520platforms%2520verify%2520system%2520fidelity%252C%2520real-time%2520execution%252C%2520trajectory%2520smoothness%252C%2520and%2520closed-loop%2520stability%2520for%2520reliable%2520autonomous%2520navigation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrated%20YOLOP%20Perception%20and%20Lyapunov-based%20Control%20for%20Autonomous%20Mobile%20Robot%20Navigation%20on%20Track&entry.906535625=Mo%20Chen&entry.1292438233=This%20work%20presents%20a%20real-time%20autonomous%20track%20navigation%20framework%20for%20nonholonomic%20differential-drive%20mobile%20robots%20by%20jointly%20integrating%20multi-task%20visual%20perception%20and%20a%20provably%20stable%20tracking%20controller.%20The%20perception%20pipeline%20reconstructs%20lane%20centerlines%20using%202D-to-3D%20camera%20projection%2C%20arc-length%20based%20uniform%20point%20resampling%2C%20and%20cubic%20polynomial%20fitting%20solved%20via%20robust%20QR%20least-squares%20optimization.%20The%20controller%20regulates%20robot%20linear%20and%20angular%20velocities%20through%20a%20Lyapunov-stability%20grounded%20design%2C%20ensuring%20bounded%20error%20dynamics%20and%20asymptotic%20convergence%20of%20position%20and%20heading%20deviations%20even%20in%20dynamic%20and%20partially%20perceived%20lane%20scenarios%2C%20without%20relying%20on%20HD%20prior%20maps%20or%20global%20satellite%20localization.%20Real-world%20experiments%20on%20embedded%20platforms%20verify%20system%20fidelity%2C%20real-time%20execution%2C%20trajectory%20smoothness%2C%20and%20closed-loop%20stability%20for%20reliable%20autonomous%20navigation.&entry.1838667208=http%3A//arxiv.org/abs/2512.01608v1&entry.124074799=Read"},
{"title": "The Active and Noise-Tolerant Strategic Perceptron", "author": "Maria-Florina Blacan and Hedyeh Beyhaghi", "abstract": "We initiate the study of active learning algorithms for classifying strategic agents. Active learning is a well-established framework in machine learning in which the learner selectively queries labels, often achieving substantially higher accuracy and efficiency than classical supervised methods-especially in settings where labeling is costly or time-consuming, such as hiring, admissions, and loan decisions. Strategic classification, however, addresses scenarios where agents modify their features to obtain more favorable outcomes, resulting in observed data that is not truthful. Such manipulation introduces challenges beyond those in learning from clean data. Our goal is to design active and noise-tolerant algorithms that remain effective in strategic environments-algorithms that classify strategic agents accurately while issuing as few label requests as possible. The central difficulty is to simultaneously account for strategic manipulation and preserve the efficiency gains of active learning.\n  Our main result is an algorithm for actively learning linear separators in the strategic setting that preserves the exponential improvement in label complexity over passive learning previously obtained only in the non-strategic case. Specifically, for data drawn uniformly from the unit sphere, we show that a modified version of the Active Perceptron algorithm [DKM05,YZ17] achieves excess error $\u03b5$ using only $\\tilde{O}(d \\ln \\frac{1}\u03b5)$ label queries and incurs at most $\\tilde{O}(d \\ln \\frac{1}\u03b5)$ additional mistakes relative to the optimal classifier, even in the nonrealizable case, when a $\\tilde\u03a9(\u03b5)$ fraction of inputs have inconsistent labels with the optimal classifier. The algorithm is computationally efficient and, under these distributional assumptions, requires substantially fewer label queries than prior work on strategic Perceptron [ABBN21].", "link": "http://arxiv.org/abs/2512.01783v1", "date": "2025-12-01", "relevancy": 2.4217, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.49}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4836}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Active%20and%20Noise-Tolerant%20Strategic%20Perceptron&body=Title%3A%20The%20Active%20and%20Noise-Tolerant%20Strategic%20Perceptron%0AAuthor%3A%20Maria-Florina%20Blacan%20and%20Hedyeh%20Beyhaghi%0AAbstract%3A%20We%20initiate%20the%20study%20of%20active%20learning%20algorithms%20for%20classifying%20strategic%20agents.%20Active%20learning%20is%20a%20well-established%20framework%20in%20machine%20learning%20in%20which%20the%20learner%20selectively%20queries%20labels%2C%20often%20achieving%20substantially%20higher%20accuracy%20and%20efficiency%20than%20classical%20supervised%20methods-especially%20in%20settings%20where%20labeling%20is%20costly%20or%20time-consuming%2C%20such%20as%20hiring%2C%20admissions%2C%20and%20loan%20decisions.%20Strategic%20classification%2C%20however%2C%20addresses%20scenarios%20where%20agents%20modify%20their%20features%20to%20obtain%20more%20favorable%20outcomes%2C%20resulting%20in%20observed%20data%20that%20is%20not%20truthful.%20Such%20manipulation%20introduces%20challenges%20beyond%20those%20in%20learning%20from%20clean%20data.%20Our%20goal%20is%20to%20design%20active%20and%20noise-tolerant%20algorithms%20that%20remain%20effective%20in%20strategic%20environments-algorithms%20that%20classify%20strategic%20agents%20accurately%20while%20issuing%20as%20few%20label%20requests%20as%20possible.%20The%20central%20difficulty%20is%20to%20simultaneously%20account%20for%20strategic%20manipulation%20and%20preserve%20the%20efficiency%20gains%20of%20active%20learning.%0A%20%20Our%20main%20result%20is%20an%20algorithm%20for%20actively%20learning%20linear%20separators%20in%20the%20strategic%20setting%20that%20preserves%20the%20exponential%20improvement%20in%20label%20complexity%20over%20passive%20learning%20previously%20obtained%20only%20in%20the%20non-strategic%20case.%20Specifically%2C%20for%20data%20drawn%20uniformly%20from%20the%20unit%20sphere%2C%20we%20show%20that%20a%20modified%20version%20of%20the%20Active%20Perceptron%20algorithm%20%5BDKM05%2CYZ17%5D%20achieves%20excess%20error%20%24%CE%B5%24%20using%20only%20%24%5Ctilde%7BO%7D%28d%20%5Cln%20%5Cfrac%7B1%7D%CE%B5%29%24%20label%20queries%20and%20incurs%20at%20most%20%24%5Ctilde%7BO%7D%28d%20%5Cln%20%5Cfrac%7B1%7D%CE%B5%29%24%20additional%20mistakes%20relative%20to%20the%20optimal%20classifier%2C%20even%20in%20the%20nonrealizable%20case%2C%20when%20a%20%24%5Ctilde%CE%A9%28%CE%B5%29%24%20fraction%20of%20inputs%20have%20inconsistent%20labels%20with%20the%20optimal%20classifier.%20The%20algorithm%20is%20computationally%20efficient%20and%2C%20under%20these%20distributional%20assumptions%2C%20requires%20substantially%20fewer%20label%20queries%20than%20prior%20work%20on%20strategic%20Perceptron%20%5BABBN21%5D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Active%2520and%2520Noise-Tolerant%2520Strategic%2520Perceptron%26entry.906535625%3DMaria-Florina%2520Blacan%2520and%2520Hedyeh%2520Beyhaghi%26entry.1292438233%3DWe%2520initiate%2520the%2520study%2520of%2520active%2520learning%2520algorithms%2520for%2520classifying%2520strategic%2520agents.%2520Active%2520learning%2520is%2520a%2520well-established%2520framework%2520in%2520machine%2520learning%2520in%2520which%2520the%2520learner%2520selectively%2520queries%2520labels%252C%2520often%2520achieving%2520substantially%2520higher%2520accuracy%2520and%2520efficiency%2520than%2520classical%2520supervised%2520methods-especially%2520in%2520settings%2520where%2520labeling%2520is%2520costly%2520or%2520time-consuming%252C%2520such%2520as%2520hiring%252C%2520admissions%252C%2520and%2520loan%2520decisions.%2520Strategic%2520classification%252C%2520however%252C%2520addresses%2520scenarios%2520where%2520agents%2520modify%2520their%2520features%2520to%2520obtain%2520more%2520favorable%2520outcomes%252C%2520resulting%2520in%2520observed%2520data%2520that%2520is%2520not%2520truthful.%2520Such%2520manipulation%2520introduces%2520challenges%2520beyond%2520those%2520in%2520learning%2520from%2520clean%2520data.%2520Our%2520goal%2520is%2520to%2520design%2520active%2520and%2520noise-tolerant%2520algorithms%2520that%2520remain%2520effective%2520in%2520strategic%2520environments-algorithms%2520that%2520classify%2520strategic%2520agents%2520accurately%2520while%2520issuing%2520as%2520few%2520label%2520requests%2520as%2520possible.%2520The%2520central%2520difficulty%2520is%2520to%2520simultaneously%2520account%2520for%2520strategic%2520manipulation%2520and%2520preserve%2520the%2520efficiency%2520gains%2520of%2520active%2520learning.%250A%2520%2520Our%2520main%2520result%2520is%2520an%2520algorithm%2520for%2520actively%2520learning%2520linear%2520separators%2520in%2520the%2520strategic%2520setting%2520that%2520preserves%2520the%2520exponential%2520improvement%2520in%2520label%2520complexity%2520over%2520passive%2520learning%2520previously%2520obtained%2520only%2520in%2520the%2520non-strategic%2520case.%2520Specifically%252C%2520for%2520data%2520drawn%2520uniformly%2520from%2520the%2520unit%2520sphere%252C%2520we%2520show%2520that%2520a%2520modified%2520version%2520of%2520the%2520Active%2520Perceptron%2520algorithm%2520%255BDKM05%252CYZ17%255D%2520achieves%2520excess%2520error%2520%2524%25CE%25B5%2524%2520using%2520only%2520%2524%255Ctilde%257BO%257D%2528d%2520%255Cln%2520%255Cfrac%257B1%257D%25CE%25B5%2529%2524%2520label%2520queries%2520and%2520incurs%2520at%2520most%2520%2524%255Ctilde%257BO%257D%2528d%2520%255Cln%2520%255Cfrac%257B1%257D%25CE%25B5%2529%2524%2520additional%2520mistakes%2520relative%2520to%2520the%2520optimal%2520classifier%252C%2520even%2520in%2520the%2520nonrealizable%2520case%252C%2520when%2520a%2520%2524%255Ctilde%25CE%25A9%2528%25CE%25B5%2529%2524%2520fraction%2520of%2520inputs%2520have%2520inconsistent%2520labels%2520with%2520the%2520optimal%2520classifier.%2520The%2520algorithm%2520is%2520computationally%2520efficient%2520and%252C%2520under%2520these%2520distributional%2520assumptions%252C%2520requires%2520substantially%2520fewer%2520label%2520queries%2520than%2520prior%2520work%2520on%2520strategic%2520Perceptron%2520%255BABBN21%255D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Active%20and%20Noise-Tolerant%20Strategic%20Perceptron&entry.906535625=Maria-Florina%20Blacan%20and%20Hedyeh%20Beyhaghi&entry.1292438233=We%20initiate%20the%20study%20of%20active%20learning%20algorithms%20for%20classifying%20strategic%20agents.%20Active%20learning%20is%20a%20well-established%20framework%20in%20machine%20learning%20in%20which%20the%20learner%20selectively%20queries%20labels%2C%20often%20achieving%20substantially%20higher%20accuracy%20and%20efficiency%20than%20classical%20supervised%20methods-especially%20in%20settings%20where%20labeling%20is%20costly%20or%20time-consuming%2C%20such%20as%20hiring%2C%20admissions%2C%20and%20loan%20decisions.%20Strategic%20classification%2C%20however%2C%20addresses%20scenarios%20where%20agents%20modify%20their%20features%20to%20obtain%20more%20favorable%20outcomes%2C%20resulting%20in%20observed%20data%20that%20is%20not%20truthful.%20Such%20manipulation%20introduces%20challenges%20beyond%20those%20in%20learning%20from%20clean%20data.%20Our%20goal%20is%20to%20design%20active%20and%20noise-tolerant%20algorithms%20that%20remain%20effective%20in%20strategic%20environments-algorithms%20that%20classify%20strategic%20agents%20accurately%20while%20issuing%20as%20few%20label%20requests%20as%20possible.%20The%20central%20difficulty%20is%20to%20simultaneously%20account%20for%20strategic%20manipulation%20and%20preserve%20the%20efficiency%20gains%20of%20active%20learning.%0A%20%20Our%20main%20result%20is%20an%20algorithm%20for%20actively%20learning%20linear%20separators%20in%20the%20strategic%20setting%20that%20preserves%20the%20exponential%20improvement%20in%20label%20complexity%20over%20passive%20learning%20previously%20obtained%20only%20in%20the%20non-strategic%20case.%20Specifically%2C%20for%20data%20drawn%20uniformly%20from%20the%20unit%20sphere%2C%20we%20show%20that%20a%20modified%20version%20of%20the%20Active%20Perceptron%20algorithm%20%5BDKM05%2CYZ17%5D%20achieves%20excess%20error%20%24%CE%B5%24%20using%20only%20%24%5Ctilde%7BO%7D%28d%20%5Cln%20%5Cfrac%7B1%7D%CE%B5%29%24%20label%20queries%20and%20incurs%20at%20most%20%24%5Ctilde%7BO%7D%28d%20%5Cln%20%5Cfrac%7B1%7D%CE%B5%29%24%20additional%20mistakes%20relative%20to%20the%20optimal%20classifier%2C%20even%20in%20the%20nonrealizable%20case%2C%20when%20a%20%24%5Ctilde%CE%A9%28%CE%B5%29%24%20fraction%20of%20inputs%20have%20inconsistent%20labels%20with%20the%20optimal%20classifier.%20The%20algorithm%20is%20computationally%20efficient%20and%2C%20under%20these%20distributional%20assumptions%2C%20requires%20substantially%20fewer%20label%20queries%20than%20prior%20work%20on%20strategic%20Perceptron%20%5BABBN21%5D.&entry.1838667208=http%3A//arxiv.org/abs/2512.01783v1&entry.124074799=Read"},
{"title": "Forget Less, Retain More: A Lightweight Regularizer for Rehearsal-Based Continual Learning", "author": "Lama Alssum and Hasan Abed Al Kader Hammoud and Motasem Alfarra and Juan C Leon Alcazar and Bernard Ghanem", "abstract": "Deep neural networks suffer from catastrophic forgetting, where performance on previous tasks degrades after training on a new task. This issue arises due to the model's tendency to overwrite previously acquired knowledge with new information. We present a novel approach to address this challenge, focusing on the intersection of memory-based methods and regularization approaches. We formulate a regularization strategy, termed Information Maximization (IM) regularizer, for memory-based continual learning methods, which is based exclusively on the expected label distribution, thus making it class-agnostic. As a consequence, IM regularizer can be directly integrated into various rehearsal-based continual learning methods, reducing forgetting and favoring faster convergence. Our empirical validation shows that, across datasets and regardless of the number of tasks, our proposed regularization strategy consistently improves baseline performance at the expense of a minimal computational overhead. The lightweight nature of IM ensures that it remains a practical and scalable solution, making it applicable to real-world continual learning scenarios where efficiency is paramount. Finally, we demonstrate the data-agnostic nature of our regularizer by applying it to video data, which presents additional challenges due to its temporal structure and higher memory requirements. Despite the significant domain gap, our experiments show that IM regularizer also improves the performance of video continual learning methods.", "link": "http://arxiv.org/abs/2512.01818v1", "date": "2025-12-01", "relevancy": 2.4169, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4851}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4829}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forget%20Less%2C%20Retain%20More%3A%20A%20Lightweight%20Regularizer%20for%20Rehearsal-Based%20Continual%20Learning&body=Title%3A%20Forget%20Less%2C%20Retain%20More%3A%20A%20Lightweight%20Regularizer%20for%20Rehearsal-Based%20Continual%20Learning%0AAuthor%3A%20Lama%20Alssum%20and%20Hasan%20Abed%20Al%20Kader%20Hammoud%20and%20Motasem%20Alfarra%20and%20Juan%20C%20Leon%20Alcazar%20and%20Bernard%20Ghanem%0AAbstract%3A%20Deep%20neural%20networks%20suffer%20from%20catastrophic%20forgetting%2C%20where%20performance%20on%20previous%20tasks%20degrades%20after%20training%20on%20a%20new%20task.%20This%20issue%20arises%20due%20to%20the%20model%27s%20tendency%20to%20overwrite%20previously%20acquired%20knowledge%20with%20new%20information.%20We%20present%20a%20novel%20approach%20to%20address%20this%20challenge%2C%20focusing%20on%20the%20intersection%20of%20memory-based%20methods%20and%20regularization%20approaches.%20We%20formulate%20a%20regularization%20strategy%2C%20termed%20Information%20Maximization%20%28IM%29%20regularizer%2C%20for%20memory-based%20continual%20learning%20methods%2C%20which%20is%20based%20exclusively%20on%20the%20expected%20label%20distribution%2C%20thus%20making%20it%20class-agnostic.%20As%20a%20consequence%2C%20IM%20regularizer%20can%20be%20directly%20integrated%20into%20various%20rehearsal-based%20continual%20learning%20methods%2C%20reducing%20forgetting%20and%20favoring%20faster%20convergence.%20Our%20empirical%20validation%20shows%20that%2C%20across%20datasets%20and%20regardless%20of%20the%20number%20of%20tasks%2C%20our%20proposed%20regularization%20strategy%20consistently%20improves%20baseline%20performance%20at%20the%20expense%20of%20a%20minimal%20computational%20overhead.%20The%20lightweight%20nature%20of%20IM%20ensures%20that%20it%20remains%20a%20practical%20and%20scalable%20solution%2C%20making%20it%20applicable%20to%20real-world%20continual%20learning%20scenarios%20where%20efficiency%20is%20paramount.%20Finally%2C%20we%20demonstrate%20the%20data-agnostic%20nature%20of%20our%20regularizer%20by%20applying%20it%20to%20video%20data%2C%20which%20presents%20additional%20challenges%20due%20to%20its%20temporal%20structure%20and%20higher%20memory%20requirements.%20Despite%20the%20significant%20domain%20gap%2C%20our%20experiments%20show%20that%20IM%20regularizer%20also%20improves%20the%20performance%20of%20video%20continual%20learning%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForget%2520Less%252C%2520Retain%2520More%253A%2520A%2520Lightweight%2520Regularizer%2520for%2520Rehearsal-Based%2520Continual%2520Learning%26entry.906535625%3DLama%2520Alssum%2520and%2520Hasan%2520Abed%2520Al%2520Kader%2520Hammoud%2520and%2520Motasem%2520Alfarra%2520and%2520Juan%2520C%2520Leon%2520Alcazar%2520and%2520Bernard%2520Ghanem%26entry.1292438233%3DDeep%2520neural%2520networks%2520suffer%2520from%2520catastrophic%2520forgetting%252C%2520where%2520performance%2520on%2520previous%2520tasks%2520degrades%2520after%2520training%2520on%2520a%2520new%2520task.%2520This%2520issue%2520arises%2520due%2520to%2520the%2520model%2527s%2520tendency%2520to%2520overwrite%2520previously%2520acquired%2520knowledge%2520with%2520new%2520information.%2520We%2520present%2520a%2520novel%2520approach%2520to%2520address%2520this%2520challenge%252C%2520focusing%2520on%2520the%2520intersection%2520of%2520memory-based%2520methods%2520and%2520regularization%2520approaches.%2520We%2520formulate%2520a%2520regularization%2520strategy%252C%2520termed%2520Information%2520Maximization%2520%2528IM%2529%2520regularizer%252C%2520for%2520memory-based%2520continual%2520learning%2520methods%252C%2520which%2520is%2520based%2520exclusively%2520on%2520the%2520expected%2520label%2520distribution%252C%2520thus%2520making%2520it%2520class-agnostic.%2520As%2520a%2520consequence%252C%2520IM%2520regularizer%2520can%2520be%2520directly%2520integrated%2520into%2520various%2520rehearsal-based%2520continual%2520learning%2520methods%252C%2520reducing%2520forgetting%2520and%2520favoring%2520faster%2520convergence.%2520Our%2520empirical%2520validation%2520shows%2520that%252C%2520across%2520datasets%2520and%2520regardless%2520of%2520the%2520number%2520of%2520tasks%252C%2520our%2520proposed%2520regularization%2520strategy%2520consistently%2520improves%2520baseline%2520performance%2520at%2520the%2520expense%2520of%2520a%2520minimal%2520computational%2520overhead.%2520The%2520lightweight%2520nature%2520of%2520IM%2520ensures%2520that%2520it%2520remains%2520a%2520practical%2520and%2520scalable%2520solution%252C%2520making%2520it%2520applicable%2520to%2520real-world%2520continual%2520learning%2520scenarios%2520where%2520efficiency%2520is%2520paramount.%2520Finally%252C%2520we%2520demonstrate%2520the%2520data-agnostic%2520nature%2520of%2520our%2520regularizer%2520by%2520applying%2520it%2520to%2520video%2520data%252C%2520which%2520presents%2520additional%2520challenges%2520due%2520to%2520its%2520temporal%2520structure%2520and%2520higher%2520memory%2520requirements.%2520Despite%2520the%2520significant%2520domain%2520gap%252C%2520our%2520experiments%2520show%2520that%2520IM%2520regularizer%2520also%2520improves%2520the%2520performance%2520of%2520video%2520continual%2520learning%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forget%20Less%2C%20Retain%20More%3A%20A%20Lightweight%20Regularizer%20for%20Rehearsal-Based%20Continual%20Learning&entry.906535625=Lama%20Alssum%20and%20Hasan%20Abed%20Al%20Kader%20Hammoud%20and%20Motasem%20Alfarra%20and%20Juan%20C%20Leon%20Alcazar%20and%20Bernard%20Ghanem&entry.1292438233=Deep%20neural%20networks%20suffer%20from%20catastrophic%20forgetting%2C%20where%20performance%20on%20previous%20tasks%20degrades%20after%20training%20on%20a%20new%20task.%20This%20issue%20arises%20due%20to%20the%20model%27s%20tendency%20to%20overwrite%20previously%20acquired%20knowledge%20with%20new%20information.%20We%20present%20a%20novel%20approach%20to%20address%20this%20challenge%2C%20focusing%20on%20the%20intersection%20of%20memory-based%20methods%20and%20regularization%20approaches.%20We%20formulate%20a%20regularization%20strategy%2C%20termed%20Information%20Maximization%20%28IM%29%20regularizer%2C%20for%20memory-based%20continual%20learning%20methods%2C%20which%20is%20based%20exclusively%20on%20the%20expected%20label%20distribution%2C%20thus%20making%20it%20class-agnostic.%20As%20a%20consequence%2C%20IM%20regularizer%20can%20be%20directly%20integrated%20into%20various%20rehearsal-based%20continual%20learning%20methods%2C%20reducing%20forgetting%20and%20favoring%20faster%20convergence.%20Our%20empirical%20validation%20shows%20that%2C%20across%20datasets%20and%20regardless%20of%20the%20number%20of%20tasks%2C%20our%20proposed%20regularization%20strategy%20consistently%20improves%20baseline%20performance%20at%20the%20expense%20of%20a%20minimal%20computational%20overhead.%20The%20lightweight%20nature%20of%20IM%20ensures%20that%20it%20remains%20a%20practical%20and%20scalable%20solution%2C%20making%20it%20applicable%20to%20real-world%20continual%20learning%20scenarios%20where%20efficiency%20is%20paramount.%20Finally%2C%20we%20demonstrate%20the%20data-agnostic%20nature%20of%20our%20regularizer%20by%20applying%20it%20to%20video%20data%2C%20which%20presents%20additional%20challenges%20due%20to%20its%20temporal%20structure%20and%20higher%20memory%20requirements.%20Despite%20the%20significant%20domain%20gap%2C%20our%20experiments%20show%20that%20IM%20regularizer%20also%20improves%20the%20performance%20of%20video%20continual%20learning%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.01818v1&entry.124074799=Read"},
{"title": "The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of Reason", "author": "Shanchao Liang and Spandan Garg and Roshanak Zilouchian Moghaddam", "abstract": "As large language models (LLMs) become increasingly capable and widely adopted, benchmarks play a central role in assessing their practical utility. For example, SWE-Bench Verified has emerged as a critical benchmark for evaluating LLMs' software engineering abilities, particularly their aptitude for resolving real-world GitHub issues. Recent LLMs show impressive performance on SWE-Bench, leading to optimism about their capacity for complex coding tasks. However, current evaluation protocols may overstate these models' true capabilities. It is crucial to distinguish LLMs' generalizable problem-solving ability and other learned artifacts. In this work, we introduce two diagnostic tasks: file path identification from issue descriptions alone and ground truth function reproduction with only the current file context and issue description to probe models' underlying knowledge. We present empirical evidence that performance gains on SWE-Bench-Verified may be partially driven by memorization rather than genuine problem-solving. We show that state-of-the-art models achieve up to 76% accuracy in identifying buggy file paths using only issue descriptions, without access to repository structure. This performance is merely up to 53% on tasks from repositories not included in SWE-Bench, pointing to possible data contamination or memorization. Similar patterns are also observed for the function reproduction task, where the verbatim similarity is much higher on SWE-Bench Verified than on other similar coding benchmarks (up to 35% consecutive 5-gram accuracy on SWE-Bench Verified and Full, but only up to 18% for tasks in other benchmarks). These findings raise concerns about the validity of existing results and underscore the need for more robust, contamination-resistant benchmarks to reliably evaluate LLMs' coding abilities.", "link": "http://arxiv.org/abs/2506.12286v4", "date": "2025-12-01", "relevancy": 2.4164, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4908}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4908}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20SWE-Bench%20Illusion%3A%20When%20State-of-the-Art%20LLMs%20Remember%20Instead%20of%20Reason&body=Title%3A%20The%20SWE-Bench%20Illusion%3A%20When%20State-of-the-Art%20LLMs%20Remember%20Instead%20of%20Reason%0AAuthor%3A%20Shanchao%20Liang%20and%20Spandan%20Garg%20and%20Roshanak%20Zilouchian%20Moghaddam%0AAbstract%3A%20As%20large%20language%20models%20%28LLMs%29%20become%20increasingly%20capable%20and%20widely%20adopted%2C%20benchmarks%20play%20a%20central%20role%20in%20assessing%20their%20practical%20utility.%20For%20example%2C%20SWE-Bench%20Verified%20has%20emerged%20as%20a%20critical%20benchmark%20for%20evaluating%20LLMs%27%20software%20engineering%20abilities%2C%20particularly%20their%20aptitude%20for%20resolving%20real-world%20GitHub%20issues.%20Recent%20LLMs%20show%20impressive%20performance%20on%20SWE-Bench%2C%20leading%20to%20optimism%20about%20their%20capacity%20for%20complex%20coding%20tasks.%20However%2C%20current%20evaluation%20protocols%20may%20overstate%20these%20models%27%20true%20capabilities.%20It%20is%20crucial%20to%20distinguish%20LLMs%27%20generalizable%20problem-solving%20ability%20and%20other%20learned%20artifacts.%20In%20this%20work%2C%20we%20introduce%20two%20diagnostic%20tasks%3A%20file%20path%20identification%20from%20issue%20descriptions%20alone%20and%20ground%20truth%20function%20reproduction%20with%20only%20the%20current%20file%20context%20and%20issue%20description%20to%20probe%20models%27%20underlying%20knowledge.%20We%20present%20empirical%20evidence%20that%20performance%20gains%20on%20SWE-Bench-Verified%20may%20be%20partially%20driven%20by%20memorization%20rather%20than%20genuine%20problem-solving.%20We%20show%20that%20state-of-the-art%20models%20achieve%20up%20to%2076%25%20accuracy%20in%20identifying%20buggy%20file%20paths%20using%20only%20issue%20descriptions%2C%20without%20access%20to%20repository%20structure.%20This%20performance%20is%20merely%20up%20to%2053%25%20on%20tasks%20from%20repositories%20not%20included%20in%20SWE-Bench%2C%20pointing%20to%20possible%20data%20contamination%20or%20memorization.%20Similar%20patterns%20are%20also%20observed%20for%20the%20function%20reproduction%20task%2C%20where%20the%20verbatim%20similarity%20is%20much%20higher%20on%20SWE-Bench%20Verified%20than%20on%20other%20similar%20coding%20benchmarks%20%28up%20to%2035%25%20consecutive%205-gram%20accuracy%20on%20SWE-Bench%20Verified%20and%20Full%2C%20but%20only%20up%20to%2018%25%20for%20tasks%20in%20other%20benchmarks%29.%20These%20findings%20raise%20concerns%20about%20the%20validity%20of%20existing%20results%20and%20underscore%20the%20need%20for%20more%20robust%2C%20contamination-resistant%20benchmarks%20to%20reliably%20evaluate%20LLMs%27%20coding%20abilities.%0ALink%3A%20http%3A//arxiv.org/abs/2506.12286v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520SWE-Bench%2520Illusion%253A%2520When%2520State-of-the-Art%2520LLMs%2520Remember%2520Instead%2520of%2520Reason%26entry.906535625%3DShanchao%2520Liang%2520and%2520Spandan%2520Garg%2520and%2520Roshanak%2520Zilouchian%2520Moghaddam%26entry.1292438233%3DAs%2520large%2520language%2520models%2520%2528LLMs%2529%2520become%2520increasingly%2520capable%2520and%2520widely%2520adopted%252C%2520benchmarks%2520play%2520a%2520central%2520role%2520in%2520assessing%2520their%2520practical%2520utility.%2520For%2520example%252C%2520SWE-Bench%2520Verified%2520has%2520emerged%2520as%2520a%2520critical%2520benchmark%2520for%2520evaluating%2520LLMs%2527%2520software%2520engineering%2520abilities%252C%2520particularly%2520their%2520aptitude%2520for%2520resolving%2520real-world%2520GitHub%2520issues.%2520Recent%2520LLMs%2520show%2520impressive%2520performance%2520on%2520SWE-Bench%252C%2520leading%2520to%2520optimism%2520about%2520their%2520capacity%2520for%2520complex%2520coding%2520tasks.%2520However%252C%2520current%2520evaluation%2520protocols%2520may%2520overstate%2520these%2520models%2527%2520true%2520capabilities.%2520It%2520is%2520crucial%2520to%2520distinguish%2520LLMs%2527%2520generalizable%2520problem-solving%2520ability%2520and%2520other%2520learned%2520artifacts.%2520In%2520this%2520work%252C%2520we%2520introduce%2520two%2520diagnostic%2520tasks%253A%2520file%2520path%2520identification%2520from%2520issue%2520descriptions%2520alone%2520and%2520ground%2520truth%2520function%2520reproduction%2520with%2520only%2520the%2520current%2520file%2520context%2520and%2520issue%2520description%2520to%2520probe%2520models%2527%2520underlying%2520knowledge.%2520We%2520present%2520empirical%2520evidence%2520that%2520performance%2520gains%2520on%2520SWE-Bench-Verified%2520may%2520be%2520partially%2520driven%2520by%2520memorization%2520rather%2520than%2520genuine%2520problem-solving.%2520We%2520show%2520that%2520state-of-the-art%2520models%2520achieve%2520up%2520to%252076%2525%2520accuracy%2520in%2520identifying%2520buggy%2520file%2520paths%2520using%2520only%2520issue%2520descriptions%252C%2520without%2520access%2520to%2520repository%2520structure.%2520This%2520performance%2520is%2520merely%2520up%2520to%252053%2525%2520on%2520tasks%2520from%2520repositories%2520not%2520included%2520in%2520SWE-Bench%252C%2520pointing%2520to%2520possible%2520data%2520contamination%2520or%2520memorization.%2520Similar%2520patterns%2520are%2520also%2520observed%2520for%2520the%2520function%2520reproduction%2520task%252C%2520where%2520the%2520verbatim%2520similarity%2520is%2520much%2520higher%2520on%2520SWE-Bench%2520Verified%2520than%2520on%2520other%2520similar%2520coding%2520benchmarks%2520%2528up%2520to%252035%2525%2520consecutive%25205-gram%2520accuracy%2520on%2520SWE-Bench%2520Verified%2520and%2520Full%252C%2520but%2520only%2520up%2520to%252018%2525%2520for%2520tasks%2520in%2520other%2520benchmarks%2529.%2520These%2520findings%2520raise%2520concerns%2520about%2520the%2520validity%2520of%2520existing%2520results%2520and%2520underscore%2520the%2520need%2520for%2520more%2520robust%252C%2520contamination-resistant%2520benchmarks%2520to%2520reliably%2520evaluate%2520LLMs%2527%2520coding%2520abilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12286v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20SWE-Bench%20Illusion%3A%20When%20State-of-the-Art%20LLMs%20Remember%20Instead%20of%20Reason&entry.906535625=Shanchao%20Liang%20and%20Spandan%20Garg%20and%20Roshanak%20Zilouchian%20Moghaddam&entry.1292438233=As%20large%20language%20models%20%28LLMs%29%20become%20increasingly%20capable%20and%20widely%20adopted%2C%20benchmarks%20play%20a%20central%20role%20in%20assessing%20their%20practical%20utility.%20For%20example%2C%20SWE-Bench%20Verified%20has%20emerged%20as%20a%20critical%20benchmark%20for%20evaluating%20LLMs%27%20software%20engineering%20abilities%2C%20particularly%20their%20aptitude%20for%20resolving%20real-world%20GitHub%20issues.%20Recent%20LLMs%20show%20impressive%20performance%20on%20SWE-Bench%2C%20leading%20to%20optimism%20about%20their%20capacity%20for%20complex%20coding%20tasks.%20However%2C%20current%20evaluation%20protocols%20may%20overstate%20these%20models%27%20true%20capabilities.%20It%20is%20crucial%20to%20distinguish%20LLMs%27%20generalizable%20problem-solving%20ability%20and%20other%20learned%20artifacts.%20In%20this%20work%2C%20we%20introduce%20two%20diagnostic%20tasks%3A%20file%20path%20identification%20from%20issue%20descriptions%20alone%20and%20ground%20truth%20function%20reproduction%20with%20only%20the%20current%20file%20context%20and%20issue%20description%20to%20probe%20models%27%20underlying%20knowledge.%20We%20present%20empirical%20evidence%20that%20performance%20gains%20on%20SWE-Bench-Verified%20may%20be%20partially%20driven%20by%20memorization%20rather%20than%20genuine%20problem-solving.%20We%20show%20that%20state-of-the-art%20models%20achieve%20up%20to%2076%25%20accuracy%20in%20identifying%20buggy%20file%20paths%20using%20only%20issue%20descriptions%2C%20without%20access%20to%20repository%20structure.%20This%20performance%20is%20merely%20up%20to%2053%25%20on%20tasks%20from%20repositories%20not%20included%20in%20SWE-Bench%2C%20pointing%20to%20possible%20data%20contamination%20or%20memorization.%20Similar%20patterns%20are%20also%20observed%20for%20the%20function%20reproduction%20task%2C%20where%20the%20verbatim%20similarity%20is%20much%20higher%20on%20SWE-Bench%20Verified%20than%20on%20other%20similar%20coding%20benchmarks%20%28up%20to%2035%25%20consecutive%205-gram%20accuracy%20on%20SWE-Bench%20Verified%20and%20Full%2C%20but%20only%20up%20to%2018%25%20for%20tasks%20in%20other%20benchmarks%29.%20These%20findings%20raise%20concerns%20about%20the%20validity%20of%20existing%20results%20and%20underscore%20the%20need%20for%20more%20robust%2C%20contamination-resistant%20benchmarks%20to%20reliably%20evaluate%20LLMs%27%20coding%20abilities.&entry.1838667208=http%3A//arxiv.org/abs/2506.12286v4&entry.124074799=Read"},
{"title": "TTSnap: Test-Time Scaling of Diffusion Models via Noise-Aware Pruning", "author": "Qingtao Yu and Changlin Song and Minghao Sun and Zhengyang Yu and Vinay Kumar Verma and Soumya Roy and Sumit Negi and Hongdong Li and Dylan Campbell", "abstract": "A prominent approach to test-time scaling for text-to-image diffusion models formulates the problem as a search over multiple noise seeds, selecting the one that maximizes a certain image-reward function. The effectiveness of this strategy heavily depends on the number and diversity of noise seeds explored. However, verifying each candidate is computationally expensive, because each must be fully denoised before a reward can be computed. This severely limits the number of samples that can be explored under a fixed budget. We propose test-time scaling with noise-aware pruning (TTSnap), a framework that prunes low-quality candidates without fully denoising them. The key challenge is that reward models are learned in the clean image domain, and the ranking of rewards predicted for intermediate estimates are often inconsistent with those predicted for clean images. To overcome this, we train noise-aware reward models via self-distillation to align the reward for intermediate estimates with that of the final clean images. To stabilize learning across different noise levels, we adopt a curriculum training strategy that progressively shifts the data domain from clean images to noise images. In addition, we introduce a new metric that measures reward alignment and computational budget utilization. Experiments demonstrate that our approach improves performance by over 16\\% compared with existing methods, enabling more efficient and effective test-time scaling. It also provides orthogonal gains when combined with post-training techniques and local test-time optimization. Code: https://github.com/TerrysLearning/TTSnap/.", "link": "http://arxiv.org/abs/2511.22242v2", "date": "2025-12-01", "relevancy": 2.4151, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6381}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5979}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TTSnap%3A%20Test-Time%20Scaling%20of%20Diffusion%20Models%20via%20Noise-Aware%20Pruning&body=Title%3A%20TTSnap%3A%20Test-Time%20Scaling%20of%20Diffusion%20Models%20via%20Noise-Aware%20Pruning%0AAuthor%3A%20Qingtao%20Yu%20and%20Changlin%20Song%20and%20Minghao%20Sun%20and%20Zhengyang%20Yu%20and%20Vinay%20Kumar%20Verma%20and%20Soumya%20Roy%20and%20Sumit%20Negi%20and%20Hongdong%20Li%20and%20Dylan%20Campbell%0AAbstract%3A%20A%20prominent%20approach%20to%20test-time%20scaling%20for%20text-to-image%20diffusion%20models%20formulates%20the%20problem%20as%20a%20search%20over%20multiple%20noise%20seeds%2C%20selecting%20the%20one%20that%20maximizes%20a%20certain%20image-reward%20function.%20The%20effectiveness%20of%20this%20strategy%20heavily%20depends%20on%20the%20number%20and%20diversity%20of%20noise%20seeds%20explored.%20However%2C%20verifying%20each%20candidate%20is%20computationally%20expensive%2C%20because%20each%20must%20be%20fully%20denoised%20before%20a%20reward%20can%20be%20computed.%20This%20severely%20limits%20the%20number%20of%20samples%20that%20can%20be%20explored%20under%20a%20fixed%20budget.%20We%20propose%20test-time%20scaling%20with%20noise-aware%20pruning%20%28TTSnap%29%2C%20a%20framework%20that%20prunes%20low-quality%20candidates%20without%20fully%20denoising%20them.%20The%20key%20challenge%20is%20that%20reward%20models%20are%20learned%20in%20the%20clean%20image%20domain%2C%20and%20the%20ranking%20of%20rewards%20predicted%20for%20intermediate%20estimates%20are%20often%20inconsistent%20with%20those%20predicted%20for%20clean%20images.%20To%20overcome%20this%2C%20we%20train%20noise-aware%20reward%20models%20via%20self-distillation%20to%20align%20the%20reward%20for%20intermediate%20estimates%20with%20that%20of%20the%20final%20clean%20images.%20To%20stabilize%20learning%20across%20different%20noise%20levels%2C%20we%20adopt%20a%20curriculum%20training%20strategy%20that%20progressively%20shifts%20the%20data%20domain%20from%20clean%20images%20to%20noise%20images.%20In%20addition%2C%20we%20introduce%20a%20new%20metric%20that%20measures%20reward%20alignment%20and%20computational%20budget%20utilization.%20Experiments%20demonstrate%20that%20our%20approach%20improves%20performance%20by%20over%2016%5C%25%20compared%20with%20existing%20methods%2C%20enabling%20more%20efficient%20and%20effective%20test-time%20scaling.%20It%20also%20provides%20orthogonal%20gains%20when%20combined%20with%20post-training%20techniques%20and%20local%20test-time%20optimization.%20Code%3A%20https%3A//github.com/TerrysLearning/TTSnap/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.22242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTTSnap%253A%2520Test-Time%2520Scaling%2520of%2520Diffusion%2520Models%2520via%2520Noise-Aware%2520Pruning%26entry.906535625%3DQingtao%2520Yu%2520and%2520Changlin%2520Song%2520and%2520Minghao%2520Sun%2520and%2520Zhengyang%2520Yu%2520and%2520Vinay%2520Kumar%2520Verma%2520and%2520Soumya%2520Roy%2520and%2520Sumit%2520Negi%2520and%2520Hongdong%2520Li%2520and%2520Dylan%2520Campbell%26entry.1292438233%3DA%2520prominent%2520approach%2520to%2520test-time%2520scaling%2520for%2520text-to-image%2520diffusion%2520models%2520formulates%2520the%2520problem%2520as%2520a%2520search%2520over%2520multiple%2520noise%2520seeds%252C%2520selecting%2520the%2520one%2520that%2520maximizes%2520a%2520certain%2520image-reward%2520function.%2520The%2520effectiveness%2520of%2520this%2520strategy%2520heavily%2520depends%2520on%2520the%2520number%2520and%2520diversity%2520of%2520noise%2520seeds%2520explored.%2520However%252C%2520verifying%2520each%2520candidate%2520is%2520computationally%2520expensive%252C%2520because%2520each%2520must%2520be%2520fully%2520denoised%2520before%2520a%2520reward%2520can%2520be%2520computed.%2520This%2520severely%2520limits%2520the%2520number%2520of%2520samples%2520that%2520can%2520be%2520explored%2520under%2520a%2520fixed%2520budget.%2520We%2520propose%2520test-time%2520scaling%2520with%2520noise-aware%2520pruning%2520%2528TTSnap%2529%252C%2520a%2520framework%2520that%2520prunes%2520low-quality%2520candidates%2520without%2520fully%2520denoising%2520them.%2520The%2520key%2520challenge%2520is%2520that%2520reward%2520models%2520are%2520learned%2520in%2520the%2520clean%2520image%2520domain%252C%2520and%2520the%2520ranking%2520of%2520rewards%2520predicted%2520for%2520intermediate%2520estimates%2520are%2520often%2520inconsistent%2520with%2520those%2520predicted%2520for%2520clean%2520images.%2520To%2520overcome%2520this%252C%2520we%2520train%2520noise-aware%2520reward%2520models%2520via%2520self-distillation%2520to%2520align%2520the%2520reward%2520for%2520intermediate%2520estimates%2520with%2520that%2520of%2520the%2520final%2520clean%2520images.%2520To%2520stabilize%2520learning%2520across%2520different%2520noise%2520levels%252C%2520we%2520adopt%2520a%2520curriculum%2520training%2520strategy%2520that%2520progressively%2520shifts%2520the%2520data%2520domain%2520from%2520clean%2520images%2520to%2520noise%2520images.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520new%2520metric%2520that%2520measures%2520reward%2520alignment%2520and%2520computational%2520budget%2520utilization.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%2520improves%2520performance%2520by%2520over%252016%255C%2525%2520compared%2520with%2520existing%2520methods%252C%2520enabling%2520more%2520efficient%2520and%2520effective%2520test-time%2520scaling.%2520It%2520also%2520provides%2520orthogonal%2520gains%2520when%2520combined%2520with%2520post-training%2520techniques%2520and%2520local%2520test-time%2520optimization.%2520Code%253A%2520https%253A//github.com/TerrysLearning/TTSnap/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.22242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTSnap%3A%20Test-Time%20Scaling%20of%20Diffusion%20Models%20via%20Noise-Aware%20Pruning&entry.906535625=Qingtao%20Yu%20and%20Changlin%20Song%20and%20Minghao%20Sun%20and%20Zhengyang%20Yu%20and%20Vinay%20Kumar%20Verma%20and%20Soumya%20Roy%20and%20Sumit%20Negi%20and%20Hongdong%20Li%20and%20Dylan%20Campbell&entry.1292438233=A%20prominent%20approach%20to%20test-time%20scaling%20for%20text-to-image%20diffusion%20models%20formulates%20the%20problem%20as%20a%20search%20over%20multiple%20noise%20seeds%2C%20selecting%20the%20one%20that%20maximizes%20a%20certain%20image-reward%20function.%20The%20effectiveness%20of%20this%20strategy%20heavily%20depends%20on%20the%20number%20and%20diversity%20of%20noise%20seeds%20explored.%20However%2C%20verifying%20each%20candidate%20is%20computationally%20expensive%2C%20because%20each%20must%20be%20fully%20denoised%20before%20a%20reward%20can%20be%20computed.%20This%20severely%20limits%20the%20number%20of%20samples%20that%20can%20be%20explored%20under%20a%20fixed%20budget.%20We%20propose%20test-time%20scaling%20with%20noise-aware%20pruning%20%28TTSnap%29%2C%20a%20framework%20that%20prunes%20low-quality%20candidates%20without%20fully%20denoising%20them.%20The%20key%20challenge%20is%20that%20reward%20models%20are%20learned%20in%20the%20clean%20image%20domain%2C%20and%20the%20ranking%20of%20rewards%20predicted%20for%20intermediate%20estimates%20are%20often%20inconsistent%20with%20those%20predicted%20for%20clean%20images.%20To%20overcome%20this%2C%20we%20train%20noise-aware%20reward%20models%20via%20self-distillation%20to%20align%20the%20reward%20for%20intermediate%20estimates%20with%20that%20of%20the%20final%20clean%20images.%20To%20stabilize%20learning%20across%20different%20noise%20levels%2C%20we%20adopt%20a%20curriculum%20training%20strategy%20that%20progressively%20shifts%20the%20data%20domain%20from%20clean%20images%20to%20noise%20images.%20In%20addition%2C%20we%20introduce%20a%20new%20metric%20that%20measures%20reward%20alignment%20and%20computational%20budget%20utilization.%20Experiments%20demonstrate%20that%20our%20approach%20improves%20performance%20by%20over%2016%5C%25%20compared%20with%20existing%20methods%2C%20enabling%20more%20efficient%20and%20effective%20test-time%20scaling.%20It%20also%20provides%20orthogonal%20gains%20when%20combined%20with%20post-training%20techniques%20and%20local%20test-time%20optimization.%20Code%3A%20https%3A//github.com/TerrysLearning/TTSnap/.&entry.1838667208=http%3A//arxiv.org/abs/2511.22242v2&entry.124074799=Read"},
{"title": "Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation", "author": "Haodong Yan and Hang Yu and Zhide Zhong and Weilin Yuan and Xin Gong and Zehang Luo and Chengxi Heyu and Junfeng Li and Wenxuan Song and Shunbo Zhou and Haoang Li", "abstract": "Generating realistic hand-object interactions (HOI) videos is a significant challenge due to the difficulty of modeling physical constraints (e.g., contact and occlusion between hands and manipulated objects). Current methods utilize HOI representation as an auxiliary generative objective to guide video synthesis. However, there is a dilemma between 2D and 3D representations that cannot simultaneously guarantee scalability and interaction fidelity. To address this limitation, we propose a structure and contact-aware representation that captures hand-object contact, hand-object occlusion, and holistic structure context without 3D annotations. This interaction-oriented and scalable supervision signal enables the model to learn fine-grained interaction physics and generalize to open-world scenarios. To fully exploit the proposed representation, we introduce a joint-generation paradigm with a share-and-specialization strategy that generates interaction-oriented representations and videos. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on two real-world datasets in generating physics-realistic and temporally coherent HOI videos. Furthermore, our approach exhibits strong generalization to challenging open-world scenarios, highlighting the benefit of our scalable design. Our project page is https://hgzn258.github.io/SCAR/.", "link": "http://arxiv.org/abs/2512.01677v1", "date": "2025-12-01", "relevancy": 2.4038, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6104}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5961}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-world%20Hand-Object%20Interaction%20Video%20Generation%20Based%20on%20Structure%20and%20Contact-aware%20Representation&body=Title%3A%20Open-world%20Hand-Object%20Interaction%20Video%20Generation%20Based%20on%20Structure%20and%20Contact-aware%20Representation%0AAuthor%3A%20Haodong%20Yan%20and%20Hang%20Yu%20and%20Zhide%20Zhong%20and%20Weilin%20Yuan%20and%20Xin%20Gong%20and%20Zehang%20Luo%20and%20Chengxi%20Heyu%20and%20Junfeng%20Li%20and%20Wenxuan%20Song%20and%20Shunbo%20Zhou%20and%20Haoang%20Li%0AAbstract%3A%20Generating%20realistic%20hand-object%20interactions%20%28HOI%29%20videos%20is%20a%20significant%20challenge%20due%20to%20the%20difficulty%20of%20modeling%20physical%20constraints%20%28e.g.%2C%20contact%20and%20occlusion%20between%20hands%20and%20manipulated%20objects%29.%20Current%20methods%20utilize%20HOI%20representation%20as%20an%20auxiliary%20generative%20objective%20to%20guide%20video%20synthesis.%20However%2C%20there%20is%20a%20dilemma%20between%202D%20and%203D%20representations%20that%20cannot%20simultaneously%20guarantee%20scalability%20and%20interaction%20fidelity.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20structure%20and%20contact-aware%20representation%20that%20captures%20hand-object%20contact%2C%20hand-object%20occlusion%2C%20and%20holistic%20structure%20context%20without%203D%20annotations.%20This%20interaction-oriented%20and%20scalable%20supervision%20signal%20enables%20the%20model%20to%20learn%20fine-grained%20interaction%20physics%20and%20generalize%20to%20open-world%20scenarios.%20To%20fully%20exploit%20the%20proposed%20representation%2C%20we%20introduce%20a%20joint-generation%20paradigm%20with%20a%20share-and-specialization%20strategy%20that%20generates%20interaction-oriented%20representations%20and%20videos.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20methods%20on%20two%20real-world%20datasets%20in%20generating%20physics-realistic%20and%20temporally%20coherent%20HOI%20videos.%20Furthermore%2C%20our%20approach%20exhibits%20strong%20generalization%20to%20challenging%20open-world%20scenarios%2C%20highlighting%20the%20benefit%20of%20our%20scalable%20design.%20Our%20project%20page%20is%20https%3A//hgzn258.github.io/SCAR/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-world%2520Hand-Object%2520Interaction%2520Video%2520Generation%2520Based%2520on%2520Structure%2520and%2520Contact-aware%2520Representation%26entry.906535625%3DHaodong%2520Yan%2520and%2520Hang%2520Yu%2520and%2520Zhide%2520Zhong%2520and%2520Weilin%2520Yuan%2520and%2520Xin%2520Gong%2520and%2520Zehang%2520Luo%2520and%2520Chengxi%2520Heyu%2520and%2520Junfeng%2520Li%2520and%2520Wenxuan%2520Song%2520and%2520Shunbo%2520Zhou%2520and%2520Haoang%2520Li%26entry.1292438233%3DGenerating%2520realistic%2520hand-object%2520interactions%2520%2528HOI%2529%2520videos%2520is%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520difficulty%2520of%2520modeling%2520physical%2520constraints%2520%2528e.g.%252C%2520contact%2520and%2520occlusion%2520between%2520hands%2520and%2520manipulated%2520objects%2529.%2520Current%2520methods%2520utilize%2520HOI%2520representation%2520as%2520an%2520auxiliary%2520generative%2520objective%2520to%2520guide%2520video%2520synthesis.%2520However%252C%2520there%2520is%2520a%2520dilemma%2520between%25202D%2520and%25203D%2520representations%2520that%2520cannot%2520simultaneously%2520guarantee%2520scalability%2520and%2520interaction%2520fidelity.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520structure%2520and%2520contact-aware%2520representation%2520that%2520captures%2520hand-object%2520contact%252C%2520hand-object%2520occlusion%252C%2520and%2520holistic%2520structure%2520context%2520without%25203D%2520annotations.%2520This%2520interaction-oriented%2520and%2520scalable%2520supervision%2520signal%2520enables%2520the%2520model%2520to%2520learn%2520fine-grained%2520interaction%2520physics%2520and%2520generalize%2520to%2520open-world%2520scenarios.%2520To%2520fully%2520exploit%2520the%2520proposed%2520representation%252C%2520we%2520introduce%2520a%2520joint-generation%2520paradigm%2520with%2520a%2520share-and-specialization%2520strategy%2520that%2520generates%2520interaction-oriented%2520representations%2520and%2520videos.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520methods%2520on%2520two%2520real-world%2520datasets%2520in%2520generating%2520physics-realistic%2520and%2520temporally%2520coherent%2520HOI%2520videos.%2520Furthermore%252C%2520our%2520approach%2520exhibits%2520strong%2520generalization%2520to%2520challenging%2520open-world%2520scenarios%252C%2520highlighting%2520the%2520benefit%2520of%2520our%2520scalable%2520design.%2520Our%2520project%2520page%2520is%2520https%253A//hgzn258.github.io/SCAR/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-world%20Hand-Object%20Interaction%20Video%20Generation%20Based%20on%20Structure%20and%20Contact-aware%20Representation&entry.906535625=Haodong%20Yan%20and%20Hang%20Yu%20and%20Zhide%20Zhong%20and%20Weilin%20Yuan%20and%20Xin%20Gong%20and%20Zehang%20Luo%20and%20Chengxi%20Heyu%20and%20Junfeng%20Li%20and%20Wenxuan%20Song%20and%20Shunbo%20Zhou%20and%20Haoang%20Li&entry.1292438233=Generating%20realistic%20hand-object%20interactions%20%28HOI%29%20videos%20is%20a%20significant%20challenge%20due%20to%20the%20difficulty%20of%20modeling%20physical%20constraints%20%28e.g.%2C%20contact%20and%20occlusion%20between%20hands%20and%20manipulated%20objects%29.%20Current%20methods%20utilize%20HOI%20representation%20as%20an%20auxiliary%20generative%20objective%20to%20guide%20video%20synthesis.%20However%2C%20there%20is%20a%20dilemma%20between%202D%20and%203D%20representations%20that%20cannot%20simultaneously%20guarantee%20scalability%20and%20interaction%20fidelity.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20structure%20and%20contact-aware%20representation%20that%20captures%20hand-object%20contact%2C%20hand-object%20occlusion%2C%20and%20holistic%20structure%20context%20without%203D%20annotations.%20This%20interaction-oriented%20and%20scalable%20supervision%20signal%20enables%20the%20model%20to%20learn%20fine-grained%20interaction%20physics%20and%20generalize%20to%20open-world%20scenarios.%20To%20fully%20exploit%20the%20proposed%20representation%2C%20we%20introduce%20a%20joint-generation%20paradigm%20with%20a%20share-and-specialization%20strategy%20that%20generates%20interaction-oriented%20representations%20and%20videos.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20methods%20on%20two%20real-world%20datasets%20in%20generating%20physics-realistic%20and%20temporally%20coherent%20HOI%20videos.%20Furthermore%2C%20our%20approach%20exhibits%20strong%20generalization%20to%20challenging%20open-world%20scenarios%2C%20highlighting%20the%20benefit%20of%20our%20scalable%20design.%20Our%20project%20page%20is%20https%3A//hgzn258.github.io/SCAR/.&entry.1838667208=http%3A//arxiv.org/abs/2512.01677v1&entry.124074799=Read"},
{"title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "author": "Juanxi Tian and Siyuan Li and Conghui He and Lijun Wu and Cheng Tan", "abstract": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "link": "http://arxiv.org/abs/2512.01816v1", "date": "2025-12-01", "relevancy": 2.4014, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Envision%3A%20Benchmarking%20Unified%20Understanding%20%26%20Generation%20for%20Causal%20World%20Process%20Insights&body=Title%3A%20Envision%3A%20Benchmarking%20Unified%20Understanding%20%26%20Generation%20for%20Causal%20World%20Process%20Insights%0AAuthor%3A%20Juanxi%20Tian%20and%20Siyuan%20Li%20and%20Conghui%20He%20and%20Lijun%20Wu%20and%20Cheng%20Tan%0AAbstract%3A%20Current%20multimodal%20models%20aim%20to%20transcend%20the%20limitations%20of%20single-modality%20representations%20by%20unifying%20understanding%20and%20generation%2C%20often%20using%20text-to-image%20%28T2I%29%20tasks%20to%20calibrate%20semantic%20consistency.%20However%2C%20their%20reliance%20on%20static%2C%20single-image%20generation%20in%20training%20and%20evaluation%20leads%20to%20overfitting%20to%20static%20pattern%20matching%20and%20semantic%20fusion%2C%20while%20fundamentally%20hindering%20their%20ability%20to%20model%20dynamic%20processes%20that%20unfold%20over%20time.%20To%20address%20these%20constraints%2C%20we%20propose%20Envision-a%20causal%20event%20progression%20benchmark%20for%20chained%20text-to-multi-image%20generation.%20Grounded%20in%20world%20knowledge%20and%20structured%20by%20spatiotemporal%20causality%2C%20it%20reorganizes%20existing%20evaluation%20dimensions%20and%20includes%201%2C000%20four-stage%20prompts%20spanning%20six%20scientific%20and%20humanities%20domains.%20To%20transition%20evaluation%20from%20single%20images%20to%20sequential%20frames%20and%20assess%20whether%20models%20truly%20internalize%20world%20knowledge%20while%20adhering%20to%20causal-temporal%20constraints%2C%20we%20introduce%20Envision-Score%2C%20a%20holistic%20metric%20integrating%20multi-dimensional%20consistency%2C%20physicality%2C%20and%20aesthetics.%20Comprehensive%20evaluation%20of%2015%20models%20%2810%20specialized%20T2I%20models%2C%205%20unified%20models%29%20uncovers%3A%20specialized%20T2I%20models%20demonstrate%20proficiency%20in%20aesthetic%20rendering%20yet%20lack%20intrinsic%20world%20knowledge.%20Unified%20multimodal%20models%20bridge%20this%20gap%2C%20consistently%20outperforming%20specialized%20counterparts%20in%20causal%20narrative%20coherence.%20However%2C%20even%20these%20unified%20architectures%20remain%20subordinate%20to%20closed-source%20models%20and%20struggle%20to%20overcome%20the%20core%20challenge%20of%20spatiotemporal%20consistency.%20This%20demonstrates%20that%20a%20focus%20on%20causally-isolated%20single%20images%20impedes%20multi-frame%20reasoning%20and%20generation%2C%20promoting%20static%20pattern%20matching%20over%20dynamic%20world%20modeling-ultimately%20limiting%20world%20knowledge%20internalization%2C%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnvision%253A%2520Benchmarking%2520Unified%2520Understanding%2520%2526%2520Generation%2520for%2520Causal%2520World%2520Process%2520Insights%26entry.906535625%3DJuanxi%2520Tian%2520and%2520Siyuan%2520Li%2520and%2520Conghui%2520He%2520and%2520Lijun%2520Wu%2520and%2520Cheng%2520Tan%26entry.1292438233%3DCurrent%2520multimodal%2520models%2520aim%2520to%2520transcend%2520the%2520limitations%2520of%2520single-modality%2520representations%2520by%2520unifying%2520understanding%2520and%2520generation%252C%2520often%2520using%2520text-to-image%2520%2528T2I%2529%2520tasks%2520to%2520calibrate%2520semantic%2520consistency.%2520However%252C%2520their%2520reliance%2520on%2520static%252C%2520single-image%2520generation%2520in%2520training%2520and%2520evaluation%2520leads%2520to%2520overfitting%2520to%2520static%2520pattern%2520matching%2520and%2520semantic%2520fusion%252C%2520while%2520fundamentally%2520hindering%2520their%2520ability%2520to%2520model%2520dynamic%2520processes%2520that%2520unfold%2520over%2520time.%2520To%2520address%2520these%2520constraints%252C%2520we%2520propose%2520Envision-a%2520causal%2520event%2520progression%2520benchmark%2520for%2520chained%2520text-to-multi-image%2520generation.%2520Grounded%2520in%2520world%2520knowledge%2520and%2520structured%2520by%2520spatiotemporal%2520causality%252C%2520it%2520reorganizes%2520existing%2520evaluation%2520dimensions%2520and%2520includes%25201%252C000%2520four-stage%2520prompts%2520spanning%2520six%2520scientific%2520and%2520humanities%2520domains.%2520To%2520transition%2520evaluation%2520from%2520single%2520images%2520to%2520sequential%2520frames%2520and%2520assess%2520whether%2520models%2520truly%2520internalize%2520world%2520knowledge%2520while%2520adhering%2520to%2520causal-temporal%2520constraints%252C%2520we%2520introduce%2520Envision-Score%252C%2520a%2520holistic%2520metric%2520integrating%2520multi-dimensional%2520consistency%252C%2520physicality%252C%2520and%2520aesthetics.%2520Comprehensive%2520evaluation%2520of%252015%2520models%2520%252810%2520specialized%2520T2I%2520models%252C%25205%2520unified%2520models%2529%2520uncovers%253A%2520specialized%2520T2I%2520models%2520demonstrate%2520proficiency%2520in%2520aesthetic%2520rendering%2520yet%2520lack%2520intrinsic%2520world%2520knowledge.%2520Unified%2520multimodal%2520models%2520bridge%2520this%2520gap%252C%2520consistently%2520outperforming%2520specialized%2520counterparts%2520in%2520causal%2520narrative%2520coherence.%2520However%252C%2520even%2520these%2520unified%2520architectures%2520remain%2520subordinate%2520to%2520closed-source%2520models%2520and%2520struggle%2520to%2520overcome%2520the%2520core%2520challenge%2520of%2520spatiotemporal%2520consistency.%2520This%2520demonstrates%2520that%2520a%2520focus%2520on%2520causally-isolated%2520single%2520images%2520impedes%2520multi-frame%2520reasoning%2520and%2520generation%252C%2520promoting%2520static%2520pattern%2520matching%2520over%2520dynamic%2520world%2520modeling-ultimately%2520limiting%2520world%2520knowledge%2520internalization%252C%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Envision%3A%20Benchmarking%20Unified%20Understanding%20%26%20Generation%20for%20Causal%20World%20Process%20Insights&entry.906535625=Juanxi%20Tian%20and%20Siyuan%20Li%20and%20Conghui%20He%20and%20Lijun%20Wu%20and%20Cheng%20Tan&entry.1292438233=Current%20multimodal%20models%20aim%20to%20transcend%20the%20limitations%20of%20single-modality%20representations%20by%20unifying%20understanding%20and%20generation%2C%20often%20using%20text-to-image%20%28T2I%29%20tasks%20to%20calibrate%20semantic%20consistency.%20However%2C%20their%20reliance%20on%20static%2C%20single-image%20generation%20in%20training%20and%20evaluation%20leads%20to%20overfitting%20to%20static%20pattern%20matching%20and%20semantic%20fusion%2C%20while%20fundamentally%20hindering%20their%20ability%20to%20model%20dynamic%20processes%20that%20unfold%20over%20time.%20To%20address%20these%20constraints%2C%20we%20propose%20Envision-a%20causal%20event%20progression%20benchmark%20for%20chained%20text-to-multi-image%20generation.%20Grounded%20in%20world%20knowledge%20and%20structured%20by%20spatiotemporal%20causality%2C%20it%20reorganizes%20existing%20evaluation%20dimensions%20and%20includes%201%2C000%20four-stage%20prompts%20spanning%20six%20scientific%20and%20humanities%20domains.%20To%20transition%20evaluation%20from%20single%20images%20to%20sequential%20frames%20and%20assess%20whether%20models%20truly%20internalize%20world%20knowledge%20while%20adhering%20to%20causal-temporal%20constraints%2C%20we%20introduce%20Envision-Score%2C%20a%20holistic%20metric%20integrating%20multi-dimensional%20consistency%2C%20physicality%2C%20and%20aesthetics.%20Comprehensive%20evaluation%20of%2015%20models%20%2810%20specialized%20T2I%20models%2C%205%20unified%20models%29%20uncovers%3A%20specialized%20T2I%20models%20demonstrate%20proficiency%20in%20aesthetic%20rendering%20yet%20lack%20intrinsic%20world%20knowledge.%20Unified%20multimodal%20models%20bridge%20this%20gap%2C%20consistently%20outperforming%20specialized%20counterparts%20in%20causal%20narrative%20coherence.%20However%2C%20even%20these%20unified%20architectures%20remain%20subordinate%20to%20closed-source%20models%20and%20struggle%20to%20overcome%20the%20core%20challenge%20of%20spatiotemporal%20consistency.%20This%20demonstrates%20that%20a%20focus%20on%20causally-isolated%20single%20images%20impedes%20multi-frame%20reasoning%20and%20generation%2C%20promoting%20static%20pattern%20matching%20over%20dynamic%20world%20modeling-ultimately%20limiting%20world%20knowledge%20internalization%2C%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.01816v1&entry.124074799=Read"},
{"title": "InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving", "author": "Ruiqi Song and Xianda Guo and Yanlun Peng and Qinggong Wei and Hangbin Wu and Long Chen", "abstract": "Conventional end-to-end autonomous driving methods often rely on explicit global scene representations, which typically consist of 3D object detection, online mapping, and motion prediction. In contrast, human drivers selectively attend to task-relevant regions and implicitly reason over the broader traffic context. Motivated by this observation, we introduce a lightweight end-to-end autonomous driving framework, InsightDrive. Unlike approaches that directly embed large language models (LLMs), InsightDrive introduces an Insight scene representation that jointly models attention-centric explicit scene representation and reasoning-centric implicit scene representation, so that scene understanding aligns more closely with human cognitive patterns for trajectory planning. To this end, we employ Chain-of-Thought (CoT) instructions to model human driving cognition and design a task-level Mixture-of-Experts (MoE) adapter that injects this knowledge into the autonomous driving model at negligible parameter cost. We further condition the planner on both explicit and implicit scene representations and employ a diffusion-based generative policy, which produces robust trajectory predictions and decisions. The overall framework establishes a knowledge distillation pipeline that transfers human driving knowledge to LLMs and subsequently to onboard models. Extensive experiments on the nuScenes and Navsim benchmarks demonstrate that InsightDrive achieves significant improvements over conventional scene representation approaches.", "link": "http://arxiv.org/abs/2503.13047v2", "date": "2025-12-01", "relevancy": 2.3984, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6007}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InsightDrive%3A%20Insight%20Scene%20Representation%20for%20End-to-End%20Autonomous%20Driving&body=Title%3A%20InsightDrive%3A%20Insight%20Scene%20Representation%20for%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Ruiqi%20Song%20and%20Xianda%20Guo%20and%20Yanlun%20Peng%20and%20Qinggong%20Wei%20and%20Hangbin%20Wu%20and%20Long%20Chen%0AAbstract%3A%20Conventional%20end-to-end%20autonomous%20driving%20methods%20often%20rely%20on%20explicit%20global%20scene%20representations%2C%20which%20typically%20consist%20of%203D%20object%20detection%2C%20online%20mapping%2C%20and%20motion%20prediction.%20In%20contrast%2C%20human%20drivers%20selectively%20attend%20to%20task-relevant%20regions%20and%20implicitly%20reason%20over%20the%20broader%20traffic%20context.%20Motivated%20by%20this%20observation%2C%20we%20introduce%20a%20lightweight%20end-to-end%20autonomous%20driving%20framework%2C%20InsightDrive.%20Unlike%20approaches%20that%20directly%20embed%20large%20language%20models%20%28LLMs%29%2C%20InsightDrive%20introduces%20an%20Insight%20scene%20representation%20that%20jointly%20models%20attention-centric%20explicit%20scene%20representation%20and%20reasoning-centric%20implicit%20scene%20representation%2C%20so%20that%20scene%20understanding%20aligns%20more%20closely%20with%20human%20cognitive%20patterns%20for%20trajectory%20planning.%20To%20this%20end%2C%20we%20employ%20Chain-of-Thought%20%28CoT%29%20instructions%20to%20model%20human%20driving%20cognition%20and%20design%20a%20task-level%20Mixture-of-Experts%20%28MoE%29%20adapter%20that%20injects%20this%20knowledge%20into%20the%20autonomous%20driving%20model%20at%20negligible%20parameter%20cost.%20We%20further%20condition%20the%20planner%20on%20both%20explicit%20and%20implicit%20scene%20representations%20and%20employ%20a%20diffusion-based%20generative%20policy%2C%20which%20produces%20robust%20trajectory%20predictions%20and%20decisions.%20The%20overall%20framework%20establishes%20a%20knowledge%20distillation%20pipeline%20that%20transfers%20human%20driving%20knowledge%20to%20LLMs%20and%20subsequently%20to%20onboard%20models.%20Extensive%20experiments%20on%20the%20nuScenes%20and%20Navsim%20benchmarks%20demonstrate%20that%20InsightDrive%20achieves%20significant%20improvements%20over%20conventional%20scene%20representation%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2503.13047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsightDrive%253A%2520Insight%2520Scene%2520Representation%2520for%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DRuiqi%2520Song%2520and%2520Xianda%2520Guo%2520and%2520Yanlun%2520Peng%2520and%2520Qinggong%2520Wei%2520and%2520Hangbin%2520Wu%2520and%2520Long%2520Chen%26entry.1292438233%3DConventional%2520end-to-end%2520autonomous%2520driving%2520methods%2520often%2520rely%2520on%2520explicit%2520global%2520scene%2520representations%252C%2520which%2520typically%2520consist%2520of%25203D%2520object%2520detection%252C%2520online%2520mapping%252C%2520and%2520motion%2520prediction.%2520In%2520contrast%252C%2520human%2520drivers%2520selectively%2520attend%2520to%2520task-relevant%2520regions%2520and%2520implicitly%2520reason%2520over%2520the%2520broader%2520traffic%2520context.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520introduce%2520a%2520lightweight%2520end-to-end%2520autonomous%2520driving%2520framework%252C%2520InsightDrive.%2520Unlike%2520approaches%2520that%2520directly%2520embed%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520InsightDrive%2520introduces%2520an%2520Insight%2520scene%2520representation%2520that%2520jointly%2520models%2520attention-centric%2520explicit%2520scene%2520representation%2520and%2520reasoning-centric%2520implicit%2520scene%2520representation%252C%2520so%2520that%2520scene%2520understanding%2520aligns%2520more%2520closely%2520with%2520human%2520cognitive%2520patterns%2520for%2520trajectory%2520planning.%2520To%2520this%2520end%252C%2520we%2520employ%2520Chain-of-Thought%2520%2528CoT%2529%2520instructions%2520to%2520model%2520human%2520driving%2520cognition%2520and%2520design%2520a%2520task-level%2520Mixture-of-Experts%2520%2528MoE%2529%2520adapter%2520that%2520injects%2520this%2520knowledge%2520into%2520the%2520autonomous%2520driving%2520model%2520at%2520negligible%2520parameter%2520cost.%2520We%2520further%2520condition%2520the%2520planner%2520on%2520both%2520explicit%2520and%2520implicit%2520scene%2520representations%2520and%2520employ%2520a%2520diffusion-based%2520generative%2520policy%252C%2520which%2520produces%2520robust%2520trajectory%2520predictions%2520and%2520decisions.%2520The%2520overall%2520framework%2520establishes%2520a%2520knowledge%2520distillation%2520pipeline%2520that%2520transfers%2520human%2520driving%2520knowledge%2520to%2520LLMs%2520and%2520subsequently%2520to%2520onboard%2520models.%2520Extensive%2520experiments%2520on%2520the%2520nuScenes%2520and%2520Navsim%2520benchmarks%2520demonstrate%2520that%2520InsightDrive%2520achieves%2520significant%2520improvements%2520over%2520conventional%2520scene%2520representation%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InsightDrive%3A%20Insight%20Scene%20Representation%20for%20End-to-End%20Autonomous%20Driving&entry.906535625=Ruiqi%20Song%20and%20Xianda%20Guo%20and%20Yanlun%20Peng%20and%20Qinggong%20Wei%20and%20Hangbin%20Wu%20and%20Long%20Chen&entry.1292438233=Conventional%20end-to-end%20autonomous%20driving%20methods%20often%20rely%20on%20explicit%20global%20scene%20representations%2C%20which%20typically%20consist%20of%203D%20object%20detection%2C%20online%20mapping%2C%20and%20motion%20prediction.%20In%20contrast%2C%20human%20drivers%20selectively%20attend%20to%20task-relevant%20regions%20and%20implicitly%20reason%20over%20the%20broader%20traffic%20context.%20Motivated%20by%20this%20observation%2C%20we%20introduce%20a%20lightweight%20end-to-end%20autonomous%20driving%20framework%2C%20InsightDrive.%20Unlike%20approaches%20that%20directly%20embed%20large%20language%20models%20%28LLMs%29%2C%20InsightDrive%20introduces%20an%20Insight%20scene%20representation%20that%20jointly%20models%20attention-centric%20explicit%20scene%20representation%20and%20reasoning-centric%20implicit%20scene%20representation%2C%20so%20that%20scene%20understanding%20aligns%20more%20closely%20with%20human%20cognitive%20patterns%20for%20trajectory%20planning.%20To%20this%20end%2C%20we%20employ%20Chain-of-Thought%20%28CoT%29%20instructions%20to%20model%20human%20driving%20cognition%20and%20design%20a%20task-level%20Mixture-of-Experts%20%28MoE%29%20adapter%20that%20injects%20this%20knowledge%20into%20the%20autonomous%20driving%20model%20at%20negligible%20parameter%20cost.%20We%20further%20condition%20the%20planner%20on%20both%20explicit%20and%20implicit%20scene%20representations%20and%20employ%20a%20diffusion-based%20generative%20policy%2C%20which%20produces%20robust%20trajectory%20predictions%20and%20decisions.%20The%20overall%20framework%20establishes%20a%20knowledge%20distillation%20pipeline%20that%20transfers%20human%20driving%20knowledge%20to%20LLMs%20and%20subsequently%20to%20onboard%20models.%20Extensive%20experiments%20on%20the%20nuScenes%20and%20Navsim%20benchmarks%20demonstrate%20that%20InsightDrive%20achieves%20significant%20improvements%20over%20conventional%20scene%20representation%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2503.13047v2&entry.124074799=Read"},
{"title": "AirSim360: A Panoramic Simulation Platform within Drone View", "author": "Xian Ge and Yuling Pan and Yuhang Zhang and Xiang Li and Weijun Zhang and Dizhe Zhang and Zhaoliang Wan and Xin Lin and Xiangkai Zhang and Juntao Liang and Jason Li and Wenjie Jiang and Bo Du and Ming-Hsuan Yang and Lu Qi", "abstract": "The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.", "link": "http://arxiv.org/abs/2512.02009v1", "date": "2025-12-01", "relevancy": 2.384, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5986}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5986}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AirSim360%3A%20A%20Panoramic%20Simulation%20Platform%20within%20Drone%20View&body=Title%3A%20AirSim360%3A%20A%20Panoramic%20Simulation%20Platform%20within%20Drone%20View%0AAuthor%3A%20Xian%20Ge%20and%20Yuling%20Pan%20and%20Yuhang%20Zhang%20and%20Xiang%20Li%20and%20Weijun%20Zhang%20and%20Dizhe%20Zhang%20and%20Zhaoliang%20Wan%20and%20Xin%20Lin%20and%20Xiangkai%20Zhang%20and%20Juntao%20Liang%20and%20Jason%20Li%20and%20Wenjie%20Jiang%20and%20Bo%20Du%20and%20Ming-Hsuan%20Yang%20and%20Lu%20Qi%0AAbstract%3A%20The%20field%20of%20360-degree%20omnidirectional%20understanding%20has%20been%20receiving%20increasing%20attention%20for%20advancing%20spatial%20intelligence.%20However%2C%20the%20lack%20of%20large-scale%20and%20diverse%20data%20remains%20a%20major%20limitation.%20In%20this%20work%2C%20we%20propose%20AirSim360%2C%20a%20simulation%20platform%20for%20omnidirectional%20data%20from%20aerial%20viewpoints%2C%20enabling%20wide-ranging%20scene%20sampling%20with%20drones.%20Specifically%2C%20AirSim360%20focuses%20on%20three%20key%20aspects%3A%20a%20render-aligned%20data%20and%20labeling%20paradigm%20for%20pixel-level%20geometric%2C%20semantic%2C%20and%20entity-level%20understanding%3B%20an%20interactive%20pedestrian-aware%20system%20for%20modeling%20human%20behavior%3B%20and%20an%20automated%20trajectory%20generation%20paradigm%20to%20support%20navigation%20tasks.%20Furthermore%2C%20we%20collect%20more%20than%2060K%20panoramic%20samples%20and%20conduct%20extensive%20experiments%20across%20various%20tasks%20to%20demonstrate%20the%20effectiveness%20of%20our%20simulator.%20Unlike%20existing%20simulators%2C%20our%20work%20is%20the%20first%20to%20systematically%20model%20the%204D%20real%20world%20under%20an%20omnidirectional%20setting.%20The%20entire%20platform%2C%20including%20the%20toolkit%2C%20plugins%2C%20and%20collected%20datasets%2C%20will%20be%20made%20publicly%20available%20at%20https%3A//insta360-research-team.github.io/AirSim360-website.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAirSim360%253A%2520A%2520Panoramic%2520Simulation%2520Platform%2520within%2520Drone%2520View%26entry.906535625%3DXian%2520Ge%2520and%2520Yuling%2520Pan%2520and%2520Yuhang%2520Zhang%2520and%2520Xiang%2520Li%2520and%2520Weijun%2520Zhang%2520and%2520Dizhe%2520Zhang%2520and%2520Zhaoliang%2520Wan%2520and%2520Xin%2520Lin%2520and%2520Xiangkai%2520Zhang%2520and%2520Juntao%2520Liang%2520and%2520Jason%2520Li%2520and%2520Wenjie%2520Jiang%2520and%2520Bo%2520Du%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Lu%2520Qi%26entry.1292438233%3DThe%2520field%2520of%2520360-degree%2520omnidirectional%2520understanding%2520has%2520been%2520receiving%2520increasing%2520attention%2520for%2520advancing%2520spatial%2520intelligence.%2520However%252C%2520the%2520lack%2520of%2520large-scale%2520and%2520diverse%2520data%2520remains%2520a%2520major%2520limitation.%2520In%2520this%2520work%252C%2520we%2520propose%2520AirSim360%252C%2520a%2520simulation%2520platform%2520for%2520omnidirectional%2520data%2520from%2520aerial%2520viewpoints%252C%2520enabling%2520wide-ranging%2520scene%2520sampling%2520with%2520drones.%2520Specifically%252C%2520AirSim360%2520focuses%2520on%2520three%2520key%2520aspects%253A%2520a%2520render-aligned%2520data%2520and%2520labeling%2520paradigm%2520for%2520pixel-level%2520geometric%252C%2520semantic%252C%2520and%2520entity-level%2520understanding%253B%2520an%2520interactive%2520pedestrian-aware%2520system%2520for%2520modeling%2520human%2520behavior%253B%2520and%2520an%2520automated%2520trajectory%2520generation%2520paradigm%2520to%2520support%2520navigation%2520tasks.%2520Furthermore%252C%2520we%2520collect%2520more%2520than%252060K%2520panoramic%2520samples%2520and%2520conduct%2520extensive%2520experiments%2520across%2520various%2520tasks%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520simulator.%2520Unlike%2520existing%2520simulators%252C%2520our%2520work%2520is%2520the%2520first%2520to%2520systematically%2520model%2520the%25204D%2520real%2520world%2520under%2520an%2520omnidirectional%2520setting.%2520The%2520entire%2520platform%252C%2520including%2520the%2520toolkit%252C%2520plugins%252C%2520and%2520collected%2520datasets%252C%2520will%2520be%2520made%2520publicly%2520available%2520at%2520https%253A//insta360-research-team.github.io/AirSim360-website.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AirSim360%3A%20A%20Panoramic%20Simulation%20Platform%20within%20Drone%20View&entry.906535625=Xian%20Ge%20and%20Yuling%20Pan%20and%20Yuhang%20Zhang%20and%20Xiang%20Li%20and%20Weijun%20Zhang%20and%20Dizhe%20Zhang%20and%20Zhaoliang%20Wan%20and%20Xin%20Lin%20and%20Xiangkai%20Zhang%20and%20Juntao%20Liang%20and%20Jason%20Li%20and%20Wenjie%20Jiang%20and%20Bo%20Du%20and%20Ming-Hsuan%20Yang%20and%20Lu%20Qi&entry.1292438233=The%20field%20of%20360-degree%20omnidirectional%20understanding%20has%20been%20receiving%20increasing%20attention%20for%20advancing%20spatial%20intelligence.%20However%2C%20the%20lack%20of%20large-scale%20and%20diverse%20data%20remains%20a%20major%20limitation.%20In%20this%20work%2C%20we%20propose%20AirSim360%2C%20a%20simulation%20platform%20for%20omnidirectional%20data%20from%20aerial%20viewpoints%2C%20enabling%20wide-ranging%20scene%20sampling%20with%20drones.%20Specifically%2C%20AirSim360%20focuses%20on%20three%20key%20aspects%3A%20a%20render-aligned%20data%20and%20labeling%20paradigm%20for%20pixel-level%20geometric%2C%20semantic%2C%20and%20entity-level%20understanding%3B%20an%20interactive%20pedestrian-aware%20system%20for%20modeling%20human%20behavior%3B%20and%20an%20automated%20trajectory%20generation%20paradigm%20to%20support%20navigation%20tasks.%20Furthermore%2C%20we%20collect%20more%20than%2060K%20panoramic%20samples%20and%20conduct%20extensive%20experiments%20across%20various%20tasks%20to%20demonstrate%20the%20effectiveness%20of%20our%20simulator.%20Unlike%20existing%20simulators%2C%20our%20work%20is%20the%20first%20to%20systematically%20model%20the%204D%20real%20world%20under%20an%20omnidirectional%20setting.%20The%20entire%20platform%2C%20including%20the%20toolkit%2C%20plugins%2C%20and%20collected%20datasets%2C%20will%20be%20made%20publicly%20available%20at%20https%3A//insta360-research-team.github.io/AirSim360-website.&entry.1838667208=http%3A//arxiv.org/abs/2512.02009v1&entry.124074799=Read"},
{"title": "Decision Tree Embedding by Leaf-Means", "author": "Cencheng Shen and Yuexiao Dong and Carey E. Priebe", "abstract": "Decision trees and random forest remain highly competitive for classification on medium-sized, standard datasets due to their robustness, minimal preprocessing requirements, and interpretability. However, a single tree suffers from high estimation variance, while large ensembles reduce this variance at the cost of substantial computational overhead and diminished interpretability. In this paper, we propose Decision Tree Embedding (DTE), a fast and effective method that leverages the leaf partitions of a trained classification tree to construct an interpretable feature representation. By using the sample means within each leaf region as anchor points, DTE maps inputs into an embedding space defined by the tree's partition structure, effectively circumventing the high variance inherent in decision-tree splitting rules. We further introduce an ensemble extension based on additional bootstrap trees, and pair the resulting embedding with linear discriminant analysis for classification. We establish several population-level theoretical properties of DTE, including its preservation of conditional density under mild conditions and a characterization of the resulting classification error. Empirical studies on synthetic and real datasets demonstrate that DTE strikes a strong balance between accuracy and computational efficiency, outperforming or matching random forest and shallow neural networks while requiring only a fraction of their training time in most cases. Overall, the proposed DTE method can be viewed either as a scalable decision tree classifier that improves upon standard split rules, or as a neural network model whose weights are learned from tree-derived anchor points, achieving an intriguing integration of both paradigms.", "link": "http://arxiv.org/abs/2512.01819v1", "date": "2025-12-01", "relevancy": 2.3625, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4906}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4767}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decision%20Tree%20Embedding%20by%20Leaf-Means&body=Title%3A%20Decision%20Tree%20Embedding%20by%20Leaf-Means%0AAuthor%3A%20Cencheng%20Shen%20and%20Yuexiao%20Dong%20and%20Carey%20E.%20Priebe%0AAbstract%3A%20Decision%20trees%20and%20random%20forest%20remain%20highly%20competitive%20for%20classification%20on%20medium-sized%2C%20standard%20datasets%20due%20to%20their%20robustness%2C%20minimal%20preprocessing%20requirements%2C%20and%20interpretability.%20However%2C%20a%20single%20tree%20suffers%20from%20high%20estimation%20variance%2C%20while%20large%20ensembles%20reduce%20this%20variance%20at%20the%20cost%20of%20substantial%20computational%20overhead%20and%20diminished%20interpretability.%20In%20this%20paper%2C%20we%20propose%20Decision%20Tree%20Embedding%20%28DTE%29%2C%20a%20fast%20and%20effective%20method%20that%20leverages%20the%20leaf%20partitions%20of%20a%20trained%20classification%20tree%20to%20construct%20an%20interpretable%20feature%20representation.%20By%20using%20the%20sample%20means%20within%20each%20leaf%20region%20as%20anchor%20points%2C%20DTE%20maps%20inputs%20into%20an%20embedding%20space%20defined%20by%20the%20tree%27s%20partition%20structure%2C%20effectively%20circumventing%20the%20high%20variance%20inherent%20in%20decision-tree%20splitting%20rules.%20We%20further%20introduce%20an%20ensemble%20extension%20based%20on%20additional%20bootstrap%20trees%2C%20and%20pair%20the%20resulting%20embedding%20with%20linear%20discriminant%20analysis%20for%20classification.%20We%20establish%20several%20population-level%20theoretical%20properties%20of%20DTE%2C%20including%20its%20preservation%20of%20conditional%20density%20under%20mild%20conditions%20and%20a%20characterization%20of%20the%20resulting%20classification%20error.%20Empirical%20studies%20on%20synthetic%20and%20real%20datasets%20demonstrate%20that%20DTE%20strikes%20a%20strong%20balance%20between%20accuracy%20and%20computational%20efficiency%2C%20outperforming%20or%20matching%20random%20forest%20and%20shallow%20neural%20networks%20while%20requiring%20only%20a%20fraction%20of%20their%20training%20time%20in%20most%20cases.%20Overall%2C%20the%20proposed%20DTE%20method%20can%20be%20viewed%20either%20as%20a%20scalable%20decision%20tree%20classifier%20that%20improves%20upon%20standard%20split%20rules%2C%20or%20as%20a%20neural%20network%20model%20whose%20weights%20are%20learned%20from%20tree-derived%20anchor%20points%2C%20achieving%20an%20intriguing%20integration%20of%20both%20paradigms.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecision%2520Tree%2520Embedding%2520by%2520Leaf-Means%26entry.906535625%3DCencheng%2520Shen%2520and%2520Yuexiao%2520Dong%2520and%2520Carey%2520E.%2520Priebe%26entry.1292438233%3DDecision%2520trees%2520and%2520random%2520forest%2520remain%2520highly%2520competitive%2520for%2520classification%2520on%2520medium-sized%252C%2520standard%2520datasets%2520due%2520to%2520their%2520robustness%252C%2520minimal%2520preprocessing%2520requirements%252C%2520and%2520interpretability.%2520However%252C%2520a%2520single%2520tree%2520suffers%2520from%2520high%2520estimation%2520variance%252C%2520while%2520large%2520ensembles%2520reduce%2520this%2520variance%2520at%2520the%2520cost%2520of%2520substantial%2520computational%2520overhead%2520and%2520diminished%2520interpretability.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Decision%2520Tree%2520Embedding%2520%2528DTE%2529%252C%2520a%2520fast%2520and%2520effective%2520method%2520that%2520leverages%2520the%2520leaf%2520partitions%2520of%2520a%2520trained%2520classification%2520tree%2520to%2520construct%2520an%2520interpretable%2520feature%2520representation.%2520By%2520using%2520the%2520sample%2520means%2520within%2520each%2520leaf%2520region%2520as%2520anchor%2520points%252C%2520DTE%2520maps%2520inputs%2520into%2520an%2520embedding%2520space%2520defined%2520by%2520the%2520tree%2527s%2520partition%2520structure%252C%2520effectively%2520circumventing%2520the%2520high%2520variance%2520inherent%2520in%2520decision-tree%2520splitting%2520rules.%2520We%2520further%2520introduce%2520an%2520ensemble%2520extension%2520based%2520on%2520additional%2520bootstrap%2520trees%252C%2520and%2520pair%2520the%2520resulting%2520embedding%2520with%2520linear%2520discriminant%2520analysis%2520for%2520classification.%2520We%2520establish%2520several%2520population-level%2520theoretical%2520properties%2520of%2520DTE%252C%2520including%2520its%2520preservation%2520of%2520conditional%2520density%2520under%2520mild%2520conditions%2520and%2520a%2520characterization%2520of%2520the%2520resulting%2520classification%2520error.%2520Empirical%2520studies%2520on%2520synthetic%2520and%2520real%2520datasets%2520demonstrate%2520that%2520DTE%2520strikes%2520a%2520strong%2520balance%2520between%2520accuracy%2520and%2520computational%2520efficiency%252C%2520outperforming%2520or%2520matching%2520random%2520forest%2520and%2520shallow%2520neural%2520networks%2520while%2520requiring%2520only%2520a%2520fraction%2520of%2520their%2520training%2520time%2520in%2520most%2520cases.%2520Overall%252C%2520the%2520proposed%2520DTE%2520method%2520can%2520be%2520viewed%2520either%2520as%2520a%2520scalable%2520decision%2520tree%2520classifier%2520that%2520improves%2520upon%2520standard%2520split%2520rules%252C%2520or%2520as%2520a%2520neural%2520network%2520model%2520whose%2520weights%2520are%2520learned%2520from%2520tree-derived%2520anchor%2520points%252C%2520achieving%2520an%2520intriguing%2520integration%2520of%2520both%2520paradigms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decision%20Tree%20Embedding%20by%20Leaf-Means&entry.906535625=Cencheng%20Shen%20and%20Yuexiao%20Dong%20and%20Carey%20E.%20Priebe&entry.1292438233=Decision%20trees%20and%20random%20forest%20remain%20highly%20competitive%20for%20classification%20on%20medium-sized%2C%20standard%20datasets%20due%20to%20their%20robustness%2C%20minimal%20preprocessing%20requirements%2C%20and%20interpretability.%20However%2C%20a%20single%20tree%20suffers%20from%20high%20estimation%20variance%2C%20while%20large%20ensembles%20reduce%20this%20variance%20at%20the%20cost%20of%20substantial%20computational%20overhead%20and%20diminished%20interpretability.%20In%20this%20paper%2C%20we%20propose%20Decision%20Tree%20Embedding%20%28DTE%29%2C%20a%20fast%20and%20effective%20method%20that%20leverages%20the%20leaf%20partitions%20of%20a%20trained%20classification%20tree%20to%20construct%20an%20interpretable%20feature%20representation.%20By%20using%20the%20sample%20means%20within%20each%20leaf%20region%20as%20anchor%20points%2C%20DTE%20maps%20inputs%20into%20an%20embedding%20space%20defined%20by%20the%20tree%27s%20partition%20structure%2C%20effectively%20circumventing%20the%20high%20variance%20inherent%20in%20decision-tree%20splitting%20rules.%20We%20further%20introduce%20an%20ensemble%20extension%20based%20on%20additional%20bootstrap%20trees%2C%20and%20pair%20the%20resulting%20embedding%20with%20linear%20discriminant%20analysis%20for%20classification.%20We%20establish%20several%20population-level%20theoretical%20properties%20of%20DTE%2C%20including%20its%20preservation%20of%20conditional%20density%20under%20mild%20conditions%20and%20a%20characterization%20of%20the%20resulting%20classification%20error.%20Empirical%20studies%20on%20synthetic%20and%20real%20datasets%20demonstrate%20that%20DTE%20strikes%20a%20strong%20balance%20between%20accuracy%20and%20computational%20efficiency%2C%20outperforming%20or%20matching%20random%20forest%20and%20shallow%20neural%20networks%20while%20requiring%20only%20a%20fraction%20of%20their%20training%20time%20in%20most%20cases.%20Overall%2C%20the%20proposed%20DTE%20method%20can%20be%20viewed%20either%20as%20a%20scalable%20decision%20tree%20classifier%20that%20improves%20upon%20standard%20split%20rules%2C%20or%20as%20a%20neural%20network%20model%20whose%20weights%20are%20learned%20from%20tree-derived%20anchor%20points%2C%20achieving%20an%20intriguing%20integration%20of%20both%20paradigms.&entry.1838667208=http%3A//arxiv.org/abs/2512.01819v1&entry.124074799=Read"},
{"title": "A unified framework for geometry-independent operator learning in cardiac electrophysiology simulations", "author": "Bei Zhou and Cesare Corrado and Shuang Qian and Maximilian Balmus and Angela W. C. Lee and Cristobal Rodero and Marco J. W. Gotte and Luuk H. G. A. Hopman and Mengyun Qiao and Steven Niederer", "abstract": "Accurate maps of atrial electrical activation are essential for personalised treatment of arrhythmias, yet biophysically detailed simulations remain computationally intensive for real-time clinical use or population-scale analyses. Here we introduce a geometry-independent operator-learning framework that predicts local activation time (LAT) fields across diverse left atrial anatomies with near-instantaneous inference. We generated a dataset of 308,700 simulations using a GPU-accelerated electrophysiology solver, systematically varying multiple pacing sites and physiologically varied conduction properties across 147 patient-specific geometries derived from two independent clinical cohorts. All anatomical and functional data are expressed in a Universal Atrium Coordinate system, providing a consistent representation that decouples electrophysiological patterns from mesh topology. Within this coordinate space, we designed a neural operator with a vision-transformer backbone to learn the mapping from structural and electrophysiological inputs to LAT fields. With a mean prediction error of 5.1 ms over a 455 ms maximum simulation time, the model outperforms established operator-learning approaches and performs inference in 0.12 ms per sample. Our framework establishes a general strategy for learning domain-invariant biophysical mappings across variable anatomical domains and enables integration of computational electrophysiology into real-time and large-scale clinical workflows.", "link": "http://arxiv.org/abs/2512.01702v1", "date": "2025-12-01", "relevancy": 2.3542, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.473}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4709}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20unified%20framework%20for%20geometry-independent%20operator%20learning%20in%20cardiac%20electrophysiology%20simulations&body=Title%3A%20A%20unified%20framework%20for%20geometry-independent%20operator%20learning%20in%20cardiac%20electrophysiology%20simulations%0AAuthor%3A%20Bei%20Zhou%20and%20Cesare%20Corrado%20and%20Shuang%20Qian%20and%20Maximilian%20Balmus%20and%20Angela%20W.%20C.%20Lee%20and%20Cristobal%20Rodero%20and%20Marco%20J.%20W.%20Gotte%20and%20Luuk%20H.%20G.%20A.%20Hopman%20and%20Mengyun%20Qiao%20and%20Steven%20Niederer%0AAbstract%3A%20Accurate%20maps%20of%20atrial%20electrical%20activation%20are%20essential%20for%20personalised%20treatment%20of%20arrhythmias%2C%20yet%20biophysically%20detailed%20simulations%20remain%20computationally%20intensive%20for%20real-time%20clinical%20use%20or%20population-scale%20analyses.%20Here%20we%20introduce%20a%20geometry-independent%20operator-learning%20framework%20that%20predicts%20local%20activation%20time%20%28LAT%29%20fields%20across%20diverse%20left%20atrial%20anatomies%20with%20near-instantaneous%20inference.%20We%20generated%20a%20dataset%20of%20308%2C700%20simulations%20using%20a%20GPU-accelerated%20electrophysiology%20solver%2C%20systematically%20varying%20multiple%20pacing%20sites%20and%20physiologically%20varied%20conduction%20properties%20across%20147%20patient-specific%20geometries%20derived%20from%20two%20independent%20clinical%20cohorts.%20All%20anatomical%20and%20functional%20data%20are%20expressed%20in%20a%20Universal%20Atrium%20Coordinate%20system%2C%20providing%20a%20consistent%20representation%20that%20decouples%20electrophysiological%20patterns%20from%20mesh%20topology.%20Within%20this%20coordinate%20space%2C%20we%20designed%20a%20neural%20operator%20with%20a%20vision-transformer%20backbone%20to%20learn%20the%20mapping%20from%20structural%20and%20electrophysiological%20inputs%20to%20LAT%20fields.%20With%20a%20mean%20prediction%20error%20of%205.1%20ms%20over%20a%20455%20ms%20maximum%20simulation%20time%2C%20the%20model%20outperforms%20established%20operator-learning%20approaches%20and%20performs%20inference%20in%200.12%20ms%20per%20sample.%20Our%20framework%20establishes%20a%20general%20strategy%20for%20learning%20domain-invariant%20biophysical%20mappings%20across%20variable%20anatomical%20domains%20and%20enables%20integration%20of%20computational%20electrophysiology%20into%20real-time%20and%20large-scale%20clinical%20workflows.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520unified%2520framework%2520for%2520geometry-independent%2520operator%2520learning%2520in%2520cardiac%2520electrophysiology%2520simulations%26entry.906535625%3DBei%2520Zhou%2520and%2520Cesare%2520Corrado%2520and%2520Shuang%2520Qian%2520and%2520Maximilian%2520Balmus%2520and%2520Angela%2520W.%2520C.%2520Lee%2520and%2520Cristobal%2520Rodero%2520and%2520Marco%2520J.%2520W.%2520Gotte%2520and%2520Luuk%2520H.%2520G.%2520A.%2520Hopman%2520and%2520Mengyun%2520Qiao%2520and%2520Steven%2520Niederer%26entry.1292438233%3DAccurate%2520maps%2520of%2520atrial%2520electrical%2520activation%2520are%2520essential%2520for%2520personalised%2520treatment%2520of%2520arrhythmias%252C%2520yet%2520biophysically%2520detailed%2520simulations%2520remain%2520computationally%2520intensive%2520for%2520real-time%2520clinical%2520use%2520or%2520population-scale%2520analyses.%2520Here%2520we%2520introduce%2520a%2520geometry-independent%2520operator-learning%2520framework%2520that%2520predicts%2520local%2520activation%2520time%2520%2528LAT%2529%2520fields%2520across%2520diverse%2520left%2520atrial%2520anatomies%2520with%2520near-instantaneous%2520inference.%2520We%2520generated%2520a%2520dataset%2520of%2520308%252C700%2520simulations%2520using%2520a%2520GPU-accelerated%2520electrophysiology%2520solver%252C%2520systematically%2520varying%2520multiple%2520pacing%2520sites%2520and%2520physiologically%2520varied%2520conduction%2520properties%2520across%2520147%2520patient-specific%2520geometries%2520derived%2520from%2520two%2520independent%2520clinical%2520cohorts.%2520All%2520anatomical%2520and%2520functional%2520data%2520are%2520expressed%2520in%2520a%2520Universal%2520Atrium%2520Coordinate%2520system%252C%2520providing%2520a%2520consistent%2520representation%2520that%2520decouples%2520electrophysiological%2520patterns%2520from%2520mesh%2520topology.%2520Within%2520this%2520coordinate%2520space%252C%2520we%2520designed%2520a%2520neural%2520operator%2520with%2520a%2520vision-transformer%2520backbone%2520to%2520learn%2520the%2520mapping%2520from%2520structural%2520and%2520electrophysiological%2520inputs%2520to%2520LAT%2520fields.%2520With%2520a%2520mean%2520prediction%2520error%2520of%25205.1%2520ms%2520over%2520a%2520455%2520ms%2520maximum%2520simulation%2520time%252C%2520the%2520model%2520outperforms%2520established%2520operator-learning%2520approaches%2520and%2520performs%2520inference%2520in%25200.12%2520ms%2520per%2520sample.%2520Our%2520framework%2520establishes%2520a%2520general%2520strategy%2520for%2520learning%2520domain-invariant%2520biophysical%2520mappings%2520across%2520variable%2520anatomical%2520domains%2520and%2520enables%2520integration%2520of%2520computational%2520electrophysiology%2520into%2520real-time%2520and%2520large-scale%2520clinical%2520workflows.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20unified%20framework%20for%20geometry-independent%20operator%20learning%20in%20cardiac%20electrophysiology%20simulations&entry.906535625=Bei%20Zhou%20and%20Cesare%20Corrado%20and%20Shuang%20Qian%20and%20Maximilian%20Balmus%20and%20Angela%20W.%20C.%20Lee%20and%20Cristobal%20Rodero%20and%20Marco%20J.%20W.%20Gotte%20and%20Luuk%20H.%20G.%20A.%20Hopman%20and%20Mengyun%20Qiao%20and%20Steven%20Niederer&entry.1292438233=Accurate%20maps%20of%20atrial%20electrical%20activation%20are%20essential%20for%20personalised%20treatment%20of%20arrhythmias%2C%20yet%20biophysically%20detailed%20simulations%20remain%20computationally%20intensive%20for%20real-time%20clinical%20use%20or%20population-scale%20analyses.%20Here%20we%20introduce%20a%20geometry-independent%20operator-learning%20framework%20that%20predicts%20local%20activation%20time%20%28LAT%29%20fields%20across%20diverse%20left%20atrial%20anatomies%20with%20near-instantaneous%20inference.%20We%20generated%20a%20dataset%20of%20308%2C700%20simulations%20using%20a%20GPU-accelerated%20electrophysiology%20solver%2C%20systematically%20varying%20multiple%20pacing%20sites%20and%20physiologically%20varied%20conduction%20properties%20across%20147%20patient-specific%20geometries%20derived%20from%20two%20independent%20clinical%20cohorts.%20All%20anatomical%20and%20functional%20data%20are%20expressed%20in%20a%20Universal%20Atrium%20Coordinate%20system%2C%20providing%20a%20consistent%20representation%20that%20decouples%20electrophysiological%20patterns%20from%20mesh%20topology.%20Within%20this%20coordinate%20space%2C%20we%20designed%20a%20neural%20operator%20with%20a%20vision-transformer%20backbone%20to%20learn%20the%20mapping%20from%20structural%20and%20electrophysiological%20inputs%20to%20LAT%20fields.%20With%20a%20mean%20prediction%20error%20of%205.1%20ms%20over%20a%20455%20ms%20maximum%20simulation%20time%2C%20the%20model%20outperforms%20established%20operator-learning%20approaches%20and%20performs%20inference%20in%200.12%20ms%20per%20sample.%20Our%20framework%20establishes%20a%20general%20strategy%20for%20learning%20domain-invariant%20biophysical%20mappings%20across%20variable%20anatomical%20domains%20and%20enables%20integration%20of%20computational%20electrophysiology%20into%20real-time%20and%20large-scale%20clinical%20workflows.&entry.1838667208=http%3A//arxiv.org/abs/2512.01702v1&entry.124074799=Read"},
{"title": "InnoGym: Benchmarking the Innovation Potential of AI Agents", "author": "Jintian Zhang and Kewei Xu and Jingsheng Zheng and Zhuoyun Yu and Yuqi Zhu and Yujie Luo and Lanning Wei and Shuofei Qiao and Lun Du and Da Zheng and Shumin Deng and Huajun Chen and Ningyu Zhang", "abstract": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.", "link": "http://arxiv.org/abs/2512.01822v1", "date": "2025-12-01", "relevancy": 2.3471, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4957}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4565}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InnoGym%3A%20Benchmarking%20the%20Innovation%20Potential%20of%20AI%20Agents&body=Title%3A%20InnoGym%3A%20Benchmarking%20the%20Innovation%20Potential%20of%20AI%20Agents%0AAuthor%3A%20Jintian%20Zhang%20and%20Kewei%20Xu%20and%20Jingsheng%20Zheng%20and%20Zhuoyun%20Yu%20and%20Yuqi%20Zhu%20and%20Yujie%20Luo%20and%20Lanning%20Wei%20and%20Shuofei%20Qiao%20and%20Lun%20Du%20and%20Da%20Zheng%20and%20Shumin%20Deng%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20LLMs%20and%20Agents%20have%20achieved%20impressive%20progress%20in%20code%20generation%2C%20mathematical%20reasoning%2C%20and%20scientific%20discovery.%20However%2C%20existing%20benchmarks%20primarily%20measure%20correctness%2C%20overlooking%20the%20diversity%20of%20methods%20behind%20solutions.%20True%20innovation%20depends%20not%20only%20on%20producing%20correct%20answers%20but%20also%20on%20the%20originality%20of%20the%20approach.%20We%20present%20InnoGym%2C%20the%20first%20benchmark%20and%20framework%20designed%20to%20systematically%20evaluate%20the%20innovation%20potential%20of%20AI%20agents.%20InnoGym%20introduces%20two%20complementary%20metrics%3A%20performance%20gain%2C%20which%20measures%20improvement%20over%20the%20best-known%20solutions%2C%20and%20novelty%2C%20which%20captures%20methodological%20differences%20from%20prior%20approaches.%20The%20benchmark%20includes%2018%20carefully%20curated%20tasks%20from%20real-world%20engineering%20and%20scientific%20domains%2C%20each%20standardized%20through%20resource%20filtering%2C%20evaluator%20validation%2C%20and%20solution%20collection.%20In%20addition%2C%20we%20provide%20iGym%2C%20a%20unified%20execution%20environment%20for%20reproducible%20and%20long-horizon%20evaluations.%20Extensive%20experiments%20show%20that%20while%20some%20agents%20produce%20novel%20approaches%2C%20their%20lack%20of%20robustness%20limits%20performance%20gains.%20These%20results%20highlight%20a%20key%20gap%20between%20creativity%20and%20effectiveness%2C%20underscoring%20the%20need%20for%20benchmarks%20that%20evaluate%20both.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInnoGym%253A%2520Benchmarking%2520the%2520Innovation%2520Potential%2520of%2520AI%2520Agents%26entry.906535625%3DJintian%2520Zhang%2520and%2520Kewei%2520Xu%2520and%2520Jingsheng%2520Zheng%2520and%2520Zhuoyun%2520Yu%2520and%2520Yuqi%2520Zhu%2520and%2520Yujie%2520Luo%2520and%2520Lanning%2520Wei%2520and%2520Shuofei%2520Qiao%2520and%2520Lun%2520Du%2520and%2520Da%2520Zheng%2520and%2520Shumin%2520Deng%2520and%2520Huajun%2520Chen%2520and%2520Ningyu%2520Zhang%26entry.1292438233%3DLLMs%2520and%2520Agents%2520have%2520achieved%2520impressive%2520progress%2520in%2520code%2520generation%252C%2520mathematical%2520reasoning%252C%2520and%2520scientific%2520discovery.%2520However%252C%2520existing%2520benchmarks%2520primarily%2520measure%2520correctness%252C%2520overlooking%2520the%2520diversity%2520of%2520methods%2520behind%2520solutions.%2520True%2520innovation%2520depends%2520not%2520only%2520on%2520producing%2520correct%2520answers%2520but%2520also%2520on%2520the%2520originality%2520of%2520the%2520approach.%2520We%2520present%2520InnoGym%252C%2520the%2520first%2520benchmark%2520and%2520framework%2520designed%2520to%2520systematically%2520evaluate%2520the%2520innovation%2520potential%2520of%2520AI%2520agents.%2520InnoGym%2520introduces%2520two%2520complementary%2520metrics%253A%2520performance%2520gain%252C%2520which%2520measures%2520improvement%2520over%2520the%2520best-known%2520solutions%252C%2520and%2520novelty%252C%2520which%2520captures%2520methodological%2520differences%2520from%2520prior%2520approaches.%2520The%2520benchmark%2520includes%252018%2520carefully%2520curated%2520tasks%2520from%2520real-world%2520engineering%2520and%2520scientific%2520domains%252C%2520each%2520standardized%2520through%2520resource%2520filtering%252C%2520evaluator%2520validation%252C%2520and%2520solution%2520collection.%2520In%2520addition%252C%2520we%2520provide%2520iGym%252C%2520a%2520unified%2520execution%2520environment%2520for%2520reproducible%2520and%2520long-horizon%2520evaluations.%2520Extensive%2520experiments%2520show%2520that%2520while%2520some%2520agents%2520produce%2520novel%2520approaches%252C%2520their%2520lack%2520of%2520robustness%2520limits%2520performance%2520gains.%2520These%2520results%2520highlight%2520a%2520key%2520gap%2520between%2520creativity%2520and%2520effectiveness%252C%2520underscoring%2520the%2520need%2520for%2520benchmarks%2520that%2520evaluate%2520both.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InnoGym%3A%20Benchmarking%20the%20Innovation%20Potential%20of%20AI%20Agents&entry.906535625=Jintian%20Zhang%20and%20Kewei%20Xu%20and%20Jingsheng%20Zheng%20and%20Zhuoyun%20Yu%20and%20Yuqi%20Zhu%20and%20Yujie%20Luo%20and%20Lanning%20Wei%20and%20Shuofei%20Qiao%20and%20Lun%20Du%20and%20Da%20Zheng%20and%20Shumin%20Deng%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=LLMs%20and%20Agents%20have%20achieved%20impressive%20progress%20in%20code%20generation%2C%20mathematical%20reasoning%2C%20and%20scientific%20discovery.%20However%2C%20existing%20benchmarks%20primarily%20measure%20correctness%2C%20overlooking%20the%20diversity%20of%20methods%20behind%20solutions.%20True%20innovation%20depends%20not%20only%20on%20producing%20correct%20answers%20but%20also%20on%20the%20originality%20of%20the%20approach.%20We%20present%20InnoGym%2C%20the%20first%20benchmark%20and%20framework%20designed%20to%20systematically%20evaluate%20the%20innovation%20potential%20of%20AI%20agents.%20InnoGym%20introduces%20two%20complementary%20metrics%3A%20performance%20gain%2C%20which%20measures%20improvement%20over%20the%20best-known%20solutions%2C%20and%20novelty%2C%20which%20captures%20methodological%20differences%20from%20prior%20approaches.%20The%20benchmark%20includes%2018%20carefully%20curated%20tasks%20from%20real-world%20engineering%20and%20scientific%20domains%2C%20each%20standardized%20through%20resource%20filtering%2C%20evaluator%20validation%2C%20and%20solution%20collection.%20In%20addition%2C%20we%20provide%20iGym%2C%20a%20unified%20execution%20environment%20for%20reproducible%20and%20long-horizon%20evaluations.%20Extensive%20experiments%20show%20that%20while%20some%20agents%20produce%20novel%20approaches%2C%20their%20lack%20of%20robustness%20limits%20performance%20gains.%20These%20results%20highlight%20a%20key%20gap%20between%20creativity%20and%20effectiveness%2C%20underscoring%20the%20need%20for%20benchmarks%20that%20evaluate%20both.&entry.1838667208=http%3A//arxiv.org/abs/2512.01822v1&entry.124074799=Read"},
{"title": "Cross-Domain Validation of a Resection-Trained Self-Supervised Model on Multicentre Mesothelioma Biopsies", "author": "Farzaneh Seyedshahi and Francesca Damiola and Sylvie Lantuejoul and Ke Yuan and John Le Quesne", "abstract": "Accurate subtype classification and outcome prediction in mesothelioma are essential for guiding therapy and patient care. Most computational pathology models are trained on large tissue images from resection specimens, limiting their use in real-world settings where small biopsies are common. We show that a self-supervised encoder trained on resection tissue can be applied to biopsy material, capturing meaningful morphological patterns. Using these patterns, the model can predict patient survival and classify tumor subtypes. This approach demonstrates the potential of AI-driven tools to support diagnosis and treatment planning in mesothelioma.", "link": "http://arxiv.org/abs/2512.01681v1", "date": "2025-12-01", "relevancy": 2.3419, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4779}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4678}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Domain%20Validation%20of%20a%20Resection-Trained%20Self-Supervised%20Model%20on%20Multicentre%20Mesothelioma%20Biopsies&body=Title%3A%20Cross-Domain%20Validation%20of%20a%20Resection-Trained%20Self-Supervised%20Model%20on%20Multicentre%20Mesothelioma%20Biopsies%0AAuthor%3A%20Farzaneh%20Seyedshahi%20and%20Francesca%20Damiola%20and%20Sylvie%20Lantuejoul%20and%20Ke%20Yuan%20and%20John%20Le%20Quesne%0AAbstract%3A%20Accurate%20subtype%20classification%20and%20outcome%20prediction%20in%20mesothelioma%20are%20essential%20for%20guiding%20therapy%20and%20patient%20care.%20Most%20computational%20pathology%20models%20are%20trained%20on%20large%20tissue%20images%20from%20resection%20specimens%2C%20limiting%20their%20use%20in%20real-world%20settings%20where%20small%20biopsies%20are%20common.%20We%20show%20that%20a%20self-supervised%20encoder%20trained%20on%20resection%20tissue%20can%20be%20applied%20to%20biopsy%20material%2C%20capturing%20meaningful%20morphological%20patterns.%20Using%20these%20patterns%2C%20the%20model%20can%20predict%20patient%20survival%20and%20classify%20tumor%20subtypes.%20This%20approach%20demonstrates%20the%20potential%20of%20AI-driven%20tools%20to%20support%20diagnosis%20and%20treatment%20planning%20in%20mesothelioma.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Domain%2520Validation%2520of%2520a%2520Resection-Trained%2520Self-Supervised%2520Model%2520on%2520Multicentre%2520Mesothelioma%2520Biopsies%26entry.906535625%3DFarzaneh%2520Seyedshahi%2520and%2520Francesca%2520Damiola%2520and%2520Sylvie%2520Lantuejoul%2520and%2520Ke%2520Yuan%2520and%2520John%2520Le%2520Quesne%26entry.1292438233%3DAccurate%2520subtype%2520classification%2520and%2520outcome%2520prediction%2520in%2520mesothelioma%2520are%2520essential%2520for%2520guiding%2520therapy%2520and%2520patient%2520care.%2520Most%2520computational%2520pathology%2520models%2520are%2520trained%2520on%2520large%2520tissue%2520images%2520from%2520resection%2520specimens%252C%2520limiting%2520their%2520use%2520in%2520real-world%2520settings%2520where%2520small%2520biopsies%2520are%2520common.%2520We%2520show%2520that%2520a%2520self-supervised%2520encoder%2520trained%2520on%2520resection%2520tissue%2520can%2520be%2520applied%2520to%2520biopsy%2520material%252C%2520capturing%2520meaningful%2520morphological%2520patterns.%2520Using%2520these%2520patterns%252C%2520the%2520model%2520can%2520predict%2520patient%2520survival%2520and%2520classify%2520tumor%2520subtypes.%2520This%2520approach%2520demonstrates%2520the%2520potential%2520of%2520AI-driven%2520tools%2520to%2520support%2520diagnosis%2520and%2520treatment%2520planning%2520in%2520mesothelioma.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Domain%20Validation%20of%20a%20Resection-Trained%20Self-Supervised%20Model%20on%20Multicentre%20Mesothelioma%20Biopsies&entry.906535625=Farzaneh%20Seyedshahi%20and%20Francesca%20Damiola%20and%20Sylvie%20Lantuejoul%20and%20Ke%20Yuan%20and%20John%20Le%20Quesne&entry.1292438233=Accurate%20subtype%20classification%20and%20outcome%20prediction%20in%20mesothelioma%20are%20essential%20for%20guiding%20therapy%20and%20patient%20care.%20Most%20computational%20pathology%20models%20are%20trained%20on%20large%20tissue%20images%20from%20resection%20specimens%2C%20limiting%20their%20use%20in%20real-world%20settings%20where%20small%20biopsies%20are%20common.%20We%20show%20that%20a%20self-supervised%20encoder%20trained%20on%20resection%20tissue%20can%20be%20applied%20to%20biopsy%20material%2C%20capturing%20meaningful%20morphological%20patterns.%20Using%20these%20patterns%2C%20the%20model%20can%20predict%20patient%20survival%20and%20classify%20tumor%20subtypes.%20This%20approach%20demonstrates%20the%20potential%20of%20AI-driven%20tools%20to%20support%20diagnosis%20and%20treatment%20planning%20in%20mesothelioma.&entry.1838667208=http%3A//arxiv.org/abs/2512.01681v1&entry.124074799=Read"},
{"title": "PAI-Bench: A Comprehensive Benchmark For Physical AI", "author": "Fengzhe Zhou and Jiannan Huang and Jialuo Li and Deva Ramanan and Humphrey Shi", "abstract": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.", "link": "http://arxiv.org/abs/2512.01989v1", "date": "2025-12-01", "relevancy": 2.3373, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6015}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5862}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAI-Bench%3A%20A%20Comprehensive%20Benchmark%20For%20Physical%20AI&body=Title%3A%20PAI-Bench%3A%20A%20Comprehensive%20Benchmark%20For%20Physical%20AI%0AAuthor%3A%20Fengzhe%20Zhou%20and%20Jiannan%20Huang%20and%20Jialuo%20Li%20and%20Deva%20Ramanan%20and%20Humphrey%20Shi%0AAbstract%3A%20Physical%20AI%20aims%20to%20develop%20models%20that%20can%20perceive%20and%20predict%20real-world%20dynamics%3B%20yet%2C%20the%20extent%20to%20which%20current%20multi-modal%20large%20language%20models%20and%20video%20generative%20models%20support%20these%20abilities%20is%20insufficiently%20understood.%20We%20introduce%20Physical%20AI%20Bench%20%28PAI-Bench%29%2C%20a%20unified%20and%20comprehensive%20benchmark%20that%20evaluates%20perception%20and%20prediction%20capabilities%20across%20video%20generation%2C%20conditional%20video%20generation%2C%20and%20video%20understanding%2C%20comprising%202%2C808%20real-world%20cases%20with%20task-aligned%20metrics%20designed%20to%20capture%20physical%20plausibility%20and%20domain-specific%20reasoning.%20Our%20study%20provides%20a%20systematic%20assessment%20of%20recent%20models%20and%20shows%20that%20video%20generative%20models%2C%20despite%20strong%20visual%20fidelity%2C%20often%20struggle%20to%20maintain%20physically%20coherent%20dynamics%2C%20while%20multi-modal%20large%20language%20models%20exhibit%20limited%20performance%20in%20forecasting%20and%20causal%20interpretation.%20These%20observations%20suggest%20that%20current%20systems%20are%20still%20at%20an%20early%20stage%20in%20handling%20the%20perceptual%20and%20predictive%20demands%20of%20Physical%20AI.%20In%20summary%2C%20PAI-Bench%20establishes%20a%20realistic%20foundation%20for%20evaluating%20Physical%20AI%20and%20highlights%20key%20gaps%20that%20future%20systems%20must%20address.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAI-Bench%253A%2520A%2520Comprehensive%2520Benchmark%2520For%2520Physical%2520AI%26entry.906535625%3DFengzhe%2520Zhou%2520and%2520Jiannan%2520Huang%2520and%2520Jialuo%2520Li%2520and%2520Deva%2520Ramanan%2520and%2520Humphrey%2520Shi%26entry.1292438233%3DPhysical%2520AI%2520aims%2520to%2520develop%2520models%2520that%2520can%2520perceive%2520and%2520predict%2520real-world%2520dynamics%253B%2520yet%252C%2520the%2520extent%2520to%2520which%2520current%2520multi-modal%2520large%2520language%2520models%2520and%2520video%2520generative%2520models%2520support%2520these%2520abilities%2520is%2520insufficiently%2520understood.%2520We%2520introduce%2520Physical%2520AI%2520Bench%2520%2528PAI-Bench%2529%252C%2520a%2520unified%2520and%2520comprehensive%2520benchmark%2520that%2520evaluates%2520perception%2520and%2520prediction%2520capabilities%2520across%2520video%2520generation%252C%2520conditional%2520video%2520generation%252C%2520and%2520video%2520understanding%252C%2520comprising%25202%252C808%2520real-world%2520cases%2520with%2520task-aligned%2520metrics%2520designed%2520to%2520capture%2520physical%2520plausibility%2520and%2520domain-specific%2520reasoning.%2520Our%2520study%2520provides%2520a%2520systematic%2520assessment%2520of%2520recent%2520models%2520and%2520shows%2520that%2520video%2520generative%2520models%252C%2520despite%2520strong%2520visual%2520fidelity%252C%2520often%2520struggle%2520to%2520maintain%2520physically%2520coherent%2520dynamics%252C%2520while%2520multi-modal%2520large%2520language%2520models%2520exhibit%2520limited%2520performance%2520in%2520forecasting%2520and%2520causal%2520interpretation.%2520These%2520observations%2520suggest%2520that%2520current%2520systems%2520are%2520still%2520at%2520an%2520early%2520stage%2520in%2520handling%2520the%2520perceptual%2520and%2520predictive%2520demands%2520of%2520Physical%2520AI.%2520In%2520summary%252C%2520PAI-Bench%2520establishes%2520a%2520realistic%2520foundation%2520for%2520evaluating%2520Physical%2520AI%2520and%2520highlights%2520key%2520gaps%2520that%2520future%2520systems%2520must%2520address.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAI-Bench%3A%20A%20Comprehensive%20Benchmark%20For%20Physical%20AI&entry.906535625=Fengzhe%20Zhou%20and%20Jiannan%20Huang%20and%20Jialuo%20Li%20and%20Deva%20Ramanan%20and%20Humphrey%20Shi&entry.1292438233=Physical%20AI%20aims%20to%20develop%20models%20that%20can%20perceive%20and%20predict%20real-world%20dynamics%3B%20yet%2C%20the%20extent%20to%20which%20current%20multi-modal%20large%20language%20models%20and%20video%20generative%20models%20support%20these%20abilities%20is%20insufficiently%20understood.%20We%20introduce%20Physical%20AI%20Bench%20%28PAI-Bench%29%2C%20a%20unified%20and%20comprehensive%20benchmark%20that%20evaluates%20perception%20and%20prediction%20capabilities%20across%20video%20generation%2C%20conditional%20video%20generation%2C%20and%20video%20understanding%2C%20comprising%202%2C808%20real-world%20cases%20with%20task-aligned%20metrics%20designed%20to%20capture%20physical%20plausibility%20and%20domain-specific%20reasoning.%20Our%20study%20provides%20a%20systematic%20assessment%20of%20recent%20models%20and%20shows%20that%20video%20generative%20models%2C%20despite%20strong%20visual%20fidelity%2C%20often%20struggle%20to%20maintain%20physically%20coherent%20dynamics%2C%20while%20multi-modal%20large%20language%20models%20exhibit%20limited%20performance%20in%20forecasting%20and%20causal%20interpretation.%20These%20observations%20suggest%20that%20current%20systems%20are%20still%20at%20an%20early%20stage%20in%20handling%20the%20perceptual%20and%20predictive%20demands%20of%20Physical%20AI.%20In%20summary%2C%20PAI-Bench%20establishes%20a%20realistic%20foundation%20for%20evaluating%20Physical%20AI%20and%20highlights%20key%20gaps%20that%20future%20systems%20must%20address.&entry.1838667208=http%3A//arxiv.org/abs/2512.01989v1&entry.124074799=Read"},
{"title": "Learning Reduced Representations for Quantum Classifiers", "author": "Patrick Odagiu and Vasilis Belis and Lennart Schulze and Panagiotis Barkoutsos and Michele Grossi and Florentin Reiter and G\u00fcnther Dissertori and Ivano Tavernelli and Sofia Vallecorsa", "abstract": "Data sets that are specified by a large number of features are currently outside the area of applicability for quantum machine learning algorithms. An immediate solution to this impasse is the application of dimensionality reduction methods before passing the data to the quantum algorithm. We investigate six conventional feature extraction algorithms and five autoencoder-based dimensionality reduction models to a particle physics data set with 67 features. The reduced representations generated by these models are then used to train a quantum support vector machine for solving a binary classification problem: whether a Higgs boson is produced in proton collisions at the LHC. We show that the autoencoder methods learn a better lower-dimensional representation of the data, with the method we design, the Sinkclass autoencoder, performing 40% better than the baseline. The methods developed here open up the applicability of quantum machine learning to a larger array of data sets. Moreover, we provide a recipe for effective dimensionality reduction in this context.", "link": "http://arxiv.org/abs/2512.01509v1", "date": "2025-12-01", "relevancy": 2.3338, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4831}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4669}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Reduced%20Representations%20for%20Quantum%20Classifiers&body=Title%3A%20Learning%20Reduced%20Representations%20for%20Quantum%20Classifiers%0AAuthor%3A%20Patrick%20Odagiu%20and%20Vasilis%20Belis%20and%20Lennart%20Schulze%20and%20Panagiotis%20Barkoutsos%20and%20Michele%20Grossi%20and%20Florentin%20Reiter%20and%20G%C3%BCnther%20Dissertori%20and%20Ivano%20Tavernelli%20and%20Sofia%20Vallecorsa%0AAbstract%3A%20Data%20sets%20that%20are%20specified%20by%20a%20large%20number%20of%20features%20are%20currently%20outside%20the%20area%20of%20applicability%20for%20quantum%20machine%20learning%20algorithms.%20An%20immediate%20solution%20to%20this%20impasse%20is%20the%20application%20of%20dimensionality%20reduction%20methods%20before%20passing%20the%20data%20to%20the%20quantum%20algorithm.%20We%20investigate%20six%20conventional%20feature%20extraction%20algorithms%20and%20five%20autoencoder-based%20dimensionality%20reduction%20models%20to%20a%20particle%20physics%20data%20set%20with%2067%20features.%20The%20reduced%20representations%20generated%20by%20these%20models%20are%20then%20used%20to%20train%20a%20quantum%20support%20vector%20machine%20for%20solving%20a%20binary%20classification%20problem%3A%20whether%20a%20Higgs%20boson%20is%20produced%20in%20proton%20collisions%20at%20the%20LHC.%20We%20show%20that%20the%20autoencoder%20methods%20learn%20a%20better%20lower-dimensional%20representation%20of%20the%20data%2C%20with%20the%20method%20we%20design%2C%20the%20Sinkclass%20autoencoder%2C%20performing%2040%25%20better%20than%20the%20baseline.%20The%20methods%20developed%20here%20open%20up%20the%20applicability%20of%20quantum%20machine%20learning%20to%20a%20larger%20array%20of%20data%20sets.%20Moreover%2C%20we%20provide%20a%20recipe%20for%20effective%20dimensionality%20reduction%20in%20this%20context.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Reduced%2520Representations%2520for%2520Quantum%2520Classifiers%26entry.906535625%3DPatrick%2520Odagiu%2520and%2520Vasilis%2520Belis%2520and%2520Lennart%2520Schulze%2520and%2520Panagiotis%2520Barkoutsos%2520and%2520Michele%2520Grossi%2520and%2520Florentin%2520Reiter%2520and%2520G%25C3%25BCnther%2520Dissertori%2520and%2520Ivano%2520Tavernelli%2520and%2520Sofia%2520Vallecorsa%26entry.1292438233%3DData%2520sets%2520that%2520are%2520specified%2520by%2520a%2520large%2520number%2520of%2520features%2520are%2520currently%2520outside%2520the%2520area%2520of%2520applicability%2520for%2520quantum%2520machine%2520learning%2520algorithms.%2520An%2520immediate%2520solution%2520to%2520this%2520impasse%2520is%2520the%2520application%2520of%2520dimensionality%2520reduction%2520methods%2520before%2520passing%2520the%2520data%2520to%2520the%2520quantum%2520algorithm.%2520We%2520investigate%2520six%2520conventional%2520feature%2520extraction%2520algorithms%2520and%2520five%2520autoencoder-based%2520dimensionality%2520reduction%2520models%2520to%2520a%2520particle%2520physics%2520data%2520set%2520with%252067%2520features.%2520The%2520reduced%2520representations%2520generated%2520by%2520these%2520models%2520are%2520then%2520used%2520to%2520train%2520a%2520quantum%2520support%2520vector%2520machine%2520for%2520solving%2520a%2520binary%2520classification%2520problem%253A%2520whether%2520a%2520Higgs%2520boson%2520is%2520produced%2520in%2520proton%2520collisions%2520at%2520the%2520LHC.%2520We%2520show%2520that%2520the%2520autoencoder%2520methods%2520learn%2520a%2520better%2520lower-dimensional%2520representation%2520of%2520the%2520data%252C%2520with%2520the%2520method%2520we%2520design%252C%2520the%2520Sinkclass%2520autoencoder%252C%2520performing%252040%2525%2520better%2520than%2520the%2520baseline.%2520The%2520methods%2520developed%2520here%2520open%2520up%2520the%2520applicability%2520of%2520quantum%2520machine%2520learning%2520to%2520a%2520larger%2520array%2520of%2520data%2520sets.%2520Moreover%252C%2520we%2520provide%2520a%2520recipe%2520for%2520effective%2520dimensionality%2520reduction%2520in%2520this%2520context.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Reduced%20Representations%20for%20Quantum%20Classifiers&entry.906535625=Patrick%20Odagiu%20and%20Vasilis%20Belis%20and%20Lennart%20Schulze%20and%20Panagiotis%20Barkoutsos%20and%20Michele%20Grossi%20and%20Florentin%20Reiter%20and%20G%C3%BCnther%20Dissertori%20and%20Ivano%20Tavernelli%20and%20Sofia%20Vallecorsa&entry.1292438233=Data%20sets%20that%20are%20specified%20by%20a%20large%20number%20of%20features%20are%20currently%20outside%20the%20area%20of%20applicability%20for%20quantum%20machine%20learning%20algorithms.%20An%20immediate%20solution%20to%20this%20impasse%20is%20the%20application%20of%20dimensionality%20reduction%20methods%20before%20passing%20the%20data%20to%20the%20quantum%20algorithm.%20We%20investigate%20six%20conventional%20feature%20extraction%20algorithms%20and%20five%20autoencoder-based%20dimensionality%20reduction%20models%20to%20a%20particle%20physics%20data%20set%20with%2067%20features.%20The%20reduced%20representations%20generated%20by%20these%20models%20are%20then%20used%20to%20train%20a%20quantum%20support%20vector%20machine%20for%20solving%20a%20binary%20classification%20problem%3A%20whether%20a%20Higgs%20boson%20is%20produced%20in%20proton%20collisions%20at%20the%20LHC.%20We%20show%20that%20the%20autoencoder%20methods%20learn%20a%20better%20lower-dimensional%20representation%20of%20the%20data%2C%20with%20the%20method%20we%20design%2C%20the%20Sinkclass%20autoencoder%2C%20performing%2040%25%20better%20than%20the%20baseline.%20The%20methods%20developed%20here%20open%20up%20the%20applicability%20of%20quantum%20machine%20learning%20to%20a%20larger%20array%20of%20data%20sets.%20Moreover%2C%20we%20provide%20a%20recipe%20for%20effective%20dimensionality%20reduction%20in%20this%20context.&entry.1838667208=http%3A//arxiv.org/abs/2512.01509v1&entry.124074799=Read"},
{"title": "Delta Sum Learning: an approach for fast and global convergence in Gossip Learning", "author": "Tom Goethals and Merlijn Sebrechts and Stijn De Schrijver and Filip De Turck and Bruno Volckaert", "abstract": "Federated Learning is a popular approach for distributed learning due to its security and computational benefits. With the advent of powerful devices in the network edge, Gossip Learning further decentralizes Federated Learning by removing centralized integration and relying fully on peer to peer updates. However, the averaging methods generally used in both Federated and Gossip Learning are not ideal for model accuracy and global convergence. Additionally, there are few options to deploy Learning workloads in the edge as part of a larger application using a declarative approach such as Kubernetes manifests. This paper proposes Delta Sum Learning as a method to improve the basic aggregation operation in Gossip Learning, and implements it in a decentralized orchestration framework based on Open Application Model, which allows for dynamic node discovery and intent-driven deployment of multi-workload applications. Evaluation results show that Delta Sum performance is on par with alternative integration methods for 10 node topologies, but results in a 58% lower global accuracy drop when scaling to 50 nodes. Overall, it shows strong global convergence and a logarithmic loss of accuracy with increasing topology size compared to a linear loss for alternatives under limited connectivity.", "link": "http://arxiv.org/abs/2512.01549v1", "date": "2025-12-01", "relevancy": 2.3214, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4676}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4627}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Delta%20Sum%20Learning%3A%20an%20approach%20for%20fast%20and%20global%20convergence%20in%20Gossip%20Learning&body=Title%3A%20Delta%20Sum%20Learning%3A%20an%20approach%20for%20fast%20and%20global%20convergence%20in%20Gossip%20Learning%0AAuthor%3A%20Tom%20Goethals%20and%20Merlijn%20Sebrechts%20and%20Stijn%20De%20Schrijver%20and%20Filip%20De%20Turck%20and%20Bruno%20Volckaert%0AAbstract%3A%20Federated%20Learning%20is%20a%20popular%20approach%20for%20distributed%20learning%20due%20to%20its%20security%20and%20computational%20benefits.%20With%20the%20advent%20of%20powerful%20devices%20in%20the%20network%20edge%2C%20Gossip%20Learning%20further%20decentralizes%20Federated%20Learning%20by%20removing%20centralized%20integration%20and%20relying%20fully%20on%20peer%20to%20peer%20updates.%20However%2C%20the%20averaging%20methods%20generally%20used%20in%20both%20Federated%20and%20Gossip%20Learning%20are%20not%20ideal%20for%20model%20accuracy%20and%20global%20convergence.%20Additionally%2C%20there%20are%20few%20options%20to%20deploy%20Learning%20workloads%20in%20the%20edge%20as%20part%20of%20a%20larger%20application%20using%20a%20declarative%20approach%20such%20as%20Kubernetes%20manifests.%20This%20paper%20proposes%20Delta%20Sum%20Learning%20as%20a%20method%20to%20improve%20the%20basic%20aggregation%20operation%20in%20Gossip%20Learning%2C%20and%20implements%20it%20in%20a%20decentralized%20orchestration%20framework%20based%20on%20Open%20Application%20Model%2C%20which%20allows%20for%20dynamic%20node%20discovery%20and%20intent-driven%20deployment%20of%20multi-workload%20applications.%20Evaluation%20results%20show%20that%20Delta%20Sum%20performance%20is%20on%20par%20with%20alternative%20integration%20methods%20for%2010%20node%20topologies%2C%20but%20results%20in%20a%2058%25%20lower%20global%20accuracy%20drop%20when%20scaling%20to%2050%20nodes.%20Overall%2C%20it%20shows%20strong%20global%20convergence%20and%20a%20logarithmic%20loss%20of%20accuracy%20with%20increasing%20topology%20size%20compared%20to%20a%20linear%20loss%20for%20alternatives%20under%20limited%20connectivity.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDelta%2520Sum%2520Learning%253A%2520an%2520approach%2520for%2520fast%2520and%2520global%2520convergence%2520in%2520Gossip%2520Learning%26entry.906535625%3DTom%2520Goethals%2520and%2520Merlijn%2520Sebrechts%2520and%2520Stijn%2520De%2520Schrijver%2520and%2520Filip%2520De%2520Turck%2520and%2520Bruno%2520Volckaert%26entry.1292438233%3DFederated%2520Learning%2520is%2520a%2520popular%2520approach%2520for%2520distributed%2520learning%2520due%2520to%2520its%2520security%2520and%2520computational%2520benefits.%2520With%2520the%2520advent%2520of%2520powerful%2520devices%2520in%2520the%2520network%2520edge%252C%2520Gossip%2520Learning%2520further%2520decentralizes%2520Federated%2520Learning%2520by%2520removing%2520centralized%2520integration%2520and%2520relying%2520fully%2520on%2520peer%2520to%2520peer%2520updates.%2520However%252C%2520the%2520averaging%2520methods%2520generally%2520used%2520in%2520both%2520Federated%2520and%2520Gossip%2520Learning%2520are%2520not%2520ideal%2520for%2520model%2520accuracy%2520and%2520global%2520convergence.%2520Additionally%252C%2520there%2520are%2520few%2520options%2520to%2520deploy%2520Learning%2520workloads%2520in%2520the%2520edge%2520as%2520part%2520of%2520a%2520larger%2520application%2520using%2520a%2520declarative%2520approach%2520such%2520as%2520Kubernetes%2520manifests.%2520This%2520paper%2520proposes%2520Delta%2520Sum%2520Learning%2520as%2520a%2520method%2520to%2520improve%2520the%2520basic%2520aggregation%2520operation%2520in%2520Gossip%2520Learning%252C%2520and%2520implements%2520it%2520in%2520a%2520decentralized%2520orchestration%2520framework%2520based%2520on%2520Open%2520Application%2520Model%252C%2520which%2520allows%2520for%2520dynamic%2520node%2520discovery%2520and%2520intent-driven%2520deployment%2520of%2520multi-workload%2520applications.%2520Evaluation%2520results%2520show%2520that%2520Delta%2520Sum%2520performance%2520is%2520on%2520par%2520with%2520alternative%2520integration%2520methods%2520for%252010%2520node%2520topologies%252C%2520but%2520results%2520in%2520a%252058%2525%2520lower%2520global%2520accuracy%2520drop%2520when%2520scaling%2520to%252050%2520nodes.%2520Overall%252C%2520it%2520shows%2520strong%2520global%2520convergence%2520and%2520a%2520logarithmic%2520loss%2520of%2520accuracy%2520with%2520increasing%2520topology%2520size%2520compared%2520to%2520a%2520linear%2520loss%2520for%2520alternatives%2520under%2520limited%2520connectivity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Delta%20Sum%20Learning%3A%20an%20approach%20for%20fast%20and%20global%20convergence%20in%20Gossip%20Learning&entry.906535625=Tom%20Goethals%20and%20Merlijn%20Sebrechts%20and%20Stijn%20De%20Schrijver%20and%20Filip%20De%20Turck%20and%20Bruno%20Volckaert&entry.1292438233=Federated%20Learning%20is%20a%20popular%20approach%20for%20distributed%20learning%20due%20to%20its%20security%20and%20computational%20benefits.%20With%20the%20advent%20of%20powerful%20devices%20in%20the%20network%20edge%2C%20Gossip%20Learning%20further%20decentralizes%20Federated%20Learning%20by%20removing%20centralized%20integration%20and%20relying%20fully%20on%20peer%20to%20peer%20updates.%20However%2C%20the%20averaging%20methods%20generally%20used%20in%20both%20Federated%20and%20Gossip%20Learning%20are%20not%20ideal%20for%20model%20accuracy%20and%20global%20convergence.%20Additionally%2C%20there%20are%20few%20options%20to%20deploy%20Learning%20workloads%20in%20the%20edge%20as%20part%20of%20a%20larger%20application%20using%20a%20declarative%20approach%20such%20as%20Kubernetes%20manifests.%20This%20paper%20proposes%20Delta%20Sum%20Learning%20as%20a%20method%20to%20improve%20the%20basic%20aggregation%20operation%20in%20Gossip%20Learning%2C%20and%20implements%20it%20in%20a%20decentralized%20orchestration%20framework%20based%20on%20Open%20Application%20Model%2C%20which%20allows%20for%20dynamic%20node%20discovery%20and%20intent-driven%20deployment%20of%20multi-workload%20applications.%20Evaluation%20results%20show%20that%20Delta%20Sum%20performance%20is%20on%20par%20with%20alternative%20integration%20methods%20for%2010%20node%20topologies%2C%20but%20results%20in%20a%2058%25%20lower%20global%20accuracy%20drop%20when%20scaling%20to%2050%20nodes.%20Overall%2C%20it%20shows%20strong%20global%20convergence%20and%20a%20logarithmic%20loss%20of%20accuracy%20with%20increasing%20topology%20size%20compared%20to%20a%20linear%20loss%20for%20alternatives%20under%20limited%20connectivity.&entry.1838667208=http%3A//arxiv.org/abs/2512.01549v1&entry.124074799=Read"},
{"title": "CaliTex: Geometry-Calibrated Attention for View-Coherent 3D Texture Generation", "author": "Chenyu Liu and Hongze Chen and Jingzhi Bao and Lingting Zhu and Runze Zhang and Weikai Chen and Zeyu Hu and Yingda Yin and Keyang Luo and Xin Wang", "abstract": "Despite major advances brought by diffusion-based models, current 3D texture generation systems remain hindered by cross-view inconsistency -- textures that appear convincing from one viewpoint often fail to align across others. We find that this issue arises from attention ambiguity, where unstructured full attention is applied indiscriminately across tokens and modalities, causing geometric confusion and unstable appearance-structure coupling. To address this, we introduce CaliTex, a framework of geometry-calibrated attention that explicitly aligns attention with 3D structure. It introduces two modules: Part-Aligned Attention that enforces spatial alignment across semantically matched parts, and Condition-Routed Attention which routes appearance information through geometry-conditioned pathways to maintain spatial fidelity. Coupled with a two-stage diffusion transformer, CaliTex makes geometric coherence an inherent behavior of the network rather than a byproduct of optimization. Empirically, CaliTex produces seamless and view-consistent textures and outperforms both open-source and commercial baselines.", "link": "http://arxiv.org/abs/2511.21309v2", "date": "2025-12-01", "relevancy": 2.3197, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.582}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.582}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaliTex%3A%20Geometry-Calibrated%20Attention%20for%20View-Coherent%203D%20Texture%20Generation&body=Title%3A%20CaliTex%3A%20Geometry-Calibrated%20Attention%20for%20View-Coherent%203D%20Texture%20Generation%0AAuthor%3A%20Chenyu%20Liu%20and%20Hongze%20Chen%20and%20Jingzhi%20Bao%20and%20Lingting%20Zhu%20and%20Runze%20Zhang%20and%20Weikai%20Chen%20and%20Zeyu%20Hu%20and%20Yingda%20Yin%20and%20Keyang%20Luo%20and%20Xin%20Wang%0AAbstract%3A%20Despite%20major%20advances%20brought%20by%20diffusion-based%20models%2C%20current%203D%20texture%20generation%20systems%20remain%20hindered%20by%20cross-view%20inconsistency%20--%20textures%20that%20appear%20convincing%20from%20one%20viewpoint%20often%20fail%20to%20align%20across%20others.%20We%20find%20that%20this%20issue%20arises%20from%20attention%20ambiguity%2C%20where%20unstructured%20full%20attention%20is%20applied%20indiscriminately%20across%20tokens%20and%20modalities%2C%20causing%20geometric%20confusion%20and%20unstable%20appearance-structure%20coupling.%20To%20address%20this%2C%20we%20introduce%20CaliTex%2C%20a%20framework%20of%20geometry-calibrated%20attention%20that%20explicitly%20aligns%20attention%20with%203D%20structure.%20It%20introduces%20two%20modules%3A%20Part-Aligned%20Attention%20that%20enforces%20spatial%20alignment%20across%20semantically%20matched%20parts%2C%20and%20Condition-Routed%20Attention%20which%20routes%20appearance%20information%20through%20geometry-conditioned%20pathways%20to%20maintain%20spatial%20fidelity.%20Coupled%20with%20a%20two-stage%20diffusion%20transformer%2C%20CaliTex%20makes%20geometric%20coherence%20an%20inherent%20behavior%20of%20the%20network%20rather%20than%20a%20byproduct%20of%20optimization.%20Empirically%2C%20CaliTex%20produces%20seamless%20and%20view-consistent%20textures%20and%20outperforms%20both%20open-source%20and%20commercial%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaliTex%253A%2520Geometry-Calibrated%2520Attention%2520for%2520View-Coherent%25203D%2520Texture%2520Generation%26entry.906535625%3DChenyu%2520Liu%2520and%2520Hongze%2520Chen%2520and%2520Jingzhi%2520Bao%2520and%2520Lingting%2520Zhu%2520and%2520Runze%2520Zhang%2520and%2520Weikai%2520Chen%2520and%2520Zeyu%2520Hu%2520and%2520Yingda%2520Yin%2520and%2520Keyang%2520Luo%2520and%2520Xin%2520Wang%26entry.1292438233%3DDespite%2520major%2520advances%2520brought%2520by%2520diffusion-based%2520models%252C%2520current%25203D%2520texture%2520generation%2520systems%2520remain%2520hindered%2520by%2520cross-view%2520inconsistency%2520--%2520textures%2520that%2520appear%2520convincing%2520from%2520one%2520viewpoint%2520often%2520fail%2520to%2520align%2520across%2520others.%2520We%2520find%2520that%2520this%2520issue%2520arises%2520from%2520attention%2520ambiguity%252C%2520where%2520unstructured%2520full%2520attention%2520is%2520applied%2520indiscriminately%2520across%2520tokens%2520and%2520modalities%252C%2520causing%2520geometric%2520confusion%2520and%2520unstable%2520appearance-structure%2520coupling.%2520To%2520address%2520this%252C%2520we%2520introduce%2520CaliTex%252C%2520a%2520framework%2520of%2520geometry-calibrated%2520attention%2520that%2520explicitly%2520aligns%2520attention%2520with%25203D%2520structure.%2520It%2520introduces%2520two%2520modules%253A%2520Part-Aligned%2520Attention%2520that%2520enforces%2520spatial%2520alignment%2520across%2520semantically%2520matched%2520parts%252C%2520and%2520Condition-Routed%2520Attention%2520which%2520routes%2520appearance%2520information%2520through%2520geometry-conditioned%2520pathways%2520to%2520maintain%2520spatial%2520fidelity.%2520Coupled%2520with%2520a%2520two-stage%2520diffusion%2520transformer%252C%2520CaliTex%2520makes%2520geometric%2520coherence%2520an%2520inherent%2520behavior%2520of%2520the%2520network%2520rather%2520than%2520a%2520byproduct%2520of%2520optimization.%2520Empirically%252C%2520CaliTex%2520produces%2520seamless%2520and%2520view-consistent%2520textures%2520and%2520outperforms%2520both%2520open-source%2520and%2520commercial%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaliTex%3A%20Geometry-Calibrated%20Attention%20for%20View-Coherent%203D%20Texture%20Generation&entry.906535625=Chenyu%20Liu%20and%20Hongze%20Chen%20and%20Jingzhi%20Bao%20and%20Lingting%20Zhu%20and%20Runze%20Zhang%20and%20Weikai%20Chen%20and%20Zeyu%20Hu%20and%20Yingda%20Yin%20and%20Keyang%20Luo%20and%20Xin%20Wang&entry.1292438233=Despite%20major%20advances%20brought%20by%20diffusion-based%20models%2C%20current%203D%20texture%20generation%20systems%20remain%20hindered%20by%20cross-view%20inconsistency%20--%20textures%20that%20appear%20convincing%20from%20one%20viewpoint%20often%20fail%20to%20align%20across%20others.%20We%20find%20that%20this%20issue%20arises%20from%20attention%20ambiguity%2C%20where%20unstructured%20full%20attention%20is%20applied%20indiscriminately%20across%20tokens%20and%20modalities%2C%20causing%20geometric%20confusion%20and%20unstable%20appearance-structure%20coupling.%20To%20address%20this%2C%20we%20introduce%20CaliTex%2C%20a%20framework%20of%20geometry-calibrated%20attention%20that%20explicitly%20aligns%20attention%20with%203D%20structure.%20It%20introduces%20two%20modules%3A%20Part-Aligned%20Attention%20that%20enforces%20spatial%20alignment%20across%20semantically%20matched%20parts%2C%20and%20Condition-Routed%20Attention%20which%20routes%20appearance%20information%20through%20geometry-conditioned%20pathways%20to%20maintain%20spatial%20fidelity.%20Coupled%20with%20a%20two-stage%20diffusion%20transformer%2C%20CaliTex%20makes%20geometric%20coherence%20an%20inherent%20behavior%20of%20the%20network%20rather%20than%20a%20byproduct%20of%20optimization.%20Empirically%2C%20CaliTex%20produces%20seamless%20and%20view-consistent%20textures%20and%20outperforms%20both%20open-source%20and%20commercial%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2511.21309v2&entry.124074799=Read"},
{"title": "Learning Sim-to-Real Humanoid Locomotion in 15 Minutes", "author": "Younggyo Seo and Carmelo Sferrazza and Juyue Chen and Guanya Shi and Rocky Duan and Pieter Abbeel", "abstract": "Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive scale with thousands of parallel environments through carefully tuned design choices and minimalist reward functions. We demonstrate rapid end-to-end learning of humanoid locomotion controllers on Unitree G1 and Booster T1 robots under strong domain randomization, e.g., randomized dynamics, rough terrain, and push perturbations, as well as fast training of whole-body human-motion tracking policies. We provide videos and open-source implementation at: https://younggyo.me/fastsac-humanoid.", "link": "http://arxiv.org/abs/2512.01996v1", "date": "2025-12-01", "relevancy": 2.2939, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5843}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5753}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Sim-to-Real%20Humanoid%20Locomotion%20in%2015%20Minutes&body=Title%3A%20Learning%20Sim-to-Real%20Humanoid%20Locomotion%20in%2015%20Minutes%0AAuthor%3A%20Younggyo%20Seo%20and%20Carmelo%20Sferrazza%20and%20Juyue%20Chen%20and%20Guanya%20Shi%20and%20Rocky%20Duan%20and%20Pieter%20Abbeel%0AAbstract%3A%20Massively%20parallel%20simulation%20has%20reduced%20reinforcement%20learning%20%28RL%29%20training%20time%20for%20robots%20from%20days%20to%20minutes.%20However%2C%20achieving%20fast%20and%20reliable%20sim-to-real%20RL%20for%20humanoid%20control%20remains%20difficult%20due%20to%20the%20challenges%20introduced%20by%20factors%20such%20as%20high%20dimensionality%20and%20domain%20randomization.%20In%20this%20work%2C%20we%20introduce%20a%20simple%20and%20practical%20recipe%20based%20on%20off-policy%20RL%20algorithms%2C%20i.e.%2C%20FastSAC%20and%20FastTD3%2C%20that%20enables%20rapid%20training%20of%20humanoid%20locomotion%20policies%20in%20just%2015%20minutes%20with%20a%20single%20RTX%204090%20GPU.%20Our%20simple%20recipe%20stabilizes%20off-policy%20RL%20algorithms%20at%20massive%20scale%20with%20thousands%20of%20parallel%20environments%20through%20carefully%20tuned%20design%20choices%20and%20minimalist%20reward%20functions.%20We%20demonstrate%20rapid%20end-to-end%20learning%20of%20humanoid%20locomotion%20controllers%20on%20Unitree%20G1%20and%20Booster%20T1%20robots%20under%20strong%20domain%20randomization%2C%20e.g.%2C%20randomized%20dynamics%2C%20rough%20terrain%2C%20and%20push%20perturbations%2C%20as%20well%20as%20fast%20training%20of%20whole-body%20human-motion%20tracking%20policies.%20We%20provide%20videos%20and%20open-source%20implementation%20at%3A%20https%3A//younggyo.me/fastsac-humanoid.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Sim-to-Real%2520Humanoid%2520Locomotion%2520in%252015%2520Minutes%26entry.906535625%3DYounggyo%2520Seo%2520and%2520Carmelo%2520Sferrazza%2520and%2520Juyue%2520Chen%2520and%2520Guanya%2520Shi%2520and%2520Rocky%2520Duan%2520and%2520Pieter%2520Abbeel%26entry.1292438233%3DMassively%2520parallel%2520simulation%2520has%2520reduced%2520reinforcement%2520learning%2520%2528RL%2529%2520training%2520time%2520for%2520robots%2520from%2520days%2520to%2520minutes.%2520However%252C%2520achieving%2520fast%2520and%2520reliable%2520sim-to-real%2520RL%2520for%2520humanoid%2520control%2520remains%2520difficult%2520due%2520to%2520the%2520challenges%2520introduced%2520by%2520factors%2520such%2520as%2520high%2520dimensionality%2520and%2520domain%2520randomization.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520simple%2520and%2520practical%2520recipe%2520based%2520on%2520off-policy%2520RL%2520algorithms%252C%2520i.e.%252C%2520FastSAC%2520and%2520FastTD3%252C%2520that%2520enables%2520rapid%2520training%2520of%2520humanoid%2520locomotion%2520policies%2520in%2520just%252015%2520minutes%2520with%2520a%2520single%2520RTX%25204090%2520GPU.%2520Our%2520simple%2520recipe%2520stabilizes%2520off-policy%2520RL%2520algorithms%2520at%2520massive%2520scale%2520with%2520thousands%2520of%2520parallel%2520environments%2520through%2520carefully%2520tuned%2520design%2520choices%2520and%2520minimalist%2520reward%2520functions.%2520We%2520demonstrate%2520rapid%2520end-to-end%2520learning%2520of%2520humanoid%2520locomotion%2520controllers%2520on%2520Unitree%2520G1%2520and%2520Booster%2520T1%2520robots%2520under%2520strong%2520domain%2520randomization%252C%2520e.g.%252C%2520randomized%2520dynamics%252C%2520rough%2520terrain%252C%2520and%2520push%2520perturbations%252C%2520as%2520well%2520as%2520fast%2520training%2520of%2520whole-body%2520human-motion%2520tracking%2520policies.%2520We%2520provide%2520videos%2520and%2520open-source%2520implementation%2520at%253A%2520https%253A//younggyo.me/fastsac-humanoid.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Sim-to-Real%20Humanoid%20Locomotion%20in%2015%20Minutes&entry.906535625=Younggyo%20Seo%20and%20Carmelo%20Sferrazza%20and%20Juyue%20Chen%20and%20Guanya%20Shi%20and%20Rocky%20Duan%20and%20Pieter%20Abbeel&entry.1292438233=Massively%20parallel%20simulation%20has%20reduced%20reinforcement%20learning%20%28RL%29%20training%20time%20for%20robots%20from%20days%20to%20minutes.%20However%2C%20achieving%20fast%20and%20reliable%20sim-to-real%20RL%20for%20humanoid%20control%20remains%20difficult%20due%20to%20the%20challenges%20introduced%20by%20factors%20such%20as%20high%20dimensionality%20and%20domain%20randomization.%20In%20this%20work%2C%20we%20introduce%20a%20simple%20and%20practical%20recipe%20based%20on%20off-policy%20RL%20algorithms%2C%20i.e.%2C%20FastSAC%20and%20FastTD3%2C%20that%20enables%20rapid%20training%20of%20humanoid%20locomotion%20policies%20in%20just%2015%20minutes%20with%20a%20single%20RTX%204090%20GPU.%20Our%20simple%20recipe%20stabilizes%20off-policy%20RL%20algorithms%20at%20massive%20scale%20with%20thousands%20of%20parallel%20environments%20through%20carefully%20tuned%20design%20choices%20and%20minimalist%20reward%20functions.%20We%20demonstrate%20rapid%20end-to-end%20learning%20of%20humanoid%20locomotion%20controllers%20on%20Unitree%20G1%20and%20Booster%20T1%20robots%20under%20strong%20domain%20randomization%2C%20e.g.%2C%20randomized%20dynamics%2C%20rough%20terrain%2C%20and%20push%20perturbations%2C%20as%20well%20as%20fast%20training%20of%20whole-body%20human-motion%20tracking%20policies.%20We%20provide%20videos%20and%20open-source%20implementation%20at%3A%20https%3A//younggyo.me/fastsac-humanoid.&entry.1838667208=http%3A//arxiv.org/abs/2512.01996v1&entry.124074799=Read"},
{"title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks", "author": "Yanlin Wang and Xinyi Xu and Jiachi Chen and Tingting Bi and Wenchao Gu and Zibin Zheng", "abstract": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.", "link": "http://arxiv.org/abs/2512.01939v1", "date": "2025-12-01", "relevancy": 2.2877, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4582}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Study%20of%20Agent%20Developer%20Practices%20in%20AI%20Agent%20Frameworks&body=Title%3A%20An%20Empirical%20Study%20of%20Agent%20Developer%20Practices%20in%20AI%20Agent%20Frameworks%0AAuthor%3A%20Yanlin%20Wang%20and%20Xinyi%20Xu%20and%20Jiachi%20Chen%20and%20Tingting%20Bi%20and%20Wenchao%20Gu%20and%20Zibin%20Zheng%0AAbstract%3A%20The%20rise%20of%20large%20language%20models%20%28LLMs%29%20has%20sparked%20a%20surge%20of%20interest%20in%20agents%2C%20leading%20to%20the%20rapid%20growth%20of%20agent%20frameworks.%20Agent%20frameworks%20are%20software%20toolkits%20and%20libraries%20that%20provide%20standardized%20components%2C%20abstractions%2C%20and%20orchestration%20mechanisms%20to%20simplify%20agent%20development.%20Despite%20widespread%20use%20of%20agent%20frameworks%2C%20their%20practical%20applications%20and%20how%20they%20influence%20the%20agent%20development%20process%20remain%20underexplored.%20Different%20agent%20frameworks%20encounter%20similar%20problems%20during%20use%2C%20indicating%20that%20these%20recurring%20issues%20deserve%20greater%20attention%20and%20call%20for%20further%20improvements%20in%20agent%20framework%20design.%20Meanwhile%2C%20as%20the%20number%20of%20agent%20frameworks%20continues%20to%20grow%20and%20evolve%2C%20more%20than%2080%25%20of%20developers%20report%20difficulties%20in%20identifying%20the%20frameworks%20that%20best%20meet%20their%20specific%20development%20requirements.%20In%20this%20paper%2C%20we%20conduct%20the%20first%20empirical%20study%20of%20LLM-based%20agent%20frameworks%2C%20exploring%20real-world%20experiences%20of%20developers%20in%20building%20AI%20agents.%20To%20compare%20how%20well%20the%20agent%20frameworks%20meet%20developer%20needs%2C%20we%20further%20collect%20developer%20discussions%20for%20the%20ten%20previously%20identified%20agent%20frameworks%2C%20resulting%20in%20a%20total%20of%2011%2C910%20discussions.%20Finally%2C%20by%20analyzing%20these%20discussions%2C%20we%20compare%20the%20frameworks%20across%20five%20dimensions%3A%20development%20efficiency%2C%20functional%20abstraction%2C%20learning%20cost%2C%20performance%20optimization%2C%20and%20maintainability%2C%20which%20refers%20to%20how%20easily%20developers%20can%20update%20and%20extend%20both%20the%20framework%20itself%20and%20the%20agents%20built%20upon%20it%20over%20time.%20Our%20comparative%20analysis%20reveals%20significant%20differences%20among%20frameworks%20in%20how%20they%20meet%20the%20needs%20of%20agent%20developers.%20Overall%2C%20we%20provide%20a%20set%20of%20findings%20and%20implications%20for%20the%20LLM-driven%20AI%20agent%20framework%20ecosystem%20and%20offer%20insights%20for%20the%20design%20of%20future%20LLM-based%20agent%20frameworks%20and%20agent%20developers.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Study%2520of%2520Agent%2520Developer%2520Practices%2520in%2520AI%2520Agent%2520Frameworks%26entry.906535625%3DYanlin%2520Wang%2520and%2520Xinyi%2520Xu%2520and%2520Jiachi%2520Chen%2520and%2520Tingting%2520Bi%2520and%2520Wenchao%2520Gu%2520and%2520Zibin%2520Zheng%26entry.1292438233%3DThe%2520rise%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520sparked%2520a%2520surge%2520of%2520interest%2520in%2520agents%252C%2520leading%2520to%2520the%2520rapid%2520growth%2520of%2520agent%2520frameworks.%2520Agent%2520frameworks%2520are%2520software%2520toolkits%2520and%2520libraries%2520that%2520provide%2520standardized%2520components%252C%2520abstractions%252C%2520and%2520orchestration%2520mechanisms%2520to%2520simplify%2520agent%2520development.%2520Despite%2520widespread%2520use%2520of%2520agent%2520frameworks%252C%2520their%2520practical%2520applications%2520and%2520how%2520they%2520influence%2520the%2520agent%2520development%2520process%2520remain%2520underexplored.%2520Different%2520agent%2520frameworks%2520encounter%2520similar%2520problems%2520during%2520use%252C%2520indicating%2520that%2520these%2520recurring%2520issues%2520deserve%2520greater%2520attention%2520and%2520call%2520for%2520further%2520improvements%2520in%2520agent%2520framework%2520design.%2520Meanwhile%252C%2520as%2520the%2520number%2520of%2520agent%2520frameworks%2520continues%2520to%2520grow%2520and%2520evolve%252C%2520more%2520than%252080%2525%2520of%2520developers%2520report%2520difficulties%2520in%2520identifying%2520the%2520frameworks%2520that%2520best%2520meet%2520their%2520specific%2520development%2520requirements.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520the%2520first%2520empirical%2520study%2520of%2520LLM-based%2520agent%2520frameworks%252C%2520exploring%2520real-world%2520experiences%2520of%2520developers%2520in%2520building%2520AI%2520agents.%2520To%2520compare%2520how%2520well%2520the%2520agent%2520frameworks%2520meet%2520developer%2520needs%252C%2520we%2520further%2520collect%2520developer%2520discussions%2520for%2520the%2520ten%2520previously%2520identified%2520agent%2520frameworks%252C%2520resulting%2520in%2520a%2520total%2520of%252011%252C910%2520discussions.%2520Finally%252C%2520by%2520analyzing%2520these%2520discussions%252C%2520we%2520compare%2520the%2520frameworks%2520across%2520five%2520dimensions%253A%2520development%2520efficiency%252C%2520functional%2520abstraction%252C%2520learning%2520cost%252C%2520performance%2520optimization%252C%2520and%2520maintainability%252C%2520which%2520refers%2520to%2520how%2520easily%2520developers%2520can%2520update%2520and%2520extend%2520both%2520the%2520framework%2520itself%2520and%2520the%2520agents%2520built%2520upon%2520it%2520over%2520time.%2520Our%2520comparative%2520analysis%2520reveals%2520significant%2520differences%2520among%2520frameworks%2520in%2520how%2520they%2520meet%2520the%2520needs%2520of%2520agent%2520developers.%2520Overall%252C%2520we%2520provide%2520a%2520set%2520of%2520findings%2520and%2520implications%2520for%2520the%2520LLM-driven%2520AI%2520agent%2520framework%2520ecosystem%2520and%2520offer%2520insights%2520for%2520the%2520design%2520of%2520future%2520LLM-based%2520agent%2520frameworks%2520and%2520agent%2520developers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Study%20of%20Agent%20Developer%20Practices%20in%20AI%20Agent%20Frameworks&entry.906535625=Yanlin%20Wang%20and%20Xinyi%20Xu%20and%20Jiachi%20Chen%20and%20Tingting%20Bi%20and%20Wenchao%20Gu%20and%20Zibin%20Zheng&entry.1292438233=The%20rise%20of%20large%20language%20models%20%28LLMs%29%20has%20sparked%20a%20surge%20of%20interest%20in%20agents%2C%20leading%20to%20the%20rapid%20growth%20of%20agent%20frameworks.%20Agent%20frameworks%20are%20software%20toolkits%20and%20libraries%20that%20provide%20standardized%20components%2C%20abstractions%2C%20and%20orchestration%20mechanisms%20to%20simplify%20agent%20development.%20Despite%20widespread%20use%20of%20agent%20frameworks%2C%20their%20practical%20applications%20and%20how%20they%20influence%20the%20agent%20development%20process%20remain%20underexplored.%20Different%20agent%20frameworks%20encounter%20similar%20problems%20during%20use%2C%20indicating%20that%20these%20recurring%20issues%20deserve%20greater%20attention%20and%20call%20for%20further%20improvements%20in%20agent%20framework%20design.%20Meanwhile%2C%20as%20the%20number%20of%20agent%20frameworks%20continues%20to%20grow%20and%20evolve%2C%20more%20than%2080%25%20of%20developers%20report%20difficulties%20in%20identifying%20the%20frameworks%20that%20best%20meet%20their%20specific%20development%20requirements.%20In%20this%20paper%2C%20we%20conduct%20the%20first%20empirical%20study%20of%20LLM-based%20agent%20frameworks%2C%20exploring%20real-world%20experiences%20of%20developers%20in%20building%20AI%20agents.%20To%20compare%20how%20well%20the%20agent%20frameworks%20meet%20developer%20needs%2C%20we%20further%20collect%20developer%20discussions%20for%20the%20ten%20previously%20identified%20agent%20frameworks%2C%20resulting%20in%20a%20total%20of%2011%2C910%20discussions.%20Finally%2C%20by%20analyzing%20these%20discussions%2C%20we%20compare%20the%20frameworks%20across%20five%20dimensions%3A%20development%20efficiency%2C%20functional%20abstraction%2C%20learning%20cost%2C%20performance%20optimization%2C%20and%20maintainability%2C%20which%20refers%20to%20how%20easily%20developers%20can%20update%20and%20extend%20both%20the%20framework%20itself%20and%20the%20agents%20built%20upon%20it%20over%20time.%20Our%20comparative%20analysis%20reveals%20significant%20differences%20among%20frameworks%20in%20how%20they%20meet%20the%20needs%20of%20agent%20developers.%20Overall%2C%20we%20provide%20a%20set%20of%20findings%20and%20implications%20for%20the%20LLM-driven%20AI%20agent%20framework%20ecosystem%20and%20offer%20insights%20for%20the%20design%20of%20future%20LLM-based%20agent%20frameworks%20and%20agent%20developers.&entry.1838667208=http%3A//arxiv.org/abs/2512.01939v1&entry.124074799=Read"},
{"title": "DeepCAVE: A Visualization and Analysis Tool for Automated Machine Learning", "author": "Sarah Segel and Helena Graf and Edward Bergman and Kristina Thieme and Marcel Wever and Alexander Tornede and Frank Hutter and Marius Lindauer", "abstract": "Hyperparameter optimization (HPO), as a central paradigm of AutoML, is crucial for leveraging the full potential of machine learning (ML) models; yet its complexity poses challenges in understanding and debugging the optimization process. We present DeepCAVE, a tool for interactive visualization and analysis, providing insights into HPO. Through an interactive dashboard, researchers, data scientists, and ML engineers can explore various aspects of the HPO process and identify issues, untouched potentials, and new insights about the ML model being tuned. By empowering users with actionable insights, DeepCAVE contributes to the interpretability of HPO and ML on a design level and aims to foster the development of more robust and efficient methodologies in the future.", "link": "http://arxiv.org/abs/2512.01810v1", "date": "2025-12-01", "relevancy": 2.2875, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4585}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepCAVE%3A%20A%20Visualization%20and%20Analysis%20Tool%20for%20Automated%20Machine%20Learning&body=Title%3A%20DeepCAVE%3A%20A%20Visualization%20and%20Analysis%20Tool%20for%20Automated%20Machine%20Learning%0AAuthor%3A%20Sarah%20Segel%20and%20Helena%20Graf%20and%20Edward%20Bergman%20and%20Kristina%20Thieme%20and%20Marcel%20Wever%20and%20Alexander%20Tornede%20and%20Frank%20Hutter%20and%20Marius%20Lindauer%0AAbstract%3A%20Hyperparameter%20optimization%20%28HPO%29%2C%20as%20a%20central%20paradigm%20of%20AutoML%2C%20is%20crucial%20for%20leveraging%20the%20full%20potential%20of%20machine%20learning%20%28ML%29%20models%3B%20yet%20its%20complexity%20poses%20challenges%20in%20understanding%20and%20debugging%20the%20optimization%20process.%20We%20present%20DeepCAVE%2C%20a%20tool%20for%20interactive%20visualization%20and%20analysis%2C%20providing%20insights%20into%20HPO.%20Through%20an%20interactive%20dashboard%2C%20researchers%2C%20data%20scientists%2C%20and%20ML%20engineers%20can%20explore%20various%20aspects%20of%20the%20HPO%20process%20and%20identify%20issues%2C%20untouched%20potentials%2C%20and%20new%20insights%20about%20the%20ML%20model%20being%20tuned.%20By%20empowering%20users%20with%20actionable%20insights%2C%20DeepCAVE%20contributes%20to%20the%20interpretability%20of%20HPO%20and%20ML%20on%20a%20design%20level%20and%20aims%20to%20foster%20the%20development%20of%20more%20robust%20and%20efficient%20methodologies%20in%20the%20future.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepCAVE%253A%2520A%2520Visualization%2520and%2520Analysis%2520Tool%2520for%2520Automated%2520Machine%2520Learning%26entry.906535625%3DSarah%2520Segel%2520and%2520Helena%2520Graf%2520and%2520Edward%2520Bergman%2520and%2520Kristina%2520Thieme%2520and%2520Marcel%2520Wever%2520and%2520Alexander%2520Tornede%2520and%2520Frank%2520Hutter%2520and%2520Marius%2520Lindauer%26entry.1292438233%3DHyperparameter%2520optimization%2520%2528HPO%2529%252C%2520as%2520a%2520central%2520paradigm%2520of%2520AutoML%252C%2520is%2520crucial%2520for%2520leveraging%2520the%2520full%2520potential%2520of%2520machine%2520learning%2520%2528ML%2529%2520models%253B%2520yet%2520its%2520complexity%2520poses%2520challenges%2520in%2520understanding%2520and%2520debugging%2520the%2520optimization%2520process.%2520We%2520present%2520DeepCAVE%252C%2520a%2520tool%2520for%2520interactive%2520visualization%2520and%2520analysis%252C%2520providing%2520insights%2520into%2520HPO.%2520Through%2520an%2520interactive%2520dashboard%252C%2520researchers%252C%2520data%2520scientists%252C%2520and%2520ML%2520engineers%2520can%2520explore%2520various%2520aspects%2520of%2520the%2520HPO%2520process%2520and%2520identify%2520issues%252C%2520untouched%2520potentials%252C%2520and%2520new%2520insights%2520about%2520the%2520ML%2520model%2520being%2520tuned.%2520By%2520empowering%2520users%2520with%2520actionable%2520insights%252C%2520DeepCAVE%2520contributes%2520to%2520the%2520interpretability%2520of%2520HPO%2520and%2520ML%2520on%2520a%2520design%2520level%2520and%2520aims%2520to%2520foster%2520the%2520development%2520of%2520more%2520robust%2520and%2520efficient%2520methodologies%2520in%2520the%2520future.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepCAVE%3A%20A%20Visualization%20and%20Analysis%20Tool%20for%20Automated%20Machine%20Learning&entry.906535625=Sarah%20Segel%20and%20Helena%20Graf%20and%20Edward%20Bergman%20and%20Kristina%20Thieme%20and%20Marcel%20Wever%20and%20Alexander%20Tornede%20and%20Frank%20Hutter%20and%20Marius%20Lindauer&entry.1292438233=Hyperparameter%20optimization%20%28HPO%29%2C%20as%20a%20central%20paradigm%20of%20AutoML%2C%20is%20crucial%20for%20leveraging%20the%20full%20potential%20of%20machine%20learning%20%28ML%29%20models%3B%20yet%20its%20complexity%20poses%20challenges%20in%20understanding%20and%20debugging%20the%20optimization%20process.%20We%20present%20DeepCAVE%2C%20a%20tool%20for%20interactive%20visualization%20and%20analysis%2C%20providing%20insights%20into%20HPO.%20Through%20an%20interactive%20dashboard%2C%20researchers%2C%20data%20scientists%2C%20and%20ML%20engineers%20can%20explore%20various%20aspects%20of%20the%20HPO%20process%20and%20identify%20issues%2C%20untouched%20potentials%2C%20and%20new%20insights%20about%20the%20ML%20model%20being%20tuned.%20By%20empowering%20users%20with%20actionable%20insights%2C%20DeepCAVE%20contributes%20to%20the%20interpretability%20of%20HPO%20and%20ML%20on%20a%20design%20level%20and%20aims%20to%20foster%20the%20development%20of%20more%20robust%20and%20efficient%20methodologies%20in%20the%20future.&entry.1838667208=http%3A//arxiv.org/abs/2512.01810v1&entry.124074799=Read"},
{"title": "High-Speed Event Vision-Based Tactile Roller Sensor for Large Surface Measurements", "author": "Akram Khairi and Hussain Sajwani and Abdallah Mohammad Alkilany and Laith AbuAssi and Mohamad Halwani and Islam Mohamed Zaid and Ahmed Awadalla and Dewald Swart and Abdulla Ayyad and Yahya Zweiri", "abstract": "Inspecting large-scale industrial surfaces like aircraft fuselages for quality control requires capturing their precise 3D surface geometry at high resolution. Vision-based tactile sensors (VBTSs) offer high local resolution but require slow 'press-and-lift' measurements stitched for large areas. Approaches with sliding or roller/belt VBTS designs provide measurements continuity. However, they face significant challenges respectively: sliding struggles with friction/wear and both approaches are speed-limited by conventional camera frame rates and motion blur, making large-area scanning time consuming. Thus, a rapid, continuous, high-resolution method is needed. We introduce a novel tactile sensor integrating a neuromorphic camera in a rolling mechanism to achieve this. Leveraging its high temporal resolution and robustness to motion blur, our system uses a modified event-based multi-view stereo approach for 3D reconstruction. We demonstrate state-of-the-art scanning speeds up to 0.5 m/s, achieving Mean Absolute Error below 100 microns -- 11 times faster than prior continuous tactile sensing methods. A multi-reference Bayesian fusion strategy enhances accuracy (reducing MAE by 25.2\\% compared to EMVS) and mitigates curvature errors. We also validate high-speed feature recognition via Braille reading 2.6 times faster than previous approaches.", "link": "http://arxiv.org/abs/2507.19914v2", "date": "2025-12-01", "relevancy": 2.2831, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5959}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5613}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Speed%20Event%20Vision-Based%20Tactile%20Roller%20Sensor%20for%20Large%20Surface%20Measurements&body=Title%3A%20High-Speed%20Event%20Vision-Based%20Tactile%20Roller%20Sensor%20for%20Large%20Surface%20Measurements%0AAuthor%3A%20Akram%20Khairi%20and%20Hussain%20Sajwani%20and%20Abdallah%20Mohammad%20Alkilany%20and%20Laith%20AbuAssi%20and%20Mohamad%20Halwani%20and%20Islam%20Mohamed%20Zaid%20and%20Ahmed%20Awadalla%20and%20Dewald%20Swart%20and%20Abdulla%20Ayyad%20and%20Yahya%20Zweiri%0AAbstract%3A%20Inspecting%20large-scale%20industrial%20surfaces%20like%20aircraft%20fuselages%20for%20quality%20control%20requires%20capturing%20their%20precise%203D%20surface%20geometry%20at%20high%20resolution.%20Vision-based%20tactile%20sensors%20%28VBTSs%29%20offer%20high%20local%20resolution%20but%20require%20slow%20%27press-and-lift%27%20measurements%20stitched%20for%20large%20areas.%20Approaches%20with%20sliding%20or%20roller/belt%20VBTS%20designs%20provide%20measurements%20continuity.%20However%2C%20they%20face%20significant%20challenges%20respectively%3A%20sliding%20struggles%20with%20friction/wear%20and%20both%20approaches%20are%20speed-limited%20by%20conventional%20camera%20frame%20rates%20and%20motion%20blur%2C%20making%20large-area%20scanning%20time%20consuming.%20Thus%2C%20a%20rapid%2C%20continuous%2C%20high-resolution%20method%20is%20needed.%20We%20introduce%20a%20novel%20tactile%20sensor%20integrating%20a%20neuromorphic%20camera%20in%20a%20rolling%20mechanism%20to%20achieve%20this.%20Leveraging%20its%20high%20temporal%20resolution%20and%20robustness%20to%20motion%20blur%2C%20our%20system%20uses%20a%20modified%20event-based%20multi-view%20stereo%20approach%20for%203D%20reconstruction.%20We%20demonstrate%20state-of-the-art%20scanning%20speeds%20up%20to%200.5%20m/s%2C%20achieving%20Mean%20Absolute%20Error%20below%20100%20microns%20--%2011%20times%20faster%20than%20prior%20continuous%20tactile%20sensing%20methods.%20A%20multi-reference%20Bayesian%20fusion%20strategy%20enhances%20accuracy%20%28reducing%20MAE%20by%2025.2%5C%25%20compared%20to%20EMVS%29%20and%20mitigates%20curvature%20errors.%20We%20also%20validate%20high-speed%20feature%20recognition%20via%20Braille%20reading%202.6%20times%20faster%20than%20previous%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2507.19914v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Speed%2520Event%2520Vision-Based%2520Tactile%2520Roller%2520Sensor%2520for%2520Large%2520Surface%2520Measurements%26entry.906535625%3DAkram%2520Khairi%2520and%2520Hussain%2520Sajwani%2520and%2520Abdallah%2520Mohammad%2520Alkilany%2520and%2520Laith%2520AbuAssi%2520and%2520Mohamad%2520Halwani%2520and%2520Islam%2520Mohamed%2520Zaid%2520and%2520Ahmed%2520Awadalla%2520and%2520Dewald%2520Swart%2520and%2520Abdulla%2520Ayyad%2520and%2520Yahya%2520Zweiri%26entry.1292438233%3DInspecting%2520large-scale%2520industrial%2520surfaces%2520like%2520aircraft%2520fuselages%2520for%2520quality%2520control%2520requires%2520capturing%2520their%2520precise%25203D%2520surface%2520geometry%2520at%2520high%2520resolution.%2520Vision-based%2520tactile%2520sensors%2520%2528VBTSs%2529%2520offer%2520high%2520local%2520resolution%2520but%2520require%2520slow%2520%2527press-and-lift%2527%2520measurements%2520stitched%2520for%2520large%2520areas.%2520Approaches%2520with%2520sliding%2520or%2520roller/belt%2520VBTS%2520designs%2520provide%2520measurements%2520continuity.%2520However%252C%2520they%2520face%2520significant%2520challenges%2520respectively%253A%2520sliding%2520struggles%2520with%2520friction/wear%2520and%2520both%2520approaches%2520are%2520speed-limited%2520by%2520conventional%2520camera%2520frame%2520rates%2520and%2520motion%2520blur%252C%2520making%2520large-area%2520scanning%2520time%2520consuming.%2520Thus%252C%2520a%2520rapid%252C%2520continuous%252C%2520high-resolution%2520method%2520is%2520needed.%2520We%2520introduce%2520a%2520novel%2520tactile%2520sensor%2520integrating%2520a%2520neuromorphic%2520camera%2520in%2520a%2520rolling%2520mechanism%2520to%2520achieve%2520this.%2520Leveraging%2520its%2520high%2520temporal%2520resolution%2520and%2520robustness%2520to%2520motion%2520blur%252C%2520our%2520system%2520uses%2520a%2520modified%2520event-based%2520multi-view%2520stereo%2520approach%2520for%25203D%2520reconstruction.%2520We%2520demonstrate%2520state-of-the-art%2520scanning%2520speeds%2520up%2520to%25200.5%2520m/s%252C%2520achieving%2520Mean%2520Absolute%2520Error%2520below%2520100%2520microns%2520--%252011%2520times%2520faster%2520than%2520prior%2520continuous%2520tactile%2520sensing%2520methods.%2520A%2520multi-reference%2520Bayesian%2520fusion%2520strategy%2520enhances%2520accuracy%2520%2528reducing%2520MAE%2520by%252025.2%255C%2525%2520compared%2520to%2520EMVS%2529%2520and%2520mitigates%2520curvature%2520errors.%2520We%2520also%2520validate%2520high-speed%2520feature%2520recognition%2520via%2520Braille%2520reading%25202.6%2520times%2520faster%2520than%2520previous%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19914v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Speed%20Event%20Vision-Based%20Tactile%20Roller%20Sensor%20for%20Large%20Surface%20Measurements&entry.906535625=Akram%20Khairi%20and%20Hussain%20Sajwani%20and%20Abdallah%20Mohammad%20Alkilany%20and%20Laith%20AbuAssi%20and%20Mohamad%20Halwani%20and%20Islam%20Mohamed%20Zaid%20and%20Ahmed%20Awadalla%20and%20Dewald%20Swart%20and%20Abdulla%20Ayyad%20and%20Yahya%20Zweiri&entry.1292438233=Inspecting%20large-scale%20industrial%20surfaces%20like%20aircraft%20fuselages%20for%20quality%20control%20requires%20capturing%20their%20precise%203D%20surface%20geometry%20at%20high%20resolution.%20Vision-based%20tactile%20sensors%20%28VBTSs%29%20offer%20high%20local%20resolution%20but%20require%20slow%20%27press-and-lift%27%20measurements%20stitched%20for%20large%20areas.%20Approaches%20with%20sliding%20or%20roller/belt%20VBTS%20designs%20provide%20measurements%20continuity.%20However%2C%20they%20face%20significant%20challenges%20respectively%3A%20sliding%20struggles%20with%20friction/wear%20and%20both%20approaches%20are%20speed-limited%20by%20conventional%20camera%20frame%20rates%20and%20motion%20blur%2C%20making%20large-area%20scanning%20time%20consuming.%20Thus%2C%20a%20rapid%2C%20continuous%2C%20high-resolution%20method%20is%20needed.%20We%20introduce%20a%20novel%20tactile%20sensor%20integrating%20a%20neuromorphic%20camera%20in%20a%20rolling%20mechanism%20to%20achieve%20this.%20Leveraging%20its%20high%20temporal%20resolution%20and%20robustness%20to%20motion%20blur%2C%20our%20system%20uses%20a%20modified%20event-based%20multi-view%20stereo%20approach%20for%203D%20reconstruction.%20We%20demonstrate%20state-of-the-art%20scanning%20speeds%20up%20to%200.5%20m/s%2C%20achieving%20Mean%20Absolute%20Error%20below%20100%20microns%20--%2011%20times%20faster%20than%20prior%20continuous%20tactile%20sensing%20methods.%20A%20multi-reference%20Bayesian%20fusion%20strategy%20enhances%20accuracy%20%28reducing%20MAE%20by%2025.2%5C%25%20compared%20to%20EMVS%29%20and%20mitigates%20curvature%20errors.%20We%20also%20validate%20high-speed%20feature%20recognition%20via%20Braille%20reading%202.6%20times%20faster%20than%20previous%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2507.19914v2&entry.124074799=Read"},
{"title": "Influence Functions for Efficient Data Selection in Reasoning", "author": "Prateek Humane and Paolo Cudrano and Daniel Z. Kaplan and Matteo Matteucci and Supriyo Chakraborty and Irina Rish", "abstract": "Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows that a small amount of high-quality data can outperform massive datasets. Yet, what constitutes \"quality\" remains ill-defined. Existing reasoning methods rely on indirect heuristics such as problem difficulty or trace length, while instruction-tuning has explored a broader range of automated selection strategies, but rarely in the context of reasoning. We propose to define reasoning data quality using influence functions, which measure the causal effect of individual CoT examples on downstream accuracy, and introduce influence-based pruning, which consistently outperforms perplexity and embedding-based baselines on math reasoning within a model family.", "link": "http://arxiv.org/abs/2510.06108v2", "date": "2025-12-01", "relevancy": 2.2779, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4574}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Influence%20Functions%20for%20Efficient%20Data%20Selection%20in%20Reasoning&body=Title%3A%20Influence%20Functions%20for%20Efficient%20Data%20Selection%20in%20Reasoning%0AAuthor%3A%20Prateek%20Humane%20and%20Paolo%20Cudrano%20and%20Daniel%20Z.%20Kaplan%20and%20Matteo%20Matteucci%20and%20Supriyo%20Chakraborty%20and%20Irina%20Rish%0AAbstract%3A%20Fine-tuning%20large%20language%20models%20%28LLMs%29%20on%20chain-of-thought%20%28CoT%29%20data%20shows%20that%20a%20small%20amount%20of%20high-quality%20data%20can%20outperform%20massive%20datasets.%20Yet%2C%20what%20constitutes%20%22quality%22%20remains%20ill-defined.%20Existing%20reasoning%20methods%20rely%20on%20indirect%20heuristics%20such%20as%20problem%20difficulty%20or%20trace%20length%2C%20while%20instruction-tuning%20has%20explored%20a%20broader%20range%20of%20automated%20selection%20strategies%2C%20but%20rarely%20in%20the%20context%20of%20reasoning.%20We%20propose%20to%20define%20reasoning%20data%20quality%20using%20influence%20functions%2C%20which%20measure%20the%20causal%20effect%20of%20individual%20CoT%20examples%20on%20downstream%20accuracy%2C%20and%20introduce%20influence-based%20pruning%2C%20which%20consistently%20outperforms%20perplexity%20and%20embedding-based%20baselines%20on%20math%20reasoning%20within%20a%20model%20family.%0ALink%3A%20http%3A//arxiv.org/abs/2510.06108v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfluence%2520Functions%2520for%2520Efficient%2520Data%2520Selection%2520in%2520Reasoning%26entry.906535625%3DPrateek%2520Humane%2520and%2520Paolo%2520Cudrano%2520and%2520Daniel%2520Z.%2520Kaplan%2520and%2520Matteo%2520Matteucci%2520and%2520Supriyo%2520Chakraborty%2520and%2520Irina%2520Rish%26entry.1292438233%3DFine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520chain-of-thought%2520%2528CoT%2529%2520data%2520shows%2520that%2520a%2520small%2520amount%2520of%2520high-quality%2520data%2520can%2520outperform%2520massive%2520datasets.%2520Yet%252C%2520what%2520constitutes%2520%2522quality%2522%2520remains%2520ill-defined.%2520Existing%2520reasoning%2520methods%2520rely%2520on%2520indirect%2520heuristics%2520such%2520as%2520problem%2520difficulty%2520or%2520trace%2520length%252C%2520while%2520instruction-tuning%2520has%2520explored%2520a%2520broader%2520range%2520of%2520automated%2520selection%2520strategies%252C%2520but%2520rarely%2520in%2520the%2520context%2520of%2520reasoning.%2520We%2520propose%2520to%2520define%2520reasoning%2520data%2520quality%2520using%2520influence%2520functions%252C%2520which%2520measure%2520the%2520causal%2520effect%2520of%2520individual%2520CoT%2520examples%2520on%2520downstream%2520accuracy%252C%2520and%2520introduce%2520influence-based%2520pruning%252C%2520which%2520consistently%2520outperforms%2520perplexity%2520and%2520embedding-based%2520baselines%2520on%2520math%2520reasoning%2520within%2520a%2520model%2520family.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06108v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Influence%20Functions%20for%20Efficient%20Data%20Selection%20in%20Reasoning&entry.906535625=Prateek%20Humane%20and%20Paolo%20Cudrano%20and%20Daniel%20Z.%20Kaplan%20and%20Matteo%20Matteucci%20and%20Supriyo%20Chakraborty%20and%20Irina%20Rish&entry.1292438233=Fine-tuning%20large%20language%20models%20%28LLMs%29%20on%20chain-of-thought%20%28CoT%29%20data%20shows%20that%20a%20small%20amount%20of%20high-quality%20data%20can%20outperform%20massive%20datasets.%20Yet%2C%20what%20constitutes%20%22quality%22%20remains%20ill-defined.%20Existing%20reasoning%20methods%20rely%20on%20indirect%20heuristics%20such%20as%20problem%20difficulty%20or%20trace%20length%2C%20while%20instruction-tuning%20has%20explored%20a%20broader%20range%20of%20automated%20selection%20strategies%2C%20but%20rarely%20in%20the%20context%20of%20reasoning.%20We%20propose%20to%20define%20reasoning%20data%20quality%20using%20influence%20functions%2C%20which%20measure%20the%20causal%20effect%20of%20individual%20CoT%20examples%20on%20downstream%20accuracy%2C%20and%20introduce%20influence-based%20pruning%2C%20which%20consistently%20outperforms%20perplexity%20and%20embedding-based%20baselines%20on%20math%20reasoning%20within%20a%20model%20family.&entry.1838667208=http%3A//arxiv.org/abs/2510.06108v2&entry.124074799=Read"},
{"title": "Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation", "author": "Thao Thi Phuong Dao and Tan-Cong Nguyen and Trong-Le Do and Truong Hoang Viet and Nguyen Chi Thanh and Huynh Nguyen Thuan and Do Vo Cong Nguyen and Minh-Khoi Pham and Mai-Khiem Tran and Viet-Tham Huynh and Trong-Thuan Nguyen and Trung-Nghia Le and Vo Thanh Toan and Tam V. Nguyen and Minh-Triet Tran and Thanh Dinh Le", "abstract": "Abscesses in the head and neck represent an acute infectious process that can potentially lead to sepsis or mortality if not diagnosed and managed promptly. Accurate detection and delineation of these lesions on imaging are essential for diagnosis, treatment planning, and surgical intervention. In this study, we introduce AbscessHeNe, a curated and comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices with clinically confirmed head and neck abscesses. The dataset is designed to facilitate the development of robust semantic segmentation models that can accurately delineate abscess boundaries and evaluate deep neck space involvement, thereby supporting informed clinical decision-making. To establish performance baselines, we evaluate several state-of-the-art segmentation architectures, including CNN, Transformer, and Mamba-based models. The highest-performing model achieved a Dice Similarity Coefficient of 0.39, Intersection-over-Union of 0.27, and Normalized Surface Distance of 0.67, indicating the challenges of this task and the need for further research. Beyond segmentation, AbscessHeNe is structured for future applications in content-based multimedia indexing and case-based retrieval. Each CT scan is linked with pixel-level annotations and clinical metadata, providing a foundation for building intelligent retrieval systems and supporting knowledge-driven clinical workflows. The dataset will be made publicly available at https://github.com/drthaodao3101/AbscessHeNe.git.", "link": "http://arxiv.org/abs/2512.01589v1", "date": "2025-12-01", "relevancy": 2.2761, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4575}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Content-based%20Indexing%20and%20Retrieval%20of%20Head%20and%20Neck%20CT%20with%20Abscess%20Segmentation&body=Title%3A%20Toward%20Content-based%20Indexing%20and%20Retrieval%20of%20Head%20and%20Neck%20CT%20with%20Abscess%20Segmentation%0AAuthor%3A%20Thao%20Thi%20Phuong%20Dao%20and%20Tan-Cong%20Nguyen%20and%20Trong-Le%20Do%20and%20Truong%20Hoang%20Viet%20and%20Nguyen%20Chi%20Thanh%20and%20Huynh%20Nguyen%20Thuan%20and%20Do%20Vo%20Cong%20Nguyen%20and%20Minh-Khoi%20Pham%20and%20Mai-Khiem%20Tran%20and%20Viet-Tham%20Huynh%20and%20Trong-Thuan%20Nguyen%20and%20Trung-Nghia%20Le%20and%20Vo%20Thanh%20Toan%20and%20Tam%20V.%20Nguyen%20and%20Minh-Triet%20Tran%20and%20Thanh%20Dinh%20Le%0AAbstract%3A%20Abscesses%20in%20the%20head%20and%20neck%20represent%20an%20acute%20infectious%20process%20that%20can%20potentially%20lead%20to%20sepsis%20or%20mortality%20if%20not%20diagnosed%20and%20managed%20promptly.%20Accurate%20detection%20and%20delineation%20of%20these%20lesions%20on%20imaging%20are%20essential%20for%20diagnosis%2C%20treatment%20planning%2C%20and%20surgical%20intervention.%20In%20this%20study%2C%20we%20introduce%20AbscessHeNe%2C%20a%20curated%20and%20comprehensively%20annotated%20dataset%20comprising%204%2C926%20contrast-enhanced%20CT%20slices%20with%20clinically%20confirmed%20head%20and%20neck%20abscesses.%20The%20dataset%20is%20designed%20to%20facilitate%20the%20development%20of%20robust%20semantic%20segmentation%20models%20that%20can%20accurately%20delineate%20abscess%20boundaries%20and%20evaluate%20deep%20neck%20space%20involvement%2C%20thereby%20supporting%20informed%20clinical%20decision-making.%20To%20establish%20performance%20baselines%2C%20we%20evaluate%20several%20state-of-the-art%20segmentation%20architectures%2C%20including%20CNN%2C%20Transformer%2C%20and%20Mamba-based%20models.%20The%20highest-performing%20model%20achieved%20a%20Dice%20Similarity%20Coefficient%20of%200.39%2C%20Intersection-over-Union%20of%200.27%2C%20and%20Normalized%20Surface%20Distance%20of%200.67%2C%20indicating%20the%20challenges%20of%20this%20task%20and%20the%20need%20for%20further%20research.%20Beyond%20segmentation%2C%20AbscessHeNe%20is%20structured%20for%20future%20applications%20in%20content-based%20multimedia%20indexing%20and%20case-based%20retrieval.%20Each%20CT%20scan%20is%20linked%20with%20pixel-level%20annotations%20and%20clinical%20metadata%2C%20providing%20a%20foundation%20for%20building%20intelligent%20retrieval%20systems%20and%20supporting%20knowledge-driven%20clinical%20workflows.%20The%20dataset%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/drthaodao3101/AbscessHeNe.git.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Content-based%2520Indexing%2520and%2520Retrieval%2520of%2520Head%2520and%2520Neck%2520CT%2520with%2520Abscess%2520Segmentation%26entry.906535625%3DThao%2520Thi%2520Phuong%2520Dao%2520and%2520Tan-Cong%2520Nguyen%2520and%2520Trong-Le%2520Do%2520and%2520Truong%2520Hoang%2520Viet%2520and%2520Nguyen%2520Chi%2520Thanh%2520and%2520Huynh%2520Nguyen%2520Thuan%2520and%2520Do%2520Vo%2520Cong%2520Nguyen%2520and%2520Minh-Khoi%2520Pham%2520and%2520Mai-Khiem%2520Tran%2520and%2520Viet-Tham%2520Huynh%2520and%2520Trong-Thuan%2520Nguyen%2520and%2520Trung-Nghia%2520Le%2520and%2520Vo%2520Thanh%2520Toan%2520and%2520Tam%2520V.%2520Nguyen%2520and%2520Minh-Triet%2520Tran%2520and%2520Thanh%2520Dinh%2520Le%26entry.1292438233%3DAbscesses%2520in%2520the%2520head%2520and%2520neck%2520represent%2520an%2520acute%2520infectious%2520process%2520that%2520can%2520potentially%2520lead%2520to%2520sepsis%2520or%2520mortality%2520if%2520not%2520diagnosed%2520and%2520managed%2520promptly.%2520Accurate%2520detection%2520and%2520delineation%2520of%2520these%2520lesions%2520on%2520imaging%2520are%2520essential%2520for%2520diagnosis%252C%2520treatment%2520planning%252C%2520and%2520surgical%2520intervention.%2520In%2520this%2520study%252C%2520we%2520introduce%2520AbscessHeNe%252C%2520a%2520curated%2520and%2520comprehensively%2520annotated%2520dataset%2520comprising%25204%252C926%2520contrast-enhanced%2520CT%2520slices%2520with%2520clinically%2520confirmed%2520head%2520and%2520neck%2520abscesses.%2520The%2520dataset%2520is%2520designed%2520to%2520facilitate%2520the%2520development%2520of%2520robust%2520semantic%2520segmentation%2520models%2520that%2520can%2520accurately%2520delineate%2520abscess%2520boundaries%2520and%2520evaluate%2520deep%2520neck%2520space%2520involvement%252C%2520thereby%2520supporting%2520informed%2520clinical%2520decision-making.%2520To%2520establish%2520performance%2520baselines%252C%2520we%2520evaluate%2520several%2520state-of-the-art%2520segmentation%2520architectures%252C%2520including%2520CNN%252C%2520Transformer%252C%2520and%2520Mamba-based%2520models.%2520The%2520highest-performing%2520model%2520achieved%2520a%2520Dice%2520Similarity%2520Coefficient%2520of%25200.39%252C%2520Intersection-over-Union%2520of%25200.27%252C%2520and%2520Normalized%2520Surface%2520Distance%2520of%25200.67%252C%2520indicating%2520the%2520challenges%2520of%2520this%2520task%2520and%2520the%2520need%2520for%2520further%2520research.%2520Beyond%2520segmentation%252C%2520AbscessHeNe%2520is%2520structured%2520for%2520future%2520applications%2520in%2520content-based%2520multimedia%2520indexing%2520and%2520case-based%2520retrieval.%2520Each%2520CT%2520scan%2520is%2520linked%2520with%2520pixel-level%2520annotations%2520and%2520clinical%2520metadata%252C%2520providing%2520a%2520foundation%2520for%2520building%2520intelligent%2520retrieval%2520systems%2520and%2520supporting%2520knowledge-driven%2520clinical%2520workflows.%2520The%2520dataset%2520will%2520be%2520made%2520publicly%2520available%2520at%2520https%253A//github.com/drthaodao3101/AbscessHeNe.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Content-based%20Indexing%20and%20Retrieval%20of%20Head%20and%20Neck%20CT%20with%20Abscess%20Segmentation&entry.906535625=Thao%20Thi%20Phuong%20Dao%20and%20Tan-Cong%20Nguyen%20and%20Trong-Le%20Do%20and%20Truong%20Hoang%20Viet%20and%20Nguyen%20Chi%20Thanh%20and%20Huynh%20Nguyen%20Thuan%20and%20Do%20Vo%20Cong%20Nguyen%20and%20Minh-Khoi%20Pham%20and%20Mai-Khiem%20Tran%20and%20Viet-Tham%20Huynh%20and%20Trong-Thuan%20Nguyen%20and%20Trung-Nghia%20Le%20and%20Vo%20Thanh%20Toan%20and%20Tam%20V.%20Nguyen%20and%20Minh-Triet%20Tran%20and%20Thanh%20Dinh%20Le&entry.1292438233=Abscesses%20in%20the%20head%20and%20neck%20represent%20an%20acute%20infectious%20process%20that%20can%20potentially%20lead%20to%20sepsis%20or%20mortality%20if%20not%20diagnosed%20and%20managed%20promptly.%20Accurate%20detection%20and%20delineation%20of%20these%20lesions%20on%20imaging%20are%20essential%20for%20diagnosis%2C%20treatment%20planning%2C%20and%20surgical%20intervention.%20In%20this%20study%2C%20we%20introduce%20AbscessHeNe%2C%20a%20curated%20and%20comprehensively%20annotated%20dataset%20comprising%204%2C926%20contrast-enhanced%20CT%20slices%20with%20clinically%20confirmed%20head%20and%20neck%20abscesses.%20The%20dataset%20is%20designed%20to%20facilitate%20the%20development%20of%20robust%20semantic%20segmentation%20models%20that%20can%20accurately%20delineate%20abscess%20boundaries%20and%20evaluate%20deep%20neck%20space%20involvement%2C%20thereby%20supporting%20informed%20clinical%20decision-making.%20To%20establish%20performance%20baselines%2C%20we%20evaluate%20several%20state-of-the-art%20segmentation%20architectures%2C%20including%20CNN%2C%20Transformer%2C%20and%20Mamba-based%20models.%20The%20highest-performing%20model%20achieved%20a%20Dice%20Similarity%20Coefficient%20of%200.39%2C%20Intersection-over-Union%20of%200.27%2C%20and%20Normalized%20Surface%20Distance%20of%200.67%2C%20indicating%20the%20challenges%20of%20this%20task%20and%20the%20need%20for%20further%20research.%20Beyond%20segmentation%2C%20AbscessHeNe%20is%20structured%20for%20future%20applications%20in%20content-based%20multimedia%20indexing%20and%20case-based%20retrieval.%20Each%20CT%20scan%20is%20linked%20with%20pixel-level%20annotations%20and%20clinical%20metadata%2C%20providing%20a%20foundation%20for%20building%20intelligent%20retrieval%20systems%20and%20supporting%20knowledge-driven%20clinical%20workflows.%20The%20dataset%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/drthaodao3101/AbscessHeNe.git.&entry.1838667208=http%3A//arxiv.org/abs/2512.01589v1&entry.124074799=Read"},
{"title": "SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception", "author": "Gurmeher Khurana and Lan Wei and Dandan Zhang", "abstract": "Contact-rich robotic manipulation requires representations that encode local geometry. Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues. Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information. Most self-supervised learning (SSL) frameworks, however, compress feature maps into a global vector, discarding spatial structure and misaligning with the needs of manipulation. To address this, we propose SARL, a spatially-aware SSL framework that augments the Bootstrap Your Own Latent (BYOL) architecture with three map-level objectives, including Saliency Alignment (SAL), Patch-Prototype Distribution Alignment (PPDA), and Region Affinity Matching (RAM), to keep attentional focus, part composition, and geometric relations consistent across views. These losses act on intermediate feature maps, complementing the global objective. SARL consistently outperforms nine SSL baselines across six downstream tasks with fused visual-tactile data. On the geometry-sensitive edge-pose regression task, SARL achieves a Mean Absolute Error (MAE) of 0.3955, a 30% relative improvement over the next-best SSL method (0.5682 MAE) and approaching the supervised upper bound. These findings indicate that, for fused visual-tactile data, the most effective signal is structured spatial equivariance, in which features vary predictably with object geometry, which enables more capable robotic perception.", "link": "http://arxiv.org/abs/2512.01908v1", "date": "2025-12-01", "relevancy": 2.2733, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5696}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5676}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SARL%3A%20Spatially-Aware%20Self-Supervised%20Representation%20Learning%20for%20Visuo-Tactile%20Perception&body=Title%3A%20SARL%3A%20Spatially-Aware%20Self-Supervised%20Representation%20Learning%20for%20Visuo-Tactile%20Perception%0AAuthor%3A%20Gurmeher%20Khurana%20and%20Lan%20Wei%20and%20Dandan%20Zhang%0AAbstract%3A%20Contact-rich%20robotic%20manipulation%20requires%20representations%20that%20encode%20local%20geometry.%20Vision%20provides%20global%20context%20but%20lacks%20direct%20measurements%20of%20properties%20such%20as%20texture%20and%20hardness%2C%20whereas%20touch%20supplies%20these%20cues.%20Modern%20visuo-tactile%20sensors%20capture%20both%20modalities%20in%20a%20single%20fused%20image%2C%20yielding%20intrinsically%20aligned%20inputs%20that%20are%20well%20suited%20to%20manipulation%20tasks%20requiring%20visual%20and%20tactile%20information.%20Most%20self-supervised%20learning%20%28SSL%29%20frameworks%2C%20however%2C%20compress%20feature%20maps%20into%20a%20global%20vector%2C%20discarding%20spatial%20structure%20and%20misaligning%20with%20the%20needs%20of%20manipulation.%20To%20address%20this%2C%20we%20propose%20SARL%2C%20a%20spatially-aware%20SSL%20framework%20that%20augments%20the%20Bootstrap%20Your%20Own%20Latent%20%28BYOL%29%20architecture%20with%20three%20map-level%20objectives%2C%20including%20Saliency%20Alignment%20%28SAL%29%2C%20Patch-Prototype%20Distribution%20Alignment%20%28PPDA%29%2C%20and%20Region%20Affinity%20Matching%20%28RAM%29%2C%20to%20keep%20attentional%20focus%2C%20part%20composition%2C%20and%20geometric%20relations%20consistent%20across%20views.%20These%20losses%20act%20on%20intermediate%20feature%20maps%2C%20complementing%20the%20global%20objective.%20SARL%20consistently%20outperforms%20nine%20SSL%20baselines%20across%20six%20downstream%20tasks%20with%20fused%20visual-tactile%20data.%20On%20the%20geometry-sensitive%20edge-pose%20regression%20task%2C%20SARL%20achieves%20a%20Mean%20Absolute%20Error%20%28MAE%29%20of%200.3955%2C%20a%2030%25%20relative%20improvement%20over%20the%20next-best%20SSL%20method%20%280.5682%20MAE%29%20and%20approaching%20the%20supervised%20upper%20bound.%20These%20findings%20indicate%20that%2C%20for%20fused%20visual-tactile%20data%2C%20the%20most%20effective%20signal%20is%20structured%20spatial%20equivariance%2C%20in%20which%20features%20vary%20predictably%20with%20object%20geometry%2C%20which%20enables%20more%20capable%20robotic%20perception.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSARL%253A%2520Spatially-Aware%2520Self-Supervised%2520Representation%2520Learning%2520for%2520Visuo-Tactile%2520Perception%26entry.906535625%3DGurmeher%2520Khurana%2520and%2520Lan%2520Wei%2520and%2520Dandan%2520Zhang%26entry.1292438233%3DContact-rich%2520robotic%2520manipulation%2520requires%2520representations%2520that%2520encode%2520local%2520geometry.%2520Vision%2520provides%2520global%2520context%2520but%2520lacks%2520direct%2520measurements%2520of%2520properties%2520such%2520as%2520texture%2520and%2520hardness%252C%2520whereas%2520touch%2520supplies%2520these%2520cues.%2520Modern%2520visuo-tactile%2520sensors%2520capture%2520both%2520modalities%2520in%2520a%2520single%2520fused%2520image%252C%2520yielding%2520intrinsically%2520aligned%2520inputs%2520that%2520are%2520well%2520suited%2520to%2520manipulation%2520tasks%2520requiring%2520visual%2520and%2520tactile%2520information.%2520Most%2520self-supervised%2520learning%2520%2528SSL%2529%2520frameworks%252C%2520however%252C%2520compress%2520feature%2520maps%2520into%2520a%2520global%2520vector%252C%2520discarding%2520spatial%2520structure%2520and%2520misaligning%2520with%2520the%2520needs%2520of%2520manipulation.%2520To%2520address%2520this%252C%2520we%2520propose%2520SARL%252C%2520a%2520spatially-aware%2520SSL%2520framework%2520that%2520augments%2520the%2520Bootstrap%2520Your%2520Own%2520Latent%2520%2528BYOL%2529%2520architecture%2520with%2520three%2520map-level%2520objectives%252C%2520including%2520Saliency%2520Alignment%2520%2528SAL%2529%252C%2520Patch-Prototype%2520Distribution%2520Alignment%2520%2528PPDA%2529%252C%2520and%2520Region%2520Affinity%2520Matching%2520%2528RAM%2529%252C%2520to%2520keep%2520attentional%2520focus%252C%2520part%2520composition%252C%2520and%2520geometric%2520relations%2520consistent%2520across%2520views.%2520These%2520losses%2520act%2520on%2520intermediate%2520feature%2520maps%252C%2520complementing%2520the%2520global%2520objective.%2520SARL%2520consistently%2520outperforms%2520nine%2520SSL%2520baselines%2520across%2520six%2520downstream%2520tasks%2520with%2520fused%2520visual-tactile%2520data.%2520On%2520the%2520geometry-sensitive%2520edge-pose%2520regression%2520task%252C%2520SARL%2520achieves%2520a%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520of%25200.3955%252C%2520a%252030%2525%2520relative%2520improvement%2520over%2520the%2520next-best%2520SSL%2520method%2520%25280.5682%2520MAE%2529%2520and%2520approaching%2520the%2520supervised%2520upper%2520bound.%2520These%2520findings%2520indicate%2520that%252C%2520for%2520fused%2520visual-tactile%2520data%252C%2520the%2520most%2520effective%2520signal%2520is%2520structured%2520spatial%2520equivariance%252C%2520in%2520which%2520features%2520vary%2520predictably%2520with%2520object%2520geometry%252C%2520which%2520enables%2520more%2520capable%2520robotic%2520perception.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SARL%3A%20Spatially-Aware%20Self-Supervised%20Representation%20Learning%20for%20Visuo-Tactile%20Perception&entry.906535625=Gurmeher%20Khurana%20and%20Lan%20Wei%20and%20Dandan%20Zhang&entry.1292438233=Contact-rich%20robotic%20manipulation%20requires%20representations%20that%20encode%20local%20geometry.%20Vision%20provides%20global%20context%20but%20lacks%20direct%20measurements%20of%20properties%20such%20as%20texture%20and%20hardness%2C%20whereas%20touch%20supplies%20these%20cues.%20Modern%20visuo-tactile%20sensors%20capture%20both%20modalities%20in%20a%20single%20fused%20image%2C%20yielding%20intrinsically%20aligned%20inputs%20that%20are%20well%20suited%20to%20manipulation%20tasks%20requiring%20visual%20and%20tactile%20information.%20Most%20self-supervised%20learning%20%28SSL%29%20frameworks%2C%20however%2C%20compress%20feature%20maps%20into%20a%20global%20vector%2C%20discarding%20spatial%20structure%20and%20misaligning%20with%20the%20needs%20of%20manipulation.%20To%20address%20this%2C%20we%20propose%20SARL%2C%20a%20spatially-aware%20SSL%20framework%20that%20augments%20the%20Bootstrap%20Your%20Own%20Latent%20%28BYOL%29%20architecture%20with%20three%20map-level%20objectives%2C%20including%20Saliency%20Alignment%20%28SAL%29%2C%20Patch-Prototype%20Distribution%20Alignment%20%28PPDA%29%2C%20and%20Region%20Affinity%20Matching%20%28RAM%29%2C%20to%20keep%20attentional%20focus%2C%20part%20composition%2C%20and%20geometric%20relations%20consistent%20across%20views.%20These%20losses%20act%20on%20intermediate%20feature%20maps%2C%20complementing%20the%20global%20objective.%20SARL%20consistently%20outperforms%20nine%20SSL%20baselines%20across%20six%20downstream%20tasks%20with%20fused%20visual-tactile%20data.%20On%20the%20geometry-sensitive%20edge-pose%20regression%20task%2C%20SARL%20achieves%20a%20Mean%20Absolute%20Error%20%28MAE%29%20of%200.3955%2C%20a%2030%25%20relative%20improvement%20over%20the%20next-best%20SSL%20method%20%280.5682%20MAE%29%20and%20approaching%20the%20supervised%20upper%20bound.%20These%20findings%20indicate%20that%2C%20for%20fused%20visual-tactile%20data%2C%20the%20most%20effective%20signal%20is%20structured%20spatial%20equivariance%2C%20in%20which%20features%20vary%20predictably%20with%20object%20geometry%2C%20which%20enables%20more%20capable%20robotic%20perception.&entry.1838667208=http%3A//arxiv.org/abs/2512.01908v1&entry.124074799=Read"},
{"title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion", "author": "Shaowei Liu and David Yifan Yao and Saurabh Gupta and Shenlong Wang", "abstract": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.", "link": "http://arxiv.org/abs/2512.02017v1", "date": "2025-12-01", "relevancy": 2.2708, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5955}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5754}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Sync%3A%20Multi-Camera%20Synchronization%20via%20Cross-View%20Object%20Motion&body=Title%3A%20Visual%20Sync%3A%20Multi-Camera%20Synchronization%20via%20Cross-View%20Object%20Motion%0AAuthor%3A%20Shaowei%20Liu%20and%20David%20Yifan%20Yao%20and%20Saurabh%20Gupta%20and%20Shenlong%20Wang%0AAbstract%3A%20Today%2C%20people%20can%20easily%20record%20memorable%20moments%2C%20ranging%20from%20concerts%2C%20sports%20events%2C%20lectures%2C%20family%20gatherings%2C%20and%20birthday%20parties%20with%20multiple%20consumer%20cameras.%20However%2C%20synchronizing%20these%20cross-camera%20streams%20remains%20challenging.%20Existing%20methods%20assume%20controlled%20settings%2C%20specific%20targets%2C%20manual%20correction%2C%20or%20costly%20hardware.%20We%20present%20VisualSync%2C%20an%20optimization%20framework%20based%20on%20multi-view%20dynamics%20that%20aligns%20unposed%2C%20unsynchronized%20videos%20at%20millisecond%20accuracy.%20Our%20key%20insight%20is%20that%20any%20moving%203D%20point%2C%20when%20co-visible%20in%20two%20cameras%2C%20obeys%20epipolar%20constraints%20once%20properly%20synchronized.%20To%20exploit%20this%2C%20VisualSync%20leverages%20off-the-shelf%203D%20reconstruction%2C%20feature%20matching%2C%20and%20dense%20tracking%20to%20extract%20tracklets%2C%20relative%20poses%2C%20and%20cross-view%20correspondences.%20It%20then%20jointly%20minimizes%20the%20epipolar%20error%20to%20estimate%20each%20camera%27s%20time%20offset.%20Experiments%20on%20four%20diverse%2C%20challenging%20datasets%20show%20that%20VisualSync%20outperforms%20baseline%20methods%2C%20achieving%20an%20median%20synchronization%20error%20below%2050%20ms.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Sync%253A%2520Multi-Camera%2520Synchronization%2520via%2520Cross-View%2520Object%2520Motion%26entry.906535625%3DShaowei%2520Liu%2520and%2520David%2520Yifan%2520Yao%2520and%2520Saurabh%2520Gupta%2520and%2520Shenlong%2520Wang%26entry.1292438233%3DToday%252C%2520people%2520can%2520easily%2520record%2520memorable%2520moments%252C%2520ranging%2520from%2520concerts%252C%2520sports%2520events%252C%2520lectures%252C%2520family%2520gatherings%252C%2520and%2520birthday%2520parties%2520with%2520multiple%2520consumer%2520cameras.%2520However%252C%2520synchronizing%2520these%2520cross-camera%2520streams%2520remains%2520challenging.%2520Existing%2520methods%2520assume%2520controlled%2520settings%252C%2520specific%2520targets%252C%2520manual%2520correction%252C%2520or%2520costly%2520hardware.%2520We%2520present%2520VisualSync%252C%2520an%2520optimization%2520framework%2520based%2520on%2520multi-view%2520dynamics%2520that%2520aligns%2520unposed%252C%2520unsynchronized%2520videos%2520at%2520millisecond%2520accuracy.%2520Our%2520key%2520insight%2520is%2520that%2520any%2520moving%25203D%2520point%252C%2520when%2520co-visible%2520in%2520two%2520cameras%252C%2520obeys%2520epipolar%2520constraints%2520once%2520properly%2520synchronized.%2520To%2520exploit%2520this%252C%2520VisualSync%2520leverages%2520off-the-shelf%25203D%2520reconstruction%252C%2520feature%2520matching%252C%2520and%2520dense%2520tracking%2520to%2520extract%2520tracklets%252C%2520relative%2520poses%252C%2520and%2520cross-view%2520correspondences.%2520It%2520then%2520jointly%2520minimizes%2520the%2520epipolar%2520error%2520to%2520estimate%2520each%2520camera%2527s%2520time%2520offset.%2520Experiments%2520on%2520four%2520diverse%252C%2520challenging%2520datasets%2520show%2520that%2520VisualSync%2520outperforms%2520baseline%2520methods%252C%2520achieving%2520an%2520median%2520synchronization%2520error%2520below%252050%2520ms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Sync%3A%20Multi-Camera%20Synchronization%20via%20Cross-View%20Object%20Motion&entry.906535625=Shaowei%20Liu%20and%20David%20Yifan%20Yao%20and%20Saurabh%20Gupta%20and%20Shenlong%20Wang&entry.1292438233=Today%2C%20people%20can%20easily%20record%20memorable%20moments%2C%20ranging%20from%20concerts%2C%20sports%20events%2C%20lectures%2C%20family%20gatherings%2C%20and%20birthday%20parties%20with%20multiple%20consumer%20cameras.%20However%2C%20synchronizing%20these%20cross-camera%20streams%20remains%20challenging.%20Existing%20methods%20assume%20controlled%20settings%2C%20specific%20targets%2C%20manual%20correction%2C%20or%20costly%20hardware.%20We%20present%20VisualSync%2C%20an%20optimization%20framework%20based%20on%20multi-view%20dynamics%20that%20aligns%20unposed%2C%20unsynchronized%20videos%20at%20millisecond%20accuracy.%20Our%20key%20insight%20is%20that%20any%20moving%203D%20point%2C%20when%20co-visible%20in%20two%20cameras%2C%20obeys%20epipolar%20constraints%20once%20properly%20synchronized.%20To%20exploit%20this%2C%20VisualSync%20leverages%20off-the-shelf%203D%20reconstruction%2C%20feature%20matching%2C%20and%20dense%20tracking%20to%20extract%20tracklets%2C%20relative%20poses%2C%20and%20cross-view%20correspondences.%20It%20then%20jointly%20minimizes%20the%20epipolar%20error%20to%20estimate%20each%20camera%27s%20time%20offset.%20Experiments%20on%20four%20diverse%2C%20challenging%20datasets%20show%20that%20VisualSync%20outperforms%20baseline%20methods%2C%20achieving%20an%20median%20synchronization%20error%20below%2050%20ms.&entry.1838667208=http%3A//arxiv.org/abs/2512.02017v1&entry.124074799=Read"},
{"title": "SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge", "author": "Yumeng He and Ying Jiang and Jiayin Lu and Yin Yang and Chenfanfu Jiang", "abstract": "Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling.", "link": "http://arxiv.org/abs/2512.01629v1", "date": "2025-12-01", "relevancy": 2.2655, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5866}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5626}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPARK%3A%20Sim-ready%20Part-level%20Articulated%20Reconstruction%20with%20VLM%20Knowledge&body=Title%3A%20SPARK%3A%20Sim-ready%20Part-level%20Articulated%20Reconstruction%20with%20VLM%20Knowledge%0AAuthor%3A%20Yumeng%20He%20and%20Ying%20Jiang%20and%20Jiayin%20Lu%20and%20Yin%20Yang%20and%20Chenfanfu%20Jiang%0AAbstract%3A%20Articulated%203D%20objects%20are%20critical%20for%20embodied%20AI%2C%20robotics%2C%20and%20interactive%20scene%20understanding%2C%20yet%20creating%20simulation-ready%20assets%20remains%20labor-intensive%20and%20requires%20expert%20modeling%20of%20part%20hierarchies%20and%20motion%20structures.%20We%20introduce%20SPARK%2C%20a%20framework%20for%20reconstructing%20physically%20consistent%2C%20kinematic%20part-level%20articulated%20objects%20from%20a%20single%20RGB%20image.%20Given%20an%20input%20image%2C%20we%20first%20leverage%20VLMs%20to%20extract%20coarse%20URDF%20parameters%20and%20generate%20part-level%20reference%20images.%20We%20then%20integrate%20the%20part-image%20guidance%20and%20the%20inferred%20structure%20graph%20into%20a%20generative%20diffusion%20transformer%20to%20synthesize%20consistent%20part%20and%20complete%20shapes%20of%20articulated%20objects.%20To%20further%20refine%20the%20URDF%20parameters%2C%20we%20incorporate%20differentiable%20forward%20kinematics%20and%20differentiable%20rendering%20to%20optimize%20joint%20types%2C%20axes%2C%20and%20origins%20under%20VLM-generated%20open-state%20supervision.%20Extensive%20experiments%20show%20that%20SPARK%20produces%20high-quality%2C%20simulation-ready%20articulated%20assets%20across%20diverse%20categories%2C%20enabling%20downstream%20applications%20such%20as%20robotic%20manipulation%20and%20interaction%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPARK%253A%2520Sim-ready%2520Part-level%2520Articulated%2520Reconstruction%2520with%2520VLM%2520Knowledge%26entry.906535625%3DYumeng%2520He%2520and%2520Ying%2520Jiang%2520and%2520Jiayin%2520Lu%2520and%2520Yin%2520Yang%2520and%2520Chenfanfu%2520Jiang%26entry.1292438233%3DArticulated%25203D%2520objects%2520are%2520critical%2520for%2520embodied%2520AI%252C%2520robotics%252C%2520and%2520interactive%2520scene%2520understanding%252C%2520yet%2520creating%2520simulation-ready%2520assets%2520remains%2520labor-intensive%2520and%2520requires%2520expert%2520modeling%2520of%2520part%2520hierarchies%2520and%2520motion%2520structures.%2520We%2520introduce%2520SPARK%252C%2520a%2520framework%2520for%2520reconstructing%2520physically%2520consistent%252C%2520kinematic%2520part-level%2520articulated%2520objects%2520from%2520a%2520single%2520RGB%2520image.%2520Given%2520an%2520input%2520image%252C%2520we%2520first%2520leverage%2520VLMs%2520to%2520extract%2520coarse%2520URDF%2520parameters%2520and%2520generate%2520part-level%2520reference%2520images.%2520We%2520then%2520integrate%2520the%2520part-image%2520guidance%2520and%2520the%2520inferred%2520structure%2520graph%2520into%2520a%2520generative%2520diffusion%2520transformer%2520to%2520synthesize%2520consistent%2520part%2520and%2520complete%2520shapes%2520of%2520articulated%2520objects.%2520To%2520further%2520refine%2520the%2520URDF%2520parameters%252C%2520we%2520incorporate%2520differentiable%2520forward%2520kinematics%2520and%2520differentiable%2520rendering%2520to%2520optimize%2520joint%2520types%252C%2520axes%252C%2520and%2520origins%2520under%2520VLM-generated%2520open-state%2520supervision.%2520Extensive%2520experiments%2520show%2520that%2520SPARK%2520produces%2520high-quality%252C%2520simulation-ready%2520articulated%2520assets%2520across%2520diverse%2520categories%252C%2520enabling%2520downstream%2520applications%2520such%2520as%2520robotic%2520manipulation%2520and%2520interaction%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPARK%3A%20Sim-ready%20Part-level%20Articulated%20Reconstruction%20with%20VLM%20Knowledge&entry.906535625=Yumeng%20He%20and%20Ying%20Jiang%20and%20Jiayin%20Lu%20and%20Yin%20Yang%20and%20Chenfanfu%20Jiang&entry.1292438233=Articulated%203D%20objects%20are%20critical%20for%20embodied%20AI%2C%20robotics%2C%20and%20interactive%20scene%20understanding%2C%20yet%20creating%20simulation-ready%20assets%20remains%20labor-intensive%20and%20requires%20expert%20modeling%20of%20part%20hierarchies%20and%20motion%20structures.%20We%20introduce%20SPARK%2C%20a%20framework%20for%20reconstructing%20physically%20consistent%2C%20kinematic%20part-level%20articulated%20objects%20from%20a%20single%20RGB%20image.%20Given%20an%20input%20image%2C%20we%20first%20leverage%20VLMs%20to%20extract%20coarse%20URDF%20parameters%20and%20generate%20part-level%20reference%20images.%20We%20then%20integrate%20the%20part-image%20guidance%20and%20the%20inferred%20structure%20graph%20into%20a%20generative%20diffusion%20transformer%20to%20synthesize%20consistent%20part%20and%20complete%20shapes%20of%20articulated%20objects.%20To%20further%20refine%20the%20URDF%20parameters%2C%20we%20incorporate%20differentiable%20forward%20kinematics%20and%20differentiable%20rendering%20to%20optimize%20joint%20types%2C%20axes%2C%20and%20origins%20under%20VLM-generated%20open-state%20supervision.%20Extensive%20experiments%20show%20that%20SPARK%20produces%20high-quality%2C%20simulation-ready%20articulated%20assets%20across%20diverse%20categories%2C%20enabling%20downstream%20applications%20such%20as%20robotic%20manipulation%20and%20interaction%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2512.01629v1&entry.124074799=Read"},
{"title": "LPCD: Unified Framework from Layer-Wise to Submodule Quantization", "author": "Yuma Ichikawa and Yudai Fujimoto and Akira Sakai", "abstract": "Post-training quantization (PTQ) aims to preserve model-level behavior; however, most methods focus on individual linear layers. Even recent extensions, such as QEP and LoaQ, which mitigate error propagation or target specific submodules, still rely on layer-wise formulations and fail to capture the behavior of larger submodules. We introduce Layer-Projected Coordinate Descent (LPCD), a unified framework that extends PTQ beyond layers by optimizing relaxed objectives across arbitrary submodules and projecting the solutions with layer-wise quantizers. LPCD generalizes existing methods and provides a principled approach to quantizing complex submodules while maintaining the efficiency and compatibility of layer-wise PTQ pipelines. Across diverse LLM architectures and bit-widths, LPCD-based submodule quantization consistently enhances both layer-wise PTQ methods and existing submodule approaches.", "link": "http://arxiv.org/abs/2512.01546v1", "date": "2025-12-01", "relevancy": 2.2638, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4571}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LPCD%3A%20Unified%20Framework%20from%20Layer-Wise%20to%20Submodule%20Quantization&body=Title%3A%20LPCD%3A%20Unified%20Framework%20from%20Layer-Wise%20to%20Submodule%20Quantization%0AAuthor%3A%20Yuma%20Ichikawa%20and%20Yudai%20Fujimoto%20and%20Akira%20Sakai%0AAbstract%3A%20Post-training%20quantization%20%28PTQ%29%20aims%20to%20preserve%20model-level%20behavior%3B%20however%2C%20most%20methods%20focus%20on%20individual%20linear%20layers.%20Even%20recent%20extensions%2C%20such%20as%20QEP%20and%20LoaQ%2C%20which%20mitigate%20error%20propagation%20or%20target%20specific%20submodules%2C%20still%20rely%20on%20layer-wise%20formulations%20and%20fail%20to%20capture%20the%20behavior%20of%20larger%20submodules.%20We%20introduce%20Layer-Projected%20Coordinate%20Descent%20%28LPCD%29%2C%20a%20unified%20framework%20that%20extends%20PTQ%20beyond%20layers%20by%20optimizing%20relaxed%20objectives%20across%20arbitrary%20submodules%20and%20projecting%20the%20solutions%20with%20layer-wise%20quantizers.%20LPCD%20generalizes%20existing%20methods%20and%20provides%20a%20principled%20approach%20to%20quantizing%20complex%20submodules%20while%20maintaining%20the%20efficiency%20and%20compatibility%20of%20layer-wise%20PTQ%20pipelines.%20Across%20diverse%20LLM%20architectures%20and%20bit-widths%2C%20LPCD-based%20submodule%20quantization%20consistently%20enhances%20both%20layer-wise%20PTQ%20methods%20and%20existing%20submodule%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLPCD%253A%2520Unified%2520Framework%2520from%2520Layer-Wise%2520to%2520Submodule%2520Quantization%26entry.906535625%3DYuma%2520Ichikawa%2520and%2520Yudai%2520Fujimoto%2520and%2520Akira%2520Sakai%26entry.1292438233%3DPost-training%2520quantization%2520%2528PTQ%2529%2520aims%2520to%2520preserve%2520model-level%2520behavior%253B%2520however%252C%2520most%2520methods%2520focus%2520on%2520individual%2520linear%2520layers.%2520Even%2520recent%2520extensions%252C%2520such%2520as%2520QEP%2520and%2520LoaQ%252C%2520which%2520mitigate%2520error%2520propagation%2520or%2520target%2520specific%2520submodules%252C%2520still%2520rely%2520on%2520layer-wise%2520formulations%2520and%2520fail%2520to%2520capture%2520the%2520behavior%2520of%2520larger%2520submodules.%2520We%2520introduce%2520Layer-Projected%2520Coordinate%2520Descent%2520%2528LPCD%2529%252C%2520a%2520unified%2520framework%2520that%2520extends%2520PTQ%2520beyond%2520layers%2520by%2520optimizing%2520relaxed%2520objectives%2520across%2520arbitrary%2520submodules%2520and%2520projecting%2520the%2520solutions%2520with%2520layer-wise%2520quantizers.%2520LPCD%2520generalizes%2520existing%2520methods%2520and%2520provides%2520a%2520principled%2520approach%2520to%2520quantizing%2520complex%2520submodules%2520while%2520maintaining%2520the%2520efficiency%2520and%2520compatibility%2520of%2520layer-wise%2520PTQ%2520pipelines.%2520Across%2520diverse%2520LLM%2520architectures%2520and%2520bit-widths%252C%2520LPCD-based%2520submodule%2520quantization%2520consistently%2520enhances%2520both%2520layer-wise%2520PTQ%2520methods%2520and%2520existing%2520submodule%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LPCD%3A%20Unified%20Framework%20from%20Layer-Wise%20to%20Submodule%20Quantization&entry.906535625=Yuma%20Ichikawa%20and%20Yudai%20Fujimoto%20and%20Akira%20Sakai&entry.1292438233=Post-training%20quantization%20%28PTQ%29%20aims%20to%20preserve%20model-level%20behavior%3B%20however%2C%20most%20methods%20focus%20on%20individual%20linear%20layers.%20Even%20recent%20extensions%2C%20such%20as%20QEP%20and%20LoaQ%2C%20which%20mitigate%20error%20propagation%20or%20target%20specific%20submodules%2C%20still%20rely%20on%20layer-wise%20formulations%20and%20fail%20to%20capture%20the%20behavior%20of%20larger%20submodules.%20We%20introduce%20Layer-Projected%20Coordinate%20Descent%20%28LPCD%29%2C%20a%20unified%20framework%20that%20extends%20PTQ%20beyond%20layers%20by%20optimizing%20relaxed%20objectives%20across%20arbitrary%20submodules%20and%20projecting%20the%20solutions%20with%20layer-wise%20quantizers.%20LPCD%20generalizes%20existing%20methods%20and%20provides%20a%20principled%20approach%20to%20quantizing%20complex%20submodules%20while%20maintaining%20the%20efficiency%20and%20compatibility%20of%20layer-wise%20PTQ%20pipelines.%20Across%20diverse%20LLM%20architectures%20and%20bit-widths%2C%20LPCD-based%20submodule%20quantization%20consistently%20enhances%20both%20layer-wise%20PTQ%20methods%20and%20existing%20submodule%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2512.01546v1&entry.124074799=Read"},
{"title": "ECO: Energy-Constrained Operator Learning for Chaotic Dynamics with Boundedness Guarantees", "author": "Andrea Goertzen and Sunbochen Tang and Navid Azizan", "abstract": "Chaos is a fundamental feature of many complex dynamical systems, including weather systems and fluid turbulence. These systems are inherently difficult to predict due to their extreme sensitivity to initial conditions. Many chaotic systems are dissipative and ergodic, motivating data-driven models that aim to learn invariant statistical properties over long time horizons. While recent models have shown empirical success in preserving invariant statistics, they are prone to generating unbounded predictions, which prevent meaningful statistics evaluation. To overcome this, we introduce the Energy-Constrained Operator (ECO) that simultaneously learns the system dynamics while enforcing boundedness in predictions. We leverage concepts from control theory to develop algebraic conditions based on a learnable energy function, ensuring the learned dynamics is dissipative. ECO enforces these algebraic conditions through an efficient closed-form quadratic projection layer, which provides provable trajectory boundedness. To our knowledge, this is the first work establishing such formal guarantees for data-driven chaotic dynamics models. Additionally, the learned invariant level set provides an outer estimate for the strange attractor, a complex structure that is computationally intractable to characterize. We demonstrate empirical success in ECO's ability to generate stable long-horizon forecasts, capturing invariant statistics on systems governed by chaotic PDEs, including the Kuramoto--Sivashinsky and the Navier--Stokes equations.", "link": "http://arxiv.org/abs/2512.01984v1", "date": "2025-12-01", "relevancy": 2.2598, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.466}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4458}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECO%3A%20Energy-Constrained%20Operator%20Learning%20for%20Chaotic%20Dynamics%20with%20Boundedness%20Guarantees&body=Title%3A%20ECO%3A%20Energy-Constrained%20Operator%20Learning%20for%20Chaotic%20Dynamics%20with%20Boundedness%20Guarantees%0AAuthor%3A%20Andrea%20Goertzen%20and%20Sunbochen%20Tang%20and%20Navid%20Azizan%0AAbstract%3A%20Chaos%20is%20a%20fundamental%20feature%20of%20many%20complex%20dynamical%20systems%2C%20including%20weather%20systems%20and%20fluid%20turbulence.%20These%20systems%20are%20inherently%20difficult%20to%20predict%20due%20to%20their%20extreme%20sensitivity%20to%20initial%20conditions.%20Many%20chaotic%20systems%20are%20dissipative%20and%20ergodic%2C%20motivating%20data-driven%20models%20that%20aim%20to%20learn%20invariant%20statistical%20properties%20over%20long%20time%20horizons.%20While%20recent%20models%20have%20shown%20empirical%20success%20in%20preserving%20invariant%20statistics%2C%20they%20are%20prone%20to%20generating%20unbounded%20predictions%2C%20which%20prevent%20meaningful%20statistics%20evaluation.%20To%20overcome%20this%2C%20we%20introduce%20the%20Energy-Constrained%20Operator%20%28ECO%29%20that%20simultaneously%20learns%20the%20system%20dynamics%20while%20enforcing%20boundedness%20in%20predictions.%20We%20leverage%20concepts%20from%20control%20theory%20to%20develop%20algebraic%20conditions%20based%20on%20a%20learnable%20energy%20function%2C%20ensuring%20the%20learned%20dynamics%20is%20dissipative.%20ECO%20enforces%20these%20algebraic%20conditions%20through%20an%20efficient%20closed-form%20quadratic%20projection%20layer%2C%20which%20provides%20provable%20trajectory%20boundedness.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20establishing%20such%20formal%20guarantees%20for%20data-driven%20chaotic%20dynamics%20models.%20Additionally%2C%20the%20learned%20invariant%20level%20set%20provides%20an%20outer%20estimate%20for%20the%20strange%20attractor%2C%20a%20complex%20structure%20that%20is%20computationally%20intractable%20to%20characterize.%20We%20demonstrate%20empirical%20success%20in%20ECO%27s%20ability%20to%20generate%20stable%20long-horizon%20forecasts%2C%20capturing%20invariant%20statistics%20on%20systems%20governed%20by%20chaotic%20PDEs%2C%20including%20the%20Kuramoto--Sivashinsky%20and%20the%20Navier--Stokes%20equations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECO%253A%2520Energy-Constrained%2520Operator%2520Learning%2520for%2520Chaotic%2520Dynamics%2520with%2520Boundedness%2520Guarantees%26entry.906535625%3DAndrea%2520Goertzen%2520and%2520Sunbochen%2520Tang%2520and%2520Navid%2520Azizan%26entry.1292438233%3DChaos%2520is%2520a%2520fundamental%2520feature%2520of%2520many%2520complex%2520dynamical%2520systems%252C%2520including%2520weather%2520systems%2520and%2520fluid%2520turbulence.%2520These%2520systems%2520are%2520inherently%2520difficult%2520to%2520predict%2520due%2520to%2520their%2520extreme%2520sensitivity%2520to%2520initial%2520conditions.%2520Many%2520chaotic%2520systems%2520are%2520dissipative%2520and%2520ergodic%252C%2520motivating%2520data-driven%2520models%2520that%2520aim%2520to%2520learn%2520invariant%2520statistical%2520properties%2520over%2520long%2520time%2520horizons.%2520While%2520recent%2520models%2520have%2520shown%2520empirical%2520success%2520in%2520preserving%2520invariant%2520statistics%252C%2520they%2520are%2520prone%2520to%2520generating%2520unbounded%2520predictions%252C%2520which%2520prevent%2520meaningful%2520statistics%2520evaluation.%2520To%2520overcome%2520this%252C%2520we%2520introduce%2520the%2520Energy-Constrained%2520Operator%2520%2528ECO%2529%2520that%2520simultaneously%2520learns%2520the%2520system%2520dynamics%2520while%2520enforcing%2520boundedness%2520in%2520predictions.%2520We%2520leverage%2520concepts%2520from%2520control%2520theory%2520to%2520develop%2520algebraic%2520conditions%2520based%2520on%2520a%2520learnable%2520energy%2520function%252C%2520ensuring%2520the%2520learned%2520dynamics%2520is%2520dissipative.%2520ECO%2520enforces%2520these%2520algebraic%2520conditions%2520through%2520an%2520efficient%2520closed-form%2520quadratic%2520projection%2520layer%252C%2520which%2520provides%2520provable%2520trajectory%2520boundedness.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520establishing%2520such%2520formal%2520guarantees%2520for%2520data-driven%2520chaotic%2520dynamics%2520models.%2520Additionally%252C%2520the%2520learned%2520invariant%2520level%2520set%2520provides%2520an%2520outer%2520estimate%2520for%2520the%2520strange%2520attractor%252C%2520a%2520complex%2520structure%2520that%2520is%2520computationally%2520intractable%2520to%2520characterize.%2520We%2520demonstrate%2520empirical%2520success%2520in%2520ECO%2527s%2520ability%2520to%2520generate%2520stable%2520long-horizon%2520forecasts%252C%2520capturing%2520invariant%2520statistics%2520on%2520systems%2520governed%2520by%2520chaotic%2520PDEs%252C%2520including%2520the%2520Kuramoto--Sivashinsky%2520and%2520the%2520Navier--Stokes%2520equations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECO%3A%20Energy-Constrained%20Operator%20Learning%20for%20Chaotic%20Dynamics%20with%20Boundedness%20Guarantees&entry.906535625=Andrea%20Goertzen%20and%20Sunbochen%20Tang%20and%20Navid%20Azizan&entry.1292438233=Chaos%20is%20a%20fundamental%20feature%20of%20many%20complex%20dynamical%20systems%2C%20including%20weather%20systems%20and%20fluid%20turbulence.%20These%20systems%20are%20inherently%20difficult%20to%20predict%20due%20to%20their%20extreme%20sensitivity%20to%20initial%20conditions.%20Many%20chaotic%20systems%20are%20dissipative%20and%20ergodic%2C%20motivating%20data-driven%20models%20that%20aim%20to%20learn%20invariant%20statistical%20properties%20over%20long%20time%20horizons.%20While%20recent%20models%20have%20shown%20empirical%20success%20in%20preserving%20invariant%20statistics%2C%20they%20are%20prone%20to%20generating%20unbounded%20predictions%2C%20which%20prevent%20meaningful%20statistics%20evaluation.%20To%20overcome%20this%2C%20we%20introduce%20the%20Energy-Constrained%20Operator%20%28ECO%29%20that%20simultaneously%20learns%20the%20system%20dynamics%20while%20enforcing%20boundedness%20in%20predictions.%20We%20leverage%20concepts%20from%20control%20theory%20to%20develop%20algebraic%20conditions%20based%20on%20a%20learnable%20energy%20function%2C%20ensuring%20the%20learned%20dynamics%20is%20dissipative.%20ECO%20enforces%20these%20algebraic%20conditions%20through%20an%20efficient%20closed-form%20quadratic%20projection%20layer%2C%20which%20provides%20provable%20trajectory%20boundedness.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20establishing%20such%20formal%20guarantees%20for%20data-driven%20chaotic%20dynamics%20models.%20Additionally%2C%20the%20learned%20invariant%20level%20set%20provides%20an%20outer%20estimate%20for%20the%20strange%20attractor%2C%20a%20complex%20structure%20that%20is%20computationally%20intractable%20to%20characterize.%20We%20demonstrate%20empirical%20success%20in%20ECO%27s%20ability%20to%20generate%20stable%20long-horizon%20forecasts%2C%20capturing%20invariant%20statistics%20on%20systems%20governed%20by%20chaotic%20PDEs%2C%20including%20the%20Kuramoto--Sivashinsky%20and%20the%20Navier--Stokes%20equations.&entry.1838667208=http%3A//arxiv.org/abs/2512.01984v1&entry.124074799=Read"},
{"title": "Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective", "author": "Wang Luo and Di Wu and Hengyuan Na and Yinlin Zhu and Miao Hu and Guocong Quan", "abstract": "Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).", "link": "http://arxiv.org/abs/2511.12170v2", "date": "2025-12-01", "relevancy": 2.2565, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5766}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5653}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Multimodal%20Point%20Cloud%20Completion%3A%20A%20Completion-by-Correction%20Perspective&body=Title%3A%20Rethinking%20Multimodal%20Point%20Cloud%20Completion%3A%20A%20Completion-by-Correction%20Perspective%0AAuthor%3A%20Wang%20Luo%20and%20Di%20Wu%20and%20Hengyuan%20Na%20and%20Yinlin%20Zhu%20and%20Miao%20Hu%20and%20Guocong%20Quan%0AAbstract%3A%20Point%20cloud%20completion%20aims%20to%20reconstruct%20complete%203D%20shapes%20from%20partial%20observations%2C%20which%20is%20a%20challenging%20problem%20due%20to%20severe%20occlusions%20and%20missing%20geometry.%20Despite%20recent%20advances%20in%20multimodal%20techniques%20that%20leverage%20complementary%20RGB%20images%20to%20compensate%20for%20missing%20geometry%2C%20most%20methods%20still%20follow%20a%20Completion-by-Inpainting%20paradigm%2C%20synthesizing%20missing%20structures%20from%20fused%20latent%20features.%20We%20empirically%20show%20that%20this%20paradigm%20often%20results%20in%20structural%20inconsistencies%20and%20topological%20artifacts%20due%20to%20limited%20geometric%20and%20semantic%20constraints.%20To%20address%20this%2C%20we%20rethink%20the%20task%20and%20propose%20a%20more%20robust%20paradigm%2C%20termed%20Completion-by-Correction%2C%20which%20begins%20with%20a%20topologically%20complete%20shape%20prior%20generated%20by%20a%20pretrained%20image-to-3D%20model%20and%20performs%20feature-space%20correction%20to%20align%20it%20with%20the%20partial%20observation.%20This%20paradigm%20shifts%20completion%20from%20unconstrained%20synthesis%20to%20guided%20refinement%2C%20enabling%20structurally%20consistent%20and%20observation-aligned%20reconstruction.%20Building%20upon%20this%20paradigm%2C%20we%20introduce%20PGNet%2C%20a%20multi-stage%20framework%20that%20conducts%20dual-feature%20encoding%20to%20ground%20the%20generative%20prior%2C%20synthesizes%20a%20coarse%20yet%20structurally%20aligned%20scaffold%2C%20and%20progressively%20refines%20geometric%20details%20via%20hierarchical%20correction.%20Experiments%20on%20the%20ShapeNetViPC%20dataset%20demonstrate%20the%20superiority%20of%20PGNet%20over%20state-of-the-art%20baselines%20in%20terms%20of%20average%20Chamfer%20Distance%20%28-23.5%25%29%20and%20F-score%20%28%2B7.1%25%29.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12170v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Multimodal%2520Point%2520Cloud%2520Completion%253A%2520A%2520Completion-by-Correction%2520Perspective%26entry.906535625%3DWang%2520Luo%2520and%2520Di%2520Wu%2520and%2520Hengyuan%2520Na%2520and%2520Yinlin%2520Zhu%2520and%2520Miao%2520Hu%2520and%2520Guocong%2520Quan%26entry.1292438233%3DPoint%2520cloud%2520completion%2520aims%2520to%2520reconstruct%2520complete%25203D%2520shapes%2520from%2520partial%2520observations%252C%2520which%2520is%2520a%2520challenging%2520problem%2520due%2520to%2520severe%2520occlusions%2520and%2520missing%2520geometry.%2520Despite%2520recent%2520advances%2520in%2520multimodal%2520techniques%2520that%2520leverage%2520complementary%2520RGB%2520images%2520to%2520compensate%2520for%2520missing%2520geometry%252C%2520most%2520methods%2520still%2520follow%2520a%2520Completion-by-Inpainting%2520paradigm%252C%2520synthesizing%2520missing%2520structures%2520from%2520fused%2520latent%2520features.%2520We%2520empirically%2520show%2520that%2520this%2520paradigm%2520often%2520results%2520in%2520structural%2520inconsistencies%2520and%2520topological%2520artifacts%2520due%2520to%2520limited%2520geometric%2520and%2520semantic%2520constraints.%2520To%2520address%2520this%252C%2520we%2520rethink%2520the%2520task%2520and%2520propose%2520a%2520more%2520robust%2520paradigm%252C%2520termed%2520Completion-by-Correction%252C%2520which%2520begins%2520with%2520a%2520topologically%2520complete%2520shape%2520prior%2520generated%2520by%2520a%2520pretrained%2520image-to-3D%2520model%2520and%2520performs%2520feature-space%2520correction%2520to%2520align%2520it%2520with%2520the%2520partial%2520observation.%2520This%2520paradigm%2520shifts%2520completion%2520from%2520unconstrained%2520synthesis%2520to%2520guided%2520refinement%252C%2520enabling%2520structurally%2520consistent%2520and%2520observation-aligned%2520reconstruction.%2520Building%2520upon%2520this%2520paradigm%252C%2520we%2520introduce%2520PGNet%252C%2520a%2520multi-stage%2520framework%2520that%2520conducts%2520dual-feature%2520encoding%2520to%2520ground%2520the%2520generative%2520prior%252C%2520synthesizes%2520a%2520coarse%2520yet%2520structurally%2520aligned%2520scaffold%252C%2520and%2520progressively%2520refines%2520geometric%2520details%2520via%2520hierarchical%2520correction.%2520Experiments%2520on%2520the%2520ShapeNetViPC%2520dataset%2520demonstrate%2520the%2520superiority%2520of%2520PGNet%2520over%2520state-of-the-art%2520baselines%2520in%2520terms%2520of%2520average%2520Chamfer%2520Distance%2520%2528-23.5%2525%2529%2520and%2520F-score%2520%2528%252B7.1%2525%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12170v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Multimodal%20Point%20Cloud%20Completion%3A%20A%20Completion-by-Correction%20Perspective&entry.906535625=Wang%20Luo%20and%20Di%20Wu%20and%20Hengyuan%20Na%20and%20Yinlin%20Zhu%20and%20Miao%20Hu%20and%20Guocong%20Quan&entry.1292438233=Point%20cloud%20completion%20aims%20to%20reconstruct%20complete%203D%20shapes%20from%20partial%20observations%2C%20which%20is%20a%20challenging%20problem%20due%20to%20severe%20occlusions%20and%20missing%20geometry.%20Despite%20recent%20advances%20in%20multimodal%20techniques%20that%20leverage%20complementary%20RGB%20images%20to%20compensate%20for%20missing%20geometry%2C%20most%20methods%20still%20follow%20a%20Completion-by-Inpainting%20paradigm%2C%20synthesizing%20missing%20structures%20from%20fused%20latent%20features.%20We%20empirically%20show%20that%20this%20paradigm%20often%20results%20in%20structural%20inconsistencies%20and%20topological%20artifacts%20due%20to%20limited%20geometric%20and%20semantic%20constraints.%20To%20address%20this%2C%20we%20rethink%20the%20task%20and%20propose%20a%20more%20robust%20paradigm%2C%20termed%20Completion-by-Correction%2C%20which%20begins%20with%20a%20topologically%20complete%20shape%20prior%20generated%20by%20a%20pretrained%20image-to-3D%20model%20and%20performs%20feature-space%20correction%20to%20align%20it%20with%20the%20partial%20observation.%20This%20paradigm%20shifts%20completion%20from%20unconstrained%20synthesis%20to%20guided%20refinement%2C%20enabling%20structurally%20consistent%20and%20observation-aligned%20reconstruction.%20Building%20upon%20this%20paradigm%2C%20we%20introduce%20PGNet%2C%20a%20multi-stage%20framework%20that%20conducts%20dual-feature%20encoding%20to%20ground%20the%20generative%20prior%2C%20synthesizes%20a%20coarse%20yet%20structurally%20aligned%20scaffold%2C%20and%20progressively%20refines%20geometric%20details%20via%20hierarchical%20correction.%20Experiments%20on%20the%20ShapeNetViPC%20dataset%20demonstrate%20the%20superiority%20of%20PGNet%20over%20state-of-the-art%20baselines%20in%20terms%20of%20average%20Chamfer%20Distance%20%28-23.5%25%29%20and%20F-score%20%28%2B7.1%25%29.&entry.1838667208=http%3A//arxiv.org/abs/2511.12170v2&entry.124074799=Read"},
{"title": "A Unified Framework for Probabilistic Dynamic-, Trajectory- and Vision-based Virtual Fixtures", "author": "Maximilian M\u00fchlbauer and Bernhard Weber and Sylvain Calinon and Freek Stulp and Alin Albu-Sch\u00e4ffer and Jo\u00e3o Silv\u00e9rio", "abstract": "Probabilistic Virtual Fixtures (VFs) enable the adaptive selection of the most suitable haptic feedback for each phase of a task, based on learned or perceived uncertainty. While keeping the human in the loop remains essential, for instance, to ensure high precision, partial automation of certain task phases is critical for productivity. We present a unified framework for probabilistic VFs that seamlessly switches between manual fixtures, semi-automated fixtures (with the human handling precise tasks), and full autonomy. We introduce a novel probabilistic Dynamical System-based VF for coarse guidance, enabling the robot to autonomously complete certain task phases while keeping the human operator in the loop. For tasks requiring precise guidance, we extend probabilistic position-based trajectory fixtures with automation allowing for seamless human interaction as well as geometry-awareness and optimal impedance gains. For manual tasks requiring very precise guidance, we also extend visual servoing fixtures with the same geometry-awareness and impedance behavior. We validate our approach experimentally on different robots, showcasing multiple operation modes and the ease of programming fixtures.", "link": "http://arxiv.org/abs/2506.10239v2", "date": "2025-12-01", "relevancy": 2.2397, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6015}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.58}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20for%20Probabilistic%20Dynamic-%2C%20Trajectory-%20and%20Vision-based%20Virtual%20Fixtures&body=Title%3A%20A%20Unified%20Framework%20for%20Probabilistic%20Dynamic-%2C%20Trajectory-%20and%20Vision-based%20Virtual%20Fixtures%0AAuthor%3A%20Maximilian%20M%C3%BChlbauer%20and%20Bernhard%20Weber%20and%20Sylvain%20Calinon%20and%20Freek%20Stulp%20and%20Alin%20Albu-Sch%C3%A4ffer%20and%20Jo%C3%A3o%20Silv%C3%A9rio%0AAbstract%3A%20Probabilistic%20Virtual%20Fixtures%20%28VFs%29%20enable%20the%20adaptive%20selection%20of%20the%20most%20suitable%20haptic%20feedback%20for%20each%20phase%20of%20a%20task%2C%20based%20on%20learned%20or%20perceived%20uncertainty.%20While%20keeping%20the%20human%20in%20the%20loop%20remains%20essential%2C%20for%20instance%2C%20to%20ensure%20high%20precision%2C%20partial%20automation%20of%20certain%20task%20phases%20is%20critical%20for%20productivity.%20We%20present%20a%20unified%20framework%20for%20probabilistic%20VFs%20that%20seamlessly%20switches%20between%20manual%20fixtures%2C%20semi-automated%20fixtures%20%28with%20the%20human%20handling%20precise%20tasks%29%2C%20and%20full%20autonomy.%20We%20introduce%20a%20novel%20probabilistic%20Dynamical%20System-based%20VF%20for%20coarse%20guidance%2C%20enabling%20the%20robot%20to%20autonomously%20complete%20certain%20task%20phases%20while%20keeping%20the%20human%20operator%20in%20the%20loop.%20For%20tasks%20requiring%20precise%20guidance%2C%20we%20extend%20probabilistic%20position-based%20trajectory%20fixtures%20with%20automation%20allowing%20for%20seamless%20human%20interaction%20as%20well%20as%20geometry-awareness%20and%20optimal%20impedance%20gains.%20For%20manual%20tasks%20requiring%20very%20precise%20guidance%2C%20we%20also%20extend%20visual%20servoing%20fixtures%20with%20the%20same%20geometry-awareness%20and%20impedance%20behavior.%20We%20validate%20our%20approach%20experimentally%20on%20different%20robots%2C%20showcasing%20multiple%20operation%20modes%20and%20the%20ease%20of%20programming%20fixtures.%0ALink%3A%20http%3A//arxiv.org/abs/2506.10239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Framework%2520for%2520Probabilistic%2520Dynamic-%252C%2520Trajectory-%2520and%2520Vision-based%2520Virtual%2520Fixtures%26entry.906535625%3DMaximilian%2520M%25C3%25BChlbauer%2520and%2520Bernhard%2520Weber%2520and%2520Sylvain%2520Calinon%2520and%2520Freek%2520Stulp%2520and%2520Alin%2520Albu-Sch%25C3%25A4ffer%2520and%2520Jo%25C3%25A3o%2520Silv%25C3%25A9rio%26entry.1292438233%3DProbabilistic%2520Virtual%2520Fixtures%2520%2528VFs%2529%2520enable%2520the%2520adaptive%2520selection%2520of%2520the%2520most%2520suitable%2520haptic%2520feedback%2520for%2520each%2520phase%2520of%2520a%2520task%252C%2520based%2520on%2520learned%2520or%2520perceived%2520uncertainty.%2520While%2520keeping%2520the%2520human%2520in%2520the%2520loop%2520remains%2520essential%252C%2520for%2520instance%252C%2520to%2520ensure%2520high%2520precision%252C%2520partial%2520automation%2520of%2520certain%2520task%2520phases%2520is%2520critical%2520for%2520productivity.%2520We%2520present%2520a%2520unified%2520framework%2520for%2520probabilistic%2520VFs%2520that%2520seamlessly%2520switches%2520between%2520manual%2520fixtures%252C%2520semi-automated%2520fixtures%2520%2528with%2520the%2520human%2520handling%2520precise%2520tasks%2529%252C%2520and%2520full%2520autonomy.%2520We%2520introduce%2520a%2520novel%2520probabilistic%2520Dynamical%2520System-based%2520VF%2520for%2520coarse%2520guidance%252C%2520enabling%2520the%2520robot%2520to%2520autonomously%2520complete%2520certain%2520task%2520phases%2520while%2520keeping%2520the%2520human%2520operator%2520in%2520the%2520loop.%2520For%2520tasks%2520requiring%2520precise%2520guidance%252C%2520we%2520extend%2520probabilistic%2520position-based%2520trajectory%2520fixtures%2520with%2520automation%2520allowing%2520for%2520seamless%2520human%2520interaction%2520as%2520well%2520as%2520geometry-awareness%2520and%2520optimal%2520impedance%2520gains.%2520For%2520manual%2520tasks%2520requiring%2520very%2520precise%2520guidance%252C%2520we%2520also%2520extend%2520visual%2520servoing%2520fixtures%2520with%2520the%2520same%2520geometry-awareness%2520and%2520impedance%2520behavior.%2520We%2520validate%2520our%2520approach%2520experimentally%2520on%2520different%2520robots%252C%2520showcasing%2520multiple%2520operation%2520modes%2520and%2520the%2520ease%2520of%2520programming%2520fixtures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20for%20Probabilistic%20Dynamic-%2C%20Trajectory-%20and%20Vision-based%20Virtual%20Fixtures&entry.906535625=Maximilian%20M%C3%BChlbauer%20and%20Bernhard%20Weber%20and%20Sylvain%20Calinon%20and%20Freek%20Stulp%20and%20Alin%20Albu-Sch%C3%A4ffer%20and%20Jo%C3%A3o%20Silv%C3%A9rio&entry.1292438233=Probabilistic%20Virtual%20Fixtures%20%28VFs%29%20enable%20the%20adaptive%20selection%20of%20the%20most%20suitable%20haptic%20feedback%20for%20each%20phase%20of%20a%20task%2C%20based%20on%20learned%20or%20perceived%20uncertainty.%20While%20keeping%20the%20human%20in%20the%20loop%20remains%20essential%2C%20for%20instance%2C%20to%20ensure%20high%20precision%2C%20partial%20automation%20of%20certain%20task%20phases%20is%20critical%20for%20productivity.%20We%20present%20a%20unified%20framework%20for%20probabilistic%20VFs%20that%20seamlessly%20switches%20between%20manual%20fixtures%2C%20semi-automated%20fixtures%20%28with%20the%20human%20handling%20precise%20tasks%29%2C%20and%20full%20autonomy.%20We%20introduce%20a%20novel%20probabilistic%20Dynamical%20System-based%20VF%20for%20coarse%20guidance%2C%20enabling%20the%20robot%20to%20autonomously%20complete%20certain%20task%20phases%20while%20keeping%20the%20human%20operator%20in%20the%20loop.%20For%20tasks%20requiring%20precise%20guidance%2C%20we%20extend%20probabilistic%20position-based%20trajectory%20fixtures%20with%20automation%20allowing%20for%20seamless%20human%20interaction%20as%20well%20as%20geometry-awareness%20and%20optimal%20impedance%20gains.%20For%20manual%20tasks%20requiring%20very%20precise%20guidance%2C%20we%20also%20extend%20visual%20servoing%20fixtures%20with%20the%20same%20geometry-awareness%20and%20impedance%20behavior.%20We%20validate%20our%20approach%20experimentally%20on%20different%20robots%2C%20showcasing%20multiple%20operation%20modes%20and%20the%20ease%20of%20programming%20fixtures.&entry.1838667208=http%3A//arxiv.org/abs/2506.10239v2&entry.124074799=Read"},
{"title": "GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment", "author": "Haoyang He and Jay Patrikar and Dong-Ki Kim and Max Smith and Daniel McGann and Ali-akbar Agha-mohammadi and Shayegan Omidshafiei and Sebastian Scherer", "abstract": "Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.", "link": "http://arxiv.org/abs/2512.01952v1", "date": "2025-12-01", "relevancy": 2.2383, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5713}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5648}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GrndCtrl%3A%20Grounding%20World%20Models%20via%20Self-Supervised%20Reward%20Alignment&body=Title%3A%20GrndCtrl%3A%20Grounding%20World%20Models%20via%20Self-Supervised%20Reward%20Alignment%0AAuthor%3A%20Haoyang%20He%20and%20Jay%20Patrikar%20and%20Dong-Ki%20Kim%20and%20Max%20Smith%20and%20Daniel%20McGann%20and%20Ali-akbar%20Agha-mohammadi%20and%20Shayegan%20Omidshafiei%20and%20Sebastian%20Scherer%0AAbstract%3A%20Recent%20advances%20in%20video%20world%20modeling%20have%20enabled%20large-scale%20generative%20models%20to%20simulate%20embodied%20environments%20with%20high%20visual%20fidelity%2C%20providing%20strong%20priors%20for%20prediction%2C%20planning%2C%20and%20control.%20Yet%2C%20despite%20their%20realism%2C%20these%20models%20often%20lack%20geometric%20grounding%2C%20limiting%20their%20use%20in%20navigation%20tasks%20that%20require%20spatial%20coherence%20and%20long-horizon%20stability.%20We%20introduce%20Reinforcement%20Learning%20with%20World%20Grounding%20%28RLWG%29%2C%20a%20self-supervised%20post-training%20framework%20that%20aligns%20pretrained%20world%20models%20with%20a%20physically%20verifiable%20structure%20through%20geometric%20and%20perceptual%20rewards.%20Analogous%20to%20reinforcement%20learning%20from%20verifiable%20feedback%20%28RLVR%29%20in%20language%20models%2C%20RLWG%20can%20use%20multiple%20rewards%20that%20measure%20pose%20cycle-consistency%2C%20depth%20reprojection%2C%20and%20temporal%20coherence.%20We%20instantiate%20this%20framework%20with%20GrndCtrl%2C%20a%20reward-aligned%20adaptation%20method%20based%20on%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20yielding%20world%20models%20that%20maintain%20stable%20trajectories%2C%20consistent%20geometry%2C%20and%20reliable%20rollouts%20for%20embodied%20navigation.%20Like%20post-training%20alignment%20in%20large%20language%20models%2C%20GrndCtrl%20leverages%20verifiable%20rewards%20to%20bridge%20generative%20pretraining%20and%20grounded%20behavior%2C%20achieving%20superior%20spatial%20coherence%20and%20navigation%20stability%20over%20supervised%20fine-tuning%20in%20outdoor%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrndCtrl%253A%2520Grounding%2520World%2520Models%2520via%2520Self-Supervised%2520Reward%2520Alignment%26entry.906535625%3DHaoyang%2520He%2520and%2520Jay%2520Patrikar%2520and%2520Dong-Ki%2520Kim%2520and%2520Max%2520Smith%2520and%2520Daniel%2520McGann%2520and%2520Ali-akbar%2520Agha-mohammadi%2520and%2520Shayegan%2520Omidshafiei%2520and%2520Sebastian%2520Scherer%26entry.1292438233%3DRecent%2520advances%2520in%2520video%2520world%2520modeling%2520have%2520enabled%2520large-scale%2520generative%2520models%2520to%2520simulate%2520embodied%2520environments%2520with%2520high%2520visual%2520fidelity%252C%2520providing%2520strong%2520priors%2520for%2520prediction%252C%2520planning%252C%2520and%2520control.%2520Yet%252C%2520despite%2520their%2520realism%252C%2520these%2520models%2520often%2520lack%2520geometric%2520grounding%252C%2520limiting%2520their%2520use%2520in%2520navigation%2520tasks%2520that%2520require%2520spatial%2520coherence%2520and%2520long-horizon%2520stability.%2520We%2520introduce%2520Reinforcement%2520Learning%2520with%2520World%2520Grounding%2520%2528RLWG%2529%252C%2520a%2520self-supervised%2520post-training%2520framework%2520that%2520aligns%2520pretrained%2520world%2520models%2520with%2520a%2520physically%2520verifiable%2520structure%2520through%2520geometric%2520and%2520perceptual%2520rewards.%2520Analogous%2520to%2520reinforcement%2520learning%2520from%2520verifiable%2520feedback%2520%2528RLVR%2529%2520in%2520language%2520models%252C%2520RLWG%2520can%2520use%2520multiple%2520rewards%2520that%2520measure%2520pose%2520cycle-consistency%252C%2520depth%2520reprojection%252C%2520and%2520temporal%2520coherence.%2520We%2520instantiate%2520this%2520framework%2520with%2520GrndCtrl%252C%2520a%2520reward-aligned%2520adaptation%2520method%2520based%2520on%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520yielding%2520world%2520models%2520that%2520maintain%2520stable%2520trajectories%252C%2520consistent%2520geometry%252C%2520and%2520reliable%2520rollouts%2520for%2520embodied%2520navigation.%2520Like%2520post-training%2520alignment%2520in%2520large%2520language%2520models%252C%2520GrndCtrl%2520leverages%2520verifiable%2520rewards%2520to%2520bridge%2520generative%2520pretraining%2520and%2520grounded%2520behavior%252C%2520achieving%2520superior%2520spatial%2520coherence%2520and%2520navigation%2520stability%2520over%2520supervised%2520fine-tuning%2520in%2520outdoor%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GrndCtrl%3A%20Grounding%20World%20Models%20via%20Self-Supervised%20Reward%20Alignment&entry.906535625=Haoyang%20He%20and%20Jay%20Patrikar%20and%20Dong-Ki%20Kim%20and%20Max%20Smith%20and%20Daniel%20McGann%20and%20Ali-akbar%20Agha-mohammadi%20and%20Shayegan%20Omidshafiei%20and%20Sebastian%20Scherer&entry.1292438233=Recent%20advances%20in%20video%20world%20modeling%20have%20enabled%20large-scale%20generative%20models%20to%20simulate%20embodied%20environments%20with%20high%20visual%20fidelity%2C%20providing%20strong%20priors%20for%20prediction%2C%20planning%2C%20and%20control.%20Yet%2C%20despite%20their%20realism%2C%20these%20models%20often%20lack%20geometric%20grounding%2C%20limiting%20their%20use%20in%20navigation%20tasks%20that%20require%20spatial%20coherence%20and%20long-horizon%20stability.%20We%20introduce%20Reinforcement%20Learning%20with%20World%20Grounding%20%28RLWG%29%2C%20a%20self-supervised%20post-training%20framework%20that%20aligns%20pretrained%20world%20models%20with%20a%20physically%20verifiable%20structure%20through%20geometric%20and%20perceptual%20rewards.%20Analogous%20to%20reinforcement%20learning%20from%20verifiable%20feedback%20%28RLVR%29%20in%20language%20models%2C%20RLWG%20can%20use%20multiple%20rewards%20that%20measure%20pose%20cycle-consistency%2C%20depth%20reprojection%2C%20and%20temporal%20coherence.%20We%20instantiate%20this%20framework%20with%20GrndCtrl%2C%20a%20reward-aligned%20adaptation%20method%20based%20on%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20yielding%20world%20models%20that%20maintain%20stable%20trajectories%2C%20consistent%20geometry%2C%20and%20reliable%20rollouts%20for%20embodied%20navigation.%20Like%20post-training%20alignment%20in%20large%20language%20models%2C%20GrndCtrl%20leverages%20verifiable%20rewards%20to%20bridge%20generative%20pretraining%20and%20grounded%20behavior%2C%20achieving%20superior%20spatial%20coherence%20and%20navigation%20stability%20over%20supervised%20fine-tuning%20in%20outdoor%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.01952v1&entry.124074799=Read"},
{"title": "StyleYourSmile: Cross-Domain Face Retargeting Without Paired Multi-Style Data", "author": "Avirup Dey and Vinay Namboodiri", "abstract": "Cross-domain face retargeting requires disentangled control over identity, expressions, and domain-specific stylistic attributes. Existing methods, typically trained on real-world faces, either fail to generalize across domains, need test-time optimizations, or require fine-tuning with carefully curated multi-style datasets to achieve domain-invariant identity representations. In this work, we introduce \\textit{StyleYourSmile}, a novel one-shot cross-domain face retargeting method that eliminates the need for curated multi-style paired data. We propose an efficient data augmentation strategy alongside a dual-encoder framework, for extracting domain-invariant identity cues and capturing domain-specific stylistic variations. Leveraging these disentangled control signals, we condition a diffusion model to retarget facial expressions across domains. Extensive experiments demonstrate that \\textit{StyleYourSmile} achieves superior identity preservation and retargeting fidelity across a wide range of visual domains.", "link": "http://arxiv.org/abs/2512.01895v1", "date": "2025-12-01", "relevancy": 2.2366, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5643}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5557}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StyleYourSmile%3A%20Cross-Domain%20Face%20Retargeting%20Without%20Paired%20Multi-Style%20Data&body=Title%3A%20StyleYourSmile%3A%20Cross-Domain%20Face%20Retargeting%20Without%20Paired%20Multi-Style%20Data%0AAuthor%3A%20Avirup%20Dey%20and%20Vinay%20Namboodiri%0AAbstract%3A%20Cross-domain%20face%20retargeting%20requires%20disentangled%20control%20over%20identity%2C%20expressions%2C%20and%20domain-specific%20stylistic%20attributes.%20Existing%20methods%2C%20typically%20trained%20on%20real-world%20faces%2C%20either%20fail%20to%20generalize%20across%20domains%2C%20need%20test-time%20optimizations%2C%20or%20require%20fine-tuning%20with%20carefully%20curated%20multi-style%20datasets%20to%20achieve%20domain-invariant%20identity%20representations.%20In%20this%20work%2C%20we%20introduce%20%5Ctextit%7BStyleYourSmile%7D%2C%20a%20novel%20one-shot%20cross-domain%20face%20retargeting%20method%20that%20eliminates%20the%20need%20for%20curated%20multi-style%20paired%20data.%20We%20propose%20an%20efficient%20data%20augmentation%20strategy%20alongside%20a%20dual-encoder%20framework%2C%20for%20extracting%20domain-invariant%20identity%20cues%20and%20capturing%20domain-specific%20stylistic%20variations.%20Leveraging%20these%20disentangled%20control%20signals%2C%20we%20condition%20a%20diffusion%20model%20to%20retarget%20facial%20expressions%20across%20domains.%20Extensive%20experiments%20demonstrate%20that%20%5Ctextit%7BStyleYourSmile%7D%20achieves%20superior%20identity%20preservation%20and%20retargeting%20fidelity%20across%20a%20wide%20range%20of%20visual%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyleYourSmile%253A%2520Cross-Domain%2520Face%2520Retargeting%2520Without%2520Paired%2520Multi-Style%2520Data%26entry.906535625%3DAvirup%2520Dey%2520and%2520Vinay%2520Namboodiri%26entry.1292438233%3DCross-domain%2520face%2520retargeting%2520requires%2520disentangled%2520control%2520over%2520identity%252C%2520expressions%252C%2520and%2520domain-specific%2520stylistic%2520attributes.%2520Existing%2520methods%252C%2520typically%2520trained%2520on%2520real-world%2520faces%252C%2520either%2520fail%2520to%2520generalize%2520across%2520domains%252C%2520need%2520test-time%2520optimizations%252C%2520or%2520require%2520fine-tuning%2520with%2520carefully%2520curated%2520multi-style%2520datasets%2520to%2520achieve%2520domain-invariant%2520identity%2520representations.%2520In%2520this%2520work%252C%2520we%2520introduce%2520%255Ctextit%257BStyleYourSmile%257D%252C%2520a%2520novel%2520one-shot%2520cross-domain%2520face%2520retargeting%2520method%2520that%2520eliminates%2520the%2520need%2520for%2520curated%2520multi-style%2520paired%2520data.%2520We%2520propose%2520an%2520efficient%2520data%2520augmentation%2520strategy%2520alongside%2520a%2520dual-encoder%2520framework%252C%2520for%2520extracting%2520domain-invariant%2520identity%2520cues%2520and%2520capturing%2520domain-specific%2520stylistic%2520variations.%2520Leveraging%2520these%2520disentangled%2520control%2520signals%252C%2520we%2520condition%2520a%2520diffusion%2520model%2520to%2520retarget%2520facial%2520expressions%2520across%2520domains.%2520Extensive%2520experiments%2520demonstrate%2520that%2520%255Ctextit%257BStyleYourSmile%257D%2520achieves%2520superior%2520identity%2520preservation%2520and%2520retargeting%2520fidelity%2520across%2520a%2520wide%2520range%2520of%2520visual%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StyleYourSmile%3A%20Cross-Domain%20Face%20Retargeting%20Without%20Paired%20Multi-Style%20Data&entry.906535625=Avirup%20Dey%20and%20Vinay%20Namboodiri&entry.1292438233=Cross-domain%20face%20retargeting%20requires%20disentangled%20control%20over%20identity%2C%20expressions%2C%20and%20domain-specific%20stylistic%20attributes.%20Existing%20methods%2C%20typically%20trained%20on%20real-world%20faces%2C%20either%20fail%20to%20generalize%20across%20domains%2C%20need%20test-time%20optimizations%2C%20or%20require%20fine-tuning%20with%20carefully%20curated%20multi-style%20datasets%20to%20achieve%20domain-invariant%20identity%20representations.%20In%20this%20work%2C%20we%20introduce%20%5Ctextit%7BStyleYourSmile%7D%2C%20a%20novel%20one-shot%20cross-domain%20face%20retargeting%20method%20that%20eliminates%20the%20need%20for%20curated%20multi-style%20paired%20data.%20We%20propose%20an%20efficient%20data%20augmentation%20strategy%20alongside%20a%20dual-encoder%20framework%2C%20for%20extracting%20domain-invariant%20identity%20cues%20and%20capturing%20domain-specific%20stylistic%20variations.%20Leveraging%20these%20disentangled%20control%20signals%2C%20we%20condition%20a%20diffusion%20model%20to%20retarget%20facial%20expressions%20across%20domains.%20Extensive%20experiments%20demonstrate%20that%20%5Ctextit%7BStyleYourSmile%7D%20achieves%20superior%20identity%20preservation%20and%20retargeting%20fidelity%20across%20a%20wide%20range%20of%20visual%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2512.01895v1&entry.124074799=Read"},
{"title": "TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction", "author": "Yukuo Ma and Cong Liu and Junke Wang and Junqi Liu and Haibin Huang and Zuxuan Wu and Chi Zhang and Xuelong Li", "abstract": "We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.", "link": "http://arxiv.org/abs/2511.12578v2", "date": "2025-12-01", "relevancy": 2.2036, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5627}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5504}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TempoMaster%3A%20Efficient%20Long%20Video%20Generation%20via%20Next-Frame-Rate%20Prediction&body=Title%3A%20TempoMaster%3A%20Efficient%20Long%20Video%20Generation%20via%20Next-Frame-Rate%20Prediction%0AAuthor%3A%20Yukuo%20Ma%20and%20Cong%20Liu%20and%20Junke%20Wang%20and%20Junqi%20Liu%20and%20Haibin%20Huang%20and%20Zuxuan%20Wu%20and%20Chi%20Zhang%20and%20Xuelong%20Li%0AAbstract%3A%20We%20present%20TempoMaster%2C%20a%20novel%20framework%20that%20formulates%20long%20video%20generation%20as%20next-frame-rate%20prediction.%20Specifically%2C%20we%20first%20generate%20a%20low-frame-rate%20clip%20that%20serves%20as%20a%20coarse%20blueprint%20of%20the%20entire%20video%20sequence%2C%20and%20then%20progressively%20increase%20the%20frame%20rate%20to%20refine%20visual%20details%20and%20motion%20continuity.%20During%20generation%2C%20TempoMaster%20employs%20bidirectional%20attention%20within%20each%20frame-rate%20level%20while%20performing%20autoregression%20across%20frame%20rates%2C%20thus%20achieving%20long-range%20temporal%20coherence%20while%20enabling%20efficient%20and%20parallel%20synthesis.%20Extensive%20experiments%20demonstrate%20that%20TempoMaster%20establishes%20a%20new%20state-of-the-art%20in%20long%20video%20generation%2C%20excelling%20in%20both%20visual%20and%20temporal%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12578v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempoMaster%253A%2520Efficient%2520Long%2520Video%2520Generation%2520via%2520Next-Frame-Rate%2520Prediction%26entry.906535625%3DYukuo%2520Ma%2520and%2520Cong%2520Liu%2520and%2520Junke%2520Wang%2520and%2520Junqi%2520Liu%2520and%2520Haibin%2520Huang%2520and%2520Zuxuan%2520Wu%2520and%2520Chi%2520Zhang%2520and%2520Xuelong%2520Li%26entry.1292438233%3DWe%2520present%2520TempoMaster%252C%2520a%2520novel%2520framework%2520that%2520formulates%2520long%2520video%2520generation%2520as%2520next-frame-rate%2520prediction.%2520Specifically%252C%2520we%2520first%2520generate%2520a%2520low-frame-rate%2520clip%2520that%2520serves%2520as%2520a%2520coarse%2520blueprint%2520of%2520the%2520entire%2520video%2520sequence%252C%2520and%2520then%2520progressively%2520increase%2520the%2520frame%2520rate%2520to%2520refine%2520visual%2520details%2520and%2520motion%2520continuity.%2520During%2520generation%252C%2520TempoMaster%2520employs%2520bidirectional%2520attention%2520within%2520each%2520frame-rate%2520level%2520while%2520performing%2520autoregression%2520across%2520frame%2520rates%252C%2520thus%2520achieving%2520long-range%2520temporal%2520coherence%2520while%2520enabling%2520efficient%2520and%2520parallel%2520synthesis.%2520Extensive%2520experiments%2520demonstrate%2520that%2520TempoMaster%2520establishes%2520a%2520new%2520state-of-the-art%2520in%2520long%2520video%2520generation%252C%2520excelling%2520in%2520both%2520visual%2520and%2520temporal%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12578v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TempoMaster%3A%20Efficient%20Long%20Video%20Generation%20via%20Next-Frame-Rate%20Prediction&entry.906535625=Yukuo%20Ma%20and%20Cong%20Liu%20and%20Junke%20Wang%20and%20Junqi%20Liu%20and%20Haibin%20Huang%20and%20Zuxuan%20Wu%20and%20Chi%20Zhang%20and%20Xuelong%20Li&entry.1292438233=We%20present%20TempoMaster%2C%20a%20novel%20framework%20that%20formulates%20long%20video%20generation%20as%20next-frame-rate%20prediction.%20Specifically%2C%20we%20first%20generate%20a%20low-frame-rate%20clip%20that%20serves%20as%20a%20coarse%20blueprint%20of%20the%20entire%20video%20sequence%2C%20and%20then%20progressively%20increase%20the%20frame%20rate%20to%20refine%20visual%20details%20and%20motion%20continuity.%20During%20generation%2C%20TempoMaster%20employs%20bidirectional%20attention%20within%20each%20frame-rate%20level%20while%20performing%20autoregression%20across%20frame%20rates%2C%20thus%20achieving%20long-range%20temporal%20coherence%20while%20enabling%20efficient%20and%20parallel%20synthesis.%20Extensive%20experiments%20demonstrate%20that%20TempoMaster%20establishes%20a%20new%20state-of-the-art%20in%20long%20video%20generation%2C%20excelling%20in%20both%20visual%20and%20temporal%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2511.12578v2&entry.124074799=Read"},
{"title": "Is Image-based Object Pose Estimation Ready to Support Grasping?", "author": "Eric C. Joyce and Qianwen Zhao and Nathaniel Burgdorfer and Long Wang and Philippos Mordohai", "abstract": "We present a framework for evaluating 6-DoF instance-level object pose estimators, focusing on those that require a single RGB (not RGB-D) image as input. Besides gaining intuition about how accurate these estimators are, we are interested in the degree to which they can serve as the sole perception mechanism for robotic grasping. To assess this, we perform grasping trials in a physics-based simulator, using image-based pose estimates to guide a parallel gripper and an underactuated robotic hand in picking up 3D models of objects. Our experiments on a subset of the BOP (Benchmark for 6D Object Pose Estimation) dataset compare five open-source object pose estimators and provide insights that were missing from the literature.", "link": "http://arxiv.org/abs/2512.01856v1", "date": "2025-12-01", "relevancy": 2.201, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5485}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Image-based%20Object%20Pose%20Estimation%20Ready%20to%20Support%20Grasping%3F&body=Title%3A%20Is%20Image-based%20Object%20Pose%20Estimation%20Ready%20to%20Support%20Grasping%3F%0AAuthor%3A%20Eric%20C.%20Joyce%20and%20Qianwen%20Zhao%20and%20Nathaniel%20Burgdorfer%20and%20Long%20Wang%20and%20Philippos%20Mordohai%0AAbstract%3A%20We%20present%20a%20framework%20for%20evaluating%206-DoF%20instance-level%20object%20pose%20estimators%2C%20focusing%20on%20those%20that%20require%20a%20single%20RGB%20%28not%20RGB-D%29%20image%20as%20input.%20Besides%20gaining%20intuition%20about%20how%20accurate%20these%20estimators%20are%2C%20we%20are%20interested%20in%20the%20degree%20to%20which%20they%20can%20serve%20as%20the%20sole%20perception%20mechanism%20for%20robotic%20grasping.%20To%20assess%20this%2C%20we%20perform%20grasping%20trials%20in%20a%20physics-based%20simulator%2C%20using%20image-based%20pose%20estimates%20to%20guide%20a%20parallel%20gripper%20and%20an%20underactuated%20robotic%20hand%20in%20picking%20up%203D%20models%20of%20objects.%20Our%20experiments%20on%20a%20subset%20of%20the%20BOP%20%28Benchmark%20for%206D%20Object%20Pose%20Estimation%29%20dataset%20compare%20five%20open-source%20object%20pose%20estimators%20and%20provide%20insights%20that%20were%20missing%20from%20the%20literature.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Image-based%2520Object%2520Pose%2520Estimation%2520Ready%2520to%2520Support%2520Grasping%253F%26entry.906535625%3DEric%2520C.%2520Joyce%2520and%2520Qianwen%2520Zhao%2520and%2520Nathaniel%2520Burgdorfer%2520and%2520Long%2520Wang%2520and%2520Philippos%2520Mordohai%26entry.1292438233%3DWe%2520present%2520a%2520framework%2520for%2520evaluating%25206-DoF%2520instance-level%2520object%2520pose%2520estimators%252C%2520focusing%2520on%2520those%2520that%2520require%2520a%2520single%2520RGB%2520%2528not%2520RGB-D%2529%2520image%2520as%2520input.%2520Besides%2520gaining%2520intuition%2520about%2520how%2520accurate%2520these%2520estimators%2520are%252C%2520we%2520are%2520interested%2520in%2520the%2520degree%2520to%2520which%2520they%2520can%2520serve%2520as%2520the%2520sole%2520perception%2520mechanism%2520for%2520robotic%2520grasping.%2520To%2520assess%2520this%252C%2520we%2520perform%2520grasping%2520trials%2520in%2520a%2520physics-based%2520simulator%252C%2520using%2520image-based%2520pose%2520estimates%2520to%2520guide%2520a%2520parallel%2520gripper%2520and%2520an%2520underactuated%2520robotic%2520hand%2520in%2520picking%2520up%25203D%2520models%2520of%2520objects.%2520Our%2520experiments%2520on%2520a%2520subset%2520of%2520the%2520BOP%2520%2528Benchmark%2520for%25206D%2520Object%2520Pose%2520Estimation%2529%2520dataset%2520compare%2520five%2520open-source%2520object%2520pose%2520estimators%2520and%2520provide%2520insights%2520that%2520were%2520missing%2520from%2520the%2520literature.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Image-based%20Object%20Pose%20Estimation%20Ready%20to%20Support%20Grasping%3F&entry.906535625=Eric%20C.%20Joyce%20and%20Qianwen%20Zhao%20and%20Nathaniel%20Burgdorfer%20and%20Long%20Wang%20and%20Philippos%20Mordohai&entry.1292438233=We%20present%20a%20framework%20for%20evaluating%206-DoF%20instance-level%20object%20pose%20estimators%2C%20focusing%20on%20those%20that%20require%20a%20single%20RGB%20%28not%20RGB-D%29%20image%20as%20input.%20Besides%20gaining%20intuition%20about%20how%20accurate%20these%20estimators%20are%2C%20we%20are%20interested%20in%20the%20degree%20to%20which%20they%20can%20serve%20as%20the%20sole%20perception%20mechanism%20for%20robotic%20grasping.%20To%20assess%20this%2C%20we%20perform%20grasping%20trials%20in%20a%20physics-based%20simulator%2C%20using%20image-based%20pose%20estimates%20to%20guide%20a%20parallel%20gripper%20and%20an%20underactuated%20robotic%20hand%20in%20picking%20up%203D%20models%20of%20objects.%20Our%20experiments%20on%20a%20subset%20of%20the%20BOP%20%28Benchmark%20for%206D%20Object%20Pose%20Estimation%29%20dataset%20compare%20five%20open-source%20object%20pose%20estimators%20and%20provide%20insights%20that%20were%20missing%20from%20the%20literature.&entry.1838667208=http%3A//arxiv.org/abs/2512.01856v1&entry.124074799=Read"},
{"title": "Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding", "author": "Zahra Mahdavi and Zahra Khodakaramimaghsoud and Hooman Khaloo and Sina Bakhshandeh Taleshani and Erfan Hashemi and Javad Mirzapour Kaleybar and Omid Nejati Manzari", "abstract": "Large vision-language models (LVLMs) are now central to healthcare applications such as medical visual question answering and imaging report generation. Yet, these models remain vulnerable to hallucination outputs that appear plausible but are in fact incorrect. In the natural image domain, several decoding strategies have been proposed to mitigate hallucinations by reinforcing visual evidence, but most rely on secondary decoding or rollback procedures that substantially slow inference. Moreover, existing solutions are often domain-specific and may introduce misalignment between modalities or between generated and ground-truth content. We introduce Med-VCD, a sparse visual-contrastive decoding method that mitigates hallucinations in medical LVLMs without the time overhead of secondary decoding. Med-VCD incorporates a novel token-sparsification strategy that selects visually informed tokens on the fly, trimming redundancy while retaining critical visual context and thus balancing efficiency with reliability. Evaluations on eight medical datasets, spanning ophthalmology, radiology, and pathology tasks in visual question answering, report generation, and dedicated hallucination benchmarks, show that Med-VCD raises factual accuracy by an average of 13\\% and improves hallucination accuracy by 6\\% relative to baseline medical LVLMs.", "link": "http://arxiv.org/abs/2512.01922v1", "date": "2025-12-01", "relevancy": 2.1978, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Med-VCD%3A%20Mitigating%20Hallucination%20for%20Medical%20Large%20Vision%20Language%20Models%20through%20Visual%20Contrastive%20Decoding&body=Title%3A%20Med-VCD%3A%20Mitigating%20Hallucination%20for%20Medical%20Large%20Vision%20Language%20Models%20through%20Visual%20Contrastive%20Decoding%0AAuthor%3A%20Zahra%20Mahdavi%20and%20Zahra%20Khodakaramimaghsoud%20and%20Hooman%20Khaloo%20and%20Sina%20Bakhshandeh%20Taleshani%20and%20Erfan%20Hashemi%20and%20Javad%20Mirzapour%20Kaleybar%20and%20Omid%20Nejati%20Manzari%0AAbstract%3A%20Large%20vision-language%20models%20%28LVLMs%29%20are%20now%20central%20to%20healthcare%20applications%20such%20as%20medical%20visual%20question%20answering%20and%20imaging%20report%20generation.%20Yet%2C%20these%20models%20remain%20vulnerable%20to%20hallucination%20outputs%20that%20appear%20plausible%20but%20are%20in%20fact%20incorrect.%20In%20the%20natural%20image%20domain%2C%20several%20decoding%20strategies%20have%20been%20proposed%20to%20mitigate%20hallucinations%20by%20reinforcing%20visual%20evidence%2C%20but%20most%20rely%20on%20secondary%20decoding%20or%20rollback%20procedures%20that%20substantially%20slow%20inference.%20Moreover%2C%20existing%20solutions%20are%20often%20domain-specific%20and%20may%20introduce%20misalignment%20between%20modalities%20or%20between%20generated%20and%20ground-truth%20content.%20We%20introduce%20Med-VCD%2C%20a%20sparse%20visual-contrastive%20decoding%20method%20that%20mitigates%20hallucinations%20in%20medical%20LVLMs%20without%20the%20time%20overhead%20of%20secondary%20decoding.%20Med-VCD%20incorporates%20a%20novel%20token-sparsification%20strategy%20that%20selects%20visually%20informed%20tokens%20on%20the%20fly%2C%20trimming%20redundancy%20while%20retaining%20critical%20visual%20context%20and%20thus%20balancing%20efficiency%20with%20reliability.%20Evaluations%20on%20eight%20medical%20datasets%2C%20spanning%20ophthalmology%2C%20radiology%2C%20and%20pathology%20tasks%20in%20visual%20question%20answering%2C%20report%20generation%2C%20and%20dedicated%20hallucination%20benchmarks%2C%20show%20that%20Med-VCD%20raises%20factual%20accuracy%20by%20an%20average%20of%2013%5C%25%20and%20improves%20hallucination%20accuracy%20by%206%5C%25%20relative%20to%20baseline%20medical%20LVLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMed-VCD%253A%2520Mitigating%2520Hallucination%2520for%2520Medical%2520Large%2520Vision%2520Language%2520Models%2520through%2520Visual%2520Contrastive%2520Decoding%26entry.906535625%3DZahra%2520Mahdavi%2520and%2520Zahra%2520Khodakaramimaghsoud%2520and%2520Hooman%2520Khaloo%2520and%2520Sina%2520Bakhshandeh%2520Taleshani%2520and%2520Erfan%2520Hashemi%2520and%2520Javad%2520Mirzapour%2520Kaleybar%2520and%2520Omid%2520Nejati%2520Manzari%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528LVLMs%2529%2520are%2520now%2520central%2520to%2520healthcare%2520applications%2520such%2520as%2520medical%2520visual%2520question%2520answering%2520and%2520imaging%2520report%2520generation.%2520Yet%252C%2520these%2520models%2520remain%2520vulnerable%2520to%2520hallucination%2520outputs%2520that%2520appear%2520plausible%2520but%2520are%2520in%2520fact%2520incorrect.%2520In%2520the%2520natural%2520image%2520domain%252C%2520several%2520decoding%2520strategies%2520have%2520been%2520proposed%2520to%2520mitigate%2520hallucinations%2520by%2520reinforcing%2520visual%2520evidence%252C%2520but%2520most%2520rely%2520on%2520secondary%2520decoding%2520or%2520rollback%2520procedures%2520that%2520substantially%2520slow%2520inference.%2520Moreover%252C%2520existing%2520solutions%2520are%2520often%2520domain-specific%2520and%2520may%2520introduce%2520misalignment%2520between%2520modalities%2520or%2520between%2520generated%2520and%2520ground-truth%2520content.%2520We%2520introduce%2520Med-VCD%252C%2520a%2520sparse%2520visual-contrastive%2520decoding%2520method%2520that%2520mitigates%2520hallucinations%2520in%2520medical%2520LVLMs%2520without%2520the%2520time%2520overhead%2520of%2520secondary%2520decoding.%2520Med-VCD%2520incorporates%2520a%2520novel%2520token-sparsification%2520strategy%2520that%2520selects%2520visually%2520informed%2520tokens%2520on%2520the%2520fly%252C%2520trimming%2520redundancy%2520while%2520retaining%2520critical%2520visual%2520context%2520and%2520thus%2520balancing%2520efficiency%2520with%2520reliability.%2520Evaluations%2520on%2520eight%2520medical%2520datasets%252C%2520spanning%2520ophthalmology%252C%2520radiology%252C%2520and%2520pathology%2520tasks%2520in%2520visual%2520question%2520answering%252C%2520report%2520generation%252C%2520and%2520dedicated%2520hallucination%2520benchmarks%252C%2520show%2520that%2520Med-VCD%2520raises%2520factual%2520accuracy%2520by%2520an%2520average%2520of%252013%255C%2525%2520and%2520improves%2520hallucination%2520accuracy%2520by%25206%255C%2525%2520relative%2520to%2520baseline%2520medical%2520LVLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Med-VCD%3A%20Mitigating%20Hallucination%20for%20Medical%20Large%20Vision%20Language%20Models%20through%20Visual%20Contrastive%20Decoding&entry.906535625=Zahra%20Mahdavi%20and%20Zahra%20Khodakaramimaghsoud%20and%20Hooman%20Khaloo%20and%20Sina%20Bakhshandeh%20Taleshani%20and%20Erfan%20Hashemi%20and%20Javad%20Mirzapour%20Kaleybar%20and%20Omid%20Nejati%20Manzari&entry.1292438233=Large%20vision-language%20models%20%28LVLMs%29%20are%20now%20central%20to%20healthcare%20applications%20such%20as%20medical%20visual%20question%20answering%20and%20imaging%20report%20generation.%20Yet%2C%20these%20models%20remain%20vulnerable%20to%20hallucination%20outputs%20that%20appear%20plausible%20but%20are%20in%20fact%20incorrect.%20In%20the%20natural%20image%20domain%2C%20several%20decoding%20strategies%20have%20been%20proposed%20to%20mitigate%20hallucinations%20by%20reinforcing%20visual%20evidence%2C%20but%20most%20rely%20on%20secondary%20decoding%20or%20rollback%20procedures%20that%20substantially%20slow%20inference.%20Moreover%2C%20existing%20solutions%20are%20often%20domain-specific%20and%20may%20introduce%20misalignment%20between%20modalities%20or%20between%20generated%20and%20ground-truth%20content.%20We%20introduce%20Med-VCD%2C%20a%20sparse%20visual-contrastive%20decoding%20method%20that%20mitigates%20hallucinations%20in%20medical%20LVLMs%20without%20the%20time%20overhead%20of%20secondary%20decoding.%20Med-VCD%20incorporates%20a%20novel%20token-sparsification%20strategy%20that%20selects%20visually%20informed%20tokens%20on%20the%20fly%2C%20trimming%20redundancy%20while%20retaining%20critical%20visual%20context%20and%20thus%20balancing%20efficiency%20with%20reliability.%20Evaluations%20on%20eight%20medical%20datasets%2C%20spanning%20ophthalmology%2C%20radiology%2C%20and%20pathology%20tasks%20in%20visual%20question%20answering%2C%20report%20generation%2C%20and%20dedicated%20hallucination%20benchmarks%2C%20show%20that%20Med-VCD%20raises%20factual%20accuracy%20by%20an%20average%20of%2013%5C%25%20and%20improves%20hallucination%20accuracy%20by%206%5C%25%20relative%20to%20baseline%20medical%20LVLMs.&entry.1838667208=http%3A//arxiv.org/abs/2512.01922v1&entry.124074799=Read"},
{"title": "AgriLiRa4D: A Multi-Sensor UAV Dataset for Robust SLAM in Challenging Agricultural Fields", "author": "Zhihao Zhan and Yuhang Ming and Shaobin Li and Jie Yuan", "abstract": "Multi-sensor Simultaneous Localization and Mapping (SLAM) is essential for Unmanned Aerial Vehicles (UAVs) performing agricultural tasks such as spraying, surveying, and inspection. However, real-world, multi-modal agricultural UAV datasets that enable research on robust operation remain scarce. To address this gap, we present AgriLiRa4D, a multi-modal UAV dataset designed for challenging outdoor agricultural environments. AgriLiRa4D spans three representative farmland types-flat, hilly, and terraced-and includes both boundary and coverage operation modes, resulting in six flight sequence groups. The dataset provides high-accuracy ground-truth trajectories from a Fiber Optic Inertial Navigation System with Real-Time Kinematic capability (FINS_RTK), along with synchronized measurements from a 3D LiDAR, a 4D Radar, and an Inertial Measurement Unit (IMU), accompanied by complete intrinsic and extrinsic calibrations. Leveraging its comprehensive sensor suite and diverse real-world scenarios, AgriLiRa4D supports diverse SLAM and localization studies and enables rigorous robustness evaluation against low-texture crops, repetitive patterns, dynamic vegetation, and other challenges of real agricultural environments. To further demonstrate its utility, we benchmark four state-of-the-art multi-sensor SLAM algorithms across different sensor combinations, highlighting the difficulty of the proposed sequences and the necessity of multi-modal approaches for reliable UAV localization. By filling a critical gap in agricultural SLAM datasets, AgriLiRa4D provides a valuable benchmark for the research community and contributes to advancing autonomous navigation technologies for agricultural UAVs. The dataset can be downloaded from: https://zhan994.github.io/AgriLiRa4D.", "link": "http://arxiv.org/abs/2512.01753v1", "date": "2025-12-01", "relevancy": 2.1915, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5778}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5297}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgriLiRa4D%3A%20A%20Multi-Sensor%20UAV%20Dataset%20for%20Robust%20SLAM%20in%20Challenging%20Agricultural%20Fields&body=Title%3A%20AgriLiRa4D%3A%20A%20Multi-Sensor%20UAV%20Dataset%20for%20Robust%20SLAM%20in%20Challenging%20Agricultural%20Fields%0AAuthor%3A%20Zhihao%20Zhan%20and%20Yuhang%20Ming%20and%20Shaobin%20Li%20and%20Jie%20Yuan%0AAbstract%3A%20Multi-sensor%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20essential%20for%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20performing%20agricultural%20tasks%20such%20as%20spraying%2C%20surveying%2C%20and%20inspection.%20However%2C%20real-world%2C%20multi-modal%20agricultural%20UAV%20datasets%20that%20enable%20research%20on%20robust%20operation%20remain%20scarce.%20To%20address%20this%20gap%2C%20we%20present%20AgriLiRa4D%2C%20a%20multi-modal%20UAV%20dataset%20designed%20for%20challenging%20outdoor%20agricultural%20environments.%20AgriLiRa4D%20spans%20three%20representative%20farmland%20types-flat%2C%20hilly%2C%20and%20terraced-and%20includes%20both%20boundary%20and%20coverage%20operation%20modes%2C%20resulting%20in%20six%20flight%20sequence%20groups.%20The%20dataset%20provides%20high-accuracy%20ground-truth%20trajectories%20from%20a%20Fiber%20Optic%20Inertial%20Navigation%20System%20with%20Real-Time%20Kinematic%20capability%20%28FINS_RTK%29%2C%20along%20with%20synchronized%20measurements%20from%20a%203D%20LiDAR%2C%20a%204D%20Radar%2C%20and%20an%20Inertial%20Measurement%20Unit%20%28IMU%29%2C%20accompanied%20by%20complete%20intrinsic%20and%20extrinsic%20calibrations.%20Leveraging%20its%20comprehensive%20sensor%20suite%20and%20diverse%20real-world%20scenarios%2C%20AgriLiRa4D%20supports%20diverse%20SLAM%20and%20localization%20studies%20and%20enables%20rigorous%20robustness%20evaluation%20against%20low-texture%20crops%2C%20repetitive%20patterns%2C%20dynamic%20vegetation%2C%20and%20other%20challenges%20of%20real%20agricultural%20environments.%20To%20further%20demonstrate%20its%20utility%2C%20we%20benchmark%20four%20state-of-the-art%20multi-sensor%20SLAM%20algorithms%20across%20different%20sensor%20combinations%2C%20highlighting%20the%20difficulty%20of%20the%20proposed%20sequences%20and%20the%20necessity%20of%20multi-modal%20approaches%20for%20reliable%20UAV%20localization.%20By%20filling%20a%20critical%20gap%20in%20agricultural%20SLAM%20datasets%2C%20AgriLiRa4D%20provides%20a%20valuable%20benchmark%20for%20the%20research%20community%20and%20contributes%20to%20advancing%20autonomous%20navigation%20technologies%20for%20agricultural%20UAVs.%20The%20dataset%20can%20be%20downloaded%20from%3A%20https%3A//zhan994.github.io/AgriLiRa4D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgriLiRa4D%253A%2520A%2520Multi-Sensor%2520UAV%2520Dataset%2520for%2520Robust%2520SLAM%2520in%2520Challenging%2520Agricultural%2520Fields%26entry.906535625%3DZhihao%2520Zhan%2520and%2520Yuhang%2520Ming%2520and%2520Shaobin%2520Li%2520and%2520Jie%2520Yuan%26entry.1292438233%3DMulti-sensor%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520is%2520essential%2520for%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520performing%2520agricultural%2520tasks%2520such%2520as%2520spraying%252C%2520surveying%252C%2520and%2520inspection.%2520However%252C%2520real-world%252C%2520multi-modal%2520agricultural%2520UAV%2520datasets%2520that%2520enable%2520research%2520on%2520robust%2520operation%2520remain%2520scarce.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520AgriLiRa4D%252C%2520a%2520multi-modal%2520UAV%2520dataset%2520designed%2520for%2520challenging%2520outdoor%2520agricultural%2520environments.%2520AgriLiRa4D%2520spans%2520three%2520representative%2520farmland%2520types-flat%252C%2520hilly%252C%2520and%2520terraced-and%2520includes%2520both%2520boundary%2520and%2520coverage%2520operation%2520modes%252C%2520resulting%2520in%2520six%2520flight%2520sequence%2520groups.%2520The%2520dataset%2520provides%2520high-accuracy%2520ground-truth%2520trajectories%2520from%2520a%2520Fiber%2520Optic%2520Inertial%2520Navigation%2520System%2520with%2520Real-Time%2520Kinematic%2520capability%2520%2528FINS_RTK%2529%252C%2520along%2520with%2520synchronized%2520measurements%2520from%2520a%25203D%2520LiDAR%252C%2520a%25204D%2520Radar%252C%2520and%2520an%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%252C%2520accompanied%2520by%2520complete%2520intrinsic%2520and%2520extrinsic%2520calibrations.%2520Leveraging%2520its%2520comprehensive%2520sensor%2520suite%2520and%2520diverse%2520real-world%2520scenarios%252C%2520AgriLiRa4D%2520supports%2520diverse%2520SLAM%2520and%2520localization%2520studies%2520and%2520enables%2520rigorous%2520robustness%2520evaluation%2520against%2520low-texture%2520crops%252C%2520repetitive%2520patterns%252C%2520dynamic%2520vegetation%252C%2520and%2520other%2520challenges%2520of%2520real%2520agricultural%2520environments.%2520To%2520further%2520demonstrate%2520its%2520utility%252C%2520we%2520benchmark%2520four%2520state-of-the-art%2520multi-sensor%2520SLAM%2520algorithms%2520across%2520different%2520sensor%2520combinations%252C%2520highlighting%2520the%2520difficulty%2520of%2520the%2520proposed%2520sequences%2520and%2520the%2520necessity%2520of%2520multi-modal%2520approaches%2520for%2520reliable%2520UAV%2520localization.%2520By%2520filling%2520a%2520critical%2520gap%2520in%2520agricultural%2520SLAM%2520datasets%252C%2520AgriLiRa4D%2520provides%2520a%2520valuable%2520benchmark%2520for%2520the%2520research%2520community%2520and%2520contributes%2520to%2520advancing%2520autonomous%2520navigation%2520technologies%2520for%2520agricultural%2520UAVs.%2520The%2520dataset%2520can%2520be%2520downloaded%2520from%253A%2520https%253A//zhan994.github.io/AgriLiRa4D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgriLiRa4D%3A%20A%20Multi-Sensor%20UAV%20Dataset%20for%20Robust%20SLAM%20in%20Challenging%20Agricultural%20Fields&entry.906535625=Zhihao%20Zhan%20and%20Yuhang%20Ming%20and%20Shaobin%20Li%20and%20Jie%20Yuan&entry.1292438233=Multi-sensor%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20essential%20for%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20performing%20agricultural%20tasks%20such%20as%20spraying%2C%20surveying%2C%20and%20inspection.%20However%2C%20real-world%2C%20multi-modal%20agricultural%20UAV%20datasets%20that%20enable%20research%20on%20robust%20operation%20remain%20scarce.%20To%20address%20this%20gap%2C%20we%20present%20AgriLiRa4D%2C%20a%20multi-modal%20UAV%20dataset%20designed%20for%20challenging%20outdoor%20agricultural%20environments.%20AgriLiRa4D%20spans%20three%20representative%20farmland%20types-flat%2C%20hilly%2C%20and%20terraced-and%20includes%20both%20boundary%20and%20coverage%20operation%20modes%2C%20resulting%20in%20six%20flight%20sequence%20groups.%20The%20dataset%20provides%20high-accuracy%20ground-truth%20trajectories%20from%20a%20Fiber%20Optic%20Inertial%20Navigation%20System%20with%20Real-Time%20Kinematic%20capability%20%28FINS_RTK%29%2C%20along%20with%20synchronized%20measurements%20from%20a%203D%20LiDAR%2C%20a%204D%20Radar%2C%20and%20an%20Inertial%20Measurement%20Unit%20%28IMU%29%2C%20accompanied%20by%20complete%20intrinsic%20and%20extrinsic%20calibrations.%20Leveraging%20its%20comprehensive%20sensor%20suite%20and%20diverse%20real-world%20scenarios%2C%20AgriLiRa4D%20supports%20diverse%20SLAM%20and%20localization%20studies%20and%20enables%20rigorous%20robustness%20evaluation%20against%20low-texture%20crops%2C%20repetitive%20patterns%2C%20dynamic%20vegetation%2C%20and%20other%20challenges%20of%20real%20agricultural%20environments.%20To%20further%20demonstrate%20its%20utility%2C%20we%20benchmark%20four%20state-of-the-art%20multi-sensor%20SLAM%20algorithms%20across%20different%20sensor%20combinations%2C%20highlighting%20the%20difficulty%20of%20the%20proposed%20sequences%20and%20the%20necessity%20of%20multi-modal%20approaches%20for%20reliable%20UAV%20localization.%20By%20filling%20a%20critical%20gap%20in%20agricultural%20SLAM%20datasets%2C%20AgriLiRa4D%20provides%20a%20valuable%20benchmark%20for%20the%20research%20community%20and%20contributes%20to%20advancing%20autonomous%20navigation%20technologies%20for%20agricultural%20UAVs.%20The%20dataset%20can%20be%20downloaded%20from%3A%20https%3A//zhan994.github.io/AgriLiRa4D.&entry.1838667208=http%3A//arxiv.org/abs/2512.01753v1&entry.124074799=Read"},
{"title": "MV-TAP: Tracking Any Point in Multi-View Videos", "author": "Jahyeok Koo and In\u00e8s Hyeonsu Kim and Mungyeom Kim and Junghyun Park and Seohyun Park and Jaeyeong Kim and Jung Yi and Seokju Cho and Seungryong Kim", "abstract": "Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos. To support this task, we construct a large-scale synthetic training dataset and real-world evaluation sets tailored for multi-view tracking. Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.", "link": "http://arxiv.org/abs/2512.02006v1", "date": "2025-12-01", "relevancy": 2.184, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.568}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5453}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MV-TAP%3A%20Tracking%20Any%20Point%20in%20Multi-View%20Videos&body=Title%3A%20MV-TAP%3A%20Tracking%20Any%20Point%20in%20Multi-View%20Videos%0AAuthor%3A%20Jahyeok%20Koo%20and%20In%C3%A8s%20Hyeonsu%20Kim%20and%20Mungyeom%20Kim%20and%20Junghyun%20Park%20and%20Seohyun%20Park%20and%20Jaeyeong%20Kim%20and%20Jung%20Yi%20and%20Seokju%20Cho%20and%20Seungryong%20Kim%0AAbstract%3A%20Multi-view%20camera%20systems%20enable%20rich%20observations%20of%20complex%20real-world%20scenes%2C%20and%20understanding%20dynamic%20objects%20in%20multi-view%20settings%20has%20become%20central%20to%20various%20applications.%20In%20this%20work%2C%20we%20present%20MV-TAP%2C%20a%20novel%20point%20tracker%20that%20tracks%20points%20across%20multi-view%20videos%20of%20dynamic%20scenes%20by%20leveraging%20cross-view%20information.%20MV-TAP%20utilizes%20camera%20geometry%20and%20a%20cross-view%20attention%20mechanism%20to%20aggregate%20spatio-temporal%20information%20across%20views%2C%20enabling%20more%20complete%20and%20reliable%20trajectory%20estimation%20in%20multi-view%20videos.%20To%20support%20this%20task%2C%20we%20construct%20a%20large-scale%20synthetic%20training%20dataset%20and%20real-world%20evaluation%20sets%20tailored%20for%20multi-view%20tracking.%20Extensive%20experiments%20demonstrate%20that%20MV-TAP%20outperforms%20existing%20point-tracking%20methods%20on%20challenging%20benchmarks%2C%20establishing%20an%20effective%20baseline%20for%20advancing%20research%20in%20multi-view%20point%20tracking.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMV-TAP%253A%2520Tracking%2520Any%2520Point%2520in%2520Multi-View%2520Videos%26entry.906535625%3DJahyeok%2520Koo%2520and%2520In%25C3%25A8s%2520Hyeonsu%2520Kim%2520and%2520Mungyeom%2520Kim%2520and%2520Junghyun%2520Park%2520and%2520Seohyun%2520Park%2520and%2520Jaeyeong%2520Kim%2520and%2520Jung%2520Yi%2520and%2520Seokju%2520Cho%2520and%2520Seungryong%2520Kim%26entry.1292438233%3DMulti-view%2520camera%2520systems%2520enable%2520rich%2520observations%2520of%2520complex%2520real-world%2520scenes%252C%2520and%2520understanding%2520dynamic%2520objects%2520in%2520multi-view%2520settings%2520has%2520become%2520central%2520to%2520various%2520applications.%2520In%2520this%2520work%252C%2520we%2520present%2520MV-TAP%252C%2520a%2520novel%2520point%2520tracker%2520that%2520tracks%2520points%2520across%2520multi-view%2520videos%2520of%2520dynamic%2520scenes%2520by%2520leveraging%2520cross-view%2520information.%2520MV-TAP%2520utilizes%2520camera%2520geometry%2520and%2520a%2520cross-view%2520attention%2520mechanism%2520to%2520aggregate%2520spatio-temporal%2520information%2520across%2520views%252C%2520enabling%2520more%2520complete%2520and%2520reliable%2520trajectory%2520estimation%2520in%2520multi-view%2520videos.%2520To%2520support%2520this%2520task%252C%2520we%2520construct%2520a%2520large-scale%2520synthetic%2520training%2520dataset%2520and%2520real-world%2520evaluation%2520sets%2520tailored%2520for%2520multi-view%2520tracking.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MV-TAP%2520outperforms%2520existing%2520point-tracking%2520methods%2520on%2520challenging%2520benchmarks%252C%2520establishing%2520an%2520effective%2520baseline%2520for%2520advancing%2520research%2520in%2520multi-view%2520point%2520tracking.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MV-TAP%3A%20Tracking%20Any%20Point%20in%20Multi-View%20Videos&entry.906535625=Jahyeok%20Koo%20and%20In%C3%A8s%20Hyeonsu%20Kim%20and%20Mungyeom%20Kim%20and%20Junghyun%20Park%20and%20Seohyun%20Park%20and%20Jaeyeong%20Kim%20and%20Jung%20Yi%20and%20Seokju%20Cho%20and%20Seungryong%20Kim&entry.1292438233=Multi-view%20camera%20systems%20enable%20rich%20observations%20of%20complex%20real-world%20scenes%2C%20and%20understanding%20dynamic%20objects%20in%20multi-view%20settings%20has%20become%20central%20to%20various%20applications.%20In%20this%20work%2C%20we%20present%20MV-TAP%2C%20a%20novel%20point%20tracker%20that%20tracks%20points%20across%20multi-view%20videos%20of%20dynamic%20scenes%20by%20leveraging%20cross-view%20information.%20MV-TAP%20utilizes%20camera%20geometry%20and%20a%20cross-view%20attention%20mechanism%20to%20aggregate%20spatio-temporal%20information%20across%20views%2C%20enabling%20more%20complete%20and%20reliable%20trajectory%20estimation%20in%20multi-view%20videos.%20To%20support%20this%20task%2C%20we%20construct%20a%20large-scale%20synthetic%20training%20dataset%20and%20real-world%20evaluation%20sets%20tailored%20for%20multi-view%20tracking.%20Extensive%20experiments%20demonstrate%20that%20MV-TAP%20outperforms%20existing%20point-tracking%20methods%20on%20challenging%20benchmarks%2C%20establishing%20an%20effective%20baseline%20for%20advancing%20research%20in%20multi-view%20point%20tracking.&entry.1838667208=http%3A//arxiv.org/abs/2512.02006v1&entry.124074799=Read"},
{"title": "Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now", "author": "Varun Varma Thozhiyoor and Shivam Tripathi and Venkatesh Babu Radhakrishnan and Anand Bhattad", "abstract": "Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\\mathrm{eff}}$ from $1.81\\,\\mathrm{m/s^2}$ to $6.43\\,\\mathrm{m/s^2}$ (reaching $65\\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.", "link": "http://arxiv.org/abs/2512.02016v1", "date": "2025-12-01", "relevancy": 2.1815, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6207}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.492}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Objects%20in%20Generated%20Videos%20Are%20Slower%20Than%20They%20Appear%3A%20Models%20Suffer%20Sub-Earth%20Gravity%20and%20Don%27t%20Know%20Galileo%27s%20Principle...for%20now&body=Title%3A%20Objects%20in%20Generated%20Videos%20Are%20Slower%20Than%20They%20Appear%3A%20Models%20Suffer%20Sub-Earth%20Gravity%20and%20Don%27t%20Know%20Galileo%27s%20Principle...for%20now%0AAuthor%3A%20Varun%20Varma%20Thozhiyoor%20and%20Shivam%20Tripathi%20and%20Venkatesh%20Babu%20Radhakrishnan%20and%20Anand%20Bhattad%0AAbstract%3A%20Video%20generators%20are%20increasingly%20evaluated%20as%20potential%20world%20models%2C%20which%20requires%20them%20to%20encode%20and%20understand%20physical%20laws.%20We%20investigate%20their%20representation%20of%20a%20fundamental%20law%3A%20gravity.%20Out-of-the-box%20video%20generators%20consistently%20generate%20objects%20falling%20at%20an%20effectively%20slower%20acceleration.%20However%2C%20these%20physical%20tests%20are%20often%20confounded%20by%20ambiguous%20metric%20scale.%20We%20first%20investigate%20if%20observed%20physical%20errors%20are%20artifacts%20of%20these%20ambiguities%20%28e.g.%2C%20incorrect%20frame%20rate%20assumptions%29.%20We%20find%20that%20even%20temporal%20rescaling%20cannot%20correct%20the%20high-variance%20gravity%20artifacts.%20To%20rigorously%20isolate%20the%20underlying%20physical%20representation%20from%20these%20confounds%2C%20we%20introduce%20a%20unit-free%2C%20two-object%20protocol%20that%20tests%20the%20timing%20ratio%20%24t_1%5E2/t_2%5E2%20%3D%20h_1/h_2%24%2C%20a%20relationship%20independent%20of%20%24g%24%2C%20focal%20length%2C%20and%20scale.%20This%20relative%20test%20reveals%20violations%20of%20Galileo%27s%20equivalence%20principle.%20We%20then%20demonstrate%20that%20this%20physical%20gap%20can%20be%20partially%20mitigated%20with%20targeted%20specialization.%20A%20lightweight%20low-rank%20adaptor%20fine-tuned%20on%20only%20100%20single-ball%20clips%20raises%20%24g_%7B%5Cmathrm%7Beff%7D%7D%24%20from%20%241.81%5C%2C%5Cmathrm%7Bm/s%5E2%7D%24%20to%20%246.43%5C%2C%5Cmathrm%7Bm/s%5E2%7D%24%20%28reaching%20%2465%5C%25%24%20of%20terrestrial%20gravity%29.%20This%20specialist%20adaptor%20also%20generalizes%20zero-shot%20to%20two-ball%20drops%20and%20inclined%20planes%2C%20offering%20initial%20evidence%20that%20specific%20physical%20laws%20can%20be%20corrected%20with%20minimal%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObjects%2520in%2520Generated%2520Videos%2520Are%2520Slower%2520Than%2520They%2520Appear%253A%2520Models%2520Suffer%2520Sub-Earth%2520Gravity%2520and%2520Don%2527t%2520Know%2520Galileo%2527s%2520Principle...for%2520now%26entry.906535625%3DVarun%2520Varma%2520Thozhiyoor%2520and%2520Shivam%2520Tripathi%2520and%2520Venkatesh%2520Babu%2520Radhakrishnan%2520and%2520Anand%2520Bhattad%26entry.1292438233%3DVideo%2520generators%2520are%2520increasingly%2520evaluated%2520as%2520potential%2520world%2520models%252C%2520which%2520requires%2520them%2520to%2520encode%2520and%2520understand%2520physical%2520laws.%2520We%2520investigate%2520their%2520representation%2520of%2520a%2520fundamental%2520law%253A%2520gravity.%2520Out-of-the-box%2520video%2520generators%2520consistently%2520generate%2520objects%2520falling%2520at%2520an%2520effectively%2520slower%2520acceleration.%2520However%252C%2520these%2520physical%2520tests%2520are%2520often%2520confounded%2520by%2520ambiguous%2520metric%2520scale.%2520We%2520first%2520investigate%2520if%2520observed%2520physical%2520errors%2520are%2520artifacts%2520of%2520these%2520ambiguities%2520%2528e.g.%252C%2520incorrect%2520frame%2520rate%2520assumptions%2529.%2520We%2520find%2520that%2520even%2520temporal%2520rescaling%2520cannot%2520correct%2520the%2520high-variance%2520gravity%2520artifacts.%2520To%2520rigorously%2520isolate%2520the%2520underlying%2520physical%2520representation%2520from%2520these%2520confounds%252C%2520we%2520introduce%2520a%2520unit-free%252C%2520two-object%2520protocol%2520that%2520tests%2520the%2520timing%2520ratio%2520%2524t_1%255E2/t_2%255E2%2520%253D%2520h_1/h_2%2524%252C%2520a%2520relationship%2520independent%2520of%2520%2524g%2524%252C%2520focal%2520length%252C%2520and%2520scale.%2520This%2520relative%2520test%2520reveals%2520violations%2520of%2520Galileo%2527s%2520equivalence%2520principle.%2520We%2520then%2520demonstrate%2520that%2520this%2520physical%2520gap%2520can%2520be%2520partially%2520mitigated%2520with%2520targeted%2520specialization.%2520A%2520lightweight%2520low-rank%2520adaptor%2520fine-tuned%2520on%2520only%2520100%2520single-ball%2520clips%2520raises%2520%2524g_%257B%255Cmathrm%257Beff%257D%257D%2524%2520from%2520%25241.81%255C%252C%255Cmathrm%257Bm/s%255E2%257D%2524%2520to%2520%25246.43%255C%252C%255Cmathrm%257Bm/s%255E2%257D%2524%2520%2528reaching%2520%252465%255C%2525%2524%2520of%2520terrestrial%2520gravity%2529.%2520This%2520specialist%2520adaptor%2520also%2520generalizes%2520zero-shot%2520to%2520two-ball%2520drops%2520and%2520inclined%2520planes%252C%2520offering%2520initial%2520evidence%2520that%2520specific%2520physical%2520laws%2520can%2520be%2520corrected%2520with%2520minimal%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Objects%20in%20Generated%20Videos%20Are%20Slower%20Than%20They%20Appear%3A%20Models%20Suffer%20Sub-Earth%20Gravity%20and%20Don%27t%20Know%20Galileo%27s%20Principle...for%20now&entry.906535625=Varun%20Varma%20Thozhiyoor%20and%20Shivam%20Tripathi%20and%20Venkatesh%20Babu%20Radhakrishnan%20and%20Anand%20Bhattad&entry.1292438233=Video%20generators%20are%20increasingly%20evaluated%20as%20potential%20world%20models%2C%20which%20requires%20them%20to%20encode%20and%20understand%20physical%20laws.%20We%20investigate%20their%20representation%20of%20a%20fundamental%20law%3A%20gravity.%20Out-of-the-box%20video%20generators%20consistently%20generate%20objects%20falling%20at%20an%20effectively%20slower%20acceleration.%20However%2C%20these%20physical%20tests%20are%20often%20confounded%20by%20ambiguous%20metric%20scale.%20We%20first%20investigate%20if%20observed%20physical%20errors%20are%20artifacts%20of%20these%20ambiguities%20%28e.g.%2C%20incorrect%20frame%20rate%20assumptions%29.%20We%20find%20that%20even%20temporal%20rescaling%20cannot%20correct%20the%20high-variance%20gravity%20artifacts.%20To%20rigorously%20isolate%20the%20underlying%20physical%20representation%20from%20these%20confounds%2C%20we%20introduce%20a%20unit-free%2C%20two-object%20protocol%20that%20tests%20the%20timing%20ratio%20%24t_1%5E2/t_2%5E2%20%3D%20h_1/h_2%24%2C%20a%20relationship%20independent%20of%20%24g%24%2C%20focal%20length%2C%20and%20scale.%20This%20relative%20test%20reveals%20violations%20of%20Galileo%27s%20equivalence%20principle.%20We%20then%20demonstrate%20that%20this%20physical%20gap%20can%20be%20partially%20mitigated%20with%20targeted%20specialization.%20A%20lightweight%20low-rank%20adaptor%20fine-tuned%20on%20only%20100%20single-ball%20clips%20raises%20%24g_%7B%5Cmathrm%7Beff%7D%7D%24%20from%20%241.81%5C%2C%5Cmathrm%7Bm/s%5E2%7D%24%20to%20%246.43%5C%2C%5Cmathrm%7Bm/s%5E2%7D%24%20%28reaching%20%2465%5C%25%24%20of%20terrestrial%20gravity%29.%20This%20specialist%20adaptor%20also%20generalizes%20zero-shot%20to%20two-ball%20drops%20and%20inclined%20planes%2C%20offering%20initial%20evidence%20that%20specific%20physical%20laws%20can%20be%20corrected%20with%20minimal%20data.&entry.1838667208=http%3A//arxiv.org/abs/2512.02016v1&entry.124074799=Read"},
{"title": "LLM-Driven Corrective Robot Operation Code Generation with Static Text-Based Simulation", "author": "Wenhao Wang and Yanyan Li and Long Jiao and Jiawei Yuan", "abstract": "Recent advances in Large language models (LLMs) have demonstrated their promising capabilities of generating robot operation code to enable LLM-driven robots. To enhance the reliability of operation code generated by LLMs, corrective designs with feedback from the observation of executing code have been increasingly adopted in existing research. However, the code execution in these designs relies on either a physical experiment or a customized simulation environment, which limits their deployment due to the high configuration effort of the environment and the potential long execution time. In this paper, we explore the possibility of directly leveraging LLM to enable static simulation of robot operation code, and then leverage it to design a new reliable LLM-driven corrective robot operation code generation framework. Our framework configures the LLM as a static simulator with enhanced capabilities that reliably simulate robot code execution by interpreting actions, reasoning over state transitions, analyzing execution outcomes, and generating se- mantic observations that accurately capture trajectory dynamics. To validate the performance of our framework, we performed experiments on various operation tasks for different robots, including UAVs and small ground vehicles. The experiment results not only demonstrated the high accuracy of our static text-based simulation but also the reliable code generation of our LLM-driven corrective framework, which achieves a comparable performance with state-of-the-art research while does not rely on dynamic code execution using physical experiments or simulators.", "link": "http://arxiv.org/abs/2512.02002v1", "date": "2025-12-01", "relevancy": 1.5102, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5351}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5128}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Driven%20Corrective%20Robot%20Operation%20Code%20Generation%20with%20Static%20Text-Based%20Simulation&body=Title%3A%20LLM-Driven%20Corrective%20Robot%20Operation%20Code%20Generation%20with%20Static%20Text-Based%20Simulation%0AAuthor%3A%20Wenhao%20Wang%20and%20Yanyan%20Li%20and%20Long%20Jiao%20and%20Jiawei%20Yuan%0AAbstract%3A%20Recent%20advances%20in%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20their%20promising%20capabilities%20of%20generating%20robot%20operation%20code%20to%20enable%20LLM-driven%20robots.%20To%20enhance%20the%20reliability%20of%20operation%20code%20generated%20by%20LLMs%2C%20corrective%20designs%20with%20feedback%20from%20the%20observation%20of%20executing%20code%20have%20been%20increasingly%20adopted%20in%20existing%20research.%20However%2C%20the%20code%20execution%20in%20these%20designs%20relies%20on%20either%20a%20physical%20experiment%20or%20a%20customized%20simulation%20environment%2C%20which%20limits%20their%20deployment%20due%20to%20the%20high%20configuration%20effort%20of%20the%20environment%20and%20the%20potential%20long%20execution%20time.%20In%20this%20paper%2C%20we%20explore%20the%20possibility%20of%20directly%20leveraging%20LLM%20to%20enable%20static%20simulation%20of%20robot%20operation%20code%2C%20and%20then%20leverage%20it%20to%20design%20a%20new%20reliable%20LLM-driven%20corrective%20robot%20operation%20code%20generation%20framework.%20Our%20framework%20configures%20the%20LLM%20as%20a%20static%20simulator%20with%20enhanced%20capabilities%20that%20reliably%20simulate%20robot%20code%20execution%20by%20interpreting%20actions%2C%20reasoning%20over%20state%20transitions%2C%20analyzing%20execution%20outcomes%2C%20and%20generating%20se-%20mantic%20observations%20that%20accurately%20capture%20trajectory%20dynamics.%20To%20validate%20the%20performance%20of%20our%20framework%2C%20we%20performed%20experiments%20on%20various%20operation%20tasks%20for%20different%20robots%2C%20including%20UAVs%20and%20small%20ground%20vehicles.%20The%20experiment%20results%20not%20only%20demonstrated%20the%20high%20accuracy%20of%20our%20static%20text-based%20simulation%20but%20also%20the%20reliable%20code%20generation%20of%20our%20LLM-driven%20corrective%20framework%2C%20which%20achieves%20a%20comparable%20performance%20with%20state-of-the-art%20research%20while%20does%20not%20rely%20on%20dynamic%20code%20execution%20using%20physical%20experiments%20or%20simulators.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Driven%2520Corrective%2520Robot%2520Operation%2520Code%2520Generation%2520with%2520Static%2520Text-Based%2520Simulation%26entry.906535625%3DWenhao%2520Wang%2520and%2520Yanyan%2520Li%2520and%2520Long%2520Jiao%2520and%2520Jiawei%2520Yuan%26entry.1292438233%3DRecent%2520advances%2520in%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520their%2520promising%2520capabilities%2520of%2520generating%2520robot%2520operation%2520code%2520to%2520enable%2520LLM-driven%2520robots.%2520To%2520enhance%2520the%2520reliability%2520of%2520operation%2520code%2520generated%2520by%2520LLMs%252C%2520corrective%2520designs%2520with%2520feedback%2520from%2520the%2520observation%2520of%2520executing%2520code%2520have%2520been%2520increasingly%2520adopted%2520in%2520existing%2520research.%2520However%252C%2520the%2520code%2520execution%2520in%2520these%2520designs%2520relies%2520on%2520either%2520a%2520physical%2520experiment%2520or%2520a%2520customized%2520simulation%2520environment%252C%2520which%2520limits%2520their%2520deployment%2520due%2520to%2520the%2520high%2520configuration%2520effort%2520of%2520the%2520environment%2520and%2520the%2520potential%2520long%2520execution%2520time.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520possibility%2520of%2520directly%2520leveraging%2520LLM%2520to%2520enable%2520static%2520simulation%2520of%2520robot%2520operation%2520code%252C%2520and%2520then%2520leverage%2520it%2520to%2520design%2520a%2520new%2520reliable%2520LLM-driven%2520corrective%2520robot%2520operation%2520code%2520generation%2520framework.%2520Our%2520framework%2520configures%2520the%2520LLM%2520as%2520a%2520static%2520simulator%2520with%2520enhanced%2520capabilities%2520that%2520reliably%2520simulate%2520robot%2520code%2520execution%2520by%2520interpreting%2520actions%252C%2520reasoning%2520over%2520state%2520transitions%252C%2520analyzing%2520execution%2520outcomes%252C%2520and%2520generating%2520se-%2520mantic%2520observations%2520that%2520accurately%2520capture%2520trajectory%2520dynamics.%2520To%2520validate%2520the%2520performance%2520of%2520our%2520framework%252C%2520we%2520performed%2520experiments%2520on%2520various%2520operation%2520tasks%2520for%2520different%2520robots%252C%2520including%2520UAVs%2520and%2520small%2520ground%2520vehicles.%2520The%2520experiment%2520results%2520not%2520only%2520demonstrated%2520the%2520high%2520accuracy%2520of%2520our%2520static%2520text-based%2520simulation%2520but%2520also%2520the%2520reliable%2520code%2520generation%2520of%2520our%2520LLM-driven%2520corrective%2520framework%252C%2520which%2520achieves%2520a%2520comparable%2520performance%2520with%2520state-of-the-art%2520research%2520while%2520does%2520not%2520rely%2520on%2520dynamic%2520code%2520execution%2520using%2520physical%2520experiments%2520or%2520simulators.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Driven%20Corrective%20Robot%20Operation%20Code%20Generation%20with%20Static%20Text-Based%20Simulation&entry.906535625=Wenhao%20Wang%20and%20Yanyan%20Li%20and%20Long%20Jiao%20and%20Jiawei%20Yuan&entry.1292438233=Recent%20advances%20in%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20their%20promising%20capabilities%20of%20generating%20robot%20operation%20code%20to%20enable%20LLM-driven%20robots.%20To%20enhance%20the%20reliability%20of%20operation%20code%20generated%20by%20LLMs%2C%20corrective%20designs%20with%20feedback%20from%20the%20observation%20of%20executing%20code%20have%20been%20increasingly%20adopted%20in%20existing%20research.%20However%2C%20the%20code%20execution%20in%20these%20designs%20relies%20on%20either%20a%20physical%20experiment%20or%20a%20customized%20simulation%20environment%2C%20which%20limits%20their%20deployment%20due%20to%20the%20high%20configuration%20effort%20of%20the%20environment%20and%20the%20potential%20long%20execution%20time.%20In%20this%20paper%2C%20we%20explore%20the%20possibility%20of%20directly%20leveraging%20LLM%20to%20enable%20static%20simulation%20of%20robot%20operation%20code%2C%20and%20then%20leverage%20it%20to%20design%20a%20new%20reliable%20LLM-driven%20corrective%20robot%20operation%20code%20generation%20framework.%20Our%20framework%20configures%20the%20LLM%20as%20a%20static%20simulator%20with%20enhanced%20capabilities%20that%20reliably%20simulate%20robot%20code%20execution%20by%20interpreting%20actions%2C%20reasoning%20over%20state%20transitions%2C%20analyzing%20execution%20outcomes%2C%20and%20generating%20se-%20mantic%20observations%20that%20accurately%20capture%20trajectory%20dynamics.%20To%20validate%20the%20performance%20of%20our%20framework%2C%20we%20performed%20experiments%20on%20various%20operation%20tasks%20for%20different%20robots%2C%20including%20UAVs%20and%20small%20ground%20vehicles.%20The%20experiment%20results%20not%20only%20demonstrated%20the%20high%20accuracy%20of%20our%20static%20text-based%20simulation%20but%20also%20the%20reliable%20code%20generation%20of%20our%20LLM-driven%20corrective%20framework%2C%20which%20achieves%20a%20comparable%20performance%20with%20state-of-the-art%20research%20while%20does%20not%20rely%20on%20dynamic%20code%20execution%20using%20physical%20experiments%20or%20simulators.&entry.1838667208=http%3A//arxiv.org/abs/2512.02002v1&entry.124074799=Read"},
{"title": "CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball", "author": "Omer Sela and Michael Chertok and Lior Wolf", "abstract": "This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.", "link": "http://arxiv.org/abs/2512.01478v1", "date": "2025-12-01", "relevancy": 1.7229, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5884}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5837}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CourtMotion%3A%20Learning%20Event-Driven%20Motion%20Representations%20from%20Skeletal%20Data%20for%20Basketball&body=Title%3A%20CourtMotion%3A%20Learning%20Event-Driven%20Motion%20Representations%20from%20Skeletal%20Data%20for%20Basketball%0AAuthor%3A%20Omer%20Sela%20and%20Michael%20Chertok%20and%20Lior%20Wolf%0AAbstract%3A%20This%20paper%20presents%20CourtMotion%2C%20a%20spatiotemporal%20modeling%20framework%20for%20analyzing%20and%20predicting%20game%20events%20and%20plays%20as%20they%20develop%20in%20professional%20basketball.%20Anticipating%20basketball%20events%20requires%20understanding%20both%20physical%20motion%20patterns%20and%20their%20semantic%20significance%20in%20the%20context%20of%20the%20game.%20Traditional%20approaches%20that%20use%20only%20player%20positions%20fail%20to%20capture%20crucial%20indicators%20such%20as%20body%20orientation%2C%20defensive%20stance%2C%20or%20shooting%20preparation%20motions.%20Our%20two-stage%20approach%20first%20processes%20skeletal%20tracking%20data%20through%20Graph%20Neural%20Networks%20to%20capture%20nuanced%20motion%20patterns%2C%20then%20employs%20a%20Transformer%20architecture%20with%20specialized%20attention%20mechanisms%20to%20model%20player%20interactions.%20We%20introduce%20event%20projection%20heads%20that%20explicitly%20connect%20player%20movements%20to%20basketball%20events%20like%20passes%2C%20shots%2C%20and%20steals%2C%20training%20the%20model%20to%20associate%20physical%20motion%20patterns%20with%20their%20tactical%20purposes.%20Experiments%20on%20NBA%20tracking%20data%20demonstrate%20significant%20improvements%20over%20position-only%20baselines%3A%2035%25%20reduction%20in%20trajectory%20prediction%20error%20compared%20to%20state-of-the-art%20position-based%20models%20and%20consistent%20performance%20gains%20across%20key%20basketball%20analytics%20tasks.%20The%20resulting%20pretrained%20model%20serves%20as%20a%20powerful%20foundation%20for%20multiple%20downstream%20tasks%2C%20with%20pick%20detection%2C%20shot%20taker%20identification%2C%20assist%20prediction%2C%20shot%20location%20classification%2C%20and%20shot%20type%20recognition%20demonstrating%20substantial%20improvements%20over%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCourtMotion%253A%2520Learning%2520Event-Driven%2520Motion%2520Representations%2520from%2520Skeletal%2520Data%2520for%2520Basketball%26entry.906535625%3DOmer%2520Sela%2520and%2520Michael%2520Chertok%2520and%2520Lior%2520Wolf%26entry.1292438233%3DThis%2520paper%2520presents%2520CourtMotion%252C%2520a%2520spatiotemporal%2520modeling%2520framework%2520for%2520analyzing%2520and%2520predicting%2520game%2520events%2520and%2520plays%2520as%2520they%2520develop%2520in%2520professional%2520basketball.%2520Anticipating%2520basketball%2520events%2520requires%2520understanding%2520both%2520physical%2520motion%2520patterns%2520and%2520their%2520semantic%2520significance%2520in%2520the%2520context%2520of%2520the%2520game.%2520Traditional%2520approaches%2520that%2520use%2520only%2520player%2520positions%2520fail%2520to%2520capture%2520crucial%2520indicators%2520such%2520as%2520body%2520orientation%252C%2520defensive%2520stance%252C%2520or%2520shooting%2520preparation%2520motions.%2520Our%2520two-stage%2520approach%2520first%2520processes%2520skeletal%2520tracking%2520data%2520through%2520Graph%2520Neural%2520Networks%2520to%2520capture%2520nuanced%2520motion%2520patterns%252C%2520then%2520employs%2520a%2520Transformer%2520architecture%2520with%2520specialized%2520attention%2520mechanisms%2520to%2520model%2520player%2520interactions.%2520We%2520introduce%2520event%2520projection%2520heads%2520that%2520explicitly%2520connect%2520player%2520movements%2520to%2520basketball%2520events%2520like%2520passes%252C%2520shots%252C%2520and%2520steals%252C%2520training%2520the%2520model%2520to%2520associate%2520physical%2520motion%2520patterns%2520with%2520their%2520tactical%2520purposes.%2520Experiments%2520on%2520NBA%2520tracking%2520data%2520demonstrate%2520significant%2520improvements%2520over%2520position-only%2520baselines%253A%252035%2525%2520reduction%2520in%2520trajectory%2520prediction%2520error%2520compared%2520to%2520state-of-the-art%2520position-based%2520models%2520and%2520consistent%2520performance%2520gains%2520across%2520key%2520basketball%2520analytics%2520tasks.%2520The%2520resulting%2520pretrained%2520model%2520serves%2520as%2520a%2520powerful%2520foundation%2520for%2520multiple%2520downstream%2520tasks%252C%2520with%2520pick%2520detection%252C%2520shot%2520taker%2520identification%252C%2520assist%2520prediction%252C%2520shot%2520location%2520classification%252C%2520and%2520shot%2520type%2520recognition%2520demonstrating%2520substantial%2520improvements%2520over%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CourtMotion%3A%20Learning%20Event-Driven%20Motion%20Representations%20from%20Skeletal%20Data%20for%20Basketball&entry.906535625=Omer%20Sela%20and%20Michael%20Chertok%20and%20Lior%20Wolf&entry.1292438233=This%20paper%20presents%20CourtMotion%2C%20a%20spatiotemporal%20modeling%20framework%20for%20analyzing%20and%20predicting%20game%20events%20and%20plays%20as%20they%20develop%20in%20professional%20basketball.%20Anticipating%20basketball%20events%20requires%20understanding%20both%20physical%20motion%20patterns%20and%20their%20semantic%20significance%20in%20the%20context%20of%20the%20game.%20Traditional%20approaches%20that%20use%20only%20player%20positions%20fail%20to%20capture%20crucial%20indicators%20such%20as%20body%20orientation%2C%20defensive%20stance%2C%20or%20shooting%20preparation%20motions.%20Our%20two-stage%20approach%20first%20processes%20skeletal%20tracking%20data%20through%20Graph%20Neural%20Networks%20to%20capture%20nuanced%20motion%20patterns%2C%20then%20employs%20a%20Transformer%20architecture%20with%20specialized%20attention%20mechanisms%20to%20model%20player%20interactions.%20We%20introduce%20event%20projection%20heads%20that%20explicitly%20connect%20player%20movements%20to%20basketball%20events%20like%20passes%2C%20shots%2C%20and%20steals%2C%20training%20the%20model%20to%20associate%20physical%20motion%20patterns%20with%20their%20tactical%20purposes.%20Experiments%20on%20NBA%20tracking%20data%20demonstrate%20significant%20improvements%20over%20position-only%20baselines%3A%2035%25%20reduction%20in%20trajectory%20prediction%20error%20compared%20to%20state-of-the-art%20position-based%20models%20and%20consistent%20performance%20gains%20across%20key%20basketball%20analytics%20tasks.%20The%20resulting%20pretrained%20model%20serves%20as%20a%20powerful%20foundation%20for%20multiple%20downstream%20tasks%2C%20with%20pick%20detection%2C%20shot%20taker%20identification%2C%20assist%20prediction%2C%20shot%20location%20classification%2C%20and%20shot%20type%20recognition%20demonstrating%20substantial%20improvements%20over%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.01478v1&entry.124074799=Read"},
{"title": "Signals, Concepts, and Laws: Toward Universal, Explainable Time-Series Forecasting", "author": "Hongwei Ma and Junbin Gao and Minh-Ngoc Tran", "abstract": "Accurate, explainable and physically credible forecasting remains a persistent challenge for multivariate time-series whose statistical properties vary across domains. We propose DORIC, a Domain-Universal, ODE-Regularized, Interpretable-Concept Transformer for Time-Series Forecasting that generates predictions through five self-supervised, domain-agnostic concepts while enforcing differentiable residuals grounded in first-principles constraints.", "link": "http://arxiv.org/abs/2508.01407v3", "date": "2025-12-01", "relevancy": 1.3908, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4663}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4644}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Signals%2C%20Concepts%2C%20and%20Laws%3A%20Toward%20Universal%2C%20Explainable%20Time-Series%20Forecasting&body=Title%3A%20Signals%2C%20Concepts%2C%20and%20Laws%3A%20Toward%20Universal%2C%20Explainable%20Time-Series%20Forecasting%0AAuthor%3A%20Hongwei%20Ma%20and%20Junbin%20Gao%20and%20Minh-Ngoc%20Tran%0AAbstract%3A%20Accurate%2C%20explainable%20and%20physically%20credible%20forecasting%20remains%20a%20persistent%20challenge%20for%20multivariate%20time-series%20whose%20statistical%20properties%20vary%20across%20domains.%20We%20propose%20DORIC%2C%20a%20Domain-Universal%2C%20ODE-Regularized%2C%20Interpretable-Concept%20Transformer%20for%20Time-Series%20Forecasting%20that%20generates%20predictions%20through%20five%20self-supervised%2C%20domain-agnostic%20concepts%20while%20enforcing%20differentiable%20residuals%20grounded%20in%20first-principles%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2508.01407v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignals%252C%2520Concepts%252C%2520and%2520Laws%253A%2520Toward%2520Universal%252C%2520Explainable%2520Time-Series%2520Forecasting%26entry.906535625%3DHongwei%2520Ma%2520and%2520Junbin%2520Gao%2520and%2520Minh-Ngoc%2520Tran%26entry.1292438233%3DAccurate%252C%2520explainable%2520and%2520physically%2520credible%2520forecasting%2520remains%2520a%2520persistent%2520challenge%2520for%2520multivariate%2520time-series%2520whose%2520statistical%2520properties%2520vary%2520across%2520domains.%2520We%2520propose%2520DORIC%252C%2520a%2520Domain-Universal%252C%2520ODE-Regularized%252C%2520Interpretable-Concept%2520Transformer%2520for%2520Time-Series%2520Forecasting%2520that%2520generates%2520predictions%2520through%2520five%2520self-supervised%252C%2520domain-agnostic%2520concepts%2520while%2520enforcing%2520differentiable%2520residuals%2520grounded%2520in%2520first-principles%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01407v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Signals%2C%20Concepts%2C%20and%20Laws%3A%20Toward%20Universal%2C%20Explainable%20Time-Series%20Forecasting&entry.906535625=Hongwei%20Ma%20and%20Junbin%20Gao%20and%20Minh-Ngoc%20Tran&entry.1292438233=Accurate%2C%20explainable%20and%20physically%20credible%20forecasting%20remains%20a%20persistent%20challenge%20for%20multivariate%20time-series%20whose%20statistical%20properties%20vary%20across%20domains.%20We%20propose%20DORIC%2C%20a%20Domain-Universal%2C%20ODE-Regularized%2C%20Interpretable-Concept%20Transformer%20for%20Time-Series%20Forecasting%20that%20generates%20predictions%20through%20five%20self-supervised%2C%20domain-agnostic%20concepts%20while%20enforcing%20differentiable%20residuals%20grounded%20in%20first-principles%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2508.01407v3&entry.124074799=Read"},
{"title": "Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions", "author": "Yifei Dong and Fengyi Wu and Sanjian Zhang and Guangyu Chen and Yuzhi Hu and Masumi Yano and Jingdong Sun and Siyu Huang and Feng Liu and Qi Dai and Zhi-Qi Cheng", "abstract": "Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure inspection, surveillance, and related tasks, yet they also introduce critical security challenges. This survey provides a wide-ranging examination of the anti-UAV domain, centering on three core objectives-classification, detection, and tracking-while detailing emerging methodologies such as diffusion-based data synthesis, multi-modal fusion, vision-language modeling, self-supervised learning, and reinforcement learning. We systematically evaluate state-of-the-art solutions across both single-modality and multi-sensor pipelines (spanning RGB, infrared, audio, radar, and RF) and discuss large-scale as well as adversarially oriented benchmarks. Our analysis reveals persistent gaps in real-time performance, stealth detection, and swarm-based scenarios, underscoring pressing needs for robust, adaptive anti-UAV systems. By highlighting open research directions, we aim to foster innovation and guide the development of next-generation defense strategies in an era marked by the extensive use of UAVs.", "link": "http://arxiv.org/abs/2504.11967v3", "date": "2025-12-01", "relevancy": 2.0428, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5468}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Securing%20the%20Skies%3A%20A%20Comprehensive%20Survey%20on%20Anti-UAV%20Methods%2C%20Benchmarking%2C%20and%20Future%20Directions&body=Title%3A%20Securing%20the%20Skies%3A%20A%20Comprehensive%20Survey%20on%20Anti-UAV%20Methods%2C%20Benchmarking%2C%20and%20Future%20Directions%0AAuthor%3A%20Yifei%20Dong%20and%20Fengyi%20Wu%20and%20Sanjian%20Zhang%20and%20Guangyu%20Chen%20and%20Yuzhi%20Hu%20and%20Masumi%20Yano%20and%20Jingdong%20Sun%20and%20Siyu%20Huang%20and%20Feng%20Liu%20and%20Qi%20Dai%20and%20Zhi-Qi%20Cheng%0AAbstract%3A%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20indispensable%20for%20infrastructure%20inspection%2C%20surveillance%2C%20and%20related%20tasks%2C%20yet%20they%20also%20introduce%20critical%20security%20challenges.%20This%20survey%20provides%20a%20wide-ranging%20examination%20of%20the%20anti-UAV%20domain%2C%20centering%20on%20three%20core%20objectives-classification%2C%20detection%2C%20and%20tracking-while%20detailing%20emerging%20methodologies%20such%20as%20diffusion-based%20data%20synthesis%2C%20multi-modal%20fusion%2C%20vision-language%20modeling%2C%20self-supervised%20learning%2C%20and%20reinforcement%20learning.%20We%20systematically%20evaluate%20state-of-the-art%20solutions%20across%20both%20single-modality%20and%20multi-sensor%20pipelines%20%28spanning%20RGB%2C%20infrared%2C%20audio%2C%20radar%2C%20and%20RF%29%20and%20discuss%20large-scale%20as%20well%20as%20adversarially%20oriented%20benchmarks.%20Our%20analysis%20reveals%20persistent%20gaps%20in%20real-time%20performance%2C%20stealth%20detection%2C%20and%20swarm-based%20scenarios%2C%20underscoring%20pressing%20needs%20for%20robust%2C%20adaptive%20anti-UAV%20systems.%20By%20highlighting%20open%20research%20directions%2C%20we%20aim%20to%20foster%20innovation%20and%20guide%20the%20development%20of%20next-generation%20defense%20strategies%20in%20an%20era%20marked%20by%20the%20extensive%20use%20of%20UAVs.%0ALink%3A%20http%3A//arxiv.org/abs/2504.11967v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecuring%2520the%2520Skies%253A%2520A%2520Comprehensive%2520Survey%2520on%2520Anti-UAV%2520Methods%252C%2520Benchmarking%252C%2520and%2520Future%2520Directions%26entry.906535625%3DYifei%2520Dong%2520and%2520Fengyi%2520Wu%2520and%2520Sanjian%2520Zhang%2520and%2520Guangyu%2520Chen%2520and%2520Yuzhi%2520Hu%2520and%2520Masumi%2520Yano%2520and%2520Jingdong%2520Sun%2520and%2520Siyu%2520Huang%2520and%2520Feng%2520Liu%2520and%2520Qi%2520Dai%2520and%2520Zhi-Qi%2520Cheng%26entry.1292438233%3DUnmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520are%2520indispensable%2520for%2520infrastructure%2520inspection%252C%2520surveillance%252C%2520and%2520related%2520tasks%252C%2520yet%2520they%2520also%2520introduce%2520critical%2520security%2520challenges.%2520This%2520survey%2520provides%2520a%2520wide-ranging%2520examination%2520of%2520the%2520anti-UAV%2520domain%252C%2520centering%2520on%2520three%2520core%2520objectives-classification%252C%2520detection%252C%2520and%2520tracking-while%2520detailing%2520emerging%2520methodologies%2520such%2520as%2520diffusion-based%2520data%2520synthesis%252C%2520multi-modal%2520fusion%252C%2520vision-language%2520modeling%252C%2520self-supervised%2520learning%252C%2520and%2520reinforcement%2520learning.%2520We%2520systematically%2520evaluate%2520state-of-the-art%2520solutions%2520across%2520both%2520single-modality%2520and%2520multi-sensor%2520pipelines%2520%2528spanning%2520RGB%252C%2520infrared%252C%2520audio%252C%2520radar%252C%2520and%2520RF%2529%2520and%2520discuss%2520large-scale%2520as%2520well%2520as%2520adversarially%2520oriented%2520benchmarks.%2520Our%2520analysis%2520reveals%2520persistent%2520gaps%2520in%2520real-time%2520performance%252C%2520stealth%2520detection%252C%2520and%2520swarm-based%2520scenarios%252C%2520underscoring%2520pressing%2520needs%2520for%2520robust%252C%2520adaptive%2520anti-UAV%2520systems.%2520By%2520highlighting%2520open%2520research%2520directions%252C%2520we%2520aim%2520to%2520foster%2520innovation%2520and%2520guide%2520the%2520development%2520of%2520next-generation%2520defense%2520strategies%2520in%2520an%2520era%2520marked%2520by%2520the%2520extensive%2520use%2520of%2520UAVs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11967v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Securing%20the%20Skies%3A%20A%20Comprehensive%20Survey%20on%20Anti-UAV%20Methods%2C%20Benchmarking%2C%20and%20Future%20Directions&entry.906535625=Yifei%20Dong%20and%20Fengyi%20Wu%20and%20Sanjian%20Zhang%20and%20Guangyu%20Chen%20and%20Yuzhi%20Hu%20and%20Masumi%20Yano%20and%20Jingdong%20Sun%20and%20Siyu%20Huang%20and%20Feng%20Liu%20and%20Qi%20Dai%20and%20Zhi-Qi%20Cheng&entry.1292438233=Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20indispensable%20for%20infrastructure%20inspection%2C%20surveillance%2C%20and%20related%20tasks%2C%20yet%20they%20also%20introduce%20critical%20security%20challenges.%20This%20survey%20provides%20a%20wide-ranging%20examination%20of%20the%20anti-UAV%20domain%2C%20centering%20on%20three%20core%20objectives-classification%2C%20detection%2C%20and%20tracking-while%20detailing%20emerging%20methodologies%20such%20as%20diffusion-based%20data%20synthesis%2C%20multi-modal%20fusion%2C%20vision-language%20modeling%2C%20self-supervised%20learning%2C%20and%20reinforcement%20learning.%20We%20systematically%20evaluate%20state-of-the-art%20solutions%20across%20both%20single-modality%20and%20multi-sensor%20pipelines%20%28spanning%20RGB%2C%20infrared%2C%20audio%2C%20radar%2C%20and%20RF%29%20and%20discuss%20large-scale%20as%20well%20as%20adversarially%20oriented%20benchmarks.%20Our%20analysis%20reveals%20persistent%20gaps%20in%20real-time%20performance%2C%20stealth%20detection%2C%20and%20swarm-based%20scenarios%2C%20underscoring%20pressing%20needs%20for%20robust%2C%20adaptive%20anti-UAV%20systems.%20By%20highlighting%20open%20research%20directions%2C%20we%20aim%20to%20foster%20innovation%20and%20guide%20the%20development%20of%20next-generation%20defense%20strategies%20in%20an%20era%20marked%20by%20the%20extensive%20use%20of%20UAVs.&entry.1838667208=http%3A//arxiv.org/abs/2504.11967v3&entry.124074799=Read"},
{"title": "iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation", "author": "Zhoujie Fu and Xianfang Zeng and Jinghong Lan and Xinyao Liao and Cheng Chen and Junyi Chen and Jiacheng Wei and Wei Cheng and Shiyu Liu and Yunuo Chen and Gang Yu and Guosheng Lin", "abstract": "Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.", "link": "http://arxiv.org/abs/2511.20635v2", "date": "2025-12-01", "relevancy": 1.8406, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6179}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6102}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iMontage%3A%20Unified%2C%20Versatile%2C%20Highly%20Dynamic%20Many-to-many%20Image%20Generation&body=Title%3A%20iMontage%3A%20Unified%2C%20Versatile%2C%20Highly%20Dynamic%20Many-to-many%20Image%20Generation%0AAuthor%3A%20Zhoujie%20Fu%20and%20Xianfang%20Zeng%20and%20Jinghong%20Lan%20and%20Xinyao%20Liao%20and%20Cheng%20Chen%20and%20Junyi%20Chen%20and%20Jiacheng%20Wei%20and%20Wei%20Cheng%20and%20Shiyu%20Liu%20and%20Yunuo%20Chen%20and%20Gang%20Yu%20and%20Guosheng%20Lin%0AAbstract%3A%20Pre-trained%20video%20models%20learn%20powerful%20priors%20for%20generating%20high-quality%2C%20temporally%20coherent%20content.%20While%20these%20models%20excel%20at%20temporal%20coherence%2C%20their%20dynamics%20are%20often%20constrained%20by%20the%20continuous%20nature%20of%20their%20training%20data.%20We%20hypothesize%20that%20by%20injecting%20the%20rich%20and%20unconstrained%20content%20diversity%20from%20image%20data%20into%20this%20coherent%20temporal%20framework%2C%20we%20can%20generate%20image%20sets%20that%20feature%20both%20natural%20transitions%20and%20a%20far%20more%20expansive%20dynamic%20range.%20To%20this%20end%2C%20we%20introduce%20iMontage%2C%20a%20unified%20framework%20designed%20to%20repurpose%20a%20powerful%20video%20model%20into%20an%20all-in-one%20image%20generator.%20The%20framework%20consumes%20and%20produces%20variable-length%20image%20sets%2C%20unifying%20a%20wide%20array%20of%20image%20generation%20and%20editing%20tasks.%20To%20achieve%20this%2C%20we%20propose%20an%20elegant%20and%20minimally%20invasive%20adaptation%20strategy%2C%20complemented%20by%20a%20tailored%20data%20curation%20process%20and%20training%20paradigm.%20This%20approach%20allows%20the%20model%20to%20acquire%20broad%20image%20manipulation%20capabilities%20without%20corrupting%20its%20invaluable%20original%20motion%20priors.%20iMontage%20excels%20across%20several%20mainstream%20many-in-many-out%20tasks%2C%20not%20only%20maintaining%20strong%20cross-image%20contextual%20consistency%20but%20also%20generating%20scenes%20with%20extraordinary%20dynamics%20that%20surpass%20conventional%20scopes.%20Find%20our%20homepage%20at%3A%20https%3A//kr1sjfu.github.io/iMontage-web/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiMontage%253A%2520Unified%252C%2520Versatile%252C%2520Highly%2520Dynamic%2520Many-to-many%2520Image%2520Generation%26entry.906535625%3DZhoujie%2520Fu%2520and%2520Xianfang%2520Zeng%2520and%2520Jinghong%2520Lan%2520and%2520Xinyao%2520Liao%2520and%2520Cheng%2520Chen%2520and%2520Junyi%2520Chen%2520and%2520Jiacheng%2520Wei%2520and%2520Wei%2520Cheng%2520and%2520Shiyu%2520Liu%2520and%2520Yunuo%2520Chen%2520and%2520Gang%2520Yu%2520and%2520Guosheng%2520Lin%26entry.1292438233%3DPre-trained%2520video%2520models%2520learn%2520powerful%2520priors%2520for%2520generating%2520high-quality%252C%2520temporally%2520coherent%2520content.%2520While%2520these%2520models%2520excel%2520at%2520temporal%2520coherence%252C%2520their%2520dynamics%2520are%2520often%2520constrained%2520by%2520the%2520continuous%2520nature%2520of%2520their%2520training%2520data.%2520We%2520hypothesize%2520that%2520by%2520injecting%2520the%2520rich%2520and%2520unconstrained%2520content%2520diversity%2520from%2520image%2520data%2520into%2520this%2520coherent%2520temporal%2520framework%252C%2520we%2520can%2520generate%2520image%2520sets%2520that%2520feature%2520both%2520natural%2520transitions%2520and%2520a%2520far%2520more%2520expansive%2520dynamic%2520range.%2520To%2520this%2520end%252C%2520we%2520introduce%2520iMontage%252C%2520a%2520unified%2520framework%2520designed%2520to%2520repurpose%2520a%2520powerful%2520video%2520model%2520into%2520an%2520all-in-one%2520image%2520generator.%2520The%2520framework%2520consumes%2520and%2520produces%2520variable-length%2520image%2520sets%252C%2520unifying%2520a%2520wide%2520array%2520of%2520image%2520generation%2520and%2520editing%2520tasks.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520an%2520elegant%2520and%2520minimally%2520invasive%2520adaptation%2520strategy%252C%2520complemented%2520by%2520a%2520tailored%2520data%2520curation%2520process%2520and%2520training%2520paradigm.%2520This%2520approach%2520allows%2520the%2520model%2520to%2520acquire%2520broad%2520image%2520manipulation%2520capabilities%2520without%2520corrupting%2520its%2520invaluable%2520original%2520motion%2520priors.%2520iMontage%2520excels%2520across%2520several%2520mainstream%2520many-in-many-out%2520tasks%252C%2520not%2520only%2520maintaining%2520strong%2520cross-image%2520contextual%2520consistency%2520but%2520also%2520generating%2520scenes%2520with%2520extraordinary%2520dynamics%2520that%2520surpass%2520conventional%2520scopes.%2520Find%2520our%2520homepage%2520at%253A%2520https%253A//kr1sjfu.github.io/iMontage-web/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iMontage%3A%20Unified%2C%20Versatile%2C%20Highly%20Dynamic%20Many-to-many%20Image%20Generation&entry.906535625=Zhoujie%20Fu%20and%20Xianfang%20Zeng%20and%20Jinghong%20Lan%20and%20Xinyao%20Liao%20and%20Cheng%20Chen%20and%20Junyi%20Chen%20and%20Jiacheng%20Wei%20and%20Wei%20Cheng%20and%20Shiyu%20Liu%20and%20Yunuo%20Chen%20and%20Gang%20Yu%20and%20Guosheng%20Lin&entry.1292438233=Pre-trained%20video%20models%20learn%20powerful%20priors%20for%20generating%20high-quality%2C%20temporally%20coherent%20content.%20While%20these%20models%20excel%20at%20temporal%20coherence%2C%20their%20dynamics%20are%20often%20constrained%20by%20the%20continuous%20nature%20of%20their%20training%20data.%20We%20hypothesize%20that%20by%20injecting%20the%20rich%20and%20unconstrained%20content%20diversity%20from%20image%20data%20into%20this%20coherent%20temporal%20framework%2C%20we%20can%20generate%20image%20sets%20that%20feature%20both%20natural%20transitions%20and%20a%20far%20more%20expansive%20dynamic%20range.%20To%20this%20end%2C%20we%20introduce%20iMontage%2C%20a%20unified%20framework%20designed%20to%20repurpose%20a%20powerful%20video%20model%20into%20an%20all-in-one%20image%20generator.%20The%20framework%20consumes%20and%20produces%20variable-length%20image%20sets%2C%20unifying%20a%20wide%20array%20of%20image%20generation%20and%20editing%20tasks.%20To%20achieve%20this%2C%20we%20propose%20an%20elegant%20and%20minimally%20invasive%20adaptation%20strategy%2C%20complemented%20by%20a%20tailored%20data%20curation%20process%20and%20training%20paradigm.%20This%20approach%20allows%20the%20model%20to%20acquire%20broad%20image%20manipulation%20capabilities%20without%20corrupting%20its%20invaluable%20original%20motion%20priors.%20iMontage%20excels%20across%20several%20mainstream%20many-in-many-out%20tasks%2C%20not%20only%20maintaining%20strong%20cross-image%20contextual%20consistency%20but%20also%20generating%20scenes%20with%20extraordinary%20dynamics%20that%20surpass%20conventional%20scopes.%20Find%20our%20homepage%20at%3A%20https%3A//kr1sjfu.github.io/iMontage-web/.&entry.1838667208=http%3A//arxiv.org/abs/2511.20635v2&entry.124074799=Read"},
{"title": "GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation", "author": "Yunfei Li and Xiao Ma and Jiafeng Xu and Yu Cui and Zhongren Cui and Zhigang Han and Liqun Huang and Tao Kong and Yuxiao Liu and Hao Niu and Wanli Peng and Jingchao Qiao and Zeyu Ren and Haixin Shi and Zhi Su and Jiawen Tian and Yuyang Xiao and Shenyu Zhang and Liwei Zheng and Hang Li and Yonghui Wu", "abstract": "We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.", "link": "http://arxiv.org/abs/2512.01801v1", "date": "2025-12-01", "relevancy": 1.8383, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6365}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6171}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GR-RL%3A%20Going%20Dexterous%20and%20Precise%20for%20Long-Horizon%20Robotic%20Manipulation&body=Title%3A%20GR-RL%3A%20Going%20Dexterous%20and%20Precise%20for%20Long-Horizon%20Robotic%20Manipulation%0AAuthor%3A%20Yunfei%20Li%20and%20Xiao%20Ma%20and%20Jiafeng%20Xu%20and%20Yu%20Cui%20and%20Zhongren%20Cui%20and%20Zhigang%20Han%20and%20Liqun%20Huang%20and%20Tao%20Kong%20and%20Yuxiao%20Liu%20and%20Hao%20Niu%20and%20Wanli%20Peng%20and%20Jingchao%20Qiao%20and%20Zeyu%20Ren%20and%20Haixin%20Shi%20and%20Zhi%20Su%20and%20Jiawen%20Tian%20and%20Yuyang%20Xiao%20and%20Shenyu%20Zhang%20and%20Liwei%20Zheng%20and%20Hang%20Li%20and%20Yonghui%20Wu%0AAbstract%3A%20We%20present%20GR-RL%2C%20a%20robotic%20learning%20framework%20that%20turns%20a%20generalist%20vision-language-action%20%28VLA%29%20policy%20into%20a%20highly%20capable%20specialist%20for%20long-horizon%20dexterous%20manipulation.%20Assuming%20the%20optimality%20of%20human%20demonstrations%20is%20core%20to%20existing%20VLA%20policies.%20However%2C%20we%20claim%20that%20in%20highly%20dexterous%20and%20precise%20manipulation%20tasks%2C%20human%20demonstrations%20are%20noisy%20and%20suboptimal.%20GR-RL%20proposes%20a%20multi-stage%20training%20pipeline%20that%20filters%2C%20augments%2C%20and%20reinforces%20the%20demonstrations%20by%20reinforcement%20learning.%20First%2C%20GR-RL%20learns%20a%20vision-language-conditioned%20task%20progress%2C%20filters%20the%20demonstration%20trajectories%2C%20and%20only%20keeps%20the%20transitions%20that%20contribute%20positively%20to%20the%20progress.%20Specifically%2C%20we%20show%20that%20by%20directly%20applying%20offline%20RL%20with%20sparse%20reward%2C%20the%20resulting%20%24Q%24-values%20can%20be%20treated%20as%20a%20robust%20progress%20function.%20Next%2C%20we%20introduce%20morphological%20symmetry%20augmentation%20that%20greatly%20improves%20the%20generalization%20and%20performance%20of%20GR-RL.%20Lastly%2C%20to%20better%20align%20the%20VLA%20policy%20with%20its%20deployment%20behaviors%20for%20high-precision%20control%2C%20we%20perform%20online%20RL%20by%20learning%20a%20latent%20space%20noise%20predictor.%20With%20this%20pipeline%2C%20GR-RL%20is%2C%20to%20our%20knowledge%2C%20the%20first%20learning-based%20policy%20that%20can%20autonomously%20lace%20up%20a%20shoe%20by%20threading%20shoelaces%20through%20multiple%20eyelets%20with%20an%2083.3%25%20success%20rate%2C%20a%20task%20requiring%20long-horizon%20reasoning%2C%20millimeter-level%20precision%2C%20and%20compliant%20soft-body%20interaction.%20We%20hope%20GR-RL%20provides%20a%20step%20toward%20enabling%20generalist%20robot%20foundations%20models%20to%20specialize%20into%20reliable%20real-world%20experts.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGR-RL%253A%2520Going%2520Dexterous%2520and%2520Precise%2520for%2520Long-Horizon%2520Robotic%2520Manipulation%26entry.906535625%3DYunfei%2520Li%2520and%2520Xiao%2520Ma%2520and%2520Jiafeng%2520Xu%2520and%2520Yu%2520Cui%2520and%2520Zhongren%2520Cui%2520and%2520Zhigang%2520Han%2520and%2520Liqun%2520Huang%2520and%2520Tao%2520Kong%2520and%2520Yuxiao%2520Liu%2520and%2520Hao%2520Niu%2520and%2520Wanli%2520Peng%2520and%2520Jingchao%2520Qiao%2520and%2520Zeyu%2520Ren%2520and%2520Haixin%2520Shi%2520and%2520Zhi%2520Su%2520and%2520Jiawen%2520Tian%2520and%2520Yuyang%2520Xiao%2520and%2520Shenyu%2520Zhang%2520and%2520Liwei%2520Zheng%2520and%2520Hang%2520Li%2520and%2520Yonghui%2520Wu%26entry.1292438233%3DWe%2520present%2520GR-RL%252C%2520a%2520robotic%2520learning%2520framework%2520that%2520turns%2520a%2520generalist%2520vision-language-action%2520%2528VLA%2529%2520policy%2520into%2520a%2520highly%2520capable%2520specialist%2520for%2520long-horizon%2520dexterous%2520manipulation.%2520Assuming%2520the%2520optimality%2520of%2520human%2520demonstrations%2520is%2520core%2520to%2520existing%2520VLA%2520policies.%2520However%252C%2520we%2520claim%2520that%2520in%2520highly%2520dexterous%2520and%2520precise%2520manipulation%2520tasks%252C%2520human%2520demonstrations%2520are%2520noisy%2520and%2520suboptimal.%2520GR-RL%2520proposes%2520a%2520multi-stage%2520training%2520pipeline%2520that%2520filters%252C%2520augments%252C%2520and%2520reinforces%2520the%2520demonstrations%2520by%2520reinforcement%2520learning.%2520First%252C%2520GR-RL%2520learns%2520a%2520vision-language-conditioned%2520task%2520progress%252C%2520filters%2520the%2520demonstration%2520trajectories%252C%2520and%2520only%2520keeps%2520the%2520transitions%2520that%2520contribute%2520positively%2520to%2520the%2520progress.%2520Specifically%252C%2520we%2520show%2520that%2520by%2520directly%2520applying%2520offline%2520RL%2520with%2520sparse%2520reward%252C%2520the%2520resulting%2520%2524Q%2524-values%2520can%2520be%2520treated%2520as%2520a%2520robust%2520progress%2520function.%2520Next%252C%2520we%2520introduce%2520morphological%2520symmetry%2520augmentation%2520that%2520greatly%2520improves%2520the%2520generalization%2520and%2520performance%2520of%2520GR-RL.%2520Lastly%252C%2520to%2520better%2520align%2520the%2520VLA%2520policy%2520with%2520its%2520deployment%2520behaviors%2520for%2520high-precision%2520control%252C%2520we%2520perform%2520online%2520RL%2520by%2520learning%2520a%2520latent%2520space%2520noise%2520predictor.%2520With%2520this%2520pipeline%252C%2520GR-RL%2520is%252C%2520to%2520our%2520knowledge%252C%2520the%2520first%2520learning-based%2520policy%2520that%2520can%2520autonomously%2520lace%2520up%2520a%2520shoe%2520by%2520threading%2520shoelaces%2520through%2520multiple%2520eyelets%2520with%2520an%252083.3%2525%2520success%2520rate%252C%2520a%2520task%2520requiring%2520long-horizon%2520reasoning%252C%2520millimeter-level%2520precision%252C%2520and%2520compliant%2520soft-body%2520interaction.%2520We%2520hope%2520GR-RL%2520provides%2520a%2520step%2520toward%2520enabling%2520generalist%2520robot%2520foundations%2520models%2520to%2520specialize%2520into%2520reliable%2520real-world%2520experts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GR-RL%3A%20Going%20Dexterous%20and%20Precise%20for%20Long-Horizon%20Robotic%20Manipulation&entry.906535625=Yunfei%20Li%20and%20Xiao%20Ma%20and%20Jiafeng%20Xu%20and%20Yu%20Cui%20and%20Zhongren%20Cui%20and%20Zhigang%20Han%20and%20Liqun%20Huang%20and%20Tao%20Kong%20and%20Yuxiao%20Liu%20and%20Hao%20Niu%20and%20Wanli%20Peng%20and%20Jingchao%20Qiao%20and%20Zeyu%20Ren%20and%20Haixin%20Shi%20and%20Zhi%20Su%20and%20Jiawen%20Tian%20and%20Yuyang%20Xiao%20and%20Shenyu%20Zhang%20and%20Liwei%20Zheng%20and%20Hang%20Li%20and%20Yonghui%20Wu&entry.1292438233=We%20present%20GR-RL%2C%20a%20robotic%20learning%20framework%20that%20turns%20a%20generalist%20vision-language-action%20%28VLA%29%20policy%20into%20a%20highly%20capable%20specialist%20for%20long-horizon%20dexterous%20manipulation.%20Assuming%20the%20optimality%20of%20human%20demonstrations%20is%20core%20to%20existing%20VLA%20policies.%20However%2C%20we%20claim%20that%20in%20highly%20dexterous%20and%20precise%20manipulation%20tasks%2C%20human%20demonstrations%20are%20noisy%20and%20suboptimal.%20GR-RL%20proposes%20a%20multi-stage%20training%20pipeline%20that%20filters%2C%20augments%2C%20and%20reinforces%20the%20demonstrations%20by%20reinforcement%20learning.%20First%2C%20GR-RL%20learns%20a%20vision-language-conditioned%20task%20progress%2C%20filters%20the%20demonstration%20trajectories%2C%20and%20only%20keeps%20the%20transitions%20that%20contribute%20positively%20to%20the%20progress.%20Specifically%2C%20we%20show%20that%20by%20directly%20applying%20offline%20RL%20with%20sparse%20reward%2C%20the%20resulting%20%24Q%24-values%20can%20be%20treated%20as%20a%20robust%20progress%20function.%20Next%2C%20we%20introduce%20morphological%20symmetry%20augmentation%20that%20greatly%20improves%20the%20generalization%20and%20performance%20of%20GR-RL.%20Lastly%2C%20to%20better%20align%20the%20VLA%20policy%20with%20its%20deployment%20behaviors%20for%20high-precision%20control%2C%20we%20perform%20online%20RL%20by%20learning%20a%20latent%20space%20noise%20predictor.%20With%20this%20pipeline%2C%20GR-RL%20is%2C%20to%20our%20knowledge%2C%20the%20first%20learning-based%20policy%20that%20can%20autonomously%20lace%20up%20a%20shoe%20by%20threading%20shoelaces%20through%20multiple%20eyelets%20with%20an%2083.3%25%20success%20rate%2C%20a%20task%20requiring%20long-horizon%20reasoning%2C%20millimeter-level%20precision%2C%20and%20compliant%20soft-body%20interaction.%20We%20hope%20GR-RL%20provides%20a%20step%20toward%20enabling%20generalist%20robot%20foundations%20models%20to%20specialize%20into%20reliable%20real-world%20experts.&entry.1838667208=http%3A//arxiv.org/abs/2512.01801v1&entry.124074799=Read"},
{"title": "Global Convergence of Policy Gradient for Entropy Regularized Linear-Quadratic Control with Multiplicative Noise", "author": "Gabriel Diaz and Lucky Li and Wenhao Zhang", "abstract": "Reinforcement Learning (RL) has emerged as a powerful framework for sequential decision-making in dynamic environments, particularly when system parameters are unknown. This paper investigates RL-based control for entropy-regularized linear-quadratic (LQ) control problems with multiplicative noise over an infinite time horizon. First, we adapt the regularized policy gradient (RPG) algorithm to stochastic optimal control settings, proving that despite the non-convexity of the problem, RPG converges globally under conditions of gradient domination and almost-smoothness. Second, based on zero-order optimization approach, we introduce a novel model free RL algorithm: Sample-based regularized policy gradient (SB-RPG). SB-RPG operates without knowledge of system parameters yet still retains strong theoretical guarantees of global convergence. Our model leverages entropy regularization to address the exploration versus exploitation trade-off inherent in RL. Numerical simulations validate the theoretical results and demonstrate the efficiency of SB-RPG in unknown-parameters environments.", "link": "http://arxiv.org/abs/2510.02896v4", "date": "2025-12-01", "relevancy": 1.8621, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.491}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4751}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Convergence%20of%20Policy%20Gradient%20for%20Entropy%20Regularized%20Linear-Quadratic%20Control%20with%20Multiplicative%20Noise&body=Title%3A%20Global%20Convergence%20of%20Policy%20Gradient%20for%20Entropy%20Regularized%20Linear-Quadratic%20Control%20with%20Multiplicative%20Noise%0AAuthor%3A%20Gabriel%20Diaz%20and%20Lucky%20Li%20and%20Wenhao%20Zhang%0AAbstract%3A%20Reinforcement%20Learning%20%28RL%29%20has%20emerged%20as%20a%20powerful%20framework%20for%20sequential%20decision-making%20in%20dynamic%20environments%2C%20particularly%20when%20system%20parameters%20are%20unknown.%20This%20paper%20investigates%20RL-based%20control%20for%20entropy-regularized%20linear-quadratic%20%28LQ%29%20control%20problems%20with%20multiplicative%20noise%20over%20an%20infinite%20time%20horizon.%20First%2C%20we%20adapt%20the%20regularized%20policy%20gradient%20%28RPG%29%20algorithm%20to%20stochastic%20optimal%20control%20settings%2C%20proving%20that%20despite%20the%20non-convexity%20of%20the%20problem%2C%20RPG%20converges%20globally%20under%20conditions%20of%20gradient%20domination%20and%20almost-smoothness.%20Second%2C%20based%20on%20zero-order%20optimization%20approach%2C%20we%20introduce%20a%20novel%20model%20free%20RL%20algorithm%3A%20Sample-based%20regularized%20policy%20gradient%20%28SB-RPG%29.%20SB-RPG%20operates%20without%20knowledge%20of%20system%20parameters%20yet%20still%20retains%20strong%20theoretical%20guarantees%20of%20global%20convergence.%20Our%20model%20leverages%20entropy%20regularization%20to%20address%20the%20exploration%20versus%20exploitation%20trade-off%20inherent%20in%20RL.%20Numerical%20simulations%20validate%20the%20theoretical%20results%20and%20demonstrate%20the%20efficiency%20of%20SB-RPG%20in%20unknown-parameters%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2510.02896v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Convergence%2520of%2520Policy%2520Gradient%2520for%2520Entropy%2520Regularized%2520Linear-Quadratic%2520Control%2520with%2520Multiplicative%2520Noise%26entry.906535625%3DGabriel%2520Diaz%2520and%2520Lucky%2520Li%2520and%2520Wenhao%2520Zhang%26entry.1292438233%3DReinforcement%2520Learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520framework%2520for%2520sequential%2520decision-making%2520in%2520dynamic%2520environments%252C%2520particularly%2520when%2520system%2520parameters%2520are%2520unknown.%2520This%2520paper%2520investigates%2520RL-based%2520control%2520for%2520entropy-regularized%2520linear-quadratic%2520%2528LQ%2529%2520control%2520problems%2520with%2520multiplicative%2520noise%2520over%2520an%2520infinite%2520time%2520horizon.%2520First%252C%2520we%2520adapt%2520the%2520regularized%2520policy%2520gradient%2520%2528RPG%2529%2520algorithm%2520to%2520stochastic%2520optimal%2520control%2520settings%252C%2520proving%2520that%2520despite%2520the%2520non-convexity%2520of%2520the%2520problem%252C%2520RPG%2520converges%2520globally%2520under%2520conditions%2520of%2520gradient%2520domination%2520and%2520almost-smoothness.%2520Second%252C%2520based%2520on%2520zero-order%2520optimization%2520approach%252C%2520we%2520introduce%2520a%2520novel%2520model%2520free%2520RL%2520algorithm%253A%2520Sample-based%2520regularized%2520policy%2520gradient%2520%2528SB-RPG%2529.%2520SB-RPG%2520operates%2520without%2520knowledge%2520of%2520system%2520parameters%2520yet%2520still%2520retains%2520strong%2520theoretical%2520guarantees%2520of%2520global%2520convergence.%2520Our%2520model%2520leverages%2520entropy%2520regularization%2520to%2520address%2520the%2520exploration%2520versus%2520exploitation%2520trade-off%2520inherent%2520in%2520RL.%2520Numerical%2520simulations%2520validate%2520the%2520theoretical%2520results%2520and%2520demonstrate%2520the%2520efficiency%2520of%2520SB-RPG%2520in%2520unknown-parameters%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02896v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Convergence%20of%20Policy%20Gradient%20for%20Entropy%20Regularized%20Linear-Quadratic%20Control%20with%20Multiplicative%20Noise&entry.906535625=Gabriel%20Diaz%20and%20Lucky%20Li%20and%20Wenhao%20Zhang&entry.1292438233=Reinforcement%20Learning%20%28RL%29%20has%20emerged%20as%20a%20powerful%20framework%20for%20sequential%20decision-making%20in%20dynamic%20environments%2C%20particularly%20when%20system%20parameters%20are%20unknown.%20This%20paper%20investigates%20RL-based%20control%20for%20entropy-regularized%20linear-quadratic%20%28LQ%29%20control%20problems%20with%20multiplicative%20noise%20over%20an%20infinite%20time%20horizon.%20First%2C%20we%20adapt%20the%20regularized%20policy%20gradient%20%28RPG%29%20algorithm%20to%20stochastic%20optimal%20control%20settings%2C%20proving%20that%20despite%20the%20non-convexity%20of%20the%20problem%2C%20RPG%20converges%20globally%20under%20conditions%20of%20gradient%20domination%20and%20almost-smoothness.%20Second%2C%20based%20on%20zero-order%20optimization%20approach%2C%20we%20introduce%20a%20novel%20model%20free%20RL%20algorithm%3A%20Sample-based%20regularized%20policy%20gradient%20%28SB-RPG%29.%20SB-RPG%20operates%20without%20knowledge%20of%20system%20parameters%20yet%20still%20retains%20strong%20theoretical%20guarantees%20of%20global%20convergence.%20Our%20model%20leverages%20entropy%20regularization%20to%20address%20the%20exploration%20versus%20exploitation%20trade-off%20inherent%20in%20RL.%20Numerical%20simulations%20validate%20the%20theoretical%20results%20and%20demonstrate%20the%20efficiency%20of%20SB-RPG%20in%20unknown-parameters%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2510.02896v4&entry.124074799=Read"},
{"title": "Sparse PCA With Multiple Components", "author": "Ryan Cory-Wright and Jean Pauphilet", "abstract": "Sparse Principal Component Analysis (sPCA) is a cardinal technique for obtaining combinations of features, or principal components (PCs), that explain the variance of high-dimensional datasets in an interpretable manner. This involves solving a sparsity and orthogonality constrained convex maximization problem, which is extremely computationally challenging. Most existing works address sparse PCA via methods-such as iteratively computing one sparse PC and deflating the covariance matrix-that do not guarantee the orthogonality, let alone the optimality, of the resulting solution when we seek multiple mutually orthogonal PCs. We challenge this status by reformulating the orthogonality conditions as rank constraints and optimizing over the sparsity and rank constraints simultaneously. We design tight semidefinite relaxations to supply high-quality upper bounds, which we strengthen via additional second-order cone inequalities when each PC's individual sparsity is specified. Further, we derive a combinatorial upper bound on the maximum amount of variance explained as a function of the support. We exploit these relaxations and bounds to propose exact methods and rounding mechanisms that, together, obtain solutions with a bound gap on the order of 0%-15% for real-world datasets with p = 100s or 1000s of features and r \\in {2, 3} components. Numerically, our algorithms match (and sometimes surpass) the best performing methods in terms of fraction of variance explained and systematically return PCs that are sparse and orthogonal. In contrast, we find that existing methods like deflation return solutions that violate the orthogonality constraints, even when the data is generated according to sparse orthogonal PCs. Altogether, our approach solves sparse PCA problems with multiple components to certifiable (near) optimality in a practically tractable fashion.", "link": "http://arxiv.org/abs/2209.14790v4", "date": "2025-12-01", "relevancy": 1.5714, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3961}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3908}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20PCA%20With%20Multiple%20Components&body=Title%3A%20Sparse%20PCA%20With%20Multiple%20Components%0AAuthor%3A%20Ryan%20Cory-Wright%20and%20Jean%20Pauphilet%0AAbstract%3A%20Sparse%20Principal%20Component%20Analysis%20%28sPCA%29%20is%20a%20cardinal%20technique%20for%20obtaining%20combinations%20of%20features%2C%20or%20principal%20components%20%28PCs%29%2C%20that%20explain%20the%20variance%20of%20high-dimensional%20datasets%20in%20an%20interpretable%20manner.%20This%20involves%20solving%20a%20sparsity%20and%20orthogonality%20constrained%20convex%20maximization%20problem%2C%20which%20is%20extremely%20computationally%20challenging.%20Most%20existing%20works%20address%20sparse%20PCA%20via%20methods-such%20as%20iteratively%20computing%20one%20sparse%20PC%20and%20deflating%20the%20covariance%20matrix-that%20do%20not%20guarantee%20the%20orthogonality%2C%20let%20alone%20the%20optimality%2C%20of%20the%20resulting%20solution%20when%20we%20seek%20multiple%20mutually%20orthogonal%20PCs.%20We%20challenge%20this%20status%20by%20reformulating%20the%20orthogonality%20conditions%20as%20rank%20constraints%20and%20optimizing%20over%20the%20sparsity%20and%20rank%20constraints%20simultaneously.%20We%20design%20tight%20semidefinite%20relaxations%20to%20supply%20high-quality%20upper%20bounds%2C%20which%20we%20strengthen%20via%20additional%20second-order%20cone%20inequalities%20when%20each%20PC%27s%20individual%20sparsity%20is%20specified.%20Further%2C%20we%20derive%20a%20combinatorial%20upper%20bound%20on%20the%20maximum%20amount%20of%20variance%20explained%20as%20a%20function%20of%20the%20support.%20We%20exploit%20these%20relaxations%20and%20bounds%20to%20propose%20exact%20methods%20and%20rounding%20mechanisms%20that%2C%20together%2C%20obtain%20solutions%20with%20a%20bound%20gap%20on%20the%20order%20of%200%25-15%25%20for%20real-world%20datasets%20with%20p%20%3D%20100s%20or%201000s%20of%20features%20and%20r%20%5Cin%20%7B2%2C%203%7D%20components.%20Numerically%2C%20our%20algorithms%20match%20%28and%20sometimes%20surpass%29%20the%20best%20performing%20methods%20in%20terms%20of%20fraction%20of%20variance%20explained%20and%20systematically%20return%20PCs%20that%20are%20sparse%20and%20orthogonal.%20In%20contrast%2C%20we%20find%20that%20existing%20methods%20like%20deflation%20return%20solutions%20that%20violate%20the%20orthogonality%20constraints%2C%20even%20when%20the%20data%20is%20generated%20according%20to%20sparse%20orthogonal%20PCs.%20Altogether%2C%20our%20approach%20solves%20sparse%20PCA%20problems%20with%20multiple%20components%20to%20certifiable%20%28near%29%20optimality%20in%20a%20practically%20tractable%20fashion.%0ALink%3A%20http%3A//arxiv.org/abs/2209.14790v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520PCA%2520With%2520Multiple%2520Components%26entry.906535625%3DRyan%2520Cory-Wright%2520and%2520Jean%2520Pauphilet%26entry.1292438233%3DSparse%2520Principal%2520Component%2520Analysis%2520%2528sPCA%2529%2520is%2520a%2520cardinal%2520technique%2520for%2520obtaining%2520combinations%2520of%2520features%252C%2520or%2520principal%2520components%2520%2528PCs%2529%252C%2520that%2520explain%2520the%2520variance%2520of%2520high-dimensional%2520datasets%2520in%2520an%2520interpretable%2520manner.%2520This%2520involves%2520solving%2520a%2520sparsity%2520and%2520orthogonality%2520constrained%2520convex%2520maximization%2520problem%252C%2520which%2520is%2520extremely%2520computationally%2520challenging.%2520Most%2520existing%2520works%2520address%2520sparse%2520PCA%2520via%2520methods-such%2520as%2520iteratively%2520computing%2520one%2520sparse%2520PC%2520and%2520deflating%2520the%2520covariance%2520matrix-that%2520do%2520not%2520guarantee%2520the%2520orthogonality%252C%2520let%2520alone%2520the%2520optimality%252C%2520of%2520the%2520resulting%2520solution%2520when%2520we%2520seek%2520multiple%2520mutually%2520orthogonal%2520PCs.%2520We%2520challenge%2520this%2520status%2520by%2520reformulating%2520the%2520orthogonality%2520conditions%2520as%2520rank%2520constraints%2520and%2520optimizing%2520over%2520the%2520sparsity%2520and%2520rank%2520constraints%2520simultaneously.%2520We%2520design%2520tight%2520semidefinite%2520relaxations%2520to%2520supply%2520high-quality%2520upper%2520bounds%252C%2520which%2520we%2520strengthen%2520via%2520additional%2520second-order%2520cone%2520inequalities%2520when%2520each%2520PC%2527s%2520individual%2520sparsity%2520is%2520specified.%2520Further%252C%2520we%2520derive%2520a%2520combinatorial%2520upper%2520bound%2520on%2520the%2520maximum%2520amount%2520of%2520variance%2520explained%2520as%2520a%2520function%2520of%2520the%2520support.%2520We%2520exploit%2520these%2520relaxations%2520and%2520bounds%2520to%2520propose%2520exact%2520methods%2520and%2520rounding%2520mechanisms%2520that%252C%2520together%252C%2520obtain%2520solutions%2520with%2520a%2520bound%2520gap%2520on%2520the%2520order%2520of%25200%2525-15%2525%2520for%2520real-world%2520datasets%2520with%2520p%2520%253D%2520100s%2520or%25201000s%2520of%2520features%2520and%2520r%2520%255Cin%2520%257B2%252C%25203%257D%2520components.%2520Numerically%252C%2520our%2520algorithms%2520match%2520%2528and%2520sometimes%2520surpass%2529%2520the%2520best%2520performing%2520methods%2520in%2520terms%2520of%2520fraction%2520of%2520variance%2520explained%2520and%2520systematically%2520return%2520PCs%2520that%2520are%2520sparse%2520and%2520orthogonal.%2520In%2520contrast%252C%2520we%2520find%2520that%2520existing%2520methods%2520like%2520deflation%2520return%2520solutions%2520that%2520violate%2520the%2520orthogonality%2520constraints%252C%2520even%2520when%2520the%2520data%2520is%2520generated%2520according%2520to%2520sparse%2520orthogonal%2520PCs.%2520Altogether%252C%2520our%2520approach%2520solves%2520sparse%2520PCA%2520problems%2520with%2520multiple%2520components%2520to%2520certifiable%2520%2528near%2529%2520optimality%2520in%2520a%2520practically%2520tractable%2520fashion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.14790v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20PCA%20With%20Multiple%20Components&entry.906535625=Ryan%20Cory-Wright%20and%20Jean%20Pauphilet&entry.1292438233=Sparse%20Principal%20Component%20Analysis%20%28sPCA%29%20is%20a%20cardinal%20technique%20for%20obtaining%20combinations%20of%20features%2C%20or%20principal%20components%20%28PCs%29%2C%20that%20explain%20the%20variance%20of%20high-dimensional%20datasets%20in%20an%20interpretable%20manner.%20This%20involves%20solving%20a%20sparsity%20and%20orthogonality%20constrained%20convex%20maximization%20problem%2C%20which%20is%20extremely%20computationally%20challenging.%20Most%20existing%20works%20address%20sparse%20PCA%20via%20methods-such%20as%20iteratively%20computing%20one%20sparse%20PC%20and%20deflating%20the%20covariance%20matrix-that%20do%20not%20guarantee%20the%20orthogonality%2C%20let%20alone%20the%20optimality%2C%20of%20the%20resulting%20solution%20when%20we%20seek%20multiple%20mutually%20orthogonal%20PCs.%20We%20challenge%20this%20status%20by%20reformulating%20the%20orthogonality%20conditions%20as%20rank%20constraints%20and%20optimizing%20over%20the%20sparsity%20and%20rank%20constraints%20simultaneously.%20We%20design%20tight%20semidefinite%20relaxations%20to%20supply%20high-quality%20upper%20bounds%2C%20which%20we%20strengthen%20via%20additional%20second-order%20cone%20inequalities%20when%20each%20PC%27s%20individual%20sparsity%20is%20specified.%20Further%2C%20we%20derive%20a%20combinatorial%20upper%20bound%20on%20the%20maximum%20amount%20of%20variance%20explained%20as%20a%20function%20of%20the%20support.%20We%20exploit%20these%20relaxations%20and%20bounds%20to%20propose%20exact%20methods%20and%20rounding%20mechanisms%20that%2C%20together%2C%20obtain%20solutions%20with%20a%20bound%20gap%20on%20the%20order%20of%200%25-15%25%20for%20real-world%20datasets%20with%20p%20%3D%20100s%20or%201000s%20of%20features%20and%20r%20%5Cin%20%7B2%2C%203%7D%20components.%20Numerically%2C%20our%20algorithms%20match%20%28and%20sometimes%20surpass%29%20the%20best%20performing%20methods%20in%20terms%20of%20fraction%20of%20variance%20explained%20and%20systematically%20return%20PCs%20that%20are%20sparse%20and%20orthogonal.%20In%20contrast%2C%20we%20find%20that%20existing%20methods%20like%20deflation%20return%20solutions%20that%20violate%20the%20orthogonality%20constraints%2C%20even%20when%20the%20data%20is%20generated%20according%20to%20sparse%20orthogonal%20PCs.%20Altogether%2C%20our%20approach%20solves%20sparse%20PCA%20problems%20with%20multiple%20components%20to%20certifiable%20%28near%29%20optimality%20in%20a%20practically%20tractable%20fashion.&entry.1838667208=http%3A//arxiv.org/abs/2209.14790v4&entry.124074799=Read"},
{"title": "Heuristic algorithms for the stochastic critical node detection problem", "author": "Tuguldur Bayarsaikhan and Altannar Chinchuluun and Ashwin Arulselvan and Panos Pardalos", "abstract": "Given a network, the critical node detection problem finds a subset of nodes whose removal disrupts the network connectivity. Since many real-world systems are naturally modeled as graphs, assessing the vulnerability of the network is essential, with applications in transportation systems, traffic forecasting, epidemic control, and biological networks. In this paper, we consider a stochastic version of the critical node detection problem, where the existence of edges is given by certain probabilities. We propose heuristics and learning-based methods for the problem and compare them with existing algorithms. Experimental results performed on random graphs from small to larger scales, with edge-survival probabilities drawn from different distributions, demonstrate the effectiveness of the methods. Heuristic methods often illustrate the strongest results with high scalability, while learning-based methods maintain nearly constant inference time as the network size and density grow.", "link": "http://arxiv.org/abs/2512.01497v1", "date": "2025-12-01", "relevancy": 1.7157, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4537}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4137}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heuristic%20algorithms%20for%20the%20stochastic%20critical%20node%20detection%20problem&body=Title%3A%20Heuristic%20algorithms%20for%20the%20stochastic%20critical%20node%20detection%20problem%0AAuthor%3A%20Tuguldur%20Bayarsaikhan%20and%20Altannar%20Chinchuluun%20and%20Ashwin%20Arulselvan%20and%20Panos%20Pardalos%0AAbstract%3A%20Given%20a%20network%2C%20the%20critical%20node%20detection%20problem%20finds%20a%20subset%20of%20nodes%20whose%20removal%20disrupts%20the%20network%20connectivity.%20Since%20many%20real-world%20systems%20are%20naturally%20modeled%20as%20graphs%2C%20assessing%20the%20vulnerability%20of%20the%20network%20is%20essential%2C%20with%20applications%20in%20transportation%20systems%2C%20traffic%20forecasting%2C%20epidemic%20control%2C%20and%20biological%20networks.%20In%20this%20paper%2C%20we%20consider%20a%20stochastic%20version%20of%20the%20critical%20node%20detection%20problem%2C%20where%20the%20existence%20of%20edges%20is%20given%20by%20certain%20probabilities.%20We%20propose%20heuristics%20and%20learning-based%20methods%20for%20the%20problem%20and%20compare%20them%20with%20existing%20algorithms.%20Experimental%20results%20performed%20on%20random%20graphs%20from%20small%20to%20larger%20scales%2C%20with%20edge-survival%20probabilities%20drawn%20from%20different%20distributions%2C%20demonstrate%20the%20effectiveness%20of%20the%20methods.%20Heuristic%20methods%20often%20illustrate%20the%20strongest%20results%20with%20high%20scalability%2C%20while%20learning-based%20methods%20maintain%20nearly%20constant%20inference%20time%20as%20the%20network%20size%20and%20density%20grow.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeuristic%2520algorithms%2520for%2520the%2520stochastic%2520critical%2520node%2520detection%2520problem%26entry.906535625%3DTuguldur%2520Bayarsaikhan%2520and%2520Altannar%2520Chinchuluun%2520and%2520Ashwin%2520Arulselvan%2520and%2520Panos%2520Pardalos%26entry.1292438233%3DGiven%2520a%2520network%252C%2520the%2520critical%2520node%2520detection%2520problem%2520finds%2520a%2520subset%2520of%2520nodes%2520whose%2520removal%2520disrupts%2520the%2520network%2520connectivity.%2520Since%2520many%2520real-world%2520systems%2520are%2520naturally%2520modeled%2520as%2520graphs%252C%2520assessing%2520the%2520vulnerability%2520of%2520the%2520network%2520is%2520essential%252C%2520with%2520applications%2520in%2520transportation%2520systems%252C%2520traffic%2520forecasting%252C%2520epidemic%2520control%252C%2520and%2520biological%2520networks.%2520In%2520this%2520paper%252C%2520we%2520consider%2520a%2520stochastic%2520version%2520of%2520the%2520critical%2520node%2520detection%2520problem%252C%2520where%2520the%2520existence%2520of%2520edges%2520is%2520given%2520by%2520certain%2520probabilities.%2520We%2520propose%2520heuristics%2520and%2520learning-based%2520methods%2520for%2520the%2520problem%2520and%2520compare%2520them%2520with%2520existing%2520algorithms.%2520Experimental%2520results%2520performed%2520on%2520random%2520graphs%2520from%2520small%2520to%2520larger%2520scales%252C%2520with%2520edge-survival%2520probabilities%2520drawn%2520from%2520different%2520distributions%252C%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520methods.%2520Heuristic%2520methods%2520often%2520illustrate%2520the%2520strongest%2520results%2520with%2520high%2520scalability%252C%2520while%2520learning-based%2520methods%2520maintain%2520nearly%2520constant%2520inference%2520time%2520as%2520the%2520network%2520size%2520and%2520density%2520grow.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heuristic%20algorithms%20for%20the%20stochastic%20critical%20node%20detection%20problem&entry.906535625=Tuguldur%20Bayarsaikhan%20and%20Altannar%20Chinchuluun%20and%20Ashwin%20Arulselvan%20and%20Panos%20Pardalos&entry.1292438233=Given%20a%20network%2C%20the%20critical%20node%20detection%20problem%20finds%20a%20subset%20of%20nodes%20whose%20removal%20disrupts%20the%20network%20connectivity.%20Since%20many%20real-world%20systems%20are%20naturally%20modeled%20as%20graphs%2C%20assessing%20the%20vulnerability%20of%20the%20network%20is%20essential%2C%20with%20applications%20in%20transportation%20systems%2C%20traffic%20forecasting%2C%20epidemic%20control%2C%20and%20biological%20networks.%20In%20this%20paper%2C%20we%20consider%20a%20stochastic%20version%20of%20the%20critical%20node%20detection%20problem%2C%20where%20the%20existence%20of%20edges%20is%20given%20by%20certain%20probabilities.%20We%20propose%20heuristics%20and%20learning-based%20methods%20for%20the%20problem%20and%20compare%20them%20with%20existing%20algorithms.%20Experimental%20results%20performed%20on%20random%20graphs%20from%20small%20to%20larger%20scales%2C%20with%20edge-survival%20probabilities%20drawn%20from%20different%20distributions%2C%20demonstrate%20the%20effectiveness%20of%20the%20methods.%20Heuristic%20methods%20often%20illustrate%20the%20strongest%20results%20with%20high%20scalability%2C%20while%20learning-based%20methods%20maintain%20nearly%20constant%20inference%20time%20as%20the%20network%20size%20and%20density%20grow.&entry.1838667208=http%3A//arxiv.org/abs/2512.01497v1&entry.124074799=Read"},
{"title": "Walking on the Fiber: A Simple Geometric Approximation for Bayesian Neural Networks", "author": "Alfredo Reichlin and Miguel Vasco and Danica Kragic", "abstract": "Bayesian Neural Networks provide a principled framework for uncertainty quantification by modeling the posterior distribution of network parameters. However, exact posterior inference is computationally intractable, and widely used approximations like the Laplace method struggle with scalability and posterior accuracy in modern deep networks. In this work, we revisit sampling techniques for posterior exploration, proposing a simple variation tailored to efficiently sample from the posterior in over-parameterized networks by leveraging the low-dimensional structure of loss minima. Building on this, we introduce a model that learns a deformation of the parameter space, enabling rapid posterior sampling without requiring iterative methods. Empirical results demonstrate that our approach achieves competitive posterior approximations with improved scalability compared to recent refinement techniques. These contributions provide a practical alternative for Bayesian inference in deep learning.", "link": "http://arxiv.org/abs/2512.01500v1", "date": "2025-12-01", "relevancy": 2.0201, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5515}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.509}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Walking%20on%20the%20Fiber%3A%20A%20Simple%20Geometric%20Approximation%20for%20Bayesian%20Neural%20Networks&body=Title%3A%20Walking%20on%20the%20Fiber%3A%20A%20Simple%20Geometric%20Approximation%20for%20Bayesian%20Neural%20Networks%0AAuthor%3A%20Alfredo%20Reichlin%20and%20Miguel%20Vasco%20and%20Danica%20Kragic%0AAbstract%3A%20Bayesian%20Neural%20Networks%20provide%20a%20principled%20framework%20for%20uncertainty%20quantification%20by%20modeling%20the%20posterior%20distribution%20of%20network%20parameters.%20However%2C%20exact%20posterior%20inference%20is%20computationally%20intractable%2C%20and%20widely%20used%20approximations%20like%20the%20Laplace%20method%20struggle%20with%20scalability%20and%20posterior%20accuracy%20in%20modern%20deep%20networks.%20In%20this%20work%2C%20we%20revisit%20sampling%20techniques%20for%20posterior%20exploration%2C%20proposing%20a%20simple%20variation%20tailored%20to%20efficiently%20sample%20from%20the%20posterior%20in%20over-parameterized%20networks%20by%20leveraging%20the%20low-dimensional%20structure%20of%20loss%20minima.%20Building%20on%20this%2C%20we%20introduce%20a%20model%20that%20learns%20a%20deformation%20of%20the%20parameter%20space%2C%20enabling%20rapid%20posterior%20sampling%20without%20requiring%20iterative%20methods.%20Empirical%20results%20demonstrate%20that%20our%20approach%20achieves%20competitive%20posterior%20approximations%20with%20improved%20scalability%20compared%20to%20recent%20refinement%20techniques.%20These%20contributions%20provide%20a%20practical%20alternative%20for%20Bayesian%20inference%20in%20deep%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWalking%2520on%2520the%2520Fiber%253A%2520A%2520Simple%2520Geometric%2520Approximation%2520for%2520Bayesian%2520Neural%2520Networks%26entry.906535625%3DAlfredo%2520Reichlin%2520and%2520Miguel%2520Vasco%2520and%2520Danica%2520Kragic%26entry.1292438233%3DBayesian%2520Neural%2520Networks%2520provide%2520a%2520principled%2520framework%2520for%2520uncertainty%2520quantification%2520by%2520modeling%2520the%2520posterior%2520distribution%2520of%2520network%2520parameters.%2520However%252C%2520exact%2520posterior%2520inference%2520is%2520computationally%2520intractable%252C%2520and%2520widely%2520used%2520approximations%2520like%2520the%2520Laplace%2520method%2520struggle%2520with%2520scalability%2520and%2520posterior%2520accuracy%2520in%2520modern%2520deep%2520networks.%2520In%2520this%2520work%252C%2520we%2520revisit%2520sampling%2520techniques%2520for%2520posterior%2520exploration%252C%2520proposing%2520a%2520simple%2520variation%2520tailored%2520to%2520efficiently%2520sample%2520from%2520the%2520posterior%2520in%2520over-parameterized%2520networks%2520by%2520leveraging%2520the%2520low-dimensional%2520structure%2520of%2520loss%2520minima.%2520Building%2520on%2520this%252C%2520we%2520introduce%2520a%2520model%2520that%2520learns%2520a%2520deformation%2520of%2520the%2520parameter%2520space%252C%2520enabling%2520rapid%2520posterior%2520sampling%2520without%2520requiring%2520iterative%2520methods.%2520Empirical%2520results%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520competitive%2520posterior%2520approximations%2520with%2520improved%2520scalability%2520compared%2520to%2520recent%2520refinement%2520techniques.%2520These%2520contributions%2520provide%2520a%2520practical%2520alternative%2520for%2520Bayesian%2520inference%2520in%2520deep%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Walking%20on%20the%20Fiber%3A%20A%20Simple%20Geometric%20Approximation%20for%20Bayesian%20Neural%20Networks&entry.906535625=Alfredo%20Reichlin%20and%20Miguel%20Vasco%20and%20Danica%20Kragic&entry.1292438233=Bayesian%20Neural%20Networks%20provide%20a%20principled%20framework%20for%20uncertainty%20quantification%20by%20modeling%20the%20posterior%20distribution%20of%20network%20parameters.%20However%2C%20exact%20posterior%20inference%20is%20computationally%20intractable%2C%20and%20widely%20used%20approximations%20like%20the%20Laplace%20method%20struggle%20with%20scalability%20and%20posterior%20accuracy%20in%20modern%20deep%20networks.%20In%20this%20work%2C%20we%20revisit%20sampling%20techniques%20for%20posterior%20exploration%2C%20proposing%20a%20simple%20variation%20tailored%20to%20efficiently%20sample%20from%20the%20posterior%20in%20over-parameterized%20networks%20by%20leveraging%20the%20low-dimensional%20structure%20of%20loss%20minima.%20Building%20on%20this%2C%20we%20introduce%20a%20model%20that%20learns%20a%20deformation%20of%20the%20parameter%20space%2C%20enabling%20rapid%20posterior%20sampling%20without%20requiring%20iterative%20methods.%20Empirical%20results%20demonstrate%20that%20our%20approach%20achieves%20competitive%20posterior%20approximations%20with%20improved%20scalability%20compared%20to%20recent%20refinement%20techniques.%20These%20contributions%20provide%20a%20practical%20alternative%20for%20Bayesian%20inference%20in%20deep%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2512.01500v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


