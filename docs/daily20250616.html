<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250615.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "How Visual Representations Map to Language Feature Space in Multimodal\n  LLMs", "author": "Constantin Venhoff and Ashkan Khakzar and Sonia Joseph and Philip Torr and Neel Nanda", "abstract": "  Effective multimodal reasoning depends on the alignment of visual and\nlinguistic representations, yet the mechanisms by which vision-language models\n(VLMs) achieve this alignment remain poorly understood. We introduce a\nmethodological framework that deliberately maintains a frozen large language\nmodel (LLM) and a frozen vision transformer (ViT), connected solely by training\na linear adapter during visual instruction tuning. This design is fundamental\nto our approach: by keeping the language model frozen, we ensure it maintains\nits original language representations without adaptation to visual data.\nConsequently, the linear adapter must map visual features directly into the\nLLM's existing representational space rather than allowing the language model\nto develop specialized visual understanding through fine-tuning. Our\nexperimental design uniquely enables the use of pre-trained sparse autoencoders\n(SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned\nwith the unchanged language model and serve as a snapshot of the learned\nlanguage feature-representations. Through systematic analysis of SAE\nreconstruction error, sparsity patterns, and feature SAE descriptions, we\nreveal the layer-wise progression through which visual representations\ngradually align with language feature representations, converging in\nmiddle-to-later layers. This suggests a fundamental misalignment between ViT\noutputs and early LLM layers, raising important questions about whether current\nadapter-based architectures optimally facilitate cross-modal representation\nlearning.\n", "link": "http://arxiv.org/abs/2506.11976v1", "date": "2025-06-13", "relevancy": 3.136, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6206}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Visual%20Representations%20Map%20to%20Language%20Feature%20Space%20in%20Multimodal%0A%20%20LLMs&body=Title%3A%20How%20Visual%20Representations%20Map%20to%20Language%20Feature%20Space%20in%20Multimodal%0A%20%20LLMs%0AAuthor%3A%20Constantin%20Venhoff%20and%20Ashkan%20Khakzar%20and%20Sonia%20Joseph%20and%20Philip%20Torr%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Effective%20multimodal%20reasoning%20depends%20on%20the%20alignment%20of%20visual%20and%0Alinguistic%20representations%2C%20yet%20the%20mechanisms%20by%20which%20vision-language%20models%0A%28VLMs%29%20achieve%20this%20alignment%20remain%20poorly%20understood.%20We%20introduce%20a%0Amethodological%20framework%20that%20deliberately%20maintains%20a%20frozen%20large%20language%0Amodel%20%28LLM%29%20and%20a%20frozen%20vision%20transformer%20%28ViT%29%2C%20connected%20solely%20by%20training%0Aa%20linear%20adapter%20during%20visual%20instruction%20tuning.%20This%20design%20is%20fundamental%0Ato%20our%20approach%3A%20by%20keeping%20the%20language%20model%20frozen%2C%20we%20ensure%20it%20maintains%0Aits%20original%20language%20representations%20without%20adaptation%20to%20visual%20data.%0AConsequently%2C%20the%20linear%20adapter%20must%20map%20visual%20features%20directly%20into%20the%0ALLM%27s%20existing%20representational%20space%20rather%20than%20allowing%20the%20language%20model%0Ato%20develop%20specialized%20visual%20understanding%20through%20fine-tuning.%20Our%0Aexperimental%20design%20uniquely%20enables%20the%20use%20of%20pre-trained%20sparse%20autoencoders%0A%28SAEs%29%20of%20the%20LLM%20as%20analytical%20probes.%20These%20SAEs%20remain%20perfectly%20aligned%0Awith%20the%20unchanged%20language%20model%20and%20serve%20as%20a%20snapshot%20of%20the%20learned%0Alanguage%20feature-representations.%20Through%20systematic%20analysis%20of%20SAE%0Areconstruction%20error%2C%20sparsity%20patterns%2C%20and%20feature%20SAE%20descriptions%2C%20we%0Areveal%20the%20layer-wise%20progression%20through%20which%20visual%20representations%0Agradually%20align%20with%20language%20feature%20representations%2C%20converging%20in%0Amiddle-to-later%20layers.%20This%20suggests%20a%20fundamental%20misalignment%20between%20ViT%0Aoutputs%20and%20early%20LLM%20layers%2C%20raising%20important%20questions%20about%20whether%20current%0Aadapter-based%20architectures%20optimally%20facilitate%20cross-modal%20representation%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Visual%2520Representations%2520Map%2520to%2520Language%2520Feature%2520Space%2520in%2520Multimodal%250A%2520%2520LLMs%26entry.906535625%3DConstantin%2520Venhoff%2520and%2520Ashkan%2520Khakzar%2520and%2520Sonia%2520Joseph%2520and%2520Philip%2520Torr%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Effective%2520multimodal%2520reasoning%2520depends%2520on%2520the%2520alignment%2520of%2520visual%2520and%250Alinguistic%2520representations%252C%2520yet%2520the%2520mechanisms%2520by%2520which%2520vision-language%2520models%250A%2528VLMs%2529%2520achieve%2520this%2520alignment%2520remain%2520poorly%2520understood.%2520We%2520introduce%2520a%250Amethodological%2520framework%2520that%2520deliberately%2520maintains%2520a%2520frozen%2520large%2520language%250Amodel%2520%2528LLM%2529%2520and%2520a%2520frozen%2520vision%2520transformer%2520%2528ViT%2529%252C%2520connected%2520solely%2520by%2520training%250Aa%2520linear%2520adapter%2520during%2520visual%2520instruction%2520tuning.%2520This%2520design%2520is%2520fundamental%250Ato%2520our%2520approach%253A%2520by%2520keeping%2520the%2520language%2520model%2520frozen%252C%2520we%2520ensure%2520it%2520maintains%250Aits%2520original%2520language%2520representations%2520without%2520adaptation%2520to%2520visual%2520data.%250AConsequently%252C%2520the%2520linear%2520adapter%2520must%2520map%2520visual%2520features%2520directly%2520into%2520the%250ALLM%2527s%2520existing%2520representational%2520space%2520rather%2520than%2520allowing%2520the%2520language%2520model%250Ato%2520develop%2520specialized%2520visual%2520understanding%2520through%2520fine-tuning.%2520Our%250Aexperimental%2520design%2520uniquely%2520enables%2520the%2520use%2520of%2520pre-trained%2520sparse%2520autoencoders%250A%2528SAEs%2529%2520of%2520the%2520LLM%2520as%2520analytical%2520probes.%2520These%2520SAEs%2520remain%2520perfectly%2520aligned%250Awith%2520the%2520unchanged%2520language%2520model%2520and%2520serve%2520as%2520a%2520snapshot%2520of%2520the%2520learned%250Alanguage%2520feature-representations.%2520Through%2520systematic%2520analysis%2520of%2520SAE%250Areconstruction%2520error%252C%2520sparsity%2520patterns%252C%2520and%2520feature%2520SAE%2520descriptions%252C%2520we%250Areveal%2520the%2520layer-wise%2520progression%2520through%2520which%2520visual%2520representations%250Agradually%2520align%2520with%2520language%2520feature%2520representations%252C%2520converging%2520in%250Amiddle-to-later%2520layers.%2520This%2520suggests%2520a%2520fundamental%2520misalignment%2520between%2520ViT%250Aoutputs%2520and%2520early%2520LLM%2520layers%252C%2520raising%2520important%2520questions%2520about%2520whether%2520current%250Aadapter-based%2520architectures%2520optimally%2520facilitate%2520cross-modal%2520representation%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Visual%20Representations%20Map%20to%20Language%20Feature%20Space%20in%20Multimodal%0A%20%20LLMs&entry.906535625=Constantin%20Venhoff%20and%20Ashkan%20Khakzar%20and%20Sonia%20Joseph%20and%20Philip%20Torr%20and%20Neel%20Nanda&entry.1292438233=%20%20Effective%20multimodal%20reasoning%20depends%20on%20the%20alignment%20of%20visual%20and%0Alinguistic%20representations%2C%20yet%20the%20mechanisms%20by%20which%20vision-language%20models%0A%28VLMs%29%20achieve%20this%20alignment%20remain%20poorly%20understood.%20We%20introduce%20a%0Amethodological%20framework%20that%20deliberately%20maintains%20a%20frozen%20large%20language%0Amodel%20%28LLM%29%20and%20a%20frozen%20vision%20transformer%20%28ViT%29%2C%20connected%20solely%20by%20training%0Aa%20linear%20adapter%20during%20visual%20instruction%20tuning.%20This%20design%20is%20fundamental%0Ato%20our%20approach%3A%20by%20keeping%20the%20language%20model%20frozen%2C%20we%20ensure%20it%20maintains%0Aits%20original%20language%20representations%20without%20adaptation%20to%20visual%20data.%0AConsequently%2C%20the%20linear%20adapter%20must%20map%20visual%20features%20directly%20into%20the%0ALLM%27s%20existing%20representational%20space%20rather%20than%20allowing%20the%20language%20model%0Ato%20develop%20specialized%20visual%20understanding%20through%20fine-tuning.%20Our%0Aexperimental%20design%20uniquely%20enables%20the%20use%20of%20pre-trained%20sparse%20autoencoders%0A%28SAEs%29%20of%20the%20LLM%20as%20analytical%20probes.%20These%20SAEs%20remain%20perfectly%20aligned%0Awith%20the%20unchanged%20language%20model%20and%20serve%20as%20a%20snapshot%20of%20the%20learned%0Alanguage%20feature-representations.%20Through%20systematic%20analysis%20of%20SAE%0Areconstruction%20error%2C%20sparsity%20patterns%2C%20and%20feature%20SAE%20descriptions%2C%20we%0Areveal%20the%20layer-wise%20progression%20through%20which%20visual%20representations%0Agradually%20align%20with%20language%20feature%20representations%2C%20converging%20in%0Amiddle-to-later%20layers.%20This%20suggests%20a%20fundamental%20misalignment%20between%20ViT%0Aoutputs%20and%20early%20LLM%20layers%2C%20raising%20important%20questions%20about%20whether%20current%0Aadapter-based%20architectures%20optimally%20facilitate%20cross-modal%20representation%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11976v1&entry.124074799=Read"},
{"title": "Rethinking Multilingual Vision-Language Translation: Dataset,\n  Evaluation, and Adaptation", "author": "Xintong Wang and Jingheng Pan and Yixiao Liu and Xiaohu Zhao and Chenyang Lyu and Minghao Wu and Chris Biemann and Longyue Wang and Linlong Xu and Weihua Luo and Kaifu Zhang", "abstract": "  Vision-Language Translation (VLT) is a challenging task that requires\naccurately recognizing multilingual text embedded in images and translating it\ninto the target language with the support of visual context. While recent Large\nVision-Language Models (LVLMs) have demonstrated strong multilingual and visual\nunderstanding capabilities, there is a lack of systematic evaluation and\nunderstanding of their performance on VLT. In this work, we present a\ncomprehensive study of VLT from three key perspectives: data quality, model\narchitecture, and evaluation metrics. (1) We identify critical limitations in\nexisting datasets, particularly in semantic and cultural fidelity, and\nintroduce AibTrans -- a multilingual, parallel, human-verified dataset with\nOCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6\nstate-of-the-art open-source models across end-to-end and cascaded\narchitectures, revealing their OCR dependency and contrasting generation versus\nreasoning behaviors. (3) We propose Density-Aware Evaluation to address metric\nreliability issues under varying contextual complexity, introducing the DA\nScore as a more robust measure of translation quality. Building upon these\nfindings, we establish a new evaluation benchmark for VLT. Notably, we observe\nthat fine-tuning LVLMs on high-resource language pairs degrades cross-lingual\nperformance, and we propose a balanced multilingual fine-tuning strategy that\neffectively adapts LVLMs to VLT without sacrificing their generalization\nability.\n", "link": "http://arxiv.org/abs/2506.11820v1", "date": "2025-06-13", "relevancy": 2.9978, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.608}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Multilingual%20Vision-Language%20Translation%3A%20Dataset%2C%0A%20%20Evaluation%2C%20and%20Adaptation&body=Title%3A%20Rethinking%20Multilingual%20Vision-Language%20Translation%3A%20Dataset%2C%0A%20%20Evaluation%2C%20and%20Adaptation%0AAuthor%3A%20Xintong%20Wang%20and%20Jingheng%20Pan%20and%20Yixiao%20Liu%20and%20Xiaohu%20Zhao%20and%20Chenyang%20Lyu%20and%20Minghao%20Wu%20and%20Chris%20Biemann%20and%20Longyue%20Wang%20and%20Linlong%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%0AAbstract%3A%20%20%20Vision-Language%20Translation%20%28VLT%29%20is%20a%20challenging%20task%20that%20requires%0Aaccurately%20recognizing%20multilingual%20text%20embedded%20in%20images%20and%20translating%20it%0Ainto%20the%20target%20language%20with%20the%20support%20of%20visual%20context.%20While%20recent%20Large%0AVision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20strong%20multilingual%20and%20visual%0Aunderstanding%20capabilities%2C%20there%20is%20a%20lack%20of%20systematic%20evaluation%20and%0Aunderstanding%20of%20their%20performance%20on%20VLT.%20In%20this%20work%2C%20we%20present%20a%0Acomprehensive%20study%20of%20VLT%20from%20three%20key%20perspectives%3A%20data%20quality%2C%20model%0Aarchitecture%2C%20and%20evaluation%20metrics.%20%281%29%20We%20identify%20critical%20limitations%20in%0Aexisting%20datasets%2C%20particularly%20in%20semantic%20and%20cultural%20fidelity%2C%20and%0Aintroduce%20AibTrans%20--%20a%20multilingual%2C%20parallel%2C%20human-verified%20dataset%20with%0AOCR-corrected%20annotations.%20%282%29%20We%20benchmark%2011%20commercial%20LVLMs/LLMs%20and%206%0Astate-of-the-art%20open-source%20models%20across%20end-to-end%20and%20cascaded%0Aarchitectures%2C%20revealing%20their%20OCR%20dependency%20and%20contrasting%20generation%20versus%0Areasoning%20behaviors.%20%283%29%20We%20propose%20Density-Aware%20Evaluation%20to%20address%20metric%0Areliability%20issues%20under%20varying%20contextual%20complexity%2C%20introducing%20the%20DA%0AScore%20as%20a%20more%20robust%20measure%20of%20translation%20quality.%20Building%20upon%20these%0Afindings%2C%20we%20establish%20a%20new%20evaluation%20benchmark%20for%20VLT.%20Notably%2C%20we%20observe%0Athat%20fine-tuning%20LVLMs%20on%20high-resource%20language%20pairs%20degrades%20cross-lingual%0Aperformance%2C%20and%20we%20propose%20a%20balanced%20multilingual%20fine-tuning%20strategy%20that%0Aeffectively%20adapts%20LVLMs%20to%20VLT%20without%20sacrificing%20their%20generalization%0Aability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Multilingual%2520Vision-Language%2520Translation%253A%2520Dataset%252C%250A%2520%2520Evaluation%252C%2520and%2520Adaptation%26entry.906535625%3DXintong%2520Wang%2520and%2520Jingheng%2520Pan%2520and%2520Yixiao%2520Liu%2520and%2520Xiaohu%2520Zhao%2520and%2520Chenyang%2520Lyu%2520and%2520Minghao%2520Wu%2520and%2520Chris%2520Biemann%2520and%2520Longyue%2520Wang%2520and%2520Linlong%2520Xu%2520and%2520Weihua%2520Luo%2520and%2520Kaifu%2520Zhang%26entry.1292438233%3D%2520%2520Vision-Language%2520Translation%2520%2528VLT%2529%2520is%2520a%2520challenging%2520task%2520that%2520requires%250Aaccurately%2520recognizing%2520multilingual%2520text%2520embedded%2520in%2520images%2520and%2520translating%2520it%250Ainto%2520the%2520target%2520language%2520with%2520the%2520support%2520of%2520visual%2520context.%2520While%2520recent%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520strong%2520multilingual%2520and%2520visual%250Aunderstanding%2520capabilities%252C%2520there%2520is%2520a%2520lack%2520of%2520systematic%2520evaluation%2520and%250Aunderstanding%2520of%2520their%2520performance%2520on%2520VLT.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Acomprehensive%2520study%2520of%2520VLT%2520from%2520three%2520key%2520perspectives%253A%2520data%2520quality%252C%2520model%250Aarchitecture%252C%2520and%2520evaluation%2520metrics.%2520%25281%2529%2520We%2520identify%2520critical%2520limitations%2520in%250Aexisting%2520datasets%252C%2520particularly%2520in%2520semantic%2520and%2520cultural%2520fidelity%252C%2520and%250Aintroduce%2520AibTrans%2520--%2520a%2520multilingual%252C%2520parallel%252C%2520human-verified%2520dataset%2520with%250AOCR-corrected%2520annotations.%2520%25282%2529%2520We%2520benchmark%252011%2520commercial%2520LVLMs/LLMs%2520and%25206%250Astate-of-the-art%2520open-source%2520models%2520across%2520end-to-end%2520and%2520cascaded%250Aarchitectures%252C%2520revealing%2520their%2520OCR%2520dependency%2520and%2520contrasting%2520generation%2520versus%250Areasoning%2520behaviors.%2520%25283%2529%2520We%2520propose%2520Density-Aware%2520Evaluation%2520to%2520address%2520metric%250Areliability%2520issues%2520under%2520varying%2520contextual%2520complexity%252C%2520introducing%2520the%2520DA%250AScore%2520as%2520a%2520more%2520robust%2520measure%2520of%2520translation%2520quality.%2520Building%2520upon%2520these%250Afindings%252C%2520we%2520establish%2520a%2520new%2520evaluation%2520benchmark%2520for%2520VLT.%2520Notably%252C%2520we%2520observe%250Athat%2520fine-tuning%2520LVLMs%2520on%2520high-resource%2520language%2520pairs%2520degrades%2520cross-lingual%250Aperformance%252C%2520and%2520we%2520propose%2520a%2520balanced%2520multilingual%2520fine-tuning%2520strategy%2520that%250Aeffectively%2520adapts%2520LVLMs%2520to%2520VLT%2520without%2520sacrificing%2520their%2520generalization%250Aability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Multilingual%20Vision-Language%20Translation%3A%20Dataset%2C%0A%20%20Evaluation%2C%20and%20Adaptation&entry.906535625=Xintong%20Wang%20and%20Jingheng%20Pan%20and%20Yixiao%20Liu%20and%20Xiaohu%20Zhao%20and%20Chenyang%20Lyu%20and%20Minghao%20Wu%20and%20Chris%20Biemann%20and%20Longyue%20Wang%20and%20Linlong%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang&entry.1292438233=%20%20Vision-Language%20Translation%20%28VLT%29%20is%20a%20challenging%20task%20that%20requires%0Aaccurately%20recognizing%20multilingual%20text%20embedded%20in%20images%20and%20translating%20it%0Ainto%20the%20target%20language%20with%20the%20support%20of%20visual%20context.%20While%20recent%20Large%0AVision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20strong%20multilingual%20and%20visual%0Aunderstanding%20capabilities%2C%20there%20is%20a%20lack%20of%20systematic%20evaluation%20and%0Aunderstanding%20of%20their%20performance%20on%20VLT.%20In%20this%20work%2C%20we%20present%20a%0Acomprehensive%20study%20of%20VLT%20from%20three%20key%20perspectives%3A%20data%20quality%2C%20model%0Aarchitecture%2C%20and%20evaluation%20metrics.%20%281%29%20We%20identify%20critical%20limitations%20in%0Aexisting%20datasets%2C%20particularly%20in%20semantic%20and%20cultural%20fidelity%2C%20and%0Aintroduce%20AibTrans%20--%20a%20multilingual%2C%20parallel%2C%20human-verified%20dataset%20with%0AOCR-corrected%20annotations.%20%282%29%20We%20benchmark%2011%20commercial%20LVLMs/LLMs%20and%206%0Astate-of-the-art%20open-source%20models%20across%20end-to-end%20and%20cascaded%0Aarchitectures%2C%20revealing%20their%20OCR%20dependency%20and%20contrasting%20generation%20versus%0Areasoning%20behaviors.%20%283%29%20We%20propose%20Density-Aware%20Evaluation%20to%20address%20metric%0Areliability%20issues%20under%20varying%20contextual%20complexity%2C%20introducing%20the%20DA%0AScore%20as%20a%20more%20robust%20measure%20of%20translation%20quality.%20Building%20upon%20these%0Afindings%2C%20we%20establish%20a%20new%20evaluation%20benchmark%20for%20VLT.%20Notably%2C%20we%20observe%0Athat%20fine-tuning%20LVLMs%20on%20high-resource%20language%20pairs%20degrades%20cross-lingual%0Aperformance%2C%20and%20we%20propose%20a%20balanced%20multilingual%20fine-tuning%20strategy%20that%0Aeffectively%20adapts%20LVLMs%20to%20VLT%20without%20sacrificing%20their%20generalization%0Aability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11820v1&entry.124074799=Read"},
{"title": "Beyond the Visible: Multispectral Vision-Language Learning for Earth\n  Observation", "author": "Clive Tinashe Marimo and Benedikt Blumenstiel and Maximilian Nitsche and Johannes Jakubik and Thomas Brunschwiler", "abstract": "  Vision-language models for Earth observation (EO) typically rely on the\nvisual spectrum of data as the only model input, thus failing to leverage the\nrich spectral information available in the multispectral channels recorded by\nsatellites. Therefore, we introduce Llama3-MS-CLIP, the first vision-language\nmodel pre-trained with contrastive learning on a large-scale multispectral\ndataset and report on the performance gains due to the extended spectral range.\nFurthermore, we present the largest-to-date image-caption dataset for\nmultispectral data, consisting of one million Sentinel-2 samples and\ncorresponding textual descriptions generated using Llama3-LLaVA-Next and\nOverture Maps data. We develop a scalable captioning pipeline, which is\nvalidated by domain experts. We evaluate Llama3-MS-CLIP on multispectral\nzero-shot image classification and retrieval using three datasets of varying\ncomplexity. Our results demonstrate that Llama3-MS-CLIP significantly\noutperforms other RGB-based approaches, improving classification accuracy by\n+6.77% on average and retrieval performance by +4.63% mAP compared to the\nsecond-best model. Our results emphasize the relevance of multispectral\nvision-language learning. The image-caption dataset, code, and model weights\nare available at https://github.com/IBM/MS-CLIP.\n", "link": "http://arxiv.org/abs/2503.15969v2", "date": "2025-06-13", "relevancy": 2.9518, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5981}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5981}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Visible%3A%20Multispectral%20Vision-Language%20Learning%20for%20Earth%0A%20%20Observation&body=Title%3A%20Beyond%20the%20Visible%3A%20Multispectral%20Vision-Language%20Learning%20for%20Earth%0A%20%20Observation%0AAuthor%3A%20Clive%20Tinashe%20Marimo%20and%20Benedikt%20Blumenstiel%20and%20Maximilian%20Nitsche%20and%20Johannes%20Jakubik%20and%20Thomas%20Brunschwiler%0AAbstract%3A%20%20%20Vision-language%20models%20for%20Earth%20observation%20%28EO%29%20typically%20rely%20on%20the%0Avisual%20spectrum%20of%20data%20as%20the%20only%20model%20input%2C%20thus%20failing%20to%20leverage%20the%0Arich%20spectral%20information%20available%20in%20the%20multispectral%20channels%20recorded%20by%0Asatellites.%20Therefore%2C%20we%20introduce%20Llama3-MS-CLIP%2C%20the%20first%20vision-language%0Amodel%20pre-trained%20with%20contrastive%20learning%20on%20a%20large-scale%20multispectral%0Adataset%20and%20report%20on%20the%20performance%20gains%20due%20to%20the%20extended%20spectral%20range.%0AFurthermore%2C%20we%20present%20the%20largest-to-date%20image-caption%20dataset%20for%0Amultispectral%20data%2C%20consisting%20of%20one%20million%20Sentinel-2%20samples%20and%0Acorresponding%20textual%20descriptions%20generated%20using%20Llama3-LLaVA-Next%20and%0AOverture%20Maps%20data.%20We%20develop%20a%20scalable%20captioning%20pipeline%2C%20which%20is%0Avalidated%20by%20domain%20experts.%20We%20evaluate%20Llama3-MS-CLIP%20on%20multispectral%0Azero-shot%20image%20classification%20and%20retrieval%20using%20three%20datasets%20of%20varying%0Acomplexity.%20Our%20results%20demonstrate%20that%20Llama3-MS-CLIP%20significantly%0Aoutperforms%20other%20RGB-based%20approaches%2C%20improving%20classification%20accuracy%20by%0A%2B6.77%25%20on%20average%20and%20retrieval%20performance%20by%20%2B4.63%25%20mAP%20compared%20to%20the%0Asecond-best%20model.%20Our%20results%20emphasize%20the%20relevance%20of%20multispectral%0Avision-language%20learning.%20The%20image-caption%20dataset%2C%20code%2C%20and%20model%20weights%0Aare%20available%20at%20https%3A//github.com/IBM/MS-CLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.15969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Visible%253A%2520Multispectral%2520Vision-Language%2520Learning%2520for%2520Earth%250A%2520%2520Observation%26entry.906535625%3DClive%2520Tinashe%2520Marimo%2520and%2520Benedikt%2520Blumenstiel%2520and%2520Maximilian%2520Nitsche%2520and%2520Johannes%2520Jakubik%2520and%2520Thomas%2520Brunschwiler%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520for%2520Earth%2520observation%2520%2528EO%2529%2520typically%2520rely%2520on%2520the%250Avisual%2520spectrum%2520of%2520data%2520as%2520the%2520only%2520model%2520input%252C%2520thus%2520failing%2520to%2520leverage%2520the%250Arich%2520spectral%2520information%2520available%2520in%2520the%2520multispectral%2520channels%2520recorded%2520by%250Asatellites.%2520Therefore%252C%2520we%2520introduce%2520Llama3-MS-CLIP%252C%2520the%2520first%2520vision-language%250Amodel%2520pre-trained%2520with%2520contrastive%2520learning%2520on%2520a%2520large-scale%2520multispectral%250Adataset%2520and%2520report%2520on%2520the%2520performance%2520gains%2520due%2520to%2520the%2520extended%2520spectral%2520range.%250AFurthermore%252C%2520we%2520present%2520the%2520largest-to-date%2520image-caption%2520dataset%2520for%250Amultispectral%2520data%252C%2520consisting%2520of%2520one%2520million%2520Sentinel-2%2520samples%2520and%250Acorresponding%2520textual%2520descriptions%2520generated%2520using%2520Llama3-LLaVA-Next%2520and%250AOverture%2520Maps%2520data.%2520We%2520develop%2520a%2520scalable%2520captioning%2520pipeline%252C%2520which%2520is%250Avalidated%2520by%2520domain%2520experts.%2520We%2520evaluate%2520Llama3-MS-CLIP%2520on%2520multispectral%250Azero-shot%2520image%2520classification%2520and%2520retrieval%2520using%2520three%2520datasets%2520of%2520varying%250Acomplexity.%2520Our%2520results%2520demonstrate%2520that%2520Llama3-MS-CLIP%2520significantly%250Aoutperforms%2520other%2520RGB-based%2520approaches%252C%2520improving%2520classification%2520accuracy%2520by%250A%252B6.77%2525%2520on%2520average%2520and%2520retrieval%2520performance%2520by%2520%252B4.63%2525%2520mAP%2520compared%2520to%2520the%250Asecond-best%2520model.%2520Our%2520results%2520emphasize%2520the%2520relevance%2520of%2520multispectral%250Avision-language%2520learning.%2520The%2520image-caption%2520dataset%252C%2520code%252C%2520and%2520model%2520weights%250Aare%2520available%2520at%2520https%253A//github.com/IBM/MS-CLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Visible%3A%20Multispectral%20Vision-Language%20Learning%20for%20Earth%0A%20%20Observation&entry.906535625=Clive%20Tinashe%20Marimo%20and%20Benedikt%20Blumenstiel%20and%20Maximilian%20Nitsche%20and%20Johannes%20Jakubik%20and%20Thomas%20Brunschwiler&entry.1292438233=%20%20Vision-language%20models%20for%20Earth%20observation%20%28EO%29%20typically%20rely%20on%20the%0Avisual%20spectrum%20of%20data%20as%20the%20only%20model%20input%2C%20thus%20failing%20to%20leverage%20the%0Arich%20spectral%20information%20available%20in%20the%20multispectral%20channels%20recorded%20by%0Asatellites.%20Therefore%2C%20we%20introduce%20Llama3-MS-CLIP%2C%20the%20first%20vision-language%0Amodel%20pre-trained%20with%20contrastive%20learning%20on%20a%20large-scale%20multispectral%0Adataset%20and%20report%20on%20the%20performance%20gains%20due%20to%20the%20extended%20spectral%20range.%0AFurthermore%2C%20we%20present%20the%20largest-to-date%20image-caption%20dataset%20for%0Amultispectral%20data%2C%20consisting%20of%20one%20million%20Sentinel-2%20samples%20and%0Acorresponding%20textual%20descriptions%20generated%20using%20Llama3-LLaVA-Next%20and%0AOverture%20Maps%20data.%20We%20develop%20a%20scalable%20captioning%20pipeline%2C%20which%20is%0Avalidated%20by%20domain%20experts.%20We%20evaluate%20Llama3-MS-CLIP%20on%20multispectral%0Azero-shot%20image%20classification%20and%20retrieval%20using%20three%20datasets%20of%20varying%0Acomplexity.%20Our%20results%20demonstrate%20that%20Llama3-MS-CLIP%20significantly%0Aoutperforms%20other%20RGB-based%20approaches%2C%20improving%20classification%20accuracy%20by%0A%2B6.77%25%20on%20average%20and%20retrieval%20performance%20by%20%2B4.63%25%20mAP%20compared%20to%20the%0Asecond-best%20model.%20Our%20results%20emphasize%20the%20relevance%20of%20multispectral%0Avision-language%20learning.%20The%20image-caption%20dataset%2C%20code%2C%20and%20model%20weights%0Aare%20available%20at%20https%3A//github.com/IBM/MS-CLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.15969v2&entry.124074799=Read"},
{"title": "MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual\n  Space", "author": "Anshul Singh and Chris Biemann and Jan Strich", "abstract": "  Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ninterpreting visual layouts and text. However, a significant challenge remains\nin their ability to interpret robustly and reason over multi-tabular data\npresented as images, a common occurrence in real-world scenarios like web pages\nand digital documents. Existing benchmarks typically address single tables or\nnon-visual data (text/structured). This leaves a critical gap: they don't\nassess the ability to parse diverse table images, correlate information across\nthem, and perform multi-hop reasoning on the combined visual data. We introduce\nMTabVQA, a novel benchmark specifically designed for multi-tabular visual\nquestion answering to bridge that gap. MTabVQA comprises 3,745 complex\nquestion-answer pairs that necessitate multi-hop reasoning across several\nvisually rendered table images. We provide extensive benchmark results for\nstate-of-the-art VLMs on MTabVQA, revealing significant performance\nlimitations. We further investigate post-training techniques to enhance these\nreasoning abilities and release MTabVQA-Instruct, a large-scale\ninstruction-tuning dataset. Our experiments show that fine-tuning VLMs with\nMTabVQA-Instruct substantially improves their performance on visual\nmulti-tabular reasoning. Code and dataset\n(https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval) are available online\n(https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E).\n", "link": "http://arxiv.org/abs/2506.11684v1", "date": "2025-06-13", "relevancy": 2.9132, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTabVQA%3A%20Evaluating%20Multi-Tabular%20Reasoning%20of%20Language%20Models%20in%20Visual%0A%20%20Space&body=Title%3A%20MTabVQA%3A%20Evaluating%20Multi-Tabular%20Reasoning%20of%20Language%20Models%20in%20Visual%0A%20%20Space%0AAuthor%3A%20Anshul%20Singh%20and%20Chris%20Biemann%20and%20Jan%20Strich%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Ainterpreting%20visual%20layouts%20and%20text.%20However%2C%20a%20significant%20challenge%20remains%0Ain%20their%20ability%20to%20interpret%20robustly%20and%20reason%20over%20multi-tabular%20data%0Apresented%20as%20images%2C%20a%20common%20occurrence%20in%20real-world%20scenarios%20like%20web%20pages%0Aand%20digital%20documents.%20Existing%20benchmarks%20typically%20address%20single%20tables%20or%0Anon-visual%20data%20%28text/structured%29.%20This%20leaves%20a%20critical%20gap%3A%20they%20don%27t%0Aassess%20the%20ability%20to%20parse%20diverse%20table%20images%2C%20correlate%20information%20across%0Athem%2C%20and%20perform%20multi-hop%20reasoning%20on%20the%20combined%20visual%20data.%20We%20introduce%0AMTabVQA%2C%20a%20novel%20benchmark%20specifically%20designed%20for%20multi-tabular%20visual%0Aquestion%20answering%20to%20bridge%20that%20gap.%20MTabVQA%20comprises%203%2C745%20complex%0Aquestion-answer%20pairs%20that%20necessitate%20multi-hop%20reasoning%20across%20several%0Avisually%20rendered%20table%20images.%20We%20provide%20extensive%20benchmark%20results%20for%0Astate-of-the-art%20VLMs%20on%20MTabVQA%2C%20revealing%20significant%20performance%0Alimitations.%20We%20further%20investigate%20post-training%20techniques%20to%20enhance%20these%0Areasoning%20abilities%20and%20release%20MTabVQA-Instruct%2C%20a%20large-scale%0Ainstruction-tuning%20dataset.%20Our%20experiments%20show%20that%20fine-tuning%20VLMs%20with%0AMTabVQA-Instruct%20substantially%20improves%20their%20performance%20on%20visual%0Amulti-tabular%20reasoning.%20Code%20and%20dataset%0A%28https%3A//huggingface.co/datasets/mtabvqa/MTabVQA-Eval%29%20are%20available%20online%0A%28https%3A//anonymous.4open.science/r/MTabVQA-EMNLP-B16E%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTabVQA%253A%2520Evaluating%2520Multi-Tabular%2520Reasoning%2520of%2520Language%2520Models%2520in%2520Visual%250A%2520%2520Space%26entry.906535625%3DAnshul%2520Singh%2520and%2520Chris%2520Biemann%2520and%2520Jan%2520Strich%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%250Ainterpreting%2520visual%2520layouts%2520and%2520text.%2520However%252C%2520a%2520significant%2520challenge%2520remains%250Ain%2520their%2520ability%2520to%2520interpret%2520robustly%2520and%2520reason%2520over%2520multi-tabular%2520data%250Apresented%2520as%2520images%252C%2520a%2520common%2520occurrence%2520in%2520real-world%2520scenarios%2520like%2520web%2520pages%250Aand%2520digital%2520documents.%2520Existing%2520benchmarks%2520typically%2520address%2520single%2520tables%2520or%250Anon-visual%2520data%2520%2528text/structured%2529.%2520This%2520leaves%2520a%2520critical%2520gap%253A%2520they%2520don%2527t%250Aassess%2520the%2520ability%2520to%2520parse%2520diverse%2520table%2520images%252C%2520correlate%2520information%2520across%250Athem%252C%2520and%2520perform%2520multi-hop%2520reasoning%2520on%2520the%2520combined%2520visual%2520data.%2520We%2520introduce%250AMTabVQA%252C%2520a%2520novel%2520benchmark%2520specifically%2520designed%2520for%2520multi-tabular%2520visual%250Aquestion%2520answering%2520to%2520bridge%2520that%2520gap.%2520MTabVQA%2520comprises%25203%252C745%2520complex%250Aquestion-answer%2520pairs%2520that%2520necessitate%2520multi-hop%2520reasoning%2520across%2520several%250Avisually%2520rendered%2520table%2520images.%2520We%2520provide%2520extensive%2520benchmark%2520results%2520for%250Astate-of-the-art%2520VLMs%2520on%2520MTabVQA%252C%2520revealing%2520significant%2520performance%250Alimitations.%2520We%2520further%2520investigate%2520post-training%2520techniques%2520to%2520enhance%2520these%250Areasoning%2520abilities%2520and%2520release%2520MTabVQA-Instruct%252C%2520a%2520large-scale%250Ainstruction-tuning%2520dataset.%2520Our%2520experiments%2520show%2520that%2520fine-tuning%2520VLMs%2520with%250AMTabVQA-Instruct%2520substantially%2520improves%2520their%2520performance%2520on%2520visual%250Amulti-tabular%2520reasoning.%2520Code%2520and%2520dataset%250A%2528https%253A//huggingface.co/datasets/mtabvqa/MTabVQA-Eval%2529%2520are%2520available%2520online%250A%2528https%253A//anonymous.4open.science/r/MTabVQA-EMNLP-B16E%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTabVQA%3A%20Evaluating%20Multi-Tabular%20Reasoning%20of%20Language%20Models%20in%20Visual%0A%20%20Space&entry.906535625=Anshul%20Singh%20and%20Chris%20Biemann%20and%20Jan%20Strich&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Ainterpreting%20visual%20layouts%20and%20text.%20However%2C%20a%20significant%20challenge%20remains%0Ain%20their%20ability%20to%20interpret%20robustly%20and%20reason%20over%20multi-tabular%20data%0Apresented%20as%20images%2C%20a%20common%20occurrence%20in%20real-world%20scenarios%20like%20web%20pages%0Aand%20digital%20documents.%20Existing%20benchmarks%20typically%20address%20single%20tables%20or%0Anon-visual%20data%20%28text/structured%29.%20This%20leaves%20a%20critical%20gap%3A%20they%20don%27t%0Aassess%20the%20ability%20to%20parse%20diverse%20table%20images%2C%20correlate%20information%20across%0Athem%2C%20and%20perform%20multi-hop%20reasoning%20on%20the%20combined%20visual%20data.%20We%20introduce%0AMTabVQA%2C%20a%20novel%20benchmark%20specifically%20designed%20for%20multi-tabular%20visual%0Aquestion%20answering%20to%20bridge%20that%20gap.%20MTabVQA%20comprises%203%2C745%20complex%0Aquestion-answer%20pairs%20that%20necessitate%20multi-hop%20reasoning%20across%20several%0Avisually%20rendered%20table%20images.%20We%20provide%20extensive%20benchmark%20results%20for%0Astate-of-the-art%20VLMs%20on%20MTabVQA%2C%20revealing%20significant%20performance%0Alimitations.%20We%20further%20investigate%20post-training%20techniques%20to%20enhance%20these%0Areasoning%20abilities%20and%20release%20MTabVQA-Instruct%2C%20a%20large-scale%0Ainstruction-tuning%20dataset.%20Our%20experiments%20show%20that%20fine-tuning%20VLMs%20with%0AMTabVQA-Instruct%20substantially%20improves%20their%20performance%20on%20visual%0Amulti-tabular%20reasoning.%20Code%20and%20dataset%0A%28https%3A//huggingface.co/datasets/mtabvqa/MTabVQA-Eval%29%20are%20available%20online%0A%28https%3A//anonymous.4open.science/r/MTabVQA-EMNLP-B16E%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11684v1&entry.124074799=Read"},
{"title": "Affogato: Learning Open-Vocabulary Affordance Grounding with Automated\n  Data Generation at Scale", "author": "Junha Lee and Eunha Park and Chunghyun Park and Dahyun Kang and Minsu Cho", "abstract": "  Affordance grounding-localizing object regions based on natural language\ndescriptions of interactions-is a critical challenge for enabling intelligent\nagents to understand and interact with their environments. However, this task\nremains challenging due to the need for fine-grained part-level localization,\nthe ambiguity arising from multiple valid interaction regions, and the scarcity\nof large-scale datasets. In this work, we introduce Affogato, a large-scale\nbenchmark comprising 150K instances, annotated with open-vocabulary text\ndescriptions and corresponding 3D affordance heatmaps across a diverse set of\nobjects and interactions. Building on this benchmark, we develop simple yet\neffective vision-language models that leverage pretrained part-aware vision\nbackbones and a text-conditional heatmap decoder. Our models trained with the\nAffogato dataset achieve promising performance on the existing 2D and 3D\nbenchmarks, and notably, exhibit effectiveness in open-vocabulary cross-domain\ngeneralization. The Affogato dataset is shared in public:\nhttps://huggingface.co/datasets/project-affogato/affogato\n", "link": "http://arxiv.org/abs/2506.12009v1", "date": "2025-06-13", "relevancy": 2.8488, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Affogato%3A%20Learning%20Open-Vocabulary%20Affordance%20Grounding%20with%20Automated%0A%20%20Data%20Generation%20at%20Scale&body=Title%3A%20Affogato%3A%20Learning%20Open-Vocabulary%20Affordance%20Grounding%20with%20Automated%0A%20%20Data%20Generation%20at%20Scale%0AAuthor%3A%20Junha%20Lee%20and%20Eunha%20Park%20and%20Chunghyun%20Park%20and%20Dahyun%20Kang%20and%20Minsu%20Cho%0AAbstract%3A%20%20%20Affordance%20grounding-localizing%20object%20regions%20based%20on%20natural%20language%0Adescriptions%20of%20interactions-is%20a%20critical%20challenge%20for%20enabling%20intelligent%0Aagents%20to%20understand%20and%20interact%20with%20their%20environments.%20However%2C%20this%20task%0Aremains%20challenging%20due%20to%20the%20need%20for%20fine-grained%20part-level%20localization%2C%0Athe%20ambiguity%20arising%20from%20multiple%20valid%20interaction%20regions%2C%20and%20the%20scarcity%0Aof%20large-scale%20datasets.%20In%20this%20work%2C%20we%20introduce%20Affogato%2C%20a%20large-scale%0Abenchmark%20comprising%20150K%20instances%2C%20annotated%20with%20open-vocabulary%20text%0Adescriptions%20and%20corresponding%203D%20affordance%20heatmaps%20across%20a%20diverse%20set%20of%0Aobjects%20and%20interactions.%20Building%20on%20this%20benchmark%2C%20we%20develop%20simple%20yet%0Aeffective%20vision-language%20models%20that%20leverage%20pretrained%20part-aware%20vision%0Abackbones%20and%20a%20text-conditional%20heatmap%20decoder.%20Our%20models%20trained%20with%20the%0AAffogato%20dataset%20achieve%20promising%20performance%20on%20the%20existing%202D%20and%203D%0Abenchmarks%2C%20and%20notably%2C%20exhibit%20effectiveness%20in%20open-vocabulary%20cross-domain%0Ageneralization.%20The%20Affogato%20dataset%20is%20shared%20in%20public%3A%0Ahttps%3A//huggingface.co/datasets/project-affogato/affogato%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAffogato%253A%2520Learning%2520Open-Vocabulary%2520Affordance%2520Grounding%2520with%2520Automated%250A%2520%2520Data%2520Generation%2520at%2520Scale%26entry.906535625%3DJunha%2520Lee%2520and%2520Eunha%2520Park%2520and%2520Chunghyun%2520Park%2520and%2520Dahyun%2520Kang%2520and%2520Minsu%2520Cho%26entry.1292438233%3D%2520%2520Affordance%2520grounding-localizing%2520object%2520regions%2520based%2520on%2520natural%2520language%250Adescriptions%2520of%2520interactions-is%2520a%2520critical%2520challenge%2520for%2520enabling%2520intelligent%250Aagents%2520to%2520understand%2520and%2520interact%2520with%2520their%2520environments.%2520However%252C%2520this%2520task%250Aremains%2520challenging%2520due%2520to%2520the%2520need%2520for%2520fine-grained%2520part-level%2520localization%252C%250Athe%2520ambiguity%2520arising%2520from%2520multiple%2520valid%2520interaction%2520regions%252C%2520and%2520the%2520scarcity%250Aof%2520large-scale%2520datasets.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Affogato%252C%2520a%2520large-scale%250Abenchmark%2520comprising%2520150K%2520instances%252C%2520annotated%2520with%2520open-vocabulary%2520text%250Adescriptions%2520and%2520corresponding%25203D%2520affordance%2520heatmaps%2520across%2520a%2520diverse%2520set%2520of%250Aobjects%2520and%2520interactions.%2520Building%2520on%2520this%2520benchmark%252C%2520we%2520develop%2520simple%2520yet%250Aeffective%2520vision-language%2520models%2520that%2520leverage%2520pretrained%2520part-aware%2520vision%250Abackbones%2520and%2520a%2520text-conditional%2520heatmap%2520decoder.%2520Our%2520models%2520trained%2520with%2520the%250AAffogato%2520dataset%2520achieve%2520promising%2520performance%2520on%2520the%2520existing%25202D%2520and%25203D%250Abenchmarks%252C%2520and%2520notably%252C%2520exhibit%2520effectiveness%2520in%2520open-vocabulary%2520cross-domain%250Ageneralization.%2520The%2520Affogato%2520dataset%2520is%2520shared%2520in%2520public%253A%250Ahttps%253A//huggingface.co/datasets/project-affogato/affogato%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Affogato%3A%20Learning%20Open-Vocabulary%20Affordance%20Grounding%20with%20Automated%0A%20%20Data%20Generation%20at%20Scale&entry.906535625=Junha%20Lee%20and%20Eunha%20Park%20and%20Chunghyun%20Park%20and%20Dahyun%20Kang%20and%20Minsu%20Cho&entry.1292438233=%20%20Affordance%20grounding-localizing%20object%20regions%20based%20on%20natural%20language%0Adescriptions%20of%20interactions-is%20a%20critical%20challenge%20for%20enabling%20intelligent%0Aagents%20to%20understand%20and%20interact%20with%20their%20environments.%20However%2C%20this%20task%0Aremains%20challenging%20due%20to%20the%20need%20for%20fine-grained%20part-level%20localization%2C%0Athe%20ambiguity%20arising%20from%20multiple%20valid%20interaction%20regions%2C%20and%20the%20scarcity%0Aof%20large-scale%20datasets.%20In%20this%20work%2C%20we%20introduce%20Affogato%2C%20a%20large-scale%0Abenchmark%20comprising%20150K%20instances%2C%20annotated%20with%20open-vocabulary%20text%0Adescriptions%20and%20corresponding%203D%20affordance%20heatmaps%20across%20a%20diverse%20set%20of%0Aobjects%20and%20interactions.%20Building%20on%20this%20benchmark%2C%20we%20develop%20simple%20yet%0Aeffective%20vision-language%20models%20that%20leverage%20pretrained%20part-aware%20vision%0Abackbones%20and%20a%20text-conditional%20heatmap%20decoder.%20Our%20models%20trained%20with%20the%0AAffogato%20dataset%20achieve%20promising%20performance%20on%20the%20existing%202D%20and%203D%0Abenchmarks%2C%20and%20notably%2C%20exhibit%20effectiveness%20in%20open-vocabulary%20cross-domain%0Ageneralization.%20The%20Affogato%20dataset%20is%20shared%20in%20public%3A%0Ahttps%3A//huggingface.co/datasets/project-affogato/affogato%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12009v1&entry.124074799=Read"},
{"title": "HandS3C: 3D Hand Mesh Reconstruction with State Space Spatial Channel\n  Attention from RGB images", "author": "Zixun Jiao and Xihan Wang and Zhaoqiang Xia and Lianhe Shao and Quanli Gao", "abstract": "  Reconstructing the hand mesh from one single RGB image is a challenging task\nbecause hands are often occluded by other objects. Most previous works attempt\nto explore more additional information and adopt attention mechanisms for\nimproving 3D reconstruction performance, while it would increase computational\ncomplexity simultaneously. To achieve a performance-reserving architecture with\nhigh computational efficiency, in this work, we propose a simple but effective\n3D hand mesh reconstruction network (i.e., HandS3C), which is the first time to\nincorporate state space model into the task of hand mesh reconstruction. In the\nnetwork, we design a novel state-space spatial-channel attention module that\nextends the effective receptive field, extracts hand features in the spatial\ndimension, and enhances regional features of hands in the channel dimension.\nThis helps to reconstruct a complete and detailed hand mesh. Extensive\nexperiments conducted on well-known datasets facing heavy occlusions (such as\nFREIHAND, DEXYCB, and HO3D) demonstrate that our proposed HandS3C achieves\nstate-of-the-art performance while maintaining a minimal parameters.\n", "link": "http://arxiv.org/abs/2405.01066v4", "date": "2025-06-13", "relevancy": 2.7855, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5706}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5532}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HandS3C%3A%203D%20Hand%20Mesh%20Reconstruction%20with%20State%20Space%20Spatial%20Channel%0A%20%20Attention%20from%20RGB%20images&body=Title%3A%20HandS3C%3A%203D%20Hand%20Mesh%20Reconstruction%20with%20State%20Space%20Spatial%20Channel%0A%20%20Attention%20from%20RGB%20images%0AAuthor%3A%20Zixun%20Jiao%20and%20Xihan%20Wang%20and%20Zhaoqiang%20Xia%20and%20Lianhe%20Shao%20and%20Quanli%20Gao%0AAbstract%3A%20%20%20Reconstructing%20the%20hand%20mesh%20from%20one%20single%20RGB%20image%20is%20a%20challenging%20task%0Abecause%20hands%20are%20often%20occluded%20by%20other%20objects.%20Most%20previous%20works%20attempt%0Ato%20explore%20more%20additional%20information%20and%20adopt%20attention%20mechanisms%20for%0Aimproving%203D%20reconstruction%20performance%2C%20while%20it%20would%20increase%20computational%0Acomplexity%20simultaneously.%20To%20achieve%20a%20performance-reserving%20architecture%20with%0Ahigh%20computational%20efficiency%2C%20in%20this%20work%2C%20we%20propose%20a%20simple%20but%20effective%0A3D%20hand%20mesh%20reconstruction%20network%20%28i.e.%2C%20HandS3C%29%2C%20which%20is%20the%20first%20time%20to%0Aincorporate%20state%20space%20model%20into%20the%20task%20of%20hand%20mesh%20reconstruction.%20In%20the%0Anetwork%2C%20we%20design%20a%20novel%20state-space%20spatial-channel%20attention%20module%20that%0Aextends%20the%20effective%20receptive%20field%2C%20extracts%20hand%20features%20in%20the%20spatial%0Adimension%2C%20and%20enhances%20regional%20features%20of%20hands%20in%20the%20channel%20dimension.%0AThis%20helps%20to%20reconstruct%20a%20complete%20and%20detailed%20hand%20mesh.%20Extensive%0Aexperiments%20conducted%20on%20well-known%20datasets%20facing%20heavy%20occlusions%20%28such%20as%0AFREIHAND%2C%20DEXYCB%2C%20and%20HO3D%29%20demonstrate%20that%20our%20proposed%20HandS3C%20achieves%0Astate-of-the-art%20performance%20while%20maintaining%20a%20minimal%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01066v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHandS3C%253A%25203D%2520Hand%2520Mesh%2520Reconstruction%2520with%2520State%2520Space%2520Spatial%2520Channel%250A%2520%2520Attention%2520from%2520RGB%2520images%26entry.906535625%3DZixun%2520Jiao%2520and%2520Xihan%2520Wang%2520and%2520Zhaoqiang%2520Xia%2520and%2520Lianhe%2520Shao%2520and%2520Quanli%2520Gao%26entry.1292438233%3D%2520%2520Reconstructing%2520the%2520hand%2520mesh%2520from%2520one%2520single%2520RGB%2520image%2520is%2520a%2520challenging%2520task%250Abecause%2520hands%2520are%2520often%2520occluded%2520by%2520other%2520objects.%2520Most%2520previous%2520works%2520attempt%250Ato%2520explore%2520more%2520additional%2520information%2520and%2520adopt%2520attention%2520mechanisms%2520for%250Aimproving%25203D%2520reconstruction%2520performance%252C%2520while%2520it%2520would%2520increase%2520computational%250Acomplexity%2520simultaneously.%2520To%2520achieve%2520a%2520performance-reserving%2520architecture%2520with%250Ahigh%2520computational%2520efficiency%252C%2520in%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520but%2520effective%250A3D%2520hand%2520mesh%2520reconstruction%2520network%2520%2528i.e.%252C%2520HandS3C%2529%252C%2520which%2520is%2520the%2520first%2520time%2520to%250Aincorporate%2520state%2520space%2520model%2520into%2520the%2520task%2520of%2520hand%2520mesh%2520reconstruction.%2520In%2520the%250Anetwork%252C%2520we%2520design%2520a%2520novel%2520state-space%2520spatial-channel%2520attention%2520module%2520that%250Aextends%2520the%2520effective%2520receptive%2520field%252C%2520extracts%2520hand%2520features%2520in%2520the%2520spatial%250Adimension%252C%2520and%2520enhances%2520regional%2520features%2520of%2520hands%2520in%2520the%2520channel%2520dimension.%250AThis%2520helps%2520to%2520reconstruct%2520a%2520complete%2520and%2520detailed%2520hand%2520mesh.%2520Extensive%250Aexperiments%2520conducted%2520on%2520well-known%2520datasets%2520facing%2520heavy%2520occlusions%2520%2528such%2520as%250AFREIHAND%252C%2520DEXYCB%252C%2520and%2520HO3D%2529%2520demonstrate%2520that%2520our%2520proposed%2520HandS3C%2520achieves%250Astate-of-the-art%2520performance%2520while%2520maintaining%2520a%2520minimal%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01066v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HandS3C%3A%203D%20Hand%20Mesh%20Reconstruction%20with%20State%20Space%20Spatial%20Channel%0A%20%20Attention%20from%20RGB%20images&entry.906535625=Zixun%20Jiao%20and%20Xihan%20Wang%20and%20Zhaoqiang%20Xia%20and%20Lianhe%20Shao%20and%20Quanli%20Gao&entry.1292438233=%20%20Reconstructing%20the%20hand%20mesh%20from%20one%20single%20RGB%20image%20is%20a%20challenging%20task%0Abecause%20hands%20are%20often%20occluded%20by%20other%20objects.%20Most%20previous%20works%20attempt%0Ato%20explore%20more%20additional%20information%20and%20adopt%20attention%20mechanisms%20for%0Aimproving%203D%20reconstruction%20performance%2C%20while%20it%20would%20increase%20computational%0Acomplexity%20simultaneously.%20To%20achieve%20a%20performance-reserving%20architecture%20with%0Ahigh%20computational%20efficiency%2C%20in%20this%20work%2C%20we%20propose%20a%20simple%20but%20effective%0A3D%20hand%20mesh%20reconstruction%20network%20%28i.e.%2C%20HandS3C%29%2C%20which%20is%20the%20first%20time%20to%0Aincorporate%20state%20space%20model%20into%20the%20task%20of%20hand%20mesh%20reconstruction.%20In%20the%0Anetwork%2C%20we%20design%20a%20novel%20state-space%20spatial-channel%20attention%20module%20that%0Aextends%20the%20effective%20receptive%20field%2C%20extracts%20hand%20features%20in%20the%20spatial%0Adimension%2C%20and%20enhances%20regional%20features%20of%20hands%20in%20the%20channel%20dimension.%0AThis%20helps%20to%20reconstruct%20a%20complete%20and%20detailed%20hand%20mesh.%20Extensive%0Aexperiments%20conducted%20on%20well-known%20datasets%20facing%20heavy%20occlusions%20%28such%20as%0AFREIHAND%2C%20DEXYCB%2C%20and%20HO3D%29%20demonstrate%20that%20our%20proposed%20HandS3C%20achieves%0Astate-of-the-art%20performance%20while%20maintaining%20a%20minimal%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01066v4&entry.124074799=Read"},
{"title": "Scaling Human Activity Recognition: A Comparative Evaluation of\n  Synthetic Data Generation and Augmentation Techniques", "author": "Zikang Leng and Archith Iyer and Thomas Pl\u00f6tz", "abstract": "  Human activity recognition (HAR) is often limited by the scarcity of labeled\ndatasets due to the high cost and complexity of real-world data collection. To\nmitigate this, recent work has explored generating virtual inertial measurement\nunit (IMU) data via cross-modality transfer. While video-based and\nlanguage-based pipelines have each shown promise, they differ in assumptions\nand computational cost. Moreover, their effectiveness relative to traditional\nsensor-level data augmentation remains unclear. In this paper, we present a\ndirect comparison between these two virtual IMU generation approaches against\nclassical data augmentation techniques. We construct a large-scale virtual IMU\ndataset spanning 100 diverse activities from Kinetics-400 and simulate sensor\nsignals at 22 body locations. The three data generation strategies are\nevaluated on benchmark HAR datasets (UTD-MHAD, PAMAP2, HAD-AW) using four\npopular models. Results show that virtual IMU data significantly improves\nperformance over real or augmented data alone, particularly under limited-data\nconditions. We offer practical guidance on choosing data generation strategies\nand highlight the distinct advantages and disadvantages of each approach.\n", "link": "http://arxiv.org/abs/2506.07612v2", "date": "2025-06-13", "relevancy": 2.776, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5597}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5561}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Human%20Activity%20Recognition%3A%20A%20Comparative%20Evaluation%20of%0A%20%20Synthetic%20Data%20Generation%20and%20Augmentation%20Techniques&body=Title%3A%20Scaling%20Human%20Activity%20Recognition%3A%20A%20Comparative%20Evaluation%20of%0A%20%20Synthetic%20Data%20Generation%20and%20Augmentation%20Techniques%0AAuthor%3A%20Zikang%20Leng%20and%20Archith%20Iyer%20and%20Thomas%20Pl%C3%B6tz%0AAbstract%3A%20%20%20Human%20activity%20recognition%20%28HAR%29%20is%20often%20limited%20by%20the%20scarcity%20of%20labeled%0Adatasets%20due%20to%20the%20high%20cost%20and%20complexity%20of%20real-world%20data%20collection.%20To%0Amitigate%20this%2C%20recent%20work%20has%20explored%20generating%20virtual%20inertial%20measurement%0Aunit%20%28IMU%29%20data%20via%20cross-modality%20transfer.%20While%20video-based%20and%0Alanguage-based%20pipelines%20have%20each%20shown%20promise%2C%20they%20differ%20in%20assumptions%0Aand%20computational%20cost.%20Moreover%2C%20their%20effectiveness%20relative%20to%20traditional%0Asensor-level%20data%20augmentation%20remains%20unclear.%20In%20this%20paper%2C%20we%20present%20a%0Adirect%20comparison%20between%20these%20two%20virtual%20IMU%20generation%20approaches%20against%0Aclassical%20data%20augmentation%20techniques.%20We%20construct%20a%20large-scale%20virtual%20IMU%0Adataset%20spanning%20100%20diverse%20activities%20from%20Kinetics-400%20and%20simulate%20sensor%0Asignals%20at%2022%20body%20locations.%20The%20three%20data%20generation%20strategies%20are%0Aevaluated%20on%20benchmark%20HAR%20datasets%20%28UTD-MHAD%2C%20PAMAP2%2C%20HAD-AW%29%20using%20four%0Apopular%20models.%20Results%20show%20that%20virtual%20IMU%20data%20significantly%20improves%0Aperformance%20over%20real%20or%20augmented%20data%20alone%2C%20particularly%20under%20limited-data%0Aconditions.%20We%20offer%20practical%20guidance%20on%20choosing%20data%20generation%20strategies%0Aand%20highlight%20the%20distinct%20advantages%20and%20disadvantages%20of%20each%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Human%2520Activity%2520Recognition%253A%2520A%2520Comparative%2520Evaluation%2520of%250A%2520%2520Synthetic%2520Data%2520Generation%2520and%2520Augmentation%2520Techniques%26entry.906535625%3DZikang%2520Leng%2520and%2520Archith%2520Iyer%2520and%2520Thomas%2520Pl%25C3%25B6tz%26entry.1292438233%3D%2520%2520Human%2520activity%2520recognition%2520%2528HAR%2529%2520is%2520often%2520limited%2520by%2520the%2520scarcity%2520of%2520labeled%250Adatasets%2520due%2520to%2520the%2520high%2520cost%2520and%2520complexity%2520of%2520real-world%2520data%2520collection.%2520To%250Amitigate%2520this%252C%2520recent%2520work%2520has%2520explored%2520generating%2520virtual%2520inertial%2520measurement%250Aunit%2520%2528IMU%2529%2520data%2520via%2520cross-modality%2520transfer.%2520While%2520video-based%2520and%250Alanguage-based%2520pipelines%2520have%2520each%2520shown%2520promise%252C%2520they%2520differ%2520in%2520assumptions%250Aand%2520computational%2520cost.%2520Moreover%252C%2520their%2520effectiveness%2520relative%2520to%2520traditional%250Asensor-level%2520data%2520augmentation%2520remains%2520unclear.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Adirect%2520comparison%2520between%2520these%2520two%2520virtual%2520IMU%2520generation%2520approaches%2520against%250Aclassical%2520data%2520augmentation%2520techniques.%2520We%2520construct%2520a%2520large-scale%2520virtual%2520IMU%250Adataset%2520spanning%2520100%2520diverse%2520activities%2520from%2520Kinetics-400%2520and%2520simulate%2520sensor%250Asignals%2520at%252022%2520body%2520locations.%2520The%2520three%2520data%2520generation%2520strategies%2520are%250Aevaluated%2520on%2520benchmark%2520HAR%2520datasets%2520%2528UTD-MHAD%252C%2520PAMAP2%252C%2520HAD-AW%2529%2520using%2520four%250Apopular%2520models.%2520Results%2520show%2520that%2520virtual%2520IMU%2520data%2520significantly%2520improves%250Aperformance%2520over%2520real%2520or%2520augmented%2520data%2520alone%252C%2520particularly%2520under%2520limited-data%250Aconditions.%2520We%2520offer%2520practical%2520guidance%2520on%2520choosing%2520data%2520generation%2520strategies%250Aand%2520highlight%2520the%2520distinct%2520advantages%2520and%2520disadvantages%2520of%2520each%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Human%20Activity%20Recognition%3A%20A%20Comparative%20Evaluation%20of%0A%20%20Synthetic%20Data%20Generation%20and%20Augmentation%20Techniques&entry.906535625=Zikang%20Leng%20and%20Archith%20Iyer%20and%20Thomas%20Pl%C3%B6tz&entry.1292438233=%20%20Human%20activity%20recognition%20%28HAR%29%20is%20often%20limited%20by%20the%20scarcity%20of%20labeled%0Adatasets%20due%20to%20the%20high%20cost%20and%20complexity%20of%20real-world%20data%20collection.%20To%0Amitigate%20this%2C%20recent%20work%20has%20explored%20generating%20virtual%20inertial%20measurement%0Aunit%20%28IMU%29%20data%20via%20cross-modality%20transfer.%20While%20video-based%20and%0Alanguage-based%20pipelines%20have%20each%20shown%20promise%2C%20they%20differ%20in%20assumptions%0Aand%20computational%20cost.%20Moreover%2C%20their%20effectiveness%20relative%20to%20traditional%0Asensor-level%20data%20augmentation%20remains%20unclear.%20In%20this%20paper%2C%20we%20present%20a%0Adirect%20comparison%20between%20these%20two%20virtual%20IMU%20generation%20approaches%20against%0Aclassical%20data%20augmentation%20techniques.%20We%20construct%20a%20large-scale%20virtual%20IMU%0Adataset%20spanning%20100%20diverse%20activities%20from%20Kinetics-400%20and%20simulate%20sensor%0Asignals%20at%2022%20body%20locations.%20The%20three%20data%20generation%20strategies%20are%0Aevaluated%20on%20benchmark%20HAR%20datasets%20%28UTD-MHAD%2C%20PAMAP2%2C%20HAD-AW%29%20using%20four%0Apopular%20models.%20Results%20show%20that%20virtual%20IMU%20data%20significantly%20improves%0Aperformance%20over%20real%20or%20augmented%20data%20alone%2C%20particularly%20under%20limited-data%0Aconditions.%20We%20offer%20practical%20guidance%20on%20choosing%20data%20generation%20strategies%0Aand%20highlight%20the%20distinct%20advantages%20and%20disadvantages%20of%20each%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07612v2&entry.124074799=Read"},
{"title": "Methods for evaluating the resolution of 3D data derived from satellite\n  images", "author": "Christina Selby and Holden Bindl and Tyler Feldman and Andrew Skow and Nicolas Norena Acosta and Shea Hagstrom and Myron Brown", "abstract": "  3D data derived from satellite images is essential for scene modeling\napplications requiring large-scale coverage or involving locations not\naccessible by airborne lidar or cameras. Measuring the resolution of this data\nis important for determining mission utility and tracking improvements. In this\nwork, we consider methods to evaluate the resolution of point clouds, digital\nsurface models, and 3D mesh models. We describe 3D metric evaluation tools and\nworkflows that enable automated evaluation based on high-resolution reference\nairborne lidar, and we present results of analyses with data of varying\nquality.\n", "link": "http://arxiv.org/abs/2506.11876v1", "date": "2025-06-13", "relevancy": 2.7635, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5634}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Methods%20for%20evaluating%20the%20resolution%20of%203D%20data%20derived%20from%20satellite%0A%20%20images&body=Title%3A%20Methods%20for%20evaluating%20the%20resolution%20of%203D%20data%20derived%20from%20satellite%0A%20%20images%0AAuthor%3A%20Christina%20Selby%20and%20Holden%20Bindl%20and%20Tyler%20Feldman%20and%20Andrew%20Skow%20and%20Nicolas%20Norena%20Acosta%20and%20Shea%20Hagstrom%20and%20Myron%20Brown%0AAbstract%3A%20%20%203D%20data%20derived%20from%20satellite%20images%20is%20essential%20for%20scene%20modeling%0Aapplications%20requiring%20large-scale%20coverage%20or%20involving%20locations%20not%0Aaccessible%20by%20airborne%20lidar%20or%20cameras.%20Measuring%20the%20resolution%20of%20this%20data%0Ais%20important%20for%20determining%20mission%20utility%20and%20tracking%20improvements.%20In%20this%0Awork%2C%20we%20consider%20methods%20to%20evaluate%20the%20resolution%20of%20point%20clouds%2C%20digital%0Asurface%20models%2C%20and%203D%20mesh%20models.%20We%20describe%203D%20metric%20evaluation%20tools%20and%0Aworkflows%20that%20enable%20automated%20evaluation%20based%20on%20high-resolution%20reference%0Aairborne%20lidar%2C%20and%20we%20present%20results%20of%20analyses%20with%20data%20of%20varying%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMethods%2520for%2520evaluating%2520the%2520resolution%2520of%25203D%2520data%2520derived%2520from%2520satellite%250A%2520%2520images%26entry.906535625%3DChristina%2520Selby%2520and%2520Holden%2520Bindl%2520and%2520Tyler%2520Feldman%2520and%2520Andrew%2520Skow%2520and%2520Nicolas%2520Norena%2520Acosta%2520and%2520Shea%2520Hagstrom%2520and%2520Myron%2520Brown%26entry.1292438233%3D%2520%25203D%2520data%2520derived%2520from%2520satellite%2520images%2520is%2520essential%2520for%2520scene%2520modeling%250Aapplications%2520requiring%2520large-scale%2520coverage%2520or%2520involving%2520locations%2520not%250Aaccessible%2520by%2520airborne%2520lidar%2520or%2520cameras.%2520Measuring%2520the%2520resolution%2520of%2520this%2520data%250Ais%2520important%2520for%2520determining%2520mission%2520utility%2520and%2520tracking%2520improvements.%2520In%2520this%250Awork%252C%2520we%2520consider%2520methods%2520to%2520evaluate%2520the%2520resolution%2520of%2520point%2520clouds%252C%2520digital%250Asurface%2520models%252C%2520and%25203D%2520mesh%2520models.%2520We%2520describe%25203D%2520metric%2520evaluation%2520tools%2520and%250Aworkflows%2520that%2520enable%2520automated%2520evaluation%2520based%2520on%2520high-resolution%2520reference%250Aairborne%2520lidar%252C%2520and%2520we%2520present%2520results%2520of%2520analyses%2520with%2520data%2520of%2520varying%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Methods%20for%20evaluating%20the%20resolution%20of%203D%20data%20derived%20from%20satellite%0A%20%20images&entry.906535625=Christina%20Selby%20and%20Holden%20Bindl%20and%20Tyler%20Feldman%20and%20Andrew%20Skow%20and%20Nicolas%20Norena%20Acosta%20and%20Shea%20Hagstrom%20and%20Myron%20Brown&entry.1292438233=%20%203D%20data%20derived%20from%20satellite%20images%20is%20essential%20for%20scene%20modeling%0Aapplications%20requiring%20large-scale%20coverage%20or%20involving%20locations%20not%0Aaccessible%20by%20airborne%20lidar%20or%20cameras.%20Measuring%20the%20resolution%20of%20this%20data%0Ais%20important%20for%20determining%20mission%20utility%20and%20tracking%20improvements.%20In%20this%0Awork%2C%20we%20consider%20methods%20to%20evaluate%20the%20resolution%20of%20point%20clouds%2C%20digital%0Asurface%20models%2C%20and%203D%20mesh%20models.%20We%20describe%203D%20metric%20evaluation%20tools%20and%0Aworkflows%20that%20enable%20automated%20evaluation%20based%20on%20high-resolution%20reference%0Aairborne%20lidar%2C%20and%20we%20present%20results%20of%20analyses%20with%20data%20of%20varying%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11876v1&entry.124074799=Read"},
{"title": "Pose Matters: Evaluating Vision Transformers and CNNs for Human Action\n  Recognition on Small COCO Subsets", "author": "MingZe Tang and Madiha Kazi", "abstract": "  This study explores human action recognition using a three-class subset of\nthe COCO image corpus, benchmarking models from simple fully connected networks\nto transformer architectures. The binary Vision Transformer (ViT) achieved 90%\nmean test accuracy, significantly exceeding multiclass classifiers such as\nconvolutional networks (approximately 35%) and CLIP-based models (approximately\n62-64%). A one-way ANOVA (F = 61.37, p < 0.001) confirmed these differences are\nstatistically significant. Qualitative analysis with SHAP explainer and LeGrad\nheatmaps indicated that the ViT localizes pose-specific regions (e.g., lower\nlimbs for walking or running), while simpler feed-forward models often focus on\nbackground textures, explaining their errors. These findings emphasize the data\nefficiency of transformer representations and the importance of explainability\ntechniques in diagnosing class-specific failures.\n", "link": "http://arxiv.org/abs/2506.11678v1", "date": "2025-06-13", "relevancy": 2.7619, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.554}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose%20Matters%3A%20Evaluating%20Vision%20Transformers%20and%20CNNs%20for%20Human%20Action%0A%20%20Recognition%20on%20Small%20COCO%20Subsets&body=Title%3A%20Pose%20Matters%3A%20Evaluating%20Vision%20Transformers%20and%20CNNs%20for%20Human%20Action%0A%20%20Recognition%20on%20Small%20COCO%20Subsets%0AAuthor%3A%20MingZe%20Tang%20and%20Madiha%20Kazi%0AAbstract%3A%20%20%20This%20study%20explores%20human%20action%20recognition%20using%20a%20three-class%20subset%20of%0Athe%20COCO%20image%20corpus%2C%20benchmarking%20models%20from%20simple%20fully%20connected%20networks%0Ato%20transformer%20architectures.%20The%20binary%20Vision%20Transformer%20%28ViT%29%20achieved%2090%25%0Amean%20test%20accuracy%2C%20significantly%20exceeding%20multiclass%20classifiers%20such%20as%0Aconvolutional%20networks%20%28approximately%2035%25%29%20and%20CLIP-based%20models%20%28approximately%0A62-64%25%29.%20A%20one-way%20ANOVA%20%28F%20%3D%2061.37%2C%20p%20%3C%200.001%29%20confirmed%20these%20differences%20are%0Astatistically%20significant.%20Qualitative%20analysis%20with%20SHAP%20explainer%20and%20LeGrad%0Aheatmaps%20indicated%20that%20the%20ViT%20localizes%20pose-specific%20regions%20%28e.g.%2C%20lower%0Alimbs%20for%20walking%20or%20running%29%2C%20while%20simpler%20feed-forward%20models%20often%20focus%20on%0Abackground%20textures%2C%20explaining%20their%20errors.%20These%20findings%20emphasize%20the%20data%0Aefficiency%20of%20transformer%20representations%20and%20the%20importance%20of%20explainability%0Atechniques%20in%20diagnosing%20class-specific%20failures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose%2520Matters%253A%2520Evaluating%2520Vision%2520Transformers%2520and%2520CNNs%2520for%2520Human%2520Action%250A%2520%2520Recognition%2520on%2520Small%2520COCO%2520Subsets%26entry.906535625%3DMingZe%2520Tang%2520and%2520Madiha%2520Kazi%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520human%2520action%2520recognition%2520using%2520a%2520three-class%2520subset%2520of%250Athe%2520COCO%2520image%2520corpus%252C%2520benchmarking%2520models%2520from%2520simple%2520fully%2520connected%2520networks%250Ato%2520transformer%2520architectures.%2520The%2520binary%2520Vision%2520Transformer%2520%2528ViT%2529%2520achieved%252090%2525%250Amean%2520test%2520accuracy%252C%2520significantly%2520exceeding%2520multiclass%2520classifiers%2520such%2520as%250Aconvolutional%2520networks%2520%2528approximately%252035%2525%2529%2520and%2520CLIP-based%2520models%2520%2528approximately%250A62-64%2525%2529.%2520A%2520one-way%2520ANOVA%2520%2528F%2520%253D%252061.37%252C%2520p%2520%253C%25200.001%2529%2520confirmed%2520these%2520differences%2520are%250Astatistically%2520significant.%2520Qualitative%2520analysis%2520with%2520SHAP%2520explainer%2520and%2520LeGrad%250Aheatmaps%2520indicated%2520that%2520the%2520ViT%2520localizes%2520pose-specific%2520regions%2520%2528e.g.%252C%2520lower%250Alimbs%2520for%2520walking%2520or%2520running%2529%252C%2520while%2520simpler%2520feed-forward%2520models%2520often%2520focus%2520on%250Abackground%2520textures%252C%2520explaining%2520their%2520errors.%2520These%2520findings%2520emphasize%2520the%2520data%250Aefficiency%2520of%2520transformer%2520representations%2520and%2520the%2520importance%2520of%2520explainability%250Atechniques%2520in%2520diagnosing%2520class-specific%2520failures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose%20Matters%3A%20Evaluating%20Vision%20Transformers%20and%20CNNs%20for%20Human%20Action%0A%20%20Recognition%20on%20Small%20COCO%20Subsets&entry.906535625=MingZe%20Tang%20and%20Madiha%20Kazi&entry.1292438233=%20%20This%20study%20explores%20human%20action%20recognition%20using%20a%20three-class%20subset%20of%0Athe%20COCO%20image%20corpus%2C%20benchmarking%20models%20from%20simple%20fully%20connected%20networks%0Ato%20transformer%20architectures.%20The%20binary%20Vision%20Transformer%20%28ViT%29%20achieved%2090%25%0Amean%20test%20accuracy%2C%20significantly%20exceeding%20multiclass%20classifiers%20such%20as%0Aconvolutional%20networks%20%28approximately%2035%25%29%20and%20CLIP-based%20models%20%28approximately%0A62-64%25%29.%20A%20one-way%20ANOVA%20%28F%20%3D%2061.37%2C%20p%20%3C%200.001%29%20confirmed%20these%20differences%20are%0Astatistically%20significant.%20Qualitative%20analysis%20with%20SHAP%20explainer%20and%20LeGrad%0Aheatmaps%20indicated%20that%20the%20ViT%20localizes%20pose-specific%20regions%20%28e.g.%2C%20lower%0Alimbs%20for%20walking%20or%20running%29%2C%20while%20simpler%20feed-forward%20models%20often%20focus%20on%0Abackground%20textures%2C%20explaining%20their%20errors.%20These%20findings%20emphasize%20the%20data%0Aefficiency%20of%20transformer%20representations%20and%20the%20importance%20of%20explainability%0Atechniques%20in%20diagnosing%20class-specific%20failures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11678v1&entry.124074799=Read"},
{"title": "Foundation Models in Medical Imaging -- A Review and Outlook", "author": "Vivien van Veldhuizen and Vanessa Botha and Chunyao Lu and Melis Erdal Cesur and Kevin Groot Lipman and Edwin D. de Jong and Hugo Horlings and Cl\u00e1risa I. Sanchez and Cees G. M. Snoek and Lodewyk Wessels and Ritse Mann and Eric Marcus and Jonas Teuwen", "abstract": "  Foundation models (FMs) are changing the way medical images are analyzed by\nlearning from large collections of unlabeled data. Instead of relying on\nmanually annotated examples, FMs are pre-trained to learn general-purpose\nvisual features that can later be adapted to specific clinical tasks with\nlittle additional supervision. In this review, we examine how FMs are being\ndeveloped and applied in pathology, radiology, and ophthalmology, drawing on\nevidence from over 150 studies. We explain the core components of FM pipelines,\nincluding model architectures, self-supervised learning methods, and strategies\nfor downstream adaptation. We also review how FMs are being used in each\nimaging domain and compare design choices across applications. Finally, we\ndiscuss key challenges and open questions to guide future research.\n", "link": "http://arxiv.org/abs/2506.09095v2", "date": "2025-06-13", "relevancy": 2.7226, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20in%20Medical%20Imaging%20--%20A%20Review%20and%20Outlook&body=Title%3A%20Foundation%20Models%20in%20Medical%20Imaging%20--%20A%20Review%20and%20Outlook%0AAuthor%3A%20Vivien%20van%20Veldhuizen%20and%20Vanessa%20Botha%20and%20Chunyao%20Lu%20and%20Melis%20Erdal%20Cesur%20and%20Kevin%20Groot%20Lipman%20and%20Edwin%20D.%20de%20Jong%20and%20Hugo%20Horlings%20and%20Cl%C3%A1risa%20I.%20Sanchez%20and%20Cees%20G.%20M.%20Snoek%20and%20Lodewyk%20Wessels%20and%20Ritse%20Mann%20and%20Eric%20Marcus%20and%20Jonas%20Teuwen%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20are%20changing%20the%20way%20medical%20images%20are%20analyzed%20by%0Alearning%20from%20large%20collections%20of%20unlabeled%20data.%20Instead%20of%20relying%20on%0Amanually%20annotated%20examples%2C%20FMs%20are%20pre-trained%20to%20learn%20general-purpose%0Avisual%20features%20that%20can%20later%20be%20adapted%20to%20specific%20clinical%20tasks%20with%0Alittle%20additional%20supervision.%20In%20this%20review%2C%20we%20examine%20how%20FMs%20are%20being%0Adeveloped%20and%20applied%20in%20pathology%2C%20radiology%2C%20and%20ophthalmology%2C%20drawing%20on%0Aevidence%20from%20over%20150%20studies.%20We%20explain%20the%20core%20components%20of%20FM%20pipelines%2C%0Aincluding%20model%20architectures%2C%20self-supervised%20learning%20methods%2C%20and%20strategies%0Afor%20downstream%20adaptation.%20We%20also%20review%20how%20FMs%20are%20being%20used%20in%20each%0Aimaging%20domain%20and%20compare%20design%20choices%20across%20applications.%20Finally%2C%20we%0Adiscuss%20key%20challenges%20and%20open%20questions%20to%20guide%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09095v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520in%2520Medical%2520Imaging%2520--%2520A%2520Review%2520and%2520Outlook%26entry.906535625%3DVivien%2520van%2520Veldhuizen%2520and%2520Vanessa%2520Botha%2520and%2520Chunyao%2520Lu%2520and%2520Melis%2520Erdal%2520Cesur%2520and%2520Kevin%2520Groot%2520Lipman%2520and%2520Edwin%2520D.%2520de%2520Jong%2520and%2520Hugo%2520Horlings%2520and%2520Cl%25C3%25A1risa%2520I.%2520Sanchez%2520and%2520Cees%2520G.%2520M.%2520Snoek%2520and%2520Lodewyk%2520Wessels%2520and%2520Ritse%2520Mann%2520and%2520Eric%2520Marcus%2520and%2520Jonas%2520Teuwen%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520are%2520changing%2520the%2520way%2520medical%2520images%2520are%2520analyzed%2520by%250Alearning%2520from%2520large%2520collections%2520of%2520unlabeled%2520data.%2520Instead%2520of%2520relying%2520on%250Amanually%2520annotated%2520examples%252C%2520FMs%2520are%2520pre-trained%2520to%2520learn%2520general-purpose%250Avisual%2520features%2520that%2520can%2520later%2520be%2520adapted%2520to%2520specific%2520clinical%2520tasks%2520with%250Alittle%2520additional%2520supervision.%2520In%2520this%2520review%252C%2520we%2520examine%2520how%2520FMs%2520are%2520being%250Adeveloped%2520and%2520applied%2520in%2520pathology%252C%2520radiology%252C%2520and%2520ophthalmology%252C%2520drawing%2520on%250Aevidence%2520from%2520over%2520150%2520studies.%2520We%2520explain%2520the%2520core%2520components%2520of%2520FM%2520pipelines%252C%250Aincluding%2520model%2520architectures%252C%2520self-supervised%2520learning%2520methods%252C%2520and%2520strategies%250Afor%2520downstream%2520adaptation.%2520We%2520also%2520review%2520how%2520FMs%2520are%2520being%2520used%2520in%2520each%250Aimaging%2520domain%2520and%2520compare%2520design%2520choices%2520across%2520applications.%2520Finally%252C%2520we%250Adiscuss%2520key%2520challenges%2520and%2520open%2520questions%2520to%2520guide%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09095v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20in%20Medical%20Imaging%20--%20A%20Review%20and%20Outlook&entry.906535625=Vivien%20van%20Veldhuizen%20and%20Vanessa%20Botha%20and%20Chunyao%20Lu%20and%20Melis%20Erdal%20Cesur%20and%20Kevin%20Groot%20Lipman%20and%20Edwin%20D.%20de%20Jong%20and%20Hugo%20Horlings%20and%20Cl%C3%A1risa%20I.%20Sanchez%20and%20Cees%20G.%20M.%20Snoek%20and%20Lodewyk%20Wessels%20and%20Ritse%20Mann%20and%20Eric%20Marcus%20and%20Jonas%20Teuwen&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20are%20changing%20the%20way%20medical%20images%20are%20analyzed%20by%0Alearning%20from%20large%20collections%20of%20unlabeled%20data.%20Instead%20of%20relying%20on%0Amanually%20annotated%20examples%2C%20FMs%20are%20pre-trained%20to%20learn%20general-purpose%0Avisual%20features%20that%20can%20later%20be%20adapted%20to%20specific%20clinical%20tasks%20with%0Alittle%20additional%20supervision.%20In%20this%20review%2C%20we%20examine%20how%20FMs%20are%20being%0Adeveloped%20and%20applied%20in%20pathology%2C%20radiology%2C%20and%20ophthalmology%2C%20drawing%20on%0Aevidence%20from%20over%20150%20studies.%20We%20explain%20the%20core%20components%20of%20FM%20pipelines%2C%0Aincluding%20model%20architectures%2C%20self-supervised%20learning%20methods%2C%20and%20strategies%0Afor%20downstream%20adaptation.%20We%20also%20review%20how%20FMs%20are%20being%20used%20in%20each%0Aimaging%20domain%20and%20compare%20design%20choices%20across%20applications.%20Finally%2C%20we%0Adiscuss%20key%20challenges%20and%20open%20questions%20to%20guide%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09095v2&entry.124074799=Read"},
{"title": "Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in\n  Interleaved Multi-Image Model", "author": "Dinh Viet Cuong and Hoang-Bao Le and An Pham Ngoc Nguyen and Liting Zhou and Cathal Gurrin", "abstract": "  This paper addresses two main objectives. Firstly, we demonstrate the\nimpressive performance of the LLaVA-NeXT-interleave on 22 datasets across three\ndifferent tasks: Multi-Image Reasoning, Documents and Knowledge-Based\nUnderstanding and Interactive Multi-Modal Communication. Secondly, we add the\nDense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and\ncompare its performance against the standard model. We find that the standard\nmodel achieves the highest overall accuracy, excelling in vision-heavy tasks\nlike VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows\nparticular strength on datasets requiring deeper semantic coherence or\nstructured change understanding such as MIT-States_PropertyCoherence and\nSlideVQA. Our results highlight the potential of combining powerful foundation\nmodels with plug-and-play techniques for Interleave tasks. The code is\navailable at https://github.com/dinhvietcuong1996/icme25-inova.\n", "link": "http://arxiv.org/abs/2506.11737v1", "date": "2025-06-13", "relevancy": 2.6958, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quizzard%40INOVA%20Challenge%202025%20--%20Track%20A%3A%20Plug-and-Play%20Technique%20in%0A%20%20Interleaved%20Multi-Image%20Model&body=Title%3A%20Quizzard%40INOVA%20Challenge%202025%20--%20Track%20A%3A%20Plug-and-Play%20Technique%20in%0A%20%20Interleaved%20Multi-Image%20Model%0AAuthor%3A%20Dinh%20Viet%20Cuong%20and%20Hoang-Bao%20Le%20and%20An%20Pham%20Ngoc%20Nguyen%20and%20Liting%20Zhou%20and%20Cathal%20Gurrin%0AAbstract%3A%20%20%20This%20paper%20addresses%20two%20main%20objectives.%20Firstly%2C%20we%20demonstrate%20the%0Aimpressive%20performance%20of%20the%20LLaVA-NeXT-interleave%20on%2022%20datasets%20across%20three%0Adifferent%20tasks%3A%20Multi-Image%20Reasoning%2C%20Documents%20and%20Knowledge-Based%0AUnderstanding%20and%20Interactive%20Multi-Modal%20Communication.%20Secondly%2C%20we%20add%20the%0ADense%20Channel%20Integration%20%28DCI%29%20connector%20to%20the%20LLaVA-NeXT-Interleave%20and%0Acompare%20its%20performance%20against%20the%20standard%20model.%20We%20find%20that%20the%20standard%0Amodel%20achieves%20the%20highest%20overall%20accuracy%2C%20excelling%20in%20vision-heavy%20tasks%0Alike%20VISION%2C%20NLVR2%2C%20and%20Fashion200K.%20Meanwhile%2C%20the%20DCI-enhanced%20version%20shows%0Aparticular%20strength%20on%20datasets%20requiring%20deeper%20semantic%20coherence%20or%0Astructured%20change%20understanding%20such%20as%20MIT-States_PropertyCoherence%20and%0ASlideVQA.%20Our%20results%20highlight%20the%20potential%20of%20combining%20powerful%20foundation%0Amodels%20with%20plug-and-play%20techniques%20for%20Interleave%20tasks.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/dinhvietcuong1996/icme25-inova.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuizzard%2540INOVA%2520Challenge%25202025%2520--%2520Track%2520A%253A%2520Plug-and-Play%2520Technique%2520in%250A%2520%2520Interleaved%2520Multi-Image%2520Model%26entry.906535625%3DDinh%2520Viet%2520Cuong%2520and%2520Hoang-Bao%2520Le%2520and%2520An%2520Pham%2520Ngoc%2520Nguyen%2520and%2520Liting%2520Zhou%2520and%2520Cathal%2520Gurrin%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520two%2520main%2520objectives.%2520Firstly%252C%2520we%2520demonstrate%2520the%250Aimpressive%2520performance%2520of%2520the%2520LLaVA-NeXT-interleave%2520on%252022%2520datasets%2520across%2520three%250Adifferent%2520tasks%253A%2520Multi-Image%2520Reasoning%252C%2520Documents%2520and%2520Knowledge-Based%250AUnderstanding%2520and%2520Interactive%2520Multi-Modal%2520Communication.%2520Secondly%252C%2520we%2520add%2520the%250ADense%2520Channel%2520Integration%2520%2528DCI%2529%2520connector%2520to%2520the%2520LLaVA-NeXT-Interleave%2520and%250Acompare%2520its%2520performance%2520against%2520the%2520standard%2520model.%2520We%2520find%2520that%2520the%2520standard%250Amodel%2520achieves%2520the%2520highest%2520overall%2520accuracy%252C%2520excelling%2520in%2520vision-heavy%2520tasks%250Alike%2520VISION%252C%2520NLVR2%252C%2520and%2520Fashion200K.%2520Meanwhile%252C%2520the%2520DCI-enhanced%2520version%2520shows%250Aparticular%2520strength%2520on%2520datasets%2520requiring%2520deeper%2520semantic%2520coherence%2520or%250Astructured%2520change%2520understanding%2520such%2520as%2520MIT-States_PropertyCoherence%2520and%250ASlideVQA.%2520Our%2520results%2520highlight%2520the%2520potential%2520of%2520combining%2520powerful%2520foundation%250Amodels%2520with%2520plug-and-play%2520techniques%2520for%2520Interleave%2520tasks.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/dinhvietcuong1996/icme25-inova.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quizzard%40INOVA%20Challenge%202025%20--%20Track%20A%3A%20Plug-and-Play%20Technique%20in%0A%20%20Interleaved%20Multi-Image%20Model&entry.906535625=Dinh%20Viet%20Cuong%20and%20Hoang-Bao%20Le%20and%20An%20Pham%20Ngoc%20Nguyen%20and%20Liting%20Zhou%20and%20Cathal%20Gurrin&entry.1292438233=%20%20This%20paper%20addresses%20two%20main%20objectives.%20Firstly%2C%20we%20demonstrate%20the%0Aimpressive%20performance%20of%20the%20LLaVA-NeXT-interleave%20on%2022%20datasets%20across%20three%0Adifferent%20tasks%3A%20Multi-Image%20Reasoning%2C%20Documents%20and%20Knowledge-Based%0AUnderstanding%20and%20Interactive%20Multi-Modal%20Communication.%20Secondly%2C%20we%20add%20the%0ADense%20Channel%20Integration%20%28DCI%29%20connector%20to%20the%20LLaVA-NeXT-Interleave%20and%0Acompare%20its%20performance%20against%20the%20standard%20model.%20We%20find%20that%20the%20standard%0Amodel%20achieves%20the%20highest%20overall%20accuracy%2C%20excelling%20in%20vision-heavy%20tasks%0Alike%20VISION%2C%20NLVR2%2C%20and%20Fashion200K.%20Meanwhile%2C%20the%20DCI-enhanced%20version%20shows%0Aparticular%20strength%20on%20datasets%20requiring%20deeper%20semantic%20coherence%20or%0Astructured%20change%20understanding%20such%20as%20MIT-States_PropertyCoherence%20and%0ASlideVQA.%20Our%20results%20highlight%20the%20potential%20of%20combining%20powerful%20foundation%0Amodels%20with%20plug-and-play%20techniques%20for%20Interleave%20tasks.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/dinhvietcuong1996/icme25-inova.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11737v1&entry.124074799=Read"},
{"title": "Visual Pre-Training on Unlabeled Images using Reinforcement Learning", "author": "Dibya Ghosh and Sergey Levine", "abstract": "  In reinforcement learning (RL), value-based algorithms learn to associate\neach observation with the states and rewards that are likely to be reached from\nit. We observe that many self-supervised image pre-training methods bear\nsimilarity to this formulation: learning features that associate crops of\nimages with those of nearby views, e.g., by taking a different crop or color\naugmentation. In this paper, we complete this analogy and explore a method that\ndirectly casts pre-training on unlabeled image data like web crawls and video\nframes as an RL problem. We train a general value function in a dynamical\nsystem where an agent transforms an image by changing the view or adding image\naugmentations. Learning in this way resembles crop-consistency\nself-supervision, but through the reward function, offers a simple lever to\nshape feature learning using curated images or weakly labeled captions when\nthey exist. Our experiments demonstrate improved representations when training\non unlabeled images in the wild, including video data like EpicKitchens, scene\ndata like COCO, and web-crawl data like CC12M.\n", "link": "http://arxiv.org/abs/2506.11967v1", "date": "2025-06-13", "relevancy": 2.6639, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5591}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5219}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Pre-Training%20on%20Unlabeled%20Images%20using%20Reinforcement%20Learning&body=Title%3A%20Visual%20Pre-Training%20on%20Unlabeled%20Images%20using%20Reinforcement%20Learning%0AAuthor%3A%20Dibya%20Ghosh%20and%20Sergey%20Levine%0AAbstract%3A%20%20%20In%20reinforcement%20learning%20%28RL%29%2C%20value-based%20algorithms%20learn%20to%20associate%0Aeach%20observation%20with%20the%20states%20and%20rewards%20that%20are%20likely%20to%20be%20reached%20from%0Ait.%20We%20observe%20that%20many%20self-supervised%20image%20pre-training%20methods%20bear%0Asimilarity%20to%20this%20formulation%3A%20learning%20features%20that%20associate%20crops%20of%0Aimages%20with%20those%20of%20nearby%20views%2C%20e.g.%2C%20by%20taking%20a%20different%20crop%20or%20color%0Aaugmentation.%20In%20this%20paper%2C%20we%20complete%20this%20analogy%20and%20explore%20a%20method%20that%0Adirectly%20casts%20pre-training%20on%20unlabeled%20image%20data%20like%20web%20crawls%20and%20video%0Aframes%20as%20an%20RL%20problem.%20We%20train%20a%20general%20value%20function%20in%20a%20dynamical%0Asystem%20where%20an%20agent%20transforms%20an%20image%20by%20changing%20the%20view%20or%20adding%20image%0Aaugmentations.%20Learning%20in%20this%20way%20resembles%20crop-consistency%0Aself-supervision%2C%20but%20through%20the%20reward%20function%2C%20offers%20a%20simple%20lever%20to%0Ashape%20feature%20learning%20using%20curated%20images%20or%20weakly%20labeled%20captions%20when%0Athey%20exist.%20Our%20experiments%20demonstrate%20improved%20representations%20when%20training%0Aon%20unlabeled%20images%20in%20the%20wild%2C%20including%20video%20data%20like%20EpicKitchens%2C%20scene%0Adata%20like%20COCO%2C%20and%20web-crawl%20data%20like%20CC12M.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Pre-Training%2520on%2520Unlabeled%2520Images%2520using%2520Reinforcement%2520Learning%26entry.906535625%3DDibya%2520Ghosh%2520and%2520Sergey%2520Levine%26entry.1292438233%3D%2520%2520In%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520value-based%2520algorithms%2520learn%2520to%2520associate%250Aeach%2520observation%2520with%2520the%2520states%2520and%2520rewards%2520that%2520are%2520likely%2520to%2520be%2520reached%2520from%250Ait.%2520We%2520observe%2520that%2520many%2520self-supervised%2520image%2520pre-training%2520methods%2520bear%250Asimilarity%2520to%2520this%2520formulation%253A%2520learning%2520features%2520that%2520associate%2520crops%2520of%250Aimages%2520with%2520those%2520of%2520nearby%2520views%252C%2520e.g.%252C%2520by%2520taking%2520a%2520different%2520crop%2520or%2520color%250Aaugmentation.%2520In%2520this%2520paper%252C%2520we%2520complete%2520this%2520analogy%2520and%2520explore%2520a%2520method%2520that%250Adirectly%2520casts%2520pre-training%2520on%2520unlabeled%2520image%2520data%2520like%2520web%2520crawls%2520and%2520video%250Aframes%2520as%2520an%2520RL%2520problem.%2520We%2520train%2520a%2520general%2520value%2520function%2520in%2520a%2520dynamical%250Asystem%2520where%2520an%2520agent%2520transforms%2520an%2520image%2520by%2520changing%2520the%2520view%2520or%2520adding%2520image%250Aaugmentations.%2520Learning%2520in%2520this%2520way%2520resembles%2520crop-consistency%250Aself-supervision%252C%2520but%2520through%2520the%2520reward%2520function%252C%2520offers%2520a%2520simple%2520lever%2520to%250Ashape%2520feature%2520learning%2520using%2520curated%2520images%2520or%2520weakly%2520labeled%2520captions%2520when%250Athey%2520exist.%2520Our%2520experiments%2520demonstrate%2520improved%2520representations%2520when%2520training%250Aon%2520unlabeled%2520images%2520in%2520the%2520wild%252C%2520including%2520video%2520data%2520like%2520EpicKitchens%252C%2520scene%250Adata%2520like%2520COCO%252C%2520and%2520web-crawl%2520data%2520like%2520CC12M.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Pre-Training%20on%20Unlabeled%20Images%20using%20Reinforcement%20Learning&entry.906535625=Dibya%20Ghosh%20and%20Sergey%20Levine&entry.1292438233=%20%20In%20reinforcement%20learning%20%28RL%29%2C%20value-based%20algorithms%20learn%20to%20associate%0Aeach%20observation%20with%20the%20states%20and%20rewards%20that%20are%20likely%20to%20be%20reached%20from%0Ait.%20We%20observe%20that%20many%20self-supervised%20image%20pre-training%20methods%20bear%0Asimilarity%20to%20this%20formulation%3A%20learning%20features%20that%20associate%20crops%20of%0Aimages%20with%20those%20of%20nearby%20views%2C%20e.g.%2C%20by%20taking%20a%20different%20crop%20or%20color%0Aaugmentation.%20In%20this%20paper%2C%20we%20complete%20this%20analogy%20and%20explore%20a%20method%20that%0Adirectly%20casts%20pre-training%20on%20unlabeled%20image%20data%20like%20web%20crawls%20and%20video%0Aframes%20as%20an%20RL%20problem.%20We%20train%20a%20general%20value%20function%20in%20a%20dynamical%0Asystem%20where%20an%20agent%20transforms%20an%20image%20by%20changing%20the%20view%20or%20adding%20image%0Aaugmentations.%20Learning%20in%20this%20way%20resembles%20crop-consistency%0Aself-supervision%2C%20but%20through%20the%20reward%20function%2C%20offers%20a%20simple%20lever%20to%0Ashape%20feature%20learning%20using%20curated%20images%20or%20weakly%20labeled%20captions%20when%0Athey%20exist.%20Our%20experiments%20demonstrate%20improved%20representations%20when%20training%0Aon%20unlabeled%20images%20in%20the%20wild%2C%20including%20video%20data%20like%20EpicKitchens%2C%20scene%0Adata%20like%20COCO%2C%20and%20web-crawl%20data%20like%20CC12M.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11967v1&entry.124074799=Read"},
{"title": "Improving Large Language Models with Concept-Aware Fine-Tuning", "author": "Michael K. Chen and Xikun Zhang and Jiaxing Huang and Dacheng Tao", "abstract": "  Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm\n", "link": "http://arxiv.org/abs/2506.07833v2", "date": "2025-06-13", "relevancy": 2.659, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5344}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5305}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Large%20Language%20Models%20with%20Concept-Aware%20Fine-Tuning&body=Title%3A%20Improving%20Large%20Language%20Models%20with%20Concept-Aware%20Fine-Tuning%0AAuthor%3A%20Michael%20K.%20Chen%20and%20Xikun%20Zhang%20and%20Jiaxing%20Huang%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20become%20the%20cornerstone%20of%20modern%20AI.%0AHowever%2C%20the%20existing%20paradigm%20of%20next-token%20prediction%20fundamentally%20limits%0Atheir%20ability%20to%20form%20coherent%2C%20high-level%20concepts%2C%20making%20it%20a%20critical%0Abarrier%20to%20human-like%20understanding%20and%20reasoning.%20Take%20the%20phrase%20%22ribonucleic%0Aacid%22%20as%20an%20example%3A%20an%20LLM%20will%20first%20decompose%20it%20into%20tokens%2C%20i.e.%2C%0Aartificial%20text%20fragments%20%28%22rib%22%2C%20%22on%22%2C%20...%29%2C%20then%20learn%20each%20token%0Asequentially%2C%20rather%20than%20grasping%20the%20phrase%20as%20a%20unified%2C%20coherent%20semantic%0Aentity.%20This%20fragmented%20representation%20hinders%20deeper%20conceptual%20understanding%0Aand%2C%20ultimately%2C%20the%20development%20of%20truly%20intelligent%20systems.%20In%20response%2C%20we%0Aintroduce%20Concept-Aware%20Fine-Tuning%20%28CAFT%29%2C%20a%20novel%20multi-token%20training%20method%0Athat%20redefines%20how%20LLMs%20are%20fine-tuned.%20By%20enabling%20the%20learning%20of%20sequences%0Athat%20span%20multiple%20tokens%2C%20this%20method%20fosters%20stronger%20concept-aware%20learning.%0AOur%20experiments%20demonstrate%20significant%20improvements%20compared%20to%20conventional%0Anext-token%20finetuning%20methods%20across%20diverse%20tasks%2C%20including%20traditional%0Aapplications%20like%20text%20summarization%20and%20domain-specific%20ones%20like%20de%20novo%0Aprotein%20design.%20Multi-token%20prediction%20was%20previously%20only%20possible%20in%20the%0Aprohibitively%20expensive%20pretraining%20phase%3B%20CAFT%2C%20to%20our%20knowledge%2C%20is%20the%20first%0Ato%20bring%20the%20multi-token%20setting%20to%20the%20post-training%20phase%2C%20thus%20effectively%0Ademocratizing%20its%20benefits%20for%20the%20broader%20community%20of%20practitioners%20and%0Aresearchers.%20Finally%2C%20the%20unexpected%20effectiveness%20of%20our%20proposed%20method%0Asuggests%20wider%20implications%20for%20the%20machine%20learning%20research%20community.%20All%0Acode%20and%20data%20are%20available%20at%20https%3A//github.com/michaelchen-lab/caft-llm%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07833v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Large%2520Language%2520Models%2520with%2520Concept-Aware%2520Fine-Tuning%26entry.906535625%3DMichael%2520K.%2520Chen%2520and%2520Xikun%2520Zhang%2520and%2520Jiaxing%2520Huang%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520become%2520the%2520cornerstone%2520of%2520modern%2520AI.%250AHowever%252C%2520the%2520existing%2520paradigm%2520of%2520next-token%2520prediction%2520fundamentally%2520limits%250Atheir%2520ability%2520to%2520form%2520coherent%252C%2520high-level%2520concepts%252C%2520making%2520it%2520a%2520critical%250Abarrier%2520to%2520human-like%2520understanding%2520and%2520reasoning.%2520Take%2520the%2520phrase%2520%2522ribonucleic%250Aacid%2522%2520as%2520an%2520example%253A%2520an%2520LLM%2520will%2520first%2520decompose%2520it%2520into%2520tokens%252C%2520i.e.%252C%250Aartificial%2520text%2520fragments%2520%2528%2522rib%2522%252C%2520%2522on%2522%252C%2520...%2529%252C%2520then%2520learn%2520each%2520token%250Asequentially%252C%2520rather%2520than%2520grasping%2520the%2520phrase%2520as%2520a%2520unified%252C%2520coherent%2520semantic%250Aentity.%2520This%2520fragmented%2520representation%2520hinders%2520deeper%2520conceptual%2520understanding%250Aand%252C%2520ultimately%252C%2520the%2520development%2520of%2520truly%2520intelligent%2520systems.%2520In%2520response%252C%2520we%250Aintroduce%2520Concept-Aware%2520Fine-Tuning%2520%2528CAFT%2529%252C%2520a%2520novel%2520multi-token%2520training%2520method%250Athat%2520redefines%2520how%2520LLMs%2520are%2520fine-tuned.%2520By%2520enabling%2520the%2520learning%2520of%2520sequences%250Athat%2520span%2520multiple%2520tokens%252C%2520this%2520method%2520fosters%2520stronger%2520concept-aware%2520learning.%250AOur%2520experiments%2520demonstrate%2520significant%2520improvements%2520compared%2520to%2520conventional%250Anext-token%2520finetuning%2520methods%2520across%2520diverse%2520tasks%252C%2520including%2520traditional%250Aapplications%2520like%2520text%2520summarization%2520and%2520domain-specific%2520ones%2520like%2520de%2520novo%250Aprotein%2520design.%2520Multi-token%2520prediction%2520was%2520previously%2520only%2520possible%2520in%2520the%250Aprohibitively%2520expensive%2520pretraining%2520phase%253B%2520CAFT%252C%2520to%2520our%2520knowledge%252C%2520is%2520the%2520first%250Ato%2520bring%2520the%2520multi-token%2520setting%2520to%2520the%2520post-training%2520phase%252C%2520thus%2520effectively%250Ademocratizing%2520its%2520benefits%2520for%2520the%2520broader%2520community%2520of%2520practitioners%2520and%250Aresearchers.%2520Finally%252C%2520the%2520unexpected%2520effectiveness%2520of%2520our%2520proposed%2520method%250Asuggests%2520wider%2520implications%2520for%2520the%2520machine%2520learning%2520research%2520community.%2520All%250Acode%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/michaelchen-lab/caft-llm%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07833v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Large%20Language%20Models%20with%20Concept-Aware%20Fine-Tuning&entry.906535625=Michael%20K.%20Chen%20and%20Xikun%20Zhang%20and%20Jiaxing%20Huang%20and%20Dacheng%20Tao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20become%20the%20cornerstone%20of%20modern%20AI.%0AHowever%2C%20the%20existing%20paradigm%20of%20next-token%20prediction%20fundamentally%20limits%0Atheir%20ability%20to%20form%20coherent%2C%20high-level%20concepts%2C%20making%20it%20a%20critical%0Abarrier%20to%20human-like%20understanding%20and%20reasoning.%20Take%20the%20phrase%20%22ribonucleic%0Aacid%22%20as%20an%20example%3A%20an%20LLM%20will%20first%20decompose%20it%20into%20tokens%2C%20i.e.%2C%0Aartificial%20text%20fragments%20%28%22rib%22%2C%20%22on%22%2C%20...%29%2C%20then%20learn%20each%20token%0Asequentially%2C%20rather%20than%20grasping%20the%20phrase%20as%20a%20unified%2C%20coherent%20semantic%0Aentity.%20This%20fragmented%20representation%20hinders%20deeper%20conceptual%20understanding%0Aand%2C%20ultimately%2C%20the%20development%20of%20truly%20intelligent%20systems.%20In%20response%2C%20we%0Aintroduce%20Concept-Aware%20Fine-Tuning%20%28CAFT%29%2C%20a%20novel%20multi-token%20training%20method%0Athat%20redefines%20how%20LLMs%20are%20fine-tuned.%20By%20enabling%20the%20learning%20of%20sequences%0Athat%20span%20multiple%20tokens%2C%20this%20method%20fosters%20stronger%20concept-aware%20learning.%0AOur%20experiments%20demonstrate%20significant%20improvements%20compared%20to%20conventional%0Anext-token%20finetuning%20methods%20across%20diverse%20tasks%2C%20including%20traditional%0Aapplications%20like%20text%20summarization%20and%20domain-specific%20ones%20like%20de%20novo%0Aprotein%20design.%20Multi-token%20prediction%20was%20previously%20only%20possible%20in%20the%0Aprohibitively%20expensive%20pretraining%20phase%3B%20CAFT%2C%20to%20our%20knowledge%2C%20is%20the%20first%0Ato%20bring%20the%20multi-token%20setting%20to%20the%20post-training%20phase%2C%20thus%20effectively%0Ademocratizing%20its%20benefits%20for%20the%20broader%20community%20of%20practitioners%20and%0Aresearchers.%20Finally%2C%20the%20unexpected%20effectiveness%20of%20our%20proposed%20method%0Asuggests%20wider%20implications%20for%20the%20machine%20learning%20research%20community.%20All%0Acode%20and%20data%20are%20available%20at%20https%3A//github.com/michaelchen-lab/caft-llm%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07833v2&entry.124074799=Read"},
{"title": "Growing with Experience: Growing Neural Networks in Deep Reinforcement\n  Learning", "author": "Lukas Fehring and Marius Lindauer and Theresa Eimer", "abstract": "  While increasingly large models have revolutionized much of the machine\nlearning landscape, training even mid-sized networks for Reinforcement Learning\n(RL) is still proving to be a struggle. This, however, severely limits the\ncomplexity of policies we are able to learn. To enable increased network\ncapacity while maintaining network trainability, we propose GrowNN, a simple\nyet effective method that utilizes progressive network growth during training.\nWe start training a small network to learn an initial policy. Then we add\nlayers without changing the encoded function. Subsequent updates can utilize\nthe added layers to learn a more expressive policy, adding capacity as the\npolicy's complexity increases. GrowNN can be seamlessly integrated into most\nexisting RL agents. Our experiments on MiniHack and Mujoco show improved agent\nperformance, with incrementally GrowNN-deeper networks outperforming their\nrespective static counterparts of the same size by up to 48% on MiniHack Room\nand 72% on Ant.\n", "link": "http://arxiv.org/abs/2506.11706v1", "date": "2025-06-13", "relevancy": 2.622, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5546}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5153}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Growing%20with%20Experience%3A%20Growing%20Neural%20Networks%20in%20Deep%20Reinforcement%0A%20%20Learning&body=Title%3A%20Growing%20with%20Experience%3A%20Growing%20Neural%20Networks%20in%20Deep%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Lukas%20Fehring%20and%20Marius%20Lindauer%20and%20Theresa%20Eimer%0AAbstract%3A%20%20%20While%20increasingly%20large%20models%20have%20revolutionized%20much%20of%20the%20machine%0Alearning%20landscape%2C%20training%20even%20mid-sized%20networks%20for%20Reinforcement%20Learning%0A%28RL%29%20is%20still%20proving%20to%20be%20a%20struggle.%20This%2C%20however%2C%20severely%20limits%20the%0Acomplexity%20of%20policies%20we%20are%20able%20to%20learn.%20To%20enable%20increased%20network%0Acapacity%20while%20maintaining%20network%20trainability%2C%20we%20propose%20GrowNN%2C%20a%20simple%0Ayet%20effective%20method%20that%20utilizes%20progressive%20network%20growth%20during%20training.%0AWe%20start%20training%20a%20small%20network%20to%20learn%20an%20initial%20policy.%20Then%20we%20add%0Alayers%20without%20changing%20the%20encoded%20function.%20Subsequent%20updates%20can%20utilize%0Athe%20added%20layers%20to%20learn%20a%20more%20expressive%20policy%2C%20adding%20capacity%20as%20the%0Apolicy%27s%20complexity%20increases.%20GrowNN%20can%20be%20seamlessly%20integrated%20into%20most%0Aexisting%20RL%20agents.%20Our%20experiments%20on%20MiniHack%20and%20Mujoco%20show%20improved%20agent%0Aperformance%2C%20with%20incrementally%20GrowNN-deeper%20networks%20outperforming%20their%0Arespective%20static%20counterparts%20of%20the%20same%20size%20by%20up%20to%2048%25%20on%20MiniHack%20Room%0Aand%2072%25%20on%20Ant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrowing%2520with%2520Experience%253A%2520Growing%2520Neural%2520Networks%2520in%2520Deep%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DLukas%2520Fehring%2520and%2520Marius%2520Lindauer%2520and%2520Theresa%2520Eimer%26entry.1292438233%3D%2520%2520While%2520increasingly%2520large%2520models%2520have%2520revolutionized%2520much%2520of%2520the%2520machine%250Alearning%2520landscape%252C%2520training%2520even%2520mid-sized%2520networks%2520for%2520Reinforcement%2520Learning%250A%2528RL%2529%2520is%2520still%2520proving%2520to%2520be%2520a%2520struggle.%2520This%252C%2520however%252C%2520severely%2520limits%2520the%250Acomplexity%2520of%2520policies%2520we%2520are%2520able%2520to%2520learn.%2520To%2520enable%2520increased%2520network%250Acapacity%2520while%2520maintaining%2520network%2520trainability%252C%2520we%2520propose%2520GrowNN%252C%2520a%2520simple%250Ayet%2520effective%2520method%2520that%2520utilizes%2520progressive%2520network%2520growth%2520during%2520training.%250AWe%2520start%2520training%2520a%2520small%2520network%2520to%2520learn%2520an%2520initial%2520policy.%2520Then%2520we%2520add%250Alayers%2520without%2520changing%2520the%2520encoded%2520function.%2520Subsequent%2520updates%2520can%2520utilize%250Athe%2520added%2520layers%2520to%2520learn%2520a%2520more%2520expressive%2520policy%252C%2520adding%2520capacity%2520as%2520the%250Apolicy%2527s%2520complexity%2520increases.%2520GrowNN%2520can%2520be%2520seamlessly%2520integrated%2520into%2520most%250Aexisting%2520RL%2520agents.%2520Our%2520experiments%2520on%2520MiniHack%2520and%2520Mujoco%2520show%2520improved%2520agent%250Aperformance%252C%2520with%2520incrementally%2520GrowNN-deeper%2520networks%2520outperforming%2520their%250Arespective%2520static%2520counterparts%2520of%2520the%2520same%2520size%2520by%2520up%2520to%252048%2525%2520on%2520MiniHack%2520Room%250Aand%252072%2525%2520on%2520Ant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Growing%20with%20Experience%3A%20Growing%20Neural%20Networks%20in%20Deep%20Reinforcement%0A%20%20Learning&entry.906535625=Lukas%20Fehring%20and%20Marius%20Lindauer%20and%20Theresa%20Eimer&entry.1292438233=%20%20While%20increasingly%20large%20models%20have%20revolutionized%20much%20of%20the%20machine%0Alearning%20landscape%2C%20training%20even%20mid-sized%20networks%20for%20Reinforcement%20Learning%0A%28RL%29%20is%20still%20proving%20to%20be%20a%20struggle.%20This%2C%20however%2C%20severely%20limits%20the%0Acomplexity%20of%20policies%20we%20are%20able%20to%20learn.%20To%20enable%20increased%20network%0Acapacity%20while%20maintaining%20network%20trainability%2C%20we%20propose%20GrowNN%2C%20a%20simple%0Ayet%20effective%20method%20that%20utilizes%20progressive%20network%20growth%20during%20training.%0AWe%20start%20training%20a%20small%20network%20to%20learn%20an%20initial%20policy.%20Then%20we%20add%0Alayers%20without%20changing%20the%20encoded%20function.%20Subsequent%20updates%20can%20utilize%0Athe%20added%20layers%20to%20learn%20a%20more%20expressive%20policy%2C%20adding%20capacity%20as%20the%0Apolicy%27s%20complexity%20increases.%20GrowNN%20can%20be%20seamlessly%20integrated%20into%20most%0Aexisting%20RL%20agents.%20Our%20experiments%20on%20MiniHack%20and%20Mujoco%20show%20improved%20agent%0Aperformance%2C%20with%20incrementally%20GrowNN-deeper%20networks%20outperforming%20their%0Arespective%20static%20counterparts%20of%20the%20same%20size%20by%20up%20to%2048%25%20on%20MiniHack%20Room%0Aand%2072%25%20on%20Ant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11706v1&entry.124074799=Read"},
{"title": "Long-Short Alignment for Effective Long-Context Modeling in LLMs", "author": "Tianqi Du and Haotian Huang and Yifei Wang and Yisen Wang", "abstract": "  Large language models (LLMs) have exhibited impressive performance and\nsurprising emergent properties. However, their effectiveness remains limited by\nthe fixed context window of the transformer architecture, posing challenges for\nlong-context modeling. Among these challenges, length generalization -- the\nability to generalize to sequences longer than those seen during training -- is\na classical and fundamental problem. In this work, we propose a fresh\nperspective on length generalization, shifting the focus from the conventional\nemphasis on input features such as positional encodings or data structures to\nthe output distribution of the model. Specifically, through case studies on\nsynthetic tasks, we highlight the critical role of \\textbf{long-short\nalignment} -- the consistency of output distributions across sequences of\nvarying lengths. Extending this insight to natural language tasks, we propose a\nmetric called Long-Short Misalignment to quantify this phenomenon, uncovering a\nstrong correlation between the metric and length generalization performance.\nBuilding on these findings, we develop a regularization term that promotes\nlong-short alignment during training. Extensive experiments validate the\neffectiveness of our approach, offering new insights for achieving more\neffective long-context modeling in LLMs. Code is available at\nhttps://github.com/PKU-ML/LongShortAlignment.\n", "link": "http://arxiv.org/abs/2506.11769v1", "date": "2025-06-13", "relevancy": 2.5885, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Short%20Alignment%20for%20Effective%20Long-Context%20Modeling%20in%20LLMs&body=Title%3A%20Long-Short%20Alignment%20for%20Effective%20Long-Context%20Modeling%20in%20LLMs%0AAuthor%3A%20Tianqi%20Du%20and%20Haotian%20Huang%20and%20Yifei%20Wang%20and%20Yisen%20Wang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20exhibited%20impressive%20performance%20and%0Asurprising%20emergent%20properties.%20However%2C%20their%20effectiveness%20remains%20limited%20by%0Athe%20fixed%20context%20window%20of%20the%20transformer%20architecture%2C%20posing%20challenges%20for%0Along-context%20modeling.%20Among%20these%20challenges%2C%20length%20generalization%20--%20the%0Aability%20to%20generalize%20to%20sequences%20longer%20than%20those%20seen%20during%20training%20--%20is%0Aa%20classical%20and%20fundamental%20problem.%20In%20this%20work%2C%20we%20propose%20a%20fresh%0Aperspective%20on%20length%20generalization%2C%20shifting%20the%20focus%20from%20the%20conventional%0Aemphasis%20on%20input%20features%20such%20as%20positional%20encodings%20or%20data%20structures%20to%0Athe%20output%20distribution%20of%20the%20model.%20Specifically%2C%20through%20case%20studies%20on%0Asynthetic%20tasks%2C%20we%20highlight%20the%20critical%20role%20of%20%5Ctextbf%7Blong-short%0Aalignment%7D%20--%20the%20consistency%20of%20output%20distributions%20across%20sequences%20of%0Avarying%20lengths.%20Extending%20this%20insight%20to%20natural%20language%20tasks%2C%20we%20propose%20a%0Ametric%20called%20Long-Short%20Misalignment%20to%20quantify%20this%20phenomenon%2C%20uncovering%20a%0Astrong%20correlation%20between%20the%20metric%20and%20length%20generalization%20performance.%0ABuilding%20on%20these%20findings%2C%20we%20develop%20a%20regularization%20term%20that%20promotes%0Along-short%20alignment%20during%20training.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20approach%2C%20offering%20new%20insights%20for%20achieving%20more%0Aeffective%20long-context%20modeling%20in%20LLMs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PKU-ML/LongShortAlignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Short%2520Alignment%2520for%2520Effective%2520Long-Context%2520Modeling%2520in%2520LLMs%26entry.906535625%3DTianqi%2520Du%2520and%2520Haotian%2520Huang%2520and%2520Yifei%2520Wang%2520and%2520Yisen%2520Wang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520exhibited%2520impressive%2520performance%2520and%250Asurprising%2520emergent%2520properties.%2520However%252C%2520their%2520effectiveness%2520remains%2520limited%2520by%250Athe%2520fixed%2520context%2520window%2520of%2520the%2520transformer%2520architecture%252C%2520posing%2520challenges%2520for%250Along-context%2520modeling.%2520Among%2520these%2520challenges%252C%2520length%2520generalization%2520--%2520the%250Aability%2520to%2520generalize%2520to%2520sequences%2520longer%2520than%2520those%2520seen%2520during%2520training%2520--%2520is%250Aa%2520classical%2520and%2520fundamental%2520problem.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520fresh%250Aperspective%2520on%2520length%2520generalization%252C%2520shifting%2520the%2520focus%2520from%2520the%2520conventional%250Aemphasis%2520on%2520input%2520features%2520such%2520as%2520positional%2520encodings%2520or%2520data%2520structures%2520to%250Athe%2520output%2520distribution%2520of%2520the%2520model.%2520Specifically%252C%2520through%2520case%2520studies%2520on%250Asynthetic%2520tasks%252C%2520we%2520highlight%2520the%2520critical%2520role%2520of%2520%255Ctextbf%257Blong-short%250Aalignment%257D%2520--%2520the%2520consistency%2520of%2520output%2520distributions%2520across%2520sequences%2520of%250Avarying%2520lengths.%2520Extending%2520this%2520insight%2520to%2520natural%2520language%2520tasks%252C%2520we%2520propose%2520a%250Ametric%2520called%2520Long-Short%2520Misalignment%2520to%2520quantify%2520this%2520phenomenon%252C%2520uncovering%2520a%250Astrong%2520correlation%2520between%2520the%2520metric%2520and%2520length%2520generalization%2520performance.%250ABuilding%2520on%2520these%2520findings%252C%2520we%2520develop%2520a%2520regularization%2520term%2520that%2520promotes%250Along-short%2520alignment%2520during%2520training.%2520Extensive%2520experiments%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520approach%252C%2520offering%2520new%2520insights%2520for%2520achieving%2520more%250Aeffective%2520long-context%2520modeling%2520in%2520LLMs.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/PKU-ML/LongShortAlignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Short%20Alignment%20for%20Effective%20Long-Context%20Modeling%20in%20LLMs&entry.906535625=Tianqi%20Du%20and%20Haotian%20Huang%20and%20Yifei%20Wang%20and%20Yisen%20Wang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20exhibited%20impressive%20performance%20and%0Asurprising%20emergent%20properties.%20However%2C%20their%20effectiveness%20remains%20limited%20by%0Athe%20fixed%20context%20window%20of%20the%20transformer%20architecture%2C%20posing%20challenges%20for%0Along-context%20modeling.%20Among%20these%20challenges%2C%20length%20generalization%20--%20the%0Aability%20to%20generalize%20to%20sequences%20longer%20than%20those%20seen%20during%20training%20--%20is%0Aa%20classical%20and%20fundamental%20problem.%20In%20this%20work%2C%20we%20propose%20a%20fresh%0Aperspective%20on%20length%20generalization%2C%20shifting%20the%20focus%20from%20the%20conventional%0Aemphasis%20on%20input%20features%20such%20as%20positional%20encodings%20or%20data%20structures%20to%0Athe%20output%20distribution%20of%20the%20model.%20Specifically%2C%20through%20case%20studies%20on%0Asynthetic%20tasks%2C%20we%20highlight%20the%20critical%20role%20of%20%5Ctextbf%7Blong-short%0Aalignment%7D%20--%20the%20consistency%20of%20output%20distributions%20across%20sequences%20of%0Avarying%20lengths.%20Extending%20this%20insight%20to%20natural%20language%20tasks%2C%20we%20propose%20a%0Ametric%20called%20Long-Short%20Misalignment%20to%20quantify%20this%20phenomenon%2C%20uncovering%20a%0Astrong%20correlation%20between%20the%20metric%20and%20length%20generalization%20performance.%0ABuilding%20on%20these%20findings%2C%20we%20develop%20a%20regularization%20term%20that%20promotes%0Along-short%20alignment%20during%20training.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20approach%2C%20offering%20new%20insights%20for%20achieving%20more%0Aeffective%20long-context%20modeling%20in%20LLMs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PKU-ML/LongShortAlignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11769v1&entry.124074799=Read"},
{"title": "O2Former:Direction-Aware and Multi-Scale Query Enhancement for SAR Ship\n  Instance Segmentation", "author": "F. Gao and Y Li and X He and J Sun and J Wang", "abstract": "  Instance segmentation of ships in synthetic aperture radar (SAR) imagery is\ncritical for applications such as maritime monitoring, environmental analysis,\nand national security. SAR ship images present challenges including scale\nvariation, object density, and fuzzy target boundary, which are often\noverlooked in existing methods, leading to suboptimal performance. In this\nwork, we propose O2Former, a tailored instance segmentation framework that\nextends Mask2Former by fully leveraging the structural characteristics of SAR\nimagery. We introduce two key components. The first is the Optimized Query\nGenerator(OQG). It enables multi-scale feature interaction by jointly encoding\nshallow positional cues and high-level semantic information. This improves\nquery quality and convergence efficiency. The second component is the\nOrientation-Aware Embedding Module(OAEM). It enhances directional sensitivity\nthrough direction-aware convolution and polar-coordinate encoding. This\neffectively addresses the challenge of uneven target orientations in SAR\nscenes. Together, these modules facilitate precise feature alignment from\nbackbone to decoder and strengthen the model's capacity to capture fine-grained\nstructural details. Extensive experiments demonstrate that O2Former outperforms\nstate of the art instance segmentation baselines, validating its effectiveness\nand generalization on SAR ship datasets.\n", "link": "http://arxiv.org/abs/2506.11913v1", "date": "2025-06-13", "relevancy": 2.5824, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5208}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5202}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20O2Former%3ADirection-Aware%20and%20Multi-Scale%20Query%20Enhancement%20for%20SAR%20Ship%0A%20%20Instance%20Segmentation&body=Title%3A%20O2Former%3ADirection-Aware%20and%20Multi-Scale%20Query%20Enhancement%20for%20SAR%20Ship%0A%20%20Instance%20Segmentation%0AAuthor%3A%20F.%20Gao%20and%20Y%20Li%20and%20X%20He%20and%20J%20Sun%20and%20J%20Wang%0AAbstract%3A%20%20%20Instance%20segmentation%20of%20ships%20in%20synthetic%20aperture%20radar%20%28SAR%29%20imagery%20is%0Acritical%20for%20applications%20such%20as%20maritime%20monitoring%2C%20environmental%20analysis%2C%0Aand%20national%20security.%20SAR%20ship%20images%20present%20challenges%20including%20scale%0Avariation%2C%20object%20density%2C%20and%20fuzzy%20target%20boundary%2C%20which%20are%20often%0Aoverlooked%20in%20existing%20methods%2C%20leading%20to%20suboptimal%20performance.%20In%20this%0Awork%2C%20we%20propose%20O2Former%2C%20a%20tailored%20instance%20segmentation%20framework%20that%0Aextends%20Mask2Former%20by%20fully%20leveraging%20the%20structural%20characteristics%20of%20SAR%0Aimagery.%20We%20introduce%20two%20key%20components.%20The%20first%20is%20the%20Optimized%20Query%0AGenerator%28OQG%29.%20It%20enables%20multi-scale%20feature%20interaction%20by%20jointly%20encoding%0Ashallow%20positional%20cues%20and%20high-level%20semantic%20information.%20This%20improves%0Aquery%20quality%20and%20convergence%20efficiency.%20The%20second%20component%20is%20the%0AOrientation-Aware%20Embedding%20Module%28OAEM%29.%20It%20enhances%20directional%20sensitivity%0Athrough%20direction-aware%20convolution%20and%20polar-coordinate%20encoding.%20This%0Aeffectively%20addresses%20the%20challenge%20of%20uneven%20target%20orientations%20in%20SAR%0Ascenes.%20Together%2C%20these%20modules%20facilitate%20precise%20feature%20alignment%20from%0Abackbone%20to%20decoder%20and%20strengthen%20the%20model%27s%20capacity%20to%20capture%20fine-grained%0Astructural%20details.%20Extensive%20experiments%20demonstrate%20that%20O2Former%20outperforms%0Astate%20of%20the%20art%20instance%20segmentation%20baselines%2C%20validating%20its%20effectiveness%0Aand%20generalization%20on%20SAR%20ship%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DO2Former%253ADirection-Aware%2520and%2520Multi-Scale%2520Query%2520Enhancement%2520for%2520SAR%2520Ship%250A%2520%2520Instance%2520Segmentation%26entry.906535625%3DF.%2520Gao%2520and%2520Y%2520Li%2520and%2520X%2520He%2520and%2520J%2520Sun%2520and%2520J%2520Wang%26entry.1292438233%3D%2520%2520Instance%2520segmentation%2520of%2520ships%2520in%2520synthetic%2520aperture%2520radar%2520%2528SAR%2529%2520imagery%2520is%250Acritical%2520for%2520applications%2520such%2520as%2520maritime%2520monitoring%252C%2520environmental%2520analysis%252C%250Aand%2520national%2520security.%2520SAR%2520ship%2520images%2520present%2520challenges%2520including%2520scale%250Avariation%252C%2520object%2520density%252C%2520and%2520fuzzy%2520target%2520boundary%252C%2520which%2520are%2520often%250Aoverlooked%2520in%2520existing%2520methods%252C%2520leading%2520to%2520suboptimal%2520performance.%2520In%2520this%250Awork%252C%2520we%2520propose%2520O2Former%252C%2520a%2520tailored%2520instance%2520segmentation%2520framework%2520that%250Aextends%2520Mask2Former%2520by%2520fully%2520leveraging%2520the%2520structural%2520characteristics%2520of%2520SAR%250Aimagery.%2520We%2520introduce%2520two%2520key%2520components.%2520The%2520first%2520is%2520the%2520Optimized%2520Query%250AGenerator%2528OQG%2529.%2520It%2520enables%2520multi-scale%2520feature%2520interaction%2520by%2520jointly%2520encoding%250Ashallow%2520positional%2520cues%2520and%2520high-level%2520semantic%2520information.%2520This%2520improves%250Aquery%2520quality%2520and%2520convergence%2520efficiency.%2520The%2520second%2520component%2520is%2520the%250AOrientation-Aware%2520Embedding%2520Module%2528OAEM%2529.%2520It%2520enhances%2520directional%2520sensitivity%250Athrough%2520direction-aware%2520convolution%2520and%2520polar-coordinate%2520encoding.%2520This%250Aeffectively%2520addresses%2520the%2520challenge%2520of%2520uneven%2520target%2520orientations%2520in%2520SAR%250Ascenes.%2520Together%252C%2520these%2520modules%2520facilitate%2520precise%2520feature%2520alignment%2520from%250Abackbone%2520to%2520decoder%2520and%2520strengthen%2520the%2520model%2527s%2520capacity%2520to%2520capture%2520fine-grained%250Astructural%2520details.%2520Extensive%2520experiments%2520demonstrate%2520that%2520O2Former%2520outperforms%250Astate%2520of%2520the%2520art%2520instance%2520segmentation%2520baselines%252C%2520validating%2520its%2520effectiveness%250Aand%2520generalization%2520on%2520SAR%2520ship%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=O2Former%3ADirection-Aware%20and%20Multi-Scale%20Query%20Enhancement%20for%20SAR%20Ship%0A%20%20Instance%20Segmentation&entry.906535625=F.%20Gao%20and%20Y%20Li%20and%20X%20He%20and%20J%20Sun%20and%20J%20Wang&entry.1292438233=%20%20Instance%20segmentation%20of%20ships%20in%20synthetic%20aperture%20radar%20%28SAR%29%20imagery%20is%0Acritical%20for%20applications%20such%20as%20maritime%20monitoring%2C%20environmental%20analysis%2C%0Aand%20national%20security.%20SAR%20ship%20images%20present%20challenges%20including%20scale%0Avariation%2C%20object%20density%2C%20and%20fuzzy%20target%20boundary%2C%20which%20are%20often%0Aoverlooked%20in%20existing%20methods%2C%20leading%20to%20suboptimal%20performance.%20In%20this%0Awork%2C%20we%20propose%20O2Former%2C%20a%20tailored%20instance%20segmentation%20framework%20that%0Aextends%20Mask2Former%20by%20fully%20leveraging%20the%20structural%20characteristics%20of%20SAR%0Aimagery.%20We%20introduce%20two%20key%20components.%20The%20first%20is%20the%20Optimized%20Query%0AGenerator%28OQG%29.%20It%20enables%20multi-scale%20feature%20interaction%20by%20jointly%20encoding%0Ashallow%20positional%20cues%20and%20high-level%20semantic%20information.%20This%20improves%0Aquery%20quality%20and%20convergence%20efficiency.%20The%20second%20component%20is%20the%0AOrientation-Aware%20Embedding%20Module%28OAEM%29.%20It%20enhances%20directional%20sensitivity%0Athrough%20direction-aware%20convolution%20and%20polar-coordinate%20encoding.%20This%0Aeffectively%20addresses%20the%20challenge%20of%20uneven%20target%20orientations%20in%20SAR%0Ascenes.%20Together%2C%20these%20modules%20facilitate%20precise%20feature%20alignment%20from%0Abackbone%20to%20decoder%20and%20strengthen%20the%20model%27s%20capacity%20to%20capture%20fine-grained%0Astructural%20details.%20Extensive%20experiments%20demonstrate%20that%20O2Former%20outperforms%0Astate%20of%20the%20art%20instance%20segmentation%20baselines%2C%20validating%20its%20effectiveness%0Aand%20generalization%20on%20SAR%20ship%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11913v1&entry.124074799=Read"},
{"title": "Self-interpreting Adversarial Images", "author": "Tingwei Zhang and Collin Zhang and John X. Morris and Eugene Bagdasarian and Vitaly Shmatikov", "abstract": "  We introduce a new type of indirect, cross-modal injection attacks against\nvisual language models that enable creation of self-interpreting images. These\nimages contain hidden \"meta-instructions\" that control how models answer users'\nquestions about the image and steer models' outputs to express an\nadversary-chosen style, sentiment, or point of view.\n  Self-interpreting images act as soft prompts, conditioning the model to\nsatisfy the adversary's (meta-)objective while still producing answers based on\nthe image's visual content. Meta-instructions are thus a stronger form of\nprompt injection. Adversarial images look natural and the model's answers are\ncoherent and plausible, yet they also follow the adversary-chosen\ninterpretation, e.g., political spin, or even objectives that are not\nachievable with explicit text instructions.\n  We evaluate the efficacy of self-interpreting images for a variety of models,\ninterpretations, and user prompts. We describe how these attacks could cause\nharm by enabling creation of self-interpreting content that carries spam,\nmisinformation, or spin. Finally, we discuss defenses.\n", "link": "http://arxiv.org/abs/2407.08970v4", "date": "2025-06-13", "relevancy": 2.5562, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5171}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5131}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-interpreting%20Adversarial%20Images&body=Title%3A%20Self-interpreting%20Adversarial%20Images%0AAuthor%3A%20Tingwei%20Zhang%20and%20Collin%20Zhang%20and%20John%20X.%20Morris%20and%20Eugene%20Bagdasarian%20and%20Vitaly%20Shmatikov%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20type%20of%20indirect%2C%20cross-modal%20injection%20attacks%20against%0Avisual%20language%20models%20that%20enable%20creation%20of%20self-interpreting%20images.%20These%0Aimages%20contain%20hidden%20%22meta-instructions%22%20that%20control%20how%20models%20answer%20users%27%0Aquestions%20about%20the%20image%20and%20steer%20models%27%20outputs%20to%20express%20an%0Aadversary-chosen%20style%2C%20sentiment%2C%20or%20point%20of%20view.%0A%20%20Self-interpreting%20images%20act%20as%20soft%20prompts%2C%20conditioning%20the%20model%20to%0Asatisfy%20the%20adversary%27s%20%28meta-%29objective%20while%20still%20producing%20answers%20based%20on%0Athe%20image%27s%20visual%20content.%20Meta-instructions%20are%20thus%20a%20stronger%20form%20of%0Aprompt%20injection.%20Adversarial%20images%20look%20natural%20and%20the%20model%27s%20answers%20are%0Acoherent%20and%20plausible%2C%20yet%20they%20also%20follow%20the%20adversary-chosen%0Ainterpretation%2C%20e.g.%2C%20political%20spin%2C%20or%20even%20objectives%20that%20are%20not%0Aachievable%20with%20explicit%20text%20instructions.%0A%20%20We%20evaluate%20the%20efficacy%20of%20self-interpreting%20images%20for%20a%20variety%20of%20models%2C%0Ainterpretations%2C%20and%20user%20prompts.%20We%20describe%20how%20these%20attacks%20could%20cause%0Aharm%20by%20enabling%20creation%20of%20self-interpreting%20content%20that%20carries%20spam%2C%0Amisinformation%2C%20or%20spin.%20Finally%2C%20we%20discuss%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08970v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-interpreting%2520Adversarial%2520Images%26entry.906535625%3DTingwei%2520Zhang%2520and%2520Collin%2520Zhang%2520and%2520John%2520X.%2520Morris%2520and%2520Eugene%2520Bagdasarian%2520and%2520Vitaly%2520Shmatikov%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520type%2520of%2520indirect%252C%2520cross-modal%2520injection%2520attacks%2520against%250Avisual%2520language%2520models%2520that%2520enable%2520creation%2520of%2520self-interpreting%2520images.%2520These%250Aimages%2520contain%2520hidden%2520%2522meta-instructions%2522%2520that%2520control%2520how%2520models%2520answer%2520users%2527%250Aquestions%2520about%2520the%2520image%2520and%2520steer%2520models%2527%2520outputs%2520to%2520express%2520an%250Aadversary-chosen%2520style%252C%2520sentiment%252C%2520or%2520point%2520of%2520view.%250A%2520%2520Self-interpreting%2520images%2520act%2520as%2520soft%2520prompts%252C%2520conditioning%2520the%2520model%2520to%250Asatisfy%2520the%2520adversary%2527s%2520%2528meta-%2529objective%2520while%2520still%2520producing%2520answers%2520based%2520on%250Athe%2520image%2527s%2520visual%2520content.%2520Meta-instructions%2520are%2520thus%2520a%2520stronger%2520form%2520of%250Aprompt%2520injection.%2520Adversarial%2520images%2520look%2520natural%2520and%2520the%2520model%2527s%2520answers%2520are%250Acoherent%2520and%2520plausible%252C%2520yet%2520they%2520also%2520follow%2520the%2520adversary-chosen%250Ainterpretation%252C%2520e.g.%252C%2520political%2520spin%252C%2520or%2520even%2520objectives%2520that%2520are%2520not%250Aachievable%2520with%2520explicit%2520text%2520instructions.%250A%2520%2520We%2520evaluate%2520the%2520efficacy%2520of%2520self-interpreting%2520images%2520for%2520a%2520variety%2520of%2520models%252C%250Ainterpretations%252C%2520and%2520user%2520prompts.%2520We%2520describe%2520how%2520these%2520attacks%2520could%2520cause%250Aharm%2520by%2520enabling%2520creation%2520of%2520self-interpreting%2520content%2520that%2520carries%2520spam%252C%250Amisinformation%252C%2520or%2520spin.%2520Finally%252C%2520we%2520discuss%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08970v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-interpreting%20Adversarial%20Images&entry.906535625=Tingwei%20Zhang%20and%20Collin%20Zhang%20and%20John%20X.%20Morris%20and%20Eugene%20Bagdasarian%20and%20Vitaly%20Shmatikov&entry.1292438233=%20%20We%20introduce%20a%20new%20type%20of%20indirect%2C%20cross-modal%20injection%20attacks%20against%0Avisual%20language%20models%20that%20enable%20creation%20of%20self-interpreting%20images.%20These%0Aimages%20contain%20hidden%20%22meta-instructions%22%20that%20control%20how%20models%20answer%20users%27%0Aquestions%20about%20the%20image%20and%20steer%20models%27%20outputs%20to%20express%20an%0Aadversary-chosen%20style%2C%20sentiment%2C%20or%20point%20of%20view.%0A%20%20Self-interpreting%20images%20act%20as%20soft%20prompts%2C%20conditioning%20the%20model%20to%0Asatisfy%20the%20adversary%27s%20%28meta-%29objective%20while%20still%20producing%20answers%20based%20on%0Athe%20image%27s%20visual%20content.%20Meta-instructions%20are%20thus%20a%20stronger%20form%20of%0Aprompt%20injection.%20Adversarial%20images%20look%20natural%20and%20the%20model%27s%20answers%20are%0Acoherent%20and%20plausible%2C%20yet%20they%20also%20follow%20the%20adversary-chosen%0Ainterpretation%2C%20e.g.%2C%20political%20spin%2C%20or%20even%20objectives%20that%20are%20not%0Aachievable%20with%20explicit%20text%20instructions.%0A%20%20We%20evaluate%20the%20efficacy%20of%20self-interpreting%20images%20for%20a%20variety%20of%20models%2C%0Ainterpretations%2C%20and%20user%20prompts.%20We%20describe%20how%20these%20attacks%20could%20cause%0Aharm%20by%20enabling%20creation%20of%20self-interpreting%20content%20that%20carries%20spam%2C%0Amisinformation%2C%20or%20spin.%20Finally%2C%20we%20discuss%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08970v4&entry.124074799=Read"},
{"title": "Deep Symmetric Autoencoders from the Eckart-Young-Schmidt Perspective", "author": "Simone Brivio and Nicola Rares Franco", "abstract": "  Deep autoencoders have become a fundamental tool in various machine learning\napplications, ranging from dimensionality reduction and reduced order modeling\nof partial differential equations to anomaly detection and neural machine\ntranslation. Despite their empirical success, a solid theoretical foundation\nfor their expressiveness remains elusive, particularly when compared to\nclassical projection-based techniques. In this work, we aim to take a step\nforward in this direction by presenting a comprehensive analysis of what we\nrefer to as symmetric autoencoders, a broad class of deep learning\narchitectures ubiquitous in the literature. Specifically, we introduce a formal\ndistinction between different classes of symmetric architectures, analyzing\ntheir strengths and limitations from a mathematical perspective. For instance,\nwe show that the reconstruction error of symmetric autoencoders with\northonormality constraints can be understood by leveraging the well-renowned\nEckart-Young-Schmidt (EYS) theorem. As a byproduct of our analysis, we end up\ndeveloping the EYS initialization strategy for symmetric autoencoders, which is\nbased on an iterated application of the Singular Value Decomposition (SVD). To\nvalidate our findings, we conduct a series of numerical experiments where we\nbenchmark our proposal against conventional deep autoencoders, discussing the\nimportance of model design and initialization.\n", "link": "http://arxiv.org/abs/2506.11641v1", "date": "2025-06-13", "relevancy": 2.5443, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5651}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4873}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Symmetric%20Autoencoders%20from%20the%20Eckart-Young-Schmidt%20Perspective&body=Title%3A%20Deep%20Symmetric%20Autoencoders%20from%20the%20Eckart-Young-Schmidt%20Perspective%0AAuthor%3A%20Simone%20Brivio%20and%20Nicola%20Rares%20Franco%0AAbstract%3A%20%20%20Deep%20autoencoders%20have%20become%20a%20fundamental%20tool%20in%20various%20machine%20learning%0Aapplications%2C%20ranging%20from%20dimensionality%20reduction%20and%20reduced%20order%20modeling%0Aof%20partial%20differential%20equations%20to%20anomaly%20detection%20and%20neural%20machine%0Atranslation.%20Despite%20their%20empirical%20success%2C%20a%20solid%20theoretical%20foundation%0Afor%20their%20expressiveness%20remains%20elusive%2C%20particularly%20when%20compared%20to%0Aclassical%20projection-based%20techniques.%20In%20this%20work%2C%20we%20aim%20to%20take%20a%20step%0Aforward%20in%20this%20direction%20by%20presenting%20a%20comprehensive%20analysis%20of%20what%20we%0Arefer%20to%20as%20symmetric%20autoencoders%2C%20a%20broad%20class%20of%20deep%20learning%0Aarchitectures%20ubiquitous%20in%20the%20literature.%20Specifically%2C%20we%20introduce%20a%20formal%0Adistinction%20between%20different%20classes%20of%20symmetric%20architectures%2C%20analyzing%0Atheir%20strengths%20and%20limitations%20from%20a%20mathematical%20perspective.%20For%20instance%2C%0Awe%20show%20that%20the%20reconstruction%20error%20of%20symmetric%20autoencoders%20with%0Aorthonormality%20constraints%20can%20be%20understood%20by%20leveraging%20the%20well-renowned%0AEckart-Young-Schmidt%20%28EYS%29%20theorem.%20As%20a%20byproduct%20of%20our%20analysis%2C%20we%20end%20up%0Adeveloping%20the%20EYS%20initialization%20strategy%20for%20symmetric%20autoencoders%2C%20which%20is%0Abased%20on%20an%20iterated%20application%20of%20the%20Singular%20Value%20Decomposition%20%28SVD%29.%20To%0Avalidate%20our%20findings%2C%20we%20conduct%20a%20series%20of%20numerical%20experiments%20where%20we%0Abenchmark%20our%20proposal%20against%20conventional%20deep%20autoencoders%2C%20discussing%20the%0Aimportance%20of%20model%20design%20and%20initialization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Symmetric%2520Autoencoders%2520from%2520the%2520Eckart-Young-Schmidt%2520Perspective%26entry.906535625%3DSimone%2520Brivio%2520and%2520Nicola%2520Rares%2520Franco%26entry.1292438233%3D%2520%2520Deep%2520autoencoders%2520have%2520become%2520a%2520fundamental%2520tool%2520in%2520various%2520machine%2520learning%250Aapplications%252C%2520ranging%2520from%2520dimensionality%2520reduction%2520and%2520reduced%2520order%2520modeling%250Aof%2520partial%2520differential%2520equations%2520to%2520anomaly%2520detection%2520and%2520neural%2520machine%250Atranslation.%2520Despite%2520their%2520empirical%2520success%252C%2520a%2520solid%2520theoretical%2520foundation%250Afor%2520their%2520expressiveness%2520remains%2520elusive%252C%2520particularly%2520when%2520compared%2520to%250Aclassical%2520projection-based%2520techniques.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520take%2520a%2520step%250Aforward%2520in%2520this%2520direction%2520by%2520presenting%2520a%2520comprehensive%2520analysis%2520of%2520what%2520we%250Arefer%2520to%2520as%2520symmetric%2520autoencoders%252C%2520a%2520broad%2520class%2520of%2520deep%2520learning%250Aarchitectures%2520ubiquitous%2520in%2520the%2520literature.%2520Specifically%252C%2520we%2520introduce%2520a%2520formal%250Adistinction%2520between%2520different%2520classes%2520of%2520symmetric%2520architectures%252C%2520analyzing%250Atheir%2520strengths%2520and%2520limitations%2520from%2520a%2520mathematical%2520perspective.%2520For%2520instance%252C%250Awe%2520show%2520that%2520the%2520reconstruction%2520error%2520of%2520symmetric%2520autoencoders%2520with%250Aorthonormality%2520constraints%2520can%2520be%2520understood%2520by%2520leveraging%2520the%2520well-renowned%250AEckart-Young-Schmidt%2520%2528EYS%2529%2520theorem.%2520As%2520a%2520byproduct%2520of%2520our%2520analysis%252C%2520we%2520end%2520up%250Adeveloping%2520the%2520EYS%2520initialization%2520strategy%2520for%2520symmetric%2520autoencoders%252C%2520which%2520is%250Abased%2520on%2520an%2520iterated%2520application%2520of%2520the%2520Singular%2520Value%2520Decomposition%2520%2528SVD%2529.%2520To%250Avalidate%2520our%2520findings%252C%2520we%2520conduct%2520a%2520series%2520of%2520numerical%2520experiments%2520where%2520we%250Abenchmark%2520our%2520proposal%2520against%2520conventional%2520deep%2520autoencoders%252C%2520discussing%2520the%250Aimportance%2520of%2520model%2520design%2520and%2520initialization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Symmetric%20Autoencoders%20from%20the%20Eckart-Young-Schmidt%20Perspective&entry.906535625=Simone%20Brivio%20and%20Nicola%20Rares%20Franco&entry.1292438233=%20%20Deep%20autoencoders%20have%20become%20a%20fundamental%20tool%20in%20various%20machine%20learning%0Aapplications%2C%20ranging%20from%20dimensionality%20reduction%20and%20reduced%20order%20modeling%0Aof%20partial%20differential%20equations%20to%20anomaly%20detection%20and%20neural%20machine%0Atranslation.%20Despite%20their%20empirical%20success%2C%20a%20solid%20theoretical%20foundation%0Afor%20their%20expressiveness%20remains%20elusive%2C%20particularly%20when%20compared%20to%0Aclassical%20projection-based%20techniques.%20In%20this%20work%2C%20we%20aim%20to%20take%20a%20step%0Aforward%20in%20this%20direction%20by%20presenting%20a%20comprehensive%20analysis%20of%20what%20we%0Arefer%20to%20as%20symmetric%20autoencoders%2C%20a%20broad%20class%20of%20deep%20learning%0Aarchitectures%20ubiquitous%20in%20the%20literature.%20Specifically%2C%20we%20introduce%20a%20formal%0Adistinction%20between%20different%20classes%20of%20symmetric%20architectures%2C%20analyzing%0Atheir%20strengths%20and%20limitations%20from%20a%20mathematical%20perspective.%20For%20instance%2C%0Awe%20show%20that%20the%20reconstruction%20error%20of%20symmetric%20autoencoders%20with%0Aorthonormality%20constraints%20can%20be%20understood%20by%20leveraging%20the%20well-renowned%0AEckart-Young-Schmidt%20%28EYS%29%20theorem.%20As%20a%20byproduct%20of%20our%20analysis%2C%20we%20end%20up%0Adeveloping%20the%20EYS%20initialization%20strategy%20for%20symmetric%20autoencoders%2C%20which%20is%0Abased%20on%20an%20iterated%20application%20of%20the%20Singular%20Value%20Decomposition%20%28SVD%29.%20To%0Avalidate%20our%20findings%2C%20we%20conduct%20a%20series%20of%20numerical%20experiments%20where%20we%0Abenchmark%20our%20proposal%20against%20conventional%20deep%20autoencoders%2C%20discussing%20the%0Aimportance%20of%20model%20design%20and%20initialization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11641v1&entry.124074799=Read"},
{"title": "crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation\n  Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to\n  2023", "author": "Navodini Wijethilake and Reuben Dorent and Marina Ivory and Aaron Kujawa and Stefan Cornelissen and Patrick Langenhuizen and Mohamed Okasha and Anna Oviedova and Hexin Dong and Bogyeong Kang and Guillaume Sall\u00e9 and Luyi Han and Ziyuan Zhao and Han Liu and Tao Yang and Shahad Hardan and Hussain Alasmawi and Santosh Sanjeev and Yuzhou Zhuang and Satoshi Kondo and Maria Baldeon Calisto and Shaikh Muhammad Uzair Noman and Cancan Chen and Ipek Oguz and Rongguo Zhang and Mina Rezaei and Susana K. Lai-Yuen and Satoshi Kasai and Chih-Cheng Hung and Mohammad Yaqub and Lisheng Wang and Benoit M. Dawant and Cuntai Guan and Ritse Mann and Vincent Jaouen and Ji-Wung Han and Li Zhang and Jonathan Shapey and Tom Vercauteren", "abstract": "  The cross-Modality Domain Adaptation (crossMoDA) challenge series, initiated\nin 2021 in conjunction with the International Conference on Medical Image\nComputing and Computer Assisted Intervention (MICCAI), focuses on unsupervised\ncross-modality segmentation, learning from contrast-enhanced T1 (ceT1) and\ntransferring to T2 MRI. The task is an extreme example of domain shift chosen\nto serve as a meaningful and illustrative benchmark. From a clinical\napplication perspective, it aims to automate Vestibular Schwannoma (VS) and\ncochlea segmentation on T2 scans for more cost-effective VS management. Over\ntime, the challenge objectives have evolved to enhance its clinical relevance.\nThe challenge evolved from using single-institutional data and basic\nsegmentation in 2021 to incorporating multi-institutional data and Koos grading\nin 2022, and by 2023, it included heterogeneous routine data and\nsub-segmentation of intra- and extra-meatal tumour components. In this work, we\nreport the findings of the 2022 and 2023 editions and perform a retrospective\nanalysis of the challenge progression over the years. The observations from the\nsuccessive challenge contributions indicate that the number of outliers\ndecreases with an expanding dataset. This is notable since the diversity of\nscanning protocols of the datasets concurrently increased. The winning approach\nof the 2023 edition reduced the number of outliers on the 2021 and 2022 testing\ndata, demonstrating how increased data heterogeneity can enhance segmentation\nperformance even on homogeneous data. However, the cochlea Dice score declined\nin 2023, likely due to the added complexity from tumour sub-annotations\naffecting overall segmentation performance. While progress is still needed for\nclinically acceptable VS segmentation, the plateauing performance suggests that\na more challenging cross-modal task may better serve future benchmarking.\n", "link": "http://arxiv.org/abs/2506.12006v1", "date": "2025-06-13", "relevancy": 2.5429, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20crossMoDA%20Challenge%3A%20Evolution%20of%20Cross-Modality%20Domain%20Adaptation%0A%20%20Techniques%20for%20Vestibular%20Schwannoma%20and%20Cochlea%20Segmentation%20from%202021%20to%0A%20%202023&body=Title%3A%20crossMoDA%20Challenge%3A%20Evolution%20of%20Cross-Modality%20Domain%20Adaptation%0A%20%20Techniques%20for%20Vestibular%20Schwannoma%20and%20Cochlea%20Segmentation%20from%202021%20to%0A%20%202023%0AAuthor%3A%20Navodini%20Wijethilake%20and%20Reuben%20Dorent%20and%20Marina%20Ivory%20and%20Aaron%20Kujawa%20and%20Stefan%20Cornelissen%20and%20Patrick%20Langenhuizen%20and%20Mohamed%20Okasha%20and%20Anna%20Oviedova%20and%20Hexin%20Dong%20and%20Bogyeong%20Kang%20and%20Guillaume%20Sall%C3%A9%20and%20Luyi%20Han%20and%20Ziyuan%20Zhao%20and%20Han%20Liu%20and%20Tao%20Yang%20and%20Shahad%20Hardan%20and%20Hussain%20Alasmawi%20and%20Santosh%20Sanjeev%20and%20Yuzhou%20Zhuang%20and%20Satoshi%20Kondo%20and%20Maria%20Baldeon%20Calisto%20and%20Shaikh%20Muhammad%20Uzair%20Noman%20and%20Cancan%20Chen%20and%20Ipek%20Oguz%20and%20Rongguo%20Zhang%20and%20Mina%20Rezaei%20and%20Susana%20K.%20Lai-Yuen%20and%20Satoshi%20Kasai%20and%20Chih-Cheng%20Hung%20and%20Mohammad%20Yaqub%20and%20Lisheng%20Wang%20and%20Benoit%20M.%20Dawant%20and%20Cuntai%20Guan%20and%20Ritse%20Mann%20and%20Vincent%20Jaouen%20and%20Ji-Wung%20Han%20and%20Li%20Zhang%20and%20Jonathan%20Shapey%20and%20Tom%20Vercauteren%0AAbstract%3A%20%20%20The%20cross-Modality%20Domain%20Adaptation%20%28crossMoDA%29%20challenge%20series%2C%20initiated%0Ain%202021%20in%20conjunction%20with%20the%20International%20Conference%20on%20Medical%20Image%0AComputing%20and%20Computer%20Assisted%20Intervention%20%28MICCAI%29%2C%20focuses%20on%20unsupervised%0Across-modality%20segmentation%2C%20learning%20from%20contrast-enhanced%20T1%20%28ceT1%29%20and%0Atransferring%20to%20T2%20MRI.%20The%20task%20is%20an%20extreme%20example%20of%20domain%20shift%20chosen%0Ato%20serve%20as%20a%20meaningful%20and%20illustrative%20benchmark.%20From%20a%20clinical%0Aapplication%20perspective%2C%20it%20aims%20to%20automate%20Vestibular%20Schwannoma%20%28VS%29%20and%0Acochlea%20segmentation%20on%20T2%20scans%20for%20more%20cost-effective%20VS%20management.%20Over%0Atime%2C%20the%20challenge%20objectives%20have%20evolved%20to%20enhance%20its%20clinical%20relevance.%0AThe%20challenge%20evolved%20from%20using%20single-institutional%20data%20and%20basic%0Asegmentation%20in%202021%20to%20incorporating%20multi-institutional%20data%20and%20Koos%20grading%0Ain%202022%2C%20and%20by%202023%2C%20it%20included%20heterogeneous%20routine%20data%20and%0Asub-segmentation%20of%20intra-%20and%20extra-meatal%20tumour%20components.%20In%20this%20work%2C%20we%0Areport%20the%20findings%20of%20the%202022%20and%202023%20editions%20and%20perform%20a%20retrospective%0Aanalysis%20of%20the%20challenge%20progression%20over%20the%20years.%20The%20observations%20from%20the%0Asuccessive%20challenge%20contributions%20indicate%20that%20the%20number%20of%20outliers%0Adecreases%20with%20an%20expanding%20dataset.%20This%20is%20notable%20since%20the%20diversity%20of%0Ascanning%20protocols%20of%20the%20datasets%20concurrently%20increased.%20The%20winning%20approach%0Aof%20the%202023%20edition%20reduced%20the%20number%20of%20outliers%20on%20the%202021%20and%202022%20testing%0Adata%2C%20demonstrating%20how%20increased%20data%20heterogeneity%20can%20enhance%20segmentation%0Aperformance%20even%20on%20homogeneous%20data.%20However%2C%20the%20cochlea%20Dice%20score%20declined%0Ain%202023%2C%20likely%20due%20to%20the%20added%20complexity%20from%20tumour%20sub-annotations%0Aaffecting%20overall%20segmentation%20performance.%20While%20progress%20is%20still%20needed%20for%0Aclinically%20acceptable%20VS%20segmentation%2C%20the%20plateauing%20performance%20suggests%20that%0Aa%20more%20challenging%20cross-modal%20task%20may%20better%20serve%20future%20benchmarking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DcrossMoDA%2520Challenge%253A%2520Evolution%2520of%2520Cross-Modality%2520Domain%2520Adaptation%250A%2520%2520Techniques%2520for%2520Vestibular%2520Schwannoma%2520and%2520Cochlea%2520Segmentation%2520from%25202021%2520to%250A%2520%25202023%26entry.906535625%3DNavodini%2520Wijethilake%2520and%2520Reuben%2520Dorent%2520and%2520Marina%2520Ivory%2520and%2520Aaron%2520Kujawa%2520and%2520Stefan%2520Cornelissen%2520and%2520Patrick%2520Langenhuizen%2520and%2520Mohamed%2520Okasha%2520and%2520Anna%2520Oviedova%2520and%2520Hexin%2520Dong%2520and%2520Bogyeong%2520Kang%2520and%2520Guillaume%2520Sall%25C3%25A9%2520and%2520Luyi%2520Han%2520and%2520Ziyuan%2520Zhao%2520and%2520Han%2520Liu%2520and%2520Tao%2520Yang%2520and%2520Shahad%2520Hardan%2520and%2520Hussain%2520Alasmawi%2520and%2520Santosh%2520Sanjeev%2520and%2520Yuzhou%2520Zhuang%2520and%2520Satoshi%2520Kondo%2520and%2520Maria%2520Baldeon%2520Calisto%2520and%2520Shaikh%2520Muhammad%2520Uzair%2520Noman%2520and%2520Cancan%2520Chen%2520and%2520Ipek%2520Oguz%2520and%2520Rongguo%2520Zhang%2520and%2520Mina%2520Rezaei%2520and%2520Susana%2520K.%2520Lai-Yuen%2520and%2520Satoshi%2520Kasai%2520and%2520Chih-Cheng%2520Hung%2520and%2520Mohammad%2520Yaqub%2520and%2520Lisheng%2520Wang%2520and%2520Benoit%2520M.%2520Dawant%2520and%2520Cuntai%2520Guan%2520and%2520Ritse%2520Mann%2520and%2520Vincent%2520Jaouen%2520and%2520Ji-Wung%2520Han%2520and%2520Li%2520Zhang%2520and%2520Jonathan%2520Shapey%2520and%2520Tom%2520Vercauteren%26entry.1292438233%3D%2520%2520The%2520cross-Modality%2520Domain%2520Adaptation%2520%2528crossMoDA%2529%2520challenge%2520series%252C%2520initiated%250Ain%25202021%2520in%2520conjunction%2520with%2520the%2520International%2520Conference%2520on%2520Medical%2520Image%250AComputing%2520and%2520Computer%2520Assisted%2520Intervention%2520%2528MICCAI%2529%252C%2520focuses%2520on%2520unsupervised%250Across-modality%2520segmentation%252C%2520learning%2520from%2520contrast-enhanced%2520T1%2520%2528ceT1%2529%2520and%250Atransferring%2520to%2520T2%2520MRI.%2520The%2520task%2520is%2520an%2520extreme%2520example%2520of%2520domain%2520shift%2520chosen%250Ato%2520serve%2520as%2520a%2520meaningful%2520and%2520illustrative%2520benchmark.%2520From%2520a%2520clinical%250Aapplication%2520perspective%252C%2520it%2520aims%2520to%2520automate%2520Vestibular%2520Schwannoma%2520%2528VS%2529%2520and%250Acochlea%2520segmentation%2520on%2520T2%2520scans%2520for%2520more%2520cost-effective%2520VS%2520management.%2520Over%250Atime%252C%2520the%2520challenge%2520objectives%2520have%2520evolved%2520to%2520enhance%2520its%2520clinical%2520relevance.%250AThe%2520challenge%2520evolved%2520from%2520using%2520single-institutional%2520data%2520and%2520basic%250Asegmentation%2520in%25202021%2520to%2520incorporating%2520multi-institutional%2520data%2520and%2520Koos%2520grading%250Ain%25202022%252C%2520and%2520by%25202023%252C%2520it%2520included%2520heterogeneous%2520routine%2520data%2520and%250Asub-segmentation%2520of%2520intra-%2520and%2520extra-meatal%2520tumour%2520components.%2520In%2520this%2520work%252C%2520we%250Areport%2520the%2520findings%2520of%2520the%25202022%2520and%25202023%2520editions%2520and%2520perform%2520a%2520retrospective%250Aanalysis%2520of%2520the%2520challenge%2520progression%2520over%2520the%2520years.%2520The%2520observations%2520from%2520the%250Asuccessive%2520challenge%2520contributions%2520indicate%2520that%2520the%2520number%2520of%2520outliers%250Adecreases%2520with%2520an%2520expanding%2520dataset.%2520This%2520is%2520notable%2520since%2520the%2520diversity%2520of%250Ascanning%2520protocols%2520of%2520the%2520datasets%2520concurrently%2520increased.%2520The%2520winning%2520approach%250Aof%2520the%25202023%2520edition%2520reduced%2520the%2520number%2520of%2520outliers%2520on%2520the%25202021%2520and%25202022%2520testing%250Adata%252C%2520demonstrating%2520how%2520increased%2520data%2520heterogeneity%2520can%2520enhance%2520segmentation%250Aperformance%2520even%2520on%2520homogeneous%2520data.%2520However%252C%2520the%2520cochlea%2520Dice%2520score%2520declined%250Ain%25202023%252C%2520likely%2520due%2520to%2520the%2520added%2520complexity%2520from%2520tumour%2520sub-annotations%250Aaffecting%2520overall%2520segmentation%2520performance.%2520While%2520progress%2520is%2520still%2520needed%2520for%250Aclinically%2520acceptable%2520VS%2520segmentation%252C%2520the%2520plateauing%2520performance%2520suggests%2520that%250Aa%2520more%2520challenging%2520cross-modal%2520task%2520may%2520better%2520serve%2520future%2520benchmarking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=crossMoDA%20Challenge%3A%20Evolution%20of%20Cross-Modality%20Domain%20Adaptation%0A%20%20Techniques%20for%20Vestibular%20Schwannoma%20and%20Cochlea%20Segmentation%20from%202021%20to%0A%20%202023&entry.906535625=Navodini%20Wijethilake%20and%20Reuben%20Dorent%20and%20Marina%20Ivory%20and%20Aaron%20Kujawa%20and%20Stefan%20Cornelissen%20and%20Patrick%20Langenhuizen%20and%20Mohamed%20Okasha%20and%20Anna%20Oviedova%20and%20Hexin%20Dong%20and%20Bogyeong%20Kang%20and%20Guillaume%20Sall%C3%A9%20and%20Luyi%20Han%20and%20Ziyuan%20Zhao%20and%20Han%20Liu%20and%20Tao%20Yang%20and%20Shahad%20Hardan%20and%20Hussain%20Alasmawi%20and%20Santosh%20Sanjeev%20and%20Yuzhou%20Zhuang%20and%20Satoshi%20Kondo%20and%20Maria%20Baldeon%20Calisto%20and%20Shaikh%20Muhammad%20Uzair%20Noman%20and%20Cancan%20Chen%20and%20Ipek%20Oguz%20and%20Rongguo%20Zhang%20and%20Mina%20Rezaei%20and%20Susana%20K.%20Lai-Yuen%20and%20Satoshi%20Kasai%20and%20Chih-Cheng%20Hung%20and%20Mohammad%20Yaqub%20and%20Lisheng%20Wang%20and%20Benoit%20M.%20Dawant%20and%20Cuntai%20Guan%20and%20Ritse%20Mann%20and%20Vincent%20Jaouen%20and%20Ji-Wung%20Han%20and%20Li%20Zhang%20and%20Jonathan%20Shapey%20and%20Tom%20Vercauteren&entry.1292438233=%20%20The%20cross-Modality%20Domain%20Adaptation%20%28crossMoDA%29%20challenge%20series%2C%20initiated%0Ain%202021%20in%20conjunction%20with%20the%20International%20Conference%20on%20Medical%20Image%0AComputing%20and%20Computer%20Assisted%20Intervention%20%28MICCAI%29%2C%20focuses%20on%20unsupervised%0Across-modality%20segmentation%2C%20learning%20from%20contrast-enhanced%20T1%20%28ceT1%29%20and%0Atransferring%20to%20T2%20MRI.%20The%20task%20is%20an%20extreme%20example%20of%20domain%20shift%20chosen%0Ato%20serve%20as%20a%20meaningful%20and%20illustrative%20benchmark.%20From%20a%20clinical%0Aapplication%20perspective%2C%20it%20aims%20to%20automate%20Vestibular%20Schwannoma%20%28VS%29%20and%0Acochlea%20segmentation%20on%20T2%20scans%20for%20more%20cost-effective%20VS%20management.%20Over%0Atime%2C%20the%20challenge%20objectives%20have%20evolved%20to%20enhance%20its%20clinical%20relevance.%0AThe%20challenge%20evolved%20from%20using%20single-institutional%20data%20and%20basic%0Asegmentation%20in%202021%20to%20incorporating%20multi-institutional%20data%20and%20Koos%20grading%0Ain%202022%2C%20and%20by%202023%2C%20it%20included%20heterogeneous%20routine%20data%20and%0Asub-segmentation%20of%20intra-%20and%20extra-meatal%20tumour%20components.%20In%20this%20work%2C%20we%0Areport%20the%20findings%20of%20the%202022%20and%202023%20editions%20and%20perform%20a%20retrospective%0Aanalysis%20of%20the%20challenge%20progression%20over%20the%20years.%20The%20observations%20from%20the%0Asuccessive%20challenge%20contributions%20indicate%20that%20the%20number%20of%20outliers%0Adecreases%20with%20an%20expanding%20dataset.%20This%20is%20notable%20since%20the%20diversity%20of%0Ascanning%20protocols%20of%20the%20datasets%20concurrently%20increased.%20The%20winning%20approach%0Aof%20the%202023%20edition%20reduced%20the%20number%20of%20outliers%20on%20the%202021%20and%202022%20testing%0Adata%2C%20demonstrating%20how%20increased%20data%20heterogeneity%20can%20enhance%20segmentation%0Aperformance%20even%20on%20homogeneous%20data.%20However%2C%20the%20cochlea%20Dice%20score%20declined%0Ain%202023%2C%20likely%20due%20to%20the%20added%20complexity%20from%20tumour%20sub-annotations%0Aaffecting%20overall%20segmentation%20performance.%20While%20progress%20is%20still%20needed%20for%0Aclinically%20acceptable%20VS%20segmentation%2C%20the%20plateauing%20performance%20suggests%20that%0Aa%20more%20challenging%20cross-modal%20task%20may%20better%20serve%20future%20benchmarking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12006v1&entry.124074799=Read"},
{"title": "Enter: Graduated Realism: A Pedagogical Framework for AI-Powered Avatars\n  in Virtual Reality Teacher Training", "author": "Judson Leroy Dean Haynes IV", "abstract": "  Virtual Reality simulators offer a powerful tool for teacher training, yet\nthe integration of AI-powered student avatars presents a critical challenge:\ndetermining the optimal level of avatar realism for effective pedagogy. This\nliterature review examines the evolution of avatar realism in VR teacher\ntraining, synthesizes its theoretical implications, and proposes a new\npedagogical framework to guide future design. Through a systematic review, this\npaper traces the progression from human-controlled avatars to generative AI\nprototypes. Applying learning theories like Cognitive Load Theory, we argue\nthat hyper-realism is not always optimal, as high-fidelity avatars can impose\nexcessive extraneous cognitive load on novices, a stance supported by recent\nempirical findings. A significant gap exists between the technological drive\nfor photorealism and the pedagogical need for scaffolded learning. To address\nthis gap, we propose Graduated Realism, a framework advocating for starting\ntrainees with lower-fidelity avatars and progressively increasing behavioral\ncomplexity as skills develop. To make this computationally feasible, we outline\na novel single-call architecture, Crazy Slots, which uses a probabilistic\nengine and a Retrieval-Augmented Generation database to generate authentic,\nreal-time responses without the latency and cost of multi-step reasoning\nmodels. This review provides evidence-based principles for designing the next\ngeneration of AI simulators, arguing that a pedagogically grounded approach to\nrealism is essential for creating scalable and effective teacher education\ntools.\n", "link": "http://arxiv.org/abs/2506.11890v1", "date": "2025-06-13", "relevancy": 2.5349, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5122}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.506}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enter%3A%20Graduated%20Realism%3A%20A%20Pedagogical%20Framework%20for%20AI-Powered%20Avatars%0A%20%20in%20Virtual%20Reality%20Teacher%20Training&body=Title%3A%20Enter%3A%20Graduated%20Realism%3A%20A%20Pedagogical%20Framework%20for%20AI-Powered%20Avatars%0A%20%20in%20Virtual%20Reality%20Teacher%20Training%0AAuthor%3A%20Judson%20Leroy%20Dean%20Haynes%20IV%0AAbstract%3A%20%20%20Virtual%20Reality%20simulators%20offer%20a%20powerful%20tool%20for%20teacher%20training%2C%20yet%0Athe%20integration%20of%20AI-powered%20student%20avatars%20presents%20a%20critical%20challenge%3A%0Adetermining%20the%20optimal%20level%20of%20avatar%20realism%20for%20effective%20pedagogy.%20This%0Aliterature%20review%20examines%20the%20evolution%20of%20avatar%20realism%20in%20VR%20teacher%0Atraining%2C%20synthesizes%20its%20theoretical%20implications%2C%20and%20proposes%20a%20new%0Apedagogical%20framework%20to%20guide%20future%20design.%20Through%20a%20systematic%20review%2C%20this%0Apaper%20traces%20the%20progression%20from%20human-controlled%20avatars%20to%20generative%20AI%0Aprototypes.%20Applying%20learning%20theories%20like%20Cognitive%20Load%20Theory%2C%20we%20argue%0Athat%20hyper-realism%20is%20not%20always%20optimal%2C%20as%20high-fidelity%20avatars%20can%20impose%0Aexcessive%20extraneous%20cognitive%20load%20on%20novices%2C%20a%20stance%20supported%20by%20recent%0Aempirical%20findings.%20A%20significant%20gap%20exists%20between%20the%20technological%20drive%0Afor%20photorealism%20and%20the%20pedagogical%20need%20for%20scaffolded%20learning.%20To%20address%0Athis%20gap%2C%20we%20propose%20Graduated%20Realism%2C%20a%20framework%20advocating%20for%20starting%0Atrainees%20with%20lower-fidelity%20avatars%20and%20progressively%20increasing%20behavioral%0Acomplexity%20as%20skills%20develop.%20To%20make%20this%20computationally%20feasible%2C%20we%20outline%0Aa%20novel%20single-call%20architecture%2C%20Crazy%20Slots%2C%20which%20uses%20a%20probabilistic%0Aengine%20and%20a%20Retrieval-Augmented%20Generation%20database%20to%20generate%20authentic%2C%0Areal-time%20responses%20without%20the%20latency%20and%20cost%20of%20multi-step%20reasoning%0Amodels.%20This%20review%20provides%20evidence-based%20principles%20for%20designing%20the%20next%0Ageneration%20of%20AI%20simulators%2C%20arguing%20that%20a%20pedagogically%20grounded%20approach%20to%0Arealism%20is%20essential%20for%20creating%20scalable%20and%20effective%20teacher%20education%0Atools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnter%253A%2520Graduated%2520Realism%253A%2520A%2520Pedagogical%2520Framework%2520for%2520AI-Powered%2520Avatars%250A%2520%2520in%2520Virtual%2520Reality%2520Teacher%2520Training%26entry.906535625%3DJudson%2520Leroy%2520Dean%2520Haynes%2520IV%26entry.1292438233%3D%2520%2520Virtual%2520Reality%2520simulators%2520offer%2520a%2520powerful%2520tool%2520for%2520teacher%2520training%252C%2520yet%250Athe%2520integration%2520of%2520AI-powered%2520student%2520avatars%2520presents%2520a%2520critical%2520challenge%253A%250Adetermining%2520the%2520optimal%2520level%2520of%2520avatar%2520realism%2520for%2520effective%2520pedagogy.%2520This%250Aliterature%2520review%2520examines%2520the%2520evolution%2520of%2520avatar%2520realism%2520in%2520VR%2520teacher%250Atraining%252C%2520synthesizes%2520its%2520theoretical%2520implications%252C%2520and%2520proposes%2520a%2520new%250Apedagogical%2520framework%2520to%2520guide%2520future%2520design.%2520Through%2520a%2520systematic%2520review%252C%2520this%250Apaper%2520traces%2520the%2520progression%2520from%2520human-controlled%2520avatars%2520to%2520generative%2520AI%250Aprototypes.%2520Applying%2520learning%2520theories%2520like%2520Cognitive%2520Load%2520Theory%252C%2520we%2520argue%250Athat%2520hyper-realism%2520is%2520not%2520always%2520optimal%252C%2520as%2520high-fidelity%2520avatars%2520can%2520impose%250Aexcessive%2520extraneous%2520cognitive%2520load%2520on%2520novices%252C%2520a%2520stance%2520supported%2520by%2520recent%250Aempirical%2520findings.%2520A%2520significant%2520gap%2520exists%2520between%2520the%2520technological%2520drive%250Afor%2520photorealism%2520and%2520the%2520pedagogical%2520need%2520for%2520scaffolded%2520learning.%2520To%2520address%250Athis%2520gap%252C%2520we%2520propose%2520Graduated%2520Realism%252C%2520a%2520framework%2520advocating%2520for%2520starting%250Atrainees%2520with%2520lower-fidelity%2520avatars%2520and%2520progressively%2520increasing%2520behavioral%250Acomplexity%2520as%2520skills%2520develop.%2520To%2520make%2520this%2520computationally%2520feasible%252C%2520we%2520outline%250Aa%2520novel%2520single-call%2520architecture%252C%2520Crazy%2520Slots%252C%2520which%2520uses%2520a%2520probabilistic%250Aengine%2520and%2520a%2520Retrieval-Augmented%2520Generation%2520database%2520to%2520generate%2520authentic%252C%250Areal-time%2520responses%2520without%2520the%2520latency%2520and%2520cost%2520of%2520multi-step%2520reasoning%250Amodels.%2520This%2520review%2520provides%2520evidence-based%2520principles%2520for%2520designing%2520the%2520next%250Ageneration%2520of%2520AI%2520simulators%252C%2520arguing%2520that%2520a%2520pedagogically%2520grounded%2520approach%2520to%250Arealism%2520is%2520essential%2520for%2520creating%2520scalable%2520and%2520effective%2520teacher%2520education%250Atools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enter%3A%20Graduated%20Realism%3A%20A%20Pedagogical%20Framework%20for%20AI-Powered%20Avatars%0A%20%20in%20Virtual%20Reality%20Teacher%20Training&entry.906535625=Judson%20Leroy%20Dean%20Haynes%20IV&entry.1292438233=%20%20Virtual%20Reality%20simulators%20offer%20a%20powerful%20tool%20for%20teacher%20training%2C%20yet%0Athe%20integration%20of%20AI-powered%20student%20avatars%20presents%20a%20critical%20challenge%3A%0Adetermining%20the%20optimal%20level%20of%20avatar%20realism%20for%20effective%20pedagogy.%20This%0Aliterature%20review%20examines%20the%20evolution%20of%20avatar%20realism%20in%20VR%20teacher%0Atraining%2C%20synthesizes%20its%20theoretical%20implications%2C%20and%20proposes%20a%20new%0Apedagogical%20framework%20to%20guide%20future%20design.%20Through%20a%20systematic%20review%2C%20this%0Apaper%20traces%20the%20progression%20from%20human-controlled%20avatars%20to%20generative%20AI%0Aprototypes.%20Applying%20learning%20theories%20like%20Cognitive%20Load%20Theory%2C%20we%20argue%0Athat%20hyper-realism%20is%20not%20always%20optimal%2C%20as%20high-fidelity%20avatars%20can%20impose%0Aexcessive%20extraneous%20cognitive%20load%20on%20novices%2C%20a%20stance%20supported%20by%20recent%0Aempirical%20findings.%20A%20significant%20gap%20exists%20between%20the%20technological%20drive%0Afor%20photorealism%20and%20the%20pedagogical%20need%20for%20scaffolded%20learning.%20To%20address%0Athis%20gap%2C%20we%20propose%20Graduated%20Realism%2C%20a%20framework%20advocating%20for%20starting%0Atrainees%20with%20lower-fidelity%20avatars%20and%20progressively%20increasing%20behavioral%0Acomplexity%20as%20skills%20develop.%20To%20make%20this%20computationally%20feasible%2C%20we%20outline%0Aa%20novel%20single-call%20architecture%2C%20Crazy%20Slots%2C%20which%20uses%20a%20probabilistic%0Aengine%20and%20a%20Retrieval-Augmented%20Generation%20database%20to%20generate%20authentic%2C%0Areal-time%20responses%20without%20the%20latency%20and%20cost%20of%20multi-step%20reasoning%0Amodels.%20This%20review%20provides%20evidence-based%20principles%20for%20designing%20the%20next%0Ageneration%20of%20AI%20simulators%2C%20arguing%20that%20a%20pedagogically%20grounded%20approach%20to%0Arealism%20is%20essential%20for%20creating%20scalable%20and%20effective%20teacher%20education%0Atools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11890v1&entry.124074799=Read"},
{"title": "Persistent Topological Features in Large Language Models", "author": "Yuri Gardinazzi and Karthik Viswanathan and Giada Panerai and Alessio Ansuini and Alberto Cazzaniga and Matteo Biagetti", "abstract": "  Understanding the decision-making processes of large language models is\ncritical given their widespread applications. To achieve this, we aim to\nconnect a formal mathematical framework - zigzag persistence from topological\ndata analysis - with practical and easily applicable algorithms. Zigzag\npersistence is particularly effective for characterizing data as it dynamically\ntransforms across model layers. Within this framework, we introduce topological\ndescriptors that measure how topological features, $p$-dimensional holes,\npersist and evolve throughout the layers. Unlike methods that assess each layer\nindividually and then aggregate the results, our approach directly tracks the\nfull evolutionary path of these features. This offers a statistical perspective\non how prompts are rearranged and their relative positions changed in the\nrepresentation space, providing insights into the system's operation as an\nintegrated whole. To demonstrate the expressivity and applicability of our\nframework, we highlight how sensitive these descriptors are to different models\nand a variety of datasets. As a showcase application to a downstream task, we\nuse zigzag persistence to establish a criterion for layer pruning, achieving\nresults comparable to state-of-the-art methods while preserving the\nsystem-level perspective.\n", "link": "http://arxiv.org/abs/2410.11042v3", "date": "2025-06-13", "relevancy": 2.5181, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persistent%20Topological%20Features%20in%20Large%20Language%20Models&body=Title%3A%20Persistent%20Topological%20Features%20in%20Large%20Language%20Models%0AAuthor%3A%20Yuri%20Gardinazzi%20and%20Karthik%20Viswanathan%20and%20Giada%20Panerai%20and%20Alessio%20Ansuini%20and%20Alberto%20Cazzaniga%20and%20Matteo%20Biagetti%0AAbstract%3A%20%20%20Understanding%20the%20decision-making%20processes%20of%20large%20language%20models%20is%0Acritical%20given%20their%20widespread%20applications.%20To%20achieve%20this%2C%20we%20aim%20to%0Aconnect%20a%20formal%20mathematical%20framework%20-%20zigzag%20persistence%20from%20topological%0Adata%20analysis%20-%20with%20practical%20and%20easily%20applicable%20algorithms.%20Zigzag%0Apersistence%20is%20particularly%20effective%20for%20characterizing%20data%20as%20it%20dynamically%0Atransforms%20across%20model%20layers.%20Within%20this%20framework%2C%20we%20introduce%20topological%0Adescriptors%20that%20measure%20how%20topological%20features%2C%20%24p%24-dimensional%20holes%2C%0Apersist%20and%20evolve%20throughout%20the%20layers.%20Unlike%20methods%20that%20assess%20each%20layer%0Aindividually%20and%20then%20aggregate%20the%20results%2C%20our%20approach%20directly%20tracks%20the%0Afull%20evolutionary%20path%20of%20these%20features.%20This%20offers%20a%20statistical%20perspective%0Aon%20how%20prompts%20are%20rearranged%20and%20their%20relative%20positions%20changed%20in%20the%0Arepresentation%20space%2C%20providing%20insights%20into%20the%20system%27s%20operation%20as%20an%0Aintegrated%20whole.%20To%20demonstrate%20the%20expressivity%20and%20applicability%20of%20our%0Aframework%2C%20we%20highlight%20how%20sensitive%20these%20descriptors%20are%20to%20different%20models%0Aand%20a%20variety%20of%20datasets.%20As%20a%20showcase%20application%20to%20a%20downstream%20task%2C%20we%0Ause%20zigzag%20persistence%20to%20establish%20a%20criterion%20for%20layer%20pruning%2C%20achieving%0Aresults%20comparable%20to%20state-of-the-art%20methods%20while%20preserving%20the%0Asystem-level%20perspective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11042v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersistent%2520Topological%2520Features%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DYuri%2520Gardinazzi%2520and%2520Karthik%2520Viswanathan%2520and%2520Giada%2520Panerai%2520and%2520Alessio%2520Ansuini%2520and%2520Alberto%2520Cazzaniga%2520and%2520Matteo%2520Biagetti%26entry.1292438233%3D%2520%2520Understanding%2520the%2520decision-making%2520processes%2520of%2520large%2520language%2520models%2520is%250Acritical%2520given%2520their%2520widespread%2520applications.%2520To%2520achieve%2520this%252C%2520we%2520aim%2520to%250Aconnect%2520a%2520formal%2520mathematical%2520framework%2520-%2520zigzag%2520persistence%2520from%2520topological%250Adata%2520analysis%2520-%2520with%2520practical%2520and%2520easily%2520applicable%2520algorithms.%2520Zigzag%250Apersistence%2520is%2520particularly%2520effective%2520for%2520characterizing%2520data%2520as%2520it%2520dynamically%250Atransforms%2520across%2520model%2520layers.%2520Within%2520this%2520framework%252C%2520we%2520introduce%2520topological%250Adescriptors%2520that%2520measure%2520how%2520topological%2520features%252C%2520%2524p%2524-dimensional%2520holes%252C%250Apersist%2520and%2520evolve%2520throughout%2520the%2520layers.%2520Unlike%2520methods%2520that%2520assess%2520each%2520layer%250Aindividually%2520and%2520then%2520aggregate%2520the%2520results%252C%2520our%2520approach%2520directly%2520tracks%2520the%250Afull%2520evolutionary%2520path%2520of%2520these%2520features.%2520This%2520offers%2520a%2520statistical%2520perspective%250Aon%2520how%2520prompts%2520are%2520rearranged%2520and%2520their%2520relative%2520positions%2520changed%2520in%2520the%250Arepresentation%2520space%252C%2520providing%2520insights%2520into%2520the%2520system%2527s%2520operation%2520as%2520an%250Aintegrated%2520whole.%2520To%2520demonstrate%2520the%2520expressivity%2520and%2520applicability%2520of%2520our%250Aframework%252C%2520we%2520highlight%2520how%2520sensitive%2520these%2520descriptors%2520are%2520to%2520different%2520models%250Aand%2520a%2520variety%2520of%2520datasets.%2520As%2520a%2520showcase%2520application%2520to%2520a%2520downstream%2520task%252C%2520we%250Ause%2520zigzag%2520persistence%2520to%2520establish%2520a%2520criterion%2520for%2520layer%2520pruning%252C%2520achieving%250Aresults%2520comparable%2520to%2520state-of-the-art%2520methods%2520while%2520preserving%2520the%250Asystem-level%2520perspective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11042v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persistent%20Topological%20Features%20in%20Large%20Language%20Models&entry.906535625=Yuri%20Gardinazzi%20and%20Karthik%20Viswanathan%20and%20Giada%20Panerai%20and%20Alessio%20Ansuini%20and%20Alberto%20Cazzaniga%20and%20Matteo%20Biagetti&entry.1292438233=%20%20Understanding%20the%20decision-making%20processes%20of%20large%20language%20models%20is%0Acritical%20given%20their%20widespread%20applications.%20To%20achieve%20this%2C%20we%20aim%20to%0Aconnect%20a%20formal%20mathematical%20framework%20-%20zigzag%20persistence%20from%20topological%0Adata%20analysis%20-%20with%20practical%20and%20easily%20applicable%20algorithms.%20Zigzag%0Apersistence%20is%20particularly%20effective%20for%20characterizing%20data%20as%20it%20dynamically%0Atransforms%20across%20model%20layers.%20Within%20this%20framework%2C%20we%20introduce%20topological%0Adescriptors%20that%20measure%20how%20topological%20features%2C%20%24p%24-dimensional%20holes%2C%0Apersist%20and%20evolve%20throughout%20the%20layers.%20Unlike%20methods%20that%20assess%20each%20layer%0Aindividually%20and%20then%20aggregate%20the%20results%2C%20our%20approach%20directly%20tracks%20the%0Afull%20evolutionary%20path%20of%20these%20features.%20This%20offers%20a%20statistical%20perspective%0Aon%20how%20prompts%20are%20rearranged%20and%20their%20relative%20positions%20changed%20in%20the%0Arepresentation%20space%2C%20providing%20insights%20into%20the%20system%27s%20operation%20as%20an%0Aintegrated%20whole.%20To%20demonstrate%20the%20expressivity%20and%20applicability%20of%20our%0Aframework%2C%20we%20highlight%20how%20sensitive%20these%20descriptors%20are%20to%20different%20models%0Aand%20a%20variety%20of%20datasets.%20As%20a%20showcase%20application%20to%20a%20downstream%20task%2C%20we%0Ause%20zigzag%20persistence%20to%20establish%20a%20criterion%20for%20layer%20pruning%2C%20achieving%0Aresults%20comparable%20to%20state-of-the-art%20methods%20while%20preserving%20the%0Asystem-level%20perspective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11042v3&entry.124074799=Read"},
{"title": "Geometry-Aware Edge Pooling for Graph Neural Networks", "author": "Katharina Limbeck and Lydia Mezrag and Guy Wolf and Bastian Rieck", "abstract": "  Graph Neural Networks (GNNs) have shown significant success for graph-based\ntasks. Motivated by the prevalence of large datasets in real-world\napplications, pooling layers are crucial components of GNNs. By reducing the\nsize of input graphs, pooling enables faster training and potentially better\ngeneralisation. However, existing pooling operations often optimise for the\nlearning task at the expense of fundamental graph structures and\ninterpretability. This leads to unreliable performance across varying dataset\ntypes, downstream tasks and pooling ratios. Addressing these concerns, we\npropose novel graph pooling layers for structure aware pooling via edge\ncollapses. Our methods leverage diffusion geometry and iteratively reduce a\ngraph's size while preserving both its metric structure and structural\ndiversity. We guide pooling using magnitude, an isometry-invariant diversity\nmeasure, which permits us to control the fidelity of the pooling process.\nFurther, we use the spread of a metric space as a faster and more stable\nalternative ensuring computational efficiency. Empirical results demonstrate\nthat our methods (i) achieve superior performance compared to alternative\npooling layers across a range of diverse graph classification tasks, (ii)\npreserve key spectral properties of the input graphs, and (iii) retain high\naccuracy across varying pooling ratios.\n", "link": "http://arxiv.org/abs/2506.11700v1", "date": "2025-06-13", "relevancy": 2.5058, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.518}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.508}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Aware%20Edge%20Pooling%20for%20Graph%20Neural%20Networks&body=Title%3A%20Geometry-Aware%20Edge%20Pooling%20for%20Graph%20Neural%20Networks%0AAuthor%3A%20Katharina%20Limbeck%20and%20Lydia%20Mezrag%20and%20Guy%20Wolf%20and%20Bastian%20Rieck%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20significant%20success%20for%20graph-based%0Atasks.%20Motivated%20by%20the%20prevalence%20of%20large%20datasets%20in%20real-world%0Aapplications%2C%20pooling%20layers%20are%20crucial%20components%20of%20GNNs.%20By%20reducing%20the%0Asize%20of%20input%20graphs%2C%20pooling%20enables%20faster%20training%20and%20potentially%20better%0Ageneralisation.%20However%2C%20existing%20pooling%20operations%20often%20optimise%20for%20the%0Alearning%20task%20at%20the%20expense%20of%20fundamental%20graph%20structures%20and%0Ainterpretability.%20This%20leads%20to%20unreliable%20performance%20across%20varying%20dataset%0Atypes%2C%20downstream%20tasks%20and%20pooling%20ratios.%20Addressing%20these%20concerns%2C%20we%0Apropose%20novel%20graph%20pooling%20layers%20for%20structure%20aware%20pooling%20via%20edge%0Acollapses.%20Our%20methods%20leverage%20diffusion%20geometry%20and%20iteratively%20reduce%20a%0Agraph%27s%20size%20while%20preserving%20both%20its%20metric%20structure%20and%20structural%0Adiversity.%20We%20guide%20pooling%20using%20magnitude%2C%20an%20isometry-invariant%20diversity%0Ameasure%2C%20which%20permits%20us%20to%20control%20the%20fidelity%20of%20the%20pooling%20process.%0AFurther%2C%20we%20use%20the%20spread%20of%20a%20metric%20space%20as%20a%20faster%20and%20more%20stable%0Aalternative%20ensuring%20computational%20efficiency.%20Empirical%20results%20demonstrate%0Athat%20our%20methods%20%28i%29%20achieve%20superior%20performance%20compared%20to%20alternative%0Apooling%20layers%20across%20a%20range%20of%20diverse%20graph%20classification%20tasks%2C%20%28ii%29%0Apreserve%20key%20spectral%20properties%20of%20the%20input%20graphs%2C%20and%20%28iii%29%20retain%20high%0Aaccuracy%20across%20varying%20pooling%20ratios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Aware%2520Edge%2520Pooling%2520for%2520Graph%2520Neural%2520Networks%26entry.906535625%3DKatharina%2520Limbeck%2520and%2520Lydia%2520Mezrag%2520and%2520Guy%2520Wolf%2520and%2520Bastian%2520Rieck%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520shown%2520significant%2520success%2520for%2520graph-based%250Atasks.%2520Motivated%2520by%2520the%2520prevalence%2520of%2520large%2520datasets%2520in%2520real-world%250Aapplications%252C%2520pooling%2520layers%2520are%2520crucial%2520components%2520of%2520GNNs.%2520By%2520reducing%2520the%250Asize%2520of%2520input%2520graphs%252C%2520pooling%2520enables%2520faster%2520training%2520and%2520potentially%2520better%250Ageneralisation.%2520However%252C%2520existing%2520pooling%2520operations%2520often%2520optimise%2520for%2520the%250Alearning%2520task%2520at%2520the%2520expense%2520of%2520fundamental%2520graph%2520structures%2520and%250Ainterpretability.%2520This%2520leads%2520to%2520unreliable%2520performance%2520across%2520varying%2520dataset%250Atypes%252C%2520downstream%2520tasks%2520and%2520pooling%2520ratios.%2520Addressing%2520these%2520concerns%252C%2520we%250Apropose%2520novel%2520graph%2520pooling%2520layers%2520for%2520structure%2520aware%2520pooling%2520via%2520edge%250Acollapses.%2520Our%2520methods%2520leverage%2520diffusion%2520geometry%2520and%2520iteratively%2520reduce%2520a%250Agraph%2527s%2520size%2520while%2520preserving%2520both%2520its%2520metric%2520structure%2520and%2520structural%250Adiversity.%2520We%2520guide%2520pooling%2520using%2520magnitude%252C%2520an%2520isometry-invariant%2520diversity%250Ameasure%252C%2520which%2520permits%2520us%2520to%2520control%2520the%2520fidelity%2520of%2520the%2520pooling%2520process.%250AFurther%252C%2520we%2520use%2520the%2520spread%2520of%2520a%2520metric%2520space%2520as%2520a%2520faster%2520and%2520more%2520stable%250Aalternative%2520ensuring%2520computational%2520efficiency.%2520Empirical%2520results%2520demonstrate%250Athat%2520our%2520methods%2520%2528i%2529%2520achieve%2520superior%2520performance%2520compared%2520to%2520alternative%250Apooling%2520layers%2520across%2520a%2520range%2520of%2520diverse%2520graph%2520classification%2520tasks%252C%2520%2528ii%2529%250Apreserve%2520key%2520spectral%2520properties%2520of%2520the%2520input%2520graphs%252C%2520and%2520%2528iii%2529%2520retain%2520high%250Aaccuracy%2520across%2520varying%2520pooling%2520ratios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Aware%20Edge%20Pooling%20for%20Graph%20Neural%20Networks&entry.906535625=Katharina%20Limbeck%20and%20Lydia%20Mezrag%20and%20Guy%20Wolf%20and%20Bastian%20Rieck&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20significant%20success%20for%20graph-based%0Atasks.%20Motivated%20by%20the%20prevalence%20of%20large%20datasets%20in%20real-world%0Aapplications%2C%20pooling%20layers%20are%20crucial%20components%20of%20GNNs.%20By%20reducing%20the%0Asize%20of%20input%20graphs%2C%20pooling%20enables%20faster%20training%20and%20potentially%20better%0Ageneralisation.%20However%2C%20existing%20pooling%20operations%20often%20optimise%20for%20the%0Alearning%20task%20at%20the%20expense%20of%20fundamental%20graph%20structures%20and%0Ainterpretability.%20This%20leads%20to%20unreliable%20performance%20across%20varying%20dataset%0Atypes%2C%20downstream%20tasks%20and%20pooling%20ratios.%20Addressing%20these%20concerns%2C%20we%0Apropose%20novel%20graph%20pooling%20layers%20for%20structure%20aware%20pooling%20via%20edge%0Acollapses.%20Our%20methods%20leverage%20diffusion%20geometry%20and%20iteratively%20reduce%20a%0Agraph%27s%20size%20while%20preserving%20both%20its%20metric%20structure%20and%20structural%0Adiversity.%20We%20guide%20pooling%20using%20magnitude%2C%20an%20isometry-invariant%20diversity%0Ameasure%2C%20which%20permits%20us%20to%20control%20the%20fidelity%20of%20the%20pooling%20process.%0AFurther%2C%20we%20use%20the%20spread%20of%20a%20metric%20space%20as%20a%20faster%20and%20more%20stable%0Aalternative%20ensuring%20computational%20efficiency.%20Empirical%20results%20demonstrate%0Athat%20our%20methods%20%28i%29%20achieve%20superior%20performance%20compared%20to%20alternative%0Apooling%20layers%20across%20a%20range%20of%20diverse%20graph%20classification%20tasks%2C%20%28ii%29%0Apreserve%20key%20spectral%20properties%20of%20the%20input%20graphs%2C%20and%20%28iii%29%20retain%20high%0Aaccuracy%20across%20varying%20pooling%20ratios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11700v1&entry.124074799=Read"},
{"title": "Brain Network Analysis Based on Fine-tuned Self-supervised Model for\n  Brain Disease Diagnosis", "author": "Yifei Tang and Hongjie Jiang and Changhong Jing and Hieu Pham and Shuqiang Wang", "abstract": "  Functional brain network analysis has become an indispensable tool for brain\ndisease analysis. It is profoundly impacted by deep learning methods, which can\ncharacterize complex connections between ROIs. However, the research on\nfoundation models of brain network is limited and constrained to a single\ndimension, which restricts their extensive application in neuroscience. In this\nstudy, we propose a fine-tuned brain network model for brain disease diagnosis.\nIt expands brain region representations across multiple dimensions based on the\noriginal brain network model, thereby enhancing its generalizability. Our model\nconsists of two key modules: (1)an adapter module that expands brain region\nfeatures across different dimensions. (2)a fine-tuned foundation brain network\nmodel, based on self-supervised learning and pre-trained on fMRI data from\nthousands of participants. Specifically, its transformer block is able to\neffectively extract brain region features and compute the inter-region\nassociations. Moreover, we derive a compact latent representation of the brain\nnetwork for brain disease diagnosis. Our downstream experiments in this study\ndemonstrate that the proposed model achieves superior performance in brain\ndisease diagnosis, which potentially offers a promising approach in brain\nnetwork analysis research.\n", "link": "http://arxiv.org/abs/2506.11671v1", "date": "2025-06-13", "relevancy": 2.5014, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20Network%20Analysis%20Based%20on%20Fine-tuned%20Self-supervised%20Model%20for%0A%20%20Brain%20Disease%20Diagnosis&body=Title%3A%20Brain%20Network%20Analysis%20Based%20on%20Fine-tuned%20Self-supervised%20Model%20for%0A%20%20Brain%20Disease%20Diagnosis%0AAuthor%3A%20Yifei%20Tang%20and%20Hongjie%20Jiang%20and%20Changhong%20Jing%20and%20Hieu%20Pham%20and%20Shuqiang%20Wang%0AAbstract%3A%20%20%20Functional%20brain%20network%20analysis%20has%20become%20an%20indispensable%20tool%20for%20brain%0Adisease%20analysis.%20It%20is%20profoundly%20impacted%20by%20deep%20learning%20methods%2C%20which%20can%0Acharacterize%20complex%20connections%20between%20ROIs.%20However%2C%20the%20research%20on%0Afoundation%20models%20of%20brain%20network%20is%20limited%20and%20constrained%20to%20a%20single%0Adimension%2C%20which%20restricts%20their%20extensive%20application%20in%20neuroscience.%20In%20this%0Astudy%2C%20we%20propose%20a%20fine-tuned%20brain%20network%20model%20for%20brain%20disease%20diagnosis.%0AIt%20expands%20brain%20region%20representations%20across%20multiple%20dimensions%20based%20on%20the%0Aoriginal%20brain%20network%20model%2C%20thereby%20enhancing%20its%20generalizability.%20Our%20model%0Aconsists%20of%20two%20key%20modules%3A%20%281%29an%20adapter%20module%20that%20expands%20brain%20region%0Afeatures%20across%20different%20dimensions.%20%282%29a%20fine-tuned%20foundation%20brain%20network%0Amodel%2C%20based%20on%20self-supervised%20learning%20and%20pre-trained%20on%20fMRI%20data%20from%0Athousands%20of%20participants.%20Specifically%2C%20its%20transformer%20block%20is%20able%20to%0Aeffectively%20extract%20brain%20region%20features%20and%20compute%20the%20inter-region%0Aassociations.%20Moreover%2C%20we%20derive%20a%20compact%20latent%20representation%20of%20the%20brain%0Anetwork%20for%20brain%20disease%20diagnosis.%20Our%20downstream%20experiments%20in%20this%20study%0Ademonstrate%20that%20the%20proposed%20model%20achieves%20superior%20performance%20in%20brain%0Adisease%20diagnosis%2C%20which%20potentially%20offers%20a%20promising%20approach%20in%20brain%0Anetwork%20analysis%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520Network%2520Analysis%2520Based%2520on%2520Fine-tuned%2520Self-supervised%2520Model%2520for%250A%2520%2520Brain%2520Disease%2520Diagnosis%26entry.906535625%3DYifei%2520Tang%2520and%2520Hongjie%2520Jiang%2520and%2520Changhong%2520Jing%2520and%2520Hieu%2520Pham%2520and%2520Shuqiang%2520Wang%26entry.1292438233%3D%2520%2520Functional%2520brain%2520network%2520analysis%2520has%2520become%2520an%2520indispensable%2520tool%2520for%2520brain%250Adisease%2520analysis.%2520It%2520is%2520profoundly%2520impacted%2520by%2520deep%2520learning%2520methods%252C%2520which%2520can%250Acharacterize%2520complex%2520connections%2520between%2520ROIs.%2520However%252C%2520the%2520research%2520on%250Afoundation%2520models%2520of%2520brain%2520network%2520is%2520limited%2520and%2520constrained%2520to%2520a%2520single%250Adimension%252C%2520which%2520restricts%2520their%2520extensive%2520application%2520in%2520neuroscience.%2520In%2520this%250Astudy%252C%2520we%2520propose%2520a%2520fine-tuned%2520brain%2520network%2520model%2520for%2520brain%2520disease%2520diagnosis.%250AIt%2520expands%2520brain%2520region%2520representations%2520across%2520multiple%2520dimensions%2520based%2520on%2520the%250Aoriginal%2520brain%2520network%2520model%252C%2520thereby%2520enhancing%2520its%2520generalizability.%2520Our%2520model%250Aconsists%2520of%2520two%2520key%2520modules%253A%2520%25281%2529an%2520adapter%2520module%2520that%2520expands%2520brain%2520region%250Afeatures%2520across%2520different%2520dimensions.%2520%25282%2529a%2520fine-tuned%2520foundation%2520brain%2520network%250Amodel%252C%2520based%2520on%2520self-supervised%2520learning%2520and%2520pre-trained%2520on%2520fMRI%2520data%2520from%250Athousands%2520of%2520participants.%2520Specifically%252C%2520its%2520transformer%2520block%2520is%2520able%2520to%250Aeffectively%2520extract%2520brain%2520region%2520features%2520and%2520compute%2520the%2520inter-region%250Aassociations.%2520Moreover%252C%2520we%2520derive%2520a%2520compact%2520latent%2520representation%2520of%2520the%2520brain%250Anetwork%2520for%2520brain%2520disease%2520diagnosis.%2520Our%2520downstream%2520experiments%2520in%2520this%2520study%250Ademonstrate%2520that%2520the%2520proposed%2520model%2520achieves%2520superior%2520performance%2520in%2520brain%250Adisease%2520diagnosis%252C%2520which%2520potentially%2520offers%2520a%2520promising%2520approach%2520in%2520brain%250Anetwork%2520analysis%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20Network%20Analysis%20Based%20on%20Fine-tuned%20Self-supervised%20Model%20for%0A%20%20Brain%20Disease%20Diagnosis&entry.906535625=Yifei%20Tang%20and%20Hongjie%20Jiang%20and%20Changhong%20Jing%20and%20Hieu%20Pham%20and%20Shuqiang%20Wang&entry.1292438233=%20%20Functional%20brain%20network%20analysis%20has%20become%20an%20indispensable%20tool%20for%20brain%0Adisease%20analysis.%20It%20is%20profoundly%20impacted%20by%20deep%20learning%20methods%2C%20which%20can%0Acharacterize%20complex%20connections%20between%20ROIs.%20However%2C%20the%20research%20on%0Afoundation%20models%20of%20brain%20network%20is%20limited%20and%20constrained%20to%20a%20single%0Adimension%2C%20which%20restricts%20their%20extensive%20application%20in%20neuroscience.%20In%20this%0Astudy%2C%20we%20propose%20a%20fine-tuned%20brain%20network%20model%20for%20brain%20disease%20diagnosis.%0AIt%20expands%20brain%20region%20representations%20across%20multiple%20dimensions%20based%20on%20the%0Aoriginal%20brain%20network%20model%2C%20thereby%20enhancing%20its%20generalizability.%20Our%20model%0Aconsists%20of%20two%20key%20modules%3A%20%281%29an%20adapter%20module%20that%20expands%20brain%20region%0Afeatures%20across%20different%20dimensions.%20%282%29a%20fine-tuned%20foundation%20brain%20network%0Amodel%2C%20based%20on%20self-supervised%20learning%20and%20pre-trained%20on%20fMRI%20data%20from%0Athousands%20of%20participants.%20Specifically%2C%20its%20transformer%20block%20is%20able%20to%0Aeffectively%20extract%20brain%20region%20features%20and%20compute%20the%20inter-region%0Aassociations.%20Moreover%2C%20we%20derive%20a%20compact%20latent%20representation%20of%20the%20brain%0Anetwork%20for%20brain%20disease%20diagnosis.%20Our%20downstream%20experiments%20in%20this%20study%0Ademonstrate%20that%20the%20proposed%20model%20achieves%20superior%20performance%20in%20brain%0Adisease%20diagnosis%2C%20which%20potentially%20offers%20a%20promising%20approach%20in%20brain%0Anetwork%20analysis%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11671v1&entry.124074799=Read"},
{"title": "Improving Large Language Model Safety with Contrastive Representation\n  Learning", "author": "Samuel Simko and Mrinmaya Sachan and Bernhard Sch\u00f6lkopf and Zhijing Jin", "abstract": "  Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense\n", "link": "http://arxiv.org/abs/2506.11938v1", "date": "2025-06-13", "relevancy": 2.5, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4927}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Large%20Language%20Model%20Safety%20with%20Contrastive%20Representation%0A%20%20Learning&body=Title%3A%20Improving%20Large%20Language%20Model%20Safety%20with%20Contrastive%20Representation%0A%20%20Learning%0AAuthor%3A%20Samuel%20Simko%20and%20Mrinmaya%20Sachan%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Zhijing%20Jin%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20powerful%20tools%20with%20profound%20societal%0Aimpacts%2C%20yet%20their%20ability%20to%20generate%20responses%20to%20diverse%20and%20uncontrolled%0Ainputs%20leaves%20them%20vulnerable%20to%20adversarial%20attacks.%20While%20existing%20defenses%0Aoften%20struggle%20to%20generalize%20across%20varying%20attack%20types%2C%20recent%20advancements%0Ain%20representation%20engineering%20offer%20promising%20alternatives.%20In%20this%20work%2C%20we%0Apropose%20a%20defense%20framework%20that%20formulates%20model%20defense%20as%20a%20contrastive%0Arepresentation%20learning%20%28CRL%29%20problem.%20Our%20method%20finetunes%20a%20model%20using%20a%0Atriplet-based%20loss%20combined%20with%20adversarial%20hard%20negative%20mining%20to%20encourage%0Aseparation%20between%20benign%20and%20harmful%20representations.%20Our%20experimental%20results%0Aacross%20multiple%20models%20demonstrate%20that%20our%20approach%20outperforms%20prior%0Arepresentation%20engineering-based%20defenses%2C%20improving%20robustness%20against%20both%0Ainput-level%20and%20embedding-space%20attacks%20without%20compromising%20standard%0Aperformance.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/samuelsimko/crl-llm-defense%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Large%2520Language%2520Model%2520Safety%2520with%2520Contrastive%2520Representation%250A%2520%2520Learning%26entry.906535625%3DSamuel%2520Simko%2520and%2520Mrinmaya%2520Sachan%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Zhijing%2520Jin%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520powerful%2520tools%2520with%2520profound%2520societal%250Aimpacts%252C%2520yet%2520their%2520ability%2520to%2520generate%2520responses%2520to%2520diverse%2520and%2520uncontrolled%250Ainputs%2520leaves%2520them%2520vulnerable%2520to%2520adversarial%2520attacks.%2520While%2520existing%2520defenses%250Aoften%2520struggle%2520to%2520generalize%2520across%2520varying%2520attack%2520types%252C%2520recent%2520advancements%250Ain%2520representation%2520engineering%2520offer%2520promising%2520alternatives.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520defense%2520framework%2520that%2520formulates%2520model%2520defense%2520as%2520a%2520contrastive%250Arepresentation%2520learning%2520%2528CRL%2529%2520problem.%2520Our%2520method%2520finetunes%2520a%2520model%2520using%2520a%250Atriplet-based%2520loss%2520combined%2520with%2520adversarial%2520hard%2520negative%2520mining%2520to%2520encourage%250Aseparation%2520between%2520benign%2520and%2520harmful%2520representations.%2520Our%2520experimental%2520results%250Aacross%2520multiple%2520models%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520prior%250Arepresentation%2520engineering-based%2520defenses%252C%2520improving%2520robustness%2520against%2520both%250Ainput-level%2520and%2520embedding-space%2520attacks%2520without%2520compromising%2520standard%250Aperformance.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/samuelsimko/crl-llm-defense%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Large%20Language%20Model%20Safety%20with%20Contrastive%20Representation%0A%20%20Learning&entry.906535625=Samuel%20Simko%20and%20Mrinmaya%20Sachan%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Zhijing%20Jin&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20powerful%20tools%20with%20profound%20societal%0Aimpacts%2C%20yet%20their%20ability%20to%20generate%20responses%20to%20diverse%20and%20uncontrolled%0Ainputs%20leaves%20them%20vulnerable%20to%20adversarial%20attacks.%20While%20existing%20defenses%0Aoften%20struggle%20to%20generalize%20across%20varying%20attack%20types%2C%20recent%20advancements%0Ain%20representation%20engineering%20offer%20promising%20alternatives.%20In%20this%20work%2C%20we%0Apropose%20a%20defense%20framework%20that%20formulates%20model%20defense%20as%20a%20contrastive%0Arepresentation%20learning%20%28CRL%29%20problem.%20Our%20method%20finetunes%20a%20model%20using%20a%0Atriplet-based%20loss%20combined%20with%20adversarial%20hard%20negative%20mining%20to%20encourage%0Aseparation%20between%20benign%20and%20harmful%20representations.%20Our%20experimental%20results%0Aacross%20multiple%20models%20demonstrate%20that%20our%20approach%20outperforms%20prior%0Arepresentation%20engineering-based%20defenses%2C%20improving%20robustness%20against%20both%0Ainput-level%20and%20embedding-space%20attacks%20without%20compromising%20standard%0Aperformance.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/samuelsimko/crl-llm-defense%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11938v1&entry.124074799=Read"},
{"title": "LoRA-Gen: Specializing Large Language Model via Online LoRA Generation", "author": "Yicheng Xiao and Lin Song and Rui Yang and Cheng Cheng and Yixiao Ge and Xiu Li and Ying Shan", "abstract": "  Recent advances have highlighted the benefits of scaling language models to\nenhance performance across a wide range of NLP tasks. However, these approaches\nstill face limitations in effectiveness and efficiency when applied to\ndomain-specific tasks, particularly for small edge-side models. We propose the\nLoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA\nparameters for edge-side models based on task descriptions. By employing the\nreparameterization technique, we merge the LoRA parameters into the edge-side\nmodel to achieve flexible specialization. Our method facilitates knowledge\ntransfer between models while significantly improving the inference efficiency\nof the specialized model by reducing the input context length. Without\nspecialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which\nachieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in\nreasoning tasks. Besides, our method delivers a compression ratio of 10.1x with\nGemma-2B on intelligent agent tasks.\n", "link": "http://arxiv.org/abs/2506.11638v1", "date": "2025-06-13", "relevancy": 2.4956, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA-Gen%3A%20Specializing%20Large%20Language%20Model%20via%20Online%20LoRA%20Generation&body=Title%3A%20LoRA-Gen%3A%20Specializing%20Large%20Language%20Model%20via%20Online%20LoRA%20Generation%0AAuthor%3A%20Yicheng%20Xiao%20and%20Lin%20Song%20and%20Rui%20Yang%20and%20Cheng%20Cheng%20and%20Yixiao%20Ge%20and%20Xiu%20Li%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Recent%20advances%20have%20highlighted%20the%20benefits%20of%20scaling%20language%20models%20to%0Aenhance%20performance%20across%20a%20wide%20range%20of%20NLP%20tasks.%20However%2C%20these%20approaches%0Astill%20face%20limitations%20in%20effectiveness%20and%20efficiency%20when%20applied%20to%0Adomain-specific%20tasks%2C%20particularly%20for%20small%20edge-side%20models.%20We%20propose%20the%0ALoRA-Gen%20framework%2C%20which%20utilizes%20a%20large%20cloud-side%20model%20to%20generate%20LoRA%0Aparameters%20for%20edge-side%20models%20based%20on%20task%20descriptions.%20By%20employing%20the%0Areparameterization%20technique%2C%20we%20merge%20the%20LoRA%20parameters%20into%20the%20edge-side%0Amodel%20to%20achieve%20flexible%20specialization.%20Our%20method%20facilitates%20knowledge%0Atransfer%20between%20models%20while%20significantly%20improving%20the%20inference%20efficiency%0Aof%20the%20specialized%20model%20by%20reducing%20the%20input%20context%20length.%20Without%0Aspecialized%20training%2C%20LoRA-Gen%20outperforms%20conventional%20LoRA%20fine-tuning%2C%20which%0Aachieves%20competitive%20accuracy%20and%20a%202.1x%20speedup%20with%20TinyLLaMA-1.1B%20in%0Areasoning%20tasks.%20Besides%2C%20our%20method%20delivers%20a%20compression%20ratio%20of%2010.1x%20with%0AGemma-2B%20on%20intelligent%20agent%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA-Gen%253A%2520Specializing%2520Large%2520Language%2520Model%2520via%2520Online%2520LoRA%2520Generation%26entry.906535625%3DYicheng%2520Xiao%2520and%2520Lin%2520Song%2520and%2520Rui%2520Yang%2520and%2520Cheng%2520Cheng%2520and%2520Yixiao%2520Ge%2520and%2520Xiu%2520Li%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520have%2520highlighted%2520the%2520benefits%2520of%2520scaling%2520language%2520models%2520to%250Aenhance%2520performance%2520across%2520a%2520wide%2520range%2520of%2520NLP%2520tasks.%2520However%252C%2520these%2520approaches%250Astill%2520face%2520limitations%2520in%2520effectiveness%2520and%2520efficiency%2520when%2520applied%2520to%250Adomain-specific%2520tasks%252C%2520particularly%2520for%2520small%2520edge-side%2520models.%2520We%2520propose%2520the%250ALoRA-Gen%2520framework%252C%2520which%2520utilizes%2520a%2520large%2520cloud-side%2520model%2520to%2520generate%2520LoRA%250Aparameters%2520for%2520edge-side%2520models%2520based%2520on%2520task%2520descriptions.%2520By%2520employing%2520the%250Areparameterization%2520technique%252C%2520we%2520merge%2520the%2520LoRA%2520parameters%2520into%2520the%2520edge-side%250Amodel%2520to%2520achieve%2520flexible%2520specialization.%2520Our%2520method%2520facilitates%2520knowledge%250Atransfer%2520between%2520models%2520while%2520significantly%2520improving%2520the%2520inference%2520efficiency%250Aof%2520the%2520specialized%2520model%2520by%2520reducing%2520the%2520input%2520context%2520length.%2520Without%250Aspecialized%2520training%252C%2520LoRA-Gen%2520outperforms%2520conventional%2520LoRA%2520fine-tuning%252C%2520which%250Aachieves%2520competitive%2520accuracy%2520and%2520a%25202.1x%2520speedup%2520with%2520TinyLLaMA-1.1B%2520in%250Areasoning%2520tasks.%2520Besides%252C%2520our%2520method%2520delivers%2520a%2520compression%2520ratio%2520of%252010.1x%2520with%250AGemma-2B%2520on%2520intelligent%2520agent%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA-Gen%3A%20Specializing%20Large%20Language%20Model%20via%20Online%20LoRA%20Generation&entry.906535625=Yicheng%20Xiao%20and%20Lin%20Song%20and%20Rui%20Yang%20and%20Cheng%20Cheng%20and%20Yixiao%20Ge%20and%20Xiu%20Li%20and%20Ying%20Shan&entry.1292438233=%20%20Recent%20advances%20have%20highlighted%20the%20benefits%20of%20scaling%20language%20models%20to%0Aenhance%20performance%20across%20a%20wide%20range%20of%20NLP%20tasks.%20However%2C%20these%20approaches%0Astill%20face%20limitations%20in%20effectiveness%20and%20efficiency%20when%20applied%20to%0Adomain-specific%20tasks%2C%20particularly%20for%20small%20edge-side%20models.%20We%20propose%20the%0ALoRA-Gen%20framework%2C%20which%20utilizes%20a%20large%20cloud-side%20model%20to%20generate%20LoRA%0Aparameters%20for%20edge-side%20models%20based%20on%20task%20descriptions.%20By%20employing%20the%0Areparameterization%20technique%2C%20we%20merge%20the%20LoRA%20parameters%20into%20the%20edge-side%0Amodel%20to%20achieve%20flexible%20specialization.%20Our%20method%20facilitates%20knowledge%0Atransfer%20between%20models%20while%20significantly%20improving%20the%20inference%20efficiency%0Aof%20the%20specialized%20model%20by%20reducing%20the%20input%20context%20length.%20Without%0Aspecialized%20training%2C%20LoRA-Gen%20outperforms%20conventional%20LoRA%20fine-tuning%2C%20which%0Aachieves%20competitive%20accuracy%20and%20a%202.1x%20speedup%20with%20TinyLLaMA-1.1B%20in%0Areasoning%20tasks.%20Besides%2C%20our%20method%20delivers%20a%20compression%20ratio%20of%2010.1x%20with%0AGemma-2B%20on%20intelligent%20agent%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11638v1&entry.124074799=Read"},
{"title": "mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity", "author": "Elvis Nava and Victoriano Montesinos and Erik Bauer and Benedek Forrai and Jonas Pai and Stefan Weirich and Stephan-Daniel Gravert and Philipp Wand and Stephan Polinski and Benjamin F. Grewe and Robert K. Katzschmann", "abstract": "  We present a diffusion-based model recipe for real-world control of a highly\ndexterous humanoid robotic hand, designed for sample-efficient learning and\nsmooth fine-motor action inference. Our system features a newly designed 16-DoF\ntendon-driven hand, equipped with wide angle wrist cameras and mounted on a\nFranka Emika Panda arm. We develop a versatile teleoperation pipeline and data\ncollection protocol using both glove-based and VR interfaces, enabling\nhigh-quality data collection across diverse tasks such as pick and place, item\nsorting and assembly insertion. Leveraging high-frequency generative control,\nwe train end-to-end policies from raw sensory inputs, enabling smooth,\nself-correcting motions in complex manipulation scenarios. Real-world\nevaluations demonstrate up to 93.3% out of distribution success rates, with up\nto a +33.3% performance boost due to emergent self-correcting behaviors, while\nalso revealing scaling trends in policy performance. Our results advance the\nstate-of-the-art in dexterous robotic manipulation through a fully integrated,\npractical approach to hardware, learning, and real-world deployment.\n", "link": "http://arxiv.org/abs/2506.11916v1", "date": "2025-06-13", "relevancy": 2.4827, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6322}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6158}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mimic-one%3A%20a%20Scalable%20Model%20Recipe%20for%20General%20Purpose%20Robot%20Dexterity&body=Title%3A%20mimic-one%3A%20a%20Scalable%20Model%20Recipe%20for%20General%20Purpose%20Robot%20Dexterity%0AAuthor%3A%20Elvis%20Nava%20and%20Victoriano%20Montesinos%20and%20Erik%20Bauer%20and%20Benedek%20Forrai%20and%20Jonas%20Pai%20and%20Stefan%20Weirich%20and%20Stephan-Daniel%20Gravert%20and%20Philipp%20Wand%20and%20Stephan%20Polinski%20and%20Benjamin%20F.%20Grewe%20and%20Robert%20K.%20Katzschmann%0AAbstract%3A%20%20%20We%20present%20a%20diffusion-based%20model%20recipe%20for%20real-world%20control%20of%20a%20highly%0Adexterous%20humanoid%20robotic%20hand%2C%20designed%20for%20sample-efficient%20learning%20and%0Asmooth%20fine-motor%20action%20inference.%20Our%20system%20features%20a%20newly%20designed%2016-DoF%0Atendon-driven%20hand%2C%20equipped%20with%20wide%20angle%20wrist%20cameras%20and%20mounted%20on%20a%0AFranka%20Emika%20Panda%20arm.%20We%20develop%20a%20versatile%20teleoperation%20pipeline%20and%20data%0Acollection%20protocol%20using%20both%20glove-based%20and%20VR%20interfaces%2C%20enabling%0Ahigh-quality%20data%20collection%20across%20diverse%20tasks%20such%20as%20pick%20and%20place%2C%20item%0Asorting%20and%20assembly%20insertion.%20Leveraging%20high-frequency%20generative%20control%2C%0Awe%20train%20end-to-end%20policies%20from%20raw%20sensory%20inputs%2C%20enabling%20smooth%2C%0Aself-correcting%20motions%20in%20complex%20manipulation%20scenarios.%20Real-world%0Aevaluations%20demonstrate%20up%20to%2093.3%25%20out%20of%20distribution%20success%20rates%2C%20with%20up%0Ato%20a%20%2B33.3%25%20performance%20boost%20due%20to%20emergent%20self-correcting%20behaviors%2C%20while%0Aalso%20revealing%20scaling%20trends%20in%20policy%20performance.%20Our%20results%20advance%20the%0Astate-of-the-art%20in%20dexterous%20robotic%20manipulation%20through%20a%20fully%20integrated%2C%0Apractical%20approach%20to%20hardware%2C%20learning%2C%20and%20real-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dmimic-one%253A%2520a%2520Scalable%2520Model%2520Recipe%2520for%2520General%2520Purpose%2520Robot%2520Dexterity%26entry.906535625%3DElvis%2520Nava%2520and%2520Victoriano%2520Montesinos%2520and%2520Erik%2520Bauer%2520and%2520Benedek%2520Forrai%2520and%2520Jonas%2520Pai%2520and%2520Stefan%2520Weirich%2520and%2520Stephan-Daniel%2520Gravert%2520and%2520Philipp%2520Wand%2520and%2520Stephan%2520Polinski%2520and%2520Benjamin%2520F.%2520Grewe%2520and%2520Robert%2520K.%2520Katzschmann%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520diffusion-based%2520model%2520recipe%2520for%2520real-world%2520control%2520of%2520a%2520highly%250Adexterous%2520humanoid%2520robotic%2520hand%252C%2520designed%2520for%2520sample-efficient%2520learning%2520and%250Asmooth%2520fine-motor%2520action%2520inference.%2520Our%2520system%2520features%2520a%2520newly%2520designed%252016-DoF%250Atendon-driven%2520hand%252C%2520equipped%2520with%2520wide%2520angle%2520wrist%2520cameras%2520and%2520mounted%2520on%2520a%250AFranka%2520Emika%2520Panda%2520arm.%2520We%2520develop%2520a%2520versatile%2520teleoperation%2520pipeline%2520and%2520data%250Acollection%2520protocol%2520using%2520both%2520glove-based%2520and%2520VR%2520interfaces%252C%2520enabling%250Ahigh-quality%2520data%2520collection%2520across%2520diverse%2520tasks%2520such%2520as%2520pick%2520and%2520place%252C%2520item%250Asorting%2520and%2520assembly%2520insertion.%2520Leveraging%2520high-frequency%2520generative%2520control%252C%250Awe%2520train%2520end-to-end%2520policies%2520from%2520raw%2520sensory%2520inputs%252C%2520enabling%2520smooth%252C%250Aself-correcting%2520motions%2520in%2520complex%2520manipulation%2520scenarios.%2520Real-world%250Aevaluations%2520demonstrate%2520up%2520to%252093.3%2525%2520out%2520of%2520distribution%2520success%2520rates%252C%2520with%2520up%250Ato%2520a%2520%252B33.3%2525%2520performance%2520boost%2520due%2520to%2520emergent%2520self-correcting%2520behaviors%252C%2520while%250Aalso%2520revealing%2520scaling%2520trends%2520in%2520policy%2520performance.%2520Our%2520results%2520advance%2520the%250Astate-of-the-art%2520in%2520dexterous%2520robotic%2520manipulation%2520through%2520a%2520fully%2520integrated%252C%250Apractical%2520approach%2520to%2520hardware%252C%2520learning%252C%2520and%2520real-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mimic-one%3A%20a%20Scalable%20Model%20Recipe%20for%20General%20Purpose%20Robot%20Dexterity&entry.906535625=Elvis%20Nava%20and%20Victoriano%20Montesinos%20and%20Erik%20Bauer%20and%20Benedek%20Forrai%20and%20Jonas%20Pai%20and%20Stefan%20Weirich%20and%20Stephan-Daniel%20Gravert%20and%20Philipp%20Wand%20and%20Stephan%20Polinski%20and%20Benjamin%20F.%20Grewe%20and%20Robert%20K.%20Katzschmann&entry.1292438233=%20%20We%20present%20a%20diffusion-based%20model%20recipe%20for%20real-world%20control%20of%20a%20highly%0Adexterous%20humanoid%20robotic%20hand%2C%20designed%20for%20sample-efficient%20learning%20and%0Asmooth%20fine-motor%20action%20inference.%20Our%20system%20features%20a%20newly%20designed%2016-DoF%0Atendon-driven%20hand%2C%20equipped%20with%20wide%20angle%20wrist%20cameras%20and%20mounted%20on%20a%0AFranka%20Emika%20Panda%20arm.%20We%20develop%20a%20versatile%20teleoperation%20pipeline%20and%20data%0Acollection%20protocol%20using%20both%20glove-based%20and%20VR%20interfaces%2C%20enabling%0Ahigh-quality%20data%20collection%20across%20diverse%20tasks%20such%20as%20pick%20and%20place%2C%20item%0Asorting%20and%20assembly%20insertion.%20Leveraging%20high-frequency%20generative%20control%2C%0Awe%20train%20end-to-end%20policies%20from%20raw%20sensory%20inputs%2C%20enabling%20smooth%2C%0Aself-correcting%20motions%20in%20complex%20manipulation%20scenarios.%20Real-world%0Aevaluations%20demonstrate%20up%20to%2093.3%25%20out%20of%20distribution%20success%20rates%2C%20with%20up%0Ato%20a%20%2B33.3%25%20performance%20boost%20due%20to%20emergent%20self-correcting%20behaviors%2C%20while%0Aalso%20revealing%20scaling%20trends%20in%20policy%20performance.%20Our%20results%20advance%20the%0Astate-of-the-art%20in%20dexterous%20robotic%20manipulation%20through%20a%20fully%20integrated%2C%0Apractical%20approach%20to%20hardware%2C%20learning%2C%20and%20real-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11916v1&entry.124074799=Read"},
{"title": "SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis", "author": "Ssharvien Kumar Sivakumar and Yannik Frisch and Ghazal Ghazaei and Anirban Mukhopadhyay", "abstract": "  Surgical simulation plays a pivotal role in training novice surgeons,\naccelerating their learning curve and reducing intra-operative errors. However,\nconventional simulation tools fall short in providing the necessary\nphotorealism and the variability of human anatomy. In response, current methods\nare shifting towards generative model-based simulators. Yet, these approaches\nprimarily focus on using increasingly complex conditioning for precise\nsynthesis while neglecting the fine-grained human control aspect. To address\nthis gap, we introduce SG2VID, the first diffusion-based video model that\nleverages Scene Graphs for both precise video synthesis and fine-grained human\ncontrol. We demonstrate SG2VID's capabilities across three public datasets\nfeaturing cataract and cholecystectomy surgery. While SG2VID outperforms\nprevious methods both qualitatively and quantitatively, it also enables precise\nsynthesis, providing accurate control over tool and anatomy's size and\nmovement, entrance of new tools, as well as the overall scene layout. We\nqualitatively motivate how SG2VID can be used for generative augmentation and\npresent an experiment demonstrating its ability to improve a downstream phase\ndetection task when the training set is extended with our synthetic videos.\nFinally, to showcase SG2VID's ability to retain human control, we interact with\nthe Scene Graphs to generate new video samples depicting major yet rare\nintra-operative irregularities.\n", "link": "http://arxiv.org/abs/2506.03082v2", "date": "2025-06-13", "relevancy": 2.4778, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6287}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6135}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SG2VID%3A%20Scene%20Graphs%20Enable%20Fine-Grained%20Control%20for%20Video%20Synthesis&body=Title%3A%20SG2VID%3A%20Scene%20Graphs%20Enable%20Fine-Grained%20Control%20for%20Video%20Synthesis%0AAuthor%3A%20Ssharvien%20Kumar%20Sivakumar%20and%20Yannik%20Frisch%20and%20Ghazal%20Ghazaei%20and%20Anirban%20Mukhopadhyay%0AAbstract%3A%20%20%20Surgical%20simulation%20plays%20a%20pivotal%20role%20in%20training%20novice%20surgeons%2C%0Aaccelerating%20their%20learning%20curve%20and%20reducing%20intra-operative%20errors.%20However%2C%0Aconventional%20simulation%20tools%20fall%20short%20in%20providing%20the%20necessary%0Aphotorealism%20and%20the%20variability%20of%20human%20anatomy.%20In%20response%2C%20current%20methods%0Aare%20shifting%20towards%20generative%20model-based%20simulators.%20Yet%2C%20these%20approaches%0Aprimarily%20focus%20on%20using%20increasingly%20complex%20conditioning%20for%20precise%0Asynthesis%20while%20neglecting%20the%20fine-grained%20human%20control%20aspect.%20To%20address%0Athis%20gap%2C%20we%20introduce%20SG2VID%2C%20the%20first%20diffusion-based%20video%20model%20that%0Aleverages%20Scene%20Graphs%20for%20both%20precise%20video%20synthesis%20and%20fine-grained%20human%0Acontrol.%20We%20demonstrate%20SG2VID%27s%20capabilities%20across%20three%20public%20datasets%0Afeaturing%20cataract%20and%20cholecystectomy%20surgery.%20While%20SG2VID%20outperforms%0Aprevious%20methods%20both%20qualitatively%20and%20quantitatively%2C%20it%20also%20enables%20precise%0Asynthesis%2C%20providing%20accurate%20control%20over%20tool%20and%20anatomy%27s%20size%20and%0Amovement%2C%20entrance%20of%20new%20tools%2C%20as%20well%20as%20the%20overall%20scene%20layout.%20We%0Aqualitatively%20motivate%20how%20SG2VID%20can%20be%20used%20for%20generative%20augmentation%20and%0Apresent%20an%20experiment%20demonstrating%20its%20ability%20to%20improve%20a%20downstream%20phase%0Adetection%20task%20when%20the%20training%20set%20is%20extended%20with%20our%20synthetic%20videos.%0AFinally%2C%20to%20showcase%20SG2VID%27s%20ability%20to%20retain%20human%20control%2C%20we%20interact%20with%0Athe%20Scene%20Graphs%20to%20generate%20new%20video%20samples%20depicting%20major%20yet%20rare%0Aintra-operative%20irregularities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSG2VID%253A%2520Scene%2520Graphs%2520Enable%2520Fine-Grained%2520Control%2520for%2520Video%2520Synthesis%26entry.906535625%3DSsharvien%2520Kumar%2520Sivakumar%2520and%2520Yannik%2520Frisch%2520and%2520Ghazal%2520Ghazaei%2520and%2520Anirban%2520Mukhopadhyay%26entry.1292438233%3D%2520%2520Surgical%2520simulation%2520plays%2520a%2520pivotal%2520role%2520in%2520training%2520novice%2520surgeons%252C%250Aaccelerating%2520their%2520learning%2520curve%2520and%2520reducing%2520intra-operative%2520errors.%2520However%252C%250Aconventional%2520simulation%2520tools%2520fall%2520short%2520in%2520providing%2520the%2520necessary%250Aphotorealism%2520and%2520the%2520variability%2520of%2520human%2520anatomy.%2520In%2520response%252C%2520current%2520methods%250Aare%2520shifting%2520towards%2520generative%2520model-based%2520simulators.%2520Yet%252C%2520these%2520approaches%250Aprimarily%2520focus%2520on%2520using%2520increasingly%2520complex%2520conditioning%2520for%2520precise%250Asynthesis%2520while%2520neglecting%2520the%2520fine-grained%2520human%2520control%2520aspect.%2520To%2520address%250Athis%2520gap%252C%2520we%2520introduce%2520SG2VID%252C%2520the%2520first%2520diffusion-based%2520video%2520model%2520that%250Aleverages%2520Scene%2520Graphs%2520for%2520both%2520precise%2520video%2520synthesis%2520and%2520fine-grained%2520human%250Acontrol.%2520We%2520demonstrate%2520SG2VID%2527s%2520capabilities%2520across%2520three%2520public%2520datasets%250Afeaturing%2520cataract%2520and%2520cholecystectomy%2520surgery.%2520While%2520SG2VID%2520outperforms%250Aprevious%2520methods%2520both%2520qualitatively%2520and%2520quantitatively%252C%2520it%2520also%2520enables%2520precise%250Asynthesis%252C%2520providing%2520accurate%2520control%2520over%2520tool%2520and%2520anatomy%2527s%2520size%2520and%250Amovement%252C%2520entrance%2520of%2520new%2520tools%252C%2520as%2520well%2520as%2520the%2520overall%2520scene%2520layout.%2520We%250Aqualitatively%2520motivate%2520how%2520SG2VID%2520can%2520be%2520used%2520for%2520generative%2520augmentation%2520and%250Apresent%2520an%2520experiment%2520demonstrating%2520its%2520ability%2520to%2520improve%2520a%2520downstream%2520phase%250Adetection%2520task%2520when%2520the%2520training%2520set%2520is%2520extended%2520with%2520our%2520synthetic%2520videos.%250AFinally%252C%2520to%2520showcase%2520SG2VID%2527s%2520ability%2520to%2520retain%2520human%2520control%252C%2520we%2520interact%2520with%250Athe%2520Scene%2520Graphs%2520to%2520generate%2520new%2520video%2520samples%2520depicting%2520major%2520yet%2520rare%250Aintra-operative%2520irregularities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SG2VID%3A%20Scene%20Graphs%20Enable%20Fine-Grained%20Control%20for%20Video%20Synthesis&entry.906535625=Ssharvien%20Kumar%20Sivakumar%20and%20Yannik%20Frisch%20and%20Ghazal%20Ghazaei%20and%20Anirban%20Mukhopadhyay&entry.1292438233=%20%20Surgical%20simulation%20plays%20a%20pivotal%20role%20in%20training%20novice%20surgeons%2C%0Aaccelerating%20their%20learning%20curve%20and%20reducing%20intra-operative%20errors.%20However%2C%0Aconventional%20simulation%20tools%20fall%20short%20in%20providing%20the%20necessary%0Aphotorealism%20and%20the%20variability%20of%20human%20anatomy.%20In%20response%2C%20current%20methods%0Aare%20shifting%20towards%20generative%20model-based%20simulators.%20Yet%2C%20these%20approaches%0Aprimarily%20focus%20on%20using%20increasingly%20complex%20conditioning%20for%20precise%0Asynthesis%20while%20neglecting%20the%20fine-grained%20human%20control%20aspect.%20To%20address%0Athis%20gap%2C%20we%20introduce%20SG2VID%2C%20the%20first%20diffusion-based%20video%20model%20that%0Aleverages%20Scene%20Graphs%20for%20both%20precise%20video%20synthesis%20and%20fine-grained%20human%0Acontrol.%20We%20demonstrate%20SG2VID%27s%20capabilities%20across%20three%20public%20datasets%0Afeaturing%20cataract%20and%20cholecystectomy%20surgery.%20While%20SG2VID%20outperforms%0Aprevious%20methods%20both%20qualitatively%20and%20quantitatively%2C%20it%20also%20enables%20precise%0Asynthesis%2C%20providing%20accurate%20control%20over%20tool%20and%20anatomy%27s%20size%20and%0Amovement%2C%20entrance%20of%20new%20tools%2C%20as%20well%20as%20the%20overall%20scene%20layout.%20We%0Aqualitatively%20motivate%20how%20SG2VID%20can%20be%20used%20for%20generative%20augmentation%20and%0Apresent%20an%20experiment%20demonstrating%20its%20ability%20to%20improve%20a%20downstream%20phase%0Adetection%20task%20when%20the%20training%20set%20is%20extended%20with%20our%20synthetic%20videos.%0AFinally%2C%20to%20showcase%20SG2VID%27s%20ability%20to%20retain%20human%20control%2C%20we%20interact%20with%0Athe%20Scene%20Graphs%20to%20generate%20new%20video%20samples%20depicting%20major%20yet%20rare%0Aintra-operative%20irregularities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03082v2&entry.124074799=Read"},
{"title": "Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation", "author": "Min-Seop Kwak and Junho Kim and Sangdoo Yun and Dongyoon Han and Taekyoung Kim and Seungryong Kim and Jin-Hwa Kim", "abstract": "  We introduce a diffusion-based framework that performs aligned novel view\nimage and geometry generation via a warping-and-inpainting methodology. Unlike\nprior methods that require dense posed images or pose-embedded generative\nmodels limited to in-domain views, our method leverages off-the-shelf geometry\npredictors to predict partial geometries viewed from reference images, and\nformulates novel-view synthesis as an inpainting task for both image and\ngeometry. To ensure accurate alignment between generated images and geometry,\nwe propose cross-modal attention distillation, where attention maps from the\nimage diffusion branch are injected into a parallel geometry diffusion branch\nduring both training and inference. This multi-task approach achieves\nsynergistic effects, facilitating geometrically robust image synthesis as well\nas well-defined geometry prediction. We further introduce proximity-based mesh\nconditioning to integrate depth and normal cues, interpolating between point\ncloud and filtering erroneously predicted geometry from influencing the\ngeneration process. Empirically, our method achieves high-fidelity\nextrapolative view synthesis on both image and geometry across a range of\nunseen scenes, delivers competitive reconstruction quality under interpolation\nsettings, and produces geometrically aligned colored point clouds for\ncomprehensive 3D completion. Project page is available at\nhttps://cvlab-kaist.github.io/MoAI.\n", "link": "http://arxiv.org/abs/2506.11924v1", "date": "2025-06-13", "relevancy": 2.4633, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.617}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.617}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.61}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligned%20Novel%20View%20Image%20and%20Geometry%20Synthesis%20via%20Cross-modal%0A%20%20Attention%20Instillation&body=Title%3A%20Aligned%20Novel%20View%20Image%20and%20Geometry%20Synthesis%20via%20Cross-modal%0A%20%20Attention%20Instillation%0AAuthor%3A%20Min-Seop%20Kwak%20and%20Junho%20Kim%20and%20Sangdoo%20Yun%20and%20Dongyoon%20Han%20and%20Taekyoung%20Kim%20and%20Seungryong%20Kim%20and%20Jin-Hwa%20Kim%0AAbstract%3A%20%20%20We%20introduce%20a%20diffusion-based%20framework%20that%20performs%20aligned%20novel%20view%0Aimage%20and%20geometry%20generation%20via%20a%20warping-and-inpainting%20methodology.%20Unlike%0Aprior%20methods%20that%20require%20dense%20posed%20images%20or%20pose-embedded%20generative%0Amodels%20limited%20to%20in-domain%20views%2C%20our%20method%20leverages%20off-the-shelf%20geometry%0Apredictors%20to%20predict%20partial%20geometries%20viewed%20from%20reference%20images%2C%20and%0Aformulates%20novel-view%20synthesis%20as%20an%20inpainting%20task%20for%20both%20image%20and%0Ageometry.%20To%20ensure%20accurate%20alignment%20between%20generated%20images%20and%20geometry%2C%0Awe%20propose%20cross-modal%20attention%20distillation%2C%20where%20attention%20maps%20from%20the%0Aimage%20diffusion%20branch%20are%20injected%20into%20a%20parallel%20geometry%20diffusion%20branch%0Aduring%20both%20training%20and%20inference.%20This%20multi-task%20approach%20achieves%0Asynergistic%20effects%2C%20facilitating%20geometrically%20robust%20image%20synthesis%20as%20well%0Aas%20well-defined%20geometry%20prediction.%20We%20further%20introduce%20proximity-based%20mesh%0Aconditioning%20to%20integrate%20depth%20and%20normal%20cues%2C%20interpolating%20between%20point%0Acloud%20and%20filtering%20erroneously%20predicted%20geometry%20from%20influencing%20the%0Ageneration%20process.%20Empirically%2C%20our%20method%20achieves%20high-fidelity%0Aextrapolative%20view%20synthesis%20on%20both%20image%20and%20geometry%20across%20a%20range%20of%0Aunseen%20scenes%2C%20delivers%20competitive%20reconstruction%20quality%20under%20interpolation%0Asettings%2C%20and%20produces%20geometrically%20aligned%20colored%20point%20clouds%20for%0Acomprehensive%203D%20completion.%20Project%20page%20is%20available%20at%0Ahttps%3A//cvlab-kaist.github.io/MoAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligned%2520Novel%2520View%2520Image%2520and%2520Geometry%2520Synthesis%2520via%2520Cross-modal%250A%2520%2520Attention%2520Instillation%26entry.906535625%3DMin-Seop%2520Kwak%2520and%2520Junho%2520Kim%2520and%2520Sangdoo%2520Yun%2520and%2520Dongyoon%2520Han%2520and%2520Taekyoung%2520Kim%2520and%2520Seungryong%2520Kim%2520and%2520Jin-Hwa%2520Kim%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520diffusion-based%2520framework%2520that%2520performs%2520aligned%2520novel%2520view%250Aimage%2520and%2520geometry%2520generation%2520via%2520a%2520warping-and-inpainting%2520methodology.%2520Unlike%250Aprior%2520methods%2520that%2520require%2520dense%2520posed%2520images%2520or%2520pose-embedded%2520generative%250Amodels%2520limited%2520to%2520in-domain%2520views%252C%2520our%2520method%2520leverages%2520off-the-shelf%2520geometry%250Apredictors%2520to%2520predict%2520partial%2520geometries%2520viewed%2520from%2520reference%2520images%252C%2520and%250Aformulates%2520novel-view%2520synthesis%2520as%2520an%2520inpainting%2520task%2520for%2520both%2520image%2520and%250Ageometry.%2520To%2520ensure%2520accurate%2520alignment%2520between%2520generated%2520images%2520and%2520geometry%252C%250Awe%2520propose%2520cross-modal%2520attention%2520distillation%252C%2520where%2520attention%2520maps%2520from%2520the%250Aimage%2520diffusion%2520branch%2520are%2520injected%2520into%2520a%2520parallel%2520geometry%2520diffusion%2520branch%250Aduring%2520both%2520training%2520and%2520inference.%2520This%2520multi-task%2520approach%2520achieves%250Asynergistic%2520effects%252C%2520facilitating%2520geometrically%2520robust%2520image%2520synthesis%2520as%2520well%250Aas%2520well-defined%2520geometry%2520prediction.%2520We%2520further%2520introduce%2520proximity-based%2520mesh%250Aconditioning%2520to%2520integrate%2520depth%2520and%2520normal%2520cues%252C%2520interpolating%2520between%2520point%250Acloud%2520and%2520filtering%2520erroneously%2520predicted%2520geometry%2520from%2520influencing%2520the%250Ageneration%2520process.%2520Empirically%252C%2520our%2520method%2520achieves%2520high-fidelity%250Aextrapolative%2520view%2520synthesis%2520on%2520both%2520image%2520and%2520geometry%2520across%2520a%2520range%2520of%250Aunseen%2520scenes%252C%2520delivers%2520competitive%2520reconstruction%2520quality%2520under%2520interpolation%250Asettings%252C%2520and%2520produces%2520geometrically%2520aligned%2520colored%2520point%2520clouds%2520for%250Acomprehensive%25203D%2520completion.%2520Project%2520page%2520is%2520available%2520at%250Ahttps%253A//cvlab-kaist.github.io/MoAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligned%20Novel%20View%20Image%20and%20Geometry%20Synthesis%20via%20Cross-modal%0A%20%20Attention%20Instillation&entry.906535625=Min-Seop%20Kwak%20and%20Junho%20Kim%20and%20Sangdoo%20Yun%20and%20Dongyoon%20Han%20and%20Taekyoung%20Kim%20and%20Seungryong%20Kim%20and%20Jin-Hwa%20Kim&entry.1292438233=%20%20We%20introduce%20a%20diffusion-based%20framework%20that%20performs%20aligned%20novel%20view%0Aimage%20and%20geometry%20generation%20via%20a%20warping-and-inpainting%20methodology.%20Unlike%0Aprior%20methods%20that%20require%20dense%20posed%20images%20or%20pose-embedded%20generative%0Amodels%20limited%20to%20in-domain%20views%2C%20our%20method%20leverages%20off-the-shelf%20geometry%0Apredictors%20to%20predict%20partial%20geometries%20viewed%20from%20reference%20images%2C%20and%0Aformulates%20novel-view%20synthesis%20as%20an%20inpainting%20task%20for%20both%20image%20and%0Ageometry.%20To%20ensure%20accurate%20alignment%20between%20generated%20images%20and%20geometry%2C%0Awe%20propose%20cross-modal%20attention%20distillation%2C%20where%20attention%20maps%20from%20the%0Aimage%20diffusion%20branch%20are%20injected%20into%20a%20parallel%20geometry%20diffusion%20branch%0Aduring%20both%20training%20and%20inference.%20This%20multi-task%20approach%20achieves%0Asynergistic%20effects%2C%20facilitating%20geometrically%20robust%20image%20synthesis%20as%20well%0Aas%20well-defined%20geometry%20prediction.%20We%20further%20introduce%20proximity-based%20mesh%0Aconditioning%20to%20integrate%20depth%20and%20normal%20cues%2C%20interpolating%20between%20point%0Acloud%20and%20filtering%20erroneously%20predicted%20geometry%20from%20influencing%20the%0Ageneration%20process.%20Empirically%2C%20our%20method%20achieves%20high-fidelity%0Aextrapolative%20view%20synthesis%20on%20both%20image%20and%20geometry%20across%20a%20range%20of%0Aunseen%20scenes%2C%20delivers%20competitive%20reconstruction%20quality%20under%20interpolation%0Asettings%2C%20and%20produces%20geometrically%20aligned%20colored%20point%20clouds%20for%0Acomprehensive%203D%20completion.%20Project%20page%20is%20available%20at%0Ahttps%3A//cvlab-kaist.github.io/MoAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11924v1&entry.124074799=Read"},
{"title": "Table-R1: Region-based Reinforcement Learning for Table Understanding", "author": "Zhenhe Wu and Jian Yang and Jiaheng Liu and Xianjie Wu and Changzai Pan and Jie Zhang and Yu Zhao and Shuangyong Song and Yongxiang Li and Zhoujun Li", "abstract": "  Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning.\n", "link": "http://arxiv.org/abs/2505.12415v2", "date": "2025-06-13", "relevancy": 2.4616, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4933}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4933}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Table-R1%3A%20Region-based%20Reinforcement%20Learning%20for%20Table%20Understanding&body=Title%3A%20Table-R1%3A%20Region-based%20Reinforcement%20Learning%20for%20Table%20Understanding%0AAuthor%3A%20Zhenhe%20Wu%20and%20Jian%20Yang%20and%20Jiaheng%20Liu%20and%20Xianjie%20Wu%20and%20Changzai%20Pan%20and%20Jie%20Zhang%20and%20Yu%20Zhao%20and%20Shuangyong%20Song%20and%20Yongxiang%20Li%20and%20Zhoujun%20Li%0AAbstract%3A%20%20%20Tables%20present%20unique%20challenges%20for%20language%20models%20due%20to%20their%20structured%0Arow-column%20interactions%2C%20necessitating%20specialized%20approaches%20for%20effective%0Acomprehension.%20While%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20potential%0Ain%20table%20reasoning%20through%20prompting%20and%20techniques%20like%20chain-of-thought%20%28CoT%29%0Aand%20program-of-thought%20%28PoT%29%2C%20optimizing%20their%20performance%20for%20table%20question%0Aanswering%20remains%20underexplored.%20In%20this%20paper%2C%20we%20introduce%20region-based%0ATable-R1%2C%20a%20novel%20reinforcement%20learning%20approach%20that%20enhances%20LLM%20table%0Aunderstanding%20by%20integrating%20region%20evidence%20into%20reasoning%20steps.%20Our%20method%0Aemploys%20Region-Enhanced%20Supervised%20Fine-Tuning%20%28RE-SFT%29%20to%20guide%20models%20in%0Aidentifying%20relevant%20table%20regions%20before%20generating%20answers%2C%20incorporating%0Atextual%2C%20symbolic%2C%20and%20program-based%20reasoning.%20Additionally%2C%20Table-Aware%20Group%0ARelative%20Policy%20Optimization%20%28TARPO%29%20introduces%20a%20mixed%20reward%20system%20to%0Adynamically%20balance%20region%20accuracy%20and%20answer%20correctness%2C%20with%20decaying%0Aregion%20rewards%20and%20consistency%20penalties%20to%20align%20reasoning%20steps.%20Experiments%0Ashow%20that%20Table-R1%20achieves%20an%20average%20performance%20improvement%20of%2014.36%20points%0Aacross%20multiple%20base%20models%20on%20three%20benchmark%20datasets%2C%20even%20outperforming%0Abaseline%20models%20with%20ten%20times%20the%20parameters%2C%20while%20TARPO%20reduces%20response%0Atoken%20consumption%20by%2067.5%25%20compared%20to%20GRPO%2C%20significantly%20advancing%20LLM%0Acapabilities%20in%20efficient%20tabular%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12415v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTable-R1%253A%2520Region-based%2520Reinforcement%2520Learning%2520for%2520Table%2520Understanding%26entry.906535625%3DZhenhe%2520Wu%2520and%2520Jian%2520Yang%2520and%2520Jiaheng%2520Liu%2520and%2520Xianjie%2520Wu%2520and%2520Changzai%2520Pan%2520and%2520Jie%2520Zhang%2520and%2520Yu%2520Zhao%2520and%2520Shuangyong%2520Song%2520and%2520Yongxiang%2520Li%2520and%2520Zhoujun%2520Li%26entry.1292438233%3D%2520%2520Tables%2520present%2520unique%2520challenges%2520for%2520language%2520models%2520due%2520to%2520their%2520structured%250Arow-column%2520interactions%252C%2520necessitating%2520specialized%2520approaches%2520for%2520effective%250Acomprehension.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520potential%250Ain%2520table%2520reasoning%2520through%2520prompting%2520and%2520techniques%2520like%2520chain-of-thought%2520%2528CoT%2529%250Aand%2520program-of-thought%2520%2528PoT%2529%252C%2520optimizing%2520their%2520performance%2520for%2520table%2520question%250Aanswering%2520remains%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520region-based%250ATable-R1%252C%2520a%2520novel%2520reinforcement%2520learning%2520approach%2520that%2520enhances%2520LLM%2520table%250Aunderstanding%2520by%2520integrating%2520region%2520evidence%2520into%2520reasoning%2520steps.%2520Our%2520method%250Aemploys%2520Region-Enhanced%2520Supervised%2520Fine-Tuning%2520%2528RE-SFT%2529%2520to%2520guide%2520models%2520in%250Aidentifying%2520relevant%2520table%2520regions%2520before%2520generating%2520answers%252C%2520incorporating%250Atextual%252C%2520symbolic%252C%2520and%2520program-based%2520reasoning.%2520Additionally%252C%2520Table-Aware%2520Group%250ARelative%2520Policy%2520Optimization%2520%2528TARPO%2529%2520introduces%2520a%2520mixed%2520reward%2520system%2520to%250Adynamically%2520balance%2520region%2520accuracy%2520and%2520answer%2520correctness%252C%2520with%2520decaying%250Aregion%2520rewards%2520and%2520consistency%2520penalties%2520to%2520align%2520reasoning%2520steps.%2520Experiments%250Ashow%2520that%2520Table-R1%2520achieves%2520an%2520average%2520performance%2520improvement%2520of%252014.36%2520points%250Aacross%2520multiple%2520base%2520models%2520on%2520three%2520benchmark%2520datasets%252C%2520even%2520outperforming%250Abaseline%2520models%2520with%2520ten%2520times%2520the%2520parameters%252C%2520while%2520TARPO%2520reduces%2520response%250Atoken%2520consumption%2520by%252067.5%2525%2520compared%2520to%2520GRPO%252C%2520significantly%2520advancing%2520LLM%250Acapabilities%2520in%2520efficient%2520tabular%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12415v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Table-R1%3A%20Region-based%20Reinforcement%20Learning%20for%20Table%20Understanding&entry.906535625=Zhenhe%20Wu%20and%20Jian%20Yang%20and%20Jiaheng%20Liu%20and%20Xianjie%20Wu%20and%20Changzai%20Pan%20and%20Jie%20Zhang%20and%20Yu%20Zhao%20and%20Shuangyong%20Song%20and%20Yongxiang%20Li%20and%20Zhoujun%20Li&entry.1292438233=%20%20Tables%20present%20unique%20challenges%20for%20language%20models%20due%20to%20their%20structured%0Arow-column%20interactions%2C%20necessitating%20specialized%20approaches%20for%20effective%0Acomprehension.%20While%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20potential%0Ain%20table%20reasoning%20through%20prompting%20and%20techniques%20like%20chain-of-thought%20%28CoT%29%0Aand%20program-of-thought%20%28PoT%29%2C%20optimizing%20their%20performance%20for%20table%20question%0Aanswering%20remains%20underexplored.%20In%20this%20paper%2C%20we%20introduce%20region-based%0ATable-R1%2C%20a%20novel%20reinforcement%20learning%20approach%20that%20enhances%20LLM%20table%0Aunderstanding%20by%20integrating%20region%20evidence%20into%20reasoning%20steps.%20Our%20method%0Aemploys%20Region-Enhanced%20Supervised%20Fine-Tuning%20%28RE-SFT%29%20to%20guide%20models%20in%0Aidentifying%20relevant%20table%20regions%20before%20generating%20answers%2C%20incorporating%0Atextual%2C%20symbolic%2C%20and%20program-based%20reasoning.%20Additionally%2C%20Table-Aware%20Group%0ARelative%20Policy%20Optimization%20%28TARPO%29%20introduces%20a%20mixed%20reward%20system%20to%0Adynamically%20balance%20region%20accuracy%20and%20answer%20correctness%2C%20with%20decaying%0Aregion%20rewards%20and%20consistency%20penalties%20to%20align%20reasoning%20steps.%20Experiments%0Ashow%20that%20Table-R1%20achieves%20an%20average%20performance%20improvement%20of%2014.36%20points%0Aacross%20multiple%20base%20models%20on%20three%20benchmark%20datasets%2C%20even%20outperforming%0Abaseline%20models%20with%20ten%20times%20the%20parameters%2C%20while%20TARPO%20reduces%20response%0Atoken%20consumption%20by%2067.5%25%20compared%20to%20GRPO%2C%20significantly%20advancing%20LLM%0Acapabilities%20in%20efficient%20tabular%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12415v2&entry.124074799=Read"},
{"title": "MindGrab for BrainChop: Fast and Accurate Skull Stripping for Command\n  Line and Browser", "author": "Armina Fani and Mike Doan and Isabelle Le and Alex Fedorov and Malte Hoffmann and Chris Rorden and Sergey Plis", "abstract": "  We developed MindGrab, a parameter- and memory-efficient deep\nfully-convolutional model for volumetric skull-stripping in head images of any\nmodality. Its architecture, informed by a spectral interpretation of dilated\nconvolutions, was trained exclusively on modality-agnostic synthetic data.\nMindGrab was evaluated on a retrospective dataset of 606 multimodal adult-brain\nscans (T1, T2, DWI, MRA, PDw MRI, EPI, CT, PET) sourced from the SynthStrip\ndataset. Performance was benchmarked against SynthStrip, ROBEX, and BET using\nDice scores, with Wilcoxon signed-rank significance tests. MindGrab achieved a\nmean Dice score of 95.9 with standard deviation (SD) 1.6 across modalities,\nsignificantly outperforming classical methods (ROBEX: 89.1 SD 7.7, P < 0.05;\nBET: 85.2 SD 14.4, P < 0.05). Compared to SynthStrip (96.5 SD 1.1, P=0.0352),\nMindGrab delivered equivalent or superior performance in nearly half of the\ntested scenarios, with minor differences (<3% Dice) in the others. MindGrab\nutilized 95% fewer parameters (146,237 vs. 2,566,561) than SynthStrip. This\nefficiency yielded at least 2x faster inference, 50% lower memory usage on\nGPUs, and enabled exceptional performance (e.g., 10-30x speedup, and up to 30x\nmemory reduction) and accessibility on a wider range of hardware, including\nsystems without high-end GPUs. MindGrab delivers state-of-the-art accuracy with\ndramatically lower resource demands, supported in brainchop-cli\n(https://pypi.org/project/brainchop/) and at brainchop.org.\n", "link": "http://arxiv.org/abs/2506.11860v1", "date": "2025-06-13", "relevancy": 2.4498, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5102}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4799}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MindGrab%20for%20BrainChop%3A%20Fast%20and%20Accurate%20Skull%20Stripping%20for%20Command%0A%20%20Line%20and%20Browser&body=Title%3A%20MindGrab%20for%20BrainChop%3A%20Fast%20and%20Accurate%20Skull%20Stripping%20for%20Command%0A%20%20Line%20and%20Browser%0AAuthor%3A%20Armina%20Fani%20and%20Mike%20Doan%20and%20Isabelle%20Le%20and%20Alex%20Fedorov%20and%20Malte%20Hoffmann%20and%20Chris%20Rorden%20and%20Sergey%20Plis%0AAbstract%3A%20%20%20We%20developed%20MindGrab%2C%20a%20parameter-%20and%20memory-efficient%20deep%0Afully-convolutional%20model%20for%20volumetric%20skull-stripping%20in%20head%20images%20of%20any%0Amodality.%20Its%20architecture%2C%20informed%20by%20a%20spectral%20interpretation%20of%20dilated%0Aconvolutions%2C%20was%20trained%20exclusively%20on%20modality-agnostic%20synthetic%20data.%0AMindGrab%20was%20evaluated%20on%20a%20retrospective%20dataset%20of%20606%20multimodal%20adult-brain%0Ascans%20%28T1%2C%20T2%2C%20DWI%2C%20MRA%2C%20PDw%20MRI%2C%20EPI%2C%20CT%2C%20PET%29%20sourced%20from%20the%20SynthStrip%0Adataset.%20Performance%20was%20benchmarked%20against%20SynthStrip%2C%20ROBEX%2C%20and%20BET%20using%0ADice%20scores%2C%20with%20Wilcoxon%20signed-rank%20significance%20tests.%20MindGrab%20achieved%20a%0Amean%20Dice%20score%20of%2095.9%20with%20standard%20deviation%20%28SD%29%201.6%20across%20modalities%2C%0Asignificantly%20outperforming%20classical%20methods%20%28ROBEX%3A%2089.1%20SD%207.7%2C%20P%20%3C%200.05%3B%0ABET%3A%2085.2%20SD%2014.4%2C%20P%20%3C%200.05%29.%20Compared%20to%20SynthStrip%20%2896.5%20SD%201.1%2C%20P%3D0.0352%29%2C%0AMindGrab%20delivered%20equivalent%20or%20superior%20performance%20in%20nearly%20half%20of%20the%0Atested%20scenarios%2C%20with%20minor%20differences%20%28%3C3%25%20Dice%29%20in%20the%20others.%20MindGrab%0Autilized%2095%25%20fewer%20parameters%20%28146%2C237%20vs.%202%2C566%2C561%29%20than%20SynthStrip.%20This%0Aefficiency%20yielded%20at%20least%202x%20faster%20inference%2C%2050%25%20lower%20memory%20usage%20on%0AGPUs%2C%20and%20enabled%20exceptional%20performance%20%28e.g.%2C%2010-30x%20speedup%2C%20and%20up%20to%2030x%0Amemory%20reduction%29%20and%20accessibility%20on%20a%20wider%20range%20of%20hardware%2C%20including%0Asystems%20without%20high-end%20GPUs.%20MindGrab%20delivers%20state-of-the-art%20accuracy%20with%0Adramatically%20lower%20resource%20demands%2C%20supported%20in%20brainchop-cli%0A%28https%3A//pypi.org/project/brainchop/%29%20and%20at%20brainchop.org.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMindGrab%2520for%2520BrainChop%253A%2520Fast%2520and%2520Accurate%2520Skull%2520Stripping%2520for%2520Command%250A%2520%2520Line%2520and%2520Browser%26entry.906535625%3DArmina%2520Fani%2520and%2520Mike%2520Doan%2520and%2520Isabelle%2520Le%2520and%2520Alex%2520Fedorov%2520and%2520Malte%2520Hoffmann%2520and%2520Chris%2520Rorden%2520and%2520Sergey%2520Plis%26entry.1292438233%3D%2520%2520We%2520developed%2520MindGrab%252C%2520a%2520parameter-%2520and%2520memory-efficient%2520deep%250Afully-convolutional%2520model%2520for%2520volumetric%2520skull-stripping%2520in%2520head%2520images%2520of%2520any%250Amodality.%2520Its%2520architecture%252C%2520informed%2520by%2520a%2520spectral%2520interpretation%2520of%2520dilated%250Aconvolutions%252C%2520was%2520trained%2520exclusively%2520on%2520modality-agnostic%2520synthetic%2520data.%250AMindGrab%2520was%2520evaluated%2520on%2520a%2520retrospective%2520dataset%2520of%2520606%2520multimodal%2520adult-brain%250Ascans%2520%2528T1%252C%2520T2%252C%2520DWI%252C%2520MRA%252C%2520PDw%2520MRI%252C%2520EPI%252C%2520CT%252C%2520PET%2529%2520sourced%2520from%2520the%2520SynthStrip%250Adataset.%2520Performance%2520was%2520benchmarked%2520against%2520SynthStrip%252C%2520ROBEX%252C%2520and%2520BET%2520using%250ADice%2520scores%252C%2520with%2520Wilcoxon%2520signed-rank%2520significance%2520tests.%2520MindGrab%2520achieved%2520a%250Amean%2520Dice%2520score%2520of%252095.9%2520with%2520standard%2520deviation%2520%2528SD%2529%25201.6%2520across%2520modalities%252C%250Asignificantly%2520outperforming%2520classical%2520methods%2520%2528ROBEX%253A%252089.1%2520SD%25207.7%252C%2520P%2520%253C%25200.05%253B%250ABET%253A%252085.2%2520SD%252014.4%252C%2520P%2520%253C%25200.05%2529.%2520Compared%2520to%2520SynthStrip%2520%252896.5%2520SD%25201.1%252C%2520P%253D0.0352%2529%252C%250AMindGrab%2520delivered%2520equivalent%2520or%2520superior%2520performance%2520in%2520nearly%2520half%2520of%2520the%250Atested%2520scenarios%252C%2520with%2520minor%2520differences%2520%2528%253C3%2525%2520Dice%2529%2520in%2520the%2520others.%2520MindGrab%250Autilized%252095%2525%2520fewer%2520parameters%2520%2528146%252C237%2520vs.%25202%252C566%252C561%2529%2520than%2520SynthStrip.%2520This%250Aefficiency%2520yielded%2520at%2520least%25202x%2520faster%2520inference%252C%252050%2525%2520lower%2520memory%2520usage%2520on%250AGPUs%252C%2520and%2520enabled%2520exceptional%2520performance%2520%2528e.g.%252C%252010-30x%2520speedup%252C%2520and%2520up%2520to%252030x%250Amemory%2520reduction%2529%2520and%2520accessibility%2520on%2520a%2520wider%2520range%2520of%2520hardware%252C%2520including%250Asystems%2520without%2520high-end%2520GPUs.%2520MindGrab%2520delivers%2520state-of-the-art%2520accuracy%2520with%250Adramatically%2520lower%2520resource%2520demands%252C%2520supported%2520in%2520brainchop-cli%250A%2528https%253A//pypi.org/project/brainchop/%2529%2520and%2520at%2520brainchop.org.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MindGrab%20for%20BrainChop%3A%20Fast%20and%20Accurate%20Skull%20Stripping%20for%20Command%0A%20%20Line%20and%20Browser&entry.906535625=Armina%20Fani%20and%20Mike%20Doan%20and%20Isabelle%20Le%20and%20Alex%20Fedorov%20and%20Malte%20Hoffmann%20and%20Chris%20Rorden%20and%20Sergey%20Plis&entry.1292438233=%20%20We%20developed%20MindGrab%2C%20a%20parameter-%20and%20memory-efficient%20deep%0Afully-convolutional%20model%20for%20volumetric%20skull-stripping%20in%20head%20images%20of%20any%0Amodality.%20Its%20architecture%2C%20informed%20by%20a%20spectral%20interpretation%20of%20dilated%0Aconvolutions%2C%20was%20trained%20exclusively%20on%20modality-agnostic%20synthetic%20data.%0AMindGrab%20was%20evaluated%20on%20a%20retrospective%20dataset%20of%20606%20multimodal%20adult-brain%0Ascans%20%28T1%2C%20T2%2C%20DWI%2C%20MRA%2C%20PDw%20MRI%2C%20EPI%2C%20CT%2C%20PET%29%20sourced%20from%20the%20SynthStrip%0Adataset.%20Performance%20was%20benchmarked%20against%20SynthStrip%2C%20ROBEX%2C%20and%20BET%20using%0ADice%20scores%2C%20with%20Wilcoxon%20signed-rank%20significance%20tests.%20MindGrab%20achieved%20a%0Amean%20Dice%20score%20of%2095.9%20with%20standard%20deviation%20%28SD%29%201.6%20across%20modalities%2C%0Asignificantly%20outperforming%20classical%20methods%20%28ROBEX%3A%2089.1%20SD%207.7%2C%20P%20%3C%200.05%3B%0ABET%3A%2085.2%20SD%2014.4%2C%20P%20%3C%200.05%29.%20Compared%20to%20SynthStrip%20%2896.5%20SD%201.1%2C%20P%3D0.0352%29%2C%0AMindGrab%20delivered%20equivalent%20or%20superior%20performance%20in%20nearly%20half%20of%20the%0Atested%20scenarios%2C%20with%20minor%20differences%20%28%3C3%25%20Dice%29%20in%20the%20others.%20MindGrab%0Autilized%2095%25%20fewer%20parameters%20%28146%2C237%20vs.%202%2C566%2C561%29%20than%20SynthStrip.%20This%0Aefficiency%20yielded%20at%20least%202x%20faster%20inference%2C%2050%25%20lower%20memory%20usage%20on%0AGPUs%2C%20and%20enabled%20exceptional%20performance%20%28e.g.%2C%2010-30x%20speedup%2C%20and%20up%20to%2030x%0Amemory%20reduction%29%20and%20accessibility%20on%20a%20wider%20range%20of%20hardware%2C%20including%0Asystems%20without%20high-end%20GPUs.%20MindGrab%20delivers%20state-of-the-art%20accuracy%20with%0Adramatically%20lower%20resource%20demands%2C%20supported%20in%20brainchop-cli%0A%28https%3A//pypi.org/project/brainchop/%29%20and%20at%20brainchop.org.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11860v1&entry.124074799=Read"},
{"title": "Vision-Language Models for Edge Networks: A Comprehensive Survey", "author": "Ahmed Sharshar and Latif U. Khan and Waseem Ullah and Mohsen Guizani", "abstract": "  Vision Large Language Models (VLMs) combine visual understanding with natural\nlanguage processing, enabling tasks like image captioning, visual question\nanswering, and video analysis. While VLMs show impressive capabilities across\ndomains such as autonomous vehicles, smart surveillance, and healthcare, their\ndeployment on resource-constrained edge devices remains challenging due to\nprocessing power, memory, and energy limitations. This survey explores recent\nadvancements in optimizing VLMs for edge environments, focusing on model\ncompression techniques, including pruning, quantization, knowledge\ndistillation, and specialized hardware solutions that enhance efficiency. We\nprovide a detailed discussion of efficient training and fine-tuning methods,\nedge deployment challenges, and privacy considerations. Additionally, we\ndiscuss the diverse applications of lightweight VLMs across healthcare,\nenvironmental monitoring, and autonomous systems, illustrating their growing\nimpact. By highlighting key design strategies, current challenges, and offering\nrecommendations for future directions, this survey aims to inspire further\nresearch into the practical deployment of VLMs, ultimately making advanced AI\naccessible in resource-limited settings.\n", "link": "http://arxiv.org/abs/2502.07855v2", "date": "2025-06-13", "relevancy": 2.4291, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6226}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6226}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Models%20for%20Edge%20Networks%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Vision-Language%20Models%20for%20Edge%20Networks%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Ahmed%20Sharshar%20and%20Latif%20U.%20Khan%20and%20Waseem%20Ullah%20and%20Mohsen%20Guizani%0AAbstract%3A%20%20%20Vision%20Large%20Language%20Models%20%28VLMs%29%20combine%20visual%20understanding%20with%20natural%0Alanguage%20processing%2C%20enabling%20tasks%20like%20image%20captioning%2C%20visual%20question%0Aanswering%2C%20and%20video%20analysis.%20While%20VLMs%20show%20impressive%20capabilities%20across%0Adomains%20such%20as%20autonomous%20vehicles%2C%20smart%20surveillance%2C%20and%20healthcare%2C%20their%0Adeployment%20on%20resource-constrained%20edge%20devices%20remains%20challenging%20due%20to%0Aprocessing%20power%2C%20memory%2C%20and%20energy%20limitations.%20This%20survey%20explores%20recent%0Aadvancements%20in%20optimizing%20VLMs%20for%20edge%20environments%2C%20focusing%20on%20model%0Acompression%20techniques%2C%20including%20pruning%2C%20quantization%2C%20knowledge%0Adistillation%2C%20and%20specialized%20hardware%20solutions%20that%20enhance%20efficiency.%20We%0Aprovide%20a%20detailed%20discussion%20of%20efficient%20training%20and%20fine-tuning%20methods%2C%0Aedge%20deployment%20challenges%2C%20and%20privacy%20considerations.%20Additionally%2C%20we%0Adiscuss%20the%20diverse%20applications%20of%20lightweight%20VLMs%20across%20healthcare%2C%0Aenvironmental%20monitoring%2C%20and%20autonomous%20systems%2C%20illustrating%20their%20growing%0Aimpact.%20By%20highlighting%20key%20design%20strategies%2C%20current%20challenges%2C%20and%20offering%0Arecommendations%20for%20future%20directions%2C%20this%20survey%20aims%20to%20inspire%20further%0Aresearch%20into%20the%20practical%20deployment%20of%20VLMs%2C%20ultimately%20making%20advanced%20AI%0Aaccessible%20in%20resource-limited%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07855v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Models%2520for%2520Edge%2520Networks%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DAhmed%2520Sharshar%2520and%2520Latif%2520U.%2520Khan%2520and%2520Waseem%2520Ullah%2520and%2520Mohsen%2520Guizani%26entry.1292438233%3D%2520%2520Vision%2520Large%2520Language%2520Models%2520%2528VLMs%2529%2520combine%2520visual%2520understanding%2520with%2520natural%250Alanguage%2520processing%252C%2520enabling%2520tasks%2520like%2520image%2520captioning%252C%2520visual%2520question%250Aanswering%252C%2520and%2520video%2520analysis.%2520While%2520VLMs%2520show%2520impressive%2520capabilities%2520across%250Adomains%2520such%2520as%2520autonomous%2520vehicles%252C%2520smart%2520surveillance%252C%2520and%2520healthcare%252C%2520their%250Adeployment%2520on%2520resource-constrained%2520edge%2520devices%2520remains%2520challenging%2520due%2520to%250Aprocessing%2520power%252C%2520memory%252C%2520and%2520energy%2520limitations.%2520This%2520survey%2520explores%2520recent%250Aadvancements%2520in%2520optimizing%2520VLMs%2520for%2520edge%2520environments%252C%2520focusing%2520on%2520model%250Acompression%2520techniques%252C%2520including%2520pruning%252C%2520quantization%252C%2520knowledge%250Adistillation%252C%2520and%2520specialized%2520hardware%2520solutions%2520that%2520enhance%2520efficiency.%2520We%250Aprovide%2520a%2520detailed%2520discussion%2520of%2520efficient%2520training%2520and%2520fine-tuning%2520methods%252C%250Aedge%2520deployment%2520challenges%252C%2520and%2520privacy%2520considerations.%2520Additionally%252C%2520we%250Adiscuss%2520the%2520diverse%2520applications%2520of%2520lightweight%2520VLMs%2520across%2520healthcare%252C%250Aenvironmental%2520monitoring%252C%2520and%2520autonomous%2520systems%252C%2520illustrating%2520their%2520growing%250Aimpact.%2520By%2520highlighting%2520key%2520design%2520strategies%252C%2520current%2520challenges%252C%2520and%2520offering%250Arecommendations%2520for%2520future%2520directions%252C%2520this%2520survey%2520aims%2520to%2520inspire%2520further%250Aresearch%2520into%2520the%2520practical%2520deployment%2520of%2520VLMs%252C%2520ultimately%2520making%2520advanced%2520AI%250Aaccessible%2520in%2520resource-limited%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07855v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Models%20for%20Edge%20Networks%3A%20A%20Comprehensive%20Survey&entry.906535625=Ahmed%20Sharshar%20and%20Latif%20U.%20Khan%20and%20Waseem%20Ullah%20and%20Mohsen%20Guizani&entry.1292438233=%20%20Vision%20Large%20Language%20Models%20%28VLMs%29%20combine%20visual%20understanding%20with%20natural%0Alanguage%20processing%2C%20enabling%20tasks%20like%20image%20captioning%2C%20visual%20question%0Aanswering%2C%20and%20video%20analysis.%20While%20VLMs%20show%20impressive%20capabilities%20across%0Adomains%20such%20as%20autonomous%20vehicles%2C%20smart%20surveillance%2C%20and%20healthcare%2C%20their%0Adeployment%20on%20resource-constrained%20edge%20devices%20remains%20challenging%20due%20to%0Aprocessing%20power%2C%20memory%2C%20and%20energy%20limitations.%20This%20survey%20explores%20recent%0Aadvancements%20in%20optimizing%20VLMs%20for%20edge%20environments%2C%20focusing%20on%20model%0Acompression%20techniques%2C%20including%20pruning%2C%20quantization%2C%20knowledge%0Adistillation%2C%20and%20specialized%20hardware%20solutions%20that%20enhance%20efficiency.%20We%0Aprovide%20a%20detailed%20discussion%20of%20efficient%20training%20and%20fine-tuning%20methods%2C%0Aedge%20deployment%20challenges%2C%20and%20privacy%20considerations.%20Additionally%2C%20we%0Adiscuss%20the%20diverse%20applications%20of%20lightweight%20VLMs%20across%20healthcare%2C%0Aenvironmental%20monitoring%2C%20and%20autonomous%20systems%2C%20illustrating%20their%20growing%0Aimpact.%20By%20highlighting%20key%20design%20strategies%2C%20current%20challenges%2C%20and%20offering%0Arecommendations%20for%20future%20directions%2C%20this%20survey%20aims%20to%20inspire%20further%0Aresearch%20into%20the%20practical%20deployment%20of%20VLMs%2C%20ultimately%20making%20advanced%20AI%0Aaccessible%20in%20resource-limited%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07855v2&entry.124074799=Read"},
{"title": "Convergence Analysis of Natural Gradient Descent for Over-parameterized\n  Physics-Informed Neural Networks", "author": "Xianliang Xu and Ting Du and Wang Kong and Bin Shan and Ye Li and Zhongyi Huang", "abstract": "  In the context of over-parameterization, there is a line of work\ndemonstrating that randomly initialized (stochastic) gradient descent (GD)\nconverges to a globally optimal solution at a linear convergence rate for the\nquadratic loss function. However, the learning rate of GD for training\ntwo-layer neural networks exhibits poor dependence on the sample size and the\nGram matrix, leading to a slow training process. In this paper, we show that\nfor training two-layer $\\text{ReLU}^3$ Physics-Informed Neural Networks\n(PINNs), the learning rate can be improved from $\\mathcal{O}(\\lambda_0)$ to\n$\\mathcal{O}(1/\\|\\bm{H}^{\\infty}\\|_2)$, implying that GD actually enjoys a\nfaster convergence rate. Despite such improvements, the convergence rate is\nstill tied to the least eigenvalue of the Gram matrix, leading to slow\nconvergence. We then develop the positive definiteness of Gram matrices with\ngeneral smooth activation functions and provide the convergence analysis of\nnatural gradient descent (NGD) in training two-layer PINNs, demonstrating that\nthe learning rate can be $\\mathcal{O}(1)$ and at this rate, the convergence\nrate is independent of the Gram matrix. In particular, for smooth activation\nfunctions, the convergence rate of NGD is quadratic. Numerical experiments are\nconducted to verify our theoretical results.\n", "link": "http://arxiv.org/abs/2408.00573v4", "date": "2025-06-13", "relevancy": 2.4198, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4986}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4845}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20Analysis%20of%20Natural%20Gradient%20Descent%20for%20Over-parameterized%0A%20%20Physics-Informed%20Neural%20Networks&body=Title%3A%20Convergence%20Analysis%20of%20Natural%20Gradient%20Descent%20for%20Over-parameterized%0A%20%20Physics-Informed%20Neural%20Networks%0AAuthor%3A%20Xianliang%20Xu%20and%20Ting%20Du%20and%20Wang%20Kong%20and%20Bin%20Shan%20and%20Ye%20Li%20and%20Zhongyi%20Huang%0AAbstract%3A%20%20%20In%20the%20context%20of%20over-parameterization%2C%20there%20is%20a%20line%20of%20work%0Ademonstrating%20that%20randomly%20initialized%20%28stochastic%29%20gradient%20descent%20%28GD%29%0Aconverges%20to%20a%20globally%20optimal%20solution%20at%20a%20linear%20convergence%20rate%20for%20the%0Aquadratic%20loss%20function.%20However%2C%20the%20learning%20rate%20of%20GD%20for%20training%0Atwo-layer%20neural%20networks%20exhibits%20poor%20dependence%20on%20the%20sample%20size%20and%20the%0AGram%20matrix%2C%20leading%20to%20a%20slow%20training%20process.%20In%20this%20paper%2C%20we%20show%20that%0Afor%20training%20two-layer%20%24%5Ctext%7BReLU%7D%5E3%24%20Physics-Informed%20Neural%20Networks%0A%28PINNs%29%2C%20the%20learning%20rate%20can%20be%20improved%20from%20%24%5Cmathcal%7BO%7D%28%5Clambda_0%29%24%20to%0A%24%5Cmathcal%7BO%7D%281/%5C%7C%5Cbm%7BH%7D%5E%7B%5Cinfty%7D%5C%7C_2%29%24%2C%20implying%20that%20GD%20actually%20enjoys%20a%0Afaster%20convergence%20rate.%20Despite%20such%20improvements%2C%20the%20convergence%20rate%20is%0Astill%20tied%20to%20the%20least%20eigenvalue%20of%20the%20Gram%20matrix%2C%20leading%20to%20slow%0Aconvergence.%20We%20then%20develop%20the%20positive%20definiteness%20of%20Gram%20matrices%20with%0Ageneral%20smooth%20activation%20functions%20and%20provide%20the%20convergence%20analysis%20of%0Anatural%20gradient%20descent%20%28NGD%29%20in%20training%20two-layer%20PINNs%2C%20demonstrating%20that%0Athe%20learning%20rate%20can%20be%20%24%5Cmathcal%7BO%7D%281%29%24%20and%20at%20this%20rate%2C%20the%20convergence%0Arate%20is%20independent%20of%20the%20Gram%20matrix.%20In%20particular%2C%20for%20smooth%20activation%0Afunctions%2C%20the%20convergence%20rate%20of%20NGD%20is%20quadratic.%20Numerical%20experiments%20are%0Aconducted%20to%20verify%20our%20theoretical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00573v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520Analysis%2520of%2520Natural%2520Gradient%2520Descent%2520for%2520Over-parameterized%250A%2520%2520Physics-Informed%2520Neural%2520Networks%26entry.906535625%3DXianliang%2520Xu%2520and%2520Ting%2520Du%2520and%2520Wang%2520Kong%2520and%2520Bin%2520Shan%2520and%2520Ye%2520Li%2520and%2520Zhongyi%2520Huang%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520over-parameterization%252C%2520there%2520is%2520a%2520line%2520of%2520work%250Ademonstrating%2520that%2520randomly%2520initialized%2520%2528stochastic%2529%2520gradient%2520descent%2520%2528GD%2529%250Aconverges%2520to%2520a%2520globally%2520optimal%2520solution%2520at%2520a%2520linear%2520convergence%2520rate%2520for%2520the%250Aquadratic%2520loss%2520function.%2520However%252C%2520the%2520learning%2520rate%2520of%2520GD%2520for%2520training%250Atwo-layer%2520neural%2520networks%2520exhibits%2520poor%2520dependence%2520on%2520the%2520sample%2520size%2520and%2520the%250AGram%2520matrix%252C%2520leading%2520to%2520a%2520slow%2520training%2520process.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%250Afor%2520training%2520two-layer%2520%2524%255Ctext%257BReLU%257D%255E3%2524%2520Physics-Informed%2520Neural%2520Networks%250A%2528PINNs%2529%252C%2520the%2520learning%2520rate%2520can%2520be%2520improved%2520from%2520%2524%255Cmathcal%257BO%257D%2528%255Clambda_0%2529%2524%2520to%250A%2524%255Cmathcal%257BO%257D%25281/%255C%257C%255Cbm%257BH%257D%255E%257B%255Cinfty%257D%255C%257C_2%2529%2524%252C%2520implying%2520that%2520GD%2520actually%2520enjoys%2520a%250Afaster%2520convergence%2520rate.%2520Despite%2520such%2520improvements%252C%2520the%2520convergence%2520rate%2520is%250Astill%2520tied%2520to%2520the%2520least%2520eigenvalue%2520of%2520the%2520Gram%2520matrix%252C%2520leading%2520to%2520slow%250Aconvergence.%2520We%2520then%2520develop%2520the%2520positive%2520definiteness%2520of%2520Gram%2520matrices%2520with%250Ageneral%2520smooth%2520activation%2520functions%2520and%2520provide%2520the%2520convergence%2520analysis%2520of%250Anatural%2520gradient%2520descent%2520%2528NGD%2529%2520in%2520training%2520two-layer%2520PINNs%252C%2520demonstrating%2520that%250Athe%2520learning%2520rate%2520can%2520be%2520%2524%255Cmathcal%257BO%257D%25281%2529%2524%2520and%2520at%2520this%2520rate%252C%2520the%2520convergence%250Arate%2520is%2520independent%2520of%2520the%2520Gram%2520matrix.%2520In%2520particular%252C%2520for%2520smooth%2520activation%250Afunctions%252C%2520the%2520convergence%2520rate%2520of%2520NGD%2520is%2520quadratic.%2520Numerical%2520experiments%2520are%250Aconducted%2520to%2520verify%2520our%2520theoretical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00573v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20Analysis%20of%20Natural%20Gradient%20Descent%20for%20Over-parameterized%0A%20%20Physics-Informed%20Neural%20Networks&entry.906535625=Xianliang%20Xu%20and%20Ting%20Du%20and%20Wang%20Kong%20and%20Bin%20Shan%20and%20Ye%20Li%20and%20Zhongyi%20Huang&entry.1292438233=%20%20In%20the%20context%20of%20over-parameterization%2C%20there%20is%20a%20line%20of%20work%0Ademonstrating%20that%20randomly%20initialized%20%28stochastic%29%20gradient%20descent%20%28GD%29%0Aconverges%20to%20a%20globally%20optimal%20solution%20at%20a%20linear%20convergence%20rate%20for%20the%0Aquadratic%20loss%20function.%20However%2C%20the%20learning%20rate%20of%20GD%20for%20training%0Atwo-layer%20neural%20networks%20exhibits%20poor%20dependence%20on%20the%20sample%20size%20and%20the%0AGram%20matrix%2C%20leading%20to%20a%20slow%20training%20process.%20In%20this%20paper%2C%20we%20show%20that%0Afor%20training%20two-layer%20%24%5Ctext%7BReLU%7D%5E3%24%20Physics-Informed%20Neural%20Networks%0A%28PINNs%29%2C%20the%20learning%20rate%20can%20be%20improved%20from%20%24%5Cmathcal%7BO%7D%28%5Clambda_0%29%24%20to%0A%24%5Cmathcal%7BO%7D%281/%5C%7C%5Cbm%7BH%7D%5E%7B%5Cinfty%7D%5C%7C_2%29%24%2C%20implying%20that%20GD%20actually%20enjoys%20a%0Afaster%20convergence%20rate.%20Despite%20such%20improvements%2C%20the%20convergence%20rate%20is%0Astill%20tied%20to%20the%20least%20eigenvalue%20of%20the%20Gram%20matrix%2C%20leading%20to%20slow%0Aconvergence.%20We%20then%20develop%20the%20positive%20definiteness%20of%20Gram%20matrices%20with%0Ageneral%20smooth%20activation%20functions%20and%20provide%20the%20convergence%20analysis%20of%0Anatural%20gradient%20descent%20%28NGD%29%20in%20training%20two-layer%20PINNs%2C%20demonstrating%20that%0Athe%20learning%20rate%20can%20be%20%24%5Cmathcal%7BO%7D%281%29%24%20and%20at%20this%20rate%2C%20the%20convergence%0Arate%20is%20independent%20of%20the%20Gram%20matrix.%20In%20particular%2C%20for%20smooth%20activation%0Afunctions%2C%20the%20convergence%20rate%20of%20NGD%20is%20quadratic.%20Numerical%20experiments%20are%0Aconducted%20to%20verify%20our%20theoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00573v4&entry.124074799=Read"},
{"title": "Explainability of Large Language Models using SMILE: Statistical\n  Model-agnostic Interpretability with Local Explanations", "author": "Zeinab Dehghani and Mohammed Naveed Akram and Koorosh Aslansefat and Adil Khan", "abstract": "  Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.\n", "link": "http://arxiv.org/abs/2505.21657v2", "date": "2025-06-13", "relevancy": 2.3964, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4806}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4786}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainability%20of%20Large%20Language%20Models%20using%20SMILE%3A%20Statistical%0A%20%20Model-agnostic%20Interpretability%20with%20Local%20Explanations&body=Title%3A%20Explainability%20of%20Large%20Language%20Models%20using%20SMILE%3A%20Statistical%0A%20%20Model-agnostic%20Interpretability%20with%20Local%20Explanations%0AAuthor%3A%20Zeinab%20Dehghani%20and%20Mohammed%20Naveed%20Akram%20and%20Koorosh%20Aslansefat%20and%20Adil%20Khan%0AAbstract%3A%20%20%20Large%20language%20models%20like%20GPT%2C%20LLAMA%2C%20and%20Claude%20have%20become%20incredibly%0Apowerful%20at%20generating%20text%2C%20but%20they%20are%20still%20black%20boxes%2C%20so%20it%20is%20hard%20to%0Aunderstand%20how%20they%20decide%20what%20to%20say.%20That%20lack%20of%20transparency%20can%20be%0Aproblematic%2C%20especially%20in%20fields%20where%20trust%20and%20accountability%20matter.%20To%0Ahelp%20with%20this%2C%20we%20introduce%20SMILE%2C%20a%20new%20method%20that%20explains%20how%20these%20models%0Arespond%20to%20different%20parts%20of%20a%20prompt.%20SMILE%20is%20model-agnostic%20and%20works%20by%0Aslightly%20changing%20the%20input%2C%20measuring%20how%20the%20output%20changes%2C%20and%20then%0Ahighlighting%20which%20words%20had%20the%20most%20impact.%20Create%20simple%20visual%20heat%20maps%0Ashowing%20which%20parts%20of%20a%20prompt%20matter%20the%20most.%20We%20tested%20SMILE%20on%20several%0Aleading%20LLMs%20and%20used%20metrics%20such%20as%20accuracy%2C%20consistency%2C%20stability%2C%20and%0Afidelity%20to%20show%20that%20it%20gives%20clear%20and%20reliable%20explanations.%20By%20making%20these%0Amodels%20easier%20to%20understand%2C%20SMILE%20brings%20us%20one%20step%20closer%20to%20making%20AI%20more%0Atransparent%20and%20trustworthy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21657v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainability%2520of%2520Large%2520Language%2520Models%2520using%2520SMILE%253A%2520Statistical%250A%2520%2520Model-agnostic%2520Interpretability%2520with%2520Local%2520Explanations%26entry.906535625%3DZeinab%2520Dehghani%2520and%2520Mohammed%2520Naveed%2520Akram%2520and%2520Koorosh%2520Aslansefat%2520and%2520Adil%2520Khan%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520like%2520GPT%252C%2520LLAMA%252C%2520and%2520Claude%2520have%2520become%2520incredibly%250Apowerful%2520at%2520generating%2520text%252C%2520but%2520they%2520are%2520still%2520black%2520boxes%252C%2520so%2520it%2520is%2520hard%2520to%250Aunderstand%2520how%2520they%2520decide%2520what%2520to%2520say.%2520That%2520lack%2520of%2520transparency%2520can%2520be%250Aproblematic%252C%2520especially%2520in%2520fields%2520where%2520trust%2520and%2520accountability%2520matter.%2520To%250Ahelp%2520with%2520this%252C%2520we%2520introduce%2520SMILE%252C%2520a%2520new%2520method%2520that%2520explains%2520how%2520these%2520models%250Arespond%2520to%2520different%2520parts%2520of%2520a%2520prompt.%2520SMILE%2520is%2520model-agnostic%2520and%2520works%2520by%250Aslightly%2520changing%2520the%2520input%252C%2520measuring%2520how%2520the%2520output%2520changes%252C%2520and%2520then%250Ahighlighting%2520which%2520words%2520had%2520the%2520most%2520impact.%2520Create%2520simple%2520visual%2520heat%2520maps%250Ashowing%2520which%2520parts%2520of%2520a%2520prompt%2520matter%2520the%2520most.%2520We%2520tested%2520SMILE%2520on%2520several%250Aleading%2520LLMs%2520and%2520used%2520metrics%2520such%2520as%2520accuracy%252C%2520consistency%252C%2520stability%252C%2520and%250Afidelity%2520to%2520show%2520that%2520it%2520gives%2520clear%2520and%2520reliable%2520explanations.%2520By%2520making%2520these%250Amodels%2520easier%2520to%2520understand%252C%2520SMILE%2520brings%2520us%2520one%2520step%2520closer%2520to%2520making%2520AI%2520more%250Atransparent%2520and%2520trustworthy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21657v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainability%20of%20Large%20Language%20Models%20using%20SMILE%3A%20Statistical%0A%20%20Model-agnostic%20Interpretability%20with%20Local%20Explanations&entry.906535625=Zeinab%20Dehghani%20and%20Mohammed%20Naveed%20Akram%20and%20Koorosh%20Aslansefat%20and%20Adil%20Khan&entry.1292438233=%20%20Large%20language%20models%20like%20GPT%2C%20LLAMA%2C%20and%20Claude%20have%20become%20incredibly%0Apowerful%20at%20generating%20text%2C%20but%20they%20are%20still%20black%20boxes%2C%20so%20it%20is%20hard%20to%0Aunderstand%20how%20they%20decide%20what%20to%20say.%20That%20lack%20of%20transparency%20can%20be%0Aproblematic%2C%20especially%20in%20fields%20where%20trust%20and%20accountability%20matter.%20To%0Ahelp%20with%20this%2C%20we%20introduce%20SMILE%2C%20a%20new%20method%20that%20explains%20how%20these%20models%0Arespond%20to%20different%20parts%20of%20a%20prompt.%20SMILE%20is%20model-agnostic%20and%20works%20by%0Aslightly%20changing%20the%20input%2C%20measuring%20how%20the%20output%20changes%2C%20and%20then%0Ahighlighting%20which%20words%20had%20the%20most%20impact.%20Create%20simple%20visual%20heat%20maps%0Ashowing%20which%20parts%20of%20a%20prompt%20matter%20the%20most.%20We%20tested%20SMILE%20on%20several%0Aleading%20LLMs%20and%20used%20metrics%20such%20as%20accuracy%2C%20consistency%2C%20stability%2C%20and%0Afidelity%20to%20show%20that%20it%20gives%20clear%20and%20reliable%20explanations.%20By%20making%20these%0Amodels%20easier%20to%20understand%2C%20SMILE%20brings%20us%20one%20step%20closer%20to%20making%20AI%20more%0Atransparent%20and%20trustworthy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21657v2&entry.124074799=Read"},
{"title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study", "author": "Sabri Eyuboglu and Ryan Ehrlich and Simran Arora and Neel Guha and Dylan Zinsley and Emily Liu and Will Tennien and Atri Rudra and James Zou and Azalia Mirhoseini and Christopher Re", "abstract": "  Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.\n", "link": "http://arxiv.org/abs/2506.06266v3", "date": "2025-06-13", "relevancy": 2.376, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cartridges%3A%20Lightweight%20and%20general-purpose%20long%20context%20representations%0A%20%20via%20self-study&body=Title%3A%20Cartridges%3A%20Lightweight%20and%20general-purpose%20long%20context%20representations%0A%20%20via%20self-study%0AAuthor%3A%20Sabri%20Eyuboglu%20and%20Ryan%20Ehrlich%20and%20Simran%20Arora%20and%20Neel%20Guha%20and%20Dylan%20Zinsley%20and%20Emily%20Liu%20and%20Will%20Tennien%20and%20Atri%20Rudra%20and%20James%20Zou%20and%20Azalia%20Mirhoseini%20and%20Christopher%20Re%0AAbstract%3A%20%20%20Large%20language%20models%20are%20often%20used%20to%20answer%20queries%20grounded%20in%20large%20text%0Acorpora%20%28e.g.%20codebases%2C%20legal%20documents%2C%20or%20chat%20histories%29%20by%20placing%20the%0Aentire%20corpus%20in%20the%20context%20window%20and%20leveraging%20in-context%20learning%20%28ICL%29.%0AAlthough%20current%20models%20support%20contexts%20of%20100K-1M%20tokens%2C%20this%20setup%20is%0Acostly%20to%20serve%20because%20the%20memory%20consumption%20of%20the%20KV%20cache%20scales%20with%0Ainput%20length.%20We%20explore%20an%20alternative%3A%20training%20a%20smaller%20KV%20cache%20offline%20on%0Aeach%20corpus.%20At%20inference%20time%2C%20we%20load%20this%20trained%20KV%20cache%2C%20which%20we%20call%20a%0ACartridge%2C%20and%20decode%20a%20response.%20Critically%2C%20the%20cost%20of%20training%20a%20Cartridge%0Acan%20be%20amortized%20across%20all%20the%20queries%20referencing%20the%20same%20corpus.%20However%2C%0Awe%20find%20that%20the%20naive%20approach%20of%20training%20the%20Cartridge%20with%20next-token%0Aprediction%20on%20the%20corpus%20is%20not%20competitive%20with%20ICL.%20Instead%2C%20we%20propose%0Aself-study%2C%20a%20training%20recipe%20in%20which%20we%20generate%20synthetic%20conversations%0Aabout%20the%20corpus%20and%20train%20the%20Cartridge%20with%20a%20context-distillation%20objective.%0AWe%20find%20that%20Cartridges%20trained%20with%20self-study%20replicate%20the%20functionality%20of%0AICL%2C%20while%20being%20significantly%20cheaper%20to%20serve.%20On%20challenging%20long-context%0Abenchmarks%2C%20Cartridges%20trained%20with%20self-study%20match%20ICL%20performance%20while%0Ausing%2038.6x%20less%20memory%20and%20enabling%2026.4x%20higher%20throughput.%20Self-study%20also%0Aextends%20the%20model%27s%20effective%20context%20length%20%28e.g.%20from%20128k%20to%20484k%20tokens%20on%0AMTOB%29%20and%20surprisingly%2C%20leads%20to%20Cartridges%20that%20can%20be%20composed%20at%20inference%0Atime%20without%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06266v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCartridges%253A%2520Lightweight%2520and%2520general-purpose%2520long%2520context%2520representations%250A%2520%2520via%2520self-study%26entry.906535625%3DSabri%2520Eyuboglu%2520and%2520Ryan%2520Ehrlich%2520and%2520Simran%2520Arora%2520and%2520Neel%2520Guha%2520and%2520Dylan%2520Zinsley%2520and%2520Emily%2520Liu%2520and%2520Will%2520Tennien%2520and%2520Atri%2520Rudra%2520and%2520James%2520Zou%2520and%2520Azalia%2520Mirhoseini%2520and%2520Christopher%2520Re%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520are%2520often%2520used%2520to%2520answer%2520queries%2520grounded%2520in%2520large%2520text%250Acorpora%2520%2528e.g.%2520codebases%252C%2520legal%2520documents%252C%2520or%2520chat%2520histories%2529%2520by%2520placing%2520the%250Aentire%2520corpus%2520in%2520the%2520context%2520window%2520and%2520leveraging%2520in-context%2520learning%2520%2528ICL%2529.%250AAlthough%2520current%2520models%2520support%2520contexts%2520of%2520100K-1M%2520tokens%252C%2520this%2520setup%2520is%250Acostly%2520to%2520serve%2520because%2520the%2520memory%2520consumption%2520of%2520the%2520KV%2520cache%2520scales%2520with%250Ainput%2520length.%2520We%2520explore%2520an%2520alternative%253A%2520training%2520a%2520smaller%2520KV%2520cache%2520offline%2520on%250Aeach%2520corpus.%2520At%2520inference%2520time%252C%2520we%2520load%2520this%2520trained%2520KV%2520cache%252C%2520which%2520we%2520call%2520a%250ACartridge%252C%2520and%2520decode%2520a%2520response.%2520Critically%252C%2520the%2520cost%2520of%2520training%2520a%2520Cartridge%250Acan%2520be%2520amortized%2520across%2520all%2520the%2520queries%2520referencing%2520the%2520same%2520corpus.%2520However%252C%250Awe%2520find%2520that%2520the%2520naive%2520approach%2520of%2520training%2520the%2520Cartridge%2520with%2520next-token%250Aprediction%2520on%2520the%2520corpus%2520is%2520not%2520competitive%2520with%2520ICL.%2520Instead%252C%2520we%2520propose%250Aself-study%252C%2520a%2520training%2520recipe%2520in%2520which%2520we%2520generate%2520synthetic%2520conversations%250Aabout%2520the%2520corpus%2520and%2520train%2520the%2520Cartridge%2520with%2520a%2520context-distillation%2520objective.%250AWe%2520find%2520that%2520Cartridges%2520trained%2520with%2520self-study%2520replicate%2520the%2520functionality%2520of%250AICL%252C%2520while%2520being%2520significantly%2520cheaper%2520to%2520serve.%2520On%2520challenging%2520long-context%250Abenchmarks%252C%2520Cartridges%2520trained%2520with%2520self-study%2520match%2520ICL%2520performance%2520while%250Ausing%252038.6x%2520less%2520memory%2520and%2520enabling%252026.4x%2520higher%2520throughput.%2520Self-study%2520also%250Aextends%2520the%2520model%2527s%2520effective%2520context%2520length%2520%2528e.g.%2520from%2520128k%2520to%2520484k%2520tokens%2520on%250AMTOB%2529%2520and%2520surprisingly%252C%2520leads%2520to%2520Cartridges%2520that%2520can%2520be%2520composed%2520at%2520inference%250Atime%2520without%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06266v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cartridges%3A%20Lightweight%20and%20general-purpose%20long%20context%20representations%0A%20%20via%20self-study&entry.906535625=Sabri%20Eyuboglu%20and%20Ryan%20Ehrlich%20and%20Simran%20Arora%20and%20Neel%20Guha%20and%20Dylan%20Zinsley%20and%20Emily%20Liu%20and%20Will%20Tennien%20and%20Atri%20Rudra%20and%20James%20Zou%20and%20Azalia%20Mirhoseini%20and%20Christopher%20Re&entry.1292438233=%20%20Large%20language%20models%20are%20often%20used%20to%20answer%20queries%20grounded%20in%20large%20text%0Acorpora%20%28e.g.%20codebases%2C%20legal%20documents%2C%20or%20chat%20histories%29%20by%20placing%20the%0Aentire%20corpus%20in%20the%20context%20window%20and%20leveraging%20in-context%20learning%20%28ICL%29.%0AAlthough%20current%20models%20support%20contexts%20of%20100K-1M%20tokens%2C%20this%20setup%20is%0Acostly%20to%20serve%20because%20the%20memory%20consumption%20of%20the%20KV%20cache%20scales%20with%0Ainput%20length.%20We%20explore%20an%20alternative%3A%20training%20a%20smaller%20KV%20cache%20offline%20on%0Aeach%20corpus.%20At%20inference%20time%2C%20we%20load%20this%20trained%20KV%20cache%2C%20which%20we%20call%20a%0ACartridge%2C%20and%20decode%20a%20response.%20Critically%2C%20the%20cost%20of%20training%20a%20Cartridge%0Acan%20be%20amortized%20across%20all%20the%20queries%20referencing%20the%20same%20corpus.%20However%2C%0Awe%20find%20that%20the%20naive%20approach%20of%20training%20the%20Cartridge%20with%20next-token%0Aprediction%20on%20the%20corpus%20is%20not%20competitive%20with%20ICL.%20Instead%2C%20we%20propose%0Aself-study%2C%20a%20training%20recipe%20in%20which%20we%20generate%20synthetic%20conversations%0Aabout%20the%20corpus%20and%20train%20the%20Cartridge%20with%20a%20context-distillation%20objective.%0AWe%20find%20that%20Cartridges%20trained%20with%20self-study%20replicate%20the%20functionality%20of%0AICL%2C%20while%20being%20significantly%20cheaper%20to%20serve.%20On%20challenging%20long-context%0Abenchmarks%2C%20Cartridges%20trained%20with%20self-study%20match%20ICL%20performance%20while%0Ausing%2038.6x%20less%20memory%20and%20enabling%2026.4x%20higher%20throughput.%20Self-study%20also%0Aextends%20the%20model%27s%20effective%20context%20length%20%28e.g.%20from%20128k%20to%20484k%20tokens%20on%0AMTOB%29%20and%20surprisingly%2C%20leads%20to%20Cartridges%20that%20can%20be%20composed%20at%20inference%0Atime%20without%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06266v3&entry.124074799=Read"},
{"title": "Spectral Estimation with Free Decompression", "author": "Siavash Ameli and Chris van der Heide and Liam Hodgkinson and Michael W. Mahoney", "abstract": "  Computing eigenvalues of very large matrices is a critical task in many\nmachine learning applications, including the evaluation of log-determinants,\nthe trace of matrix functions, and other important metrics. As datasets\ncontinue to grow in scale, the corresponding covariance and kernel matrices\nbecome increasingly large, often reaching magnitudes that make their direct\nformation impractical or impossible. Existing techniques typically rely on\nmatrix-vector products, which can provide efficient approximations, if the\nmatrix spectrum behaves well. However, in settings like distributed learning,\nor when the matrix is defined only indirectly, access to the full data set can\nbe restricted to only very small sub-matrices of the original matrix. In these\ncases, the matrix of nominal interest is not even available as an implicit\noperator, meaning that even matrix-vector products may not be available. In\nsuch settings, the matrix is \"impalpable,\" in the sense that we have access to\nonly masked snapshots of it. We draw on principles from free probability theory\nto introduce a novel method of \"free decompression\" to estimate the spectrum of\nsuch matrices. Our method can be used to extrapolate from the empirical\nspectral densities of small submatrices to infer the eigenspectrum of extremely\nlarge (impalpable) matrices (that we cannot form or even evaluate with full\nmatrix-vector products). We demonstrate the effectiveness of this approach\nthrough a series of examples, comparing its performance against known limiting\ndistributions from random matrix theory in synthetic settings, as well as\napplying it to submatrices of real-world datasets, matching them with their\nfull empirical eigenspectra.\n", "link": "http://arxiv.org/abs/2506.11994v1", "date": "2025-06-13", "relevancy": 2.373, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4817}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4721}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Estimation%20with%20Free%20Decompression&body=Title%3A%20Spectral%20Estimation%20with%20Free%20Decompression%0AAuthor%3A%20Siavash%20Ameli%20and%20Chris%20van%20der%20Heide%20and%20Liam%20Hodgkinson%20and%20Michael%20W.%20Mahoney%0AAbstract%3A%20%20%20Computing%20eigenvalues%20of%20very%20large%20matrices%20is%20a%20critical%20task%20in%20many%0Amachine%20learning%20applications%2C%20including%20the%20evaluation%20of%20log-determinants%2C%0Athe%20trace%20of%20matrix%20functions%2C%20and%20other%20important%20metrics.%20As%20datasets%0Acontinue%20to%20grow%20in%20scale%2C%20the%20corresponding%20covariance%20and%20kernel%20matrices%0Abecome%20increasingly%20large%2C%20often%20reaching%20magnitudes%20that%20make%20their%20direct%0Aformation%20impractical%20or%20impossible.%20Existing%20techniques%20typically%20rely%20on%0Amatrix-vector%20products%2C%20which%20can%20provide%20efficient%20approximations%2C%20if%20the%0Amatrix%20spectrum%20behaves%20well.%20However%2C%20in%20settings%20like%20distributed%20learning%2C%0Aor%20when%20the%20matrix%20is%20defined%20only%20indirectly%2C%20access%20to%20the%20full%20data%20set%20can%0Abe%20restricted%20to%20only%20very%20small%20sub-matrices%20of%20the%20original%20matrix.%20In%20these%0Acases%2C%20the%20matrix%20of%20nominal%20interest%20is%20not%20even%20available%20as%20an%20implicit%0Aoperator%2C%20meaning%20that%20even%20matrix-vector%20products%20may%20not%20be%20available.%20In%0Asuch%20settings%2C%20the%20matrix%20is%20%22impalpable%2C%22%20in%20the%20sense%20that%20we%20have%20access%20to%0Aonly%20masked%20snapshots%20of%20it.%20We%20draw%20on%20principles%20from%20free%20probability%20theory%0Ato%20introduce%20a%20novel%20method%20of%20%22free%20decompression%22%20to%20estimate%20the%20spectrum%20of%0Asuch%20matrices.%20Our%20method%20can%20be%20used%20to%20extrapolate%20from%20the%20empirical%0Aspectral%20densities%20of%20small%20submatrices%20to%20infer%20the%20eigenspectrum%20of%20extremely%0Alarge%20%28impalpable%29%20matrices%20%28that%20we%20cannot%20form%20or%20even%20evaluate%20with%20full%0Amatrix-vector%20products%29.%20We%20demonstrate%20the%20effectiveness%20of%20this%20approach%0Athrough%20a%20series%20of%20examples%2C%20comparing%20its%20performance%20against%20known%20limiting%0Adistributions%20from%20random%20matrix%20theory%20in%20synthetic%20settings%2C%20as%20well%20as%0Aapplying%20it%20to%20submatrices%20of%20real-world%20datasets%2C%20matching%20them%20with%20their%0Afull%20empirical%20eigenspectra.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Estimation%2520with%2520Free%2520Decompression%26entry.906535625%3DSiavash%2520Ameli%2520and%2520Chris%2520van%2520der%2520Heide%2520and%2520Liam%2520Hodgkinson%2520and%2520Michael%2520W.%2520Mahoney%26entry.1292438233%3D%2520%2520Computing%2520eigenvalues%2520of%2520very%2520large%2520matrices%2520is%2520a%2520critical%2520task%2520in%2520many%250Amachine%2520learning%2520applications%252C%2520including%2520the%2520evaluation%2520of%2520log-determinants%252C%250Athe%2520trace%2520of%2520matrix%2520functions%252C%2520and%2520other%2520important%2520metrics.%2520As%2520datasets%250Acontinue%2520to%2520grow%2520in%2520scale%252C%2520the%2520corresponding%2520covariance%2520and%2520kernel%2520matrices%250Abecome%2520increasingly%2520large%252C%2520often%2520reaching%2520magnitudes%2520that%2520make%2520their%2520direct%250Aformation%2520impractical%2520or%2520impossible.%2520Existing%2520techniques%2520typically%2520rely%2520on%250Amatrix-vector%2520products%252C%2520which%2520can%2520provide%2520efficient%2520approximations%252C%2520if%2520the%250Amatrix%2520spectrum%2520behaves%2520well.%2520However%252C%2520in%2520settings%2520like%2520distributed%2520learning%252C%250Aor%2520when%2520the%2520matrix%2520is%2520defined%2520only%2520indirectly%252C%2520access%2520to%2520the%2520full%2520data%2520set%2520can%250Abe%2520restricted%2520to%2520only%2520very%2520small%2520sub-matrices%2520of%2520the%2520original%2520matrix.%2520In%2520these%250Acases%252C%2520the%2520matrix%2520of%2520nominal%2520interest%2520is%2520not%2520even%2520available%2520as%2520an%2520implicit%250Aoperator%252C%2520meaning%2520that%2520even%2520matrix-vector%2520products%2520may%2520not%2520be%2520available.%2520In%250Asuch%2520settings%252C%2520the%2520matrix%2520is%2520%2522impalpable%252C%2522%2520in%2520the%2520sense%2520that%2520we%2520have%2520access%2520to%250Aonly%2520masked%2520snapshots%2520of%2520it.%2520We%2520draw%2520on%2520principles%2520from%2520free%2520probability%2520theory%250Ato%2520introduce%2520a%2520novel%2520method%2520of%2520%2522free%2520decompression%2522%2520to%2520estimate%2520the%2520spectrum%2520of%250Asuch%2520matrices.%2520Our%2520method%2520can%2520be%2520used%2520to%2520extrapolate%2520from%2520the%2520empirical%250Aspectral%2520densities%2520of%2520small%2520submatrices%2520to%2520infer%2520the%2520eigenspectrum%2520of%2520extremely%250Alarge%2520%2528impalpable%2529%2520matrices%2520%2528that%2520we%2520cannot%2520form%2520or%2520even%2520evaluate%2520with%2520full%250Amatrix-vector%2520products%2529.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520approach%250Athrough%2520a%2520series%2520of%2520examples%252C%2520comparing%2520its%2520performance%2520against%2520known%2520limiting%250Adistributions%2520from%2520random%2520matrix%2520theory%2520in%2520synthetic%2520settings%252C%2520as%2520well%2520as%250Aapplying%2520it%2520to%2520submatrices%2520of%2520real-world%2520datasets%252C%2520matching%2520them%2520with%2520their%250Afull%2520empirical%2520eigenspectra.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Estimation%20with%20Free%20Decompression&entry.906535625=Siavash%20Ameli%20and%20Chris%20van%20der%20Heide%20and%20Liam%20Hodgkinson%20and%20Michael%20W.%20Mahoney&entry.1292438233=%20%20Computing%20eigenvalues%20of%20very%20large%20matrices%20is%20a%20critical%20task%20in%20many%0Amachine%20learning%20applications%2C%20including%20the%20evaluation%20of%20log-determinants%2C%0Athe%20trace%20of%20matrix%20functions%2C%20and%20other%20important%20metrics.%20As%20datasets%0Acontinue%20to%20grow%20in%20scale%2C%20the%20corresponding%20covariance%20and%20kernel%20matrices%0Abecome%20increasingly%20large%2C%20often%20reaching%20magnitudes%20that%20make%20their%20direct%0Aformation%20impractical%20or%20impossible.%20Existing%20techniques%20typically%20rely%20on%0Amatrix-vector%20products%2C%20which%20can%20provide%20efficient%20approximations%2C%20if%20the%0Amatrix%20spectrum%20behaves%20well.%20However%2C%20in%20settings%20like%20distributed%20learning%2C%0Aor%20when%20the%20matrix%20is%20defined%20only%20indirectly%2C%20access%20to%20the%20full%20data%20set%20can%0Abe%20restricted%20to%20only%20very%20small%20sub-matrices%20of%20the%20original%20matrix.%20In%20these%0Acases%2C%20the%20matrix%20of%20nominal%20interest%20is%20not%20even%20available%20as%20an%20implicit%0Aoperator%2C%20meaning%20that%20even%20matrix-vector%20products%20may%20not%20be%20available.%20In%0Asuch%20settings%2C%20the%20matrix%20is%20%22impalpable%2C%22%20in%20the%20sense%20that%20we%20have%20access%20to%0Aonly%20masked%20snapshots%20of%20it.%20We%20draw%20on%20principles%20from%20free%20probability%20theory%0Ato%20introduce%20a%20novel%20method%20of%20%22free%20decompression%22%20to%20estimate%20the%20spectrum%20of%0Asuch%20matrices.%20Our%20method%20can%20be%20used%20to%20extrapolate%20from%20the%20empirical%0Aspectral%20densities%20of%20small%20submatrices%20to%20infer%20the%20eigenspectrum%20of%20extremely%0Alarge%20%28impalpable%29%20matrices%20%28that%20we%20cannot%20form%20or%20even%20evaluate%20with%20full%0Amatrix-vector%20products%29.%20We%20demonstrate%20the%20effectiveness%20of%20this%20approach%0Athrough%20a%20series%20of%20examples%2C%20comparing%20its%20performance%20against%20known%20limiting%0Adistributions%20from%20random%20matrix%20theory%20in%20synthetic%20settings%2C%20as%20well%20as%0Aapplying%20it%20to%20submatrices%20of%20real-world%20datasets%2C%20matching%20them%20with%20their%0Afull%20empirical%20eigenspectra.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11994v1&entry.124074799=Read"},
{"title": "Learning Overspecified Gaussian Mixtures Exponentially Fast with the EM\n  Algorithm", "author": "Zhenisbek Assylbekov and Alan Legg and Artur Pak", "abstract": "  We investigate the convergence properties of the EM algorithm when applied to\noverspecified Gaussian mixture models -- that is, when the number of components\nin the fitted model exceeds that of the true underlying distribution. Focusing\non a structured configuration where the component means are positioned at the\nvertices of a regular simplex and the mixture weights satisfy a non-degeneracy\ncondition, we demonstrate that the population EM algorithm converges\nexponentially fast in terms of the Kullback-Leibler (KL) distance. Our analysis\nleverages the strong convexity of the negative log-likelihood function in a\nneighborhood around the optimum and utilizes the Polyak-{\\L}ojasiewicz\ninequality to establish that an $\\epsilon$-accurate approximation is achievable\nin $O(\\log(1/\\epsilon))$ iterations. Furthermore, we extend these results to a\nfinite-sample setting by deriving explicit statistical convergence guarantees.\nNumerical experiments on synthetic datasets corroborate our theoretical\nfindings, highlighting the dramatic acceleration in convergence compared to\nconventional sublinear rates. This work not only deepens the understanding of\nEM's behavior in overspecified settings but also offers practical insights into\ninitialization strategies and model design for high-dimensional clustering and\ndensity estimation tasks.\n", "link": "http://arxiv.org/abs/2506.11850v1", "date": "2025-06-13", "relevancy": 2.3447, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4827}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4745}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Overspecified%20Gaussian%20Mixtures%20Exponentially%20Fast%20with%20the%20EM%0A%20%20Algorithm&body=Title%3A%20Learning%20Overspecified%20Gaussian%20Mixtures%20Exponentially%20Fast%20with%20the%20EM%0A%20%20Algorithm%0AAuthor%3A%20Zhenisbek%20Assylbekov%20and%20Alan%20Legg%20and%20Artur%20Pak%0AAbstract%3A%20%20%20We%20investigate%20the%20convergence%20properties%20of%20the%20EM%20algorithm%20when%20applied%20to%0Aoverspecified%20Gaussian%20mixture%20models%20--%20that%20is%2C%20when%20the%20number%20of%20components%0Ain%20the%20fitted%20model%20exceeds%20that%20of%20the%20true%20underlying%20distribution.%20Focusing%0Aon%20a%20structured%20configuration%20where%20the%20component%20means%20are%20positioned%20at%20the%0Avertices%20of%20a%20regular%20simplex%20and%20the%20mixture%20weights%20satisfy%20a%20non-degeneracy%0Acondition%2C%20we%20demonstrate%20that%20the%20population%20EM%20algorithm%20converges%0Aexponentially%20fast%20in%20terms%20of%20the%20Kullback-Leibler%20%28KL%29%20distance.%20Our%20analysis%0Aleverages%20the%20strong%20convexity%20of%20the%20negative%20log-likelihood%20function%20in%20a%0Aneighborhood%20around%20the%20optimum%20and%20utilizes%20the%20Polyak-%7B%5CL%7Dojasiewicz%0Ainequality%20to%20establish%20that%20an%20%24%5Cepsilon%24-accurate%20approximation%20is%20achievable%0Ain%20%24O%28%5Clog%281/%5Cepsilon%29%29%24%20iterations.%20Furthermore%2C%20we%20extend%20these%20results%20to%20a%0Afinite-sample%20setting%20by%20deriving%20explicit%20statistical%20convergence%20guarantees.%0ANumerical%20experiments%20on%20synthetic%20datasets%20corroborate%20our%20theoretical%0Afindings%2C%20highlighting%20the%20dramatic%20acceleration%20in%20convergence%20compared%20to%0Aconventional%20sublinear%20rates.%20This%20work%20not%20only%20deepens%20the%20understanding%20of%0AEM%27s%20behavior%20in%20overspecified%20settings%20but%20also%20offers%20practical%20insights%20into%0Ainitialization%20strategies%20and%20model%20design%20for%20high-dimensional%20clustering%20and%0Adensity%20estimation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Overspecified%2520Gaussian%2520Mixtures%2520Exponentially%2520Fast%2520with%2520the%2520EM%250A%2520%2520Algorithm%26entry.906535625%3DZhenisbek%2520Assylbekov%2520and%2520Alan%2520Legg%2520and%2520Artur%2520Pak%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520convergence%2520properties%2520of%2520the%2520EM%2520algorithm%2520when%2520applied%2520to%250Aoverspecified%2520Gaussian%2520mixture%2520models%2520--%2520that%2520is%252C%2520when%2520the%2520number%2520of%2520components%250Ain%2520the%2520fitted%2520model%2520exceeds%2520that%2520of%2520the%2520true%2520underlying%2520distribution.%2520Focusing%250Aon%2520a%2520structured%2520configuration%2520where%2520the%2520component%2520means%2520are%2520positioned%2520at%2520the%250Avertices%2520of%2520a%2520regular%2520simplex%2520and%2520the%2520mixture%2520weights%2520satisfy%2520a%2520non-degeneracy%250Acondition%252C%2520we%2520demonstrate%2520that%2520the%2520population%2520EM%2520algorithm%2520converges%250Aexponentially%2520fast%2520in%2520terms%2520of%2520the%2520Kullback-Leibler%2520%2528KL%2529%2520distance.%2520Our%2520analysis%250Aleverages%2520the%2520strong%2520convexity%2520of%2520the%2520negative%2520log-likelihood%2520function%2520in%2520a%250Aneighborhood%2520around%2520the%2520optimum%2520and%2520utilizes%2520the%2520Polyak-%257B%255CL%257Dojasiewicz%250Ainequality%2520to%2520establish%2520that%2520an%2520%2524%255Cepsilon%2524-accurate%2520approximation%2520is%2520achievable%250Ain%2520%2524O%2528%255Clog%25281/%255Cepsilon%2529%2529%2524%2520iterations.%2520Furthermore%252C%2520we%2520extend%2520these%2520results%2520to%2520a%250Afinite-sample%2520setting%2520by%2520deriving%2520explicit%2520statistical%2520convergence%2520guarantees.%250ANumerical%2520experiments%2520on%2520synthetic%2520datasets%2520corroborate%2520our%2520theoretical%250Afindings%252C%2520highlighting%2520the%2520dramatic%2520acceleration%2520in%2520convergence%2520compared%2520to%250Aconventional%2520sublinear%2520rates.%2520This%2520work%2520not%2520only%2520deepens%2520the%2520understanding%2520of%250AEM%2527s%2520behavior%2520in%2520overspecified%2520settings%2520but%2520also%2520offers%2520practical%2520insights%2520into%250Ainitialization%2520strategies%2520and%2520model%2520design%2520for%2520high-dimensional%2520clustering%2520and%250Adensity%2520estimation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Overspecified%20Gaussian%20Mixtures%20Exponentially%20Fast%20with%20the%20EM%0A%20%20Algorithm&entry.906535625=Zhenisbek%20Assylbekov%20and%20Alan%20Legg%20and%20Artur%20Pak&entry.1292438233=%20%20We%20investigate%20the%20convergence%20properties%20of%20the%20EM%20algorithm%20when%20applied%20to%0Aoverspecified%20Gaussian%20mixture%20models%20--%20that%20is%2C%20when%20the%20number%20of%20components%0Ain%20the%20fitted%20model%20exceeds%20that%20of%20the%20true%20underlying%20distribution.%20Focusing%0Aon%20a%20structured%20configuration%20where%20the%20component%20means%20are%20positioned%20at%20the%0Avertices%20of%20a%20regular%20simplex%20and%20the%20mixture%20weights%20satisfy%20a%20non-degeneracy%0Acondition%2C%20we%20demonstrate%20that%20the%20population%20EM%20algorithm%20converges%0Aexponentially%20fast%20in%20terms%20of%20the%20Kullback-Leibler%20%28KL%29%20distance.%20Our%20analysis%0Aleverages%20the%20strong%20convexity%20of%20the%20negative%20log-likelihood%20function%20in%20a%0Aneighborhood%20around%20the%20optimum%20and%20utilizes%20the%20Polyak-%7B%5CL%7Dojasiewicz%0Ainequality%20to%20establish%20that%20an%20%24%5Cepsilon%24-accurate%20approximation%20is%20achievable%0Ain%20%24O%28%5Clog%281/%5Cepsilon%29%29%24%20iterations.%20Furthermore%2C%20we%20extend%20these%20results%20to%20a%0Afinite-sample%20setting%20by%20deriving%20explicit%20statistical%20convergence%20guarantees.%0ANumerical%20experiments%20on%20synthetic%20datasets%20corroborate%20our%20theoretical%0Afindings%2C%20highlighting%20the%20dramatic%20acceleration%20in%20convergence%20compared%20to%0Aconventional%20sublinear%20rates.%20This%20work%20not%20only%20deepens%20the%20understanding%20of%0AEM%27s%20behavior%20in%20overspecified%20settings%20but%20also%20offers%20practical%20insights%20into%0Ainitialization%20strategies%20and%20model%20design%20for%20high-dimensional%20clustering%20and%0Adensity%20estimation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11850v1&entry.124074799=Read"},
{"title": "Automated Treatment Planning for Interstitial HDR Brachytherapy for\n  Locally Advanced Cervical Cancer using Deep Reinforcement Learning", "author": "Mohammadamin Moradi and Runyu Jiang and Yingzi Liu and Malvern Madondo and Tianming Wu and James J. Sohn and Xiaofeng Yang and Yasmin Hasan and Zhen Tian", "abstract": "  High-dose-rate (HDR) brachytherapy plays a critical role in the treatment of\nlocally advanced cervical cancer but remains highly dependent on manual\ntreatment planning expertise. The objective of this study is to develop a fully\nautomated HDR brachytherapy planning framework that integrates reinforcement\nlearning (RL) and dose-based optimization to generate clinically acceptable\ntreatment plans with improved consistency and efficiency. We propose a\nhierarchical two-stage autoplanning framework. In the first stage, a deep\nQ-network (DQN)-based RL agent iteratively selects treatment planning\nparameters (TPPs), which control the trade-offs between target coverage and\norgan-at-risk (OAR) sparing. The agent's state representation includes both\ndose-volume histogram (DVH) metrics and current TPP values, while its reward\nfunction incorporates clinical dose objectives and safety constraints,\nincluding D90, V150, V200 for targets, and D2cc for all relevant OARs (bladder,\nrectum, sigmoid, small bowel, and large bowel). In the second stage, a\ncustomized Adam-based optimizer computes the corresponding dwell time\ndistribution for the selected TPPs using a clinically informed loss function.\nThe framework was evaluated on a cohort of patients with complex applicator\ngeometries. The proposed framework successfully learned clinically meaningful\nTPP adjustments across diverse patient anatomies. For the unseen test patients,\nthe RL-based automated planning method achieved an average score of 93.89%,\noutperforming the clinical plans which averaged 91.86%. These findings are\nnotable given that score improvements were achieved while maintaining full\ntarget coverage and reducing CTV hot spots in most cases.\n", "link": "http://arxiv.org/abs/2506.11957v1", "date": "2025-06-13", "relevancy": 2.336, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4812}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4615}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Treatment%20Planning%20for%20Interstitial%20HDR%20Brachytherapy%20for%0A%20%20Locally%20Advanced%20Cervical%20Cancer%20using%20Deep%20Reinforcement%20Learning&body=Title%3A%20Automated%20Treatment%20Planning%20for%20Interstitial%20HDR%20Brachytherapy%20for%0A%20%20Locally%20Advanced%20Cervical%20Cancer%20using%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Mohammadamin%20Moradi%20and%20Runyu%20Jiang%20and%20Yingzi%20Liu%20and%20Malvern%20Madondo%20and%20Tianming%20Wu%20and%20James%20J.%20Sohn%20and%20Xiaofeng%20Yang%20and%20Yasmin%20Hasan%20and%20Zhen%20Tian%0AAbstract%3A%20%20%20High-dose-rate%20%28HDR%29%20brachytherapy%20plays%20a%20critical%20role%20in%20the%20treatment%20of%0Alocally%20advanced%20cervical%20cancer%20but%20remains%20highly%20dependent%20on%20manual%0Atreatment%20planning%20expertise.%20The%20objective%20of%20this%20study%20is%20to%20develop%20a%20fully%0Aautomated%20HDR%20brachytherapy%20planning%20framework%20that%20integrates%20reinforcement%0Alearning%20%28RL%29%20and%20dose-based%20optimization%20to%20generate%20clinically%20acceptable%0Atreatment%20plans%20with%20improved%20consistency%20and%20efficiency.%20We%20propose%20a%0Ahierarchical%20two-stage%20autoplanning%20framework.%20In%20the%20first%20stage%2C%20a%20deep%0AQ-network%20%28DQN%29-based%20RL%20agent%20iteratively%20selects%20treatment%20planning%0Aparameters%20%28TPPs%29%2C%20which%20control%20the%20trade-offs%20between%20target%20coverage%20and%0Aorgan-at-risk%20%28OAR%29%20sparing.%20The%20agent%27s%20state%20representation%20includes%20both%0Adose-volume%20histogram%20%28DVH%29%20metrics%20and%20current%20TPP%20values%2C%20while%20its%20reward%0Afunction%20incorporates%20clinical%20dose%20objectives%20and%20safety%20constraints%2C%0Aincluding%20D90%2C%20V150%2C%20V200%20for%20targets%2C%20and%20D2cc%20for%20all%20relevant%20OARs%20%28bladder%2C%0Arectum%2C%20sigmoid%2C%20small%20bowel%2C%20and%20large%20bowel%29.%20In%20the%20second%20stage%2C%20a%0Acustomized%20Adam-based%20optimizer%20computes%20the%20corresponding%20dwell%20time%0Adistribution%20for%20the%20selected%20TPPs%20using%20a%20clinically%20informed%20loss%20function.%0AThe%20framework%20was%20evaluated%20on%20a%20cohort%20of%20patients%20with%20complex%20applicator%0Ageometries.%20The%20proposed%20framework%20successfully%20learned%20clinically%20meaningful%0ATPP%20adjustments%20across%20diverse%20patient%20anatomies.%20For%20the%20unseen%20test%20patients%2C%0Athe%20RL-based%20automated%20planning%20method%20achieved%20an%20average%20score%20of%2093.89%25%2C%0Aoutperforming%20the%20clinical%20plans%20which%20averaged%2091.86%25.%20These%20findings%20are%0Anotable%20given%20that%20score%20improvements%20were%20achieved%20while%20maintaining%20full%0Atarget%20coverage%20and%20reducing%20CTV%20hot%20spots%20in%20most%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Treatment%2520Planning%2520for%2520Interstitial%2520HDR%2520Brachytherapy%2520for%250A%2520%2520Locally%2520Advanced%2520Cervical%2520Cancer%2520using%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DMohammadamin%2520Moradi%2520and%2520Runyu%2520Jiang%2520and%2520Yingzi%2520Liu%2520and%2520Malvern%2520Madondo%2520and%2520Tianming%2520Wu%2520and%2520James%2520J.%2520Sohn%2520and%2520Xiaofeng%2520Yang%2520and%2520Yasmin%2520Hasan%2520and%2520Zhen%2520Tian%26entry.1292438233%3D%2520%2520High-dose-rate%2520%2528HDR%2529%2520brachytherapy%2520plays%2520a%2520critical%2520role%2520in%2520the%2520treatment%2520of%250Alocally%2520advanced%2520cervical%2520cancer%2520but%2520remains%2520highly%2520dependent%2520on%2520manual%250Atreatment%2520planning%2520expertise.%2520The%2520objective%2520of%2520this%2520study%2520is%2520to%2520develop%2520a%2520fully%250Aautomated%2520HDR%2520brachytherapy%2520planning%2520framework%2520that%2520integrates%2520reinforcement%250Alearning%2520%2528RL%2529%2520and%2520dose-based%2520optimization%2520to%2520generate%2520clinically%2520acceptable%250Atreatment%2520plans%2520with%2520improved%2520consistency%2520and%2520efficiency.%2520We%2520propose%2520a%250Ahierarchical%2520two-stage%2520autoplanning%2520framework.%2520In%2520the%2520first%2520stage%252C%2520a%2520deep%250AQ-network%2520%2528DQN%2529-based%2520RL%2520agent%2520iteratively%2520selects%2520treatment%2520planning%250Aparameters%2520%2528TPPs%2529%252C%2520which%2520control%2520the%2520trade-offs%2520between%2520target%2520coverage%2520and%250Aorgan-at-risk%2520%2528OAR%2529%2520sparing.%2520The%2520agent%2527s%2520state%2520representation%2520includes%2520both%250Adose-volume%2520histogram%2520%2528DVH%2529%2520metrics%2520and%2520current%2520TPP%2520values%252C%2520while%2520its%2520reward%250Afunction%2520incorporates%2520clinical%2520dose%2520objectives%2520and%2520safety%2520constraints%252C%250Aincluding%2520D90%252C%2520V150%252C%2520V200%2520for%2520targets%252C%2520and%2520D2cc%2520for%2520all%2520relevant%2520OARs%2520%2528bladder%252C%250Arectum%252C%2520sigmoid%252C%2520small%2520bowel%252C%2520and%2520large%2520bowel%2529.%2520In%2520the%2520second%2520stage%252C%2520a%250Acustomized%2520Adam-based%2520optimizer%2520computes%2520the%2520corresponding%2520dwell%2520time%250Adistribution%2520for%2520the%2520selected%2520TPPs%2520using%2520a%2520clinically%2520informed%2520loss%2520function.%250AThe%2520framework%2520was%2520evaluated%2520on%2520a%2520cohort%2520of%2520patients%2520with%2520complex%2520applicator%250Ageometries.%2520The%2520proposed%2520framework%2520successfully%2520learned%2520clinically%2520meaningful%250ATPP%2520adjustments%2520across%2520diverse%2520patient%2520anatomies.%2520For%2520the%2520unseen%2520test%2520patients%252C%250Athe%2520RL-based%2520automated%2520planning%2520method%2520achieved%2520an%2520average%2520score%2520of%252093.89%2525%252C%250Aoutperforming%2520the%2520clinical%2520plans%2520which%2520averaged%252091.86%2525.%2520These%2520findings%2520are%250Anotable%2520given%2520that%2520score%2520improvements%2520were%2520achieved%2520while%2520maintaining%2520full%250Atarget%2520coverage%2520and%2520reducing%2520CTV%2520hot%2520spots%2520in%2520most%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Treatment%20Planning%20for%20Interstitial%20HDR%20Brachytherapy%20for%0A%20%20Locally%20Advanced%20Cervical%20Cancer%20using%20Deep%20Reinforcement%20Learning&entry.906535625=Mohammadamin%20Moradi%20and%20Runyu%20Jiang%20and%20Yingzi%20Liu%20and%20Malvern%20Madondo%20and%20Tianming%20Wu%20and%20James%20J.%20Sohn%20and%20Xiaofeng%20Yang%20and%20Yasmin%20Hasan%20and%20Zhen%20Tian&entry.1292438233=%20%20High-dose-rate%20%28HDR%29%20brachytherapy%20plays%20a%20critical%20role%20in%20the%20treatment%20of%0Alocally%20advanced%20cervical%20cancer%20but%20remains%20highly%20dependent%20on%20manual%0Atreatment%20planning%20expertise.%20The%20objective%20of%20this%20study%20is%20to%20develop%20a%20fully%0Aautomated%20HDR%20brachytherapy%20planning%20framework%20that%20integrates%20reinforcement%0Alearning%20%28RL%29%20and%20dose-based%20optimization%20to%20generate%20clinically%20acceptable%0Atreatment%20plans%20with%20improved%20consistency%20and%20efficiency.%20We%20propose%20a%0Ahierarchical%20two-stage%20autoplanning%20framework.%20In%20the%20first%20stage%2C%20a%20deep%0AQ-network%20%28DQN%29-based%20RL%20agent%20iteratively%20selects%20treatment%20planning%0Aparameters%20%28TPPs%29%2C%20which%20control%20the%20trade-offs%20between%20target%20coverage%20and%0Aorgan-at-risk%20%28OAR%29%20sparing.%20The%20agent%27s%20state%20representation%20includes%20both%0Adose-volume%20histogram%20%28DVH%29%20metrics%20and%20current%20TPP%20values%2C%20while%20its%20reward%0Afunction%20incorporates%20clinical%20dose%20objectives%20and%20safety%20constraints%2C%0Aincluding%20D90%2C%20V150%2C%20V200%20for%20targets%2C%20and%20D2cc%20for%20all%20relevant%20OARs%20%28bladder%2C%0Arectum%2C%20sigmoid%2C%20small%20bowel%2C%20and%20large%20bowel%29.%20In%20the%20second%20stage%2C%20a%0Acustomized%20Adam-based%20optimizer%20computes%20the%20corresponding%20dwell%20time%0Adistribution%20for%20the%20selected%20TPPs%20using%20a%20clinically%20informed%20loss%20function.%0AThe%20framework%20was%20evaluated%20on%20a%20cohort%20of%20patients%20with%20complex%20applicator%0Ageometries.%20The%20proposed%20framework%20successfully%20learned%20clinically%20meaningful%0ATPP%20adjustments%20across%20diverse%20patient%20anatomies.%20For%20the%20unseen%20test%20patients%2C%0Athe%20RL-based%20automated%20planning%20method%20achieved%20an%20average%20score%20of%2093.89%25%2C%0Aoutperforming%20the%20clinical%20plans%20which%20averaged%2091.86%25.%20These%20findings%20are%0Anotable%20given%20that%20score%20improvements%20were%20achieved%20while%20maintaining%20full%0Atarget%20coverage%20and%20reducing%20CTV%20hot%20spots%20in%20most%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11957v1&entry.124074799=Read"},
{"title": "VGR: Visual Grounded Reasoning", "author": "Jiacong Wang and Zijiang Kang and Haochen Wang and Haiyong Jiang and Jiawen Li and Bohong Wu and Ya Wang and Jiao Ran and Xiao Liang and Chao Feng and Jun Xiao", "abstract": "  In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.\n", "link": "http://arxiv.org/abs/2506.11991v1", "date": "2025-06-13", "relevancy": 2.3324, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5907}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VGR%3A%20Visual%20Grounded%20Reasoning&body=Title%3A%20VGR%3A%20Visual%20Grounded%20Reasoning%0AAuthor%3A%20Jiacong%20Wang%20and%20Zijiang%20Kang%20and%20Haochen%20Wang%20and%20Haiyong%20Jiang%20and%20Jiawen%20Li%20and%20Bohong%20Wu%20and%20Ya%20Wang%20and%20Jiao%20Ran%20and%20Xiao%20Liang%20and%20Chao%20Feng%20and%20Jun%20Xiao%0AAbstract%3A%20%20%20In%20the%20field%20of%20multimodal%20chain-of-thought%20%28CoT%29%20reasoning%2C%20existing%0Aapproaches%20predominantly%20rely%20on%20reasoning%20on%20pure%20language%20space%2C%20which%0Ainherently%20suffers%20from%20language%20bias%20and%20is%20largely%20confined%20to%20math%20or%0Ascience%20domains.%20This%20narrow%20focus%20limits%20their%20ability%20to%20handle%20complex%0Avisual%20reasoning%20tasks%20that%20demand%20comprehensive%20understanding%20of%20image%0Adetails.%20To%20address%20these%20limitations%2C%20this%20paper%20introduces%20VGR%2C%20a%20novel%0Areasoning%20multimodal%20large%20language%20model%20%28MLLM%29%20with%20enhanced%20fine-grained%0Avisual%20perception%20capabilities.%20Unlike%20traditional%20MLLMs%20that%20answer%20the%0Aquestion%20or%20reasoning%20solely%20on%20the%20language%20space%2C%20our%20VGR%20first%20detects%0Arelevant%20regions%20that%20may%20help%20to%20solve%20problems%2C%20and%20then%20provides%20precise%0Aanswers%20based%20on%20replayed%20image%20regions.%20To%20achieve%20this%2C%20we%20conduct%20a%0Alarge-scale%20SFT%20dataset%20called%20VGR%20-SFT%20that%20contains%20reasoning%20data%20with%20mixed%0Avision%20grounding%20and%20language%20deduction.%20The%20inference%20pipeline%20of%20VGR%20allows%0Athe%20model%20to%20choose%20bounding%20boxes%20for%20visual%20reference%20and%20a%20replay%20stage%20is%0Aintroduced%20to%20integrates%20the%20corresponding%20regions%20into%20the%20reasoning%20process%2C%0Aenhancing%20multimodel%20comprehension.%20Experiments%20on%20the%20LLaVA-NeXT-7B%20baseline%0Ashow%20that%20VGR%20achieves%20superior%20performance%20on%20multi-modal%20benchmarks%20requiring%0Acomprehensive%20image%20detail%20understanding.%20Compared%20to%20the%20baseline%2C%20VGR%20uses%0Aonly%2030%5C%25%20of%20the%20image%20token%20count%20while%20delivering%20scores%20of%20%2B4.1%20on%20MMStar%2C%0A%2B7.1%20on%20AI2D%2C%20and%20a%20%2B12.9%20improvement%20on%20ChartQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVGR%253A%2520Visual%2520Grounded%2520Reasoning%26entry.906535625%3DJiacong%2520Wang%2520and%2520Zijiang%2520Kang%2520and%2520Haochen%2520Wang%2520and%2520Haiyong%2520Jiang%2520and%2520Jiawen%2520Li%2520and%2520Bohong%2520Wu%2520and%2520Ya%2520Wang%2520and%2520Jiao%2520Ran%2520and%2520Xiao%2520Liang%2520and%2520Chao%2520Feng%2520and%2520Jun%2520Xiao%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520multimodal%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%252C%2520existing%250Aapproaches%2520predominantly%2520rely%2520on%2520reasoning%2520on%2520pure%2520language%2520space%252C%2520which%250Ainherently%2520suffers%2520from%2520language%2520bias%2520and%2520is%2520largely%2520confined%2520to%2520math%2520or%250Ascience%2520domains.%2520This%2520narrow%2520focus%2520limits%2520their%2520ability%2520to%2520handle%2520complex%250Avisual%2520reasoning%2520tasks%2520that%2520demand%2520comprehensive%2520understanding%2520of%2520image%250Adetails.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%2520introduces%2520VGR%252C%2520a%2520novel%250Areasoning%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520with%2520enhanced%2520fine-grained%250Avisual%2520perception%2520capabilities.%2520Unlike%2520traditional%2520MLLMs%2520that%2520answer%2520the%250Aquestion%2520or%2520reasoning%2520solely%2520on%2520the%2520language%2520space%252C%2520our%2520VGR%2520first%2520detects%250Arelevant%2520regions%2520that%2520may%2520help%2520to%2520solve%2520problems%252C%2520and%2520then%2520provides%2520precise%250Aanswers%2520based%2520on%2520replayed%2520image%2520regions.%2520To%2520achieve%2520this%252C%2520we%2520conduct%2520a%250Alarge-scale%2520SFT%2520dataset%2520called%2520VGR%2520-SFT%2520that%2520contains%2520reasoning%2520data%2520with%2520mixed%250Avision%2520grounding%2520and%2520language%2520deduction.%2520The%2520inference%2520pipeline%2520of%2520VGR%2520allows%250Athe%2520model%2520to%2520choose%2520bounding%2520boxes%2520for%2520visual%2520reference%2520and%2520a%2520replay%2520stage%2520is%250Aintroduced%2520to%2520integrates%2520the%2520corresponding%2520regions%2520into%2520the%2520reasoning%2520process%252C%250Aenhancing%2520multimodel%2520comprehension.%2520Experiments%2520on%2520the%2520LLaVA-NeXT-7B%2520baseline%250Ashow%2520that%2520VGR%2520achieves%2520superior%2520performance%2520on%2520multi-modal%2520benchmarks%2520requiring%250Acomprehensive%2520image%2520detail%2520understanding.%2520Compared%2520to%2520the%2520baseline%252C%2520VGR%2520uses%250Aonly%252030%255C%2525%2520of%2520the%2520image%2520token%2520count%2520while%2520delivering%2520scores%2520of%2520%252B4.1%2520on%2520MMStar%252C%250A%252B7.1%2520on%2520AI2D%252C%2520and%2520a%2520%252B12.9%2520improvement%2520on%2520ChartQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VGR%3A%20Visual%20Grounded%20Reasoning&entry.906535625=Jiacong%20Wang%20and%20Zijiang%20Kang%20and%20Haochen%20Wang%20and%20Haiyong%20Jiang%20and%20Jiawen%20Li%20and%20Bohong%20Wu%20and%20Ya%20Wang%20and%20Jiao%20Ran%20and%20Xiao%20Liang%20and%20Chao%20Feng%20and%20Jun%20Xiao&entry.1292438233=%20%20In%20the%20field%20of%20multimodal%20chain-of-thought%20%28CoT%29%20reasoning%2C%20existing%0Aapproaches%20predominantly%20rely%20on%20reasoning%20on%20pure%20language%20space%2C%20which%0Ainherently%20suffers%20from%20language%20bias%20and%20is%20largely%20confined%20to%20math%20or%0Ascience%20domains.%20This%20narrow%20focus%20limits%20their%20ability%20to%20handle%20complex%0Avisual%20reasoning%20tasks%20that%20demand%20comprehensive%20understanding%20of%20image%0Adetails.%20To%20address%20these%20limitations%2C%20this%20paper%20introduces%20VGR%2C%20a%20novel%0Areasoning%20multimodal%20large%20language%20model%20%28MLLM%29%20with%20enhanced%20fine-grained%0Avisual%20perception%20capabilities.%20Unlike%20traditional%20MLLMs%20that%20answer%20the%0Aquestion%20or%20reasoning%20solely%20on%20the%20language%20space%2C%20our%20VGR%20first%20detects%0Arelevant%20regions%20that%20may%20help%20to%20solve%20problems%2C%20and%20then%20provides%20precise%0Aanswers%20based%20on%20replayed%20image%20regions.%20To%20achieve%20this%2C%20we%20conduct%20a%0Alarge-scale%20SFT%20dataset%20called%20VGR%20-SFT%20that%20contains%20reasoning%20data%20with%20mixed%0Avision%20grounding%20and%20language%20deduction.%20The%20inference%20pipeline%20of%20VGR%20allows%0Athe%20model%20to%20choose%20bounding%20boxes%20for%20visual%20reference%20and%20a%20replay%20stage%20is%0Aintroduced%20to%20integrates%20the%20corresponding%20regions%20into%20the%20reasoning%20process%2C%0Aenhancing%20multimodel%20comprehension.%20Experiments%20on%20the%20LLaVA-NeXT-7B%20baseline%0Ashow%20that%20VGR%20achieves%20superior%20performance%20on%20multi-modal%20benchmarks%20requiring%0Acomprehensive%20image%20detail%20understanding.%20Compared%20to%20the%20baseline%2C%20VGR%20uses%0Aonly%2030%5C%25%20of%20the%20image%20token%20count%20while%20delivering%20scores%20of%20%2B4.1%20on%20MMStar%2C%0A%2B7.1%20on%20AI2D%2C%20and%20a%20%2B12.9%20improvement%20on%20ChartQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11991v1&entry.124074799=Read"},
{"title": "Vision-based Lifting of 2D Object Detections for Automated Driving", "author": "Hendrik K\u00f6nigshof and Kun Li and Christoph Stiller", "abstract": "  Image-based 3D object detection is an inevitable part of autonomous driving\nbecause cheap onboard cameras are already available in most modern cars.\nBecause of the accurate depth information, currently, most state-of-the-art 3D\nobject detectors heavily rely on LiDAR data. In this paper, we propose a\npipeline which lifts the results of existing vision-based 2D algorithms to 3D\ndetections using only cameras as a cost-effective alternative to LiDAR. In\ncontrast to existing approaches, we focus not only on cars but on all types of\nroad users. To the best of our knowledge, we are the first using a 2D CNN to\nprocess the point cloud for each 2D detection to keep the computational effort\nas low as possible. Our evaluation on the challenging KITTI 3D object detection\nbenchmark shows results comparable to state-of-the-art image-based approaches\nwhile having a runtime of only a third.\n", "link": "http://arxiv.org/abs/2506.11839v1", "date": "2025-06-13", "relevancy": 2.3152, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5854}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5788}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-based%20Lifting%20of%202D%20Object%20Detections%20for%20Automated%20Driving&body=Title%3A%20Vision-based%20Lifting%20of%202D%20Object%20Detections%20for%20Automated%20Driving%0AAuthor%3A%20Hendrik%20K%C3%B6nigshof%20and%20Kun%20Li%20and%20Christoph%20Stiller%0AAbstract%3A%20%20%20Image-based%203D%20object%20detection%20is%20an%20inevitable%20part%20of%20autonomous%20driving%0Abecause%20cheap%20onboard%20cameras%20are%20already%20available%20in%20most%20modern%20cars.%0ABecause%20of%20the%20accurate%20depth%20information%2C%20currently%2C%20most%20state-of-the-art%203D%0Aobject%20detectors%20heavily%20rely%20on%20LiDAR%20data.%20In%20this%20paper%2C%20we%20propose%20a%0Apipeline%20which%20lifts%20the%20results%20of%20existing%20vision-based%202D%20algorithms%20to%203D%0Adetections%20using%20only%20cameras%20as%20a%20cost-effective%20alternative%20to%20LiDAR.%20In%0Acontrast%20to%20existing%20approaches%2C%20we%20focus%20not%20only%20on%20cars%20but%20on%20all%20types%20of%0Aroad%20users.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20using%20a%202D%20CNN%20to%0Aprocess%20the%20point%20cloud%20for%20each%202D%20detection%20to%20keep%20the%20computational%20effort%0Aas%20low%20as%20possible.%20Our%20evaluation%20on%20the%20challenging%20KITTI%203D%20object%20detection%0Abenchmark%20shows%20results%20comparable%20to%20state-of-the-art%20image-based%20approaches%0Awhile%20having%20a%20runtime%20of%20only%20a%20third.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-based%2520Lifting%2520of%25202D%2520Object%2520Detections%2520for%2520Automated%2520Driving%26entry.906535625%3DHendrik%2520K%25C3%25B6nigshof%2520and%2520Kun%2520Li%2520and%2520Christoph%2520Stiller%26entry.1292438233%3D%2520%2520Image-based%25203D%2520object%2520detection%2520is%2520an%2520inevitable%2520part%2520of%2520autonomous%2520driving%250Abecause%2520cheap%2520onboard%2520cameras%2520are%2520already%2520available%2520in%2520most%2520modern%2520cars.%250ABecause%2520of%2520the%2520accurate%2520depth%2520information%252C%2520currently%252C%2520most%2520state-of-the-art%25203D%250Aobject%2520detectors%2520heavily%2520rely%2520on%2520LiDAR%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Apipeline%2520which%2520lifts%2520the%2520results%2520of%2520existing%2520vision-based%25202D%2520algorithms%2520to%25203D%250Adetections%2520using%2520only%2520cameras%2520as%2520a%2520cost-effective%2520alternative%2520to%2520LiDAR.%2520In%250Acontrast%2520to%2520existing%2520approaches%252C%2520we%2520focus%2520not%2520only%2520on%2520cars%2520but%2520on%2520all%2520types%2520of%250Aroad%2520users.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520we%2520are%2520the%2520first%2520using%2520a%25202D%2520CNN%2520to%250Aprocess%2520the%2520point%2520cloud%2520for%2520each%25202D%2520detection%2520to%2520keep%2520the%2520computational%2520effort%250Aas%2520low%2520as%2520possible.%2520Our%2520evaluation%2520on%2520the%2520challenging%2520KITTI%25203D%2520object%2520detection%250Abenchmark%2520shows%2520results%2520comparable%2520to%2520state-of-the-art%2520image-based%2520approaches%250Awhile%2520having%2520a%2520runtime%2520of%2520only%2520a%2520third.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-based%20Lifting%20of%202D%20Object%20Detections%20for%20Automated%20Driving&entry.906535625=Hendrik%20K%C3%B6nigshof%20and%20Kun%20Li%20and%20Christoph%20Stiller&entry.1292438233=%20%20Image-based%203D%20object%20detection%20is%20an%20inevitable%20part%20of%20autonomous%20driving%0Abecause%20cheap%20onboard%20cameras%20are%20already%20available%20in%20most%20modern%20cars.%0ABecause%20of%20the%20accurate%20depth%20information%2C%20currently%2C%20most%20state-of-the-art%203D%0Aobject%20detectors%20heavily%20rely%20on%20LiDAR%20data.%20In%20this%20paper%2C%20we%20propose%20a%0Apipeline%20which%20lifts%20the%20results%20of%20existing%20vision-based%202D%20algorithms%20to%203D%0Adetections%20using%20only%20cameras%20as%20a%20cost-effective%20alternative%20to%20LiDAR.%20In%0Acontrast%20to%20existing%20approaches%2C%20we%20focus%20not%20only%20on%20cars%20but%20on%20all%20types%20of%0Aroad%20users.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20using%20a%202D%20CNN%20to%0Aprocess%20the%20point%20cloud%20for%20each%202D%20detection%20to%20keep%20the%20computational%20effort%0Aas%20low%20as%20possible.%20Our%20evaluation%20on%20the%20challenging%20KITTI%203D%20object%20detection%0Abenchmark%20shows%20results%20comparable%20to%20state-of-the-art%20image-based%20approaches%0Awhile%20having%20a%20runtime%20of%20only%20a%20third.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11839v1&entry.124074799=Read"},
{"title": "ExoStart: Efficient learning for dexterous manipulation with sensorized\n  exoskeleton demonstrations", "author": "Zilin Si and Jose Enrique Chen and M. Emre Karagozler and Antonia Bronars and Jonathan Hutchinson and Thomas Lampe and Nimrod Gileadi and Taylor Howell and Stefano Saliceti and Lukasz Barczyk and Ilan Olivarez Correa and Tom Erez and Mohit Shridhar and Murilo Fernandes Martins and Konstantinos Bousmalis and Nicolas Heess and Francesco Nori and Maria Bauza Villalonga", "abstract": "  Recent advancements in teleoperation systems have enabled high-quality data\ncollection for robotic manipulators, showing impressive results in learning\nmanipulation at scale. This progress suggests that extending these capabilities\nto robotic hands could unlock an even broader range of manipulation skills,\nespecially if we could achieve the same level of dexterity that human hands\nexhibit. However, teleoperating robotic hands is far from a solved problem, as\nit presents a significant challenge due to the high degrees of freedom of\nrobotic hands and the complex dynamics occurring during contact-rich settings.\nIn this work, we present ExoStart, a general and scalable learning framework\nthat leverages human dexterity to improve robotic hand control. In particular,\nwe obtain high-quality data by collecting direct demonstrations without a robot\nin the loop using a sensorized low-cost wearable exoskeleton, capturing the\nrich behaviors that humans can demonstrate with their own hands. We also\npropose a simulation-based dynamics filter that generates dynamically feasible\ntrajectories from the collected demonstrations and use the generated\ntrajectories to bootstrap an auto-curriculum reinforcement learning method that\nrelies only on simple sparse rewards. The ExoStart pipeline is generalizable\nand yields robust policies that transfer zero-shot to the real robot. Our\nresults demonstrate that ExoStart can generate dexterous real-world hand\nskills, achieving a success rate above 50% on a wide range of complex tasks\nsuch as opening an AirPods case or inserting and turning a key in a lock. More\ndetails and videos can be found in https://sites.google.com/view/exostart.\n", "link": "http://arxiv.org/abs/2506.11775v1", "date": "2025-06-13", "relevancy": 2.308, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6109}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5716}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExoStart%3A%20Efficient%20learning%20for%20dexterous%20manipulation%20with%20sensorized%0A%20%20exoskeleton%20demonstrations&body=Title%3A%20ExoStart%3A%20Efficient%20learning%20for%20dexterous%20manipulation%20with%20sensorized%0A%20%20exoskeleton%20demonstrations%0AAuthor%3A%20Zilin%20Si%20and%20Jose%20Enrique%20Chen%20and%20M.%20Emre%20Karagozler%20and%20Antonia%20Bronars%20and%20Jonathan%20Hutchinson%20and%20Thomas%20Lampe%20and%20Nimrod%20Gileadi%20and%20Taylor%20Howell%20and%20Stefano%20Saliceti%20and%20Lukasz%20Barczyk%20and%20Ilan%20Olivarez%20Correa%20and%20Tom%20Erez%20and%20Mohit%20Shridhar%20and%20Murilo%20Fernandes%20Martins%20and%20Konstantinos%20Bousmalis%20and%20Nicolas%20Heess%20and%20Francesco%20Nori%20and%20Maria%20Bauza%20Villalonga%0AAbstract%3A%20%20%20Recent%20advancements%20in%20teleoperation%20systems%20have%20enabled%20high-quality%20data%0Acollection%20for%20robotic%20manipulators%2C%20showing%20impressive%20results%20in%20learning%0Amanipulation%20at%20scale.%20This%20progress%20suggests%20that%20extending%20these%20capabilities%0Ato%20robotic%20hands%20could%20unlock%20an%20even%20broader%20range%20of%20manipulation%20skills%2C%0Aespecially%20if%20we%20could%20achieve%20the%20same%20level%20of%20dexterity%20that%20human%20hands%0Aexhibit.%20However%2C%20teleoperating%20robotic%20hands%20is%20far%20from%20a%20solved%20problem%2C%20as%0Ait%20presents%20a%20significant%20challenge%20due%20to%20the%20high%20degrees%20of%20freedom%20of%0Arobotic%20hands%20and%20the%20complex%20dynamics%20occurring%20during%20contact-rich%20settings.%0AIn%20this%20work%2C%20we%20present%20ExoStart%2C%20a%20general%20and%20scalable%20learning%20framework%0Athat%20leverages%20human%20dexterity%20to%20improve%20robotic%20hand%20control.%20In%20particular%2C%0Awe%20obtain%20high-quality%20data%20by%20collecting%20direct%20demonstrations%20without%20a%20robot%0Ain%20the%20loop%20using%20a%20sensorized%20low-cost%20wearable%20exoskeleton%2C%20capturing%20the%0Arich%20behaviors%20that%20humans%20can%20demonstrate%20with%20their%20own%20hands.%20We%20also%0Apropose%20a%20simulation-based%20dynamics%20filter%20that%20generates%20dynamically%20feasible%0Atrajectories%20from%20the%20collected%20demonstrations%20and%20use%20the%20generated%0Atrajectories%20to%20bootstrap%20an%20auto-curriculum%20reinforcement%20learning%20method%20that%0Arelies%20only%20on%20simple%20sparse%20rewards.%20The%20ExoStart%20pipeline%20is%20generalizable%0Aand%20yields%20robust%20policies%20that%20transfer%20zero-shot%20to%20the%20real%20robot.%20Our%0Aresults%20demonstrate%20that%20ExoStart%20can%20generate%20dexterous%20real-world%20hand%0Askills%2C%20achieving%20a%20success%20rate%20above%2050%25%20on%20a%20wide%20range%20of%20complex%20tasks%0Asuch%20as%20opening%20an%20AirPods%20case%20or%20inserting%20and%20turning%20a%20key%20in%20a%20lock.%20More%0Adetails%20and%20videos%20can%20be%20found%20in%20https%3A//sites.google.com/view/exostart.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExoStart%253A%2520Efficient%2520learning%2520for%2520dexterous%2520manipulation%2520with%2520sensorized%250A%2520%2520exoskeleton%2520demonstrations%26entry.906535625%3DZilin%2520Si%2520and%2520Jose%2520Enrique%2520Chen%2520and%2520M.%2520Emre%2520Karagozler%2520and%2520Antonia%2520Bronars%2520and%2520Jonathan%2520Hutchinson%2520and%2520Thomas%2520Lampe%2520and%2520Nimrod%2520Gileadi%2520and%2520Taylor%2520Howell%2520and%2520Stefano%2520Saliceti%2520and%2520Lukasz%2520Barczyk%2520and%2520Ilan%2520Olivarez%2520Correa%2520and%2520Tom%2520Erez%2520and%2520Mohit%2520Shridhar%2520and%2520Murilo%2520Fernandes%2520Martins%2520and%2520Konstantinos%2520Bousmalis%2520and%2520Nicolas%2520Heess%2520and%2520Francesco%2520Nori%2520and%2520Maria%2520Bauza%2520Villalonga%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520teleoperation%2520systems%2520have%2520enabled%2520high-quality%2520data%250Acollection%2520for%2520robotic%2520manipulators%252C%2520showing%2520impressive%2520results%2520in%2520learning%250Amanipulation%2520at%2520scale.%2520This%2520progress%2520suggests%2520that%2520extending%2520these%2520capabilities%250Ato%2520robotic%2520hands%2520could%2520unlock%2520an%2520even%2520broader%2520range%2520of%2520manipulation%2520skills%252C%250Aespecially%2520if%2520we%2520could%2520achieve%2520the%2520same%2520level%2520of%2520dexterity%2520that%2520human%2520hands%250Aexhibit.%2520However%252C%2520teleoperating%2520robotic%2520hands%2520is%2520far%2520from%2520a%2520solved%2520problem%252C%2520as%250Ait%2520presents%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520high%2520degrees%2520of%2520freedom%2520of%250Arobotic%2520hands%2520and%2520the%2520complex%2520dynamics%2520occurring%2520during%2520contact-rich%2520settings.%250AIn%2520this%2520work%252C%2520we%2520present%2520ExoStart%252C%2520a%2520general%2520and%2520scalable%2520learning%2520framework%250Athat%2520leverages%2520human%2520dexterity%2520to%2520improve%2520robotic%2520hand%2520control.%2520In%2520particular%252C%250Awe%2520obtain%2520high-quality%2520data%2520by%2520collecting%2520direct%2520demonstrations%2520without%2520a%2520robot%250Ain%2520the%2520loop%2520using%2520a%2520sensorized%2520low-cost%2520wearable%2520exoskeleton%252C%2520capturing%2520the%250Arich%2520behaviors%2520that%2520humans%2520can%2520demonstrate%2520with%2520their%2520own%2520hands.%2520We%2520also%250Apropose%2520a%2520simulation-based%2520dynamics%2520filter%2520that%2520generates%2520dynamically%2520feasible%250Atrajectories%2520from%2520the%2520collected%2520demonstrations%2520and%2520use%2520the%2520generated%250Atrajectories%2520to%2520bootstrap%2520an%2520auto-curriculum%2520reinforcement%2520learning%2520method%2520that%250Arelies%2520only%2520on%2520simple%2520sparse%2520rewards.%2520The%2520ExoStart%2520pipeline%2520is%2520generalizable%250Aand%2520yields%2520robust%2520policies%2520that%2520transfer%2520zero-shot%2520to%2520the%2520real%2520robot.%2520Our%250Aresults%2520demonstrate%2520that%2520ExoStart%2520can%2520generate%2520dexterous%2520real-world%2520hand%250Askills%252C%2520achieving%2520a%2520success%2520rate%2520above%252050%2525%2520on%2520a%2520wide%2520range%2520of%2520complex%2520tasks%250Asuch%2520as%2520opening%2520an%2520AirPods%2520case%2520or%2520inserting%2520and%2520turning%2520a%2520key%2520in%2520a%2520lock.%2520More%250Adetails%2520and%2520videos%2520can%2520be%2520found%2520in%2520https%253A//sites.google.com/view/exostart.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExoStart%3A%20Efficient%20learning%20for%20dexterous%20manipulation%20with%20sensorized%0A%20%20exoskeleton%20demonstrations&entry.906535625=Zilin%20Si%20and%20Jose%20Enrique%20Chen%20and%20M.%20Emre%20Karagozler%20and%20Antonia%20Bronars%20and%20Jonathan%20Hutchinson%20and%20Thomas%20Lampe%20and%20Nimrod%20Gileadi%20and%20Taylor%20Howell%20and%20Stefano%20Saliceti%20and%20Lukasz%20Barczyk%20and%20Ilan%20Olivarez%20Correa%20and%20Tom%20Erez%20and%20Mohit%20Shridhar%20and%20Murilo%20Fernandes%20Martins%20and%20Konstantinos%20Bousmalis%20and%20Nicolas%20Heess%20and%20Francesco%20Nori%20and%20Maria%20Bauza%20Villalonga&entry.1292438233=%20%20Recent%20advancements%20in%20teleoperation%20systems%20have%20enabled%20high-quality%20data%0Acollection%20for%20robotic%20manipulators%2C%20showing%20impressive%20results%20in%20learning%0Amanipulation%20at%20scale.%20This%20progress%20suggests%20that%20extending%20these%20capabilities%0Ato%20robotic%20hands%20could%20unlock%20an%20even%20broader%20range%20of%20manipulation%20skills%2C%0Aespecially%20if%20we%20could%20achieve%20the%20same%20level%20of%20dexterity%20that%20human%20hands%0Aexhibit.%20However%2C%20teleoperating%20robotic%20hands%20is%20far%20from%20a%20solved%20problem%2C%20as%0Ait%20presents%20a%20significant%20challenge%20due%20to%20the%20high%20degrees%20of%20freedom%20of%0Arobotic%20hands%20and%20the%20complex%20dynamics%20occurring%20during%20contact-rich%20settings.%0AIn%20this%20work%2C%20we%20present%20ExoStart%2C%20a%20general%20and%20scalable%20learning%20framework%0Athat%20leverages%20human%20dexterity%20to%20improve%20robotic%20hand%20control.%20In%20particular%2C%0Awe%20obtain%20high-quality%20data%20by%20collecting%20direct%20demonstrations%20without%20a%20robot%0Ain%20the%20loop%20using%20a%20sensorized%20low-cost%20wearable%20exoskeleton%2C%20capturing%20the%0Arich%20behaviors%20that%20humans%20can%20demonstrate%20with%20their%20own%20hands.%20We%20also%0Apropose%20a%20simulation-based%20dynamics%20filter%20that%20generates%20dynamically%20feasible%0Atrajectories%20from%20the%20collected%20demonstrations%20and%20use%20the%20generated%0Atrajectories%20to%20bootstrap%20an%20auto-curriculum%20reinforcement%20learning%20method%20that%0Arelies%20only%20on%20simple%20sparse%20rewards.%20The%20ExoStart%20pipeline%20is%20generalizable%0Aand%20yields%20robust%20policies%20that%20transfer%20zero-shot%20to%20the%20real%20robot.%20Our%0Aresults%20demonstrate%20that%20ExoStart%20can%20generate%20dexterous%20real-world%20hand%0Askills%2C%20achieving%20a%20success%20rate%20above%2050%25%20on%20a%20wide%20range%20of%20complex%20tasks%0Asuch%20as%20opening%20an%20AirPods%20case%20or%20inserting%20and%20turning%20a%20key%20in%20a%20lock.%20More%0Adetails%20and%20videos%20can%20be%20found%20in%20https%3A//sites.google.com/view/exostart.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11775v1&entry.124074799=Read"},
{"title": "3D-WAG: Hierarchical Wavelet-Guided Autoregressive Generation for\n  High-Fidelity 3D Shapes", "author": "Tejaswini Medi and Arianna Rampini and Pradyumna Reddy and Pradeep Kumar Jayaraman and Margret Keuper", "abstract": "  Autoregressive (AR) models have achieved remarkable success in natural\nlanguage and image generation, but their application to 3D shape modeling\nremains largely unexplored. Unlike diffusion models, AR models enable more\nefficient and controllable generation with faster inference times, making them\nespecially suitable for data-intensive domains. Traditional 3D generative\nmodels using AR approaches often rely on ``next-token\" predictions at the voxel\nor point level. While effective for certain applications, these methods can be\nrestrictive and computationally expensive when dealing with large-scale 3D\ndata. To tackle these challenges, we introduce 3D-WAG, an AR model for 3D\nimplicit distance fields that can perform unconditional shape generation,\nclass-conditioned and also text-conditioned shape generation. Our key idea is\nto encode shapes as multi-scale wavelet token maps and use a Transformer to\npredict the ``next higher-resolution token map\" in an autoregressive manner. By\nredefining 3D AR generation task as ``next-scale\" prediction, we reduce the\ncomputational cost of generation compared to traditional ``next-token\"\nprediction models, while preserving essential geometric details of 3D shapes in\na more structured and hierarchical manner. We evaluate 3D-WAG to showcase its\nbenefit by quantitative and qualitative comparisons with state-of-the-art\nmethods on widely used benchmarks. Our results show 3D-WAG achieves superior\nperformance in key metrics like Coverage and MMD, generating high-fidelity 3D\nshapes that closely match the real data distribution.\n", "link": "http://arxiv.org/abs/2411.19037v2", "date": "2025-06-13", "relevancy": 2.3015, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5909}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5754}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-WAG%3A%20Hierarchical%20Wavelet-Guided%20Autoregressive%20Generation%20for%0A%20%20High-Fidelity%203D%20Shapes&body=Title%3A%203D-WAG%3A%20Hierarchical%20Wavelet-Guided%20Autoregressive%20Generation%20for%0A%20%20High-Fidelity%203D%20Shapes%0AAuthor%3A%20Tejaswini%20Medi%20and%20Arianna%20Rampini%20and%20Pradyumna%20Reddy%20and%20Pradeep%20Kumar%20Jayaraman%20and%20Margret%20Keuper%0AAbstract%3A%20%20%20Autoregressive%20%28AR%29%20models%20have%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20and%20image%20generation%2C%20but%20their%20application%20to%203D%20shape%20modeling%0Aremains%20largely%20unexplored.%20Unlike%20diffusion%20models%2C%20AR%20models%20enable%20more%0Aefficient%20and%20controllable%20generation%20with%20faster%20inference%20times%2C%20making%20them%0Aespecially%20suitable%20for%20data-intensive%20domains.%20Traditional%203D%20generative%0Amodels%20using%20AR%20approaches%20often%20rely%20on%20%60%60next-token%22%20predictions%20at%20the%20voxel%0Aor%20point%20level.%20While%20effective%20for%20certain%20applications%2C%20these%20methods%20can%20be%0Arestrictive%20and%20computationally%20expensive%20when%20dealing%20with%20large-scale%203D%0Adata.%20To%20tackle%20these%20challenges%2C%20we%20introduce%203D-WAG%2C%20an%20AR%20model%20for%203D%0Aimplicit%20distance%20fields%20that%20can%20perform%20unconditional%20shape%20generation%2C%0Aclass-conditioned%20and%20also%20text-conditioned%20shape%20generation.%20Our%20key%20idea%20is%0Ato%20encode%20shapes%20as%20multi-scale%20wavelet%20token%20maps%20and%20use%20a%20Transformer%20to%0Apredict%20the%20%60%60next%20higher-resolution%20token%20map%22%20in%20an%20autoregressive%20manner.%20By%0Aredefining%203D%20AR%20generation%20task%20as%20%60%60next-scale%22%20prediction%2C%20we%20reduce%20the%0Acomputational%20cost%20of%20generation%20compared%20to%20traditional%20%60%60next-token%22%0Aprediction%20models%2C%20while%20preserving%20essential%20geometric%20details%20of%203D%20shapes%20in%0Aa%20more%20structured%20and%20hierarchical%20manner.%20We%20evaluate%203D-WAG%20to%20showcase%20its%0Abenefit%20by%20quantitative%20and%20qualitative%20comparisons%20with%20state-of-the-art%0Amethods%20on%20widely%20used%20benchmarks.%20Our%20results%20show%203D-WAG%20achieves%20superior%0Aperformance%20in%20key%20metrics%20like%20Coverage%20and%20MMD%2C%20generating%20high-fidelity%203D%0Ashapes%20that%20closely%20match%20the%20real%20data%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19037v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-WAG%253A%2520Hierarchical%2520Wavelet-Guided%2520Autoregressive%2520Generation%2520for%250A%2520%2520High-Fidelity%25203D%2520Shapes%26entry.906535625%3DTejaswini%2520Medi%2520and%2520Arianna%2520Rampini%2520and%2520Pradyumna%2520Reddy%2520and%2520Pradeep%2520Kumar%2520Jayaraman%2520and%2520Margret%2520Keuper%26entry.1292438233%3D%2520%2520Autoregressive%2520%2528AR%2529%2520models%2520have%2520achieved%2520remarkable%2520success%2520in%2520natural%250Alanguage%2520and%2520image%2520generation%252C%2520but%2520their%2520application%2520to%25203D%2520shape%2520modeling%250Aremains%2520largely%2520unexplored.%2520Unlike%2520diffusion%2520models%252C%2520AR%2520models%2520enable%2520more%250Aefficient%2520and%2520controllable%2520generation%2520with%2520faster%2520inference%2520times%252C%2520making%2520them%250Aespecially%2520suitable%2520for%2520data-intensive%2520domains.%2520Traditional%25203D%2520generative%250Amodels%2520using%2520AR%2520approaches%2520often%2520rely%2520on%2520%2560%2560next-token%2522%2520predictions%2520at%2520the%2520voxel%250Aor%2520point%2520level.%2520While%2520effective%2520for%2520certain%2520applications%252C%2520these%2520methods%2520can%2520be%250Arestrictive%2520and%2520computationally%2520expensive%2520when%2520dealing%2520with%2520large-scale%25203D%250Adata.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520introduce%25203D-WAG%252C%2520an%2520AR%2520model%2520for%25203D%250Aimplicit%2520distance%2520fields%2520that%2520can%2520perform%2520unconditional%2520shape%2520generation%252C%250Aclass-conditioned%2520and%2520also%2520text-conditioned%2520shape%2520generation.%2520Our%2520key%2520idea%2520is%250Ato%2520encode%2520shapes%2520as%2520multi-scale%2520wavelet%2520token%2520maps%2520and%2520use%2520a%2520Transformer%2520to%250Apredict%2520the%2520%2560%2560next%2520higher-resolution%2520token%2520map%2522%2520in%2520an%2520autoregressive%2520manner.%2520By%250Aredefining%25203D%2520AR%2520generation%2520task%2520as%2520%2560%2560next-scale%2522%2520prediction%252C%2520we%2520reduce%2520the%250Acomputational%2520cost%2520of%2520generation%2520compared%2520to%2520traditional%2520%2560%2560next-token%2522%250Aprediction%2520models%252C%2520while%2520preserving%2520essential%2520geometric%2520details%2520of%25203D%2520shapes%2520in%250Aa%2520more%2520structured%2520and%2520hierarchical%2520manner.%2520We%2520evaluate%25203D-WAG%2520to%2520showcase%2520its%250Abenefit%2520by%2520quantitative%2520and%2520qualitative%2520comparisons%2520with%2520state-of-the-art%250Amethods%2520on%2520widely%2520used%2520benchmarks.%2520Our%2520results%2520show%25203D-WAG%2520achieves%2520superior%250Aperformance%2520in%2520key%2520metrics%2520like%2520Coverage%2520and%2520MMD%252C%2520generating%2520high-fidelity%25203D%250Ashapes%2520that%2520closely%2520match%2520the%2520real%2520data%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19037v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-WAG%3A%20Hierarchical%20Wavelet-Guided%20Autoregressive%20Generation%20for%0A%20%20High-Fidelity%203D%20Shapes&entry.906535625=Tejaswini%20Medi%20and%20Arianna%20Rampini%20and%20Pradyumna%20Reddy%20and%20Pradeep%20Kumar%20Jayaraman%20and%20Margret%20Keuper&entry.1292438233=%20%20Autoregressive%20%28AR%29%20models%20have%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20and%20image%20generation%2C%20but%20their%20application%20to%203D%20shape%20modeling%0Aremains%20largely%20unexplored.%20Unlike%20diffusion%20models%2C%20AR%20models%20enable%20more%0Aefficient%20and%20controllable%20generation%20with%20faster%20inference%20times%2C%20making%20them%0Aespecially%20suitable%20for%20data-intensive%20domains.%20Traditional%203D%20generative%0Amodels%20using%20AR%20approaches%20often%20rely%20on%20%60%60next-token%22%20predictions%20at%20the%20voxel%0Aor%20point%20level.%20While%20effective%20for%20certain%20applications%2C%20these%20methods%20can%20be%0Arestrictive%20and%20computationally%20expensive%20when%20dealing%20with%20large-scale%203D%0Adata.%20To%20tackle%20these%20challenges%2C%20we%20introduce%203D-WAG%2C%20an%20AR%20model%20for%203D%0Aimplicit%20distance%20fields%20that%20can%20perform%20unconditional%20shape%20generation%2C%0Aclass-conditioned%20and%20also%20text-conditioned%20shape%20generation.%20Our%20key%20idea%20is%0Ato%20encode%20shapes%20as%20multi-scale%20wavelet%20token%20maps%20and%20use%20a%20Transformer%20to%0Apredict%20the%20%60%60next%20higher-resolution%20token%20map%22%20in%20an%20autoregressive%20manner.%20By%0Aredefining%203D%20AR%20generation%20task%20as%20%60%60next-scale%22%20prediction%2C%20we%20reduce%20the%0Acomputational%20cost%20of%20generation%20compared%20to%20traditional%20%60%60next-token%22%0Aprediction%20models%2C%20while%20preserving%20essential%20geometric%20details%20of%203D%20shapes%20in%0Aa%20more%20structured%20and%20hierarchical%20manner.%20We%20evaluate%203D-WAG%20to%20showcase%20its%0Abenefit%20by%20quantitative%20and%20qualitative%20comparisons%20with%20state-of-the-art%0Amethods%20on%20widely%20used%20benchmarks.%20Our%20results%20show%203D-WAG%20achieves%20superior%0Aperformance%20in%20key%20metrics%20like%20Coverage%20and%20MMD%2C%20generating%20high-fidelity%203D%0Ashapes%20that%20closely%20match%20the%20real%20data%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19037v2&entry.124074799=Read"},
{"title": "SSPINNpose: A Self-Supervised PINN for Inertial Pose and Dynamics\n  Estimation", "author": "Markus Gambietz and Eva Dorschky and Altan Akat and Marcel Sch\u00f6ckel and J\u00f6rg Miehling and Anne D. Koelewijn", "abstract": "  Accurate real-time estimation of human movement dynamics, including internal\njoint moments and muscle forces, is essential for applications in clinical\ndiagnostics and sports performance monitoring. Inertial measurement units\n(IMUs) provide a minimally intrusive solution for capturing motion data,\nparticularly when used in sparse sensor configurations. However, current\nreal-time methods rely on supervised learning, where a ground truth dataset\nneeds to be measured with laboratory measurement systems, such as optical\nmotion capture. These systems are known to introduce measurement and processing\nerrors and often fail to generalize to real-world or previously unseen\nmovements, necessitating new data collection efforts that are time-consuming\nand impractical. To overcome these limitations, we propose SSPINNpose, a\nself-supervised, physics-informed neural network that estimates joint\nkinematics and kinetics directly from IMU data, without requiring ground truth\nlabels for training. We run the network output through a physics model of the\nhuman body to optimize physical plausibility and generate virtual measurement\ndata. Using this virtual sensor data, the network is trained directly on the\nmeasured sensor data instead of a ground truth. When compared to optical motion\ncapture, SSPINNpose is able to accurately estimate joint angles and joint\nmoments at an RMSD of 8.7 deg and 4.9 BWBH%, respectively, for walking and\nrunning at speeds up to 4.9 m/s at a latency of 3.5 ms. Furthermore, the\nframework demonstrates robustness across sparse sensor configurations and can\ninfer the anatomical locations of the sensors. These results underscore the\npotential of SSPINNpose as a scalable and adaptable solution for real-time\nbiomechanical analysis in both laboratory and field environments.\n", "link": "http://arxiv.org/abs/2506.11786v1", "date": "2025-06-13", "relevancy": 2.2908, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5954}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5919}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSPINNpose%3A%20A%20Self-Supervised%20PINN%20for%20Inertial%20Pose%20and%20Dynamics%0A%20%20Estimation&body=Title%3A%20SSPINNpose%3A%20A%20Self-Supervised%20PINN%20for%20Inertial%20Pose%20and%20Dynamics%0A%20%20Estimation%0AAuthor%3A%20Markus%20Gambietz%20and%20Eva%20Dorschky%20and%20Altan%20Akat%20and%20Marcel%20Sch%C3%B6ckel%20and%20J%C3%B6rg%20Miehling%20and%20Anne%20D.%20Koelewijn%0AAbstract%3A%20%20%20Accurate%20real-time%20estimation%20of%20human%20movement%20dynamics%2C%20including%20internal%0Ajoint%20moments%20and%20muscle%20forces%2C%20is%20essential%20for%20applications%20in%20clinical%0Adiagnostics%20and%20sports%20performance%20monitoring.%20Inertial%20measurement%20units%0A%28IMUs%29%20provide%20a%20minimally%20intrusive%20solution%20for%20capturing%20motion%20data%2C%0Aparticularly%20when%20used%20in%20sparse%20sensor%20configurations.%20However%2C%20current%0Areal-time%20methods%20rely%20on%20supervised%20learning%2C%20where%20a%20ground%20truth%20dataset%0Aneeds%20to%20be%20measured%20with%20laboratory%20measurement%20systems%2C%20such%20as%20optical%0Amotion%20capture.%20These%20systems%20are%20known%20to%20introduce%20measurement%20and%20processing%0Aerrors%20and%20often%20fail%20to%20generalize%20to%20real-world%20or%20previously%20unseen%0Amovements%2C%20necessitating%20new%20data%20collection%20efforts%20that%20are%20time-consuming%0Aand%20impractical.%20To%20overcome%20these%20limitations%2C%20we%20propose%20SSPINNpose%2C%20a%0Aself-supervised%2C%20physics-informed%20neural%20network%20that%20estimates%20joint%0Akinematics%20and%20kinetics%20directly%20from%20IMU%20data%2C%20without%20requiring%20ground%20truth%0Alabels%20for%20training.%20We%20run%20the%20network%20output%20through%20a%20physics%20model%20of%20the%0Ahuman%20body%20to%20optimize%20physical%20plausibility%20and%20generate%20virtual%20measurement%0Adata.%20Using%20this%20virtual%20sensor%20data%2C%20the%20network%20is%20trained%20directly%20on%20the%0Ameasured%20sensor%20data%20instead%20of%20a%20ground%20truth.%20When%20compared%20to%20optical%20motion%0Acapture%2C%20SSPINNpose%20is%20able%20to%20accurately%20estimate%20joint%20angles%20and%20joint%0Amoments%20at%20an%20RMSD%20of%208.7%20deg%20and%204.9%20BWBH%25%2C%20respectively%2C%20for%20walking%20and%0Arunning%20at%20speeds%20up%20to%204.9%20m/s%20at%20a%20latency%20of%203.5%20ms.%20Furthermore%2C%20the%0Aframework%20demonstrates%20robustness%20across%20sparse%20sensor%20configurations%20and%20can%0Ainfer%20the%20anatomical%20locations%20of%20the%20sensors.%20These%20results%20underscore%20the%0Apotential%20of%20SSPINNpose%20as%20a%20scalable%20and%20adaptable%20solution%20for%20real-time%0Abiomechanical%20analysis%20in%20both%20laboratory%20and%20field%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSPINNpose%253A%2520A%2520Self-Supervised%2520PINN%2520for%2520Inertial%2520Pose%2520and%2520Dynamics%250A%2520%2520Estimation%26entry.906535625%3DMarkus%2520Gambietz%2520and%2520Eva%2520Dorschky%2520and%2520Altan%2520Akat%2520and%2520Marcel%2520Sch%25C3%25B6ckel%2520and%2520J%25C3%25B6rg%2520Miehling%2520and%2520Anne%2520D.%2520Koelewijn%26entry.1292438233%3D%2520%2520Accurate%2520real-time%2520estimation%2520of%2520human%2520movement%2520dynamics%252C%2520including%2520internal%250Ajoint%2520moments%2520and%2520muscle%2520forces%252C%2520is%2520essential%2520for%2520applications%2520in%2520clinical%250Adiagnostics%2520and%2520sports%2520performance%2520monitoring.%2520Inertial%2520measurement%2520units%250A%2528IMUs%2529%2520provide%2520a%2520minimally%2520intrusive%2520solution%2520for%2520capturing%2520motion%2520data%252C%250Aparticularly%2520when%2520used%2520in%2520sparse%2520sensor%2520configurations.%2520However%252C%2520current%250Areal-time%2520methods%2520rely%2520on%2520supervised%2520learning%252C%2520where%2520a%2520ground%2520truth%2520dataset%250Aneeds%2520to%2520be%2520measured%2520with%2520laboratory%2520measurement%2520systems%252C%2520such%2520as%2520optical%250Amotion%2520capture.%2520These%2520systems%2520are%2520known%2520to%2520introduce%2520measurement%2520and%2520processing%250Aerrors%2520and%2520often%2520fail%2520to%2520generalize%2520to%2520real-world%2520or%2520previously%2520unseen%250Amovements%252C%2520necessitating%2520new%2520data%2520collection%2520efforts%2520that%2520are%2520time-consuming%250Aand%2520impractical.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520SSPINNpose%252C%2520a%250Aself-supervised%252C%2520physics-informed%2520neural%2520network%2520that%2520estimates%2520joint%250Akinematics%2520and%2520kinetics%2520directly%2520from%2520IMU%2520data%252C%2520without%2520requiring%2520ground%2520truth%250Alabels%2520for%2520training.%2520We%2520run%2520the%2520network%2520output%2520through%2520a%2520physics%2520model%2520of%2520the%250Ahuman%2520body%2520to%2520optimize%2520physical%2520plausibility%2520and%2520generate%2520virtual%2520measurement%250Adata.%2520Using%2520this%2520virtual%2520sensor%2520data%252C%2520the%2520network%2520is%2520trained%2520directly%2520on%2520the%250Ameasured%2520sensor%2520data%2520instead%2520of%2520a%2520ground%2520truth.%2520When%2520compared%2520to%2520optical%2520motion%250Acapture%252C%2520SSPINNpose%2520is%2520able%2520to%2520accurately%2520estimate%2520joint%2520angles%2520and%2520joint%250Amoments%2520at%2520an%2520RMSD%2520of%25208.7%2520deg%2520and%25204.9%2520BWBH%2525%252C%2520respectively%252C%2520for%2520walking%2520and%250Arunning%2520at%2520speeds%2520up%2520to%25204.9%2520m/s%2520at%2520a%2520latency%2520of%25203.5%2520ms.%2520Furthermore%252C%2520the%250Aframework%2520demonstrates%2520robustness%2520across%2520sparse%2520sensor%2520configurations%2520and%2520can%250Ainfer%2520the%2520anatomical%2520locations%2520of%2520the%2520sensors.%2520These%2520results%2520underscore%2520the%250Apotential%2520of%2520SSPINNpose%2520as%2520a%2520scalable%2520and%2520adaptable%2520solution%2520for%2520real-time%250Abiomechanical%2520analysis%2520in%2520both%2520laboratory%2520and%2520field%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSPINNpose%3A%20A%20Self-Supervised%20PINN%20for%20Inertial%20Pose%20and%20Dynamics%0A%20%20Estimation&entry.906535625=Markus%20Gambietz%20and%20Eva%20Dorschky%20and%20Altan%20Akat%20and%20Marcel%20Sch%C3%B6ckel%20and%20J%C3%B6rg%20Miehling%20and%20Anne%20D.%20Koelewijn&entry.1292438233=%20%20Accurate%20real-time%20estimation%20of%20human%20movement%20dynamics%2C%20including%20internal%0Ajoint%20moments%20and%20muscle%20forces%2C%20is%20essential%20for%20applications%20in%20clinical%0Adiagnostics%20and%20sports%20performance%20monitoring.%20Inertial%20measurement%20units%0A%28IMUs%29%20provide%20a%20minimally%20intrusive%20solution%20for%20capturing%20motion%20data%2C%0Aparticularly%20when%20used%20in%20sparse%20sensor%20configurations.%20However%2C%20current%0Areal-time%20methods%20rely%20on%20supervised%20learning%2C%20where%20a%20ground%20truth%20dataset%0Aneeds%20to%20be%20measured%20with%20laboratory%20measurement%20systems%2C%20such%20as%20optical%0Amotion%20capture.%20These%20systems%20are%20known%20to%20introduce%20measurement%20and%20processing%0Aerrors%20and%20often%20fail%20to%20generalize%20to%20real-world%20or%20previously%20unseen%0Amovements%2C%20necessitating%20new%20data%20collection%20efforts%20that%20are%20time-consuming%0Aand%20impractical.%20To%20overcome%20these%20limitations%2C%20we%20propose%20SSPINNpose%2C%20a%0Aself-supervised%2C%20physics-informed%20neural%20network%20that%20estimates%20joint%0Akinematics%20and%20kinetics%20directly%20from%20IMU%20data%2C%20without%20requiring%20ground%20truth%0Alabels%20for%20training.%20We%20run%20the%20network%20output%20through%20a%20physics%20model%20of%20the%0Ahuman%20body%20to%20optimize%20physical%20plausibility%20and%20generate%20virtual%20measurement%0Adata.%20Using%20this%20virtual%20sensor%20data%2C%20the%20network%20is%20trained%20directly%20on%20the%0Ameasured%20sensor%20data%20instead%20of%20a%20ground%20truth.%20When%20compared%20to%20optical%20motion%0Acapture%2C%20SSPINNpose%20is%20able%20to%20accurately%20estimate%20joint%20angles%20and%20joint%0Amoments%20at%20an%20RMSD%20of%208.7%20deg%20and%204.9%20BWBH%25%2C%20respectively%2C%20for%20walking%20and%0Arunning%20at%20speeds%20up%20to%204.9%20m/s%20at%20a%20latency%20of%203.5%20ms.%20Furthermore%2C%20the%0Aframework%20demonstrates%20robustness%20across%20sparse%20sensor%20configurations%20and%20can%0Ainfer%20the%20anatomical%20locations%20of%20the%20sensors.%20These%20results%20underscore%20the%0Apotential%20of%20SSPINNpose%20as%20a%20scalable%20and%20adaptable%20solution%20for%20real-time%0Abiomechanical%20analysis%20in%20both%20laboratory%20and%20field%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11786v1&entry.124074799=Read"},
{"title": "Spectra-to-Structure and Structure-to-Spectra Inference Across the\n  Periodic Table", "author": "Yufeng Wang and Peiyao Wang and Lu Ma and Yuewei Lin and Qun Liu and Haibin Ling", "abstract": "  X-ray Absorption Spectroscopy (XAS) is a powerful technique for probing local\natomic environments, yet its interpretation remains limited by the need for\nexpert-driven analysis, computationally expensive simulations, and\nelement-specific heuristics. Recent advances in machine learning have shown\npromise for accelerating XAS interpretation, but many existing models are\nnarrowly focused on specific elements, edge types, or spectral regimes. In this\nwork, we present XAStruct, a learning framework capable of both predicting XAS\nspectra from crystal structures and inferring local structural descriptors from\nXAS input. XAStruct is trained on a large-scale dataset spanning over 70\nelements across the periodic table, enabling generalization to a wide variety\nof chemistries and bonding environments. The model includes the first machine\nlearning approach for predicting neighbor atom types directly from XAS spectra,\nas well as a unified regression model for mean nearest-neighbor distance that\nrequires no element-specific tuning. While we explored integrating the two\npipelines into a single end-to-end model, empirical results showed performance\ndegradation. As a result, the two tasks were trained independently to ensure\noptimal accuracy and task-specific performance. By combining deep neural\nnetworks for complex structure-property mappings with efficient baseline models\nfor simpler tasks, XAStruct offers a scalable and extensible solution for\ndata-driven XAS analysis and local structure inference. The source code will be\nreleased upon paper acceptance.\n", "link": "http://arxiv.org/abs/2506.11908v1", "date": "2025-06-13", "relevancy": 2.2831, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4598}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4598}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectra-to-Structure%20and%20Structure-to-Spectra%20Inference%20Across%20the%0A%20%20Periodic%20Table&body=Title%3A%20Spectra-to-Structure%20and%20Structure-to-Spectra%20Inference%20Across%20the%0A%20%20Periodic%20Table%0AAuthor%3A%20Yufeng%20Wang%20and%20Peiyao%20Wang%20and%20Lu%20Ma%20and%20Yuewei%20Lin%20and%20Qun%20Liu%20and%20Haibin%20Ling%0AAbstract%3A%20%20%20X-ray%20Absorption%20Spectroscopy%20%28XAS%29%20is%20a%20powerful%20technique%20for%20probing%20local%0Aatomic%20environments%2C%20yet%20its%20interpretation%20remains%20limited%20by%20the%20need%20for%0Aexpert-driven%20analysis%2C%20computationally%20expensive%20simulations%2C%20and%0Aelement-specific%20heuristics.%20Recent%20advances%20in%20machine%20learning%20have%20shown%0Apromise%20for%20accelerating%20XAS%20interpretation%2C%20but%20many%20existing%20models%20are%0Anarrowly%20focused%20on%20specific%20elements%2C%20edge%20types%2C%20or%20spectral%20regimes.%20In%20this%0Awork%2C%20we%20present%20XAStruct%2C%20a%20learning%20framework%20capable%20of%20both%20predicting%20XAS%0Aspectra%20from%20crystal%20structures%20and%20inferring%20local%20structural%20descriptors%20from%0AXAS%20input.%20XAStruct%20is%20trained%20on%20a%20large-scale%20dataset%20spanning%20over%2070%0Aelements%20across%20the%20periodic%20table%2C%20enabling%20generalization%20to%20a%20wide%20variety%0Aof%20chemistries%20and%20bonding%20environments.%20The%20model%20includes%20the%20first%20machine%0Alearning%20approach%20for%20predicting%20neighbor%20atom%20types%20directly%20from%20XAS%20spectra%2C%0Aas%20well%20as%20a%20unified%20regression%20model%20for%20mean%20nearest-neighbor%20distance%20that%0Arequires%20no%20element-specific%20tuning.%20While%20we%20explored%20integrating%20the%20two%0Apipelines%20into%20a%20single%20end-to-end%20model%2C%20empirical%20results%20showed%20performance%0Adegradation.%20As%20a%20result%2C%20the%20two%20tasks%20were%20trained%20independently%20to%20ensure%0Aoptimal%20accuracy%20and%20task-specific%20performance.%20By%20combining%20deep%20neural%0Anetworks%20for%20complex%20structure-property%20mappings%20with%20efficient%20baseline%20models%0Afor%20simpler%20tasks%2C%20XAStruct%20offers%20a%20scalable%20and%20extensible%20solution%20for%0Adata-driven%20XAS%20analysis%20and%20local%20structure%20inference.%20The%20source%20code%20will%20be%0Areleased%20upon%20paper%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectra-to-Structure%2520and%2520Structure-to-Spectra%2520Inference%2520Across%2520the%250A%2520%2520Periodic%2520Table%26entry.906535625%3DYufeng%2520Wang%2520and%2520Peiyao%2520Wang%2520and%2520Lu%2520Ma%2520and%2520Yuewei%2520Lin%2520and%2520Qun%2520Liu%2520and%2520Haibin%2520Ling%26entry.1292438233%3D%2520%2520X-ray%2520Absorption%2520Spectroscopy%2520%2528XAS%2529%2520is%2520a%2520powerful%2520technique%2520for%2520probing%2520local%250Aatomic%2520environments%252C%2520yet%2520its%2520interpretation%2520remains%2520limited%2520by%2520the%2520need%2520for%250Aexpert-driven%2520analysis%252C%2520computationally%2520expensive%2520simulations%252C%2520and%250Aelement-specific%2520heuristics.%2520Recent%2520advances%2520in%2520machine%2520learning%2520have%2520shown%250Apromise%2520for%2520accelerating%2520XAS%2520interpretation%252C%2520but%2520many%2520existing%2520models%2520are%250Anarrowly%2520focused%2520on%2520specific%2520elements%252C%2520edge%2520types%252C%2520or%2520spectral%2520regimes.%2520In%2520this%250Awork%252C%2520we%2520present%2520XAStruct%252C%2520a%2520learning%2520framework%2520capable%2520of%2520both%2520predicting%2520XAS%250Aspectra%2520from%2520crystal%2520structures%2520and%2520inferring%2520local%2520structural%2520descriptors%2520from%250AXAS%2520input.%2520XAStruct%2520is%2520trained%2520on%2520a%2520large-scale%2520dataset%2520spanning%2520over%252070%250Aelements%2520across%2520the%2520periodic%2520table%252C%2520enabling%2520generalization%2520to%2520a%2520wide%2520variety%250Aof%2520chemistries%2520and%2520bonding%2520environments.%2520The%2520model%2520includes%2520the%2520first%2520machine%250Alearning%2520approach%2520for%2520predicting%2520neighbor%2520atom%2520types%2520directly%2520from%2520XAS%2520spectra%252C%250Aas%2520well%2520as%2520a%2520unified%2520regression%2520model%2520for%2520mean%2520nearest-neighbor%2520distance%2520that%250Arequires%2520no%2520element-specific%2520tuning.%2520While%2520we%2520explored%2520integrating%2520the%2520two%250Apipelines%2520into%2520a%2520single%2520end-to-end%2520model%252C%2520empirical%2520results%2520showed%2520performance%250Adegradation.%2520As%2520a%2520result%252C%2520the%2520two%2520tasks%2520were%2520trained%2520independently%2520to%2520ensure%250Aoptimal%2520accuracy%2520and%2520task-specific%2520performance.%2520By%2520combining%2520deep%2520neural%250Anetworks%2520for%2520complex%2520structure-property%2520mappings%2520with%2520efficient%2520baseline%2520models%250Afor%2520simpler%2520tasks%252C%2520XAStruct%2520offers%2520a%2520scalable%2520and%2520extensible%2520solution%2520for%250Adata-driven%2520XAS%2520analysis%2520and%2520local%2520structure%2520inference.%2520The%2520source%2520code%2520will%2520be%250Areleased%2520upon%2520paper%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectra-to-Structure%20and%20Structure-to-Spectra%20Inference%20Across%20the%0A%20%20Periodic%20Table&entry.906535625=Yufeng%20Wang%20and%20Peiyao%20Wang%20and%20Lu%20Ma%20and%20Yuewei%20Lin%20and%20Qun%20Liu%20and%20Haibin%20Ling&entry.1292438233=%20%20X-ray%20Absorption%20Spectroscopy%20%28XAS%29%20is%20a%20powerful%20technique%20for%20probing%20local%0Aatomic%20environments%2C%20yet%20its%20interpretation%20remains%20limited%20by%20the%20need%20for%0Aexpert-driven%20analysis%2C%20computationally%20expensive%20simulations%2C%20and%0Aelement-specific%20heuristics.%20Recent%20advances%20in%20machine%20learning%20have%20shown%0Apromise%20for%20accelerating%20XAS%20interpretation%2C%20but%20many%20existing%20models%20are%0Anarrowly%20focused%20on%20specific%20elements%2C%20edge%20types%2C%20or%20spectral%20regimes.%20In%20this%0Awork%2C%20we%20present%20XAStruct%2C%20a%20learning%20framework%20capable%20of%20both%20predicting%20XAS%0Aspectra%20from%20crystal%20structures%20and%20inferring%20local%20structural%20descriptors%20from%0AXAS%20input.%20XAStruct%20is%20trained%20on%20a%20large-scale%20dataset%20spanning%20over%2070%0Aelements%20across%20the%20periodic%20table%2C%20enabling%20generalization%20to%20a%20wide%20variety%0Aof%20chemistries%20and%20bonding%20environments.%20The%20model%20includes%20the%20first%20machine%0Alearning%20approach%20for%20predicting%20neighbor%20atom%20types%20directly%20from%20XAS%20spectra%2C%0Aas%20well%20as%20a%20unified%20regression%20model%20for%20mean%20nearest-neighbor%20distance%20that%0Arequires%20no%20element-specific%20tuning.%20While%20we%20explored%20integrating%20the%20two%0Apipelines%20into%20a%20single%20end-to-end%20model%2C%20empirical%20results%20showed%20performance%0Adegradation.%20As%20a%20result%2C%20the%20two%20tasks%20were%20trained%20independently%20to%20ensure%0Aoptimal%20accuracy%20and%20task-specific%20performance.%20By%20combining%20deep%20neural%0Anetworks%20for%20complex%20structure-property%20mappings%20with%20efficient%20baseline%20models%0Afor%20simpler%20tasks%2C%20XAStruct%20offers%20a%20scalable%20and%20extensible%20solution%20for%0Adata-driven%20XAS%20analysis%20and%20local%20structure%20inference.%20The%20source%20code%20will%20be%0Areleased%20upon%20paper%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11908v1&entry.124074799=Read"},
{"title": "New Dataset and Methods for Fine-Grained Compositional Referring\n  Expression Comprehension via Specialist-MLLM Collaboration", "author": "Xuzheng Yang and Junzhuo Liu and Peng Wang and Guoqing Wang and Yang Yang and Heng Tao Shen", "abstract": "  Referring Expression Comprehension (REC) is a foundational cross-modal task\nthat evaluates the interplay of language understanding, image comprehension,\nand language-to-image grounding. It serves as an essential testing ground for\nMultimodal Large Language Models (MLLMs). To advance this field, we introduced\na new REC dataset in our previous conference paper, characterized by two key\nfeatures. First, it is designed with controllable difficulty levels, requiring\nmulti-level fine-grained reasoning across object categories, attributes, and\nmulti-hop relationships. Second, it incorporates negative text and images\ngenerated through fine-grained editing and augmentation, explicitly testing a\nmodel's ability to reject scenarios where the target object is absent, an often\noverlooked yet critical challenge in existing datasets. In this extended work,\nwe propose two new methods to tackle the challenges of fine-grained REC by\ncombining the strengths of Specialist Models and MLLMs. The first method\nadaptively assigns simple cases to faster, lightweight models and reserves\ncomplex ones for powerful MLLMs, balancing accuracy and efficiency. The second\nmethod lets a specialist generate a set of possible object regions, and the\nMLLM selects the most plausible one using its reasoning ability. These\ncollaborative strategies lead to significant improvements on our dataset and\nother challenging benchmarks. Our results show that combining specialized and\ngeneral-purpose models offers a practical path toward solving complex\nreal-world vision-language tasks. Our dataset and code are available at\nhttps://github.com/sleepyshep/FineCops-Ref.\n", "link": "http://arxiv.org/abs/2502.20104v3", "date": "2025-06-13", "relevancy": 2.266, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.573}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20New%20Dataset%20and%20Methods%20for%20Fine-Grained%20Compositional%20Referring%0A%20%20Expression%20Comprehension%20via%20Specialist-MLLM%20Collaboration&body=Title%3A%20New%20Dataset%20and%20Methods%20for%20Fine-Grained%20Compositional%20Referring%0A%20%20Expression%20Comprehension%20via%20Specialist-MLLM%20Collaboration%0AAuthor%3A%20Xuzheng%20Yang%20and%20Junzhuo%20Liu%20and%20Peng%20Wang%20and%20Guoqing%20Wang%20and%20Yang%20Yang%20and%20Heng%20Tao%20Shen%0AAbstract%3A%20%20%20Referring%20Expression%20Comprehension%20%28REC%29%20is%20a%20foundational%20cross-modal%20task%0Athat%20evaluates%20the%20interplay%20of%20language%20understanding%2C%20image%20comprehension%2C%0Aand%20language-to-image%20grounding.%20It%20serves%20as%20an%20essential%20testing%20ground%20for%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29.%20To%20advance%20this%20field%2C%20we%20introduced%0Aa%20new%20REC%20dataset%20in%20our%20previous%20conference%20paper%2C%20characterized%20by%20two%20key%0Afeatures.%20First%2C%20it%20is%20designed%20with%20controllable%20difficulty%20levels%2C%20requiring%0Amulti-level%20fine-grained%20reasoning%20across%20object%20categories%2C%20attributes%2C%20and%0Amulti-hop%20relationships.%20Second%2C%20it%20incorporates%20negative%20text%20and%20images%0Agenerated%20through%20fine-grained%20editing%20and%20augmentation%2C%20explicitly%20testing%20a%0Amodel%27s%20ability%20to%20reject%20scenarios%20where%20the%20target%20object%20is%20absent%2C%20an%20often%0Aoverlooked%20yet%20critical%20challenge%20in%20existing%20datasets.%20In%20this%20extended%20work%2C%0Awe%20propose%20two%20new%20methods%20to%20tackle%20the%20challenges%20of%20fine-grained%20REC%20by%0Acombining%20the%20strengths%20of%20Specialist%20Models%20and%20MLLMs.%20The%20first%20method%0Aadaptively%20assigns%20simple%20cases%20to%20faster%2C%20lightweight%20models%20and%20reserves%0Acomplex%20ones%20for%20powerful%20MLLMs%2C%20balancing%20accuracy%20and%20efficiency.%20The%20second%0Amethod%20lets%20a%20specialist%20generate%20a%20set%20of%20possible%20object%20regions%2C%20and%20the%0AMLLM%20selects%20the%20most%20plausible%20one%20using%20its%20reasoning%20ability.%20These%0Acollaborative%20strategies%20lead%20to%20significant%20improvements%20on%20our%20dataset%20and%0Aother%20challenging%20benchmarks.%20Our%20results%20show%20that%20combining%20specialized%20and%0Ageneral-purpose%20models%20offers%20a%20practical%20path%20toward%20solving%20complex%0Areal-world%20vision-language%20tasks.%20Our%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/sleepyshep/FineCops-Ref.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20104v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNew%2520Dataset%2520and%2520Methods%2520for%2520Fine-Grained%2520Compositional%2520Referring%250A%2520%2520Expression%2520Comprehension%2520via%2520Specialist-MLLM%2520Collaboration%26entry.906535625%3DXuzheng%2520Yang%2520and%2520Junzhuo%2520Liu%2520and%2520Peng%2520Wang%2520and%2520Guoqing%2520Wang%2520and%2520Yang%2520Yang%2520and%2520Heng%2520Tao%2520Shen%26entry.1292438233%3D%2520%2520Referring%2520Expression%2520Comprehension%2520%2528REC%2529%2520is%2520a%2520foundational%2520cross-modal%2520task%250Athat%2520evaluates%2520the%2520interplay%2520of%2520language%2520understanding%252C%2520image%2520comprehension%252C%250Aand%2520language-to-image%2520grounding.%2520It%2520serves%2520as%2520an%2520essential%2520testing%2520ground%2520for%250AMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520To%2520advance%2520this%2520field%252C%2520we%2520introduced%250Aa%2520new%2520REC%2520dataset%2520in%2520our%2520previous%2520conference%2520paper%252C%2520characterized%2520by%2520two%2520key%250Afeatures.%2520First%252C%2520it%2520is%2520designed%2520with%2520controllable%2520difficulty%2520levels%252C%2520requiring%250Amulti-level%2520fine-grained%2520reasoning%2520across%2520object%2520categories%252C%2520attributes%252C%2520and%250Amulti-hop%2520relationships.%2520Second%252C%2520it%2520incorporates%2520negative%2520text%2520and%2520images%250Agenerated%2520through%2520fine-grained%2520editing%2520and%2520augmentation%252C%2520explicitly%2520testing%2520a%250Amodel%2527s%2520ability%2520to%2520reject%2520scenarios%2520where%2520the%2520target%2520object%2520is%2520absent%252C%2520an%2520often%250Aoverlooked%2520yet%2520critical%2520challenge%2520in%2520existing%2520datasets.%2520In%2520this%2520extended%2520work%252C%250Awe%2520propose%2520two%2520new%2520methods%2520to%2520tackle%2520the%2520challenges%2520of%2520fine-grained%2520REC%2520by%250Acombining%2520the%2520strengths%2520of%2520Specialist%2520Models%2520and%2520MLLMs.%2520The%2520first%2520method%250Aadaptively%2520assigns%2520simple%2520cases%2520to%2520faster%252C%2520lightweight%2520models%2520and%2520reserves%250Acomplex%2520ones%2520for%2520powerful%2520MLLMs%252C%2520balancing%2520accuracy%2520and%2520efficiency.%2520The%2520second%250Amethod%2520lets%2520a%2520specialist%2520generate%2520a%2520set%2520of%2520possible%2520object%2520regions%252C%2520and%2520the%250AMLLM%2520selects%2520the%2520most%2520plausible%2520one%2520using%2520its%2520reasoning%2520ability.%2520These%250Acollaborative%2520strategies%2520lead%2520to%2520significant%2520improvements%2520on%2520our%2520dataset%2520and%250Aother%2520challenging%2520benchmarks.%2520Our%2520results%2520show%2520that%2520combining%2520specialized%2520and%250Ageneral-purpose%2520models%2520offers%2520a%2520practical%2520path%2520toward%2520solving%2520complex%250Areal-world%2520vision-language%2520tasks.%2520Our%2520dataset%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/sleepyshep/FineCops-Ref.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20104v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=New%20Dataset%20and%20Methods%20for%20Fine-Grained%20Compositional%20Referring%0A%20%20Expression%20Comprehension%20via%20Specialist-MLLM%20Collaboration&entry.906535625=Xuzheng%20Yang%20and%20Junzhuo%20Liu%20and%20Peng%20Wang%20and%20Guoqing%20Wang%20and%20Yang%20Yang%20and%20Heng%20Tao%20Shen&entry.1292438233=%20%20Referring%20Expression%20Comprehension%20%28REC%29%20is%20a%20foundational%20cross-modal%20task%0Athat%20evaluates%20the%20interplay%20of%20language%20understanding%2C%20image%20comprehension%2C%0Aand%20language-to-image%20grounding.%20It%20serves%20as%20an%20essential%20testing%20ground%20for%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29.%20To%20advance%20this%20field%2C%20we%20introduced%0Aa%20new%20REC%20dataset%20in%20our%20previous%20conference%20paper%2C%20characterized%20by%20two%20key%0Afeatures.%20First%2C%20it%20is%20designed%20with%20controllable%20difficulty%20levels%2C%20requiring%0Amulti-level%20fine-grained%20reasoning%20across%20object%20categories%2C%20attributes%2C%20and%0Amulti-hop%20relationships.%20Second%2C%20it%20incorporates%20negative%20text%20and%20images%0Agenerated%20through%20fine-grained%20editing%20and%20augmentation%2C%20explicitly%20testing%20a%0Amodel%27s%20ability%20to%20reject%20scenarios%20where%20the%20target%20object%20is%20absent%2C%20an%20often%0Aoverlooked%20yet%20critical%20challenge%20in%20existing%20datasets.%20In%20this%20extended%20work%2C%0Awe%20propose%20two%20new%20methods%20to%20tackle%20the%20challenges%20of%20fine-grained%20REC%20by%0Acombining%20the%20strengths%20of%20Specialist%20Models%20and%20MLLMs.%20The%20first%20method%0Aadaptively%20assigns%20simple%20cases%20to%20faster%2C%20lightweight%20models%20and%20reserves%0Acomplex%20ones%20for%20powerful%20MLLMs%2C%20balancing%20accuracy%20and%20efficiency.%20The%20second%0Amethod%20lets%20a%20specialist%20generate%20a%20set%20of%20possible%20object%20regions%2C%20and%20the%0AMLLM%20selects%20the%20most%20plausible%20one%20using%20its%20reasoning%20ability.%20These%0Acollaborative%20strategies%20lead%20to%20significant%20improvements%20on%20our%20dataset%20and%0Aother%20challenging%20benchmarks.%20Our%20results%20show%20that%20combining%20specialized%20and%0Ageneral-purpose%20models%20offers%20a%20practical%20path%20toward%20solving%20complex%0Areal-world%20vision-language%20tasks.%20Our%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/sleepyshep/FineCops-Ref.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20104v3&entry.124074799=Read"},
{"title": "MambaVSR: Content-Aware Scanning State Space Model for Video\n  Super-Resolution", "author": "Linfeng He and Meiqin Liu and Qi Tang and Chao Yao and Yao Zhao", "abstract": "  Video super-resolution (VSR) faces critical challenges in effectively\nmodeling non-local dependencies across misaligned frames while preserving\ncomputational efficiency. Existing VSR methods typically rely on optical flow\nstrategies or transformer architectures, which struggle with large motion\ndisplacements and long video sequences. To address this, we propose MambaVSR,\nthe first state-space model framework for VSR that incorporates an innovative\ncontent-aware scanning mechanism. Unlike rigid 1D sequential processing in\nconventional vision Mamba methods, our MambaVSR enables dynamic spatiotemporal\ninteractions through the Shared Compass Construction (SCC) and the\nContent-Aware Sequentialization (CAS). Specifically, the SCC module constructs\nintra-frame semantic connectivity graphs via efficient sparse attention and\ngenerates adaptive spatial scanning sequences through spectral clustering.\nBuilding upon SCC, the CAS module effectively aligns and aggregates non-local\nsimilar content across multiple frames by interleaving temporal features along\nthe learned spatial order. To bridge global dependencies with local details,\nthe Global-Local State Space Block (GLSSB) synergistically integrates window\nself-attention operations with SSM-based feature propagation, enabling\nhigh-frequency detail recovery under global dependency guidance. Extensive\nexperiments validate MambaVSR's superiority, outperforming the\nTransformer-based method by 0.58 dB PSNR on the REDS dataset with 55% fewer\nparameters.\n", "link": "http://arxiv.org/abs/2506.11768v1", "date": "2025-06-13", "relevancy": 2.252, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5844}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5667}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaVSR%3A%20Content-Aware%20Scanning%20State%20Space%20Model%20for%20Video%0A%20%20Super-Resolution&body=Title%3A%20MambaVSR%3A%20Content-Aware%20Scanning%20State%20Space%20Model%20for%20Video%0A%20%20Super-Resolution%0AAuthor%3A%20Linfeng%20He%20and%20Meiqin%20Liu%20and%20Qi%20Tang%20and%20Chao%20Yao%20and%20Yao%20Zhao%0AAbstract%3A%20%20%20Video%20super-resolution%20%28VSR%29%20faces%20critical%20challenges%20in%20effectively%0Amodeling%20non-local%20dependencies%20across%20misaligned%20frames%20while%20preserving%0Acomputational%20efficiency.%20Existing%20VSR%20methods%20typically%20rely%20on%20optical%20flow%0Astrategies%20or%20transformer%20architectures%2C%20which%20struggle%20with%20large%20motion%0Adisplacements%20and%20long%20video%20sequences.%20To%20address%20this%2C%20we%20propose%20MambaVSR%2C%0Athe%20first%20state-space%20model%20framework%20for%20VSR%20that%20incorporates%20an%20innovative%0Acontent-aware%20scanning%20mechanism.%20Unlike%20rigid%201D%20sequential%20processing%20in%0Aconventional%20vision%20Mamba%20methods%2C%20our%20MambaVSR%20enables%20dynamic%20spatiotemporal%0Ainteractions%20through%20the%20Shared%20Compass%20Construction%20%28SCC%29%20and%20the%0AContent-Aware%20Sequentialization%20%28CAS%29.%20Specifically%2C%20the%20SCC%20module%20constructs%0Aintra-frame%20semantic%20connectivity%20graphs%20via%20efficient%20sparse%20attention%20and%0Agenerates%20adaptive%20spatial%20scanning%20sequences%20through%20spectral%20clustering.%0ABuilding%20upon%20SCC%2C%20the%20CAS%20module%20effectively%20aligns%20and%20aggregates%20non-local%0Asimilar%20content%20across%20multiple%20frames%20by%20interleaving%20temporal%20features%20along%0Athe%20learned%20spatial%20order.%20To%20bridge%20global%20dependencies%20with%20local%20details%2C%0Athe%20Global-Local%20State%20Space%20Block%20%28GLSSB%29%20synergistically%20integrates%20window%0Aself-attention%20operations%20with%20SSM-based%20feature%20propagation%2C%20enabling%0Ahigh-frequency%20detail%20recovery%20under%20global%20dependency%20guidance.%20Extensive%0Aexperiments%20validate%20MambaVSR%27s%20superiority%2C%20outperforming%20the%0ATransformer-based%20method%20by%200.58%20dB%20PSNR%20on%20the%20REDS%20dataset%20with%2055%25%20fewer%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaVSR%253A%2520Content-Aware%2520Scanning%2520State%2520Space%2520Model%2520for%2520Video%250A%2520%2520Super-Resolution%26entry.906535625%3DLinfeng%2520He%2520and%2520Meiqin%2520Liu%2520and%2520Qi%2520Tang%2520and%2520Chao%2520Yao%2520and%2520Yao%2520Zhao%26entry.1292438233%3D%2520%2520Video%2520super-resolution%2520%2528VSR%2529%2520faces%2520critical%2520challenges%2520in%2520effectively%250Amodeling%2520non-local%2520dependencies%2520across%2520misaligned%2520frames%2520while%2520preserving%250Acomputational%2520efficiency.%2520Existing%2520VSR%2520methods%2520typically%2520rely%2520on%2520optical%2520flow%250Astrategies%2520or%2520transformer%2520architectures%252C%2520which%2520struggle%2520with%2520large%2520motion%250Adisplacements%2520and%2520long%2520video%2520sequences.%2520To%2520address%2520this%252C%2520we%2520propose%2520MambaVSR%252C%250Athe%2520first%2520state-space%2520model%2520framework%2520for%2520VSR%2520that%2520incorporates%2520an%2520innovative%250Acontent-aware%2520scanning%2520mechanism.%2520Unlike%2520rigid%25201D%2520sequential%2520processing%2520in%250Aconventional%2520vision%2520Mamba%2520methods%252C%2520our%2520MambaVSR%2520enables%2520dynamic%2520spatiotemporal%250Ainteractions%2520through%2520the%2520Shared%2520Compass%2520Construction%2520%2528SCC%2529%2520and%2520the%250AContent-Aware%2520Sequentialization%2520%2528CAS%2529.%2520Specifically%252C%2520the%2520SCC%2520module%2520constructs%250Aintra-frame%2520semantic%2520connectivity%2520graphs%2520via%2520efficient%2520sparse%2520attention%2520and%250Agenerates%2520adaptive%2520spatial%2520scanning%2520sequences%2520through%2520spectral%2520clustering.%250ABuilding%2520upon%2520SCC%252C%2520the%2520CAS%2520module%2520effectively%2520aligns%2520and%2520aggregates%2520non-local%250Asimilar%2520content%2520across%2520multiple%2520frames%2520by%2520interleaving%2520temporal%2520features%2520along%250Athe%2520learned%2520spatial%2520order.%2520To%2520bridge%2520global%2520dependencies%2520with%2520local%2520details%252C%250Athe%2520Global-Local%2520State%2520Space%2520Block%2520%2528GLSSB%2529%2520synergistically%2520integrates%2520window%250Aself-attention%2520operations%2520with%2520SSM-based%2520feature%2520propagation%252C%2520enabling%250Ahigh-frequency%2520detail%2520recovery%2520under%2520global%2520dependency%2520guidance.%2520Extensive%250Aexperiments%2520validate%2520MambaVSR%2527s%2520superiority%252C%2520outperforming%2520the%250ATransformer-based%2520method%2520by%25200.58%2520dB%2520PSNR%2520on%2520the%2520REDS%2520dataset%2520with%252055%2525%2520fewer%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaVSR%3A%20Content-Aware%20Scanning%20State%20Space%20Model%20for%20Video%0A%20%20Super-Resolution&entry.906535625=Linfeng%20He%20and%20Meiqin%20Liu%20and%20Qi%20Tang%20and%20Chao%20Yao%20and%20Yao%20Zhao&entry.1292438233=%20%20Video%20super-resolution%20%28VSR%29%20faces%20critical%20challenges%20in%20effectively%0Amodeling%20non-local%20dependencies%20across%20misaligned%20frames%20while%20preserving%0Acomputational%20efficiency.%20Existing%20VSR%20methods%20typically%20rely%20on%20optical%20flow%0Astrategies%20or%20transformer%20architectures%2C%20which%20struggle%20with%20large%20motion%0Adisplacements%20and%20long%20video%20sequences.%20To%20address%20this%2C%20we%20propose%20MambaVSR%2C%0Athe%20first%20state-space%20model%20framework%20for%20VSR%20that%20incorporates%20an%20innovative%0Acontent-aware%20scanning%20mechanism.%20Unlike%20rigid%201D%20sequential%20processing%20in%0Aconventional%20vision%20Mamba%20methods%2C%20our%20MambaVSR%20enables%20dynamic%20spatiotemporal%0Ainteractions%20through%20the%20Shared%20Compass%20Construction%20%28SCC%29%20and%20the%0AContent-Aware%20Sequentialization%20%28CAS%29.%20Specifically%2C%20the%20SCC%20module%20constructs%0Aintra-frame%20semantic%20connectivity%20graphs%20via%20efficient%20sparse%20attention%20and%0Agenerates%20adaptive%20spatial%20scanning%20sequences%20through%20spectral%20clustering.%0ABuilding%20upon%20SCC%2C%20the%20CAS%20module%20effectively%20aligns%20and%20aggregates%20non-local%0Asimilar%20content%20across%20multiple%20frames%20by%20interleaving%20temporal%20features%20along%0Athe%20learned%20spatial%20order.%20To%20bridge%20global%20dependencies%20with%20local%20details%2C%0Athe%20Global-Local%20State%20Space%20Block%20%28GLSSB%29%20synergistically%20integrates%20window%0Aself-attention%20operations%20with%20SSM-based%20feature%20propagation%2C%20enabling%0Ahigh-frequency%20detail%20recovery%20under%20global%20dependency%20guidance.%20Extensive%0Aexperiments%20validate%20MambaVSR%27s%20superiority%2C%20outperforming%20the%0ATransformer-based%20method%20by%200.58%20dB%20PSNR%20on%20the%20REDS%20dataset%20with%2055%25%20fewer%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11768v1&entry.124074799=Read"},
{"title": "YOLO advances to its genesis: a decadal and comprehensive review of the\n  You Only Look Once (YOLO) series", "author": "Ranjan Sapkota and Marco Flores Calero and Rizwan Qureshi and Chetan Badgujar and Upesh Nepal and Alwin Poulose and Peter Zeno and Uday Bhanu Prakash Vaddevolu and Sheheryar Khan and Maged Shoman and Hong Yan and Manoj Karkee", "abstract": "  This review systematically examines the progression of the You Only Look Once\n(YOLO) object detection algorithms from YOLOv1 to the recently unveiled\nYOLOv12. Employing a reverse chronological analysis, this study examines the\nadvancements introduced by YOLO algorithms, beginning with YOLOv12 and\nprogressing through YOLO11 (or YOLOv11), YOLOv10, YOLOv9, YOLOv8, and\nsubsequent versions to explore each version's contributions to enhancing speed,\ndetection accuracy, and computational efficiency in real-time object detection.\nAdditionally, this study reviews the alternative versions derived from YOLO\narchitectural advancements of YOLO-NAS, YOLO-X, YOLO-R, DAMO-YOLO, and\nGold-YOLO. Moreover, the study highlights the transformative impact of YOLO\nmodels across five critical application areas: autonomous vehicles and traffic\nsafety, healthcare and medical imaging, industrial manufacturing, surveillance\nand security, and agriculture. By detailing the incremental technological\nadvancements in subsequent YOLO versions, this review chronicles the evolution\nof YOLO, and discusses the challenges and limitations in each of the earlier\nversions. The evolution signifies a path towards integrating YOLO with\nmultimodal, context-aware, and Artificial General Intelligence (AGI) systems\nfor the next YOLO decade, promising significant implications for future\ndevelopments in AI-driven applications. YOLO Review, YOLO Advances, YOLOv13,\nYOLOv14, YOLOv15, YOLOv16, YOLOv17, YOLOv18, YOLOv19, YOLOv20, YOLO review,\nYOLO Object Detection\n", "link": "http://arxiv.org/abs/2406.19407v8", "date": "2025-06-13", "relevancy": 2.244, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4633}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOLO%20advances%20to%20its%20genesis%3A%20a%20decadal%20and%20comprehensive%20review%20of%20the%0A%20%20You%20Only%20Look%20Once%20%28YOLO%29%20series&body=Title%3A%20YOLO%20advances%20to%20its%20genesis%3A%20a%20decadal%20and%20comprehensive%20review%20of%20the%0A%20%20You%20Only%20Look%20Once%20%28YOLO%29%20series%0AAuthor%3A%20Ranjan%20Sapkota%20and%20Marco%20Flores%20Calero%20and%20Rizwan%20Qureshi%20and%20Chetan%20Badgujar%20and%20Upesh%20Nepal%20and%20Alwin%20Poulose%20and%20Peter%20Zeno%20and%20Uday%20Bhanu%20Prakash%20Vaddevolu%20and%20Sheheryar%20Khan%20and%20Maged%20Shoman%20and%20Hong%20Yan%20and%20Manoj%20Karkee%0AAbstract%3A%20%20%20This%20review%20systematically%20examines%20the%20progression%20of%20the%20You%20Only%20Look%20Once%0A%28YOLO%29%20object%20detection%20algorithms%20from%20YOLOv1%20to%20the%20recently%20unveiled%0AYOLOv12.%20Employing%20a%20reverse%20chronological%20analysis%2C%20this%20study%20examines%20the%0Aadvancements%20introduced%20by%20YOLO%20algorithms%2C%20beginning%20with%20YOLOv12%20and%0Aprogressing%20through%20YOLO11%20%28or%20YOLOv11%29%2C%20YOLOv10%2C%20YOLOv9%2C%20YOLOv8%2C%20and%0Asubsequent%20versions%20to%20explore%20each%20version%27s%20contributions%20to%20enhancing%20speed%2C%0Adetection%20accuracy%2C%20and%20computational%20efficiency%20in%20real-time%20object%20detection.%0AAdditionally%2C%20this%20study%20reviews%20the%20alternative%20versions%20derived%20from%20YOLO%0Aarchitectural%20advancements%20of%20YOLO-NAS%2C%20YOLO-X%2C%20YOLO-R%2C%20DAMO-YOLO%2C%20and%0AGold-YOLO.%20Moreover%2C%20the%20study%20highlights%20the%20transformative%20impact%20of%20YOLO%0Amodels%20across%20five%20critical%20application%20areas%3A%20autonomous%20vehicles%20and%20traffic%0Asafety%2C%20healthcare%20and%20medical%20imaging%2C%20industrial%20manufacturing%2C%20surveillance%0Aand%20security%2C%20and%20agriculture.%20By%20detailing%20the%20incremental%20technological%0Aadvancements%20in%20subsequent%20YOLO%20versions%2C%20this%20review%20chronicles%20the%20evolution%0Aof%20YOLO%2C%20and%20discusses%20the%20challenges%20and%20limitations%20in%20each%20of%20the%20earlier%0Aversions.%20The%20evolution%20signifies%20a%20path%20towards%20integrating%20YOLO%20with%0Amultimodal%2C%20context-aware%2C%20and%20Artificial%20General%20Intelligence%20%28AGI%29%20systems%0Afor%20the%20next%20YOLO%20decade%2C%20promising%20significant%20implications%20for%20future%0Adevelopments%20in%20AI-driven%20applications.%20YOLO%20Review%2C%20YOLO%20Advances%2C%20YOLOv13%2C%0AYOLOv14%2C%20YOLOv15%2C%20YOLOv16%2C%20YOLOv17%2C%20YOLOv18%2C%20YOLOv19%2C%20YOLOv20%2C%20YOLO%20review%2C%0AYOLO%20Object%20Detection%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19407v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOLO%2520advances%2520to%2520its%2520genesis%253A%2520a%2520decadal%2520and%2520comprehensive%2520review%2520of%2520the%250A%2520%2520You%2520Only%2520Look%2520Once%2520%2528YOLO%2529%2520series%26entry.906535625%3DRanjan%2520Sapkota%2520and%2520Marco%2520Flores%2520Calero%2520and%2520Rizwan%2520Qureshi%2520and%2520Chetan%2520Badgujar%2520and%2520Upesh%2520Nepal%2520and%2520Alwin%2520Poulose%2520and%2520Peter%2520Zeno%2520and%2520Uday%2520Bhanu%2520Prakash%2520Vaddevolu%2520and%2520Sheheryar%2520Khan%2520and%2520Maged%2520Shoman%2520and%2520Hong%2520Yan%2520and%2520Manoj%2520Karkee%26entry.1292438233%3D%2520%2520This%2520review%2520systematically%2520examines%2520the%2520progression%2520of%2520the%2520You%2520Only%2520Look%2520Once%250A%2528YOLO%2529%2520object%2520detection%2520algorithms%2520from%2520YOLOv1%2520to%2520the%2520recently%2520unveiled%250AYOLOv12.%2520Employing%2520a%2520reverse%2520chronological%2520analysis%252C%2520this%2520study%2520examines%2520the%250Aadvancements%2520introduced%2520by%2520YOLO%2520algorithms%252C%2520beginning%2520with%2520YOLOv12%2520and%250Aprogressing%2520through%2520YOLO11%2520%2528or%2520YOLOv11%2529%252C%2520YOLOv10%252C%2520YOLOv9%252C%2520YOLOv8%252C%2520and%250Asubsequent%2520versions%2520to%2520explore%2520each%2520version%2527s%2520contributions%2520to%2520enhancing%2520speed%252C%250Adetection%2520accuracy%252C%2520and%2520computational%2520efficiency%2520in%2520real-time%2520object%2520detection.%250AAdditionally%252C%2520this%2520study%2520reviews%2520the%2520alternative%2520versions%2520derived%2520from%2520YOLO%250Aarchitectural%2520advancements%2520of%2520YOLO-NAS%252C%2520YOLO-X%252C%2520YOLO-R%252C%2520DAMO-YOLO%252C%2520and%250AGold-YOLO.%2520Moreover%252C%2520the%2520study%2520highlights%2520the%2520transformative%2520impact%2520of%2520YOLO%250Amodels%2520across%2520five%2520critical%2520application%2520areas%253A%2520autonomous%2520vehicles%2520and%2520traffic%250Asafety%252C%2520healthcare%2520and%2520medical%2520imaging%252C%2520industrial%2520manufacturing%252C%2520surveillance%250Aand%2520security%252C%2520and%2520agriculture.%2520By%2520detailing%2520the%2520incremental%2520technological%250Aadvancements%2520in%2520subsequent%2520YOLO%2520versions%252C%2520this%2520review%2520chronicles%2520the%2520evolution%250Aof%2520YOLO%252C%2520and%2520discusses%2520the%2520challenges%2520and%2520limitations%2520in%2520each%2520of%2520the%2520earlier%250Aversions.%2520The%2520evolution%2520signifies%2520a%2520path%2520towards%2520integrating%2520YOLO%2520with%250Amultimodal%252C%2520context-aware%252C%2520and%2520Artificial%2520General%2520Intelligence%2520%2528AGI%2529%2520systems%250Afor%2520the%2520next%2520YOLO%2520decade%252C%2520promising%2520significant%2520implications%2520for%2520future%250Adevelopments%2520in%2520AI-driven%2520applications.%2520YOLO%2520Review%252C%2520YOLO%2520Advances%252C%2520YOLOv13%252C%250AYOLOv14%252C%2520YOLOv15%252C%2520YOLOv16%252C%2520YOLOv17%252C%2520YOLOv18%252C%2520YOLOv19%252C%2520YOLOv20%252C%2520YOLO%2520review%252C%250AYOLO%2520Object%2520Detection%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19407v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLO%20advances%20to%20its%20genesis%3A%20a%20decadal%20and%20comprehensive%20review%20of%20the%0A%20%20You%20Only%20Look%20Once%20%28YOLO%29%20series&entry.906535625=Ranjan%20Sapkota%20and%20Marco%20Flores%20Calero%20and%20Rizwan%20Qureshi%20and%20Chetan%20Badgujar%20and%20Upesh%20Nepal%20and%20Alwin%20Poulose%20and%20Peter%20Zeno%20and%20Uday%20Bhanu%20Prakash%20Vaddevolu%20and%20Sheheryar%20Khan%20and%20Maged%20Shoman%20and%20Hong%20Yan%20and%20Manoj%20Karkee&entry.1292438233=%20%20This%20review%20systematically%20examines%20the%20progression%20of%20the%20You%20Only%20Look%20Once%0A%28YOLO%29%20object%20detection%20algorithms%20from%20YOLOv1%20to%20the%20recently%20unveiled%0AYOLOv12.%20Employing%20a%20reverse%20chronological%20analysis%2C%20this%20study%20examines%20the%0Aadvancements%20introduced%20by%20YOLO%20algorithms%2C%20beginning%20with%20YOLOv12%20and%0Aprogressing%20through%20YOLO11%20%28or%20YOLOv11%29%2C%20YOLOv10%2C%20YOLOv9%2C%20YOLOv8%2C%20and%0Asubsequent%20versions%20to%20explore%20each%20version%27s%20contributions%20to%20enhancing%20speed%2C%0Adetection%20accuracy%2C%20and%20computational%20efficiency%20in%20real-time%20object%20detection.%0AAdditionally%2C%20this%20study%20reviews%20the%20alternative%20versions%20derived%20from%20YOLO%0Aarchitectural%20advancements%20of%20YOLO-NAS%2C%20YOLO-X%2C%20YOLO-R%2C%20DAMO-YOLO%2C%20and%0AGold-YOLO.%20Moreover%2C%20the%20study%20highlights%20the%20transformative%20impact%20of%20YOLO%0Amodels%20across%20five%20critical%20application%20areas%3A%20autonomous%20vehicles%20and%20traffic%0Asafety%2C%20healthcare%20and%20medical%20imaging%2C%20industrial%20manufacturing%2C%20surveillance%0Aand%20security%2C%20and%20agriculture.%20By%20detailing%20the%20incremental%20technological%0Aadvancements%20in%20subsequent%20YOLO%20versions%2C%20this%20review%20chronicles%20the%20evolution%0Aof%20YOLO%2C%20and%20discusses%20the%20challenges%20and%20limitations%20in%20each%20of%20the%20earlier%0Aversions.%20The%20evolution%20signifies%20a%20path%20towards%20integrating%20YOLO%20with%0Amultimodal%2C%20context-aware%2C%20and%20Artificial%20General%20Intelligence%20%28AGI%29%20systems%0Afor%20the%20next%20YOLO%20decade%2C%20promising%20significant%20implications%20for%20future%0Adevelopments%20in%20AI-driven%20applications.%20YOLO%20Review%2C%20YOLO%20Advances%2C%20YOLOv13%2C%0AYOLOv14%2C%20YOLOv15%2C%20YOLOv16%2C%20YOLOv17%2C%20YOLOv18%2C%20YOLOv19%2C%20YOLOv20%2C%20YOLO%20review%2C%0AYOLO%20Object%20Detection%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19407v8&entry.124074799=Read"},
{"title": "Understanding the Emergence of Multimodal Representation Alignment", "author": "Megan Tjandrasuwita and Chanakya Ekbote and Liu Ziyin and Paul Pu Liang", "abstract": "  Multimodal representation learning is fundamentally about transforming\nincomparable modalities into comparable representations. While prior research\nprimarily focused on explicitly aligning these representations through targeted\nlearning objectives and model architectures, a recent line of work has found\nthat independently trained unimodal models of increasing scale and performance\ncan become implicitly aligned with each other. These findings raise fundamental\nquestions regarding the emergence of aligned representations in multimodal\nlearning. Specifically: (1) when and why does alignment emerge implicitly? and\n(2) is alignment a reliable indicator of performance? Through a comprehensive\nempirical investigation, we demonstrate that both the emergence of alignment\nand its relationship with task performance depend on several critical data\ncharacteristics. These include, but are not necessarily limited to, the degree\nof similarity between the modalities and the balance between redundant and\nunique information they provide for the task. Our findings suggest that\nalignment may not be universally beneficial; rather, its impact on performance\nvaries depending on the dataset and task. These insights can help practitioners\ndetermine whether increasing alignment between modalities is advantageous or,\nin some cases, detrimental to achieving optimal performance. Code is released\nat https://github.com/MeganTj/multimodal_alignment.\n", "link": "http://arxiv.org/abs/2502.16282v2", "date": "2025-06-13", "relevancy": 2.2373, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.616}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Emergence%20of%20Multimodal%20Representation%20Alignment&body=Title%3A%20Understanding%20the%20Emergence%20of%20Multimodal%20Representation%20Alignment%0AAuthor%3A%20Megan%20Tjandrasuwita%20and%20Chanakya%20Ekbote%20and%20Liu%20Ziyin%20and%20Paul%20Pu%20Liang%0AAbstract%3A%20%20%20Multimodal%20representation%20learning%20is%20fundamentally%20about%20transforming%0Aincomparable%20modalities%20into%20comparable%20representations.%20While%20prior%20research%0Aprimarily%20focused%20on%20explicitly%20aligning%20these%20representations%20through%20targeted%0Alearning%20objectives%20and%20model%20architectures%2C%20a%20recent%20line%20of%20work%20has%20found%0Athat%20independently%20trained%20unimodal%20models%20of%20increasing%20scale%20and%20performance%0Acan%20become%20implicitly%20aligned%20with%20each%20other.%20These%20findings%20raise%20fundamental%0Aquestions%20regarding%20the%20emergence%20of%20aligned%20representations%20in%20multimodal%0Alearning.%20Specifically%3A%20%281%29%20when%20and%20why%20does%20alignment%20emerge%20implicitly%3F%20and%0A%282%29%20is%20alignment%20a%20reliable%20indicator%20of%20performance%3F%20Through%20a%20comprehensive%0Aempirical%20investigation%2C%20we%20demonstrate%20that%20both%20the%20emergence%20of%20alignment%0Aand%20its%20relationship%20with%20task%20performance%20depend%20on%20several%20critical%20data%0Acharacteristics.%20These%20include%2C%20but%20are%20not%20necessarily%20limited%20to%2C%20the%20degree%0Aof%20similarity%20between%20the%20modalities%20and%20the%20balance%20between%20redundant%20and%0Aunique%20information%20they%20provide%20for%20the%20task.%20Our%20findings%20suggest%20that%0Aalignment%20may%20not%20be%20universally%20beneficial%3B%20rather%2C%20its%20impact%20on%20performance%0Avaries%20depending%20on%20the%20dataset%20and%20task.%20These%20insights%20can%20help%20practitioners%0Adetermine%20whether%20increasing%20alignment%20between%20modalities%20is%20advantageous%20or%2C%0Ain%20some%20cases%2C%20detrimental%20to%20achieving%20optimal%20performance.%20Code%20is%20released%0Aat%20https%3A//github.com/MeganTj/multimodal_alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16282v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520the%2520Emergence%2520of%2520Multimodal%2520Representation%2520Alignment%26entry.906535625%3DMegan%2520Tjandrasuwita%2520and%2520Chanakya%2520Ekbote%2520and%2520Liu%2520Ziyin%2520and%2520Paul%2520Pu%2520Liang%26entry.1292438233%3D%2520%2520Multimodal%2520representation%2520learning%2520is%2520fundamentally%2520about%2520transforming%250Aincomparable%2520modalities%2520into%2520comparable%2520representations.%2520While%2520prior%2520research%250Aprimarily%2520focused%2520on%2520explicitly%2520aligning%2520these%2520representations%2520through%2520targeted%250Alearning%2520objectives%2520and%2520model%2520architectures%252C%2520a%2520recent%2520line%2520of%2520work%2520has%2520found%250Athat%2520independently%2520trained%2520unimodal%2520models%2520of%2520increasing%2520scale%2520and%2520performance%250Acan%2520become%2520implicitly%2520aligned%2520with%2520each%2520other.%2520These%2520findings%2520raise%2520fundamental%250Aquestions%2520regarding%2520the%2520emergence%2520of%2520aligned%2520representations%2520in%2520multimodal%250Alearning.%2520Specifically%253A%2520%25281%2529%2520when%2520and%2520why%2520does%2520alignment%2520emerge%2520implicitly%253F%2520and%250A%25282%2529%2520is%2520alignment%2520a%2520reliable%2520indicator%2520of%2520performance%253F%2520Through%2520a%2520comprehensive%250Aempirical%2520investigation%252C%2520we%2520demonstrate%2520that%2520both%2520the%2520emergence%2520of%2520alignment%250Aand%2520its%2520relationship%2520with%2520task%2520performance%2520depend%2520on%2520several%2520critical%2520data%250Acharacteristics.%2520These%2520include%252C%2520but%2520are%2520not%2520necessarily%2520limited%2520to%252C%2520the%2520degree%250Aof%2520similarity%2520between%2520the%2520modalities%2520and%2520the%2520balance%2520between%2520redundant%2520and%250Aunique%2520information%2520they%2520provide%2520for%2520the%2520task.%2520Our%2520findings%2520suggest%2520that%250Aalignment%2520may%2520not%2520be%2520universally%2520beneficial%253B%2520rather%252C%2520its%2520impact%2520on%2520performance%250Avaries%2520depending%2520on%2520the%2520dataset%2520and%2520task.%2520These%2520insights%2520can%2520help%2520practitioners%250Adetermine%2520whether%2520increasing%2520alignment%2520between%2520modalities%2520is%2520advantageous%2520or%252C%250Ain%2520some%2520cases%252C%2520detrimental%2520to%2520achieving%2520optimal%2520performance.%2520Code%2520is%2520released%250Aat%2520https%253A//github.com/MeganTj/multimodal_alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16282v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Emergence%20of%20Multimodal%20Representation%20Alignment&entry.906535625=Megan%20Tjandrasuwita%20and%20Chanakya%20Ekbote%20and%20Liu%20Ziyin%20and%20Paul%20Pu%20Liang&entry.1292438233=%20%20Multimodal%20representation%20learning%20is%20fundamentally%20about%20transforming%0Aincomparable%20modalities%20into%20comparable%20representations.%20While%20prior%20research%0Aprimarily%20focused%20on%20explicitly%20aligning%20these%20representations%20through%20targeted%0Alearning%20objectives%20and%20model%20architectures%2C%20a%20recent%20line%20of%20work%20has%20found%0Athat%20independently%20trained%20unimodal%20models%20of%20increasing%20scale%20and%20performance%0Acan%20become%20implicitly%20aligned%20with%20each%20other.%20These%20findings%20raise%20fundamental%0Aquestions%20regarding%20the%20emergence%20of%20aligned%20representations%20in%20multimodal%0Alearning.%20Specifically%3A%20%281%29%20when%20and%20why%20does%20alignment%20emerge%20implicitly%3F%20and%0A%282%29%20is%20alignment%20a%20reliable%20indicator%20of%20performance%3F%20Through%20a%20comprehensive%0Aempirical%20investigation%2C%20we%20demonstrate%20that%20both%20the%20emergence%20of%20alignment%0Aand%20its%20relationship%20with%20task%20performance%20depend%20on%20several%20critical%20data%0Acharacteristics.%20These%20include%2C%20but%20are%20not%20necessarily%20limited%20to%2C%20the%20degree%0Aof%20similarity%20between%20the%20modalities%20and%20the%20balance%20between%20redundant%20and%0Aunique%20information%20they%20provide%20for%20the%20task.%20Our%20findings%20suggest%20that%0Aalignment%20may%20not%20be%20universally%20beneficial%3B%20rather%2C%20its%20impact%20on%20performance%0Avaries%20depending%20on%20the%20dataset%20and%20task.%20These%20insights%20can%20help%20practitioners%0Adetermine%20whether%20increasing%20alignment%20between%20modalities%20is%20advantageous%20or%2C%0Ain%20some%20cases%2C%20detrimental%20to%20achieving%20optimal%20performance.%20Code%20is%20released%0Aat%20https%3A//github.com/MeganTj/multimodal_alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16282v2&entry.124074799=Read"},
{"title": "Structural Similarity-Inspired Unfolding for Lightweight Image\n  Super-Resolution", "author": "Zhangkai Ni and Yang Zhang and Wenhan Yang and Hanli Wang and Shiqi Wang and Sam Kwong", "abstract": "  Major efforts in data-driven image super-resolution (SR) primarily focus on\nexpanding the receptive field of the model to better capture contextual\ninformation. However, these methods are typically implemented by stacking\ndeeper networks or leveraging transformer-based attention mechanisms, which\nconsequently increases model complexity. In contrast, model-driven methods\nbased on the unfolding paradigm show promise in improving performance while\neffectively maintaining model compactness through sophisticated module design.\nBased on these insights, we propose a Structural Similarity-Inspired Unfolding\n(SSIU) method for efficient image SR. This method is designed through unfolding\nan SR optimization function constrained by structural similarity, aiming to\ncombine the strengths of both data-driven and model-driven approaches. Our\nmodel operates progressively following the unfolding paradigm. Each iteration\nconsists of multiple Mixed-Scale Gating Modules (MSGM) and an Efficient Sparse\nAttention Module (ESAM). The former implements comprehensive constraints on\nfeatures, including a structural similarity constraint, while the latter aims\nto achieve sparse activation. In addition, we design a Mixture-of-Experts-based\nFeature Selector (MoE-FS) that fully utilizes multi-level feature information\nby combining features from different steps. Extensive experiments validate the\nefficacy and efficiency of our unfolding-inspired network. Our model\noutperforms current state-of-the-art models, boasting lower parameter counts\nand reduced memory consumption. Our code will be available at:\nhttps://github.com/eezkni/SSIU\n", "link": "http://arxiv.org/abs/2506.11823v1", "date": "2025-06-13", "relevancy": 2.2351, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5647}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5645}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural%20Similarity-Inspired%20Unfolding%20for%20Lightweight%20Image%0A%20%20Super-Resolution&body=Title%3A%20Structural%20Similarity-Inspired%20Unfolding%20for%20Lightweight%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Zhangkai%20Ni%20and%20Yang%20Zhang%20and%20Wenhan%20Yang%20and%20Hanli%20Wang%20and%20Shiqi%20Wang%20and%20Sam%20Kwong%0AAbstract%3A%20%20%20Major%20efforts%20in%20data-driven%20image%20super-resolution%20%28SR%29%20primarily%20focus%20on%0Aexpanding%20the%20receptive%20field%20of%20the%20model%20to%20better%20capture%20contextual%0Ainformation.%20However%2C%20these%20methods%20are%20typically%20implemented%20by%20stacking%0Adeeper%20networks%20or%20leveraging%20transformer-based%20attention%20mechanisms%2C%20which%0Aconsequently%20increases%20model%20complexity.%20In%20contrast%2C%20model-driven%20methods%0Abased%20on%20the%20unfolding%20paradigm%20show%20promise%20in%20improving%20performance%20while%0Aeffectively%20maintaining%20model%20compactness%20through%20sophisticated%20module%20design.%0ABased%20on%20these%20insights%2C%20we%20propose%20a%20Structural%20Similarity-Inspired%20Unfolding%0A%28SSIU%29%20method%20for%20efficient%20image%20SR.%20This%20method%20is%20designed%20through%20unfolding%0Aan%20SR%20optimization%20function%20constrained%20by%20structural%20similarity%2C%20aiming%20to%0Acombine%20the%20strengths%20of%20both%20data-driven%20and%20model-driven%20approaches.%20Our%0Amodel%20operates%20progressively%20following%20the%20unfolding%20paradigm.%20Each%20iteration%0Aconsists%20of%20multiple%20Mixed-Scale%20Gating%20Modules%20%28MSGM%29%20and%20an%20Efficient%20Sparse%0AAttention%20Module%20%28ESAM%29.%20The%20former%20implements%20comprehensive%20constraints%20on%0Afeatures%2C%20including%20a%20structural%20similarity%20constraint%2C%20while%20the%20latter%20aims%0Ato%20achieve%20sparse%20activation.%20In%20addition%2C%20we%20design%20a%20Mixture-of-Experts-based%0AFeature%20Selector%20%28MoE-FS%29%20that%20fully%20utilizes%20multi-level%20feature%20information%0Aby%20combining%20features%20from%20different%20steps.%20Extensive%20experiments%20validate%20the%0Aefficacy%20and%20efficiency%20of%20our%20unfolding-inspired%20network.%20Our%20model%0Aoutperforms%20current%20state-of-the-art%20models%2C%20boasting%20lower%20parameter%20counts%0Aand%20reduced%20memory%20consumption.%20Our%20code%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/eezkni/SSIU%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural%2520Similarity-Inspired%2520Unfolding%2520for%2520Lightweight%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DZhangkai%2520Ni%2520and%2520Yang%2520Zhang%2520and%2520Wenhan%2520Yang%2520and%2520Hanli%2520Wang%2520and%2520Shiqi%2520Wang%2520and%2520Sam%2520Kwong%26entry.1292438233%3D%2520%2520Major%2520efforts%2520in%2520data-driven%2520image%2520super-resolution%2520%2528SR%2529%2520primarily%2520focus%2520on%250Aexpanding%2520the%2520receptive%2520field%2520of%2520the%2520model%2520to%2520better%2520capture%2520contextual%250Ainformation.%2520However%252C%2520these%2520methods%2520are%2520typically%2520implemented%2520by%2520stacking%250Adeeper%2520networks%2520or%2520leveraging%2520transformer-based%2520attention%2520mechanisms%252C%2520which%250Aconsequently%2520increases%2520model%2520complexity.%2520In%2520contrast%252C%2520model-driven%2520methods%250Abased%2520on%2520the%2520unfolding%2520paradigm%2520show%2520promise%2520in%2520improving%2520performance%2520while%250Aeffectively%2520maintaining%2520model%2520compactness%2520through%2520sophisticated%2520module%2520design.%250ABased%2520on%2520these%2520insights%252C%2520we%2520propose%2520a%2520Structural%2520Similarity-Inspired%2520Unfolding%250A%2528SSIU%2529%2520method%2520for%2520efficient%2520image%2520SR.%2520This%2520method%2520is%2520designed%2520through%2520unfolding%250Aan%2520SR%2520optimization%2520function%2520constrained%2520by%2520structural%2520similarity%252C%2520aiming%2520to%250Acombine%2520the%2520strengths%2520of%2520both%2520data-driven%2520and%2520model-driven%2520approaches.%2520Our%250Amodel%2520operates%2520progressively%2520following%2520the%2520unfolding%2520paradigm.%2520Each%2520iteration%250Aconsists%2520of%2520multiple%2520Mixed-Scale%2520Gating%2520Modules%2520%2528MSGM%2529%2520and%2520an%2520Efficient%2520Sparse%250AAttention%2520Module%2520%2528ESAM%2529.%2520The%2520former%2520implements%2520comprehensive%2520constraints%2520on%250Afeatures%252C%2520including%2520a%2520structural%2520similarity%2520constraint%252C%2520while%2520the%2520latter%2520aims%250Ato%2520achieve%2520sparse%2520activation.%2520In%2520addition%252C%2520we%2520design%2520a%2520Mixture-of-Experts-based%250AFeature%2520Selector%2520%2528MoE-FS%2529%2520that%2520fully%2520utilizes%2520multi-level%2520feature%2520information%250Aby%2520combining%2520features%2520from%2520different%2520steps.%2520Extensive%2520experiments%2520validate%2520the%250Aefficacy%2520and%2520efficiency%2520of%2520our%2520unfolding-inspired%2520network.%2520Our%2520model%250Aoutperforms%2520current%2520state-of-the-art%2520models%252C%2520boasting%2520lower%2520parameter%2520counts%250Aand%2520reduced%2520memory%2520consumption.%2520Our%2520code%2520will%2520be%2520available%2520at%253A%250Ahttps%253A//github.com/eezkni/SSIU%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20Similarity-Inspired%20Unfolding%20for%20Lightweight%20Image%0A%20%20Super-Resolution&entry.906535625=Zhangkai%20Ni%20and%20Yang%20Zhang%20and%20Wenhan%20Yang%20and%20Hanli%20Wang%20and%20Shiqi%20Wang%20and%20Sam%20Kwong&entry.1292438233=%20%20Major%20efforts%20in%20data-driven%20image%20super-resolution%20%28SR%29%20primarily%20focus%20on%0Aexpanding%20the%20receptive%20field%20of%20the%20model%20to%20better%20capture%20contextual%0Ainformation.%20However%2C%20these%20methods%20are%20typically%20implemented%20by%20stacking%0Adeeper%20networks%20or%20leveraging%20transformer-based%20attention%20mechanisms%2C%20which%0Aconsequently%20increases%20model%20complexity.%20In%20contrast%2C%20model-driven%20methods%0Abased%20on%20the%20unfolding%20paradigm%20show%20promise%20in%20improving%20performance%20while%0Aeffectively%20maintaining%20model%20compactness%20through%20sophisticated%20module%20design.%0ABased%20on%20these%20insights%2C%20we%20propose%20a%20Structural%20Similarity-Inspired%20Unfolding%0A%28SSIU%29%20method%20for%20efficient%20image%20SR.%20This%20method%20is%20designed%20through%20unfolding%0Aan%20SR%20optimization%20function%20constrained%20by%20structural%20similarity%2C%20aiming%20to%0Acombine%20the%20strengths%20of%20both%20data-driven%20and%20model-driven%20approaches.%20Our%0Amodel%20operates%20progressively%20following%20the%20unfolding%20paradigm.%20Each%20iteration%0Aconsists%20of%20multiple%20Mixed-Scale%20Gating%20Modules%20%28MSGM%29%20and%20an%20Efficient%20Sparse%0AAttention%20Module%20%28ESAM%29.%20The%20former%20implements%20comprehensive%20constraints%20on%0Afeatures%2C%20including%20a%20structural%20similarity%20constraint%2C%20while%20the%20latter%20aims%0Ato%20achieve%20sparse%20activation.%20In%20addition%2C%20we%20design%20a%20Mixture-of-Experts-based%0AFeature%20Selector%20%28MoE-FS%29%20that%20fully%20utilizes%20multi-level%20feature%20information%0Aby%20combining%20features%20from%20different%20steps.%20Extensive%20experiments%20validate%20the%0Aefficacy%20and%20efficiency%20of%20our%20unfolding-inspired%20network.%20Our%20model%0Aoutperforms%20current%20state-of-the-art%20models%2C%20boasting%20lower%20parameter%20counts%0Aand%20reduced%20memory%20consumption.%20Our%20code%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/eezkni/SSIU%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11823v1&entry.124074799=Read"},
{"title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse\n  MoE", "author": "Zongle Huang and Lei Zhu and Zongyuan Zhan and Ting Hu and Weikai Mao and Xianzhi Yu and Yongpan Liu and Tianyu Zhang", "abstract": "  Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.\n", "link": "http://arxiv.org/abs/2505.19645v2", "date": "2025-06-13", "relevancy": 2.2324, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoESD%3A%20Unveil%20Speculative%20Decoding%27s%20Potential%20for%20Accelerating%20Sparse%0A%20%20MoE&body=Title%3A%20MoESD%3A%20Unveil%20Speculative%20Decoding%27s%20Potential%20for%20Accelerating%20Sparse%0A%20%20MoE%0AAuthor%3A%20Zongle%20Huang%20and%20Lei%20Zhu%20and%20Zongyuan%20Zhan%20and%20Ting%20Hu%20and%20Weikai%20Mao%20and%20Xianzhi%20Yu%20and%20Yongpan%20Liu%20and%20Tianyu%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20across%20many%0Aapplications%2C%20with%20Mixture%20of%20Experts%20%28MoE%29%20models%20demonstrating%20great%0Apotential.%20Compared%20to%20traditional%20dense%20models%2C%20MoEs%20achieve%20better%0Aperformance%20with%20less%20computation.%20Speculative%20decoding%20%28SD%29%20is%20a%20widely%20used%0Atechnique%20to%20accelerate%20LLM%20inference%20without%20accuracy%20loss%2C%20but%20it%20has%20been%0Aconsidered%20efficient%20only%20for%20dense%20models.%20In%20this%20work%2C%20we%20first%20demonstrate%0Athat%2C%20under%20medium%20batch%20sizes%2C%20MoE%20surprisingly%20benefits%20more%20from%20SD%20than%0Adense%20models.%20Furthermore%2C%20as%20MoE%20becomes%20sparser%20--%20the%20prevailing%20trend%20in%0AMoE%20designs%20--%20the%20batch%20size%20range%20where%20SD%20acceleration%20is%20expected%20to%20be%0Aeffective%20becomes%20broader.%20To%20quantitatively%20understand%20tradeoffs%20involved%20in%0ASD%2C%20we%20develop%20a%20reliable%20modeling%20based%20on%20theoretical%20analyses.%20While%20current%0ASD%20research%20primarily%20focuses%20on%20improving%20acceptance%20rates%20of%20algorithms%2C%0Achanges%20in%20workload%20and%20model%20architecture%20can%20still%20lead%20to%20degraded%20SD%0Aacceleration%20even%20with%20high%20acceptance%20rates.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20a%20new%20metric%20%27target%20efficiency%27%20that%20characterizes%20these%20effects%2C%0Athus%20helping%20researchers%20identify%20system%20bottlenecks%20and%20understand%20SD%0Aacceleration%20more%20comprehensively.%20For%20scenarios%20like%20private%20serving%2C%20this%0Awork%20unveils%20a%20new%20perspective%20to%20speed%20up%20MoE%20inference%2C%20where%20existing%0Asolutions%20struggle.%20Experiments%20on%20different%20GPUs%20show%20up%20to%202.29x%20speedup%20for%0AQwen2-57B-A14B%20at%20medium%20batch%20sizes%20and%20validate%20our%20theoretical%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19645v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoESD%253A%2520Unveil%2520Speculative%2520Decoding%2527s%2520Potential%2520for%2520Accelerating%2520Sparse%250A%2520%2520MoE%26entry.906535625%3DZongle%2520Huang%2520and%2520Lei%2520Zhu%2520and%2520Zongyuan%2520Zhan%2520and%2520Ting%2520Hu%2520and%2520Weikai%2520Mao%2520and%2520Xianzhi%2520Yu%2520and%2520Yongpan%2520Liu%2520and%2520Tianyu%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520across%2520many%250Aapplications%252C%2520with%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520models%2520demonstrating%2520great%250Apotential.%2520Compared%2520to%2520traditional%2520dense%2520models%252C%2520MoEs%2520achieve%2520better%250Aperformance%2520with%2520less%2520computation.%2520Speculative%2520decoding%2520%2528SD%2529%2520is%2520a%2520widely%2520used%250Atechnique%2520to%2520accelerate%2520LLM%2520inference%2520without%2520accuracy%2520loss%252C%2520but%2520it%2520has%2520been%250Aconsidered%2520efficient%2520only%2520for%2520dense%2520models.%2520In%2520this%2520work%252C%2520we%2520first%2520demonstrate%250Athat%252C%2520under%2520medium%2520batch%2520sizes%252C%2520MoE%2520surprisingly%2520benefits%2520more%2520from%2520SD%2520than%250Adense%2520models.%2520Furthermore%252C%2520as%2520MoE%2520becomes%2520sparser%2520--%2520the%2520prevailing%2520trend%2520in%250AMoE%2520designs%2520--%2520the%2520batch%2520size%2520range%2520where%2520SD%2520acceleration%2520is%2520expected%2520to%2520be%250Aeffective%2520becomes%2520broader.%2520To%2520quantitatively%2520understand%2520tradeoffs%2520involved%2520in%250ASD%252C%2520we%2520develop%2520a%2520reliable%2520modeling%2520based%2520on%2520theoretical%2520analyses.%2520While%2520current%250ASD%2520research%2520primarily%2520focuses%2520on%2520improving%2520acceptance%2520rates%2520of%2520algorithms%252C%250Achanges%2520in%2520workload%2520and%2520model%2520architecture%2520can%2520still%2520lead%2520to%2520degraded%2520SD%250Aacceleration%2520even%2520with%2520high%2520acceptance%2520rates.%2520To%2520address%2520this%2520limitation%252C%2520we%250Aintroduce%2520a%2520new%2520metric%2520%2527target%2520efficiency%2527%2520that%2520characterizes%2520these%2520effects%252C%250Athus%2520helping%2520researchers%2520identify%2520system%2520bottlenecks%2520and%2520understand%2520SD%250Aacceleration%2520more%2520comprehensively.%2520For%2520scenarios%2520like%2520private%2520serving%252C%2520this%250Awork%2520unveils%2520a%2520new%2520perspective%2520to%2520speed%2520up%2520MoE%2520inference%252C%2520where%2520existing%250Asolutions%2520struggle.%2520Experiments%2520on%2520different%2520GPUs%2520show%2520up%2520to%25202.29x%2520speedup%2520for%250AQwen2-57B-A14B%2520at%2520medium%2520batch%2520sizes%2520and%2520validate%2520our%2520theoretical%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19645v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoESD%3A%20Unveil%20Speculative%20Decoding%27s%20Potential%20for%20Accelerating%20Sparse%0A%20%20MoE&entry.906535625=Zongle%20Huang%20and%20Lei%20Zhu%20and%20Zongyuan%20Zhan%20and%20Ting%20Hu%20and%20Weikai%20Mao%20and%20Xianzhi%20Yu%20and%20Yongpan%20Liu%20and%20Tianyu%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20across%20many%0Aapplications%2C%20with%20Mixture%20of%20Experts%20%28MoE%29%20models%20demonstrating%20great%0Apotential.%20Compared%20to%20traditional%20dense%20models%2C%20MoEs%20achieve%20better%0Aperformance%20with%20less%20computation.%20Speculative%20decoding%20%28SD%29%20is%20a%20widely%20used%0Atechnique%20to%20accelerate%20LLM%20inference%20without%20accuracy%20loss%2C%20but%20it%20has%20been%0Aconsidered%20efficient%20only%20for%20dense%20models.%20In%20this%20work%2C%20we%20first%20demonstrate%0Athat%2C%20under%20medium%20batch%20sizes%2C%20MoE%20surprisingly%20benefits%20more%20from%20SD%20than%0Adense%20models.%20Furthermore%2C%20as%20MoE%20becomes%20sparser%20--%20the%20prevailing%20trend%20in%0AMoE%20designs%20--%20the%20batch%20size%20range%20where%20SD%20acceleration%20is%20expected%20to%20be%0Aeffective%20becomes%20broader.%20To%20quantitatively%20understand%20tradeoffs%20involved%20in%0ASD%2C%20we%20develop%20a%20reliable%20modeling%20based%20on%20theoretical%20analyses.%20While%20current%0ASD%20research%20primarily%20focuses%20on%20improving%20acceptance%20rates%20of%20algorithms%2C%0Achanges%20in%20workload%20and%20model%20architecture%20can%20still%20lead%20to%20degraded%20SD%0Aacceleration%20even%20with%20high%20acceptance%20rates.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20a%20new%20metric%20%27target%20efficiency%27%20that%20characterizes%20these%20effects%2C%0Athus%20helping%20researchers%20identify%20system%20bottlenecks%20and%20understand%20SD%0Aacceleration%20more%20comprehensively.%20For%20scenarios%20like%20private%20serving%2C%20this%0Awork%20unveils%20a%20new%20perspective%20to%20speed%20up%20MoE%20inference%2C%20where%20existing%0Asolutions%20struggle.%20Experiments%20on%20different%20GPUs%20show%20up%20to%202.29x%20speedup%20for%0AQwen2-57B-A14B%20at%20medium%20batch%20sizes%20and%20validate%20our%20theoretical%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19645v2&entry.124074799=Read"},
{"title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation", "author": "Edoardo Bianchi and Antonio Liotta", "abstract": "  Assessing human skill levels in complex activities is a challenging problem\nwith applications in sports, rehabilitation, and training. In this work, we\npresent SkillFormer, a parameter-efficient architecture for unified multi-view\nproficiency estimation from egocentric and exocentric videos. Building on the\nTimeSformer backbone, SkillFormer introduces a CrossViewFusion module that\nfuses view-specific features using multi-head cross-attention, learnable\ngating, and adaptive self-calibration. We leverage Low-Rank Adaptation to\nfine-tune only a small subset of parameters, significantly reducing training\ncosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves\nstate-of-the-art accuracy in multi-view settings while demonstrating remarkable\ncomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer\ntraining epochs than prior baselines. It excels in multiple structured tasks,\nconfirming the value of multi-view integration for fine-grained skill\nassessment.\n", "link": "http://arxiv.org/abs/2505.08665v2", "date": "2025-06-13", "relevancy": 2.2282, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkillFormer%3A%20Unified%20Multi-View%20Video%20Understanding%20for%20Proficiency%0A%20%20Estimation&body=Title%3A%20SkillFormer%3A%20Unified%20Multi-View%20Video%20Understanding%20for%20Proficiency%0A%20%20Estimation%0AAuthor%3A%20Edoardo%20Bianchi%20and%20Antonio%20Liotta%0AAbstract%3A%20%20%20Assessing%20human%20skill%20levels%20in%20complex%20activities%20is%20a%20challenging%20problem%0Awith%20applications%20in%20sports%2C%20rehabilitation%2C%20and%20training.%20In%20this%20work%2C%20we%0Apresent%20SkillFormer%2C%20a%20parameter-efficient%20architecture%20for%20unified%20multi-view%0Aproficiency%20estimation%20from%20egocentric%20and%20exocentric%20videos.%20Building%20on%20the%0ATimeSformer%20backbone%2C%20SkillFormer%20introduces%20a%20CrossViewFusion%20module%20that%0Afuses%20view-specific%20features%20using%20multi-head%20cross-attention%2C%20learnable%0Agating%2C%20and%20adaptive%20self-calibration.%20We%20leverage%20Low-Rank%20Adaptation%20to%0Afine-tune%20only%20a%20small%20subset%20of%20parameters%2C%20significantly%20reducing%20training%0Acosts.%20In%20fact%2C%20when%20evaluated%20on%20the%20EgoExo4D%20dataset%2C%20SkillFormer%20achieves%0Astate-of-the-art%20accuracy%20in%20multi-view%20settings%20while%20demonstrating%20remarkable%0Acomputational%20efficiency%2C%20using%204.5x%20fewer%20parameters%20and%20requiring%203.75x%20fewer%0Atraining%20epochs%20than%20prior%20baselines.%20It%20excels%20in%20multiple%20structured%20tasks%2C%0Aconfirming%20the%20value%20of%20multi-view%20integration%20for%20fine-grained%20skill%0Aassessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08665v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkillFormer%253A%2520Unified%2520Multi-View%2520Video%2520Understanding%2520for%2520Proficiency%250A%2520%2520Estimation%26entry.906535625%3DEdoardo%2520Bianchi%2520and%2520Antonio%2520Liotta%26entry.1292438233%3D%2520%2520Assessing%2520human%2520skill%2520levels%2520in%2520complex%2520activities%2520is%2520a%2520challenging%2520problem%250Awith%2520applications%2520in%2520sports%252C%2520rehabilitation%252C%2520and%2520training.%2520In%2520this%2520work%252C%2520we%250Apresent%2520SkillFormer%252C%2520a%2520parameter-efficient%2520architecture%2520for%2520unified%2520multi-view%250Aproficiency%2520estimation%2520from%2520egocentric%2520and%2520exocentric%2520videos.%2520Building%2520on%2520the%250ATimeSformer%2520backbone%252C%2520SkillFormer%2520introduces%2520a%2520CrossViewFusion%2520module%2520that%250Afuses%2520view-specific%2520features%2520using%2520multi-head%2520cross-attention%252C%2520learnable%250Agating%252C%2520and%2520adaptive%2520self-calibration.%2520We%2520leverage%2520Low-Rank%2520Adaptation%2520to%250Afine-tune%2520only%2520a%2520small%2520subset%2520of%2520parameters%252C%2520significantly%2520reducing%2520training%250Acosts.%2520In%2520fact%252C%2520when%2520evaluated%2520on%2520the%2520EgoExo4D%2520dataset%252C%2520SkillFormer%2520achieves%250Astate-of-the-art%2520accuracy%2520in%2520multi-view%2520settings%2520while%2520demonstrating%2520remarkable%250Acomputational%2520efficiency%252C%2520using%25204.5x%2520fewer%2520parameters%2520and%2520requiring%25203.75x%2520fewer%250Atraining%2520epochs%2520than%2520prior%2520baselines.%2520It%2520excels%2520in%2520multiple%2520structured%2520tasks%252C%250Aconfirming%2520the%2520value%2520of%2520multi-view%2520integration%2520for%2520fine-grained%2520skill%250Aassessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08665v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkillFormer%3A%20Unified%20Multi-View%20Video%20Understanding%20for%20Proficiency%0A%20%20Estimation&entry.906535625=Edoardo%20Bianchi%20and%20Antonio%20Liotta&entry.1292438233=%20%20Assessing%20human%20skill%20levels%20in%20complex%20activities%20is%20a%20challenging%20problem%0Awith%20applications%20in%20sports%2C%20rehabilitation%2C%20and%20training.%20In%20this%20work%2C%20we%0Apresent%20SkillFormer%2C%20a%20parameter-efficient%20architecture%20for%20unified%20multi-view%0Aproficiency%20estimation%20from%20egocentric%20and%20exocentric%20videos.%20Building%20on%20the%0ATimeSformer%20backbone%2C%20SkillFormer%20introduces%20a%20CrossViewFusion%20module%20that%0Afuses%20view-specific%20features%20using%20multi-head%20cross-attention%2C%20learnable%0Agating%2C%20and%20adaptive%20self-calibration.%20We%20leverage%20Low-Rank%20Adaptation%20to%0Afine-tune%20only%20a%20small%20subset%20of%20parameters%2C%20significantly%20reducing%20training%0Acosts.%20In%20fact%2C%20when%20evaluated%20on%20the%20EgoExo4D%20dataset%2C%20SkillFormer%20achieves%0Astate-of-the-art%20accuracy%20in%20multi-view%20settings%20while%20demonstrating%20remarkable%0Acomputational%20efficiency%2C%20using%204.5x%20fewer%20parameters%20and%20requiring%203.75x%20fewer%0Atraining%20epochs%20than%20prior%20baselines.%20It%20excels%20in%20multiple%20structured%20tasks%2C%0Aconfirming%20the%20value%20of%20multi-view%20integration%20for%20fine-grained%20skill%0Aassessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08665v2&entry.124074799=Read"},
{"title": "Evidential Spectrum-Aware Contrastive Learning for OOD Detection in\n  Dynamic Graphs", "author": "Nan Sun and Xixun Lin and Zhiheng Zhou and Yanmin Shang and Zhenlin Cheng and Yanan Cao", "abstract": "  Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims\nto identify whether incoming data deviates from the distribution of the\nin-distribution (ID) training set, has garnered considerable attention in\nsecurity-sensitive fields. Current OOD detection paradigms primarily focus on\nstatic graphs and confront two critical challenges: i) high bias and high\nvariance caused by single-point estimation, which makes the predictions\nsensitive to randomness in the data; ii) score homogenization resulting from\nthe lack of OOD training data, where the model only learns ID-specific\npatterns, resulting in overall low OOD scores and a narrow score gap between ID\nand OOD data. To tackle these issues, we first investigate OOD detection in\ndynamic graphs through the lens of Evidential Deep Learning (EDL).\nSpecifically, we propose EviSEC, an innovative and effective OOD detector via\nEvidential Spectrum-awarE Contrastive Learning. We design an evidential neural\nnetwork to redefine the output as the posterior Dirichlet distribution,\nexplaining the randomness of inputs through the uncertainty of distribution,\nwhich is overlooked by single-point estimation. Moreover, spectrum-aware\naugmentation module generates OOD approximations to identify patterns with high\nOOD scores, thereby widening the score gap between ID and OOD data and\nmitigating score homogenization. Extensive experiments on real-world datasets\ndemonstrate that EviSAC effectively detects OOD samples in dynamic graphs.\n", "link": "http://arxiv.org/abs/2506.07417v2", "date": "2025-06-13", "relevancy": 2.2114, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5611}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5584}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evidential%20Spectrum-Aware%20Contrastive%20Learning%20for%20OOD%20Detection%20in%0A%20%20Dynamic%20Graphs&body=Title%3A%20Evidential%20Spectrum-Aware%20Contrastive%20Learning%20for%20OOD%20Detection%20in%0A%20%20Dynamic%20Graphs%0AAuthor%3A%20Nan%20Sun%20and%20Xixun%20Lin%20and%20Zhiheng%20Zhou%20and%20Yanmin%20Shang%20and%20Zhenlin%20Cheng%20and%20Yanan%20Cao%0AAbstract%3A%20%20%20Recently%2C%20Out-of-distribution%20%28OOD%29%20detection%20in%20dynamic%20graphs%2C%20which%20aims%0Ato%20identify%20whether%20incoming%20data%20deviates%20from%20the%20distribution%20of%20the%0Ain-distribution%20%28ID%29%20training%20set%2C%20has%20garnered%20considerable%20attention%20in%0Asecurity-sensitive%20fields.%20Current%20OOD%20detection%20paradigms%20primarily%20focus%20on%0Astatic%20graphs%20and%20confront%20two%20critical%20challenges%3A%20i%29%20high%20bias%20and%20high%0Avariance%20caused%20by%20single-point%20estimation%2C%20which%20makes%20the%20predictions%0Asensitive%20to%20randomness%20in%20the%20data%3B%20ii%29%20score%20homogenization%20resulting%20from%0Athe%20lack%20of%20OOD%20training%20data%2C%20where%20the%20model%20only%20learns%20ID-specific%0Apatterns%2C%20resulting%20in%20overall%20low%20OOD%20scores%20and%20a%20narrow%20score%20gap%20between%20ID%0Aand%20OOD%20data.%20To%20tackle%20these%20issues%2C%20we%20first%20investigate%20OOD%20detection%20in%0Adynamic%20graphs%20through%20the%20lens%20of%20Evidential%20Deep%20Learning%20%28EDL%29.%0ASpecifically%2C%20we%20propose%20EviSEC%2C%20an%20innovative%20and%20effective%20OOD%20detector%20via%0AEvidential%20Spectrum-awarE%20Contrastive%20Learning.%20We%20design%20an%20evidential%20neural%0Anetwork%20to%20redefine%20the%20output%20as%20the%20posterior%20Dirichlet%20distribution%2C%0Aexplaining%20the%20randomness%20of%20inputs%20through%20the%20uncertainty%20of%20distribution%2C%0Awhich%20is%20overlooked%20by%20single-point%20estimation.%20Moreover%2C%20spectrum-aware%0Aaugmentation%20module%20generates%20OOD%20approximations%20to%20identify%20patterns%20with%20high%0AOOD%20scores%2C%20thereby%20widening%20the%20score%20gap%20between%20ID%20and%20OOD%20data%20and%0Amitigating%20score%20homogenization.%20Extensive%20experiments%20on%20real-world%20datasets%0Ademonstrate%20that%20EviSAC%20effectively%20detects%20OOD%20samples%20in%20dynamic%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvidential%2520Spectrum-Aware%2520Contrastive%2520Learning%2520for%2520OOD%2520Detection%2520in%250A%2520%2520Dynamic%2520Graphs%26entry.906535625%3DNan%2520Sun%2520and%2520Xixun%2520Lin%2520and%2520Zhiheng%2520Zhou%2520and%2520Yanmin%2520Shang%2520and%2520Zhenlin%2520Cheng%2520and%2520Yanan%2520Cao%26entry.1292438233%3D%2520%2520Recently%252C%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520in%2520dynamic%2520graphs%252C%2520which%2520aims%250Ato%2520identify%2520whether%2520incoming%2520data%2520deviates%2520from%2520the%2520distribution%2520of%2520the%250Ain-distribution%2520%2528ID%2529%2520training%2520set%252C%2520has%2520garnered%2520considerable%2520attention%2520in%250Asecurity-sensitive%2520fields.%2520Current%2520OOD%2520detection%2520paradigms%2520primarily%2520focus%2520on%250Astatic%2520graphs%2520and%2520confront%2520two%2520critical%2520challenges%253A%2520i%2529%2520high%2520bias%2520and%2520high%250Avariance%2520caused%2520by%2520single-point%2520estimation%252C%2520which%2520makes%2520the%2520predictions%250Asensitive%2520to%2520randomness%2520in%2520the%2520data%253B%2520ii%2529%2520score%2520homogenization%2520resulting%2520from%250Athe%2520lack%2520of%2520OOD%2520training%2520data%252C%2520where%2520the%2520model%2520only%2520learns%2520ID-specific%250Apatterns%252C%2520resulting%2520in%2520overall%2520low%2520OOD%2520scores%2520and%2520a%2520narrow%2520score%2520gap%2520between%2520ID%250Aand%2520OOD%2520data.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520first%2520investigate%2520OOD%2520detection%2520in%250Adynamic%2520graphs%2520through%2520the%2520lens%2520of%2520Evidential%2520Deep%2520Learning%2520%2528EDL%2529.%250ASpecifically%252C%2520we%2520propose%2520EviSEC%252C%2520an%2520innovative%2520and%2520effective%2520OOD%2520detector%2520via%250AEvidential%2520Spectrum-awarE%2520Contrastive%2520Learning.%2520We%2520design%2520an%2520evidential%2520neural%250Anetwork%2520to%2520redefine%2520the%2520output%2520as%2520the%2520posterior%2520Dirichlet%2520distribution%252C%250Aexplaining%2520the%2520randomness%2520of%2520inputs%2520through%2520the%2520uncertainty%2520of%2520distribution%252C%250Awhich%2520is%2520overlooked%2520by%2520single-point%2520estimation.%2520Moreover%252C%2520spectrum-aware%250Aaugmentation%2520module%2520generates%2520OOD%2520approximations%2520to%2520identify%2520patterns%2520with%2520high%250AOOD%2520scores%252C%2520thereby%2520widening%2520the%2520score%2520gap%2520between%2520ID%2520and%2520OOD%2520data%2520and%250Amitigating%2520score%2520homogenization.%2520Extensive%2520experiments%2520on%2520real-world%2520datasets%250Ademonstrate%2520that%2520EviSAC%2520effectively%2520detects%2520OOD%2520samples%2520in%2520dynamic%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evidential%20Spectrum-Aware%20Contrastive%20Learning%20for%20OOD%20Detection%20in%0A%20%20Dynamic%20Graphs&entry.906535625=Nan%20Sun%20and%20Xixun%20Lin%20and%20Zhiheng%20Zhou%20and%20Yanmin%20Shang%20and%20Zhenlin%20Cheng%20and%20Yanan%20Cao&entry.1292438233=%20%20Recently%2C%20Out-of-distribution%20%28OOD%29%20detection%20in%20dynamic%20graphs%2C%20which%20aims%0Ato%20identify%20whether%20incoming%20data%20deviates%20from%20the%20distribution%20of%20the%0Ain-distribution%20%28ID%29%20training%20set%2C%20has%20garnered%20considerable%20attention%20in%0Asecurity-sensitive%20fields.%20Current%20OOD%20detection%20paradigms%20primarily%20focus%20on%0Astatic%20graphs%20and%20confront%20two%20critical%20challenges%3A%20i%29%20high%20bias%20and%20high%0Avariance%20caused%20by%20single-point%20estimation%2C%20which%20makes%20the%20predictions%0Asensitive%20to%20randomness%20in%20the%20data%3B%20ii%29%20score%20homogenization%20resulting%20from%0Athe%20lack%20of%20OOD%20training%20data%2C%20where%20the%20model%20only%20learns%20ID-specific%0Apatterns%2C%20resulting%20in%20overall%20low%20OOD%20scores%20and%20a%20narrow%20score%20gap%20between%20ID%0Aand%20OOD%20data.%20To%20tackle%20these%20issues%2C%20we%20first%20investigate%20OOD%20detection%20in%0Adynamic%20graphs%20through%20the%20lens%20of%20Evidential%20Deep%20Learning%20%28EDL%29.%0ASpecifically%2C%20we%20propose%20EviSEC%2C%20an%20innovative%20and%20effective%20OOD%20detector%20via%0AEvidential%20Spectrum-awarE%20Contrastive%20Learning.%20We%20design%20an%20evidential%20neural%0Anetwork%20to%20redefine%20the%20output%20as%20the%20posterior%20Dirichlet%20distribution%2C%0Aexplaining%20the%20randomness%20of%20inputs%20through%20the%20uncertainty%20of%20distribution%2C%0Awhich%20is%20overlooked%20by%20single-point%20estimation.%20Moreover%2C%20spectrum-aware%0Aaugmentation%20module%20generates%20OOD%20approximations%20to%20identify%20patterns%20with%20high%0AOOD%20scores%2C%20thereby%20widening%20the%20score%20gap%20between%20ID%20and%20OOD%20data%20and%0Amitigating%20score%20homogenization.%20Extensive%20experiments%20on%20real-world%20datasets%0Ademonstrate%20that%20EviSAC%20effectively%20detects%20OOD%20samples%20in%20dynamic%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07417v2&entry.124074799=Read"},
{"title": "Cross-Modal Clustering-Guided Negative Sampling for Self-Supervised\n  Joint Learning from Medical Images and Reports", "author": "Libin Lan and Hongxing Li and Zunhui Xia and Juan Zhou and Xiaofei Zhu and Yongmei Li and Yudong Zhang and Xin Luo", "abstract": "  Learning medical visual representations directly from paired images and\nreports through multimodal self-supervised learning has emerged as a novel and\nefficient approach to digital diagnosis in recent years. However, existing\nmodels suffer from several severe limitations. 1) neglecting the selection of\nnegative samples, resulting in the scarcity of hard negatives and the inclusion\nof false negatives; 2) focusing on global feature extraction, but overlooking\nthe fine-grained local details that are crucial for medical image recognition\ntasks; and 3) contrastive learning primarily targets high-level features but\nignoring low-level details which are essential for accurate medical analysis.\nMotivated by these critical issues, this paper presents a Cross-Modal\nCluster-Guided Negative Sampling (CM-CGNS) method with two-fold ideas. First,\nit extends the k-means clustering used for local text features in the\nsingle-modal domain to the multimodal domain through cross-modal attention.\nThis improvement increases the number of negative samples and boosts the model\nrepresentation capability. Second, it introduces a Cross-Modal Masked Image\nReconstruction (CM-MIR) module that leverages local text-to-image features\nobtained via cross-modal attention to reconstruct masked local image regions.\nThis module significantly strengthens the model's cross-modal information\ninteraction capabilities and retains low-level image features essential for\ndownstream tasks. By well handling the aforementioned limitations, the proposed\nCM-CGNS can learn effective and robust medical visual representations suitable\nfor various recognition tasks. Extensive experimental results on\nclassification, detection, and segmentation tasks across five downstream\ndatasets show that our method outperforms state-of-the-art approaches on\nmultiple metrics, verifying its superior performance.\n", "link": "http://arxiv.org/abs/2506.11674v1", "date": "2025-06-13", "relevancy": 2.2111, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5682}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5459}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Modal%20Clustering-Guided%20Negative%20Sampling%20for%20Self-Supervised%0A%20%20Joint%20Learning%20from%20Medical%20Images%20and%20Reports&body=Title%3A%20Cross-Modal%20Clustering-Guided%20Negative%20Sampling%20for%20Self-Supervised%0A%20%20Joint%20Learning%20from%20Medical%20Images%20and%20Reports%0AAuthor%3A%20Libin%20Lan%20and%20Hongxing%20Li%20and%20Zunhui%20Xia%20and%20Juan%20Zhou%20and%20Xiaofei%20Zhu%20and%20Yongmei%20Li%20and%20Yudong%20Zhang%20and%20Xin%20Luo%0AAbstract%3A%20%20%20Learning%20medical%20visual%20representations%20directly%20from%20paired%20images%20and%0Areports%20through%20multimodal%20self-supervised%20learning%20has%20emerged%20as%20a%20novel%20and%0Aefficient%20approach%20to%20digital%20diagnosis%20in%20recent%20years.%20However%2C%20existing%0Amodels%20suffer%20from%20several%20severe%20limitations.%201%29%20neglecting%20the%20selection%20of%0Anegative%20samples%2C%20resulting%20in%20the%20scarcity%20of%20hard%20negatives%20and%20the%20inclusion%0Aof%20false%20negatives%3B%202%29%20focusing%20on%20global%20feature%20extraction%2C%20but%20overlooking%0Athe%20fine-grained%20local%20details%20that%20are%20crucial%20for%20medical%20image%20recognition%0Atasks%3B%20and%203%29%20contrastive%20learning%20primarily%20targets%20high-level%20features%20but%0Aignoring%20low-level%20details%20which%20are%20essential%20for%20accurate%20medical%20analysis.%0AMotivated%20by%20these%20critical%20issues%2C%20this%20paper%20presents%20a%20Cross-Modal%0ACluster-Guided%20Negative%20Sampling%20%28CM-CGNS%29%20method%20with%20two-fold%20ideas.%20First%2C%0Ait%20extends%20the%20k-means%20clustering%20used%20for%20local%20text%20features%20in%20the%0Asingle-modal%20domain%20to%20the%20multimodal%20domain%20through%20cross-modal%20attention.%0AThis%20improvement%20increases%20the%20number%20of%20negative%20samples%20and%20boosts%20the%20model%0Arepresentation%20capability.%20Second%2C%20it%20introduces%20a%20Cross-Modal%20Masked%20Image%0AReconstruction%20%28CM-MIR%29%20module%20that%20leverages%20local%20text-to-image%20features%0Aobtained%20via%20cross-modal%20attention%20to%20reconstruct%20masked%20local%20image%20regions.%0AThis%20module%20significantly%20strengthens%20the%20model%27s%20cross-modal%20information%0Ainteraction%20capabilities%20and%20retains%20low-level%20image%20features%20essential%20for%0Adownstream%20tasks.%20By%20well%20handling%20the%20aforementioned%20limitations%2C%20the%20proposed%0ACM-CGNS%20can%20learn%20effective%20and%20robust%20medical%20visual%20representations%20suitable%0Afor%20various%20recognition%20tasks.%20Extensive%20experimental%20results%20on%0Aclassification%2C%20detection%2C%20and%20segmentation%20tasks%20across%20five%20downstream%0Adatasets%20show%20that%20our%20method%20outperforms%20state-of-the-art%20approaches%20on%0Amultiple%20metrics%2C%20verifying%20its%20superior%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Modal%2520Clustering-Guided%2520Negative%2520Sampling%2520for%2520Self-Supervised%250A%2520%2520Joint%2520Learning%2520from%2520Medical%2520Images%2520and%2520Reports%26entry.906535625%3DLibin%2520Lan%2520and%2520Hongxing%2520Li%2520and%2520Zunhui%2520Xia%2520and%2520Juan%2520Zhou%2520and%2520Xiaofei%2520Zhu%2520and%2520Yongmei%2520Li%2520and%2520Yudong%2520Zhang%2520and%2520Xin%2520Luo%26entry.1292438233%3D%2520%2520Learning%2520medical%2520visual%2520representations%2520directly%2520from%2520paired%2520images%2520and%250Areports%2520through%2520multimodal%2520self-supervised%2520learning%2520has%2520emerged%2520as%2520a%2520novel%2520and%250Aefficient%2520approach%2520to%2520digital%2520diagnosis%2520in%2520recent%2520years.%2520However%252C%2520existing%250Amodels%2520suffer%2520from%2520several%2520severe%2520limitations.%25201%2529%2520neglecting%2520the%2520selection%2520of%250Anegative%2520samples%252C%2520resulting%2520in%2520the%2520scarcity%2520of%2520hard%2520negatives%2520and%2520the%2520inclusion%250Aof%2520false%2520negatives%253B%25202%2529%2520focusing%2520on%2520global%2520feature%2520extraction%252C%2520but%2520overlooking%250Athe%2520fine-grained%2520local%2520details%2520that%2520are%2520crucial%2520for%2520medical%2520image%2520recognition%250Atasks%253B%2520and%25203%2529%2520contrastive%2520learning%2520primarily%2520targets%2520high-level%2520features%2520but%250Aignoring%2520low-level%2520details%2520which%2520are%2520essential%2520for%2520accurate%2520medical%2520analysis.%250AMotivated%2520by%2520these%2520critical%2520issues%252C%2520this%2520paper%2520presents%2520a%2520Cross-Modal%250ACluster-Guided%2520Negative%2520Sampling%2520%2528CM-CGNS%2529%2520method%2520with%2520two-fold%2520ideas.%2520First%252C%250Ait%2520extends%2520the%2520k-means%2520clustering%2520used%2520for%2520local%2520text%2520features%2520in%2520the%250Asingle-modal%2520domain%2520to%2520the%2520multimodal%2520domain%2520through%2520cross-modal%2520attention.%250AThis%2520improvement%2520increases%2520the%2520number%2520of%2520negative%2520samples%2520and%2520boosts%2520the%2520model%250Arepresentation%2520capability.%2520Second%252C%2520it%2520introduces%2520a%2520Cross-Modal%2520Masked%2520Image%250AReconstruction%2520%2528CM-MIR%2529%2520module%2520that%2520leverages%2520local%2520text-to-image%2520features%250Aobtained%2520via%2520cross-modal%2520attention%2520to%2520reconstruct%2520masked%2520local%2520image%2520regions.%250AThis%2520module%2520significantly%2520strengthens%2520the%2520model%2527s%2520cross-modal%2520information%250Ainteraction%2520capabilities%2520and%2520retains%2520low-level%2520image%2520features%2520essential%2520for%250Adownstream%2520tasks.%2520By%2520well%2520handling%2520the%2520aforementioned%2520limitations%252C%2520the%2520proposed%250ACM-CGNS%2520can%2520learn%2520effective%2520and%2520robust%2520medical%2520visual%2520representations%2520suitable%250Afor%2520various%2520recognition%2520tasks.%2520Extensive%2520experimental%2520results%2520on%250Aclassification%252C%2520detection%252C%2520and%2520segmentation%2520tasks%2520across%2520five%2520downstream%250Adatasets%2520show%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520approaches%2520on%250Amultiple%2520metrics%252C%2520verifying%2520its%2520superior%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Modal%20Clustering-Guided%20Negative%20Sampling%20for%20Self-Supervised%0A%20%20Joint%20Learning%20from%20Medical%20Images%20and%20Reports&entry.906535625=Libin%20Lan%20and%20Hongxing%20Li%20and%20Zunhui%20Xia%20and%20Juan%20Zhou%20and%20Xiaofei%20Zhu%20and%20Yongmei%20Li%20and%20Yudong%20Zhang%20and%20Xin%20Luo&entry.1292438233=%20%20Learning%20medical%20visual%20representations%20directly%20from%20paired%20images%20and%0Areports%20through%20multimodal%20self-supervised%20learning%20has%20emerged%20as%20a%20novel%20and%0Aefficient%20approach%20to%20digital%20diagnosis%20in%20recent%20years.%20However%2C%20existing%0Amodels%20suffer%20from%20several%20severe%20limitations.%201%29%20neglecting%20the%20selection%20of%0Anegative%20samples%2C%20resulting%20in%20the%20scarcity%20of%20hard%20negatives%20and%20the%20inclusion%0Aof%20false%20negatives%3B%202%29%20focusing%20on%20global%20feature%20extraction%2C%20but%20overlooking%0Athe%20fine-grained%20local%20details%20that%20are%20crucial%20for%20medical%20image%20recognition%0Atasks%3B%20and%203%29%20contrastive%20learning%20primarily%20targets%20high-level%20features%20but%0Aignoring%20low-level%20details%20which%20are%20essential%20for%20accurate%20medical%20analysis.%0AMotivated%20by%20these%20critical%20issues%2C%20this%20paper%20presents%20a%20Cross-Modal%0ACluster-Guided%20Negative%20Sampling%20%28CM-CGNS%29%20method%20with%20two-fold%20ideas.%20First%2C%0Ait%20extends%20the%20k-means%20clustering%20used%20for%20local%20text%20features%20in%20the%0Asingle-modal%20domain%20to%20the%20multimodal%20domain%20through%20cross-modal%20attention.%0AThis%20improvement%20increases%20the%20number%20of%20negative%20samples%20and%20boosts%20the%20model%0Arepresentation%20capability.%20Second%2C%20it%20introduces%20a%20Cross-Modal%20Masked%20Image%0AReconstruction%20%28CM-MIR%29%20module%20that%20leverages%20local%20text-to-image%20features%0Aobtained%20via%20cross-modal%20attention%20to%20reconstruct%20masked%20local%20image%20regions.%0AThis%20module%20significantly%20strengthens%20the%20model%27s%20cross-modal%20information%0Ainteraction%20capabilities%20and%20retains%20low-level%20image%20features%20essential%20for%0Adownstream%20tasks.%20By%20well%20handling%20the%20aforementioned%20limitations%2C%20the%20proposed%0ACM-CGNS%20can%20learn%20effective%20and%20robust%20medical%20visual%20representations%20suitable%0Afor%20various%20recognition%20tasks.%20Extensive%20experimental%20results%20on%0Aclassification%2C%20detection%2C%20and%20segmentation%20tasks%20across%20five%20downstream%0Adatasets%20show%20that%20our%20method%20outperforms%20state-of-the-art%20approaches%20on%0Amultiple%20metrics%2C%20verifying%20its%20superior%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11674v1&entry.124074799=Read"},
{"title": "GPLQ: A General, Practical, and Lightning QAT Method for Vision\n  Transformers", "author": "Guang Liang and Xinyao Liu and Jianxin Wu", "abstract": "  Vision Transformers (ViTs) are essential in computer vision but are\ncomputationally intensive, too. Model quantization, particularly to low\nbit-widths like 4-bit, aims to alleviate this difficulty, yet existing\nPost-Training Quantization (PTQ) and Quantization-Aware Training (QAT) methods\nexhibit significant limitations. PTQ often incurs substantial accuracy drop,\nwhile QAT achieves high accuracy but suffers from prohibitive computational\ncosts, limited generalization to downstream tasks, training instability, and\nlacking of open-source codebase. To address these challenges, this paper\nintroduces General, Practical, and Lightning Quantization (GPLQ), a novel\nframework designed for efficient and effective ViT quantization. GPLQ is\nfounded on two key empirical insights: the paramount importance of activation\nquantization and the necessity of preserving the model's original optimization\n``basin'' to maintain generalization. Consequently, GPLQ employs a sequential\n``activation-first, weights-later'' strategy. Stage 1 keeps weights in FP32\nwhile quantizing activations with a feature mimicking loss in only 1 epoch to\nkeep it stay in the same ``basin'', thereby preserving generalization. Stage 2\nquantizes weights using a PTQ method. As a result, GPLQ is 100x faster than\nexisting QAT methods, lowers memory footprint to levels even below FP32\ntraining, and achieves 4-bit model performance that is highly competitive with\nFP32 models in terms of both accuracy on ImageNet and generalization to diverse\ndownstream tasks, including fine-grained visual classification and object\ndetection. We will release an easy-to-use open-source toolkit supporting\nmultiple vision tasks.\n", "link": "http://arxiv.org/abs/2506.11784v1", "date": "2025-06-13", "relevancy": 2.2092, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5582}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5514}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPLQ%3A%20A%20General%2C%20Practical%2C%20and%20Lightning%20QAT%20Method%20for%20Vision%0A%20%20Transformers&body=Title%3A%20GPLQ%3A%20A%20General%2C%20Practical%2C%20and%20Lightning%20QAT%20Method%20for%20Vision%0A%20%20Transformers%0AAuthor%3A%20Guang%20Liang%20and%20Xinyao%20Liu%20and%20Jianxin%20Wu%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20are%20essential%20in%20computer%20vision%20but%20are%0Acomputationally%20intensive%2C%20too.%20Model%20quantization%2C%20particularly%20to%20low%0Abit-widths%20like%204-bit%2C%20aims%20to%20alleviate%20this%20difficulty%2C%20yet%20existing%0APost-Training%20Quantization%20%28PTQ%29%20and%20Quantization-Aware%20Training%20%28QAT%29%20methods%0Aexhibit%20significant%20limitations.%20PTQ%20often%20incurs%20substantial%20accuracy%20drop%2C%0Awhile%20QAT%20achieves%20high%20accuracy%20but%20suffers%20from%20prohibitive%20computational%0Acosts%2C%20limited%20generalization%20to%20downstream%20tasks%2C%20training%20instability%2C%20and%0Alacking%20of%20open-source%20codebase.%20To%20address%20these%20challenges%2C%20this%20paper%0Aintroduces%20General%2C%20Practical%2C%20and%20Lightning%20Quantization%20%28GPLQ%29%2C%20a%20novel%0Aframework%20designed%20for%20efficient%20and%20effective%20ViT%20quantization.%20GPLQ%20is%0Afounded%20on%20two%20key%20empirical%20insights%3A%20the%20paramount%20importance%20of%20activation%0Aquantization%20and%20the%20necessity%20of%20preserving%20the%20model%27s%20original%20optimization%0A%60%60basin%27%27%20to%20maintain%20generalization.%20Consequently%2C%20GPLQ%20employs%20a%20sequential%0A%60%60activation-first%2C%20weights-later%27%27%20strategy.%20Stage%201%20keeps%20weights%20in%20FP32%0Awhile%20quantizing%20activations%20with%20a%20feature%20mimicking%20loss%20in%20only%201%20epoch%20to%0Akeep%20it%20stay%20in%20the%20same%20%60%60basin%27%27%2C%20thereby%20preserving%20generalization.%20Stage%202%0Aquantizes%20weights%20using%20a%20PTQ%20method.%20As%20a%20result%2C%20GPLQ%20is%20100x%20faster%20than%0Aexisting%20QAT%20methods%2C%20lowers%20memory%20footprint%20to%20levels%20even%20below%20FP32%0Atraining%2C%20and%20achieves%204-bit%20model%20performance%20that%20is%20highly%20competitive%20with%0AFP32%20models%20in%20terms%20of%20both%20accuracy%20on%20ImageNet%20and%20generalization%20to%20diverse%0Adownstream%20tasks%2C%20including%20fine-grained%20visual%20classification%20and%20object%0Adetection.%20We%20will%20release%20an%20easy-to-use%20open-source%20toolkit%20supporting%0Amultiple%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPLQ%253A%2520A%2520General%252C%2520Practical%252C%2520and%2520Lightning%2520QAT%2520Method%2520for%2520Vision%250A%2520%2520Transformers%26entry.906535625%3DGuang%2520Liang%2520and%2520Xinyao%2520Liu%2520and%2520Jianxin%2520Wu%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520are%2520essential%2520in%2520computer%2520vision%2520but%2520are%250Acomputationally%2520intensive%252C%2520too.%2520Model%2520quantization%252C%2520particularly%2520to%2520low%250Abit-widths%2520like%25204-bit%252C%2520aims%2520to%2520alleviate%2520this%2520difficulty%252C%2520yet%2520existing%250APost-Training%2520Quantization%2520%2528PTQ%2529%2520and%2520Quantization-Aware%2520Training%2520%2528QAT%2529%2520methods%250Aexhibit%2520significant%2520limitations.%2520PTQ%2520often%2520incurs%2520substantial%2520accuracy%2520drop%252C%250Awhile%2520QAT%2520achieves%2520high%2520accuracy%2520but%2520suffers%2520from%2520prohibitive%2520computational%250Acosts%252C%2520limited%2520generalization%2520to%2520downstream%2520tasks%252C%2520training%2520instability%252C%2520and%250Alacking%2520of%2520open-source%2520codebase.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%250Aintroduces%2520General%252C%2520Practical%252C%2520and%2520Lightning%2520Quantization%2520%2528GPLQ%2529%252C%2520a%2520novel%250Aframework%2520designed%2520for%2520efficient%2520and%2520effective%2520ViT%2520quantization.%2520GPLQ%2520is%250Afounded%2520on%2520two%2520key%2520empirical%2520insights%253A%2520the%2520paramount%2520importance%2520of%2520activation%250Aquantization%2520and%2520the%2520necessity%2520of%2520preserving%2520the%2520model%2527s%2520original%2520optimization%250A%2560%2560basin%2527%2527%2520to%2520maintain%2520generalization.%2520Consequently%252C%2520GPLQ%2520employs%2520a%2520sequential%250A%2560%2560activation-first%252C%2520weights-later%2527%2527%2520strategy.%2520Stage%25201%2520keeps%2520weights%2520in%2520FP32%250Awhile%2520quantizing%2520activations%2520with%2520a%2520feature%2520mimicking%2520loss%2520in%2520only%25201%2520epoch%2520to%250Akeep%2520it%2520stay%2520in%2520the%2520same%2520%2560%2560basin%2527%2527%252C%2520thereby%2520preserving%2520generalization.%2520Stage%25202%250Aquantizes%2520weights%2520using%2520a%2520PTQ%2520method.%2520As%2520a%2520result%252C%2520GPLQ%2520is%2520100x%2520faster%2520than%250Aexisting%2520QAT%2520methods%252C%2520lowers%2520memory%2520footprint%2520to%2520levels%2520even%2520below%2520FP32%250Atraining%252C%2520and%2520achieves%25204-bit%2520model%2520performance%2520that%2520is%2520highly%2520competitive%2520with%250AFP32%2520models%2520in%2520terms%2520of%2520both%2520accuracy%2520on%2520ImageNet%2520and%2520generalization%2520to%2520diverse%250Adownstream%2520tasks%252C%2520including%2520fine-grained%2520visual%2520classification%2520and%2520object%250Adetection.%2520We%2520will%2520release%2520an%2520easy-to-use%2520open-source%2520toolkit%2520supporting%250Amultiple%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPLQ%3A%20A%20General%2C%20Practical%2C%20and%20Lightning%20QAT%20Method%20for%20Vision%0A%20%20Transformers&entry.906535625=Guang%20Liang%20and%20Xinyao%20Liu%20and%20Jianxin%20Wu&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20are%20essential%20in%20computer%20vision%20but%20are%0Acomputationally%20intensive%2C%20too.%20Model%20quantization%2C%20particularly%20to%20low%0Abit-widths%20like%204-bit%2C%20aims%20to%20alleviate%20this%20difficulty%2C%20yet%20existing%0APost-Training%20Quantization%20%28PTQ%29%20and%20Quantization-Aware%20Training%20%28QAT%29%20methods%0Aexhibit%20significant%20limitations.%20PTQ%20often%20incurs%20substantial%20accuracy%20drop%2C%0Awhile%20QAT%20achieves%20high%20accuracy%20but%20suffers%20from%20prohibitive%20computational%0Acosts%2C%20limited%20generalization%20to%20downstream%20tasks%2C%20training%20instability%2C%20and%0Alacking%20of%20open-source%20codebase.%20To%20address%20these%20challenges%2C%20this%20paper%0Aintroduces%20General%2C%20Practical%2C%20and%20Lightning%20Quantization%20%28GPLQ%29%2C%20a%20novel%0Aframework%20designed%20for%20efficient%20and%20effective%20ViT%20quantization.%20GPLQ%20is%0Afounded%20on%20two%20key%20empirical%20insights%3A%20the%20paramount%20importance%20of%20activation%0Aquantization%20and%20the%20necessity%20of%20preserving%20the%20model%27s%20original%20optimization%0A%60%60basin%27%27%20to%20maintain%20generalization.%20Consequently%2C%20GPLQ%20employs%20a%20sequential%0A%60%60activation-first%2C%20weights-later%27%27%20strategy.%20Stage%201%20keeps%20weights%20in%20FP32%0Awhile%20quantizing%20activations%20with%20a%20feature%20mimicking%20loss%20in%20only%201%20epoch%20to%0Akeep%20it%20stay%20in%20the%20same%20%60%60basin%27%27%2C%20thereby%20preserving%20generalization.%20Stage%202%0Aquantizes%20weights%20using%20a%20PTQ%20method.%20As%20a%20result%2C%20GPLQ%20is%20100x%20faster%20than%0Aexisting%20QAT%20methods%2C%20lowers%20memory%20footprint%20to%20levels%20even%20below%20FP32%0Atraining%2C%20and%20achieves%204-bit%20model%20performance%20that%20is%20highly%20competitive%20with%0AFP32%20models%20in%20terms%20of%20both%20accuracy%20on%20ImageNet%20and%20generalization%20to%20diverse%0Adownstream%20tasks%2C%20including%20fine-grained%20visual%20classification%20and%20object%0Adetection.%20We%20will%20release%20an%20easy-to-use%20open-source%20toolkit%20supporting%0Amultiple%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11784v1&entry.124074799=Read"},
{"title": "Self-supervised Learning of Echocardiographic Video Representations via\n  Online Cluster Distillation", "author": "Divyanshu Mishra and Mohammadreza Salehi and Pramit Saha and Olga Patey and Aris T. Papageorghiou and Yuki M. Asano and J. Alison Noble", "abstract": "  Self-supervised learning (SSL) has achieved major advances in natural images\nand video understanding, but challenges remain in domains like echocardiography\n(heart ultrasound) due to subtle anatomical structures, complex temporal\ndynamics, and the current lack of domain-specific pre-trained models. Existing\nSSL approaches such as contrastive, masked modeling, and clustering-based\nmethods struggle with high intersample similarity, sensitivity to low PSNR\ninputs common in ultrasound, or aggressive augmentations that distort\nclinically relevant features. We present DISCOVR (Distilled Image Supervision\nfor Cross Modal Video Representation), a self-supervised dual branch framework\nfor cardiac ultrasound video representation learning. DISCOVR combines a\nclustering-based video encoder that models temporal dynamics with an online\nimage encoder that extracts fine-grained spatial semantics. These branches are\nconnected through a semantic cluster distillation loss that transfers\nanatomical knowledge from the evolving image encoder to the video encoder,\nenabling temporally coherent representations enriched with fine-grained\nsemantic understanding. Evaluated on six echocardiography datasets spanning\nfetal, pediatric, and adult populations, DISCOVR outperforms both specialized\nvideo anomaly detection methods and state-of-the-art video-SSL baselines in\nzero-shot and linear probing setups, and achieves superior segmentation\ntransfer.\n", "link": "http://arxiv.org/abs/2506.11777v1", "date": "2025-06-13", "relevancy": 2.2019, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5606}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5471}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Learning%20of%20Echocardiographic%20Video%20Representations%20via%0A%20%20Online%20Cluster%20Distillation&body=Title%3A%20Self-supervised%20Learning%20of%20Echocardiographic%20Video%20Representations%20via%0A%20%20Online%20Cluster%20Distillation%0AAuthor%3A%20Divyanshu%20Mishra%20and%20Mohammadreza%20Salehi%20and%20Pramit%20Saha%20and%20Olga%20Patey%20and%20Aris%20T.%20Papageorghiou%20and%20Yuki%20M.%20Asano%20and%20J.%20Alison%20Noble%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20achieved%20major%20advances%20in%20natural%20images%0Aand%20video%20understanding%2C%20but%20challenges%20remain%20in%20domains%20like%20echocardiography%0A%28heart%20ultrasound%29%20due%20to%20subtle%20anatomical%20structures%2C%20complex%20temporal%0Adynamics%2C%20and%20the%20current%20lack%20of%20domain-specific%20pre-trained%20models.%20Existing%0ASSL%20approaches%20such%20as%20contrastive%2C%20masked%20modeling%2C%20and%20clustering-based%0Amethods%20struggle%20with%20high%20intersample%20similarity%2C%20sensitivity%20to%20low%20PSNR%0Ainputs%20common%20in%20ultrasound%2C%20or%20aggressive%20augmentations%20that%20distort%0Aclinically%20relevant%20features.%20We%20present%20DISCOVR%20%28Distilled%20Image%20Supervision%0Afor%20Cross%20Modal%20Video%20Representation%29%2C%20a%20self-supervised%20dual%20branch%20framework%0Afor%20cardiac%20ultrasound%20video%20representation%20learning.%20DISCOVR%20combines%20a%0Aclustering-based%20video%20encoder%20that%20models%20temporal%20dynamics%20with%20an%20online%0Aimage%20encoder%20that%20extracts%20fine-grained%20spatial%20semantics.%20These%20branches%20are%0Aconnected%20through%20a%20semantic%20cluster%20distillation%20loss%20that%20transfers%0Aanatomical%20knowledge%20from%20the%20evolving%20image%20encoder%20to%20the%20video%20encoder%2C%0Aenabling%20temporally%20coherent%20representations%20enriched%20with%20fine-grained%0Asemantic%20understanding.%20Evaluated%20on%20six%20echocardiography%20datasets%20spanning%0Afetal%2C%20pediatric%2C%20and%20adult%20populations%2C%20DISCOVR%20outperforms%20both%20specialized%0Avideo%20anomaly%20detection%20methods%20and%20state-of-the-art%20video-SSL%20baselines%20in%0Azero-shot%20and%20linear%20probing%20setups%2C%20and%20achieves%20superior%20segmentation%0Atransfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Learning%2520of%2520Echocardiographic%2520Video%2520Representations%2520via%250A%2520%2520Online%2520Cluster%2520Distillation%26entry.906535625%3DDivyanshu%2520Mishra%2520and%2520Mohammadreza%2520Salehi%2520and%2520Pramit%2520Saha%2520and%2520Olga%2520Patey%2520and%2520Aris%2520T.%2520Papageorghiou%2520and%2520Yuki%2520M.%2520Asano%2520and%2520J.%2520Alison%2520Noble%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520achieved%2520major%2520advances%2520in%2520natural%2520images%250Aand%2520video%2520understanding%252C%2520but%2520challenges%2520remain%2520in%2520domains%2520like%2520echocardiography%250A%2528heart%2520ultrasound%2529%2520due%2520to%2520subtle%2520anatomical%2520structures%252C%2520complex%2520temporal%250Adynamics%252C%2520and%2520the%2520current%2520lack%2520of%2520domain-specific%2520pre-trained%2520models.%2520Existing%250ASSL%2520approaches%2520such%2520as%2520contrastive%252C%2520masked%2520modeling%252C%2520and%2520clustering-based%250Amethods%2520struggle%2520with%2520high%2520intersample%2520similarity%252C%2520sensitivity%2520to%2520low%2520PSNR%250Ainputs%2520common%2520in%2520ultrasound%252C%2520or%2520aggressive%2520augmentations%2520that%2520distort%250Aclinically%2520relevant%2520features.%2520We%2520present%2520DISCOVR%2520%2528Distilled%2520Image%2520Supervision%250Afor%2520Cross%2520Modal%2520Video%2520Representation%2529%252C%2520a%2520self-supervised%2520dual%2520branch%2520framework%250Afor%2520cardiac%2520ultrasound%2520video%2520representation%2520learning.%2520DISCOVR%2520combines%2520a%250Aclustering-based%2520video%2520encoder%2520that%2520models%2520temporal%2520dynamics%2520with%2520an%2520online%250Aimage%2520encoder%2520that%2520extracts%2520fine-grained%2520spatial%2520semantics.%2520These%2520branches%2520are%250Aconnected%2520through%2520a%2520semantic%2520cluster%2520distillation%2520loss%2520that%2520transfers%250Aanatomical%2520knowledge%2520from%2520the%2520evolving%2520image%2520encoder%2520to%2520the%2520video%2520encoder%252C%250Aenabling%2520temporally%2520coherent%2520representations%2520enriched%2520with%2520fine-grained%250Asemantic%2520understanding.%2520Evaluated%2520on%2520six%2520echocardiography%2520datasets%2520spanning%250Afetal%252C%2520pediatric%252C%2520and%2520adult%2520populations%252C%2520DISCOVR%2520outperforms%2520both%2520specialized%250Avideo%2520anomaly%2520detection%2520methods%2520and%2520state-of-the-art%2520video-SSL%2520baselines%2520in%250Azero-shot%2520and%2520linear%2520probing%2520setups%252C%2520and%2520achieves%2520superior%2520segmentation%250Atransfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Learning%20of%20Echocardiographic%20Video%20Representations%20via%0A%20%20Online%20Cluster%20Distillation&entry.906535625=Divyanshu%20Mishra%20and%20Mohammadreza%20Salehi%20and%20Pramit%20Saha%20and%20Olga%20Patey%20and%20Aris%20T.%20Papageorghiou%20and%20Yuki%20M.%20Asano%20and%20J.%20Alison%20Noble&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20achieved%20major%20advances%20in%20natural%20images%0Aand%20video%20understanding%2C%20but%20challenges%20remain%20in%20domains%20like%20echocardiography%0A%28heart%20ultrasound%29%20due%20to%20subtle%20anatomical%20structures%2C%20complex%20temporal%0Adynamics%2C%20and%20the%20current%20lack%20of%20domain-specific%20pre-trained%20models.%20Existing%0ASSL%20approaches%20such%20as%20contrastive%2C%20masked%20modeling%2C%20and%20clustering-based%0Amethods%20struggle%20with%20high%20intersample%20similarity%2C%20sensitivity%20to%20low%20PSNR%0Ainputs%20common%20in%20ultrasound%2C%20or%20aggressive%20augmentations%20that%20distort%0Aclinically%20relevant%20features.%20We%20present%20DISCOVR%20%28Distilled%20Image%20Supervision%0Afor%20Cross%20Modal%20Video%20Representation%29%2C%20a%20self-supervised%20dual%20branch%20framework%0Afor%20cardiac%20ultrasound%20video%20representation%20learning.%20DISCOVR%20combines%20a%0Aclustering-based%20video%20encoder%20that%20models%20temporal%20dynamics%20with%20an%20online%0Aimage%20encoder%20that%20extracts%20fine-grained%20spatial%20semantics.%20These%20branches%20are%0Aconnected%20through%20a%20semantic%20cluster%20distillation%20loss%20that%20transfers%0Aanatomical%20knowledge%20from%20the%20evolving%20image%20encoder%20to%20the%20video%20encoder%2C%0Aenabling%20temporally%20coherent%20representations%20enriched%20with%20fine-grained%0Asemantic%20understanding.%20Evaluated%20on%20six%20echocardiography%20datasets%20spanning%0Afetal%2C%20pediatric%2C%20and%20adult%20populations%2C%20DISCOVR%20outperforms%20both%20specialized%0Avideo%20anomaly%20detection%20methods%20and%20state-of-the-art%20video-SSL%20baselines%20in%0Azero-shot%20and%20linear%20probing%20setups%2C%20and%20achieves%20superior%20segmentation%0Atransfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11777v1&entry.124074799=Read"},
{"title": "Interior Point Differential Dynamic Programming, Redux", "author": "Ming Xu and Stephen Gould and Iman Shames", "abstract": "  We present IPDDP2, a structure-exploiting algorithm for solving\ndiscrete-time, finite-horizon optimal control problems (OCPs) with nonlinear\nconstraints. Inequality constraints are handled using a primal-dual interior\npoint formulation and step acceptance for equality constraints follows a\nline-search filter approach. The iterates of the algorithm are derived under\nthe Differential Dynamic Programming (DDP) framework. A proof of local\nquadratic convergence of the IPDDP2 iterates is provided. Our numerical\nexperiments evaluate IPDDP2 on over 500 OCPs derived from five different\nclasses of robotic motion planning problems, three of which are\ncontact-implicit trajectory optimisation problems. IPDDP2 demonstrates\nimprovements in robustness against existing constrained DDP algorithms for\ncontact-implicit planning, while being significantly faster than\ngeneral-purpose solver IPOPT. We provide a full implementation of IPDDP2 in the\nJulia programming language.\n", "link": "http://arxiv.org/abs/2504.08278v3", "date": "2025-06-13", "relevancy": 2.1994, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4442}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4403}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interior%20Point%20Differential%20Dynamic%20Programming%2C%20Redux&body=Title%3A%20Interior%20Point%20Differential%20Dynamic%20Programming%2C%20Redux%0AAuthor%3A%20Ming%20Xu%20and%20Stephen%20Gould%20and%20Iman%20Shames%0AAbstract%3A%20%20%20We%20present%20IPDDP2%2C%20a%20structure-exploiting%20algorithm%20for%20solving%0Adiscrete-time%2C%20finite-horizon%20optimal%20control%20problems%20%28OCPs%29%20with%20nonlinear%0Aconstraints.%20Inequality%20constraints%20are%20handled%20using%20a%20primal-dual%20interior%0Apoint%20formulation%20and%20step%20acceptance%20for%20equality%20constraints%20follows%20a%0Aline-search%20filter%20approach.%20The%20iterates%20of%20the%20algorithm%20are%20derived%20under%0Athe%20Differential%20Dynamic%20Programming%20%28DDP%29%20framework.%20A%20proof%20of%20local%0Aquadratic%20convergence%20of%20the%20IPDDP2%20iterates%20is%20provided.%20Our%20numerical%0Aexperiments%20evaluate%20IPDDP2%20on%20over%20500%20OCPs%20derived%20from%20five%20different%0Aclasses%20of%20robotic%20motion%20planning%20problems%2C%20three%20of%20which%20are%0Acontact-implicit%20trajectory%20optimisation%20problems.%20IPDDP2%20demonstrates%0Aimprovements%20in%20robustness%20against%20existing%20constrained%20DDP%20algorithms%20for%0Acontact-implicit%20planning%2C%20while%20being%20significantly%20faster%20than%0Ageneral-purpose%20solver%20IPOPT.%20We%20provide%20a%20full%20implementation%20of%20IPDDP2%20in%20the%0AJulia%20programming%20language.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08278v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterior%2520Point%2520Differential%2520Dynamic%2520Programming%252C%2520Redux%26entry.906535625%3DMing%2520Xu%2520and%2520Stephen%2520Gould%2520and%2520Iman%2520Shames%26entry.1292438233%3D%2520%2520We%2520present%2520IPDDP2%252C%2520a%2520structure-exploiting%2520algorithm%2520for%2520solving%250Adiscrete-time%252C%2520finite-horizon%2520optimal%2520control%2520problems%2520%2528OCPs%2529%2520with%2520nonlinear%250Aconstraints.%2520Inequality%2520constraints%2520are%2520handled%2520using%2520a%2520primal-dual%2520interior%250Apoint%2520formulation%2520and%2520step%2520acceptance%2520for%2520equality%2520constraints%2520follows%2520a%250Aline-search%2520filter%2520approach.%2520The%2520iterates%2520of%2520the%2520algorithm%2520are%2520derived%2520under%250Athe%2520Differential%2520Dynamic%2520Programming%2520%2528DDP%2529%2520framework.%2520A%2520proof%2520of%2520local%250Aquadratic%2520convergence%2520of%2520the%2520IPDDP2%2520iterates%2520is%2520provided.%2520Our%2520numerical%250Aexperiments%2520evaluate%2520IPDDP2%2520on%2520over%2520500%2520OCPs%2520derived%2520from%2520five%2520different%250Aclasses%2520of%2520robotic%2520motion%2520planning%2520problems%252C%2520three%2520of%2520which%2520are%250Acontact-implicit%2520trajectory%2520optimisation%2520problems.%2520IPDDP2%2520demonstrates%250Aimprovements%2520in%2520robustness%2520against%2520existing%2520constrained%2520DDP%2520algorithms%2520for%250Acontact-implicit%2520planning%252C%2520while%2520being%2520significantly%2520faster%2520than%250Ageneral-purpose%2520solver%2520IPOPT.%2520We%2520provide%2520a%2520full%2520implementation%2520of%2520IPDDP2%2520in%2520the%250AJulia%2520programming%2520language.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08278v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interior%20Point%20Differential%20Dynamic%20Programming%2C%20Redux&entry.906535625=Ming%20Xu%20and%20Stephen%20Gould%20and%20Iman%20Shames&entry.1292438233=%20%20We%20present%20IPDDP2%2C%20a%20structure-exploiting%20algorithm%20for%20solving%0Adiscrete-time%2C%20finite-horizon%20optimal%20control%20problems%20%28OCPs%29%20with%20nonlinear%0Aconstraints.%20Inequality%20constraints%20are%20handled%20using%20a%20primal-dual%20interior%0Apoint%20formulation%20and%20step%20acceptance%20for%20equality%20constraints%20follows%20a%0Aline-search%20filter%20approach.%20The%20iterates%20of%20the%20algorithm%20are%20derived%20under%0Athe%20Differential%20Dynamic%20Programming%20%28DDP%29%20framework.%20A%20proof%20of%20local%0Aquadratic%20convergence%20of%20the%20IPDDP2%20iterates%20is%20provided.%20Our%20numerical%0Aexperiments%20evaluate%20IPDDP2%20on%20over%20500%20OCPs%20derived%20from%20five%20different%0Aclasses%20of%20robotic%20motion%20planning%20problems%2C%20three%20of%20which%20are%0Acontact-implicit%20trajectory%20optimisation%20problems.%20IPDDP2%20demonstrates%0Aimprovements%20in%20robustness%20against%20existing%20constrained%20DDP%20algorithms%20for%0Acontact-implicit%20planning%2C%20while%20being%20significantly%20faster%20than%0Ageneral-purpose%20solver%20IPOPT.%20We%20provide%20a%20full%20implementation%20of%20IPDDP2%20in%20the%0AJulia%20programming%20language.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08278v3&entry.124074799=Read"},
{"title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and\n  English", "author": "Haoyang Zhang and Hexin Liu and Xiangyu Zhang and Qiquan Zhang and Yuchen Hu and Junqi Zhao and Fei Tian and Xuerui Yang and Leibny Paola Garcia and Eng Siong Chng", "abstract": "  The speech tokenizer plays a crucial role in recent speech tasks, generally\nserving as a bridge between speech signals and language models. While\nlow-frame-rate codecs are widely employed as speech tokenizers, the impact of\nframe rates on speech tokens remains underexplored. In this study, we\ninvestigate how varying frame rates affect speech tokenization by examining\nMandarin and English, two typologically distinct languages. We encode speech at\ndifferent frame rates and evaluate the resulting semantic tokens in the speech\nrecognition task. Our findings reveal that frame rate variations influence\nspeech tokenization differently for each language, highlighting the interplay\nbetween frame rates, phonetic density, and language-specific acoustic features.\nThe results provide insights into optimizing frame rate selection for speech\ntokenizers, with implications for automatic speech recognition, text-to-speech,\nand other speech-related applications.\n", "link": "http://arxiv.org/abs/2505.17076v3", "date": "2025-06-13", "relevancy": 2.169, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4248}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impact%20of%20Frame%20Rates%20on%20Speech%20Tokenizer%3A%20A%20Case%20Study%20on%20Mandarin%20and%0A%20%20English&body=Title%3A%20Impact%20of%20Frame%20Rates%20on%20Speech%20Tokenizer%3A%20A%20Case%20Study%20on%20Mandarin%20and%0A%20%20English%0AAuthor%3A%20Haoyang%20Zhang%20and%20Hexin%20Liu%20and%20Xiangyu%20Zhang%20and%20Qiquan%20Zhang%20and%20Yuchen%20Hu%20and%20Junqi%20Zhao%20and%20Fei%20Tian%20and%20Xuerui%20Yang%20and%20Leibny%20Paola%20Garcia%20and%20Eng%20Siong%20Chng%0AAbstract%3A%20%20%20The%20speech%20tokenizer%20plays%20a%20crucial%20role%20in%20recent%20speech%20tasks%2C%20generally%0Aserving%20as%20a%20bridge%20between%20speech%20signals%20and%20language%20models.%20While%0Alow-frame-rate%20codecs%20are%20widely%20employed%20as%20speech%20tokenizers%2C%20the%20impact%20of%0Aframe%20rates%20on%20speech%20tokens%20remains%20underexplored.%20In%20this%20study%2C%20we%0Ainvestigate%20how%20varying%20frame%20rates%20affect%20speech%20tokenization%20by%20examining%0AMandarin%20and%20English%2C%20two%20typologically%20distinct%20languages.%20We%20encode%20speech%20at%0Adifferent%20frame%20rates%20and%20evaluate%20the%20resulting%20semantic%20tokens%20in%20the%20speech%0Arecognition%20task.%20Our%20findings%20reveal%20that%20frame%20rate%20variations%20influence%0Aspeech%20tokenization%20differently%20for%20each%20language%2C%20highlighting%20the%20interplay%0Abetween%20frame%20rates%2C%20phonetic%20density%2C%20and%20language-specific%20acoustic%20features.%0AThe%20results%20provide%20insights%20into%20optimizing%20frame%20rate%20selection%20for%20speech%0Atokenizers%2C%20with%20implications%20for%20automatic%20speech%20recognition%2C%20text-to-speech%2C%0Aand%20other%20speech-related%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17076v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpact%2520of%2520Frame%2520Rates%2520on%2520Speech%2520Tokenizer%253A%2520A%2520Case%2520Study%2520on%2520Mandarin%2520and%250A%2520%2520English%26entry.906535625%3DHaoyang%2520Zhang%2520and%2520Hexin%2520Liu%2520and%2520Xiangyu%2520Zhang%2520and%2520Qiquan%2520Zhang%2520and%2520Yuchen%2520Hu%2520and%2520Junqi%2520Zhao%2520and%2520Fei%2520Tian%2520and%2520Xuerui%2520Yang%2520and%2520Leibny%2520Paola%2520Garcia%2520and%2520Eng%2520Siong%2520Chng%26entry.1292438233%3D%2520%2520The%2520speech%2520tokenizer%2520plays%2520a%2520crucial%2520role%2520in%2520recent%2520speech%2520tasks%252C%2520generally%250Aserving%2520as%2520a%2520bridge%2520between%2520speech%2520signals%2520and%2520language%2520models.%2520While%250Alow-frame-rate%2520codecs%2520are%2520widely%2520employed%2520as%2520speech%2520tokenizers%252C%2520the%2520impact%2520of%250Aframe%2520rates%2520on%2520speech%2520tokens%2520remains%2520underexplored.%2520In%2520this%2520study%252C%2520we%250Ainvestigate%2520how%2520varying%2520frame%2520rates%2520affect%2520speech%2520tokenization%2520by%2520examining%250AMandarin%2520and%2520English%252C%2520two%2520typologically%2520distinct%2520languages.%2520We%2520encode%2520speech%2520at%250Adifferent%2520frame%2520rates%2520and%2520evaluate%2520the%2520resulting%2520semantic%2520tokens%2520in%2520the%2520speech%250Arecognition%2520task.%2520Our%2520findings%2520reveal%2520that%2520frame%2520rate%2520variations%2520influence%250Aspeech%2520tokenization%2520differently%2520for%2520each%2520language%252C%2520highlighting%2520the%2520interplay%250Abetween%2520frame%2520rates%252C%2520phonetic%2520density%252C%2520and%2520language-specific%2520acoustic%2520features.%250AThe%2520results%2520provide%2520insights%2520into%2520optimizing%2520frame%2520rate%2520selection%2520for%2520speech%250Atokenizers%252C%2520with%2520implications%2520for%2520automatic%2520speech%2520recognition%252C%2520text-to-speech%252C%250Aand%2520other%2520speech-related%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17076v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impact%20of%20Frame%20Rates%20on%20Speech%20Tokenizer%3A%20A%20Case%20Study%20on%20Mandarin%20and%0A%20%20English&entry.906535625=Haoyang%20Zhang%20and%20Hexin%20Liu%20and%20Xiangyu%20Zhang%20and%20Qiquan%20Zhang%20and%20Yuchen%20Hu%20and%20Junqi%20Zhao%20and%20Fei%20Tian%20and%20Xuerui%20Yang%20and%20Leibny%20Paola%20Garcia%20and%20Eng%20Siong%20Chng&entry.1292438233=%20%20The%20speech%20tokenizer%20plays%20a%20crucial%20role%20in%20recent%20speech%20tasks%2C%20generally%0Aserving%20as%20a%20bridge%20between%20speech%20signals%20and%20language%20models.%20While%0Alow-frame-rate%20codecs%20are%20widely%20employed%20as%20speech%20tokenizers%2C%20the%20impact%20of%0Aframe%20rates%20on%20speech%20tokens%20remains%20underexplored.%20In%20this%20study%2C%20we%0Ainvestigate%20how%20varying%20frame%20rates%20affect%20speech%20tokenization%20by%20examining%0AMandarin%20and%20English%2C%20two%20typologically%20distinct%20languages.%20We%20encode%20speech%20at%0Adifferent%20frame%20rates%20and%20evaluate%20the%20resulting%20semantic%20tokens%20in%20the%20speech%0Arecognition%20task.%20Our%20findings%20reveal%20that%20frame%20rate%20variations%20influence%0Aspeech%20tokenization%20differently%20for%20each%20language%2C%20highlighting%20the%20interplay%0Abetween%20frame%20rates%2C%20phonetic%20density%2C%20and%20language-specific%20acoustic%20features.%0AThe%20results%20provide%20insights%20into%20optimizing%20frame%20rate%20selection%20for%20speech%0Atokenizers%2C%20with%20implications%20for%20automatic%20speech%20recognition%2C%20text-to-speech%2C%0Aand%20other%20speech-related%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17076v3&entry.124074799=Read"},
{"title": "SphereDrag: Spherical Geometry-Aware Panoramic Image Editing", "author": "Zhiao Feng and Xuewei Li and Junjie Yang and Yuxin Peng and Xi Li", "abstract": "  Image editing has made great progress on planar images, but panoramic image\nediting remains underexplored. Due to their spherical geometry and projection\ndistortions, panoramic images present three key challenges: boundary\ndiscontinuity, trajectory deformation, and uneven pixel density. To tackle\nthese issues, we propose SphereDrag, a novel panoramic editing framework\nutilizing spherical geometry knowledge for accurate and controllable editing.\nSpecifically, adaptive reprojection (AR) uses adaptive spherical rotation to\ndeal with discontinuity; great-circle trajectory adjustment (GCTA) tracks the\nmovement trajectory more accurate; spherical search region tracking (SSRT)\nadaptively scales the search range based on spherical location to address\nuneven pixel density. Also, we construct PanoBench, a panoramic editing\nbenchmark, including complex editing tasks involving multiple objects and\ndiverse styles, which provides a standardized evaluation framework. Experiments\nshow that SphereDrag gains a considerable improvement compared with existing\nmethods in geometric consistency and image quality, achieving up to 10.5%\nrelative improvement.\n", "link": "http://arxiv.org/abs/2506.11863v1", "date": "2025-06-13", "relevancy": 2.1664, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5512}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5406}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SphereDrag%3A%20Spherical%20Geometry-Aware%20Panoramic%20Image%20Editing&body=Title%3A%20SphereDrag%3A%20Spherical%20Geometry-Aware%20Panoramic%20Image%20Editing%0AAuthor%3A%20Zhiao%20Feng%20and%20Xuewei%20Li%20and%20Junjie%20Yang%20and%20Yuxin%20Peng%20and%20Xi%20Li%0AAbstract%3A%20%20%20Image%20editing%20has%20made%20great%20progress%20on%20planar%20images%2C%20but%20panoramic%20image%0Aediting%20remains%20underexplored.%20Due%20to%20their%20spherical%20geometry%20and%20projection%0Adistortions%2C%20panoramic%20images%20present%20three%20key%20challenges%3A%20boundary%0Adiscontinuity%2C%20trajectory%20deformation%2C%20and%20uneven%20pixel%20density.%20To%20tackle%0Athese%20issues%2C%20we%20propose%20SphereDrag%2C%20a%20novel%20panoramic%20editing%20framework%0Autilizing%20spherical%20geometry%20knowledge%20for%20accurate%20and%20controllable%20editing.%0ASpecifically%2C%20adaptive%20reprojection%20%28AR%29%20uses%20adaptive%20spherical%20rotation%20to%0Adeal%20with%20discontinuity%3B%20great-circle%20trajectory%20adjustment%20%28GCTA%29%20tracks%20the%0Amovement%20trajectory%20more%20accurate%3B%20spherical%20search%20region%20tracking%20%28SSRT%29%0Aadaptively%20scales%20the%20search%20range%20based%20on%20spherical%20location%20to%20address%0Auneven%20pixel%20density.%20Also%2C%20we%20construct%20PanoBench%2C%20a%20panoramic%20editing%0Abenchmark%2C%20including%20complex%20editing%20tasks%20involving%20multiple%20objects%20and%0Adiverse%20styles%2C%20which%20provides%20a%20standardized%20evaluation%20framework.%20Experiments%0Ashow%20that%20SphereDrag%20gains%20a%20considerable%20improvement%20compared%20with%20existing%0Amethods%20in%20geometric%20consistency%20and%20image%20quality%2C%20achieving%20up%20to%2010.5%25%0Arelative%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSphereDrag%253A%2520Spherical%2520Geometry-Aware%2520Panoramic%2520Image%2520Editing%26entry.906535625%3DZhiao%2520Feng%2520and%2520Xuewei%2520Li%2520and%2520Junjie%2520Yang%2520and%2520Yuxin%2520Peng%2520and%2520Xi%2520Li%26entry.1292438233%3D%2520%2520Image%2520editing%2520has%2520made%2520great%2520progress%2520on%2520planar%2520images%252C%2520but%2520panoramic%2520image%250Aediting%2520remains%2520underexplored.%2520Due%2520to%2520their%2520spherical%2520geometry%2520and%2520projection%250Adistortions%252C%2520panoramic%2520images%2520present%2520three%2520key%2520challenges%253A%2520boundary%250Adiscontinuity%252C%2520trajectory%2520deformation%252C%2520and%2520uneven%2520pixel%2520density.%2520To%2520tackle%250Athese%2520issues%252C%2520we%2520propose%2520SphereDrag%252C%2520a%2520novel%2520panoramic%2520editing%2520framework%250Autilizing%2520spherical%2520geometry%2520knowledge%2520for%2520accurate%2520and%2520controllable%2520editing.%250ASpecifically%252C%2520adaptive%2520reprojection%2520%2528AR%2529%2520uses%2520adaptive%2520spherical%2520rotation%2520to%250Adeal%2520with%2520discontinuity%253B%2520great-circle%2520trajectory%2520adjustment%2520%2528GCTA%2529%2520tracks%2520the%250Amovement%2520trajectory%2520more%2520accurate%253B%2520spherical%2520search%2520region%2520tracking%2520%2528SSRT%2529%250Aadaptively%2520scales%2520the%2520search%2520range%2520based%2520on%2520spherical%2520location%2520to%2520address%250Auneven%2520pixel%2520density.%2520Also%252C%2520we%2520construct%2520PanoBench%252C%2520a%2520panoramic%2520editing%250Abenchmark%252C%2520including%2520complex%2520editing%2520tasks%2520involving%2520multiple%2520objects%2520and%250Adiverse%2520styles%252C%2520which%2520provides%2520a%2520standardized%2520evaluation%2520framework.%2520Experiments%250Ashow%2520that%2520SphereDrag%2520gains%2520a%2520considerable%2520improvement%2520compared%2520with%2520existing%250Amethods%2520in%2520geometric%2520consistency%2520and%2520image%2520quality%252C%2520achieving%2520up%2520to%252010.5%2525%250Arelative%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SphereDrag%3A%20Spherical%20Geometry-Aware%20Panoramic%20Image%20Editing&entry.906535625=Zhiao%20Feng%20and%20Xuewei%20Li%20and%20Junjie%20Yang%20and%20Yuxin%20Peng%20and%20Xi%20Li&entry.1292438233=%20%20Image%20editing%20has%20made%20great%20progress%20on%20planar%20images%2C%20but%20panoramic%20image%0Aediting%20remains%20underexplored.%20Due%20to%20their%20spherical%20geometry%20and%20projection%0Adistortions%2C%20panoramic%20images%20present%20three%20key%20challenges%3A%20boundary%0Adiscontinuity%2C%20trajectory%20deformation%2C%20and%20uneven%20pixel%20density.%20To%20tackle%0Athese%20issues%2C%20we%20propose%20SphereDrag%2C%20a%20novel%20panoramic%20editing%20framework%0Autilizing%20spherical%20geometry%20knowledge%20for%20accurate%20and%20controllable%20editing.%0ASpecifically%2C%20adaptive%20reprojection%20%28AR%29%20uses%20adaptive%20spherical%20rotation%20to%0Adeal%20with%20discontinuity%3B%20great-circle%20trajectory%20adjustment%20%28GCTA%29%20tracks%20the%0Amovement%20trajectory%20more%20accurate%3B%20spherical%20search%20region%20tracking%20%28SSRT%29%0Aadaptively%20scales%20the%20search%20range%20based%20on%20spherical%20location%20to%20address%0Auneven%20pixel%20density.%20Also%2C%20we%20construct%20PanoBench%2C%20a%20panoramic%20editing%0Abenchmark%2C%20including%20complex%20editing%20tasks%20involving%20multiple%20objects%20and%0Adiverse%20styles%2C%20which%20provides%20a%20standardized%20evaluation%20framework.%20Experiments%0Ashow%20that%20SphereDrag%20gains%20a%20considerable%20improvement%20compared%20with%20existing%0Amethods%20in%20geometric%20consistency%20and%20image%20quality%2C%20achieving%20up%20to%2010.5%25%0Arelative%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11863v1&entry.124074799=Read"},
{"title": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for\n  LLMs", "author": "Amrith Setlur and Matthew Y. R. Yang and Charlie Snell and Jeremy Greer and Ian Wu and Virginia Smith and Max Simchowitz and Aviral Kumar", "abstract": "  Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model.\n", "link": "http://arxiv.org/abs/2506.09026v2", "date": "2025-06-13", "relevancy": 2.1619, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5428}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5428}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20e3%3A%20Learning%20to%20Explore%20Enables%20Extrapolation%20of%20Test-Time%20Compute%20for%0A%20%20LLMs&body=Title%3A%20e3%3A%20Learning%20to%20Explore%20Enables%20Extrapolation%20of%20Test-Time%20Compute%20for%0A%20%20LLMs%0AAuthor%3A%20Amrith%20Setlur%20and%20Matthew%20Y.%20R.%20Yang%20and%20Charlie%20Snell%20and%20Jeremy%20Greer%20and%20Ian%20Wu%20and%20Virginia%20Smith%20and%20Max%20Simchowitz%20and%20Aviral%20Kumar%0AAbstract%3A%20%20%20Test-time%20scaling%20offers%20a%20promising%20path%20to%20improve%20LLM%20reasoning%20by%0Autilizing%20more%20compute%20at%20inference%20time%3B%20however%2C%20the%20true%20promise%20of%20this%0Aparadigm%20lies%20in%20extrapolation%20%28i.e.%2C%20improvement%20in%20performance%20on%20hard%0Aproblems%20as%20LLMs%20keep%20%22thinking%22%20for%20longer%2C%20beyond%20the%20maximum%20token%20budget%0Athey%20were%20trained%20on%29.%20Surprisingly%2C%20we%20find%20that%20most%20existing%20reasoning%0Amodels%20do%20not%20extrapolate%20well.%20We%20show%20that%20one%20way%20to%20enable%20extrapolation%20is%0Aby%20training%20the%20LLM%20to%20perform%20in-context%20exploration%3A%20training%20the%20LLM%20to%0Aeffectively%20spend%20its%20test%20time%20budget%20by%20chaining%20operations%20%28such%20as%0Ageneration%2C%20verification%2C%20refinement%2C%20etc.%29%2C%20or%20testing%20multiple%20hypotheses%0Abefore%20it%20commits%20to%20an%20answer.%20To%20enable%20in-context%20exploration%2C%20we%20identify%0Athree%20key%20ingredients%20as%20part%20of%20our%20recipe%20e3%3A%20%281%29%20chaining%20skills%20that%20the%0Abase%20LLM%20has%20asymmetric%20competence%20in%2C%20e.g.%2C%20chaining%20verification%20%28easy%29%20with%0Ageneration%20%28hard%29%2C%20as%20a%20way%20to%20implement%20in-context%20search%3B%20%282%29%20leveraging%0A%22negative%22%20gradients%20from%20incorrect%20traces%20to%20amplify%20exploration%20during%20RL%2C%0Aresulting%20in%20longer%20search%20traces%20that%20chains%20additional%20asymmetries%3B%20and%20%283%29%0Acoupling%20task%20difficulty%20with%20training%20token%20budget%20during%20training%20via%20a%0Aspecifically-designed%20curriculum%20to%20structure%20in-context%20exploration.%20Our%0Arecipe%20e3%20produces%20the%20best%20known%201.7B%20model%20according%20to%20AIME%2725%20and%20HMMT%2725%0Ascores%2C%20and%20extrapolates%20to%202x%20the%20training%20token%20budget.%20Our%20e3-1.7B%20model%20not%0Aonly%20attains%20high%20pass%401%20scores%2C%20but%20also%20improves%20pass%40k%20over%20the%20base%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09026v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3De3%253A%2520Learning%2520to%2520Explore%2520Enables%2520Extrapolation%2520of%2520Test-Time%2520Compute%2520for%250A%2520%2520LLMs%26entry.906535625%3DAmrith%2520Setlur%2520and%2520Matthew%2520Y.%2520R.%2520Yang%2520and%2520Charlie%2520Snell%2520and%2520Jeremy%2520Greer%2520and%2520Ian%2520Wu%2520and%2520Virginia%2520Smith%2520and%2520Max%2520Simchowitz%2520and%2520Aviral%2520Kumar%26entry.1292438233%3D%2520%2520Test-time%2520scaling%2520offers%2520a%2520promising%2520path%2520to%2520improve%2520LLM%2520reasoning%2520by%250Autilizing%2520more%2520compute%2520at%2520inference%2520time%253B%2520however%252C%2520the%2520true%2520promise%2520of%2520this%250Aparadigm%2520lies%2520in%2520extrapolation%2520%2528i.e.%252C%2520improvement%2520in%2520performance%2520on%2520hard%250Aproblems%2520as%2520LLMs%2520keep%2520%2522thinking%2522%2520for%2520longer%252C%2520beyond%2520the%2520maximum%2520token%2520budget%250Athey%2520were%2520trained%2520on%2529.%2520Surprisingly%252C%2520we%2520find%2520that%2520most%2520existing%2520reasoning%250Amodels%2520do%2520not%2520extrapolate%2520well.%2520We%2520show%2520that%2520one%2520way%2520to%2520enable%2520extrapolation%2520is%250Aby%2520training%2520the%2520LLM%2520to%2520perform%2520in-context%2520exploration%253A%2520training%2520the%2520LLM%2520to%250Aeffectively%2520spend%2520its%2520test%2520time%2520budget%2520by%2520chaining%2520operations%2520%2528such%2520as%250Ageneration%252C%2520verification%252C%2520refinement%252C%2520etc.%2529%252C%2520or%2520testing%2520multiple%2520hypotheses%250Abefore%2520it%2520commits%2520to%2520an%2520answer.%2520To%2520enable%2520in-context%2520exploration%252C%2520we%2520identify%250Athree%2520key%2520ingredients%2520as%2520part%2520of%2520our%2520recipe%2520e3%253A%2520%25281%2529%2520chaining%2520skills%2520that%2520the%250Abase%2520LLM%2520has%2520asymmetric%2520competence%2520in%252C%2520e.g.%252C%2520chaining%2520verification%2520%2528easy%2529%2520with%250Ageneration%2520%2528hard%2529%252C%2520as%2520a%2520way%2520to%2520implement%2520in-context%2520search%253B%2520%25282%2529%2520leveraging%250A%2522negative%2522%2520gradients%2520from%2520incorrect%2520traces%2520to%2520amplify%2520exploration%2520during%2520RL%252C%250Aresulting%2520in%2520longer%2520search%2520traces%2520that%2520chains%2520additional%2520asymmetries%253B%2520and%2520%25283%2529%250Acoupling%2520task%2520difficulty%2520with%2520training%2520token%2520budget%2520during%2520training%2520via%2520a%250Aspecifically-designed%2520curriculum%2520to%2520structure%2520in-context%2520exploration.%2520Our%250Arecipe%2520e3%2520produces%2520the%2520best%2520known%25201.7B%2520model%2520according%2520to%2520AIME%252725%2520and%2520HMMT%252725%250Ascores%252C%2520and%2520extrapolates%2520to%25202x%2520the%2520training%2520token%2520budget.%2520Our%2520e3-1.7B%2520model%2520not%250Aonly%2520attains%2520high%2520pass%25401%2520scores%252C%2520but%2520also%2520improves%2520pass%2540k%2520over%2520the%2520base%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09026v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=e3%3A%20Learning%20to%20Explore%20Enables%20Extrapolation%20of%20Test-Time%20Compute%20for%0A%20%20LLMs&entry.906535625=Amrith%20Setlur%20and%20Matthew%20Y.%20R.%20Yang%20and%20Charlie%20Snell%20and%20Jeremy%20Greer%20and%20Ian%20Wu%20and%20Virginia%20Smith%20and%20Max%20Simchowitz%20and%20Aviral%20Kumar&entry.1292438233=%20%20Test-time%20scaling%20offers%20a%20promising%20path%20to%20improve%20LLM%20reasoning%20by%0Autilizing%20more%20compute%20at%20inference%20time%3B%20however%2C%20the%20true%20promise%20of%20this%0Aparadigm%20lies%20in%20extrapolation%20%28i.e.%2C%20improvement%20in%20performance%20on%20hard%0Aproblems%20as%20LLMs%20keep%20%22thinking%22%20for%20longer%2C%20beyond%20the%20maximum%20token%20budget%0Athey%20were%20trained%20on%29.%20Surprisingly%2C%20we%20find%20that%20most%20existing%20reasoning%0Amodels%20do%20not%20extrapolate%20well.%20We%20show%20that%20one%20way%20to%20enable%20extrapolation%20is%0Aby%20training%20the%20LLM%20to%20perform%20in-context%20exploration%3A%20training%20the%20LLM%20to%0Aeffectively%20spend%20its%20test%20time%20budget%20by%20chaining%20operations%20%28such%20as%0Ageneration%2C%20verification%2C%20refinement%2C%20etc.%29%2C%20or%20testing%20multiple%20hypotheses%0Abefore%20it%20commits%20to%20an%20answer.%20To%20enable%20in-context%20exploration%2C%20we%20identify%0Athree%20key%20ingredients%20as%20part%20of%20our%20recipe%20e3%3A%20%281%29%20chaining%20skills%20that%20the%0Abase%20LLM%20has%20asymmetric%20competence%20in%2C%20e.g.%2C%20chaining%20verification%20%28easy%29%20with%0Ageneration%20%28hard%29%2C%20as%20a%20way%20to%20implement%20in-context%20search%3B%20%282%29%20leveraging%0A%22negative%22%20gradients%20from%20incorrect%20traces%20to%20amplify%20exploration%20during%20RL%2C%0Aresulting%20in%20longer%20search%20traces%20that%20chains%20additional%20asymmetries%3B%20and%20%283%29%0Acoupling%20task%20difficulty%20with%20training%20token%20budget%20during%20training%20via%20a%0Aspecifically-designed%20curriculum%20to%20structure%20in-context%20exploration.%20Our%0Arecipe%20e3%20produces%20the%20best%20known%201.7B%20model%20according%20to%20AIME%2725%20and%20HMMT%2725%0Ascores%2C%20and%20extrapolates%20to%202x%20the%20training%20token%20budget.%20Our%20e3-1.7B%20model%20not%0Aonly%20attains%20high%20pass%401%20scores%2C%20but%20also%20improves%20pass%40k%20over%20the%20base%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09026v2&entry.124074799=Read"},
{"title": "Teleoperated Driving: a New Challenge for 3D Object Detection in\n  Compressed Point Clouds", "author": "Filippo Bragato and Michael Neri and Paolo Testolina and Marco Giordani and Federica Battisti", "abstract": "  In recent years, the development of interconnected devices has expanded in\nmany fields, from infotainment to education and industrial applications. This\ntrend has been accelerated by the increased number of sensors and accessibility\nto powerful hardware and software. One area that significantly benefits from\nthese advancements is Teleoperated Driving (TD). In this scenario, a controller\ndrives safely a vehicle from remote leveraging sensors data generated onboard\nthe vehicle, and exchanged via Vehicle-to-Everything (V2X) communications. In\nthis work, we tackle the problem of detecting the presence of cars and\npedestrians from point cloud data to enable safe TD operations. More\nspecifically, we exploit the SELMA dataset, a multimodal, open-source,\nsynthetic dataset for autonomous driving, that we expanded by including the\nground-truth bounding boxes of 3D objects to support object detection. We\nanalyze the performance of state-of-the-art compression algorithms and object\ndetectors under several metrics, including compression efficiency,\n(de)compression and inference time, and detection accuracy. Moreover, we\nmeasure the impact of compression and detection on the V2X network in terms of\ndata rate and latency with respect to 3GPP requirements for TD applications.\n", "link": "http://arxiv.org/abs/2506.11804v1", "date": "2025-06-13", "relevancy": 2.1598, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5461}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5365}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teleoperated%20Driving%3A%20a%20New%20Challenge%20for%203D%20Object%20Detection%20in%0A%20%20Compressed%20Point%20Clouds&body=Title%3A%20Teleoperated%20Driving%3A%20a%20New%20Challenge%20for%203D%20Object%20Detection%20in%0A%20%20Compressed%20Point%20Clouds%0AAuthor%3A%20Filippo%20Bragato%20and%20Michael%20Neri%20and%20Paolo%20Testolina%20and%20Marco%20Giordani%20and%20Federica%20Battisti%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20development%20of%20interconnected%20devices%20has%20expanded%20in%0Amany%20fields%2C%20from%20infotainment%20to%20education%20and%20industrial%20applications.%20This%0Atrend%20has%20been%20accelerated%20by%20the%20increased%20number%20of%20sensors%20and%20accessibility%0Ato%20powerful%20hardware%20and%20software.%20One%20area%20that%20significantly%20benefits%20from%0Athese%20advancements%20is%20Teleoperated%20Driving%20%28TD%29.%20In%20this%20scenario%2C%20a%20controller%0Adrives%20safely%20a%20vehicle%20from%20remote%20leveraging%20sensors%20data%20generated%20onboard%0Athe%20vehicle%2C%20and%20exchanged%20via%20Vehicle-to-Everything%20%28V2X%29%20communications.%20In%0Athis%20work%2C%20we%20tackle%20the%20problem%20of%20detecting%20the%20presence%20of%20cars%20and%0Apedestrians%20from%20point%20cloud%20data%20to%20enable%20safe%20TD%20operations.%20More%0Aspecifically%2C%20we%20exploit%20the%20SELMA%20dataset%2C%20a%20multimodal%2C%20open-source%2C%0Asynthetic%20dataset%20for%20autonomous%20driving%2C%20that%20we%20expanded%20by%20including%20the%0Aground-truth%20bounding%20boxes%20of%203D%20objects%20to%20support%20object%20detection.%20We%0Aanalyze%20the%20performance%20of%20state-of-the-art%20compression%20algorithms%20and%20object%0Adetectors%20under%20several%20metrics%2C%20including%20compression%20efficiency%2C%0A%28de%29compression%20and%20inference%20time%2C%20and%20detection%20accuracy.%20Moreover%2C%20we%0Ameasure%20the%20impact%20of%20compression%20and%20detection%20on%20the%20V2X%20network%20in%20terms%20of%0Adata%20rate%20and%20latency%20with%20respect%20to%203GPP%20requirements%20for%20TD%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeleoperated%2520Driving%253A%2520a%2520New%2520Challenge%2520for%25203D%2520Object%2520Detection%2520in%250A%2520%2520Compressed%2520Point%2520Clouds%26entry.906535625%3DFilippo%2520Bragato%2520and%2520Michael%2520Neri%2520and%2520Paolo%2520Testolina%2520and%2520Marco%2520Giordani%2520and%2520Federica%2520Battisti%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520development%2520of%2520interconnected%2520devices%2520has%2520expanded%2520in%250Amany%2520fields%252C%2520from%2520infotainment%2520to%2520education%2520and%2520industrial%2520applications.%2520This%250Atrend%2520has%2520been%2520accelerated%2520by%2520the%2520increased%2520number%2520of%2520sensors%2520and%2520accessibility%250Ato%2520powerful%2520hardware%2520and%2520software.%2520One%2520area%2520that%2520significantly%2520benefits%2520from%250Athese%2520advancements%2520is%2520Teleoperated%2520Driving%2520%2528TD%2529.%2520In%2520this%2520scenario%252C%2520a%2520controller%250Adrives%2520safely%2520a%2520vehicle%2520from%2520remote%2520leveraging%2520sensors%2520data%2520generated%2520onboard%250Athe%2520vehicle%252C%2520and%2520exchanged%2520via%2520Vehicle-to-Everything%2520%2528V2X%2529%2520communications.%2520In%250Athis%2520work%252C%2520we%2520tackle%2520the%2520problem%2520of%2520detecting%2520the%2520presence%2520of%2520cars%2520and%250Apedestrians%2520from%2520point%2520cloud%2520data%2520to%2520enable%2520safe%2520TD%2520operations.%2520More%250Aspecifically%252C%2520we%2520exploit%2520the%2520SELMA%2520dataset%252C%2520a%2520multimodal%252C%2520open-source%252C%250Asynthetic%2520dataset%2520for%2520autonomous%2520driving%252C%2520that%2520we%2520expanded%2520by%2520including%2520the%250Aground-truth%2520bounding%2520boxes%2520of%25203D%2520objects%2520to%2520support%2520object%2520detection.%2520We%250Aanalyze%2520the%2520performance%2520of%2520state-of-the-art%2520compression%2520algorithms%2520and%2520object%250Adetectors%2520under%2520several%2520metrics%252C%2520including%2520compression%2520efficiency%252C%250A%2528de%2529compression%2520and%2520inference%2520time%252C%2520and%2520detection%2520accuracy.%2520Moreover%252C%2520we%250Ameasure%2520the%2520impact%2520of%2520compression%2520and%2520detection%2520on%2520the%2520V2X%2520network%2520in%2520terms%2520of%250Adata%2520rate%2520and%2520latency%2520with%2520respect%2520to%25203GPP%2520requirements%2520for%2520TD%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teleoperated%20Driving%3A%20a%20New%20Challenge%20for%203D%20Object%20Detection%20in%0A%20%20Compressed%20Point%20Clouds&entry.906535625=Filippo%20Bragato%20and%20Michael%20Neri%20and%20Paolo%20Testolina%20and%20Marco%20Giordani%20and%20Federica%20Battisti&entry.1292438233=%20%20In%20recent%20years%2C%20the%20development%20of%20interconnected%20devices%20has%20expanded%20in%0Amany%20fields%2C%20from%20infotainment%20to%20education%20and%20industrial%20applications.%20This%0Atrend%20has%20been%20accelerated%20by%20the%20increased%20number%20of%20sensors%20and%20accessibility%0Ato%20powerful%20hardware%20and%20software.%20One%20area%20that%20significantly%20benefits%20from%0Athese%20advancements%20is%20Teleoperated%20Driving%20%28TD%29.%20In%20this%20scenario%2C%20a%20controller%0Adrives%20safely%20a%20vehicle%20from%20remote%20leveraging%20sensors%20data%20generated%20onboard%0Athe%20vehicle%2C%20and%20exchanged%20via%20Vehicle-to-Everything%20%28V2X%29%20communications.%20In%0Athis%20work%2C%20we%20tackle%20the%20problem%20of%20detecting%20the%20presence%20of%20cars%20and%0Apedestrians%20from%20point%20cloud%20data%20to%20enable%20safe%20TD%20operations.%20More%0Aspecifically%2C%20we%20exploit%20the%20SELMA%20dataset%2C%20a%20multimodal%2C%20open-source%2C%0Asynthetic%20dataset%20for%20autonomous%20driving%2C%20that%20we%20expanded%20by%20including%20the%0Aground-truth%20bounding%20boxes%20of%203D%20objects%20to%20support%20object%20detection.%20We%0Aanalyze%20the%20performance%20of%20state-of-the-art%20compression%20algorithms%20and%20object%0Adetectors%20under%20several%20metrics%2C%20including%20compression%20efficiency%2C%0A%28de%29compression%20and%20inference%20time%2C%20and%20detection%20accuracy.%20Moreover%2C%20we%0Ameasure%20the%20impact%20of%20compression%20and%20detection%20on%20the%20V2X%20network%20in%20terms%20of%0Adata%20rate%20and%20latency%20with%20respect%20to%203GPP%20requirements%20for%20TD%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11804v1&entry.124074799=Read"},
{"title": "Autonomous Robotic Radio Source Localization via a Novel Gaussian\n  Mixture Filtering Approach", "author": "Sukkeun Kim and Sangwoo Moon and Ivan Petrunin and Hyo-Sang Shin and Shehryar Khattak", "abstract": "  This study proposes a new Gaussian Mixture Filter (GMF) to improve the\nestimation performance for the autonomous robotic radio signal source search\nand localization problem in unknown environments. The proposed filter is first\ntested with a benchmark numerical problem to validate the performance with\nother state-of-the-practice approaches such as Particle Filter (PF) and\nParticle Gaussian Mixture (PGM) filters. Then the proposed approach is tested\nand compared against PF and PGM filters in real-world robotic field experiments\nto validate its impact for real-world applications. The considered real-world\nscenarios have partial observability with the range-only measurement and\nuncertainty with the measurement model. The results show that the proposed\nfilter can handle this partial observability effectively whilst showing\nimproved performance compared to PF, reducing the computation requirements\nwhile demonstrating improved robustness over compared techniques.\n", "link": "http://arxiv.org/abs/2503.10349v3", "date": "2025-06-13", "relevancy": 2.1585, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5505}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5344}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Robotic%20Radio%20Source%20Localization%20via%20a%20Novel%20Gaussian%0A%20%20Mixture%20Filtering%20Approach&body=Title%3A%20Autonomous%20Robotic%20Radio%20Source%20Localization%20via%20a%20Novel%20Gaussian%0A%20%20Mixture%20Filtering%20Approach%0AAuthor%3A%20Sukkeun%20Kim%20and%20Sangwoo%20Moon%20and%20Ivan%20Petrunin%20and%20Hyo-Sang%20Shin%20and%20Shehryar%20Khattak%0AAbstract%3A%20%20%20This%20study%20proposes%20a%20new%20Gaussian%20Mixture%20Filter%20%28GMF%29%20to%20improve%20the%0Aestimation%20performance%20for%20the%20autonomous%20robotic%20radio%20signal%20source%20search%0Aand%20localization%20problem%20in%20unknown%20environments.%20The%20proposed%20filter%20is%20first%0Atested%20with%20a%20benchmark%20numerical%20problem%20to%20validate%20the%20performance%20with%0Aother%20state-of-the-practice%20approaches%20such%20as%20Particle%20Filter%20%28PF%29%20and%0AParticle%20Gaussian%20Mixture%20%28PGM%29%20filters.%20Then%20the%20proposed%20approach%20is%20tested%0Aand%20compared%20against%20PF%20and%20PGM%20filters%20in%20real-world%20robotic%20field%20experiments%0Ato%20validate%20its%20impact%20for%20real-world%20applications.%20The%20considered%20real-world%0Ascenarios%20have%20partial%20observability%20with%20the%20range-only%20measurement%20and%0Auncertainty%20with%20the%20measurement%20model.%20The%20results%20show%20that%20the%20proposed%0Afilter%20can%20handle%20this%20partial%20observability%20effectively%20whilst%20showing%0Aimproved%20performance%20compared%20to%20PF%2C%20reducing%20the%20computation%20requirements%0Awhile%20demonstrating%20improved%20robustness%20over%20compared%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10349v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Robotic%2520Radio%2520Source%2520Localization%2520via%2520a%2520Novel%2520Gaussian%250A%2520%2520Mixture%2520Filtering%2520Approach%26entry.906535625%3DSukkeun%2520Kim%2520and%2520Sangwoo%2520Moon%2520and%2520Ivan%2520Petrunin%2520and%2520Hyo-Sang%2520Shin%2520and%2520Shehryar%2520Khattak%26entry.1292438233%3D%2520%2520This%2520study%2520proposes%2520a%2520new%2520Gaussian%2520Mixture%2520Filter%2520%2528GMF%2529%2520to%2520improve%2520the%250Aestimation%2520performance%2520for%2520the%2520autonomous%2520robotic%2520radio%2520signal%2520source%2520search%250Aand%2520localization%2520problem%2520in%2520unknown%2520environments.%2520The%2520proposed%2520filter%2520is%2520first%250Atested%2520with%2520a%2520benchmark%2520numerical%2520problem%2520to%2520validate%2520the%2520performance%2520with%250Aother%2520state-of-the-practice%2520approaches%2520such%2520as%2520Particle%2520Filter%2520%2528PF%2529%2520and%250AParticle%2520Gaussian%2520Mixture%2520%2528PGM%2529%2520filters.%2520Then%2520the%2520proposed%2520approach%2520is%2520tested%250Aand%2520compared%2520against%2520PF%2520and%2520PGM%2520filters%2520in%2520real-world%2520robotic%2520field%2520experiments%250Ato%2520validate%2520its%2520impact%2520for%2520real-world%2520applications.%2520The%2520considered%2520real-world%250Ascenarios%2520have%2520partial%2520observability%2520with%2520the%2520range-only%2520measurement%2520and%250Auncertainty%2520with%2520the%2520measurement%2520model.%2520The%2520results%2520show%2520that%2520the%2520proposed%250Afilter%2520can%2520handle%2520this%2520partial%2520observability%2520effectively%2520whilst%2520showing%250Aimproved%2520performance%2520compared%2520to%2520PF%252C%2520reducing%2520the%2520computation%2520requirements%250Awhile%2520demonstrating%2520improved%2520robustness%2520over%2520compared%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10349v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Robotic%20Radio%20Source%20Localization%20via%20a%20Novel%20Gaussian%0A%20%20Mixture%20Filtering%20Approach&entry.906535625=Sukkeun%20Kim%20and%20Sangwoo%20Moon%20and%20Ivan%20Petrunin%20and%20Hyo-Sang%20Shin%20and%20Shehryar%20Khattak&entry.1292438233=%20%20This%20study%20proposes%20a%20new%20Gaussian%20Mixture%20Filter%20%28GMF%29%20to%20improve%20the%0Aestimation%20performance%20for%20the%20autonomous%20robotic%20radio%20signal%20source%20search%0Aand%20localization%20problem%20in%20unknown%20environments.%20The%20proposed%20filter%20is%20first%0Atested%20with%20a%20benchmark%20numerical%20problem%20to%20validate%20the%20performance%20with%0Aother%20state-of-the-practice%20approaches%20such%20as%20Particle%20Filter%20%28PF%29%20and%0AParticle%20Gaussian%20Mixture%20%28PGM%29%20filters.%20Then%20the%20proposed%20approach%20is%20tested%0Aand%20compared%20against%20PF%20and%20PGM%20filters%20in%20real-world%20robotic%20field%20experiments%0Ato%20validate%20its%20impact%20for%20real-world%20applications.%20The%20considered%20real-world%0Ascenarios%20have%20partial%20observability%20with%20the%20range-only%20measurement%20and%0Auncertainty%20with%20the%20measurement%20model.%20The%20results%20show%20that%20the%20proposed%0Afilter%20can%20handle%20this%20partial%20observability%20effectively%20whilst%20showing%0Aimproved%20performance%20compared%20to%20PF%2C%20reducing%20the%20computation%20requirements%0Awhile%20demonstrating%20improved%20robustness%20over%20compared%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10349v3&entry.124074799=Read"},
{"title": "Real-time Seafloor Segmentation and Mapping", "author": "Michele Grimaldi and Nouf Alkaabi and Francesco Ruscio and Sebastian Realpe Rua and Rafael Garcia and Nuno Gracias", "abstract": "  Posidonia oceanica meadows are a species of seagrass highly dependent on\nrocks for their survival and conservation. In recent years, there has been a\nconcerning global decline in this species, emphasizing the critical need for\nefficient monitoring and assessment tools. While deep learning-based semantic\nsegmentation and visual automated monitoring systems have shown promise in a\nvariety of applications, their performance in underwater environments remains\nchallenging due to complex water conditions and limited datasets. This paper\nintroduces a framework that combines machine learning and computer vision\ntechniques to enable an autonomous underwater vehicle (AUV) to inspect the\nboundaries of Posidonia oceanica meadows autonomously. The framework\nincorporates an image segmentation module using an existing Mask R-CNN model\nand a strategy for Posidonia oceanica meadow boundary tracking. Furthermore, a\nnew class dedicated to rocks is introduced to enhance the existing model,\naiming to contribute to a comprehensive monitoring approach and provide a\ndeeper understanding of the intricate interactions between the meadow and its\nsurrounding environment. The image segmentation model is validated using real\nunderwater images, while the overall inspection framework is evaluated in a\nrealistic simulation environment, replicating actual monitoring scenarios with\nreal underwater images. The results demonstrate that the proposed framework\nenables the AUV to autonomously accomplish the main tasks of underwater\ninspection and segmentation of rocks. Consequently, this work holds significant\npotential for the conservation and protection of marine environments, providing\nvaluable insights into the status of Posidonia oceanica meadows and supporting\ntargeted preservation efforts\n", "link": "http://arxiv.org/abs/2504.10750v2", "date": "2025-06-13", "relevancy": 2.155, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5548}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Seafloor%20Segmentation%20and%20Mapping&body=Title%3A%20Real-time%20Seafloor%20Segmentation%20and%20Mapping%0AAuthor%3A%20Michele%20Grimaldi%20and%20Nouf%20Alkaabi%20and%20Francesco%20Ruscio%20and%20Sebastian%20Realpe%20Rua%20and%20Rafael%20Garcia%20and%20Nuno%20Gracias%0AAbstract%3A%20%20%20Posidonia%20oceanica%20meadows%20are%20a%20species%20of%20seagrass%20highly%20dependent%20on%0Arocks%20for%20their%20survival%20and%20conservation.%20In%20recent%20years%2C%20there%20has%20been%20a%0Aconcerning%20global%20decline%20in%20this%20species%2C%20emphasizing%20the%20critical%20need%20for%0Aefficient%20monitoring%20and%20assessment%20tools.%20While%20deep%20learning-based%20semantic%0Asegmentation%20and%20visual%20automated%20monitoring%20systems%20have%20shown%20promise%20in%20a%0Avariety%20of%20applications%2C%20their%20performance%20in%20underwater%20environments%20remains%0Achallenging%20due%20to%20complex%20water%20conditions%20and%20limited%20datasets.%20This%20paper%0Aintroduces%20a%20framework%20that%20combines%20machine%20learning%20and%20computer%20vision%0Atechniques%20to%20enable%20an%20autonomous%20underwater%20vehicle%20%28AUV%29%20to%20inspect%20the%0Aboundaries%20of%20Posidonia%20oceanica%20meadows%20autonomously.%20The%20framework%0Aincorporates%20an%20image%20segmentation%20module%20using%20an%20existing%20Mask%20R-CNN%20model%0Aand%20a%20strategy%20for%20Posidonia%20oceanica%20meadow%20boundary%20tracking.%20Furthermore%2C%20a%0Anew%20class%20dedicated%20to%20rocks%20is%20introduced%20to%20enhance%20the%20existing%20model%2C%0Aaiming%20to%20contribute%20to%20a%20comprehensive%20monitoring%20approach%20and%20provide%20a%0Adeeper%20understanding%20of%20the%20intricate%20interactions%20between%20the%20meadow%20and%20its%0Asurrounding%20environment.%20The%20image%20segmentation%20model%20is%20validated%20using%20real%0Aunderwater%20images%2C%20while%20the%20overall%20inspection%20framework%20is%20evaluated%20in%20a%0Arealistic%20simulation%20environment%2C%20replicating%20actual%20monitoring%20scenarios%20with%0Areal%20underwater%20images.%20The%20results%20demonstrate%20that%20the%20proposed%20framework%0Aenables%20the%20AUV%20to%20autonomously%20accomplish%20the%20main%20tasks%20of%20underwater%0Ainspection%20and%20segmentation%20of%20rocks.%20Consequently%2C%20this%20work%20holds%20significant%0Apotential%20for%20the%20conservation%20and%20protection%20of%20marine%20environments%2C%20providing%0Avaluable%20insights%20into%20the%20status%20of%20Posidonia%20oceanica%20meadows%20and%20supporting%0Atargeted%20preservation%20efforts%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10750v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Seafloor%2520Segmentation%2520and%2520Mapping%26entry.906535625%3DMichele%2520Grimaldi%2520and%2520Nouf%2520Alkaabi%2520and%2520Francesco%2520Ruscio%2520and%2520Sebastian%2520Realpe%2520Rua%2520and%2520Rafael%2520Garcia%2520and%2520Nuno%2520Gracias%26entry.1292438233%3D%2520%2520Posidonia%2520oceanica%2520meadows%2520are%2520a%2520species%2520of%2520seagrass%2520highly%2520dependent%2520on%250Arocks%2520for%2520their%2520survival%2520and%2520conservation.%2520In%2520recent%2520years%252C%2520there%2520has%2520been%2520a%250Aconcerning%2520global%2520decline%2520in%2520this%2520species%252C%2520emphasizing%2520the%2520critical%2520need%2520for%250Aefficient%2520monitoring%2520and%2520assessment%2520tools.%2520While%2520deep%2520learning-based%2520semantic%250Asegmentation%2520and%2520visual%2520automated%2520monitoring%2520systems%2520have%2520shown%2520promise%2520in%2520a%250Avariety%2520of%2520applications%252C%2520their%2520performance%2520in%2520underwater%2520environments%2520remains%250Achallenging%2520due%2520to%2520complex%2520water%2520conditions%2520and%2520limited%2520datasets.%2520This%2520paper%250Aintroduces%2520a%2520framework%2520that%2520combines%2520machine%2520learning%2520and%2520computer%2520vision%250Atechniques%2520to%2520enable%2520an%2520autonomous%2520underwater%2520vehicle%2520%2528AUV%2529%2520to%2520inspect%2520the%250Aboundaries%2520of%2520Posidonia%2520oceanica%2520meadows%2520autonomously.%2520The%2520framework%250Aincorporates%2520an%2520image%2520segmentation%2520module%2520using%2520an%2520existing%2520Mask%2520R-CNN%2520model%250Aand%2520a%2520strategy%2520for%2520Posidonia%2520oceanica%2520meadow%2520boundary%2520tracking.%2520Furthermore%252C%2520a%250Anew%2520class%2520dedicated%2520to%2520rocks%2520is%2520introduced%2520to%2520enhance%2520the%2520existing%2520model%252C%250Aaiming%2520to%2520contribute%2520to%2520a%2520comprehensive%2520monitoring%2520approach%2520and%2520provide%2520a%250Adeeper%2520understanding%2520of%2520the%2520intricate%2520interactions%2520between%2520the%2520meadow%2520and%2520its%250Asurrounding%2520environment.%2520The%2520image%2520segmentation%2520model%2520is%2520validated%2520using%2520real%250Aunderwater%2520images%252C%2520while%2520the%2520overall%2520inspection%2520framework%2520is%2520evaluated%2520in%2520a%250Arealistic%2520simulation%2520environment%252C%2520replicating%2520actual%2520monitoring%2520scenarios%2520with%250Areal%2520underwater%2520images.%2520The%2520results%2520demonstrate%2520that%2520the%2520proposed%2520framework%250Aenables%2520the%2520AUV%2520to%2520autonomously%2520accomplish%2520the%2520main%2520tasks%2520of%2520underwater%250Ainspection%2520and%2520segmentation%2520of%2520rocks.%2520Consequently%252C%2520this%2520work%2520holds%2520significant%250Apotential%2520for%2520the%2520conservation%2520and%2520protection%2520of%2520marine%2520environments%252C%2520providing%250Avaluable%2520insights%2520into%2520the%2520status%2520of%2520Posidonia%2520oceanica%2520meadows%2520and%2520supporting%250Atargeted%2520preservation%2520efforts%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10750v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Seafloor%20Segmentation%20and%20Mapping&entry.906535625=Michele%20Grimaldi%20and%20Nouf%20Alkaabi%20and%20Francesco%20Ruscio%20and%20Sebastian%20Realpe%20Rua%20and%20Rafael%20Garcia%20and%20Nuno%20Gracias&entry.1292438233=%20%20Posidonia%20oceanica%20meadows%20are%20a%20species%20of%20seagrass%20highly%20dependent%20on%0Arocks%20for%20their%20survival%20and%20conservation.%20In%20recent%20years%2C%20there%20has%20been%20a%0Aconcerning%20global%20decline%20in%20this%20species%2C%20emphasizing%20the%20critical%20need%20for%0Aefficient%20monitoring%20and%20assessment%20tools.%20While%20deep%20learning-based%20semantic%0Asegmentation%20and%20visual%20automated%20monitoring%20systems%20have%20shown%20promise%20in%20a%0Avariety%20of%20applications%2C%20their%20performance%20in%20underwater%20environments%20remains%0Achallenging%20due%20to%20complex%20water%20conditions%20and%20limited%20datasets.%20This%20paper%0Aintroduces%20a%20framework%20that%20combines%20machine%20learning%20and%20computer%20vision%0Atechniques%20to%20enable%20an%20autonomous%20underwater%20vehicle%20%28AUV%29%20to%20inspect%20the%0Aboundaries%20of%20Posidonia%20oceanica%20meadows%20autonomously.%20The%20framework%0Aincorporates%20an%20image%20segmentation%20module%20using%20an%20existing%20Mask%20R-CNN%20model%0Aand%20a%20strategy%20for%20Posidonia%20oceanica%20meadow%20boundary%20tracking.%20Furthermore%2C%20a%0Anew%20class%20dedicated%20to%20rocks%20is%20introduced%20to%20enhance%20the%20existing%20model%2C%0Aaiming%20to%20contribute%20to%20a%20comprehensive%20monitoring%20approach%20and%20provide%20a%0Adeeper%20understanding%20of%20the%20intricate%20interactions%20between%20the%20meadow%20and%20its%0Asurrounding%20environment.%20The%20image%20segmentation%20model%20is%20validated%20using%20real%0Aunderwater%20images%2C%20while%20the%20overall%20inspection%20framework%20is%20evaluated%20in%20a%0Arealistic%20simulation%20environment%2C%20replicating%20actual%20monitoring%20scenarios%20with%0Areal%20underwater%20images.%20The%20results%20demonstrate%20that%20the%20proposed%20framework%0Aenables%20the%20AUV%20to%20autonomously%20accomplish%20the%20main%20tasks%20of%20underwater%0Ainspection%20and%20segmentation%20of%20rocks.%20Consequently%2C%20this%20work%20holds%20significant%0Apotential%20for%20the%20conservation%20and%20protection%20of%20marine%20environments%2C%20providing%0Avaluable%20insights%20into%20the%20status%20of%20Posidonia%20oceanica%20meadows%20and%20supporting%0Atargeted%20preservation%20efforts%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10750v2&entry.124074799=Read"},
{"title": "A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead\n  Adversarial Example Detection", "author": "Sanggeon Yun and Ryozo Masukawa and Hyunwoo Oh and Nathaniel D. Bastian and Mohsen Imani", "abstract": "  Deep neural networks (DNNs) are highly susceptible to adversarial\nexamples--subtle, imperceptible perturbations that can lead to incorrect\npredictions. While detection-based defenses offer a practical alternative to\nadversarial training, many existing methods depend on external models, complex\narchitectures, heavy augmentations, or adversarial data, limiting their\nefficiency and generalizability. We introduce a lightweight, plug-in detection\nframework that leverages internal layer-wise inconsistencies within the target\nmodel itself, requiring only benign data for calibration. Our approach is\ngrounded in the A Few Large Shifts Assumption, which posits that adversarial\nperturbations typically induce large representation shifts in a small subset of\nlayers. Building on this, we propose two complementary strategies--Recovery\nTesting (RT) and Logit-layer Testing (LT)--to expose internal disruptions\ncaused by adversaries. Evaluated on CIFAR-10, CIFAR-100, and ImageNet under\nboth standard and adaptive threat models, our method achieves state-of-the-art\ndetection performance with negligible computational overhead and no compromise\nto clean accuracy. The code is available here:\nhttps://github.com/c0510gy/AFLS-AED.\n", "link": "http://arxiv.org/abs/2505.12586v4", "date": "2025-06-13", "relevancy": 2.1539, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5621}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.526}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Few%20Large%20Shifts%3A%20Layer-Inconsistency%20Based%20Minimal%20Overhead%0A%20%20Adversarial%20Example%20Detection&body=Title%3A%20A%20Few%20Large%20Shifts%3A%20Layer-Inconsistency%20Based%20Minimal%20Overhead%0A%20%20Adversarial%20Example%20Detection%0AAuthor%3A%20Sanggeon%20Yun%20and%20Ryozo%20Masukawa%20and%20Hyunwoo%20Oh%20and%20Nathaniel%20D.%20Bastian%20and%20Mohsen%20Imani%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20highly%20susceptible%20to%20adversarial%0Aexamples--subtle%2C%20imperceptible%20perturbations%20that%20can%20lead%20to%20incorrect%0Apredictions.%20While%20detection-based%20defenses%20offer%20a%20practical%20alternative%20to%0Aadversarial%20training%2C%20many%20existing%20methods%20depend%20on%20external%20models%2C%20complex%0Aarchitectures%2C%20heavy%20augmentations%2C%20or%20adversarial%20data%2C%20limiting%20their%0Aefficiency%20and%20generalizability.%20We%20introduce%20a%20lightweight%2C%20plug-in%20detection%0Aframework%20that%20leverages%20internal%20layer-wise%20inconsistencies%20within%20the%20target%0Amodel%20itself%2C%20requiring%20only%20benign%20data%20for%20calibration.%20Our%20approach%20is%0Agrounded%20in%20the%20A%20Few%20Large%20Shifts%20Assumption%2C%20which%20posits%20that%20adversarial%0Aperturbations%20typically%20induce%20large%20representation%20shifts%20in%20a%20small%20subset%20of%0Alayers.%20Building%20on%20this%2C%20we%20propose%20two%20complementary%20strategies--Recovery%0ATesting%20%28RT%29%20and%20Logit-layer%20Testing%20%28LT%29--to%20expose%20internal%20disruptions%0Acaused%20by%20adversaries.%20Evaluated%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet%20under%0Aboth%20standard%20and%20adaptive%20threat%20models%2C%20our%20method%20achieves%20state-of-the-art%0Adetection%20performance%20with%20negligible%20computational%20overhead%20and%20no%20compromise%0Ato%20clean%20accuracy.%20The%20code%20is%20available%20here%3A%0Ahttps%3A//github.com/c0510gy/AFLS-AED.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12586v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Few%2520Large%2520Shifts%253A%2520Layer-Inconsistency%2520Based%2520Minimal%2520Overhead%250A%2520%2520Adversarial%2520Example%2520Detection%26entry.906535625%3DSanggeon%2520Yun%2520and%2520Ryozo%2520Masukawa%2520and%2520Hyunwoo%2520Oh%2520and%2520Nathaniel%2520D.%2520Bastian%2520and%2520Mohsen%2520Imani%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520highly%2520susceptible%2520to%2520adversarial%250Aexamples--subtle%252C%2520imperceptible%2520perturbations%2520that%2520can%2520lead%2520to%2520incorrect%250Apredictions.%2520While%2520detection-based%2520defenses%2520offer%2520a%2520practical%2520alternative%2520to%250Aadversarial%2520training%252C%2520many%2520existing%2520methods%2520depend%2520on%2520external%2520models%252C%2520complex%250Aarchitectures%252C%2520heavy%2520augmentations%252C%2520or%2520adversarial%2520data%252C%2520limiting%2520their%250Aefficiency%2520and%2520generalizability.%2520We%2520introduce%2520a%2520lightweight%252C%2520plug-in%2520detection%250Aframework%2520that%2520leverages%2520internal%2520layer-wise%2520inconsistencies%2520within%2520the%2520target%250Amodel%2520itself%252C%2520requiring%2520only%2520benign%2520data%2520for%2520calibration.%2520Our%2520approach%2520is%250Agrounded%2520in%2520the%2520A%2520Few%2520Large%2520Shifts%2520Assumption%252C%2520which%2520posits%2520that%2520adversarial%250Aperturbations%2520typically%2520induce%2520large%2520representation%2520shifts%2520in%2520a%2520small%2520subset%2520of%250Alayers.%2520Building%2520on%2520this%252C%2520we%2520propose%2520two%2520complementary%2520strategies--Recovery%250ATesting%2520%2528RT%2529%2520and%2520Logit-layer%2520Testing%2520%2528LT%2529--to%2520expose%2520internal%2520disruptions%250Acaused%2520by%2520adversaries.%2520Evaluated%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520ImageNet%2520under%250Aboth%2520standard%2520and%2520adaptive%2520threat%2520models%252C%2520our%2520method%2520achieves%2520state-of-the-art%250Adetection%2520performance%2520with%2520negligible%2520computational%2520overhead%2520and%2520no%2520compromise%250Ato%2520clean%2520accuracy.%2520The%2520code%2520is%2520available%2520here%253A%250Ahttps%253A//github.com/c0510gy/AFLS-AED.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12586v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Few%20Large%20Shifts%3A%20Layer-Inconsistency%20Based%20Minimal%20Overhead%0A%20%20Adversarial%20Example%20Detection&entry.906535625=Sanggeon%20Yun%20and%20Ryozo%20Masukawa%20and%20Hyunwoo%20Oh%20and%20Nathaniel%20D.%20Bastian%20and%20Mohsen%20Imani&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20highly%20susceptible%20to%20adversarial%0Aexamples--subtle%2C%20imperceptible%20perturbations%20that%20can%20lead%20to%20incorrect%0Apredictions.%20While%20detection-based%20defenses%20offer%20a%20practical%20alternative%20to%0Aadversarial%20training%2C%20many%20existing%20methods%20depend%20on%20external%20models%2C%20complex%0Aarchitectures%2C%20heavy%20augmentations%2C%20or%20adversarial%20data%2C%20limiting%20their%0Aefficiency%20and%20generalizability.%20We%20introduce%20a%20lightweight%2C%20plug-in%20detection%0Aframework%20that%20leverages%20internal%20layer-wise%20inconsistencies%20within%20the%20target%0Amodel%20itself%2C%20requiring%20only%20benign%20data%20for%20calibration.%20Our%20approach%20is%0Agrounded%20in%20the%20A%20Few%20Large%20Shifts%20Assumption%2C%20which%20posits%20that%20adversarial%0Aperturbations%20typically%20induce%20large%20representation%20shifts%20in%20a%20small%20subset%20of%0Alayers.%20Building%20on%20this%2C%20we%20propose%20two%20complementary%20strategies--Recovery%0ATesting%20%28RT%29%20and%20Logit-layer%20Testing%20%28LT%29--to%20expose%20internal%20disruptions%0Acaused%20by%20adversaries.%20Evaluated%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet%20under%0Aboth%20standard%20and%20adaptive%20threat%20models%2C%20our%20method%20achieves%20state-of-the-art%0Adetection%20performance%20with%20negligible%20computational%20overhead%20and%20no%20compromise%0Ato%20clean%20accuracy.%20The%20code%20is%20available%20here%3A%0Ahttps%3A//github.com/c0510gy/AFLS-AED.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12586v4&entry.124074799=Read"},
{"title": "Foundation Models for Anomaly Detection: Vision and Challenges", "author": "Jing Ren and Tao Tang and Hong Jia and Ziqi Xu and Haytham Fayek and Xiaodong Li and Suyu Ma and Xiwei Xu and Feng Xia", "abstract": "  As data continues to grow in volume and complexity across domains such as\nfinance, manufacturing, and healthcare, effective anomaly detection is\nessential for identifying irregular patterns that may signal critical issues.\nRecently, foundation models (FMs) have emerged as a powerful tool for advancing\nanomaly detection. They have demonstrated unprecedented capabilities in\nenhancing anomaly identification, generating detailed data descriptions, and\nproviding visual explanations. This survey presents the first comprehensive\nreview of recent advancements in FM-based anomaly detection. We propose a novel\ntaxonomy that classifies FMs into three categories based on their roles in\nanomaly detection tasks, i.e., as encoders, detectors, or interpreters. We\nprovide a systematic analysis of state-of-the-art methods and discuss key\nchallenges in leveraging FMs for improved anomaly detection. We also outline\nfuture research directions in this rapidly evolving field.\n", "link": "http://arxiv.org/abs/2502.06911v2", "date": "2025-06-13", "relevancy": 2.1536, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5503}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20for%20Anomaly%20Detection%3A%20Vision%20and%20Challenges&body=Title%3A%20Foundation%20Models%20for%20Anomaly%20Detection%3A%20Vision%20and%20Challenges%0AAuthor%3A%20Jing%20Ren%20and%20Tao%20Tang%20and%20Hong%20Jia%20and%20Ziqi%20Xu%20and%20Haytham%20Fayek%20and%20Xiaodong%20Li%20and%20Suyu%20Ma%20and%20Xiwei%20Xu%20and%20Feng%20Xia%0AAbstract%3A%20%20%20As%20data%20continues%20to%20grow%20in%20volume%20and%20complexity%20across%20domains%20such%20as%0Afinance%2C%20manufacturing%2C%20and%20healthcare%2C%20effective%20anomaly%20detection%20is%0Aessential%20for%20identifying%20irregular%20patterns%20that%20may%20signal%20critical%20issues.%0ARecently%2C%20foundation%20models%20%28FMs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20advancing%0Aanomaly%20detection.%20They%20have%20demonstrated%20unprecedented%20capabilities%20in%0Aenhancing%20anomaly%20identification%2C%20generating%20detailed%20data%20descriptions%2C%20and%0Aproviding%20visual%20explanations.%20This%20survey%20presents%20the%20first%20comprehensive%0Areview%20of%20recent%20advancements%20in%20FM-based%20anomaly%20detection.%20We%20propose%20a%20novel%0Ataxonomy%20that%20classifies%20FMs%20into%20three%20categories%20based%20on%20their%20roles%20in%0Aanomaly%20detection%20tasks%2C%20i.e.%2C%20as%20encoders%2C%20detectors%2C%20or%20interpreters.%20We%0Aprovide%20a%20systematic%20analysis%20of%20state-of-the-art%20methods%20and%20discuss%20key%0Achallenges%20in%20leveraging%20FMs%20for%20improved%20anomaly%20detection.%20We%20also%20outline%0Afuture%20research%20directions%20in%20this%20rapidly%20evolving%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06911v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520for%2520Anomaly%2520Detection%253A%2520Vision%2520and%2520Challenges%26entry.906535625%3DJing%2520Ren%2520and%2520Tao%2520Tang%2520and%2520Hong%2520Jia%2520and%2520Ziqi%2520Xu%2520and%2520Haytham%2520Fayek%2520and%2520Xiaodong%2520Li%2520and%2520Suyu%2520Ma%2520and%2520Xiwei%2520Xu%2520and%2520Feng%2520Xia%26entry.1292438233%3D%2520%2520As%2520data%2520continues%2520to%2520grow%2520in%2520volume%2520and%2520complexity%2520across%2520domains%2520such%2520as%250Afinance%252C%2520manufacturing%252C%2520and%2520healthcare%252C%2520effective%2520anomaly%2520detection%2520is%250Aessential%2520for%2520identifying%2520irregular%2520patterns%2520that%2520may%2520signal%2520critical%2520issues.%250ARecently%252C%2520foundation%2520models%2520%2528FMs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520advancing%250Aanomaly%2520detection.%2520They%2520have%2520demonstrated%2520unprecedented%2520capabilities%2520in%250Aenhancing%2520anomaly%2520identification%252C%2520generating%2520detailed%2520data%2520descriptions%252C%2520and%250Aproviding%2520visual%2520explanations.%2520This%2520survey%2520presents%2520the%2520first%2520comprehensive%250Areview%2520of%2520recent%2520advancements%2520in%2520FM-based%2520anomaly%2520detection.%2520We%2520propose%2520a%2520novel%250Ataxonomy%2520that%2520classifies%2520FMs%2520into%2520three%2520categories%2520based%2520on%2520their%2520roles%2520in%250Aanomaly%2520detection%2520tasks%252C%2520i.e.%252C%2520as%2520encoders%252C%2520detectors%252C%2520or%2520interpreters.%2520We%250Aprovide%2520a%2520systematic%2520analysis%2520of%2520state-of-the-art%2520methods%2520and%2520discuss%2520key%250Achallenges%2520in%2520leveraging%2520FMs%2520for%2520improved%2520anomaly%2520detection.%2520We%2520also%2520outline%250Afuture%2520research%2520directions%2520in%2520this%2520rapidly%2520evolving%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06911v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20for%20Anomaly%20Detection%3A%20Vision%20and%20Challenges&entry.906535625=Jing%20Ren%20and%20Tao%20Tang%20and%20Hong%20Jia%20and%20Ziqi%20Xu%20and%20Haytham%20Fayek%20and%20Xiaodong%20Li%20and%20Suyu%20Ma%20and%20Xiwei%20Xu%20and%20Feng%20Xia&entry.1292438233=%20%20As%20data%20continues%20to%20grow%20in%20volume%20and%20complexity%20across%20domains%20such%20as%0Afinance%2C%20manufacturing%2C%20and%20healthcare%2C%20effective%20anomaly%20detection%20is%0Aessential%20for%20identifying%20irregular%20patterns%20that%20may%20signal%20critical%20issues.%0ARecently%2C%20foundation%20models%20%28FMs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20advancing%0Aanomaly%20detection.%20They%20have%20demonstrated%20unprecedented%20capabilities%20in%0Aenhancing%20anomaly%20identification%2C%20generating%20detailed%20data%20descriptions%2C%20and%0Aproviding%20visual%20explanations.%20This%20survey%20presents%20the%20first%20comprehensive%0Areview%20of%20recent%20advancements%20in%20FM-based%20anomaly%20detection.%20We%20propose%20a%20novel%0Ataxonomy%20that%20classifies%20FMs%20into%20three%20categories%20based%20on%20their%20roles%20in%0Aanomaly%20detection%20tasks%2C%20i.e.%2C%20as%20encoders%2C%20detectors%2C%20or%20interpreters.%20We%0Aprovide%20a%20systematic%20analysis%20of%20state-of-the-art%20methods%20and%20discuss%20key%0Achallenges%20in%20leveraging%20FMs%20for%20improved%20anomaly%20detection.%20We%20also%20outline%0Afuture%20research%20directions%20in%20this%20rapidly%20evolving%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06911v2&entry.124074799=Read"},
{"title": "Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with\n  Physics-informed Fine-tuning", "author": "Peimian Du and Jiabin Liu and Xiaowei Jin and Wangmeng Zuo and Hui Li", "abstract": "  This research confronts the challenge of substantial physical equation\ndiscrepancies encountered in the generation of spatiotemporal physical fields\nthrough data-driven trained models. A spatiotemporal physical field generation\nmodel, named HMT-PF, is developed based on the hybrid Mamba-Transformer\narchitecture, incorporating unstructured grid information as input. A\nfine-tuning block, enhanced with physical information, is introduced to\neffectively reduce the physical equation discrepancies. The physical equation\nresiduals are computed through a point query mechanism for efficient gradient\nevaluation, then encoded into latent space for refinement. The fine-tuning\nprocess employs a self-supervised learning approach to achieve physical\nconsistency while maintaining essential field characteristics. Results show\nthat the hybrid Mamba-Transformer model achieves good performance in generating\nspatiotemporal fields, while the physics-informed fine-tuning mechanism further\nreduces significant physical errors effectively. A MSE-R evaluation method is\ndeveloped to assess the accuracy and realism of physical field generation.\n", "link": "http://arxiv.org/abs/2505.11578v4", "date": "2025-06-13", "relevancy": 2.1505, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5528}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5371}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatiotemporal%20Field%20Generation%20Based%20on%20Hybrid%20Mamba-Transformer%20with%0A%20%20Physics-informed%20Fine-tuning&body=Title%3A%20Spatiotemporal%20Field%20Generation%20Based%20on%20Hybrid%20Mamba-Transformer%20with%0A%20%20Physics-informed%20Fine-tuning%0AAuthor%3A%20Peimian%20Du%20and%20Jiabin%20Liu%20and%20Xiaowei%20Jin%20and%20Wangmeng%20Zuo%20and%20Hui%20Li%0AAbstract%3A%20%20%20This%20research%20confronts%20the%20challenge%20of%20substantial%20physical%20equation%0Adiscrepancies%20encountered%20in%20the%20generation%20of%20spatiotemporal%20physical%20fields%0Athrough%20data-driven%20trained%20models.%20A%20spatiotemporal%20physical%20field%20generation%0Amodel%2C%20named%20HMT-PF%2C%20is%20developed%20based%20on%20the%20hybrid%20Mamba-Transformer%0Aarchitecture%2C%20incorporating%20unstructured%20grid%20information%20as%20input.%20A%0Afine-tuning%20block%2C%20enhanced%20with%20physical%20information%2C%20is%20introduced%20to%0Aeffectively%20reduce%20the%20physical%20equation%20discrepancies.%20The%20physical%20equation%0Aresiduals%20are%20computed%20through%20a%20point%20query%20mechanism%20for%20efficient%20gradient%0Aevaluation%2C%20then%20encoded%20into%20latent%20space%20for%20refinement.%20The%20fine-tuning%0Aprocess%20employs%20a%20self-supervised%20learning%20approach%20to%20achieve%20physical%0Aconsistency%20while%20maintaining%20essential%20field%20characteristics.%20Results%20show%0Athat%20the%20hybrid%20Mamba-Transformer%20model%20achieves%20good%20performance%20in%20generating%0Aspatiotemporal%20fields%2C%20while%20the%20physics-informed%20fine-tuning%20mechanism%20further%0Areduces%20significant%20physical%20errors%20effectively.%20A%20MSE-R%20evaluation%20method%20is%0Adeveloped%20to%20assess%20the%20accuracy%20and%20realism%20of%20physical%20field%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11578v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatiotemporal%2520Field%2520Generation%2520Based%2520on%2520Hybrid%2520Mamba-Transformer%2520with%250A%2520%2520Physics-informed%2520Fine-tuning%26entry.906535625%3DPeimian%2520Du%2520and%2520Jiabin%2520Liu%2520and%2520Xiaowei%2520Jin%2520and%2520Wangmeng%2520Zuo%2520and%2520Hui%2520Li%26entry.1292438233%3D%2520%2520This%2520research%2520confronts%2520the%2520challenge%2520of%2520substantial%2520physical%2520equation%250Adiscrepancies%2520encountered%2520in%2520the%2520generation%2520of%2520spatiotemporal%2520physical%2520fields%250Athrough%2520data-driven%2520trained%2520models.%2520A%2520spatiotemporal%2520physical%2520field%2520generation%250Amodel%252C%2520named%2520HMT-PF%252C%2520is%2520developed%2520based%2520on%2520the%2520hybrid%2520Mamba-Transformer%250Aarchitecture%252C%2520incorporating%2520unstructured%2520grid%2520information%2520as%2520input.%2520A%250Afine-tuning%2520block%252C%2520enhanced%2520with%2520physical%2520information%252C%2520is%2520introduced%2520to%250Aeffectively%2520reduce%2520the%2520physical%2520equation%2520discrepancies.%2520The%2520physical%2520equation%250Aresiduals%2520are%2520computed%2520through%2520a%2520point%2520query%2520mechanism%2520for%2520efficient%2520gradient%250Aevaluation%252C%2520then%2520encoded%2520into%2520latent%2520space%2520for%2520refinement.%2520The%2520fine-tuning%250Aprocess%2520employs%2520a%2520self-supervised%2520learning%2520approach%2520to%2520achieve%2520physical%250Aconsistency%2520while%2520maintaining%2520essential%2520field%2520characteristics.%2520Results%2520show%250Athat%2520the%2520hybrid%2520Mamba-Transformer%2520model%2520achieves%2520good%2520performance%2520in%2520generating%250Aspatiotemporal%2520fields%252C%2520while%2520the%2520physics-informed%2520fine-tuning%2520mechanism%2520further%250Areduces%2520significant%2520physical%2520errors%2520effectively.%2520A%2520MSE-R%2520evaluation%2520method%2520is%250Adeveloped%2520to%2520assess%2520the%2520accuracy%2520and%2520realism%2520of%2520physical%2520field%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11578v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatiotemporal%20Field%20Generation%20Based%20on%20Hybrid%20Mamba-Transformer%20with%0A%20%20Physics-informed%20Fine-tuning&entry.906535625=Peimian%20Du%20and%20Jiabin%20Liu%20and%20Xiaowei%20Jin%20and%20Wangmeng%20Zuo%20and%20Hui%20Li&entry.1292438233=%20%20This%20research%20confronts%20the%20challenge%20of%20substantial%20physical%20equation%0Adiscrepancies%20encountered%20in%20the%20generation%20of%20spatiotemporal%20physical%20fields%0Athrough%20data-driven%20trained%20models.%20A%20spatiotemporal%20physical%20field%20generation%0Amodel%2C%20named%20HMT-PF%2C%20is%20developed%20based%20on%20the%20hybrid%20Mamba-Transformer%0Aarchitecture%2C%20incorporating%20unstructured%20grid%20information%20as%20input.%20A%0Afine-tuning%20block%2C%20enhanced%20with%20physical%20information%2C%20is%20introduced%20to%0Aeffectively%20reduce%20the%20physical%20equation%20discrepancies.%20The%20physical%20equation%0Aresiduals%20are%20computed%20through%20a%20point%20query%20mechanism%20for%20efficient%20gradient%0Aevaluation%2C%20then%20encoded%20into%20latent%20space%20for%20refinement.%20The%20fine-tuning%0Aprocess%20employs%20a%20self-supervised%20learning%20approach%20to%20achieve%20physical%0Aconsistency%20while%20maintaining%20essential%20field%20characteristics.%20Results%20show%0Athat%20the%20hybrid%20Mamba-Transformer%20model%20achieves%20good%20performance%20in%20generating%0Aspatiotemporal%20fields%2C%20while%20the%20physics-informed%20fine-tuning%20mechanism%20further%0Areduces%20significant%20physical%20errors%20effectively.%20A%20MSE-R%20evaluation%20method%20is%0Adeveloped%20to%20assess%20the%20accuracy%20and%20realism%20of%20physical%20field%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11578v4&entry.124074799=Read"},
{"title": "Scalable unsupervised feature selection via weight stability", "author": "Xudong Zhang and Renato Cordeiro de Amorim", "abstract": "  Unsupervised feature selection is critical for improving clustering\nperformance in high-dimensional data, where irrelevant features can obscure\nmeaningful structure. In this work, we introduce the Minkowski weighted\n$k$-means++, a novel initialisation strategy for the Minkowski Weighted\n$k$-means. Our initialisation selects centroids probabilistically using feature\nrelevance estimates derived from the data itself. Building on this, we propose\ntwo new feature selection algorithms, FS-MWK++, which aggregates feature\nweights across a range of Minkowski exponents to identify stable and\ninformative features, and SFS-MWK++, a scalable variant based on subsampling.\nWe support our approach with a theoretical guarantee under mild assumptions and\nextensive experiments showing that our methods consistently outperform existing\nalternatives. Our software can be found at\nhttps://github.com/xzhang4-ops1/FSMWK.\n", "link": "http://arxiv.org/abs/2506.06114v3", "date": "2025-06-13", "relevancy": 2.1371, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4486}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4244}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20unsupervised%20feature%20selection%20via%20weight%20stability&body=Title%3A%20Scalable%20unsupervised%20feature%20selection%20via%20weight%20stability%0AAuthor%3A%20Xudong%20Zhang%20and%20Renato%20Cordeiro%20de%20Amorim%0AAbstract%3A%20%20%20Unsupervised%20feature%20selection%20is%20critical%20for%20improving%20clustering%0Aperformance%20in%20high-dimensional%20data%2C%20where%20irrelevant%20features%20can%20obscure%0Ameaningful%20structure.%20In%20this%20work%2C%20we%20introduce%20the%20Minkowski%20weighted%0A%24k%24-means%2B%2B%2C%20a%20novel%20initialisation%20strategy%20for%20the%20Minkowski%20Weighted%0A%24k%24-means.%20Our%20initialisation%20selects%20centroids%20probabilistically%20using%20feature%0Arelevance%20estimates%20derived%20from%20the%20data%20itself.%20Building%20on%20this%2C%20we%20propose%0Atwo%20new%20feature%20selection%20algorithms%2C%20FS-MWK%2B%2B%2C%20which%20aggregates%20feature%0Aweights%20across%20a%20range%20of%20Minkowski%20exponents%20to%20identify%20stable%20and%0Ainformative%20features%2C%20and%20SFS-MWK%2B%2B%2C%20a%20scalable%20variant%20based%20on%20subsampling.%0AWe%20support%20our%20approach%20with%20a%20theoretical%20guarantee%20under%20mild%20assumptions%20and%0Aextensive%20experiments%20showing%20that%20our%20methods%20consistently%20outperform%20existing%0Aalternatives.%20Our%20software%20can%20be%20found%20at%0Ahttps%3A//github.com/xzhang4-ops1/FSMWK.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06114v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520unsupervised%2520feature%2520selection%2520via%2520weight%2520stability%26entry.906535625%3DXudong%2520Zhang%2520and%2520Renato%2520Cordeiro%2520de%2520Amorim%26entry.1292438233%3D%2520%2520Unsupervised%2520feature%2520selection%2520is%2520critical%2520for%2520improving%2520clustering%250Aperformance%2520in%2520high-dimensional%2520data%252C%2520where%2520irrelevant%2520features%2520can%2520obscure%250Ameaningful%2520structure.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Minkowski%2520weighted%250A%2524k%2524-means%252B%252B%252C%2520a%2520novel%2520initialisation%2520strategy%2520for%2520the%2520Minkowski%2520Weighted%250A%2524k%2524-means.%2520Our%2520initialisation%2520selects%2520centroids%2520probabilistically%2520using%2520feature%250Arelevance%2520estimates%2520derived%2520from%2520the%2520data%2520itself.%2520Building%2520on%2520this%252C%2520we%2520propose%250Atwo%2520new%2520feature%2520selection%2520algorithms%252C%2520FS-MWK%252B%252B%252C%2520which%2520aggregates%2520feature%250Aweights%2520across%2520a%2520range%2520of%2520Minkowski%2520exponents%2520to%2520identify%2520stable%2520and%250Ainformative%2520features%252C%2520and%2520SFS-MWK%252B%252B%252C%2520a%2520scalable%2520variant%2520based%2520on%2520subsampling.%250AWe%2520support%2520our%2520approach%2520with%2520a%2520theoretical%2520guarantee%2520under%2520mild%2520assumptions%2520and%250Aextensive%2520experiments%2520showing%2520that%2520our%2520methods%2520consistently%2520outperform%2520existing%250Aalternatives.%2520Our%2520software%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/xzhang4-ops1/FSMWK.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06114v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20unsupervised%20feature%20selection%20via%20weight%20stability&entry.906535625=Xudong%20Zhang%20and%20Renato%20Cordeiro%20de%20Amorim&entry.1292438233=%20%20Unsupervised%20feature%20selection%20is%20critical%20for%20improving%20clustering%0Aperformance%20in%20high-dimensional%20data%2C%20where%20irrelevant%20features%20can%20obscure%0Ameaningful%20structure.%20In%20this%20work%2C%20we%20introduce%20the%20Minkowski%20weighted%0A%24k%24-means%2B%2B%2C%20a%20novel%20initialisation%20strategy%20for%20the%20Minkowski%20Weighted%0A%24k%24-means.%20Our%20initialisation%20selects%20centroids%20probabilistically%20using%20feature%0Arelevance%20estimates%20derived%20from%20the%20data%20itself.%20Building%20on%20this%2C%20we%20propose%0Atwo%20new%20feature%20selection%20algorithms%2C%20FS-MWK%2B%2B%2C%20which%20aggregates%20feature%0Aweights%20across%20a%20range%20of%20Minkowski%20exponents%20to%20identify%20stable%20and%0Ainformative%20features%2C%20and%20SFS-MWK%2B%2B%2C%20a%20scalable%20variant%20based%20on%20subsampling.%0AWe%20support%20our%20approach%20with%20a%20theoretical%20guarantee%20under%20mild%20assumptions%20and%0Aextensive%20experiments%20showing%20that%20our%20methods%20consistently%20outperform%20existing%0Aalternatives.%20Our%20software%20can%20be%20found%20at%0Ahttps%3A//github.com/xzhang4-ops1/FSMWK.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06114v3&entry.124074799=Read"},
{"title": "Extended Hybrid Zero Dynamics for Bipedal Walking of the Knee-less Robot\n  SLIDER", "author": "Rui Zong and Martin Liang and Yuntian Fang and Ke Wang and Xiaoshuai Chen and Wei Chen and Petar Kormushev", "abstract": "  Knee-less bipedal robots like SLIDER have the advantage of ultra-lightweight\nlegs and improved walking energy efficiency compared to traditional humanoid\nrobots. In this paper, we firstly introduce an improved hardware design of the\nSLIDER bipedal robot with new line-feet and more optimized mass distribution\nthat enables higher locomotion speeds. Secondly, we propose an extended Hybrid\nZero Dynamics (eHZD) method, which can be applied to prismatic joint robots\nlike SLIDER. The eHZD method is then used to generate a library of gaits with\nvarying reference velocities in an offline way. Thirdly, a Guided Deep\nReinforcement Learning (DRL) algorithm is proposed to use the pre-generated\nlibrary to create walking control policies in real-time. This approach allows\nus to combine the advantages of both HZD (for generating stable gaits with a\nfull-dynamics model) and DRL (for real-time adaptive gait generation). The\nexperimental results show that this approach achieves 150% higher walking\nvelocity than the previous MPC-based approach.\n", "link": "http://arxiv.org/abs/2504.01165v2", "date": "2025-06-13", "relevancy": 2.1333, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5609}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5343}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extended%20Hybrid%20Zero%20Dynamics%20for%20Bipedal%20Walking%20of%20the%20Knee-less%20Robot%0A%20%20SLIDER&body=Title%3A%20Extended%20Hybrid%20Zero%20Dynamics%20for%20Bipedal%20Walking%20of%20the%20Knee-less%20Robot%0A%20%20SLIDER%0AAuthor%3A%20Rui%20Zong%20and%20Martin%20Liang%20and%20Yuntian%20Fang%20and%20Ke%20Wang%20and%20Xiaoshuai%20Chen%20and%20Wei%20Chen%20and%20Petar%20Kormushev%0AAbstract%3A%20%20%20Knee-less%20bipedal%20robots%20like%20SLIDER%20have%20the%20advantage%20of%20ultra-lightweight%0Alegs%20and%20improved%20walking%20energy%20efficiency%20compared%20to%20traditional%20humanoid%0Arobots.%20In%20this%20paper%2C%20we%20firstly%20introduce%20an%20improved%20hardware%20design%20of%20the%0ASLIDER%20bipedal%20robot%20with%20new%20line-feet%20and%20more%20optimized%20mass%20distribution%0Athat%20enables%20higher%20locomotion%20speeds.%20Secondly%2C%20we%20propose%20an%20extended%20Hybrid%0AZero%20Dynamics%20%28eHZD%29%20method%2C%20which%20can%20be%20applied%20to%20prismatic%20joint%20robots%0Alike%20SLIDER.%20The%20eHZD%20method%20is%20then%20used%20to%20generate%20a%20library%20of%20gaits%20with%0Avarying%20reference%20velocities%20in%20an%20offline%20way.%20Thirdly%2C%20a%20Guided%20Deep%0AReinforcement%20Learning%20%28DRL%29%20algorithm%20is%20proposed%20to%20use%20the%20pre-generated%0Alibrary%20to%20create%20walking%20control%20policies%20in%20real-time.%20This%20approach%20allows%0Aus%20to%20combine%20the%20advantages%20of%20both%20HZD%20%28for%20generating%20stable%20gaits%20with%20a%0Afull-dynamics%20model%29%20and%20DRL%20%28for%20real-time%20adaptive%20gait%20generation%29.%20The%0Aexperimental%20results%20show%20that%20this%20approach%20achieves%20150%25%20higher%20walking%0Avelocity%20than%20the%20previous%20MPC-based%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01165v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtended%2520Hybrid%2520Zero%2520Dynamics%2520for%2520Bipedal%2520Walking%2520of%2520the%2520Knee-less%2520Robot%250A%2520%2520SLIDER%26entry.906535625%3DRui%2520Zong%2520and%2520Martin%2520Liang%2520and%2520Yuntian%2520Fang%2520and%2520Ke%2520Wang%2520and%2520Xiaoshuai%2520Chen%2520and%2520Wei%2520Chen%2520and%2520Petar%2520Kormushev%26entry.1292438233%3D%2520%2520Knee-less%2520bipedal%2520robots%2520like%2520SLIDER%2520have%2520the%2520advantage%2520of%2520ultra-lightweight%250Alegs%2520and%2520improved%2520walking%2520energy%2520efficiency%2520compared%2520to%2520traditional%2520humanoid%250Arobots.%2520In%2520this%2520paper%252C%2520we%2520firstly%2520introduce%2520an%2520improved%2520hardware%2520design%2520of%2520the%250ASLIDER%2520bipedal%2520robot%2520with%2520new%2520line-feet%2520and%2520more%2520optimized%2520mass%2520distribution%250Athat%2520enables%2520higher%2520locomotion%2520speeds.%2520Secondly%252C%2520we%2520propose%2520an%2520extended%2520Hybrid%250AZero%2520Dynamics%2520%2528eHZD%2529%2520method%252C%2520which%2520can%2520be%2520applied%2520to%2520prismatic%2520joint%2520robots%250Alike%2520SLIDER.%2520The%2520eHZD%2520method%2520is%2520then%2520used%2520to%2520generate%2520a%2520library%2520of%2520gaits%2520with%250Avarying%2520reference%2520velocities%2520in%2520an%2520offline%2520way.%2520Thirdly%252C%2520a%2520Guided%2520Deep%250AReinforcement%2520Learning%2520%2528DRL%2529%2520algorithm%2520is%2520proposed%2520to%2520use%2520the%2520pre-generated%250Alibrary%2520to%2520create%2520walking%2520control%2520policies%2520in%2520real-time.%2520This%2520approach%2520allows%250Aus%2520to%2520combine%2520the%2520advantages%2520of%2520both%2520HZD%2520%2528for%2520generating%2520stable%2520gaits%2520with%2520a%250Afull-dynamics%2520model%2529%2520and%2520DRL%2520%2528for%2520real-time%2520adaptive%2520gait%2520generation%2529.%2520The%250Aexperimental%2520results%2520show%2520that%2520this%2520approach%2520achieves%2520150%2525%2520higher%2520walking%250Avelocity%2520than%2520the%2520previous%2520MPC-based%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01165v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extended%20Hybrid%20Zero%20Dynamics%20for%20Bipedal%20Walking%20of%20the%20Knee-less%20Robot%0A%20%20SLIDER&entry.906535625=Rui%20Zong%20and%20Martin%20Liang%20and%20Yuntian%20Fang%20and%20Ke%20Wang%20and%20Xiaoshuai%20Chen%20and%20Wei%20Chen%20and%20Petar%20Kormushev&entry.1292438233=%20%20Knee-less%20bipedal%20robots%20like%20SLIDER%20have%20the%20advantage%20of%20ultra-lightweight%0Alegs%20and%20improved%20walking%20energy%20efficiency%20compared%20to%20traditional%20humanoid%0Arobots.%20In%20this%20paper%2C%20we%20firstly%20introduce%20an%20improved%20hardware%20design%20of%20the%0ASLIDER%20bipedal%20robot%20with%20new%20line-feet%20and%20more%20optimized%20mass%20distribution%0Athat%20enables%20higher%20locomotion%20speeds.%20Secondly%2C%20we%20propose%20an%20extended%20Hybrid%0AZero%20Dynamics%20%28eHZD%29%20method%2C%20which%20can%20be%20applied%20to%20prismatic%20joint%20robots%0Alike%20SLIDER.%20The%20eHZD%20method%20is%20then%20used%20to%20generate%20a%20library%20of%20gaits%20with%0Avarying%20reference%20velocities%20in%20an%20offline%20way.%20Thirdly%2C%20a%20Guided%20Deep%0AReinforcement%20Learning%20%28DRL%29%20algorithm%20is%20proposed%20to%20use%20the%20pre-generated%0Alibrary%20to%20create%20walking%20control%20policies%20in%20real-time.%20This%20approach%20allows%0Aus%20to%20combine%20the%20advantages%20of%20both%20HZD%20%28for%20generating%20stable%20gaits%20with%20a%0Afull-dynamics%20model%29%20and%20DRL%20%28for%20real-time%20adaptive%20gait%20generation%29.%20The%0Aexperimental%20results%20show%20that%20this%20approach%20achieves%20150%25%20higher%20walking%0Avelocity%20than%20the%20previous%20MPC-based%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01165v2&entry.124074799=Read"},
{"title": "Prohibited Items Segmentation via Occlusion-aware Bilayer Modeling", "author": "Yunhan Ren and Ruihuang Li and Lingbo Liu and Changwen Chen", "abstract": "  Instance segmentation of prohibited items in security X-ray images is a\ncritical yet challenging task. This is mainly caused by the significant\nappearance gap between prohibited items in X-ray images and natural objects, as\nwell as the severe overlapping among objects in X-ray images. To address these\nissues, we propose an occlusion-aware instance segmentation pipeline designed\nto identify prohibited items in X-ray images. Specifically, to bridge the\nrepresentation gap, we integrate the Segment Anything Model (SAM) into our\npipeline, taking advantage of its rich priors and zero-shot generalization\ncapabilities. To address the overlap between prohibited items, we design an\nocclusion-aware bilayer mask decoder module that explicitly models the\nocclusion relationships. To supervise occlusion estimation, we manually\nannotated occlusion areas of prohibited items in two large-scale X-ray image\nsegmentation datasets, PIDray and PIXray. We then reorganized these additional\nannotations together with the original information as two occlusion-annotated\ndatasets, PIDray-A and PIXray-A. Extensive experimental results on these\nocclusion-annotated datasets demonstrate the effectiveness of our proposed\nmethod. The datasets and codes are available at: https://github.com/Ryh1218/Occ\n", "link": "http://arxiv.org/abs/2506.11661v1", "date": "2025-06-13", "relevancy": 2.1328, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prohibited%20Items%20Segmentation%20via%20Occlusion-aware%20Bilayer%20Modeling&body=Title%3A%20Prohibited%20Items%20Segmentation%20via%20Occlusion-aware%20Bilayer%20Modeling%0AAuthor%3A%20Yunhan%20Ren%20and%20Ruihuang%20Li%20and%20Lingbo%20Liu%20and%20Changwen%20Chen%0AAbstract%3A%20%20%20Instance%20segmentation%20of%20prohibited%20items%20in%20security%20X-ray%20images%20is%20a%0Acritical%20yet%20challenging%20task.%20This%20is%20mainly%20caused%20by%20the%20significant%0Aappearance%20gap%20between%20prohibited%20items%20in%20X-ray%20images%20and%20natural%20objects%2C%20as%0Awell%20as%20the%20severe%20overlapping%20among%20objects%20in%20X-ray%20images.%20To%20address%20these%0Aissues%2C%20we%20propose%20an%20occlusion-aware%20instance%20segmentation%20pipeline%20designed%0Ato%20identify%20prohibited%20items%20in%20X-ray%20images.%20Specifically%2C%20to%20bridge%20the%0Arepresentation%20gap%2C%20we%20integrate%20the%20Segment%20Anything%20Model%20%28SAM%29%20into%20our%0Apipeline%2C%20taking%20advantage%20of%20its%20rich%20priors%20and%20zero-shot%20generalization%0Acapabilities.%20To%20address%20the%20overlap%20between%20prohibited%20items%2C%20we%20design%20an%0Aocclusion-aware%20bilayer%20mask%20decoder%20module%20that%20explicitly%20models%20the%0Aocclusion%20relationships.%20To%20supervise%20occlusion%20estimation%2C%20we%20manually%0Aannotated%20occlusion%20areas%20of%20prohibited%20items%20in%20two%20large-scale%20X-ray%20image%0Asegmentation%20datasets%2C%20PIDray%20and%20PIXray.%20We%20then%20reorganized%20these%20additional%0Aannotations%20together%20with%20the%20original%20information%20as%20two%20occlusion-annotated%0Adatasets%2C%20PIDray-A%20and%20PIXray-A.%20Extensive%20experimental%20results%20on%20these%0Aocclusion-annotated%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Amethod.%20The%20datasets%20and%20codes%20are%20available%20at%3A%20https%3A//github.com/Ryh1218/Occ%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProhibited%2520Items%2520Segmentation%2520via%2520Occlusion-aware%2520Bilayer%2520Modeling%26entry.906535625%3DYunhan%2520Ren%2520and%2520Ruihuang%2520Li%2520and%2520Lingbo%2520Liu%2520and%2520Changwen%2520Chen%26entry.1292438233%3D%2520%2520Instance%2520segmentation%2520of%2520prohibited%2520items%2520in%2520security%2520X-ray%2520images%2520is%2520a%250Acritical%2520yet%2520challenging%2520task.%2520This%2520is%2520mainly%2520caused%2520by%2520the%2520significant%250Aappearance%2520gap%2520between%2520prohibited%2520items%2520in%2520X-ray%2520images%2520and%2520natural%2520objects%252C%2520as%250Awell%2520as%2520the%2520severe%2520overlapping%2520among%2520objects%2520in%2520X-ray%2520images.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520an%2520occlusion-aware%2520instance%2520segmentation%2520pipeline%2520designed%250Ato%2520identify%2520prohibited%2520items%2520in%2520X-ray%2520images.%2520Specifically%252C%2520to%2520bridge%2520the%250Arepresentation%2520gap%252C%2520we%2520integrate%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520into%2520our%250Apipeline%252C%2520taking%2520advantage%2520of%2520its%2520rich%2520priors%2520and%2520zero-shot%2520generalization%250Acapabilities.%2520To%2520address%2520the%2520overlap%2520between%2520prohibited%2520items%252C%2520we%2520design%2520an%250Aocclusion-aware%2520bilayer%2520mask%2520decoder%2520module%2520that%2520explicitly%2520models%2520the%250Aocclusion%2520relationships.%2520To%2520supervise%2520occlusion%2520estimation%252C%2520we%2520manually%250Aannotated%2520occlusion%2520areas%2520of%2520prohibited%2520items%2520in%2520two%2520large-scale%2520X-ray%2520image%250Asegmentation%2520datasets%252C%2520PIDray%2520and%2520PIXray.%2520We%2520then%2520reorganized%2520these%2520additional%250Aannotations%2520together%2520with%2520the%2520original%2520information%2520as%2520two%2520occlusion-annotated%250Adatasets%252C%2520PIDray-A%2520and%2520PIXray-A.%2520Extensive%2520experimental%2520results%2520on%2520these%250Aocclusion-annotated%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%250Amethod.%2520The%2520datasets%2520and%2520codes%2520are%2520available%2520at%253A%2520https%253A//github.com/Ryh1218/Occ%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prohibited%20Items%20Segmentation%20via%20Occlusion-aware%20Bilayer%20Modeling&entry.906535625=Yunhan%20Ren%20and%20Ruihuang%20Li%20and%20Lingbo%20Liu%20and%20Changwen%20Chen&entry.1292438233=%20%20Instance%20segmentation%20of%20prohibited%20items%20in%20security%20X-ray%20images%20is%20a%0Acritical%20yet%20challenging%20task.%20This%20is%20mainly%20caused%20by%20the%20significant%0Aappearance%20gap%20between%20prohibited%20items%20in%20X-ray%20images%20and%20natural%20objects%2C%20as%0Awell%20as%20the%20severe%20overlapping%20among%20objects%20in%20X-ray%20images.%20To%20address%20these%0Aissues%2C%20we%20propose%20an%20occlusion-aware%20instance%20segmentation%20pipeline%20designed%0Ato%20identify%20prohibited%20items%20in%20X-ray%20images.%20Specifically%2C%20to%20bridge%20the%0Arepresentation%20gap%2C%20we%20integrate%20the%20Segment%20Anything%20Model%20%28SAM%29%20into%20our%0Apipeline%2C%20taking%20advantage%20of%20its%20rich%20priors%20and%20zero-shot%20generalization%0Acapabilities.%20To%20address%20the%20overlap%20between%20prohibited%20items%2C%20we%20design%20an%0Aocclusion-aware%20bilayer%20mask%20decoder%20module%20that%20explicitly%20models%20the%0Aocclusion%20relationships.%20To%20supervise%20occlusion%20estimation%2C%20we%20manually%0Aannotated%20occlusion%20areas%20of%20prohibited%20items%20in%20two%20large-scale%20X-ray%20image%0Asegmentation%20datasets%2C%20PIDray%20and%20PIXray.%20We%20then%20reorganized%20these%20additional%0Aannotations%20together%20with%20the%20original%20information%20as%20two%20occlusion-annotated%0Adatasets%2C%20PIDray-A%20and%20PIXray-A.%20Extensive%20experimental%20results%20on%20these%0Aocclusion-annotated%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Amethod.%20The%20datasets%20and%20codes%20are%20available%20at%3A%20https%3A//github.com/Ryh1218/Occ%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11661v1&entry.124074799=Read"},
{"title": "Real-World Deployment of a Lane Change Prediction Architecture Based on\n  Knowledge Graph Embeddings and Bayesian Inference", "author": "M. Manzour and Catherine M. Elias and Omar M. Shehata and R. Izquierdo and M. A. Sotelo", "abstract": "  Research on lane change prediction has gained a lot of momentum in the last\ncouple of years. However, most research is confined to simulation or results\nobtained from datasets, leaving a gap between algorithmic advances and on-road\ndeployment. This work closes that gap by demonstrating, on real hardware, a\nlane-change prediction system based on Knowledge Graph Embeddings (KGEs) and\nBayesian inference. Moreover, the ego-vehicle employs a longitudinal braking\naction to ensure the safety of both itself and the surrounding vehicles. Our\narchitecture consists of two modules: (i) a perception module that senses the\nenvironment, derives input numerical features, and converts them into\nlinguistic categories; and communicates them to the prediction module; (ii) a\npretrained prediction module that executes a KGE and Bayesian inference model\nto anticipate the target vehicle's maneuver and transforms the prediction into\nlongitudinal braking action. Real-world hardware experimental validation\ndemonstrates that our prediction system anticipates the target vehicle's lane\nchange three to four seconds in advance, providing the ego vehicle sufficient\ntime to react and allowing the target vehicle to make the lane change safely.\n", "link": "http://arxiv.org/abs/2506.11925v1", "date": "2025-06-13", "relevancy": 2.1303, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.631}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5254}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-World%20Deployment%20of%20a%20Lane%20Change%20Prediction%20Architecture%20Based%20on%0A%20%20Knowledge%20Graph%20Embeddings%20and%20Bayesian%20Inference&body=Title%3A%20Real-World%20Deployment%20of%20a%20Lane%20Change%20Prediction%20Architecture%20Based%20on%0A%20%20Knowledge%20Graph%20Embeddings%20and%20Bayesian%20Inference%0AAuthor%3A%20M.%20Manzour%20and%20Catherine%20M.%20Elias%20and%20Omar%20M.%20Shehata%20and%20R.%20Izquierdo%20and%20M.%20A.%20Sotelo%0AAbstract%3A%20%20%20Research%20on%20lane%20change%20prediction%20has%20gained%20a%20lot%20of%20momentum%20in%20the%20last%0Acouple%20of%20years.%20However%2C%20most%20research%20is%20confined%20to%20simulation%20or%20results%0Aobtained%20from%20datasets%2C%20leaving%20a%20gap%20between%20algorithmic%20advances%20and%20on-road%0Adeployment.%20This%20work%20closes%20that%20gap%20by%20demonstrating%2C%20on%20real%20hardware%2C%20a%0Alane-change%20prediction%20system%20based%20on%20Knowledge%20Graph%20Embeddings%20%28KGEs%29%20and%0ABayesian%20inference.%20Moreover%2C%20the%20ego-vehicle%20employs%20a%20longitudinal%20braking%0Aaction%20to%20ensure%20the%20safety%20of%20both%20itself%20and%20the%20surrounding%20vehicles.%20Our%0Aarchitecture%20consists%20of%20two%20modules%3A%20%28i%29%20a%20perception%20module%20that%20senses%20the%0Aenvironment%2C%20derives%20input%20numerical%20features%2C%20and%20converts%20them%20into%0Alinguistic%20categories%3B%20and%20communicates%20them%20to%20the%20prediction%20module%3B%20%28ii%29%20a%0Apretrained%20prediction%20module%20that%20executes%20a%20KGE%20and%20Bayesian%20inference%20model%0Ato%20anticipate%20the%20target%20vehicle%27s%20maneuver%20and%20transforms%20the%20prediction%20into%0Alongitudinal%20braking%20action.%20Real-world%20hardware%20experimental%20validation%0Ademonstrates%20that%20our%20prediction%20system%20anticipates%20the%20target%20vehicle%27s%20lane%0Achange%20three%20to%20four%20seconds%20in%20advance%2C%20providing%20the%20ego%20vehicle%20sufficient%0Atime%20to%20react%20and%20allowing%20the%20target%20vehicle%20to%20make%20the%20lane%20change%20safely.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-World%2520Deployment%2520of%2520a%2520Lane%2520Change%2520Prediction%2520Architecture%2520Based%2520on%250A%2520%2520Knowledge%2520Graph%2520Embeddings%2520and%2520Bayesian%2520Inference%26entry.906535625%3DM.%2520Manzour%2520and%2520Catherine%2520M.%2520Elias%2520and%2520Omar%2520M.%2520Shehata%2520and%2520R.%2520Izquierdo%2520and%2520M.%2520A.%2520Sotelo%26entry.1292438233%3D%2520%2520Research%2520on%2520lane%2520change%2520prediction%2520has%2520gained%2520a%2520lot%2520of%2520momentum%2520in%2520the%2520last%250Acouple%2520of%2520years.%2520However%252C%2520most%2520research%2520is%2520confined%2520to%2520simulation%2520or%2520results%250Aobtained%2520from%2520datasets%252C%2520leaving%2520a%2520gap%2520between%2520algorithmic%2520advances%2520and%2520on-road%250Adeployment.%2520This%2520work%2520closes%2520that%2520gap%2520by%2520demonstrating%252C%2520on%2520real%2520hardware%252C%2520a%250Alane-change%2520prediction%2520system%2520based%2520on%2520Knowledge%2520Graph%2520Embeddings%2520%2528KGEs%2529%2520and%250ABayesian%2520inference.%2520Moreover%252C%2520the%2520ego-vehicle%2520employs%2520a%2520longitudinal%2520braking%250Aaction%2520to%2520ensure%2520the%2520safety%2520of%2520both%2520itself%2520and%2520the%2520surrounding%2520vehicles.%2520Our%250Aarchitecture%2520consists%2520of%2520two%2520modules%253A%2520%2528i%2529%2520a%2520perception%2520module%2520that%2520senses%2520the%250Aenvironment%252C%2520derives%2520input%2520numerical%2520features%252C%2520and%2520converts%2520them%2520into%250Alinguistic%2520categories%253B%2520and%2520communicates%2520them%2520to%2520the%2520prediction%2520module%253B%2520%2528ii%2529%2520a%250Apretrained%2520prediction%2520module%2520that%2520executes%2520a%2520KGE%2520and%2520Bayesian%2520inference%2520model%250Ato%2520anticipate%2520the%2520target%2520vehicle%2527s%2520maneuver%2520and%2520transforms%2520the%2520prediction%2520into%250Alongitudinal%2520braking%2520action.%2520Real-world%2520hardware%2520experimental%2520validation%250Ademonstrates%2520that%2520our%2520prediction%2520system%2520anticipates%2520the%2520target%2520vehicle%2527s%2520lane%250Achange%2520three%2520to%2520four%2520seconds%2520in%2520advance%252C%2520providing%2520the%2520ego%2520vehicle%2520sufficient%250Atime%2520to%2520react%2520and%2520allowing%2520the%2520target%2520vehicle%2520to%2520make%2520the%2520lane%2520change%2520safely.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-World%20Deployment%20of%20a%20Lane%20Change%20Prediction%20Architecture%20Based%20on%0A%20%20Knowledge%20Graph%20Embeddings%20and%20Bayesian%20Inference&entry.906535625=M.%20Manzour%20and%20Catherine%20M.%20Elias%20and%20Omar%20M.%20Shehata%20and%20R.%20Izquierdo%20and%20M.%20A.%20Sotelo&entry.1292438233=%20%20Research%20on%20lane%20change%20prediction%20has%20gained%20a%20lot%20of%20momentum%20in%20the%20last%0Acouple%20of%20years.%20However%2C%20most%20research%20is%20confined%20to%20simulation%20or%20results%0Aobtained%20from%20datasets%2C%20leaving%20a%20gap%20between%20algorithmic%20advances%20and%20on-road%0Adeployment.%20This%20work%20closes%20that%20gap%20by%20demonstrating%2C%20on%20real%20hardware%2C%20a%0Alane-change%20prediction%20system%20based%20on%20Knowledge%20Graph%20Embeddings%20%28KGEs%29%20and%0ABayesian%20inference.%20Moreover%2C%20the%20ego-vehicle%20employs%20a%20longitudinal%20braking%0Aaction%20to%20ensure%20the%20safety%20of%20both%20itself%20and%20the%20surrounding%20vehicles.%20Our%0Aarchitecture%20consists%20of%20two%20modules%3A%20%28i%29%20a%20perception%20module%20that%20senses%20the%0Aenvironment%2C%20derives%20input%20numerical%20features%2C%20and%20converts%20them%20into%0Alinguistic%20categories%3B%20and%20communicates%20them%20to%20the%20prediction%20module%3B%20%28ii%29%20a%0Apretrained%20prediction%20module%20that%20executes%20a%20KGE%20and%20Bayesian%20inference%20model%0Ato%20anticipate%20the%20target%20vehicle%27s%20maneuver%20and%20transforms%20the%20prediction%20into%0Alongitudinal%20braking%20action.%20Real-world%20hardware%20experimental%20validation%0Ademonstrates%20that%20our%20prediction%20system%20anticipates%20the%20target%20vehicle%27s%20lane%0Achange%20three%20to%20four%20seconds%20in%20advance%2C%20providing%20the%20ego%20vehicle%20sufficient%0Atime%20to%20react%20and%20allowing%20the%20target%20vehicle%20to%20make%20the%20lane%20change%20safely.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11925v1&entry.124074799=Read"},
{"title": "\"It's not a representation of me\": Examining Accent Bias and Digital\n  Exclusion in Synthetic AI Voice Services", "author": "Shira Michel and Sufi Kaur and Sarah Elizabeth Gillespie and Jeffrey Gleason and Christo Wilson and Avijit Ghosh", "abstract": "  Recent advances in artificial intelligence (AI) speech generation and voice\ncloning technologies have produced naturalistic speech and accurate voice\nreplication, yet their influence on sociotechnical systems across diverse\naccents and linguistic traits is not fully understood. This study evaluates two\nsynthetic AI voice services (Speechify and ElevenLabs) through a mixed methods\napproach using surveys and interviews to assess technical performance and\nuncover how users' lived experiences influence their perceptions of accent\nvariations in these speech technologies. Our findings reveal technical\nperformance disparities across five regional, English-language accents and\ndemonstrate how current speech generation technologies may inadvertently\nreinforce linguistic privilege and accent-based discrimination, potentially\ncreating new forms of digital exclusion. Overall, our study highlights the need\nfor inclusive design and regulation by providing actionable insights for\ndevelopers, policymakers, and organizations to ensure equitable and socially\nresponsible AI speech technologies.\n", "link": "http://arxiv.org/abs/2504.09346v2", "date": "2025-06-13", "relevancy": 2.1262, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22It%27s%20not%20a%20representation%20of%20me%22%3A%20Examining%20Accent%20Bias%20and%20Digital%0A%20%20Exclusion%20in%20Synthetic%20AI%20Voice%20Services&body=Title%3A%20%22It%27s%20not%20a%20representation%20of%20me%22%3A%20Examining%20Accent%20Bias%20and%20Digital%0A%20%20Exclusion%20in%20Synthetic%20AI%20Voice%20Services%0AAuthor%3A%20Shira%20Michel%20and%20Sufi%20Kaur%20and%20Sarah%20Elizabeth%20Gillespie%20and%20Jeffrey%20Gleason%20and%20Christo%20Wilson%20and%20Avijit%20Ghosh%0AAbstract%3A%20%20%20Recent%20advances%20in%20artificial%20intelligence%20%28AI%29%20speech%20generation%20and%20voice%0Acloning%20technologies%20have%20produced%20naturalistic%20speech%20and%20accurate%20voice%0Areplication%2C%20yet%20their%20influence%20on%20sociotechnical%20systems%20across%20diverse%0Aaccents%20and%20linguistic%20traits%20is%20not%20fully%20understood.%20This%20study%20evaluates%20two%0Asynthetic%20AI%20voice%20services%20%28Speechify%20and%20ElevenLabs%29%20through%20a%20mixed%20methods%0Aapproach%20using%20surveys%20and%20interviews%20to%20assess%20technical%20performance%20and%0Auncover%20how%20users%27%20lived%20experiences%20influence%20their%20perceptions%20of%20accent%0Avariations%20in%20these%20speech%20technologies.%20Our%20findings%20reveal%20technical%0Aperformance%20disparities%20across%20five%20regional%2C%20English-language%20accents%20and%0Ademonstrate%20how%20current%20speech%20generation%20technologies%20may%20inadvertently%0Areinforce%20linguistic%20privilege%20and%20accent-based%20discrimination%2C%20potentially%0Acreating%20new%20forms%20of%20digital%20exclusion.%20Overall%2C%20our%20study%20highlights%20the%20need%0Afor%20inclusive%20design%20and%20regulation%20by%20providing%20actionable%20insights%20for%0Adevelopers%2C%20policymakers%2C%20and%20organizations%20to%20ensure%20equitable%20and%20socially%0Aresponsible%20AI%20speech%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522It%2527s%2520not%2520a%2520representation%2520of%2520me%2522%253A%2520Examining%2520Accent%2520Bias%2520and%2520Digital%250A%2520%2520Exclusion%2520in%2520Synthetic%2520AI%2520Voice%2520Services%26entry.906535625%3DShira%2520Michel%2520and%2520Sufi%2520Kaur%2520and%2520Sarah%2520Elizabeth%2520Gillespie%2520and%2520Jeffrey%2520Gleason%2520and%2520Christo%2520Wilson%2520and%2520Avijit%2520Ghosh%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520artificial%2520intelligence%2520%2528AI%2529%2520speech%2520generation%2520and%2520voice%250Acloning%2520technologies%2520have%2520produced%2520naturalistic%2520speech%2520and%2520accurate%2520voice%250Areplication%252C%2520yet%2520their%2520influence%2520on%2520sociotechnical%2520systems%2520across%2520diverse%250Aaccents%2520and%2520linguistic%2520traits%2520is%2520not%2520fully%2520understood.%2520This%2520study%2520evaluates%2520two%250Asynthetic%2520AI%2520voice%2520services%2520%2528Speechify%2520and%2520ElevenLabs%2529%2520through%2520a%2520mixed%2520methods%250Aapproach%2520using%2520surveys%2520and%2520interviews%2520to%2520assess%2520technical%2520performance%2520and%250Auncover%2520how%2520users%2527%2520lived%2520experiences%2520influence%2520their%2520perceptions%2520of%2520accent%250Avariations%2520in%2520these%2520speech%2520technologies.%2520Our%2520findings%2520reveal%2520technical%250Aperformance%2520disparities%2520across%2520five%2520regional%252C%2520English-language%2520accents%2520and%250Ademonstrate%2520how%2520current%2520speech%2520generation%2520technologies%2520may%2520inadvertently%250Areinforce%2520linguistic%2520privilege%2520and%2520accent-based%2520discrimination%252C%2520potentially%250Acreating%2520new%2520forms%2520of%2520digital%2520exclusion.%2520Overall%252C%2520our%2520study%2520highlights%2520the%2520need%250Afor%2520inclusive%2520design%2520and%2520regulation%2520by%2520providing%2520actionable%2520insights%2520for%250Adevelopers%252C%2520policymakers%252C%2520and%2520organizations%2520to%2520ensure%2520equitable%2520and%2520socially%250Aresponsible%2520AI%2520speech%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22It%27s%20not%20a%20representation%20of%20me%22%3A%20Examining%20Accent%20Bias%20and%20Digital%0A%20%20Exclusion%20in%20Synthetic%20AI%20Voice%20Services&entry.906535625=Shira%20Michel%20and%20Sufi%20Kaur%20and%20Sarah%20Elizabeth%20Gillespie%20and%20Jeffrey%20Gleason%20and%20Christo%20Wilson%20and%20Avijit%20Ghosh&entry.1292438233=%20%20Recent%20advances%20in%20artificial%20intelligence%20%28AI%29%20speech%20generation%20and%20voice%0Acloning%20technologies%20have%20produced%20naturalistic%20speech%20and%20accurate%20voice%0Areplication%2C%20yet%20their%20influence%20on%20sociotechnical%20systems%20across%20diverse%0Aaccents%20and%20linguistic%20traits%20is%20not%20fully%20understood.%20This%20study%20evaluates%20two%0Asynthetic%20AI%20voice%20services%20%28Speechify%20and%20ElevenLabs%29%20through%20a%20mixed%20methods%0Aapproach%20using%20surveys%20and%20interviews%20to%20assess%20technical%20performance%20and%0Auncover%20how%20users%27%20lived%20experiences%20influence%20their%20perceptions%20of%20accent%0Avariations%20in%20these%20speech%20technologies.%20Our%20findings%20reveal%20technical%0Aperformance%20disparities%20across%20five%20regional%2C%20English-language%20accents%20and%0Ademonstrate%20how%20current%20speech%20generation%20technologies%20may%20inadvertently%0Areinforce%20linguistic%20privilege%20and%20accent-based%20discrimination%2C%20potentially%0Acreating%20new%20forms%20of%20digital%20exclusion.%20Overall%2C%20our%20study%20highlights%20the%20need%0Afor%20inclusive%20design%20and%20regulation%20by%20providing%20actionable%20insights%20for%0Adevelopers%2C%20policymakers%2C%20and%20organizations%20to%20ensure%20equitable%20and%20socially%0Aresponsible%20AI%20speech%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09346v2&entry.124074799=Read"},
{"title": "Guiding Time-Varying Generative Models with Natural Gradients on\n  Exponential Family Manifold", "author": "Song Liu and Leyang Wang and Yakun Wang", "abstract": "  Optimising probabilistic models is a well-studied field in statistics.\nHowever, its connection with the training of generative models remains largely\nunder-explored. In this paper, we show that the evolution of time-varying\ngenerative models can be projected onto an exponential family manifold,\nnaturally creating a link between the parameters of a generative model and\nthose of a probabilistic model. We then train the generative model by moving\nits projection on the manifold according to the natural gradient descent\nscheme. This approach also allows us to efficiently approximate the natural\ngradient of the KL divergence without relying on MCMC for intractable models.\nFurthermore, we propose particle versions of the algorithm, which feature\nclosed-form update rules for any parametric model within the exponential\nfamily. Through toy and real-world experiments, we validate the effectiveness\nof the proposed algorithms. The code of the proposed algorithms can be found at\nhttps://github.com/anewgithubname/iNGD.\n", "link": "http://arxiv.org/abs/2502.07650v2", "date": "2025-06-13", "relevancy": 2.1166, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5341}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.527}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guiding%20Time-Varying%20Generative%20Models%20with%20Natural%20Gradients%20on%0A%20%20Exponential%20Family%20Manifold&body=Title%3A%20Guiding%20Time-Varying%20Generative%20Models%20with%20Natural%20Gradients%20on%0A%20%20Exponential%20Family%20Manifold%0AAuthor%3A%20Song%20Liu%20and%20Leyang%20Wang%20and%20Yakun%20Wang%0AAbstract%3A%20%20%20Optimising%20probabilistic%20models%20is%20a%20well-studied%20field%20in%20statistics.%0AHowever%2C%20its%20connection%20with%20the%20training%20of%20generative%20models%20remains%20largely%0Aunder-explored.%20In%20this%20paper%2C%20we%20show%20that%20the%20evolution%20of%20time-varying%0Agenerative%20models%20can%20be%20projected%20onto%20an%20exponential%20family%20manifold%2C%0Anaturally%20creating%20a%20link%20between%20the%20parameters%20of%20a%20generative%20model%20and%0Athose%20of%20a%20probabilistic%20model.%20We%20then%20train%20the%20generative%20model%20by%20moving%0Aits%20projection%20on%20the%20manifold%20according%20to%20the%20natural%20gradient%20descent%0Ascheme.%20This%20approach%20also%20allows%20us%20to%20efficiently%20approximate%20the%20natural%0Agradient%20of%20the%20KL%20divergence%20without%20relying%20on%20MCMC%20for%20intractable%20models.%0AFurthermore%2C%20we%20propose%20particle%20versions%20of%20the%20algorithm%2C%20which%20feature%0Aclosed-form%20update%20rules%20for%20any%20parametric%20model%20within%20the%20exponential%0Afamily.%20Through%20toy%20and%20real-world%20experiments%2C%20we%20validate%20the%20effectiveness%0Aof%20the%20proposed%20algorithms.%20The%20code%20of%20the%20proposed%20algorithms%20can%20be%20found%20at%0Ahttps%3A//github.com/anewgithubname/iNGD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07650v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuiding%2520Time-Varying%2520Generative%2520Models%2520with%2520Natural%2520Gradients%2520on%250A%2520%2520Exponential%2520Family%2520Manifold%26entry.906535625%3DSong%2520Liu%2520and%2520Leyang%2520Wang%2520and%2520Yakun%2520Wang%26entry.1292438233%3D%2520%2520Optimising%2520probabilistic%2520models%2520is%2520a%2520well-studied%2520field%2520in%2520statistics.%250AHowever%252C%2520its%2520connection%2520with%2520the%2520training%2520of%2520generative%2520models%2520remains%2520largely%250Aunder-explored.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520the%2520evolution%2520of%2520time-varying%250Agenerative%2520models%2520can%2520be%2520projected%2520onto%2520an%2520exponential%2520family%2520manifold%252C%250Anaturally%2520creating%2520a%2520link%2520between%2520the%2520parameters%2520of%2520a%2520generative%2520model%2520and%250Athose%2520of%2520a%2520probabilistic%2520model.%2520We%2520then%2520train%2520the%2520generative%2520model%2520by%2520moving%250Aits%2520projection%2520on%2520the%2520manifold%2520according%2520to%2520the%2520natural%2520gradient%2520descent%250Ascheme.%2520This%2520approach%2520also%2520allows%2520us%2520to%2520efficiently%2520approximate%2520the%2520natural%250Agradient%2520of%2520the%2520KL%2520divergence%2520without%2520relying%2520on%2520MCMC%2520for%2520intractable%2520models.%250AFurthermore%252C%2520we%2520propose%2520particle%2520versions%2520of%2520the%2520algorithm%252C%2520which%2520feature%250Aclosed-form%2520update%2520rules%2520for%2520any%2520parametric%2520model%2520within%2520the%2520exponential%250Afamily.%2520Through%2520toy%2520and%2520real-world%2520experiments%252C%2520we%2520validate%2520the%2520effectiveness%250Aof%2520the%2520proposed%2520algorithms.%2520The%2520code%2520of%2520the%2520proposed%2520algorithms%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/anewgithubname/iNGD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07650v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guiding%20Time-Varying%20Generative%20Models%20with%20Natural%20Gradients%20on%0A%20%20Exponential%20Family%20Manifold&entry.906535625=Song%20Liu%20and%20Leyang%20Wang%20and%20Yakun%20Wang&entry.1292438233=%20%20Optimising%20probabilistic%20models%20is%20a%20well-studied%20field%20in%20statistics.%0AHowever%2C%20its%20connection%20with%20the%20training%20of%20generative%20models%20remains%20largely%0Aunder-explored.%20In%20this%20paper%2C%20we%20show%20that%20the%20evolution%20of%20time-varying%0Agenerative%20models%20can%20be%20projected%20onto%20an%20exponential%20family%20manifold%2C%0Anaturally%20creating%20a%20link%20between%20the%20parameters%20of%20a%20generative%20model%20and%0Athose%20of%20a%20probabilistic%20model.%20We%20then%20train%20the%20generative%20model%20by%20moving%0Aits%20projection%20on%20the%20manifold%20according%20to%20the%20natural%20gradient%20descent%0Ascheme.%20This%20approach%20also%20allows%20us%20to%20efficiently%20approximate%20the%20natural%0Agradient%20of%20the%20KL%20divergence%20without%20relying%20on%20MCMC%20for%20intractable%20models.%0AFurthermore%2C%20we%20propose%20particle%20versions%20of%20the%20algorithm%2C%20which%20feature%0Aclosed-form%20update%20rules%20for%20any%20parametric%20model%20within%20the%20exponential%0Afamily.%20Through%20toy%20and%20real-world%20experiments%2C%20we%20validate%20the%20effectiveness%0Aof%20the%20proposed%20algorithms.%20The%20code%20of%20the%20proposed%20algorithms%20can%20be%20found%20at%0Ahttps%3A//github.com/anewgithubname/iNGD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07650v2&entry.124074799=Read"},
{"title": "Scalable Generalized Bayesian Online Neural Network Training for\n  Sequential Decision Making", "author": "Gerardo Duran-Martin and Leandro S\u00e1nchez-Betancourt and \u00c1lvaro Cartea and Kevin Murphy", "abstract": "  We introduce scalable algorithms for online learning and generalized Bayesian\ninference of neural network parameters, designed for sequential decision making\ntasks. Our methods combine the strengths of frequentist and Bayesian filtering,\nwhich include fast low-rank updates via a block-diagonal approximation of the\nparameter error covariance, and a well-defined posterior predictive\ndistribution that we use for decision making. More precisely, our main method\nupdates a low-rank error covariance for the hidden layers parameters, and a\nfull-rank error covariance for the final layer parameters. Although this\ncharacterizes an improper posterior, we show that the resulting posterior\npredictive distribution is well-defined. Our methods update all network\nparameters online, with no need for replay buffers or offline retraining. We\nshow, empirically, that our methods achieve a competitive tradeoff between\nspeed and accuracy on (non-stationary) contextual bandit problems and Bayesian\noptimization problems.\n", "link": "http://arxiv.org/abs/2506.11898v1", "date": "2025-06-13", "relevancy": 2.1155, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5627}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5539}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Generalized%20Bayesian%20Online%20Neural%20Network%20Training%20for%0A%20%20Sequential%20Decision%20Making&body=Title%3A%20Scalable%20Generalized%20Bayesian%20Online%20Neural%20Network%20Training%20for%0A%20%20Sequential%20Decision%20Making%0AAuthor%3A%20Gerardo%20Duran-Martin%20and%20Leandro%20S%C3%A1nchez-Betancourt%20and%20%C3%81lvaro%20Cartea%20and%20Kevin%20Murphy%0AAbstract%3A%20%20%20We%20introduce%20scalable%20algorithms%20for%20online%20learning%20and%20generalized%20Bayesian%0Ainference%20of%20neural%20network%20parameters%2C%20designed%20for%20sequential%20decision%20making%0Atasks.%20Our%20methods%20combine%20the%20strengths%20of%20frequentist%20and%20Bayesian%20filtering%2C%0Awhich%20include%20fast%20low-rank%20updates%20via%20a%20block-diagonal%20approximation%20of%20the%0Aparameter%20error%20covariance%2C%20and%20a%20well-defined%20posterior%20predictive%0Adistribution%20that%20we%20use%20for%20decision%20making.%20More%20precisely%2C%20our%20main%20method%0Aupdates%20a%20low-rank%20error%20covariance%20for%20the%20hidden%20layers%20parameters%2C%20and%20a%0Afull-rank%20error%20covariance%20for%20the%20final%20layer%20parameters.%20Although%20this%0Acharacterizes%20an%20improper%20posterior%2C%20we%20show%20that%20the%20resulting%20posterior%0Apredictive%20distribution%20is%20well-defined.%20Our%20methods%20update%20all%20network%0Aparameters%20online%2C%20with%20no%20need%20for%20replay%20buffers%20or%20offline%20retraining.%20We%0Ashow%2C%20empirically%2C%20that%20our%20methods%20achieve%20a%20competitive%20tradeoff%20between%0Aspeed%20and%20accuracy%20on%20%28non-stationary%29%20contextual%20bandit%20problems%20and%20Bayesian%0Aoptimization%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Generalized%2520Bayesian%2520Online%2520Neural%2520Network%2520Training%2520for%250A%2520%2520Sequential%2520Decision%2520Making%26entry.906535625%3DGerardo%2520Duran-Martin%2520and%2520Leandro%2520S%25C3%25A1nchez-Betancourt%2520and%2520%25C3%2581lvaro%2520Cartea%2520and%2520Kevin%2520Murphy%26entry.1292438233%3D%2520%2520We%2520introduce%2520scalable%2520algorithms%2520for%2520online%2520learning%2520and%2520generalized%2520Bayesian%250Ainference%2520of%2520neural%2520network%2520parameters%252C%2520designed%2520for%2520sequential%2520decision%2520making%250Atasks.%2520Our%2520methods%2520combine%2520the%2520strengths%2520of%2520frequentist%2520and%2520Bayesian%2520filtering%252C%250Awhich%2520include%2520fast%2520low-rank%2520updates%2520via%2520a%2520block-diagonal%2520approximation%2520of%2520the%250Aparameter%2520error%2520covariance%252C%2520and%2520a%2520well-defined%2520posterior%2520predictive%250Adistribution%2520that%2520we%2520use%2520for%2520decision%2520making.%2520More%2520precisely%252C%2520our%2520main%2520method%250Aupdates%2520a%2520low-rank%2520error%2520covariance%2520for%2520the%2520hidden%2520layers%2520parameters%252C%2520and%2520a%250Afull-rank%2520error%2520covariance%2520for%2520the%2520final%2520layer%2520parameters.%2520Although%2520this%250Acharacterizes%2520an%2520improper%2520posterior%252C%2520we%2520show%2520that%2520the%2520resulting%2520posterior%250Apredictive%2520distribution%2520is%2520well-defined.%2520Our%2520methods%2520update%2520all%2520network%250Aparameters%2520online%252C%2520with%2520no%2520need%2520for%2520replay%2520buffers%2520or%2520offline%2520retraining.%2520We%250Ashow%252C%2520empirically%252C%2520that%2520our%2520methods%2520achieve%2520a%2520competitive%2520tradeoff%2520between%250Aspeed%2520and%2520accuracy%2520on%2520%2528non-stationary%2529%2520contextual%2520bandit%2520problems%2520and%2520Bayesian%250Aoptimization%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Generalized%20Bayesian%20Online%20Neural%20Network%20Training%20for%0A%20%20Sequential%20Decision%20Making&entry.906535625=Gerardo%20Duran-Martin%20and%20Leandro%20S%C3%A1nchez-Betancourt%20and%20%C3%81lvaro%20Cartea%20and%20Kevin%20Murphy&entry.1292438233=%20%20We%20introduce%20scalable%20algorithms%20for%20online%20learning%20and%20generalized%20Bayesian%0Ainference%20of%20neural%20network%20parameters%2C%20designed%20for%20sequential%20decision%20making%0Atasks.%20Our%20methods%20combine%20the%20strengths%20of%20frequentist%20and%20Bayesian%20filtering%2C%0Awhich%20include%20fast%20low-rank%20updates%20via%20a%20block-diagonal%20approximation%20of%20the%0Aparameter%20error%20covariance%2C%20and%20a%20well-defined%20posterior%20predictive%0Adistribution%20that%20we%20use%20for%20decision%20making.%20More%20precisely%2C%20our%20main%20method%0Aupdates%20a%20low-rank%20error%20covariance%20for%20the%20hidden%20layers%20parameters%2C%20and%20a%0Afull-rank%20error%20covariance%20for%20the%20final%20layer%20parameters.%20Although%20this%0Acharacterizes%20an%20improper%20posterior%2C%20we%20show%20that%20the%20resulting%20posterior%0Apredictive%20distribution%20is%20well-defined.%20Our%20methods%20update%20all%20network%0Aparameters%20online%2C%20with%20no%20need%20for%20replay%20buffers%20or%20offline%20retraining.%20We%0Ashow%2C%20empirically%2C%20that%20our%20methods%20achieve%20a%20competitive%20tradeoff%20between%0Aspeed%20and%20accuracy%20on%20%28non-stationary%29%20contextual%20bandit%20problems%20and%20Bayesian%0Aoptimization%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11898v1&entry.124074799=Read"},
{"title": "Framework of a multiscale data-driven digital twin of the\n  muscle-skeletal system", "author": "Martina Paccini and Simone Cammarasana and Giuseppe Patan\u00e8", "abstract": "  Musculoskeletal disorders (MSDs) are a leading cause of disability worldwide,\nrequiring advanced diagnostic and therapeutic tools for personalised assessment\nand treatment. Effective management of MSDs involves the interaction of\nheterogeneous data sources, making the Digital Twin (DT) paradigm a valuable\noption. This paper introduces the Musculoskeletal Digital Twin (MS-DT), a novel\nframework that integrates multiscale biomechanical data with computational\nmodelling to create a detailed, patient-specific representation of the\nmusculoskeletal system. By combining motion capture, ultrasound imaging,\nelectromyography, and medical imaging, the MS-DT enables the analysis of spinal\nkinematics, posture, and muscle function. An interactive visualisation platform\nprovides clinicians and researchers with an intuitive interface for exploring\nbiomechanical parameters and tracking patient-specific changes. Results\ndemonstrate the effectiveness of MS-DT in extracting precise kinematic and\ndynamic tissue features, offering a comprehensive tool for monitoring spine\nbiomechanics and rehabilitation. This framework provides high-fidelity\nmodelling and real-time visualization to improve patient-specific diagnosis and\nintervention planning.\n", "link": "http://arxiv.org/abs/2506.11821v1", "date": "2025-06-13", "relevancy": 2.1113, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5415}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5235}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Framework%20of%20a%20multiscale%20data-driven%20digital%20twin%20of%20the%0A%20%20muscle-skeletal%20system&body=Title%3A%20Framework%20of%20a%20multiscale%20data-driven%20digital%20twin%20of%20the%0A%20%20muscle-skeletal%20system%0AAuthor%3A%20Martina%20Paccini%20and%20Simone%20Cammarasana%20and%20Giuseppe%20Patan%C3%A8%0AAbstract%3A%20%20%20Musculoskeletal%20disorders%20%28MSDs%29%20are%20a%20leading%20cause%20of%20disability%20worldwide%2C%0Arequiring%20advanced%20diagnostic%20and%20therapeutic%20tools%20for%20personalised%20assessment%0Aand%20treatment.%20Effective%20management%20of%20MSDs%20involves%20the%20interaction%20of%0Aheterogeneous%20data%20sources%2C%20making%20the%20Digital%20Twin%20%28DT%29%20paradigm%20a%20valuable%0Aoption.%20This%20paper%20introduces%20the%20Musculoskeletal%20Digital%20Twin%20%28MS-DT%29%2C%20a%20novel%0Aframework%20that%20integrates%20multiscale%20biomechanical%20data%20with%20computational%0Amodelling%20to%20create%20a%20detailed%2C%20patient-specific%20representation%20of%20the%0Amusculoskeletal%20system.%20By%20combining%20motion%20capture%2C%20ultrasound%20imaging%2C%0Aelectromyography%2C%20and%20medical%20imaging%2C%20the%20MS-DT%20enables%20the%20analysis%20of%20spinal%0Akinematics%2C%20posture%2C%20and%20muscle%20function.%20An%20interactive%20visualisation%20platform%0Aprovides%20clinicians%20and%20researchers%20with%20an%20intuitive%20interface%20for%20exploring%0Abiomechanical%20parameters%20and%20tracking%20patient-specific%20changes.%20Results%0Ademonstrate%20the%20effectiveness%20of%20MS-DT%20in%20extracting%20precise%20kinematic%20and%0Adynamic%20tissue%20features%2C%20offering%20a%20comprehensive%20tool%20for%20monitoring%20spine%0Abiomechanics%20and%20rehabilitation.%20This%20framework%20provides%20high-fidelity%0Amodelling%20and%20real-time%20visualization%20to%20improve%20patient-specific%20diagnosis%20and%0Aintervention%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFramework%2520of%2520a%2520multiscale%2520data-driven%2520digital%2520twin%2520of%2520the%250A%2520%2520muscle-skeletal%2520system%26entry.906535625%3DMartina%2520Paccini%2520and%2520Simone%2520Cammarasana%2520and%2520Giuseppe%2520Patan%25C3%25A8%26entry.1292438233%3D%2520%2520Musculoskeletal%2520disorders%2520%2528MSDs%2529%2520are%2520a%2520leading%2520cause%2520of%2520disability%2520worldwide%252C%250Arequiring%2520advanced%2520diagnostic%2520and%2520therapeutic%2520tools%2520for%2520personalised%2520assessment%250Aand%2520treatment.%2520Effective%2520management%2520of%2520MSDs%2520involves%2520the%2520interaction%2520of%250Aheterogeneous%2520data%2520sources%252C%2520making%2520the%2520Digital%2520Twin%2520%2528DT%2529%2520paradigm%2520a%2520valuable%250Aoption.%2520This%2520paper%2520introduces%2520the%2520Musculoskeletal%2520Digital%2520Twin%2520%2528MS-DT%2529%252C%2520a%2520novel%250Aframework%2520that%2520integrates%2520multiscale%2520biomechanical%2520data%2520with%2520computational%250Amodelling%2520to%2520create%2520a%2520detailed%252C%2520patient-specific%2520representation%2520of%2520the%250Amusculoskeletal%2520system.%2520By%2520combining%2520motion%2520capture%252C%2520ultrasound%2520imaging%252C%250Aelectromyography%252C%2520and%2520medical%2520imaging%252C%2520the%2520MS-DT%2520enables%2520the%2520analysis%2520of%2520spinal%250Akinematics%252C%2520posture%252C%2520and%2520muscle%2520function.%2520An%2520interactive%2520visualisation%2520platform%250Aprovides%2520clinicians%2520and%2520researchers%2520with%2520an%2520intuitive%2520interface%2520for%2520exploring%250Abiomechanical%2520parameters%2520and%2520tracking%2520patient-specific%2520changes.%2520Results%250Ademonstrate%2520the%2520effectiveness%2520of%2520MS-DT%2520in%2520extracting%2520precise%2520kinematic%2520and%250Adynamic%2520tissue%2520features%252C%2520offering%2520a%2520comprehensive%2520tool%2520for%2520monitoring%2520spine%250Abiomechanics%2520and%2520rehabilitation.%2520This%2520framework%2520provides%2520high-fidelity%250Amodelling%2520and%2520real-time%2520visualization%2520to%2520improve%2520patient-specific%2520diagnosis%2520and%250Aintervention%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Framework%20of%20a%20multiscale%20data-driven%20digital%20twin%20of%20the%0A%20%20muscle-skeletal%20system&entry.906535625=Martina%20Paccini%20and%20Simone%20Cammarasana%20and%20Giuseppe%20Patan%C3%A8&entry.1292438233=%20%20Musculoskeletal%20disorders%20%28MSDs%29%20are%20a%20leading%20cause%20of%20disability%20worldwide%2C%0Arequiring%20advanced%20diagnostic%20and%20therapeutic%20tools%20for%20personalised%20assessment%0Aand%20treatment.%20Effective%20management%20of%20MSDs%20involves%20the%20interaction%20of%0Aheterogeneous%20data%20sources%2C%20making%20the%20Digital%20Twin%20%28DT%29%20paradigm%20a%20valuable%0Aoption.%20This%20paper%20introduces%20the%20Musculoskeletal%20Digital%20Twin%20%28MS-DT%29%2C%20a%20novel%0Aframework%20that%20integrates%20multiscale%20biomechanical%20data%20with%20computational%0Amodelling%20to%20create%20a%20detailed%2C%20patient-specific%20representation%20of%20the%0Amusculoskeletal%20system.%20By%20combining%20motion%20capture%2C%20ultrasound%20imaging%2C%0Aelectromyography%2C%20and%20medical%20imaging%2C%20the%20MS-DT%20enables%20the%20analysis%20of%20spinal%0Akinematics%2C%20posture%2C%20and%20muscle%20function.%20An%20interactive%20visualisation%20platform%0Aprovides%20clinicians%20and%20researchers%20with%20an%20intuitive%20interface%20for%20exploring%0Abiomechanical%20parameters%20and%20tracking%20patient-specific%20changes.%20Results%0Ademonstrate%20the%20effectiveness%20of%20MS-DT%20in%20extracting%20precise%20kinematic%20and%0Adynamic%20tissue%20features%2C%20offering%20a%20comprehensive%20tool%20for%20monitoring%20spine%0Abiomechanics%20and%20rehabilitation.%20This%20framework%20provides%20high-fidelity%0Amodelling%20and%20real-time%20visualization%20to%20improve%20patient-specific%20diagnosis%20and%0Aintervention%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11821v1&entry.124074799=Read"},
{"title": "Mixup Regularization: A Probabilistic Perspective", "author": "Yousef El-Laham and Niccol\u00f2 Dalmasso and Svitlana Vyetrenko and Vamsi K. Potluru and Manuela Veloso", "abstract": "  In recent years, mixup regularization has gained popularity as an effective\nway to improve the generalization performance of deep learning models by\ntraining on convex combinations of training data. While many mixup variants\nhave been explored, the proper adoption of the technique to conditional density\nestimation and probabilistic machine learning remains relatively unexplored.\nThis work introduces a novel framework for mixup regularization based on\nprobabilistic fusion that is better suited for conditional density estimation\ntasks. For data distributed according to a member of the exponential family, we\nshow that likelihood functions can be analytically fused using log-linear\npooling. We further propose an extension of probabilistic mixup, which allows\nfor fusion of inputs at an arbitrary intermediate layer of the neural network.\nWe provide a theoretical analysis comparing our approach to standard mixup\nvariants. Empirical results on synthetic and real datasets demonstrate the\nbenefits of our proposed framework compared to existing mixup variants.\n", "link": "http://arxiv.org/abs/2502.13825v2", "date": "2025-06-13", "relevancy": 2.1033, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5412}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.541}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixup%20Regularization%3A%20A%20Probabilistic%20Perspective&body=Title%3A%20Mixup%20Regularization%3A%20A%20Probabilistic%20Perspective%0AAuthor%3A%20Yousef%20El-Laham%20and%20Niccol%C3%B2%20Dalmasso%20and%20Svitlana%20Vyetrenko%20and%20Vamsi%20K.%20Potluru%20and%20Manuela%20Veloso%0AAbstract%3A%20%20%20In%20recent%20years%2C%20mixup%20regularization%20has%20gained%20popularity%20as%20an%20effective%0Away%20to%20improve%20the%20generalization%20performance%20of%20deep%20learning%20models%20by%0Atraining%20on%20convex%20combinations%20of%20training%20data.%20While%20many%20mixup%20variants%0Ahave%20been%20explored%2C%20the%20proper%20adoption%20of%20the%20technique%20to%20conditional%20density%0Aestimation%20and%20probabilistic%20machine%20learning%20remains%20relatively%20unexplored.%0AThis%20work%20introduces%20a%20novel%20framework%20for%20mixup%20regularization%20based%20on%0Aprobabilistic%20fusion%20that%20is%20better%20suited%20for%20conditional%20density%20estimation%0Atasks.%20For%20data%20distributed%20according%20to%20a%20member%20of%20the%20exponential%20family%2C%20we%0Ashow%20that%20likelihood%20functions%20can%20be%20analytically%20fused%20using%20log-linear%0Apooling.%20We%20further%20propose%20an%20extension%20of%20probabilistic%20mixup%2C%20which%20allows%0Afor%20fusion%20of%20inputs%20at%20an%20arbitrary%20intermediate%20layer%20of%20the%20neural%20network.%0AWe%20provide%20a%20theoretical%20analysis%20comparing%20our%20approach%20to%20standard%20mixup%0Avariants.%20Empirical%20results%20on%20synthetic%20and%20real%20datasets%20demonstrate%20the%0Abenefits%20of%20our%20proposed%20framework%20compared%20to%20existing%20mixup%20variants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13825v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixup%2520Regularization%253A%2520A%2520Probabilistic%2520Perspective%26entry.906535625%3DYousef%2520El-Laham%2520and%2520Niccol%25C3%25B2%2520Dalmasso%2520and%2520Svitlana%2520Vyetrenko%2520and%2520Vamsi%2520K.%2520Potluru%2520and%2520Manuela%2520Veloso%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520mixup%2520regularization%2520has%2520gained%2520popularity%2520as%2520an%2520effective%250Away%2520to%2520improve%2520the%2520generalization%2520performance%2520of%2520deep%2520learning%2520models%2520by%250Atraining%2520on%2520convex%2520combinations%2520of%2520training%2520data.%2520While%2520many%2520mixup%2520variants%250Ahave%2520been%2520explored%252C%2520the%2520proper%2520adoption%2520of%2520the%2520technique%2520to%2520conditional%2520density%250Aestimation%2520and%2520probabilistic%2520machine%2520learning%2520remains%2520relatively%2520unexplored.%250AThis%2520work%2520introduces%2520a%2520novel%2520framework%2520for%2520mixup%2520regularization%2520based%2520on%250Aprobabilistic%2520fusion%2520that%2520is%2520better%2520suited%2520for%2520conditional%2520density%2520estimation%250Atasks.%2520For%2520data%2520distributed%2520according%2520to%2520a%2520member%2520of%2520the%2520exponential%2520family%252C%2520we%250Ashow%2520that%2520likelihood%2520functions%2520can%2520be%2520analytically%2520fused%2520using%2520log-linear%250Apooling.%2520We%2520further%2520propose%2520an%2520extension%2520of%2520probabilistic%2520mixup%252C%2520which%2520allows%250Afor%2520fusion%2520of%2520inputs%2520at%2520an%2520arbitrary%2520intermediate%2520layer%2520of%2520the%2520neural%2520network.%250AWe%2520provide%2520a%2520theoretical%2520analysis%2520comparing%2520our%2520approach%2520to%2520standard%2520mixup%250Avariants.%2520Empirical%2520results%2520on%2520synthetic%2520and%2520real%2520datasets%2520demonstrate%2520the%250Abenefits%2520of%2520our%2520proposed%2520framework%2520compared%2520to%2520existing%2520mixup%2520variants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13825v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixup%20Regularization%3A%20A%20Probabilistic%20Perspective&entry.906535625=Yousef%20El-Laham%20and%20Niccol%C3%B2%20Dalmasso%20and%20Svitlana%20Vyetrenko%20and%20Vamsi%20K.%20Potluru%20and%20Manuela%20Veloso&entry.1292438233=%20%20In%20recent%20years%2C%20mixup%20regularization%20has%20gained%20popularity%20as%20an%20effective%0Away%20to%20improve%20the%20generalization%20performance%20of%20deep%20learning%20models%20by%0Atraining%20on%20convex%20combinations%20of%20training%20data.%20While%20many%20mixup%20variants%0Ahave%20been%20explored%2C%20the%20proper%20adoption%20of%20the%20technique%20to%20conditional%20density%0Aestimation%20and%20probabilistic%20machine%20learning%20remains%20relatively%20unexplored.%0AThis%20work%20introduces%20a%20novel%20framework%20for%20mixup%20regularization%20based%20on%0Aprobabilistic%20fusion%20that%20is%20better%20suited%20for%20conditional%20density%20estimation%0Atasks.%20For%20data%20distributed%20according%20to%20a%20member%20of%20the%20exponential%20family%2C%20we%0Ashow%20that%20likelihood%20functions%20can%20be%20analytically%20fused%20using%20log-linear%0Apooling.%20We%20further%20propose%20an%20extension%20of%20probabilistic%20mixup%2C%20which%20allows%0Afor%20fusion%20of%20inputs%20at%20an%20arbitrary%20intermediate%20layer%20of%20the%20neural%20network.%0AWe%20provide%20a%20theoretical%20analysis%20comparing%20our%20approach%20to%20standard%20mixup%0Avariants.%20Empirical%20results%20on%20synthetic%20and%20real%20datasets%20demonstrate%20the%0Abenefits%20of%20our%20proposed%20framework%20compared%20to%20existing%20mixup%20variants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13825v2&entry.124074799=Read"},
{"title": "pLSTM: parallelizable Linear Source Transition Mark networks", "author": "Korbinian P\u00f6ppel and Richard Freinschlag and Thomas Schmied and Wei Lin and Sepp Hochreiter", "abstract": "  Modern recurrent architectures, such as xLSTM and Mamba, have recently\nchallenged the Transformer in language modeling. However, their structure\nconstrains their applicability to sequences only or requires processing\nmulti-dimensional data structures, such as images or molecular graphs, in a\npre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are\nwell suited for data with a higher level structure, like 2D grids, trees, and\ndirected acyclic graphs (DAGs). In this work, we extend the notion of\nmulti-dimensionality to linear RNNs. We introduce parallelizable Linear Source\nTransition Mark networks (pLSTMs) using Source, Transition, and Mark gates that\nact on the line graph of a general DAG. This enables parallelization in analogy\nto parallel associative scans and the chunkwise-recurrent form of sequential\nlinear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this\nscheme can be efficiently implemented using einsum operations, concatenations,\nand padding in logarithmic time. pLSTMs tackle the vanishing/exploding\nactivation/gradient problem for long distances in DAGs via two distinct modes:\na directed propagation mode (P-mode) and a diffusive distribution mode\n(D-mode). To showcase the long-range capabilities of pLSTM, we introduce\narrow-pointing extrapolation as a synthetic computer vision task that contains\nlong-distance directional information. We demonstrate that pLSTMs generalize\nwell to larger image sizes, whereas Transformers struggle to extrapolate. On\nestablished molecular graph and computer vision benchmarks, pLSTMs also show\nstrong performance. Code and Datasets are available at:\nhttps://github.com/ml-jku/plstm_experiments.\n", "link": "http://arxiv.org/abs/2506.11997v1", "date": "2025-06-13", "relevancy": 2.0854, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5333}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5294}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20pLSTM%3A%20parallelizable%20Linear%20Source%20Transition%20Mark%20networks&body=Title%3A%20pLSTM%3A%20parallelizable%20Linear%20Source%20Transition%20Mark%20networks%0AAuthor%3A%20Korbinian%20P%C3%B6ppel%20and%20Richard%20Freinschlag%20and%20Thomas%20Schmied%20and%20Wei%20Lin%20and%20Sepp%20Hochreiter%0AAbstract%3A%20%20%20Modern%20recurrent%20architectures%2C%20such%20as%20xLSTM%20and%20Mamba%2C%20have%20recently%0Achallenged%20the%20Transformer%20in%20language%20modeling.%20However%2C%20their%20structure%0Aconstrains%20their%20applicability%20to%20sequences%20only%20or%20requires%20processing%0Amulti-dimensional%20data%20structures%2C%20such%20as%20images%20or%20molecular%20graphs%2C%20in%20a%0Apre-defined%20sequential%20order.%20In%20contrast%2C%20Multi-Dimensional%20RNNs%20%28MDRNNs%29%20are%0Awell%20suited%20for%20data%20with%20a%20higher%20level%20structure%2C%20like%202D%20grids%2C%20trees%2C%20and%0Adirected%20acyclic%20graphs%20%28DAGs%29.%20In%20this%20work%2C%20we%20extend%20the%20notion%20of%0Amulti-dimensionality%20to%20linear%20RNNs.%20We%20introduce%20parallelizable%20Linear%20Source%0ATransition%20Mark%20networks%20%28pLSTMs%29%20using%20Source%2C%20Transition%2C%20and%20Mark%20gates%20that%0Aact%20on%20the%20line%20graph%20of%20a%20general%20DAG.%20This%20enables%20parallelization%20in%20analogy%0Ato%20parallel%20associative%20scans%20and%20the%20chunkwise-recurrent%20form%20of%20sequential%0Alinear%20RNNs%2C%20but%20for%20DAGs.%20For%20regular%20grids%20%281D%20and%202D%29%2C%20like%20images%2C%20this%0Ascheme%20can%20be%20efficiently%20implemented%20using%20einsum%20operations%2C%20concatenations%2C%0Aand%20padding%20in%20logarithmic%20time.%20pLSTMs%20tackle%20the%20vanishing/exploding%0Aactivation/gradient%20problem%20for%20long%20distances%20in%20DAGs%20via%20two%20distinct%20modes%3A%0Aa%20directed%20propagation%20mode%20%28P-mode%29%20and%20a%20diffusive%20distribution%20mode%0A%28D-mode%29.%20To%20showcase%20the%20long-range%20capabilities%20of%20pLSTM%2C%20we%20introduce%0Aarrow-pointing%20extrapolation%20as%20a%20synthetic%20computer%20vision%20task%20that%20contains%0Along-distance%20directional%20information.%20We%20demonstrate%20that%20pLSTMs%20generalize%0Awell%20to%20larger%20image%20sizes%2C%20whereas%20Transformers%20struggle%20to%20extrapolate.%20On%0Aestablished%20molecular%20graph%20and%20computer%20vision%20benchmarks%2C%20pLSTMs%20also%20show%0Astrong%20performance.%20Code%20and%20Datasets%20are%20available%20at%3A%0Ahttps%3A//github.com/ml-jku/plstm_experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DpLSTM%253A%2520parallelizable%2520Linear%2520Source%2520Transition%2520Mark%2520networks%26entry.906535625%3DKorbinian%2520P%25C3%25B6ppel%2520and%2520Richard%2520Freinschlag%2520and%2520Thomas%2520Schmied%2520and%2520Wei%2520Lin%2520and%2520Sepp%2520Hochreiter%26entry.1292438233%3D%2520%2520Modern%2520recurrent%2520architectures%252C%2520such%2520as%2520xLSTM%2520and%2520Mamba%252C%2520have%2520recently%250Achallenged%2520the%2520Transformer%2520in%2520language%2520modeling.%2520However%252C%2520their%2520structure%250Aconstrains%2520their%2520applicability%2520to%2520sequences%2520only%2520or%2520requires%2520processing%250Amulti-dimensional%2520data%2520structures%252C%2520such%2520as%2520images%2520or%2520molecular%2520graphs%252C%2520in%2520a%250Apre-defined%2520sequential%2520order.%2520In%2520contrast%252C%2520Multi-Dimensional%2520RNNs%2520%2528MDRNNs%2529%2520are%250Awell%2520suited%2520for%2520data%2520with%2520a%2520higher%2520level%2520structure%252C%2520like%25202D%2520grids%252C%2520trees%252C%2520and%250Adirected%2520acyclic%2520graphs%2520%2528DAGs%2529.%2520In%2520this%2520work%252C%2520we%2520extend%2520the%2520notion%2520of%250Amulti-dimensionality%2520to%2520linear%2520RNNs.%2520We%2520introduce%2520parallelizable%2520Linear%2520Source%250ATransition%2520Mark%2520networks%2520%2528pLSTMs%2529%2520using%2520Source%252C%2520Transition%252C%2520and%2520Mark%2520gates%2520that%250Aact%2520on%2520the%2520line%2520graph%2520of%2520a%2520general%2520DAG.%2520This%2520enables%2520parallelization%2520in%2520analogy%250Ato%2520parallel%2520associative%2520scans%2520and%2520the%2520chunkwise-recurrent%2520form%2520of%2520sequential%250Alinear%2520RNNs%252C%2520but%2520for%2520DAGs.%2520For%2520regular%2520grids%2520%25281D%2520and%25202D%2529%252C%2520like%2520images%252C%2520this%250Ascheme%2520can%2520be%2520efficiently%2520implemented%2520using%2520einsum%2520operations%252C%2520concatenations%252C%250Aand%2520padding%2520in%2520logarithmic%2520time.%2520pLSTMs%2520tackle%2520the%2520vanishing/exploding%250Aactivation/gradient%2520problem%2520for%2520long%2520distances%2520in%2520DAGs%2520via%2520two%2520distinct%2520modes%253A%250Aa%2520directed%2520propagation%2520mode%2520%2528P-mode%2529%2520and%2520a%2520diffusive%2520distribution%2520mode%250A%2528D-mode%2529.%2520To%2520showcase%2520the%2520long-range%2520capabilities%2520of%2520pLSTM%252C%2520we%2520introduce%250Aarrow-pointing%2520extrapolation%2520as%2520a%2520synthetic%2520computer%2520vision%2520task%2520that%2520contains%250Along-distance%2520directional%2520information.%2520We%2520demonstrate%2520that%2520pLSTMs%2520generalize%250Awell%2520to%2520larger%2520image%2520sizes%252C%2520whereas%2520Transformers%2520struggle%2520to%2520extrapolate.%2520On%250Aestablished%2520molecular%2520graph%2520and%2520computer%2520vision%2520benchmarks%252C%2520pLSTMs%2520also%2520show%250Astrong%2520performance.%2520Code%2520and%2520Datasets%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/ml-jku/plstm_experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=pLSTM%3A%20parallelizable%20Linear%20Source%20Transition%20Mark%20networks&entry.906535625=Korbinian%20P%C3%B6ppel%20and%20Richard%20Freinschlag%20and%20Thomas%20Schmied%20and%20Wei%20Lin%20and%20Sepp%20Hochreiter&entry.1292438233=%20%20Modern%20recurrent%20architectures%2C%20such%20as%20xLSTM%20and%20Mamba%2C%20have%20recently%0Achallenged%20the%20Transformer%20in%20language%20modeling.%20However%2C%20their%20structure%0Aconstrains%20their%20applicability%20to%20sequences%20only%20or%20requires%20processing%0Amulti-dimensional%20data%20structures%2C%20such%20as%20images%20or%20molecular%20graphs%2C%20in%20a%0Apre-defined%20sequential%20order.%20In%20contrast%2C%20Multi-Dimensional%20RNNs%20%28MDRNNs%29%20are%0Awell%20suited%20for%20data%20with%20a%20higher%20level%20structure%2C%20like%202D%20grids%2C%20trees%2C%20and%0Adirected%20acyclic%20graphs%20%28DAGs%29.%20In%20this%20work%2C%20we%20extend%20the%20notion%20of%0Amulti-dimensionality%20to%20linear%20RNNs.%20We%20introduce%20parallelizable%20Linear%20Source%0ATransition%20Mark%20networks%20%28pLSTMs%29%20using%20Source%2C%20Transition%2C%20and%20Mark%20gates%20that%0Aact%20on%20the%20line%20graph%20of%20a%20general%20DAG.%20This%20enables%20parallelization%20in%20analogy%0Ato%20parallel%20associative%20scans%20and%20the%20chunkwise-recurrent%20form%20of%20sequential%0Alinear%20RNNs%2C%20but%20for%20DAGs.%20For%20regular%20grids%20%281D%20and%202D%29%2C%20like%20images%2C%20this%0Ascheme%20can%20be%20efficiently%20implemented%20using%20einsum%20operations%2C%20concatenations%2C%0Aand%20padding%20in%20logarithmic%20time.%20pLSTMs%20tackle%20the%20vanishing/exploding%0Aactivation/gradient%20problem%20for%20long%20distances%20in%20DAGs%20via%20two%20distinct%20modes%3A%0Aa%20directed%20propagation%20mode%20%28P-mode%29%20and%20a%20diffusive%20distribution%20mode%0A%28D-mode%29.%20To%20showcase%20the%20long-range%20capabilities%20of%20pLSTM%2C%20we%20introduce%0Aarrow-pointing%20extrapolation%20as%20a%20synthetic%20computer%20vision%20task%20that%20contains%0Along-distance%20directional%20information.%20We%20demonstrate%20that%20pLSTMs%20generalize%0Awell%20to%20larger%20image%20sizes%2C%20whereas%20Transformers%20struggle%20to%20extrapolate.%20On%0Aestablished%20molecular%20graph%20and%20computer%20vision%20benchmarks%2C%20pLSTMs%20also%20show%0Astrong%20performance.%20Code%20and%20Datasets%20are%20available%20at%3A%0Ahttps%3A//github.com/ml-jku/plstm_experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11997v1&entry.124074799=Read"},
{"title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction", "author": "Hsi-Che Lin and Yu-Chu Yu and Kai-Po Chang and Yu-Chiang Frank Wang", "abstract": "  Open-source foundation models have seen rapid adoption and development,\nenabling powerful general-purpose capabilities across diverse domains. However,\nfine-tuning large foundation models for domain-specific or personalized tasks\nremains prohibitively expensive for most users due to the significant memory\noverhead beyond that of inference. We introduce EMLoC, an Emulator-based\nMemory-efficient fine-tuning framework with LoRA Correction, which enables\nmodel fine-tuning within the same memory budget required for inference. EMLoC\nconstructs a task-specific light-weight emulator using activation-aware\nsingular value decomposition (SVD) on a small downstream calibration set.\nFine-tuning then is performed on this lightweight emulator via LoRA. To tackle\nthe misalignment between the original model and the compressed emulator, we\npropose a novel compensation algorithm to correct the fine-tuned LoRA module,\nwhich thus can be merged into the original model for inference. EMLoC supports\nflexible compression ratios and standard training pipelines, making it\nadaptable to a wide range of applications. Extensive experiments demonstrate\nthat EMLoC outperforms other baselines across multiple datasets and modalities.\nMoreover, without quantization, EMLoC enables fine-tuning of a 38B model on a\nsingle 24GB consumer GPU-bringing efficient and practical model adaptation to\nindividual users.\n", "link": "http://arxiv.org/abs/2506.12015v1", "date": "2025-06-13", "relevancy": 2.0817, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMLoC%3A%20Emulator-based%20Memory-efficient%20Fine-tuning%20with%20LoRA%20Correction&body=Title%3A%20EMLoC%3A%20Emulator-based%20Memory-efficient%20Fine-tuning%20with%20LoRA%20Correction%0AAuthor%3A%20Hsi-Che%20Lin%20and%20Yu-Chu%20Yu%20and%20Kai-Po%20Chang%20and%20Yu-Chiang%20Frank%20Wang%0AAbstract%3A%20%20%20Open-source%20foundation%20models%20have%20seen%20rapid%20adoption%20and%20development%2C%0Aenabling%20powerful%20general-purpose%20capabilities%20across%20diverse%20domains.%20However%2C%0Afine-tuning%20large%20foundation%20models%20for%20domain-specific%20or%20personalized%20tasks%0Aremains%20prohibitively%20expensive%20for%20most%20users%20due%20to%20the%20significant%20memory%0Aoverhead%20beyond%20that%20of%20inference.%20We%20introduce%20EMLoC%2C%20an%20Emulator-based%0AMemory-efficient%20fine-tuning%20framework%20with%20LoRA%20Correction%2C%20which%20enables%0Amodel%20fine-tuning%20within%20the%20same%20memory%20budget%20required%20for%20inference.%20EMLoC%0Aconstructs%20a%20task-specific%20light-weight%20emulator%20using%20activation-aware%0Asingular%20value%20decomposition%20%28SVD%29%20on%20a%20small%20downstream%20calibration%20set.%0AFine-tuning%20then%20is%20performed%20on%20this%20lightweight%20emulator%20via%20LoRA.%20To%20tackle%0Athe%20misalignment%20between%20the%20original%20model%20and%20the%20compressed%20emulator%2C%20we%0Apropose%20a%20novel%20compensation%20algorithm%20to%20correct%20the%20fine-tuned%20LoRA%20module%2C%0Awhich%20thus%20can%20be%20merged%20into%20the%20original%20model%20for%20inference.%20EMLoC%20supports%0Aflexible%20compression%20ratios%20and%20standard%20training%20pipelines%2C%20making%20it%0Aadaptable%20to%20a%20wide%20range%20of%20applications.%20Extensive%20experiments%20demonstrate%0Athat%20EMLoC%20outperforms%20other%20baselines%20across%20multiple%20datasets%20and%20modalities.%0AMoreover%2C%20without%20quantization%2C%20EMLoC%20enables%20fine-tuning%20of%20a%2038B%20model%20on%20a%0Asingle%2024GB%20consumer%20GPU-bringing%20efficient%20and%20practical%20model%20adaptation%20to%0Aindividual%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMLoC%253A%2520Emulator-based%2520Memory-efficient%2520Fine-tuning%2520with%2520LoRA%2520Correction%26entry.906535625%3DHsi-Che%2520Lin%2520and%2520Yu-Chu%2520Yu%2520and%2520Kai-Po%2520Chang%2520and%2520Yu-Chiang%2520Frank%2520Wang%26entry.1292438233%3D%2520%2520Open-source%2520foundation%2520models%2520have%2520seen%2520rapid%2520adoption%2520and%2520development%252C%250Aenabling%2520powerful%2520general-purpose%2520capabilities%2520across%2520diverse%2520domains.%2520However%252C%250Afine-tuning%2520large%2520foundation%2520models%2520for%2520domain-specific%2520or%2520personalized%2520tasks%250Aremains%2520prohibitively%2520expensive%2520for%2520most%2520users%2520due%2520to%2520the%2520significant%2520memory%250Aoverhead%2520beyond%2520that%2520of%2520inference.%2520We%2520introduce%2520EMLoC%252C%2520an%2520Emulator-based%250AMemory-efficient%2520fine-tuning%2520framework%2520with%2520LoRA%2520Correction%252C%2520which%2520enables%250Amodel%2520fine-tuning%2520within%2520the%2520same%2520memory%2520budget%2520required%2520for%2520inference.%2520EMLoC%250Aconstructs%2520a%2520task-specific%2520light-weight%2520emulator%2520using%2520activation-aware%250Asingular%2520value%2520decomposition%2520%2528SVD%2529%2520on%2520a%2520small%2520downstream%2520calibration%2520set.%250AFine-tuning%2520then%2520is%2520performed%2520on%2520this%2520lightweight%2520emulator%2520via%2520LoRA.%2520To%2520tackle%250Athe%2520misalignment%2520between%2520the%2520original%2520model%2520and%2520the%2520compressed%2520emulator%252C%2520we%250Apropose%2520a%2520novel%2520compensation%2520algorithm%2520to%2520correct%2520the%2520fine-tuned%2520LoRA%2520module%252C%250Awhich%2520thus%2520can%2520be%2520merged%2520into%2520the%2520original%2520model%2520for%2520inference.%2520EMLoC%2520supports%250Aflexible%2520compression%2520ratios%2520and%2520standard%2520training%2520pipelines%252C%2520making%2520it%250Aadaptable%2520to%2520a%2520wide%2520range%2520of%2520applications.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520EMLoC%2520outperforms%2520other%2520baselines%2520across%2520multiple%2520datasets%2520and%2520modalities.%250AMoreover%252C%2520without%2520quantization%252C%2520EMLoC%2520enables%2520fine-tuning%2520of%2520a%252038B%2520model%2520on%2520a%250Asingle%252024GB%2520consumer%2520GPU-bringing%2520efficient%2520and%2520practical%2520model%2520adaptation%2520to%250Aindividual%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMLoC%3A%20Emulator-based%20Memory-efficient%20Fine-tuning%20with%20LoRA%20Correction&entry.906535625=Hsi-Che%20Lin%20and%20Yu-Chu%20Yu%20and%20Kai-Po%20Chang%20and%20Yu-Chiang%20Frank%20Wang&entry.1292438233=%20%20Open-source%20foundation%20models%20have%20seen%20rapid%20adoption%20and%20development%2C%0Aenabling%20powerful%20general-purpose%20capabilities%20across%20diverse%20domains.%20However%2C%0Afine-tuning%20large%20foundation%20models%20for%20domain-specific%20or%20personalized%20tasks%0Aremains%20prohibitively%20expensive%20for%20most%20users%20due%20to%20the%20significant%20memory%0Aoverhead%20beyond%20that%20of%20inference.%20We%20introduce%20EMLoC%2C%20an%20Emulator-based%0AMemory-efficient%20fine-tuning%20framework%20with%20LoRA%20Correction%2C%20which%20enables%0Amodel%20fine-tuning%20within%20the%20same%20memory%20budget%20required%20for%20inference.%20EMLoC%0Aconstructs%20a%20task-specific%20light-weight%20emulator%20using%20activation-aware%0Asingular%20value%20decomposition%20%28SVD%29%20on%20a%20small%20downstream%20calibration%20set.%0AFine-tuning%20then%20is%20performed%20on%20this%20lightweight%20emulator%20via%20LoRA.%20To%20tackle%0Athe%20misalignment%20between%20the%20original%20model%20and%20the%20compressed%20emulator%2C%20we%0Apropose%20a%20novel%20compensation%20algorithm%20to%20correct%20the%20fine-tuned%20LoRA%20module%2C%0Awhich%20thus%20can%20be%20merged%20into%20the%20original%20model%20for%20inference.%20EMLoC%20supports%0Aflexible%20compression%20ratios%20and%20standard%20training%20pipelines%2C%20making%20it%0Aadaptable%20to%20a%20wide%20range%20of%20applications.%20Extensive%20experiments%20demonstrate%0Athat%20EMLoC%20outperforms%20other%20baselines%20across%20multiple%20datasets%20and%20modalities.%0AMoreover%2C%20without%20quantization%2C%20EMLoC%20enables%20fine-tuning%20of%20a%2038B%20model%20on%20a%0Asingle%2024GB%20consumer%20GPU-bringing%20efficient%20and%20practical%20model%20adaptation%20to%0Aindividual%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12015v1&entry.124074799=Read"},
{"title": "An Efficient Compression of Deep Neural Network Checkpoints Based on\n  Prediction and Context Modeling", "author": "Yuriy Kim and Evgeny Belyaev", "abstract": "  This paper is dedicated to an efficient compression of weights and optimizer\nstates (called checkpoints) obtained at different stages during a neural\nnetwork training process. First, we propose a prediction-based compression\napproach, where values from the previously saved checkpoint are used for\ncontext modeling in arithmetic coding. Second, in order to enhance the\ncompression performance, we also propose to apply pruning and quantization of\nthe checkpoint values. Experimental results show that our approach achieves\nsubstantial bit size reduction, while enabling near-lossless training recovery\nfrom restored checkpoints, preserving the model's performance and making it\nsuitable for storage-limited environments.\n", "link": "http://arxiv.org/abs/2506.12000v1", "date": "2025-06-13", "relevancy": 2.0808, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5463}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5023}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Compression%20of%20Deep%20Neural%20Network%20Checkpoints%20Based%20on%0A%20%20Prediction%20and%20Context%20Modeling&body=Title%3A%20An%20Efficient%20Compression%20of%20Deep%20Neural%20Network%20Checkpoints%20Based%20on%0A%20%20Prediction%20and%20Context%20Modeling%0AAuthor%3A%20Yuriy%20Kim%20and%20Evgeny%20Belyaev%0AAbstract%3A%20%20%20This%20paper%20is%20dedicated%20to%20an%20efficient%20compression%20of%20weights%20and%20optimizer%0Astates%20%28called%20checkpoints%29%20obtained%20at%20different%20stages%20during%20a%20neural%0Anetwork%20training%20process.%20First%2C%20we%20propose%20a%20prediction-based%20compression%0Aapproach%2C%20where%20values%20from%20the%20previously%20saved%20checkpoint%20are%20used%20for%0Acontext%20modeling%20in%20arithmetic%20coding.%20Second%2C%20in%20order%20to%20enhance%20the%0Acompression%20performance%2C%20we%20also%20propose%20to%20apply%20pruning%20and%20quantization%20of%0Athe%20checkpoint%20values.%20Experimental%20results%20show%20that%20our%20approach%20achieves%0Asubstantial%20bit%20size%20reduction%2C%20while%20enabling%20near-lossless%20training%20recovery%0Afrom%20restored%20checkpoints%2C%20preserving%20the%20model%27s%20performance%20and%20making%20it%0Asuitable%20for%20storage-limited%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Compression%2520of%2520Deep%2520Neural%2520Network%2520Checkpoints%2520Based%2520on%250A%2520%2520Prediction%2520and%2520Context%2520Modeling%26entry.906535625%3DYuriy%2520Kim%2520and%2520Evgeny%2520Belyaev%26entry.1292438233%3D%2520%2520This%2520paper%2520is%2520dedicated%2520to%2520an%2520efficient%2520compression%2520of%2520weights%2520and%2520optimizer%250Astates%2520%2528called%2520checkpoints%2529%2520obtained%2520at%2520different%2520stages%2520during%2520a%2520neural%250Anetwork%2520training%2520process.%2520First%252C%2520we%2520propose%2520a%2520prediction-based%2520compression%250Aapproach%252C%2520where%2520values%2520from%2520the%2520previously%2520saved%2520checkpoint%2520are%2520used%2520for%250Acontext%2520modeling%2520in%2520arithmetic%2520coding.%2520Second%252C%2520in%2520order%2520to%2520enhance%2520the%250Acompression%2520performance%252C%2520we%2520also%2520propose%2520to%2520apply%2520pruning%2520and%2520quantization%2520of%250Athe%2520checkpoint%2520values.%2520Experimental%2520results%2520show%2520that%2520our%2520approach%2520achieves%250Asubstantial%2520bit%2520size%2520reduction%252C%2520while%2520enabling%2520near-lossless%2520training%2520recovery%250Afrom%2520restored%2520checkpoints%252C%2520preserving%2520the%2520model%2527s%2520performance%2520and%2520making%2520it%250Asuitable%2520for%2520storage-limited%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Compression%20of%20Deep%20Neural%20Network%20Checkpoints%20Based%20on%0A%20%20Prediction%20and%20Context%20Modeling&entry.906535625=Yuriy%20Kim%20and%20Evgeny%20Belyaev&entry.1292438233=%20%20This%20paper%20is%20dedicated%20to%20an%20efficient%20compression%20of%20weights%20and%20optimizer%0Astates%20%28called%20checkpoints%29%20obtained%20at%20different%20stages%20during%20a%20neural%0Anetwork%20training%20process.%20First%2C%20we%20propose%20a%20prediction-based%20compression%0Aapproach%2C%20where%20values%20from%20the%20previously%20saved%20checkpoint%20are%20used%20for%0Acontext%20modeling%20in%20arithmetic%20coding.%20Second%2C%20in%20order%20to%20enhance%20the%0Acompression%20performance%2C%20we%20also%20propose%20to%20apply%20pruning%20and%20quantization%20of%0Athe%20checkpoint%20values.%20Experimental%20results%20show%20that%20our%20approach%20achieves%0Asubstantial%20bit%20size%20reduction%2C%20while%20enabling%20near-lossless%20training%20recovery%0Afrom%20restored%20checkpoints%2C%20preserving%20the%20model%27s%20performance%20and%20making%20it%0Asuitable%20for%20storage-limited%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12000v1&entry.124074799=Read"},
{"title": "Improving Causal Interventions in Amnesic Probing with Mean Projection\n  or LEACE", "author": "Alicja Dobrzeniecka and Antske Fokkens and Pia Sommerauer", "abstract": "  Amnesic probing is a technique used to examine the influence of specific\nlinguistic information on the behaviour of a model. This involves identifying\nand removing the relevant information and then assessing whether the model's\nperformance on the main task changes. If the removed information is relevant,\nthe model's performance should decline. The difficulty with this approach lies\nin removing only the target information while leaving other information\nunchanged. It has been shown that Iterative Nullspace Projection (INLP), a\nwidely used removal technique, introduces random modifications to\nrepresentations when eliminating target information. We demonstrate that Mean\nProjection (MP) and LEACE, two proposed alternatives, remove information in a\nmore targeted manner, thereby enhancing the potential for obtaining behavioural\nexplanations through Amnesic Probing.\n", "link": "http://arxiv.org/abs/2506.11673v1", "date": "2025-06-13", "relevancy": 2.0799, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.531}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Causal%20Interventions%20in%20Amnesic%20Probing%20with%20Mean%20Projection%0A%20%20or%20LEACE&body=Title%3A%20Improving%20Causal%20Interventions%20in%20Amnesic%20Probing%20with%20Mean%20Projection%0A%20%20or%20LEACE%0AAuthor%3A%20Alicja%20Dobrzeniecka%20and%20Antske%20Fokkens%20and%20Pia%20Sommerauer%0AAbstract%3A%20%20%20Amnesic%20probing%20is%20a%20technique%20used%20to%20examine%20the%20influence%20of%20specific%0Alinguistic%20information%20on%20the%20behaviour%20of%20a%20model.%20This%20involves%20identifying%0Aand%20removing%20the%20relevant%20information%20and%20then%20assessing%20whether%20the%20model%27s%0Aperformance%20on%20the%20main%20task%20changes.%20If%20the%20removed%20information%20is%20relevant%2C%0Athe%20model%27s%20performance%20should%20decline.%20The%20difficulty%20with%20this%20approach%20lies%0Ain%20removing%20only%20the%20target%20information%20while%20leaving%20other%20information%0Aunchanged.%20It%20has%20been%20shown%20that%20Iterative%20Nullspace%20Projection%20%28INLP%29%2C%20a%0Awidely%20used%20removal%20technique%2C%20introduces%20random%20modifications%20to%0Arepresentations%20when%20eliminating%20target%20information.%20We%20demonstrate%20that%20Mean%0AProjection%20%28MP%29%20and%20LEACE%2C%20two%20proposed%20alternatives%2C%20remove%20information%20in%20a%0Amore%20targeted%20manner%2C%20thereby%20enhancing%20the%20potential%20for%20obtaining%20behavioural%0Aexplanations%20through%20Amnesic%20Probing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Causal%2520Interventions%2520in%2520Amnesic%2520Probing%2520with%2520Mean%2520Projection%250A%2520%2520or%2520LEACE%26entry.906535625%3DAlicja%2520Dobrzeniecka%2520and%2520Antske%2520Fokkens%2520and%2520Pia%2520Sommerauer%26entry.1292438233%3D%2520%2520Amnesic%2520probing%2520is%2520a%2520technique%2520used%2520to%2520examine%2520the%2520influence%2520of%2520specific%250Alinguistic%2520information%2520on%2520the%2520behaviour%2520of%2520a%2520model.%2520This%2520involves%2520identifying%250Aand%2520removing%2520the%2520relevant%2520information%2520and%2520then%2520assessing%2520whether%2520the%2520model%2527s%250Aperformance%2520on%2520the%2520main%2520task%2520changes.%2520If%2520the%2520removed%2520information%2520is%2520relevant%252C%250Athe%2520model%2527s%2520performance%2520should%2520decline.%2520The%2520difficulty%2520with%2520this%2520approach%2520lies%250Ain%2520removing%2520only%2520the%2520target%2520information%2520while%2520leaving%2520other%2520information%250Aunchanged.%2520It%2520has%2520been%2520shown%2520that%2520Iterative%2520Nullspace%2520Projection%2520%2528INLP%2529%252C%2520a%250Awidely%2520used%2520removal%2520technique%252C%2520introduces%2520random%2520modifications%2520to%250Arepresentations%2520when%2520eliminating%2520target%2520information.%2520We%2520demonstrate%2520that%2520Mean%250AProjection%2520%2528MP%2529%2520and%2520LEACE%252C%2520two%2520proposed%2520alternatives%252C%2520remove%2520information%2520in%2520a%250Amore%2520targeted%2520manner%252C%2520thereby%2520enhancing%2520the%2520potential%2520for%2520obtaining%2520behavioural%250Aexplanations%2520through%2520Amnesic%2520Probing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Causal%20Interventions%20in%20Amnesic%20Probing%20with%20Mean%20Projection%0A%20%20or%20LEACE&entry.906535625=Alicja%20Dobrzeniecka%20and%20Antske%20Fokkens%20and%20Pia%20Sommerauer&entry.1292438233=%20%20Amnesic%20probing%20is%20a%20technique%20used%20to%20examine%20the%20influence%20of%20specific%0Alinguistic%20information%20on%20the%20behaviour%20of%20a%20model.%20This%20involves%20identifying%0Aand%20removing%20the%20relevant%20information%20and%20then%20assessing%20whether%20the%20model%27s%0Aperformance%20on%20the%20main%20task%20changes.%20If%20the%20removed%20information%20is%20relevant%2C%0Athe%20model%27s%20performance%20should%20decline.%20The%20difficulty%20with%20this%20approach%20lies%0Ain%20removing%20only%20the%20target%20information%20while%20leaving%20other%20information%0Aunchanged.%20It%20has%20been%20shown%20that%20Iterative%20Nullspace%20Projection%20%28INLP%29%2C%20a%0Awidely%20used%20removal%20technique%2C%20introduces%20random%20modifications%20to%0Arepresentations%20when%20eliminating%20target%20information.%20We%20demonstrate%20that%20Mean%0AProjection%20%28MP%29%20and%20LEACE%2C%20two%20proposed%20alternatives%2C%20remove%20information%20in%20a%0Amore%20targeted%20manner%2C%20thereby%20enhancing%20the%20potential%20for%20obtaining%20behavioural%0Aexplanations%20through%20Amnesic%20Probing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11673v1&entry.124074799=Read"},
{"title": "Tracing LLM Reasoning Processes with Strategic Games: A Framework for\n  Planning, Revision, and Resource-Constrained Decision Making", "author": "Xiaopeng Yuan and Xingjian Zhang and Ke Xu and Yifan Xu and Lijun Yu and Jindong Wang and Yushun Dong and Haohan Wang", "abstract": "  Large language models (LLMs) are increasingly used for tasks that require\ncomplex reasoning. Most benchmarks focus on final outcomes but overlook the\nintermediate reasoning steps - such as planning, revision, and decision making\nunder resource constraints. We argue that measuring these internal processes is\nessential for understanding model behavior and improving reliability. We\npropose using strategic games as a natural evaluation environment: closed,\nrule-based systems with clear states, limited resources, and automatic\nfeedback. We introduce a framework that evaluates LLMs along three core\ndimensions: planning, revision, and resource-constrained decision making. To\noperationalize this, we define metrics beyond win rate, including\novercorrection risk rate, correction success rate, improvement slope, and\nover-budget ratio. In 4320 adversarial rounds across 12 leading models,\nChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7\npercent, a correction success rate of 78.6 percent, and an improvement slope of\n0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6\npercent, wins only 25.6 percent of its matches - primarily due to excessive\nresource use. We also observe a negative correlation between overcorrection\nrisk rate and correction success rate (Pearson r = -0.51, p = 0.093),\nsuggesting that more frequent edits do not always improve outcomes. Our\nfindings highlight the value of assessing not only what LLMs decide but how\nthey arrive at those decisions\n", "link": "http://arxiv.org/abs/2506.12012v1", "date": "2025-06-13", "relevancy": 2.0626, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tracing%20LLM%20Reasoning%20Processes%20with%20Strategic%20Games%3A%20A%20Framework%20for%0A%20%20Planning%2C%20Revision%2C%20and%20Resource-Constrained%20Decision%20Making&body=Title%3A%20Tracing%20LLM%20Reasoning%20Processes%20with%20Strategic%20Games%3A%20A%20Framework%20for%0A%20%20Planning%2C%20Revision%2C%20and%20Resource-Constrained%20Decision%20Making%0AAuthor%3A%20Xiaopeng%20Yuan%20and%20Xingjian%20Zhang%20and%20Ke%20Xu%20and%20Yifan%20Xu%20and%20Lijun%20Yu%20and%20Jindong%20Wang%20and%20Yushun%20Dong%20and%20Haohan%20Wang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20for%20tasks%20that%20require%0Acomplex%20reasoning.%20Most%20benchmarks%20focus%20on%20final%20outcomes%20but%20overlook%20the%0Aintermediate%20reasoning%20steps%20-%20such%20as%20planning%2C%20revision%2C%20and%20decision%20making%0Aunder%20resource%20constraints.%20We%20argue%20that%20measuring%20these%20internal%20processes%20is%0Aessential%20for%20understanding%20model%20behavior%20and%20improving%20reliability.%20We%0Apropose%20using%20strategic%20games%20as%20a%20natural%20evaluation%20environment%3A%20closed%2C%0Arule-based%20systems%20with%20clear%20states%2C%20limited%20resources%2C%20and%20automatic%0Afeedback.%20We%20introduce%20a%20framework%20that%20evaluates%20LLMs%20along%20three%20core%0Adimensions%3A%20planning%2C%20revision%2C%20and%20resource-constrained%20decision%20making.%20To%0Aoperationalize%20this%2C%20we%20define%20metrics%20beyond%20win%20rate%2C%20including%0Aovercorrection%20risk%20rate%2C%20correction%20success%20rate%2C%20improvement%20slope%2C%20and%0Aover-budget%20ratio.%20In%204320%20adversarial%20rounds%20across%2012%20leading%20models%2C%0AChatGPT-o3-mini%20achieves%20the%20top%20composite%20score%2C%20with%20a%20win%20rate%20of%2074.7%0Apercent%2C%20a%20correction%20success%20rate%20of%2078.6%20percent%2C%20and%20an%20improvement%20slope%20of%0A0.041.%20By%20contrast%2C%20Qwen-Plus%2C%20despite%20an%20overcorrection%20risk%20rate%20of%2081.6%0Apercent%2C%20wins%20only%2025.6%20percent%20of%20its%20matches%20-%20primarily%20due%20to%20excessive%0Aresource%20use.%20We%20also%20observe%20a%20negative%20correlation%20between%20overcorrection%0Arisk%20rate%20and%20correction%20success%20rate%20%28Pearson%20r%20%3D%20-0.51%2C%20p%20%3D%200.093%29%2C%0Asuggesting%20that%20more%20frequent%20edits%20do%20not%20always%20improve%20outcomes.%20Our%0Afindings%20highlight%20the%20value%20of%20assessing%20not%20only%20what%20LLMs%20decide%20but%20how%0Athey%20arrive%20at%20those%20decisions%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTracing%2520LLM%2520Reasoning%2520Processes%2520with%2520Strategic%2520Games%253A%2520A%2520Framework%2520for%250A%2520%2520Planning%252C%2520Revision%252C%2520and%2520Resource-Constrained%2520Decision%2520Making%26entry.906535625%3DXiaopeng%2520Yuan%2520and%2520Xingjian%2520Zhang%2520and%2520Ke%2520Xu%2520and%2520Yifan%2520Xu%2520and%2520Lijun%2520Yu%2520and%2520Jindong%2520Wang%2520and%2520Yushun%2520Dong%2520and%2520Haohan%2520Wang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520for%2520tasks%2520that%2520require%250Acomplex%2520reasoning.%2520Most%2520benchmarks%2520focus%2520on%2520final%2520outcomes%2520but%2520overlook%2520the%250Aintermediate%2520reasoning%2520steps%2520-%2520such%2520as%2520planning%252C%2520revision%252C%2520and%2520decision%2520making%250Aunder%2520resource%2520constraints.%2520We%2520argue%2520that%2520measuring%2520these%2520internal%2520processes%2520is%250Aessential%2520for%2520understanding%2520model%2520behavior%2520and%2520improving%2520reliability.%2520We%250Apropose%2520using%2520strategic%2520games%2520as%2520a%2520natural%2520evaluation%2520environment%253A%2520closed%252C%250Arule-based%2520systems%2520with%2520clear%2520states%252C%2520limited%2520resources%252C%2520and%2520automatic%250Afeedback.%2520We%2520introduce%2520a%2520framework%2520that%2520evaluates%2520LLMs%2520along%2520three%2520core%250Adimensions%253A%2520planning%252C%2520revision%252C%2520and%2520resource-constrained%2520decision%2520making.%2520To%250Aoperationalize%2520this%252C%2520we%2520define%2520metrics%2520beyond%2520win%2520rate%252C%2520including%250Aovercorrection%2520risk%2520rate%252C%2520correction%2520success%2520rate%252C%2520improvement%2520slope%252C%2520and%250Aover-budget%2520ratio.%2520In%25204320%2520adversarial%2520rounds%2520across%252012%2520leading%2520models%252C%250AChatGPT-o3-mini%2520achieves%2520the%2520top%2520composite%2520score%252C%2520with%2520a%2520win%2520rate%2520of%252074.7%250Apercent%252C%2520a%2520correction%2520success%2520rate%2520of%252078.6%2520percent%252C%2520and%2520an%2520improvement%2520slope%2520of%250A0.041.%2520By%2520contrast%252C%2520Qwen-Plus%252C%2520despite%2520an%2520overcorrection%2520risk%2520rate%2520of%252081.6%250Apercent%252C%2520wins%2520only%252025.6%2520percent%2520of%2520its%2520matches%2520-%2520primarily%2520due%2520to%2520excessive%250Aresource%2520use.%2520We%2520also%2520observe%2520a%2520negative%2520correlation%2520between%2520overcorrection%250Arisk%2520rate%2520and%2520correction%2520success%2520rate%2520%2528Pearson%2520r%2520%253D%2520-0.51%252C%2520p%2520%253D%25200.093%2529%252C%250Asuggesting%2520that%2520more%2520frequent%2520edits%2520do%2520not%2520always%2520improve%2520outcomes.%2520Our%250Afindings%2520highlight%2520the%2520value%2520of%2520assessing%2520not%2520only%2520what%2520LLMs%2520decide%2520but%2520how%250Athey%2520arrive%2520at%2520those%2520decisions%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tracing%20LLM%20Reasoning%20Processes%20with%20Strategic%20Games%3A%20A%20Framework%20for%0A%20%20Planning%2C%20Revision%2C%20and%20Resource-Constrained%20Decision%20Making&entry.906535625=Xiaopeng%20Yuan%20and%20Xingjian%20Zhang%20and%20Ke%20Xu%20and%20Yifan%20Xu%20and%20Lijun%20Yu%20and%20Jindong%20Wang%20and%20Yushun%20Dong%20and%20Haohan%20Wang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20for%20tasks%20that%20require%0Acomplex%20reasoning.%20Most%20benchmarks%20focus%20on%20final%20outcomes%20but%20overlook%20the%0Aintermediate%20reasoning%20steps%20-%20such%20as%20planning%2C%20revision%2C%20and%20decision%20making%0Aunder%20resource%20constraints.%20We%20argue%20that%20measuring%20these%20internal%20processes%20is%0Aessential%20for%20understanding%20model%20behavior%20and%20improving%20reliability.%20We%0Apropose%20using%20strategic%20games%20as%20a%20natural%20evaluation%20environment%3A%20closed%2C%0Arule-based%20systems%20with%20clear%20states%2C%20limited%20resources%2C%20and%20automatic%0Afeedback.%20We%20introduce%20a%20framework%20that%20evaluates%20LLMs%20along%20three%20core%0Adimensions%3A%20planning%2C%20revision%2C%20and%20resource-constrained%20decision%20making.%20To%0Aoperationalize%20this%2C%20we%20define%20metrics%20beyond%20win%20rate%2C%20including%0Aovercorrection%20risk%20rate%2C%20correction%20success%20rate%2C%20improvement%20slope%2C%20and%0Aover-budget%20ratio.%20In%204320%20adversarial%20rounds%20across%2012%20leading%20models%2C%0AChatGPT-o3-mini%20achieves%20the%20top%20composite%20score%2C%20with%20a%20win%20rate%20of%2074.7%0Apercent%2C%20a%20correction%20success%20rate%20of%2078.6%20percent%2C%20and%20an%20improvement%20slope%20of%0A0.041.%20By%20contrast%2C%20Qwen-Plus%2C%20despite%20an%20overcorrection%20risk%20rate%20of%2081.6%0Apercent%2C%20wins%20only%2025.6%20percent%20of%20its%20matches%20-%20primarily%20due%20to%20excessive%0Aresource%20use.%20We%20also%20observe%20a%20negative%20correlation%20between%20overcorrection%0Arisk%20rate%20and%20correction%20success%20rate%20%28Pearson%20r%20%3D%20-0.51%2C%20p%20%3D%200.093%29%2C%0Asuggesting%20that%20more%20frequent%20edits%20do%20not%20always%20improve%20outcomes.%20Our%0Afindings%20highlight%20the%20value%20of%20assessing%20not%20only%20what%20LLMs%20decide%20but%20how%0Athey%20arrive%20at%20those%20decisions%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12012v1&entry.124074799=Read"},
{"title": "Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for\n  Geospatial Foundation Models", "author": "Francesc Marti-Escofet and Benedikt Blumenstiel and Linus Scheibenreif and Paolo Fraccaro and Konrad Schindler", "abstract": "  Earth observation (EO) is crucial for monitoring environmental changes,\nresponding to disasters, and managing natural resources. In this context,\nfoundation models facilitate remote sensing image analysis to retrieve relevant\ngeoinformation accurately and efficiently. However, as these models grow in\nsize, fine-tuning becomes increasingly challenging due to the associated\ncomputational resources and costs, limiting their accessibility and\nscalability. Furthermore, full fine-tuning can lead to forgetting pre-trained\nfeatures and even degrade model generalization. To address this,\nParameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution.\nIn this paper, we conduct extensive experiments with various foundation model\narchitectures and PEFT techniques to evaluate their effectiveness on five\ndifferent EO datasets. Our results provide a comprehensive comparison, offering\ninsights into when and how PEFT methods support the adaptation of pre-trained\ngeospatial models. We demonstrate that PEFT techniques match or even exceed\nfull fine-tuning performance and enhance model generalisation to unseen\ngeographic regions, while reducing training time and memory requirements.\nAdditional experiments investigate the effect of architecture choices such as\nthe decoder type or the use of metadata, suggesting UNet decoders and\nfine-tuning without metadata as the recommended configuration. We have\nintegrated all evaluated foundation models and techniques into the open-source\npackage TerraTorch to support quick, scalable, and cost-effective model\nadaptation.\n", "link": "http://arxiv.org/abs/2504.17397v2", "date": "2025-06-13", "relevancy": 2.0537, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-tune%20Smarter%2C%20Not%20Harder%3A%20Parameter-Efficient%20Fine-Tuning%20for%0A%20%20Geospatial%20Foundation%20Models&body=Title%3A%20Fine-tune%20Smarter%2C%20Not%20Harder%3A%20Parameter-Efficient%20Fine-Tuning%20for%0A%20%20Geospatial%20Foundation%20Models%0AAuthor%3A%20Francesc%20Marti-Escofet%20and%20Benedikt%20Blumenstiel%20and%20Linus%20Scheibenreif%20and%20Paolo%20Fraccaro%20and%20Konrad%20Schindler%0AAbstract%3A%20%20%20Earth%20observation%20%28EO%29%20is%20crucial%20for%20monitoring%20environmental%20changes%2C%0Aresponding%20to%20disasters%2C%20and%20managing%20natural%20resources.%20In%20this%20context%2C%0Afoundation%20models%20facilitate%20remote%20sensing%20image%20analysis%20to%20retrieve%20relevant%0Ageoinformation%20accurately%20and%20efficiently.%20However%2C%20as%20these%20models%20grow%20in%0Asize%2C%20fine-tuning%20becomes%20increasingly%20challenging%20due%20to%20the%20associated%0Acomputational%20resources%20and%20costs%2C%20limiting%20their%20accessibility%20and%0Ascalability.%20Furthermore%2C%20full%20fine-tuning%20can%20lead%20to%20forgetting%20pre-trained%0Afeatures%20and%20even%20degrade%20model%20generalization.%20To%20address%20this%2C%0AParameter-Efficient%20Fine-Tuning%20%28PEFT%29%20techniques%20offer%20a%20promising%20solution.%0AIn%20this%20paper%2C%20we%20conduct%20extensive%20experiments%20with%20various%20foundation%20model%0Aarchitectures%20and%20PEFT%20techniques%20to%20evaluate%20their%20effectiveness%20on%20five%0Adifferent%20EO%20datasets.%20Our%20results%20provide%20a%20comprehensive%20comparison%2C%20offering%0Ainsights%20into%20when%20and%20how%20PEFT%20methods%20support%20the%20adaptation%20of%20pre-trained%0Ageospatial%20models.%20We%20demonstrate%20that%20PEFT%20techniques%20match%20or%20even%20exceed%0Afull%20fine-tuning%20performance%20and%20enhance%20model%20generalisation%20to%20unseen%0Ageographic%20regions%2C%20while%20reducing%20training%20time%20and%20memory%20requirements.%0AAdditional%20experiments%20investigate%20the%20effect%20of%20architecture%20choices%20such%20as%0Athe%20decoder%20type%20or%20the%20use%20of%20metadata%2C%20suggesting%20UNet%20decoders%20and%0Afine-tuning%20without%20metadata%20as%20the%20recommended%20configuration.%20We%20have%0Aintegrated%20all%20evaluated%20foundation%20models%20and%20techniques%20into%20the%20open-source%0Apackage%20TerraTorch%20to%20support%20quick%2C%20scalable%2C%20and%20cost-effective%20model%0Aadaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-tune%2520Smarter%252C%2520Not%2520Harder%253A%2520Parameter-Efficient%2520Fine-Tuning%2520for%250A%2520%2520Geospatial%2520Foundation%2520Models%26entry.906535625%3DFrancesc%2520Marti-Escofet%2520and%2520Benedikt%2520Blumenstiel%2520and%2520Linus%2520Scheibenreif%2520and%2520Paolo%2520Fraccaro%2520and%2520Konrad%2520Schindler%26entry.1292438233%3D%2520%2520Earth%2520observation%2520%2528EO%2529%2520is%2520crucial%2520for%2520monitoring%2520environmental%2520changes%252C%250Aresponding%2520to%2520disasters%252C%2520and%2520managing%2520natural%2520resources.%2520In%2520this%2520context%252C%250Afoundation%2520models%2520facilitate%2520remote%2520sensing%2520image%2520analysis%2520to%2520retrieve%2520relevant%250Ageoinformation%2520accurately%2520and%2520efficiently.%2520However%252C%2520as%2520these%2520models%2520grow%2520in%250Asize%252C%2520fine-tuning%2520becomes%2520increasingly%2520challenging%2520due%2520to%2520the%2520associated%250Acomputational%2520resources%2520and%2520costs%252C%2520limiting%2520their%2520accessibility%2520and%250Ascalability.%2520Furthermore%252C%2520full%2520fine-tuning%2520can%2520lead%2520to%2520forgetting%2520pre-trained%250Afeatures%2520and%2520even%2520degrade%2520model%2520generalization.%2520To%2520address%2520this%252C%250AParameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520techniques%2520offer%2520a%2520promising%2520solution.%250AIn%2520this%2520paper%252C%2520we%2520conduct%2520extensive%2520experiments%2520with%2520various%2520foundation%2520model%250Aarchitectures%2520and%2520PEFT%2520techniques%2520to%2520evaluate%2520their%2520effectiveness%2520on%2520five%250Adifferent%2520EO%2520datasets.%2520Our%2520results%2520provide%2520a%2520comprehensive%2520comparison%252C%2520offering%250Ainsights%2520into%2520when%2520and%2520how%2520PEFT%2520methods%2520support%2520the%2520adaptation%2520of%2520pre-trained%250Ageospatial%2520models.%2520We%2520demonstrate%2520that%2520PEFT%2520techniques%2520match%2520or%2520even%2520exceed%250Afull%2520fine-tuning%2520performance%2520and%2520enhance%2520model%2520generalisation%2520to%2520unseen%250Ageographic%2520regions%252C%2520while%2520reducing%2520training%2520time%2520and%2520memory%2520requirements.%250AAdditional%2520experiments%2520investigate%2520the%2520effect%2520of%2520architecture%2520choices%2520such%2520as%250Athe%2520decoder%2520type%2520or%2520the%2520use%2520of%2520metadata%252C%2520suggesting%2520UNet%2520decoders%2520and%250Afine-tuning%2520without%2520metadata%2520as%2520the%2520recommended%2520configuration.%2520We%2520have%250Aintegrated%2520all%2520evaluated%2520foundation%2520models%2520and%2520techniques%2520into%2520the%2520open-source%250Apackage%2520TerraTorch%2520to%2520support%2520quick%252C%2520scalable%252C%2520and%2520cost-effective%2520model%250Aadaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-tune%20Smarter%2C%20Not%20Harder%3A%20Parameter-Efficient%20Fine-Tuning%20for%0A%20%20Geospatial%20Foundation%20Models&entry.906535625=Francesc%20Marti-Escofet%20and%20Benedikt%20Blumenstiel%20and%20Linus%20Scheibenreif%20and%20Paolo%20Fraccaro%20and%20Konrad%20Schindler&entry.1292438233=%20%20Earth%20observation%20%28EO%29%20is%20crucial%20for%20monitoring%20environmental%20changes%2C%0Aresponding%20to%20disasters%2C%20and%20managing%20natural%20resources.%20In%20this%20context%2C%0Afoundation%20models%20facilitate%20remote%20sensing%20image%20analysis%20to%20retrieve%20relevant%0Ageoinformation%20accurately%20and%20efficiently.%20However%2C%20as%20these%20models%20grow%20in%0Asize%2C%20fine-tuning%20becomes%20increasingly%20challenging%20due%20to%20the%20associated%0Acomputational%20resources%20and%20costs%2C%20limiting%20their%20accessibility%20and%0Ascalability.%20Furthermore%2C%20full%20fine-tuning%20can%20lead%20to%20forgetting%20pre-trained%0Afeatures%20and%20even%20degrade%20model%20generalization.%20To%20address%20this%2C%0AParameter-Efficient%20Fine-Tuning%20%28PEFT%29%20techniques%20offer%20a%20promising%20solution.%0AIn%20this%20paper%2C%20we%20conduct%20extensive%20experiments%20with%20various%20foundation%20model%0Aarchitectures%20and%20PEFT%20techniques%20to%20evaluate%20their%20effectiveness%20on%20five%0Adifferent%20EO%20datasets.%20Our%20results%20provide%20a%20comprehensive%20comparison%2C%20offering%0Ainsights%20into%20when%20and%20how%20PEFT%20methods%20support%20the%20adaptation%20of%20pre-trained%0Ageospatial%20models.%20We%20demonstrate%20that%20PEFT%20techniques%20match%20or%20even%20exceed%0Afull%20fine-tuning%20performance%20and%20enhance%20model%20generalisation%20to%20unseen%0Ageographic%20regions%2C%20while%20reducing%20training%20time%20and%20memory%20requirements.%0AAdditional%20experiments%20investigate%20the%20effect%20of%20architecture%20choices%20such%20as%0Athe%20decoder%20type%20or%20the%20use%20of%20metadata%2C%20suggesting%20UNet%20decoders%20and%0Afine-tuning%20without%20metadata%20as%20the%20recommended%20configuration.%20We%20have%0Aintegrated%20all%20evaluated%20foundation%20models%20and%20techniques%20into%20the%20open-source%0Apackage%20TerraTorch%20to%20support%20quick%2C%20scalable%2C%20and%20cost-effective%20model%0Aadaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17397v2&entry.124074799=Read"},
{"title": "Interpretable representation learning of quantum data enabled by\n  probabilistic variational autoencoders", "author": "Paulin de Schoulepnikoff and Gorka Mu\u00f1oz-Gil and Hendrik Poulsen Nautrup and Hans J. Briegel", "abstract": "  Interpretable machine learning is rapidly becoming a crucial tool for\nscientific discovery. Among existing approaches, variational autoencoders\n(VAEs) have shown promise in extracting the hidden physical features of some\ninput data, with no supervision nor prior knowledge of the system at study.\nYet, the ability of VAEs to create meaningful, interpretable representations\nrelies on their accurate approximation of the underlying probability\ndistribution of their input. When dealing with quantum data, VAEs must hence\naccount for its intrinsic randomness and complex correlations. While VAEs have\nbeen previously applied to quantum data, they have often neglected its\nprobabilistic nature, hindering the extraction of meaningful physical\ndescriptors. Here, we demonstrate that two key modifications enable VAEs to\nlearn physically meaningful latent representations: a decoder capable of\nfaithfully reproduce quantum states and a probabilistic loss tailored to this\ntask. Using benchmark quantum spin models, we identify regimes where standard\nmethods fail while the representations learned by our approach remain\nmeaningful and interpretable. Applied to experimental data from Rydberg atom\narrays, the model autonomously uncovers the phase structure without access to\nprior labels, Hamiltonian details, or knowledge of relevant order parameters,\nhighlighting its potential as an unsupervised and interpretable tool for the\nstudy of quantum systems.\n", "link": "http://arxiv.org/abs/2506.11982v1", "date": "2025-06-13", "relevancy": 2.0418, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5401}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.527}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20representation%20learning%20of%20quantum%20data%20enabled%20by%0A%20%20probabilistic%20variational%20autoencoders&body=Title%3A%20Interpretable%20representation%20learning%20of%20quantum%20data%20enabled%20by%0A%20%20probabilistic%20variational%20autoencoders%0AAuthor%3A%20Paulin%20de%20Schoulepnikoff%20and%20Gorka%20Mu%C3%B1oz-Gil%20and%20Hendrik%20Poulsen%20Nautrup%20and%20Hans%20J.%20Briegel%0AAbstract%3A%20%20%20Interpretable%20machine%20learning%20is%20rapidly%20becoming%20a%20crucial%20tool%20for%0Ascientific%20discovery.%20Among%20existing%20approaches%2C%20variational%20autoencoders%0A%28VAEs%29%20have%20shown%20promise%20in%20extracting%20the%20hidden%20physical%20features%20of%20some%0Ainput%20data%2C%20with%20no%20supervision%20nor%20prior%20knowledge%20of%20the%20system%20at%20study.%0AYet%2C%20the%20ability%20of%20VAEs%20to%20create%20meaningful%2C%20interpretable%20representations%0Arelies%20on%20their%20accurate%20approximation%20of%20the%20underlying%20probability%0Adistribution%20of%20their%20input.%20When%20dealing%20with%20quantum%20data%2C%20VAEs%20must%20hence%0Aaccount%20for%20its%20intrinsic%20randomness%20and%20complex%20correlations.%20While%20VAEs%20have%0Abeen%20previously%20applied%20to%20quantum%20data%2C%20they%20have%20often%20neglected%20its%0Aprobabilistic%20nature%2C%20hindering%20the%20extraction%20of%20meaningful%20physical%0Adescriptors.%20Here%2C%20we%20demonstrate%20that%20two%20key%20modifications%20enable%20VAEs%20to%0Alearn%20physically%20meaningful%20latent%20representations%3A%20a%20decoder%20capable%20of%0Afaithfully%20reproduce%20quantum%20states%20and%20a%20probabilistic%20loss%20tailored%20to%20this%0Atask.%20Using%20benchmark%20quantum%20spin%20models%2C%20we%20identify%20regimes%20where%20standard%0Amethods%20fail%20while%20the%20representations%20learned%20by%20our%20approach%20remain%0Ameaningful%20and%20interpretable.%20Applied%20to%20experimental%20data%20from%20Rydberg%20atom%0Aarrays%2C%20the%20model%20autonomously%20uncovers%20the%20phase%20structure%20without%20access%20to%0Aprior%20labels%2C%20Hamiltonian%20details%2C%20or%20knowledge%20of%20relevant%20order%20parameters%2C%0Ahighlighting%20its%20potential%20as%20an%20unsupervised%20and%20interpretable%20tool%20for%20the%0Astudy%20of%20quantum%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520representation%2520learning%2520of%2520quantum%2520data%2520enabled%2520by%250A%2520%2520probabilistic%2520variational%2520autoencoders%26entry.906535625%3DPaulin%2520de%2520Schoulepnikoff%2520and%2520Gorka%2520Mu%25C3%25B1oz-Gil%2520and%2520Hendrik%2520Poulsen%2520Nautrup%2520and%2520Hans%2520J.%2520Briegel%26entry.1292438233%3D%2520%2520Interpretable%2520machine%2520learning%2520is%2520rapidly%2520becoming%2520a%2520crucial%2520tool%2520for%250Ascientific%2520discovery.%2520Among%2520existing%2520approaches%252C%2520variational%2520autoencoders%250A%2528VAEs%2529%2520have%2520shown%2520promise%2520in%2520extracting%2520the%2520hidden%2520physical%2520features%2520of%2520some%250Ainput%2520data%252C%2520with%2520no%2520supervision%2520nor%2520prior%2520knowledge%2520of%2520the%2520system%2520at%2520study.%250AYet%252C%2520the%2520ability%2520of%2520VAEs%2520to%2520create%2520meaningful%252C%2520interpretable%2520representations%250Arelies%2520on%2520their%2520accurate%2520approximation%2520of%2520the%2520underlying%2520probability%250Adistribution%2520of%2520their%2520input.%2520When%2520dealing%2520with%2520quantum%2520data%252C%2520VAEs%2520must%2520hence%250Aaccount%2520for%2520its%2520intrinsic%2520randomness%2520and%2520complex%2520correlations.%2520While%2520VAEs%2520have%250Abeen%2520previously%2520applied%2520to%2520quantum%2520data%252C%2520they%2520have%2520often%2520neglected%2520its%250Aprobabilistic%2520nature%252C%2520hindering%2520the%2520extraction%2520of%2520meaningful%2520physical%250Adescriptors.%2520Here%252C%2520we%2520demonstrate%2520that%2520two%2520key%2520modifications%2520enable%2520VAEs%2520to%250Alearn%2520physically%2520meaningful%2520latent%2520representations%253A%2520a%2520decoder%2520capable%2520of%250Afaithfully%2520reproduce%2520quantum%2520states%2520and%2520a%2520probabilistic%2520loss%2520tailored%2520to%2520this%250Atask.%2520Using%2520benchmark%2520quantum%2520spin%2520models%252C%2520we%2520identify%2520regimes%2520where%2520standard%250Amethods%2520fail%2520while%2520the%2520representations%2520learned%2520by%2520our%2520approach%2520remain%250Ameaningful%2520and%2520interpretable.%2520Applied%2520to%2520experimental%2520data%2520from%2520Rydberg%2520atom%250Aarrays%252C%2520the%2520model%2520autonomously%2520uncovers%2520the%2520phase%2520structure%2520without%2520access%2520to%250Aprior%2520labels%252C%2520Hamiltonian%2520details%252C%2520or%2520knowledge%2520of%2520relevant%2520order%2520parameters%252C%250Ahighlighting%2520its%2520potential%2520as%2520an%2520unsupervised%2520and%2520interpretable%2520tool%2520for%2520the%250Astudy%2520of%2520quantum%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20representation%20learning%20of%20quantum%20data%20enabled%20by%0A%20%20probabilistic%20variational%20autoencoders&entry.906535625=Paulin%20de%20Schoulepnikoff%20and%20Gorka%20Mu%C3%B1oz-Gil%20and%20Hendrik%20Poulsen%20Nautrup%20and%20Hans%20J.%20Briegel&entry.1292438233=%20%20Interpretable%20machine%20learning%20is%20rapidly%20becoming%20a%20crucial%20tool%20for%0Ascientific%20discovery.%20Among%20existing%20approaches%2C%20variational%20autoencoders%0A%28VAEs%29%20have%20shown%20promise%20in%20extracting%20the%20hidden%20physical%20features%20of%20some%0Ainput%20data%2C%20with%20no%20supervision%20nor%20prior%20knowledge%20of%20the%20system%20at%20study.%0AYet%2C%20the%20ability%20of%20VAEs%20to%20create%20meaningful%2C%20interpretable%20representations%0Arelies%20on%20their%20accurate%20approximation%20of%20the%20underlying%20probability%0Adistribution%20of%20their%20input.%20When%20dealing%20with%20quantum%20data%2C%20VAEs%20must%20hence%0Aaccount%20for%20its%20intrinsic%20randomness%20and%20complex%20correlations.%20While%20VAEs%20have%0Abeen%20previously%20applied%20to%20quantum%20data%2C%20they%20have%20often%20neglected%20its%0Aprobabilistic%20nature%2C%20hindering%20the%20extraction%20of%20meaningful%20physical%0Adescriptors.%20Here%2C%20we%20demonstrate%20that%20two%20key%20modifications%20enable%20VAEs%20to%0Alearn%20physically%20meaningful%20latent%20representations%3A%20a%20decoder%20capable%20of%0Afaithfully%20reproduce%20quantum%20states%20and%20a%20probabilistic%20loss%20tailored%20to%20this%0Atask.%20Using%20benchmark%20quantum%20spin%20models%2C%20we%20identify%20regimes%20where%20standard%0Amethods%20fail%20while%20the%20representations%20learned%20by%20our%20approach%20remain%0Ameaningful%20and%20interpretable.%20Applied%20to%20experimental%20data%20from%20Rydberg%20atom%0Aarrays%2C%20the%20model%20autonomously%20uncovers%20the%20phase%20structure%20without%20access%20to%0Aprior%20labels%2C%20Hamiltonian%20details%2C%20or%20knowledge%20of%20relevant%20order%20parameters%2C%0Ahighlighting%20its%20potential%20as%20an%20unsupervised%20and%20interpretable%20tool%20for%20the%0Astudy%20of%20quantum%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11982v1&entry.124074799=Read"},
{"title": "T1: Advancing Language Model Reasoning through Reinforcement Learning\n  and Inference Scaling", "author": "Zhenyu Hou and Xin Lv and Rui Lu and Jiajie Zhang and Yujiang Li and Zijun Yao and Juanzi Li and Jie Tang and Yuxiao Dong", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration, recent\nattempts yield modest improvements in complex reasoning. In this paper, we\npresent T1 to scale RL by encouraging exploration and understand inference\nscaling. We first initialize the LLM using synthesized chain-of-thought data\nthat integrates trial-and-error and self-verification. To scale RL training, we\npromote increased sampling diversity through oversampling. We demonstrate that\nT1 with open LLMs as its base exhibits inference scaling behavior and achieves\nsuperior performance on challenging math reasoning benchmarks. More\nimportantly, we present a simple strategy to examine inference scaling, where\nincreased inference budgets directly lead to T1's better performance without\nany additional verification.\n", "link": "http://arxiv.org/abs/2501.11651v2", "date": "2025-06-13", "relevancy": 2.039, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T1%3A%20Advancing%20Language%20Model%20Reasoning%20through%20Reinforcement%20Learning%0A%20%20and%20Inference%20Scaling&body=Title%3A%20T1%3A%20Advancing%20Language%20Model%20Reasoning%20through%20Reinforcement%20Learning%0A%20%20and%20Inference%20Scaling%0AAuthor%3A%20Zhenyu%20Hou%20and%20Xin%20Lv%20and%20Rui%20Lu%20and%20Jiajie%20Zhang%20and%20Yujiang%20Li%20and%20Zijun%20Yao%20and%20Juanzi%20Li%20and%20Jie%20Tang%20and%20Yuxiao%20Dong%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Acomplex%20reasoning%20tasks.%20However%2C%20existing%20approaches%20mainly%20rely%20on%20imitation%0Alearning%20and%20struggle%20to%20achieve%20effective%20test-time%20scaling.%20While%0Areinforcement%20learning%20%28RL%29%20holds%20promise%20for%20enabling%20self-exploration%2C%20recent%0Aattempts%20yield%20modest%20improvements%20in%20complex%20reasoning.%20In%20this%20paper%2C%20we%0Apresent%20T1%20to%20scale%20RL%20by%20encouraging%20exploration%20and%20understand%20inference%0Ascaling.%20We%20first%20initialize%20the%20LLM%20using%20synthesized%20chain-of-thought%20data%0Athat%20integrates%20trial-and-error%20and%20self-verification.%20To%20scale%20RL%20training%2C%20we%0Apromote%20increased%20sampling%20diversity%20through%20oversampling.%20We%20demonstrate%20that%0AT1%20with%20open%20LLMs%20as%20its%20base%20exhibits%20inference%20scaling%20behavior%20and%20achieves%0Asuperior%20performance%20on%20challenging%20math%20reasoning%20benchmarks.%20More%0Aimportantly%2C%20we%20present%20a%20simple%20strategy%20to%20examine%20inference%20scaling%2C%20where%0Aincreased%20inference%20budgets%20directly%20lead%20to%20T1%27s%20better%20performance%20without%0Aany%20additional%20verification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11651v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT1%253A%2520Advancing%2520Language%2520Model%2520Reasoning%2520through%2520Reinforcement%2520Learning%250A%2520%2520and%2520Inference%2520Scaling%26entry.906535625%3DZhenyu%2520Hou%2520and%2520Xin%2520Lv%2520and%2520Rui%2520Lu%2520and%2520Jiajie%2520Zhang%2520and%2520Yujiang%2520Li%2520and%2520Zijun%2520Yao%2520and%2520Juanzi%2520Li%2520and%2520Jie%2520Tang%2520and%2520Yuxiao%2520Dong%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%250Acomplex%2520reasoning%2520tasks.%2520However%252C%2520existing%2520approaches%2520mainly%2520rely%2520on%2520imitation%250Alearning%2520and%2520struggle%2520to%2520achieve%2520effective%2520test-time%2520scaling.%2520While%250Areinforcement%2520learning%2520%2528RL%2529%2520holds%2520promise%2520for%2520enabling%2520self-exploration%252C%2520recent%250Aattempts%2520yield%2520modest%2520improvements%2520in%2520complex%2520reasoning.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520T1%2520to%2520scale%2520RL%2520by%2520encouraging%2520exploration%2520and%2520understand%2520inference%250Ascaling.%2520We%2520first%2520initialize%2520the%2520LLM%2520using%2520synthesized%2520chain-of-thought%2520data%250Athat%2520integrates%2520trial-and-error%2520and%2520self-verification.%2520To%2520scale%2520RL%2520training%252C%2520we%250Apromote%2520increased%2520sampling%2520diversity%2520through%2520oversampling.%2520We%2520demonstrate%2520that%250AT1%2520with%2520open%2520LLMs%2520as%2520its%2520base%2520exhibits%2520inference%2520scaling%2520behavior%2520and%2520achieves%250Asuperior%2520performance%2520on%2520challenging%2520math%2520reasoning%2520benchmarks.%2520More%250Aimportantly%252C%2520we%2520present%2520a%2520simple%2520strategy%2520to%2520examine%2520inference%2520scaling%252C%2520where%250Aincreased%2520inference%2520budgets%2520directly%2520lead%2520to%2520T1%2527s%2520better%2520performance%2520without%250Aany%2520additional%2520verification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11651v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T1%3A%20Advancing%20Language%20Model%20Reasoning%20through%20Reinforcement%20Learning%0A%20%20and%20Inference%20Scaling&entry.906535625=Zhenyu%20Hou%20and%20Xin%20Lv%20and%20Rui%20Lu%20and%20Jiajie%20Zhang%20and%20Yujiang%20Li%20and%20Zijun%20Yao%20and%20Juanzi%20Li%20and%20Jie%20Tang%20and%20Yuxiao%20Dong&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Acomplex%20reasoning%20tasks.%20However%2C%20existing%20approaches%20mainly%20rely%20on%20imitation%0Alearning%20and%20struggle%20to%20achieve%20effective%20test-time%20scaling.%20While%0Areinforcement%20learning%20%28RL%29%20holds%20promise%20for%20enabling%20self-exploration%2C%20recent%0Aattempts%20yield%20modest%20improvements%20in%20complex%20reasoning.%20In%20this%20paper%2C%20we%0Apresent%20T1%20to%20scale%20RL%20by%20encouraging%20exploration%20and%20understand%20inference%0Ascaling.%20We%20first%20initialize%20the%20LLM%20using%20synthesized%20chain-of-thought%20data%0Athat%20integrates%20trial-and-error%20and%20self-verification.%20To%20scale%20RL%20training%2C%20we%0Apromote%20increased%20sampling%20diversity%20through%20oversampling.%20We%20demonstrate%20that%0AT1%20with%20open%20LLMs%20as%20its%20base%20exhibits%20inference%20scaling%20behavior%20and%20achieves%0Asuperior%20performance%20on%20challenging%20math%20reasoning%20benchmarks.%20More%0Aimportantly%2C%20we%20present%20a%20simple%20strategy%20to%20examine%20inference%20scaling%2C%20where%0Aincreased%20inference%20budgets%20directly%20lead%20to%20T1%27s%20better%20performance%20without%0Aany%20additional%20verification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11651v2&entry.124074799=Read"},
{"title": "DeePoly: A High-Order Accuracy Scientific Machine Learning Framework for\n  Function Approximation and Solving PDEs", "author": "Li Liu and Heng Yong", "abstract": "  Recently, machine learning methods have gained significant traction in\nscientific computing, particularly for solving Partial Differential Equations\n(PDEs). However, methods based on deep neural networks (DNNs) often lack\nconvergence guarantees and computational efficiency compared to traditional\nnumerical schemes. This work introduces DeePoly, a novel framework that\ntransforms the solution paradigm from pure non-convex parameter optimization to\na two-stage approach: first employing a DNN to capture complex global features,\nfollowed by linear space optimization with combined DNN-extracted features\n(Spotter) and polynomial basis functions (Sniper). This strategic combination\nleverages the complementary strengths of both methods -- DNNs excel at\napproximating complex global features (i.e., high-gradient features) and\nstabilize the polynomial approximation while polynomial bases provide\nhigh-precision local corrections with convergence guarantees. Theoretical\nanalysis and numerical experiments demonstrate that this approach significantly\nenhances both high-order accuracy and efficiency across diverse problem types\nwhile maintaining mesh-free and scheme-free properties. This paper also serves\nas a theoretical exposition for the open-source project DeePoly.\n", "link": "http://arxiv.org/abs/2506.04613v3", "date": "2025-06-13", "relevancy": 2.0125, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.509}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5043}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeePoly%3A%20A%20High-Order%20Accuracy%20Scientific%20Machine%20Learning%20Framework%20for%0A%20%20Function%20Approximation%20and%20Solving%20PDEs&body=Title%3A%20DeePoly%3A%20A%20High-Order%20Accuracy%20Scientific%20Machine%20Learning%20Framework%20for%0A%20%20Function%20Approximation%20and%20Solving%20PDEs%0AAuthor%3A%20Li%20Liu%20and%20Heng%20Yong%0AAbstract%3A%20%20%20Recently%2C%20machine%20learning%20methods%20have%20gained%20significant%20traction%20in%0Ascientific%20computing%2C%20particularly%20for%20solving%20Partial%20Differential%20Equations%0A%28PDEs%29.%20However%2C%20methods%20based%20on%20deep%20neural%20networks%20%28DNNs%29%20often%20lack%0Aconvergence%20guarantees%20and%20computational%20efficiency%20compared%20to%20traditional%0Anumerical%20schemes.%20This%20work%20introduces%20DeePoly%2C%20a%20novel%20framework%20that%0Atransforms%20the%20solution%20paradigm%20from%20pure%20non-convex%20parameter%20optimization%20to%0Aa%20two-stage%20approach%3A%20first%20employing%20a%20DNN%20to%20capture%20complex%20global%20features%2C%0Afollowed%20by%20linear%20space%20optimization%20with%20combined%20DNN-extracted%20features%0A%28Spotter%29%20and%20polynomial%20basis%20functions%20%28Sniper%29.%20This%20strategic%20combination%0Aleverages%20the%20complementary%20strengths%20of%20both%20methods%20--%20DNNs%20excel%20at%0Aapproximating%20complex%20global%20features%20%28i.e.%2C%20high-gradient%20features%29%20and%0Astabilize%20the%20polynomial%20approximation%20while%20polynomial%20bases%20provide%0Ahigh-precision%20local%20corrections%20with%20convergence%20guarantees.%20Theoretical%0Aanalysis%20and%20numerical%20experiments%20demonstrate%20that%20this%20approach%20significantly%0Aenhances%20both%20high-order%20accuracy%20and%20efficiency%20across%20diverse%20problem%20types%0Awhile%20maintaining%20mesh-free%20and%20scheme-free%20properties.%20This%20paper%20also%20serves%0Aas%20a%20theoretical%20exposition%20for%20the%20open-source%20project%20DeePoly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04613v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeePoly%253A%2520A%2520High-Order%2520Accuracy%2520Scientific%2520Machine%2520Learning%2520Framework%2520for%250A%2520%2520Function%2520Approximation%2520and%2520Solving%2520PDEs%26entry.906535625%3DLi%2520Liu%2520and%2520Heng%2520Yong%26entry.1292438233%3D%2520%2520Recently%252C%2520machine%2520learning%2520methods%2520have%2520gained%2520significant%2520traction%2520in%250Ascientific%2520computing%252C%2520particularly%2520for%2520solving%2520Partial%2520Differential%2520Equations%250A%2528PDEs%2529.%2520However%252C%2520methods%2520based%2520on%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520often%2520lack%250Aconvergence%2520guarantees%2520and%2520computational%2520efficiency%2520compared%2520to%2520traditional%250Anumerical%2520schemes.%2520This%2520work%2520introduces%2520DeePoly%252C%2520a%2520novel%2520framework%2520that%250Atransforms%2520the%2520solution%2520paradigm%2520from%2520pure%2520non-convex%2520parameter%2520optimization%2520to%250Aa%2520two-stage%2520approach%253A%2520first%2520employing%2520a%2520DNN%2520to%2520capture%2520complex%2520global%2520features%252C%250Afollowed%2520by%2520linear%2520space%2520optimization%2520with%2520combined%2520DNN-extracted%2520features%250A%2528Spotter%2529%2520and%2520polynomial%2520basis%2520functions%2520%2528Sniper%2529.%2520This%2520strategic%2520combination%250Aleverages%2520the%2520complementary%2520strengths%2520of%2520both%2520methods%2520--%2520DNNs%2520excel%2520at%250Aapproximating%2520complex%2520global%2520features%2520%2528i.e.%252C%2520high-gradient%2520features%2529%2520and%250Astabilize%2520the%2520polynomial%2520approximation%2520while%2520polynomial%2520bases%2520provide%250Ahigh-precision%2520local%2520corrections%2520with%2520convergence%2520guarantees.%2520Theoretical%250Aanalysis%2520and%2520numerical%2520experiments%2520demonstrate%2520that%2520this%2520approach%2520significantly%250Aenhances%2520both%2520high-order%2520accuracy%2520and%2520efficiency%2520across%2520diverse%2520problem%2520types%250Awhile%2520maintaining%2520mesh-free%2520and%2520scheme-free%2520properties.%2520This%2520paper%2520also%2520serves%250Aas%2520a%2520theoretical%2520exposition%2520for%2520the%2520open-source%2520project%2520DeePoly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04613v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeePoly%3A%20A%20High-Order%20Accuracy%20Scientific%20Machine%20Learning%20Framework%20for%0A%20%20Function%20Approximation%20and%20Solving%20PDEs&entry.906535625=Li%20Liu%20and%20Heng%20Yong&entry.1292438233=%20%20Recently%2C%20machine%20learning%20methods%20have%20gained%20significant%20traction%20in%0Ascientific%20computing%2C%20particularly%20for%20solving%20Partial%20Differential%20Equations%0A%28PDEs%29.%20However%2C%20methods%20based%20on%20deep%20neural%20networks%20%28DNNs%29%20often%20lack%0Aconvergence%20guarantees%20and%20computational%20efficiency%20compared%20to%20traditional%0Anumerical%20schemes.%20This%20work%20introduces%20DeePoly%2C%20a%20novel%20framework%20that%0Atransforms%20the%20solution%20paradigm%20from%20pure%20non-convex%20parameter%20optimization%20to%0Aa%20two-stage%20approach%3A%20first%20employing%20a%20DNN%20to%20capture%20complex%20global%20features%2C%0Afollowed%20by%20linear%20space%20optimization%20with%20combined%20DNN-extracted%20features%0A%28Spotter%29%20and%20polynomial%20basis%20functions%20%28Sniper%29.%20This%20strategic%20combination%0Aleverages%20the%20complementary%20strengths%20of%20both%20methods%20--%20DNNs%20excel%20at%0Aapproximating%20complex%20global%20features%20%28i.e.%2C%20high-gradient%20features%29%20and%0Astabilize%20the%20polynomial%20approximation%20while%20polynomial%20bases%20provide%0Ahigh-precision%20local%20corrections%20with%20convergence%20guarantees.%20Theoretical%0Aanalysis%20and%20numerical%20experiments%20demonstrate%20that%20this%20approach%20significantly%0Aenhances%20both%20high-order%20accuracy%20and%20efficiency%20across%20diverse%20problem%20types%0Awhile%20maintaining%20mesh-free%20and%20scheme-free%20properties.%20This%20paper%20also%20serves%0Aas%20a%20theoretical%20exposition%20for%20the%20open-source%20project%20DeePoly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04613v3&entry.124074799=Read"},
{"title": "Training RL Agents for Multi-Objective Network Defense Tasks", "author": "Andres Molina-Markham and Luis Robaina and Sean Steinle and Akash Trivedi and Derek Tsui and Nicholas Potteiger and Lauren Brandt and Ransom Winder and Ahmad Ridley", "abstract": "  Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work.\n", "link": "http://arxiv.org/abs/2505.22531v2", "date": "2025-06-13", "relevancy": 2.0112, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5361}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5006}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20RL%20Agents%20for%20Multi-Objective%20Network%20Defense%20Tasks&body=Title%3A%20Training%20RL%20Agents%20for%20Multi-Objective%20Network%20Defense%20Tasks%0AAuthor%3A%20Andres%20Molina-Markham%20and%20Luis%20Robaina%20and%20Sean%20Steinle%20and%20Akash%20Trivedi%20and%20Derek%20Tsui%20and%20Nicholas%20Potteiger%20and%20Lauren%20Brandt%20and%20Ransom%20Winder%20and%20Ahmad%20Ridley%0AAbstract%3A%20%20%20Open-ended%20learning%20%28OEL%29%20--%20which%20emphasizes%20training%20agents%20that%20achieve%0Abroad%20capability%20over%20narrow%20competency%20--%20is%20emerging%20as%20a%20paradigm%20to%20develop%0Aartificial%20intelligence%20%28AI%29%20agents%20to%20achieve%20robustness%20and%20generalization.%0AHowever%2C%20despite%20promising%20results%20that%20demonstrate%20the%20benefits%20of%20OEL%2C%0Aapplying%20OEL%20to%20develop%20autonomous%20agents%20for%20real-world%20cybersecurity%0Aapplications%20remains%20a%20challenge.%0A%20%20We%20propose%20a%20training%20approach%2C%20inspired%20by%20OEL%2C%20to%20develop%20autonomous%0Anetwork%20defenders.%20Our%20results%20demonstrate%20that%20like%20in%20other%20domains%2C%20OEL%0Aprinciples%20can%20translate%20into%20more%20robust%20and%20generalizable%20agents%20for%20cyber%0Adefense.%20To%20apply%20OEL%20to%20network%20defense%2C%20it%20is%20necessary%20to%20address%20several%0Atechnical%20challenges.%20Most%20importantly%2C%20it%20is%20critical%20to%20provide%20a%20task%0Arepresentation%20approach%20over%20a%20broad%20universe%20of%20tasks%20that%20maintains%20a%0Aconsistent%20interface%20over%20goals%2C%20rewards%20and%20action%20spaces.%20This%20way%2C%20the%0Alearning%20agent%20can%20train%20with%20varying%20network%20conditions%2C%20attacker%20behaviors%2C%0Aand%20defender%20goals%20while%20being%20able%20to%20build%20on%20previously%20gained%20knowledge.%0A%20%20With%20our%20tools%20and%20results%2C%20we%20aim%20to%20fundamentally%20impact%20research%20that%0Aapplies%20AI%20to%20solve%20cybersecurity%20problems.%20Specifically%2C%20as%20researchers%0Adevelop%20gyms%20and%20benchmarks%20for%20cyber%20defense%2C%20it%20is%20paramount%20that%20they%0Aconsider%20diverse%20tasks%20with%20consistent%20representations%2C%20such%20as%20those%20we%0Apropose%20in%20our%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22531v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520RL%2520Agents%2520for%2520Multi-Objective%2520Network%2520Defense%2520Tasks%26entry.906535625%3DAndres%2520Molina-Markham%2520and%2520Luis%2520Robaina%2520and%2520Sean%2520Steinle%2520and%2520Akash%2520Trivedi%2520and%2520Derek%2520Tsui%2520and%2520Nicholas%2520Potteiger%2520and%2520Lauren%2520Brandt%2520and%2520Ransom%2520Winder%2520and%2520Ahmad%2520Ridley%26entry.1292438233%3D%2520%2520Open-ended%2520learning%2520%2528OEL%2529%2520--%2520which%2520emphasizes%2520training%2520agents%2520that%2520achieve%250Abroad%2520capability%2520over%2520narrow%2520competency%2520--%2520is%2520emerging%2520as%2520a%2520paradigm%2520to%2520develop%250Aartificial%2520intelligence%2520%2528AI%2529%2520agents%2520to%2520achieve%2520robustness%2520and%2520generalization.%250AHowever%252C%2520despite%2520promising%2520results%2520that%2520demonstrate%2520the%2520benefits%2520of%2520OEL%252C%250Aapplying%2520OEL%2520to%2520develop%2520autonomous%2520agents%2520for%2520real-world%2520cybersecurity%250Aapplications%2520remains%2520a%2520challenge.%250A%2520%2520We%2520propose%2520a%2520training%2520approach%252C%2520inspired%2520by%2520OEL%252C%2520to%2520develop%2520autonomous%250Anetwork%2520defenders.%2520Our%2520results%2520demonstrate%2520that%2520like%2520in%2520other%2520domains%252C%2520OEL%250Aprinciples%2520can%2520translate%2520into%2520more%2520robust%2520and%2520generalizable%2520agents%2520for%2520cyber%250Adefense.%2520To%2520apply%2520OEL%2520to%2520network%2520defense%252C%2520it%2520is%2520necessary%2520to%2520address%2520several%250Atechnical%2520challenges.%2520Most%2520importantly%252C%2520it%2520is%2520critical%2520to%2520provide%2520a%2520task%250Arepresentation%2520approach%2520over%2520a%2520broad%2520universe%2520of%2520tasks%2520that%2520maintains%2520a%250Aconsistent%2520interface%2520over%2520goals%252C%2520rewards%2520and%2520action%2520spaces.%2520This%2520way%252C%2520the%250Alearning%2520agent%2520can%2520train%2520with%2520varying%2520network%2520conditions%252C%2520attacker%2520behaviors%252C%250Aand%2520defender%2520goals%2520while%2520being%2520able%2520to%2520build%2520on%2520previously%2520gained%2520knowledge.%250A%2520%2520With%2520our%2520tools%2520and%2520results%252C%2520we%2520aim%2520to%2520fundamentally%2520impact%2520research%2520that%250Aapplies%2520AI%2520to%2520solve%2520cybersecurity%2520problems.%2520Specifically%252C%2520as%2520researchers%250Adevelop%2520gyms%2520and%2520benchmarks%2520for%2520cyber%2520defense%252C%2520it%2520is%2520paramount%2520that%2520they%250Aconsider%2520diverse%2520tasks%2520with%2520consistent%2520representations%252C%2520such%2520as%2520those%2520we%250Apropose%2520in%2520our%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22531v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20RL%20Agents%20for%20Multi-Objective%20Network%20Defense%20Tasks&entry.906535625=Andres%20Molina-Markham%20and%20Luis%20Robaina%20and%20Sean%20Steinle%20and%20Akash%20Trivedi%20and%20Derek%20Tsui%20and%20Nicholas%20Potteiger%20and%20Lauren%20Brandt%20and%20Ransom%20Winder%20and%20Ahmad%20Ridley&entry.1292438233=%20%20Open-ended%20learning%20%28OEL%29%20--%20which%20emphasizes%20training%20agents%20that%20achieve%0Abroad%20capability%20over%20narrow%20competency%20--%20is%20emerging%20as%20a%20paradigm%20to%20develop%0Aartificial%20intelligence%20%28AI%29%20agents%20to%20achieve%20robustness%20and%20generalization.%0AHowever%2C%20despite%20promising%20results%20that%20demonstrate%20the%20benefits%20of%20OEL%2C%0Aapplying%20OEL%20to%20develop%20autonomous%20agents%20for%20real-world%20cybersecurity%0Aapplications%20remains%20a%20challenge.%0A%20%20We%20propose%20a%20training%20approach%2C%20inspired%20by%20OEL%2C%20to%20develop%20autonomous%0Anetwork%20defenders.%20Our%20results%20demonstrate%20that%20like%20in%20other%20domains%2C%20OEL%0Aprinciples%20can%20translate%20into%20more%20robust%20and%20generalizable%20agents%20for%20cyber%0Adefense.%20To%20apply%20OEL%20to%20network%20defense%2C%20it%20is%20necessary%20to%20address%20several%0Atechnical%20challenges.%20Most%20importantly%2C%20it%20is%20critical%20to%20provide%20a%20task%0Arepresentation%20approach%20over%20a%20broad%20universe%20of%20tasks%20that%20maintains%20a%0Aconsistent%20interface%20over%20goals%2C%20rewards%20and%20action%20spaces.%20This%20way%2C%20the%0Alearning%20agent%20can%20train%20with%20varying%20network%20conditions%2C%20attacker%20behaviors%2C%0Aand%20defender%20goals%20while%20being%20able%20to%20build%20on%20previously%20gained%20knowledge.%0A%20%20With%20our%20tools%20and%20results%2C%20we%20aim%20to%20fundamentally%20impact%20research%20that%0Aapplies%20AI%20to%20solve%20cybersecurity%20problems.%20Specifically%2C%20as%20researchers%0Adevelop%20gyms%20and%20benchmarks%20for%20cyber%20defense%2C%20it%20is%20paramount%20that%20they%0Aconsider%20diverse%20tasks%20with%20consistent%20representations%2C%20such%20as%20those%20we%0Apropose%20in%20our%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22531v2&entry.124074799=Read"},
{"title": "Epistemic Artificial Intelligence is Essential for Machine Learning\n  Models to Truly `Know When They Do Not Know'", "author": "Shireen Kudukkil Manchingal and Andrew Bradley and Julian F. P. Kooij and Keivan Shariatmadar and Neil Yorke-Smith and Fabio Cuzzolin", "abstract": "  Despite AI's impressive achievements, including recent advances in generative\nand large language models, there remains a significant gap in the ability of AI\nsystems to handle uncertainty and generalize beyond their training data. AI\nmodels consistently fail to make robust enough predictions when facing\nunfamiliar or adversarial data. Traditional machine learning approaches\nstruggle to address this issue, due to an overemphasis on data fitting, while\ncurrent uncertainty quantification approaches suffer from serious limitations.\nThis position paper posits a paradigm shift towards epistemic artificial\nintelligence, emphasizing the need for models to learn from what they know\nwhile at the same time acknowledging their ignorance, using the mathematics of\nsecond-order uncertainty measures. This approach, which leverages the\nexpressive power of such measures to efficiently manage uncertainty, offers an\neffective way to improve the resilience and robustness of AI systems, allowing\nthem to better handle unpredictable real-world environments.\n", "link": "http://arxiv.org/abs/2505.04950v2", "date": "2025-06-13", "relevancy": 2.0068, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5411}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5162}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Epistemic%20Artificial%20Intelligence%20is%20Essential%20for%20Machine%20Learning%0A%20%20Models%20to%20Truly%20%60Know%20When%20They%20Do%20Not%20Know%27&body=Title%3A%20Epistemic%20Artificial%20Intelligence%20is%20Essential%20for%20Machine%20Learning%0A%20%20Models%20to%20Truly%20%60Know%20When%20They%20Do%20Not%20Know%27%0AAuthor%3A%20Shireen%20Kudukkil%20Manchingal%20and%20Andrew%20Bradley%20and%20Julian%20F.%20P.%20Kooij%20and%20Keivan%20Shariatmadar%20and%20Neil%20Yorke-Smith%20and%20Fabio%20Cuzzolin%0AAbstract%3A%20%20%20Despite%20AI%27s%20impressive%20achievements%2C%20including%20recent%20advances%20in%20generative%0Aand%20large%20language%20models%2C%20there%20remains%20a%20significant%20gap%20in%20the%20ability%20of%20AI%0Asystems%20to%20handle%20uncertainty%20and%20generalize%20beyond%20their%20training%20data.%20AI%0Amodels%20consistently%20fail%20to%20make%20robust%20enough%20predictions%20when%20facing%0Aunfamiliar%20or%20adversarial%20data.%20Traditional%20machine%20learning%20approaches%0Astruggle%20to%20address%20this%20issue%2C%20due%20to%20an%20overemphasis%20on%20data%20fitting%2C%20while%0Acurrent%20uncertainty%20quantification%20approaches%20suffer%20from%20serious%20limitations.%0AThis%20position%20paper%20posits%20a%20paradigm%20shift%20towards%20epistemic%20artificial%0Aintelligence%2C%20emphasizing%20the%20need%20for%20models%20to%20learn%20from%20what%20they%20know%0Awhile%20at%20the%20same%20time%20acknowledging%20their%20ignorance%2C%20using%20the%20mathematics%20of%0Asecond-order%20uncertainty%20measures.%20This%20approach%2C%20which%20leverages%20the%0Aexpressive%20power%20of%20such%20measures%20to%20efficiently%20manage%20uncertainty%2C%20offers%20an%0Aeffective%20way%20to%20improve%20the%20resilience%20and%20robustness%20of%20AI%20systems%2C%20allowing%0Athem%20to%20better%20handle%20unpredictable%20real-world%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04950v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEpistemic%2520Artificial%2520Intelligence%2520is%2520Essential%2520for%2520Machine%2520Learning%250A%2520%2520Models%2520to%2520Truly%2520%2560Know%2520When%2520They%2520Do%2520Not%2520Know%2527%26entry.906535625%3DShireen%2520Kudukkil%2520Manchingal%2520and%2520Andrew%2520Bradley%2520and%2520Julian%2520F.%2520P.%2520Kooij%2520and%2520Keivan%2520Shariatmadar%2520and%2520Neil%2520Yorke-Smith%2520and%2520Fabio%2520Cuzzolin%26entry.1292438233%3D%2520%2520Despite%2520AI%2527s%2520impressive%2520achievements%252C%2520including%2520recent%2520advances%2520in%2520generative%250Aand%2520large%2520language%2520models%252C%2520there%2520remains%2520a%2520significant%2520gap%2520in%2520the%2520ability%2520of%2520AI%250Asystems%2520to%2520handle%2520uncertainty%2520and%2520generalize%2520beyond%2520their%2520training%2520data.%2520AI%250Amodels%2520consistently%2520fail%2520to%2520make%2520robust%2520enough%2520predictions%2520when%2520facing%250Aunfamiliar%2520or%2520adversarial%2520data.%2520Traditional%2520machine%2520learning%2520approaches%250Astruggle%2520to%2520address%2520this%2520issue%252C%2520due%2520to%2520an%2520overemphasis%2520on%2520data%2520fitting%252C%2520while%250Acurrent%2520uncertainty%2520quantification%2520approaches%2520suffer%2520from%2520serious%2520limitations.%250AThis%2520position%2520paper%2520posits%2520a%2520paradigm%2520shift%2520towards%2520epistemic%2520artificial%250Aintelligence%252C%2520emphasizing%2520the%2520need%2520for%2520models%2520to%2520learn%2520from%2520what%2520they%2520know%250Awhile%2520at%2520the%2520same%2520time%2520acknowledging%2520their%2520ignorance%252C%2520using%2520the%2520mathematics%2520of%250Asecond-order%2520uncertainty%2520measures.%2520This%2520approach%252C%2520which%2520leverages%2520the%250Aexpressive%2520power%2520of%2520such%2520measures%2520to%2520efficiently%2520manage%2520uncertainty%252C%2520offers%2520an%250Aeffective%2520way%2520to%2520improve%2520the%2520resilience%2520and%2520robustness%2520of%2520AI%2520systems%252C%2520allowing%250Athem%2520to%2520better%2520handle%2520unpredictable%2520real-world%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04950v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Epistemic%20Artificial%20Intelligence%20is%20Essential%20for%20Machine%20Learning%0A%20%20Models%20to%20Truly%20%60Know%20When%20They%20Do%20Not%20Know%27&entry.906535625=Shireen%20Kudukkil%20Manchingal%20and%20Andrew%20Bradley%20and%20Julian%20F.%20P.%20Kooij%20and%20Keivan%20Shariatmadar%20and%20Neil%20Yorke-Smith%20and%20Fabio%20Cuzzolin&entry.1292438233=%20%20Despite%20AI%27s%20impressive%20achievements%2C%20including%20recent%20advances%20in%20generative%0Aand%20large%20language%20models%2C%20there%20remains%20a%20significant%20gap%20in%20the%20ability%20of%20AI%0Asystems%20to%20handle%20uncertainty%20and%20generalize%20beyond%20their%20training%20data.%20AI%0Amodels%20consistently%20fail%20to%20make%20robust%20enough%20predictions%20when%20facing%0Aunfamiliar%20or%20adversarial%20data.%20Traditional%20machine%20learning%20approaches%0Astruggle%20to%20address%20this%20issue%2C%20due%20to%20an%20overemphasis%20on%20data%20fitting%2C%20while%0Acurrent%20uncertainty%20quantification%20approaches%20suffer%20from%20serious%20limitations.%0AThis%20position%20paper%20posits%20a%20paradigm%20shift%20towards%20epistemic%20artificial%0Aintelligence%2C%20emphasizing%20the%20need%20for%20models%20to%20learn%20from%20what%20they%20know%0Awhile%20at%20the%20same%20time%20acknowledging%20their%20ignorance%2C%20using%20the%20mathematics%20of%0Asecond-order%20uncertainty%20measures.%20This%20approach%2C%20which%20leverages%20the%0Aexpressive%20power%20of%20such%20measures%20to%20efficiently%20manage%20uncertainty%2C%20offers%20an%0Aeffective%20way%20to%20improve%20the%20resilience%20and%20robustness%20of%20AI%20systems%2C%20allowing%0Athem%20to%20better%20handle%20unpredictable%20real-world%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04950v2&entry.124074799=Read"},
{"title": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper", "author": "Sargam Yadav and Asifa Mehmood Qureshi and Abhishek Kaushik and Shubham Sharma and Roisin Loughran and Subramaniam Kazhuparambil and Andrew Shaw and Mohammed Sabry and Niamh St John Lynch and . Nikhil Singh and Padraic O'Hara and Pranay Jaiswal and Roshan Chandru and David Lillis", "abstract": "  The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.\n", "link": "http://arxiv.org/abs/2503.07450v4", "date": "2025-06-13", "relevancy": 2.0046, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Idea%20to%20Implementation%3A%20Evaluating%20the%20Influence%20of%20Large%20Language%0A%20%20Models%20in%20Software%20Development%20--%20An%20Opinion%20Paper&body=Title%3A%20From%20Idea%20to%20Implementation%3A%20Evaluating%20the%20Influence%20of%20Large%20Language%0A%20%20Models%20in%20Software%20Development%20--%20An%20Opinion%20Paper%0AAuthor%3A%20Sargam%20Yadav%20and%20Asifa%20Mehmood%20Qureshi%20and%20Abhishek%20Kaushik%20and%20Shubham%20Sharma%20and%20Roisin%20Loughran%20and%20Subramaniam%20Kazhuparambil%20and%20Andrew%20Shaw%20and%20Mohammed%20Sabry%20and%20Niamh%20St%20John%20Lynch%20and%20.%20Nikhil%20Singh%20and%20Padraic%20O%27Hara%20and%20Pranay%20Jaiswal%20and%20Roshan%20Chandru%20and%20David%20Lillis%0AAbstract%3A%20%20%20The%20introduction%20of%20transformer%20architecture%20was%20a%20turning%20point%20in%20Natural%0ALanguage%20Processing%20%28NLP%29.%20Models%20based%20on%20the%20transformer%20architecture%20such%20as%0ABidirectional%20Encoder%20Representations%20from%20Transformers%20%28BERT%29%20and%20Generative%0APre-Trained%20Transformer%20%28GPT%29%20have%20gained%20widespread%20popularity%20in%20various%0Aapplications%20such%20as%20software%20development%20and%20education.%20The%20availability%20of%0ALarge%20Language%20Models%20%28LLMs%29%20such%20as%20ChatGPT%20and%20Bard%20to%20the%20general%20public%20has%0Ashowcased%20the%20tremendous%20potential%20of%20these%20models%20and%20encouraged%20their%0Aintegration%20into%20various%20domains%20such%20as%20software%20development%20for%20tasks%20such%20as%0Acode%20generation%2C%20debugging%2C%20and%20documentation%20generation.%20In%20this%20study%2C%0Aopinions%20from%2011%20experts%20regarding%20their%20experience%20with%20LLMs%20for%20software%0Adevelopment%20have%20been%20gathered%20and%20analysed%20to%20draw%20insights%20that%20can%20guide%0Asuccessful%20and%20responsible%20integration.%20The%20overall%20opinion%20of%20the%20experts%20is%0Apositive%2C%20with%20the%20experts%20identifying%20advantages%20such%20as%20increase%20in%0Aproductivity%20and%20reduced%20coding%20time.%20Potential%20concerns%20and%20challenges%20such%20as%0Arisk%20of%20over-dependence%20and%20ethical%20considerations%20have%20also%20been%20highlighted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07450v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Idea%2520to%2520Implementation%253A%2520Evaluating%2520the%2520Influence%2520of%2520Large%2520Language%250A%2520%2520Models%2520in%2520Software%2520Development%2520--%2520An%2520Opinion%2520Paper%26entry.906535625%3DSargam%2520Yadav%2520and%2520Asifa%2520Mehmood%2520Qureshi%2520and%2520Abhishek%2520Kaushik%2520and%2520Shubham%2520Sharma%2520and%2520Roisin%2520Loughran%2520and%2520Subramaniam%2520Kazhuparambil%2520and%2520Andrew%2520Shaw%2520and%2520Mohammed%2520Sabry%2520and%2520Niamh%2520St%2520John%2520Lynch%2520and%2520.%2520Nikhil%2520Singh%2520and%2520Padraic%2520O%2527Hara%2520and%2520Pranay%2520Jaiswal%2520and%2520Roshan%2520Chandru%2520and%2520David%2520Lillis%26entry.1292438233%3D%2520%2520The%2520introduction%2520of%2520transformer%2520architecture%2520was%2520a%2520turning%2520point%2520in%2520Natural%250ALanguage%2520Processing%2520%2528NLP%2529.%2520Models%2520based%2520on%2520the%2520transformer%2520architecture%2520such%2520as%250ABidirectional%2520Encoder%2520Representations%2520from%2520Transformers%2520%2528BERT%2529%2520and%2520Generative%250APre-Trained%2520Transformer%2520%2528GPT%2529%2520have%2520gained%2520widespread%2520popularity%2520in%2520various%250Aapplications%2520such%2520as%2520software%2520development%2520and%2520education.%2520The%2520availability%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520such%2520as%2520ChatGPT%2520and%2520Bard%2520to%2520the%2520general%2520public%2520has%250Ashowcased%2520the%2520tremendous%2520potential%2520of%2520these%2520models%2520and%2520encouraged%2520their%250Aintegration%2520into%2520various%2520domains%2520such%2520as%2520software%2520development%2520for%2520tasks%2520such%2520as%250Acode%2520generation%252C%2520debugging%252C%2520and%2520documentation%2520generation.%2520In%2520this%2520study%252C%250Aopinions%2520from%252011%2520experts%2520regarding%2520their%2520experience%2520with%2520LLMs%2520for%2520software%250Adevelopment%2520have%2520been%2520gathered%2520and%2520analysed%2520to%2520draw%2520insights%2520that%2520can%2520guide%250Asuccessful%2520and%2520responsible%2520integration.%2520The%2520overall%2520opinion%2520of%2520the%2520experts%2520is%250Apositive%252C%2520with%2520the%2520experts%2520identifying%2520advantages%2520such%2520as%2520increase%2520in%250Aproductivity%2520and%2520reduced%2520coding%2520time.%2520Potential%2520concerns%2520and%2520challenges%2520such%2520as%250Arisk%2520of%2520over-dependence%2520and%2520ethical%2520considerations%2520have%2520also%2520been%2520highlighted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07450v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Idea%20to%20Implementation%3A%20Evaluating%20the%20Influence%20of%20Large%20Language%0A%20%20Models%20in%20Software%20Development%20--%20An%20Opinion%20Paper&entry.906535625=Sargam%20Yadav%20and%20Asifa%20Mehmood%20Qureshi%20and%20Abhishek%20Kaushik%20and%20Shubham%20Sharma%20and%20Roisin%20Loughran%20and%20Subramaniam%20Kazhuparambil%20and%20Andrew%20Shaw%20and%20Mohammed%20Sabry%20and%20Niamh%20St%20John%20Lynch%20and%20.%20Nikhil%20Singh%20and%20Padraic%20O%27Hara%20and%20Pranay%20Jaiswal%20and%20Roshan%20Chandru%20and%20David%20Lillis&entry.1292438233=%20%20The%20introduction%20of%20transformer%20architecture%20was%20a%20turning%20point%20in%20Natural%0ALanguage%20Processing%20%28NLP%29.%20Models%20based%20on%20the%20transformer%20architecture%20such%20as%0ABidirectional%20Encoder%20Representations%20from%20Transformers%20%28BERT%29%20and%20Generative%0APre-Trained%20Transformer%20%28GPT%29%20have%20gained%20widespread%20popularity%20in%20various%0Aapplications%20such%20as%20software%20development%20and%20education.%20The%20availability%20of%0ALarge%20Language%20Models%20%28LLMs%29%20such%20as%20ChatGPT%20and%20Bard%20to%20the%20general%20public%20has%0Ashowcased%20the%20tremendous%20potential%20of%20these%20models%20and%20encouraged%20their%0Aintegration%20into%20various%20domains%20such%20as%20software%20development%20for%20tasks%20such%20as%0Acode%20generation%2C%20debugging%2C%20and%20documentation%20generation.%20In%20this%20study%2C%0Aopinions%20from%2011%20experts%20regarding%20their%20experience%20with%20LLMs%20for%20software%0Adevelopment%20have%20been%20gathered%20and%20analysed%20to%20draw%20insights%20that%20can%20guide%0Asuccessful%20and%20responsible%20integration.%20The%20overall%20opinion%20of%20the%20experts%20is%0Apositive%2C%20with%20the%20experts%20identifying%20advantages%20such%20as%20increase%20in%0Aproductivity%20and%20reduced%20coding%20time.%20Potential%20concerns%20and%20challenges%20such%20as%0Arisk%20of%20over-dependence%20and%20ethical%20considerations%20have%20also%20been%20highlighted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07450v4&entry.124074799=Read"},
{"title": "Neural Network Reprogrammability: A Unified Theme on Model\n  Reprogramming, Prompt Tuning, and Prompt Instruction", "author": "Zesheng Ye and Chengyi Cai and Ruijiang Dong and Jianzhong Qi and Lei Feng and Pin-Yu Chen and Feng Liu", "abstract": "  As large-scale pre-trained foundation models continue to expand in size and\ncapability, efficiently adapting them to specific downstream tasks has become\nincreasingly critical. Despite substantial progress, existing adaptation\napproaches have evolved largely in isolation, without a clear understanding of\ntheir interrelationships. This survey introduces neural network\nreprogrammability as a unifying framework that bridges mainstream model\nadaptation techniques--model reprogramming, prompt tuning, and prompt\ninstruction--previously fragmented research areas yet converges on a shared\nprinciple: repurposing a pre-trained model by manipulating information at the\ninterfaces while keeping the model parameters frozen. These methods exploit\nneural networks' sensitivity to manipulation on different interfaces, be it\nthrough perturbing inputs, inserting tokens into intermediate layers, or\nproviding task-specific examples in context, to redirect model behaviors\ntowards desired outcomes. We then present a taxonomy that categorizes such\ninformation manipulation-based adaptation approaches across four key\ndimensions: manipulation format (fixed or learnable), location (interfaces\nwhere manipulations occur), operator (how they are applied), and output\nalignment requirement (post-processing needed to align outputs with downstream\ntasks). Notably, this framework applies consistently across data modalities,\nindependent of specific model architectures. Moreover, viewing established\ntechniques like in-context learning and chain-of-thought prompting through this\nlens reveals both their theoretical connections and practical distinctions. We\nfurther analyze remaining technical challenges and ethical considerations,\npositioning neural network reprogrammability as a fundamental paradigm for\nefficient model adaptation. We lastly identify promising research directions\nemerging from this integrative viewpoint.\n", "link": "http://arxiv.org/abs/2506.04650v2", "date": "2025-06-13", "relevancy": 2.0045, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Network%20Reprogrammability%3A%20A%20Unified%20Theme%20on%20Model%0A%20%20Reprogramming%2C%20Prompt%20Tuning%2C%20and%20Prompt%20Instruction&body=Title%3A%20Neural%20Network%20Reprogrammability%3A%20A%20Unified%20Theme%20on%20Model%0A%20%20Reprogramming%2C%20Prompt%20Tuning%2C%20and%20Prompt%20Instruction%0AAuthor%3A%20Zesheng%20Ye%20and%20Chengyi%20Cai%20and%20Ruijiang%20Dong%20and%20Jianzhong%20Qi%20and%20Lei%20Feng%20and%20Pin-Yu%20Chen%20and%20Feng%20Liu%0AAbstract%3A%20%20%20As%20large-scale%20pre-trained%20foundation%20models%20continue%20to%20expand%20in%20size%20and%0Acapability%2C%20efficiently%20adapting%20them%20to%20specific%20downstream%20tasks%20has%20become%0Aincreasingly%20critical.%20Despite%20substantial%20progress%2C%20existing%20adaptation%0Aapproaches%20have%20evolved%20largely%20in%20isolation%2C%20without%20a%20clear%20understanding%20of%0Atheir%20interrelationships.%20This%20survey%20introduces%20neural%20network%0Areprogrammability%20as%20a%20unifying%20framework%20that%20bridges%20mainstream%20model%0Aadaptation%20techniques--model%20reprogramming%2C%20prompt%20tuning%2C%20and%20prompt%0Ainstruction--previously%20fragmented%20research%20areas%20yet%20converges%20on%20a%20shared%0Aprinciple%3A%20repurposing%20a%20pre-trained%20model%20by%20manipulating%20information%20at%20the%0Ainterfaces%20while%20keeping%20the%20model%20parameters%20frozen.%20These%20methods%20exploit%0Aneural%20networks%27%20sensitivity%20to%20manipulation%20on%20different%20interfaces%2C%20be%20it%0Athrough%20perturbing%20inputs%2C%20inserting%20tokens%20into%20intermediate%20layers%2C%20or%0Aproviding%20task-specific%20examples%20in%20context%2C%20to%20redirect%20model%20behaviors%0Atowards%20desired%20outcomes.%20We%20then%20present%20a%20taxonomy%20that%20categorizes%20such%0Ainformation%20manipulation-based%20adaptation%20approaches%20across%20four%20key%0Adimensions%3A%20manipulation%20format%20%28fixed%20or%20learnable%29%2C%20location%20%28interfaces%0Awhere%20manipulations%20occur%29%2C%20operator%20%28how%20they%20are%20applied%29%2C%20and%20output%0Aalignment%20requirement%20%28post-processing%20needed%20to%20align%20outputs%20with%20downstream%0Atasks%29.%20Notably%2C%20this%20framework%20applies%20consistently%20across%20data%20modalities%2C%0Aindependent%20of%20specific%20model%20architectures.%20Moreover%2C%20viewing%20established%0Atechniques%20like%20in-context%20learning%20and%20chain-of-thought%20prompting%20through%20this%0Alens%20reveals%20both%20their%20theoretical%20connections%20and%20practical%20distinctions.%20We%0Afurther%20analyze%20remaining%20technical%20challenges%20and%20ethical%20considerations%2C%0Apositioning%20neural%20network%20reprogrammability%20as%20a%20fundamental%20paradigm%20for%0Aefficient%20model%20adaptation.%20We%20lastly%20identify%20promising%20research%20directions%0Aemerging%20from%20this%20integrative%20viewpoint.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04650v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Network%2520Reprogrammability%253A%2520A%2520Unified%2520Theme%2520on%2520Model%250A%2520%2520Reprogramming%252C%2520Prompt%2520Tuning%252C%2520and%2520Prompt%2520Instruction%26entry.906535625%3DZesheng%2520Ye%2520and%2520Chengyi%2520Cai%2520and%2520Ruijiang%2520Dong%2520and%2520Jianzhong%2520Qi%2520and%2520Lei%2520Feng%2520and%2520Pin-Yu%2520Chen%2520and%2520Feng%2520Liu%26entry.1292438233%3D%2520%2520As%2520large-scale%2520pre-trained%2520foundation%2520models%2520continue%2520to%2520expand%2520in%2520size%2520and%250Acapability%252C%2520efficiently%2520adapting%2520them%2520to%2520specific%2520downstream%2520tasks%2520has%2520become%250Aincreasingly%2520critical.%2520Despite%2520substantial%2520progress%252C%2520existing%2520adaptation%250Aapproaches%2520have%2520evolved%2520largely%2520in%2520isolation%252C%2520without%2520a%2520clear%2520understanding%2520of%250Atheir%2520interrelationships.%2520This%2520survey%2520introduces%2520neural%2520network%250Areprogrammability%2520as%2520a%2520unifying%2520framework%2520that%2520bridges%2520mainstream%2520model%250Aadaptation%2520techniques--model%2520reprogramming%252C%2520prompt%2520tuning%252C%2520and%2520prompt%250Ainstruction--previously%2520fragmented%2520research%2520areas%2520yet%2520converges%2520on%2520a%2520shared%250Aprinciple%253A%2520repurposing%2520a%2520pre-trained%2520model%2520by%2520manipulating%2520information%2520at%2520the%250Ainterfaces%2520while%2520keeping%2520the%2520model%2520parameters%2520frozen.%2520These%2520methods%2520exploit%250Aneural%2520networks%2527%2520sensitivity%2520to%2520manipulation%2520on%2520different%2520interfaces%252C%2520be%2520it%250Athrough%2520perturbing%2520inputs%252C%2520inserting%2520tokens%2520into%2520intermediate%2520layers%252C%2520or%250Aproviding%2520task-specific%2520examples%2520in%2520context%252C%2520to%2520redirect%2520model%2520behaviors%250Atowards%2520desired%2520outcomes.%2520We%2520then%2520present%2520a%2520taxonomy%2520that%2520categorizes%2520such%250Ainformation%2520manipulation-based%2520adaptation%2520approaches%2520across%2520four%2520key%250Adimensions%253A%2520manipulation%2520format%2520%2528fixed%2520or%2520learnable%2529%252C%2520location%2520%2528interfaces%250Awhere%2520manipulations%2520occur%2529%252C%2520operator%2520%2528how%2520they%2520are%2520applied%2529%252C%2520and%2520output%250Aalignment%2520requirement%2520%2528post-processing%2520needed%2520to%2520align%2520outputs%2520with%2520downstream%250Atasks%2529.%2520Notably%252C%2520this%2520framework%2520applies%2520consistently%2520across%2520data%2520modalities%252C%250Aindependent%2520of%2520specific%2520model%2520architectures.%2520Moreover%252C%2520viewing%2520established%250Atechniques%2520like%2520in-context%2520learning%2520and%2520chain-of-thought%2520prompting%2520through%2520this%250Alens%2520reveals%2520both%2520their%2520theoretical%2520connections%2520and%2520practical%2520distinctions.%2520We%250Afurther%2520analyze%2520remaining%2520technical%2520challenges%2520and%2520ethical%2520considerations%252C%250Apositioning%2520neural%2520network%2520reprogrammability%2520as%2520a%2520fundamental%2520paradigm%2520for%250Aefficient%2520model%2520adaptation.%2520We%2520lastly%2520identify%2520promising%2520research%2520directions%250Aemerging%2520from%2520this%2520integrative%2520viewpoint.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04650v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Network%20Reprogrammability%3A%20A%20Unified%20Theme%20on%20Model%0A%20%20Reprogramming%2C%20Prompt%20Tuning%2C%20and%20Prompt%20Instruction&entry.906535625=Zesheng%20Ye%20and%20Chengyi%20Cai%20and%20Ruijiang%20Dong%20and%20Jianzhong%20Qi%20and%20Lei%20Feng%20and%20Pin-Yu%20Chen%20and%20Feng%20Liu&entry.1292438233=%20%20As%20large-scale%20pre-trained%20foundation%20models%20continue%20to%20expand%20in%20size%20and%0Acapability%2C%20efficiently%20adapting%20them%20to%20specific%20downstream%20tasks%20has%20become%0Aincreasingly%20critical.%20Despite%20substantial%20progress%2C%20existing%20adaptation%0Aapproaches%20have%20evolved%20largely%20in%20isolation%2C%20without%20a%20clear%20understanding%20of%0Atheir%20interrelationships.%20This%20survey%20introduces%20neural%20network%0Areprogrammability%20as%20a%20unifying%20framework%20that%20bridges%20mainstream%20model%0Aadaptation%20techniques--model%20reprogramming%2C%20prompt%20tuning%2C%20and%20prompt%0Ainstruction--previously%20fragmented%20research%20areas%20yet%20converges%20on%20a%20shared%0Aprinciple%3A%20repurposing%20a%20pre-trained%20model%20by%20manipulating%20information%20at%20the%0Ainterfaces%20while%20keeping%20the%20model%20parameters%20frozen.%20These%20methods%20exploit%0Aneural%20networks%27%20sensitivity%20to%20manipulation%20on%20different%20interfaces%2C%20be%20it%0Athrough%20perturbing%20inputs%2C%20inserting%20tokens%20into%20intermediate%20layers%2C%20or%0Aproviding%20task-specific%20examples%20in%20context%2C%20to%20redirect%20model%20behaviors%0Atowards%20desired%20outcomes.%20We%20then%20present%20a%20taxonomy%20that%20categorizes%20such%0Ainformation%20manipulation-based%20adaptation%20approaches%20across%20four%20key%0Adimensions%3A%20manipulation%20format%20%28fixed%20or%20learnable%29%2C%20location%20%28interfaces%0Awhere%20manipulations%20occur%29%2C%20operator%20%28how%20they%20are%20applied%29%2C%20and%20output%0Aalignment%20requirement%20%28post-processing%20needed%20to%20align%20outputs%20with%20downstream%0Atasks%29.%20Notably%2C%20this%20framework%20applies%20consistently%20across%20data%20modalities%2C%0Aindependent%20of%20specific%20model%20architectures.%20Moreover%2C%20viewing%20established%0Atechniques%20like%20in-context%20learning%20and%20chain-of-thought%20prompting%20through%20this%0Alens%20reveals%20both%20their%20theoretical%20connections%20and%20practical%20distinctions.%20We%0Afurther%20analyze%20remaining%20technical%20challenges%20and%20ethical%20considerations%2C%0Apositioning%20neural%20network%20reprogrammability%20as%20a%20fundamental%20paradigm%20for%0Aefficient%20model%20adaptation.%20We%20lastly%20identify%20promising%20research%20directions%0Aemerging%20from%20this%20integrative%20viewpoint.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04650v2&entry.124074799=Read"},
{"title": "Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation", "author": "Abhishek Jaiswal and Armeet Singh Luthra and Purav Jangir and Bhavya Garg and Nisheeth Srivastava", "abstract": "  Isometric exercises appeal to individuals seeking convenience, privacy, and\nminimal dependence on equipments. However, such fitness training is often\noverdependent on unreliable digital media content instead of expert\nsupervision, introducing serious risks, including incorrect posture, injury,\nand disengagement due to lack of corrective feedback. To address these\nchallenges, we present a real-time feedback system for assessing isometric\nposes. Our contributions include the release of the largest multiclass\nisometric exercise video dataset to date, comprising over 3,600 clips across\nsix poses with correct and incorrect variations. To support robust evaluation,\nwe benchmark state-of-the-art models-including graph-based networks-on this\ndataset and introduce a novel three-part metric that captures classification\naccuracy, mistake localization, and model confidence. Our results enhance the\nfeasibility of intelligent and personalized exercise training systems for home\nworkouts. This expert-level diagnosis, delivered directly to the users, also\nexpands the potential applications of these systems to rehabilitation,\nphysiotherapy, and various other fitness disciplines that involve physical\nmotion.\n", "link": "http://arxiv.org/abs/2506.11774v1", "date": "2025-06-13", "relevancy": 2.0039, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5037}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5015}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Feedback%20and%20Benchmark%20Dataset%20for%20Isometric%20Pose%20Evaluation&body=Title%3A%20Real-Time%20Feedback%20and%20Benchmark%20Dataset%20for%20Isometric%20Pose%20Evaluation%0AAuthor%3A%20Abhishek%20Jaiswal%20and%20Armeet%20Singh%20Luthra%20and%20Purav%20Jangir%20and%20Bhavya%20Garg%20and%20Nisheeth%20Srivastava%0AAbstract%3A%20%20%20Isometric%20exercises%20appeal%20to%20individuals%20seeking%20convenience%2C%20privacy%2C%20and%0Aminimal%20dependence%20on%20equipments.%20However%2C%20such%20fitness%20training%20is%20often%0Aoverdependent%20on%20unreliable%20digital%20media%20content%20instead%20of%20expert%0Asupervision%2C%20introducing%20serious%20risks%2C%20including%20incorrect%20posture%2C%20injury%2C%0Aand%20disengagement%20due%20to%20lack%20of%20corrective%20feedback.%20To%20address%20these%0Achallenges%2C%20we%20present%20a%20real-time%20feedback%20system%20for%20assessing%20isometric%0Aposes.%20Our%20contributions%20include%20the%20release%20of%20the%20largest%20multiclass%0Aisometric%20exercise%20video%20dataset%20to%20date%2C%20comprising%20over%203%2C600%20clips%20across%0Asix%20poses%20with%20correct%20and%20incorrect%20variations.%20To%20support%20robust%20evaluation%2C%0Awe%20benchmark%20state-of-the-art%20models-including%20graph-based%20networks-on%20this%0Adataset%20and%20introduce%20a%20novel%20three-part%20metric%20that%20captures%20classification%0Aaccuracy%2C%20mistake%20localization%2C%20and%20model%20confidence.%20Our%20results%20enhance%20the%0Afeasibility%20of%20intelligent%20and%20personalized%20exercise%20training%20systems%20for%20home%0Aworkouts.%20This%20expert-level%20diagnosis%2C%20delivered%20directly%20to%20the%20users%2C%20also%0Aexpands%20the%20potential%20applications%20of%20these%20systems%20to%20rehabilitation%2C%0Aphysiotherapy%2C%20and%20various%20other%20fitness%20disciplines%20that%20involve%20physical%0Amotion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Feedback%2520and%2520Benchmark%2520Dataset%2520for%2520Isometric%2520Pose%2520Evaluation%26entry.906535625%3DAbhishek%2520Jaiswal%2520and%2520Armeet%2520Singh%2520Luthra%2520and%2520Purav%2520Jangir%2520and%2520Bhavya%2520Garg%2520and%2520Nisheeth%2520Srivastava%26entry.1292438233%3D%2520%2520Isometric%2520exercises%2520appeal%2520to%2520individuals%2520seeking%2520convenience%252C%2520privacy%252C%2520and%250Aminimal%2520dependence%2520on%2520equipments.%2520However%252C%2520such%2520fitness%2520training%2520is%2520often%250Aoverdependent%2520on%2520unreliable%2520digital%2520media%2520content%2520instead%2520of%2520expert%250Asupervision%252C%2520introducing%2520serious%2520risks%252C%2520including%2520incorrect%2520posture%252C%2520injury%252C%250Aand%2520disengagement%2520due%2520to%2520lack%2520of%2520corrective%2520feedback.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520present%2520a%2520real-time%2520feedback%2520system%2520for%2520assessing%2520isometric%250Aposes.%2520Our%2520contributions%2520include%2520the%2520release%2520of%2520the%2520largest%2520multiclass%250Aisometric%2520exercise%2520video%2520dataset%2520to%2520date%252C%2520comprising%2520over%25203%252C600%2520clips%2520across%250Asix%2520poses%2520with%2520correct%2520and%2520incorrect%2520variations.%2520To%2520support%2520robust%2520evaluation%252C%250Awe%2520benchmark%2520state-of-the-art%2520models-including%2520graph-based%2520networks-on%2520this%250Adataset%2520and%2520introduce%2520a%2520novel%2520three-part%2520metric%2520that%2520captures%2520classification%250Aaccuracy%252C%2520mistake%2520localization%252C%2520and%2520model%2520confidence.%2520Our%2520results%2520enhance%2520the%250Afeasibility%2520of%2520intelligent%2520and%2520personalized%2520exercise%2520training%2520systems%2520for%2520home%250Aworkouts.%2520This%2520expert-level%2520diagnosis%252C%2520delivered%2520directly%2520to%2520the%2520users%252C%2520also%250Aexpands%2520the%2520potential%2520applications%2520of%2520these%2520systems%2520to%2520rehabilitation%252C%250Aphysiotherapy%252C%2520and%2520various%2520other%2520fitness%2520disciplines%2520that%2520involve%2520physical%250Amotion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Feedback%20and%20Benchmark%20Dataset%20for%20Isometric%20Pose%20Evaluation&entry.906535625=Abhishek%20Jaiswal%20and%20Armeet%20Singh%20Luthra%20and%20Purav%20Jangir%20and%20Bhavya%20Garg%20and%20Nisheeth%20Srivastava&entry.1292438233=%20%20Isometric%20exercises%20appeal%20to%20individuals%20seeking%20convenience%2C%20privacy%2C%20and%0Aminimal%20dependence%20on%20equipments.%20However%2C%20such%20fitness%20training%20is%20often%0Aoverdependent%20on%20unreliable%20digital%20media%20content%20instead%20of%20expert%0Asupervision%2C%20introducing%20serious%20risks%2C%20including%20incorrect%20posture%2C%20injury%2C%0Aand%20disengagement%20due%20to%20lack%20of%20corrective%20feedback.%20To%20address%20these%0Achallenges%2C%20we%20present%20a%20real-time%20feedback%20system%20for%20assessing%20isometric%0Aposes.%20Our%20contributions%20include%20the%20release%20of%20the%20largest%20multiclass%0Aisometric%20exercise%20video%20dataset%20to%20date%2C%20comprising%20over%203%2C600%20clips%20across%0Asix%20poses%20with%20correct%20and%20incorrect%20variations.%20To%20support%20robust%20evaluation%2C%0Awe%20benchmark%20state-of-the-art%20models-including%20graph-based%20networks-on%20this%0Adataset%20and%20introduce%20a%20novel%20three-part%20metric%20that%20captures%20classification%0Aaccuracy%2C%20mistake%20localization%2C%20and%20model%20confidence.%20Our%20results%20enhance%20the%0Afeasibility%20of%20intelligent%20and%20personalized%20exercise%20training%20systems%20for%20home%0Aworkouts.%20This%20expert-level%20diagnosis%2C%20delivered%20directly%20to%20the%20users%2C%20also%0Aexpands%20the%20potential%20applications%20of%20these%20systems%20to%20rehabilitation%2C%0Aphysiotherapy%2C%20and%20various%20other%20fitness%20disciplines%20that%20involve%20physical%0Amotion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11774v1&entry.124074799=Read"},
{"title": "How do Probabilistic Graphical Models and Graph Neural Networks Look at\n  Network Data?", "author": "Michela Lapenna and Caterina De Bacco", "abstract": "  Graphs are a powerful data structure for representing relational data and are\nwidely used to describe complex real-world systems. Probabilistic Graphical\nModels (PGMs) and Graph Neural Networks (GNNs) can both leverage\ngraph-structured data, but their inherent functioning is different. The\nquestion is how do they compare in capturing the information contained in\nnetworked datasets? We address this objective by solving a link prediction task\nand we conduct three main experiments, on both synthetic and real networks: one\nfocuses on how PGMs and GNNs handle input features, while the other two\ninvestigate their robustness to noisy features and increasing heterophily of\nthe graph. PGMs do not necessarily require features on nodes, while GNNs cannot\nexploit the network edges alone, and the choice of input features matters. We\nfind that GNNs are outperformed by PGMs when input features are low-dimensional\nor noisy, mimicking many real scenarios where node attributes might be scalar\nor noisy. Then, we find that PGMs are more robust than GNNs when the\nheterophily of the graph is increased. Finally, to assess performance beyond\nprediction tasks, we also compare the two frameworks in terms of their\ncomputational complexity and interpretability.\n", "link": "http://arxiv.org/abs/2506.11869v1", "date": "2025-06-13", "relevancy": 1.996, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5035}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4999}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20do%20Probabilistic%20Graphical%20Models%20and%20Graph%20Neural%20Networks%20Look%20at%0A%20%20Network%20Data%3F&body=Title%3A%20How%20do%20Probabilistic%20Graphical%20Models%20and%20Graph%20Neural%20Networks%20Look%20at%0A%20%20Network%20Data%3F%0AAuthor%3A%20Michela%20Lapenna%20and%20Caterina%20De%20Bacco%0AAbstract%3A%20%20%20Graphs%20are%20a%20powerful%20data%20structure%20for%20representing%20relational%20data%20and%20are%0Awidely%20used%20to%20describe%20complex%20real-world%20systems.%20Probabilistic%20Graphical%0AModels%20%28PGMs%29%20and%20Graph%20Neural%20Networks%20%28GNNs%29%20can%20both%20leverage%0Agraph-structured%20data%2C%20but%20their%20inherent%20functioning%20is%20different.%20The%0Aquestion%20is%20how%20do%20they%20compare%20in%20capturing%20the%20information%20contained%20in%0Anetworked%20datasets%3F%20We%20address%20this%20objective%20by%20solving%20a%20link%20prediction%20task%0Aand%20we%20conduct%20three%20main%20experiments%2C%20on%20both%20synthetic%20and%20real%20networks%3A%20one%0Afocuses%20on%20how%20PGMs%20and%20GNNs%20handle%20input%20features%2C%20while%20the%20other%20two%0Ainvestigate%20their%20robustness%20to%20noisy%20features%20and%20increasing%20heterophily%20of%0Athe%20graph.%20PGMs%20do%20not%20necessarily%20require%20features%20on%20nodes%2C%20while%20GNNs%20cannot%0Aexploit%20the%20network%20edges%20alone%2C%20and%20the%20choice%20of%20input%20features%20matters.%20We%0Afind%20that%20GNNs%20are%20outperformed%20by%20PGMs%20when%20input%20features%20are%20low-dimensional%0Aor%20noisy%2C%20mimicking%20many%20real%20scenarios%20where%20node%20attributes%20might%20be%20scalar%0Aor%20noisy.%20Then%2C%20we%20find%20that%20PGMs%20are%20more%20robust%20than%20GNNs%20when%20the%0Aheterophily%20of%20the%20graph%20is%20increased.%20Finally%2C%20to%20assess%20performance%20beyond%0Aprediction%20tasks%2C%20we%20also%20compare%20the%20two%20frameworks%20in%20terms%20of%20their%0Acomputational%20complexity%20and%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520do%2520Probabilistic%2520Graphical%2520Models%2520and%2520Graph%2520Neural%2520Networks%2520Look%2520at%250A%2520%2520Network%2520Data%253F%26entry.906535625%3DMichela%2520Lapenna%2520and%2520Caterina%2520De%2520Bacco%26entry.1292438233%3D%2520%2520Graphs%2520are%2520a%2520powerful%2520data%2520structure%2520for%2520representing%2520relational%2520data%2520and%2520are%250Awidely%2520used%2520to%2520describe%2520complex%2520real-world%2520systems.%2520Probabilistic%2520Graphical%250AModels%2520%2528PGMs%2529%2520and%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520can%2520both%2520leverage%250Agraph-structured%2520data%252C%2520but%2520their%2520inherent%2520functioning%2520is%2520different.%2520The%250Aquestion%2520is%2520how%2520do%2520they%2520compare%2520in%2520capturing%2520the%2520information%2520contained%2520in%250Anetworked%2520datasets%253F%2520We%2520address%2520this%2520objective%2520by%2520solving%2520a%2520link%2520prediction%2520task%250Aand%2520we%2520conduct%2520three%2520main%2520experiments%252C%2520on%2520both%2520synthetic%2520and%2520real%2520networks%253A%2520one%250Afocuses%2520on%2520how%2520PGMs%2520and%2520GNNs%2520handle%2520input%2520features%252C%2520while%2520the%2520other%2520two%250Ainvestigate%2520their%2520robustness%2520to%2520noisy%2520features%2520and%2520increasing%2520heterophily%2520of%250Athe%2520graph.%2520PGMs%2520do%2520not%2520necessarily%2520require%2520features%2520on%2520nodes%252C%2520while%2520GNNs%2520cannot%250Aexploit%2520the%2520network%2520edges%2520alone%252C%2520and%2520the%2520choice%2520of%2520input%2520features%2520matters.%2520We%250Afind%2520that%2520GNNs%2520are%2520outperformed%2520by%2520PGMs%2520when%2520input%2520features%2520are%2520low-dimensional%250Aor%2520noisy%252C%2520mimicking%2520many%2520real%2520scenarios%2520where%2520node%2520attributes%2520might%2520be%2520scalar%250Aor%2520noisy.%2520Then%252C%2520we%2520find%2520that%2520PGMs%2520are%2520more%2520robust%2520than%2520GNNs%2520when%2520the%250Aheterophily%2520of%2520the%2520graph%2520is%2520increased.%2520Finally%252C%2520to%2520assess%2520performance%2520beyond%250Aprediction%2520tasks%252C%2520we%2520also%2520compare%2520the%2520two%2520frameworks%2520in%2520terms%2520of%2520their%250Acomputational%2520complexity%2520and%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20do%20Probabilistic%20Graphical%20Models%20and%20Graph%20Neural%20Networks%20Look%20at%0A%20%20Network%20Data%3F&entry.906535625=Michela%20Lapenna%20and%20Caterina%20De%20Bacco&entry.1292438233=%20%20Graphs%20are%20a%20powerful%20data%20structure%20for%20representing%20relational%20data%20and%20are%0Awidely%20used%20to%20describe%20complex%20real-world%20systems.%20Probabilistic%20Graphical%0AModels%20%28PGMs%29%20and%20Graph%20Neural%20Networks%20%28GNNs%29%20can%20both%20leverage%0Agraph-structured%20data%2C%20but%20their%20inherent%20functioning%20is%20different.%20The%0Aquestion%20is%20how%20do%20they%20compare%20in%20capturing%20the%20information%20contained%20in%0Anetworked%20datasets%3F%20We%20address%20this%20objective%20by%20solving%20a%20link%20prediction%20task%0Aand%20we%20conduct%20three%20main%20experiments%2C%20on%20both%20synthetic%20and%20real%20networks%3A%20one%0Afocuses%20on%20how%20PGMs%20and%20GNNs%20handle%20input%20features%2C%20while%20the%20other%20two%0Ainvestigate%20their%20robustness%20to%20noisy%20features%20and%20increasing%20heterophily%20of%0Athe%20graph.%20PGMs%20do%20not%20necessarily%20require%20features%20on%20nodes%2C%20while%20GNNs%20cannot%0Aexploit%20the%20network%20edges%20alone%2C%20and%20the%20choice%20of%20input%20features%20matters.%20We%0Afind%20that%20GNNs%20are%20outperformed%20by%20PGMs%20when%20input%20features%20are%20low-dimensional%0Aor%20noisy%2C%20mimicking%20many%20real%20scenarios%20where%20node%20attributes%20might%20be%20scalar%0Aor%20noisy.%20Then%2C%20we%20find%20that%20PGMs%20are%20more%20robust%20than%20GNNs%20when%20the%0Aheterophily%20of%20the%20graph%20is%20increased.%20Finally%2C%20to%20assess%20performance%20beyond%0Aprediction%20tasks%2C%20we%20also%20compare%20the%20two%20frameworks%20in%20terms%20of%20their%0Acomputational%20complexity%20and%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11869v1&entry.124074799=Read"},
{"title": "PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill\n  Assessment", "author": "Edoardo Bianchi and Antonio Liotta", "abstract": "  Automated sports skill assessment requires capturing fundamental movement\npatterns that distinguish expert from novice performance, yet current video\nsampling methods disrupt the temporal continuity essential for proficiency\nevaluation. To this end, we introduce Proficiency-Aware Temporal Sampling\n(PATS), a novel sampling strategy that preserves complete fundamental movements\nwithin continuous temporal segments for multi-view skill assessment. PATS\nadaptively segments videos to ensure each analyzed portion contains full\nexecution of critical performance components, repeating this process across\nmultiple segments to maximize information coverage while maintaining temporal\ncoherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses\nthe state-of-the-art accuracy across all viewing configurations (+0.65% to\n+3.05%) and delivers substantial gains in challenging domains (+26.22%\nbouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that\nPATS successfully adapts to diverse activity characteristics-from\nhigh-frequency sampling for dynamic sports to fine-grained segmentation for\nsequential skills-demonstrating its effectiveness as an adaptive approach to\ntemporal sampling that advances automated skill assessment for real-world\napplications.\n", "link": "http://arxiv.org/abs/2506.04996v2", "date": "2025-06-13", "relevancy": 1.9931, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5171}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4955}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PATS%3A%20Proficiency-Aware%20Temporal%20Sampling%20for%20Multi-View%20Sports%20Skill%0A%20%20Assessment&body=Title%3A%20PATS%3A%20Proficiency-Aware%20Temporal%20Sampling%20for%20Multi-View%20Sports%20Skill%0A%20%20Assessment%0AAuthor%3A%20Edoardo%20Bianchi%20and%20Antonio%20Liotta%0AAbstract%3A%20%20%20Automated%20sports%20skill%20assessment%20requires%20capturing%20fundamental%20movement%0Apatterns%20that%20distinguish%20expert%20from%20novice%20performance%2C%20yet%20current%20video%0Asampling%20methods%20disrupt%20the%20temporal%20continuity%20essential%20for%20proficiency%0Aevaluation.%20To%20this%20end%2C%20we%20introduce%20Proficiency-Aware%20Temporal%20Sampling%0A%28PATS%29%2C%20a%20novel%20sampling%20strategy%20that%20preserves%20complete%20fundamental%20movements%0Awithin%20continuous%20temporal%20segments%20for%20multi-view%20skill%20assessment.%20PATS%0Aadaptively%20segments%20videos%20to%20ensure%20each%20analyzed%20portion%20contains%20full%0Aexecution%20of%20critical%20performance%20components%2C%20repeating%20this%20process%20across%0Amultiple%20segments%20to%20maximize%20information%20coverage%20while%20maintaining%20temporal%0Acoherence.%20Evaluated%20on%20the%20EgoExo4D%20benchmark%20with%20SkillFormer%2C%20PATS%20surpasses%0Athe%20state-of-the-art%20accuracy%20across%20all%20viewing%20configurations%20%28%2B0.65%25%20to%0A%2B3.05%25%29%20and%20delivers%20substantial%20gains%20in%20challenging%20domains%20%28%2B26.22%25%0Abouldering%2C%20%2B2.39%25%20music%2C%20%2B1.13%25%20basketball%29.%20Systematic%20analysis%20reveals%20that%0APATS%20successfully%20adapts%20to%20diverse%20activity%20characteristics-from%0Ahigh-frequency%20sampling%20for%20dynamic%20sports%20to%20fine-grained%20segmentation%20for%0Asequential%20skills-demonstrating%20its%20effectiveness%20as%20an%20adaptive%20approach%20to%0Atemporal%20sampling%20that%20advances%20automated%20skill%20assessment%20for%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04996v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPATS%253A%2520Proficiency-Aware%2520Temporal%2520Sampling%2520for%2520Multi-View%2520Sports%2520Skill%250A%2520%2520Assessment%26entry.906535625%3DEdoardo%2520Bianchi%2520and%2520Antonio%2520Liotta%26entry.1292438233%3D%2520%2520Automated%2520sports%2520skill%2520assessment%2520requires%2520capturing%2520fundamental%2520movement%250Apatterns%2520that%2520distinguish%2520expert%2520from%2520novice%2520performance%252C%2520yet%2520current%2520video%250Asampling%2520methods%2520disrupt%2520the%2520temporal%2520continuity%2520essential%2520for%2520proficiency%250Aevaluation.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Proficiency-Aware%2520Temporal%2520Sampling%250A%2528PATS%2529%252C%2520a%2520novel%2520sampling%2520strategy%2520that%2520preserves%2520complete%2520fundamental%2520movements%250Awithin%2520continuous%2520temporal%2520segments%2520for%2520multi-view%2520skill%2520assessment.%2520PATS%250Aadaptively%2520segments%2520videos%2520to%2520ensure%2520each%2520analyzed%2520portion%2520contains%2520full%250Aexecution%2520of%2520critical%2520performance%2520components%252C%2520repeating%2520this%2520process%2520across%250Amultiple%2520segments%2520to%2520maximize%2520information%2520coverage%2520while%2520maintaining%2520temporal%250Acoherence.%2520Evaluated%2520on%2520the%2520EgoExo4D%2520benchmark%2520with%2520SkillFormer%252C%2520PATS%2520surpasses%250Athe%2520state-of-the-art%2520accuracy%2520across%2520all%2520viewing%2520configurations%2520%2528%252B0.65%2525%2520to%250A%252B3.05%2525%2529%2520and%2520delivers%2520substantial%2520gains%2520in%2520challenging%2520domains%2520%2528%252B26.22%2525%250Abouldering%252C%2520%252B2.39%2525%2520music%252C%2520%252B1.13%2525%2520basketball%2529.%2520Systematic%2520analysis%2520reveals%2520that%250APATS%2520successfully%2520adapts%2520to%2520diverse%2520activity%2520characteristics-from%250Ahigh-frequency%2520sampling%2520for%2520dynamic%2520sports%2520to%2520fine-grained%2520segmentation%2520for%250Asequential%2520skills-demonstrating%2520its%2520effectiveness%2520as%2520an%2520adaptive%2520approach%2520to%250Atemporal%2520sampling%2520that%2520advances%2520automated%2520skill%2520assessment%2520for%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04996v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PATS%3A%20Proficiency-Aware%20Temporal%20Sampling%20for%20Multi-View%20Sports%20Skill%0A%20%20Assessment&entry.906535625=Edoardo%20Bianchi%20and%20Antonio%20Liotta&entry.1292438233=%20%20Automated%20sports%20skill%20assessment%20requires%20capturing%20fundamental%20movement%0Apatterns%20that%20distinguish%20expert%20from%20novice%20performance%2C%20yet%20current%20video%0Asampling%20methods%20disrupt%20the%20temporal%20continuity%20essential%20for%20proficiency%0Aevaluation.%20To%20this%20end%2C%20we%20introduce%20Proficiency-Aware%20Temporal%20Sampling%0A%28PATS%29%2C%20a%20novel%20sampling%20strategy%20that%20preserves%20complete%20fundamental%20movements%0Awithin%20continuous%20temporal%20segments%20for%20multi-view%20skill%20assessment.%20PATS%0Aadaptively%20segments%20videos%20to%20ensure%20each%20analyzed%20portion%20contains%20full%0Aexecution%20of%20critical%20performance%20components%2C%20repeating%20this%20process%20across%0Amultiple%20segments%20to%20maximize%20information%20coverage%20while%20maintaining%20temporal%0Acoherence.%20Evaluated%20on%20the%20EgoExo4D%20benchmark%20with%20SkillFormer%2C%20PATS%20surpasses%0Athe%20state-of-the-art%20accuracy%20across%20all%20viewing%20configurations%20%28%2B0.65%25%20to%0A%2B3.05%25%29%20and%20delivers%20substantial%20gains%20in%20challenging%20domains%20%28%2B26.22%25%0Abouldering%2C%20%2B2.39%25%20music%2C%20%2B1.13%25%20basketball%29.%20Systematic%20analysis%20reveals%20that%0APATS%20successfully%20adapts%20to%20diverse%20activity%20characteristics-from%0Ahigh-frequency%20sampling%20for%20dynamic%20sports%20to%20fine-grained%20segmentation%20for%0Asequential%20skills-demonstrating%20its%20effectiveness%20as%20an%20adaptive%20approach%20to%0Atemporal%20sampling%20that%20advances%20automated%20skill%20assessment%20for%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04996v2&entry.124074799=Read"},
{"title": "Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal", "author": "Yue Yao and Zelin Wen and Yan Tong and Xinyu Tian and Xuqing Li and Xiao Ma and Dongliang Xu and Tom Gedeon", "abstract": "  Test-time scaling offers a promising way to improve the reasoning performance\nof vision-language large models (VLLMs) without additional training. In this\npaper, we explore a simple but effective approach for applying test-time\nscaling to radiology report generation. Specifically, we introduce a\nlightweight Thought Graph Traversal (TGT) framework that guides the model to\nreason through organ-specific findings in a medically coherent order. This\nframework integrates structured medical priors into the prompt, enabling deeper\nand more logical analysis with no changes to the underlying model. To further\nenhance reasoning depth, we apply a reasoning budget forcing strategy that\nadjusts the model's inference depth at test time by dynamically extending its\ngeneration process. This simple yet powerful combination allows a frozen\nradiology VLLM to self-correct and generate more accurate, consistent chest\nX-ray reports. Our method outperforms baseline prompting approaches on standard\nbenchmarks, and also reveals dataset biases through traceable reasoning paths.\nCode and prompts are open-sourced for reproducibility at\nhttps://github.com/glerium/Thought-Graph-Traversal.\n", "link": "http://arxiv.org/abs/2506.11989v1", "date": "2025-06-13", "relevancy": 1.9914, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4992}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple%20Radiology%20VLLM%20Test-time%20Scaling%20with%20Thought%20Graph%20Traversal&body=Title%3A%20Simple%20Radiology%20VLLM%20Test-time%20Scaling%20with%20Thought%20Graph%20Traversal%0AAuthor%3A%20Yue%20Yao%20and%20Zelin%20Wen%20and%20Yan%20Tong%20and%20Xinyu%20Tian%20and%20Xuqing%20Li%20and%20Xiao%20Ma%20and%20Dongliang%20Xu%20and%20Tom%20Gedeon%0AAbstract%3A%20%20%20Test-time%20scaling%20offers%20a%20promising%20way%20to%20improve%20the%20reasoning%20performance%0Aof%20vision-language%20large%20models%20%28VLLMs%29%20without%20additional%20training.%20In%20this%0Apaper%2C%20we%20explore%20a%20simple%20but%20effective%20approach%20for%20applying%20test-time%0Ascaling%20to%20radiology%20report%20generation.%20Specifically%2C%20we%20introduce%20a%0Alightweight%20Thought%20Graph%20Traversal%20%28TGT%29%20framework%20that%20guides%20the%20model%20to%0Areason%20through%20organ-specific%20findings%20in%20a%20medically%20coherent%20order.%20This%0Aframework%20integrates%20structured%20medical%20priors%20into%20the%20prompt%2C%20enabling%20deeper%0Aand%20more%20logical%20analysis%20with%20no%20changes%20to%20the%20underlying%20model.%20To%20further%0Aenhance%20reasoning%20depth%2C%20we%20apply%20a%20reasoning%20budget%20forcing%20strategy%20that%0Aadjusts%20the%20model%27s%20inference%20depth%20at%20test%20time%20by%20dynamically%20extending%20its%0Ageneration%20process.%20This%20simple%20yet%20powerful%20combination%20allows%20a%20frozen%0Aradiology%20VLLM%20to%20self-correct%20and%20generate%20more%20accurate%2C%20consistent%20chest%0AX-ray%20reports.%20Our%20method%20outperforms%20baseline%20prompting%20approaches%20on%20standard%0Abenchmarks%2C%20and%20also%20reveals%20dataset%20biases%20through%20traceable%20reasoning%20paths.%0ACode%20and%20prompts%20are%20open-sourced%20for%20reproducibility%20at%0Ahttps%3A//github.com/glerium/Thought-Graph-Traversal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple%2520Radiology%2520VLLM%2520Test-time%2520Scaling%2520with%2520Thought%2520Graph%2520Traversal%26entry.906535625%3DYue%2520Yao%2520and%2520Zelin%2520Wen%2520and%2520Yan%2520Tong%2520and%2520Xinyu%2520Tian%2520and%2520Xuqing%2520Li%2520and%2520Xiao%2520Ma%2520and%2520Dongliang%2520Xu%2520and%2520Tom%2520Gedeon%26entry.1292438233%3D%2520%2520Test-time%2520scaling%2520offers%2520a%2520promising%2520way%2520to%2520improve%2520the%2520reasoning%2520performance%250Aof%2520vision-language%2520large%2520models%2520%2528VLLMs%2529%2520without%2520additional%2520training.%2520In%2520this%250Apaper%252C%2520we%2520explore%2520a%2520simple%2520but%2520effective%2520approach%2520for%2520applying%2520test-time%250Ascaling%2520to%2520radiology%2520report%2520generation.%2520Specifically%252C%2520we%2520introduce%2520a%250Alightweight%2520Thought%2520Graph%2520Traversal%2520%2528TGT%2529%2520framework%2520that%2520guides%2520the%2520model%2520to%250Areason%2520through%2520organ-specific%2520findings%2520in%2520a%2520medically%2520coherent%2520order.%2520This%250Aframework%2520integrates%2520structured%2520medical%2520priors%2520into%2520the%2520prompt%252C%2520enabling%2520deeper%250Aand%2520more%2520logical%2520analysis%2520with%2520no%2520changes%2520to%2520the%2520underlying%2520model.%2520To%2520further%250Aenhance%2520reasoning%2520depth%252C%2520we%2520apply%2520a%2520reasoning%2520budget%2520forcing%2520strategy%2520that%250Aadjusts%2520the%2520model%2527s%2520inference%2520depth%2520at%2520test%2520time%2520by%2520dynamically%2520extending%2520its%250Ageneration%2520process.%2520This%2520simple%2520yet%2520powerful%2520combination%2520allows%2520a%2520frozen%250Aradiology%2520VLLM%2520to%2520self-correct%2520and%2520generate%2520more%2520accurate%252C%2520consistent%2520chest%250AX-ray%2520reports.%2520Our%2520method%2520outperforms%2520baseline%2520prompting%2520approaches%2520on%2520standard%250Abenchmarks%252C%2520and%2520also%2520reveals%2520dataset%2520biases%2520through%2520traceable%2520reasoning%2520paths.%250ACode%2520and%2520prompts%2520are%2520open-sourced%2520for%2520reproducibility%2520at%250Ahttps%253A//github.com/glerium/Thought-Graph-Traversal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Radiology%20VLLM%20Test-time%20Scaling%20with%20Thought%20Graph%20Traversal&entry.906535625=Yue%20Yao%20and%20Zelin%20Wen%20and%20Yan%20Tong%20and%20Xinyu%20Tian%20and%20Xuqing%20Li%20and%20Xiao%20Ma%20and%20Dongliang%20Xu%20and%20Tom%20Gedeon&entry.1292438233=%20%20Test-time%20scaling%20offers%20a%20promising%20way%20to%20improve%20the%20reasoning%20performance%0Aof%20vision-language%20large%20models%20%28VLLMs%29%20without%20additional%20training.%20In%20this%0Apaper%2C%20we%20explore%20a%20simple%20but%20effective%20approach%20for%20applying%20test-time%0Ascaling%20to%20radiology%20report%20generation.%20Specifically%2C%20we%20introduce%20a%0Alightweight%20Thought%20Graph%20Traversal%20%28TGT%29%20framework%20that%20guides%20the%20model%20to%0Areason%20through%20organ-specific%20findings%20in%20a%20medically%20coherent%20order.%20This%0Aframework%20integrates%20structured%20medical%20priors%20into%20the%20prompt%2C%20enabling%20deeper%0Aand%20more%20logical%20analysis%20with%20no%20changes%20to%20the%20underlying%20model.%20To%20further%0Aenhance%20reasoning%20depth%2C%20we%20apply%20a%20reasoning%20budget%20forcing%20strategy%20that%0Aadjusts%20the%20model%27s%20inference%20depth%20at%20test%20time%20by%20dynamically%20extending%20its%0Ageneration%20process.%20This%20simple%20yet%20powerful%20combination%20allows%20a%20frozen%0Aradiology%20VLLM%20to%20self-correct%20and%20generate%20more%20accurate%2C%20consistent%20chest%0AX-ray%20reports.%20Our%20method%20outperforms%20baseline%20prompting%20approaches%20on%20standard%0Abenchmarks%2C%20and%20also%20reveals%20dataset%20biases%20through%20traceable%20reasoning%20paths.%0ACode%20and%20prompts%20are%20open-sourced%20for%20reproducibility%20at%0Ahttps%3A//github.com/glerium/Thought-Graph-Traversal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11989v1&entry.124074799=Read"},
{"title": "Enhancing multimodal analogical reasoning with Logic Augmented\n  Generation", "author": "Anna Sofia Lippolis and Andrea Giovanni Nuzzolese and Aldo Gangemi", "abstract": "  Recent advances in Large Language Models have demonstrated their capabilities\nacross a variety of tasks. However, automatically extracting implicit knowledge\nfrom natural language remains a significant challenge, as machines lack active\nexperience with the physical world. Given this scenario, semantic knowledge\ngraphs can serve as conceptual spaces that guide the automated text generation\nreasoning process to achieve more efficient and explainable results. In this\npaper, we apply a logic-augmented generation (LAG) framework that leverages the\nexplicit representation of a text through a semantic knowledge graph and\napplies it in combination with prompt heuristics to elicit implicit analogical\nconnections. This method generates extended knowledge graph triples\nrepresenting implicit meaning, enabling systems to reason on unlabeled\nmultimodal data regardless of the domain. We validate our work through three\nmetaphor detection and understanding tasks across four datasets, as they\nrequire deep analogical reasoning capabilities. The results show that this\nintegrated approach surpasses current baselines, performs better than humans in\nunderstanding visual metaphors, and enables more explainable reasoning\nprocesses, though still has inherent limitations in metaphor understanding,\nespecially for domain-specific metaphors. Furthermore, we propose a thorough\nerror analysis, discussing issues with metaphorical annotations and current\nevaluation methods.\n", "link": "http://arxiv.org/abs/2504.11190v2", "date": "2025-06-13", "relevancy": 1.5825, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5503}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5225}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20multimodal%20analogical%20reasoning%20with%20Logic%20Augmented%0A%20%20Generation&body=Title%3A%20Enhancing%20multimodal%20analogical%20reasoning%20with%20Logic%20Augmented%0A%20%20Generation%0AAuthor%3A%20Anna%20Sofia%20Lippolis%20and%20Andrea%20Giovanni%20Nuzzolese%20and%20Aldo%20Gangemi%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20have%20demonstrated%20their%20capabilities%0Aacross%20a%20variety%20of%20tasks.%20However%2C%20automatically%20extracting%20implicit%20knowledge%0Afrom%20natural%20language%20remains%20a%20significant%20challenge%2C%20as%20machines%20lack%20active%0Aexperience%20with%20the%20physical%20world.%20Given%20this%20scenario%2C%20semantic%20knowledge%0Agraphs%20can%20serve%20as%20conceptual%20spaces%20that%20guide%20the%20automated%20text%20generation%0Areasoning%20process%20to%20achieve%20more%20efficient%20and%20explainable%20results.%20In%20this%0Apaper%2C%20we%20apply%20a%20logic-augmented%20generation%20%28LAG%29%20framework%20that%20leverages%20the%0Aexplicit%20representation%20of%20a%20text%20through%20a%20semantic%20knowledge%20graph%20and%0Aapplies%20it%20in%20combination%20with%20prompt%20heuristics%20to%20elicit%20implicit%20analogical%0Aconnections.%20This%20method%20generates%20extended%20knowledge%20graph%20triples%0Arepresenting%20implicit%20meaning%2C%20enabling%20systems%20to%20reason%20on%20unlabeled%0Amultimodal%20data%20regardless%20of%20the%20domain.%20We%20validate%20our%20work%20through%20three%0Ametaphor%20detection%20and%20understanding%20tasks%20across%20four%20datasets%2C%20as%20they%0Arequire%20deep%20analogical%20reasoning%20capabilities.%20The%20results%20show%20that%20this%0Aintegrated%20approach%20surpasses%20current%20baselines%2C%20performs%20better%20than%20humans%20in%0Aunderstanding%20visual%20metaphors%2C%20and%20enables%20more%20explainable%20reasoning%0Aprocesses%2C%20though%20still%20has%20inherent%20limitations%20in%20metaphor%20understanding%2C%0Aespecially%20for%20domain-specific%20metaphors.%20Furthermore%2C%20we%20propose%20a%20thorough%0Aerror%20analysis%2C%20discussing%20issues%20with%20metaphorical%20annotations%20and%20current%0Aevaluation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11190v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520multimodal%2520analogical%2520reasoning%2520with%2520Logic%2520Augmented%250A%2520%2520Generation%26entry.906535625%3DAnna%2520Sofia%2520Lippolis%2520and%2520Andrea%2520Giovanni%2520Nuzzolese%2520and%2520Aldo%2520Gangemi%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520have%2520demonstrated%2520their%2520capabilities%250Aacross%2520a%2520variety%2520of%2520tasks.%2520However%252C%2520automatically%2520extracting%2520implicit%2520knowledge%250Afrom%2520natural%2520language%2520remains%2520a%2520significant%2520challenge%252C%2520as%2520machines%2520lack%2520active%250Aexperience%2520with%2520the%2520physical%2520world.%2520Given%2520this%2520scenario%252C%2520semantic%2520knowledge%250Agraphs%2520can%2520serve%2520as%2520conceptual%2520spaces%2520that%2520guide%2520the%2520automated%2520text%2520generation%250Areasoning%2520process%2520to%2520achieve%2520more%2520efficient%2520and%2520explainable%2520results.%2520In%2520this%250Apaper%252C%2520we%2520apply%2520a%2520logic-augmented%2520generation%2520%2528LAG%2529%2520framework%2520that%2520leverages%2520the%250Aexplicit%2520representation%2520of%2520a%2520text%2520through%2520a%2520semantic%2520knowledge%2520graph%2520and%250Aapplies%2520it%2520in%2520combination%2520with%2520prompt%2520heuristics%2520to%2520elicit%2520implicit%2520analogical%250Aconnections.%2520This%2520method%2520generates%2520extended%2520knowledge%2520graph%2520triples%250Arepresenting%2520implicit%2520meaning%252C%2520enabling%2520systems%2520to%2520reason%2520on%2520unlabeled%250Amultimodal%2520data%2520regardless%2520of%2520the%2520domain.%2520We%2520validate%2520our%2520work%2520through%2520three%250Ametaphor%2520detection%2520and%2520understanding%2520tasks%2520across%2520four%2520datasets%252C%2520as%2520they%250Arequire%2520deep%2520analogical%2520reasoning%2520capabilities.%2520The%2520results%2520show%2520that%2520this%250Aintegrated%2520approach%2520surpasses%2520current%2520baselines%252C%2520performs%2520better%2520than%2520humans%2520in%250Aunderstanding%2520visual%2520metaphors%252C%2520and%2520enables%2520more%2520explainable%2520reasoning%250Aprocesses%252C%2520though%2520still%2520has%2520inherent%2520limitations%2520in%2520metaphor%2520understanding%252C%250Aespecially%2520for%2520domain-specific%2520metaphors.%2520Furthermore%252C%2520we%2520propose%2520a%2520thorough%250Aerror%2520analysis%252C%2520discussing%2520issues%2520with%2520metaphorical%2520annotations%2520and%2520current%250Aevaluation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11190v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20multimodal%20analogical%20reasoning%20with%20Logic%20Augmented%0A%20%20Generation&entry.906535625=Anna%20Sofia%20Lippolis%20and%20Andrea%20Giovanni%20Nuzzolese%20and%20Aldo%20Gangemi&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20have%20demonstrated%20their%20capabilities%0Aacross%20a%20variety%20of%20tasks.%20However%2C%20automatically%20extracting%20implicit%20knowledge%0Afrom%20natural%20language%20remains%20a%20significant%20challenge%2C%20as%20machines%20lack%20active%0Aexperience%20with%20the%20physical%20world.%20Given%20this%20scenario%2C%20semantic%20knowledge%0Agraphs%20can%20serve%20as%20conceptual%20spaces%20that%20guide%20the%20automated%20text%20generation%0Areasoning%20process%20to%20achieve%20more%20efficient%20and%20explainable%20results.%20In%20this%0Apaper%2C%20we%20apply%20a%20logic-augmented%20generation%20%28LAG%29%20framework%20that%20leverages%20the%0Aexplicit%20representation%20of%20a%20text%20through%20a%20semantic%20knowledge%20graph%20and%0Aapplies%20it%20in%20combination%20with%20prompt%20heuristics%20to%20elicit%20implicit%20analogical%0Aconnections.%20This%20method%20generates%20extended%20knowledge%20graph%20triples%0Arepresenting%20implicit%20meaning%2C%20enabling%20systems%20to%20reason%20on%20unlabeled%0Amultimodal%20data%20regardless%20of%20the%20domain.%20We%20validate%20our%20work%20through%20three%0Ametaphor%20detection%20and%20understanding%20tasks%20across%20four%20datasets%2C%20as%20they%0Arequire%20deep%20analogical%20reasoning%20capabilities.%20The%20results%20show%20that%20this%0Aintegrated%20approach%20surpasses%20current%20baselines%2C%20performs%20better%20than%20humans%20in%0Aunderstanding%20visual%20metaphors%2C%20and%20enables%20more%20explainable%20reasoning%0Aprocesses%2C%20though%20still%20has%20inherent%20limitations%20in%20metaphor%20understanding%2C%0Aespecially%20for%20domain-specific%20metaphors.%20Furthermore%2C%20we%20propose%20a%20thorough%0Aerror%20analysis%2C%20discussing%20issues%20with%20metaphorical%20annotations%20and%20current%0Aevaluation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11190v2&entry.124074799=Read"},
{"title": "Converting Annotated Clinical Cases into Structured Case Report Forms", "author": "Pietro Ferrazzi and Alberto Lavelli and Bernardo Magnini", "abstract": "  Case Report Forms (CRFs) are largely used in medical research as they ensure\naccuracy, reliability, and validity of results in clinical studies. However,\npublicly available, wellannotated CRF datasets are scarce, limiting the\ndevelopment of CRF slot filling systems able to fill in a CRF from clinical\nnotes. To mitigate the scarcity of CRF datasets, we propose to take advantage\nof available datasets annotated for information extraction tasks and to convert\nthem into structured CRFs. We present a semi-automatic conversion methodology,\nwhich has been applied to the E3C dataset in two languages (English and\nItalian), resulting in a new, high-quality dataset for CRF slot filling.\nThrough several experiments on the created dataset, we report that slot filling\nachieves 59.7% for Italian and 67.3% for English on a closed Large Language\nModels (zero-shot) and worse performances on three families of open-source\nmodels, showing that filling CRFs is challenging even for recent\nstate-of-the-art LLMs. We release the datest at\nhttps://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166\n", "link": "http://arxiv.org/abs/2506.11666v1", "date": "2025-06-13", "relevancy": 1.7193, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Converting%20Annotated%20Clinical%20Cases%20into%20Structured%20Case%20Report%20Forms&body=Title%3A%20Converting%20Annotated%20Clinical%20Cases%20into%20Structured%20Case%20Report%20Forms%0AAuthor%3A%20Pietro%20Ferrazzi%20and%20Alberto%20Lavelli%20and%20Bernardo%20Magnini%0AAbstract%3A%20%20%20Case%20Report%20Forms%20%28CRFs%29%20are%20largely%20used%20in%20medical%20research%20as%20they%20ensure%0Aaccuracy%2C%20reliability%2C%20and%20validity%20of%20results%20in%20clinical%20studies.%20However%2C%0Apublicly%20available%2C%20wellannotated%20CRF%20datasets%20are%20scarce%2C%20limiting%20the%0Adevelopment%20of%20CRF%20slot%20filling%20systems%20able%20to%20fill%20in%20a%20CRF%20from%20clinical%0Anotes.%20To%20mitigate%20the%20scarcity%20of%20CRF%20datasets%2C%20we%20propose%20to%20take%20advantage%0Aof%20available%20datasets%20annotated%20for%20information%20extraction%20tasks%20and%20to%20convert%0Athem%20into%20structured%20CRFs.%20We%20present%20a%20semi-automatic%20conversion%20methodology%2C%0Awhich%20has%20been%20applied%20to%20the%20E3C%20dataset%20in%20two%20languages%20%28English%20and%0AItalian%29%2C%20resulting%20in%20a%20new%2C%20high-quality%20dataset%20for%20CRF%20slot%20filling.%0AThrough%20several%20experiments%20on%20the%20created%20dataset%2C%20we%20report%20that%20slot%20filling%0Aachieves%2059.7%25%20for%20Italian%20and%2067.3%25%20for%20English%20on%20a%20closed%20Large%20Language%0AModels%20%28zero-shot%29%20and%20worse%20performances%20on%20three%20families%20of%20open-source%0Amodels%2C%20showing%20that%20filling%20CRFs%20is%20challenging%20even%20for%20recent%0Astate-of-the-art%20LLMs.%20We%20release%20the%20datest%20at%0Ahttps%3A//huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConverting%2520Annotated%2520Clinical%2520Cases%2520into%2520Structured%2520Case%2520Report%2520Forms%26entry.906535625%3DPietro%2520Ferrazzi%2520and%2520Alberto%2520Lavelli%2520and%2520Bernardo%2520Magnini%26entry.1292438233%3D%2520%2520Case%2520Report%2520Forms%2520%2528CRFs%2529%2520are%2520largely%2520used%2520in%2520medical%2520research%2520as%2520they%2520ensure%250Aaccuracy%252C%2520reliability%252C%2520and%2520validity%2520of%2520results%2520in%2520clinical%2520studies.%2520However%252C%250Apublicly%2520available%252C%2520wellannotated%2520CRF%2520datasets%2520are%2520scarce%252C%2520limiting%2520the%250Adevelopment%2520of%2520CRF%2520slot%2520filling%2520systems%2520able%2520to%2520fill%2520in%2520a%2520CRF%2520from%2520clinical%250Anotes.%2520To%2520mitigate%2520the%2520scarcity%2520of%2520CRF%2520datasets%252C%2520we%2520propose%2520to%2520take%2520advantage%250Aof%2520available%2520datasets%2520annotated%2520for%2520information%2520extraction%2520tasks%2520and%2520to%2520convert%250Athem%2520into%2520structured%2520CRFs.%2520We%2520present%2520a%2520semi-automatic%2520conversion%2520methodology%252C%250Awhich%2520has%2520been%2520applied%2520to%2520the%2520E3C%2520dataset%2520in%2520two%2520languages%2520%2528English%2520and%250AItalian%2529%252C%2520resulting%2520in%2520a%2520new%252C%2520high-quality%2520dataset%2520for%2520CRF%2520slot%2520filling.%250AThrough%2520several%2520experiments%2520on%2520the%2520created%2520dataset%252C%2520we%2520report%2520that%2520slot%2520filling%250Aachieves%252059.7%2525%2520for%2520Italian%2520and%252067.3%2525%2520for%2520English%2520on%2520a%2520closed%2520Large%2520Language%250AModels%2520%2528zero-shot%2529%2520and%2520worse%2520performances%2520on%2520three%2520families%2520of%2520open-source%250Amodels%252C%2520showing%2520that%2520filling%2520CRFs%2520is%2520challenging%2520even%2520for%2520recent%250Astate-of-the-art%2520LLMs.%2520We%2520release%2520the%2520datest%2520at%250Ahttps%253A//huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Converting%20Annotated%20Clinical%20Cases%20into%20Structured%20Case%20Report%20Forms&entry.906535625=Pietro%20Ferrazzi%20and%20Alberto%20Lavelli%20and%20Bernardo%20Magnini&entry.1292438233=%20%20Case%20Report%20Forms%20%28CRFs%29%20are%20largely%20used%20in%20medical%20research%20as%20they%20ensure%0Aaccuracy%2C%20reliability%2C%20and%20validity%20of%20results%20in%20clinical%20studies.%20However%2C%0Apublicly%20available%2C%20wellannotated%20CRF%20datasets%20are%20scarce%2C%20limiting%20the%0Adevelopment%20of%20CRF%20slot%20filling%20systems%20able%20to%20fill%20in%20a%20CRF%20from%20clinical%0Anotes.%20To%20mitigate%20the%20scarcity%20of%20CRF%20datasets%2C%20we%20propose%20to%20take%20advantage%0Aof%20available%20datasets%20annotated%20for%20information%20extraction%20tasks%20and%20to%20convert%0Athem%20into%20structured%20CRFs.%20We%20present%20a%20semi-automatic%20conversion%20methodology%2C%0Awhich%20has%20been%20applied%20to%20the%20E3C%20dataset%20in%20two%20languages%20%28English%20and%0AItalian%29%2C%20resulting%20in%20a%20new%2C%20high-quality%20dataset%20for%20CRF%20slot%20filling.%0AThrough%20several%20experiments%20on%20the%20created%20dataset%2C%20we%20report%20that%20slot%20filling%0Aachieves%2059.7%25%20for%20Italian%20and%2067.3%25%20for%20English%20on%20a%20closed%20Large%20Language%0AModels%20%28zero-shot%29%20and%20worse%20performances%20on%20three%20families%20of%20open-source%0Amodels%2C%20showing%20that%20filling%20CRFs%20is%20challenging%20even%20for%20recent%0Astate-of-the-art%20LLMs.%20We%20release%20the%20datest%20at%0Ahttps%3A//huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11666v1&entry.124074799=Read"},
{"title": "Approximating Fixpoints of Approximated Functions", "author": "Paolo Baldan and Sebastian Gurke and Barbara K\u00f6nig and Tommaso Padoan and Florian Wittbold", "abstract": "  Fixpoints are ubiquitous in computer science and when dealing with\nquantitative semantics and verification one often considers least fixpoints of\n(higher-dimensional) functions over the non-negative reals. We show how to\napproximate the least fixpoint of such functions, focusing on the case in which\nthey are not known precisely, but represented by a sequence of approximating\nfunctions that converge to them. We concentrate on monotone and non-expansive\nfunctions, for which uniqueness of fixpoints is not guaranteed and standard\nfixpoint iteration schemes might get stuck at a fixpoint that is not the least.\nOur main contribution is the identification of an iteration scheme, a variation\nof Mann iteration with a dampening factor, which, under suitable conditions, is\nshown to guarantee convergence to the least fixpoint of the function of\ninterest. We then argue that these results are relevant in the context of\nmodel-based reinforcement learning for Markov decision processes, showing how\nthe proposed iteration scheme instantiates and allows us to derive convergence\nto the optimal expected return. More generally, we show that our results can be\nused to iterate to the least fixpoint almost surely for systems where the\nfunction of interest can be approximated with given probabilistic error bounds,\nas it happens for probabilistic systems, such as simple stochastic games, which\ncan be explored via sampling.\n", "link": "http://arxiv.org/abs/2501.08950v2", "date": "2025-06-13", "relevancy": 1.7757, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4569}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4526}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approximating%20Fixpoints%20of%20Approximated%20Functions&body=Title%3A%20Approximating%20Fixpoints%20of%20Approximated%20Functions%0AAuthor%3A%20Paolo%20Baldan%20and%20Sebastian%20Gurke%20and%20Barbara%20K%C3%B6nig%20and%20Tommaso%20Padoan%20and%20Florian%20Wittbold%0AAbstract%3A%20%20%20Fixpoints%20are%20ubiquitous%20in%20computer%20science%20and%20when%20dealing%20with%0Aquantitative%20semantics%20and%20verification%20one%20often%20considers%20least%20fixpoints%20of%0A%28higher-dimensional%29%20functions%20over%20the%20non-negative%20reals.%20We%20show%20how%20to%0Aapproximate%20the%20least%20fixpoint%20of%20such%20functions%2C%20focusing%20on%20the%20case%20in%20which%0Athey%20are%20not%20known%20precisely%2C%20but%20represented%20by%20a%20sequence%20of%20approximating%0Afunctions%20that%20converge%20to%20them.%20We%20concentrate%20on%20monotone%20and%20non-expansive%0Afunctions%2C%20for%20which%20uniqueness%20of%20fixpoints%20is%20not%20guaranteed%20and%20standard%0Afixpoint%20iteration%20schemes%20might%20get%20stuck%20at%20a%20fixpoint%20that%20is%20not%20the%20least.%0AOur%20main%20contribution%20is%20the%20identification%20of%20an%20iteration%20scheme%2C%20a%20variation%0Aof%20Mann%20iteration%20with%20a%20dampening%20factor%2C%20which%2C%20under%20suitable%20conditions%2C%20is%0Ashown%20to%20guarantee%20convergence%20to%20the%20least%20fixpoint%20of%20the%20function%20of%0Ainterest.%20We%20then%20argue%20that%20these%20results%20are%20relevant%20in%20the%20context%20of%0Amodel-based%20reinforcement%20learning%20for%20Markov%20decision%20processes%2C%20showing%20how%0Athe%20proposed%20iteration%20scheme%20instantiates%20and%20allows%20us%20to%20derive%20convergence%0Ato%20the%20optimal%20expected%20return.%20More%20generally%2C%20we%20show%20that%20our%20results%20can%20be%0Aused%20to%20iterate%20to%20the%20least%20fixpoint%20almost%20surely%20for%20systems%20where%20the%0Afunction%20of%20interest%20can%20be%20approximated%20with%20given%20probabilistic%20error%20bounds%2C%0Aas%20it%20happens%20for%20probabilistic%20systems%2C%20such%20as%20simple%20stochastic%20games%2C%20which%0Acan%20be%20explored%20via%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08950v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproximating%2520Fixpoints%2520of%2520Approximated%2520Functions%26entry.906535625%3DPaolo%2520Baldan%2520and%2520Sebastian%2520Gurke%2520and%2520Barbara%2520K%25C3%25B6nig%2520and%2520Tommaso%2520Padoan%2520and%2520Florian%2520Wittbold%26entry.1292438233%3D%2520%2520Fixpoints%2520are%2520ubiquitous%2520in%2520computer%2520science%2520and%2520when%2520dealing%2520with%250Aquantitative%2520semantics%2520and%2520verification%2520one%2520often%2520considers%2520least%2520fixpoints%2520of%250A%2528higher-dimensional%2529%2520functions%2520over%2520the%2520non-negative%2520reals.%2520We%2520show%2520how%2520to%250Aapproximate%2520the%2520least%2520fixpoint%2520of%2520such%2520functions%252C%2520focusing%2520on%2520the%2520case%2520in%2520which%250Athey%2520are%2520not%2520known%2520precisely%252C%2520but%2520represented%2520by%2520a%2520sequence%2520of%2520approximating%250Afunctions%2520that%2520converge%2520to%2520them.%2520We%2520concentrate%2520on%2520monotone%2520and%2520non-expansive%250Afunctions%252C%2520for%2520which%2520uniqueness%2520of%2520fixpoints%2520is%2520not%2520guaranteed%2520and%2520standard%250Afixpoint%2520iteration%2520schemes%2520might%2520get%2520stuck%2520at%2520a%2520fixpoint%2520that%2520is%2520not%2520the%2520least.%250AOur%2520main%2520contribution%2520is%2520the%2520identification%2520of%2520an%2520iteration%2520scheme%252C%2520a%2520variation%250Aof%2520Mann%2520iteration%2520with%2520a%2520dampening%2520factor%252C%2520which%252C%2520under%2520suitable%2520conditions%252C%2520is%250Ashown%2520to%2520guarantee%2520convergence%2520to%2520the%2520least%2520fixpoint%2520of%2520the%2520function%2520of%250Ainterest.%2520We%2520then%2520argue%2520that%2520these%2520results%2520are%2520relevant%2520in%2520the%2520context%2520of%250Amodel-based%2520reinforcement%2520learning%2520for%2520Markov%2520decision%2520processes%252C%2520showing%2520how%250Athe%2520proposed%2520iteration%2520scheme%2520instantiates%2520and%2520allows%2520us%2520to%2520derive%2520convergence%250Ato%2520the%2520optimal%2520expected%2520return.%2520More%2520generally%252C%2520we%2520show%2520that%2520our%2520results%2520can%2520be%250Aused%2520to%2520iterate%2520to%2520the%2520least%2520fixpoint%2520almost%2520surely%2520for%2520systems%2520where%2520the%250Afunction%2520of%2520interest%2520can%2520be%2520approximated%2520with%2520given%2520probabilistic%2520error%2520bounds%252C%250Aas%2520it%2520happens%2520for%2520probabilistic%2520systems%252C%2520such%2520as%2520simple%2520stochastic%2520games%252C%2520which%250Acan%2520be%2520explored%2520via%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08950v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximating%20Fixpoints%20of%20Approximated%20Functions&entry.906535625=Paolo%20Baldan%20and%20Sebastian%20Gurke%20and%20Barbara%20K%C3%B6nig%20and%20Tommaso%20Padoan%20and%20Florian%20Wittbold&entry.1292438233=%20%20Fixpoints%20are%20ubiquitous%20in%20computer%20science%20and%20when%20dealing%20with%0Aquantitative%20semantics%20and%20verification%20one%20often%20considers%20least%20fixpoints%20of%0A%28higher-dimensional%29%20functions%20over%20the%20non-negative%20reals.%20We%20show%20how%20to%0Aapproximate%20the%20least%20fixpoint%20of%20such%20functions%2C%20focusing%20on%20the%20case%20in%20which%0Athey%20are%20not%20known%20precisely%2C%20but%20represented%20by%20a%20sequence%20of%20approximating%0Afunctions%20that%20converge%20to%20them.%20We%20concentrate%20on%20monotone%20and%20non-expansive%0Afunctions%2C%20for%20which%20uniqueness%20of%20fixpoints%20is%20not%20guaranteed%20and%20standard%0Afixpoint%20iteration%20schemes%20might%20get%20stuck%20at%20a%20fixpoint%20that%20is%20not%20the%20least.%0AOur%20main%20contribution%20is%20the%20identification%20of%20an%20iteration%20scheme%2C%20a%20variation%0Aof%20Mann%20iteration%20with%20a%20dampening%20factor%2C%20which%2C%20under%20suitable%20conditions%2C%20is%0Ashown%20to%20guarantee%20convergence%20to%20the%20least%20fixpoint%20of%20the%20function%20of%0Ainterest.%20We%20then%20argue%20that%20these%20results%20are%20relevant%20in%20the%20context%20of%0Amodel-based%20reinforcement%20learning%20for%20Markov%20decision%20processes%2C%20showing%20how%0Athe%20proposed%20iteration%20scheme%20instantiates%20and%20allows%20us%20to%20derive%20convergence%0Ato%20the%20optimal%20expected%20return.%20More%20generally%2C%20we%20show%20that%20our%20results%20can%20be%0Aused%20to%20iterate%20to%20the%20least%20fixpoint%20almost%20surely%20for%20systems%20where%20the%0Afunction%20of%20interest%20can%20be%20approximated%20with%20given%20probabilistic%20error%20bounds%2C%0Aas%20it%20happens%20for%20probabilistic%20systems%2C%20such%20as%20simple%20stochastic%20games%2C%20which%0Acan%20be%20explored%20via%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08950v2&entry.124074799=Read"},
{"title": "Differential Privacy in Machine Learning: From Symbolic AI to LLMs", "author": "Francisco Aguilera-Mart\u00ednez and Fernando Berzal", "abstract": "  Machine learning models should not reveal particular information that is not\notherwise accessible. Differential privacy provides a formal framework to\nmitigate privacy risks by ensuring that the inclusion or exclusion of any\nsingle data point does not significantly alter the output of an algorithm, thus\nlimiting the exposure of private information. This survey paper explores the\nfoundational definitions of differential privacy, reviews its original\nformulations and tracing its evolution through key research contributions. It\nthen provides an in-depth examination of how DP has been integrated into\nmachine learning models, analyzing existing proposals and methods to preserve\nprivacy when training ML models. Finally, it describes how DP-based ML\ntechniques can be evaluated in practice. %Finally, it discusses the broader\nimplications of DP, highlighting its potential for public benefit, its\nreal-world applications, and the challenges it faces, including vulnerabilities\nto adversarial attacks. By offering a comprehensive overview of differential\nprivacy in machine learning, this work aims to contribute to the ongoing\ndevelopment of secure and responsible AI systems.\n", "link": "http://arxiv.org/abs/2506.11687v1", "date": "2025-06-13", "relevancy": 1.2911, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4651}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4268}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differential%20Privacy%20in%20Machine%20Learning%3A%20From%20Symbolic%20AI%20to%20LLMs&body=Title%3A%20Differential%20Privacy%20in%20Machine%20Learning%3A%20From%20Symbolic%20AI%20to%20LLMs%0AAuthor%3A%20Francisco%20Aguilera-Mart%C3%ADnez%20and%20Fernando%20Berzal%0AAbstract%3A%20%20%20Machine%20learning%20models%20should%20not%20reveal%20particular%20information%20that%20is%20not%0Aotherwise%20accessible.%20Differential%20privacy%20provides%20a%20formal%20framework%20to%0Amitigate%20privacy%20risks%20by%20ensuring%20that%20the%20inclusion%20or%20exclusion%20of%20any%0Asingle%20data%20point%20does%20not%20significantly%20alter%20the%20output%20of%20an%20algorithm%2C%20thus%0Alimiting%20the%20exposure%20of%20private%20information.%20This%20survey%20paper%20explores%20the%0Afoundational%20definitions%20of%20differential%20privacy%2C%20reviews%20its%20original%0Aformulations%20and%20tracing%20its%20evolution%20through%20key%20research%20contributions.%20It%0Athen%20provides%20an%20in-depth%20examination%20of%20how%20DP%20has%20been%20integrated%20into%0Amachine%20learning%20models%2C%20analyzing%20existing%20proposals%20and%20methods%20to%20preserve%0Aprivacy%20when%20training%20ML%20models.%20Finally%2C%20it%20describes%20how%20DP-based%20ML%0Atechniques%20can%20be%20evaluated%20in%20practice.%20%25Finally%2C%20it%20discusses%20the%20broader%0Aimplications%20of%20DP%2C%20highlighting%20its%20potential%20for%20public%20benefit%2C%20its%0Areal-world%20applications%2C%20and%20the%20challenges%20it%20faces%2C%20including%20vulnerabilities%0Ato%20adversarial%20attacks.%20By%20offering%20a%20comprehensive%20overview%20of%20differential%0Aprivacy%20in%20machine%20learning%2C%20this%20work%20aims%20to%20contribute%20to%20the%20ongoing%0Adevelopment%20of%20secure%20and%20responsible%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferential%2520Privacy%2520in%2520Machine%2520Learning%253A%2520From%2520Symbolic%2520AI%2520to%2520LLMs%26entry.906535625%3DFrancisco%2520Aguilera-Mart%25C3%25ADnez%2520and%2520Fernando%2520Berzal%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520should%2520not%2520reveal%2520particular%2520information%2520that%2520is%2520not%250Aotherwise%2520accessible.%2520Differential%2520privacy%2520provides%2520a%2520formal%2520framework%2520to%250Amitigate%2520privacy%2520risks%2520by%2520ensuring%2520that%2520the%2520inclusion%2520or%2520exclusion%2520of%2520any%250Asingle%2520data%2520point%2520does%2520not%2520significantly%2520alter%2520the%2520output%2520of%2520an%2520algorithm%252C%2520thus%250Alimiting%2520the%2520exposure%2520of%2520private%2520information.%2520This%2520survey%2520paper%2520explores%2520the%250Afoundational%2520definitions%2520of%2520differential%2520privacy%252C%2520reviews%2520its%2520original%250Aformulations%2520and%2520tracing%2520its%2520evolution%2520through%2520key%2520research%2520contributions.%2520It%250Athen%2520provides%2520an%2520in-depth%2520examination%2520of%2520how%2520DP%2520has%2520been%2520integrated%2520into%250Amachine%2520learning%2520models%252C%2520analyzing%2520existing%2520proposals%2520and%2520methods%2520to%2520preserve%250Aprivacy%2520when%2520training%2520ML%2520models.%2520Finally%252C%2520it%2520describes%2520how%2520DP-based%2520ML%250Atechniques%2520can%2520be%2520evaluated%2520in%2520practice.%2520%2525Finally%252C%2520it%2520discusses%2520the%2520broader%250Aimplications%2520of%2520DP%252C%2520highlighting%2520its%2520potential%2520for%2520public%2520benefit%252C%2520its%250Areal-world%2520applications%252C%2520and%2520the%2520challenges%2520it%2520faces%252C%2520including%2520vulnerabilities%250Ato%2520adversarial%2520attacks.%2520By%2520offering%2520a%2520comprehensive%2520overview%2520of%2520differential%250Aprivacy%2520in%2520machine%2520learning%252C%2520this%2520work%2520aims%2520to%2520contribute%2520to%2520the%2520ongoing%250Adevelopment%2520of%2520secure%2520and%2520responsible%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differential%20Privacy%20in%20Machine%20Learning%3A%20From%20Symbolic%20AI%20to%20LLMs&entry.906535625=Francisco%20Aguilera-Mart%C3%ADnez%20and%20Fernando%20Berzal&entry.1292438233=%20%20Machine%20learning%20models%20should%20not%20reveal%20particular%20information%20that%20is%20not%0Aotherwise%20accessible.%20Differential%20privacy%20provides%20a%20formal%20framework%20to%0Amitigate%20privacy%20risks%20by%20ensuring%20that%20the%20inclusion%20or%20exclusion%20of%20any%0Asingle%20data%20point%20does%20not%20significantly%20alter%20the%20output%20of%20an%20algorithm%2C%20thus%0Alimiting%20the%20exposure%20of%20private%20information.%20This%20survey%20paper%20explores%20the%0Afoundational%20definitions%20of%20differential%20privacy%2C%20reviews%20its%20original%0Aformulations%20and%20tracing%20its%20evolution%20through%20key%20research%20contributions.%20It%0Athen%20provides%20an%20in-depth%20examination%20of%20how%20DP%20has%20been%20integrated%20into%0Amachine%20learning%20models%2C%20analyzing%20existing%20proposals%20and%20methods%20to%20preserve%0Aprivacy%20when%20training%20ML%20models.%20Finally%2C%20it%20describes%20how%20DP-based%20ML%0Atechniques%20can%20be%20evaluated%20in%20practice.%20%25Finally%2C%20it%20discusses%20the%20broader%0Aimplications%20of%20DP%2C%20highlighting%20its%20potential%20for%20public%20benefit%2C%20its%0Areal-world%20applications%2C%20and%20the%20challenges%20it%20faces%2C%20including%20vulnerabilities%0Ato%20adversarial%20attacks.%20By%20offering%20a%20comprehensive%20overview%20of%20differential%0Aprivacy%20in%20machine%20learning%2C%20this%20work%20aims%20to%20contribute%20to%20the%20ongoing%0Adevelopment%20of%20secure%20and%20responsible%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11687v1&entry.124074799=Read"},
{"title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data", "author": "Jina Kim and Jeffrey Willette and Bruno Andreis and Sung Ju Hwang", "abstract": "  A widely recognized limitation of molecular prediction models is their\nreliance on structures observed in the training data, resulting in poor\ngeneralization to out-of-distribution compounds. Yet in drug discovery, the\ncompounds most critical for advancing research often lie beyond the training\nset, making the bias toward the training data particularly problematic. This\nmismatch introduces substantial covariate shift, under which standard deep\nlearning models produce unstable and inaccurate predictions. Furthermore, the\nscarcity of labeled data, stemming from the onerous and costly nature of\nexperimental validation, further exacerbates the difficulty of achieving\nreliable generalization. To address these limitations, we propose a novel\nmeta-learning-based approach that leverages unlabeled data to interpolate\nbetween in-distribution (ID) and out-of-distribution (OOD) data, enabling the\nmodel to meta-learn how to generalize beyond the training distribution. We\ndemonstrate significant performance gains over state-of-the-art methods on\nchallenging real-world datasets that exhibit substantial covariate shift.\n", "link": "http://arxiv.org/abs/2506.11877v1", "date": "2025-06-13", "relevancy": 1.0447, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5544}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5436}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Molecular%20Property%20Prediction%20via%20Densifying%20Scarce%20Labeled%20Data&body=Title%3A%20Robust%20Molecular%20Property%20Prediction%20via%20Densifying%20Scarce%20Labeled%20Data%0AAuthor%3A%20Jina%20Kim%20and%20Jeffrey%20Willette%20and%20Bruno%20Andreis%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20A%20widely%20recognized%20limitation%20of%20molecular%20prediction%20models%20is%20their%0Areliance%20on%20structures%20observed%20in%20the%20training%20data%2C%20resulting%20in%20poor%0Ageneralization%20to%20out-of-distribution%20compounds.%20Yet%20in%20drug%20discovery%2C%20the%0Acompounds%20most%20critical%20for%20advancing%20research%20often%20lie%20beyond%20the%20training%0Aset%2C%20making%20the%20bias%20toward%20the%20training%20data%20particularly%20problematic.%20This%0Amismatch%20introduces%20substantial%20covariate%20shift%2C%20under%20which%20standard%20deep%0Alearning%20models%20produce%20unstable%20and%20inaccurate%20predictions.%20Furthermore%2C%20the%0Ascarcity%20of%20labeled%20data%2C%20stemming%20from%20the%20onerous%20and%20costly%20nature%20of%0Aexperimental%20validation%2C%20further%20exacerbates%20the%20difficulty%20of%20achieving%0Areliable%20generalization.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Ameta-learning-based%20approach%20that%20leverages%20unlabeled%20data%20to%20interpolate%0Abetween%20in-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%20data%2C%20enabling%20the%0Amodel%20to%20meta-learn%20how%20to%20generalize%20beyond%20the%20training%20distribution.%20We%0Ademonstrate%20significant%20performance%20gains%20over%20state-of-the-art%20methods%20on%0Achallenging%20real-world%20datasets%20that%20exhibit%20substantial%20covariate%20shift.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Molecular%2520Property%2520Prediction%2520via%2520Densifying%2520Scarce%2520Labeled%2520Data%26entry.906535625%3DJina%2520Kim%2520and%2520Jeffrey%2520Willette%2520and%2520Bruno%2520Andreis%2520and%2520Sung%2520Ju%2520Hwang%26entry.1292438233%3D%2520%2520A%2520widely%2520recognized%2520limitation%2520of%2520molecular%2520prediction%2520models%2520is%2520their%250Areliance%2520on%2520structures%2520observed%2520in%2520the%2520training%2520data%252C%2520resulting%2520in%2520poor%250Ageneralization%2520to%2520out-of-distribution%2520compounds.%2520Yet%2520in%2520drug%2520discovery%252C%2520the%250Acompounds%2520most%2520critical%2520for%2520advancing%2520research%2520often%2520lie%2520beyond%2520the%2520training%250Aset%252C%2520making%2520the%2520bias%2520toward%2520the%2520training%2520data%2520particularly%2520problematic.%2520This%250Amismatch%2520introduces%2520substantial%2520covariate%2520shift%252C%2520under%2520which%2520standard%2520deep%250Alearning%2520models%2520produce%2520unstable%2520and%2520inaccurate%2520predictions.%2520Furthermore%252C%2520the%250Ascarcity%2520of%2520labeled%2520data%252C%2520stemming%2520from%2520the%2520onerous%2520and%2520costly%2520nature%2520of%250Aexperimental%2520validation%252C%2520further%2520exacerbates%2520the%2520difficulty%2520of%2520achieving%250Areliable%2520generalization.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%250Ameta-learning-based%2520approach%2520that%2520leverages%2520unlabeled%2520data%2520to%2520interpolate%250Abetween%2520in-distribution%2520%2528ID%2529%2520and%2520out-of-distribution%2520%2528OOD%2529%2520data%252C%2520enabling%2520the%250Amodel%2520to%2520meta-learn%2520how%2520to%2520generalize%2520beyond%2520the%2520training%2520distribution.%2520We%250Ademonstrate%2520significant%2520performance%2520gains%2520over%2520state-of-the-art%2520methods%2520on%250Achallenging%2520real-world%2520datasets%2520that%2520exhibit%2520substantial%2520covariate%2520shift.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Molecular%20Property%20Prediction%20via%20Densifying%20Scarce%20Labeled%20Data&entry.906535625=Jina%20Kim%20and%20Jeffrey%20Willette%20and%20Bruno%20Andreis%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20A%20widely%20recognized%20limitation%20of%20molecular%20prediction%20models%20is%20their%0Areliance%20on%20structures%20observed%20in%20the%20training%20data%2C%20resulting%20in%20poor%0Ageneralization%20to%20out-of-distribution%20compounds.%20Yet%20in%20drug%20discovery%2C%20the%0Acompounds%20most%20critical%20for%20advancing%20research%20often%20lie%20beyond%20the%20training%0Aset%2C%20making%20the%20bias%20toward%20the%20training%20data%20particularly%20problematic.%20This%0Amismatch%20introduces%20substantial%20covariate%20shift%2C%20under%20which%20standard%20deep%0Alearning%20models%20produce%20unstable%20and%20inaccurate%20predictions.%20Furthermore%2C%20the%0Ascarcity%20of%20labeled%20data%2C%20stemming%20from%20the%20onerous%20and%20costly%20nature%20of%0Aexperimental%20validation%2C%20further%20exacerbates%20the%20difficulty%20of%20achieving%0Areliable%20generalization.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Ameta-learning-based%20approach%20that%20leverages%20unlabeled%20data%20to%20interpolate%0Abetween%20in-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%20data%2C%20enabling%20the%0Amodel%20to%20meta-learn%20how%20to%20generalize%20beyond%20the%20training%20distribution.%20We%0Ademonstrate%20significant%20performance%20gains%20over%20state-of-the-art%20methods%20on%0Achallenging%20real-world%20datasets%20that%20exhibit%20substantial%20covariate%20shift.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11877v1&entry.124074799=Read"},
{"title": "Schema-R1: A reasoning training approach for schema linking in\n  Text-to-SQL Task", "author": "Wuzhenghong Wen and Su Pan and yuwei Sun", "abstract": "  Schema linking is a critical step in Text-to-SQL task, aiming to accurately\npredict the table names and column names required for the SQL query based on\nthe given question. However, current fine-tuning approaches for schema linking\nmodels employ a rote-learning paradigm, excessively optimizing for ground truth\nschema linking outcomes while compromising reasoning ability. This limitation\narises because of the difficulty in acquiring a high-quality reasoning sample\nfor downstream tasks. To address this, we propose Schema-R1, a reasoning schema\nlinking model trained using reinforcement learning. Specifically, Schema-R1\nconsists of three key steps: constructing small batches of high-quality\nreasoning samples, supervised fine-tuning for cold-start initialization, and\nrule-based reinforcement learning training. The final results demonstrate that\nour method effectively enhances the reasoning ability of the schema linking\nmodel, achieving a 10\\% improvement in filter accuracy compared to the existing\nmethod. Our code is available at https://github.com/hongWin/Schema-R1/.\n", "link": "http://arxiv.org/abs/2506.11986v1", "date": "2025-06-13", "relevancy": 1.8579, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4687}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4687}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Schema-R1%3A%20A%20reasoning%20training%20approach%20for%20schema%20linking%20in%0A%20%20Text-to-SQL%20Task&body=Title%3A%20Schema-R1%3A%20A%20reasoning%20training%20approach%20for%20schema%20linking%20in%0A%20%20Text-to-SQL%20Task%0AAuthor%3A%20Wuzhenghong%20Wen%20and%20Su%20Pan%20and%20yuwei%20Sun%0AAbstract%3A%20%20%20Schema%20linking%20is%20a%20critical%20step%20in%20Text-to-SQL%20task%2C%20aiming%20to%20accurately%0Apredict%20the%20table%20names%20and%20column%20names%20required%20for%20the%20SQL%20query%20based%20on%0Athe%20given%20question.%20However%2C%20current%20fine-tuning%20approaches%20for%20schema%20linking%0Amodels%20employ%20a%20rote-learning%20paradigm%2C%20excessively%20optimizing%20for%20ground%20truth%0Aschema%20linking%20outcomes%20while%20compromising%20reasoning%20ability.%20This%20limitation%0Aarises%20because%20of%20the%20difficulty%20in%20acquiring%20a%20high-quality%20reasoning%20sample%0Afor%20downstream%20tasks.%20To%20address%20this%2C%20we%20propose%20Schema-R1%2C%20a%20reasoning%20schema%0Alinking%20model%20trained%20using%20reinforcement%20learning.%20Specifically%2C%20Schema-R1%0Aconsists%20of%20three%20key%20steps%3A%20constructing%20small%20batches%20of%20high-quality%0Areasoning%20samples%2C%20supervised%20fine-tuning%20for%20cold-start%20initialization%2C%20and%0Arule-based%20reinforcement%20learning%20training.%20The%20final%20results%20demonstrate%20that%0Aour%20method%20effectively%20enhances%20the%20reasoning%20ability%20of%20the%20schema%20linking%0Amodel%2C%20achieving%20a%2010%5C%25%20improvement%20in%20filter%20accuracy%20compared%20to%20the%20existing%0Amethod.%20Our%20code%20is%20available%20at%20https%3A//github.com/hongWin/Schema-R1/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSchema-R1%253A%2520A%2520reasoning%2520training%2520approach%2520for%2520schema%2520linking%2520in%250A%2520%2520Text-to-SQL%2520Task%26entry.906535625%3DWuzhenghong%2520Wen%2520and%2520Su%2520Pan%2520and%2520yuwei%2520Sun%26entry.1292438233%3D%2520%2520Schema%2520linking%2520is%2520a%2520critical%2520step%2520in%2520Text-to-SQL%2520task%252C%2520aiming%2520to%2520accurately%250Apredict%2520the%2520table%2520names%2520and%2520column%2520names%2520required%2520for%2520the%2520SQL%2520query%2520based%2520on%250Athe%2520given%2520question.%2520However%252C%2520current%2520fine-tuning%2520approaches%2520for%2520schema%2520linking%250Amodels%2520employ%2520a%2520rote-learning%2520paradigm%252C%2520excessively%2520optimizing%2520for%2520ground%2520truth%250Aschema%2520linking%2520outcomes%2520while%2520compromising%2520reasoning%2520ability.%2520This%2520limitation%250Aarises%2520because%2520of%2520the%2520difficulty%2520in%2520acquiring%2520a%2520high-quality%2520reasoning%2520sample%250Afor%2520downstream%2520tasks.%2520To%2520address%2520this%252C%2520we%2520propose%2520Schema-R1%252C%2520a%2520reasoning%2520schema%250Alinking%2520model%2520trained%2520using%2520reinforcement%2520learning.%2520Specifically%252C%2520Schema-R1%250Aconsists%2520of%2520three%2520key%2520steps%253A%2520constructing%2520small%2520batches%2520of%2520high-quality%250Areasoning%2520samples%252C%2520supervised%2520fine-tuning%2520for%2520cold-start%2520initialization%252C%2520and%250Arule-based%2520reinforcement%2520learning%2520training.%2520The%2520final%2520results%2520demonstrate%2520that%250Aour%2520method%2520effectively%2520enhances%2520the%2520reasoning%2520ability%2520of%2520the%2520schema%2520linking%250Amodel%252C%2520achieving%2520a%252010%255C%2525%2520improvement%2520in%2520filter%2520accuracy%2520compared%2520to%2520the%2520existing%250Amethod.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/hongWin/Schema-R1/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Schema-R1%3A%20A%20reasoning%20training%20approach%20for%20schema%20linking%20in%0A%20%20Text-to-SQL%20Task&entry.906535625=Wuzhenghong%20Wen%20and%20Su%20Pan%20and%20yuwei%20Sun&entry.1292438233=%20%20Schema%20linking%20is%20a%20critical%20step%20in%20Text-to-SQL%20task%2C%20aiming%20to%20accurately%0Apredict%20the%20table%20names%20and%20column%20names%20required%20for%20the%20SQL%20query%20based%20on%0Athe%20given%20question.%20However%2C%20current%20fine-tuning%20approaches%20for%20schema%20linking%0Amodels%20employ%20a%20rote-learning%20paradigm%2C%20excessively%20optimizing%20for%20ground%20truth%0Aschema%20linking%20outcomes%20while%20compromising%20reasoning%20ability.%20This%20limitation%0Aarises%20because%20of%20the%20difficulty%20in%20acquiring%20a%20high-quality%20reasoning%20sample%0Afor%20downstream%20tasks.%20To%20address%20this%2C%20we%20propose%20Schema-R1%2C%20a%20reasoning%20schema%0Alinking%20model%20trained%20using%20reinforcement%20learning.%20Specifically%2C%20Schema-R1%0Aconsists%20of%20three%20key%20steps%3A%20constructing%20small%20batches%20of%20high-quality%0Areasoning%20samples%2C%20supervised%20fine-tuning%20for%20cold-start%20initialization%2C%20and%0Arule-based%20reinforcement%20learning%20training.%20The%20final%20results%20demonstrate%20that%0Aour%20method%20effectively%20enhances%20the%20reasoning%20ability%20of%20the%20schema%20linking%0Amodel%2C%20achieving%20a%2010%5C%25%20improvement%20in%20filter%20accuracy%20compared%20to%20the%20existing%0Amethod.%20Our%20code%20is%20available%20at%20https%3A//github.com/hongWin/Schema-R1/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11986v1&entry.124074799=Read"},
{"title": "Taxonomy of reduction matrices for Graph Coarsening", "author": "Antonin Joly and Nicolas Keriven and Aline Roumy", "abstract": "  Graph coarsening aims to diminish the size of a graph to lighten its memory\nfootprint, and has numerous applications in graph signal processing and machine\nlearning. It is usually defined using a reduction matrix and a lifting matrix,\nwhich, respectively, allows to project a graph signal from the original graph\nto the coarsened one and back. This results in a loss of information measured\nby the so-called Restricted Spectral Approximation (RSA). Most coarsening\nframeworks impose a fixed relationship between the reduction and lifting\nmatrices, generally as pseudo-inverses of each other, and seek to define a\ncoarsening that minimizes the RSA. In this paper, we remark that the roles of\nthese two matrices are not entirely symmetric: indeed, putting constraints on\nthe lifting matrix alone ensures the existence of important objects such as the\ncoarsened graph's adjacency matrix or Laplacian. In light of this, in this\npaper, we introduce a more general notion of reduction matrix, that is not\nnecessarily the pseudo-inverse of the lifting matrix. We establish a taxonomy\nof ``admissible'' families of reduction matrices, discuss the different\nproperties that they must satisfy and whether they admit a closed-form\ndescription or not. We show that, for a fixed coarsening represented by a fixed\nlifting matrix, the RSA can be further reduced simply by modifying the\nreduction matrix. We explore different examples, including some based on a\nconstrained optimization process of the RSA. Since this criterion has also been\nlinked to the performance of Graph Neural Networks, we also illustrate the\nimpact of this choices on different node classification tasks on coarsened\ngraphs.\n", "link": "http://arxiv.org/abs/2506.11743v1", "date": "2025-06-13", "relevancy": 1.7984, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4565}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4466}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taxonomy%20of%20reduction%20matrices%20for%20Graph%20Coarsening&body=Title%3A%20Taxonomy%20of%20reduction%20matrices%20for%20Graph%20Coarsening%0AAuthor%3A%20Antonin%20Joly%20and%20Nicolas%20Keriven%20and%20Aline%20Roumy%0AAbstract%3A%20%20%20Graph%20coarsening%20aims%20to%20diminish%20the%20size%20of%20a%20graph%20to%20lighten%20its%20memory%0Afootprint%2C%20and%20has%20numerous%20applications%20in%20graph%20signal%20processing%20and%20machine%0Alearning.%20It%20is%20usually%20defined%20using%20a%20reduction%20matrix%20and%20a%20lifting%20matrix%2C%0Awhich%2C%20respectively%2C%20allows%20to%20project%20a%20graph%20signal%20from%20the%20original%20graph%0Ato%20the%20coarsened%20one%20and%20back.%20This%20results%20in%20a%20loss%20of%20information%20measured%0Aby%20the%20so-called%20Restricted%20Spectral%20Approximation%20%28RSA%29.%20Most%20coarsening%0Aframeworks%20impose%20a%20fixed%20relationship%20between%20the%20reduction%20and%20lifting%0Amatrices%2C%20generally%20as%20pseudo-inverses%20of%20each%20other%2C%20and%20seek%20to%20define%20a%0Acoarsening%20that%20minimizes%20the%20RSA.%20In%20this%20paper%2C%20we%20remark%20that%20the%20roles%20of%0Athese%20two%20matrices%20are%20not%20entirely%20symmetric%3A%20indeed%2C%20putting%20constraints%20on%0Athe%20lifting%20matrix%20alone%20ensures%20the%20existence%20of%20important%20objects%20such%20as%20the%0Acoarsened%20graph%27s%20adjacency%20matrix%20or%20Laplacian.%20In%20light%20of%20this%2C%20in%20this%0Apaper%2C%20we%20introduce%20a%20more%20general%20notion%20of%20reduction%20matrix%2C%20that%20is%20not%0Anecessarily%20the%20pseudo-inverse%20of%20the%20lifting%20matrix.%20We%20establish%20a%20taxonomy%0Aof%20%60%60admissible%27%27%20families%20of%20reduction%20matrices%2C%20discuss%20the%20different%0Aproperties%20that%20they%20must%20satisfy%20and%20whether%20they%20admit%20a%20closed-form%0Adescription%20or%20not.%20We%20show%20that%2C%20for%20a%20fixed%20coarsening%20represented%20by%20a%20fixed%0Alifting%20matrix%2C%20the%20RSA%20can%20be%20further%20reduced%20simply%20by%20modifying%20the%0Areduction%20matrix.%20We%20explore%20different%20examples%2C%20including%20some%20based%20on%20a%0Aconstrained%20optimization%20process%20of%20the%20RSA.%20Since%20this%20criterion%20has%20also%20been%0Alinked%20to%20the%20performance%20of%20Graph%20Neural%20Networks%2C%20we%20also%20illustrate%20the%0Aimpact%20of%20this%20choices%20on%20different%20node%20classification%20tasks%20on%20coarsened%0Agraphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaxonomy%2520of%2520reduction%2520matrices%2520for%2520Graph%2520Coarsening%26entry.906535625%3DAntonin%2520Joly%2520and%2520Nicolas%2520Keriven%2520and%2520Aline%2520Roumy%26entry.1292438233%3D%2520%2520Graph%2520coarsening%2520aims%2520to%2520diminish%2520the%2520size%2520of%2520a%2520graph%2520to%2520lighten%2520its%2520memory%250Afootprint%252C%2520and%2520has%2520numerous%2520applications%2520in%2520graph%2520signal%2520processing%2520and%2520machine%250Alearning.%2520It%2520is%2520usually%2520defined%2520using%2520a%2520reduction%2520matrix%2520and%2520a%2520lifting%2520matrix%252C%250Awhich%252C%2520respectively%252C%2520allows%2520to%2520project%2520a%2520graph%2520signal%2520from%2520the%2520original%2520graph%250Ato%2520the%2520coarsened%2520one%2520and%2520back.%2520This%2520results%2520in%2520a%2520loss%2520of%2520information%2520measured%250Aby%2520the%2520so-called%2520Restricted%2520Spectral%2520Approximation%2520%2528RSA%2529.%2520Most%2520coarsening%250Aframeworks%2520impose%2520a%2520fixed%2520relationship%2520between%2520the%2520reduction%2520and%2520lifting%250Amatrices%252C%2520generally%2520as%2520pseudo-inverses%2520of%2520each%2520other%252C%2520and%2520seek%2520to%2520define%2520a%250Acoarsening%2520that%2520minimizes%2520the%2520RSA.%2520In%2520this%2520paper%252C%2520we%2520remark%2520that%2520the%2520roles%2520of%250Athese%2520two%2520matrices%2520are%2520not%2520entirely%2520symmetric%253A%2520indeed%252C%2520putting%2520constraints%2520on%250Athe%2520lifting%2520matrix%2520alone%2520ensures%2520the%2520existence%2520of%2520important%2520objects%2520such%2520as%2520the%250Acoarsened%2520graph%2527s%2520adjacency%2520matrix%2520or%2520Laplacian.%2520In%2520light%2520of%2520this%252C%2520in%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520more%2520general%2520notion%2520of%2520reduction%2520matrix%252C%2520that%2520is%2520not%250Anecessarily%2520the%2520pseudo-inverse%2520of%2520the%2520lifting%2520matrix.%2520We%2520establish%2520a%2520taxonomy%250Aof%2520%2560%2560admissible%2527%2527%2520families%2520of%2520reduction%2520matrices%252C%2520discuss%2520the%2520different%250Aproperties%2520that%2520they%2520must%2520satisfy%2520and%2520whether%2520they%2520admit%2520a%2520closed-form%250Adescription%2520or%2520not.%2520We%2520show%2520that%252C%2520for%2520a%2520fixed%2520coarsening%2520represented%2520by%2520a%2520fixed%250Alifting%2520matrix%252C%2520the%2520RSA%2520can%2520be%2520further%2520reduced%2520simply%2520by%2520modifying%2520the%250Areduction%2520matrix.%2520We%2520explore%2520different%2520examples%252C%2520including%2520some%2520based%2520on%2520a%250Aconstrained%2520optimization%2520process%2520of%2520the%2520RSA.%2520Since%2520this%2520criterion%2520has%2520also%2520been%250Alinked%2520to%2520the%2520performance%2520of%2520Graph%2520Neural%2520Networks%252C%2520we%2520also%2520illustrate%2520the%250Aimpact%2520of%2520this%2520choices%2520on%2520different%2520node%2520classification%2520tasks%2520on%2520coarsened%250Agraphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taxonomy%20of%20reduction%20matrices%20for%20Graph%20Coarsening&entry.906535625=Antonin%20Joly%20and%20Nicolas%20Keriven%20and%20Aline%20Roumy&entry.1292438233=%20%20Graph%20coarsening%20aims%20to%20diminish%20the%20size%20of%20a%20graph%20to%20lighten%20its%20memory%0Afootprint%2C%20and%20has%20numerous%20applications%20in%20graph%20signal%20processing%20and%20machine%0Alearning.%20It%20is%20usually%20defined%20using%20a%20reduction%20matrix%20and%20a%20lifting%20matrix%2C%0Awhich%2C%20respectively%2C%20allows%20to%20project%20a%20graph%20signal%20from%20the%20original%20graph%0Ato%20the%20coarsened%20one%20and%20back.%20This%20results%20in%20a%20loss%20of%20information%20measured%0Aby%20the%20so-called%20Restricted%20Spectral%20Approximation%20%28RSA%29.%20Most%20coarsening%0Aframeworks%20impose%20a%20fixed%20relationship%20between%20the%20reduction%20and%20lifting%0Amatrices%2C%20generally%20as%20pseudo-inverses%20of%20each%20other%2C%20and%20seek%20to%20define%20a%0Acoarsening%20that%20minimizes%20the%20RSA.%20In%20this%20paper%2C%20we%20remark%20that%20the%20roles%20of%0Athese%20two%20matrices%20are%20not%20entirely%20symmetric%3A%20indeed%2C%20putting%20constraints%20on%0Athe%20lifting%20matrix%20alone%20ensures%20the%20existence%20of%20important%20objects%20such%20as%20the%0Acoarsened%20graph%27s%20adjacency%20matrix%20or%20Laplacian.%20In%20light%20of%20this%2C%20in%20this%0Apaper%2C%20we%20introduce%20a%20more%20general%20notion%20of%20reduction%20matrix%2C%20that%20is%20not%0Anecessarily%20the%20pseudo-inverse%20of%20the%20lifting%20matrix.%20We%20establish%20a%20taxonomy%0Aof%20%60%60admissible%27%27%20families%20of%20reduction%20matrices%2C%20discuss%20the%20different%0Aproperties%20that%20they%20must%20satisfy%20and%20whether%20they%20admit%20a%20closed-form%0Adescription%20or%20not.%20We%20show%20that%2C%20for%20a%20fixed%20coarsening%20represented%20by%20a%20fixed%0Alifting%20matrix%2C%20the%20RSA%20can%20be%20further%20reduced%20simply%20by%20modifying%20the%0Areduction%20matrix.%20We%20explore%20different%20examples%2C%20including%20some%20based%20on%20a%0Aconstrained%20optimization%20process%20of%20the%20RSA.%20Since%20this%20criterion%20has%20also%20been%0Alinked%20to%20the%20performance%20of%20Graph%20Neural%20Networks%2C%20we%20also%20illustrate%20the%0Aimpact%20of%20this%20choices%20on%20different%20node%20classification%20tasks%20on%20coarsened%0Agraphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11743v1&entry.124074799=Read"},
{"title": "PIPO: Pipelined Offloading for Efficient Inference on Consumer Devices", "author": "Yangyijian Liu and Jun Li and Wu-Jun Li", "abstract": "  The high memory and computation demand of large language models (LLMs) makes\nthem challenging to be deployed on consumer devices due to limited GPU memory.\nOffloading can mitigate the memory constraint but often suffers from low GPU\nutilization, leading to low inference efficiency. In this work, we propose a\nnovel framework, called pipelined offloading (PIPO), for efficient inference on\nconsumer devices. PIPO designs a fine-grained offloading pipeline, complemented\nwith optimized data transfer and computation, to achieve high concurrency and\nefficient scheduling for inference. Experimental results show that compared\nwith state-of-the-art baseline, PIPO increases GPU utilization from below 40%\nto over 90% and achieves up to 3.1$\\times$ higher throughput, running on a\nlaptop equipped with a RTX3060 GPU of 6GB memory.\n", "link": "http://arxiv.org/abs/2504.03664v2", "date": "2025-06-13", "relevancy": 1.4014, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4949}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4634}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIPO%3A%20Pipelined%20Offloading%20for%20Efficient%20Inference%20on%20Consumer%20Devices&body=Title%3A%20PIPO%3A%20Pipelined%20Offloading%20for%20Efficient%20Inference%20on%20Consumer%20Devices%0AAuthor%3A%20Yangyijian%20Liu%20and%20Jun%20Li%20and%20Wu-Jun%20Li%0AAbstract%3A%20%20%20The%20high%20memory%20and%20computation%20demand%20of%20large%20language%20models%20%28LLMs%29%20makes%0Athem%20challenging%20to%20be%20deployed%20on%20consumer%20devices%20due%20to%20limited%20GPU%20memory.%0AOffloading%20can%20mitigate%20the%20memory%20constraint%20but%20often%20suffers%20from%20low%20GPU%0Autilization%2C%20leading%20to%20low%20inference%20efficiency.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20framework%2C%20called%20pipelined%20offloading%20%28PIPO%29%2C%20for%20efficient%20inference%20on%0Aconsumer%20devices.%20PIPO%20designs%20a%20fine-grained%20offloading%20pipeline%2C%20complemented%0Awith%20optimized%20data%20transfer%20and%20computation%2C%20to%20achieve%20high%20concurrency%20and%0Aefficient%20scheduling%20for%20inference.%20Experimental%20results%20show%20that%20compared%0Awith%20state-of-the-art%20baseline%2C%20PIPO%20increases%20GPU%20utilization%20from%20below%2040%25%0Ato%20over%2090%25%20and%20achieves%20up%20to%203.1%24%5Ctimes%24%20higher%20throughput%2C%20running%20on%20a%0Alaptop%20equipped%20with%20a%20RTX3060%20GPU%20of%206GB%20memory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIPO%253A%2520Pipelined%2520Offloading%2520for%2520Efficient%2520Inference%2520on%2520Consumer%2520Devices%26entry.906535625%3DYangyijian%2520Liu%2520and%2520Jun%2520Li%2520and%2520Wu-Jun%2520Li%26entry.1292438233%3D%2520%2520The%2520high%2520memory%2520and%2520computation%2520demand%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520makes%250Athem%2520challenging%2520to%2520be%2520deployed%2520on%2520consumer%2520devices%2520due%2520to%2520limited%2520GPU%2520memory.%250AOffloading%2520can%2520mitigate%2520the%2520memory%2520constraint%2520but%2520often%2520suffers%2520from%2520low%2520GPU%250Autilization%252C%2520leading%2520to%2520low%2520inference%2520efficiency.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520framework%252C%2520called%2520pipelined%2520offloading%2520%2528PIPO%2529%252C%2520for%2520efficient%2520inference%2520on%250Aconsumer%2520devices.%2520PIPO%2520designs%2520a%2520fine-grained%2520offloading%2520pipeline%252C%2520complemented%250Awith%2520optimized%2520data%2520transfer%2520and%2520computation%252C%2520to%2520achieve%2520high%2520concurrency%2520and%250Aefficient%2520scheduling%2520for%2520inference.%2520Experimental%2520results%2520show%2520that%2520compared%250Awith%2520state-of-the-art%2520baseline%252C%2520PIPO%2520increases%2520GPU%2520utilization%2520from%2520below%252040%2525%250Ato%2520over%252090%2525%2520and%2520achieves%2520up%2520to%25203.1%2524%255Ctimes%2524%2520higher%2520throughput%252C%2520running%2520on%2520a%250Alaptop%2520equipped%2520with%2520a%2520RTX3060%2520GPU%2520of%25206GB%2520memory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIPO%3A%20Pipelined%20Offloading%20for%20Efficient%20Inference%20on%20Consumer%20Devices&entry.906535625=Yangyijian%20Liu%20and%20Jun%20Li%20and%20Wu-Jun%20Li&entry.1292438233=%20%20The%20high%20memory%20and%20computation%20demand%20of%20large%20language%20models%20%28LLMs%29%20makes%0Athem%20challenging%20to%20be%20deployed%20on%20consumer%20devices%20due%20to%20limited%20GPU%20memory.%0AOffloading%20can%20mitigate%20the%20memory%20constraint%20but%20often%20suffers%20from%20low%20GPU%0Autilization%2C%20leading%20to%20low%20inference%20efficiency.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20framework%2C%20called%20pipelined%20offloading%20%28PIPO%29%2C%20for%20efficient%20inference%20on%0Aconsumer%20devices.%20PIPO%20designs%20a%20fine-grained%20offloading%20pipeline%2C%20complemented%0Awith%20optimized%20data%20transfer%20and%20computation%2C%20to%20achieve%20high%20concurrency%20and%0Aefficient%20scheduling%20for%20inference.%20Experimental%20results%20show%20that%20compared%0Awith%20state-of-the-art%20baseline%2C%20PIPO%20increases%20GPU%20utilization%20from%20below%2040%25%0Ato%20over%2090%25%20and%20achieves%20up%20to%203.1%24%5Ctimes%24%20higher%20throughput%2C%20running%20on%20a%0Alaptop%20equipped%20with%20a%20RTX3060%20GPU%20of%206GB%20memory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03664v2&entry.124074799=Read"},
{"title": "Attention-based Adversarial Robust Distillation in Radio Signal\n  Classifications for Low-Power IoT Devices", "author": "Lu Zhang and Sangarapillai Lambotharan and Gan Zheng and Guisheng Liao and Basil AsSadhan and Fabio Roli", "abstract": "  Due to great success of transformers in many applications such as natural\nlanguage processing and computer vision, transformers have been successfully\napplied in automatic modulation classification. We have shown that\ntransformer-based radio signal classification is vulnerable to imperceptible\nand carefully crafted attacks called adversarial examples. Therefore, we\npropose a defense system against adversarial examples in transformer-based\nmodulation classifications. Considering the need for computationally efficient\narchitecture particularly for Internet of Things (IoT)-based applications or\noperation of devices in environment where power supply is limited, we propose a\ncompact transformer for modulation classification. The advantages of robust\ntraining such as adversarial training in transformers may not be attainable in\ncompact transformers. By demonstrating this, we propose a novel compact\ntransformer that can enhance robustness in the presence of adversarial attacks.\nThe new method is aimed at transferring the adversarial attention map from the\nrobustly trained large transformer to a compact transformer. The proposed\nmethod outperforms the state-of-the-art techniques for the considered white-box\nscenarios including fast gradient method and projected gradient descent\nattacks. We have provided reasoning of the underlying working mechanisms and\ninvestigated the transferability of the adversarial examples between different\narchitectures. The proposed method has the potential to protect the transformer\nfrom the transferability of adversarial examples.\n", "link": "http://arxiv.org/abs/2506.11892v1", "date": "2025-06-13", "relevancy": 1.5086, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5463}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4982}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention-based%20Adversarial%20Robust%20Distillation%20in%20Radio%20Signal%0A%20%20Classifications%20for%20Low-Power%20IoT%20Devices&body=Title%3A%20Attention-based%20Adversarial%20Robust%20Distillation%20in%20Radio%20Signal%0A%20%20Classifications%20for%20Low-Power%20IoT%20Devices%0AAuthor%3A%20Lu%20Zhang%20and%20Sangarapillai%20Lambotharan%20and%20Gan%20Zheng%20and%20Guisheng%20Liao%20and%20Basil%20AsSadhan%20and%20Fabio%20Roli%0AAbstract%3A%20%20%20Due%20to%20great%20success%20of%20transformers%20in%20many%20applications%20such%20as%20natural%0Alanguage%20processing%20and%20computer%20vision%2C%20transformers%20have%20been%20successfully%0Aapplied%20in%20automatic%20modulation%20classification.%20We%20have%20shown%20that%0Atransformer-based%20radio%20signal%20classification%20is%20vulnerable%20to%20imperceptible%0Aand%20carefully%20crafted%20attacks%20called%20adversarial%20examples.%20Therefore%2C%20we%0Apropose%20a%20defense%20system%20against%20adversarial%20examples%20in%20transformer-based%0Amodulation%20classifications.%20Considering%20the%20need%20for%20computationally%20efficient%0Aarchitecture%20particularly%20for%20Internet%20of%20Things%20%28IoT%29-based%20applications%20or%0Aoperation%20of%20devices%20in%20environment%20where%20power%20supply%20is%20limited%2C%20we%20propose%20a%0Acompact%20transformer%20for%20modulation%20classification.%20The%20advantages%20of%20robust%0Atraining%20such%20as%20adversarial%20training%20in%20transformers%20may%20not%20be%20attainable%20in%0Acompact%20transformers.%20By%20demonstrating%20this%2C%20we%20propose%20a%20novel%20compact%0Atransformer%20that%20can%20enhance%20robustness%20in%20the%20presence%20of%20adversarial%20attacks.%0AThe%20new%20method%20is%20aimed%20at%20transferring%20the%20adversarial%20attention%20map%20from%20the%0Arobustly%20trained%20large%20transformer%20to%20a%20compact%20transformer.%20The%20proposed%0Amethod%20outperforms%20the%20state-of-the-art%20techniques%20for%20the%20considered%20white-box%0Ascenarios%20including%20fast%20gradient%20method%20and%20projected%20gradient%20descent%0Aattacks.%20We%20have%20provided%20reasoning%20of%20the%20underlying%20working%20mechanisms%20and%0Ainvestigated%20the%20transferability%20of%20the%20adversarial%20examples%20between%20different%0Aarchitectures.%20The%20proposed%20method%20has%20the%20potential%20to%20protect%20the%20transformer%0Afrom%20the%20transferability%20of%20adversarial%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention-based%2520Adversarial%2520Robust%2520Distillation%2520in%2520Radio%2520Signal%250A%2520%2520Classifications%2520for%2520Low-Power%2520IoT%2520Devices%26entry.906535625%3DLu%2520Zhang%2520and%2520Sangarapillai%2520Lambotharan%2520and%2520Gan%2520Zheng%2520and%2520Guisheng%2520Liao%2520and%2520Basil%2520AsSadhan%2520and%2520Fabio%2520Roli%26entry.1292438233%3D%2520%2520Due%2520to%2520great%2520success%2520of%2520transformers%2520in%2520many%2520applications%2520such%2520as%2520natural%250Alanguage%2520processing%2520and%2520computer%2520vision%252C%2520transformers%2520have%2520been%2520successfully%250Aapplied%2520in%2520automatic%2520modulation%2520classification.%2520We%2520have%2520shown%2520that%250Atransformer-based%2520radio%2520signal%2520classification%2520is%2520vulnerable%2520to%2520imperceptible%250Aand%2520carefully%2520crafted%2520attacks%2520called%2520adversarial%2520examples.%2520Therefore%252C%2520we%250Apropose%2520a%2520defense%2520system%2520against%2520adversarial%2520examples%2520in%2520transformer-based%250Amodulation%2520classifications.%2520Considering%2520the%2520need%2520for%2520computationally%2520efficient%250Aarchitecture%2520particularly%2520for%2520Internet%2520of%2520Things%2520%2528IoT%2529-based%2520applications%2520or%250Aoperation%2520of%2520devices%2520in%2520environment%2520where%2520power%2520supply%2520is%2520limited%252C%2520we%2520propose%2520a%250Acompact%2520transformer%2520for%2520modulation%2520classification.%2520The%2520advantages%2520of%2520robust%250Atraining%2520such%2520as%2520adversarial%2520training%2520in%2520transformers%2520may%2520not%2520be%2520attainable%2520in%250Acompact%2520transformers.%2520By%2520demonstrating%2520this%252C%2520we%2520propose%2520a%2520novel%2520compact%250Atransformer%2520that%2520can%2520enhance%2520robustness%2520in%2520the%2520presence%2520of%2520adversarial%2520attacks.%250AThe%2520new%2520method%2520is%2520aimed%2520at%2520transferring%2520the%2520adversarial%2520attention%2520map%2520from%2520the%250Arobustly%2520trained%2520large%2520transformer%2520to%2520a%2520compact%2520transformer.%2520The%2520proposed%250Amethod%2520outperforms%2520the%2520state-of-the-art%2520techniques%2520for%2520the%2520considered%2520white-box%250Ascenarios%2520including%2520fast%2520gradient%2520method%2520and%2520projected%2520gradient%2520descent%250Aattacks.%2520We%2520have%2520provided%2520reasoning%2520of%2520the%2520underlying%2520working%2520mechanisms%2520and%250Ainvestigated%2520the%2520transferability%2520of%2520the%2520adversarial%2520examples%2520between%2520different%250Aarchitectures.%2520The%2520proposed%2520method%2520has%2520the%2520potential%2520to%2520protect%2520the%2520transformer%250Afrom%2520the%2520transferability%2520of%2520adversarial%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-based%20Adversarial%20Robust%20Distillation%20in%20Radio%20Signal%0A%20%20Classifications%20for%20Low-Power%20IoT%20Devices&entry.906535625=Lu%20Zhang%20and%20Sangarapillai%20Lambotharan%20and%20Gan%20Zheng%20and%20Guisheng%20Liao%20and%20Basil%20AsSadhan%20and%20Fabio%20Roli&entry.1292438233=%20%20Due%20to%20great%20success%20of%20transformers%20in%20many%20applications%20such%20as%20natural%0Alanguage%20processing%20and%20computer%20vision%2C%20transformers%20have%20been%20successfully%0Aapplied%20in%20automatic%20modulation%20classification.%20We%20have%20shown%20that%0Atransformer-based%20radio%20signal%20classification%20is%20vulnerable%20to%20imperceptible%0Aand%20carefully%20crafted%20attacks%20called%20adversarial%20examples.%20Therefore%2C%20we%0Apropose%20a%20defense%20system%20against%20adversarial%20examples%20in%20transformer-based%0Amodulation%20classifications.%20Considering%20the%20need%20for%20computationally%20efficient%0Aarchitecture%20particularly%20for%20Internet%20of%20Things%20%28IoT%29-based%20applications%20or%0Aoperation%20of%20devices%20in%20environment%20where%20power%20supply%20is%20limited%2C%20we%20propose%20a%0Acompact%20transformer%20for%20modulation%20classification.%20The%20advantages%20of%20robust%0Atraining%20such%20as%20adversarial%20training%20in%20transformers%20may%20not%20be%20attainable%20in%0Acompact%20transformers.%20By%20demonstrating%20this%2C%20we%20propose%20a%20novel%20compact%0Atransformer%20that%20can%20enhance%20robustness%20in%20the%20presence%20of%20adversarial%20attacks.%0AThe%20new%20method%20is%20aimed%20at%20transferring%20the%20adversarial%20attention%20map%20from%20the%0Arobustly%20trained%20large%20transformer%20to%20a%20compact%20transformer.%20The%20proposed%0Amethod%20outperforms%20the%20state-of-the-art%20techniques%20for%20the%20considered%20white-box%0Ascenarios%20including%20fast%20gradient%20method%20and%20projected%20gradient%20descent%0Aattacks.%20We%20have%20provided%20reasoning%20of%20the%20underlying%20working%20mechanisms%20and%0Ainvestigated%20the%20transferability%20of%20the%20adversarial%20examples%20between%20different%0Aarchitectures.%20The%20proposed%20method%20has%20the%20potential%20to%20protect%20the%20transformer%0Afrom%20the%20transferability%20of%20adversarial%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11892v1&entry.124074799=Read"},
{"title": "Kernel Logistic Regression Learning for High-Capacity Hopfield Networks", "author": "Akira Tamamori", "abstract": "  Hebbian learning limits Hopfield network storage capacity (pattern-to-neuron\nratio around 0.14). We propose Kernel Logistic Regression (KLR) learning.\nUnlike linear methods, KLR uses kernels to implicitly map patterns to\nhigh-dimensional feature space, enhancing separability. By learning dual\nvariables, KLR dramatically improves storage capacity, achieving perfect recall\neven when pattern numbers exceed neuron numbers (up to ratio 1.5 shown), and\nenhances noise robustness. KLR demonstrably outperforms Hebbian and linear\nlogistic regression approaches.\n", "link": "http://arxiv.org/abs/2504.07633v3", "date": "2025-06-13", "relevancy": 1.8353, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4616}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4587}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel%20Logistic%20Regression%20Learning%20for%20High-Capacity%20Hopfield%20Networks&body=Title%3A%20Kernel%20Logistic%20Regression%20Learning%20for%20High-Capacity%20Hopfield%20Networks%0AAuthor%3A%20Akira%20Tamamori%0AAbstract%3A%20%20%20Hebbian%20learning%20limits%20Hopfield%20network%20storage%20capacity%20%28pattern-to-neuron%0Aratio%20around%200.14%29.%20We%20propose%20Kernel%20Logistic%20Regression%20%28KLR%29%20learning.%0AUnlike%20linear%20methods%2C%20KLR%20uses%20kernels%20to%20implicitly%20map%20patterns%20to%0Ahigh-dimensional%20feature%20space%2C%20enhancing%20separability.%20By%20learning%20dual%0Avariables%2C%20KLR%20dramatically%20improves%20storage%20capacity%2C%20achieving%20perfect%20recall%0Aeven%20when%20pattern%20numbers%20exceed%20neuron%20numbers%20%28up%20to%20ratio%201.5%20shown%29%2C%20and%0Aenhances%20noise%20robustness.%20KLR%20demonstrably%20outperforms%20Hebbian%20and%20linear%0Alogistic%20regression%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07633v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel%2520Logistic%2520Regression%2520Learning%2520for%2520High-Capacity%2520Hopfield%2520Networks%26entry.906535625%3DAkira%2520Tamamori%26entry.1292438233%3D%2520%2520Hebbian%2520learning%2520limits%2520Hopfield%2520network%2520storage%2520capacity%2520%2528pattern-to-neuron%250Aratio%2520around%25200.14%2529.%2520We%2520propose%2520Kernel%2520Logistic%2520Regression%2520%2528KLR%2529%2520learning.%250AUnlike%2520linear%2520methods%252C%2520KLR%2520uses%2520kernels%2520to%2520implicitly%2520map%2520patterns%2520to%250Ahigh-dimensional%2520feature%2520space%252C%2520enhancing%2520separability.%2520By%2520learning%2520dual%250Avariables%252C%2520KLR%2520dramatically%2520improves%2520storage%2520capacity%252C%2520achieving%2520perfect%2520recall%250Aeven%2520when%2520pattern%2520numbers%2520exceed%2520neuron%2520numbers%2520%2528up%2520to%2520ratio%25201.5%2520shown%2529%252C%2520and%250Aenhances%2520noise%2520robustness.%2520KLR%2520demonstrably%2520outperforms%2520Hebbian%2520and%2520linear%250Alogistic%2520regression%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07633v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel%20Logistic%20Regression%20Learning%20for%20High-Capacity%20Hopfield%20Networks&entry.906535625=Akira%20Tamamori&entry.1292438233=%20%20Hebbian%20learning%20limits%20Hopfield%20network%20storage%20capacity%20%28pattern-to-neuron%0Aratio%20around%200.14%29.%20We%20propose%20Kernel%20Logistic%20Regression%20%28KLR%29%20learning.%0AUnlike%20linear%20methods%2C%20KLR%20uses%20kernels%20to%20implicitly%20map%20patterns%20to%0Ahigh-dimensional%20feature%20space%2C%20enhancing%20separability.%20By%20learning%20dual%0Avariables%2C%20KLR%20dramatically%20improves%20storage%20capacity%2C%20achieving%20perfect%20recall%0Aeven%20when%20pattern%20numbers%20exceed%20neuron%20numbers%20%28up%20to%20ratio%201.5%20shown%29%2C%20and%0Aenhances%20noise%20robustness.%20KLR%20demonstrably%20outperforms%20Hebbian%20and%20linear%0Alogistic%20regression%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07633v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


