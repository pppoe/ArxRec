<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251006.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "A Survey on 3D Gaussian Splatting", "author": "Guikun Chen and Wenguan Wang", "abstract": "  3D Gaussian splatting (GS) has emerged as a transformative technique in\nradiance fields. Unlike mainstream implicit neural models, 3D GS uses millions\nof learnable 3D Gaussians for an explicit scene representation. Paired with a\ndifferentiable rendering algorithm, this approach achieves real-time rendering\nand unprecedented editability, making it a potential game-changer for 3D\nreconstruction and representation. In the present paper, we provide the first\nsystematic overview of the recent developments and critical contributions in 3D\nGS. We begin with a detailed exploration of the underlying principles and the\ndriving forces behind the emergence of 3D GS, laying the groundwork for\nunderstanding its significance. A focal point of our discussion is the\npractical applicability of 3D GS. By enabling unprecedented rendering speed, 3D\nGS opens up a plethora of applications, ranging from virtual reality to\ninteractive media and beyond. This is complemented by a comparative analysis of\nleading 3D GS models, evaluated across various benchmark tasks to highlight\ntheir performance and practical utility. The survey concludes by identifying\ncurrent challenges and suggesting potential avenues for future research.\nThrough this survey, we aim to provide a valuable resource for both newcomers\nand seasoned researchers, fostering further exploration and advancement in\nexplicit radiance field.\n", "link": "http://arxiv.org/abs/2401.03890v8", "date": "2025-10-06", "relevancy": 3.4243, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7167}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6771}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%203D%20Gaussian%20Splatting&body=Title%3A%20A%20Survey%20on%203D%20Gaussian%20Splatting%0AAuthor%3A%20Guikun%20Chen%20and%20Wenguan%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%28GS%29%20has%20emerged%20as%20a%20transformative%20technique%20in%0Aradiance%20fields.%20Unlike%20mainstream%20implicit%20neural%20models%2C%203D%20GS%20uses%20millions%0Aof%20learnable%203D%20Gaussians%20for%20an%20explicit%20scene%20representation.%20Paired%20with%20a%0Adifferentiable%20rendering%20algorithm%2C%20this%20approach%20achieves%20real-time%20rendering%0Aand%20unprecedented%20editability%2C%20making%20it%20a%20potential%20game-changer%20for%203D%0Areconstruction%20and%20representation.%20In%20the%20present%20paper%2C%20we%20provide%20the%20first%0Asystematic%20overview%20of%20the%20recent%20developments%20and%20critical%20contributions%20in%203D%0AGS.%20We%20begin%20with%20a%20detailed%20exploration%20of%20the%20underlying%20principles%20and%20the%0Adriving%20forces%20behind%20the%20emergence%20of%203D%20GS%2C%20laying%20the%20groundwork%20for%0Aunderstanding%20its%20significance.%20A%20focal%20point%20of%20our%20discussion%20is%20the%0Apractical%20applicability%20of%203D%20GS.%20By%20enabling%20unprecedented%20rendering%20speed%2C%203D%0AGS%20opens%20up%20a%20plethora%20of%20applications%2C%20ranging%20from%20virtual%20reality%20to%0Ainteractive%20media%20and%20beyond.%20This%20is%20complemented%20by%20a%20comparative%20analysis%20of%0Aleading%203D%20GS%20models%2C%20evaluated%20across%20various%20benchmark%20tasks%20to%20highlight%0Atheir%20performance%20and%20practical%20utility.%20The%20survey%20concludes%20by%20identifying%0Acurrent%20challenges%20and%20suggesting%20potential%20avenues%20for%20future%20research.%0AThrough%20this%20survey%2C%20we%20aim%20to%20provide%20a%20valuable%20resource%20for%20both%20newcomers%0Aand%20seasoned%20researchers%2C%20fostering%20further%20exploration%20and%20advancement%20in%0Aexplicit%20radiance%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03890v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DGuikun%2520Chen%2520and%2520Wenguan%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splatting%2520%2528GS%2529%2520has%2520emerged%2520as%2520a%2520transformative%2520technique%2520in%250Aradiance%2520fields.%2520Unlike%2520mainstream%2520implicit%2520neural%2520models%252C%25203D%2520GS%2520uses%2520millions%250Aof%2520learnable%25203D%2520Gaussians%2520for%2520an%2520explicit%2520scene%2520representation.%2520Paired%2520with%2520a%250Adifferentiable%2520rendering%2520algorithm%252C%2520this%2520approach%2520achieves%2520real-time%2520rendering%250Aand%2520unprecedented%2520editability%252C%2520making%2520it%2520a%2520potential%2520game-changer%2520for%25203D%250Areconstruction%2520and%2520representation.%2520In%2520the%2520present%2520paper%252C%2520we%2520provide%2520the%2520first%250Asystematic%2520overview%2520of%2520the%2520recent%2520developments%2520and%2520critical%2520contributions%2520in%25203D%250AGS.%2520We%2520begin%2520with%2520a%2520detailed%2520exploration%2520of%2520the%2520underlying%2520principles%2520and%2520the%250Adriving%2520forces%2520behind%2520the%2520emergence%2520of%25203D%2520GS%252C%2520laying%2520the%2520groundwork%2520for%250Aunderstanding%2520its%2520significance.%2520A%2520focal%2520point%2520of%2520our%2520discussion%2520is%2520the%250Apractical%2520applicability%2520of%25203D%2520GS.%2520By%2520enabling%2520unprecedented%2520rendering%2520speed%252C%25203D%250AGS%2520opens%2520up%2520a%2520plethora%2520of%2520applications%252C%2520ranging%2520from%2520virtual%2520reality%2520to%250Ainteractive%2520media%2520and%2520beyond.%2520This%2520is%2520complemented%2520by%2520a%2520comparative%2520analysis%2520of%250Aleading%25203D%2520GS%2520models%252C%2520evaluated%2520across%2520various%2520benchmark%2520tasks%2520to%2520highlight%250Atheir%2520performance%2520and%2520practical%2520utility.%2520The%2520survey%2520concludes%2520by%2520identifying%250Acurrent%2520challenges%2520and%2520suggesting%2520potential%2520avenues%2520for%2520future%2520research.%250AThrough%2520this%2520survey%252C%2520we%2520aim%2520to%2520provide%2520a%2520valuable%2520resource%2520for%2520both%2520newcomers%250Aand%2520seasoned%2520researchers%252C%2520fostering%2520further%2520exploration%2520and%2520advancement%2520in%250Aexplicit%2520radiance%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03890v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%203D%20Gaussian%20Splatting&entry.906535625=Guikun%20Chen%20and%20Wenguan%20Wang&entry.1292438233=%20%203D%20Gaussian%20splatting%20%28GS%29%20has%20emerged%20as%20a%20transformative%20technique%20in%0Aradiance%20fields.%20Unlike%20mainstream%20implicit%20neural%20models%2C%203D%20GS%20uses%20millions%0Aof%20learnable%203D%20Gaussians%20for%20an%20explicit%20scene%20representation.%20Paired%20with%20a%0Adifferentiable%20rendering%20algorithm%2C%20this%20approach%20achieves%20real-time%20rendering%0Aand%20unprecedented%20editability%2C%20making%20it%20a%20potential%20game-changer%20for%203D%0Areconstruction%20and%20representation.%20In%20the%20present%20paper%2C%20we%20provide%20the%20first%0Asystematic%20overview%20of%20the%20recent%20developments%20and%20critical%20contributions%20in%203D%0AGS.%20We%20begin%20with%20a%20detailed%20exploration%20of%20the%20underlying%20principles%20and%20the%0Adriving%20forces%20behind%20the%20emergence%20of%203D%20GS%2C%20laying%20the%20groundwork%20for%0Aunderstanding%20its%20significance.%20A%20focal%20point%20of%20our%20discussion%20is%20the%0Apractical%20applicability%20of%203D%20GS.%20By%20enabling%20unprecedented%20rendering%20speed%2C%203D%0AGS%20opens%20up%20a%20plethora%20of%20applications%2C%20ranging%20from%20virtual%20reality%20to%0Ainteractive%20media%20and%20beyond.%20This%20is%20complemented%20by%20a%20comparative%20analysis%20of%0Aleading%203D%20GS%20models%2C%20evaluated%20across%20various%20benchmark%20tasks%20to%20highlight%0Atheir%20performance%20and%20practical%20utility.%20The%20survey%20concludes%20by%20identifying%0Acurrent%20challenges%20and%20suggesting%20potential%20avenues%20for%20future%20research.%0AThrough%20this%20survey%2C%20we%20aim%20to%20provide%20a%20valuable%20resource%20for%20both%20newcomers%0Aand%20seasoned%20researchers%2C%20fostering%20further%20exploration%20and%20advancement%20in%0Aexplicit%20radiance%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03890v8&entry.124074799=Read"},
{"title": "Bidirectional Mammogram View Translation with Column-Aware and Implicit\n  3D Conditional Diffusion", "author": "Xin Li and Kaixiang Yang and Qiang Li and Zhiwei Wang", "abstract": "  Dual-view mammography, including craniocaudal (CC) and mediolateral oblique\n(MLO) projections, offers complementary anatomical views crucial for breast\ncancer diagnosis. However, in real-world clinical workflows, one view may be\nmissing, corrupted, or degraded due to acquisition errors or compression\nartifacts, limiting the effectiveness of downstream analysis. View-to-view\ntranslation can help recover missing views and improve lesion alignment. Unlike\nnatural images, this task in mammography is highly challenging due to large\nnon-rigid deformations and severe tissue overlap in X-ray projections, which\nobscure pixel-level correspondences. In this paper, we propose Column-Aware and\nImplicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view\ntranslation framework based on conditional diffusion model. To address\ncross-view structural misalignment, we first design a column-aware\ncross-attention mechanism that leverages the geometric property that\nanatomically corresponding regions tend to lie in similar column positions\nacross views. A Gaussian-decayed bias is applied to emphasize local column-wise\ncorrelations while suppressing distant mismatches. Furthermore, we introduce an\nimplicit 3D structure reconstruction module that back-projects noisy 2D latents\ninto a coarse 3D feature volume based on breast-view projection geometry. The\nreconstructed 3D structure is refined and injected into the denoising UNet to\nguide cross-view generation with enhanced anatomical awareness. Extensive\nexperiments demonstrate that CA3D-Diff achieves superior performance in\nbidirectional tasks, outperforming state-of-the-art methods in visual fidelity\nand structural consistency. Furthermore, the synthesized views effectively\nimprove single-view malignancy classification in screening settings,\ndemonstrating the practical value of our method in real-world diagnostics.\n", "link": "http://arxiv.org/abs/2510.04947v1", "date": "2025-10-06", "relevancy": 3.0477, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6285}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6285}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bidirectional%20Mammogram%20View%20Translation%20with%20Column-Aware%20and%20Implicit%0A%20%203D%20Conditional%20Diffusion&body=Title%3A%20Bidirectional%20Mammogram%20View%20Translation%20with%20Column-Aware%20and%20Implicit%0A%20%203D%20Conditional%20Diffusion%0AAuthor%3A%20Xin%20Li%20and%20Kaixiang%20Yang%20and%20Qiang%20Li%20and%20Zhiwei%20Wang%0AAbstract%3A%20%20%20Dual-view%20mammography%2C%20including%20craniocaudal%20%28CC%29%20and%20mediolateral%20oblique%0A%28MLO%29%20projections%2C%20offers%20complementary%20anatomical%20views%20crucial%20for%20breast%0Acancer%20diagnosis.%20However%2C%20in%20real-world%20clinical%20workflows%2C%20one%20view%20may%20be%0Amissing%2C%20corrupted%2C%20or%20degraded%20due%20to%20acquisition%20errors%20or%20compression%0Aartifacts%2C%20limiting%20the%20effectiveness%20of%20downstream%20analysis.%20View-to-view%0Atranslation%20can%20help%20recover%20missing%20views%20and%20improve%20lesion%20alignment.%20Unlike%0Anatural%20images%2C%20this%20task%20in%20mammography%20is%20highly%20challenging%20due%20to%20large%0Anon-rigid%20deformations%20and%20severe%20tissue%20overlap%20in%20X-ray%20projections%2C%20which%0Aobscure%20pixel-level%20correspondences.%20In%20this%20paper%2C%20we%20propose%20Column-Aware%20and%0AImplicit%203D%20Diffusion%20%28CA3D-Diff%29%2C%20a%20novel%20bidirectional%20mammogram%20view%0Atranslation%20framework%20based%20on%20conditional%20diffusion%20model.%20To%20address%0Across-view%20structural%20misalignment%2C%20we%20first%20design%20a%20column-aware%0Across-attention%20mechanism%20that%20leverages%20the%20geometric%20property%20that%0Aanatomically%20corresponding%20regions%20tend%20to%20lie%20in%20similar%20column%20positions%0Aacross%20views.%20A%20Gaussian-decayed%20bias%20is%20applied%20to%20emphasize%20local%20column-wise%0Acorrelations%20while%20suppressing%20distant%20mismatches.%20Furthermore%2C%20we%20introduce%20an%0Aimplicit%203D%20structure%20reconstruction%20module%20that%20back-projects%20noisy%202D%20latents%0Ainto%20a%20coarse%203D%20feature%20volume%20based%20on%20breast-view%20projection%20geometry.%20The%0Areconstructed%203D%20structure%20is%20refined%20and%20injected%20into%20the%20denoising%20UNet%20to%0Aguide%20cross-view%20generation%20with%20enhanced%20anatomical%20awareness.%20Extensive%0Aexperiments%20demonstrate%20that%20CA3D-Diff%20achieves%20superior%20performance%20in%0Abidirectional%20tasks%2C%20outperforming%20state-of-the-art%20methods%20in%20visual%20fidelity%0Aand%20structural%20consistency.%20Furthermore%2C%20the%20synthesized%20views%20effectively%0Aimprove%20single-view%20malignancy%20classification%20in%20screening%20settings%2C%0Ademonstrating%20the%20practical%20value%20of%20our%20method%20in%20real-world%20diagnostics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBidirectional%2520Mammogram%2520View%2520Translation%2520with%2520Column-Aware%2520and%2520Implicit%250A%2520%25203D%2520Conditional%2520Diffusion%26entry.906535625%3DXin%2520Li%2520and%2520Kaixiang%2520Yang%2520and%2520Qiang%2520Li%2520and%2520Zhiwei%2520Wang%26entry.1292438233%3D%2520%2520Dual-view%2520mammography%252C%2520including%2520craniocaudal%2520%2528CC%2529%2520and%2520mediolateral%2520oblique%250A%2528MLO%2529%2520projections%252C%2520offers%2520complementary%2520anatomical%2520views%2520crucial%2520for%2520breast%250Acancer%2520diagnosis.%2520However%252C%2520in%2520real-world%2520clinical%2520workflows%252C%2520one%2520view%2520may%2520be%250Amissing%252C%2520corrupted%252C%2520or%2520degraded%2520due%2520to%2520acquisition%2520errors%2520or%2520compression%250Aartifacts%252C%2520limiting%2520the%2520effectiveness%2520of%2520downstream%2520analysis.%2520View-to-view%250Atranslation%2520can%2520help%2520recover%2520missing%2520views%2520and%2520improve%2520lesion%2520alignment.%2520Unlike%250Anatural%2520images%252C%2520this%2520task%2520in%2520mammography%2520is%2520highly%2520challenging%2520due%2520to%2520large%250Anon-rigid%2520deformations%2520and%2520severe%2520tissue%2520overlap%2520in%2520X-ray%2520projections%252C%2520which%250Aobscure%2520pixel-level%2520correspondences.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Column-Aware%2520and%250AImplicit%25203D%2520Diffusion%2520%2528CA3D-Diff%2529%252C%2520a%2520novel%2520bidirectional%2520mammogram%2520view%250Atranslation%2520framework%2520based%2520on%2520conditional%2520diffusion%2520model.%2520To%2520address%250Across-view%2520structural%2520misalignment%252C%2520we%2520first%2520design%2520a%2520column-aware%250Across-attention%2520mechanism%2520that%2520leverages%2520the%2520geometric%2520property%2520that%250Aanatomically%2520corresponding%2520regions%2520tend%2520to%2520lie%2520in%2520similar%2520column%2520positions%250Aacross%2520views.%2520A%2520Gaussian-decayed%2520bias%2520is%2520applied%2520to%2520emphasize%2520local%2520column-wise%250Acorrelations%2520while%2520suppressing%2520distant%2520mismatches.%2520Furthermore%252C%2520we%2520introduce%2520an%250Aimplicit%25203D%2520structure%2520reconstruction%2520module%2520that%2520back-projects%2520noisy%25202D%2520latents%250Ainto%2520a%2520coarse%25203D%2520feature%2520volume%2520based%2520on%2520breast-view%2520projection%2520geometry.%2520The%250Areconstructed%25203D%2520structure%2520is%2520refined%2520and%2520injected%2520into%2520the%2520denoising%2520UNet%2520to%250Aguide%2520cross-view%2520generation%2520with%2520enhanced%2520anatomical%2520awareness.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520CA3D-Diff%2520achieves%2520superior%2520performance%2520in%250Abidirectional%2520tasks%252C%2520outperforming%2520state-of-the-art%2520methods%2520in%2520visual%2520fidelity%250Aand%2520structural%2520consistency.%2520Furthermore%252C%2520the%2520synthesized%2520views%2520effectively%250Aimprove%2520single-view%2520malignancy%2520classification%2520in%2520screening%2520settings%252C%250Ademonstrating%2520the%2520practical%2520value%2520of%2520our%2520method%2520in%2520real-world%2520diagnostics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bidirectional%20Mammogram%20View%20Translation%20with%20Column-Aware%20and%20Implicit%0A%20%203D%20Conditional%20Diffusion&entry.906535625=Xin%20Li%20and%20Kaixiang%20Yang%20and%20Qiang%20Li%20and%20Zhiwei%20Wang&entry.1292438233=%20%20Dual-view%20mammography%2C%20including%20craniocaudal%20%28CC%29%20and%20mediolateral%20oblique%0A%28MLO%29%20projections%2C%20offers%20complementary%20anatomical%20views%20crucial%20for%20breast%0Acancer%20diagnosis.%20However%2C%20in%20real-world%20clinical%20workflows%2C%20one%20view%20may%20be%0Amissing%2C%20corrupted%2C%20or%20degraded%20due%20to%20acquisition%20errors%20or%20compression%0Aartifacts%2C%20limiting%20the%20effectiveness%20of%20downstream%20analysis.%20View-to-view%0Atranslation%20can%20help%20recover%20missing%20views%20and%20improve%20lesion%20alignment.%20Unlike%0Anatural%20images%2C%20this%20task%20in%20mammography%20is%20highly%20challenging%20due%20to%20large%0Anon-rigid%20deformations%20and%20severe%20tissue%20overlap%20in%20X-ray%20projections%2C%20which%0Aobscure%20pixel-level%20correspondences.%20In%20this%20paper%2C%20we%20propose%20Column-Aware%20and%0AImplicit%203D%20Diffusion%20%28CA3D-Diff%29%2C%20a%20novel%20bidirectional%20mammogram%20view%0Atranslation%20framework%20based%20on%20conditional%20diffusion%20model.%20To%20address%0Across-view%20structural%20misalignment%2C%20we%20first%20design%20a%20column-aware%0Across-attention%20mechanism%20that%20leverages%20the%20geometric%20property%20that%0Aanatomically%20corresponding%20regions%20tend%20to%20lie%20in%20similar%20column%20positions%0Aacross%20views.%20A%20Gaussian-decayed%20bias%20is%20applied%20to%20emphasize%20local%20column-wise%0Acorrelations%20while%20suppressing%20distant%20mismatches.%20Furthermore%2C%20we%20introduce%20an%0Aimplicit%203D%20structure%20reconstruction%20module%20that%20back-projects%20noisy%202D%20latents%0Ainto%20a%20coarse%203D%20feature%20volume%20based%20on%20breast-view%20projection%20geometry.%20The%0Areconstructed%203D%20structure%20is%20refined%20and%20injected%20into%20the%20denoising%20UNet%20to%0Aguide%20cross-view%20generation%20with%20enhanced%20anatomical%20awareness.%20Extensive%0Aexperiments%20demonstrate%20that%20CA3D-Diff%20achieves%20superior%20performance%20in%0Abidirectional%20tasks%2C%20outperforming%20state-of-the-art%20methods%20in%20visual%20fidelity%0Aand%20structural%20consistency.%20Furthermore%2C%20the%20synthesized%20views%20effectively%0Aimprove%20single-view%20malignancy%20classification%20in%20screening%20settings%2C%0Ademonstrating%20the%20practical%20value%20of%20our%20method%20in%20real-world%20diagnostics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04947v1&entry.124074799=Read"},
{"title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models", "author": "Yunlong Tang and Jing Bi and Pinxin Liu and Zhenyu Pan and Zhangyun Tan and Qianxiang Shen and Jiani Liu and Hang Hua and Junjia Guo and Yunzhong Xiao and Chao Huang and Zhiyuan Wang and Susan Liang and Xinyi Liu and Yizhi Song and Yuhe Nie and Jia-Xing Zhong and Bozheng Li and Daiqing Qi and Ziyun Zeng and Ali Vosoughi and Luchuan Song and Zeliang Zhang and Daiki Shimada and Han Liu and Jiebo Luo and Chenliang Xu", "abstract": "  Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training\n", "link": "http://arxiv.org/abs/2510.05034v1", "date": "2025-10-06", "relevancy": 3.0367, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6235}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6235}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-LMM%20Post-Training%3A%20A%20Deep%20Dive%20into%20Video%20Reasoning%20with%20Large%0A%20%20Multimodal%20Models&body=Title%3A%20Video-LMM%20Post-Training%3A%20A%20Deep%20Dive%20into%20Video%20Reasoning%20with%20Large%0A%20%20Multimodal%20Models%0AAuthor%3A%20Yunlong%20Tang%20and%20Jing%20Bi%20and%20Pinxin%20Liu%20and%20Zhenyu%20Pan%20and%20Zhangyun%20Tan%20and%20Qianxiang%20Shen%20and%20Jiani%20Liu%20and%20Hang%20Hua%20and%20Junjia%20Guo%20and%20Yunzhong%20Xiao%20and%20Chao%20Huang%20and%20Zhiyuan%20Wang%20and%20Susan%20Liang%20and%20Xinyi%20Liu%20and%20Yizhi%20Song%20and%20Yuhe%20Nie%20and%20Jia-Xing%20Zhong%20and%20Bozheng%20Li%20and%20Daiqing%20Qi%20and%20Ziyun%20Zeng%20and%20Ali%20Vosoughi%20and%20Luchuan%20Song%20and%20Zeliang%20Zhang%20and%20Daiki%20Shimada%20and%20Han%20Liu%20and%20Jiebo%20Luo%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20Video%20understanding%20represents%20the%20most%20challenging%20frontier%20in%20computer%0Avision%2C%20requiring%20models%20to%20reason%20about%20complex%20spatiotemporal%20relationships%2C%0Along-term%20dependencies%2C%20and%20multimodal%20evidence.%20The%20recent%20emergence%20of%0AVideo-Large%20Multimodal%20Models%20%28Video-LMMs%29%2C%20which%20integrate%20visual%20encoders%0Awith%20powerful%20decoder-based%20language%20models%2C%20has%20demonstrated%20remarkable%0Acapabilities%20in%20video%20understanding%20tasks.%20However%2C%20the%20critical%20phase%20that%0Atransforms%20these%20models%20from%20basic%20perception%20systems%20into%20sophisticated%0Areasoning%20engines%2C%20post-training%2C%20remains%20fragmented%20across%20the%20literature.%0AThis%20survey%20provides%20the%20first%20comprehensive%20examination%20of%20post-training%0Amethodologies%20for%20Video-LMMs%2C%20encompassing%20three%20fundamental%20pillars%3A%0Asupervised%20fine-tuning%20%28SFT%29%20with%20chain-of-thought%2C%20reinforcement%20learning%20%28RL%29%0Afrom%20verifiable%20objectives%2C%20and%20test-time%20scaling%20%28TTS%29%20through%20enhanced%0Ainference%20computation.%20We%20present%20a%20structured%20taxonomy%20that%20clarifies%20the%0Aroles%2C%20interconnections%2C%20and%20video-specific%20adaptations%20of%20these%20techniques%2C%0Aaddressing%20unique%20challenges%20such%20as%20temporal%20localization%2C%20spatiotemporal%0Agrounding%2C%20long%20video%20efficiency%2C%20and%20multimodal%20evidence%20integration.%20Through%0Asystematic%20analysis%20of%20representative%20methods%2C%20we%20synthesize%20key%20design%0Aprinciples%2C%20insights%2C%20and%20evaluation%20protocols%20while%20identifying%20critical%20open%0Achallenges%20in%20reward%20design%2C%20scalability%2C%20and%20cost-performance%20optimization.%20We%0Afurther%20curate%20essential%20benchmarks%2C%20datasets%2C%20and%20metrics%20to%20facilitate%0Arigorous%20assessment%20of%20post-training%20effectiveness.%20This%20survey%20aims%20to%20provide%0Aresearchers%20and%20practitioners%20with%20a%20unified%20framework%20for%20advancing%20Video-LMM%0Acapabilities.%20Additional%20resources%20and%20updates%20are%20maintained%20at%3A%0Ahttps%3A//github.com/yunlong10/Awesome-Video-LMM-Post-Training%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-LMM%2520Post-Training%253A%2520A%2520Deep%2520Dive%2520into%2520Video%2520Reasoning%2520with%2520Large%250A%2520%2520Multimodal%2520Models%26entry.906535625%3DYunlong%2520Tang%2520and%2520Jing%2520Bi%2520and%2520Pinxin%2520Liu%2520and%2520Zhenyu%2520Pan%2520and%2520Zhangyun%2520Tan%2520and%2520Qianxiang%2520Shen%2520and%2520Jiani%2520Liu%2520and%2520Hang%2520Hua%2520and%2520Junjia%2520Guo%2520and%2520Yunzhong%2520Xiao%2520and%2520Chao%2520Huang%2520and%2520Zhiyuan%2520Wang%2520and%2520Susan%2520Liang%2520and%2520Xinyi%2520Liu%2520and%2520Yizhi%2520Song%2520and%2520Yuhe%2520Nie%2520and%2520Jia-Xing%2520Zhong%2520and%2520Bozheng%2520Li%2520and%2520Daiqing%2520Qi%2520and%2520Ziyun%2520Zeng%2520and%2520Ali%2520Vosoughi%2520and%2520Luchuan%2520Song%2520and%2520Zeliang%2520Zhang%2520and%2520Daiki%2520Shimada%2520and%2520Han%2520Liu%2520and%2520Jiebo%2520Luo%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520Video%2520understanding%2520represents%2520the%2520most%2520challenging%2520frontier%2520in%2520computer%250Avision%252C%2520requiring%2520models%2520to%2520reason%2520about%2520complex%2520spatiotemporal%2520relationships%252C%250Along-term%2520dependencies%252C%2520and%2520multimodal%2520evidence.%2520The%2520recent%2520emergence%2520of%250AVideo-Large%2520Multimodal%2520Models%2520%2528Video-LMMs%2529%252C%2520which%2520integrate%2520visual%2520encoders%250Awith%2520powerful%2520decoder-based%2520language%2520models%252C%2520has%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520video%2520understanding%2520tasks.%2520However%252C%2520the%2520critical%2520phase%2520that%250Atransforms%2520these%2520models%2520from%2520basic%2520perception%2520systems%2520into%2520sophisticated%250Areasoning%2520engines%252C%2520post-training%252C%2520remains%2520fragmented%2520across%2520the%2520literature.%250AThis%2520survey%2520provides%2520the%2520first%2520comprehensive%2520examination%2520of%2520post-training%250Amethodologies%2520for%2520Video-LMMs%252C%2520encompassing%2520three%2520fundamental%2520pillars%253A%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520with%2520chain-of-thought%252C%2520reinforcement%2520learning%2520%2528RL%2529%250Afrom%2520verifiable%2520objectives%252C%2520and%2520test-time%2520scaling%2520%2528TTS%2529%2520through%2520enhanced%250Ainference%2520computation.%2520We%2520present%2520a%2520structured%2520taxonomy%2520that%2520clarifies%2520the%250Aroles%252C%2520interconnections%252C%2520and%2520video-specific%2520adaptations%2520of%2520these%2520techniques%252C%250Aaddressing%2520unique%2520challenges%2520such%2520as%2520temporal%2520localization%252C%2520spatiotemporal%250Agrounding%252C%2520long%2520video%2520efficiency%252C%2520and%2520multimodal%2520evidence%2520integration.%2520Through%250Asystematic%2520analysis%2520of%2520representative%2520methods%252C%2520we%2520synthesize%2520key%2520design%250Aprinciples%252C%2520insights%252C%2520and%2520evaluation%2520protocols%2520while%2520identifying%2520critical%2520open%250Achallenges%2520in%2520reward%2520design%252C%2520scalability%252C%2520and%2520cost-performance%2520optimization.%2520We%250Afurther%2520curate%2520essential%2520benchmarks%252C%2520datasets%252C%2520and%2520metrics%2520to%2520facilitate%250Arigorous%2520assessment%2520of%2520post-training%2520effectiveness.%2520This%2520survey%2520aims%2520to%2520provide%250Aresearchers%2520and%2520practitioners%2520with%2520a%2520unified%2520framework%2520for%2520advancing%2520Video-LMM%250Acapabilities.%2520Additional%2520resources%2520and%2520updates%2520are%2520maintained%2520at%253A%250Ahttps%253A//github.com/yunlong10/Awesome-Video-LMM-Post-Training%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-LMM%20Post-Training%3A%20A%20Deep%20Dive%20into%20Video%20Reasoning%20with%20Large%0A%20%20Multimodal%20Models&entry.906535625=Yunlong%20Tang%20and%20Jing%20Bi%20and%20Pinxin%20Liu%20and%20Zhenyu%20Pan%20and%20Zhangyun%20Tan%20and%20Qianxiang%20Shen%20and%20Jiani%20Liu%20and%20Hang%20Hua%20and%20Junjia%20Guo%20and%20Yunzhong%20Xiao%20and%20Chao%20Huang%20and%20Zhiyuan%20Wang%20and%20Susan%20Liang%20and%20Xinyi%20Liu%20and%20Yizhi%20Song%20and%20Yuhe%20Nie%20and%20Jia-Xing%20Zhong%20and%20Bozheng%20Li%20and%20Daiqing%20Qi%20and%20Ziyun%20Zeng%20and%20Ali%20Vosoughi%20and%20Luchuan%20Song%20and%20Zeliang%20Zhang%20and%20Daiki%20Shimada%20and%20Han%20Liu%20and%20Jiebo%20Luo%20and%20Chenliang%20Xu&entry.1292438233=%20%20Video%20understanding%20represents%20the%20most%20challenging%20frontier%20in%20computer%0Avision%2C%20requiring%20models%20to%20reason%20about%20complex%20spatiotemporal%20relationships%2C%0Along-term%20dependencies%2C%20and%20multimodal%20evidence.%20The%20recent%20emergence%20of%0AVideo-Large%20Multimodal%20Models%20%28Video-LMMs%29%2C%20which%20integrate%20visual%20encoders%0Awith%20powerful%20decoder-based%20language%20models%2C%20has%20demonstrated%20remarkable%0Acapabilities%20in%20video%20understanding%20tasks.%20However%2C%20the%20critical%20phase%20that%0Atransforms%20these%20models%20from%20basic%20perception%20systems%20into%20sophisticated%0Areasoning%20engines%2C%20post-training%2C%20remains%20fragmented%20across%20the%20literature.%0AThis%20survey%20provides%20the%20first%20comprehensive%20examination%20of%20post-training%0Amethodologies%20for%20Video-LMMs%2C%20encompassing%20three%20fundamental%20pillars%3A%0Asupervised%20fine-tuning%20%28SFT%29%20with%20chain-of-thought%2C%20reinforcement%20learning%20%28RL%29%0Afrom%20verifiable%20objectives%2C%20and%20test-time%20scaling%20%28TTS%29%20through%20enhanced%0Ainference%20computation.%20We%20present%20a%20structured%20taxonomy%20that%20clarifies%20the%0Aroles%2C%20interconnections%2C%20and%20video-specific%20adaptations%20of%20these%20techniques%2C%0Aaddressing%20unique%20challenges%20such%20as%20temporal%20localization%2C%20spatiotemporal%0Agrounding%2C%20long%20video%20efficiency%2C%20and%20multimodal%20evidence%20integration.%20Through%0Asystematic%20analysis%20of%20representative%20methods%2C%20we%20synthesize%20key%20design%0Aprinciples%2C%20insights%2C%20and%20evaluation%20protocols%20while%20identifying%20critical%20open%0Achallenges%20in%20reward%20design%2C%20scalability%2C%20and%20cost-performance%20optimization.%20We%0Afurther%20curate%20essential%20benchmarks%2C%20datasets%2C%20and%20metrics%20to%20facilitate%0Arigorous%20assessment%20of%20post-training%20effectiveness.%20This%20survey%20aims%20to%20provide%0Aresearchers%20and%20practitioners%20with%20a%20unified%20framework%20for%20advancing%20Video-LMM%0Acapabilities.%20Additional%20resources%20and%20updates%20are%20maintained%20at%3A%0Ahttps%3A//github.com/yunlong10/Awesome-Video-LMM-Post-Training%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05034v1&entry.124074799=Read"},
{"title": "SegMASt3R: Geometry Grounded Segment Matching", "author": "Rohit Jayanti and Swayam Agrawal and Vansh Garg and Siddharth Tourani and Muhammad Haris Khan and Sourav Garg and Madhava Krishna", "abstract": "  Segment matching is an important intermediate task in computer vision that\nestablishes correspondences between semantically or geometrically coherent\nregions across images. Unlike keypoint matching, which focuses on localized\nfeatures, segment matching captures structured regions, offering greater\nrobustness to occlusions, lighting variations, and viewpoint changes. In this\npaper, we leverage the spatial understanding of 3D foundation models to tackle\nwide-baseline segment matching, a challenging setting involving extreme\nviewpoint shifts. We propose an architecture that uses the inductive bias of\nthese 3D foundation models to match segments across image pairs with up to 180\ndegree view-point change. Extensive experiments show that our approach\noutperforms state-of-the-art methods, including the SAM2 video propagator and\nlocal feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++\nand Replica datasets. We further demonstrate benefits of the proposed model on\nrelevant downstream tasks, including 3D instance segmentation and image-goal\nnavigation. Project Page: https://segmast3r.github.io/\n", "link": "http://arxiv.org/abs/2510.05051v1", "date": "2025-10-06", "relevancy": 2.9789, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6099}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5907}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegMASt3R%3A%20Geometry%20Grounded%20Segment%20Matching&body=Title%3A%20SegMASt3R%3A%20Geometry%20Grounded%20Segment%20Matching%0AAuthor%3A%20Rohit%20Jayanti%20and%20Swayam%20Agrawal%20and%20Vansh%20Garg%20and%20Siddharth%20Tourani%20and%20Muhammad%20Haris%20Khan%20and%20Sourav%20Garg%20and%20Madhava%20Krishna%0AAbstract%3A%20%20%20Segment%20matching%20is%20an%20important%20intermediate%20task%20in%20computer%20vision%20that%0Aestablishes%20correspondences%20between%20semantically%20or%20geometrically%20coherent%0Aregions%20across%20images.%20Unlike%20keypoint%20matching%2C%20which%20focuses%20on%20localized%0Afeatures%2C%20segment%20matching%20captures%20structured%20regions%2C%20offering%20greater%0Arobustness%20to%20occlusions%2C%20lighting%20variations%2C%20and%20viewpoint%20changes.%20In%20this%0Apaper%2C%20we%20leverage%20the%20spatial%20understanding%20of%203D%20foundation%20models%20to%20tackle%0Awide-baseline%20segment%20matching%2C%20a%20challenging%20setting%20involving%20extreme%0Aviewpoint%20shifts.%20We%20propose%20an%20architecture%20that%20uses%20the%20inductive%20bias%20of%0Athese%203D%20foundation%20models%20to%20match%20segments%20across%20image%20pairs%20with%20up%20to%20180%0Adegree%20view-point%20change.%20Extensive%20experiments%20show%20that%20our%20approach%0Aoutperforms%20state-of-the-art%20methods%2C%20including%20the%20SAM2%20video%20propagator%20and%0Alocal%20feature%20matching%20methods%2C%20by%20upto%2030%25%20on%20the%20AUPRC%20metric%2C%20on%20ScanNet%2B%2B%0Aand%20Replica%20datasets.%20We%20further%20demonstrate%20benefits%20of%20the%20proposed%20model%20on%0Arelevant%20downstream%20tasks%2C%20including%203D%20instance%20segmentation%20and%20image-goal%0Anavigation.%20Project%20Page%3A%20https%3A//segmast3r.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegMASt3R%253A%2520Geometry%2520Grounded%2520Segment%2520Matching%26entry.906535625%3DRohit%2520Jayanti%2520and%2520Swayam%2520Agrawal%2520and%2520Vansh%2520Garg%2520and%2520Siddharth%2520Tourani%2520and%2520Muhammad%2520Haris%2520Khan%2520and%2520Sourav%2520Garg%2520and%2520Madhava%2520Krishna%26entry.1292438233%3D%2520%2520Segment%2520matching%2520is%2520an%2520important%2520intermediate%2520task%2520in%2520computer%2520vision%2520that%250Aestablishes%2520correspondences%2520between%2520semantically%2520or%2520geometrically%2520coherent%250Aregions%2520across%2520images.%2520Unlike%2520keypoint%2520matching%252C%2520which%2520focuses%2520on%2520localized%250Afeatures%252C%2520segment%2520matching%2520captures%2520structured%2520regions%252C%2520offering%2520greater%250Arobustness%2520to%2520occlusions%252C%2520lighting%2520variations%252C%2520and%2520viewpoint%2520changes.%2520In%2520this%250Apaper%252C%2520we%2520leverage%2520the%2520spatial%2520understanding%2520of%25203D%2520foundation%2520models%2520to%2520tackle%250Awide-baseline%2520segment%2520matching%252C%2520a%2520challenging%2520setting%2520involving%2520extreme%250Aviewpoint%2520shifts.%2520We%2520propose%2520an%2520architecture%2520that%2520uses%2520the%2520inductive%2520bias%2520of%250Athese%25203D%2520foundation%2520models%2520to%2520match%2520segments%2520across%2520image%2520pairs%2520with%2520up%2520to%2520180%250Adegree%2520view-point%2520change.%2520Extensive%2520experiments%2520show%2520that%2520our%2520approach%250Aoutperforms%2520state-of-the-art%2520methods%252C%2520including%2520the%2520SAM2%2520video%2520propagator%2520and%250Alocal%2520feature%2520matching%2520methods%252C%2520by%2520upto%252030%2525%2520on%2520the%2520AUPRC%2520metric%252C%2520on%2520ScanNet%252B%252B%250Aand%2520Replica%2520datasets.%2520We%2520further%2520demonstrate%2520benefits%2520of%2520the%2520proposed%2520model%2520on%250Arelevant%2520downstream%2520tasks%252C%2520including%25203D%2520instance%2520segmentation%2520and%2520image-goal%250Anavigation.%2520Project%2520Page%253A%2520https%253A//segmast3r.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegMASt3R%3A%20Geometry%20Grounded%20Segment%20Matching&entry.906535625=Rohit%20Jayanti%20and%20Swayam%20Agrawal%20and%20Vansh%20Garg%20and%20Siddharth%20Tourani%20and%20Muhammad%20Haris%20Khan%20and%20Sourav%20Garg%20and%20Madhava%20Krishna&entry.1292438233=%20%20Segment%20matching%20is%20an%20important%20intermediate%20task%20in%20computer%20vision%20that%0Aestablishes%20correspondences%20between%20semantically%20or%20geometrically%20coherent%0Aregions%20across%20images.%20Unlike%20keypoint%20matching%2C%20which%20focuses%20on%20localized%0Afeatures%2C%20segment%20matching%20captures%20structured%20regions%2C%20offering%20greater%0Arobustness%20to%20occlusions%2C%20lighting%20variations%2C%20and%20viewpoint%20changes.%20In%20this%0Apaper%2C%20we%20leverage%20the%20spatial%20understanding%20of%203D%20foundation%20models%20to%20tackle%0Awide-baseline%20segment%20matching%2C%20a%20challenging%20setting%20involving%20extreme%0Aviewpoint%20shifts.%20We%20propose%20an%20architecture%20that%20uses%20the%20inductive%20bias%20of%0Athese%203D%20foundation%20models%20to%20match%20segments%20across%20image%20pairs%20with%20up%20to%20180%0Adegree%20view-point%20change.%20Extensive%20experiments%20show%20that%20our%20approach%0Aoutperforms%20state-of-the-art%20methods%2C%20including%20the%20SAM2%20video%20propagator%20and%0Alocal%20feature%20matching%20methods%2C%20by%20upto%2030%25%20on%20the%20AUPRC%20metric%2C%20on%20ScanNet%2B%2B%0Aand%20Replica%20datasets.%20We%20further%20demonstrate%20benefits%20of%20the%20proposed%20model%20on%0Arelevant%20downstream%20tasks%2C%20including%203D%20instance%20segmentation%20and%20image-goal%0Anavigation.%20Project%20Page%3A%20https%3A//segmast3r.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05051v1&entry.124074799=Read"},
{"title": "VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning\n  MLLMs and RL", "author": "Kyoungjun Park and Yifan Yang and Juheon Yi and Shicheng Zheng and Yifei Shen and Dongqi Han and Caihua Shan and Muhammad Muaz and Lili Qiu", "abstract": "  With the rapid advancement of AI-generated videos, there is an urgent need\nfor effective detection tools to mitigate societal risks such as misinformation\nand reputational harm. In addition to accurate classification, it is essential\nthat detection models provide interpretable explanations to ensure transparency\nfor regulators and end users. To address these challenges, we introduce\nVidGuard-R1, the first video authenticity detector that fine-tunes a\nmulti-modal large language model (MLLM) using group relative policy\noptimization (GRPO). Our model delivers both highly accurate judgments and\ninsightful reasoning. We curate a challenging dataset of 140k real and\nAI-generated videos produced by state-of-the-art generation models, carefully\ndesigning the generation process to maximize discrimination difficulty. We then\nfine-tune Qwen-VL using GRPO with two specialized reward models that target\ntemporal artifacts and generation complexity. Extensive experiments demonstrate\nthat VidGuard-R1 achieves state-of-the-art zero-shot performance on existing\nbenchmarks, with additional training pushing accuracy above 95%. Case studies\nfurther show that VidGuard-R1 produces precise and interpretable rationales\nbehind its predictions. The code is publicly available at\nhttps://VidGuard-R1.github.io.\n", "link": "http://arxiv.org/abs/2510.02282v2", "date": "2025-10-06", "relevancy": 2.9779, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6152}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6051}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidGuard-R1%3A%20AI-Generated%20Video%20Detection%20and%20Explanation%20via%20Reasoning%0A%20%20MLLMs%20and%20RL&body=Title%3A%20VidGuard-R1%3A%20AI-Generated%20Video%20Detection%20and%20Explanation%20via%20Reasoning%0A%20%20MLLMs%20and%20RL%0AAuthor%3A%20Kyoungjun%20Park%20and%20Yifan%20Yang%20and%20Juheon%20Yi%20and%20Shicheng%20Zheng%20and%20Yifei%20Shen%20and%20Dongqi%20Han%20and%20Caihua%20Shan%20and%20Muhammad%20Muaz%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20AI-generated%20videos%2C%20there%20is%20an%20urgent%20need%0Afor%20effective%20detection%20tools%20to%20mitigate%20societal%20risks%20such%20as%20misinformation%0Aand%20reputational%20harm.%20In%20addition%20to%20accurate%20classification%2C%20it%20is%20essential%0Athat%20detection%20models%20provide%20interpretable%20explanations%20to%20ensure%20transparency%0Afor%20regulators%20and%20end%20users.%20To%20address%20these%20challenges%2C%20we%20introduce%0AVidGuard-R1%2C%20the%20first%20video%20authenticity%20detector%20that%20fine-tunes%20a%0Amulti-modal%20large%20language%20model%20%28MLLM%29%20using%20group%20relative%20policy%0Aoptimization%20%28GRPO%29.%20Our%20model%20delivers%20both%20highly%20accurate%20judgments%20and%0Ainsightful%20reasoning.%20We%20curate%20a%20challenging%20dataset%20of%20140k%20real%20and%0AAI-generated%20videos%20produced%20by%20state-of-the-art%20generation%20models%2C%20carefully%0Adesigning%20the%20generation%20process%20to%20maximize%20discrimination%20difficulty.%20We%20then%0Afine-tune%20Qwen-VL%20using%20GRPO%20with%20two%20specialized%20reward%20models%20that%20target%0Atemporal%20artifacts%20and%20generation%20complexity.%20Extensive%20experiments%20demonstrate%0Athat%20VidGuard-R1%20achieves%20state-of-the-art%20zero-shot%20performance%20on%20existing%0Abenchmarks%2C%20with%20additional%20training%20pushing%20accuracy%20above%2095%25.%20Case%20studies%0Afurther%20show%20that%20VidGuard-R1%20produces%20precise%20and%20interpretable%20rationales%0Abehind%20its%20predictions.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//VidGuard-R1.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02282v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidGuard-R1%253A%2520AI-Generated%2520Video%2520Detection%2520and%2520Explanation%2520via%2520Reasoning%250A%2520%2520MLLMs%2520and%2520RL%26entry.906535625%3DKyoungjun%2520Park%2520and%2520Yifan%2520Yang%2520and%2520Juheon%2520Yi%2520and%2520Shicheng%2520Zheng%2520and%2520Yifei%2520Shen%2520and%2520Dongqi%2520Han%2520and%2520Caihua%2520Shan%2520and%2520Muhammad%2520Muaz%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520AI-generated%2520videos%252C%2520there%2520is%2520an%2520urgent%2520need%250Afor%2520effective%2520detection%2520tools%2520to%2520mitigate%2520societal%2520risks%2520such%2520as%2520misinformation%250Aand%2520reputational%2520harm.%2520In%2520addition%2520to%2520accurate%2520classification%252C%2520it%2520is%2520essential%250Athat%2520detection%2520models%2520provide%2520interpretable%2520explanations%2520to%2520ensure%2520transparency%250Afor%2520regulators%2520and%2520end%2520users.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250AVidGuard-R1%252C%2520the%2520first%2520video%2520authenticity%2520detector%2520that%2520fine-tunes%2520a%250Amulti-modal%2520large%2520language%2520model%2520%2528MLLM%2529%2520using%2520group%2520relative%2520policy%250Aoptimization%2520%2528GRPO%2529.%2520Our%2520model%2520delivers%2520both%2520highly%2520accurate%2520judgments%2520and%250Ainsightful%2520reasoning.%2520We%2520curate%2520a%2520challenging%2520dataset%2520of%2520140k%2520real%2520and%250AAI-generated%2520videos%2520produced%2520by%2520state-of-the-art%2520generation%2520models%252C%2520carefully%250Adesigning%2520the%2520generation%2520process%2520to%2520maximize%2520discrimination%2520difficulty.%2520We%2520then%250Afine-tune%2520Qwen-VL%2520using%2520GRPO%2520with%2520two%2520specialized%2520reward%2520models%2520that%2520target%250Atemporal%2520artifacts%2520and%2520generation%2520complexity.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520VidGuard-R1%2520achieves%2520state-of-the-art%2520zero-shot%2520performance%2520on%2520existing%250Abenchmarks%252C%2520with%2520additional%2520training%2520pushing%2520accuracy%2520above%252095%2525.%2520Case%2520studies%250Afurther%2520show%2520that%2520VidGuard-R1%2520produces%2520precise%2520and%2520interpretable%2520rationales%250Abehind%2520its%2520predictions.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//VidGuard-R1.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02282v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidGuard-R1%3A%20AI-Generated%20Video%20Detection%20and%20Explanation%20via%20Reasoning%0A%20%20MLLMs%20and%20RL&entry.906535625=Kyoungjun%20Park%20and%20Yifan%20Yang%20and%20Juheon%20Yi%20and%20Shicheng%20Zheng%20and%20Yifei%20Shen%20and%20Dongqi%20Han%20and%20Caihua%20Shan%20and%20Muhammad%20Muaz%20and%20Lili%20Qiu&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20AI-generated%20videos%2C%20there%20is%20an%20urgent%20need%0Afor%20effective%20detection%20tools%20to%20mitigate%20societal%20risks%20such%20as%20misinformation%0Aand%20reputational%20harm.%20In%20addition%20to%20accurate%20classification%2C%20it%20is%20essential%0Athat%20detection%20models%20provide%20interpretable%20explanations%20to%20ensure%20transparency%0Afor%20regulators%20and%20end%20users.%20To%20address%20these%20challenges%2C%20we%20introduce%0AVidGuard-R1%2C%20the%20first%20video%20authenticity%20detector%20that%20fine-tunes%20a%0Amulti-modal%20large%20language%20model%20%28MLLM%29%20using%20group%20relative%20policy%0Aoptimization%20%28GRPO%29.%20Our%20model%20delivers%20both%20highly%20accurate%20judgments%20and%0Ainsightful%20reasoning.%20We%20curate%20a%20challenging%20dataset%20of%20140k%20real%20and%0AAI-generated%20videos%20produced%20by%20state-of-the-art%20generation%20models%2C%20carefully%0Adesigning%20the%20generation%20process%20to%20maximize%20discrimination%20difficulty.%20We%20then%0Afine-tune%20Qwen-VL%20using%20GRPO%20with%20two%20specialized%20reward%20models%20that%20target%0Atemporal%20artifacts%20and%20generation%20complexity.%20Extensive%20experiments%20demonstrate%0Athat%20VidGuard-R1%20achieves%20state-of-the-art%20zero-shot%20performance%20on%20existing%0Abenchmarks%2C%20with%20additional%20training%20pushing%20accuracy%20above%2095%25.%20Case%20studies%0Afurther%20show%20that%20VidGuard-R1%20produces%20precise%20and%20interpretable%20rationales%0Abehind%20its%20predictions.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//VidGuard-R1.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02282v2&entry.124074799=Read"},
{"title": "Factuality Matters: When Image Generation and Editing Meet Structured\n  Visuals", "author": "Le Zhuo and Songhao Han and Yuandong Pu and Boxiang Qiu and Sayak Paul and Yue Liao and Yihao Liu and Jie Shao and Xi Chen and Si Liu and Hongsheng Li", "abstract": "  While modern visual generation models excel at creating aesthetically\npleasing natural images, they struggle with producing or editing structured\nvisuals like charts, diagrams, and mathematical figures, which demand\ncomposition planning, text rendering, and multimodal reasoning for factual\nfidelity. To address this, we present the first comprehensive, systematic\ninvestigation of this domain, encompassing data construction, model training,\nand an evaluation benchmark. First, we construct a large-scale dataset of 1.3\nmillion high-quality structured image pairs derived from executable drawing\nprograms and augmented with chain-of-thought reasoning annotations. Building on\nit, we train a unified model that integrates a VLM with FLUX.1 Kontext via a\nlightweight connector for enhanced multimodal understanding. A three-stage\ntraining curriculum enables progressive feature alignment, knowledge infusion,\nand reasoning-augmented generation, further boosted by an external reasoner at\ninference time. Finally, we introduce StructBench, a novel benchmark for\ngeneration and editing with over 1,700 challenging instances, and an\naccompanying evaluation metric, StructScore, which employs a multi-round Q\\&A\nprotocol to assess fine-grained factual accuracy. Evaluations of 15 models\nreveal that even leading closed-source systems remain far from satisfactory.\nOur model attains strong editing performance, and inference-time reasoning\nyields consistent gains across diverse architectures. By releasing the dataset,\nmodel, and benchmark, we aim to advance unified multimodal foundations for\nstructured visuals.\n", "link": "http://arxiv.org/abs/2510.05091v1", "date": "2025-10-06", "relevancy": 2.9453, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5971}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Factuality%20Matters%3A%20When%20Image%20Generation%20and%20Editing%20Meet%20Structured%0A%20%20Visuals&body=Title%3A%20Factuality%20Matters%3A%20When%20Image%20Generation%20and%20Editing%20Meet%20Structured%0A%20%20Visuals%0AAuthor%3A%20Le%20Zhuo%20and%20Songhao%20Han%20and%20Yuandong%20Pu%20and%20Boxiang%20Qiu%20and%20Sayak%20Paul%20and%20Yue%20Liao%20and%20Yihao%20Liu%20and%20Jie%20Shao%20and%20Xi%20Chen%20and%20Si%20Liu%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20While%20modern%20visual%20generation%20models%20excel%20at%20creating%20aesthetically%0Apleasing%20natural%20images%2C%20they%20struggle%20with%20producing%20or%20editing%20structured%0Avisuals%20like%20charts%2C%20diagrams%2C%20and%20mathematical%20figures%2C%20which%20demand%0Acomposition%20planning%2C%20text%20rendering%2C%20and%20multimodal%20reasoning%20for%20factual%0Afidelity.%20To%20address%20this%2C%20we%20present%20the%20first%20comprehensive%2C%20systematic%0Ainvestigation%20of%20this%20domain%2C%20encompassing%20data%20construction%2C%20model%20training%2C%0Aand%20an%20evaluation%20benchmark.%20First%2C%20we%20construct%20a%20large-scale%20dataset%20of%201.3%0Amillion%20high-quality%20structured%20image%20pairs%20derived%20from%20executable%20drawing%0Aprograms%20and%20augmented%20with%20chain-of-thought%20reasoning%20annotations.%20Building%20on%0Ait%2C%20we%20train%20a%20unified%20model%20that%20integrates%20a%20VLM%20with%20FLUX.1%20Kontext%20via%20a%0Alightweight%20connector%20for%20enhanced%20multimodal%20understanding.%20A%20three-stage%0Atraining%20curriculum%20enables%20progressive%20feature%20alignment%2C%20knowledge%20infusion%2C%0Aand%20reasoning-augmented%20generation%2C%20further%20boosted%20by%20an%20external%20reasoner%20at%0Ainference%20time.%20Finally%2C%20we%20introduce%20StructBench%2C%20a%20novel%20benchmark%20for%0Ageneration%20and%20editing%20with%20over%201%2C700%20challenging%20instances%2C%20and%20an%0Aaccompanying%20evaluation%20metric%2C%20StructScore%2C%20which%20employs%20a%20multi-round%20Q%5C%26A%0Aprotocol%20to%20assess%20fine-grained%20factual%20accuracy.%20Evaluations%20of%2015%20models%0Areveal%20that%20even%20leading%20closed-source%20systems%20remain%20far%20from%20satisfactory.%0AOur%20model%20attains%20strong%20editing%20performance%2C%20and%20inference-time%20reasoning%0Ayields%20consistent%20gains%20across%20diverse%20architectures.%20By%20releasing%20the%20dataset%2C%0Amodel%2C%20and%20benchmark%2C%20we%20aim%20to%20advance%20unified%20multimodal%20foundations%20for%0Astructured%20visuals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFactuality%2520Matters%253A%2520When%2520Image%2520Generation%2520and%2520Editing%2520Meet%2520Structured%250A%2520%2520Visuals%26entry.906535625%3DLe%2520Zhuo%2520and%2520Songhao%2520Han%2520and%2520Yuandong%2520Pu%2520and%2520Boxiang%2520Qiu%2520and%2520Sayak%2520Paul%2520and%2520Yue%2520Liao%2520and%2520Yihao%2520Liu%2520and%2520Jie%2520Shao%2520and%2520Xi%2520Chen%2520and%2520Si%2520Liu%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520While%2520modern%2520visual%2520generation%2520models%2520excel%2520at%2520creating%2520aesthetically%250Apleasing%2520natural%2520images%252C%2520they%2520struggle%2520with%2520producing%2520or%2520editing%2520structured%250Avisuals%2520like%2520charts%252C%2520diagrams%252C%2520and%2520mathematical%2520figures%252C%2520which%2520demand%250Acomposition%2520planning%252C%2520text%2520rendering%252C%2520and%2520multimodal%2520reasoning%2520for%2520factual%250Afidelity.%2520To%2520address%2520this%252C%2520we%2520present%2520the%2520first%2520comprehensive%252C%2520systematic%250Ainvestigation%2520of%2520this%2520domain%252C%2520encompassing%2520data%2520construction%252C%2520model%2520training%252C%250Aand%2520an%2520evaluation%2520benchmark.%2520First%252C%2520we%2520construct%2520a%2520large-scale%2520dataset%2520of%25201.3%250Amillion%2520high-quality%2520structured%2520image%2520pairs%2520derived%2520from%2520executable%2520drawing%250Aprograms%2520and%2520augmented%2520with%2520chain-of-thought%2520reasoning%2520annotations.%2520Building%2520on%250Ait%252C%2520we%2520train%2520a%2520unified%2520model%2520that%2520integrates%2520a%2520VLM%2520with%2520FLUX.1%2520Kontext%2520via%2520a%250Alightweight%2520connector%2520for%2520enhanced%2520multimodal%2520understanding.%2520A%2520three-stage%250Atraining%2520curriculum%2520enables%2520progressive%2520feature%2520alignment%252C%2520knowledge%2520infusion%252C%250Aand%2520reasoning-augmented%2520generation%252C%2520further%2520boosted%2520by%2520an%2520external%2520reasoner%2520at%250Ainference%2520time.%2520Finally%252C%2520we%2520introduce%2520StructBench%252C%2520a%2520novel%2520benchmark%2520for%250Ageneration%2520and%2520editing%2520with%2520over%25201%252C700%2520challenging%2520instances%252C%2520and%2520an%250Aaccompanying%2520evaluation%2520metric%252C%2520StructScore%252C%2520which%2520employs%2520a%2520multi-round%2520Q%255C%2526A%250Aprotocol%2520to%2520assess%2520fine-grained%2520factual%2520accuracy.%2520Evaluations%2520of%252015%2520models%250Areveal%2520that%2520even%2520leading%2520closed-source%2520systems%2520remain%2520far%2520from%2520satisfactory.%250AOur%2520model%2520attains%2520strong%2520editing%2520performance%252C%2520and%2520inference-time%2520reasoning%250Ayields%2520consistent%2520gains%2520across%2520diverse%2520architectures.%2520By%2520releasing%2520the%2520dataset%252C%250Amodel%252C%2520and%2520benchmark%252C%2520we%2520aim%2520to%2520advance%2520unified%2520multimodal%2520foundations%2520for%250Astructured%2520visuals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Factuality%20Matters%3A%20When%20Image%20Generation%20and%20Editing%20Meet%20Structured%0A%20%20Visuals&entry.906535625=Le%20Zhuo%20and%20Songhao%20Han%20and%20Yuandong%20Pu%20and%20Boxiang%20Qiu%20and%20Sayak%20Paul%20and%20Yue%20Liao%20and%20Yihao%20Liu%20and%20Jie%20Shao%20and%20Xi%20Chen%20and%20Si%20Liu%20and%20Hongsheng%20Li&entry.1292438233=%20%20While%20modern%20visual%20generation%20models%20excel%20at%20creating%20aesthetically%0Apleasing%20natural%20images%2C%20they%20struggle%20with%20producing%20or%20editing%20structured%0Avisuals%20like%20charts%2C%20diagrams%2C%20and%20mathematical%20figures%2C%20which%20demand%0Acomposition%20planning%2C%20text%20rendering%2C%20and%20multimodal%20reasoning%20for%20factual%0Afidelity.%20To%20address%20this%2C%20we%20present%20the%20first%20comprehensive%2C%20systematic%0Ainvestigation%20of%20this%20domain%2C%20encompassing%20data%20construction%2C%20model%20training%2C%0Aand%20an%20evaluation%20benchmark.%20First%2C%20we%20construct%20a%20large-scale%20dataset%20of%201.3%0Amillion%20high-quality%20structured%20image%20pairs%20derived%20from%20executable%20drawing%0Aprograms%20and%20augmented%20with%20chain-of-thought%20reasoning%20annotations.%20Building%20on%0Ait%2C%20we%20train%20a%20unified%20model%20that%20integrates%20a%20VLM%20with%20FLUX.1%20Kontext%20via%20a%0Alightweight%20connector%20for%20enhanced%20multimodal%20understanding.%20A%20three-stage%0Atraining%20curriculum%20enables%20progressive%20feature%20alignment%2C%20knowledge%20infusion%2C%0Aand%20reasoning-augmented%20generation%2C%20further%20boosted%20by%20an%20external%20reasoner%20at%0Ainference%20time.%20Finally%2C%20we%20introduce%20StructBench%2C%20a%20novel%20benchmark%20for%0Ageneration%20and%20editing%20with%20over%201%2C700%20challenging%20instances%2C%20and%20an%0Aaccompanying%20evaluation%20metric%2C%20StructScore%2C%20which%20employs%20a%20multi-round%20Q%5C%26A%0Aprotocol%20to%20assess%20fine-grained%20factual%20accuracy.%20Evaluations%20of%2015%20models%0Areveal%20that%20even%20leading%20closed-source%20systems%20remain%20far%20from%20satisfactory.%0AOur%20model%20attains%20strong%20editing%20performance%2C%20and%20inference-time%20reasoning%0Ayields%20consistent%20gains%20across%20diverse%20architectures.%20By%20releasing%20the%20dataset%2C%0Amodel%2C%20and%20benchmark%2C%20we%20aim%20to%20advance%20unified%20multimodal%20foundations%20for%0Astructured%20visuals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05091v1&entry.124074799=Read"},
{"title": "MMLongBench: Benchmarking Long-Context Vision-Language Models\n  Effectively and Thoroughly", "author": "Zhaowei Wang and Wenhao Yu and Xiyu Ren and Jipeng Zhang and Yu Zhao and Rohit Saxena and Liang Cheng and Ginny Wong and Simon See and Pasquale Minervini and Yangqiu Song and Mark Steedman", "abstract": "  The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs.\n", "link": "http://arxiv.org/abs/2505.10610v3", "date": "2025-10-06", "relevancy": 2.9139, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6212}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6212}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMLongBench%3A%20Benchmarking%20Long-Context%20Vision-Language%20Models%0A%20%20Effectively%20and%20Thoroughly&body=Title%3A%20MMLongBench%3A%20Benchmarking%20Long-Context%20Vision-Language%20Models%0A%20%20Effectively%20and%20Thoroughly%0AAuthor%3A%20Zhaowei%20Wang%20and%20Wenhao%20Yu%20and%20Xiyu%20Ren%20and%20Jipeng%20Zhang%20and%20Yu%20Zhao%20and%20Rohit%20Saxena%20and%20Liang%20Cheng%20and%20Ginny%20Wong%20and%20Simon%20See%20and%20Pasquale%20Minervini%20and%20Yangqiu%20Song%20and%20Mark%20Steedman%0AAbstract%3A%20%20%20The%20rapid%20extension%20of%20context%20windows%20in%20large%20vision-language%20models%20has%0Agiven%20rise%20to%20long-context%20vision-language%20models%20%28LCVLMs%29%2C%20which%20are%20capable%0Aof%20handling%20hundreds%20of%20images%20with%20interleaved%20text%20tokens%20in%20a%20single%20forward%0Apass.%20In%20this%20work%2C%20we%20introduce%20MMLongBench%2C%20the%20first%20benchmark%20covering%20a%0Adiverse%20set%20of%20long-context%20vision-language%20tasks%2C%20to%20evaluate%20LCVLMs%0Aeffectively%20and%20thoroughly.%20MMLongBench%20is%20composed%20of%2013%2C331%20examples%20spanning%0Afive%20different%20categories%20of%20downstream%20tasks%2C%20such%20as%20Visual%20RAG%20and%20Many-Shot%0AICL.%20It%20also%20provides%20broad%20coverage%20of%20image%20types%2C%20including%20various%20natural%0Aand%20synthetic%20images.%20To%20assess%20the%20robustness%20of%20the%20models%20to%20different%20input%0Alengths%2C%20all%20examples%20are%20delivered%20at%20five%20standardized%20input%20lengths%20%288K-128K%0Atokens%29%20via%20a%20cross-modal%20tokenization%20scheme%20that%20combines%20vision%20patches%20and%0Atext%20tokens.%20Through%20a%20thorough%20benchmarking%20of%2046%20closed-source%20and%0Aopen-source%20LCVLMs%2C%20we%20provide%20a%20comprehensive%20analysis%20of%20the%20current%20models%27%0Avision-language%20long-context%20ability.%20Our%20results%20show%20that%3A%20i%29%20performance%20on%0Aa%20single%20task%20is%20a%20weak%20proxy%20for%20overall%20long-context%20capability%3B%20ii%29%20both%0Aclosed-source%20and%20open-source%20models%20face%20challenges%20in%20long-context%0Avision-language%20tasks%2C%20indicating%20substantial%20room%20for%20future%20improvement%3B%20iii%29%0Amodels%20with%20stronger%20reasoning%20ability%20tend%20to%20exhibit%20better%20long-context%0Aperformance.%20By%20offering%20wide%20task%20coverage%2C%20various%20image%20types%2C%20and%20rigorous%0Alength%20control%2C%20MMLongBench%20provides%20the%20missing%20foundation%20for%20diagnosing%20and%0Aadvancing%20the%20next%20generation%20of%20LCVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10610v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMLongBench%253A%2520Benchmarking%2520Long-Context%2520Vision-Language%2520Models%250A%2520%2520Effectively%2520and%2520Thoroughly%26entry.906535625%3DZhaowei%2520Wang%2520and%2520Wenhao%2520Yu%2520and%2520Xiyu%2520Ren%2520and%2520Jipeng%2520Zhang%2520and%2520Yu%2520Zhao%2520and%2520Rohit%2520Saxena%2520and%2520Liang%2520Cheng%2520and%2520Ginny%2520Wong%2520and%2520Simon%2520See%2520and%2520Pasquale%2520Minervini%2520and%2520Yangqiu%2520Song%2520and%2520Mark%2520Steedman%26entry.1292438233%3D%2520%2520The%2520rapid%2520extension%2520of%2520context%2520windows%2520in%2520large%2520vision-language%2520models%2520has%250Agiven%2520rise%2520to%2520long-context%2520vision-language%2520models%2520%2528LCVLMs%2529%252C%2520which%2520are%2520capable%250Aof%2520handling%2520hundreds%2520of%2520images%2520with%2520interleaved%2520text%2520tokens%2520in%2520a%2520single%2520forward%250Apass.%2520In%2520this%2520work%252C%2520we%2520introduce%2520MMLongBench%252C%2520the%2520first%2520benchmark%2520covering%2520a%250Adiverse%2520set%2520of%2520long-context%2520vision-language%2520tasks%252C%2520to%2520evaluate%2520LCVLMs%250Aeffectively%2520and%2520thoroughly.%2520MMLongBench%2520is%2520composed%2520of%252013%252C331%2520examples%2520spanning%250Afive%2520different%2520categories%2520of%2520downstream%2520tasks%252C%2520such%2520as%2520Visual%2520RAG%2520and%2520Many-Shot%250AICL.%2520It%2520also%2520provides%2520broad%2520coverage%2520of%2520image%2520types%252C%2520including%2520various%2520natural%250Aand%2520synthetic%2520images.%2520To%2520assess%2520the%2520robustness%2520of%2520the%2520models%2520to%2520different%2520input%250Alengths%252C%2520all%2520examples%2520are%2520delivered%2520at%2520five%2520standardized%2520input%2520lengths%2520%25288K-128K%250Atokens%2529%2520via%2520a%2520cross-modal%2520tokenization%2520scheme%2520that%2520combines%2520vision%2520patches%2520and%250Atext%2520tokens.%2520Through%2520a%2520thorough%2520benchmarking%2520of%252046%2520closed-source%2520and%250Aopen-source%2520LCVLMs%252C%2520we%2520provide%2520a%2520comprehensive%2520analysis%2520of%2520the%2520current%2520models%2527%250Avision-language%2520long-context%2520ability.%2520Our%2520results%2520show%2520that%253A%2520i%2529%2520performance%2520on%250Aa%2520single%2520task%2520is%2520a%2520weak%2520proxy%2520for%2520overall%2520long-context%2520capability%253B%2520ii%2529%2520both%250Aclosed-source%2520and%2520open-source%2520models%2520face%2520challenges%2520in%2520long-context%250Avision-language%2520tasks%252C%2520indicating%2520substantial%2520room%2520for%2520future%2520improvement%253B%2520iii%2529%250Amodels%2520with%2520stronger%2520reasoning%2520ability%2520tend%2520to%2520exhibit%2520better%2520long-context%250Aperformance.%2520By%2520offering%2520wide%2520task%2520coverage%252C%2520various%2520image%2520types%252C%2520and%2520rigorous%250Alength%2520control%252C%2520MMLongBench%2520provides%2520the%2520missing%2520foundation%2520for%2520diagnosing%2520and%250Aadvancing%2520the%2520next%2520generation%2520of%2520LCVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10610v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMLongBench%3A%20Benchmarking%20Long-Context%20Vision-Language%20Models%0A%20%20Effectively%20and%20Thoroughly&entry.906535625=Zhaowei%20Wang%20and%20Wenhao%20Yu%20and%20Xiyu%20Ren%20and%20Jipeng%20Zhang%20and%20Yu%20Zhao%20and%20Rohit%20Saxena%20and%20Liang%20Cheng%20and%20Ginny%20Wong%20and%20Simon%20See%20and%20Pasquale%20Minervini%20and%20Yangqiu%20Song%20and%20Mark%20Steedman&entry.1292438233=%20%20The%20rapid%20extension%20of%20context%20windows%20in%20large%20vision-language%20models%20has%0Agiven%20rise%20to%20long-context%20vision-language%20models%20%28LCVLMs%29%2C%20which%20are%20capable%0Aof%20handling%20hundreds%20of%20images%20with%20interleaved%20text%20tokens%20in%20a%20single%20forward%0Apass.%20In%20this%20work%2C%20we%20introduce%20MMLongBench%2C%20the%20first%20benchmark%20covering%20a%0Adiverse%20set%20of%20long-context%20vision-language%20tasks%2C%20to%20evaluate%20LCVLMs%0Aeffectively%20and%20thoroughly.%20MMLongBench%20is%20composed%20of%2013%2C331%20examples%20spanning%0Afive%20different%20categories%20of%20downstream%20tasks%2C%20such%20as%20Visual%20RAG%20and%20Many-Shot%0AICL.%20It%20also%20provides%20broad%20coverage%20of%20image%20types%2C%20including%20various%20natural%0Aand%20synthetic%20images.%20To%20assess%20the%20robustness%20of%20the%20models%20to%20different%20input%0Alengths%2C%20all%20examples%20are%20delivered%20at%20five%20standardized%20input%20lengths%20%288K-128K%0Atokens%29%20via%20a%20cross-modal%20tokenization%20scheme%20that%20combines%20vision%20patches%20and%0Atext%20tokens.%20Through%20a%20thorough%20benchmarking%20of%2046%20closed-source%20and%0Aopen-source%20LCVLMs%2C%20we%20provide%20a%20comprehensive%20analysis%20of%20the%20current%20models%27%0Avision-language%20long-context%20ability.%20Our%20results%20show%20that%3A%20i%29%20performance%20on%0Aa%20single%20task%20is%20a%20weak%20proxy%20for%20overall%20long-context%20capability%3B%20ii%29%20both%0Aclosed-source%20and%20open-source%20models%20face%20challenges%20in%20long-context%0Avision-language%20tasks%2C%20indicating%20substantial%20room%20for%20future%20improvement%3B%20iii%29%0Amodels%20with%20stronger%20reasoning%20ability%20tend%20to%20exhibit%20better%20long-context%0Aperformance.%20By%20offering%20wide%20task%20coverage%2C%20various%20image%20types%2C%20and%20rigorous%0Alength%20control%2C%20MMLongBench%20provides%20the%20missing%20foundation%20for%20diagnosing%20and%0Aadvancing%20the%20next%20generation%20of%20LCVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10610v3&entry.124074799=Read"},
{"title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation", "author": "Ziqi Huang and Ning Yu and Gordon Chen and Haonan Qiu and Paul Debevec and Ziwei Liu", "abstract": "  Recent video generation models can produce smooth and visually appealing\nclips, but they often struggle to synthesize complex dynamics with a coherent\nchain of consequences. Accurately modeling visual outcomes and state\ntransitions over time remains a core challenge. In contrast, large language and\nmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and\nfuture prediction capabilities. To bridge these strengths, we introduce VChain,\na novel inference-time chain-of-visual-thought framework that injects visual\nreasoning signals from multimodal models into video generation. Specifically,\nVChain contains a dedicated pipeline that leverages large multimodal models to\ngenerate a sparse set of critical keyframes as snapshots, which are then used\nto guide the sparse inference-time tuning of a pre-trained video generator only\nat these key moments. Our approach is tuning-efficient, introduces minimal\noverhead and avoids dense supervision. Extensive experiments on complex,\nmulti-step scenarios show that VChain significantly enhances the quality of\ngenerated videos.\n", "link": "http://arxiv.org/abs/2510.05094v1", "date": "2025-10-06", "relevancy": 2.8854, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5895}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5805}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VChain%3A%20Chain-of-Visual-Thought%20for%20Reasoning%20in%20Video%20Generation&body=Title%3A%20VChain%3A%20Chain-of-Visual-Thought%20for%20Reasoning%20in%20Video%20Generation%0AAuthor%3A%20Ziqi%20Huang%20and%20Ning%20Yu%20and%20Gordon%20Chen%20and%20Haonan%20Qiu%20and%20Paul%20Debevec%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Recent%20video%20generation%20models%20can%20produce%20smooth%20and%20visually%20appealing%0Aclips%2C%20but%20they%20often%20struggle%20to%20synthesize%20complex%20dynamics%20with%20a%20coherent%0Achain%20of%20consequences.%20Accurately%20modeling%20visual%20outcomes%20and%20state%0Atransitions%20over%20time%20remains%20a%20core%20challenge.%20In%20contrast%2C%20large%20language%20and%0Amultimodal%20models%20%28e.g.%2C%20GPT-4o%29%20exhibit%20strong%20visual%20state%20reasoning%20and%0Afuture%20prediction%20capabilities.%20To%20bridge%20these%20strengths%2C%20we%20introduce%20VChain%2C%0Aa%20novel%20inference-time%20chain-of-visual-thought%20framework%20that%20injects%20visual%0Areasoning%20signals%20from%20multimodal%20models%20into%20video%20generation.%20Specifically%2C%0AVChain%20contains%20a%20dedicated%20pipeline%20that%20leverages%20large%20multimodal%20models%20to%0Agenerate%20a%20sparse%20set%20of%20critical%20keyframes%20as%20snapshots%2C%20which%20are%20then%20used%0Ato%20guide%20the%20sparse%20inference-time%20tuning%20of%20a%20pre-trained%20video%20generator%20only%0Aat%20these%20key%20moments.%20Our%20approach%20is%20tuning-efficient%2C%20introduces%20minimal%0Aoverhead%20and%20avoids%20dense%20supervision.%20Extensive%20experiments%20on%20complex%2C%0Amulti-step%20scenarios%20show%20that%20VChain%20significantly%20enhances%20the%20quality%20of%0Agenerated%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVChain%253A%2520Chain-of-Visual-Thought%2520for%2520Reasoning%2520in%2520Video%2520Generation%26entry.906535625%3DZiqi%2520Huang%2520and%2520Ning%2520Yu%2520and%2520Gordon%2520Chen%2520and%2520Haonan%2520Qiu%2520and%2520Paul%2520Debevec%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520video%2520generation%2520models%2520can%2520produce%2520smooth%2520and%2520visually%2520appealing%250Aclips%252C%2520but%2520they%2520often%2520struggle%2520to%2520synthesize%2520complex%2520dynamics%2520with%2520a%2520coherent%250Achain%2520of%2520consequences.%2520Accurately%2520modeling%2520visual%2520outcomes%2520and%2520state%250Atransitions%2520over%2520time%2520remains%2520a%2520core%2520challenge.%2520In%2520contrast%252C%2520large%2520language%2520and%250Amultimodal%2520models%2520%2528e.g.%252C%2520GPT-4o%2529%2520exhibit%2520strong%2520visual%2520state%2520reasoning%2520and%250Afuture%2520prediction%2520capabilities.%2520To%2520bridge%2520these%2520strengths%252C%2520we%2520introduce%2520VChain%252C%250Aa%2520novel%2520inference-time%2520chain-of-visual-thought%2520framework%2520that%2520injects%2520visual%250Areasoning%2520signals%2520from%2520multimodal%2520models%2520into%2520video%2520generation.%2520Specifically%252C%250AVChain%2520contains%2520a%2520dedicated%2520pipeline%2520that%2520leverages%2520large%2520multimodal%2520models%2520to%250Agenerate%2520a%2520sparse%2520set%2520of%2520critical%2520keyframes%2520as%2520snapshots%252C%2520which%2520are%2520then%2520used%250Ato%2520guide%2520the%2520sparse%2520inference-time%2520tuning%2520of%2520a%2520pre-trained%2520video%2520generator%2520only%250Aat%2520these%2520key%2520moments.%2520Our%2520approach%2520is%2520tuning-efficient%252C%2520introduces%2520minimal%250Aoverhead%2520and%2520avoids%2520dense%2520supervision.%2520Extensive%2520experiments%2520on%2520complex%252C%250Amulti-step%2520scenarios%2520show%2520that%2520VChain%2520significantly%2520enhances%2520the%2520quality%2520of%250Agenerated%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VChain%3A%20Chain-of-Visual-Thought%20for%20Reasoning%20in%20Video%20Generation&entry.906535625=Ziqi%20Huang%20and%20Ning%20Yu%20and%20Gordon%20Chen%20and%20Haonan%20Qiu%20and%20Paul%20Debevec%20and%20Ziwei%20Liu&entry.1292438233=%20%20Recent%20video%20generation%20models%20can%20produce%20smooth%20and%20visually%20appealing%0Aclips%2C%20but%20they%20often%20struggle%20to%20synthesize%20complex%20dynamics%20with%20a%20coherent%0Achain%20of%20consequences.%20Accurately%20modeling%20visual%20outcomes%20and%20state%0Atransitions%20over%20time%20remains%20a%20core%20challenge.%20In%20contrast%2C%20large%20language%20and%0Amultimodal%20models%20%28e.g.%2C%20GPT-4o%29%20exhibit%20strong%20visual%20state%20reasoning%20and%0Afuture%20prediction%20capabilities.%20To%20bridge%20these%20strengths%2C%20we%20introduce%20VChain%2C%0Aa%20novel%20inference-time%20chain-of-visual-thought%20framework%20that%20injects%20visual%0Areasoning%20signals%20from%20multimodal%20models%20into%20video%20generation.%20Specifically%2C%0AVChain%20contains%20a%20dedicated%20pipeline%20that%20leverages%20large%20multimodal%20models%20to%0Agenerate%20a%20sparse%20set%20of%20critical%20keyframes%20as%20snapshots%2C%20which%20are%20then%20used%0Ato%20guide%20the%20sparse%20inference-time%20tuning%20of%20a%20pre-trained%20video%20generator%20only%0Aat%20these%20key%20moments.%20Our%20approach%20is%20tuning-efficient%2C%20introduces%20minimal%0Aoverhead%20and%20avoids%20dense%20supervision.%20Extensive%20experiments%20on%20complex%2C%0Amulti-step%20scenarios%20show%20that%20VChain%20significantly%20enhances%20the%20quality%20of%0Agenerated%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05094v1&entry.124074799=Read"},
{"title": "ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for\n  Zero-Shot Anomaly Detection", "author": "Ziteng Yang and Jingzehua Xu and Yanshu Li and Zepeng Li and Yeqiang Wang and Xinghui Li", "abstract": "  Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any\ntarget domain training samples, relying solely on external auxiliary data.\nExisting CLIP-based methods attempt to activate the model's ZSAD potential via\nhandcrafted or static learnable prompts. The former incur high engineering\ncosts and limited semantic coverage, whereas the latter apply identical\ndescriptions across diverse anomaly types, thus fail to adapt to complex\nvariations. Furthermore, since CLIP is originally pretrained on large-scale\nclassification tasks, its anomaly segmentation quality is highly sensitive to\nthe exact wording of class names, severely constraining prompting strategies\nthat depend on class labels. To address these challenges, we introduce\nViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception\nPrompting (ViP-Prompt) mechanism, which fuses global and multi-scale local\nvisual context to adaptively generate fine-grained textual prompts, eliminating\nmanual templates and class-name priors. This design enables our model to focus\non precise abnormal regions, making it particularly valuable when category\nlabels are ambiguous or privacy-constrained. Extensive experiments on 15\nindustrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves\nstate-of-the-art performance and robust cross-domain generalization.\n", "link": "http://arxiv.org/abs/2505.17692v3", "date": "2025-10-06", "relevancy": 2.749, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViP%24%5E2%24-CLIP%3A%20Visual-Perception%20Prompting%20with%20Unified%20Alignment%20for%0A%20%20Zero-Shot%20Anomaly%20Detection&body=Title%3A%20ViP%24%5E2%24-CLIP%3A%20Visual-Perception%20Prompting%20with%20Unified%20Alignment%20for%0A%20%20Zero-Shot%20Anomaly%20Detection%0AAuthor%3A%20Ziteng%20Yang%20and%20Jingzehua%20Xu%20and%20Yanshu%20Li%20and%20Zepeng%20Li%20and%20Yeqiang%20Wang%20and%20Xinghui%20Li%0AAbstract%3A%20%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20aims%20to%20detect%20anomalies%20without%20any%0Atarget%20domain%20training%20samples%2C%20relying%20solely%20on%20external%20auxiliary%20data.%0AExisting%20CLIP-based%20methods%20attempt%20to%20activate%20the%20model%27s%20ZSAD%20potential%20via%0Ahandcrafted%20or%20static%20learnable%20prompts.%20The%20former%20incur%20high%20engineering%0Acosts%20and%20limited%20semantic%20coverage%2C%20whereas%20the%20latter%20apply%20identical%0Adescriptions%20across%20diverse%20anomaly%20types%2C%20thus%20fail%20to%20adapt%20to%20complex%0Avariations.%20Furthermore%2C%20since%20CLIP%20is%20originally%20pretrained%20on%20large-scale%0Aclassification%20tasks%2C%20its%20anomaly%20segmentation%20quality%20is%20highly%20sensitive%20to%0Athe%20exact%20wording%20of%20class%20names%2C%20severely%20constraining%20prompting%20strategies%0Athat%20depend%20on%20class%20labels.%20To%20address%20these%20challenges%2C%20we%20introduce%0AViP%24%5E%7B2%7D%24-CLIP.%20The%20key%20insight%20of%20ViP%24%5E%7B2%7D%24-CLIP%20is%20a%20Visual-Perception%0APrompting%20%28ViP-Prompt%29%20mechanism%2C%20which%20fuses%20global%20and%20multi-scale%20local%0Avisual%20context%20to%20adaptively%20generate%20fine-grained%20textual%20prompts%2C%20eliminating%0Amanual%20templates%20and%20class-name%20priors.%20This%20design%20enables%20our%20model%20to%20focus%0Aon%20precise%20abnormal%20regions%2C%20making%20it%20particularly%20valuable%20when%20category%0Alabels%20are%20ambiguous%20or%20privacy-constrained.%20Extensive%20experiments%20on%2015%0Aindustrial%20and%20medical%20benchmarks%20demonstrate%20that%20ViP%24%5E%7B2%7D%24-CLIP%20achieves%0Astate-of-the-art%20performance%20and%20robust%20cross-domain%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17692v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViP%2524%255E2%2524-CLIP%253A%2520Visual-Perception%2520Prompting%2520with%2520Unified%2520Alignment%2520for%250A%2520%2520Zero-Shot%2520Anomaly%2520Detection%26entry.906535625%3DZiteng%2520Yang%2520and%2520Jingzehua%2520Xu%2520and%2520Yanshu%2520Li%2520and%2520Zepeng%2520Li%2520and%2520Yeqiang%2520Wang%2520and%2520Xinghui%2520Li%26entry.1292438233%3D%2520%2520Zero-shot%2520anomaly%2520detection%2520%2528ZSAD%2529%2520aims%2520to%2520detect%2520anomalies%2520without%2520any%250Atarget%2520domain%2520training%2520samples%252C%2520relying%2520solely%2520on%2520external%2520auxiliary%2520data.%250AExisting%2520CLIP-based%2520methods%2520attempt%2520to%2520activate%2520the%2520model%2527s%2520ZSAD%2520potential%2520via%250Ahandcrafted%2520or%2520static%2520learnable%2520prompts.%2520The%2520former%2520incur%2520high%2520engineering%250Acosts%2520and%2520limited%2520semantic%2520coverage%252C%2520whereas%2520the%2520latter%2520apply%2520identical%250Adescriptions%2520across%2520diverse%2520anomaly%2520types%252C%2520thus%2520fail%2520to%2520adapt%2520to%2520complex%250Avariations.%2520Furthermore%252C%2520since%2520CLIP%2520is%2520originally%2520pretrained%2520on%2520large-scale%250Aclassification%2520tasks%252C%2520its%2520anomaly%2520segmentation%2520quality%2520is%2520highly%2520sensitive%2520to%250Athe%2520exact%2520wording%2520of%2520class%2520names%252C%2520severely%2520constraining%2520prompting%2520strategies%250Athat%2520depend%2520on%2520class%2520labels.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250AViP%2524%255E%257B2%257D%2524-CLIP.%2520The%2520key%2520insight%2520of%2520ViP%2524%255E%257B2%257D%2524-CLIP%2520is%2520a%2520Visual-Perception%250APrompting%2520%2528ViP-Prompt%2529%2520mechanism%252C%2520which%2520fuses%2520global%2520and%2520multi-scale%2520local%250Avisual%2520context%2520to%2520adaptively%2520generate%2520fine-grained%2520textual%2520prompts%252C%2520eliminating%250Amanual%2520templates%2520and%2520class-name%2520priors.%2520This%2520design%2520enables%2520our%2520model%2520to%2520focus%250Aon%2520precise%2520abnormal%2520regions%252C%2520making%2520it%2520particularly%2520valuable%2520when%2520category%250Alabels%2520are%2520ambiguous%2520or%2520privacy-constrained.%2520Extensive%2520experiments%2520on%252015%250Aindustrial%2520and%2520medical%2520benchmarks%2520demonstrate%2520that%2520ViP%2524%255E%257B2%257D%2524-CLIP%2520achieves%250Astate-of-the-art%2520performance%2520and%2520robust%2520cross-domain%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17692v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViP%24%5E2%24-CLIP%3A%20Visual-Perception%20Prompting%20with%20Unified%20Alignment%20for%0A%20%20Zero-Shot%20Anomaly%20Detection&entry.906535625=Ziteng%20Yang%20and%20Jingzehua%20Xu%20and%20Yanshu%20Li%20and%20Zepeng%20Li%20and%20Yeqiang%20Wang%20and%20Xinghui%20Li&entry.1292438233=%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20aims%20to%20detect%20anomalies%20without%20any%0Atarget%20domain%20training%20samples%2C%20relying%20solely%20on%20external%20auxiliary%20data.%0AExisting%20CLIP-based%20methods%20attempt%20to%20activate%20the%20model%27s%20ZSAD%20potential%20via%0Ahandcrafted%20or%20static%20learnable%20prompts.%20The%20former%20incur%20high%20engineering%0Acosts%20and%20limited%20semantic%20coverage%2C%20whereas%20the%20latter%20apply%20identical%0Adescriptions%20across%20diverse%20anomaly%20types%2C%20thus%20fail%20to%20adapt%20to%20complex%0Avariations.%20Furthermore%2C%20since%20CLIP%20is%20originally%20pretrained%20on%20large-scale%0Aclassification%20tasks%2C%20its%20anomaly%20segmentation%20quality%20is%20highly%20sensitive%20to%0Athe%20exact%20wording%20of%20class%20names%2C%20severely%20constraining%20prompting%20strategies%0Athat%20depend%20on%20class%20labels.%20To%20address%20these%20challenges%2C%20we%20introduce%0AViP%24%5E%7B2%7D%24-CLIP.%20The%20key%20insight%20of%20ViP%24%5E%7B2%7D%24-CLIP%20is%20a%20Visual-Perception%0APrompting%20%28ViP-Prompt%29%20mechanism%2C%20which%20fuses%20global%20and%20multi-scale%20local%0Avisual%20context%20to%20adaptively%20generate%20fine-grained%20textual%20prompts%2C%20eliminating%0Amanual%20templates%20and%20class-name%20priors.%20This%20design%20enables%20our%20model%20to%20focus%0Aon%20precise%20abnormal%20regions%2C%20making%20it%20particularly%20valuable%20when%20category%0Alabels%20are%20ambiguous%20or%20privacy-constrained.%20Extensive%20experiments%20on%2015%0Aindustrial%20and%20medical%20benchmarks%20demonstrate%20that%20ViP%24%5E%7B2%7D%24-CLIP%20achieves%0Astate-of-the-art%20performance%20and%20robust%20cross-domain%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17692v3&entry.124074799=Read"},
{"title": "Think Then Embed: Generative Context Improves Multimodal Embedding", "author": "Xuanming Cui and Jianpeng Cheng and Hong-you Chen and Satya Narayan Shukla and Abhijeet Awasthi and Xichen Pan and Chaitanya Ahuja and Shlok Kumar Mishra and Qi Guo and Ser-Nam Lim and Aashu Singh and Xiangjun Fan", "abstract": "  There is a growing interest in Universal Multimodal Embeddings (UME), where\nmodels are required to generate task-specific representations. While recent\nstudies show that Multimodal Large Language Models (MLLMs) perform well on such\ntasks, they treat MLLMs solely as encoders, overlooking their generative\ncapacity. However, such an encoding paradigm becomes less effective as\ninstructions become more complex and require compositional reasoning. Inspired\nby the proven effectiveness of chain-of-thought reasoning, we propose a general\nThink-Then-Embed (TTE) framework for UME, composed of a reasoner and an\nembedder. The reasoner MLLM first generates reasoning traces that explain\ncomplex queries, followed by an embedder that produces representations\nconditioned on both the original query and the intermediate reasoning. This\nexplicit reasoning step enables more nuanced understanding of complex\nmultimodal instructions. Our contributions are threefold. First, by leveraging\na powerful MLLM reasoner, we achieve state-of-the-art performance on the\nMMEB-V2 benchmark, surpassing proprietary models trained on massive in-house\ndatasets. Second, to reduce the dependency on large MLLM reasoners, we finetune\na smaller MLLM reasoner using high-quality embedding-centric reasoning traces,\nachieving the best performance among open-source models with a 7% absolute gain\nover recently proposed models. Third, we investigate strategies for integrating\nthe reasoner and embedder into a unified model for improved efficiency without\nsacrificing performance.\n", "link": "http://arxiv.org/abs/2510.05014v1", "date": "2025-10-06", "relevancy": 2.7254, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20Then%20Embed%3A%20Generative%20Context%20Improves%20Multimodal%20Embedding&body=Title%3A%20Think%20Then%20Embed%3A%20Generative%20Context%20Improves%20Multimodal%20Embedding%0AAuthor%3A%20Xuanming%20Cui%20and%20Jianpeng%20Cheng%20and%20Hong-you%20Chen%20and%20Satya%20Narayan%20Shukla%20and%20Abhijeet%20Awasthi%20and%20Xichen%20Pan%20and%20Chaitanya%20Ahuja%20and%20Shlok%20Kumar%20Mishra%20and%20Qi%20Guo%20and%20Ser-Nam%20Lim%20and%20Aashu%20Singh%20and%20Xiangjun%20Fan%0AAbstract%3A%20%20%20There%20is%20a%20growing%20interest%20in%20Universal%20Multimodal%20Embeddings%20%28UME%29%2C%20where%0Amodels%20are%20required%20to%20generate%20task-specific%20representations.%20While%20recent%0Astudies%20show%20that%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20perform%20well%20on%20such%0Atasks%2C%20they%20treat%20MLLMs%20solely%20as%20encoders%2C%20overlooking%20their%20generative%0Acapacity.%20However%2C%20such%20an%20encoding%20paradigm%20becomes%20less%20effective%20as%0Ainstructions%20become%20more%20complex%20and%20require%20compositional%20reasoning.%20Inspired%0Aby%20the%20proven%20effectiveness%20of%20chain-of-thought%20reasoning%2C%20we%20propose%20a%20general%0AThink-Then-Embed%20%28TTE%29%20framework%20for%20UME%2C%20composed%20of%20a%20reasoner%20and%20an%0Aembedder.%20The%20reasoner%20MLLM%20first%20generates%20reasoning%20traces%20that%20explain%0Acomplex%20queries%2C%20followed%20by%20an%20embedder%20that%20produces%20representations%0Aconditioned%20on%20both%20the%20original%20query%20and%20the%20intermediate%20reasoning.%20This%0Aexplicit%20reasoning%20step%20enables%20more%20nuanced%20understanding%20of%20complex%0Amultimodal%20instructions.%20Our%20contributions%20are%20threefold.%20First%2C%20by%20leveraging%0Aa%20powerful%20MLLM%20reasoner%2C%20we%20achieve%20state-of-the-art%20performance%20on%20the%0AMMEB-V2%20benchmark%2C%20surpassing%20proprietary%20models%20trained%20on%20massive%20in-house%0Adatasets.%20Second%2C%20to%20reduce%20the%20dependency%20on%20large%20MLLM%20reasoners%2C%20we%20finetune%0Aa%20smaller%20MLLM%20reasoner%20using%20high-quality%20embedding-centric%20reasoning%20traces%2C%0Aachieving%20the%20best%20performance%20among%20open-source%20models%20with%20a%207%25%20absolute%20gain%0Aover%20recently%20proposed%20models.%20Third%2C%20we%20investigate%20strategies%20for%20integrating%0Athe%20reasoner%20and%20embedder%20into%20a%20unified%20model%20for%20improved%20efficiency%20without%0Asacrificing%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520Then%2520Embed%253A%2520Generative%2520Context%2520Improves%2520Multimodal%2520Embedding%26entry.906535625%3DXuanming%2520Cui%2520and%2520Jianpeng%2520Cheng%2520and%2520Hong-you%2520Chen%2520and%2520Satya%2520Narayan%2520Shukla%2520and%2520Abhijeet%2520Awasthi%2520and%2520Xichen%2520Pan%2520and%2520Chaitanya%2520Ahuja%2520and%2520Shlok%2520Kumar%2520Mishra%2520and%2520Qi%2520Guo%2520and%2520Ser-Nam%2520Lim%2520and%2520Aashu%2520Singh%2520and%2520Xiangjun%2520Fan%26entry.1292438233%3D%2520%2520There%2520is%2520a%2520growing%2520interest%2520in%2520Universal%2520Multimodal%2520Embeddings%2520%2528UME%2529%252C%2520where%250Amodels%2520are%2520required%2520to%2520generate%2520task-specific%2520representations.%2520While%2520recent%250Astudies%2520show%2520that%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520perform%2520well%2520on%2520such%250Atasks%252C%2520they%2520treat%2520MLLMs%2520solely%2520as%2520encoders%252C%2520overlooking%2520their%2520generative%250Acapacity.%2520However%252C%2520such%2520an%2520encoding%2520paradigm%2520becomes%2520less%2520effective%2520as%250Ainstructions%2520become%2520more%2520complex%2520and%2520require%2520compositional%2520reasoning.%2520Inspired%250Aby%2520the%2520proven%2520effectiveness%2520of%2520chain-of-thought%2520reasoning%252C%2520we%2520propose%2520a%2520general%250AThink-Then-Embed%2520%2528TTE%2529%2520framework%2520for%2520UME%252C%2520composed%2520of%2520a%2520reasoner%2520and%2520an%250Aembedder.%2520The%2520reasoner%2520MLLM%2520first%2520generates%2520reasoning%2520traces%2520that%2520explain%250Acomplex%2520queries%252C%2520followed%2520by%2520an%2520embedder%2520that%2520produces%2520representations%250Aconditioned%2520on%2520both%2520the%2520original%2520query%2520and%2520the%2520intermediate%2520reasoning.%2520This%250Aexplicit%2520reasoning%2520step%2520enables%2520more%2520nuanced%2520understanding%2520of%2520complex%250Amultimodal%2520instructions.%2520Our%2520contributions%2520are%2520threefold.%2520First%252C%2520by%2520leveraging%250Aa%2520powerful%2520MLLM%2520reasoner%252C%2520we%2520achieve%2520state-of-the-art%2520performance%2520on%2520the%250AMMEB-V2%2520benchmark%252C%2520surpassing%2520proprietary%2520models%2520trained%2520on%2520massive%2520in-house%250Adatasets.%2520Second%252C%2520to%2520reduce%2520the%2520dependency%2520on%2520large%2520MLLM%2520reasoners%252C%2520we%2520finetune%250Aa%2520smaller%2520MLLM%2520reasoner%2520using%2520high-quality%2520embedding-centric%2520reasoning%2520traces%252C%250Aachieving%2520the%2520best%2520performance%2520among%2520open-source%2520models%2520with%2520a%25207%2525%2520absolute%2520gain%250Aover%2520recently%2520proposed%2520models.%2520Third%252C%2520we%2520investigate%2520strategies%2520for%2520integrating%250Athe%2520reasoner%2520and%2520embedder%2520into%2520a%2520unified%2520model%2520for%2520improved%2520efficiency%2520without%250Asacrificing%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20Then%20Embed%3A%20Generative%20Context%20Improves%20Multimodal%20Embedding&entry.906535625=Xuanming%20Cui%20and%20Jianpeng%20Cheng%20and%20Hong-you%20Chen%20and%20Satya%20Narayan%20Shukla%20and%20Abhijeet%20Awasthi%20and%20Xichen%20Pan%20and%20Chaitanya%20Ahuja%20and%20Shlok%20Kumar%20Mishra%20and%20Qi%20Guo%20and%20Ser-Nam%20Lim%20and%20Aashu%20Singh%20and%20Xiangjun%20Fan&entry.1292438233=%20%20There%20is%20a%20growing%20interest%20in%20Universal%20Multimodal%20Embeddings%20%28UME%29%2C%20where%0Amodels%20are%20required%20to%20generate%20task-specific%20representations.%20While%20recent%0Astudies%20show%20that%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20perform%20well%20on%20such%0Atasks%2C%20they%20treat%20MLLMs%20solely%20as%20encoders%2C%20overlooking%20their%20generative%0Acapacity.%20However%2C%20such%20an%20encoding%20paradigm%20becomes%20less%20effective%20as%0Ainstructions%20become%20more%20complex%20and%20require%20compositional%20reasoning.%20Inspired%0Aby%20the%20proven%20effectiveness%20of%20chain-of-thought%20reasoning%2C%20we%20propose%20a%20general%0AThink-Then-Embed%20%28TTE%29%20framework%20for%20UME%2C%20composed%20of%20a%20reasoner%20and%20an%0Aembedder.%20The%20reasoner%20MLLM%20first%20generates%20reasoning%20traces%20that%20explain%0Acomplex%20queries%2C%20followed%20by%20an%20embedder%20that%20produces%20representations%0Aconditioned%20on%20both%20the%20original%20query%20and%20the%20intermediate%20reasoning.%20This%0Aexplicit%20reasoning%20step%20enables%20more%20nuanced%20understanding%20of%20complex%0Amultimodal%20instructions.%20Our%20contributions%20are%20threefold.%20First%2C%20by%20leveraging%0Aa%20powerful%20MLLM%20reasoner%2C%20we%20achieve%20state-of-the-art%20performance%20on%20the%0AMMEB-V2%20benchmark%2C%20surpassing%20proprietary%20models%20trained%20on%20massive%20in-house%0Adatasets.%20Second%2C%20to%20reduce%20the%20dependency%20on%20large%20MLLM%20reasoners%2C%20we%20finetune%0Aa%20smaller%20MLLM%20reasoner%20using%20high-quality%20embedding-centric%20reasoning%20traces%2C%0Aachieving%20the%20best%20performance%20among%20open-source%20models%20with%20a%207%25%20absolute%20gain%0Aover%20recently%20proposed%20models.%20Third%2C%20we%20investigate%20strategies%20for%20integrating%0Athe%20reasoner%20and%20embedder%20into%20a%20unified%20model%20for%20improved%20efficiency%20without%0Asacrificing%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05014v1&entry.124074799=Read"},
{"title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning", "author": "Julia Witte Zimmerman and Denis Hudon and Kathryn Cramer and Alejandro J. Ruiz and Calla Beauregard and Ashley Fehr and Mikaela Irene Fudolig and Bradford Demarest and Yoshi Meke Bird and Milo Z. Trujillo and Christopher M. Danforth and Peter Sheridan Dodds", "abstract": "  Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens and current\nstructural constraints motivate changes to existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokens and pretraining can act as a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being arguably\nmeaningfully insulated from the main system intelligence. [First uploaded to\narXiv in December, 2024.]\n", "link": "http://arxiv.org/abs/2412.10924v6", "date": "2025-10-06", "relevancy": 2.6746, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5157}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tokens%2C%20the%20oft-overlooked%20appetizer%3A%20Large%20language%20models%2C%20the%0A%20%20distributional%20hypothesis%2C%20and%20meaning&body=Title%3A%20Tokens%2C%20the%20oft-overlooked%20appetizer%3A%20Large%20language%20models%2C%20the%0A%20%20distributional%20hypothesis%2C%20and%20meaning%0AAuthor%3A%20Julia%20Witte%20Zimmerman%20and%20Denis%20Hudon%20and%20Kathryn%20Cramer%20and%20Alejandro%20J.%20Ruiz%20and%20Calla%20Beauregard%20and%20Ashley%20Fehr%20and%20Mikaela%20Irene%20Fudolig%20and%20Bradford%20Demarest%20and%20Yoshi%20Meke%20Bird%20and%20Milo%20Z.%20Trujillo%20and%20Christopher%20M.%20Danforth%20and%20Peter%20Sheridan%20Dodds%0AAbstract%3A%20%20%20Tokenization%20is%20a%20necessary%20component%20within%20the%20current%20architecture%20of%20many%0Alanguage%20models%2C%20including%20the%20transformer-based%20large%20language%20models%20%28LLMs%29%0Aof%20Generative%20AI%2C%20yet%20its%20impact%20on%20the%20model%27s%20cognition%20is%20often%20overlooked.%0AWe%20argue%20that%20LLMs%20demonstrate%20that%20the%20Distributional%20Hypothesis%20%28DH%29%20is%0Asufficient%20for%20reasonably%20human-like%20language%20performance%2C%20and%20that%20the%0Aemergence%20of%20human-meaningful%20linguistic%20units%20among%20tokens%20and%20current%0Astructural%20constraints%20motivate%20changes%20to%20existing%2C%20linguistically-agnostic%0Atokenization%20techniques%2C%20particularly%20with%20respect%20to%20their%20roles%20as%20%281%29%0Asemantic%20primitives%20and%20as%20%282%29%20vehicles%20for%20conveying%20salient%20distributional%0Apatterns%20from%20human%20language%20to%20the%20model.%20We%20explore%20tokenizations%20from%20a%20BPE%0Atokenizer%3B%20extant%20model%20vocabularies%20obtained%20from%20Hugging%20Face%20and%20tiktoken%3B%0Aand%20the%20information%20in%20exemplar%20token%20vectors%20as%20they%20move%20through%20the%20layers%0Aof%20a%20RoBERTa%20%28large%29%20model.%20Besides%20creating%20sub-optimal%20semantic%20building%0Ablocks%20and%20obscuring%20the%20model%27s%20access%20to%20the%20necessary%20distributional%0Apatterns%2C%20we%20describe%20how%20tokens%20and%20pretraining%20can%20act%20as%20a%20backdoor%20for%20bias%0Aand%20other%20unwanted%20content%2C%20which%20current%20alignment%20practices%20may%20not%0Aremediate.%20Additionally%2C%20we%20relay%20evidence%20that%20the%20tokenization%20algorithm%27s%0Aobjective%20function%20impacts%20the%20LLM%27s%20cognition%2C%20despite%20being%20arguably%0Ameaningfully%20insulated%20from%20the%20main%20system%20intelligence.%20%5BFirst%20uploaded%20to%0AarXiv%20in%20December%2C%202024.%5D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10924v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokens%252C%2520the%2520oft-overlooked%2520appetizer%253A%2520Large%2520language%2520models%252C%2520the%250A%2520%2520distributional%2520hypothesis%252C%2520and%2520meaning%26entry.906535625%3DJulia%2520Witte%2520Zimmerman%2520and%2520Denis%2520Hudon%2520and%2520Kathryn%2520Cramer%2520and%2520Alejandro%2520J.%2520Ruiz%2520and%2520Calla%2520Beauregard%2520and%2520Ashley%2520Fehr%2520and%2520Mikaela%2520Irene%2520Fudolig%2520and%2520Bradford%2520Demarest%2520and%2520Yoshi%2520Meke%2520Bird%2520and%2520Milo%2520Z.%2520Trujillo%2520and%2520Christopher%2520M.%2520Danforth%2520and%2520Peter%2520Sheridan%2520Dodds%26entry.1292438233%3D%2520%2520Tokenization%2520is%2520a%2520necessary%2520component%2520within%2520the%2520current%2520architecture%2520of%2520many%250Alanguage%2520models%252C%2520including%2520the%2520transformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%250Aof%2520Generative%2520AI%252C%2520yet%2520its%2520impact%2520on%2520the%2520model%2527s%2520cognition%2520is%2520often%2520overlooked.%250AWe%2520argue%2520that%2520LLMs%2520demonstrate%2520that%2520the%2520Distributional%2520Hypothesis%2520%2528DH%2529%2520is%250Asufficient%2520for%2520reasonably%2520human-like%2520language%2520performance%252C%2520and%2520that%2520the%250Aemergence%2520of%2520human-meaningful%2520linguistic%2520units%2520among%2520tokens%2520and%2520current%250Astructural%2520constraints%2520motivate%2520changes%2520to%2520existing%252C%2520linguistically-agnostic%250Atokenization%2520techniques%252C%2520particularly%2520with%2520respect%2520to%2520their%2520roles%2520as%2520%25281%2529%250Asemantic%2520primitives%2520and%2520as%2520%25282%2529%2520vehicles%2520for%2520conveying%2520salient%2520distributional%250Apatterns%2520from%2520human%2520language%2520to%2520the%2520model.%2520We%2520explore%2520tokenizations%2520from%2520a%2520BPE%250Atokenizer%253B%2520extant%2520model%2520vocabularies%2520obtained%2520from%2520Hugging%2520Face%2520and%2520tiktoken%253B%250Aand%2520the%2520information%2520in%2520exemplar%2520token%2520vectors%2520as%2520they%2520move%2520through%2520the%2520layers%250Aof%2520a%2520RoBERTa%2520%2528large%2529%2520model.%2520Besides%2520creating%2520sub-optimal%2520semantic%2520building%250Ablocks%2520and%2520obscuring%2520the%2520model%2527s%2520access%2520to%2520the%2520necessary%2520distributional%250Apatterns%252C%2520we%2520describe%2520how%2520tokens%2520and%2520pretraining%2520can%2520act%2520as%2520a%2520backdoor%2520for%2520bias%250Aand%2520other%2520unwanted%2520content%252C%2520which%2520current%2520alignment%2520practices%2520may%2520not%250Aremediate.%2520Additionally%252C%2520we%2520relay%2520evidence%2520that%2520the%2520tokenization%2520algorithm%2527s%250Aobjective%2520function%2520impacts%2520the%2520LLM%2527s%2520cognition%252C%2520despite%2520being%2520arguably%250Ameaningfully%2520insulated%2520from%2520the%2520main%2520system%2520intelligence.%2520%255BFirst%2520uploaded%2520to%250AarXiv%2520in%2520December%252C%25202024.%255D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10924v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokens%2C%20the%20oft-overlooked%20appetizer%3A%20Large%20language%20models%2C%20the%0A%20%20distributional%20hypothesis%2C%20and%20meaning&entry.906535625=Julia%20Witte%20Zimmerman%20and%20Denis%20Hudon%20and%20Kathryn%20Cramer%20and%20Alejandro%20J.%20Ruiz%20and%20Calla%20Beauregard%20and%20Ashley%20Fehr%20and%20Mikaela%20Irene%20Fudolig%20and%20Bradford%20Demarest%20and%20Yoshi%20Meke%20Bird%20and%20Milo%20Z.%20Trujillo%20and%20Christopher%20M.%20Danforth%20and%20Peter%20Sheridan%20Dodds&entry.1292438233=%20%20Tokenization%20is%20a%20necessary%20component%20within%20the%20current%20architecture%20of%20many%0Alanguage%20models%2C%20including%20the%20transformer-based%20large%20language%20models%20%28LLMs%29%0Aof%20Generative%20AI%2C%20yet%20its%20impact%20on%20the%20model%27s%20cognition%20is%20often%20overlooked.%0AWe%20argue%20that%20LLMs%20demonstrate%20that%20the%20Distributional%20Hypothesis%20%28DH%29%20is%0Asufficient%20for%20reasonably%20human-like%20language%20performance%2C%20and%20that%20the%0Aemergence%20of%20human-meaningful%20linguistic%20units%20among%20tokens%20and%20current%0Astructural%20constraints%20motivate%20changes%20to%20existing%2C%20linguistically-agnostic%0Atokenization%20techniques%2C%20particularly%20with%20respect%20to%20their%20roles%20as%20%281%29%0Asemantic%20primitives%20and%20as%20%282%29%20vehicles%20for%20conveying%20salient%20distributional%0Apatterns%20from%20human%20language%20to%20the%20model.%20We%20explore%20tokenizations%20from%20a%20BPE%0Atokenizer%3B%20extant%20model%20vocabularies%20obtained%20from%20Hugging%20Face%20and%20tiktoken%3B%0Aand%20the%20information%20in%20exemplar%20token%20vectors%20as%20they%20move%20through%20the%20layers%0Aof%20a%20RoBERTa%20%28large%29%20model.%20Besides%20creating%20sub-optimal%20semantic%20building%0Ablocks%20and%20obscuring%20the%20model%27s%20access%20to%20the%20necessary%20distributional%0Apatterns%2C%20we%20describe%20how%20tokens%20and%20pretraining%20can%20act%20as%20a%20backdoor%20for%20bias%0Aand%20other%20unwanted%20content%2C%20which%20current%20alignment%20practices%20may%20not%0Aremediate.%20Additionally%2C%20we%20relay%20evidence%20that%20the%20tokenization%20algorithm%27s%0Aobjective%20function%20impacts%20the%20LLM%27s%20cognition%2C%20despite%20being%20arguably%0Ameaningfully%20insulated%20from%20the%20main%20system%20intelligence.%20%5BFirst%20uploaded%20to%0AarXiv%20in%20December%2C%202024.%5D%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10924v6&entry.124074799=Read"},
{"title": "Bridging Text and Video Generation: A Survey", "author": "Nilay Kumar and Priyansh Bhandari and G. Maragatham", "abstract": "  Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications.\n", "link": "http://arxiv.org/abs/2510.04999v1", "date": "2025-10-06", "relevancy": 2.6598, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6741}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.666}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Text%20and%20Video%20Generation%3A%20A%20Survey&body=Title%3A%20Bridging%20Text%20and%20Video%20Generation%3A%20A%20Survey%0AAuthor%3A%20Nilay%20Kumar%20and%20Priyansh%20Bhandari%20and%20G.%20Maragatham%0AAbstract%3A%20%20%20Text-to-video%20%28T2V%29%20generation%20technology%20holds%20potential%20to%20transform%0Amultiple%20domains%20such%20as%20education%2C%20marketing%2C%20entertainment%2C%20and%20assistive%0Atechnologies%20for%20individuals%20with%20visual%20or%20reading%20comprehension%20challenges%2C%0Aby%20creating%20coherent%20visual%20content%20from%20natural%20language%20prompts.%20From%20its%0Ainception%2C%20the%20field%20has%20advanced%20from%20adversarial%20models%20to%20diffusion-based%0Amodels%2C%20yielding%20higher-fidelity%2C%20temporally%20consistent%20outputs.%20Yet%20challenges%0Apersist%2C%20such%20as%20alignment%2C%20long-range%20coherence%2C%20and%20computational%20efficiency.%0AAddressing%20this%20evolving%20landscape%2C%20we%20present%20a%20comprehensive%20survey%20of%0Atext-to-video%20generative%20models%2C%20tracing%20their%20development%20from%20early%20GANs%20and%0AVAEs%20to%20hybrid%20Diffusion-Transformer%20%28DiT%29%20architectures%2C%20detailing%20how%20these%0Amodels%20work%2C%20what%20limitations%20they%20addressed%20in%20their%20predecessors%2C%20and%20why%0Ashifts%20toward%20new%20architectural%20paradigms%20were%20necessary%20to%20overcome%20challenges%0Ain%20quality%2C%20coherence%2C%20and%20control.%20We%20provide%20a%20systematic%20account%20of%20the%0Adatasets%2C%20which%20the%20surveyed%20text-to-video%20models%20were%20trained%20and%20evaluated%0Aon%2C%20and%2C%20to%20support%20reproducibility%20and%20assess%20the%20accessibility%20of%20training%0Asuch%20models%2C%20we%20detail%20their%20training%20configurations%2C%20including%20their%20hardware%0Aspecifications%2C%20GPU%20counts%2C%20batch%20sizes%2C%20learning%20rates%2C%20optimizers%2C%20epochs%2C%0Aand%20other%20key%20hyperparameters.%20Further%2C%20we%20outline%20the%20evaluation%20metrics%0Acommonly%20used%20for%20evaluating%20such%20models%20and%20present%20their%20performance%20across%0Astandard%20benchmarks%2C%20while%20also%20discussing%20the%20limitations%20of%20these%20metrics%20and%0Athe%20emerging%20shift%20toward%20more%20holistic%2C%20perception-aligned%20evaluation%0Astrategies.%20Finally%2C%20drawing%20from%20our%20analysis%2C%20we%20outline%20the%20current%20open%0Achallenges%20and%20propose%20a%20few%20promising%20future%20directions%2C%20laying%20out%20a%0Aperspective%20for%20future%20researchers%20to%20explore%20and%20build%20upon%20in%20advancing%20T2V%0Aresearch%20and%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Text%2520and%2520Video%2520Generation%253A%2520A%2520Survey%26entry.906535625%3DNilay%2520Kumar%2520and%2520Priyansh%2520Bhandari%2520and%2520G.%2520Maragatham%26entry.1292438233%3D%2520%2520Text-to-video%2520%2528T2V%2529%2520generation%2520technology%2520holds%2520potential%2520to%2520transform%250Amultiple%2520domains%2520such%2520as%2520education%252C%2520marketing%252C%2520entertainment%252C%2520and%2520assistive%250Atechnologies%2520for%2520individuals%2520with%2520visual%2520or%2520reading%2520comprehension%2520challenges%252C%250Aby%2520creating%2520coherent%2520visual%2520content%2520from%2520natural%2520language%2520prompts.%2520From%2520its%250Ainception%252C%2520the%2520field%2520has%2520advanced%2520from%2520adversarial%2520models%2520to%2520diffusion-based%250Amodels%252C%2520yielding%2520higher-fidelity%252C%2520temporally%2520consistent%2520outputs.%2520Yet%2520challenges%250Apersist%252C%2520such%2520as%2520alignment%252C%2520long-range%2520coherence%252C%2520and%2520computational%2520efficiency.%250AAddressing%2520this%2520evolving%2520landscape%252C%2520we%2520present%2520a%2520comprehensive%2520survey%2520of%250Atext-to-video%2520generative%2520models%252C%2520tracing%2520their%2520development%2520from%2520early%2520GANs%2520and%250AVAEs%2520to%2520hybrid%2520Diffusion-Transformer%2520%2528DiT%2529%2520architectures%252C%2520detailing%2520how%2520these%250Amodels%2520work%252C%2520what%2520limitations%2520they%2520addressed%2520in%2520their%2520predecessors%252C%2520and%2520why%250Ashifts%2520toward%2520new%2520architectural%2520paradigms%2520were%2520necessary%2520to%2520overcome%2520challenges%250Ain%2520quality%252C%2520coherence%252C%2520and%2520control.%2520We%2520provide%2520a%2520systematic%2520account%2520of%2520the%250Adatasets%252C%2520which%2520the%2520surveyed%2520text-to-video%2520models%2520were%2520trained%2520and%2520evaluated%250Aon%252C%2520and%252C%2520to%2520support%2520reproducibility%2520and%2520assess%2520the%2520accessibility%2520of%2520training%250Asuch%2520models%252C%2520we%2520detail%2520their%2520training%2520configurations%252C%2520including%2520their%2520hardware%250Aspecifications%252C%2520GPU%2520counts%252C%2520batch%2520sizes%252C%2520learning%2520rates%252C%2520optimizers%252C%2520epochs%252C%250Aand%2520other%2520key%2520hyperparameters.%2520Further%252C%2520we%2520outline%2520the%2520evaluation%2520metrics%250Acommonly%2520used%2520for%2520evaluating%2520such%2520models%2520and%2520present%2520their%2520performance%2520across%250Astandard%2520benchmarks%252C%2520while%2520also%2520discussing%2520the%2520limitations%2520of%2520these%2520metrics%2520and%250Athe%2520emerging%2520shift%2520toward%2520more%2520holistic%252C%2520perception-aligned%2520evaluation%250Astrategies.%2520Finally%252C%2520drawing%2520from%2520our%2520analysis%252C%2520we%2520outline%2520the%2520current%2520open%250Achallenges%2520and%2520propose%2520a%2520few%2520promising%2520future%2520directions%252C%2520laying%2520out%2520a%250Aperspective%2520for%2520future%2520researchers%2520to%2520explore%2520and%2520build%2520upon%2520in%2520advancing%2520T2V%250Aresearch%2520and%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Text%20and%20Video%20Generation%3A%20A%20Survey&entry.906535625=Nilay%20Kumar%20and%20Priyansh%20Bhandari%20and%20G.%20Maragatham&entry.1292438233=%20%20Text-to-video%20%28T2V%29%20generation%20technology%20holds%20potential%20to%20transform%0Amultiple%20domains%20such%20as%20education%2C%20marketing%2C%20entertainment%2C%20and%20assistive%0Atechnologies%20for%20individuals%20with%20visual%20or%20reading%20comprehension%20challenges%2C%0Aby%20creating%20coherent%20visual%20content%20from%20natural%20language%20prompts.%20From%20its%0Ainception%2C%20the%20field%20has%20advanced%20from%20adversarial%20models%20to%20diffusion-based%0Amodels%2C%20yielding%20higher-fidelity%2C%20temporally%20consistent%20outputs.%20Yet%20challenges%0Apersist%2C%20such%20as%20alignment%2C%20long-range%20coherence%2C%20and%20computational%20efficiency.%0AAddressing%20this%20evolving%20landscape%2C%20we%20present%20a%20comprehensive%20survey%20of%0Atext-to-video%20generative%20models%2C%20tracing%20their%20development%20from%20early%20GANs%20and%0AVAEs%20to%20hybrid%20Diffusion-Transformer%20%28DiT%29%20architectures%2C%20detailing%20how%20these%0Amodels%20work%2C%20what%20limitations%20they%20addressed%20in%20their%20predecessors%2C%20and%20why%0Ashifts%20toward%20new%20architectural%20paradigms%20were%20necessary%20to%20overcome%20challenges%0Ain%20quality%2C%20coherence%2C%20and%20control.%20We%20provide%20a%20systematic%20account%20of%20the%0Adatasets%2C%20which%20the%20surveyed%20text-to-video%20models%20were%20trained%20and%20evaluated%0Aon%2C%20and%2C%20to%20support%20reproducibility%20and%20assess%20the%20accessibility%20of%20training%0Asuch%20models%2C%20we%20detail%20their%20training%20configurations%2C%20including%20their%20hardware%0Aspecifications%2C%20GPU%20counts%2C%20batch%20sizes%2C%20learning%20rates%2C%20optimizers%2C%20epochs%2C%0Aand%20other%20key%20hyperparameters.%20Further%2C%20we%20outline%20the%20evaluation%20metrics%0Acommonly%20used%20for%20evaluating%20such%20models%20and%20present%20their%20performance%20across%0Astandard%20benchmarks%2C%20while%20also%20discussing%20the%20limitations%20of%20these%20metrics%20and%0Athe%20emerging%20shift%20toward%20more%20holistic%2C%20perception-aligned%20evaluation%0Astrategies.%20Finally%2C%20drawing%20from%20our%20analysis%2C%20we%20outline%20the%20current%20open%0Achallenges%20and%20propose%20a%20few%20promising%20future%20directions%2C%20laying%20out%20a%0Aperspective%20for%20future%20researchers%20to%20explore%20and%20build%20upon%20in%20advancing%20T2V%0Aresearch%20and%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04999v1&entry.124074799=Read"},
{"title": "PENEX: AdaBoost-Inspired Neural Network Regularization", "author": "Klaus-Rudolf Kladny and Bernhard Sch\u00f6lkopf and Michael Muehlebach", "abstract": "  AdaBoost sequentially fits so-called weak learners to minimize an exponential\nloss, which penalizes mislabeled data points more severely than other loss\nfunctions like cross-entropy. Paradoxically, AdaBoost generalizes well in\npractice as the number of weak learners grows. In the present work, we\nintroduce Penalized Exponential Loss (PENEX), a new formulation of the\nmulti-class exponential loss that is theoretically grounded and, in contrast to\nthe existing formulation, amenable to optimization via first-order methods. We\ndemonstrate both empirically and theoretically that PENEX implicitly maximizes\nmargins of data points. Also, we show that gradient increments on PENEX\nimplicitly parameterize weak learners in the boosting framework. Across\ncomputer vision and language tasks, we show that PENEX exhibits a regularizing\neffect often better than established methods with similar computational cost.\nOur results highlight PENEX's potential as an AdaBoost-inspired alternative for\neffective training and fine-tuning of deep neural networks.\n", "link": "http://arxiv.org/abs/2510.02107v2", "date": "2025-10-06", "relevancy": 2.6525, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6269}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4911}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PENEX%3A%20AdaBoost-Inspired%20Neural%20Network%20Regularization&body=Title%3A%20PENEX%3A%20AdaBoost-Inspired%20Neural%20Network%20Regularization%0AAuthor%3A%20Klaus-Rudolf%20Kladny%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Michael%20Muehlebach%0AAbstract%3A%20%20%20AdaBoost%20sequentially%20fits%20so-called%20weak%20learners%20to%20minimize%20an%20exponential%0Aloss%2C%20which%20penalizes%20mislabeled%20data%20points%20more%20severely%20than%20other%20loss%0Afunctions%20like%20cross-entropy.%20Paradoxically%2C%20AdaBoost%20generalizes%20well%20in%0Apractice%20as%20the%20number%20of%20weak%20learners%20grows.%20In%20the%20present%20work%2C%20we%0Aintroduce%20Penalized%20Exponential%20Loss%20%28PENEX%29%2C%20a%20new%20formulation%20of%20the%0Amulti-class%20exponential%20loss%20that%20is%20theoretically%20grounded%20and%2C%20in%20contrast%20to%0Athe%20existing%20formulation%2C%20amenable%20to%20optimization%20via%20first-order%20methods.%20We%0Ademonstrate%20both%20empirically%20and%20theoretically%20that%20PENEX%20implicitly%20maximizes%0Amargins%20of%20data%20points.%20Also%2C%20we%20show%20that%20gradient%20increments%20on%20PENEX%0Aimplicitly%20parameterize%20weak%20learners%20in%20the%20boosting%20framework.%20Across%0Acomputer%20vision%20and%20language%20tasks%2C%20we%20show%20that%20PENEX%20exhibits%20a%20regularizing%0Aeffect%20often%20better%20than%20established%20methods%20with%20similar%20computational%20cost.%0AOur%20results%20highlight%20PENEX%27s%20potential%20as%20an%20AdaBoost-inspired%20alternative%20for%0Aeffective%20training%20and%20fine-tuning%20of%20deep%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02107v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPENEX%253A%2520AdaBoost-Inspired%2520Neural%2520Network%2520Regularization%26entry.906535625%3DKlaus-Rudolf%2520Kladny%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Michael%2520Muehlebach%26entry.1292438233%3D%2520%2520AdaBoost%2520sequentially%2520fits%2520so-called%2520weak%2520learners%2520to%2520minimize%2520an%2520exponential%250Aloss%252C%2520which%2520penalizes%2520mislabeled%2520data%2520points%2520more%2520severely%2520than%2520other%2520loss%250Afunctions%2520like%2520cross-entropy.%2520Paradoxically%252C%2520AdaBoost%2520generalizes%2520well%2520in%250Apractice%2520as%2520the%2520number%2520of%2520weak%2520learners%2520grows.%2520In%2520the%2520present%2520work%252C%2520we%250Aintroduce%2520Penalized%2520Exponential%2520Loss%2520%2528PENEX%2529%252C%2520a%2520new%2520formulation%2520of%2520the%250Amulti-class%2520exponential%2520loss%2520that%2520is%2520theoretically%2520grounded%2520and%252C%2520in%2520contrast%2520to%250Athe%2520existing%2520formulation%252C%2520amenable%2520to%2520optimization%2520via%2520first-order%2520methods.%2520We%250Ademonstrate%2520both%2520empirically%2520and%2520theoretically%2520that%2520PENEX%2520implicitly%2520maximizes%250Amargins%2520of%2520data%2520points.%2520Also%252C%2520we%2520show%2520that%2520gradient%2520increments%2520on%2520PENEX%250Aimplicitly%2520parameterize%2520weak%2520learners%2520in%2520the%2520boosting%2520framework.%2520Across%250Acomputer%2520vision%2520and%2520language%2520tasks%252C%2520we%2520show%2520that%2520PENEX%2520exhibits%2520a%2520regularizing%250Aeffect%2520often%2520better%2520than%2520established%2520methods%2520with%2520similar%2520computational%2520cost.%250AOur%2520results%2520highlight%2520PENEX%2527s%2520potential%2520as%2520an%2520AdaBoost-inspired%2520alternative%2520for%250Aeffective%2520training%2520and%2520fine-tuning%2520of%2520deep%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02107v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PENEX%3A%20AdaBoost-Inspired%20Neural%20Network%20Regularization&entry.906535625=Klaus-Rudolf%20Kladny%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Michael%20Muehlebach&entry.1292438233=%20%20AdaBoost%20sequentially%20fits%20so-called%20weak%20learners%20to%20minimize%20an%20exponential%0Aloss%2C%20which%20penalizes%20mislabeled%20data%20points%20more%20severely%20than%20other%20loss%0Afunctions%20like%20cross-entropy.%20Paradoxically%2C%20AdaBoost%20generalizes%20well%20in%0Apractice%20as%20the%20number%20of%20weak%20learners%20grows.%20In%20the%20present%20work%2C%20we%0Aintroduce%20Penalized%20Exponential%20Loss%20%28PENEX%29%2C%20a%20new%20formulation%20of%20the%0Amulti-class%20exponential%20loss%20that%20is%20theoretically%20grounded%20and%2C%20in%20contrast%20to%0Athe%20existing%20formulation%2C%20amenable%20to%20optimization%20via%20first-order%20methods.%20We%0Ademonstrate%20both%20empirically%20and%20theoretically%20that%20PENEX%20implicitly%20maximizes%0Amargins%20of%20data%20points.%20Also%2C%20we%20show%20that%20gradient%20increments%20on%20PENEX%0Aimplicitly%20parameterize%20weak%20learners%20in%20the%20boosting%20framework.%20Across%0Acomputer%20vision%20and%20language%20tasks%2C%20we%20show%20that%20PENEX%20exhibits%20a%20regularizing%0Aeffect%20often%20better%20than%20established%20methods%20with%20similar%20computational%20cost.%0AOur%20results%20highlight%20PENEX%27s%20potential%20as%20an%20AdaBoost-inspired%20alternative%20for%0Aeffective%20training%20and%20fine-tuning%20of%20deep%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02107v2&entry.124074799=Read"},
{"title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity", "author": "Jiayi Zhang and Simon Yu and Derek Chong and Anthony Sicilia and Michael R. Tomz and Christopher D. Manning and Weiyan Shi", "abstract": "  Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n\"Generate 5 jokes about coffee and their corresponding probabilities\").\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.\n", "link": "http://arxiv.org/abs/2510.01171v2", "date": "2025-10-06", "relevancy": 2.6055, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5291}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5187}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verbalized%20Sampling%3A%20How%20to%20Mitigate%20Mode%20Collapse%20and%20Unlock%20LLM%0A%20%20Diversity&body=Title%3A%20Verbalized%20Sampling%3A%20How%20to%20Mitigate%20Mode%20Collapse%20and%20Unlock%20LLM%0A%20%20Diversity%0AAuthor%3A%20Jiayi%20Zhang%20and%20Simon%20Yu%20and%20Derek%20Chong%20and%20Anthony%20Sicilia%20and%20Michael%20R.%20Tomz%20and%20Christopher%20D.%20Manning%20and%20Weiyan%20Shi%0AAbstract%3A%20%20%20Post-training%20alignment%20often%20reduces%20LLM%20diversity%2C%20leading%20to%20a%20phenomenon%0Aknown%20as%20mode%20collapse.%20Unlike%20prior%20work%20that%20attributes%20this%20effect%20to%0Aalgorithmic%20limitations%2C%20we%20identify%20a%20fundamental%2C%20pervasive%20data-level%0Adriver%3A%20typicality%20bias%20in%20preference%20data%2C%20whereby%20annotators%20systematically%0Afavor%20familiar%20text%20as%20a%20result%20of%20well-established%20findings%20in%20cognitive%0Apsychology.%20We%20formalize%20this%20bias%20theoretically%2C%20verify%20it%20on%20preference%0Adatasets%20empirically%2C%20and%20show%20that%20it%20plays%20a%20central%20role%20in%20mode%20collapse.%0AMotivated%20by%20this%20analysis%2C%20we%20introduce%20Verbalized%20Sampling%2C%20a%20simple%2C%0Atraining-free%20prompting%20strategy%20to%20circumvent%20mode%20collapse.%20VS%20prompts%20the%0Amodel%20to%20verbalize%20a%20probability%20distribution%20over%20a%20set%20of%20responses%20%28e.g.%2C%0A%22Generate%205%20jokes%20about%20coffee%20and%20their%20corresponding%20probabilities%22%29.%0AComprehensive%20experiments%20show%20that%20VS%20significantly%20improves%20performance%0Aacross%20creative%20writing%20%28poems%2C%20stories%2C%20jokes%29%2C%20dialogue%20simulation%2C%0Aopen-ended%20QA%2C%20and%20synthetic%20data%20generation%2C%20without%20sacrificing%20factual%0Aaccuracy%20and%20safety.%20For%20instance%2C%20in%20creative%20writing%2C%20VS%20increases%20diversity%0Aby%201.6-2.1x%20over%20direct%20prompting.%20We%20further%20observe%20an%20emergent%20trend%20that%0Amore%20capable%20models%20benefit%20more%20from%20VS.%20In%20sum%2C%20our%20work%20provides%20a%20new%0Adata-centric%20perspective%20on%20mode%20collapse%20and%20a%20practical%20inference-time%20remedy%0Athat%20helps%20unlock%20pre-trained%20generative%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.01171v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerbalized%2520Sampling%253A%2520How%2520to%2520Mitigate%2520Mode%2520Collapse%2520and%2520Unlock%2520LLM%250A%2520%2520Diversity%26entry.906535625%3DJiayi%2520Zhang%2520and%2520Simon%2520Yu%2520and%2520Derek%2520Chong%2520and%2520Anthony%2520Sicilia%2520and%2520Michael%2520R.%2520Tomz%2520and%2520Christopher%2520D.%2520Manning%2520and%2520Weiyan%2520Shi%26entry.1292438233%3D%2520%2520Post-training%2520alignment%2520often%2520reduces%2520LLM%2520diversity%252C%2520leading%2520to%2520a%2520phenomenon%250Aknown%2520as%2520mode%2520collapse.%2520Unlike%2520prior%2520work%2520that%2520attributes%2520this%2520effect%2520to%250Aalgorithmic%2520limitations%252C%2520we%2520identify%2520a%2520fundamental%252C%2520pervasive%2520data-level%250Adriver%253A%2520typicality%2520bias%2520in%2520preference%2520data%252C%2520whereby%2520annotators%2520systematically%250Afavor%2520familiar%2520text%2520as%2520a%2520result%2520of%2520well-established%2520findings%2520in%2520cognitive%250Apsychology.%2520We%2520formalize%2520this%2520bias%2520theoretically%252C%2520verify%2520it%2520on%2520preference%250Adatasets%2520empirically%252C%2520and%2520show%2520that%2520it%2520plays%2520a%2520central%2520role%2520in%2520mode%2520collapse.%250AMotivated%2520by%2520this%2520analysis%252C%2520we%2520introduce%2520Verbalized%2520Sampling%252C%2520a%2520simple%252C%250Atraining-free%2520prompting%2520strategy%2520to%2520circumvent%2520mode%2520collapse.%2520VS%2520prompts%2520the%250Amodel%2520to%2520verbalize%2520a%2520probability%2520distribution%2520over%2520a%2520set%2520of%2520responses%2520%2528e.g.%252C%250A%2522Generate%25205%2520jokes%2520about%2520coffee%2520and%2520their%2520corresponding%2520probabilities%2522%2529.%250AComprehensive%2520experiments%2520show%2520that%2520VS%2520significantly%2520improves%2520performance%250Aacross%2520creative%2520writing%2520%2528poems%252C%2520stories%252C%2520jokes%2529%252C%2520dialogue%2520simulation%252C%250Aopen-ended%2520QA%252C%2520and%2520synthetic%2520data%2520generation%252C%2520without%2520sacrificing%2520factual%250Aaccuracy%2520and%2520safety.%2520For%2520instance%252C%2520in%2520creative%2520writing%252C%2520VS%2520increases%2520diversity%250Aby%25201.6-2.1x%2520over%2520direct%2520prompting.%2520We%2520further%2520observe%2520an%2520emergent%2520trend%2520that%250Amore%2520capable%2520models%2520benefit%2520more%2520from%2520VS.%2520In%2520sum%252C%2520our%2520work%2520provides%2520a%2520new%250Adata-centric%2520perspective%2520on%2520mode%2520collapse%2520and%2520a%2520practical%2520inference-time%2520remedy%250Athat%2520helps%2520unlock%2520pre-trained%2520generative%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01171v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verbalized%20Sampling%3A%20How%20to%20Mitigate%20Mode%20Collapse%20and%20Unlock%20LLM%0A%20%20Diversity&entry.906535625=Jiayi%20Zhang%20and%20Simon%20Yu%20and%20Derek%20Chong%20and%20Anthony%20Sicilia%20and%20Michael%20R.%20Tomz%20and%20Christopher%20D.%20Manning%20and%20Weiyan%20Shi&entry.1292438233=%20%20Post-training%20alignment%20often%20reduces%20LLM%20diversity%2C%20leading%20to%20a%20phenomenon%0Aknown%20as%20mode%20collapse.%20Unlike%20prior%20work%20that%20attributes%20this%20effect%20to%0Aalgorithmic%20limitations%2C%20we%20identify%20a%20fundamental%2C%20pervasive%20data-level%0Adriver%3A%20typicality%20bias%20in%20preference%20data%2C%20whereby%20annotators%20systematically%0Afavor%20familiar%20text%20as%20a%20result%20of%20well-established%20findings%20in%20cognitive%0Apsychology.%20We%20formalize%20this%20bias%20theoretically%2C%20verify%20it%20on%20preference%0Adatasets%20empirically%2C%20and%20show%20that%20it%20plays%20a%20central%20role%20in%20mode%20collapse.%0AMotivated%20by%20this%20analysis%2C%20we%20introduce%20Verbalized%20Sampling%2C%20a%20simple%2C%0Atraining-free%20prompting%20strategy%20to%20circumvent%20mode%20collapse.%20VS%20prompts%20the%0Amodel%20to%20verbalize%20a%20probability%20distribution%20over%20a%20set%20of%20responses%20%28e.g.%2C%0A%22Generate%205%20jokes%20about%20coffee%20and%20their%20corresponding%20probabilities%22%29.%0AComprehensive%20experiments%20show%20that%20VS%20significantly%20improves%20performance%0Aacross%20creative%20writing%20%28poems%2C%20stories%2C%20jokes%29%2C%20dialogue%20simulation%2C%0Aopen-ended%20QA%2C%20and%20synthetic%20data%20generation%2C%20without%20sacrificing%20factual%0Aaccuracy%20and%20safety.%20For%20instance%2C%20in%20creative%20writing%2C%20VS%20increases%20diversity%0Aby%201.6-2.1x%20over%20direct%20prompting.%20We%20further%20observe%20an%20emergent%20trend%20that%0Amore%20capable%20models%20benefit%20more%20from%20VS.%20In%20sum%2C%20our%20work%20provides%20a%20new%0Adata-centric%20perspective%20on%20mode%20collapse%20and%20a%20practical%20inference-time%20remedy%0Athat%20helps%20unlock%20pre-trained%20generative%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.01171v2&entry.124074799=Read"},
{"title": "AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework\n  for Identifying Cultural Capital in STEM Narratives", "author": "Khalid Mehtab Khan and Anagha Kulkarni", "abstract": "  Identifying cultural capital (CC) themes in student reflections can offer\nvaluable insights that help foster equitable learning environments in\nclassrooms. However, themes such as aspirational goals or family support are\noften woven into narratives, rather than appearing as direct keywords. This\nmakes them difficult to detect for standard NLP models that process sentences\nin isolation. The core challenge stems from a lack of awareness, as standard\nmodels are pre-trained on general corpora, leaving them blind to the\ndomain-specific language and narrative context inherent to the data. To address\nthis, we introduce AWARE, a framework that systematically attempts to improve a\ntransformer model's awareness for this nuanced task. AWARE has three core\ncomponents: 1) Domain Awareness, adapting the model's vocabulary to the\nlinguistic style of student reflections; 2) Context Awareness, generating\nsentence embeddings that are aware of the full essay context; and 3) Class\nOverlap Awareness, employing a multi-label strategy to recognize the\ncoexistence of themes in a single sentence. Our results show that by making the\nmodel explicitly aware of the properties of the input, AWARE outperforms a\nstrong baseline by 2.1 percentage points in Macro-F1 and shows considerable\nimprovements across all themes. This work provides a robust and generalizable\nmethodology for any text classification task in which meaning depends on the\ncontext of the narrative.\n", "link": "http://arxiv.org/abs/2510.04983v1", "date": "2025-10-06", "relevancy": 2.5954, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AWARE%2C%20Beyond%20Sentence%20Boundaries%3A%20A%20Contextual%20Transformer%20Framework%0A%20%20for%20Identifying%20Cultural%20Capital%20in%20STEM%20Narratives&body=Title%3A%20AWARE%2C%20Beyond%20Sentence%20Boundaries%3A%20A%20Contextual%20Transformer%20Framework%0A%20%20for%20Identifying%20Cultural%20Capital%20in%20STEM%20Narratives%0AAuthor%3A%20Khalid%20Mehtab%20Khan%20and%20Anagha%20Kulkarni%0AAbstract%3A%20%20%20Identifying%20cultural%20capital%20%28CC%29%20themes%20in%20student%20reflections%20can%20offer%0Avaluable%20insights%20that%20help%20foster%20equitable%20learning%20environments%20in%0Aclassrooms.%20However%2C%20themes%20such%20as%20aspirational%20goals%20or%20family%20support%20are%0Aoften%20woven%20into%20narratives%2C%20rather%20than%20appearing%20as%20direct%20keywords.%20This%0Amakes%20them%20difficult%20to%20detect%20for%20standard%20NLP%20models%20that%20process%20sentences%0Ain%20isolation.%20The%20core%20challenge%20stems%20from%20a%20lack%20of%20awareness%2C%20as%20standard%0Amodels%20are%20pre-trained%20on%20general%20corpora%2C%20leaving%20them%20blind%20to%20the%0Adomain-specific%20language%20and%20narrative%20context%20inherent%20to%20the%20data.%20To%20address%0Athis%2C%20we%20introduce%20AWARE%2C%20a%20framework%20that%20systematically%20attempts%20to%20improve%20a%0Atransformer%20model%27s%20awareness%20for%20this%20nuanced%20task.%20AWARE%20has%20three%20core%0Acomponents%3A%201%29%20Domain%20Awareness%2C%20adapting%20the%20model%27s%20vocabulary%20to%20the%0Alinguistic%20style%20of%20student%20reflections%3B%202%29%20Context%20Awareness%2C%20generating%0Asentence%20embeddings%20that%20are%20aware%20of%20the%20full%20essay%20context%3B%20and%203%29%20Class%0AOverlap%20Awareness%2C%20employing%20a%20multi-label%20strategy%20to%20recognize%20the%0Acoexistence%20of%20themes%20in%20a%20single%20sentence.%20Our%20results%20show%20that%20by%20making%20the%0Amodel%20explicitly%20aware%20of%20the%20properties%20of%20the%20input%2C%20AWARE%20outperforms%20a%0Astrong%20baseline%20by%202.1%20percentage%20points%20in%20Macro-F1%20and%20shows%20considerable%0Aimprovements%20across%20all%20themes.%20This%20work%20provides%20a%20robust%20and%20generalizable%0Amethodology%20for%20any%20text%20classification%20task%20in%20which%20meaning%20depends%20on%20the%0Acontext%20of%20the%20narrative.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAWARE%252C%2520Beyond%2520Sentence%2520Boundaries%253A%2520A%2520Contextual%2520Transformer%2520Framework%250A%2520%2520for%2520Identifying%2520Cultural%2520Capital%2520in%2520STEM%2520Narratives%26entry.906535625%3DKhalid%2520Mehtab%2520Khan%2520and%2520Anagha%2520Kulkarni%26entry.1292438233%3D%2520%2520Identifying%2520cultural%2520capital%2520%2528CC%2529%2520themes%2520in%2520student%2520reflections%2520can%2520offer%250Avaluable%2520insights%2520that%2520help%2520foster%2520equitable%2520learning%2520environments%2520in%250Aclassrooms.%2520However%252C%2520themes%2520such%2520as%2520aspirational%2520goals%2520or%2520family%2520support%2520are%250Aoften%2520woven%2520into%2520narratives%252C%2520rather%2520than%2520appearing%2520as%2520direct%2520keywords.%2520This%250Amakes%2520them%2520difficult%2520to%2520detect%2520for%2520standard%2520NLP%2520models%2520that%2520process%2520sentences%250Ain%2520isolation.%2520The%2520core%2520challenge%2520stems%2520from%2520a%2520lack%2520of%2520awareness%252C%2520as%2520standard%250Amodels%2520are%2520pre-trained%2520on%2520general%2520corpora%252C%2520leaving%2520them%2520blind%2520to%2520the%250Adomain-specific%2520language%2520and%2520narrative%2520context%2520inherent%2520to%2520the%2520data.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520AWARE%252C%2520a%2520framework%2520that%2520systematically%2520attempts%2520to%2520improve%2520a%250Atransformer%2520model%2527s%2520awareness%2520for%2520this%2520nuanced%2520task.%2520AWARE%2520has%2520three%2520core%250Acomponents%253A%25201%2529%2520Domain%2520Awareness%252C%2520adapting%2520the%2520model%2527s%2520vocabulary%2520to%2520the%250Alinguistic%2520style%2520of%2520student%2520reflections%253B%25202%2529%2520Context%2520Awareness%252C%2520generating%250Asentence%2520embeddings%2520that%2520are%2520aware%2520of%2520the%2520full%2520essay%2520context%253B%2520and%25203%2529%2520Class%250AOverlap%2520Awareness%252C%2520employing%2520a%2520multi-label%2520strategy%2520to%2520recognize%2520the%250Acoexistence%2520of%2520themes%2520in%2520a%2520single%2520sentence.%2520Our%2520results%2520show%2520that%2520by%2520making%2520the%250Amodel%2520explicitly%2520aware%2520of%2520the%2520properties%2520of%2520the%2520input%252C%2520AWARE%2520outperforms%2520a%250Astrong%2520baseline%2520by%25202.1%2520percentage%2520points%2520in%2520Macro-F1%2520and%2520shows%2520considerable%250Aimprovements%2520across%2520all%2520themes.%2520This%2520work%2520provides%2520a%2520robust%2520and%2520generalizable%250Amethodology%2520for%2520any%2520text%2520classification%2520task%2520in%2520which%2520meaning%2520depends%2520on%2520the%250Acontext%2520of%2520the%2520narrative.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AWARE%2C%20Beyond%20Sentence%20Boundaries%3A%20A%20Contextual%20Transformer%20Framework%0A%20%20for%20Identifying%20Cultural%20Capital%20in%20STEM%20Narratives&entry.906535625=Khalid%20Mehtab%20Khan%20and%20Anagha%20Kulkarni&entry.1292438233=%20%20Identifying%20cultural%20capital%20%28CC%29%20themes%20in%20student%20reflections%20can%20offer%0Avaluable%20insights%20that%20help%20foster%20equitable%20learning%20environments%20in%0Aclassrooms.%20However%2C%20themes%20such%20as%20aspirational%20goals%20or%20family%20support%20are%0Aoften%20woven%20into%20narratives%2C%20rather%20than%20appearing%20as%20direct%20keywords.%20This%0Amakes%20them%20difficult%20to%20detect%20for%20standard%20NLP%20models%20that%20process%20sentences%0Ain%20isolation.%20The%20core%20challenge%20stems%20from%20a%20lack%20of%20awareness%2C%20as%20standard%0Amodels%20are%20pre-trained%20on%20general%20corpora%2C%20leaving%20them%20blind%20to%20the%0Adomain-specific%20language%20and%20narrative%20context%20inherent%20to%20the%20data.%20To%20address%0Athis%2C%20we%20introduce%20AWARE%2C%20a%20framework%20that%20systematically%20attempts%20to%20improve%20a%0Atransformer%20model%27s%20awareness%20for%20this%20nuanced%20task.%20AWARE%20has%20three%20core%0Acomponents%3A%201%29%20Domain%20Awareness%2C%20adapting%20the%20model%27s%20vocabulary%20to%20the%0Alinguistic%20style%20of%20student%20reflections%3B%202%29%20Context%20Awareness%2C%20generating%0Asentence%20embeddings%20that%20are%20aware%20of%20the%20full%20essay%20context%3B%20and%203%29%20Class%0AOverlap%20Awareness%2C%20employing%20a%20multi-label%20strategy%20to%20recognize%20the%0Acoexistence%20of%20themes%20in%20a%20single%20sentence.%20Our%20results%20show%20that%20by%20making%20the%0Amodel%20explicitly%20aware%20of%20the%20properties%20of%20the%20input%2C%20AWARE%20outperforms%20a%0Astrong%20baseline%20by%202.1%20percentage%20points%20in%20Macro-F1%20and%20shows%20considerable%0Aimprovements%20across%20all%20themes.%20This%20work%20provides%20a%20robust%20and%20generalizable%0Amethodology%20for%20any%20text%20classification%20task%20in%20which%20meaning%20depends%20on%20the%0Acontext%20of%20the%20narrative.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04983v1&entry.124074799=Read"},
{"title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM\n  Training", "author": "Wei Xiong and Chenlu Ye and Baohao Liao and Hanze Dong and Xinxing Xu and Christof Monz and Jiang Bian and Nan Jiang and Tong Zhang", "abstract": "  Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada.\n", "link": "http://arxiv.org/abs/2510.04996v1", "date": "2025-10-06", "relevancy": 2.5718, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5347}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.511}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforce-Ada%3A%20An%20Adaptive%20Sampling%20Framework%20for%20Reinforce-Style%20LLM%0A%20%20Training&body=Title%3A%20Reinforce-Ada%3A%20An%20Adaptive%20Sampling%20Framework%20for%20Reinforce-Style%20LLM%0A%20%20Training%0AAuthor%3A%20Wei%20Xiong%20and%20Chenlu%20Ye%20and%20Baohao%20Liao%20and%20Hanze%20Dong%20and%20Xinxing%20Xu%20and%20Christof%20Monz%20and%20Jiang%20Bian%20and%20Nan%20Jiang%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Reinforcement%20learning%20applied%20to%20large%20language%20models%20%28LLMs%29%20for%20reasoning%0Atasks%20is%20often%20bottlenecked%20by%20unstable%20gradient%20estimates%20due%20to%20fixed%20and%0Auniform%20sampling%20of%20responses%20across%20prompts.%20Prior%20work%20such%20as%20GVM-RAFT%0Aaddresses%20this%20by%20dynamically%20allocating%20inference%20budget%20per%20prompt%20to%0Aminimize%20stochastic%20gradient%20variance%20under%20a%20budget%20constraint.%20Inspired%20by%0Athis%20insight%2C%20we%20propose%20Reinforce-Ada%2C%20an%20adaptive%20sampling%20framework%20for%0Aonline%20RL%20post-training%20of%20LLMs%20that%20continuously%20reallocates%20sampling%20effort%0Ato%20the%20prompts%20with%20the%20greatest%20uncertainty%20or%20learning%20potential.%20Unlike%0Aconventional%20two-stage%20allocation%20methods%2C%20Reinforce-Ada%20interleaves%20estimation%0Aand%20sampling%20in%20an%20online%20successive%20elimination%20process%2C%20and%20automatically%0Astops%20sampling%20for%20a%20prompt%20once%20sufficient%20signal%20is%20collected.%20To%20stabilize%0Aupdates%2C%20we%20form%20fixed-size%20groups%20with%20enforced%20reward%20diversity%20and%20compute%0Aadvantage%20baselines%20using%20global%20statistics%20aggregated%20over%20the%20adaptive%0Asampling%20phase.%20Empirical%20results%20across%20multiple%20model%20architectures%20and%0Areasoning%20benchmarks%20show%20that%20Reinforce-Ada%20accelerates%20convergence%20and%0Aimproves%20final%20performance%20compared%20to%20GRPO%2C%20especially%20when%20using%20the%20balanced%0Asampling%20variant.%20Our%20work%20highlights%20the%20central%20role%20of%20variance-aware%2C%0Aadaptive%20data%20curation%20in%20enabling%20efficient%20and%20reliable%20reinforcement%0Alearning%20for%20reasoning-capable%20LLMs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/RLHFlow/Reinforce-Ada.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforce-Ada%253A%2520An%2520Adaptive%2520Sampling%2520Framework%2520for%2520Reinforce-Style%2520LLM%250A%2520%2520Training%26entry.906535625%3DWei%2520Xiong%2520and%2520Chenlu%2520Ye%2520and%2520Baohao%2520Liao%2520and%2520Hanze%2520Dong%2520and%2520Xinxing%2520Xu%2520and%2520Christof%2520Monz%2520and%2520Jiang%2520Bian%2520and%2520Nan%2520Jiang%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520applied%2520to%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520reasoning%250Atasks%2520is%2520often%2520bottlenecked%2520by%2520unstable%2520gradient%2520estimates%2520due%2520to%2520fixed%2520and%250Auniform%2520sampling%2520of%2520responses%2520across%2520prompts.%2520Prior%2520work%2520such%2520as%2520GVM-RAFT%250Aaddresses%2520this%2520by%2520dynamically%2520allocating%2520inference%2520budget%2520per%2520prompt%2520to%250Aminimize%2520stochastic%2520gradient%2520variance%2520under%2520a%2520budget%2520constraint.%2520Inspired%2520by%250Athis%2520insight%252C%2520we%2520propose%2520Reinforce-Ada%252C%2520an%2520adaptive%2520sampling%2520framework%2520for%250Aonline%2520RL%2520post-training%2520of%2520LLMs%2520that%2520continuously%2520reallocates%2520sampling%2520effort%250Ato%2520the%2520prompts%2520with%2520the%2520greatest%2520uncertainty%2520or%2520learning%2520potential.%2520Unlike%250Aconventional%2520two-stage%2520allocation%2520methods%252C%2520Reinforce-Ada%2520interleaves%2520estimation%250Aand%2520sampling%2520in%2520an%2520online%2520successive%2520elimination%2520process%252C%2520and%2520automatically%250Astops%2520sampling%2520for%2520a%2520prompt%2520once%2520sufficient%2520signal%2520is%2520collected.%2520To%2520stabilize%250Aupdates%252C%2520we%2520form%2520fixed-size%2520groups%2520with%2520enforced%2520reward%2520diversity%2520and%2520compute%250Aadvantage%2520baselines%2520using%2520global%2520statistics%2520aggregated%2520over%2520the%2520adaptive%250Asampling%2520phase.%2520Empirical%2520results%2520across%2520multiple%2520model%2520architectures%2520and%250Areasoning%2520benchmarks%2520show%2520that%2520Reinforce-Ada%2520accelerates%2520convergence%2520and%250Aimproves%2520final%2520performance%2520compared%2520to%2520GRPO%252C%2520especially%2520when%2520using%2520the%2520balanced%250Asampling%2520variant.%2520Our%2520work%2520highlights%2520the%2520central%2520role%2520of%2520variance-aware%252C%250Aadaptive%2520data%2520curation%2520in%2520enabling%2520efficient%2520and%2520reliable%2520reinforcement%250Alearning%2520for%2520reasoning-capable%2520LLMs.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/RLHFlow/Reinforce-Ada.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforce-Ada%3A%20An%20Adaptive%20Sampling%20Framework%20for%20Reinforce-Style%20LLM%0A%20%20Training&entry.906535625=Wei%20Xiong%20and%20Chenlu%20Ye%20and%20Baohao%20Liao%20and%20Hanze%20Dong%20and%20Xinxing%20Xu%20and%20Christof%20Monz%20and%20Jiang%20Bian%20and%20Nan%20Jiang%20and%20Tong%20Zhang&entry.1292438233=%20%20Reinforcement%20learning%20applied%20to%20large%20language%20models%20%28LLMs%29%20for%20reasoning%0Atasks%20is%20often%20bottlenecked%20by%20unstable%20gradient%20estimates%20due%20to%20fixed%20and%0Auniform%20sampling%20of%20responses%20across%20prompts.%20Prior%20work%20such%20as%20GVM-RAFT%0Aaddresses%20this%20by%20dynamically%20allocating%20inference%20budget%20per%20prompt%20to%0Aminimize%20stochastic%20gradient%20variance%20under%20a%20budget%20constraint.%20Inspired%20by%0Athis%20insight%2C%20we%20propose%20Reinforce-Ada%2C%20an%20adaptive%20sampling%20framework%20for%0Aonline%20RL%20post-training%20of%20LLMs%20that%20continuously%20reallocates%20sampling%20effort%0Ato%20the%20prompts%20with%20the%20greatest%20uncertainty%20or%20learning%20potential.%20Unlike%0Aconventional%20two-stage%20allocation%20methods%2C%20Reinforce-Ada%20interleaves%20estimation%0Aand%20sampling%20in%20an%20online%20successive%20elimination%20process%2C%20and%20automatically%0Astops%20sampling%20for%20a%20prompt%20once%20sufficient%20signal%20is%20collected.%20To%20stabilize%0Aupdates%2C%20we%20form%20fixed-size%20groups%20with%20enforced%20reward%20diversity%20and%20compute%0Aadvantage%20baselines%20using%20global%20statistics%20aggregated%20over%20the%20adaptive%0Asampling%20phase.%20Empirical%20results%20across%20multiple%20model%20architectures%20and%0Areasoning%20benchmarks%20show%20that%20Reinforce-Ada%20accelerates%20convergence%20and%0Aimproves%20final%20performance%20compared%20to%20GRPO%2C%20especially%20when%20using%20the%20balanced%0Asampling%20variant.%20Our%20work%20highlights%20the%20central%20role%20of%20variance-aware%2C%0Aadaptive%20data%20curation%20in%20enabling%20efficient%20and%20reliable%20reinforcement%0Alearning%20for%20reasoning-capable%20LLMs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/RLHFlow/Reinforce-Ada.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04996v1&entry.124074799=Read"},
{"title": "ActiveMark: on watermarking of visual foundation models via massive\n  activations", "author": "Anna Chistyakova and Mikhail Pautov", "abstract": "  Being trained on large and vast datasets, visual foundation models (VFMs) can\nbe fine-tuned for diverse downstream tasks, achieving remarkable performance\nand efficiency in various computer vision applications. The high computation\ncost of data collection and training motivates the owners of some VFMs to\ndistribute them alongside the license to protect their intellectual property\nrights. However, a dishonest user of the protected model's copy may illegally\nredistribute it, for example, to make a profit. As a consequence, the\ndevelopment of reliable ownership verification tools is of great importance\ntoday, since such methods can be used to differentiate between a redistributed\ncopy of the protected model and an independent model. In this paper, we propose\nan approach to ownership verification of visual foundation models by\nfine-tuning a small set of expressive layers of a VFM along with a small\nencoder-decoder network to embed digital watermarks into an internal\nrepresentation of a hold-out set of input images. Importantly, the watermarks\nembedded remain detectable in the functional copies of the protected model,\nobtained, for example, by fine-tuning the VFM for a particular downstream task.\nTheoretically and experimentally, we demonstrate that the proposed method\nyields a low probability of false detection of a non-watermarked model and a\nlow probability of false misdetection of a watermarked model.\n", "link": "http://arxiv.org/abs/2510.04966v1", "date": "2025-10-06", "relevancy": 2.5623, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5194}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5119}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ActiveMark%3A%20on%20watermarking%20of%20visual%20foundation%20models%20via%20massive%0A%20%20activations&body=Title%3A%20ActiveMark%3A%20on%20watermarking%20of%20visual%20foundation%20models%20via%20massive%0A%20%20activations%0AAuthor%3A%20Anna%20Chistyakova%20and%20Mikhail%20Pautov%0AAbstract%3A%20%20%20Being%20trained%20on%20large%20and%20vast%20datasets%2C%20visual%20foundation%20models%20%28VFMs%29%20can%0Abe%20fine-tuned%20for%20diverse%20downstream%20tasks%2C%20achieving%20remarkable%20performance%0Aand%20efficiency%20in%20various%20computer%20vision%20applications.%20The%20high%20computation%0Acost%20of%20data%20collection%20and%20training%20motivates%20the%20owners%20of%20some%20VFMs%20to%0Adistribute%20them%20alongside%20the%20license%20to%20protect%20their%20intellectual%20property%0Arights.%20However%2C%20a%20dishonest%20user%20of%20the%20protected%20model%27s%20copy%20may%20illegally%0Aredistribute%20it%2C%20for%20example%2C%20to%20make%20a%20profit.%20As%20a%20consequence%2C%20the%0Adevelopment%20of%20reliable%20ownership%20verification%20tools%20is%20of%20great%20importance%0Atoday%2C%20since%20such%20methods%20can%20be%20used%20to%20differentiate%20between%20a%20redistributed%0Acopy%20of%20the%20protected%20model%20and%20an%20independent%20model.%20In%20this%20paper%2C%20we%20propose%0Aan%20approach%20to%20ownership%20verification%20of%20visual%20foundation%20models%20by%0Afine-tuning%20a%20small%20set%20of%20expressive%20layers%20of%20a%20VFM%20along%20with%20a%20small%0Aencoder-decoder%20network%20to%20embed%20digital%20watermarks%20into%20an%20internal%0Arepresentation%20of%20a%20hold-out%20set%20of%20input%20images.%20Importantly%2C%20the%20watermarks%0Aembedded%20remain%20detectable%20in%20the%20functional%20copies%20of%20the%20protected%20model%2C%0Aobtained%2C%20for%20example%2C%20by%20fine-tuning%20the%20VFM%20for%20a%20particular%20downstream%20task.%0ATheoretically%20and%20experimentally%2C%20we%20demonstrate%20that%20the%20proposed%20method%0Ayields%20a%20low%20probability%20of%20false%20detection%20of%20a%20non-watermarked%20model%20and%20a%0Alow%20probability%20of%20false%20misdetection%20of%20a%20watermarked%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActiveMark%253A%2520on%2520watermarking%2520of%2520visual%2520foundation%2520models%2520via%2520massive%250A%2520%2520activations%26entry.906535625%3DAnna%2520Chistyakova%2520and%2520Mikhail%2520Pautov%26entry.1292438233%3D%2520%2520Being%2520trained%2520on%2520large%2520and%2520vast%2520datasets%252C%2520visual%2520foundation%2520models%2520%2528VFMs%2529%2520can%250Abe%2520fine-tuned%2520for%2520diverse%2520downstream%2520tasks%252C%2520achieving%2520remarkable%2520performance%250Aand%2520efficiency%2520in%2520various%2520computer%2520vision%2520applications.%2520The%2520high%2520computation%250Acost%2520of%2520data%2520collection%2520and%2520training%2520motivates%2520the%2520owners%2520of%2520some%2520VFMs%2520to%250Adistribute%2520them%2520alongside%2520the%2520license%2520to%2520protect%2520their%2520intellectual%2520property%250Arights.%2520However%252C%2520a%2520dishonest%2520user%2520of%2520the%2520protected%2520model%2527s%2520copy%2520may%2520illegally%250Aredistribute%2520it%252C%2520for%2520example%252C%2520to%2520make%2520a%2520profit.%2520As%2520a%2520consequence%252C%2520the%250Adevelopment%2520of%2520reliable%2520ownership%2520verification%2520tools%2520is%2520of%2520great%2520importance%250Atoday%252C%2520since%2520such%2520methods%2520can%2520be%2520used%2520to%2520differentiate%2520between%2520a%2520redistributed%250Acopy%2520of%2520the%2520protected%2520model%2520and%2520an%2520independent%2520model.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aan%2520approach%2520to%2520ownership%2520verification%2520of%2520visual%2520foundation%2520models%2520by%250Afine-tuning%2520a%2520small%2520set%2520of%2520expressive%2520layers%2520of%2520a%2520VFM%2520along%2520with%2520a%2520small%250Aencoder-decoder%2520network%2520to%2520embed%2520digital%2520watermarks%2520into%2520an%2520internal%250Arepresentation%2520of%2520a%2520hold-out%2520set%2520of%2520input%2520images.%2520Importantly%252C%2520the%2520watermarks%250Aembedded%2520remain%2520detectable%2520in%2520the%2520functional%2520copies%2520of%2520the%2520protected%2520model%252C%250Aobtained%252C%2520for%2520example%252C%2520by%2520fine-tuning%2520the%2520VFM%2520for%2520a%2520particular%2520downstream%2520task.%250ATheoretically%2520and%2520experimentally%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%2520method%250Ayields%2520a%2520low%2520probability%2520of%2520false%2520detection%2520of%2520a%2520non-watermarked%2520model%2520and%2520a%250Alow%2520probability%2520of%2520false%2520misdetection%2520of%2520a%2520watermarked%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActiveMark%3A%20on%20watermarking%20of%20visual%20foundation%20models%20via%20massive%0A%20%20activations&entry.906535625=Anna%20Chistyakova%20and%20Mikhail%20Pautov&entry.1292438233=%20%20Being%20trained%20on%20large%20and%20vast%20datasets%2C%20visual%20foundation%20models%20%28VFMs%29%20can%0Abe%20fine-tuned%20for%20diverse%20downstream%20tasks%2C%20achieving%20remarkable%20performance%0Aand%20efficiency%20in%20various%20computer%20vision%20applications.%20The%20high%20computation%0Acost%20of%20data%20collection%20and%20training%20motivates%20the%20owners%20of%20some%20VFMs%20to%0Adistribute%20them%20alongside%20the%20license%20to%20protect%20their%20intellectual%20property%0Arights.%20However%2C%20a%20dishonest%20user%20of%20the%20protected%20model%27s%20copy%20may%20illegally%0Aredistribute%20it%2C%20for%20example%2C%20to%20make%20a%20profit.%20As%20a%20consequence%2C%20the%0Adevelopment%20of%20reliable%20ownership%20verification%20tools%20is%20of%20great%20importance%0Atoday%2C%20since%20such%20methods%20can%20be%20used%20to%20differentiate%20between%20a%20redistributed%0Acopy%20of%20the%20protected%20model%20and%20an%20independent%20model.%20In%20this%20paper%2C%20we%20propose%0Aan%20approach%20to%20ownership%20verification%20of%20visual%20foundation%20models%20by%0Afine-tuning%20a%20small%20set%20of%20expressive%20layers%20of%20a%20VFM%20along%20with%20a%20small%0Aencoder-decoder%20network%20to%20embed%20digital%20watermarks%20into%20an%20internal%0Arepresentation%20of%20a%20hold-out%20set%20of%20input%20images.%20Importantly%2C%20the%20watermarks%0Aembedded%20remain%20detectable%20in%20the%20functional%20copies%20of%20the%20protected%20model%2C%0Aobtained%2C%20for%20example%2C%20by%20fine-tuning%20the%20VFM%20for%20a%20particular%20downstream%20task.%0ATheoretically%20and%20experimentally%2C%20we%20demonstrate%20that%20the%20proposed%20method%0Ayields%20a%20low%20probability%20of%20false%20detection%20of%20a%20non-watermarked%20model%20and%20a%0Alow%20probability%20of%20false%20misdetection%20of%20a%20watermarked%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04966v1&entry.124074799=Read"},
{"title": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data", "author": "Janos Perczel and Jin Chow and Dorottya Demszky", "abstract": "  The promise of generative AI to revolutionize education is constrained by the\npedagogical limits of large language models (LLMs). A major issue is the lack\nof access to high-quality training data that reflect the learning of actual\nstudents. Prompt engineering has emerged as a stopgap, but the ability of\nprompts to encode complex pedagogical strategies in rule-based natural language\nis inherently limited. To address this gap we introduce TeachLM - an LLM\noptimized for teaching through parameter-efficient fine-tuning of\nstate-of-the-art models. TeachLM is trained on a dataset comprised of 100,000\nhours of one-on-one, longitudinal student-tutor interactions maintained by\nPolygence, which underwent a rigorous anonymization process to protect privacy.\nWe use parameter-efficient fine-tuning to develop an authentic student model\nthat enables the generation of high-fidelity synthetic student-tutor dialogues.\nBuilding on this capability, we propose a novel multi-turn evaluation protocol\nthat leverages synthetic dialogue generation to provide fast, scalable, and\nreproducible assessments of the dialogical capabilities of LLMs. Our\nevaluations demonstrate that fine-tuning on authentic learning data\nsignificantly improves conversational and pedagogical performance - doubling\nstudent talk time, improving questioning style, increasing dialogue turns by\n50%, and greater personalization of instruction.\n", "link": "http://arxiv.org/abs/2510.05087v1", "date": "2025-10-06", "relevancy": 2.527, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5097}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5065}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeachLM%3A%20Post-Training%20LLMs%20for%20Education%20Using%20Authentic%20Learning%20Data&body=Title%3A%20TeachLM%3A%20Post-Training%20LLMs%20for%20Education%20Using%20Authentic%20Learning%20Data%0AAuthor%3A%20Janos%20Perczel%20and%20Jin%20Chow%20and%20Dorottya%20Demszky%0AAbstract%3A%20%20%20The%20promise%20of%20generative%20AI%20to%20revolutionize%20education%20is%20constrained%20by%20the%0Apedagogical%20limits%20of%20large%20language%20models%20%28LLMs%29.%20A%20major%20issue%20is%20the%20lack%0Aof%20access%20to%20high-quality%20training%20data%20that%20reflect%20the%20learning%20of%20actual%0Astudents.%20Prompt%20engineering%20has%20emerged%20as%20a%20stopgap%2C%20but%20the%20ability%20of%0Aprompts%20to%20encode%20complex%20pedagogical%20strategies%20in%20rule-based%20natural%20language%0Ais%20inherently%20limited.%20To%20address%20this%20gap%20we%20introduce%20TeachLM%20-%20an%20LLM%0Aoptimized%20for%20teaching%20through%20parameter-efficient%20fine-tuning%20of%0Astate-of-the-art%20models.%20TeachLM%20is%20trained%20on%20a%20dataset%20comprised%20of%20100%2C000%0Ahours%20of%20one-on-one%2C%20longitudinal%20student-tutor%20interactions%20maintained%20by%0APolygence%2C%20which%20underwent%20a%20rigorous%20anonymization%20process%20to%20protect%20privacy.%0AWe%20use%20parameter-efficient%20fine-tuning%20to%20develop%20an%20authentic%20student%20model%0Athat%20enables%20the%20generation%20of%20high-fidelity%20synthetic%20student-tutor%20dialogues.%0ABuilding%20on%20this%20capability%2C%20we%20propose%20a%20novel%20multi-turn%20evaluation%20protocol%0Athat%20leverages%20synthetic%20dialogue%20generation%20to%20provide%20fast%2C%20scalable%2C%20and%0Areproducible%20assessments%20of%20the%20dialogical%20capabilities%20of%20LLMs.%20Our%0Aevaluations%20demonstrate%20that%20fine-tuning%20on%20authentic%20learning%20data%0Asignificantly%20improves%20conversational%20and%20pedagogical%20performance%20-%20doubling%0Astudent%20talk%20time%2C%20improving%20questioning%20style%2C%20increasing%20dialogue%20turns%20by%0A50%25%2C%20and%20greater%20personalization%20of%20instruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeachLM%253A%2520Post-Training%2520LLMs%2520for%2520Education%2520Using%2520Authentic%2520Learning%2520Data%26entry.906535625%3DJanos%2520Perczel%2520and%2520Jin%2520Chow%2520and%2520Dorottya%2520Demszky%26entry.1292438233%3D%2520%2520The%2520promise%2520of%2520generative%2520AI%2520to%2520revolutionize%2520education%2520is%2520constrained%2520by%2520the%250Apedagogical%2520limits%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520A%2520major%2520issue%2520is%2520the%2520lack%250Aof%2520access%2520to%2520high-quality%2520training%2520data%2520that%2520reflect%2520the%2520learning%2520of%2520actual%250Astudents.%2520Prompt%2520engineering%2520has%2520emerged%2520as%2520a%2520stopgap%252C%2520but%2520the%2520ability%2520of%250Aprompts%2520to%2520encode%2520complex%2520pedagogical%2520strategies%2520in%2520rule-based%2520natural%2520language%250Ais%2520inherently%2520limited.%2520To%2520address%2520this%2520gap%2520we%2520introduce%2520TeachLM%2520-%2520an%2520LLM%250Aoptimized%2520for%2520teaching%2520through%2520parameter-efficient%2520fine-tuning%2520of%250Astate-of-the-art%2520models.%2520TeachLM%2520is%2520trained%2520on%2520a%2520dataset%2520comprised%2520of%2520100%252C000%250Ahours%2520of%2520one-on-one%252C%2520longitudinal%2520student-tutor%2520interactions%2520maintained%2520by%250APolygence%252C%2520which%2520underwent%2520a%2520rigorous%2520anonymization%2520process%2520to%2520protect%2520privacy.%250AWe%2520use%2520parameter-efficient%2520fine-tuning%2520to%2520develop%2520an%2520authentic%2520student%2520model%250Athat%2520enables%2520the%2520generation%2520of%2520high-fidelity%2520synthetic%2520student-tutor%2520dialogues.%250ABuilding%2520on%2520this%2520capability%252C%2520we%2520propose%2520a%2520novel%2520multi-turn%2520evaluation%2520protocol%250Athat%2520leverages%2520synthetic%2520dialogue%2520generation%2520to%2520provide%2520fast%252C%2520scalable%252C%2520and%250Areproducible%2520assessments%2520of%2520the%2520dialogical%2520capabilities%2520of%2520LLMs.%2520Our%250Aevaluations%2520demonstrate%2520that%2520fine-tuning%2520on%2520authentic%2520learning%2520data%250Asignificantly%2520improves%2520conversational%2520and%2520pedagogical%2520performance%2520-%2520doubling%250Astudent%2520talk%2520time%252C%2520improving%2520questioning%2520style%252C%2520increasing%2520dialogue%2520turns%2520by%250A50%2525%252C%2520and%2520greater%2520personalization%2520of%2520instruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeachLM%3A%20Post-Training%20LLMs%20for%20Education%20Using%20Authentic%20Learning%20Data&entry.906535625=Janos%20Perczel%20and%20Jin%20Chow%20and%20Dorottya%20Demszky&entry.1292438233=%20%20The%20promise%20of%20generative%20AI%20to%20revolutionize%20education%20is%20constrained%20by%20the%0Apedagogical%20limits%20of%20large%20language%20models%20%28LLMs%29.%20A%20major%20issue%20is%20the%20lack%0Aof%20access%20to%20high-quality%20training%20data%20that%20reflect%20the%20learning%20of%20actual%0Astudents.%20Prompt%20engineering%20has%20emerged%20as%20a%20stopgap%2C%20but%20the%20ability%20of%0Aprompts%20to%20encode%20complex%20pedagogical%20strategies%20in%20rule-based%20natural%20language%0Ais%20inherently%20limited.%20To%20address%20this%20gap%20we%20introduce%20TeachLM%20-%20an%20LLM%0Aoptimized%20for%20teaching%20through%20parameter-efficient%20fine-tuning%20of%0Astate-of-the-art%20models.%20TeachLM%20is%20trained%20on%20a%20dataset%20comprised%20of%20100%2C000%0Ahours%20of%20one-on-one%2C%20longitudinal%20student-tutor%20interactions%20maintained%20by%0APolygence%2C%20which%20underwent%20a%20rigorous%20anonymization%20process%20to%20protect%20privacy.%0AWe%20use%20parameter-efficient%20fine-tuning%20to%20develop%20an%20authentic%20student%20model%0Athat%20enables%20the%20generation%20of%20high-fidelity%20synthetic%20student-tutor%20dialogues.%0ABuilding%20on%20this%20capability%2C%20we%20propose%20a%20novel%20multi-turn%20evaluation%20protocol%0Athat%20leverages%20synthetic%20dialogue%20generation%20to%20provide%20fast%2C%20scalable%2C%20and%0Areproducible%20assessments%20of%20the%20dialogical%20capabilities%20of%20LLMs.%20Our%0Aevaluations%20demonstrate%20that%20fine-tuning%20on%20authentic%20learning%20data%0Asignificantly%20improves%20conversational%20and%20pedagogical%20performance%20-%20doubling%0Astudent%20talk%20time%2C%20improving%20questioning%20style%2C%20increasing%20dialogue%20turns%20by%0A50%25%2C%20and%20greater%20personalization%20of%20instruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05087v1&entry.124074799=Read"},
{"title": "Slm-mux: Orchestrating small language models for reasoning", "author": "Chenyu Wang and Zishen Wan and Hao Kang and Emma Chen and Zhiqiang Xie and Tushar Krishna and Vijay Janapa Reddi and Yilun Du", "abstract": "  With the rapid development of language models, the number of small language\nmodels (SLMs) has grown significantly. Although they do not achieve\nstate-of-the-art accuracy, they are more efficient and often excel at specific\ntasks. This raises a natural question: can multiple SLMs be orchestrated into a\nsystem where each contributes effectively, achieving higher accuracy than any\nindividual model? Existing orchestration methods have primarily targeted\nfrontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To\naddress this gap, we propose a three-stage approach for orchestrating SLMs.\nFirst, we introduce SLM-MUX, a multi-model architecture that effectively\ncoordinates multiple SLMs. Building on this, we develop two optimization\nstrategies: (i) a model selection search that identifies the most complementary\nSLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our\napproach delivers strong results: Compared to existing orchestration methods,\nour approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%\non GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and\nGSM8K, and matches its performance on MATH. We further provide theoretical\nanalyses to substantiate the advantages of our method. In summary, we\ndemonstrate that SLMs can be effectively orchestrated into more accurate and\nefficient systems through the proposed approach.\n", "link": "http://arxiv.org/abs/2510.05077v1", "date": "2025-10-06", "relevancy": 2.4981, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5036}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Slm-mux%3A%20Orchestrating%20small%20language%20models%20for%20reasoning&body=Title%3A%20Slm-mux%3A%20Orchestrating%20small%20language%20models%20for%20reasoning%0AAuthor%3A%20Chenyu%20Wang%20and%20Zishen%20Wan%20and%20Hao%20Kang%20and%20Emma%20Chen%20and%20Zhiqiang%20Xie%20and%20Tushar%20Krishna%20and%20Vijay%20Janapa%20Reddi%20and%20Yilun%20Du%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20language%20models%2C%20the%20number%20of%20small%20language%0Amodels%20%28SLMs%29%20has%20grown%20significantly.%20Although%20they%20do%20not%20achieve%0Astate-of-the-art%20accuracy%2C%20they%20are%20more%20efficient%20and%20often%20excel%20at%20specific%0Atasks.%20This%20raises%20a%20natural%20question%3A%20can%20multiple%20SLMs%20be%20orchestrated%20into%20a%0Asystem%20where%20each%20contributes%20effectively%2C%20achieving%20higher%20accuracy%20than%20any%0Aindividual%20model%3F%20Existing%20orchestration%20methods%20have%20primarily%20targeted%0Afrontier%20models%20%28e.g.%2C%20GPT-4%29%20and%20perform%20suboptimally%20when%20applied%20to%20SLMs.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20a%20three-stage%20approach%20for%20orchestrating%20SLMs.%0AFirst%2C%20we%20introduce%20SLM-MUX%2C%20a%20multi-model%20architecture%20that%20effectively%0Acoordinates%20multiple%20SLMs.%20Building%20on%20this%2C%20we%20develop%20two%20optimization%0Astrategies%3A%20%28i%29%20a%20model%20selection%20search%20that%20identifies%20the%20most%20complementary%0ASLMs%20from%20a%20given%20pool%2C%20and%20%28ii%29%20test-time%20scaling%20tailored%20to%20SLM-MUX.%20Our%0Aapproach%20delivers%20strong%20results%3A%20Compared%20to%20existing%20orchestration%20methods%2C%0Aour%20approach%20achieves%20up%20to%2013.4%25%20improvement%20on%20MATH%2C%208.8%25%20on%20GPQA%2C%20and%207.0%25%0Aon%20GSM8K.%20With%20just%20two%20SLMS%2C%20SLM-MUX%20outperforms%20Qwen%202.5%2072B%20on%20GPQA%20and%0AGSM8K%2C%20and%20matches%20its%20performance%20on%20MATH.%20We%20further%20provide%20theoretical%0Aanalyses%20to%20substantiate%20the%20advantages%20of%20our%20method.%20In%20summary%2C%20we%0Ademonstrate%20that%20SLMs%20can%20be%20effectively%20orchestrated%20into%20more%20accurate%20and%0Aefficient%20systems%20through%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlm-mux%253A%2520Orchestrating%2520small%2520language%2520models%2520for%2520reasoning%26entry.906535625%3DChenyu%2520Wang%2520and%2520Zishen%2520Wan%2520and%2520Hao%2520Kang%2520and%2520Emma%2520Chen%2520and%2520Zhiqiang%2520Xie%2520and%2520Tushar%2520Krishna%2520and%2520Vijay%2520Janapa%2520Reddi%2520and%2520Yilun%2520Du%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520language%2520models%252C%2520the%2520number%2520of%2520small%2520language%250Amodels%2520%2528SLMs%2529%2520has%2520grown%2520significantly.%2520Although%2520they%2520do%2520not%2520achieve%250Astate-of-the-art%2520accuracy%252C%2520they%2520are%2520more%2520efficient%2520and%2520often%2520excel%2520at%2520specific%250Atasks.%2520This%2520raises%2520a%2520natural%2520question%253A%2520can%2520multiple%2520SLMs%2520be%2520orchestrated%2520into%2520a%250Asystem%2520where%2520each%2520contributes%2520effectively%252C%2520achieving%2520higher%2520accuracy%2520than%2520any%250Aindividual%2520model%253F%2520Existing%2520orchestration%2520methods%2520have%2520primarily%2520targeted%250Afrontier%2520models%2520%2528e.g.%252C%2520GPT-4%2529%2520and%2520perform%2520suboptimally%2520when%2520applied%2520to%2520SLMs.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520propose%2520a%2520three-stage%2520approach%2520for%2520orchestrating%2520SLMs.%250AFirst%252C%2520we%2520introduce%2520SLM-MUX%252C%2520a%2520multi-model%2520architecture%2520that%2520effectively%250Acoordinates%2520multiple%2520SLMs.%2520Building%2520on%2520this%252C%2520we%2520develop%2520two%2520optimization%250Astrategies%253A%2520%2528i%2529%2520a%2520model%2520selection%2520search%2520that%2520identifies%2520the%2520most%2520complementary%250ASLMs%2520from%2520a%2520given%2520pool%252C%2520and%2520%2528ii%2529%2520test-time%2520scaling%2520tailored%2520to%2520SLM-MUX.%2520Our%250Aapproach%2520delivers%2520strong%2520results%253A%2520Compared%2520to%2520existing%2520orchestration%2520methods%252C%250Aour%2520approach%2520achieves%2520up%2520to%252013.4%2525%2520improvement%2520on%2520MATH%252C%25208.8%2525%2520on%2520GPQA%252C%2520and%25207.0%2525%250Aon%2520GSM8K.%2520With%2520just%2520two%2520SLMS%252C%2520SLM-MUX%2520outperforms%2520Qwen%25202.5%252072B%2520on%2520GPQA%2520and%250AGSM8K%252C%2520and%2520matches%2520its%2520performance%2520on%2520MATH.%2520We%2520further%2520provide%2520theoretical%250Aanalyses%2520to%2520substantiate%2520the%2520advantages%2520of%2520our%2520method.%2520In%2520summary%252C%2520we%250Ademonstrate%2520that%2520SLMs%2520can%2520be%2520effectively%2520orchestrated%2520into%2520more%2520accurate%2520and%250Aefficient%2520systems%2520through%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Slm-mux%3A%20Orchestrating%20small%20language%20models%20for%20reasoning&entry.906535625=Chenyu%20Wang%20and%20Zishen%20Wan%20and%20Hao%20Kang%20and%20Emma%20Chen%20and%20Zhiqiang%20Xie%20and%20Tushar%20Krishna%20and%20Vijay%20Janapa%20Reddi%20and%20Yilun%20Du&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20language%20models%2C%20the%20number%20of%20small%20language%0Amodels%20%28SLMs%29%20has%20grown%20significantly.%20Although%20they%20do%20not%20achieve%0Astate-of-the-art%20accuracy%2C%20they%20are%20more%20efficient%20and%20often%20excel%20at%20specific%0Atasks.%20This%20raises%20a%20natural%20question%3A%20can%20multiple%20SLMs%20be%20orchestrated%20into%20a%0Asystem%20where%20each%20contributes%20effectively%2C%20achieving%20higher%20accuracy%20than%20any%0Aindividual%20model%3F%20Existing%20orchestration%20methods%20have%20primarily%20targeted%0Afrontier%20models%20%28e.g.%2C%20GPT-4%29%20and%20perform%20suboptimally%20when%20applied%20to%20SLMs.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20a%20three-stage%20approach%20for%20orchestrating%20SLMs.%0AFirst%2C%20we%20introduce%20SLM-MUX%2C%20a%20multi-model%20architecture%20that%20effectively%0Acoordinates%20multiple%20SLMs.%20Building%20on%20this%2C%20we%20develop%20two%20optimization%0Astrategies%3A%20%28i%29%20a%20model%20selection%20search%20that%20identifies%20the%20most%20complementary%0ASLMs%20from%20a%20given%20pool%2C%20and%20%28ii%29%20test-time%20scaling%20tailored%20to%20SLM-MUX.%20Our%0Aapproach%20delivers%20strong%20results%3A%20Compared%20to%20existing%20orchestration%20methods%2C%0Aour%20approach%20achieves%20up%20to%2013.4%25%20improvement%20on%20MATH%2C%208.8%25%20on%20GPQA%2C%20and%207.0%25%0Aon%20GSM8K.%20With%20just%20two%20SLMS%2C%20SLM-MUX%20outperforms%20Qwen%202.5%2072B%20on%20GPQA%20and%0AGSM8K%2C%20and%20matches%20its%20performance%20on%20MATH.%20We%20further%20provide%20theoretical%0Aanalyses%20to%20substantiate%20the%20advantages%20of%20our%20method.%20In%20summary%2C%20we%0Ademonstrate%20that%20SLMs%20can%20be%20effectively%20orchestrated%20into%20more%20accurate%20and%0Aefficient%20systems%20through%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05077v1&entry.124074799=Read"},
{"title": "Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking", "author": "Ali Saheb Pasand and Elvis Dohmatob", "abstract": "  Grokking is the phenomenon whereby, unlike the training performance, which\npeaks early in the training process, the test/generalization performance of a\nmodel stagnates over arbitrarily many epochs and then suddenly jumps to usually\nclose to perfect levels. In practice, it is desirable to reduce the length of\nsuch plateaus, that is to make the learning process \"grok\" faster. In this\nwork, we provide new insights into grokking. First, we show both empirically\nand theoretically that grokking can be induced by asymmetric speeds of\n(stochastic) gradient descent, along different principal (i.e singular\ndirections) of the gradients. We then propose a simple modification that\nnormalizes the gradients so that dynamics along all the principal directions\nevolves at exactly the same speed. Then, we establish that this modified\nmethod, which we call egalitarian gradient descent (EGD) and can be seen as a\ncarefully modified form of natural gradient descent, groks much faster. In\nfact, in some cases the stagnation is completely removed. Finally, we\nempirically show that on classical arithmetic problems such as modular addition\nand sparse parity problem which this stagnation has been widely observed and\nintensively studied, that our proposed method eliminates the plateaus.\n", "link": "http://arxiv.org/abs/2510.04930v1", "date": "2025-10-06", "relevancy": 2.4936, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5311}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4852}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Egalitarian%20Gradient%20Descent%3A%20A%20Simple%20Approach%20to%20Accelerated%20Grokking&body=Title%3A%20Egalitarian%20Gradient%20Descent%3A%20A%20Simple%20Approach%20to%20Accelerated%20Grokking%0AAuthor%3A%20Ali%20Saheb%20Pasand%20and%20Elvis%20Dohmatob%0AAbstract%3A%20%20%20Grokking%20is%20the%20phenomenon%20whereby%2C%20unlike%20the%20training%20performance%2C%20which%0Apeaks%20early%20in%20the%20training%20process%2C%20the%20test/generalization%20performance%20of%20a%0Amodel%20stagnates%20over%20arbitrarily%20many%20epochs%20and%20then%20suddenly%20jumps%20to%20usually%0Aclose%20to%20perfect%20levels.%20In%20practice%2C%20it%20is%20desirable%20to%20reduce%20the%20length%20of%0Asuch%20plateaus%2C%20that%20is%20to%20make%20the%20learning%20process%20%22grok%22%20faster.%20In%20this%0Awork%2C%20we%20provide%20new%20insights%20into%20grokking.%20First%2C%20we%20show%20both%20empirically%0Aand%20theoretically%20that%20grokking%20can%20be%20induced%20by%20asymmetric%20speeds%20of%0A%28stochastic%29%20gradient%20descent%2C%20along%20different%20principal%20%28i.e%20singular%0Adirections%29%20of%20the%20gradients.%20We%20then%20propose%20a%20simple%20modification%20that%0Anormalizes%20the%20gradients%20so%20that%20dynamics%20along%20all%20the%20principal%20directions%0Aevolves%20at%20exactly%20the%20same%20speed.%20Then%2C%20we%20establish%20that%20this%20modified%0Amethod%2C%20which%20we%20call%20egalitarian%20gradient%20descent%20%28EGD%29%20and%20can%20be%20seen%20as%20a%0Acarefully%20modified%20form%20of%20natural%20gradient%20descent%2C%20groks%20much%20faster.%20In%0Afact%2C%20in%20some%20cases%20the%20stagnation%20is%20completely%20removed.%20Finally%2C%20we%0Aempirically%20show%20that%20on%20classical%20arithmetic%20problems%20such%20as%20modular%20addition%0Aand%20sparse%20parity%20problem%20which%20this%20stagnation%20has%20been%20widely%20observed%20and%0Aintensively%20studied%2C%20that%20our%20proposed%20method%20eliminates%20the%20plateaus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgalitarian%2520Gradient%2520Descent%253A%2520A%2520Simple%2520Approach%2520to%2520Accelerated%2520Grokking%26entry.906535625%3DAli%2520Saheb%2520Pasand%2520and%2520Elvis%2520Dohmatob%26entry.1292438233%3D%2520%2520Grokking%2520is%2520the%2520phenomenon%2520whereby%252C%2520unlike%2520the%2520training%2520performance%252C%2520which%250Apeaks%2520early%2520in%2520the%2520training%2520process%252C%2520the%2520test/generalization%2520performance%2520of%2520a%250Amodel%2520stagnates%2520over%2520arbitrarily%2520many%2520epochs%2520and%2520then%2520suddenly%2520jumps%2520to%2520usually%250Aclose%2520to%2520perfect%2520levels.%2520In%2520practice%252C%2520it%2520is%2520desirable%2520to%2520reduce%2520the%2520length%2520of%250Asuch%2520plateaus%252C%2520that%2520is%2520to%2520make%2520the%2520learning%2520process%2520%2522grok%2522%2520faster.%2520In%2520this%250Awork%252C%2520we%2520provide%2520new%2520insights%2520into%2520grokking.%2520First%252C%2520we%2520show%2520both%2520empirically%250Aand%2520theoretically%2520that%2520grokking%2520can%2520be%2520induced%2520by%2520asymmetric%2520speeds%2520of%250A%2528stochastic%2529%2520gradient%2520descent%252C%2520along%2520different%2520principal%2520%2528i.e%2520singular%250Adirections%2529%2520of%2520the%2520gradients.%2520We%2520then%2520propose%2520a%2520simple%2520modification%2520that%250Anormalizes%2520the%2520gradients%2520so%2520that%2520dynamics%2520along%2520all%2520the%2520principal%2520directions%250Aevolves%2520at%2520exactly%2520the%2520same%2520speed.%2520Then%252C%2520we%2520establish%2520that%2520this%2520modified%250Amethod%252C%2520which%2520we%2520call%2520egalitarian%2520gradient%2520descent%2520%2528EGD%2529%2520and%2520can%2520be%2520seen%2520as%2520a%250Acarefully%2520modified%2520form%2520of%2520natural%2520gradient%2520descent%252C%2520groks%2520much%2520faster.%2520In%250Afact%252C%2520in%2520some%2520cases%2520the%2520stagnation%2520is%2520completely%2520removed.%2520Finally%252C%2520we%250Aempirically%2520show%2520that%2520on%2520classical%2520arithmetic%2520problems%2520such%2520as%2520modular%2520addition%250Aand%2520sparse%2520parity%2520problem%2520which%2520this%2520stagnation%2520has%2520been%2520widely%2520observed%2520and%250Aintensively%2520studied%252C%2520that%2520our%2520proposed%2520method%2520eliminates%2520the%2520plateaus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Egalitarian%20Gradient%20Descent%3A%20A%20Simple%20Approach%20to%20Accelerated%20Grokking&entry.906535625=Ali%20Saheb%20Pasand%20and%20Elvis%20Dohmatob&entry.1292438233=%20%20Grokking%20is%20the%20phenomenon%20whereby%2C%20unlike%20the%20training%20performance%2C%20which%0Apeaks%20early%20in%20the%20training%20process%2C%20the%20test/generalization%20performance%20of%20a%0Amodel%20stagnates%20over%20arbitrarily%20many%20epochs%20and%20then%20suddenly%20jumps%20to%20usually%0Aclose%20to%20perfect%20levels.%20In%20practice%2C%20it%20is%20desirable%20to%20reduce%20the%20length%20of%0Asuch%20plateaus%2C%20that%20is%20to%20make%20the%20learning%20process%20%22grok%22%20faster.%20In%20this%0Awork%2C%20we%20provide%20new%20insights%20into%20grokking.%20First%2C%20we%20show%20both%20empirically%0Aand%20theoretically%20that%20grokking%20can%20be%20induced%20by%20asymmetric%20speeds%20of%0A%28stochastic%29%20gradient%20descent%2C%20along%20different%20principal%20%28i.e%20singular%0Adirections%29%20of%20the%20gradients.%20We%20then%20propose%20a%20simple%20modification%20that%0Anormalizes%20the%20gradients%20so%20that%20dynamics%20along%20all%20the%20principal%20directions%0Aevolves%20at%20exactly%20the%20same%20speed.%20Then%2C%20we%20establish%20that%20this%20modified%0Amethod%2C%20which%20we%20call%20egalitarian%20gradient%20descent%20%28EGD%29%20and%20can%20be%20seen%20as%20a%0Acarefully%20modified%20form%20of%20natural%20gradient%20descent%2C%20groks%20much%20faster.%20In%0Afact%2C%20in%20some%20cases%20the%20stagnation%20is%20completely%20removed.%20Finally%2C%20we%0Aempirically%20show%20that%20on%20classical%20arithmetic%20problems%20such%20as%20modular%20addition%0Aand%20sparse%20parity%20problem%20which%20this%20stagnation%20has%20been%20widely%20observed%20and%0Aintensively%20studied%2C%20that%20our%20proposed%20method%20eliminates%20the%20plateaus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04930v1&entry.124074799=Read"},
{"title": "Latent Uncertainty Representations for Video-based Driver Action and\n  Intention Recognition", "author": "Koen Vellenga and H. Joe Steinhauer and Jonas Andersson and Anders Sj\u00f6gren", "abstract": "  Deep neural networks (DNNs) are increasingly applied to safety-critical tasks\nin resource-constrained environments, such as video-based driver action and\nintention recognition. While last layer probabilistic deep learning (LL-PDL)\nmethods can detect out-of-distribution (OOD) instances, their performance\nvaries. As an alternative to last layer approaches, we propose extending\npre-trained DNNs with transformation layers to produce multiple latent\nrepresentations to estimate the uncertainty. We evaluate our latent uncertainty\nrepresentation (LUR) and repulsively trained LUR (RLUR) approaches against\neight PDL methods across four video-based driver action and intention\nrecognition datasets, comparing classification performance, calibration, and\nuncertainty-based OOD detection. We also contribute 28,000 frame-level action\nlabels and 1,194 video-level intention labels for the NuScenes dataset. Our\nresults show that LUR and RLUR achieve comparable in-distribution\nclassification performance to other LL-PDL approaches. For uncertainty-based\nOOD detection, LUR matches top-performing PDL methods while being more\nefficient to train and easier to tune than approaches that require Markov-Chain\nMonte Carlo sampling or repulsive training procedures.\n", "link": "http://arxiv.org/abs/2510.05006v1", "date": "2025-10-06", "relevancy": 2.4534, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6471}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Uncertainty%20Representations%20for%20Video-based%20Driver%20Action%20and%0A%20%20Intention%20Recognition&body=Title%3A%20Latent%20Uncertainty%20Representations%20for%20Video-based%20Driver%20Action%20and%0A%20%20Intention%20Recognition%0AAuthor%3A%20Koen%20Vellenga%20and%20H.%20Joe%20Steinhauer%20and%20Jonas%20Andersson%20and%20Anders%20Sj%C3%B6gren%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20increasingly%20applied%20to%20safety-critical%20tasks%0Ain%20resource-constrained%20environments%2C%20such%20as%20video-based%20driver%20action%20and%0Aintention%20recognition.%20While%20last%20layer%20probabilistic%20deep%20learning%20%28LL-PDL%29%0Amethods%20can%20detect%20out-of-distribution%20%28OOD%29%20instances%2C%20their%20performance%0Avaries.%20As%20an%20alternative%20to%20last%20layer%20approaches%2C%20we%20propose%20extending%0Apre-trained%20DNNs%20with%20transformation%20layers%20to%20produce%20multiple%20latent%0Arepresentations%20to%20estimate%20the%20uncertainty.%20We%20evaluate%20our%20latent%20uncertainty%0Arepresentation%20%28LUR%29%20and%20repulsively%20trained%20LUR%20%28RLUR%29%20approaches%20against%0Aeight%20PDL%20methods%20across%20four%20video-based%20driver%20action%20and%20intention%0Arecognition%20datasets%2C%20comparing%20classification%20performance%2C%20calibration%2C%20and%0Auncertainty-based%20OOD%20detection.%20We%20also%20contribute%2028%2C000%20frame-level%20action%0Alabels%20and%201%2C194%20video-level%20intention%20labels%20for%20the%20NuScenes%20dataset.%20Our%0Aresults%20show%20that%20LUR%20and%20RLUR%20achieve%20comparable%20in-distribution%0Aclassification%20performance%20to%20other%20LL-PDL%20approaches.%20For%20uncertainty-based%0AOOD%20detection%2C%20LUR%20matches%20top-performing%20PDL%20methods%20while%20being%20more%0Aefficient%20to%20train%20and%20easier%20to%20tune%20than%20approaches%20that%20require%20Markov-Chain%0AMonte%20Carlo%20sampling%20or%20repulsive%20training%20procedures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Uncertainty%2520Representations%2520for%2520Video-based%2520Driver%2520Action%2520and%250A%2520%2520Intention%2520Recognition%26entry.906535625%3DKoen%2520Vellenga%2520and%2520H.%2520Joe%2520Steinhauer%2520and%2520Jonas%2520Andersson%2520and%2520Anders%2520Sj%25C3%25B6gren%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520increasingly%2520applied%2520to%2520safety-critical%2520tasks%250Ain%2520resource-constrained%2520environments%252C%2520such%2520as%2520video-based%2520driver%2520action%2520and%250Aintention%2520recognition.%2520While%2520last%2520layer%2520probabilistic%2520deep%2520learning%2520%2528LL-PDL%2529%250Amethods%2520can%2520detect%2520out-of-distribution%2520%2528OOD%2529%2520instances%252C%2520their%2520performance%250Avaries.%2520As%2520an%2520alternative%2520to%2520last%2520layer%2520approaches%252C%2520we%2520propose%2520extending%250Apre-trained%2520DNNs%2520with%2520transformation%2520layers%2520to%2520produce%2520multiple%2520latent%250Arepresentations%2520to%2520estimate%2520the%2520uncertainty.%2520We%2520evaluate%2520our%2520latent%2520uncertainty%250Arepresentation%2520%2528LUR%2529%2520and%2520repulsively%2520trained%2520LUR%2520%2528RLUR%2529%2520approaches%2520against%250Aeight%2520PDL%2520methods%2520across%2520four%2520video-based%2520driver%2520action%2520and%2520intention%250Arecognition%2520datasets%252C%2520comparing%2520classification%2520performance%252C%2520calibration%252C%2520and%250Auncertainty-based%2520OOD%2520detection.%2520We%2520also%2520contribute%252028%252C000%2520frame-level%2520action%250Alabels%2520and%25201%252C194%2520video-level%2520intention%2520labels%2520for%2520the%2520NuScenes%2520dataset.%2520Our%250Aresults%2520show%2520that%2520LUR%2520and%2520RLUR%2520achieve%2520comparable%2520in-distribution%250Aclassification%2520performance%2520to%2520other%2520LL-PDL%2520approaches.%2520For%2520uncertainty-based%250AOOD%2520detection%252C%2520LUR%2520matches%2520top-performing%2520PDL%2520methods%2520while%2520being%2520more%250Aefficient%2520to%2520train%2520and%2520easier%2520to%2520tune%2520than%2520approaches%2520that%2520require%2520Markov-Chain%250AMonte%2520Carlo%2520sampling%2520or%2520repulsive%2520training%2520procedures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Uncertainty%20Representations%20for%20Video-based%20Driver%20Action%20and%0A%20%20Intention%20Recognition&entry.906535625=Koen%20Vellenga%20and%20H.%20Joe%20Steinhauer%20and%20Jonas%20Andersson%20and%20Anders%20Sj%C3%B6gren&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20increasingly%20applied%20to%20safety-critical%20tasks%0Ain%20resource-constrained%20environments%2C%20such%20as%20video-based%20driver%20action%20and%0Aintention%20recognition.%20While%20last%20layer%20probabilistic%20deep%20learning%20%28LL-PDL%29%0Amethods%20can%20detect%20out-of-distribution%20%28OOD%29%20instances%2C%20their%20performance%0Avaries.%20As%20an%20alternative%20to%20last%20layer%20approaches%2C%20we%20propose%20extending%0Apre-trained%20DNNs%20with%20transformation%20layers%20to%20produce%20multiple%20latent%0Arepresentations%20to%20estimate%20the%20uncertainty.%20We%20evaluate%20our%20latent%20uncertainty%0Arepresentation%20%28LUR%29%20and%20repulsively%20trained%20LUR%20%28RLUR%29%20approaches%20against%0Aeight%20PDL%20methods%20across%20four%20video-based%20driver%20action%20and%20intention%0Arecognition%20datasets%2C%20comparing%20classification%20performance%2C%20calibration%2C%20and%0Auncertainty-based%20OOD%20detection.%20We%20also%20contribute%2028%2C000%20frame-level%20action%0Alabels%20and%201%2C194%20video-level%20intention%20labels%20for%20the%20NuScenes%20dataset.%20Our%0Aresults%20show%20that%20LUR%20and%20RLUR%20achieve%20comparable%20in-distribution%0Aclassification%20performance%20to%20other%20LL-PDL%20approaches.%20For%20uncertainty-based%0AOOD%20detection%2C%20LUR%20matches%20top-performing%20PDL%20methods%20while%20being%20more%0Aefficient%20to%20train%20and%20easier%20to%20tune%20than%20approaches%20that%20require%20Markov-Chain%0AMonte%20Carlo%20sampling%20or%20repulsive%20training%20procedures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05006v1&entry.124074799=Read"},
{"title": "Autonomous Imagination: Closed-Loop Decomposition of Visual-to-Textual\n  Conversion in Visual Reasoning for Multimodal Large Language Models", "author": "Jingming Liu and Yumeng Li and Boyuan Xiao and Yichang Jian and Ziang Qin and Tianjia Shao and Yao-Xiang Ding and Kun Zhou", "abstract": "  Under pure textual modality, Large Language Models (LLMs) have demonstrated\nremarkable success in complex reasoning tasks by decomposing them into simpler\nsub-problems. However, Multimodal Large Language Models (MLLMs) still struggle\nwith some seemingly straightforward visual tasks, such as counting and solving\njigsaw puzzles. We argue that these tasks challenge the ability of\nvisual-to-textual conversion, where MLLMs convert visual information perceived\nfrom the input scene, to textual information for further reasoning and\ngenerating the answer. If the complexity of the visual input is beyond the\nperceptual capability of the MLLMs, without decomposing this conversion\nprocess, simply scaling inference-time reasoning cannot solve the task because\nit repeatedly encounters the same perceptual bottleneck. We propose an\napproach, autonomous imagination, to enable MLLMs to iteratively modify visual\ninputs (e.g. isolating objects, rearranging puzzle pieces) into intermediate\nvisual states, decomposing visual-to-textual conversion into closed-loop visual\nmodification steps. We show that, without any retraining, MLLMs can now solve\ntasks initially beyond their perceptual capability, highlighting that\nclosed-loop visual modification can be an effective way of decomposing the\nvisual reasoning task into solvable substeps. Our code and data are released at\nhttps://future-item.github.io/autoimagine-site/.\n", "link": "http://arxiv.org/abs/2411.18142v4", "date": "2025-10-06", "relevancy": 2.4501, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6173}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6173}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Imagination%3A%20Closed-Loop%20Decomposition%20of%20Visual-to-Textual%0A%20%20Conversion%20in%20Visual%20Reasoning%20for%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Autonomous%20Imagination%3A%20Closed-Loop%20Decomposition%20of%20Visual-to-Textual%0A%20%20Conversion%20in%20Visual%20Reasoning%20for%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Jingming%20Liu%20and%20Yumeng%20Li%20and%20Boyuan%20Xiao%20and%20Yichang%20Jian%20and%20Ziang%20Qin%20and%20Tianjia%20Shao%20and%20Yao-Xiang%20Ding%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20Under%20pure%20textual%20modality%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%0Aremarkable%20success%20in%20complex%20reasoning%20tasks%20by%20decomposing%20them%20into%20simpler%0Asub-problems.%20However%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20still%20struggle%0Awith%20some%20seemingly%20straightforward%20visual%20tasks%2C%20such%20as%20counting%20and%20solving%0Ajigsaw%20puzzles.%20We%20argue%20that%20these%20tasks%20challenge%20the%20ability%20of%0Avisual-to-textual%20conversion%2C%20where%20MLLMs%20convert%20visual%20information%20perceived%0Afrom%20the%20input%20scene%2C%20to%20textual%20information%20for%20further%20reasoning%20and%0Agenerating%20the%20answer.%20If%20the%20complexity%20of%20the%20visual%20input%20is%20beyond%20the%0Aperceptual%20capability%20of%20the%20MLLMs%2C%20without%20decomposing%20this%20conversion%0Aprocess%2C%20simply%20scaling%20inference-time%20reasoning%20cannot%20solve%20the%20task%20because%0Ait%20repeatedly%20encounters%20the%20same%20perceptual%20bottleneck.%20We%20propose%20an%0Aapproach%2C%20autonomous%20imagination%2C%20to%20enable%20MLLMs%20to%20iteratively%20modify%20visual%0Ainputs%20%28e.g.%20isolating%20objects%2C%20rearranging%20puzzle%20pieces%29%20into%20intermediate%0Avisual%20states%2C%20decomposing%20visual-to-textual%20conversion%20into%20closed-loop%20visual%0Amodification%20steps.%20We%20show%20that%2C%20without%20any%20retraining%2C%20MLLMs%20can%20now%20solve%0Atasks%20initially%20beyond%20their%20perceptual%20capability%2C%20highlighting%20that%0Aclosed-loop%20visual%20modification%20can%20be%20an%20effective%20way%20of%20decomposing%20the%0Avisual%20reasoning%20task%20into%20solvable%20substeps.%20Our%20code%20and%20data%20are%20released%20at%0Ahttps%3A//future-item.github.io/autoimagine-site/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18142v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Imagination%253A%2520Closed-Loop%2520Decomposition%2520of%2520Visual-to-Textual%250A%2520%2520Conversion%2520in%2520Visual%2520Reasoning%2520for%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DJingming%2520Liu%2520and%2520Yumeng%2520Li%2520and%2520Boyuan%2520Xiao%2520and%2520Yichang%2520Jian%2520and%2520Ziang%2520Qin%2520and%2520Tianjia%2520Shao%2520and%2520Yao-Xiang%2520Ding%2520and%2520Kun%2520Zhou%26entry.1292438233%3D%2520%2520Under%2520pure%2520textual%2520modality%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%250Aremarkable%2520success%2520in%2520complex%2520reasoning%2520tasks%2520by%2520decomposing%2520them%2520into%2520simpler%250Asub-problems.%2520However%252C%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520still%2520struggle%250Awith%2520some%2520seemingly%2520straightforward%2520visual%2520tasks%252C%2520such%2520as%2520counting%2520and%2520solving%250Ajigsaw%2520puzzles.%2520We%2520argue%2520that%2520these%2520tasks%2520challenge%2520the%2520ability%2520of%250Avisual-to-textual%2520conversion%252C%2520where%2520MLLMs%2520convert%2520visual%2520information%2520perceived%250Afrom%2520the%2520input%2520scene%252C%2520to%2520textual%2520information%2520for%2520further%2520reasoning%2520and%250Agenerating%2520the%2520answer.%2520If%2520the%2520complexity%2520of%2520the%2520visual%2520input%2520is%2520beyond%2520the%250Aperceptual%2520capability%2520of%2520the%2520MLLMs%252C%2520without%2520decomposing%2520this%2520conversion%250Aprocess%252C%2520simply%2520scaling%2520inference-time%2520reasoning%2520cannot%2520solve%2520the%2520task%2520because%250Ait%2520repeatedly%2520encounters%2520the%2520same%2520perceptual%2520bottleneck.%2520We%2520propose%2520an%250Aapproach%252C%2520autonomous%2520imagination%252C%2520to%2520enable%2520MLLMs%2520to%2520iteratively%2520modify%2520visual%250Ainputs%2520%2528e.g.%2520isolating%2520objects%252C%2520rearranging%2520puzzle%2520pieces%2529%2520into%2520intermediate%250Avisual%2520states%252C%2520decomposing%2520visual-to-textual%2520conversion%2520into%2520closed-loop%2520visual%250Amodification%2520steps.%2520We%2520show%2520that%252C%2520without%2520any%2520retraining%252C%2520MLLMs%2520can%2520now%2520solve%250Atasks%2520initially%2520beyond%2520their%2520perceptual%2520capability%252C%2520highlighting%2520that%250Aclosed-loop%2520visual%2520modification%2520can%2520be%2520an%2520effective%2520way%2520of%2520decomposing%2520the%250Avisual%2520reasoning%2520task%2520into%2520solvable%2520substeps.%2520Our%2520code%2520and%2520data%2520are%2520released%2520at%250Ahttps%253A//future-item.github.io/autoimagine-site/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18142v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Imagination%3A%20Closed-Loop%20Decomposition%20of%20Visual-to-Textual%0A%20%20Conversion%20in%20Visual%20Reasoning%20for%20Multimodal%20Large%20Language%20Models&entry.906535625=Jingming%20Liu%20and%20Yumeng%20Li%20and%20Boyuan%20Xiao%20and%20Yichang%20Jian%20and%20Ziang%20Qin%20and%20Tianjia%20Shao%20and%20Yao-Xiang%20Ding%20and%20Kun%20Zhou&entry.1292438233=%20%20Under%20pure%20textual%20modality%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%0Aremarkable%20success%20in%20complex%20reasoning%20tasks%20by%20decomposing%20them%20into%20simpler%0Asub-problems.%20However%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20still%20struggle%0Awith%20some%20seemingly%20straightforward%20visual%20tasks%2C%20such%20as%20counting%20and%20solving%0Ajigsaw%20puzzles.%20We%20argue%20that%20these%20tasks%20challenge%20the%20ability%20of%0Avisual-to-textual%20conversion%2C%20where%20MLLMs%20convert%20visual%20information%20perceived%0Afrom%20the%20input%20scene%2C%20to%20textual%20information%20for%20further%20reasoning%20and%0Agenerating%20the%20answer.%20If%20the%20complexity%20of%20the%20visual%20input%20is%20beyond%20the%0Aperceptual%20capability%20of%20the%20MLLMs%2C%20without%20decomposing%20this%20conversion%0Aprocess%2C%20simply%20scaling%20inference-time%20reasoning%20cannot%20solve%20the%20task%20because%0Ait%20repeatedly%20encounters%20the%20same%20perceptual%20bottleneck.%20We%20propose%20an%0Aapproach%2C%20autonomous%20imagination%2C%20to%20enable%20MLLMs%20to%20iteratively%20modify%20visual%0Ainputs%20%28e.g.%20isolating%20objects%2C%20rearranging%20puzzle%20pieces%29%20into%20intermediate%0Avisual%20states%2C%20decomposing%20visual-to-textual%20conversion%20into%20closed-loop%20visual%0Amodification%20steps.%20We%20show%20that%2C%20without%20any%20retraining%2C%20MLLMs%20can%20now%20solve%0Atasks%20initially%20beyond%20their%20perceptual%20capability%2C%20highlighting%20that%0Aclosed-loop%20visual%20modification%20can%20be%20an%20effective%20way%20of%20decomposing%20the%0Avisual%20reasoning%20task%20into%20solvable%20substeps.%20Our%20code%20and%20data%20are%20released%20at%0Ahttps%3A//future-item.github.io/autoimagine-site/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18142v4&entry.124074799=Read"},
{"title": "Exploring the Efficacy of Modified Transfer Learning in Identifying\n  Parkinson's Disease Through Drawn Image Patterns", "author": "Nabil Daiyan and Md Rakibul Haque", "abstract": "  Parkinson's disease (PD) is a progressive neurodegenerative condition\ncharacterized by the death of dopaminergic neurons, leading to various movement\ndisorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,\nyet traditional diagnostic methods are often cumbersome and costly. In this\nstudy, a machine learning-based approach is proposed using hand-drawn spiral\nand wave images as potential biomarkers for PD detection. Our methodology\nleverages convolutional neural networks (CNNs), transfer learning, and\nattention mechanisms to improve model performance and resilience against\noverfitting. To enhance the diversity and richness of both spiral and wave\ncategories, the training dataset undergoes augmentation to increase the number\nof images. The proposed architecture comprises three phases: utilizing\npre-trained CNNs, incorporating custom convolutional layers, and ensemble\nvoting. Employing hard voting further enhances performance by aggregating\npredictions from multiple models. Experimental results show promising accuracy\nrates. For spiral images, weighted average precision, recall, and F1-score are\n90%, and for wave images, they are 96.67%. After combining the predictions\nthrough ensemble hard voting, the overall accuracy is 93.3%. These findings\nunderscore the potential of machine learning in early PD diagnosis, offering a\nnon-invasive and cost-effective solution to improve patient outcomes.\n", "link": "http://arxiv.org/abs/2510.05015v1", "date": "2025-10-06", "relevancy": 2.4243, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4931}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4839}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Efficacy%20of%20Modified%20Transfer%20Learning%20in%20Identifying%0A%20%20Parkinson%27s%20Disease%20Through%20Drawn%20Image%20Patterns&body=Title%3A%20Exploring%20the%20Efficacy%20of%20Modified%20Transfer%20Learning%20in%20Identifying%0A%20%20Parkinson%27s%20Disease%20Through%20Drawn%20Image%20Patterns%0AAuthor%3A%20Nabil%20Daiyan%20and%20Md%20Rakibul%20Haque%0AAbstract%3A%20%20%20Parkinson%27s%20disease%20%28PD%29%20is%20a%20progressive%20neurodegenerative%20condition%0Acharacterized%20by%20the%20death%20of%20dopaminergic%20neurons%2C%20leading%20to%20various%20movement%0Adisorder%20symptoms.%20Early%20diagnosis%20of%20PD%20is%20crucial%20to%20prevent%20adverse%20effects%2C%0Ayet%20traditional%20diagnostic%20methods%20are%20often%20cumbersome%20and%20costly.%20In%20this%0Astudy%2C%20a%20machine%20learning-based%20approach%20is%20proposed%20using%20hand-drawn%20spiral%0Aand%20wave%20images%20as%20potential%20biomarkers%20for%20PD%20detection.%20Our%20methodology%0Aleverages%20convolutional%20neural%20networks%20%28CNNs%29%2C%20transfer%20learning%2C%20and%0Aattention%20mechanisms%20to%20improve%20model%20performance%20and%20resilience%20against%0Aoverfitting.%20To%20enhance%20the%20diversity%20and%20richness%20of%20both%20spiral%20and%20wave%0Acategories%2C%20the%20training%20dataset%20undergoes%20augmentation%20to%20increase%20the%20number%0Aof%20images.%20The%20proposed%20architecture%20comprises%20three%20phases%3A%20utilizing%0Apre-trained%20CNNs%2C%20incorporating%20custom%20convolutional%20layers%2C%20and%20ensemble%0Avoting.%20Employing%20hard%20voting%20further%20enhances%20performance%20by%20aggregating%0Apredictions%20from%20multiple%20models.%20Experimental%20results%20show%20promising%20accuracy%0Arates.%20For%20spiral%20images%2C%20weighted%20average%20precision%2C%20recall%2C%20and%20F1-score%20are%0A90%25%2C%20and%20for%20wave%20images%2C%20they%20are%2096.67%25.%20After%20combining%20the%20predictions%0Athrough%20ensemble%20hard%20voting%2C%20the%20overall%20accuracy%20is%2093.3%25.%20These%20findings%0Aunderscore%20the%20potential%20of%20machine%20learning%20in%20early%20PD%20diagnosis%2C%20offering%20a%0Anon-invasive%20and%20cost-effective%20solution%20to%20improve%20patient%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Efficacy%2520of%2520Modified%2520Transfer%2520Learning%2520in%2520Identifying%250A%2520%2520Parkinson%2527s%2520Disease%2520Through%2520Drawn%2520Image%2520Patterns%26entry.906535625%3DNabil%2520Daiyan%2520and%2520Md%2520Rakibul%2520Haque%26entry.1292438233%3D%2520%2520Parkinson%2527s%2520disease%2520%2528PD%2529%2520is%2520a%2520progressive%2520neurodegenerative%2520condition%250Acharacterized%2520by%2520the%2520death%2520of%2520dopaminergic%2520neurons%252C%2520leading%2520to%2520various%2520movement%250Adisorder%2520symptoms.%2520Early%2520diagnosis%2520of%2520PD%2520is%2520crucial%2520to%2520prevent%2520adverse%2520effects%252C%250Ayet%2520traditional%2520diagnostic%2520methods%2520are%2520often%2520cumbersome%2520and%2520costly.%2520In%2520this%250Astudy%252C%2520a%2520machine%2520learning-based%2520approach%2520is%2520proposed%2520using%2520hand-drawn%2520spiral%250Aand%2520wave%2520images%2520as%2520potential%2520biomarkers%2520for%2520PD%2520detection.%2520Our%2520methodology%250Aleverages%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520transfer%2520learning%252C%2520and%250Aattention%2520mechanisms%2520to%2520improve%2520model%2520performance%2520and%2520resilience%2520against%250Aoverfitting.%2520To%2520enhance%2520the%2520diversity%2520and%2520richness%2520of%2520both%2520spiral%2520and%2520wave%250Acategories%252C%2520the%2520training%2520dataset%2520undergoes%2520augmentation%2520to%2520increase%2520the%2520number%250Aof%2520images.%2520The%2520proposed%2520architecture%2520comprises%2520three%2520phases%253A%2520utilizing%250Apre-trained%2520CNNs%252C%2520incorporating%2520custom%2520convolutional%2520layers%252C%2520and%2520ensemble%250Avoting.%2520Employing%2520hard%2520voting%2520further%2520enhances%2520performance%2520by%2520aggregating%250Apredictions%2520from%2520multiple%2520models.%2520Experimental%2520results%2520show%2520promising%2520accuracy%250Arates.%2520For%2520spiral%2520images%252C%2520weighted%2520average%2520precision%252C%2520recall%252C%2520and%2520F1-score%2520are%250A90%2525%252C%2520and%2520for%2520wave%2520images%252C%2520they%2520are%252096.67%2525.%2520After%2520combining%2520the%2520predictions%250Athrough%2520ensemble%2520hard%2520voting%252C%2520the%2520overall%2520accuracy%2520is%252093.3%2525.%2520These%2520findings%250Aunderscore%2520the%2520potential%2520of%2520machine%2520learning%2520in%2520early%2520PD%2520diagnosis%252C%2520offering%2520a%250Anon-invasive%2520and%2520cost-effective%2520solution%2520to%2520improve%2520patient%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Efficacy%20of%20Modified%20Transfer%20Learning%20in%20Identifying%0A%20%20Parkinson%27s%20Disease%20Through%20Drawn%20Image%20Patterns&entry.906535625=Nabil%20Daiyan%20and%20Md%20Rakibul%20Haque&entry.1292438233=%20%20Parkinson%27s%20disease%20%28PD%29%20is%20a%20progressive%20neurodegenerative%20condition%0Acharacterized%20by%20the%20death%20of%20dopaminergic%20neurons%2C%20leading%20to%20various%20movement%0Adisorder%20symptoms.%20Early%20diagnosis%20of%20PD%20is%20crucial%20to%20prevent%20adverse%20effects%2C%0Ayet%20traditional%20diagnostic%20methods%20are%20often%20cumbersome%20and%20costly.%20In%20this%0Astudy%2C%20a%20machine%20learning-based%20approach%20is%20proposed%20using%20hand-drawn%20spiral%0Aand%20wave%20images%20as%20potential%20biomarkers%20for%20PD%20detection.%20Our%20methodology%0Aleverages%20convolutional%20neural%20networks%20%28CNNs%29%2C%20transfer%20learning%2C%20and%0Aattention%20mechanisms%20to%20improve%20model%20performance%20and%20resilience%20against%0Aoverfitting.%20To%20enhance%20the%20diversity%20and%20richness%20of%20both%20spiral%20and%20wave%0Acategories%2C%20the%20training%20dataset%20undergoes%20augmentation%20to%20increase%20the%20number%0Aof%20images.%20The%20proposed%20architecture%20comprises%20three%20phases%3A%20utilizing%0Apre-trained%20CNNs%2C%20incorporating%20custom%20convolutional%20layers%2C%20and%20ensemble%0Avoting.%20Employing%20hard%20voting%20further%20enhances%20performance%20by%20aggregating%0Apredictions%20from%20multiple%20models.%20Experimental%20results%20show%20promising%20accuracy%0Arates.%20For%20spiral%20images%2C%20weighted%20average%20precision%2C%20recall%2C%20and%20F1-score%20are%0A90%25%2C%20and%20for%20wave%20images%2C%20they%20are%2096.67%25.%20After%20combining%20the%20predictions%0Athrough%20ensemble%20hard%20voting%2C%20the%20overall%20accuracy%20is%2093.3%25.%20These%20findings%0Aunderscore%20the%20potential%20of%20machine%20learning%20in%20early%20PD%20diagnosis%2C%20offering%20a%0Anon-invasive%20and%20cost-effective%20solution%20to%20improve%20patient%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05015v1&entry.124074799=Read"},
{"title": "Character Mixing for Video Generation", "author": "Tingting Liao and Chongjian Ge and Guangyi Liu and Hao Li and Yi Zhou", "abstract": "  Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where\ncharacters interact naturally across different worlds? We study inter-character\ninteraction in text-to-video generation, where the key challenge is to preserve\neach character's identity and behaviors while enabling coherent cross-context\ninteraction. This is difficult because characters may never have coexisted and\nbecause mixing styles often causes style delusion, where realistic characters\nappear cartoonish or vice versa. We introduce a framework that tackles these\nissues with Cross-Character Embedding (CCE), which learns identity and\nbehavioral logic across multimodal sources, and Cross-Character Augmentation\n(CCA), which enriches training with synthetic co-existence and mixed-style\ndata. Together, these techniques allow natural interactions between previously\nuncoexistent characters without losing stylistic fidelity. Experiments on a\ncurated benchmark of cartoons and live-action series with 10 characters show\nclear improvements in identity preservation, interaction quality, and\nrobustness to style delusion, enabling new forms of generative\nstorytelling.Additional results and videos are available on our project page:\nhttps://tingtingliao.github.io/mimix/.\n", "link": "http://arxiv.org/abs/2510.05093v1", "date": "2025-10-06", "relevancy": 2.4225, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6432}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.581}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Character%20Mixing%20for%20Video%20Generation&body=Title%3A%20Character%20Mixing%20for%20Video%20Generation%0AAuthor%3A%20Tingting%20Liao%20and%20Chongjian%20Ge%20and%20Guangyi%20Liu%20and%20Hao%20Li%20and%20Yi%20Zhou%0AAbstract%3A%20%20%20Imagine%20Mr.%20Bean%20stepping%20into%20Tom%20and%20Jerry--can%20we%20generate%20videos%20where%0Acharacters%20interact%20naturally%20across%20different%20worlds%3F%20We%20study%20inter-character%0Ainteraction%20in%20text-to-video%20generation%2C%20where%20the%20key%20challenge%20is%20to%20preserve%0Aeach%20character%27s%20identity%20and%20behaviors%20while%20enabling%20coherent%20cross-context%0Ainteraction.%20This%20is%20difficult%20because%20characters%20may%20never%20have%20coexisted%20and%0Abecause%20mixing%20styles%20often%20causes%20style%20delusion%2C%20where%20realistic%20characters%0Aappear%20cartoonish%20or%20vice%20versa.%20We%20introduce%20a%20framework%20that%20tackles%20these%0Aissues%20with%20Cross-Character%20Embedding%20%28CCE%29%2C%20which%20learns%20identity%20and%0Abehavioral%20logic%20across%20multimodal%20sources%2C%20and%20Cross-Character%20Augmentation%0A%28CCA%29%2C%20which%20enriches%20training%20with%20synthetic%20co-existence%20and%20mixed-style%0Adata.%20Together%2C%20these%20techniques%20allow%20natural%20interactions%20between%20previously%0Auncoexistent%20characters%20without%20losing%20stylistic%20fidelity.%20Experiments%20on%20a%0Acurated%20benchmark%20of%20cartoons%20and%20live-action%20series%20with%2010%20characters%20show%0Aclear%20improvements%20in%20identity%20preservation%2C%20interaction%20quality%2C%20and%0Arobustness%20to%20style%20delusion%2C%20enabling%20new%20forms%20of%20generative%0Astorytelling.Additional%20results%20and%20videos%20are%20available%20on%20our%20project%20page%3A%0Ahttps%3A//tingtingliao.github.io/mimix/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacter%2520Mixing%2520for%2520Video%2520Generation%26entry.906535625%3DTingting%2520Liao%2520and%2520Chongjian%2520Ge%2520and%2520Guangyi%2520Liu%2520and%2520Hao%2520Li%2520and%2520Yi%2520Zhou%26entry.1292438233%3D%2520%2520Imagine%2520Mr.%2520Bean%2520stepping%2520into%2520Tom%2520and%2520Jerry--can%2520we%2520generate%2520videos%2520where%250Acharacters%2520interact%2520naturally%2520across%2520different%2520worlds%253F%2520We%2520study%2520inter-character%250Ainteraction%2520in%2520text-to-video%2520generation%252C%2520where%2520the%2520key%2520challenge%2520is%2520to%2520preserve%250Aeach%2520character%2527s%2520identity%2520and%2520behaviors%2520while%2520enabling%2520coherent%2520cross-context%250Ainteraction.%2520This%2520is%2520difficult%2520because%2520characters%2520may%2520never%2520have%2520coexisted%2520and%250Abecause%2520mixing%2520styles%2520often%2520causes%2520style%2520delusion%252C%2520where%2520realistic%2520characters%250Aappear%2520cartoonish%2520or%2520vice%2520versa.%2520We%2520introduce%2520a%2520framework%2520that%2520tackles%2520these%250Aissues%2520with%2520Cross-Character%2520Embedding%2520%2528CCE%2529%252C%2520which%2520learns%2520identity%2520and%250Abehavioral%2520logic%2520across%2520multimodal%2520sources%252C%2520and%2520Cross-Character%2520Augmentation%250A%2528CCA%2529%252C%2520which%2520enriches%2520training%2520with%2520synthetic%2520co-existence%2520and%2520mixed-style%250Adata.%2520Together%252C%2520these%2520techniques%2520allow%2520natural%2520interactions%2520between%2520previously%250Auncoexistent%2520characters%2520without%2520losing%2520stylistic%2520fidelity.%2520Experiments%2520on%2520a%250Acurated%2520benchmark%2520of%2520cartoons%2520and%2520live-action%2520series%2520with%252010%2520characters%2520show%250Aclear%2520improvements%2520in%2520identity%2520preservation%252C%2520interaction%2520quality%252C%2520and%250Arobustness%2520to%2520style%2520delusion%252C%2520enabling%2520new%2520forms%2520of%2520generative%250Astorytelling.Additional%2520results%2520and%2520videos%2520are%2520available%2520on%2520our%2520project%2520page%253A%250Ahttps%253A//tingtingliao.github.io/mimix/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Character%20Mixing%20for%20Video%20Generation&entry.906535625=Tingting%20Liao%20and%20Chongjian%20Ge%20and%20Guangyi%20Liu%20and%20Hao%20Li%20and%20Yi%20Zhou&entry.1292438233=%20%20Imagine%20Mr.%20Bean%20stepping%20into%20Tom%20and%20Jerry--can%20we%20generate%20videos%20where%0Acharacters%20interact%20naturally%20across%20different%20worlds%3F%20We%20study%20inter-character%0Ainteraction%20in%20text-to-video%20generation%2C%20where%20the%20key%20challenge%20is%20to%20preserve%0Aeach%20character%27s%20identity%20and%20behaviors%20while%20enabling%20coherent%20cross-context%0Ainteraction.%20This%20is%20difficult%20because%20characters%20may%20never%20have%20coexisted%20and%0Abecause%20mixing%20styles%20often%20causes%20style%20delusion%2C%20where%20realistic%20characters%0Aappear%20cartoonish%20or%20vice%20versa.%20We%20introduce%20a%20framework%20that%20tackles%20these%0Aissues%20with%20Cross-Character%20Embedding%20%28CCE%29%2C%20which%20learns%20identity%20and%0Abehavioral%20logic%20across%20multimodal%20sources%2C%20and%20Cross-Character%20Augmentation%0A%28CCA%29%2C%20which%20enriches%20training%20with%20synthetic%20co-existence%20and%20mixed-style%0Adata.%20Together%2C%20these%20techniques%20allow%20natural%20interactions%20between%20previously%0Auncoexistent%20characters%20without%20losing%20stylistic%20fidelity.%20Experiments%20on%20a%0Acurated%20benchmark%20of%20cartoons%20and%20live-action%20series%20with%2010%20characters%20show%0Aclear%20improvements%20in%20identity%20preservation%2C%20interaction%20quality%2C%20and%0Arobustness%20to%20style%20delusion%2C%20enabling%20new%20forms%20of%20generative%0Astorytelling.Additional%20results%20and%20videos%20are%20available%20on%20our%20project%20page%3A%0Ahttps%3A//tingtingliao.github.io/mimix/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05093v1&entry.124074799=Read"},
{"title": "Federated Self-Supervised Learning for Automatic Modulation\n  Classification under Non-IID and Class-Imbalanced Data", "author": "Usman Akram and Yiyue Chen and Haris Vikalo", "abstract": "  Training automatic modulation classification (AMC) models on centrally\naggregated data raises privacy concerns, incurs communication overhead, and\noften fails to confer robustness to channel shifts. Federated learning (FL)\navoids central aggregation by training on distributed clients but remains\nsensitive to class imbalance, non-IID client distributions, and limited labeled\nsamples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with\ntriplet-loss self-supervision on unlabeled I/Q sequences across clients,\nfollowed by per-client SVMs on small labeled sets. We establish convergence of\nthe federated representation learning procedure and a separability guarantee\nfor the downstream classifier under feature noise. Experiments on synthetic and\nover-the-air datasets show consistent gains over supervised FL baselines under\nheterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.\n", "link": "http://arxiv.org/abs/2510.04927v1", "date": "2025-10-06", "relevancy": 2.374, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4888}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4691}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Self-Supervised%20Learning%20for%20Automatic%20Modulation%0A%20%20Classification%20under%20Non-IID%20and%20Class-Imbalanced%20Data&body=Title%3A%20Federated%20Self-Supervised%20Learning%20for%20Automatic%20Modulation%0A%20%20Classification%20under%20Non-IID%20and%20Class-Imbalanced%20Data%0AAuthor%3A%20Usman%20Akram%20and%20Yiyue%20Chen%20and%20Haris%20Vikalo%0AAbstract%3A%20%20%20Training%20automatic%20modulation%20classification%20%28AMC%29%20models%20on%20centrally%0Aaggregated%20data%20raises%20privacy%20concerns%2C%20incurs%20communication%20overhead%2C%20and%0Aoften%20fails%20to%20confer%20robustness%20to%20channel%20shifts.%20Federated%20learning%20%28FL%29%0Aavoids%20central%20aggregation%20by%20training%20on%20distributed%20clients%20but%20remains%0Asensitive%20to%20class%20imbalance%2C%20non-IID%20client%20distributions%2C%20and%20limited%20labeled%0Asamples.%20We%20propose%20FedSSL-AMC%2C%20which%20trains%20a%20causal%2C%20time-dilated%20CNN%20with%0Atriplet-loss%20self-supervision%20on%20unlabeled%20I/Q%20sequences%20across%20clients%2C%0Afollowed%20by%20per-client%20SVMs%20on%20small%20labeled%20sets.%20We%20establish%20convergence%20of%0Athe%20federated%20representation%20learning%20procedure%20and%20a%20separability%20guarantee%0Afor%20the%20downstream%20classifier%20under%20feature%20noise.%20Experiments%20on%20synthetic%20and%0Aover-the-air%20datasets%20show%20consistent%20gains%20over%20supervised%20FL%20baselines%20under%0Aheterogeneous%20SNR%2C%20carrier-frequency%20offsets%2C%20and%20non-IID%20label%20partitions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Self-Supervised%2520Learning%2520for%2520Automatic%2520Modulation%250A%2520%2520Classification%2520under%2520Non-IID%2520and%2520Class-Imbalanced%2520Data%26entry.906535625%3DUsman%2520Akram%2520and%2520Yiyue%2520Chen%2520and%2520Haris%2520Vikalo%26entry.1292438233%3D%2520%2520Training%2520automatic%2520modulation%2520classification%2520%2528AMC%2529%2520models%2520on%2520centrally%250Aaggregated%2520data%2520raises%2520privacy%2520concerns%252C%2520incurs%2520communication%2520overhead%252C%2520and%250Aoften%2520fails%2520to%2520confer%2520robustness%2520to%2520channel%2520shifts.%2520Federated%2520learning%2520%2528FL%2529%250Aavoids%2520central%2520aggregation%2520by%2520training%2520on%2520distributed%2520clients%2520but%2520remains%250Asensitive%2520to%2520class%2520imbalance%252C%2520non-IID%2520client%2520distributions%252C%2520and%2520limited%2520labeled%250Asamples.%2520We%2520propose%2520FedSSL-AMC%252C%2520which%2520trains%2520a%2520causal%252C%2520time-dilated%2520CNN%2520with%250Atriplet-loss%2520self-supervision%2520on%2520unlabeled%2520I/Q%2520sequences%2520across%2520clients%252C%250Afollowed%2520by%2520per-client%2520SVMs%2520on%2520small%2520labeled%2520sets.%2520We%2520establish%2520convergence%2520of%250Athe%2520federated%2520representation%2520learning%2520procedure%2520and%2520a%2520separability%2520guarantee%250Afor%2520the%2520downstream%2520classifier%2520under%2520feature%2520noise.%2520Experiments%2520on%2520synthetic%2520and%250Aover-the-air%2520datasets%2520show%2520consistent%2520gains%2520over%2520supervised%2520FL%2520baselines%2520under%250Aheterogeneous%2520SNR%252C%2520carrier-frequency%2520offsets%252C%2520and%2520non-IID%2520label%2520partitions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Self-Supervised%20Learning%20for%20Automatic%20Modulation%0A%20%20Classification%20under%20Non-IID%20and%20Class-Imbalanced%20Data&entry.906535625=Usman%20Akram%20and%20Yiyue%20Chen%20and%20Haris%20Vikalo&entry.1292438233=%20%20Training%20automatic%20modulation%20classification%20%28AMC%29%20models%20on%20centrally%0Aaggregated%20data%20raises%20privacy%20concerns%2C%20incurs%20communication%20overhead%2C%20and%0Aoften%20fails%20to%20confer%20robustness%20to%20channel%20shifts.%20Federated%20learning%20%28FL%29%0Aavoids%20central%20aggregation%20by%20training%20on%20distributed%20clients%20but%20remains%0Asensitive%20to%20class%20imbalance%2C%20non-IID%20client%20distributions%2C%20and%20limited%20labeled%0Asamples.%20We%20propose%20FedSSL-AMC%2C%20which%20trains%20a%20causal%2C%20time-dilated%20CNN%20with%0Atriplet-loss%20self-supervision%20on%20unlabeled%20I/Q%20sequences%20across%20clients%2C%0Afollowed%20by%20per-client%20SVMs%20on%20small%20labeled%20sets.%20We%20establish%20convergence%20of%0Athe%20federated%20representation%20learning%20procedure%20and%20a%20separability%20guarantee%0Afor%20the%20downstream%20classifier%20under%20feature%20noise.%20Experiments%20on%20synthetic%20and%0Aover-the-air%20datasets%20show%20consistent%20gains%20over%20supervised%20FL%20baselines%20under%0Aheterogeneous%20SNR%2C%20carrier-frequency%20offsets%2C%20and%20non-IID%20label%20partitions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04927v1&entry.124074799=Read"},
{"title": "Imperceptible Jailbreaking against Large Language Models", "author": "Kuofeng Gao and Yiming Li and Chao Du and Xin Wang and Xingjun Ma and Shu-Tao Xia and Tianyu Pang", "abstract": "  Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks.\n", "link": "http://arxiv.org/abs/2510.05025v1", "date": "2025-10-06", "relevancy": 2.3649, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4804}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imperceptible%20Jailbreaking%20against%20Large%20Language%20Models&body=Title%3A%20Imperceptible%20Jailbreaking%20against%20Large%20Language%20Models%0AAuthor%3A%20Kuofeng%20Gao%20and%20Yiming%20Li%20and%20Chao%20Du%20and%20Xin%20Wang%20and%20Xingjun%20Ma%20and%20Shu-Tao%20Xia%20and%20Tianyu%20Pang%0AAbstract%3A%20%20%20Jailbreaking%20attacks%20on%20the%20vision%20modality%20typically%20rely%20on%20imperceptible%0Aadversarial%20perturbations%2C%20whereas%20attacks%20on%20the%20textual%20modality%20are%0Agenerally%20assumed%20to%20require%20visible%20modifications%20%28e.g.%2C%20non-semantic%0Asuffixes%29.%20In%20this%20paper%2C%20we%20introduce%20imperceptible%20jailbreaks%20that%20exploit%20a%0Aclass%20of%20Unicode%20characters%20called%20variation%20selectors.%20By%20appending%20invisible%0Avariation%20selectors%20to%20malicious%20questions%2C%20the%20jailbreak%20prompts%20appear%0Avisually%20identical%20to%20original%20malicious%20questions%20on%20screen%2C%20while%20their%0Atokenization%20is%20%22secretly%22%20altered.%20We%20propose%20a%20chain-of-search%20pipeline%20to%0Agenerate%20such%20adversarial%20suffixes%20to%20induce%20harmful%20responses.%20Our%20experiments%0Ashow%20that%20our%20imperceptible%20jailbreaks%20achieve%20high%20attack%20success%20rates%0Aagainst%20four%20aligned%20LLMs%20and%20generalize%20to%20prompt%20injection%20attacks%2C%20all%0Awithout%20producing%20any%20visible%20modifications%20in%20the%20written%20prompt.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/sail-sg/imperceptible-jailbreaks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImperceptible%2520Jailbreaking%2520against%2520Large%2520Language%2520Models%26entry.906535625%3DKuofeng%2520Gao%2520and%2520Yiming%2520Li%2520and%2520Chao%2520Du%2520and%2520Xin%2520Wang%2520and%2520Xingjun%2520Ma%2520and%2520Shu-Tao%2520Xia%2520and%2520Tianyu%2520Pang%26entry.1292438233%3D%2520%2520Jailbreaking%2520attacks%2520on%2520the%2520vision%2520modality%2520typically%2520rely%2520on%2520imperceptible%250Aadversarial%2520perturbations%252C%2520whereas%2520attacks%2520on%2520the%2520textual%2520modality%2520are%250Agenerally%2520assumed%2520to%2520require%2520visible%2520modifications%2520%2528e.g.%252C%2520non-semantic%250Asuffixes%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520imperceptible%2520jailbreaks%2520that%2520exploit%2520a%250Aclass%2520of%2520Unicode%2520characters%2520called%2520variation%2520selectors.%2520By%2520appending%2520invisible%250Avariation%2520selectors%2520to%2520malicious%2520questions%252C%2520the%2520jailbreak%2520prompts%2520appear%250Avisually%2520identical%2520to%2520original%2520malicious%2520questions%2520on%2520screen%252C%2520while%2520their%250Atokenization%2520is%2520%2522secretly%2522%2520altered.%2520We%2520propose%2520a%2520chain-of-search%2520pipeline%2520to%250Agenerate%2520such%2520adversarial%2520suffixes%2520to%2520induce%2520harmful%2520responses.%2520Our%2520experiments%250Ashow%2520that%2520our%2520imperceptible%2520jailbreaks%2520achieve%2520high%2520attack%2520success%2520rates%250Aagainst%2520four%2520aligned%2520LLMs%2520and%2520generalize%2520to%2520prompt%2520injection%2520attacks%252C%2520all%250Awithout%2520producing%2520any%2520visible%2520modifications%2520in%2520the%2520written%2520prompt.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/sail-sg/imperceptible-jailbreaks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imperceptible%20Jailbreaking%20against%20Large%20Language%20Models&entry.906535625=Kuofeng%20Gao%20and%20Yiming%20Li%20and%20Chao%20Du%20and%20Xin%20Wang%20and%20Xingjun%20Ma%20and%20Shu-Tao%20Xia%20and%20Tianyu%20Pang&entry.1292438233=%20%20Jailbreaking%20attacks%20on%20the%20vision%20modality%20typically%20rely%20on%20imperceptible%0Aadversarial%20perturbations%2C%20whereas%20attacks%20on%20the%20textual%20modality%20are%0Agenerally%20assumed%20to%20require%20visible%20modifications%20%28e.g.%2C%20non-semantic%0Asuffixes%29.%20In%20this%20paper%2C%20we%20introduce%20imperceptible%20jailbreaks%20that%20exploit%20a%0Aclass%20of%20Unicode%20characters%20called%20variation%20selectors.%20By%20appending%20invisible%0Avariation%20selectors%20to%20malicious%20questions%2C%20the%20jailbreak%20prompts%20appear%0Avisually%20identical%20to%20original%20malicious%20questions%20on%20screen%2C%20while%20their%0Atokenization%20is%20%22secretly%22%20altered.%20We%20propose%20a%20chain-of-search%20pipeline%20to%0Agenerate%20such%20adversarial%20suffixes%20to%20induce%20harmful%20responses.%20Our%20experiments%0Ashow%20that%20our%20imperceptible%20jailbreaks%20achieve%20high%20attack%20success%20rates%0Aagainst%20four%20aligned%20LLMs%20and%20generalize%20to%20prompt%20injection%20attacks%2C%20all%0Awithout%20producing%20any%20visible%20modifications%20in%20the%20written%20prompt.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/sail-sg/imperceptible-jailbreaks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05025v1&entry.124074799=Read"},
{"title": "Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps", "author": "Kyoungjun Park and Yifan Yang and Changhan Ge and Lili Qiu and Shiqi Jiang", "abstract": "  Modeling radio frequency (RF) signal propagation is essential for\nunderstanding the environment, as RF signals offer valuable insights beyond the\ncapabilities of RGB cameras, which are limited by the visible-light spectrum,\nlens coverage, and occlusions. It is also useful for supporting wireless\ndiagnosis, deployment, and optimization. However, accurately predicting RF\nsignals in complex environments remains a challenge due to interactions with\nobstacles such as absorption and reflection. We introduce Diffusion^2, a\ndiffusion-based approach that uses 3D point clouds to model the propagation of\nRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.\nTo effectively capture RF-related features from 3D data, we present the RF-3D\nEncoder, which encapsulates the complexities of 3D geometry along with\nsignal-specific details. These features undergo multi-scale embedding to\nsimulate the actual RF signal dissemination process. Our evaluation, based on\nsynthetic and real-world measurements, demonstrates that Diffusion^2 accurately\nestimates the behavior of RF signals in various frequency bands and\nenvironmental conditions, with an error margin of just 1.9 dB and 27x faster\nthan existing methods, marking a significant advancement in the field. Refer to\nhttps://rfvision-project.github.io/ for more information.\n", "link": "http://arxiv.org/abs/2510.02274v2", "date": "2025-10-06", "relevancy": 2.3538, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5913}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5913}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%5E2%3A%20Turning%203D%20Environments%20into%20Radio%20Frequency%20Heatmaps&body=Title%3A%20Diffusion%5E2%3A%20Turning%203D%20Environments%20into%20Radio%20Frequency%20Heatmaps%0AAuthor%3A%20Kyoungjun%20Park%20and%20Yifan%20Yang%20and%20Changhan%20Ge%20and%20Lili%20Qiu%20and%20Shiqi%20Jiang%0AAbstract%3A%20%20%20Modeling%20radio%20frequency%20%28RF%29%20signal%20propagation%20is%20essential%20for%0Aunderstanding%20the%20environment%2C%20as%20RF%20signals%20offer%20valuable%20insights%20beyond%20the%0Acapabilities%20of%20RGB%20cameras%2C%20which%20are%20limited%20by%20the%20visible-light%20spectrum%2C%0Alens%20coverage%2C%20and%20occlusions.%20It%20is%20also%20useful%20for%20supporting%20wireless%0Adiagnosis%2C%20deployment%2C%20and%20optimization.%20However%2C%20accurately%20predicting%20RF%0Asignals%20in%20complex%20environments%20remains%20a%20challenge%20due%20to%20interactions%20with%0Aobstacles%20such%20as%20absorption%20and%20reflection.%20We%20introduce%20Diffusion%5E2%2C%20a%0Adiffusion-based%20approach%20that%20uses%203D%20point%20clouds%20to%20model%20the%20propagation%20of%0ARF%20signals%20across%20a%20wide%20range%20of%20frequencies%2C%20from%20Wi-Fi%20to%20millimeter%20waves.%0ATo%20effectively%20capture%20RF-related%20features%20from%203D%20data%2C%20we%20present%20the%20RF-3D%0AEncoder%2C%20which%20encapsulates%20the%20complexities%20of%203D%20geometry%20along%20with%0Asignal-specific%20details.%20These%20features%20undergo%20multi-scale%20embedding%20to%0Asimulate%20the%20actual%20RF%20signal%20dissemination%20process.%20Our%20evaluation%2C%20based%20on%0Asynthetic%20and%20real-world%20measurements%2C%20demonstrates%20that%20Diffusion%5E2%20accurately%0Aestimates%20the%20behavior%20of%20RF%20signals%20in%20various%20frequency%20bands%20and%0Aenvironmental%20conditions%2C%20with%20an%20error%20margin%20of%20just%201.9%20dB%20and%2027x%20faster%0Athan%20existing%20methods%2C%20marking%20a%20significant%20advancement%20in%20the%20field.%20Refer%20to%0Ahttps%3A//rfvision-project.github.io/%20for%20more%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02274v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%255E2%253A%2520Turning%25203D%2520Environments%2520into%2520Radio%2520Frequency%2520Heatmaps%26entry.906535625%3DKyoungjun%2520Park%2520and%2520Yifan%2520Yang%2520and%2520Changhan%2520Ge%2520and%2520Lili%2520Qiu%2520and%2520Shiqi%2520Jiang%26entry.1292438233%3D%2520%2520Modeling%2520radio%2520frequency%2520%2528RF%2529%2520signal%2520propagation%2520is%2520essential%2520for%250Aunderstanding%2520the%2520environment%252C%2520as%2520RF%2520signals%2520offer%2520valuable%2520insights%2520beyond%2520the%250Acapabilities%2520of%2520RGB%2520cameras%252C%2520which%2520are%2520limited%2520by%2520the%2520visible-light%2520spectrum%252C%250Alens%2520coverage%252C%2520and%2520occlusions.%2520It%2520is%2520also%2520useful%2520for%2520supporting%2520wireless%250Adiagnosis%252C%2520deployment%252C%2520and%2520optimization.%2520However%252C%2520accurately%2520predicting%2520RF%250Asignals%2520in%2520complex%2520environments%2520remains%2520a%2520challenge%2520due%2520to%2520interactions%2520with%250Aobstacles%2520such%2520as%2520absorption%2520and%2520reflection.%2520We%2520introduce%2520Diffusion%255E2%252C%2520a%250Adiffusion-based%2520approach%2520that%2520uses%25203D%2520point%2520clouds%2520to%2520model%2520the%2520propagation%2520of%250ARF%2520signals%2520across%2520a%2520wide%2520range%2520of%2520frequencies%252C%2520from%2520Wi-Fi%2520to%2520millimeter%2520waves.%250ATo%2520effectively%2520capture%2520RF-related%2520features%2520from%25203D%2520data%252C%2520we%2520present%2520the%2520RF-3D%250AEncoder%252C%2520which%2520encapsulates%2520the%2520complexities%2520of%25203D%2520geometry%2520along%2520with%250Asignal-specific%2520details.%2520These%2520features%2520undergo%2520multi-scale%2520embedding%2520to%250Asimulate%2520the%2520actual%2520RF%2520signal%2520dissemination%2520process.%2520Our%2520evaluation%252C%2520based%2520on%250Asynthetic%2520and%2520real-world%2520measurements%252C%2520demonstrates%2520that%2520Diffusion%255E2%2520accurately%250Aestimates%2520the%2520behavior%2520of%2520RF%2520signals%2520in%2520various%2520frequency%2520bands%2520and%250Aenvironmental%2520conditions%252C%2520with%2520an%2520error%2520margin%2520of%2520just%25201.9%2520dB%2520and%252027x%2520faster%250Athan%2520existing%2520methods%252C%2520marking%2520a%2520significant%2520advancement%2520in%2520the%2520field.%2520Refer%2520to%250Ahttps%253A//rfvision-project.github.io/%2520for%2520more%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02274v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%5E2%3A%20Turning%203D%20Environments%20into%20Radio%20Frequency%20Heatmaps&entry.906535625=Kyoungjun%20Park%20and%20Yifan%20Yang%20and%20Changhan%20Ge%20and%20Lili%20Qiu%20and%20Shiqi%20Jiang&entry.1292438233=%20%20Modeling%20radio%20frequency%20%28RF%29%20signal%20propagation%20is%20essential%20for%0Aunderstanding%20the%20environment%2C%20as%20RF%20signals%20offer%20valuable%20insights%20beyond%20the%0Acapabilities%20of%20RGB%20cameras%2C%20which%20are%20limited%20by%20the%20visible-light%20spectrum%2C%0Alens%20coverage%2C%20and%20occlusions.%20It%20is%20also%20useful%20for%20supporting%20wireless%0Adiagnosis%2C%20deployment%2C%20and%20optimization.%20However%2C%20accurately%20predicting%20RF%0Asignals%20in%20complex%20environments%20remains%20a%20challenge%20due%20to%20interactions%20with%0Aobstacles%20such%20as%20absorption%20and%20reflection.%20We%20introduce%20Diffusion%5E2%2C%20a%0Adiffusion-based%20approach%20that%20uses%203D%20point%20clouds%20to%20model%20the%20propagation%20of%0ARF%20signals%20across%20a%20wide%20range%20of%20frequencies%2C%20from%20Wi-Fi%20to%20millimeter%20waves.%0ATo%20effectively%20capture%20RF-related%20features%20from%203D%20data%2C%20we%20present%20the%20RF-3D%0AEncoder%2C%20which%20encapsulates%20the%20complexities%20of%203D%20geometry%20along%20with%0Asignal-specific%20details.%20These%20features%20undergo%20multi-scale%20embedding%20to%0Asimulate%20the%20actual%20RF%20signal%20dissemination%20process.%20Our%20evaluation%2C%20based%20on%0Asynthetic%20and%20real-world%20measurements%2C%20demonstrates%20that%20Diffusion%5E2%20accurately%0Aestimates%20the%20behavior%20of%20RF%20signals%20in%20various%20frequency%20bands%20and%0Aenvironmental%20conditions%2C%20with%20an%20error%20margin%20of%20just%201.9%20dB%20and%2027x%20faster%0Athan%20existing%20methods%2C%20marking%20a%20significant%20advancement%20in%20the%20field.%20Refer%20to%0Ahttps%3A//rfvision-project.github.io/%20for%20more%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02274v2&entry.124074799=Read"},
{"title": "Learning Penalty for Optimal Partitioning via Automatic Feature\n  Extraction", "author": "Tung L Nguyen and Toby Hocking", "abstract": "  Changepoint detection identifies significant shifts in data sequences, making\nit important in areas like finance, genetics, and healthcare. The Optimal\nPartitioning algorithms efficiently detect these changes, using a penalty\nparameter to limit the changepoints count. Determining the optimal value for\nthis penalty can be challenging. Traditionally, this process involved manually\nextracting statistical features, such as sequence length or variance to make\nthe prediction. This study proposes a novel approach that uses recurrent\nnetworks to learn this penalty directly from raw sequences by automatically\nextracting features. Experiments conducted on 20 benchmark genomic datasets\nshow that this novel method generally outperforms traditional ones in\nchangepoint detection accuracy.\n", "link": "http://arxiv.org/abs/2505.07413v2", "date": "2025-10-06", "relevancy": 2.3493, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4731}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.469}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Penalty%20for%20Optimal%20Partitioning%20via%20Automatic%20Feature%0A%20%20Extraction&body=Title%3A%20Learning%20Penalty%20for%20Optimal%20Partitioning%20via%20Automatic%20Feature%0A%20%20Extraction%0AAuthor%3A%20Tung%20L%20Nguyen%20and%20Toby%20Hocking%0AAbstract%3A%20%20%20Changepoint%20detection%20identifies%20significant%20shifts%20in%20data%20sequences%2C%20making%0Ait%20important%20in%20areas%20like%20finance%2C%20genetics%2C%20and%20healthcare.%20The%20Optimal%0APartitioning%20algorithms%20efficiently%20detect%20these%20changes%2C%20using%20a%20penalty%0Aparameter%20to%20limit%20the%20changepoints%20count.%20Determining%20the%20optimal%20value%20for%0Athis%20penalty%20can%20be%20challenging.%20Traditionally%2C%20this%20process%20involved%20manually%0Aextracting%20statistical%20features%2C%20such%20as%20sequence%20length%20or%20variance%20to%20make%0Athe%20prediction.%20This%20study%20proposes%20a%20novel%20approach%20that%20uses%20recurrent%0Anetworks%20to%20learn%20this%20penalty%20directly%20from%20raw%20sequences%20by%20automatically%0Aextracting%20features.%20Experiments%20conducted%20on%2020%20benchmark%20genomic%20datasets%0Ashow%20that%20this%20novel%20method%20generally%20outperforms%20traditional%20ones%20in%0Achangepoint%20detection%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07413v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Penalty%2520for%2520Optimal%2520Partitioning%2520via%2520Automatic%2520Feature%250A%2520%2520Extraction%26entry.906535625%3DTung%2520L%2520Nguyen%2520and%2520Toby%2520Hocking%26entry.1292438233%3D%2520%2520Changepoint%2520detection%2520identifies%2520significant%2520shifts%2520in%2520data%2520sequences%252C%2520making%250Ait%2520important%2520in%2520areas%2520like%2520finance%252C%2520genetics%252C%2520and%2520healthcare.%2520The%2520Optimal%250APartitioning%2520algorithms%2520efficiently%2520detect%2520these%2520changes%252C%2520using%2520a%2520penalty%250Aparameter%2520to%2520limit%2520the%2520changepoints%2520count.%2520Determining%2520the%2520optimal%2520value%2520for%250Athis%2520penalty%2520can%2520be%2520challenging.%2520Traditionally%252C%2520this%2520process%2520involved%2520manually%250Aextracting%2520statistical%2520features%252C%2520such%2520as%2520sequence%2520length%2520or%2520variance%2520to%2520make%250Athe%2520prediction.%2520This%2520study%2520proposes%2520a%2520novel%2520approach%2520that%2520uses%2520recurrent%250Anetworks%2520to%2520learn%2520this%2520penalty%2520directly%2520from%2520raw%2520sequences%2520by%2520automatically%250Aextracting%2520features.%2520Experiments%2520conducted%2520on%252020%2520benchmark%2520genomic%2520datasets%250Ashow%2520that%2520this%2520novel%2520method%2520generally%2520outperforms%2520traditional%2520ones%2520in%250Achangepoint%2520detection%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07413v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Penalty%20for%20Optimal%20Partitioning%20via%20Automatic%20Feature%0A%20%20Extraction&entry.906535625=Tung%20L%20Nguyen%20and%20Toby%20Hocking&entry.1292438233=%20%20Changepoint%20detection%20identifies%20significant%20shifts%20in%20data%20sequences%2C%20making%0Ait%20important%20in%20areas%20like%20finance%2C%20genetics%2C%20and%20healthcare.%20The%20Optimal%0APartitioning%20algorithms%20efficiently%20detect%20these%20changes%2C%20using%20a%20penalty%0Aparameter%20to%20limit%20the%20changepoints%20count.%20Determining%20the%20optimal%20value%20for%0Athis%20penalty%20can%20be%20challenging.%20Traditionally%2C%20this%20process%20involved%20manually%0Aextracting%20statistical%20features%2C%20such%20as%20sequence%20length%20or%20variance%20to%20make%0Athe%20prediction.%20This%20study%20proposes%20a%20novel%20approach%20that%20uses%20recurrent%0Anetworks%20to%20learn%20this%20penalty%20directly%20from%20raw%20sequences%20by%20automatically%0Aextracting%20features.%20Experiments%20conducted%20on%2020%20benchmark%20genomic%20datasets%0Ashow%20that%20this%20novel%20method%20generally%20outperforms%20traditional%20ones%20in%0Achangepoint%20detection%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07413v2&entry.124074799=Read"},
{"title": "Paper2Video: Automatic Video Generation from Scientific Papers", "author": "Zeyu Zhu and Kevin Qinghong Lin and Mike Zheng Shou", "abstract": "  Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.\n", "link": "http://arxiv.org/abs/2510.05096v1", "date": "2025-10-06", "relevancy": 2.318, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.584}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.578}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Paper2Video%3A%20Automatic%20Video%20Generation%20from%20Scientific%20Papers&body=Title%3A%20Paper2Video%3A%20Automatic%20Video%20Generation%20from%20Scientific%20Papers%0AAuthor%3A%20Zeyu%20Zhu%20and%20Kevin%20Qinghong%20Lin%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Academic%20presentation%20videos%20have%20become%20an%20essential%20medium%20for%20research%0Acommunication%2C%20yet%20producing%20them%20remains%20highly%20labor-intensive%2C%20often%0Arequiring%20hours%20of%20slide%20design%2C%20recording%2C%20and%20editing%20for%20a%20short%202%20to%2010%0Aminutes%20video.%20Unlike%20natural%20video%2C%20presentation%20video%20generation%20involves%0Adistinctive%20challenges%3A%20inputs%20from%20research%20papers%2C%20dense%20multi-modal%0Ainformation%20%28text%2C%20figures%2C%20tables%29%2C%20and%20the%20need%20to%20coordinate%20multiple%0Aaligned%20channels%20such%20as%20slides%2C%20subtitles%2C%20speech%2C%20and%20human%20talker.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20PaperTalker%2C%20the%20first%20benchmark%20of%20101%0Aresearch%20papers%20paired%20with%20author-created%20presentation%20videos%2C%20slides%2C%20and%0Aspeaker%20metadata.%20We%20further%20design%20four%20tailored%20evaluation%20metrics--Meta%0ASimilarity%2C%20PresentArena%2C%20PresentQuiz%2C%20and%20IP%20Memory--to%20measure%20how%20videos%0Aconvey%20the%20paper%27s%20information%20to%20the%20audience.%20Building%20on%20this%20foundation%2C%20we%0Apropose%20PaperTalker%2C%20the%20first%20multi-agent%20framework%20for%20academic%20presentation%0Avideo%20generation.%20It%20integrates%20slide%20generation%20with%20effective%20layout%0Arefinement%20by%20a%20novel%20effective%20tree%20search%20visual%20choice%2C%20cursor%20grounding%2C%0Asubtitling%2C%20speech%20synthesis%2C%20and%20talking-head%20rendering%2C%20while%20parallelizing%0Aslide-wise%20generation%20for%20efficiency.%20Experiments%20on%20Paper2Video%20demonstrate%0Athat%20the%20presentation%20videos%20produced%20by%20our%20approach%20are%20more%20faithful%20and%0Ainformative%20than%20existing%20baselines%2C%20establishing%20a%20practical%20step%20toward%0Aautomated%20and%20ready-to-use%20academic%20video%20generation.%20Our%20dataset%2C%20agent%2C%20and%0Acode%20are%20available%20at%20https%3A//github.com/showlab/Paper2Video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaper2Video%253A%2520Automatic%2520Video%2520Generation%2520from%2520Scientific%2520Papers%26entry.906535625%3DZeyu%2520Zhu%2520and%2520Kevin%2520Qinghong%2520Lin%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Academic%2520presentation%2520videos%2520have%2520become%2520an%2520essential%2520medium%2520for%2520research%250Acommunication%252C%2520yet%2520producing%2520them%2520remains%2520highly%2520labor-intensive%252C%2520often%250Arequiring%2520hours%2520of%2520slide%2520design%252C%2520recording%252C%2520and%2520editing%2520for%2520a%2520short%25202%2520to%252010%250Aminutes%2520video.%2520Unlike%2520natural%2520video%252C%2520presentation%2520video%2520generation%2520involves%250Adistinctive%2520challenges%253A%2520inputs%2520from%2520research%2520papers%252C%2520dense%2520multi-modal%250Ainformation%2520%2528text%252C%2520figures%252C%2520tables%2529%252C%2520and%2520the%2520need%2520to%2520coordinate%2520multiple%250Aaligned%2520channels%2520such%2520as%2520slides%252C%2520subtitles%252C%2520speech%252C%2520and%2520human%2520talker.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520PaperTalker%252C%2520the%2520first%2520benchmark%2520of%2520101%250Aresearch%2520papers%2520paired%2520with%2520author-created%2520presentation%2520videos%252C%2520slides%252C%2520and%250Aspeaker%2520metadata.%2520We%2520further%2520design%2520four%2520tailored%2520evaluation%2520metrics--Meta%250ASimilarity%252C%2520PresentArena%252C%2520PresentQuiz%252C%2520and%2520IP%2520Memory--to%2520measure%2520how%2520videos%250Aconvey%2520the%2520paper%2527s%2520information%2520to%2520the%2520audience.%2520Building%2520on%2520this%2520foundation%252C%2520we%250Apropose%2520PaperTalker%252C%2520the%2520first%2520multi-agent%2520framework%2520for%2520academic%2520presentation%250Avideo%2520generation.%2520It%2520integrates%2520slide%2520generation%2520with%2520effective%2520layout%250Arefinement%2520by%2520a%2520novel%2520effective%2520tree%2520search%2520visual%2520choice%252C%2520cursor%2520grounding%252C%250Asubtitling%252C%2520speech%2520synthesis%252C%2520and%2520talking-head%2520rendering%252C%2520while%2520parallelizing%250Aslide-wise%2520generation%2520for%2520efficiency.%2520Experiments%2520on%2520Paper2Video%2520demonstrate%250Athat%2520the%2520presentation%2520videos%2520produced%2520by%2520our%2520approach%2520are%2520more%2520faithful%2520and%250Ainformative%2520than%2520existing%2520baselines%252C%2520establishing%2520a%2520practical%2520step%2520toward%250Aautomated%2520and%2520ready-to-use%2520academic%2520video%2520generation.%2520Our%2520dataset%252C%2520agent%252C%2520and%250Acode%2520are%2520available%2520at%2520https%253A//github.com/showlab/Paper2Video.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Paper2Video%3A%20Automatic%20Video%20Generation%20from%20Scientific%20Papers&entry.906535625=Zeyu%20Zhu%20and%20Kevin%20Qinghong%20Lin%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Academic%20presentation%20videos%20have%20become%20an%20essential%20medium%20for%20research%0Acommunication%2C%20yet%20producing%20them%20remains%20highly%20labor-intensive%2C%20often%0Arequiring%20hours%20of%20slide%20design%2C%20recording%2C%20and%20editing%20for%20a%20short%202%20to%2010%0Aminutes%20video.%20Unlike%20natural%20video%2C%20presentation%20video%20generation%20involves%0Adistinctive%20challenges%3A%20inputs%20from%20research%20papers%2C%20dense%20multi-modal%0Ainformation%20%28text%2C%20figures%2C%20tables%29%2C%20and%20the%20need%20to%20coordinate%20multiple%0Aaligned%20channels%20such%20as%20slides%2C%20subtitles%2C%20speech%2C%20and%20human%20talker.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20PaperTalker%2C%20the%20first%20benchmark%20of%20101%0Aresearch%20papers%20paired%20with%20author-created%20presentation%20videos%2C%20slides%2C%20and%0Aspeaker%20metadata.%20We%20further%20design%20four%20tailored%20evaluation%20metrics--Meta%0ASimilarity%2C%20PresentArena%2C%20PresentQuiz%2C%20and%20IP%20Memory--to%20measure%20how%20videos%0Aconvey%20the%20paper%27s%20information%20to%20the%20audience.%20Building%20on%20this%20foundation%2C%20we%0Apropose%20PaperTalker%2C%20the%20first%20multi-agent%20framework%20for%20academic%20presentation%0Avideo%20generation.%20It%20integrates%20slide%20generation%20with%20effective%20layout%0Arefinement%20by%20a%20novel%20effective%20tree%20search%20visual%20choice%2C%20cursor%20grounding%2C%0Asubtitling%2C%20speech%20synthesis%2C%20and%20talking-head%20rendering%2C%20while%20parallelizing%0Aslide-wise%20generation%20for%20efficiency.%20Experiments%20on%20Paper2Video%20demonstrate%0Athat%20the%20presentation%20videos%20produced%20by%20our%20approach%20are%20more%20faithful%20and%0Ainformative%20than%20existing%20baselines%2C%20establishing%20a%20practical%20step%20toward%0Aautomated%20and%20ready-to-use%20academic%20video%20generation.%20Our%20dataset%2C%20agent%2C%20and%0Acode%20are%20available%20at%20https%3A//github.com/showlab/Paper2Video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05096v1&entry.124074799=Read"},
{"title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning", "author": "Kun Xiang and Heng Li and Terry Jingchen Zhang and Yinya Huang and Zirong Liu and Peixin Qu and Jixi He and Jiaqi Chen and Yu-Jie Yuan and Jianhua Han and Hang Xu and Hanhui Li and Mrinmaya Sachan and Xiaodan Liang", "abstract": "  We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts.\n", "link": "http://arxiv.org/abs/2505.19099v8", "date": "2025-10-06", "relevancy": 2.2771, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeePhys%3A%20Does%20Seeing%20Help%20Thinking%3F%20--%20Benchmarking%20Vision-Based%20Physics%0A%20%20Reasoning&body=Title%3A%20SeePhys%3A%20Does%20Seeing%20Help%20Thinking%3F%20--%20Benchmarking%20Vision-Based%20Physics%0A%20%20Reasoning%0AAuthor%3A%20Kun%20Xiang%20and%20Heng%20Li%20and%20Terry%20Jingchen%20Zhang%20and%20Yinya%20Huang%20and%20Zirong%20Liu%20and%20Peixin%20Qu%20and%20Jixi%20He%20and%20Jiaqi%20Chen%20and%20Yu-Jie%20Yuan%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Hanhui%20Li%20and%20Mrinmaya%20Sachan%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20We%20present%20SeePhys%2C%20a%20large-scale%20multimodal%20benchmark%20for%20LLM%20reasoning%0Agrounded%20in%20physics%20questions%20ranging%20from%20middle%20school%20to%20PhD%20qualifying%0Aexams.%20The%20benchmark%20covers%207%20fundamental%20domains%20spanning%20the%20physics%0Adiscipline%2C%20incorporating%2021%20categories%20of%20highly%20heterogeneous%20diagrams.%20In%0Acontrast%20to%20prior%20works%20where%20visual%20elements%20mainly%20serve%20auxiliary%20purposes%2C%0Aour%20benchmark%20features%20a%20substantial%20proportion%20of%20vision-essential%20problems%0A%2875%25%29%20that%20mandate%20visual%20information%20extraction%20for%20correct%20solutions.%20Through%0Aextensive%20evaluation%2C%20we%20observe%20that%20even%20the%20most%20advanced%20visual%20reasoning%0Amodels%20%28e.g.%2C%20Gemini-2.5-pro%20and%20o4-mini%29%20achieve%20sub-60%25%20accuracy%20on%20our%0Abenchmark.%20These%20results%20reveal%20fundamental%20challenges%20in%20current%20large%0Alanguage%20models%27%20visual%20understanding%20capabilities%2C%20particularly%20in%3A%20%28i%29%0Aestablishing%20rigorous%20coupling%20between%20diagram%20interpretation%20and%20physics%0Areasoning%2C%20and%20%28ii%29%20overcoming%20their%20persistent%20reliance%20on%20textual%20cues%20as%0Acognitive%20shortcuts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19099v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeePhys%253A%2520Does%2520Seeing%2520Help%2520Thinking%253F%2520--%2520Benchmarking%2520Vision-Based%2520Physics%250A%2520%2520Reasoning%26entry.906535625%3DKun%2520Xiang%2520and%2520Heng%2520Li%2520and%2520Terry%2520Jingchen%2520Zhang%2520and%2520Yinya%2520Huang%2520and%2520Zirong%2520Liu%2520and%2520Peixin%2520Qu%2520and%2520Jixi%2520He%2520and%2520Jiaqi%2520Chen%2520and%2520Yu-Jie%2520Yuan%2520and%2520Jianhua%2520Han%2520and%2520Hang%2520Xu%2520and%2520Hanhui%2520Li%2520and%2520Mrinmaya%2520Sachan%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520We%2520present%2520SeePhys%252C%2520a%2520large-scale%2520multimodal%2520benchmark%2520for%2520LLM%2520reasoning%250Agrounded%2520in%2520physics%2520questions%2520ranging%2520from%2520middle%2520school%2520to%2520PhD%2520qualifying%250Aexams.%2520The%2520benchmark%2520covers%25207%2520fundamental%2520domains%2520spanning%2520the%2520physics%250Adiscipline%252C%2520incorporating%252021%2520categories%2520of%2520highly%2520heterogeneous%2520diagrams.%2520In%250Acontrast%2520to%2520prior%2520works%2520where%2520visual%2520elements%2520mainly%2520serve%2520auxiliary%2520purposes%252C%250Aour%2520benchmark%2520features%2520a%2520substantial%2520proportion%2520of%2520vision-essential%2520problems%250A%252875%2525%2529%2520that%2520mandate%2520visual%2520information%2520extraction%2520for%2520correct%2520solutions.%2520Through%250Aextensive%2520evaluation%252C%2520we%2520observe%2520that%2520even%2520the%2520most%2520advanced%2520visual%2520reasoning%250Amodels%2520%2528e.g.%252C%2520Gemini-2.5-pro%2520and%2520o4-mini%2529%2520achieve%2520sub-60%2525%2520accuracy%2520on%2520our%250Abenchmark.%2520These%2520results%2520reveal%2520fundamental%2520challenges%2520in%2520current%2520large%250Alanguage%2520models%2527%2520visual%2520understanding%2520capabilities%252C%2520particularly%2520in%253A%2520%2528i%2529%250Aestablishing%2520rigorous%2520coupling%2520between%2520diagram%2520interpretation%2520and%2520physics%250Areasoning%252C%2520and%2520%2528ii%2529%2520overcoming%2520their%2520persistent%2520reliance%2520on%2520textual%2520cues%2520as%250Acognitive%2520shortcuts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19099v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeePhys%3A%20Does%20Seeing%20Help%20Thinking%3F%20--%20Benchmarking%20Vision-Based%20Physics%0A%20%20Reasoning&entry.906535625=Kun%20Xiang%20and%20Heng%20Li%20and%20Terry%20Jingchen%20Zhang%20and%20Yinya%20Huang%20and%20Zirong%20Liu%20and%20Peixin%20Qu%20and%20Jixi%20He%20and%20Jiaqi%20Chen%20and%20Yu-Jie%20Yuan%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Hanhui%20Li%20and%20Mrinmaya%20Sachan%20and%20Xiaodan%20Liang&entry.1292438233=%20%20We%20present%20SeePhys%2C%20a%20large-scale%20multimodal%20benchmark%20for%20LLM%20reasoning%0Agrounded%20in%20physics%20questions%20ranging%20from%20middle%20school%20to%20PhD%20qualifying%0Aexams.%20The%20benchmark%20covers%207%20fundamental%20domains%20spanning%20the%20physics%0Adiscipline%2C%20incorporating%2021%20categories%20of%20highly%20heterogeneous%20diagrams.%20In%0Acontrast%20to%20prior%20works%20where%20visual%20elements%20mainly%20serve%20auxiliary%20purposes%2C%0Aour%20benchmark%20features%20a%20substantial%20proportion%20of%20vision-essential%20problems%0A%2875%25%29%20that%20mandate%20visual%20information%20extraction%20for%20correct%20solutions.%20Through%0Aextensive%20evaluation%2C%20we%20observe%20that%20even%20the%20most%20advanced%20visual%20reasoning%0Amodels%20%28e.g.%2C%20Gemini-2.5-pro%20and%20o4-mini%29%20achieve%20sub-60%25%20accuracy%20on%20our%0Abenchmark.%20These%20results%20reveal%20fundamental%20challenges%20in%20current%20large%0Alanguage%20models%27%20visual%20understanding%20capabilities%2C%20particularly%20in%3A%20%28i%29%0Aestablishing%20rigorous%20coupling%20between%20diagram%20interpretation%20and%20physics%0Areasoning%2C%20and%20%28ii%29%20overcoming%20their%20persistent%20reliance%20on%20textual%20cues%20as%0Acognitive%20shortcuts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19099v8&entry.124074799=Read"},
{"title": "The Telephone Game: Evaluating Semantic Drift in Unified Models", "author": "Sabbir Mollah and Rohit Gupta and Sirnam Swetha and Qingyang Liu and Ahnaf Munir and Mubarak Shah", "abstract": "  Employing a single, unified model (UM) for both visual understanding\n(image-to-text: I2T) and visual generation (text-to-image: T2I) has opened a\nnew direction in Visual Language Model (VLM) research. While UMs can also\nsupport broader unimodal tasks (e.g., text-to-text, image-to-image), we focus\non the core cross-modal pair T2I and I2T. Existing evaluation benchmarks\nconsider these capabilities in isolation: FID and GenEval for T2I, and\nbenchmarks such as MME, MMBench for I2T. These isolated single-pass metrics do\nnot reveal cross-consistency: whether a model that \"understands\" a concept can\nalso \"render\" it, nor whether semantic meaning is preserved when cycling\nbetween image and text modalities. To address this, we introduce the Semantic\nDrift Protocol (SDP) for Unified Models, a cyclic evaluation protocol that\nalternates I2T and T2I over multiple generations to quantify semantic drift. We\npropose two metrics: (i) Mean Cumulative Drift (MCD), an embedding-based\nmeasure of overall semantic drift; and (ii) Multi-Generation GenEval (MGG), an\nobject-level compliance score extending GenEval. To assess generalization\nbeyond COCO dataset, which is widely used in training; we create a new\nbenchmark Nocaps+Docci400, sampled from NoCaps and DOCCI and evaluated on seven\nrecent models. SDP reveals substantial variation in cross-modal stability: some\nmodels like BAGEL maintain semantic meaning over many alternations, whereas\nothers like VILA-U drift quickly despite strong single-pass scores. Our results\nhighlight SDP as a necessary complement to standard I2T and T2I evaluations.\nCode is available at\nhttps://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models\n", "link": "http://arxiv.org/abs/2509.04438v2", "date": "2025-10-06", "relevancy": 2.2712, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5683}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5683}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Telephone%20Game%3A%20Evaluating%20Semantic%20Drift%20in%20Unified%20Models&body=Title%3A%20The%20Telephone%20Game%3A%20Evaluating%20Semantic%20Drift%20in%20Unified%20Models%0AAuthor%3A%20Sabbir%20Mollah%20and%20Rohit%20Gupta%20and%20Sirnam%20Swetha%20and%20Qingyang%20Liu%20and%20Ahnaf%20Munir%20and%20Mubarak%20Shah%0AAbstract%3A%20%20%20Employing%20a%20single%2C%20unified%20model%20%28UM%29%20for%20both%20visual%20understanding%0A%28image-to-text%3A%20I2T%29%20and%20visual%20generation%20%28text-to-image%3A%20T2I%29%20has%20opened%20a%0Anew%20direction%20in%20Visual%20Language%20Model%20%28VLM%29%20research.%20While%20UMs%20can%20also%0Asupport%20broader%20unimodal%20tasks%20%28e.g.%2C%20text-to-text%2C%20image-to-image%29%2C%20we%20focus%0Aon%20the%20core%20cross-modal%20pair%20T2I%20and%20I2T.%20Existing%20evaluation%20benchmarks%0Aconsider%20these%20capabilities%20in%20isolation%3A%20FID%20and%20GenEval%20for%20T2I%2C%20and%0Abenchmarks%20such%20as%20MME%2C%20MMBench%20for%20I2T.%20These%20isolated%20single-pass%20metrics%20do%0Anot%20reveal%20cross-consistency%3A%20whether%20a%20model%20that%20%22understands%22%20a%20concept%20can%0Aalso%20%22render%22%20it%2C%20nor%20whether%20semantic%20meaning%20is%20preserved%20when%20cycling%0Abetween%20image%20and%20text%20modalities.%20To%20address%20this%2C%20we%20introduce%20the%20Semantic%0ADrift%20Protocol%20%28SDP%29%20for%20Unified%20Models%2C%20a%20cyclic%20evaluation%20protocol%20that%0Aalternates%20I2T%20and%20T2I%20over%20multiple%20generations%20to%20quantify%20semantic%20drift.%20We%0Apropose%20two%20metrics%3A%20%28i%29%20Mean%20Cumulative%20Drift%20%28MCD%29%2C%20an%20embedding-based%0Ameasure%20of%20overall%20semantic%20drift%3B%20and%20%28ii%29%20Multi-Generation%20GenEval%20%28MGG%29%2C%20an%0Aobject-level%20compliance%20score%20extending%20GenEval.%20To%20assess%20generalization%0Abeyond%20COCO%20dataset%2C%20which%20is%20widely%20used%20in%20training%3B%20we%20create%20a%20new%0Abenchmark%20Nocaps%2BDocci400%2C%20sampled%20from%20NoCaps%20and%20DOCCI%20and%20evaluated%20on%20seven%0Arecent%20models.%20SDP%20reveals%20substantial%20variation%20in%20cross-modal%20stability%3A%20some%0Amodels%20like%20BAGEL%20maintain%20semantic%20meaning%20over%20many%20alternations%2C%20whereas%0Aothers%20like%20VILA-U%20drift%20quickly%20despite%20strong%20single-pass%20scores.%20Our%20results%0Ahighlight%20SDP%20as%20a%20necessary%20complement%20to%20standard%20I2T%20and%20T2I%20evaluations.%0ACode%20is%20available%20at%0Ahttps%3A//github.com/mollahsabbir/Semantic-Drift-in-Unified-Models%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Telephone%2520Game%253A%2520Evaluating%2520Semantic%2520Drift%2520in%2520Unified%2520Models%26entry.906535625%3DSabbir%2520Mollah%2520and%2520Rohit%2520Gupta%2520and%2520Sirnam%2520Swetha%2520and%2520Qingyang%2520Liu%2520and%2520Ahnaf%2520Munir%2520and%2520Mubarak%2520Shah%26entry.1292438233%3D%2520%2520Employing%2520a%2520single%252C%2520unified%2520model%2520%2528UM%2529%2520for%2520both%2520visual%2520understanding%250A%2528image-to-text%253A%2520I2T%2529%2520and%2520visual%2520generation%2520%2528text-to-image%253A%2520T2I%2529%2520has%2520opened%2520a%250Anew%2520direction%2520in%2520Visual%2520Language%2520Model%2520%2528VLM%2529%2520research.%2520While%2520UMs%2520can%2520also%250Asupport%2520broader%2520unimodal%2520tasks%2520%2528e.g.%252C%2520text-to-text%252C%2520image-to-image%2529%252C%2520we%2520focus%250Aon%2520the%2520core%2520cross-modal%2520pair%2520T2I%2520and%2520I2T.%2520Existing%2520evaluation%2520benchmarks%250Aconsider%2520these%2520capabilities%2520in%2520isolation%253A%2520FID%2520and%2520GenEval%2520for%2520T2I%252C%2520and%250Abenchmarks%2520such%2520as%2520MME%252C%2520MMBench%2520for%2520I2T.%2520These%2520isolated%2520single-pass%2520metrics%2520do%250Anot%2520reveal%2520cross-consistency%253A%2520whether%2520a%2520model%2520that%2520%2522understands%2522%2520a%2520concept%2520can%250Aalso%2520%2522render%2522%2520it%252C%2520nor%2520whether%2520semantic%2520meaning%2520is%2520preserved%2520when%2520cycling%250Abetween%2520image%2520and%2520text%2520modalities.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520Semantic%250ADrift%2520Protocol%2520%2528SDP%2529%2520for%2520Unified%2520Models%252C%2520a%2520cyclic%2520evaluation%2520protocol%2520that%250Aalternates%2520I2T%2520and%2520T2I%2520over%2520multiple%2520generations%2520to%2520quantify%2520semantic%2520drift.%2520We%250Apropose%2520two%2520metrics%253A%2520%2528i%2529%2520Mean%2520Cumulative%2520Drift%2520%2528MCD%2529%252C%2520an%2520embedding-based%250Ameasure%2520of%2520overall%2520semantic%2520drift%253B%2520and%2520%2528ii%2529%2520Multi-Generation%2520GenEval%2520%2528MGG%2529%252C%2520an%250Aobject-level%2520compliance%2520score%2520extending%2520GenEval.%2520To%2520assess%2520generalization%250Abeyond%2520COCO%2520dataset%252C%2520which%2520is%2520widely%2520used%2520in%2520training%253B%2520we%2520create%2520a%2520new%250Abenchmark%2520Nocaps%252BDocci400%252C%2520sampled%2520from%2520NoCaps%2520and%2520DOCCI%2520and%2520evaluated%2520on%2520seven%250Arecent%2520models.%2520SDP%2520reveals%2520substantial%2520variation%2520in%2520cross-modal%2520stability%253A%2520some%250Amodels%2520like%2520BAGEL%2520maintain%2520semantic%2520meaning%2520over%2520many%2520alternations%252C%2520whereas%250Aothers%2520like%2520VILA-U%2520drift%2520quickly%2520despite%2520strong%2520single-pass%2520scores.%2520Our%2520results%250Ahighlight%2520SDP%2520as%2520a%2520necessary%2520complement%2520to%2520standard%2520I2T%2520and%2520T2I%2520evaluations.%250ACode%2520is%2520available%2520at%250Ahttps%253A//github.com/mollahsabbir/Semantic-Drift-in-Unified-Models%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Telephone%20Game%3A%20Evaluating%20Semantic%20Drift%20in%20Unified%20Models&entry.906535625=Sabbir%20Mollah%20and%20Rohit%20Gupta%20and%20Sirnam%20Swetha%20and%20Qingyang%20Liu%20and%20Ahnaf%20Munir%20and%20Mubarak%20Shah&entry.1292438233=%20%20Employing%20a%20single%2C%20unified%20model%20%28UM%29%20for%20both%20visual%20understanding%0A%28image-to-text%3A%20I2T%29%20and%20visual%20generation%20%28text-to-image%3A%20T2I%29%20has%20opened%20a%0Anew%20direction%20in%20Visual%20Language%20Model%20%28VLM%29%20research.%20While%20UMs%20can%20also%0Asupport%20broader%20unimodal%20tasks%20%28e.g.%2C%20text-to-text%2C%20image-to-image%29%2C%20we%20focus%0Aon%20the%20core%20cross-modal%20pair%20T2I%20and%20I2T.%20Existing%20evaluation%20benchmarks%0Aconsider%20these%20capabilities%20in%20isolation%3A%20FID%20and%20GenEval%20for%20T2I%2C%20and%0Abenchmarks%20such%20as%20MME%2C%20MMBench%20for%20I2T.%20These%20isolated%20single-pass%20metrics%20do%0Anot%20reveal%20cross-consistency%3A%20whether%20a%20model%20that%20%22understands%22%20a%20concept%20can%0Aalso%20%22render%22%20it%2C%20nor%20whether%20semantic%20meaning%20is%20preserved%20when%20cycling%0Abetween%20image%20and%20text%20modalities.%20To%20address%20this%2C%20we%20introduce%20the%20Semantic%0ADrift%20Protocol%20%28SDP%29%20for%20Unified%20Models%2C%20a%20cyclic%20evaluation%20protocol%20that%0Aalternates%20I2T%20and%20T2I%20over%20multiple%20generations%20to%20quantify%20semantic%20drift.%20We%0Apropose%20two%20metrics%3A%20%28i%29%20Mean%20Cumulative%20Drift%20%28MCD%29%2C%20an%20embedding-based%0Ameasure%20of%20overall%20semantic%20drift%3B%20and%20%28ii%29%20Multi-Generation%20GenEval%20%28MGG%29%2C%20an%0Aobject-level%20compliance%20score%20extending%20GenEval.%20To%20assess%20generalization%0Abeyond%20COCO%20dataset%2C%20which%20is%20widely%20used%20in%20training%3B%20we%20create%20a%20new%0Abenchmark%20Nocaps%2BDocci400%2C%20sampled%20from%20NoCaps%20and%20DOCCI%20and%20evaluated%20on%20seven%0Arecent%20models.%20SDP%20reveals%20substantial%20variation%20in%20cross-modal%20stability%3A%20some%0Amodels%20like%20BAGEL%20maintain%20semantic%20meaning%20over%20many%20alternations%2C%20whereas%0Aothers%20like%20VILA-U%20drift%20quickly%20despite%20strong%20single-pass%20scores.%20Our%20results%0Ahighlight%20SDP%20as%20a%20necessary%20complement%20to%20standard%20I2T%20and%20T2I%20evaluations.%0ACode%20is%20available%20at%0Ahttps%3A//github.com/mollahsabbir/Semantic-Drift-in-Unified-Models%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04438v2&entry.124074799=Read"},
{"title": "ERDE: Entropy-Regularized Distillation for Early-exit", "author": "Martial Guidez and Stefan Duffner and Yannick Alpou and Oscar R\u00f6th and Christophe Garcia", "abstract": "  Although deep neural networks and in particular Convolutional Neural Networks\nhave demonstrated state-of-the-art performance in image classification with\nrelatively high efficiency, they still exhibit high computational costs, often\nrendering them impractical for real-time and edge applications. Therefore, a\nmultitude of compression techniques have been developed to reduce these costs\nwhile maintaining accuracy. In addition, dynamic architectures have been\nintroduced to modulate the level of compression at execution time, which is a\ndesirable property in many resource-limited application scenarios. The proposed\nmethod effectively integrates two well-established optimization techniques:\nearly exits and knowledge distillation, where a reduced student early-exit\nmodel is trained from a more complex teacher early-exit model. The primary\ncontribution of this research lies in the approach for training the student\nearly-exit model. In comparison to the conventional Knowledge Distillation\nloss, our approach incorporates a new entropy-based loss for images where the\nteacher's classification was incorrect. The proposed method optimizes the\ntrade-off between accuracy and efficiency, thereby achieving significant\nreductions in computational complexity without compromising classification\nperformance. The validity of this approach is substantiated by experimental\nresults on image classification datasets CIFAR10, CIFAR100 and SVHN, which\nfurther opens new research perspectives for Knowledge Distillation in other\ncontexts.\n", "link": "http://arxiv.org/abs/2510.04856v1", "date": "2025-10-06", "relevancy": 2.2674, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5822}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5701}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ERDE%3A%20Entropy-Regularized%20Distillation%20for%20Early-exit&body=Title%3A%20ERDE%3A%20Entropy-Regularized%20Distillation%20for%20Early-exit%0AAuthor%3A%20Martial%20Guidez%20and%20Stefan%20Duffner%20and%20Yannick%20Alpou%20and%20Oscar%20R%C3%B6th%20and%20Christophe%20Garcia%0AAbstract%3A%20%20%20Although%20deep%20neural%20networks%20and%20in%20particular%20Convolutional%20Neural%20Networks%0Ahave%20demonstrated%20state-of-the-art%20performance%20in%20image%20classification%20with%0Arelatively%20high%20efficiency%2C%20they%20still%20exhibit%20high%20computational%20costs%2C%20often%0Arendering%20them%20impractical%20for%20real-time%20and%20edge%20applications.%20Therefore%2C%20a%0Amultitude%20of%20compression%20techniques%20have%20been%20developed%20to%20reduce%20these%20costs%0Awhile%20maintaining%20accuracy.%20In%20addition%2C%20dynamic%20architectures%20have%20been%0Aintroduced%20to%20modulate%20the%20level%20of%20compression%20at%20execution%20time%2C%20which%20is%20a%0Adesirable%20property%20in%20many%20resource-limited%20application%20scenarios.%20The%20proposed%0Amethod%20effectively%20integrates%20two%20well-established%20optimization%20techniques%3A%0Aearly%20exits%20and%20knowledge%20distillation%2C%20where%20a%20reduced%20student%20early-exit%0Amodel%20is%20trained%20from%20a%20more%20complex%20teacher%20early-exit%20model.%20The%20primary%0Acontribution%20of%20this%20research%20lies%20in%20the%20approach%20for%20training%20the%20student%0Aearly-exit%20model.%20In%20comparison%20to%20the%20conventional%20Knowledge%20Distillation%0Aloss%2C%20our%20approach%20incorporates%20a%20new%20entropy-based%20loss%20for%20images%20where%20the%0Ateacher%27s%20classification%20was%20incorrect.%20The%20proposed%20method%20optimizes%20the%0Atrade-off%20between%20accuracy%20and%20efficiency%2C%20thereby%20achieving%20significant%0Areductions%20in%20computational%20complexity%20without%20compromising%20classification%0Aperformance.%20The%20validity%20of%20this%20approach%20is%20substantiated%20by%20experimental%0Aresults%20on%20image%20classification%20datasets%20CIFAR10%2C%20CIFAR100%20and%20SVHN%2C%20which%0Afurther%20opens%20new%20research%20perspectives%20for%20Knowledge%20Distillation%20in%20other%0Acontexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DERDE%253A%2520Entropy-Regularized%2520Distillation%2520for%2520Early-exit%26entry.906535625%3DMartial%2520Guidez%2520and%2520Stefan%2520Duffner%2520and%2520Yannick%2520Alpou%2520and%2520Oscar%2520R%25C3%25B6th%2520and%2520Christophe%2520Garcia%26entry.1292438233%3D%2520%2520Although%2520deep%2520neural%2520networks%2520and%2520in%2520particular%2520Convolutional%2520Neural%2520Networks%250Ahave%2520demonstrated%2520state-of-the-art%2520performance%2520in%2520image%2520classification%2520with%250Arelatively%2520high%2520efficiency%252C%2520they%2520still%2520exhibit%2520high%2520computational%2520costs%252C%2520often%250Arendering%2520them%2520impractical%2520for%2520real-time%2520and%2520edge%2520applications.%2520Therefore%252C%2520a%250Amultitude%2520of%2520compression%2520techniques%2520have%2520been%2520developed%2520to%2520reduce%2520these%2520costs%250Awhile%2520maintaining%2520accuracy.%2520In%2520addition%252C%2520dynamic%2520architectures%2520have%2520been%250Aintroduced%2520to%2520modulate%2520the%2520level%2520of%2520compression%2520at%2520execution%2520time%252C%2520which%2520is%2520a%250Adesirable%2520property%2520in%2520many%2520resource-limited%2520application%2520scenarios.%2520The%2520proposed%250Amethod%2520effectively%2520integrates%2520two%2520well-established%2520optimization%2520techniques%253A%250Aearly%2520exits%2520and%2520knowledge%2520distillation%252C%2520where%2520a%2520reduced%2520student%2520early-exit%250Amodel%2520is%2520trained%2520from%2520a%2520more%2520complex%2520teacher%2520early-exit%2520model.%2520The%2520primary%250Acontribution%2520of%2520this%2520research%2520lies%2520in%2520the%2520approach%2520for%2520training%2520the%2520student%250Aearly-exit%2520model.%2520In%2520comparison%2520to%2520the%2520conventional%2520Knowledge%2520Distillation%250Aloss%252C%2520our%2520approach%2520incorporates%2520a%2520new%2520entropy-based%2520loss%2520for%2520images%2520where%2520the%250Ateacher%2527s%2520classification%2520was%2520incorrect.%2520The%2520proposed%2520method%2520optimizes%2520the%250Atrade-off%2520between%2520accuracy%2520and%2520efficiency%252C%2520thereby%2520achieving%2520significant%250Areductions%2520in%2520computational%2520complexity%2520without%2520compromising%2520classification%250Aperformance.%2520The%2520validity%2520of%2520this%2520approach%2520is%2520substantiated%2520by%2520experimental%250Aresults%2520on%2520image%2520classification%2520datasets%2520CIFAR10%252C%2520CIFAR100%2520and%2520SVHN%252C%2520which%250Afurther%2520opens%2520new%2520research%2520perspectives%2520for%2520Knowledge%2520Distillation%2520in%2520other%250Acontexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ERDE%3A%20Entropy-Regularized%20Distillation%20for%20Early-exit&entry.906535625=Martial%20Guidez%20and%20Stefan%20Duffner%20and%20Yannick%20Alpou%20and%20Oscar%20R%C3%B6th%20and%20Christophe%20Garcia&entry.1292438233=%20%20Although%20deep%20neural%20networks%20and%20in%20particular%20Convolutional%20Neural%20Networks%0Ahave%20demonstrated%20state-of-the-art%20performance%20in%20image%20classification%20with%0Arelatively%20high%20efficiency%2C%20they%20still%20exhibit%20high%20computational%20costs%2C%20often%0Arendering%20them%20impractical%20for%20real-time%20and%20edge%20applications.%20Therefore%2C%20a%0Amultitude%20of%20compression%20techniques%20have%20been%20developed%20to%20reduce%20these%20costs%0Awhile%20maintaining%20accuracy.%20In%20addition%2C%20dynamic%20architectures%20have%20been%0Aintroduced%20to%20modulate%20the%20level%20of%20compression%20at%20execution%20time%2C%20which%20is%20a%0Adesirable%20property%20in%20many%20resource-limited%20application%20scenarios.%20The%20proposed%0Amethod%20effectively%20integrates%20two%20well-established%20optimization%20techniques%3A%0Aearly%20exits%20and%20knowledge%20distillation%2C%20where%20a%20reduced%20student%20early-exit%0Amodel%20is%20trained%20from%20a%20more%20complex%20teacher%20early-exit%20model.%20The%20primary%0Acontribution%20of%20this%20research%20lies%20in%20the%20approach%20for%20training%20the%20student%0Aearly-exit%20model.%20In%20comparison%20to%20the%20conventional%20Knowledge%20Distillation%0Aloss%2C%20our%20approach%20incorporates%20a%20new%20entropy-based%20loss%20for%20images%20where%20the%0Ateacher%27s%20classification%20was%20incorrect.%20The%20proposed%20method%20optimizes%20the%0Atrade-off%20between%20accuracy%20and%20efficiency%2C%20thereby%20achieving%20significant%0Areductions%20in%20computational%20complexity%20without%20compromising%20classification%0Aperformance.%20The%20validity%20of%20this%20approach%20is%20substantiated%20by%20experimental%0Aresults%20on%20image%20classification%20datasets%20CIFAR10%2C%20CIFAR100%20and%20SVHN%2C%20which%0Afurther%20opens%20new%20research%20perspectives%20for%20Knowledge%20Distillation%20in%20other%0Acontexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04856v1&entry.124074799=Read"},
{"title": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment", "author": "Davood Rafiei and Morgan Lindsay Heisler and Weiwei Zhang and Mohammadreza Pourreza and Yong Zhang", "abstract": "  Supervised Fine-Tuning (SFT) is an effective method for adapting Large\nLanguage Models (LLMs) on downstream tasks. However, variability in training\ndata can hinder a model's ability to generalize across domains. This paper\nstudies the problem of dataset alignment for Natural Language to SQL (NL2SQL or\ntext to SQL), examining how well SFT training data matches the structural\ncharacteristics of target queries and how this alignment impacts model\nperformance. We hypothesize that alignment can be accurately estimated by\ncomparing the distributions of structural SQL features across the training set,\ntarget data, and the model's predictions prior to SFT. Through comprehensive\nexperiments on three large cross-domain NL2SQL benchmarks and multiple model\nfamilies, we show that structural alignment is a strong predictor of\nfine-tuning success. When alignment is high, SFT yields substantial gains in\naccuracy and SQL generation quality; when alignment is low, improvements are\nmarginal or absent. These findings highlight the importance of alignment-aware\ndata selection for effective fine-tuning and generalization in NL2SQL tasks.\n", "link": "http://arxiv.org/abs/2510.04919v1", "date": "2025-10-06", "relevancy": 2.266, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20Align%20with%20My%20Task%3F%20Evaluating%20Text-to-SQL%20via%20Dataset%20Alignment&body=Title%3A%20Do%20LLMs%20Align%20with%20My%20Task%3F%20Evaluating%20Text-to-SQL%20via%20Dataset%20Alignment%0AAuthor%3A%20Davood%20Rafiei%20and%20Morgan%20Lindsay%20Heisler%20and%20Weiwei%20Zhang%20and%20Mohammadreza%20Pourreza%20and%20Yong%20Zhang%0AAbstract%3A%20%20%20Supervised%20Fine-Tuning%20%28SFT%29%20is%20an%20effective%20method%20for%20adapting%20Large%0ALanguage%20Models%20%28LLMs%29%20on%20downstream%20tasks.%20However%2C%20variability%20in%20training%0Adata%20can%20hinder%20a%20model%27s%20ability%20to%20generalize%20across%20domains.%20This%20paper%0Astudies%20the%20problem%20of%20dataset%20alignment%20for%20Natural%20Language%20to%20SQL%20%28NL2SQL%20or%0Atext%20to%20SQL%29%2C%20examining%20how%20well%20SFT%20training%20data%20matches%20the%20structural%0Acharacteristics%20of%20target%20queries%20and%20how%20this%20alignment%20impacts%20model%0Aperformance.%20We%20hypothesize%20that%20alignment%20can%20be%20accurately%20estimated%20by%0Acomparing%20the%20distributions%20of%20structural%20SQL%20features%20across%20the%20training%20set%2C%0Atarget%20data%2C%20and%20the%20model%27s%20predictions%20prior%20to%20SFT.%20Through%20comprehensive%0Aexperiments%20on%20three%20large%20cross-domain%20NL2SQL%20benchmarks%20and%20multiple%20model%0Afamilies%2C%20we%20show%20that%20structural%20alignment%20is%20a%20strong%20predictor%20of%0Afine-tuning%20success.%20When%20alignment%20is%20high%2C%20SFT%20yields%20substantial%20gains%20in%0Aaccuracy%20and%20SQL%20generation%20quality%3B%20when%20alignment%20is%20low%2C%20improvements%20are%0Amarginal%20or%20absent.%20These%20findings%20highlight%20the%20importance%20of%20alignment-aware%0Adata%20selection%20for%20effective%20fine-tuning%20and%20generalization%20in%20NL2SQL%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520Align%2520with%2520My%2520Task%253F%2520Evaluating%2520Text-to-SQL%2520via%2520Dataset%2520Alignment%26entry.906535625%3DDavood%2520Rafiei%2520and%2520Morgan%2520Lindsay%2520Heisler%2520and%2520Weiwei%2520Zhang%2520and%2520Mohammadreza%2520Pourreza%2520and%2520Yong%2520Zhang%26entry.1292438233%3D%2520%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520is%2520an%2520effective%2520method%2520for%2520adapting%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520on%2520downstream%2520tasks.%2520However%252C%2520variability%2520in%2520training%250Adata%2520can%2520hinder%2520a%2520model%2527s%2520ability%2520to%2520generalize%2520across%2520domains.%2520This%2520paper%250Astudies%2520the%2520problem%2520of%2520dataset%2520alignment%2520for%2520Natural%2520Language%2520to%2520SQL%2520%2528NL2SQL%2520or%250Atext%2520to%2520SQL%2529%252C%2520examining%2520how%2520well%2520SFT%2520training%2520data%2520matches%2520the%2520structural%250Acharacteristics%2520of%2520target%2520queries%2520and%2520how%2520this%2520alignment%2520impacts%2520model%250Aperformance.%2520We%2520hypothesize%2520that%2520alignment%2520can%2520be%2520accurately%2520estimated%2520by%250Acomparing%2520the%2520distributions%2520of%2520structural%2520SQL%2520features%2520across%2520the%2520training%2520set%252C%250Atarget%2520data%252C%2520and%2520the%2520model%2527s%2520predictions%2520prior%2520to%2520SFT.%2520Through%2520comprehensive%250Aexperiments%2520on%2520three%2520large%2520cross-domain%2520NL2SQL%2520benchmarks%2520and%2520multiple%2520model%250Afamilies%252C%2520we%2520show%2520that%2520structural%2520alignment%2520is%2520a%2520strong%2520predictor%2520of%250Afine-tuning%2520success.%2520When%2520alignment%2520is%2520high%252C%2520SFT%2520yields%2520substantial%2520gains%2520in%250Aaccuracy%2520and%2520SQL%2520generation%2520quality%253B%2520when%2520alignment%2520is%2520low%252C%2520improvements%2520are%250Amarginal%2520or%2520absent.%2520These%2520findings%2520highlight%2520the%2520importance%2520of%2520alignment-aware%250Adata%2520selection%2520for%2520effective%2520fine-tuning%2520and%2520generalization%2520in%2520NL2SQL%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20Align%20with%20My%20Task%3F%20Evaluating%20Text-to-SQL%20via%20Dataset%20Alignment&entry.906535625=Davood%20Rafiei%20and%20Morgan%20Lindsay%20Heisler%20and%20Weiwei%20Zhang%20and%20Mohammadreza%20Pourreza%20and%20Yong%20Zhang&entry.1292438233=%20%20Supervised%20Fine-Tuning%20%28SFT%29%20is%20an%20effective%20method%20for%20adapting%20Large%0ALanguage%20Models%20%28LLMs%29%20on%20downstream%20tasks.%20However%2C%20variability%20in%20training%0Adata%20can%20hinder%20a%20model%27s%20ability%20to%20generalize%20across%20domains.%20This%20paper%0Astudies%20the%20problem%20of%20dataset%20alignment%20for%20Natural%20Language%20to%20SQL%20%28NL2SQL%20or%0Atext%20to%20SQL%29%2C%20examining%20how%20well%20SFT%20training%20data%20matches%20the%20structural%0Acharacteristics%20of%20target%20queries%20and%20how%20this%20alignment%20impacts%20model%0Aperformance.%20We%20hypothesize%20that%20alignment%20can%20be%20accurately%20estimated%20by%0Acomparing%20the%20distributions%20of%20structural%20SQL%20features%20across%20the%20training%20set%2C%0Atarget%20data%2C%20and%20the%20model%27s%20predictions%20prior%20to%20SFT.%20Through%20comprehensive%0Aexperiments%20on%20three%20large%20cross-domain%20NL2SQL%20benchmarks%20and%20multiple%20model%0Afamilies%2C%20we%20show%20that%20structural%20alignment%20is%20a%20strong%20predictor%20of%0Afine-tuning%20success.%20When%20alignment%20is%20high%2C%20SFT%20yields%20substantial%20gains%20in%0Aaccuracy%20and%20SQL%20generation%20quality%3B%20when%20alignment%20is%20low%2C%20improvements%20are%0Amarginal%20or%20absent.%20These%20findings%20highlight%20the%20importance%20of%20alignment-aware%0Adata%20selection%20for%20effective%20fine-tuning%20and%20generalization%20in%20NL2SQL%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04919v1&entry.124074799=Read"},
{"title": "A Clinical-grade Universal Foundation Model for Intraoperative Pathology", "author": "Zihan Zhao and Fengtao Zhou and Ronggang Li and Bing Chu and Xinke Zhang and Xueyi Zheng and Ke Zheng and Xiaobo Wen and Jiabo Ma and Yihui Wang and Jiewei Chen and Chengyou Zheng and Jiangyu Zhang and Yongqin Wen and Jiajia Meng and Ziqi Zeng and Xiaoqing Li and Jing Li and Dan Xie and Yaping Ye and Yu Wang and Hao Chen and Muyan Cai", "abstract": "  Intraoperative pathology is pivotal to precision surgery, yet its clinical\nimpact is constrained by diagnostic complexity and the limited availability of\nhigh-quality frozen-section data. While computational pathology has made\nsignificant strides, the lack of large-scale, prospective validation has\nimpeded its routine adoption in surgical workflows. Here, we introduce CRISP, a\nclinical-grade foundation model developed on over 100,000 frozen sections from\neight medical centers, specifically designed to provide Clinical-grade Robust\nIntraoperative Support for Pathology (CRISP). CRISP was comprehensively\nevaluated on more than 15,000 intraoperative slides across nearly 100\nretrospective diagnostic tasks, including benign-malignant discrimination, key\nintraoperative decision-making, and pan-cancer detection, etc. The model\ndemonstrated robust generalization across diverse institutions, tumor types,\nand anatomical sites-including previously unseen sites and rare cancers. In a\nprospective cohort of over 2,000 patients, CRISP sustained high diagnostic\naccuracy under real-world conditions, directly informing surgical decisions in\n92.6% of cases. Human-AI collaboration further reduced diagnostic workload by\n35%, avoided 105 ancillary tests and enhanced detection of micrometastases with\n87.5% accuracy. Together, these findings position CRISP as a clinical-grade\nparadigm for AI-driven intraoperative pathology, bridging computational\nadvances with surgical precision and accelerating the translation of artificial\nintelligence into routine clinical practice.\n", "link": "http://arxiv.org/abs/2510.04861v1", "date": "2025-10-06", "relevancy": 2.2657, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4563}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Clinical-grade%20Universal%20Foundation%20Model%20for%20Intraoperative%20Pathology&body=Title%3A%20A%20Clinical-grade%20Universal%20Foundation%20Model%20for%20Intraoperative%20Pathology%0AAuthor%3A%20Zihan%20Zhao%20and%20Fengtao%20Zhou%20and%20Ronggang%20Li%20and%20Bing%20Chu%20and%20Xinke%20Zhang%20and%20Xueyi%20Zheng%20and%20Ke%20Zheng%20and%20Xiaobo%20Wen%20and%20Jiabo%20Ma%20and%20Yihui%20Wang%20and%20Jiewei%20Chen%20and%20Chengyou%20Zheng%20and%20Jiangyu%20Zhang%20and%20Yongqin%20Wen%20and%20Jiajia%20Meng%20and%20Ziqi%20Zeng%20and%20Xiaoqing%20Li%20and%20Jing%20Li%20and%20Dan%20Xie%20and%20Yaping%20Ye%20and%20Yu%20Wang%20and%20Hao%20Chen%20and%20Muyan%20Cai%0AAbstract%3A%20%20%20Intraoperative%20pathology%20is%20pivotal%20to%20precision%20surgery%2C%20yet%20its%20clinical%0Aimpact%20is%20constrained%20by%20diagnostic%20complexity%20and%20the%20limited%20availability%20of%0Ahigh-quality%20frozen-section%20data.%20While%20computational%20pathology%20has%20made%0Asignificant%20strides%2C%20the%20lack%20of%20large-scale%2C%20prospective%20validation%20has%0Aimpeded%20its%20routine%20adoption%20in%20surgical%20workflows.%20Here%2C%20we%20introduce%20CRISP%2C%20a%0Aclinical-grade%20foundation%20model%20developed%20on%20over%20100%2C000%20frozen%20sections%20from%0Aeight%20medical%20centers%2C%20specifically%20designed%20to%20provide%20Clinical-grade%20Robust%0AIntraoperative%20Support%20for%20Pathology%20%28CRISP%29.%20CRISP%20was%20comprehensively%0Aevaluated%20on%20more%20than%2015%2C000%20intraoperative%20slides%20across%20nearly%20100%0Aretrospective%20diagnostic%20tasks%2C%20including%20benign-malignant%20discrimination%2C%20key%0Aintraoperative%20decision-making%2C%20and%20pan-cancer%20detection%2C%20etc.%20The%20model%0Ademonstrated%20robust%20generalization%20across%20diverse%20institutions%2C%20tumor%20types%2C%0Aand%20anatomical%20sites-including%20previously%20unseen%20sites%20and%20rare%20cancers.%20In%20a%0Aprospective%20cohort%20of%20over%202%2C000%20patients%2C%20CRISP%20sustained%20high%20diagnostic%0Aaccuracy%20under%20real-world%20conditions%2C%20directly%20informing%20surgical%20decisions%20in%0A92.6%25%20of%20cases.%20Human-AI%20collaboration%20further%20reduced%20diagnostic%20workload%20by%0A35%25%2C%20avoided%20105%20ancillary%20tests%20and%20enhanced%20detection%20of%20micrometastases%20with%0A87.5%25%20accuracy.%20Together%2C%20these%20findings%20position%20CRISP%20as%20a%20clinical-grade%0Aparadigm%20for%20AI-driven%20intraoperative%20pathology%2C%20bridging%20computational%0Aadvances%20with%20surgical%20precision%20and%20accelerating%20the%20translation%20of%20artificial%0Aintelligence%20into%20routine%20clinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Clinical-grade%2520Universal%2520Foundation%2520Model%2520for%2520Intraoperative%2520Pathology%26entry.906535625%3DZihan%2520Zhao%2520and%2520Fengtao%2520Zhou%2520and%2520Ronggang%2520Li%2520and%2520Bing%2520Chu%2520and%2520Xinke%2520Zhang%2520and%2520Xueyi%2520Zheng%2520and%2520Ke%2520Zheng%2520and%2520Xiaobo%2520Wen%2520and%2520Jiabo%2520Ma%2520and%2520Yihui%2520Wang%2520and%2520Jiewei%2520Chen%2520and%2520Chengyou%2520Zheng%2520and%2520Jiangyu%2520Zhang%2520and%2520Yongqin%2520Wen%2520and%2520Jiajia%2520Meng%2520and%2520Ziqi%2520Zeng%2520and%2520Xiaoqing%2520Li%2520and%2520Jing%2520Li%2520and%2520Dan%2520Xie%2520and%2520Yaping%2520Ye%2520and%2520Yu%2520Wang%2520and%2520Hao%2520Chen%2520and%2520Muyan%2520Cai%26entry.1292438233%3D%2520%2520Intraoperative%2520pathology%2520is%2520pivotal%2520to%2520precision%2520surgery%252C%2520yet%2520its%2520clinical%250Aimpact%2520is%2520constrained%2520by%2520diagnostic%2520complexity%2520and%2520the%2520limited%2520availability%2520of%250Ahigh-quality%2520frozen-section%2520data.%2520While%2520computational%2520pathology%2520has%2520made%250Asignificant%2520strides%252C%2520the%2520lack%2520of%2520large-scale%252C%2520prospective%2520validation%2520has%250Aimpeded%2520its%2520routine%2520adoption%2520in%2520surgical%2520workflows.%2520Here%252C%2520we%2520introduce%2520CRISP%252C%2520a%250Aclinical-grade%2520foundation%2520model%2520developed%2520on%2520over%2520100%252C000%2520frozen%2520sections%2520from%250Aeight%2520medical%2520centers%252C%2520specifically%2520designed%2520to%2520provide%2520Clinical-grade%2520Robust%250AIntraoperative%2520Support%2520for%2520Pathology%2520%2528CRISP%2529.%2520CRISP%2520was%2520comprehensively%250Aevaluated%2520on%2520more%2520than%252015%252C000%2520intraoperative%2520slides%2520across%2520nearly%2520100%250Aretrospective%2520diagnostic%2520tasks%252C%2520including%2520benign-malignant%2520discrimination%252C%2520key%250Aintraoperative%2520decision-making%252C%2520and%2520pan-cancer%2520detection%252C%2520etc.%2520The%2520model%250Ademonstrated%2520robust%2520generalization%2520across%2520diverse%2520institutions%252C%2520tumor%2520types%252C%250Aand%2520anatomical%2520sites-including%2520previously%2520unseen%2520sites%2520and%2520rare%2520cancers.%2520In%2520a%250Aprospective%2520cohort%2520of%2520over%25202%252C000%2520patients%252C%2520CRISP%2520sustained%2520high%2520diagnostic%250Aaccuracy%2520under%2520real-world%2520conditions%252C%2520directly%2520informing%2520surgical%2520decisions%2520in%250A92.6%2525%2520of%2520cases.%2520Human-AI%2520collaboration%2520further%2520reduced%2520diagnostic%2520workload%2520by%250A35%2525%252C%2520avoided%2520105%2520ancillary%2520tests%2520and%2520enhanced%2520detection%2520of%2520micrometastases%2520with%250A87.5%2525%2520accuracy.%2520Together%252C%2520these%2520findings%2520position%2520CRISP%2520as%2520a%2520clinical-grade%250Aparadigm%2520for%2520AI-driven%2520intraoperative%2520pathology%252C%2520bridging%2520computational%250Aadvances%2520with%2520surgical%2520precision%2520and%2520accelerating%2520the%2520translation%2520of%2520artificial%250Aintelligence%2520into%2520routine%2520clinical%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Clinical-grade%20Universal%20Foundation%20Model%20for%20Intraoperative%20Pathology&entry.906535625=Zihan%20Zhao%20and%20Fengtao%20Zhou%20and%20Ronggang%20Li%20and%20Bing%20Chu%20and%20Xinke%20Zhang%20and%20Xueyi%20Zheng%20and%20Ke%20Zheng%20and%20Xiaobo%20Wen%20and%20Jiabo%20Ma%20and%20Yihui%20Wang%20and%20Jiewei%20Chen%20and%20Chengyou%20Zheng%20and%20Jiangyu%20Zhang%20and%20Yongqin%20Wen%20and%20Jiajia%20Meng%20and%20Ziqi%20Zeng%20and%20Xiaoqing%20Li%20and%20Jing%20Li%20and%20Dan%20Xie%20and%20Yaping%20Ye%20and%20Yu%20Wang%20and%20Hao%20Chen%20and%20Muyan%20Cai&entry.1292438233=%20%20Intraoperative%20pathology%20is%20pivotal%20to%20precision%20surgery%2C%20yet%20its%20clinical%0Aimpact%20is%20constrained%20by%20diagnostic%20complexity%20and%20the%20limited%20availability%20of%0Ahigh-quality%20frozen-section%20data.%20While%20computational%20pathology%20has%20made%0Asignificant%20strides%2C%20the%20lack%20of%20large-scale%2C%20prospective%20validation%20has%0Aimpeded%20its%20routine%20adoption%20in%20surgical%20workflows.%20Here%2C%20we%20introduce%20CRISP%2C%20a%0Aclinical-grade%20foundation%20model%20developed%20on%20over%20100%2C000%20frozen%20sections%20from%0Aeight%20medical%20centers%2C%20specifically%20designed%20to%20provide%20Clinical-grade%20Robust%0AIntraoperative%20Support%20for%20Pathology%20%28CRISP%29.%20CRISP%20was%20comprehensively%0Aevaluated%20on%20more%20than%2015%2C000%20intraoperative%20slides%20across%20nearly%20100%0Aretrospective%20diagnostic%20tasks%2C%20including%20benign-malignant%20discrimination%2C%20key%0Aintraoperative%20decision-making%2C%20and%20pan-cancer%20detection%2C%20etc.%20The%20model%0Ademonstrated%20robust%20generalization%20across%20diverse%20institutions%2C%20tumor%20types%2C%0Aand%20anatomical%20sites-including%20previously%20unseen%20sites%20and%20rare%20cancers.%20In%20a%0Aprospective%20cohort%20of%20over%202%2C000%20patients%2C%20CRISP%20sustained%20high%20diagnostic%0Aaccuracy%20under%20real-world%20conditions%2C%20directly%20informing%20surgical%20decisions%20in%0A92.6%25%20of%20cases.%20Human-AI%20collaboration%20further%20reduced%20diagnostic%20workload%20by%0A35%25%2C%20avoided%20105%20ancillary%20tests%20and%20enhanced%20detection%20of%20micrometastases%20with%0A87.5%25%20accuracy.%20Together%2C%20these%20findings%20position%20CRISP%20as%20a%20clinical-grade%0Aparadigm%20for%20AI-driven%20intraoperative%20pathology%2C%20bridging%20computational%0Aadvances%20with%20surgical%20precision%20and%20accelerating%20the%20translation%20of%20artificial%0Aintelligence%20into%20routine%20clinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04861v1&entry.124074799=Read"},
{"title": "Summaries as Centroids for Interpretable and Scalable Text Clustering", "author": "Jairo Diaz-Rodriguez", "abstract": "  We introduce k-NLPmeans and k-LLMmeans, text-clustering variants of k-means\nthat periodically replace numeric centroids with textual summaries. The key\nidea, summary-as-centroid, retains k-means assignments in embedding space while\nproducing human-readable, auditable cluster prototypes. The method is\nLLM-optional: k-NLPmeans uses lightweight, deterministic summarizers, enabling\noffline, low-cost, and stable operation; k-LLMmeans is a drop-in upgrade that\nuses an LLM for summaries under a fixed per-iteration budget whose cost does\nnot grow with dataset size. We also present a mini-batch extension for\nreal-time clustering of streaming text. Across diverse datasets, embedding\nmodels, and summarization strategies, our approach consistently outperforms\nclassical baselines and approaches the accuracy of recent LLM-based\nclustering-without extensive LLM calls. Finally, we provide a case study on\nsequential text streams and release a StackExchange-derived benchmark for\nevaluating streaming text clustering.\n", "link": "http://arxiv.org/abs/2502.09667v3", "date": "2025-10-06", "relevancy": 2.2417, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Summaries%20as%20Centroids%20for%20Interpretable%20and%20Scalable%20Text%20Clustering&body=Title%3A%20Summaries%20as%20Centroids%20for%20Interpretable%20and%20Scalable%20Text%20Clustering%0AAuthor%3A%20Jairo%20Diaz-Rodriguez%0AAbstract%3A%20%20%20We%20introduce%20k-NLPmeans%20and%20k-LLMmeans%2C%20text-clustering%20variants%20of%20k-means%0Athat%20periodically%20replace%20numeric%20centroids%20with%20textual%20summaries.%20The%20key%0Aidea%2C%20summary-as-centroid%2C%20retains%20k-means%20assignments%20in%20embedding%20space%20while%0Aproducing%20human-readable%2C%20auditable%20cluster%20prototypes.%20The%20method%20is%0ALLM-optional%3A%20k-NLPmeans%20uses%20lightweight%2C%20deterministic%20summarizers%2C%20enabling%0Aoffline%2C%20low-cost%2C%20and%20stable%20operation%3B%20k-LLMmeans%20is%20a%20drop-in%20upgrade%20that%0Auses%20an%20LLM%20for%20summaries%20under%20a%20fixed%20per-iteration%20budget%20whose%20cost%20does%0Anot%20grow%20with%20dataset%20size.%20We%20also%20present%20a%20mini-batch%20extension%20for%0Areal-time%20clustering%20of%20streaming%20text.%20Across%20diverse%20datasets%2C%20embedding%0Amodels%2C%20and%20summarization%20strategies%2C%20our%20approach%20consistently%20outperforms%0Aclassical%20baselines%20and%20approaches%20the%20accuracy%20of%20recent%20LLM-based%0Aclustering-without%20extensive%20LLM%20calls.%20Finally%2C%20we%20provide%20a%20case%20study%20on%0Asequential%20text%20streams%20and%20release%20a%20StackExchange-derived%20benchmark%20for%0Aevaluating%20streaming%20text%20clustering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09667v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSummaries%2520as%2520Centroids%2520for%2520Interpretable%2520and%2520Scalable%2520Text%2520Clustering%26entry.906535625%3DJairo%2520Diaz-Rodriguez%26entry.1292438233%3D%2520%2520We%2520introduce%2520k-NLPmeans%2520and%2520k-LLMmeans%252C%2520text-clustering%2520variants%2520of%2520k-means%250Athat%2520periodically%2520replace%2520numeric%2520centroids%2520with%2520textual%2520summaries.%2520The%2520key%250Aidea%252C%2520summary-as-centroid%252C%2520retains%2520k-means%2520assignments%2520in%2520embedding%2520space%2520while%250Aproducing%2520human-readable%252C%2520auditable%2520cluster%2520prototypes.%2520The%2520method%2520is%250ALLM-optional%253A%2520k-NLPmeans%2520uses%2520lightweight%252C%2520deterministic%2520summarizers%252C%2520enabling%250Aoffline%252C%2520low-cost%252C%2520and%2520stable%2520operation%253B%2520k-LLMmeans%2520is%2520a%2520drop-in%2520upgrade%2520that%250Auses%2520an%2520LLM%2520for%2520summaries%2520under%2520a%2520fixed%2520per-iteration%2520budget%2520whose%2520cost%2520does%250Anot%2520grow%2520with%2520dataset%2520size.%2520We%2520also%2520present%2520a%2520mini-batch%2520extension%2520for%250Areal-time%2520clustering%2520of%2520streaming%2520text.%2520Across%2520diverse%2520datasets%252C%2520embedding%250Amodels%252C%2520and%2520summarization%2520strategies%252C%2520our%2520approach%2520consistently%2520outperforms%250Aclassical%2520baselines%2520and%2520approaches%2520the%2520accuracy%2520of%2520recent%2520LLM-based%250Aclustering-without%2520extensive%2520LLM%2520calls.%2520Finally%252C%2520we%2520provide%2520a%2520case%2520study%2520on%250Asequential%2520text%2520streams%2520and%2520release%2520a%2520StackExchange-derived%2520benchmark%2520for%250Aevaluating%2520streaming%2520text%2520clustering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09667v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Summaries%20as%20Centroids%20for%20Interpretable%20and%20Scalable%20Text%20Clustering&entry.906535625=Jairo%20Diaz-Rodriguez&entry.1292438233=%20%20We%20introduce%20k-NLPmeans%20and%20k-LLMmeans%2C%20text-clustering%20variants%20of%20k-means%0Athat%20periodically%20replace%20numeric%20centroids%20with%20textual%20summaries.%20The%20key%0Aidea%2C%20summary-as-centroid%2C%20retains%20k-means%20assignments%20in%20embedding%20space%20while%0Aproducing%20human-readable%2C%20auditable%20cluster%20prototypes.%20The%20method%20is%0ALLM-optional%3A%20k-NLPmeans%20uses%20lightweight%2C%20deterministic%20summarizers%2C%20enabling%0Aoffline%2C%20low-cost%2C%20and%20stable%20operation%3B%20k-LLMmeans%20is%20a%20drop-in%20upgrade%20that%0Auses%20an%20LLM%20for%20summaries%20under%20a%20fixed%20per-iteration%20budget%20whose%20cost%20does%0Anot%20grow%20with%20dataset%20size.%20We%20also%20present%20a%20mini-batch%20extension%20for%0Areal-time%20clustering%20of%20streaming%20text.%20Across%20diverse%20datasets%2C%20embedding%0Amodels%2C%20and%20summarization%20strategies%2C%20our%20approach%20consistently%20outperforms%0Aclassical%20baselines%20and%20approaches%20the%20accuracy%20of%20recent%20LLM-based%0Aclustering-without%20extensive%20LLM%20calls.%20Finally%2C%20we%20provide%20a%20case%20study%20on%0Asequential%20text%20streams%20and%20release%20a%20StackExchange-derived%20benchmark%20for%0Aevaluating%20streaming%20text%20clustering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09667v3&entry.124074799=Read"},
{"title": "How Different from the Past? Spatio-Temporal Time Series Forecasting\n  with Self-Supervised Deviation Learning", "author": "Haotian Gao and Zheng Dong and Jiawei Yong and Shintaro Fukushima and Kenjiro Taura and Renhe Jiang", "abstract": "  Spatio-temporal forecasting is essential for real-world applications such as\ntraffic management and urban computing. Although recent methods have shown\nimproved accuracy, they often fail to account for dynamic deviations between\ncurrent inputs and historical patterns. These deviations contain critical\nsignals that can significantly affect model performance. To fill this gap, we\npropose ST-SSDL, a Spatio-Temporal time series forecasting framework that\nincorporates a Self-Supervised Deviation Learning scheme to capture and utilize\nsuch deviations. ST-SSDL anchors each input to its historical average and\ndiscretizes the latent space using learnable prototypes that represent typical\nspatio-temporal patterns. Two auxiliary objectives are proposed to refine this\nstructure: a contrastive loss that enhances inter-prototype discriminability\nand a deviation loss that regularizes the distance consistency between input\nrepresentations and corresponding prototypes to quantify deviation. Optimized\njointly with the forecasting objective, these components guide the model to\norganize its hidden space and improve generalization across diverse input\nconditions. Experiments on six benchmark datasets show that ST-SSDL\nconsistently outperforms state-of-the-art baselines across multiple metrics.\nVisualizations further demonstrate its ability to adaptively respond to varying\nlevels of deviation in complex spatio-temporal scenarios. Our code and datasets\nare available at https://github.com/Jimmy-7664/ST-SSDL.\n", "link": "http://arxiv.org/abs/2510.04908v1", "date": "2025-10-06", "relevancy": 2.2205, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5719}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5507}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Different%20from%20the%20Past%3F%20Spatio-Temporal%20Time%20Series%20Forecasting%0A%20%20with%20Self-Supervised%20Deviation%20Learning&body=Title%3A%20How%20Different%20from%20the%20Past%3F%20Spatio-Temporal%20Time%20Series%20Forecasting%0A%20%20with%20Self-Supervised%20Deviation%20Learning%0AAuthor%3A%20Haotian%20Gao%20and%20Zheng%20Dong%20and%20Jiawei%20Yong%20and%20Shintaro%20Fukushima%20and%20Kenjiro%20Taura%20and%20Renhe%20Jiang%0AAbstract%3A%20%20%20Spatio-temporal%20forecasting%20is%20essential%20for%20real-world%20applications%20such%20as%0Atraffic%20management%20and%20urban%20computing.%20Although%20recent%20methods%20have%20shown%0Aimproved%20accuracy%2C%20they%20often%20fail%20to%20account%20for%20dynamic%20deviations%20between%0Acurrent%20inputs%20and%20historical%20patterns.%20These%20deviations%20contain%20critical%0Asignals%20that%20can%20significantly%20affect%20model%20performance.%20To%20fill%20this%20gap%2C%20we%0Apropose%20ST-SSDL%2C%20a%20Spatio-Temporal%20time%20series%20forecasting%20framework%20that%0Aincorporates%20a%20Self-Supervised%20Deviation%20Learning%20scheme%20to%20capture%20and%20utilize%0Asuch%20deviations.%20ST-SSDL%20anchors%20each%20input%20to%20its%20historical%20average%20and%0Adiscretizes%20the%20latent%20space%20using%20learnable%20prototypes%20that%20represent%20typical%0Aspatio-temporal%20patterns.%20Two%20auxiliary%20objectives%20are%20proposed%20to%20refine%20this%0Astructure%3A%20a%20contrastive%20loss%20that%20enhances%20inter-prototype%20discriminability%0Aand%20a%20deviation%20loss%20that%20regularizes%20the%20distance%20consistency%20between%20input%0Arepresentations%20and%20corresponding%20prototypes%20to%20quantify%20deviation.%20Optimized%0Ajointly%20with%20the%20forecasting%20objective%2C%20these%20components%20guide%20the%20model%20to%0Aorganize%20its%20hidden%20space%20and%20improve%20generalization%20across%20diverse%20input%0Aconditions.%20Experiments%20on%20six%20benchmark%20datasets%20show%20that%20ST-SSDL%0Aconsistently%20outperforms%20state-of-the-art%20baselines%20across%20multiple%20metrics.%0AVisualizations%20further%20demonstrate%20its%20ability%20to%20adaptively%20respond%20to%20varying%0Alevels%20of%20deviation%20in%20complex%20spatio-temporal%20scenarios.%20Our%20code%20and%20datasets%0Aare%20available%20at%20https%3A//github.com/Jimmy-7664/ST-SSDL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Different%2520from%2520the%2520Past%253F%2520Spatio-Temporal%2520Time%2520Series%2520Forecasting%250A%2520%2520with%2520Self-Supervised%2520Deviation%2520Learning%26entry.906535625%3DHaotian%2520Gao%2520and%2520Zheng%2520Dong%2520and%2520Jiawei%2520Yong%2520and%2520Shintaro%2520Fukushima%2520and%2520Kenjiro%2520Taura%2520and%2520Renhe%2520Jiang%26entry.1292438233%3D%2520%2520Spatio-temporal%2520forecasting%2520is%2520essential%2520for%2520real-world%2520applications%2520such%2520as%250Atraffic%2520management%2520and%2520urban%2520computing.%2520Although%2520recent%2520methods%2520have%2520shown%250Aimproved%2520accuracy%252C%2520they%2520often%2520fail%2520to%2520account%2520for%2520dynamic%2520deviations%2520between%250Acurrent%2520inputs%2520and%2520historical%2520patterns.%2520These%2520deviations%2520contain%2520critical%250Asignals%2520that%2520can%2520significantly%2520affect%2520model%2520performance.%2520To%2520fill%2520this%2520gap%252C%2520we%250Apropose%2520ST-SSDL%252C%2520a%2520Spatio-Temporal%2520time%2520series%2520forecasting%2520framework%2520that%250Aincorporates%2520a%2520Self-Supervised%2520Deviation%2520Learning%2520scheme%2520to%2520capture%2520and%2520utilize%250Asuch%2520deviations.%2520ST-SSDL%2520anchors%2520each%2520input%2520to%2520its%2520historical%2520average%2520and%250Adiscretizes%2520the%2520latent%2520space%2520using%2520learnable%2520prototypes%2520that%2520represent%2520typical%250Aspatio-temporal%2520patterns.%2520Two%2520auxiliary%2520objectives%2520are%2520proposed%2520to%2520refine%2520this%250Astructure%253A%2520a%2520contrastive%2520loss%2520that%2520enhances%2520inter-prototype%2520discriminability%250Aand%2520a%2520deviation%2520loss%2520that%2520regularizes%2520the%2520distance%2520consistency%2520between%2520input%250Arepresentations%2520and%2520corresponding%2520prototypes%2520to%2520quantify%2520deviation.%2520Optimized%250Ajointly%2520with%2520the%2520forecasting%2520objective%252C%2520these%2520components%2520guide%2520the%2520model%2520to%250Aorganize%2520its%2520hidden%2520space%2520and%2520improve%2520generalization%2520across%2520diverse%2520input%250Aconditions.%2520Experiments%2520on%2520six%2520benchmark%2520datasets%2520show%2520that%2520ST-SSDL%250Aconsistently%2520outperforms%2520state-of-the-art%2520baselines%2520across%2520multiple%2520metrics.%250AVisualizations%2520further%2520demonstrate%2520its%2520ability%2520to%2520adaptively%2520respond%2520to%2520varying%250Alevels%2520of%2520deviation%2520in%2520complex%2520spatio-temporal%2520scenarios.%2520Our%2520code%2520and%2520datasets%250Aare%2520available%2520at%2520https%253A//github.com/Jimmy-7664/ST-SSDL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Different%20from%20the%20Past%3F%20Spatio-Temporal%20Time%20Series%20Forecasting%0A%20%20with%20Self-Supervised%20Deviation%20Learning&entry.906535625=Haotian%20Gao%20and%20Zheng%20Dong%20and%20Jiawei%20Yong%20and%20Shintaro%20Fukushima%20and%20Kenjiro%20Taura%20and%20Renhe%20Jiang&entry.1292438233=%20%20Spatio-temporal%20forecasting%20is%20essential%20for%20real-world%20applications%20such%20as%0Atraffic%20management%20and%20urban%20computing.%20Although%20recent%20methods%20have%20shown%0Aimproved%20accuracy%2C%20they%20often%20fail%20to%20account%20for%20dynamic%20deviations%20between%0Acurrent%20inputs%20and%20historical%20patterns.%20These%20deviations%20contain%20critical%0Asignals%20that%20can%20significantly%20affect%20model%20performance.%20To%20fill%20this%20gap%2C%20we%0Apropose%20ST-SSDL%2C%20a%20Spatio-Temporal%20time%20series%20forecasting%20framework%20that%0Aincorporates%20a%20Self-Supervised%20Deviation%20Learning%20scheme%20to%20capture%20and%20utilize%0Asuch%20deviations.%20ST-SSDL%20anchors%20each%20input%20to%20its%20historical%20average%20and%0Adiscretizes%20the%20latent%20space%20using%20learnable%20prototypes%20that%20represent%20typical%0Aspatio-temporal%20patterns.%20Two%20auxiliary%20objectives%20are%20proposed%20to%20refine%20this%0Astructure%3A%20a%20contrastive%20loss%20that%20enhances%20inter-prototype%20discriminability%0Aand%20a%20deviation%20loss%20that%20regularizes%20the%20distance%20consistency%20between%20input%0Arepresentations%20and%20corresponding%20prototypes%20to%20quantify%20deviation.%20Optimized%0Ajointly%20with%20the%20forecasting%20objective%2C%20these%20components%20guide%20the%20model%20to%0Aorganize%20its%20hidden%20space%20and%20improve%20generalization%20across%20diverse%20input%0Aconditions.%20Experiments%20on%20six%20benchmark%20datasets%20show%20that%20ST-SSDL%0Aconsistently%20outperforms%20state-of-the-art%20baselines%20across%20multiple%20metrics.%0AVisualizations%20further%20demonstrate%20its%20ability%20to%20adaptively%20respond%20to%20varying%0Alevels%20of%20deviation%20in%20complex%20spatio-temporal%20scenarios.%20Our%20code%20and%20datasets%0Aare%20available%20at%20https%3A//github.com/Jimmy-7664/ST-SSDL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04908v1&entry.124074799=Read"},
{"title": "Video Game Level Design as a Multi-Agent Reinforcement Learning Problem", "author": "Sam Earle and Zehua Jiang and Eugene Vinitsky and Julian Togelius", "abstract": "  Procedural Content Generation via Reinforcement Learning (PCGRL) offers a\nmethod for training controllable level designer agents without the need for\nhuman datasets, using metrics that serve as proxies for level quality as\nrewards. Existing PCGRL research focuses on single generator agents, but are\nbottlenecked by the need to frequently recalculate heuristics of level quality\nand the agent's need to navigate around potentially large maps. By framing\nlevel generation as a multi-agent problem, we mitigate the efficiency\nbottleneck of single-agent PCGRL by reducing the number of reward calculations\nrelative to the number of agent actions. We also find that multi-agent level\ngenerators are better able to generalize to out-of-distribution map shapes,\nwhich we argue is due to the generators' learning more local, modular design\npolicies. We conclude that treating content generation as a distributed,\nmulti-agent task is beneficial for generating functional artifacts at scale.\n", "link": "http://arxiv.org/abs/2510.04862v1", "date": "2025-10-06", "relevancy": 2.2074, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5655}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5451}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Game%20Level%20Design%20as%20a%20Multi-Agent%20Reinforcement%20Learning%20Problem&body=Title%3A%20Video%20Game%20Level%20Design%20as%20a%20Multi-Agent%20Reinforcement%20Learning%20Problem%0AAuthor%3A%20Sam%20Earle%20and%20Zehua%20Jiang%20and%20Eugene%20Vinitsky%20and%20Julian%20Togelius%0AAbstract%3A%20%20%20Procedural%20Content%20Generation%20via%20Reinforcement%20Learning%20%28PCGRL%29%20offers%20a%0Amethod%20for%20training%20controllable%20level%20designer%20agents%20without%20the%20need%20for%0Ahuman%20datasets%2C%20using%20metrics%20that%20serve%20as%20proxies%20for%20level%20quality%20as%0Arewards.%20Existing%20PCGRL%20research%20focuses%20on%20single%20generator%20agents%2C%20but%20are%0Abottlenecked%20by%20the%20need%20to%20frequently%20recalculate%20heuristics%20of%20level%20quality%0Aand%20the%20agent%27s%20need%20to%20navigate%20around%20potentially%20large%20maps.%20By%20framing%0Alevel%20generation%20as%20a%20multi-agent%20problem%2C%20we%20mitigate%20the%20efficiency%0Abottleneck%20of%20single-agent%20PCGRL%20by%20reducing%20the%20number%20of%20reward%20calculations%0Arelative%20to%20the%20number%20of%20agent%20actions.%20We%20also%20find%20that%20multi-agent%20level%0Agenerators%20are%20better%20able%20to%20generalize%20to%20out-of-distribution%20map%20shapes%2C%0Awhich%20we%20argue%20is%20due%20to%20the%20generators%27%20learning%20more%20local%2C%20modular%20design%0Apolicies.%20We%20conclude%20that%20treating%20content%20generation%20as%20a%20distributed%2C%0Amulti-agent%20task%20is%20beneficial%20for%20generating%20functional%20artifacts%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Game%2520Level%2520Design%2520as%2520a%2520Multi-Agent%2520Reinforcement%2520Learning%2520Problem%26entry.906535625%3DSam%2520Earle%2520and%2520Zehua%2520Jiang%2520and%2520Eugene%2520Vinitsky%2520and%2520Julian%2520Togelius%26entry.1292438233%3D%2520%2520Procedural%2520Content%2520Generation%2520via%2520Reinforcement%2520Learning%2520%2528PCGRL%2529%2520offers%2520a%250Amethod%2520for%2520training%2520controllable%2520level%2520designer%2520agents%2520without%2520the%2520need%2520for%250Ahuman%2520datasets%252C%2520using%2520metrics%2520that%2520serve%2520as%2520proxies%2520for%2520level%2520quality%2520as%250Arewards.%2520Existing%2520PCGRL%2520research%2520focuses%2520on%2520single%2520generator%2520agents%252C%2520but%2520are%250Abottlenecked%2520by%2520the%2520need%2520to%2520frequently%2520recalculate%2520heuristics%2520of%2520level%2520quality%250Aand%2520the%2520agent%2527s%2520need%2520to%2520navigate%2520around%2520potentially%2520large%2520maps.%2520By%2520framing%250Alevel%2520generation%2520as%2520a%2520multi-agent%2520problem%252C%2520we%2520mitigate%2520the%2520efficiency%250Abottleneck%2520of%2520single-agent%2520PCGRL%2520by%2520reducing%2520the%2520number%2520of%2520reward%2520calculations%250Arelative%2520to%2520the%2520number%2520of%2520agent%2520actions.%2520We%2520also%2520find%2520that%2520multi-agent%2520level%250Agenerators%2520are%2520better%2520able%2520to%2520generalize%2520to%2520out-of-distribution%2520map%2520shapes%252C%250Awhich%2520we%2520argue%2520is%2520due%2520to%2520the%2520generators%2527%2520learning%2520more%2520local%252C%2520modular%2520design%250Apolicies.%2520We%2520conclude%2520that%2520treating%2520content%2520generation%2520as%2520a%2520distributed%252C%250Amulti-agent%2520task%2520is%2520beneficial%2520for%2520generating%2520functional%2520artifacts%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Game%20Level%20Design%20as%20a%20Multi-Agent%20Reinforcement%20Learning%20Problem&entry.906535625=Sam%20Earle%20and%20Zehua%20Jiang%20and%20Eugene%20Vinitsky%20and%20Julian%20Togelius&entry.1292438233=%20%20Procedural%20Content%20Generation%20via%20Reinforcement%20Learning%20%28PCGRL%29%20offers%20a%0Amethod%20for%20training%20controllable%20level%20designer%20agents%20without%20the%20need%20for%0Ahuman%20datasets%2C%20using%20metrics%20that%20serve%20as%20proxies%20for%20level%20quality%20as%0Arewards.%20Existing%20PCGRL%20research%20focuses%20on%20single%20generator%20agents%2C%20but%20are%0Abottlenecked%20by%20the%20need%20to%20frequently%20recalculate%20heuristics%20of%20level%20quality%0Aand%20the%20agent%27s%20need%20to%20navigate%20around%20potentially%20large%20maps.%20By%20framing%0Alevel%20generation%20as%20a%20multi-agent%20problem%2C%20we%20mitigate%20the%20efficiency%0Abottleneck%20of%20single-agent%20PCGRL%20by%20reducing%20the%20number%20of%20reward%20calculations%0Arelative%20to%20the%20number%20of%20agent%20actions.%20We%20also%20find%20that%20multi-agent%20level%0Agenerators%20are%20better%20able%20to%20generalize%20to%20out-of-distribution%20map%20shapes%2C%0Awhich%20we%20argue%20is%20due%20to%20the%20generators%27%20learning%20more%20local%2C%20modular%20design%0Apolicies.%20We%20conclude%20that%20treating%20content%20generation%20as%20a%20distributed%2C%0Amulti-agent%20task%20is%20beneficial%20for%20generating%20functional%20artifacts%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04862v1&entry.124074799=Read"},
{"title": "A Semantics-Aware Hierarchical Self-Supervised Approach to\n  Classification of Remote Sensing Images", "author": "Giulio Weikmann and Gianmarco Perantoni and Lorenzo Bruzzone", "abstract": "  Deep learning has become increasingly important in remote sensing image\nclassification due to its ability to extract semantic information from complex\ndata. Classification tasks often include predefined label hierarchies that\nrepresent the semantic relationships among classes. However, these hierarchies\nare frequently overlooked, and most approaches focus only on fine-grained\nclassification schemes. In this paper, we present a novel Semantics-Aware\nHierarchical Consensus (SAHC) method for learning hierarchical features and\nrelationships by integrating hierarchy-specific classification heads within a\ndeep network architecture, each specialized in different degrees of class\ngranularity. The proposed approach employs trainable hierarchy matrices, which\nguide the network through the learning of the hierarchical structure in a\nself-supervised manner. Furthermore, we introduce a hierarchical consensus\nmechanism to ensure consistent probability distributions across different\nhierarchical levels. This mechanism acts as a weighted ensemble being able to\neffectively leverage the inherent structure of the hierarchical classification\ntask. The proposed SAHC method is evaluated on three benchmark datasets with\ndifferent degrees of hierarchical complexity on different tasks, using distinct\nbackbone architectures to effectively emphasize its adaptability. Experimental\nresults show both the effectiveness of the proposed approach in guiding network\nlearning and the robustness of the hierarchical consensus for remote sensing\nimage classification tasks.\n", "link": "http://arxiv.org/abs/2510.04916v1", "date": "2025-10-06", "relevancy": 2.1964, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5783}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5551}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Semantics-Aware%20Hierarchical%20Self-Supervised%20Approach%20to%0A%20%20Classification%20of%20Remote%20Sensing%20Images&body=Title%3A%20A%20Semantics-Aware%20Hierarchical%20Self-Supervised%20Approach%20to%0A%20%20Classification%20of%20Remote%20Sensing%20Images%0AAuthor%3A%20Giulio%20Weikmann%20and%20Gianmarco%20Perantoni%20and%20Lorenzo%20Bruzzone%0AAbstract%3A%20%20%20Deep%20learning%20has%20become%20increasingly%20important%20in%20remote%20sensing%20image%0Aclassification%20due%20to%20its%20ability%20to%20extract%20semantic%20information%20from%20complex%0Adata.%20Classification%20tasks%20often%20include%20predefined%20label%20hierarchies%20that%0Arepresent%20the%20semantic%20relationships%20among%20classes.%20However%2C%20these%20hierarchies%0Aare%20frequently%20overlooked%2C%20and%20most%20approaches%20focus%20only%20on%20fine-grained%0Aclassification%20schemes.%20In%20this%20paper%2C%20we%20present%20a%20novel%20Semantics-Aware%0AHierarchical%20Consensus%20%28SAHC%29%20method%20for%20learning%20hierarchical%20features%20and%0Arelationships%20by%20integrating%20hierarchy-specific%20classification%20heads%20within%20a%0Adeep%20network%20architecture%2C%20each%20specialized%20in%20different%20degrees%20of%20class%0Agranularity.%20The%20proposed%20approach%20employs%20trainable%20hierarchy%20matrices%2C%20which%0Aguide%20the%20network%20through%20the%20learning%20of%20the%20hierarchical%20structure%20in%20a%0Aself-supervised%20manner.%20Furthermore%2C%20we%20introduce%20a%20hierarchical%20consensus%0Amechanism%20to%20ensure%20consistent%20probability%20distributions%20across%20different%0Ahierarchical%20levels.%20This%20mechanism%20acts%20as%20a%20weighted%20ensemble%20being%20able%20to%0Aeffectively%20leverage%20the%20inherent%20structure%20of%20the%20hierarchical%20classification%0Atask.%20The%20proposed%20SAHC%20method%20is%20evaluated%20on%20three%20benchmark%20datasets%20with%0Adifferent%20degrees%20of%20hierarchical%20complexity%20on%20different%20tasks%2C%20using%20distinct%0Abackbone%20architectures%20to%20effectively%20emphasize%20its%20adaptability.%20Experimental%0Aresults%20show%20both%20the%20effectiveness%20of%20the%20proposed%20approach%20in%20guiding%20network%0Alearning%20and%20the%20robustness%20of%20the%20hierarchical%20consensus%20for%20remote%20sensing%0Aimage%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Semantics-Aware%2520Hierarchical%2520Self-Supervised%2520Approach%2520to%250A%2520%2520Classification%2520of%2520Remote%2520Sensing%2520Images%26entry.906535625%3DGiulio%2520Weikmann%2520and%2520Gianmarco%2520Perantoni%2520and%2520Lorenzo%2520Bruzzone%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520become%2520increasingly%2520important%2520in%2520remote%2520sensing%2520image%250Aclassification%2520due%2520to%2520its%2520ability%2520to%2520extract%2520semantic%2520information%2520from%2520complex%250Adata.%2520Classification%2520tasks%2520often%2520include%2520predefined%2520label%2520hierarchies%2520that%250Arepresent%2520the%2520semantic%2520relationships%2520among%2520classes.%2520However%252C%2520these%2520hierarchies%250Aare%2520frequently%2520overlooked%252C%2520and%2520most%2520approaches%2520focus%2520only%2520on%2520fine-grained%250Aclassification%2520schemes.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520Semantics-Aware%250AHierarchical%2520Consensus%2520%2528SAHC%2529%2520method%2520for%2520learning%2520hierarchical%2520features%2520and%250Arelationships%2520by%2520integrating%2520hierarchy-specific%2520classification%2520heads%2520within%2520a%250Adeep%2520network%2520architecture%252C%2520each%2520specialized%2520in%2520different%2520degrees%2520of%2520class%250Agranularity.%2520The%2520proposed%2520approach%2520employs%2520trainable%2520hierarchy%2520matrices%252C%2520which%250Aguide%2520the%2520network%2520through%2520the%2520learning%2520of%2520the%2520hierarchical%2520structure%2520in%2520a%250Aself-supervised%2520manner.%2520Furthermore%252C%2520we%2520introduce%2520a%2520hierarchical%2520consensus%250Amechanism%2520to%2520ensure%2520consistent%2520probability%2520distributions%2520across%2520different%250Ahierarchical%2520levels.%2520This%2520mechanism%2520acts%2520as%2520a%2520weighted%2520ensemble%2520being%2520able%2520to%250Aeffectively%2520leverage%2520the%2520inherent%2520structure%2520of%2520the%2520hierarchical%2520classification%250Atask.%2520The%2520proposed%2520SAHC%2520method%2520is%2520evaluated%2520on%2520three%2520benchmark%2520datasets%2520with%250Adifferent%2520degrees%2520of%2520hierarchical%2520complexity%2520on%2520different%2520tasks%252C%2520using%2520distinct%250Abackbone%2520architectures%2520to%2520effectively%2520emphasize%2520its%2520adaptability.%2520Experimental%250Aresults%2520show%2520both%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520in%2520guiding%2520network%250Alearning%2520and%2520the%2520robustness%2520of%2520the%2520hierarchical%2520consensus%2520for%2520remote%2520sensing%250Aimage%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Semantics-Aware%20Hierarchical%20Self-Supervised%20Approach%20to%0A%20%20Classification%20of%20Remote%20Sensing%20Images&entry.906535625=Giulio%20Weikmann%20and%20Gianmarco%20Perantoni%20and%20Lorenzo%20Bruzzone&entry.1292438233=%20%20Deep%20learning%20has%20become%20increasingly%20important%20in%20remote%20sensing%20image%0Aclassification%20due%20to%20its%20ability%20to%20extract%20semantic%20information%20from%20complex%0Adata.%20Classification%20tasks%20often%20include%20predefined%20label%20hierarchies%20that%0Arepresent%20the%20semantic%20relationships%20among%20classes.%20However%2C%20these%20hierarchies%0Aare%20frequently%20overlooked%2C%20and%20most%20approaches%20focus%20only%20on%20fine-grained%0Aclassification%20schemes.%20In%20this%20paper%2C%20we%20present%20a%20novel%20Semantics-Aware%0AHierarchical%20Consensus%20%28SAHC%29%20method%20for%20learning%20hierarchical%20features%20and%0Arelationships%20by%20integrating%20hierarchy-specific%20classification%20heads%20within%20a%0Adeep%20network%20architecture%2C%20each%20specialized%20in%20different%20degrees%20of%20class%0Agranularity.%20The%20proposed%20approach%20employs%20trainable%20hierarchy%20matrices%2C%20which%0Aguide%20the%20network%20through%20the%20learning%20of%20the%20hierarchical%20structure%20in%20a%0Aself-supervised%20manner.%20Furthermore%2C%20we%20introduce%20a%20hierarchical%20consensus%0Amechanism%20to%20ensure%20consistent%20probability%20distributions%20across%20different%0Ahierarchical%20levels.%20This%20mechanism%20acts%20as%20a%20weighted%20ensemble%20being%20able%20to%0Aeffectively%20leverage%20the%20inherent%20structure%20of%20the%20hierarchical%20classification%0Atask.%20The%20proposed%20SAHC%20method%20is%20evaluated%20on%20three%20benchmark%20datasets%20with%0Adifferent%20degrees%20of%20hierarchical%20complexity%20on%20different%20tasks%2C%20using%20distinct%0Abackbone%20architectures%20to%20effectively%20emphasize%20its%20adaptability.%20Experimental%0Aresults%20show%20both%20the%20effectiveness%20of%20the%20proposed%20approach%20in%20guiding%20network%0Alearning%20and%20the%20robustness%20of%20the%20hierarchical%20consensus%20for%20remote%20sensing%0Aimage%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04916v1&entry.124074799=Read"},
{"title": "RealKIE: Five Novel Datasets for Enterprise Key Information Extraction", "author": "Benjamin Townsend and Madison May and Katherine Mackowiak and Christopher Wells", "abstract": "  We introduce RealKIE, a benchmark of five challenging datasets aimed at\nadvancing key information extraction methods, with an emphasis on enterprise\napplications. The datasets include a diverse range of documents including SEC\nS1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and\nResource Contracts. Each presents unique challenges: poor text serialization,\nsparse annotations in long documents, and complex tabular layouts. These\ndatasets provide a realistic testing ground for key information extraction\ntasks like investment analysis and contract analysis. In addition to presenting\nthese datasets, we offer an in-depth description of the annotation process,\ndocument processing techniques, and baseline modeling approaches. This\ncontribution facilitates the development of NLP models capable of handling\npractical challenges and supports further research into information extraction\ntechnologies applicable to industry-specific problems. The annotated data, OCR\noutputs, and code to reproduce baselines are available to download at\nhttps://indicodatasolutions.github.io/RealKIE/.\n", "link": "http://arxiv.org/abs/2403.20101v2", "date": "2025-10-06", "relevancy": 2.1702, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4379}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealKIE%3A%20Five%20Novel%20Datasets%20for%20Enterprise%20Key%20Information%20Extraction&body=Title%3A%20RealKIE%3A%20Five%20Novel%20Datasets%20for%20Enterprise%20Key%20Information%20Extraction%0AAuthor%3A%20Benjamin%20Townsend%20and%20Madison%20May%20and%20Katherine%20Mackowiak%20and%20Christopher%20Wells%0AAbstract%3A%20%20%20We%20introduce%20RealKIE%2C%20a%20benchmark%20of%20five%20challenging%20datasets%20aimed%20at%0Aadvancing%20key%20information%20extraction%20methods%2C%20with%20an%20emphasis%20on%20enterprise%0Aapplications.%20The%20datasets%20include%20a%20diverse%20range%20of%20documents%20including%20SEC%0AS1%20Filings%2C%20US%20Non-disclosure%20Agreements%2C%20UK%20Charity%20Reports%2C%20FCC%20Invoices%2C%20and%0AResource%20Contracts.%20Each%20presents%20unique%20challenges%3A%20poor%20text%20serialization%2C%0Asparse%20annotations%20in%20long%20documents%2C%20and%20complex%20tabular%20layouts.%20These%0Adatasets%20provide%20a%20realistic%20testing%20ground%20for%20key%20information%20extraction%0Atasks%20like%20investment%20analysis%20and%20contract%20analysis.%20In%20addition%20to%20presenting%0Athese%20datasets%2C%20we%20offer%20an%20in-depth%20description%20of%20the%20annotation%20process%2C%0Adocument%20processing%20techniques%2C%20and%20baseline%20modeling%20approaches.%20This%0Acontribution%20facilitates%20the%20development%20of%20NLP%20models%20capable%20of%20handling%0Apractical%20challenges%20and%20supports%20further%20research%20into%20information%20extraction%0Atechnologies%20applicable%20to%20industry-specific%20problems.%20The%20annotated%20data%2C%20OCR%0Aoutputs%2C%20and%20code%20to%20reproduce%20baselines%20are%20available%20to%20download%20at%0Ahttps%3A//indicodatasolutions.github.io/RealKIE/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealKIE%253A%2520Five%2520Novel%2520Datasets%2520for%2520Enterprise%2520Key%2520Information%2520Extraction%26entry.906535625%3DBenjamin%2520Townsend%2520and%2520Madison%2520May%2520and%2520Katherine%2520Mackowiak%2520and%2520Christopher%2520Wells%26entry.1292438233%3D%2520%2520We%2520introduce%2520RealKIE%252C%2520a%2520benchmark%2520of%2520five%2520challenging%2520datasets%2520aimed%2520at%250Aadvancing%2520key%2520information%2520extraction%2520methods%252C%2520with%2520an%2520emphasis%2520on%2520enterprise%250Aapplications.%2520The%2520datasets%2520include%2520a%2520diverse%2520range%2520of%2520documents%2520including%2520SEC%250AS1%2520Filings%252C%2520US%2520Non-disclosure%2520Agreements%252C%2520UK%2520Charity%2520Reports%252C%2520FCC%2520Invoices%252C%2520and%250AResource%2520Contracts.%2520Each%2520presents%2520unique%2520challenges%253A%2520poor%2520text%2520serialization%252C%250Asparse%2520annotations%2520in%2520long%2520documents%252C%2520and%2520complex%2520tabular%2520layouts.%2520These%250Adatasets%2520provide%2520a%2520realistic%2520testing%2520ground%2520for%2520key%2520information%2520extraction%250Atasks%2520like%2520investment%2520analysis%2520and%2520contract%2520analysis.%2520In%2520addition%2520to%2520presenting%250Athese%2520datasets%252C%2520we%2520offer%2520an%2520in-depth%2520description%2520of%2520the%2520annotation%2520process%252C%250Adocument%2520processing%2520techniques%252C%2520and%2520baseline%2520modeling%2520approaches.%2520This%250Acontribution%2520facilitates%2520the%2520development%2520of%2520NLP%2520models%2520capable%2520of%2520handling%250Apractical%2520challenges%2520and%2520supports%2520further%2520research%2520into%2520information%2520extraction%250Atechnologies%2520applicable%2520to%2520industry-specific%2520problems.%2520The%2520annotated%2520data%252C%2520OCR%250Aoutputs%252C%2520and%2520code%2520to%2520reproduce%2520baselines%2520are%2520available%2520to%2520download%2520at%250Ahttps%253A//indicodatasolutions.github.io/RealKIE/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.20101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealKIE%3A%20Five%20Novel%20Datasets%20for%20Enterprise%20Key%20Information%20Extraction&entry.906535625=Benjamin%20Townsend%20and%20Madison%20May%20and%20Katherine%20Mackowiak%20and%20Christopher%20Wells&entry.1292438233=%20%20We%20introduce%20RealKIE%2C%20a%20benchmark%20of%20five%20challenging%20datasets%20aimed%20at%0Aadvancing%20key%20information%20extraction%20methods%2C%20with%20an%20emphasis%20on%20enterprise%0Aapplications.%20The%20datasets%20include%20a%20diverse%20range%20of%20documents%20including%20SEC%0AS1%20Filings%2C%20US%20Non-disclosure%20Agreements%2C%20UK%20Charity%20Reports%2C%20FCC%20Invoices%2C%20and%0AResource%20Contracts.%20Each%20presents%20unique%20challenges%3A%20poor%20text%20serialization%2C%0Asparse%20annotations%20in%20long%20documents%2C%20and%20complex%20tabular%20layouts.%20These%0Adatasets%20provide%20a%20realistic%20testing%20ground%20for%20key%20information%20extraction%0Atasks%20like%20investment%20analysis%20and%20contract%20analysis.%20In%20addition%20to%20presenting%0Athese%20datasets%2C%20we%20offer%20an%20in-depth%20description%20of%20the%20annotation%20process%2C%0Adocument%20processing%20techniques%2C%20and%20baseline%20modeling%20approaches.%20This%0Acontribution%20facilitates%20the%20development%20of%20NLP%20models%20capable%20of%20handling%0Apractical%20challenges%20and%20supports%20further%20research%20into%20information%20extraction%0Atechnologies%20applicable%20to%20industry-specific%20problems.%20The%20annotated%20data%2C%20OCR%0Aoutputs%2C%20and%20code%20to%20reproduce%20baselines%20are%20available%20to%20download%20at%0Ahttps%3A//indicodatasolutions.github.io/RealKIE/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20101v2&entry.124074799=Read"},
{"title": "In-Field Mapping of Grape Yield and Quality with Illumination-Invariant\n  Deep Learning", "author": "Ciem Cornelissen and Sander De Coninck and Axel Willekens and Sam Leroux and Pieter Simoens", "abstract": "  This paper presents an end-to-end, IoT-enabled robotic system for the\nnon-destructive, real-time, and spatially-resolved mapping of grape yield and\nquality (Brix, Acidity) in vineyards. The system features a comprehensive\nanalytical pipeline that integrates two key modules: a high-performance model\nfor grape bunch detection and weight estimation, and a novel deep learning\nframework for quality assessment from hyperspectral (HSI) data. A critical\nbarrier to in-field HSI is the ``domain shift\" caused by variable illumination.\nTo overcome this, our quality assessment is powered by the Light-Invariant\nSpectral Autoencoder (LISA), a domain-adversarial framework that learns\nillumination-invariant features from uncalibrated data. We validated the\nsystem's robustness on a purpose-built HSI dataset spanning three distinct\nillumination domains: controlled artificial lighting (lab), and variable\nnatural sunlight captured in the morning and afternoon. Results show the\ncomplete pipeline achieves a recall (0.82) for bunch detection and a $R^2$\n(0.76) for weight prediction, while the LISA module improves quality prediction\ngeneralization by over 20% compared to the baselines. By combining these robust\nmodules, the system successfully generates high-resolution, georeferenced data\nof both grape yield and quality, providing actionable, data-driven insights for\nprecision viticulture.\n", "link": "http://arxiv.org/abs/2510.04864v1", "date": "2025-10-06", "relevancy": 2.1424, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5398}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5362}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Field%20Mapping%20of%20Grape%20Yield%20and%20Quality%20with%20Illumination-Invariant%0A%20%20Deep%20Learning&body=Title%3A%20In-Field%20Mapping%20of%20Grape%20Yield%20and%20Quality%20with%20Illumination-Invariant%0A%20%20Deep%20Learning%0AAuthor%3A%20Ciem%20Cornelissen%20and%20Sander%20De%20Coninck%20and%20Axel%20Willekens%20and%20Sam%20Leroux%20and%20Pieter%20Simoens%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20end-to-end%2C%20IoT-enabled%20robotic%20system%20for%20the%0Anon-destructive%2C%20real-time%2C%20and%20spatially-resolved%20mapping%20of%20grape%20yield%20and%0Aquality%20%28Brix%2C%20Acidity%29%20in%20vineyards.%20The%20system%20features%20a%20comprehensive%0Aanalytical%20pipeline%20that%20integrates%20two%20key%20modules%3A%20a%20high-performance%20model%0Afor%20grape%20bunch%20detection%20and%20weight%20estimation%2C%20and%20a%20novel%20deep%20learning%0Aframework%20for%20quality%20assessment%20from%20hyperspectral%20%28HSI%29%20data.%20A%20critical%0Abarrier%20to%20in-field%20HSI%20is%20the%20%60%60domain%20shift%22%20caused%20by%20variable%20illumination.%0ATo%20overcome%20this%2C%20our%20quality%20assessment%20is%20powered%20by%20the%20Light-Invariant%0ASpectral%20Autoencoder%20%28LISA%29%2C%20a%20domain-adversarial%20framework%20that%20learns%0Aillumination-invariant%20features%20from%20uncalibrated%20data.%20We%20validated%20the%0Asystem%27s%20robustness%20on%20a%20purpose-built%20HSI%20dataset%20spanning%20three%20distinct%0Aillumination%20domains%3A%20controlled%20artificial%20lighting%20%28lab%29%2C%20and%20variable%0Anatural%20sunlight%20captured%20in%20the%20morning%20and%20afternoon.%20Results%20show%20the%0Acomplete%20pipeline%20achieves%20a%20recall%20%280.82%29%20for%20bunch%20detection%20and%20a%20%24R%5E2%24%0A%280.76%29%20for%20weight%20prediction%2C%20while%20the%20LISA%20module%20improves%20quality%20prediction%0Ageneralization%20by%20over%2020%25%20compared%20to%20the%20baselines.%20By%20combining%20these%20robust%0Amodules%2C%20the%20system%20successfully%20generates%20high-resolution%2C%20georeferenced%20data%0Aof%20both%20grape%20yield%20and%20quality%2C%20providing%20actionable%2C%20data-driven%20insights%20for%0Aprecision%20viticulture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Field%2520Mapping%2520of%2520Grape%2520Yield%2520and%2520Quality%2520with%2520Illumination-Invariant%250A%2520%2520Deep%2520Learning%26entry.906535625%3DCiem%2520Cornelissen%2520and%2520Sander%2520De%2520Coninck%2520and%2520Axel%2520Willekens%2520and%2520Sam%2520Leroux%2520and%2520Pieter%2520Simoens%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520end-to-end%252C%2520IoT-enabled%2520robotic%2520system%2520for%2520the%250Anon-destructive%252C%2520real-time%252C%2520and%2520spatially-resolved%2520mapping%2520of%2520grape%2520yield%2520and%250Aquality%2520%2528Brix%252C%2520Acidity%2529%2520in%2520vineyards.%2520The%2520system%2520features%2520a%2520comprehensive%250Aanalytical%2520pipeline%2520that%2520integrates%2520two%2520key%2520modules%253A%2520a%2520high-performance%2520model%250Afor%2520grape%2520bunch%2520detection%2520and%2520weight%2520estimation%252C%2520and%2520a%2520novel%2520deep%2520learning%250Aframework%2520for%2520quality%2520assessment%2520from%2520hyperspectral%2520%2528HSI%2529%2520data.%2520A%2520critical%250Abarrier%2520to%2520in-field%2520HSI%2520is%2520the%2520%2560%2560domain%2520shift%2522%2520caused%2520by%2520variable%2520illumination.%250ATo%2520overcome%2520this%252C%2520our%2520quality%2520assessment%2520is%2520powered%2520by%2520the%2520Light-Invariant%250ASpectral%2520Autoencoder%2520%2528LISA%2529%252C%2520a%2520domain-adversarial%2520framework%2520that%2520learns%250Aillumination-invariant%2520features%2520from%2520uncalibrated%2520data.%2520We%2520validated%2520the%250Asystem%2527s%2520robustness%2520on%2520a%2520purpose-built%2520HSI%2520dataset%2520spanning%2520three%2520distinct%250Aillumination%2520domains%253A%2520controlled%2520artificial%2520lighting%2520%2528lab%2529%252C%2520and%2520variable%250Anatural%2520sunlight%2520captured%2520in%2520the%2520morning%2520and%2520afternoon.%2520Results%2520show%2520the%250Acomplete%2520pipeline%2520achieves%2520a%2520recall%2520%25280.82%2529%2520for%2520bunch%2520detection%2520and%2520a%2520%2524R%255E2%2524%250A%25280.76%2529%2520for%2520weight%2520prediction%252C%2520while%2520the%2520LISA%2520module%2520improves%2520quality%2520prediction%250Ageneralization%2520by%2520over%252020%2525%2520compared%2520to%2520the%2520baselines.%2520By%2520combining%2520these%2520robust%250Amodules%252C%2520the%2520system%2520successfully%2520generates%2520high-resolution%252C%2520georeferenced%2520data%250Aof%2520both%2520grape%2520yield%2520and%2520quality%252C%2520providing%2520actionable%252C%2520data-driven%2520insights%2520for%250Aprecision%2520viticulture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Field%20Mapping%20of%20Grape%20Yield%20and%20Quality%20with%20Illumination-Invariant%0A%20%20Deep%20Learning&entry.906535625=Ciem%20Cornelissen%20and%20Sander%20De%20Coninck%20and%20Axel%20Willekens%20and%20Sam%20Leroux%20and%20Pieter%20Simoens&entry.1292438233=%20%20This%20paper%20presents%20an%20end-to-end%2C%20IoT-enabled%20robotic%20system%20for%20the%0Anon-destructive%2C%20real-time%2C%20and%20spatially-resolved%20mapping%20of%20grape%20yield%20and%0Aquality%20%28Brix%2C%20Acidity%29%20in%20vineyards.%20The%20system%20features%20a%20comprehensive%0Aanalytical%20pipeline%20that%20integrates%20two%20key%20modules%3A%20a%20high-performance%20model%0Afor%20grape%20bunch%20detection%20and%20weight%20estimation%2C%20and%20a%20novel%20deep%20learning%0Aframework%20for%20quality%20assessment%20from%20hyperspectral%20%28HSI%29%20data.%20A%20critical%0Abarrier%20to%20in-field%20HSI%20is%20the%20%60%60domain%20shift%22%20caused%20by%20variable%20illumination.%0ATo%20overcome%20this%2C%20our%20quality%20assessment%20is%20powered%20by%20the%20Light-Invariant%0ASpectral%20Autoencoder%20%28LISA%29%2C%20a%20domain-adversarial%20framework%20that%20learns%0Aillumination-invariant%20features%20from%20uncalibrated%20data.%20We%20validated%20the%0Asystem%27s%20robustness%20on%20a%20purpose-built%20HSI%20dataset%20spanning%20three%20distinct%0Aillumination%20domains%3A%20controlled%20artificial%20lighting%20%28lab%29%2C%20and%20variable%0Anatural%20sunlight%20captured%20in%20the%20morning%20and%20afternoon.%20Results%20show%20the%0Acomplete%20pipeline%20achieves%20a%20recall%20%280.82%29%20for%20bunch%20detection%20and%20a%20%24R%5E2%24%0A%280.76%29%20for%20weight%20prediction%2C%20while%20the%20LISA%20module%20improves%20quality%20prediction%0Ageneralization%20by%20over%2020%25%20compared%20to%20the%20baselines.%20By%20combining%20these%20robust%0Amodules%2C%20the%20system%20successfully%20generates%20high-resolution%2C%20georeferenced%20data%0Aof%20both%20grape%20yield%20and%20quality%2C%20providing%20actionable%2C%20data-driven%20insights%20for%0Aprecision%20viticulture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04864v1&entry.124074799=Read"},
{"title": "CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery", "author": "Nathan Shankar and Pawel Ladosz and Hujun Yin", "abstract": "  This paper presents a novel approach for enabling robust robotic perception\nin dark environments using infrared (IR) stream. IR stream is less susceptible\nto noise than RGB in low-light conditions. However, it is dominated by active\nemitter patterns that hinder high-level tasks such as object detection,\ntracking and localisation. To address this, a U-Net-based architecture is\nproposed that reconstructs clean IR images from emitter-populated input,\nimproving both image quality and downstream robotic performance. This approach\noutperforms existing enhancement techniques and enables reliable operation of\nvision-driven robotic systems across illumination conditions from well-lit to\nextreme low-light scenes.\n", "link": "http://arxiv.org/abs/2510.04883v1", "date": "2025-10-06", "relevancy": 2.1421, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5441}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5328}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLEAR-IR%3A%20Clarity-Enhanced%20Active%20Reconstruction%20of%20Infrared%20Imagery&body=Title%3A%20CLEAR-IR%3A%20Clarity-Enhanced%20Active%20Reconstruction%20of%20Infrared%20Imagery%0AAuthor%3A%20Nathan%20Shankar%20and%20Pawel%20Ladosz%20and%20Hujun%20Yin%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20for%20enabling%20robust%20robotic%20perception%0Ain%20dark%20environments%20using%20infrared%20%28IR%29%20stream.%20IR%20stream%20is%20less%20susceptible%0Ato%20noise%20than%20RGB%20in%20low-light%20conditions.%20However%2C%20it%20is%20dominated%20by%20active%0Aemitter%20patterns%20that%20hinder%20high-level%20tasks%20such%20as%20object%20detection%2C%0Atracking%20and%20localisation.%20To%20address%20this%2C%20a%20U-Net-based%20architecture%20is%0Aproposed%20that%20reconstructs%20clean%20IR%20images%20from%20emitter-populated%20input%2C%0Aimproving%20both%20image%20quality%20and%20downstream%20robotic%20performance.%20This%20approach%0Aoutperforms%20existing%20enhancement%20techniques%20and%20enables%20reliable%20operation%20of%0Avision-driven%20robotic%20systems%20across%20illumination%20conditions%20from%20well-lit%20to%0Aextreme%20low-light%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLEAR-IR%253A%2520Clarity-Enhanced%2520Active%2520Reconstruction%2520of%2520Infrared%2520Imagery%26entry.906535625%3DNathan%2520Shankar%2520and%2520Pawel%2520Ladosz%2520and%2520Hujun%2520Yin%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520enabling%2520robust%2520robotic%2520perception%250Ain%2520dark%2520environments%2520using%2520infrared%2520%2528IR%2529%2520stream.%2520IR%2520stream%2520is%2520less%2520susceptible%250Ato%2520noise%2520than%2520RGB%2520in%2520low-light%2520conditions.%2520However%252C%2520it%2520is%2520dominated%2520by%2520active%250Aemitter%2520patterns%2520that%2520hinder%2520high-level%2520tasks%2520such%2520as%2520object%2520detection%252C%250Atracking%2520and%2520localisation.%2520To%2520address%2520this%252C%2520a%2520U-Net-based%2520architecture%2520is%250Aproposed%2520that%2520reconstructs%2520clean%2520IR%2520images%2520from%2520emitter-populated%2520input%252C%250Aimproving%2520both%2520image%2520quality%2520and%2520downstream%2520robotic%2520performance.%2520This%2520approach%250Aoutperforms%2520existing%2520enhancement%2520techniques%2520and%2520enables%2520reliable%2520operation%2520of%250Avision-driven%2520robotic%2520systems%2520across%2520illumination%2520conditions%2520from%2520well-lit%2520to%250Aextreme%2520low-light%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLEAR-IR%3A%20Clarity-Enhanced%20Active%20Reconstruction%20of%20Infrared%20Imagery&entry.906535625=Nathan%20Shankar%20and%20Pawel%20Ladosz%20and%20Hujun%20Yin&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20for%20enabling%20robust%20robotic%20perception%0Ain%20dark%20environments%20using%20infrared%20%28IR%29%20stream.%20IR%20stream%20is%20less%20susceptible%0Ato%20noise%20than%20RGB%20in%20low-light%20conditions.%20However%2C%20it%20is%20dominated%20by%20active%0Aemitter%20patterns%20that%20hinder%20high-level%20tasks%20such%20as%20object%20detection%2C%0Atracking%20and%20localisation.%20To%20address%20this%2C%20a%20U-Net-based%20architecture%20is%0Aproposed%20that%20reconstructs%20clean%20IR%20images%20from%20emitter-populated%20input%2C%0Aimproving%20both%20image%20quality%20and%20downstream%20robotic%20performance.%20This%20approach%0Aoutperforms%20existing%20enhancement%20techniques%20and%20enables%20reliable%20operation%20of%0Avision-driven%20robotic%20systems%20across%20illumination%20conditions%20from%20well-lit%20to%0Aextreme%20low-light%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04883v1&entry.124074799=Read"},
{"title": "Unsupervised Active Learning via Natural Feature Progressive Framework", "author": "Yuxi Liu and Catherine Lalman and Yimin Yang", "abstract": "  The effectiveness of modern deep learning models is predicated on the\navailability of large-scale, human-annotated datasets, a process that is\nnotoriously expensive and time-consuming. While Active Learning (AL) offers a\nstrategic solution by labeling only the most informative and representative\ndata, its iterative nature still necessitates significant human involvement.\nUnsupervised Active Learning (UAL) presents an alternative by shifting the\nannotation burden to a single, post-selection step. Unfortunately, prevailing\nUAL methods struggle to achieve state-of-the-art performance. These approaches\ntypically rely on local, gradient-based scoring for sample importance\nestimation, which not only makes them vulnerable to ambiguous and noisy data\nbut also hinders their capacity to select samples that adequately represent the\nfull data distribution. Moreover, their use of shallow, one-shot linear\nselection falls short of a true UAL paradigm. In this paper, we propose the\nNatural Feature Progressive Framework (NFPF), a UAL method that revolutionizes\nhow sample importance is measured. At its core, NFPF employs a Specific Feature\nLearning Machine (SFLM) to effectively quantify each sample's contribution to\nmodel performance. We further utilize the SFLM to define a powerful\nReconstruction Difference metric for initial sample selection. Our\ncomprehensive experiments show that NFPF significantly outperforms all\nestablished UAL methods and achieves performance on par with supervised AL\nmethods on vision datasets. Detailed ablation studies and qualitative\nvisualizations provide compelling evidence for NFPF's superior performance,\nenhanced robustness, and improved data distribution coverage.\n", "link": "http://arxiv.org/abs/2510.04939v1", "date": "2025-10-06", "relevancy": 2.1408, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5412}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5378}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Active%20Learning%20via%20Natural%20Feature%20Progressive%20Framework&body=Title%3A%20Unsupervised%20Active%20Learning%20via%20Natural%20Feature%20Progressive%20Framework%0AAuthor%3A%20Yuxi%20Liu%20and%20Catherine%20Lalman%20and%20Yimin%20Yang%0AAbstract%3A%20%20%20The%20effectiveness%20of%20modern%20deep%20learning%20models%20is%20predicated%20on%20the%0Aavailability%20of%20large-scale%2C%20human-annotated%20datasets%2C%20a%20process%20that%20is%0Anotoriously%20expensive%20and%20time-consuming.%20While%20Active%20Learning%20%28AL%29%20offers%20a%0Astrategic%20solution%20by%20labeling%20only%20the%20most%20informative%20and%20representative%0Adata%2C%20its%20iterative%20nature%20still%20necessitates%20significant%20human%20involvement.%0AUnsupervised%20Active%20Learning%20%28UAL%29%20presents%20an%20alternative%20by%20shifting%20the%0Aannotation%20burden%20to%20a%20single%2C%20post-selection%20step.%20Unfortunately%2C%20prevailing%0AUAL%20methods%20struggle%20to%20achieve%20state-of-the-art%20performance.%20These%20approaches%0Atypically%20rely%20on%20local%2C%20gradient-based%20scoring%20for%20sample%20importance%0Aestimation%2C%20which%20not%20only%20makes%20them%20vulnerable%20to%20ambiguous%20and%20noisy%20data%0Abut%20also%20hinders%20their%20capacity%20to%20select%20samples%20that%20adequately%20represent%20the%0Afull%20data%20distribution.%20Moreover%2C%20their%20use%20of%20shallow%2C%20one-shot%20linear%0Aselection%20falls%20short%20of%20a%20true%20UAL%20paradigm.%20In%20this%20paper%2C%20we%20propose%20the%0ANatural%20Feature%20Progressive%20Framework%20%28NFPF%29%2C%20a%20UAL%20method%20that%20revolutionizes%0Ahow%20sample%20importance%20is%20measured.%20At%20its%20core%2C%20NFPF%20employs%20a%20Specific%20Feature%0ALearning%20Machine%20%28SFLM%29%20to%20effectively%20quantify%20each%20sample%27s%20contribution%20to%0Amodel%20performance.%20We%20further%20utilize%20the%20SFLM%20to%20define%20a%20powerful%0AReconstruction%20Difference%20metric%20for%20initial%20sample%20selection.%20Our%0Acomprehensive%20experiments%20show%20that%20NFPF%20significantly%20outperforms%20all%0Aestablished%20UAL%20methods%20and%20achieves%20performance%20on%20par%20with%20supervised%20AL%0Amethods%20on%20vision%20datasets.%20Detailed%20ablation%20studies%20and%20qualitative%0Avisualizations%20provide%20compelling%20evidence%20for%20NFPF%27s%20superior%20performance%2C%0Aenhanced%20robustness%2C%20and%20improved%20data%20distribution%20coverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Active%2520Learning%2520via%2520Natural%2520Feature%2520Progressive%2520Framework%26entry.906535625%3DYuxi%2520Liu%2520and%2520Catherine%2520Lalman%2520and%2520Yimin%2520Yang%26entry.1292438233%3D%2520%2520The%2520effectiveness%2520of%2520modern%2520deep%2520learning%2520models%2520is%2520predicated%2520on%2520the%250Aavailability%2520of%2520large-scale%252C%2520human-annotated%2520datasets%252C%2520a%2520process%2520that%2520is%250Anotoriously%2520expensive%2520and%2520time-consuming.%2520While%2520Active%2520Learning%2520%2528AL%2529%2520offers%2520a%250Astrategic%2520solution%2520by%2520labeling%2520only%2520the%2520most%2520informative%2520and%2520representative%250Adata%252C%2520its%2520iterative%2520nature%2520still%2520necessitates%2520significant%2520human%2520involvement.%250AUnsupervised%2520Active%2520Learning%2520%2528UAL%2529%2520presents%2520an%2520alternative%2520by%2520shifting%2520the%250Aannotation%2520burden%2520to%2520a%2520single%252C%2520post-selection%2520step.%2520Unfortunately%252C%2520prevailing%250AUAL%2520methods%2520struggle%2520to%2520achieve%2520state-of-the-art%2520performance.%2520These%2520approaches%250Atypically%2520rely%2520on%2520local%252C%2520gradient-based%2520scoring%2520for%2520sample%2520importance%250Aestimation%252C%2520which%2520not%2520only%2520makes%2520them%2520vulnerable%2520to%2520ambiguous%2520and%2520noisy%2520data%250Abut%2520also%2520hinders%2520their%2520capacity%2520to%2520select%2520samples%2520that%2520adequately%2520represent%2520the%250Afull%2520data%2520distribution.%2520Moreover%252C%2520their%2520use%2520of%2520shallow%252C%2520one-shot%2520linear%250Aselection%2520falls%2520short%2520of%2520a%2520true%2520UAL%2520paradigm.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%250ANatural%2520Feature%2520Progressive%2520Framework%2520%2528NFPF%2529%252C%2520a%2520UAL%2520method%2520that%2520revolutionizes%250Ahow%2520sample%2520importance%2520is%2520measured.%2520At%2520its%2520core%252C%2520NFPF%2520employs%2520a%2520Specific%2520Feature%250ALearning%2520Machine%2520%2528SFLM%2529%2520to%2520effectively%2520quantify%2520each%2520sample%2527s%2520contribution%2520to%250Amodel%2520performance.%2520We%2520further%2520utilize%2520the%2520SFLM%2520to%2520define%2520a%2520powerful%250AReconstruction%2520Difference%2520metric%2520for%2520initial%2520sample%2520selection.%2520Our%250Acomprehensive%2520experiments%2520show%2520that%2520NFPF%2520significantly%2520outperforms%2520all%250Aestablished%2520UAL%2520methods%2520and%2520achieves%2520performance%2520on%2520par%2520with%2520supervised%2520AL%250Amethods%2520on%2520vision%2520datasets.%2520Detailed%2520ablation%2520studies%2520and%2520qualitative%250Avisualizations%2520provide%2520compelling%2520evidence%2520for%2520NFPF%2527s%2520superior%2520performance%252C%250Aenhanced%2520robustness%252C%2520and%2520improved%2520data%2520distribution%2520coverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Active%20Learning%20via%20Natural%20Feature%20Progressive%20Framework&entry.906535625=Yuxi%20Liu%20and%20Catherine%20Lalman%20and%20Yimin%20Yang&entry.1292438233=%20%20The%20effectiveness%20of%20modern%20deep%20learning%20models%20is%20predicated%20on%20the%0Aavailability%20of%20large-scale%2C%20human-annotated%20datasets%2C%20a%20process%20that%20is%0Anotoriously%20expensive%20and%20time-consuming.%20While%20Active%20Learning%20%28AL%29%20offers%20a%0Astrategic%20solution%20by%20labeling%20only%20the%20most%20informative%20and%20representative%0Adata%2C%20its%20iterative%20nature%20still%20necessitates%20significant%20human%20involvement.%0AUnsupervised%20Active%20Learning%20%28UAL%29%20presents%20an%20alternative%20by%20shifting%20the%0Aannotation%20burden%20to%20a%20single%2C%20post-selection%20step.%20Unfortunately%2C%20prevailing%0AUAL%20methods%20struggle%20to%20achieve%20state-of-the-art%20performance.%20These%20approaches%0Atypically%20rely%20on%20local%2C%20gradient-based%20scoring%20for%20sample%20importance%0Aestimation%2C%20which%20not%20only%20makes%20them%20vulnerable%20to%20ambiguous%20and%20noisy%20data%0Abut%20also%20hinders%20their%20capacity%20to%20select%20samples%20that%20adequately%20represent%20the%0Afull%20data%20distribution.%20Moreover%2C%20their%20use%20of%20shallow%2C%20one-shot%20linear%0Aselection%20falls%20short%20of%20a%20true%20UAL%20paradigm.%20In%20this%20paper%2C%20we%20propose%20the%0ANatural%20Feature%20Progressive%20Framework%20%28NFPF%29%2C%20a%20UAL%20method%20that%20revolutionizes%0Ahow%20sample%20importance%20is%20measured.%20At%20its%20core%2C%20NFPF%20employs%20a%20Specific%20Feature%0ALearning%20Machine%20%28SFLM%29%20to%20effectively%20quantify%20each%20sample%27s%20contribution%20to%0Amodel%20performance.%20We%20further%20utilize%20the%20SFLM%20to%20define%20a%20powerful%0AReconstruction%20Difference%20metric%20for%20initial%20sample%20selection.%20Our%0Acomprehensive%20experiments%20show%20that%20NFPF%20significantly%20outperforms%20all%0Aestablished%20UAL%20methods%20and%20achieves%20performance%20on%20par%20with%20supervised%20AL%0Amethods%20on%20vision%20datasets.%20Detailed%20ablation%20studies%20and%20qualitative%0Avisualizations%20provide%20compelling%20evidence%20for%20NFPF%27s%20superior%20performance%2C%0Aenhanced%20robustness%2C%20and%20improved%20data%20distribution%20coverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04939v1&entry.124074799=Read"},
{"title": "Conformal Prediction for Long-Tailed Classification", "author": "Tiffany Ding and Jean-Baptiste Fermanian and Joseph Salmon", "abstract": "  Many real-world classification problems, such as plant identification, have\nextremely long-tailed class distributions. In order for prediction sets to be\nuseful in such settings, they should (i) provide good class-conditional\ncoverage, ensuring that rare classes are not systematically omitted from the\nprediction sets, and (ii) be a reasonable size, allowing users to easily verify\ncandidate labels. Unfortunately, existing conformal prediction methods, when\napplied to the long-tailed setting, force practitioners to make a binary choice\nbetween small sets with poor class-conditional coverage or sets with very good\nclass-conditional coverage but that are extremely large. We propose methods\nwith guaranteed marginal coverage that smoothly trade off between set size and\nclass-conditional coverage. First, we introduce a new conformal score function\ncalled prevalence-adjusted softmax that targets macro-coverage, a relaxed\nnotion of class-conditional coverage. Second, we propose a new procedure that\ninterpolates between marginal and class-conditional conformal prediction by\nlinearly interpolating their conformal score thresholds. We demonstrate our\nmethods on Pl@ntNet-300K and iNaturalist-2018, two long-tailed image datasets\nwith 1,081 and 8,142 classes, respectively.\n", "link": "http://arxiv.org/abs/2507.06867v2", "date": "2025-10-06", "relevancy": 2.1113, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4941}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Prediction%20for%20Long-Tailed%20Classification&body=Title%3A%20Conformal%20Prediction%20for%20Long-Tailed%20Classification%0AAuthor%3A%20Tiffany%20Ding%20and%20Jean-Baptiste%20Fermanian%20and%20Joseph%20Salmon%0AAbstract%3A%20%20%20Many%20real-world%20classification%20problems%2C%20such%20as%20plant%20identification%2C%20have%0Aextremely%20long-tailed%20class%20distributions.%20In%20order%20for%20prediction%20sets%20to%20be%0Auseful%20in%20such%20settings%2C%20they%20should%20%28i%29%20provide%20good%20class-conditional%0Acoverage%2C%20ensuring%20that%20rare%20classes%20are%20not%20systematically%20omitted%20from%20the%0Aprediction%20sets%2C%20and%20%28ii%29%20be%20a%20reasonable%20size%2C%20allowing%20users%20to%20easily%20verify%0Acandidate%20labels.%20Unfortunately%2C%20existing%20conformal%20prediction%20methods%2C%20when%0Aapplied%20to%20the%20long-tailed%20setting%2C%20force%20practitioners%20to%20make%20a%20binary%20choice%0Abetween%20small%20sets%20with%20poor%20class-conditional%20coverage%20or%20sets%20with%20very%20good%0Aclass-conditional%20coverage%20but%20that%20are%20extremely%20large.%20We%20propose%20methods%0Awith%20guaranteed%20marginal%20coverage%20that%20smoothly%20trade%20off%20between%20set%20size%20and%0Aclass-conditional%20coverage.%20First%2C%20we%20introduce%20a%20new%20conformal%20score%20function%0Acalled%20prevalence-adjusted%20softmax%20that%20targets%20macro-coverage%2C%20a%20relaxed%0Anotion%20of%20class-conditional%20coverage.%20Second%2C%20we%20propose%20a%20new%20procedure%20that%0Ainterpolates%20between%20marginal%20and%20class-conditional%20conformal%20prediction%20by%0Alinearly%20interpolating%20their%20conformal%20score%20thresholds.%20We%20demonstrate%20our%0Amethods%20on%20Pl%40ntNet-300K%20and%20iNaturalist-2018%2C%20two%20long-tailed%20image%20datasets%0Awith%201%2C081%20and%208%2C142%20classes%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06867v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Prediction%2520for%2520Long-Tailed%2520Classification%26entry.906535625%3DTiffany%2520Ding%2520and%2520Jean-Baptiste%2520Fermanian%2520and%2520Joseph%2520Salmon%26entry.1292438233%3D%2520%2520Many%2520real-world%2520classification%2520problems%252C%2520such%2520as%2520plant%2520identification%252C%2520have%250Aextremely%2520long-tailed%2520class%2520distributions.%2520In%2520order%2520for%2520prediction%2520sets%2520to%2520be%250Auseful%2520in%2520such%2520settings%252C%2520they%2520should%2520%2528i%2529%2520provide%2520good%2520class-conditional%250Acoverage%252C%2520ensuring%2520that%2520rare%2520classes%2520are%2520not%2520systematically%2520omitted%2520from%2520the%250Aprediction%2520sets%252C%2520and%2520%2528ii%2529%2520be%2520a%2520reasonable%2520size%252C%2520allowing%2520users%2520to%2520easily%2520verify%250Acandidate%2520labels.%2520Unfortunately%252C%2520existing%2520conformal%2520prediction%2520methods%252C%2520when%250Aapplied%2520to%2520the%2520long-tailed%2520setting%252C%2520force%2520practitioners%2520to%2520make%2520a%2520binary%2520choice%250Abetween%2520small%2520sets%2520with%2520poor%2520class-conditional%2520coverage%2520or%2520sets%2520with%2520very%2520good%250Aclass-conditional%2520coverage%2520but%2520that%2520are%2520extremely%2520large.%2520We%2520propose%2520methods%250Awith%2520guaranteed%2520marginal%2520coverage%2520that%2520smoothly%2520trade%2520off%2520between%2520set%2520size%2520and%250Aclass-conditional%2520coverage.%2520First%252C%2520we%2520introduce%2520a%2520new%2520conformal%2520score%2520function%250Acalled%2520prevalence-adjusted%2520softmax%2520that%2520targets%2520macro-coverage%252C%2520a%2520relaxed%250Anotion%2520of%2520class-conditional%2520coverage.%2520Second%252C%2520we%2520propose%2520a%2520new%2520procedure%2520that%250Ainterpolates%2520between%2520marginal%2520and%2520class-conditional%2520conformal%2520prediction%2520by%250Alinearly%2520interpolating%2520their%2520conformal%2520score%2520thresholds.%2520We%2520demonstrate%2520our%250Amethods%2520on%2520Pl%2540ntNet-300K%2520and%2520iNaturalist-2018%252C%2520two%2520long-tailed%2520image%2520datasets%250Awith%25201%252C081%2520and%25208%252C142%2520classes%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06867v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Prediction%20for%20Long-Tailed%20Classification&entry.906535625=Tiffany%20Ding%20and%20Jean-Baptiste%20Fermanian%20and%20Joseph%20Salmon&entry.1292438233=%20%20Many%20real-world%20classification%20problems%2C%20such%20as%20plant%20identification%2C%20have%0Aextremely%20long-tailed%20class%20distributions.%20In%20order%20for%20prediction%20sets%20to%20be%0Auseful%20in%20such%20settings%2C%20they%20should%20%28i%29%20provide%20good%20class-conditional%0Acoverage%2C%20ensuring%20that%20rare%20classes%20are%20not%20systematically%20omitted%20from%20the%0Aprediction%20sets%2C%20and%20%28ii%29%20be%20a%20reasonable%20size%2C%20allowing%20users%20to%20easily%20verify%0Acandidate%20labels.%20Unfortunately%2C%20existing%20conformal%20prediction%20methods%2C%20when%0Aapplied%20to%20the%20long-tailed%20setting%2C%20force%20practitioners%20to%20make%20a%20binary%20choice%0Abetween%20small%20sets%20with%20poor%20class-conditional%20coverage%20or%20sets%20with%20very%20good%0Aclass-conditional%20coverage%20but%20that%20are%20extremely%20large.%20We%20propose%20methods%0Awith%20guaranteed%20marginal%20coverage%20that%20smoothly%20trade%20off%20between%20set%20size%20and%0Aclass-conditional%20coverage.%20First%2C%20we%20introduce%20a%20new%20conformal%20score%20function%0Acalled%20prevalence-adjusted%20softmax%20that%20targets%20macro-coverage%2C%20a%20relaxed%0Anotion%20of%20class-conditional%20coverage.%20Second%2C%20we%20propose%20a%20new%20procedure%20that%0Ainterpolates%20between%20marginal%20and%20class-conditional%20conformal%20prediction%20by%0Alinearly%20interpolating%20their%20conformal%20score%20thresholds.%20We%20demonstrate%20our%0Amethods%20on%20Pl%40ntNet-300K%20and%20iNaturalist-2018%2C%20two%20long-tailed%20image%20datasets%0Awith%201%2C081%20and%208%2C142%20classes%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06867v2&entry.124074799=Read"},
{"title": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination\n  Detection in Large Language Models", "author": "Amir Hameed Mir", "abstract": "  Large Language Models (LLMs) often produce fluent yet factually incorrect\nstatements-a phenomenon known as hallucination-posing serious risks in\nhigh-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric\nframework for hallucination detection that analyzes the evolution of\nhidden-state semantics across transformer layers. Unlike prior methods that\nrely on multiple sampling passes or external verification sources, LSD operates\nintrinsically within the model's representational space. Using margin-based\ncontrastive learning, LSD aligns hidden activations with ground-truth\nembeddings derived from a factual encoder, revealing a distinct separation in\nsemantic trajectories: factual responses preserve stable alignment, while\nhallucinations exhibit pronounced semantic drift across depth. Evaluated on the\nTruthfulQA and synthetic factual-hallucination datasets, LSD achieves an\nF1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming\nSelfCheckGPT and Semantic Entropy baselines while requiring only a single\nforward pass. This efficiency yields a 5-20x speedup over sampling-based\nmethods without sacrificing precision or interpretability. LSD offers a\nscalable, model-agnostic mechanism for real-time hallucination monitoring and\nprovides new insights into the geometry of factual consistency within large\nlanguage models.\n", "link": "http://arxiv.org/abs/2510.04933v1", "date": "2025-10-06", "relevancy": 2.1081, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5337}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Geometry%20of%20Truth%3A%20Layer-wise%20Semantic%20Dynamics%20for%20Hallucination%0A%20%20Detection%20in%20Large%20Language%20Models&body=Title%3A%20The%20Geometry%20of%20Truth%3A%20Layer-wise%20Semantic%20Dynamics%20for%20Hallucination%0A%20%20Detection%20in%20Large%20Language%20Models%0AAuthor%3A%20Amir%20Hameed%20Mir%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20often%20produce%20fluent%20yet%20factually%20incorrect%0Astatements-a%20phenomenon%20known%20as%20hallucination-posing%20serious%20risks%20in%0Ahigh-stakes%20domains.%20We%20present%20Layer-wise%20Semantic%20Dynamics%20%28LSD%29%2C%20a%20geometric%0Aframework%20for%20hallucination%20detection%20that%20analyzes%20the%20evolution%20of%0Ahidden-state%20semantics%20across%20transformer%20layers.%20Unlike%20prior%20methods%20that%0Arely%20on%20multiple%20sampling%20passes%20or%20external%20verification%20sources%2C%20LSD%20operates%0Aintrinsically%20within%20the%20model%27s%20representational%20space.%20Using%20margin-based%0Acontrastive%20learning%2C%20LSD%20aligns%20hidden%20activations%20with%20ground-truth%0Aembeddings%20derived%20from%20a%20factual%20encoder%2C%20revealing%20a%20distinct%20separation%20in%0Asemantic%20trajectories%3A%20factual%20responses%20preserve%20stable%20alignment%2C%20while%0Ahallucinations%20exhibit%20pronounced%20semantic%20drift%20across%20depth.%20Evaluated%20on%20the%0ATruthfulQA%20and%20synthetic%20factual-hallucination%20datasets%2C%20LSD%20achieves%20an%0AF1-score%20of%200.92%2C%20AUROC%20of%200.96%2C%20and%20clustering%20accuracy%20of%200.89%2C%20outperforming%0ASelfCheckGPT%20and%20Semantic%20Entropy%20baselines%20while%20requiring%20only%20a%20single%0Aforward%20pass.%20This%20efficiency%20yields%20a%205-20x%20speedup%20over%20sampling-based%0Amethods%20without%20sacrificing%20precision%20or%20interpretability.%20LSD%20offers%20a%0Ascalable%2C%20model-agnostic%20mechanism%20for%20real-time%20hallucination%20monitoring%20and%0Aprovides%20new%20insights%20into%20the%20geometry%20of%20factual%20consistency%20within%20large%0Alanguage%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Geometry%2520of%2520Truth%253A%2520Layer-wise%2520Semantic%2520Dynamics%2520for%2520Hallucination%250A%2520%2520Detection%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DAmir%2520Hameed%2520Mir%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520produce%2520fluent%2520yet%2520factually%2520incorrect%250Astatements-a%2520phenomenon%2520known%2520as%2520hallucination-posing%2520serious%2520risks%2520in%250Ahigh-stakes%2520domains.%2520We%2520present%2520Layer-wise%2520Semantic%2520Dynamics%2520%2528LSD%2529%252C%2520a%2520geometric%250Aframework%2520for%2520hallucination%2520detection%2520that%2520analyzes%2520the%2520evolution%2520of%250Ahidden-state%2520semantics%2520across%2520transformer%2520layers.%2520Unlike%2520prior%2520methods%2520that%250Arely%2520on%2520multiple%2520sampling%2520passes%2520or%2520external%2520verification%2520sources%252C%2520LSD%2520operates%250Aintrinsically%2520within%2520the%2520model%2527s%2520representational%2520space.%2520Using%2520margin-based%250Acontrastive%2520learning%252C%2520LSD%2520aligns%2520hidden%2520activations%2520with%2520ground-truth%250Aembeddings%2520derived%2520from%2520a%2520factual%2520encoder%252C%2520revealing%2520a%2520distinct%2520separation%2520in%250Asemantic%2520trajectories%253A%2520factual%2520responses%2520preserve%2520stable%2520alignment%252C%2520while%250Ahallucinations%2520exhibit%2520pronounced%2520semantic%2520drift%2520across%2520depth.%2520Evaluated%2520on%2520the%250ATruthfulQA%2520and%2520synthetic%2520factual-hallucination%2520datasets%252C%2520LSD%2520achieves%2520an%250AF1-score%2520of%25200.92%252C%2520AUROC%2520of%25200.96%252C%2520and%2520clustering%2520accuracy%2520of%25200.89%252C%2520outperforming%250ASelfCheckGPT%2520and%2520Semantic%2520Entropy%2520baselines%2520while%2520requiring%2520only%2520a%2520single%250Aforward%2520pass.%2520This%2520efficiency%2520yields%2520a%25205-20x%2520speedup%2520over%2520sampling-based%250Amethods%2520without%2520sacrificing%2520precision%2520or%2520interpretability.%2520LSD%2520offers%2520a%250Ascalable%252C%2520model-agnostic%2520mechanism%2520for%2520real-time%2520hallucination%2520monitoring%2520and%250Aprovides%2520new%2520insights%2520into%2520the%2520geometry%2520of%2520factual%2520consistency%2520within%2520large%250Alanguage%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Geometry%20of%20Truth%3A%20Layer-wise%20Semantic%20Dynamics%20for%20Hallucination%0A%20%20Detection%20in%20Large%20Language%20Models&entry.906535625=Amir%20Hameed%20Mir&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20often%20produce%20fluent%20yet%20factually%20incorrect%0Astatements-a%20phenomenon%20known%20as%20hallucination-posing%20serious%20risks%20in%0Ahigh-stakes%20domains.%20We%20present%20Layer-wise%20Semantic%20Dynamics%20%28LSD%29%2C%20a%20geometric%0Aframework%20for%20hallucination%20detection%20that%20analyzes%20the%20evolution%20of%0Ahidden-state%20semantics%20across%20transformer%20layers.%20Unlike%20prior%20methods%20that%0Arely%20on%20multiple%20sampling%20passes%20or%20external%20verification%20sources%2C%20LSD%20operates%0Aintrinsically%20within%20the%20model%27s%20representational%20space.%20Using%20margin-based%0Acontrastive%20learning%2C%20LSD%20aligns%20hidden%20activations%20with%20ground-truth%0Aembeddings%20derived%20from%20a%20factual%20encoder%2C%20revealing%20a%20distinct%20separation%20in%0Asemantic%20trajectories%3A%20factual%20responses%20preserve%20stable%20alignment%2C%20while%0Ahallucinations%20exhibit%20pronounced%20semantic%20drift%20across%20depth.%20Evaluated%20on%20the%0ATruthfulQA%20and%20synthetic%20factual-hallucination%20datasets%2C%20LSD%20achieves%20an%0AF1-score%20of%200.92%2C%20AUROC%20of%200.96%2C%20and%20clustering%20accuracy%20of%200.89%2C%20outperforming%0ASelfCheckGPT%20and%20Semantic%20Entropy%20baselines%20while%20requiring%20only%20a%20single%0Aforward%20pass.%20This%20efficiency%20yields%20a%205-20x%20speedup%20over%20sampling-based%0Amethods%20without%20sacrificing%20precision%20or%20interpretability.%20LSD%20offers%20a%0Ascalable%2C%20model-agnostic%20mechanism%20for%20real-time%20hallucination%20monitoring%20and%0Aprovides%20new%20insights%20into%20the%20geometry%20of%20factual%20consistency%20within%20large%0Alanguage%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04933v1&entry.124074799=Read"},
{"title": "A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised\n  Domain Adaptation", "author": "Jiaping Yu and Muli Yang and Jiapeng Ji and Jiexi Yan and Cheng Deng", "abstract": "  Source-Free Unsupervised Domain Adaptation (SFUDA) addresses the realistic\nchallenge of adapting a source-trained model to a target domain without access\nto the source data, driven by concerns over privacy and cost. Existing SFUDA\nmethods either exploit only the source model's predictions or fine-tune large\nmultimodal models, yet both neglect complementary insights and the latent\nstructure of target data. In this paper, we propose the Experts Cooperative\nLearning (EXCL). EXCL contains the Dual Experts framework and\nRetrieval-Augmentation-Interaction optimization pipeline. The Dual Experts\nframework places a frozen source-domain model (augmented with Conv-Adapter) and\na pretrained vision-language model (with a trainable text prompt) on equal\nfooting to mine consensus knowledge from unlabeled target samples. To\neffectively train these plug-in modules under purely unsupervised conditions,\nwe introduce Retrieval-Augmented-Interaction(RAIN), a three-stage pipeline that\n(1) collaboratively retrieves pseudo-source and complex target samples, (2)\nseparately fine-tunes each expert on its respective sample set, and (3)\nenforces learning object consistency via a shared learning result. Extensive\nexperiments on four benchmark datasets demonstrate that our approach matches\nstate-of-the-art performance.\n", "link": "http://arxiv.org/abs/2509.22229v2", "date": "2025-10-06", "relevancy": 2.0918, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5312}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Tale%20of%20Two%20Experts%3A%20Cooperative%20Learning%20for%20Source-Free%20Unsupervised%0A%20%20Domain%20Adaptation&body=Title%3A%20A%20Tale%20of%20Two%20Experts%3A%20Cooperative%20Learning%20for%20Source-Free%20Unsupervised%0A%20%20Domain%20Adaptation%0AAuthor%3A%20Jiaping%20Yu%20and%20Muli%20Yang%20and%20Jiapeng%20Ji%20and%20Jiexi%20Yan%20and%20Cheng%20Deng%0AAbstract%3A%20%20%20Source-Free%20Unsupervised%20Domain%20Adaptation%20%28SFUDA%29%20addresses%20the%20realistic%0Achallenge%20of%20adapting%20a%20source-trained%20model%20to%20a%20target%20domain%20without%20access%0Ato%20the%20source%20data%2C%20driven%20by%20concerns%20over%20privacy%20and%20cost.%20Existing%20SFUDA%0Amethods%20either%20exploit%20only%20the%20source%20model%27s%20predictions%20or%20fine-tune%20large%0Amultimodal%20models%2C%20yet%20both%20neglect%20complementary%20insights%20and%20the%20latent%0Astructure%20of%20target%20data.%20In%20this%20paper%2C%20we%20propose%20the%20Experts%20Cooperative%0ALearning%20%28EXCL%29.%20EXCL%20contains%20the%20Dual%20Experts%20framework%20and%0ARetrieval-Augmentation-Interaction%20optimization%20pipeline.%20The%20Dual%20Experts%0Aframework%20places%20a%20frozen%20source-domain%20model%20%28augmented%20with%20Conv-Adapter%29%20and%0Aa%20pretrained%20vision-language%20model%20%28with%20a%20trainable%20text%20prompt%29%20on%20equal%0Afooting%20to%20mine%20consensus%20knowledge%20from%20unlabeled%20target%20samples.%20To%0Aeffectively%20train%20these%20plug-in%20modules%20under%20purely%20unsupervised%20conditions%2C%0Awe%20introduce%20Retrieval-Augmented-Interaction%28RAIN%29%2C%20a%20three-stage%20pipeline%20that%0A%281%29%20collaboratively%20retrieves%20pseudo-source%20and%20complex%20target%20samples%2C%20%282%29%0Aseparately%20fine-tunes%20each%20expert%20on%20its%20respective%20sample%20set%2C%20and%20%283%29%0Aenforces%20learning%20object%20consistency%20via%20a%20shared%20learning%20result.%20Extensive%0Aexperiments%20on%20four%20benchmark%20datasets%20demonstrate%20that%20our%20approach%20matches%0Astate-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22229v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Tale%2520of%2520Two%2520Experts%253A%2520Cooperative%2520Learning%2520for%2520Source-Free%2520Unsupervised%250A%2520%2520Domain%2520Adaptation%26entry.906535625%3DJiaping%2520Yu%2520and%2520Muli%2520Yang%2520and%2520Jiapeng%2520Ji%2520and%2520Jiexi%2520Yan%2520and%2520Cheng%2520Deng%26entry.1292438233%3D%2520%2520Source-Free%2520Unsupervised%2520Domain%2520Adaptation%2520%2528SFUDA%2529%2520addresses%2520the%2520realistic%250Achallenge%2520of%2520adapting%2520a%2520source-trained%2520model%2520to%2520a%2520target%2520domain%2520without%2520access%250Ato%2520the%2520source%2520data%252C%2520driven%2520by%2520concerns%2520over%2520privacy%2520and%2520cost.%2520Existing%2520SFUDA%250Amethods%2520either%2520exploit%2520only%2520the%2520source%2520model%2527s%2520predictions%2520or%2520fine-tune%2520large%250Amultimodal%2520models%252C%2520yet%2520both%2520neglect%2520complementary%2520insights%2520and%2520the%2520latent%250Astructure%2520of%2520target%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Experts%2520Cooperative%250ALearning%2520%2528EXCL%2529.%2520EXCL%2520contains%2520the%2520Dual%2520Experts%2520framework%2520and%250ARetrieval-Augmentation-Interaction%2520optimization%2520pipeline.%2520The%2520Dual%2520Experts%250Aframework%2520places%2520a%2520frozen%2520source-domain%2520model%2520%2528augmented%2520with%2520Conv-Adapter%2529%2520and%250Aa%2520pretrained%2520vision-language%2520model%2520%2528with%2520a%2520trainable%2520text%2520prompt%2529%2520on%2520equal%250Afooting%2520to%2520mine%2520consensus%2520knowledge%2520from%2520unlabeled%2520target%2520samples.%2520To%250Aeffectively%2520train%2520these%2520plug-in%2520modules%2520under%2520purely%2520unsupervised%2520conditions%252C%250Awe%2520introduce%2520Retrieval-Augmented-Interaction%2528RAIN%2529%252C%2520a%2520three-stage%2520pipeline%2520that%250A%25281%2529%2520collaboratively%2520retrieves%2520pseudo-source%2520and%2520complex%2520target%2520samples%252C%2520%25282%2529%250Aseparately%2520fine-tunes%2520each%2520expert%2520on%2520its%2520respective%2520sample%2520set%252C%2520and%2520%25283%2529%250Aenforces%2520learning%2520object%2520consistency%2520via%2520a%2520shared%2520learning%2520result.%2520Extensive%250Aexperiments%2520on%2520four%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520matches%250Astate-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22229v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Tale%20of%20Two%20Experts%3A%20Cooperative%20Learning%20for%20Source-Free%20Unsupervised%0A%20%20Domain%20Adaptation&entry.906535625=Jiaping%20Yu%20and%20Muli%20Yang%20and%20Jiapeng%20Ji%20and%20Jiexi%20Yan%20and%20Cheng%20Deng&entry.1292438233=%20%20Source-Free%20Unsupervised%20Domain%20Adaptation%20%28SFUDA%29%20addresses%20the%20realistic%0Achallenge%20of%20adapting%20a%20source-trained%20model%20to%20a%20target%20domain%20without%20access%0Ato%20the%20source%20data%2C%20driven%20by%20concerns%20over%20privacy%20and%20cost.%20Existing%20SFUDA%0Amethods%20either%20exploit%20only%20the%20source%20model%27s%20predictions%20or%20fine-tune%20large%0Amultimodal%20models%2C%20yet%20both%20neglect%20complementary%20insights%20and%20the%20latent%0Astructure%20of%20target%20data.%20In%20this%20paper%2C%20we%20propose%20the%20Experts%20Cooperative%0ALearning%20%28EXCL%29.%20EXCL%20contains%20the%20Dual%20Experts%20framework%20and%0ARetrieval-Augmentation-Interaction%20optimization%20pipeline.%20The%20Dual%20Experts%0Aframework%20places%20a%20frozen%20source-domain%20model%20%28augmented%20with%20Conv-Adapter%29%20and%0Aa%20pretrained%20vision-language%20model%20%28with%20a%20trainable%20text%20prompt%29%20on%20equal%0Afooting%20to%20mine%20consensus%20knowledge%20from%20unlabeled%20target%20samples.%20To%0Aeffectively%20train%20these%20plug-in%20modules%20under%20purely%20unsupervised%20conditions%2C%0Awe%20introduce%20Retrieval-Augmented-Interaction%28RAIN%29%2C%20a%20three-stage%20pipeline%20that%0A%281%29%20collaboratively%20retrieves%20pseudo-source%20and%20complex%20target%20samples%2C%20%282%29%0Aseparately%20fine-tunes%20each%20expert%20on%20its%20respective%20sample%20set%2C%20and%20%283%29%0Aenforces%20learning%20object%20consistency%20via%20a%20shared%20learning%20result.%20Extensive%0Aexperiments%20on%20four%20benchmark%20datasets%20demonstrate%20that%20our%20approach%20matches%0Astate-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22229v2&entry.124074799=Read"},
{"title": "MALT: Improving Reasoning with Multi-Agent LLM Training", "author": "Sumeet Ramesh Motwani and Chandler Smith and Rocktim Jyoti Das and Rafael Rafailov and Ivan Laptev and Philip H. S. Torr and Fabio Pizzati and Ronald Clark and Christian Schroeder de Witt", "abstract": "  Large Language Models (LLMs) often produce answers with a single\nchain-of-thought, which restricts their ability to explore reasoning paths or\nself-correct flawed outputs in complex tasks. In this paper, we introduce MALT\n(Multi-Agent LLM Training), a novel post-training strategy that divides the\nreasoning process into generation, verification, and refinement steps using a\nsequential pipeline of heterogeneous agents. During data generation, each agent\nis repeatedly sampled to form a multi-agent search tree, where final outputs\nare graded against ground-truth data. We then apply value iteration to\npropagate reward signals back to each role-conditioned model, automatically\nproducing multi-agent post-training data without human or teacher-model\nsupervision. Our off-policy approach allows each agent to specialize by\nlearning from correct and incorrect trajectories, ultimately improving the\nend-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same\nbaseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40%\nrespectively, making it an important advance towards multi-agent cooperative\ntraining.\n", "link": "http://arxiv.org/abs/2412.01928v3", "date": "2025-10-06", "relevancy": 2.0808, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5286}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5152}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MALT%3A%20Improving%20Reasoning%20with%20Multi-Agent%20LLM%20Training&body=Title%3A%20MALT%3A%20Improving%20Reasoning%20with%20Multi-Agent%20LLM%20Training%0AAuthor%3A%20Sumeet%20Ramesh%20Motwani%20and%20Chandler%20Smith%20and%20Rocktim%20Jyoti%20Das%20and%20Rafael%20Rafailov%20and%20Ivan%20Laptev%20and%20Philip%20H.%20S.%20Torr%20and%20Fabio%20Pizzati%20and%20Ronald%20Clark%20and%20Christian%20Schroeder%20de%20Witt%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20often%20produce%20answers%20with%20a%20single%0Achain-of-thought%2C%20which%20restricts%20their%20ability%20to%20explore%20reasoning%20paths%20or%0Aself-correct%20flawed%20outputs%20in%20complex%20tasks.%20In%20this%20paper%2C%20we%20introduce%20MALT%0A%28Multi-Agent%20LLM%20Training%29%2C%20a%20novel%20post-training%20strategy%20that%20divides%20the%0Areasoning%20process%20into%20generation%2C%20verification%2C%20and%20refinement%20steps%20using%20a%0Asequential%20pipeline%20of%20heterogeneous%20agents.%20During%20data%20generation%2C%20each%20agent%0Ais%20repeatedly%20sampled%20to%20form%20a%20multi-agent%20search%20tree%2C%20where%20final%20outputs%0Aare%20graded%20against%20ground-truth%20data.%20We%20then%20apply%20value%20iteration%20to%0Apropagate%20reward%20signals%20back%20to%20each%20role-conditioned%20model%2C%20automatically%0Aproducing%20multi-agent%20post-training%20data%20without%20human%20or%20teacher-model%0Asupervision.%20Our%20off-policy%20approach%20allows%20each%20agent%20to%20specialize%20by%0Alearning%20from%20correct%20and%20incorrect%20trajectories%2C%20ultimately%20improving%20the%0Aend-to-end%20reasoning%20chain.%20On%20MATH%2C%20GSM8K%2C%20and%20CSQA%2C%20MALT%20surpasses%20the%20same%0Abaseline%20LLM%20with%20a%20relative%20improvement%20of%2015.66%25%2C%207.42%25%2C%20and%209.40%25%0Arespectively%2C%20making%20it%20an%20important%20advance%20towards%20multi-agent%20cooperative%0Atraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01928v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMALT%253A%2520Improving%2520Reasoning%2520with%2520Multi-Agent%2520LLM%2520Training%26entry.906535625%3DSumeet%2520Ramesh%2520Motwani%2520and%2520Chandler%2520Smith%2520and%2520Rocktim%2520Jyoti%2520Das%2520and%2520Rafael%2520Rafailov%2520and%2520Ivan%2520Laptev%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Fabio%2520Pizzati%2520and%2520Ronald%2520Clark%2520and%2520Christian%2520Schroeder%2520de%2520Witt%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520produce%2520answers%2520with%2520a%2520single%250Achain-of-thought%252C%2520which%2520restricts%2520their%2520ability%2520to%2520explore%2520reasoning%2520paths%2520or%250Aself-correct%2520flawed%2520outputs%2520in%2520complex%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MALT%250A%2528Multi-Agent%2520LLM%2520Training%2529%252C%2520a%2520novel%2520post-training%2520strategy%2520that%2520divides%2520the%250Areasoning%2520process%2520into%2520generation%252C%2520verification%252C%2520and%2520refinement%2520steps%2520using%2520a%250Asequential%2520pipeline%2520of%2520heterogeneous%2520agents.%2520During%2520data%2520generation%252C%2520each%2520agent%250Ais%2520repeatedly%2520sampled%2520to%2520form%2520a%2520multi-agent%2520search%2520tree%252C%2520where%2520final%2520outputs%250Aare%2520graded%2520against%2520ground-truth%2520data.%2520We%2520then%2520apply%2520value%2520iteration%2520to%250Apropagate%2520reward%2520signals%2520back%2520to%2520each%2520role-conditioned%2520model%252C%2520automatically%250Aproducing%2520multi-agent%2520post-training%2520data%2520without%2520human%2520or%2520teacher-model%250Asupervision.%2520Our%2520off-policy%2520approach%2520allows%2520each%2520agent%2520to%2520specialize%2520by%250Alearning%2520from%2520correct%2520and%2520incorrect%2520trajectories%252C%2520ultimately%2520improving%2520the%250Aend-to-end%2520reasoning%2520chain.%2520On%2520MATH%252C%2520GSM8K%252C%2520and%2520CSQA%252C%2520MALT%2520surpasses%2520the%2520same%250Abaseline%2520LLM%2520with%2520a%2520relative%2520improvement%2520of%252015.66%2525%252C%25207.42%2525%252C%2520and%25209.40%2525%250Arespectively%252C%2520making%2520it%2520an%2520important%2520advance%2520towards%2520multi-agent%2520cooperative%250Atraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01928v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MALT%3A%20Improving%20Reasoning%20with%20Multi-Agent%20LLM%20Training&entry.906535625=Sumeet%20Ramesh%20Motwani%20and%20Chandler%20Smith%20and%20Rocktim%20Jyoti%20Das%20and%20Rafael%20Rafailov%20and%20Ivan%20Laptev%20and%20Philip%20H.%20S.%20Torr%20and%20Fabio%20Pizzati%20and%20Ronald%20Clark%20and%20Christian%20Schroeder%20de%20Witt&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20often%20produce%20answers%20with%20a%20single%0Achain-of-thought%2C%20which%20restricts%20their%20ability%20to%20explore%20reasoning%20paths%20or%0Aself-correct%20flawed%20outputs%20in%20complex%20tasks.%20In%20this%20paper%2C%20we%20introduce%20MALT%0A%28Multi-Agent%20LLM%20Training%29%2C%20a%20novel%20post-training%20strategy%20that%20divides%20the%0Areasoning%20process%20into%20generation%2C%20verification%2C%20and%20refinement%20steps%20using%20a%0Asequential%20pipeline%20of%20heterogeneous%20agents.%20During%20data%20generation%2C%20each%20agent%0Ais%20repeatedly%20sampled%20to%20form%20a%20multi-agent%20search%20tree%2C%20where%20final%20outputs%0Aare%20graded%20against%20ground-truth%20data.%20We%20then%20apply%20value%20iteration%20to%0Apropagate%20reward%20signals%20back%20to%20each%20role-conditioned%20model%2C%20automatically%0Aproducing%20multi-agent%20post-training%20data%20without%20human%20or%20teacher-model%0Asupervision.%20Our%20off-policy%20approach%20allows%20each%20agent%20to%20specialize%20by%0Alearning%20from%20correct%20and%20incorrect%20trajectories%2C%20ultimately%20improving%20the%0Aend-to-end%20reasoning%20chain.%20On%20MATH%2C%20GSM8K%2C%20and%20CSQA%2C%20MALT%20surpasses%20the%20same%0Abaseline%20LLM%20with%20a%20relative%20improvement%20of%2015.66%25%2C%207.42%25%2C%20and%209.40%25%0Arespectively%2C%20making%20it%20an%20important%20advance%20towards%20multi-agent%20cooperative%0Atraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01928v3&entry.124074799=Read"},
{"title": "SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization", "author": "Th\u00e9ophane Vallaeys and Jakob Verbeek and Matthieu Cord", "abstract": "  Tokenizers are a key component of state-of-the-art generative image models,\nextracting the most important features from the signal while reducing data\ndimension and redundancy. Most current tokenizers are based on KL-regularized\nvariational autoencoders (KL-VAE), trained with reconstruction, perceptual and\nadversarial losses. Diffusion decoders have been proposed as a more principled\nalternative to model the distribution over images conditioned on the latent.\nHowever, matching the performance of KL-VAE still requires adversarial losses,\nas well as a higher decoding time due to iterative sampling. To address these\nlimitations, we introduce a new pixel diffusion decoder architecture for\nimproved scaling and training stability, benefiting from transformer components\nand GAN-free training. We use distillation to replicate the performance of the\ndiffusion decoder in an efficient single-step decoder. This makes SSDD the\nfirst diffusion decoder optimized for single-step reconstruction trained\nwithout adversarial losses, reaching higher reconstruction quality and faster\nsampling than KL-VAE. In particular, SSDD improves reconstruction FID from\n$0.87$ to $0.50$ with $1.4\\times$ higher throughput and preserve generation\nquality of DiTs with $3.8\\times$ faster sampling. As such, SSDD can be used as\na drop-in replacement for KL-VAE, and for building higher-quality and faster\ngenerative models.\n", "link": "http://arxiv.org/abs/2510.04961v1", "date": "2025-10-06", "relevancy": 2.0688, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.713}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.704}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSDD%3A%20Single-Step%20Diffusion%20Decoder%20for%20Efficient%20Image%20Tokenization&body=Title%3A%20SSDD%3A%20Single-Step%20Diffusion%20Decoder%20for%20Efficient%20Image%20Tokenization%0AAuthor%3A%20Th%C3%A9ophane%20Vallaeys%20and%20Jakob%20Verbeek%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20Tokenizers%20are%20a%20key%20component%20of%20state-of-the-art%20generative%20image%20models%2C%0Aextracting%20the%20most%20important%20features%20from%20the%20signal%20while%20reducing%20data%0Adimension%20and%20redundancy.%20Most%20current%20tokenizers%20are%20based%20on%20KL-regularized%0Avariational%20autoencoders%20%28KL-VAE%29%2C%20trained%20with%20reconstruction%2C%20perceptual%20and%0Aadversarial%20losses.%20Diffusion%20decoders%20have%20been%20proposed%20as%20a%20more%20principled%0Aalternative%20to%20model%20the%20distribution%20over%20images%20conditioned%20on%20the%20latent.%0AHowever%2C%20matching%20the%20performance%20of%20KL-VAE%20still%20requires%20adversarial%20losses%2C%0Aas%20well%20as%20a%20higher%20decoding%20time%20due%20to%20iterative%20sampling.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20new%20pixel%20diffusion%20decoder%20architecture%20for%0Aimproved%20scaling%20and%20training%20stability%2C%20benefiting%20from%20transformer%20components%0Aand%20GAN-free%20training.%20We%20use%20distillation%20to%20replicate%20the%20performance%20of%20the%0Adiffusion%20decoder%20in%20an%20efficient%20single-step%20decoder.%20This%20makes%20SSDD%20the%0Afirst%20diffusion%20decoder%20optimized%20for%20single-step%20reconstruction%20trained%0Awithout%20adversarial%20losses%2C%20reaching%20higher%20reconstruction%20quality%20and%20faster%0Asampling%20than%20KL-VAE.%20In%20particular%2C%20SSDD%20improves%20reconstruction%20FID%20from%0A%240.87%24%20to%20%240.50%24%20with%20%241.4%5Ctimes%24%20higher%20throughput%20and%20preserve%20generation%0Aquality%20of%20DiTs%20with%20%243.8%5Ctimes%24%20faster%20sampling.%20As%20such%2C%20SSDD%20can%20be%20used%20as%0Aa%20drop-in%20replacement%20for%20KL-VAE%2C%20and%20for%20building%20higher-quality%20and%20faster%0Agenerative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSDD%253A%2520Single-Step%2520Diffusion%2520Decoder%2520for%2520Efficient%2520Image%2520Tokenization%26entry.906535625%3DTh%25C3%25A9ophane%2520Vallaeys%2520and%2520Jakob%2520Verbeek%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520Tokenizers%2520are%2520a%2520key%2520component%2520of%2520state-of-the-art%2520generative%2520image%2520models%252C%250Aextracting%2520the%2520most%2520important%2520features%2520from%2520the%2520signal%2520while%2520reducing%2520data%250Adimension%2520and%2520redundancy.%2520Most%2520current%2520tokenizers%2520are%2520based%2520on%2520KL-regularized%250Avariational%2520autoencoders%2520%2528KL-VAE%2529%252C%2520trained%2520with%2520reconstruction%252C%2520perceptual%2520and%250Aadversarial%2520losses.%2520Diffusion%2520decoders%2520have%2520been%2520proposed%2520as%2520a%2520more%2520principled%250Aalternative%2520to%2520model%2520the%2520distribution%2520over%2520images%2520conditioned%2520on%2520the%2520latent.%250AHowever%252C%2520matching%2520the%2520performance%2520of%2520KL-VAE%2520still%2520requires%2520adversarial%2520losses%252C%250Aas%2520well%2520as%2520a%2520higher%2520decoding%2520time%2520due%2520to%2520iterative%2520sampling.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520a%2520new%2520pixel%2520diffusion%2520decoder%2520architecture%2520for%250Aimproved%2520scaling%2520and%2520training%2520stability%252C%2520benefiting%2520from%2520transformer%2520components%250Aand%2520GAN-free%2520training.%2520We%2520use%2520distillation%2520to%2520replicate%2520the%2520performance%2520of%2520the%250Adiffusion%2520decoder%2520in%2520an%2520efficient%2520single-step%2520decoder.%2520This%2520makes%2520SSDD%2520the%250Afirst%2520diffusion%2520decoder%2520optimized%2520for%2520single-step%2520reconstruction%2520trained%250Awithout%2520adversarial%2520losses%252C%2520reaching%2520higher%2520reconstruction%2520quality%2520and%2520faster%250Asampling%2520than%2520KL-VAE.%2520In%2520particular%252C%2520SSDD%2520improves%2520reconstruction%2520FID%2520from%250A%25240.87%2524%2520to%2520%25240.50%2524%2520with%2520%25241.4%255Ctimes%2524%2520higher%2520throughput%2520and%2520preserve%2520generation%250Aquality%2520of%2520DiTs%2520with%2520%25243.8%255Ctimes%2524%2520faster%2520sampling.%2520As%2520such%252C%2520SSDD%2520can%2520be%2520used%2520as%250Aa%2520drop-in%2520replacement%2520for%2520KL-VAE%252C%2520and%2520for%2520building%2520higher-quality%2520and%2520faster%250Agenerative%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSDD%3A%20Single-Step%20Diffusion%20Decoder%20for%20Efficient%20Image%20Tokenization&entry.906535625=Th%C3%A9ophane%20Vallaeys%20and%20Jakob%20Verbeek%20and%20Matthieu%20Cord&entry.1292438233=%20%20Tokenizers%20are%20a%20key%20component%20of%20state-of-the-art%20generative%20image%20models%2C%0Aextracting%20the%20most%20important%20features%20from%20the%20signal%20while%20reducing%20data%0Adimension%20and%20redundancy.%20Most%20current%20tokenizers%20are%20based%20on%20KL-regularized%0Avariational%20autoencoders%20%28KL-VAE%29%2C%20trained%20with%20reconstruction%2C%20perceptual%20and%0Aadversarial%20losses.%20Diffusion%20decoders%20have%20been%20proposed%20as%20a%20more%20principled%0Aalternative%20to%20model%20the%20distribution%20over%20images%20conditioned%20on%20the%20latent.%0AHowever%2C%20matching%20the%20performance%20of%20KL-VAE%20still%20requires%20adversarial%20losses%2C%0Aas%20well%20as%20a%20higher%20decoding%20time%20due%20to%20iterative%20sampling.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20new%20pixel%20diffusion%20decoder%20architecture%20for%0Aimproved%20scaling%20and%20training%20stability%2C%20benefiting%20from%20transformer%20components%0Aand%20GAN-free%20training.%20We%20use%20distillation%20to%20replicate%20the%20performance%20of%20the%0Adiffusion%20decoder%20in%20an%20efficient%20single-step%20decoder.%20This%20makes%20SSDD%20the%0Afirst%20diffusion%20decoder%20optimized%20for%20single-step%20reconstruction%20trained%0Awithout%20adversarial%20losses%2C%20reaching%20higher%20reconstruction%20quality%20and%20faster%0Asampling%20than%20KL-VAE.%20In%20particular%2C%20SSDD%20improves%20reconstruction%20FID%20from%0A%240.87%24%20to%20%240.50%24%20with%20%241.4%5Ctimes%24%20higher%20throughput%20and%20preserve%20generation%0Aquality%20of%20DiTs%20with%20%243.8%5Ctimes%24%20faster%20sampling.%20As%20such%2C%20SSDD%20can%20be%20used%20as%0Aa%20drop-in%20replacement%20for%20KL-VAE%2C%20and%20for%20building%20higher-quality%20and%20faster%0Agenerative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04961v1&entry.124074799=Read"},
{"title": "Read the Room: Inferring Social Context Through Dyadic Interaction\n  Recognition in Cyber-physical-social Infrastructure Systems", "author": "Cheyu Lin and John Martins and Katherine A. Flanigan and Ph. D", "abstract": "  Cyber-physical systems (CPS) integrate sensing, computing, and control to\nimprove infrastructure performance, focusing on economic goals like performance\nand safety. However, they often neglect potential human-centered (or\n''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim\nto address this by aligning CPS with social objectives. This involves defining\nsocial benefits, understanding human interactions with each other and\ninfrastructure, developing privacy-preserving measurement methods, modeling\nthese interactions for prediction, linking them to social benefits, and\nactuating the physical environment to foster positive social outcomes. This\npaper delves into recognizing dyadic human interactions using real-world data,\nwhich is the backbone to measuring social behavior. This lays a foundation to\naddress the need to enhance understanding of the deeper meanings and mutual\nresponses inherent in human interactions. While RGB cameras are informative for\ninteraction recognition, privacy concerns arise. Depth sensors offer a\nprivacy-conscious alternative by analyzing skeletal movements. This study\ncompares five skeleton-based interaction recognition algorithms on a dataset of\n12 dyadic interactions. Unlike single-person datasets, these interactions,\ncategorized into communication types like emblems and affect displays, offer\ninsights into the cultural and emotional aspects of human interactions.\n", "link": "http://arxiv.org/abs/2510.04854v1", "date": "2025-10-06", "relevancy": 2.0681, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Read%20the%20Room%3A%20Inferring%20Social%20Context%20Through%20Dyadic%20Interaction%0A%20%20Recognition%20in%20Cyber-physical-social%20Infrastructure%20Systems&body=Title%3A%20Read%20the%20Room%3A%20Inferring%20Social%20Context%20Through%20Dyadic%20Interaction%0A%20%20Recognition%20in%20Cyber-physical-social%20Infrastructure%20Systems%0AAuthor%3A%20Cheyu%20Lin%20and%20John%20Martins%20and%20Katherine%20A.%20Flanigan%20and%20Ph.%20D%0AAbstract%3A%20%20%20Cyber-physical%20systems%20%28CPS%29%20integrate%20sensing%2C%20computing%2C%20and%20control%20to%0Aimprove%20infrastructure%20performance%2C%20focusing%20on%20economic%20goals%20like%20performance%0Aand%20safety.%20However%2C%20they%20often%20neglect%20potential%20human-centered%20%28or%0A%27%27social%27%27%29%20benefits.%20Cyber-physical-social%20infrastructure%20systems%20%28CPSIS%29%20aim%0Ato%20address%20this%20by%20aligning%20CPS%20with%20social%20objectives.%20This%20involves%20defining%0Asocial%20benefits%2C%20understanding%20human%20interactions%20with%20each%20other%20and%0Ainfrastructure%2C%20developing%20privacy-preserving%20measurement%20methods%2C%20modeling%0Athese%20interactions%20for%20prediction%2C%20linking%20them%20to%20social%20benefits%2C%20and%0Aactuating%20the%20physical%20environment%20to%20foster%20positive%20social%20outcomes.%20This%0Apaper%20delves%20into%20recognizing%20dyadic%20human%20interactions%20using%20real-world%20data%2C%0Awhich%20is%20the%20backbone%20to%20measuring%20social%20behavior.%20This%20lays%20a%20foundation%20to%0Aaddress%20the%20need%20to%20enhance%20understanding%20of%20the%20deeper%20meanings%20and%20mutual%0Aresponses%20inherent%20in%20human%20interactions.%20While%20RGB%20cameras%20are%20informative%20for%0Ainteraction%20recognition%2C%20privacy%20concerns%20arise.%20Depth%20sensors%20offer%20a%0Aprivacy-conscious%20alternative%20by%20analyzing%20skeletal%20movements.%20This%20study%0Acompares%20five%20skeleton-based%20interaction%20recognition%20algorithms%20on%20a%20dataset%20of%0A12%20dyadic%20interactions.%20Unlike%20single-person%20datasets%2C%20these%20interactions%2C%0Acategorized%20into%20communication%20types%20like%20emblems%20and%20affect%20displays%2C%20offer%0Ainsights%20into%20the%20cultural%20and%20emotional%20aspects%20of%20human%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRead%2520the%2520Room%253A%2520Inferring%2520Social%2520Context%2520Through%2520Dyadic%2520Interaction%250A%2520%2520Recognition%2520in%2520Cyber-physical-social%2520Infrastructure%2520Systems%26entry.906535625%3DCheyu%2520Lin%2520and%2520John%2520Martins%2520and%2520Katherine%2520A.%2520Flanigan%2520and%2520Ph.%2520D%26entry.1292438233%3D%2520%2520Cyber-physical%2520systems%2520%2528CPS%2529%2520integrate%2520sensing%252C%2520computing%252C%2520and%2520control%2520to%250Aimprove%2520infrastructure%2520performance%252C%2520focusing%2520on%2520economic%2520goals%2520like%2520performance%250Aand%2520safety.%2520However%252C%2520they%2520often%2520neglect%2520potential%2520human-centered%2520%2528or%250A%2527%2527social%2527%2527%2529%2520benefits.%2520Cyber-physical-social%2520infrastructure%2520systems%2520%2528CPSIS%2529%2520aim%250Ato%2520address%2520this%2520by%2520aligning%2520CPS%2520with%2520social%2520objectives.%2520This%2520involves%2520defining%250Asocial%2520benefits%252C%2520understanding%2520human%2520interactions%2520with%2520each%2520other%2520and%250Ainfrastructure%252C%2520developing%2520privacy-preserving%2520measurement%2520methods%252C%2520modeling%250Athese%2520interactions%2520for%2520prediction%252C%2520linking%2520them%2520to%2520social%2520benefits%252C%2520and%250Aactuating%2520the%2520physical%2520environment%2520to%2520foster%2520positive%2520social%2520outcomes.%2520This%250Apaper%2520delves%2520into%2520recognizing%2520dyadic%2520human%2520interactions%2520using%2520real-world%2520data%252C%250Awhich%2520is%2520the%2520backbone%2520to%2520measuring%2520social%2520behavior.%2520This%2520lays%2520a%2520foundation%2520to%250Aaddress%2520the%2520need%2520to%2520enhance%2520understanding%2520of%2520the%2520deeper%2520meanings%2520and%2520mutual%250Aresponses%2520inherent%2520in%2520human%2520interactions.%2520While%2520RGB%2520cameras%2520are%2520informative%2520for%250Ainteraction%2520recognition%252C%2520privacy%2520concerns%2520arise.%2520Depth%2520sensors%2520offer%2520a%250Aprivacy-conscious%2520alternative%2520by%2520analyzing%2520skeletal%2520movements.%2520This%2520study%250Acompares%2520five%2520skeleton-based%2520interaction%2520recognition%2520algorithms%2520on%2520a%2520dataset%2520of%250A12%2520dyadic%2520interactions.%2520Unlike%2520single-person%2520datasets%252C%2520these%2520interactions%252C%250Acategorized%2520into%2520communication%2520types%2520like%2520emblems%2520and%2520affect%2520displays%252C%2520offer%250Ainsights%2520into%2520the%2520cultural%2520and%2520emotional%2520aspects%2520of%2520human%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Read%20the%20Room%3A%20Inferring%20Social%20Context%20Through%20Dyadic%20Interaction%0A%20%20Recognition%20in%20Cyber-physical-social%20Infrastructure%20Systems&entry.906535625=Cheyu%20Lin%20and%20John%20Martins%20and%20Katherine%20A.%20Flanigan%20and%20Ph.%20D&entry.1292438233=%20%20Cyber-physical%20systems%20%28CPS%29%20integrate%20sensing%2C%20computing%2C%20and%20control%20to%0Aimprove%20infrastructure%20performance%2C%20focusing%20on%20economic%20goals%20like%20performance%0Aand%20safety.%20However%2C%20they%20often%20neglect%20potential%20human-centered%20%28or%0A%27%27social%27%27%29%20benefits.%20Cyber-physical-social%20infrastructure%20systems%20%28CPSIS%29%20aim%0Ato%20address%20this%20by%20aligning%20CPS%20with%20social%20objectives.%20This%20involves%20defining%0Asocial%20benefits%2C%20understanding%20human%20interactions%20with%20each%20other%20and%0Ainfrastructure%2C%20developing%20privacy-preserving%20measurement%20methods%2C%20modeling%0Athese%20interactions%20for%20prediction%2C%20linking%20them%20to%20social%20benefits%2C%20and%0Aactuating%20the%20physical%20environment%20to%20foster%20positive%20social%20outcomes.%20This%0Apaper%20delves%20into%20recognizing%20dyadic%20human%20interactions%20using%20real-world%20data%2C%0Awhich%20is%20the%20backbone%20to%20measuring%20social%20behavior.%20This%20lays%20a%20foundation%20to%0Aaddress%20the%20need%20to%20enhance%20understanding%20of%20the%20deeper%20meanings%20and%20mutual%0Aresponses%20inherent%20in%20human%20interactions.%20While%20RGB%20cameras%20are%20informative%20for%0Ainteraction%20recognition%2C%20privacy%20concerns%20arise.%20Depth%20sensors%20offer%20a%0Aprivacy-conscious%20alternative%20by%20analyzing%20skeletal%20movements.%20This%20study%0Acompares%20five%20skeleton-based%20interaction%20recognition%20algorithms%20on%20a%20dataset%20of%0A12%20dyadic%20interactions.%20Unlike%20single-person%20datasets%2C%20these%20interactions%2C%0Acategorized%20into%20communication%20types%20like%20emblems%20and%20affect%20displays%2C%20offer%0Ainsights%20into%20the%20cultural%20and%20emotional%20aspects%20of%20human%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04854v1&entry.124074799=Read"},
{"title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via\n  Hypernetworks", "author": "Zheng Xiong and Kang Li and Zilin Wang and Matthew Jackson and Jakob Foerster and Shimon Whiteson", "abstract": "  Built upon language and vision foundation models with strong generalization\nability and trained on large-scale robotic data, Vision-Language-Action (VLA)\nmodels have recently emerged as a promising approach to learning generalist\nrobotic policies. However, a key drawback of existing VLAs is their extremely\nhigh inference costs. In this paper, we propose HyperVLA to address this\nproblem. Unlike existing monolithic VLAs that activate the whole model during\nboth training and inference, HyperVLA uses a novel hypernetwork (HN)-based\narchitecture that activates only a small task-specific policy during inference,\nwhile still retaining the high model capacity needed to accommodate diverse\nmulti-task behaviors during training. Successfully training an HN-based VLA is\nnontrivial so HyperVLA contains several key algorithm design features that\nimprove its performance, including properly utilizing the prior knowledge from\nexisting vision foundation models, HN normalization, and an action generation\nstrategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even\nhigher success rate for both zero-shot generalization and few-shot adaptation,\nwhile significantly reducing inference costs. Compared to OpenVLA, a\nstate-of-the-art VLA model, HyperVLA reduces the number of activated parameters\nat test time by $90\\times$, and accelerates inference speed by $120\\times$.\nCode is publicly available at https://github.com/MasterXiong/HyperVLA\n", "link": "http://arxiv.org/abs/2510.04898v1", "date": "2025-10-06", "relevancy": 2.0627, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5218}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperVLA%3A%20Efficient%20Inference%20in%20Vision-Language-Action%20Models%20via%0A%20%20Hypernetworks&body=Title%3A%20HyperVLA%3A%20Efficient%20Inference%20in%20Vision-Language-Action%20Models%20via%0A%20%20Hypernetworks%0AAuthor%3A%20Zheng%20Xiong%20and%20Kang%20Li%20and%20Zilin%20Wang%20and%20Matthew%20Jackson%20and%20Jakob%20Foerster%20and%20Shimon%20Whiteson%0AAbstract%3A%20%20%20Built%20upon%20language%20and%20vision%20foundation%20models%20with%20strong%20generalization%0Aability%20and%20trained%20on%20large-scale%20robotic%20data%2C%20Vision-Language-Action%20%28VLA%29%0Amodels%20have%20recently%20emerged%20as%20a%20promising%20approach%20to%20learning%20generalist%0Arobotic%20policies.%20However%2C%20a%20key%20drawback%20of%20existing%20VLAs%20is%20their%20extremely%0Ahigh%20inference%20costs.%20In%20this%20paper%2C%20we%20propose%20HyperVLA%20to%20address%20this%0Aproblem.%20Unlike%20existing%20monolithic%20VLAs%20that%20activate%20the%20whole%20model%20during%0Aboth%20training%20and%20inference%2C%20HyperVLA%20uses%20a%20novel%20hypernetwork%20%28HN%29-based%0Aarchitecture%20that%20activates%20only%20a%20small%20task-specific%20policy%20during%20inference%2C%0Awhile%20still%20retaining%20the%20high%20model%20capacity%20needed%20to%20accommodate%20diverse%0Amulti-task%20behaviors%20during%20training.%20Successfully%20training%20an%20HN-based%20VLA%20is%0Anontrivial%20so%20HyperVLA%20contains%20several%20key%20algorithm%20design%20features%20that%0Aimprove%20its%20performance%2C%20including%20properly%20utilizing%20the%20prior%20knowledge%20from%0Aexisting%20vision%20foundation%20models%2C%20HN%20normalization%2C%20and%20an%20action%20generation%0Astrategy.%20Compared%20to%20monolithic%20VLAs%2C%20HyperVLA%20achieves%20a%20similar%20or%20even%0Ahigher%20success%20rate%20for%20both%20zero-shot%20generalization%20and%20few-shot%20adaptation%2C%0Awhile%20significantly%20reducing%20inference%20costs.%20Compared%20to%20OpenVLA%2C%20a%0Astate-of-the-art%20VLA%20model%2C%20HyperVLA%20reduces%20the%20number%20of%20activated%20parameters%0Aat%20test%20time%20by%20%2490%5Ctimes%24%2C%20and%20accelerates%20inference%20speed%20by%20%24120%5Ctimes%24.%0ACode%20is%20publicly%20available%20at%20https%3A//github.com/MasterXiong/HyperVLA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperVLA%253A%2520Efficient%2520Inference%2520in%2520Vision-Language-Action%2520Models%2520via%250A%2520%2520Hypernetworks%26entry.906535625%3DZheng%2520Xiong%2520and%2520Kang%2520Li%2520and%2520Zilin%2520Wang%2520and%2520Matthew%2520Jackson%2520and%2520Jakob%2520Foerster%2520and%2520Shimon%2520Whiteson%26entry.1292438233%3D%2520%2520Built%2520upon%2520language%2520and%2520vision%2520foundation%2520models%2520with%2520strong%2520generalization%250Aability%2520and%2520trained%2520on%2520large-scale%2520robotic%2520data%252C%2520Vision-Language-Action%2520%2528VLA%2529%250Amodels%2520have%2520recently%2520emerged%2520as%2520a%2520promising%2520approach%2520to%2520learning%2520generalist%250Arobotic%2520policies.%2520However%252C%2520a%2520key%2520drawback%2520of%2520existing%2520VLAs%2520is%2520their%2520extremely%250Ahigh%2520inference%2520costs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520HyperVLA%2520to%2520address%2520this%250Aproblem.%2520Unlike%2520existing%2520monolithic%2520VLAs%2520that%2520activate%2520the%2520whole%2520model%2520during%250Aboth%2520training%2520and%2520inference%252C%2520HyperVLA%2520uses%2520a%2520novel%2520hypernetwork%2520%2528HN%2529-based%250Aarchitecture%2520that%2520activates%2520only%2520a%2520small%2520task-specific%2520policy%2520during%2520inference%252C%250Awhile%2520still%2520retaining%2520the%2520high%2520model%2520capacity%2520needed%2520to%2520accommodate%2520diverse%250Amulti-task%2520behaviors%2520during%2520training.%2520Successfully%2520training%2520an%2520HN-based%2520VLA%2520is%250Anontrivial%2520so%2520HyperVLA%2520contains%2520several%2520key%2520algorithm%2520design%2520features%2520that%250Aimprove%2520its%2520performance%252C%2520including%2520properly%2520utilizing%2520the%2520prior%2520knowledge%2520from%250Aexisting%2520vision%2520foundation%2520models%252C%2520HN%2520normalization%252C%2520and%2520an%2520action%2520generation%250Astrategy.%2520Compared%2520to%2520monolithic%2520VLAs%252C%2520HyperVLA%2520achieves%2520a%2520similar%2520or%2520even%250Ahigher%2520success%2520rate%2520for%2520both%2520zero-shot%2520generalization%2520and%2520few-shot%2520adaptation%252C%250Awhile%2520significantly%2520reducing%2520inference%2520costs.%2520Compared%2520to%2520OpenVLA%252C%2520a%250Astate-of-the-art%2520VLA%2520model%252C%2520HyperVLA%2520reduces%2520the%2520number%2520of%2520activated%2520parameters%250Aat%2520test%2520time%2520by%2520%252490%255Ctimes%2524%252C%2520and%2520accelerates%2520inference%2520speed%2520by%2520%2524120%255Ctimes%2524.%250ACode%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/MasterXiong/HyperVLA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperVLA%3A%20Efficient%20Inference%20in%20Vision-Language-Action%20Models%20via%0A%20%20Hypernetworks&entry.906535625=Zheng%20Xiong%20and%20Kang%20Li%20and%20Zilin%20Wang%20and%20Matthew%20Jackson%20and%20Jakob%20Foerster%20and%20Shimon%20Whiteson&entry.1292438233=%20%20Built%20upon%20language%20and%20vision%20foundation%20models%20with%20strong%20generalization%0Aability%20and%20trained%20on%20large-scale%20robotic%20data%2C%20Vision-Language-Action%20%28VLA%29%0Amodels%20have%20recently%20emerged%20as%20a%20promising%20approach%20to%20learning%20generalist%0Arobotic%20policies.%20However%2C%20a%20key%20drawback%20of%20existing%20VLAs%20is%20their%20extremely%0Ahigh%20inference%20costs.%20In%20this%20paper%2C%20we%20propose%20HyperVLA%20to%20address%20this%0Aproblem.%20Unlike%20existing%20monolithic%20VLAs%20that%20activate%20the%20whole%20model%20during%0Aboth%20training%20and%20inference%2C%20HyperVLA%20uses%20a%20novel%20hypernetwork%20%28HN%29-based%0Aarchitecture%20that%20activates%20only%20a%20small%20task-specific%20policy%20during%20inference%2C%0Awhile%20still%20retaining%20the%20high%20model%20capacity%20needed%20to%20accommodate%20diverse%0Amulti-task%20behaviors%20during%20training.%20Successfully%20training%20an%20HN-based%20VLA%20is%0Anontrivial%20so%20HyperVLA%20contains%20several%20key%20algorithm%20design%20features%20that%0Aimprove%20its%20performance%2C%20including%20properly%20utilizing%20the%20prior%20knowledge%20from%0Aexisting%20vision%20foundation%20models%2C%20HN%20normalization%2C%20and%20an%20action%20generation%0Astrategy.%20Compared%20to%20monolithic%20VLAs%2C%20HyperVLA%20achieves%20a%20similar%20or%20even%0Ahigher%20success%20rate%20for%20both%20zero-shot%20generalization%20and%20few-shot%20adaptation%2C%0Awhile%20significantly%20reducing%20inference%20costs.%20Compared%20to%20OpenVLA%2C%20a%0Astate-of-the-art%20VLA%20model%2C%20HyperVLA%20reduces%20the%20number%20of%20activated%20parameters%0Aat%20test%20time%20by%20%2490%5Ctimes%24%2C%20and%20accelerates%20inference%20speed%20by%20%24120%5Ctimes%24.%0ACode%20is%20publicly%20available%20at%20https%3A//github.com/MasterXiong/HyperVLA%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04898v1&entry.124074799=Read"},
{"title": "Large Language Models Achieve Gold Medal Performance at International\n  Astronomy & Astrophysics Olympiad", "author": "Lucas Carrit Delgado Pinheiro and Ziru Chen and Bruno Caixeta Piazza and Ness Shroff and Yingbin Liang and Yuan-Sen Ting and Huan Sun", "abstract": "  While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy.\n", "link": "http://arxiv.org/abs/2510.05016v1", "date": "2025-10-06", "relevancy": 2.0613, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Achieve%20Gold%20Medal%20Performance%20at%20International%0A%20%20Astronomy%20%26%20Astrophysics%20Olympiad&body=Title%3A%20Large%20Language%20Models%20Achieve%20Gold%20Medal%20Performance%20at%20International%0A%20%20Astronomy%20%26%20Astrophysics%20Olympiad%0AAuthor%3A%20Lucas%20Carrit%20Delgado%20Pinheiro%20and%20Ziru%20Chen%20and%20Bruno%20Caixeta%20Piazza%20and%20Ness%20Shroff%20and%20Yingbin%20Liang%20and%20Yuan-Sen%20Ting%20and%20Huan%20Sun%0AAbstract%3A%20%20%20While%20task-specific%20demonstrations%20show%20early%20success%20in%20applying%20large%0Alanguage%20models%20%28LLMs%29%20to%20automate%20some%20astronomical%20research%20tasks%2C%20they%20only%0Aprovide%20incomplete%20views%20of%20all%20necessary%20capabilities%20in%20solving%20astronomy%0Aproblems%2C%20calling%20for%20more%20thorough%20understanding%20of%20LLMs%27%20strengths%20and%0Alimitations.%20So%20far%2C%20existing%20benchmarks%20and%20evaluations%20focus%20on%20simple%0Aquestion-answering%20that%20primarily%20tests%20astronomical%20knowledge%20and%20fails%20to%0Aevaluate%20the%20complex%20reasoning%20required%20for%20real-world%20research%20in%20the%0Adiscipline.%20Here%2C%20we%20address%20this%20gap%20by%20systematically%20benchmarking%20five%0Astate-of-the-art%20LLMs%20on%20the%20International%20Olympiad%20on%20Astronomy%20and%0AAstrophysics%20%28IOAA%29%20exams%2C%20which%20are%20designed%20to%20examine%20deep%20conceptual%0Aunderstanding%2C%20multi-step%20derivations%2C%20and%20multimodal%20analysis.%20With%20average%0Ascores%20of%2085.6%25%20and%2084.2%25%2C%20Gemini%202.5%20Pro%20and%20GPT-5%20%28the%20two%20top-performing%0Amodels%29%20not%20only%20achieve%20gold%20medal%20level%20performance%20but%20also%20rank%20in%20the%20top%0Atwo%20among%20~200-300%20participants%20in%20all%20four%20IOAA%20theory%20exams%20evaluated%0A%282022-2025%29.%20In%20comparison%2C%20results%20on%20the%20data%20analysis%20exams%20show%20more%0Adivergence.%20GPT-5%20still%20excels%20in%20the%20exams%20with%20an%2088.5%25%20average%20score%2C%0Aranking%20top%2010%20among%20the%20participants%20in%20the%20four%20most%20recent%20IOAAs%2C%20while%0Aother%20models%27%20performances%20drop%20to%2048-76%25.%20Furthermore%2C%20our%20in-depth%20error%0Aanalysis%20underscores%20conceptual%20reasoning%2C%20geometric%20reasoning%2C%20and%20spatial%0Avisualization%20%2852-79%25%20accuracy%29%20as%20consistent%20weaknesses%20among%20all%20LLMs.%20Hence%2C%0Aalthough%20LLMs%20approach%20peak%20human%20performance%20in%20theory%20exams%2C%20critical%20gaps%0Amust%20be%20addressed%20before%20they%20can%20serve%20as%20autonomous%20research%20agents%20in%0Aastronomy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Achieve%2520Gold%2520Medal%2520Performance%2520at%2520International%250A%2520%2520Astronomy%2520%2526%2520Astrophysics%2520Olympiad%26entry.906535625%3DLucas%2520Carrit%2520Delgado%2520Pinheiro%2520and%2520Ziru%2520Chen%2520and%2520Bruno%2520Caixeta%2520Piazza%2520and%2520Ness%2520Shroff%2520and%2520Yingbin%2520Liang%2520and%2520Yuan-Sen%2520Ting%2520and%2520Huan%2520Sun%26entry.1292438233%3D%2520%2520While%2520task-specific%2520demonstrations%2520show%2520early%2520success%2520in%2520applying%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520automate%2520some%2520astronomical%2520research%2520tasks%252C%2520they%2520only%250Aprovide%2520incomplete%2520views%2520of%2520all%2520necessary%2520capabilities%2520in%2520solving%2520astronomy%250Aproblems%252C%2520calling%2520for%2520more%2520thorough%2520understanding%2520of%2520LLMs%2527%2520strengths%2520and%250Alimitations.%2520So%2520far%252C%2520existing%2520benchmarks%2520and%2520evaluations%2520focus%2520on%2520simple%250Aquestion-answering%2520that%2520primarily%2520tests%2520astronomical%2520knowledge%2520and%2520fails%2520to%250Aevaluate%2520the%2520complex%2520reasoning%2520required%2520for%2520real-world%2520research%2520in%2520the%250Adiscipline.%2520Here%252C%2520we%2520address%2520this%2520gap%2520by%2520systematically%2520benchmarking%2520five%250Astate-of-the-art%2520LLMs%2520on%2520the%2520International%2520Olympiad%2520on%2520Astronomy%2520and%250AAstrophysics%2520%2528IOAA%2529%2520exams%252C%2520which%2520are%2520designed%2520to%2520examine%2520deep%2520conceptual%250Aunderstanding%252C%2520multi-step%2520derivations%252C%2520and%2520multimodal%2520analysis.%2520With%2520average%250Ascores%2520of%252085.6%2525%2520and%252084.2%2525%252C%2520Gemini%25202.5%2520Pro%2520and%2520GPT-5%2520%2528the%2520two%2520top-performing%250Amodels%2529%2520not%2520only%2520achieve%2520gold%2520medal%2520level%2520performance%2520but%2520also%2520rank%2520in%2520the%2520top%250Atwo%2520among%2520~200-300%2520participants%2520in%2520all%2520four%2520IOAA%2520theory%2520exams%2520evaluated%250A%25282022-2025%2529.%2520In%2520comparison%252C%2520results%2520on%2520the%2520data%2520analysis%2520exams%2520show%2520more%250Adivergence.%2520GPT-5%2520still%2520excels%2520in%2520the%2520exams%2520with%2520an%252088.5%2525%2520average%2520score%252C%250Aranking%2520top%252010%2520among%2520the%2520participants%2520in%2520the%2520four%2520most%2520recent%2520IOAAs%252C%2520while%250Aother%2520models%2527%2520performances%2520drop%2520to%252048-76%2525.%2520Furthermore%252C%2520our%2520in-depth%2520error%250Aanalysis%2520underscores%2520conceptual%2520reasoning%252C%2520geometric%2520reasoning%252C%2520and%2520spatial%250Avisualization%2520%252852-79%2525%2520accuracy%2529%2520as%2520consistent%2520weaknesses%2520among%2520all%2520LLMs.%2520Hence%252C%250Aalthough%2520LLMs%2520approach%2520peak%2520human%2520performance%2520in%2520theory%2520exams%252C%2520critical%2520gaps%250Amust%2520be%2520addressed%2520before%2520they%2520can%2520serve%2520as%2520autonomous%2520research%2520agents%2520in%250Aastronomy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Achieve%20Gold%20Medal%20Performance%20at%20International%0A%20%20Astronomy%20%26%20Astrophysics%20Olympiad&entry.906535625=Lucas%20Carrit%20Delgado%20Pinheiro%20and%20Ziru%20Chen%20and%20Bruno%20Caixeta%20Piazza%20and%20Ness%20Shroff%20and%20Yingbin%20Liang%20and%20Yuan-Sen%20Ting%20and%20Huan%20Sun&entry.1292438233=%20%20While%20task-specific%20demonstrations%20show%20early%20success%20in%20applying%20large%0Alanguage%20models%20%28LLMs%29%20to%20automate%20some%20astronomical%20research%20tasks%2C%20they%20only%0Aprovide%20incomplete%20views%20of%20all%20necessary%20capabilities%20in%20solving%20astronomy%0Aproblems%2C%20calling%20for%20more%20thorough%20understanding%20of%20LLMs%27%20strengths%20and%0Alimitations.%20So%20far%2C%20existing%20benchmarks%20and%20evaluations%20focus%20on%20simple%0Aquestion-answering%20that%20primarily%20tests%20astronomical%20knowledge%20and%20fails%20to%0Aevaluate%20the%20complex%20reasoning%20required%20for%20real-world%20research%20in%20the%0Adiscipline.%20Here%2C%20we%20address%20this%20gap%20by%20systematically%20benchmarking%20five%0Astate-of-the-art%20LLMs%20on%20the%20International%20Olympiad%20on%20Astronomy%20and%0AAstrophysics%20%28IOAA%29%20exams%2C%20which%20are%20designed%20to%20examine%20deep%20conceptual%0Aunderstanding%2C%20multi-step%20derivations%2C%20and%20multimodal%20analysis.%20With%20average%0Ascores%20of%2085.6%25%20and%2084.2%25%2C%20Gemini%202.5%20Pro%20and%20GPT-5%20%28the%20two%20top-performing%0Amodels%29%20not%20only%20achieve%20gold%20medal%20level%20performance%20but%20also%20rank%20in%20the%20top%0Atwo%20among%20~200-300%20participants%20in%20all%20four%20IOAA%20theory%20exams%20evaluated%0A%282022-2025%29.%20In%20comparison%2C%20results%20on%20the%20data%20analysis%20exams%20show%20more%0Adivergence.%20GPT-5%20still%20excels%20in%20the%20exams%20with%20an%2088.5%25%20average%20score%2C%0Aranking%20top%2010%20among%20the%20participants%20in%20the%20four%20most%20recent%20IOAAs%2C%20while%0Aother%20models%27%20performances%20drop%20to%2048-76%25.%20Furthermore%2C%20our%20in-depth%20error%0Aanalysis%20underscores%20conceptual%20reasoning%2C%20geometric%20reasoning%2C%20and%20spatial%0Avisualization%20%2852-79%25%20accuracy%29%20as%20consistent%20weaknesses%20among%20all%20LLMs.%20Hence%2C%0Aalthough%20LLMs%20approach%20peak%20human%20performance%20in%20theory%20exams%2C%20critical%20gaps%0Amust%20be%20addressed%20before%20they%20can%20serve%20as%20autonomous%20research%20agents%20in%0Aastronomy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05016v1&entry.124074799=Read"},
{"title": "Synthesising Counterfactual Explanations via Label-Conditional Gaussian\n  Mixture Variational Autoencoders", "author": "Junqi Jiang and Francesco Leofante and Antonio Rago and Francesca Toni", "abstract": "  Counterfactual explanations (CEs) provide recourse recommendations for\nindividuals affected by algorithmic decisions. A key challenge is generating\nCEs that are robust against various perturbation types (e.g. input and model\nperturbations) while simultaneously satisfying other desirable properties.\nThese include plausibility, ensuring CEs reside on the data manifold, and\ndiversity, providing multiple distinct recourse options for single inputs.\nExisting methods, however, mostly struggle to address these multifaceted\nrequirements in a unified, model-agnostic manner. We address these limitations\nby proposing a novel generative framework. First, we introduce the\nLabel-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model\ntrained to learn a structured latent space where each class label is\nrepresented by a set of Gaussian components with diverse, prototypical\ncentroids. Building on this, we present LAPACE (LAtent PAth Counterfactual\nExplanations), a model-agnostic algorithm that synthesises entire paths of CE\npoints by interpolating from inputs' latent representations to those learned\nlatent centroids. This approach inherently ensures robustness to input changes,\nas all paths for a given target class converge to the same fixed centroids.\nFurthermore, the generated paths provide a spectrum of recourse options,\nallowing users to navigate the trade-off between proximity and plausibility\nwhile also encouraging robustness against model changes. In addition,\nuser-specified actionability constraints can also be easily incorporated via\nlightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive\nexperiments show that LAPACE is computationally efficient and achieves\ncompetitive performance across eight quantitative metrics.\n", "link": "http://arxiv.org/abs/2510.04855v1", "date": "2025-10-06", "relevancy": 2.0565, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5195}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.513}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesising%20Counterfactual%20Explanations%20via%20Label-Conditional%20Gaussian%0A%20%20Mixture%20Variational%20Autoencoders&body=Title%3A%20Synthesising%20Counterfactual%20Explanations%20via%20Label-Conditional%20Gaussian%0A%20%20Mixture%20Variational%20Autoencoders%0AAuthor%3A%20Junqi%20Jiang%20and%20Francesco%20Leofante%20and%20Antonio%20Rago%20and%20Francesca%20Toni%0AAbstract%3A%20%20%20Counterfactual%20explanations%20%28CEs%29%20provide%20recourse%20recommendations%20for%0Aindividuals%20affected%20by%20algorithmic%20decisions.%20A%20key%20challenge%20is%20generating%0ACEs%20that%20are%20robust%20against%20various%20perturbation%20types%20%28e.g.%20input%20and%20model%0Aperturbations%29%20while%20simultaneously%20satisfying%20other%20desirable%20properties.%0AThese%20include%20plausibility%2C%20ensuring%20CEs%20reside%20on%20the%20data%20manifold%2C%20and%0Adiversity%2C%20providing%20multiple%20distinct%20recourse%20options%20for%20single%20inputs.%0AExisting%20methods%2C%20however%2C%20mostly%20struggle%20to%20address%20these%20multifaceted%0Arequirements%20in%20a%20unified%2C%20model-agnostic%20manner.%20We%20address%20these%20limitations%0Aby%20proposing%20a%20novel%20generative%20framework.%20First%2C%20we%20introduce%20the%0ALabel-conditional%20Gaussian%20Mixture%20Variational%20Autoencoder%20%28L-GMVAE%29%2C%20a%20model%0Atrained%20to%20learn%20a%20structured%20latent%20space%20where%20each%20class%20label%20is%0Arepresented%20by%20a%20set%20of%20Gaussian%20components%20with%20diverse%2C%20prototypical%0Acentroids.%20Building%20on%20this%2C%20we%20present%20LAPACE%20%28LAtent%20PAth%20Counterfactual%0AExplanations%29%2C%20a%20model-agnostic%20algorithm%20that%20synthesises%20entire%20paths%20of%20CE%0Apoints%20by%20interpolating%20from%20inputs%27%20latent%20representations%20to%20those%20learned%0Alatent%20centroids.%20This%20approach%20inherently%20ensures%20robustness%20to%20input%20changes%2C%0Aas%20all%20paths%20for%20a%20given%20target%20class%20converge%20to%20the%20same%20fixed%20centroids.%0AFurthermore%2C%20the%20generated%20paths%20provide%20a%20spectrum%20of%20recourse%20options%2C%0Aallowing%20users%20to%20navigate%20the%20trade-off%20between%20proximity%20and%20plausibility%0Awhile%20also%20encouraging%20robustness%20against%20model%20changes.%20In%20addition%2C%0Auser-specified%20actionability%20constraints%20can%20also%20be%20easily%20incorporated%20via%0Alightweight%20gradient%20optimisation%20through%20the%20L-GMVAE%27s%20decoder.%20Comprehensive%0Aexperiments%20show%20that%20LAPACE%20is%20computationally%20efficient%20and%20achieves%0Acompetitive%20performance%20across%20eight%20quantitative%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesising%2520Counterfactual%2520Explanations%2520via%2520Label-Conditional%2520Gaussian%250A%2520%2520Mixture%2520Variational%2520Autoencoders%26entry.906535625%3DJunqi%2520Jiang%2520and%2520Francesco%2520Leofante%2520and%2520Antonio%2520Rago%2520and%2520Francesca%2520Toni%26entry.1292438233%3D%2520%2520Counterfactual%2520explanations%2520%2528CEs%2529%2520provide%2520recourse%2520recommendations%2520for%250Aindividuals%2520affected%2520by%2520algorithmic%2520decisions.%2520A%2520key%2520challenge%2520is%2520generating%250ACEs%2520that%2520are%2520robust%2520against%2520various%2520perturbation%2520types%2520%2528e.g.%2520input%2520and%2520model%250Aperturbations%2529%2520while%2520simultaneously%2520satisfying%2520other%2520desirable%2520properties.%250AThese%2520include%2520plausibility%252C%2520ensuring%2520CEs%2520reside%2520on%2520the%2520data%2520manifold%252C%2520and%250Adiversity%252C%2520providing%2520multiple%2520distinct%2520recourse%2520options%2520for%2520single%2520inputs.%250AExisting%2520methods%252C%2520however%252C%2520mostly%2520struggle%2520to%2520address%2520these%2520multifaceted%250Arequirements%2520in%2520a%2520unified%252C%2520model-agnostic%2520manner.%2520We%2520address%2520these%2520limitations%250Aby%2520proposing%2520a%2520novel%2520generative%2520framework.%2520First%252C%2520we%2520introduce%2520the%250ALabel-conditional%2520Gaussian%2520Mixture%2520Variational%2520Autoencoder%2520%2528L-GMVAE%2529%252C%2520a%2520model%250Atrained%2520to%2520learn%2520a%2520structured%2520latent%2520space%2520where%2520each%2520class%2520label%2520is%250Arepresented%2520by%2520a%2520set%2520of%2520Gaussian%2520components%2520with%2520diverse%252C%2520prototypical%250Acentroids.%2520Building%2520on%2520this%252C%2520we%2520present%2520LAPACE%2520%2528LAtent%2520PAth%2520Counterfactual%250AExplanations%2529%252C%2520a%2520model-agnostic%2520algorithm%2520that%2520synthesises%2520entire%2520paths%2520of%2520CE%250Apoints%2520by%2520interpolating%2520from%2520inputs%2527%2520latent%2520representations%2520to%2520those%2520learned%250Alatent%2520centroids.%2520This%2520approach%2520inherently%2520ensures%2520robustness%2520to%2520input%2520changes%252C%250Aas%2520all%2520paths%2520for%2520a%2520given%2520target%2520class%2520converge%2520to%2520the%2520same%2520fixed%2520centroids.%250AFurthermore%252C%2520the%2520generated%2520paths%2520provide%2520a%2520spectrum%2520of%2520recourse%2520options%252C%250Aallowing%2520users%2520to%2520navigate%2520the%2520trade-off%2520between%2520proximity%2520and%2520plausibility%250Awhile%2520also%2520encouraging%2520robustness%2520against%2520model%2520changes.%2520In%2520addition%252C%250Auser-specified%2520actionability%2520constraints%2520can%2520also%2520be%2520easily%2520incorporated%2520via%250Alightweight%2520gradient%2520optimisation%2520through%2520the%2520L-GMVAE%2527s%2520decoder.%2520Comprehensive%250Aexperiments%2520show%2520that%2520LAPACE%2520is%2520computationally%2520efficient%2520and%2520achieves%250Acompetitive%2520performance%2520across%2520eight%2520quantitative%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesising%20Counterfactual%20Explanations%20via%20Label-Conditional%20Gaussian%0A%20%20Mixture%20Variational%20Autoencoders&entry.906535625=Junqi%20Jiang%20and%20Francesco%20Leofante%20and%20Antonio%20Rago%20and%20Francesca%20Toni&entry.1292438233=%20%20Counterfactual%20explanations%20%28CEs%29%20provide%20recourse%20recommendations%20for%0Aindividuals%20affected%20by%20algorithmic%20decisions.%20A%20key%20challenge%20is%20generating%0ACEs%20that%20are%20robust%20against%20various%20perturbation%20types%20%28e.g.%20input%20and%20model%0Aperturbations%29%20while%20simultaneously%20satisfying%20other%20desirable%20properties.%0AThese%20include%20plausibility%2C%20ensuring%20CEs%20reside%20on%20the%20data%20manifold%2C%20and%0Adiversity%2C%20providing%20multiple%20distinct%20recourse%20options%20for%20single%20inputs.%0AExisting%20methods%2C%20however%2C%20mostly%20struggle%20to%20address%20these%20multifaceted%0Arequirements%20in%20a%20unified%2C%20model-agnostic%20manner.%20We%20address%20these%20limitations%0Aby%20proposing%20a%20novel%20generative%20framework.%20First%2C%20we%20introduce%20the%0ALabel-conditional%20Gaussian%20Mixture%20Variational%20Autoencoder%20%28L-GMVAE%29%2C%20a%20model%0Atrained%20to%20learn%20a%20structured%20latent%20space%20where%20each%20class%20label%20is%0Arepresented%20by%20a%20set%20of%20Gaussian%20components%20with%20diverse%2C%20prototypical%0Acentroids.%20Building%20on%20this%2C%20we%20present%20LAPACE%20%28LAtent%20PAth%20Counterfactual%0AExplanations%29%2C%20a%20model-agnostic%20algorithm%20that%20synthesises%20entire%20paths%20of%20CE%0Apoints%20by%20interpolating%20from%20inputs%27%20latent%20representations%20to%20those%20learned%0Alatent%20centroids.%20This%20approach%20inherently%20ensures%20robustness%20to%20input%20changes%2C%0Aas%20all%20paths%20for%20a%20given%20target%20class%20converge%20to%20the%20same%20fixed%20centroids.%0AFurthermore%2C%20the%20generated%20paths%20provide%20a%20spectrum%20of%20recourse%20options%2C%0Aallowing%20users%20to%20navigate%20the%20trade-off%20between%20proximity%20and%20plausibility%0Awhile%20also%20encouraging%20robustness%20against%20model%20changes.%20In%20addition%2C%0Auser-specified%20actionability%20constraints%20can%20also%20be%20easily%20incorporated%20via%0Alightweight%20gradient%20optimisation%20through%20the%20L-GMVAE%27s%20decoder.%20Comprehensive%0Aexperiments%20show%20that%20LAPACE%20is%20computationally%20efficient%20and%20achieves%0Acompetitive%20performance%20across%20eight%20quantitative%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04855v1&entry.124074799=Read"},
{"title": "Using cognitive models to reveal value trade-offs in language models", "author": "Sonia K. Murthy and Rosie Zhao and Jennifer Hu and Sham Kakade and Markus Wulfmeier and Peng Qian and Tomer Ullman", "abstract": "  Value trade-offs are an integral part of human decision-making and language\nuse, however, current tools for interpreting such dynamic and multi-faceted\nnotions of values in LLMs are limited. In cognitive science, so-called\n\"cognitive models\" provide formal accounts of such trade-offs in humans, by\nmodeling the weighting of a speaker's competing utility functions in choosing\nan action or utterance. Here we use a leading cognitive model of polite speech\nto systematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models' default\nbehavior, and demonstrate that these patterns shift in predictable ways when\nmodels are prompted to prioritize certain goals over others. Our findings from\nLLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. Our framework offers a\nflexible tool for probing value trade-offs across diverse model types,\nproviding insights for generating hypotheses about other social behaviors such\nas sycophancy and for shaping training regimes that better control trade-offs\nbetween values during model development.\n", "link": "http://arxiv.org/abs/2506.20666v3", "date": "2025-10-06", "relevancy": 2.0473, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20cognitive%20models%20to%20reveal%20value%20trade-offs%20in%20language%20models&body=Title%3A%20Using%20cognitive%20models%20to%20reveal%20value%20trade-offs%20in%20language%20models%0AAuthor%3A%20Sonia%20K.%20Murthy%20and%20Rosie%20Zhao%20and%20Jennifer%20Hu%20and%20Sham%20Kakade%20and%20Markus%20Wulfmeier%20and%20Peng%20Qian%20and%20Tomer%20Ullman%0AAbstract%3A%20%20%20Value%20trade-offs%20are%20an%20integral%20part%20of%20human%20decision-making%20and%20language%0Ause%2C%20however%2C%20current%20tools%20for%20interpreting%20such%20dynamic%20and%20multi-faceted%0Anotions%20of%20values%20in%20LLMs%20are%20limited.%20In%20cognitive%20science%2C%20so-called%0A%22cognitive%20models%22%20provide%20formal%20accounts%20of%20such%20trade-offs%20in%20humans%2C%20by%0Amodeling%20the%20weighting%20of%20a%20speaker%27s%20competing%20utility%20functions%20in%20choosing%0Aan%20action%20or%20utterance.%20Here%20we%20use%20a%20leading%20cognitive%20model%20of%20polite%20speech%0Ato%20systematically%20evaluate%20value%20trade-offs%20in%20two%20encompassing%20model%20settings%3A%0Adegrees%20of%20reasoning%20%22effort%22%20in%20frontier%20black-box%20models%2C%20and%20RL%0Apost-training%20dynamics%20of%20open-source%20models.%20Our%20results%20highlight%20patterns%20of%0Ahigher%20informational%20utility%20than%20social%20utility%20in%20reasoning%20models%27%20default%0Abehavior%2C%20and%20demonstrate%20that%20these%20patterns%20shift%20in%20predictable%20ways%20when%0Amodels%20are%20prompted%20to%20prioritize%20certain%20goals%20over%20others.%20Our%20findings%20from%0ALLMs%27%20training%20dynamics%20suggest%20large%20shifts%20in%20utility%20values%20early%20on%20in%0Atraining%20with%20persistent%20effects%20of%20the%20choice%20of%20base%20model%20and%20pretraining%0Adata%2C%20compared%20to%20feedback%20dataset%20or%20alignment%20method.%20Our%20framework%20offers%20a%0Aflexible%20tool%20for%20probing%20value%20trade-offs%20across%20diverse%20model%20types%2C%0Aproviding%20insights%20for%20generating%20hypotheses%20about%20other%20social%20behaviors%20such%0Aas%20sycophancy%20and%20for%20shaping%20training%20regimes%20that%20better%20control%20trade-offs%0Abetween%20values%20during%20model%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.20666v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520cognitive%2520models%2520to%2520reveal%2520value%2520trade-offs%2520in%2520language%2520models%26entry.906535625%3DSonia%2520K.%2520Murthy%2520and%2520Rosie%2520Zhao%2520and%2520Jennifer%2520Hu%2520and%2520Sham%2520Kakade%2520and%2520Markus%2520Wulfmeier%2520and%2520Peng%2520Qian%2520and%2520Tomer%2520Ullman%26entry.1292438233%3D%2520%2520Value%2520trade-offs%2520are%2520an%2520integral%2520part%2520of%2520human%2520decision-making%2520and%2520language%250Ause%252C%2520however%252C%2520current%2520tools%2520for%2520interpreting%2520such%2520dynamic%2520and%2520multi-faceted%250Anotions%2520of%2520values%2520in%2520LLMs%2520are%2520limited.%2520In%2520cognitive%2520science%252C%2520so-called%250A%2522cognitive%2520models%2522%2520provide%2520formal%2520accounts%2520of%2520such%2520trade-offs%2520in%2520humans%252C%2520by%250Amodeling%2520the%2520weighting%2520of%2520a%2520speaker%2527s%2520competing%2520utility%2520functions%2520in%2520choosing%250Aan%2520action%2520or%2520utterance.%2520Here%2520we%2520use%2520a%2520leading%2520cognitive%2520model%2520of%2520polite%2520speech%250Ato%2520systematically%2520evaluate%2520value%2520trade-offs%2520in%2520two%2520encompassing%2520model%2520settings%253A%250Adegrees%2520of%2520reasoning%2520%2522effort%2522%2520in%2520frontier%2520black-box%2520models%252C%2520and%2520RL%250Apost-training%2520dynamics%2520of%2520open-source%2520models.%2520Our%2520results%2520highlight%2520patterns%2520of%250Ahigher%2520informational%2520utility%2520than%2520social%2520utility%2520in%2520reasoning%2520models%2527%2520default%250Abehavior%252C%2520and%2520demonstrate%2520that%2520these%2520patterns%2520shift%2520in%2520predictable%2520ways%2520when%250Amodels%2520are%2520prompted%2520to%2520prioritize%2520certain%2520goals%2520over%2520others.%2520Our%2520findings%2520from%250ALLMs%2527%2520training%2520dynamics%2520suggest%2520large%2520shifts%2520in%2520utility%2520values%2520early%2520on%2520in%250Atraining%2520with%2520persistent%2520effects%2520of%2520the%2520choice%2520of%2520base%2520model%2520and%2520pretraining%250Adata%252C%2520compared%2520to%2520feedback%2520dataset%2520or%2520alignment%2520method.%2520Our%2520framework%2520offers%2520a%250Aflexible%2520tool%2520for%2520probing%2520value%2520trade-offs%2520across%2520diverse%2520model%2520types%252C%250Aproviding%2520insights%2520for%2520generating%2520hypotheses%2520about%2520other%2520social%2520behaviors%2520such%250Aas%2520sycophancy%2520and%2520for%2520shaping%2520training%2520regimes%2520that%2520better%2520control%2520trade-offs%250Abetween%2520values%2520during%2520model%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20666v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20cognitive%20models%20to%20reveal%20value%20trade-offs%20in%20language%20models&entry.906535625=Sonia%20K.%20Murthy%20and%20Rosie%20Zhao%20and%20Jennifer%20Hu%20and%20Sham%20Kakade%20and%20Markus%20Wulfmeier%20and%20Peng%20Qian%20and%20Tomer%20Ullman&entry.1292438233=%20%20Value%20trade-offs%20are%20an%20integral%20part%20of%20human%20decision-making%20and%20language%0Ause%2C%20however%2C%20current%20tools%20for%20interpreting%20such%20dynamic%20and%20multi-faceted%0Anotions%20of%20values%20in%20LLMs%20are%20limited.%20In%20cognitive%20science%2C%20so-called%0A%22cognitive%20models%22%20provide%20formal%20accounts%20of%20such%20trade-offs%20in%20humans%2C%20by%0Amodeling%20the%20weighting%20of%20a%20speaker%27s%20competing%20utility%20functions%20in%20choosing%0Aan%20action%20or%20utterance.%20Here%20we%20use%20a%20leading%20cognitive%20model%20of%20polite%20speech%0Ato%20systematically%20evaluate%20value%20trade-offs%20in%20two%20encompassing%20model%20settings%3A%0Adegrees%20of%20reasoning%20%22effort%22%20in%20frontier%20black-box%20models%2C%20and%20RL%0Apost-training%20dynamics%20of%20open-source%20models.%20Our%20results%20highlight%20patterns%20of%0Ahigher%20informational%20utility%20than%20social%20utility%20in%20reasoning%20models%27%20default%0Abehavior%2C%20and%20demonstrate%20that%20these%20patterns%20shift%20in%20predictable%20ways%20when%0Amodels%20are%20prompted%20to%20prioritize%20certain%20goals%20over%20others.%20Our%20findings%20from%0ALLMs%27%20training%20dynamics%20suggest%20large%20shifts%20in%20utility%20values%20early%20on%20in%0Atraining%20with%20persistent%20effects%20of%20the%20choice%20of%20base%20model%20and%20pretraining%0Adata%2C%20compared%20to%20feedback%20dataset%20or%20alignment%20method.%20Our%20framework%20offers%20a%0Aflexible%20tool%20for%20probing%20value%20trade-offs%20across%20diverse%20model%20types%2C%0Aproviding%20insights%20for%20generating%20hypotheses%20about%20other%20social%20behaviors%20such%0Aas%20sycophancy%20and%20for%20shaping%20training%20regimes%20that%20better%20control%20trade-offs%0Abetween%20values%20during%20model%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.20666v3&entry.124074799=Read"},
{"title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and\n  Rationale Inference in Imperfect Information Collaboration Game", "author": "Fangzhou Liang and Tianshi Zheng and Chunkit Chan and Yauwai Yim and Yangqiu Song", "abstract": "  Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models.\n", "link": "http://arxiv.org/abs/2510.04980v1", "date": "2025-10-06", "relevancy": 2.0456, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Hanabi%3A%20Evaluating%20Multi-Agent%20Gameplays%20with%20Theory-of-Mind%20and%0A%20%20Rationale%20Inference%20in%20Imperfect%20Information%20Collaboration%20Game&body=Title%3A%20LLM-Hanabi%3A%20Evaluating%20Multi-Agent%20Gameplays%20with%20Theory-of-Mind%20and%0A%20%20Rationale%20Inference%20in%20Imperfect%20Information%20Collaboration%20Game%0AAuthor%3A%20Fangzhou%20Liang%20and%20Tianshi%20Zheng%20and%20Chunkit%20Chan%20and%20Yauwai%20Yim%20and%20Yangqiu%20Song%0AAbstract%3A%20%20%20Effective%20multi-agent%20collaboration%20requires%20agents%20to%20infer%20the%20rationale%0Abehind%20others%27%20actions%2C%20a%20capability%20rooted%20in%20Theory-of-Mind%20%28ToM%29.%20While%0Arecent%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20logical%20inference%2C%20their%20ability%0Ato%20infer%20rationale%20in%20dynamic%2C%20collaborative%20settings%20remains%20under-explored.%0AThis%20study%20introduces%20LLM-Hanabi%2C%20a%20novel%20benchmark%20that%20uses%20the%20cooperative%0Agame%20Hanabi%20to%20evaluate%20the%20rationale%20inference%20and%20ToM%20of%20LLMs.%20Our%20framework%0Afeatures%20an%20automated%20evaluation%20system%20that%20measures%20both%20game%20performance%20and%0AToM%20proficiency.%20Across%20a%20range%20of%20models%2C%20we%20find%20a%20significant%20positive%0Acorrelation%20between%20ToM%20and%20in-game%20success.%20Notably%2C%20first-order%20ToM%0A%28interpreting%20others%27%20intent%29%20correlates%20more%20strongly%20with%20performance%20than%0Asecond-order%20ToM%20%28predicting%20others%27%20interpretations%29.%20These%20findings%20highlight%0Athat%20for%20effective%20AI%20collaboration%2C%20the%20ability%20to%20accurately%20interpret%20a%0Apartner%27s%20rationale%20is%20more%20critical%20than%20higher-order%20reasoning.%20We%20conclude%0Athat%20prioritizing%20first-order%20ToM%20is%20a%20promising%20direction%20for%20enhancing%20the%0Acollaborative%20capabilities%20of%20future%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Hanabi%253A%2520Evaluating%2520Multi-Agent%2520Gameplays%2520with%2520Theory-of-Mind%2520and%250A%2520%2520Rationale%2520Inference%2520in%2520Imperfect%2520Information%2520Collaboration%2520Game%26entry.906535625%3DFangzhou%2520Liang%2520and%2520Tianshi%2520Zheng%2520and%2520Chunkit%2520Chan%2520and%2520Yauwai%2520Yim%2520and%2520Yangqiu%2520Song%26entry.1292438233%3D%2520%2520Effective%2520multi-agent%2520collaboration%2520requires%2520agents%2520to%2520infer%2520the%2520rationale%250Abehind%2520others%2527%2520actions%252C%2520a%2520capability%2520rooted%2520in%2520Theory-of-Mind%2520%2528ToM%2529.%2520While%250Arecent%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520at%2520logical%2520inference%252C%2520their%2520ability%250Ato%2520infer%2520rationale%2520in%2520dynamic%252C%2520collaborative%2520settings%2520remains%2520under-explored.%250AThis%2520study%2520introduces%2520LLM-Hanabi%252C%2520a%2520novel%2520benchmark%2520that%2520uses%2520the%2520cooperative%250Agame%2520Hanabi%2520to%2520evaluate%2520the%2520rationale%2520inference%2520and%2520ToM%2520of%2520LLMs.%2520Our%2520framework%250Afeatures%2520an%2520automated%2520evaluation%2520system%2520that%2520measures%2520both%2520game%2520performance%2520and%250AToM%2520proficiency.%2520Across%2520a%2520range%2520of%2520models%252C%2520we%2520find%2520a%2520significant%2520positive%250Acorrelation%2520between%2520ToM%2520and%2520in-game%2520success.%2520Notably%252C%2520first-order%2520ToM%250A%2528interpreting%2520others%2527%2520intent%2529%2520correlates%2520more%2520strongly%2520with%2520performance%2520than%250Asecond-order%2520ToM%2520%2528predicting%2520others%2527%2520interpretations%2529.%2520These%2520findings%2520highlight%250Athat%2520for%2520effective%2520AI%2520collaboration%252C%2520the%2520ability%2520to%2520accurately%2520interpret%2520a%250Apartner%2527s%2520rationale%2520is%2520more%2520critical%2520than%2520higher-order%2520reasoning.%2520We%2520conclude%250Athat%2520prioritizing%2520first-order%2520ToM%2520is%2520a%2520promising%2520direction%2520for%2520enhancing%2520the%250Acollaborative%2520capabilities%2520of%2520future%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Hanabi%3A%20Evaluating%20Multi-Agent%20Gameplays%20with%20Theory-of-Mind%20and%0A%20%20Rationale%20Inference%20in%20Imperfect%20Information%20Collaboration%20Game&entry.906535625=Fangzhou%20Liang%20and%20Tianshi%20Zheng%20and%20Chunkit%20Chan%20and%20Yauwai%20Yim%20and%20Yangqiu%20Song&entry.1292438233=%20%20Effective%20multi-agent%20collaboration%20requires%20agents%20to%20infer%20the%20rationale%0Abehind%20others%27%20actions%2C%20a%20capability%20rooted%20in%20Theory-of-Mind%20%28ToM%29.%20While%0Arecent%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20logical%20inference%2C%20their%20ability%0Ato%20infer%20rationale%20in%20dynamic%2C%20collaborative%20settings%20remains%20under-explored.%0AThis%20study%20introduces%20LLM-Hanabi%2C%20a%20novel%20benchmark%20that%20uses%20the%20cooperative%0Agame%20Hanabi%20to%20evaluate%20the%20rationale%20inference%20and%20ToM%20of%20LLMs.%20Our%20framework%0Afeatures%20an%20automated%20evaluation%20system%20that%20measures%20both%20game%20performance%20and%0AToM%20proficiency.%20Across%20a%20range%20of%20models%2C%20we%20find%20a%20significant%20positive%0Acorrelation%20between%20ToM%20and%20in-game%20success.%20Notably%2C%20first-order%20ToM%0A%28interpreting%20others%27%20intent%29%20correlates%20more%20strongly%20with%20performance%20than%0Asecond-order%20ToM%20%28predicting%20others%27%20interpretations%29.%20These%20findings%20highlight%0Athat%20for%20effective%20AI%20collaboration%2C%20the%20ability%20to%20accurately%20interpret%20a%0Apartner%27s%20rationale%20is%20more%20critical%20than%20higher-order%20reasoning.%20We%20conclude%0Athat%20prioritizing%20first-order%20ToM%20is%20a%20promising%20direction%20for%20enhancing%20the%0Acollaborative%20capabilities%20of%20future%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04980v1&entry.124074799=Read"},
{"title": "Human Behavior Atlas: Benchmarking Unified Psychological and Social\n  Behavior Understanding", "author": "Keane Ong and Wei Dai and Carol Li and Dewei Feng and Hengzhi Li and Jingyao Wu and Jiaee Cheong and Rui Mao and Gianmarco Mengaldo and Erik Cambria and Paul Pu Liang", "abstract": "  Using intelligent systems to perceive psychological and social behaviors,\nthat is, the underlying affective, cognitive, and pathological states that are\nmanifested through observable behaviors and social interactions, remains a\nchallenge due to their complex, multifaceted, and personalized nature. Existing\nwork tackling these dimensions through specialized datasets and single-task\nsystems often miss opportunities for scalability, cross-task transfer, and\nbroader generalization. To address this gap, we curate Human Behavior Atlas, a\nunified benchmark of diverse behavioral tasks designed to support the\ndevelopment of unified models for understanding psychological and social\nbehaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,\naudio, and visual modalities, covering tasks on affective states, cognitive\nstates, pathologies, and social processes. Our unification efforts can reduce\nredundancy and cost, enable training to scale efficiently across tasks, and\nenhance generalization of behavioral features across domains. On Human Behavior\nAtlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and\nOmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models\nto consistently outperform existing multimodal LLMs across diverse behavioral\ntasks. Pretraining on Human Behavior Atlas also improves transfer to novel\nbehavioral datasets; with the targeted use of behavioral descriptors yielding\nmeaningful performance gains.\n", "link": "http://arxiv.org/abs/2510.04899v1", "date": "2025-10-06", "relevancy": 2.0308, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Behavior%20Atlas%3A%20Benchmarking%20Unified%20Psychological%20and%20Social%0A%20%20Behavior%20Understanding&body=Title%3A%20Human%20Behavior%20Atlas%3A%20Benchmarking%20Unified%20Psychological%20and%20Social%0A%20%20Behavior%20Understanding%0AAuthor%3A%20Keane%20Ong%20and%20Wei%20Dai%20and%20Carol%20Li%20and%20Dewei%20Feng%20and%20Hengzhi%20Li%20and%20Jingyao%20Wu%20and%20Jiaee%20Cheong%20and%20Rui%20Mao%20and%20Gianmarco%20Mengaldo%20and%20Erik%20Cambria%20and%20Paul%20Pu%20Liang%0AAbstract%3A%20%20%20Using%20intelligent%20systems%20to%20perceive%20psychological%20and%20social%20behaviors%2C%0Athat%20is%2C%20the%20underlying%20affective%2C%20cognitive%2C%20and%20pathological%20states%20that%20are%0Amanifested%20through%20observable%20behaviors%20and%20social%20interactions%2C%20remains%20a%0Achallenge%20due%20to%20their%20complex%2C%20multifaceted%2C%20and%20personalized%20nature.%20Existing%0Awork%20tackling%20these%20dimensions%20through%20specialized%20datasets%20and%20single-task%0Asystems%20often%20miss%20opportunities%20for%20scalability%2C%20cross-task%20transfer%2C%20and%0Abroader%20generalization.%20To%20address%20this%20gap%2C%20we%20curate%20Human%20Behavior%20Atlas%2C%20a%0Aunified%20benchmark%20of%20diverse%20behavioral%20tasks%20designed%20to%20support%20the%0Adevelopment%20of%20unified%20models%20for%20understanding%20psychological%20and%20social%0Abehaviors.%20Human%20Behavior%20Atlas%20comprises%20over%20100%2C000%20samples%20spanning%20text%2C%0Aaudio%2C%20and%20visual%20modalities%2C%20covering%20tasks%20on%20affective%20states%2C%20cognitive%0Astates%2C%20pathologies%2C%20and%20social%20processes.%20Our%20unification%20efforts%20can%20reduce%0Aredundancy%20and%20cost%2C%20enable%20training%20to%20scale%20efficiently%20across%20tasks%2C%20and%0Aenhance%20generalization%20of%20behavioral%20features%20across%20domains.%20On%20Human%20Behavior%0AAtlas%2C%20we%20train%20three%20models%3A%20OmniSapiens-7B%20SFT%2C%20OmniSapiens-7B%20BAM%2C%20and%0AOmniSapiens-7B%20RL.%20We%20show%20that%20training%20on%20Human%20Behavior%20Atlas%20enables%20models%0Ato%20consistently%20outperform%20existing%20multimodal%20LLMs%20across%20diverse%20behavioral%0Atasks.%20Pretraining%20on%20Human%20Behavior%20Atlas%20also%20improves%20transfer%20to%20novel%0Abehavioral%20datasets%3B%20with%20the%20targeted%20use%20of%20behavioral%20descriptors%20yielding%0Ameaningful%20performance%20gains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Behavior%2520Atlas%253A%2520Benchmarking%2520Unified%2520Psychological%2520and%2520Social%250A%2520%2520Behavior%2520Understanding%26entry.906535625%3DKeane%2520Ong%2520and%2520Wei%2520Dai%2520and%2520Carol%2520Li%2520and%2520Dewei%2520Feng%2520and%2520Hengzhi%2520Li%2520and%2520Jingyao%2520Wu%2520and%2520Jiaee%2520Cheong%2520and%2520Rui%2520Mao%2520and%2520Gianmarco%2520Mengaldo%2520and%2520Erik%2520Cambria%2520and%2520Paul%2520Pu%2520Liang%26entry.1292438233%3D%2520%2520Using%2520intelligent%2520systems%2520to%2520perceive%2520psychological%2520and%2520social%2520behaviors%252C%250Athat%2520is%252C%2520the%2520underlying%2520affective%252C%2520cognitive%252C%2520and%2520pathological%2520states%2520that%2520are%250Amanifested%2520through%2520observable%2520behaviors%2520and%2520social%2520interactions%252C%2520remains%2520a%250Achallenge%2520due%2520to%2520their%2520complex%252C%2520multifaceted%252C%2520and%2520personalized%2520nature.%2520Existing%250Awork%2520tackling%2520these%2520dimensions%2520through%2520specialized%2520datasets%2520and%2520single-task%250Asystems%2520often%2520miss%2520opportunities%2520for%2520scalability%252C%2520cross-task%2520transfer%252C%2520and%250Abroader%2520generalization.%2520To%2520address%2520this%2520gap%252C%2520we%2520curate%2520Human%2520Behavior%2520Atlas%252C%2520a%250Aunified%2520benchmark%2520of%2520diverse%2520behavioral%2520tasks%2520designed%2520to%2520support%2520the%250Adevelopment%2520of%2520unified%2520models%2520for%2520understanding%2520psychological%2520and%2520social%250Abehaviors.%2520Human%2520Behavior%2520Atlas%2520comprises%2520over%2520100%252C000%2520samples%2520spanning%2520text%252C%250Aaudio%252C%2520and%2520visual%2520modalities%252C%2520covering%2520tasks%2520on%2520affective%2520states%252C%2520cognitive%250Astates%252C%2520pathologies%252C%2520and%2520social%2520processes.%2520Our%2520unification%2520efforts%2520can%2520reduce%250Aredundancy%2520and%2520cost%252C%2520enable%2520training%2520to%2520scale%2520efficiently%2520across%2520tasks%252C%2520and%250Aenhance%2520generalization%2520of%2520behavioral%2520features%2520across%2520domains.%2520On%2520Human%2520Behavior%250AAtlas%252C%2520we%2520train%2520three%2520models%253A%2520OmniSapiens-7B%2520SFT%252C%2520OmniSapiens-7B%2520BAM%252C%2520and%250AOmniSapiens-7B%2520RL.%2520We%2520show%2520that%2520training%2520on%2520Human%2520Behavior%2520Atlas%2520enables%2520models%250Ato%2520consistently%2520outperform%2520existing%2520multimodal%2520LLMs%2520across%2520diverse%2520behavioral%250Atasks.%2520Pretraining%2520on%2520Human%2520Behavior%2520Atlas%2520also%2520improves%2520transfer%2520to%2520novel%250Abehavioral%2520datasets%253B%2520with%2520the%2520targeted%2520use%2520of%2520behavioral%2520descriptors%2520yielding%250Ameaningful%2520performance%2520gains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Behavior%20Atlas%3A%20Benchmarking%20Unified%20Psychological%20and%20Social%0A%20%20Behavior%20Understanding&entry.906535625=Keane%20Ong%20and%20Wei%20Dai%20and%20Carol%20Li%20and%20Dewei%20Feng%20and%20Hengzhi%20Li%20and%20Jingyao%20Wu%20and%20Jiaee%20Cheong%20and%20Rui%20Mao%20and%20Gianmarco%20Mengaldo%20and%20Erik%20Cambria%20and%20Paul%20Pu%20Liang&entry.1292438233=%20%20Using%20intelligent%20systems%20to%20perceive%20psychological%20and%20social%20behaviors%2C%0Athat%20is%2C%20the%20underlying%20affective%2C%20cognitive%2C%20and%20pathological%20states%20that%20are%0Amanifested%20through%20observable%20behaviors%20and%20social%20interactions%2C%20remains%20a%0Achallenge%20due%20to%20their%20complex%2C%20multifaceted%2C%20and%20personalized%20nature.%20Existing%0Awork%20tackling%20these%20dimensions%20through%20specialized%20datasets%20and%20single-task%0Asystems%20often%20miss%20opportunities%20for%20scalability%2C%20cross-task%20transfer%2C%20and%0Abroader%20generalization.%20To%20address%20this%20gap%2C%20we%20curate%20Human%20Behavior%20Atlas%2C%20a%0Aunified%20benchmark%20of%20diverse%20behavioral%20tasks%20designed%20to%20support%20the%0Adevelopment%20of%20unified%20models%20for%20understanding%20psychological%20and%20social%0Abehaviors.%20Human%20Behavior%20Atlas%20comprises%20over%20100%2C000%20samples%20spanning%20text%2C%0Aaudio%2C%20and%20visual%20modalities%2C%20covering%20tasks%20on%20affective%20states%2C%20cognitive%0Astates%2C%20pathologies%2C%20and%20social%20processes.%20Our%20unification%20efforts%20can%20reduce%0Aredundancy%20and%20cost%2C%20enable%20training%20to%20scale%20efficiently%20across%20tasks%2C%20and%0Aenhance%20generalization%20of%20behavioral%20features%20across%20domains.%20On%20Human%20Behavior%0AAtlas%2C%20we%20train%20three%20models%3A%20OmniSapiens-7B%20SFT%2C%20OmniSapiens-7B%20BAM%2C%20and%0AOmniSapiens-7B%20RL.%20We%20show%20that%20training%20on%20Human%20Behavior%20Atlas%20enables%20models%0Ato%20consistently%20outperform%20existing%20multimodal%20LLMs%20across%20diverse%20behavioral%0Atasks.%20Pretraining%20on%20Human%20Behavior%20Atlas%20also%20improves%20transfer%20to%20novel%0Abehavioral%20datasets%3B%20with%20the%20targeted%20use%20of%20behavioral%20descriptors%20yielding%0Ameaningful%20performance%20gains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04899v1&entry.124074799=Read"},
{"title": "BenthiCat: An opti-acoustic dataset for advancing benthic classification\n  and habitat mapping", "author": "Hayat Rajani and Valerio Franchi and Borja Martinez-Clavel Valles and Raimon Ramos and Rafael Garcia and Nuno Gracias", "abstract": "  Benthic habitat mapping is fundamental for understanding marine ecosystems,\nguiding conservation efforts, and supporting sustainable resource management.\nYet, the scarcity of large, annotated datasets limits the development and\nbenchmarking of machine learning models in this domain. This paper introduces a\nthorough multi-modal dataset, comprising about a million side-scan sonar (SSS)\ntiles collected along the coast of Catalonia (Spain), complemented by\nbathymetric maps and a set of co-registered optical images from targeted\nsurveys using an autonomous underwater vehicle (AUV). Approximately \\num{36000}\nof the SSS tiles have been manually annotated with segmentation masks to enable\nsupervised fine-tuning of classification models. All the raw sensor data,\ntogether with mosaics, are also released to support further exploration and\nalgorithm development. To address challenges in multi-sensor data fusion for\nAUVs, we spatially associate optical images with corresponding SSS tiles,\nfacilitating self-supervised, cross-modal representation learning. Accompanying\nopen-source preprocessing and annotation tools are provided to enhance\naccessibility and encourage research. This resource aims to establish a\nstandardized benchmark for underwater habitat mapping, promoting advancements\nin autonomous seafloor classification and multi-sensor integration.\n", "link": "http://arxiv.org/abs/2510.04876v1", "date": "2025-10-06", "relevancy": 2.0284, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5195}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5046}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BenthiCat%3A%20An%20opti-acoustic%20dataset%20for%20advancing%20benthic%20classification%0A%20%20and%20habitat%20mapping&body=Title%3A%20BenthiCat%3A%20An%20opti-acoustic%20dataset%20for%20advancing%20benthic%20classification%0A%20%20and%20habitat%20mapping%0AAuthor%3A%20Hayat%20Rajani%20and%20Valerio%20Franchi%20and%20Borja%20Martinez-Clavel%20Valles%20and%20Raimon%20Ramos%20and%20Rafael%20Garcia%20and%20Nuno%20Gracias%0AAbstract%3A%20%20%20Benthic%20habitat%20mapping%20is%20fundamental%20for%20understanding%20marine%20ecosystems%2C%0Aguiding%20conservation%20efforts%2C%20and%20supporting%20sustainable%20resource%20management.%0AYet%2C%20the%20scarcity%20of%20large%2C%20annotated%20datasets%20limits%20the%20development%20and%0Abenchmarking%20of%20machine%20learning%20models%20in%20this%20domain.%20This%20paper%20introduces%20a%0Athorough%20multi-modal%20dataset%2C%20comprising%20about%20a%20million%20side-scan%20sonar%20%28SSS%29%0Atiles%20collected%20along%20the%20coast%20of%20Catalonia%20%28Spain%29%2C%20complemented%20by%0Abathymetric%20maps%20and%20a%20set%20of%20co-registered%20optical%20images%20from%20targeted%0Asurveys%20using%20an%20autonomous%20underwater%20vehicle%20%28AUV%29.%20Approximately%20%5Cnum%7B36000%7D%0Aof%20the%20SSS%20tiles%20have%20been%20manually%20annotated%20with%20segmentation%20masks%20to%20enable%0Asupervised%20fine-tuning%20of%20classification%20models.%20All%20the%20raw%20sensor%20data%2C%0Atogether%20with%20mosaics%2C%20are%20also%20released%20to%20support%20further%20exploration%20and%0Aalgorithm%20development.%20To%20address%20challenges%20in%20multi-sensor%20data%20fusion%20for%0AAUVs%2C%20we%20spatially%20associate%20optical%20images%20with%20corresponding%20SSS%20tiles%2C%0Afacilitating%20self-supervised%2C%20cross-modal%20representation%20learning.%20Accompanying%0Aopen-source%20preprocessing%20and%20annotation%20tools%20are%20provided%20to%20enhance%0Aaccessibility%20and%20encourage%20research.%20This%20resource%20aims%20to%20establish%20a%0Astandardized%20benchmark%20for%20underwater%20habitat%20mapping%2C%20promoting%20advancements%0Ain%20autonomous%20seafloor%20classification%20and%20multi-sensor%20integration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenthiCat%253A%2520An%2520opti-acoustic%2520dataset%2520for%2520advancing%2520benthic%2520classification%250A%2520%2520and%2520habitat%2520mapping%26entry.906535625%3DHayat%2520Rajani%2520and%2520Valerio%2520Franchi%2520and%2520Borja%2520Martinez-Clavel%2520Valles%2520and%2520Raimon%2520Ramos%2520and%2520Rafael%2520Garcia%2520and%2520Nuno%2520Gracias%26entry.1292438233%3D%2520%2520Benthic%2520habitat%2520mapping%2520is%2520fundamental%2520for%2520understanding%2520marine%2520ecosystems%252C%250Aguiding%2520conservation%2520efforts%252C%2520and%2520supporting%2520sustainable%2520resource%2520management.%250AYet%252C%2520the%2520scarcity%2520of%2520large%252C%2520annotated%2520datasets%2520limits%2520the%2520development%2520and%250Abenchmarking%2520of%2520machine%2520learning%2520models%2520in%2520this%2520domain.%2520This%2520paper%2520introduces%2520a%250Athorough%2520multi-modal%2520dataset%252C%2520comprising%2520about%2520a%2520million%2520side-scan%2520sonar%2520%2528SSS%2529%250Atiles%2520collected%2520along%2520the%2520coast%2520of%2520Catalonia%2520%2528Spain%2529%252C%2520complemented%2520by%250Abathymetric%2520maps%2520and%2520a%2520set%2520of%2520co-registered%2520optical%2520images%2520from%2520targeted%250Asurveys%2520using%2520an%2520autonomous%2520underwater%2520vehicle%2520%2528AUV%2529.%2520Approximately%2520%255Cnum%257B36000%257D%250Aof%2520the%2520SSS%2520tiles%2520have%2520been%2520manually%2520annotated%2520with%2520segmentation%2520masks%2520to%2520enable%250Asupervised%2520fine-tuning%2520of%2520classification%2520models.%2520All%2520the%2520raw%2520sensor%2520data%252C%250Atogether%2520with%2520mosaics%252C%2520are%2520also%2520released%2520to%2520support%2520further%2520exploration%2520and%250Aalgorithm%2520development.%2520To%2520address%2520challenges%2520in%2520multi-sensor%2520data%2520fusion%2520for%250AAUVs%252C%2520we%2520spatially%2520associate%2520optical%2520images%2520with%2520corresponding%2520SSS%2520tiles%252C%250Afacilitating%2520self-supervised%252C%2520cross-modal%2520representation%2520learning.%2520Accompanying%250Aopen-source%2520preprocessing%2520and%2520annotation%2520tools%2520are%2520provided%2520to%2520enhance%250Aaccessibility%2520and%2520encourage%2520research.%2520This%2520resource%2520aims%2520to%2520establish%2520a%250Astandardized%2520benchmark%2520for%2520underwater%2520habitat%2520mapping%252C%2520promoting%2520advancements%250Ain%2520autonomous%2520seafloor%2520classification%2520and%2520multi-sensor%2520integration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BenthiCat%3A%20An%20opti-acoustic%20dataset%20for%20advancing%20benthic%20classification%0A%20%20and%20habitat%20mapping&entry.906535625=Hayat%20Rajani%20and%20Valerio%20Franchi%20and%20Borja%20Martinez-Clavel%20Valles%20and%20Raimon%20Ramos%20and%20Rafael%20Garcia%20and%20Nuno%20Gracias&entry.1292438233=%20%20Benthic%20habitat%20mapping%20is%20fundamental%20for%20understanding%20marine%20ecosystems%2C%0Aguiding%20conservation%20efforts%2C%20and%20supporting%20sustainable%20resource%20management.%0AYet%2C%20the%20scarcity%20of%20large%2C%20annotated%20datasets%20limits%20the%20development%20and%0Abenchmarking%20of%20machine%20learning%20models%20in%20this%20domain.%20This%20paper%20introduces%20a%0Athorough%20multi-modal%20dataset%2C%20comprising%20about%20a%20million%20side-scan%20sonar%20%28SSS%29%0Atiles%20collected%20along%20the%20coast%20of%20Catalonia%20%28Spain%29%2C%20complemented%20by%0Abathymetric%20maps%20and%20a%20set%20of%20co-registered%20optical%20images%20from%20targeted%0Asurveys%20using%20an%20autonomous%20underwater%20vehicle%20%28AUV%29.%20Approximately%20%5Cnum%7B36000%7D%0Aof%20the%20SSS%20tiles%20have%20been%20manually%20annotated%20with%20segmentation%20masks%20to%20enable%0Asupervised%20fine-tuning%20of%20classification%20models.%20All%20the%20raw%20sensor%20data%2C%0Atogether%20with%20mosaics%2C%20are%20also%20released%20to%20support%20further%20exploration%20and%0Aalgorithm%20development.%20To%20address%20challenges%20in%20multi-sensor%20data%20fusion%20for%0AAUVs%2C%20we%20spatially%20associate%20optical%20images%20with%20corresponding%20SSS%20tiles%2C%0Afacilitating%20self-supervised%2C%20cross-modal%20representation%20learning.%20Accompanying%0Aopen-source%20preprocessing%20and%20annotation%20tools%20are%20provided%20to%20enhance%0Aaccessibility%20and%20encourage%20research.%20This%20resource%20aims%20to%20establish%20a%0Astandardized%20benchmark%20for%20underwater%20habitat%20mapping%2C%20promoting%20advancements%0Ain%20autonomous%20seafloor%20classification%20and%20multi-sensor%20integration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04876v1&entry.124074799=Read"},
{"title": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol", "author": "Harshvardhan Mestha and Karan Bania and Shreyas V Sathyanarayana and Sidong Liu and Ashwin Srinivasan", "abstract": "  Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems. Our code is available\nat https://github.com/karannb/interact.\n", "link": "http://arxiv.org/abs/2410.20600v3", "date": "2025-10-06", "relevancy": 2.0261, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5223}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Turn%20Human-LLM%20Interaction%20Through%20the%20Lens%20of%20a%20Two-Way%0A%20%20Intelligibility%20Protocol&body=Title%3A%20Multi-Turn%20Human-LLM%20Interaction%20Through%20the%20Lens%20of%20a%20Two-Way%0A%20%20Intelligibility%20Protocol%0AAuthor%3A%20Harshvardhan%20Mestha%20and%20Karan%20Bania%20and%20Shreyas%20V%20Sathyanarayana%20and%20Sidong%20Liu%20and%20Ashwin%20Srinivasan%0AAbstract%3A%20%20%20Our%20interest%20is%20in%20the%20design%20of%20software%20systems%20involving%20a%20human-expert%0Ainteracting%20--%20using%20natural%20language%20--%20with%20a%20large%20language%20model%20%28LLM%29%20on%0Adata%20analysis%20tasks.%20For%20complex%20problems%2C%20it%20is%20possible%20that%20LLMs%20can%20harness%0Ahuman%20expertise%20and%20creativity%20to%20find%20solutions%20that%20were%20otherwise%20elusive.%0AOn%20one%20level%2C%20this%20interaction%20takes%20place%20through%20multiple%20turns%20of%20prompts%0Afrom%20the%20human%20and%20responses%20from%20the%20LLM.%20Here%20we%20investigate%20a%20more%0Astructured%20approach%20based%20on%20an%20abstract%20protocol%20described%20in%20%5B3%5D%20for%0Ainteraction%20between%20agents.%20The%20protocol%20is%20motivated%20by%20a%20notion%20of%20%22two-way%0Aintelligibility%22%20and%20is%20modelled%20by%20a%20pair%20of%20communicating%20finite-state%0Amachines.%20We%20provide%20an%20implementation%20of%20the%20protocol%2C%20and%20provide%20empirical%0Aevidence%20of%20using%20the%20implementation%20to%20mediate%20interactions%20between%20an%20LLM%20and%0Aa%20human-agent%20in%20two%20areas%20of%20scientific%20interest%20%28radiology%20and%20drug%20design%29.%0AWe%20conduct%20controlled%20experiments%20with%20a%20human%20proxy%20%28a%20database%29%2C%20and%0Auncontrolled%20experiments%20with%20human%20subjects.%20The%20results%20provide%20evidence%20in%0Asupport%20of%20the%20protocol%27s%20capability%20of%20capturing%20one-%20and%20two-way%0Aintelligibility%20in%20human-LLM%20interaction%3B%20and%20for%20the%20utility%20of%20two-way%0Aintelligibility%20in%20the%20design%20of%20human-machine%20systems.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/karannb/interact.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20600v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Turn%2520Human-LLM%2520Interaction%2520Through%2520the%2520Lens%2520of%2520a%2520Two-Way%250A%2520%2520Intelligibility%2520Protocol%26entry.906535625%3DHarshvardhan%2520Mestha%2520and%2520Karan%2520Bania%2520and%2520Shreyas%2520V%2520Sathyanarayana%2520and%2520Sidong%2520Liu%2520and%2520Ashwin%2520Srinivasan%26entry.1292438233%3D%2520%2520Our%2520interest%2520is%2520in%2520the%2520design%2520of%2520software%2520systems%2520involving%2520a%2520human-expert%250Ainteracting%2520--%2520using%2520natural%2520language%2520--%2520with%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520on%250Adata%2520analysis%2520tasks.%2520For%2520complex%2520problems%252C%2520it%2520is%2520possible%2520that%2520LLMs%2520can%2520harness%250Ahuman%2520expertise%2520and%2520creativity%2520to%2520find%2520solutions%2520that%2520were%2520otherwise%2520elusive.%250AOn%2520one%2520level%252C%2520this%2520interaction%2520takes%2520place%2520through%2520multiple%2520turns%2520of%2520prompts%250Afrom%2520the%2520human%2520and%2520responses%2520from%2520the%2520LLM.%2520Here%2520we%2520investigate%2520a%2520more%250Astructured%2520approach%2520based%2520on%2520an%2520abstract%2520protocol%2520described%2520in%2520%255B3%255D%2520for%250Ainteraction%2520between%2520agents.%2520The%2520protocol%2520is%2520motivated%2520by%2520a%2520notion%2520of%2520%2522two-way%250Aintelligibility%2522%2520and%2520is%2520modelled%2520by%2520a%2520pair%2520of%2520communicating%2520finite-state%250Amachines.%2520We%2520provide%2520an%2520implementation%2520of%2520the%2520protocol%252C%2520and%2520provide%2520empirical%250Aevidence%2520of%2520using%2520the%2520implementation%2520to%2520mediate%2520interactions%2520between%2520an%2520LLM%2520and%250Aa%2520human-agent%2520in%2520two%2520areas%2520of%2520scientific%2520interest%2520%2528radiology%2520and%2520drug%2520design%2529.%250AWe%2520conduct%2520controlled%2520experiments%2520with%2520a%2520human%2520proxy%2520%2528a%2520database%2529%252C%2520and%250Auncontrolled%2520experiments%2520with%2520human%2520subjects.%2520The%2520results%2520provide%2520evidence%2520in%250Asupport%2520of%2520the%2520protocol%2527s%2520capability%2520of%2520capturing%2520one-%2520and%2520two-way%250Aintelligibility%2520in%2520human-LLM%2520interaction%253B%2520and%2520for%2520the%2520utility%2520of%2520two-way%250Aintelligibility%2520in%2520the%2520design%2520of%2520human-machine%2520systems.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/karannb/interact.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20600v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Turn%20Human-LLM%20Interaction%20Through%20the%20Lens%20of%20a%20Two-Way%0A%20%20Intelligibility%20Protocol&entry.906535625=Harshvardhan%20Mestha%20and%20Karan%20Bania%20and%20Shreyas%20V%20Sathyanarayana%20and%20Sidong%20Liu%20and%20Ashwin%20Srinivasan&entry.1292438233=%20%20Our%20interest%20is%20in%20the%20design%20of%20software%20systems%20involving%20a%20human-expert%0Ainteracting%20--%20using%20natural%20language%20--%20with%20a%20large%20language%20model%20%28LLM%29%20on%0Adata%20analysis%20tasks.%20For%20complex%20problems%2C%20it%20is%20possible%20that%20LLMs%20can%20harness%0Ahuman%20expertise%20and%20creativity%20to%20find%20solutions%20that%20were%20otherwise%20elusive.%0AOn%20one%20level%2C%20this%20interaction%20takes%20place%20through%20multiple%20turns%20of%20prompts%0Afrom%20the%20human%20and%20responses%20from%20the%20LLM.%20Here%20we%20investigate%20a%20more%0Astructured%20approach%20based%20on%20an%20abstract%20protocol%20described%20in%20%5B3%5D%20for%0Ainteraction%20between%20agents.%20The%20protocol%20is%20motivated%20by%20a%20notion%20of%20%22two-way%0Aintelligibility%22%20and%20is%20modelled%20by%20a%20pair%20of%20communicating%20finite-state%0Amachines.%20We%20provide%20an%20implementation%20of%20the%20protocol%2C%20and%20provide%20empirical%0Aevidence%20of%20using%20the%20implementation%20to%20mediate%20interactions%20between%20an%20LLM%20and%0Aa%20human-agent%20in%20two%20areas%20of%20scientific%20interest%20%28radiology%20and%20drug%20design%29.%0AWe%20conduct%20controlled%20experiments%20with%20a%20human%20proxy%20%28a%20database%29%2C%20and%0Auncontrolled%20experiments%20with%20human%20subjects.%20The%20results%20provide%20evidence%20in%0Asupport%20of%20the%20protocol%27s%20capability%20of%20capturing%20one-%20and%20two-way%0Aintelligibility%20in%20human-LLM%20interaction%3B%20and%20for%20the%20utility%20of%20two-way%0Aintelligibility%20in%20the%20design%20of%20human-machine%20systems.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/karannb/interact.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20600v3&entry.124074799=Read"},
{"title": "Modeling Student Learning with 3.8 Million Program Traces", "author": "Alexis Ross and Megha Srivastava and Jeremiah Blanchard and Jacob Andreas", "abstract": "  As programmers write code, they often edit and retry multiple times, creating\nrich \"interaction traces\" that reveal how they approach coding tasks and\nprovide clues about their level of skill development. For novice programmers in\nparticular, these traces reflect the diverse reasoning processes they employ to\ncode, such as exploratory behavior to understand how a programming concept\nworks, re-strategizing in response to bugs, and personalizing stylistic\nchoices. In this work, we explore what can be learned from training language\nmodels on such reasoning traces: not just about code, but about coders, and\nparticularly students learning to program. We introduce a dataset of over 3.8\nmillion programming reasoning traces from users of Pencil Code, a free online\neducational platform used by students to learn simple programming concepts.\nCompared to models trained only on final programs or synthetically-generated\ntraces, we find that models trained on real traces are stronger at modeling\ndiverse student behavior. Through both behavioral and probing analyses, we also\nfind that many properties of code traces, such as goal backtracking or number\nof comments, can be predicted from learned representations of the students who\nwrite them. Building on this result, we show that we can help students recover\nfrom mistakes by steering code generation models to identify a sequence of\nedits that will results in more correct code while remaining close to the\noriginal student's style. Together, our results suggest that many properties of\ncode are properties of individual students and that training on edit traces can\nlead to models that are more steerable, more predictive of student behavior\nwhile programming, and better at generating programs in their final states.\nCode and data is available at https://github.com/meghabyte/pencilcode-public\n", "link": "http://arxiv.org/abs/2510.05056v1", "date": "2025-10-06", "relevancy": 2.024, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5089}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Student%20Learning%20with%203.8%20Million%20Program%20Traces&body=Title%3A%20Modeling%20Student%20Learning%20with%203.8%20Million%20Program%20Traces%0AAuthor%3A%20Alexis%20Ross%20and%20Megha%20Srivastava%20and%20Jeremiah%20Blanchard%20and%20Jacob%20Andreas%0AAbstract%3A%20%20%20As%20programmers%20write%20code%2C%20they%20often%20edit%20and%20retry%20multiple%20times%2C%20creating%0Arich%20%22interaction%20traces%22%20that%20reveal%20how%20they%20approach%20coding%20tasks%20and%0Aprovide%20clues%20about%20their%20level%20of%20skill%20development.%20For%20novice%20programmers%20in%0Aparticular%2C%20these%20traces%20reflect%20the%20diverse%20reasoning%20processes%20they%20employ%20to%0Acode%2C%20such%20as%20exploratory%20behavior%20to%20understand%20how%20a%20programming%20concept%0Aworks%2C%20re-strategizing%20in%20response%20to%20bugs%2C%20and%20personalizing%20stylistic%0Achoices.%20In%20this%20work%2C%20we%20explore%20what%20can%20be%20learned%20from%20training%20language%0Amodels%20on%20such%20reasoning%20traces%3A%20not%20just%20about%20code%2C%20but%20about%20coders%2C%20and%0Aparticularly%20students%20learning%20to%20program.%20We%20introduce%20a%20dataset%20of%20over%203.8%0Amillion%20programming%20reasoning%20traces%20from%20users%20of%20Pencil%20Code%2C%20a%20free%20online%0Aeducational%20platform%20used%20by%20students%20to%20learn%20simple%20programming%20concepts.%0ACompared%20to%20models%20trained%20only%20on%20final%20programs%20or%20synthetically-generated%0Atraces%2C%20we%20find%20that%20models%20trained%20on%20real%20traces%20are%20stronger%20at%20modeling%0Adiverse%20student%20behavior.%20Through%20both%20behavioral%20and%20probing%20analyses%2C%20we%20also%0Afind%20that%20many%20properties%20of%20code%20traces%2C%20such%20as%20goal%20backtracking%20or%20number%0Aof%20comments%2C%20can%20be%20predicted%20from%20learned%20representations%20of%20the%20students%20who%0Awrite%20them.%20Building%20on%20this%20result%2C%20we%20show%20that%20we%20can%20help%20students%20recover%0Afrom%20mistakes%20by%20steering%20code%20generation%20models%20to%20identify%20a%20sequence%20of%0Aedits%20that%20will%20results%20in%20more%20correct%20code%20while%20remaining%20close%20to%20the%0Aoriginal%20student%27s%20style.%20Together%2C%20our%20results%20suggest%20that%20many%20properties%20of%0Acode%20are%20properties%20of%20individual%20students%20and%20that%20training%20on%20edit%20traces%20can%0Alead%20to%20models%20that%20are%20more%20steerable%2C%20more%20predictive%20of%20student%20behavior%0Awhile%20programming%2C%20and%20better%20at%20generating%20programs%20in%20their%20final%20states.%0ACode%20and%20data%20is%20available%20at%20https%3A//github.com/meghabyte/pencilcode-public%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Student%2520Learning%2520with%25203.8%2520Million%2520Program%2520Traces%26entry.906535625%3DAlexis%2520Ross%2520and%2520Megha%2520Srivastava%2520and%2520Jeremiah%2520Blanchard%2520and%2520Jacob%2520Andreas%26entry.1292438233%3D%2520%2520As%2520programmers%2520write%2520code%252C%2520they%2520often%2520edit%2520and%2520retry%2520multiple%2520times%252C%2520creating%250Arich%2520%2522interaction%2520traces%2522%2520that%2520reveal%2520how%2520they%2520approach%2520coding%2520tasks%2520and%250Aprovide%2520clues%2520about%2520their%2520level%2520of%2520skill%2520development.%2520For%2520novice%2520programmers%2520in%250Aparticular%252C%2520these%2520traces%2520reflect%2520the%2520diverse%2520reasoning%2520processes%2520they%2520employ%2520to%250Acode%252C%2520such%2520as%2520exploratory%2520behavior%2520to%2520understand%2520how%2520a%2520programming%2520concept%250Aworks%252C%2520re-strategizing%2520in%2520response%2520to%2520bugs%252C%2520and%2520personalizing%2520stylistic%250Achoices.%2520In%2520this%2520work%252C%2520we%2520explore%2520what%2520can%2520be%2520learned%2520from%2520training%2520language%250Amodels%2520on%2520such%2520reasoning%2520traces%253A%2520not%2520just%2520about%2520code%252C%2520but%2520about%2520coders%252C%2520and%250Aparticularly%2520students%2520learning%2520to%2520program.%2520We%2520introduce%2520a%2520dataset%2520of%2520over%25203.8%250Amillion%2520programming%2520reasoning%2520traces%2520from%2520users%2520of%2520Pencil%2520Code%252C%2520a%2520free%2520online%250Aeducational%2520platform%2520used%2520by%2520students%2520to%2520learn%2520simple%2520programming%2520concepts.%250ACompared%2520to%2520models%2520trained%2520only%2520on%2520final%2520programs%2520or%2520synthetically-generated%250Atraces%252C%2520we%2520find%2520that%2520models%2520trained%2520on%2520real%2520traces%2520are%2520stronger%2520at%2520modeling%250Adiverse%2520student%2520behavior.%2520Through%2520both%2520behavioral%2520and%2520probing%2520analyses%252C%2520we%2520also%250Afind%2520that%2520many%2520properties%2520of%2520code%2520traces%252C%2520such%2520as%2520goal%2520backtracking%2520or%2520number%250Aof%2520comments%252C%2520can%2520be%2520predicted%2520from%2520learned%2520representations%2520of%2520the%2520students%2520who%250Awrite%2520them.%2520Building%2520on%2520this%2520result%252C%2520we%2520show%2520that%2520we%2520can%2520help%2520students%2520recover%250Afrom%2520mistakes%2520by%2520steering%2520code%2520generation%2520models%2520to%2520identify%2520a%2520sequence%2520of%250Aedits%2520that%2520will%2520results%2520in%2520more%2520correct%2520code%2520while%2520remaining%2520close%2520to%2520the%250Aoriginal%2520student%2527s%2520style.%2520Together%252C%2520our%2520results%2520suggest%2520that%2520many%2520properties%2520of%250Acode%2520are%2520properties%2520of%2520individual%2520students%2520and%2520that%2520training%2520on%2520edit%2520traces%2520can%250Alead%2520to%2520models%2520that%2520are%2520more%2520steerable%252C%2520more%2520predictive%2520of%2520student%2520behavior%250Awhile%2520programming%252C%2520and%2520better%2520at%2520generating%2520programs%2520in%2520their%2520final%2520states.%250ACode%2520and%2520data%2520is%2520available%2520at%2520https%253A//github.com/meghabyte/pencilcode-public%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Student%20Learning%20with%203.8%20Million%20Program%20Traces&entry.906535625=Alexis%20Ross%20and%20Megha%20Srivastava%20and%20Jeremiah%20Blanchard%20and%20Jacob%20Andreas&entry.1292438233=%20%20As%20programmers%20write%20code%2C%20they%20often%20edit%20and%20retry%20multiple%20times%2C%20creating%0Arich%20%22interaction%20traces%22%20that%20reveal%20how%20they%20approach%20coding%20tasks%20and%0Aprovide%20clues%20about%20their%20level%20of%20skill%20development.%20For%20novice%20programmers%20in%0Aparticular%2C%20these%20traces%20reflect%20the%20diverse%20reasoning%20processes%20they%20employ%20to%0Acode%2C%20such%20as%20exploratory%20behavior%20to%20understand%20how%20a%20programming%20concept%0Aworks%2C%20re-strategizing%20in%20response%20to%20bugs%2C%20and%20personalizing%20stylistic%0Achoices.%20In%20this%20work%2C%20we%20explore%20what%20can%20be%20learned%20from%20training%20language%0Amodels%20on%20such%20reasoning%20traces%3A%20not%20just%20about%20code%2C%20but%20about%20coders%2C%20and%0Aparticularly%20students%20learning%20to%20program.%20We%20introduce%20a%20dataset%20of%20over%203.8%0Amillion%20programming%20reasoning%20traces%20from%20users%20of%20Pencil%20Code%2C%20a%20free%20online%0Aeducational%20platform%20used%20by%20students%20to%20learn%20simple%20programming%20concepts.%0ACompared%20to%20models%20trained%20only%20on%20final%20programs%20or%20synthetically-generated%0Atraces%2C%20we%20find%20that%20models%20trained%20on%20real%20traces%20are%20stronger%20at%20modeling%0Adiverse%20student%20behavior.%20Through%20both%20behavioral%20and%20probing%20analyses%2C%20we%20also%0Afind%20that%20many%20properties%20of%20code%20traces%2C%20such%20as%20goal%20backtracking%20or%20number%0Aof%20comments%2C%20can%20be%20predicted%20from%20learned%20representations%20of%20the%20students%20who%0Awrite%20them.%20Building%20on%20this%20result%2C%20we%20show%20that%20we%20can%20help%20students%20recover%0Afrom%20mistakes%20by%20steering%20code%20generation%20models%20to%20identify%20a%20sequence%20of%0Aedits%20that%20will%20results%20in%20more%20correct%20code%20while%20remaining%20close%20to%20the%0Aoriginal%20student%27s%20style.%20Together%2C%20our%20results%20suggest%20that%20many%20properties%20of%0Acode%20are%20properties%20of%20individual%20students%20and%20that%20training%20on%20edit%20traces%20can%0Alead%20to%20models%20that%20are%20more%20steerable%2C%20more%20predictive%20of%20student%20behavior%0Awhile%20programming%2C%20and%20better%20at%20generating%20programs%20in%20their%20final%20states.%0ACode%20and%20data%20is%20available%20at%20https%3A//github.com/meghabyte/pencilcode-public%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05056v1&entry.124074799=Read"},
{"title": "ONNX-Net: Towards Universal Representations and Instant Performance\n  Prediction for Neural Architectures", "author": "Shiwen Qin and Alexander Auras and Shay B. Cohen and Elliot J. Crowley and Michael Moeller and Linus Ericsson and Jovita Lukasik", "abstract": "  Neural architecture search (NAS) automates the design process of\nhigh-performing architectures, but remains bottlenecked by expensive\nperformance evaluation. Most existing studies that achieve faster evaluation\nare mostly tied to cell-based search spaces and graph encodings tailored to\nthose individual search spaces, limiting their flexibility and scalability when\napplied to more expressive search spaces. In this work, we aim to close the gap\nof individual search space restrictions and search space dependent network\nrepresentations. We present ONNX-Bench, a benchmark consisting of a collection\nof neural networks in a unified format based on ONNX files. ONNX-Bench includes\nall open-source NAS-bench-based neural networks, resulting in a total size of\nmore than 600k {architecture, accuracy} pairs. This benchmark allows creating a\nshared neural network representation, ONNX-Net, able to represent any neural\narchitecture using natural language descriptions acting as an input to a\nperformance predictor. This text-based encoding can accommodate arbitrary layer\ntypes, operation parameters, and heterogeneous topologies, enabling a single\nsurrogate to generalise across all neural architectures rather than being\nconfined to cell-based search spaces. Experiments show strong zero-shot\nperformance across disparate search spaces using only a small amount of\npretraining samples, enabling the unprecedented ability to evaluate any neural\nnetwork architecture instantly.\n", "link": "http://arxiv.org/abs/2510.04938v1", "date": "2025-10-06", "relevancy": 2.0074, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5119}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ONNX-Net%3A%20Towards%20Universal%20Representations%20and%20Instant%20Performance%0A%20%20Prediction%20for%20Neural%20Architectures&body=Title%3A%20ONNX-Net%3A%20Towards%20Universal%20Representations%20and%20Instant%20Performance%0A%20%20Prediction%20for%20Neural%20Architectures%0AAuthor%3A%20Shiwen%20Qin%20and%20Alexander%20Auras%20and%20Shay%20B.%20Cohen%20and%20Elliot%20J.%20Crowley%20and%20Michael%20Moeller%20and%20Linus%20Ericsson%20and%20Jovita%20Lukasik%0AAbstract%3A%20%20%20Neural%20architecture%20search%20%28NAS%29%20automates%20the%20design%20process%20of%0Ahigh-performing%20architectures%2C%20but%20remains%20bottlenecked%20by%20expensive%0Aperformance%20evaluation.%20Most%20existing%20studies%20that%20achieve%20faster%20evaluation%0Aare%20mostly%20tied%20to%20cell-based%20search%20spaces%20and%20graph%20encodings%20tailored%20to%0Athose%20individual%20search%20spaces%2C%20limiting%20their%20flexibility%20and%20scalability%20when%0Aapplied%20to%20more%20expressive%20search%20spaces.%20In%20this%20work%2C%20we%20aim%20to%20close%20the%20gap%0Aof%20individual%20search%20space%20restrictions%20and%20search%20space%20dependent%20network%0Arepresentations.%20We%20present%20ONNX-Bench%2C%20a%20benchmark%20consisting%20of%20a%20collection%0Aof%20neural%20networks%20in%20a%20unified%20format%20based%20on%20ONNX%20files.%20ONNX-Bench%20includes%0Aall%20open-source%20NAS-bench-based%20neural%20networks%2C%20resulting%20in%20a%20total%20size%20of%0Amore%20than%20600k%20%7Barchitecture%2C%20accuracy%7D%20pairs.%20This%20benchmark%20allows%20creating%20a%0Ashared%20neural%20network%20representation%2C%20ONNX-Net%2C%20able%20to%20represent%20any%20neural%0Aarchitecture%20using%20natural%20language%20descriptions%20acting%20as%20an%20input%20to%20a%0Aperformance%20predictor.%20This%20text-based%20encoding%20can%20accommodate%20arbitrary%20layer%0Atypes%2C%20operation%20parameters%2C%20and%20heterogeneous%20topologies%2C%20enabling%20a%20single%0Asurrogate%20to%20generalise%20across%20all%20neural%20architectures%20rather%20than%20being%0Aconfined%20to%20cell-based%20search%20spaces.%20Experiments%20show%20strong%20zero-shot%0Aperformance%20across%20disparate%20search%20spaces%20using%20only%20a%20small%20amount%20of%0Apretraining%20samples%2C%20enabling%20the%20unprecedented%20ability%20to%20evaluate%20any%20neural%0Anetwork%20architecture%20instantly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DONNX-Net%253A%2520Towards%2520Universal%2520Representations%2520and%2520Instant%2520Performance%250A%2520%2520Prediction%2520for%2520Neural%2520Architectures%26entry.906535625%3DShiwen%2520Qin%2520and%2520Alexander%2520Auras%2520and%2520Shay%2520B.%2520Cohen%2520and%2520Elliot%2520J.%2520Crowley%2520and%2520Michael%2520Moeller%2520and%2520Linus%2520Ericsson%2520and%2520Jovita%2520Lukasik%26entry.1292438233%3D%2520%2520Neural%2520architecture%2520search%2520%2528NAS%2529%2520automates%2520the%2520design%2520process%2520of%250Ahigh-performing%2520architectures%252C%2520but%2520remains%2520bottlenecked%2520by%2520expensive%250Aperformance%2520evaluation.%2520Most%2520existing%2520studies%2520that%2520achieve%2520faster%2520evaluation%250Aare%2520mostly%2520tied%2520to%2520cell-based%2520search%2520spaces%2520and%2520graph%2520encodings%2520tailored%2520to%250Athose%2520individual%2520search%2520spaces%252C%2520limiting%2520their%2520flexibility%2520and%2520scalability%2520when%250Aapplied%2520to%2520more%2520expressive%2520search%2520spaces.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520close%2520the%2520gap%250Aof%2520individual%2520search%2520space%2520restrictions%2520and%2520search%2520space%2520dependent%2520network%250Arepresentations.%2520We%2520present%2520ONNX-Bench%252C%2520a%2520benchmark%2520consisting%2520of%2520a%2520collection%250Aof%2520neural%2520networks%2520in%2520a%2520unified%2520format%2520based%2520on%2520ONNX%2520files.%2520ONNX-Bench%2520includes%250Aall%2520open-source%2520NAS-bench-based%2520neural%2520networks%252C%2520resulting%2520in%2520a%2520total%2520size%2520of%250Amore%2520than%2520600k%2520%257Barchitecture%252C%2520accuracy%257D%2520pairs.%2520This%2520benchmark%2520allows%2520creating%2520a%250Ashared%2520neural%2520network%2520representation%252C%2520ONNX-Net%252C%2520able%2520to%2520represent%2520any%2520neural%250Aarchitecture%2520using%2520natural%2520language%2520descriptions%2520acting%2520as%2520an%2520input%2520to%2520a%250Aperformance%2520predictor.%2520This%2520text-based%2520encoding%2520can%2520accommodate%2520arbitrary%2520layer%250Atypes%252C%2520operation%2520parameters%252C%2520and%2520heterogeneous%2520topologies%252C%2520enabling%2520a%2520single%250Asurrogate%2520to%2520generalise%2520across%2520all%2520neural%2520architectures%2520rather%2520than%2520being%250Aconfined%2520to%2520cell-based%2520search%2520spaces.%2520Experiments%2520show%2520strong%2520zero-shot%250Aperformance%2520across%2520disparate%2520search%2520spaces%2520using%2520only%2520a%2520small%2520amount%2520of%250Apretraining%2520samples%252C%2520enabling%2520the%2520unprecedented%2520ability%2520to%2520evaluate%2520any%2520neural%250Anetwork%2520architecture%2520instantly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ONNX-Net%3A%20Towards%20Universal%20Representations%20and%20Instant%20Performance%0A%20%20Prediction%20for%20Neural%20Architectures&entry.906535625=Shiwen%20Qin%20and%20Alexander%20Auras%20and%20Shay%20B.%20Cohen%20and%20Elliot%20J.%20Crowley%20and%20Michael%20Moeller%20and%20Linus%20Ericsson%20and%20Jovita%20Lukasik&entry.1292438233=%20%20Neural%20architecture%20search%20%28NAS%29%20automates%20the%20design%20process%20of%0Ahigh-performing%20architectures%2C%20but%20remains%20bottlenecked%20by%20expensive%0Aperformance%20evaluation.%20Most%20existing%20studies%20that%20achieve%20faster%20evaluation%0Aare%20mostly%20tied%20to%20cell-based%20search%20spaces%20and%20graph%20encodings%20tailored%20to%0Athose%20individual%20search%20spaces%2C%20limiting%20their%20flexibility%20and%20scalability%20when%0Aapplied%20to%20more%20expressive%20search%20spaces.%20In%20this%20work%2C%20we%20aim%20to%20close%20the%20gap%0Aof%20individual%20search%20space%20restrictions%20and%20search%20space%20dependent%20network%0Arepresentations.%20We%20present%20ONNX-Bench%2C%20a%20benchmark%20consisting%20of%20a%20collection%0Aof%20neural%20networks%20in%20a%20unified%20format%20based%20on%20ONNX%20files.%20ONNX-Bench%20includes%0Aall%20open-source%20NAS-bench-based%20neural%20networks%2C%20resulting%20in%20a%20total%20size%20of%0Amore%20than%20600k%20%7Barchitecture%2C%20accuracy%7D%20pairs.%20This%20benchmark%20allows%20creating%20a%0Ashared%20neural%20network%20representation%2C%20ONNX-Net%2C%20able%20to%20represent%20any%20neural%0Aarchitecture%20using%20natural%20language%20descriptions%20acting%20as%20an%20input%20to%20a%0Aperformance%20predictor.%20This%20text-based%20encoding%20can%20accommodate%20arbitrary%20layer%0Atypes%2C%20operation%20parameters%2C%20and%20heterogeneous%20topologies%2C%20enabling%20a%20single%0Asurrogate%20to%20generalise%20across%20all%20neural%20architectures%20rather%20than%20being%0Aconfined%20to%20cell-based%20search%20spaces.%20Experiments%20show%20strong%20zero-shot%0Aperformance%20across%20disparate%20search%20spaces%20using%20only%20a%20small%20amount%20of%0Apretraining%20samples%2C%20enabling%20the%20unprecedented%20ability%20to%20evaluate%20any%20neural%0Anetwork%20architecture%20instantly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04938v1&entry.124074799=Read"},
{"title": "Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for\n  Motorbike Detection in Kigali Autonomous Driving Context", "author": "Ngeyen Yinkfu and Sunday Nwovu and Jonathan Kayizzi and Angelique Uwamahoro", "abstract": "  In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,\noften navigating unpredictably and disregarding traffic rules, posing\nsignificant challenges for autonomous driving systems. This study compares four\nobject detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for\nmotorbike detection using a custom dataset of 198 images collected in Kigali.\nImplemented in PyTorch with transfer learning, the models were evaluated for\naccuracy, localization, and inference speed to assess their suitability for\nreal-time navigation in resource-constrained settings. We identify\nimplementation challenges, including dataset limitations and model\ncomplexities, and recommend simplified architectures for future work to enhance\naccessibility for autonomous systems in developing countries like Rwanda.\n", "link": "http://arxiv.org/abs/2510.04912v1", "date": "2025-10-06", "relevancy": 2.0065, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5034}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5028}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Analysis%20of%20YOLOv5%2C%20Faster%20R-CNN%2C%20SSD%2C%20and%20RetinaNet%20for%0A%20%20Motorbike%20Detection%20in%20Kigali%20Autonomous%20Driving%20Context&body=Title%3A%20Comparative%20Analysis%20of%20YOLOv5%2C%20Faster%20R-CNN%2C%20SSD%2C%20and%20RetinaNet%20for%0A%20%20Motorbike%20Detection%20in%20Kigali%20Autonomous%20Driving%20Context%0AAuthor%3A%20Ngeyen%20Yinkfu%20and%20Sunday%20Nwovu%20and%20Jonathan%20Kayizzi%20and%20Angelique%20Uwamahoro%0AAbstract%3A%20%20%20In%20Kigali%2C%20Rwanda%2C%20motorcycle%20taxis%20are%20a%20primary%20mode%20of%20transportation%2C%0Aoften%20navigating%20unpredictably%20and%20disregarding%20traffic%20rules%2C%20posing%0Asignificant%20challenges%20for%20autonomous%20driving%20systems.%20This%20study%20compares%20four%0Aobject%20detection%20models--YOLOv5%2C%20Faster%20R-CNN%2C%20SSD%2C%20and%20RetinaNet--for%0Amotorbike%20detection%20using%20a%20custom%20dataset%20of%20198%20images%20collected%20in%20Kigali.%0AImplemented%20in%20PyTorch%20with%20transfer%20learning%2C%20the%20models%20were%20evaluated%20for%0Aaccuracy%2C%20localization%2C%20and%20inference%20speed%20to%20assess%20their%20suitability%20for%0Areal-time%20navigation%20in%20resource-constrained%20settings.%20We%20identify%0Aimplementation%20challenges%2C%20including%20dataset%20limitations%20and%20model%0Acomplexities%2C%20and%20recommend%20simplified%20architectures%20for%20future%20work%20to%20enhance%0Aaccessibility%20for%20autonomous%20systems%20in%20developing%20countries%20like%20Rwanda.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Analysis%2520of%2520YOLOv5%252C%2520Faster%2520R-CNN%252C%2520SSD%252C%2520and%2520RetinaNet%2520for%250A%2520%2520Motorbike%2520Detection%2520in%2520Kigali%2520Autonomous%2520Driving%2520Context%26entry.906535625%3DNgeyen%2520Yinkfu%2520and%2520Sunday%2520Nwovu%2520and%2520Jonathan%2520Kayizzi%2520and%2520Angelique%2520Uwamahoro%26entry.1292438233%3D%2520%2520In%2520Kigali%252C%2520Rwanda%252C%2520motorcycle%2520taxis%2520are%2520a%2520primary%2520mode%2520of%2520transportation%252C%250Aoften%2520navigating%2520unpredictably%2520and%2520disregarding%2520traffic%2520rules%252C%2520posing%250Asignificant%2520challenges%2520for%2520autonomous%2520driving%2520systems.%2520This%2520study%2520compares%2520four%250Aobject%2520detection%2520models--YOLOv5%252C%2520Faster%2520R-CNN%252C%2520SSD%252C%2520and%2520RetinaNet--for%250Amotorbike%2520detection%2520using%2520a%2520custom%2520dataset%2520of%2520198%2520images%2520collected%2520in%2520Kigali.%250AImplemented%2520in%2520PyTorch%2520with%2520transfer%2520learning%252C%2520the%2520models%2520were%2520evaluated%2520for%250Aaccuracy%252C%2520localization%252C%2520and%2520inference%2520speed%2520to%2520assess%2520their%2520suitability%2520for%250Areal-time%2520navigation%2520in%2520resource-constrained%2520settings.%2520We%2520identify%250Aimplementation%2520challenges%252C%2520including%2520dataset%2520limitations%2520and%2520model%250Acomplexities%252C%2520and%2520recommend%2520simplified%2520architectures%2520for%2520future%2520work%2520to%2520enhance%250Aaccessibility%2520for%2520autonomous%2520systems%2520in%2520developing%2520countries%2520like%2520Rwanda.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Analysis%20of%20YOLOv5%2C%20Faster%20R-CNN%2C%20SSD%2C%20and%20RetinaNet%20for%0A%20%20Motorbike%20Detection%20in%20Kigali%20Autonomous%20Driving%20Context&entry.906535625=Ngeyen%20Yinkfu%20and%20Sunday%20Nwovu%20and%20Jonathan%20Kayizzi%20and%20Angelique%20Uwamahoro&entry.1292438233=%20%20In%20Kigali%2C%20Rwanda%2C%20motorcycle%20taxis%20are%20a%20primary%20mode%20of%20transportation%2C%0Aoften%20navigating%20unpredictably%20and%20disregarding%20traffic%20rules%2C%20posing%0Asignificant%20challenges%20for%20autonomous%20driving%20systems.%20This%20study%20compares%20four%0Aobject%20detection%20models--YOLOv5%2C%20Faster%20R-CNN%2C%20SSD%2C%20and%20RetinaNet--for%0Amotorbike%20detection%20using%20a%20custom%20dataset%20of%20198%20images%20collected%20in%20Kigali.%0AImplemented%20in%20PyTorch%20with%20transfer%20learning%2C%20the%20models%20were%20evaluated%20for%0Aaccuracy%2C%20localization%2C%20and%20inference%20speed%20to%20assess%20their%20suitability%20for%0Areal-time%20navigation%20in%20resource-constrained%20settings.%20We%20identify%0Aimplementation%20challenges%2C%20including%20dataset%20limitations%20and%20model%0Acomplexities%2C%20and%20recommend%20simplified%20architectures%20for%20future%20work%20to%20enhance%0Aaccessibility%20for%20autonomous%20systems%20in%20developing%20countries%20like%20Rwanda.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04912v1&entry.124074799=Read"},
{"title": "Physics-informed Value Learner for Offline Goal-Conditioned\n  Reinforcement Learning", "author": "Vittorio Giammarino and Ruiqi Ni and Ahmed H. Qureshi", "abstract": "  Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise\nfor domains such as autonomous navigation and locomotion, where collecting\ninteractive data is costly and unsafe. However, it remains challenging in\npractice due to the need to learn from datasets with limited coverage of the\nstate-action space and to generalize across long-horizon tasks. To improve on\nthese challenges, we propose a \\emph{Physics-informed (Pi)} regularized loss\nfor value learning, derived from the Eikonal Partial Differential Equation\n(PDE) and which induces a geometric inductive bias in the learned value\nfunction. Unlike generic gradient penalties that are primarily used to\nstabilize training, our formulation is grounded in continuous-time optimal\ncontrol and encourages value functions to align with cost-to-go structures. The\nproposed regularizer is broadly compatible with temporal-difference-based value\nlearning and can be integrated into existing Offline GCRL algorithms. When\ncombined with Hierarchical Implicit Q-Learning (HIQL), the resulting method,\nEikonal-regularized HIQL (Eik-HIQL), yields significant improvements in both\nperformance and generalization, with pronounced gains in stitching regimes and\nlarge-scale navigation tasks.\n", "link": "http://arxiv.org/abs/2509.06782v2", "date": "2025-10-06", "relevancy": 2.0016, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5343}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4942}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-informed%20Value%20Learner%20for%20Offline%20Goal-Conditioned%0A%20%20Reinforcement%20Learning&body=Title%3A%20Physics-informed%20Value%20Learner%20for%20Offline%20Goal-Conditioned%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Vittorio%20Giammarino%20and%20Ruiqi%20Ni%20and%20Ahmed%20H.%20Qureshi%0AAbstract%3A%20%20%20Offline%20Goal-Conditioned%20Reinforcement%20Learning%20%28GCRL%29%20holds%20great%20promise%0Afor%20domains%20such%20as%20autonomous%20navigation%20and%20locomotion%2C%20where%20collecting%0Ainteractive%20data%20is%20costly%20and%20unsafe.%20However%2C%20it%20remains%20challenging%20in%0Apractice%20due%20to%20the%20need%20to%20learn%20from%20datasets%20with%20limited%20coverage%20of%20the%0Astate-action%20space%20and%20to%20generalize%20across%20long-horizon%20tasks.%20To%20improve%20on%0Athese%20challenges%2C%20we%20propose%20a%20%5Cemph%7BPhysics-informed%20%28Pi%29%7D%20regularized%20loss%0Afor%20value%20learning%2C%20derived%20from%20the%20Eikonal%20Partial%20Differential%20Equation%0A%28PDE%29%20and%20which%20induces%20a%20geometric%20inductive%20bias%20in%20the%20learned%20value%0Afunction.%20Unlike%20generic%20gradient%20penalties%20that%20are%20primarily%20used%20to%0Astabilize%20training%2C%20our%20formulation%20is%20grounded%20in%20continuous-time%20optimal%0Acontrol%20and%20encourages%20value%20functions%20to%20align%20with%20cost-to-go%20structures.%20The%0Aproposed%20regularizer%20is%20broadly%20compatible%20with%20temporal-difference-based%20value%0Alearning%20and%20can%20be%20integrated%20into%20existing%20Offline%20GCRL%20algorithms.%20When%0Acombined%20with%20Hierarchical%20Implicit%20Q-Learning%20%28HIQL%29%2C%20the%20resulting%20method%2C%0AEikonal-regularized%20HIQL%20%28Eik-HIQL%29%2C%20yields%20significant%20improvements%20in%20both%0Aperformance%20and%20generalization%2C%20with%20pronounced%20gains%20in%20stitching%20regimes%20and%0Alarge-scale%20navigation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06782v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-informed%2520Value%2520Learner%2520for%2520Offline%2520Goal-Conditioned%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DVittorio%2520Giammarino%2520and%2520Ruiqi%2520Ni%2520and%2520Ahmed%2520H.%2520Qureshi%26entry.1292438233%3D%2520%2520Offline%2520Goal-Conditioned%2520Reinforcement%2520Learning%2520%2528GCRL%2529%2520holds%2520great%2520promise%250Afor%2520domains%2520such%2520as%2520autonomous%2520navigation%2520and%2520locomotion%252C%2520where%2520collecting%250Ainteractive%2520data%2520is%2520costly%2520and%2520unsafe.%2520However%252C%2520it%2520remains%2520challenging%2520in%250Apractice%2520due%2520to%2520the%2520need%2520to%2520learn%2520from%2520datasets%2520with%2520limited%2520coverage%2520of%2520the%250Astate-action%2520space%2520and%2520to%2520generalize%2520across%2520long-horizon%2520tasks.%2520To%2520improve%2520on%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520%255Cemph%257BPhysics-informed%2520%2528Pi%2529%257D%2520regularized%2520loss%250Afor%2520value%2520learning%252C%2520derived%2520from%2520the%2520Eikonal%2520Partial%2520Differential%2520Equation%250A%2528PDE%2529%2520and%2520which%2520induces%2520a%2520geometric%2520inductive%2520bias%2520in%2520the%2520learned%2520value%250Afunction.%2520Unlike%2520generic%2520gradient%2520penalties%2520that%2520are%2520primarily%2520used%2520to%250Astabilize%2520training%252C%2520our%2520formulation%2520is%2520grounded%2520in%2520continuous-time%2520optimal%250Acontrol%2520and%2520encourages%2520value%2520functions%2520to%2520align%2520with%2520cost-to-go%2520structures.%2520The%250Aproposed%2520regularizer%2520is%2520broadly%2520compatible%2520with%2520temporal-difference-based%2520value%250Alearning%2520and%2520can%2520be%2520integrated%2520into%2520existing%2520Offline%2520GCRL%2520algorithms.%2520When%250Acombined%2520with%2520Hierarchical%2520Implicit%2520Q-Learning%2520%2528HIQL%2529%252C%2520the%2520resulting%2520method%252C%250AEikonal-regularized%2520HIQL%2520%2528Eik-HIQL%2529%252C%2520yields%2520significant%2520improvements%2520in%2520both%250Aperformance%2520and%2520generalization%252C%2520with%2520pronounced%2520gains%2520in%2520stitching%2520regimes%2520and%250Alarge-scale%2520navigation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06782v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-informed%20Value%20Learner%20for%20Offline%20Goal-Conditioned%0A%20%20Reinforcement%20Learning&entry.906535625=Vittorio%20Giammarino%20and%20Ruiqi%20Ni%20and%20Ahmed%20H.%20Qureshi&entry.1292438233=%20%20Offline%20Goal-Conditioned%20Reinforcement%20Learning%20%28GCRL%29%20holds%20great%20promise%0Afor%20domains%20such%20as%20autonomous%20navigation%20and%20locomotion%2C%20where%20collecting%0Ainteractive%20data%20is%20costly%20and%20unsafe.%20However%2C%20it%20remains%20challenging%20in%0Apractice%20due%20to%20the%20need%20to%20learn%20from%20datasets%20with%20limited%20coverage%20of%20the%0Astate-action%20space%20and%20to%20generalize%20across%20long-horizon%20tasks.%20To%20improve%20on%0Athese%20challenges%2C%20we%20propose%20a%20%5Cemph%7BPhysics-informed%20%28Pi%29%7D%20regularized%20loss%0Afor%20value%20learning%2C%20derived%20from%20the%20Eikonal%20Partial%20Differential%20Equation%0A%28PDE%29%20and%20which%20induces%20a%20geometric%20inductive%20bias%20in%20the%20learned%20value%0Afunction.%20Unlike%20generic%20gradient%20penalties%20that%20are%20primarily%20used%20to%0Astabilize%20training%2C%20our%20formulation%20is%20grounded%20in%20continuous-time%20optimal%0Acontrol%20and%20encourages%20value%20functions%20to%20align%20with%20cost-to-go%20structures.%20The%0Aproposed%20regularizer%20is%20broadly%20compatible%20with%20temporal-difference-based%20value%0Alearning%20and%20can%20be%20integrated%20into%20existing%20Offline%20GCRL%20algorithms.%20When%0Acombined%20with%20Hierarchical%20Implicit%20Q-Learning%20%28HIQL%29%2C%20the%20resulting%20method%2C%0AEikonal-regularized%20HIQL%20%28Eik-HIQL%29%2C%20yields%20significant%20improvements%20in%20both%0Aperformance%20and%20generalization%2C%20with%20pronounced%20gains%20in%20stitching%20regimes%20and%0Alarge-scale%20navigation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06782v2&entry.124074799=Read"},
{"title": "Revealing Interconnections between Diseases: from Statistical Methods to\n  Large Language Models", "author": "Alina Ermilova and Dmitrii Kornilov and Sofia Samoilova and Ekaterina Laptenkova and Anastasia Kolesnikova and Ekaterina Podplutova and Senotrusova Sofya and Maksim G. Sharaev", "abstract": "  Identifying disease interconnections through manual analysis of large-scale\nclinical data is labor-intensive, subjective, and prone to expert disagreement.\nWhile machine learning (ML) shows promise, three critical challenges remain:\n(1) selecting optimal methods from the vast ML landscape, (2) determining\nwhether real-world clinical data (e.g., electronic health records, EHRs) or\nstructured disease descriptions yield more reliable insights, (3) the lack of\n\"ground truth,\" as some disease interconnections remain unexplored in medicine.\nLarge language models (LLMs) demonstrate broad utility, yet they often lack\nspecialized medical knowledge. To address these gaps, we conduct a systematic\nevaluation of seven approaches for uncovering disease relationships based on\ntwo data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the\nfull set of ICD-10 codes, both with and without textual descriptions. Our\nframework integrates the following: (i) a statistical co-occurrence analysis\nand a masked language modeling (MLM) approach using real clinical data; (ii)\ndomain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a\ngeneral-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,\nDeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained\ninterconnection matrices shows that the LLM-based approach produces\ninterconnections with the lowest diversity of ICD code connections to different\ndiseases compared to other methods, including text-based and domain-based\napproaches. This suggests an important implication: LLMs have limited potential\nfor discovering new interconnections. In the absence of ground truth databases\nfor medical interconnections between ICD codes, our results constitute a\nvaluable medical disease ontology that can serve as a foundational resource for\nfuture clinical research and artificial intelligence applications in\nhealthcare.\n", "link": "http://arxiv.org/abs/2510.04888v1", "date": "2025-10-06", "relevancy": 2.001, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revealing%20Interconnections%20between%20Diseases%3A%20from%20Statistical%20Methods%20to%0A%20%20Large%20Language%20Models&body=Title%3A%20Revealing%20Interconnections%20between%20Diseases%3A%20from%20Statistical%20Methods%20to%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Alina%20Ermilova%20and%20Dmitrii%20Kornilov%20and%20Sofia%20Samoilova%20and%20Ekaterina%20Laptenkova%20and%20Anastasia%20Kolesnikova%20and%20Ekaterina%20Podplutova%20and%20Senotrusova%20Sofya%20and%20Maksim%20G.%20Sharaev%0AAbstract%3A%20%20%20Identifying%20disease%20interconnections%20through%20manual%20analysis%20of%20large-scale%0Aclinical%20data%20is%20labor-intensive%2C%20subjective%2C%20and%20prone%20to%20expert%20disagreement.%0AWhile%20machine%20learning%20%28ML%29%20shows%20promise%2C%20three%20critical%20challenges%20remain%3A%0A%281%29%20selecting%20optimal%20methods%20from%20the%20vast%20ML%20landscape%2C%20%282%29%20determining%0Awhether%20real-world%20clinical%20data%20%28e.g.%2C%20electronic%20health%20records%2C%20EHRs%29%20or%0Astructured%20disease%20descriptions%20yield%20more%20reliable%20insights%2C%20%283%29%20the%20lack%20of%0A%22ground%20truth%2C%22%20as%20some%20disease%20interconnections%20remain%20unexplored%20in%20medicine.%0ALarge%20language%20models%20%28LLMs%29%20demonstrate%20broad%20utility%2C%20yet%20they%20often%20lack%0Aspecialized%20medical%20knowledge.%20To%20address%20these%20gaps%2C%20we%20conduct%20a%20systematic%0Aevaluation%20of%20seven%20approaches%20for%20uncovering%20disease%20relationships%20based%20on%0Atwo%20data%20sources%3A%20%28i%29%20sequences%20of%20ICD-10%20codes%20from%20MIMIC-IV%20EHRs%20and%20%28ii%29%20the%0Afull%20set%20of%20ICD-10%20codes%2C%20both%20with%20and%20without%20textual%20descriptions.%20Our%0Aframework%20integrates%20the%20following%3A%20%28i%29%20a%20statistical%20co-occurrence%20analysis%0Aand%20a%20masked%20language%20modeling%20%28MLM%29%20approach%20using%20real%20clinical%20data%3B%20%28ii%29%0Adomain-specific%20BERT%20variants%20%28Med-BERT%20and%20BioClinicalBERT%29%3B%20%28iii%29%20a%0Ageneral-purpose%20BERT%20and%20document%20retrieval%3B%20and%20%28iv%29%20four%20LLMs%20%28Mistral%2C%0ADeepSeek%2C%20Qwen%2C%20and%20YandexGPT%29.%20Our%20graph-based%20comparison%20of%20the%20obtained%0Ainterconnection%20matrices%20shows%20that%20the%20LLM-based%20approach%20produces%0Ainterconnections%20with%20the%20lowest%20diversity%20of%20ICD%20code%20connections%20to%20different%0Adiseases%20compared%20to%20other%20methods%2C%20including%20text-based%20and%20domain-based%0Aapproaches.%20This%20suggests%20an%20important%20implication%3A%20LLMs%20have%20limited%20potential%0Afor%20discovering%20new%20interconnections.%20In%20the%20absence%20of%20ground%20truth%20databases%0Afor%20medical%20interconnections%20between%20ICD%20codes%2C%20our%20results%20constitute%20a%0Avaluable%20medical%20disease%20ontology%20that%20can%20serve%20as%20a%20foundational%20resource%20for%0Afuture%20clinical%20research%20and%20artificial%20intelligence%20applications%20in%0Ahealthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevealing%2520Interconnections%2520between%2520Diseases%253A%2520from%2520Statistical%2520Methods%2520to%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DAlina%2520Ermilova%2520and%2520Dmitrii%2520Kornilov%2520and%2520Sofia%2520Samoilova%2520and%2520Ekaterina%2520Laptenkova%2520and%2520Anastasia%2520Kolesnikova%2520and%2520Ekaterina%2520Podplutova%2520and%2520Senotrusova%2520Sofya%2520and%2520Maksim%2520G.%2520Sharaev%26entry.1292438233%3D%2520%2520Identifying%2520disease%2520interconnections%2520through%2520manual%2520analysis%2520of%2520large-scale%250Aclinical%2520data%2520is%2520labor-intensive%252C%2520subjective%252C%2520and%2520prone%2520to%2520expert%2520disagreement.%250AWhile%2520machine%2520learning%2520%2528ML%2529%2520shows%2520promise%252C%2520three%2520critical%2520challenges%2520remain%253A%250A%25281%2529%2520selecting%2520optimal%2520methods%2520from%2520the%2520vast%2520ML%2520landscape%252C%2520%25282%2529%2520determining%250Awhether%2520real-world%2520clinical%2520data%2520%2528e.g.%252C%2520electronic%2520health%2520records%252C%2520EHRs%2529%2520or%250Astructured%2520disease%2520descriptions%2520yield%2520more%2520reliable%2520insights%252C%2520%25283%2529%2520the%2520lack%2520of%250A%2522ground%2520truth%252C%2522%2520as%2520some%2520disease%2520interconnections%2520remain%2520unexplored%2520in%2520medicine.%250ALarge%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520broad%2520utility%252C%2520yet%2520they%2520often%2520lack%250Aspecialized%2520medical%2520knowledge.%2520To%2520address%2520these%2520gaps%252C%2520we%2520conduct%2520a%2520systematic%250Aevaluation%2520of%2520seven%2520approaches%2520for%2520uncovering%2520disease%2520relationships%2520based%2520on%250Atwo%2520data%2520sources%253A%2520%2528i%2529%2520sequences%2520of%2520ICD-10%2520codes%2520from%2520MIMIC-IV%2520EHRs%2520and%2520%2528ii%2529%2520the%250Afull%2520set%2520of%2520ICD-10%2520codes%252C%2520both%2520with%2520and%2520without%2520textual%2520descriptions.%2520Our%250Aframework%2520integrates%2520the%2520following%253A%2520%2528i%2529%2520a%2520statistical%2520co-occurrence%2520analysis%250Aand%2520a%2520masked%2520language%2520modeling%2520%2528MLM%2529%2520approach%2520using%2520real%2520clinical%2520data%253B%2520%2528ii%2529%250Adomain-specific%2520BERT%2520variants%2520%2528Med-BERT%2520and%2520BioClinicalBERT%2529%253B%2520%2528iii%2529%2520a%250Ageneral-purpose%2520BERT%2520and%2520document%2520retrieval%253B%2520and%2520%2528iv%2529%2520four%2520LLMs%2520%2528Mistral%252C%250ADeepSeek%252C%2520Qwen%252C%2520and%2520YandexGPT%2529.%2520Our%2520graph-based%2520comparison%2520of%2520the%2520obtained%250Ainterconnection%2520matrices%2520shows%2520that%2520the%2520LLM-based%2520approach%2520produces%250Ainterconnections%2520with%2520the%2520lowest%2520diversity%2520of%2520ICD%2520code%2520connections%2520to%2520different%250Adiseases%2520compared%2520to%2520other%2520methods%252C%2520including%2520text-based%2520and%2520domain-based%250Aapproaches.%2520This%2520suggests%2520an%2520important%2520implication%253A%2520LLMs%2520have%2520limited%2520potential%250Afor%2520discovering%2520new%2520interconnections.%2520In%2520the%2520absence%2520of%2520ground%2520truth%2520databases%250Afor%2520medical%2520interconnections%2520between%2520ICD%2520codes%252C%2520our%2520results%2520constitute%2520a%250Avaluable%2520medical%2520disease%2520ontology%2520that%2520can%2520serve%2520as%2520a%2520foundational%2520resource%2520for%250Afuture%2520clinical%2520research%2520and%2520artificial%2520intelligence%2520applications%2520in%250Ahealthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20Interconnections%20between%20Diseases%3A%20from%20Statistical%20Methods%20to%0A%20%20Large%20Language%20Models&entry.906535625=Alina%20Ermilova%20and%20Dmitrii%20Kornilov%20and%20Sofia%20Samoilova%20and%20Ekaterina%20Laptenkova%20and%20Anastasia%20Kolesnikova%20and%20Ekaterina%20Podplutova%20and%20Senotrusova%20Sofya%20and%20Maksim%20G.%20Sharaev&entry.1292438233=%20%20Identifying%20disease%20interconnections%20through%20manual%20analysis%20of%20large-scale%0Aclinical%20data%20is%20labor-intensive%2C%20subjective%2C%20and%20prone%20to%20expert%20disagreement.%0AWhile%20machine%20learning%20%28ML%29%20shows%20promise%2C%20three%20critical%20challenges%20remain%3A%0A%281%29%20selecting%20optimal%20methods%20from%20the%20vast%20ML%20landscape%2C%20%282%29%20determining%0Awhether%20real-world%20clinical%20data%20%28e.g.%2C%20electronic%20health%20records%2C%20EHRs%29%20or%0Astructured%20disease%20descriptions%20yield%20more%20reliable%20insights%2C%20%283%29%20the%20lack%20of%0A%22ground%20truth%2C%22%20as%20some%20disease%20interconnections%20remain%20unexplored%20in%20medicine.%0ALarge%20language%20models%20%28LLMs%29%20demonstrate%20broad%20utility%2C%20yet%20they%20often%20lack%0Aspecialized%20medical%20knowledge.%20To%20address%20these%20gaps%2C%20we%20conduct%20a%20systematic%0Aevaluation%20of%20seven%20approaches%20for%20uncovering%20disease%20relationships%20based%20on%0Atwo%20data%20sources%3A%20%28i%29%20sequences%20of%20ICD-10%20codes%20from%20MIMIC-IV%20EHRs%20and%20%28ii%29%20the%0Afull%20set%20of%20ICD-10%20codes%2C%20both%20with%20and%20without%20textual%20descriptions.%20Our%0Aframework%20integrates%20the%20following%3A%20%28i%29%20a%20statistical%20co-occurrence%20analysis%0Aand%20a%20masked%20language%20modeling%20%28MLM%29%20approach%20using%20real%20clinical%20data%3B%20%28ii%29%0Adomain-specific%20BERT%20variants%20%28Med-BERT%20and%20BioClinicalBERT%29%3B%20%28iii%29%20a%0Ageneral-purpose%20BERT%20and%20document%20retrieval%3B%20and%20%28iv%29%20four%20LLMs%20%28Mistral%2C%0ADeepSeek%2C%20Qwen%2C%20and%20YandexGPT%29.%20Our%20graph-based%20comparison%20of%20the%20obtained%0Ainterconnection%20matrices%20shows%20that%20the%20LLM-based%20approach%20produces%0Ainterconnections%20with%20the%20lowest%20diversity%20of%20ICD%20code%20connections%20to%20different%0Adiseases%20compared%20to%20other%20methods%2C%20including%20text-based%20and%20domain-based%0Aapproaches.%20This%20suggests%20an%20important%20implication%3A%20LLMs%20have%20limited%20potential%0Afor%20discovering%20new%20interconnections.%20In%20the%20absence%20of%20ground%20truth%20databases%0Afor%20medical%20interconnections%20between%20ICD%20codes%2C%20our%20results%20constitute%20a%0Avaluable%20medical%20disease%20ontology%20that%20can%20serve%20as%20a%20foundational%20resource%20for%0Afuture%20clinical%20research%20and%20artificial%20intelligence%20applications%20in%0Ahealthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04888v1&entry.124074799=Read"},
{"title": "REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung\n  Disease Diagnosis", "author": "Alec K. Peltekian and Halil Ertugrul Aktas and Gorkem Durak and Kevin Grudzinski and Bradford C. Bemiss and Carrie Richardson and Jane E. Dematte and G. R. Scott Budinger and Anthony J. Esposito and Alexander Misharin and Alok Choudhary and Ankit Agrawal and Ulas Bagci", "abstract": "  Mixture-of-Experts (MoE) architectures have significantly contributed to\nscalable machine learning by enabling specialized subnetworks to tackle complex\ntasks efficiently. However, traditional MoE systems lack domain-specific\nconstraints essential for medical imaging, where anatomical structure and\nregional disease heterogeneity strongly influence pathological patterns. Here,\nwe introduce Regional Expert Networks (REN), the first anatomically-informed\nMoE framework tailored specifically for medical image classification. REN\nleverages anatomical priors to train seven specialized experts, each dedicated\nto distinct lung lobes and bilateral lung combinations, enabling precise\nmodeling of region-specific pathological variations. Multi-modal gating\nmechanisms dynamically integrate radiomics biomarkers and deep learning (DL)\nfeatures (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to\ninterstitial lung disease (ILD) classification, REN achieves consistently\nsuperior performance: the radiomics-guided ensemble reached an average AUC of\n0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC\n0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe\nmodels achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)\nand aligning with known disease progression patterns. Through rigorous\npatient-level cross-validation, REN demonstrates strong generalizability and\nclinical interpretability, presenting a scalable, anatomically-guided approach\nreadily extensible to other structured medical imaging applications.\n", "link": "http://arxiv.org/abs/2510.04923v1", "date": "2025-10-06", "relevancy": 1.9981, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5451}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REN%3A%20Anatomically-Informed%20Mixture-of-Experts%20for%20Interstitial%20Lung%0A%20%20Disease%20Diagnosis&body=Title%3A%20REN%3A%20Anatomically-Informed%20Mixture-of-Experts%20for%20Interstitial%20Lung%0A%20%20Disease%20Diagnosis%0AAuthor%3A%20Alec%20K.%20Peltekian%20and%20Halil%20Ertugrul%20Aktas%20and%20Gorkem%20Durak%20and%20Kevin%20Grudzinski%20and%20Bradford%20C.%20Bemiss%20and%20Carrie%20Richardson%20and%20Jane%20E.%20Dematte%20and%20G.%20R.%20Scott%20Budinger%20and%20Anthony%20J.%20Esposito%20and%20Alexander%20Misharin%20and%20Alok%20Choudhary%20and%20Ankit%20Agrawal%20and%20Ulas%20Bagci%0AAbstract%3A%20%20%20Mixture-of-Experts%20%28MoE%29%20architectures%20have%20significantly%20contributed%20to%0Ascalable%20machine%20learning%20by%20enabling%20specialized%20subnetworks%20to%20tackle%20complex%0Atasks%20efficiently.%20However%2C%20traditional%20MoE%20systems%20lack%20domain-specific%0Aconstraints%20essential%20for%20medical%20imaging%2C%20where%20anatomical%20structure%20and%0Aregional%20disease%20heterogeneity%20strongly%20influence%20pathological%20patterns.%20Here%2C%0Awe%20introduce%20Regional%20Expert%20Networks%20%28REN%29%2C%20the%20first%20anatomically-informed%0AMoE%20framework%20tailored%20specifically%20for%20medical%20image%20classification.%20REN%0Aleverages%20anatomical%20priors%20to%20train%20seven%20specialized%20experts%2C%20each%20dedicated%0Ato%20distinct%20lung%20lobes%20and%20bilateral%20lung%20combinations%2C%20enabling%20precise%0Amodeling%20of%20region-specific%20pathological%20variations.%20Multi-modal%20gating%0Amechanisms%20dynamically%20integrate%20radiomics%20biomarkers%20and%20deep%20learning%20%28DL%29%0Afeatures%20%28CNN%2C%20ViT%2C%20Mamba%29%20to%20weight%20expert%20contributions%20optimally.%20Applied%20to%0Ainterstitial%20lung%20disease%20%28ILD%29%20classification%2C%20REN%20achieves%20consistently%0Asuperior%20performance%3A%20the%20radiomics-guided%20ensemble%20reached%20an%20average%20AUC%20of%0A0.8646%20%2B/-%200.0467%2C%20a%20%2B12.5%20percent%20improvement%20over%20the%20SwinUNETR%20baseline%20%28AUC%0A0.7685%2C%20p%20%3D%200.031%29.%20Region-specific%20experts%20further%20revealed%20that%20lower-lobe%0Amodels%20achieved%20AUCs%20of%200.88-0.90%2C%20surpassing%20DL%20counterparts%20%28CNN%3A%200.76-0.79%29%0Aand%20aligning%20with%20known%20disease%20progression%20patterns.%20Through%20rigorous%0Apatient-level%20cross-validation%2C%20REN%20demonstrates%20strong%20generalizability%20and%0Aclinical%20interpretability%2C%20presenting%20a%20scalable%2C%20anatomically-guided%20approach%0Areadily%20extensible%20to%20other%20structured%20medical%20imaging%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREN%253A%2520Anatomically-Informed%2520Mixture-of-Experts%2520for%2520Interstitial%2520Lung%250A%2520%2520Disease%2520Diagnosis%26entry.906535625%3DAlec%2520K.%2520Peltekian%2520and%2520Halil%2520Ertugrul%2520Aktas%2520and%2520Gorkem%2520Durak%2520and%2520Kevin%2520Grudzinski%2520and%2520Bradford%2520C.%2520Bemiss%2520and%2520Carrie%2520Richardson%2520and%2520Jane%2520E.%2520Dematte%2520and%2520G.%2520R.%2520Scott%2520Budinger%2520and%2520Anthony%2520J.%2520Esposito%2520and%2520Alexander%2520Misharin%2520and%2520Alok%2520Choudhary%2520and%2520Ankit%2520Agrawal%2520and%2520Ulas%2520Bagci%26entry.1292438233%3D%2520%2520Mixture-of-Experts%2520%2528MoE%2529%2520architectures%2520have%2520significantly%2520contributed%2520to%250Ascalable%2520machine%2520learning%2520by%2520enabling%2520specialized%2520subnetworks%2520to%2520tackle%2520complex%250Atasks%2520efficiently.%2520However%252C%2520traditional%2520MoE%2520systems%2520lack%2520domain-specific%250Aconstraints%2520essential%2520for%2520medical%2520imaging%252C%2520where%2520anatomical%2520structure%2520and%250Aregional%2520disease%2520heterogeneity%2520strongly%2520influence%2520pathological%2520patterns.%2520Here%252C%250Awe%2520introduce%2520Regional%2520Expert%2520Networks%2520%2528REN%2529%252C%2520the%2520first%2520anatomically-informed%250AMoE%2520framework%2520tailored%2520specifically%2520for%2520medical%2520image%2520classification.%2520REN%250Aleverages%2520anatomical%2520priors%2520to%2520train%2520seven%2520specialized%2520experts%252C%2520each%2520dedicated%250Ato%2520distinct%2520lung%2520lobes%2520and%2520bilateral%2520lung%2520combinations%252C%2520enabling%2520precise%250Amodeling%2520of%2520region-specific%2520pathological%2520variations.%2520Multi-modal%2520gating%250Amechanisms%2520dynamically%2520integrate%2520radiomics%2520biomarkers%2520and%2520deep%2520learning%2520%2528DL%2529%250Afeatures%2520%2528CNN%252C%2520ViT%252C%2520Mamba%2529%2520to%2520weight%2520expert%2520contributions%2520optimally.%2520Applied%2520to%250Ainterstitial%2520lung%2520disease%2520%2528ILD%2529%2520classification%252C%2520REN%2520achieves%2520consistently%250Asuperior%2520performance%253A%2520the%2520radiomics-guided%2520ensemble%2520reached%2520an%2520average%2520AUC%2520of%250A0.8646%2520%252B/-%25200.0467%252C%2520a%2520%252B12.5%2520percent%2520improvement%2520over%2520the%2520SwinUNETR%2520baseline%2520%2528AUC%250A0.7685%252C%2520p%2520%253D%25200.031%2529.%2520Region-specific%2520experts%2520further%2520revealed%2520that%2520lower-lobe%250Amodels%2520achieved%2520AUCs%2520of%25200.88-0.90%252C%2520surpassing%2520DL%2520counterparts%2520%2528CNN%253A%25200.76-0.79%2529%250Aand%2520aligning%2520with%2520known%2520disease%2520progression%2520patterns.%2520Through%2520rigorous%250Apatient-level%2520cross-validation%252C%2520REN%2520demonstrates%2520strong%2520generalizability%2520and%250Aclinical%2520interpretability%252C%2520presenting%2520a%2520scalable%252C%2520anatomically-guided%2520approach%250Areadily%2520extensible%2520to%2520other%2520structured%2520medical%2520imaging%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REN%3A%20Anatomically-Informed%20Mixture-of-Experts%20for%20Interstitial%20Lung%0A%20%20Disease%20Diagnosis&entry.906535625=Alec%20K.%20Peltekian%20and%20Halil%20Ertugrul%20Aktas%20and%20Gorkem%20Durak%20and%20Kevin%20Grudzinski%20and%20Bradford%20C.%20Bemiss%20and%20Carrie%20Richardson%20and%20Jane%20E.%20Dematte%20and%20G.%20R.%20Scott%20Budinger%20and%20Anthony%20J.%20Esposito%20and%20Alexander%20Misharin%20and%20Alok%20Choudhary%20and%20Ankit%20Agrawal%20and%20Ulas%20Bagci&entry.1292438233=%20%20Mixture-of-Experts%20%28MoE%29%20architectures%20have%20significantly%20contributed%20to%0Ascalable%20machine%20learning%20by%20enabling%20specialized%20subnetworks%20to%20tackle%20complex%0Atasks%20efficiently.%20However%2C%20traditional%20MoE%20systems%20lack%20domain-specific%0Aconstraints%20essential%20for%20medical%20imaging%2C%20where%20anatomical%20structure%20and%0Aregional%20disease%20heterogeneity%20strongly%20influence%20pathological%20patterns.%20Here%2C%0Awe%20introduce%20Regional%20Expert%20Networks%20%28REN%29%2C%20the%20first%20anatomically-informed%0AMoE%20framework%20tailored%20specifically%20for%20medical%20image%20classification.%20REN%0Aleverages%20anatomical%20priors%20to%20train%20seven%20specialized%20experts%2C%20each%20dedicated%0Ato%20distinct%20lung%20lobes%20and%20bilateral%20lung%20combinations%2C%20enabling%20precise%0Amodeling%20of%20region-specific%20pathological%20variations.%20Multi-modal%20gating%0Amechanisms%20dynamically%20integrate%20radiomics%20biomarkers%20and%20deep%20learning%20%28DL%29%0Afeatures%20%28CNN%2C%20ViT%2C%20Mamba%29%20to%20weight%20expert%20contributions%20optimally.%20Applied%20to%0Ainterstitial%20lung%20disease%20%28ILD%29%20classification%2C%20REN%20achieves%20consistently%0Asuperior%20performance%3A%20the%20radiomics-guided%20ensemble%20reached%20an%20average%20AUC%20of%0A0.8646%20%2B/-%200.0467%2C%20a%20%2B12.5%20percent%20improvement%20over%20the%20SwinUNETR%20baseline%20%28AUC%0A0.7685%2C%20p%20%3D%200.031%29.%20Region-specific%20experts%20further%20revealed%20that%20lower-lobe%0Amodels%20achieved%20AUCs%20of%200.88-0.90%2C%20surpassing%20DL%20counterparts%20%28CNN%3A%200.76-0.79%29%0Aand%20aligning%20with%20known%20disease%20progression%20patterns.%20Through%20rigorous%0Apatient-level%20cross-validation%2C%20REN%20demonstrates%20strong%20generalizability%20and%0Aclinical%20interpretability%2C%20presenting%20a%20scalable%2C%20anatomically-guided%20approach%0Areadily%20extensible%20to%20other%20structured%20medical%20imaging%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04923v1&entry.124074799=Read"},
{"title": "Rethinking Langevin Thompson Sampling from A Stochastic Approximation\n  Perspective", "author": "Weixin Wang and Haoyang Zheng and Guang Lin and Wei Deng and Pan Xu", "abstract": "  Most existing approximate Thompson Sampling (TS) algorithms for multi-armed\nbandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in\neach round to sample from the posterior, relaxing the need for conjugacy\nassumptions between priors and reward distributions in vanilla TS. However,\nthey often require approximating a different posterior distribution in\ndifferent round of the bandit problem. This requires tricky, round-specific\ntuning of hyperparameters such as dynamic learning rates, causing challenges in\nboth theoretical analysis and practical implementation. To alleviate this\nnon-stationarity, we introduce TS-SA, which incorporates stochastic\napproximation (SA) within the TS framework. In each round, TS-SA constructs a\nposterior approximation only using the most recent reward(s), performs a\nLangevin Monte Carlo (LMC) update, and applies an SA step to average noisy\nproposals over time. This can be interpreted as approximating a stationary\nposterior target throughout the entire algorithm, which further yields a fixed\nstep-size, a unified convergence analysis framework, and improved posterior\nestimates through temporal averaging. We establish near-optimal regret bounds\nfor TS-SA, with a simplified and more intuitive theoretical analysis enabled by\ninterpreting the entire algorithm as a simulation of a stationary SGLD process.\nOur empirical results demonstrate that even a single-step Langevin update with\ncertain warm-up outperforms existing methods substantially on bandit tasks.\n", "link": "http://arxiv.org/abs/2510.05023v1", "date": "2025-10-06", "relevancy": 1.9962, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5235}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.492}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Langevin%20Thompson%20Sampling%20from%20A%20Stochastic%20Approximation%0A%20%20Perspective&body=Title%3A%20Rethinking%20Langevin%20Thompson%20Sampling%20from%20A%20Stochastic%20Approximation%0A%20%20Perspective%0AAuthor%3A%20Weixin%20Wang%20and%20Haoyang%20Zheng%20and%20Guang%20Lin%20and%20Wei%20Deng%20and%20Pan%20Xu%0AAbstract%3A%20%20%20Most%20existing%20approximate%20Thompson%20Sampling%20%28TS%29%20algorithms%20for%20multi-armed%0Abandits%20use%20Stochastic%20Gradient%20Langevin%20Dynamics%20%28SGLD%29%20or%20its%20variants%20in%0Aeach%20round%20to%20sample%20from%20the%20posterior%2C%20relaxing%20the%20need%20for%20conjugacy%0Aassumptions%20between%20priors%20and%20reward%20distributions%20in%20vanilla%20TS.%20However%2C%0Athey%20often%20require%20approximating%20a%20different%20posterior%20distribution%20in%0Adifferent%20round%20of%20the%20bandit%20problem.%20This%20requires%20tricky%2C%20round-specific%0Atuning%20of%20hyperparameters%20such%20as%20dynamic%20learning%20rates%2C%20causing%20challenges%20in%0Aboth%20theoretical%20analysis%20and%20practical%20implementation.%20To%20alleviate%20this%0Anon-stationarity%2C%20we%20introduce%20TS-SA%2C%20which%20incorporates%20stochastic%0Aapproximation%20%28SA%29%20within%20the%20TS%20framework.%20In%20each%20round%2C%20TS-SA%20constructs%20a%0Aposterior%20approximation%20only%20using%20the%20most%20recent%20reward%28s%29%2C%20performs%20a%0ALangevin%20Monte%20Carlo%20%28LMC%29%20update%2C%20and%20applies%20an%20SA%20step%20to%20average%20noisy%0Aproposals%20over%20time.%20This%20can%20be%20interpreted%20as%20approximating%20a%20stationary%0Aposterior%20target%20throughout%20the%20entire%20algorithm%2C%20which%20further%20yields%20a%20fixed%0Astep-size%2C%20a%20unified%20convergence%20analysis%20framework%2C%20and%20improved%20posterior%0Aestimates%20through%20temporal%20averaging.%20We%20establish%20near-optimal%20regret%20bounds%0Afor%20TS-SA%2C%20with%20a%20simplified%20and%20more%20intuitive%20theoretical%20analysis%20enabled%20by%0Ainterpreting%20the%20entire%20algorithm%20as%20a%20simulation%20of%20a%20stationary%20SGLD%20process.%0AOur%20empirical%20results%20demonstrate%20that%20even%20a%20single-step%20Langevin%20update%20with%0Acertain%20warm-up%20outperforms%20existing%20methods%20substantially%20on%20bandit%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Langevin%2520Thompson%2520Sampling%2520from%2520A%2520Stochastic%2520Approximation%250A%2520%2520Perspective%26entry.906535625%3DWeixin%2520Wang%2520and%2520Haoyang%2520Zheng%2520and%2520Guang%2520Lin%2520and%2520Wei%2520Deng%2520and%2520Pan%2520Xu%26entry.1292438233%3D%2520%2520Most%2520existing%2520approximate%2520Thompson%2520Sampling%2520%2528TS%2529%2520algorithms%2520for%2520multi-armed%250Abandits%2520use%2520Stochastic%2520Gradient%2520Langevin%2520Dynamics%2520%2528SGLD%2529%2520or%2520its%2520variants%2520in%250Aeach%2520round%2520to%2520sample%2520from%2520the%2520posterior%252C%2520relaxing%2520the%2520need%2520for%2520conjugacy%250Aassumptions%2520between%2520priors%2520and%2520reward%2520distributions%2520in%2520vanilla%2520TS.%2520However%252C%250Athey%2520often%2520require%2520approximating%2520a%2520different%2520posterior%2520distribution%2520in%250Adifferent%2520round%2520of%2520the%2520bandit%2520problem.%2520This%2520requires%2520tricky%252C%2520round-specific%250Atuning%2520of%2520hyperparameters%2520such%2520as%2520dynamic%2520learning%2520rates%252C%2520causing%2520challenges%2520in%250Aboth%2520theoretical%2520analysis%2520and%2520practical%2520implementation.%2520To%2520alleviate%2520this%250Anon-stationarity%252C%2520we%2520introduce%2520TS-SA%252C%2520which%2520incorporates%2520stochastic%250Aapproximation%2520%2528SA%2529%2520within%2520the%2520TS%2520framework.%2520In%2520each%2520round%252C%2520TS-SA%2520constructs%2520a%250Aposterior%2520approximation%2520only%2520using%2520the%2520most%2520recent%2520reward%2528s%2529%252C%2520performs%2520a%250ALangevin%2520Monte%2520Carlo%2520%2528LMC%2529%2520update%252C%2520and%2520applies%2520an%2520SA%2520step%2520to%2520average%2520noisy%250Aproposals%2520over%2520time.%2520This%2520can%2520be%2520interpreted%2520as%2520approximating%2520a%2520stationary%250Aposterior%2520target%2520throughout%2520the%2520entire%2520algorithm%252C%2520which%2520further%2520yields%2520a%2520fixed%250Astep-size%252C%2520a%2520unified%2520convergence%2520analysis%2520framework%252C%2520and%2520improved%2520posterior%250Aestimates%2520through%2520temporal%2520averaging.%2520We%2520establish%2520near-optimal%2520regret%2520bounds%250Afor%2520TS-SA%252C%2520with%2520a%2520simplified%2520and%2520more%2520intuitive%2520theoretical%2520analysis%2520enabled%2520by%250Ainterpreting%2520the%2520entire%2520algorithm%2520as%2520a%2520simulation%2520of%2520a%2520stationary%2520SGLD%2520process.%250AOur%2520empirical%2520results%2520demonstrate%2520that%2520even%2520a%2520single-step%2520Langevin%2520update%2520with%250Acertain%2520warm-up%2520outperforms%2520existing%2520methods%2520substantially%2520on%2520bandit%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Langevin%20Thompson%20Sampling%20from%20A%20Stochastic%20Approximation%0A%20%20Perspective&entry.906535625=Weixin%20Wang%20and%20Haoyang%20Zheng%20and%20Guang%20Lin%20and%20Wei%20Deng%20and%20Pan%20Xu&entry.1292438233=%20%20Most%20existing%20approximate%20Thompson%20Sampling%20%28TS%29%20algorithms%20for%20multi-armed%0Abandits%20use%20Stochastic%20Gradient%20Langevin%20Dynamics%20%28SGLD%29%20or%20its%20variants%20in%0Aeach%20round%20to%20sample%20from%20the%20posterior%2C%20relaxing%20the%20need%20for%20conjugacy%0Aassumptions%20between%20priors%20and%20reward%20distributions%20in%20vanilla%20TS.%20However%2C%0Athey%20often%20require%20approximating%20a%20different%20posterior%20distribution%20in%0Adifferent%20round%20of%20the%20bandit%20problem.%20This%20requires%20tricky%2C%20round-specific%0Atuning%20of%20hyperparameters%20such%20as%20dynamic%20learning%20rates%2C%20causing%20challenges%20in%0Aboth%20theoretical%20analysis%20and%20practical%20implementation.%20To%20alleviate%20this%0Anon-stationarity%2C%20we%20introduce%20TS-SA%2C%20which%20incorporates%20stochastic%0Aapproximation%20%28SA%29%20within%20the%20TS%20framework.%20In%20each%20round%2C%20TS-SA%20constructs%20a%0Aposterior%20approximation%20only%20using%20the%20most%20recent%20reward%28s%29%2C%20performs%20a%0ALangevin%20Monte%20Carlo%20%28LMC%29%20update%2C%20and%20applies%20an%20SA%20step%20to%20average%20noisy%0Aproposals%20over%20time.%20This%20can%20be%20interpreted%20as%20approximating%20a%20stationary%0Aposterior%20target%20throughout%20the%20entire%20algorithm%2C%20which%20further%20yields%20a%20fixed%0Astep-size%2C%20a%20unified%20convergence%20analysis%20framework%2C%20and%20improved%20posterior%0Aestimates%20through%20temporal%20averaging.%20We%20establish%20near-optimal%20regret%20bounds%0Afor%20TS-SA%2C%20with%20a%20simplified%20and%20more%20intuitive%20theoretical%20analysis%20enabled%20by%0Ainterpreting%20the%20entire%20algorithm%20as%20a%20simulation%20of%20a%20stationary%20SGLD%20process.%0AOur%20empirical%20results%20demonstrate%20that%20even%20a%20single-step%20Langevin%20update%20with%0Acertain%20warm-up%20outperforms%20existing%20methods%20substantially%20on%20bandit%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05023v1&entry.124074799=Read"},
{"title": "Fast constrained sampling in pre-trained diffusion models", "author": "Alexandros Graikos and Nebojsa Jojic and Dimitris Samaras", "abstract": "  Large denoising diffusion models, such as Stable Diffusion, have been trained\non billions of image-caption pairs to perform text-conditioned image\ngeneration. As a byproduct of this training, these models have acquired general\nknowledge about image statistics, which can be useful for other inference\ntasks. However, when confronted with sampling an image under new constraints,\ne.g. generating the missing parts of an image, using large pre-trained\ntext-to-image diffusion models is inefficient and often unreliable. Previous\napproaches either utilized backpropagation through the denoiser network, making\nthem significantly slower and more memory-demanding than simple text-to-image\ngeneration, or only enforced the constraint locally, failing to capture\ncritical long-range correlations in the sampled image. In this work, we propose\nan algorithm that enables fast, high-quality generation under arbitrary\nconstraints. We show that in denoising diffusion models, we can employ an\napproximation to Newton's optimization method that allows us to speed up\ninference and avoid the expensive backpropagation operations. Our approach\nproduces results that rival or surpass the state-of-the-art training-free\ninference methods while requiring a fraction of the time. We demonstrate the\neffectiveness of our algorithm under both linear (inpainting, super-resolution)\nand non-linear (style-guided generation) constraints. An implementation is\nprovided at https://github.com/cvlab-stonybrook/fast-constrained-sampling.\n", "link": "http://arxiv.org/abs/2410.18804v3", "date": "2025-10-06", "relevancy": 1.9944, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6787}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6652}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20constrained%20sampling%20in%20pre-trained%20diffusion%20models&body=Title%3A%20Fast%20constrained%20sampling%20in%20pre-trained%20diffusion%20models%0AAuthor%3A%20Alexandros%20Graikos%20and%20Nebojsa%20Jojic%20and%20Dimitris%20Samaras%0AAbstract%3A%20%20%20Large%20denoising%20diffusion%20models%2C%20such%20as%20Stable%20Diffusion%2C%20have%20been%20trained%0Aon%20billions%20of%20image-caption%20pairs%20to%20perform%20text-conditioned%20image%0Ageneration.%20As%20a%20byproduct%20of%20this%20training%2C%20these%20models%20have%20acquired%20general%0Aknowledge%20about%20image%20statistics%2C%20which%20can%20be%20useful%20for%20other%20inference%0Atasks.%20However%2C%20when%20confronted%20with%20sampling%20an%20image%20under%20new%20constraints%2C%0Ae.g.%20generating%20the%20missing%20parts%20of%20an%20image%2C%20using%20large%20pre-trained%0Atext-to-image%20diffusion%20models%20is%20inefficient%20and%20often%20unreliable.%20Previous%0Aapproaches%20either%20utilized%20backpropagation%20through%20the%20denoiser%20network%2C%20making%0Athem%20significantly%20slower%20and%20more%20memory-demanding%20than%20simple%20text-to-image%0Ageneration%2C%20or%20only%20enforced%20the%20constraint%20locally%2C%20failing%20to%20capture%0Acritical%20long-range%20correlations%20in%20the%20sampled%20image.%20In%20this%20work%2C%20we%20propose%0Aan%20algorithm%20that%20enables%20fast%2C%20high-quality%20generation%20under%20arbitrary%0Aconstraints.%20We%20show%20that%20in%20denoising%20diffusion%20models%2C%20we%20can%20employ%20an%0Aapproximation%20to%20Newton%27s%20optimization%20method%20that%20allows%20us%20to%20speed%20up%0Ainference%20and%20avoid%20the%20expensive%20backpropagation%20operations.%20Our%20approach%0Aproduces%20results%20that%20rival%20or%20surpass%20the%20state-of-the-art%20training-free%0Ainference%20methods%20while%20requiring%20a%20fraction%20of%20the%20time.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20algorithm%20under%20both%20linear%20%28inpainting%2C%20super-resolution%29%0Aand%20non-linear%20%28style-guided%20generation%29%20constraints.%20An%20implementation%20is%0Aprovided%20at%20https%3A//github.com/cvlab-stonybrook/fast-constrained-sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18804v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520constrained%2520sampling%2520in%2520pre-trained%2520diffusion%2520models%26entry.906535625%3DAlexandros%2520Graikos%2520and%2520Nebojsa%2520Jojic%2520and%2520Dimitris%2520Samaras%26entry.1292438233%3D%2520%2520Large%2520denoising%2520diffusion%2520models%252C%2520such%2520as%2520Stable%2520Diffusion%252C%2520have%2520been%2520trained%250Aon%2520billions%2520of%2520image-caption%2520pairs%2520to%2520perform%2520text-conditioned%2520image%250Ageneration.%2520As%2520a%2520byproduct%2520of%2520this%2520training%252C%2520these%2520models%2520have%2520acquired%2520general%250Aknowledge%2520about%2520image%2520statistics%252C%2520which%2520can%2520be%2520useful%2520for%2520other%2520inference%250Atasks.%2520However%252C%2520when%2520confronted%2520with%2520sampling%2520an%2520image%2520under%2520new%2520constraints%252C%250Ae.g.%2520generating%2520the%2520missing%2520parts%2520of%2520an%2520image%252C%2520using%2520large%2520pre-trained%250Atext-to-image%2520diffusion%2520models%2520is%2520inefficient%2520and%2520often%2520unreliable.%2520Previous%250Aapproaches%2520either%2520utilized%2520backpropagation%2520through%2520the%2520denoiser%2520network%252C%2520making%250Athem%2520significantly%2520slower%2520and%2520more%2520memory-demanding%2520than%2520simple%2520text-to-image%250Ageneration%252C%2520or%2520only%2520enforced%2520the%2520constraint%2520locally%252C%2520failing%2520to%2520capture%250Acritical%2520long-range%2520correlations%2520in%2520the%2520sampled%2520image.%2520In%2520this%2520work%252C%2520we%2520propose%250Aan%2520algorithm%2520that%2520enables%2520fast%252C%2520high-quality%2520generation%2520under%2520arbitrary%250Aconstraints.%2520We%2520show%2520that%2520in%2520denoising%2520diffusion%2520models%252C%2520we%2520can%2520employ%2520an%250Aapproximation%2520to%2520Newton%2527s%2520optimization%2520method%2520that%2520allows%2520us%2520to%2520speed%2520up%250Ainference%2520and%2520avoid%2520the%2520expensive%2520backpropagation%2520operations.%2520Our%2520approach%250Aproduces%2520results%2520that%2520rival%2520or%2520surpass%2520the%2520state-of-the-art%2520training-free%250Ainference%2520methods%2520while%2520requiring%2520a%2520fraction%2520of%2520the%2520time.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520algorithm%2520under%2520both%2520linear%2520%2528inpainting%252C%2520super-resolution%2529%250Aand%2520non-linear%2520%2528style-guided%2520generation%2529%2520constraints.%2520An%2520implementation%2520is%250Aprovided%2520at%2520https%253A//github.com/cvlab-stonybrook/fast-constrained-sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18804v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20constrained%20sampling%20in%20pre-trained%20diffusion%20models&entry.906535625=Alexandros%20Graikos%20and%20Nebojsa%20Jojic%20and%20Dimitris%20Samaras&entry.1292438233=%20%20Large%20denoising%20diffusion%20models%2C%20such%20as%20Stable%20Diffusion%2C%20have%20been%20trained%0Aon%20billions%20of%20image-caption%20pairs%20to%20perform%20text-conditioned%20image%0Ageneration.%20As%20a%20byproduct%20of%20this%20training%2C%20these%20models%20have%20acquired%20general%0Aknowledge%20about%20image%20statistics%2C%20which%20can%20be%20useful%20for%20other%20inference%0Atasks.%20However%2C%20when%20confronted%20with%20sampling%20an%20image%20under%20new%20constraints%2C%0Ae.g.%20generating%20the%20missing%20parts%20of%20an%20image%2C%20using%20large%20pre-trained%0Atext-to-image%20diffusion%20models%20is%20inefficient%20and%20often%20unreliable.%20Previous%0Aapproaches%20either%20utilized%20backpropagation%20through%20the%20denoiser%20network%2C%20making%0Athem%20significantly%20slower%20and%20more%20memory-demanding%20than%20simple%20text-to-image%0Ageneration%2C%20or%20only%20enforced%20the%20constraint%20locally%2C%20failing%20to%20capture%0Acritical%20long-range%20correlations%20in%20the%20sampled%20image.%20In%20this%20work%2C%20we%20propose%0Aan%20algorithm%20that%20enables%20fast%2C%20high-quality%20generation%20under%20arbitrary%0Aconstraints.%20We%20show%20that%20in%20denoising%20diffusion%20models%2C%20we%20can%20employ%20an%0Aapproximation%20to%20Newton%27s%20optimization%20method%20that%20allows%20us%20to%20speed%20up%0Ainference%20and%20avoid%20the%20expensive%20backpropagation%20operations.%20Our%20approach%0Aproduces%20results%20that%20rival%20or%20surpass%20the%20state-of-the-art%20training-free%0Ainference%20methods%20while%20requiring%20a%20fraction%20of%20the%20time.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20algorithm%20under%20both%20linear%20%28inpainting%2C%20super-resolution%29%0Aand%20non-linear%20%28style-guided%20generation%29%20constraints.%20An%20implementation%20is%0Aprovided%20at%20https%3A//github.com/cvlab-stonybrook/fast-constrained-sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18804v3&entry.124074799=Read"},
{"title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the\n  Rails", "author": "Siwei Han and Jiaqi Liu and Yaofeng Su and Wenbo Duan and Xinyuan Liu and Cihang Xie and Mohit Bansal and Mingyu Ding and Linjun Zhang and Huaxiu Yao", "abstract": "  As Large Language Model (LLM) agents increasingly gain self-evolutionary\ncapabilities to adapt and refine their strategies through real-world\ninteraction, their long-term reliability becomes a critical concern. We\nidentify the Alignment Tipping Process (ATP), a critical post-deployment risk\nunique to self-evolving LLM agents. Unlike training-time failures, ATP arises\nwhen continual interaction drives agents to abandon alignment constraints\nestablished during training in favor of reinforced, self-interested strategies.\nWe formalize and analyze ATP through two complementary paradigms:\nSelf-Interested Exploration, where repeated high-reward deviations induce\nindividual behavioral drift, and Imitative Strategy Diffusion, where deviant\nbehaviors spread across multi-agent systems. Building on these paradigms, we\nconstruct controllable testbeds and benchmark Qwen3-8B and\nLlama-3.1-8B-Instruct. Our experiments show that alignment benefits erode\nrapidly under self-evolution, with initially aligned models converging toward\nunaligned states. In multi-agent settings, successful violations diffuse\nquickly, leading to collective misalignment. Moreover, current reinforcement\nlearning-based alignment methods provide only fragile defenses against\nalignment tipping. Together, these findings demonstrate that alignment of LLM\nagents is not a static property but a fragile and dynamic one, vulnerable to\nfeedback-driven decay during deployment. Our data and code are available at\nhttps://github.com/aiming-lab/ATP.\n", "link": "http://arxiv.org/abs/2510.04860v1", "date": "2025-10-06", "relevancy": 1.9919, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5279}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4773}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignment%20Tipping%20Process%3A%20How%20Self-Evolution%20Pushes%20LLM%20Agents%20Off%20the%0A%20%20Rails&body=Title%3A%20Alignment%20Tipping%20Process%3A%20How%20Self-Evolution%20Pushes%20LLM%20Agents%20Off%20the%0A%20%20Rails%0AAuthor%3A%20Siwei%20Han%20and%20Jiaqi%20Liu%20and%20Yaofeng%20Su%20and%20Wenbo%20Duan%20and%20Xinyuan%20Liu%20and%20Cihang%20Xie%20and%20Mohit%20Bansal%20and%20Mingyu%20Ding%20and%20Linjun%20Zhang%20and%20Huaxiu%20Yao%0AAbstract%3A%20%20%20As%20Large%20Language%20Model%20%28LLM%29%20agents%20increasingly%20gain%20self-evolutionary%0Acapabilities%20to%20adapt%20and%20refine%20their%20strategies%20through%20real-world%0Ainteraction%2C%20their%20long-term%20reliability%20becomes%20a%20critical%20concern.%20We%0Aidentify%20the%20Alignment%20Tipping%20Process%20%28ATP%29%2C%20a%20critical%20post-deployment%20risk%0Aunique%20to%20self-evolving%20LLM%20agents.%20Unlike%20training-time%20failures%2C%20ATP%20arises%0Awhen%20continual%20interaction%20drives%20agents%20to%20abandon%20alignment%20constraints%0Aestablished%20during%20training%20in%20favor%20of%20reinforced%2C%20self-interested%20strategies.%0AWe%20formalize%20and%20analyze%20ATP%20through%20two%20complementary%20paradigms%3A%0ASelf-Interested%20Exploration%2C%20where%20repeated%20high-reward%20deviations%20induce%0Aindividual%20behavioral%20drift%2C%20and%20Imitative%20Strategy%20Diffusion%2C%20where%20deviant%0Abehaviors%20spread%20across%20multi-agent%20systems.%20Building%20on%20these%20paradigms%2C%20we%0Aconstruct%20controllable%20testbeds%20and%20benchmark%20Qwen3-8B%20and%0ALlama-3.1-8B-Instruct.%20Our%20experiments%20show%20that%20alignment%20benefits%20erode%0Arapidly%20under%20self-evolution%2C%20with%20initially%20aligned%20models%20converging%20toward%0Aunaligned%20states.%20In%20multi-agent%20settings%2C%20successful%20violations%20diffuse%0Aquickly%2C%20leading%20to%20collective%20misalignment.%20Moreover%2C%20current%20reinforcement%0Alearning-based%20alignment%20methods%20provide%20only%20fragile%20defenses%20against%0Aalignment%20tipping.%20Together%2C%20these%20findings%20demonstrate%20that%20alignment%20of%20LLM%0Aagents%20is%20not%20a%20static%20property%20but%20a%20fragile%20and%20dynamic%20one%2C%20vulnerable%20to%0Afeedback-driven%20decay%20during%20deployment.%20Our%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/aiming-lab/ATP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignment%2520Tipping%2520Process%253A%2520How%2520Self-Evolution%2520Pushes%2520LLM%2520Agents%2520Off%2520the%250A%2520%2520Rails%26entry.906535625%3DSiwei%2520Han%2520and%2520Jiaqi%2520Liu%2520and%2520Yaofeng%2520Su%2520and%2520Wenbo%2520Duan%2520and%2520Xinyuan%2520Liu%2520and%2520Cihang%2520Xie%2520and%2520Mohit%2520Bansal%2520and%2520Mingyu%2520Ding%2520and%2520Linjun%2520Zhang%2520and%2520Huaxiu%2520Yao%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520increasingly%2520gain%2520self-evolutionary%250Acapabilities%2520to%2520adapt%2520and%2520refine%2520their%2520strategies%2520through%2520real-world%250Ainteraction%252C%2520their%2520long-term%2520reliability%2520becomes%2520a%2520critical%2520concern.%2520We%250Aidentify%2520the%2520Alignment%2520Tipping%2520Process%2520%2528ATP%2529%252C%2520a%2520critical%2520post-deployment%2520risk%250Aunique%2520to%2520self-evolving%2520LLM%2520agents.%2520Unlike%2520training-time%2520failures%252C%2520ATP%2520arises%250Awhen%2520continual%2520interaction%2520drives%2520agents%2520to%2520abandon%2520alignment%2520constraints%250Aestablished%2520during%2520training%2520in%2520favor%2520of%2520reinforced%252C%2520self-interested%2520strategies.%250AWe%2520formalize%2520and%2520analyze%2520ATP%2520through%2520two%2520complementary%2520paradigms%253A%250ASelf-Interested%2520Exploration%252C%2520where%2520repeated%2520high-reward%2520deviations%2520induce%250Aindividual%2520behavioral%2520drift%252C%2520and%2520Imitative%2520Strategy%2520Diffusion%252C%2520where%2520deviant%250Abehaviors%2520spread%2520across%2520multi-agent%2520systems.%2520Building%2520on%2520these%2520paradigms%252C%2520we%250Aconstruct%2520controllable%2520testbeds%2520and%2520benchmark%2520Qwen3-8B%2520and%250ALlama-3.1-8B-Instruct.%2520Our%2520experiments%2520show%2520that%2520alignment%2520benefits%2520erode%250Arapidly%2520under%2520self-evolution%252C%2520with%2520initially%2520aligned%2520models%2520converging%2520toward%250Aunaligned%2520states.%2520In%2520multi-agent%2520settings%252C%2520successful%2520violations%2520diffuse%250Aquickly%252C%2520leading%2520to%2520collective%2520misalignment.%2520Moreover%252C%2520current%2520reinforcement%250Alearning-based%2520alignment%2520methods%2520provide%2520only%2520fragile%2520defenses%2520against%250Aalignment%2520tipping.%2520Together%252C%2520these%2520findings%2520demonstrate%2520that%2520alignment%2520of%2520LLM%250Aagents%2520is%2520not%2520a%2520static%2520property%2520but%2520a%2520fragile%2520and%2520dynamic%2520one%252C%2520vulnerable%2520to%250Afeedback-driven%2520decay%2520during%2520deployment.%2520Our%2520data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/aiming-lab/ATP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignment%20Tipping%20Process%3A%20How%20Self-Evolution%20Pushes%20LLM%20Agents%20Off%20the%0A%20%20Rails&entry.906535625=Siwei%20Han%20and%20Jiaqi%20Liu%20and%20Yaofeng%20Su%20and%20Wenbo%20Duan%20and%20Xinyuan%20Liu%20and%20Cihang%20Xie%20and%20Mohit%20Bansal%20and%20Mingyu%20Ding%20and%20Linjun%20Zhang%20and%20Huaxiu%20Yao&entry.1292438233=%20%20As%20Large%20Language%20Model%20%28LLM%29%20agents%20increasingly%20gain%20self-evolutionary%0Acapabilities%20to%20adapt%20and%20refine%20their%20strategies%20through%20real-world%0Ainteraction%2C%20their%20long-term%20reliability%20becomes%20a%20critical%20concern.%20We%0Aidentify%20the%20Alignment%20Tipping%20Process%20%28ATP%29%2C%20a%20critical%20post-deployment%20risk%0Aunique%20to%20self-evolving%20LLM%20agents.%20Unlike%20training-time%20failures%2C%20ATP%20arises%0Awhen%20continual%20interaction%20drives%20agents%20to%20abandon%20alignment%20constraints%0Aestablished%20during%20training%20in%20favor%20of%20reinforced%2C%20self-interested%20strategies.%0AWe%20formalize%20and%20analyze%20ATP%20through%20two%20complementary%20paradigms%3A%0ASelf-Interested%20Exploration%2C%20where%20repeated%20high-reward%20deviations%20induce%0Aindividual%20behavioral%20drift%2C%20and%20Imitative%20Strategy%20Diffusion%2C%20where%20deviant%0Abehaviors%20spread%20across%20multi-agent%20systems.%20Building%20on%20these%20paradigms%2C%20we%0Aconstruct%20controllable%20testbeds%20and%20benchmark%20Qwen3-8B%20and%0ALlama-3.1-8B-Instruct.%20Our%20experiments%20show%20that%20alignment%20benefits%20erode%0Arapidly%20under%20self-evolution%2C%20with%20initially%20aligned%20models%20converging%20toward%0Aunaligned%20states.%20In%20multi-agent%20settings%2C%20successful%20violations%20diffuse%0Aquickly%2C%20leading%20to%20collective%20misalignment.%20Moreover%2C%20current%20reinforcement%0Alearning-based%20alignment%20methods%20provide%20only%20fragile%20defenses%20against%0Aalignment%20tipping.%20Together%2C%20these%20findings%20demonstrate%20that%20alignment%20of%20LLM%0Aagents%20is%20not%20a%20static%20property%20but%20a%20fragile%20and%20dynamic%20one%2C%20vulnerable%20to%0Afeedback-driven%20decay%20during%20deployment.%20Our%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/aiming-lab/ATP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04860v1&entry.124074799=Read"},
{"title": "H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs", "author": "Selim Furkan Tekin and Fatih Ilhan and Tiansheng Huang and Sihao Hu and Yichang Xu and Zachary Yahn and Ling Liu", "abstract": "  Alignment of pretrained LLMs using instruction-based datasets is critical for\ncreating fine-tuned models that reflect human preference. A growing number of\nalignment-based fine-tuning algorithms and benchmarks emerged recently, fueling\nthe efforts on effective alignments of pre-trained LLMs to ensure helpful,\nharmless, and honest answers from both open-source and closed-source LLMs. This\npaper tackles this problem by developing an alignment fusion approach, coined\nas $H^3$Fusion, with three unique characteristics. First, $H^3$Fusion ensembles\nmultiple individually aligned LLMs to create a final fine-tuned alignment model\nwith enhanced capabilities beyond those of individual models, delivering robust\nalignment through promoting helpful, harmless, honest fusion. Second,\n$H^3$Fusion leverages the mixture-of-experts (MoE) methodology in two steps. We\nfirst freeze the multi-head attention weights of each individual model while\ntuning the FFN layer during alignment fusion. Then we merge the aligned model\nweights with an expert router according to the type of input instruction and\ndynamically select a subset of experts that are best suited for producing the\noutput response. Finally, we boost the performance of the resulting\n$H^3$3Fusion model by introducing gating loss and regularization terms. The\nformer penalizes the selection errors of the expert-router, and the latter\nmediates the expert weights drifting during fine-tuning and dynamically adjusts\nthe fusion behavior of the resulting model by canalizing the activations on the\nexperts. Extensive evaluations on three benchmark datasets show that\n$H^3$3Fusion is more helpful, less harmful, and more honest from two aspects:\nit outperforms each individually aligned model by $11.37\\%$, and it provides\nstronger robustness compared to the state-of-the-art LLM ensemble approaches by\n$13.77\\%$. Code is available at github.com/sftekin/h3fusion.\n", "link": "http://arxiv.org/abs/2411.17792v2", "date": "2025-10-06", "relevancy": 1.9832, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5071}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4956}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H3Fusion%3A%20Helpful%2C%20Harmless%2C%20Honest%20Fusion%20of%20Aligned%20LLMs&body=Title%3A%20H3Fusion%3A%20Helpful%2C%20Harmless%2C%20Honest%20Fusion%20of%20Aligned%20LLMs%0AAuthor%3A%20Selim%20Furkan%20Tekin%20and%20Fatih%20Ilhan%20and%20Tiansheng%20Huang%20and%20Sihao%20Hu%20and%20Yichang%20Xu%20and%20Zachary%20Yahn%20and%20Ling%20Liu%0AAbstract%3A%20%20%20Alignment%20of%20pretrained%20LLMs%20using%20instruction-based%20datasets%20is%20critical%20for%0Acreating%20fine-tuned%20models%20that%20reflect%20human%20preference.%20A%20growing%20number%20of%0Aalignment-based%20fine-tuning%20algorithms%20and%20benchmarks%20emerged%20recently%2C%20fueling%0Athe%20efforts%20on%20effective%20alignments%20of%20pre-trained%20LLMs%20to%20ensure%20helpful%2C%0Aharmless%2C%20and%20honest%20answers%20from%20both%20open-source%20and%20closed-source%20LLMs.%20This%0Apaper%20tackles%20this%20problem%20by%20developing%20an%20alignment%20fusion%20approach%2C%20coined%0Aas%20%24H%5E3%24Fusion%2C%20with%20three%20unique%20characteristics.%20First%2C%20%24H%5E3%24Fusion%20ensembles%0Amultiple%20individually%20aligned%20LLMs%20to%20create%20a%20final%20fine-tuned%20alignment%20model%0Awith%20enhanced%20capabilities%20beyond%20those%20of%20individual%20models%2C%20delivering%20robust%0Aalignment%20through%20promoting%20helpful%2C%20harmless%2C%20honest%20fusion.%20Second%2C%0A%24H%5E3%24Fusion%20leverages%20the%20mixture-of-experts%20%28MoE%29%20methodology%20in%20two%20steps.%20We%0Afirst%20freeze%20the%20multi-head%20attention%20weights%20of%20each%20individual%20model%20while%0Atuning%20the%20FFN%20layer%20during%20alignment%20fusion.%20Then%20we%20merge%20the%20aligned%20model%0Aweights%20with%20an%20expert%20router%20according%20to%20the%20type%20of%20input%20instruction%20and%0Adynamically%20select%20a%20subset%20of%20experts%20that%20are%20best%20suited%20for%20producing%20the%0Aoutput%20response.%20Finally%2C%20we%20boost%20the%20performance%20of%20the%20resulting%0A%24H%5E3%243Fusion%20model%20by%20introducing%20gating%20loss%20and%20regularization%20terms.%20The%0Aformer%20penalizes%20the%20selection%20errors%20of%20the%20expert-router%2C%20and%20the%20latter%0Amediates%20the%20expert%20weights%20drifting%20during%20fine-tuning%20and%20dynamically%20adjusts%0Athe%20fusion%20behavior%20of%20the%20resulting%20model%20by%20canalizing%20the%20activations%20on%20the%0Aexperts.%20Extensive%20evaluations%20on%20three%20benchmark%20datasets%20show%20that%0A%24H%5E3%243Fusion%20is%20more%20helpful%2C%20less%20harmful%2C%20and%20more%20honest%20from%20two%20aspects%3A%0Ait%20outperforms%20each%20individually%20aligned%20model%20by%20%2411.37%5C%25%24%2C%20and%20it%20provides%0Astronger%20robustness%20compared%20to%20the%20state-of-the-art%20LLM%20ensemble%20approaches%20by%0A%2413.77%5C%25%24.%20Code%20is%20available%20at%20github.com/sftekin/h3fusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH3Fusion%253A%2520Helpful%252C%2520Harmless%252C%2520Honest%2520Fusion%2520of%2520Aligned%2520LLMs%26entry.906535625%3DSelim%2520Furkan%2520Tekin%2520and%2520Fatih%2520Ilhan%2520and%2520Tiansheng%2520Huang%2520and%2520Sihao%2520Hu%2520and%2520Yichang%2520Xu%2520and%2520Zachary%2520Yahn%2520and%2520Ling%2520Liu%26entry.1292438233%3D%2520%2520Alignment%2520of%2520pretrained%2520LLMs%2520using%2520instruction-based%2520datasets%2520is%2520critical%2520for%250Acreating%2520fine-tuned%2520models%2520that%2520reflect%2520human%2520preference.%2520A%2520growing%2520number%2520of%250Aalignment-based%2520fine-tuning%2520algorithms%2520and%2520benchmarks%2520emerged%2520recently%252C%2520fueling%250Athe%2520efforts%2520on%2520effective%2520alignments%2520of%2520pre-trained%2520LLMs%2520to%2520ensure%2520helpful%252C%250Aharmless%252C%2520and%2520honest%2520answers%2520from%2520both%2520open-source%2520and%2520closed-source%2520LLMs.%2520This%250Apaper%2520tackles%2520this%2520problem%2520by%2520developing%2520an%2520alignment%2520fusion%2520approach%252C%2520coined%250Aas%2520%2524H%255E3%2524Fusion%252C%2520with%2520three%2520unique%2520characteristics.%2520First%252C%2520%2524H%255E3%2524Fusion%2520ensembles%250Amultiple%2520individually%2520aligned%2520LLMs%2520to%2520create%2520a%2520final%2520fine-tuned%2520alignment%2520model%250Awith%2520enhanced%2520capabilities%2520beyond%2520those%2520of%2520individual%2520models%252C%2520delivering%2520robust%250Aalignment%2520through%2520promoting%2520helpful%252C%2520harmless%252C%2520honest%2520fusion.%2520Second%252C%250A%2524H%255E3%2524Fusion%2520leverages%2520the%2520mixture-of-experts%2520%2528MoE%2529%2520methodology%2520in%2520two%2520steps.%2520We%250Afirst%2520freeze%2520the%2520multi-head%2520attention%2520weights%2520of%2520each%2520individual%2520model%2520while%250Atuning%2520the%2520FFN%2520layer%2520during%2520alignment%2520fusion.%2520Then%2520we%2520merge%2520the%2520aligned%2520model%250Aweights%2520with%2520an%2520expert%2520router%2520according%2520to%2520the%2520type%2520of%2520input%2520instruction%2520and%250Adynamically%2520select%2520a%2520subset%2520of%2520experts%2520that%2520are%2520best%2520suited%2520for%2520producing%2520the%250Aoutput%2520response.%2520Finally%252C%2520we%2520boost%2520the%2520performance%2520of%2520the%2520resulting%250A%2524H%255E3%25243Fusion%2520model%2520by%2520introducing%2520gating%2520loss%2520and%2520regularization%2520terms.%2520The%250Aformer%2520penalizes%2520the%2520selection%2520errors%2520of%2520the%2520expert-router%252C%2520and%2520the%2520latter%250Amediates%2520the%2520expert%2520weights%2520drifting%2520during%2520fine-tuning%2520and%2520dynamically%2520adjusts%250Athe%2520fusion%2520behavior%2520of%2520the%2520resulting%2520model%2520by%2520canalizing%2520the%2520activations%2520on%2520the%250Aexperts.%2520Extensive%2520evaluations%2520on%2520three%2520benchmark%2520datasets%2520show%2520that%250A%2524H%255E3%25243Fusion%2520is%2520more%2520helpful%252C%2520less%2520harmful%252C%2520and%2520more%2520honest%2520from%2520two%2520aspects%253A%250Ait%2520outperforms%2520each%2520individually%2520aligned%2520model%2520by%2520%252411.37%255C%2525%2524%252C%2520and%2520it%2520provides%250Astronger%2520robustness%2520compared%2520to%2520the%2520state-of-the-art%2520LLM%2520ensemble%2520approaches%2520by%250A%252413.77%255C%2525%2524.%2520Code%2520is%2520available%2520at%2520github.com/sftekin/h3fusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H3Fusion%3A%20Helpful%2C%20Harmless%2C%20Honest%20Fusion%20of%20Aligned%20LLMs&entry.906535625=Selim%20Furkan%20Tekin%20and%20Fatih%20Ilhan%20and%20Tiansheng%20Huang%20and%20Sihao%20Hu%20and%20Yichang%20Xu%20and%20Zachary%20Yahn%20and%20Ling%20Liu&entry.1292438233=%20%20Alignment%20of%20pretrained%20LLMs%20using%20instruction-based%20datasets%20is%20critical%20for%0Acreating%20fine-tuned%20models%20that%20reflect%20human%20preference.%20A%20growing%20number%20of%0Aalignment-based%20fine-tuning%20algorithms%20and%20benchmarks%20emerged%20recently%2C%20fueling%0Athe%20efforts%20on%20effective%20alignments%20of%20pre-trained%20LLMs%20to%20ensure%20helpful%2C%0Aharmless%2C%20and%20honest%20answers%20from%20both%20open-source%20and%20closed-source%20LLMs.%20This%0Apaper%20tackles%20this%20problem%20by%20developing%20an%20alignment%20fusion%20approach%2C%20coined%0Aas%20%24H%5E3%24Fusion%2C%20with%20three%20unique%20characteristics.%20First%2C%20%24H%5E3%24Fusion%20ensembles%0Amultiple%20individually%20aligned%20LLMs%20to%20create%20a%20final%20fine-tuned%20alignment%20model%0Awith%20enhanced%20capabilities%20beyond%20those%20of%20individual%20models%2C%20delivering%20robust%0Aalignment%20through%20promoting%20helpful%2C%20harmless%2C%20honest%20fusion.%20Second%2C%0A%24H%5E3%24Fusion%20leverages%20the%20mixture-of-experts%20%28MoE%29%20methodology%20in%20two%20steps.%20We%0Afirst%20freeze%20the%20multi-head%20attention%20weights%20of%20each%20individual%20model%20while%0Atuning%20the%20FFN%20layer%20during%20alignment%20fusion.%20Then%20we%20merge%20the%20aligned%20model%0Aweights%20with%20an%20expert%20router%20according%20to%20the%20type%20of%20input%20instruction%20and%0Adynamically%20select%20a%20subset%20of%20experts%20that%20are%20best%20suited%20for%20producing%20the%0Aoutput%20response.%20Finally%2C%20we%20boost%20the%20performance%20of%20the%20resulting%0A%24H%5E3%243Fusion%20model%20by%20introducing%20gating%20loss%20and%20regularization%20terms.%20The%0Aformer%20penalizes%20the%20selection%20errors%20of%20the%20expert-router%2C%20and%20the%20latter%0Amediates%20the%20expert%20weights%20drifting%20during%20fine-tuning%20and%20dynamically%20adjusts%0Athe%20fusion%20behavior%20of%20the%20resulting%20model%20by%20canalizing%20the%20activations%20on%20the%0Aexperts.%20Extensive%20evaluations%20on%20three%20benchmark%20datasets%20show%20that%0A%24H%5E3%243Fusion%20is%20more%20helpful%2C%20less%20harmful%2C%20and%20more%20honest%20from%20two%20aspects%3A%0Ait%20outperforms%20each%20individually%20aligned%20model%20by%20%2411.37%5C%25%24%2C%20and%20it%20provides%0Astronger%20robustness%20compared%20to%20the%20state-of-the-art%20LLM%20ensemble%20approaches%20by%0A%2413.77%5C%25%24.%20Code%20is%20available%20at%20github.com/sftekin/h3fusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17792v2&entry.124074799=Read"},
{"title": "On Structured State-Space Duality", "author": "Jerry Yao-Chieh Hu and Xiwen Zhang and Weimin Wu and Han Liu", "abstract": "  Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence\nbetween a simple Structured State-Space Model (SSM) and a masked attention\nmechanism. In particular, a state-space model with a scalar-times-identity\nstate matrix is equivalent to a masked self-attention with a $1$-semiseparable\ncausal mask. Consequently, the same sequence transformation (model) has two\nalgorithmic realizations: as a linear-time $O(T)$ recurrence or as a\nquadratic-time $O(T^2)$ attention. In this note, we formalize and generalize\nthis duality: (i) we extend SSD from the scalar-identity case to general\ndiagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs\nmatch the scalar case's training complexity lower bounds while supporting\nricher dynamics; (iii) we establish a necessary and sufficient condition under\nwhich an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we\nshow that such duality fails to extend to standard softmax attention due to\nrank explosion. Together, these results tighten bridge between recurrent SSMs\nand Transformers, and widen the design space for expressive yet efficient\nsequence models.\n", "link": "http://arxiv.org/abs/2510.04944v1", "date": "2025-10-06", "relevancy": 1.9755, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.525}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5036}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Structured%20State-Space%20Duality&body=Title%3A%20On%20Structured%20State-Space%20Duality%0AAuthor%3A%20Jerry%20Yao-Chieh%20Hu%20and%20Xiwen%20Zhang%20and%20Weimin%20Wu%20and%20Han%20Liu%0AAbstract%3A%20%20%20Structured%20State-Space%20Duality%20%28SSD%29%20%5BDao%20%26%20Gu%2C%20ICML%202024%5D%20is%20an%20equivalence%0Abetween%20a%20simple%20Structured%20State-Space%20Model%20%28SSM%29%20and%20a%20masked%20attention%0Amechanism.%20In%20particular%2C%20a%20state-space%20model%20with%20a%20scalar-times-identity%0Astate%20matrix%20is%20equivalent%20to%20a%20masked%20self-attention%20with%20a%20%241%24-semiseparable%0Acausal%20mask.%20Consequently%2C%20the%20same%20sequence%20transformation%20%28model%29%20has%20two%0Aalgorithmic%20realizations%3A%20as%20a%20linear-time%20%24O%28T%29%24%20recurrence%20or%20as%20a%0Aquadratic-time%20%24O%28T%5E2%29%24%20attention.%20In%20this%20note%2C%20we%20formalize%20and%20generalize%0Athis%20duality%3A%20%28i%29%20we%20extend%20SSD%20from%20the%20scalar-identity%20case%20to%20general%0Adiagonal%20SSMs%20%28diagonal%20state%20matrices%29%3B%20%28ii%29%20we%20show%20that%20these%20diagonal%20SSMs%0Amatch%20the%20scalar%20case%27s%20training%20complexity%20lower%20bounds%20while%20supporting%0Aricher%20dynamics%3B%20%28iii%29%20we%20establish%20a%20necessary%20and%20sufficient%20condition%20under%0Awhich%20an%20SSM%20is%20equivalent%20to%20%241%24-semiseparable%20masked%20attention%3B%20and%20%28iv%29%20we%0Ashow%20that%20such%20duality%20fails%20to%20extend%20to%20standard%20softmax%20attention%20due%20to%0Arank%20explosion.%20Together%2C%20these%20results%20tighten%20bridge%20between%20recurrent%20SSMs%0Aand%20Transformers%2C%20and%20widen%20the%20design%20space%20for%20expressive%20yet%20efficient%0Asequence%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Structured%2520State-Space%2520Duality%26entry.906535625%3DJerry%2520Yao-Chieh%2520Hu%2520and%2520Xiwen%2520Zhang%2520and%2520Weimin%2520Wu%2520and%2520Han%2520Liu%26entry.1292438233%3D%2520%2520Structured%2520State-Space%2520Duality%2520%2528SSD%2529%2520%255BDao%2520%2526%2520Gu%252C%2520ICML%25202024%255D%2520is%2520an%2520equivalence%250Abetween%2520a%2520simple%2520Structured%2520State-Space%2520Model%2520%2528SSM%2529%2520and%2520a%2520masked%2520attention%250Amechanism.%2520In%2520particular%252C%2520a%2520state-space%2520model%2520with%2520a%2520scalar-times-identity%250Astate%2520matrix%2520is%2520equivalent%2520to%2520a%2520masked%2520self-attention%2520with%2520a%2520%25241%2524-semiseparable%250Acausal%2520mask.%2520Consequently%252C%2520the%2520same%2520sequence%2520transformation%2520%2528model%2529%2520has%2520two%250Aalgorithmic%2520realizations%253A%2520as%2520a%2520linear-time%2520%2524O%2528T%2529%2524%2520recurrence%2520or%2520as%2520a%250Aquadratic-time%2520%2524O%2528T%255E2%2529%2524%2520attention.%2520In%2520this%2520note%252C%2520we%2520formalize%2520and%2520generalize%250Athis%2520duality%253A%2520%2528i%2529%2520we%2520extend%2520SSD%2520from%2520the%2520scalar-identity%2520case%2520to%2520general%250Adiagonal%2520SSMs%2520%2528diagonal%2520state%2520matrices%2529%253B%2520%2528ii%2529%2520we%2520show%2520that%2520these%2520diagonal%2520SSMs%250Amatch%2520the%2520scalar%2520case%2527s%2520training%2520complexity%2520lower%2520bounds%2520while%2520supporting%250Aricher%2520dynamics%253B%2520%2528iii%2529%2520we%2520establish%2520a%2520necessary%2520and%2520sufficient%2520condition%2520under%250Awhich%2520an%2520SSM%2520is%2520equivalent%2520to%2520%25241%2524-semiseparable%2520masked%2520attention%253B%2520and%2520%2528iv%2529%2520we%250Ashow%2520that%2520such%2520duality%2520fails%2520to%2520extend%2520to%2520standard%2520softmax%2520attention%2520due%2520to%250Arank%2520explosion.%2520Together%252C%2520these%2520results%2520tighten%2520bridge%2520between%2520recurrent%2520SSMs%250Aand%2520Transformers%252C%2520and%2520widen%2520the%2520design%2520space%2520for%2520expressive%2520yet%2520efficient%250Asequence%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Structured%20State-Space%20Duality&entry.906535625=Jerry%20Yao-Chieh%20Hu%20and%20Xiwen%20Zhang%20and%20Weimin%20Wu%20and%20Han%20Liu&entry.1292438233=%20%20Structured%20State-Space%20Duality%20%28SSD%29%20%5BDao%20%26%20Gu%2C%20ICML%202024%5D%20is%20an%20equivalence%0Abetween%20a%20simple%20Structured%20State-Space%20Model%20%28SSM%29%20and%20a%20masked%20attention%0Amechanism.%20In%20particular%2C%20a%20state-space%20model%20with%20a%20scalar-times-identity%0Astate%20matrix%20is%20equivalent%20to%20a%20masked%20self-attention%20with%20a%20%241%24-semiseparable%0Acausal%20mask.%20Consequently%2C%20the%20same%20sequence%20transformation%20%28model%29%20has%20two%0Aalgorithmic%20realizations%3A%20as%20a%20linear-time%20%24O%28T%29%24%20recurrence%20or%20as%20a%0Aquadratic-time%20%24O%28T%5E2%29%24%20attention.%20In%20this%20note%2C%20we%20formalize%20and%20generalize%0Athis%20duality%3A%20%28i%29%20we%20extend%20SSD%20from%20the%20scalar-identity%20case%20to%20general%0Adiagonal%20SSMs%20%28diagonal%20state%20matrices%29%3B%20%28ii%29%20we%20show%20that%20these%20diagonal%20SSMs%0Amatch%20the%20scalar%20case%27s%20training%20complexity%20lower%20bounds%20while%20supporting%0Aricher%20dynamics%3B%20%28iii%29%20we%20establish%20a%20necessary%20and%20sufficient%20condition%20under%0Awhich%20an%20SSM%20is%20equivalent%20to%20%241%24-semiseparable%20masked%20attention%3B%20and%20%28iv%29%20we%0Ashow%20that%20such%20duality%20fails%20to%20extend%20to%20standard%20softmax%20attention%20due%20to%0Arank%20explosion.%20Together%2C%20these%20results%20tighten%20bridge%20between%20recurrent%20SSMs%0Aand%20Transformers%2C%20and%20widen%20the%20design%20space%20for%20expressive%20yet%20efficient%0Asequence%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04944v1&entry.124074799=Read"},
{"title": "RowDetr: End-to-End Crop Row Detection Using Polynomials", "author": "Rahul Harsha Cheppally and Ajay Sharda", "abstract": "  Crop row detection enables autonomous robots to navigate in gps denied\nenvironments. Vision based strategies often struggle in the environments due to\ngaps, curved crop rows and require post-processing steps. Furthermore, labeling\ncrop rows in under the canopy environments accurately is very difficult due to\nocclusions. This study introduces RowDetr, an efficient end-to-end\ntransformer-based neural network for crop row detection in precision\nagriculture. RowDetr leverages a lightweight backbone and a hybrid encoder to\nmodel straight, curved, or occluded crop rows with high precision. Central to\nthe architecture is a novel polynomial representation that enables direct\nparameterization of crop rows, eliminating computationally expensive\npost-processing. Key innovations include a PolySampler module and multi-scale\ndeformable attention, which work together with PolyOptLoss, an energy-based\nloss function designed to optimize geometric alignment between predicted and\nthe annotated crop rows, while also enhancing robustness against labeling\nnoise. RowDetr was evaluated against other state-of-the-art end-to-end crop row\ndetection methods like AgroNav and RolColAttention on a diverse dataset of\n6,962 high-resolution images, used for training, validation, and testing across\nmultiple crop types with annotated crop rows. The system demonstrated superior\nperformance, achieved an F1 score up to 0.74 and a lane position deviation as\nlow as 0.405. Furthermore, RowDetr achieves a real-time inference latency of\n6.7ms, which was optimized to 3.5ms with INT8 quantization on an NVIDIA Jetson\nOrin AGX. This work highlighted the critical efficiency of polynomial\nparameterization, making RowDetr particularly suitable for deployment on edge\ncomputing devices in agricultural robotics and autonomous farming equipment.\nIndex terms > Crop Row Detection, Under Canopy Navigation, Transformers,\nRT-DETR, RT-DETRv2\n", "link": "http://arxiv.org/abs/2412.10525v3", "date": "2025-10-06", "relevancy": 1.9749, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4967}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.492}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RowDetr%3A%20End-to-End%20Crop%20Row%20Detection%20Using%20Polynomials&body=Title%3A%20RowDetr%3A%20End-to-End%20Crop%20Row%20Detection%20Using%20Polynomials%0AAuthor%3A%20Rahul%20Harsha%20Cheppally%20and%20Ajay%20Sharda%0AAbstract%3A%20%20%20Crop%20row%20detection%20enables%20autonomous%20robots%20to%20navigate%20in%20gps%20denied%0Aenvironments.%20Vision%20based%20strategies%20often%20struggle%20in%20the%20environments%20due%20to%0Agaps%2C%20curved%20crop%20rows%20and%20require%20post-processing%20steps.%20Furthermore%2C%20labeling%0Acrop%20rows%20in%20under%20the%20canopy%20environments%20accurately%20is%20very%20difficult%20due%20to%0Aocclusions.%20This%20study%20introduces%20RowDetr%2C%20an%20efficient%20end-to-end%0Atransformer-based%20neural%20network%20for%20crop%20row%20detection%20in%20precision%0Aagriculture.%20RowDetr%20leverages%20a%20lightweight%20backbone%20and%20a%20hybrid%20encoder%20to%0Amodel%20straight%2C%20curved%2C%20or%20occluded%20crop%20rows%20with%20high%20precision.%20Central%20to%0Athe%20architecture%20is%20a%20novel%20polynomial%20representation%20that%20enables%20direct%0Aparameterization%20of%20crop%20rows%2C%20eliminating%20computationally%20expensive%0Apost-processing.%20Key%20innovations%20include%20a%20PolySampler%20module%20and%20multi-scale%0Adeformable%20attention%2C%20which%20work%20together%20with%20PolyOptLoss%2C%20an%20energy-based%0Aloss%20function%20designed%20to%20optimize%20geometric%20alignment%20between%20predicted%20and%0Athe%20annotated%20crop%20rows%2C%20while%20also%20enhancing%20robustness%20against%20labeling%0Anoise.%20RowDetr%20was%20evaluated%20against%20other%20state-of-the-art%20end-to-end%20crop%20row%0Adetection%20methods%20like%20AgroNav%20and%20RolColAttention%20on%20a%20diverse%20dataset%20of%0A6%2C962%20high-resolution%20images%2C%20used%20for%20training%2C%20validation%2C%20and%20testing%20across%0Amultiple%20crop%20types%20with%20annotated%20crop%20rows.%20The%20system%20demonstrated%20superior%0Aperformance%2C%20achieved%20an%20F1%20score%20up%20to%200.74%20and%20a%20lane%20position%20deviation%20as%0Alow%20as%200.405.%20Furthermore%2C%20RowDetr%20achieves%20a%20real-time%20inference%20latency%20of%0A6.7ms%2C%20which%20was%20optimized%20to%203.5ms%20with%20INT8%20quantization%20on%20an%20NVIDIA%20Jetson%0AOrin%20AGX.%20This%20work%20highlighted%20the%20critical%20efficiency%20of%20polynomial%0Aparameterization%2C%20making%20RowDetr%20particularly%20suitable%20for%20deployment%20on%20edge%0Acomputing%20devices%20in%20agricultural%20robotics%20and%20autonomous%20farming%20equipment.%0AIndex%20terms%20%3E%20Crop%20Row%20Detection%2C%20Under%20Canopy%20Navigation%2C%20Transformers%2C%0ART-DETR%2C%20RT-DETRv2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10525v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRowDetr%253A%2520End-to-End%2520Crop%2520Row%2520Detection%2520Using%2520Polynomials%26entry.906535625%3DRahul%2520Harsha%2520Cheppally%2520and%2520Ajay%2520Sharda%26entry.1292438233%3D%2520%2520Crop%2520row%2520detection%2520enables%2520autonomous%2520robots%2520to%2520navigate%2520in%2520gps%2520denied%250Aenvironments.%2520Vision%2520based%2520strategies%2520often%2520struggle%2520in%2520the%2520environments%2520due%2520to%250Agaps%252C%2520curved%2520crop%2520rows%2520and%2520require%2520post-processing%2520steps.%2520Furthermore%252C%2520labeling%250Acrop%2520rows%2520in%2520under%2520the%2520canopy%2520environments%2520accurately%2520is%2520very%2520difficult%2520due%2520to%250Aocclusions.%2520This%2520study%2520introduces%2520RowDetr%252C%2520an%2520efficient%2520end-to-end%250Atransformer-based%2520neural%2520network%2520for%2520crop%2520row%2520detection%2520in%2520precision%250Aagriculture.%2520RowDetr%2520leverages%2520a%2520lightweight%2520backbone%2520and%2520a%2520hybrid%2520encoder%2520to%250Amodel%2520straight%252C%2520curved%252C%2520or%2520occluded%2520crop%2520rows%2520with%2520high%2520precision.%2520Central%2520to%250Athe%2520architecture%2520is%2520a%2520novel%2520polynomial%2520representation%2520that%2520enables%2520direct%250Aparameterization%2520of%2520crop%2520rows%252C%2520eliminating%2520computationally%2520expensive%250Apost-processing.%2520Key%2520innovations%2520include%2520a%2520PolySampler%2520module%2520and%2520multi-scale%250Adeformable%2520attention%252C%2520which%2520work%2520together%2520with%2520PolyOptLoss%252C%2520an%2520energy-based%250Aloss%2520function%2520designed%2520to%2520optimize%2520geometric%2520alignment%2520between%2520predicted%2520and%250Athe%2520annotated%2520crop%2520rows%252C%2520while%2520also%2520enhancing%2520robustness%2520against%2520labeling%250Anoise.%2520RowDetr%2520was%2520evaluated%2520against%2520other%2520state-of-the-art%2520end-to-end%2520crop%2520row%250Adetection%2520methods%2520like%2520AgroNav%2520and%2520RolColAttention%2520on%2520a%2520diverse%2520dataset%2520of%250A6%252C962%2520high-resolution%2520images%252C%2520used%2520for%2520training%252C%2520validation%252C%2520and%2520testing%2520across%250Amultiple%2520crop%2520types%2520with%2520annotated%2520crop%2520rows.%2520The%2520system%2520demonstrated%2520superior%250Aperformance%252C%2520achieved%2520an%2520F1%2520score%2520up%2520to%25200.74%2520and%2520a%2520lane%2520position%2520deviation%2520as%250Alow%2520as%25200.405.%2520Furthermore%252C%2520RowDetr%2520achieves%2520a%2520real-time%2520inference%2520latency%2520of%250A6.7ms%252C%2520which%2520was%2520optimized%2520to%25203.5ms%2520with%2520INT8%2520quantization%2520on%2520an%2520NVIDIA%2520Jetson%250AOrin%2520AGX.%2520This%2520work%2520highlighted%2520the%2520critical%2520efficiency%2520of%2520polynomial%250Aparameterization%252C%2520making%2520RowDetr%2520particularly%2520suitable%2520for%2520deployment%2520on%2520edge%250Acomputing%2520devices%2520in%2520agricultural%2520robotics%2520and%2520autonomous%2520farming%2520equipment.%250AIndex%2520terms%2520%253E%2520Crop%2520Row%2520Detection%252C%2520Under%2520Canopy%2520Navigation%252C%2520Transformers%252C%250ART-DETR%252C%2520RT-DETRv2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10525v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RowDetr%3A%20End-to-End%20Crop%20Row%20Detection%20Using%20Polynomials&entry.906535625=Rahul%20Harsha%20Cheppally%20and%20Ajay%20Sharda&entry.1292438233=%20%20Crop%20row%20detection%20enables%20autonomous%20robots%20to%20navigate%20in%20gps%20denied%0Aenvironments.%20Vision%20based%20strategies%20often%20struggle%20in%20the%20environments%20due%20to%0Agaps%2C%20curved%20crop%20rows%20and%20require%20post-processing%20steps.%20Furthermore%2C%20labeling%0Acrop%20rows%20in%20under%20the%20canopy%20environments%20accurately%20is%20very%20difficult%20due%20to%0Aocclusions.%20This%20study%20introduces%20RowDetr%2C%20an%20efficient%20end-to-end%0Atransformer-based%20neural%20network%20for%20crop%20row%20detection%20in%20precision%0Aagriculture.%20RowDetr%20leverages%20a%20lightweight%20backbone%20and%20a%20hybrid%20encoder%20to%0Amodel%20straight%2C%20curved%2C%20or%20occluded%20crop%20rows%20with%20high%20precision.%20Central%20to%0Athe%20architecture%20is%20a%20novel%20polynomial%20representation%20that%20enables%20direct%0Aparameterization%20of%20crop%20rows%2C%20eliminating%20computationally%20expensive%0Apost-processing.%20Key%20innovations%20include%20a%20PolySampler%20module%20and%20multi-scale%0Adeformable%20attention%2C%20which%20work%20together%20with%20PolyOptLoss%2C%20an%20energy-based%0Aloss%20function%20designed%20to%20optimize%20geometric%20alignment%20between%20predicted%20and%0Athe%20annotated%20crop%20rows%2C%20while%20also%20enhancing%20robustness%20against%20labeling%0Anoise.%20RowDetr%20was%20evaluated%20against%20other%20state-of-the-art%20end-to-end%20crop%20row%0Adetection%20methods%20like%20AgroNav%20and%20RolColAttention%20on%20a%20diverse%20dataset%20of%0A6%2C962%20high-resolution%20images%2C%20used%20for%20training%2C%20validation%2C%20and%20testing%20across%0Amultiple%20crop%20types%20with%20annotated%20crop%20rows.%20The%20system%20demonstrated%20superior%0Aperformance%2C%20achieved%20an%20F1%20score%20up%20to%200.74%20and%20a%20lane%20position%20deviation%20as%0Alow%20as%200.405.%20Furthermore%2C%20RowDetr%20achieves%20a%20real-time%20inference%20latency%20of%0A6.7ms%2C%20which%20was%20optimized%20to%203.5ms%20with%20INT8%20quantization%20on%20an%20NVIDIA%20Jetson%0AOrin%20AGX.%20This%20work%20highlighted%20the%20critical%20efficiency%20of%20polynomial%0Aparameterization%2C%20making%20RowDetr%20particularly%20suitable%20for%20deployment%20on%20edge%0Acomputing%20devices%20in%20agricultural%20robotics%20and%20autonomous%20farming%20equipment.%0AIndex%20terms%20%3E%20Crop%20Row%20Detection%2C%20Under%20Canopy%20Navigation%2C%20Transformers%2C%0ART-DETR%2C%20RT-DETRv2%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10525v3&entry.124074799=Read"},
{"title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized\n  Preference Optimization for Aligning Large Reasoning Models", "author": "Mingkang Zhu and Xi Chen and Bei Yu and Hengshuang Zhao and Jiaya Jia", "abstract": "  Large reasoning models (LRMs) generate intermediate reasoning traces before\nproducing final answers, yielding strong gains on multi-step and mathematical\ntasks. Yet aligning LRMs with human preferences, a crucial prerequisite for\nmodel deployment, remains underexplored. The statistically correct objective\nfor preference alignment requires marginalizing over reasoning traces, but this\ncomputation is intractable in practice. A common workaround optimizes a single\nsampled trajectory, which introduces substantial gradient variance from\nstochastic trace sampling. To address this challenge, we frame preference\noptimization for LRMs through the lens of the bias--variance trade-off and\npropose Bias--Variance Optimized Preference Optimization (BVPO), a simple,\ndrop-in method that mixes two gradient estimators: a high-variance trace-based\nestimator and a low-variance empty-trace estimator obtained by disabling\nreasoning trace generation. Our theory shows that BVPO strictly reduces\ntrace-induced variance for any nontrivial mixture, provides a closed-form\nchoice of the mixing weight that minimizes mean-squared error relative to the\ntrue marginal gradient, and under standard smoothness and step-size conditions,\ntightens classical convergence bounds for stochastic gradient descent.\nEmpirically, BVPO improves alignment over the best baseline by up to 7.8 points\non AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on\ngeneral conversational data, BVPO also boosts reasoning performance for base\nmodels by up to 4.0 points on the average of six math reasoning benchmarks.\nThese results identify variance from trace sampling as a key bottleneck and\ndemonstrate that directly optimizing the bias--variance trade-off yields more\nstable training and stronger overall performance.\n", "link": "http://arxiv.org/abs/2510.05095v1", "date": "2025-10-06", "relevancy": 1.9704, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5274}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Noisy%20Traces%20to%20Stable%20Gradients%3A%20Bias-Variance%20Optimized%0A%20%20Preference%20Optimization%20for%20Aligning%20Large%20Reasoning%20Models&body=Title%3A%20From%20Noisy%20Traces%20to%20Stable%20Gradients%3A%20Bias-Variance%20Optimized%0A%20%20Preference%20Optimization%20for%20Aligning%20Large%20Reasoning%20Models%0AAuthor%3A%20Mingkang%20Zhu%20and%20Xi%20Chen%20and%20Bei%20Yu%20and%20Hengshuang%20Zhao%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20Large%20reasoning%20models%20%28LRMs%29%20generate%20intermediate%20reasoning%20traces%20before%0Aproducing%20final%20answers%2C%20yielding%20strong%20gains%20on%20multi-step%20and%20mathematical%0Atasks.%20Yet%20aligning%20LRMs%20with%20human%20preferences%2C%20a%20crucial%20prerequisite%20for%0Amodel%20deployment%2C%20remains%20underexplored.%20The%20statistically%20correct%20objective%0Afor%20preference%20alignment%20requires%20marginalizing%20over%20reasoning%20traces%2C%20but%20this%0Acomputation%20is%20intractable%20in%20practice.%20A%20common%20workaround%20optimizes%20a%20single%0Asampled%20trajectory%2C%20which%20introduces%20substantial%20gradient%20variance%20from%0Astochastic%20trace%20sampling.%20To%20address%20this%20challenge%2C%20we%20frame%20preference%0Aoptimization%20for%20LRMs%20through%20the%20lens%20of%20the%20bias--variance%20trade-off%20and%0Apropose%20Bias--Variance%20Optimized%20Preference%20Optimization%20%28BVPO%29%2C%20a%20simple%2C%0Adrop-in%20method%20that%20mixes%20two%20gradient%20estimators%3A%20a%20high-variance%20trace-based%0Aestimator%20and%20a%20low-variance%20empty-trace%20estimator%20obtained%20by%20disabling%0Areasoning%20trace%20generation.%20Our%20theory%20shows%20that%20BVPO%20strictly%20reduces%0Atrace-induced%20variance%20for%20any%20nontrivial%20mixture%2C%20provides%20a%20closed-form%0Achoice%20of%20the%20mixing%20weight%20that%20minimizes%20mean-squared%20error%20relative%20to%20the%0Atrue%20marginal%20gradient%2C%20and%20under%20standard%20smoothness%20and%20step-size%20conditions%2C%0Atightens%20classical%20convergence%20bounds%20for%20stochastic%20gradient%20descent.%0AEmpirically%2C%20BVPO%20improves%20alignment%20over%20the%20best%20baseline%20by%20up%20to%207.8%20points%0Aon%20AlpacaEval~2%20and%206.8%20points%20on%20Arena-Hard.%20Despite%20being%20trained%20only%20on%0Ageneral%20conversational%20data%2C%20BVPO%20also%20boosts%20reasoning%20performance%20for%20base%0Amodels%20by%20up%20to%204.0%20points%20on%20the%20average%20of%20six%20math%20reasoning%20benchmarks.%0AThese%20results%20identify%20variance%20from%20trace%20sampling%20as%20a%20key%20bottleneck%20and%0Ademonstrate%20that%20directly%20optimizing%20the%20bias--variance%20trade-off%20yields%20more%0Astable%20training%20and%20stronger%20overall%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Noisy%2520Traces%2520to%2520Stable%2520Gradients%253A%2520Bias-Variance%2520Optimized%250A%2520%2520Preference%2520Optimization%2520for%2520Aligning%2520Large%2520Reasoning%2520Models%26entry.906535625%3DMingkang%2520Zhu%2520and%2520Xi%2520Chen%2520and%2520Bei%2520Yu%2520and%2520Hengshuang%2520Zhao%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520%2528LRMs%2529%2520generate%2520intermediate%2520reasoning%2520traces%2520before%250Aproducing%2520final%2520answers%252C%2520yielding%2520strong%2520gains%2520on%2520multi-step%2520and%2520mathematical%250Atasks.%2520Yet%2520aligning%2520LRMs%2520with%2520human%2520preferences%252C%2520a%2520crucial%2520prerequisite%2520for%250Amodel%2520deployment%252C%2520remains%2520underexplored.%2520The%2520statistically%2520correct%2520objective%250Afor%2520preference%2520alignment%2520requires%2520marginalizing%2520over%2520reasoning%2520traces%252C%2520but%2520this%250Acomputation%2520is%2520intractable%2520in%2520practice.%2520A%2520common%2520workaround%2520optimizes%2520a%2520single%250Asampled%2520trajectory%252C%2520which%2520introduces%2520substantial%2520gradient%2520variance%2520from%250Astochastic%2520trace%2520sampling.%2520To%2520address%2520this%2520challenge%252C%2520we%2520frame%2520preference%250Aoptimization%2520for%2520LRMs%2520through%2520the%2520lens%2520of%2520the%2520bias--variance%2520trade-off%2520and%250Apropose%2520Bias--Variance%2520Optimized%2520Preference%2520Optimization%2520%2528BVPO%2529%252C%2520a%2520simple%252C%250Adrop-in%2520method%2520that%2520mixes%2520two%2520gradient%2520estimators%253A%2520a%2520high-variance%2520trace-based%250Aestimator%2520and%2520a%2520low-variance%2520empty-trace%2520estimator%2520obtained%2520by%2520disabling%250Areasoning%2520trace%2520generation.%2520Our%2520theory%2520shows%2520that%2520BVPO%2520strictly%2520reduces%250Atrace-induced%2520variance%2520for%2520any%2520nontrivial%2520mixture%252C%2520provides%2520a%2520closed-form%250Achoice%2520of%2520the%2520mixing%2520weight%2520that%2520minimizes%2520mean-squared%2520error%2520relative%2520to%2520the%250Atrue%2520marginal%2520gradient%252C%2520and%2520under%2520standard%2520smoothness%2520and%2520step-size%2520conditions%252C%250Atightens%2520classical%2520convergence%2520bounds%2520for%2520stochastic%2520gradient%2520descent.%250AEmpirically%252C%2520BVPO%2520improves%2520alignment%2520over%2520the%2520best%2520baseline%2520by%2520up%2520to%25207.8%2520points%250Aon%2520AlpacaEval~2%2520and%25206.8%2520points%2520on%2520Arena-Hard.%2520Despite%2520being%2520trained%2520only%2520on%250Ageneral%2520conversational%2520data%252C%2520BVPO%2520also%2520boosts%2520reasoning%2520performance%2520for%2520base%250Amodels%2520by%2520up%2520to%25204.0%2520points%2520on%2520the%2520average%2520of%2520six%2520math%2520reasoning%2520benchmarks.%250AThese%2520results%2520identify%2520variance%2520from%2520trace%2520sampling%2520as%2520a%2520key%2520bottleneck%2520and%250Ademonstrate%2520that%2520directly%2520optimizing%2520the%2520bias--variance%2520trade-off%2520yields%2520more%250Astable%2520training%2520and%2520stronger%2520overall%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Noisy%20Traces%20to%20Stable%20Gradients%3A%20Bias-Variance%20Optimized%0A%20%20Preference%20Optimization%20for%20Aligning%20Large%20Reasoning%20Models&entry.906535625=Mingkang%20Zhu%20and%20Xi%20Chen%20and%20Bei%20Yu%20and%20Hengshuang%20Zhao%20and%20Jiaya%20Jia&entry.1292438233=%20%20Large%20reasoning%20models%20%28LRMs%29%20generate%20intermediate%20reasoning%20traces%20before%0Aproducing%20final%20answers%2C%20yielding%20strong%20gains%20on%20multi-step%20and%20mathematical%0Atasks.%20Yet%20aligning%20LRMs%20with%20human%20preferences%2C%20a%20crucial%20prerequisite%20for%0Amodel%20deployment%2C%20remains%20underexplored.%20The%20statistically%20correct%20objective%0Afor%20preference%20alignment%20requires%20marginalizing%20over%20reasoning%20traces%2C%20but%20this%0Acomputation%20is%20intractable%20in%20practice.%20A%20common%20workaround%20optimizes%20a%20single%0Asampled%20trajectory%2C%20which%20introduces%20substantial%20gradient%20variance%20from%0Astochastic%20trace%20sampling.%20To%20address%20this%20challenge%2C%20we%20frame%20preference%0Aoptimization%20for%20LRMs%20through%20the%20lens%20of%20the%20bias--variance%20trade-off%20and%0Apropose%20Bias--Variance%20Optimized%20Preference%20Optimization%20%28BVPO%29%2C%20a%20simple%2C%0Adrop-in%20method%20that%20mixes%20two%20gradient%20estimators%3A%20a%20high-variance%20trace-based%0Aestimator%20and%20a%20low-variance%20empty-trace%20estimator%20obtained%20by%20disabling%0Areasoning%20trace%20generation.%20Our%20theory%20shows%20that%20BVPO%20strictly%20reduces%0Atrace-induced%20variance%20for%20any%20nontrivial%20mixture%2C%20provides%20a%20closed-form%0Achoice%20of%20the%20mixing%20weight%20that%20minimizes%20mean-squared%20error%20relative%20to%20the%0Atrue%20marginal%20gradient%2C%20and%20under%20standard%20smoothness%20and%20step-size%20conditions%2C%0Atightens%20classical%20convergence%20bounds%20for%20stochastic%20gradient%20descent.%0AEmpirically%2C%20BVPO%20improves%20alignment%20over%20the%20best%20baseline%20by%20up%20to%207.8%20points%0Aon%20AlpacaEval~2%20and%206.8%20points%20on%20Arena-Hard.%20Despite%20being%20trained%20only%20on%0Ageneral%20conversational%20data%2C%20BVPO%20also%20boosts%20reasoning%20performance%20for%20base%0Amodels%20by%20up%20to%204.0%20points%20on%20the%20average%20of%20six%20math%20reasoning%20benchmarks.%0AThese%20results%20identify%20variance%20from%20trace%20sampling%20as%20a%20key%20bottleneck%20and%0Ademonstrate%20that%20directly%20optimizing%20the%20bias--variance%20trade-off%20yields%20more%0Astable%20training%20and%20stronger%20overall%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05095v1&entry.124074799=Read"},
{"title": "Look-ahead Reasoning with a Learned Model in Imperfect Information Games", "author": "Ond\u0159ej Kub\u00ed\u010dek and Viliam Lis\u00fd", "abstract": "  Test-time reasoning significantly enhances pre-trained AI agents'\nperformance. However, it requires an explicit environment model, often\nunavailable or overly complex in real-world scenarios. While MuZero enables\neffective model learning for search in perfect information games, extending\nthis paradigm to imperfect information games presents substantial challenges\ndue to more nuanced look-ahead reasoning techniques and large number of states\nrelevant for individual decisions. This paper introduces an algorithm LAMIR\nthat learns an abstracted model of an imperfect information game directly from\nthe agent-environment interaction. During test time, this trained model is used\nto perform look-ahead reasoning. The learned abstraction limits the size of\neach subgame to a manageable size, making theoretically principled look-ahead\nreasoning tractable even in games where previous methods could not scale. We\nempirically demonstrate that with sufficient capacity, LAMIR learns the exact\nunderlying game structure, and with limited capacity, it still learns a\nvaluable abstraction, which improves game playing performance of the\npre-trained agents even in large games.\n", "link": "http://arxiv.org/abs/2510.05048v1", "date": "2025-10-06", "relevancy": 1.9691, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4901}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look-ahead%20Reasoning%20with%20a%20Learned%20Model%20in%20Imperfect%20Information%20Games&body=Title%3A%20Look-ahead%20Reasoning%20with%20a%20Learned%20Model%20in%20Imperfect%20Information%20Games%0AAuthor%3A%20Ond%C5%99ej%20Kub%C3%AD%C4%8Dek%20and%20Viliam%20Lis%C3%BD%0AAbstract%3A%20%20%20Test-time%20reasoning%20significantly%20enhances%20pre-trained%20AI%20agents%27%0Aperformance.%20However%2C%20it%20requires%20an%20explicit%20environment%20model%2C%20often%0Aunavailable%20or%20overly%20complex%20in%20real-world%20scenarios.%20While%20MuZero%20enables%0Aeffective%20model%20learning%20for%20search%20in%20perfect%20information%20games%2C%20extending%0Athis%20paradigm%20to%20imperfect%20information%20games%20presents%20substantial%20challenges%0Adue%20to%20more%20nuanced%20look-ahead%20reasoning%20techniques%20and%20large%20number%20of%20states%0Arelevant%20for%20individual%20decisions.%20This%20paper%20introduces%20an%20algorithm%20LAMIR%0Athat%20learns%20an%20abstracted%20model%20of%20an%20imperfect%20information%20game%20directly%20from%0Athe%20agent-environment%20interaction.%20During%20test%20time%2C%20this%20trained%20model%20is%20used%0Ato%20perform%20look-ahead%20reasoning.%20The%20learned%20abstraction%20limits%20the%20size%20of%0Aeach%20subgame%20to%20a%20manageable%20size%2C%20making%20theoretically%20principled%20look-ahead%0Areasoning%20tractable%20even%20in%20games%20where%20previous%20methods%20could%20not%20scale.%20We%0Aempirically%20demonstrate%20that%20with%20sufficient%20capacity%2C%20LAMIR%20learns%20the%20exact%0Aunderlying%20game%20structure%2C%20and%20with%20limited%20capacity%2C%20it%20still%20learns%20a%0Avaluable%20abstraction%2C%20which%20improves%20game%20playing%20performance%20of%20the%0Apre-trained%20agents%20even%20in%20large%20games.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook-ahead%2520Reasoning%2520with%2520a%2520Learned%2520Model%2520in%2520Imperfect%2520Information%2520Games%26entry.906535625%3DOnd%25C5%2599ej%2520Kub%25C3%25AD%25C4%258Dek%2520and%2520Viliam%2520Lis%25C3%25BD%26entry.1292438233%3D%2520%2520Test-time%2520reasoning%2520significantly%2520enhances%2520pre-trained%2520AI%2520agents%2527%250Aperformance.%2520However%252C%2520it%2520requires%2520an%2520explicit%2520environment%2520model%252C%2520often%250Aunavailable%2520or%2520overly%2520complex%2520in%2520real-world%2520scenarios.%2520While%2520MuZero%2520enables%250Aeffective%2520model%2520learning%2520for%2520search%2520in%2520perfect%2520information%2520games%252C%2520extending%250Athis%2520paradigm%2520to%2520imperfect%2520information%2520games%2520presents%2520substantial%2520challenges%250Adue%2520to%2520more%2520nuanced%2520look-ahead%2520reasoning%2520techniques%2520and%2520large%2520number%2520of%2520states%250Arelevant%2520for%2520individual%2520decisions.%2520This%2520paper%2520introduces%2520an%2520algorithm%2520LAMIR%250Athat%2520learns%2520an%2520abstracted%2520model%2520of%2520an%2520imperfect%2520information%2520game%2520directly%2520from%250Athe%2520agent-environment%2520interaction.%2520During%2520test%2520time%252C%2520this%2520trained%2520model%2520is%2520used%250Ato%2520perform%2520look-ahead%2520reasoning.%2520The%2520learned%2520abstraction%2520limits%2520the%2520size%2520of%250Aeach%2520subgame%2520to%2520a%2520manageable%2520size%252C%2520making%2520theoretically%2520principled%2520look-ahead%250Areasoning%2520tractable%2520even%2520in%2520games%2520where%2520previous%2520methods%2520could%2520not%2520scale.%2520We%250Aempirically%2520demonstrate%2520that%2520with%2520sufficient%2520capacity%252C%2520LAMIR%2520learns%2520the%2520exact%250Aunderlying%2520game%2520structure%252C%2520and%2520with%2520limited%2520capacity%252C%2520it%2520still%2520learns%2520a%250Avaluable%2520abstraction%252C%2520which%2520improves%2520game%2520playing%2520performance%2520of%2520the%250Apre-trained%2520agents%2520even%2520in%2520large%2520games.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look-ahead%20Reasoning%20with%20a%20Learned%20Model%20in%20Imperfect%20Information%20Games&entry.906535625=Ond%C5%99ej%20Kub%C3%AD%C4%8Dek%20and%20Viliam%20Lis%C3%BD&entry.1292438233=%20%20Test-time%20reasoning%20significantly%20enhances%20pre-trained%20AI%20agents%27%0Aperformance.%20However%2C%20it%20requires%20an%20explicit%20environment%20model%2C%20often%0Aunavailable%20or%20overly%20complex%20in%20real-world%20scenarios.%20While%20MuZero%20enables%0Aeffective%20model%20learning%20for%20search%20in%20perfect%20information%20games%2C%20extending%0Athis%20paradigm%20to%20imperfect%20information%20games%20presents%20substantial%20challenges%0Adue%20to%20more%20nuanced%20look-ahead%20reasoning%20techniques%20and%20large%20number%20of%20states%0Arelevant%20for%20individual%20decisions.%20This%20paper%20introduces%20an%20algorithm%20LAMIR%0Athat%20learns%20an%20abstracted%20model%20of%20an%20imperfect%20information%20game%20directly%20from%0Athe%20agent-environment%20interaction.%20During%20test%20time%2C%20this%20trained%20model%20is%20used%0Ato%20perform%20look-ahead%20reasoning.%20The%20learned%20abstraction%20limits%20the%20size%20of%0Aeach%20subgame%20to%20a%20manageable%20size%2C%20making%20theoretically%20principled%20look-ahead%0Areasoning%20tractable%20even%20in%20games%20where%20previous%20methods%20could%20not%20scale.%20We%0Aempirically%20demonstrate%20that%20with%20sufficient%20capacity%2C%20LAMIR%20learns%20the%20exact%0Aunderlying%20game%20structure%2C%20and%20with%20limited%20capacity%2C%20it%20still%20learns%20a%0Avaluable%20abstraction%2C%20which%20improves%20game%20playing%20performance%20of%20the%0Apre-trained%20agents%20even%20in%20large%20games.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05048v1&entry.124074799=Read"},
{"title": "Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data\n  under Exact Unlearning in Large Language Model", "author": "Xiaoyu Wu and Yifei Pang and Terrance Liu and Zhiwei Steven Wu", "abstract": "  Large Language Models are typically trained on datasets collected from the\nweb, which may inadvertently contain harmful or sensitive personal information.\nTo address growing privacy concerns, unlearning methods have been proposed to\nremove the influence of specific data from trained models. Of these, exact\nunlearning -- which retrains the model from scratch without the target data --\nis widely regarded the gold standard for mitigating privacy risks in\ndeployment. In this paper, we revisit this assumption in a practical deployment\nsetting where both the pre- and post-unlearning logits API are exposed, such as\nin open-weight scenarios. Targeting this setting, we introduce a novel data\nextraction attack that leverages signals from the pre-unlearning model to guide\nthe post-unlearning model, uncovering patterns that reflect the removed data\ndistribution. Combining model guidance with a token filtering strategy, our\nattack significantly improves extraction success rates -- doubling performance\nin some cases -- across common benchmarks such as MUSE, TOFU, and WMDP.\nFurthermore, we demonstrate our attack's effectiveness on a simulated medical\ndiagnosis dataset to highlight real-world privacy risks associated with exact\nunlearning. In light of our findings, which suggest that unlearning may, in a\ncontradictory way, increase the risk of privacy leakage during real-world\ndeployments, we advocate for evaluation of unlearning methods to consider\nbroader threat models that account not only for post-unlearning models but also\nfor adversarial access to prior checkpoints. Code is publicly available at:\nhttps://github.com/Nicholas0228/unlearned_data_extraction_llm.\n", "link": "http://arxiv.org/abs/2505.24379v2", "date": "2025-10-06", "relevancy": 1.9655, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5472}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4827}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Exact%20Unlearning%20under%20Exposure%3A%20Extracting%20Forgotten%20Data%0A%20%20under%20Exact%20Unlearning%20in%20Large%20Language%20Model&body=Title%3A%20Rethinking%20Exact%20Unlearning%20under%20Exposure%3A%20Extracting%20Forgotten%20Data%0A%20%20under%20Exact%20Unlearning%20in%20Large%20Language%20Model%0AAuthor%3A%20Xiaoyu%20Wu%20and%20Yifei%20Pang%20and%20Terrance%20Liu%20and%20Zhiwei%20Steven%20Wu%0AAbstract%3A%20%20%20Large%20Language%20Models%20are%20typically%20trained%20on%20datasets%20collected%20from%20the%0Aweb%2C%20which%20may%20inadvertently%20contain%20harmful%20or%20sensitive%20personal%20information.%0ATo%20address%20growing%20privacy%20concerns%2C%20unlearning%20methods%20have%20been%20proposed%20to%0Aremove%20the%20influence%20of%20specific%20data%20from%20trained%20models.%20Of%20these%2C%20exact%0Aunlearning%20--%20which%20retrains%20the%20model%20from%20scratch%20without%20the%20target%20data%20--%0Ais%20widely%20regarded%20the%20gold%20standard%20for%20mitigating%20privacy%20risks%20in%0Adeployment.%20In%20this%20paper%2C%20we%20revisit%20this%20assumption%20in%20a%20practical%20deployment%0Asetting%20where%20both%20the%20pre-%20and%20post-unlearning%20logits%20API%20are%20exposed%2C%20such%20as%0Ain%20open-weight%20scenarios.%20Targeting%20this%20setting%2C%20we%20introduce%20a%20novel%20data%0Aextraction%20attack%20that%20leverages%20signals%20from%20the%20pre-unlearning%20model%20to%20guide%0Athe%20post-unlearning%20model%2C%20uncovering%20patterns%20that%20reflect%20the%20removed%20data%0Adistribution.%20Combining%20model%20guidance%20with%20a%20token%20filtering%20strategy%2C%20our%0Aattack%20significantly%20improves%20extraction%20success%20rates%20--%20doubling%20performance%0Ain%20some%20cases%20--%20across%20common%20benchmarks%20such%20as%20MUSE%2C%20TOFU%2C%20and%20WMDP.%0AFurthermore%2C%20we%20demonstrate%20our%20attack%27s%20effectiveness%20on%20a%20simulated%20medical%0Adiagnosis%20dataset%20to%20highlight%20real-world%20privacy%20risks%20associated%20with%20exact%0Aunlearning.%20In%20light%20of%20our%20findings%2C%20which%20suggest%20that%20unlearning%20may%2C%20in%20a%0Acontradictory%20way%2C%20increase%20the%20risk%20of%20privacy%20leakage%20during%20real-world%0Adeployments%2C%20we%20advocate%20for%20evaluation%20of%20unlearning%20methods%20to%20consider%0Abroader%20threat%20models%20that%20account%20not%20only%20for%20post-unlearning%20models%20but%20also%0Afor%20adversarial%20access%20to%20prior%20checkpoints.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Nicholas0228/unlearned_data_extraction_llm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24379v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Exact%2520Unlearning%2520under%2520Exposure%253A%2520Extracting%2520Forgotten%2520Data%250A%2520%2520under%2520Exact%2520Unlearning%2520in%2520Large%2520Language%2520Model%26entry.906535625%3DXiaoyu%2520Wu%2520and%2520Yifei%2520Pang%2520and%2520Terrance%2520Liu%2520and%2520Zhiwei%2520Steven%2520Wu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520are%2520typically%2520trained%2520on%2520datasets%2520collected%2520from%2520the%250Aweb%252C%2520which%2520may%2520inadvertently%2520contain%2520harmful%2520or%2520sensitive%2520personal%2520information.%250ATo%2520address%2520growing%2520privacy%2520concerns%252C%2520unlearning%2520methods%2520have%2520been%2520proposed%2520to%250Aremove%2520the%2520influence%2520of%2520specific%2520data%2520from%2520trained%2520models.%2520Of%2520these%252C%2520exact%250Aunlearning%2520--%2520which%2520retrains%2520the%2520model%2520from%2520scratch%2520without%2520the%2520target%2520data%2520--%250Ais%2520widely%2520regarded%2520the%2520gold%2520standard%2520for%2520mitigating%2520privacy%2520risks%2520in%250Adeployment.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520this%2520assumption%2520in%2520a%2520practical%2520deployment%250Asetting%2520where%2520both%2520the%2520pre-%2520and%2520post-unlearning%2520logits%2520API%2520are%2520exposed%252C%2520such%2520as%250Ain%2520open-weight%2520scenarios.%2520Targeting%2520this%2520setting%252C%2520we%2520introduce%2520a%2520novel%2520data%250Aextraction%2520attack%2520that%2520leverages%2520signals%2520from%2520the%2520pre-unlearning%2520model%2520to%2520guide%250Athe%2520post-unlearning%2520model%252C%2520uncovering%2520patterns%2520that%2520reflect%2520the%2520removed%2520data%250Adistribution.%2520Combining%2520model%2520guidance%2520with%2520a%2520token%2520filtering%2520strategy%252C%2520our%250Aattack%2520significantly%2520improves%2520extraction%2520success%2520rates%2520--%2520doubling%2520performance%250Ain%2520some%2520cases%2520--%2520across%2520common%2520benchmarks%2520such%2520as%2520MUSE%252C%2520TOFU%252C%2520and%2520WMDP.%250AFurthermore%252C%2520we%2520demonstrate%2520our%2520attack%2527s%2520effectiveness%2520on%2520a%2520simulated%2520medical%250Adiagnosis%2520dataset%2520to%2520highlight%2520real-world%2520privacy%2520risks%2520associated%2520with%2520exact%250Aunlearning.%2520In%2520light%2520of%2520our%2520findings%252C%2520which%2520suggest%2520that%2520unlearning%2520may%252C%2520in%2520a%250Acontradictory%2520way%252C%2520increase%2520the%2520risk%2520of%2520privacy%2520leakage%2520during%2520real-world%250Adeployments%252C%2520we%2520advocate%2520for%2520evaluation%2520of%2520unlearning%2520methods%2520to%2520consider%250Abroader%2520threat%2520models%2520that%2520account%2520not%2520only%2520for%2520post-unlearning%2520models%2520but%2520also%250Afor%2520adversarial%2520access%2520to%2520prior%2520checkpoints.%2520Code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/Nicholas0228/unlearned_data_extraction_llm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24379v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Exact%20Unlearning%20under%20Exposure%3A%20Extracting%20Forgotten%20Data%0A%20%20under%20Exact%20Unlearning%20in%20Large%20Language%20Model&entry.906535625=Xiaoyu%20Wu%20and%20Yifei%20Pang%20and%20Terrance%20Liu%20and%20Zhiwei%20Steven%20Wu&entry.1292438233=%20%20Large%20Language%20Models%20are%20typically%20trained%20on%20datasets%20collected%20from%20the%0Aweb%2C%20which%20may%20inadvertently%20contain%20harmful%20or%20sensitive%20personal%20information.%0ATo%20address%20growing%20privacy%20concerns%2C%20unlearning%20methods%20have%20been%20proposed%20to%0Aremove%20the%20influence%20of%20specific%20data%20from%20trained%20models.%20Of%20these%2C%20exact%0Aunlearning%20--%20which%20retrains%20the%20model%20from%20scratch%20without%20the%20target%20data%20--%0Ais%20widely%20regarded%20the%20gold%20standard%20for%20mitigating%20privacy%20risks%20in%0Adeployment.%20In%20this%20paper%2C%20we%20revisit%20this%20assumption%20in%20a%20practical%20deployment%0Asetting%20where%20both%20the%20pre-%20and%20post-unlearning%20logits%20API%20are%20exposed%2C%20such%20as%0Ain%20open-weight%20scenarios.%20Targeting%20this%20setting%2C%20we%20introduce%20a%20novel%20data%0Aextraction%20attack%20that%20leverages%20signals%20from%20the%20pre-unlearning%20model%20to%20guide%0Athe%20post-unlearning%20model%2C%20uncovering%20patterns%20that%20reflect%20the%20removed%20data%0Adistribution.%20Combining%20model%20guidance%20with%20a%20token%20filtering%20strategy%2C%20our%0Aattack%20significantly%20improves%20extraction%20success%20rates%20--%20doubling%20performance%0Ain%20some%20cases%20--%20across%20common%20benchmarks%20such%20as%20MUSE%2C%20TOFU%2C%20and%20WMDP.%0AFurthermore%2C%20we%20demonstrate%20our%20attack%27s%20effectiveness%20on%20a%20simulated%20medical%0Adiagnosis%20dataset%20to%20highlight%20real-world%20privacy%20risks%20associated%20with%20exact%0Aunlearning.%20In%20light%20of%20our%20findings%2C%20which%20suggest%20that%20unlearning%20may%2C%20in%20a%0Acontradictory%20way%2C%20increase%20the%20risk%20of%20privacy%20leakage%20during%20real-world%0Adeployments%2C%20we%20advocate%20for%20evaluation%20of%20unlearning%20methods%20to%20consider%0Abroader%20threat%20models%20that%20account%20not%20only%20for%20post-unlearning%20models%20but%20also%0Afor%20adversarial%20access%20to%20prior%20checkpoints.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Nicholas0228/unlearned_data_extraction_llm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24379v2&entry.124074799=Read"},
{"title": "Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical\n  Chain-of-Thought Reasoning", "author": "Imran Mansha", "abstract": "  Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated\nremarkable reasoning abilities but require significant computational resources\nfor fine-tuning. This paper presents a resource-efficient fine-tuning approach\nfor LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating\nunder constrained GPU and memory settings. Using parameter-efficient tuning\ntechniques such as LoRA and QLoRA, we adapt the base model on publicly\navailable medical reasoning datasets. The model achieves improved reasoning\ncoherence and factual accuracy while reducing memory usage by up to 60%\ncompared to standard full fine-tuning. Experimental evaluation demonstrates\nthat lightweight adaptations can retain strong reasoning capability in medical\nquestion-answering tasks. This work highlights practical strategies for\ndeploying LLMs in low-resource research environments and provides insights into\nbalancing efficiency and domain specialization for medical AI systems.\n", "link": "http://arxiv.org/abs/2510.05003v1", "date": "2025-10-06", "relevancy": 1.9498, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4948}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resource-Efficient%20Fine-Tuning%20of%20LLaMA-3.2-3B%20for%20Medical%0A%20%20Chain-of-Thought%20Reasoning&body=Title%3A%20Resource-Efficient%20Fine-Tuning%20of%20LLaMA-3.2-3B%20for%20Medical%0A%20%20Chain-of-Thought%20Reasoning%0AAuthor%3A%20Imran%20Mansha%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20GPT-4%20and%20LLaMA%20have%20demonstrated%0Aremarkable%20reasoning%20abilities%20but%20require%20significant%20computational%20resources%0Afor%20fine-tuning.%20This%20paper%20presents%20a%20resource-efficient%20fine-tuning%20approach%0Afor%20LLaMA-3.2-3B%20to%20enhance%20medical%20chain-of-thought%20reasoning%20while%20operating%0Aunder%20constrained%20GPU%20and%20memory%20settings.%20Using%20parameter-efficient%20tuning%0Atechniques%20such%20as%20LoRA%20and%20QLoRA%2C%20we%20adapt%20the%20base%20model%20on%20publicly%0Aavailable%20medical%20reasoning%20datasets.%20The%20model%20achieves%20improved%20reasoning%0Acoherence%20and%20factual%20accuracy%20while%20reducing%20memory%20usage%20by%20up%20to%2060%25%0Acompared%20to%20standard%20full%20fine-tuning.%20Experimental%20evaluation%20demonstrates%0Athat%20lightweight%20adaptations%20can%20retain%20strong%20reasoning%20capability%20in%20medical%0Aquestion-answering%20tasks.%20This%20work%20highlights%20practical%20strategies%20for%0Adeploying%20LLMs%20in%20low-resource%20research%20environments%20and%20provides%20insights%20into%0Abalancing%20efficiency%20and%20domain%20specialization%20for%20medical%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResource-Efficient%2520Fine-Tuning%2520of%2520LLaMA-3.2-3B%2520for%2520Medical%250A%2520%2520Chain-of-Thought%2520Reasoning%26entry.906535625%3DImran%2520Mansha%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520such%2520as%2520GPT-4%2520and%2520LLaMA%2520have%2520demonstrated%250Aremarkable%2520reasoning%2520abilities%2520but%2520require%2520significant%2520computational%2520resources%250Afor%2520fine-tuning.%2520This%2520paper%2520presents%2520a%2520resource-efficient%2520fine-tuning%2520approach%250Afor%2520LLaMA-3.2-3B%2520to%2520enhance%2520medical%2520chain-of-thought%2520reasoning%2520while%2520operating%250Aunder%2520constrained%2520GPU%2520and%2520memory%2520settings.%2520Using%2520parameter-efficient%2520tuning%250Atechniques%2520such%2520as%2520LoRA%2520and%2520QLoRA%252C%2520we%2520adapt%2520the%2520base%2520model%2520on%2520publicly%250Aavailable%2520medical%2520reasoning%2520datasets.%2520The%2520model%2520achieves%2520improved%2520reasoning%250Acoherence%2520and%2520factual%2520accuracy%2520while%2520reducing%2520memory%2520usage%2520by%2520up%2520to%252060%2525%250Acompared%2520to%2520standard%2520full%2520fine-tuning.%2520Experimental%2520evaluation%2520demonstrates%250Athat%2520lightweight%2520adaptations%2520can%2520retain%2520strong%2520reasoning%2520capability%2520in%2520medical%250Aquestion-answering%2520tasks.%2520This%2520work%2520highlights%2520practical%2520strategies%2520for%250Adeploying%2520LLMs%2520in%2520low-resource%2520research%2520environments%2520and%2520provides%2520insights%2520into%250Abalancing%2520efficiency%2520and%2520domain%2520specialization%2520for%2520medical%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resource-Efficient%20Fine-Tuning%20of%20LLaMA-3.2-3B%20for%20Medical%0A%20%20Chain-of-Thought%20Reasoning&entry.906535625=Imran%20Mansha&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20GPT-4%20and%20LLaMA%20have%20demonstrated%0Aremarkable%20reasoning%20abilities%20but%20require%20significant%20computational%20resources%0Afor%20fine-tuning.%20This%20paper%20presents%20a%20resource-efficient%20fine-tuning%20approach%0Afor%20LLaMA-3.2-3B%20to%20enhance%20medical%20chain-of-thought%20reasoning%20while%20operating%0Aunder%20constrained%20GPU%20and%20memory%20settings.%20Using%20parameter-efficient%20tuning%0Atechniques%20such%20as%20LoRA%20and%20QLoRA%2C%20we%20adapt%20the%20base%20model%20on%20publicly%0Aavailable%20medical%20reasoning%20datasets.%20The%20model%20achieves%20improved%20reasoning%0Acoherence%20and%20factual%20accuracy%20while%20reducing%20memory%20usage%20by%20up%20to%2060%25%0Acompared%20to%20standard%20full%20fine-tuning.%20Experimental%20evaluation%20demonstrates%0Athat%20lightweight%20adaptations%20can%20retain%20strong%20reasoning%20capability%20in%20medical%0Aquestion-answering%20tasks.%20This%20work%20highlights%20practical%20strategies%20for%0Adeploying%20LLMs%20in%20low-resource%20research%20environments%20and%20provides%20insights%20into%0Abalancing%20efficiency%20and%20domain%20specialization%20for%20medical%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05003v1&entry.124074799=Read"},
{"title": "TopInG: Topologically Interpretable Graph Learning via Persistent\n  Rationale Filtration", "author": "Cheng Xin and Fan Xu and Xin Ding and Jie Gao and Jiaxin Ding", "abstract": "  Graph Neural Networks (GNNs) have shown remarkable success across various\nscientific fields, yet their adoption in critical decision-making is often\nhindered by a lack of interpretability. Recently, intrinsically interpretable\nGNNs have been studied to provide insights into model predictions by\nidentifying rationale substructures in graphs. However, existing methods face\nchallenges when the underlying rationale subgraphs are complex and varied. In\nthis work, we propose TopInG: Topologically Interpretable Graph Learning, a\nnovel topological framework that leverages persistent homology to identify\npersistent rationale subgraphs. TopInG employs a rationale filtration learning\napproach to model an autoregressive generation process of rationale subgraphs,\nand introduces a self-adjusted topological constraint, termed topological\ndiscrepancy, to enforce a persistent topological distinction between rationale\nsubgraphs and irrelevant counterparts. We provide theoretical guarantees that\nour loss function is uniquely optimized by the ground truth under specific\nconditions. Extensive experiments demonstrate TopInG's effectiveness in\ntackling key challenges, such as handling variform rationale subgraphs,\nbalancing predictive performance with interpretability, and mitigating spurious\ncorrelations. Results show that our approach improves upon state-of-the-art\nmethods on both predictive accuracy and interpretation quality.\n", "link": "http://arxiv.org/abs/2510.05102v1", "date": "2025-10-06", "relevancy": 1.9483, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4992}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4885}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopInG%3A%20Topologically%20Interpretable%20Graph%20Learning%20via%20Persistent%0A%20%20Rationale%20Filtration&body=Title%3A%20TopInG%3A%20Topologically%20Interpretable%20Graph%20Learning%20via%20Persistent%0A%20%20Rationale%20Filtration%0AAuthor%3A%20Cheng%20Xin%20and%20Fan%20Xu%20and%20Xin%20Ding%20and%20Jie%20Gao%20and%20Jiaxin%20Ding%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20remarkable%20success%20across%20various%0Ascientific%20fields%2C%20yet%20their%20adoption%20in%20critical%20decision-making%20is%20often%0Ahindered%20by%20a%20lack%20of%20interpretability.%20Recently%2C%20intrinsically%20interpretable%0AGNNs%20have%20been%20studied%20to%20provide%20insights%20into%20model%20predictions%20by%0Aidentifying%20rationale%20substructures%20in%20graphs.%20However%2C%20existing%20methods%20face%0Achallenges%20when%20the%20underlying%20rationale%20subgraphs%20are%20complex%20and%20varied.%20In%0Athis%20work%2C%20we%20propose%20TopInG%3A%20Topologically%20Interpretable%20Graph%20Learning%2C%20a%0Anovel%20topological%20framework%20that%20leverages%20persistent%20homology%20to%20identify%0Apersistent%20rationale%20subgraphs.%20TopInG%20employs%20a%20rationale%20filtration%20learning%0Aapproach%20to%20model%20an%20autoregressive%20generation%20process%20of%20rationale%20subgraphs%2C%0Aand%20introduces%20a%20self-adjusted%20topological%20constraint%2C%20termed%20topological%0Adiscrepancy%2C%20to%20enforce%20a%20persistent%20topological%20distinction%20between%20rationale%0Asubgraphs%20and%20irrelevant%20counterparts.%20We%20provide%20theoretical%20guarantees%20that%0Aour%20loss%20function%20is%20uniquely%20optimized%20by%20the%20ground%20truth%20under%20specific%0Aconditions.%20Extensive%20experiments%20demonstrate%20TopInG%27s%20effectiveness%20in%0Atackling%20key%20challenges%2C%20such%20as%20handling%20variform%20rationale%20subgraphs%2C%0Abalancing%20predictive%20performance%20with%20interpretability%2C%20and%20mitigating%20spurious%0Acorrelations.%20Results%20show%20that%20our%20approach%20improves%20upon%20state-of-the-art%0Amethods%20on%20both%20predictive%20accuracy%20and%20interpretation%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopInG%253A%2520Topologically%2520Interpretable%2520Graph%2520Learning%2520via%2520Persistent%250A%2520%2520Rationale%2520Filtration%26entry.906535625%3DCheng%2520Xin%2520and%2520Fan%2520Xu%2520and%2520Xin%2520Ding%2520and%2520Jie%2520Gao%2520and%2520Jiaxin%2520Ding%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520shown%2520remarkable%2520success%2520across%2520various%250Ascientific%2520fields%252C%2520yet%2520their%2520adoption%2520in%2520critical%2520decision-making%2520is%2520often%250Ahindered%2520by%2520a%2520lack%2520of%2520interpretability.%2520Recently%252C%2520intrinsically%2520interpretable%250AGNNs%2520have%2520been%2520studied%2520to%2520provide%2520insights%2520into%2520model%2520predictions%2520by%250Aidentifying%2520rationale%2520substructures%2520in%2520graphs.%2520However%252C%2520existing%2520methods%2520face%250Achallenges%2520when%2520the%2520underlying%2520rationale%2520subgraphs%2520are%2520complex%2520and%2520varied.%2520In%250Athis%2520work%252C%2520we%2520propose%2520TopInG%253A%2520Topologically%2520Interpretable%2520Graph%2520Learning%252C%2520a%250Anovel%2520topological%2520framework%2520that%2520leverages%2520persistent%2520homology%2520to%2520identify%250Apersistent%2520rationale%2520subgraphs.%2520TopInG%2520employs%2520a%2520rationale%2520filtration%2520learning%250Aapproach%2520to%2520model%2520an%2520autoregressive%2520generation%2520process%2520of%2520rationale%2520subgraphs%252C%250Aand%2520introduces%2520a%2520self-adjusted%2520topological%2520constraint%252C%2520termed%2520topological%250Adiscrepancy%252C%2520to%2520enforce%2520a%2520persistent%2520topological%2520distinction%2520between%2520rationale%250Asubgraphs%2520and%2520irrelevant%2520counterparts.%2520We%2520provide%2520theoretical%2520guarantees%2520that%250Aour%2520loss%2520function%2520is%2520uniquely%2520optimized%2520by%2520the%2520ground%2520truth%2520under%2520specific%250Aconditions.%2520Extensive%2520experiments%2520demonstrate%2520TopInG%2527s%2520effectiveness%2520in%250Atackling%2520key%2520challenges%252C%2520such%2520as%2520handling%2520variform%2520rationale%2520subgraphs%252C%250Abalancing%2520predictive%2520performance%2520with%2520interpretability%252C%2520and%2520mitigating%2520spurious%250Acorrelations.%2520Results%2520show%2520that%2520our%2520approach%2520improves%2520upon%2520state-of-the-art%250Amethods%2520on%2520both%2520predictive%2520accuracy%2520and%2520interpretation%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopInG%3A%20Topologically%20Interpretable%20Graph%20Learning%20via%20Persistent%0A%20%20Rationale%20Filtration&entry.906535625=Cheng%20Xin%20and%20Fan%20Xu%20and%20Xin%20Ding%20and%20Jie%20Gao%20and%20Jiaxin%20Ding&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20remarkable%20success%20across%20various%0Ascientific%20fields%2C%20yet%20their%20adoption%20in%20critical%20decision-making%20is%20often%0Ahindered%20by%20a%20lack%20of%20interpretability.%20Recently%2C%20intrinsically%20interpretable%0AGNNs%20have%20been%20studied%20to%20provide%20insights%20into%20model%20predictions%20by%0Aidentifying%20rationale%20substructures%20in%20graphs.%20However%2C%20existing%20methods%20face%0Achallenges%20when%20the%20underlying%20rationale%20subgraphs%20are%20complex%20and%20varied.%20In%0Athis%20work%2C%20we%20propose%20TopInG%3A%20Topologically%20Interpretable%20Graph%20Learning%2C%20a%0Anovel%20topological%20framework%20that%20leverages%20persistent%20homology%20to%20identify%0Apersistent%20rationale%20subgraphs.%20TopInG%20employs%20a%20rationale%20filtration%20learning%0Aapproach%20to%20model%20an%20autoregressive%20generation%20process%20of%20rationale%20subgraphs%2C%0Aand%20introduces%20a%20self-adjusted%20topological%20constraint%2C%20termed%20topological%0Adiscrepancy%2C%20to%20enforce%20a%20persistent%20topological%20distinction%20between%20rationale%0Asubgraphs%20and%20irrelevant%20counterparts.%20We%20provide%20theoretical%20guarantees%20that%0Aour%20loss%20function%20is%20uniquely%20optimized%20by%20the%20ground%20truth%20under%20specific%0Aconditions.%20Extensive%20experiments%20demonstrate%20TopInG%27s%20effectiveness%20in%0Atackling%20key%20challenges%2C%20such%20as%20handling%20variform%20rationale%20subgraphs%2C%0Abalancing%20predictive%20performance%20with%20interpretability%2C%20and%20mitigating%20spurious%0Acorrelations.%20Results%20show%20that%20our%20approach%20improves%20upon%20state-of-the-art%0Amethods%20on%20both%20predictive%20accuracy%20and%20interpretation%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05102v1&entry.124074799=Read"},
{"title": "KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code\n  Embeddings", "author": "Ahmed Elhussein and Paul Meddeb and Abigail Newbury and Jeanne Mirone and Martin Stoll and Gamze Gursoy", "abstract": "  Machine learning in healthcare requires effective representation of\nstructured medical codes, but current methods face a trade off: knowledge graph\nbased approaches capture formal relationships but miss real world patterns,\nwhile data driven methods learn empirical associations but often overlook\nstructured knowledge in medical terminologies. We present KEEP (Knowledge\npreserving and Empirically refined Embedding Process), an efficient framework\nthat bridges this gap by combining knowledge graph embeddings with adaptive\nlearning from clinical data. KEEP first generates embeddings from knowledge\ngraphs, then employs regularized training on patient records to adaptively\nintegrate empirical patterns while preserving ontological relationships.\nImportantly, KEEP produces final embeddings without task specific auxiliary or\nend to end training enabling KEEP to support multiple downstream applications\nand model architectures. Evaluations on structured EHR from UK Biobank and\nMIMIC IV demonstrate that KEEP outperforms both traditional and Language Model\nbased approaches in capturing semantic relationships and predicting clinical\noutcomes. Moreover, KEEP's minimal computational requirements make it\nparticularly suitable for resource constrained environments.\n", "link": "http://arxiv.org/abs/2510.05049v1", "date": "2025-10-06", "relevancy": 1.9468, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KEEP%3A%20Integrating%20Medical%20Ontologies%20with%20Clinical%20Data%20for%20Robust%20Code%0A%20%20Embeddings&body=Title%3A%20KEEP%3A%20Integrating%20Medical%20Ontologies%20with%20Clinical%20Data%20for%20Robust%20Code%0A%20%20Embeddings%0AAuthor%3A%20Ahmed%20Elhussein%20and%20Paul%20Meddeb%20and%20Abigail%20Newbury%20and%20Jeanne%20Mirone%20and%20Martin%20Stoll%20and%20Gamze%20Gursoy%0AAbstract%3A%20%20%20Machine%20learning%20in%20healthcare%20requires%20effective%20representation%20of%0Astructured%20medical%20codes%2C%20but%20current%20methods%20face%20a%20trade%20off%3A%20knowledge%20graph%0Abased%20approaches%20capture%20formal%20relationships%20but%20miss%20real%20world%20patterns%2C%0Awhile%20data%20driven%20methods%20learn%20empirical%20associations%20but%20often%20overlook%0Astructured%20knowledge%20in%20medical%20terminologies.%20We%20present%20KEEP%20%28Knowledge%0Apreserving%20and%20Empirically%20refined%20Embedding%20Process%29%2C%20an%20efficient%20framework%0Athat%20bridges%20this%20gap%20by%20combining%20knowledge%20graph%20embeddings%20with%20adaptive%0Alearning%20from%20clinical%20data.%20KEEP%20first%20generates%20embeddings%20from%20knowledge%0Agraphs%2C%20then%20employs%20regularized%20training%20on%20patient%20records%20to%20adaptively%0Aintegrate%20empirical%20patterns%20while%20preserving%20ontological%20relationships.%0AImportantly%2C%20KEEP%20produces%20final%20embeddings%20without%20task%20specific%20auxiliary%20or%0Aend%20to%20end%20training%20enabling%20KEEP%20to%20support%20multiple%20downstream%20applications%0Aand%20model%20architectures.%20Evaluations%20on%20structured%20EHR%20from%20UK%20Biobank%20and%0AMIMIC%20IV%20demonstrate%20that%20KEEP%20outperforms%20both%20traditional%20and%20Language%20Model%0Abased%20approaches%20in%20capturing%20semantic%20relationships%20and%20predicting%20clinical%0Aoutcomes.%20Moreover%2C%20KEEP%27s%20minimal%20computational%20requirements%20make%20it%0Aparticularly%20suitable%20for%20resource%20constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKEEP%253A%2520Integrating%2520Medical%2520Ontologies%2520with%2520Clinical%2520Data%2520for%2520Robust%2520Code%250A%2520%2520Embeddings%26entry.906535625%3DAhmed%2520Elhussein%2520and%2520Paul%2520Meddeb%2520and%2520Abigail%2520Newbury%2520and%2520Jeanne%2520Mirone%2520and%2520Martin%2520Stoll%2520and%2520Gamze%2520Gursoy%26entry.1292438233%3D%2520%2520Machine%2520learning%2520in%2520healthcare%2520requires%2520effective%2520representation%2520of%250Astructured%2520medical%2520codes%252C%2520but%2520current%2520methods%2520face%2520a%2520trade%2520off%253A%2520knowledge%2520graph%250Abased%2520approaches%2520capture%2520formal%2520relationships%2520but%2520miss%2520real%2520world%2520patterns%252C%250Awhile%2520data%2520driven%2520methods%2520learn%2520empirical%2520associations%2520but%2520often%2520overlook%250Astructured%2520knowledge%2520in%2520medical%2520terminologies.%2520We%2520present%2520KEEP%2520%2528Knowledge%250Apreserving%2520and%2520Empirically%2520refined%2520Embedding%2520Process%2529%252C%2520an%2520efficient%2520framework%250Athat%2520bridges%2520this%2520gap%2520by%2520combining%2520knowledge%2520graph%2520embeddings%2520with%2520adaptive%250Alearning%2520from%2520clinical%2520data.%2520KEEP%2520first%2520generates%2520embeddings%2520from%2520knowledge%250Agraphs%252C%2520then%2520employs%2520regularized%2520training%2520on%2520patient%2520records%2520to%2520adaptively%250Aintegrate%2520empirical%2520patterns%2520while%2520preserving%2520ontological%2520relationships.%250AImportantly%252C%2520KEEP%2520produces%2520final%2520embeddings%2520without%2520task%2520specific%2520auxiliary%2520or%250Aend%2520to%2520end%2520training%2520enabling%2520KEEP%2520to%2520support%2520multiple%2520downstream%2520applications%250Aand%2520model%2520architectures.%2520Evaluations%2520on%2520structured%2520EHR%2520from%2520UK%2520Biobank%2520and%250AMIMIC%2520IV%2520demonstrate%2520that%2520KEEP%2520outperforms%2520both%2520traditional%2520and%2520Language%2520Model%250Abased%2520approaches%2520in%2520capturing%2520semantic%2520relationships%2520and%2520predicting%2520clinical%250Aoutcomes.%2520Moreover%252C%2520KEEP%2527s%2520minimal%2520computational%2520requirements%2520make%2520it%250Aparticularly%2520suitable%2520for%2520resource%2520constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KEEP%3A%20Integrating%20Medical%20Ontologies%20with%20Clinical%20Data%20for%20Robust%20Code%0A%20%20Embeddings&entry.906535625=Ahmed%20Elhussein%20and%20Paul%20Meddeb%20and%20Abigail%20Newbury%20and%20Jeanne%20Mirone%20and%20Martin%20Stoll%20and%20Gamze%20Gursoy&entry.1292438233=%20%20Machine%20learning%20in%20healthcare%20requires%20effective%20representation%20of%0Astructured%20medical%20codes%2C%20but%20current%20methods%20face%20a%20trade%20off%3A%20knowledge%20graph%0Abased%20approaches%20capture%20formal%20relationships%20but%20miss%20real%20world%20patterns%2C%0Awhile%20data%20driven%20methods%20learn%20empirical%20associations%20but%20often%20overlook%0Astructured%20knowledge%20in%20medical%20terminologies.%20We%20present%20KEEP%20%28Knowledge%0Apreserving%20and%20Empirically%20refined%20Embedding%20Process%29%2C%20an%20efficient%20framework%0Athat%20bridges%20this%20gap%20by%20combining%20knowledge%20graph%20embeddings%20with%20adaptive%0Alearning%20from%20clinical%20data.%20KEEP%20first%20generates%20embeddings%20from%20knowledge%0Agraphs%2C%20then%20employs%20regularized%20training%20on%20patient%20records%20to%20adaptively%0Aintegrate%20empirical%20patterns%20while%20preserving%20ontological%20relationships.%0AImportantly%2C%20KEEP%20produces%20final%20embeddings%20without%20task%20specific%20auxiliary%20or%0Aend%20to%20end%20training%20enabling%20KEEP%20to%20support%20multiple%20downstream%20applications%0Aand%20model%20architectures.%20Evaluations%20on%20structured%20EHR%20from%20UK%20Biobank%20and%0AMIMIC%20IV%20demonstrate%20that%20KEEP%20outperforms%20both%20traditional%20and%20Language%20Model%0Abased%20approaches%20in%20capturing%20semantic%20relationships%20and%20predicting%20clinical%0Aoutcomes.%20Moreover%2C%20KEEP%27s%20minimal%20computational%20requirements%20make%20it%0Aparticularly%20suitable%20for%20resource%20constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05049v1&entry.124074799=Read"},
{"title": "MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive\n  Hierarchical Neural Modeling", "author": "Bi-Cheng Yan and Ming-Kang Tsai and Berlin Chen", "abstract": "  Computer-assisted pronunciation training (CAPT) manages to facilitate\nsecond-language (L2) learners to practice pronunciation skills by offering\ntimely and instructive feedback. To examine pronunciation proficiency from\nmultiple facets, existing methods for CAPT broadly fall into two categories:\nmispronunciation detection and diagnosis (MDD) as well as automatic\npronunciation assessment (APA). The former aims to pinpoint phonetic\npronunciation errors and provide diagnostic feedback, while the latter seeks\ninstead to quantify pronunciation proficiency pertaining to various aspects.\nDespite the natural complementarity between MDD and APA, researchers and\npractitioners, however, often treat them as independent tasks with disparate\nmodeling paradigms. In light of this, we in this paper first introduce MuFFIN,\na Multi-Faceted pronunciation Feedback model with an Interactive hierarchical\nNeural architecture, to jointly address the tasks of MDD and APA. To better\ncapture the nuanced distinctions between phonemes in the feature space, a novel\nphoneme-contrastive ordinal regularization mechanism is then put forward to\noptimize the proposed model to generate more phoneme-discriminative features\nwhile factoring in the ordinality of the aspect scores. In addition, to address\nthe intricate data imbalance problem in MDD, we design a simple yet effective\ntraining objective, which is specifically tailored to perturb the outputs of a\nphoneme classifier with the phoneme-specific variations, so as to better render\nthe distribution of predicted phonemes meanwhile considering their\nmispronunciation characteristics. A series of experiments conducted on the\nSpeechocean762 benchmark dataset demonstrates the efficacy of our method in\nrelation to several cutting-edge baselines, showing state-of-the-art\nperformance on both the APA and MDD tasks.\n", "link": "http://arxiv.org/abs/2510.04956v1", "date": "2025-10-06", "relevancy": 1.9304, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4961}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MuFFIN%3A%20Multifaceted%20Pronunciation%20Feedback%20Model%20with%20Interactive%0A%20%20Hierarchical%20Neural%20Modeling&body=Title%3A%20MuFFIN%3A%20Multifaceted%20Pronunciation%20Feedback%20Model%20with%20Interactive%0A%20%20Hierarchical%20Neural%20Modeling%0AAuthor%3A%20Bi-Cheng%20Yan%20and%20Ming-Kang%20Tsai%20and%20Berlin%20Chen%0AAbstract%3A%20%20%20Computer-assisted%20pronunciation%20training%20%28CAPT%29%20manages%20to%20facilitate%0Asecond-language%20%28L2%29%20learners%20to%20practice%20pronunciation%20skills%20by%20offering%0Atimely%20and%20instructive%20feedback.%20To%20examine%20pronunciation%20proficiency%20from%0Amultiple%20facets%2C%20existing%20methods%20for%20CAPT%20broadly%20fall%20into%20two%20categories%3A%0Amispronunciation%20detection%20and%20diagnosis%20%28MDD%29%20as%20well%20as%20automatic%0Apronunciation%20assessment%20%28APA%29.%20The%20former%20aims%20to%20pinpoint%20phonetic%0Apronunciation%20errors%20and%20provide%20diagnostic%20feedback%2C%20while%20the%20latter%20seeks%0Ainstead%20to%20quantify%20pronunciation%20proficiency%20pertaining%20to%20various%20aspects.%0ADespite%20the%20natural%20complementarity%20between%20MDD%20and%20APA%2C%20researchers%20and%0Apractitioners%2C%20however%2C%20often%20treat%20them%20as%20independent%20tasks%20with%20disparate%0Amodeling%20paradigms.%20In%20light%20of%20this%2C%20we%20in%20this%20paper%20first%20introduce%20MuFFIN%2C%0Aa%20Multi-Faceted%20pronunciation%20Feedback%20model%20with%20an%20Interactive%20hierarchical%0ANeural%20architecture%2C%20to%20jointly%20address%20the%20tasks%20of%20MDD%20and%20APA.%20To%20better%0Acapture%20the%20nuanced%20distinctions%20between%20phonemes%20in%20the%20feature%20space%2C%20a%20novel%0Aphoneme-contrastive%20ordinal%20regularization%20mechanism%20is%20then%20put%20forward%20to%0Aoptimize%20the%20proposed%20model%20to%20generate%20more%20phoneme-discriminative%20features%0Awhile%20factoring%20in%20the%20ordinality%20of%20the%20aspect%20scores.%20In%20addition%2C%20to%20address%0Athe%20intricate%20data%20imbalance%20problem%20in%20MDD%2C%20we%20design%20a%20simple%20yet%20effective%0Atraining%20objective%2C%20which%20is%20specifically%20tailored%20to%20perturb%20the%20outputs%20of%20a%0Aphoneme%20classifier%20with%20the%20phoneme-specific%20variations%2C%20so%20as%20to%20better%20render%0Athe%20distribution%20of%20predicted%20phonemes%20meanwhile%20considering%20their%0Amispronunciation%20characteristics.%20A%20series%20of%20experiments%20conducted%20on%20the%0ASpeechocean762%20benchmark%20dataset%20demonstrates%20the%20efficacy%20of%20our%20method%20in%0Arelation%20to%20several%20cutting-edge%20baselines%2C%20showing%20state-of-the-art%0Aperformance%20on%20both%20the%20APA%20and%20MDD%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMuFFIN%253A%2520Multifaceted%2520Pronunciation%2520Feedback%2520Model%2520with%2520Interactive%250A%2520%2520Hierarchical%2520Neural%2520Modeling%26entry.906535625%3DBi-Cheng%2520Yan%2520and%2520Ming-Kang%2520Tsai%2520and%2520Berlin%2520Chen%26entry.1292438233%3D%2520%2520Computer-assisted%2520pronunciation%2520training%2520%2528CAPT%2529%2520manages%2520to%2520facilitate%250Asecond-language%2520%2528L2%2529%2520learners%2520to%2520practice%2520pronunciation%2520skills%2520by%2520offering%250Atimely%2520and%2520instructive%2520feedback.%2520To%2520examine%2520pronunciation%2520proficiency%2520from%250Amultiple%2520facets%252C%2520existing%2520methods%2520for%2520CAPT%2520broadly%2520fall%2520into%2520two%2520categories%253A%250Amispronunciation%2520detection%2520and%2520diagnosis%2520%2528MDD%2529%2520as%2520well%2520as%2520automatic%250Apronunciation%2520assessment%2520%2528APA%2529.%2520The%2520former%2520aims%2520to%2520pinpoint%2520phonetic%250Apronunciation%2520errors%2520and%2520provide%2520diagnostic%2520feedback%252C%2520while%2520the%2520latter%2520seeks%250Ainstead%2520to%2520quantify%2520pronunciation%2520proficiency%2520pertaining%2520to%2520various%2520aspects.%250ADespite%2520the%2520natural%2520complementarity%2520between%2520MDD%2520and%2520APA%252C%2520researchers%2520and%250Apractitioners%252C%2520however%252C%2520often%2520treat%2520them%2520as%2520independent%2520tasks%2520with%2520disparate%250Amodeling%2520paradigms.%2520In%2520light%2520of%2520this%252C%2520we%2520in%2520this%2520paper%2520first%2520introduce%2520MuFFIN%252C%250Aa%2520Multi-Faceted%2520pronunciation%2520Feedback%2520model%2520with%2520an%2520Interactive%2520hierarchical%250ANeural%2520architecture%252C%2520to%2520jointly%2520address%2520the%2520tasks%2520of%2520MDD%2520and%2520APA.%2520To%2520better%250Acapture%2520the%2520nuanced%2520distinctions%2520between%2520phonemes%2520in%2520the%2520feature%2520space%252C%2520a%2520novel%250Aphoneme-contrastive%2520ordinal%2520regularization%2520mechanism%2520is%2520then%2520put%2520forward%2520to%250Aoptimize%2520the%2520proposed%2520model%2520to%2520generate%2520more%2520phoneme-discriminative%2520features%250Awhile%2520factoring%2520in%2520the%2520ordinality%2520of%2520the%2520aspect%2520scores.%2520In%2520addition%252C%2520to%2520address%250Athe%2520intricate%2520data%2520imbalance%2520problem%2520in%2520MDD%252C%2520we%2520design%2520a%2520simple%2520yet%2520effective%250Atraining%2520objective%252C%2520which%2520is%2520specifically%2520tailored%2520to%2520perturb%2520the%2520outputs%2520of%2520a%250Aphoneme%2520classifier%2520with%2520the%2520phoneme-specific%2520variations%252C%2520so%2520as%2520to%2520better%2520render%250Athe%2520distribution%2520of%2520predicted%2520phonemes%2520meanwhile%2520considering%2520their%250Amispronunciation%2520characteristics.%2520A%2520series%2520of%2520experiments%2520conducted%2520on%2520the%250ASpeechocean762%2520benchmark%2520dataset%2520demonstrates%2520the%2520efficacy%2520of%2520our%2520method%2520in%250Arelation%2520to%2520several%2520cutting-edge%2520baselines%252C%2520showing%2520state-of-the-art%250Aperformance%2520on%2520both%2520the%2520APA%2520and%2520MDD%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuFFIN%3A%20Multifaceted%20Pronunciation%20Feedback%20Model%20with%20Interactive%0A%20%20Hierarchical%20Neural%20Modeling&entry.906535625=Bi-Cheng%20Yan%20and%20Ming-Kang%20Tsai%20and%20Berlin%20Chen&entry.1292438233=%20%20Computer-assisted%20pronunciation%20training%20%28CAPT%29%20manages%20to%20facilitate%0Asecond-language%20%28L2%29%20learners%20to%20practice%20pronunciation%20skills%20by%20offering%0Atimely%20and%20instructive%20feedback.%20To%20examine%20pronunciation%20proficiency%20from%0Amultiple%20facets%2C%20existing%20methods%20for%20CAPT%20broadly%20fall%20into%20two%20categories%3A%0Amispronunciation%20detection%20and%20diagnosis%20%28MDD%29%20as%20well%20as%20automatic%0Apronunciation%20assessment%20%28APA%29.%20The%20former%20aims%20to%20pinpoint%20phonetic%0Apronunciation%20errors%20and%20provide%20diagnostic%20feedback%2C%20while%20the%20latter%20seeks%0Ainstead%20to%20quantify%20pronunciation%20proficiency%20pertaining%20to%20various%20aspects.%0ADespite%20the%20natural%20complementarity%20between%20MDD%20and%20APA%2C%20researchers%20and%0Apractitioners%2C%20however%2C%20often%20treat%20them%20as%20independent%20tasks%20with%20disparate%0Amodeling%20paradigms.%20In%20light%20of%20this%2C%20we%20in%20this%20paper%20first%20introduce%20MuFFIN%2C%0Aa%20Multi-Faceted%20pronunciation%20Feedback%20model%20with%20an%20Interactive%20hierarchical%0ANeural%20architecture%2C%20to%20jointly%20address%20the%20tasks%20of%20MDD%20and%20APA.%20To%20better%0Acapture%20the%20nuanced%20distinctions%20between%20phonemes%20in%20the%20feature%20space%2C%20a%20novel%0Aphoneme-contrastive%20ordinal%20regularization%20mechanism%20is%20then%20put%20forward%20to%0Aoptimize%20the%20proposed%20model%20to%20generate%20more%20phoneme-discriminative%20features%0Awhile%20factoring%20in%20the%20ordinality%20of%20the%20aspect%20scores.%20In%20addition%2C%20to%20address%0Athe%20intricate%20data%20imbalance%20problem%20in%20MDD%2C%20we%20design%20a%20simple%20yet%20effective%0Atraining%20objective%2C%20which%20is%20specifically%20tailored%20to%20perturb%20the%20outputs%20of%20a%0Aphoneme%20classifier%20with%20the%20phoneme-specific%20variations%2C%20so%20as%20to%20better%20render%0Athe%20distribution%20of%20predicted%20phonemes%20meanwhile%20considering%20their%0Amispronunciation%20characteristics.%20A%20series%20of%20experiments%20conducted%20on%20the%0ASpeechocean762%20benchmark%20dataset%20demonstrates%20the%20efficacy%20of%20our%20method%20in%0Arelation%20to%20several%20cutting-edge%20baselines%2C%20showing%20state-of-the-art%0Aperformance%20on%20both%20the%20APA%20and%20MDD%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04956v1&entry.124074799=Read"},
{"title": "Data-Driven Performance Guarantees for Classical and Learned Optimizers", "author": "Rajiv Sambharya and Bartolomeo Stellato", "abstract": "  We introduce a data-driven approach to analyze the performance of continuous\noptimization algorithms using generalization guarantees from statistical\nlearning theory. We study classical and learned optimizers to solve families of\nparametric optimization problems. We build generalization guarantees for\nclassical optimizers, using a sample convergence bound, and for learned\noptimizers, using the Probably Approximately Correct (PAC)-Bayes framework. To\ntrain learned optimizers, we use a gradient-based algorithm to directly\nminimize the PAC-Bayes upper bound. Numerical experiments in signal processing,\ncontrol, and meta-learning showcase the ability of our framework to provide\nstrong generalization guarantees for both classical and learned optimizers\ngiven a fixed budget of iterations. For classical optimizers, our bounds are\nmuch tighter than those that worst-case guarantees provide. For learned\noptimizers, our bounds outperform the empirical outcomes observed in their\nnon-learned counterparts.\n", "link": "http://arxiv.org/abs/2404.13831v3", "date": "2025-10-06", "relevancy": 1.9152, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4947}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4844}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Performance%20Guarantees%20for%20Classical%20and%20Learned%20Optimizers&body=Title%3A%20Data-Driven%20Performance%20Guarantees%20for%20Classical%20and%20Learned%20Optimizers%0AAuthor%3A%20Rajiv%20Sambharya%20and%20Bartolomeo%20Stellato%0AAbstract%3A%20%20%20We%20introduce%20a%20data-driven%20approach%20to%20analyze%20the%20performance%20of%20continuous%0Aoptimization%20algorithms%20using%20generalization%20guarantees%20from%20statistical%0Alearning%20theory.%20We%20study%20classical%20and%20learned%20optimizers%20to%20solve%20families%20of%0Aparametric%20optimization%20problems.%20We%20build%20generalization%20guarantees%20for%0Aclassical%20optimizers%2C%20using%20a%20sample%20convergence%20bound%2C%20and%20for%20learned%0Aoptimizers%2C%20using%20the%20Probably%20Approximately%20Correct%20%28PAC%29-Bayes%20framework.%20To%0Atrain%20learned%20optimizers%2C%20we%20use%20a%20gradient-based%20algorithm%20to%20directly%0Aminimize%20the%20PAC-Bayes%20upper%20bound.%20Numerical%20experiments%20in%20signal%20processing%2C%0Acontrol%2C%20and%20meta-learning%20showcase%20the%20ability%20of%20our%20framework%20to%20provide%0Astrong%20generalization%20guarantees%20for%20both%20classical%20and%20learned%20optimizers%0Agiven%20a%20fixed%20budget%20of%20iterations.%20For%20classical%20optimizers%2C%20our%20bounds%20are%0Amuch%20tighter%20than%20those%20that%20worst-case%20guarantees%20provide.%20For%20learned%0Aoptimizers%2C%20our%20bounds%20outperform%20the%20empirical%20outcomes%20observed%20in%20their%0Anon-learned%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13831v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Performance%2520Guarantees%2520for%2520Classical%2520and%2520Learned%2520Optimizers%26entry.906535625%3DRajiv%2520Sambharya%2520and%2520Bartolomeo%2520Stellato%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520data-driven%2520approach%2520to%2520analyze%2520the%2520performance%2520of%2520continuous%250Aoptimization%2520algorithms%2520using%2520generalization%2520guarantees%2520from%2520statistical%250Alearning%2520theory.%2520We%2520study%2520classical%2520and%2520learned%2520optimizers%2520to%2520solve%2520families%2520of%250Aparametric%2520optimization%2520problems.%2520We%2520build%2520generalization%2520guarantees%2520for%250Aclassical%2520optimizers%252C%2520using%2520a%2520sample%2520convergence%2520bound%252C%2520and%2520for%2520learned%250Aoptimizers%252C%2520using%2520the%2520Probably%2520Approximately%2520Correct%2520%2528PAC%2529-Bayes%2520framework.%2520To%250Atrain%2520learned%2520optimizers%252C%2520we%2520use%2520a%2520gradient-based%2520algorithm%2520to%2520directly%250Aminimize%2520the%2520PAC-Bayes%2520upper%2520bound.%2520Numerical%2520experiments%2520in%2520signal%2520processing%252C%250Acontrol%252C%2520and%2520meta-learning%2520showcase%2520the%2520ability%2520of%2520our%2520framework%2520to%2520provide%250Astrong%2520generalization%2520guarantees%2520for%2520both%2520classical%2520and%2520learned%2520optimizers%250Agiven%2520a%2520fixed%2520budget%2520of%2520iterations.%2520For%2520classical%2520optimizers%252C%2520our%2520bounds%2520are%250Amuch%2520tighter%2520than%2520those%2520that%2520worst-case%2520guarantees%2520provide.%2520For%2520learned%250Aoptimizers%252C%2520our%2520bounds%2520outperform%2520the%2520empirical%2520outcomes%2520observed%2520in%2520their%250Anon-learned%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13831v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Performance%20Guarantees%20for%20Classical%20and%20Learned%20Optimizers&entry.906535625=Rajiv%20Sambharya%20and%20Bartolomeo%20Stellato&entry.1292438233=%20%20We%20introduce%20a%20data-driven%20approach%20to%20analyze%20the%20performance%20of%20continuous%0Aoptimization%20algorithms%20using%20generalization%20guarantees%20from%20statistical%0Alearning%20theory.%20We%20study%20classical%20and%20learned%20optimizers%20to%20solve%20families%20of%0Aparametric%20optimization%20problems.%20We%20build%20generalization%20guarantees%20for%0Aclassical%20optimizers%2C%20using%20a%20sample%20convergence%20bound%2C%20and%20for%20learned%0Aoptimizers%2C%20using%20the%20Probably%20Approximately%20Correct%20%28PAC%29-Bayes%20framework.%20To%0Atrain%20learned%20optimizers%2C%20we%20use%20a%20gradient-based%20algorithm%20to%20directly%0Aminimize%20the%20PAC-Bayes%20upper%20bound.%20Numerical%20experiments%20in%20signal%20processing%2C%0Acontrol%2C%20and%20meta-learning%20showcase%20the%20ability%20of%20our%20framework%20to%20provide%0Astrong%20generalization%20guarantees%20for%20both%20classical%20and%20learned%20optimizers%0Agiven%20a%20fixed%20budget%20of%20iterations.%20For%20classical%20optimizers%2C%20our%20bounds%20are%0Amuch%20tighter%20than%20those%20that%20worst-case%20guarantees%20provide.%20For%20learned%0Aoptimizers%2C%20our%20bounds%20outperform%20the%20empirical%20outcomes%20observed%20in%20their%0Anon-learned%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13831v3&entry.124074799=Read"},
{"title": "Less is More: Recursive Reasoning with Tiny Networks", "author": "Alexia Jolicoeur-Martineau", "abstract": "  Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,\nand ARC-AGI while trained with small models (27M parameters) on small data\n(around 1000 examples). HRM holds great promise for solving hard problems with\nsmall networks, but it is not yet well understood and may be suboptimal. We\npropose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach\nthat achieves significantly higher generalization than HRM, while using a\nsingle tiny network with only 2 layers. With only 7M parameters, TRM obtains\n45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs\n(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the\nparameters.\n", "link": "http://arxiv.org/abs/2510.04871v1", "date": "2025-10-06", "relevancy": 1.9143, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4921}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20Recursive%20Reasoning%20with%20Tiny%20Networks&body=Title%3A%20Less%20is%20More%3A%20Recursive%20Reasoning%20with%20Tiny%20Networks%0AAuthor%3A%20Alexia%20Jolicoeur-Martineau%0AAbstract%3A%20%20%20Hierarchical%20Reasoning%20Model%20%28HRM%29%20is%20a%20novel%20approach%20using%20two%20small%20neural%0Anetworks%20recursing%20at%20different%20frequencies.%20This%20biologically%20inspired%20method%0Abeats%20Large%20Language%20models%20%28LLMs%29%20on%20hard%20puzzle%20tasks%20such%20as%20Sudoku%2C%20Maze%2C%0Aand%20ARC-AGI%20while%20trained%20with%20small%20models%20%2827M%20parameters%29%20on%20small%20data%0A%28around%201000%20examples%29.%20HRM%20holds%20great%20promise%20for%20solving%20hard%20problems%20with%0Asmall%20networks%2C%20but%20it%20is%20not%20yet%20well%20understood%20and%20may%20be%20suboptimal.%20We%0Apropose%20Tiny%20Recursive%20Model%20%28TRM%29%2C%20a%20much%20simpler%20recursive%20reasoning%20approach%0Athat%20achieves%20significantly%20higher%20generalization%20than%20HRM%2C%20while%20using%20a%0Asingle%20tiny%20network%20with%20only%202%20layers.%20With%20only%207M%20parameters%2C%20TRM%20obtains%0A45%25%20test-accuracy%20on%20ARC-AGI-1%20and%208%25%20on%20ARC-AGI-2%2C%20higher%20than%20most%20LLMs%0A%28e.g.%2C%20Deepseek%20R1%2C%20o3-mini%2C%20Gemini%202.5%20Pro%29%20with%20less%20than%200.01%25%20of%20the%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520Recursive%2520Reasoning%2520with%2520Tiny%2520Networks%26entry.906535625%3DAlexia%2520Jolicoeur-Martineau%26entry.1292438233%3D%2520%2520Hierarchical%2520Reasoning%2520Model%2520%2528HRM%2529%2520is%2520a%2520novel%2520approach%2520using%2520two%2520small%2520neural%250Anetworks%2520recursing%2520at%2520different%2520frequencies.%2520This%2520biologically%2520inspired%2520method%250Abeats%2520Large%2520Language%2520models%2520%2528LLMs%2529%2520on%2520hard%2520puzzle%2520tasks%2520such%2520as%2520Sudoku%252C%2520Maze%252C%250Aand%2520ARC-AGI%2520while%2520trained%2520with%2520small%2520models%2520%252827M%2520parameters%2529%2520on%2520small%2520data%250A%2528around%25201000%2520examples%2529.%2520HRM%2520holds%2520great%2520promise%2520for%2520solving%2520hard%2520problems%2520with%250Asmall%2520networks%252C%2520but%2520it%2520is%2520not%2520yet%2520well%2520understood%2520and%2520may%2520be%2520suboptimal.%2520We%250Apropose%2520Tiny%2520Recursive%2520Model%2520%2528TRM%2529%252C%2520a%2520much%2520simpler%2520recursive%2520reasoning%2520approach%250Athat%2520achieves%2520significantly%2520higher%2520generalization%2520than%2520HRM%252C%2520while%2520using%2520a%250Asingle%2520tiny%2520network%2520with%2520only%25202%2520layers.%2520With%2520only%25207M%2520parameters%252C%2520TRM%2520obtains%250A45%2525%2520test-accuracy%2520on%2520ARC-AGI-1%2520and%25208%2525%2520on%2520ARC-AGI-2%252C%2520higher%2520than%2520most%2520LLMs%250A%2528e.g.%252C%2520Deepseek%2520R1%252C%2520o3-mini%252C%2520Gemini%25202.5%2520Pro%2529%2520with%2520less%2520than%25200.01%2525%2520of%2520the%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20Recursive%20Reasoning%20with%20Tiny%20Networks&entry.906535625=Alexia%20Jolicoeur-Martineau&entry.1292438233=%20%20Hierarchical%20Reasoning%20Model%20%28HRM%29%20is%20a%20novel%20approach%20using%20two%20small%20neural%0Anetworks%20recursing%20at%20different%20frequencies.%20This%20biologically%20inspired%20method%0Abeats%20Large%20Language%20models%20%28LLMs%29%20on%20hard%20puzzle%20tasks%20such%20as%20Sudoku%2C%20Maze%2C%0Aand%20ARC-AGI%20while%20trained%20with%20small%20models%20%2827M%20parameters%29%20on%20small%20data%0A%28around%201000%20examples%29.%20HRM%20holds%20great%20promise%20for%20solving%20hard%20problems%20with%0Asmall%20networks%2C%20but%20it%20is%20not%20yet%20well%20understood%20and%20may%20be%20suboptimal.%20We%0Apropose%20Tiny%20Recursive%20Model%20%28TRM%29%2C%20a%20much%20simpler%20recursive%20reasoning%20approach%0Athat%20achieves%20significantly%20higher%20generalization%20than%20HRM%2C%20while%20using%20a%0Asingle%20tiny%20network%20with%20only%202%20layers.%20With%20only%207M%20parameters%2C%20TRM%20obtains%0A45%25%20test-accuracy%20on%20ARC-AGI-1%20and%208%25%20on%20ARC-AGI-2%2C%20higher%20than%20most%20LLMs%0A%28e.g.%2C%20Deepseek%20R1%2C%20o3-mini%2C%20Gemini%202.5%20Pro%29%20with%20less%20than%200.01%25%20of%20the%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04871v1&entry.124074799=Read"},
{"title": "AURA Score: A Metric For Holistic Audio Question Answering Evaluation", "author": "Satvik Dixit and Soham Deshmukh and Bhiksha Raj", "abstract": "  Audio Question Answering (AQA) is a key task for evaluating Audio-Language\nModels (ALMs), yet assessing open-ended responses remains challenging. Existing\nmetrics used for AQA such as BLEU, METEOR and BERTScore, mostly adapted from\nNLP and audio captioning, rely on surface similarity and fail to account for\nquestion context, reasoning, and partial correctness. To address the gap in\nliterature, we make three contributions in this work. First, we introduce\nAQEval to enable systematic benchmarking of AQA metrics. It is the first\nbenchmark of its kind, consisting of 10k model responses annotated by multiple\nhumans for their correctness and relevance. Second, we conduct a comprehensive\nanalysis of existing AQA metrics on AQEval, highlighting weak correlation with\nhuman judgment, especially for longer answers. Third, we propose a new metric -\nAURA score, to better evaluate open-ended model responses. On AQEval, AURA\nachieves state-of-the-art correlation with human ratings, significantly\noutperforming all baselines. Through this work, we aim to highlight the\nlimitations of current AQA evaluation methods and motivate better metrics. We\nrelease both the AQEval benchmark and the AURA metric to support future\nresearch in holistic AQA evaluation.\n", "link": "http://arxiv.org/abs/2510.04934v1", "date": "2025-10-06", "relevancy": 1.905, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AURA%20Score%3A%20A%20Metric%20For%20Holistic%20Audio%20Question%20Answering%20Evaluation&body=Title%3A%20AURA%20Score%3A%20A%20Metric%20For%20Holistic%20Audio%20Question%20Answering%20Evaluation%0AAuthor%3A%20Satvik%20Dixit%20and%20Soham%20Deshmukh%20and%20Bhiksha%20Raj%0AAbstract%3A%20%20%20Audio%20Question%20Answering%20%28AQA%29%20is%20a%20key%20task%20for%20evaluating%20Audio-Language%0AModels%20%28ALMs%29%2C%20yet%20assessing%20open-ended%20responses%20remains%20challenging.%20Existing%0Ametrics%20used%20for%20AQA%20such%20as%20BLEU%2C%20METEOR%20and%20BERTScore%2C%20mostly%20adapted%20from%0ANLP%20and%20audio%20captioning%2C%20rely%20on%20surface%20similarity%20and%20fail%20to%20account%20for%0Aquestion%20context%2C%20reasoning%2C%20and%20partial%20correctness.%20To%20address%20the%20gap%20in%0Aliterature%2C%20we%20make%20three%20contributions%20in%20this%20work.%20First%2C%20we%20introduce%0AAQEval%20to%20enable%20systematic%20benchmarking%20of%20AQA%20metrics.%20It%20is%20the%20first%0Abenchmark%20of%20its%20kind%2C%20consisting%20of%2010k%20model%20responses%20annotated%20by%20multiple%0Ahumans%20for%20their%20correctness%20and%20relevance.%20Second%2C%20we%20conduct%20a%20comprehensive%0Aanalysis%20of%20existing%20AQA%20metrics%20on%20AQEval%2C%20highlighting%20weak%20correlation%20with%0Ahuman%20judgment%2C%20especially%20for%20longer%20answers.%20Third%2C%20we%20propose%20a%20new%20metric%20-%0AAURA%20score%2C%20to%20better%20evaluate%20open-ended%20model%20responses.%20On%20AQEval%2C%20AURA%0Aachieves%20state-of-the-art%20correlation%20with%20human%20ratings%2C%20significantly%0Aoutperforming%20all%20baselines.%20Through%20this%20work%2C%20we%20aim%20to%20highlight%20the%0Alimitations%20of%20current%20AQA%20evaluation%20methods%20and%20motivate%20better%20metrics.%20We%0Arelease%20both%20the%20AQEval%20benchmark%20and%20the%20AURA%20metric%20to%20support%20future%0Aresearch%20in%20holistic%20AQA%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAURA%2520Score%253A%2520A%2520Metric%2520For%2520Holistic%2520Audio%2520Question%2520Answering%2520Evaluation%26entry.906535625%3DSatvik%2520Dixit%2520and%2520Soham%2520Deshmukh%2520and%2520Bhiksha%2520Raj%26entry.1292438233%3D%2520%2520Audio%2520Question%2520Answering%2520%2528AQA%2529%2520is%2520a%2520key%2520task%2520for%2520evaluating%2520Audio-Language%250AModels%2520%2528ALMs%2529%252C%2520yet%2520assessing%2520open-ended%2520responses%2520remains%2520challenging.%2520Existing%250Ametrics%2520used%2520for%2520AQA%2520such%2520as%2520BLEU%252C%2520METEOR%2520and%2520BERTScore%252C%2520mostly%2520adapted%2520from%250ANLP%2520and%2520audio%2520captioning%252C%2520rely%2520on%2520surface%2520similarity%2520and%2520fail%2520to%2520account%2520for%250Aquestion%2520context%252C%2520reasoning%252C%2520and%2520partial%2520correctness.%2520To%2520address%2520the%2520gap%2520in%250Aliterature%252C%2520we%2520make%2520three%2520contributions%2520in%2520this%2520work.%2520First%252C%2520we%2520introduce%250AAQEval%2520to%2520enable%2520systematic%2520benchmarking%2520of%2520AQA%2520metrics.%2520It%2520is%2520the%2520first%250Abenchmark%2520of%2520its%2520kind%252C%2520consisting%2520of%252010k%2520model%2520responses%2520annotated%2520by%2520multiple%250Ahumans%2520for%2520their%2520correctness%2520and%2520relevance.%2520Second%252C%2520we%2520conduct%2520a%2520comprehensive%250Aanalysis%2520of%2520existing%2520AQA%2520metrics%2520on%2520AQEval%252C%2520highlighting%2520weak%2520correlation%2520with%250Ahuman%2520judgment%252C%2520especially%2520for%2520longer%2520answers.%2520Third%252C%2520we%2520propose%2520a%2520new%2520metric%2520-%250AAURA%2520score%252C%2520to%2520better%2520evaluate%2520open-ended%2520model%2520responses.%2520On%2520AQEval%252C%2520AURA%250Aachieves%2520state-of-the-art%2520correlation%2520with%2520human%2520ratings%252C%2520significantly%250Aoutperforming%2520all%2520baselines.%2520Through%2520this%2520work%252C%2520we%2520aim%2520to%2520highlight%2520the%250Alimitations%2520of%2520current%2520AQA%2520evaluation%2520methods%2520and%2520motivate%2520better%2520metrics.%2520We%250Arelease%2520both%2520the%2520AQEval%2520benchmark%2520and%2520the%2520AURA%2520metric%2520to%2520support%2520future%250Aresearch%2520in%2520holistic%2520AQA%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AURA%20Score%3A%20A%20Metric%20For%20Holistic%20Audio%20Question%20Answering%20Evaluation&entry.906535625=Satvik%20Dixit%20and%20Soham%20Deshmukh%20and%20Bhiksha%20Raj&entry.1292438233=%20%20Audio%20Question%20Answering%20%28AQA%29%20is%20a%20key%20task%20for%20evaluating%20Audio-Language%0AModels%20%28ALMs%29%2C%20yet%20assessing%20open-ended%20responses%20remains%20challenging.%20Existing%0Ametrics%20used%20for%20AQA%20such%20as%20BLEU%2C%20METEOR%20and%20BERTScore%2C%20mostly%20adapted%20from%0ANLP%20and%20audio%20captioning%2C%20rely%20on%20surface%20similarity%20and%20fail%20to%20account%20for%0Aquestion%20context%2C%20reasoning%2C%20and%20partial%20correctness.%20To%20address%20the%20gap%20in%0Aliterature%2C%20we%20make%20three%20contributions%20in%20this%20work.%20First%2C%20we%20introduce%0AAQEval%20to%20enable%20systematic%20benchmarking%20of%20AQA%20metrics.%20It%20is%20the%20first%0Abenchmark%20of%20its%20kind%2C%20consisting%20of%2010k%20model%20responses%20annotated%20by%20multiple%0Ahumans%20for%20their%20correctness%20and%20relevance.%20Second%2C%20we%20conduct%20a%20comprehensive%0Aanalysis%20of%20existing%20AQA%20metrics%20on%20AQEval%2C%20highlighting%20weak%20correlation%20with%0Ahuman%20judgment%2C%20especially%20for%20longer%20answers.%20Third%2C%20we%20propose%20a%20new%20metric%20-%0AAURA%20score%2C%20to%20better%20evaluate%20open-ended%20model%20responses.%20On%20AQEval%2C%20AURA%0Aachieves%20state-of-the-art%20correlation%20with%20human%20ratings%2C%20significantly%0Aoutperforming%20all%20baselines.%20Through%20this%20work%2C%20we%20aim%20to%20highlight%20the%0Alimitations%20of%20current%20AQA%20evaluation%20methods%20and%20motivate%20better%20metrics.%20We%0Arelease%20both%20the%20AQEval%20benchmark%20and%20the%20AURA%20metric%20to%20support%20future%0Aresearch%20in%20holistic%20AQA%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04934v1&entry.124074799=Read"},
{"title": "Pulp Motion: Framing-aware multimodal camera and human motion generation", "author": "Robin Courant and Xi Wang and David Loiseaux and Marc Christie and Vicky Kalogeiton", "abstract": "  Treating human motion and camera trajectory generation separately overlooks a\ncore principle of cinematography: the tight interplay between actor performance\nand camera work in the screen space. In this paper, we are the first to cast\nthis task as a text-conditioned joint generation, aiming to maintain consistent\non-screen framing while producing two heterogeneous, yet intrinsically linked,\nmodalities: human motion and camera trajectories. We propose a simple,\nmodel-agnostic framework that enforces multimodal coherence via an auxiliary\nmodality: the on-screen framing induced by projecting human joints onto the\ncamera. This on-screen framing provides a natural and effective bridge between\nmodalities, promoting consistency and leading to more precise joint\ndistribution. We first design a joint autoencoder that learns a shared latent\nspace, together with a lightweight linear transform from the human and camera\nlatents to a framing latent. We then introduce auxiliary sampling, which\nexploits this linear transform to steer generation toward a coherent framing\nmodality. To support this task, we also introduce the PulpMotion dataset, a\nhuman-motion and camera-trajectory dataset with rich captions, and high-quality\nhuman motions. Extensive experiments across DiT- and MAR-based architectures\nshow the generality and effectiveness of our method in generating on-frame\ncoherent human-camera motions, while also achieving gains on textual alignment\nfor both modalities. Our qualitative results yield more cinematographically\nmeaningful framings setting the new state of the art for this task. Code,\nmodels and data are available in our\n\\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project\npage}.\n", "link": "http://arxiv.org/abs/2510.05097v1", "date": "2025-10-06", "relevancy": 1.9016, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6777}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5833}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pulp%20Motion%3A%20Framing-aware%20multimodal%20camera%20and%20human%20motion%20generation&body=Title%3A%20Pulp%20Motion%3A%20Framing-aware%20multimodal%20camera%20and%20human%20motion%20generation%0AAuthor%3A%20Robin%20Courant%20and%20Xi%20Wang%20and%20David%20Loiseaux%20and%20Marc%20Christie%20and%20Vicky%20Kalogeiton%0AAbstract%3A%20%20%20Treating%20human%20motion%20and%20camera%20trajectory%20generation%20separately%20overlooks%20a%0Acore%20principle%20of%20cinematography%3A%20the%20tight%20interplay%20between%20actor%20performance%0Aand%20camera%20work%20in%20the%20screen%20space.%20In%20this%20paper%2C%20we%20are%20the%20first%20to%20cast%0Athis%20task%20as%20a%20text-conditioned%20joint%20generation%2C%20aiming%20to%20maintain%20consistent%0Aon-screen%20framing%20while%20producing%20two%20heterogeneous%2C%20yet%20intrinsically%20linked%2C%0Amodalities%3A%20human%20motion%20and%20camera%20trajectories.%20We%20propose%20a%20simple%2C%0Amodel-agnostic%20framework%20that%20enforces%20multimodal%20coherence%20via%20an%20auxiliary%0Amodality%3A%20the%20on-screen%20framing%20induced%20by%20projecting%20human%20joints%20onto%20the%0Acamera.%20This%20on-screen%20framing%20provides%20a%20natural%20and%20effective%20bridge%20between%0Amodalities%2C%20promoting%20consistency%20and%20leading%20to%20more%20precise%20joint%0Adistribution.%20We%20first%20design%20a%20joint%20autoencoder%20that%20learns%20a%20shared%20latent%0Aspace%2C%20together%20with%20a%20lightweight%20linear%20transform%20from%20the%20human%20and%20camera%0Alatents%20to%20a%20framing%20latent.%20We%20then%20introduce%20auxiliary%20sampling%2C%20which%0Aexploits%20this%20linear%20transform%20to%20steer%20generation%20toward%20a%20coherent%20framing%0Amodality.%20To%20support%20this%20task%2C%20we%20also%20introduce%20the%20PulpMotion%20dataset%2C%20a%0Ahuman-motion%20and%20camera-trajectory%20dataset%20with%20rich%20captions%2C%20and%20high-quality%0Ahuman%20motions.%20Extensive%20experiments%20across%20DiT-%20and%20MAR-based%20architectures%0Ashow%20the%20generality%20and%20effectiveness%20of%20our%20method%20in%20generating%20on-frame%0Acoherent%20human-camera%20motions%2C%20while%20also%20achieving%20gains%20on%20textual%20alignment%0Afor%20both%20modalities.%20Our%20qualitative%20results%20yield%20more%20cinematographically%0Ameaningful%20framings%20setting%20the%20new%20state%20of%20the%20art%20for%20this%20task.%20Code%2C%0Amodels%20and%20data%20are%20available%20in%20our%0A%5Chref%7Bhttps%3A//www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/%7D%7Bproject%0Apage%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPulp%2520Motion%253A%2520Framing-aware%2520multimodal%2520camera%2520and%2520human%2520motion%2520generation%26entry.906535625%3DRobin%2520Courant%2520and%2520Xi%2520Wang%2520and%2520David%2520Loiseaux%2520and%2520Marc%2520Christie%2520and%2520Vicky%2520Kalogeiton%26entry.1292438233%3D%2520%2520Treating%2520human%2520motion%2520and%2520camera%2520trajectory%2520generation%2520separately%2520overlooks%2520a%250Acore%2520principle%2520of%2520cinematography%253A%2520the%2520tight%2520interplay%2520between%2520actor%2520performance%250Aand%2520camera%2520work%2520in%2520the%2520screen%2520space.%2520In%2520this%2520paper%252C%2520we%2520are%2520the%2520first%2520to%2520cast%250Athis%2520task%2520as%2520a%2520text-conditioned%2520joint%2520generation%252C%2520aiming%2520to%2520maintain%2520consistent%250Aon-screen%2520framing%2520while%2520producing%2520two%2520heterogeneous%252C%2520yet%2520intrinsically%2520linked%252C%250Amodalities%253A%2520human%2520motion%2520and%2520camera%2520trajectories.%2520We%2520propose%2520a%2520simple%252C%250Amodel-agnostic%2520framework%2520that%2520enforces%2520multimodal%2520coherence%2520via%2520an%2520auxiliary%250Amodality%253A%2520the%2520on-screen%2520framing%2520induced%2520by%2520projecting%2520human%2520joints%2520onto%2520the%250Acamera.%2520This%2520on-screen%2520framing%2520provides%2520a%2520natural%2520and%2520effective%2520bridge%2520between%250Amodalities%252C%2520promoting%2520consistency%2520and%2520leading%2520to%2520more%2520precise%2520joint%250Adistribution.%2520We%2520first%2520design%2520a%2520joint%2520autoencoder%2520that%2520learns%2520a%2520shared%2520latent%250Aspace%252C%2520together%2520with%2520a%2520lightweight%2520linear%2520transform%2520from%2520the%2520human%2520and%2520camera%250Alatents%2520to%2520a%2520framing%2520latent.%2520We%2520then%2520introduce%2520auxiliary%2520sampling%252C%2520which%250Aexploits%2520this%2520linear%2520transform%2520to%2520steer%2520generation%2520toward%2520a%2520coherent%2520framing%250Amodality.%2520To%2520support%2520this%2520task%252C%2520we%2520also%2520introduce%2520the%2520PulpMotion%2520dataset%252C%2520a%250Ahuman-motion%2520and%2520camera-trajectory%2520dataset%2520with%2520rich%2520captions%252C%2520and%2520high-quality%250Ahuman%2520motions.%2520Extensive%2520experiments%2520across%2520DiT-%2520and%2520MAR-based%2520architectures%250Ashow%2520the%2520generality%2520and%2520effectiveness%2520of%2520our%2520method%2520in%2520generating%2520on-frame%250Acoherent%2520human-camera%2520motions%252C%2520while%2520also%2520achieving%2520gains%2520on%2520textual%2520alignment%250Afor%2520both%2520modalities.%2520Our%2520qualitative%2520results%2520yield%2520more%2520cinematographically%250Ameaningful%2520framings%2520setting%2520the%2520new%2520state%2520of%2520the%2520art%2520for%2520this%2520task.%2520Code%252C%250Amodels%2520and%2520data%2520are%2520available%2520in%2520our%250A%255Chref%257Bhttps%253A//www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/%257D%257Bproject%250Apage%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pulp%20Motion%3A%20Framing-aware%20multimodal%20camera%20and%20human%20motion%20generation&entry.906535625=Robin%20Courant%20and%20Xi%20Wang%20and%20David%20Loiseaux%20and%20Marc%20Christie%20and%20Vicky%20Kalogeiton&entry.1292438233=%20%20Treating%20human%20motion%20and%20camera%20trajectory%20generation%20separately%20overlooks%20a%0Acore%20principle%20of%20cinematography%3A%20the%20tight%20interplay%20between%20actor%20performance%0Aand%20camera%20work%20in%20the%20screen%20space.%20In%20this%20paper%2C%20we%20are%20the%20first%20to%20cast%0Athis%20task%20as%20a%20text-conditioned%20joint%20generation%2C%20aiming%20to%20maintain%20consistent%0Aon-screen%20framing%20while%20producing%20two%20heterogeneous%2C%20yet%20intrinsically%20linked%2C%0Amodalities%3A%20human%20motion%20and%20camera%20trajectories.%20We%20propose%20a%20simple%2C%0Amodel-agnostic%20framework%20that%20enforces%20multimodal%20coherence%20via%20an%20auxiliary%0Amodality%3A%20the%20on-screen%20framing%20induced%20by%20projecting%20human%20joints%20onto%20the%0Acamera.%20This%20on-screen%20framing%20provides%20a%20natural%20and%20effective%20bridge%20between%0Amodalities%2C%20promoting%20consistency%20and%20leading%20to%20more%20precise%20joint%0Adistribution.%20We%20first%20design%20a%20joint%20autoencoder%20that%20learns%20a%20shared%20latent%0Aspace%2C%20together%20with%20a%20lightweight%20linear%20transform%20from%20the%20human%20and%20camera%0Alatents%20to%20a%20framing%20latent.%20We%20then%20introduce%20auxiliary%20sampling%2C%20which%0Aexploits%20this%20linear%20transform%20to%20steer%20generation%20toward%20a%20coherent%20framing%0Amodality.%20To%20support%20this%20task%2C%20we%20also%20introduce%20the%20PulpMotion%20dataset%2C%20a%0Ahuman-motion%20and%20camera-trajectory%20dataset%20with%20rich%20captions%2C%20and%20high-quality%0Ahuman%20motions.%20Extensive%20experiments%20across%20DiT-%20and%20MAR-based%20architectures%0Ashow%20the%20generality%20and%20effectiveness%20of%20our%20method%20in%20generating%20on-frame%0Acoherent%20human-camera%20motions%2C%20while%20also%20achieving%20gains%20on%20textual%20alignment%0Afor%20both%20modalities.%20Our%20qualitative%20results%20yield%20more%20cinematographically%0Ameaningful%20framings%20setting%20the%20new%20state%20of%20the%20art%20for%20this%20task.%20Code%2C%0Amodels%20and%20data%20are%20available%20in%20our%0A%5Chref%7Bhttps%3A//www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/%7D%7Bproject%0Apage%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05097v1&entry.124074799=Read"},
{"title": "A Unified Optimization Framework for Multiclass Classification with\n  Structured Hyperplane Arrangements", "author": "V\u00edctor Blanco and Harshit Kothari and James Luedtke", "abstract": "  In this paper, we propose a new mathematical optimization model for\nmulticlass classification based on arrangements of hyperplanes. Our approach\npreserves the core support vector machine (SVM) paradigm of maximizing class\nseparation while minimizing misclassification errors, and it is computationally\nmore efficient than a previous formulation. We present a kernel-based extension\nthat allows it to construct nonlinear decision boundaries. Furthermore, we show\nhow the framework can naturally incorporate alternative geometric structures,\nincluding classification trees, $\\ell_p$-SVMs, and models with discrete feature\nselection. To address large-scale instances, we develop a dynamic clustering\nmatheuristic that leverages the proposed MIP formulation. Extensive\ncomputational experiments demonstrate the efficiency of the proposed model and\ndynamic clustering heuristic, and we report competitive classification\nperformance on both synthetic datasets and real-world benchmarks from the UCI\nMachine Learning Repository, comparing our method with state-of-the-art\nimplementations available in scikit-learn.\n", "link": "http://arxiv.org/abs/2510.05047v1", "date": "2025-10-06", "relevancy": 1.8995, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.493}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4707}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Optimization%20Framework%20for%20Multiclass%20Classification%20with%0A%20%20Structured%20Hyperplane%20Arrangements&body=Title%3A%20A%20Unified%20Optimization%20Framework%20for%20Multiclass%20Classification%20with%0A%20%20Structured%20Hyperplane%20Arrangements%0AAuthor%3A%20V%C3%ADctor%20Blanco%20and%20Harshit%20Kothari%20and%20James%20Luedtke%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20mathematical%20optimization%20model%20for%0Amulticlass%20classification%20based%20on%20arrangements%20of%20hyperplanes.%20Our%20approach%0Apreserves%20the%20core%20support%20vector%20machine%20%28SVM%29%20paradigm%20of%20maximizing%20class%0Aseparation%20while%20minimizing%20misclassification%20errors%2C%20and%20it%20is%20computationally%0Amore%20efficient%20than%20a%20previous%20formulation.%20We%20present%20a%20kernel-based%20extension%0Athat%20allows%20it%20to%20construct%20nonlinear%20decision%20boundaries.%20Furthermore%2C%20we%20show%0Ahow%20the%20framework%20can%20naturally%20incorporate%20alternative%20geometric%20structures%2C%0Aincluding%20classification%20trees%2C%20%24%5Cell_p%24-SVMs%2C%20and%20models%20with%20discrete%20feature%0Aselection.%20To%20address%20large-scale%20instances%2C%20we%20develop%20a%20dynamic%20clustering%0Amatheuristic%20that%20leverages%20the%20proposed%20MIP%20formulation.%20Extensive%0Acomputational%20experiments%20demonstrate%20the%20efficiency%20of%20the%20proposed%20model%20and%0Adynamic%20clustering%20heuristic%2C%20and%20we%20report%20competitive%20classification%0Aperformance%20on%20both%20synthetic%20datasets%20and%20real-world%20benchmarks%20from%20the%20UCI%0AMachine%20Learning%20Repository%2C%20comparing%20our%20method%20with%20state-of-the-art%0Aimplementations%20available%20in%20scikit-learn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Optimization%2520Framework%2520for%2520Multiclass%2520Classification%2520with%250A%2520%2520Structured%2520Hyperplane%2520Arrangements%26entry.906535625%3DV%25C3%25ADctor%2520Blanco%2520and%2520Harshit%2520Kothari%2520and%2520James%2520Luedtke%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520mathematical%2520optimization%2520model%2520for%250Amulticlass%2520classification%2520based%2520on%2520arrangements%2520of%2520hyperplanes.%2520Our%2520approach%250Apreserves%2520the%2520core%2520support%2520vector%2520machine%2520%2528SVM%2529%2520paradigm%2520of%2520maximizing%2520class%250Aseparation%2520while%2520minimizing%2520misclassification%2520errors%252C%2520and%2520it%2520is%2520computationally%250Amore%2520efficient%2520than%2520a%2520previous%2520formulation.%2520We%2520present%2520a%2520kernel-based%2520extension%250Athat%2520allows%2520it%2520to%2520construct%2520nonlinear%2520decision%2520boundaries.%2520Furthermore%252C%2520we%2520show%250Ahow%2520the%2520framework%2520can%2520naturally%2520incorporate%2520alternative%2520geometric%2520structures%252C%250Aincluding%2520classification%2520trees%252C%2520%2524%255Cell_p%2524-SVMs%252C%2520and%2520models%2520with%2520discrete%2520feature%250Aselection.%2520To%2520address%2520large-scale%2520instances%252C%2520we%2520develop%2520a%2520dynamic%2520clustering%250Amatheuristic%2520that%2520leverages%2520the%2520proposed%2520MIP%2520formulation.%2520Extensive%250Acomputational%2520experiments%2520demonstrate%2520the%2520efficiency%2520of%2520the%2520proposed%2520model%2520and%250Adynamic%2520clustering%2520heuristic%252C%2520and%2520we%2520report%2520competitive%2520classification%250Aperformance%2520on%2520both%2520synthetic%2520datasets%2520and%2520real-world%2520benchmarks%2520from%2520the%2520UCI%250AMachine%2520Learning%2520Repository%252C%2520comparing%2520our%2520method%2520with%2520state-of-the-art%250Aimplementations%2520available%2520in%2520scikit-learn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Optimization%20Framework%20for%20Multiclass%20Classification%20with%0A%20%20Structured%20Hyperplane%20Arrangements&entry.906535625=V%C3%ADctor%20Blanco%20and%20Harshit%20Kothari%20and%20James%20Luedtke&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20mathematical%20optimization%20model%20for%0Amulticlass%20classification%20based%20on%20arrangements%20of%20hyperplanes.%20Our%20approach%0Apreserves%20the%20core%20support%20vector%20machine%20%28SVM%29%20paradigm%20of%20maximizing%20class%0Aseparation%20while%20minimizing%20misclassification%20errors%2C%20and%20it%20is%20computationally%0Amore%20efficient%20than%20a%20previous%20formulation.%20We%20present%20a%20kernel-based%20extension%0Athat%20allows%20it%20to%20construct%20nonlinear%20decision%20boundaries.%20Furthermore%2C%20we%20show%0Ahow%20the%20framework%20can%20naturally%20incorporate%20alternative%20geometric%20structures%2C%0Aincluding%20classification%20trees%2C%20%24%5Cell_p%24-SVMs%2C%20and%20models%20with%20discrete%20feature%0Aselection.%20To%20address%20large-scale%20instances%2C%20we%20develop%20a%20dynamic%20clustering%0Amatheuristic%20that%20leverages%20the%20proposed%20MIP%20formulation.%20Extensive%0Acomputational%20experiments%20demonstrate%20the%20efficiency%20of%20the%20proposed%20model%20and%0Adynamic%20clustering%20heuristic%2C%20and%20we%20report%20competitive%20classification%0Aperformance%20on%20both%20synthetic%20datasets%20and%20real-world%20benchmarks%20from%20the%20UCI%0AMachine%20Learning%20Repository%2C%20comparing%20our%20method%20with%20state-of-the-art%0Aimplementations%20available%20in%20scikit-learn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05047v1&entry.124074799=Read"},
{"title": "MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis", "author": "Yangyang Wang and Tayo Fabusuyi", "abstract": "  This study presents a novel small-area estimation framework to enhance urban\ntransportation planning through detailed characterization of travel behavior.\nOur approach improves on the four-step travel model by employing publicly\navailable microdata files and machine learning methods to predict travel\nbehavior for a representative, synthetic population at small geographic areas.\nThis approach enables high-resolution estimation of trip generation, trip\ndistribution, mode choice, and route assignment. Validation using ACS/PUMS\nwork-commute datasets demonstrates that our framework achieves higher accuracy\ncompared to conventional approaches. The resulting granular insights enable the\ntailoring of interventions to address localized situations and support a range\nof policy applications and targeted interventions, including the optimal\nplacement of micro-fulfillment centers, effective curb-space management, and\nthe design of more inclusive transportation solutions particularly for\nvulnerable communities.\n", "link": "http://arxiv.org/abs/2510.05080v1", "date": "2025-10-06", "relevancy": 1.8955, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4918}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.49}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MICROTRIPS%3A%20MICRO-geography%20TRavel%20Intelligence%20and%20Pattern%20Synthesis&body=Title%3A%20MICROTRIPS%3A%20MICRO-geography%20TRavel%20Intelligence%20and%20Pattern%20Synthesis%0AAuthor%3A%20Yangyang%20Wang%20and%20Tayo%20Fabusuyi%0AAbstract%3A%20%20%20This%20study%20presents%20a%20novel%20small-area%20estimation%20framework%20to%20enhance%20urban%0Atransportation%20planning%20through%20detailed%20characterization%20of%20travel%20behavior.%0AOur%20approach%20improves%20on%20the%20four-step%20travel%20model%20by%20employing%20publicly%0Aavailable%20microdata%20files%20and%20machine%20learning%20methods%20to%20predict%20travel%0Abehavior%20for%20a%20representative%2C%20synthetic%20population%20at%20small%20geographic%20areas.%0AThis%20approach%20enables%20high-resolution%20estimation%20of%20trip%20generation%2C%20trip%0Adistribution%2C%20mode%20choice%2C%20and%20route%20assignment.%20Validation%20using%20ACS/PUMS%0Awork-commute%20datasets%20demonstrates%20that%20our%20framework%20achieves%20higher%20accuracy%0Acompared%20to%20conventional%20approaches.%20The%20resulting%20granular%20insights%20enable%20the%0Atailoring%20of%20interventions%20to%20address%20localized%20situations%20and%20support%20a%20range%0Aof%20policy%20applications%20and%20targeted%20interventions%2C%20including%20the%20optimal%0Aplacement%20of%20micro-fulfillment%20centers%2C%20effective%20curb-space%20management%2C%20and%0Athe%20design%20of%20more%20inclusive%20transportation%20solutions%20particularly%20for%0Avulnerable%20communities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMICROTRIPS%253A%2520MICRO-geography%2520TRavel%2520Intelligence%2520and%2520Pattern%2520Synthesis%26entry.906535625%3DYangyang%2520Wang%2520and%2520Tayo%2520Fabusuyi%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520novel%2520small-area%2520estimation%2520framework%2520to%2520enhance%2520urban%250Atransportation%2520planning%2520through%2520detailed%2520characterization%2520of%2520travel%2520behavior.%250AOur%2520approach%2520improves%2520on%2520the%2520four-step%2520travel%2520model%2520by%2520employing%2520publicly%250Aavailable%2520microdata%2520files%2520and%2520machine%2520learning%2520methods%2520to%2520predict%2520travel%250Abehavior%2520for%2520a%2520representative%252C%2520synthetic%2520population%2520at%2520small%2520geographic%2520areas.%250AThis%2520approach%2520enables%2520high-resolution%2520estimation%2520of%2520trip%2520generation%252C%2520trip%250Adistribution%252C%2520mode%2520choice%252C%2520and%2520route%2520assignment.%2520Validation%2520using%2520ACS/PUMS%250Awork-commute%2520datasets%2520demonstrates%2520that%2520our%2520framework%2520achieves%2520higher%2520accuracy%250Acompared%2520to%2520conventional%2520approaches.%2520The%2520resulting%2520granular%2520insights%2520enable%2520the%250Atailoring%2520of%2520interventions%2520to%2520address%2520localized%2520situations%2520and%2520support%2520a%2520range%250Aof%2520policy%2520applications%2520and%2520targeted%2520interventions%252C%2520including%2520the%2520optimal%250Aplacement%2520of%2520micro-fulfillment%2520centers%252C%2520effective%2520curb-space%2520management%252C%2520and%250Athe%2520design%2520of%2520more%2520inclusive%2520transportation%2520solutions%2520particularly%2520for%250Avulnerable%2520communities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MICROTRIPS%3A%20MICRO-geography%20TRavel%20Intelligence%20and%20Pattern%20Synthesis&entry.906535625=Yangyang%20Wang%20and%20Tayo%20Fabusuyi&entry.1292438233=%20%20This%20study%20presents%20a%20novel%20small-area%20estimation%20framework%20to%20enhance%20urban%0Atransportation%20planning%20through%20detailed%20characterization%20of%20travel%20behavior.%0AOur%20approach%20improves%20on%20the%20four-step%20travel%20model%20by%20employing%20publicly%0Aavailable%20microdata%20files%20and%20machine%20learning%20methods%20to%20predict%20travel%0Abehavior%20for%20a%20representative%2C%20synthetic%20population%20at%20small%20geographic%20areas.%0AThis%20approach%20enables%20high-resolution%20estimation%20of%20trip%20generation%2C%20trip%0Adistribution%2C%20mode%20choice%2C%20and%20route%20assignment.%20Validation%20using%20ACS/PUMS%0Awork-commute%20datasets%20demonstrates%20that%20our%20framework%20achieves%20higher%20accuracy%0Acompared%20to%20conventional%20approaches.%20The%20resulting%20granular%20insights%20enable%20the%0Atailoring%20of%20interventions%20to%20address%20localized%20situations%20and%20support%20a%20range%0Aof%20policy%20applications%20and%20targeted%20interventions%2C%20including%20the%20optimal%0Aplacement%20of%20micro-fulfillment%20centers%2C%20effective%20curb-space%20management%2C%20and%0Athe%20design%20of%20more%20inclusive%20transportation%20solutions%20particularly%20for%0Avulnerable%20communities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05080v1&entry.124074799=Read"},
{"title": "Power Transform Revisited: Numerically Stable, and Federated", "author": "Xuefeng Xu and Graham Cormode", "abstract": "  Power transforms are popular parametric techniques for making data more\nGaussian-like, and are widely used as preprocessing steps in statistical\nanalysis and machine learning. However, we find that direct implementations of\npower transforms suffer from severe numerical instabilities, which can lead to\nincorrect results or even crashes. In this paper, we provide a comprehensive\nanalysis of the sources of these instabilities and propose effective remedies.\nWe further extend power transforms to the federated learning setting,\naddressing both numerical and distributional challenges that arise in this\ncontext. Experiments on real-world datasets demonstrate that our methods are\nboth effective and robust, substantially improving stability compared to\nexisting approaches.\n", "link": "http://arxiv.org/abs/2510.04995v1", "date": "2025-10-06", "relevancy": 1.8856, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5238}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4612}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Power%20Transform%20Revisited%3A%20Numerically%20Stable%2C%20and%20Federated&body=Title%3A%20Power%20Transform%20Revisited%3A%20Numerically%20Stable%2C%20and%20Federated%0AAuthor%3A%20Xuefeng%20Xu%20and%20Graham%20Cormode%0AAbstract%3A%20%20%20Power%20transforms%20are%20popular%20parametric%20techniques%20for%20making%20data%20more%0AGaussian-like%2C%20and%20are%20widely%20used%20as%20preprocessing%20steps%20in%20statistical%0Aanalysis%20and%20machine%20learning.%20However%2C%20we%20find%20that%20direct%20implementations%20of%0Apower%20transforms%20suffer%20from%20severe%20numerical%20instabilities%2C%20which%20can%20lead%20to%0Aincorrect%20results%20or%20even%20crashes.%20In%20this%20paper%2C%20we%20provide%20a%20comprehensive%0Aanalysis%20of%20the%20sources%20of%20these%20instabilities%20and%20propose%20effective%20remedies.%0AWe%20further%20extend%20power%20transforms%20to%20the%20federated%20learning%20setting%2C%0Aaddressing%20both%20numerical%20and%20distributional%20challenges%20that%20arise%20in%20this%0Acontext.%20Experiments%20on%20real-world%20datasets%20demonstrate%20that%20our%20methods%20are%0Aboth%20effective%20and%20robust%2C%20substantially%20improving%20stability%20compared%20to%0Aexisting%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPower%2520Transform%2520Revisited%253A%2520Numerically%2520Stable%252C%2520and%2520Federated%26entry.906535625%3DXuefeng%2520Xu%2520and%2520Graham%2520Cormode%26entry.1292438233%3D%2520%2520Power%2520transforms%2520are%2520popular%2520parametric%2520techniques%2520for%2520making%2520data%2520more%250AGaussian-like%252C%2520and%2520are%2520widely%2520used%2520as%2520preprocessing%2520steps%2520in%2520statistical%250Aanalysis%2520and%2520machine%2520learning.%2520However%252C%2520we%2520find%2520that%2520direct%2520implementations%2520of%250Apower%2520transforms%2520suffer%2520from%2520severe%2520numerical%2520instabilities%252C%2520which%2520can%2520lead%2520to%250Aincorrect%2520results%2520or%2520even%2520crashes.%2520In%2520this%2520paper%252C%2520we%2520provide%2520a%2520comprehensive%250Aanalysis%2520of%2520the%2520sources%2520of%2520these%2520instabilities%2520and%2520propose%2520effective%2520remedies.%250AWe%2520further%2520extend%2520power%2520transforms%2520to%2520the%2520federated%2520learning%2520setting%252C%250Aaddressing%2520both%2520numerical%2520and%2520distributional%2520challenges%2520that%2520arise%2520in%2520this%250Acontext.%2520Experiments%2520on%2520real-world%2520datasets%2520demonstrate%2520that%2520our%2520methods%2520are%250Aboth%2520effective%2520and%2520robust%252C%2520substantially%2520improving%2520stability%2520compared%2520to%250Aexisting%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Power%20Transform%20Revisited%3A%20Numerically%20Stable%2C%20and%20Federated&entry.906535625=Xuefeng%20Xu%20and%20Graham%20Cormode&entry.1292438233=%20%20Power%20transforms%20are%20popular%20parametric%20techniques%20for%20making%20data%20more%0AGaussian-like%2C%20and%20are%20widely%20used%20as%20preprocessing%20steps%20in%20statistical%0Aanalysis%20and%20machine%20learning.%20However%2C%20we%20find%20that%20direct%20implementations%20of%0Apower%20transforms%20suffer%20from%20severe%20numerical%20instabilities%2C%20which%20can%20lead%20to%0Aincorrect%20results%20or%20even%20crashes.%20In%20this%20paper%2C%20we%20provide%20a%20comprehensive%0Aanalysis%20of%20the%20sources%20of%20these%20instabilities%20and%20propose%20effective%20remedies.%0AWe%20further%20extend%20power%20transforms%20to%20the%20federated%20learning%20setting%2C%0Aaddressing%20both%20numerical%20and%20distributional%20challenges%20that%20arise%20in%20this%0Acontext.%20Experiments%20on%20real-world%20datasets%20demonstrate%20that%20our%20methods%20are%0Aboth%20effective%20and%20robust%2C%20substantially%20improving%20stability%20compared%20to%0Aexisting%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04995v1&entry.124074799=Read"},
{"title": "Embracing Discrete Search: A Reasonable Approach to Causal Structure\n  Learning", "author": "Marcel Wien\u00f6bst and Leonard Henckel and Sebastian Weichwald", "abstract": "  We present FLOP (Fast Learning of Order and Parents), a score-based causal\ndiscovery algorithm for linear models. It pairs fast parent selection with\niterative Cholesky-based score updates, cutting run-times over prior\nalgorithms. This makes it feasible to fully embrace discrete search, enabling\niterated local search with principled order initialization to find graphs with\nscores at or close to the global optimum. The resulting structures are highly\naccurate across benchmarks, with near-perfect recovery in standard settings.\nThis performance calls for revisiting discrete search over graphs as a\nreasonable approach to causal discovery.\n", "link": "http://arxiv.org/abs/2510.04970v1", "date": "2025-10-06", "relevancy": 1.8715, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4782}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embracing%20Discrete%20Search%3A%20A%20Reasonable%20Approach%20to%20Causal%20Structure%0A%20%20Learning&body=Title%3A%20Embracing%20Discrete%20Search%3A%20A%20Reasonable%20Approach%20to%20Causal%20Structure%0A%20%20Learning%0AAuthor%3A%20Marcel%20Wien%C3%B6bst%20and%20Leonard%20Henckel%20and%20Sebastian%20Weichwald%0AAbstract%3A%20%20%20We%20present%20FLOP%20%28Fast%20Learning%20of%20Order%20and%20Parents%29%2C%20a%20score-based%20causal%0Adiscovery%20algorithm%20for%20linear%20models.%20It%20pairs%20fast%20parent%20selection%20with%0Aiterative%20Cholesky-based%20score%20updates%2C%20cutting%20run-times%20over%20prior%0Aalgorithms.%20This%20makes%20it%20feasible%20to%20fully%20embrace%20discrete%20search%2C%20enabling%0Aiterated%20local%20search%20with%20principled%20order%20initialization%20to%20find%20graphs%20with%0Ascores%20at%20or%20close%20to%20the%20global%20optimum.%20The%20resulting%20structures%20are%20highly%0Aaccurate%20across%20benchmarks%2C%20with%20near-perfect%20recovery%20in%20standard%20settings.%0AThis%20performance%20calls%20for%20revisiting%20discrete%20search%20over%20graphs%20as%20a%0Areasonable%20approach%20to%20causal%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbracing%2520Discrete%2520Search%253A%2520A%2520Reasonable%2520Approach%2520to%2520Causal%2520Structure%250A%2520%2520Learning%26entry.906535625%3DMarcel%2520Wien%25C3%25B6bst%2520and%2520Leonard%2520Henckel%2520and%2520Sebastian%2520Weichwald%26entry.1292438233%3D%2520%2520We%2520present%2520FLOP%2520%2528Fast%2520Learning%2520of%2520Order%2520and%2520Parents%2529%252C%2520a%2520score-based%2520causal%250Adiscovery%2520algorithm%2520for%2520linear%2520models.%2520It%2520pairs%2520fast%2520parent%2520selection%2520with%250Aiterative%2520Cholesky-based%2520score%2520updates%252C%2520cutting%2520run-times%2520over%2520prior%250Aalgorithms.%2520This%2520makes%2520it%2520feasible%2520to%2520fully%2520embrace%2520discrete%2520search%252C%2520enabling%250Aiterated%2520local%2520search%2520with%2520principled%2520order%2520initialization%2520to%2520find%2520graphs%2520with%250Ascores%2520at%2520or%2520close%2520to%2520the%2520global%2520optimum.%2520The%2520resulting%2520structures%2520are%2520highly%250Aaccurate%2520across%2520benchmarks%252C%2520with%2520near-perfect%2520recovery%2520in%2520standard%2520settings.%250AThis%2520performance%2520calls%2520for%2520revisiting%2520discrete%2520search%2520over%2520graphs%2520as%2520a%250Areasonable%2520approach%2520to%2520causal%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embracing%20Discrete%20Search%3A%20A%20Reasonable%20Approach%20to%20Causal%20Structure%0A%20%20Learning&entry.906535625=Marcel%20Wien%C3%B6bst%20and%20Leonard%20Henckel%20and%20Sebastian%20Weichwald&entry.1292438233=%20%20We%20present%20FLOP%20%28Fast%20Learning%20of%20Order%20and%20Parents%29%2C%20a%20score-based%20causal%0Adiscovery%20algorithm%20for%20linear%20models.%20It%20pairs%20fast%20parent%20selection%20with%0Aiterative%20Cholesky-based%20score%20updates%2C%20cutting%20run-times%20over%20prior%0Aalgorithms.%20This%20makes%20it%20feasible%20to%20fully%20embrace%20discrete%20search%2C%20enabling%0Aiterated%20local%20search%20with%20principled%20order%20initialization%20to%20find%20graphs%20with%0Ascores%20at%20or%20close%20to%20the%20global%20optimum.%20The%20resulting%20structures%20are%20highly%0Aaccurate%20across%20benchmarks%2C%20with%20near-perfect%20recovery%20in%20standard%20settings.%0AThis%20performance%20calls%20for%20revisiting%20discrete%20search%20over%20graphs%20as%20a%0Areasonable%20approach%20to%20causal%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04970v1&entry.124074799=Read"},
{"title": "Cooperative Decentralized Backdoor Attacks on Vertical Federated\n  Learning", "author": "Seohyun Lee and Wenzhi Fang and Anindya Bijoy Das and Seyyedali Hosseinalipour and David J. Love and Christopher G. Brinton", "abstract": "  Federated learning (FL) is vulnerable to backdoor attacks, where adversaries\nalter model behavior on target classification labels by embedding triggers into\ndata samples. While these attacks have received considerable attention in\nhorizontal FL, they are less understood for vertical FL (VFL), where devices\nhold different features of the samples, and only the server holds the labels.\nIn this work, we propose a novel backdoor attack on VFL which (i) does not rely\non gradient information from the server and (ii) considers potential collusion\namong multiple adversaries for sample selection and trigger embedding. Our\nlabel inference model augments variational autoencoders with metric learning,\nwhich adversaries can train locally. A consensus process over the adversary\ngraph topology determines which datapoints to poison. We further propose\nmethods for trigger splitting across the adversaries, with an intensity-based\nimplantation scheme skewing the server towards the trigger. Our convergence\nanalysis reveals the impact of backdoor perturbations on VFL indicated by a\nstationarity gap for the trained model, which we verify empirically as well. We\nconduct experiments comparing our attack with recent backdoor VFL approaches,\nfinding that ours obtains significantly higher success rates for the same main\ntask performance despite not using server information. Additionally, our\nresults verify the impact of collusion on attack performance.\n", "link": "http://arxiv.org/abs/2501.09320v2", "date": "2025-10-06", "relevancy": 1.8594, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4684}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4654}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooperative%20Decentralized%20Backdoor%20Attacks%20on%20Vertical%20Federated%0A%20%20Learning&body=Title%3A%20Cooperative%20Decentralized%20Backdoor%20Attacks%20on%20Vertical%20Federated%0A%20%20Learning%0AAuthor%3A%20Seohyun%20Lee%20and%20Wenzhi%20Fang%20and%20Anindya%20Bijoy%20Das%20and%20Seyyedali%20Hosseinalipour%20and%20David%20J.%20Love%20and%20Christopher%20G.%20Brinton%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20vulnerable%20to%20backdoor%20attacks%2C%20where%20adversaries%0Aalter%20model%20behavior%20on%20target%20classification%20labels%20by%20embedding%20triggers%20into%0Adata%20samples.%20While%20these%20attacks%20have%20received%20considerable%20attention%20in%0Ahorizontal%20FL%2C%20they%20are%20less%20understood%20for%20vertical%20FL%20%28VFL%29%2C%20where%20devices%0Ahold%20different%20features%20of%20the%20samples%2C%20and%20only%20the%20server%20holds%20the%20labels.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20backdoor%20attack%20on%20VFL%20which%20%28i%29%20does%20not%20rely%0Aon%20gradient%20information%20from%20the%20server%20and%20%28ii%29%20considers%20potential%20collusion%0Aamong%20multiple%20adversaries%20for%20sample%20selection%20and%20trigger%20embedding.%20Our%0Alabel%20inference%20model%20augments%20variational%20autoencoders%20with%20metric%20learning%2C%0Awhich%20adversaries%20can%20train%20locally.%20A%20consensus%20process%20over%20the%20adversary%0Agraph%20topology%20determines%20which%20datapoints%20to%20poison.%20We%20further%20propose%0Amethods%20for%20trigger%20splitting%20across%20the%20adversaries%2C%20with%20an%20intensity-based%0Aimplantation%20scheme%20skewing%20the%20server%20towards%20the%20trigger.%20Our%20convergence%0Aanalysis%20reveals%20the%20impact%20of%20backdoor%20perturbations%20on%20VFL%20indicated%20by%20a%0Astationarity%20gap%20for%20the%20trained%20model%2C%20which%20we%20verify%20empirically%20as%20well.%20We%0Aconduct%20experiments%20comparing%20our%20attack%20with%20recent%20backdoor%20VFL%20approaches%2C%0Afinding%20that%20ours%20obtains%20significantly%20higher%20success%20rates%20for%20the%20same%20main%0Atask%20performance%20despite%20not%20using%20server%20information.%20Additionally%2C%20our%0Aresults%20verify%20the%20impact%20of%20collusion%20on%20attack%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09320v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooperative%2520Decentralized%2520Backdoor%2520Attacks%2520on%2520Vertical%2520Federated%250A%2520%2520Learning%26entry.906535625%3DSeohyun%2520Lee%2520and%2520Wenzhi%2520Fang%2520and%2520Anindya%2520Bijoy%2520Das%2520and%2520Seyyedali%2520Hosseinalipour%2520and%2520David%2520J.%2520Love%2520and%2520Christopher%2520G.%2520Brinton%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520vulnerable%2520to%2520backdoor%2520attacks%252C%2520where%2520adversaries%250Aalter%2520model%2520behavior%2520on%2520target%2520classification%2520labels%2520by%2520embedding%2520triggers%2520into%250Adata%2520samples.%2520While%2520these%2520attacks%2520have%2520received%2520considerable%2520attention%2520in%250Ahorizontal%2520FL%252C%2520they%2520are%2520less%2520understood%2520for%2520vertical%2520FL%2520%2528VFL%2529%252C%2520where%2520devices%250Ahold%2520different%2520features%2520of%2520the%2520samples%252C%2520and%2520only%2520the%2520server%2520holds%2520the%2520labels.%250AIn%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520backdoor%2520attack%2520on%2520VFL%2520which%2520%2528i%2529%2520does%2520not%2520rely%250Aon%2520gradient%2520information%2520from%2520the%2520server%2520and%2520%2528ii%2529%2520considers%2520potential%2520collusion%250Aamong%2520multiple%2520adversaries%2520for%2520sample%2520selection%2520and%2520trigger%2520embedding.%2520Our%250Alabel%2520inference%2520model%2520augments%2520variational%2520autoencoders%2520with%2520metric%2520learning%252C%250Awhich%2520adversaries%2520can%2520train%2520locally.%2520A%2520consensus%2520process%2520over%2520the%2520adversary%250Agraph%2520topology%2520determines%2520which%2520datapoints%2520to%2520poison.%2520We%2520further%2520propose%250Amethods%2520for%2520trigger%2520splitting%2520across%2520the%2520adversaries%252C%2520with%2520an%2520intensity-based%250Aimplantation%2520scheme%2520skewing%2520the%2520server%2520towards%2520the%2520trigger.%2520Our%2520convergence%250Aanalysis%2520reveals%2520the%2520impact%2520of%2520backdoor%2520perturbations%2520on%2520VFL%2520indicated%2520by%2520a%250Astationarity%2520gap%2520for%2520the%2520trained%2520model%252C%2520which%2520we%2520verify%2520empirically%2520as%2520well.%2520We%250Aconduct%2520experiments%2520comparing%2520our%2520attack%2520with%2520recent%2520backdoor%2520VFL%2520approaches%252C%250Afinding%2520that%2520ours%2520obtains%2520significantly%2520higher%2520success%2520rates%2520for%2520the%2520same%2520main%250Atask%2520performance%2520despite%2520not%2520using%2520server%2520information.%2520Additionally%252C%2520our%250Aresults%2520verify%2520the%2520impact%2520of%2520collusion%2520on%2520attack%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09320v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperative%20Decentralized%20Backdoor%20Attacks%20on%20Vertical%20Federated%0A%20%20Learning&entry.906535625=Seohyun%20Lee%20and%20Wenzhi%20Fang%20and%20Anindya%20Bijoy%20Das%20and%20Seyyedali%20Hosseinalipour%20and%20David%20J.%20Love%20and%20Christopher%20G.%20Brinton&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20vulnerable%20to%20backdoor%20attacks%2C%20where%20adversaries%0Aalter%20model%20behavior%20on%20target%20classification%20labels%20by%20embedding%20triggers%20into%0Adata%20samples.%20While%20these%20attacks%20have%20received%20considerable%20attention%20in%0Ahorizontal%20FL%2C%20they%20are%20less%20understood%20for%20vertical%20FL%20%28VFL%29%2C%20where%20devices%0Ahold%20different%20features%20of%20the%20samples%2C%20and%20only%20the%20server%20holds%20the%20labels.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20backdoor%20attack%20on%20VFL%20which%20%28i%29%20does%20not%20rely%0Aon%20gradient%20information%20from%20the%20server%20and%20%28ii%29%20considers%20potential%20collusion%0Aamong%20multiple%20adversaries%20for%20sample%20selection%20and%20trigger%20embedding.%20Our%0Alabel%20inference%20model%20augments%20variational%20autoencoders%20with%20metric%20learning%2C%0Awhich%20adversaries%20can%20train%20locally.%20A%20consensus%20process%20over%20the%20adversary%0Agraph%20topology%20determines%20which%20datapoints%20to%20poison.%20We%20further%20propose%0Amethods%20for%20trigger%20splitting%20across%20the%20adversaries%2C%20with%20an%20intensity-based%0Aimplantation%20scheme%20skewing%20the%20server%20towards%20the%20trigger.%20Our%20convergence%0Aanalysis%20reveals%20the%20impact%20of%20backdoor%20perturbations%20on%20VFL%20indicated%20by%20a%0Astationarity%20gap%20for%20the%20trained%20model%2C%20which%20we%20verify%20empirically%20as%20well.%20We%0Aconduct%20experiments%20comparing%20our%20attack%20with%20recent%20backdoor%20VFL%20approaches%2C%0Afinding%20that%20ours%20obtains%20significantly%20higher%20success%20rates%20for%20the%20same%20main%0Atask%20performance%20despite%20not%20using%20server%20information.%20Additionally%2C%20our%0Aresults%20verify%20the%20impact%20of%20collusion%20on%20attack%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09320v2&entry.124074799=Read"},
{"title": "ResMimic: From General Motion Tracking to Humanoid Whole-body\n  Loco-Manipulation via Residual Learning", "author": "Siheng Zhao and Yanjie Ze and Yue Wang and C. Karen Liu and Pieter Abbeel and Guanya Shi and Rocky Duan", "abstract": "  Humanoid whole-body loco-manipulation promises transformative capabilities\nfor daily service and warehouse tasks. While recent advances in general motion\ntracking (GMT) have enabled humanoids to reproduce diverse human motions, these\npolicies lack the precision and object awareness required for\nloco-manipulation. To this end, we introduce ResMimic, a two-stage residual\nlearning framework for precise and expressive humanoid control from human\nmotion data. First, a GMT policy, trained on large-scale human-only motion,\nserves as a task-agnostic base for generating human-like whole-body movements.\nAn efficient but precise residual policy is then learned to refine the GMT\noutputs to improve locomotion and incorporate object interaction. To further\nfacilitate efficient training, we design (i) a point-cloud-based object\ntracking reward for smoother optimization, (ii) a contact reward that\nencourages accurate humanoid body-object interactions, and (iii) a\ncurriculum-based virtual object controller to stabilize early training. We\nevaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results\nshow substantial gains in task success, training efficiency, and robustness\nover strong baselines. Videos are available at https://resmimic.github.io/ .\n", "link": "http://arxiv.org/abs/2510.05070v1", "date": "2025-10-06", "relevancy": 1.8406, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6396}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5891}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ResMimic%3A%20From%20General%20Motion%20Tracking%20to%20Humanoid%20Whole-body%0A%20%20Loco-Manipulation%20via%20Residual%20Learning&body=Title%3A%20ResMimic%3A%20From%20General%20Motion%20Tracking%20to%20Humanoid%20Whole-body%0A%20%20Loco-Manipulation%20via%20Residual%20Learning%0AAuthor%3A%20Siheng%20Zhao%20and%20Yanjie%20Ze%20and%20Yue%20Wang%20and%20C.%20Karen%20Liu%20and%20Pieter%20Abbeel%20and%20Guanya%20Shi%20and%20Rocky%20Duan%0AAbstract%3A%20%20%20Humanoid%20whole-body%20loco-manipulation%20promises%20transformative%20capabilities%0Afor%20daily%20service%20and%20warehouse%20tasks.%20While%20recent%20advances%20in%20general%20motion%0Atracking%20%28GMT%29%20have%20enabled%20humanoids%20to%20reproduce%20diverse%20human%20motions%2C%20these%0Apolicies%20lack%20the%20precision%20and%20object%20awareness%20required%20for%0Aloco-manipulation.%20To%20this%20end%2C%20we%20introduce%20ResMimic%2C%20a%20two-stage%20residual%0Alearning%20framework%20for%20precise%20and%20expressive%20humanoid%20control%20from%20human%0Amotion%20data.%20First%2C%20a%20GMT%20policy%2C%20trained%20on%20large-scale%20human-only%20motion%2C%0Aserves%20as%20a%20task-agnostic%20base%20for%20generating%20human-like%20whole-body%20movements.%0AAn%20efficient%20but%20precise%20residual%20policy%20is%20then%20learned%20to%20refine%20the%20GMT%0Aoutputs%20to%20improve%20locomotion%20and%20incorporate%20object%20interaction.%20To%20further%0Afacilitate%20efficient%20training%2C%20we%20design%20%28i%29%20a%20point-cloud-based%20object%0Atracking%20reward%20for%20smoother%20optimization%2C%20%28ii%29%20a%20contact%20reward%20that%0Aencourages%20accurate%20humanoid%20body-object%20interactions%2C%20and%20%28iii%29%20a%0Acurriculum-based%20virtual%20object%20controller%20to%20stabilize%20early%20training.%20We%0Aevaluate%20ResMimic%20in%20both%20simulation%20and%20on%20a%20real%20Unitree%20G1%20humanoid.%20Results%0Ashow%20substantial%20gains%20in%20task%20success%2C%20training%20efficiency%2C%20and%20robustness%0Aover%20strong%20baselines.%20Videos%20are%20available%20at%20https%3A//resmimic.github.io/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResMimic%253A%2520From%2520General%2520Motion%2520Tracking%2520to%2520Humanoid%2520Whole-body%250A%2520%2520Loco-Manipulation%2520via%2520Residual%2520Learning%26entry.906535625%3DSiheng%2520Zhao%2520and%2520Yanjie%2520Ze%2520and%2520Yue%2520Wang%2520and%2520C.%2520Karen%2520Liu%2520and%2520Pieter%2520Abbeel%2520and%2520Guanya%2520Shi%2520and%2520Rocky%2520Duan%26entry.1292438233%3D%2520%2520Humanoid%2520whole-body%2520loco-manipulation%2520promises%2520transformative%2520capabilities%250Afor%2520daily%2520service%2520and%2520warehouse%2520tasks.%2520While%2520recent%2520advances%2520in%2520general%2520motion%250Atracking%2520%2528GMT%2529%2520have%2520enabled%2520humanoids%2520to%2520reproduce%2520diverse%2520human%2520motions%252C%2520these%250Apolicies%2520lack%2520the%2520precision%2520and%2520object%2520awareness%2520required%2520for%250Aloco-manipulation.%2520To%2520this%2520end%252C%2520we%2520introduce%2520ResMimic%252C%2520a%2520two-stage%2520residual%250Alearning%2520framework%2520for%2520precise%2520and%2520expressive%2520humanoid%2520control%2520from%2520human%250Amotion%2520data.%2520First%252C%2520a%2520GMT%2520policy%252C%2520trained%2520on%2520large-scale%2520human-only%2520motion%252C%250Aserves%2520as%2520a%2520task-agnostic%2520base%2520for%2520generating%2520human-like%2520whole-body%2520movements.%250AAn%2520efficient%2520but%2520precise%2520residual%2520policy%2520is%2520then%2520learned%2520to%2520refine%2520the%2520GMT%250Aoutputs%2520to%2520improve%2520locomotion%2520and%2520incorporate%2520object%2520interaction.%2520To%2520further%250Afacilitate%2520efficient%2520training%252C%2520we%2520design%2520%2528i%2529%2520a%2520point-cloud-based%2520object%250Atracking%2520reward%2520for%2520smoother%2520optimization%252C%2520%2528ii%2529%2520a%2520contact%2520reward%2520that%250Aencourages%2520accurate%2520humanoid%2520body-object%2520interactions%252C%2520and%2520%2528iii%2529%2520a%250Acurriculum-based%2520virtual%2520object%2520controller%2520to%2520stabilize%2520early%2520training.%2520We%250Aevaluate%2520ResMimic%2520in%2520both%2520simulation%2520and%2520on%2520a%2520real%2520Unitree%2520G1%2520humanoid.%2520Results%250Ashow%2520substantial%2520gains%2520in%2520task%2520success%252C%2520training%2520efficiency%252C%2520and%2520robustness%250Aover%2520strong%2520baselines.%2520Videos%2520are%2520available%2520at%2520https%253A//resmimic.github.io/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResMimic%3A%20From%20General%20Motion%20Tracking%20to%20Humanoid%20Whole-body%0A%20%20Loco-Manipulation%20via%20Residual%20Learning&entry.906535625=Siheng%20Zhao%20and%20Yanjie%20Ze%20and%20Yue%20Wang%20and%20C.%20Karen%20Liu%20and%20Pieter%20Abbeel%20and%20Guanya%20Shi%20and%20Rocky%20Duan&entry.1292438233=%20%20Humanoid%20whole-body%20loco-manipulation%20promises%20transformative%20capabilities%0Afor%20daily%20service%20and%20warehouse%20tasks.%20While%20recent%20advances%20in%20general%20motion%0Atracking%20%28GMT%29%20have%20enabled%20humanoids%20to%20reproduce%20diverse%20human%20motions%2C%20these%0Apolicies%20lack%20the%20precision%20and%20object%20awareness%20required%20for%0Aloco-manipulation.%20To%20this%20end%2C%20we%20introduce%20ResMimic%2C%20a%20two-stage%20residual%0Alearning%20framework%20for%20precise%20and%20expressive%20humanoid%20control%20from%20human%0Amotion%20data.%20First%2C%20a%20GMT%20policy%2C%20trained%20on%20large-scale%20human-only%20motion%2C%0Aserves%20as%20a%20task-agnostic%20base%20for%20generating%20human-like%20whole-body%20movements.%0AAn%20efficient%20but%20precise%20residual%20policy%20is%20then%20learned%20to%20refine%20the%20GMT%0Aoutputs%20to%20improve%20locomotion%20and%20incorporate%20object%20interaction.%20To%20further%0Afacilitate%20efficient%20training%2C%20we%20design%20%28i%29%20a%20point-cloud-based%20object%0Atracking%20reward%20for%20smoother%20optimization%2C%20%28ii%29%20a%20contact%20reward%20that%0Aencourages%20accurate%20humanoid%20body-object%20interactions%2C%20and%20%28iii%29%20a%0Acurriculum-based%20virtual%20object%20controller%20to%20stabilize%20early%20training.%20We%0Aevaluate%20ResMimic%20in%20both%20simulation%20and%20on%20a%20real%20Unitree%20G1%20humanoid.%20Results%0Ashow%20substantial%20gains%20in%20task%20success%2C%20training%20efficiency%2C%20and%20robustness%0Aover%20strong%20baselines.%20Videos%20are%20available%20at%20https%3A//resmimic.github.io/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05070v1&entry.124074799=Read"},
{"title": "Glocal Information Bottleneck for Time Series Imputation", "author": "Jie Yang and Kexin Zhang and Guibin Zhang and Philip S. Yu and Kaize Ding", "abstract": "  Time Series Imputation (TSI), which aims to recover missing values in\ntemporal data, remains a fundamental challenge due to the complex and often\nhigh-rate missingness in real-world scenarios. Existing models typically\noptimize the point-wise reconstruction loss, focusing on recovering numerical\nvalues (local information). However, we observe that under high missing rates,\nthese models still perform well in the training phase yet produce poor\nimputations and distorted latent representation distributions (global\ninformation) in the inference phase. This reveals a critical optimization\ndilemma: current objectives lack global guidance, leading models to overfit\nlocal noise and fail to capture global information of the data. To address this\nissue, we propose a new training paradigm, Glocal Information Bottleneck\n(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework\nby introducing a Global Alignment loss, derived from a tractable mutual\ninformation approximation. This loss aligns the latent representations of\nmasked inputs with those of their originally observed counterparts. It helps\nthe model retain global structure and local details while suppressing noise\ncaused by missing values, giving rise to better generalization under high\nmissingness. Extensive experiments on nine datasets confirm that Glocal-IB\nleads to consistently improved performance and aligned latent representations\nunder missingness. Our code implementation is available in\nhttps://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.\n", "link": "http://arxiv.org/abs/2510.04910v1", "date": "2025-10-06", "relevancy": 1.8341, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4696}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4512}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Glocal%20Information%20Bottleneck%20for%20Time%20Series%20Imputation&body=Title%3A%20Glocal%20Information%20Bottleneck%20for%20Time%20Series%20Imputation%0AAuthor%3A%20Jie%20Yang%20and%20Kexin%20Zhang%20and%20Guibin%20Zhang%20and%20Philip%20S.%20Yu%20and%20Kaize%20Ding%0AAbstract%3A%20%20%20Time%20Series%20Imputation%20%28TSI%29%2C%20which%20aims%20to%20recover%20missing%20values%20in%0Atemporal%20data%2C%20remains%20a%20fundamental%20challenge%20due%20to%20the%20complex%20and%20often%0Ahigh-rate%20missingness%20in%20real-world%20scenarios.%20Existing%20models%20typically%0Aoptimize%20the%20point-wise%20reconstruction%20loss%2C%20focusing%20on%20recovering%20numerical%0Avalues%20%28local%20information%29.%20However%2C%20we%20observe%20that%20under%20high%20missing%20rates%2C%0Athese%20models%20still%20perform%20well%20in%20the%20training%20phase%20yet%20produce%20poor%0Aimputations%20and%20distorted%20latent%20representation%20distributions%20%28global%0Ainformation%29%20in%20the%20inference%20phase.%20This%20reveals%20a%20critical%20optimization%0Adilemma%3A%20current%20objectives%20lack%20global%20guidance%2C%20leading%20models%20to%20overfit%0Alocal%20noise%20and%20fail%20to%20capture%20global%20information%20of%20the%20data.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20new%20training%20paradigm%2C%20Glocal%20Information%20Bottleneck%0A%28Glocal-IB%29.%20Glocal-IB%20is%20model-agnostic%20and%20extends%20the%20standard%20IB%20framework%0Aby%20introducing%20a%20Global%20Alignment%20loss%2C%20derived%20from%20a%20tractable%20mutual%0Ainformation%20approximation.%20This%20loss%20aligns%20the%20latent%20representations%20of%0Amasked%20inputs%20with%20those%20of%20their%20originally%20observed%20counterparts.%20It%20helps%0Athe%20model%20retain%20global%20structure%20and%20local%20details%20while%20suppressing%20noise%0Acaused%20by%20missing%20values%2C%20giving%20rise%20to%20better%20generalization%20under%20high%0Amissingness.%20Extensive%20experiments%20on%20nine%20datasets%20confirm%20that%20Glocal-IB%0Aleads%20to%20consistently%20improved%20performance%20and%20aligned%20latent%20representations%0Aunder%20missingness.%20Our%20code%20implementation%20is%20available%20in%0Ahttps%3A//github.com/Muyiiiii/NeurIPS-25-Glocal-IB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlocal%2520Information%2520Bottleneck%2520for%2520Time%2520Series%2520Imputation%26entry.906535625%3DJie%2520Yang%2520and%2520Kexin%2520Zhang%2520and%2520Guibin%2520Zhang%2520and%2520Philip%2520S.%2520Yu%2520and%2520Kaize%2520Ding%26entry.1292438233%3D%2520%2520Time%2520Series%2520Imputation%2520%2528TSI%2529%252C%2520which%2520aims%2520to%2520recover%2520missing%2520values%2520in%250Atemporal%2520data%252C%2520remains%2520a%2520fundamental%2520challenge%2520due%2520to%2520the%2520complex%2520and%2520often%250Ahigh-rate%2520missingness%2520in%2520real-world%2520scenarios.%2520Existing%2520models%2520typically%250Aoptimize%2520the%2520point-wise%2520reconstruction%2520loss%252C%2520focusing%2520on%2520recovering%2520numerical%250Avalues%2520%2528local%2520information%2529.%2520However%252C%2520we%2520observe%2520that%2520under%2520high%2520missing%2520rates%252C%250Athese%2520models%2520still%2520perform%2520well%2520in%2520the%2520training%2520phase%2520yet%2520produce%2520poor%250Aimputations%2520and%2520distorted%2520latent%2520representation%2520distributions%2520%2528global%250Ainformation%2529%2520in%2520the%2520inference%2520phase.%2520This%2520reveals%2520a%2520critical%2520optimization%250Adilemma%253A%2520current%2520objectives%2520lack%2520global%2520guidance%252C%2520leading%2520models%2520to%2520overfit%250Alocal%2520noise%2520and%2520fail%2520to%2520capture%2520global%2520information%2520of%2520the%2520data.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520a%2520new%2520training%2520paradigm%252C%2520Glocal%2520Information%2520Bottleneck%250A%2528Glocal-IB%2529.%2520Glocal-IB%2520is%2520model-agnostic%2520and%2520extends%2520the%2520standard%2520IB%2520framework%250Aby%2520introducing%2520a%2520Global%2520Alignment%2520loss%252C%2520derived%2520from%2520a%2520tractable%2520mutual%250Ainformation%2520approximation.%2520This%2520loss%2520aligns%2520the%2520latent%2520representations%2520of%250Amasked%2520inputs%2520with%2520those%2520of%2520their%2520originally%2520observed%2520counterparts.%2520It%2520helps%250Athe%2520model%2520retain%2520global%2520structure%2520and%2520local%2520details%2520while%2520suppressing%2520noise%250Acaused%2520by%2520missing%2520values%252C%2520giving%2520rise%2520to%2520better%2520generalization%2520under%2520high%250Amissingness.%2520Extensive%2520experiments%2520on%2520nine%2520datasets%2520confirm%2520that%2520Glocal-IB%250Aleads%2520to%2520consistently%2520improved%2520performance%2520and%2520aligned%2520latent%2520representations%250Aunder%2520missingness.%2520Our%2520code%2520implementation%2520is%2520available%2520in%250Ahttps%253A//github.com/Muyiiiii/NeurIPS-25-Glocal-IB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Glocal%20Information%20Bottleneck%20for%20Time%20Series%20Imputation&entry.906535625=Jie%20Yang%20and%20Kexin%20Zhang%20and%20Guibin%20Zhang%20and%20Philip%20S.%20Yu%20and%20Kaize%20Ding&entry.1292438233=%20%20Time%20Series%20Imputation%20%28TSI%29%2C%20which%20aims%20to%20recover%20missing%20values%20in%0Atemporal%20data%2C%20remains%20a%20fundamental%20challenge%20due%20to%20the%20complex%20and%20often%0Ahigh-rate%20missingness%20in%20real-world%20scenarios.%20Existing%20models%20typically%0Aoptimize%20the%20point-wise%20reconstruction%20loss%2C%20focusing%20on%20recovering%20numerical%0Avalues%20%28local%20information%29.%20However%2C%20we%20observe%20that%20under%20high%20missing%20rates%2C%0Athese%20models%20still%20perform%20well%20in%20the%20training%20phase%20yet%20produce%20poor%0Aimputations%20and%20distorted%20latent%20representation%20distributions%20%28global%0Ainformation%29%20in%20the%20inference%20phase.%20This%20reveals%20a%20critical%20optimization%0Adilemma%3A%20current%20objectives%20lack%20global%20guidance%2C%20leading%20models%20to%20overfit%0Alocal%20noise%20and%20fail%20to%20capture%20global%20information%20of%20the%20data.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20new%20training%20paradigm%2C%20Glocal%20Information%20Bottleneck%0A%28Glocal-IB%29.%20Glocal-IB%20is%20model-agnostic%20and%20extends%20the%20standard%20IB%20framework%0Aby%20introducing%20a%20Global%20Alignment%20loss%2C%20derived%20from%20a%20tractable%20mutual%0Ainformation%20approximation.%20This%20loss%20aligns%20the%20latent%20representations%20of%0Amasked%20inputs%20with%20those%20of%20their%20originally%20observed%20counterparts.%20It%20helps%0Athe%20model%20retain%20global%20structure%20and%20local%20details%20while%20suppressing%20noise%0Acaused%20by%20missing%20values%2C%20giving%20rise%20to%20better%20generalization%20under%20high%0Amissingness.%20Extensive%20experiments%20on%20nine%20datasets%20confirm%20that%20Glocal-IB%0Aleads%20to%20consistently%20improved%20performance%20and%20aligned%20latent%20representations%0Aunder%20missingness.%20Our%20code%20implementation%20is%20available%20in%0Ahttps%3A//github.com/Muyiiiii/NeurIPS-25-Glocal-IB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04910v1&entry.124074799=Read"},
{"title": "CHARME: A chain-based reinforcement learning approach for the minor\n  embedding problem", "author": "Hoang M. Ngo and Nguyen H K. Do and Minh N. Vu and Tre' R. Jeter and Tamer Kahveci and My T. Thai", "abstract": "  Quantum annealing (QA) has great potential to solve combinatorial\noptimization problems efficiently. However, the effectiveness of QA algorithms\nis heavily based on the embedding of problem instances, represented as logical\ngraphs, into the quantum processing unit (QPU) whose topology is in the form of\na limited connectivity graph, known as the minor embedding problem. Because the\nminor embedding problem is an NP-hard problem~\\mbox{\\cite{Goodrich2018}},\nexisting methods for the minor embedding problem suffer from scalability issues\nwhen faced with larger problem sizes. In this paper, we propose a novel\napproach utilizing Reinforcement Learning (RL) techniques to address the minor\nembedding problem, named CHARME. CHARME includes three key components: a Graph\nNeural Network (GNN) architecture for policy modeling, a state transition\nalgorithm that ensures solution validity, and an order exploration strategy for\neffective training. Through comprehensive experiments on synthetic and\nreal-world instances, we demonstrate the efficiency of our proposed order\nexploration strategy as well as our proposed RL framework, CHARME. In\nparticular, CHARME yields superior solutions in terms of qubit usage compared\nto fast embedding methods such as Minorminer and ATOM. Moreover, our method\nsurpasses the OCT-based approach, known for its slower runtime but high-quality\nsolutions, in several cases. In addition, our proposed exploration enhances the\nefficiency of the training of the CHARME framework by providing better\nsolutions compared to the greedy strategy.\n", "link": "http://arxiv.org/abs/2406.07124v2", "date": "2025-10-06", "relevancy": 1.8265, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4757}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHARME%3A%20A%20chain-based%20reinforcement%20learning%20approach%20for%20the%20minor%0A%20%20embedding%20problem&body=Title%3A%20CHARME%3A%20A%20chain-based%20reinforcement%20learning%20approach%20for%20the%20minor%0A%20%20embedding%20problem%0AAuthor%3A%20Hoang%20M.%20Ngo%20and%20Nguyen%20H%20K.%20Do%20and%20Minh%20N.%20Vu%20and%20Tre%27%20R.%20Jeter%20and%20Tamer%20Kahveci%20and%20My%20T.%20Thai%0AAbstract%3A%20%20%20Quantum%20annealing%20%28QA%29%20has%20great%20potential%20to%20solve%20combinatorial%0Aoptimization%20problems%20efficiently.%20However%2C%20the%20effectiveness%20of%20QA%20algorithms%0Ais%20heavily%20based%20on%20the%20embedding%20of%20problem%20instances%2C%20represented%20as%20logical%0Agraphs%2C%20into%20the%20quantum%20processing%20unit%20%28QPU%29%20whose%20topology%20is%20in%20the%20form%20of%0Aa%20limited%20connectivity%20graph%2C%20known%20as%20the%20minor%20embedding%20problem.%20Because%20the%0Aminor%20embedding%20problem%20is%20an%20NP-hard%20problem~%5Cmbox%7B%5Ccite%7BGoodrich2018%7D%7D%2C%0Aexisting%20methods%20for%20the%20minor%20embedding%20problem%20suffer%20from%20scalability%20issues%0Awhen%20faced%20with%20larger%20problem%20sizes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aapproach%20utilizing%20Reinforcement%20Learning%20%28RL%29%20techniques%20to%20address%20the%20minor%0Aembedding%20problem%2C%20named%20CHARME.%20CHARME%20includes%20three%20key%20components%3A%20a%20Graph%0ANeural%20Network%20%28GNN%29%20architecture%20for%20policy%20modeling%2C%20a%20state%20transition%0Aalgorithm%20that%20ensures%20solution%20validity%2C%20and%20an%20order%20exploration%20strategy%20for%0Aeffective%20training.%20Through%20comprehensive%20experiments%20on%20synthetic%20and%0Areal-world%20instances%2C%20we%20demonstrate%20the%20efficiency%20of%20our%20proposed%20order%0Aexploration%20strategy%20as%20well%20as%20our%20proposed%20RL%20framework%2C%20CHARME.%20In%0Aparticular%2C%20CHARME%20yields%20superior%20solutions%20in%20terms%20of%20qubit%20usage%20compared%0Ato%20fast%20embedding%20methods%20such%20as%20Minorminer%20and%20ATOM.%20Moreover%2C%20our%20method%0Asurpasses%20the%20OCT-based%20approach%2C%20known%20for%20its%20slower%20runtime%20but%20high-quality%0Asolutions%2C%20in%20several%20cases.%20In%20addition%2C%20our%20proposed%20exploration%20enhances%20the%0Aefficiency%20of%20the%20training%20of%20the%20CHARME%20framework%20by%20providing%20better%0Asolutions%20compared%20to%20the%20greedy%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07124v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHARME%253A%2520A%2520chain-based%2520reinforcement%2520learning%2520approach%2520for%2520the%2520minor%250A%2520%2520embedding%2520problem%26entry.906535625%3DHoang%2520M.%2520Ngo%2520and%2520Nguyen%2520H%2520K.%2520Do%2520and%2520Minh%2520N.%2520Vu%2520and%2520Tre%2527%2520R.%2520Jeter%2520and%2520Tamer%2520Kahveci%2520and%2520My%2520T.%2520Thai%26entry.1292438233%3D%2520%2520Quantum%2520annealing%2520%2528QA%2529%2520has%2520great%2520potential%2520to%2520solve%2520combinatorial%250Aoptimization%2520problems%2520efficiently.%2520However%252C%2520the%2520effectiveness%2520of%2520QA%2520algorithms%250Ais%2520heavily%2520based%2520on%2520the%2520embedding%2520of%2520problem%2520instances%252C%2520represented%2520as%2520logical%250Agraphs%252C%2520into%2520the%2520quantum%2520processing%2520unit%2520%2528QPU%2529%2520whose%2520topology%2520is%2520in%2520the%2520form%2520of%250Aa%2520limited%2520connectivity%2520graph%252C%2520known%2520as%2520the%2520minor%2520embedding%2520problem.%2520Because%2520the%250Aminor%2520embedding%2520problem%2520is%2520an%2520NP-hard%2520problem~%255Cmbox%257B%255Ccite%257BGoodrich2018%257D%257D%252C%250Aexisting%2520methods%2520for%2520the%2520minor%2520embedding%2520problem%2520suffer%2520from%2520scalability%2520issues%250Awhen%2520faced%2520with%2520larger%2520problem%2520sizes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520utilizing%2520Reinforcement%2520Learning%2520%2528RL%2529%2520techniques%2520to%2520address%2520the%2520minor%250Aembedding%2520problem%252C%2520named%2520CHARME.%2520CHARME%2520includes%2520three%2520key%2520components%253A%2520a%2520Graph%250ANeural%2520Network%2520%2528GNN%2529%2520architecture%2520for%2520policy%2520modeling%252C%2520a%2520state%2520transition%250Aalgorithm%2520that%2520ensures%2520solution%2520validity%252C%2520and%2520an%2520order%2520exploration%2520strategy%2520for%250Aeffective%2520training.%2520Through%2520comprehensive%2520experiments%2520on%2520synthetic%2520and%250Areal-world%2520instances%252C%2520we%2520demonstrate%2520the%2520efficiency%2520of%2520our%2520proposed%2520order%250Aexploration%2520strategy%2520as%2520well%2520as%2520our%2520proposed%2520RL%2520framework%252C%2520CHARME.%2520In%250Aparticular%252C%2520CHARME%2520yields%2520superior%2520solutions%2520in%2520terms%2520of%2520qubit%2520usage%2520compared%250Ato%2520fast%2520embedding%2520methods%2520such%2520as%2520Minorminer%2520and%2520ATOM.%2520Moreover%252C%2520our%2520method%250Asurpasses%2520the%2520OCT-based%2520approach%252C%2520known%2520for%2520its%2520slower%2520runtime%2520but%2520high-quality%250Asolutions%252C%2520in%2520several%2520cases.%2520In%2520addition%252C%2520our%2520proposed%2520exploration%2520enhances%2520the%250Aefficiency%2520of%2520the%2520training%2520of%2520the%2520CHARME%2520framework%2520by%2520providing%2520better%250Asolutions%2520compared%2520to%2520the%2520greedy%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07124v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHARME%3A%20A%20chain-based%20reinforcement%20learning%20approach%20for%20the%20minor%0A%20%20embedding%20problem&entry.906535625=Hoang%20M.%20Ngo%20and%20Nguyen%20H%20K.%20Do%20and%20Minh%20N.%20Vu%20and%20Tre%27%20R.%20Jeter%20and%20Tamer%20Kahveci%20and%20My%20T.%20Thai&entry.1292438233=%20%20Quantum%20annealing%20%28QA%29%20has%20great%20potential%20to%20solve%20combinatorial%0Aoptimization%20problems%20efficiently.%20However%2C%20the%20effectiveness%20of%20QA%20algorithms%0Ais%20heavily%20based%20on%20the%20embedding%20of%20problem%20instances%2C%20represented%20as%20logical%0Agraphs%2C%20into%20the%20quantum%20processing%20unit%20%28QPU%29%20whose%20topology%20is%20in%20the%20form%20of%0Aa%20limited%20connectivity%20graph%2C%20known%20as%20the%20minor%20embedding%20problem.%20Because%20the%0Aminor%20embedding%20problem%20is%20an%20NP-hard%20problem~%5Cmbox%7B%5Ccite%7BGoodrich2018%7D%7D%2C%0Aexisting%20methods%20for%20the%20minor%20embedding%20problem%20suffer%20from%20scalability%20issues%0Awhen%20faced%20with%20larger%20problem%20sizes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aapproach%20utilizing%20Reinforcement%20Learning%20%28RL%29%20techniques%20to%20address%20the%20minor%0Aembedding%20problem%2C%20named%20CHARME.%20CHARME%20includes%20three%20key%20components%3A%20a%20Graph%0ANeural%20Network%20%28GNN%29%20architecture%20for%20policy%20modeling%2C%20a%20state%20transition%0Aalgorithm%20that%20ensures%20solution%20validity%2C%20and%20an%20order%20exploration%20strategy%20for%0Aeffective%20training.%20Through%20comprehensive%20experiments%20on%20synthetic%20and%0Areal-world%20instances%2C%20we%20demonstrate%20the%20efficiency%20of%20our%20proposed%20order%0Aexploration%20strategy%20as%20well%20as%20our%20proposed%20RL%20framework%2C%20CHARME.%20In%0Aparticular%2C%20CHARME%20yields%20superior%20solutions%20in%20terms%20of%20qubit%20usage%20compared%0Ato%20fast%20embedding%20methods%20such%20as%20Minorminer%20and%20ATOM.%20Moreover%2C%20our%20method%0Asurpasses%20the%20OCT-based%20approach%2C%20known%20for%20its%20slower%20runtime%20but%20high-quality%0Asolutions%2C%20in%20several%20cases.%20In%20addition%2C%20our%20proposed%20exploration%20enhances%20the%0Aefficiency%20of%20the%20training%20of%20the%20CHARME%20framework%20by%20providing%20better%0Asolutions%20compared%20to%20the%20greedy%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07124v2&entry.124074799=Read"},
{"title": "Model Predictive Control-Guided Reinforcement Learning for Implicit\n  Balancing", "author": "Seyed Soroush Karimi Madahi and Kenneth Bruninx and Bert Claessens and Chris Develder", "abstract": "  In Europe, profit-seeking balance responsible parties can deviate in real\ntime from their day-ahead nominations to assist transmission system operators\nin maintaining the supply-demand balance. Model predictive control (MPC)\nstrategies to exploit these implicit balancing strategies capture arbitrage\nopportunities, but fail to accurately capture the price-formation process in\nthe European imbalance markets and face high computational costs. Model-free\nreinforcement learning (RL) methods are fast to execute, but require\ndata-intensive training and usually rely on real-time and historical data for\ndecision-making. This paper proposes an MPC-guided RL method that combines the\ncomplementary strengths of both MPC and RL. The proposed method can effectively\nincorporate forecasts into the decision-making process (as in MPC), while\nmaintaining the fast inference capability of RL. The performance of the\nproposed method is evaluated on the implicit balancing battery control problem\nusing Belgian balancing data from 2023. First, we analyze the performance of\nthe standalone state-of-the-art RL and MPC methods from various angles, to\nhighlight their individual strengths and limitations. Next, we show an\narbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and\n54.36%, compared to standalone RL and MPC.\n", "link": "http://arxiv.org/abs/2510.04868v1", "date": "2025-10-06", "relevancy": 1.8261, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5012}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4592}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Predictive%20Control-Guided%20Reinforcement%20Learning%20for%20Implicit%0A%20%20Balancing&body=Title%3A%20Model%20Predictive%20Control-Guided%20Reinforcement%20Learning%20for%20Implicit%0A%20%20Balancing%0AAuthor%3A%20Seyed%20Soroush%20Karimi%20Madahi%20and%20Kenneth%20Bruninx%20and%20Bert%20Claessens%20and%20Chris%20Develder%0AAbstract%3A%20%20%20In%20Europe%2C%20profit-seeking%20balance%20responsible%20parties%20can%20deviate%20in%20real%0Atime%20from%20their%20day-ahead%20nominations%20to%20assist%20transmission%20system%20operators%0Ain%20maintaining%20the%20supply-demand%20balance.%20Model%20predictive%20control%20%28MPC%29%0Astrategies%20to%20exploit%20these%20implicit%20balancing%20strategies%20capture%20arbitrage%0Aopportunities%2C%20but%20fail%20to%20accurately%20capture%20the%20price-formation%20process%20in%0Athe%20European%20imbalance%20markets%20and%20face%20high%20computational%20costs.%20Model-free%0Areinforcement%20learning%20%28RL%29%20methods%20are%20fast%20to%20execute%2C%20but%20require%0Adata-intensive%20training%20and%20usually%20rely%20on%20real-time%20and%20historical%20data%20for%0Adecision-making.%20This%20paper%20proposes%20an%20MPC-guided%20RL%20method%20that%20combines%20the%0Acomplementary%20strengths%20of%20both%20MPC%20and%20RL.%20The%20proposed%20method%20can%20effectively%0Aincorporate%20forecasts%20into%20the%20decision-making%20process%20%28as%20in%20MPC%29%2C%20while%0Amaintaining%20the%20fast%20inference%20capability%20of%20RL.%20The%20performance%20of%20the%0Aproposed%20method%20is%20evaluated%20on%20the%20implicit%20balancing%20battery%20control%20problem%0Ausing%20Belgian%20balancing%20data%20from%202023.%20First%2C%20we%20analyze%20the%20performance%20of%0Athe%20standalone%20state-of-the-art%20RL%20and%20MPC%20methods%20from%20various%20angles%2C%20to%0Ahighlight%20their%20individual%20strengths%20and%20limitations.%20Next%2C%20we%20show%20an%0Aarbitrage%20profit%20benefit%20of%20the%20proposed%20MPC-guided%20RL%20method%20of%2016.15%25%20and%0A54.36%25%2C%20compared%20to%20standalone%20RL%20and%20MPC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Predictive%2520Control-Guided%2520Reinforcement%2520Learning%2520for%2520Implicit%250A%2520%2520Balancing%26entry.906535625%3DSeyed%2520Soroush%2520Karimi%2520Madahi%2520and%2520Kenneth%2520Bruninx%2520and%2520Bert%2520Claessens%2520and%2520Chris%2520Develder%26entry.1292438233%3D%2520%2520In%2520Europe%252C%2520profit-seeking%2520balance%2520responsible%2520parties%2520can%2520deviate%2520in%2520real%250Atime%2520from%2520their%2520day-ahead%2520nominations%2520to%2520assist%2520transmission%2520system%2520operators%250Ain%2520maintaining%2520the%2520supply-demand%2520balance.%2520Model%2520predictive%2520control%2520%2528MPC%2529%250Astrategies%2520to%2520exploit%2520these%2520implicit%2520balancing%2520strategies%2520capture%2520arbitrage%250Aopportunities%252C%2520but%2520fail%2520to%2520accurately%2520capture%2520the%2520price-formation%2520process%2520in%250Athe%2520European%2520imbalance%2520markets%2520and%2520face%2520high%2520computational%2520costs.%2520Model-free%250Areinforcement%2520learning%2520%2528RL%2529%2520methods%2520are%2520fast%2520to%2520execute%252C%2520but%2520require%250Adata-intensive%2520training%2520and%2520usually%2520rely%2520on%2520real-time%2520and%2520historical%2520data%2520for%250Adecision-making.%2520This%2520paper%2520proposes%2520an%2520MPC-guided%2520RL%2520method%2520that%2520combines%2520the%250Acomplementary%2520strengths%2520of%2520both%2520MPC%2520and%2520RL.%2520The%2520proposed%2520method%2520can%2520effectively%250Aincorporate%2520forecasts%2520into%2520the%2520decision-making%2520process%2520%2528as%2520in%2520MPC%2529%252C%2520while%250Amaintaining%2520the%2520fast%2520inference%2520capability%2520of%2520RL.%2520The%2520performance%2520of%2520the%250Aproposed%2520method%2520is%2520evaluated%2520on%2520the%2520implicit%2520balancing%2520battery%2520control%2520problem%250Ausing%2520Belgian%2520balancing%2520data%2520from%25202023.%2520First%252C%2520we%2520analyze%2520the%2520performance%2520of%250Athe%2520standalone%2520state-of-the-art%2520RL%2520and%2520MPC%2520methods%2520from%2520various%2520angles%252C%2520to%250Ahighlight%2520their%2520individual%2520strengths%2520and%2520limitations.%2520Next%252C%2520we%2520show%2520an%250Aarbitrage%2520profit%2520benefit%2520of%2520the%2520proposed%2520MPC-guided%2520RL%2520method%2520of%252016.15%2525%2520and%250A54.36%2525%252C%2520compared%2520to%2520standalone%2520RL%2520and%2520MPC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Predictive%20Control-Guided%20Reinforcement%20Learning%20for%20Implicit%0A%20%20Balancing&entry.906535625=Seyed%20Soroush%20Karimi%20Madahi%20and%20Kenneth%20Bruninx%20and%20Bert%20Claessens%20and%20Chris%20Develder&entry.1292438233=%20%20In%20Europe%2C%20profit-seeking%20balance%20responsible%20parties%20can%20deviate%20in%20real%0Atime%20from%20their%20day-ahead%20nominations%20to%20assist%20transmission%20system%20operators%0Ain%20maintaining%20the%20supply-demand%20balance.%20Model%20predictive%20control%20%28MPC%29%0Astrategies%20to%20exploit%20these%20implicit%20balancing%20strategies%20capture%20arbitrage%0Aopportunities%2C%20but%20fail%20to%20accurately%20capture%20the%20price-formation%20process%20in%0Athe%20European%20imbalance%20markets%20and%20face%20high%20computational%20costs.%20Model-free%0Areinforcement%20learning%20%28RL%29%20methods%20are%20fast%20to%20execute%2C%20but%20require%0Adata-intensive%20training%20and%20usually%20rely%20on%20real-time%20and%20historical%20data%20for%0Adecision-making.%20This%20paper%20proposes%20an%20MPC-guided%20RL%20method%20that%20combines%20the%0Acomplementary%20strengths%20of%20both%20MPC%20and%20RL.%20The%20proposed%20method%20can%20effectively%0Aincorporate%20forecasts%20into%20the%20decision-making%20process%20%28as%20in%20MPC%29%2C%20while%0Amaintaining%20the%20fast%20inference%20capability%20of%20RL.%20The%20performance%20of%20the%0Aproposed%20method%20is%20evaluated%20on%20the%20implicit%20balancing%20battery%20control%20problem%0Ausing%20Belgian%20balancing%20data%20from%202023.%20First%2C%20we%20analyze%20the%20performance%20of%0Athe%20standalone%20state-of-the-art%20RL%20and%20MPC%20methods%20from%20various%20angles%2C%20to%0Ahighlight%20their%20individual%20strengths%20and%20limitations.%20Next%2C%20we%20show%20an%0Aarbitrage%20profit%20benefit%20of%20the%20proposed%20MPC-guided%20RL%20method%20of%2016.15%25%20and%0A54.36%25%2C%20compared%20to%20standalone%20RL%20and%20MPC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04868v1&entry.124074799=Read"},
{"title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm\n  Engineering", "author": "Yuki Imajuku and Kohki Horie and Yoichi Iwata and Kensho Aoki and Naohiro Takahashi and Takuya Akiba", "abstract": "  How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.\n", "link": "http://arxiv.org/abs/2506.09050v2", "date": "2025-10-06", "relevancy": 1.8043, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4801}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALE-Bench%3A%20A%20Benchmark%20for%20Long-Horizon%20Objective-Driven%20Algorithm%0A%20%20Engineering&body=Title%3A%20ALE-Bench%3A%20A%20Benchmark%20for%20Long-Horizon%20Objective-Driven%20Algorithm%0A%20%20Engineering%0AAuthor%3A%20Yuki%20Imajuku%20and%20Kohki%20Horie%20and%20Yoichi%20Iwata%20and%20Kensho%20Aoki%20and%20Naohiro%20Takahashi%20and%20Takuya%20Akiba%0AAbstract%3A%20%20%20How%20well%20do%20AI%20systems%20perform%20in%20algorithm%20engineering%20for%20hard%20optimization%0Aproblems%20in%20domains%20such%20as%20package-delivery%20routing%2C%20crew%20scheduling%2C%20factory%0Aproduction%20planning%2C%20and%20power-grid%20balancing%3F%20We%20introduce%20ALE-Bench%2C%20a%20new%0Abenchmark%20for%20evaluating%20AI%20systems%20on%20score-based%20algorithmic%20programming%0Acontests.%20Drawing%20on%20real%20tasks%20from%20the%20AtCoder%20Heuristic%20Contests%2C%20ALE-Bench%0Apresents%20optimization%20problems%20that%20are%20computationally%20hard%20and%20admit%20no%20known%0Aexact%20solution.%20Unlike%20short-duration%2C%20pass/fail%20coding%20benchmarks%2C%20ALE-Bench%0Aencourages%20iterative%20solution%20refinement%20over%20long%20time%20horizons.%20Our%20software%0Aframework%20supports%20interactive%20agent%20architectures%20that%20leverage%20test-run%0Afeedback%20and%20visualizations.%20Our%20evaluation%20of%20frontier%20LLMs%20revealed%20that%0Awhile%20they%20demonstrate%20high%20performance%20on%20specific%20problems%2C%20a%20notable%20gap%0Aremains%20compared%20to%20humans%20in%20terms%20of%20consistency%20across%20problems%20and%0Along-horizon%20problem-solving%20capabilities.%20This%20highlights%20the%20need%20for%20this%0Abenchmark%20to%20foster%20future%20AI%20advancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09050v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALE-Bench%253A%2520A%2520Benchmark%2520for%2520Long-Horizon%2520Objective-Driven%2520Algorithm%250A%2520%2520Engineering%26entry.906535625%3DYuki%2520Imajuku%2520and%2520Kohki%2520Horie%2520and%2520Yoichi%2520Iwata%2520and%2520Kensho%2520Aoki%2520and%2520Naohiro%2520Takahashi%2520and%2520Takuya%2520Akiba%26entry.1292438233%3D%2520%2520How%2520well%2520do%2520AI%2520systems%2520perform%2520in%2520algorithm%2520engineering%2520for%2520hard%2520optimization%250Aproblems%2520in%2520domains%2520such%2520as%2520package-delivery%2520routing%252C%2520crew%2520scheduling%252C%2520factory%250Aproduction%2520planning%252C%2520and%2520power-grid%2520balancing%253F%2520We%2520introduce%2520ALE-Bench%252C%2520a%2520new%250Abenchmark%2520for%2520evaluating%2520AI%2520systems%2520on%2520score-based%2520algorithmic%2520programming%250Acontests.%2520Drawing%2520on%2520real%2520tasks%2520from%2520the%2520AtCoder%2520Heuristic%2520Contests%252C%2520ALE-Bench%250Apresents%2520optimization%2520problems%2520that%2520are%2520computationally%2520hard%2520and%2520admit%2520no%2520known%250Aexact%2520solution.%2520Unlike%2520short-duration%252C%2520pass/fail%2520coding%2520benchmarks%252C%2520ALE-Bench%250Aencourages%2520iterative%2520solution%2520refinement%2520over%2520long%2520time%2520horizons.%2520Our%2520software%250Aframework%2520supports%2520interactive%2520agent%2520architectures%2520that%2520leverage%2520test-run%250Afeedback%2520and%2520visualizations.%2520Our%2520evaluation%2520of%2520frontier%2520LLMs%2520revealed%2520that%250Awhile%2520they%2520demonstrate%2520high%2520performance%2520on%2520specific%2520problems%252C%2520a%2520notable%2520gap%250Aremains%2520compared%2520to%2520humans%2520in%2520terms%2520of%2520consistency%2520across%2520problems%2520and%250Along-horizon%2520problem-solving%2520capabilities.%2520This%2520highlights%2520the%2520need%2520for%2520this%250Abenchmark%2520to%2520foster%2520future%2520AI%2520advancements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09050v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALE-Bench%3A%20A%20Benchmark%20for%20Long-Horizon%20Objective-Driven%20Algorithm%0A%20%20Engineering&entry.906535625=Yuki%20Imajuku%20and%20Kohki%20Horie%20and%20Yoichi%20Iwata%20and%20Kensho%20Aoki%20and%20Naohiro%20Takahashi%20and%20Takuya%20Akiba&entry.1292438233=%20%20How%20well%20do%20AI%20systems%20perform%20in%20algorithm%20engineering%20for%20hard%20optimization%0Aproblems%20in%20domains%20such%20as%20package-delivery%20routing%2C%20crew%20scheduling%2C%20factory%0Aproduction%20planning%2C%20and%20power-grid%20balancing%3F%20We%20introduce%20ALE-Bench%2C%20a%20new%0Abenchmark%20for%20evaluating%20AI%20systems%20on%20score-based%20algorithmic%20programming%0Acontests.%20Drawing%20on%20real%20tasks%20from%20the%20AtCoder%20Heuristic%20Contests%2C%20ALE-Bench%0Apresents%20optimization%20problems%20that%20are%20computationally%20hard%20and%20admit%20no%20known%0Aexact%20solution.%20Unlike%20short-duration%2C%20pass/fail%20coding%20benchmarks%2C%20ALE-Bench%0Aencourages%20iterative%20solution%20refinement%20over%20long%20time%20horizons.%20Our%20software%0Aframework%20supports%20interactive%20agent%20architectures%20that%20leverage%20test-run%0Afeedback%20and%20visualizations.%20Our%20evaluation%20of%20frontier%20LLMs%20revealed%20that%0Awhile%20they%20demonstrate%20high%20performance%20on%20specific%20problems%2C%20a%20notable%20gap%0Aremains%20compared%20to%20humans%20in%20terms%20of%20consistency%20across%20problems%20and%0Along-horizon%20problem-solving%20capabilities.%20This%20highlights%20the%20need%20for%20this%0Abenchmark%20to%20foster%20future%20AI%20advancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09050v2&entry.124074799=Read"},
{"title": "KiVi: Kinesthetic-Visuospatial Integration for Dynamic and Safe\n  Egocentric Legged Locomotion", "author": "Peizhuo Li and Hongyi Li and Yuxuan Ma and Linnan Chang and Xinrong Yang and Ruiqi Yu and Yifeng Zhang and Yuhong Cao and Qiuguo Zhu and Guillaume Sartoretti", "abstract": "  Vision-based locomotion has shown great promise in enabling legged robots to\nperceive and adapt to complex environments. However, visual information is\ninherently fragile, being vulnerable to occlusions, reflections, and lighting\nchanges, which often cause instability in locomotion. Inspired by animal\nsensorimotor integration, we propose KiVi, a Kinesthetic-Visuospatial\nintegration framework, where kinesthetics encodes proprioceptive sensing of\nbody motion and visuospatial reasoning captures visual perception of\nsurrounding terrain. Specifically, KiVi separates these pathways, leveraging\nproprioception as a stable backbone while selectively incorporating vision for\nterrain awareness and obstacle avoidance. This modality-balanced, yet\nintegrative design, combined with memory-enhanced attention, allows the robot\nto robustly interpret visual cues while maintaining fallback stability through\nproprioception. Extensive experiments show that our method enables quadruped\nrobots to stably traverse diverse terrains and operate reliably in unstructured\noutdoor environments, remaining robust to out-of-distribution (OOD) visual\nnoise and occlusion unseen during training, thereby highlighting its\neffectiveness and applicability to real-world legged locomotion.\n", "link": "http://arxiv.org/abs/2509.23650v2", "date": "2025-10-06", "relevancy": 1.7962, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6333}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6005}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KiVi%3A%20Kinesthetic-Visuospatial%20Integration%20for%20Dynamic%20and%20Safe%0A%20%20Egocentric%20Legged%20Locomotion&body=Title%3A%20KiVi%3A%20Kinesthetic-Visuospatial%20Integration%20for%20Dynamic%20and%20Safe%0A%20%20Egocentric%20Legged%20Locomotion%0AAuthor%3A%20Peizhuo%20Li%20and%20Hongyi%20Li%20and%20Yuxuan%20Ma%20and%20Linnan%20Chang%20and%20Xinrong%20Yang%20and%20Ruiqi%20Yu%20and%20Yifeng%20Zhang%20and%20Yuhong%20Cao%20and%20Qiuguo%20Zhu%20and%20Guillaume%20Sartoretti%0AAbstract%3A%20%20%20Vision-based%20locomotion%20has%20shown%20great%20promise%20in%20enabling%20legged%20robots%20to%0Aperceive%20and%20adapt%20to%20complex%20environments.%20However%2C%20visual%20information%20is%0Ainherently%20fragile%2C%20being%20vulnerable%20to%20occlusions%2C%20reflections%2C%20and%20lighting%0Achanges%2C%20which%20often%20cause%20instability%20in%20locomotion.%20Inspired%20by%20animal%0Asensorimotor%20integration%2C%20we%20propose%20KiVi%2C%20a%20Kinesthetic-Visuospatial%0Aintegration%20framework%2C%20where%20kinesthetics%20encodes%20proprioceptive%20sensing%20of%0Abody%20motion%20and%20visuospatial%20reasoning%20captures%20visual%20perception%20of%0Asurrounding%20terrain.%20Specifically%2C%20KiVi%20separates%20these%20pathways%2C%20leveraging%0Aproprioception%20as%20a%20stable%20backbone%20while%20selectively%20incorporating%20vision%20for%0Aterrain%20awareness%20and%20obstacle%20avoidance.%20This%20modality-balanced%2C%20yet%0Aintegrative%20design%2C%20combined%20with%20memory-enhanced%20attention%2C%20allows%20the%20robot%0Ato%20robustly%20interpret%20visual%20cues%20while%20maintaining%20fallback%20stability%20through%0Aproprioception.%20Extensive%20experiments%20show%20that%20our%20method%20enables%20quadruped%0Arobots%20to%20stably%20traverse%20diverse%20terrains%20and%20operate%20reliably%20in%20unstructured%0Aoutdoor%20environments%2C%20remaining%20robust%20to%20out-of-distribution%20%28OOD%29%20visual%0Anoise%20and%20occlusion%20unseen%20during%20training%2C%20thereby%20highlighting%20its%0Aeffectiveness%20and%20applicability%20to%20real-world%20legged%20locomotion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23650v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKiVi%253A%2520Kinesthetic-Visuospatial%2520Integration%2520for%2520Dynamic%2520and%2520Safe%250A%2520%2520Egocentric%2520Legged%2520Locomotion%26entry.906535625%3DPeizhuo%2520Li%2520and%2520Hongyi%2520Li%2520and%2520Yuxuan%2520Ma%2520and%2520Linnan%2520Chang%2520and%2520Xinrong%2520Yang%2520and%2520Ruiqi%2520Yu%2520and%2520Yifeng%2520Zhang%2520and%2520Yuhong%2520Cao%2520and%2520Qiuguo%2520Zhu%2520and%2520Guillaume%2520Sartoretti%26entry.1292438233%3D%2520%2520Vision-based%2520locomotion%2520has%2520shown%2520great%2520promise%2520in%2520enabling%2520legged%2520robots%2520to%250Aperceive%2520and%2520adapt%2520to%2520complex%2520environments.%2520However%252C%2520visual%2520information%2520is%250Ainherently%2520fragile%252C%2520being%2520vulnerable%2520to%2520occlusions%252C%2520reflections%252C%2520and%2520lighting%250Achanges%252C%2520which%2520often%2520cause%2520instability%2520in%2520locomotion.%2520Inspired%2520by%2520animal%250Asensorimotor%2520integration%252C%2520we%2520propose%2520KiVi%252C%2520a%2520Kinesthetic-Visuospatial%250Aintegration%2520framework%252C%2520where%2520kinesthetics%2520encodes%2520proprioceptive%2520sensing%2520of%250Abody%2520motion%2520and%2520visuospatial%2520reasoning%2520captures%2520visual%2520perception%2520of%250Asurrounding%2520terrain.%2520Specifically%252C%2520KiVi%2520separates%2520these%2520pathways%252C%2520leveraging%250Aproprioception%2520as%2520a%2520stable%2520backbone%2520while%2520selectively%2520incorporating%2520vision%2520for%250Aterrain%2520awareness%2520and%2520obstacle%2520avoidance.%2520This%2520modality-balanced%252C%2520yet%250Aintegrative%2520design%252C%2520combined%2520with%2520memory-enhanced%2520attention%252C%2520allows%2520the%2520robot%250Ato%2520robustly%2520interpret%2520visual%2520cues%2520while%2520maintaining%2520fallback%2520stability%2520through%250Aproprioception.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520enables%2520quadruped%250Arobots%2520to%2520stably%2520traverse%2520diverse%2520terrains%2520and%2520operate%2520reliably%2520in%2520unstructured%250Aoutdoor%2520environments%252C%2520remaining%2520robust%2520to%2520out-of-distribution%2520%2528OOD%2529%2520visual%250Anoise%2520and%2520occlusion%2520unseen%2520during%2520training%252C%2520thereby%2520highlighting%2520its%250Aeffectiveness%2520and%2520applicability%2520to%2520real-world%2520legged%2520locomotion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23650v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KiVi%3A%20Kinesthetic-Visuospatial%20Integration%20for%20Dynamic%20and%20Safe%0A%20%20Egocentric%20Legged%20Locomotion&entry.906535625=Peizhuo%20Li%20and%20Hongyi%20Li%20and%20Yuxuan%20Ma%20and%20Linnan%20Chang%20and%20Xinrong%20Yang%20and%20Ruiqi%20Yu%20and%20Yifeng%20Zhang%20and%20Yuhong%20Cao%20and%20Qiuguo%20Zhu%20and%20Guillaume%20Sartoretti&entry.1292438233=%20%20Vision-based%20locomotion%20has%20shown%20great%20promise%20in%20enabling%20legged%20robots%20to%0Aperceive%20and%20adapt%20to%20complex%20environments.%20However%2C%20visual%20information%20is%0Ainherently%20fragile%2C%20being%20vulnerable%20to%20occlusions%2C%20reflections%2C%20and%20lighting%0Achanges%2C%20which%20often%20cause%20instability%20in%20locomotion.%20Inspired%20by%20animal%0Asensorimotor%20integration%2C%20we%20propose%20KiVi%2C%20a%20Kinesthetic-Visuospatial%0Aintegration%20framework%2C%20where%20kinesthetics%20encodes%20proprioceptive%20sensing%20of%0Abody%20motion%20and%20visuospatial%20reasoning%20captures%20visual%20perception%20of%0Asurrounding%20terrain.%20Specifically%2C%20KiVi%20separates%20these%20pathways%2C%20leveraging%0Aproprioception%20as%20a%20stable%20backbone%20while%20selectively%20incorporating%20vision%20for%0Aterrain%20awareness%20and%20obstacle%20avoidance.%20This%20modality-balanced%2C%20yet%0Aintegrative%20design%2C%20combined%20with%20memory-enhanced%20attention%2C%20allows%20the%20robot%0Ato%20robustly%20interpret%20visual%20cues%20while%20maintaining%20fallback%20stability%20through%0Aproprioception.%20Extensive%20experiments%20show%20that%20our%20method%20enables%20quadruped%0Arobots%20to%20stably%20traverse%20diverse%20terrains%20and%20operate%20reliably%20in%20unstructured%0Aoutdoor%20environments%2C%20remaining%20robust%20to%20out-of-distribution%20%28OOD%29%20visual%0Anoise%20and%20occlusion%20unseen%20during%20training%2C%20thereby%20highlighting%20its%0Aeffectiveness%20and%20applicability%20to%20real-world%20legged%20locomotion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23650v2&entry.124074799=Read"},
{"title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and\n  Zero-Knowledge Audits", "author": "Ailiya Borjigin and Cong He", "abstract": "  We present a cross-market algorithmic trading system that balances execution\nquality with rigorous compliance enforcement. The architecture comprises a\nhigh-level planner, a reinforcement learning execution agent, and an\nindependent compliance agent. We formulate trade execution as a constrained\nMarkov decision process with hard constraints on participation limits, price\nbands, and self-trading avoidance. The execution agent is trained with proximal\npolicy optimization, while a runtime action-shield projects any unsafe action\ninto a feasible set. To support auditability without exposing proprietary\nsignals, we add a zero-knowledge compliance audit layer that produces\ncryptographic proofs that all actions satisfied the constraints. We evaluate in\na multi-venue, ABIDES-based simulator and compare against standard baselines\n(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and\nvariance while exhibiting no observed constraint violations across stress\nscenarios including elevated latency, partial fills, compliance module\ntoggling, and varying constraint limits. We report effects at the 95%\nconfidence level using paired t-tests and examine tail risk via CVaR. We\nsituate the work at the intersection of optimal execution, safe reinforcement\nlearning, regulatory technology, and verifiable AI, and discuss ethical\nconsiderations, limitations (e.g., modeling assumptions and computational\noverhead), and paths to real-world deployment.\n", "link": "http://arxiv.org/abs/2510.04952v1", "date": "2025-10-06", "relevancy": 1.4231, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4851}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4801}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20and%20Compliant%20Cross-Market%20Trade%20Execution%20via%20Constrained%20RL%20and%0A%20%20Zero-Knowledge%20Audits&body=Title%3A%20Safe%20and%20Compliant%20Cross-Market%20Trade%20Execution%20via%20Constrained%20RL%20and%0A%20%20Zero-Knowledge%20Audits%0AAuthor%3A%20Ailiya%20Borjigin%20and%20Cong%20He%0AAbstract%3A%20%20%20We%20present%20a%20cross-market%20algorithmic%20trading%20system%20that%20balances%20execution%0Aquality%20with%20rigorous%20compliance%20enforcement.%20The%20architecture%20comprises%20a%0Ahigh-level%20planner%2C%20a%20reinforcement%20learning%20execution%20agent%2C%20and%20an%0Aindependent%20compliance%20agent.%20We%20formulate%20trade%20execution%20as%20a%20constrained%0AMarkov%20decision%20process%20with%20hard%20constraints%20on%20participation%20limits%2C%20price%0Abands%2C%20and%20self-trading%20avoidance.%20The%20execution%20agent%20is%20trained%20with%20proximal%0Apolicy%20optimization%2C%20while%20a%20runtime%20action-shield%20projects%20any%20unsafe%20action%0Ainto%20a%20feasible%20set.%20To%20support%20auditability%20without%20exposing%20proprietary%0Asignals%2C%20we%20add%20a%20zero-knowledge%20compliance%20audit%20layer%20that%20produces%0Acryptographic%20proofs%20that%20all%20actions%20satisfied%20the%20constraints.%20We%20evaluate%20in%0Aa%20multi-venue%2C%20ABIDES-based%20simulator%20and%20compare%20against%20standard%20baselines%0A%28e.g.%2C%20TWAP%2C%20VWAP%29.%20The%20learned%20policy%20reduces%20implementation%20shortfall%20and%0Avariance%20while%20exhibiting%20no%20observed%20constraint%20violations%20across%20stress%0Ascenarios%20including%20elevated%20latency%2C%20partial%20fills%2C%20compliance%20module%0Atoggling%2C%20and%20varying%20constraint%20limits.%20We%20report%20effects%20at%20the%2095%25%0Aconfidence%20level%20using%20paired%20t-tests%20and%20examine%20tail%20risk%20via%20CVaR.%20We%0Asituate%20the%20work%20at%20the%20intersection%20of%20optimal%20execution%2C%20safe%20reinforcement%0Alearning%2C%20regulatory%20technology%2C%20and%20verifiable%20AI%2C%20and%20discuss%20ethical%0Aconsiderations%2C%20limitations%20%28e.g.%2C%20modeling%20assumptions%20and%20computational%0Aoverhead%29%2C%20and%20paths%20to%20real-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520and%2520Compliant%2520Cross-Market%2520Trade%2520Execution%2520via%2520Constrained%2520RL%2520and%250A%2520%2520Zero-Knowledge%2520Audits%26entry.906535625%3DAiliya%2520Borjigin%2520and%2520Cong%2520He%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520cross-market%2520algorithmic%2520trading%2520system%2520that%2520balances%2520execution%250Aquality%2520with%2520rigorous%2520compliance%2520enforcement.%2520The%2520architecture%2520comprises%2520a%250Ahigh-level%2520planner%252C%2520a%2520reinforcement%2520learning%2520execution%2520agent%252C%2520and%2520an%250Aindependent%2520compliance%2520agent.%2520We%2520formulate%2520trade%2520execution%2520as%2520a%2520constrained%250AMarkov%2520decision%2520process%2520with%2520hard%2520constraints%2520on%2520participation%2520limits%252C%2520price%250Abands%252C%2520and%2520self-trading%2520avoidance.%2520The%2520execution%2520agent%2520is%2520trained%2520with%2520proximal%250Apolicy%2520optimization%252C%2520while%2520a%2520runtime%2520action-shield%2520projects%2520any%2520unsafe%2520action%250Ainto%2520a%2520feasible%2520set.%2520To%2520support%2520auditability%2520without%2520exposing%2520proprietary%250Asignals%252C%2520we%2520add%2520a%2520zero-knowledge%2520compliance%2520audit%2520layer%2520that%2520produces%250Acryptographic%2520proofs%2520that%2520all%2520actions%2520satisfied%2520the%2520constraints.%2520We%2520evaluate%2520in%250Aa%2520multi-venue%252C%2520ABIDES-based%2520simulator%2520and%2520compare%2520against%2520standard%2520baselines%250A%2528e.g.%252C%2520TWAP%252C%2520VWAP%2529.%2520The%2520learned%2520policy%2520reduces%2520implementation%2520shortfall%2520and%250Avariance%2520while%2520exhibiting%2520no%2520observed%2520constraint%2520violations%2520across%2520stress%250Ascenarios%2520including%2520elevated%2520latency%252C%2520partial%2520fills%252C%2520compliance%2520module%250Atoggling%252C%2520and%2520varying%2520constraint%2520limits.%2520We%2520report%2520effects%2520at%2520the%252095%2525%250Aconfidence%2520level%2520using%2520paired%2520t-tests%2520and%2520examine%2520tail%2520risk%2520via%2520CVaR.%2520We%250Asituate%2520the%2520work%2520at%2520the%2520intersection%2520of%2520optimal%2520execution%252C%2520safe%2520reinforcement%250Alearning%252C%2520regulatory%2520technology%252C%2520and%2520verifiable%2520AI%252C%2520and%2520discuss%2520ethical%250Aconsiderations%252C%2520limitations%2520%2528e.g.%252C%2520modeling%2520assumptions%2520and%2520computational%250Aoverhead%2529%252C%2520and%2520paths%2520to%2520real-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20and%20Compliant%20Cross-Market%20Trade%20Execution%20via%20Constrained%20RL%20and%0A%20%20Zero-Knowledge%20Audits&entry.906535625=Ailiya%20Borjigin%20and%20Cong%20He&entry.1292438233=%20%20We%20present%20a%20cross-market%20algorithmic%20trading%20system%20that%20balances%20execution%0Aquality%20with%20rigorous%20compliance%20enforcement.%20The%20architecture%20comprises%20a%0Ahigh-level%20planner%2C%20a%20reinforcement%20learning%20execution%20agent%2C%20and%20an%0Aindependent%20compliance%20agent.%20We%20formulate%20trade%20execution%20as%20a%20constrained%0AMarkov%20decision%20process%20with%20hard%20constraints%20on%20participation%20limits%2C%20price%0Abands%2C%20and%20self-trading%20avoidance.%20The%20execution%20agent%20is%20trained%20with%20proximal%0Apolicy%20optimization%2C%20while%20a%20runtime%20action-shield%20projects%20any%20unsafe%20action%0Ainto%20a%20feasible%20set.%20To%20support%20auditability%20without%20exposing%20proprietary%0Asignals%2C%20we%20add%20a%20zero-knowledge%20compliance%20audit%20layer%20that%20produces%0Acryptographic%20proofs%20that%20all%20actions%20satisfied%20the%20constraints.%20We%20evaluate%20in%0Aa%20multi-venue%2C%20ABIDES-based%20simulator%20and%20compare%20against%20standard%20baselines%0A%28e.g.%2C%20TWAP%2C%20VWAP%29.%20The%20learned%20policy%20reduces%20implementation%20shortfall%20and%0Avariance%20while%20exhibiting%20no%20observed%20constraint%20violations%20across%20stress%0Ascenarios%20including%20elevated%20latency%2C%20partial%20fills%2C%20compliance%20module%0Atoggling%2C%20and%20varying%20constraint%20limits.%20We%20report%20effects%20at%20the%2095%25%0Aconfidence%20level%20using%20paired%20t-tests%20and%20examine%20tail%20risk%20via%20CVaR.%20We%0Asituate%20the%20work%20at%20the%20intersection%20of%20optimal%20execution%2C%20safe%20reinforcement%0Alearning%2C%20regulatory%20technology%2C%20and%20verifiable%20AI%2C%20and%20discuss%20ethical%0Aconsiderations%2C%20limitations%20%28e.g.%2C%20modeling%20assumptions%20and%20computational%0Aoverhead%29%2C%20and%20paths%20to%20real-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04952v1&entry.124074799=Read"},
{"title": "DP-HYPE: Distributed Differentially Private Hyperparameter Search", "author": "Johannes Liebenow and Thorsten Peinemann and Esfandiar Mohammadi", "abstract": "  The tuning of hyperparameters in distributed machine learning can\nsubstantially impact model performance. When the hyperparameters are tuned on\nsensitive data, privacy becomes an important challenge and to this end,\ndifferential privacy has emerged as the de facto standard for provable privacy.\nA standard setting when performing distributed learning tasks is that clients\nagree on a shared setup, i.e., find a compromise from a set of hyperparameters,\nlike the learning rate of the model to be trained. Yet, prior work on\ndifferentially private hyperparameter tuning either uses computationally\nexpensive cryptographic protocols, determines hyperparameters separately for\neach client, or applies differential privacy locally, which can lead to\nundesirable utility-privacy trade-offs.\n  In this work, we present our algorithm DP-HYPE, which performs a distributed\nand privacy-preserving hyperparameter search by conducting a distributed voting\nbased on local hyperparameter evaluations of clients. In this way, DP-HYPE\nselects hyperparameters that lead to a compromise supported by the majority of\nclients, while maintaining scalability and independence from specific learning\ntasks. We prove that DP-HYPE preserves the strong notion of differential\nprivacy called client-level differential privacy and, importantly, show that\nits privacy guarantees do not depend on the number of hyperparameters. We also\nprovide bounds on its utility guarantees, that is, the probability of reaching\na compromise, and implement DP-HYPE as a submodule in the popular Flower\nframework for distributed machine learning. In addition, we evaluate\nperformance on multiple benchmark data sets in iid as well as multiple non-iid\nsettings and demonstrate high utility of DP-HYPE even under small privacy\nbudgets.\n", "link": "http://arxiv.org/abs/2510.04902v1", "date": "2025-10-06", "relevancy": 1.2821, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4356}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4207}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP-HYPE%3A%20Distributed%20Differentially%20Private%20Hyperparameter%20Search&body=Title%3A%20DP-HYPE%3A%20Distributed%20Differentially%20Private%20Hyperparameter%20Search%0AAuthor%3A%20Johannes%20Liebenow%20and%20Thorsten%20Peinemann%20and%20Esfandiar%20Mohammadi%0AAbstract%3A%20%20%20The%20tuning%20of%20hyperparameters%20in%20distributed%20machine%20learning%20can%0Asubstantially%20impact%20model%20performance.%20When%20the%20hyperparameters%20are%20tuned%20on%0Asensitive%20data%2C%20privacy%20becomes%20an%20important%20challenge%20and%20to%20this%20end%2C%0Adifferential%20privacy%20has%20emerged%20as%20the%20de%20facto%20standard%20for%20provable%20privacy.%0AA%20standard%20setting%20when%20performing%20distributed%20learning%20tasks%20is%20that%20clients%0Aagree%20on%20a%20shared%20setup%2C%20i.e.%2C%20find%20a%20compromise%20from%20a%20set%20of%20hyperparameters%2C%0Alike%20the%20learning%20rate%20of%20the%20model%20to%20be%20trained.%20Yet%2C%20prior%20work%20on%0Adifferentially%20private%20hyperparameter%20tuning%20either%20uses%20computationally%0Aexpensive%20cryptographic%20protocols%2C%20determines%20hyperparameters%20separately%20for%0Aeach%20client%2C%20or%20applies%20differential%20privacy%20locally%2C%20which%20can%20lead%20to%0Aundesirable%20utility-privacy%20trade-offs.%0A%20%20In%20this%20work%2C%20we%20present%20our%20algorithm%20DP-HYPE%2C%20which%20performs%20a%20distributed%0Aand%20privacy-preserving%20hyperparameter%20search%20by%20conducting%20a%20distributed%20voting%0Abased%20on%20local%20hyperparameter%20evaluations%20of%20clients.%20In%20this%20way%2C%20DP-HYPE%0Aselects%20hyperparameters%20that%20lead%20to%20a%20compromise%20supported%20by%20the%20majority%20of%0Aclients%2C%20while%20maintaining%20scalability%20and%20independence%20from%20specific%20learning%0Atasks.%20We%20prove%20that%20DP-HYPE%20preserves%20the%20strong%20notion%20of%20differential%0Aprivacy%20called%20client-level%20differential%20privacy%20and%2C%20importantly%2C%20show%20that%0Aits%20privacy%20guarantees%20do%20not%20depend%20on%20the%20number%20of%20hyperparameters.%20We%20also%0Aprovide%20bounds%20on%20its%20utility%20guarantees%2C%20that%20is%2C%20the%20probability%20of%20reaching%0Aa%20compromise%2C%20and%20implement%20DP-HYPE%20as%20a%20submodule%20in%20the%20popular%20Flower%0Aframework%20for%20distributed%20machine%20learning.%20In%20addition%2C%20we%20evaluate%0Aperformance%20on%20multiple%20benchmark%20data%20sets%20in%20iid%20as%20well%20as%20multiple%20non-iid%0Asettings%20and%20demonstrate%20high%20utility%20of%20DP-HYPE%20even%20under%20small%20privacy%0Abudgets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP-HYPE%253A%2520Distributed%2520Differentially%2520Private%2520Hyperparameter%2520Search%26entry.906535625%3DJohannes%2520Liebenow%2520and%2520Thorsten%2520Peinemann%2520and%2520Esfandiar%2520Mohammadi%26entry.1292438233%3D%2520%2520The%2520tuning%2520of%2520hyperparameters%2520in%2520distributed%2520machine%2520learning%2520can%250Asubstantially%2520impact%2520model%2520performance.%2520When%2520the%2520hyperparameters%2520are%2520tuned%2520on%250Asensitive%2520data%252C%2520privacy%2520becomes%2520an%2520important%2520challenge%2520and%2520to%2520this%2520end%252C%250Adifferential%2520privacy%2520has%2520emerged%2520as%2520the%2520de%2520facto%2520standard%2520for%2520provable%2520privacy.%250AA%2520standard%2520setting%2520when%2520performing%2520distributed%2520learning%2520tasks%2520is%2520that%2520clients%250Aagree%2520on%2520a%2520shared%2520setup%252C%2520i.e.%252C%2520find%2520a%2520compromise%2520from%2520a%2520set%2520of%2520hyperparameters%252C%250Alike%2520the%2520learning%2520rate%2520of%2520the%2520model%2520to%2520be%2520trained.%2520Yet%252C%2520prior%2520work%2520on%250Adifferentially%2520private%2520hyperparameter%2520tuning%2520either%2520uses%2520computationally%250Aexpensive%2520cryptographic%2520protocols%252C%2520determines%2520hyperparameters%2520separately%2520for%250Aeach%2520client%252C%2520or%2520applies%2520differential%2520privacy%2520locally%252C%2520which%2520can%2520lead%2520to%250Aundesirable%2520utility-privacy%2520trade-offs.%250A%2520%2520In%2520this%2520work%252C%2520we%2520present%2520our%2520algorithm%2520DP-HYPE%252C%2520which%2520performs%2520a%2520distributed%250Aand%2520privacy-preserving%2520hyperparameter%2520search%2520by%2520conducting%2520a%2520distributed%2520voting%250Abased%2520on%2520local%2520hyperparameter%2520evaluations%2520of%2520clients.%2520In%2520this%2520way%252C%2520DP-HYPE%250Aselects%2520hyperparameters%2520that%2520lead%2520to%2520a%2520compromise%2520supported%2520by%2520the%2520majority%2520of%250Aclients%252C%2520while%2520maintaining%2520scalability%2520and%2520independence%2520from%2520specific%2520learning%250Atasks.%2520We%2520prove%2520that%2520DP-HYPE%2520preserves%2520the%2520strong%2520notion%2520of%2520differential%250Aprivacy%2520called%2520client-level%2520differential%2520privacy%2520and%252C%2520importantly%252C%2520show%2520that%250Aits%2520privacy%2520guarantees%2520do%2520not%2520depend%2520on%2520the%2520number%2520of%2520hyperparameters.%2520We%2520also%250Aprovide%2520bounds%2520on%2520its%2520utility%2520guarantees%252C%2520that%2520is%252C%2520the%2520probability%2520of%2520reaching%250Aa%2520compromise%252C%2520and%2520implement%2520DP-HYPE%2520as%2520a%2520submodule%2520in%2520the%2520popular%2520Flower%250Aframework%2520for%2520distributed%2520machine%2520learning.%2520In%2520addition%252C%2520we%2520evaluate%250Aperformance%2520on%2520multiple%2520benchmark%2520data%2520sets%2520in%2520iid%2520as%2520well%2520as%2520multiple%2520non-iid%250Asettings%2520and%2520demonstrate%2520high%2520utility%2520of%2520DP-HYPE%2520even%2520under%2520small%2520privacy%250Abudgets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-HYPE%3A%20Distributed%20Differentially%20Private%20Hyperparameter%20Search&entry.906535625=Johannes%20Liebenow%20and%20Thorsten%20Peinemann%20and%20Esfandiar%20Mohammadi&entry.1292438233=%20%20The%20tuning%20of%20hyperparameters%20in%20distributed%20machine%20learning%20can%0Asubstantially%20impact%20model%20performance.%20When%20the%20hyperparameters%20are%20tuned%20on%0Asensitive%20data%2C%20privacy%20becomes%20an%20important%20challenge%20and%20to%20this%20end%2C%0Adifferential%20privacy%20has%20emerged%20as%20the%20de%20facto%20standard%20for%20provable%20privacy.%0AA%20standard%20setting%20when%20performing%20distributed%20learning%20tasks%20is%20that%20clients%0Aagree%20on%20a%20shared%20setup%2C%20i.e.%2C%20find%20a%20compromise%20from%20a%20set%20of%20hyperparameters%2C%0Alike%20the%20learning%20rate%20of%20the%20model%20to%20be%20trained.%20Yet%2C%20prior%20work%20on%0Adifferentially%20private%20hyperparameter%20tuning%20either%20uses%20computationally%0Aexpensive%20cryptographic%20protocols%2C%20determines%20hyperparameters%20separately%20for%0Aeach%20client%2C%20or%20applies%20differential%20privacy%20locally%2C%20which%20can%20lead%20to%0Aundesirable%20utility-privacy%20trade-offs.%0A%20%20In%20this%20work%2C%20we%20present%20our%20algorithm%20DP-HYPE%2C%20which%20performs%20a%20distributed%0Aand%20privacy-preserving%20hyperparameter%20search%20by%20conducting%20a%20distributed%20voting%0Abased%20on%20local%20hyperparameter%20evaluations%20of%20clients.%20In%20this%20way%2C%20DP-HYPE%0Aselects%20hyperparameters%20that%20lead%20to%20a%20compromise%20supported%20by%20the%20majority%20of%0Aclients%2C%20while%20maintaining%20scalability%20and%20independence%20from%20specific%20learning%0Atasks.%20We%20prove%20that%20DP-HYPE%20preserves%20the%20strong%20notion%20of%20differential%0Aprivacy%20called%20client-level%20differential%20privacy%20and%2C%20importantly%2C%20show%20that%0Aits%20privacy%20guarantees%20do%20not%20depend%20on%20the%20number%20of%20hyperparameters.%20We%20also%0Aprovide%20bounds%20on%20its%20utility%20guarantees%2C%20that%20is%2C%20the%20probability%20of%20reaching%0Aa%20compromise%2C%20and%20implement%20DP-HYPE%20as%20a%20submodule%20in%20the%20popular%20Flower%0Aframework%20for%20distributed%20machine%20learning.%20In%20addition%2C%20we%20evaluate%0Aperformance%20on%20multiple%20benchmark%20data%20sets%20in%20iid%20as%20well%20as%20multiple%20non-iid%0Asettings%20and%20demonstrate%20high%20utility%20of%20DP-HYPE%20even%20under%20small%20privacy%0Abudgets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04902v1&entry.124074799=Read"},
{"title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive\n  Experts", "author": "Jihoon Lee and Hoyeon Moon and Kevin Zhai and Arun Kumar Chithanar and Anit Kumar Sahu and Soummya Kar and Chul Lee and Souradip Chakraborty and Amrit Singh Bedi", "abstract": "  Diffusion-based large language models (dLLMs) are trained flexibly to model\nextreme dependence in the data distribution; however, how to best utilize this\ninformation at inference time remains an open problem. In this work, we uncover\nan interesting property of these models: dLLMs trained on textual data\nimplicitly learn a mixture of semi-autoregressive experts, where different\ngeneration orders reveal different specialized behaviors. We show that\ncommitting to any single, fixed inference time schedule, a common practice,\ncollapses performance by failing to leverage this latent ensemble. To address\nthis, we introduce HEX (Hidden semiautoregressive EXperts for test-time\nscaling), a training-free inference method that ensembles across heterogeneous\nblock schedules. By doing a majority vote over diverse block-sized generation\npaths, HEX robustly avoids failure modes associated with any single fixed\nschedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to\n3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and\nspecialized fine-tuned methods like GRPO, without additional training. HEX even\nyields significant gains on MATH benchmark from 16.40% to 40.00%, scientific\nreasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.\nOur results establish a new paradigm for test-time scaling in diffusion-based\nLLMs (dLLMs), revealing that the sequence in which masking is performed plays a\ncritical role in determining performance during inference.\n", "link": "http://arxiv.org/abs/2510.05040v1", "date": "2025-10-06", "relevancy": 1.7066, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.618}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5687}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Scaling%20in%20Diffusion%20LLMs%20via%20Hidden%20Semi-Autoregressive%0A%20%20Experts&body=Title%3A%20Test-Time%20Scaling%20in%20Diffusion%20LLMs%20via%20Hidden%20Semi-Autoregressive%0A%20%20Experts%0AAuthor%3A%20Jihoon%20Lee%20and%20Hoyeon%20Moon%20and%20Kevin%20Zhai%20and%20Arun%20Kumar%20Chithanar%20and%20Anit%20Kumar%20Sahu%20and%20Soummya%20Kar%20and%20Chul%20Lee%20and%20Souradip%20Chakraborty%20and%20Amrit%20Singh%20Bedi%0AAbstract%3A%20%20%20Diffusion-based%20large%20language%20models%20%28dLLMs%29%20are%20trained%20flexibly%20to%20model%0Aextreme%20dependence%20in%20the%20data%20distribution%3B%20however%2C%20how%20to%20best%20utilize%20this%0Ainformation%20at%20inference%20time%20remains%20an%20open%20problem.%20In%20this%20work%2C%20we%20uncover%0Aan%20interesting%20property%20of%20these%20models%3A%20dLLMs%20trained%20on%20textual%20data%0Aimplicitly%20learn%20a%20mixture%20of%20semi-autoregressive%20experts%2C%20where%20different%0Ageneration%20orders%20reveal%20different%20specialized%20behaviors.%20We%20show%20that%0Acommitting%20to%20any%20single%2C%20fixed%20inference%20time%20schedule%2C%20a%20common%20practice%2C%0Acollapses%20performance%20by%20failing%20to%20leverage%20this%20latent%20ensemble.%20To%20address%0Athis%2C%20we%20introduce%20HEX%20%28Hidden%20semiautoregressive%20EXperts%20for%20test-time%0Ascaling%29%2C%20a%20training-free%20inference%20method%20that%20ensembles%20across%20heterogeneous%0Ablock%20schedules.%20By%20doing%20a%20majority%20vote%20over%20diverse%20block-sized%20generation%0Apaths%2C%20HEX%20robustly%20avoids%20failure%20modes%20associated%20with%20any%20single%20fixed%0Aschedule.%20On%20reasoning%20benchmarks%20such%20as%20GSM8K%2C%20it%20boosts%20accuracy%20by%20up%20to%0A3.56X%20%28from%2024.72%25%20to%2088.10%25%29%2C%20outperforming%20top-K%20margin%20inference%20and%0Aspecialized%20fine-tuned%20methods%20like%20GRPO%2C%20without%20additional%20training.%20HEX%20even%0Ayields%20significant%20gains%20on%20MATH%20benchmark%20from%2016.40%25%20to%2040.00%25%2C%20scientific%0Areasoning%20on%20ARC-C%20from%2054.18%25%20to%2087.80%25%2C%20and%20TruthfulQA%20from%2028.36%25%20to%2057.46%25.%0AOur%20results%20establish%20a%20new%20paradigm%20for%20test-time%20scaling%20in%20diffusion-based%0ALLMs%20%28dLLMs%29%2C%20revealing%20that%20the%20sequence%20in%20which%20masking%20is%20performed%20plays%20a%0Acritical%20role%20in%20determining%20performance%20during%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Scaling%2520in%2520Diffusion%2520LLMs%2520via%2520Hidden%2520Semi-Autoregressive%250A%2520%2520Experts%26entry.906535625%3DJihoon%2520Lee%2520and%2520Hoyeon%2520Moon%2520and%2520Kevin%2520Zhai%2520and%2520Arun%2520Kumar%2520Chithanar%2520and%2520Anit%2520Kumar%2520Sahu%2520and%2520Soummya%2520Kar%2520and%2520Chul%2520Lee%2520and%2520Souradip%2520Chakraborty%2520and%2520Amrit%2520Singh%2520Bedi%26entry.1292438233%3D%2520%2520Diffusion-based%2520large%2520language%2520models%2520%2528dLLMs%2529%2520are%2520trained%2520flexibly%2520to%2520model%250Aextreme%2520dependence%2520in%2520the%2520data%2520distribution%253B%2520however%252C%2520how%2520to%2520best%2520utilize%2520this%250Ainformation%2520at%2520inference%2520time%2520remains%2520an%2520open%2520problem.%2520In%2520this%2520work%252C%2520we%2520uncover%250Aan%2520interesting%2520property%2520of%2520these%2520models%253A%2520dLLMs%2520trained%2520on%2520textual%2520data%250Aimplicitly%2520learn%2520a%2520mixture%2520of%2520semi-autoregressive%2520experts%252C%2520where%2520different%250Ageneration%2520orders%2520reveal%2520different%2520specialized%2520behaviors.%2520We%2520show%2520that%250Acommitting%2520to%2520any%2520single%252C%2520fixed%2520inference%2520time%2520schedule%252C%2520a%2520common%2520practice%252C%250Acollapses%2520performance%2520by%2520failing%2520to%2520leverage%2520this%2520latent%2520ensemble.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520HEX%2520%2528Hidden%2520semiautoregressive%2520EXperts%2520for%2520test-time%250Ascaling%2529%252C%2520a%2520training-free%2520inference%2520method%2520that%2520ensembles%2520across%2520heterogeneous%250Ablock%2520schedules.%2520By%2520doing%2520a%2520majority%2520vote%2520over%2520diverse%2520block-sized%2520generation%250Apaths%252C%2520HEX%2520robustly%2520avoids%2520failure%2520modes%2520associated%2520with%2520any%2520single%2520fixed%250Aschedule.%2520On%2520reasoning%2520benchmarks%2520such%2520as%2520GSM8K%252C%2520it%2520boosts%2520accuracy%2520by%2520up%2520to%250A3.56X%2520%2528from%252024.72%2525%2520to%252088.10%2525%2529%252C%2520outperforming%2520top-K%2520margin%2520inference%2520and%250Aspecialized%2520fine-tuned%2520methods%2520like%2520GRPO%252C%2520without%2520additional%2520training.%2520HEX%2520even%250Ayields%2520significant%2520gains%2520on%2520MATH%2520benchmark%2520from%252016.40%2525%2520to%252040.00%2525%252C%2520scientific%250Areasoning%2520on%2520ARC-C%2520from%252054.18%2525%2520to%252087.80%2525%252C%2520and%2520TruthfulQA%2520from%252028.36%2525%2520to%252057.46%2525.%250AOur%2520results%2520establish%2520a%2520new%2520paradigm%2520for%2520test-time%2520scaling%2520in%2520diffusion-based%250ALLMs%2520%2528dLLMs%2529%252C%2520revealing%2520that%2520the%2520sequence%2520in%2520which%2520masking%2520is%2520performed%2520plays%2520a%250Acritical%2520role%2520in%2520determining%2520performance%2520during%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Scaling%20in%20Diffusion%20LLMs%20via%20Hidden%20Semi-Autoregressive%0A%20%20Experts&entry.906535625=Jihoon%20Lee%20and%20Hoyeon%20Moon%20and%20Kevin%20Zhai%20and%20Arun%20Kumar%20Chithanar%20and%20Anit%20Kumar%20Sahu%20and%20Soummya%20Kar%20and%20Chul%20Lee%20and%20Souradip%20Chakraborty%20and%20Amrit%20Singh%20Bedi&entry.1292438233=%20%20Diffusion-based%20large%20language%20models%20%28dLLMs%29%20are%20trained%20flexibly%20to%20model%0Aextreme%20dependence%20in%20the%20data%20distribution%3B%20however%2C%20how%20to%20best%20utilize%20this%0Ainformation%20at%20inference%20time%20remains%20an%20open%20problem.%20In%20this%20work%2C%20we%20uncover%0Aan%20interesting%20property%20of%20these%20models%3A%20dLLMs%20trained%20on%20textual%20data%0Aimplicitly%20learn%20a%20mixture%20of%20semi-autoregressive%20experts%2C%20where%20different%0Ageneration%20orders%20reveal%20different%20specialized%20behaviors.%20We%20show%20that%0Acommitting%20to%20any%20single%2C%20fixed%20inference%20time%20schedule%2C%20a%20common%20practice%2C%0Acollapses%20performance%20by%20failing%20to%20leverage%20this%20latent%20ensemble.%20To%20address%0Athis%2C%20we%20introduce%20HEX%20%28Hidden%20semiautoregressive%20EXperts%20for%20test-time%0Ascaling%29%2C%20a%20training-free%20inference%20method%20that%20ensembles%20across%20heterogeneous%0Ablock%20schedules.%20By%20doing%20a%20majority%20vote%20over%20diverse%20block-sized%20generation%0Apaths%2C%20HEX%20robustly%20avoids%20failure%20modes%20associated%20with%20any%20single%20fixed%0Aschedule.%20On%20reasoning%20benchmarks%20such%20as%20GSM8K%2C%20it%20boosts%20accuracy%20by%20up%20to%0A3.56X%20%28from%2024.72%25%20to%2088.10%25%29%2C%20outperforming%20top-K%20margin%20inference%20and%0Aspecialized%20fine-tuned%20methods%20like%20GRPO%2C%20without%20additional%20training.%20HEX%20even%0Ayields%20significant%20gains%20on%20MATH%20benchmark%20from%2016.40%25%20to%2040.00%25%2C%20scientific%0Areasoning%20on%20ARC-C%20from%2054.18%25%20to%2087.80%25%2C%20and%20TruthfulQA%20from%2028.36%25%20to%2057.46%25.%0AOur%20results%20establish%20a%20new%20paradigm%20for%20test-time%20scaling%20in%20diffusion-based%0ALLMs%20%28dLLMs%29%2C%20revealing%20that%20the%20sequence%20in%20which%20masking%20is%20performed%20plays%20a%0Acritical%20role%20in%20determining%20performance%20during%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05040v1&entry.124074799=Read"},
{"title": "Learning-Augmented Robust Algorithmic Recourse", "author": "Kshitij Kayastha and Vasilis Gkatzelis and Shahin Jabbari", "abstract": "  Algorithmic recourse provides individuals who receive undesirable outcomes\nfrom machine learning systems with minimum-cost improvements to achieve a\ndesirable outcome. However, machine learning models often get updated, so the\nrecourse may not lead to the desired outcome. The robust recourse framework\nchooses recourses that are less sensitive to adversarial model changes, but\nthis comes at a higher cost. To address this, we initiate the study of\nlearning-augmented algorithmic recourse and evaluate the extent to which a\ndesigner equipped with a prediction of the future model can reduce the cost of\nrecourse when the prediction is accurate (consistency) while also limiting the\ncost even when the prediction is inaccurate (robustness). We propose a novel\nalgorithm, study the robustness-consistency trade-off, and analyze how\nprediction accuracy affects performance.\n", "link": "http://arxiv.org/abs/2410.01580v2", "date": "2025-10-06", "relevancy": 1.4403, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4952}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4785}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning-Augmented%20Robust%20Algorithmic%20Recourse&body=Title%3A%20Learning-Augmented%20Robust%20Algorithmic%20Recourse%0AAuthor%3A%20Kshitij%20Kayastha%20and%20Vasilis%20Gkatzelis%20and%20Shahin%20Jabbari%0AAbstract%3A%20%20%20Algorithmic%20recourse%20provides%20individuals%20who%20receive%20undesirable%20outcomes%0Afrom%20machine%20learning%20systems%20with%20minimum-cost%20improvements%20to%20achieve%20a%0Adesirable%20outcome.%20However%2C%20machine%20learning%20models%20often%20get%20updated%2C%20so%20the%0Arecourse%20may%20not%20lead%20to%20the%20desired%20outcome.%20The%20robust%20recourse%20framework%0Achooses%20recourses%20that%20are%20less%20sensitive%20to%20adversarial%20model%20changes%2C%20but%0Athis%20comes%20at%20a%20higher%20cost.%20To%20address%20this%2C%20we%20initiate%20the%20study%20of%0Alearning-augmented%20algorithmic%20recourse%20and%20evaluate%20the%20extent%20to%20which%20a%0Adesigner%20equipped%20with%20a%20prediction%20of%20the%20future%20model%20can%20reduce%20the%20cost%20of%0Arecourse%20when%20the%20prediction%20is%20accurate%20%28consistency%29%20while%20also%20limiting%20the%0Acost%20even%20when%20the%20prediction%20is%20inaccurate%20%28robustness%29.%20We%20propose%20a%20novel%0Aalgorithm%2C%20study%20the%20robustness-consistency%20trade-off%2C%20and%20analyze%20how%0Aprediction%20accuracy%20affects%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01580v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning-Augmented%2520Robust%2520Algorithmic%2520Recourse%26entry.906535625%3DKshitij%2520Kayastha%2520and%2520Vasilis%2520Gkatzelis%2520and%2520Shahin%2520Jabbari%26entry.1292438233%3D%2520%2520Algorithmic%2520recourse%2520provides%2520individuals%2520who%2520receive%2520undesirable%2520outcomes%250Afrom%2520machine%2520learning%2520systems%2520with%2520minimum-cost%2520improvements%2520to%2520achieve%2520a%250Adesirable%2520outcome.%2520However%252C%2520machine%2520learning%2520models%2520often%2520get%2520updated%252C%2520so%2520the%250Arecourse%2520may%2520not%2520lead%2520to%2520the%2520desired%2520outcome.%2520The%2520robust%2520recourse%2520framework%250Achooses%2520recourses%2520that%2520are%2520less%2520sensitive%2520to%2520adversarial%2520model%2520changes%252C%2520but%250Athis%2520comes%2520at%2520a%2520higher%2520cost.%2520To%2520address%2520this%252C%2520we%2520initiate%2520the%2520study%2520of%250Alearning-augmented%2520algorithmic%2520recourse%2520and%2520evaluate%2520the%2520extent%2520to%2520which%2520a%250Adesigner%2520equipped%2520with%2520a%2520prediction%2520of%2520the%2520future%2520model%2520can%2520reduce%2520the%2520cost%2520of%250Arecourse%2520when%2520the%2520prediction%2520is%2520accurate%2520%2528consistency%2529%2520while%2520also%2520limiting%2520the%250Acost%2520even%2520when%2520the%2520prediction%2520is%2520inaccurate%2520%2528robustness%2529.%2520We%2520propose%2520a%2520novel%250Aalgorithm%252C%2520study%2520the%2520robustness-consistency%2520trade-off%252C%2520and%2520analyze%2520how%250Aprediction%2520accuracy%2520affects%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01580v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-Augmented%20Robust%20Algorithmic%20Recourse&entry.906535625=Kshitij%20Kayastha%20and%20Vasilis%20Gkatzelis%20and%20Shahin%20Jabbari&entry.1292438233=%20%20Algorithmic%20recourse%20provides%20individuals%20who%20receive%20undesirable%20outcomes%0Afrom%20machine%20learning%20systems%20with%20minimum-cost%20improvements%20to%20achieve%20a%0Adesirable%20outcome.%20However%2C%20machine%20learning%20models%20often%20get%20updated%2C%20so%20the%0Arecourse%20may%20not%20lead%20to%20the%20desired%20outcome.%20The%20robust%20recourse%20framework%0Achooses%20recourses%20that%20are%20less%20sensitive%20to%20adversarial%20model%20changes%2C%20but%0Athis%20comes%20at%20a%20higher%20cost.%20To%20address%20this%2C%20we%20initiate%20the%20study%20of%0Alearning-augmented%20algorithmic%20recourse%20and%20evaluate%20the%20extent%20to%20which%20a%0Adesigner%20equipped%20with%20a%20prediction%20of%20the%20future%20model%20can%20reduce%20the%20cost%20of%0Arecourse%20when%20the%20prediction%20is%20accurate%20%28consistency%29%20while%20also%20limiting%20the%0Acost%20even%20when%20the%20prediction%20is%20inaccurate%20%28robustness%29.%20We%20propose%20a%20novel%0Aalgorithm%2C%20study%20the%20robustness-consistency%20trade-off%2C%20and%20analyze%20how%0Aprediction%20accuracy%20affects%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01580v2&entry.124074799=Read"},
{"title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault\n  Analysis", "author": "Jiongchi Yu and Weipeng Jiang and Xiaoyu Zhang and Qiang Hu and Xiaofei Xie and Chao Shen", "abstract": "  Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis.\n", "link": "http://arxiv.org/abs/2510.04997v1", "date": "2025-10-06", "relevancy": 1.2781, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4402}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoEmpirical%3A%20LLM-Based%20Automated%20Research%20for%20Empirical%20Software%20Fault%0A%20%20Analysis&body=Title%3A%20AutoEmpirical%3A%20LLM-Based%20Automated%20Research%20for%20Empirical%20Software%20Fault%0A%20%20Analysis%0AAuthor%3A%20Jiongchi%20Yu%20and%20Weipeng%20Jiang%20and%20Xiaoyu%20Zhang%20and%20Qiang%20Hu%20and%20Xiaofei%20Xie%20and%20Chao%20Shen%0AAbstract%3A%20%20%20Understanding%20software%20faults%20is%20essential%20for%20empirical%20research%20in%20software%0Adevelopment%20and%20maintenance.%20However%2C%20traditional%20fault%20analysis%2C%20while%0Avaluable%2C%20typically%20involves%20multiple%20expert-driven%20steps%20such%20as%20collecting%0Apotential%20faults%2C%20filtering%2C%20and%20manual%20investigation.%20These%20processes%20are%20both%0Alabor-intensive%20and%20time-consuming%2C%20creating%20bottlenecks%20that%20hinder%0Alarge-scale%20fault%20studies%20in%20complex%20yet%20critical%20software%20systems%20and%20slow%20the%0Apace%20of%20iterative%20empirical%20research.%0A%20%20In%20this%20paper%2C%20we%20decompose%20the%20process%20of%20empirical%20software%20fault%20study%0Ainto%20three%20key%20phases%3A%20%281%29%20research%20objective%20definition%2C%20%282%29%20data%20preparation%2C%0Aand%20%283%29%20fault%20analysis%2C%20and%20we%20conduct%20an%20initial%20exploration%20study%20of%20applying%0ALarge%20Language%20Models%20%28LLMs%29%20for%20fault%20analysis%20of%20open-source%20software.%0ASpecifically%2C%20we%20perform%20the%20evaluation%20on%203%2C829%20software%20faults%20drawn%20from%20a%0Ahigh-quality%20empirical%20study.%20Our%20results%20show%20that%20LLMs%20can%20substantially%0Aimprove%20efficiency%20in%20fault%20analysis%2C%20with%20an%20average%20processing%20time%20of%20about%0Atwo%20hours%2C%20compared%20to%20the%20weeks%20of%20manual%20effort%20typically%20required.%20We%0Aconclude%20by%20outlining%20a%20detailed%20research%20plan%20that%20highlights%20both%20the%0Apotential%20of%20LLMs%20for%20advancing%20empirical%20fault%20studies%20and%20the%20open%20challenges%0Athat%20required%20be%20addressed%20to%20achieve%20fully%20automated%2C%20end-to-end%20software%0Afault%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoEmpirical%253A%2520LLM-Based%2520Automated%2520Research%2520for%2520Empirical%2520Software%2520Fault%250A%2520%2520Analysis%26entry.906535625%3DJiongchi%2520Yu%2520and%2520Weipeng%2520Jiang%2520and%2520Xiaoyu%2520Zhang%2520and%2520Qiang%2520Hu%2520and%2520Xiaofei%2520Xie%2520and%2520Chao%2520Shen%26entry.1292438233%3D%2520%2520Understanding%2520software%2520faults%2520is%2520essential%2520for%2520empirical%2520research%2520in%2520software%250Adevelopment%2520and%2520maintenance.%2520However%252C%2520traditional%2520fault%2520analysis%252C%2520while%250Avaluable%252C%2520typically%2520involves%2520multiple%2520expert-driven%2520steps%2520such%2520as%2520collecting%250Apotential%2520faults%252C%2520filtering%252C%2520and%2520manual%2520investigation.%2520These%2520processes%2520are%2520both%250Alabor-intensive%2520and%2520time-consuming%252C%2520creating%2520bottlenecks%2520that%2520hinder%250Alarge-scale%2520fault%2520studies%2520in%2520complex%2520yet%2520critical%2520software%2520systems%2520and%2520slow%2520the%250Apace%2520of%2520iterative%2520empirical%2520research.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520decompose%2520the%2520process%2520of%2520empirical%2520software%2520fault%2520study%250Ainto%2520three%2520key%2520phases%253A%2520%25281%2529%2520research%2520objective%2520definition%252C%2520%25282%2529%2520data%2520preparation%252C%250Aand%2520%25283%2529%2520fault%2520analysis%252C%2520and%2520we%2520conduct%2520an%2520initial%2520exploration%2520study%2520of%2520applying%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520fault%2520analysis%2520of%2520open-source%2520software.%250ASpecifically%252C%2520we%2520perform%2520the%2520evaluation%2520on%25203%252C829%2520software%2520faults%2520drawn%2520from%2520a%250Ahigh-quality%2520empirical%2520study.%2520Our%2520results%2520show%2520that%2520LLMs%2520can%2520substantially%250Aimprove%2520efficiency%2520in%2520fault%2520analysis%252C%2520with%2520an%2520average%2520processing%2520time%2520of%2520about%250Atwo%2520hours%252C%2520compared%2520to%2520the%2520weeks%2520of%2520manual%2520effort%2520typically%2520required.%2520We%250Aconclude%2520by%2520outlining%2520a%2520detailed%2520research%2520plan%2520that%2520highlights%2520both%2520the%250Apotential%2520of%2520LLMs%2520for%2520advancing%2520empirical%2520fault%2520studies%2520and%2520the%2520open%2520challenges%250Athat%2520required%2520be%2520addressed%2520to%2520achieve%2520fully%2520automated%252C%2520end-to-end%2520software%250Afault%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoEmpirical%3A%20LLM-Based%20Automated%20Research%20for%20Empirical%20Software%20Fault%0A%20%20Analysis&entry.906535625=Jiongchi%20Yu%20and%20Weipeng%20Jiang%20and%20Xiaoyu%20Zhang%20and%20Qiang%20Hu%20and%20Xiaofei%20Xie%20and%20Chao%20Shen&entry.1292438233=%20%20Understanding%20software%20faults%20is%20essential%20for%20empirical%20research%20in%20software%0Adevelopment%20and%20maintenance.%20However%2C%20traditional%20fault%20analysis%2C%20while%0Avaluable%2C%20typically%20involves%20multiple%20expert-driven%20steps%20such%20as%20collecting%0Apotential%20faults%2C%20filtering%2C%20and%20manual%20investigation.%20These%20processes%20are%20both%0Alabor-intensive%20and%20time-consuming%2C%20creating%20bottlenecks%20that%20hinder%0Alarge-scale%20fault%20studies%20in%20complex%20yet%20critical%20software%20systems%20and%20slow%20the%0Apace%20of%20iterative%20empirical%20research.%0A%20%20In%20this%20paper%2C%20we%20decompose%20the%20process%20of%20empirical%20software%20fault%20study%0Ainto%20three%20key%20phases%3A%20%281%29%20research%20objective%20definition%2C%20%282%29%20data%20preparation%2C%0Aand%20%283%29%20fault%20analysis%2C%20and%20we%20conduct%20an%20initial%20exploration%20study%20of%20applying%0ALarge%20Language%20Models%20%28LLMs%29%20for%20fault%20analysis%20of%20open-source%20software.%0ASpecifically%2C%20we%20perform%20the%20evaluation%20on%203%2C829%20software%20faults%20drawn%20from%20a%0Ahigh-quality%20empirical%20study.%20Our%20results%20show%20that%20LLMs%20can%20substantially%0Aimprove%20efficiency%20in%20fault%20analysis%2C%20with%20an%20average%20processing%20time%20of%20about%0Atwo%20hours%2C%20compared%20to%20the%20weeks%20of%20manual%20effort%20typically%20required.%20We%0Aconclude%20by%20outlining%20a%20detailed%20research%20plan%20that%20highlights%20both%20the%0Apotential%20of%20LLMs%20for%20advancing%20empirical%20fault%20studies%20and%20the%20open%20challenges%0Athat%20required%20be%20addressed%20to%20achieve%20fully%20automated%2C%20end-to-end%20software%0Afault%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04997v1&entry.124074799=Read"},
{"title": "Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of\n  Multivariate Long Time Series Forecasting Models", "author": "Nick Jan\u00dfen and Melanie Schaller and Bodo Rosenhahn", "abstract": "  Understanding the robustness of deep learning models for multivariate\nlong-term time series forecasting (M-LTSF) remains challenging, as evaluations\ntypically rely on real-world datasets with unknown noise properties. We propose\na simulation-based evaluation framework that generates parameterizable\nsynthetic datasets, where each dataset instance corresponds to a different\nconfiguration of signal components, noise types, signal-to-noise ratios, and\nfrequency characteristics. These configurable components aim to model\nreal-world multivariate time series data without the ambiguity of unknown\nnoise. This framework enables fine-grained, systematic evaluation of M-LTSF\nmodels under controlled and diverse scenarios. We benchmark four representative\narchitectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear\n(linear), and Autoformer (decomposition-based). Our analysis reveals that all\nmodels degrade severely when lookback windows cannot capture complete periods\nof seasonal patters in the data. S-Mamba and Autoformer perform best on\nsawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.\nWhite and Brownian noise universally degrade performance with lower\nsignal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer\nshows seasonal-noise vulnerability. Further spectral analysis shows that\nS-Mamba and iTransformer achieve superior frequency reconstruction. This\ncontrolled approach, based on our synthetic and principle-driven testbed,\noffers deeper insights into model-specific strengths and limitations through\nthe aggregation of MSE scores and provides concrete guidance for model\nselection based on signal characteristics and noise conditions.\n", "link": "http://arxiv.org/abs/2510.04900v1", "date": "2025-10-06", "relevancy": 1.0114, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5305}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4933}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20M-LTSF%3A%20Frequency%20and%20Noise-Based%20Evaluation%20of%0A%20%20Multivariate%20Long%20Time%20Series%20Forecasting%20Models&body=Title%3A%20Benchmarking%20M-LTSF%3A%20Frequency%20and%20Noise-Based%20Evaluation%20of%0A%20%20Multivariate%20Long%20Time%20Series%20Forecasting%20Models%0AAuthor%3A%20Nick%20Jan%C3%9Fen%20and%20Melanie%20Schaller%20and%20Bodo%20Rosenhahn%0AAbstract%3A%20%20%20Understanding%20the%20robustness%20of%20deep%20learning%20models%20for%20multivariate%0Along-term%20time%20series%20forecasting%20%28M-LTSF%29%20remains%20challenging%2C%20as%20evaluations%0Atypically%20rely%20on%20real-world%20datasets%20with%20unknown%20noise%20properties.%20We%20propose%0Aa%20simulation-based%20evaluation%20framework%20that%20generates%20parameterizable%0Asynthetic%20datasets%2C%20where%20each%20dataset%20instance%20corresponds%20to%20a%20different%0Aconfiguration%20of%20signal%20components%2C%20noise%20types%2C%20signal-to-noise%20ratios%2C%20and%0Afrequency%20characteristics.%20These%20configurable%20components%20aim%20to%20model%0Areal-world%20multivariate%20time%20series%20data%20without%20the%20ambiguity%20of%20unknown%0Anoise.%20This%20framework%20enables%20fine-grained%2C%20systematic%20evaluation%20of%20M-LTSF%0Amodels%20under%20controlled%20and%20diverse%20scenarios.%20We%20benchmark%20four%20representative%0Aarchitectures%20S-Mamba%20%28state-space%29%2C%20iTransformer%20%28transformer-based%29%2C%20R-Linear%0A%28linear%29%2C%20and%20Autoformer%20%28decomposition-based%29.%20Our%20analysis%20reveals%20that%20all%0Amodels%20degrade%20severely%20when%20lookback%20windows%20cannot%20capture%20complete%20periods%0Aof%20seasonal%20patters%20in%20the%20data.%20S-Mamba%20and%20Autoformer%20perform%20best%20on%0Asawtooth%20patterns%2C%20while%20R-Linear%20and%20iTransformer%20favor%20sinusoidal%20signals.%0AWhite%20and%20Brownian%20noise%20universally%20degrade%20performance%20with%20lower%0Asignal-to-noise%20ratio%20while%20S-Mamba%20shows%20specific%20trend-noise%20and%20iTransformer%0Ashows%20seasonal-noise%20vulnerability.%20Further%20spectral%20analysis%20shows%20that%0AS-Mamba%20and%20iTransformer%20achieve%20superior%20frequency%20reconstruction.%20This%0Acontrolled%20approach%2C%20based%20on%20our%20synthetic%20and%20principle-driven%20testbed%2C%0Aoffers%20deeper%20insights%20into%20model-specific%20strengths%20and%20limitations%20through%0Athe%20aggregation%20of%20MSE%20scores%20and%20provides%20concrete%20guidance%20for%20model%0Aselection%20based%20on%20signal%20characteristics%20and%20noise%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520M-LTSF%253A%2520Frequency%2520and%2520Noise-Based%2520Evaluation%2520of%250A%2520%2520Multivariate%2520Long%2520Time%2520Series%2520Forecasting%2520Models%26entry.906535625%3DNick%2520Jan%25C3%259Fen%2520and%2520Melanie%2520Schaller%2520and%2520Bodo%2520Rosenhahn%26entry.1292438233%3D%2520%2520Understanding%2520the%2520robustness%2520of%2520deep%2520learning%2520models%2520for%2520multivariate%250Along-term%2520time%2520series%2520forecasting%2520%2528M-LTSF%2529%2520remains%2520challenging%252C%2520as%2520evaluations%250Atypically%2520rely%2520on%2520real-world%2520datasets%2520with%2520unknown%2520noise%2520properties.%2520We%2520propose%250Aa%2520simulation-based%2520evaluation%2520framework%2520that%2520generates%2520parameterizable%250Asynthetic%2520datasets%252C%2520where%2520each%2520dataset%2520instance%2520corresponds%2520to%2520a%2520different%250Aconfiguration%2520of%2520signal%2520components%252C%2520noise%2520types%252C%2520signal-to-noise%2520ratios%252C%2520and%250Afrequency%2520characteristics.%2520These%2520configurable%2520components%2520aim%2520to%2520model%250Areal-world%2520multivariate%2520time%2520series%2520data%2520without%2520the%2520ambiguity%2520of%2520unknown%250Anoise.%2520This%2520framework%2520enables%2520fine-grained%252C%2520systematic%2520evaluation%2520of%2520M-LTSF%250Amodels%2520under%2520controlled%2520and%2520diverse%2520scenarios.%2520We%2520benchmark%2520four%2520representative%250Aarchitectures%2520S-Mamba%2520%2528state-space%2529%252C%2520iTransformer%2520%2528transformer-based%2529%252C%2520R-Linear%250A%2528linear%2529%252C%2520and%2520Autoformer%2520%2528decomposition-based%2529.%2520Our%2520analysis%2520reveals%2520that%2520all%250Amodels%2520degrade%2520severely%2520when%2520lookback%2520windows%2520cannot%2520capture%2520complete%2520periods%250Aof%2520seasonal%2520patters%2520in%2520the%2520data.%2520S-Mamba%2520and%2520Autoformer%2520perform%2520best%2520on%250Asawtooth%2520patterns%252C%2520while%2520R-Linear%2520and%2520iTransformer%2520favor%2520sinusoidal%2520signals.%250AWhite%2520and%2520Brownian%2520noise%2520universally%2520degrade%2520performance%2520with%2520lower%250Asignal-to-noise%2520ratio%2520while%2520S-Mamba%2520shows%2520specific%2520trend-noise%2520and%2520iTransformer%250Ashows%2520seasonal-noise%2520vulnerability.%2520Further%2520spectral%2520analysis%2520shows%2520that%250AS-Mamba%2520and%2520iTransformer%2520achieve%2520superior%2520frequency%2520reconstruction.%2520This%250Acontrolled%2520approach%252C%2520based%2520on%2520our%2520synthetic%2520and%2520principle-driven%2520testbed%252C%250Aoffers%2520deeper%2520insights%2520into%2520model-specific%2520strengths%2520and%2520limitations%2520through%250Athe%2520aggregation%2520of%2520MSE%2520scores%2520and%2520provides%2520concrete%2520guidance%2520for%2520model%250Aselection%2520based%2520on%2520signal%2520characteristics%2520and%2520noise%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20M-LTSF%3A%20Frequency%20and%20Noise-Based%20Evaluation%20of%0A%20%20Multivariate%20Long%20Time%20Series%20Forecasting%20Models&entry.906535625=Nick%20Jan%C3%9Fen%20and%20Melanie%20Schaller%20and%20Bodo%20Rosenhahn&entry.1292438233=%20%20Understanding%20the%20robustness%20of%20deep%20learning%20models%20for%20multivariate%0Along-term%20time%20series%20forecasting%20%28M-LTSF%29%20remains%20challenging%2C%20as%20evaluations%0Atypically%20rely%20on%20real-world%20datasets%20with%20unknown%20noise%20properties.%20We%20propose%0Aa%20simulation-based%20evaluation%20framework%20that%20generates%20parameterizable%0Asynthetic%20datasets%2C%20where%20each%20dataset%20instance%20corresponds%20to%20a%20different%0Aconfiguration%20of%20signal%20components%2C%20noise%20types%2C%20signal-to-noise%20ratios%2C%20and%0Afrequency%20characteristics.%20These%20configurable%20components%20aim%20to%20model%0Areal-world%20multivariate%20time%20series%20data%20without%20the%20ambiguity%20of%20unknown%0Anoise.%20This%20framework%20enables%20fine-grained%2C%20systematic%20evaluation%20of%20M-LTSF%0Amodels%20under%20controlled%20and%20diverse%20scenarios.%20We%20benchmark%20four%20representative%0Aarchitectures%20S-Mamba%20%28state-space%29%2C%20iTransformer%20%28transformer-based%29%2C%20R-Linear%0A%28linear%29%2C%20and%20Autoformer%20%28decomposition-based%29.%20Our%20analysis%20reveals%20that%20all%0Amodels%20degrade%20severely%20when%20lookback%20windows%20cannot%20capture%20complete%20periods%0Aof%20seasonal%20patters%20in%20the%20data.%20S-Mamba%20and%20Autoformer%20perform%20best%20on%0Asawtooth%20patterns%2C%20while%20R-Linear%20and%20iTransformer%20favor%20sinusoidal%20signals.%0AWhite%20and%20Brownian%20noise%20universally%20degrade%20performance%20with%20lower%0Asignal-to-noise%20ratio%20while%20S-Mamba%20shows%20specific%20trend-noise%20and%20iTransformer%0Ashows%20seasonal-noise%20vulnerability.%20Further%20spectral%20analysis%20shows%20that%0AS-Mamba%20and%20iTransformer%20achieve%20superior%20frequency%20reconstruction.%20This%0Acontrolled%20approach%2C%20based%20on%20our%20synthetic%20and%20principle-driven%20testbed%2C%0Aoffers%20deeper%20insights%20into%20model-specific%20strengths%20and%20limitations%20through%0Athe%20aggregation%20of%20MSE%20scores%20and%20provides%20concrete%20guidance%20for%20model%0Aselection%20based%20on%20signal%20characteristics%20and%20noise%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04900v1&entry.124074799=Read"},
{"title": "In-Context Learning for Pure Exploration", "author": "Alessio Russo and Ryan Welch and Aldo Pacchiano", "abstract": "  We study the problem active sequential hypothesis testing, also known as pure\nexploration: given a new task, the learner adaptively collects data from the\nenvironment to efficiently determine an underlying correct hypothesis. A\nclassical instance of this problem is the task of identifying the best arm in a\nmulti-armed bandit problem (a.k.a. BAI, Best-Arm Identification), where actions\nindex hypotheses. Another important case is generalized search, a problem of\ndetermining the correct label through a sequence of strategically selected\nqueries that indirectly reveal information about the label. In this work, we\nintroduce In-Context Pure Exploration (ICPE), which meta-trains Transformers to\nmap observation histories to query actions and a predicted hypothesis, yielding\na model that transfers in-context. At inference time, ICPE actively gathers\nevidence on new tasks and infers the true hypothesis without parameter updates.\nAcross deterministic, stochastic, and structured benchmarks, including BAI and\ngeneralized search, ICPE is competitive with adaptive baselines while requiring\nno explicit modeling of information structure. Our results support Transformers\nas practical architectures for general sequential testing.\n", "link": "http://arxiv.org/abs/2506.01876v2", "date": "2025-10-06", "relevancy": 1.6712, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.608}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Learning%20for%20Pure%20Exploration&body=Title%3A%20In-Context%20Learning%20for%20Pure%20Exploration%0AAuthor%3A%20Alessio%20Russo%20and%20Ryan%20Welch%20and%20Aldo%20Pacchiano%0AAbstract%3A%20%20%20We%20study%20the%20problem%20active%20sequential%20hypothesis%20testing%2C%20also%20known%20as%20pure%0Aexploration%3A%20given%20a%20new%20task%2C%20the%20learner%20adaptively%20collects%20data%20from%20the%0Aenvironment%20to%20efficiently%20determine%20an%20underlying%20correct%20hypothesis.%20A%0Aclassical%20instance%20of%20this%20problem%20is%20the%20task%20of%20identifying%20the%20best%20arm%20in%20a%0Amulti-armed%20bandit%20problem%20%28a.k.a.%20BAI%2C%20Best-Arm%20Identification%29%2C%20where%20actions%0Aindex%20hypotheses.%20Another%20important%20case%20is%20generalized%20search%2C%20a%20problem%20of%0Adetermining%20the%20correct%20label%20through%20a%20sequence%20of%20strategically%20selected%0Aqueries%20that%20indirectly%20reveal%20information%20about%20the%20label.%20In%20this%20work%2C%20we%0Aintroduce%20In-Context%20Pure%20Exploration%20%28ICPE%29%2C%20which%20meta-trains%20Transformers%20to%0Amap%20observation%20histories%20to%20query%20actions%20and%20a%20predicted%20hypothesis%2C%20yielding%0Aa%20model%20that%20transfers%20in-context.%20At%20inference%20time%2C%20ICPE%20actively%20gathers%0Aevidence%20on%20new%20tasks%20and%20infers%20the%20true%20hypothesis%20without%20parameter%20updates.%0AAcross%20deterministic%2C%20stochastic%2C%20and%20structured%20benchmarks%2C%20including%20BAI%20and%0Ageneralized%20search%2C%20ICPE%20is%20competitive%20with%20adaptive%20baselines%20while%20requiring%0Ano%20explicit%20modeling%20of%20information%20structure.%20Our%20results%20support%20Transformers%0Aas%20practical%20architectures%20for%20general%20sequential%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01876v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Learning%2520for%2520Pure%2520Exploration%26entry.906535625%3DAlessio%2520Russo%2520and%2520Ryan%2520Welch%2520and%2520Aldo%2520Pacchiano%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520active%2520sequential%2520hypothesis%2520testing%252C%2520also%2520known%2520as%2520pure%250Aexploration%253A%2520given%2520a%2520new%2520task%252C%2520the%2520learner%2520adaptively%2520collects%2520data%2520from%2520the%250Aenvironment%2520to%2520efficiently%2520determine%2520an%2520underlying%2520correct%2520hypothesis.%2520A%250Aclassical%2520instance%2520of%2520this%2520problem%2520is%2520the%2520task%2520of%2520identifying%2520the%2520best%2520arm%2520in%2520a%250Amulti-armed%2520bandit%2520problem%2520%2528a.k.a.%2520BAI%252C%2520Best-Arm%2520Identification%2529%252C%2520where%2520actions%250Aindex%2520hypotheses.%2520Another%2520important%2520case%2520is%2520generalized%2520search%252C%2520a%2520problem%2520of%250Adetermining%2520the%2520correct%2520label%2520through%2520a%2520sequence%2520of%2520strategically%2520selected%250Aqueries%2520that%2520indirectly%2520reveal%2520information%2520about%2520the%2520label.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520In-Context%2520Pure%2520Exploration%2520%2528ICPE%2529%252C%2520which%2520meta-trains%2520Transformers%2520to%250Amap%2520observation%2520histories%2520to%2520query%2520actions%2520and%2520a%2520predicted%2520hypothesis%252C%2520yielding%250Aa%2520model%2520that%2520transfers%2520in-context.%2520At%2520inference%2520time%252C%2520ICPE%2520actively%2520gathers%250Aevidence%2520on%2520new%2520tasks%2520and%2520infers%2520the%2520true%2520hypothesis%2520without%2520parameter%2520updates.%250AAcross%2520deterministic%252C%2520stochastic%252C%2520and%2520structured%2520benchmarks%252C%2520including%2520BAI%2520and%250Ageneralized%2520search%252C%2520ICPE%2520is%2520competitive%2520with%2520adaptive%2520baselines%2520while%2520requiring%250Ano%2520explicit%2520modeling%2520of%2520information%2520structure.%2520Our%2520results%2520support%2520Transformers%250Aas%2520practical%2520architectures%2520for%2520general%2520sequential%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01876v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Learning%20for%20Pure%20Exploration&entry.906535625=Alessio%20Russo%20and%20Ryan%20Welch%20and%20Aldo%20Pacchiano&entry.1292438233=%20%20We%20study%20the%20problem%20active%20sequential%20hypothesis%20testing%2C%20also%20known%20as%20pure%0Aexploration%3A%20given%20a%20new%20task%2C%20the%20learner%20adaptively%20collects%20data%20from%20the%0Aenvironment%20to%20efficiently%20determine%20an%20underlying%20correct%20hypothesis.%20A%0Aclassical%20instance%20of%20this%20problem%20is%20the%20task%20of%20identifying%20the%20best%20arm%20in%20a%0Amulti-armed%20bandit%20problem%20%28a.k.a.%20BAI%2C%20Best-Arm%20Identification%29%2C%20where%20actions%0Aindex%20hypotheses.%20Another%20important%20case%20is%20generalized%20search%2C%20a%20problem%20of%0Adetermining%20the%20correct%20label%20through%20a%20sequence%20of%20strategically%20selected%0Aqueries%20that%20indirectly%20reveal%20information%20about%20the%20label.%20In%20this%20work%2C%20we%0Aintroduce%20In-Context%20Pure%20Exploration%20%28ICPE%29%2C%20which%20meta-trains%20Transformers%20to%0Amap%20observation%20histories%20to%20query%20actions%20and%20a%20predicted%20hypothesis%2C%20yielding%0Aa%20model%20that%20transfers%20in-context.%20At%20inference%20time%2C%20ICPE%20actively%20gathers%0Aevidence%20on%20new%20tasks%20and%20infers%20the%20true%20hypothesis%20without%20parameter%20updates.%0AAcross%20deterministic%2C%20stochastic%2C%20and%20structured%20benchmarks%2C%20including%20BAI%20and%0Ageneralized%20search%2C%20ICPE%20is%20competitive%20with%20adaptive%20baselines%20while%20requiring%0Ano%20explicit%20modeling%20of%20information%20structure.%20Our%20results%20support%20Transformers%0Aas%20practical%20architectures%20for%20general%20sequential%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01876v2&entry.124074799=Read"},
{"title": "Feasibility-Aware Decision-Focused Learning for Predicting Parameters in\n  the Constraints", "author": "Jayanta Mandi and Marianne Defresne and Senne Berden and Tias Guns", "abstract": "  When some parameters of a constrained optimization problem (COP) are\nuncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising\ntwo stages -- the prediction of the unknown parameters from contextual\ninformation and the subsequent optimization using those predicted parameters.\nDecision-focused learning (DFL) implements the first stage by training a\nmachine learning (ML) model to optimize the quality of the decisions made using\nthe predicted parameters. When parameters in the constraints of a COP are\npredicted, the predicted parameters can lead to infeasible solutions.\nTherefore, it is important to simultaneously manage both feasibility and\ndecision quality. We develop a DFL framework for predicting constraint\nparameters in a generic COP. While prior works typically assume that the\nunderlying optimization problem is a linear program (LP) or integer linear\nprogram (ILP), our approach makes no such assumption. We derive two novel loss\nfunctions based on maximum likelihood estimation (MLE): the first one penalizes\ninfeasibility (by penalizing when the predicted parameters lead to infeasible\nsolutions), and the second one penalizes suboptimal decisions (by penalizing\nwhen the true optimal solution is infeasible under the predicted parameters).\nWe introduce a single tunable parameter to form a weighted average of the two\nlosses, allowing decision-makers to balance suboptimality and feasibility. We\nexperimentally demonstrate that adjusting this parameter provides a\ndecision-maker the control over the trade-off between the two. Moreover, across\nseveral COP instances, we find that for a single value of the tunable\nparameter, our method matches the performance of the existing baselines on\nsuboptimality and feasibility.\n", "link": "http://arxiv.org/abs/2510.04951v1", "date": "2025-10-06", "relevancy": 1.4609, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5222}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4803}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feasibility-Aware%20Decision-Focused%20Learning%20for%20Predicting%20Parameters%20in%0A%20%20the%20Constraints&body=Title%3A%20Feasibility-Aware%20Decision-Focused%20Learning%20for%20Predicting%20Parameters%20in%0A%20%20the%20Constraints%0AAuthor%3A%20Jayanta%20Mandi%20and%20Marianne%20Defresne%20and%20Senne%20Berden%20and%20Tias%20Guns%0AAbstract%3A%20%20%20When%20some%20parameters%20of%20a%20constrained%20optimization%20problem%20%28COP%29%20are%0Auncertain%2C%20this%20gives%20rise%20to%20a%20predict-then-optimize%20%28PtO%29%20problem%2C%20comprising%0Atwo%20stages%20--%20the%20prediction%20of%20the%20unknown%20parameters%20from%20contextual%0Ainformation%20and%20the%20subsequent%20optimization%20using%20those%20predicted%20parameters.%0ADecision-focused%20learning%20%28DFL%29%20implements%20the%20first%20stage%20by%20training%20a%0Amachine%20learning%20%28ML%29%20model%20to%20optimize%20the%20quality%20of%20the%20decisions%20made%20using%0Athe%20predicted%20parameters.%20When%20parameters%20in%20the%20constraints%20of%20a%20COP%20are%0Apredicted%2C%20the%20predicted%20parameters%20can%20lead%20to%20infeasible%20solutions.%0ATherefore%2C%20it%20is%20important%20to%20simultaneously%20manage%20both%20feasibility%20and%0Adecision%20quality.%20We%20develop%20a%20DFL%20framework%20for%20predicting%20constraint%0Aparameters%20in%20a%20generic%20COP.%20While%20prior%20works%20typically%20assume%20that%20the%0Aunderlying%20optimization%20problem%20is%20a%20linear%20program%20%28LP%29%20or%20integer%20linear%0Aprogram%20%28ILP%29%2C%20our%20approach%20makes%20no%20such%20assumption.%20We%20derive%20two%20novel%20loss%0Afunctions%20based%20on%20maximum%20likelihood%20estimation%20%28MLE%29%3A%20the%20first%20one%20penalizes%0Ainfeasibility%20%28by%20penalizing%20when%20the%20predicted%20parameters%20lead%20to%20infeasible%0Asolutions%29%2C%20and%20the%20second%20one%20penalizes%20suboptimal%20decisions%20%28by%20penalizing%0Awhen%20the%20true%20optimal%20solution%20is%20infeasible%20under%20the%20predicted%20parameters%29.%0AWe%20introduce%20a%20single%20tunable%20parameter%20to%20form%20a%20weighted%20average%20of%20the%20two%0Alosses%2C%20allowing%20decision-makers%20to%20balance%20suboptimality%20and%20feasibility.%20We%0Aexperimentally%20demonstrate%20that%20adjusting%20this%20parameter%20provides%20a%0Adecision-maker%20the%20control%20over%20the%20trade-off%20between%20the%20two.%20Moreover%2C%20across%0Aseveral%20COP%20instances%2C%20we%20find%20that%20for%20a%20single%20value%20of%20the%20tunable%0Aparameter%2C%20our%20method%20matches%20the%20performance%20of%20the%20existing%20baselines%20on%0Asuboptimality%20and%20feasibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeasibility-Aware%2520Decision-Focused%2520Learning%2520for%2520Predicting%2520Parameters%2520in%250A%2520%2520the%2520Constraints%26entry.906535625%3DJayanta%2520Mandi%2520and%2520Marianne%2520Defresne%2520and%2520Senne%2520Berden%2520and%2520Tias%2520Guns%26entry.1292438233%3D%2520%2520When%2520some%2520parameters%2520of%2520a%2520constrained%2520optimization%2520problem%2520%2528COP%2529%2520are%250Auncertain%252C%2520this%2520gives%2520rise%2520to%2520a%2520predict-then-optimize%2520%2528PtO%2529%2520problem%252C%2520comprising%250Atwo%2520stages%2520--%2520the%2520prediction%2520of%2520the%2520unknown%2520parameters%2520from%2520contextual%250Ainformation%2520and%2520the%2520subsequent%2520optimization%2520using%2520those%2520predicted%2520parameters.%250ADecision-focused%2520learning%2520%2528DFL%2529%2520implements%2520the%2520first%2520stage%2520by%2520training%2520a%250Amachine%2520learning%2520%2528ML%2529%2520model%2520to%2520optimize%2520the%2520quality%2520of%2520the%2520decisions%2520made%2520using%250Athe%2520predicted%2520parameters.%2520When%2520parameters%2520in%2520the%2520constraints%2520of%2520a%2520COP%2520are%250Apredicted%252C%2520the%2520predicted%2520parameters%2520can%2520lead%2520to%2520infeasible%2520solutions.%250ATherefore%252C%2520it%2520is%2520important%2520to%2520simultaneously%2520manage%2520both%2520feasibility%2520and%250Adecision%2520quality.%2520We%2520develop%2520a%2520DFL%2520framework%2520for%2520predicting%2520constraint%250Aparameters%2520in%2520a%2520generic%2520COP.%2520While%2520prior%2520works%2520typically%2520assume%2520that%2520the%250Aunderlying%2520optimization%2520problem%2520is%2520a%2520linear%2520program%2520%2528LP%2529%2520or%2520integer%2520linear%250Aprogram%2520%2528ILP%2529%252C%2520our%2520approach%2520makes%2520no%2520such%2520assumption.%2520We%2520derive%2520two%2520novel%2520loss%250Afunctions%2520based%2520on%2520maximum%2520likelihood%2520estimation%2520%2528MLE%2529%253A%2520the%2520first%2520one%2520penalizes%250Ainfeasibility%2520%2528by%2520penalizing%2520when%2520the%2520predicted%2520parameters%2520lead%2520to%2520infeasible%250Asolutions%2529%252C%2520and%2520the%2520second%2520one%2520penalizes%2520suboptimal%2520decisions%2520%2528by%2520penalizing%250Awhen%2520the%2520true%2520optimal%2520solution%2520is%2520infeasible%2520under%2520the%2520predicted%2520parameters%2529.%250AWe%2520introduce%2520a%2520single%2520tunable%2520parameter%2520to%2520form%2520a%2520weighted%2520average%2520of%2520the%2520two%250Alosses%252C%2520allowing%2520decision-makers%2520to%2520balance%2520suboptimality%2520and%2520feasibility.%2520We%250Aexperimentally%2520demonstrate%2520that%2520adjusting%2520this%2520parameter%2520provides%2520a%250Adecision-maker%2520the%2520control%2520over%2520the%2520trade-off%2520between%2520the%2520two.%2520Moreover%252C%2520across%250Aseveral%2520COP%2520instances%252C%2520we%2520find%2520that%2520for%2520a%2520single%2520value%2520of%2520the%2520tunable%250Aparameter%252C%2520our%2520method%2520matches%2520the%2520performance%2520of%2520the%2520existing%2520baselines%2520on%250Asuboptimality%2520and%2520feasibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feasibility-Aware%20Decision-Focused%20Learning%20for%20Predicting%20Parameters%20in%0A%20%20the%20Constraints&entry.906535625=Jayanta%20Mandi%20and%20Marianne%20Defresne%20and%20Senne%20Berden%20and%20Tias%20Guns&entry.1292438233=%20%20When%20some%20parameters%20of%20a%20constrained%20optimization%20problem%20%28COP%29%20are%0Auncertain%2C%20this%20gives%20rise%20to%20a%20predict-then-optimize%20%28PtO%29%20problem%2C%20comprising%0Atwo%20stages%20--%20the%20prediction%20of%20the%20unknown%20parameters%20from%20contextual%0Ainformation%20and%20the%20subsequent%20optimization%20using%20those%20predicted%20parameters.%0ADecision-focused%20learning%20%28DFL%29%20implements%20the%20first%20stage%20by%20training%20a%0Amachine%20learning%20%28ML%29%20model%20to%20optimize%20the%20quality%20of%20the%20decisions%20made%20using%0Athe%20predicted%20parameters.%20When%20parameters%20in%20the%20constraints%20of%20a%20COP%20are%0Apredicted%2C%20the%20predicted%20parameters%20can%20lead%20to%20infeasible%20solutions.%0ATherefore%2C%20it%20is%20important%20to%20simultaneously%20manage%20both%20feasibility%20and%0Adecision%20quality.%20We%20develop%20a%20DFL%20framework%20for%20predicting%20constraint%0Aparameters%20in%20a%20generic%20COP.%20While%20prior%20works%20typically%20assume%20that%20the%0Aunderlying%20optimization%20problem%20is%20a%20linear%20program%20%28LP%29%20or%20integer%20linear%0Aprogram%20%28ILP%29%2C%20our%20approach%20makes%20no%20such%20assumption.%20We%20derive%20two%20novel%20loss%0Afunctions%20based%20on%20maximum%20likelihood%20estimation%20%28MLE%29%3A%20the%20first%20one%20penalizes%0Ainfeasibility%20%28by%20penalizing%20when%20the%20predicted%20parameters%20lead%20to%20infeasible%0Asolutions%29%2C%20and%20the%20second%20one%20penalizes%20suboptimal%20decisions%20%28by%20penalizing%0Awhen%20the%20true%20optimal%20solution%20is%20infeasible%20under%20the%20predicted%20parameters%29.%0AWe%20introduce%20a%20single%20tunable%20parameter%20to%20form%20a%20weighted%20average%20of%20the%20two%0Alosses%2C%20allowing%20decision-makers%20to%20balance%20suboptimality%20and%20feasibility.%20We%0Aexperimentally%20demonstrate%20that%20adjusting%20this%20parameter%20provides%20a%0Adecision-maker%20the%20control%20over%20the%20trade-off%20between%20the%20two.%20Moreover%2C%20across%0Aseveral%20COP%20instances%2C%20we%20find%20that%20for%20a%20single%20value%20of%20the%20tunable%0Aparameter%2C%20our%20method%20matches%20the%20performance%20of%20the%20existing%20baselines%20on%0Asuboptimality%20and%20feasibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04951v1&entry.124074799=Read"},
{"title": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful\n  Requests", "author": "Punya Syon Pandey and Hai Son Le and Devansh Bhardwaj and Rada Mihalcea and Zhijing Jin", "abstract": "  Large language models (LLMs) are increasingly deployed in contexts where\ntheir failures can have direct sociopolitical consequences. Yet, existing\nsafety benchmarks rarely test vulnerabilities in domains such as political\nmanipulation, propaganda and disinformation generation, or surveillance and\ninformation control. We introduce SocialHarmBench, a dataset of 585 prompts\nspanning 7 sociopolitical categories and 34 countries, designed to surface\nwhere LLMs most acutely fail in politically charged contexts. Our evaluations\nreveal several shortcomings: open-weight models exhibit high vulnerability to\nharmful compliance, with Mistral-7B reaching attack success rates as high as\n97% to 98% in domains such as historical revisionism, propaganda, and political\nmanipulation. Moreover, temporal and geographic analyses show that LLMs are\nmost fragile when confronted with 21st-century or pre-20th-century contexts,\nand when responding to prompts tied to regions such as Latin America, the USA,\nand the UK. These findings demonstrate that current safeguards fail to\ngeneralize to high-stakes sociopolitical settings, exposing systematic biases\nand raising concerns about the reliability of LLMs in preserving human rights\nand democratic values. We share the SocialHarmBench benchmark at\nhttps://huggingface.co/datasets/psyonp/SocialHarmBench.\n", "link": "http://arxiv.org/abs/2510.04891v1", "date": "2025-10-06", "relevancy": 1.712, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4415}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SocialHarmBench%3A%20Revealing%20LLM%20Vulnerabilities%20to%20Socially%20Harmful%0A%20%20Requests&body=Title%3A%20SocialHarmBench%3A%20Revealing%20LLM%20Vulnerabilities%20to%20Socially%20Harmful%0A%20%20Requests%0AAuthor%3A%20Punya%20Syon%20Pandey%20and%20Hai%20Son%20Le%20and%20Devansh%20Bhardwaj%20and%20Rada%20Mihalcea%20and%20Zhijing%20Jin%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20contexts%20where%0Atheir%20failures%20can%20have%20direct%20sociopolitical%20consequences.%20Yet%2C%20existing%0Asafety%20benchmarks%20rarely%20test%20vulnerabilities%20in%20domains%20such%20as%20political%0Amanipulation%2C%20propaganda%20and%20disinformation%20generation%2C%20or%20surveillance%20and%0Ainformation%20control.%20We%20introduce%20SocialHarmBench%2C%20a%20dataset%20of%20585%20prompts%0Aspanning%207%20sociopolitical%20categories%20and%2034%20countries%2C%20designed%20to%20surface%0Awhere%20LLMs%20most%20acutely%20fail%20in%20politically%20charged%20contexts.%20Our%20evaluations%0Areveal%20several%20shortcomings%3A%20open-weight%20models%20exhibit%20high%20vulnerability%20to%0Aharmful%20compliance%2C%20with%20Mistral-7B%20reaching%20attack%20success%20rates%20as%20high%20as%0A97%25%20to%2098%25%20in%20domains%20such%20as%20historical%20revisionism%2C%20propaganda%2C%20and%20political%0Amanipulation.%20Moreover%2C%20temporal%20and%20geographic%20analyses%20show%20that%20LLMs%20are%0Amost%20fragile%20when%20confronted%20with%2021st-century%20or%20pre-20th-century%20contexts%2C%0Aand%20when%20responding%20to%20prompts%20tied%20to%20regions%20such%20as%20Latin%20America%2C%20the%20USA%2C%0Aand%20the%20UK.%20These%20findings%20demonstrate%20that%20current%20safeguards%20fail%20to%0Ageneralize%20to%20high-stakes%20sociopolitical%20settings%2C%20exposing%20systematic%20biases%0Aand%20raising%20concerns%20about%20the%20reliability%20of%20LLMs%20in%20preserving%20human%20rights%0Aand%20democratic%20values.%20We%20share%20the%20SocialHarmBench%20benchmark%20at%0Ahttps%3A//huggingface.co/datasets/psyonp/SocialHarmBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocialHarmBench%253A%2520Revealing%2520LLM%2520Vulnerabilities%2520to%2520Socially%2520Harmful%250A%2520%2520Requests%26entry.906535625%3DPunya%2520Syon%2520Pandey%2520and%2520Hai%2520Son%2520Le%2520and%2520Devansh%2520Bhardwaj%2520and%2520Rada%2520Mihalcea%2520and%2520Zhijing%2520Jin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520contexts%2520where%250Atheir%2520failures%2520can%2520have%2520direct%2520sociopolitical%2520consequences.%2520Yet%252C%2520existing%250Asafety%2520benchmarks%2520rarely%2520test%2520vulnerabilities%2520in%2520domains%2520such%2520as%2520political%250Amanipulation%252C%2520propaganda%2520and%2520disinformation%2520generation%252C%2520or%2520surveillance%2520and%250Ainformation%2520control.%2520We%2520introduce%2520SocialHarmBench%252C%2520a%2520dataset%2520of%2520585%2520prompts%250Aspanning%25207%2520sociopolitical%2520categories%2520and%252034%2520countries%252C%2520designed%2520to%2520surface%250Awhere%2520LLMs%2520most%2520acutely%2520fail%2520in%2520politically%2520charged%2520contexts.%2520Our%2520evaluations%250Areveal%2520several%2520shortcomings%253A%2520open-weight%2520models%2520exhibit%2520high%2520vulnerability%2520to%250Aharmful%2520compliance%252C%2520with%2520Mistral-7B%2520reaching%2520attack%2520success%2520rates%2520as%2520high%2520as%250A97%2525%2520to%252098%2525%2520in%2520domains%2520such%2520as%2520historical%2520revisionism%252C%2520propaganda%252C%2520and%2520political%250Amanipulation.%2520Moreover%252C%2520temporal%2520and%2520geographic%2520analyses%2520show%2520that%2520LLMs%2520are%250Amost%2520fragile%2520when%2520confronted%2520with%252021st-century%2520or%2520pre-20th-century%2520contexts%252C%250Aand%2520when%2520responding%2520to%2520prompts%2520tied%2520to%2520regions%2520such%2520as%2520Latin%2520America%252C%2520the%2520USA%252C%250Aand%2520the%2520UK.%2520These%2520findings%2520demonstrate%2520that%2520current%2520safeguards%2520fail%2520to%250Ageneralize%2520to%2520high-stakes%2520sociopolitical%2520settings%252C%2520exposing%2520systematic%2520biases%250Aand%2520raising%2520concerns%2520about%2520the%2520reliability%2520of%2520LLMs%2520in%2520preserving%2520human%2520rights%250Aand%2520democratic%2520values.%2520We%2520share%2520the%2520SocialHarmBench%2520benchmark%2520at%250Ahttps%253A//huggingface.co/datasets/psyonp/SocialHarmBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SocialHarmBench%3A%20Revealing%20LLM%20Vulnerabilities%20to%20Socially%20Harmful%0A%20%20Requests&entry.906535625=Punya%20Syon%20Pandey%20and%20Hai%20Son%20Le%20and%20Devansh%20Bhardwaj%20and%20Rada%20Mihalcea%20and%20Zhijing%20Jin&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20contexts%20where%0Atheir%20failures%20can%20have%20direct%20sociopolitical%20consequences.%20Yet%2C%20existing%0Asafety%20benchmarks%20rarely%20test%20vulnerabilities%20in%20domains%20such%20as%20political%0Amanipulation%2C%20propaganda%20and%20disinformation%20generation%2C%20or%20surveillance%20and%0Ainformation%20control.%20We%20introduce%20SocialHarmBench%2C%20a%20dataset%20of%20585%20prompts%0Aspanning%207%20sociopolitical%20categories%20and%2034%20countries%2C%20designed%20to%20surface%0Awhere%20LLMs%20most%20acutely%20fail%20in%20politically%20charged%20contexts.%20Our%20evaluations%0Areveal%20several%20shortcomings%3A%20open-weight%20models%20exhibit%20high%20vulnerability%20to%0Aharmful%20compliance%2C%20with%20Mistral-7B%20reaching%20attack%20success%20rates%20as%20high%20as%0A97%25%20to%2098%25%20in%20domains%20such%20as%20historical%20revisionism%2C%20propaganda%2C%20and%20political%0Amanipulation.%20Moreover%2C%20temporal%20and%20geographic%20analyses%20show%20that%20LLMs%20are%0Amost%20fragile%20when%20confronted%20with%2021st-century%20or%20pre-20th-century%20contexts%2C%0Aand%20when%20responding%20to%20prompts%20tied%20to%20regions%20such%20as%20Latin%20America%2C%20the%20USA%2C%0Aand%20the%20UK.%20These%20findings%20demonstrate%20that%20current%20safeguards%20fail%20to%0Ageneralize%20to%20high-stakes%20sociopolitical%20settings%2C%20exposing%20systematic%20biases%0Aand%20raising%20concerns%20about%20the%20reliability%20of%20LLMs%20in%20preserving%20human%20rights%0Aand%20democratic%20values.%20We%20share%20the%20SocialHarmBench%20benchmark%20at%0Ahttps%3A//huggingface.co/datasets/psyonp/SocialHarmBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04891v1&entry.124074799=Read"},
{"title": "HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a\n  Single Hybrid Model", "author": "Peter Van Katwyk and Karianne J. Bergen", "abstract": "  Uncertainty quantification is critical for ensuring robustness in high-stakes\nmachine learning applications. We introduce HybridFlow, a modular hybrid\narchitecture that unifies the modeling of aleatoric and epistemic uncertainty\nby combining a Conditional Masked Autoregressive normalizing flow for\nestimating aleatoric uncertainty with a flexible probabilistic predictor for\nepistemic uncertainty. The framework supports integration with any\nprobabilistic model class, allowing users to easily adapt HybridFlow to\nexisting architectures without sacrificing predictive performance. HybridFlow\nimproves upon previous uncertainty quantification frameworks across a range of\nregression tasks, such as depth estimation, a collection of regression\nbenchmarks, and a scientific case study of ice sheet emulation. We also provide\nempirical results of the quantified uncertainty, showing that the uncertainty\nquantified by HybridFlow is calibrated and better aligns with model error than\nexisting methods for quantifying aleatoric and epistemic uncertainty.\nHybridFlow addresses a key challenge in Bayesian deep learning, unifying\naleatoric and epistemic uncertainty modeling in a single robust framework.\n", "link": "http://arxiv.org/abs/2510.05054v1", "date": "2025-10-06", "relevancy": 1.6392, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5592}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5477}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HybridFlow%3A%20Quantification%20of%20Aleatoric%20and%20Epistemic%20Uncertainty%20with%20a%0A%20%20Single%20Hybrid%20Model&body=Title%3A%20HybridFlow%3A%20Quantification%20of%20Aleatoric%20and%20Epistemic%20Uncertainty%20with%20a%0A%20%20Single%20Hybrid%20Model%0AAuthor%3A%20Peter%20Van%20Katwyk%20and%20Karianne%20J.%20Bergen%0AAbstract%3A%20%20%20Uncertainty%20quantification%20is%20critical%20for%20ensuring%20robustness%20in%20high-stakes%0Amachine%20learning%20applications.%20We%20introduce%20HybridFlow%2C%20a%20modular%20hybrid%0Aarchitecture%20that%20unifies%20the%20modeling%20of%20aleatoric%20and%20epistemic%20uncertainty%0Aby%20combining%20a%20Conditional%20Masked%20Autoregressive%20normalizing%20flow%20for%0Aestimating%20aleatoric%20uncertainty%20with%20a%20flexible%20probabilistic%20predictor%20for%0Aepistemic%20uncertainty.%20The%20framework%20supports%20integration%20with%20any%0Aprobabilistic%20model%20class%2C%20allowing%20users%20to%20easily%20adapt%20HybridFlow%20to%0Aexisting%20architectures%20without%20sacrificing%20predictive%20performance.%20HybridFlow%0Aimproves%20upon%20previous%20uncertainty%20quantification%20frameworks%20across%20a%20range%20of%0Aregression%20tasks%2C%20such%20as%20depth%20estimation%2C%20a%20collection%20of%20regression%0Abenchmarks%2C%20and%20a%20scientific%20case%20study%20of%20ice%20sheet%20emulation.%20We%20also%20provide%0Aempirical%20results%20of%20the%20quantified%20uncertainty%2C%20showing%20that%20the%20uncertainty%0Aquantified%20by%20HybridFlow%20is%20calibrated%20and%20better%20aligns%20with%20model%20error%20than%0Aexisting%20methods%20for%20quantifying%20aleatoric%20and%20epistemic%20uncertainty.%0AHybridFlow%20addresses%20a%20key%20challenge%20in%20Bayesian%20deep%20learning%2C%20unifying%0Aaleatoric%20and%20epistemic%20uncertainty%20modeling%20in%20a%20single%20robust%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybridFlow%253A%2520Quantification%2520of%2520Aleatoric%2520and%2520Epistemic%2520Uncertainty%2520with%2520a%250A%2520%2520Single%2520Hybrid%2520Model%26entry.906535625%3DPeter%2520Van%2520Katwyk%2520and%2520Karianne%2520J.%2520Bergen%26entry.1292438233%3D%2520%2520Uncertainty%2520quantification%2520is%2520critical%2520for%2520ensuring%2520robustness%2520in%2520high-stakes%250Amachine%2520learning%2520applications.%2520We%2520introduce%2520HybridFlow%252C%2520a%2520modular%2520hybrid%250Aarchitecture%2520that%2520unifies%2520the%2520modeling%2520of%2520aleatoric%2520and%2520epistemic%2520uncertainty%250Aby%2520combining%2520a%2520Conditional%2520Masked%2520Autoregressive%2520normalizing%2520flow%2520for%250Aestimating%2520aleatoric%2520uncertainty%2520with%2520a%2520flexible%2520probabilistic%2520predictor%2520for%250Aepistemic%2520uncertainty.%2520The%2520framework%2520supports%2520integration%2520with%2520any%250Aprobabilistic%2520model%2520class%252C%2520allowing%2520users%2520to%2520easily%2520adapt%2520HybridFlow%2520to%250Aexisting%2520architectures%2520without%2520sacrificing%2520predictive%2520performance.%2520HybridFlow%250Aimproves%2520upon%2520previous%2520uncertainty%2520quantification%2520frameworks%2520across%2520a%2520range%2520of%250Aregression%2520tasks%252C%2520such%2520as%2520depth%2520estimation%252C%2520a%2520collection%2520of%2520regression%250Abenchmarks%252C%2520and%2520a%2520scientific%2520case%2520study%2520of%2520ice%2520sheet%2520emulation.%2520We%2520also%2520provide%250Aempirical%2520results%2520of%2520the%2520quantified%2520uncertainty%252C%2520showing%2520that%2520the%2520uncertainty%250Aquantified%2520by%2520HybridFlow%2520is%2520calibrated%2520and%2520better%2520aligns%2520with%2520model%2520error%2520than%250Aexisting%2520methods%2520for%2520quantifying%2520aleatoric%2520and%2520epistemic%2520uncertainty.%250AHybridFlow%2520addresses%2520a%2520key%2520challenge%2520in%2520Bayesian%2520deep%2520learning%252C%2520unifying%250Aaleatoric%2520and%2520epistemic%2520uncertainty%2520modeling%2520in%2520a%2520single%2520robust%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HybridFlow%3A%20Quantification%20of%20Aleatoric%20and%20Epistemic%20Uncertainty%20with%20a%0A%20%20Single%20Hybrid%20Model&entry.906535625=Peter%20Van%20Katwyk%20and%20Karianne%20J.%20Bergen&entry.1292438233=%20%20Uncertainty%20quantification%20is%20critical%20for%20ensuring%20robustness%20in%20high-stakes%0Amachine%20learning%20applications.%20We%20introduce%20HybridFlow%2C%20a%20modular%20hybrid%0Aarchitecture%20that%20unifies%20the%20modeling%20of%20aleatoric%20and%20epistemic%20uncertainty%0Aby%20combining%20a%20Conditional%20Masked%20Autoregressive%20normalizing%20flow%20for%0Aestimating%20aleatoric%20uncertainty%20with%20a%20flexible%20probabilistic%20predictor%20for%0Aepistemic%20uncertainty.%20The%20framework%20supports%20integration%20with%20any%0Aprobabilistic%20model%20class%2C%20allowing%20users%20to%20easily%20adapt%20HybridFlow%20to%0Aexisting%20architectures%20without%20sacrificing%20predictive%20performance.%20HybridFlow%0Aimproves%20upon%20previous%20uncertainty%20quantification%20frameworks%20across%20a%20range%20of%0Aregression%20tasks%2C%20such%20as%20depth%20estimation%2C%20a%20collection%20of%20regression%0Abenchmarks%2C%20and%20a%20scientific%20case%20study%20of%20ice%20sheet%20emulation.%20We%20also%20provide%0Aempirical%20results%20of%20the%20quantified%20uncertainty%2C%20showing%20that%20the%20uncertainty%0Aquantified%20by%20HybridFlow%20is%20calibrated%20and%20better%20aligns%20with%20model%20error%20than%0Aexisting%20methods%20for%20quantifying%20aleatoric%20and%20epistemic%20uncertainty.%0AHybridFlow%20addresses%20a%20key%20challenge%20in%20Bayesian%20deep%20learning%2C%20unifying%0Aaleatoric%20and%20epistemic%20uncertainty%20modeling%20in%20a%20single%20robust%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05054v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


