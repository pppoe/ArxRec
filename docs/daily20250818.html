<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250817.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head\n  Reconstruction and Real-Time Editing", "author": "Heyi Sun and Cong Wang and Tian-Xing Xu and Jingwei Huang and Di Kang and Chunchao Guo and Song-Hai Zhang", "abstract": "  Creating high-fidelity and editable head avatars is a pivotal challenge in\ncomputer vision and graphics, boosting many AR/VR applications. While recent\nadvancements have achieved photorealistic renderings and plausible animation,\nhead editing, especially real-time appearance editing, remains challenging due\nto the implicit representation and entangled modeling of the geometry and\nglobal appearance. To address this, we propose Surface-Volumetric Gaussian Head\nAvatar (SVG-Head), a novel hybrid representation that explicitly models the\ngeometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled\ntexture images to capture the global appearance. Technically, it contains two\ntypes of Gaussians, in which surface Gaussians explicitly model the appearance\nof head avatars using learnable texture images, facilitating real-time texture\nediting, while volumetric Gaussians enhance the reconstruction quality of\nnon-Lambertian regions (e.g., lips and hair). To model the correspondence\nbetween 3D world and texture space, we provide a mesh-aware Gaussian UV mapping\nmethod, which leverages UV coordinates given by the FLAME mesh to obtain sharp\ntexture images and real-time rendering speed. A hierarchical optimization\nstrategy is further designed to pursue the optimal performance in both\nreconstruction quality and editing flexibility. Experiments on the NeRSemble\ndataset show that SVG-Head not only generates high-fidelity rendering results,\nbut also is the first method to obtain explicit texture images for Gaussian\nhead avatars and support real-time appearance editing.\n", "link": "http://arxiv.org/abs/2508.09597v2", "date": "2025-08-15", "relevancy": 3.6558, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7631}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7631}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVG-Head%3A%20Hybrid%20Surface-Volumetric%20Gaussians%20for%20High-Fidelity%20Head%0A%20%20Reconstruction%20and%20Real-Time%20Editing&body=Title%3A%20SVG-Head%3A%20Hybrid%20Surface-Volumetric%20Gaussians%20for%20High-Fidelity%20Head%0A%20%20Reconstruction%20and%20Real-Time%20Editing%0AAuthor%3A%20Heyi%20Sun%20and%20Cong%20Wang%20and%20Tian-Xing%20Xu%20and%20Jingwei%20Huang%20and%20Di%20Kang%20and%20Chunchao%20Guo%20and%20Song-Hai%20Zhang%0AAbstract%3A%20%20%20Creating%20high-fidelity%20and%20editable%20head%20avatars%20is%20a%20pivotal%20challenge%20in%0Acomputer%20vision%20and%20graphics%2C%20boosting%20many%20AR/VR%20applications.%20While%20recent%0Aadvancements%20have%20achieved%20photorealistic%20renderings%20and%20plausible%20animation%2C%0Ahead%20editing%2C%20especially%20real-time%20appearance%20editing%2C%20remains%20challenging%20due%0Ato%20the%20implicit%20representation%20and%20entangled%20modeling%20of%20the%20geometry%20and%0Aglobal%20appearance.%20To%20address%20this%2C%20we%20propose%20Surface-Volumetric%20Gaussian%20Head%0AAvatar%20%28SVG-Head%29%2C%20a%20novel%20hybrid%20representation%20that%20explicitly%20models%20the%0Ageometry%20with%203D%20Gaussians%20bound%20on%20a%20FLAME%20mesh%20and%20leverages%20disentangled%0Atexture%20images%20to%20capture%20the%20global%20appearance.%20Technically%2C%20it%20contains%20two%0Atypes%20of%20Gaussians%2C%20in%20which%20surface%20Gaussians%20explicitly%20model%20the%20appearance%0Aof%20head%20avatars%20using%20learnable%20texture%20images%2C%20facilitating%20real-time%20texture%0Aediting%2C%20while%20volumetric%20Gaussians%20enhance%20the%20reconstruction%20quality%20of%0Anon-Lambertian%20regions%20%28e.g.%2C%20lips%20and%20hair%29.%20To%20model%20the%20correspondence%0Abetween%203D%20world%20and%20texture%20space%2C%20we%20provide%20a%20mesh-aware%20Gaussian%20UV%20mapping%0Amethod%2C%20which%20leverages%20UV%20coordinates%20given%20by%20the%20FLAME%20mesh%20to%20obtain%20sharp%0Atexture%20images%20and%20real-time%20rendering%20speed.%20A%20hierarchical%20optimization%0Astrategy%20is%20further%20designed%20to%20pursue%20the%20optimal%20performance%20in%20both%0Areconstruction%20quality%20and%20editing%20flexibility.%20Experiments%20on%20the%20NeRSemble%0Adataset%20show%20that%20SVG-Head%20not%20only%20generates%20high-fidelity%20rendering%20results%2C%0Abut%20also%20is%20the%20first%20method%20to%20obtain%20explicit%20texture%20images%20for%20Gaussian%0Ahead%20avatars%20and%20support%20real-time%20appearance%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09597v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVG-Head%253A%2520Hybrid%2520Surface-Volumetric%2520Gaussians%2520for%2520High-Fidelity%2520Head%250A%2520%2520Reconstruction%2520and%2520Real-Time%2520Editing%26entry.906535625%3DHeyi%2520Sun%2520and%2520Cong%2520Wang%2520and%2520Tian-Xing%2520Xu%2520and%2520Jingwei%2520Huang%2520and%2520Di%2520Kang%2520and%2520Chunchao%2520Guo%2520and%2520Song-Hai%2520Zhang%26entry.1292438233%3D%2520%2520Creating%2520high-fidelity%2520and%2520editable%2520head%2520avatars%2520is%2520a%2520pivotal%2520challenge%2520in%250Acomputer%2520vision%2520and%2520graphics%252C%2520boosting%2520many%2520AR/VR%2520applications.%2520While%2520recent%250Aadvancements%2520have%2520achieved%2520photorealistic%2520renderings%2520and%2520plausible%2520animation%252C%250Ahead%2520editing%252C%2520especially%2520real-time%2520appearance%2520editing%252C%2520remains%2520challenging%2520due%250Ato%2520the%2520implicit%2520representation%2520and%2520entangled%2520modeling%2520of%2520the%2520geometry%2520and%250Aglobal%2520appearance.%2520To%2520address%2520this%252C%2520we%2520propose%2520Surface-Volumetric%2520Gaussian%2520Head%250AAvatar%2520%2528SVG-Head%2529%252C%2520a%2520novel%2520hybrid%2520representation%2520that%2520explicitly%2520models%2520the%250Ageometry%2520with%25203D%2520Gaussians%2520bound%2520on%2520a%2520FLAME%2520mesh%2520and%2520leverages%2520disentangled%250Atexture%2520images%2520to%2520capture%2520the%2520global%2520appearance.%2520Technically%252C%2520it%2520contains%2520two%250Atypes%2520of%2520Gaussians%252C%2520in%2520which%2520surface%2520Gaussians%2520explicitly%2520model%2520the%2520appearance%250Aof%2520head%2520avatars%2520using%2520learnable%2520texture%2520images%252C%2520facilitating%2520real-time%2520texture%250Aediting%252C%2520while%2520volumetric%2520Gaussians%2520enhance%2520the%2520reconstruction%2520quality%2520of%250Anon-Lambertian%2520regions%2520%2528e.g.%252C%2520lips%2520and%2520hair%2529.%2520To%2520model%2520the%2520correspondence%250Abetween%25203D%2520world%2520and%2520texture%2520space%252C%2520we%2520provide%2520a%2520mesh-aware%2520Gaussian%2520UV%2520mapping%250Amethod%252C%2520which%2520leverages%2520UV%2520coordinates%2520given%2520by%2520the%2520FLAME%2520mesh%2520to%2520obtain%2520sharp%250Atexture%2520images%2520and%2520real-time%2520rendering%2520speed.%2520A%2520hierarchical%2520optimization%250Astrategy%2520is%2520further%2520designed%2520to%2520pursue%2520the%2520optimal%2520performance%2520in%2520both%250Areconstruction%2520quality%2520and%2520editing%2520flexibility.%2520Experiments%2520on%2520the%2520NeRSemble%250Adataset%2520show%2520that%2520SVG-Head%2520not%2520only%2520generates%2520high-fidelity%2520rendering%2520results%252C%250Abut%2520also%2520is%2520the%2520first%2520method%2520to%2520obtain%2520explicit%2520texture%2520images%2520for%2520Gaussian%250Ahead%2520avatars%2520and%2520support%2520real-time%2520appearance%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09597v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVG-Head%3A%20Hybrid%20Surface-Volumetric%20Gaussians%20for%20High-Fidelity%20Head%0A%20%20Reconstruction%20and%20Real-Time%20Editing&entry.906535625=Heyi%20Sun%20and%20Cong%20Wang%20and%20Tian-Xing%20Xu%20and%20Jingwei%20Huang%20and%20Di%20Kang%20and%20Chunchao%20Guo%20and%20Song-Hai%20Zhang&entry.1292438233=%20%20Creating%20high-fidelity%20and%20editable%20head%20avatars%20is%20a%20pivotal%20challenge%20in%0Acomputer%20vision%20and%20graphics%2C%20boosting%20many%20AR/VR%20applications.%20While%20recent%0Aadvancements%20have%20achieved%20photorealistic%20renderings%20and%20plausible%20animation%2C%0Ahead%20editing%2C%20especially%20real-time%20appearance%20editing%2C%20remains%20challenging%20due%0Ato%20the%20implicit%20representation%20and%20entangled%20modeling%20of%20the%20geometry%20and%0Aglobal%20appearance.%20To%20address%20this%2C%20we%20propose%20Surface-Volumetric%20Gaussian%20Head%0AAvatar%20%28SVG-Head%29%2C%20a%20novel%20hybrid%20representation%20that%20explicitly%20models%20the%0Ageometry%20with%203D%20Gaussians%20bound%20on%20a%20FLAME%20mesh%20and%20leverages%20disentangled%0Atexture%20images%20to%20capture%20the%20global%20appearance.%20Technically%2C%20it%20contains%20two%0Atypes%20of%20Gaussians%2C%20in%20which%20surface%20Gaussians%20explicitly%20model%20the%20appearance%0Aof%20head%20avatars%20using%20learnable%20texture%20images%2C%20facilitating%20real-time%20texture%0Aediting%2C%20while%20volumetric%20Gaussians%20enhance%20the%20reconstruction%20quality%20of%0Anon-Lambertian%20regions%20%28e.g.%2C%20lips%20and%20hair%29.%20To%20model%20the%20correspondence%0Abetween%203D%20world%20and%20texture%20space%2C%20we%20provide%20a%20mesh-aware%20Gaussian%20UV%20mapping%0Amethod%2C%20which%20leverages%20UV%20coordinates%20given%20by%20the%20FLAME%20mesh%20to%20obtain%20sharp%0Atexture%20images%20and%20real-time%20rendering%20speed.%20A%20hierarchical%20optimization%0Astrategy%20is%20further%20designed%20to%20pursue%20the%20optimal%20performance%20in%20both%0Areconstruction%20quality%20and%20editing%20flexibility.%20Experiments%20on%20the%20NeRSemble%0Adataset%20show%20that%20SVG-Head%20not%20only%20generates%20high-fidelity%20rendering%20results%2C%0Abut%20also%20is%20the%20first%20method%20to%20obtain%20explicit%20texture%20images%20for%20Gaussian%0Ahead%20avatars%20and%20support%20real-time%20appearance%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09597v2&entry.124074799=Read"},
{"title": "Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy\n  Surface Reconstruction", "author": "Yixin Yang and Yang Zhou and Hui Huang", "abstract": "  Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry\nreconstruction quality than the popular 3DGS by using 2D surfels to approximate\nthin surfaces. However, it falls short when dealing with glossy surfaces,\nresulting in visible holes in these areas. We find that the reflection\ndiscontinuity causes the issue. To fit the jump from diffuse to specular\nreflection at different viewing angles, depth bias is introduced in the\noptimized Gaussian primitives. To address that, we first replace the depth\ndistortion loss in 2DGS with a novel depth convergence loss, which imposes a\nstrong constraint on depth continuity. Then, we rectify the depth criterion in\ndetermining the actual surface, which fully accounts for all the intersecting\nGaussians along the ray. Qualitative and quantitative evaluations across\nvarious datasets reveal that our method significantly improves reconstruction\nquality, with more complete and accurate surfaces than 2DGS. Code is available\nat https://github.com/XiaoXinyyx/Unbiased_Surfel.\n", "link": "http://arxiv.org/abs/2503.06587v3", "date": "2025-08-15", "relevancy": 3.1855, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6664}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6228}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20Unbiased%20Depth%20into%202D%20Gaussian%20Splatting%20for%20High-accuracy%0A%20%20Surface%20Reconstruction&body=Title%3A%20Introducing%20Unbiased%20Depth%20into%202D%20Gaussian%20Splatting%20for%20High-accuracy%0A%20%20Surface%20Reconstruction%0AAuthor%3A%20Yixin%20Yang%20and%20Yang%20Zhou%20and%20Hui%20Huang%0AAbstract%3A%20%20%20Recently%2C%202D%20Gaussian%20Splatting%20%282DGS%29%20has%20demonstrated%20superior%20geometry%0Areconstruction%20quality%20than%20the%20popular%203DGS%20by%20using%202D%20surfels%20to%20approximate%0Athin%20surfaces.%20However%2C%20it%20falls%20short%20when%20dealing%20with%20glossy%20surfaces%2C%0Aresulting%20in%20visible%20holes%20in%20these%20areas.%20We%20find%20that%20the%20reflection%0Adiscontinuity%20causes%20the%20issue.%20To%20fit%20the%20jump%20from%20diffuse%20to%20specular%0Areflection%20at%20different%20viewing%20angles%2C%20depth%20bias%20is%20introduced%20in%20the%0Aoptimized%20Gaussian%20primitives.%20To%20address%20that%2C%20we%20first%20replace%20the%20depth%0Adistortion%20loss%20in%202DGS%20with%20a%20novel%20depth%20convergence%20loss%2C%20which%20imposes%20a%0Astrong%20constraint%20on%20depth%20continuity.%20Then%2C%20we%20rectify%20the%20depth%20criterion%20in%0Adetermining%20the%20actual%20surface%2C%20which%20fully%20accounts%20for%20all%20the%20intersecting%0AGaussians%20along%20the%20ray.%20Qualitative%20and%20quantitative%20evaluations%20across%0Avarious%20datasets%20reveal%20that%20our%20method%20significantly%20improves%20reconstruction%0Aquality%2C%20with%20more%20complete%20and%20accurate%20surfaces%20than%202DGS.%20Code%20is%20available%0Aat%20https%3A//github.com/XiaoXinyyx/Unbiased_Surfel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06587v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520Unbiased%2520Depth%2520into%25202D%2520Gaussian%2520Splatting%2520for%2520High-accuracy%250A%2520%2520Surface%2520Reconstruction%26entry.906535625%3DYixin%2520Yang%2520and%2520Yang%2520Zhou%2520and%2520Hui%2520Huang%26entry.1292438233%3D%2520%2520Recently%252C%25202D%2520Gaussian%2520Splatting%2520%25282DGS%2529%2520has%2520demonstrated%2520superior%2520geometry%250Areconstruction%2520quality%2520than%2520the%2520popular%25203DGS%2520by%2520using%25202D%2520surfels%2520to%2520approximate%250Athin%2520surfaces.%2520However%252C%2520it%2520falls%2520short%2520when%2520dealing%2520with%2520glossy%2520surfaces%252C%250Aresulting%2520in%2520visible%2520holes%2520in%2520these%2520areas.%2520We%2520find%2520that%2520the%2520reflection%250Adiscontinuity%2520causes%2520the%2520issue.%2520To%2520fit%2520the%2520jump%2520from%2520diffuse%2520to%2520specular%250Areflection%2520at%2520different%2520viewing%2520angles%252C%2520depth%2520bias%2520is%2520introduced%2520in%2520the%250Aoptimized%2520Gaussian%2520primitives.%2520To%2520address%2520that%252C%2520we%2520first%2520replace%2520the%2520depth%250Adistortion%2520loss%2520in%25202DGS%2520with%2520a%2520novel%2520depth%2520convergence%2520loss%252C%2520which%2520imposes%2520a%250Astrong%2520constraint%2520on%2520depth%2520continuity.%2520Then%252C%2520we%2520rectify%2520the%2520depth%2520criterion%2520in%250Adetermining%2520the%2520actual%2520surface%252C%2520which%2520fully%2520accounts%2520for%2520all%2520the%2520intersecting%250AGaussians%2520along%2520the%2520ray.%2520Qualitative%2520and%2520quantitative%2520evaluations%2520across%250Avarious%2520datasets%2520reveal%2520that%2520our%2520method%2520significantly%2520improves%2520reconstruction%250Aquality%252C%2520with%2520more%2520complete%2520and%2520accurate%2520surfaces%2520than%25202DGS.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/XiaoXinyyx/Unbiased_Surfel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06587v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20Unbiased%20Depth%20into%202D%20Gaussian%20Splatting%20for%20High-accuracy%0A%20%20Surface%20Reconstruction&entry.906535625=Yixin%20Yang%20and%20Yang%20Zhou%20and%20Hui%20Huang&entry.1292438233=%20%20Recently%2C%202D%20Gaussian%20Splatting%20%282DGS%29%20has%20demonstrated%20superior%20geometry%0Areconstruction%20quality%20than%20the%20popular%203DGS%20by%20using%202D%20surfels%20to%20approximate%0Athin%20surfaces.%20However%2C%20it%20falls%20short%20when%20dealing%20with%20glossy%20surfaces%2C%0Aresulting%20in%20visible%20holes%20in%20these%20areas.%20We%20find%20that%20the%20reflection%0Adiscontinuity%20causes%20the%20issue.%20To%20fit%20the%20jump%20from%20diffuse%20to%20specular%0Areflection%20at%20different%20viewing%20angles%2C%20depth%20bias%20is%20introduced%20in%20the%0Aoptimized%20Gaussian%20primitives.%20To%20address%20that%2C%20we%20first%20replace%20the%20depth%0Adistortion%20loss%20in%202DGS%20with%20a%20novel%20depth%20convergence%20loss%2C%20which%20imposes%20a%0Astrong%20constraint%20on%20depth%20continuity.%20Then%2C%20we%20rectify%20the%20depth%20criterion%20in%0Adetermining%20the%20actual%20surface%2C%20which%20fully%20accounts%20for%20all%20the%20intersecting%0AGaussians%20along%20the%20ray.%20Qualitative%20and%20quantitative%20evaluations%20across%0Avarious%20datasets%20reveal%20that%20our%20method%20significantly%20improves%20reconstruction%0Aquality%2C%20with%20more%20complete%20and%20accurate%20surfaces%20than%202DGS.%20Code%20is%20available%0Aat%20https%3A//github.com/XiaoXinyyx/Unbiased_Surfel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06587v3&entry.124074799=Read"},
{"title": "Reconstructing Satellites in 3D from Amateur Telescope Images", "author": "Zhiming Chang and Boyang Liu and Yifei Xia and Youming Guo and Boxin Shi and He Sun", "abstract": "  Monitoring space objects is crucial for space situational awareness, yet\nreconstructing 3D satellite models from ground-based telescope images is\nchallenging due to atmospheric turbulence, long observation distances, limited\nviewpoints, and low signal-to-noise ratios. In this paper, we propose a novel\ncomputational imaging framework that overcomes these obstacles by integrating a\nhybrid image pre-processing pipeline with a joint pose estimation and 3D\nreconstruction module based on controlled Gaussian Splatting (GS) and\nBranch-and-Bound (BnB) search. We validate our approach on both synthetic\nsatellite datasets and on-sky observations of China's Tiangong Space Station\nand the International Space Station, achieving robust 3D reconstructions of\nlow-Earth orbit satellites from ground-based data. Quantitative evaluations\nusing SSIM, PSNR, LPIPS, and Chamfer Distance demonstrate that our method\noutperforms state-of-the-art NeRF-based approaches, and ablation studies\nconfirm the critical role of each component. Our framework enables\nhigh-fidelity 3D satellite monitoring from Earth, offering a cost-effective\nalternative for space situational awareness. Project page:\nhttps://ai4scientificimaging.org/ReconstructingSatellites\n", "link": "http://arxiv.org/abs/2404.18394v5", "date": "2025-08-15", "relevancy": 3.1453, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6499}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.625}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstructing%20Satellites%20in%203D%20from%20Amateur%20Telescope%20Images&body=Title%3A%20Reconstructing%20Satellites%20in%203D%20from%20Amateur%20Telescope%20Images%0AAuthor%3A%20Zhiming%20Chang%20and%20Boyang%20Liu%20and%20Yifei%20Xia%20and%20Youming%20Guo%20and%20Boxin%20Shi%20and%20He%20Sun%0AAbstract%3A%20%20%20Monitoring%20space%20objects%20is%20crucial%20for%20space%20situational%20awareness%2C%20yet%0Areconstructing%203D%20satellite%20models%20from%20ground-based%20telescope%20images%20is%0Achallenging%20due%20to%20atmospheric%20turbulence%2C%20long%20observation%20distances%2C%20limited%0Aviewpoints%2C%20and%20low%20signal-to-noise%20ratios.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Acomputational%20imaging%20framework%20that%20overcomes%20these%20obstacles%20by%20integrating%20a%0Ahybrid%20image%20pre-processing%20pipeline%20with%20a%20joint%20pose%20estimation%20and%203D%0Areconstruction%20module%20based%20on%20controlled%20Gaussian%20Splatting%20%28GS%29%20and%0ABranch-and-Bound%20%28BnB%29%20search.%20We%20validate%20our%20approach%20on%20both%20synthetic%0Asatellite%20datasets%20and%20on-sky%20observations%20of%20China%27s%20Tiangong%20Space%20Station%0Aand%20the%20International%20Space%20Station%2C%20achieving%20robust%203D%20reconstructions%20of%0Alow-Earth%20orbit%20satellites%20from%20ground-based%20data.%20Quantitative%20evaluations%0Ausing%20SSIM%2C%20PSNR%2C%20LPIPS%2C%20and%20Chamfer%20Distance%20demonstrate%20that%20our%20method%0Aoutperforms%20state-of-the-art%20NeRF-based%20approaches%2C%20and%20ablation%20studies%0Aconfirm%20the%20critical%20role%20of%20each%20component.%20Our%20framework%20enables%0Ahigh-fidelity%203D%20satellite%20monitoring%20from%20Earth%2C%20offering%20a%20cost-effective%0Aalternative%20for%20space%20situational%20awareness.%20Project%20page%3A%0Ahttps%3A//ai4scientificimaging.org/ReconstructingSatellites%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18394v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstructing%2520Satellites%2520in%25203D%2520from%2520Amateur%2520Telescope%2520Images%26entry.906535625%3DZhiming%2520Chang%2520and%2520Boyang%2520Liu%2520and%2520Yifei%2520Xia%2520and%2520Youming%2520Guo%2520and%2520Boxin%2520Shi%2520and%2520He%2520Sun%26entry.1292438233%3D%2520%2520Monitoring%2520space%2520objects%2520is%2520crucial%2520for%2520space%2520situational%2520awareness%252C%2520yet%250Areconstructing%25203D%2520satellite%2520models%2520from%2520ground-based%2520telescope%2520images%2520is%250Achallenging%2520due%2520to%2520atmospheric%2520turbulence%252C%2520long%2520observation%2520distances%252C%2520limited%250Aviewpoints%252C%2520and%2520low%2520signal-to-noise%2520ratios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Acomputational%2520imaging%2520framework%2520that%2520overcomes%2520these%2520obstacles%2520by%2520integrating%2520a%250Ahybrid%2520image%2520pre-processing%2520pipeline%2520with%2520a%2520joint%2520pose%2520estimation%2520and%25203D%250Areconstruction%2520module%2520based%2520on%2520controlled%2520Gaussian%2520Splatting%2520%2528GS%2529%2520and%250ABranch-and-Bound%2520%2528BnB%2529%2520search.%2520We%2520validate%2520our%2520approach%2520on%2520both%2520synthetic%250Asatellite%2520datasets%2520and%2520on-sky%2520observations%2520of%2520China%2527s%2520Tiangong%2520Space%2520Station%250Aand%2520the%2520International%2520Space%2520Station%252C%2520achieving%2520robust%25203D%2520reconstructions%2520of%250Alow-Earth%2520orbit%2520satellites%2520from%2520ground-based%2520data.%2520Quantitative%2520evaluations%250Ausing%2520SSIM%252C%2520PSNR%252C%2520LPIPS%252C%2520and%2520Chamfer%2520Distance%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520state-of-the-art%2520NeRF-based%2520approaches%252C%2520and%2520ablation%2520studies%250Aconfirm%2520the%2520critical%2520role%2520of%2520each%2520component.%2520Our%2520framework%2520enables%250Ahigh-fidelity%25203D%2520satellite%2520monitoring%2520from%2520Earth%252C%2520offering%2520a%2520cost-effective%250Aalternative%2520for%2520space%2520situational%2520awareness.%2520Project%2520page%253A%250Ahttps%253A//ai4scientificimaging.org/ReconstructingSatellites%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18394v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstructing%20Satellites%20in%203D%20from%20Amateur%20Telescope%20Images&entry.906535625=Zhiming%20Chang%20and%20Boyang%20Liu%20and%20Yifei%20Xia%20and%20Youming%20Guo%20and%20Boxin%20Shi%20and%20He%20Sun&entry.1292438233=%20%20Monitoring%20space%20objects%20is%20crucial%20for%20space%20situational%20awareness%2C%20yet%0Areconstructing%203D%20satellite%20models%20from%20ground-based%20telescope%20images%20is%0Achallenging%20due%20to%20atmospheric%20turbulence%2C%20long%20observation%20distances%2C%20limited%0Aviewpoints%2C%20and%20low%20signal-to-noise%20ratios.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Acomputational%20imaging%20framework%20that%20overcomes%20these%20obstacles%20by%20integrating%20a%0Ahybrid%20image%20pre-processing%20pipeline%20with%20a%20joint%20pose%20estimation%20and%203D%0Areconstruction%20module%20based%20on%20controlled%20Gaussian%20Splatting%20%28GS%29%20and%0ABranch-and-Bound%20%28BnB%29%20search.%20We%20validate%20our%20approach%20on%20both%20synthetic%0Asatellite%20datasets%20and%20on-sky%20observations%20of%20China%27s%20Tiangong%20Space%20Station%0Aand%20the%20International%20Space%20Station%2C%20achieving%20robust%203D%20reconstructions%20of%0Alow-Earth%20orbit%20satellites%20from%20ground-based%20data.%20Quantitative%20evaluations%0Ausing%20SSIM%2C%20PSNR%2C%20LPIPS%2C%20and%20Chamfer%20Distance%20demonstrate%20that%20our%20method%0Aoutperforms%20state-of-the-art%20NeRF-based%20approaches%2C%20and%20ablation%20studies%0Aconfirm%20the%20critical%20role%20of%20each%20component.%20Our%20framework%20enables%0Ahigh-fidelity%203D%20satellite%20monitoring%20from%20Earth%2C%20offering%20a%20cost-effective%0Aalternative%20for%20space%20situational%20awareness.%20Project%20page%3A%0Ahttps%3A//ai4scientificimaging.org/ReconstructingSatellites%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18394v5&entry.124074799=Read"},
{"title": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian\n  Splatting", "author": "Simona Kocour and Assia Benbihi and Torsten Sattler", "abstract": "  Understanding what semantic information persists after object removal is\ncritical for privacy-preserving 3D reconstruction and editable scene\nrepresentations. In this work, we introduce a novel benchmark and evaluation\nframework to measure semantic residuals, the unintended semantic traces left\nbehind, after object removal in 3D Gaussian Splatting. We conduct experiments\nacross a diverse set of indoor and outdoor scenes, showing that current methods\ncan preserve semantic information despite the absence of visual geometry. We\nalso release Remove360, a dataset of pre/post-removal RGB images and\nobject-level masks captured in real-world environments. While prior datasets\nhave focused on isolated object instances, Remove360 covers a broader and more\ncomplex range of indoor and outdoor scenes, enabling evaluation of object\nremoval in the context of full-scene representations. Given ground truth images\nof a scene before and after object removal, we assess whether we can truly\neliminate semantic presence, and if downstream models can still infer what was\nremoved. Our findings reveal critical limitations in current 3D object removal\ntechniques and underscore the need for more robust solutions capable of\nhandling real-world complexity. The evaluation framework is available at\ngithub.com/spatial-intelligence-ai/Remove360.git. Data are available at\nhuggingface.co/datasets/simkoc/Remove360.\n", "link": "http://arxiv.org/abs/2508.11431v1", "date": "2025-08-15", "relevancy": 3.1012, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6451}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6366}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Remove360%3A%20Benchmarking%20Residuals%20After%20Object%20Removal%20in%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20Remove360%3A%20Benchmarking%20Residuals%20After%20Object%20Removal%20in%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Simona%20Kocour%20and%20Assia%20Benbihi%20and%20Torsten%20Sattler%0AAbstract%3A%20%20%20Understanding%20what%20semantic%20information%20persists%20after%20object%20removal%20is%0Acritical%20for%20privacy-preserving%203D%20reconstruction%20and%20editable%20scene%0Arepresentations.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20benchmark%20and%20evaluation%0Aframework%20to%20measure%20semantic%20residuals%2C%20the%20unintended%20semantic%20traces%20left%0Abehind%2C%20after%20object%20removal%20in%203D%20Gaussian%20Splatting.%20We%20conduct%20experiments%0Aacross%20a%20diverse%20set%20of%20indoor%20and%20outdoor%20scenes%2C%20showing%20that%20current%20methods%0Acan%20preserve%20semantic%20information%20despite%20the%20absence%20of%20visual%20geometry.%20We%0Aalso%20release%20Remove360%2C%20a%20dataset%20of%20pre/post-removal%20RGB%20images%20and%0Aobject-level%20masks%20captured%20in%20real-world%20environments.%20While%20prior%20datasets%0Ahave%20focused%20on%20isolated%20object%20instances%2C%20Remove360%20covers%20a%20broader%20and%20more%0Acomplex%20range%20of%20indoor%20and%20outdoor%20scenes%2C%20enabling%20evaluation%20of%20object%0Aremoval%20in%20the%20context%20of%20full-scene%20representations.%20Given%20ground%20truth%20images%0Aof%20a%20scene%20before%20and%20after%20object%20removal%2C%20we%20assess%20whether%20we%20can%20truly%0Aeliminate%20semantic%20presence%2C%20and%20if%20downstream%20models%20can%20still%20infer%20what%20was%0Aremoved.%20Our%20findings%20reveal%20critical%20limitations%20in%20current%203D%20object%20removal%0Atechniques%20and%20underscore%20the%20need%20for%20more%20robust%20solutions%20capable%20of%0Ahandling%20real-world%20complexity.%20The%20evaluation%20framework%20is%20available%20at%0Agithub.com/spatial-intelligence-ai/Remove360.git.%20Data%20are%20available%20at%0Ahuggingface.co/datasets/simkoc/Remove360.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRemove360%253A%2520Benchmarking%2520Residuals%2520After%2520Object%2520Removal%2520in%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DSimona%2520Kocour%2520and%2520Assia%2520Benbihi%2520and%2520Torsten%2520Sattler%26entry.1292438233%3D%2520%2520Understanding%2520what%2520semantic%2520information%2520persists%2520after%2520object%2520removal%2520is%250Acritical%2520for%2520privacy-preserving%25203D%2520reconstruction%2520and%2520editable%2520scene%250Arepresentations.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520benchmark%2520and%2520evaluation%250Aframework%2520to%2520measure%2520semantic%2520residuals%252C%2520the%2520unintended%2520semantic%2520traces%2520left%250Abehind%252C%2520after%2520object%2520removal%2520in%25203D%2520Gaussian%2520Splatting.%2520We%2520conduct%2520experiments%250Aacross%2520a%2520diverse%2520set%2520of%2520indoor%2520and%2520outdoor%2520scenes%252C%2520showing%2520that%2520current%2520methods%250Acan%2520preserve%2520semantic%2520information%2520despite%2520the%2520absence%2520of%2520visual%2520geometry.%2520We%250Aalso%2520release%2520Remove360%252C%2520a%2520dataset%2520of%2520pre/post-removal%2520RGB%2520images%2520and%250Aobject-level%2520masks%2520captured%2520in%2520real-world%2520environments.%2520While%2520prior%2520datasets%250Ahave%2520focused%2520on%2520isolated%2520object%2520instances%252C%2520Remove360%2520covers%2520a%2520broader%2520and%2520more%250Acomplex%2520range%2520of%2520indoor%2520and%2520outdoor%2520scenes%252C%2520enabling%2520evaluation%2520of%2520object%250Aremoval%2520in%2520the%2520context%2520of%2520full-scene%2520representations.%2520Given%2520ground%2520truth%2520images%250Aof%2520a%2520scene%2520before%2520and%2520after%2520object%2520removal%252C%2520we%2520assess%2520whether%2520we%2520can%2520truly%250Aeliminate%2520semantic%2520presence%252C%2520and%2520if%2520downstream%2520models%2520can%2520still%2520infer%2520what%2520was%250Aremoved.%2520Our%2520findings%2520reveal%2520critical%2520limitations%2520in%2520current%25203D%2520object%2520removal%250Atechniques%2520and%2520underscore%2520the%2520need%2520for%2520more%2520robust%2520solutions%2520capable%2520of%250Ahandling%2520real-world%2520complexity.%2520The%2520evaluation%2520framework%2520is%2520available%2520at%250Agithub.com/spatial-intelligence-ai/Remove360.git.%2520Data%2520are%2520available%2520at%250Ahuggingface.co/datasets/simkoc/Remove360.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Remove360%3A%20Benchmarking%20Residuals%20After%20Object%20Removal%20in%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Simona%20Kocour%20and%20Assia%20Benbihi%20and%20Torsten%20Sattler&entry.1292438233=%20%20Understanding%20what%20semantic%20information%20persists%20after%20object%20removal%20is%0Acritical%20for%20privacy-preserving%203D%20reconstruction%20and%20editable%20scene%0Arepresentations.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20benchmark%20and%20evaluation%0Aframework%20to%20measure%20semantic%20residuals%2C%20the%20unintended%20semantic%20traces%20left%0Abehind%2C%20after%20object%20removal%20in%203D%20Gaussian%20Splatting.%20We%20conduct%20experiments%0Aacross%20a%20diverse%20set%20of%20indoor%20and%20outdoor%20scenes%2C%20showing%20that%20current%20methods%0Acan%20preserve%20semantic%20information%20despite%20the%20absence%20of%20visual%20geometry.%20We%0Aalso%20release%20Remove360%2C%20a%20dataset%20of%20pre/post-removal%20RGB%20images%20and%0Aobject-level%20masks%20captured%20in%20real-world%20environments.%20While%20prior%20datasets%0Ahave%20focused%20on%20isolated%20object%20instances%2C%20Remove360%20covers%20a%20broader%20and%20more%0Acomplex%20range%20of%20indoor%20and%20outdoor%20scenes%2C%20enabling%20evaluation%20of%20object%0Aremoval%20in%20the%20context%20of%20full-scene%20representations.%20Given%20ground%20truth%20images%0Aof%20a%20scene%20before%20and%20after%20object%20removal%2C%20we%20assess%20whether%20we%20can%20truly%0Aeliminate%20semantic%20presence%2C%20and%20if%20downstream%20models%20can%20still%20infer%20what%20was%0Aremoved.%20Our%20findings%20reveal%20critical%20limitations%20in%20current%203D%20object%20removal%0Atechniques%20and%20underscore%20the%20need%20for%20more%20robust%20solutions%20capable%20of%0Ahandling%20real-world%20complexity.%20The%20evaluation%20framework%20is%20available%20at%0Agithub.com/spatial-intelligence-ai/Remove360.git.%20Data%20are%20available%20at%0Ahuggingface.co/datasets/simkoc/Remove360.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11431v1&entry.124074799=Read"},
{"title": "GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning\n  with Scalable Reinforcement Learning", "author": "GLM-V Team and  : and Wenyi Hong and Wenmeng Yu and Xiaotao Gu and Guo Wang and Guobing Gan and Haomiao Tang and Jiale Cheng and Ji Qi and Junhui Ji and Lihang Pan and Shuaiqi Duan and Weihan Wang and Yan Wang and Yean Cheng and Zehai He and Zhe Su and Zhen Yang and Ziyang Pan and Aohan Zeng and Baoxu Wang and Bin Chen and Boyan Shi and Changyu Pang and Chenhui Zhang and Da Yin and Fan Yang and Guoqing Chen and Jiazheng Xu and Jiale Zhu and Jiali Chen and Jing Chen and Jinhao Chen and Jinghao Lin and Jinjiang Wang and Junjie Chen and Leqi Lei and Letian Gong and Leyi Pan and Mingdao Liu and Mingde Xu and Mingzhi Zhang and Qinkai Zheng and Sheng Yang and Shi Zhong and Shiyu Huang and Shuyuan Zhao and Siyan Xue and Shangqin Tu and Shengbiao Meng and Tianshu Zhang and Tianwei Luo and Tianxiang Hao and Tianyu Tong and Wenkai Li and Wei Jia and Xiao Liu and Xiaohan Zhang and Xin Lyu and Xinyue Fan and Xuancheng Huang and Yanling Wang and Yadong Xue and Yanfeng Wang and Yanzi Wang and Yifan An and Yifan Du and Yiming Shi and Yiheng Huang and Yilin Niu and Yuan Wang and Yuanchang Yue and Yuchen Li and Yutao Zhang and Yuting Wang and Yu Wang and Yuxuan Zhang and Zhao Xue and Zhenyu Hou and Zhengxiao Du and Zihan Wang and Peng Zhang and Debing Liu and Bin Xu and Juanzi Li and Minlie Huang and Yuxiao Dong and Jie Tang", "abstract": "  We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models\n(VLMs) designed to advance general-purpose multimodal understanding and\nreasoning. In this report, we share our key findings in the development of the\nreasoning-centric training framework. We first develop a capable vision\nfoundation model with significant potential through large-scale pre-training,\nwhich arguably sets the upper bound for the final performance. We then propose\nReinforcement Learning with Curriculum Sampling (RLCS) to unlock the full\npotential of the model, leading to comprehensive capability enhancement across\na diverse range of tasks, including STEM problem solving, video understanding,\ncontent recognition, coding, grounding, GUI-based agents, and long document\ninterpretation. In a comprehensive evaluation across 42 public benchmarks,\nGLM-4.5V achieves state-of-the-art performance on nearly all tasks among\nopen-source models of similar size, and demonstrates competitive or even\nsuperior results compared to closed-source models such as Gemini-2.5-Flash on\nchallenging tasks including Coding and GUI Agents. Meanwhile, the smaller\nGLM-4.1V-9B-Thinking remains highly competitive-achieving superior results to\nthe much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source both\nGLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information are\nreleased at https://github.com/zai-org/GLM-V.\n", "link": "http://arxiv.org/abs/2507.01006v5", "date": "2025-08-15", "relevancy": 2.9228, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5965}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5965}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLM-4.5V%20and%20GLM-4.1V-Thinking%3A%20Towards%20Versatile%20Multimodal%20Reasoning%0A%20%20with%20Scalable%20Reinforcement%20Learning&body=Title%3A%20GLM-4.5V%20and%20GLM-4.1V-Thinking%3A%20Towards%20Versatile%20Multimodal%20Reasoning%0A%20%20with%20Scalable%20Reinforcement%20Learning%0AAuthor%3A%20GLM-V%20Team%20and%20%20%3A%20and%20Wenyi%20Hong%20and%20Wenmeng%20Yu%20and%20Xiaotao%20Gu%20and%20Guo%20Wang%20and%20Guobing%20Gan%20and%20Haomiao%20Tang%20and%20Jiale%20Cheng%20and%20Ji%20Qi%20and%20Junhui%20Ji%20and%20Lihang%20Pan%20and%20Shuaiqi%20Duan%20and%20Weihan%20Wang%20and%20Yan%20Wang%20and%20Yean%20Cheng%20and%20Zehai%20He%20and%20Zhe%20Su%20and%20Zhen%20Yang%20and%20Ziyang%20Pan%20and%20Aohan%20Zeng%20and%20Baoxu%20Wang%20and%20Bin%20Chen%20and%20Boyan%20Shi%20and%20Changyu%20Pang%20and%20Chenhui%20Zhang%20and%20Da%20Yin%20and%20Fan%20Yang%20and%20Guoqing%20Chen%20and%20Jiazheng%20Xu%20and%20Jiale%20Zhu%20and%20Jiali%20Chen%20and%20Jing%20Chen%20and%20Jinhao%20Chen%20and%20Jinghao%20Lin%20and%20Jinjiang%20Wang%20and%20Junjie%20Chen%20and%20Leqi%20Lei%20and%20Letian%20Gong%20and%20Leyi%20Pan%20and%20Mingdao%20Liu%20and%20Mingde%20Xu%20and%20Mingzhi%20Zhang%20and%20Qinkai%20Zheng%20and%20Sheng%20Yang%20and%20Shi%20Zhong%20and%20Shiyu%20Huang%20and%20Shuyuan%20Zhao%20and%20Siyan%20Xue%20and%20Shangqin%20Tu%20and%20Shengbiao%20Meng%20and%20Tianshu%20Zhang%20and%20Tianwei%20Luo%20and%20Tianxiang%20Hao%20and%20Tianyu%20Tong%20and%20Wenkai%20Li%20and%20Wei%20Jia%20and%20Xiao%20Liu%20and%20Xiaohan%20Zhang%20and%20Xin%20Lyu%20and%20Xinyue%20Fan%20and%20Xuancheng%20Huang%20and%20Yanling%20Wang%20and%20Yadong%20Xue%20and%20Yanfeng%20Wang%20and%20Yanzi%20Wang%20and%20Yifan%20An%20and%20Yifan%20Du%20and%20Yiming%20Shi%20and%20Yiheng%20Huang%20and%20Yilin%20Niu%20and%20Yuan%20Wang%20and%20Yuanchang%20Yue%20and%20Yuchen%20Li%20and%20Yutao%20Zhang%20and%20Yuting%20Wang%20and%20Yu%20Wang%20and%20Yuxuan%20Zhang%20and%20Zhao%20Xue%20and%20Zhenyu%20Hou%20and%20Zhengxiao%20Du%20and%20Zihan%20Wang%20and%20Peng%20Zhang%20and%20Debing%20Liu%20and%20Bin%20Xu%20and%20Juanzi%20Li%20and%20Minlie%20Huang%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%0AAbstract%3A%20%20%20We%20present%20GLM-4.1V-Thinking%20and%20GLM-4.5V%2C%20a%20family%20of%20vision-language%20models%0A%28VLMs%29%20designed%20to%20advance%20general-purpose%20multimodal%20understanding%20and%0Areasoning.%20In%20this%20report%2C%20we%20share%20our%20key%20findings%20in%20the%20development%20of%20the%0Areasoning-centric%20training%20framework.%20We%20first%20develop%20a%20capable%20vision%0Afoundation%20model%20with%20significant%20potential%20through%20large-scale%20pre-training%2C%0Awhich%20arguably%20sets%20the%20upper%20bound%20for%20the%20final%20performance.%20We%20then%20propose%0AReinforcement%20Learning%20with%20Curriculum%20Sampling%20%28RLCS%29%20to%20unlock%20the%20full%0Apotential%20of%20the%20model%2C%20leading%20to%20comprehensive%20capability%20enhancement%20across%0Aa%20diverse%20range%20of%20tasks%2C%20including%20STEM%20problem%20solving%2C%20video%20understanding%2C%0Acontent%20recognition%2C%20coding%2C%20grounding%2C%20GUI-based%20agents%2C%20and%20long%20document%0Ainterpretation.%20In%20a%20comprehensive%20evaluation%20across%2042%20public%20benchmarks%2C%0AGLM-4.5V%20achieves%20state-of-the-art%20performance%20on%20nearly%20all%20tasks%20among%0Aopen-source%20models%20of%20similar%20size%2C%20and%20demonstrates%20competitive%20or%20even%0Asuperior%20results%20compared%20to%20closed-source%20models%20such%20as%20Gemini-2.5-Flash%20on%0Achallenging%20tasks%20including%20Coding%20and%20GUI%20Agents.%20Meanwhile%2C%20the%20smaller%0AGLM-4.1V-9B-Thinking%20remains%20highly%20competitive-achieving%20superior%20results%20to%0Athe%20much%20larger%20Qwen2.5-VL-72B%20on%2029%20benchmarks.%20We%20open-source%20both%0AGLM-4.1V-9B-Thinking%20and%20GLM-4.5V.%20Code%2C%20models%20and%20more%20information%20are%0Areleased%20at%20https%3A//github.com/zai-org/GLM-V.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01006v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLM-4.5V%2520and%2520GLM-4.1V-Thinking%253A%2520Towards%2520Versatile%2520Multimodal%2520Reasoning%250A%2520%2520with%2520Scalable%2520Reinforcement%2520Learning%26entry.906535625%3DGLM-V%2520Team%2520and%2520%2520%253A%2520and%2520Wenyi%2520Hong%2520and%2520Wenmeng%2520Yu%2520and%2520Xiaotao%2520Gu%2520and%2520Guo%2520Wang%2520and%2520Guobing%2520Gan%2520and%2520Haomiao%2520Tang%2520and%2520Jiale%2520Cheng%2520and%2520Ji%2520Qi%2520and%2520Junhui%2520Ji%2520and%2520Lihang%2520Pan%2520and%2520Shuaiqi%2520Duan%2520and%2520Weihan%2520Wang%2520and%2520Yan%2520Wang%2520and%2520Yean%2520Cheng%2520and%2520Zehai%2520He%2520and%2520Zhe%2520Su%2520and%2520Zhen%2520Yang%2520and%2520Ziyang%2520Pan%2520and%2520Aohan%2520Zeng%2520and%2520Baoxu%2520Wang%2520and%2520Bin%2520Chen%2520and%2520Boyan%2520Shi%2520and%2520Changyu%2520Pang%2520and%2520Chenhui%2520Zhang%2520and%2520Da%2520Yin%2520and%2520Fan%2520Yang%2520and%2520Guoqing%2520Chen%2520and%2520Jiazheng%2520Xu%2520and%2520Jiale%2520Zhu%2520and%2520Jiali%2520Chen%2520and%2520Jing%2520Chen%2520and%2520Jinhao%2520Chen%2520and%2520Jinghao%2520Lin%2520and%2520Jinjiang%2520Wang%2520and%2520Junjie%2520Chen%2520and%2520Leqi%2520Lei%2520and%2520Letian%2520Gong%2520and%2520Leyi%2520Pan%2520and%2520Mingdao%2520Liu%2520and%2520Mingde%2520Xu%2520and%2520Mingzhi%2520Zhang%2520and%2520Qinkai%2520Zheng%2520and%2520Sheng%2520Yang%2520and%2520Shi%2520Zhong%2520and%2520Shiyu%2520Huang%2520and%2520Shuyuan%2520Zhao%2520and%2520Siyan%2520Xue%2520and%2520Shangqin%2520Tu%2520and%2520Shengbiao%2520Meng%2520and%2520Tianshu%2520Zhang%2520and%2520Tianwei%2520Luo%2520and%2520Tianxiang%2520Hao%2520and%2520Tianyu%2520Tong%2520and%2520Wenkai%2520Li%2520and%2520Wei%2520Jia%2520and%2520Xiao%2520Liu%2520and%2520Xiaohan%2520Zhang%2520and%2520Xin%2520Lyu%2520and%2520Xinyue%2520Fan%2520and%2520Xuancheng%2520Huang%2520and%2520Yanling%2520Wang%2520and%2520Yadong%2520Xue%2520and%2520Yanfeng%2520Wang%2520and%2520Yanzi%2520Wang%2520and%2520Yifan%2520An%2520and%2520Yifan%2520Du%2520and%2520Yiming%2520Shi%2520and%2520Yiheng%2520Huang%2520and%2520Yilin%2520Niu%2520and%2520Yuan%2520Wang%2520and%2520Yuanchang%2520Yue%2520and%2520Yuchen%2520Li%2520and%2520Yutao%2520Zhang%2520and%2520Yuting%2520Wang%2520and%2520Yu%2520Wang%2520and%2520Yuxuan%2520Zhang%2520and%2520Zhao%2520Xue%2520and%2520Zhenyu%2520Hou%2520and%2520Zhengxiao%2520Du%2520and%2520Zihan%2520Wang%2520and%2520Peng%2520Zhang%2520and%2520Debing%2520Liu%2520and%2520Bin%2520Xu%2520and%2520Juanzi%2520Li%2520and%2520Minlie%2520Huang%2520and%2520Yuxiao%2520Dong%2520and%2520Jie%2520Tang%26entry.1292438233%3D%2520%2520We%2520present%2520GLM-4.1V-Thinking%2520and%2520GLM-4.5V%252C%2520a%2520family%2520of%2520vision-language%2520models%250A%2528VLMs%2529%2520designed%2520to%2520advance%2520general-purpose%2520multimodal%2520understanding%2520and%250Areasoning.%2520In%2520this%2520report%252C%2520we%2520share%2520our%2520key%2520findings%2520in%2520the%2520development%2520of%2520the%250Areasoning-centric%2520training%2520framework.%2520We%2520first%2520develop%2520a%2520capable%2520vision%250Afoundation%2520model%2520with%2520significant%2520potential%2520through%2520large-scale%2520pre-training%252C%250Awhich%2520arguably%2520sets%2520the%2520upper%2520bound%2520for%2520the%2520final%2520performance.%2520We%2520then%2520propose%250AReinforcement%2520Learning%2520with%2520Curriculum%2520Sampling%2520%2528RLCS%2529%2520to%2520unlock%2520the%2520full%250Apotential%2520of%2520the%2520model%252C%2520leading%2520to%2520comprehensive%2520capability%2520enhancement%2520across%250Aa%2520diverse%2520range%2520of%2520tasks%252C%2520including%2520STEM%2520problem%2520solving%252C%2520video%2520understanding%252C%250Acontent%2520recognition%252C%2520coding%252C%2520grounding%252C%2520GUI-based%2520agents%252C%2520and%2520long%2520document%250Ainterpretation.%2520In%2520a%2520comprehensive%2520evaluation%2520across%252042%2520public%2520benchmarks%252C%250AGLM-4.5V%2520achieves%2520state-of-the-art%2520performance%2520on%2520nearly%2520all%2520tasks%2520among%250Aopen-source%2520models%2520of%2520similar%2520size%252C%2520and%2520demonstrates%2520competitive%2520or%2520even%250Asuperior%2520results%2520compared%2520to%2520closed-source%2520models%2520such%2520as%2520Gemini-2.5-Flash%2520on%250Achallenging%2520tasks%2520including%2520Coding%2520and%2520GUI%2520Agents.%2520Meanwhile%252C%2520the%2520smaller%250AGLM-4.1V-9B-Thinking%2520remains%2520highly%2520competitive-achieving%2520superior%2520results%2520to%250Athe%2520much%2520larger%2520Qwen2.5-VL-72B%2520on%252029%2520benchmarks.%2520We%2520open-source%2520both%250AGLM-4.1V-9B-Thinking%2520and%2520GLM-4.5V.%2520Code%252C%2520models%2520and%2520more%2520information%2520are%250Areleased%2520at%2520https%253A//github.com/zai-org/GLM-V.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01006v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLM-4.5V%20and%20GLM-4.1V-Thinking%3A%20Towards%20Versatile%20Multimodal%20Reasoning%0A%20%20with%20Scalable%20Reinforcement%20Learning&entry.906535625=GLM-V%20Team%20and%20%20%3A%20and%20Wenyi%20Hong%20and%20Wenmeng%20Yu%20and%20Xiaotao%20Gu%20and%20Guo%20Wang%20and%20Guobing%20Gan%20and%20Haomiao%20Tang%20and%20Jiale%20Cheng%20and%20Ji%20Qi%20and%20Junhui%20Ji%20and%20Lihang%20Pan%20and%20Shuaiqi%20Duan%20and%20Weihan%20Wang%20and%20Yan%20Wang%20and%20Yean%20Cheng%20and%20Zehai%20He%20and%20Zhe%20Su%20and%20Zhen%20Yang%20and%20Ziyang%20Pan%20and%20Aohan%20Zeng%20and%20Baoxu%20Wang%20and%20Bin%20Chen%20and%20Boyan%20Shi%20and%20Changyu%20Pang%20and%20Chenhui%20Zhang%20and%20Da%20Yin%20and%20Fan%20Yang%20and%20Guoqing%20Chen%20and%20Jiazheng%20Xu%20and%20Jiale%20Zhu%20and%20Jiali%20Chen%20and%20Jing%20Chen%20and%20Jinhao%20Chen%20and%20Jinghao%20Lin%20and%20Jinjiang%20Wang%20and%20Junjie%20Chen%20and%20Leqi%20Lei%20and%20Letian%20Gong%20and%20Leyi%20Pan%20and%20Mingdao%20Liu%20and%20Mingde%20Xu%20and%20Mingzhi%20Zhang%20and%20Qinkai%20Zheng%20and%20Sheng%20Yang%20and%20Shi%20Zhong%20and%20Shiyu%20Huang%20and%20Shuyuan%20Zhao%20and%20Siyan%20Xue%20and%20Shangqin%20Tu%20and%20Shengbiao%20Meng%20and%20Tianshu%20Zhang%20and%20Tianwei%20Luo%20and%20Tianxiang%20Hao%20and%20Tianyu%20Tong%20and%20Wenkai%20Li%20and%20Wei%20Jia%20and%20Xiao%20Liu%20and%20Xiaohan%20Zhang%20and%20Xin%20Lyu%20and%20Xinyue%20Fan%20and%20Xuancheng%20Huang%20and%20Yanling%20Wang%20and%20Yadong%20Xue%20and%20Yanfeng%20Wang%20and%20Yanzi%20Wang%20and%20Yifan%20An%20and%20Yifan%20Du%20and%20Yiming%20Shi%20and%20Yiheng%20Huang%20and%20Yilin%20Niu%20and%20Yuan%20Wang%20and%20Yuanchang%20Yue%20and%20Yuchen%20Li%20and%20Yutao%20Zhang%20and%20Yuting%20Wang%20and%20Yu%20Wang%20and%20Yuxuan%20Zhang%20and%20Zhao%20Xue%20and%20Zhenyu%20Hou%20and%20Zhengxiao%20Du%20and%20Zihan%20Wang%20and%20Peng%20Zhang%20and%20Debing%20Liu%20and%20Bin%20Xu%20and%20Juanzi%20Li%20and%20Minlie%20Huang%20and%20Yuxiao%20Dong%20and%20Jie%20Tang&entry.1292438233=%20%20We%20present%20GLM-4.1V-Thinking%20and%20GLM-4.5V%2C%20a%20family%20of%20vision-language%20models%0A%28VLMs%29%20designed%20to%20advance%20general-purpose%20multimodal%20understanding%20and%0Areasoning.%20In%20this%20report%2C%20we%20share%20our%20key%20findings%20in%20the%20development%20of%20the%0Areasoning-centric%20training%20framework.%20We%20first%20develop%20a%20capable%20vision%0Afoundation%20model%20with%20significant%20potential%20through%20large-scale%20pre-training%2C%0Awhich%20arguably%20sets%20the%20upper%20bound%20for%20the%20final%20performance.%20We%20then%20propose%0AReinforcement%20Learning%20with%20Curriculum%20Sampling%20%28RLCS%29%20to%20unlock%20the%20full%0Apotential%20of%20the%20model%2C%20leading%20to%20comprehensive%20capability%20enhancement%20across%0Aa%20diverse%20range%20of%20tasks%2C%20including%20STEM%20problem%20solving%2C%20video%20understanding%2C%0Acontent%20recognition%2C%20coding%2C%20grounding%2C%20GUI-based%20agents%2C%20and%20long%20document%0Ainterpretation.%20In%20a%20comprehensive%20evaluation%20across%2042%20public%20benchmarks%2C%0AGLM-4.5V%20achieves%20state-of-the-art%20performance%20on%20nearly%20all%20tasks%20among%0Aopen-source%20models%20of%20similar%20size%2C%20and%20demonstrates%20competitive%20or%20even%0Asuperior%20results%20compared%20to%20closed-source%20models%20such%20as%20Gemini-2.5-Flash%20on%0Achallenging%20tasks%20including%20Coding%20and%20GUI%20Agents.%20Meanwhile%2C%20the%20smaller%0AGLM-4.1V-9B-Thinking%20remains%20highly%20competitive-achieving%20superior%20results%20to%0Athe%20much%20larger%20Qwen2.5-VL-72B%20on%2029%20benchmarks.%20We%20open-source%20both%0AGLM-4.1V-9B-Thinking%20and%20GLM-4.5V.%20Code%2C%20models%20and%20more%20information%20are%0Areleased%20at%20https%3A//github.com/zai-org/GLM-V.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01006v5&entry.124074799=Read"},
{"title": "AIM: Amending Inherent Interpretability via Self-Supervised Masking", "author": "Eyad Alshami and Shashank Agnihotri and Bernt Schiele and Margret Keuper", "abstract": "  It has been observed that deep neural networks (DNNs) often use both genuine\nas well as spurious features. In this work, we propose \"Amending Inherent\nInterpretability via Self-Supervised Masking\" (AIM), a simple yet interestingly\neffective method that promotes the network's utilization of genuine features\nover spurious alternatives without requiring additional annotations. In\nparticular, AIM uses features at multiple encoding stages to guide a\nself-supervised, sample-specific feature-masking process. As a result, AIM\nenables the training of well-performing and inherently interpretable models\nthat faithfully summarize the decision process. We validate AIM across a\ndiverse range of challenging datasets that test both out-of-distribution\ngeneralization and fine-grained visual understanding. These include\ngeneral-purpose classification benchmarks such as ImageNet100, HardImageNet,\nand ImageWoof, as well as fine-grained classification datasets such as\nWaterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual\nbenefits: interpretability improvements, as measured by the Energy Pointing\nGame (EPG) score, and accuracy gains over strong baselines. These consistent\ngains across domains and architectures provide compelling evidence that AIM\npromotes the use of genuine and meaningful features that directly contribute to\nimproved generalization and human-aligned interpretability.\n", "link": "http://arxiv.org/abs/2508.11502v1", "date": "2025-08-15", "relevancy": 2.7598, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5842}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5369}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIM%3A%20Amending%20Inherent%20Interpretability%20via%20Self-Supervised%20Masking&body=Title%3A%20AIM%3A%20Amending%20Inherent%20Interpretability%20via%20Self-Supervised%20Masking%0AAuthor%3A%20Eyad%20Alshami%20and%20Shashank%20Agnihotri%20and%20Bernt%20Schiele%20and%20Margret%20Keuper%0AAbstract%3A%20%20%20It%20has%20been%20observed%20that%20deep%20neural%20networks%20%28DNNs%29%20often%20use%20both%20genuine%0Aas%20well%20as%20spurious%20features.%20In%20this%20work%2C%20we%20propose%20%22Amending%20Inherent%0AInterpretability%20via%20Self-Supervised%20Masking%22%20%28AIM%29%2C%20a%20simple%20yet%20interestingly%0Aeffective%20method%20that%20promotes%20the%20network%27s%20utilization%20of%20genuine%20features%0Aover%20spurious%20alternatives%20without%20requiring%20additional%20annotations.%20In%0Aparticular%2C%20AIM%20uses%20features%20at%20multiple%20encoding%20stages%20to%20guide%20a%0Aself-supervised%2C%20sample-specific%20feature-masking%20process.%20As%20a%20result%2C%20AIM%0Aenables%20the%20training%20of%20well-performing%20and%20inherently%20interpretable%20models%0Athat%20faithfully%20summarize%20the%20decision%20process.%20We%20validate%20AIM%20across%20a%0Adiverse%20range%20of%20challenging%20datasets%20that%20test%20both%20out-of-distribution%0Ageneralization%20and%20fine-grained%20visual%20understanding.%20These%20include%0Ageneral-purpose%20classification%20benchmarks%20such%20as%20ImageNet100%2C%20HardImageNet%2C%0Aand%20ImageWoof%2C%20as%20well%20as%20fine-grained%20classification%20datasets%20such%20as%0AWaterbirds%2C%20TravelingBirds%2C%20and%20CUB-200.%20AIM%20demonstrates%20significant%20dual%0Abenefits%3A%20interpretability%20improvements%2C%20as%20measured%20by%20the%20Energy%20Pointing%0AGame%20%28EPG%29%20score%2C%20and%20accuracy%20gains%20over%20strong%20baselines.%20These%20consistent%0Agains%20across%20domains%20and%20architectures%20provide%20compelling%20evidence%20that%20AIM%0Apromotes%20the%20use%20of%20genuine%20and%20meaningful%20features%20that%20directly%20contribute%20to%0Aimproved%20generalization%20and%20human-aligned%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIM%253A%2520Amending%2520Inherent%2520Interpretability%2520via%2520Self-Supervised%2520Masking%26entry.906535625%3DEyad%2520Alshami%2520and%2520Shashank%2520Agnihotri%2520and%2520Bernt%2520Schiele%2520and%2520Margret%2520Keuper%26entry.1292438233%3D%2520%2520It%2520has%2520been%2520observed%2520that%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520often%2520use%2520both%2520genuine%250Aas%2520well%2520as%2520spurious%2520features.%2520In%2520this%2520work%252C%2520we%2520propose%2520%2522Amending%2520Inherent%250AInterpretability%2520via%2520Self-Supervised%2520Masking%2522%2520%2528AIM%2529%252C%2520a%2520simple%2520yet%2520interestingly%250Aeffective%2520method%2520that%2520promotes%2520the%2520network%2527s%2520utilization%2520of%2520genuine%2520features%250Aover%2520spurious%2520alternatives%2520without%2520requiring%2520additional%2520annotations.%2520In%250Aparticular%252C%2520AIM%2520uses%2520features%2520at%2520multiple%2520encoding%2520stages%2520to%2520guide%2520a%250Aself-supervised%252C%2520sample-specific%2520feature-masking%2520process.%2520As%2520a%2520result%252C%2520AIM%250Aenables%2520the%2520training%2520of%2520well-performing%2520and%2520inherently%2520interpretable%2520models%250Athat%2520faithfully%2520summarize%2520the%2520decision%2520process.%2520We%2520validate%2520AIM%2520across%2520a%250Adiverse%2520range%2520of%2520challenging%2520datasets%2520that%2520test%2520both%2520out-of-distribution%250Ageneralization%2520and%2520fine-grained%2520visual%2520understanding.%2520These%2520include%250Ageneral-purpose%2520classification%2520benchmarks%2520such%2520as%2520ImageNet100%252C%2520HardImageNet%252C%250Aand%2520ImageWoof%252C%2520as%2520well%2520as%2520fine-grained%2520classification%2520datasets%2520such%2520as%250AWaterbirds%252C%2520TravelingBirds%252C%2520and%2520CUB-200.%2520AIM%2520demonstrates%2520significant%2520dual%250Abenefits%253A%2520interpretability%2520improvements%252C%2520as%2520measured%2520by%2520the%2520Energy%2520Pointing%250AGame%2520%2528EPG%2529%2520score%252C%2520and%2520accuracy%2520gains%2520over%2520strong%2520baselines.%2520These%2520consistent%250Agains%2520across%2520domains%2520and%2520architectures%2520provide%2520compelling%2520evidence%2520that%2520AIM%250Apromotes%2520the%2520use%2520of%2520genuine%2520and%2520meaningful%2520features%2520that%2520directly%2520contribute%2520to%250Aimproved%2520generalization%2520and%2520human-aligned%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIM%3A%20Amending%20Inherent%20Interpretability%20via%20Self-Supervised%20Masking&entry.906535625=Eyad%20Alshami%20and%20Shashank%20Agnihotri%20and%20Bernt%20Schiele%20and%20Margret%20Keuper&entry.1292438233=%20%20It%20has%20been%20observed%20that%20deep%20neural%20networks%20%28DNNs%29%20often%20use%20both%20genuine%0Aas%20well%20as%20spurious%20features.%20In%20this%20work%2C%20we%20propose%20%22Amending%20Inherent%0AInterpretability%20via%20Self-Supervised%20Masking%22%20%28AIM%29%2C%20a%20simple%20yet%20interestingly%0Aeffective%20method%20that%20promotes%20the%20network%27s%20utilization%20of%20genuine%20features%0Aover%20spurious%20alternatives%20without%20requiring%20additional%20annotations.%20In%0Aparticular%2C%20AIM%20uses%20features%20at%20multiple%20encoding%20stages%20to%20guide%20a%0Aself-supervised%2C%20sample-specific%20feature-masking%20process.%20As%20a%20result%2C%20AIM%0Aenables%20the%20training%20of%20well-performing%20and%20inherently%20interpretable%20models%0Athat%20faithfully%20summarize%20the%20decision%20process.%20We%20validate%20AIM%20across%20a%0Adiverse%20range%20of%20challenging%20datasets%20that%20test%20both%20out-of-distribution%0Ageneralization%20and%20fine-grained%20visual%20understanding.%20These%20include%0Ageneral-purpose%20classification%20benchmarks%20such%20as%20ImageNet100%2C%20HardImageNet%2C%0Aand%20ImageWoof%2C%20as%20well%20as%20fine-grained%20classification%20datasets%20such%20as%0AWaterbirds%2C%20TravelingBirds%2C%20and%20CUB-200.%20AIM%20demonstrates%20significant%20dual%0Abenefits%3A%20interpretability%20improvements%2C%20as%20measured%20by%20the%20Energy%20Pointing%0AGame%20%28EPG%29%20score%2C%20and%20accuracy%20gains%20over%20strong%20baselines.%20These%20consistent%0Agains%20across%20domains%20and%20architectures%20provide%20compelling%20evidence%20that%20AIM%0Apromotes%20the%20use%20of%20genuine%20and%20meaningful%20features%20that%20directly%20contribute%20to%0Aimproved%20generalization%20and%20human-aligned%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11502v1&entry.124074799=Read"},
{"title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme\n  Reasoning Efficiency in Large Language Models", "author": "Qiguang Chen and Dengyun Peng and Jinhao Liu and HuiKang Su and Jiannan Guan and Libo Qin and Wanxiang Che", "abstract": "  Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.\n", "link": "http://arxiv.org/abs/2508.11582v1", "date": "2025-08-15", "relevancy": 2.7555, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aware%20First%2C%20Think%20Less%3A%20Dynamic%20Boundary%20Self-Awareness%20Drives%20Extreme%0A%20%20Reasoning%20Efficiency%20in%20Large%20Language%20Models&body=Title%3A%20Aware%20First%2C%20Think%20Less%3A%20Dynamic%20Boundary%20Self-Awareness%20Drives%20Extreme%0A%20%20Reasoning%20Efficiency%20in%20Large%20Language%20Models%0AAuthor%3A%20Qiguang%20Chen%20and%20Dengyun%20Peng%20and%20Jinhao%20Liu%20and%20HuiKang%20Su%20and%20Jiannan%20Guan%20and%20Libo%20Qin%20and%20Wanxiang%20Che%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20greatly%20improved%0Atheir%20capabilities%20on%20complex%20reasoning%20tasks%20through%20Long%20Chain-of-Thought%0A%28CoT%29.%20However%2C%20this%20approach%20often%20results%20in%20substantial%20redundancy%2C%0Aimpairing%20computational%20efficiency%20and%20causing%20significant%20delays%20in%20real-time%0Aapplications.%20To%20improve%20the%20efficiency%2C%20current%20methods%20often%20rely%20on%0Ahuman-defined%20difficulty%20priors%2C%20which%20do%20not%20align%20with%20the%20LLM%27s%20self-awared%0Adifficulty%2C%20leading%20to%20inefficiencies.%20In%20this%20paper%2C%20we%20introduce%20the%20Dynamic%0AReasoning-Boundary%20Self-Awareness%20Framework%20%28DR.%20SAF%29%2C%20which%20enables%20models%20to%0Adynamically%20assess%20and%20adjust%20their%20reasoning%20depth%20in%20response%20to%20problem%0Acomplexity.%20DR.%20SAF%20integrates%20three%20key%20components%3A%20Boundary%20Self-Awareness%0AAlignment%2C%20Adaptive%20Reward%20Management%2C%20and%20a%20Boundary%20Preservation%20Mechanism.%0AThese%20components%20allow%20models%20to%20optimize%20their%20reasoning%20processes%2C%20balancing%0Aefficiency%20and%20accuracy%20without%20compromising%20performance.%20Our%20experimental%0Aresults%20demonstrate%20that%20DR.%20SAF%20achieves%20a%2049.27%25%20reduction%20in%20total%20response%0Atokens%20with%20minimal%20loss%20in%20accuracy.%20The%20framework%20also%20delivers%20a%206.59x%20gain%0Ain%20token%20efficiency%20and%20a%205x%20reduction%20in%20training%20time%2C%20making%20it%20well-suited%0Ato%20resource-limited%20settings.%20During%20extreme%20training%2C%20DR.%20SAF%20can%20even%20surpass%0Atraditional%20instruction-based%20models%20in%20token%20efficiency%20with%20more%20than%2016%25%0Aaccuracy%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAware%2520First%252C%2520Think%2520Less%253A%2520Dynamic%2520Boundary%2520Self-Awareness%2520Drives%2520Extreme%250A%2520%2520Reasoning%2520Efficiency%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DQiguang%2520Chen%2520and%2520Dengyun%2520Peng%2520and%2520Jinhao%2520Liu%2520and%2520HuiKang%2520Su%2520and%2520Jiannan%2520Guan%2520and%2520Libo%2520Qin%2520and%2520Wanxiang%2520Che%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520greatly%2520improved%250Atheir%2520capabilities%2520on%2520complex%2520reasoning%2520tasks%2520through%2520Long%2520Chain-of-Thought%250A%2528CoT%2529.%2520However%252C%2520this%2520approach%2520often%2520results%2520in%2520substantial%2520redundancy%252C%250Aimpairing%2520computational%2520efficiency%2520and%2520causing%2520significant%2520delays%2520in%2520real-time%250Aapplications.%2520To%2520improve%2520the%2520efficiency%252C%2520current%2520methods%2520often%2520rely%2520on%250Ahuman-defined%2520difficulty%2520priors%252C%2520which%2520do%2520not%2520align%2520with%2520the%2520LLM%2527s%2520self-awared%250Adifficulty%252C%2520leading%2520to%2520inefficiencies.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Dynamic%250AReasoning-Boundary%2520Self-Awareness%2520Framework%2520%2528DR.%2520SAF%2529%252C%2520which%2520enables%2520models%2520to%250Adynamically%2520assess%2520and%2520adjust%2520their%2520reasoning%2520depth%2520in%2520response%2520to%2520problem%250Acomplexity.%2520DR.%2520SAF%2520integrates%2520three%2520key%2520components%253A%2520Boundary%2520Self-Awareness%250AAlignment%252C%2520Adaptive%2520Reward%2520Management%252C%2520and%2520a%2520Boundary%2520Preservation%2520Mechanism.%250AThese%2520components%2520allow%2520models%2520to%2520optimize%2520their%2520reasoning%2520processes%252C%2520balancing%250Aefficiency%2520and%2520accuracy%2520without%2520compromising%2520performance.%2520Our%2520experimental%250Aresults%2520demonstrate%2520that%2520DR.%2520SAF%2520achieves%2520a%252049.27%2525%2520reduction%2520in%2520total%2520response%250Atokens%2520with%2520minimal%2520loss%2520in%2520accuracy.%2520The%2520framework%2520also%2520delivers%2520a%25206.59x%2520gain%250Ain%2520token%2520efficiency%2520and%2520a%25205x%2520reduction%2520in%2520training%2520time%252C%2520making%2520it%2520well-suited%250Ato%2520resource-limited%2520settings.%2520During%2520extreme%2520training%252C%2520DR.%2520SAF%2520can%2520even%2520surpass%250Atraditional%2520instruction-based%2520models%2520in%2520token%2520efficiency%2520with%2520more%2520than%252016%2525%250Aaccuracy%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aware%20First%2C%20Think%20Less%3A%20Dynamic%20Boundary%20Self-Awareness%20Drives%20Extreme%0A%20%20Reasoning%20Efficiency%20in%20Large%20Language%20Models&entry.906535625=Qiguang%20Chen%20and%20Dengyun%20Peng%20and%20Jinhao%20Liu%20and%20HuiKang%20Su%20and%20Jiannan%20Guan%20and%20Libo%20Qin%20and%20Wanxiang%20Che&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20greatly%20improved%0Atheir%20capabilities%20on%20complex%20reasoning%20tasks%20through%20Long%20Chain-of-Thought%0A%28CoT%29.%20However%2C%20this%20approach%20often%20results%20in%20substantial%20redundancy%2C%0Aimpairing%20computational%20efficiency%20and%20causing%20significant%20delays%20in%20real-time%0Aapplications.%20To%20improve%20the%20efficiency%2C%20current%20methods%20often%20rely%20on%0Ahuman-defined%20difficulty%20priors%2C%20which%20do%20not%20align%20with%20the%20LLM%27s%20self-awared%0Adifficulty%2C%20leading%20to%20inefficiencies.%20In%20this%20paper%2C%20we%20introduce%20the%20Dynamic%0AReasoning-Boundary%20Self-Awareness%20Framework%20%28DR.%20SAF%29%2C%20which%20enables%20models%20to%0Adynamically%20assess%20and%20adjust%20their%20reasoning%20depth%20in%20response%20to%20problem%0Acomplexity.%20DR.%20SAF%20integrates%20three%20key%20components%3A%20Boundary%20Self-Awareness%0AAlignment%2C%20Adaptive%20Reward%20Management%2C%20and%20a%20Boundary%20Preservation%20Mechanism.%0AThese%20components%20allow%20models%20to%20optimize%20their%20reasoning%20processes%2C%20balancing%0Aefficiency%20and%20accuracy%20without%20compromising%20performance.%20Our%20experimental%0Aresults%20demonstrate%20that%20DR.%20SAF%20achieves%20a%2049.27%25%20reduction%20in%20total%20response%0Atokens%20with%20minimal%20loss%20in%20accuracy.%20The%20framework%20also%20delivers%20a%206.59x%20gain%0Ain%20token%20efficiency%20and%20a%205x%20reduction%20in%20training%20time%2C%20making%20it%20well-suited%0Ato%20resource-limited%20settings.%20During%20extreme%20training%2C%20DR.%20SAF%20can%20even%20surpass%0Atraditional%20instruction-based%20models%20in%20token%20efficiency%20with%20more%20than%2016%25%0Aaccuracy%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11582v1&entry.124074799=Read"},
{"title": "FancyVideo: Towards Dynamic and Consistent Video Generation via\n  Cross-frame Textual Guidance", "author": "Jiasong Feng and Ao Ma and Jing Wang and Ke Cao and Zhanjie Zhang", "abstract": "  Synthesizing motion-rich and temporally consistent videos remains a challenge\nin artificial intelligence, especially when dealing with extended durations.\nExisting text-to-video (T2V) models commonly employ spatial cross-attention for\ntext control, equivalently guiding different frame generations without\nframe-specific textual guidance. Thus, the model's capacity to comprehend the\ntemporal logic conveyed in prompts and generate videos with coherent motion is\nrestricted. To tackle this limitation, we introduce FancyVideo, an innovative\nvideo generator that improves the existing text-control mechanism with the\nwell-designed Cross-frame Textual Guidance Module (CTGM). Specifically, CTGM\nincorporates the Temporal Information Injector (TII) and Temporal Affinity\nRefiner (TAR) at the beginning and end of cross-attention, respectively, to\nachieve frame-specific textual guidance. Firstly, TII injects frame-specific\ninformation from latent features into text conditions, thereby obtaining\ncross-frame textual conditions. Then, TAR refines the correlation matrix\nbetween cross-frame textual conditions and latent features along the time\ndimension. Extensive experiments comprising both quantitative and qualitative\nevaluations demonstrate the effectiveness of FancyVideo. Our approach achieves\nstate-of-the-art T2V generation results on the EvalCrafter benchmark and\nfacilitates the synthesis of dynamic and consistent videos. Note that the T2V\nprocess of FancyVideo essentially involves a text-to-image step followed by\nT+I2V. This means it also supports the generation of videos from user images,\ni.e., the image-to-video (I2V) task. A significant number of experiments have\nshown that its performance is also outstanding.\n", "link": "http://arxiv.org/abs/2408.08189v4", "date": "2025-08-15", "relevancy": 2.7036, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.703}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6994}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FancyVideo%3A%20Towards%20Dynamic%20and%20Consistent%20Video%20Generation%20via%0A%20%20Cross-frame%20Textual%20Guidance&body=Title%3A%20FancyVideo%3A%20Towards%20Dynamic%20and%20Consistent%20Video%20Generation%20via%0A%20%20Cross-frame%20Textual%20Guidance%0AAuthor%3A%20Jiasong%20Feng%20and%20Ao%20Ma%20and%20Jing%20Wang%20and%20Ke%20Cao%20and%20Zhanjie%20Zhang%0AAbstract%3A%20%20%20Synthesizing%20motion-rich%20and%20temporally%20consistent%20videos%20remains%20a%20challenge%0Ain%20artificial%20intelligence%2C%20especially%20when%20dealing%20with%20extended%20durations.%0AExisting%20text-to-video%20%28T2V%29%20models%20commonly%20employ%20spatial%20cross-attention%20for%0Atext%20control%2C%20equivalently%20guiding%20different%20frame%20generations%20without%0Aframe-specific%20textual%20guidance.%20Thus%2C%20the%20model%27s%20capacity%20to%20comprehend%20the%0Atemporal%20logic%20conveyed%20in%20prompts%20and%20generate%20videos%20with%20coherent%20motion%20is%0Arestricted.%20To%20tackle%20this%20limitation%2C%20we%20introduce%20FancyVideo%2C%20an%20innovative%0Avideo%20generator%20that%20improves%20the%20existing%20text-control%20mechanism%20with%20the%0Awell-designed%20Cross-frame%20Textual%20Guidance%20Module%20%28CTGM%29.%20Specifically%2C%20CTGM%0Aincorporates%20the%20Temporal%20Information%20Injector%20%28TII%29%20and%20Temporal%20Affinity%0ARefiner%20%28TAR%29%20at%20the%20beginning%20and%20end%20of%20cross-attention%2C%20respectively%2C%20to%0Aachieve%20frame-specific%20textual%20guidance.%20Firstly%2C%20TII%20injects%20frame-specific%0Ainformation%20from%20latent%20features%20into%20text%20conditions%2C%20thereby%20obtaining%0Across-frame%20textual%20conditions.%20Then%2C%20TAR%20refines%20the%20correlation%20matrix%0Abetween%20cross-frame%20textual%20conditions%20and%20latent%20features%20along%20the%20time%0Adimension.%20Extensive%20experiments%20comprising%20both%20quantitative%20and%20qualitative%0Aevaluations%20demonstrate%20the%20effectiveness%20of%20FancyVideo.%20Our%20approach%20achieves%0Astate-of-the-art%20T2V%20generation%20results%20on%20the%20EvalCrafter%20benchmark%20and%0Afacilitates%20the%20synthesis%20of%20dynamic%20and%20consistent%20videos.%20Note%20that%20the%20T2V%0Aprocess%20of%20FancyVideo%20essentially%20involves%20a%20text-to-image%20step%20followed%20by%0AT%2BI2V.%20This%20means%20it%20also%20supports%20the%20generation%20of%20videos%20from%20user%20images%2C%0Ai.e.%2C%20the%20image-to-video%20%28I2V%29%20task.%20A%20significant%20number%20of%20experiments%20have%0Ashown%20that%20its%20performance%20is%20also%20outstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08189v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFancyVideo%253A%2520Towards%2520Dynamic%2520and%2520Consistent%2520Video%2520Generation%2520via%250A%2520%2520Cross-frame%2520Textual%2520Guidance%26entry.906535625%3DJiasong%2520Feng%2520and%2520Ao%2520Ma%2520and%2520Jing%2520Wang%2520and%2520Ke%2520Cao%2520and%2520Zhanjie%2520Zhang%26entry.1292438233%3D%2520%2520Synthesizing%2520motion-rich%2520and%2520temporally%2520consistent%2520videos%2520remains%2520a%2520challenge%250Ain%2520artificial%2520intelligence%252C%2520especially%2520when%2520dealing%2520with%2520extended%2520durations.%250AExisting%2520text-to-video%2520%2528T2V%2529%2520models%2520commonly%2520employ%2520spatial%2520cross-attention%2520for%250Atext%2520control%252C%2520equivalently%2520guiding%2520different%2520frame%2520generations%2520without%250Aframe-specific%2520textual%2520guidance.%2520Thus%252C%2520the%2520model%2527s%2520capacity%2520to%2520comprehend%2520the%250Atemporal%2520logic%2520conveyed%2520in%2520prompts%2520and%2520generate%2520videos%2520with%2520coherent%2520motion%2520is%250Arestricted.%2520To%2520tackle%2520this%2520limitation%252C%2520we%2520introduce%2520FancyVideo%252C%2520an%2520innovative%250Avideo%2520generator%2520that%2520improves%2520the%2520existing%2520text-control%2520mechanism%2520with%2520the%250Awell-designed%2520Cross-frame%2520Textual%2520Guidance%2520Module%2520%2528CTGM%2529.%2520Specifically%252C%2520CTGM%250Aincorporates%2520the%2520Temporal%2520Information%2520Injector%2520%2528TII%2529%2520and%2520Temporal%2520Affinity%250ARefiner%2520%2528TAR%2529%2520at%2520the%2520beginning%2520and%2520end%2520of%2520cross-attention%252C%2520respectively%252C%2520to%250Aachieve%2520frame-specific%2520textual%2520guidance.%2520Firstly%252C%2520TII%2520injects%2520frame-specific%250Ainformation%2520from%2520latent%2520features%2520into%2520text%2520conditions%252C%2520thereby%2520obtaining%250Across-frame%2520textual%2520conditions.%2520Then%252C%2520TAR%2520refines%2520the%2520correlation%2520matrix%250Abetween%2520cross-frame%2520textual%2520conditions%2520and%2520latent%2520features%2520along%2520the%2520time%250Adimension.%2520Extensive%2520experiments%2520comprising%2520both%2520quantitative%2520and%2520qualitative%250Aevaluations%2520demonstrate%2520the%2520effectiveness%2520of%2520FancyVideo.%2520Our%2520approach%2520achieves%250Astate-of-the-art%2520T2V%2520generation%2520results%2520on%2520the%2520EvalCrafter%2520benchmark%2520and%250Afacilitates%2520the%2520synthesis%2520of%2520dynamic%2520and%2520consistent%2520videos.%2520Note%2520that%2520the%2520T2V%250Aprocess%2520of%2520FancyVideo%2520essentially%2520involves%2520a%2520text-to-image%2520step%2520followed%2520by%250AT%252BI2V.%2520This%2520means%2520it%2520also%2520supports%2520the%2520generation%2520of%2520videos%2520from%2520user%2520images%252C%250Ai.e.%252C%2520the%2520image-to-video%2520%2528I2V%2529%2520task.%2520A%2520significant%2520number%2520of%2520experiments%2520have%250Ashown%2520that%2520its%2520performance%2520is%2520also%2520outstanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08189v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FancyVideo%3A%20Towards%20Dynamic%20and%20Consistent%20Video%20Generation%20via%0A%20%20Cross-frame%20Textual%20Guidance&entry.906535625=Jiasong%20Feng%20and%20Ao%20Ma%20and%20Jing%20Wang%20and%20Ke%20Cao%20and%20Zhanjie%20Zhang&entry.1292438233=%20%20Synthesizing%20motion-rich%20and%20temporally%20consistent%20videos%20remains%20a%20challenge%0Ain%20artificial%20intelligence%2C%20especially%20when%20dealing%20with%20extended%20durations.%0AExisting%20text-to-video%20%28T2V%29%20models%20commonly%20employ%20spatial%20cross-attention%20for%0Atext%20control%2C%20equivalently%20guiding%20different%20frame%20generations%20without%0Aframe-specific%20textual%20guidance.%20Thus%2C%20the%20model%27s%20capacity%20to%20comprehend%20the%0Atemporal%20logic%20conveyed%20in%20prompts%20and%20generate%20videos%20with%20coherent%20motion%20is%0Arestricted.%20To%20tackle%20this%20limitation%2C%20we%20introduce%20FancyVideo%2C%20an%20innovative%0Avideo%20generator%20that%20improves%20the%20existing%20text-control%20mechanism%20with%20the%0Awell-designed%20Cross-frame%20Textual%20Guidance%20Module%20%28CTGM%29.%20Specifically%2C%20CTGM%0Aincorporates%20the%20Temporal%20Information%20Injector%20%28TII%29%20and%20Temporal%20Affinity%0ARefiner%20%28TAR%29%20at%20the%20beginning%20and%20end%20of%20cross-attention%2C%20respectively%2C%20to%0Aachieve%20frame-specific%20textual%20guidance.%20Firstly%2C%20TII%20injects%20frame-specific%0Ainformation%20from%20latent%20features%20into%20text%20conditions%2C%20thereby%20obtaining%0Across-frame%20textual%20conditions.%20Then%2C%20TAR%20refines%20the%20correlation%20matrix%0Abetween%20cross-frame%20textual%20conditions%20and%20latent%20features%20along%20the%20time%0Adimension.%20Extensive%20experiments%20comprising%20both%20quantitative%20and%20qualitative%0Aevaluations%20demonstrate%20the%20effectiveness%20of%20FancyVideo.%20Our%20approach%20achieves%0Astate-of-the-art%20T2V%20generation%20results%20on%20the%20EvalCrafter%20benchmark%20and%0Afacilitates%20the%20synthesis%20of%20dynamic%20and%20consistent%20videos.%20Note%20that%20the%20T2V%0Aprocess%20of%20FancyVideo%20essentially%20involves%20a%20text-to-image%20step%20followed%20by%0AT%2BI2V.%20This%20means%20it%20also%20supports%20the%20generation%20of%20videos%20from%20user%20images%2C%0Ai.e.%2C%20the%20image-to-video%20%28I2V%29%20task.%20A%20significant%20number%20of%20experiments%20have%0Ashown%20that%20its%20performance%20is%20also%20outstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08189v4&entry.124074799=Read"},
{"title": "SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models", "author": "Fabian H. Reith and Jannik Franzen and Dinesh R. Palli and J. Lorenz Rumberger and Dagmar Kainmueller", "abstract": "  Deep neural networks have become the go-to method for biomedical instance\nsegmentation. Generalist models like Cellpose demonstrate state-of-the-art\nperformance across diverse cellular data, though their effectiveness often\ndegrades on domains that differ from their training data. While supervised\nfine-tuning can address this limitation, it requires annotated data that may\nnot be readily available. We propose SelfAdapt, a method that enables the\nadaptation of pre-trained cell segmentation models without the need for labels.\nOur approach builds upon student-teacher augmentation consistency training,\nintroducing L2-SP regularization and label-free stopping criteria. We evaluate\nour method on the LiveCell and TissueNet datasets, demonstrating relative\nimprovements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we\nshow that our unsupervised adaptation can further improve models that were\npreviously fine-tuned with supervision. We release SelfAdapt as an easy-to-use\nextension of the Cellpose framework. The code for our method is publicly\navailable at https: //github.com/Kainmueller-Lab/self_adapt.\n", "link": "http://arxiv.org/abs/2508.11411v1", "date": "2025-08-15", "relevancy": 2.6817, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5456}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.543}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelfAdapt%3A%20Unsupervised%20Domain%20Adaptation%20of%20Cell%20Segmentation%20Models&body=Title%3A%20SelfAdapt%3A%20Unsupervised%20Domain%20Adaptation%20of%20Cell%20Segmentation%20Models%0AAuthor%3A%20Fabian%20H.%20Reith%20and%20Jannik%20Franzen%20and%20Dinesh%20R.%20Palli%20and%20J.%20Lorenz%20Rumberger%20and%20Dagmar%20Kainmueller%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20become%20the%20go-to%20method%20for%20biomedical%20instance%0Asegmentation.%20Generalist%20models%20like%20Cellpose%20demonstrate%20state-of-the-art%0Aperformance%20across%20diverse%20cellular%20data%2C%20though%20their%20effectiveness%20often%0Adegrades%20on%20domains%20that%20differ%20from%20their%20training%20data.%20While%20supervised%0Afine-tuning%20can%20address%20this%20limitation%2C%20it%20requires%20annotated%20data%20that%20may%0Anot%20be%20readily%20available.%20We%20propose%20SelfAdapt%2C%20a%20method%20that%20enables%20the%0Aadaptation%20of%20pre-trained%20cell%20segmentation%20models%20without%20the%20need%20for%20labels.%0AOur%20approach%20builds%20upon%20student-teacher%20augmentation%20consistency%20training%2C%0Aintroducing%20L2-SP%20regularization%20and%20label-free%20stopping%20criteria.%20We%20evaluate%0Aour%20method%20on%20the%20LiveCell%20and%20TissueNet%20datasets%2C%20demonstrating%20relative%0Aimprovements%20in%20AP0.5%20of%20up%20to%2029.64%25%20over%20baseline%20Cellpose.%20Additionally%2C%20we%0Ashow%20that%20our%20unsupervised%20adaptation%20can%20further%20improve%20models%20that%20were%0Apreviously%20fine-tuned%20with%20supervision.%20We%20release%20SelfAdapt%20as%20an%20easy-to-use%0Aextension%20of%20the%20Cellpose%20framework.%20The%20code%20for%20our%20method%20is%20publicly%0Aavailable%20at%20https%3A%20//github.com/Kainmueller-Lab/self_adapt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfAdapt%253A%2520Unsupervised%2520Domain%2520Adaptation%2520of%2520Cell%2520Segmentation%2520Models%26entry.906535625%3DFabian%2520H.%2520Reith%2520and%2520Jannik%2520Franzen%2520and%2520Dinesh%2520R.%2520Palli%2520and%2520J.%2520Lorenz%2520Rumberger%2520and%2520Dagmar%2520Kainmueller%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520become%2520the%2520go-to%2520method%2520for%2520biomedical%2520instance%250Asegmentation.%2520Generalist%2520models%2520like%2520Cellpose%2520demonstrate%2520state-of-the-art%250Aperformance%2520across%2520diverse%2520cellular%2520data%252C%2520though%2520their%2520effectiveness%2520often%250Adegrades%2520on%2520domains%2520that%2520differ%2520from%2520their%2520training%2520data.%2520While%2520supervised%250Afine-tuning%2520can%2520address%2520this%2520limitation%252C%2520it%2520requires%2520annotated%2520data%2520that%2520may%250Anot%2520be%2520readily%2520available.%2520We%2520propose%2520SelfAdapt%252C%2520a%2520method%2520that%2520enables%2520the%250Aadaptation%2520of%2520pre-trained%2520cell%2520segmentation%2520models%2520without%2520the%2520need%2520for%2520labels.%250AOur%2520approach%2520builds%2520upon%2520student-teacher%2520augmentation%2520consistency%2520training%252C%250Aintroducing%2520L2-SP%2520regularization%2520and%2520label-free%2520stopping%2520criteria.%2520We%2520evaluate%250Aour%2520method%2520on%2520the%2520LiveCell%2520and%2520TissueNet%2520datasets%252C%2520demonstrating%2520relative%250Aimprovements%2520in%2520AP0.5%2520of%2520up%2520to%252029.64%2525%2520over%2520baseline%2520Cellpose.%2520Additionally%252C%2520we%250Ashow%2520that%2520our%2520unsupervised%2520adaptation%2520can%2520further%2520improve%2520models%2520that%2520were%250Apreviously%2520fine-tuned%2520with%2520supervision.%2520We%2520release%2520SelfAdapt%2520as%2520an%2520easy-to-use%250Aextension%2520of%2520the%2520Cellpose%2520framework.%2520The%2520code%2520for%2520our%2520method%2520is%2520publicly%250Aavailable%2520at%2520https%253A%2520//github.com/Kainmueller-Lab/self_adapt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfAdapt%3A%20Unsupervised%20Domain%20Adaptation%20of%20Cell%20Segmentation%20Models&entry.906535625=Fabian%20H.%20Reith%20and%20Jannik%20Franzen%20and%20Dinesh%20R.%20Palli%20and%20J.%20Lorenz%20Rumberger%20and%20Dagmar%20Kainmueller&entry.1292438233=%20%20Deep%20neural%20networks%20have%20become%20the%20go-to%20method%20for%20biomedical%20instance%0Asegmentation.%20Generalist%20models%20like%20Cellpose%20demonstrate%20state-of-the-art%0Aperformance%20across%20diverse%20cellular%20data%2C%20though%20their%20effectiveness%20often%0Adegrades%20on%20domains%20that%20differ%20from%20their%20training%20data.%20While%20supervised%0Afine-tuning%20can%20address%20this%20limitation%2C%20it%20requires%20annotated%20data%20that%20may%0Anot%20be%20readily%20available.%20We%20propose%20SelfAdapt%2C%20a%20method%20that%20enables%20the%0Aadaptation%20of%20pre-trained%20cell%20segmentation%20models%20without%20the%20need%20for%20labels.%0AOur%20approach%20builds%20upon%20student-teacher%20augmentation%20consistency%20training%2C%0Aintroducing%20L2-SP%20regularization%20and%20label-free%20stopping%20criteria.%20We%20evaluate%0Aour%20method%20on%20the%20LiveCell%20and%20TissueNet%20datasets%2C%20demonstrating%20relative%0Aimprovements%20in%20AP0.5%20of%20up%20to%2029.64%25%20over%20baseline%20Cellpose.%20Additionally%2C%20we%0Ashow%20that%20our%20unsupervised%20adaptation%20can%20further%20improve%20models%20that%20were%0Apreviously%20fine-tuned%20with%20supervision.%20We%20release%20SelfAdapt%20as%20an%20easy-to-use%0Aextension%20of%20the%20Cellpose%20framework.%20The%20code%20for%20our%20method%20is%20publicly%0Aavailable%20at%20https%3A%20//github.com/Kainmueller-Lab/self_adapt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11411v1&entry.124074799=Read"},
{"title": "TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors\n  Enhanced with Coordinate and Task-Aware Representations", "author": "Xinyi Yin and Wenbo Yuan and Xuecheng Wu and Liangyu Fu and Danlei Huang", "abstract": "  Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming\nincreasingly crucial. While YOLO-based detection methods excel in real-time\ntasks, they remain hindered by challenges including small objects, task\nconflicts, and multi-scale fusion in AHBD. To tackle them, we propose\nTACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate\nAttention Module to enhance small object detection, a Task-Aware Attention\nModule to deal with classification-regression conflicts, and a Strengthen Neck\nNetwork for refined multi-scale fusion, respectively. In addition, we optimize\nAnchor Box sizes using K-means clustering and deploy DIoU-Loss to improve\nbounding box regression. The Personnel Anomalous Behavior Detection (PABD)\ndataset, which includes 8,529 samples across four behavior categories, is also\npresented. Extensive experimental results indicate that TACR-YOLO achieves\n91.92% mAP on PABD, with competitive speed and robustness. Ablation studies\nhighlight the contribution of each improvement. This work provides new insights\nfor abnormal behavior detection under special scenarios, advancing its\nprogress.\n", "link": "http://arxiv.org/abs/2508.11478v1", "date": "2025-08-15", "relevancy": 2.6526, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5472}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5248}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TACR-YOLO%3A%20A%20Real-time%20Detection%20Framework%20for%20Abnormal%20Human%20Behaviors%0A%20%20Enhanced%20with%20Coordinate%20and%20Task-Aware%20Representations&body=Title%3A%20TACR-YOLO%3A%20A%20Real-time%20Detection%20Framework%20for%20Abnormal%20Human%20Behaviors%0A%20%20Enhanced%20with%20Coordinate%20and%20Task-Aware%20Representations%0AAuthor%3A%20Xinyi%20Yin%20and%20Wenbo%20Yuan%20and%20Xuecheng%20Wu%20and%20Liangyu%20Fu%20and%20Danlei%20Huang%0AAbstract%3A%20%20%20Abnormal%20Human%20Behavior%20Detection%20%28AHBD%29%20under%20special%20scenarios%20is%20becoming%0Aincreasingly%20crucial.%20While%20YOLO-based%20detection%20methods%20excel%20in%20real-time%0Atasks%2C%20they%20remain%20hindered%20by%20challenges%20including%20small%20objects%2C%20task%0Aconflicts%2C%20and%20multi-scale%20fusion%20in%20AHBD.%20To%20tackle%20them%2C%20we%20propose%0ATACR-YOLO%2C%20a%20new%20real-time%20framework%20for%20AHBD.%20We%20introduce%20a%20Coordinate%0AAttention%20Module%20to%20enhance%20small%20object%20detection%2C%20a%20Task-Aware%20Attention%0AModule%20to%20deal%20with%20classification-regression%20conflicts%2C%20and%20a%20Strengthen%20Neck%0ANetwork%20for%20refined%20multi-scale%20fusion%2C%20respectively.%20In%20addition%2C%20we%20optimize%0AAnchor%20Box%20sizes%20using%20K-means%20clustering%20and%20deploy%20DIoU-Loss%20to%20improve%0Abounding%20box%20regression.%20The%20Personnel%20Anomalous%20Behavior%20Detection%20%28PABD%29%0Adataset%2C%20which%20includes%208%2C529%20samples%20across%20four%20behavior%20categories%2C%20is%20also%0Apresented.%20Extensive%20experimental%20results%20indicate%20that%20TACR-YOLO%20achieves%0A91.92%25%20mAP%20on%20PABD%2C%20with%20competitive%20speed%20and%20robustness.%20Ablation%20studies%0Ahighlight%20the%20contribution%20of%20each%20improvement.%20This%20work%20provides%20new%20insights%0Afor%20abnormal%20behavior%20detection%20under%20special%20scenarios%2C%20advancing%20its%0Aprogress.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTACR-YOLO%253A%2520A%2520Real-time%2520Detection%2520Framework%2520for%2520Abnormal%2520Human%2520Behaviors%250A%2520%2520Enhanced%2520with%2520Coordinate%2520and%2520Task-Aware%2520Representations%26entry.906535625%3DXinyi%2520Yin%2520and%2520Wenbo%2520Yuan%2520and%2520Xuecheng%2520Wu%2520and%2520Liangyu%2520Fu%2520and%2520Danlei%2520Huang%26entry.1292438233%3D%2520%2520Abnormal%2520Human%2520Behavior%2520Detection%2520%2528AHBD%2529%2520under%2520special%2520scenarios%2520is%2520becoming%250Aincreasingly%2520crucial.%2520While%2520YOLO-based%2520detection%2520methods%2520excel%2520in%2520real-time%250Atasks%252C%2520they%2520remain%2520hindered%2520by%2520challenges%2520including%2520small%2520objects%252C%2520task%250Aconflicts%252C%2520and%2520multi-scale%2520fusion%2520in%2520AHBD.%2520To%2520tackle%2520them%252C%2520we%2520propose%250ATACR-YOLO%252C%2520a%2520new%2520real-time%2520framework%2520for%2520AHBD.%2520We%2520introduce%2520a%2520Coordinate%250AAttention%2520Module%2520to%2520enhance%2520small%2520object%2520detection%252C%2520a%2520Task-Aware%2520Attention%250AModule%2520to%2520deal%2520with%2520classification-regression%2520conflicts%252C%2520and%2520a%2520Strengthen%2520Neck%250ANetwork%2520for%2520refined%2520multi-scale%2520fusion%252C%2520respectively.%2520In%2520addition%252C%2520we%2520optimize%250AAnchor%2520Box%2520sizes%2520using%2520K-means%2520clustering%2520and%2520deploy%2520DIoU-Loss%2520to%2520improve%250Abounding%2520box%2520regression.%2520The%2520Personnel%2520Anomalous%2520Behavior%2520Detection%2520%2528PABD%2529%250Adataset%252C%2520which%2520includes%25208%252C529%2520samples%2520across%2520four%2520behavior%2520categories%252C%2520is%2520also%250Apresented.%2520Extensive%2520experimental%2520results%2520indicate%2520that%2520TACR-YOLO%2520achieves%250A91.92%2525%2520mAP%2520on%2520PABD%252C%2520with%2520competitive%2520speed%2520and%2520robustness.%2520Ablation%2520studies%250Ahighlight%2520the%2520contribution%2520of%2520each%2520improvement.%2520This%2520work%2520provides%2520new%2520insights%250Afor%2520abnormal%2520behavior%2520detection%2520under%2520special%2520scenarios%252C%2520advancing%2520its%250Aprogress.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TACR-YOLO%3A%20A%20Real-time%20Detection%20Framework%20for%20Abnormal%20Human%20Behaviors%0A%20%20Enhanced%20with%20Coordinate%20and%20Task-Aware%20Representations&entry.906535625=Xinyi%20Yin%20and%20Wenbo%20Yuan%20and%20Xuecheng%20Wu%20and%20Liangyu%20Fu%20and%20Danlei%20Huang&entry.1292438233=%20%20Abnormal%20Human%20Behavior%20Detection%20%28AHBD%29%20under%20special%20scenarios%20is%20becoming%0Aincreasingly%20crucial.%20While%20YOLO-based%20detection%20methods%20excel%20in%20real-time%0Atasks%2C%20they%20remain%20hindered%20by%20challenges%20including%20small%20objects%2C%20task%0Aconflicts%2C%20and%20multi-scale%20fusion%20in%20AHBD.%20To%20tackle%20them%2C%20we%20propose%0ATACR-YOLO%2C%20a%20new%20real-time%20framework%20for%20AHBD.%20We%20introduce%20a%20Coordinate%0AAttention%20Module%20to%20enhance%20small%20object%20detection%2C%20a%20Task-Aware%20Attention%0AModule%20to%20deal%20with%20classification-regression%20conflicts%2C%20and%20a%20Strengthen%20Neck%0ANetwork%20for%20refined%20multi-scale%20fusion%2C%20respectively.%20In%20addition%2C%20we%20optimize%0AAnchor%20Box%20sizes%20using%20K-means%20clustering%20and%20deploy%20DIoU-Loss%20to%20improve%0Abounding%20box%20regression.%20The%20Personnel%20Anomalous%20Behavior%20Detection%20%28PABD%29%0Adataset%2C%20which%20includes%208%2C529%20samples%20across%20four%20behavior%20categories%2C%20is%20also%0Apresented.%20Extensive%20experimental%20results%20indicate%20that%20TACR-YOLO%20achieves%0A91.92%25%20mAP%20on%20PABD%2C%20with%20competitive%20speed%20and%20robustness.%20Ablation%20studies%0Ahighlight%20the%20contribution%20of%20each%20improvement.%20This%20work%20provides%20new%20insights%0Afor%20abnormal%20behavior%20detection%20under%20special%20scenarios%2C%20advancing%20its%0Aprogress.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11478v1&entry.124074799=Read"},
{"title": "Casual3DHDR: High Dynamic Range 3D Gaussian Splatting from Casually\n  Captured Videos", "author": "Shucheng Gong and Lingzhe Zhao and Wenpu Li and Hong Xie and Yin Zhang and Shiyu Zhao and Peidong Liu", "abstract": "  Photo-realistic novel view synthesis from multi-view images, such as neural\nradiance field (NeRF) and 3D Gaussian Splatting (3DGS), has gained significant\nattention for its superior performance. However, most existing methods rely on\nlow dynamic range (LDR) images, limiting their ability to capture detailed\nscenes in high-contrast environments. While some prior works address high\ndynamic range (HDR) scene reconstruction, they typically require multi-view\nsharp images with varying exposure times captured at fixed camera positions,\nwhich is time-consuming and impractical. To make data acquisition more\nflexible, we propose \\textbf{Casual3DHDR}, a robust one-stage method that\nreconstructs 3D HDR scenes from casually-captured auto-exposure (AE) videos,\neven under severe motion blur and unknown, varying exposure times. Our approach\nintegrates a continuous camera trajectory into a unified physical imaging\nmodel, jointly optimizing exposure times, camera trajectory, and the camera\nresponse function (CRF). Extensive experiments on synthetic and real-world\ndatasets demonstrate that \\textbf{Casual3DHDR} outperforms existing methods in\nrobustness and rendering quality. Our source code and dataset will be available\nat https://lingzhezhao.github.io/CasualHDRSplat/\n", "link": "http://arxiv.org/abs/2504.17728v2", "date": "2025-08-15", "relevancy": 2.6468, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6899}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6456}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.64}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Casual3DHDR%3A%20High%20Dynamic%20Range%203D%20Gaussian%20Splatting%20from%20Casually%0A%20%20Captured%20Videos&body=Title%3A%20Casual3DHDR%3A%20High%20Dynamic%20Range%203D%20Gaussian%20Splatting%20from%20Casually%0A%20%20Captured%20Videos%0AAuthor%3A%20Shucheng%20Gong%20and%20Lingzhe%20Zhao%20and%20Wenpu%20Li%20and%20Hong%20Xie%20and%20Yin%20Zhang%20and%20Shiyu%20Zhao%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Photo-realistic%20novel%20view%20synthesis%20from%20multi-view%20images%2C%20such%20as%20neural%0Aradiance%20field%20%28NeRF%29%20and%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20has%20gained%20significant%0Aattention%20for%20its%20superior%20performance.%20However%2C%20most%20existing%20methods%20rely%20on%0Alow%20dynamic%20range%20%28LDR%29%20images%2C%20limiting%20their%20ability%20to%20capture%20detailed%0Ascenes%20in%20high-contrast%20environments.%20While%20some%20prior%20works%20address%20high%0Adynamic%20range%20%28HDR%29%20scene%20reconstruction%2C%20they%20typically%20require%20multi-view%0Asharp%20images%20with%20varying%20exposure%20times%20captured%20at%20fixed%20camera%20positions%2C%0Awhich%20is%20time-consuming%20and%20impractical.%20To%20make%20data%20acquisition%20more%0Aflexible%2C%20we%20propose%20%5Ctextbf%7BCasual3DHDR%7D%2C%20a%20robust%20one-stage%20method%20that%0Areconstructs%203D%20HDR%20scenes%20from%20casually-captured%20auto-exposure%20%28AE%29%20videos%2C%0Aeven%20under%20severe%20motion%20blur%20and%20unknown%2C%20varying%20exposure%20times.%20Our%20approach%0Aintegrates%20a%20continuous%20camera%20trajectory%20into%20a%20unified%20physical%20imaging%0Amodel%2C%20jointly%20optimizing%20exposure%20times%2C%20camera%20trajectory%2C%20and%20the%20camera%0Aresponse%20function%20%28CRF%29.%20Extensive%20experiments%20on%20synthetic%20and%20real-world%0Adatasets%20demonstrate%20that%20%5Ctextbf%7BCasual3DHDR%7D%20outperforms%20existing%20methods%20in%0Arobustness%20and%20rendering%20quality.%20Our%20source%20code%20and%20dataset%20will%20be%20available%0Aat%20https%3A//lingzhezhao.github.io/CasualHDRSplat/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17728v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCasual3DHDR%253A%2520High%2520Dynamic%2520Range%25203D%2520Gaussian%2520Splatting%2520from%2520Casually%250A%2520%2520Captured%2520Videos%26entry.906535625%3DShucheng%2520Gong%2520and%2520Lingzhe%2520Zhao%2520and%2520Wenpu%2520Li%2520and%2520Hong%2520Xie%2520and%2520Yin%2520Zhang%2520and%2520Shiyu%2520Zhao%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Photo-realistic%2520novel%2520view%2520synthesis%2520from%2520multi-view%2520images%252C%2520such%2520as%2520neural%250Aradiance%2520field%2520%2528NeRF%2529%2520and%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520has%2520gained%2520significant%250Aattention%2520for%2520its%2520superior%2520performance.%2520However%252C%2520most%2520existing%2520methods%2520rely%2520on%250Alow%2520dynamic%2520range%2520%2528LDR%2529%2520images%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520detailed%250Ascenes%2520in%2520high-contrast%2520environments.%2520While%2520some%2520prior%2520works%2520address%2520high%250Adynamic%2520range%2520%2528HDR%2529%2520scene%2520reconstruction%252C%2520they%2520typically%2520require%2520multi-view%250Asharp%2520images%2520with%2520varying%2520exposure%2520times%2520captured%2520at%2520fixed%2520camera%2520positions%252C%250Awhich%2520is%2520time-consuming%2520and%2520impractical.%2520To%2520make%2520data%2520acquisition%2520more%250Aflexible%252C%2520we%2520propose%2520%255Ctextbf%257BCasual3DHDR%257D%252C%2520a%2520robust%2520one-stage%2520method%2520that%250Areconstructs%25203D%2520HDR%2520scenes%2520from%2520casually-captured%2520auto-exposure%2520%2528AE%2529%2520videos%252C%250Aeven%2520under%2520severe%2520motion%2520blur%2520and%2520unknown%252C%2520varying%2520exposure%2520times.%2520Our%2520approach%250Aintegrates%2520a%2520continuous%2520camera%2520trajectory%2520into%2520a%2520unified%2520physical%2520imaging%250Amodel%252C%2520jointly%2520optimizing%2520exposure%2520times%252C%2520camera%2520trajectory%252C%2520and%2520the%2520camera%250Aresponse%2520function%2520%2528CRF%2529.%2520Extensive%2520experiments%2520on%2520synthetic%2520and%2520real-world%250Adatasets%2520demonstrate%2520that%2520%255Ctextbf%257BCasual3DHDR%257D%2520outperforms%2520existing%2520methods%2520in%250Arobustness%2520and%2520rendering%2520quality.%2520Our%2520source%2520code%2520and%2520dataset%2520will%2520be%2520available%250Aat%2520https%253A//lingzhezhao.github.io/CasualHDRSplat/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17728v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Casual3DHDR%3A%20High%20Dynamic%20Range%203D%20Gaussian%20Splatting%20from%20Casually%0A%20%20Captured%20Videos&entry.906535625=Shucheng%20Gong%20and%20Lingzhe%20Zhao%20and%20Wenpu%20Li%20and%20Hong%20Xie%20and%20Yin%20Zhang%20and%20Shiyu%20Zhao%20and%20Peidong%20Liu&entry.1292438233=%20%20Photo-realistic%20novel%20view%20synthesis%20from%20multi-view%20images%2C%20such%20as%20neural%0Aradiance%20field%20%28NeRF%29%20and%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20has%20gained%20significant%0Aattention%20for%20its%20superior%20performance.%20However%2C%20most%20existing%20methods%20rely%20on%0Alow%20dynamic%20range%20%28LDR%29%20images%2C%20limiting%20their%20ability%20to%20capture%20detailed%0Ascenes%20in%20high-contrast%20environments.%20While%20some%20prior%20works%20address%20high%0Adynamic%20range%20%28HDR%29%20scene%20reconstruction%2C%20they%20typically%20require%20multi-view%0Asharp%20images%20with%20varying%20exposure%20times%20captured%20at%20fixed%20camera%20positions%2C%0Awhich%20is%20time-consuming%20and%20impractical.%20To%20make%20data%20acquisition%20more%0Aflexible%2C%20we%20propose%20%5Ctextbf%7BCasual3DHDR%7D%2C%20a%20robust%20one-stage%20method%20that%0Areconstructs%203D%20HDR%20scenes%20from%20casually-captured%20auto-exposure%20%28AE%29%20videos%2C%0Aeven%20under%20severe%20motion%20blur%20and%20unknown%2C%20varying%20exposure%20times.%20Our%20approach%0Aintegrates%20a%20continuous%20camera%20trajectory%20into%20a%20unified%20physical%20imaging%0Amodel%2C%20jointly%20optimizing%20exposure%20times%2C%20camera%20trajectory%2C%20and%20the%20camera%0Aresponse%20function%20%28CRF%29.%20Extensive%20experiments%20on%20synthetic%20and%20real-world%0Adatasets%20demonstrate%20that%20%5Ctextbf%7BCasual3DHDR%7D%20outperforms%20existing%20methods%20in%0Arobustness%20and%20rendering%20quality.%20Our%20source%20code%20and%20dataset%20will%20be%20available%0Aat%20https%3A//lingzhezhao.github.io/CasualHDRSplat/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17728v2&entry.124074799=Read"},
{"title": "Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of\n  LLMs", "author": "Shangpin Peng and Weinong Wang and Zhuotao Tian and Senqiao Yang and Xing Wu and Haotian Xu and Chengquan Zhang and Takashi Isobe and Baotian Hu and Min Zhang", "abstract": "  Direct Preference Optimization (DPO) has become a cornerstone of\nreinforcement learning from human feedback (RLHF) due to its simplicity and\nefficiency. However, existing DPO-based approaches typically treat all\npreference pairs uniformly, ignoring critical variations in their inherent\nquality and learning utility, leading to suboptimal data utilization and\nperformance. To address this challenge, we propose Omni-DPO, a dual-perspective\noptimization framework that jointly accounts for (1) the inherent quality of\neach preference pair and (2) the model's evolving performance on those pairs.\nBy adaptively weighting samples according to both data quality and the model's\nlearning dynamics during training, Omni-DPO enables more effective training\ndata utilization and achieves better performance. Experimental results on\nvarious models and benchmarks demonstrate the superiority and generalization\ncapabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it\nfinetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant\nmargin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning\ntasks, Omni-DPO consistently outperforms the baseline methods across all\nbenchmarks, providing strong empirical evidence for the effectiveness and\nrobustness of our approach. Code and models will be available at\nhttps://github.com/pspdada/Omni-DPO.\n", "link": "http://arxiv.org/abs/2506.10054v2", "date": "2025-08-15", "relevancy": 2.6183, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5273}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5273}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-DPO%3A%20A%20Dual-Perspective%20Paradigm%20for%20Dynamic%20Preference%20Learning%20of%0A%20%20LLMs&body=Title%3A%20Omni-DPO%3A%20A%20Dual-Perspective%20Paradigm%20for%20Dynamic%20Preference%20Learning%20of%0A%20%20LLMs%0AAuthor%3A%20Shangpin%20Peng%20and%20Weinong%20Wang%20and%20Zhuotao%20Tian%20and%20Senqiao%20Yang%20and%20Xing%20Wu%20and%20Haotian%20Xu%20and%20Chengquan%20Zhang%20and%20Takashi%20Isobe%20and%20Baotian%20Hu%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20become%20a%20cornerstone%20of%0Areinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20due%20to%20its%20simplicity%20and%0Aefficiency.%20However%2C%20existing%20DPO-based%20approaches%20typically%20treat%20all%0Apreference%20pairs%20uniformly%2C%20ignoring%20critical%20variations%20in%20their%20inherent%0Aquality%20and%20learning%20utility%2C%20leading%20to%20suboptimal%20data%20utilization%20and%0Aperformance.%20To%20address%20this%20challenge%2C%20we%20propose%20Omni-DPO%2C%20a%20dual-perspective%0Aoptimization%20framework%20that%20jointly%20accounts%20for%20%281%29%20the%20inherent%20quality%20of%0Aeach%20preference%20pair%20and%20%282%29%20the%20model%27s%20evolving%20performance%20on%20those%20pairs.%0ABy%20adaptively%20weighting%20samples%20according%20to%20both%20data%20quality%20and%20the%20model%27s%0Alearning%20dynamics%20during%20training%2C%20Omni-DPO%20enables%20more%20effective%20training%0Adata%20utilization%20and%20achieves%20better%20performance.%20Experimental%20results%20on%0Avarious%20models%20and%20benchmarks%20demonstrate%20the%20superiority%20and%20generalization%0Acapabilities%20of%20Omni-DPO.%20On%20textual%20understanding%20tasks%2C%20Gemma-2-9b-it%0Afinetuned%20with%20Omni-DPO%20beats%20the%20leading%20LLM%2C%20Claude%203%20Opus%2C%20by%20a%20significant%0Amargin%20of%206.7%20points%20on%20the%20Arena-Hard%20benchmark.%20On%20mathematical%20reasoning%0Atasks%2C%20Omni-DPO%20consistently%20outperforms%20the%20baseline%20methods%20across%20all%0Abenchmarks%2C%20providing%20strong%20empirical%20evidence%20for%20the%20effectiveness%20and%0Arobustness%20of%20our%20approach.%20Code%20and%20models%20will%20be%20available%20at%0Ahttps%3A//github.com/pspdada/Omni-DPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10054v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-DPO%253A%2520A%2520Dual-Perspective%2520Paradigm%2520for%2520Dynamic%2520Preference%2520Learning%2520of%250A%2520%2520LLMs%26entry.906535625%3DShangpin%2520Peng%2520and%2520Weinong%2520Wang%2520and%2520Zhuotao%2520Tian%2520and%2520Senqiao%2520Yang%2520and%2520Xing%2520Wu%2520and%2520Haotian%2520Xu%2520and%2520Chengquan%2520Zhang%2520and%2520Takashi%2520Isobe%2520and%2520Baotian%2520Hu%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520has%2520become%2520a%2520cornerstone%2520of%250Areinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529%2520due%2520to%2520its%2520simplicity%2520and%250Aefficiency.%2520However%252C%2520existing%2520DPO-based%2520approaches%2520typically%2520treat%2520all%250Apreference%2520pairs%2520uniformly%252C%2520ignoring%2520critical%2520variations%2520in%2520their%2520inherent%250Aquality%2520and%2520learning%2520utility%252C%2520leading%2520to%2520suboptimal%2520data%2520utilization%2520and%250Aperformance.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520Omni-DPO%252C%2520a%2520dual-perspective%250Aoptimization%2520framework%2520that%2520jointly%2520accounts%2520for%2520%25281%2529%2520the%2520inherent%2520quality%2520of%250Aeach%2520preference%2520pair%2520and%2520%25282%2529%2520the%2520model%2527s%2520evolving%2520performance%2520on%2520those%2520pairs.%250ABy%2520adaptively%2520weighting%2520samples%2520according%2520to%2520both%2520data%2520quality%2520and%2520the%2520model%2527s%250Alearning%2520dynamics%2520during%2520training%252C%2520Omni-DPO%2520enables%2520more%2520effective%2520training%250Adata%2520utilization%2520and%2520achieves%2520better%2520performance.%2520Experimental%2520results%2520on%250Avarious%2520models%2520and%2520benchmarks%2520demonstrate%2520the%2520superiority%2520and%2520generalization%250Acapabilities%2520of%2520Omni-DPO.%2520On%2520textual%2520understanding%2520tasks%252C%2520Gemma-2-9b-it%250Afinetuned%2520with%2520Omni-DPO%2520beats%2520the%2520leading%2520LLM%252C%2520Claude%25203%2520Opus%252C%2520by%2520a%2520significant%250Amargin%2520of%25206.7%2520points%2520on%2520the%2520Arena-Hard%2520benchmark.%2520On%2520mathematical%2520reasoning%250Atasks%252C%2520Omni-DPO%2520consistently%2520outperforms%2520the%2520baseline%2520methods%2520across%2520all%250Abenchmarks%252C%2520providing%2520strong%2520empirical%2520evidence%2520for%2520the%2520effectiveness%2520and%250Arobustness%2520of%2520our%2520approach.%2520Code%2520and%2520models%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/pspdada/Omni-DPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10054v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-DPO%3A%20A%20Dual-Perspective%20Paradigm%20for%20Dynamic%20Preference%20Learning%20of%0A%20%20LLMs&entry.906535625=Shangpin%20Peng%20and%20Weinong%20Wang%20and%20Zhuotao%20Tian%20and%20Senqiao%20Yang%20and%20Xing%20Wu%20and%20Haotian%20Xu%20and%20Chengquan%20Zhang%20and%20Takashi%20Isobe%20and%20Baotian%20Hu%20and%20Min%20Zhang&entry.1292438233=%20%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20become%20a%20cornerstone%20of%0Areinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20due%20to%20its%20simplicity%20and%0Aefficiency.%20However%2C%20existing%20DPO-based%20approaches%20typically%20treat%20all%0Apreference%20pairs%20uniformly%2C%20ignoring%20critical%20variations%20in%20their%20inherent%0Aquality%20and%20learning%20utility%2C%20leading%20to%20suboptimal%20data%20utilization%20and%0Aperformance.%20To%20address%20this%20challenge%2C%20we%20propose%20Omni-DPO%2C%20a%20dual-perspective%0Aoptimization%20framework%20that%20jointly%20accounts%20for%20%281%29%20the%20inherent%20quality%20of%0Aeach%20preference%20pair%20and%20%282%29%20the%20model%27s%20evolving%20performance%20on%20those%20pairs.%0ABy%20adaptively%20weighting%20samples%20according%20to%20both%20data%20quality%20and%20the%20model%27s%0Alearning%20dynamics%20during%20training%2C%20Omni-DPO%20enables%20more%20effective%20training%0Adata%20utilization%20and%20achieves%20better%20performance.%20Experimental%20results%20on%0Avarious%20models%20and%20benchmarks%20demonstrate%20the%20superiority%20and%20generalization%0Acapabilities%20of%20Omni-DPO.%20On%20textual%20understanding%20tasks%2C%20Gemma-2-9b-it%0Afinetuned%20with%20Omni-DPO%20beats%20the%20leading%20LLM%2C%20Claude%203%20Opus%2C%20by%20a%20significant%0Amargin%20of%206.7%20points%20on%20the%20Arena-Hard%20benchmark.%20On%20mathematical%20reasoning%0Atasks%2C%20Omni-DPO%20consistently%20outperforms%20the%20baseline%20methods%20across%20all%0Abenchmarks%2C%20providing%20strong%20empirical%20evidence%20for%20the%20effectiveness%20and%0Arobustness%20of%20our%20approach.%20Code%20and%20models%20will%20be%20available%20at%0Ahttps%3A//github.com/pspdada/Omni-DPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10054v2&entry.124074799=Read"},
{"title": "Lightweight Attribute Localizing Models for Pedestrian Attribute\n  Recognition", "author": "Ashish Jha and Dimitrii Ermilov and Konstantin Sobolev and Anh Huy Phan and Salman Ahmadi-Asl and Naveed Ahmed and Imran Junejo and Zaher AL Aghbari and Thar Baker and Ahmed Mohamed Khedr and Andrzej Cichocki", "abstract": "  Pedestrian Attribute Recognition (PAR) focuses on identifying various\nattributes in pedestrian images, with key applications in person retrieval,\nsuspect re-identification, and soft biometrics. However, Deep Neural Networks\n(DNNs) for PAR often suffer from over-parameterization and high computational\ncomplexity, making them unsuitable for resource-constrained devices.\nTraditional tensor-based compression methods typically factorize layers without\nadequately preserving the gradient direction during compression, leading to\ninefficient compression and a significant accuracy loss. In this work, we\npropose a novel approach for determining the optimal ranks of low-rank layers,\nensuring that the gradient direction of the compressed model closely aligns\nwith that of the original model. This means that the compressed model\neffectively preserves the update direction of the full model, enabling more\nefficient compression for PAR tasks. The proposed procedure optimizes the\ncompression ranks for each layer within the ALM model, followed by compression\nusing CPD-EPC or truncated SVD. This results in a reduction in model complexity\nwhile maintaining high performance.\n", "link": "http://arxiv.org/abs/2306.09822v2", "date": "2025-08-15", "relevancy": 2.597, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5255}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5242}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Attribute%20Localizing%20Models%20for%20Pedestrian%20Attribute%0A%20%20Recognition&body=Title%3A%20Lightweight%20Attribute%20Localizing%20Models%20for%20Pedestrian%20Attribute%0A%20%20Recognition%0AAuthor%3A%20Ashish%20Jha%20and%20Dimitrii%20Ermilov%20and%20Konstantin%20Sobolev%20and%20Anh%20Huy%20Phan%20and%20Salman%20Ahmadi-Asl%20and%20Naveed%20Ahmed%20and%20Imran%20Junejo%20and%20Zaher%20AL%20Aghbari%20and%20Thar%20Baker%20and%20Ahmed%20Mohamed%20Khedr%20and%20Andrzej%20Cichocki%0AAbstract%3A%20%20%20Pedestrian%20Attribute%20Recognition%20%28PAR%29%20focuses%20on%20identifying%20various%0Aattributes%20in%20pedestrian%20images%2C%20with%20key%20applications%20in%20person%20retrieval%2C%0Asuspect%20re-identification%2C%20and%20soft%20biometrics.%20However%2C%20Deep%20Neural%20Networks%0A%28DNNs%29%20for%20PAR%20often%20suffer%20from%20over-parameterization%20and%20high%20computational%0Acomplexity%2C%20making%20them%20unsuitable%20for%20resource-constrained%20devices.%0ATraditional%20tensor-based%20compression%20methods%20typically%20factorize%20layers%20without%0Aadequately%20preserving%20the%20gradient%20direction%20during%20compression%2C%20leading%20to%0Ainefficient%20compression%20and%20a%20significant%20accuracy%20loss.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20approach%20for%20determining%20the%20optimal%20ranks%20of%20low-rank%20layers%2C%0Aensuring%20that%20the%20gradient%20direction%20of%20the%20compressed%20model%20closely%20aligns%0Awith%20that%20of%20the%20original%20model.%20This%20means%20that%20the%20compressed%20model%0Aeffectively%20preserves%20the%20update%20direction%20of%20the%20full%20model%2C%20enabling%20more%0Aefficient%20compression%20for%20PAR%20tasks.%20The%20proposed%20procedure%20optimizes%20the%0Acompression%20ranks%20for%20each%20layer%20within%20the%20ALM%20model%2C%20followed%20by%20compression%0Ausing%20CPD-EPC%20or%20truncated%20SVD.%20This%20results%20in%20a%20reduction%20in%20model%20complexity%0Awhile%20maintaining%20high%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.09822v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Attribute%2520Localizing%2520Models%2520for%2520Pedestrian%2520Attribute%250A%2520%2520Recognition%26entry.906535625%3DAshish%2520Jha%2520and%2520Dimitrii%2520Ermilov%2520and%2520Konstantin%2520Sobolev%2520and%2520Anh%2520Huy%2520Phan%2520and%2520Salman%2520Ahmadi-Asl%2520and%2520Naveed%2520Ahmed%2520and%2520Imran%2520Junejo%2520and%2520Zaher%2520AL%2520Aghbari%2520and%2520Thar%2520Baker%2520and%2520Ahmed%2520Mohamed%2520Khedr%2520and%2520Andrzej%2520Cichocki%26entry.1292438233%3D%2520%2520Pedestrian%2520Attribute%2520Recognition%2520%2528PAR%2529%2520focuses%2520on%2520identifying%2520various%250Aattributes%2520in%2520pedestrian%2520images%252C%2520with%2520key%2520applications%2520in%2520person%2520retrieval%252C%250Asuspect%2520re-identification%252C%2520and%2520soft%2520biometrics.%2520However%252C%2520Deep%2520Neural%2520Networks%250A%2528DNNs%2529%2520for%2520PAR%2520often%2520suffer%2520from%2520over-parameterization%2520and%2520high%2520computational%250Acomplexity%252C%2520making%2520them%2520unsuitable%2520for%2520resource-constrained%2520devices.%250ATraditional%2520tensor-based%2520compression%2520methods%2520typically%2520factorize%2520layers%2520without%250Aadequately%2520preserving%2520the%2520gradient%2520direction%2520during%2520compression%252C%2520leading%2520to%250Ainefficient%2520compression%2520and%2520a%2520significant%2520accuracy%2520loss.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520approach%2520for%2520determining%2520the%2520optimal%2520ranks%2520of%2520low-rank%2520layers%252C%250Aensuring%2520that%2520the%2520gradient%2520direction%2520of%2520the%2520compressed%2520model%2520closely%2520aligns%250Awith%2520that%2520of%2520the%2520original%2520model.%2520This%2520means%2520that%2520the%2520compressed%2520model%250Aeffectively%2520preserves%2520the%2520update%2520direction%2520of%2520the%2520full%2520model%252C%2520enabling%2520more%250Aefficient%2520compression%2520for%2520PAR%2520tasks.%2520The%2520proposed%2520procedure%2520optimizes%2520the%250Acompression%2520ranks%2520for%2520each%2520layer%2520within%2520the%2520ALM%2520model%252C%2520followed%2520by%2520compression%250Ausing%2520CPD-EPC%2520or%2520truncated%2520SVD.%2520This%2520results%2520in%2520a%2520reduction%2520in%2520model%2520complexity%250Awhile%2520maintaining%2520high%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.09822v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Attribute%20Localizing%20Models%20for%20Pedestrian%20Attribute%0A%20%20Recognition&entry.906535625=Ashish%20Jha%20and%20Dimitrii%20Ermilov%20and%20Konstantin%20Sobolev%20and%20Anh%20Huy%20Phan%20and%20Salman%20Ahmadi-Asl%20and%20Naveed%20Ahmed%20and%20Imran%20Junejo%20and%20Zaher%20AL%20Aghbari%20and%20Thar%20Baker%20and%20Ahmed%20Mohamed%20Khedr%20and%20Andrzej%20Cichocki&entry.1292438233=%20%20Pedestrian%20Attribute%20Recognition%20%28PAR%29%20focuses%20on%20identifying%20various%0Aattributes%20in%20pedestrian%20images%2C%20with%20key%20applications%20in%20person%20retrieval%2C%0Asuspect%20re-identification%2C%20and%20soft%20biometrics.%20However%2C%20Deep%20Neural%20Networks%0A%28DNNs%29%20for%20PAR%20often%20suffer%20from%20over-parameterization%20and%20high%20computational%0Acomplexity%2C%20making%20them%20unsuitable%20for%20resource-constrained%20devices.%0ATraditional%20tensor-based%20compression%20methods%20typically%20factorize%20layers%20without%0Aadequately%20preserving%20the%20gradient%20direction%20during%20compression%2C%20leading%20to%0Ainefficient%20compression%20and%20a%20significant%20accuracy%20loss.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20approach%20for%20determining%20the%20optimal%20ranks%20of%20low-rank%20layers%2C%0Aensuring%20that%20the%20gradient%20direction%20of%20the%20compressed%20model%20closely%20aligns%0Awith%20that%20of%20the%20original%20model.%20This%20means%20that%20the%20compressed%20model%0Aeffectively%20preserves%20the%20update%20direction%20of%20the%20full%20model%2C%20enabling%20more%0Aefficient%20compression%20for%20PAR%20tasks.%20The%20proposed%20procedure%20optimizes%20the%0Acompression%20ranks%20for%20each%20layer%20within%20the%20ALM%20model%2C%20followed%20by%20compression%0Ausing%20CPD-EPC%20or%20truncated%20SVD.%20This%20results%20in%20a%20reduction%20in%20model%20complexity%0Awhile%20maintaining%20high%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.09822v2&entry.124074799=Read"},
{"title": "Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image\n  Analysis", "author": "Yu Xin and Gorkem Can Ates and Kuang Gong and Wei Shao", "abstract": "  Vision-language models (VLMs) have shown promise in 2D medical image\nanalysis, but extending them to 3D remains challenging due to the high\ncomputational demands of volumetric data and the difficulty of aligning 3D\nspatial features with clinical text. We present Med3DVLM, a 3D VLM designed to\naddress these challenges through three key innovations: (1) DCFormer, an\nefficient encoder that uses decomposed 3D convolutions to capture fine-grained\nspatial features at scale; (2) SigLIP, a contrastive learning strategy with\npairwise sigmoid loss that improves image-text alignment without relying on\nlarge negative batches; and (3) a dual-stream MLP-Mixer projector that fuses\nlow- and high-level image features with text embeddings for richer multi-modal\nrepresentations. We evaluate our model on the M3D dataset, which includes\nradiology reports and VQA data for 120,084 3D medical images. Results show that\nMed3DVLM achieves superior performance across multiple benchmarks. For\nimage-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantly\noutperforming the current state-of-the-art M3D model (19.10%). For report\ngeneration, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-ended\nvisual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and in\nclosed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These results\nhighlight Med3DVLM's ability to bridge the gap between 3D imaging and language,\nenabling scalable, multi-task reasoning across clinical applications. Our code\nis publicly available at https://github.com/mirthAI/Med3DVLM.\n", "link": "http://arxiv.org/abs/2503.20047v2", "date": "2025-08-15", "relevancy": 2.5951, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6531}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Med3DVLM%3A%20An%20Efficient%20Vision-Language%20Model%20for%203D%20Medical%20Image%0A%20%20Analysis&body=Title%3A%20Med3DVLM%3A%20An%20Efficient%20Vision-Language%20Model%20for%203D%20Medical%20Image%0A%20%20Analysis%0AAuthor%3A%20Yu%20Xin%20and%20Gorkem%20Can%20Ates%20and%20Kuang%20Gong%20and%20Wei%20Shao%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20shown%20promise%20in%202D%20medical%20image%0Aanalysis%2C%20but%20extending%20them%20to%203D%20remains%20challenging%20due%20to%20the%20high%0Acomputational%20demands%20of%20volumetric%20data%20and%20the%20difficulty%20of%20aligning%203D%0Aspatial%20features%20with%20clinical%20text.%20We%20present%20Med3DVLM%2C%20a%203D%20VLM%20designed%20to%0Aaddress%20these%20challenges%20through%20three%20key%20innovations%3A%20%281%29%20DCFormer%2C%20an%0Aefficient%20encoder%20that%20uses%20decomposed%203D%20convolutions%20to%20capture%20fine-grained%0Aspatial%20features%20at%20scale%3B%20%282%29%20SigLIP%2C%20a%20contrastive%20learning%20strategy%20with%0Apairwise%20sigmoid%20loss%20that%20improves%20image-text%20alignment%20without%20relying%20on%0Alarge%20negative%20batches%3B%20and%20%283%29%20a%20dual-stream%20MLP-Mixer%20projector%20that%20fuses%0Alow-%20and%20high-level%20image%20features%20with%20text%20embeddings%20for%20richer%20multi-modal%0Arepresentations.%20We%20evaluate%20our%20model%20on%20the%20M3D%20dataset%2C%20which%20includes%0Aradiology%20reports%20and%20VQA%20data%20for%20120%2C084%203D%20medical%20images.%20Results%20show%20that%0AMed3DVLM%20achieves%20superior%20performance%20across%20multiple%20benchmarks.%20For%0Aimage-text%20retrieval%2C%20it%20reaches%2061.00%25%20R%401%20on%202%2C000%20samples%2C%20significantly%0Aoutperforming%20the%20current%20state-of-the-art%20M3D%20model%20%2819.10%25%29.%20For%20report%0Ageneration%2C%20it%20achieves%20a%20METEOR%20score%20of%2036.42%25%20%28vs.%2014.38%25%29.%20In%20open-ended%0Avisual%20question%20answering%20%28VQA%29%2C%20it%20scores%2036.76%25%20METEOR%20%28vs.%2033.58%25%29%2C%20and%20in%0Aclosed-ended%20VQA%2C%20it%20achieves%2079.95%25%20accuracy%20%28vs.%2075.78%25%29.%20These%20results%0Ahighlight%20Med3DVLM%27s%20ability%20to%20bridge%20the%20gap%20between%203D%20imaging%20and%20language%2C%0Aenabling%20scalable%2C%20multi-task%20reasoning%20across%20clinical%20applications.%20Our%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/mirthAI/Med3DVLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.20047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMed3DVLM%253A%2520An%2520Efficient%2520Vision-Language%2520Model%2520for%25203D%2520Medical%2520Image%250A%2520%2520Analysis%26entry.906535625%3DYu%2520Xin%2520and%2520Gorkem%2520Can%2520Ates%2520and%2520Kuang%2520Gong%2520and%2520Wei%2520Shao%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520promise%2520in%25202D%2520medical%2520image%250Aanalysis%252C%2520but%2520extending%2520them%2520to%25203D%2520remains%2520challenging%2520due%2520to%2520the%2520high%250Acomputational%2520demands%2520of%2520volumetric%2520data%2520and%2520the%2520difficulty%2520of%2520aligning%25203D%250Aspatial%2520features%2520with%2520clinical%2520text.%2520We%2520present%2520Med3DVLM%252C%2520a%25203D%2520VLM%2520designed%2520to%250Aaddress%2520these%2520challenges%2520through%2520three%2520key%2520innovations%253A%2520%25281%2529%2520DCFormer%252C%2520an%250Aefficient%2520encoder%2520that%2520uses%2520decomposed%25203D%2520convolutions%2520to%2520capture%2520fine-grained%250Aspatial%2520features%2520at%2520scale%253B%2520%25282%2529%2520SigLIP%252C%2520a%2520contrastive%2520learning%2520strategy%2520with%250Apairwise%2520sigmoid%2520loss%2520that%2520improves%2520image-text%2520alignment%2520without%2520relying%2520on%250Alarge%2520negative%2520batches%253B%2520and%2520%25283%2529%2520a%2520dual-stream%2520MLP-Mixer%2520projector%2520that%2520fuses%250Alow-%2520and%2520high-level%2520image%2520features%2520with%2520text%2520embeddings%2520for%2520richer%2520multi-modal%250Arepresentations.%2520We%2520evaluate%2520our%2520model%2520on%2520the%2520M3D%2520dataset%252C%2520which%2520includes%250Aradiology%2520reports%2520and%2520VQA%2520data%2520for%2520120%252C084%25203D%2520medical%2520images.%2520Results%2520show%2520that%250AMed3DVLM%2520achieves%2520superior%2520performance%2520across%2520multiple%2520benchmarks.%2520For%250Aimage-text%2520retrieval%252C%2520it%2520reaches%252061.00%2525%2520R%25401%2520on%25202%252C000%2520samples%252C%2520significantly%250Aoutperforming%2520the%2520current%2520state-of-the-art%2520M3D%2520model%2520%252819.10%2525%2529.%2520For%2520report%250Ageneration%252C%2520it%2520achieves%2520a%2520METEOR%2520score%2520of%252036.42%2525%2520%2528vs.%252014.38%2525%2529.%2520In%2520open-ended%250Avisual%2520question%2520answering%2520%2528VQA%2529%252C%2520it%2520scores%252036.76%2525%2520METEOR%2520%2528vs.%252033.58%2525%2529%252C%2520and%2520in%250Aclosed-ended%2520VQA%252C%2520it%2520achieves%252079.95%2525%2520accuracy%2520%2528vs.%252075.78%2525%2529.%2520These%2520results%250Ahighlight%2520Med3DVLM%2527s%2520ability%2520to%2520bridge%2520the%2520gap%2520between%25203D%2520imaging%2520and%2520language%252C%250Aenabling%2520scalable%252C%2520multi-task%2520reasoning%2520across%2520clinical%2520applications.%2520Our%2520code%250Ais%2520publicly%2520available%2520at%2520https%253A//github.com/mirthAI/Med3DVLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Med3DVLM%3A%20An%20Efficient%20Vision-Language%20Model%20for%203D%20Medical%20Image%0A%20%20Analysis&entry.906535625=Yu%20Xin%20and%20Gorkem%20Can%20Ates%20and%20Kuang%20Gong%20and%20Wei%20Shao&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20shown%20promise%20in%202D%20medical%20image%0Aanalysis%2C%20but%20extending%20them%20to%203D%20remains%20challenging%20due%20to%20the%20high%0Acomputational%20demands%20of%20volumetric%20data%20and%20the%20difficulty%20of%20aligning%203D%0Aspatial%20features%20with%20clinical%20text.%20We%20present%20Med3DVLM%2C%20a%203D%20VLM%20designed%20to%0Aaddress%20these%20challenges%20through%20three%20key%20innovations%3A%20%281%29%20DCFormer%2C%20an%0Aefficient%20encoder%20that%20uses%20decomposed%203D%20convolutions%20to%20capture%20fine-grained%0Aspatial%20features%20at%20scale%3B%20%282%29%20SigLIP%2C%20a%20contrastive%20learning%20strategy%20with%0Apairwise%20sigmoid%20loss%20that%20improves%20image-text%20alignment%20without%20relying%20on%0Alarge%20negative%20batches%3B%20and%20%283%29%20a%20dual-stream%20MLP-Mixer%20projector%20that%20fuses%0Alow-%20and%20high-level%20image%20features%20with%20text%20embeddings%20for%20richer%20multi-modal%0Arepresentations.%20We%20evaluate%20our%20model%20on%20the%20M3D%20dataset%2C%20which%20includes%0Aradiology%20reports%20and%20VQA%20data%20for%20120%2C084%203D%20medical%20images.%20Results%20show%20that%0AMed3DVLM%20achieves%20superior%20performance%20across%20multiple%20benchmarks.%20For%0Aimage-text%20retrieval%2C%20it%20reaches%2061.00%25%20R%401%20on%202%2C000%20samples%2C%20significantly%0Aoutperforming%20the%20current%20state-of-the-art%20M3D%20model%20%2819.10%25%29.%20For%20report%0Ageneration%2C%20it%20achieves%20a%20METEOR%20score%20of%2036.42%25%20%28vs.%2014.38%25%29.%20In%20open-ended%0Avisual%20question%20answering%20%28VQA%29%2C%20it%20scores%2036.76%25%20METEOR%20%28vs.%2033.58%25%29%2C%20and%20in%0Aclosed-ended%20VQA%2C%20it%20achieves%2079.95%25%20accuracy%20%28vs.%2075.78%25%29.%20These%20results%0Ahighlight%20Med3DVLM%27s%20ability%20to%20bridge%20the%20gap%20between%203D%20imaging%20and%20language%2C%0Aenabling%20scalable%2C%20multi-task%20reasoning%20across%20clinical%20applications.%20Our%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/mirthAI/Med3DVLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.20047v2&entry.124074799=Read"},
{"title": "MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation\n  Tutor with LLMs", "author": "Xiaoxue Gao and Huayun Zhang and Nancy F. Chen", "abstract": "  Generative speech models have demonstrated significant potential in\npersonalizing teacher-student interactions, offering valuable real-world\napplications for language learning in children's education. However, achieving\nhigh-quality, child-friendly speech generation remains challenging,\nparticularly for low-resource languages across diverse languages and cultural\ncontexts. In this paper, we propose MultiAiTutor, an educational multilingual\ngenerative AI tutor with child-friendly designs, leveraging LLM architecture\nfor speech generation tailored for educational purposes. We propose to\nintegrate age-appropriate multilingual speech generation using LLM\narchitectures, facilitating young children's language learning through\nculturally relevant image-description tasks in three low-resource languages:\nSingaporean-accent Mandarin, Malay, and Tamil. Experimental results from both\nobjective metrics and subjective evaluations demonstrate the superior\nperformance of the proposed MultiAiTutor compared to baseline methods.\n", "link": "http://arxiv.org/abs/2508.08715v2", "date": "2025-08-15", "relevancy": 2.5931, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5345}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5327}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiAiTutor%3A%20Child-Friendly%20Educational%20Multilingual%20Speech%20Generation%0A%20%20Tutor%20with%20LLMs&body=Title%3A%20MultiAiTutor%3A%20Child-Friendly%20Educational%20Multilingual%20Speech%20Generation%0A%20%20Tutor%20with%20LLMs%0AAuthor%3A%20Xiaoxue%20Gao%20and%20Huayun%20Zhang%20and%20Nancy%20F.%20Chen%0AAbstract%3A%20%20%20Generative%20speech%20models%20have%20demonstrated%20significant%20potential%20in%0Apersonalizing%20teacher-student%20interactions%2C%20offering%20valuable%20real-world%0Aapplications%20for%20language%20learning%20in%20children%27s%20education.%20However%2C%20achieving%0Ahigh-quality%2C%20child-friendly%20speech%20generation%20remains%20challenging%2C%0Aparticularly%20for%20low-resource%20languages%20across%20diverse%20languages%20and%20cultural%0Acontexts.%20In%20this%20paper%2C%20we%20propose%20MultiAiTutor%2C%20an%20educational%20multilingual%0Agenerative%20AI%20tutor%20with%20child-friendly%20designs%2C%20leveraging%20LLM%20architecture%0Afor%20speech%20generation%20tailored%20for%20educational%20purposes.%20We%20propose%20to%0Aintegrate%20age-appropriate%20multilingual%20speech%20generation%20using%20LLM%0Aarchitectures%2C%20facilitating%20young%20children%27s%20language%20learning%20through%0Aculturally%20relevant%20image-description%20tasks%20in%20three%20low-resource%20languages%3A%0ASingaporean-accent%20Mandarin%2C%20Malay%2C%20and%20Tamil.%20Experimental%20results%20from%20both%0Aobjective%20metrics%20and%20subjective%20evaluations%20demonstrate%20the%20superior%0Aperformance%20of%20the%20proposed%20MultiAiTutor%20compared%20to%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiAiTutor%253A%2520Child-Friendly%2520Educational%2520Multilingual%2520Speech%2520Generation%250A%2520%2520Tutor%2520with%2520LLMs%26entry.906535625%3DXiaoxue%2520Gao%2520and%2520Huayun%2520Zhang%2520and%2520Nancy%2520F.%2520Chen%26entry.1292438233%3D%2520%2520Generative%2520speech%2520models%2520have%2520demonstrated%2520significant%2520potential%2520in%250Apersonalizing%2520teacher-student%2520interactions%252C%2520offering%2520valuable%2520real-world%250Aapplications%2520for%2520language%2520learning%2520in%2520children%2527s%2520education.%2520However%252C%2520achieving%250Ahigh-quality%252C%2520child-friendly%2520speech%2520generation%2520remains%2520challenging%252C%250Aparticularly%2520for%2520low-resource%2520languages%2520across%2520diverse%2520languages%2520and%2520cultural%250Acontexts.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MultiAiTutor%252C%2520an%2520educational%2520multilingual%250Agenerative%2520AI%2520tutor%2520with%2520child-friendly%2520designs%252C%2520leveraging%2520LLM%2520architecture%250Afor%2520speech%2520generation%2520tailored%2520for%2520educational%2520purposes.%2520We%2520propose%2520to%250Aintegrate%2520age-appropriate%2520multilingual%2520speech%2520generation%2520using%2520LLM%250Aarchitectures%252C%2520facilitating%2520young%2520children%2527s%2520language%2520learning%2520through%250Aculturally%2520relevant%2520image-description%2520tasks%2520in%2520three%2520low-resource%2520languages%253A%250ASingaporean-accent%2520Mandarin%252C%2520Malay%252C%2520and%2520Tamil.%2520Experimental%2520results%2520from%2520both%250Aobjective%2520metrics%2520and%2520subjective%2520evaluations%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520the%2520proposed%2520MultiAiTutor%2520compared%2520to%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiAiTutor%3A%20Child-Friendly%20Educational%20Multilingual%20Speech%20Generation%0A%20%20Tutor%20with%20LLMs&entry.906535625=Xiaoxue%20Gao%20and%20Huayun%20Zhang%20and%20Nancy%20F.%20Chen&entry.1292438233=%20%20Generative%20speech%20models%20have%20demonstrated%20significant%20potential%20in%0Apersonalizing%20teacher-student%20interactions%2C%20offering%20valuable%20real-world%0Aapplications%20for%20language%20learning%20in%20children%27s%20education.%20However%2C%20achieving%0Ahigh-quality%2C%20child-friendly%20speech%20generation%20remains%20challenging%2C%0Aparticularly%20for%20low-resource%20languages%20across%20diverse%20languages%20and%20cultural%0Acontexts.%20In%20this%20paper%2C%20we%20propose%20MultiAiTutor%2C%20an%20educational%20multilingual%0Agenerative%20AI%20tutor%20with%20child-friendly%20designs%2C%20leveraging%20LLM%20architecture%0Afor%20speech%20generation%20tailored%20for%20educational%20purposes.%20We%20propose%20to%0Aintegrate%20age-appropriate%20multilingual%20speech%20generation%20using%20LLM%0Aarchitectures%2C%20facilitating%20young%20children%27s%20language%20learning%20through%0Aculturally%20relevant%20image-description%20tasks%20in%20three%20low-resource%20languages%3A%0ASingaporean-accent%20Mandarin%2C%20Malay%2C%20and%20Tamil.%20Experimental%20results%20from%20both%0Aobjective%20metrics%20and%20subjective%20evaluations%20demonstrate%20the%20superior%0Aperformance%20of%20the%20proposed%20MultiAiTutor%20compared%20to%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08715v2&entry.124074799=Read"},
{"title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior\n  Integration", "author": "Ramil Khafizov and Artem Komarichev and Ruslan Rakhimov and Peter Wonka and Evgeny Burnaev", "abstract": "  We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene\nreconstruction that enhances the CUT3R model by integrating prior information.\nUnlike existing feed-forward methods that rely solely on input images, our\nmethod leverages auxiliary data, such as depth, camera calibrations, or camera\npositions, commonly available in real-world scenarios. We propose a lightweight\nmodification to CUT3R, incorporating a dedicated encoder for each modality to\nextract features, which are fused with RGB image tokens via zero convolution.\nThis flexible design enables seamless integration of any combination of prior\ninformation during inference. Evaluated across multiple benchmarks, including\n3D reconstruction and other multi-view tasks, our approach demonstrates\nsignificant performance improvements, showing its ability to effectively\nutilize available priors while maintaining compatibility with varying input\nmodalities.\n", "link": "http://arxiv.org/abs/2508.11379v1", "date": "2025-08-15", "relevancy": 2.5853, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6561}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6444}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-CUT3R%3A%20Guided%203D%20Reconstruction%20with%20Camera%20and%20Depth%20Prior%0A%20%20Integration&body=Title%3A%20G-CUT3R%3A%20Guided%203D%20Reconstruction%20with%20Camera%20and%20Depth%20Prior%0A%20%20Integration%0AAuthor%3A%20Ramil%20Khafizov%20and%20Artem%20Komarichev%20and%20Ruslan%20Rakhimov%20and%20Peter%20Wonka%20and%20Evgeny%20Burnaev%0AAbstract%3A%20%20%20We%20introduce%20G-CUT3R%2C%20a%20novel%20feed-forward%20approach%20for%20guided%203D%20scene%0Areconstruction%20that%20enhances%20the%20CUT3R%20model%20by%20integrating%20prior%20information.%0AUnlike%20existing%20feed-forward%20methods%20that%20rely%20solely%20on%20input%20images%2C%20our%0Amethod%20leverages%20auxiliary%20data%2C%20such%20as%20depth%2C%20camera%20calibrations%2C%20or%20camera%0Apositions%2C%20commonly%20available%20in%20real-world%20scenarios.%20We%20propose%20a%20lightweight%0Amodification%20to%20CUT3R%2C%20incorporating%20a%20dedicated%20encoder%20for%20each%20modality%20to%0Aextract%20features%2C%20which%20are%20fused%20with%20RGB%20image%20tokens%20via%20zero%20convolution.%0AThis%20flexible%20design%20enables%20seamless%20integration%20of%20any%20combination%20of%20prior%0Ainformation%20during%20inference.%20Evaluated%20across%20multiple%20benchmarks%2C%20including%0A3D%20reconstruction%20and%20other%20multi-view%20tasks%2C%20our%20approach%20demonstrates%0Asignificant%20performance%20improvements%2C%20showing%20its%20ability%20to%20effectively%0Autilize%20available%20priors%20while%20maintaining%20compatibility%20with%20varying%20input%0Amodalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-CUT3R%253A%2520Guided%25203D%2520Reconstruction%2520with%2520Camera%2520and%2520Depth%2520Prior%250A%2520%2520Integration%26entry.906535625%3DRamil%2520Khafizov%2520and%2520Artem%2520Komarichev%2520and%2520Ruslan%2520Rakhimov%2520and%2520Peter%2520Wonka%2520and%2520Evgeny%2520Burnaev%26entry.1292438233%3D%2520%2520We%2520introduce%2520G-CUT3R%252C%2520a%2520novel%2520feed-forward%2520approach%2520for%2520guided%25203D%2520scene%250Areconstruction%2520that%2520enhances%2520the%2520CUT3R%2520model%2520by%2520integrating%2520prior%2520information.%250AUnlike%2520existing%2520feed-forward%2520methods%2520that%2520rely%2520solely%2520on%2520input%2520images%252C%2520our%250Amethod%2520leverages%2520auxiliary%2520data%252C%2520such%2520as%2520depth%252C%2520camera%2520calibrations%252C%2520or%2520camera%250Apositions%252C%2520commonly%2520available%2520in%2520real-world%2520scenarios.%2520We%2520propose%2520a%2520lightweight%250Amodification%2520to%2520CUT3R%252C%2520incorporating%2520a%2520dedicated%2520encoder%2520for%2520each%2520modality%2520to%250Aextract%2520features%252C%2520which%2520are%2520fused%2520with%2520RGB%2520image%2520tokens%2520via%2520zero%2520convolution.%250AThis%2520flexible%2520design%2520enables%2520seamless%2520integration%2520of%2520any%2520combination%2520of%2520prior%250Ainformation%2520during%2520inference.%2520Evaluated%2520across%2520multiple%2520benchmarks%252C%2520including%250A3D%2520reconstruction%2520and%2520other%2520multi-view%2520tasks%252C%2520our%2520approach%2520demonstrates%250Asignificant%2520performance%2520improvements%252C%2520showing%2520its%2520ability%2520to%2520effectively%250Autilize%2520available%2520priors%2520while%2520maintaining%2520compatibility%2520with%2520varying%2520input%250Amodalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-CUT3R%3A%20Guided%203D%20Reconstruction%20with%20Camera%20and%20Depth%20Prior%0A%20%20Integration&entry.906535625=Ramil%20Khafizov%20and%20Artem%20Komarichev%20and%20Ruslan%20Rakhimov%20and%20Peter%20Wonka%20and%20Evgeny%20Burnaev&entry.1292438233=%20%20We%20introduce%20G-CUT3R%2C%20a%20novel%20feed-forward%20approach%20for%20guided%203D%20scene%0Areconstruction%20that%20enhances%20the%20CUT3R%20model%20by%20integrating%20prior%20information.%0AUnlike%20existing%20feed-forward%20methods%20that%20rely%20solely%20on%20input%20images%2C%20our%0Amethod%20leverages%20auxiliary%20data%2C%20such%20as%20depth%2C%20camera%20calibrations%2C%20or%20camera%0Apositions%2C%20commonly%20available%20in%20real-world%20scenarios.%20We%20propose%20a%20lightweight%0Amodification%20to%20CUT3R%2C%20incorporating%20a%20dedicated%20encoder%20for%20each%20modality%20to%0Aextract%20features%2C%20which%20are%20fused%20with%20RGB%20image%20tokens%20via%20zero%20convolution.%0AThis%20flexible%20design%20enables%20seamless%20integration%20of%20any%20combination%20of%20prior%0Ainformation%20during%20inference.%20Evaluated%20across%20multiple%20benchmarks%2C%20including%0A3D%20reconstruction%20and%20other%20multi-view%20tasks%2C%20our%20approach%20demonstrates%0Asignificant%20performance%20improvements%2C%20showing%20its%20ability%20to%20effectively%0Autilize%20available%20priors%20while%20maintaining%20compatibility%20with%20varying%20input%0Amodalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11379v1&entry.124074799=Read"},
{"title": "Hierarchical Graph Feature Enhancement with Adaptive Frequency\n  Modulation for Visual Recognition", "author": "Feiyue Zhao and Zhichao Zhang", "abstract": "  Convolutional neural networks (CNNs) have\n  demonstrated strong performance in visual recognition tasks,\n  but their inherent reliance on regular grid structures limits\n  their capacity to model complex topological relationships and\n  non-local semantics within images. To address this limita tion, we propose\nthe hierarchical graph feature enhancement\n  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to\nenhance both structural awareness and\n  feature representation. HGFE builds two complementary levels\n  of graph structures: intra-window graph convolution to cap ture local spatial\ndependencies and inter-window supernode\n  interactions to model global semantic relationships. Moreover,\n  we introduce an adaptive frequency modulation module that\n  dynamically balances low-frequency and high-frequency signal\n  propagation, preserving critical edge and texture information\n  while mitigating over-smoothing. The proposed HGFE module\n  is lightweight, end-to-end trainable, and can be seamlessly\n  integrated into standard CNN backbone networks. Extensive\n  experiments on CIFAR-100 (classification), PASCAL VOC,\n  and VisDrone (detection), as well as CrackSeg and CarParts\n  (segmentation), validated the effectiveness of the HGFE in\n  improving structural representation and enhancing overall\n  recognition performance.\n", "link": "http://arxiv.org/abs/2508.11497v1", "date": "2025-08-15", "relevancy": 2.5761, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5184}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.515}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Graph%20Feature%20Enhancement%20with%20Adaptive%20Frequency%0A%20%20Modulation%20for%20Visual%20Recognition&body=Title%3A%20Hierarchical%20Graph%20Feature%20Enhancement%20with%20Adaptive%20Frequency%0A%20%20Modulation%20for%20Visual%20Recognition%0AAuthor%3A%20Feiyue%20Zhao%20and%20Zhichao%20Zhang%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20have%0A%20%20demonstrated%20strong%20performance%20in%20visual%20recognition%20tasks%2C%0A%20%20but%20their%20inherent%20reliance%20on%20regular%20grid%20structures%20limits%0A%20%20their%20capacity%20to%20model%20complex%20topological%20relationships%20and%0A%20%20non-local%20semantics%20within%20images.%20To%20address%20this%20limita%20tion%2C%20we%20propose%0Athe%20hierarchical%20graph%20feature%20enhancement%0A%20%20%28HGFE%29%2C%20a%20novel%20framework%20that%20integrates%20graph-based%20rea%20soning%20into%20CNNs%20to%0Aenhance%20both%20structural%20awareness%20and%0A%20%20feature%20representation.%20HGFE%20builds%20two%20complementary%20levels%0A%20%20of%20graph%20structures%3A%20intra-window%20graph%20convolution%20to%20cap%20ture%20local%20spatial%0Adependencies%20and%20inter-window%20supernode%0A%20%20interactions%20to%20model%20global%20semantic%20relationships.%20Moreover%2C%0A%20%20we%20introduce%20an%20adaptive%20frequency%20modulation%20module%20that%0A%20%20dynamically%20balances%20low-frequency%20and%20high-frequency%20signal%0A%20%20propagation%2C%20preserving%20critical%20edge%20and%20texture%20information%0A%20%20while%20mitigating%20over-smoothing.%20The%20proposed%20HGFE%20module%0A%20%20is%20lightweight%2C%20end-to-end%20trainable%2C%20and%20can%20be%20seamlessly%0A%20%20integrated%20into%20standard%20CNN%20backbone%20networks.%20Extensive%0A%20%20experiments%20on%20CIFAR-100%20%28classification%29%2C%20PASCAL%20VOC%2C%0A%20%20and%20VisDrone%20%28detection%29%2C%20as%20well%20as%20CrackSeg%20and%20CarParts%0A%20%20%28segmentation%29%2C%20validated%20the%20effectiveness%20of%20the%20HGFE%20in%0A%20%20improving%20structural%20representation%20and%20enhancing%20overall%0A%20%20recognition%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Graph%2520Feature%2520Enhancement%2520with%2520Adaptive%2520Frequency%250A%2520%2520Modulation%2520for%2520Visual%2520Recognition%26entry.906535625%3DFeiyue%2520Zhao%2520and%2520Zhichao%2520Zhang%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520have%250A%2520%2520demonstrated%2520strong%2520performance%2520in%2520visual%2520recognition%2520tasks%252C%250A%2520%2520but%2520their%2520inherent%2520reliance%2520on%2520regular%2520grid%2520structures%2520limits%250A%2520%2520their%2520capacity%2520to%2520model%2520complex%2520topological%2520relationships%2520and%250A%2520%2520non-local%2520semantics%2520within%2520images.%2520To%2520address%2520this%2520limita%2520tion%252C%2520we%2520propose%250Athe%2520hierarchical%2520graph%2520feature%2520enhancement%250A%2520%2520%2528HGFE%2529%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520graph-based%2520rea%2520soning%2520into%2520CNNs%2520to%250Aenhance%2520both%2520structural%2520awareness%2520and%250A%2520%2520feature%2520representation.%2520HGFE%2520builds%2520two%2520complementary%2520levels%250A%2520%2520of%2520graph%2520structures%253A%2520intra-window%2520graph%2520convolution%2520to%2520cap%2520ture%2520local%2520spatial%250Adependencies%2520and%2520inter-window%2520supernode%250A%2520%2520interactions%2520to%2520model%2520global%2520semantic%2520relationships.%2520Moreover%252C%250A%2520%2520we%2520introduce%2520an%2520adaptive%2520frequency%2520modulation%2520module%2520that%250A%2520%2520dynamically%2520balances%2520low-frequency%2520and%2520high-frequency%2520signal%250A%2520%2520propagation%252C%2520preserving%2520critical%2520edge%2520and%2520texture%2520information%250A%2520%2520while%2520mitigating%2520over-smoothing.%2520The%2520proposed%2520HGFE%2520module%250A%2520%2520is%2520lightweight%252C%2520end-to-end%2520trainable%252C%2520and%2520can%2520be%2520seamlessly%250A%2520%2520integrated%2520into%2520standard%2520CNN%2520backbone%2520networks.%2520Extensive%250A%2520%2520experiments%2520on%2520CIFAR-100%2520%2528classification%2529%252C%2520PASCAL%2520VOC%252C%250A%2520%2520and%2520VisDrone%2520%2528detection%2529%252C%2520as%2520well%2520as%2520CrackSeg%2520and%2520CarParts%250A%2520%2520%2528segmentation%2529%252C%2520validated%2520the%2520effectiveness%2520of%2520the%2520HGFE%2520in%250A%2520%2520improving%2520structural%2520representation%2520and%2520enhancing%2520overall%250A%2520%2520recognition%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Graph%20Feature%20Enhancement%20with%20Adaptive%20Frequency%0A%20%20Modulation%20for%20Visual%20Recognition&entry.906535625=Feiyue%20Zhao%20and%20Zhichao%20Zhang&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20have%0A%20%20demonstrated%20strong%20performance%20in%20visual%20recognition%20tasks%2C%0A%20%20but%20their%20inherent%20reliance%20on%20regular%20grid%20structures%20limits%0A%20%20their%20capacity%20to%20model%20complex%20topological%20relationships%20and%0A%20%20non-local%20semantics%20within%20images.%20To%20address%20this%20limita%20tion%2C%20we%20propose%0Athe%20hierarchical%20graph%20feature%20enhancement%0A%20%20%28HGFE%29%2C%20a%20novel%20framework%20that%20integrates%20graph-based%20rea%20soning%20into%20CNNs%20to%0Aenhance%20both%20structural%20awareness%20and%0A%20%20feature%20representation.%20HGFE%20builds%20two%20complementary%20levels%0A%20%20of%20graph%20structures%3A%20intra-window%20graph%20convolution%20to%20cap%20ture%20local%20spatial%0Adependencies%20and%20inter-window%20supernode%0A%20%20interactions%20to%20model%20global%20semantic%20relationships.%20Moreover%2C%0A%20%20we%20introduce%20an%20adaptive%20frequency%20modulation%20module%20that%0A%20%20dynamically%20balances%20low-frequency%20and%20high-frequency%20signal%0A%20%20propagation%2C%20preserving%20critical%20edge%20and%20texture%20information%0A%20%20while%20mitigating%20over-smoothing.%20The%20proposed%20HGFE%20module%0A%20%20is%20lightweight%2C%20end-to-end%20trainable%2C%20and%20can%20be%20seamlessly%0A%20%20integrated%20into%20standard%20CNN%20backbone%20networks.%20Extensive%0A%20%20experiments%20on%20CIFAR-100%20%28classification%29%2C%20PASCAL%20VOC%2C%0A%20%20and%20VisDrone%20%28detection%29%2C%20as%20well%20as%20CrackSeg%20and%20CarParts%0A%20%20%28segmentation%29%2C%20validated%20the%20effectiveness%20of%20the%20HGFE%20in%0A%20%20improving%20structural%20representation%20and%20enhancing%20overall%0A%20%20recognition%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11497v1&entry.124074799=Read"},
{"title": "AFR-CLIP: Enhancing Zero-Shot Industrial Anomaly Detection with\n  Stateless-to-Stateful Anomaly Feature Rectification", "author": "Jingyi Yuan and Chenqiang Gao and Pengyu Jie and Xuan Xia and Shangri Huang and Wanquan Liu", "abstract": "  Recently, zero-shot anomaly detection (ZSAD) has emerged as a pivotal\nparadigm for industrial inspection and medical diagnostics, detecting defects\nin novel objects without requiring any target-dataset samples during training.\nExisting CLIP-based ZSAD methods generate anomaly maps by measuring the cosine\nsimilarity between visual and textual features. However, CLIP's alignment with\nobject categories instead of their anomalous states limits its effectiveness\nfor anomaly detection. To address this limitation, we propose AFR-CLIP, a\nCLIP-based anomaly feature rectification framework. AFR-CLIP first performs\nimage-guided textual rectification, embedding the implicit defect information\nfrom the image into a stateless prompt that describes the object category\nwithout indicating any anomalous state. The enriched textual embeddings are\nthen compared with two pre-defined stateful (normal or abnormal) embeddings,\nand their text-on-text similarity yields the anomaly map that highlights\ndefective regions. To further enhance perception to multi-scale features and\ncomplex anomalies, we introduce self prompting (SP) and multi-patch feature\naggregation (MPFA) modules. Extensive experiments are conducted on eleven\nanomaly detection benchmarks across industrial and medical domains,\ndemonstrating AFR-CLIP's superiority in ZSAD.\n", "link": "http://arxiv.org/abs/2503.12910v2", "date": "2025-08-15", "relevancy": 2.5293, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5358}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4921}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AFR-CLIP%3A%20Enhancing%20Zero-Shot%20Industrial%20Anomaly%20Detection%20with%0A%20%20Stateless-to-Stateful%20Anomaly%20Feature%20Rectification&body=Title%3A%20AFR-CLIP%3A%20Enhancing%20Zero-Shot%20Industrial%20Anomaly%20Detection%20with%0A%20%20Stateless-to-Stateful%20Anomaly%20Feature%20Rectification%0AAuthor%3A%20Jingyi%20Yuan%20and%20Chenqiang%20Gao%20and%20Pengyu%20Jie%20and%20Xuan%20Xia%20and%20Shangri%20Huang%20and%20Wanquan%20Liu%0AAbstract%3A%20%20%20Recently%2C%20zero-shot%20anomaly%20detection%20%28ZSAD%29%20has%20emerged%20as%20a%20pivotal%0Aparadigm%20for%20industrial%20inspection%20and%20medical%20diagnostics%2C%20detecting%20defects%0Ain%20novel%20objects%20without%20requiring%20any%20target-dataset%20samples%20during%20training.%0AExisting%20CLIP-based%20ZSAD%20methods%20generate%20anomaly%20maps%20by%20measuring%20the%20cosine%0Asimilarity%20between%20visual%20and%20textual%20features.%20However%2C%20CLIP%27s%20alignment%20with%0Aobject%20categories%20instead%20of%20their%20anomalous%20states%20limits%20its%20effectiveness%0Afor%20anomaly%20detection.%20To%20address%20this%20limitation%2C%20we%20propose%20AFR-CLIP%2C%20a%0ACLIP-based%20anomaly%20feature%20rectification%20framework.%20AFR-CLIP%20first%20performs%0Aimage-guided%20textual%20rectification%2C%20embedding%20the%20implicit%20defect%20information%0Afrom%20the%20image%20into%20a%20stateless%20prompt%20that%20describes%20the%20object%20category%0Awithout%20indicating%20any%20anomalous%20state.%20The%20enriched%20textual%20embeddings%20are%0Athen%20compared%20with%20two%20pre-defined%20stateful%20%28normal%20or%20abnormal%29%20embeddings%2C%0Aand%20their%20text-on-text%20similarity%20yields%20the%20anomaly%20map%20that%20highlights%0Adefective%20regions.%20To%20further%20enhance%20perception%20to%20multi-scale%20features%20and%0Acomplex%20anomalies%2C%20we%20introduce%20self%20prompting%20%28SP%29%20and%20multi-patch%20feature%0Aaggregation%20%28MPFA%29%20modules.%20Extensive%20experiments%20are%20conducted%20on%20eleven%0Aanomaly%20detection%20benchmarks%20across%20industrial%20and%20medical%20domains%2C%0Ademonstrating%20AFR-CLIP%27s%20superiority%20in%20ZSAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.12910v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAFR-CLIP%253A%2520Enhancing%2520Zero-Shot%2520Industrial%2520Anomaly%2520Detection%2520with%250A%2520%2520Stateless-to-Stateful%2520Anomaly%2520Feature%2520Rectification%26entry.906535625%3DJingyi%2520Yuan%2520and%2520Chenqiang%2520Gao%2520and%2520Pengyu%2520Jie%2520and%2520Xuan%2520Xia%2520and%2520Shangri%2520Huang%2520and%2520Wanquan%2520Liu%26entry.1292438233%3D%2520%2520Recently%252C%2520zero-shot%2520anomaly%2520detection%2520%2528ZSAD%2529%2520has%2520emerged%2520as%2520a%2520pivotal%250Aparadigm%2520for%2520industrial%2520inspection%2520and%2520medical%2520diagnostics%252C%2520detecting%2520defects%250Ain%2520novel%2520objects%2520without%2520requiring%2520any%2520target-dataset%2520samples%2520during%2520training.%250AExisting%2520CLIP-based%2520ZSAD%2520methods%2520generate%2520anomaly%2520maps%2520by%2520measuring%2520the%2520cosine%250Asimilarity%2520between%2520visual%2520and%2520textual%2520features.%2520However%252C%2520CLIP%2527s%2520alignment%2520with%250Aobject%2520categories%2520instead%2520of%2520their%2520anomalous%2520states%2520limits%2520its%2520effectiveness%250Afor%2520anomaly%2520detection.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520AFR-CLIP%252C%2520a%250ACLIP-based%2520anomaly%2520feature%2520rectification%2520framework.%2520AFR-CLIP%2520first%2520performs%250Aimage-guided%2520textual%2520rectification%252C%2520embedding%2520the%2520implicit%2520defect%2520information%250Afrom%2520the%2520image%2520into%2520a%2520stateless%2520prompt%2520that%2520describes%2520the%2520object%2520category%250Awithout%2520indicating%2520any%2520anomalous%2520state.%2520The%2520enriched%2520textual%2520embeddings%2520are%250Athen%2520compared%2520with%2520two%2520pre-defined%2520stateful%2520%2528normal%2520or%2520abnormal%2529%2520embeddings%252C%250Aand%2520their%2520text-on-text%2520similarity%2520yields%2520the%2520anomaly%2520map%2520that%2520highlights%250Adefective%2520regions.%2520To%2520further%2520enhance%2520perception%2520to%2520multi-scale%2520features%2520and%250Acomplex%2520anomalies%252C%2520we%2520introduce%2520self%2520prompting%2520%2528SP%2529%2520and%2520multi-patch%2520feature%250Aaggregation%2520%2528MPFA%2529%2520modules.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520eleven%250Aanomaly%2520detection%2520benchmarks%2520across%2520industrial%2520and%2520medical%2520domains%252C%250Ademonstrating%2520AFR-CLIP%2527s%2520superiority%2520in%2520ZSAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12910v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AFR-CLIP%3A%20Enhancing%20Zero-Shot%20Industrial%20Anomaly%20Detection%20with%0A%20%20Stateless-to-Stateful%20Anomaly%20Feature%20Rectification&entry.906535625=Jingyi%20Yuan%20and%20Chenqiang%20Gao%20and%20Pengyu%20Jie%20and%20Xuan%20Xia%20and%20Shangri%20Huang%20and%20Wanquan%20Liu&entry.1292438233=%20%20Recently%2C%20zero-shot%20anomaly%20detection%20%28ZSAD%29%20has%20emerged%20as%20a%20pivotal%0Aparadigm%20for%20industrial%20inspection%20and%20medical%20diagnostics%2C%20detecting%20defects%0Ain%20novel%20objects%20without%20requiring%20any%20target-dataset%20samples%20during%20training.%0AExisting%20CLIP-based%20ZSAD%20methods%20generate%20anomaly%20maps%20by%20measuring%20the%20cosine%0Asimilarity%20between%20visual%20and%20textual%20features.%20However%2C%20CLIP%27s%20alignment%20with%0Aobject%20categories%20instead%20of%20their%20anomalous%20states%20limits%20its%20effectiveness%0Afor%20anomaly%20detection.%20To%20address%20this%20limitation%2C%20we%20propose%20AFR-CLIP%2C%20a%0ACLIP-based%20anomaly%20feature%20rectification%20framework.%20AFR-CLIP%20first%20performs%0Aimage-guided%20textual%20rectification%2C%20embedding%20the%20implicit%20defect%20information%0Afrom%20the%20image%20into%20a%20stateless%20prompt%20that%20describes%20the%20object%20category%0Awithout%20indicating%20any%20anomalous%20state.%20The%20enriched%20textual%20embeddings%20are%0Athen%20compared%20with%20two%20pre-defined%20stateful%20%28normal%20or%20abnormal%29%20embeddings%2C%0Aand%20their%20text-on-text%20similarity%20yields%20the%20anomaly%20map%20that%20highlights%0Adefective%20regions.%20To%20further%20enhance%20perception%20to%20multi-scale%20features%20and%0Acomplex%20anomalies%2C%20we%20introduce%20self%20prompting%20%28SP%29%20and%20multi-patch%20feature%0Aaggregation%20%28MPFA%29%20modules.%20Extensive%20experiments%20are%20conducted%20on%20eleven%0Aanomaly%20detection%20benchmarks%20across%20industrial%20and%20medical%20domains%2C%0Ademonstrating%20AFR-CLIP%27s%20superiority%20in%20ZSAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.12910v2&entry.124074799=Read"},
{"title": "OpenConstruction: A Systematic Synthesis of Open Visual Datasets for\n  Data-Centric Artificial Intelligence in Construction Monitoring", "author": "Ruoxin Xiong and Yanyu Wang and Jiannan Cai and Kaijian Liu and Yuansheng Zhu and Pingbo Tang and Nora El-Gohary", "abstract": "  The construction industry increasingly relies on visual data to support\nArtificial Intelligence (AI) and Machine Learning (ML) applications for site\nmonitoring. High-quality, domain-specific datasets, comprising images, videos,\nand point clouds, capture site geometry and spatiotemporal dynamics, including\nthe location and interaction of objects, workers, and materials. However,\ndespite growing interest in leveraging visual datasets, existing resources vary\nwidely in sizes, data modalities, annotation quality, and representativeness of\nreal-world construction conditions. A systematic review to categorize their\ndata characteristics and application contexts is still lacking, limiting the\ncommunity's ability to fully understand the dataset landscape, identify\ncritical gaps, and guide future directions toward more effective, reliable, and\nscalable AI applications in construction. To address this gap, this study\nconducts an extensive search of academic databases and open-data platforms,\nyielding 51 publicly available visual datasets that span the 2005-2024 period.\nThese datasets are categorized using a structured data schema covering (i) data\nfundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and\npoint cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)\ndownstream application domains (e.g., progress tracking). This study\nsynthesizes these findings into an open-source catalog, OpenConstruction,\nsupporting data-driven method development. Furthermore, the study discusses\nseveral critical limitations in the existing construction dataset landscape and\npresents a roadmap for future data infrastructure anchored in the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) principles. By\nreviewing the current landscape and outlining strategic priorities, this study\nsupports the advancement of data-centric solutions in the construction sector.\n", "link": "http://arxiv.org/abs/2508.11482v1", "date": "2025-08-15", "relevancy": 2.5289, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenConstruction%3A%20A%20Systematic%20Synthesis%20of%20Open%20Visual%20Datasets%20for%0A%20%20Data-Centric%20Artificial%20Intelligence%20in%20Construction%20Monitoring&body=Title%3A%20OpenConstruction%3A%20A%20Systematic%20Synthesis%20of%20Open%20Visual%20Datasets%20for%0A%20%20Data-Centric%20Artificial%20Intelligence%20in%20Construction%20Monitoring%0AAuthor%3A%20Ruoxin%20Xiong%20and%20Yanyu%20Wang%20and%20Jiannan%20Cai%20and%20Kaijian%20Liu%20and%20Yuansheng%20Zhu%20and%20Pingbo%20Tang%20and%20Nora%20El-Gohary%0AAbstract%3A%20%20%20The%20construction%20industry%20increasingly%20relies%20on%20visual%20data%20to%20support%0AArtificial%20Intelligence%20%28AI%29%20and%20Machine%20Learning%20%28ML%29%20applications%20for%20site%0Amonitoring.%20High-quality%2C%20domain-specific%20datasets%2C%20comprising%20images%2C%20videos%2C%0Aand%20point%20clouds%2C%20capture%20site%20geometry%20and%20spatiotemporal%20dynamics%2C%20including%0Athe%20location%20and%20interaction%20of%20objects%2C%20workers%2C%20and%20materials.%20However%2C%0Adespite%20growing%20interest%20in%20leveraging%20visual%20datasets%2C%20existing%20resources%20vary%0Awidely%20in%20sizes%2C%20data%20modalities%2C%20annotation%20quality%2C%20and%20representativeness%20of%0Areal-world%20construction%20conditions.%20A%20systematic%20review%20to%20categorize%20their%0Adata%20characteristics%20and%20application%20contexts%20is%20still%20lacking%2C%20limiting%20the%0Acommunity%27s%20ability%20to%20fully%20understand%20the%20dataset%20landscape%2C%20identify%0Acritical%20gaps%2C%20and%20guide%20future%20directions%20toward%20more%20effective%2C%20reliable%2C%20and%0Ascalable%20AI%20applications%20in%20construction.%20To%20address%20this%20gap%2C%20this%20study%0Aconducts%20an%20extensive%20search%20of%20academic%20databases%20and%20open-data%20platforms%2C%0Ayielding%2051%20publicly%20available%20visual%20datasets%20that%20span%20the%202005-2024%20period.%0AThese%20datasets%20are%20categorized%20using%20a%20structured%20data%20schema%20covering%20%28i%29%20data%0Afundamentals%20%28e.g.%2C%20size%20and%20license%29%2C%20%28ii%29%20data%20modalities%20%28e.g.%2C%20RGB%20and%0Apoint%20cloud%29%2C%20%28iii%29%20annotation%20frameworks%20%28e.g.%2C%20bounding%20boxes%29%2C%20and%20%28iv%29%0Adownstream%20application%20domains%20%28e.g.%2C%20progress%20tracking%29.%20This%20study%0Asynthesizes%20these%20findings%20into%20an%20open-source%20catalog%2C%20OpenConstruction%2C%0Asupporting%20data-driven%20method%20development.%20Furthermore%2C%20the%20study%20discusses%0Aseveral%20critical%20limitations%20in%20the%20existing%20construction%20dataset%20landscape%20and%0Apresents%20a%20roadmap%20for%20future%20data%20infrastructure%20anchored%20in%20the%20Findability%2C%0AAccessibility%2C%20Interoperability%2C%20and%20Reusability%20%28FAIR%29%20principles.%20By%0Areviewing%20the%20current%20landscape%20and%20outlining%20strategic%20priorities%2C%20this%20study%0Asupports%20the%20advancement%20of%20data-centric%20solutions%20in%20the%20construction%20sector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenConstruction%253A%2520A%2520Systematic%2520Synthesis%2520of%2520Open%2520Visual%2520Datasets%2520for%250A%2520%2520Data-Centric%2520Artificial%2520Intelligence%2520in%2520Construction%2520Monitoring%26entry.906535625%3DRuoxin%2520Xiong%2520and%2520Yanyu%2520Wang%2520and%2520Jiannan%2520Cai%2520and%2520Kaijian%2520Liu%2520and%2520Yuansheng%2520Zhu%2520and%2520Pingbo%2520Tang%2520and%2520Nora%2520El-Gohary%26entry.1292438233%3D%2520%2520The%2520construction%2520industry%2520increasingly%2520relies%2520on%2520visual%2520data%2520to%2520support%250AArtificial%2520Intelligence%2520%2528AI%2529%2520and%2520Machine%2520Learning%2520%2528ML%2529%2520applications%2520for%2520site%250Amonitoring.%2520High-quality%252C%2520domain-specific%2520datasets%252C%2520comprising%2520images%252C%2520videos%252C%250Aand%2520point%2520clouds%252C%2520capture%2520site%2520geometry%2520and%2520spatiotemporal%2520dynamics%252C%2520including%250Athe%2520location%2520and%2520interaction%2520of%2520objects%252C%2520workers%252C%2520and%2520materials.%2520However%252C%250Adespite%2520growing%2520interest%2520in%2520leveraging%2520visual%2520datasets%252C%2520existing%2520resources%2520vary%250Awidely%2520in%2520sizes%252C%2520data%2520modalities%252C%2520annotation%2520quality%252C%2520and%2520representativeness%2520of%250Areal-world%2520construction%2520conditions.%2520A%2520systematic%2520review%2520to%2520categorize%2520their%250Adata%2520characteristics%2520and%2520application%2520contexts%2520is%2520still%2520lacking%252C%2520limiting%2520the%250Acommunity%2527s%2520ability%2520to%2520fully%2520understand%2520the%2520dataset%2520landscape%252C%2520identify%250Acritical%2520gaps%252C%2520and%2520guide%2520future%2520directions%2520toward%2520more%2520effective%252C%2520reliable%252C%2520and%250Ascalable%2520AI%2520applications%2520in%2520construction.%2520To%2520address%2520this%2520gap%252C%2520this%2520study%250Aconducts%2520an%2520extensive%2520search%2520of%2520academic%2520databases%2520and%2520open-data%2520platforms%252C%250Ayielding%252051%2520publicly%2520available%2520visual%2520datasets%2520that%2520span%2520the%25202005-2024%2520period.%250AThese%2520datasets%2520are%2520categorized%2520using%2520a%2520structured%2520data%2520schema%2520covering%2520%2528i%2529%2520data%250Afundamentals%2520%2528e.g.%252C%2520size%2520and%2520license%2529%252C%2520%2528ii%2529%2520data%2520modalities%2520%2528e.g.%252C%2520RGB%2520and%250Apoint%2520cloud%2529%252C%2520%2528iii%2529%2520annotation%2520frameworks%2520%2528e.g.%252C%2520bounding%2520boxes%2529%252C%2520and%2520%2528iv%2529%250Adownstream%2520application%2520domains%2520%2528e.g.%252C%2520progress%2520tracking%2529.%2520This%2520study%250Asynthesizes%2520these%2520findings%2520into%2520an%2520open-source%2520catalog%252C%2520OpenConstruction%252C%250Asupporting%2520data-driven%2520method%2520development.%2520Furthermore%252C%2520the%2520study%2520discusses%250Aseveral%2520critical%2520limitations%2520in%2520the%2520existing%2520construction%2520dataset%2520landscape%2520and%250Apresents%2520a%2520roadmap%2520for%2520future%2520data%2520infrastructure%2520anchored%2520in%2520the%2520Findability%252C%250AAccessibility%252C%2520Interoperability%252C%2520and%2520Reusability%2520%2528FAIR%2529%2520principles.%2520By%250Areviewing%2520the%2520current%2520landscape%2520and%2520outlining%2520strategic%2520priorities%252C%2520this%2520study%250Asupports%2520the%2520advancement%2520of%2520data-centric%2520solutions%2520in%2520the%2520construction%2520sector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenConstruction%3A%20A%20Systematic%20Synthesis%20of%20Open%20Visual%20Datasets%20for%0A%20%20Data-Centric%20Artificial%20Intelligence%20in%20Construction%20Monitoring&entry.906535625=Ruoxin%20Xiong%20and%20Yanyu%20Wang%20and%20Jiannan%20Cai%20and%20Kaijian%20Liu%20and%20Yuansheng%20Zhu%20and%20Pingbo%20Tang%20and%20Nora%20El-Gohary&entry.1292438233=%20%20The%20construction%20industry%20increasingly%20relies%20on%20visual%20data%20to%20support%0AArtificial%20Intelligence%20%28AI%29%20and%20Machine%20Learning%20%28ML%29%20applications%20for%20site%0Amonitoring.%20High-quality%2C%20domain-specific%20datasets%2C%20comprising%20images%2C%20videos%2C%0Aand%20point%20clouds%2C%20capture%20site%20geometry%20and%20spatiotemporal%20dynamics%2C%20including%0Athe%20location%20and%20interaction%20of%20objects%2C%20workers%2C%20and%20materials.%20However%2C%0Adespite%20growing%20interest%20in%20leveraging%20visual%20datasets%2C%20existing%20resources%20vary%0Awidely%20in%20sizes%2C%20data%20modalities%2C%20annotation%20quality%2C%20and%20representativeness%20of%0Areal-world%20construction%20conditions.%20A%20systematic%20review%20to%20categorize%20their%0Adata%20characteristics%20and%20application%20contexts%20is%20still%20lacking%2C%20limiting%20the%0Acommunity%27s%20ability%20to%20fully%20understand%20the%20dataset%20landscape%2C%20identify%0Acritical%20gaps%2C%20and%20guide%20future%20directions%20toward%20more%20effective%2C%20reliable%2C%20and%0Ascalable%20AI%20applications%20in%20construction.%20To%20address%20this%20gap%2C%20this%20study%0Aconducts%20an%20extensive%20search%20of%20academic%20databases%20and%20open-data%20platforms%2C%0Ayielding%2051%20publicly%20available%20visual%20datasets%20that%20span%20the%202005-2024%20period.%0AThese%20datasets%20are%20categorized%20using%20a%20structured%20data%20schema%20covering%20%28i%29%20data%0Afundamentals%20%28e.g.%2C%20size%20and%20license%29%2C%20%28ii%29%20data%20modalities%20%28e.g.%2C%20RGB%20and%0Apoint%20cloud%29%2C%20%28iii%29%20annotation%20frameworks%20%28e.g.%2C%20bounding%20boxes%29%2C%20and%20%28iv%29%0Adownstream%20application%20domains%20%28e.g.%2C%20progress%20tracking%29.%20This%20study%0Asynthesizes%20these%20findings%20into%20an%20open-source%20catalog%2C%20OpenConstruction%2C%0Asupporting%20data-driven%20method%20development.%20Furthermore%2C%20the%20study%20discusses%0Aseveral%20critical%20limitations%20in%20the%20existing%20construction%20dataset%20landscape%20and%0Apresents%20a%20roadmap%20for%20future%20data%20infrastructure%20anchored%20in%20the%20Findability%2C%0AAccessibility%2C%20Interoperability%2C%20and%20Reusability%20%28FAIR%29%20principles.%20By%0Areviewing%20the%20current%20landscape%20and%20outlining%20strategic%20priorities%2C%20this%20study%0Asupports%20the%20advancement%20of%20data-centric%20solutions%20in%20the%20construction%20sector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11482v1&entry.124074799=Read"},
{"title": "Convolutional Autoencoders for Data Compression and Anomaly Detection in\n  Small Satellite Technologies", "author": "Dishanand Jayeprokash and Julia Gonski", "abstract": "  Small satellite technologies have enhanced the potential and feasibility of\ngeodesic missions, through simplification of design and decreased costs\nallowing for more frequent launches. On-satellite data acquisition systems can\nbenefit from the implementation of machine learning (ML), for better\nperformance and greater efficiency on tasks such as image processing or feature\nextraction. This work presents convolutional autoencoders for implementation on\nthe payload of small satellites, designed to achieve dual functionality of data\ncompression for more efficient off-satellite transmission, and at-source\nanomaly detection to inform satellite data-taking. This capability is\ndemonstrated for a use case of disaster monitoring using aerial image datasets\nof the African continent, offering avenues for both novel ML-based approaches\nin small satellite applications along with the expansion of space technology\nand artificial intelligence in Africa.\n", "link": "http://arxiv.org/abs/2505.00040v2", "date": "2025-08-15", "relevancy": 2.4601, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5085}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4938}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convolutional%20Autoencoders%20for%20Data%20Compression%20and%20Anomaly%20Detection%20in%0A%20%20Small%20Satellite%20Technologies&body=Title%3A%20Convolutional%20Autoencoders%20for%20Data%20Compression%20and%20Anomaly%20Detection%20in%0A%20%20Small%20Satellite%20Technologies%0AAuthor%3A%20Dishanand%20Jayeprokash%20and%20Julia%20Gonski%0AAbstract%3A%20%20%20Small%20satellite%20technologies%20have%20enhanced%20the%20potential%20and%20feasibility%20of%0Ageodesic%20missions%2C%20through%20simplification%20of%20design%20and%20decreased%20costs%0Aallowing%20for%20more%20frequent%20launches.%20On-satellite%20data%20acquisition%20systems%20can%0Abenefit%20from%20the%20implementation%20of%20machine%20learning%20%28ML%29%2C%20for%20better%0Aperformance%20and%20greater%20efficiency%20on%20tasks%20such%20as%20image%20processing%20or%20feature%0Aextraction.%20This%20work%20presents%20convolutional%20autoencoders%20for%20implementation%20on%0Athe%20payload%20of%20small%20satellites%2C%20designed%20to%20achieve%20dual%20functionality%20of%20data%0Acompression%20for%20more%20efficient%20off-satellite%20transmission%2C%20and%20at-source%0Aanomaly%20detection%20to%20inform%20satellite%20data-taking.%20This%20capability%20is%0Ademonstrated%20for%20a%20use%20case%20of%20disaster%20monitoring%20using%20aerial%20image%20datasets%0Aof%20the%20African%20continent%2C%20offering%20avenues%20for%20both%20novel%20ML-based%20approaches%0Ain%20small%20satellite%20applications%20along%20with%20the%20expansion%20of%20space%20technology%0Aand%20artificial%20intelligence%20in%20Africa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvolutional%2520Autoencoders%2520for%2520Data%2520Compression%2520and%2520Anomaly%2520Detection%2520in%250A%2520%2520Small%2520Satellite%2520Technologies%26entry.906535625%3DDishanand%2520Jayeprokash%2520and%2520Julia%2520Gonski%26entry.1292438233%3D%2520%2520Small%2520satellite%2520technologies%2520have%2520enhanced%2520the%2520potential%2520and%2520feasibility%2520of%250Ageodesic%2520missions%252C%2520through%2520simplification%2520of%2520design%2520and%2520decreased%2520costs%250Aallowing%2520for%2520more%2520frequent%2520launches.%2520On-satellite%2520data%2520acquisition%2520systems%2520can%250Abenefit%2520from%2520the%2520implementation%2520of%2520machine%2520learning%2520%2528ML%2529%252C%2520for%2520better%250Aperformance%2520and%2520greater%2520efficiency%2520on%2520tasks%2520such%2520as%2520image%2520processing%2520or%2520feature%250Aextraction.%2520This%2520work%2520presents%2520convolutional%2520autoencoders%2520for%2520implementation%2520on%250Athe%2520payload%2520of%2520small%2520satellites%252C%2520designed%2520to%2520achieve%2520dual%2520functionality%2520of%2520data%250Acompression%2520for%2520more%2520efficient%2520off-satellite%2520transmission%252C%2520and%2520at-source%250Aanomaly%2520detection%2520to%2520inform%2520satellite%2520data-taking.%2520This%2520capability%2520is%250Ademonstrated%2520for%2520a%2520use%2520case%2520of%2520disaster%2520monitoring%2520using%2520aerial%2520image%2520datasets%250Aof%2520the%2520African%2520continent%252C%2520offering%2520avenues%2520for%2520both%2520novel%2520ML-based%2520approaches%250Ain%2520small%2520satellite%2520applications%2520along%2520with%2520the%2520expansion%2520of%2520space%2520technology%250Aand%2520artificial%2520intelligence%2520in%2520Africa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convolutional%20Autoencoders%20for%20Data%20Compression%20and%20Anomaly%20Detection%20in%0A%20%20Small%20Satellite%20Technologies&entry.906535625=Dishanand%20Jayeprokash%20and%20Julia%20Gonski&entry.1292438233=%20%20Small%20satellite%20technologies%20have%20enhanced%20the%20potential%20and%20feasibility%20of%0Ageodesic%20missions%2C%20through%20simplification%20of%20design%20and%20decreased%20costs%0Aallowing%20for%20more%20frequent%20launches.%20On-satellite%20data%20acquisition%20systems%20can%0Abenefit%20from%20the%20implementation%20of%20machine%20learning%20%28ML%29%2C%20for%20better%0Aperformance%20and%20greater%20efficiency%20on%20tasks%20such%20as%20image%20processing%20or%20feature%0Aextraction.%20This%20work%20presents%20convolutional%20autoencoders%20for%20implementation%20on%0Athe%20payload%20of%20small%20satellites%2C%20designed%20to%20achieve%20dual%20functionality%20of%20data%0Acompression%20for%20more%20efficient%20off-satellite%20transmission%2C%20and%20at-source%0Aanomaly%20detection%20to%20inform%20satellite%20data-taking.%20This%20capability%20is%0Ademonstrated%20for%20a%20use%20case%20of%20disaster%20monitoring%20using%20aerial%20image%20datasets%0Aof%20the%20African%20continent%2C%20offering%20avenues%20for%20both%20novel%20ML-based%20approaches%0Ain%20small%20satellite%20applications%20along%20with%20the%20expansion%20of%20space%20technology%0Aand%20artificial%20intelligence%20in%20Africa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00040v2&entry.124074799=Read"},
{"title": "DFed-SST: Building Semantic- and Structure-aware Topologies for\n  Decentralized Federated Graph Learning", "author": "Lianshuai Guo and Zhongzheng Yuan and Xunkai Li and Yinlin Zhu and Meixia Qu and Wenyu Wang", "abstract": "  Decentralized Federated Learning (DFL) has emerged as a robust distributed\nparadigm that circumvents the single-point-of-failure and communication\nbottleneck risks of centralized architectures. However, a significant challenge\narises as existing DFL optimization strategies, primarily designed for tasks\nsuch as computer vision, fail to address the unique topological information\ninherent in the local subgraph. Notably, while Federated Graph Learning (FGL)\nis tailored for graph data, it is predominantly implemented in a centralized\nserver-client model, failing to leverage the benefits of decentralization.To\nbridge this gap, we propose DFed-SST, a decentralized federated graph learning\nframework with adaptive communication. The core of our method is a\ndual-topology adaptive communication mechanism that leverages the unique\ntopological features of each client's local subgraph to dynamically construct\nand optimize the inter-client communication topology. This allows our framework\nto guide model aggregation efficiently in the face of heterogeneity. Extensive\nexperiments on eight real-world datasets consistently demonstrate the\nsuperiority of DFed-SST, achieving 3.26% improvement in average accuracy over\nbaseline methods.\n", "link": "http://arxiv.org/abs/2508.11530v1", "date": "2025-08-15", "relevancy": 2.432, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4962}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4824}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DFed-SST%3A%20Building%20Semantic-%20and%20Structure-aware%20Topologies%20for%0A%20%20Decentralized%20Federated%20Graph%20Learning&body=Title%3A%20DFed-SST%3A%20Building%20Semantic-%20and%20Structure-aware%20Topologies%20for%0A%20%20Decentralized%20Federated%20Graph%20Learning%0AAuthor%3A%20Lianshuai%20Guo%20and%20Zhongzheng%20Yuan%20and%20Xunkai%20Li%20and%20Yinlin%20Zhu%20and%20Meixia%20Qu%20and%20Wenyu%20Wang%0AAbstract%3A%20%20%20Decentralized%20Federated%20Learning%20%28DFL%29%20has%20emerged%20as%20a%20robust%20distributed%0Aparadigm%20that%20circumvents%20the%20single-point-of-failure%20and%20communication%0Abottleneck%20risks%20of%20centralized%20architectures.%20However%2C%20a%20significant%20challenge%0Aarises%20as%20existing%20DFL%20optimization%20strategies%2C%20primarily%20designed%20for%20tasks%0Asuch%20as%20computer%20vision%2C%20fail%20to%20address%20the%20unique%20topological%20information%0Ainherent%20in%20the%20local%20subgraph.%20Notably%2C%20while%20Federated%20Graph%20Learning%20%28FGL%29%0Ais%20tailored%20for%20graph%20data%2C%20it%20is%20predominantly%20implemented%20in%20a%20centralized%0Aserver-client%20model%2C%20failing%20to%20leverage%20the%20benefits%20of%20decentralization.To%0Abridge%20this%20gap%2C%20we%20propose%20DFed-SST%2C%20a%20decentralized%20federated%20graph%20learning%0Aframework%20with%20adaptive%20communication.%20The%20core%20of%20our%20method%20is%20a%0Adual-topology%20adaptive%20communication%20mechanism%20that%20leverages%20the%20unique%0Atopological%20features%20of%20each%20client%27s%20local%20subgraph%20to%20dynamically%20construct%0Aand%20optimize%20the%20inter-client%20communication%20topology.%20This%20allows%20our%20framework%0Ato%20guide%20model%20aggregation%20efficiently%20in%20the%20face%20of%20heterogeneity.%20Extensive%0Aexperiments%20on%20eight%20real-world%20datasets%20consistently%20demonstrate%20the%0Asuperiority%20of%20DFed-SST%2C%20achieving%203.26%25%20improvement%20in%20average%20accuracy%20over%0Abaseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDFed-SST%253A%2520Building%2520Semantic-%2520and%2520Structure-aware%2520Topologies%2520for%250A%2520%2520Decentralized%2520Federated%2520Graph%2520Learning%26entry.906535625%3DLianshuai%2520Guo%2520and%2520Zhongzheng%2520Yuan%2520and%2520Xunkai%2520Li%2520and%2520Yinlin%2520Zhu%2520and%2520Meixia%2520Qu%2520and%2520Wenyu%2520Wang%26entry.1292438233%3D%2520%2520Decentralized%2520Federated%2520Learning%2520%2528DFL%2529%2520has%2520emerged%2520as%2520a%2520robust%2520distributed%250Aparadigm%2520that%2520circumvents%2520the%2520single-point-of-failure%2520and%2520communication%250Abottleneck%2520risks%2520of%2520centralized%2520architectures.%2520However%252C%2520a%2520significant%2520challenge%250Aarises%2520as%2520existing%2520DFL%2520optimization%2520strategies%252C%2520primarily%2520designed%2520for%2520tasks%250Asuch%2520as%2520computer%2520vision%252C%2520fail%2520to%2520address%2520the%2520unique%2520topological%2520information%250Ainherent%2520in%2520the%2520local%2520subgraph.%2520Notably%252C%2520while%2520Federated%2520Graph%2520Learning%2520%2528FGL%2529%250Ais%2520tailored%2520for%2520graph%2520data%252C%2520it%2520is%2520predominantly%2520implemented%2520in%2520a%2520centralized%250Aserver-client%2520model%252C%2520failing%2520to%2520leverage%2520the%2520benefits%2520of%2520decentralization.To%250Abridge%2520this%2520gap%252C%2520we%2520propose%2520DFed-SST%252C%2520a%2520decentralized%2520federated%2520graph%2520learning%250Aframework%2520with%2520adaptive%2520communication.%2520The%2520core%2520of%2520our%2520method%2520is%2520a%250Adual-topology%2520adaptive%2520communication%2520mechanism%2520that%2520leverages%2520the%2520unique%250Atopological%2520features%2520of%2520each%2520client%2527s%2520local%2520subgraph%2520to%2520dynamically%2520construct%250Aand%2520optimize%2520the%2520inter-client%2520communication%2520topology.%2520This%2520allows%2520our%2520framework%250Ato%2520guide%2520model%2520aggregation%2520efficiently%2520in%2520the%2520face%2520of%2520heterogeneity.%2520Extensive%250Aexperiments%2520on%2520eight%2520real-world%2520datasets%2520consistently%2520demonstrate%2520the%250Asuperiority%2520of%2520DFed-SST%252C%2520achieving%25203.26%2525%2520improvement%2520in%2520average%2520accuracy%2520over%250Abaseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DFed-SST%3A%20Building%20Semantic-%20and%20Structure-aware%20Topologies%20for%0A%20%20Decentralized%20Federated%20Graph%20Learning&entry.906535625=Lianshuai%20Guo%20and%20Zhongzheng%20Yuan%20and%20Xunkai%20Li%20and%20Yinlin%20Zhu%20and%20Meixia%20Qu%20and%20Wenyu%20Wang&entry.1292438233=%20%20Decentralized%20Federated%20Learning%20%28DFL%29%20has%20emerged%20as%20a%20robust%20distributed%0Aparadigm%20that%20circumvents%20the%20single-point-of-failure%20and%20communication%0Abottleneck%20risks%20of%20centralized%20architectures.%20However%2C%20a%20significant%20challenge%0Aarises%20as%20existing%20DFL%20optimization%20strategies%2C%20primarily%20designed%20for%20tasks%0Asuch%20as%20computer%20vision%2C%20fail%20to%20address%20the%20unique%20topological%20information%0Ainherent%20in%20the%20local%20subgraph.%20Notably%2C%20while%20Federated%20Graph%20Learning%20%28FGL%29%0Ais%20tailored%20for%20graph%20data%2C%20it%20is%20predominantly%20implemented%20in%20a%20centralized%0Aserver-client%20model%2C%20failing%20to%20leverage%20the%20benefits%20of%20decentralization.To%0Abridge%20this%20gap%2C%20we%20propose%20DFed-SST%2C%20a%20decentralized%20federated%20graph%20learning%0Aframework%20with%20adaptive%20communication.%20The%20core%20of%20our%20method%20is%20a%0Adual-topology%20adaptive%20communication%20mechanism%20that%20leverages%20the%20unique%0Atopological%20features%20of%20each%20client%27s%20local%20subgraph%20to%20dynamically%20construct%0Aand%20optimize%20the%20inter-client%20communication%20topology.%20This%20allows%20our%20framework%0Ato%20guide%20model%20aggregation%20efficiently%20in%20the%20face%20of%20heterogeneity.%20Extensive%0Aexperiments%20on%20eight%20real-world%20datasets%20consistently%20demonstrate%20the%0Asuperiority%20of%20DFed-SST%2C%20achieving%203.26%25%20improvement%20in%20average%20accuracy%20over%0Abaseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11530v1&entry.124074799=Read"},
{"title": "Does the Skeleton-Recall Loss Really Work?", "author": "Devansh Arora and Nitin Kumar and Sukrit Gupta", "abstract": "  Image segmentation is an important and widely performed task in computer\nvision. Accomplishing effective image segmentation in diverse settings often\nrequires custom model architectures and loss functions. A set of models that\nspecialize in segmenting thin tubular structures are topology\npreservation-based loss functions. These models often utilize a pixel\nskeletonization process claimed to generate more precise segmentation masks of\nthin tubes and better capture the structures that other models often miss. One\nsuch model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\\cite\n{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark\ntubular datasets. In this work, we performed a theoretical analysis of the\ngradients for the SRL loss. Upon comparing the performance of the proposed\nmethod on some of the tubular datasets (used in the original work, along with\nsome additional datasets), we found that the performance of SRL-based\nsegmentation models did not exceed traditional baseline models. By providing\nboth a theoretical explanation and empirical evidence, this work critically\nevaluates the limitations of topology-based loss functions, offering valuable\ninsights for researchers aiming to develop more effective segmentation models\nfor complex tubular structures.\n", "link": "http://arxiv.org/abs/2508.11374v1", "date": "2025-08-15", "relevancy": 2.4153, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4841}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20the%20Skeleton-Recall%20Loss%20Really%20Work%3F&body=Title%3A%20Does%20the%20Skeleton-Recall%20Loss%20Really%20Work%3F%0AAuthor%3A%20Devansh%20Arora%20and%20Nitin%20Kumar%20and%20Sukrit%20Gupta%0AAbstract%3A%20%20%20Image%20segmentation%20is%20an%20important%20and%20widely%20performed%20task%20in%20computer%0Avision.%20Accomplishing%20effective%20image%20segmentation%20in%20diverse%20settings%20often%0Arequires%20custom%20model%20architectures%20and%20loss%20functions.%20A%20set%20of%20models%20that%0Aspecialize%20in%20segmenting%20thin%20tubular%20structures%20are%20topology%0Apreservation-based%20loss%20functions.%20These%20models%20often%20utilize%20a%20pixel%0Askeletonization%20process%20claimed%20to%20generate%20more%20precise%20segmentation%20masks%20of%0Athin%20tubes%20and%20better%20capture%20the%20structures%20that%20other%20models%20often%20miss.%20One%0Asuch%20model%2C%20Skeleton%20Recall%20Loss%20%28SRL%29%20proposed%20by%20Kirchhoff%20et%20al.~%5Ccite%0A%7Bkirchhoff2024srl%7D%2C%20was%20stated%20to%20produce%20state-of-the-art%20results%20on%20benchmark%0Atubular%20datasets.%20In%20this%20work%2C%20we%20performed%20a%20theoretical%20analysis%20of%20the%0Agradients%20for%20the%20SRL%20loss.%20Upon%20comparing%20the%20performance%20of%20the%20proposed%0Amethod%20on%20some%20of%20the%20tubular%20datasets%20%28used%20in%20the%20original%20work%2C%20along%20with%0Asome%20additional%20datasets%29%2C%20we%20found%20that%20the%20performance%20of%20SRL-based%0Asegmentation%20models%20did%20not%20exceed%20traditional%20baseline%20models.%20By%20providing%0Aboth%20a%20theoretical%20explanation%20and%20empirical%20evidence%2C%20this%20work%20critically%0Aevaluates%20the%20limitations%20of%20topology-based%20loss%20functions%2C%20offering%20valuable%0Ainsights%20for%20researchers%20aiming%20to%20develop%20more%20effective%20segmentation%20models%0Afor%20complex%20tubular%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520the%2520Skeleton-Recall%2520Loss%2520Really%2520Work%253F%26entry.906535625%3DDevansh%2520Arora%2520and%2520Nitin%2520Kumar%2520and%2520Sukrit%2520Gupta%26entry.1292438233%3D%2520%2520Image%2520segmentation%2520is%2520an%2520important%2520and%2520widely%2520performed%2520task%2520in%2520computer%250Avision.%2520Accomplishing%2520effective%2520image%2520segmentation%2520in%2520diverse%2520settings%2520often%250Arequires%2520custom%2520model%2520architectures%2520and%2520loss%2520functions.%2520A%2520set%2520of%2520models%2520that%250Aspecialize%2520in%2520segmenting%2520thin%2520tubular%2520structures%2520are%2520topology%250Apreservation-based%2520loss%2520functions.%2520These%2520models%2520often%2520utilize%2520a%2520pixel%250Askeletonization%2520process%2520claimed%2520to%2520generate%2520more%2520precise%2520segmentation%2520masks%2520of%250Athin%2520tubes%2520and%2520better%2520capture%2520the%2520structures%2520that%2520other%2520models%2520often%2520miss.%2520One%250Asuch%2520model%252C%2520Skeleton%2520Recall%2520Loss%2520%2528SRL%2529%2520proposed%2520by%2520Kirchhoff%2520et%2520al.~%255Ccite%250A%257Bkirchhoff2024srl%257D%252C%2520was%2520stated%2520to%2520produce%2520state-of-the-art%2520results%2520on%2520benchmark%250Atubular%2520datasets.%2520In%2520this%2520work%252C%2520we%2520performed%2520a%2520theoretical%2520analysis%2520of%2520the%250Agradients%2520for%2520the%2520SRL%2520loss.%2520Upon%2520comparing%2520the%2520performance%2520of%2520the%2520proposed%250Amethod%2520on%2520some%2520of%2520the%2520tubular%2520datasets%2520%2528used%2520in%2520the%2520original%2520work%252C%2520along%2520with%250Asome%2520additional%2520datasets%2529%252C%2520we%2520found%2520that%2520the%2520performance%2520of%2520SRL-based%250Asegmentation%2520models%2520did%2520not%2520exceed%2520traditional%2520baseline%2520models.%2520By%2520providing%250Aboth%2520a%2520theoretical%2520explanation%2520and%2520empirical%2520evidence%252C%2520this%2520work%2520critically%250Aevaluates%2520the%2520limitations%2520of%2520topology-based%2520loss%2520functions%252C%2520offering%2520valuable%250Ainsights%2520for%2520researchers%2520aiming%2520to%2520develop%2520more%2520effective%2520segmentation%2520models%250Afor%2520complex%2520tubular%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20the%20Skeleton-Recall%20Loss%20Really%20Work%3F&entry.906535625=Devansh%20Arora%20and%20Nitin%20Kumar%20and%20Sukrit%20Gupta&entry.1292438233=%20%20Image%20segmentation%20is%20an%20important%20and%20widely%20performed%20task%20in%20computer%0Avision.%20Accomplishing%20effective%20image%20segmentation%20in%20diverse%20settings%20often%0Arequires%20custom%20model%20architectures%20and%20loss%20functions.%20A%20set%20of%20models%20that%0Aspecialize%20in%20segmenting%20thin%20tubular%20structures%20are%20topology%0Apreservation-based%20loss%20functions.%20These%20models%20often%20utilize%20a%20pixel%0Askeletonization%20process%20claimed%20to%20generate%20more%20precise%20segmentation%20masks%20of%0Athin%20tubes%20and%20better%20capture%20the%20structures%20that%20other%20models%20often%20miss.%20One%0Asuch%20model%2C%20Skeleton%20Recall%20Loss%20%28SRL%29%20proposed%20by%20Kirchhoff%20et%20al.~%5Ccite%0A%7Bkirchhoff2024srl%7D%2C%20was%20stated%20to%20produce%20state-of-the-art%20results%20on%20benchmark%0Atubular%20datasets.%20In%20this%20work%2C%20we%20performed%20a%20theoretical%20analysis%20of%20the%0Agradients%20for%20the%20SRL%20loss.%20Upon%20comparing%20the%20performance%20of%20the%20proposed%0Amethod%20on%20some%20of%20the%20tubular%20datasets%20%28used%20in%20the%20original%20work%2C%20along%20with%0Asome%20additional%20datasets%29%2C%20we%20found%20that%20the%20performance%20of%20SRL-based%0Asegmentation%20models%20did%20not%20exceed%20traditional%20baseline%20models.%20By%20providing%0Aboth%20a%20theoretical%20explanation%20and%20empirical%20evidence%2C%20this%20work%20critically%0Aevaluates%20the%20limitations%20of%20topology-based%20loss%20functions%2C%20offering%20valuable%0Ainsights%20for%20researchers%20aiming%20to%20develop%20more%20effective%20segmentation%20models%0Afor%20complex%20tubular%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11374v1&entry.124074799=Read"},
{"title": "Discovering Invariant Neighborhood Patterns for Heterophilic Graphs", "author": "Jinluan Yang and Ruihao Zhang and Zhengyu Chen and Teng Xiao and Yueyang Wang and Fei Wu and Kun Kuang", "abstract": "  This paper studies the problem of distribution shifts on non-homophilous\ngraphs Mosting existing graph neural network methods rely on the homophilous\nassumption that nodes from the same class are more likely to be linked.\nHowever, such assumptions of homophily do not always hold in real-world graphs,\nwhich leads to more complex distribution shifts unaccounted for in previous\nmethods. The distribution shifts of neighborhood patterns are much more diverse\non non-homophilous graphs. We propose a novel Invariant Neighborhood Pattern\nLearning (INPL) to alleviate the distribution shifts problem on non-homophilous\ngraphs. Specifically, we propose the Adaptive Neighborhood Propagation (ANP)\nmodule to capture the adaptive neighborhood information, which could alleviate\nthe neighborhood pattern distribution shifts problem on non-homophilous graphs.\nWe propose Invariant Non-Homophilous Graph Learning (INHGL) module to constrain\nthe ANP and learn invariant graph representation on non-homophilous graphs.\nExtensive experimental results on real-world non-homophilous graphs show that\nINPL could achieve state-of-the-art performance for learning on large\nnon-homophilous graphs.\n", "link": "http://arxiv.org/abs/2403.10572v2", "date": "2025-08-15", "relevancy": 2.4029, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4887}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4806}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Invariant%20Neighborhood%20Patterns%20for%20Heterophilic%20Graphs&body=Title%3A%20Discovering%20Invariant%20Neighborhood%20Patterns%20for%20Heterophilic%20Graphs%0AAuthor%3A%20Jinluan%20Yang%20and%20Ruihao%20Zhang%20and%20Zhengyu%20Chen%20and%20Teng%20Xiao%20and%20Yueyang%20Wang%20and%20Fei%20Wu%20and%20Kun%20Kuang%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20problem%20of%20distribution%20shifts%20on%20non-homophilous%0Agraphs%20Mosting%20existing%20graph%20neural%20network%20methods%20rely%20on%20the%20homophilous%0Aassumption%20that%20nodes%20from%20the%20same%20class%20are%20more%20likely%20to%20be%20linked.%0AHowever%2C%20such%20assumptions%20of%20homophily%20do%20not%20always%20hold%20in%20real-world%20graphs%2C%0Awhich%20leads%20to%20more%20complex%20distribution%20shifts%20unaccounted%20for%20in%20previous%0Amethods.%20The%20distribution%20shifts%20of%20neighborhood%20patterns%20are%20much%20more%20diverse%0Aon%20non-homophilous%20graphs.%20We%20propose%20a%20novel%20Invariant%20Neighborhood%20Pattern%0ALearning%20%28INPL%29%20to%20alleviate%20the%20distribution%20shifts%20problem%20on%20non-homophilous%0Agraphs.%20Specifically%2C%20we%20propose%20the%20Adaptive%20Neighborhood%20Propagation%20%28ANP%29%0Amodule%20to%20capture%20the%20adaptive%20neighborhood%20information%2C%20which%20could%20alleviate%0Athe%20neighborhood%20pattern%20distribution%20shifts%20problem%20on%20non-homophilous%20graphs.%0AWe%20propose%20Invariant%20Non-Homophilous%20Graph%20Learning%20%28INHGL%29%20module%20to%20constrain%0Athe%20ANP%20and%20learn%20invariant%20graph%20representation%20on%20non-homophilous%20graphs.%0AExtensive%20experimental%20results%20on%20real-world%20non-homophilous%20graphs%20show%20that%0AINPL%20could%20achieve%20state-of-the-art%20performance%20for%20learning%20on%20large%0Anon-homophilous%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Invariant%2520Neighborhood%2520Patterns%2520for%2520Heterophilic%2520Graphs%26entry.906535625%3DJinluan%2520Yang%2520and%2520Ruihao%2520Zhang%2520and%2520Zhengyu%2520Chen%2520and%2520Teng%2520Xiao%2520and%2520Yueyang%2520Wang%2520and%2520Fei%2520Wu%2520and%2520Kun%2520Kuang%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520problem%2520of%2520distribution%2520shifts%2520on%2520non-homophilous%250Agraphs%2520Mosting%2520existing%2520graph%2520neural%2520network%2520methods%2520rely%2520on%2520the%2520homophilous%250Aassumption%2520that%2520nodes%2520from%2520the%2520same%2520class%2520are%2520more%2520likely%2520to%2520be%2520linked.%250AHowever%252C%2520such%2520assumptions%2520of%2520homophily%2520do%2520not%2520always%2520hold%2520in%2520real-world%2520graphs%252C%250Awhich%2520leads%2520to%2520more%2520complex%2520distribution%2520shifts%2520unaccounted%2520for%2520in%2520previous%250Amethods.%2520The%2520distribution%2520shifts%2520of%2520neighborhood%2520patterns%2520are%2520much%2520more%2520diverse%250Aon%2520non-homophilous%2520graphs.%2520We%2520propose%2520a%2520novel%2520Invariant%2520Neighborhood%2520Pattern%250ALearning%2520%2528INPL%2529%2520to%2520alleviate%2520the%2520distribution%2520shifts%2520problem%2520on%2520non-homophilous%250Agraphs.%2520Specifically%252C%2520we%2520propose%2520the%2520Adaptive%2520Neighborhood%2520Propagation%2520%2528ANP%2529%250Amodule%2520to%2520capture%2520the%2520adaptive%2520neighborhood%2520information%252C%2520which%2520could%2520alleviate%250Athe%2520neighborhood%2520pattern%2520distribution%2520shifts%2520problem%2520on%2520non-homophilous%2520graphs.%250AWe%2520propose%2520Invariant%2520Non-Homophilous%2520Graph%2520Learning%2520%2528INHGL%2529%2520module%2520to%2520constrain%250Athe%2520ANP%2520and%2520learn%2520invariant%2520graph%2520representation%2520on%2520non-homophilous%2520graphs.%250AExtensive%2520experimental%2520results%2520on%2520real-world%2520non-homophilous%2520graphs%2520show%2520that%250AINPL%2520could%2520achieve%2520state-of-the-art%2520performance%2520for%2520learning%2520on%2520large%250Anon-homophilous%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Invariant%20Neighborhood%20Patterns%20for%20Heterophilic%20Graphs&entry.906535625=Jinluan%20Yang%20and%20Ruihao%20Zhang%20and%20Zhengyu%20Chen%20and%20Teng%20Xiao%20and%20Yueyang%20Wang%20and%20Fei%20Wu%20and%20Kun%20Kuang&entry.1292438233=%20%20This%20paper%20studies%20the%20problem%20of%20distribution%20shifts%20on%20non-homophilous%0Agraphs%20Mosting%20existing%20graph%20neural%20network%20methods%20rely%20on%20the%20homophilous%0Aassumption%20that%20nodes%20from%20the%20same%20class%20are%20more%20likely%20to%20be%20linked.%0AHowever%2C%20such%20assumptions%20of%20homophily%20do%20not%20always%20hold%20in%20real-world%20graphs%2C%0Awhich%20leads%20to%20more%20complex%20distribution%20shifts%20unaccounted%20for%20in%20previous%0Amethods.%20The%20distribution%20shifts%20of%20neighborhood%20patterns%20are%20much%20more%20diverse%0Aon%20non-homophilous%20graphs.%20We%20propose%20a%20novel%20Invariant%20Neighborhood%20Pattern%0ALearning%20%28INPL%29%20to%20alleviate%20the%20distribution%20shifts%20problem%20on%20non-homophilous%0Agraphs.%20Specifically%2C%20we%20propose%20the%20Adaptive%20Neighborhood%20Propagation%20%28ANP%29%0Amodule%20to%20capture%20the%20adaptive%20neighborhood%20information%2C%20which%20could%20alleviate%0Athe%20neighborhood%20pattern%20distribution%20shifts%20problem%20on%20non-homophilous%20graphs.%0AWe%20propose%20Invariant%20Non-Homophilous%20Graph%20Learning%20%28INHGL%29%20module%20to%20constrain%0Athe%20ANP%20and%20learn%20invariant%20graph%20representation%20on%20non-homophilous%20graphs.%0AExtensive%20experimental%20results%20on%20real-world%20non-homophilous%20graphs%20show%20that%0AINPL%20could%20achieve%20state-of-the-art%20performance%20for%20learning%20on%20large%0Anon-homophilous%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10572v2&entry.124074799=Read"},
{"title": "Towards Faithful Class-level Self-explainability in Graph Neural\n  Networks by Subgraph Dependencies", "author": "Fanzhen Liu and Xiaoxiao Ma and Jian Yang and Alsharif Abuadbba and Kristen Moore and Surya Nepal and Cecile Paris and Quan Z. Sheng and Jia Wu", "abstract": "  Enhancing the interpretability of graph neural networks (GNNs) is crucial to\nensure their safe and fair deployment. Recent work has introduced\nself-explainable GNNs that generate explanations as part of training, improving\nboth faithfulness and efficiency. Some of these models, such as ProtGNN and\nPGIB, learn class-specific prototypes, offering a potential pathway toward\nclass-level explanations. However, their evaluations focus solely on\ninstance-level explanations, leaving open the question of whether these\nprototypes meaningfully generalize across instances of the same class. In this\npaper, we introduce GraphOracle, a novel self-explainable GNN framework\ndesigned to generate and evaluate class-level explanations for GNNs. Our model\njointly learns a GNN classifier and a set of structured, sparse subgraphs that\nare discriminative for each class. We propose a novel integrated training that\ncaptures graph$\\unicode{x2013}$subgraph$\\unicode{x2013}$prediction dependencies\nefficiently and faithfully, validated through a masking-based evaluation\nstrategy. This strategy enables us to retroactively assess whether prior\nmethods like ProtGNN and PGIB deliver effective class-level explanations. Our\nresults show that they do not. In contrast, GraphOracle achieves superior\nfidelity, explainability, and scalability across a range of graph\nclassification tasks. We further demonstrate that GraphOracle avoids the\ncomputational bottlenecks of previous methods$\\unicode{x2014}$like Monte Carlo\nTree Search$\\unicode{x2014}$by using entropy-regularized subgraph selection and\nlightweight random walk extraction, enabling faster and more scalable training.\nThese findings position GraphOracle as a practical and principled solution for\nfaithful class-level self-explainability in GNNs.\n", "link": "http://arxiv.org/abs/2508.11513v1", "date": "2025-08-15", "relevancy": 2.4009, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4957}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4794}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Faithful%20Class-level%20Self-explainability%20in%20Graph%20Neural%0A%20%20Networks%20by%20Subgraph%20Dependencies&body=Title%3A%20Towards%20Faithful%20Class-level%20Self-explainability%20in%20Graph%20Neural%0A%20%20Networks%20by%20Subgraph%20Dependencies%0AAuthor%3A%20Fanzhen%20Liu%20and%20Xiaoxiao%20Ma%20and%20Jian%20Yang%20and%20Alsharif%20Abuadbba%20and%20Kristen%20Moore%20and%20Surya%20Nepal%20and%20Cecile%20Paris%20and%20Quan%20Z.%20Sheng%20and%20Jia%20Wu%0AAbstract%3A%20%20%20Enhancing%20the%20interpretability%20of%20graph%20neural%20networks%20%28GNNs%29%20is%20crucial%20to%0Aensure%20their%20safe%20and%20fair%20deployment.%20Recent%20work%20has%20introduced%0Aself-explainable%20GNNs%20that%20generate%20explanations%20as%20part%20of%20training%2C%20improving%0Aboth%20faithfulness%20and%20efficiency.%20Some%20of%20these%20models%2C%20such%20as%20ProtGNN%20and%0APGIB%2C%20learn%20class-specific%20prototypes%2C%20offering%20a%20potential%20pathway%20toward%0Aclass-level%20explanations.%20However%2C%20their%20evaluations%20focus%20solely%20on%0Ainstance-level%20explanations%2C%20leaving%20open%20the%20question%20of%20whether%20these%0Aprototypes%20meaningfully%20generalize%20across%20instances%20of%20the%20same%20class.%20In%20this%0Apaper%2C%20we%20introduce%20GraphOracle%2C%20a%20novel%20self-explainable%20GNN%20framework%0Adesigned%20to%20generate%20and%20evaluate%20class-level%20explanations%20for%20GNNs.%20Our%20model%0Ajointly%20learns%20a%20GNN%20classifier%20and%20a%20set%20of%20structured%2C%20sparse%20subgraphs%20that%0Aare%20discriminative%20for%20each%20class.%20We%20propose%20a%20novel%20integrated%20training%20that%0Acaptures%20graph%24%5Cunicode%7Bx2013%7D%24subgraph%24%5Cunicode%7Bx2013%7D%24prediction%20dependencies%0Aefficiently%20and%20faithfully%2C%20validated%20through%20a%20masking-based%20evaluation%0Astrategy.%20This%20strategy%20enables%20us%20to%20retroactively%20assess%20whether%20prior%0Amethods%20like%20ProtGNN%20and%20PGIB%20deliver%20effective%20class-level%20explanations.%20Our%0Aresults%20show%20that%20they%20do%20not.%20In%20contrast%2C%20GraphOracle%20achieves%20superior%0Afidelity%2C%20explainability%2C%20and%20scalability%20across%20a%20range%20of%20graph%0Aclassification%20tasks.%20We%20further%20demonstrate%20that%20GraphOracle%20avoids%20the%0Acomputational%20bottlenecks%20of%20previous%20methods%24%5Cunicode%7Bx2014%7D%24like%20Monte%20Carlo%0ATree%20Search%24%5Cunicode%7Bx2014%7D%24by%20using%20entropy-regularized%20subgraph%20selection%20and%0Alightweight%20random%20walk%20extraction%2C%20enabling%20faster%20and%20more%20scalable%20training.%0AThese%20findings%20position%20GraphOracle%20as%20a%20practical%20and%20principled%20solution%20for%0Afaithful%20class-level%20self-explainability%20in%20GNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Faithful%2520Class-level%2520Self-explainability%2520in%2520Graph%2520Neural%250A%2520%2520Networks%2520by%2520Subgraph%2520Dependencies%26entry.906535625%3DFanzhen%2520Liu%2520and%2520Xiaoxiao%2520Ma%2520and%2520Jian%2520Yang%2520and%2520Alsharif%2520Abuadbba%2520and%2520Kristen%2520Moore%2520and%2520Surya%2520Nepal%2520and%2520Cecile%2520Paris%2520and%2520Quan%2520Z.%2520Sheng%2520and%2520Jia%2520Wu%26entry.1292438233%3D%2520%2520Enhancing%2520the%2520interpretability%2520of%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520is%2520crucial%2520to%250Aensure%2520their%2520safe%2520and%2520fair%2520deployment.%2520Recent%2520work%2520has%2520introduced%250Aself-explainable%2520GNNs%2520that%2520generate%2520explanations%2520as%2520part%2520of%2520training%252C%2520improving%250Aboth%2520faithfulness%2520and%2520efficiency.%2520Some%2520of%2520these%2520models%252C%2520such%2520as%2520ProtGNN%2520and%250APGIB%252C%2520learn%2520class-specific%2520prototypes%252C%2520offering%2520a%2520potential%2520pathway%2520toward%250Aclass-level%2520explanations.%2520However%252C%2520their%2520evaluations%2520focus%2520solely%2520on%250Ainstance-level%2520explanations%252C%2520leaving%2520open%2520the%2520question%2520of%2520whether%2520these%250Aprototypes%2520meaningfully%2520generalize%2520across%2520instances%2520of%2520the%2520same%2520class.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520GraphOracle%252C%2520a%2520novel%2520self-explainable%2520GNN%2520framework%250Adesigned%2520to%2520generate%2520and%2520evaluate%2520class-level%2520explanations%2520for%2520GNNs.%2520Our%2520model%250Ajointly%2520learns%2520a%2520GNN%2520classifier%2520and%2520a%2520set%2520of%2520structured%252C%2520sparse%2520subgraphs%2520that%250Aare%2520discriminative%2520for%2520each%2520class.%2520We%2520propose%2520a%2520novel%2520integrated%2520training%2520that%250Acaptures%2520graph%2524%255Cunicode%257Bx2013%257D%2524subgraph%2524%255Cunicode%257Bx2013%257D%2524prediction%2520dependencies%250Aefficiently%2520and%2520faithfully%252C%2520validated%2520through%2520a%2520masking-based%2520evaluation%250Astrategy.%2520This%2520strategy%2520enables%2520us%2520to%2520retroactively%2520assess%2520whether%2520prior%250Amethods%2520like%2520ProtGNN%2520and%2520PGIB%2520deliver%2520effective%2520class-level%2520explanations.%2520Our%250Aresults%2520show%2520that%2520they%2520do%2520not.%2520In%2520contrast%252C%2520GraphOracle%2520achieves%2520superior%250Afidelity%252C%2520explainability%252C%2520and%2520scalability%2520across%2520a%2520range%2520of%2520graph%250Aclassification%2520tasks.%2520We%2520further%2520demonstrate%2520that%2520GraphOracle%2520avoids%2520the%250Acomputational%2520bottlenecks%2520of%2520previous%2520methods%2524%255Cunicode%257Bx2014%257D%2524like%2520Monte%2520Carlo%250ATree%2520Search%2524%255Cunicode%257Bx2014%257D%2524by%2520using%2520entropy-regularized%2520subgraph%2520selection%2520and%250Alightweight%2520random%2520walk%2520extraction%252C%2520enabling%2520faster%2520and%2520more%2520scalable%2520training.%250AThese%2520findings%2520position%2520GraphOracle%2520as%2520a%2520practical%2520and%2520principled%2520solution%2520for%250Afaithful%2520class-level%2520self-explainability%2520in%2520GNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Faithful%20Class-level%20Self-explainability%20in%20Graph%20Neural%0A%20%20Networks%20by%20Subgraph%20Dependencies&entry.906535625=Fanzhen%20Liu%20and%20Xiaoxiao%20Ma%20and%20Jian%20Yang%20and%20Alsharif%20Abuadbba%20and%20Kristen%20Moore%20and%20Surya%20Nepal%20and%20Cecile%20Paris%20and%20Quan%20Z.%20Sheng%20and%20Jia%20Wu&entry.1292438233=%20%20Enhancing%20the%20interpretability%20of%20graph%20neural%20networks%20%28GNNs%29%20is%20crucial%20to%0Aensure%20their%20safe%20and%20fair%20deployment.%20Recent%20work%20has%20introduced%0Aself-explainable%20GNNs%20that%20generate%20explanations%20as%20part%20of%20training%2C%20improving%0Aboth%20faithfulness%20and%20efficiency.%20Some%20of%20these%20models%2C%20such%20as%20ProtGNN%20and%0APGIB%2C%20learn%20class-specific%20prototypes%2C%20offering%20a%20potential%20pathway%20toward%0Aclass-level%20explanations.%20However%2C%20their%20evaluations%20focus%20solely%20on%0Ainstance-level%20explanations%2C%20leaving%20open%20the%20question%20of%20whether%20these%0Aprototypes%20meaningfully%20generalize%20across%20instances%20of%20the%20same%20class.%20In%20this%0Apaper%2C%20we%20introduce%20GraphOracle%2C%20a%20novel%20self-explainable%20GNN%20framework%0Adesigned%20to%20generate%20and%20evaluate%20class-level%20explanations%20for%20GNNs.%20Our%20model%0Ajointly%20learns%20a%20GNN%20classifier%20and%20a%20set%20of%20structured%2C%20sparse%20subgraphs%20that%0Aare%20discriminative%20for%20each%20class.%20We%20propose%20a%20novel%20integrated%20training%20that%0Acaptures%20graph%24%5Cunicode%7Bx2013%7D%24subgraph%24%5Cunicode%7Bx2013%7D%24prediction%20dependencies%0Aefficiently%20and%20faithfully%2C%20validated%20through%20a%20masking-based%20evaluation%0Astrategy.%20This%20strategy%20enables%20us%20to%20retroactively%20assess%20whether%20prior%0Amethods%20like%20ProtGNN%20and%20PGIB%20deliver%20effective%20class-level%20explanations.%20Our%0Aresults%20show%20that%20they%20do%20not.%20In%20contrast%2C%20GraphOracle%20achieves%20superior%0Afidelity%2C%20explainability%2C%20and%20scalability%20across%20a%20range%20of%20graph%0Aclassification%20tasks.%20We%20further%20demonstrate%20that%20GraphOracle%20avoids%20the%0Acomputational%20bottlenecks%20of%20previous%20methods%24%5Cunicode%7Bx2014%7D%24like%20Monte%20Carlo%0ATree%20Search%24%5Cunicode%7Bx2014%7D%24by%20using%20entropy-regularized%20subgraph%20selection%20and%0Alightweight%20random%20walk%20extraction%2C%20enabling%20faster%20and%20more%20scalable%20training.%0AThese%20findings%20position%20GraphOracle%20as%20a%20practical%20and%20principled%20solution%20for%0Afaithful%20class-level%20self-explainability%20in%20GNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11513v1&entry.124074799=Read"},
{"title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks", "author": "Zonglin Wu and Yule Xue and Yaoyao Feng and Xiaolong Wang and Yiren Song", "abstract": "  As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online.\n", "link": "http://arxiv.org/abs/2506.05982v4", "date": "2025-08-15", "relevancy": 2.3881, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4782}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4782}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCA-Bench%3A%20A%20Multimodal%20Benchmark%20for%20Evaluating%20CAPTCHA%20Robustness%0A%20%20Against%20VLM-based%20Attacks&body=Title%3A%20MCA-Bench%3A%20A%20Multimodal%20Benchmark%20for%20Evaluating%20CAPTCHA%20Robustness%0A%20%20Against%20VLM-based%20Attacks%0AAuthor%3A%20Zonglin%20Wu%20and%20Yule%20Xue%20and%20Yaoyao%20Feng%20and%20Xiaolong%20Wang%20and%20Yiren%20Song%0AAbstract%3A%20%20%20As%20automated%20attack%20techniques%20rapidly%20advance%2C%20CAPTCHAs%20remain%20a%20critical%0Adefense%20mechanism%20against%20malicious%20bots.%20However%2C%20existing%20CAPTCHA%20schemes%0Aencompass%20a%20diverse%20range%20of%20modalities%20--%20from%20static%20distorted%20text%20and%0Aobfuscated%20images%20to%20interactive%20clicks%2C%20sliding%20puzzles%2C%20and%20logic-based%0Aquestions%20--%20yet%20the%20community%20still%20lacks%20a%20unified%2C%20large-scale%2C%20multimodal%0Abenchmark%20to%20rigorously%20evaluate%20their%20security%20robustness.%20To%20address%20this%0Agap%2C%20we%20introduce%20MCA-Bench%2C%20a%20comprehensive%20and%20reproducible%20benchmarking%0Asuite%20that%20integrates%20heterogeneous%20CAPTCHA%20types%20into%20a%20single%20evaluation%0Aprotocol.%20Leveraging%20a%20shared%20vision-language%20model%20backbone%2C%20we%20fine-tune%0Aspecialized%20cracking%20agents%20for%20each%20CAPTCHA%20category%2C%20enabling%20consistent%2C%0Across-modal%20assessments.%20Extensive%20experiments%20reveal%20that%20MCA-Bench%0Aeffectively%20maps%20the%20vulnerability%20spectrum%20of%20modern%20CAPTCHA%20designs%20under%0Avaried%20attack%20settings%2C%20and%20crucially%20offers%20the%20first%20quantitative%20analysis%20of%0Ahow%20challenge%20complexity%2C%20interaction%20depth%2C%20and%20model%20solvability%20interrelate.%0ABased%20on%20these%20findings%2C%20we%20propose%20three%20actionable%20design%20principles%20and%0Aidentify%20key%20open%20challenges%2C%20laying%20the%20groundwork%20for%20systematic%20CAPTCHA%0Ahardening%2C%20fair%20benchmarking%2C%20and%20broader%20community%20collaboration.%20Datasets%20and%0Acode%20are%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05982v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCA-Bench%253A%2520A%2520Multimodal%2520Benchmark%2520for%2520Evaluating%2520CAPTCHA%2520Robustness%250A%2520%2520Against%2520VLM-based%2520Attacks%26entry.906535625%3DZonglin%2520Wu%2520and%2520Yule%2520Xue%2520and%2520Yaoyao%2520Feng%2520and%2520Xiaolong%2520Wang%2520and%2520Yiren%2520Song%26entry.1292438233%3D%2520%2520As%2520automated%2520attack%2520techniques%2520rapidly%2520advance%252C%2520CAPTCHAs%2520remain%2520a%2520critical%250Adefense%2520mechanism%2520against%2520malicious%2520bots.%2520However%252C%2520existing%2520CAPTCHA%2520schemes%250Aencompass%2520a%2520diverse%2520range%2520of%2520modalities%2520--%2520from%2520static%2520distorted%2520text%2520and%250Aobfuscated%2520images%2520to%2520interactive%2520clicks%252C%2520sliding%2520puzzles%252C%2520and%2520logic-based%250Aquestions%2520--%2520yet%2520the%2520community%2520still%2520lacks%2520a%2520unified%252C%2520large-scale%252C%2520multimodal%250Abenchmark%2520to%2520rigorously%2520evaluate%2520their%2520security%2520robustness.%2520To%2520address%2520this%250Agap%252C%2520we%2520introduce%2520MCA-Bench%252C%2520a%2520comprehensive%2520and%2520reproducible%2520benchmarking%250Asuite%2520that%2520integrates%2520heterogeneous%2520CAPTCHA%2520types%2520into%2520a%2520single%2520evaluation%250Aprotocol.%2520Leveraging%2520a%2520shared%2520vision-language%2520model%2520backbone%252C%2520we%2520fine-tune%250Aspecialized%2520cracking%2520agents%2520for%2520each%2520CAPTCHA%2520category%252C%2520enabling%2520consistent%252C%250Across-modal%2520assessments.%2520Extensive%2520experiments%2520reveal%2520that%2520MCA-Bench%250Aeffectively%2520maps%2520the%2520vulnerability%2520spectrum%2520of%2520modern%2520CAPTCHA%2520designs%2520under%250Avaried%2520attack%2520settings%252C%2520and%2520crucially%2520offers%2520the%2520first%2520quantitative%2520analysis%2520of%250Ahow%2520challenge%2520complexity%252C%2520interaction%2520depth%252C%2520and%2520model%2520solvability%2520interrelate.%250ABased%2520on%2520these%2520findings%252C%2520we%2520propose%2520three%2520actionable%2520design%2520principles%2520and%250Aidentify%2520key%2520open%2520challenges%252C%2520laying%2520the%2520groundwork%2520for%2520systematic%2520CAPTCHA%250Ahardening%252C%2520fair%2520benchmarking%252C%2520and%2520broader%2520community%2520collaboration.%2520Datasets%2520and%250Acode%2520are%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05982v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCA-Bench%3A%20A%20Multimodal%20Benchmark%20for%20Evaluating%20CAPTCHA%20Robustness%0A%20%20Against%20VLM-based%20Attacks&entry.906535625=Zonglin%20Wu%20and%20Yule%20Xue%20and%20Yaoyao%20Feng%20and%20Xiaolong%20Wang%20and%20Yiren%20Song&entry.1292438233=%20%20As%20automated%20attack%20techniques%20rapidly%20advance%2C%20CAPTCHAs%20remain%20a%20critical%0Adefense%20mechanism%20against%20malicious%20bots.%20However%2C%20existing%20CAPTCHA%20schemes%0Aencompass%20a%20diverse%20range%20of%20modalities%20--%20from%20static%20distorted%20text%20and%0Aobfuscated%20images%20to%20interactive%20clicks%2C%20sliding%20puzzles%2C%20and%20logic-based%0Aquestions%20--%20yet%20the%20community%20still%20lacks%20a%20unified%2C%20large-scale%2C%20multimodal%0Abenchmark%20to%20rigorously%20evaluate%20their%20security%20robustness.%20To%20address%20this%0Agap%2C%20we%20introduce%20MCA-Bench%2C%20a%20comprehensive%20and%20reproducible%20benchmarking%0Asuite%20that%20integrates%20heterogeneous%20CAPTCHA%20types%20into%20a%20single%20evaluation%0Aprotocol.%20Leveraging%20a%20shared%20vision-language%20model%20backbone%2C%20we%20fine-tune%0Aspecialized%20cracking%20agents%20for%20each%20CAPTCHA%20category%2C%20enabling%20consistent%2C%0Across-modal%20assessments.%20Extensive%20experiments%20reveal%20that%20MCA-Bench%0Aeffectively%20maps%20the%20vulnerability%20spectrum%20of%20modern%20CAPTCHA%20designs%20under%0Avaried%20attack%20settings%2C%20and%20crucially%20offers%20the%20first%20quantitative%20analysis%20of%0Ahow%20challenge%20complexity%2C%20interaction%20depth%2C%20and%20model%20solvability%20interrelate.%0ABased%20on%20these%20findings%2C%20we%20propose%20three%20actionable%20design%20principles%20and%0Aidentify%20key%20open%20challenges%2C%20laying%20the%20groundwork%20for%20systematic%20CAPTCHA%0Ahardening%2C%20fair%20benchmarking%2C%20and%20broader%20community%20collaboration.%20Datasets%20and%0Acode%20are%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05982v4&entry.124074799=Read"},
{"title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments", "author": "Sitong Gong and Lu Zhang and Yunzhi Zhuge and Xu Jia and Pingping Zhang and Huchuan Lu", "abstract": "  Video reasoning segmentation (VRS) endeavors to delineate referred objects in\nvideos guided by implicit instructions that encapsulate human intent and\ntemporal logic. Previous approaches leverage large vision language models\n(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.\nHowever, this paradigm suffers from limited interpretability during inference\nand suboptimal performance due to inadequate spatiotemporal reasoning. Drawing\ninspiration from seminal breakthroughs in reinforcement learning, we introduce\nVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in\nsegmentation. Veason-R1 is trained through Group Relative Policy Optimization\n(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we\ncurate high-quality CoT training data to instill structured reasoning\ntrajectories, bridging video-level semantics and frame-level spatial grounding,\nyielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO\nfine-tuning encourages efficient exploration of the reasoning space by\noptimizing reasoning chains. To this end, we incorporate a holistic reward\nmechanism that synergistically enhances spatial alignment and temporal\nconsistency, bolstering keyframe localization and fine-grained grounding.\nComprehensive empirical evaluations demonstrate that Veason-R1 achieves\nstate-of-the-art performance on multiple benchmarks, surpassing prior art by\nsignificant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),\nwhile exhibiting robustness to hallucinations (+8.8 R). Our code and model\nweights will be available at Veason-R1.\n", "link": "http://arxiv.org/abs/2508.11538v1", "date": "2025-08-15", "relevancy": 2.3696, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5991}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcing%20Video%20Reasoning%20Segmentation%20to%20Think%20Before%20It%20Segments&body=Title%3A%20Reinforcing%20Video%20Reasoning%20Segmentation%20to%20Think%20Before%20It%20Segments%0AAuthor%3A%20Sitong%20Gong%20and%20Lu%20Zhang%20and%20Yunzhi%20Zhuge%20and%20Xu%20Jia%20and%20Pingping%20Zhang%20and%20Huchuan%20Lu%0AAbstract%3A%20%20%20Video%20reasoning%20segmentation%20%28VRS%29%20endeavors%20to%20delineate%20referred%20objects%20in%0Avideos%20guided%20by%20implicit%20instructions%20that%20encapsulate%20human%20intent%20and%0Atemporal%20logic.%20Previous%20approaches%20leverage%20large%20vision%20language%20models%0A%28LVLMs%29%20to%20encode%20object%20semantics%20into%20%3CSEG%3E%20tokens%20for%20mask%20prediction.%0AHowever%2C%20this%20paradigm%20suffers%20from%20limited%20interpretability%20during%20inference%0Aand%20suboptimal%20performance%20due%20to%20inadequate%20spatiotemporal%20reasoning.%20Drawing%0Ainspiration%20from%20seminal%20breakthroughs%20in%20reinforcement%20learning%2C%20we%20introduce%0AVeason-R1%2C%20a%20specialized%20LVLM%20for%20VRS%20that%20emphasizes%20structured%20reasoning%20in%0Asegmentation.%20Veason-R1%20is%20trained%20through%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20augmented%20with%20Chain-of-Thought%20%28CoT%29%20initialization.%20To%20begin%20with%2C%20we%0Acurate%20high-quality%20CoT%20training%20data%20to%20instill%20structured%20reasoning%0Atrajectories%2C%20bridging%20video-level%20semantics%20and%20frame-level%20spatial%20grounding%2C%0Ayielding%20the%20supervised%20fine-tuned%20model%20Veason-SFT.%20Subsequently%2C%20GRPO%0Afine-tuning%20encourages%20efficient%20exploration%20of%20the%20reasoning%20space%20by%0Aoptimizing%20reasoning%20chains.%20To%20this%20end%2C%20we%20incorporate%20a%20holistic%20reward%0Amechanism%20that%20synergistically%20enhances%20spatial%20alignment%20and%20temporal%0Aconsistency%2C%20bolstering%20keyframe%20localization%20and%20fine-grained%20grounding.%0AComprehensive%20empirical%20evaluations%20demonstrate%20that%20Veason-R1%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%2C%20surpassing%20prior%20art%20by%0Asignificant%20margins%20%28e.g.%2C%20%2B1.3%20J%20%26F%20in%20ReVOS%20and%20%2B10.0%20J%20%26F%20in%20ReasonVOS%29%2C%0Awhile%20exhibiting%20robustness%20to%20hallucinations%20%28%2B8.8%20R%29.%20Our%20code%20and%20model%0Aweights%20will%20be%20available%20at%20Veason-R1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcing%2520Video%2520Reasoning%2520Segmentation%2520to%2520Think%2520Before%2520It%2520Segments%26entry.906535625%3DSitong%2520Gong%2520and%2520Lu%2520Zhang%2520and%2520Yunzhi%2520Zhuge%2520and%2520Xu%2520Jia%2520and%2520Pingping%2520Zhang%2520and%2520Huchuan%2520Lu%26entry.1292438233%3D%2520%2520Video%2520reasoning%2520segmentation%2520%2528VRS%2529%2520endeavors%2520to%2520delineate%2520referred%2520objects%2520in%250Avideos%2520guided%2520by%2520implicit%2520instructions%2520that%2520encapsulate%2520human%2520intent%2520and%250Atemporal%2520logic.%2520Previous%2520approaches%2520leverage%2520large%2520vision%2520language%2520models%250A%2528LVLMs%2529%2520to%2520encode%2520object%2520semantics%2520into%2520%253CSEG%253E%2520tokens%2520for%2520mask%2520prediction.%250AHowever%252C%2520this%2520paradigm%2520suffers%2520from%2520limited%2520interpretability%2520during%2520inference%250Aand%2520suboptimal%2520performance%2520due%2520to%2520inadequate%2520spatiotemporal%2520reasoning.%2520Drawing%250Ainspiration%2520from%2520seminal%2520breakthroughs%2520in%2520reinforcement%2520learning%252C%2520we%2520introduce%250AVeason-R1%252C%2520a%2520specialized%2520LVLM%2520for%2520VRS%2520that%2520emphasizes%2520structured%2520reasoning%2520in%250Asegmentation.%2520Veason-R1%2520is%2520trained%2520through%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%2520augmented%2520with%2520Chain-of-Thought%2520%2528CoT%2529%2520initialization.%2520To%2520begin%2520with%252C%2520we%250Acurate%2520high-quality%2520CoT%2520training%2520data%2520to%2520instill%2520structured%2520reasoning%250Atrajectories%252C%2520bridging%2520video-level%2520semantics%2520and%2520frame-level%2520spatial%2520grounding%252C%250Ayielding%2520the%2520supervised%2520fine-tuned%2520model%2520Veason-SFT.%2520Subsequently%252C%2520GRPO%250Afine-tuning%2520encourages%2520efficient%2520exploration%2520of%2520the%2520reasoning%2520space%2520by%250Aoptimizing%2520reasoning%2520chains.%2520To%2520this%2520end%252C%2520we%2520incorporate%2520a%2520holistic%2520reward%250Amechanism%2520that%2520synergistically%2520enhances%2520spatial%2520alignment%2520and%2520temporal%250Aconsistency%252C%2520bolstering%2520keyframe%2520localization%2520and%2520fine-grained%2520grounding.%250AComprehensive%2520empirical%2520evaluations%2520demonstrate%2520that%2520Veason-R1%2520achieves%250Astate-of-the-art%2520performance%2520on%2520multiple%2520benchmarks%252C%2520surpassing%2520prior%2520art%2520by%250Asignificant%2520margins%2520%2528e.g.%252C%2520%252B1.3%2520J%2520%2526F%2520in%2520ReVOS%2520and%2520%252B10.0%2520J%2520%2526F%2520in%2520ReasonVOS%2529%252C%250Awhile%2520exhibiting%2520robustness%2520to%2520hallucinations%2520%2528%252B8.8%2520R%2529.%2520Our%2520code%2520and%2520model%250Aweights%2520will%2520be%2520available%2520at%2520Veason-R1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcing%20Video%20Reasoning%20Segmentation%20to%20Think%20Before%20It%20Segments&entry.906535625=Sitong%20Gong%20and%20Lu%20Zhang%20and%20Yunzhi%20Zhuge%20and%20Xu%20Jia%20and%20Pingping%20Zhang%20and%20Huchuan%20Lu&entry.1292438233=%20%20Video%20reasoning%20segmentation%20%28VRS%29%20endeavors%20to%20delineate%20referred%20objects%20in%0Avideos%20guided%20by%20implicit%20instructions%20that%20encapsulate%20human%20intent%20and%0Atemporal%20logic.%20Previous%20approaches%20leverage%20large%20vision%20language%20models%0A%28LVLMs%29%20to%20encode%20object%20semantics%20into%20%3CSEG%3E%20tokens%20for%20mask%20prediction.%0AHowever%2C%20this%20paradigm%20suffers%20from%20limited%20interpretability%20during%20inference%0Aand%20suboptimal%20performance%20due%20to%20inadequate%20spatiotemporal%20reasoning.%20Drawing%0Ainspiration%20from%20seminal%20breakthroughs%20in%20reinforcement%20learning%2C%20we%20introduce%0AVeason-R1%2C%20a%20specialized%20LVLM%20for%20VRS%20that%20emphasizes%20structured%20reasoning%20in%0Asegmentation.%20Veason-R1%20is%20trained%20through%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20augmented%20with%20Chain-of-Thought%20%28CoT%29%20initialization.%20To%20begin%20with%2C%20we%0Acurate%20high-quality%20CoT%20training%20data%20to%20instill%20structured%20reasoning%0Atrajectories%2C%20bridging%20video-level%20semantics%20and%20frame-level%20spatial%20grounding%2C%0Ayielding%20the%20supervised%20fine-tuned%20model%20Veason-SFT.%20Subsequently%2C%20GRPO%0Afine-tuning%20encourages%20efficient%20exploration%20of%20the%20reasoning%20space%20by%0Aoptimizing%20reasoning%20chains.%20To%20this%20end%2C%20we%20incorporate%20a%20holistic%20reward%0Amechanism%20that%20synergistically%20enhances%20spatial%20alignment%20and%20temporal%0Aconsistency%2C%20bolstering%20keyframe%20localization%20and%20fine-grained%20grounding.%0AComprehensive%20empirical%20evaluations%20demonstrate%20that%20Veason-R1%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%2C%20surpassing%20prior%20art%20by%0Asignificant%20margins%20%28e.g.%2C%20%2B1.3%20J%20%26F%20in%20ReVOS%20and%20%2B10.0%20J%20%26F%20in%20ReasonVOS%29%2C%0Awhile%20exhibiting%20robustness%20to%20hallucinations%20%28%2B8.8%20R%29.%20Our%20code%20and%20model%0Aweights%20will%20be%20available%20at%20Veason-R1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11538v1&entry.124074799=Read"},
{"title": "ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous\n  Driving", "author": "Jingyu Li and Bozhou Zhang and Xin Jin and Jiankang Deng and Xiatian Zhu and Li Zhang", "abstract": "  Autonomous driving requires rich contextual comprehension and precise\npredictive reasoning to navigate dynamic and complex environments safely.\nVision-Language Models (VLMs) and Driving World Models (DWMs) have\nindependently emerged as powerful recipes addressing different aspects of this\nchallenge. VLMs provide interpretability and robust action prediction through\ntheir ability to understand multi-modal context, while DWMs excel in generating\ndetailed and plausible future driving scenarios essential for proactive\nplanning. Integrating VLMs with DWMs is an intuitive, promising, yet\nunderstudied strategy to exploit the complementary strengths of accurate\nbehavioral prediction and realistic scene generation. Nevertheless, this\nintegration presents notable challenges, particularly in effectively connecting\naction-level decisions with high-fidelity pixel-level predictions and\nmaintaining computational efficiency. In this paper, we propose ImagiDrive, a\nnovel end-to-end autonomous driving framework that integrates a VLM-based\ndriving agent with a DWM-based scene imaginer to form a unified\nimagination-and-planning loop. The driving agent predicts initial driving\ntrajectories based on multi-modal inputs, guiding the scene imaginer to\ngenerate corresponding future scenarios. These imagined scenarios are\nsubsequently utilized to iteratively refine the driving agent's planning\ndecisions. To address efficiency and predictive accuracy challenges inherent in\nthis integration, we introduce an early stopping mechanism and a trajectory\nselection strategy. Extensive experimental validation on the nuScenes and\nNAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over\nprevious alternatives under both open-loop and closed-loop conditions.\n", "link": "http://arxiv.org/abs/2508.11428v1", "date": "2025-08-15", "relevancy": 2.3317, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.589}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5812}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImagiDrive%3A%20A%20Unified%20Imagination-and-Planning%20Framework%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20ImagiDrive%3A%20A%20Unified%20Imagination-and-Planning%20Framework%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Jingyu%20Li%20and%20Bozhou%20Zhang%20and%20Xin%20Jin%20and%20Jiankang%20Deng%20and%20Xiatian%20Zhu%20and%20Li%20Zhang%0AAbstract%3A%20%20%20Autonomous%20driving%20requires%20rich%20contextual%20comprehension%20and%20precise%0Apredictive%20reasoning%20to%20navigate%20dynamic%20and%20complex%20environments%20safely.%0AVision-Language%20Models%20%28VLMs%29%20and%20Driving%20World%20Models%20%28DWMs%29%20have%0Aindependently%20emerged%20as%20powerful%20recipes%20addressing%20different%20aspects%20of%20this%0Achallenge.%20VLMs%20provide%20interpretability%20and%20robust%20action%20prediction%20through%0Atheir%20ability%20to%20understand%20multi-modal%20context%2C%20while%20DWMs%20excel%20in%20generating%0Adetailed%20and%20plausible%20future%20driving%20scenarios%20essential%20for%20proactive%0Aplanning.%20Integrating%20VLMs%20with%20DWMs%20is%20an%20intuitive%2C%20promising%2C%20yet%0Aunderstudied%20strategy%20to%20exploit%20the%20complementary%20strengths%20of%20accurate%0Abehavioral%20prediction%20and%20realistic%20scene%20generation.%20Nevertheless%2C%20this%0Aintegration%20presents%20notable%20challenges%2C%20particularly%20in%20effectively%20connecting%0Aaction-level%20decisions%20with%20high-fidelity%20pixel-level%20predictions%20and%0Amaintaining%20computational%20efficiency.%20In%20this%20paper%2C%20we%20propose%20ImagiDrive%2C%20a%0Anovel%20end-to-end%20autonomous%20driving%20framework%20that%20integrates%20a%20VLM-based%0Adriving%20agent%20with%20a%20DWM-based%20scene%20imaginer%20to%20form%20a%20unified%0Aimagination-and-planning%20loop.%20The%20driving%20agent%20predicts%20initial%20driving%0Atrajectories%20based%20on%20multi-modal%20inputs%2C%20guiding%20the%20scene%20imaginer%20to%0Agenerate%20corresponding%20future%20scenarios.%20These%20imagined%20scenarios%20are%0Asubsequently%20utilized%20to%20iteratively%20refine%20the%20driving%20agent%27s%20planning%0Adecisions.%20To%20address%20efficiency%20and%20predictive%20accuracy%20challenges%20inherent%20in%0Athis%20integration%2C%20we%20introduce%20an%20early%20stopping%20mechanism%20and%20a%20trajectory%0Aselection%20strategy.%20Extensive%20experimental%20validation%20on%20the%20nuScenes%20and%0ANAVSIM%20datasets%20demonstrates%20the%20robustness%20and%20superiority%20of%20ImagiDrive%20over%0Aprevious%20alternatives%20under%20both%20open-loop%20and%20closed-loop%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagiDrive%253A%2520A%2520Unified%2520Imagination-and-Planning%2520Framework%2520for%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DJingyu%2520Li%2520and%2520Bozhou%2520Zhang%2520and%2520Xin%2520Jin%2520and%2520Jiankang%2520Deng%2520and%2520Xiatian%2520Zhu%2520and%2520Li%2520Zhang%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520requires%2520rich%2520contextual%2520comprehension%2520and%2520precise%250Apredictive%2520reasoning%2520to%2520navigate%2520dynamic%2520and%2520complex%2520environments%2520safely.%250AVision-Language%2520Models%2520%2528VLMs%2529%2520and%2520Driving%2520World%2520Models%2520%2528DWMs%2529%2520have%250Aindependently%2520emerged%2520as%2520powerful%2520recipes%2520addressing%2520different%2520aspects%2520of%2520this%250Achallenge.%2520VLMs%2520provide%2520interpretability%2520and%2520robust%2520action%2520prediction%2520through%250Atheir%2520ability%2520to%2520understand%2520multi-modal%2520context%252C%2520while%2520DWMs%2520excel%2520in%2520generating%250Adetailed%2520and%2520plausible%2520future%2520driving%2520scenarios%2520essential%2520for%2520proactive%250Aplanning.%2520Integrating%2520VLMs%2520with%2520DWMs%2520is%2520an%2520intuitive%252C%2520promising%252C%2520yet%250Aunderstudied%2520strategy%2520to%2520exploit%2520the%2520complementary%2520strengths%2520of%2520accurate%250Abehavioral%2520prediction%2520and%2520realistic%2520scene%2520generation.%2520Nevertheless%252C%2520this%250Aintegration%2520presents%2520notable%2520challenges%252C%2520particularly%2520in%2520effectively%2520connecting%250Aaction-level%2520decisions%2520with%2520high-fidelity%2520pixel-level%2520predictions%2520and%250Amaintaining%2520computational%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ImagiDrive%252C%2520a%250Anovel%2520end-to-end%2520autonomous%2520driving%2520framework%2520that%2520integrates%2520a%2520VLM-based%250Adriving%2520agent%2520with%2520a%2520DWM-based%2520scene%2520imaginer%2520to%2520form%2520a%2520unified%250Aimagination-and-planning%2520loop.%2520The%2520driving%2520agent%2520predicts%2520initial%2520driving%250Atrajectories%2520based%2520on%2520multi-modal%2520inputs%252C%2520guiding%2520the%2520scene%2520imaginer%2520to%250Agenerate%2520corresponding%2520future%2520scenarios.%2520These%2520imagined%2520scenarios%2520are%250Asubsequently%2520utilized%2520to%2520iteratively%2520refine%2520the%2520driving%2520agent%2527s%2520planning%250Adecisions.%2520To%2520address%2520efficiency%2520and%2520predictive%2520accuracy%2520challenges%2520inherent%2520in%250Athis%2520integration%252C%2520we%2520introduce%2520an%2520early%2520stopping%2520mechanism%2520and%2520a%2520trajectory%250Aselection%2520strategy.%2520Extensive%2520experimental%2520validation%2520on%2520the%2520nuScenes%2520and%250ANAVSIM%2520datasets%2520demonstrates%2520the%2520robustness%2520and%2520superiority%2520of%2520ImagiDrive%2520over%250Aprevious%2520alternatives%2520under%2520both%2520open-loop%2520and%2520closed-loop%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImagiDrive%3A%20A%20Unified%20Imagination-and-Planning%20Framework%20for%20Autonomous%0A%20%20Driving&entry.906535625=Jingyu%20Li%20and%20Bozhou%20Zhang%20and%20Xin%20Jin%20and%20Jiankang%20Deng%20and%20Xiatian%20Zhu%20and%20Li%20Zhang&entry.1292438233=%20%20Autonomous%20driving%20requires%20rich%20contextual%20comprehension%20and%20precise%0Apredictive%20reasoning%20to%20navigate%20dynamic%20and%20complex%20environments%20safely.%0AVision-Language%20Models%20%28VLMs%29%20and%20Driving%20World%20Models%20%28DWMs%29%20have%0Aindependently%20emerged%20as%20powerful%20recipes%20addressing%20different%20aspects%20of%20this%0Achallenge.%20VLMs%20provide%20interpretability%20and%20robust%20action%20prediction%20through%0Atheir%20ability%20to%20understand%20multi-modal%20context%2C%20while%20DWMs%20excel%20in%20generating%0Adetailed%20and%20plausible%20future%20driving%20scenarios%20essential%20for%20proactive%0Aplanning.%20Integrating%20VLMs%20with%20DWMs%20is%20an%20intuitive%2C%20promising%2C%20yet%0Aunderstudied%20strategy%20to%20exploit%20the%20complementary%20strengths%20of%20accurate%0Abehavioral%20prediction%20and%20realistic%20scene%20generation.%20Nevertheless%2C%20this%0Aintegration%20presents%20notable%20challenges%2C%20particularly%20in%20effectively%20connecting%0Aaction-level%20decisions%20with%20high-fidelity%20pixel-level%20predictions%20and%0Amaintaining%20computational%20efficiency.%20In%20this%20paper%2C%20we%20propose%20ImagiDrive%2C%20a%0Anovel%20end-to-end%20autonomous%20driving%20framework%20that%20integrates%20a%20VLM-based%0Adriving%20agent%20with%20a%20DWM-based%20scene%20imaginer%20to%20form%20a%20unified%0Aimagination-and-planning%20loop.%20The%20driving%20agent%20predicts%20initial%20driving%0Atrajectories%20based%20on%20multi-modal%20inputs%2C%20guiding%20the%20scene%20imaginer%20to%0Agenerate%20corresponding%20future%20scenarios.%20These%20imagined%20scenarios%20are%0Asubsequently%20utilized%20to%20iteratively%20refine%20the%20driving%20agent%27s%20planning%0Adecisions.%20To%20address%20efficiency%20and%20predictive%20accuracy%20challenges%20inherent%20in%0Athis%20integration%2C%20we%20introduce%20an%20early%20stopping%20mechanism%20and%20a%20trajectory%0Aselection%20strategy.%20Extensive%20experimental%20validation%20on%20the%20nuScenes%20and%0ANAVSIM%20datasets%20demonstrates%20the%20robustness%20and%20superiority%20of%20ImagiDrive%20over%0Aprevious%20alternatives%20under%20both%20open-loop%20and%20closed-loop%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11428v1&entry.124074799=Read"},
{"title": "CoreEditor: Consistent 3D Editing via Correspondence-constrained\n  Diffusion", "author": "Zhe Zhu and Honghua Chen and Peng Li and Mingqiang Wei", "abstract": "  Text-driven 3D editing seeks to modify 3D scenes according to textual\ndescriptions, and most existing approaches tackle this by adapting pre-trained\n2D image editors to multi-view inputs. However, without explicit control over\nmulti-view information exchange, they often fail to maintain cross-view\nconsistency, leading to insufficient edits and blurry details. We introduce\nCoreEditor, a novel framework for consistent text-to-3D editing. The key\ninnovation is a correspondence-constrained attention mechanism that enforces\nprecise interactions between pixels expected to remain consistent throughout\nthe diffusion denoising process. Beyond relying solely on geometric alignment,\nwe further incorporate semantic similarity estimated during denoising, enabling\nmore reliable correspondence modeling and robust multi-view editing. In\naddition, we design a selective editing pipeline that allows users to choose\npreferred results from multiple candidates, offering greater flexibility and\nuser control. Extensive experiments show that CoreEditor produces high-quality,\n3D-consistent edits with sharper details, significantly outperforming prior\nmethods.\n", "link": "http://arxiv.org/abs/2508.11603v1", "date": "2025-08-15", "relevancy": 2.3276, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5997}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5783}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoreEditor%3A%20Consistent%203D%20Editing%20via%20Correspondence-constrained%0A%20%20Diffusion&body=Title%3A%20CoreEditor%3A%20Consistent%203D%20Editing%20via%20Correspondence-constrained%0A%20%20Diffusion%0AAuthor%3A%20Zhe%20Zhu%20and%20Honghua%20Chen%20and%20Peng%20Li%20and%20Mingqiang%20Wei%0AAbstract%3A%20%20%20Text-driven%203D%20editing%20seeks%20to%20modify%203D%20scenes%20according%20to%20textual%0Adescriptions%2C%20and%20most%20existing%20approaches%20tackle%20this%20by%20adapting%20pre-trained%0A2D%20image%20editors%20to%20multi-view%20inputs.%20However%2C%20without%20explicit%20control%20over%0Amulti-view%20information%20exchange%2C%20they%20often%20fail%20to%20maintain%20cross-view%0Aconsistency%2C%20leading%20to%20insufficient%20edits%20and%20blurry%20details.%20We%20introduce%0ACoreEditor%2C%20a%20novel%20framework%20for%20consistent%20text-to-3D%20editing.%20The%20key%0Ainnovation%20is%20a%20correspondence-constrained%20attention%20mechanism%20that%20enforces%0Aprecise%20interactions%20between%20pixels%20expected%20to%20remain%20consistent%20throughout%0Athe%20diffusion%20denoising%20process.%20Beyond%20relying%20solely%20on%20geometric%20alignment%2C%0Awe%20further%20incorporate%20semantic%20similarity%20estimated%20during%20denoising%2C%20enabling%0Amore%20reliable%20correspondence%20modeling%20and%20robust%20multi-view%20editing.%20In%0Aaddition%2C%20we%20design%20a%20selective%20editing%20pipeline%20that%20allows%20users%20to%20choose%0Apreferred%20results%20from%20multiple%20candidates%2C%20offering%20greater%20flexibility%20and%0Auser%20control.%20Extensive%20experiments%20show%20that%20CoreEditor%20produces%20high-quality%2C%0A3D-consistent%20edits%20with%20sharper%20details%2C%20significantly%20outperforming%20prior%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoreEditor%253A%2520Consistent%25203D%2520Editing%2520via%2520Correspondence-constrained%250A%2520%2520Diffusion%26entry.906535625%3DZhe%2520Zhu%2520and%2520Honghua%2520Chen%2520and%2520Peng%2520Li%2520and%2520Mingqiang%2520Wei%26entry.1292438233%3D%2520%2520Text-driven%25203D%2520editing%2520seeks%2520to%2520modify%25203D%2520scenes%2520according%2520to%2520textual%250Adescriptions%252C%2520and%2520most%2520existing%2520approaches%2520tackle%2520this%2520by%2520adapting%2520pre-trained%250A2D%2520image%2520editors%2520to%2520multi-view%2520inputs.%2520However%252C%2520without%2520explicit%2520control%2520over%250Amulti-view%2520information%2520exchange%252C%2520they%2520often%2520fail%2520to%2520maintain%2520cross-view%250Aconsistency%252C%2520leading%2520to%2520insufficient%2520edits%2520and%2520blurry%2520details.%2520We%2520introduce%250ACoreEditor%252C%2520a%2520novel%2520framework%2520for%2520consistent%2520text-to-3D%2520editing.%2520The%2520key%250Ainnovation%2520is%2520a%2520correspondence-constrained%2520attention%2520mechanism%2520that%2520enforces%250Aprecise%2520interactions%2520between%2520pixels%2520expected%2520to%2520remain%2520consistent%2520throughout%250Athe%2520diffusion%2520denoising%2520process.%2520Beyond%2520relying%2520solely%2520on%2520geometric%2520alignment%252C%250Awe%2520further%2520incorporate%2520semantic%2520similarity%2520estimated%2520during%2520denoising%252C%2520enabling%250Amore%2520reliable%2520correspondence%2520modeling%2520and%2520robust%2520multi-view%2520editing.%2520In%250Aaddition%252C%2520we%2520design%2520a%2520selective%2520editing%2520pipeline%2520that%2520allows%2520users%2520to%2520choose%250Apreferred%2520results%2520from%2520multiple%2520candidates%252C%2520offering%2520greater%2520flexibility%2520and%250Auser%2520control.%2520Extensive%2520experiments%2520show%2520that%2520CoreEditor%2520produces%2520high-quality%252C%250A3D-consistent%2520edits%2520with%2520sharper%2520details%252C%2520significantly%2520outperforming%2520prior%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoreEditor%3A%20Consistent%203D%20Editing%20via%20Correspondence-constrained%0A%20%20Diffusion&entry.906535625=Zhe%20Zhu%20and%20Honghua%20Chen%20and%20Peng%20Li%20and%20Mingqiang%20Wei&entry.1292438233=%20%20Text-driven%203D%20editing%20seeks%20to%20modify%203D%20scenes%20according%20to%20textual%0Adescriptions%2C%20and%20most%20existing%20approaches%20tackle%20this%20by%20adapting%20pre-trained%0A2D%20image%20editors%20to%20multi-view%20inputs.%20However%2C%20without%20explicit%20control%20over%0Amulti-view%20information%20exchange%2C%20they%20often%20fail%20to%20maintain%20cross-view%0Aconsistency%2C%20leading%20to%20insufficient%20edits%20and%20blurry%20details.%20We%20introduce%0ACoreEditor%2C%20a%20novel%20framework%20for%20consistent%20text-to-3D%20editing.%20The%20key%0Ainnovation%20is%20a%20correspondence-constrained%20attention%20mechanism%20that%20enforces%0Aprecise%20interactions%20between%20pixels%20expected%20to%20remain%20consistent%20throughout%0Athe%20diffusion%20denoising%20process.%20Beyond%20relying%20solely%20on%20geometric%20alignment%2C%0Awe%20further%20incorporate%20semantic%20similarity%20estimated%20during%20denoising%2C%20enabling%0Amore%20reliable%20correspondence%20modeling%20and%20robust%20multi-view%20editing.%20In%0Aaddition%2C%20we%20design%20a%20selective%20editing%20pipeline%20that%20allows%20users%20to%20choose%0Apreferred%20results%20from%20multiple%20candidates%2C%20offering%20greater%20flexibility%20and%0Auser%20control.%20Extensive%20experiments%20show%20that%20CoreEditor%20produces%20high-quality%2C%0A3D-consistent%20edits%20with%20sharper%20details%2C%20significantly%20outperforming%20prior%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11603v1&entry.124074799=Read"},
{"title": "i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor\n  Fusion Navigation and Mapping", "author": "Hailiang Tang and Tisheng Zhang and Liqiang Wang and Xin Ding and Man Yuan and Zhiyu Xiang and Jujin Chen and Yuhan Bian and Shuangyan Liu and Yuqing Wang and Guan Wang and Xiaoji Niu", "abstract": "  Accurate and reliable navigation is crucial for autonomous unmanned ground\nvehicle (UGV). However, current UGV datasets fall short in meeting the demands\nfor advancing navigation and mapping techniques due to limitations in sensor\nconfiguration, time synchronization, ground truth, and scenario diversity. To\naddress these challenges, we present i2Nav-Robot, a large-scale dataset\ndesigned for multi-sensor fusion navigation and mapping in indoor-outdoor\nenvironments. We integrate multi-modal sensors, including the newest front-view\nand 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,\nodometer, global navigation satellite system (GNSS) receiver, and inertial\nmeasurement units (IMU) on an omnidirectional wheeled robot. Accurate\ntimestamps are obtained through both online hardware synchronization and\noffline calibration for all sensors. The dataset comprises ten larger-scale\nsequences covering diverse UGV operating scenarios, such as outdoor streets,\nand indoor parking lots, with a total length of about 17060 meters.\nHigh-frequency ground truth, with centimeter-level accuracy for position, is\nderived from post-processing integrated navigation methods using a\nnavigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more\nthan ten open-sourced multi-sensor fusion systems, and it has proven to have\nsuperior data quality.\n", "link": "http://arxiv.org/abs/2508.11485v1", "date": "2025-08-15", "relevancy": 2.3165, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6069}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5626}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20i2Nav-Robot%3A%20A%20Large-Scale%20Indoor-Outdoor%20Robot%20Dataset%20for%20Multi-Sensor%0A%20%20Fusion%20Navigation%20and%20Mapping&body=Title%3A%20i2Nav-Robot%3A%20A%20Large-Scale%20Indoor-Outdoor%20Robot%20Dataset%20for%20Multi-Sensor%0A%20%20Fusion%20Navigation%20and%20Mapping%0AAuthor%3A%20Hailiang%20Tang%20and%20Tisheng%20Zhang%20and%20Liqiang%20Wang%20and%20Xin%20Ding%20and%20Man%20Yuan%20and%20Zhiyu%20Xiang%20and%20Jujin%20Chen%20and%20Yuhan%20Bian%20and%20Shuangyan%20Liu%20and%20Yuqing%20Wang%20and%20Guan%20Wang%20and%20Xiaoji%20Niu%0AAbstract%3A%20%20%20Accurate%20and%20reliable%20navigation%20is%20crucial%20for%20autonomous%20unmanned%20ground%0Avehicle%20%28UGV%29.%20However%2C%20current%20UGV%20datasets%20fall%20short%20in%20meeting%20the%20demands%0Afor%20advancing%20navigation%20and%20mapping%20techniques%20due%20to%20limitations%20in%20sensor%0Aconfiguration%2C%20time%20synchronization%2C%20ground%20truth%2C%20and%20scenario%20diversity.%20To%0Aaddress%20these%20challenges%2C%20we%20present%20i2Nav-Robot%2C%20a%20large-scale%20dataset%0Adesigned%20for%20multi-sensor%20fusion%20navigation%20and%20mapping%20in%20indoor-outdoor%0Aenvironments.%20We%20integrate%20multi-modal%20sensors%2C%20including%20the%20newest%20front-view%0Aand%20360-degree%20solid-state%20LiDARs%2C%204-dimensional%20%284D%29%20radar%2C%20stereo%20cameras%2C%0Aodometer%2C%20global%20navigation%20satellite%20system%20%28GNSS%29%20receiver%2C%20and%20inertial%0Ameasurement%20units%20%28IMU%29%20on%20an%20omnidirectional%20wheeled%20robot.%20Accurate%0Atimestamps%20are%20obtained%20through%20both%20online%20hardware%20synchronization%20and%0Aoffline%20calibration%20for%20all%20sensors.%20The%20dataset%20comprises%20ten%20larger-scale%0Asequences%20covering%20diverse%20UGV%20operating%20scenarios%2C%20such%20as%20outdoor%20streets%2C%0Aand%20indoor%20parking%20lots%2C%20with%20a%20total%20length%20of%20about%2017060%20meters.%0AHigh-frequency%20ground%20truth%2C%20with%20centimeter-level%20accuracy%20for%20position%2C%20is%0Aderived%20from%20post-processing%20integrated%20navigation%20methods%20using%20a%0Anavigation-grade%20IMU.%20The%20proposed%20i2Nav-Robot%20dataset%20is%20evaluated%20by%20more%0Athan%20ten%20open-sourced%20multi-sensor%20fusion%20systems%2C%20and%20it%20has%20proven%20to%20have%0Asuperior%20data%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Di2Nav-Robot%253A%2520A%2520Large-Scale%2520Indoor-Outdoor%2520Robot%2520Dataset%2520for%2520Multi-Sensor%250A%2520%2520Fusion%2520Navigation%2520and%2520Mapping%26entry.906535625%3DHailiang%2520Tang%2520and%2520Tisheng%2520Zhang%2520and%2520Liqiang%2520Wang%2520and%2520Xin%2520Ding%2520and%2520Man%2520Yuan%2520and%2520Zhiyu%2520Xiang%2520and%2520Jujin%2520Chen%2520and%2520Yuhan%2520Bian%2520and%2520Shuangyan%2520Liu%2520and%2520Yuqing%2520Wang%2520and%2520Guan%2520Wang%2520and%2520Xiaoji%2520Niu%26entry.1292438233%3D%2520%2520Accurate%2520and%2520reliable%2520navigation%2520is%2520crucial%2520for%2520autonomous%2520unmanned%2520ground%250Avehicle%2520%2528UGV%2529.%2520However%252C%2520current%2520UGV%2520datasets%2520fall%2520short%2520in%2520meeting%2520the%2520demands%250Afor%2520advancing%2520navigation%2520and%2520mapping%2520techniques%2520due%2520to%2520limitations%2520in%2520sensor%250Aconfiguration%252C%2520time%2520synchronization%252C%2520ground%2520truth%252C%2520and%2520scenario%2520diversity.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520present%2520i2Nav-Robot%252C%2520a%2520large-scale%2520dataset%250Adesigned%2520for%2520multi-sensor%2520fusion%2520navigation%2520and%2520mapping%2520in%2520indoor-outdoor%250Aenvironments.%2520We%2520integrate%2520multi-modal%2520sensors%252C%2520including%2520the%2520newest%2520front-view%250Aand%2520360-degree%2520solid-state%2520LiDARs%252C%25204-dimensional%2520%25284D%2529%2520radar%252C%2520stereo%2520cameras%252C%250Aodometer%252C%2520global%2520navigation%2520satellite%2520system%2520%2528GNSS%2529%2520receiver%252C%2520and%2520inertial%250Ameasurement%2520units%2520%2528IMU%2529%2520on%2520an%2520omnidirectional%2520wheeled%2520robot.%2520Accurate%250Atimestamps%2520are%2520obtained%2520through%2520both%2520online%2520hardware%2520synchronization%2520and%250Aoffline%2520calibration%2520for%2520all%2520sensors.%2520The%2520dataset%2520comprises%2520ten%2520larger-scale%250Asequences%2520covering%2520diverse%2520UGV%2520operating%2520scenarios%252C%2520such%2520as%2520outdoor%2520streets%252C%250Aand%2520indoor%2520parking%2520lots%252C%2520with%2520a%2520total%2520length%2520of%2520about%252017060%2520meters.%250AHigh-frequency%2520ground%2520truth%252C%2520with%2520centimeter-level%2520accuracy%2520for%2520position%252C%2520is%250Aderived%2520from%2520post-processing%2520integrated%2520navigation%2520methods%2520using%2520a%250Anavigation-grade%2520IMU.%2520The%2520proposed%2520i2Nav-Robot%2520dataset%2520is%2520evaluated%2520by%2520more%250Athan%2520ten%2520open-sourced%2520multi-sensor%2520fusion%2520systems%252C%2520and%2520it%2520has%2520proven%2520to%2520have%250Asuperior%2520data%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=i2Nav-Robot%3A%20A%20Large-Scale%20Indoor-Outdoor%20Robot%20Dataset%20for%20Multi-Sensor%0A%20%20Fusion%20Navigation%20and%20Mapping&entry.906535625=Hailiang%20Tang%20and%20Tisheng%20Zhang%20and%20Liqiang%20Wang%20and%20Xin%20Ding%20and%20Man%20Yuan%20and%20Zhiyu%20Xiang%20and%20Jujin%20Chen%20and%20Yuhan%20Bian%20and%20Shuangyan%20Liu%20and%20Yuqing%20Wang%20and%20Guan%20Wang%20and%20Xiaoji%20Niu&entry.1292438233=%20%20Accurate%20and%20reliable%20navigation%20is%20crucial%20for%20autonomous%20unmanned%20ground%0Avehicle%20%28UGV%29.%20However%2C%20current%20UGV%20datasets%20fall%20short%20in%20meeting%20the%20demands%0Afor%20advancing%20navigation%20and%20mapping%20techniques%20due%20to%20limitations%20in%20sensor%0Aconfiguration%2C%20time%20synchronization%2C%20ground%20truth%2C%20and%20scenario%20diversity.%20To%0Aaddress%20these%20challenges%2C%20we%20present%20i2Nav-Robot%2C%20a%20large-scale%20dataset%0Adesigned%20for%20multi-sensor%20fusion%20navigation%20and%20mapping%20in%20indoor-outdoor%0Aenvironments.%20We%20integrate%20multi-modal%20sensors%2C%20including%20the%20newest%20front-view%0Aand%20360-degree%20solid-state%20LiDARs%2C%204-dimensional%20%284D%29%20radar%2C%20stereo%20cameras%2C%0Aodometer%2C%20global%20navigation%20satellite%20system%20%28GNSS%29%20receiver%2C%20and%20inertial%0Ameasurement%20units%20%28IMU%29%20on%20an%20omnidirectional%20wheeled%20robot.%20Accurate%0Atimestamps%20are%20obtained%20through%20both%20online%20hardware%20synchronization%20and%0Aoffline%20calibration%20for%20all%20sensors.%20The%20dataset%20comprises%20ten%20larger-scale%0Asequences%20covering%20diverse%20UGV%20operating%20scenarios%2C%20such%20as%20outdoor%20streets%2C%0Aand%20indoor%20parking%20lots%2C%20with%20a%20total%20length%20of%20about%2017060%20meters.%0AHigh-frequency%20ground%20truth%2C%20with%20centimeter-level%20accuracy%20for%20position%2C%20is%0Aderived%20from%20post-processing%20integrated%20navigation%20methods%20using%20a%0Anavigation-grade%20IMU.%20The%20proposed%20i2Nav-Robot%20dataset%20is%20evaluated%20by%20more%0Athan%20ten%20open-sourced%20multi-sensor%20fusion%20systems%2C%20and%20it%20has%20proven%20to%20have%0Asuperior%20data%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11485v1&entry.124074799=Read"},
{"title": "Is ChatGPT-5 Ready for Mammogram VQA?", "author": "Qiang Li and Shansong Wang and Mingzhe Hu and Mojtaba Safari and Zachary Eidex and Xiaofeng Yang", "abstract": "  Mammogram visual question answering (VQA) integrates image interpretation\nwith clinical reasoning and has potential to support breast cancer screening.\nWe systematically evaluated the GPT-5 family and GPT-4o model on four public\nmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,\nabnormality detection, and malignancy classification tasks. GPT-5 consistently\nwas the best performing model but lagged behind both human experts and\ndomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores\namong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),\ncalcification (63.5%), and malignancy (52.8%) classification. On InBreast, it\nattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%\nmalignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection\nand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS\naccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared\nwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and\nspecificity (52.3%). While GPT-5 exhibits promising capabilities for screening\ntasks, its performance remains insufficient for high-stakes clinical imaging\napplications without targeted domain adaptation and optimization. However, the\ntremendous improvements in performance from GPT-4o to GPT-5 show a promising\ntrend in the potential for general large language models (LLMs) to assist with\nmammography VQA tasks.\n", "link": "http://arxiv.org/abs/2508.11628v1", "date": "2025-08-15", "relevancy": 2.3158, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4765}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4627}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20ChatGPT-5%20Ready%20for%20Mammogram%20VQA%3F&body=Title%3A%20Is%20ChatGPT-5%20Ready%20for%20Mammogram%20VQA%3F%0AAuthor%3A%20Qiang%20Li%20and%20Shansong%20Wang%20and%20Mingzhe%20Hu%20and%20Mojtaba%20Safari%20and%20Zachary%20Eidex%20and%20Xiaofeng%20Yang%0AAbstract%3A%20%20%20Mammogram%20visual%20question%20answering%20%28VQA%29%20integrates%20image%20interpretation%0Awith%20clinical%20reasoning%20and%20has%20potential%20to%20support%20breast%20cancer%20screening.%0AWe%20systematically%20evaluated%20the%20GPT-5%20family%20and%20GPT-4o%20model%20on%20four%20public%0Amammography%20datasets%20%28EMBED%2C%20InBreast%2C%20CMMD%2C%20CBIS-DDSM%29%20for%20BI-RADS%20assessment%2C%0Aabnormality%20detection%2C%20and%20malignancy%20classification%20tasks.%20GPT-5%20consistently%0Awas%20the%20best%20performing%20model%20but%20lagged%20behind%20both%20human%20experts%20and%0Adomain-specific%20fine-tuned%20models.%20On%20EMBED%2C%20GPT-5%20achieved%20the%20highest%20scores%0Aamong%20GPT%20variants%20in%20density%20%2856.8%25%29%2C%20distortion%20%2852.5%25%29%2C%20mass%20%2864.5%25%29%2C%0Acalcification%20%2863.5%25%29%2C%20and%20malignancy%20%2852.8%25%29%20classification.%20On%20InBreast%2C%20it%0Aattained%2036.9%25%20BI-RADS%20accuracy%2C%2045.9%25%20abnormality%20detection%2C%20and%2035.0%25%0Amalignancy%20classification.%20On%20CMMD%2C%20GPT-5%20reached%2032.3%25%20abnormality%20detection%0Aand%2055.0%25%20malignancy%20accuracy.%20On%20CBIS-DDSM%2C%20it%20achieved%2069.3%25%20BI-RADS%0Aaccuracy%2C%2066.0%25%20abnormality%20detection%2C%20and%2058.2%25%20malignancy%20accuracy.%20Compared%0Awith%20human%20expert%20estimations%2C%20GPT-5%20exhibited%20lower%20sensitivity%20%2863.5%25%29%20and%0Aspecificity%20%2852.3%25%29.%20While%20GPT-5%20exhibits%20promising%20capabilities%20for%20screening%0Atasks%2C%20its%20performance%20remains%20insufficient%20for%20high-stakes%20clinical%20imaging%0Aapplications%20without%20targeted%20domain%20adaptation%20and%20optimization.%20However%2C%20the%0Atremendous%20improvements%20in%20performance%20from%20GPT-4o%20to%20GPT-5%20show%20a%20promising%0Atrend%20in%20the%20potential%20for%20general%20large%20language%20models%20%28LLMs%29%20to%20assist%20with%0Amammography%20VQA%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520ChatGPT-5%2520Ready%2520for%2520Mammogram%2520VQA%253F%26entry.906535625%3DQiang%2520Li%2520and%2520Shansong%2520Wang%2520and%2520Mingzhe%2520Hu%2520and%2520Mojtaba%2520Safari%2520and%2520Zachary%2520Eidex%2520and%2520Xiaofeng%2520Yang%26entry.1292438233%3D%2520%2520Mammogram%2520visual%2520question%2520answering%2520%2528VQA%2529%2520integrates%2520image%2520interpretation%250Awith%2520clinical%2520reasoning%2520and%2520has%2520potential%2520to%2520support%2520breast%2520cancer%2520screening.%250AWe%2520systematically%2520evaluated%2520the%2520GPT-5%2520family%2520and%2520GPT-4o%2520model%2520on%2520four%2520public%250Amammography%2520datasets%2520%2528EMBED%252C%2520InBreast%252C%2520CMMD%252C%2520CBIS-DDSM%2529%2520for%2520BI-RADS%2520assessment%252C%250Aabnormality%2520detection%252C%2520and%2520malignancy%2520classification%2520tasks.%2520GPT-5%2520consistently%250Awas%2520the%2520best%2520performing%2520model%2520but%2520lagged%2520behind%2520both%2520human%2520experts%2520and%250Adomain-specific%2520fine-tuned%2520models.%2520On%2520EMBED%252C%2520GPT-5%2520achieved%2520the%2520highest%2520scores%250Aamong%2520GPT%2520variants%2520in%2520density%2520%252856.8%2525%2529%252C%2520distortion%2520%252852.5%2525%2529%252C%2520mass%2520%252864.5%2525%2529%252C%250Acalcification%2520%252863.5%2525%2529%252C%2520and%2520malignancy%2520%252852.8%2525%2529%2520classification.%2520On%2520InBreast%252C%2520it%250Aattained%252036.9%2525%2520BI-RADS%2520accuracy%252C%252045.9%2525%2520abnormality%2520detection%252C%2520and%252035.0%2525%250Amalignancy%2520classification.%2520On%2520CMMD%252C%2520GPT-5%2520reached%252032.3%2525%2520abnormality%2520detection%250Aand%252055.0%2525%2520malignancy%2520accuracy.%2520On%2520CBIS-DDSM%252C%2520it%2520achieved%252069.3%2525%2520BI-RADS%250Aaccuracy%252C%252066.0%2525%2520abnormality%2520detection%252C%2520and%252058.2%2525%2520malignancy%2520accuracy.%2520Compared%250Awith%2520human%2520expert%2520estimations%252C%2520GPT-5%2520exhibited%2520lower%2520sensitivity%2520%252863.5%2525%2529%2520and%250Aspecificity%2520%252852.3%2525%2529.%2520While%2520GPT-5%2520exhibits%2520promising%2520capabilities%2520for%2520screening%250Atasks%252C%2520its%2520performance%2520remains%2520insufficient%2520for%2520high-stakes%2520clinical%2520imaging%250Aapplications%2520without%2520targeted%2520domain%2520adaptation%2520and%2520optimization.%2520However%252C%2520the%250Atremendous%2520improvements%2520in%2520performance%2520from%2520GPT-4o%2520to%2520GPT-5%2520show%2520a%2520promising%250Atrend%2520in%2520the%2520potential%2520for%2520general%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520assist%2520with%250Amammography%2520VQA%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20ChatGPT-5%20Ready%20for%20Mammogram%20VQA%3F&entry.906535625=Qiang%20Li%20and%20Shansong%20Wang%20and%20Mingzhe%20Hu%20and%20Mojtaba%20Safari%20and%20Zachary%20Eidex%20and%20Xiaofeng%20Yang&entry.1292438233=%20%20Mammogram%20visual%20question%20answering%20%28VQA%29%20integrates%20image%20interpretation%0Awith%20clinical%20reasoning%20and%20has%20potential%20to%20support%20breast%20cancer%20screening.%0AWe%20systematically%20evaluated%20the%20GPT-5%20family%20and%20GPT-4o%20model%20on%20four%20public%0Amammography%20datasets%20%28EMBED%2C%20InBreast%2C%20CMMD%2C%20CBIS-DDSM%29%20for%20BI-RADS%20assessment%2C%0Aabnormality%20detection%2C%20and%20malignancy%20classification%20tasks.%20GPT-5%20consistently%0Awas%20the%20best%20performing%20model%20but%20lagged%20behind%20both%20human%20experts%20and%0Adomain-specific%20fine-tuned%20models.%20On%20EMBED%2C%20GPT-5%20achieved%20the%20highest%20scores%0Aamong%20GPT%20variants%20in%20density%20%2856.8%25%29%2C%20distortion%20%2852.5%25%29%2C%20mass%20%2864.5%25%29%2C%0Acalcification%20%2863.5%25%29%2C%20and%20malignancy%20%2852.8%25%29%20classification.%20On%20InBreast%2C%20it%0Aattained%2036.9%25%20BI-RADS%20accuracy%2C%2045.9%25%20abnormality%20detection%2C%20and%2035.0%25%0Amalignancy%20classification.%20On%20CMMD%2C%20GPT-5%20reached%2032.3%25%20abnormality%20detection%0Aand%2055.0%25%20malignancy%20accuracy.%20On%20CBIS-DDSM%2C%20it%20achieved%2069.3%25%20BI-RADS%0Aaccuracy%2C%2066.0%25%20abnormality%20detection%2C%20and%2058.2%25%20malignancy%20accuracy.%20Compared%0Awith%20human%20expert%20estimations%2C%20GPT-5%20exhibited%20lower%20sensitivity%20%2863.5%25%29%20and%0Aspecificity%20%2852.3%25%29.%20While%20GPT-5%20exhibits%20promising%20capabilities%20for%20screening%0Atasks%2C%20its%20performance%20remains%20insufficient%20for%20high-stakes%20clinical%20imaging%0Aapplications%20without%20targeted%20domain%20adaptation%20and%20optimization.%20However%2C%20the%0Atremendous%20improvements%20in%20performance%20from%20GPT-4o%20to%20GPT-5%20show%20a%20promising%0Atrend%20in%20the%20potential%20for%20general%20large%20language%20models%20%28LLMs%29%20to%20assist%20with%0Amammography%20VQA%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11628v1&entry.124074799=Read"},
{"title": "Training-Free Anomaly Generation via Dual-Attention Enhancement in\n  Diffusion Model", "author": "Zuo Zuo and Jiahao Dong and Yanyun Qu and Zongze Wu", "abstract": "  Industrial anomaly detection (AD) plays a significant role in manufacturing\nwhere a long-standing challenge is data scarcity. A growing body of works have\nemerged to address insufficient anomaly data via anomaly generation. However,\nthese anomaly generation methods suffer from lack of fidelity or need to be\ntrained with extra data. To this end, we propose a training-free anomaly\ngeneration framework dubbed AAG, which is based on Stable Diffusion (SD)'s\nstrong generation ability for effective anomaly image generation. Given a\nnormal image, mask and a simple text prompt, AAG can generate realistic and\nnatural anomalies in the specific regions and simultaneously keep contents in\nother regions unchanged. In particular, we propose Cross-Attention Enhancement\n(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion\nbased on the given mask. CAE increases the similarity between visual tokens in\nspecific regions and text embeddings, which guides these generated visual\ntokens in accordance with the text description. Besides, generated anomalies\nneed to be more natural and plausible with object in given image. We propose\nSelf-Attention Enhancement (SAE) which improves similarity between each normal\nvisual token and anomaly visual tokens. SAE ensures that generated anomalies\nare coherent with original pattern. Extensive experiments on MVTec AD and VisA\ndatasets demonstrate effectiveness of AAG in anomaly generation and its\nutility. Furthermore, anomaly images generated by AAG can bolster performance\nof various downstream anomaly inspection tasks.\n", "link": "http://arxiv.org/abs/2508.11550v1", "date": "2025-08-15", "relevancy": 2.3118, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5921}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5807}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Anomaly%20Generation%20via%20Dual-Attention%20Enhancement%20in%0A%20%20Diffusion%20Model&body=Title%3A%20Training-Free%20Anomaly%20Generation%20via%20Dual-Attention%20Enhancement%20in%0A%20%20Diffusion%20Model%0AAuthor%3A%20Zuo%20Zuo%20and%20Jiahao%20Dong%20and%20Yanyun%20Qu%20and%20Zongze%20Wu%0AAbstract%3A%20%20%20Industrial%20anomaly%20detection%20%28AD%29%20plays%20a%20significant%20role%20in%20manufacturing%0Awhere%20a%20long-standing%20challenge%20is%20data%20scarcity.%20A%20growing%20body%20of%20works%20have%0Aemerged%20to%20address%20insufficient%20anomaly%20data%20via%20anomaly%20generation.%20However%2C%0Athese%20anomaly%20generation%20methods%20suffer%20from%20lack%20of%20fidelity%20or%20need%20to%20be%0Atrained%20with%20extra%20data.%20To%20this%20end%2C%20we%20propose%20a%20training-free%20anomaly%0Ageneration%20framework%20dubbed%20AAG%2C%20which%20is%20based%20on%20Stable%20Diffusion%20%28SD%29%27s%0Astrong%20generation%20ability%20for%20effective%20anomaly%20image%20generation.%20Given%20a%0Anormal%20image%2C%20mask%20and%20a%20simple%20text%20prompt%2C%20AAG%20can%20generate%20realistic%20and%0Anatural%20anomalies%20in%20the%20specific%20regions%20and%20simultaneously%20keep%20contents%20in%0Aother%20regions%20unchanged.%20In%20particular%2C%20we%20propose%20Cross-Attention%20Enhancement%0A%28CAE%29%20to%20re-engineer%20the%20cross-attention%20mechanism%20within%20Stable%20Diffusion%0Abased%20on%20the%20given%20mask.%20CAE%20increases%20the%20similarity%20between%20visual%20tokens%20in%0Aspecific%20regions%20and%20text%20embeddings%2C%20which%20guides%20these%20generated%20visual%0Atokens%20in%20accordance%20with%20the%20text%20description.%20Besides%2C%20generated%20anomalies%0Aneed%20to%20be%20more%20natural%20and%20plausible%20with%20object%20in%20given%20image.%20We%20propose%0ASelf-Attention%20Enhancement%20%28SAE%29%20which%20improves%20similarity%20between%20each%20normal%0Avisual%20token%20and%20anomaly%20visual%20tokens.%20SAE%20ensures%20that%20generated%20anomalies%0Aare%20coherent%20with%20original%20pattern.%20Extensive%20experiments%20on%20MVTec%20AD%20and%20VisA%0Adatasets%20demonstrate%20effectiveness%20of%20AAG%20in%20anomaly%20generation%20and%20its%0Autility.%20Furthermore%2C%20anomaly%20images%20generated%20by%20AAG%20can%20bolster%20performance%0Aof%20various%20downstream%20anomaly%20inspection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Anomaly%2520Generation%2520via%2520Dual-Attention%2520Enhancement%2520in%250A%2520%2520Diffusion%2520Model%26entry.906535625%3DZuo%2520Zuo%2520and%2520Jiahao%2520Dong%2520and%2520Yanyun%2520Qu%2520and%2520Zongze%2520Wu%26entry.1292438233%3D%2520%2520Industrial%2520anomaly%2520detection%2520%2528AD%2529%2520plays%2520a%2520significant%2520role%2520in%2520manufacturing%250Awhere%2520a%2520long-standing%2520challenge%2520is%2520data%2520scarcity.%2520A%2520growing%2520body%2520of%2520works%2520have%250Aemerged%2520to%2520address%2520insufficient%2520anomaly%2520data%2520via%2520anomaly%2520generation.%2520However%252C%250Athese%2520anomaly%2520generation%2520methods%2520suffer%2520from%2520lack%2520of%2520fidelity%2520or%2520need%2520to%2520be%250Atrained%2520with%2520extra%2520data.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520training-free%2520anomaly%250Ageneration%2520framework%2520dubbed%2520AAG%252C%2520which%2520is%2520based%2520on%2520Stable%2520Diffusion%2520%2528SD%2529%2527s%250Astrong%2520generation%2520ability%2520for%2520effective%2520anomaly%2520image%2520generation.%2520Given%2520a%250Anormal%2520image%252C%2520mask%2520and%2520a%2520simple%2520text%2520prompt%252C%2520AAG%2520can%2520generate%2520realistic%2520and%250Anatural%2520anomalies%2520in%2520the%2520specific%2520regions%2520and%2520simultaneously%2520keep%2520contents%2520in%250Aother%2520regions%2520unchanged.%2520In%2520particular%252C%2520we%2520propose%2520Cross-Attention%2520Enhancement%250A%2528CAE%2529%2520to%2520re-engineer%2520the%2520cross-attention%2520mechanism%2520within%2520Stable%2520Diffusion%250Abased%2520on%2520the%2520given%2520mask.%2520CAE%2520increases%2520the%2520similarity%2520between%2520visual%2520tokens%2520in%250Aspecific%2520regions%2520and%2520text%2520embeddings%252C%2520which%2520guides%2520these%2520generated%2520visual%250Atokens%2520in%2520accordance%2520with%2520the%2520text%2520description.%2520Besides%252C%2520generated%2520anomalies%250Aneed%2520to%2520be%2520more%2520natural%2520and%2520plausible%2520with%2520object%2520in%2520given%2520image.%2520We%2520propose%250ASelf-Attention%2520Enhancement%2520%2528SAE%2529%2520which%2520improves%2520similarity%2520between%2520each%2520normal%250Avisual%2520token%2520and%2520anomaly%2520visual%2520tokens.%2520SAE%2520ensures%2520that%2520generated%2520anomalies%250Aare%2520coherent%2520with%2520original%2520pattern.%2520Extensive%2520experiments%2520on%2520MVTec%2520AD%2520and%2520VisA%250Adatasets%2520demonstrate%2520effectiveness%2520of%2520AAG%2520in%2520anomaly%2520generation%2520and%2520its%250Autility.%2520Furthermore%252C%2520anomaly%2520images%2520generated%2520by%2520AAG%2520can%2520bolster%2520performance%250Aof%2520various%2520downstream%2520anomaly%2520inspection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Anomaly%20Generation%20via%20Dual-Attention%20Enhancement%20in%0A%20%20Diffusion%20Model&entry.906535625=Zuo%20Zuo%20and%20Jiahao%20Dong%20and%20Yanyun%20Qu%20and%20Zongze%20Wu&entry.1292438233=%20%20Industrial%20anomaly%20detection%20%28AD%29%20plays%20a%20significant%20role%20in%20manufacturing%0Awhere%20a%20long-standing%20challenge%20is%20data%20scarcity.%20A%20growing%20body%20of%20works%20have%0Aemerged%20to%20address%20insufficient%20anomaly%20data%20via%20anomaly%20generation.%20However%2C%0Athese%20anomaly%20generation%20methods%20suffer%20from%20lack%20of%20fidelity%20or%20need%20to%20be%0Atrained%20with%20extra%20data.%20To%20this%20end%2C%20we%20propose%20a%20training-free%20anomaly%0Ageneration%20framework%20dubbed%20AAG%2C%20which%20is%20based%20on%20Stable%20Diffusion%20%28SD%29%27s%0Astrong%20generation%20ability%20for%20effective%20anomaly%20image%20generation.%20Given%20a%0Anormal%20image%2C%20mask%20and%20a%20simple%20text%20prompt%2C%20AAG%20can%20generate%20realistic%20and%0Anatural%20anomalies%20in%20the%20specific%20regions%20and%20simultaneously%20keep%20contents%20in%0Aother%20regions%20unchanged.%20In%20particular%2C%20we%20propose%20Cross-Attention%20Enhancement%0A%28CAE%29%20to%20re-engineer%20the%20cross-attention%20mechanism%20within%20Stable%20Diffusion%0Abased%20on%20the%20given%20mask.%20CAE%20increases%20the%20similarity%20between%20visual%20tokens%20in%0Aspecific%20regions%20and%20text%20embeddings%2C%20which%20guides%20these%20generated%20visual%0Atokens%20in%20accordance%20with%20the%20text%20description.%20Besides%2C%20generated%20anomalies%0Aneed%20to%20be%20more%20natural%20and%20plausible%20with%20object%20in%20given%20image.%20We%20propose%0ASelf-Attention%20Enhancement%20%28SAE%29%20which%20improves%20similarity%20between%20each%20normal%0Avisual%20token%20and%20anomaly%20visual%20tokens.%20SAE%20ensures%20that%20generated%20anomalies%0Aare%20coherent%20with%20original%20pattern.%20Extensive%20experiments%20on%20MVTec%20AD%20and%20VisA%0Adatasets%20demonstrate%20effectiveness%20of%20AAG%20in%20anomaly%20generation%20and%20its%0Autility.%20Furthermore%2C%20anomaly%20images%20generated%20by%20AAG%20can%20bolster%20performance%0Aof%20various%20downstream%20anomaly%20inspection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11550v1&entry.124074799=Read"},
{"title": "PhysLab: A Benchmark Dataset for Multi-Granularity Visual Parsing of\n  Physics Experiments", "author": "Minghao Zou and Qingtian Zeng and Yongping Miao and Shangkun Liu and Zilong Wang and Hantao Liu and Wei Zhou", "abstract": "  Visual parsing of images and videos is critical for a wide range of\nreal-world applications. However, progress in this field is constrained by\nlimitations of existing datasets: (1) insufficient annotation granularity,\nwhich impedes fine-grained scene understanding and high-level reasoning; (2)\nlimited coverage of domains, particularly a lack of datasets tailored for\neducational scenarios; and (3) lack of explicit procedural guidance, with\nminimal logical rules and insufficient representation of structured task\nprocess. To address these gaps, we introduce PhysLab, the first video dataset\nthat captures students conducting complex physics experiments. The dataset\nincludes four representative experiments that feature diverse scientific\ninstruments and rich human-object interaction (HOI) patterns. PhysLab comprises\n620 long-form videos and provides multilevel annotations that support a variety\nof vision tasks, including action recognition, object detection, HOI analysis,\netc. We establish strong baselines and perform extensive evaluations to\nhighlight key challenges in the parsing of procedural educational videos. We\nexpect PhysLab to serve as a valuable resource for advancing fine-grained\nvisual parsing, facilitating intelligent classroom systems, and fostering\ncloser integration between computer vision and educational technologies. The\ndataset and the evaluation toolkit are publicly available at\nhttps://github.com/ZMH-SDUST/PhysLab.\n", "link": "http://arxiv.org/abs/2506.06631v2", "date": "2025-08-15", "relevancy": 2.3035, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6123}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysLab%3A%20A%20Benchmark%20Dataset%20for%20Multi-Granularity%20Visual%20Parsing%20of%0A%20%20Physics%20Experiments&body=Title%3A%20PhysLab%3A%20A%20Benchmark%20Dataset%20for%20Multi-Granularity%20Visual%20Parsing%20of%0A%20%20Physics%20Experiments%0AAuthor%3A%20Minghao%20Zou%20and%20Qingtian%20Zeng%20and%20Yongping%20Miao%20and%20Shangkun%20Liu%20and%20Zilong%20Wang%20and%20Hantao%20Liu%20and%20Wei%20Zhou%0AAbstract%3A%20%20%20Visual%20parsing%20of%20images%20and%20videos%20is%20critical%20for%20a%20wide%20range%20of%0Areal-world%20applications.%20However%2C%20progress%20in%20this%20field%20is%20constrained%20by%0Alimitations%20of%20existing%20datasets%3A%20%281%29%20insufficient%20annotation%20granularity%2C%0Awhich%20impedes%20fine-grained%20scene%20understanding%20and%20high-level%20reasoning%3B%20%282%29%0Alimited%20coverage%20of%20domains%2C%20particularly%20a%20lack%20of%20datasets%20tailored%20for%0Aeducational%20scenarios%3B%20and%20%283%29%20lack%20of%20explicit%20procedural%20guidance%2C%20with%0Aminimal%20logical%20rules%20and%20insufficient%20representation%20of%20structured%20task%0Aprocess.%20To%20address%20these%20gaps%2C%20we%20introduce%20PhysLab%2C%20the%20first%20video%20dataset%0Athat%20captures%20students%20conducting%20complex%20physics%20experiments.%20The%20dataset%0Aincludes%20four%20representative%20experiments%20that%20feature%20diverse%20scientific%0Ainstruments%20and%20rich%20human-object%20interaction%20%28HOI%29%20patterns.%20PhysLab%20comprises%0A620%20long-form%20videos%20and%20provides%20multilevel%20annotations%20that%20support%20a%20variety%0Aof%20vision%20tasks%2C%20including%20action%20recognition%2C%20object%20detection%2C%20HOI%20analysis%2C%0Aetc.%20We%20establish%20strong%20baselines%20and%20perform%20extensive%20evaluations%20to%0Ahighlight%20key%20challenges%20in%20the%20parsing%20of%20procedural%20educational%20videos.%20We%0Aexpect%20PhysLab%20to%20serve%20as%20a%20valuable%20resource%20for%20advancing%20fine-grained%0Avisual%20parsing%2C%20facilitating%20intelligent%20classroom%20systems%2C%20and%20fostering%0Acloser%20integration%20between%20computer%20vision%20and%20educational%20technologies.%20The%0Adataset%20and%20the%20evaluation%20toolkit%20are%20publicly%20available%20at%0Ahttps%3A//github.com/ZMH-SDUST/PhysLab.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysLab%253A%2520A%2520Benchmark%2520Dataset%2520for%2520Multi-Granularity%2520Visual%2520Parsing%2520of%250A%2520%2520Physics%2520Experiments%26entry.906535625%3DMinghao%2520Zou%2520and%2520Qingtian%2520Zeng%2520and%2520Yongping%2520Miao%2520and%2520Shangkun%2520Liu%2520and%2520Zilong%2520Wang%2520and%2520Hantao%2520Liu%2520and%2520Wei%2520Zhou%26entry.1292438233%3D%2520%2520Visual%2520parsing%2520of%2520images%2520and%2520videos%2520is%2520critical%2520for%2520a%2520wide%2520range%2520of%250Areal-world%2520applications.%2520However%252C%2520progress%2520in%2520this%2520field%2520is%2520constrained%2520by%250Alimitations%2520of%2520existing%2520datasets%253A%2520%25281%2529%2520insufficient%2520annotation%2520granularity%252C%250Awhich%2520impedes%2520fine-grained%2520scene%2520understanding%2520and%2520high-level%2520reasoning%253B%2520%25282%2529%250Alimited%2520coverage%2520of%2520domains%252C%2520particularly%2520a%2520lack%2520of%2520datasets%2520tailored%2520for%250Aeducational%2520scenarios%253B%2520and%2520%25283%2529%2520lack%2520of%2520explicit%2520procedural%2520guidance%252C%2520with%250Aminimal%2520logical%2520rules%2520and%2520insufficient%2520representation%2520of%2520structured%2520task%250Aprocess.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520PhysLab%252C%2520the%2520first%2520video%2520dataset%250Athat%2520captures%2520students%2520conducting%2520complex%2520physics%2520experiments.%2520The%2520dataset%250Aincludes%2520four%2520representative%2520experiments%2520that%2520feature%2520diverse%2520scientific%250Ainstruments%2520and%2520rich%2520human-object%2520interaction%2520%2528HOI%2529%2520patterns.%2520PhysLab%2520comprises%250A620%2520long-form%2520videos%2520and%2520provides%2520multilevel%2520annotations%2520that%2520support%2520a%2520variety%250Aof%2520vision%2520tasks%252C%2520including%2520action%2520recognition%252C%2520object%2520detection%252C%2520HOI%2520analysis%252C%250Aetc.%2520We%2520establish%2520strong%2520baselines%2520and%2520perform%2520extensive%2520evaluations%2520to%250Ahighlight%2520key%2520challenges%2520in%2520the%2520parsing%2520of%2520procedural%2520educational%2520videos.%2520We%250Aexpect%2520PhysLab%2520to%2520serve%2520as%2520a%2520valuable%2520resource%2520for%2520advancing%2520fine-grained%250Avisual%2520parsing%252C%2520facilitating%2520intelligent%2520classroom%2520systems%252C%2520and%2520fostering%250Acloser%2520integration%2520between%2520computer%2520vision%2520and%2520educational%2520technologies.%2520The%250Adataset%2520and%2520the%2520evaluation%2520toolkit%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/ZMH-SDUST/PhysLab.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysLab%3A%20A%20Benchmark%20Dataset%20for%20Multi-Granularity%20Visual%20Parsing%20of%0A%20%20Physics%20Experiments&entry.906535625=Minghao%20Zou%20and%20Qingtian%20Zeng%20and%20Yongping%20Miao%20and%20Shangkun%20Liu%20and%20Zilong%20Wang%20and%20Hantao%20Liu%20and%20Wei%20Zhou&entry.1292438233=%20%20Visual%20parsing%20of%20images%20and%20videos%20is%20critical%20for%20a%20wide%20range%20of%0Areal-world%20applications.%20However%2C%20progress%20in%20this%20field%20is%20constrained%20by%0Alimitations%20of%20existing%20datasets%3A%20%281%29%20insufficient%20annotation%20granularity%2C%0Awhich%20impedes%20fine-grained%20scene%20understanding%20and%20high-level%20reasoning%3B%20%282%29%0Alimited%20coverage%20of%20domains%2C%20particularly%20a%20lack%20of%20datasets%20tailored%20for%0Aeducational%20scenarios%3B%20and%20%283%29%20lack%20of%20explicit%20procedural%20guidance%2C%20with%0Aminimal%20logical%20rules%20and%20insufficient%20representation%20of%20structured%20task%0Aprocess.%20To%20address%20these%20gaps%2C%20we%20introduce%20PhysLab%2C%20the%20first%20video%20dataset%0Athat%20captures%20students%20conducting%20complex%20physics%20experiments.%20The%20dataset%0Aincludes%20four%20representative%20experiments%20that%20feature%20diverse%20scientific%0Ainstruments%20and%20rich%20human-object%20interaction%20%28HOI%29%20patterns.%20PhysLab%20comprises%0A620%20long-form%20videos%20and%20provides%20multilevel%20annotations%20that%20support%20a%20variety%0Aof%20vision%20tasks%2C%20including%20action%20recognition%2C%20object%20detection%2C%20HOI%20analysis%2C%0Aetc.%20We%20establish%20strong%20baselines%20and%20perform%20extensive%20evaluations%20to%0Ahighlight%20key%20challenges%20in%20the%20parsing%20of%20procedural%20educational%20videos.%20We%0Aexpect%20PhysLab%20to%20serve%20as%20a%20valuable%20resource%20for%20advancing%20fine-grained%0Avisual%20parsing%2C%20facilitating%20intelligent%20classroom%20systems%2C%20and%20fostering%0Acloser%20integration%20between%20computer%20vision%20and%20educational%20technologies.%20The%0Adataset%20and%20the%20evaluation%20toolkit%20are%20publicly%20available%20at%0Ahttps%3A//github.com/ZMH-SDUST/PhysLab.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06631v2&entry.124074799=Read"},
{"title": "RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator", "author": "Zhiming Liu and Nantheera Anantrasirichai", "abstract": "  Atmospheric turbulence severely degrades video quality by introducing\ndistortions such as geometric warping, blur, and temporal flickering, posing\nsignificant challenges to both visual clarity and temporal consistency. Current\nstate-of-the-art methods are based on transformer and 3D architectures and\nrequire multi-frame input, but their large computational cost and memory usage\nlimit real-time deployment, especially in resource-constrained scenarios. In\nthis work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric\nTurbulence Mitigator, designed for efficient and temporally consistent video\nrestoration under AT conditions. RMFAT adopts a lightweight recurrent framework\nthat restores each frame using only two inputs at a time, significantly\nreducing temporal window size and computational burden. It further integrates\nmulti-scale feature encoding and decoding with temporal warping modules at both\nencoder and decoder stages to enhance spatial detail and temporal coherence.\nExtensive experiments on synthetic and real-world atmospheric turbulence\ndatasets demonstrate that RMFAT not only outperforms existing methods in terms\nof clarity restoration (with nearly a 9\\% improvement in SSIM) but also\nachieves significantly improved inference speed (more than a fourfold reduction\nin runtime), making it particularly suitable for real-time atmospheric\nturbulence suppression tasks.\n", "link": "http://arxiv.org/abs/2508.11409v1", "date": "2025-08-15", "relevancy": 2.2967, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5953}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5827}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RMFAT%3A%20Recurrent%20Multi-scale%20Feature%20Atmospheric%20Turbulence%20Mitigator&body=Title%3A%20RMFAT%3A%20Recurrent%20Multi-scale%20Feature%20Atmospheric%20Turbulence%20Mitigator%0AAuthor%3A%20Zhiming%20Liu%20and%20Nantheera%20Anantrasirichai%0AAbstract%3A%20%20%20Atmospheric%20turbulence%20severely%20degrades%20video%20quality%20by%20introducing%0Adistortions%20such%20as%20geometric%20warping%2C%20blur%2C%20and%20temporal%20flickering%2C%20posing%0Asignificant%20challenges%20to%20both%20visual%20clarity%20and%20temporal%20consistency.%20Current%0Astate-of-the-art%20methods%20are%20based%20on%20transformer%20and%203D%20architectures%20and%0Arequire%20multi-frame%20input%2C%20but%20their%20large%20computational%20cost%20and%20memory%20usage%0Alimit%20real-time%20deployment%2C%20especially%20in%20resource-constrained%20scenarios.%20In%0Athis%20work%2C%20we%20propose%20RMFAT%3A%20Recurrent%20Multi-scale%20Feature%20Atmospheric%0ATurbulence%20Mitigator%2C%20designed%20for%20efficient%20and%20temporally%20consistent%20video%0Arestoration%20under%20AT%20conditions.%20RMFAT%20adopts%20a%20lightweight%20recurrent%20framework%0Athat%20restores%20each%20frame%20using%20only%20two%20inputs%20at%20a%20time%2C%20significantly%0Areducing%20temporal%20window%20size%20and%20computational%20burden.%20It%20further%20integrates%0Amulti-scale%20feature%20encoding%20and%20decoding%20with%20temporal%20warping%20modules%20at%20both%0Aencoder%20and%20decoder%20stages%20to%20enhance%20spatial%20detail%20and%20temporal%20coherence.%0AExtensive%20experiments%20on%20synthetic%20and%20real-world%20atmospheric%20turbulence%0Adatasets%20demonstrate%20that%20RMFAT%20not%20only%20outperforms%20existing%20methods%20in%20terms%0Aof%20clarity%20restoration%20%28with%20nearly%20a%209%5C%25%20improvement%20in%20SSIM%29%20but%20also%0Aachieves%20significantly%20improved%20inference%20speed%20%28more%20than%20a%20fourfold%20reduction%0Ain%20runtime%29%2C%20making%20it%20particularly%20suitable%20for%20real-time%20atmospheric%0Aturbulence%20suppression%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRMFAT%253A%2520Recurrent%2520Multi-scale%2520Feature%2520Atmospheric%2520Turbulence%2520Mitigator%26entry.906535625%3DZhiming%2520Liu%2520and%2520Nantheera%2520Anantrasirichai%26entry.1292438233%3D%2520%2520Atmospheric%2520turbulence%2520severely%2520degrades%2520video%2520quality%2520by%2520introducing%250Adistortions%2520such%2520as%2520geometric%2520warping%252C%2520blur%252C%2520and%2520temporal%2520flickering%252C%2520posing%250Asignificant%2520challenges%2520to%2520both%2520visual%2520clarity%2520and%2520temporal%2520consistency.%2520Current%250Astate-of-the-art%2520methods%2520are%2520based%2520on%2520transformer%2520and%25203D%2520architectures%2520and%250Arequire%2520multi-frame%2520input%252C%2520but%2520their%2520large%2520computational%2520cost%2520and%2520memory%2520usage%250Alimit%2520real-time%2520deployment%252C%2520especially%2520in%2520resource-constrained%2520scenarios.%2520In%250Athis%2520work%252C%2520we%2520propose%2520RMFAT%253A%2520Recurrent%2520Multi-scale%2520Feature%2520Atmospheric%250ATurbulence%2520Mitigator%252C%2520designed%2520for%2520efficient%2520and%2520temporally%2520consistent%2520video%250Arestoration%2520under%2520AT%2520conditions.%2520RMFAT%2520adopts%2520a%2520lightweight%2520recurrent%2520framework%250Athat%2520restores%2520each%2520frame%2520using%2520only%2520two%2520inputs%2520at%2520a%2520time%252C%2520significantly%250Areducing%2520temporal%2520window%2520size%2520and%2520computational%2520burden.%2520It%2520further%2520integrates%250Amulti-scale%2520feature%2520encoding%2520and%2520decoding%2520with%2520temporal%2520warping%2520modules%2520at%2520both%250Aencoder%2520and%2520decoder%2520stages%2520to%2520enhance%2520spatial%2520detail%2520and%2520temporal%2520coherence.%250AExtensive%2520experiments%2520on%2520synthetic%2520and%2520real-world%2520atmospheric%2520turbulence%250Adatasets%2520demonstrate%2520that%2520RMFAT%2520not%2520only%2520outperforms%2520existing%2520methods%2520in%2520terms%250Aof%2520clarity%2520restoration%2520%2528with%2520nearly%2520a%25209%255C%2525%2520improvement%2520in%2520SSIM%2529%2520but%2520also%250Aachieves%2520significantly%2520improved%2520inference%2520speed%2520%2528more%2520than%2520a%2520fourfold%2520reduction%250Ain%2520runtime%2529%252C%2520making%2520it%2520particularly%2520suitable%2520for%2520real-time%2520atmospheric%250Aturbulence%2520suppression%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RMFAT%3A%20Recurrent%20Multi-scale%20Feature%20Atmospheric%20Turbulence%20Mitigator&entry.906535625=Zhiming%20Liu%20and%20Nantheera%20Anantrasirichai&entry.1292438233=%20%20Atmospheric%20turbulence%20severely%20degrades%20video%20quality%20by%20introducing%0Adistortions%20such%20as%20geometric%20warping%2C%20blur%2C%20and%20temporal%20flickering%2C%20posing%0Asignificant%20challenges%20to%20both%20visual%20clarity%20and%20temporal%20consistency.%20Current%0Astate-of-the-art%20methods%20are%20based%20on%20transformer%20and%203D%20architectures%20and%0Arequire%20multi-frame%20input%2C%20but%20their%20large%20computational%20cost%20and%20memory%20usage%0Alimit%20real-time%20deployment%2C%20especially%20in%20resource-constrained%20scenarios.%20In%0Athis%20work%2C%20we%20propose%20RMFAT%3A%20Recurrent%20Multi-scale%20Feature%20Atmospheric%0ATurbulence%20Mitigator%2C%20designed%20for%20efficient%20and%20temporally%20consistent%20video%0Arestoration%20under%20AT%20conditions.%20RMFAT%20adopts%20a%20lightweight%20recurrent%20framework%0Athat%20restores%20each%20frame%20using%20only%20two%20inputs%20at%20a%20time%2C%20significantly%0Areducing%20temporal%20window%20size%20and%20computational%20burden.%20It%20further%20integrates%0Amulti-scale%20feature%20encoding%20and%20decoding%20with%20temporal%20warping%20modules%20at%20both%0Aencoder%20and%20decoder%20stages%20to%20enhance%20spatial%20detail%20and%20temporal%20coherence.%0AExtensive%20experiments%20on%20synthetic%20and%20real-world%20atmospheric%20turbulence%0Adatasets%20demonstrate%20that%20RMFAT%20not%20only%20outperforms%20existing%20methods%20in%20terms%0Aof%20clarity%20restoration%20%28with%20nearly%20a%209%5C%25%20improvement%20in%20SSIM%29%20but%20also%0Aachieves%20significantly%20improved%20inference%20speed%20%28more%20than%20a%20fourfold%20reduction%0Ain%20runtime%29%2C%20making%20it%20particularly%20suitable%20for%20real-time%20atmospheric%0Aturbulence%20suppression%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11409v1&entry.124074799=Read"},
{"title": "MM-R1: Unleashing the Power of Unified Multimodal Large Language Models\n  for Personalized Image Generation", "author": "Qian Liang and Yujia Wu and Kuncheng Li and Jiwei Wei and Shiyuan He and Jinyu Guo and Ning Xie", "abstract": "  Multimodal Large Language Models (MLLMs) with unified architectures excel\nacross a wide range of vision-language tasks, yet aligning them with\npersonalized image generation remains a significant challenge. Existing methods\nfor MLLMs are frequently subject-specific, demanding a data-intensive\nfine-tuning process for every new subject, which limits their scalability. In\nthis paper, we introduce MM-R1, a framework that integrates a cross-modal\nChain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of\nunified MLLMs for personalized image generation. Specifically, we structure\npersonalization as an integrated visual reasoning and generation process: (1)\ngrounding subject concepts by interpreting and understanding user-provided\nimages and contextual cues, and (2) generating personalized images conditioned\non both the extracted subject representations and user prompts. To further\nenhance the reasoning capability, we adopt Grouped Reward Proximal Policy\nOptimization (GRPO) to explicitly align the generation. Experiments demonstrate\nthat MM-R1 unleashes the personalization capability of unified MLLMs to\ngenerate images with high subject fidelity and strong text alignment in a\nzero-shot manner.\n", "link": "http://arxiv.org/abs/2508.11433v1", "date": "2025-08-15", "relevancy": 2.2893, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5846}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5645}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-R1%3A%20Unleashing%20the%20Power%20of%20Unified%20Multimodal%20Large%20Language%20Models%0A%20%20for%20Personalized%20Image%20Generation&body=Title%3A%20MM-R1%3A%20Unleashing%20the%20Power%20of%20Unified%20Multimodal%20Large%20Language%20Models%0A%20%20for%20Personalized%20Image%20Generation%0AAuthor%3A%20Qian%20Liang%20and%20Yujia%20Wu%20and%20Kuncheng%20Li%20and%20Jiwei%20Wei%20and%20Shiyuan%20He%20and%20Jinyu%20Guo%20and%20Ning%20Xie%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20with%20unified%20architectures%20excel%0Aacross%20a%20wide%20range%20of%20vision-language%20tasks%2C%20yet%20aligning%20them%20with%0Apersonalized%20image%20generation%20remains%20a%20significant%20challenge.%20Existing%20methods%0Afor%20MLLMs%20are%20frequently%20subject-specific%2C%20demanding%20a%20data-intensive%0Afine-tuning%20process%20for%20every%20new%20subject%2C%20which%20limits%20their%20scalability.%20In%0Athis%20paper%2C%20we%20introduce%20MM-R1%2C%20a%20framework%20that%20integrates%20a%20cross-modal%0AChain-of-Thought%20%28X-CoT%29%20reasoning%20strategy%20to%20unlock%20the%20inherent%20potential%20of%0Aunified%20MLLMs%20for%20personalized%20image%20generation.%20Specifically%2C%20we%20structure%0Apersonalization%20as%20an%20integrated%20visual%20reasoning%20and%20generation%20process%3A%20%281%29%0Agrounding%20subject%20concepts%20by%20interpreting%20and%20understanding%20user-provided%0Aimages%20and%20contextual%20cues%2C%20and%20%282%29%20generating%20personalized%20images%20conditioned%0Aon%20both%20the%20extracted%20subject%20representations%20and%20user%20prompts.%20To%20further%0Aenhance%20the%20reasoning%20capability%2C%20we%20adopt%20Grouped%20Reward%20Proximal%20Policy%0AOptimization%20%28GRPO%29%20to%20explicitly%20align%20the%20generation.%20Experiments%20demonstrate%0Athat%20MM-R1%20unleashes%20the%20personalization%20capability%20of%20unified%20MLLMs%20to%0Agenerate%20images%20with%20high%20subject%20fidelity%20and%20strong%20text%20alignment%20in%20a%0Azero-shot%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-R1%253A%2520Unleashing%2520the%2520Power%2520of%2520Unified%2520Multimodal%2520Large%2520Language%2520Models%250A%2520%2520for%2520Personalized%2520Image%2520Generation%26entry.906535625%3DQian%2520Liang%2520and%2520Yujia%2520Wu%2520and%2520Kuncheng%2520Li%2520and%2520Jiwei%2520Wei%2520and%2520Shiyuan%2520He%2520and%2520Jinyu%2520Guo%2520and%2520Ning%2520Xie%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520with%2520unified%2520architectures%2520excel%250Aacross%2520a%2520wide%2520range%2520of%2520vision-language%2520tasks%252C%2520yet%2520aligning%2520them%2520with%250Apersonalized%2520image%2520generation%2520remains%2520a%2520significant%2520challenge.%2520Existing%2520methods%250Afor%2520MLLMs%2520are%2520frequently%2520subject-specific%252C%2520demanding%2520a%2520data-intensive%250Afine-tuning%2520process%2520for%2520every%2520new%2520subject%252C%2520which%2520limits%2520their%2520scalability.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520MM-R1%252C%2520a%2520framework%2520that%2520integrates%2520a%2520cross-modal%250AChain-of-Thought%2520%2528X-CoT%2529%2520reasoning%2520strategy%2520to%2520unlock%2520the%2520inherent%2520potential%2520of%250Aunified%2520MLLMs%2520for%2520personalized%2520image%2520generation.%2520Specifically%252C%2520we%2520structure%250Apersonalization%2520as%2520an%2520integrated%2520visual%2520reasoning%2520and%2520generation%2520process%253A%2520%25281%2529%250Agrounding%2520subject%2520concepts%2520by%2520interpreting%2520and%2520understanding%2520user-provided%250Aimages%2520and%2520contextual%2520cues%252C%2520and%2520%25282%2529%2520generating%2520personalized%2520images%2520conditioned%250Aon%2520both%2520the%2520extracted%2520subject%2520representations%2520and%2520user%2520prompts.%2520To%2520further%250Aenhance%2520the%2520reasoning%2520capability%252C%2520we%2520adopt%2520Grouped%2520Reward%2520Proximal%2520Policy%250AOptimization%2520%2528GRPO%2529%2520to%2520explicitly%2520align%2520the%2520generation.%2520Experiments%2520demonstrate%250Athat%2520MM-R1%2520unleashes%2520the%2520personalization%2520capability%2520of%2520unified%2520MLLMs%2520to%250Agenerate%2520images%2520with%2520high%2520subject%2520fidelity%2520and%2520strong%2520text%2520alignment%2520in%2520a%250Azero-shot%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-R1%3A%20Unleashing%20the%20Power%20of%20Unified%20Multimodal%20Large%20Language%20Models%0A%20%20for%20Personalized%20Image%20Generation&entry.906535625=Qian%20Liang%20and%20Yujia%20Wu%20and%20Kuncheng%20Li%20and%20Jiwei%20Wei%20and%20Shiyuan%20He%20and%20Jinyu%20Guo%20and%20Ning%20Xie&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20with%20unified%20architectures%20excel%0Aacross%20a%20wide%20range%20of%20vision-language%20tasks%2C%20yet%20aligning%20them%20with%0Apersonalized%20image%20generation%20remains%20a%20significant%20challenge.%20Existing%20methods%0Afor%20MLLMs%20are%20frequently%20subject-specific%2C%20demanding%20a%20data-intensive%0Afine-tuning%20process%20for%20every%20new%20subject%2C%20which%20limits%20their%20scalability.%20In%0Athis%20paper%2C%20we%20introduce%20MM-R1%2C%20a%20framework%20that%20integrates%20a%20cross-modal%0AChain-of-Thought%20%28X-CoT%29%20reasoning%20strategy%20to%20unlock%20the%20inherent%20potential%20of%0Aunified%20MLLMs%20for%20personalized%20image%20generation.%20Specifically%2C%20we%20structure%0Apersonalization%20as%20an%20integrated%20visual%20reasoning%20and%20generation%20process%3A%20%281%29%0Agrounding%20subject%20concepts%20by%20interpreting%20and%20understanding%20user-provided%0Aimages%20and%20contextual%20cues%2C%20and%20%282%29%20generating%20personalized%20images%20conditioned%0Aon%20both%20the%20extracted%20subject%20representations%20and%20user%20prompts.%20To%20further%0Aenhance%20the%20reasoning%20capability%2C%20we%20adopt%20Grouped%20Reward%20Proximal%20Policy%0AOptimization%20%28GRPO%29%20to%20explicitly%20align%20the%20generation.%20Experiments%20demonstrate%0Athat%20MM-R1%20unleashes%20the%20personalization%20capability%20of%20unified%20MLLMs%20to%0Agenerate%20images%20with%20high%20subject%20fidelity%20and%20strong%20text%20alignment%20in%20a%0Azero-shot%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11433v1&entry.124074799=Read"},
{"title": "A Systematic Literature Review of Retrieval-Augmented Generation:\n  Techniques, Metrics, and Challenges", "author": "Andrew Brown and Muhammad Roman and Barry Devereux", "abstract": "  This systematic review of the research literature on retrieval-augmented\ngeneration (RAG) provides a focused analysis of the most highly cited studies\npublished between 2020 and May 2025. A total of 128 articles met our inclusion\ncriteria. The records were retrieved from ACM Digital Library, IEEE Xplore,\nScopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).\nRAG couples a neural retriever with a generative language model, grounding\noutput in up-to-date, non-parametric memory while retaining the semantic\ngeneralisation stored in model weights. Guided by the PRISMA 2020 framework, we\n(i) specify explicit inclusion and exclusion criteria based on citation count\nand research questions, (ii) catalogue datasets, architectures, and evaluation\npractices, and (iii) synthesise empirical evidence on the effectiveness and\nlimitations of RAG. To mitigate citation-lag bias, we applied a lower\ncitation-count threshold to papers published in 2025 so that emerging\nbreakthroughs with naturally fewer citations were still captured. This review\nclarifies the current research landscape, highlights methodological gaps, and\ncharts priority directions for future research.\n", "link": "http://arxiv.org/abs/2508.06401v2", "date": "2025-08-15", "relevancy": 2.2855, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4611}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4562}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Systematic%20Literature%20Review%20of%20Retrieval-Augmented%20Generation%3A%0A%20%20Techniques%2C%20Metrics%2C%20and%20Challenges&body=Title%3A%20A%20Systematic%20Literature%20Review%20of%20Retrieval-Augmented%20Generation%3A%0A%20%20Techniques%2C%20Metrics%2C%20and%20Challenges%0AAuthor%3A%20Andrew%20Brown%20and%20Muhammad%20Roman%20and%20Barry%20Devereux%0AAbstract%3A%20%20%20This%20systematic%20review%20of%20the%20research%20literature%20on%20retrieval-augmented%0Ageneration%20%28RAG%29%20provides%20a%20focused%20analysis%20of%20the%20most%20highly%20cited%20studies%0Apublished%20between%202020%20and%20May%202025.%20A%20total%20of%20128%20articles%20met%20our%20inclusion%0Acriteria.%20The%20records%20were%20retrieved%20from%20ACM%20Digital%20Library%2C%20IEEE%20Xplore%2C%0AScopus%2C%20ScienceDirect%2C%20and%20the%20Digital%20Bibliography%20and%20Library%20Project%20%28DBLP%29.%0ARAG%20couples%20a%20neural%20retriever%20with%20a%20generative%20language%20model%2C%20grounding%0Aoutput%20in%20up-to-date%2C%20non-parametric%20memory%20while%20retaining%20the%20semantic%0Ageneralisation%20stored%20in%20model%20weights.%20Guided%20by%20the%20PRISMA%202020%20framework%2C%20we%0A%28i%29%20specify%20explicit%20inclusion%20and%20exclusion%20criteria%20based%20on%20citation%20count%0Aand%20research%20questions%2C%20%28ii%29%20catalogue%20datasets%2C%20architectures%2C%20and%20evaluation%0Apractices%2C%20and%20%28iii%29%20synthesise%20empirical%20evidence%20on%20the%20effectiveness%20and%0Alimitations%20of%20RAG.%20To%20mitigate%20citation-lag%20bias%2C%20we%20applied%20a%20lower%0Acitation-count%20threshold%20to%20papers%20published%20in%202025%20so%20that%20emerging%0Abreakthroughs%20with%20naturally%20fewer%20citations%20were%20still%20captured.%20This%20review%0Aclarifies%20the%20current%20research%20landscape%2C%20highlights%20methodological%20gaps%2C%20and%0Acharts%20priority%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06401v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Systematic%2520Literature%2520Review%2520of%2520Retrieval-Augmented%2520Generation%253A%250A%2520%2520Techniques%252C%2520Metrics%252C%2520and%2520Challenges%26entry.906535625%3DAndrew%2520Brown%2520and%2520Muhammad%2520Roman%2520and%2520Barry%2520Devereux%26entry.1292438233%3D%2520%2520This%2520systematic%2520review%2520of%2520the%2520research%2520literature%2520on%2520retrieval-augmented%250Ageneration%2520%2528RAG%2529%2520provides%2520a%2520focused%2520analysis%2520of%2520the%2520most%2520highly%2520cited%2520studies%250Apublished%2520between%25202020%2520and%2520May%25202025.%2520A%2520total%2520of%2520128%2520articles%2520met%2520our%2520inclusion%250Acriteria.%2520The%2520records%2520were%2520retrieved%2520from%2520ACM%2520Digital%2520Library%252C%2520IEEE%2520Xplore%252C%250AScopus%252C%2520ScienceDirect%252C%2520and%2520the%2520Digital%2520Bibliography%2520and%2520Library%2520Project%2520%2528DBLP%2529.%250ARAG%2520couples%2520a%2520neural%2520retriever%2520with%2520a%2520generative%2520language%2520model%252C%2520grounding%250Aoutput%2520in%2520up-to-date%252C%2520non-parametric%2520memory%2520while%2520retaining%2520the%2520semantic%250Ageneralisation%2520stored%2520in%2520model%2520weights.%2520Guided%2520by%2520the%2520PRISMA%25202020%2520framework%252C%2520we%250A%2528i%2529%2520specify%2520explicit%2520inclusion%2520and%2520exclusion%2520criteria%2520based%2520on%2520citation%2520count%250Aand%2520research%2520questions%252C%2520%2528ii%2529%2520catalogue%2520datasets%252C%2520architectures%252C%2520and%2520evaluation%250Apractices%252C%2520and%2520%2528iii%2529%2520synthesise%2520empirical%2520evidence%2520on%2520the%2520effectiveness%2520and%250Alimitations%2520of%2520RAG.%2520To%2520mitigate%2520citation-lag%2520bias%252C%2520we%2520applied%2520a%2520lower%250Acitation-count%2520threshold%2520to%2520papers%2520published%2520in%25202025%2520so%2520that%2520emerging%250Abreakthroughs%2520with%2520naturally%2520fewer%2520citations%2520were%2520still%2520captured.%2520This%2520review%250Aclarifies%2520the%2520current%2520research%2520landscape%252C%2520highlights%2520methodological%2520gaps%252C%2520and%250Acharts%2520priority%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06401v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Systematic%20Literature%20Review%20of%20Retrieval-Augmented%20Generation%3A%0A%20%20Techniques%2C%20Metrics%2C%20and%20Challenges&entry.906535625=Andrew%20Brown%20and%20Muhammad%20Roman%20and%20Barry%20Devereux&entry.1292438233=%20%20This%20systematic%20review%20of%20the%20research%20literature%20on%20retrieval-augmented%0Ageneration%20%28RAG%29%20provides%20a%20focused%20analysis%20of%20the%20most%20highly%20cited%20studies%0Apublished%20between%202020%20and%20May%202025.%20A%20total%20of%20128%20articles%20met%20our%20inclusion%0Acriteria.%20The%20records%20were%20retrieved%20from%20ACM%20Digital%20Library%2C%20IEEE%20Xplore%2C%0AScopus%2C%20ScienceDirect%2C%20and%20the%20Digital%20Bibliography%20and%20Library%20Project%20%28DBLP%29.%0ARAG%20couples%20a%20neural%20retriever%20with%20a%20generative%20language%20model%2C%20grounding%0Aoutput%20in%20up-to-date%2C%20non-parametric%20memory%20while%20retaining%20the%20semantic%0Ageneralisation%20stored%20in%20model%20weights.%20Guided%20by%20the%20PRISMA%202020%20framework%2C%20we%0A%28i%29%20specify%20explicit%20inclusion%20and%20exclusion%20criteria%20based%20on%20citation%20count%0Aand%20research%20questions%2C%20%28ii%29%20catalogue%20datasets%2C%20architectures%2C%20and%20evaluation%0Apractices%2C%20and%20%28iii%29%20synthesise%20empirical%20evidence%20on%20the%20effectiveness%20and%0Alimitations%20of%20RAG.%20To%20mitigate%20citation-lag%20bias%2C%20we%20applied%20a%20lower%0Acitation-count%20threshold%20to%20papers%20published%20in%202025%20so%20that%20emerging%0Abreakthroughs%20with%20naturally%20fewer%20citations%20were%20still%20captured.%20This%20review%0Aclarifies%20the%20current%20research%20landscape%2C%20highlights%20methodological%20gaps%2C%20and%0Acharts%20priority%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06401v2&entry.124074799=Read"},
{"title": "Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media", "author": "Andrej Orsula and Matthieu Geist and Miguel Olivares-Mendez and Carol Martinez", "abstract": "  Reliable autonomous navigation across the unstructured terrains of distant\nplanetary surfaces is a critical enabler for future space exploration. However,\nthe deployment of learning-based controllers is hindered by the inherent\nsim-to-real gap, particularly for the complex dynamics of wheel interactions\nwith granular media. This work presents a complete sim-to-real framework for\ndeveloping and validating robust control policies for dynamic waypoint tracking\non such challenging surfaces. We leverage massively parallel simulation to\ntrain reinforcement learning agents across a vast distribution of procedurally\ngenerated environments with randomized physics. These policies are then\ntransferred zero-shot to a physical wheeled rover operating in a lunar-analogue\nfacility. Our experiments systematically compare multiple reinforcement\nlearning algorithms and action smoothing filters to identify the most effective\ncombinations for real-world deployment. Crucially, we provide strong empirical\nevidence that agents trained with procedural diversity achieve superior\nzero-shot performance compared to those trained on static scenarios. We also\nanalyze the trade-offs of fine-tuning with high-fidelity particle physics,\nwhich offers minor gains in low-speed precision at a significant computational\ncost. Together, these contributions establish a validated workflow for creating\nreliable learning-based navigation systems, marking a critical step towards\ndeploying autonomous robots in the final frontier.\n", "link": "http://arxiv.org/abs/2508.11503v1", "date": "2025-08-15", "relevancy": 2.2814, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5869}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.57}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sim2Dust%3A%20Mastering%20Dynamic%20Waypoint%20Tracking%20on%20Granular%20Media&body=Title%3A%20Sim2Dust%3A%20Mastering%20Dynamic%20Waypoint%20Tracking%20on%20Granular%20Media%0AAuthor%3A%20Andrej%20Orsula%20and%20Matthieu%20Geist%20and%20Miguel%20Olivares-Mendez%20and%20Carol%20Martinez%0AAbstract%3A%20%20%20Reliable%20autonomous%20navigation%20across%20the%20unstructured%20terrains%20of%20distant%0Aplanetary%20surfaces%20is%20a%20critical%20enabler%20for%20future%20space%20exploration.%20However%2C%0Athe%20deployment%20of%20learning-based%20controllers%20is%20hindered%20by%20the%20inherent%0Asim-to-real%20gap%2C%20particularly%20for%20the%20complex%20dynamics%20of%20wheel%20interactions%0Awith%20granular%20media.%20This%20work%20presents%20a%20complete%20sim-to-real%20framework%20for%0Adeveloping%20and%20validating%20robust%20control%20policies%20for%20dynamic%20waypoint%20tracking%0Aon%20such%20challenging%20surfaces.%20We%20leverage%20massively%20parallel%20simulation%20to%0Atrain%20reinforcement%20learning%20agents%20across%20a%20vast%20distribution%20of%20procedurally%0Agenerated%20environments%20with%20randomized%20physics.%20These%20policies%20are%20then%0Atransferred%20zero-shot%20to%20a%20physical%20wheeled%20rover%20operating%20in%20a%20lunar-analogue%0Afacility.%20Our%20experiments%20systematically%20compare%20multiple%20reinforcement%0Alearning%20algorithms%20and%20action%20smoothing%20filters%20to%20identify%20the%20most%20effective%0Acombinations%20for%20real-world%20deployment.%20Crucially%2C%20we%20provide%20strong%20empirical%0Aevidence%20that%20agents%20trained%20with%20procedural%20diversity%20achieve%20superior%0Azero-shot%20performance%20compared%20to%20those%20trained%20on%20static%20scenarios.%20We%20also%0Aanalyze%20the%20trade-offs%20of%20fine-tuning%20with%20high-fidelity%20particle%20physics%2C%0Awhich%20offers%20minor%20gains%20in%20low-speed%20precision%20at%20a%20significant%20computational%0Acost.%20Together%2C%20these%20contributions%20establish%20a%20validated%20workflow%20for%20creating%0Areliable%20learning-based%20navigation%20systems%2C%20marking%20a%20critical%20step%20towards%0Adeploying%20autonomous%20robots%20in%20the%20final%20frontier.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSim2Dust%253A%2520Mastering%2520Dynamic%2520Waypoint%2520Tracking%2520on%2520Granular%2520Media%26entry.906535625%3DAndrej%2520Orsula%2520and%2520Matthieu%2520Geist%2520and%2520Miguel%2520Olivares-Mendez%2520and%2520Carol%2520Martinez%26entry.1292438233%3D%2520%2520Reliable%2520autonomous%2520navigation%2520across%2520the%2520unstructured%2520terrains%2520of%2520distant%250Aplanetary%2520surfaces%2520is%2520a%2520critical%2520enabler%2520for%2520future%2520space%2520exploration.%2520However%252C%250Athe%2520deployment%2520of%2520learning-based%2520controllers%2520is%2520hindered%2520by%2520the%2520inherent%250Asim-to-real%2520gap%252C%2520particularly%2520for%2520the%2520complex%2520dynamics%2520of%2520wheel%2520interactions%250Awith%2520granular%2520media.%2520This%2520work%2520presents%2520a%2520complete%2520sim-to-real%2520framework%2520for%250Adeveloping%2520and%2520validating%2520robust%2520control%2520policies%2520for%2520dynamic%2520waypoint%2520tracking%250Aon%2520such%2520challenging%2520surfaces.%2520We%2520leverage%2520massively%2520parallel%2520simulation%2520to%250Atrain%2520reinforcement%2520learning%2520agents%2520across%2520a%2520vast%2520distribution%2520of%2520procedurally%250Agenerated%2520environments%2520with%2520randomized%2520physics.%2520These%2520policies%2520are%2520then%250Atransferred%2520zero-shot%2520to%2520a%2520physical%2520wheeled%2520rover%2520operating%2520in%2520a%2520lunar-analogue%250Afacility.%2520Our%2520experiments%2520systematically%2520compare%2520multiple%2520reinforcement%250Alearning%2520algorithms%2520and%2520action%2520smoothing%2520filters%2520to%2520identify%2520the%2520most%2520effective%250Acombinations%2520for%2520real-world%2520deployment.%2520Crucially%252C%2520we%2520provide%2520strong%2520empirical%250Aevidence%2520that%2520agents%2520trained%2520with%2520procedural%2520diversity%2520achieve%2520superior%250Azero-shot%2520performance%2520compared%2520to%2520those%2520trained%2520on%2520static%2520scenarios.%2520We%2520also%250Aanalyze%2520the%2520trade-offs%2520of%2520fine-tuning%2520with%2520high-fidelity%2520particle%2520physics%252C%250Awhich%2520offers%2520minor%2520gains%2520in%2520low-speed%2520precision%2520at%2520a%2520significant%2520computational%250Acost.%2520Together%252C%2520these%2520contributions%2520establish%2520a%2520validated%2520workflow%2520for%2520creating%250Areliable%2520learning-based%2520navigation%2520systems%252C%2520marking%2520a%2520critical%2520step%2520towards%250Adeploying%2520autonomous%2520robots%2520in%2520the%2520final%2520frontier.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sim2Dust%3A%20Mastering%20Dynamic%20Waypoint%20Tracking%20on%20Granular%20Media&entry.906535625=Andrej%20Orsula%20and%20Matthieu%20Geist%20and%20Miguel%20Olivares-Mendez%20and%20Carol%20Martinez&entry.1292438233=%20%20Reliable%20autonomous%20navigation%20across%20the%20unstructured%20terrains%20of%20distant%0Aplanetary%20surfaces%20is%20a%20critical%20enabler%20for%20future%20space%20exploration.%20However%2C%0Athe%20deployment%20of%20learning-based%20controllers%20is%20hindered%20by%20the%20inherent%0Asim-to-real%20gap%2C%20particularly%20for%20the%20complex%20dynamics%20of%20wheel%20interactions%0Awith%20granular%20media.%20This%20work%20presents%20a%20complete%20sim-to-real%20framework%20for%0Adeveloping%20and%20validating%20robust%20control%20policies%20for%20dynamic%20waypoint%20tracking%0Aon%20such%20challenging%20surfaces.%20We%20leverage%20massively%20parallel%20simulation%20to%0Atrain%20reinforcement%20learning%20agents%20across%20a%20vast%20distribution%20of%20procedurally%0Agenerated%20environments%20with%20randomized%20physics.%20These%20policies%20are%20then%0Atransferred%20zero-shot%20to%20a%20physical%20wheeled%20rover%20operating%20in%20a%20lunar-analogue%0Afacility.%20Our%20experiments%20systematically%20compare%20multiple%20reinforcement%0Alearning%20algorithms%20and%20action%20smoothing%20filters%20to%20identify%20the%20most%20effective%0Acombinations%20for%20real-world%20deployment.%20Crucially%2C%20we%20provide%20strong%20empirical%0Aevidence%20that%20agents%20trained%20with%20procedural%20diversity%20achieve%20superior%0Azero-shot%20performance%20compared%20to%20those%20trained%20on%20static%20scenarios.%20We%20also%0Aanalyze%20the%20trade-offs%20of%20fine-tuning%20with%20high-fidelity%20particle%20physics%2C%0Awhich%20offers%20minor%20gains%20in%20low-speed%20precision%20at%20a%20significant%20computational%0Acost.%20Together%2C%20these%20contributions%20establish%20a%20validated%20workflow%20for%20creating%0Areliable%20learning-based%20navigation%20systems%2C%20marking%20a%20critical%20step%20towards%0Adeploying%20autonomous%20robots%20in%20the%20final%20frontier.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11503v1&entry.124074799=Read"},
{"title": "Causality Matters: How Temporal Information Emerges in Video Language\n  Models", "author": "Yumeng Shi and Quanyu Long and Yin Wu and Wenya Wang", "abstract": "  Video language models (VideoLMs) have made significant progress in multimodal\nunderstanding. However, temporal understanding, which involves identifying\nevent order, duration, and relationships across time, still remains a core\nchallenge. Prior works emphasize positional encodings (PEs) as a key mechanism\nfor encoding temporal structure. Surprisingly, we find that removing or\nmodifying PEs in video inputs yields minimal degradation in the performance of\ntemporal understanding. In contrast, reversing the frame sequence while\npreserving the original PEs causes a substantial drop. To explain this\nbehavior, we conduct substantial analysis experiments to trace how temporal\ninformation is integrated within the model. We uncover a causal information\npathway: temporal cues are progressively synthesized through inter-frame\nattention, aggregated in the final frame, and subsequently integrated into the\nquery tokens. This emergent mechanism shows that temporal reasoning emerges\nfrom inter-visual token interactions under the constraints of causal attention,\nwhich implicitly encodes temporal structure. Based on these insights, we\npropose two efficiency-oriented strategies: staged cross-modal attention and a\ntemporal exit mechanism for early token truncation. Experiments on two\nbenchmarks validate the effectiveness of both approaches. To the best of our\nknowledge, this is the first work to systematically investigate video temporal\nunderstanding in VideoLMs, offering insights for future model improvement.\n", "link": "http://arxiv.org/abs/2508.11576v1", "date": "2025-08-15", "relevancy": 2.2749, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5708}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causality%20Matters%3A%20How%20Temporal%20Information%20Emerges%20in%20Video%20Language%0A%20%20Models&body=Title%3A%20Causality%20Matters%3A%20How%20Temporal%20Information%20Emerges%20in%20Video%20Language%0A%20%20Models%0AAuthor%3A%20Yumeng%20Shi%20and%20Quanyu%20Long%20and%20Yin%20Wu%20and%20Wenya%20Wang%0AAbstract%3A%20%20%20Video%20language%20models%20%28VideoLMs%29%20have%20made%20significant%20progress%20in%20multimodal%0Aunderstanding.%20However%2C%20temporal%20understanding%2C%20which%20involves%20identifying%0Aevent%20order%2C%20duration%2C%20and%20relationships%20across%20time%2C%20still%20remains%20a%20core%0Achallenge.%20Prior%20works%20emphasize%20positional%20encodings%20%28PEs%29%20as%20a%20key%20mechanism%0Afor%20encoding%20temporal%20structure.%20Surprisingly%2C%20we%20find%20that%20removing%20or%0Amodifying%20PEs%20in%20video%20inputs%20yields%20minimal%20degradation%20in%20the%20performance%20of%0Atemporal%20understanding.%20In%20contrast%2C%20reversing%20the%20frame%20sequence%20while%0Apreserving%20the%20original%20PEs%20causes%20a%20substantial%20drop.%20To%20explain%20this%0Abehavior%2C%20we%20conduct%20substantial%20analysis%20experiments%20to%20trace%20how%20temporal%0Ainformation%20is%20integrated%20within%20the%20model.%20We%20uncover%20a%20causal%20information%0Apathway%3A%20temporal%20cues%20are%20progressively%20synthesized%20through%20inter-frame%0Aattention%2C%20aggregated%20in%20the%20final%20frame%2C%20and%20subsequently%20integrated%20into%20the%0Aquery%20tokens.%20This%20emergent%20mechanism%20shows%20that%20temporal%20reasoning%20emerges%0Afrom%20inter-visual%20token%20interactions%20under%20the%20constraints%20of%20causal%20attention%2C%0Awhich%20implicitly%20encodes%20temporal%20structure.%20Based%20on%20these%20insights%2C%20we%0Apropose%20two%20efficiency-oriented%20strategies%3A%20staged%20cross-modal%20attention%20and%20a%0Atemporal%20exit%20mechanism%20for%20early%20token%20truncation.%20Experiments%20on%20two%0Abenchmarks%20validate%20the%20effectiveness%20of%20both%20approaches.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20to%20systematically%20investigate%20video%20temporal%0Aunderstanding%20in%20VideoLMs%2C%20offering%20insights%20for%20future%20model%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausality%2520Matters%253A%2520How%2520Temporal%2520Information%2520Emerges%2520in%2520Video%2520Language%250A%2520%2520Models%26entry.906535625%3DYumeng%2520Shi%2520and%2520Quanyu%2520Long%2520and%2520Yin%2520Wu%2520and%2520Wenya%2520Wang%26entry.1292438233%3D%2520%2520Video%2520language%2520models%2520%2528VideoLMs%2529%2520have%2520made%2520significant%2520progress%2520in%2520multimodal%250Aunderstanding.%2520However%252C%2520temporal%2520understanding%252C%2520which%2520involves%2520identifying%250Aevent%2520order%252C%2520duration%252C%2520and%2520relationships%2520across%2520time%252C%2520still%2520remains%2520a%2520core%250Achallenge.%2520Prior%2520works%2520emphasize%2520positional%2520encodings%2520%2528PEs%2529%2520as%2520a%2520key%2520mechanism%250Afor%2520encoding%2520temporal%2520structure.%2520Surprisingly%252C%2520we%2520find%2520that%2520removing%2520or%250Amodifying%2520PEs%2520in%2520video%2520inputs%2520yields%2520minimal%2520degradation%2520in%2520the%2520performance%2520of%250Atemporal%2520understanding.%2520In%2520contrast%252C%2520reversing%2520the%2520frame%2520sequence%2520while%250Apreserving%2520the%2520original%2520PEs%2520causes%2520a%2520substantial%2520drop.%2520To%2520explain%2520this%250Abehavior%252C%2520we%2520conduct%2520substantial%2520analysis%2520experiments%2520to%2520trace%2520how%2520temporal%250Ainformation%2520is%2520integrated%2520within%2520the%2520model.%2520We%2520uncover%2520a%2520causal%2520information%250Apathway%253A%2520temporal%2520cues%2520are%2520progressively%2520synthesized%2520through%2520inter-frame%250Aattention%252C%2520aggregated%2520in%2520the%2520final%2520frame%252C%2520and%2520subsequently%2520integrated%2520into%2520the%250Aquery%2520tokens.%2520This%2520emergent%2520mechanism%2520shows%2520that%2520temporal%2520reasoning%2520emerges%250Afrom%2520inter-visual%2520token%2520interactions%2520under%2520the%2520constraints%2520of%2520causal%2520attention%252C%250Awhich%2520implicitly%2520encodes%2520temporal%2520structure.%2520Based%2520on%2520these%2520insights%252C%2520we%250Apropose%2520two%2520efficiency-oriented%2520strategies%253A%2520staged%2520cross-modal%2520attention%2520and%2520a%250Atemporal%2520exit%2520mechanism%2520for%2520early%2520token%2520truncation.%2520Experiments%2520on%2520two%250Abenchmarks%2520validate%2520the%2520effectiveness%2520of%2520both%2520approaches.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520systematically%2520investigate%2520video%2520temporal%250Aunderstanding%2520in%2520VideoLMs%252C%2520offering%2520insights%2520for%2520future%2520model%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causality%20Matters%3A%20How%20Temporal%20Information%20Emerges%20in%20Video%20Language%0A%20%20Models&entry.906535625=Yumeng%20Shi%20and%20Quanyu%20Long%20and%20Yin%20Wu%20and%20Wenya%20Wang&entry.1292438233=%20%20Video%20language%20models%20%28VideoLMs%29%20have%20made%20significant%20progress%20in%20multimodal%0Aunderstanding.%20However%2C%20temporal%20understanding%2C%20which%20involves%20identifying%0Aevent%20order%2C%20duration%2C%20and%20relationships%20across%20time%2C%20still%20remains%20a%20core%0Achallenge.%20Prior%20works%20emphasize%20positional%20encodings%20%28PEs%29%20as%20a%20key%20mechanism%0Afor%20encoding%20temporal%20structure.%20Surprisingly%2C%20we%20find%20that%20removing%20or%0Amodifying%20PEs%20in%20video%20inputs%20yields%20minimal%20degradation%20in%20the%20performance%20of%0Atemporal%20understanding.%20In%20contrast%2C%20reversing%20the%20frame%20sequence%20while%0Apreserving%20the%20original%20PEs%20causes%20a%20substantial%20drop.%20To%20explain%20this%0Abehavior%2C%20we%20conduct%20substantial%20analysis%20experiments%20to%20trace%20how%20temporal%0Ainformation%20is%20integrated%20within%20the%20model.%20We%20uncover%20a%20causal%20information%0Apathway%3A%20temporal%20cues%20are%20progressively%20synthesized%20through%20inter-frame%0Aattention%2C%20aggregated%20in%20the%20final%20frame%2C%20and%20subsequently%20integrated%20into%20the%0Aquery%20tokens.%20This%20emergent%20mechanism%20shows%20that%20temporal%20reasoning%20emerges%0Afrom%20inter-visual%20token%20interactions%20under%20the%20constraints%20of%20causal%20attention%2C%0Awhich%20implicitly%20encodes%20temporal%20structure.%20Based%20on%20these%20insights%2C%20we%0Apropose%20two%20efficiency-oriented%20strategies%3A%20staged%20cross-modal%20attention%20and%20a%0Atemporal%20exit%20mechanism%20for%20early%20token%20truncation.%20Experiments%20on%20two%0Abenchmarks%20validate%20the%20effectiveness%20of%20both%20approaches.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20to%20systematically%20investigate%20video%20temporal%0Aunderstanding%20in%20VideoLMs%2C%20offering%20insights%20for%20future%20model%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11576v1&entry.124074799=Read"},
{"title": "OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal\n  Navigation", "author": "Tatiana Zemskova and Aleksei Staroverov and Dmitry Yudin and Aleksandr Panov", "abstract": "  Open-vocabulary Object Goal Navigation requires an embodied agent to reach\nobjects described by free-form language, including categories never seen during\ntraining. Existing end-to-end policies overfit small simulator datasets,\nachieving high success on training scenes but failing to generalize and\nexhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a\nlightweight transformer policy that tackles these issues with two synergistic\ncomponents. The first component is the semantic branch, which includes an\nencoder for the target binary mask and an auxiliary segmentation loss function,\ngrounding the textual goal and providing precise spatial cues. The second\ncomponent consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample\nscheduler that continuously balances imitation and reinforcement signals\naccording to the policy entropy, eliminating brittle manual phase switches.\nThese additions cut the sample complexity of training by 33%, and reduce\ncollision count in two times while keeping inference cost low (130M parameters,\nRGB-only input). On HM3D-OVON, our model matches the performance on unseen\ncategories to that on seen ones and establishes state-of-the-art results (40.1%\nSR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language\nmodels. Code is available at https://github.com/CognitiveAISystems/OVSegDT.\n", "link": "http://arxiv.org/abs/2508.11479v1", "date": "2025-08-15", "relevancy": 2.2509, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5627}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OVSegDT%3A%20Segmenting%20Transformer%20for%20Open-Vocabulary%20Object%20Goal%0A%20%20Navigation&body=Title%3A%20OVSegDT%3A%20Segmenting%20Transformer%20for%20Open-Vocabulary%20Object%20Goal%0A%20%20Navigation%0AAuthor%3A%20Tatiana%20Zemskova%20and%20Aleksei%20Staroverov%20and%20Dmitry%20Yudin%20and%20Aleksandr%20Panov%0AAbstract%3A%20%20%20Open-vocabulary%20Object%20Goal%20Navigation%20requires%20an%20embodied%20agent%20to%20reach%0Aobjects%20described%20by%20free-form%20language%2C%20including%20categories%20never%20seen%20during%0Atraining.%20Existing%20end-to-end%20policies%20overfit%20small%20simulator%20datasets%2C%0Aachieving%20high%20success%20on%20training%20scenes%20but%20failing%20to%20generalize%20and%0Aexhibiting%20unsafe%20behaviour%20%28frequent%20collisions%29.%20We%20introduce%20OVSegDT%2C%20a%0Alightweight%20transformer%20policy%20that%20tackles%20these%20issues%20with%20two%20synergistic%0Acomponents.%20The%20first%20component%20is%20the%20semantic%20branch%2C%20which%20includes%20an%0Aencoder%20for%20the%20target%20binary%20mask%20and%20an%20auxiliary%20segmentation%20loss%20function%2C%0Agrounding%20the%20textual%20goal%20and%20providing%20precise%20spatial%20cues.%20The%20second%0Acomponent%20consists%20of%20a%20proposed%20Entropy-Adaptive%20Loss%20Modulation%2C%20a%20per-sample%0Ascheduler%20that%20continuously%20balances%20imitation%20and%20reinforcement%20signals%0Aaccording%20to%20the%20policy%20entropy%2C%20eliminating%20brittle%20manual%20phase%20switches.%0AThese%20additions%20cut%20the%20sample%20complexity%20of%20training%20by%2033%25%2C%20and%20reduce%0Acollision%20count%20in%20two%20times%20while%20keeping%20inference%20cost%20low%20%28130M%20parameters%2C%0ARGB-only%20input%29.%20On%20HM3D-OVON%2C%20our%20model%20matches%20the%20performance%20on%20unseen%0Acategories%20to%20that%20on%20seen%20ones%20and%20establishes%20state-of-the-art%20results%20%2840.1%25%0ASR%2C%2020.9%25%20SPL%20on%20val%20unseen%29%20without%20depth%2C%20odometry%2C%20or%20large%20vision-language%0Amodels.%20Code%20is%20available%20at%20https%3A//github.com/CognitiveAISystems/OVSegDT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOVSegDT%253A%2520Segmenting%2520Transformer%2520for%2520Open-Vocabulary%2520Object%2520Goal%250A%2520%2520Navigation%26entry.906535625%3DTatiana%2520Zemskova%2520and%2520Aleksei%2520Staroverov%2520and%2520Dmitry%2520Yudin%2520and%2520Aleksandr%2520Panov%26entry.1292438233%3D%2520%2520Open-vocabulary%2520Object%2520Goal%2520Navigation%2520requires%2520an%2520embodied%2520agent%2520to%2520reach%250Aobjects%2520described%2520by%2520free-form%2520language%252C%2520including%2520categories%2520never%2520seen%2520during%250Atraining.%2520Existing%2520end-to-end%2520policies%2520overfit%2520small%2520simulator%2520datasets%252C%250Aachieving%2520high%2520success%2520on%2520training%2520scenes%2520but%2520failing%2520to%2520generalize%2520and%250Aexhibiting%2520unsafe%2520behaviour%2520%2528frequent%2520collisions%2529.%2520We%2520introduce%2520OVSegDT%252C%2520a%250Alightweight%2520transformer%2520policy%2520that%2520tackles%2520these%2520issues%2520with%2520two%2520synergistic%250Acomponents.%2520The%2520first%2520component%2520is%2520the%2520semantic%2520branch%252C%2520which%2520includes%2520an%250Aencoder%2520for%2520the%2520target%2520binary%2520mask%2520and%2520an%2520auxiliary%2520segmentation%2520loss%2520function%252C%250Agrounding%2520the%2520textual%2520goal%2520and%2520providing%2520precise%2520spatial%2520cues.%2520The%2520second%250Acomponent%2520consists%2520of%2520a%2520proposed%2520Entropy-Adaptive%2520Loss%2520Modulation%252C%2520a%2520per-sample%250Ascheduler%2520that%2520continuously%2520balances%2520imitation%2520and%2520reinforcement%2520signals%250Aaccording%2520to%2520the%2520policy%2520entropy%252C%2520eliminating%2520brittle%2520manual%2520phase%2520switches.%250AThese%2520additions%2520cut%2520the%2520sample%2520complexity%2520of%2520training%2520by%252033%2525%252C%2520and%2520reduce%250Acollision%2520count%2520in%2520two%2520times%2520while%2520keeping%2520inference%2520cost%2520low%2520%2528130M%2520parameters%252C%250ARGB-only%2520input%2529.%2520On%2520HM3D-OVON%252C%2520our%2520model%2520matches%2520the%2520performance%2520on%2520unseen%250Acategories%2520to%2520that%2520on%2520seen%2520ones%2520and%2520establishes%2520state-of-the-art%2520results%2520%252840.1%2525%250ASR%252C%252020.9%2525%2520SPL%2520on%2520val%2520unseen%2529%2520without%2520depth%252C%2520odometry%252C%2520or%2520large%2520vision-language%250Amodels.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/CognitiveAISystems/OVSegDT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OVSegDT%3A%20Segmenting%20Transformer%20for%20Open-Vocabulary%20Object%20Goal%0A%20%20Navigation&entry.906535625=Tatiana%20Zemskova%20and%20Aleksei%20Staroverov%20and%20Dmitry%20Yudin%20and%20Aleksandr%20Panov&entry.1292438233=%20%20Open-vocabulary%20Object%20Goal%20Navigation%20requires%20an%20embodied%20agent%20to%20reach%0Aobjects%20described%20by%20free-form%20language%2C%20including%20categories%20never%20seen%20during%0Atraining.%20Existing%20end-to-end%20policies%20overfit%20small%20simulator%20datasets%2C%0Aachieving%20high%20success%20on%20training%20scenes%20but%20failing%20to%20generalize%20and%0Aexhibiting%20unsafe%20behaviour%20%28frequent%20collisions%29.%20We%20introduce%20OVSegDT%2C%20a%0Alightweight%20transformer%20policy%20that%20tackles%20these%20issues%20with%20two%20synergistic%0Acomponents.%20The%20first%20component%20is%20the%20semantic%20branch%2C%20which%20includes%20an%0Aencoder%20for%20the%20target%20binary%20mask%20and%20an%20auxiliary%20segmentation%20loss%20function%2C%0Agrounding%20the%20textual%20goal%20and%20providing%20precise%20spatial%20cues.%20The%20second%0Acomponent%20consists%20of%20a%20proposed%20Entropy-Adaptive%20Loss%20Modulation%2C%20a%20per-sample%0Ascheduler%20that%20continuously%20balances%20imitation%20and%20reinforcement%20signals%0Aaccording%20to%20the%20policy%20entropy%2C%20eliminating%20brittle%20manual%20phase%20switches.%0AThese%20additions%20cut%20the%20sample%20complexity%20of%20training%20by%2033%25%2C%20and%20reduce%0Acollision%20count%20in%20two%20times%20while%20keeping%20inference%20cost%20low%20%28130M%20parameters%2C%0ARGB-only%20input%29.%20On%20HM3D-OVON%2C%20our%20model%20matches%20the%20performance%20on%20unseen%0Acategories%20to%20that%20on%20seen%20ones%20and%20establishes%20state-of-the-art%20results%20%2840.1%25%0ASR%2C%2020.9%25%20SPL%20on%20val%20unseen%29%20without%20depth%2C%20odometry%2C%20or%20large%20vision-language%0Amodels.%20Code%20is%20available%20at%20https%3A//github.com/CognitiveAISystems/OVSegDT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11479v1&entry.124074799=Read"},
{"title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with\n  Long-Term Memory", "author": "Lin Long and Yichen He and Wentao Ye and Yiyuan Pan and Yuan Lin and Hang Li and Junbo Zhao and Wei Li", "abstract": "  We introduce M3-Agent, a novel multimodal agent framework equipped with\nlong-term memory. Like humans, M3-Agent can process real-time visual and\nauditory inputs to build and update its long-term memory. Beyond episodic\nmemory, it also develops semantic memory, enabling it to accumulate world\nknowledge over time. Its memory is organized in an entity-centric, multimodal\nformat, allowing deeper and more consistent understanding of the environment.\nGiven an instruction, M3-Agent autonomously performs multi-turn, iterative\nreasoning and retrieves relevant information from memory to accomplish the\ntask. To evaluate memory effectiveness and memory-based reasoning in multimodal\nagents, we develop M3-Bench, a new long-video question answering benchmark.\nM3-Bench comprises 100 newly recorded real-world videos captured from a robot's\nperspective (M3-Bench-robot) and 920 web-sourced videos across diverse\nscenarios (M3-Bench-web). We annotate question-answer pairs designed to test\nkey capabilities essential for agent applications, such as human understanding,\ngeneral knowledge extraction, and cross-modal reasoning. Experimental results\nshow that M3-Agent, trained via reinforcement learning, outperforms the\nstrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,\nachieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web\nand VideoMME-long, respectively. Our work advances the multimodal agents toward\nmore human-like long-term memory and provides insights into their practical\ndesign. Model, code and data are available at\nhttps://github.com/bytedance-seed/m3-agent\n", "link": "http://arxiv.org/abs/2508.09736v2", "date": "2025-08-15", "relevancy": 2.2452, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5755}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5685}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%2C%20Listening%2C%20Remembering%2C%20and%20Reasoning%3A%20A%20Multimodal%20Agent%20with%0A%20%20Long-Term%20Memory&body=Title%3A%20Seeing%2C%20Listening%2C%20Remembering%2C%20and%20Reasoning%3A%20A%20Multimodal%20Agent%20with%0A%20%20Long-Term%20Memory%0AAuthor%3A%20Lin%20Long%20and%20Yichen%20He%20and%20Wentao%20Ye%20and%20Yiyuan%20Pan%20and%20Yuan%20Lin%20and%20Hang%20Li%20and%20Junbo%20Zhao%20and%20Wei%20Li%0AAbstract%3A%20%20%20We%20introduce%20M3-Agent%2C%20a%20novel%20multimodal%20agent%20framework%20equipped%20with%0Along-term%20memory.%20Like%20humans%2C%20M3-Agent%20can%20process%20real-time%20visual%20and%0Aauditory%20inputs%20to%20build%20and%20update%20its%20long-term%20memory.%20Beyond%20episodic%0Amemory%2C%20it%20also%20develops%20semantic%20memory%2C%20enabling%20it%20to%20accumulate%20world%0Aknowledge%20over%20time.%20Its%20memory%20is%20organized%20in%20an%20entity-centric%2C%20multimodal%0Aformat%2C%20allowing%20deeper%20and%20more%20consistent%20understanding%20of%20the%20environment.%0AGiven%20an%20instruction%2C%20M3-Agent%20autonomously%20performs%20multi-turn%2C%20iterative%0Areasoning%20and%20retrieves%20relevant%20information%20from%20memory%20to%20accomplish%20the%0Atask.%20To%20evaluate%20memory%20effectiveness%20and%20memory-based%20reasoning%20in%20multimodal%0Aagents%2C%20we%20develop%20M3-Bench%2C%20a%20new%20long-video%20question%20answering%20benchmark.%0AM3-Bench%20comprises%20100%20newly%20recorded%20real-world%20videos%20captured%20from%20a%20robot%27s%0Aperspective%20%28M3-Bench-robot%29%20and%20920%20web-sourced%20videos%20across%20diverse%0Ascenarios%20%28M3-Bench-web%29.%20We%20annotate%20question-answer%20pairs%20designed%20to%20test%0Akey%20capabilities%20essential%20for%20agent%20applications%2C%20such%20as%20human%20understanding%2C%0Ageneral%20knowledge%20extraction%2C%20and%20cross-modal%20reasoning.%20Experimental%20results%0Ashow%20that%20M3-Agent%2C%20trained%20via%20reinforcement%20learning%2C%20outperforms%20the%0Astrongest%20baseline%2C%20a%20prompting%20agent%20using%20Gemini-1.5-pro%20and%20GPT-4o%2C%0Aachieving%206.7%25%2C%207.7%25%2C%20and%205.3%25%20higher%20accuracy%20on%20M3-Bench-robot%2C%20M3-Bench-web%0Aand%20VideoMME-long%2C%20respectively.%20Our%20work%20advances%20the%20multimodal%20agents%20toward%0Amore%20human-like%20long-term%20memory%20and%20provides%20insights%20into%20their%20practical%0Adesign.%20Model%2C%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/bytedance-seed/m3-agent%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09736v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%252C%2520Listening%252C%2520Remembering%252C%2520and%2520Reasoning%253A%2520A%2520Multimodal%2520Agent%2520with%250A%2520%2520Long-Term%2520Memory%26entry.906535625%3DLin%2520Long%2520and%2520Yichen%2520He%2520and%2520Wentao%2520Ye%2520and%2520Yiyuan%2520Pan%2520and%2520Yuan%2520Lin%2520and%2520Hang%2520Li%2520and%2520Junbo%2520Zhao%2520and%2520Wei%2520Li%26entry.1292438233%3D%2520%2520We%2520introduce%2520M3-Agent%252C%2520a%2520novel%2520multimodal%2520agent%2520framework%2520equipped%2520with%250Along-term%2520memory.%2520Like%2520humans%252C%2520M3-Agent%2520can%2520process%2520real-time%2520visual%2520and%250Aauditory%2520inputs%2520to%2520build%2520and%2520update%2520its%2520long-term%2520memory.%2520Beyond%2520episodic%250Amemory%252C%2520it%2520also%2520develops%2520semantic%2520memory%252C%2520enabling%2520it%2520to%2520accumulate%2520world%250Aknowledge%2520over%2520time.%2520Its%2520memory%2520is%2520organized%2520in%2520an%2520entity-centric%252C%2520multimodal%250Aformat%252C%2520allowing%2520deeper%2520and%2520more%2520consistent%2520understanding%2520of%2520the%2520environment.%250AGiven%2520an%2520instruction%252C%2520M3-Agent%2520autonomously%2520performs%2520multi-turn%252C%2520iterative%250Areasoning%2520and%2520retrieves%2520relevant%2520information%2520from%2520memory%2520to%2520accomplish%2520the%250Atask.%2520To%2520evaluate%2520memory%2520effectiveness%2520and%2520memory-based%2520reasoning%2520in%2520multimodal%250Aagents%252C%2520we%2520develop%2520M3-Bench%252C%2520a%2520new%2520long-video%2520question%2520answering%2520benchmark.%250AM3-Bench%2520comprises%2520100%2520newly%2520recorded%2520real-world%2520videos%2520captured%2520from%2520a%2520robot%2527s%250Aperspective%2520%2528M3-Bench-robot%2529%2520and%2520920%2520web-sourced%2520videos%2520across%2520diverse%250Ascenarios%2520%2528M3-Bench-web%2529.%2520We%2520annotate%2520question-answer%2520pairs%2520designed%2520to%2520test%250Akey%2520capabilities%2520essential%2520for%2520agent%2520applications%252C%2520such%2520as%2520human%2520understanding%252C%250Ageneral%2520knowledge%2520extraction%252C%2520and%2520cross-modal%2520reasoning.%2520Experimental%2520results%250Ashow%2520that%2520M3-Agent%252C%2520trained%2520via%2520reinforcement%2520learning%252C%2520outperforms%2520the%250Astrongest%2520baseline%252C%2520a%2520prompting%2520agent%2520using%2520Gemini-1.5-pro%2520and%2520GPT-4o%252C%250Aachieving%25206.7%2525%252C%25207.7%2525%252C%2520and%25205.3%2525%2520higher%2520accuracy%2520on%2520M3-Bench-robot%252C%2520M3-Bench-web%250Aand%2520VideoMME-long%252C%2520respectively.%2520Our%2520work%2520advances%2520the%2520multimodal%2520agents%2520toward%250Amore%2520human-like%2520long-term%2520memory%2520and%2520provides%2520insights%2520into%2520their%2520practical%250Adesign.%2520Model%252C%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/bytedance-seed/m3-agent%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09736v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%2C%20Listening%2C%20Remembering%2C%20and%20Reasoning%3A%20A%20Multimodal%20Agent%20with%0A%20%20Long-Term%20Memory&entry.906535625=Lin%20Long%20and%20Yichen%20He%20and%20Wentao%20Ye%20and%20Yiyuan%20Pan%20and%20Yuan%20Lin%20and%20Hang%20Li%20and%20Junbo%20Zhao%20and%20Wei%20Li&entry.1292438233=%20%20We%20introduce%20M3-Agent%2C%20a%20novel%20multimodal%20agent%20framework%20equipped%20with%0Along-term%20memory.%20Like%20humans%2C%20M3-Agent%20can%20process%20real-time%20visual%20and%0Aauditory%20inputs%20to%20build%20and%20update%20its%20long-term%20memory.%20Beyond%20episodic%0Amemory%2C%20it%20also%20develops%20semantic%20memory%2C%20enabling%20it%20to%20accumulate%20world%0Aknowledge%20over%20time.%20Its%20memory%20is%20organized%20in%20an%20entity-centric%2C%20multimodal%0Aformat%2C%20allowing%20deeper%20and%20more%20consistent%20understanding%20of%20the%20environment.%0AGiven%20an%20instruction%2C%20M3-Agent%20autonomously%20performs%20multi-turn%2C%20iterative%0Areasoning%20and%20retrieves%20relevant%20information%20from%20memory%20to%20accomplish%20the%0Atask.%20To%20evaluate%20memory%20effectiveness%20and%20memory-based%20reasoning%20in%20multimodal%0Aagents%2C%20we%20develop%20M3-Bench%2C%20a%20new%20long-video%20question%20answering%20benchmark.%0AM3-Bench%20comprises%20100%20newly%20recorded%20real-world%20videos%20captured%20from%20a%20robot%27s%0Aperspective%20%28M3-Bench-robot%29%20and%20920%20web-sourced%20videos%20across%20diverse%0Ascenarios%20%28M3-Bench-web%29.%20We%20annotate%20question-answer%20pairs%20designed%20to%20test%0Akey%20capabilities%20essential%20for%20agent%20applications%2C%20such%20as%20human%20understanding%2C%0Ageneral%20knowledge%20extraction%2C%20and%20cross-modal%20reasoning.%20Experimental%20results%0Ashow%20that%20M3-Agent%2C%20trained%20via%20reinforcement%20learning%2C%20outperforms%20the%0Astrongest%20baseline%2C%20a%20prompting%20agent%20using%20Gemini-1.5-pro%20and%20GPT-4o%2C%0Aachieving%206.7%25%2C%207.7%25%2C%20and%205.3%25%20higher%20accuracy%20on%20M3-Bench-robot%2C%20M3-Bench-web%0Aand%20VideoMME-long%2C%20respectively.%20Our%20work%20advances%20the%20multimodal%20agents%20toward%0Amore%20human-like%20long-term%20memory%20and%20provides%20insights%20into%20their%20practical%0Adesign.%20Model%2C%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/bytedance-seed/m3-agent%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09736v2&entry.124074799=Read"},
{"title": "AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for\n  Medical Semantic Image Synthesis", "author": "Zonglin Wu and Yule Xue and Qianxiang Hu and Yaoyao Feng and Yuqi Ma and Shanxiong Chen", "abstract": "  Medical semantic-mask synthesis boosts data augmentation and analysis, yet\nmost GAN-based approaches still produce one-to-one images and lack spatial\nconsistency in complex scans. To address this, we propose AnatoMaskGAN, a novel\nsynthesis framework that embeds slice-related spatial features to precisely\naggregate inter-slice contextual dependencies, introduces diverse\nimage-augmentation strategies, and optimizes deep feature learning to improve\nperformance on complex medical images. Specifically, we design a GNN-based\nstrongly correlated slice-feature fusion module to model spatial relationships\nbetween slices and integrate contextual information from neighboring slices,\nthereby capturing anatomical details more comprehensively; we introduce a\nthree-dimensional spatial noise-injection strategy that weights and fuses\nspatial features with noise to enhance modeling of structural diversity; and we\nincorporate a grayscale-texture classifier to optimize grayscale distribution\nand texture representation during generation. Extensive experiments on the\npublic L2R-OASIS and L2R-Abdomen CT datasets show that AnatoMaskGAN raises PSNR\non L2R-OASIS to 26.50 dB (0.43 dB higher than the current state of the art) and\nachieves an SSIM of 0.8602 on L2R-Abdomen CT--a 0.48 percentage-point gain over\nthe best model, demonstrating its superiority in reconstruction accuracy and\nperceptual quality. Ablation studies that successively remove the slice-feature\nfusion module, spatial 3D noise-injection strategy, and grayscale-texture\nclassifier reveal that each component contributes significantly to PSNR, SSIM,\nand LPIPS, further confirming the independent value of each core design in\nenhancing reconstruction accuracy and perceptual quality.\n", "link": "http://arxiv.org/abs/2508.11375v1", "date": "2025-08-15", "relevancy": 2.2402, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5673}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5637}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnatoMaskGAN%3A%20GNN-Driven%20Slice%20Feature%20Fusion%20and%20Noise%20Augmentation%20for%0A%20%20Medical%20Semantic%20Image%20Synthesis&body=Title%3A%20AnatoMaskGAN%3A%20GNN-Driven%20Slice%20Feature%20Fusion%20and%20Noise%20Augmentation%20for%0A%20%20Medical%20Semantic%20Image%20Synthesis%0AAuthor%3A%20Zonglin%20Wu%20and%20Yule%20Xue%20and%20Qianxiang%20Hu%20and%20Yaoyao%20Feng%20and%20Yuqi%20Ma%20and%20Shanxiong%20Chen%0AAbstract%3A%20%20%20Medical%20semantic-mask%20synthesis%20boosts%20data%20augmentation%20and%20analysis%2C%20yet%0Amost%20GAN-based%20approaches%20still%20produce%20one-to-one%20images%20and%20lack%20spatial%0Aconsistency%20in%20complex%20scans.%20To%20address%20this%2C%20we%20propose%20AnatoMaskGAN%2C%20a%20novel%0Asynthesis%20framework%20that%20embeds%20slice-related%20spatial%20features%20to%20precisely%0Aaggregate%20inter-slice%20contextual%20dependencies%2C%20introduces%20diverse%0Aimage-augmentation%20strategies%2C%20and%20optimizes%20deep%20feature%20learning%20to%20improve%0Aperformance%20on%20complex%20medical%20images.%20Specifically%2C%20we%20design%20a%20GNN-based%0Astrongly%20correlated%20slice-feature%20fusion%20module%20to%20model%20spatial%20relationships%0Abetween%20slices%20and%20integrate%20contextual%20information%20from%20neighboring%20slices%2C%0Athereby%20capturing%20anatomical%20details%20more%20comprehensively%3B%20we%20introduce%20a%0Athree-dimensional%20spatial%20noise-injection%20strategy%20that%20weights%20and%20fuses%0Aspatial%20features%20with%20noise%20to%20enhance%20modeling%20of%20structural%20diversity%3B%20and%20we%0Aincorporate%20a%20grayscale-texture%20classifier%20to%20optimize%20grayscale%20distribution%0Aand%20texture%20representation%20during%20generation.%20Extensive%20experiments%20on%20the%0Apublic%20L2R-OASIS%20and%20L2R-Abdomen%20CT%20datasets%20show%20that%20AnatoMaskGAN%20raises%20PSNR%0Aon%20L2R-OASIS%20to%2026.50%20dB%20%280.43%20dB%20higher%20than%20the%20current%20state%20of%20the%20art%29%20and%0Aachieves%20an%20SSIM%20of%200.8602%20on%20L2R-Abdomen%20CT--a%200.48%20percentage-point%20gain%20over%0Athe%20best%20model%2C%20demonstrating%20its%20superiority%20in%20reconstruction%20accuracy%20and%0Aperceptual%20quality.%20Ablation%20studies%20that%20successively%20remove%20the%20slice-feature%0Afusion%20module%2C%20spatial%203D%20noise-injection%20strategy%2C%20and%20grayscale-texture%0Aclassifier%20reveal%20that%20each%20component%20contributes%20significantly%20to%20PSNR%2C%20SSIM%2C%0Aand%20LPIPS%2C%20further%20confirming%20the%20independent%20value%20of%20each%20core%20design%20in%0Aenhancing%20reconstruction%20accuracy%20and%20perceptual%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnatoMaskGAN%253A%2520GNN-Driven%2520Slice%2520Feature%2520Fusion%2520and%2520Noise%2520Augmentation%2520for%250A%2520%2520Medical%2520Semantic%2520Image%2520Synthesis%26entry.906535625%3DZonglin%2520Wu%2520and%2520Yule%2520Xue%2520and%2520Qianxiang%2520Hu%2520and%2520Yaoyao%2520Feng%2520and%2520Yuqi%2520Ma%2520and%2520Shanxiong%2520Chen%26entry.1292438233%3D%2520%2520Medical%2520semantic-mask%2520synthesis%2520boosts%2520data%2520augmentation%2520and%2520analysis%252C%2520yet%250Amost%2520GAN-based%2520approaches%2520still%2520produce%2520one-to-one%2520images%2520and%2520lack%2520spatial%250Aconsistency%2520in%2520complex%2520scans.%2520To%2520address%2520this%252C%2520we%2520propose%2520AnatoMaskGAN%252C%2520a%2520novel%250Asynthesis%2520framework%2520that%2520embeds%2520slice-related%2520spatial%2520features%2520to%2520precisely%250Aaggregate%2520inter-slice%2520contextual%2520dependencies%252C%2520introduces%2520diverse%250Aimage-augmentation%2520strategies%252C%2520and%2520optimizes%2520deep%2520feature%2520learning%2520to%2520improve%250Aperformance%2520on%2520complex%2520medical%2520images.%2520Specifically%252C%2520we%2520design%2520a%2520GNN-based%250Astrongly%2520correlated%2520slice-feature%2520fusion%2520module%2520to%2520model%2520spatial%2520relationships%250Abetween%2520slices%2520and%2520integrate%2520contextual%2520information%2520from%2520neighboring%2520slices%252C%250Athereby%2520capturing%2520anatomical%2520details%2520more%2520comprehensively%253B%2520we%2520introduce%2520a%250Athree-dimensional%2520spatial%2520noise-injection%2520strategy%2520that%2520weights%2520and%2520fuses%250Aspatial%2520features%2520with%2520noise%2520to%2520enhance%2520modeling%2520of%2520structural%2520diversity%253B%2520and%2520we%250Aincorporate%2520a%2520grayscale-texture%2520classifier%2520to%2520optimize%2520grayscale%2520distribution%250Aand%2520texture%2520representation%2520during%2520generation.%2520Extensive%2520experiments%2520on%2520the%250Apublic%2520L2R-OASIS%2520and%2520L2R-Abdomen%2520CT%2520datasets%2520show%2520that%2520AnatoMaskGAN%2520raises%2520PSNR%250Aon%2520L2R-OASIS%2520to%252026.50%2520dB%2520%25280.43%2520dB%2520higher%2520than%2520the%2520current%2520state%2520of%2520the%2520art%2529%2520and%250Aachieves%2520an%2520SSIM%2520of%25200.8602%2520on%2520L2R-Abdomen%2520CT--a%25200.48%2520percentage-point%2520gain%2520over%250Athe%2520best%2520model%252C%2520demonstrating%2520its%2520superiority%2520in%2520reconstruction%2520accuracy%2520and%250Aperceptual%2520quality.%2520Ablation%2520studies%2520that%2520successively%2520remove%2520the%2520slice-feature%250Afusion%2520module%252C%2520spatial%25203D%2520noise-injection%2520strategy%252C%2520and%2520grayscale-texture%250Aclassifier%2520reveal%2520that%2520each%2520component%2520contributes%2520significantly%2520to%2520PSNR%252C%2520SSIM%252C%250Aand%2520LPIPS%252C%2520further%2520confirming%2520the%2520independent%2520value%2520of%2520each%2520core%2520design%2520in%250Aenhancing%2520reconstruction%2520accuracy%2520and%2520perceptual%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnatoMaskGAN%3A%20GNN-Driven%20Slice%20Feature%20Fusion%20and%20Noise%20Augmentation%20for%0A%20%20Medical%20Semantic%20Image%20Synthesis&entry.906535625=Zonglin%20Wu%20and%20Yule%20Xue%20and%20Qianxiang%20Hu%20and%20Yaoyao%20Feng%20and%20Yuqi%20Ma%20and%20Shanxiong%20Chen&entry.1292438233=%20%20Medical%20semantic-mask%20synthesis%20boosts%20data%20augmentation%20and%20analysis%2C%20yet%0Amost%20GAN-based%20approaches%20still%20produce%20one-to-one%20images%20and%20lack%20spatial%0Aconsistency%20in%20complex%20scans.%20To%20address%20this%2C%20we%20propose%20AnatoMaskGAN%2C%20a%20novel%0Asynthesis%20framework%20that%20embeds%20slice-related%20spatial%20features%20to%20precisely%0Aaggregate%20inter-slice%20contextual%20dependencies%2C%20introduces%20diverse%0Aimage-augmentation%20strategies%2C%20and%20optimizes%20deep%20feature%20learning%20to%20improve%0Aperformance%20on%20complex%20medical%20images.%20Specifically%2C%20we%20design%20a%20GNN-based%0Astrongly%20correlated%20slice-feature%20fusion%20module%20to%20model%20spatial%20relationships%0Abetween%20slices%20and%20integrate%20contextual%20information%20from%20neighboring%20slices%2C%0Athereby%20capturing%20anatomical%20details%20more%20comprehensively%3B%20we%20introduce%20a%0Athree-dimensional%20spatial%20noise-injection%20strategy%20that%20weights%20and%20fuses%0Aspatial%20features%20with%20noise%20to%20enhance%20modeling%20of%20structural%20diversity%3B%20and%20we%0Aincorporate%20a%20grayscale-texture%20classifier%20to%20optimize%20grayscale%20distribution%0Aand%20texture%20representation%20during%20generation.%20Extensive%20experiments%20on%20the%0Apublic%20L2R-OASIS%20and%20L2R-Abdomen%20CT%20datasets%20show%20that%20AnatoMaskGAN%20raises%20PSNR%0Aon%20L2R-OASIS%20to%2026.50%20dB%20%280.43%20dB%20higher%20than%20the%20current%20state%20of%20the%20art%29%20and%0Aachieves%20an%20SSIM%20of%200.8602%20on%20L2R-Abdomen%20CT--a%200.48%20percentage-point%20gain%20over%0Athe%20best%20model%2C%20demonstrating%20its%20superiority%20in%20reconstruction%20accuracy%20and%0Aperceptual%20quality.%20Ablation%20studies%20that%20successively%20remove%20the%20slice-feature%0Afusion%20module%2C%20spatial%203D%20noise-injection%20strategy%2C%20and%20grayscale-texture%0Aclassifier%20reveal%20that%20each%20component%20contributes%20significantly%20to%20PSNR%2C%20SSIM%2C%0Aand%20LPIPS%2C%20further%20confirming%20the%20independent%20value%20of%20each%20core%20design%20in%0Aenhancing%20reconstruction%20accuracy%20and%20perceptual%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11375v1&entry.124074799=Read"},
{"title": "DashCam Video: A complementary low-cost data stream for on-demand\n  forest-infrastructure system monitoring", "author": "Durga Joshi and Chandi Witharana and Robert Fahey and Thomas Worthley and Zhe Zhu and Diego Cerrai", "abstract": "  Our study introduces a novel, low-cost, and reproducible framework for\nreal-time, object-level structural assessment and geolocation of roadside\nvegetation and infrastructure with commonly available but underutilized\ndashboard camera (dashcam) video data. We developed an end-to-end pipeline that\ncombines monocular depth estimation, depth error correction, and geometric\ntriangulation to generate accurate spatial and structural data from\nstreet-level video streams from vehicle-mounted dashcams. Depth maps were first\nestimated using a state-of-the-art monocular depth model, then refined via a\ngradient-boosted regression framework to correct underestimations, particularly\nfor distant objects. The depth correction model achieved strong predictive\nperformance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly\nreducing bias beyond 15 m. Further, object locations were estimated using\nGPS-based triangulation, while object heights were calculated using pin hole\ncamera geometry. Our method was evaluated under varying conditions of camera\nplacement and vehicle speed. Low-speed vehicle with inside camera gave the\nhighest accuracy, with mean geolocation error of 2.83 m, and mean absolute\nerror (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To\nthe best of our knowledge, it is the first framework to combine monocular depth\nmodeling, triangulated GPS-based geolocation, and real-time structural\nassessment for urban vegetation and infrastructure using consumer-grade video\ndata. Our approach complements conventional RS methods, such as LiDAR and image\nby offering a fast, real-time, and cost-effective solution for object-level\nmonitoring of vegetation risks and infrastructure exposure, making it\nespecially valuable for utility companies, and urban planners aiming for\nscalable and frequent assessments in dynamic urban environments.\n", "link": "http://arxiv.org/abs/2508.11591v1", "date": "2025-08-15", "relevancy": 2.2374, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5791}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5556}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DashCam%20Video%3A%20A%20complementary%20low-cost%20data%20stream%20for%20on-demand%0A%20%20forest-infrastructure%20system%20monitoring&body=Title%3A%20DashCam%20Video%3A%20A%20complementary%20low-cost%20data%20stream%20for%20on-demand%0A%20%20forest-infrastructure%20system%20monitoring%0AAuthor%3A%20Durga%20Joshi%20and%20Chandi%20Witharana%20and%20Robert%20Fahey%20and%20Thomas%20Worthley%20and%20Zhe%20Zhu%20and%20Diego%20Cerrai%0AAbstract%3A%20%20%20Our%20study%20introduces%20a%20novel%2C%20low-cost%2C%20and%20reproducible%20framework%20for%0Areal-time%2C%20object-level%20structural%20assessment%20and%20geolocation%20of%20roadside%0Avegetation%20and%20infrastructure%20with%20commonly%20available%20but%20underutilized%0Adashboard%20camera%20%28dashcam%29%20video%20data.%20We%20developed%20an%20end-to-end%20pipeline%20that%0Acombines%20monocular%20depth%20estimation%2C%20depth%20error%20correction%2C%20and%20geometric%0Atriangulation%20to%20generate%20accurate%20spatial%20and%20structural%20data%20from%0Astreet-level%20video%20streams%20from%20vehicle-mounted%20dashcams.%20Depth%20maps%20were%20first%0Aestimated%20using%20a%20state-of-the-art%20monocular%20depth%20model%2C%20then%20refined%20via%20a%0Agradient-boosted%20regression%20framework%20to%20correct%20underestimations%2C%20particularly%0Afor%20distant%20objects.%20The%20depth%20correction%20model%20achieved%20strong%20predictive%0Aperformance%20%28R2%20%3D%200.92%2C%20MAE%20%3D%200.31%20on%20transformed%20scale%29%2C%20significantly%0Areducing%20bias%20beyond%2015%20m.%20Further%2C%20object%20locations%20were%20estimated%20using%0AGPS-based%20triangulation%2C%20while%20object%20heights%20were%20calculated%20using%20pin%20hole%0Acamera%20geometry.%20Our%20method%20was%20evaluated%20under%20varying%20conditions%20of%20camera%0Aplacement%20and%20vehicle%20speed.%20Low-speed%20vehicle%20with%20inside%20camera%20gave%20the%0Ahighest%20accuracy%2C%20with%20mean%20geolocation%20error%20of%202.83%20m%2C%20and%20mean%20absolute%0Aerror%20%28MAE%29%20in%20height%20estimation%20of%202.09%20m%20for%20trees%20and%200.88%20m%20for%20poles.%20To%0Athe%20best%20of%20our%20knowledge%2C%20it%20is%20the%20first%20framework%20to%20combine%20monocular%20depth%0Amodeling%2C%20triangulated%20GPS-based%20geolocation%2C%20and%20real-time%20structural%0Aassessment%20for%20urban%20vegetation%20and%20infrastructure%20using%20consumer-grade%20video%0Adata.%20Our%20approach%20complements%20conventional%20RS%20methods%2C%20such%20as%20LiDAR%20and%20image%0Aby%20offering%20a%20fast%2C%20real-time%2C%20and%20cost-effective%20solution%20for%20object-level%0Amonitoring%20of%20vegetation%20risks%20and%20infrastructure%20exposure%2C%20making%20it%0Aespecially%20valuable%20for%20utility%20companies%2C%20and%20urban%20planners%20aiming%20for%0Ascalable%20and%20frequent%20assessments%20in%20dynamic%20urban%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDashCam%2520Video%253A%2520A%2520complementary%2520low-cost%2520data%2520stream%2520for%2520on-demand%250A%2520%2520forest-infrastructure%2520system%2520monitoring%26entry.906535625%3DDurga%2520Joshi%2520and%2520Chandi%2520Witharana%2520and%2520Robert%2520Fahey%2520and%2520Thomas%2520Worthley%2520and%2520Zhe%2520Zhu%2520and%2520Diego%2520Cerrai%26entry.1292438233%3D%2520%2520Our%2520study%2520introduces%2520a%2520novel%252C%2520low-cost%252C%2520and%2520reproducible%2520framework%2520for%250Areal-time%252C%2520object-level%2520structural%2520assessment%2520and%2520geolocation%2520of%2520roadside%250Avegetation%2520and%2520infrastructure%2520with%2520commonly%2520available%2520but%2520underutilized%250Adashboard%2520camera%2520%2528dashcam%2529%2520video%2520data.%2520We%2520developed%2520an%2520end-to-end%2520pipeline%2520that%250Acombines%2520monocular%2520depth%2520estimation%252C%2520depth%2520error%2520correction%252C%2520and%2520geometric%250Atriangulation%2520to%2520generate%2520accurate%2520spatial%2520and%2520structural%2520data%2520from%250Astreet-level%2520video%2520streams%2520from%2520vehicle-mounted%2520dashcams.%2520Depth%2520maps%2520were%2520first%250Aestimated%2520using%2520a%2520state-of-the-art%2520monocular%2520depth%2520model%252C%2520then%2520refined%2520via%2520a%250Agradient-boosted%2520regression%2520framework%2520to%2520correct%2520underestimations%252C%2520particularly%250Afor%2520distant%2520objects.%2520The%2520depth%2520correction%2520model%2520achieved%2520strong%2520predictive%250Aperformance%2520%2528R2%2520%253D%25200.92%252C%2520MAE%2520%253D%25200.31%2520on%2520transformed%2520scale%2529%252C%2520significantly%250Areducing%2520bias%2520beyond%252015%2520m.%2520Further%252C%2520object%2520locations%2520were%2520estimated%2520using%250AGPS-based%2520triangulation%252C%2520while%2520object%2520heights%2520were%2520calculated%2520using%2520pin%2520hole%250Acamera%2520geometry.%2520Our%2520method%2520was%2520evaluated%2520under%2520varying%2520conditions%2520of%2520camera%250Aplacement%2520and%2520vehicle%2520speed.%2520Low-speed%2520vehicle%2520with%2520inside%2520camera%2520gave%2520the%250Ahighest%2520accuracy%252C%2520with%2520mean%2520geolocation%2520error%2520of%25202.83%2520m%252C%2520and%2520mean%2520absolute%250Aerror%2520%2528MAE%2529%2520in%2520height%2520estimation%2520of%25202.09%2520m%2520for%2520trees%2520and%25200.88%2520m%2520for%2520poles.%2520To%250Athe%2520best%2520of%2520our%2520knowledge%252C%2520it%2520is%2520the%2520first%2520framework%2520to%2520combine%2520monocular%2520depth%250Amodeling%252C%2520triangulated%2520GPS-based%2520geolocation%252C%2520and%2520real-time%2520structural%250Aassessment%2520for%2520urban%2520vegetation%2520and%2520infrastructure%2520using%2520consumer-grade%2520video%250Adata.%2520Our%2520approach%2520complements%2520conventional%2520RS%2520methods%252C%2520such%2520as%2520LiDAR%2520and%2520image%250Aby%2520offering%2520a%2520fast%252C%2520real-time%252C%2520and%2520cost-effective%2520solution%2520for%2520object-level%250Amonitoring%2520of%2520vegetation%2520risks%2520and%2520infrastructure%2520exposure%252C%2520making%2520it%250Aespecially%2520valuable%2520for%2520utility%2520companies%252C%2520and%2520urban%2520planners%2520aiming%2520for%250Ascalable%2520and%2520frequent%2520assessments%2520in%2520dynamic%2520urban%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DashCam%20Video%3A%20A%20complementary%20low-cost%20data%20stream%20for%20on-demand%0A%20%20forest-infrastructure%20system%20monitoring&entry.906535625=Durga%20Joshi%20and%20Chandi%20Witharana%20and%20Robert%20Fahey%20and%20Thomas%20Worthley%20and%20Zhe%20Zhu%20and%20Diego%20Cerrai&entry.1292438233=%20%20Our%20study%20introduces%20a%20novel%2C%20low-cost%2C%20and%20reproducible%20framework%20for%0Areal-time%2C%20object-level%20structural%20assessment%20and%20geolocation%20of%20roadside%0Avegetation%20and%20infrastructure%20with%20commonly%20available%20but%20underutilized%0Adashboard%20camera%20%28dashcam%29%20video%20data.%20We%20developed%20an%20end-to-end%20pipeline%20that%0Acombines%20monocular%20depth%20estimation%2C%20depth%20error%20correction%2C%20and%20geometric%0Atriangulation%20to%20generate%20accurate%20spatial%20and%20structural%20data%20from%0Astreet-level%20video%20streams%20from%20vehicle-mounted%20dashcams.%20Depth%20maps%20were%20first%0Aestimated%20using%20a%20state-of-the-art%20monocular%20depth%20model%2C%20then%20refined%20via%20a%0Agradient-boosted%20regression%20framework%20to%20correct%20underestimations%2C%20particularly%0Afor%20distant%20objects.%20The%20depth%20correction%20model%20achieved%20strong%20predictive%0Aperformance%20%28R2%20%3D%200.92%2C%20MAE%20%3D%200.31%20on%20transformed%20scale%29%2C%20significantly%0Areducing%20bias%20beyond%2015%20m.%20Further%2C%20object%20locations%20were%20estimated%20using%0AGPS-based%20triangulation%2C%20while%20object%20heights%20were%20calculated%20using%20pin%20hole%0Acamera%20geometry.%20Our%20method%20was%20evaluated%20under%20varying%20conditions%20of%20camera%0Aplacement%20and%20vehicle%20speed.%20Low-speed%20vehicle%20with%20inside%20camera%20gave%20the%0Ahighest%20accuracy%2C%20with%20mean%20geolocation%20error%20of%202.83%20m%2C%20and%20mean%20absolute%0Aerror%20%28MAE%29%20in%20height%20estimation%20of%202.09%20m%20for%20trees%20and%200.88%20m%20for%20poles.%20To%0Athe%20best%20of%20our%20knowledge%2C%20it%20is%20the%20first%20framework%20to%20combine%20monocular%20depth%0Amodeling%2C%20triangulated%20GPS-based%20geolocation%2C%20and%20real-time%20structural%0Aassessment%20for%20urban%20vegetation%20and%20infrastructure%20using%20consumer-grade%20video%0Adata.%20Our%20approach%20complements%20conventional%20RS%20methods%2C%20such%20as%20LiDAR%20and%20image%0Aby%20offering%20a%20fast%2C%20real-time%2C%20and%20cost-effective%20solution%20for%20object-level%0Amonitoring%20of%20vegetation%20risks%20and%20infrastructure%20exposure%2C%20making%20it%0Aespecially%20valuable%20for%20utility%20companies%2C%20and%20urban%20planners%20aiming%20for%0Ascalable%20and%20frequent%20assessments%20in%20dynamic%20urban%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11591v1&entry.124074799=Read"},
{"title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models\n  with Real-World Apps", "author": "Kangyu Wang and Hongliang He and Lin Liu and Ruiqi Liang and Zhenzhong Lan and Jianguo Li", "abstract": "  Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.\n", "link": "http://arxiv.org/abs/2508.11452v1", "date": "2025-08-15", "relevancy": 2.2211, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inclusion%20Arena%3A%20An%20Open%20Platform%20for%20Evaluating%20Large%20Foundation%20Models%0A%20%20with%20Real-World%20Apps&body=Title%3A%20Inclusion%20Arena%3A%20An%20Open%20Platform%20for%20Evaluating%20Large%20Foundation%20Models%0A%20%20with%20Real-World%20Apps%0AAuthor%3A%20Kangyu%20Wang%20and%20Hongliang%20He%20and%20Lin%20Liu%20and%20Ruiqi%20Liang%20and%20Zhenzhong%20Lan%20and%20Jianguo%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Ahave%20ushered%20in%20a%20new%20era%20of%20AI%20capabilities%2C%20demonstrating%20near-human-level%0Aperformance%20across%20diverse%20scenarios.%20While%20numerous%20benchmarks%20%28e.g.%2C%20MMLU%29%0Aand%20leaderboards%20%28e.g.%2C%20Chatbot%20Arena%29%20have%20been%20proposed%20to%20help%20evolve%20the%0Adevelopment%20of%20LLMs%20and%20MLLMs%2C%20most%20rely%20on%20static%20datasets%20or%20crowdsourced%0Ageneral-domain%20prompts%2C%20often%20falling%20short%20of%20reflecting%20performance%20in%0Areal-world%20applications.%20To%20bridge%20this%20critical%20gap%2C%20we%20present%20Inclusion%0AArena%2C%20a%20live%20leaderboard%20that%20ranks%20models%20based%20on%20human%20feedback%20collected%0Adirectly%20from%20AI-powered%20applications.%20Our%20platform%20integrates%20pairwise%20model%0Acomparisons%20into%20natural%20user%20interactions%2C%20ensuring%20evaluations%20reflect%0Apractical%20usage%20scenarios.%20For%20robust%20model%20ranking%2C%20we%20employ%20the%0ABradley-Terry%20model%20augmented%20with%20two%20key%20innovations%3A%20%281%29%20Placement%20Matches%2C%0Aa%20cold-start%20mechanism%20to%20quickly%20estimate%20initial%20ratings%20for%20newly%20integrated%0Amodels%2C%20and%20%282%29%20Proximity%20Sampling%2C%20an%20intelligent%20comparison%20strategy%20that%0Aprioritizes%20battles%20between%20models%20of%20similar%20capabilities%20to%20maximize%0Ainformation%20gain%20and%20enhance%20rating%20stability.%20Extensive%20empirical%20analyses%20and%0Asimulations%20demonstrate%20that%20Inclusion%20Arena%20yields%20reliable%20and%20stable%0Arankings%2C%20exhibits%20higher%20data%20transitivity%20compared%20to%20general%20crowdsourced%0Adatasets%2C%20and%20significantly%20mitigates%20the%20risk%20of%20malicious%20manipulation.%20By%0Afostering%20an%20open%20alliance%20between%20foundation%20models%20and%20real-world%0Aapplications%2C%20Inclusion%20Arena%20aims%20to%20accelerate%20the%20development%20of%20LLMs%20and%0AMLLMs%20truly%20optimized%20for%20practical%2C%20user-centric%20deployments.%20The%20platform%20is%0Apublicly%20accessible%20at%20https%3A//doraemon.alipay.com/model-ranking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInclusion%2520Arena%253A%2520An%2520Open%2520Platform%2520for%2520Evaluating%2520Large%2520Foundation%2520Models%250A%2520%2520with%2520Real-World%2520Apps%26entry.906535625%3DKangyu%2520Wang%2520and%2520Hongliang%2520He%2520and%2520Lin%2520Liu%2520and%2520Ruiqi%2520Liang%2520and%2520Zhenzhong%2520Lan%2520and%2520Jianguo%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%250Ahave%2520ushered%2520in%2520a%2520new%2520era%2520of%2520AI%2520capabilities%252C%2520demonstrating%2520near-human-level%250Aperformance%2520across%2520diverse%2520scenarios.%2520While%2520numerous%2520benchmarks%2520%2528e.g.%252C%2520MMLU%2529%250Aand%2520leaderboards%2520%2528e.g.%252C%2520Chatbot%2520Arena%2529%2520have%2520been%2520proposed%2520to%2520help%2520evolve%2520the%250Adevelopment%2520of%2520LLMs%2520and%2520MLLMs%252C%2520most%2520rely%2520on%2520static%2520datasets%2520or%2520crowdsourced%250Ageneral-domain%2520prompts%252C%2520often%2520falling%2520short%2520of%2520reflecting%2520performance%2520in%250Areal-world%2520applications.%2520To%2520bridge%2520this%2520critical%2520gap%252C%2520we%2520present%2520Inclusion%250AArena%252C%2520a%2520live%2520leaderboard%2520that%2520ranks%2520models%2520based%2520on%2520human%2520feedback%2520collected%250Adirectly%2520from%2520AI-powered%2520applications.%2520Our%2520platform%2520integrates%2520pairwise%2520model%250Acomparisons%2520into%2520natural%2520user%2520interactions%252C%2520ensuring%2520evaluations%2520reflect%250Apractical%2520usage%2520scenarios.%2520For%2520robust%2520model%2520ranking%252C%2520we%2520employ%2520the%250ABradley-Terry%2520model%2520augmented%2520with%2520two%2520key%2520innovations%253A%2520%25281%2529%2520Placement%2520Matches%252C%250Aa%2520cold-start%2520mechanism%2520to%2520quickly%2520estimate%2520initial%2520ratings%2520for%2520newly%2520integrated%250Amodels%252C%2520and%2520%25282%2529%2520Proximity%2520Sampling%252C%2520an%2520intelligent%2520comparison%2520strategy%2520that%250Aprioritizes%2520battles%2520between%2520models%2520of%2520similar%2520capabilities%2520to%2520maximize%250Ainformation%2520gain%2520and%2520enhance%2520rating%2520stability.%2520Extensive%2520empirical%2520analyses%2520and%250Asimulations%2520demonstrate%2520that%2520Inclusion%2520Arena%2520yields%2520reliable%2520and%2520stable%250Arankings%252C%2520exhibits%2520higher%2520data%2520transitivity%2520compared%2520to%2520general%2520crowdsourced%250Adatasets%252C%2520and%2520significantly%2520mitigates%2520the%2520risk%2520of%2520malicious%2520manipulation.%2520By%250Afostering%2520an%2520open%2520alliance%2520between%2520foundation%2520models%2520and%2520real-world%250Aapplications%252C%2520Inclusion%2520Arena%2520aims%2520to%2520accelerate%2520the%2520development%2520of%2520LLMs%2520and%250AMLLMs%2520truly%2520optimized%2520for%2520practical%252C%2520user-centric%2520deployments.%2520The%2520platform%2520is%250Apublicly%2520accessible%2520at%2520https%253A//doraemon.alipay.com/model-ranking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inclusion%20Arena%3A%20An%20Open%20Platform%20for%20Evaluating%20Large%20Foundation%20Models%0A%20%20with%20Real-World%20Apps&entry.906535625=Kangyu%20Wang%20and%20Hongliang%20He%20and%20Lin%20Liu%20and%20Ruiqi%20Liang%20and%20Zhenzhong%20Lan%20and%20Jianguo%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Ahave%20ushered%20in%20a%20new%20era%20of%20AI%20capabilities%2C%20demonstrating%20near-human-level%0Aperformance%20across%20diverse%20scenarios.%20While%20numerous%20benchmarks%20%28e.g.%2C%20MMLU%29%0Aand%20leaderboards%20%28e.g.%2C%20Chatbot%20Arena%29%20have%20been%20proposed%20to%20help%20evolve%20the%0Adevelopment%20of%20LLMs%20and%20MLLMs%2C%20most%20rely%20on%20static%20datasets%20or%20crowdsourced%0Ageneral-domain%20prompts%2C%20often%20falling%20short%20of%20reflecting%20performance%20in%0Areal-world%20applications.%20To%20bridge%20this%20critical%20gap%2C%20we%20present%20Inclusion%0AArena%2C%20a%20live%20leaderboard%20that%20ranks%20models%20based%20on%20human%20feedback%20collected%0Adirectly%20from%20AI-powered%20applications.%20Our%20platform%20integrates%20pairwise%20model%0Acomparisons%20into%20natural%20user%20interactions%2C%20ensuring%20evaluations%20reflect%0Apractical%20usage%20scenarios.%20For%20robust%20model%20ranking%2C%20we%20employ%20the%0ABradley-Terry%20model%20augmented%20with%20two%20key%20innovations%3A%20%281%29%20Placement%20Matches%2C%0Aa%20cold-start%20mechanism%20to%20quickly%20estimate%20initial%20ratings%20for%20newly%20integrated%0Amodels%2C%20and%20%282%29%20Proximity%20Sampling%2C%20an%20intelligent%20comparison%20strategy%20that%0Aprioritizes%20battles%20between%20models%20of%20similar%20capabilities%20to%20maximize%0Ainformation%20gain%20and%20enhance%20rating%20stability.%20Extensive%20empirical%20analyses%20and%0Asimulations%20demonstrate%20that%20Inclusion%20Arena%20yields%20reliable%20and%20stable%0Arankings%2C%20exhibits%20higher%20data%20transitivity%20compared%20to%20general%20crowdsourced%0Adatasets%2C%20and%20significantly%20mitigates%20the%20risk%20of%20malicious%20manipulation.%20By%0Afostering%20an%20open%20alliance%20between%20foundation%20models%20and%20real-world%0Aapplications%2C%20Inclusion%20Arena%20aims%20to%20accelerate%20the%20development%20of%20LLMs%20and%0AMLLMs%20truly%20optimized%20for%20practical%2C%20user-centric%20deployments.%20The%20platform%20is%0Apublicly%20accessible%20at%20https%3A//doraemon.alipay.com/model-ranking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11452v1&entry.124074799=Read"},
{"title": "LKFMixer: Exploring Large Kernel Feature For Efficient Image\n  Super-Resolution", "author": "Yinggan Tang and Quanwei Hu", "abstract": "  The success of self-attention (SA) in Transformer demonstrates the importance\nof non-local information to image super-resolution (SR), but the huge computing\npower required makes it difficult to implement lightweight models. To solve\nthis problem, we propose a pure convolutional neural network (CNN) model,\nLKFMixer, which utilizes large convolutional kernel to simulate the ability of\nself-attention to capture non-local features. Specifically, we increase the\nkernel size to 31 to obtain the larger receptive field as possible, and reduce\nthe parameters and computations by coordinate decomposition. Meanwhile, a\nspatial feature modulation block (SFMB) is designed to enhance the focus of\nfeature information on both spatial and channel dimension. In addition, by\nintroducing feature selection block (FSB), the model can adaptively adjust the\nweights between local features and non-local features. Extensive experiments\nshow that the proposed LKFMixer family outperform other state-of-the-art (SOTA)\nmethods in terms of SR performance and reconstruction quality. In particular,\ncompared with SwinIR-light on Manga109 dataset, LKFMixer-L achieves 0.6dB PSNR\nimprovement at $\\times$4 scale, while the inference speed is $\\times$5 times\nfaster. The code is available at https://github.com/Supereeeee/LKFMixer.\n", "link": "http://arxiv.org/abs/2508.11391v1", "date": "2025-08-15", "relevancy": 2.2153, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5662}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5534}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LKFMixer%3A%20Exploring%20Large%20Kernel%20Feature%20For%20Efficient%20Image%0A%20%20Super-Resolution&body=Title%3A%20LKFMixer%3A%20Exploring%20Large%20Kernel%20Feature%20For%20Efficient%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Yinggan%20Tang%20and%20Quanwei%20Hu%0AAbstract%3A%20%20%20The%20success%20of%20self-attention%20%28SA%29%20in%20Transformer%20demonstrates%20the%20importance%0Aof%20non-local%20information%20to%20image%20super-resolution%20%28SR%29%2C%20but%20the%20huge%20computing%0Apower%20required%20makes%20it%20difficult%20to%20implement%20lightweight%20models.%20To%20solve%0Athis%20problem%2C%20we%20propose%20a%20pure%20convolutional%20neural%20network%20%28CNN%29%20model%2C%0ALKFMixer%2C%20which%20utilizes%20large%20convolutional%20kernel%20to%20simulate%20the%20ability%20of%0Aself-attention%20to%20capture%20non-local%20features.%20Specifically%2C%20we%20increase%20the%0Akernel%20size%20to%2031%20to%20obtain%20the%20larger%20receptive%20field%20as%20possible%2C%20and%20reduce%0Athe%20parameters%20and%20computations%20by%20coordinate%20decomposition.%20Meanwhile%2C%20a%0Aspatial%20feature%20modulation%20block%20%28SFMB%29%20is%20designed%20to%20enhance%20the%20focus%20of%0Afeature%20information%20on%20both%20spatial%20and%20channel%20dimension.%20In%20addition%2C%20by%0Aintroducing%20feature%20selection%20block%20%28FSB%29%2C%20the%20model%20can%20adaptively%20adjust%20the%0Aweights%20between%20local%20features%20and%20non-local%20features.%20Extensive%20experiments%0Ashow%20that%20the%20proposed%20LKFMixer%20family%20outperform%20other%20state-of-the-art%20%28SOTA%29%0Amethods%20in%20terms%20of%20SR%20performance%20and%20reconstruction%20quality.%20In%20particular%2C%0Acompared%20with%20SwinIR-light%20on%20Manga109%20dataset%2C%20LKFMixer-L%20achieves%200.6dB%20PSNR%0Aimprovement%20at%20%24%5Ctimes%244%20scale%2C%20while%20the%20inference%20speed%20is%20%24%5Ctimes%245%20times%0Afaster.%20The%20code%20is%20available%20at%20https%3A//github.com/Supereeeee/LKFMixer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLKFMixer%253A%2520Exploring%2520Large%2520Kernel%2520Feature%2520For%2520Efficient%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DYinggan%2520Tang%2520and%2520Quanwei%2520Hu%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520self-attention%2520%2528SA%2529%2520in%2520Transformer%2520demonstrates%2520the%2520importance%250Aof%2520non-local%2520information%2520to%2520image%2520super-resolution%2520%2528SR%2529%252C%2520but%2520the%2520huge%2520computing%250Apower%2520required%2520makes%2520it%2520difficult%2520to%2520implement%2520lightweight%2520models.%2520To%2520solve%250Athis%2520problem%252C%2520we%2520propose%2520a%2520pure%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520model%252C%250ALKFMixer%252C%2520which%2520utilizes%2520large%2520convolutional%2520kernel%2520to%2520simulate%2520the%2520ability%2520of%250Aself-attention%2520to%2520capture%2520non-local%2520features.%2520Specifically%252C%2520we%2520increase%2520the%250Akernel%2520size%2520to%252031%2520to%2520obtain%2520the%2520larger%2520receptive%2520field%2520as%2520possible%252C%2520and%2520reduce%250Athe%2520parameters%2520and%2520computations%2520by%2520coordinate%2520decomposition.%2520Meanwhile%252C%2520a%250Aspatial%2520feature%2520modulation%2520block%2520%2528SFMB%2529%2520is%2520designed%2520to%2520enhance%2520the%2520focus%2520of%250Afeature%2520information%2520on%2520both%2520spatial%2520and%2520channel%2520dimension.%2520In%2520addition%252C%2520by%250Aintroducing%2520feature%2520selection%2520block%2520%2528FSB%2529%252C%2520the%2520model%2520can%2520adaptively%2520adjust%2520the%250Aweights%2520between%2520local%2520features%2520and%2520non-local%2520features.%2520Extensive%2520experiments%250Ashow%2520that%2520the%2520proposed%2520LKFMixer%2520family%2520outperform%2520other%2520state-of-the-art%2520%2528SOTA%2529%250Amethods%2520in%2520terms%2520of%2520SR%2520performance%2520and%2520reconstruction%2520quality.%2520In%2520particular%252C%250Acompared%2520with%2520SwinIR-light%2520on%2520Manga109%2520dataset%252C%2520LKFMixer-L%2520achieves%25200.6dB%2520PSNR%250Aimprovement%2520at%2520%2524%255Ctimes%25244%2520scale%252C%2520while%2520the%2520inference%2520speed%2520is%2520%2524%255Ctimes%25245%2520times%250Afaster.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Supereeeee/LKFMixer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LKFMixer%3A%20Exploring%20Large%20Kernel%20Feature%20For%20Efficient%20Image%0A%20%20Super-Resolution&entry.906535625=Yinggan%20Tang%20and%20Quanwei%20Hu&entry.1292438233=%20%20The%20success%20of%20self-attention%20%28SA%29%20in%20Transformer%20demonstrates%20the%20importance%0Aof%20non-local%20information%20to%20image%20super-resolution%20%28SR%29%2C%20but%20the%20huge%20computing%0Apower%20required%20makes%20it%20difficult%20to%20implement%20lightweight%20models.%20To%20solve%0Athis%20problem%2C%20we%20propose%20a%20pure%20convolutional%20neural%20network%20%28CNN%29%20model%2C%0ALKFMixer%2C%20which%20utilizes%20large%20convolutional%20kernel%20to%20simulate%20the%20ability%20of%0Aself-attention%20to%20capture%20non-local%20features.%20Specifically%2C%20we%20increase%20the%0Akernel%20size%20to%2031%20to%20obtain%20the%20larger%20receptive%20field%20as%20possible%2C%20and%20reduce%0Athe%20parameters%20and%20computations%20by%20coordinate%20decomposition.%20Meanwhile%2C%20a%0Aspatial%20feature%20modulation%20block%20%28SFMB%29%20is%20designed%20to%20enhance%20the%20focus%20of%0Afeature%20information%20on%20both%20spatial%20and%20channel%20dimension.%20In%20addition%2C%20by%0Aintroducing%20feature%20selection%20block%20%28FSB%29%2C%20the%20model%20can%20adaptively%20adjust%20the%0Aweights%20between%20local%20features%20and%20non-local%20features.%20Extensive%20experiments%0Ashow%20that%20the%20proposed%20LKFMixer%20family%20outperform%20other%20state-of-the-art%20%28SOTA%29%0Amethods%20in%20terms%20of%20SR%20performance%20and%20reconstruction%20quality.%20In%20particular%2C%0Acompared%20with%20SwinIR-light%20on%20Manga109%20dataset%2C%20LKFMixer-L%20achieves%200.6dB%20PSNR%0Aimprovement%20at%20%24%5Ctimes%244%20scale%2C%20while%20the%20inference%20speed%20is%20%24%5Ctimes%245%20times%0Afaster.%20The%20code%20is%20available%20at%20https%3A//github.com/Supereeeee/LKFMixer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11391v1&entry.124074799=Read"},
{"title": "Bridging AI Innovation and Healthcare Needs: Lessons Learned from\n  Incorporating Modern NLP at The BC Cancer Registry", "author": "Lovedeep Gondara and Gregory Arbour and Raymond Ng and Jonathan Simkin and Shebnum Devji", "abstract": "  Automating data extraction from clinical documents offers significant\npotential to improve efficiency in healthcare settings, yet deploying Natural\nLanguage Processing (NLP) solutions presents practical challenges. Drawing upon\nour experience implementing various NLP models for information extraction and\nclassification tasks at the British Columbia Cancer Registry (BCCR), this paper\nshares key lessons learned throughout the project lifecycle. We emphasize the\ncritical importance of defining problems based on clear business objectives\nrather than solely technical accuracy, adopting an iterative approach to\ndevelopment, and fostering deep interdisciplinary collaboration and co-design\ninvolving domain experts, end-users, and ML specialists from inception. Further\ninsights highlight the need for pragmatic model selection (including hybrid\napproaches and simpler methods where appropriate), rigorous attention to data\nquality (representativeness, drift, annotation), robust error mitigation\nstrategies involving human-in-the-loop validation and ongoing audits, and\nbuilding organizational AI literacy. These practical considerations,\ngeneralizable beyond cancer registries, provide guidance for healthcare\norganizations seeking to successfully implement AI/NLP solutions to enhance\ndata management processes and ultimately improve patient care and public health\noutcomes.\n", "link": "http://arxiv.org/abs/2508.09991v2", "date": "2025-08-15", "relevancy": 2.193, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4394}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20AI%20Innovation%20and%20Healthcare%20Needs%3A%20Lessons%20Learned%20from%0A%20%20Incorporating%20Modern%20NLP%20at%20The%20BC%20Cancer%20Registry&body=Title%3A%20Bridging%20AI%20Innovation%20and%20Healthcare%20Needs%3A%20Lessons%20Learned%20from%0A%20%20Incorporating%20Modern%20NLP%20at%20The%20BC%20Cancer%20Registry%0AAuthor%3A%20Lovedeep%20Gondara%20and%20Gregory%20Arbour%20and%20Raymond%20Ng%20and%20Jonathan%20Simkin%20and%20Shebnum%20Devji%0AAbstract%3A%20%20%20Automating%20data%20extraction%20from%20clinical%20documents%20offers%20significant%0Apotential%20to%20improve%20efficiency%20in%20healthcare%20settings%2C%20yet%20deploying%20Natural%0ALanguage%20Processing%20%28NLP%29%20solutions%20presents%20practical%20challenges.%20Drawing%20upon%0Aour%20experience%20implementing%20various%20NLP%20models%20for%20information%20extraction%20and%0Aclassification%20tasks%20at%20the%20British%20Columbia%20Cancer%20Registry%20%28BCCR%29%2C%20this%20paper%0Ashares%20key%20lessons%20learned%20throughout%20the%20project%20lifecycle.%20We%20emphasize%20the%0Acritical%20importance%20of%20defining%20problems%20based%20on%20clear%20business%20objectives%0Arather%20than%20solely%20technical%20accuracy%2C%20adopting%20an%20iterative%20approach%20to%0Adevelopment%2C%20and%20fostering%20deep%20interdisciplinary%20collaboration%20and%20co-design%0Ainvolving%20domain%20experts%2C%20end-users%2C%20and%20ML%20specialists%20from%20inception.%20Further%0Ainsights%20highlight%20the%20need%20for%20pragmatic%20model%20selection%20%28including%20hybrid%0Aapproaches%20and%20simpler%20methods%20where%20appropriate%29%2C%20rigorous%20attention%20to%20data%0Aquality%20%28representativeness%2C%20drift%2C%20annotation%29%2C%20robust%20error%20mitigation%0Astrategies%20involving%20human-in-the-loop%20validation%20and%20ongoing%20audits%2C%20and%0Abuilding%20organizational%20AI%20literacy.%20These%20practical%20considerations%2C%0Ageneralizable%20beyond%20cancer%20registries%2C%20provide%20guidance%20for%20healthcare%0Aorganizations%20seeking%20to%20successfully%20implement%20AI/NLP%20solutions%20to%20enhance%0Adata%20management%20processes%20and%20ultimately%20improve%20patient%20care%20and%20public%20health%0Aoutcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09991v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520AI%2520Innovation%2520and%2520Healthcare%2520Needs%253A%2520Lessons%2520Learned%2520from%250A%2520%2520Incorporating%2520Modern%2520NLP%2520at%2520The%2520BC%2520Cancer%2520Registry%26entry.906535625%3DLovedeep%2520Gondara%2520and%2520Gregory%2520Arbour%2520and%2520Raymond%2520Ng%2520and%2520Jonathan%2520Simkin%2520and%2520Shebnum%2520Devji%26entry.1292438233%3D%2520%2520Automating%2520data%2520extraction%2520from%2520clinical%2520documents%2520offers%2520significant%250Apotential%2520to%2520improve%2520efficiency%2520in%2520healthcare%2520settings%252C%2520yet%2520deploying%2520Natural%250ALanguage%2520Processing%2520%2528NLP%2529%2520solutions%2520presents%2520practical%2520challenges.%2520Drawing%2520upon%250Aour%2520experience%2520implementing%2520various%2520NLP%2520models%2520for%2520information%2520extraction%2520and%250Aclassification%2520tasks%2520at%2520the%2520British%2520Columbia%2520Cancer%2520Registry%2520%2528BCCR%2529%252C%2520this%2520paper%250Ashares%2520key%2520lessons%2520learned%2520throughout%2520the%2520project%2520lifecycle.%2520We%2520emphasize%2520the%250Acritical%2520importance%2520of%2520defining%2520problems%2520based%2520on%2520clear%2520business%2520objectives%250Arather%2520than%2520solely%2520technical%2520accuracy%252C%2520adopting%2520an%2520iterative%2520approach%2520to%250Adevelopment%252C%2520and%2520fostering%2520deep%2520interdisciplinary%2520collaboration%2520and%2520co-design%250Ainvolving%2520domain%2520experts%252C%2520end-users%252C%2520and%2520ML%2520specialists%2520from%2520inception.%2520Further%250Ainsights%2520highlight%2520the%2520need%2520for%2520pragmatic%2520model%2520selection%2520%2528including%2520hybrid%250Aapproaches%2520and%2520simpler%2520methods%2520where%2520appropriate%2529%252C%2520rigorous%2520attention%2520to%2520data%250Aquality%2520%2528representativeness%252C%2520drift%252C%2520annotation%2529%252C%2520robust%2520error%2520mitigation%250Astrategies%2520involving%2520human-in-the-loop%2520validation%2520and%2520ongoing%2520audits%252C%2520and%250Abuilding%2520organizational%2520AI%2520literacy.%2520These%2520practical%2520considerations%252C%250Ageneralizable%2520beyond%2520cancer%2520registries%252C%2520provide%2520guidance%2520for%2520healthcare%250Aorganizations%2520seeking%2520to%2520successfully%2520implement%2520AI/NLP%2520solutions%2520to%2520enhance%250Adata%2520management%2520processes%2520and%2520ultimately%2520improve%2520patient%2520care%2520and%2520public%2520health%250Aoutcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09991v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20AI%20Innovation%20and%20Healthcare%20Needs%3A%20Lessons%20Learned%20from%0A%20%20Incorporating%20Modern%20NLP%20at%20The%20BC%20Cancer%20Registry&entry.906535625=Lovedeep%20Gondara%20and%20Gregory%20Arbour%20and%20Raymond%20Ng%20and%20Jonathan%20Simkin%20and%20Shebnum%20Devji&entry.1292438233=%20%20Automating%20data%20extraction%20from%20clinical%20documents%20offers%20significant%0Apotential%20to%20improve%20efficiency%20in%20healthcare%20settings%2C%20yet%20deploying%20Natural%0ALanguage%20Processing%20%28NLP%29%20solutions%20presents%20practical%20challenges.%20Drawing%20upon%0Aour%20experience%20implementing%20various%20NLP%20models%20for%20information%20extraction%20and%0Aclassification%20tasks%20at%20the%20British%20Columbia%20Cancer%20Registry%20%28BCCR%29%2C%20this%20paper%0Ashares%20key%20lessons%20learned%20throughout%20the%20project%20lifecycle.%20We%20emphasize%20the%0Acritical%20importance%20of%20defining%20problems%20based%20on%20clear%20business%20objectives%0Arather%20than%20solely%20technical%20accuracy%2C%20adopting%20an%20iterative%20approach%20to%0Adevelopment%2C%20and%20fostering%20deep%20interdisciplinary%20collaboration%20and%20co-design%0Ainvolving%20domain%20experts%2C%20end-users%2C%20and%20ML%20specialists%20from%20inception.%20Further%0Ainsights%20highlight%20the%20need%20for%20pragmatic%20model%20selection%20%28including%20hybrid%0Aapproaches%20and%20simpler%20methods%20where%20appropriate%29%2C%20rigorous%20attention%20to%20data%0Aquality%20%28representativeness%2C%20drift%2C%20annotation%29%2C%20robust%20error%20mitigation%0Astrategies%20involving%20human-in-the-loop%20validation%20and%20ongoing%20audits%2C%20and%0Abuilding%20organizational%20AI%20literacy.%20These%20practical%20considerations%2C%0Ageneralizable%20beyond%20cancer%20registries%2C%20provide%20guidance%20for%20healthcare%0Aorganizations%20seeking%20to%20successfully%20implement%20AI/NLP%20solutions%20to%20enhance%0Adata%20management%20processes%20and%20ultimately%20improve%20patient%20care%20and%20public%20health%0Aoutcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09991v2&entry.124074799=Read"},
{"title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition", "author": "Niki Foteinopoulou and Ignas Budvytis and Stephan Liwicki", "abstract": "  Low-Rank Adaptation (LoRA) has become a widely adopted technique in\ntext-to-image diffusion models, enabling the personalisation of visual concepts\nsuch as characters, styles, and objects. However, existing approaches struggle\nto effectively compose multiple LoRA adapters, particularly in open-ended\nsettings where the number and nature of required skills are not known in\nadvance. In this work, we present LoRAtorio, a novel train-free framework for\nmulti-LoRA composition that leverages intrinsic model behaviour. Our method is\nmotivated by two key observations: (1) LoRA adapters trained on narrow domains\nproduce denoised outputs that diverge from the base model, and (2) when\noperating out-of-distribution, LoRA outputs show behaviour closer to the base\nmodel than when conditioned in distribution. The balance between these two\nobservations allows for exceptional performance in the single LoRA scenario,\nwhich nevertheless deteriorates when multiple LoRAs are loaded. Our method\noperates in the latent space by dividing it into spatial patches and computing\ncosine similarity between each patch's predicted noise and that of the base\nmodel. These similarities are used to construct a spatially-aware weight\nmatrix, which guides a weighted aggregation of LoRA outputs. To address domain\ndrift, we further propose a modification to classifier-free guidance that\nincorporates the base model's unconditional score into the composition. We\nextend this formulation to a dynamic module selection setting, enabling\ninference-time selection of relevant LoRA adapters from a large pool. LoRAtorio\nachieves state-of-the-art performance, showing up to a 1.3% improvement in\nClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises\neffectively to multiple latent diffusion models.\n", "link": "http://arxiv.org/abs/2508.11624v1", "date": "2025-08-15", "relevancy": 2.1921, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5604}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5436}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRAtorio%3A%20An%20intrinsic%20approach%20to%20LoRA%20Skill%20Composition&body=Title%3A%20LoRAtorio%3A%20An%20intrinsic%20approach%20to%20LoRA%20Skill%20Composition%0AAuthor%3A%20Niki%20Foteinopoulou%20and%20Ignas%20Budvytis%20and%20Stephan%20Liwicki%0AAbstract%3A%20%20%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20become%20a%20widely%20adopted%20technique%20in%0Atext-to-image%20diffusion%20models%2C%20enabling%20the%20personalisation%20of%20visual%20concepts%0Asuch%20as%20characters%2C%20styles%2C%20and%20objects.%20However%2C%20existing%20approaches%20struggle%0Ato%20effectively%20compose%20multiple%20LoRA%20adapters%2C%20particularly%20in%20open-ended%0Asettings%20where%20the%20number%20and%20nature%20of%20required%20skills%20are%20not%20known%20in%0Aadvance.%20In%20this%20work%2C%20we%20present%20LoRAtorio%2C%20a%20novel%20train-free%20framework%20for%0Amulti-LoRA%20composition%20that%20leverages%20intrinsic%20model%20behaviour.%20Our%20method%20is%0Amotivated%20by%20two%20key%20observations%3A%20%281%29%20LoRA%20adapters%20trained%20on%20narrow%20domains%0Aproduce%20denoised%20outputs%20that%20diverge%20from%20the%20base%20model%2C%20and%20%282%29%20when%0Aoperating%20out-of-distribution%2C%20LoRA%20outputs%20show%20behaviour%20closer%20to%20the%20base%0Amodel%20than%20when%20conditioned%20in%20distribution.%20The%20balance%20between%20these%20two%0Aobservations%20allows%20for%20exceptional%20performance%20in%20the%20single%20LoRA%20scenario%2C%0Awhich%20nevertheless%20deteriorates%20when%20multiple%20LoRAs%20are%20loaded.%20Our%20method%0Aoperates%20in%20the%20latent%20space%20by%20dividing%20it%20into%20spatial%20patches%20and%20computing%0Acosine%20similarity%20between%20each%20patch%27s%20predicted%20noise%20and%20that%20of%20the%20base%0Amodel.%20These%20similarities%20are%20used%20to%20construct%20a%20spatially-aware%20weight%0Amatrix%2C%20which%20guides%20a%20weighted%20aggregation%20of%20LoRA%20outputs.%20To%20address%20domain%0Adrift%2C%20we%20further%20propose%20a%20modification%20to%20classifier-free%20guidance%20that%0Aincorporates%20the%20base%20model%27s%20unconditional%20score%20into%20the%20composition.%20We%0Aextend%20this%20formulation%20to%20a%20dynamic%20module%20selection%20setting%2C%20enabling%0Ainference-time%20selection%20of%20relevant%20LoRA%20adapters%20from%20a%20large%20pool.%20LoRAtorio%0Aachieves%20state-of-the-art%20performance%2C%20showing%20up%20to%20a%201.3%25%20improvement%20in%0AClipScore%20and%20a%2072.43%25%20win%20rate%20in%20GPT-4V%20pairwise%20evaluations%2C%20and%20generalises%0Aeffectively%20to%20multiple%20latent%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRAtorio%253A%2520An%2520intrinsic%2520approach%2520to%2520LoRA%2520Skill%2520Composition%26entry.906535625%3DNiki%2520Foteinopoulou%2520and%2520Ignas%2520Budvytis%2520and%2520Stephan%2520Liwicki%26entry.1292438233%3D%2520%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520has%2520become%2520a%2520widely%2520adopted%2520technique%2520in%250Atext-to-image%2520diffusion%2520models%252C%2520enabling%2520the%2520personalisation%2520of%2520visual%2520concepts%250Asuch%2520as%2520characters%252C%2520styles%252C%2520and%2520objects.%2520However%252C%2520existing%2520approaches%2520struggle%250Ato%2520effectively%2520compose%2520multiple%2520LoRA%2520adapters%252C%2520particularly%2520in%2520open-ended%250Asettings%2520where%2520the%2520number%2520and%2520nature%2520of%2520required%2520skills%2520are%2520not%2520known%2520in%250Aadvance.%2520In%2520this%2520work%252C%2520we%2520present%2520LoRAtorio%252C%2520a%2520novel%2520train-free%2520framework%2520for%250Amulti-LoRA%2520composition%2520that%2520leverages%2520intrinsic%2520model%2520behaviour.%2520Our%2520method%2520is%250Amotivated%2520by%2520two%2520key%2520observations%253A%2520%25281%2529%2520LoRA%2520adapters%2520trained%2520on%2520narrow%2520domains%250Aproduce%2520denoised%2520outputs%2520that%2520diverge%2520from%2520the%2520base%2520model%252C%2520and%2520%25282%2529%2520when%250Aoperating%2520out-of-distribution%252C%2520LoRA%2520outputs%2520show%2520behaviour%2520closer%2520to%2520the%2520base%250Amodel%2520than%2520when%2520conditioned%2520in%2520distribution.%2520The%2520balance%2520between%2520these%2520two%250Aobservations%2520allows%2520for%2520exceptional%2520performance%2520in%2520the%2520single%2520LoRA%2520scenario%252C%250Awhich%2520nevertheless%2520deteriorates%2520when%2520multiple%2520LoRAs%2520are%2520loaded.%2520Our%2520method%250Aoperates%2520in%2520the%2520latent%2520space%2520by%2520dividing%2520it%2520into%2520spatial%2520patches%2520and%2520computing%250Acosine%2520similarity%2520between%2520each%2520patch%2527s%2520predicted%2520noise%2520and%2520that%2520of%2520the%2520base%250Amodel.%2520These%2520similarities%2520are%2520used%2520to%2520construct%2520a%2520spatially-aware%2520weight%250Amatrix%252C%2520which%2520guides%2520a%2520weighted%2520aggregation%2520of%2520LoRA%2520outputs.%2520To%2520address%2520domain%250Adrift%252C%2520we%2520further%2520propose%2520a%2520modification%2520to%2520classifier-free%2520guidance%2520that%250Aincorporates%2520the%2520base%2520model%2527s%2520unconditional%2520score%2520into%2520the%2520composition.%2520We%250Aextend%2520this%2520formulation%2520to%2520a%2520dynamic%2520module%2520selection%2520setting%252C%2520enabling%250Ainference-time%2520selection%2520of%2520relevant%2520LoRA%2520adapters%2520from%2520a%2520large%2520pool.%2520LoRAtorio%250Aachieves%2520state-of-the-art%2520performance%252C%2520showing%2520up%2520to%2520a%25201.3%2525%2520improvement%2520in%250AClipScore%2520and%2520a%252072.43%2525%2520win%2520rate%2520in%2520GPT-4V%2520pairwise%2520evaluations%252C%2520and%2520generalises%250Aeffectively%2520to%2520multiple%2520latent%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRAtorio%3A%20An%20intrinsic%20approach%20to%20LoRA%20Skill%20Composition&entry.906535625=Niki%20Foteinopoulou%20and%20Ignas%20Budvytis%20and%20Stephan%20Liwicki&entry.1292438233=%20%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20become%20a%20widely%20adopted%20technique%20in%0Atext-to-image%20diffusion%20models%2C%20enabling%20the%20personalisation%20of%20visual%20concepts%0Asuch%20as%20characters%2C%20styles%2C%20and%20objects.%20However%2C%20existing%20approaches%20struggle%0Ato%20effectively%20compose%20multiple%20LoRA%20adapters%2C%20particularly%20in%20open-ended%0Asettings%20where%20the%20number%20and%20nature%20of%20required%20skills%20are%20not%20known%20in%0Aadvance.%20In%20this%20work%2C%20we%20present%20LoRAtorio%2C%20a%20novel%20train-free%20framework%20for%0Amulti-LoRA%20composition%20that%20leverages%20intrinsic%20model%20behaviour.%20Our%20method%20is%0Amotivated%20by%20two%20key%20observations%3A%20%281%29%20LoRA%20adapters%20trained%20on%20narrow%20domains%0Aproduce%20denoised%20outputs%20that%20diverge%20from%20the%20base%20model%2C%20and%20%282%29%20when%0Aoperating%20out-of-distribution%2C%20LoRA%20outputs%20show%20behaviour%20closer%20to%20the%20base%0Amodel%20than%20when%20conditioned%20in%20distribution.%20The%20balance%20between%20these%20two%0Aobservations%20allows%20for%20exceptional%20performance%20in%20the%20single%20LoRA%20scenario%2C%0Awhich%20nevertheless%20deteriorates%20when%20multiple%20LoRAs%20are%20loaded.%20Our%20method%0Aoperates%20in%20the%20latent%20space%20by%20dividing%20it%20into%20spatial%20patches%20and%20computing%0Acosine%20similarity%20between%20each%20patch%27s%20predicted%20noise%20and%20that%20of%20the%20base%0Amodel.%20These%20similarities%20are%20used%20to%20construct%20a%20spatially-aware%20weight%0Amatrix%2C%20which%20guides%20a%20weighted%20aggregation%20of%20LoRA%20outputs.%20To%20address%20domain%0Adrift%2C%20we%20further%20propose%20a%20modification%20to%20classifier-free%20guidance%20that%0Aincorporates%20the%20base%20model%27s%20unconditional%20score%20into%20the%20composition.%20We%0Aextend%20this%20formulation%20to%20a%20dynamic%20module%20selection%20setting%2C%20enabling%0Ainference-time%20selection%20of%20relevant%20LoRA%20adapters%20from%20a%20large%20pool.%20LoRAtorio%0Aachieves%20state-of-the-art%20performance%2C%20showing%20up%20to%20a%201.3%25%20improvement%20in%0AClipScore%20and%20a%2072.43%25%20win%20rate%20in%20GPT-4V%20pairwise%20evaluations%2C%20and%20generalises%0Aeffectively%20to%20multiple%20latent%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11624v1&entry.124074799=Read"},
{"title": "Perception in Plan: Coupled Perception and Planning for End-to-End\n  Autonomous Driving", "author": "Bozhou Zhang and Jingyu Li and Nan Song and Li Zhang", "abstract": "  End-to-end autonomous driving has achieved remarkable advancements in recent\nyears. Existing methods primarily follow a perception-planning paradigm, where\nperception and planning are executed sequentially within a fully differentiable\nframework for planning-oriented optimization. We further advance this paradigm\nthrough a perception-in-plan framework design, which integrates perception into\nthe planning process. This design facilitates targeted perception guided by\nevolving planning objectives over time, ultimately enhancing planning\nperformance. Building on this insight, we introduce VeteranAD, a coupled\nperception and planning framework for end-to-end autonomous driving. By\nincorporating multi-mode anchored trajectories as planning priors, the\nperception module is specifically designed to gather traffic elements along\nthese trajectories, enabling comprehensive and targeted perception. Planning\ntrajectories are then generated based on both the perception results and the\nplanning priors. To make perception fully serve planning, we adopt an\nautoregressive strategy that progressively predicts future trajectories while\nfocusing on relevant regions for targeted perception at each step. With this\nsimple yet effective design, VeteranAD fully unleashes the potential of\nplanning-oriented end-to-end methods, leading to more accurate and reliable\ndriving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets\ndemonstrate that our VeteranAD achieves state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2508.11488v1", "date": "2025-08-15", "relevancy": 2.1854, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5643}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5362}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception%20in%20Plan%3A%20Coupled%20Perception%20and%20Planning%20for%20End-to-End%0A%20%20Autonomous%20Driving&body=Title%3A%20Perception%20in%20Plan%3A%20Coupled%20Perception%20and%20Planning%20for%20End-to-End%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Bozhou%20Zhang%20and%20Jingyu%20Li%20and%20Nan%20Song%20and%20Li%20Zhang%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20has%20achieved%20remarkable%20advancements%20in%20recent%0Ayears.%20Existing%20methods%20primarily%20follow%20a%20perception-planning%20paradigm%2C%20where%0Aperception%20and%20planning%20are%20executed%20sequentially%20within%20a%20fully%20differentiable%0Aframework%20for%20planning-oriented%20optimization.%20We%20further%20advance%20this%20paradigm%0Athrough%20a%20perception-in-plan%20framework%20design%2C%20which%20integrates%20perception%20into%0Athe%20planning%20process.%20This%20design%20facilitates%20targeted%20perception%20guided%20by%0Aevolving%20planning%20objectives%20over%20time%2C%20ultimately%20enhancing%20planning%0Aperformance.%20Building%20on%20this%20insight%2C%20we%20introduce%20VeteranAD%2C%20a%20coupled%0Aperception%20and%20planning%20framework%20for%20end-to-end%20autonomous%20driving.%20By%0Aincorporating%20multi-mode%20anchored%20trajectories%20as%20planning%20priors%2C%20the%0Aperception%20module%20is%20specifically%20designed%20to%20gather%20traffic%20elements%20along%0Athese%20trajectories%2C%20enabling%20comprehensive%20and%20targeted%20perception.%20Planning%0Atrajectories%20are%20then%20generated%20based%20on%20both%20the%20perception%20results%20and%20the%0Aplanning%20priors.%20To%20make%20perception%20fully%20serve%20planning%2C%20we%20adopt%20an%0Aautoregressive%20strategy%20that%20progressively%20predicts%20future%20trajectories%20while%0Afocusing%20on%20relevant%20regions%20for%20targeted%20perception%20at%20each%20step.%20With%20this%0Asimple%20yet%20effective%20design%2C%20VeteranAD%20fully%20unleashes%20the%20potential%20of%0Aplanning-oriented%20end-to-end%20methods%2C%20leading%20to%20more%20accurate%20and%20reliable%0Adriving%20behavior.%20Extensive%20experiments%20on%20the%20NAVSIM%20and%20Bench2Drive%20datasets%0Ademonstrate%20that%20our%20VeteranAD%20achieves%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception%2520in%2520Plan%253A%2520Coupled%2520Perception%2520and%2520Planning%2520for%2520End-to-End%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DBozhou%2520Zhang%2520and%2520Jingyu%2520Li%2520and%2520Nan%2520Song%2520and%2520Li%2520Zhang%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520has%2520achieved%2520remarkable%2520advancements%2520in%2520recent%250Ayears.%2520Existing%2520methods%2520primarily%2520follow%2520a%2520perception-planning%2520paradigm%252C%2520where%250Aperception%2520and%2520planning%2520are%2520executed%2520sequentially%2520within%2520a%2520fully%2520differentiable%250Aframework%2520for%2520planning-oriented%2520optimization.%2520We%2520further%2520advance%2520this%2520paradigm%250Athrough%2520a%2520perception-in-plan%2520framework%2520design%252C%2520which%2520integrates%2520perception%2520into%250Athe%2520planning%2520process.%2520This%2520design%2520facilitates%2520targeted%2520perception%2520guided%2520by%250Aevolving%2520planning%2520objectives%2520over%2520time%252C%2520ultimately%2520enhancing%2520planning%250Aperformance.%2520Building%2520on%2520this%2520insight%252C%2520we%2520introduce%2520VeteranAD%252C%2520a%2520coupled%250Aperception%2520and%2520planning%2520framework%2520for%2520end-to-end%2520autonomous%2520driving.%2520By%250Aincorporating%2520multi-mode%2520anchored%2520trajectories%2520as%2520planning%2520priors%252C%2520the%250Aperception%2520module%2520is%2520specifically%2520designed%2520to%2520gather%2520traffic%2520elements%2520along%250Athese%2520trajectories%252C%2520enabling%2520comprehensive%2520and%2520targeted%2520perception.%2520Planning%250Atrajectories%2520are%2520then%2520generated%2520based%2520on%2520both%2520the%2520perception%2520results%2520and%2520the%250Aplanning%2520priors.%2520To%2520make%2520perception%2520fully%2520serve%2520planning%252C%2520we%2520adopt%2520an%250Aautoregressive%2520strategy%2520that%2520progressively%2520predicts%2520future%2520trajectories%2520while%250Afocusing%2520on%2520relevant%2520regions%2520for%2520targeted%2520perception%2520at%2520each%2520step.%2520With%2520this%250Asimple%2520yet%2520effective%2520design%252C%2520VeteranAD%2520fully%2520unleashes%2520the%2520potential%2520of%250Aplanning-oriented%2520end-to-end%2520methods%252C%2520leading%2520to%2520more%2520accurate%2520and%2520reliable%250Adriving%2520behavior.%2520Extensive%2520experiments%2520on%2520the%2520NAVSIM%2520and%2520Bench2Drive%2520datasets%250Ademonstrate%2520that%2520our%2520VeteranAD%2520achieves%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%20in%20Plan%3A%20Coupled%20Perception%20and%20Planning%20for%20End-to-End%0A%20%20Autonomous%20Driving&entry.906535625=Bozhou%20Zhang%20and%20Jingyu%20Li%20and%20Nan%20Song%20and%20Li%20Zhang&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20has%20achieved%20remarkable%20advancements%20in%20recent%0Ayears.%20Existing%20methods%20primarily%20follow%20a%20perception-planning%20paradigm%2C%20where%0Aperception%20and%20planning%20are%20executed%20sequentially%20within%20a%20fully%20differentiable%0Aframework%20for%20planning-oriented%20optimization.%20We%20further%20advance%20this%20paradigm%0Athrough%20a%20perception-in-plan%20framework%20design%2C%20which%20integrates%20perception%20into%0Athe%20planning%20process.%20This%20design%20facilitates%20targeted%20perception%20guided%20by%0Aevolving%20planning%20objectives%20over%20time%2C%20ultimately%20enhancing%20planning%0Aperformance.%20Building%20on%20this%20insight%2C%20we%20introduce%20VeteranAD%2C%20a%20coupled%0Aperception%20and%20planning%20framework%20for%20end-to-end%20autonomous%20driving.%20By%0Aincorporating%20multi-mode%20anchored%20trajectories%20as%20planning%20priors%2C%20the%0Aperception%20module%20is%20specifically%20designed%20to%20gather%20traffic%20elements%20along%0Athese%20trajectories%2C%20enabling%20comprehensive%20and%20targeted%20perception.%20Planning%0Atrajectories%20are%20then%20generated%20based%20on%20both%20the%20perception%20results%20and%20the%0Aplanning%20priors.%20To%20make%20perception%20fully%20serve%20planning%2C%20we%20adopt%20an%0Aautoregressive%20strategy%20that%20progressively%20predicts%20future%20trajectories%20while%0Afocusing%20on%20relevant%20regions%20for%20targeted%20perception%20at%20each%20step.%20With%20this%0Asimple%20yet%20effective%20design%2C%20VeteranAD%20fully%20unleashes%20the%20potential%20of%0Aplanning-oriented%20end-to-end%20methods%2C%20leading%20to%20more%20accurate%20and%20reliable%0Adriving%20behavior.%20Extensive%20experiments%20on%20the%20NAVSIM%20and%20Bench2Drive%20datasets%0Ademonstrate%20that%20our%20VeteranAD%20achieves%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11488v1&entry.124074799=Read"},
{"title": "Unified Knowledge Distillation Framework: Fine-Grained Alignment and\n  Geometric Relationship Preservation for Deep Face Recognition", "author": "Durgesh Mishra and Rishabh Uikey", "abstract": "  Knowledge Distillation is crucial for optimizing face recognition models for\ndeployment in computationally limited settings, such as edge devices.\nTraditional KD methods, such as Raw L2 Feature Distillation or Feature\nConsistency loss, often fail to capture both fine-grained instance-level\ndetails and complex relational structures, leading to suboptimal performance.\nWe propose a unified approach that integrates two novel loss functions,\nInstance-Level Embedding Distillation and Relation-Based Pairwise Similarity\nDistillation. Instance-Level Embedding Distillation focuses on aligning\nindividual feature embeddings by leveraging a dynamic hard mining strategy,\nthereby enhancing learning from challenging examples. Relation-Based Pairwise\nSimilarity Distillation captures relational information through pairwise\nsimilarity relationships, employing a memory bank mechanism and a sample mining\nstrategy. This unified framework ensures both effective instance-level\nalignment and preservation of geometric relationships between samples, leading\nto a more comprehensive distillation process. Our unified framework outperforms\nstate-of-the-art distillation methods across multiple benchmark face\nrecognition datasets, as demonstrated by extensive experimental evaluations.\nInterestingly, when using strong teacher networks compared to the student, our\nunified KD enables the student to even surpass the teacher's accuracy.\n", "link": "http://arxiv.org/abs/2508.11376v1", "date": "2025-08-15", "relevancy": 2.1777, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5679}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5464}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Knowledge%20Distillation%20Framework%3A%20Fine-Grained%20Alignment%20and%0A%20%20Geometric%20Relationship%20Preservation%20for%20Deep%20Face%20Recognition&body=Title%3A%20Unified%20Knowledge%20Distillation%20Framework%3A%20Fine-Grained%20Alignment%20and%0A%20%20Geometric%20Relationship%20Preservation%20for%20Deep%20Face%20Recognition%0AAuthor%3A%20Durgesh%20Mishra%20and%20Rishabh%20Uikey%0AAbstract%3A%20%20%20Knowledge%20Distillation%20is%20crucial%20for%20optimizing%20face%20recognition%20models%20for%0Adeployment%20in%20computationally%20limited%20settings%2C%20such%20as%20edge%20devices.%0ATraditional%20KD%20methods%2C%20such%20as%20Raw%20L2%20Feature%20Distillation%20or%20Feature%0AConsistency%20loss%2C%20often%20fail%20to%20capture%20both%20fine-grained%20instance-level%0Adetails%20and%20complex%20relational%20structures%2C%20leading%20to%20suboptimal%20performance.%0AWe%20propose%20a%20unified%20approach%20that%20integrates%20two%20novel%20loss%20functions%2C%0AInstance-Level%20Embedding%20Distillation%20and%20Relation-Based%20Pairwise%20Similarity%0ADistillation.%20Instance-Level%20Embedding%20Distillation%20focuses%20on%20aligning%0Aindividual%20feature%20embeddings%20by%20leveraging%20a%20dynamic%20hard%20mining%20strategy%2C%0Athereby%20enhancing%20learning%20from%20challenging%20examples.%20Relation-Based%20Pairwise%0ASimilarity%20Distillation%20captures%20relational%20information%20through%20pairwise%0Asimilarity%20relationships%2C%20employing%20a%20memory%20bank%20mechanism%20and%20a%20sample%20mining%0Astrategy.%20This%20unified%20framework%20ensures%20both%20effective%20instance-level%0Aalignment%20and%20preservation%20of%20geometric%20relationships%20between%20samples%2C%20leading%0Ato%20a%20more%20comprehensive%20distillation%20process.%20Our%20unified%20framework%20outperforms%0Astate-of-the-art%20distillation%20methods%20across%20multiple%20benchmark%20face%0Arecognition%20datasets%2C%20as%20demonstrated%20by%20extensive%20experimental%20evaluations.%0AInterestingly%2C%20when%20using%20strong%20teacher%20networks%20compared%20to%20the%20student%2C%20our%0Aunified%20KD%20enables%20the%20student%20to%20even%20surpass%20the%20teacher%27s%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Knowledge%2520Distillation%2520Framework%253A%2520Fine-Grained%2520Alignment%2520and%250A%2520%2520Geometric%2520Relationship%2520Preservation%2520for%2520Deep%2520Face%2520Recognition%26entry.906535625%3DDurgesh%2520Mishra%2520and%2520Rishabh%2520Uikey%26entry.1292438233%3D%2520%2520Knowledge%2520Distillation%2520is%2520crucial%2520for%2520optimizing%2520face%2520recognition%2520models%2520for%250Adeployment%2520in%2520computationally%2520limited%2520settings%252C%2520such%2520as%2520edge%2520devices.%250ATraditional%2520KD%2520methods%252C%2520such%2520as%2520Raw%2520L2%2520Feature%2520Distillation%2520or%2520Feature%250AConsistency%2520loss%252C%2520often%2520fail%2520to%2520capture%2520both%2520fine-grained%2520instance-level%250Adetails%2520and%2520complex%2520relational%2520structures%252C%2520leading%2520to%2520suboptimal%2520performance.%250AWe%2520propose%2520a%2520unified%2520approach%2520that%2520integrates%2520two%2520novel%2520loss%2520functions%252C%250AInstance-Level%2520Embedding%2520Distillation%2520and%2520Relation-Based%2520Pairwise%2520Similarity%250ADistillation.%2520Instance-Level%2520Embedding%2520Distillation%2520focuses%2520on%2520aligning%250Aindividual%2520feature%2520embeddings%2520by%2520leveraging%2520a%2520dynamic%2520hard%2520mining%2520strategy%252C%250Athereby%2520enhancing%2520learning%2520from%2520challenging%2520examples.%2520Relation-Based%2520Pairwise%250ASimilarity%2520Distillation%2520captures%2520relational%2520information%2520through%2520pairwise%250Asimilarity%2520relationships%252C%2520employing%2520a%2520memory%2520bank%2520mechanism%2520and%2520a%2520sample%2520mining%250Astrategy.%2520This%2520unified%2520framework%2520ensures%2520both%2520effective%2520instance-level%250Aalignment%2520and%2520preservation%2520of%2520geometric%2520relationships%2520between%2520samples%252C%2520leading%250Ato%2520a%2520more%2520comprehensive%2520distillation%2520process.%2520Our%2520unified%2520framework%2520outperforms%250Astate-of-the-art%2520distillation%2520methods%2520across%2520multiple%2520benchmark%2520face%250Arecognition%2520datasets%252C%2520as%2520demonstrated%2520by%2520extensive%2520experimental%2520evaluations.%250AInterestingly%252C%2520when%2520using%2520strong%2520teacher%2520networks%2520compared%2520to%2520the%2520student%252C%2520our%250Aunified%2520KD%2520enables%2520the%2520student%2520to%2520even%2520surpass%2520the%2520teacher%2527s%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Knowledge%20Distillation%20Framework%3A%20Fine-Grained%20Alignment%20and%0A%20%20Geometric%20Relationship%20Preservation%20for%20Deep%20Face%20Recognition&entry.906535625=Durgesh%20Mishra%20and%20Rishabh%20Uikey&entry.1292438233=%20%20Knowledge%20Distillation%20is%20crucial%20for%20optimizing%20face%20recognition%20models%20for%0Adeployment%20in%20computationally%20limited%20settings%2C%20such%20as%20edge%20devices.%0ATraditional%20KD%20methods%2C%20such%20as%20Raw%20L2%20Feature%20Distillation%20or%20Feature%0AConsistency%20loss%2C%20often%20fail%20to%20capture%20both%20fine-grained%20instance-level%0Adetails%20and%20complex%20relational%20structures%2C%20leading%20to%20suboptimal%20performance.%0AWe%20propose%20a%20unified%20approach%20that%20integrates%20two%20novel%20loss%20functions%2C%0AInstance-Level%20Embedding%20Distillation%20and%20Relation-Based%20Pairwise%20Similarity%0ADistillation.%20Instance-Level%20Embedding%20Distillation%20focuses%20on%20aligning%0Aindividual%20feature%20embeddings%20by%20leveraging%20a%20dynamic%20hard%20mining%20strategy%2C%0Athereby%20enhancing%20learning%20from%20challenging%20examples.%20Relation-Based%20Pairwise%0ASimilarity%20Distillation%20captures%20relational%20information%20through%20pairwise%0Asimilarity%20relationships%2C%20employing%20a%20memory%20bank%20mechanism%20and%20a%20sample%20mining%0Astrategy.%20This%20unified%20framework%20ensures%20both%20effective%20instance-level%0Aalignment%20and%20preservation%20of%20geometric%20relationships%20between%20samples%2C%20leading%0Ato%20a%20more%20comprehensive%20distillation%20process.%20Our%20unified%20framework%20outperforms%0Astate-of-the-art%20distillation%20methods%20across%20multiple%20benchmark%20face%0Arecognition%20datasets%2C%20as%20demonstrated%20by%20extensive%20experimental%20evaluations.%0AInterestingly%2C%20when%20using%20strong%20teacher%20networks%20compared%20to%20the%20student%2C%20our%0Aunified%20KD%20enables%20the%20student%20to%20even%20surpass%20the%20teacher%27s%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11376v1&entry.124074799=Read"},
{"title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical\n  Planners with Large Language Models", "author": "Wenkai Yu and Jianhang Tang and Yang Zhang and Shanjiang Tang and Kebing Jin and Hankz Hankui Zhuo", "abstract": "  Addressing large-scale planning problems has become one of the central\nchallenges in the planning community, deriving from the state-space explosion\ncaused by growing objects and actions. Recently, researchers have explored the\neffectiveness of leveraging Large Language Models (LLMs) to generate helpful\nactions and states to prune the search space. However, prior works have largely\noverlooked integrating LLMs with domain-specific knowledge to ensure valid\nplans. In this paper, we propose a novel LLM-assisted planner integrated with\nproblem decomposition, which first decomposes large planning problems into\nmultiple simpler sub-tasks. Then we explore two novel paradigms to utilize\nLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where\nLLM4Inspire provides heuristic guidance according to general knowledge and\nLLM4Predict employs domain-specific knowledge to infer intermediate conditions.\nWe empirically validate the effectiveness of our planner across multiple\ndomains, demonstrating the ability of search space partition when solving\nlarge-scale planning problems. The experimental results show that LLMs\neffectively locate feasible solutions when pruning the search space, where\ninfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds\nparticular promise compared with LLM4Inspire, which offers general knowledge\nwithin LLMs.\n", "link": "http://arxiv.org/abs/2508.11524v1", "date": "2025-08-15", "relevancy": 2.1631, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5418}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5418}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inspire%20or%20Predict%3F%20Exploring%20New%20Paradigms%20in%20Assisting%20Classical%0A%20%20Planners%20with%20Large%20Language%20Models&body=Title%3A%20Inspire%20or%20Predict%3F%20Exploring%20New%20Paradigms%20in%20Assisting%20Classical%0A%20%20Planners%20with%20Large%20Language%20Models%0AAuthor%3A%20Wenkai%20Yu%20and%20Jianhang%20Tang%20and%20Yang%20Zhang%20and%20Shanjiang%20Tang%20and%20Kebing%20Jin%20and%20Hankz%20Hankui%20Zhuo%0AAbstract%3A%20%20%20Addressing%20large-scale%20planning%20problems%20has%20become%20one%20of%20the%20central%0Achallenges%20in%20the%20planning%20community%2C%20deriving%20from%20the%20state-space%20explosion%0Acaused%20by%20growing%20objects%20and%20actions.%20Recently%2C%20researchers%20have%20explored%20the%0Aeffectiveness%20of%20leveraging%20Large%20Language%20Models%20%28LLMs%29%20to%20generate%20helpful%0Aactions%20and%20states%20to%20prune%20the%20search%20space.%20However%2C%20prior%20works%20have%20largely%0Aoverlooked%20integrating%20LLMs%20with%20domain-specific%20knowledge%20to%20ensure%20valid%0Aplans.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20LLM-assisted%20planner%20integrated%20with%0Aproblem%20decomposition%2C%20which%20first%20decomposes%20large%20planning%20problems%20into%0Amultiple%20simpler%20sub-tasks.%20Then%20we%20explore%20two%20novel%20paradigms%20to%20utilize%0ALLMs%2C%20i.e.%2C%20LLM4Inspire%20and%20LLM4Predict%2C%20to%20assist%20problem%20decomposition%2C%20where%0ALLM4Inspire%20provides%20heuristic%20guidance%20according%20to%20general%20knowledge%20and%0ALLM4Predict%20employs%20domain-specific%20knowledge%20to%20infer%20intermediate%20conditions.%0AWe%20empirically%20validate%20the%20effectiveness%20of%20our%20planner%20across%20multiple%0Adomains%2C%20demonstrating%20the%20ability%20of%20search%20space%20partition%20when%20solving%0Alarge-scale%20planning%20problems.%20The%20experimental%20results%20show%20that%20LLMs%0Aeffectively%20locate%20feasible%20solutions%20when%20pruning%20the%20search%20space%2C%20where%0Ainfusing%20domain-specific%20knowledge%20into%20LLMs%2C%20i.e.%2C%20LLM4Predict%2C%20holds%0Aparticular%20promise%20compared%20with%20LLM4Inspire%2C%20which%20offers%20general%20knowledge%0Awithin%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInspire%2520or%2520Predict%253F%2520Exploring%2520New%2520Paradigms%2520in%2520Assisting%2520Classical%250A%2520%2520Planners%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DWenkai%2520Yu%2520and%2520Jianhang%2520Tang%2520and%2520Yang%2520Zhang%2520and%2520Shanjiang%2520Tang%2520and%2520Kebing%2520Jin%2520and%2520Hankz%2520Hankui%2520Zhuo%26entry.1292438233%3D%2520%2520Addressing%2520large-scale%2520planning%2520problems%2520has%2520become%2520one%2520of%2520the%2520central%250Achallenges%2520in%2520the%2520planning%2520community%252C%2520deriving%2520from%2520the%2520state-space%2520explosion%250Acaused%2520by%2520growing%2520objects%2520and%2520actions.%2520Recently%252C%2520researchers%2520have%2520explored%2520the%250Aeffectiveness%2520of%2520leveraging%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520generate%2520helpful%250Aactions%2520and%2520states%2520to%2520prune%2520the%2520search%2520space.%2520However%252C%2520prior%2520works%2520have%2520largely%250Aoverlooked%2520integrating%2520LLMs%2520with%2520domain-specific%2520knowledge%2520to%2520ensure%2520valid%250Aplans.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520LLM-assisted%2520planner%2520integrated%2520with%250Aproblem%2520decomposition%252C%2520which%2520first%2520decomposes%2520large%2520planning%2520problems%2520into%250Amultiple%2520simpler%2520sub-tasks.%2520Then%2520we%2520explore%2520two%2520novel%2520paradigms%2520to%2520utilize%250ALLMs%252C%2520i.e.%252C%2520LLM4Inspire%2520and%2520LLM4Predict%252C%2520to%2520assist%2520problem%2520decomposition%252C%2520where%250ALLM4Inspire%2520provides%2520heuristic%2520guidance%2520according%2520to%2520general%2520knowledge%2520and%250ALLM4Predict%2520employs%2520domain-specific%2520knowledge%2520to%2520infer%2520intermediate%2520conditions.%250AWe%2520empirically%2520validate%2520the%2520effectiveness%2520of%2520our%2520planner%2520across%2520multiple%250Adomains%252C%2520demonstrating%2520the%2520ability%2520of%2520search%2520space%2520partition%2520when%2520solving%250Alarge-scale%2520planning%2520problems.%2520The%2520experimental%2520results%2520show%2520that%2520LLMs%250Aeffectively%2520locate%2520feasible%2520solutions%2520when%2520pruning%2520the%2520search%2520space%252C%2520where%250Ainfusing%2520domain-specific%2520knowledge%2520into%2520LLMs%252C%2520i.e.%252C%2520LLM4Predict%252C%2520holds%250Aparticular%2520promise%2520compared%2520with%2520LLM4Inspire%252C%2520which%2520offers%2520general%2520knowledge%250Awithin%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inspire%20or%20Predict%3F%20Exploring%20New%20Paradigms%20in%20Assisting%20Classical%0A%20%20Planners%20with%20Large%20Language%20Models&entry.906535625=Wenkai%20Yu%20and%20Jianhang%20Tang%20and%20Yang%20Zhang%20and%20Shanjiang%20Tang%20and%20Kebing%20Jin%20and%20Hankz%20Hankui%20Zhuo&entry.1292438233=%20%20Addressing%20large-scale%20planning%20problems%20has%20become%20one%20of%20the%20central%0Achallenges%20in%20the%20planning%20community%2C%20deriving%20from%20the%20state-space%20explosion%0Acaused%20by%20growing%20objects%20and%20actions.%20Recently%2C%20researchers%20have%20explored%20the%0Aeffectiveness%20of%20leveraging%20Large%20Language%20Models%20%28LLMs%29%20to%20generate%20helpful%0Aactions%20and%20states%20to%20prune%20the%20search%20space.%20However%2C%20prior%20works%20have%20largely%0Aoverlooked%20integrating%20LLMs%20with%20domain-specific%20knowledge%20to%20ensure%20valid%0Aplans.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20LLM-assisted%20planner%20integrated%20with%0Aproblem%20decomposition%2C%20which%20first%20decomposes%20large%20planning%20problems%20into%0Amultiple%20simpler%20sub-tasks.%20Then%20we%20explore%20two%20novel%20paradigms%20to%20utilize%0ALLMs%2C%20i.e.%2C%20LLM4Inspire%20and%20LLM4Predict%2C%20to%20assist%20problem%20decomposition%2C%20where%0ALLM4Inspire%20provides%20heuristic%20guidance%20according%20to%20general%20knowledge%20and%0ALLM4Predict%20employs%20domain-specific%20knowledge%20to%20infer%20intermediate%20conditions.%0AWe%20empirically%20validate%20the%20effectiveness%20of%20our%20planner%20across%20multiple%0Adomains%2C%20demonstrating%20the%20ability%20of%20search%20space%20partition%20when%20solving%0Alarge-scale%20planning%20problems.%20The%20experimental%20results%20show%20that%20LLMs%0Aeffectively%20locate%20feasible%20solutions%20when%20pruning%20the%20search%20space%2C%20where%0Ainfusing%20domain-specific%20knowledge%20into%20LLMs%2C%20i.e.%2C%20LLM4Predict%2C%20holds%0Aparticular%20promise%20compared%20with%20LLM4Inspire%2C%20which%20offers%20general%20knowledge%0Awithin%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11524v1&entry.124074799=Read"},
{"title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for\n  Robotic Vision Tasks", "author": "Jakub \u0141ucki and Jonathan Becktor and Georgios Georgakis and Robert Royce and Shehryar Khattak", "abstract": "  Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.\n", "link": "http://arxiv.org/abs/2508.11584v1", "date": "2025-08-15", "relevancy": 2.151, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5712}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Perception%20Engine%3A%20Fast%20and%20Flexible%20Multi-Head%20Inference%20for%0A%20%20Robotic%20Vision%20Tasks&body=Title%3A%20Visual%20Perception%20Engine%3A%20Fast%20and%20Flexible%20Multi-Head%20Inference%20for%0A%20%20Robotic%20Vision%20Tasks%0AAuthor%3A%20Jakub%20%C5%81ucki%20and%20Jonathan%20Becktor%20and%20Georgios%20Georgakis%20and%20Robert%20Royce%20and%20Shehryar%20Khattak%0AAbstract%3A%20%20%20Deploying%20multiple%20machine%20learning%20models%20on%20resource-constrained%20robotic%0Aplatforms%20for%20different%20perception%20tasks%20often%20results%20in%20redundant%0Acomputations%2C%20large%20memory%20footprints%2C%20and%20complex%20integration%20challenges.%20In%0Aresponse%2C%20this%20work%20presents%20Visual%20Perception%20Engine%20%28VPEngine%29%2C%20a%20modular%0Aframework%20designed%20to%20enable%20efficient%20GPU%20usage%20for%20visual%20multitasking%20while%0Amaintaining%20extensibility%20and%20developer%20accessibility.%20Our%20framework%0Aarchitecture%20leverages%20a%20shared%20foundation%20model%20backbone%20that%20extracts%20image%0Arepresentations%2C%20which%20are%20efficiently%20shared%2C%20without%20any%20unnecessary%20GPU-CPU%0Amemory%20transfers%2C%20across%20multiple%20specialized%20task-specific%20model%20heads%20running%0Ain%20parallel.%20This%20design%20eliminates%20the%20computational%20redundancy%20inherent%20in%0Afeature%20extraction%20component%20when%20deploying%20traditional%20sequential%20models%20while%0Aenabling%20dynamic%20task%20prioritization%20based%20on%20application%20demands.%20We%0Ademonstrate%20our%20framework%27s%20capabilities%20through%20an%20example%20implementation%0Ausing%20DINOv2%20as%20the%20foundation%20model%20with%20multiple%20task%20%28depth%2C%20object%0Adetection%20and%20semantic%20segmentation%29%20heads%2C%20achieving%20up%20to%203x%20speedup%20compared%0Ato%20sequential%20execution.%20Building%20on%20CUDA%20Multi-Process%20Service%20%28MPS%29%2C%20VPEngine%0Aoffers%20efficient%20GPU%20utilization%20and%20maintains%20a%20constant%20memory%20footprint%0Awhile%20allowing%20per-task%20inference%20frequencies%20to%20be%20adjusted%20dynamically%20during%0Aruntime.%20The%20framework%20is%20written%20in%20Python%20and%20is%20open%20source%20with%20ROS2%20C%2B%2B%0A%28Humble%29%20bindings%20for%20ease%20of%20use%20by%20the%20robotics%20community%20across%20diverse%0Arobotic%20platforms.%20Our%20example%20implementation%20demonstrates%20end-to-end%20real-time%0Aperformance%20at%20%24%5Cgeq%2450%20Hz%20on%20NVIDIA%20Jetson%20Orin%20AGX%20for%20TensorRT%20optimized%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Perception%2520Engine%253A%2520Fast%2520and%2520Flexible%2520Multi-Head%2520Inference%2520for%250A%2520%2520Robotic%2520Vision%2520Tasks%26entry.906535625%3DJakub%2520%25C5%2581ucki%2520and%2520Jonathan%2520Becktor%2520and%2520Georgios%2520Georgakis%2520and%2520Robert%2520Royce%2520and%2520Shehryar%2520Khattak%26entry.1292438233%3D%2520%2520Deploying%2520multiple%2520machine%2520learning%2520models%2520on%2520resource-constrained%2520robotic%250Aplatforms%2520for%2520different%2520perception%2520tasks%2520often%2520results%2520in%2520redundant%250Acomputations%252C%2520large%2520memory%2520footprints%252C%2520and%2520complex%2520integration%2520challenges.%2520In%250Aresponse%252C%2520this%2520work%2520presents%2520Visual%2520Perception%2520Engine%2520%2528VPEngine%2529%252C%2520a%2520modular%250Aframework%2520designed%2520to%2520enable%2520efficient%2520GPU%2520usage%2520for%2520visual%2520multitasking%2520while%250Amaintaining%2520extensibility%2520and%2520developer%2520accessibility.%2520Our%2520framework%250Aarchitecture%2520leverages%2520a%2520shared%2520foundation%2520model%2520backbone%2520that%2520extracts%2520image%250Arepresentations%252C%2520which%2520are%2520efficiently%2520shared%252C%2520without%2520any%2520unnecessary%2520GPU-CPU%250Amemory%2520transfers%252C%2520across%2520multiple%2520specialized%2520task-specific%2520model%2520heads%2520running%250Ain%2520parallel.%2520This%2520design%2520eliminates%2520the%2520computational%2520redundancy%2520inherent%2520in%250Afeature%2520extraction%2520component%2520when%2520deploying%2520traditional%2520sequential%2520models%2520while%250Aenabling%2520dynamic%2520task%2520prioritization%2520based%2520on%2520application%2520demands.%2520We%250Ademonstrate%2520our%2520framework%2527s%2520capabilities%2520through%2520an%2520example%2520implementation%250Ausing%2520DINOv2%2520as%2520the%2520foundation%2520model%2520with%2520multiple%2520task%2520%2528depth%252C%2520object%250Adetection%2520and%2520semantic%2520segmentation%2529%2520heads%252C%2520achieving%2520up%2520to%25203x%2520speedup%2520compared%250Ato%2520sequential%2520execution.%2520Building%2520on%2520CUDA%2520Multi-Process%2520Service%2520%2528MPS%2529%252C%2520VPEngine%250Aoffers%2520efficient%2520GPU%2520utilization%2520and%2520maintains%2520a%2520constant%2520memory%2520footprint%250Awhile%2520allowing%2520per-task%2520inference%2520frequencies%2520to%2520be%2520adjusted%2520dynamically%2520during%250Aruntime.%2520The%2520framework%2520is%2520written%2520in%2520Python%2520and%2520is%2520open%2520source%2520with%2520ROS2%2520C%252B%252B%250A%2528Humble%2529%2520bindings%2520for%2520ease%2520of%2520use%2520by%2520the%2520robotics%2520community%2520across%2520diverse%250Arobotic%2520platforms.%2520Our%2520example%2520implementation%2520demonstrates%2520end-to-end%2520real-time%250Aperformance%2520at%2520%2524%255Cgeq%252450%2520Hz%2520on%2520NVIDIA%2520Jetson%2520Orin%2520AGX%2520for%2520TensorRT%2520optimized%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Perception%20Engine%3A%20Fast%20and%20Flexible%20Multi-Head%20Inference%20for%0A%20%20Robotic%20Vision%20Tasks&entry.906535625=Jakub%20%C5%81ucki%20and%20Jonathan%20Becktor%20and%20Georgios%20Georgakis%20and%20Robert%20Royce%20and%20Shehryar%20Khattak&entry.1292438233=%20%20Deploying%20multiple%20machine%20learning%20models%20on%20resource-constrained%20robotic%0Aplatforms%20for%20different%20perception%20tasks%20often%20results%20in%20redundant%0Acomputations%2C%20large%20memory%20footprints%2C%20and%20complex%20integration%20challenges.%20In%0Aresponse%2C%20this%20work%20presents%20Visual%20Perception%20Engine%20%28VPEngine%29%2C%20a%20modular%0Aframework%20designed%20to%20enable%20efficient%20GPU%20usage%20for%20visual%20multitasking%20while%0Amaintaining%20extensibility%20and%20developer%20accessibility.%20Our%20framework%0Aarchitecture%20leverages%20a%20shared%20foundation%20model%20backbone%20that%20extracts%20image%0Arepresentations%2C%20which%20are%20efficiently%20shared%2C%20without%20any%20unnecessary%20GPU-CPU%0Amemory%20transfers%2C%20across%20multiple%20specialized%20task-specific%20model%20heads%20running%0Ain%20parallel.%20This%20design%20eliminates%20the%20computational%20redundancy%20inherent%20in%0Afeature%20extraction%20component%20when%20deploying%20traditional%20sequential%20models%20while%0Aenabling%20dynamic%20task%20prioritization%20based%20on%20application%20demands.%20We%0Ademonstrate%20our%20framework%27s%20capabilities%20through%20an%20example%20implementation%0Ausing%20DINOv2%20as%20the%20foundation%20model%20with%20multiple%20task%20%28depth%2C%20object%0Adetection%20and%20semantic%20segmentation%29%20heads%2C%20achieving%20up%20to%203x%20speedup%20compared%0Ato%20sequential%20execution.%20Building%20on%20CUDA%20Multi-Process%20Service%20%28MPS%29%2C%20VPEngine%0Aoffers%20efficient%20GPU%20utilization%20and%20maintains%20a%20constant%20memory%20footprint%0Awhile%20allowing%20per-task%20inference%20frequencies%20to%20be%20adjusted%20dynamically%20during%0Aruntime.%20The%20framework%20is%20written%20in%20Python%20and%20is%20open%20source%20with%20ROS2%20C%2B%2B%0A%28Humble%29%20bindings%20for%20ease%20of%20use%20by%20the%20robotics%20community%20across%20diverse%0Arobotic%20platforms.%20Our%20example%20implementation%20demonstrates%20end-to-end%20real-time%0Aperformance%20at%20%24%5Cgeq%2450%20Hz%20on%20NVIDIA%20Jetson%20Orin%20AGX%20for%20TensorRT%20optimized%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11584v1&entry.124074799=Read"},
{"title": "KDPE: A Kernel Density Estimation Strategy for Diffusion Policy\n  Trajectory Selection", "author": "Andrea Rosasco and Federico Ceola and Giulia Pasquale and Lorenzo Natale", "abstract": "  Learning robot policies that capture multimodality in the training data has\nbeen a long-standing open challenge for behavior cloning. Recent approaches\ntackle the problem by modeling the conditional action distribution with\ngenerative models. One of these approaches is Diffusion Policy, which relies on\na diffusion model to denoise random points into robot action trajectories.\nWhile achieving state-of-the-art performance, it has two main drawbacks that\nmay lead the robot out of the data distribution during policy execution. First,\nthe stochasticity of the denoising process can highly impact on the quality of\ngenerated trajectory of actions. Second, being a supervised learning approach,\nit can learn data outliers from the dataset used for training. Recent work\nfocuses on mitigating these limitations by combining Diffusion Policy either\nwith large-scale training or with classical behavior cloning algorithms.\nInstead, we propose KDPE, a Kernel Density Estimation-based strategy that\nfilters out potentially harmful trajectories output of Diffusion Policy while\nkeeping a low test-time computational overhead. For Kernel Density Estimation,\nwe propose a manifold-aware kernel to model a probability density function for\nactions composed of end-effector Cartesian position, orientation, and gripper\nstate. KDPE overall achieves better performance than Diffusion Policy on\nsimulated single-arm tasks and real robot experiments.\n  Additional material and code are available on our project page at\nhttps://hsp-iit.github.io/KDPE/.\n", "link": "http://arxiv.org/abs/2508.10511v2", "date": "2025-08-15", "relevancy": 2.141, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5681}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5474}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KDPE%3A%20A%20Kernel%20Density%20Estimation%20Strategy%20for%20Diffusion%20Policy%0A%20%20Trajectory%20Selection&body=Title%3A%20KDPE%3A%20A%20Kernel%20Density%20Estimation%20Strategy%20for%20Diffusion%20Policy%0A%20%20Trajectory%20Selection%0AAuthor%3A%20Andrea%20Rosasco%20and%20Federico%20Ceola%20and%20Giulia%20Pasquale%20and%20Lorenzo%20Natale%0AAbstract%3A%20%20%20Learning%20robot%20policies%20that%20capture%20multimodality%20in%20the%20training%20data%20has%0Abeen%20a%20long-standing%20open%20challenge%20for%20behavior%20cloning.%20Recent%20approaches%0Atackle%20the%20problem%20by%20modeling%20the%20conditional%20action%20distribution%20with%0Agenerative%20models.%20One%20of%20these%20approaches%20is%20Diffusion%20Policy%2C%20which%20relies%20on%0Aa%20diffusion%20model%20to%20denoise%20random%20points%20into%20robot%20action%20trajectories.%0AWhile%20achieving%20state-of-the-art%20performance%2C%20it%20has%20two%20main%20drawbacks%20that%0Amay%20lead%20the%20robot%20out%20of%20the%20data%20distribution%20during%20policy%20execution.%20First%2C%0Athe%20stochasticity%20of%20the%20denoising%20process%20can%20highly%20impact%20on%20the%20quality%20of%0Agenerated%20trajectory%20of%20actions.%20Second%2C%20being%20a%20supervised%20learning%20approach%2C%0Ait%20can%20learn%20data%20outliers%20from%20the%20dataset%20used%20for%20training.%20Recent%20work%0Afocuses%20on%20mitigating%20these%20limitations%20by%20combining%20Diffusion%20Policy%20either%0Awith%20large-scale%20training%20or%20with%20classical%20behavior%20cloning%20algorithms.%0AInstead%2C%20we%20propose%20KDPE%2C%20a%20Kernel%20Density%20Estimation-based%20strategy%20that%0Afilters%20out%20potentially%20harmful%20trajectories%20output%20of%20Diffusion%20Policy%20while%0Akeeping%20a%20low%20test-time%20computational%20overhead.%20For%20Kernel%20Density%20Estimation%2C%0Awe%20propose%20a%20manifold-aware%20kernel%20to%20model%20a%20probability%20density%20function%20for%0Aactions%20composed%20of%20end-effector%20Cartesian%20position%2C%20orientation%2C%20and%20gripper%0Astate.%20KDPE%20overall%20achieves%20better%20performance%20than%20Diffusion%20Policy%20on%0Asimulated%20single-arm%20tasks%20and%20real%20robot%20experiments.%0A%20%20Additional%20material%20and%20code%20are%20available%20on%20our%20project%20page%20at%0Ahttps%3A//hsp-iit.github.io/KDPE/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKDPE%253A%2520A%2520Kernel%2520Density%2520Estimation%2520Strategy%2520for%2520Diffusion%2520Policy%250A%2520%2520Trajectory%2520Selection%26entry.906535625%3DAndrea%2520Rosasco%2520and%2520Federico%2520Ceola%2520and%2520Giulia%2520Pasquale%2520and%2520Lorenzo%2520Natale%26entry.1292438233%3D%2520%2520Learning%2520robot%2520policies%2520that%2520capture%2520multimodality%2520in%2520the%2520training%2520data%2520has%250Abeen%2520a%2520long-standing%2520open%2520challenge%2520for%2520behavior%2520cloning.%2520Recent%2520approaches%250Atackle%2520the%2520problem%2520by%2520modeling%2520the%2520conditional%2520action%2520distribution%2520with%250Agenerative%2520models.%2520One%2520of%2520these%2520approaches%2520is%2520Diffusion%2520Policy%252C%2520which%2520relies%2520on%250Aa%2520diffusion%2520model%2520to%2520denoise%2520random%2520points%2520into%2520robot%2520action%2520trajectories.%250AWhile%2520achieving%2520state-of-the-art%2520performance%252C%2520it%2520has%2520two%2520main%2520drawbacks%2520that%250Amay%2520lead%2520the%2520robot%2520out%2520of%2520the%2520data%2520distribution%2520during%2520policy%2520execution.%2520First%252C%250Athe%2520stochasticity%2520of%2520the%2520denoising%2520process%2520can%2520highly%2520impact%2520on%2520the%2520quality%2520of%250Agenerated%2520trajectory%2520of%2520actions.%2520Second%252C%2520being%2520a%2520supervised%2520learning%2520approach%252C%250Ait%2520can%2520learn%2520data%2520outliers%2520from%2520the%2520dataset%2520used%2520for%2520training.%2520Recent%2520work%250Afocuses%2520on%2520mitigating%2520these%2520limitations%2520by%2520combining%2520Diffusion%2520Policy%2520either%250Awith%2520large-scale%2520training%2520or%2520with%2520classical%2520behavior%2520cloning%2520algorithms.%250AInstead%252C%2520we%2520propose%2520KDPE%252C%2520a%2520Kernel%2520Density%2520Estimation-based%2520strategy%2520that%250Afilters%2520out%2520potentially%2520harmful%2520trajectories%2520output%2520of%2520Diffusion%2520Policy%2520while%250Akeeping%2520a%2520low%2520test-time%2520computational%2520overhead.%2520For%2520Kernel%2520Density%2520Estimation%252C%250Awe%2520propose%2520a%2520manifold-aware%2520kernel%2520to%2520model%2520a%2520probability%2520density%2520function%2520for%250Aactions%2520composed%2520of%2520end-effector%2520Cartesian%2520position%252C%2520orientation%252C%2520and%2520gripper%250Astate.%2520KDPE%2520overall%2520achieves%2520better%2520performance%2520than%2520Diffusion%2520Policy%2520on%250Asimulated%2520single-arm%2520tasks%2520and%2520real%2520robot%2520experiments.%250A%2520%2520Additional%2520material%2520and%2520code%2520are%2520available%2520on%2520our%2520project%2520page%2520at%250Ahttps%253A//hsp-iit.github.io/KDPE/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KDPE%3A%20A%20Kernel%20Density%20Estimation%20Strategy%20for%20Diffusion%20Policy%0A%20%20Trajectory%20Selection&entry.906535625=Andrea%20Rosasco%20and%20Federico%20Ceola%20and%20Giulia%20Pasquale%20and%20Lorenzo%20Natale&entry.1292438233=%20%20Learning%20robot%20policies%20that%20capture%20multimodality%20in%20the%20training%20data%20has%0Abeen%20a%20long-standing%20open%20challenge%20for%20behavior%20cloning.%20Recent%20approaches%0Atackle%20the%20problem%20by%20modeling%20the%20conditional%20action%20distribution%20with%0Agenerative%20models.%20One%20of%20these%20approaches%20is%20Diffusion%20Policy%2C%20which%20relies%20on%0Aa%20diffusion%20model%20to%20denoise%20random%20points%20into%20robot%20action%20trajectories.%0AWhile%20achieving%20state-of-the-art%20performance%2C%20it%20has%20two%20main%20drawbacks%20that%0Amay%20lead%20the%20robot%20out%20of%20the%20data%20distribution%20during%20policy%20execution.%20First%2C%0Athe%20stochasticity%20of%20the%20denoising%20process%20can%20highly%20impact%20on%20the%20quality%20of%0Agenerated%20trajectory%20of%20actions.%20Second%2C%20being%20a%20supervised%20learning%20approach%2C%0Ait%20can%20learn%20data%20outliers%20from%20the%20dataset%20used%20for%20training.%20Recent%20work%0Afocuses%20on%20mitigating%20these%20limitations%20by%20combining%20Diffusion%20Policy%20either%0Awith%20large-scale%20training%20or%20with%20classical%20behavior%20cloning%20algorithms.%0AInstead%2C%20we%20propose%20KDPE%2C%20a%20Kernel%20Density%20Estimation-based%20strategy%20that%0Afilters%20out%20potentially%20harmful%20trajectories%20output%20of%20Diffusion%20Policy%20while%0Akeeping%20a%20low%20test-time%20computational%20overhead.%20For%20Kernel%20Density%20Estimation%2C%0Awe%20propose%20a%20manifold-aware%20kernel%20to%20model%20a%20probability%20density%20function%20for%0Aactions%20composed%20of%20end-effector%20Cartesian%20position%2C%20orientation%2C%20and%20gripper%0Astate.%20KDPE%20overall%20achieves%20better%20performance%20than%20Diffusion%20Policy%20on%0Asimulated%20single-arm%20tasks%20and%20real%20robot%20experiments.%0A%20%20Additional%20material%20and%20code%20are%20available%20on%20our%20project%20page%20at%0Ahttps%3A//hsp-iit.github.io/KDPE/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10511v2&entry.124074799=Read"},
{"title": "MultiPark: Multimodal Parking Transformer with Next-Segment Prediction", "author": "Han Zheng and Zikang Zhou and Guli Zhang and Zhepei Wang and Kaixuan Wang and Peiliang Li and Shaojie Shen and Ming Yang and Tong Qin", "abstract": "  Parking accurately and safely in highly constrained spaces remains a critical\nchallenge. Unlike structured driving environments, parking requires executing\ncomplex maneuvers such as frequent gear shifts and steering saturation. Recent\nattempts to employ imitation learning (IL) for parking have achieved promising\nresults. However, existing works ignore the multimodal nature of parking\nbehavior in lane-free open space, failing to derive multiple plausible\nsolutions under the same situation. Notably, IL-based methods encompass\ninherent causal confusion, so enabling a neural network to generalize across\ndiverse parking scenarios is particularly difficult. To address these\nchallenges, we propose MultiPark, an autoregressive transformer for multimodal\nparking. To handle paths filled with abrupt turning points, we introduce a\ndata-efficient next-segment prediction paradigm, enabling spatial\ngeneralization and temporal extrapolation. Furthermore, we design learnable\nparking queries factorized into gear, longitudinal, and lateral components,\nparallelly decoding diverse parking behaviors. To mitigate causal confusion in\nIL, our method employs target-centric pose and ego-centric collision as\noutcome-oriented loss across all modalities beyond pure imitation loss.\nEvaluations on real-world datasets demonstrate that MultiPark achieves\nstate-of-the-art performance across various scenarios. We deploy MultiPark on a\nproduction vehicle, further confirming our approach's robustness in real-world\nparking environments.\n", "link": "http://arxiv.org/abs/2508.11537v1", "date": "2025-08-15", "relevancy": 2.1306, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5915}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5266}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiPark%3A%20Multimodal%20Parking%20Transformer%20with%20Next-Segment%20Prediction&body=Title%3A%20MultiPark%3A%20Multimodal%20Parking%20Transformer%20with%20Next-Segment%20Prediction%0AAuthor%3A%20Han%20Zheng%20and%20Zikang%20Zhou%20and%20Guli%20Zhang%20and%20Zhepei%20Wang%20and%20Kaixuan%20Wang%20and%20Peiliang%20Li%20and%20Shaojie%20Shen%20and%20Ming%20Yang%20and%20Tong%20Qin%0AAbstract%3A%20%20%20Parking%20accurately%20and%20safely%20in%20highly%20constrained%20spaces%20remains%20a%20critical%0Achallenge.%20Unlike%20structured%20driving%20environments%2C%20parking%20requires%20executing%0Acomplex%20maneuvers%20such%20as%20frequent%20gear%20shifts%20and%20steering%20saturation.%20Recent%0Aattempts%20to%20employ%20imitation%20learning%20%28IL%29%20for%20parking%20have%20achieved%20promising%0Aresults.%20However%2C%20existing%20works%20ignore%20the%20multimodal%20nature%20of%20parking%0Abehavior%20in%20lane-free%20open%20space%2C%20failing%20to%20derive%20multiple%20plausible%0Asolutions%20under%20the%20same%20situation.%20Notably%2C%20IL-based%20methods%20encompass%0Ainherent%20causal%20confusion%2C%20so%20enabling%20a%20neural%20network%20to%20generalize%20across%0Adiverse%20parking%20scenarios%20is%20particularly%20difficult.%20To%20address%20these%0Achallenges%2C%20we%20propose%20MultiPark%2C%20an%20autoregressive%20transformer%20for%20multimodal%0Aparking.%20To%20handle%20paths%20filled%20with%20abrupt%20turning%20points%2C%20we%20introduce%20a%0Adata-efficient%20next-segment%20prediction%20paradigm%2C%20enabling%20spatial%0Ageneralization%20and%20temporal%20extrapolation.%20Furthermore%2C%20we%20design%20learnable%0Aparking%20queries%20factorized%20into%20gear%2C%20longitudinal%2C%20and%20lateral%20components%2C%0Aparallelly%20decoding%20diverse%20parking%20behaviors.%20To%20mitigate%20causal%20confusion%20in%0AIL%2C%20our%20method%20employs%20target-centric%20pose%20and%20ego-centric%20collision%20as%0Aoutcome-oriented%20loss%20across%20all%20modalities%20beyond%20pure%20imitation%20loss.%0AEvaluations%20on%20real-world%20datasets%20demonstrate%20that%20MultiPark%20achieves%0Astate-of-the-art%20performance%20across%20various%20scenarios.%20We%20deploy%20MultiPark%20on%20a%0Aproduction%20vehicle%2C%20further%20confirming%20our%20approach%27s%20robustness%20in%20real-world%0Aparking%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiPark%253A%2520Multimodal%2520Parking%2520Transformer%2520with%2520Next-Segment%2520Prediction%26entry.906535625%3DHan%2520Zheng%2520and%2520Zikang%2520Zhou%2520and%2520Guli%2520Zhang%2520and%2520Zhepei%2520Wang%2520and%2520Kaixuan%2520Wang%2520and%2520Peiliang%2520Li%2520and%2520Shaojie%2520Shen%2520and%2520Ming%2520Yang%2520and%2520Tong%2520Qin%26entry.1292438233%3D%2520%2520Parking%2520accurately%2520and%2520safely%2520in%2520highly%2520constrained%2520spaces%2520remains%2520a%2520critical%250Achallenge.%2520Unlike%2520structured%2520driving%2520environments%252C%2520parking%2520requires%2520executing%250Acomplex%2520maneuvers%2520such%2520as%2520frequent%2520gear%2520shifts%2520and%2520steering%2520saturation.%2520Recent%250Aattempts%2520to%2520employ%2520imitation%2520learning%2520%2528IL%2529%2520for%2520parking%2520have%2520achieved%2520promising%250Aresults.%2520However%252C%2520existing%2520works%2520ignore%2520the%2520multimodal%2520nature%2520of%2520parking%250Abehavior%2520in%2520lane-free%2520open%2520space%252C%2520failing%2520to%2520derive%2520multiple%2520plausible%250Asolutions%2520under%2520the%2520same%2520situation.%2520Notably%252C%2520IL-based%2520methods%2520encompass%250Ainherent%2520causal%2520confusion%252C%2520so%2520enabling%2520a%2520neural%2520network%2520to%2520generalize%2520across%250Adiverse%2520parking%2520scenarios%2520is%2520particularly%2520difficult.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520MultiPark%252C%2520an%2520autoregressive%2520transformer%2520for%2520multimodal%250Aparking.%2520To%2520handle%2520paths%2520filled%2520with%2520abrupt%2520turning%2520points%252C%2520we%2520introduce%2520a%250Adata-efficient%2520next-segment%2520prediction%2520paradigm%252C%2520enabling%2520spatial%250Ageneralization%2520and%2520temporal%2520extrapolation.%2520Furthermore%252C%2520we%2520design%2520learnable%250Aparking%2520queries%2520factorized%2520into%2520gear%252C%2520longitudinal%252C%2520and%2520lateral%2520components%252C%250Aparallelly%2520decoding%2520diverse%2520parking%2520behaviors.%2520To%2520mitigate%2520causal%2520confusion%2520in%250AIL%252C%2520our%2520method%2520employs%2520target-centric%2520pose%2520and%2520ego-centric%2520collision%2520as%250Aoutcome-oriented%2520loss%2520across%2520all%2520modalities%2520beyond%2520pure%2520imitation%2520loss.%250AEvaluations%2520on%2520real-world%2520datasets%2520demonstrate%2520that%2520MultiPark%2520achieves%250Astate-of-the-art%2520performance%2520across%2520various%2520scenarios.%2520We%2520deploy%2520MultiPark%2520on%2520a%250Aproduction%2520vehicle%252C%2520further%2520confirming%2520our%2520approach%2527s%2520robustness%2520in%2520real-world%250Aparking%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiPark%3A%20Multimodal%20Parking%20Transformer%20with%20Next-Segment%20Prediction&entry.906535625=Han%20Zheng%20and%20Zikang%20Zhou%20and%20Guli%20Zhang%20and%20Zhepei%20Wang%20and%20Kaixuan%20Wang%20and%20Peiliang%20Li%20and%20Shaojie%20Shen%20and%20Ming%20Yang%20and%20Tong%20Qin&entry.1292438233=%20%20Parking%20accurately%20and%20safely%20in%20highly%20constrained%20spaces%20remains%20a%20critical%0Achallenge.%20Unlike%20structured%20driving%20environments%2C%20parking%20requires%20executing%0Acomplex%20maneuvers%20such%20as%20frequent%20gear%20shifts%20and%20steering%20saturation.%20Recent%0Aattempts%20to%20employ%20imitation%20learning%20%28IL%29%20for%20parking%20have%20achieved%20promising%0Aresults.%20However%2C%20existing%20works%20ignore%20the%20multimodal%20nature%20of%20parking%0Abehavior%20in%20lane-free%20open%20space%2C%20failing%20to%20derive%20multiple%20plausible%0Asolutions%20under%20the%20same%20situation.%20Notably%2C%20IL-based%20methods%20encompass%0Ainherent%20causal%20confusion%2C%20so%20enabling%20a%20neural%20network%20to%20generalize%20across%0Adiverse%20parking%20scenarios%20is%20particularly%20difficult.%20To%20address%20these%0Achallenges%2C%20we%20propose%20MultiPark%2C%20an%20autoregressive%20transformer%20for%20multimodal%0Aparking.%20To%20handle%20paths%20filled%20with%20abrupt%20turning%20points%2C%20we%20introduce%20a%0Adata-efficient%20next-segment%20prediction%20paradigm%2C%20enabling%20spatial%0Ageneralization%20and%20temporal%20extrapolation.%20Furthermore%2C%20we%20design%20learnable%0Aparking%20queries%20factorized%20into%20gear%2C%20longitudinal%2C%20and%20lateral%20components%2C%0Aparallelly%20decoding%20diverse%20parking%20behaviors.%20To%20mitigate%20causal%20confusion%20in%0AIL%2C%20our%20method%20employs%20target-centric%20pose%20and%20ego-centric%20collision%20as%0Aoutcome-oriented%20loss%20across%20all%20modalities%20beyond%20pure%20imitation%20loss.%0AEvaluations%20on%20real-world%20datasets%20demonstrate%20that%20MultiPark%20achieves%0Astate-of-the-art%20performance%20across%20various%20scenarios.%20We%20deploy%20MultiPark%20on%20a%0Aproduction%20vehicle%2C%20further%20confirming%20our%20approach%27s%20robustness%20in%20real-world%0Aparking%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11537v1&entry.124074799=Read"},
{"title": "Model Interpretability and Rationale Extraction by Input Mask\n  Optimization", "author": "Marc Brinner and Sina Zarriess", "abstract": "  Concurrent to the rapid progress in the development of neural-network based\nmodels in areas like natural language processing and computer vision, the need\nfor creating explanations for the predictions of these black-box models has\nrisen steadily. We propose a new method to generate extractive explanations for\npredictions made by neural networks, that is based on masking parts of the\ninput which the model does not consider to be indicative of the respective\nclass. The masking is done using gradient-based optimization combined with a\nnew regularization scheme that enforces sufficiency, comprehensiveness and\ncompactness of the generated explanation, three properties that are known to be\ndesirable from the related field of rationale extraction in natural language\nprocessing. In this way, we bridge the gap between model interpretability and\nrationale extraction, thereby proving that the latter of which can be performed\nwithout training a specialized model, only on the basis of a trained\nclassifier. We further apply the same method to image inputs and obtain high\nquality explanations for image classifications, which indicates that the\nconditions proposed for rationale extraction in natural language processing are\nmore broadly applicable to different input types.\n", "link": "http://arxiv.org/abs/2508.11388v1", "date": "2025-08-15", "relevancy": 2.1292, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5437}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Interpretability%20and%20Rationale%20Extraction%20by%20Input%20Mask%0A%20%20Optimization&body=Title%3A%20Model%20Interpretability%20and%20Rationale%20Extraction%20by%20Input%20Mask%0A%20%20Optimization%0AAuthor%3A%20Marc%20Brinner%20and%20Sina%20Zarriess%0AAbstract%3A%20%20%20Concurrent%20to%20the%20rapid%20progress%20in%20the%20development%20of%20neural-network%20based%0Amodels%20in%20areas%20like%20natural%20language%20processing%20and%20computer%20vision%2C%20the%20need%0Afor%20creating%20explanations%20for%20the%20predictions%20of%20these%20black-box%20models%20has%0Arisen%20steadily.%20We%20propose%20a%20new%20method%20to%20generate%20extractive%20explanations%20for%0Apredictions%20made%20by%20neural%20networks%2C%20that%20is%20based%20on%20masking%20parts%20of%20the%0Ainput%20which%20the%20model%20does%20not%20consider%20to%20be%20indicative%20of%20the%20respective%0Aclass.%20The%20masking%20is%20done%20using%20gradient-based%20optimization%20combined%20with%20a%0Anew%20regularization%20scheme%20that%20enforces%20sufficiency%2C%20comprehensiveness%20and%0Acompactness%20of%20the%20generated%20explanation%2C%20three%20properties%20that%20are%20known%20to%20be%0Adesirable%20from%20the%20related%20field%20of%20rationale%20extraction%20in%20natural%20language%0Aprocessing.%20In%20this%20way%2C%20we%20bridge%20the%20gap%20between%20model%20interpretability%20and%0Arationale%20extraction%2C%20thereby%20proving%20that%20the%20latter%20of%20which%20can%20be%20performed%0Awithout%20training%20a%20specialized%20model%2C%20only%20on%20the%20basis%20of%20a%20trained%0Aclassifier.%20We%20further%20apply%20the%20same%20method%20to%20image%20inputs%20and%20obtain%20high%0Aquality%20explanations%20for%20image%20classifications%2C%20which%20indicates%20that%20the%0Aconditions%20proposed%20for%20rationale%20extraction%20in%20natural%20language%20processing%20are%0Amore%20broadly%20applicable%20to%20different%20input%20types.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Interpretability%2520and%2520Rationale%2520Extraction%2520by%2520Input%2520Mask%250A%2520%2520Optimization%26entry.906535625%3DMarc%2520Brinner%2520and%2520Sina%2520Zarriess%26entry.1292438233%3D%2520%2520Concurrent%2520to%2520the%2520rapid%2520progress%2520in%2520the%2520development%2520of%2520neural-network%2520based%250Amodels%2520in%2520areas%2520like%2520natural%2520language%2520processing%2520and%2520computer%2520vision%252C%2520the%2520need%250Afor%2520creating%2520explanations%2520for%2520the%2520predictions%2520of%2520these%2520black-box%2520models%2520has%250Arisen%2520steadily.%2520We%2520propose%2520a%2520new%2520method%2520to%2520generate%2520extractive%2520explanations%2520for%250Apredictions%2520made%2520by%2520neural%2520networks%252C%2520that%2520is%2520based%2520on%2520masking%2520parts%2520of%2520the%250Ainput%2520which%2520the%2520model%2520does%2520not%2520consider%2520to%2520be%2520indicative%2520of%2520the%2520respective%250Aclass.%2520The%2520masking%2520is%2520done%2520using%2520gradient-based%2520optimization%2520combined%2520with%2520a%250Anew%2520regularization%2520scheme%2520that%2520enforces%2520sufficiency%252C%2520comprehensiveness%2520and%250Acompactness%2520of%2520the%2520generated%2520explanation%252C%2520three%2520properties%2520that%2520are%2520known%2520to%2520be%250Adesirable%2520from%2520the%2520related%2520field%2520of%2520rationale%2520extraction%2520in%2520natural%2520language%250Aprocessing.%2520In%2520this%2520way%252C%2520we%2520bridge%2520the%2520gap%2520between%2520model%2520interpretability%2520and%250Arationale%2520extraction%252C%2520thereby%2520proving%2520that%2520the%2520latter%2520of%2520which%2520can%2520be%2520performed%250Awithout%2520training%2520a%2520specialized%2520model%252C%2520only%2520on%2520the%2520basis%2520of%2520a%2520trained%250Aclassifier.%2520We%2520further%2520apply%2520the%2520same%2520method%2520to%2520image%2520inputs%2520and%2520obtain%2520high%250Aquality%2520explanations%2520for%2520image%2520classifications%252C%2520which%2520indicates%2520that%2520the%250Aconditions%2520proposed%2520for%2520rationale%2520extraction%2520in%2520natural%2520language%2520processing%2520are%250Amore%2520broadly%2520applicable%2520to%2520different%2520input%2520types.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Interpretability%20and%20Rationale%20Extraction%20by%20Input%20Mask%0A%20%20Optimization&entry.906535625=Marc%20Brinner%20and%20Sina%20Zarriess&entry.1292438233=%20%20Concurrent%20to%20the%20rapid%20progress%20in%20the%20development%20of%20neural-network%20based%0Amodels%20in%20areas%20like%20natural%20language%20processing%20and%20computer%20vision%2C%20the%20need%0Afor%20creating%20explanations%20for%20the%20predictions%20of%20these%20black-box%20models%20has%0Arisen%20steadily.%20We%20propose%20a%20new%20method%20to%20generate%20extractive%20explanations%20for%0Apredictions%20made%20by%20neural%20networks%2C%20that%20is%20based%20on%20masking%20parts%20of%20the%0Ainput%20which%20the%20model%20does%20not%20consider%20to%20be%20indicative%20of%20the%20respective%0Aclass.%20The%20masking%20is%20done%20using%20gradient-based%20optimization%20combined%20with%20a%0Anew%20regularization%20scheme%20that%20enforces%20sufficiency%2C%20comprehensiveness%20and%0Acompactness%20of%20the%20generated%20explanation%2C%20three%20properties%20that%20are%20known%20to%20be%0Adesirable%20from%20the%20related%20field%20of%20rationale%20extraction%20in%20natural%20language%0Aprocessing.%20In%20this%20way%2C%20we%20bridge%20the%20gap%20between%20model%20interpretability%20and%0Arationale%20extraction%2C%20thereby%20proving%20that%20the%20latter%20of%20which%20can%20be%20performed%0Awithout%20training%20a%20specialized%20model%2C%20only%20on%20the%20basis%20of%20a%20trained%0Aclassifier.%20We%20further%20apply%20the%20same%20method%20to%20image%20inputs%20and%20obtain%20high%0Aquality%20explanations%20for%20image%20classifications%2C%20which%20indicates%20that%20the%0Aconditions%20proposed%20for%20rationale%20extraction%20in%20natural%20language%20processing%20are%0Amore%20broadly%20applicable%20to%20different%20input%20types.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11388v1&entry.124074799=Read"},
{"title": "AirTrafficGen: Configurable Air Traffic Scenario Generation with Large\n  Language Models", "author": "Dewi Sid William Gould and George De Ath and Ben Carvell and Nick Pepper", "abstract": "  The manual design of scenarios for Air Traffic Control (ATC) training is a\ndemanding and time-consuming bottleneck that limits the diversity of\nsimulations available to controllers. To address this, we introduce a novel,\nend-to-end approach, $\\texttt{AirTrafficGen}$, that leverages large language\nmodels (LLMs) to automate and control the generation of complex ATC scenarios.\nOur method uses a purpose-built, graph-based representation to encode sector\ntopology (including airspace geometry, routes, and fixes) into a format LLMs\ncan process. Through rigorous benchmarking, we show that state-of-the-art\nmodels like Gemini 2.5 Pro, OpenAI o3, GPT-oss-120b and GPT-5 can generate\nhigh-traffic scenarios while maintaining operational realism. Our engineered\nprompting enables fine-grained control over interaction presence, type, and\nlocation. Initial findings suggest these models are also capable of iterative\nrefinement, correcting flawed scenarios based on simple textual feedback. This\napproach provides a scalable alternative to manual scenario design, addressing\nthe need for a greater volume and variety of ATC training and validation\nsimulations. More broadly, this work showcases the potential of LLMs for\ncomplex planning in safety-critical domains.\n", "link": "http://arxiv.org/abs/2508.02269v2", "date": "2025-08-15", "relevancy": 2.1227, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5568}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AirTrafficGen%3A%20Configurable%20Air%20Traffic%20Scenario%20Generation%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20AirTrafficGen%3A%20Configurable%20Air%20Traffic%20Scenario%20Generation%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Dewi%20Sid%20William%20Gould%20and%20George%20De%20Ath%20and%20Ben%20Carvell%20and%20Nick%20Pepper%0AAbstract%3A%20%20%20The%20manual%20design%20of%20scenarios%20for%20Air%20Traffic%20Control%20%28ATC%29%20training%20is%20a%0Ademanding%20and%20time-consuming%20bottleneck%20that%20limits%20the%20diversity%20of%0Asimulations%20available%20to%20controllers.%20To%20address%20this%2C%20we%20introduce%20a%20novel%2C%0Aend-to-end%20approach%2C%20%24%5Ctexttt%7BAirTrafficGen%7D%24%2C%20that%20leverages%20large%20language%0Amodels%20%28LLMs%29%20to%20automate%20and%20control%20the%20generation%20of%20complex%20ATC%20scenarios.%0AOur%20method%20uses%20a%20purpose-built%2C%20graph-based%20representation%20to%20encode%20sector%0Atopology%20%28including%20airspace%20geometry%2C%20routes%2C%20and%20fixes%29%20into%20a%20format%20LLMs%0Acan%20process.%20Through%20rigorous%20benchmarking%2C%20we%20show%20that%20state-of-the-art%0Amodels%20like%20Gemini%202.5%20Pro%2C%20OpenAI%20o3%2C%20GPT-oss-120b%20and%20GPT-5%20can%20generate%0Ahigh-traffic%20scenarios%20while%20maintaining%20operational%20realism.%20Our%20engineered%0Aprompting%20enables%20fine-grained%20control%20over%20interaction%20presence%2C%20type%2C%20and%0Alocation.%20Initial%20findings%20suggest%20these%20models%20are%20also%20capable%20of%20iterative%0Arefinement%2C%20correcting%20flawed%20scenarios%20based%20on%20simple%20textual%20feedback.%20This%0Aapproach%20provides%20a%20scalable%20alternative%20to%20manual%20scenario%20design%2C%20addressing%0Athe%20need%20for%20a%20greater%20volume%20and%20variety%20of%20ATC%20training%20and%20validation%0Asimulations.%20More%20broadly%2C%20this%20work%20showcases%20the%20potential%20of%20LLMs%20for%0Acomplex%20planning%20in%20safety-critical%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02269v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAirTrafficGen%253A%2520Configurable%2520Air%2520Traffic%2520Scenario%2520Generation%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DDewi%2520Sid%2520William%2520Gould%2520and%2520George%2520De%2520Ath%2520and%2520Ben%2520Carvell%2520and%2520Nick%2520Pepper%26entry.1292438233%3D%2520%2520The%2520manual%2520design%2520of%2520scenarios%2520for%2520Air%2520Traffic%2520Control%2520%2528ATC%2529%2520training%2520is%2520a%250Ademanding%2520and%2520time-consuming%2520bottleneck%2520that%2520limits%2520the%2520diversity%2520of%250Asimulations%2520available%2520to%2520controllers.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%252C%250Aend-to-end%2520approach%252C%2520%2524%255Ctexttt%257BAirTrafficGen%257D%2524%252C%2520that%2520leverages%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520automate%2520and%2520control%2520the%2520generation%2520of%2520complex%2520ATC%2520scenarios.%250AOur%2520method%2520uses%2520a%2520purpose-built%252C%2520graph-based%2520representation%2520to%2520encode%2520sector%250Atopology%2520%2528including%2520airspace%2520geometry%252C%2520routes%252C%2520and%2520fixes%2529%2520into%2520a%2520format%2520LLMs%250Acan%2520process.%2520Through%2520rigorous%2520benchmarking%252C%2520we%2520show%2520that%2520state-of-the-art%250Amodels%2520like%2520Gemini%25202.5%2520Pro%252C%2520OpenAI%2520o3%252C%2520GPT-oss-120b%2520and%2520GPT-5%2520can%2520generate%250Ahigh-traffic%2520scenarios%2520while%2520maintaining%2520operational%2520realism.%2520Our%2520engineered%250Aprompting%2520enables%2520fine-grained%2520control%2520over%2520interaction%2520presence%252C%2520type%252C%2520and%250Alocation.%2520Initial%2520findings%2520suggest%2520these%2520models%2520are%2520also%2520capable%2520of%2520iterative%250Arefinement%252C%2520correcting%2520flawed%2520scenarios%2520based%2520on%2520simple%2520textual%2520feedback.%2520This%250Aapproach%2520provides%2520a%2520scalable%2520alternative%2520to%2520manual%2520scenario%2520design%252C%2520addressing%250Athe%2520need%2520for%2520a%2520greater%2520volume%2520and%2520variety%2520of%2520ATC%2520training%2520and%2520validation%250Asimulations.%2520More%2520broadly%252C%2520this%2520work%2520showcases%2520the%2520potential%2520of%2520LLMs%2520for%250Acomplex%2520planning%2520in%2520safety-critical%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02269v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AirTrafficGen%3A%20Configurable%20Air%20Traffic%20Scenario%20Generation%20with%20Large%0A%20%20Language%20Models&entry.906535625=Dewi%20Sid%20William%20Gould%20and%20George%20De%20Ath%20and%20Ben%20Carvell%20and%20Nick%20Pepper&entry.1292438233=%20%20The%20manual%20design%20of%20scenarios%20for%20Air%20Traffic%20Control%20%28ATC%29%20training%20is%20a%0Ademanding%20and%20time-consuming%20bottleneck%20that%20limits%20the%20diversity%20of%0Asimulations%20available%20to%20controllers.%20To%20address%20this%2C%20we%20introduce%20a%20novel%2C%0Aend-to-end%20approach%2C%20%24%5Ctexttt%7BAirTrafficGen%7D%24%2C%20that%20leverages%20large%20language%0Amodels%20%28LLMs%29%20to%20automate%20and%20control%20the%20generation%20of%20complex%20ATC%20scenarios.%0AOur%20method%20uses%20a%20purpose-built%2C%20graph-based%20representation%20to%20encode%20sector%0Atopology%20%28including%20airspace%20geometry%2C%20routes%2C%20and%20fixes%29%20into%20a%20format%20LLMs%0Acan%20process.%20Through%20rigorous%20benchmarking%2C%20we%20show%20that%20state-of-the-art%0Amodels%20like%20Gemini%202.5%20Pro%2C%20OpenAI%20o3%2C%20GPT-oss-120b%20and%20GPT-5%20can%20generate%0Ahigh-traffic%20scenarios%20while%20maintaining%20operational%20realism.%20Our%20engineered%0Aprompting%20enables%20fine-grained%20control%20over%20interaction%20presence%2C%20type%2C%20and%0Alocation.%20Initial%20findings%20suggest%20these%20models%20are%20also%20capable%20of%20iterative%0Arefinement%2C%20correcting%20flawed%20scenarios%20based%20on%20simple%20textual%20feedback.%20This%0Aapproach%20provides%20a%20scalable%20alternative%20to%20manual%20scenario%20design%2C%20addressing%0Athe%20need%20for%20a%20greater%20volume%20and%20variety%20of%20ATC%20training%20and%20validation%0Asimulations.%20More%20broadly%2C%20this%20work%20showcases%20the%20potential%20of%20LLMs%20for%0Acomplex%20planning%20in%20safety-critical%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02269v2&entry.124074799=Read"},
{"title": "Controlling Multimodal LLMs via Reward-guided Decoding", "author": "Oscar Ma\u00f1as and Pierluca D'Oro and Koustuv Sinha and Adriana Romero-Soriano and Michal Drozdzal and Aishwarya Agrawal", "abstract": "  As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.\n", "link": "http://arxiv.org/abs/2508.11616v1", "date": "2025-08-15", "relevancy": 2.1153, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controlling%20Multimodal%20LLMs%20via%20Reward-guided%20Decoding&body=Title%3A%20Controlling%20Multimodal%20LLMs%20via%20Reward-guided%20Decoding%0AAuthor%3A%20Oscar%20Ma%C3%B1as%20and%20Pierluca%20D%27Oro%20and%20Koustuv%20Sinha%20and%20Adriana%20Romero-Soriano%20and%20Michal%20Drozdzal%20and%20Aishwarya%20Agrawal%0AAbstract%3A%20%20%20As%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20gain%20widespread%20applicability%2C%20it%0Ais%20becoming%20increasingly%20desirable%20to%20adapt%20them%20for%20diverse%20user%20needs.%20In%0Athis%20paper%2C%20we%20study%20the%20adaptation%20of%20MLLMs%20through%20controlled%20decoding.%20To%0Aachieve%20this%2C%20we%20introduce%20the%20first%20method%20for%20reward-guided%20decoding%20of%20MLLMs%0Aand%20demonstrate%20its%20application%20in%20improving%20their%20visual%20grounding.%20Our%20method%0Ainvolves%20building%20reward%20models%20for%20visual%20grounding%20and%20using%20them%20to%20guide%0Athe%20MLLM%27s%20decoding%20process.%20Concretely%2C%20we%20build%20two%20separate%20reward%20models%20to%0Aindependently%20control%20the%20degree%20of%20object%20precision%20and%20recall%20in%20the%20model%27s%0Aoutput.%20Our%20approach%20enables%20on-the-fly%20controllability%20of%20an%20MLLM%27s%20inference%0Aprocess%20in%20two%20ways%3A%20first%2C%20by%20giving%20control%20over%20the%20relative%20importance%20of%0Aeach%20reward%20function%20during%20decoding%2C%20allowing%20a%20user%20to%20dynamically%20trade%20off%0Aobject%20precision%20for%20recall%20in%20image%20captioning%20tasks%3B%20second%2C%20by%20giving%0Acontrol%20over%20the%20breadth%20of%20the%20search%20during%20decoding%2C%20allowing%20the%20user%20to%0Acontrol%20the%20trade-off%20between%20the%20amount%20of%20test-time%20compute%20and%20the%20degree%20of%0Avisual%20grounding.%20We%20evaluate%20our%20method%20on%20standard%20object%20hallucination%0Abenchmarks%2C%20showing%20that%20it%20provides%20significant%20controllability%20over%20MLLM%0Ainference%2C%20while%20consistently%20outperforming%20existing%20hallucination%20mitigation%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlling%2520Multimodal%2520LLMs%2520via%2520Reward-guided%2520Decoding%26entry.906535625%3DOscar%2520Ma%25C3%25B1as%2520and%2520Pierluca%2520D%2527Oro%2520and%2520Koustuv%2520Sinha%2520and%2520Adriana%2520Romero-Soriano%2520and%2520Michal%2520Drozdzal%2520and%2520Aishwarya%2520Agrawal%26entry.1292438233%3D%2520%2520As%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520gain%2520widespread%2520applicability%252C%2520it%250Ais%2520becoming%2520increasingly%2520desirable%2520to%2520adapt%2520them%2520for%2520diverse%2520user%2520needs.%2520In%250Athis%2520paper%252C%2520we%2520study%2520the%2520adaptation%2520of%2520MLLMs%2520through%2520controlled%2520decoding.%2520To%250Aachieve%2520this%252C%2520we%2520introduce%2520the%2520first%2520method%2520for%2520reward-guided%2520decoding%2520of%2520MLLMs%250Aand%2520demonstrate%2520its%2520application%2520in%2520improving%2520their%2520visual%2520grounding.%2520Our%2520method%250Ainvolves%2520building%2520reward%2520models%2520for%2520visual%2520grounding%2520and%2520using%2520them%2520to%2520guide%250Athe%2520MLLM%2527s%2520decoding%2520process.%2520Concretely%252C%2520we%2520build%2520two%2520separate%2520reward%2520models%2520to%250Aindependently%2520control%2520the%2520degree%2520of%2520object%2520precision%2520and%2520recall%2520in%2520the%2520model%2527s%250Aoutput.%2520Our%2520approach%2520enables%2520on-the-fly%2520controllability%2520of%2520an%2520MLLM%2527s%2520inference%250Aprocess%2520in%2520two%2520ways%253A%2520first%252C%2520by%2520giving%2520control%2520over%2520the%2520relative%2520importance%2520of%250Aeach%2520reward%2520function%2520during%2520decoding%252C%2520allowing%2520a%2520user%2520to%2520dynamically%2520trade%2520off%250Aobject%2520precision%2520for%2520recall%2520in%2520image%2520captioning%2520tasks%253B%2520second%252C%2520by%2520giving%250Acontrol%2520over%2520the%2520breadth%2520of%2520the%2520search%2520during%2520decoding%252C%2520allowing%2520the%2520user%2520to%250Acontrol%2520the%2520trade-off%2520between%2520the%2520amount%2520of%2520test-time%2520compute%2520and%2520the%2520degree%2520of%250Avisual%2520grounding.%2520We%2520evaluate%2520our%2520method%2520on%2520standard%2520object%2520hallucination%250Abenchmarks%252C%2520showing%2520that%2520it%2520provides%2520significant%2520controllability%2520over%2520MLLM%250Ainference%252C%2520while%2520consistently%2520outperforming%2520existing%2520hallucination%2520mitigation%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20Multimodal%20LLMs%20via%20Reward-guided%20Decoding&entry.906535625=Oscar%20Ma%C3%B1as%20and%20Pierluca%20D%27Oro%20and%20Koustuv%20Sinha%20and%20Adriana%20Romero-Soriano%20and%20Michal%20Drozdzal%20and%20Aishwarya%20Agrawal&entry.1292438233=%20%20As%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20gain%20widespread%20applicability%2C%20it%0Ais%20becoming%20increasingly%20desirable%20to%20adapt%20them%20for%20diverse%20user%20needs.%20In%0Athis%20paper%2C%20we%20study%20the%20adaptation%20of%20MLLMs%20through%20controlled%20decoding.%20To%0Aachieve%20this%2C%20we%20introduce%20the%20first%20method%20for%20reward-guided%20decoding%20of%20MLLMs%0Aand%20demonstrate%20its%20application%20in%20improving%20their%20visual%20grounding.%20Our%20method%0Ainvolves%20building%20reward%20models%20for%20visual%20grounding%20and%20using%20them%20to%20guide%0Athe%20MLLM%27s%20decoding%20process.%20Concretely%2C%20we%20build%20two%20separate%20reward%20models%20to%0Aindependently%20control%20the%20degree%20of%20object%20precision%20and%20recall%20in%20the%20model%27s%0Aoutput.%20Our%20approach%20enables%20on-the-fly%20controllability%20of%20an%20MLLM%27s%20inference%0Aprocess%20in%20two%20ways%3A%20first%2C%20by%20giving%20control%20over%20the%20relative%20importance%20of%0Aeach%20reward%20function%20during%20decoding%2C%20allowing%20a%20user%20to%20dynamically%20trade%20off%0Aobject%20precision%20for%20recall%20in%20image%20captioning%20tasks%3B%20second%2C%20by%20giving%0Acontrol%20over%20the%20breadth%20of%20the%20search%20during%20decoding%2C%20allowing%20the%20user%20to%0Acontrol%20the%20trade-off%20between%20the%20amount%20of%20test-time%20compute%20and%20the%20degree%20of%0Avisual%20grounding.%20We%20evaluate%20our%20method%20on%20standard%20object%20hallucination%0Abenchmarks%2C%20showing%20that%20it%20provides%20significant%20controllability%20over%20MLLM%0Ainference%2C%20while%20consistently%20outperforming%20existing%20hallucination%20mitigation%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11616v1&entry.124074799=Read"},
{"title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks", "author": "Songqin Nong and Jingxuan Xu and Sheng Zhou and Jianfeng Chen and Xiaoxuan Tang and Tao Jiang and Wenhao Xu", "abstract": "  As autonomous agents become adept at understanding and interacting with\ngraphical user interface (GUI) environments, a new era of automated task\nexecution is emerging. Recent studies have demonstrated that Reinforcement\nLearning (RL) can effectively enhance agents' performance in dynamic\ninteractive GUI environments. However, these methods face two key limitations:\n(1) they overlook the significant variation in difficulty across different GUI\ntasks by treating the entire training data as a uniform set, which hampers the\nagent's ability to adapt its learning process; and (2) most approaches collapse\ntask-specific nuances into a single, coarse reward, leaving the agent with a\nuniform signal that yields inefficient policy updates. To address these\nlimitations, we propose CRAFT-GUI, a curriculum learning framework based on\nGroup Relative Policy Optimization (GRPO) that explicitly accounts for the\nvarying difficulty across trajectories. To enable more fine-grained policy\noptimization, we design a reward function that combines simple rule-based\nsignals with model-judged evaluation, providing richer and more nuanced\nfeedback during training. Experimental results demonstrate that our method\nachieves significant improvements over previous state-of-the-art approaches,\noutperforming them by 5.6% on public benchmarks Android Control and 10.3% on\nour internal online benchmarks, respectively. These findings empirically\nvalidate the effectiveness of integrating reinforcement learning with\ncurriculum learning in GUI interaction tasks.\n", "link": "http://arxiv.org/abs/2508.11360v1", "date": "2025-08-15", "relevancy": 2.1059, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5512}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5256}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRAFT-GUI%3A%20Curriculum-Reinforced%20Agent%20For%20GUI%20Tasks&body=Title%3A%20CRAFT-GUI%3A%20Curriculum-Reinforced%20Agent%20For%20GUI%20Tasks%0AAuthor%3A%20Songqin%20Nong%20and%20Jingxuan%20Xu%20and%20Sheng%20Zhou%20and%20Jianfeng%20Chen%20and%20Xiaoxuan%20Tang%20and%20Tao%20Jiang%20and%20Wenhao%20Xu%0AAbstract%3A%20%20%20As%20autonomous%20agents%20become%20adept%20at%20understanding%20and%20interacting%20with%0Agraphical%20user%20interface%20%28GUI%29%20environments%2C%20a%20new%20era%20of%20automated%20task%0Aexecution%20is%20emerging.%20Recent%20studies%20have%20demonstrated%20that%20Reinforcement%0ALearning%20%28RL%29%20can%20effectively%20enhance%20agents%27%20performance%20in%20dynamic%0Ainteractive%20GUI%20environments.%20However%2C%20these%20methods%20face%20two%20key%20limitations%3A%0A%281%29%20they%20overlook%20the%20significant%20variation%20in%20difficulty%20across%20different%20GUI%0Atasks%20by%20treating%20the%20entire%20training%20data%20as%20a%20uniform%20set%2C%20which%20hampers%20the%0Aagent%27s%20ability%20to%20adapt%20its%20learning%20process%3B%20and%20%282%29%20most%20approaches%20collapse%0Atask-specific%20nuances%20into%20a%20single%2C%20coarse%20reward%2C%20leaving%20the%20agent%20with%20a%0Auniform%20signal%20that%20yields%20inefficient%20policy%20updates.%20To%20address%20these%0Alimitations%2C%20we%20propose%20CRAFT-GUI%2C%20a%20curriculum%20learning%20framework%20based%20on%0AGroup%20Relative%20Policy%20Optimization%20%28GRPO%29%20that%20explicitly%20accounts%20for%20the%0Avarying%20difficulty%20across%20trajectories.%20To%20enable%20more%20fine-grained%20policy%0Aoptimization%2C%20we%20design%20a%20reward%20function%20that%20combines%20simple%20rule-based%0Asignals%20with%20model-judged%20evaluation%2C%20providing%20richer%20and%20more%20nuanced%0Afeedback%20during%20training.%20Experimental%20results%20demonstrate%20that%20our%20method%0Aachieves%20significant%20improvements%20over%20previous%20state-of-the-art%20approaches%2C%0Aoutperforming%20them%20by%205.6%25%20on%20public%20benchmarks%20Android%20Control%20and%2010.3%25%20on%0Aour%20internal%20online%20benchmarks%2C%20respectively.%20These%20findings%20empirically%0Avalidate%20the%20effectiveness%20of%20integrating%20reinforcement%20learning%20with%0Acurriculum%20learning%20in%20GUI%20interaction%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRAFT-GUI%253A%2520Curriculum-Reinforced%2520Agent%2520For%2520GUI%2520Tasks%26entry.906535625%3DSongqin%2520Nong%2520and%2520Jingxuan%2520Xu%2520and%2520Sheng%2520Zhou%2520and%2520Jianfeng%2520Chen%2520and%2520Xiaoxuan%2520Tang%2520and%2520Tao%2520Jiang%2520and%2520Wenhao%2520Xu%26entry.1292438233%3D%2520%2520As%2520autonomous%2520agents%2520become%2520adept%2520at%2520understanding%2520and%2520interacting%2520with%250Agraphical%2520user%2520interface%2520%2528GUI%2529%2520environments%252C%2520a%2520new%2520era%2520of%2520automated%2520task%250Aexecution%2520is%2520emerging.%2520Recent%2520studies%2520have%2520demonstrated%2520that%2520Reinforcement%250ALearning%2520%2528RL%2529%2520can%2520effectively%2520enhance%2520agents%2527%2520performance%2520in%2520dynamic%250Ainteractive%2520GUI%2520environments.%2520However%252C%2520these%2520methods%2520face%2520two%2520key%2520limitations%253A%250A%25281%2529%2520they%2520overlook%2520the%2520significant%2520variation%2520in%2520difficulty%2520across%2520different%2520GUI%250Atasks%2520by%2520treating%2520the%2520entire%2520training%2520data%2520as%2520a%2520uniform%2520set%252C%2520which%2520hampers%2520the%250Aagent%2527s%2520ability%2520to%2520adapt%2520its%2520learning%2520process%253B%2520and%2520%25282%2529%2520most%2520approaches%2520collapse%250Atask-specific%2520nuances%2520into%2520a%2520single%252C%2520coarse%2520reward%252C%2520leaving%2520the%2520agent%2520with%2520a%250Auniform%2520signal%2520that%2520yields%2520inefficient%2520policy%2520updates.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520CRAFT-GUI%252C%2520a%2520curriculum%2520learning%2520framework%2520based%2520on%250AGroup%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520that%2520explicitly%2520accounts%2520for%2520the%250Avarying%2520difficulty%2520across%2520trajectories.%2520To%2520enable%2520more%2520fine-grained%2520policy%250Aoptimization%252C%2520we%2520design%2520a%2520reward%2520function%2520that%2520combines%2520simple%2520rule-based%250Asignals%2520with%2520model-judged%2520evaluation%252C%2520providing%2520richer%2520and%2520more%2520nuanced%250Afeedback%2520during%2520training.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%250Aachieves%2520significant%2520improvements%2520over%2520previous%2520state-of-the-art%2520approaches%252C%250Aoutperforming%2520them%2520by%25205.6%2525%2520on%2520public%2520benchmarks%2520Android%2520Control%2520and%252010.3%2525%2520on%250Aour%2520internal%2520online%2520benchmarks%252C%2520respectively.%2520These%2520findings%2520empirically%250Avalidate%2520the%2520effectiveness%2520of%2520integrating%2520reinforcement%2520learning%2520with%250Acurriculum%2520learning%2520in%2520GUI%2520interaction%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRAFT-GUI%3A%20Curriculum-Reinforced%20Agent%20For%20GUI%20Tasks&entry.906535625=Songqin%20Nong%20and%20Jingxuan%20Xu%20and%20Sheng%20Zhou%20and%20Jianfeng%20Chen%20and%20Xiaoxuan%20Tang%20and%20Tao%20Jiang%20and%20Wenhao%20Xu&entry.1292438233=%20%20As%20autonomous%20agents%20become%20adept%20at%20understanding%20and%20interacting%20with%0Agraphical%20user%20interface%20%28GUI%29%20environments%2C%20a%20new%20era%20of%20automated%20task%0Aexecution%20is%20emerging.%20Recent%20studies%20have%20demonstrated%20that%20Reinforcement%0ALearning%20%28RL%29%20can%20effectively%20enhance%20agents%27%20performance%20in%20dynamic%0Ainteractive%20GUI%20environments.%20However%2C%20these%20methods%20face%20two%20key%20limitations%3A%0A%281%29%20they%20overlook%20the%20significant%20variation%20in%20difficulty%20across%20different%20GUI%0Atasks%20by%20treating%20the%20entire%20training%20data%20as%20a%20uniform%20set%2C%20which%20hampers%20the%0Aagent%27s%20ability%20to%20adapt%20its%20learning%20process%3B%20and%20%282%29%20most%20approaches%20collapse%0Atask-specific%20nuances%20into%20a%20single%2C%20coarse%20reward%2C%20leaving%20the%20agent%20with%20a%0Auniform%20signal%20that%20yields%20inefficient%20policy%20updates.%20To%20address%20these%0Alimitations%2C%20we%20propose%20CRAFT-GUI%2C%20a%20curriculum%20learning%20framework%20based%20on%0AGroup%20Relative%20Policy%20Optimization%20%28GRPO%29%20that%20explicitly%20accounts%20for%20the%0Avarying%20difficulty%20across%20trajectories.%20To%20enable%20more%20fine-grained%20policy%0Aoptimization%2C%20we%20design%20a%20reward%20function%20that%20combines%20simple%20rule-based%0Asignals%20with%20model-judged%20evaluation%2C%20providing%20richer%20and%20more%20nuanced%0Afeedback%20during%20training.%20Experimental%20results%20demonstrate%20that%20our%20method%0Aachieves%20significant%20improvements%20over%20previous%20state-of-the-art%20approaches%2C%0Aoutperforming%20them%20by%205.6%25%20on%20public%20benchmarks%20Android%20Control%20and%2010.3%25%20on%0Aour%20internal%20online%20benchmarks%2C%20respectively.%20These%20findings%20empirically%0Avalidate%20the%20effectiveness%20of%20integrating%20reinforcement%20learning%20with%0Acurriculum%20learning%20in%20GUI%20interaction%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11360v1&entry.124074799=Read"},
{"title": "Reference Points in LLM Sentiment Analysis: The Role of Structured\n  Context", "author": "Junichiro Niimi", "abstract": "  Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment.\n", "link": "http://arxiv.org/abs/2508.11454v1", "date": "2025-08-15", "relevancy": 2.1008, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5328}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5328}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reference%20Points%20in%20LLM%20Sentiment%20Analysis%3A%20The%20Role%20of%20Structured%0A%20%20Context&body=Title%3A%20Reference%20Points%20in%20LLM%20Sentiment%20Analysis%3A%20The%20Role%20of%20Structured%0A%20%20Context%0AAuthor%3A%20Junichiro%20Niimi%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20now%20widely%20used%20across%20many%20fields%2C%0Aincluding%20marketing%20research.%20Sentiment%20analysis%2C%20in%20particular%2C%20helps%20firms%0Aunderstand%20consumer%20preferences.%20While%20most%20NLP%20studies%20classify%20sentiment%20from%0Areview%20text%20alone%2C%20marketing%20theories%2C%20such%20as%20prospect%20theory%20and%0Aexpectation--disconfirmation%20theory%2C%20point%20out%20that%20customer%20evaluations%20are%0Ashaped%20not%20only%20by%20the%20actual%20experience%20but%20also%20by%20additional%20reference%0Apoints.%20This%20study%20therefore%20investigates%20how%20the%20content%20and%20format%20of%20such%0Asupplementary%20information%20affect%20sentiment%20analysis%20using%20LLMs.%20We%20compare%0Anatural%20language%20%28NL%29%20and%20JSON-formatted%20prompts%20using%20a%20lightweight%203B%0Aparameter%20model%20suitable%20for%20practical%20marketing%20applications.%20Experiments%20on%0Atwo%20Yelp%20categories%20%28Restaurant%20and%20Nightlife%29%20show%20that%20the%20JSON%20prompt%20with%0Aadditional%20information%20outperforms%20all%20baselines%20without%20fine-tuning%3A%20Macro-F1%0Arises%20by%201.6%25%20and%204%25%20while%20RMSE%20falls%20by%2016%25%20and%209.1%25%2C%20respectively%2C%20making%20it%0Adeployable%20in%20resource-constrained%20edge%20devices.%20Furthermore%2C%20a%20follow-up%0Aanalysis%20confirms%20that%20performance%20gains%20stem%20from%20genuine%20contextual%20reasoning%0Arather%20than%20label%20proxying.%20This%20work%20demonstrates%20that%20structured%20prompting%0Acan%20enable%20smaller%20models%20to%20achieve%20competitive%20performance%2C%20offering%20a%0Apractical%20alternative%20to%20large-scale%20model%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReference%2520Points%2520in%2520LLM%2520Sentiment%2520Analysis%253A%2520The%2520Role%2520of%2520Structured%250A%2520%2520Context%26entry.906535625%3DJunichiro%2520Niimi%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520now%2520widely%2520used%2520across%2520many%2520fields%252C%250Aincluding%2520marketing%2520research.%2520Sentiment%2520analysis%252C%2520in%2520particular%252C%2520helps%2520firms%250Aunderstand%2520consumer%2520preferences.%2520While%2520most%2520NLP%2520studies%2520classify%2520sentiment%2520from%250Areview%2520text%2520alone%252C%2520marketing%2520theories%252C%2520such%2520as%2520prospect%2520theory%2520and%250Aexpectation--disconfirmation%2520theory%252C%2520point%2520out%2520that%2520customer%2520evaluations%2520are%250Ashaped%2520not%2520only%2520by%2520the%2520actual%2520experience%2520but%2520also%2520by%2520additional%2520reference%250Apoints.%2520This%2520study%2520therefore%2520investigates%2520how%2520the%2520content%2520and%2520format%2520of%2520such%250Asupplementary%2520information%2520affect%2520sentiment%2520analysis%2520using%2520LLMs.%2520We%2520compare%250Anatural%2520language%2520%2528NL%2529%2520and%2520JSON-formatted%2520prompts%2520using%2520a%2520lightweight%25203B%250Aparameter%2520model%2520suitable%2520for%2520practical%2520marketing%2520applications.%2520Experiments%2520on%250Atwo%2520Yelp%2520categories%2520%2528Restaurant%2520and%2520Nightlife%2529%2520show%2520that%2520the%2520JSON%2520prompt%2520with%250Aadditional%2520information%2520outperforms%2520all%2520baselines%2520without%2520fine-tuning%253A%2520Macro-F1%250Arises%2520by%25201.6%2525%2520and%25204%2525%2520while%2520RMSE%2520falls%2520by%252016%2525%2520and%25209.1%2525%252C%2520respectively%252C%2520making%2520it%250Adeployable%2520in%2520resource-constrained%2520edge%2520devices.%2520Furthermore%252C%2520a%2520follow-up%250Aanalysis%2520confirms%2520that%2520performance%2520gains%2520stem%2520from%2520genuine%2520contextual%2520reasoning%250Arather%2520than%2520label%2520proxying.%2520This%2520work%2520demonstrates%2520that%2520structured%2520prompting%250Acan%2520enable%2520smaller%2520models%2520to%2520achieve%2520competitive%2520performance%252C%2520offering%2520a%250Apractical%2520alternative%2520to%2520large-scale%2520model%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reference%20Points%20in%20LLM%20Sentiment%20Analysis%3A%20The%20Role%20of%20Structured%0A%20%20Context&entry.906535625=Junichiro%20Niimi&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20now%20widely%20used%20across%20many%20fields%2C%0Aincluding%20marketing%20research.%20Sentiment%20analysis%2C%20in%20particular%2C%20helps%20firms%0Aunderstand%20consumer%20preferences.%20While%20most%20NLP%20studies%20classify%20sentiment%20from%0Areview%20text%20alone%2C%20marketing%20theories%2C%20such%20as%20prospect%20theory%20and%0Aexpectation--disconfirmation%20theory%2C%20point%20out%20that%20customer%20evaluations%20are%0Ashaped%20not%20only%20by%20the%20actual%20experience%20but%20also%20by%20additional%20reference%0Apoints.%20This%20study%20therefore%20investigates%20how%20the%20content%20and%20format%20of%20such%0Asupplementary%20information%20affect%20sentiment%20analysis%20using%20LLMs.%20We%20compare%0Anatural%20language%20%28NL%29%20and%20JSON-formatted%20prompts%20using%20a%20lightweight%203B%0Aparameter%20model%20suitable%20for%20practical%20marketing%20applications.%20Experiments%20on%0Atwo%20Yelp%20categories%20%28Restaurant%20and%20Nightlife%29%20show%20that%20the%20JSON%20prompt%20with%0Aadditional%20information%20outperforms%20all%20baselines%20without%20fine-tuning%3A%20Macro-F1%0Arises%20by%201.6%25%20and%204%25%20while%20RMSE%20falls%20by%2016%25%20and%209.1%25%2C%20respectively%2C%20making%20it%0Adeployable%20in%20resource-constrained%20edge%20devices.%20Furthermore%2C%20a%20follow-up%0Aanalysis%20confirms%20that%20performance%20gains%20stem%20from%20genuine%20contextual%20reasoning%0Arather%20than%20label%20proxying.%20This%20work%20demonstrates%20that%20structured%20prompting%0Acan%20enable%20smaller%20models%20to%20achieve%20competitive%20performance%2C%20offering%20a%0Apractical%20alternative%20to%20large-scale%20model%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11454v1&entry.124074799=Read"},
{"title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four\n  Habits of Highly Effective STaRs", "author": "Kanishk Gandhi and Ayush Chakravarthy and Anikait Singh and Nathan Lile and Noah D. Goodman", "abstract": "  Test-time inference has emerged as a powerful paradigm for enabling language\nmodels to ``think'' longer and more carefully about complex challenges, much\nlike skilled human experts. While reinforcement learning (RL) can drive\nself-improvement in language models on verifiable tasks, some models exhibit\nsubstantial gains while others quickly plateau. For instance, we find that\nQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game\nof Countdown. This discrepancy raises a critical question: what intrinsic\nproperties enable effective self-improvement? We introduce a framework to\ninvestigate this question by analyzing four key cognitive behaviors --\nverification, backtracking, subgoal setting, and backward chaining -- that both\nexpert human problem solvers and successful language models employ. Our study\nreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama\ninitially lacks them. In systematic experimentation with controlled behavioral\ndatasets, we find that priming Llama with examples containing these reasoning\nbehaviors enables substantial improvements during RL, matching or exceeding\nQwen's performance. Importantly, the presence of reasoning behaviors, rather\nthan correctness of answers, proves to be the critical factor -- models primed\nwith incorrect solutions containing proper reasoning patterns achieve\ncomparable performance to those trained on correct solutions. Finally,\nleveraging continued pretraining with OpenWebMath data, filtered to amplify\nreasoning behaviors, enables the Llama model to match Qwen's self-improvement\ntrajectory. Our findings establish a fundamental relationship between initial\nreasoning behaviors and the capacity for improvement, explaining why some\nlanguage models effectively utilize additional computation while others\nplateau.\n", "link": "http://arxiv.org/abs/2503.01307v2", "date": "2025-08-15", "relevancy": 2.0859, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cognitive%20Behaviors%20that%20Enable%20Self-Improving%20Reasoners%2C%20or%2C%20Four%0A%20%20Habits%20of%20Highly%20Effective%20STaRs&body=Title%3A%20Cognitive%20Behaviors%20that%20Enable%20Self-Improving%20Reasoners%2C%20or%2C%20Four%0A%20%20Habits%20of%20Highly%20Effective%20STaRs%0AAuthor%3A%20Kanishk%20Gandhi%20and%20Ayush%20Chakravarthy%20and%20Anikait%20Singh%20and%20Nathan%20Lile%20and%20Noah%20D.%20Goodman%0AAbstract%3A%20%20%20Test-time%20inference%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20enabling%20language%0Amodels%20to%20%60%60think%27%27%20longer%20and%20more%20carefully%20about%20complex%20challenges%2C%20much%0Alike%20skilled%20human%20experts.%20While%20reinforcement%20learning%20%28RL%29%20can%20drive%0Aself-improvement%20in%20language%20models%20on%20verifiable%20tasks%2C%20some%20models%20exhibit%0Asubstantial%20gains%20while%20others%20quickly%20plateau.%20For%20instance%2C%20we%20find%20that%0AQwen-2.5-3B%20far%20exceeds%20Llama-3.2-3B%20under%20identical%20RL%20training%20for%20the%20game%0Aof%20Countdown.%20This%20discrepancy%20raises%20a%20critical%20question%3A%20what%20intrinsic%0Aproperties%20enable%20effective%20self-improvement%3F%20We%20introduce%20a%20framework%20to%0Ainvestigate%20this%20question%20by%20analyzing%20four%20key%20cognitive%20behaviors%20--%0Averification%2C%20backtracking%2C%20subgoal%20setting%2C%20and%20backward%20chaining%20--%20that%20both%0Aexpert%20human%20problem%20solvers%20and%20successful%20language%20models%20employ.%20Our%20study%0Areveals%20that%20Qwen%20naturally%20exhibits%20these%20reasoning%20behaviors%2C%20whereas%20Llama%0Ainitially%20lacks%20them.%20In%20systematic%20experimentation%20with%20controlled%20behavioral%0Adatasets%2C%20we%20find%20that%20priming%20Llama%20with%20examples%20containing%20these%20reasoning%0Abehaviors%20enables%20substantial%20improvements%20during%20RL%2C%20matching%20or%20exceeding%0AQwen%27s%20performance.%20Importantly%2C%20the%20presence%20of%20reasoning%20behaviors%2C%20rather%0Athan%20correctness%20of%20answers%2C%20proves%20to%20be%20the%20critical%20factor%20--%20models%20primed%0Awith%20incorrect%20solutions%20containing%20proper%20reasoning%20patterns%20achieve%0Acomparable%20performance%20to%20those%20trained%20on%20correct%20solutions.%20Finally%2C%0Aleveraging%20continued%20pretraining%20with%20OpenWebMath%20data%2C%20filtered%20to%20amplify%0Areasoning%20behaviors%2C%20enables%20the%20Llama%20model%20to%20match%20Qwen%27s%20self-improvement%0Atrajectory.%20Our%20findings%20establish%20a%20fundamental%20relationship%20between%20initial%0Areasoning%20behaviors%20and%20the%20capacity%20for%20improvement%2C%20explaining%20why%20some%0Alanguage%20models%20effectively%20utilize%20additional%20computation%20while%20others%0Aplateau.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01307v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCognitive%2520Behaviors%2520that%2520Enable%2520Self-Improving%2520Reasoners%252C%2520or%252C%2520Four%250A%2520%2520Habits%2520of%2520Highly%2520Effective%2520STaRs%26entry.906535625%3DKanishk%2520Gandhi%2520and%2520Ayush%2520Chakravarthy%2520and%2520Anikait%2520Singh%2520and%2520Nathan%2520Lile%2520and%2520Noah%2520D.%2520Goodman%26entry.1292438233%3D%2520%2520Test-time%2520inference%2520has%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%2520enabling%2520language%250Amodels%2520to%2520%2560%2560think%2527%2527%2520longer%2520and%2520more%2520carefully%2520about%2520complex%2520challenges%252C%2520much%250Alike%2520skilled%2520human%2520experts.%2520While%2520reinforcement%2520learning%2520%2528RL%2529%2520can%2520drive%250Aself-improvement%2520in%2520language%2520models%2520on%2520verifiable%2520tasks%252C%2520some%2520models%2520exhibit%250Asubstantial%2520gains%2520while%2520others%2520quickly%2520plateau.%2520For%2520instance%252C%2520we%2520find%2520that%250AQwen-2.5-3B%2520far%2520exceeds%2520Llama-3.2-3B%2520under%2520identical%2520RL%2520training%2520for%2520the%2520game%250Aof%2520Countdown.%2520This%2520discrepancy%2520raises%2520a%2520critical%2520question%253A%2520what%2520intrinsic%250Aproperties%2520enable%2520effective%2520self-improvement%253F%2520We%2520introduce%2520a%2520framework%2520to%250Ainvestigate%2520this%2520question%2520by%2520analyzing%2520four%2520key%2520cognitive%2520behaviors%2520--%250Averification%252C%2520backtracking%252C%2520subgoal%2520setting%252C%2520and%2520backward%2520chaining%2520--%2520that%2520both%250Aexpert%2520human%2520problem%2520solvers%2520and%2520successful%2520language%2520models%2520employ.%2520Our%2520study%250Areveals%2520that%2520Qwen%2520naturally%2520exhibits%2520these%2520reasoning%2520behaviors%252C%2520whereas%2520Llama%250Ainitially%2520lacks%2520them.%2520In%2520systematic%2520experimentation%2520with%2520controlled%2520behavioral%250Adatasets%252C%2520we%2520find%2520that%2520priming%2520Llama%2520with%2520examples%2520containing%2520these%2520reasoning%250Abehaviors%2520enables%2520substantial%2520improvements%2520during%2520RL%252C%2520matching%2520or%2520exceeding%250AQwen%2527s%2520performance.%2520Importantly%252C%2520the%2520presence%2520of%2520reasoning%2520behaviors%252C%2520rather%250Athan%2520correctness%2520of%2520answers%252C%2520proves%2520to%2520be%2520the%2520critical%2520factor%2520--%2520models%2520primed%250Awith%2520incorrect%2520solutions%2520containing%2520proper%2520reasoning%2520patterns%2520achieve%250Acomparable%2520performance%2520to%2520those%2520trained%2520on%2520correct%2520solutions.%2520Finally%252C%250Aleveraging%2520continued%2520pretraining%2520with%2520OpenWebMath%2520data%252C%2520filtered%2520to%2520amplify%250Areasoning%2520behaviors%252C%2520enables%2520the%2520Llama%2520model%2520to%2520match%2520Qwen%2527s%2520self-improvement%250Atrajectory.%2520Our%2520findings%2520establish%2520a%2520fundamental%2520relationship%2520between%2520initial%250Areasoning%2520behaviors%2520and%2520the%2520capacity%2520for%2520improvement%252C%2520explaining%2520why%2520some%250Alanguage%2520models%2520effectively%2520utilize%2520additional%2520computation%2520while%2520others%250Aplateau.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01307v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cognitive%20Behaviors%20that%20Enable%20Self-Improving%20Reasoners%2C%20or%2C%20Four%0A%20%20Habits%20of%20Highly%20Effective%20STaRs&entry.906535625=Kanishk%20Gandhi%20and%20Ayush%20Chakravarthy%20and%20Anikait%20Singh%20and%20Nathan%20Lile%20and%20Noah%20D.%20Goodman&entry.1292438233=%20%20Test-time%20inference%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20enabling%20language%0Amodels%20to%20%60%60think%27%27%20longer%20and%20more%20carefully%20about%20complex%20challenges%2C%20much%0Alike%20skilled%20human%20experts.%20While%20reinforcement%20learning%20%28RL%29%20can%20drive%0Aself-improvement%20in%20language%20models%20on%20verifiable%20tasks%2C%20some%20models%20exhibit%0Asubstantial%20gains%20while%20others%20quickly%20plateau.%20For%20instance%2C%20we%20find%20that%0AQwen-2.5-3B%20far%20exceeds%20Llama-3.2-3B%20under%20identical%20RL%20training%20for%20the%20game%0Aof%20Countdown.%20This%20discrepancy%20raises%20a%20critical%20question%3A%20what%20intrinsic%0Aproperties%20enable%20effective%20self-improvement%3F%20We%20introduce%20a%20framework%20to%0Ainvestigate%20this%20question%20by%20analyzing%20four%20key%20cognitive%20behaviors%20--%0Averification%2C%20backtracking%2C%20subgoal%20setting%2C%20and%20backward%20chaining%20--%20that%20both%0Aexpert%20human%20problem%20solvers%20and%20successful%20language%20models%20employ.%20Our%20study%0Areveals%20that%20Qwen%20naturally%20exhibits%20these%20reasoning%20behaviors%2C%20whereas%20Llama%0Ainitially%20lacks%20them.%20In%20systematic%20experimentation%20with%20controlled%20behavioral%0Adatasets%2C%20we%20find%20that%20priming%20Llama%20with%20examples%20containing%20these%20reasoning%0Abehaviors%20enables%20substantial%20improvements%20during%20RL%2C%20matching%20or%20exceeding%0AQwen%27s%20performance.%20Importantly%2C%20the%20presence%20of%20reasoning%20behaviors%2C%20rather%0Athan%20correctness%20of%20answers%2C%20proves%20to%20be%20the%20critical%20factor%20--%20models%20primed%0Awith%20incorrect%20solutions%20containing%20proper%20reasoning%20patterns%20achieve%0Acomparable%20performance%20to%20those%20trained%20on%20correct%20solutions.%20Finally%2C%0Aleveraging%20continued%20pretraining%20with%20OpenWebMath%20data%2C%20filtered%20to%20amplify%0Areasoning%20behaviors%2C%20enables%20the%20Llama%20model%20to%20match%20Qwen%27s%20self-improvement%0Atrajectory.%20Our%20findings%20establish%20a%20fundamental%20relationship%20between%20initial%0Areasoning%20behaviors%20and%20the%20capacity%20for%20improvement%2C%20explaining%20why%20some%0Alanguage%20models%20effectively%20utilize%20additional%20computation%20while%20others%0Aplateau.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01307v2&entry.124074799=Read"},
{"title": "CryptoScope: Utilizing Large Language Models for Automated Cryptographic\n  Logic Vulnerability Detection", "author": "Zhihao Li and Zimo Ji and Tao Zheng and Hao Ren and Xiao Lan", "abstract": "  Cryptographic algorithms are fundamental to modern security, yet their\nimplementations frequently harbor subtle logic flaws that are hard to detect.\nWe introduce CryptoScope, a novel framework for automated cryptographic\nvulnerability detection powered by Large Language Models (LLMs). CryptoScope\ncombines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation\n(RAG), guided by a curated cryptographic knowledge base containing over 12,000\nentries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily\nderived from real-world CVE vulnerabilities, complemented by cryptographic\nchallenges from major Capture The Flag (CTF) competitions and synthetic\nexamples across 11 programming languages. CryptoScope consistently improves\nperformance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,\nGPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9\npreviously undisclosed flaws in widely used open-source cryptographic projects.\n", "link": "http://arxiv.org/abs/2508.11599v1", "date": "2025-08-15", "relevancy": 2.078, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5347}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CryptoScope%3A%20Utilizing%20Large%20Language%20Models%20for%20Automated%20Cryptographic%0A%20%20Logic%20Vulnerability%20Detection&body=Title%3A%20CryptoScope%3A%20Utilizing%20Large%20Language%20Models%20for%20Automated%20Cryptographic%0A%20%20Logic%20Vulnerability%20Detection%0AAuthor%3A%20Zhihao%20Li%20and%20Zimo%20Ji%20and%20Tao%20Zheng%20and%20Hao%20Ren%20and%20Xiao%20Lan%0AAbstract%3A%20%20%20Cryptographic%20algorithms%20are%20fundamental%20to%20modern%20security%2C%20yet%20their%0Aimplementations%20frequently%20harbor%20subtle%20logic%20flaws%20that%20are%20hard%20to%20detect.%0AWe%20introduce%20CryptoScope%2C%20a%20novel%20framework%20for%20automated%20cryptographic%0Avulnerability%20detection%20powered%20by%20Large%20Language%20Models%20%28LLMs%29.%20CryptoScope%0Acombines%20Chain-of-Thought%20%28CoT%29%20prompting%20with%20Retrieval-Augmented%20Generation%0A%28RAG%29%2C%20guided%20by%20a%20curated%20cryptographic%20knowledge%20base%20containing%20over%2012%2C000%0Aentries.%20We%20evaluate%20CryptoScope%20on%20LLM-CLVA%2C%20a%20benchmark%20of%2092%20cases%20primarily%0Aderived%20from%20real-world%20CVE%20vulnerabilities%2C%20complemented%20by%20cryptographic%0Achallenges%20from%20major%20Capture%20The%20Flag%20%28CTF%29%20competitions%20and%20synthetic%0Aexamples%20across%2011%20programming%20languages.%20CryptoScope%20consistently%20improves%0Aperformance%20over%20strong%20LLM%20baselines%2C%20boosting%20DeepSeek-V3%20by%2011.62%25%2C%0AGPT-4o-mini%20by%2020.28%25%2C%20and%20GLM-4-Flash%20by%2028.69%25.%20Additionally%2C%20it%20identifies%209%0Apreviously%20undisclosed%20flaws%20in%20widely%20used%20open-source%20cryptographic%20projects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCryptoScope%253A%2520Utilizing%2520Large%2520Language%2520Models%2520for%2520Automated%2520Cryptographic%250A%2520%2520Logic%2520Vulnerability%2520Detection%26entry.906535625%3DZhihao%2520Li%2520and%2520Zimo%2520Ji%2520and%2520Tao%2520Zheng%2520and%2520Hao%2520Ren%2520and%2520Xiao%2520Lan%26entry.1292438233%3D%2520%2520Cryptographic%2520algorithms%2520are%2520fundamental%2520to%2520modern%2520security%252C%2520yet%2520their%250Aimplementations%2520frequently%2520harbor%2520subtle%2520logic%2520flaws%2520that%2520are%2520hard%2520to%2520detect.%250AWe%2520introduce%2520CryptoScope%252C%2520a%2520novel%2520framework%2520for%2520automated%2520cryptographic%250Avulnerability%2520detection%2520powered%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520CryptoScope%250Acombines%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520with%2520Retrieval-Augmented%2520Generation%250A%2528RAG%2529%252C%2520guided%2520by%2520a%2520curated%2520cryptographic%2520knowledge%2520base%2520containing%2520over%252012%252C000%250Aentries.%2520We%2520evaluate%2520CryptoScope%2520on%2520LLM-CLVA%252C%2520a%2520benchmark%2520of%252092%2520cases%2520primarily%250Aderived%2520from%2520real-world%2520CVE%2520vulnerabilities%252C%2520complemented%2520by%2520cryptographic%250Achallenges%2520from%2520major%2520Capture%2520The%2520Flag%2520%2528CTF%2529%2520competitions%2520and%2520synthetic%250Aexamples%2520across%252011%2520programming%2520languages.%2520CryptoScope%2520consistently%2520improves%250Aperformance%2520over%2520strong%2520LLM%2520baselines%252C%2520boosting%2520DeepSeek-V3%2520by%252011.62%2525%252C%250AGPT-4o-mini%2520by%252020.28%2525%252C%2520and%2520GLM-4-Flash%2520by%252028.69%2525.%2520Additionally%252C%2520it%2520identifies%25209%250Apreviously%2520undisclosed%2520flaws%2520in%2520widely%2520used%2520open-source%2520cryptographic%2520projects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CryptoScope%3A%20Utilizing%20Large%20Language%20Models%20for%20Automated%20Cryptographic%0A%20%20Logic%20Vulnerability%20Detection&entry.906535625=Zhihao%20Li%20and%20Zimo%20Ji%20and%20Tao%20Zheng%20and%20Hao%20Ren%20and%20Xiao%20Lan&entry.1292438233=%20%20Cryptographic%20algorithms%20are%20fundamental%20to%20modern%20security%2C%20yet%20their%0Aimplementations%20frequently%20harbor%20subtle%20logic%20flaws%20that%20are%20hard%20to%20detect.%0AWe%20introduce%20CryptoScope%2C%20a%20novel%20framework%20for%20automated%20cryptographic%0Avulnerability%20detection%20powered%20by%20Large%20Language%20Models%20%28LLMs%29.%20CryptoScope%0Acombines%20Chain-of-Thought%20%28CoT%29%20prompting%20with%20Retrieval-Augmented%20Generation%0A%28RAG%29%2C%20guided%20by%20a%20curated%20cryptographic%20knowledge%20base%20containing%20over%2012%2C000%0Aentries.%20We%20evaluate%20CryptoScope%20on%20LLM-CLVA%2C%20a%20benchmark%20of%2092%20cases%20primarily%0Aderived%20from%20real-world%20CVE%20vulnerabilities%2C%20complemented%20by%20cryptographic%0Achallenges%20from%20major%20Capture%20The%20Flag%20%28CTF%29%20competitions%20and%20synthetic%0Aexamples%20across%2011%20programming%20languages.%20CryptoScope%20consistently%20improves%0Aperformance%20over%20strong%20LLM%20baselines%2C%20boosting%20DeepSeek-V3%20by%2011.62%25%2C%0AGPT-4o-mini%20by%2020.28%25%2C%20and%20GLM-4-Flash%20by%2028.69%25.%20Additionally%2C%20it%20identifies%209%0Apreviously%20undisclosed%20flaws%20in%20widely%20used%20open-source%20cryptographic%20projects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11599v1&entry.124074799=Read"},
{"title": "An Efficient Medical Image Classification Method Based on a Lightweight\n  Improved ConvNeXt-Tiny Architecture", "author": "Jingsong Xia and Yue Yin and Xiuhan Li", "abstract": "  Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis. However, achieving efficient and high-accuracy image\nclassification in resource-constrained computational environments remains\nchallenging. This study proposes a medical image classification method based on\nan improved ConvNeXt-Tiny architecture. Through structural optimization and\nloss function design, the proposed method enhances feature extraction\ncapability and classification performance while reducing computational\ncomplexity. Specifically, the method introduces a dual global pooling (Global\nAverage Pooling and Global Max Pooling) feature fusion strategy into the\nConvNeXt-Tiny backbone to simultaneously preserve global statistical features\nand salient response information. A lightweight channel attention module,\ntermed Squeeze-and-Excitation Vector (SEVector), is designed to improve the\nadaptive allocation of channel weights while minimizing parameter overhead.\nAdditionally, a Feature Smoothing Loss is incorporated into the loss function\nto enhance intra-class feature consistency and suppress intra-class variance.\nUnder CPU-only conditions (8 threads), the method achieves a maximum\nclassification accuracy of 89.10% on the test set within 10 training epochs,\nexhibiting a stable convergence trend in loss values. Experimental results\ndemonstrate that the proposed method effectively improves medical image\nclassification performance in resource-limited settings, providing a feasible\nand efficient solution for the deployment and promotion of medical imaging\nanalysis models.\n", "link": "http://arxiv.org/abs/2508.11532v1", "date": "2025-08-15", "relevancy": 2.0725, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5545}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5347}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Medical%20Image%20Classification%20Method%20Based%20on%20a%20Lightweight%0A%20%20Improved%20ConvNeXt-Tiny%20Architecture&body=Title%3A%20An%20Efficient%20Medical%20Image%20Classification%20Method%20Based%20on%20a%20Lightweight%0A%20%20Improved%20ConvNeXt-Tiny%20Architecture%0AAuthor%3A%20Jingsong%20Xia%20and%20Yue%20Yin%20and%20Xiuhan%20Li%0AAbstract%3A%20%20%20Intelligent%20analysis%20of%20medical%20imaging%20plays%20a%20crucial%20role%20in%20assisting%0Aclinical%20diagnosis.%20However%2C%20achieving%20efficient%20and%20high-accuracy%20image%0Aclassification%20in%20resource-constrained%20computational%20environments%20remains%0Achallenging.%20This%20study%20proposes%20a%20medical%20image%20classification%20method%20based%20on%0Aan%20improved%20ConvNeXt-Tiny%20architecture.%20Through%20structural%20optimization%20and%0Aloss%20function%20design%2C%20the%20proposed%20method%20enhances%20feature%20extraction%0Acapability%20and%20classification%20performance%20while%20reducing%20computational%0Acomplexity.%20Specifically%2C%20the%20method%20introduces%20a%20dual%20global%20pooling%20%28Global%0AAverage%20Pooling%20and%20Global%20Max%20Pooling%29%20feature%20fusion%20strategy%20into%20the%0AConvNeXt-Tiny%20backbone%20to%20simultaneously%20preserve%20global%20statistical%20features%0Aand%20salient%20response%20information.%20A%20lightweight%20channel%20attention%20module%2C%0Atermed%20Squeeze-and-Excitation%20Vector%20%28SEVector%29%2C%20is%20designed%20to%20improve%20the%0Aadaptive%20allocation%20of%20channel%20weights%20while%20minimizing%20parameter%20overhead.%0AAdditionally%2C%20a%20Feature%20Smoothing%20Loss%20is%20incorporated%20into%20the%20loss%20function%0Ato%20enhance%20intra-class%20feature%20consistency%20and%20suppress%20intra-class%20variance.%0AUnder%20CPU-only%20conditions%20%288%20threads%29%2C%20the%20method%20achieves%20a%20maximum%0Aclassification%20accuracy%20of%2089.10%25%20on%20the%20test%20set%20within%2010%20training%20epochs%2C%0Aexhibiting%20a%20stable%20convergence%20trend%20in%20loss%20values.%20Experimental%20results%0Ademonstrate%20that%20the%20proposed%20method%20effectively%20improves%20medical%20image%0Aclassification%20performance%20in%20resource-limited%20settings%2C%20providing%20a%20feasible%0Aand%20efficient%20solution%20for%20the%20deployment%20and%20promotion%20of%20medical%20imaging%0Aanalysis%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Medical%2520Image%2520Classification%2520Method%2520Based%2520on%2520a%2520Lightweight%250A%2520%2520Improved%2520ConvNeXt-Tiny%2520Architecture%26entry.906535625%3DJingsong%2520Xia%2520and%2520Yue%2520Yin%2520and%2520Xiuhan%2520Li%26entry.1292438233%3D%2520%2520Intelligent%2520analysis%2520of%2520medical%2520imaging%2520plays%2520a%2520crucial%2520role%2520in%2520assisting%250Aclinical%2520diagnosis.%2520However%252C%2520achieving%2520efficient%2520and%2520high-accuracy%2520image%250Aclassification%2520in%2520resource-constrained%2520computational%2520environments%2520remains%250Achallenging.%2520This%2520study%2520proposes%2520a%2520medical%2520image%2520classification%2520method%2520based%2520on%250Aan%2520improved%2520ConvNeXt-Tiny%2520architecture.%2520Through%2520structural%2520optimization%2520and%250Aloss%2520function%2520design%252C%2520the%2520proposed%2520method%2520enhances%2520feature%2520extraction%250Acapability%2520and%2520classification%2520performance%2520while%2520reducing%2520computational%250Acomplexity.%2520Specifically%252C%2520the%2520method%2520introduces%2520a%2520dual%2520global%2520pooling%2520%2528Global%250AAverage%2520Pooling%2520and%2520Global%2520Max%2520Pooling%2529%2520feature%2520fusion%2520strategy%2520into%2520the%250AConvNeXt-Tiny%2520backbone%2520to%2520simultaneously%2520preserve%2520global%2520statistical%2520features%250Aand%2520salient%2520response%2520information.%2520A%2520lightweight%2520channel%2520attention%2520module%252C%250Atermed%2520Squeeze-and-Excitation%2520Vector%2520%2528SEVector%2529%252C%2520is%2520designed%2520to%2520improve%2520the%250Aadaptive%2520allocation%2520of%2520channel%2520weights%2520while%2520minimizing%2520parameter%2520overhead.%250AAdditionally%252C%2520a%2520Feature%2520Smoothing%2520Loss%2520is%2520incorporated%2520into%2520the%2520loss%2520function%250Ato%2520enhance%2520intra-class%2520feature%2520consistency%2520and%2520suppress%2520intra-class%2520variance.%250AUnder%2520CPU-only%2520conditions%2520%25288%2520threads%2529%252C%2520the%2520method%2520achieves%2520a%2520maximum%250Aclassification%2520accuracy%2520of%252089.10%2525%2520on%2520the%2520test%2520set%2520within%252010%2520training%2520epochs%252C%250Aexhibiting%2520a%2520stable%2520convergence%2520trend%2520in%2520loss%2520values.%2520Experimental%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520effectively%2520improves%2520medical%2520image%250Aclassification%2520performance%2520in%2520resource-limited%2520settings%252C%2520providing%2520a%2520feasible%250Aand%2520efficient%2520solution%2520for%2520the%2520deployment%2520and%2520promotion%2520of%2520medical%2520imaging%250Aanalysis%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Medical%20Image%20Classification%20Method%20Based%20on%20a%20Lightweight%0A%20%20Improved%20ConvNeXt-Tiny%20Architecture&entry.906535625=Jingsong%20Xia%20and%20Yue%20Yin%20and%20Xiuhan%20Li&entry.1292438233=%20%20Intelligent%20analysis%20of%20medical%20imaging%20plays%20a%20crucial%20role%20in%20assisting%0Aclinical%20diagnosis.%20However%2C%20achieving%20efficient%20and%20high-accuracy%20image%0Aclassification%20in%20resource-constrained%20computational%20environments%20remains%0Achallenging.%20This%20study%20proposes%20a%20medical%20image%20classification%20method%20based%20on%0Aan%20improved%20ConvNeXt-Tiny%20architecture.%20Through%20structural%20optimization%20and%0Aloss%20function%20design%2C%20the%20proposed%20method%20enhances%20feature%20extraction%0Acapability%20and%20classification%20performance%20while%20reducing%20computational%0Acomplexity.%20Specifically%2C%20the%20method%20introduces%20a%20dual%20global%20pooling%20%28Global%0AAverage%20Pooling%20and%20Global%20Max%20Pooling%29%20feature%20fusion%20strategy%20into%20the%0AConvNeXt-Tiny%20backbone%20to%20simultaneously%20preserve%20global%20statistical%20features%0Aand%20salient%20response%20information.%20A%20lightweight%20channel%20attention%20module%2C%0Atermed%20Squeeze-and-Excitation%20Vector%20%28SEVector%29%2C%20is%20designed%20to%20improve%20the%0Aadaptive%20allocation%20of%20channel%20weights%20while%20minimizing%20parameter%20overhead.%0AAdditionally%2C%20a%20Feature%20Smoothing%20Loss%20is%20incorporated%20into%20the%20loss%20function%0Ato%20enhance%20intra-class%20feature%20consistency%20and%20suppress%20intra-class%20variance.%0AUnder%20CPU-only%20conditions%20%288%20threads%29%2C%20the%20method%20achieves%20a%20maximum%0Aclassification%20accuracy%20of%2089.10%25%20on%20the%20test%20set%20within%2010%20training%20epochs%2C%0Aexhibiting%20a%20stable%20convergence%20trend%20in%20loss%20values.%20Experimental%20results%0Ademonstrate%20that%20the%20proposed%20method%20effectively%20improves%20medical%20image%0Aclassification%20performance%20in%20resource-limited%20settings%2C%20providing%20a%20feasible%0Aand%20efficient%20solution%20for%20the%20deployment%20and%20promotion%20of%20medical%20imaging%0Aanalysis%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11532v1&entry.124074799=Read"},
{"title": "A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature\n  based Graph-to-Hypergraph Structural Lifting", "author": "Michael Banf and Dominik Filipiak and Max Schattauer and Liliya Imasheva", "abstract": "  Graph Neural Networks are highly effective at learning from relational data,\nleveraging node and edge features while maintaining the symmetries inherent to\ngraph structures. However, many real-world systems, such as social or\nbiological networks, exhibit complex interactions that are more naturally\nrepresented by higher-order topological domains. The emerging field of\nGeometric and Topological Deep Learning addresses this challenge by introducing\nmethods that utilize and benefit from higher-order structures. Central to TDL\nis the concept of lifting, which transforms data representations from basic\ngraph forms to more expressive topologies before the application of GNN models\nfor learning. In this work, we propose a structural lifting strategy using\nForman-Ricci curvature, which defines an edge-based network characteristic\nbased on Riemannian geometry. Curvature reveals local and global properties of\na graph, such as a network's backbones, i.e. coarse, structure-preserving graph\ngeometries that form connections between major communities - most suitably\nrepresented as hyperedges to model information flows between clusters across\nlarge distances in the network. To this end, our approach provides a remedy to\nthe problem of information distortion in message passing across long distances\nand graph bottlenecks - a phenomenon known in graph learning as over-squashing.\n", "link": "http://arxiv.org/abs/2508.11390v1", "date": "2025-08-15", "relevancy": 2.0692, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.531}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5298}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Remedy%20for%20Over-Squashing%20in%20Graph%20Learning%20via%20Forman-Ricci%20Curvature%0A%20%20based%20Graph-to-Hypergraph%20Structural%20Lifting&body=Title%3A%20A%20Remedy%20for%20Over-Squashing%20in%20Graph%20Learning%20via%20Forman-Ricci%20Curvature%0A%20%20based%20Graph-to-Hypergraph%20Structural%20Lifting%0AAuthor%3A%20Michael%20Banf%20and%20Dominik%20Filipiak%20and%20Max%20Schattauer%20and%20Liliya%20Imasheva%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20are%20highly%20effective%20at%20learning%20from%20relational%20data%2C%0Aleveraging%20node%20and%20edge%20features%20while%20maintaining%20the%20symmetries%20inherent%20to%0Agraph%20structures.%20However%2C%20many%20real-world%20systems%2C%20such%20as%20social%20or%0Abiological%20networks%2C%20exhibit%20complex%20interactions%20that%20are%20more%20naturally%0Arepresented%20by%20higher-order%20topological%20domains.%20The%20emerging%20field%20of%0AGeometric%20and%20Topological%20Deep%20Learning%20addresses%20this%20challenge%20by%20introducing%0Amethods%20that%20utilize%20and%20benefit%20from%20higher-order%20structures.%20Central%20to%20TDL%0Ais%20the%20concept%20of%20lifting%2C%20which%20transforms%20data%20representations%20from%20basic%0Agraph%20forms%20to%20more%20expressive%20topologies%20before%20the%20application%20of%20GNN%20models%0Afor%20learning.%20In%20this%20work%2C%20we%20propose%20a%20structural%20lifting%20strategy%20using%0AForman-Ricci%20curvature%2C%20which%20defines%20an%20edge-based%20network%20characteristic%0Abased%20on%20Riemannian%20geometry.%20Curvature%20reveals%20local%20and%20global%20properties%20of%0Aa%20graph%2C%20such%20as%20a%20network%27s%20backbones%2C%20i.e.%20coarse%2C%20structure-preserving%20graph%0Ageometries%20that%20form%20connections%20between%20major%20communities%20-%20most%20suitably%0Arepresented%20as%20hyperedges%20to%20model%20information%20flows%20between%20clusters%20across%0Alarge%20distances%20in%20the%20network.%20To%20this%20end%2C%20our%20approach%20provides%20a%20remedy%20to%0Athe%20problem%20of%20information%20distortion%20in%20message%20passing%20across%20long%20distances%0Aand%20graph%20bottlenecks%20-%20a%20phenomenon%20known%20in%20graph%20learning%20as%20over-squashing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Remedy%2520for%2520Over-Squashing%2520in%2520Graph%2520Learning%2520via%2520Forman-Ricci%2520Curvature%250A%2520%2520based%2520Graph-to-Hypergraph%2520Structural%2520Lifting%26entry.906535625%3DMichael%2520Banf%2520and%2520Dominik%2520Filipiak%2520and%2520Max%2520Schattauer%2520and%2520Liliya%2520Imasheva%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520are%2520highly%2520effective%2520at%2520learning%2520from%2520relational%2520data%252C%250Aleveraging%2520node%2520and%2520edge%2520features%2520while%2520maintaining%2520the%2520symmetries%2520inherent%2520to%250Agraph%2520structures.%2520However%252C%2520many%2520real-world%2520systems%252C%2520such%2520as%2520social%2520or%250Abiological%2520networks%252C%2520exhibit%2520complex%2520interactions%2520that%2520are%2520more%2520naturally%250Arepresented%2520by%2520higher-order%2520topological%2520domains.%2520The%2520emerging%2520field%2520of%250AGeometric%2520and%2520Topological%2520Deep%2520Learning%2520addresses%2520this%2520challenge%2520by%2520introducing%250Amethods%2520that%2520utilize%2520and%2520benefit%2520from%2520higher-order%2520structures.%2520Central%2520to%2520TDL%250Ais%2520the%2520concept%2520of%2520lifting%252C%2520which%2520transforms%2520data%2520representations%2520from%2520basic%250Agraph%2520forms%2520to%2520more%2520expressive%2520topologies%2520before%2520the%2520application%2520of%2520GNN%2520models%250Afor%2520learning.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520structural%2520lifting%2520strategy%2520using%250AForman-Ricci%2520curvature%252C%2520which%2520defines%2520an%2520edge-based%2520network%2520characteristic%250Abased%2520on%2520Riemannian%2520geometry.%2520Curvature%2520reveals%2520local%2520and%2520global%2520properties%2520of%250Aa%2520graph%252C%2520such%2520as%2520a%2520network%2527s%2520backbones%252C%2520i.e.%2520coarse%252C%2520structure-preserving%2520graph%250Ageometries%2520that%2520form%2520connections%2520between%2520major%2520communities%2520-%2520most%2520suitably%250Arepresented%2520as%2520hyperedges%2520to%2520model%2520information%2520flows%2520between%2520clusters%2520across%250Alarge%2520distances%2520in%2520the%2520network.%2520To%2520this%2520end%252C%2520our%2520approach%2520provides%2520a%2520remedy%2520to%250Athe%2520problem%2520of%2520information%2520distortion%2520in%2520message%2520passing%2520across%2520long%2520distances%250Aand%2520graph%2520bottlenecks%2520-%2520a%2520phenomenon%2520known%2520in%2520graph%2520learning%2520as%2520over-squashing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Remedy%20for%20Over-Squashing%20in%20Graph%20Learning%20via%20Forman-Ricci%20Curvature%0A%20%20based%20Graph-to-Hypergraph%20Structural%20Lifting&entry.906535625=Michael%20Banf%20and%20Dominik%20Filipiak%20and%20Max%20Schattauer%20and%20Liliya%20Imasheva&entry.1292438233=%20%20Graph%20Neural%20Networks%20are%20highly%20effective%20at%20learning%20from%20relational%20data%2C%0Aleveraging%20node%20and%20edge%20features%20while%20maintaining%20the%20symmetries%20inherent%20to%0Agraph%20structures.%20However%2C%20many%20real-world%20systems%2C%20such%20as%20social%20or%0Abiological%20networks%2C%20exhibit%20complex%20interactions%20that%20are%20more%20naturally%0Arepresented%20by%20higher-order%20topological%20domains.%20The%20emerging%20field%20of%0AGeometric%20and%20Topological%20Deep%20Learning%20addresses%20this%20challenge%20by%20introducing%0Amethods%20that%20utilize%20and%20benefit%20from%20higher-order%20structures.%20Central%20to%20TDL%0Ais%20the%20concept%20of%20lifting%2C%20which%20transforms%20data%20representations%20from%20basic%0Agraph%20forms%20to%20more%20expressive%20topologies%20before%20the%20application%20of%20GNN%20models%0Afor%20learning.%20In%20this%20work%2C%20we%20propose%20a%20structural%20lifting%20strategy%20using%0AForman-Ricci%20curvature%2C%20which%20defines%20an%20edge-based%20network%20characteristic%0Abased%20on%20Riemannian%20geometry.%20Curvature%20reveals%20local%20and%20global%20properties%20of%0Aa%20graph%2C%20such%20as%20a%20network%27s%20backbones%2C%20i.e.%20coarse%2C%20structure-preserving%20graph%0Ageometries%20that%20form%20connections%20between%20major%20communities%20-%20most%20suitably%0Arepresented%20as%20hyperedges%20to%20model%20information%20flows%20between%20clusters%20across%0Alarge%20distances%20in%20the%20network.%20To%20this%20end%2C%20our%20approach%20provides%20a%20remedy%20to%0Athe%20problem%20of%20information%20distortion%20in%20message%20passing%20across%20long%20distances%0Aand%20graph%20bottlenecks%20-%20a%20phenomenon%20known%20in%20graph%20learning%20as%20over-squashing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11390v1&entry.124074799=Read"},
{"title": "Tapping into the Black Box: Uncovering Aligned Representations in\n  Pretrained Neural Networks", "author": "Maciej Satkiewicz", "abstract": "  In ReLU networks, gradients of output units can be seen as their input-level\nrepresentations, as they correspond to the units' pullbacks through the active\nsubnetwork. However, gradients of deeper models are notoriously misaligned,\nsignificantly contributing to their black-box nature. We claim that this is\nbecause active subnetworks are inherently noisy due to the ReLU hard-gating. To\ntackle that noise, we propose soft-gating in the backward pass only. The\nresulting input-level vector field (called ''excitation pullback'') exhibits\nremarkable perceptual alignment, revealing high-resolution input- and\ntarget-specific features that ''just make sense'', therefore establishing a\ncompelling novel explanation method. Furthermore, we speculate that excitation\npullbacks approximate (directionally) the gradients of a simpler model, linear\nin the network's path space, learned implicitly during optimization and largely\ndetermining the network's decision; thus arguing for the faithfulness of the\nproduced explanations and their overall significance.\n", "link": "http://arxiv.org/abs/2507.22832v2", "date": "2025-08-15", "relevancy": 2.0682, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5444}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5209}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tapping%20into%20the%20Black%20Box%3A%20Uncovering%20Aligned%20Representations%20in%0A%20%20Pretrained%20Neural%20Networks&body=Title%3A%20Tapping%20into%20the%20Black%20Box%3A%20Uncovering%20Aligned%20Representations%20in%0A%20%20Pretrained%20Neural%20Networks%0AAuthor%3A%20Maciej%20Satkiewicz%0AAbstract%3A%20%20%20In%20ReLU%20networks%2C%20gradients%20of%20output%20units%20can%20be%20seen%20as%20their%20input-level%0Arepresentations%2C%20as%20they%20correspond%20to%20the%20units%27%20pullbacks%20through%20the%20active%0Asubnetwork.%20However%2C%20gradients%20of%20deeper%20models%20are%20notoriously%20misaligned%2C%0Asignificantly%20contributing%20to%20their%20black-box%20nature.%20We%20claim%20that%20this%20is%0Abecause%20active%20subnetworks%20are%20inherently%20noisy%20due%20to%20the%20ReLU%20hard-gating.%20To%0Atackle%20that%20noise%2C%20we%20propose%20soft-gating%20in%20the%20backward%20pass%20only.%20The%0Aresulting%20input-level%20vector%20field%20%28called%20%27%27excitation%20pullback%27%27%29%20exhibits%0Aremarkable%20perceptual%20alignment%2C%20revealing%20high-resolution%20input-%20and%0Atarget-specific%20features%20that%20%27%27just%20make%20sense%27%27%2C%20therefore%20establishing%20a%0Acompelling%20novel%20explanation%20method.%20Furthermore%2C%20we%20speculate%20that%20excitation%0Apullbacks%20approximate%20%28directionally%29%20the%20gradients%20of%20a%20simpler%20model%2C%20linear%0Ain%20the%20network%27s%20path%20space%2C%20learned%20implicitly%20during%20optimization%20and%20largely%0Adetermining%20the%20network%27s%20decision%3B%20thus%20arguing%20for%20the%20faithfulness%20of%20the%0Aproduced%20explanations%20and%20their%20overall%20significance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22832v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTapping%2520into%2520the%2520Black%2520Box%253A%2520Uncovering%2520Aligned%2520Representations%2520in%250A%2520%2520Pretrained%2520Neural%2520Networks%26entry.906535625%3DMaciej%2520Satkiewicz%26entry.1292438233%3D%2520%2520In%2520ReLU%2520networks%252C%2520gradients%2520of%2520output%2520units%2520can%2520be%2520seen%2520as%2520their%2520input-level%250Arepresentations%252C%2520as%2520they%2520correspond%2520to%2520the%2520units%2527%2520pullbacks%2520through%2520the%2520active%250Asubnetwork.%2520However%252C%2520gradients%2520of%2520deeper%2520models%2520are%2520notoriously%2520misaligned%252C%250Asignificantly%2520contributing%2520to%2520their%2520black-box%2520nature.%2520We%2520claim%2520that%2520this%2520is%250Abecause%2520active%2520subnetworks%2520are%2520inherently%2520noisy%2520due%2520to%2520the%2520ReLU%2520hard-gating.%2520To%250Atackle%2520that%2520noise%252C%2520we%2520propose%2520soft-gating%2520in%2520the%2520backward%2520pass%2520only.%2520The%250Aresulting%2520input-level%2520vector%2520field%2520%2528called%2520%2527%2527excitation%2520pullback%2527%2527%2529%2520exhibits%250Aremarkable%2520perceptual%2520alignment%252C%2520revealing%2520high-resolution%2520input-%2520and%250Atarget-specific%2520features%2520that%2520%2527%2527just%2520make%2520sense%2527%2527%252C%2520therefore%2520establishing%2520a%250Acompelling%2520novel%2520explanation%2520method.%2520Furthermore%252C%2520we%2520speculate%2520that%2520excitation%250Apullbacks%2520approximate%2520%2528directionally%2529%2520the%2520gradients%2520of%2520a%2520simpler%2520model%252C%2520linear%250Ain%2520the%2520network%2527s%2520path%2520space%252C%2520learned%2520implicitly%2520during%2520optimization%2520and%2520largely%250Adetermining%2520the%2520network%2527s%2520decision%253B%2520thus%2520arguing%2520for%2520the%2520faithfulness%2520of%2520the%250Aproduced%2520explanations%2520and%2520their%2520overall%2520significance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22832v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tapping%20into%20the%20Black%20Box%3A%20Uncovering%20Aligned%20Representations%20in%0A%20%20Pretrained%20Neural%20Networks&entry.906535625=Maciej%20Satkiewicz&entry.1292438233=%20%20In%20ReLU%20networks%2C%20gradients%20of%20output%20units%20can%20be%20seen%20as%20their%20input-level%0Arepresentations%2C%20as%20they%20correspond%20to%20the%20units%27%20pullbacks%20through%20the%20active%0Asubnetwork.%20However%2C%20gradients%20of%20deeper%20models%20are%20notoriously%20misaligned%2C%0Asignificantly%20contributing%20to%20their%20black-box%20nature.%20We%20claim%20that%20this%20is%0Abecause%20active%20subnetworks%20are%20inherently%20noisy%20due%20to%20the%20ReLU%20hard-gating.%20To%0Atackle%20that%20noise%2C%20we%20propose%20soft-gating%20in%20the%20backward%20pass%20only.%20The%0Aresulting%20input-level%20vector%20field%20%28called%20%27%27excitation%20pullback%27%27%29%20exhibits%0Aremarkable%20perceptual%20alignment%2C%20revealing%20high-resolution%20input-%20and%0Atarget-specific%20features%20that%20%27%27just%20make%20sense%27%27%2C%20therefore%20establishing%20a%0Acompelling%20novel%20explanation%20method.%20Furthermore%2C%20we%20speculate%20that%20excitation%0Apullbacks%20approximate%20%28directionally%29%20the%20gradients%20of%20a%20simpler%20model%2C%20linear%0Ain%20the%20network%27s%20path%20space%2C%20learned%20implicitly%20during%20optimization%20and%20largely%0Adetermining%20the%20network%27s%20decision%3B%20thus%20arguing%20for%20the%20faithfulness%20of%20the%0Aproduced%20explanations%20and%20their%20overall%20significance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22832v2&entry.124074799=Read"},
{"title": "Relative Position Matters: Trajectory Prediction and Planning with Polar\n  Representation", "author": "Bozhou Zhang and Nan Song and Bingzhao Gao and Li Zhang", "abstract": "  Trajectory prediction and planning in autonomous driving are highly\nchallenging due to the complexity of predicting surrounding agents' movements\nand planning the ego agent's actions in dynamic environments. Existing methods\nencode map and agent positions and decode future trajectories in Cartesian\ncoordinates. However, modeling the relationships between the ego vehicle and\nsurrounding traffic elements in Cartesian space can be suboptimal, as it does\nnot naturally capture the varying influence of different elements based on\ntheir relative distances and directions. To address this limitation, we adopt\nthe Polar coordinate system, where positions are represented by radius and\nangle. This representation provides a more intuitive and effective way to model\nspatial changes and relative relationships, especially in terms of distance and\ndirectional influence. Based on this insight, we propose Polaris, a novel\nmethod that operates entirely in Polar coordinates, distinguishing itself from\nconventional Cartesian-based approaches. By leveraging the Polar\nrepresentation, this method explicitly models distance and direction variations\nand captures relative relationships through dedicated encoding and refinement\nmodules, enabling more structured and spatially aware trajectory prediction and\nplanning. Extensive experiments on the challenging prediction (Argoverse 2) and\nplanning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2508.11492v1", "date": "2025-08-15", "relevancy": 2.063, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5451}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5322}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relative%20Position%20Matters%3A%20Trajectory%20Prediction%20and%20Planning%20with%20Polar%0A%20%20Representation&body=Title%3A%20Relative%20Position%20Matters%3A%20Trajectory%20Prediction%20and%20Planning%20with%20Polar%0A%20%20Representation%0AAuthor%3A%20Bozhou%20Zhang%20and%20Nan%20Song%20and%20Bingzhao%20Gao%20and%20Li%20Zhang%0AAbstract%3A%20%20%20Trajectory%20prediction%20and%20planning%20in%20autonomous%20driving%20are%20highly%0Achallenging%20due%20to%20the%20complexity%20of%20predicting%20surrounding%20agents%27%20movements%0Aand%20planning%20the%20ego%20agent%27s%20actions%20in%20dynamic%20environments.%20Existing%20methods%0Aencode%20map%20and%20agent%20positions%20and%20decode%20future%20trajectories%20in%20Cartesian%0Acoordinates.%20However%2C%20modeling%20the%20relationships%20between%20the%20ego%20vehicle%20and%0Asurrounding%20traffic%20elements%20in%20Cartesian%20space%20can%20be%20suboptimal%2C%20as%20it%20does%0Anot%20naturally%20capture%20the%20varying%20influence%20of%20different%20elements%20based%20on%0Atheir%20relative%20distances%20and%20directions.%20To%20address%20this%20limitation%2C%20we%20adopt%0Athe%20Polar%20coordinate%20system%2C%20where%20positions%20are%20represented%20by%20radius%20and%0Aangle.%20This%20representation%20provides%20a%20more%20intuitive%20and%20effective%20way%20to%20model%0Aspatial%20changes%20and%20relative%20relationships%2C%20especially%20in%20terms%20of%20distance%20and%0Adirectional%20influence.%20Based%20on%20this%20insight%2C%20we%20propose%20Polaris%2C%20a%20novel%0Amethod%20that%20operates%20entirely%20in%20Polar%20coordinates%2C%20distinguishing%20itself%20from%0Aconventional%20Cartesian-based%20approaches.%20By%20leveraging%20the%20Polar%0Arepresentation%2C%20this%20method%20explicitly%20models%20distance%20and%20direction%20variations%0Aand%20captures%20relative%20relationships%20through%20dedicated%20encoding%20and%20refinement%0Amodules%2C%20enabling%20more%20structured%20and%20spatially%20aware%20trajectory%20prediction%20and%0Aplanning.%20Extensive%20experiments%20on%20the%20challenging%20prediction%20%28Argoverse%202%29%20and%0Aplanning%20benchmarks%20%28nuPlan%29%20demonstrate%20that%20Polaris%20achieves%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelative%2520Position%2520Matters%253A%2520Trajectory%2520Prediction%2520and%2520Planning%2520with%2520Polar%250A%2520%2520Representation%26entry.906535625%3DBozhou%2520Zhang%2520and%2520Nan%2520Song%2520and%2520Bingzhao%2520Gao%2520and%2520Li%2520Zhang%26entry.1292438233%3D%2520%2520Trajectory%2520prediction%2520and%2520planning%2520in%2520autonomous%2520driving%2520are%2520highly%250Achallenging%2520due%2520to%2520the%2520complexity%2520of%2520predicting%2520surrounding%2520agents%2527%2520movements%250Aand%2520planning%2520the%2520ego%2520agent%2527s%2520actions%2520in%2520dynamic%2520environments.%2520Existing%2520methods%250Aencode%2520map%2520and%2520agent%2520positions%2520and%2520decode%2520future%2520trajectories%2520in%2520Cartesian%250Acoordinates.%2520However%252C%2520modeling%2520the%2520relationships%2520between%2520the%2520ego%2520vehicle%2520and%250Asurrounding%2520traffic%2520elements%2520in%2520Cartesian%2520space%2520can%2520be%2520suboptimal%252C%2520as%2520it%2520does%250Anot%2520naturally%2520capture%2520the%2520varying%2520influence%2520of%2520different%2520elements%2520based%2520on%250Atheir%2520relative%2520distances%2520and%2520directions.%2520To%2520address%2520this%2520limitation%252C%2520we%2520adopt%250Athe%2520Polar%2520coordinate%2520system%252C%2520where%2520positions%2520are%2520represented%2520by%2520radius%2520and%250Aangle.%2520This%2520representation%2520provides%2520a%2520more%2520intuitive%2520and%2520effective%2520way%2520to%2520model%250Aspatial%2520changes%2520and%2520relative%2520relationships%252C%2520especially%2520in%2520terms%2520of%2520distance%2520and%250Adirectional%2520influence.%2520Based%2520on%2520this%2520insight%252C%2520we%2520propose%2520Polaris%252C%2520a%2520novel%250Amethod%2520that%2520operates%2520entirely%2520in%2520Polar%2520coordinates%252C%2520distinguishing%2520itself%2520from%250Aconventional%2520Cartesian-based%2520approaches.%2520By%2520leveraging%2520the%2520Polar%250Arepresentation%252C%2520this%2520method%2520explicitly%2520models%2520distance%2520and%2520direction%2520variations%250Aand%2520captures%2520relative%2520relationships%2520through%2520dedicated%2520encoding%2520and%2520refinement%250Amodules%252C%2520enabling%2520more%2520structured%2520and%2520spatially%2520aware%2520trajectory%2520prediction%2520and%250Aplanning.%2520Extensive%2520experiments%2520on%2520the%2520challenging%2520prediction%2520%2528Argoverse%25202%2529%2520and%250Aplanning%2520benchmarks%2520%2528nuPlan%2529%2520demonstrate%2520that%2520Polaris%2520achieves%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relative%20Position%20Matters%3A%20Trajectory%20Prediction%20and%20Planning%20with%20Polar%0A%20%20Representation&entry.906535625=Bozhou%20Zhang%20and%20Nan%20Song%20and%20Bingzhao%20Gao%20and%20Li%20Zhang&entry.1292438233=%20%20Trajectory%20prediction%20and%20planning%20in%20autonomous%20driving%20are%20highly%0Achallenging%20due%20to%20the%20complexity%20of%20predicting%20surrounding%20agents%27%20movements%0Aand%20planning%20the%20ego%20agent%27s%20actions%20in%20dynamic%20environments.%20Existing%20methods%0Aencode%20map%20and%20agent%20positions%20and%20decode%20future%20trajectories%20in%20Cartesian%0Acoordinates.%20However%2C%20modeling%20the%20relationships%20between%20the%20ego%20vehicle%20and%0Asurrounding%20traffic%20elements%20in%20Cartesian%20space%20can%20be%20suboptimal%2C%20as%20it%20does%0Anot%20naturally%20capture%20the%20varying%20influence%20of%20different%20elements%20based%20on%0Atheir%20relative%20distances%20and%20directions.%20To%20address%20this%20limitation%2C%20we%20adopt%0Athe%20Polar%20coordinate%20system%2C%20where%20positions%20are%20represented%20by%20radius%20and%0Aangle.%20This%20representation%20provides%20a%20more%20intuitive%20and%20effective%20way%20to%20model%0Aspatial%20changes%20and%20relative%20relationships%2C%20especially%20in%20terms%20of%20distance%20and%0Adirectional%20influence.%20Based%20on%20this%20insight%2C%20we%20propose%20Polaris%2C%20a%20novel%0Amethod%20that%20operates%20entirely%20in%20Polar%20coordinates%2C%20distinguishing%20itself%20from%0Aconventional%20Cartesian-based%20approaches.%20By%20leveraging%20the%20Polar%0Arepresentation%2C%20this%20method%20explicitly%20models%20distance%20and%20direction%20variations%0Aand%20captures%20relative%20relationships%20through%20dedicated%20encoding%20and%20refinement%0Amodules%2C%20enabling%20more%20structured%20and%20spatially%20aware%20trajectory%20prediction%20and%0Aplanning.%20Extensive%20experiments%20on%20the%20challenging%20prediction%20%28Argoverse%202%29%20and%0Aplanning%20benchmarks%20%28nuPlan%29%20demonstrate%20that%20Polaris%20achieves%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11492v1&entry.124074799=Read"},
{"title": "Robust Convolution Neural ODEs via Contractivity-promoting\n  regularization", "author": "Muhammad Zakwan and Liang Xu and Giancarlo Ferrari-Trecate", "abstract": "  Neural networks can be fragile to input noise and adversarial attacks.\n  In this work, we consider Convolutional Neural Ordinary Differential\nEquations (NODEs), a family of continuous-depth neural networks represented by\ndynamical systems, and propose to use contraction theory to improve their\nrobustness.\n  For a contractive dynamical system two trajectories starting from different\ninitial conditions converge to each other exponentially fast.\n  Contractive Convolutional NODEs can enjoy increased robustness as slight\nperturbations of the features do not cause a significant change in the output.\n  Contractivity can be induced during training by using a regularization term\ninvolving the Jacobian of the system dynamics.\n  To reduce the computational burden, we show that it can also be promoted\nusing carefully selected weight regularization terms for a class of NODEs with\nslope-restricted activation functions.\n  The performance of the proposed regularizers is illustrated through benchmark\nimage classification tasks on MNIST and FashionMNIST datasets, where images are\ncorrupted by different kinds of noise and attacks.\n", "link": "http://arxiv.org/abs/2508.11432v1", "date": "2025-08-15", "relevancy": 2.0579, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5437}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4947}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Convolution%20Neural%20ODEs%20via%20Contractivity-promoting%0A%20%20regularization&body=Title%3A%20Robust%20Convolution%20Neural%20ODEs%20via%20Contractivity-promoting%0A%20%20regularization%0AAuthor%3A%20Muhammad%20Zakwan%20and%20Liang%20Xu%20and%20Giancarlo%20Ferrari-Trecate%0AAbstract%3A%20%20%20Neural%20networks%20can%20be%20fragile%20to%20input%20noise%20and%20adversarial%20attacks.%0A%20%20In%20this%20work%2C%20we%20consider%20Convolutional%20Neural%20Ordinary%20Differential%0AEquations%20%28NODEs%29%2C%20a%20family%20of%20continuous-depth%20neural%20networks%20represented%20by%0Adynamical%20systems%2C%20and%20propose%20to%20use%20contraction%20theory%20to%20improve%20their%0Arobustness.%0A%20%20For%20a%20contractive%20dynamical%20system%20two%20trajectories%20starting%20from%20different%0Ainitial%20conditions%20converge%20to%20each%20other%20exponentially%20fast.%0A%20%20Contractive%20Convolutional%20NODEs%20can%20enjoy%20increased%20robustness%20as%20slight%0Aperturbations%20of%20the%20features%20do%20not%20cause%20a%20significant%20change%20in%20the%20output.%0A%20%20Contractivity%20can%20be%20induced%20during%20training%20by%20using%20a%20regularization%20term%0Ainvolving%20the%20Jacobian%20of%20the%20system%20dynamics.%0A%20%20To%20reduce%20the%20computational%20burden%2C%20we%20show%20that%20it%20can%20also%20be%20promoted%0Ausing%20carefully%20selected%20weight%20regularization%20terms%20for%20a%20class%20of%20NODEs%20with%0Aslope-restricted%20activation%20functions.%0A%20%20The%20performance%20of%20the%20proposed%20regularizers%20is%20illustrated%20through%20benchmark%0Aimage%20classification%20tasks%20on%20MNIST%20and%20FashionMNIST%20datasets%2C%20where%20images%20are%0Acorrupted%20by%20different%20kinds%20of%20noise%20and%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Convolution%2520Neural%2520ODEs%2520via%2520Contractivity-promoting%250A%2520%2520regularization%26entry.906535625%3DMuhammad%2520Zakwan%2520and%2520Liang%2520Xu%2520and%2520Giancarlo%2520Ferrari-Trecate%26entry.1292438233%3D%2520%2520Neural%2520networks%2520can%2520be%2520fragile%2520to%2520input%2520noise%2520and%2520adversarial%2520attacks.%250A%2520%2520In%2520this%2520work%252C%2520we%2520consider%2520Convolutional%2520Neural%2520Ordinary%2520Differential%250AEquations%2520%2528NODEs%2529%252C%2520a%2520family%2520of%2520continuous-depth%2520neural%2520networks%2520represented%2520by%250Adynamical%2520systems%252C%2520and%2520propose%2520to%2520use%2520contraction%2520theory%2520to%2520improve%2520their%250Arobustness.%250A%2520%2520For%2520a%2520contractive%2520dynamical%2520system%2520two%2520trajectories%2520starting%2520from%2520different%250Ainitial%2520conditions%2520converge%2520to%2520each%2520other%2520exponentially%2520fast.%250A%2520%2520Contractive%2520Convolutional%2520NODEs%2520can%2520enjoy%2520increased%2520robustness%2520as%2520slight%250Aperturbations%2520of%2520the%2520features%2520do%2520not%2520cause%2520a%2520significant%2520change%2520in%2520the%2520output.%250A%2520%2520Contractivity%2520can%2520be%2520induced%2520during%2520training%2520by%2520using%2520a%2520regularization%2520term%250Ainvolving%2520the%2520Jacobian%2520of%2520the%2520system%2520dynamics.%250A%2520%2520To%2520reduce%2520the%2520computational%2520burden%252C%2520we%2520show%2520that%2520it%2520can%2520also%2520be%2520promoted%250Ausing%2520carefully%2520selected%2520weight%2520regularization%2520terms%2520for%2520a%2520class%2520of%2520NODEs%2520with%250Aslope-restricted%2520activation%2520functions.%250A%2520%2520The%2520performance%2520of%2520the%2520proposed%2520regularizers%2520is%2520illustrated%2520through%2520benchmark%250Aimage%2520classification%2520tasks%2520on%2520MNIST%2520and%2520FashionMNIST%2520datasets%252C%2520where%2520images%2520are%250Acorrupted%2520by%2520different%2520kinds%2520of%2520noise%2520and%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Convolution%20Neural%20ODEs%20via%20Contractivity-promoting%0A%20%20regularization&entry.906535625=Muhammad%20Zakwan%20and%20Liang%20Xu%20and%20Giancarlo%20Ferrari-Trecate&entry.1292438233=%20%20Neural%20networks%20can%20be%20fragile%20to%20input%20noise%20and%20adversarial%20attacks.%0A%20%20In%20this%20work%2C%20we%20consider%20Convolutional%20Neural%20Ordinary%20Differential%0AEquations%20%28NODEs%29%2C%20a%20family%20of%20continuous-depth%20neural%20networks%20represented%20by%0Adynamical%20systems%2C%20and%20propose%20to%20use%20contraction%20theory%20to%20improve%20their%0Arobustness.%0A%20%20For%20a%20contractive%20dynamical%20system%20two%20trajectories%20starting%20from%20different%0Ainitial%20conditions%20converge%20to%20each%20other%20exponentially%20fast.%0A%20%20Contractive%20Convolutional%20NODEs%20can%20enjoy%20increased%20robustness%20as%20slight%0Aperturbations%20of%20the%20features%20do%20not%20cause%20a%20significant%20change%20in%20the%20output.%0A%20%20Contractivity%20can%20be%20induced%20during%20training%20by%20using%20a%20regularization%20term%0Ainvolving%20the%20Jacobian%20of%20the%20system%20dynamics.%0A%20%20To%20reduce%20the%20computational%20burden%2C%20we%20show%20that%20it%20can%20also%20be%20promoted%0Ausing%20carefully%20selected%20weight%20regularization%20terms%20for%20a%20class%20of%20NODEs%20with%0Aslope-restricted%20activation%20functions.%0A%20%20The%20performance%20of%20the%20proposed%20regularizers%20is%20illustrated%20through%20benchmark%0Aimage%20classification%20tasks%20on%20MNIST%20and%20FashionMNIST%20datasets%2C%20where%20images%20are%0Acorrupted%20by%20different%20kinds%20of%20noise%20and%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11432v1&entry.124074799=Read"},
{"title": "PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for\n  Cross-Subject EEG Decoding", "author": "Changhong Jing and Yan Liu and Shuqiang Wang and Bruce X. B. Yu and Gong Chen and Zhejing Hu and Zhi Zhang and Yanyan Shen", "abstract": "  Cross-subject electroencephalography (EEG) decoding remains a fundamental\nchallenge in brain-computer interface (BCI) research due to substantial\ninter-subject variability and the scarcity of subject-invariant\nrepresentations. This paper proposed PTSM (Physiology-aware and Task-invariant\nSpatio-temporal Modeling), a novel framework for interpretable and robust EEG\ndecoding across unseen subjects. PTSM employs a dual-branch masking mechanism\nthat independently learns personalized and shared spatio-temporal patterns,\nenabling the model to preserve individual-specific neural characteristics while\nextracting task-relevant, population-shared features. The masks are factorized\nacross temporal and spatial dimensions, allowing fine-grained modulation of\ndynamic EEG patterns with low computational overhead. To further address\nrepresentational entanglement, PTSM enforces information-theoretic constraints\nthat decompose latent embeddings into orthogonal task-related and\nsubject-related subspaces. The model is trained end-to-end via a\nmulti-objective loss integrating classification, contrastive, and\ndisentanglement objectives. Extensive experiments on cross-subject motor\nimagery datasets demonstrate that PTSM achieves strong zero-shot\ngeneralization, outperforming state-of-the-art baselines without\nsubject-specific calibration. Results highlight the efficacy of disentangled\nneural representations for achieving both personalized and transferable\ndecoding in non-stationary neurophysiological settings.\n", "link": "http://arxiv.org/abs/2508.11357v1", "date": "2025-08-15", "relevancy": 2.0555, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5186}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5161}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PTSM%3A%20Physiology-aware%20and%20Task-invariant%20Spatio-temporal%20Modeling%20for%0A%20%20Cross-Subject%20EEG%20Decoding&body=Title%3A%20PTSM%3A%20Physiology-aware%20and%20Task-invariant%20Spatio-temporal%20Modeling%20for%0A%20%20Cross-Subject%20EEG%20Decoding%0AAuthor%3A%20Changhong%20Jing%20and%20Yan%20Liu%20and%20Shuqiang%20Wang%20and%20Bruce%20X.%20B.%20Yu%20and%20Gong%20Chen%20and%20Zhejing%20Hu%20and%20Zhi%20Zhang%20and%20Yanyan%20Shen%0AAbstract%3A%20%20%20Cross-subject%20electroencephalography%20%28EEG%29%20decoding%20remains%20a%20fundamental%0Achallenge%20in%20brain-computer%20interface%20%28BCI%29%20research%20due%20to%20substantial%0Ainter-subject%20variability%20and%20the%20scarcity%20of%20subject-invariant%0Arepresentations.%20This%20paper%20proposed%20PTSM%20%28Physiology-aware%20and%20Task-invariant%0ASpatio-temporal%20Modeling%29%2C%20a%20novel%20framework%20for%20interpretable%20and%20robust%20EEG%0Adecoding%20across%20unseen%20subjects.%20PTSM%20employs%20a%20dual-branch%20masking%20mechanism%0Athat%20independently%20learns%20personalized%20and%20shared%20spatio-temporal%20patterns%2C%0Aenabling%20the%20model%20to%20preserve%20individual-specific%20neural%20characteristics%20while%0Aextracting%20task-relevant%2C%20population-shared%20features.%20The%20masks%20are%20factorized%0Aacross%20temporal%20and%20spatial%20dimensions%2C%20allowing%20fine-grained%20modulation%20of%0Adynamic%20EEG%20patterns%20with%20low%20computational%20overhead.%20To%20further%20address%0Arepresentational%20entanglement%2C%20PTSM%20enforces%20information-theoretic%20constraints%0Athat%20decompose%20latent%20embeddings%20into%20orthogonal%20task-related%20and%0Asubject-related%20subspaces.%20The%20model%20is%20trained%20end-to-end%20via%20a%0Amulti-objective%20loss%20integrating%20classification%2C%20contrastive%2C%20and%0Adisentanglement%20objectives.%20Extensive%20experiments%20on%20cross-subject%20motor%0Aimagery%20datasets%20demonstrate%20that%20PTSM%20achieves%20strong%20zero-shot%0Ageneralization%2C%20outperforming%20state-of-the-art%20baselines%20without%0Asubject-specific%20calibration.%20Results%20highlight%20the%20efficacy%20of%20disentangled%0Aneural%20representations%20for%20achieving%20both%20personalized%20and%20transferable%0Adecoding%20in%20non-stationary%20neurophysiological%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPTSM%253A%2520Physiology-aware%2520and%2520Task-invariant%2520Spatio-temporal%2520Modeling%2520for%250A%2520%2520Cross-Subject%2520EEG%2520Decoding%26entry.906535625%3DChanghong%2520Jing%2520and%2520Yan%2520Liu%2520and%2520Shuqiang%2520Wang%2520and%2520Bruce%2520X.%2520B.%2520Yu%2520and%2520Gong%2520Chen%2520and%2520Zhejing%2520Hu%2520and%2520Zhi%2520Zhang%2520and%2520Yanyan%2520Shen%26entry.1292438233%3D%2520%2520Cross-subject%2520electroencephalography%2520%2528EEG%2529%2520decoding%2520remains%2520a%2520fundamental%250Achallenge%2520in%2520brain-computer%2520interface%2520%2528BCI%2529%2520research%2520due%2520to%2520substantial%250Ainter-subject%2520variability%2520and%2520the%2520scarcity%2520of%2520subject-invariant%250Arepresentations.%2520This%2520paper%2520proposed%2520PTSM%2520%2528Physiology-aware%2520and%2520Task-invariant%250ASpatio-temporal%2520Modeling%2529%252C%2520a%2520novel%2520framework%2520for%2520interpretable%2520and%2520robust%2520EEG%250Adecoding%2520across%2520unseen%2520subjects.%2520PTSM%2520employs%2520a%2520dual-branch%2520masking%2520mechanism%250Athat%2520independently%2520learns%2520personalized%2520and%2520shared%2520spatio-temporal%2520patterns%252C%250Aenabling%2520the%2520model%2520to%2520preserve%2520individual-specific%2520neural%2520characteristics%2520while%250Aextracting%2520task-relevant%252C%2520population-shared%2520features.%2520The%2520masks%2520are%2520factorized%250Aacross%2520temporal%2520and%2520spatial%2520dimensions%252C%2520allowing%2520fine-grained%2520modulation%2520of%250Adynamic%2520EEG%2520patterns%2520with%2520low%2520computational%2520overhead.%2520To%2520further%2520address%250Arepresentational%2520entanglement%252C%2520PTSM%2520enforces%2520information-theoretic%2520constraints%250Athat%2520decompose%2520latent%2520embeddings%2520into%2520orthogonal%2520task-related%2520and%250Asubject-related%2520subspaces.%2520The%2520model%2520is%2520trained%2520end-to-end%2520via%2520a%250Amulti-objective%2520loss%2520integrating%2520classification%252C%2520contrastive%252C%2520and%250Adisentanglement%2520objectives.%2520Extensive%2520experiments%2520on%2520cross-subject%2520motor%250Aimagery%2520datasets%2520demonstrate%2520that%2520PTSM%2520achieves%2520strong%2520zero-shot%250Ageneralization%252C%2520outperforming%2520state-of-the-art%2520baselines%2520without%250Asubject-specific%2520calibration.%2520Results%2520highlight%2520the%2520efficacy%2520of%2520disentangled%250Aneural%2520representations%2520for%2520achieving%2520both%2520personalized%2520and%2520transferable%250Adecoding%2520in%2520non-stationary%2520neurophysiological%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PTSM%3A%20Physiology-aware%20and%20Task-invariant%20Spatio-temporal%20Modeling%20for%0A%20%20Cross-Subject%20EEG%20Decoding&entry.906535625=Changhong%20Jing%20and%20Yan%20Liu%20and%20Shuqiang%20Wang%20and%20Bruce%20X.%20B.%20Yu%20and%20Gong%20Chen%20and%20Zhejing%20Hu%20and%20Zhi%20Zhang%20and%20Yanyan%20Shen&entry.1292438233=%20%20Cross-subject%20electroencephalography%20%28EEG%29%20decoding%20remains%20a%20fundamental%0Achallenge%20in%20brain-computer%20interface%20%28BCI%29%20research%20due%20to%20substantial%0Ainter-subject%20variability%20and%20the%20scarcity%20of%20subject-invariant%0Arepresentations.%20This%20paper%20proposed%20PTSM%20%28Physiology-aware%20and%20Task-invariant%0ASpatio-temporal%20Modeling%29%2C%20a%20novel%20framework%20for%20interpretable%20and%20robust%20EEG%0Adecoding%20across%20unseen%20subjects.%20PTSM%20employs%20a%20dual-branch%20masking%20mechanism%0Athat%20independently%20learns%20personalized%20and%20shared%20spatio-temporal%20patterns%2C%0Aenabling%20the%20model%20to%20preserve%20individual-specific%20neural%20characteristics%20while%0Aextracting%20task-relevant%2C%20population-shared%20features.%20The%20masks%20are%20factorized%0Aacross%20temporal%20and%20spatial%20dimensions%2C%20allowing%20fine-grained%20modulation%20of%0Adynamic%20EEG%20patterns%20with%20low%20computational%20overhead.%20To%20further%20address%0Arepresentational%20entanglement%2C%20PTSM%20enforces%20information-theoretic%20constraints%0Athat%20decompose%20latent%20embeddings%20into%20orthogonal%20task-related%20and%0Asubject-related%20subspaces.%20The%20model%20is%20trained%20end-to-end%20via%20a%0Amulti-objective%20loss%20integrating%20classification%2C%20contrastive%2C%20and%0Adisentanglement%20objectives.%20Extensive%20experiments%20on%20cross-subject%20motor%0Aimagery%20datasets%20demonstrate%20that%20PTSM%20achieves%20strong%20zero-shot%0Ageneralization%2C%20outperforming%20state-of-the-art%20baselines%20without%0Asubject-specific%20calibration.%20Results%20highlight%20the%20efficacy%20of%20disentangled%0Aneural%20representations%20for%20achieving%20both%20personalized%20and%20transferable%0Adecoding%20in%20non-stationary%20neurophysiological%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11357v1&entry.124074799=Read"},
{"title": "Sketch Decompositions for Classical Planning via Deep Reinforcement\n  Learning", "author": "Michael Aichm\u00fcller and Hector Geffner", "abstract": "  In planning and reinforcement learning, the identification of common subgoal\nstructures across problems is important when goals are to be achieved over long\nhorizons. Recently, it has been shown that such structures can be expressed as\nfeature-based rules, called sketches, over a number of classical planning\ndomains. These sketches split problems into subproblems which then become\nsolvable in low polynomial time by a greedy sequence of IW$(k)$ searches.\nMethods for learning sketches using feature pools and min-SAT solvers have been\ndeveloped, yet they face two key limitations: scalability and expressivity. In\nthis work, we address these limitations by formulating the problem of learning\nsketch decompositions as a deep reinforcement learning (DRL) task, where\ngeneral policies are sought in a modified planning problem where the successor\nstates of a state s are defined as those reachable from s through an IW$(k)$\nsearch. The sketch decompositions obtained through this method are\nexperimentally evaluated across various domains, and problems are regarded as\nsolved by the decomposition when the goal is reached through a greedy sequence\nof IW$(k)$ searches. While our DRL approach for learning sketch decompositions\ndoes not yield interpretable sketches in the form of rules, we demonstrate that\nthe resulting decompositions can often be understood in a crisp manner.\n", "link": "http://arxiv.org/abs/2412.08574v2", "date": "2025-08-15", "relevancy": 2.047, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.539}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch%20Decompositions%20for%20Classical%20Planning%20via%20Deep%20Reinforcement%0A%20%20Learning&body=Title%3A%20Sketch%20Decompositions%20for%20Classical%20Planning%20via%20Deep%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Michael%20Aichm%C3%BCller%20and%20Hector%20Geffner%0AAbstract%3A%20%20%20In%20planning%20and%20reinforcement%20learning%2C%20the%20identification%20of%20common%20subgoal%0Astructures%20across%20problems%20is%20important%20when%20goals%20are%20to%20be%20achieved%20over%20long%0Ahorizons.%20Recently%2C%20it%20has%20been%20shown%20that%20such%20structures%20can%20be%20expressed%20as%0Afeature-based%20rules%2C%20called%20sketches%2C%20over%20a%20number%20of%20classical%20planning%0Adomains.%20These%20sketches%20split%20problems%20into%20subproblems%20which%20then%20become%0Asolvable%20in%20low%20polynomial%20time%20by%20a%20greedy%20sequence%20of%20IW%24%28k%29%24%20searches.%0AMethods%20for%20learning%20sketches%20using%20feature%20pools%20and%20min-SAT%20solvers%20have%20been%0Adeveloped%2C%20yet%20they%20face%20two%20key%20limitations%3A%20scalability%20and%20expressivity.%20In%0Athis%20work%2C%20we%20address%20these%20limitations%20by%20formulating%20the%20problem%20of%20learning%0Asketch%20decompositions%20as%20a%20deep%20reinforcement%20learning%20%28DRL%29%20task%2C%20where%0Ageneral%20policies%20are%20sought%20in%20a%20modified%20planning%20problem%20where%20the%20successor%0Astates%20of%20a%20state%20s%20are%20defined%20as%20those%20reachable%20from%20s%20through%20an%20IW%24%28k%29%24%0Asearch.%20The%20sketch%20decompositions%20obtained%20through%20this%20method%20are%0Aexperimentally%20evaluated%20across%20various%20domains%2C%20and%20problems%20are%20regarded%20as%0Asolved%20by%20the%20decomposition%20when%20the%20goal%20is%20reached%20through%20a%20greedy%20sequence%0Aof%20IW%24%28k%29%24%20searches.%20While%20our%20DRL%20approach%20for%20learning%20sketch%20decompositions%0Adoes%20not%20yield%20interpretable%20sketches%20in%20the%20form%20of%20rules%2C%20we%20demonstrate%20that%0Athe%20resulting%20decompositions%20can%20often%20be%20understood%20in%20a%20crisp%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08574v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch%2520Decompositions%2520for%2520Classical%2520Planning%2520via%2520Deep%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DMichael%2520Aichm%25C3%25BCller%2520and%2520Hector%2520Geffner%26entry.1292438233%3D%2520%2520In%2520planning%2520and%2520reinforcement%2520learning%252C%2520the%2520identification%2520of%2520common%2520subgoal%250Astructures%2520across%2520problems%2520is%2520important%2520when%2520goals%2520are%2520to%2520be%2520achieved%2520over%2520long%250Ahorizons.%2520Recently%252C%2520it%2520has%2520been%2520shown%2520that%2520such%2520structures%2520can%2520be%2520expressed%2520as%250Afeature-based%2520rules%252C%2520called%2520sketches%252C%2520over%2520a%2520number%2520of%2520classical%2520planning%250Adomains.%2520These%2520sketches%2520split%2520problems%2520into%2520subproblems%2520which%2520then%2520become%250Asolvable%2520in%2520low%2520polynomial%2520time%2520by%2520a%2520greedy%2520sequence%2520of%2520IW%2524%2528k%2529%2524%2520searches.%250AMethods%2520for%2520learning%2520sketches%2520using%2520feature%2520pools%2520and%2520min-SAT%2520solvers%2520have%2520been%250Adeveloped%252C%2520yet%2520they%2520face%2520two%2520key%2520limitations%253A%2520scalability%2520and%2520expressivity.%2520In%250Athis%2520work%252C%2520we%2520address%2520these%2520limitations%2520by%2520formulating%2520the%2520problem%2520of%2520learning%250Asketch%2520decompositions%2520as%2520a%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%2520task%252C%2520where%250Ageneral%2520policies%2520are%2520sought%2520in%2520a%2520modified%2520planning%2520problem%2520where%2520the%2520successor%250Astates%2520of%2520a%2520state%2520s%2520are%2520defined%2520as%2520those%2520reachable%2520from%2520s%2520through%2520an%2520IW%2524%2528k%2529%2524%250Asearch.%2520The%2520sketch%2520decompositions%2520obtained%2520through%2520this%2520method%2520are%250Aexperimentally%2520evaluated%2520across%2520various%2520domains%252C%2520and%2520problems%2520are%2520regarded%2520as%250Asolved%2520by%2520the%2520decomposition%2520when%2520the%2520goal%2520is%2520reached%2520through%2520a%2520greedy%2520sequence%250Aof%2520IW%2524%2528k%2529%2524%2520searches.%2520While%2520our%2520DRL%2520approach%2520for%2520learning%2520sketch%2520decompositions%250Adoes%2520not%2520yield%2520interpretable%2520sketches%2520in%2520the%2520form%2520of%2520rules%252C%2520we%2520demonstrate%2520that%250Athe%2520resulting%2520decompositions%2520can%2520often%2520be%2520understood%2520in%2520a%2520crisp%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08574v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch%20Decompositions%20for%20Classical%20Planning%20via%20Deep%20Reinforcement%0A%20%20Learning&entry.906535625=Michael%20Aichm%C3%BCller%20and%20Hector%20Geffner&entry.1292438233=%20%20In%20planning%20and%20reinforcement%20learning%2C%20the%20identification%20of%20common%20subgoal%0Astructures%20across%20problems%20is%20important%20when%20goals%20are%20to%20be%20achieved%20over%20long%0Ahorizons.%20Recently%2C%20it%20has%20been%20shown%20that%20such%20structures%20can%20be%20expressed%20as%0Afeature-based%20rules%2C%20called%20sketches%2C%20over%20a%20number%20of%20classical%20planning%0Adomains.%20These%20sketches%20split%20problems%20into%20subproblems%20which%20then%20become%0Asolvable%20in%20low%20polynomial%20time%20by%20a%20greedy%20sequence%20of%20IW%24%28k%29%24%20searches.%0AMethods%20for%20learning%20sketches%20using%20feature%20pools%20and%20min-SAT%20solvers%20have%20been%0Adeveloped%2C%20yet%20they%20face%20two%20key%20limitations%3A%20scalability%20and%20expressivity.%20In%0Athis%20work%2C%20we%20address%20these%20limitations%20by%20formulating%20the%20problem%20of%20learning%0Asketch%20decompositions%20as%20a%20deep%20reinforcement%20learning%20%28DRL%29%20task%2C%20where%0Ageneral%20policies%20are%20sought%20in%20a%20modified%20planning%20problem%20where%20the%20successor%0Astates%20of%20a%20state%20s%20are%20defined%20as%20those%20reachable%20from%20s%20through%20an%20IW%24%28k%29%24%0Asearch.%20The%20sketch%20decompositions%20obtained%20through%20this%20method%20are%0Aexperimentally%20evaluated%20across%20various%20domains%2C%20and%20problems%20are%20regarded%20as%0Asolved%20by%20the%20decomposition%20when%20the%20goal%20is%20reached%20through%20a%20greedy%20sequence%0Aof%20IW%24%28k%29%24%20searches.%20While%20our%20DRL%20approach%20for%20learning%20sketch%20decompositions%0Adoes%20not%20yield%20interpretable%20sketches%20in%20the%20form%20of%20rules%2C%20we%20demonstrate%20that%0Athe%20resulting%20decompositions%20can%20often%20be%20understood%20in%20a%20crisp%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08574v2&entry.124074799=Read"},
{"title": "A Real-time Concrete Crack Detection and Segmentation Model Based on\n  YOLOv11", "author": "Shaoze Huang and Qi Liu and Chao Chen and Yuhang Chen", "abstract": "  Accelerated aging of transportation infrastructure in the rapidly developing\nYangtze River Delta region necessitates efficient concrete crack detection, as\ncrack deterioration critically compromises structural integrity and regional\neconomic growth. To overcome the limitations of inefficient manual inspection\nand the suboptimal performance of existing deep learning models, particularly\nfor small-target crack detection within complex backgrounds, this paper\nproposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and\nsegmentation model based on the YOLOv11n architecture. The proposed model\nintegrates a three-stage optimization framework: (1) Embedding dynamic\nKernelWarehouse convolution (KWConv) within the backbone network to enhance\nfeature representation through a dynamic kernel sharing mechanism; (2)\nIncorporating a triple attention mechanism (TA) into the feature pyramid to\nstrengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU\nloss function to facilitate adaptive bounding box regression penalization.\nExperimental validation demonstrates that the enhanced model achieves\nsignificant performance improvements over the baseline, attaining 91.3%\nprecision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the\nsynergistic efficacy of the proposed modules. Furthermore, robustness tests\nindicate stable performance under conditions of data scarcity and noise\ninterference. This research delivers an efficient computer vision solution for\nautomated infrastructure inspection, exhibiting substantial practical\nengineering value.\n", "link": "http://arxiv.org/abs/2508.11517v1", "date": "2025-08-15", "relevancy": 2.0412, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Real-time%20Concrete%20Crack%20Detection%20and%20Segmentation%20Model%20Based%20on%0A%20%20YOLOv11&body=Title%3A%20A%20Real-time%20Concrete%20Crack%20Detection%20and%20Segmentation%20Model%20Based%20on%0A%20%20YOLOv11%0AAuthor%3A%20Shaoze%20Huang%20and%20Qi%20Liu%20and%20Chao%20Chen%20and%20Yuhang%20Chen%0AAbstract%3A%20%20%20Accelerated%20aging%20of%20transportation%20infrastructure%20in%20the%20rapidly%20developing%0AYangtze%20River%20Delta%20region%20necessitates%20efficient%20concrete%20crack%20detection%2C%20as%0Acrack%20deterioration%20critically%20compromises%20structural%20integrity%20and%20regional%0Aeconomic%20growth.%20To%20overcome%20the%20limitations%20of%20inefficient%20manual%20inspection%0Aand%20the%20suboptimal%20performance%20of%20existing%20deep%20learning%20models%2C%20particularly%0Afor%20small-target%20crack%20detection%20within%20complex%20backgrounds%2C%20this%20paper%0Aproposes%20YOLOv11-KW-TA-FP%2C%20a%20multi-task%20concrete%20crack%20detection%20and%0Asegmentation%20model%20based%20on%20the%20YOLOv11n%20architecture.%20The%20proposed%20model%0Aintegrates%20a%20three-stage%20optimization%20framework%3A%20%281%29%20Embedding%20dynamic%0AKernelWarehouse%20convolution%20%28KWConv%29%20within%20the%20backbone%20network%20to%20enhance%0Afeature%20representation%20through%20a%20dynamic%20kernel%20sharing%20mechanism%3B%20%282%29%0AIncorporating%20a%20triple%20attention%20mechanism%20%28TA%29%20into%20the%20feature%20pyramid%20to%0Astrengthen%20channel-spatial%20interaction%20modeling%3B%20and%20%283%29%20Designing%20an%20FP-IoU%0Aloss%20function%20to%20facilitate%20adaptive%20bounding%20box%20regression%20penalization.%0AExperimental%20validation%20demonstrates%20that%20the%20enhanced%20model%20achieves%0Asignificant%20performance%20improvements%20over%20the%20baseline%2C%20attaining%2091.3%25%0Aprecision%2C%2076.6%25%20recall%2C%20and%2086.4%25%20mAP%4050.%20Ablation%20studies%20confirm%20the%0Asynergistic%20efficacy%20of%20the%20proposed%20modules.%20Furthermore%2C%20robustness%20tests%0Aindicate%20stable%20performance%20under%20conditions%20of%20data%20scarcity%20and%20noise%0Ainterference.%20This%20research%20delivers%20an%20efficient%20computer%20vision%20solution%20for%0Aautomated%20infrastructure%20inspection%2C%20exhibiting%20substantial%20practical%0Aengineering%20value.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Real-time%2520Concrete%2520Crack%2520Detection%2520and%2520Segmentation%2520Model%2520Based%2520on%250A%2520%2520YOLOv11%26entry.906535625%3DShaoze%2520Huang%2520and%2520Qi%2520Liu%2520and%2520Chao%2520Chen%2520and%2520Yuhang%2520Chen%26entry.1292438233%3D%2520%2520Accelerated%2520aging%2520of%2520transportation%2520infrastructure%2520in%2520the%2520rapidly%2520developing%250AYangtze%2520River%2520Delta%2520region%2520necessitates%2520efficient%2520concrete%2520crack%2520detection%252C%2520as%250Acrack%2520deterioration%2520critically%2520compromises%2520structural%2520integrity%2520and%2520regional%250Aeconomic%2520growth.%2520To%2520overcome%2520the%2520limitations%2520of%2520inefficient%2520manual%2520inspection%250Aand%2520the%2520suboptimal%2520performance%2520of%2520existing%2520deep%2520learning%2520models%252C%2520particularly%250Afor%2520small-target%2520crack%2520detection%2520within%2520complex%2520backgrounds%252C%2520this%2520paper%250Aproposes%2520YOLOv11-KW-TA-FP%252C%2520a%2520multi-task%2520concrete%2520crack%2520detection%2520and%250Asegmentation%2520model%2520based%2520on%2520the%2520YOLOv11n%2520architecture.%2520The%2520proposed%2520model%250Aintegrates%2520a%2520three-stage%2520optimization%2520framework%253A%2520%25281%2529%2520Embedding%2520dynamic%250AKernelWarehouse%2520convolution%2520%2528KWConv%2529%2520within%2520the%2520backbone%2520network%2520to%2520enhance%250Afeature%2520representation%2520through%2520a%2520dynamic%2520kernel%2520sharing%2520mechanism%253B%2520%25282%2529%250AIncorporating%2520a%2520triple%2520attention%2520mechanism%2520%2528TA%2529%2520into%2520the%2520feature%2520pyramid%2520to%250Astrengthen%2520channel-spatial%2520interaction%2520modeling%253B%2520and%2520%25283%2529%2520Designing%2520an%2520FP-IoU%250Aloss%2520function%2520to%2520facilitate%2520adaptive%2520bounding%2520box%2520regression%2520penalization.%250AExperimental%2520validation%2520demonstrates%2520that%2520the%2520enhanced%2520model%2520achieves%250Asignificant%2520performance%2520improvements%2520over%2520the%2520baseline%252C%2520attaining%252091.3%2525%250Aprecision%252C%252076.6%2525%2520recall%252C%2520and%252086.4%2525%2520mAP%254050.%2520Ablation%2520studies%2520confirm%2520the%250Asynergistic%2520efficacy%2520of%2520the%2520proposed%2520modules.%2520Furthermore%252C%2520robustness%2520tests%250Aindicate%2520stable%2520performance%2520under%2520conditions%2520of%2520data%2520scarcity%2520and%2520noise%250Ainterference.%2520This%2520research%2520delivers%2520an%2520efficient%2520computer%2520vision%2520solution%2520for%250Aautomated%2520infrastructure%2520inspection%252C%2520exhibiting%2520substantial%2520practical%250Aengineering%2520value.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Real-time%20Concrete%20Crack%20Detection%20and%20Segmentation%20Model%20Based%20on%0A%20%20YOLOv11&entry.906535625=Shaoze%20Huang%20and%20Qi%20Liu%20and%20Chao%20Chen%20and%20Yuhang%20Chen&entry.1292438233=%20%20Accelerated%20aging%20of%20transportation%20infrastructure%20in%20the%20rapidly%20developing%0AYangtze%20River%20Delta%20region%20necessitates%20efficient%20concrete%20crack%20detection%2C%20as%0Acrack%20deterioration%20critically%20compromises%20structural%20integrity%20and%20regional%0Aeconomic%20growth.%20To%20overcome%20the%20limitations%20of%20inefficient%20manual%20inspection%0Aand%20the%20suboptimal%20performance%20of%20existing%20deep%20learning%20models%2C%20particularly%0Afor%20small-target%20crack%20detection%20within%20complex%20backgrounds%2C%20this%20paper%0Aproposes%20YOLOv11-KW-TA-FP%2C%20a%20multi-task%20concrete%20crack%20detection%20and%0Asegmentation%20model%20based%20on%20the%20YOLOv11n%20architecture.%20The%20proposed%20model%0Aintegrates%20a%20three-stage%20optimization%20framework%3A%20%281%29%20Embedding%20dynamic%0AKernelWarehouse%20convolution%20%28KWConv%29%20within%20the%20backbone%20network%20to%20enhance%0Afeature%20representation%20through%20a%20dynamic%20kernel%20sharing%20mechanism%3B%20%282%29%0AIncorporating%20a%20triple%20attention%20mechanism%20%28TA%29%20into%20the%20feature%20pyramid%20to%0Astrengthen%20channel-spatial%20interaction%20modeling%3B%20and%20%283%29%20Designing%20an%20FP-IoU%0Aloss%20function%20to%20facilitate%20adaptive%20bounding%20box%20regression%20penalization.%0AExperimental%20validation%20demonstrates%20that%20the%20enhanced%20model%20achieves%0Asignificant%20performance%20improvements%20over%20the%20baseline%2C%20attaining%2091.3%25%0Aprecision%2C%2076.6%25%20recall%2C%20and%2086.4%25%20mAP%4050.%20Ablation%20studies%20confirm%20the%0Asynergistic%20efficacy%20of%20the%20proposed%20modules.%20Furthermore%2C%20robustness%20tests%0Aindicate%20stable%20performance%20under%20conditions%20of%20data%20scarcity%20and%20noise%0Ainterference.%20This%20research%20delivers%20an%20efficient%20computer%20vision%20solution%20for%0Aautomated%20infrastructure%20inspection%2C%20exhibiting%20substantial%20practical%0Aengineering%20value.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11517v1&entry.124074799=Read"},
{"title": "Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State\n  Specialization and Interaction", "author": "Shilei Wang and Gong Cheng and Pujian Lai and Dong Gao and Junwei Han", "abstract": "  Efficient trackers achieve faster runtime by reducing computational\ncomplexity and model parameters. However, this efficiency often compromises the\nexpense of weakened feature representation capacity, thus limiting their\nability to accurately capture target states using single-layer features. To\novercome this limitation, we propose Multi-State Tracker (MST), which utilizes\nhighly lightweight state-specific enhancement (SSE) to perform specialized\nenhancement on multi-state features produced by multi-state generation (MSG)\nand aggregates them in an interactive and adaptive manner using cross-state\ninteraction (CSI). This design greatly enhances feature representation while\nincurring minimal computational overhead, leading to improved tracking\nrobustness in complex environments. Specifically, the MSG generates multiple\nstate representations at multiple stages during feature extraction, while SSE\nrefines them to highlight target-specific features. The CSI module facilitates\ninformation exchange between these states and ensures the integration of\ncomplementary features. Notably, the introduced SSE and CSI modules adopt a\nhighly lightweight hidden state adaptation-based state space duality (HSA-SSD)\ndesign, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.\nExperimental results demonstrate that MST outperforms all previous efficient\ntrackers across multiple datasets, significantly improving tracking accuracy\nand robustness. In particular, it shows excellent runtime performance, with an\nAO score improvement of 4.5\\% over the previous SOTA efficient tracker HCAT on\nthe GOT-10K dataset. The code is available at https://github.com/wsumel/MST.\n", "link": "http://arxiv.org/abs/2508.11531v1", "date": "2025-08-15", "relevancy": 2.0402, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5274}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5068}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-State%20Tracker%3A%20Enhancing%20Efficient%20Object%20Tracking%20via%20Multi-State%0A%20%20Specialization%20and%20Interaction&body=Title%3A%20Multi-State%20Tracker%3A%20Enhancing%20Efficient%20Object%20Tracking%20via%20Multi-State%0A%20%20Specialization%20and%20Interaction%0AAuthor%3A%20Shilei%20Wang%20and%20Gong%20Cheng%20and%20Pujian%20Lai%20and%20Dong%20Gao%20and%20Junwei%20Han%0AAbstract%3A%20%20%20Efficient%20trackers%20achieve%20faster%20runtime%20by%20reducing%20computational%0Acomplexity%20and%20model%20parameters.%20However%2C%20this%20efficiency%20often%20compromises%20the%0Aexpense%20of%20weakened%20feature%20representation%20capacity%2C%20thus%20limiting%20their%0Aability%20to%20accurately%20capture%20target%20states%20using%20single-layer%20features.%20To%0Aovercome%20this%20limitation%2C%20we%20propose%20Multi-State%20Tracker%20%28MST%29%2C%20which%20utilizes%0Ahighly%20lightweight%20state-specific%20enhancement%20%28SSE%29%20to%20perform%20specialized%0Aenhancement%20on%20multi-state%20features%20produced%20by%20multi-state%20generation%20%28MSG%29%0Aand%20aggregates%20them%20in%20an%20interactive%20and%20adaptive%20manner%20using%20cross-state%0Ainteraction%20%28CSI%29.%20This%20design%20greatly%20enhances%20feature%20representation%20while%0Aincurring%20minimal%20computational%20overhead%2C%20leading%20to%20improved%20tracking%0Arobustness%20in%20complex%20environments.%20Specifically%2C%20the%20MSG%20generates%20multiple%0Astate%20representations%20at%20multiple%20stages%20during%20feature%20extraction%2C%20while%20SSE%0Arefines%20them%20to%20highlight%20target-specific%20features.%20The%20CSI%20module%20facilitates%0Ainformation%20exchange%20between%20these%20states%20and%20ensures%20the%20integration%20of%0Acomplementary%20features.%20Notably%2C%20the%20introduced%20SSE%20and%20CSI%20modules%20adopt%20a%0Ahighly%20lightweight%20hidden%20state%20adaptation-based%20state%20space%20duality%20%28HSA-SSD%29%0Adesign%2C%20incurring%20only%200.1%20GFLOPs%20in%20computation%20and%200.66%20M%20in%20parameters.%0AExperimental%20results%20demonstrate%20that%20MST%20outperforms%20all%20previous%20efficient%0Atrackers%20across%20multiple%20datasets%2C%20significantly%20improving%20tracking%20accuracy%0Aand%20robustness.%20In%20particular%2C%20it%20shows%20excellent%20runtime%20performance%2C%20with%20an%0AAO%20score%20improvement%20of%204.5%5C%25%20over%20the%20previous%20SOTA%20efficient%20tracker%20HCAT%20on%0Athe%20GOT-10K%20dataset.%20The%20code%20is%20available%20at%20https%3A//github.com/wsumel/MST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-State%2520Tracker%253A%2520Enhancing%2520Efficient%2520Object%2520Tracking%2520via%2520Multi-State%250A%2520%2520Specialization%2520and%2520Interaction%26entry.906535625%3DShilei%2520Wang%2520and%2520Gong%2520Cheng%2520and%2520Pujian%2520Lai%2520and%2520Dong%2520Gao%2520and%2520Junwei%2520Han%26entry.1292438233%3D%2520%2520Efficient%2520trackers%2520achieve%2520faster%2520runtime%2520by%2520reducing%2520computational%250Acomplexity%2520and%2520model%2520parameters.%2520However%252C%2520this%2520efficiency%2520often%2520compromises%2520the%250Aexpense%2520of%2520weakened%2520feature%2520representation%2520capacity%252C%2520thus%2520limiting%2520their%250Aability%2520to%2520accurately%2520capture%2520target%2520states%2520using%2520single-layer%2520features.%2520To%250Aovercome%2520this%2520limitation%252C%2520we%2520propose%2520Multi-State%2520Tracker%2520%2528MST%2529%252C%2520which%2520utilizes%250Ahighly%2520lightweight%2520state-specific%2520enhancement%2520%2528SSE%2529%2520to%2520perform%2520specialized%250Aenhancement%2520on%2520multi-state%2520features%2520produced%2520by%2520multi-state%2520generation%2520%2528MSG%2529%250Aand%2520aggregates%2520them%2520in%2520an%2520interactive%2520and%2520adaptive%2520manner%2520using%2520cross-state%250Ainteraction%2520%2528CSI%2529.%2520This%2520design%2520greatly%2520enhances%2520feature%2520representation%2520while%250Aincurring%2520minimal%2520computational%2520overhead%252C%2520leading%2520to%2520improved%2520tracking%250Arobustness%2520in%2520complex%2520environments.%2520Specifically%252C%2520the%2520MSG%2520generates%2520multiple%250Astate%2520representations%2520at%2520multiple%2520stages%2520during%2520feature%2520extraction%252C%2520while%2520SSE%250Arefines%2520them%2520to%2520highlight%2520target-specific%2520features.%2520The%2520CSI%2520module%2520facilitates%250Ainformation%2520exchange%2520between%2520these%2520states%2520and%2520ensures%2520the%2520integration%2520of%250Acomplementary%2520features.%2520Notably%252C%2520the%2520introduced%2520SSE%2520and%2520CSI%2520modules%2520adopt%2520a%250Ahighly%2520lightweight%2520hidden%2520state%2520adaptation-based%2520state%2520space%2520duality%2520%2528HSA-SSD%2529%250Adesign%252C%2520incurring%2520only%25200.1%2520GFLOPs%2520in%2520computation%2520and%25200.66%2520M%2520in%2520parameters.%250AExperimental%2520results%2520demonstrate%2520that%2520MST%2520outperforms%2520all%2520previous%2520efficient%250Atrackers%2520across%2520multiple%2520datasets%252C%2520significantly%2520improving%2520tracking%2520accuracy%250Aand%2520robustness.%2520In%2520particular%252C%2520it%2520shows%2520excellent%2520runtime%2520performance%252C%2520with%2520an%250AAO%2520score%2520improvement%2520of%25204.5%255C%2525%2520over%2520the%2520previous%2520SOTA%2520efficient%2520tracker%2520HCAT%2520on%250Athe%2520GOT-10K%2520dataset.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/wsumel/MST.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-State%20Tracker%3A%20Enhancing%20Efficient%20Object%20Tracking%20via%20Multi-State%0A%20%20Specialization%20and%20Interaction&entry.906535625=Shilei%20Wang%20and%20Gong%20Cheng%20and%20Pujian%20Lai%20and%20Dong%20Gao%20and%20Junwei%20Han&entry.1292438233=%20%20Efficient%20trackers%20achieve%20faster%20runtime%20by%20reducing%20computational%0Acomplexity%20and%20model%20parameters.%20However%2C%20this%20efficiency%20often%20compromises%20the%0Aexpense%20of%20weakened%20feature%20representation%20capacity%2C%20thus%20limiting%20their%0Aability%20to%20accurately%20capture%20target%20states%20using%20single-layer%20features.%20To%0Aovercome%20this%20limitation%2C%20we%20propose%20Multi-State%20Tracker%20%28MST%29%2C%20which%20utilizes%0Ahighly%20lightweight%20state-specific%20enhancement%20%28SSE%29%20to%20perform%20specialized%0Aenhancement%20on%20multi-state%20features%20produced%20by%20multi-state%20generation%20%28MSG%29%0Aand%20aggregates%20them%20in%20an%20interactive%20and%20adaptive%20manner%20using%20cross-state%0Ainteraction%20%28CSI%29.%20This%20design%20greatly%20enhances%20feature%20representation%20while%0Aincurring%20minimal%20computational%20overhead%2C%20leading%20to%20improved%20tracking%0Arobustness%20in%20complex%20environments.%20Specifically%2C%20the%20MSG%20generates%20multiple%0Astate%20representations%20at%20multiple%20stages%20during%20feature%20extraction%2C%20while%20SSE%0Arefines%20them%20to%20highlight%20target-specific%20features.%20The%20CSI%20module%20facilitates%0Ainformation%20exchange%20between%20these%20states%20and%20ensures%20the%20integration%20of%0Acomplementary%20features.%20Notably%2C%20the%20introduced%20SSE%20and%20CSI%20modules%20adopt%20a%0Ahighly%20lightweight%20hidden%20state%20adaptation-based%20state%20space%20duality%20%28HSA-SSD%29%0Adesign%2C%20incurring%20only%200.1%20GFLOPs%20in%20computation%20and%200.66%20M%20in%20parameters.%0AExperimental%20results%20demonstrate%20that%20MST%20outperforms%20all%20previous%20efficient%0Atrackers%20across%20multiple%20datasets%2C%20significantly%20improving%20tracking%20accuracy%0Aand%20robustness.%20In%20particular%2C%20it%20shows%20excellent%20runtime%20performance%2C%20with%20an%0AAO%20score%20improvement%20of%204.5%5C%25%20over%20the%20previous%20SOTA%20efficient%20tracker%20HCAT%20on%0Athe%20GOT-10K%20dataset.%20The%20code%20is%20available%20at%20https%3A//github.com/wsumel/MST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11531v1&entry.124074799=Read"},
{"title": "A Comprehensive Perspective on Explainable AI across the Machine\n  Learning Workflow", "author": "George Paterakis and Andrea Castellani and George Papoutsoglou and Tobias Rodemann and Ioannis Tsamardinos", "abstract": "  Artificial intelligence is reshaping science and industry, yet many users\nstill regard its models as opaque \"black boxes\". Conventional explainable\nartificial-intelligence methods clarify individual predictions but overlook the\nupstream decisions and downstream quality checks that determine whether\ninsights can be trusted. In this work, we present Holistic Explainable\nArtificial Intelligence (HXAI), a user-centric framework that embeds\nexplanation into every stage of the data-analysis workflow and tailors those\nexplanations to users. HXAI unifies six components (data, analysis set-up,\nlearning process, model output, model quality, communication channel) into a\nsingle taxonomy and aligns each component with the needs of domain experts,\ndata analysts and data scientists. A 112-item question bank covers these needs;\nour survey of contemporary tools highlights critical coverage gaps. Grounded in\ntheories of human explanation, principles from human-computer interaction and\nfindings from empirical user studies, HXAI identifies the characteristics that\nmake explanations clear, actionable and cognitively manageable. A comprehensive\ntaxonomy operationalises these insights, reducing terminological ambiguity and\nenabling rigorous coverage analysis of existing toolchains. We further\ndemonstrate how AI agents that embed large-language models can orchestrate\ndiverse explanation techniques, translating technical artifacts into\nstakeholder-specific narratives that bridge the gap between AI developers and\ndomain experts. Departing from traditional surveys or perspective articles,\nthis work melds concepts from multiple disciplines, lessons from real-world\nprojects and a critical synthesis of the literature to advance a novel,\nend-to-end viewpoint on transparency, trustworthiness and responsible AI\ndeployment.\n", "link": "http://arxiv.org/abs/2508.11529v1", "date": "2025-08-15", "relevancy": 2.0364, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Perspective%20on%20Explainable%20AI%20across%20the%20Machine%0A%20%20Learning%20Workflow&body=Title%3A%20A%20Comprehensive%20Perspective%20on%20Explainable%20AI%20across%20the%20Machine%0A%20%20Learning%20Workflow%0AAuthor%3A%20George%20Paterakis%20and%20Andrea%20Castellani%20and%20George%20Papoutsoglou%20and%20Tobias%20Rodemann%20and%20Ioannis%20Tsamardinos%0AAbstract%3A%20%20%20Artificial%20intelligence%20is%20reshaping%20science%20and%20industry%2C%20yet%20many%20users%0Astill%20regard%20its%20models%20as%20opaque%20%22black%20boxes%22.%20Conventional%20explainable%0Aartificial-intelligence%20methods%20clarify%20individual%20predictions%20but%20overlook%20the%0Aupstream%20decisions%20and%20downstream%20quality%20checks%20that%20determine%20whether%0Ainsights%20can%20be%20trusted.%20In%20this%20work%2C%20we%20present%20Holistic%20Explainable%0AArtificial%20Intelligence%20%28HXAI%29%2C%20a%20user-centric%20framework%20that%20embeds%0Aexplanation%20into%20every%20stage%20of%20the%20data-analysis%20workflow%20and%20tailors%20those%0Aexplanations%20to%20users.%20HXAI%20unifies%20six%20components%20%28data%2C%20analysis%20set-up%2C%0Alearning%20process%2C%20model%20output%2C%20model%20quality%2C%20communication%20channel%29%20into%20a%0Asingle%20taxonomy%20and%20aligns%20each%20component%20with%20the%20needs%20of%20domain%20experts%2C%0Adata%20analysts%20and%20data%20scientists.%20A%20112-item%20question%20bank%20covers%20these%20needs%3B%0Aour%20survey%20of%20contemporary%20tools%20highlights%20critical%20coverage%20gaps.%20Grounded%20in%0Atheories%20of%20human%20explanation%2C%20principles%20from%20human-computer%20interaction%20and%0Afindings%20from%20empirical%20user%20studies%2C%20HXAI%20identifies%20the%20characteristics%20that%0Amake%20explanations%20clear%2C%20actionable%20and%20cognitively%20manageable.%20A%20comprehensive%0Ataxonomy%20operationalises%20these%20insights%2C%20reducing%20terminological%20ambiguity%20and%0Aenabling%20rigorous%20coverage%20analysis%20of%20existing%20toolchains.%20We%20further%0Ademonstrate%20how%20AI%20agents%20that%20embed%20large-language%20models%20can%20orchestrate%0Adiverse%20explanation%20techniques%2C%20translating%20technical%20artifacts%20into%0Astakeholder-specific%20narratives%20that%20bridge%20the%20gap%20between%20AI%20developers%20and%0Adomain%20experts.%20Departing%20from%20traditional%20surveys%20or%20perspective%20articles%2C%0Athis%20work%20melds%20concepts%20from%20multiple%20disciplines%2C%20lessons%20from%20real-world%0Aprojects%20and%20a%20critical%20synthesis%20of%20the%20literature%20to%20advance%20a%20novel%2C%0Aend-to-end%20viewpoint%20on%20transparency%2C%20trustworthiness%20and%20responsible%20AI%0Adeployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Perspective%2520on%2520Explainable%2520AI%2520across%2520the%2520Machine%250A%2520%2520Learning%2520Workflow%26entry.906535625%3DGeorge%2520Paterakis%2520and%2520Andrea%2520Castellani%2520and%2520George%2520Papoutsoglou%2520and%2520Tobias%2520Rodemann%2520and%2520Ioannis%2520Tsamardinos%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520is%2520reshaping%2520science%2520and%2520industry%252C%2520yet%2520many%2520users%250Astill%2520regard%2520its%2520models%2520as%2520opaque%2520%2522black%2520boxes%2522.%2520Conventional%2520explainable%250Aartificial-intelligence%2520methods%2520clarify%2520individual%2520predictions%2520but%2520overlook%2520the%250Aupstream%2520decisions%2520and%2520downstream%2520quality%2520checks%2520that%2520determine%2520whether%250Ainsights%2520can%2520be%2520trusted.%2520In%2520this%2520work%252C%2520we%2520present%2520Holistic%2520Explainable%250AArtificial%2520Intelligence%2520%2528HXAI%2529%252C%2520a%2520user-centric%2520framework%2520that%2520embeds%250Aexplanation%2520into%2520every%2520stage%2520of%2520the%2520data-analysis%2520workflow%2520and%2520tailors%2520those%250Aexplanations%2520to%2520users.%2520HXAI%2520unifies%2520six%2520components%2520%2528data%252C%2520analysis%2520set-up%252C%250Alearning%2520process%252C%2520model%2520output%252C%2520model%2520quality%252C%2520communication%2520channel%2529%2520into%2520a%250Asingle%2520taxonomy%2520and%2520aligns%2520each%2520component%2520with%2520the%2520needs%2520of%2520domain%2520experts%252C%250Adata%2520analysts%2520and%2520data%2520scientists.%2520A%2520112-item%2520question%2520bank%2520covers%2520these%2520needs%253B%250Aour%2520survey%2520of%2520contemporary%2520tools%2520highlights%2520critical%2520coverage%2520gaps.%2520Grounded%2520in%250Atheories%2520of%2520human%2520explanation%252C%2520principles%2520from%2520human-computer%2520interaction%2520and%250Afindings%2520from%2520empirical%2520user%2520studies%252C%2520HXAI%2520identifies%2520the%2520characteristics%2520that%250Amake%2520explanations%2520clear%252C%2520actionable%2520and%2520cognitively%2520manageable.%2520A%2520comprehensive%250Ataxonomy%2520operationalises%2520these%2520insights%252C%2520reducing%2520terminological%2520ambiguity%2520and%250Aenabling%2520rigorous%2520coverage%2520analysis%2520of%2520existing%2520toolchains.%2520We%2520further%250Ademonstrate%2520how%2520AI%2520agents%2520that%2520embed%2520large-language%2520models%2520can%2520orchestrate%250Adiverse%2520explanation%2520techniques%252C%2520translating%2520technical%2520artifacts%2520into%250Astakeholder-specific%2520narratives%2520that%2520bridge%2520the%2520gap%2520between%2520AI%2520developers%2520and%250Adomain%2520experts.%2520Departing%2520from%2520traditional%2520surveys%2520or%2520perspective%2520articles%252C%250Athis%2520work%2520melds%2520concepts%2520from%2520multiple%2520disciplines%252C%2520lessons%2520from%2520real-world%250Aprojects%2520and%2520a%2520critical%2520synthesis%2520of%2520the%2520literature%2520to%2520advance%2520a%2520novel%252C%250Aend-to-end%2520viewpoint%2520on%2520transparency%252C%2520trustworthiness%2520and%2520responsible%2520AI%250Adeployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Perspective%20on%20Explainable%20AI%20across%20the%20Machine%0A%20%20Learning%20Workflow&entry.906535625=George%20Paterakis%20and%20Andrea%20Castellani%20and%20George%20Papoutsoglou%20and%20Tobias%20Rodemann%20and%20Ioannis%20Tsamardinos&entry.1292438233=%20%20Artificial%20intelligence%20is%20reshaping%20science%20and%20industry%2C%20yet%20many%20users%0Astill%20regard%20its%20models%20as%20opaque%20%22black%20boxes%22.%20Conventional%20explainable%0Aartificial-intelligence%20methods%20clarify%20individual%20predictions%20but%20overlook%20the%0Aupstream%20decisions%20and%20downstream%20quality%20checks%20that%20determine%20whether%0Ainsights%20can%20be%20trusted.%20In%20this%20work%2C%20we%20present%20Holistic%20Explainable%0AArtificial%20Intelligence%20%28HXAI%29%2C%20a%20user-centric%20framework%20that%20embeds%0Aexplanation%20into%20every%20stage%20of%20the%20data-analysis%20workflow%20and%20tailors%20those%0Aexplanations%20to%20users.%20HXAI%20unifies%20six%20components%20%28data%2C%20analysis%20set-up%2C%0Alearning%20process%2C%20model%20output%2C%20model%20quality%2C%20communication%20channel%29%20into%20a%0Asingle%20taxonomy%20and%20aligns%20each%20component%20with%20the%20needs%20of%20domain%20experts%2C%0Adata%20analysts%20and%20data%20scientists.%20A%20112-item%20question%20bank%20covers%20these%20needs%3B%0Aour%20survey%20of%20contemporary%20tools%20highlights%20critical%20coverage%20gaps.%20Grounded%20in%0Atheories%20of%20human%20explanation%2C%20principles%20from%20human-computer%20interaction%20and%0Afindings%20from%20empirical%20user%20studies%2C%20HXAI%20identifies%20the%20characteristics%20that%0Amake%20explanations%20clear%2C%20actionable%20and%20cognitively%20manageable.%20A%20comprehensive%0Ataxonomy%20operationalises%20these%20insights%2C%20reducing%20terminological%20ambiguity%20and%0Aenabling%20rigorous%20coverage%20analysis%20of%20existing%20toolchains.%20We%20further%0Ademonstrate%20how%20AI%20agents%20that%20embed%20large-language%20models%20can%20orchestrate%0Adiverse%20explanation%20techniques%2C%20translating%20technical%20artifacts%20into%0Astakeholder-specific%20narratives%20that%20bridge%20the%20gap%20between%20AI%20developers%20and%0Adomain%20experts.%20Departing%20from%20traditional%20surveys%20or%20perspective%20articles%2C%0Athis%20work%20melds%20concepts%20from%20multiple%20disciplines%2C%20lessons%20from%20real-world%0Aprojects%20and%20a%20critical%20synthesis%20of%20the%20literature%20to%20advance%20a%20novel%2C%0Aend-to-end%20viewpoint%20on%20transparency%2C%20trustworthiness%20and%20responsible%20AI%0Adeployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11529v1&entry.124074799=Read"},
{"title": "Exploring Superior Function Calls via Reinforcement Learning", "author": "Bingguang Hao and Maolin Wang and Zengzhuang Xu and Yicheng Chen and Cunyin Peng and Jinjie GU and Chenyi Zhuang", "abstract": "  Function calling capabilities are crucial for deploying Large Language Models\nin real-world applications, yet current training approaches fail to develop\nrobust reasoning strategies. Supervised fine-tuning produces models that rely\non superficial pattern matching, while standard reinforcement learning methods\nstruggle with the complex action space of structured function calls. We present\na novel reinforcement learning framework designed to enhance group relative\npolicy optimization through strategic entropy based exploration specifically\ntailored for function calling tasks. Our approach addresses three critical\nchallenges in function calling: insufficient exploration during policy\nlearning, lack of structured reasoning in chain-of-thought generation, and\ninadequate verification of parameter extraction. Our two-stage data preparation\npipeline ensures high-quality training samples through iterative LLM evaluation\nand abstract syntax tree validation. Extensive experiments on the Berkeley\nFunction Calling Leaderboard demonstrate that this framework achieves\nstate-of-the-art performance among open-source models with 86.02\\% overall\naccuracy, outperforming standard GRPO by up to 6\\% on complex multi-function\nscenarios. Notably, our method shows particularly strong improvements on\ncode-pretrained models, suggesting that structured language generation\ncapabilities provide an advantageous starting point for reinforcement learning\nin function calling tasks. We will release all the code, models and dataset to\nbenefit the community.\n", "link": "http://arxiv.org/abs/2508.05118v3", "date": "2025-08-15", "relevancy": 2.0286, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Superior%20Function%20Calls%20via%20Reinforcement%20Learning&body=Title%3A%20Exploring%20Superior%20Function%20Calls%20via%20Reinforcement%20Learning%0AAuthor%3A%20Bingguang%20Hao%20and%20Maolin%20Wang%20and%20Zengzhuang%20Xu%20and%20Yicheng%20Chen%20and%20Cunyin%20Peng%20and%20Jinjie%20GU%20and%20Chenyi%20Zhuang%0AAbstract%3A%20%20%20Function%20calling%20capabilities%20are%20crucial%20for%20deploying%20Large%20Language%20Models%0Ain%20real-world%20applications%2C%20yet%20current%20training%20approaches%20fail%20to%20develop%0Arobust%20reasoning%20strategies.%20Supervised%20fine-tuning%20produces%20models%20that%20rely%0Aon%20superficial%20pattern%20matching%2C%20while%20standard%20reinforcement%20learning%20methods%0Astruggle%20with%20the%20complex%20action%20space%20of%20structured%20function%20calls.%20We%20present%0Aa%20novel%20reinforcement%20learning%20framework%20designed%20to%20enhance%20group%20relative%0Apolicy%20optimization%20through%20strategic%20entropy%20based%20exploration%20specifically%0Atailored%20for%20function%20calling%20tasks.%20Our%20approach%20addresses%20three%20critical%0Achallenges%20in%20function%20calling%3A%20insufficient%20exploration%20during%20policy%0Alearning%2C%20lack%20of%20structured%20reasoning%20in%20chain-of-thought%20generation%2C%20and%0Ainadequate%20verification%20of%20parameter%20extraction.%20Our%20two-stage%20data%20preparation%0Apipeline%20ensures%20high-quality%20training%20samples%20through%20iterative%20LLM%20evaluation%0Aand%20abstract%20syntax%20tree%20validation.%20Extensive%20experiments%20on%20the%20Berkeley%0AFunction%20Calling%20Leaderboard%20demonstrate%20that%20this%20framework%20achieves%0Astate-of-the-art%20performance%20among%20open-source%20models%20with%2086.02%5C%25%20overall%0Aaccuracy%2C%20outperforming%20standard%20GRPO%20by%20up%20to%206%5C%25%20on%20complex%20multi-function%0Ascenarios.%20Notably%2C%20our%20method%20shows%20particularly%20strong%20improvements%20on%0Acode-pretrained%20models%2C%20suggesting%20that%20structured%20language%20generation%0Acapabilities%20provide%20an%20advantageous%20starting%20point%20for%20reinforcement%20learning%0Ain%20function%20calling%20tasks.%20We%20will%20release%20all%20the%20code%2C%20models%20and%20dataset%20to%0Abenefit%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05118v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Superior%2520Function%2520Calls%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DBingguang%2520Hao%2520and%2520Maolin%2520Wang%2520and%2520Zengzhuang%2520Xu%2520and%2520Yicheng%2520Chen%2520and%2520Cunyin%2520Peng%2520and%2520Jinjie%2520GU%2520and%2520Chenyi%2520Zhuang%26entry.1292438233%3D%2520%2520Function%2520calling%2520capabilities%2520are%2520crucial%2520for%2520deploying%2520Large%2520Language%2520Models%250Ain%2520real-world%2520applications%252C%2520yet%2520current%2520training%2520approaches%2520fail%2520to%2520develop%250Arobust%2520reasoning%2520strategies.%2520Supervised%2520fine-tuning%2520produces%2520models%2520that%2520rely%250Aon%2520superficial%2520pattern%2520matching%252C%2520while%2520standard%2520reinforcement%2520learning%2520methods%250Astruggle%2520with%2520the%2520complex%2520action%2520space%2520of%2520structured%2520function%2520calls.%2520We%2520present%250Aa%2520novel%2520reinforcement%2520learning%2520framework%2520designed%2520to%2520enhance%2520group%2520relative%250Apolicy%2520optimization%2520through%2520strategic%2520entropy%2520based%2520exploration%2520specifically%250Atailored%2520for%2520function%2520calling%2520tasks.%2520Our%2520approach%2520addresses%2520three%2520critical%250Achallenges%2520in%2520function%2520calling%253A%2520insufficient%2520exploration%2520during%2520policy%250Alearning%252C%2520lack%2520of%2520structured%2520reasoning%2520in%2520chain-of-thought%2520generation%252C%2520and%250Ainadequate%2520verification%2520of%2520parameter%2520extraction.%2520Our%2520two-stage%2520data%2520preparation%250Apipeline%2520ensures%2520high-quality%2520training%2520samples%2520through%2520iterative%2520LLM%2520evaluation%250Aand%2520abstract%2520syntax%2520tree%2520validation.%2520Extensive%2520experiments%2520on%2520the%2520Berkeley%250AFunction%2520Calling%2520Leaderboard%2520demonstrate%2520that%2520this%2520framework%2520achieves%250Astate-of-the-art%2520performance%2520among%2520open-source%2520models%2520with%252086.02%255C%2525%2520overall%250Aaccuracy%252C%2520outperforming%2520standard%2520GRPO%2520by%2520up%2520to%25206%255C%2525%2520on%2520complex%2520multi-function%250Ascenarios.%2520Notably%252C%2520our%2520method%2520shows%2520particularly%2520strong%2520improvements%2520on%250Acode-pretrained%2520models%252C%2520suggesting%2520that%2520structured%2520language%2520generation%250Acapabilities%2520provide%2520an%2520advantageous%2520starting%2520point%2520for%2520reinforcement%2520learning%250Ain%2520function%2520calling%2520tasks.%2520We%2520will%2520release%2520all%2520the%2520code%252C%2520models%2520and%2520dataset%2520to%250Abenefit%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05118v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Superior%20Function%20Calls%20via%20Reinforcement%20Learning&entry.906535625=Bingguang%20Hao%20and%20Maolin%20Wang%20and%20Zengzhuang%20Xu%20and%20Yicheng%20Chen%20and%20Cunyin%20Peng%20and%20Jinjie%20GU%20and%20Chenyi%20Zhuang&entry.1292438233=%20%20Function%20calling%20capabilities%20are%20crucial%20for%20deploying%20Large%20Language%20Models%0Ain%20real-world%20applications%2C%20yet%20current%20training%20approaches%20fail%20to%20develop%0Arobust%20reasoning%20strategies.%20Supervised%20fine-tuning%20produces%20models%20that%20rely%0Aon%20superficial%20pattern%20matching%2C%20while%20standard%20reinforcement%20learning%20methods%0Astruggle%20with%20the%20complex%20action%20space%20of%20structured%20function%20calls.%20We%20present%0Aa%20novel%20reinforcement%20learning%20framework%20designed%20to%20enhance%20group%20relative%0Apolicy%20optimization%20through%20strategic%20entropy%20based%20exploration%20specifically%0Atailored%20for%20function%20calling%20tasks.%20Our%20approach%20addresses%20three%20critical%0Achallenges%20in%20function%20calling%3A%20insufficient%20exploration%20during%20policy%0Alearning%2C%20lack%20of%20structured%20reasoning%20in%20chain-of-thought%20generation%2C%20and%0Ainadequate%20verification%20of%20parameter%20extraction.%20Our%20two-stage%20data%20preparation%0Apipeline%20ensures%20high-quality%20training%20samples%20through%20iterative%20LLM%20evaluation%0Aand%20abstract%20syntax%20tree%20validation.%20Extensive%20experiments%20on%20the%20Berkeley%0AFunction%20Calling%20Leaderboard%20demonstrate%20that%20this%20framework%20achieves%0Astate-of-the-art%20performance%20among%20open-source%20models%20with%2086.02%5C%25%20overall%0Aaccuracy%2C%20outperforming%20standard%20GRPO%20by%20up%20to%206%5C%25%20on%20complex%20multi-function%0Ascenarios.%20Notably%2C%20our%20method%20shows%20particularly%20strong%20improvements%20on%0Acode-pretrained%20models%2C%20suggesting%20that%20structured%20language%20generation%0Acapabilities%20provide%20an%20advantageous%20starting%20point%20for%20reinforcement%20learning%0Ain%20function%20calling%20tasks.%20We%20will%20release%20all%20the%20code%2C%20models%20and%20dataset%20to%0Abenefit%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05118v3&entry.124074799=Read"},
{"title": "Pedestrian Dead Reckoning using Invariant Extended Kalman Filter", "author": "Jingran Zhang and Zhengzhang Yan and Yiming Chen and Zeqiang He and Jiahao Chen", "abstract": "  This paper presents a cost-effective inertial pedestrian dead reckoning\nmethod for the bipedal robot in the GPS-denied environment. Each time when the\ninertial measurement unit (IMU) is on the stance foot, a stationary\npseudo-measurement can be executed to provide innovation to the IMU measurement\nbased prediction. The matrix Lie group based theoretical development of the\nadopted invariant extended Kalman filter (InEKF) is set forth for tutorial\npurpose. Three experiments are conducted to compare between InEKF and standard\nEKF, including motion capture benchmark experiment, large-scale multi-floor\nwalking experiment, and bipedal robot experiment, as an effort to show our\nmethod's feasibility in real-world robot system. In addition, a sensitivity\nanalysis is included to show that InEKF is much easier to tune than EKF.\n", "link": "http://arxiv.org/abs/2508.11396v1", "date": "2025-08-15", "relevancy": 2.0057, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.528}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5192}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pedestrian%20Dead%20Reckoning%20using%20Invariant%20Extended%20Kalman%20Filter&body=Title%3A%20Pedestrian%20Dead%20Reckoning%20using%20Invariant%20Extended%20Kalman%20Filter%0AAuthor%3A%20Jingran%20Zhang%20and%20Zhengzhang%20Yan%20and%20Yiming%20Chen%20and%20Zeqiang%20He%20and%20Jiahao%20Chen%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20cost-effective%20inertial%20pedestrian%20dead%20reckoning%0Amethod%20for%20the%20bipedal%20robot%20in%20the%20GPS-denied%20environment.%20Each%20time%20when%20the%0Ainertial%20measurement%20unit%20%28IMU%29%20is%20on%20the%20stance%20foot%2C%20a%20stationary%0Apseudo-measurement%20can%20be%20executed%20to%20provide%20innovation%20to%20the%20IMU%20measurement%0Abased%20prediction.%20The%20matrix%20Lie%20group%20based%20theoretical%20development%20of%20the%0Aadopted%20invariant%20extended%20Kalman%20filter%20%28InEKF%29%20is%20set%20forth%20for%20tutorial%0Apurpose.%20Three%20experiments%20are%20conducted%20to%20compare%20between%20InEKF%20and%20standard%0AEKF%2C%20including%20motion%20capture%20benchmark%20experiment%2C%20large-scale%20multi-floor%0Awalking%20experiment%2C%20and%20bipedal%20robot%20experiment%2C%20as%20an%20effort%20to%20show%20our%0Amethod%27s%20feasibility%20in%20real-world%20robot%20system.%20In%20addition%2C%20a%20sensitivity%0Aanalysis%20is%20included%20to%20show%20that%20InEKF%20is%20much%20easier%20to%20tune%20than%20EKF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPedestrian%2520Dead%2520Reckoning%2520using%2520Invariant%2520Extended%2520Kalman%2520Filter%26entry.906535625%3DJingran%2520Zhang%2520and%2520Zhengzhang%2520Yan%2520and%2520Yiming%2520Chen%2520and%2520Zeqiang%2520He%2520and%2520Jiahao%2520Chen%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520cost-effective%2520inertial%2520pedestrian%2520dead%2520reckoning%250Amethod%2520for%2520the%2520bipedal%2520robot%2520in%2520the%2520GPS-denied%2520environment.%2520Each%2520time%2520when%2520the%250Ainertial%2520measurement%2520unit%2520%2528IMU%2529%2520is%2520on%2520the%2520stance%2520foot%252C%2520a%2520stationary%250Apseudo-measurement%2520can%2520be%2520executed%2520to%2520provide%2520innovation%2520to%2520the%2520IMU%2520measurement%250Abased%2520prediction.%2520The%2520matrix%2520Lie%2520group%2520based%2520theoretical%2520development%2520of%2520the%250Aadopted%2520invariant%2520extended%2520Kalman%2520filter%2520%2528InEKF%2529%2520is%2520set%2520forth%2520for%2520tutorial%250Apurpose.%2520Three%2520experiments%2520are%2520conducted%2520to%2520compare%2520between%2520InEKF%2520and%2520standard%250AEKF%252C%2520including%2520motion%2520capture%2520benchmark%2520experiment%252C%2520large-scale%2520multi-floor%250Awalking%2520experiment%252C%2520and%2520bipedal%2520robot%2520experiment%252C%2520as%2520an%2520effort%2520to%2520show%2520our%250Amethod%2527s%2520feasibility%2520in%2520real-world%2520robot%2520system.%2520In%2520addition%252C%2520a%2520sensitivity%250Aanalysis%2520is%2520included%2520to%2520show%2520that%2520InEKF%2520is%2520much%2520easier%2520to%2520tune%2520than%2520EKF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pedestrian%20Dead%20Reckoning%20using%20Invariant%20Extended%20Kalman%20Filter&entry.906535625=Jingran%20Zhang%20and%20Zhengzhang%20Yan%20and%20Yiming%20Chen%20and%20Zeqiang%20He%20and%20Jiahao%20Chen&entry.1292438233=%20%20This%20paper%20presents%20a%20cost-effective%20inertial%20pedestrian%20dead%20reckoning%0Amethod%20for%20the%20bipedal%20robot%20in%20the%20GPS-denied%20environment.%20Each%20time%20when%20the%0Ainertial%20measurement%20unit%20%28IMU%29%20is%20on%20the%20stance%20foot%2C%20a%20stationary%0Apseudo-measurement%20can%20be%20executed%20to%20provide%20innovation%20to%20the%20IMU%20measurement%0Abased%20prediction.%20The%20matrix%20Lie%20group%20based%20theoretical%20development%20of%20the%0Aadopted%20invariant%20extended%20Kalman%20filter%20%28InEKF%29%20is%20set%20forth%20for%20tutorial%0Apurpose.%20Three%20experiments%20are%20conducted%20to%20compare%20between%20InEKF%20and%20standard%0AEKF%2C%20including%20motion%20capture%20benchmark%20experiment%2C%20large-scale%20multi-floor%0Awalking%20experiment%2C%20and%20bipedal%20robot%20experiment%2C%20as%20an%20effort%20to%20show%20our%0Amethod%27s%20feasibility%20in%20real-world%20robot%20system.%20In%20addition%2C%20a%20sensitivity%0Aanalysis%20is%20included%20to%20show%20that%20InEKF%20is%20much%20easier%20to%20tune%20than%20EKF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11396v1&entry.124074799=Read"},
{"title": "ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning\n  Framework for Implicit Hate Speech Detection in Videos", "author": "Mohammad Zia Ur Rehman and Anukriti Bhatnagar and Omkar Kabde and Shubhi Bansal and Nagendra Kumar", "abstract": "  The existing research has primarily focused on text and image-based hate\nspeech detection, video-based approaches remain underexplored. In this work, we\nintroduce a novel dataset, ImpliHateVid, specifically curated for implicit hate\nspeech detection in videos. ImpliHateVid consists of 2,009 videos comprising\n509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,\nmaking it one of the first large-scale video datasets dedicated to implicit\nhate detection. We also propose a novel two-stage contrastive learning\nframework for hate speech detection in videos. In the first stage, we train\nmodality-specific encoders for audio, text, and image using contrastive loss by\nconcatenating features from the three encoders. In the second stage, we train\ncross-encoders using contrastive learning to refine multimodal representations.\nAdditionally, we incorporate sentiment, emotion, and caption-based features to\nenhance implicit hate detection. We evaluate our method on two datasets,\nImpliHateVid for implicit hate speech detection and another dataset for general\nhate speech detection in videos, HateMM dataset, demonstrating the\neffectiveness of the proposed multimodal contrastive learning for hateful\ncontent detection in videos and the significance of our dataset.\n", "link": "http://arxiv.org/abs/2508.06570v2", "date": "2025-08-15", "relevancy": 1.9999, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5135}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4942}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImpliHateVid%3A%20A%20Benchmark%20Dataset%20and%20Two-stage%20Contrastive%20Learning%0A%20%20Framework%20for%20Implicit%20Hate%20Speech%20Detection%20in%20Videos&body=Title%3A%20ImpliHateVid%3A%20A%20Benchmark%20Dataset%20and%20Two-stage%20Contrastive%20Learning%0A%20%20Framework%20for%20Implicit%20Hate%20Speech%20Detection%20in%20Videos%0AAuthor%3A%20Mohammad%20Zia%20Ur%20Rehman%20and%20Anukriti%20Bhatnagar%20and%20Omkar%20Kabde%20and%20Shubhi%20Bansal%20and%20Nagendra%20Kumar%0AAbstract%3A%20%20%20The%20existing%20research%20has%20primarily%20focused%20on%20text%20and%20image-based%20hate%0Aspeech%20detection%2C%20video-based%20approaches%20remain%20underexplored.%20In%20this%20work%2C%20we%0Aintroduce%20a%20novel%20dataset%2C%20ImpliHateVid%2C%20specifically%20curated%20for%20implicit%20hate%0Aspeech%20detection%20in%20videos.%20ImpliHateVid%20consists%20of%202%2C009%20videos%20comprising%0A509%20implicit%20hate%20videos%2C%20500%20explicit%20hate%20videos%2C%20and%201%2C000%20non-hate%20videos%2C%0Amaking%20it%20one%20of%20the%20first%20large-scale%20video%20datasets%20dedicated%20to%20implicit%0Ahate%20detection.%20We%20also%20propose%20a%20novel%20two-stage%20contrastive%20learning%0Aframework%20for%20hate%20speech%20detection%20in%20videos.%20In%20the%20first%20stage%2C%20we%20train%0Amodality-specific%20encoders%20for%20audio%2C%20text%2C%20and%20image%20using%20contrastive%20loss%20by%0Aconcatenating%20features%20from%20the%20three%20encoders.%20In%20the%20second%20stage%2C%20we%20train%0Across-encoders%20using%20contrastive%20learning%20to%20refine%20multimodal%20representations.%0AAdditionally%2C%20we%20incorporate%20sentiment%2C%20emotion%2C%20and%20caption-based%20features%20to%0Aenhance%20implicit%20hate%20detection.%20We%20evaluate%20our%20method%20on%20two%20datasets%2C%0AImpliHateVid%20for%20implicit%20hate%20speech%20detection%20and%20another%20dataset%20for%20general%0Ahate%20speech%20detection%20in%20videos%2C%20HateMM%20dataset%2C%20demonstrating%20the%0Aeffectiveness%20of%20the%20proposed%20multimodal%20contrastive%20learning%20for%20hateful%0Acontent%20detection%20in%20videos%20and%20the%20significance%20of%20our%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06570v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpliHateVid%253A%2520A%2520Benchmark%2520Dataset%2520and%2520Two-stage%2520Contrastive%2520Learning%250A%2520%2520Framework%2520for%2520Implicit%2520Hate%2520Speech%2520Detection%2520in%2520Videos%26entry.906535625%3DMohammad%2520Zia%2520Ur%2520Rehman%2520and%2520Anukriti%2520Bhatnagar%2520and%2520Omkar%2520Kabde%2520and%2520Shubhi%2520Bansal%2520and%2520Nagendra%2520Kumar%26entry.1292438233%3D%2520%2520The%2520existing%2520research%2520has%2520primarily%2520focused%2520on%2520text%2520and%2520image-based%2520hate%250Aspeech%2520detection%252C%2520video-based%2520approaches%2520remain%2520underexplored.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520novel%2520dataset%252C%2520ImpliHateVid%252C%2520specifically%2520curated%2520for%2520implicit%2520hate%250Aspeech%2520detection%2520in%2520videos.%2520ImpliHateVid%2520consists%2520of%25202%252C009%2520videos%2520comprising%250A509%2520implicit%2520hate%2520videos%252C%2520500%2520explicit%2520hate%2520videos%252C%2520and%25201%252C000%2520non-hate%2520videos%252C%250Amaking%2520it%2520one%2520of%2520the%2520first%2520large-scale%2520video%2520datasets%2520dedicated%2520to%2520implicit%250Ahate%2520detection.%2520We%2520also%2520propose%2520a%2520novel%2520two-stage%2520contrastive%2520learning%250Aframework%2520for%2520hate%2520speech%2520detection%2520in%2520videos.%2520In%2520the%2520first%2520stage%252C%2520we%2520train%250Amodality-specific%2520encoders%2520for%2520audio%252C%2520text%252C%2520and%2520image%2520using%2520contrastive%2520loss%2520by%250Aconcatenating%2520features%2520from%2520the%2520three%2520encoders.%2520In%2520the%2520second%2520stage%252C%2520we%2520train%250Across-encoders%2520using%2520contrastive%2520learning%2520to%2520refine%2520multimodal%2520representations.%250AAdditionally%252C%2520we%2520incorporate%2520sentiment%252C%2520emotion%252C%2520and%2520caption-based%2520features%2520to%250Aenhance%2520implicit%2520hate%2520detection.%2520We%2520evaluate%2520our%2520method%2520on%2520two%2520datasets%252C%250AImpliHateVid%2520for%2520implicit%2520hate%2520speech%2520detection%2520and%2520another%2520dataset%2520for%2520general%250Ahate%2520speech%2520detection%2520in%2520videos%252C%2520HateMM%2520dataset%252C%2520demonstrating%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520multimodal%2520contrastive%2520learning%2520for%2520hateful%250Acontent%2520detection%2520in%2520videos%2520and%2520the%2520significance%2520of%2520our%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06570v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImpliHateVid%3A%20A%20Benchmark%20Dataset%20and%20Two-stage%20Contrastive%20Learning%0A%20%20Framework%20for%20Implicit%20Hate%20Speech%20Detection%20in%20Videos&entry.906535625=Mohammad%20Zia%20Ur%20Rehman%20and%20Anukriti%20Bhatnagar%20and%20Omkar%20Kabde%20and%20Shubhi%20Bansal%20and%20Nagendra%20Kumar&entry.1292438233=%20%20The%20existing%20research%20has%20primarily%20focused%20on%20text%20and%20image-based%20hate%0Aspeech%20detection%2C%20video-based%20approaches%20remain%20underexplored.%20In%20this%20work%2C%20we%0Aintroduce%20a%20novel%20dataset%2C%20ImpliHateVid%2C%20specifically%20curated%20for%20implicit%20hate%0Aspeech%20detection%20in%20videos.%20ImpliHateVid%20consists%20of%202%2C009%20videos%20comprising%0A509%20implicit%20hate%20videos%2C%20500%20explicit%20hate%20videos%2C%20and%201%2C000%20non-hate%20videos%2C%0Amaking%20it%20one%20of%20the%20first%20large-scale%20video%20datasets%20dedicated%20to%20implicit%0Ahate%20detection.%20We%20also%20propose%20a%20novel%20two-stage%20contrastive%20learning%0Aframework%20for%20hate%20speech%20detection%20in%20videos.%20In%20the%20first%20stage%2C%20we%20train%0Amodality-specific%20encoders%20for%20audio%2C%20text%2C%20and%20image%20using%20contrastive%20loss%20by%0Aconcatenating%20features%20from%20the%20three%20encoders.%20In%20the%20second%20stage%2C%20we%20train%0Across-encoders%20using%20contrastive%20learning%20to%20refine%20multimodal%20representations.%0AAdditionally%2C%20we%20incorporate%20sentiment%2C%20emotion%2C%20and%20caption-based%20features%20to%0Aenhance%20implicit%20hate%20detection.%20We%20evaluate%20our%20method%20on%20two%20datasets%2C%0AImpliHateVid%20for%20implicit%20hate%20speech%20detection%20and%20another%20dataset%20for%20general%0Ahate%20speech%20detection%20in%20videos%2C%20HateMM%20dataset%2C%20demonstrating%20the%0Aeffectiveness%20of%20the%20proposed%20multimodal%20contrastive%20learning%20for%20hateful%0Acontent%20detection%20in%20videos%20and%20the%20significance%20of%20our%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06570v2&entry.124074799=Read"},
{"title": "L3AC: Towards a Lightweight and Lossless Audio Codec", "author": "Linwei Zhai and Han Ding and Cui Zhao and fei wang and Ge Wang and Wang Zhi and Wei Xi", "abstract": "  Neural audio codecs have recently gained traction for their ability to\ncompress high-fidelity audio and provide discrete tokens for generative\nmodeling. However, leading approaches often rely on resource-intensive models\nand complex multi-quantizer architectures, limiting their practicality in\nreal-world applications. In this work, we introduce L3AC, a lightweight neural\naudio codec that addresses these challenges by leveraging a single quantizer\nand a highly efficient architecture. To enhance reconstruction fidelity while\nminimizing model complexity, L3AC explores streamlined convolutional networks\nand local Transformer modules, alongside TConv--a novel structure designed to\ncapture acoustic variations across multiple temporal scales. Despite its\ncompact design, extensive experiments across diverse datasets demonstrate that\nL3AC matches or exceeds the reconstruction quality of leading codecs while\nreducing computational overhead by an order of magnitude. The single-quantizer\ndesign further enhances its adaptability for downstream tasks. The source code\nis publicly available at https://github.com/zhai-lw/L3AC.\n", "link": "http://arxiv.org/abs/2504.04949v2", "date": "2025-08-15", "relevancy": 1.9897, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5135}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4864}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L3AC%3A%20Towards%20a%20Lightweight%20and%20Lossless%20Audio%20Codec&body=Title%3A%20L3AC%3A%20Towards%20a%20Lightweight%20and%20Lossless%20Audio%20Codec%0AAuthor%3A%20Linwei%20Zhai%20and%20Han%20Ding%20and%20Cui%20Zhao%20and%20fei%20wang%20and%20Ge%20Wang%20and%20Wang%20Zhi%20and%20Wei%20Xi%0AAbstract%3A%20%20%20Neural%20audio%20codecs%20have%20recently%20gained%20traction%20for%20their%20ability%20to%0Acompress%20high-fidelity%20audio%20and%20provide%20discrete%20tokens%20for%20generative%0Amodeling.%20However%2C%20leading%20approaches%20often%20rely%20on%20resource-intensive%20models%0Aand%20complex%20multi-quantizer%20architectures%2C%20limiting%20their%20practicality%20in%0Areal-world%20applications.%20In%20this%20work%2C%20we%20introduce%20L3AC%2C%20a%20lightweight%20neural%0Aaudio%20codec%20that%20addresses%20these%20challenges%20by%20leveraging%20a%20single%20quantizer%0Aand%20a%20highly%20efficient%20architecture.%20To%20enhance%20reconstruction%20fidelity%20while%0Aminimizing%20model%20complexity%2C%20L3AC%20explores%20streamlined%20convolutional%20networks%0Aand%20local%20Transformer%20modules%2C%20alongside%20TConv--a%20novel%20structure%20designed%20to%0Acapture%20acoustic%20variations%20across%20multiple%20temporal%20scales.%20Despite%20its%0Acompact%20design%2C%20extensive%20experiments%20across%20diverse%20datasets%20demonstrate%20that%0AL3AC%20matches%20or%20exceeds%20the%20reconstruction%20quality%20of%20leading%20codecs%20while%0Areducing%20computational%20overhead%20by%20an%20order%20of%20magnitude.%20The%20single-quantizer%0Adesign%20further%20enhances%20its%20adaptability%20for%20downstream%20tasks.%20The%20source%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/zhai-lw/L3AC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04949v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL3AC%253A%2520Towards%2520a%2520Lightweight%2520and%2520Lossless%2520Audio%2520Codec%26entry.906535625%3DLinwei%2520Zhai%2520and%2520Han%2520Ding%2520and%2520Cui%2520Zhao%2520and%2520fei%2520wang%2520and%2520Ge%2520Wang%2520and%2520Wang%2520Zhi%2520and%2520Wei%2520Xi%26entry.1292438233%3D%2520%2520Neural%2520audio%2520codecs%2520have%2520recently%2520gained%2520traction%2520for%2520their%2520ability%2520to%250Acompress%2520high-fidelity%2520audio%2520and%2520provide%2520discrete%2520tokens%2520for%2520generative%250Amodeling.%2520However%252C%2520leading%2520approaches%2520often%2520rely%2520on%2520resource-intensive%2520models%250Aand%2520complex%2520multi-quantizer%2520architectures%252C%2520limiting%2520their%2520practicality%2520in%250Areal-world%2520applications.%2520In%2520this%2520work%252C%2520we%2520introduce%2520L3AC%252C%2520a%2520lightweight%2520neural%250Aaudio%2520codec%2520that%2520addresses%2520these%2520challenges%2520by%2520leveraging%2520a%2520single%2520quantizer%250Aand%2520a%2520highly%2520efficient%2520architecture.%2520To%2520enhance%2520reconstruction%2520fidelity%2520while%250Aminimizing%2520model%2520complexity%252C%2520L3AC%2520explores%2520streamlined%2520convolutional%2520networks%250Aand%2520local%2520Transformer%2520modules%252C%2520alongside%2520TConv--a%2520novel%2520structure%2520designed%2520to%250Acapture%2520acoustic%2520variations%2520across%2520multiple%2520temporal%2520scales.%2520Despite%2520its%250Acompact%2520design%252C%2520extensive%2520experiments%2520across%2520diverse%2520datasets%2520demonstrate%2520that%250AL3AC%2520matches%2520or%2520exceeds%2520the%2520reconstruction%2520quality%2520of%2520leading%2520codecs%2520while%250Areducing%2520computational%2520overhead%2520by%2520an%2520order%2520of%2520magnitude.%2520The%2520single-quantizer%250Adesign%2520further%2520enhances%2520its%2520adaptability%2520for%2520downstream%2520tasks.%2520The%2520source%2520code%250Ais%2520publicly%2520available%2520at%2520https%253A//github.com/zhai-lw/L3AC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04949v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L3AC%3A%20Towards%20a%20Lightweight%20and%20Lossless%20Audio%20Codec&entry.906535625=Linwei%20Zhai%20and%20Han%20Ding%20and%20Cui%20Zhao%20and%20fei%20wang%20and%20Ge%20Wang%20and%20Wang%20Zhi%20and%20Wei%20Xi&entry.1292438233=%20%20Neural%20audio%20codecs%20have%20recently%20gained%20traction%20for%20their%20ability%20to%0Acompress%20high-fidelity%20audio%20and%20provide%20discrete%20tokens%20for%20generative%0Amodeling.%20However%2C%20leading%20approaches%20often%20rely%20on%20resource-intensive%20models%0Aand%20complex%20multi-quantizer%20architectures%2C%20limiting%20their%20practicality%20in%0Areal-world%20applications.%20In%20this%20work%2C%20we%20introduce%20L3AC%2C%20a%20lightweight%20neural%0Aaudio%20codec%20that%20addresses%20these%20challenges%20by%20leveraging%20a%20single%20quantizer%0Aand%20a%20highly%20efficient%20architecture.%20To%20enhance%20reconstruction%20fidelity%20while%0Aminimizing%20model%20complexity%2C%20L3AC%20explores%20streamlined%20convolutional%20networks%0Aand%20local%20Transformer%20modules%2C%20alongside%20TConv--a%20novel%20structure%20designed%20to%0Acapture%20acoustic%20variations%20across%20multiple%20temporal%20scales.%20Despite%20its%0Acompact%20design%2C%20extensive%20experiments%20across%20diverse%20datasets%20demonstrate%20that%0AL3AC%20matches%20or%20exceeds%20the%20reconstruction%20quality%20of%20leading%20codecs%20while%0Areducing%20computational%20overhead%20by%20an%20order%20of%20magnitude.%20The%20single-quantizer%0Adesign%20further%20enhances%20its%20adaptability%20for%20downstream%20tasks.%20The%20source%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/zhai-lw/L3AC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04949v2&entry.124074799=Read"},
{"title": "Synthetic Data for Robust Stroke Segmentation", "author": "Liam Chalcroft and Ioannis Pappas and Cathy J. Price and John Ashburner", "abstract": "  Current deep learning-based approaches to lesion segmentation in neuroimaging\noften depend on high-resolution images and extensive annotated data, limiting\nclinical applicability. This paper introduces a novel synthetic data framework\ntailored for stroke lesion segmentation, expanding the SynthSeg methodology to\nincorporate lesion-specific augmentations that simulate diverse pathological\nfeatures. Using a modified nnUNet architecture, our approach trains models with\nlabel maps from healthy and stroke datasets, facilitating segmentation across\nboth normal and pathological tissue without reliance on specific sequence-based\ntraining. Evaluation across in-domain and out-of-domain (OOD) datasets reveals\nthat our method matches state-of-the-art performance within the training domain\nand significantly outperforms existing methods on OOD data. By minimizing\ndependence on large annotated datasets and allowing for cross-sequence\napplicability, our framework holds potential to improve clinical neuroimaging\nworkflows, particularly in stroke pathology. PyTorch training code and weights\nare publicly available at https://github.com/liamchalcroft/SynthStroke, along\nwith an SPM toolbox featuring a plug-and-play model at\nhttps://github.com/liamchalcroft/SynthStrokeSPM.\n", "link": "http://arxiv.org/abs/2404.01946v3", "date": "2025-08-15", "relevancy": 1.9853, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5214}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Data%20for%20Robust%20Stroke%20Segmentation&body=Title%3A%20Synthetic%20Data%20for%20Robust%20Stroke%20Segmentation%0AAuthor%3A%20Liam%20Chalcroft%20and%20Ioannis%20Pappas%20and%20Cathy%20J.%20Price%20and%20John%20Ashburner%0AAbstract%3A%20%20%20Current%20deep%20learning-based%20approaches%20to%20lesion%20segmentation%20in%20neuroimaging%0Aoften%20depend%20on%20high-resolution%20images%20and%20extensive%20annotated%20data%2C%20limiting%0Aclinical%20applicability.%20This%20paper%20introduces%20a%20novel%20synthetic%20data%20framework%0Atailored%20for%20stroke%20lesion%20segmentation%2C%20expanding%20the%20SynthSeg%20methodology%20to%0Aincorporate%20lesion-specific%20augmentations%20that%20simulate%20diverse%20pathological%0Afeatures.%20Using%20a%20modified%20nnUNet%20architecture%2C%20our%20approach%20trains%20models%20with%0Alabel%20maps%20from%20healthy%20and%20stroke%20datasets%2C%20facilitating%20segmentation%20across%0Aboth%20normal%20and%20pathological%20tissue%20without%20reliance%20on%20specific%20sequence-based%0Atraining.%20Evaluation%20across%20in-domain%20and%20out-of-domain%20%28OOD%29%20datasets%20reveals%0Athat%20our%20method%20matches%20state-of-the-art%20performance%20within%20the%20training%20domain%0Aand%20significantly%20outperforms%20existing%20methods%20on%20OOD%20data.%20By%20minimizing%0Adependence%20on%20large%20annotated%20datasets%20and%20allowing%20for%20cross-sequence%0Aapplicability%2C%20our%20framework%20holds%20potential%20to%20improve%20clinical%20neuroimaging%0Aworkflows%2C%20particularly%20in%20stroke%20pathology.%20PyTorch%20training%20code%20and%20weights%0Aare%20publicly%20available%20at%20https%3A//github.com/liamchalcroft/SynthStroke%2C%20along%0Awith%20an%20SPM%20toolbox%20featuring%20a%20plug-and-play%20model%20at%0Ahttps%3A//github.com/liamchalcroft/SynthStrokeSPM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01946v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Data%2520for%2520Robust%2520Stroke%2520Segmentation%26entry.906535625%3DLiam%2520Chalcroft%2520and%2520Ioannis%2520Pappas%2520and%2520Cathy%2520J.%2520Price%2520and%2520John%2520Ashburner%26entry.1292438233%3D%2520%2520Current%2520deep%2520learning-based%2520approaches%2520to%2520lesion%2520segmentation%2520in%2520neuroimaging%250Aoften%2520depend%2520on%2520high-resolution%2520images%2520and%2520extensive%2520annotated%2520data%252C%2520limiting%250Aclinical%2520applicability.%2520This%2520paper%2520introduces%2520a%2520novel%2520synthetic%2520data%2520framework%250Atailored%2520for%2520stroke%2520lesion%2520segmentation%252C%2520expanding%2520the%2520SynthSeg%2520methodology%2520to%250Aincorporate%2520lesion-specific%2520augmentations%2520that%2520simulate%2520diverse%2520pathological%250Afeatures.%2520Using%2520a%2520modified%2520nnUNet%2520architecture%252C%2520our%2520approach%2520trains%2520models%2520with%250Alabel%2520maps%2520from%2520healthy%2520and%2520stroke%2520datasets%252C%2520facilitating%2520segmentation%2520across%250Aboth%2520normal%2520and%2520pathological%2520tissue%2520without%2520reliance%2520on%2520specific%2520sequence-based%250Atraining.%2520Evaluation%2520across%2520in-domain%2520and%2520out-of-domain%2520%2528OOD%2529%2520datasets%2520reveals%250Athat%2520our%2520method%2520matches%2520state-of-the-art%2520performance%2520within%2520the%2520training%2520domain%250Aand%2520significantly%2520outperforms%2520existing%2520methods%2520on%2520OOD%2520data.%2520By%2520minimizing%250Adependence%2520on%2520large%2520annotated%2520datasets%2520and%2520allowing%2520for%2520cross-sequence%250Aapplicability%252C%2520our%2520framework%2520holds%2520potential%2520to%2520improve%2520clinical%2520neuroimaging%250Aworkflows%252C%2520particularly%2520in%2520stroke%2520pathology.%2520PyTorch%2520training%2520code%2520and%2520weights%250Aare%2520publicly%2520available%2520at%2520https%253A//github.com/liamchalcroft/SynthStroke%252C%2520along%250Awith%2520an%2520SPM%2520toolbox%2520featuring%2520a%2520plug-and-play%2520model%2520at%250Ahttps%253A//github.com/liamchalcroft/SynthStrokeSPM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01946v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Data%20for%20Robust%20Stroke%20Segmentation&entry.906535625=Liam%20Chalcroft%20and%20Ioannis%20Pappas%20and%20Cathy%20J.%20Price%20and%20John%20Ashburner&entry.1292438233=%20%20Current%20deep%20learning-based%20approaches%20to%20lesion%20segmentation%20in%20neuroimaging%0Aoften%20depend%20on%20high-resolution%20images%20and%20extensive%20annotated%20data%2C%20limiting%0Aclinical%20applicability.%20This%20paper%20introduces%20a%20novel%20synthetic%20data%20framework%0Atailored%20for%20stroke%20lesion%20segmentation%2C%20expanding%20the%20SynthSeg%20methodology%20to%0Aincorporate%20lesion-specific%20augmentations%20that%20simulate%20diverse%20pathological%0Afeatures.%20Using%20a%20modified%20nnUNet%20architecture%2C%20our%20approach%20trains%20models%20with%0Alabel%20maps%20from%20healthy%20and%20stroke%20datasets%2C%20facilitating%20segmentation%20across%0Aboth%20normal%20and%20pathological%20tissue%20without%20reliance%20on%20specific%20sequence-based%0Atraining.%20Evaluation%20across%20in-domain%20and%20out-of-domain%20%28OOD%29%20datasets%20reveals%0Athat%20our%20method%20matches%20state-of-the-art%20performance%20within%20the%20training%20domain%0Aand%20significantly%20outperforms%20existing%20methods%20on%20OOD%20data.%20By%20minimizing%0Adependence%20on%20large%20annotated%20datasets%20and%20allowing%20for%20cross-sequence%0Aapplicability%2C%20our%20framework%20holds%20potential%20to%20improve%20clinical%20neuroimaging%0Aworkflows%2C%20particularly%20in%20stroke%20pathology.%20PyTorch%20training%20code%20and%20weights%0Aare%20publicly%20available%20at%20https%3A//github.com/liamchalcroft/SynthStroke%2C%20along%0Awith%20an%20SPM%20toolbox%20featuring%20a%20plug-and-play%20model%20at%0Ahttps%3A//github.com/liamchalcroft/SynthStrokeSPM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01946v3&entry.124074799=Read"},
{"title": "Handwritten Text Recognition of Historical Manuscripts Using\n  Transformer-Based Models", "author": "Erez Meoded", "abstract": "  Historical handwritten text recognition (HTR) is essential for unlocking the\ncultural and scholarly value of archival documents, yet digitization is often\nhindered by scarce transcriptions, linguistic variation, and highly diverse\nhandwriting styles. In this study, we apply TrOCR, a state-of-the-art\ntransformer-based HTR model, to 16th-century Latin manuscripts authored by\nRudolf Gwalther. We investigate targeted image preprocessing and a broad suite\nof data augmentation techniques, introducing four novel augmentation methods\ndesigned specifically for historical handwriting characteristics. We also\nevaluate ensemble learning approaches to leverage the complementary strengths\nof augmentation-trained models. On the Gwalther dataset, our best single-model\naugmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a\ntop-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative\nimprovement over the best reported TrOCR_BASE result and a 42% improvement over\nthe previous state of the art. These results highlight the impact of\ndomain-specific augmentations and ensemble strategies in advancing HTR\nperformance for historical manuscripts.\n", "link": "http://arxiv.org/abs/2508.11499v1", "date": "2025-08-15", "relevancy": 1.9841, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5489}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4869}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Handwritten%20Text%20Recognition%20of%20Historical%20Manuscripts%20Using%0A%20%20Transformer-Based%20Models&body=Title%3A%20Handwritten%20Text%20Recognition%20of%20Historical%20Manuscripts%20Using%0A%20%20Transformer-Based%20Models%0AAuthor%3A%20Erez%20Meoded%0AAbstract%3A%20%20%20Historical%20handwritten%20text%20recognition%20%28HTR%29%20is%20essential%20for%20unlocking%20the%0Acultural%20and%20scholarly%20value%20of%20archival%20documents%2C%20yet%20digitization%20is%20often%0Ahindered%20by%20scarce%20transcriptions%2C%20linguistic%20variation%2C%20and%20highly%20diverse%0Ahandwriting%20styles.%20In%20this%20study%2C%20we%20apply%20TrOCR%2C%20a%20state-of-the-art%0Atransformer-based%20HTR%20model%2C%20to%2016th-century%20Latin%20manuscripts%20authored%20by%0ARudolf%20Gwalther.%20We%20investigate%20targeted%20image%20preprocessing%20and%20a%20broad%20suite%0Aof%20data%20augmentation%20techniques%2C%20introducing%20four%20novel%20augmentation%20methods%0Adesigned%20specifically%20for%20historical%20handwriting%20characteristics.%20We%20also%0Aevaluate%20ensemble%20learning%20approaches%20to%20leverage%20the%20complementary%20strengths%0Aof%20augmentation-trained%20models.%20On%20the%20Gwalther%20dataset%2C%20our%20best%20single-model%0Aaugmentation%20%28Elastic%29%20achieves%20a%20Character%20Error%20Rate%20%28CER%29%20of%201.86%2C%20while%20a%0Atop-5%20voting%20ensemble%20achieves%20a%20CER%20of%201.60%20-%20representing%20a%2050%25%20relative%0Aimprovement%20over%20the%20best%20reported%20TrOCR_BASE%20result%20and%20a%2042%25%20improvement%20over%0Athe%20previous%20state%20of%20the%20art.%20These%20results%20highlight%20the%20impact%20of%0Adomain-specific%20augmentations%20and%20ensemble%20strategies%20in%20advancing%20HTR%0Aperformance%20for%20historical%20manuscripts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHandwritten%2520Text%2520Recognition%2520of%2520Historical%2520Manuscripts%2520Using%250A%2520%2520Transformer-Based%2520Models%26entry.906535625%3DErez%2520Meoded%26entry.1292438233%3D%2520%2520Historical%2520handwritten%2520text%2520recognition%2520%2528HTR%2529%2520is%2520essential%2520for%2520unlocking%2520the%250Acultural%2520and%2520scholarly%2520value%2520of%2520archival%2520documents%252C%2520yet%2520digitization%2520is%2520often%250Ahindered%2520by%2520scarce%2520transcriptions%252C%2520linguistic%2520variation%252C%2520and%2520highly%2520diverse%250Ahandwriting%2520styles.%2520In%2520this%2520study%252C%2520we%2520apply%2520TrOCR%252C%2520a%2520state-of-the-art%250Atransformer-based%2520HTR%2520model%252C%2520to%252016th-century%2520Latin%2520manuscripts%2520authored%2520by%250ARudolf%2520Gwalther.%2520We%2520investigate%2520targeted%2520image%2520preprocessing%2520and%2520a%2520broad%2520suite%250Aof%2520data%2520augmentation%2520techniques%252C%2520introducing%2520four%2520novel%2520augmentation%2520methods%250Adesigned%2520specifically%2520for%2520historical%2520handwriting%2520characteristics.%2520We%2520also%250Aevaluate%2520ensemble%2520learning%2520approaches%2520to%2520leverage%2520the%2520complementary%2520strengths%250Aof%2520augmentation-trained%2520models.%2520On%2520the%2520Gwalther%2520dataset%252C%2520our%2520best%2520single-model%250Aaugmentation%2520%2528Elastic%2529%2520achieves%2520a%2520Character%2520Error%2520Rate%2520%2528CER%2529%2520of%25201.86%252C%2520while%2520a%250Atop-5%2520voting%2520ensemble%2520achieves%2520a%2520CER%2520of%25201.60%2520-%2520representing%2520a%252050%2525%2520relative%250Aimprovement%2520over%2520the%2520best%2520reported%2520TrOCR_BASE%2520result%2520and%2520a%252042%2525%2520improvement%2520over%250Athe%2520previous%2520state%2520of%2520the%2520art.%2520These%2520results%2520highlight%2520the%2520impact%2520of%250Adomain-specific%2520augmentations%2520and%2520ensemble%2520strategies%2520in%2520advancing%2520HTR%250Aperformance%2520for%2520historical%2520manuscripts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Handwritten%20Text%20Recognition%20of%20Historical%20Manuscripts%20Using%0A%20%20Transformer-Based%20Models&entry.906535625=Erez%20Meoded&entry.1292438233=%20%20Historical%20handwritten%20text%20recognition%20%28HTR%29%20is%20essential%20for%20unlocking%20the%0Acultural%20and%20scholarly%20value%20of%20archival%20documents%2C%20yet%20digitization%20is%20often%0Ahindered%20by%20scarce%20transcriptions%2C%20linguistic%20variation%2C%20and%20highly%20diverse%0Ahandwriting%20styles.%20In%20this%20study%2C%20we%20apply%20TrOCR%2C%20a%20state-of-the-art%0Atransformer-based%20HTR%20model%2C%20to%2016th-century%20Latin%20manuscripts%20authored%20by%0ARudolf%20Gwalther.%20We%20investigate%20targeted%20image%20preprocessing%20and%20a%20broad%20suite%0Aof%20data%20augmentation%20techniques%2C%20introducing%20four%20novel%20augmentation%20methods%0Adesigned%20specifically%20for%20historical%20handwriting%20characteristics.%20We%20also%0Aevaluate%20ensemble%20learning%20approaches%20to%20leverage%20the%20complementary%20strengths%0Aof%20augmentation-trained%20models.%20On%20the%20Gwalther%20dataset%2C%20our%20best%20single-model%0Aaugmentation%20%28Elastic%29%20achieves%20a%20Character%20Error%20Rate%20%28CER%29%20of%201.86%2C%20while%20a%0Atop-5%20voting%20ensemble%20achieves%20a%20CER%20of%201.60%20-%20representing%20a%2050%25%20relative%0Aimprovement%20over%20the%20best%20reported%20TrOCR_BASE%20result%20and%20a%2042%25%20improvement%20over%0Athe%20previous%20state%20of%20the%20art.%20These%20results%20highlight%20the%20impact%20of%0Adomain-specific%20augmentations%20and%20ensemble%20strategies%20in%20advancing%20HTR%0Aperformance%20for%20historical%20manuscripts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11499v1&entry.124074799=Read"},
{"title": "Minimizing Surrogate Losses for Decision-Focused Learning using\n  Differentiable Optimization", "author": "Jayanta Mandi and Ali \u0130rfan Mahmuto\u011fullar\u0131 and Senne Berden and Tias Guns", "abstract": "  Decision-focused learning (DFL) trains a machine learning (ML) model to\npredict parameters of an optimization problem, to directly minimize decision\nregret, i.e., maximize decision quality. Gradient-based DFL requires computing\nthe derivative of the solution to the optimization problem with respect to the\npredicted parameters. However, for many optimization problems, such as linear\nprograms (LPs), the gradient of the regret with respect to the predicted\nparameters is zero almost everywhere. Existing gradient-based DFL approaches\nfor LPs try to circumvent this issue in one of two ways: (a) smoothing the LP\ninto a differentiable optimization problem by adding a quadratic regularizer\nand then minimizing the regret directly or (b) minimizing surrogate losses that\nhave informative (sub)gradients. In this paper, we show that the former\napproach still results in zero gradients, because even after smoothing the\nregret remains constant across large regions of the parameter space. To address\nthis, we propose minimizing surrogate losses -- even when a differentiable\noptimization layer is used and regret can be minimized directly. Our\nexperiments demonstrate that minimizing surrogate losses allows differentiable\noptimization layers to achieve regret comparable to or better than\nsurrogate-loss based DFL methods. Further, we demonstrate that this also holds\nfor DYS-Net, a recently proposed differentiable optimization technique for LPs,\nthat computes approximate solutions and gradients through operations that can\nbe performed using feedforward neural network layers. Because DYS-Net executes\nthe forward and the backward pass very efficiently, by minimizing surrogate\nlosses using DYS-Net, we are able to attain regret on par with the\nstate-of-the-art while reducing training time by a significant margin.\n", "link": "http://arxiv.org/abs/2508.11365v1", "date": "2025-08-15", "relevancy": 1.9806, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5185}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4855}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimizing%20Surrogate%20Losses%20for%20Decision-Focused%20Learning%20using%0A%20%20Differentiable%20Optimization&body=Title%3A%20Minimizing%20Surrogate%20Losses%20for%20Decision-Focused%20Learning%20using%0A%20%20Differentiable%20Optimization%0AAuthor%3A%20Jayanta%20Mandi%20and%20Ali%20%C4%B0rfan%20Mahmuto%C4%9Fullar%C4%B1%20and%20Senne%20Berden%20and%20Tias%20Guns%0AAbstract%3A%20%20%20Decision-focused%20learning%20%28DFL%29%20trains%20a%20machine%20learning%20%28ML%29%20model%20to%0Apredict%20parameters%20of%20an%20optimization%20problem%2C%20to%20directly%20minimize%20decision%0Aregret%2C%20i.e.%2C%20maximize%20decision%20quality.%20Gradient-based%20DFL%20requires%20computing%0Athe%20derivative%20of%20the%20solution%20to%20the%20optimization%20problem%20with%20respect%20to%20the%0Apredicted%20parameters.%20However%2C%20for%20many%20optimization%20problems%2C%20such%20as%20linear%0Aprograms%20%28LPs%29%2C%20the%20gradient%20of%20the%20regret%20with%20respect%20to%20the%20predicted%0Aparameters%20is%20zero%20almost%20everywhere.%20Existing%20gradient-based%20DFL%20approaches%0Afor%20LPs%20try%20to%20circumvent%20this%20issue%20in%20one%20of%20two%20ways%3A%20%28a%29%20smoothing%20the%20LP%0Ainto%20a%20differentiable%20optimization%20problem%20by%20adding%20a%20quadratic%20regularizer%0Aand%20then%20minimizing%20the%20regret%20directly%20or%20%28b%29%20minimizing%20surrogate%20losses%20that%0Ahave%20informative%20%28sub%29gradients.%20In%20this%20paper%2C%20we%20show%20that%20the%20former%0Aapproach%20still%20results%20in%20zero%20gradients%2C%20because%20even%20after%20smoothing%20the%0Aregret%20remains%20constant%20across%20large%20regions%20of%20the%20parameter%20space.%20To%20address%0Athis%2C%20we%20propose%20minimizing%20surrogate%20losses%20--%20even%20when%20a%20differentiable%0Aoptimization%20layer%20is%20used%20and%20regret%20can%20be%20minimized%20directly.%20Our%0Aexperiments%20demonstrate%20that%20minimizing%20surrogate%20losses%20allows%20differentiable%0Aoptimization%20layers%20to%20achieve%20regret%20comparable%20to%20or%20better%20than%0Asurrogate-loss%20based%20DFL%20methods.%20Further%2C%20we%20demonstrate%20that%20this%20also%20holds%0Afor%20DYS-Net%2C%20a%20recently%20proposed%20differentiable%20optimization%20technique%20for%20LPs%2C%0Athat%20computes%20approximate%20solutions%20and%20gradients%20through%20operations%20that%20can%0Abe%20performed%20using%20feedforward%20neural%20network%20layers.%20Because%20DYS-Net%20executes%0Athe%20forward%20and%20the%20backward%20pass%20very%20efficiently%2C%20by%20minimizing%20surrogate%0Alosses%20using%20DYS-Net%2C%20we%20are%20able%20to%20attain%20regret%20on%20par%20with%20the%0Astate-of-the-art%20while%20reducing%20training%20time%20by%20a%20significant%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimizing%2520Surrogate%2520Losses%2520for%2520Decision-Focused%2520Learning%2520using%250A%2520%2520Differentiable%2520Optimization%26entry.906535625%3DJayanta%2520Mandi%2520and%2520Ali%2520%25C4%25B0rfan%2520Mahmuto%25C4%259Fullar%25C4%25B1%2520and%2520Senne%2520Berden%2520and%2520Tias%2520Guns%26entry.1292438233%3D%2520%2520Decision-focused%2520learning%2520%2528DFL%2529%2520trains%2520a%2520machine%2520learning%2520%2528ML%2529%2520model%2520to%250Apredict%2520parameters%2520of%2520an%2520optimization%2520problem%252C%2520to%2520directly%2520minimize%2520decision%250Aregret%252C%2520i.e.%252C%2520maximize%2520decision%2520quality.%2520Gradient-based%2520DFL%2520requires%2520computing%250Athe%2520derivative%2520of%2520the%2520solution%2520to%2520the%2520optimization%2520problem%2520with%2520respect%2520to%2520the%250Apredicted%2520parameters.%2520However%252C%2520for%2520many%2520optimization%2520problems%252C%2520such%2520as%2520linear%250Aprograms%2520%2528LPs%2529%252C%2520the%2520gradient%2520of%2520the%2520regret%2520with%2520respect%2520to%2520the%2520predicted%250Aparameters%2520is%2520zero%2520almost%2520everywhere.%2520Existing%2520gradient-based%2520DFL%2520approaches%250Afor%2520LPs%2520try%2520to%2520circumvent%2520this%2520issue%2520in%2520one%2520of%2520two%2520ways%253A%2520%2528a%2529%2520smoothing%2520the%2520LP%250Ainto%2520a%2520differentiable%2520optimization%2520problem%2520by%2520adding%2520a%2520quadratic%2520regularizer%250Aand%2520then%2520minimizing%2520the%2520regret%2520directly%2520or%2520%2528b%2529%2520minimizing%2520surrogate%2520losses%2520that%250Ahave%2520informative%2520%2528sub%2529gradients.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520the%2520former%250Aapproach%2520still%2520results%2520in%2520zero%2520gradients%252C%2520because%2520even%2520after%2520smoothing%2520the%250Aregret%2520remains%2520constant%2520across%2520large%2520regions%2520of%2520the%2520parameter%2520space.%2520To%2520address%250Athis%252C%2520we%2520propose%2520minimizing%2520surrogate%2520losses%2520--%2520even%2520when%2520a%2520differentiable%250Aoptimization%2520layer%2520is%2520used%2520and%2520regret%2520can%2520be%2520minimized%2520directly.%2520Our%250Aexperiments%2520demonstrate%2520that%2520minimizing%2520surrogate%2520losses%2520allows%2520differentiable%250Aoptimization%2520layers%2520to%2520achieve%2520regret%2520comparable%2520to%2520or%2520better%2520than%250Asurrogate-loss%2520based%2520DFL%2520methods.%2520Further%252C%2520we%2520demonstrate%2520that%2520this%2520also%2520holds%250Afor%2520DYS-Net%252C%2520a%2520recently%2520proposed%2520differentiable%2520optimization%2520technique%2520for%2520LPs%252C%250Athat%2520computes%2520approximate%2520solutions%2520and%2520gradients%2520through%2520operations%2520that%2520can%250Abe%2520performed%2520using%2520feedforward%2520neural%2520network%2520layers.%2520Because%2520DYS-Net%2520executes%250Athe%2520forward%2520and%2520the%2520backward%2520pass%2520very%2520efficiently%252C%2520by%2520minimizing%2520surrogate%250Alosses%2520using%2520DYS-Net%252C%2520we%2520are%2520able%2520to%2520attain%2520regret%2520on%2520par%2520with%2520the%250Astate-of-the-art%2520while%2520reducing%2520training%2520time%2520by%2520a%2520significant%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimizing%20Surrogate%20Losses%20for%20Decision-Focused%20Learning%20using%0A%20%20Differentiable%20Optimization&entry.906535625=Jayanta%20Mandi%20and%20Ali%20%C4%B0rfan%20Mahmuto%C4%9Fullar%C4%B1%20and%20Senne%20Berden%20and%20Tias%20Guns&entry.1292438233=%20%20Decision-focused%20learning%20%28DFL%29%20trains%20a%20machine%20learning%20%28ML%29%20model%20to%0Apredict%20parameters%20of%20an%20optimization%20problem%2C%20to%20directly%20minimize%20decision%0Aregret%2C%20i.e.%2C%20maximize%20decision%20quality.%20Gradient-based%20DFL%20requires%20computing%0Athe%20derivative%20of%20the%20solution%20to%20the%20optimization%20problem%20with%20respect%20to%20the%0Apredicted%20parameters.%20However%2C%20for%20many%20optimization%20problems%2C%20such%20as%20linear%0Aprograms%20%28LPs%29%2C%20the%20gradient%20of%20the%20regret%20with%20respect%20to%20the%20predicted%0Aparameters%20is%20zero%20almost%20everywhere.%20Existing%20gradient-based%20DFL%20approaches%0Afor%20LPs%20try%20to%20circumvent%20this%20issue%20in%20one%20of%20two%20ways%3A%20%28a%29%20smoothing%20the%20LP%0Ainto%20a%20differentiable%20optimization%20problem%20by%20adding%20a%20quadratic%20regularizer%0Aand%20then%20minimizing%20the%20regret%20directly%20or%20%28b%29%20minimizing%20surrogate%20losses%20that%0Ahave%20informative%20%28sub%29gradients.%20In%20this%20paper%2C%20we%20show%20that%20the%20former%0Aapproach%20still%20results%20in%20zero%20gradients%2C%20because%20even%20after%20smoothing%20the%0Aregret%20remains%20constant%20across%20large%20regions%20of%20the%20parameter%20space.%20To%20address%0Athis%2C%20we%20propose%20minimizing%20surrogate%20losses%20--%20even%20when%20a%20differentiable%0Aoptimization%20layer%20is%20used%20and%20regret%20can%20be%20minimized%20directly.%20Our%0Aexperiments%20demonstrate%20that%20minimizing%20surrogate%20losses%20allows%20differentiable%0Aoptimization%20layers%20to%20achieve%20regret%20comparable%20to%20or%20better%20than%0Asurrogate-loss%20based%20DFL%20methods.%20Further%2C%20we%20demonstrate%20that%20this%20also%20holds%0Afor%20DYS-Net%2C%20a%20recently%20proposed%20differentiable%20optimization%20technique%20for%20LPs%2C%0Athat%20computes%20approximate%20solutions%20and%20gradients%20through%20operations%20that%20can%0Abe%20performed%20using%20feedforward%20neural%20network%20layers.%20Because%20DYS-Net%20executes%0Athe%20forward%20and%20the%20backward%20pass%20very%20efficiently%2C%20by%20minimizing%20surrogate%0Alosses%20using%20DYS-Net%2C%20we%20are%20able%20to%20attain%20regret%20on%20par%20with%20the%0Astate-of-the-art%20while%20reducing%20training%20time%20by%20a%20significant%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11365v1&entry.124074799=Read"},
{"title": "Rationalizing Transformer Predictions via End-To-End Differentiable\n  Self-Training", "author": "Marc Brinner and Sina Zarrie\u00df", "abstract": "  We propose an end-to-end differentiable training paradigm for stable training\nof a rationalized transformer classifier. Our approach results in a single\nmodel that simultaneously classifies a sample and scores input tokens based on\ntheir relevance to the classification. To this end, we build on the widely-used\nthree-player-game for training rationalized models, which typically relies on\ntraining a rationale selector, a classifier and a complement classifier. We\nsimplify this approach by making a single model fulfill all three roles,\nleading to a more efficient training paradigm that is not susceptible to the\ncommon training instabilities that plague existing approaches. Further, we\nextend this paradigm to produce class-wise rationales while incorporating\nrecent advances in parameterizing and regularizing the resulting rationales,\nthus leading to substantially improved and state-of-the-art alignment with\nhuman annotations without any explicit supervision.\n", "link": "http://arxiv.org/abs/2508.11393v1", "date": "2025-08-15", "relevancy": 1.9787, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5483}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4903}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rationalizing%20Transformer%20Predictions%20via%20End-To-End%20Differentiable%0A%20%20Self-Training&body=Title%3A%20Rationalizing%20Transformer%20Predictions%20via%20End-To-End%20Differentiable%0A%20%20Self-Training%0AAuthor%3A%20Marc%20Brinner%20and%20Sina%20Zarrie%C3%9F%0AAbstract%3A%20%20%20We%20propose%20an%20end-to-end%20differentiable%20training%20paradigm%20for%20stable%20training%0Aof%20a%20rationalized%20transformer%20classifier.%20Our%20approach%20results%20in%20a%20single%0Amodel%20that%20simultaneously%20classifies%20a%20sample%20and%20scores%20input%20tokens%20based%20on%0Atheir%20relevance%20to%20the%20classification.%20To%20this%20end%2C%20we%20build%20on%20the%20widely-used%0Athree-player-game%20for%20training%20rationalized%20models%2C%20which%20typically%20relies%20on%0Atraining%20a%20rationale%20selector%2C%20a%20classifier%20and%20a%20complement%20classifier.%20We%0Asimplify%20this%20approach%20by%20making%20a%20single%20model%20fulfill%20all%20three%20roles%2C%0Aleading%20to%20a%20more%20efficient%20training%20paradigm%20that%20is%20not%20susceptible%20to%20the%0Acommon%20training%20instabilities%20that%20plague%20existing%20approaches.%20Further%2C%20we%0Aextend%20this%20paradigm%20to%20produce%20class-wise%20rationales%20while%20incorporating%0Arecent%20advances%20in%20parameterizing%20and%20regularizing%20the%20resulting%20rationales%2C%0Athus%20leading%20to%20substantially%20improved%20and%20state-of-the-art%20alignment%20with%0Ahuman%20annotations%20without%20any%20explicit%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRationalizing%2520Transformer%2520Predictions%2520via%2520End-To-End%2520Differentiable%250A%2520%2520Self-Training%26entry.906535625%3DMarc%2520Brinner%2520and%2520Sina%2520Zarrie%25C3%259F%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520end-to-end%2520differentiable%2520training%2520paradigm%2520for%2520stable%2520training%250Aof%2520a%2520rationalized%2520transformer%2520classifier.%2520Our%2520approach%2520results%2520in%2520a%2520single%250Amodel%2520that%2520simultaneously%2520classifies%2520a%2520sample%2520and%2520scores%2520input%2520tokens%2520based%2520on%250Atheir%2520relevance%2520to%2520the%2520classification.%2520To%2520this%2520end%252C%2520we%2520build%2520on%2520the%2520widely-used%250Athree-player-game%2520for%2520training%2520rationalized%2520models%252C%2520which%2520typically%2520relies%2520on%250Atraining%2520a%2520rationale%2520selector%252C%2520a%2520classifier%2520and%2520a%2520complement%2520classifier.%2520We%250Asimplify%2520this%2520approach%2520by%2520making%2520a%2520single%2520model%2520fulfill%2520all%2520three%2520roles%252C%250Aleading%2520to%2520a%2520more%2520efficient%2520training%2520paradigm%2520that%2520is%2520not%2520susceptible%2520to%2520the%250Acommon%2520training%2520instabilities%2520that%2520plague%2520existing%2520approaches.%2520Further%252C%2520we%250Aextend%2520this%2520paradigm%2520to%2520produce%2520class-wise%2520rationales%2520while%2520incorporating%250Arecent%2520advances%2520in%2520parameterizing%2520and%2520regularizing%2520the%2520resulting%2520rationales%252C%250Athus%2520leading%2520to%2520substantially%2520improved%2520and%2520state-of-the-art%2520alignment%2520with%250Ahuman%2520annotations%2520without%2520any%2520explicit%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rationalizing%20Transformer%20Predictions%20via%20End-To-End%20Differentiable%0A%20%20Self-Training&entry.906535625=Marc%20Brinner%20and%20Sina%20Zarrie%C3%9F&entry.1292438233=%20%20We%20propose%20an%20end-to-end%20differentiable%20training%20paradigm%20for%20stable%20training%0Aof%20a%20rationalized%20transformer%20classifier.%20Our%20approach%20results%20in%20a%20single%0Amodel%20that%20simultaneously%20classifies%20a%20sample%20and%20scores%20input%20tokens%20based%20on%0Atheir%20relevance%20to%20the%20classification.%20To%20this%20end%2C%20we%20build%20on%20the%20widely-used%0Athree-player-game%20for%20training%20rationalized%20models%2C%20which%20typically%20relies%20on%0Atraining%20a%20rationale%20selector%2C%20a%20classifier%20and%20a%20complement%20classifier.%20We%0Asimplify%20this%20approach%20by%20making%20a%20single%20model%20fulfill%20all%20three%20roles%2C%0Aleading%20to%20a%20more%20efficient%20training%20paradigm%20that%20is%20not%20susceptible%20to%20the%0Acommon%20training%20instabilities%20that%20plague%20existing%20approaches.%20Further%2C%20we%0Aextend%20this%20paradigm%20to%20produce%20class-wise%20rationales%20while%20incorporating%0Arecent%20advances%20in%20parameterizing%20and%20regularizing%20the%20resulting%20rationales%2C%0Athus%20leading%20to%20substantially%20improved%20and%20state-of-the-art%20alignment%20with%0Ahuman%20annotations%20without%20any%20explicit%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11393v1&entry.124074799=Read"},
{"title": "Optimal Planning and Machine Learning for Responsive Tracking and\n  Enhanced Forecasting of Wildfires using a Spacecraft Constellation", "author": "Sreeja Roy-Singh and Vinay Ravindra and Richard Levinson and Mahta Moghaddam and Jan Mandel and Adam Kochanski and Angel Farguell Caus and Kurtis Nelson and Samira Alkaee Taleghan and Archana Kannan and Amer Melebari", "abstract": "  We propose a novel concept of operations using optimal planning methods and\nmachine learning (ML) to collect spaceborne data that is unprecedented for\nmonitoring wildfires, process it to create new or enhanced products in the\ncontext of wildfire danger or spread monitoring, and assimilate them to improve\nexisting, wildfire decision support tools delivered to firefighters within\nlatency appropriate for time-critical applications. The concept is studied with\nrespect to NASA's CYGNSS Mission, a constellation of passive microwave\nreceivers that measure specular GNSS-R reflections despite clouds and smoke.\nOur planner uses a Mixed Integer Program formulation to schedule joint\nobservation data collection and downlink for all satellites. Optimal solutions\nare found quickly that collect 98-100% of available observation opportunities.\nML-based fire predictions that drive the planner objective are greater than 40%\nmore correlated with ground truth than existing state-of-art. The presented\ncase study on the TX Smokehouse Creek fire in 2024 and LA fires in 2025\nrepresents the first high-resolution data collected by CYGNSS of active fires.\nCreation of Burnt Area Maps (BAM) using ML on data from active fires and BAM\nassimilation into NASA's Weather Research and Forecasting Model using neural\nnets to broadcast fire spread are novel outcomes. BAM and CYGNSS obtained soil\nmoisture are integrated for the first time into USGS fire danger maps.\nInclusion of CYGNSS data in ML-based burn predictions boosts accuracy by 13%,\nand inclusion of high-resolution data boosts ML recall by another 15%. The\nproposed workflow has an expected latency of 6-30h, improving on the current\ndelivery time of multiple days. All components in the proposed concept are\nshown to be computationally scalable and globally generalizable, with\nsustainability considerations such as edge efficiency and low latency on small\ndevices.\n", "link": "http://arxiv.org/abs/2508.06687v2", "date": "2025-08-15", "relevancy": 1.9758, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5274}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.497}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Planning%20and%20Machine%20Learning%20for%20Responsive%20Tracking%20and%0A%20%20Enhanced%20Forecasting%20of%20Wildfires%20using%20a%20Spacecraft%20Constellation&body=Title%3A%20Optimal%20Planning%20and%20Machine%20Learning%20for%20Responsive%20Tracking%20and%0A%20%20Enhanced%20Forecasting%20of%20Wildfires%20using%20a%20Spacecraft%20Constellation%0AAuthor%3A%20Sreeja%20Roy-Singh%20and%20Vinay%20Ravindra%20and%20Richard%20Levinson%20and%20Mahta%20Moghaddam%20and%20Jan%20Mandel%20and%20Adam%20Kochanski%20and%20Angel%20Farguell%20Caus%20and%20Kurtis%20Nelson%20and%20Samira%20Alkaee%20Taleghan%20and%20Archana%20Kannan%20and%20Amer%20Melebari%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20concept%20of%20operations%20using%20optimal%20planning%20methods%20and%0Amachine%20learning%20%28ML%29%20to%20collect%20spaceborne%20data%20that%20is%20unprecedented%20for%0Amonitoring%20wildfires%2C%20process%20it%20to%20create%20new%20or%20enhanced%20products%20in%20the%0Acontext%20of%20wildfire%20danger%20or%20spread%20monitoring%2C%20and%20assimilate%20them%20to%20improve%0Aexisting%2C%20wildfire%20decision%20support%20tools%20delivered%20to%20firefighters%20within%0Alatency%20appropriate%20for%20time-critical%20applications.%20The%20concept%20is%20studied%20with%0Arespect%20to%20NASA%27s%20CYGNSS%20Mission%2C%20a%20constellation%20of%20passive%20microwave%0Areceivers%20that%20measure%20specular%20GNSS-R%20reflections%20despite%20clouds%20and%20smoke.%0AOur%20planner%20uses%20a%20Mixed%20Integer%20Program%20formulation%20to%20schedule%20joint%0Aobservation%20data%20collection%20and%20downlink%20for%20all%20satellites.%20Optimal%20solutions%0Aare%20found%20quickly%20that%20collect%2098-100%25%20of%20available%20observation%20opportunities.%0AML-based%20fire%20predictions%20that%20drive%20the%20planner%20objective%20are%20greater%20than%2040%25%0Amore%20correlated%20with%20ground%20truth%20than%20existing%20state-of-art.%20The%20presented%0Acase%20study%20on%20the%20TX%20Smokehouse%20Creek%20fire%20in%202024%20and%20LA%20fires%20in%202025%0Arepresents%20the%20first%20high-resolution%20data%20collected%20by%20CYGNSS%20of%20active%20fires.%0ACreation%20of%20Burnt%20Area%20Maps%20%28BAM%29%20using%20ML%20on%20data%20from%20active%20fires%20and%20BAM%0Aassimilation%20into%20NASA%27s%20Weather%20Research%20and%20Forecasting%20Model%20using%20neural%0Anets%20to%20broadcast%20fire%20spread%20are%20novel%20outcomes.%20BAM%20and%20CYGNSS%20obtained%20soil%0Amoisture%20are%20integrated%20for%20the%20first%20time%20into%20USGS%20fire%20danger%20maps.%0AInclusion%20of%20CYGNSS%20data%20in%20ML-based%20burn%20predictions%20boosts%20accuracy%20by%2013%25%2C%0Aand%20inclusion%20of%20high-resolution%20data%20boosts%20ML%20recall%20by%20another%2015%25.%20The%0Aproposed%20workflow%20has%20an%20expected%20latency%20of%206-30h%2C%20improving%20on%20the%20current%0Adelivery%20time%20of%20multiple%20days.%20All%20components%20in%20the%20proposed%20concept%20are%0Ashown%20to%20be%20computationally%20scalable%20and%20globally%20generalizable%2C%20with%0Asustainability%20considerations%20such%20as%20edge%20efficiency%20and%20low%20latency%20on%20small%0Adevices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Planning%2520and%2520Machine%2520Learning%2520for%2520Responsive%2520Tracking%2520and%250A%2520%2520Enhanced%2520Forecasting%2520of%2520Wildfires%2520using%2520a%2520Spacecraft%2520Constellation%26entry.906535625%3DSreeja%2520Roy-Singh%2520and%2520Vinay%2520Ravindra%2520and%2520Richard%2520Levinson%2520and%2520Mahta%2520Moghaddam%2520and%2520Jan%2520Mandel%2520and%2520Adam%2520Kochanski%2520and%2520Angel%2520Farguell%2520Caus%2520and%2520Kurtis%2520Nelson%2520and%2520Samira%2520Alkaee%2520Taleghan%2520and%2520Archana%2520Kannan%2520and%2520Amer%2520Melebari%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520concept%2520of%2520operations%2520using%2520optimal%2520planning%2520methods%2520and%250Amachine%2520learning%2520%2528ML%2529%2520to%2520collect%2520spaceborne%2520data%2520that%2520is%2520unprecedented%2520for%250Amonitoring%2520wildfires%252C%2520process%2520it%2520to%2520create%2520new%2520or%2520enhanced%2520products%2520in%2520the%250Acontext%2520of%2520wildfire%2520danger%2520or%2520spread%2520monitoring%252C%2520and%2520assimilate%2520them%2520to%2520improve%250Aexisting%252C%2520wildfire%2520decision%2520support%2520tools%2520delivered%2520to%2520firefighters%2520within%250Alatency%2520appropriate%2520for%2520time-critical%2520applications.%2520The%2520concept%2520is%2520studied%2520with%250Arespect%2520to%2520NASA%2527s%2520CYGNSS%2520Mission%252C%2520a%2520constellation%2520of%2520passive%2520microwave%250Areceivers%2520that%2520measure%2520specular%2520GNSS-R%2520reflections%2520despite%2520clouds%2520and%2520smoke.%250AOur%2520planner%2520uses%2520a%2520Mixed%2520Integer%2520Program%2520formulation%2520to%2520schedule%2520joint%250Aobservation%2520data%2520collection%2520and%2520downlink%2520for%2520all%2520satellites.%2520Optimal%2520solutions%250Aare%2520found%2520quickly%2520that%2520collect%252098-100%2525%2520of%2520available%2520observation%2520opportunities.%250AML-based%2520fire%2520predictions%2520that%2520drive%2520the%2520planner%2520objective%2520are%2520greater%2520than%252040%2525%250Amore%2520correlated%2520with%2520ground%2520truth%2520than%2520existing%2520state-of-art.%2520The%2520presented%250Acase%2520study%2520on%2520the%2520TX%2520Smokehouse%2520Creek%2520fire%2520in%25202024%2520and%2520LA%2520fires%2520in%25202025%250Arepresents%2520the%2520first%2520high-resolution%2520data%2520collected%2520by%2520CYGNSS%2520of%2520active%2520fires.%250ACreation%2520of%2520Burnt%2520Area%2520Maps%2520%2528BAM%2529%2520using%2520ML%2520on%2520data%2520from%2520active%2520fires%2520and%2520BAM%250Aassimilation%2520into%2520NASA%2527s%2520Weather%2520Research%2520and%2520Forecasting%2520Model%2520using%2520neural%250Anets%2520to%2520broadcast%2520fire%2520spread%2520are%2520novel%2520outcomes.%2520BAM%2520and%2520CYGNSS%2520obtained%2520soil%250Amoisture%2520are%2520integrated%2520for%2520the%2520first%2520time%2520into%2520USGS%2520fire%2520danger%2520maps.%250AInclusion%2520of%2520CYGNSS%2520data%2520in%2520ML-based%2520burn%2520predictions%2520boosts%2520accuracy%2520by%252013%2525%252C%250Aand%2520inclusion%2520of%2520high-resolution%2520data%2520boosts%2520ML%2520recall%2520by%2520another%252015%2525.%2520The%250Aproposed%2520workflow%2520has%2520an%2520expected%2520latency%2520of%25206-30h%252C%2520improving%2520on%2520the%2520current%250Adelivery%2520time%2520of%2520multiple%2520days.%2520All%2520components%2520in%2520the%2520proposed%2520concept%2520are%250Ashown%2520to%2520be%2520computationally%2520scalable%2520and%2520globally%2520generalizable%252C%2520with%250Asustainability%2520considerations%2520such%2520as%2520edge%2520efficiency%2520and%2520low%2520latency%2520on%2520small%250Adevices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Planning%20and%20Machine%20Learning%20for%20Responsive%20Tracking%20and%0A%20%20Enhanced%20Forecasting%20of%20Wildfires%20using%20a%20Spacecraft%20Constellation&entry.906535625=Sreeja%20Roy-Singh%20and%20Vinay%20Ravindra%20and%20Richard%20Levinson%20and%20Mahta%20Moghaddam%20and%20Jan%20Mandel%20and%20Adam%20Kochanski%20and%20Angel%20Farguell%20Caus%20and%20Kurtis%20Nelson%20and%20Samira%20Alkaee%20Taleghan%20and%20Archana%20Kannan%20and%20Amer%20Melebari&entry.1292438233=%20%20We%20propose%20a%20novel%20concept%20of%20operations%20using%20optimal%20planning%20methods%20and%0Amachine%20learning%20%28ML%29%20to%20collect%20spaceborne%20data%20that%20is%20unprecedented%20for%0Amonitoring%20wildfires%2C%20process%20it%20to%20create%20new%20or%20enhanced%20products%20in%20the%0Acontext%20of%20wildfire%20danger%20or%20spread%20monitoring%2C%20and%20assimilate%20them%20to%20improve%0Aexisting%2C%20wildfire%20decision%20support%20tools%20delivered%20to%20firefighters%20within%0Alatency%20appropriate%20for%20time-critical%20applications.%20The%20concept%20is%20studied%20with%0Arespect%20to%20NASA%27s%20CYGNSS%20Mission%2C%20a%20constellation%20of%20passive%20microwave%0Areceivers%20that%20measure%20specular%20GNSS-R%20reflections%20despite%20clouds%20and%20smoke.%0AOur%20planner%20uses%20a%20Mixed%20Integer%20Program%20formulation%20to%20schedule%20joint%0Aobservation%20data%20collection%20and%20downlink%20for%20all%20satellites.%20Optimal%20solutions%0Aare%20found%20quickly%20that%20collect%2098-100%25%20of%20available%20observation%20opportunities.%0AML-based%20fire%20predictions%20that%20drive%20the%20planner%20objective%20are%20greater%20than%2040%25%0Amore%20correlated%20with%20ground%20truth%20than%20existing%20state-of-art.%20The%20presented%0Acase%20study%20on%20the%20TX%20Smokehouse%20Creek%20fire%20in%202024%20and%20LA%20fires%20in%202025%0Arepresents%20the%20first%20high-resolution%20data%20collected%20by%20CYGNSS%20of%20active%20fires.%0ACreation%20of%20Burnt%20Area%20Maps%20%28BAM%29%20using%20ML%20on%20data%20from%20active%20fires%20and%20BAM%0Aassimilation%20into%20NASA%27s%20Weather%20Research%20and%20Forecasting%20Model%20using%20neural%0Anets%20to%20broadcast%20fire%20spread%20are%20novel%20outcomes.%20BAM%20and%20CYGNSS%20obtained%20soil%0Amoisture%20are%20integrated%20for%20the%20first%20time%20into%20USGS%20fire%20danger%20maps.%0AInclusion%20of%20CYGNSS%20data%20in%20ML-based%20burn%20predictions%20boosts%20accuracy%20by%2013%25%2C%0Aand%20inclusion%20of%20high-resolution%20data%20boosts%20ML%20recall%20by%20another%2015%25.%20The%0Aproposed%20workflow%20has%20an%20expected%20latency%20of%206-30h%2C%20improving%20on%20the%20current%0Adelivery%20time%20of%20multiple%20days.%20All%20components%20in%20the%20proposed%20concept%20are%0Ashown%20to%20be%20computationally%20scalable%20and%20globally%20generalizable%2C%20with%0Asustainability%20considerations%20such%20as%20edge%20efficiency%20and%20low%20latency%20on%20small%0Adevices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06687v2&entry.124074799=Read"},
{"title": "Foldable SuperNets: Scalable Merging of Transformers with Different\n  Initializations and Tasks", "author": "Edan Kinderman and Itay Hubara and Haggai Maron and Daniel Soudry", "abstract": "  Recent methods aim to merge neural networks (NNs) with identical\narchitectures trained on different tasks into a single multi-task model. While\nmost works focus on the simpler setup of merging NNs initialized from a common\npre-trained network, we target the harder problem of merging large transformers\ntrained on different tasks from distinct initializations. We show that\ntraditional merging methods fail catastrophically in this setup, while\nKnowledge Distillation (KD) achieves much better results, though at a higher\ncost. However, KD is data-inefficient, as it does not exploit the original\nmodels' weights. To solve this, we introduce \"Foldable SuperNet Merge\"\n(FS-Merge), which trains a SuperNet containing the original models (with frozen\nweights) using a feature reconstruction objective. After training, the SuperNet\nis folded back to the size of a single original model. FS-Merge is simple,\ndata-efficient, has a computational cost comparable to KD, and is proven to\nhave superior expressiveness compared to traditional merging methods on MLP\nmodels. It achieves SOTA results when tested on MLPs and transformers across\nvarious sizes, tasks, modalities, and distribution shifts, especially in\nlow-data scenarios.\n", "link": "http://arxiv.org/abs/2410.01483v2", "date": "2025-08-15", "relevancy": 1.9597, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5088}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5005}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foldable%20SuperNets%3A%20Scalable%20Merging%20of%20Transformers%20with%20Different%0A%20%20Initializations%20and%20Tasks&body=Title%3A%20Foldable%20SuperNets%3A%20Scalable%20Merging%20of%20Transformers%20with%20Different%0A%20%20Initializations%20and%20Tasks%0AAuthor%3A%20Edan%20Kinderman%20and%20Itay%20Hubara%20and%20Haggai%20Maron%20and%20Daniel%20Soudry%0AAbstract%3A%20%20%20Recent%20methods%20aim%20to%20merge%20neural%20networks%20%28NNs%29%20with%20identical%0Aarchitectures%20trained%20on%20different%20tasks%20into%20a%20single%20multi-task%20model.%20While%0Amost%20works%20focus%20on%20the%20simpler%20setup%20of%20merging%20NNs%20initialized%20from%20a%20common%0Apre-trained%20network%2C%20we%20target%20the%20harder%20problem%20of%20merging%20large%20transformers%0Atrained%20on%20different%20tasks%20from%20distinct%20initializations.%20We%20show%20that%0Atraditional%20merging%20methods%20fail%20catastrophically%20in%20this%20setup%2C%20while%0AKnowledge%20Distillation%20%28KD%29%20achieves%20much%20better%20results%2C%20though%20at%20a%20higher%0Acost.%20However%2C%20KD%20is%20data-inefficient%2C%20as%20it%20does%20not%20exploit%20the%20original%0Amodels%27%20weights.%20To%20solve%20this%2C%20we%20introduce%20%22Foldable%20SuperNet%20Merge%22%0A%28FS-Merge%29%2C%20which%20trains%20a%20SuperNet%20containing%20the%20original%20models%20%28with%20frozen%0Aweights%29%20using%20a%20feature%20reconstruction%20objective.%20After%20training%2C%20the%20SuperNet%0Ais%20folded%20back%20to%20the%20size%20of%20a%20single%20original%20model.%20FS-Merge%20is%20simple%2C%0Adata-efficient%2C%20has%20a%20computational%20cost%20comparable%20to%20KD%2C%20and%20is%20proven%20to%0Ahave%20superior%20expressiveness%20compared%20to%20traditional%20merging%20methods%20on%20MLP%0Amodels.%20It%20achieves%20SOTA%20results%20when%20tested%20on%20MLPs%20and%20transformers%20across%0Avarious%20sizes%2C%20tasks%2C%20modalities%2C%20and%20distribution%20shifts%2C%20especially%20in%0Alow-data%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01483v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoldable%2520SuperNets%253A%2520Scalable%2520Merging%2520of%2520Transformers%2520with%2520Different%250A%2520%2520Initializations%2520and%2520Tasks%26entry.906535625%3DEdan%2520Kinderman%2520and%2520Itay%2520Hubara%2520and%2520Haggai%2520Maron%2520and%2520Daniel%2520Soudry%26entry.1292438233%3D%2520%2520Recent%2520methods%2520aim%2520to%2520merge%2520neural%2520networks%2520%2528NNs%2529%2520with%2520identical%250Aarchitectures%2520trained%2520on%2520different%2520tasks%2520into%2520a%2520single%2520multi-task%2520model.%2520While%250Amost%2520works%2520focus%2520on%2520the%2520simpler%2520setup%2520of%2520merging%2520NNs%2520initialized%2520from%2520a%2520common%250Apre-trained%2520network%252C%2520we%2520target%2520the%2520harder%2520problem%2520of%2520merging%2520large%2520transformers%250Atrained%2520on%2520different%2520tasks%2520from%2520distinct%2520initializations.%2520We%2520show%2520that%250Atraditional%2520merging%2520methods%2520fail%2520catastrophically%2520in%2520this%2520setup%252C%2520while%250AKnowledge%2520Distillation%2520%2528KD%2529%2520achieves%2520much%2520better%2520results%252C%2520though%2520at%2520a%2520higher%250Acost.%2520However%252C%2520KD%2520is%2520data-inefficient%252C%2520as%2520it%2520does%2520not%2520exploit%2520the%2520original%250Amodels%2527%2520weights.%2520To%2520solve%2520this%252C%2520we%2520introduce%2520%2522Foldable%2520SuperNet%2520Merge%2522%250A%2528FS-Merge%2529%252C%2520which%2520trains%2520a%2520SuperNet%2520containing%2520the%2520original%2520models%2520%2528with%2520frozen%250Aweights%2529%2520using%2520a%2520feature%2520reconstruction%2520objective.%2520After%2520training%252C%2520the%2520SuperNet%250Ais%2520folded%2520back%2520to%2520the%2520size%2520of%2520a%2520single%2520original%2520model.%2520FS-Merge%2520is%2520simple%252C%250Adata-efficient%252C%2520has%2520a%2520computational%2520cost%2520comparable%2520to%2520KD%252C%2520and%2520is%2520proven%2520to%250Ahave%2520superior%2520expressiveness%2520compared%2520to%2520traditional%2520merging%2520methods%2520on%2520MLP%250Amodels.%2520It%2520achieves%2520SOTA%2520results%2520when%2520tested%2520on%2520MLPs%2520and%2520transformers%2520across%250Avarious%2520sizes%252C%2520tasks%252C%2520modalities%252C%2520and%2520distribution%2520shifts%252C%2520especially%2520in%250Alow-data%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01483v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foldable%20SuperNets%3A%20Scalable%20Merging%20of%20Transformers%20with%20Different%0A%20%20Initializations%20and%20Tasks&entry.906535625=Edan%20Kinderman%20and%20Itay%20Hubara%20and%20Haggai%20Maron%20and%20Daniel%20Soudry&entry.1292438233=%20%20Recent%20methods%20aim%20to%20merge%20neural%20networks%20%28NNs%29%20with%20identical%0Aarchitectures%20trained%20on%20different%20tasks%20into%20a%20single%20multi-task%20model.%20While%0Amost%20works%20focus%20on%20the%20simpler%20setup%20of%20merging%20NNs%20initialized%20from%20a%20common%0Apre-trained%20network%2C%20we%20target%20the%20harder%20problem%20of%20merging%20large%20transformers%0Atrained%20on%20different%20tasks%20from%20distinct%20initializations.%20We%20show%20that%0Atraditional%20merging%20methods%20fail%20catastrophically%20in%20this%20setup%2C%20while%0AKnowledge%20Distillation%20%28KD%29%20achieves%20much%20better%20results%2C%20though%20at%20a%20higher%0Acost.%20However%2C%20KD%20is%20data-inefficient%2C%20as%20it%20does%20not%20exploit%20the%20original%0Amodels%27%20weights.%20To%20solve%20this%2C%20we%20introduce%20%22Foldable%20SuperNet%20Merge%22%0A%28FS-Merge%29%2C%20which%20trains%20a%20SuperNet%20containing%20the%20original%20models%20%28with%20frozen%0Aweights%29%20using%20a%20feature%20reconstruction%20objective.%20After%20training%2C%20the%20SuperNet%0Ais%20folded%20back%20to%20the%20size%20of%20a%20single%20original%20model.%20FS-Merge%20is%20simple%2C%0Adata-efficient%2C%20has%20a%20computational%20cost%20comparable%20to%20KD%2C%20and%20is%20proven%20to%0Ahave%20superior%20expressiveness%20compared%20to%20traditional%20merging%20methods%20on%20MLP%0Amodels.%20It%20achieves%20SOTA%20results%20when%20tested%20on%20MLPs%20and%20transformers%20across%0Avarious%20sizes%2C%20tasks%2C%20modalities%2C%20and%20distribution%20shifts%2C%20especially%20in%0Alow-data%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01483v2&entry.124074799=Read"},
{"title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised\n  Fine-Tuning and Reinforcement Learning via Dynamic Weighting", "author": "Wenhao Zhang and Yuexiang Xie and Yuchang Sun and Yanxi Chen and Guoyin Wang and Yaliang Li and Bolin Ding and Jingren Zhou", "abstract": "  Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two\nprominent post-training paradigms for refining the capabilities and aligning\nthe behavior of Large Language Models (LLMs). Existing approaches that\nintegrate SFT and RL often face the risk of disrupting established model\npatterns and inducing overfitting to expert data. To address this, we present a\nnovel investigation into the unified view of SFT and RL through an off-policy\nversus on-policy lens. We propose CHORD, a framework for the Controllable\nHarmonization of On- and Off-Policy Reinforcement Learning via Dynamic\nWeighting, which reframes SFT not as a separate stage but as a dynamically\nweighted auxiliary objective within the on-policy RL process. Based on an\nanalysis of off-policy expert data's influence at both holistic and granular\nlevels, we incorporate a dual-control mechanism in CHORD. Specifically, the\nframework first employs a global coefficient to holistically guide the\ntransition from off-policy imitation to on-policy exploration, and then applies\na token-wise weighting function that enables granular learning from expert\ntokens, which preserves on-policy exploration and mitigates disruption from\noff-policy data. We conduct extensive experiments on widely used benchmarks,\nproviding empirical evidence that CHORD achieves a stable and efficient\nlearning process. By effectively harmonizing off-policy expert data with\non-policy exploration, CHORD demonstrates significant improvements over\nbaselines. We release the implementation at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to\ninspire further research.\n", "link": "http://arxiv.org/abs/2508.11408v1", "date": "2025-08-15", "relevancy": 1.9533, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4982}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4943}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On-Policy%20RL%20Meets%20Off-Policy%20Experts%3A%20Harmonizing%20Supervised%0A%20%20Fine-Tuning%20and%20Reinforcement%20Learning%20via%20Dynamic%20Weighting&body=Title%3A%20On-Policy%20RL%20Meets%20Off-Policy%20Experts%3A%20Harmonizing%20Supervised%0A%20%20Fine-Tuning%20and%20Reinforcement%20Learning%20via%20Dynamic%20Weighting%0AAuthor%3A%20Wenhao%20Zhang%20and%20Yuexiang%20Xie%20and%20Yuchang%20Sun%20and%20Yanxi%20Chen%20and%20Guoyin%20Wang%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Reinforcement%20Learning%20%28RL%29%20are%20two%0Aprominent%20post-training%20paradigms%20for%20refining%20the%20capabilities%20and%20aligning%0Athe%20behavior%20of%20Large%20Language%20Models%20%28LLMs%29.%20Existing%20approaches%20that%0Aintegrate%20SFT%20and%20RL%20often%20face%20the%20risk%20of%20disrupting%20established%20model%0Apatterns%20and%20inducing%20overfitting%20to%20expert%20data.%20To%20address%20this%2C%20we%20present%20a%0Anovel%20investigation%20into%20the%20unified%20view%20of%20SFT%20and%20RL%20through%20an%20off-policy%0Aversus%20on-policy%20lens.%20We%20propose%20CHORD%2C%20a%20framework%20for%20the%20Controllable%0AHarmonization%20of%20On-%20and%20Off-Policy%20Reinforcement%20Learning%20via%20Dynamic%0AWeighting%2C%20which%20reframes%20SFT%20not%20as%20a%20separate%20stage%20but%20as%20a%20dynamically%0Aweighted%20auxiliary%20objective%20within%20the%20on-policy%20RL%20process.%20Based%20on%20an%0Aanalysis%20of%20off-policy%20expert%20data%27s%20influence%20at%20both%20holistic%20and%20granular%0Alevels%2C%20we%20incorporate%20a%20dual-control%20mechanism%20in%20CHORD.%20Specifically%2C%20the%0Aframework%20first%20employs%20a%20global%20coefficient%20to%20holistically%20guide%20the%0Atransition%20from%20off-policy%20imitation%20to%20on-policy%20exploration%2C%20and%20then%20applies%0Aa%20token-wise%20weighting%20function%20that%20enables%20granular%20learning%20from%20expert%0Atokens%2C%20which%20preserves%20on-policy%20exploration%20and%20mitigates%20disruption%20from%0Aoff-policy%20data.%20We%20conduct%20extensive%20experiments%20on%20widely%20used%20benchmarks%2C%0Aproviding%20empirical%20evidence%20that%20CHORD%20achieves%20a%20stable%20and%20efficient%0Alearning%20process.%20By%20effectively%20harmonizing%20off-policy%20expert%20data%20with%0Aon-policy%20exploration%2C%20CHORD%20demonstrates%20significant%20improvements%20over%0Abaselines.%20We%20release%20the%20implementation%20at%0Ahttps%3A//github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord%20to%0Ainspire%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn-Policy%2520RL%2520Meets%2520Off-Policy%2520Experts%253A%2520Harmonizing%2520Supervised%250A%2520%2520Fine-Tuning%2520and%2520Reinforcement%2520Learning%2520via%2520Dynamic%2520Weighting%26entry.906535625%3DWenhao%2520Zhang%2520and%2520Yuexiang%2520Xie%2520and%2520Yuchang%2520Sun%2520and%2520Yanxi%2520Chen%2520and%2520Guoyin%2520Wang%2520and%2520Yaliang%2520Li%2520and%2520Bolin%2520Ding%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520and%2520Reinforcement%2520Learning%2520%2528RL%2529%2520are%2520two%250Aprominent%2520post-training%2520paradigms%2520for%2520refining%2520the%2520capabilities%2520and%2520aligning%250Athe%2520behavior%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Existing%2520approaches%2520that%250Aintegrate%2520SFT%2520and%2520RL%2520often%2520face%2520the%2520risk%2520of%2520disrupting%2520established%2520model%250Apatterns%2520and%2520inducing%2520overfitting%2520to%2520expert%2520data.%2520To%2520address%2520this%252C%2520we%2520present%2520a%250Anovel%2520investigation%2520into%2520the%2520unified%2520view%2520of%2520SFT%2520and%2520RL%2520through%2520an%2520off-policy%250Aversus%2520on-policy%2520lens.%2520We%2520propose%2520CHORD%252C%2520a%2520framework%2520for%2520the%2520Controllable%250AHarmonization%2520of%2520On-%2520and%2520Off-Policy%2520Reinforcement%2520Learning%2520via%2520Dynamic%250AWeighting%252C%2520which%2520reframes%2520SFT%2520not%2520as%2520a%2520separate%2520stage%2520but%2520as%2520a%2520dynamically%250Aweighted%2520auxiliary%2520objective%2520within%2520the%2520on-policy%2520RL%2520process.%2520Based%2520on%2520an%250Aanalysis%2520of%2520off-policy%2520expert%2520data%2527s%2520influence%2520at%2520both%2520holistic%2520and%2520granular%250Alevels%252C%2520we%2520incorporate%2520a%2520dual-control%2520mechanism%2520in%2520CHORD.%2520Specifically%252C%2520the%250Aframework%2520first%2520employs%2520a%2520global%2520coefficient%2520to%2520holistically%2520guide%2520the%250Atransition%2520from%2520off-policy%2520imitation%2520to%2520on-policy%2520exploration%252C%2520and%2520then%2520applies%250Aa%2520token-wise%2520weighting%2520function%2520that%2520enables%2520granular%2520learning%2520from%2520expert%250Atokens%252C%2520which%2520preserves%2520on-policy%2520exploration%2520and%2520mitigates%2520disruption%2520from%250Aoff-policy%2520data.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520widely%2520used%2520benchmarks%252C%250Aproviding%2520empirical%2520evidence%2520that%2520CHORD%2520achieves%2520a%2520stable%2520and%2520efficient%250Alearning%2520process.%2520By%2520effectively%2520harmonizing%2520off-policy%2520expert%2520data%2520with%250Aon-policy%2520exploration%252C%2520CHORD%2520demonstrates%2520significant%2520improvements%2520over%250Abaselines.%2520We%2520release%2520the%2520implementation%2520at%250Ahttps%253A//github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord%2520to%250Ainspire%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-Policy%20RL%20Meets%20Off-Policy%20Experts%3A%20Harmonizing%20Supervised%0A%20%20Fine-Tuning%20and%20Reinforcement%20Learning%20via%20Dynamic%20Weighting&entry.906535625=Wenhao%20Zhang%20and%20Yuexiang%20Xie%20and%20Yuchang%20Sun%20and%20Yanxi%20Chen%20and%20Guoyin%20Wang%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Jingren%20Zhou&entry.1292438233=%20%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Reinforcement%20Learning%20%28RL%29%20are%20two%0Aprominent%20post-training%20paradigms%20for%20refining%20the%20capabilities%20and%20aligning%0Athe%20behavior%20of%20Large%20Language%20Models%20%28LLMs%29.%20Existing%20approaches%20that%0Aintegrate%20SFT%20and%20RL%20often%20face%20the%20risk%20of%20disrupting%20established%20model%0Apatterns%20and%20inducing%20overfitting%20to%20expert%20data.%20To%20address%20this%2C%20we%20present%20a%0Anovel%20investigation%20into%20the%20unified%20view%20of%20SFT%20and%20RL%20through%20an%20off-policy%0Aversus%20on-policy%20lens.%20We%20propose%20CHORD%2C%20a%20framework%20for%20the%20Controllable%0AHarmonization%20of%20On-%20and%20Off-Policy%20Reinforcement%20Learning%20via%20Dynamic%0AWeighting%2C%20which%20reframes%20SFT%20not%20as%20a%20separate%20stage%20but%20as%20a%20dynamically%0Aweighted%20auxiliary%20objective%20within%20the%20on-policy%20RL%20process.%20Based%20on%20an%0Aanalysis%20of%20off-policy%20expert%20data%27s%20influence%20at%20both%20holistic%20and%20granular%0Alevels%2C%20we%20incorporate%20a%20dual-control%20mechanism%20in%20CHORD.%20Specifically%2C%20the%0Aframework%20first%20employs%20a%20global%20coefficient%20to%20holistically%20guide%20the%0Atransition%20from%20off-policy%20imitation%20to%20on-policy%20exploration%2C%20and%20then%20applies%0Aa%20token-wise%20weighting%20function%20that%20enables%20granular%20learning%20from%20expert%0Atokens%2C%20which%20preserves%20on-policy%20exploration%20and%20mitigates%20disruption%20from%0Aoff-policy%20data.%20We%20conduct%20extensive%20experiments%20on%20widely%20used%20benchmarks%2C%0Aproviding%20empirical%20evidence%20that%20CHORD%20achieves%20a%20stable%20and%20efficient%0Alearning%20process.%20By%20effectively%20harmonizing%20off-policy%20expert%20data%20with%0Aon-policy%20exploration%2C%20CHORD%20demonstrates%20significant%20improvements%20over%0Abaselines.%20We%20release%20the%20implementation%20at%0Ahttps%3A//github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord%20to%0Ainspire%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11408v1&entry.124074799=Read"},
{"title": "Generative Co-Design of Antibody Sequences and Structures via Black-Box\n  Guidance in a Shared Latent Space", "author": "Yinghua Yao and Yuangang Pan and Xixian Chen", "abstract": "  Advancements in deep generative models have enabled the joint modeling of\nantibody sequence and structure, given the antigen-antibody complex as context.\nHowever, existing approaches for optimizing complementarity-determining regions\n(CDRs) to improve developability properties operate in the raw data space,\nleading to excessively costly evaluations due to the inefficient search\nprocess. To address this, we propose LatEnt blAck-box Design (LEAD), a\nsequence-structure co-design framework that optimizes both sequence and\nstructure within their shared latent space. Optimizing shared latent codes can\nnot only break through the limitations of existing methods, but also ensure\nsynchronization of different modality designs. Particularly, we design a\nblack-box guidance strategy to accommodate real-world scenarios where many\nproperty evaluators are non-differentiable. Experimental results demonstrate\nthat our LEAD achieves superior optimization performance for both single and\nmulti-property objectives. Notably, LEAD reduces query consumption by a half\nwhile surpassing baseline methods in property optimization. The code is\navailable at https://github.com/EvaFlower/LatEnt-blAck-box-Design.\n", "link": "http://arxiv.org/abs/2508.11424v1", "date": "2025-08-15", "relevancy": 1.9529, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5172}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4881}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Co-Design%20of%20Antibody%20Sequences%20and%20Structures%20via%20Black-Box%0A%20%20Guidance%20in%20a%20Shared%20Latent%20Space&body=Title%3A%20Generative%20Co-Design%20of%20Antibody%20Sequences%20and%20Structures%20via%20Black-Box%0A%20%20Guidance%20in%20a%20Shared%20Latent%20Space%0AAuthor%3A%20Yinghua%20Yao%20and%20Yuangang%20Pan%20and%20Xixian%20Chen%0AAbstract%3A%20%20%20Advancements%20in%20deep%20generative%20models%20have%20enabled%20the%20joint%20modeling%20of%0Aantibody%20sequence%20and%20structure%2C%20given%20the%20antigen-antibody%20complex%20as%20context.%0AHowever%2C%20existing%20approaches%20for%20optimizing%20complementarity-determining%20regions%0A%28CDRs%29%20to%20improve%20developability%20properties%20operate%20in%20the%20raw%20data%20space%2C%0Aleading%20to%20excessively%20costly%20evaluations%20due%20to%20the%20inefficient%20search%0Aprocess.%20To%20address%20this%2C%20we%20propose%20LatEnt%20blAck-box%20Design%20%28LEAD%29%2C%20a%0Asequence-structure%20co-design%20framework%20that%20optimizes%20both%20sequence%20and%0Astructure%20within%20their%20shared%20latent%20space.%20Optimizing%20shared%20latent%20codes%20can%0Anot%20only%20break%20through%20the%20limitations%20of%20existing%20methods%2C%20but%20also%20ensure%0Asynchronization%20of%20different%20modality%20designs.%20Particularly%2C%20we%20design%20a%0Ablack-box%20guidance%20strategy%20to%20accommodate%20real-world%20scenarios%20where%20many%0Aproperty%20evaluators%20are%20non-differentiable.%20Experimental%20results%20demonstrate%0Athat%20our%20LEAD%20achieves%20superior%20optimization%20performance%20for%20both%20single%20and%0Amulti-property%20objectives.%20Notably%2C%20LEAD%20reduces%20query%20consumption%20by%20a%20half%0Awhile%20surpassing%20baseline%20methods%20in%20property%20optimization.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/EvaFlower/LatEnt-blAck-box-Design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Co-Design%2520of%2520Antibody%2520Sequences%2520and%2520Structures%2520via%2520Black-Box%250A%2520%2520Guidance%2520in%2520a%2520Shared%2520Latent%2520Space%26entry.906535625%3DYinghua%2520Yao%2520and%2520Yuangang%2520Pan%2520and%2520Xixian%2520Chen%26entry.1292438233%3D%2520%2520Advancements%2520in%2520deep%2520generative%2520models%2520have%2520enabled%2520the%2520joint%2520modeling%2520of%250Aantibody%2520sequence%2520and%2520structure%252C%2520given%2520the%2520antigen-antibody%2520complex%2520as%2520context.%250AHowever%252C%2520existing%2520approaches%2520for%2520optimizing%2520complementarity-determining%2520regions%250A%2528CDRs%2529%2520to%2520improve%2520developability%2520properties%2520operate%2520in%2520the%2520raw%2520data%2520space%252C%250Aleading%2520to%2520excessively%2520costly%2520evaluations%2520due%2520to%2520the%2520inefficient%2520search%250Aprocess.%2520To%2520address%2520this%252C%2520we%2520propose%2520LatEnt%2520blAck-box%2520Design%2520%2528LEAD%2529%252C%2520a%250Asequence-structure%2520co-design%2520framework%2520that%2520optimizes%2520both%2520sequence%2520and%250Astructure%2520within%2520their%2520shared%2520latent%2520space.%2520Optimizing%2520shared%2520latent%2520codes%2520can%250Anot%2520only%2520break%2520through%2520the%2520limitations%2520of%2520existing%2520methods%252C%2520but%2520also%2520ensure%250Asynchronization%2520of%2520different%2520modality%2520designs.%2520Particularly%252C%2520we%2520design%2520a%250Ablack-box%2520guidance%2520strategy%2520to%2520accommodate%2520real-world%2520scenarios%2520where%2520many%250Aproperty%2520evaluators%2520are%2520non-differentiable.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520LEAD%2520achieves%2520superior%2520optimization%2520performance%2520for%2520both%2520single%2520and%250Amulti-property%2520objectives.%2520Notably%252C%2520LEAD%2520reduces%2520query%2520consumption%2520by%2520a%2520half%250Awhile%2520surpassing%2520baseline%2520methods%2520in%2520property%2520optimization.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/EvaFlower/LatEnt-blAck-box-Design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Co-Design%20of%20Antibody%20Sequences%20and%20Structures%20via%20Black-Box%0A%20%20Guidance%20in%20a%20Shared%20Latent%20Space&entry.906535625=Yinghua%20Yao%20and%20Yuangang%20Pan%20and%20Xixian%20Chen&entry.1292438233=%20%20Advancements%20in%20deep%20generative%20models%20have%20enabled%20the%20joint%20modeling%20of%0Aantibody%20sequence%20and%20structure%2C%20given%20the%20antigen-antibody%20complex%20as%20context.%0AHowever%2C%20existing%20approaches%20for%20optimizing%20complementarity-determining%20regions%0A%28CDRs%29%20to%20improve%20developability%20properties%20operate%20in%20the%20raw%20data%20space%2C%0Aleading%20to%20excessively%20costly%20evaluations%20due%20to%20the%20inefficient%20search%0Aprocess.%20To%20address%20this%2C%20we%20propose%20LatEnt%20blAck-box%20Design%20%28LEAD%29%2C%20a%0Asequence-structure%20co-design%20framework%20that%20optimizes%20both%20sequence%20and%0Astructure%20within%20their%20shared%20latent%20space.%20Optimizing%20shared%20latent%20codes%20can%0Anot%20only%20break%20through%20the%20limitations%20of%20existing%20methods%2C%20but%20also%20ensure%0Asynchronization%20of%20different%20modality%20designs.%20Particularly%2C%20we%20design%20a%0Ablack-box%20guidance%20strategy%20to%20accommodate%20real-world%20scenarios%20where%20many%0Aproperty%20evaluators%20are%20non-differentiable.%20Experimental%20results%20demonstrate%0Athat%20our%20LEAD%20achieves%20superior%20optimization%20performance%20for%20both%20single%20and%0Amulti-property%20objectives.%20Notably%2C%20LEAD%20reduces%20query%20consumption%20by%20a%20half%0Awhile%20surpassing%20baseline%20methods%20in%20property%20optimization.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/EvaFlower/LatEnt-blAck-box-Design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11424v1&entry.124074799=Read"},
{"title": "CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement\n  Membrane Segmentation", "author": "Hongjin Fang and Daniel Reisenb\u00fcchler and Kenji Ikemura and Mert R. Sabuncu and Yihe Yang and Ruining Deng", "abstract": "  Accurate segmentation of the glomerular basement membrane (GBM) in electron\nmicroscopy (EM) images is fundamental for quantifying membrane thickness and\nsupporting the diagnosis of various kidney diseases. While supervised deep\nlearning approaches achieve high segmentation accuracy, their reliance on\nextensive pixel-level annotation renders them impractical for clinical\nworkflows. Few-shot learning can reduce this annotation burden but often\nstruggles to capture the fine structural details necessary for GBM analysis. In\nthis study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot\nsegmentation pipeline designed for GBM delineation in EM images. CoFi first\ntrains a lightweight neural network using only three annotated images to\nproduce an initial coarse segmentation mask. This mask is then automatically\nprocessed to generate high-quality point prompts with morphology-aware pruning,\nwhich are subsequently used to guide SAM in refining the segmentation. The\nproposed method achieved exceptional GBM segmentation performance, with a Dice\ncoefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that\nCoFi not only alleviates the annotation and computational burdens associated\nwith conventional methods, but also achieves accurate and reliable segmentation\nresults. The pipeline's speed and annotation efficiency make it well-suited for\nresearch and hold strong potential for clinical applications in renal\npathology. The pipeline is publicly available at:\nhttps://github.com/ddrrnn123/CoFi.\n", "link": "http://arxiv.org/abs/2508.11469v1", "date": "2025-08-15", "relevancy": 1.9522, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4886}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.488}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoFi%3A%20A%20Fast%20Coarse-to-Fine%20Few-Shot%20Pipeline%20for%20Glomerular%20Basement%0A%20%20Membrane%20Segmentation&body=Title%3A%20CoFi%3A%20A%20Fast%20Coarse-to-Fine%20Few-Shot%20Pipeline%20for%20Glomerular%20Basement%0A%20%20Membrane%20Segmentation%0AAuthor%3A%20Hongjin%20Fang%20and%20Daniel%20Reisenb%C3%BCchler%20and%20Kenji%20Ikemura%20and%20Mert%20R.%20Sabuncu%20and%20Yihe%20Yang%20and%20Ruining%20Deng%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20the%20glomerular%20basement%20membrane%20%28GBM%29%20in%20electron%0Amicroscopy%20%28EM%29%20images%20is%20fundamental%20for%20quantifying%20membrane%20thickness%20and%0Asupporting%20the%20diagnosis%20of%20various%20kidney%20diseases.%20While%20supervised%20deep%0Alearning%20approaches%20achieve%20high%20segmentation%20accuracy%2C%20their%20reliance%20on%0Aextensive%20pixel-level%20annotation%20renders%20them%20impractical%20for%20clinical%0Aworkflows.%20Few-shot%20learning%20can%20reduce%20this%20annotation%20burden%20but%20often%0Astruggles%20to%20capture%20the%20fine%20structural%20details%20necessary%20for%20GBM%20analysis.%20In%0Athis%20study%2C%20we%20introduce%20CoFi%2C%20a%20fast%20and%20efficient%20coarse-to-fine%20few-shot%0Asegmentation%20pipeline%20designed%20for%20GBM%20delineation%20in%20EM%20images.%20CoFi%20first%0Atrains%20a%20lightweight%20neural%20network%20using%20only%20three%20annotated%20images%20to%0Aproduce%20an%20initial%20coarse%20segmentation%20mask.%20This%20mask%20is%20then%20automatically%0Aprocessed%20to%20generate%20high-quality%20point%20prompts%20with%20morphology-aware%20pruning%2C%0Awhich%20are%20subsequently%20used%20to%20guide%20SAM%20in%20refining%20the%20segmentation.%20The%0Aproposed%20method%20achieved%20exceptional%20GBM%20segmentation%20performance%2C%20with%20a%20Dice%0Acoefficient%20of%2074.54%25%20and%20an%20inference%20speed%20of%201.9%20FPS.%20We%20demonstrate%20that%0ACoFi%20not%20only%20alleviates%20the%20annotation%20and%20computational%20burdens%20associated%0Awith%20conventional%20methods%2C%20but%20also%20achieves%20accurate%20and%20reliable%20segmentation%0Aresults.%20The%20pipeline%27s%20speed%20and%20annotation%20efficiency%20make%20it%20well-suited%20for%0Aresearch%20and%20hold%20strong%20potential%20for%20clinical%20applications%20in%20renal%0Apathology.%20The%20pipeline%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/ddrrnn123/CoFi.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoFi%253A%2520A%2520Fast%2520Coarse-to-Fine%2520Few-Shot%2520Pipeline%2520for%2520Glomerular%2520Basement%250A%2520%2520Membrane%2520Segmentation%26entry.906535625%3DHongjin%2520Fang%2520and%2520Daniel%2520Reisenb%25C3%25BCchler%2520and%2520Kenji%2520Ikemura%2520and%2520Mert%2520R.%2520Sabuncu%2520and%2520Yihe%2520Yang%2520and%2520Ruining%2520Deng%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520the%2520glomerular%2520basement%2520membrane%2520%2528GBM%2529%2520in%2520electron%250Amicroscopy%2520%2528EM%2529%2520images%2520is%2520fundamental%2520for%2520quantifying%2520membrane%2520thickness%2520and%250Asupporting%2520the%2520diagnosis%2520of%2520various%2520kidney%2520diseases.%2520While%2520supervised%2520deep%250Alearning%2520approaches%2520achieve%2520high%2520segmentation%2520accuracy%252C%2520their%2520reliance%2520on%250Aextensive%2520pixel-level%2520annotation%2520renders%2520them%2520impractical%2520for%2520clinical%250Aworkflows.%2520Few-shot%2520learning%2520can%2520reduce%2520this%2520annotation%2520burden%2520but%2520often%250Astruggles%2520to%2520capture%2520the%2520fine%2520structural%2520details%2520necessary%2520for%2520GBM%2520analysis.%2520In%250Athis%2520study%252C%2520we%2520introduce%2520CoFi%252C%2520a%2520fast%2520and%2520efficient%2520coarse-to-fine%2520few-shot%250Asegmentation%2520pipeline%2520designed%2520for%2520GBM%2520delineation%2520in%2520EM%2520images.%2520CoFi%2520first%250Atrains%2520a%2520lightweight%2520neural%2520network%2520using%2520only%2520three%2520annotated%2520images%2520to%250Aproduce%2520an%2520initial%2520coarse%2520segmentation%2520mask.%2520This%2520mask%2520is%2520then%2520automatically%250Aprocessed%2520to%2520generate%2520high-quality%2520point%2520prompts%2520with%2520morphology-aware%2520pruning%252C%250Awhich%2520are%2520subsequently%2520used%2520to%2520guide%2520SAM%2520in%2520refining%2520the%2520segmentation.%2520The%250Aproposed%2520method%2520achieved%2520exceptional%2520GBM%2520segmentation%2520performance%252C%2520with%2520a%2520Dice%250Acoefficient%2520of%252074.54%2525%2520and%2520an%2520inference%2520speed%2520of%25201.9%2520FPS.%2520We%2520demonstrate%2520that%250ACoFi%2520not%2520only%2520alleviates%2520the%2520annotation%2520and%2520computational%2520burdens%2520associated%250Awith%2520conventional%2520methods%252C%2520but%2520also%2520achieves%2520accurate%2520and%2520reliable%2520segmentation%250Aresults.%2520The%2520pipeline%2527s%2520speed%2520and%2520annotation%2520efficiency%2520make%2520it%2520well-suited%2520for%250Aresearch%2520and%2520hold%2520strong%2520potential%2520for%2520clinical%2520applications%2520in%2520renal%250Apathology.%2520The%2520pipeline%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/ddrrnn123/CoFi.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoFi%3A%20A%20Fast%20Coarse-to-Fine%20Few-Shot%20Pipeline%20for%20Glomerular%20Basement%0A%20%20Membrane%20Segmentation&entry.906535625=Hongjin%20Fang%20and%20Daniel%20Reisenb%C3%BCchler%20and%20Kenji%20Ikemura%20and%20Mert%20R.%20Sabuncu%20and%20Yihe%20Yang%20and%20Ruining%20Deng&entry.1292438233=%20%20Accurate%20segmentation%20of%20the%20glomerular%20basement%20membrane%20%28GBM%29%20in%20electron%0Amicroscopy%20%28EM%29%20images%20is%20fundamental%20for%20quantifying%20membrane%20thickness%20and%0Asupporting%20the%20diagnosis%20of%20various%20kidney%20diseases.%20While%20supervised%20deep%0Alearning%20approaches%20achieve%20high%20segmentation%20accuracy%2C%20their%20reliance%20on%0Aextensive%20pixel-level%20annotation%20renders%20them%20impractical%20for%20clinical%0Aworkflows.%20Few-shot%20learning%20can%20reduce%20this%20annotation%20burden%20but%20often%0Astruggles%20to%20capture%20the%20fine%20structural%20details%20necessary%20for%20GBM%20analysis.%20In%0Athis%20study%2C%20we%20introduce%20CoFi%2C%20a%20fast%20and%20efficient%20coarse-to-fine%20few-shot%0Asegmentation%20pipeline%20designed%20for%20GBM%20delineation%20in%20EM%20images.%20CoFi%20first%0Atrains%20a%20lightweight%20neural%20network%20using%20only%20three%20annotated%20images%20to%0Aproduce%20an%20initial%20coarse%20segmentation%20mask.%20This%20mask%20is%20then%20automatically%0Aprocessed%20to%20generate%20high-quality%20point%20prompts%20with%20morphology-aware%20pruning%2C%0Awhich%20are%20subsequently%20used%20to%20guide%20SAM%20in%20refining%20the%20segmentation.%20The%0Aproposed%20method%20achieved%20exceptional%20GBM%20segmentation%20performance%2C%20with%20a%20Dice%0Acoefficient%20of%2074.54%25%20and%20an%20inference%20speed%20of%201.9%20FPS.%20We%20demonstrate%20that%0ACoFi%20not%20only%20alleviates%20the%20annotation%20and%20computational%20burdens%20associated%0Awith%20conventional%20methods%2C%20but%20also%20achieves%20accurate%20and%20reliable%20segmentation%0Aresults.%20The%20pipeline%27s%20speed%20and%20annotation%20efficiency%20make%20it%20well-suited%20for%0Aresearch%20and%20hold%20strong%20potential%20for%20clinical%20applications%20in%20renal%0Apathology.%20The%20pipeline%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/ddrrnn123/CoFi.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11469v1&entry.124074799=Read"},
{"title": "Retrieval-augmented reasoning with lean language models", "author": "Ryan Sze-Yin Chan and Federico Nanni and Tomas Lazauskas and Rosie Wood and Penelope Yong and Lionel Tarassenko and Mark Girolami and James Geddes and Andrew Duncan", "abstract": "  This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains.\n", "link": "http://arxiv.org/abs/2508.11386v1", "date": "2025-08-15", "relevancy": 1.9508, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval-augmented%20reasoning%20with%20lean%20language%20models&body=Title%3A%20Retrieval-augmented%20reasoning%20with%20lean%20language%20models%0AAuthor%3A%20Ryan%20Sze-Yin%20Chan%20and%20Federico%20Nanni%20and%20Tomas%20Lazauskas%20and%20Rosie%20Wood%20and%20Penelope%20Yong%20and%20Lionel%20Tarassenko%20and%20Mark%20Girolami%20and%20James%20Geddes%20and%20Andrew%20Duncan%0AAbstract%3A%20%20%20This%20technical%20report%20details%20a%20novel%20approach%20to%20combining%20reasoning%20and%0Aretrieval%20augmented%20generation%20%28RAG%29%20within%20a%20single%2C%20lean%20language%20model%0Aarchitecture.%20While%20existing%20RAG%20systems%20typically%20rely%20on%20large-scale%20models%0Aand%20external%20APIs%2C%20our%20work%20addresses%20the%20increasing%20demand%20for%20performant%20and%0Aprivacy-preserving%20solutions%20deployable%20in%20resource-constrained%20or%20secure%0Aenvironments.%20Building%20on%20recent%20developments%20in%20test-time%20scaling%20and%0Asmall-scale%20reasoning%20models%2C%20we%20develop%20a%20retrieval%20augmented%20conversational%0Aagent%20capable%20of%20interpreting%20complex%2C%20domain-specific%20queries%20using%20a%0Alightweight%20backbone%20model.%20Our%20system%20integrates%20a%20dense%20retriever%20with%0Afine-tuned%20Qwen2.5-Instruct%20models%2C%20using%20synthetic%20query%20generation%20and%0Areasoning%20traces%20derived%20from%20frontier%20models%20%28e.g.%2C%20DeepSeek-R1%29%20over%20a%0Acurated%20corpus%2C%20in%20this%20case%2C%20the%20NHS%20A-to-Z%20condition%20pages.%20We%20explore%20the%0Aimpact%20of%20summarisation-based%20document%20compression%2C%20synthetic%20data%20design%2C%20and%0Areasoning-aware%20fine-tuning%20on%20model%20performance.%20Evaluation%20against%20both%0Anon-reasoning%20and%20general-purpose%20lean%20models%20demonstrates%20that%20our%0Adomain-specific%20fine-tuning%20approach%20yields%20substantial%20gains%20in%20answer%0Aaccuracy%20and%20consistency%2C%20approaching%20frontier-level%20performance%20while%0Aremaining%20feasible%20for%20local%20deployment.%20All%20implementation%20details%20and%20code%0Aare%20publicly%20released%20to%20support%20reproducibility%20and%20adaptation%20across%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval-augmented%2520reasoning%2520with%2520lean%2520language%2520models%26entry.906535625%3DRyan%2520Sze-Yin%2520Chan%2520and%2520Federico%2520Nanni%2520and%2520Tomas%2520Lazauskas%2520and%2520Rosie%2520Wood%2520and%2520Penelope%2520Yong%2520and%2520Lionel%2520Tarassenko%2520and%2520Mark%2520Girolami%2520and%2520James%2520Geddes%2520and%2520Andrew%2520Duncan%26entry.1292438233%3D%2520%2520This%2520technical%2520report%2520details%2520a%2520novel%2520approach%2520to%2520combining%2520reasoning%2520and%250Aretrieval%2520augmented%2520generation%2520%2528RAG%2529%2520within%2520a%2520single%252C%2520lean%2520language%2520model%250Aarchitecture.%2520While%2520existing%2520RAG%2520systems%2520typically%2520rely%2520on%2520large-scale%2520models%250Aand%2520external%2520APIs%252C%2520our%2520work%2520addresses%2520the%2520increasing%2520demand%2520for%2520performant%2520and%250Aprivacy-preserving%2520solutions%2520deployable%2520in%2520resource-constrained%2520or%2520secure%250Aenvironments.%2520Building%2520on%2520recent%2520developments%2520in%2520test-time%2520scaling%2520and%250Asmall-scale%2520reasoning%2520models%252C%2520we%2520develop%2520a%2520retrieval%2520augmented%2520conversational%250Aagent%2520capable%2520of%2520interpreting%2520complex%252C%2520domain-specific%2520queries%2520using%2520a%250Alightweight%2520backbone%2520model.%2520Our%2520system%2520integrates%2520a%2520dense%2520retriever%2520with%250Afine-tuned%2520Qwen2.5-Instruct%2520models%252C%2520using%2520synthetic%2520query%2520generation%2520and%250Areasoning%2520traces%2520derived%2520from%2520frontier%2520models%2520%2528e.g.%252C%2520DeepSeek-R1%2529%2520over%2520a%250Acurated%2520corpus%252C%2520in%2520this%2520case%252C%2520the%2520NHS%2520A-to-Z%2520condition%2520pages.%2520We%2520explore%2520the%250Aimpact%2520of%2520summarisation-based%2520document%2520compression%252C%2520synthetic%2520data%2520design%252C%2520and%250Areasoning-aware%2520fine-tuning%2520on%2520model%2520performance.%2520Evaluation%2520against%2520both%250Anon-reasoning%2520and%2520general-purpose%2520lean%2520models%2520demonstrates%2520that%2520our%250Adomain-specific%2520fine-tuning%2520approach%2520yields%2520substantial%2520gains%2520in%2520answer%250Aaccuracy%2520and%2520consistency%252C%2520approaching%2520frontier-level%2520performance%2520while%250Aremaining%2520feasible%2520for%2520local%2520deployment.%2520All%2520implementation%2520details%2520and%2520code%250Aare%2520publicly%2520released%2520to%2520support%2520reproducibility%2520and%2520adaptation%2520across%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval-augmented%20reasoning%20with%20lean%20language%20models&entry.906535625=Ryan%20Sze-Yin%20Chan%20and%20Federico%20Nanni%20and%20Tomas%20Lazauskas%20and%20Rosie%20Wood%20and%20Penelope%20Yong%20and%20Lionel%20Tarassenko%20and%20Mark%20Girolami%20and%20James%20Geddes%20and%20Andrew%20Duncan&entry.1292438233=%20%20This%20technical%20report%20details%20a%20novel%20approach%20to%20combining%20reasoning%20and%0Aretrieval%20augmented%20generation%20%28RAG%29%20within%20a%20single%2C%20lean%20language%20model%0Aarchitecture.%20While%20existing%20RAG%20systems%20typically%20rely%20on%20large-scale%20models%0Aand%20external%20APIs%2C%20our%20work%20addresses%20the%20increasing%20demand%20for%20performant%20and%0Aprivacy-preserving%20solutions%20deployable%20in%20resource-constrained%20or%20secure%0Aenvironments.%20Building%20on%20recent%20developments%20in%20test-time%20scaling%20and%0Asmall-scale%20reasoning%20models%2C%20we%20develop%20a%20retrieval%20augmented%20conversational%0Aagent%20capable%20of%20interpreting%20complex%2C%20domain-specific%20queries%20using%20a%0Alightweight%20backbone%20model.%20Our%20system%20integrates%20a%20dense%20retriever%20with%0Afine-tuned%20Qwen2.5-Instruct%20models%2C%20using%20synthetic%20query%20generation%20and%0Areasoning%20traces%20derived%20from%20frontier%20models%20%28e.g.%2C%20DeepSeek-R1%29%20over%20a%0Acurated%20corpus%2C%20in%20this%20case%2C%20the%20NHS%20A-to-Z%20condition%20pages.%20We%20explore%20the%0Aimpact%20of%20summarisation-based%20document%20compression%2C%20synthetic%20data%20design%2C%20and%0Areasoning-aware%20fine-tuning%20on%20model%20performance.%20Evaluation%20against%20both%0Anon-reasoning%20and%20general-purpose%20lean%20models%20demonstrates%20that%20our%0Adomain-specific%20fine-tuning%20approach%20yields%20substantial%20gains%20in%20answer%0Aaccuracy%20and%20consistency%2C%20approaching%20frontier-level%20performance%20while%0Aremaining%20feasible%20for%20local%20deployment.%20All%20implementation%20details%20and%20code%0Aare%20publicly%20released%20to%20support%20reproducibility%20and%20adaptation%20across%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11386v1&entry.124074799=Read"},
{"title": "Once Upon an AI: Six Scaffolds for Child-AI Interaction Design, Inspired\n  by Disney", "author": "Nomisha Kurian", "abstract": "  To build AI that children can intuitively understand and benefit from,\ndesigners need a design grammar that serves their developmental needs. This\npaper bridges artificial intelligence design for children - an emerging field\nstill defining its best practices - and animation, a well established field\nwith decades of experience in engaging children through accessible\nstorytelling. Pairing Piagetian developmental theory with design pattern\nextraction from 52 works of animation, the paper presents a six scaffold\nframework that integrates design insights transferable to child centred AI\ndesign: (1) signals for visual animacy and clarity, (2) sound for musical and\nauditory scaffolding, (3) synchrony in audiovisual cues, (4) sidekick style\npersonas, (5) storyplay that supports symbolic play and imaginative\nexploration, and (6) structure in the form of predictable narratives. These\nstrategies, long refined in animation, function as multimodal scaffolds for\nattention, understanding, and attunement, supporting learning and comfort. This\nstructured design grammar is transferable to AI design. By reframing cinematic\nstorytelling and child development theory as design logic for AI, the paper\noffers heuristics for AI that aligns with the cognitive stages and emotional\nneeds of young users. The work contributes to design theory by showing how\nsensory, affective, and narrative techniques can inform developmentally attuned\nAI design. Future directions include empirical testing, cultural adaptation,\nand participatory co design.\n", "link": "http://arxiv.org/abs/2504.08670v3", "date": "2025-08-15", "relevancy": 1.94, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5071}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4814}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Once%20Upon%20an%20AI%3A%20Six%20Scaffolds%20for%20Child-AI%20Interaction%20Design%2C%20Inspired%0A%20%20by%20Disney&body=Title%3A%20Once%20Upon%20an%20AI%3A%20Six%20Scaffolds%20for%20Child-AI%20Interaction%20Design%2C%20Inspired%0A%20%20by%20Disney%0AAuthor%3A%20Nomisha%20Kurian%0AAbstract%3A%20%20%20To%20build%20AI%20that%20children%20can%20intuitively%20understand%20and%20benefit%20from%2C%0Adesigners%20need%20a%20design%20grammar%20that%20serves%20their%20developmental%20needs.%20This%0Apaper%20bridges%20artificial%20intelligence%20design%20for%20children%20-%20an%20emerging%20field%0Astill%20defining%20its%20best%20practices%20-%20and%20animation%2C%20a%20well%20established%20field%0Awith%20decades%20of%20experience%20in%20engaging%20children%20through%20accessible%0Astorytelling.%20Pairing%20Piagetian%20developmental%20theory%20with%20design%20pattern%0Aextraction%20from%2052%20works%20of%20animation%2C%20the%20paper%20presents%20a%20six%20scaffold%0Aframework%20that%20integrates%20design%20insights%20transferable%20to%20child%20centred%20AI%0Adesign%3A%20%281%29%20signals%20for%20visual%20animacy%20and%20clarity%2C%20%282%29%20sound%20for%20musical%20and%0Aauditory%20scaffolding%2C%20%283%29%20synchrony%20in%20audiovisual%20cues%2C%20%284%29%20sidekick%20style%0Apersonas%2C%20%285%29%20storyplay%20that%20supports%20symbolic%20play%20and%20imaginative%0Aexploration%2C%20and%20%286%29%20structure%20in%20the%20form%20of%20predictable%20narratives.%20These%0Astrategies%2C%20long%20refined%20in%20animation%2C%20function%20as%20multimodal%20scaffolds%20for%0Aattention%2C%20understanding%2C%20and%20attunement%2C%20supporting%20learning%20and%20comfort.%20This%0Astructured%20design%20grammar%20is%20transferable%20to%20AI%20design.%20By%20reframing%20cinematic%0Astorytelling%20and%20child%20development%20theory%20as%20design%20logic%20for%20AI%2C%20the%20paper%0Aoffers%20heuristics%20for%20AI%20that%20aligns%20with%20the%20cognitive%20stages%20and%20emotional%0Aneeds%20of%20young%20users.%20The%20work%20contributes%20to%20design%20theory%20by%20showing%20how%0Asensory%2C%20affective%2C%20and%20narrative%20techniques%20can%20inform%20developmentally%20attuned%0AAI%20design.%20Future%20directions%20include%20empirical%20testing%2C%20cultural%20adaptation%2C%0Aand%20participatory%20co%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08670v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnce%2520Upon%2520an%2520AI%253A%2520Six%2520Scaffolds%2520for%2520Child-AI%2520Interaction%2520Design%252C%2520Inspired%250A%2520%2520by%2520Disney%26entry.906535625%3DNomisha%2520Kurian%26entry.1292438233%3D%2520%2520To%2520build%2520AI%2520that%2520children%2520can%2520intuitively%2520understand%2520and%2520benefit%2520from%252C%250Adesigners%2520need%2520a%2520design%2520grammar%2520that%2520serves%2520their%2520developmental%2520needs.%2520This%250Apaper%2520bridges%2520artificial%2520intelligence%2520design%2520for%2520children%2520-%2520an%2520emerging%2520field%250Astill%2520defining%2520its%2520best%2520practices%2520-%2520and%2520animation%252C%2520a%2520well%2520established%2520field%250Awith%2520decades%2520of%2520experience%2520in%2520engaging%2520children%2520through%2520accessible%250Astorytelling.%2520Pairing%2520Piagetian%2520developmental%2520theory%2520with%2520design%2520pattern%250Aextraction%2520from%252052%2520works%2520of%2520animation%252C%2520the%2520paper%2520presents%2520a%2520six%2520scaffold%250Aframework%2520that%2520integrates%2520design%2520insights%2520transferable%2520to%2520child%2520centred%2520AI%250Adesign%253A%2520%25281%2529%2520signals%2520for%2520visual%2520animacy%2520and%2520clarity%252C%2520%25282%2529%2520sound%2520for%2520musical%2520and%250Aauditory%2520scaffolding%252C%2520%25283%2529%2520synchrony%2520in%2520audiovisual%2520cues%252C%2520%25284%2529%2520sidekick%2520style%250Apersonas%252C%2520%25285%2529%2520storyplay%2520that%2520supports%2520symbolic%2520play%2520and%2520imaginative%250Aexploration%252C%2520and%2520%25286%2529%2520structure%2520in%2520the%2520form%2520of%2520predictable%2520narratives.%2520These%250Astrategies%252C%2520long%2520refined%2520in%2520animation%252C%2520function%2520as%2520multimodal%2520scaffolds%2520for%250Aattention%252C%2520understanding%252C%2520and%2520attunement%252C%2520supporting%2520learning%2520and%2520comfort.%2520This%250Astructured%2520design%2520grammar%2520is%2520transferable%2520to%2520AI%2520design.%2520By%2520reframing%2520cinematic%250Astorytelling%2520and%2520child%2520development%2520theory%2520as%2520design%2520logic%2520for%2520AI%252C%2520the%2520paper%250Aoffers%2520heuristics%2520for%2520AI%2520that%2520aligns%2520with%2520the%2520cognitive%2520stages%2520and%2520emotional%250Aneeds%2520of%2520young%2520users.%2520The%2520work%2520contributes%2520to%2520design%2520theory%2520by%2520showing%2520how%250Asensory%252C%2520affective%252C%2520and%2520narrative%2520techniques%2520can%2520inform%2520developmentally%2520attuned%250AAI%2520design.%2520Future%2520directions%2520include%2520empirical%2520testing%252C%2520cultural%2520adaptation%252C%250Aand%2520participatory%2520co%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08670v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Once%20Upon%20an%20AI%3A%20Six%20Scaffolds%20for%20Child-AI%20Interaction%20Design%2C%20Inspired%0A%20%20by%20Disney&entry.906535625=Nomisha%20Kurian&entry.1292438233=%20%20To%20build%20AI%20that%20children%20can%20intuitively%20understand%20and%20benefit%20from%2C%0Adesigners%20need%20a%20design%20grammar%20that%20serves%20their%20developmental%20needs.%20This%0Apaper%20bridges%20artificial%20intelligence%20design%20for%20children%20-%20an%20emerging%20field%0Astill%20defining%20its%20best%20practices%20-%20and%20animation%2C%20a%20well%20established%20field%0Awith%20decades%20of%20experience%20in%20engaging%20children%20through%20accessible%0Astorytelling.%20Pairing%20Piagetian%20developmental%20theory%20with%20design%20pattern%0Aextraction%20from%2052%20works%20of%20animation%2C%20the%20paper%20presents%20a%20six%20scaffold%0Aframework%20that%20integrates%20design%20insights%20transferable%20to%20child%20centred%20AI%0Adesign%3A%20%281%29%20signals%20for%20visual%20animacy%20and%20clarity%2C%20%282%29%20sound%20for%20musical%20and%0Aauditory%20scaffolding%2C%20%283%29%20synchrony%20in%20audiovisual%20cues%2C%20%284%29%20sidekick%20style%0Apersonas%2C%20%285%29%20storyplay%20that%20supports%20symbolic%20play%20and%20imaginative%0Aexploration%2C%20and%20%286%29%20structure%20in%20the%20form%20of%20predictable%20narratives.%20These%0Astrategies%2C%20long%20refined%20in%20animation%2C%20function%20as%20multimodal%20scaffolds%20for%0Aattention%2C%20understanding%2C%20and%20attunement%2C%20supporting%20learning%20and%20comfort.%20This%0Astructured%20design%20grammar%20is%20transferable%20to%20AI%20design.%20By%20reframing%20cinematic%0Astorytelling%20and%20child%20development%20theory%20as%20design%20logic%20for%20AI%2C%20the%20paper%0Aoffers%20heuristics%20for%20AI%20that%20aligns%20with%20the%20cognitive%20stages%20and%20emotional%0Aneeds%20of%20young%20users.%20The%20work%20contributes%20to%20design%20theory%20by%20showing%20how%0Asensory%2C%20affective%2C%20and%20narrative%20techniques%20can%20inform%20developmentally%20attuned%0AAI%20design.%20Future%20directions%20include%20empirical%20testing%2C%20cultural%20adaptation%2C%0Aand%20participatory%20co%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08670v3&entry.124074799=Read"},
{"title": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality", "author": "Agnes Axelsson and Merle Reimann and Ronald Cumbal and Hannah Pelikan and Divesh Lala", "abstract": "  Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods.\n", "link": "http://arxiv.org/abs/2508.10603v2", "date": "2025-08-15", "relevancy": 1.9377, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5477}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4827}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Report%20Failed%20Interactions%20With%20Robots%3F%21%20Towards%20Vignette-based%0A%20%20Interaction%20Quality&body=Title%3A%20Why%20Report%20Failed%20Interactions%20With%20Robots%3F%21%20Towards%20Vignette-based%0A%20%20Interaction%20Quality%0AAuthor%3A%20Agnes%20Axelsson%20and%20Merle%20Reimann%20and%20Ronald%20Cumbal%20and%20Hannah%20Pelikan%20and%20Divesh%20Lala%0AAbstract%3A%20%20%20Although%20the%20quality%20of%20human-robot%20interactions%20has%20improved%20with%20the%20advent%0Aof%20LLMs%2C%20there%20are%20still%20various%20factors%20that%20cause%20systems%20to%20be%20sub-optimal%0Awhen%20compared%20to%20human-human%20interactions.%20The%20nature%20and%20criticality%20of%0Afailures%20are%20often%20dependent%20on%20the%20context%20of%20the%20interaction%20and%20so%20cannot%20be%0Ageneralized%20across%20the%20wide%20range%20of%20scenarios%20and%20experiments%20which%20have%20been%0Aimplemented%20in%20HRI%20research.%20In%20this%20work%20we%20propose%20the%20use%20of%20a%20technique%0Aoverlooked%20in%20the%20field%20of%20HRI%2C%20ethnographic%20vignettes%2C%20to%20clearly%20highlight%0Athese%20failures%2C%20particularly%20those%20that%20are%20rarely%20documented.%20We%20describe%20the%0Amethodology%20behind%20the%20process%20of%20writing%20vignettes%20and%20create%20our%20own%20based%20on%0Aour%20personal%20experiences%20with%20failures%20in%20HRI%20systems.%20We%20emphasize%20the%0Astrength%20of%20vignettes%20as%20the%20ability%20to%20communicate%20failures%20from%20a%0Amulti-disciplinary%20perspective%2C%20promote%20transparency%20about%20the%20capabilities%20of%0Arobots%2C%20and%20document%20unexpected%20behaviours%20which%20would%20otherwise%20be%20omitted%0Afrom%20research%20reports.%20We%20encourage%20the%20use%20of%20vignettes%20to%20augment%20existing%0Ainteraction%20evaluation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10603v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Report%2520Failed%2520Interactions%2520With%2520Robots%253F%2521%2520Towards%2520Vignette-based%250A%2520%2520Interaction%2520Quality%26entry.906535625%3DAgnes%2520Axelsson%2520and%2520Merle%2520Reimann%2520and%2520Ronald%2520Cumbal%2520and%2520Hannah%2520Pelikan%2520and%2520Divesh%2520Lala%26entry.1292438233%3D%2520%2520Although%2520the%2520quality%2520of%2520human-robot%2520interactions%2520has%2520improved%2520with%2520the%2520advent%250Aof%2520LLMs%252C%2520there%2520are%2520still%2520various%2520factors%2520that%2520cause%2520systems%2520to%2520be%2520sub-optimal%250Awhen%2520compared%2520to%2520human-human%2520interactions.%2520The%2520nature%2520and%2520criticality%2520of%250Afailures%2520are%2520often%2520dependent%2520on%2520the%2520context%2520of%2520the%2520interaction%2520and%2520so%2520cannot%2520be%250Ageneralized%2520across%2520the%2520wide%2520range%2520of%2520scenarios%2520and%2520experiments%2520which%2520have%2520been%250Aimplemented%2520in%2520HRI%2520research.%2520In%2520this%2520work%2520we%2520propose%2520the%2520use%2520of%2520a%2520technique%250Aoverlooked%2520in%2520the%2520field%2520of%2520HRI%252C%2520ethnographic%2520vignettes%252C%2520to%2520clearly%2520highlight%250Athese%2520failures%252C%2520particularly%2520those%2520that%2520are%2520rarely%2520documented.%2520We%2520describe%2520the%250Amethodology%2520behind%2520the%2520process%2520of%2520writing%2520vignettes%2520and%2520create%2520our%2520own%2520based%2520on%250Aour%2520personal%2520experiences%2520with%2520failures%2520in%2520HRI%2520systems.%2520We%2520emphasize%2520the%250Astrength%2520of%2520vignettes%2520as%2520the%2520ability%2520to%2520communicate%2520failures%2520from%2520a%250Amulti-disciplinary%2520perspective%252C%2520promote%2520transparency%2520about%2520the%2520capabilities%2520of%250Arobots%252C%2520and%2520document%2520unexpected%2520behaviours%2520which%2520would%2520otherwise%2520be%2520omitted%250Afrom%2520research%2520reports.%2520We%2520encourage%2520the%2520use%2520of%2520vignettes%2520to%2520augment%2520existing%250Ainteraction%2520evaluation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10603v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Report%20Failed%20Interactions%20With%20Robots%3F%21%20Towards%20Vignette-based%0A%20%20Interaction%20Quality&entry.906535625=Agnes%20Axelsson%20and%20Merle%20Reimann%20and%20Ronald%20Cumbal%20and%20Hannah%20Pelikan%20and%20Divesh%20Lala&entry.1292438233=%20%20Although%20the%20quality%20of%20human-robot%20interactions%20has%20improved%20with%20the%20advent%0Aof%20LLMs%2C%20there%20are%20still%20various%20factors%20that%20cause%20systems%20to%20be%20sub-optimal%0Awhen%20compared%20to%20human-human%20interactions.%20The%20nature%20and%20criticality%20of%0Afailures%20are%20often%20dependent%20on%20the%20context%20of%20the%20interaction%20and%20so%20cannot%20be%0Ageneralized%20across%20the%20wide%20range%20of%20scenarios%20and%20experiments%20which%20have%20been%0Aimplemented%20in%20HRI%20research.%20In%20this%20work%20we%20propose%20the%20use%20of%20a%20technique%0Aoverlooked%20in%20the%20field%20of%20HRI%2C%20ethnographic%20vignettes%2C%20to%20clearly%20highlight%0Athese%20failures%2C%20particularly%20those%20that%20are%20rarely%20documented.%20We%20describe%20the%0Amethodology%20behind%20the%20process%20of%20writing%20vignettes%20and%20create%20our%20own%20based%20on%0Aour%20personal%20experiences%20with%20failures%20in%20HRI%20systems.%20We%20emphasize%20the%0Astrength%20of%20vignettes%20as%20the%20ability%20to%20communicate%20failures%20from%20a%0Amulti-disciplinary%20perspective%2C%20promote%20transparency%20about%20the%20capabilities%20of%0Arobots%2C%20and%20document%20unexpected%20behaviours%20which%20would%20otherwise%20be%20omitted%0Afrom%20research%20reports.%20We%20encourage%20the%20use%20of%20vignettes%20to%20augment%20existing%0Ainteraction%20evaluation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10603v2&entry.124074799=Read"},
{"title": "CineTrans: Learning to Generate Videos with Cinematic Transitions via\n  Masked Diffusion Models", "author": "Xiaoxue Wu and Bingjie Gao and Yu Qiao and Yaohui Wang and Xinyuan Chen", "abstract": "  Despite significant advances in video synthesis, research into multi-shot\nvideo generation remains in its infancy. Even with scaled-up models and massive\ndatasets, the shot transition capabilities remain rudimentary and unstable,\nlargely confining generated videos to single-shot sequences. In this work, we\nintroduce CineTrans, a novel framework for generating coherent multi-shot\nvideos with cinematic, film-style transitions. To facilitate insights into the\nfilm editing style, we construct a multi-shot video-text dataset Cine250K with\ndetailed shot annotations. Furthermore, our analysis of existing video\ndiffusion models uncovers a correspondence between attention maps in the\ndiffusion model and shot boundaries, which we leverage to design a mask-based\ncontrol mechanism that enables transitions at arbitrary positions and transfers\neffectively in a training-free setting. After fine-tuning on our dataset with\nthe mask mechanism, CineTrans produces cinematic multi-shot sequences while\nadhering to the film editing style, avoiding unstable transitions or naive\nconcatenations. Finally, we propose specialized evaluation metrics for\ntransition control, temporal consistency and overall quality, and demonstrate\nthrough extensive experiments that CineTrans significantly outperforms existing\nbaselines across all criteria.\n", "link": "http://arxiv.org/abs/2508.11484v1", "date": "2025-08-15", "relevancy": 1.9364, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6515}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6444}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CineTrans%3A%20Learning%20to%20Generate%20Videos%20with%20Cinematic%20Transitions%20via%0A%20%20Masked%20Diffusion%20Models&body=Title%3A%20CineTrans%3A%20Learning%20to%20Generate%20Videos%20with%20Cinematic%20Transitions%20via%0A%20%20Masked%20Diffusion%20Models%0AAuthor%3A%20Xiaoxue%20Wu%20and%20Bingjie%20Gao%20and%20Yu%20Qiao%20and%20Yaohui%20Wang%20and%20Xinyuan%20Chen%0AAbstract%3A%20%20%20Despite%20significant%20advances%20in%20video%20synthesis%2C%20research%20into%20multi-shot%0Avideo%20generation%20remains%20in%20its%20infancy.%20Even%20with%20scaled-up%20models%20and%20massive%0Adatasets%2C%20the%20shot%20transition%20capabilities%20remain%20rudimentary%20and%20unstable%2C%0Alargely%20confining%20generated%20videos%20to%20single-shot%20sequences.%20In%20this%20work%2C%20we%0Aintroduce%20CineTrans%2C%20a%20novel%20framework%20for%20generating%20coherent%20multi-shot%0Avideos%20with%20cinematic%2C%20film-style%20transitions.%20To%20facilitate%20insights%20into%20the%0Afilm%20editing%20style%2C%20we%20construct%20a%20multi-shot%20video-text%20dataset%20Cine250K%20with%0Adetailed%20shot%20annotations.%20Furthermore%2C%20our%20analysis%20of%20existing%20video%0Adiffusion%20models%20uncovers%20a%20correspondence%20between%20attention%20maps%20in%20the%0Adiffusion%20model%20and%20shot%20boundaries%2C%20which%20we%20leverage%20to%20design%20a%20mask-based%0Acontrol%20mechanism%20that%20enables%20transitions%20at%20arbitrary%20positions%20and%20transfers%0Aeffectively%20in%20a%20training-free%20setting.%20After%20fine-tuning%20on%20our%20dataset%20with%0Athe%20mask%20mechanism%2C%20CineTrans%20produces%20cinematic%20multi-shot%20sequences%20while%0Aadhering%20to%20the%20film%20editing%20style%2C%20avoiding%20unstable%20transitions%20or%20naive%0Aconcatenations.%20Finally%2C%20we%20propose%20specialized%20evaluation%20metrics%20for%0Atransition%20control%2C%20temporal%20consistency%20and%20overall%20quality%2C%20and%20demonstrate%0Athrough%20extensive%20experiments%20that%20CineTrans%20significantly%20outperforms%20existing%0Abaselines%20across%20all%20criteria.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCineTrans%253A%2520Learning%2520to%2520Generate%2520Videos%2520with%2520Cinematic%2520Transitions%2520via%250A%2520%2520Masked%2520Diffusion%2520Models%26entry.906535625%3DXiaoxue%2520Wu%2520and%2520Bingjie%2520Gao%2520and%2520Yu%2520Qiao%2520and%2520Yaohui%2520Wang%2520and%2520Xinyuan%2520Chen%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advances%2520in%2520video%2520synthesis%252C%2520research%2520into%2520multi-shot%250Avideo%2520generation%2520remains%2520in%2520its%2520infancy.%2520Even%2520with%2520scaled-up%2520models%2520and%2520massive%250Adatasets%252C%2520the%2520shot%2520transition%2520capabilities%2520remain%2520rudimentary%2520and%2520unstable%252C%250Alargely%2520confining%2520generated%2520videos%2520to%2520single-shot%2520sequences.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520CineTrans%252C%2520a%2520novel%2520framework%2520for%2520generating%2520coherent%2520multi-shot%250Avideos%2520with%2520cinematic%252C%2520film-style%2520transitions.%2520To%2520facilitate%2520insights%2520into%2520the%250Afilm%2520editing%2520style%252C%2520we%2520construct%2520a%2520multi-shot%2520video-text%2520dataset%2520Cine250K%2520with%250Adetailed%2520shot%2520annotations.%2520Furthermore%252C%2520our%2520analysis%2520of%2520existing%2520video%250Adiffusion%2520models%2520uncovers%2520a%2520correspondence%2520between%2520attention%2520maps%2520in%2520the%250Adiffusion%2520model%2520and%2520shot%2520boundaries%252C%2520which%2520we%2520leverage%2520to%2520design%2520a%2520mask-based%250Acontrol%2520mechanism%2520that%2520enables%2520transitions%2520at%2520arbitrary%2520positions%2520and%2520transfers%250Aeffectively%2520in%2520a%2520training-free%2520setting.%2520After%2520fine-tuning%2520on%2520our%2520dataset%2520with%250Athe%2520mask%2520mechanism%252C%2520CineTrans%2520produces%2520cinematic%2520multi-shot%2520sequences%2520while%250Aadhering%2520to%2520the%2520film%2520editing%2520style%252C%2520avoiding%2520unstable%2520transitions%2520or%2520naive%250Aconcatenations.%2520Finally%252C%2520we%2520propose%2520specialized%2520evaluation%2520metrics%2520for%250Atransition%2520control%252C%2520temporal%2520consistency%2520and%2520overall%2520quality%252C%2520and%2520demonstrate%250Athrough%2520extensive%2520experiments%2520that%2520CineTrans%2520significantly%2520outperforms%2520existing%250Abaselines%2520across%2520all%2520criteria.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CineTrans%3A%20Learning%20to%20Generate%20Videos%20with%20Cinematic%20Transitions%20via%0A%20%20Masked%20Diffusion%20Models&entry.906535625=Xiaoxue%20Wu%20and%20Bingjie%20Gao%20and%20Yu%20Qiao%20and%20Yaohui%20Wang%20and%20Xinyuan%20Chen&entry.1292438233=%20%20Despite%20significant%20advances%20in%20video%20synthesis%2C%20research%20into%20multi-shot%0Avideo%20generation%20remains%20in%20its%20infancy.%20Even%20with%20scaled-up%20models%20and%20massive%0Adatasets%2C%20the%20shot%20transition%20capabilities%20remain%20rudimentary%20and%20unstable%2C%0Alargely%20confining%20generated%20videos%20to%20single-shot%20sequences.%20In%20this%20work%2C%20we%0Aintroduce%20CineTrans%2C%20a%20novel%20framework%20for%20generating%20coherent%20multi-shot%0Avideos%20with%20cinematic%2C%20film-style%20transitions.%20To%20facilitate%20insights%20into%20the%0Afilm%20editing%20style%2C%20we%20construct%20a%20multi-shot%20video-text%20dataset%20Cine250K%20with%0Adetailed%20shot%20annotations.%20Furthermore%2C%20our%20analysis%20of%20existing%20video%0Adiffusion%20models%20uncovers%20a%20correspondence%20between%20attention%20maps%20in%20the%0Adiffusion%20model%20and%20shot%20boundaries%2C%20which%20we%20leverage%20to%20design%20a%20mask-based%0Acontrol%20mechanism%20that%20enables%20transitions%20at%20arbitrary%20positions%20and%20transfers%0Aeffectively%20in%20a%20training-free%20setting.%20After%20fine-tuning%20on%20our%20dataset%20with%0Athe%20mask%20mechanism%2C%20CineTrans%20produces%20cinematic%20multi-shot%20sequences%20while%0Aadhering%20to%20the%20film%20editing%20style%2C%20avoiding%20unstable%20transitions%20or%20naive%0Aconcatenations.%20Finally%2C%20we%20propose%20specialized%20evaluation%20metrics%20for%0Atransition%20control%2C%20temporal%20consistency%20and%20overall%20quality%2C%20and%20demonstrate%0Athrough%20extensive%20experiments%20that%20CineTrans%20significantly%20outperforms%20existing%0Abaselines%20across%20all%20criteria.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11484v1&entry.124074799=Read"},
{"title": "Learning-based Sketches for Frequency Estimation in Data Streams without\n  Ground Truth", "author": "Xinyu Yuan and Yan Qiao and Meng Li and Zhenchun Wei and Cuiying Feng and Zonghui Wang and Wenzhi Chen", "abstract": "  Estimating the frequency of items on the high-volume, fast data stream has\nbeen extensively studied in many areas, such as database and network\nmeasurement. Traditional sketches provide only coarse estimates under strict\nmemory constraints. Although some learning-augmented methods have emerged\nrecently, they typically rely on offline training with real frequencies or/and\nlabels, which are often unavailable. Moreover, these methods suffer from slow\nupdate speeds, limiting their suitability for real-time processing despite\noffering only marginal accuracy improvements. To overcome these challenges, we\npropose UCL-sketch, a practical learning-based paradigm for per-key frequency\nestimation. Our design introduces two key innovations: (i) an online training\nmechanism based on equivalent learning that requires no ground truth (GT), and\n(ii) a highly scalable architecture leveraging logically structured estimation\nbuckets to scale to real-world data stream. The UCL-sketch, which utilizes\ncompressive sensing (CS), converges to an estimator that provably yields a\nerror bound far lower than that of prior works, without sacrificing the speed\nof processing. Extensive experiments on both real-world and synthetic datasets\ndemonstrate that our approach outperforms previously proposed approaches\nregarding per-key accuracy and distribution. Notably, under extremely tight\nmemory budgets, its quality almost matches that of an (infeasible) omniscient\noracle. Moreover, compared to the existing equation-based sketch, UCL-sketch\nachieves an average decoding speedup of nearly 500 times. To help further\nresearch and development, our code is publicly available at\nhttps://github.com/Y-debug-sys/UCL-sketch.\n", "link": "http://arxiv.org/abs/2412.03611v3", "date": "2025-08-15", "relevancy": 1.9161, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4839}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4788}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning-based%20Sketches%20for%20Frequency%20Estimation%20in%20Data%20Streams%20without%0A%20%20Ground%20Truth&body=Title%3A%20Learning-based%20Sketches%20for%20Frequency%20Estimation%20in%20Data%20Streams%20without%0A%20%20Ground%20Truth%0AAuthor%3A%20Xinyu%20Yuan%20and%20Yan%20Qiao%20and%20Meng%20Li%20and%20Zhenchun%20Wei%20and%20Cuiying%20Feng%20and%20Zonghui%20Wang%20and%20Wenzhi%20Chen%0AAbstract%3A%20%20%20Estimating%20the%20frequency%20of%20items%20on%20the%20high-volume%2C%20fast%20data%20stream%20has%0Abeen%20extensively%20studied%20in%20many%20areas%2C%20such%20as%20database%20and%20network%0Ameasurement.%20Traditional%20sketches%20provide%20only%20coarse%20estimates%20under%20strict%0Amemory%20constraints.%20Although%20some%20learning-augmented%20methods%20have%20emerged%0Arecently%2C%20they%20typically%20rely%20on%20offline%20training%20with%20real%20frequencies%20or/and%0Alabels%2C%20which%20are%20often%20unavailable.%20Moreover%2C%20these%20methods%20suffer%20from%20slow%0Aupdate%20speeds%2C%20limiting%20their%20suitability%20for%20real-time%20processing%20despite%0Aoffering%20only%20marginal%20accuracy%20improvements.%20To%20overcome%20these%20challenges%2C%20we%0Apropose%20UCL-sketch%2C%20a%20practical%20learning-based%20paradigm%20for%20per-key%20frequency%0Aestimation.%20Our%20design%20introduces%20two%20key%20innovations%3A%20%28i%29%20an%20online%20training%0Amechanism%20based%20on%20equivalent%20learning%20that%20requires%20no%20ground%20truth%20%28GT%29%2C%20and%0A%28ii%29%20a%20highly%20scalable%20architecture%20leveraging%20logically%20structured%20estimation%0Abuckets%20to%20scale%20to%20real-world%20data%20stream.%20The%20UCL-sketch%2C%20which%20utilizes%0Acompressive%20sensing%20%28CS%29%2C%20converges%20to%20an%20estimator%20that%20provably%20yields%20a%0Aerror%20bound%20far%20lower%20than%20that%20of%20prior%20works%2C%20without%20sacrificing%20the%20speed%0Aof%20processing.%20Extensive%20experiments%20on%20both%20real-world%20and%20synthetic%20datasets%0Ademonstrate%20that%20our%20approach%20outperforms%20previously%20proposed%20approaches%0Aregarding%20per-key%20accuracy%20and%20distribution.%20Notably%2C%20under%20extremely%20tight%0Amemory%20budgets%2C%20its%20quality%20almost%20matches%20that%20of%20an%20%28infeasible%29%20omniscient%0Aoracle.%20Moreover%2C%20compared%20to%20the%20existing%20equation-based%20sketch%2C%20UCL-sketch%0Aachieves%20an%20average%20decoding%20speedup%20of%20nearly%20500%20times.%20To%20help%20further%0Aresearch%20and%20development%2C%20our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Y-debug-sys/UCL-sketch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03611v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning-based%2520Sketches%2520for%2520Frequency%2520Estimation%2520in%2520Data%2520Streams%2520without%250A%2520%2520Ground%2520Truth%26entry.906535625%3DXinyu%2520Yuan%2520and%2520Yan%2520Qiao%2520and%2520Meng%2520Li%2520and%2520Zhenchun%2520Wei%2520and%2520Cuiying%2520Feng%2520and%2520Zonghui%2520Wang%2520and%2520Wenzhi%2520Chen%26entry.1292438233%3D%2520%2520Estimating%2520the%2520frequency%2520of%2520items%2520on%2520the%2520high-volume%252C%2520fast%2520data%2520stream%2520has%250Abeen%2520extensively%2520studied%2520in%2520many%2520areas%252C%2520such%2520as%2520database%2520and%2520network%250Ameasurement.%2520Traditional%2520sketches%2520provide%2520only%2520coarse%2520estimates%2520under%2520strict%250Amemory%2520constraints.%2520Although%2520some%2520learning-augmented%2520methods%2520have%2520emerged%250Arecently%252C%2520they%2520typically%2520rely%2520on%2520offline%2520training%2520with%2520real%2520frequencies%2520or/and%250Alabels%252C%2520which%2520are%2520often%2520unavailable.%2520Moreover%252C%2520these%2520methods%2520suffer%2520from%2520slow%250Aupdate%2520speeds%252C%2520limiting%2520their%2520suitability%2520for%2520real-time%2520processing%2520despite%250Aoffering%2520only%2520marginal%2520accuracy%2520improvements.%2520To%2520overcome%2520these%2520challenges%252C%2520we%250Apropose%2520UCL-sketch%252C%2520a%2520practical%2520learning-based%2520paradigm%2520for%2520per-key%2520frequency%250Aestimation.%2520Our%2520design%2520introduces%2520two%2520key%2520innovations%253A%2520%2528i%2529%2520an%2520online%2520training%250Amechanism%2520based%2520on%2520equivalent%2520learning%2520that%2520requires%2520no%2520ground%2520truth%2520%2528GT%2529%252C%2520and%250A%2528ii%2529%2520a%2520highly%2520scalable%2520architecture%2520leveraging%2520logically%2520structured%2520estimation%250Abuckets%2520to%2520scale%2520to%2520real-world%2520data%2520stream.%2520The%2520UCL-sketch%252C%2520which%2520utilizes%250Acompressive%2520sensing%2520%2528CS%2529%252C%2520converges%2520to%2520an%2520estimator%2520that%2520provably%2520yields%2520a%250Aerror%2520bound%2520far%2520lower%2520than%2520that%2520of%2520prior%2520works%252C%2520without%2520sacrificing%2520the%2520speed%250Aof%2520processing.%2520Extensive%2520experiments%2520on%2520both%2520real-world%2520and%2520synthetic%2520datasets%250Ademonstrate%2520that%2520our%2520approach%2520outperforms%2520previously%2520proposed%2520approaches%250Aregarding%2520per-key%2520accuracy%2520and%2520distribution.%2520Notably%252C%2520under%2520extremely%2520tight%250Amemory%2520budgets%252C%2520its%2520quality%2520almost%2520matches%2520that%2520of%2520an%2520%2528infeasible%2529%2520omniscient%250Aoracle.%2520Moreover%252C%2520compared%2520to%2520the%2520existing%2520equation-based%2520sketch%252C%2520UCL-sketch%250Aachieves%2520an%2520average%2520decoding%2520speedup%2520of%2520nearly%2520500%2520times.%2520To%2520help%2520further%250Aresearch%2520and%2520development%252C%2520our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Y-debug-sys/UCL-sketch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03611v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-based%20Sketches%20for%20Frequency%20Estimation%20in%20Data%20Streams%20without%0A%20%20Ground%20Truth&entry.906535625=Xinyu%20Yuan%20and%20Yan%20Qiao%20and%20Meng%20Li%20and%20Zhenchun%20Wei%20and%20Cuiying%20Feng%20and%20Zonghui%20Wang%20and%20Wenzhi%20Chen&entry.1292438233=%20%20Estimating%20the%20frequency%20of%20items%20on%20the%20high-volume%2C%20fast%20data%20stream%20has%0Abeen%20extensively%20studied%20in%20many%20areas%2C%20such%20as%20database%20and%20network%0Ameasurement.%20Traditional%20sketches%20provide%20only%20coarse%20estimates%20under%20strict%0Amemory%20constraints.%20Although%20some%20learning-augmented%20methods%20have%20emerged%0Arecently%2C%20they%20typically%20rely%20on%20offline%20training%20with%20real%20frequencies%20or/and%0Alabels%2C%20which%20are%20often%20unavailable.%20Moreover%2C%20these%20methods%20suffer%20from%20slow%0Aupdate%20speeds%2C%20limiting%20their%20suitability%20for%20real-time%20processing%20despite%0Aoffering%20only%20marginal%20accuracy%20improvements.%20To%20overcome%20these%20challenges%2C%20we%0Apropose%20UCL-sketch%2C%20a%20practical%20learning-based%20paradigm%20for%20per-key%20frequency%0Aestimation.%20Our%20design%20introduces%20two%20key%20innovations%3A%20%28i%29%20an%20online%20training%0Amechanism%20based%20on%20equivalent%20learning%20that%20requires%20no%20ground%20truth%20%28GT%29%2C%20and%0A%28ii%29%20a%20highly%20scalable%20architecture%20leveraging%20logically%20structured%20estimation%0Abuckets%20to%20scale%20to%20real-world%20data%20stream.%20The%20UCL-sketch%2C%20which%20utilizes%0Acompressive%20sensing%20%28CS%29%2C%20converges%20to%20an%20estimator%20that%20provably%20yields%20a%0Aerror%20bound%20far%20lower%20than%20that%20of%20prior%20works%2C%20without%20sacrificing%20the%20speed%0Aof%20processing.%20Extensive%20experiments%20on%20both%20real-world%20and%20synthetic%20datasets%0Ademonstrate%20that%20our%20approach%20outperforms%20previously%20proposed%20approaches%0Aregarding%20per-key%20accuracy%20and%20distribution.%20Notably%2C%20under%20extremely%20tight%0Amemory%20budgets%2C%20its%20quality%20almost%20matches%20that%20of%20an%20%28infeasible%29%20omniscient%0Aoracle.%20Moreover%2C%20compared%20to%20the%20existing%20equation-based%20sketch%2C%20UCL-sketch%0Aachieves%20an%20average%20decoding%20speedup%20of%20nearly%20500%20times.%20To%20help%20further%0Aresearch%20and%20development%2C%20our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Y-debug-sys/UCL-sketch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03611v3&entry.124074799=Read"},
{"title": "Pretrained Conformers for Audio Fingerprinting and Retrieval", "author": "Kemal Altwlkany and Elmedin Selmanovic and Sead Delalic", "abstract": "  Conformers have shown great results in speech processing due to their ability\nto capture both local and global interactions. In this work, we utilize a\nself-supervised contrastive learning framework to train conformer-based\nencoders that are capable of generating unique embeddings for small segments of\naudio, generalizing well to previously unseen data. We achieve state-of-the-art\nresults for audio retrieval tasks while using only 3 seconds of audio to\ngenerate embeddings. Our models are almost completely immune to temporal\nmisalignments and achieve state-of-the-art results in cases of other audio\ndistortions such as noise, reverb or extreme temporal stretching. Code and\nmodels are made publicly available and the results are easy to reproduce as we\ntrain and test using popular and freely available datasets of different sizes.\n", "link": "http://arxiv.org/abs/2508.11609v1", "date": "2025-08-15", "relevancy": 1.9156, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4809}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4789}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pretrained%20Conformers%20for%20Audio%20Fingerprinting%20and%20Retrieval&body=Title%3A%20Pretrained%20Conformers%20for%20Audio%20Fingerprinting%20and%20Retrieval%0AAuthor%3A%20Kemal%20Altwlkany%20and%20Elmedin%20Selmanovic%20and%20Sead%20Delalic%0AAbstract%3A%20%20%20Conformers%20have%20shown%20great%20results%20in%20speech%20processing%20due%20to%20their%20ability%0Ato%20capture%20both%20local%20and%20global%20interactions.%20In%20this%20work%2C%20we%20utilize%20a%0Aself-supervised%20contrastive%20learning%20framework%20to%20train%20conformer-based%0Aencoders%20that%20are%20capable%20of%20generating%20unique%20embeddings%20for%20small%20segments%20of%0Aaudio%2C%20generalizing%20well%20to%20previously%20unseen%20data.%20We%20achieve%20state-of-the-art%0Aresults%20for%20audio%20retrieval%20tasks%20while%20using%20only%203%20seconds%20of%20audio%20to%0Agenerate%20embeddings.%20Our%20models%20are%20almost%20completely%20immune%20to%20temporal%0Amisalignments%20and%20achieve%20state-of-the-art%20results%20in%20cases%20of%20other%20audio%0Adistortions%20such%20as%20noise%2C%20reverb%20or%20extreme%20temporal%20stretching.%20Code%20and%0Amodels%20are%20made%20publicly%20available%20and%20the%20results%20are%20easy%20to%20reproduce%20as%20we%0Atrain%20and%20test%20using%20popular%20and%20freely%20available%20datasets%20of%20different%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPretrained%2520Conformers%2520for%2520Audio%2520Fingerprinting%2520and%2520Retrieval%26entry.906535625%3DKemal%2520Altwlkany%2520and%2520Elmedin%2520Selmanovic%2520and%2520Sead%2520Delalic%26entry.1292438233%3D%2520%2520Conformers%2520have%2520shown%2520great%2520results%2520in%2520speech%2520processing%2520due%2520to%2520their%2520ability%250Ato%2520capture%2520both%2520local%2520and%2520global%2520interactions.%2520In%2520this%2520work%252C%2520we%2520utilize%2520a%250Aself-supervised%2520contrastive%2520learning%2520framework%2520to%2520train%2520conformer-based%250Aencoders%2520that%2520are%2520capable%2520of%2520generating%2520unique%2520embeddings%2520for%2520small%2520segments%2520of%250Aaudio%252C%2520generalizing%2520well%2520to%2520previously%2520unseen%2520data.%2520We%2520achieve%2520state-of-the-art%250Aresults%2520for%2520audio%2520retrieval%2520tasks%2520while%2520using%2520only%25203%2520seconds%2520of%2520audio%2520to%250Agenerate%2520embeddings.%2520Our%2520models%2520are%2520almost%2520completely%2520immune%2520to%2520temporal%250Amisalignments%2520and%2520achieve%2520state-of-the-art%2520results%2520in%2520cases%2520of%2520other%2520audio%250Adistortions%2520such%2520as%2520noise%252C%2520reverb%2520or%2520extreme%2520temporal%2520stretching.%2520Code%2520and%250Amodels%2520are%2520made%2520publicly%2520available%2520and%2520the%2520results%2520are%2520easy%2520to%2520reproduce%2520as%2520we%250Atrain%2520and%2520test%2520using%2520popular%2520and%2520freely%2520available%2520datasets%2520of%2520different%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretrained%20Conformers%20for%20Audio%20Fingerprinting%20and%20Retrieval&entry.906535625=Kemal%20Altwlkany%20and%20Elmedin%20Selmanovic%20and%20Sead%20Delalic&entry.1292438233=%20%20Conformers%20have%20shown%20great%20results%20in%20speech%20processing%20due%20to%20their%20ability%0Ato%20capture%20both%20local%20and%20global%20interactions.%20In%20this%20work%2C%20we%20utilize%20a%0Aself-supervised%20contrastive%20learning%20framework%20to%20train%20conformer-based%0Aencoders%20that%20are%20capable%20of%20generating%20unique%20embeddings%20for%20small%20segments%20of%0Aaudio%2C%20generalizing%20well%20to%20previously%20unseen%20data.%20We%20achieve%20state-of-the-art%0Aresults%20for%20audio%20retrieval%20tasks%20while%20using%20only%203%20seconds%20of%20audio%20to%0Agenerate%20embeddings.%20Our%20models%20are%20almost%20completely%20immune%20to%20temporal%0Amisalignments%20and%20achieve%20state-of-the-art%20results%20in%20cases%20of%20other%20audio%0Adistortions%20such%20as%20noise%2C%20reverb%20or%20extreme%20temporal%20stretching.%20Code%20and%0Amodels%20are%20made%20publicly%20available%20and%20the%20results%20are%20easy%20to%20reproduce%20as%20we%0Atrain%20and%20test%20using%20popular%20and%20freely%20available%20datasets%20of%20different%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11609v1&entry.124074799=Read"},
{"title": "Training-free Dimensionality Reduction via Feature Truncation: Enhancing\n  Efficiency in Privacy-preserving Multi-Biometric Systems", "author": "Florian Bayer and Maximilian Russo and Christian Rathgeb", "abstract": "  Biometric recognition is widely used, making the privacy and security of\nextracted templates a critical concern. Biometric Template Protection schemes,\nespecially those utilizing Homomorphic Encryption, introduce significant\ncomputational challenges due to increased workload. Recent advances in deep\nneural networks have enabled state-of-the-art feature extraction for face,\nfingerprint, and iris modalities. The ubiquity and affordability of biometric\nsensors further facilitate multi-modal fusion, which can enhance security by\ncombining features from different modalities. This work investigates the\nbiometric performance of reduced multi-biometric template sizes. Experiments\nare conducted on an in-house virtual multi-biometric database, derived from\nDNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,\nand CASIA databases. The evaluated approaches are (i) explainable and\nstraightforward to implement under encryption, (ii) training-free, and (iii)\ncapable of generalization. Dimensionality reduction of feature vectors leads to\nfewer operations in the Homomorphic Encryption (HE) domain, enabling more\nefficient encrypted processing while maintaining biometric accuracy and\nsecurity at a level equivalent to or exceeding single-biometric recognition.\nOur results demonstrate that, by fusing feature vectors from multiple\nmodalities, template size can be reduced by 67 % with no loss in Equal Error\nRate (EER) compared to the best-performing single modality.\n", "link": "http://arxiv.org/abs/2508.11419v1", "date": "2025-08-15", "relevancy": 1.9078, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4844}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4735}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-free%20Dimensionality%20Reduction%20via%20Feature%20Truncation%3A%20Enhancing%0A%20%20Efficiency%20in%20Privacy-preserving%20Multi-Biometric%20Systems&body=Title%3A%20Training-free%20Dimensionality%20Reduction%20via%20Feature%20Truncation%3A%20Enhancing%0A%20%20Efficiency%20in%20Privacy-preserving%20Multi-Biometric%20Systems%0AAuthor%3A%20Florian%20Bayer%20and%20Maximilian%20Russo%20and%20Christian%20Rathgeb%0AAbstract%3A%20%20%20Biometric%20recognition%20is%20widely%20used%2C%20making%20the%20privacy%20and%20security%20of%0Aextracted%20templates%20a%20critical%20concern.%20Biometric%20Template%20Protection%20schemes%2C%0Aespecially%20those%20utilizing%20Homomorphic%20Encryption%2C%20introduce%20significant%0Acomputational%20challenges%20due%20to%20increased%20workload.%20Recent%20advances%20in%20deep%0Aneural%20networks%20have%20enabled%20state-of-the-art%20feature%20extraction%20for%20face%2C%0Afingerprint%2C%20and%20iris%20modalities.%20The%20ubiquity%20and%20affordability%20of%20biometric%0Asensors%20further%20facilitate%20multi-modal%20fusion%2C%20which%20can%20enhance%20security%20by%0Acombining%20features%20from%20different%20modalities.%20This%20work%20investigates%20the%0Abiometric%20performance%20of%20reduced%20multi-biometric%20template%20sizes.%20Experiments%0Aare%20conducted%20on%20an%20in-house%20virtual%20multi-biometric%20database%2C%20derived%20from%0ADNN-extracted%20features%20for%20face%2C%20fingerprint%2C%20and%20iris%2C%20using%20the%20FRGC%2C%20MCYT%2C%0Aand%20CASIA%20databases.%20The%20evaluated%20approaches%20are%20%28i%29%20explainable%20and%0Astraightforward%20to%20implement%20under%20encryption%2C%20%28ii%29%20training-free%2C%20and%20%28iii%29%0Acapable%20of%20generalization.%20Dimensionality%20reduction%20of%20feature%20vectors%20leads%20to%0Afewer%20operations%20in%20the%20Homomorphic%20Encryption%20%28HE%29%20domain%2C%20enabling%20more%0Aefficient%20encrypted%20processing%20while%20maintaining%20biometric%20accuracy%20and%0Asecurity%20at%20a%20level%20equivalent%20to%20or%20exceeding%20single-biometric%20recognition.%0AOur%20results%20demonstrate%20that%2C%20by%20fusing%20feature%20vectors%20from%20multiple%0Amodalities%2C%20template%20size%20can%20be%20reduced%20by%2067%20%25%20with%20no%20loss%20in%20Equal%20Error%0ARate%20%28EER%29%20compared%20to%20the%20best-performing%20single%20modality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-free%2520Dimensionality%2520Reduction%2520via%2520Feature%2520Truncation%253A%2520Enhancing%250A%2520%2520Efficiency%2520in%2520Privacy-preserving%2520Multi-Biometric%2520Systems%26entry.906535625%3DFlorian%2520Bayer%2520and%2520Maximilian%2520Russo%2520and%2520Christian%2520Rathgeb%26entry.1292438233%3D%2520%2520Biometric%2520recognition%2520is%2520widely%2520used%252C%2520making%2520the%2520privacy%2520and%2520security%2520of%250Aextracted%2520templates%2520a%2520critical%2520concern.%2520Biometric%2520Template%2520Protection%2520schemes%252C%250Aespecially%2520those%2520utilizing%2520Homomorphic%2520Encryption%252C%2520introduce%2520significant%250Acomputational%2520challenges%2520due%2520to%2520increased%2520workload.%2520Recent%2520advances%2520in%2520deep%250Aneural%2520networks%2520have%2520enabled%2520state-of-the-art%2520feature%2520extraction%2520for%2520face%252C%250Afingerprint%252C%2520and%2520iris%2520modalities.%2520The%2520ubiquity%2520and%2520affordability%2520of%2520biometric%250Asensors%2520further%2520facilitate%2520multi-modal%2520fusion%252C%2520which%2520can%2520enhance%2520security%2520by%250Acombining%2520features%2520from%2520different%2520modalities.%2520This%2520work%2520investigates%2520the%250Abiometric%2520performance%2520of%2520reduced%2520multi-biometric%2520template%2520sizes.%2520Experiments%250Aare%2520conducted%2520on%2520an%2520in-house%2520virtual%2520multi-biometric%2520database%252C%2520derived%2520from%250ADNN-extracted%2520features%2520for%2520face%252C%2520fingerprint%252C%2520and%2520iris%252C%2520using%2520the%2520FRGC%252C%2520MCYT%252C%250Aand%2520CASIA%2520databases.%2520The%2520evaluated%2520approaches%2520are%2520%2528i%2529%2520explainable%2520and%250Astraightforward%2520to%2520implement%2520under%2520encryption%252C%2520%2528ii%2529%2520training-free%252C%2520and%2520%2528iii%2529%250Acapable%2520of%2520generalization.%2520Dimensionality%2520reduction%2520of%2520feature%2520vectors%2520leads%2520to%250Afewer%2520operations%2520in%2520the%2520Homomorphic%2520Encryption%2520%2528HE%2529%2520domain%252C%2520enabling%2520more%250Aefficient%2520encrypted%2520processing%2520while%2520maintaining%2520biometric%2520accuracy%2520and%250Asecurity%2520at%2520a%2520level%2520equivalent%2520to%2520or%2520exceeding%2520single-biometric%2520recognition.%250AOur%2520results%2520demonstrate%2520that%252C%2520by%2520fusing%2520feature%2520vectors%2520from%2520multiple%250Amodalities%252C%2520template%2520size%2520can%2520be%2520reduced%2520by%252067%2520%2525%2520with%2520no%2520loss%2520in%2520Equal%2520Error%250ARate%2520%2528EER%2529%2520compared%2520to%2520the%2520best-performing%2520single%2520modality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-free%20Dimensionality%20Reduction%20via%20Feature%20Truncation%3A%20Enhancing%0A%20%20Efficiency%20in%20Privacy-preserving%20Multi-Biometric%20Systems&entry.906535625=Florian%20Bayer%20and%20Maximilian%20Russo%20and%20Christian%20Rathgeb&entry.1292438233=%20%20Biometric%20recognition%20is%20widely%20used%2C%20making%20the%20privacy%20and%20security%20of%0Aextracted%20templates%20a%20critical%20concern.%20Biometric%20Template%20Protection%20schemes%2C%0Aespecially%20those%20utilizing%20Homomorphic%20Encryption%2C%20introduce%20significant%0Acomputational%20challenges%20due%20to%20increased%20workload.%20Recent%20advances%20in%20deep%0Aneural%20networks%20have%20enabled%20state-of-the-art%20feature%20extraction%20for%20face%2C%0Afingerprint%2C%20and%20iris%20modalities.%20The%20ubiquity%20and%20affordability%20of%20biometric%0Asensors%20further%20facilitate%20multi-modal%20fusion%2C%20which%20can%20enhance%20security%20by%0Acombining%20features%20from%20different%20modalities.%20This%20work%20investigates%20the%0Abiometric%20performance%20of%20reduced%20multi-biometric%20template%20sizes.%20Experiments%0Aare%20conducted%20on%20an%20in-house%20virtual%20multi-biometric%20database%2C%20derived%20from%0ADNN-extracted%20features%20for%20face%2C%20fingerprint%2C%20and%20iris%2C%20using%20the%20FRGC%2C%20MCYT%2C%0Aand%20CASIA%20databases.%20The%20evaluated%20approaches%20are%20%28i%29%20explainable%20and%0Astraightforward%20to%20implement%20under%20encryption%2C%20%28ii%29%20training-free%2C%20and%20%28iii%29%0Acapable%20of%20generalization.%20Dimensionality%20reduction%20of%20feature%20vectors%20leads%20to%0Afewer%20operations%20in%20the%20Homomorphic%20Encryption%20%28HE%29%20domain%2C%20enabling%20more%0Aefficient%20encrypted%20processing%20while%20maintaining%20biometric%20accuracy%20and%0Asecurity%20at%20a%20level%20equivalent%20to%20or%20exceeding%20single-biometric%20recognition.%0AOur%20results%20demonstrate%20that%2C%20by%20fusing%20feature%20vectors%20from%20multiple%0Amodalities%2C%20template%20size%20can%20be%20reduced%20by%2067%20%25%20with%20no%20loss%20in%20Equal%20Error%0ARate%20%28EER%29%20compared%20to%20the%20best-performing%20single%20modality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11419v1&entry.124074799=Read"},
{"title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness\n  Methods for LLMs", "author": "Mikhail Seleznyov and Mikhail Chaichuk and Gleb Ershov and Alexander Panchenko and Elena Tutubalina and Oleg Somov", "abstract": "  Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.\n", "link": "http://arxiv.org/abs/2508.11383v1", "date": "2025-08-15", "relevancy": 1.9066, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4827}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Punctuation%20Matters%3A%20A%20Large-Scale%20Comparison%20of%20Prompt%20Robustness%0A%20%20Methods%20for%20LLMs&body=Title%3A%20When%20Punctuation%20Matters%3A%20A%20Large-Scale%20Comparison%20of%20Prompt%20Robustness%0A%20%20Methods%20for%20LLMs%0AAuthor%3A%20Mikhail%20Seleznyov%20and%20Mikhail%20Chaichuk%20and%20Gleb%20Ershov%20and%20Alexander%20Panchenko%20and%20Elena%20Tutubalina%20and%20Oleg%20Somov%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20highly%20sensitive%20to%20subtle%2C%20non-semantic%0Avariations%20in%20prompt%20phrasing%20and%20formatting.%20In%20this%20work%2C%20we%20present%20the%0Afirst%20systematic%20evaluation%20of%205%20methods%20for%20improving%20prompt%20robustness%20within%0Aa%20unified%20experimental%20framework.%20We%20benchmark%20these%20techniques%20on%208%20models%0Afrom%20Llama%2C%20Qwen%20and%20Gemma%20families%20across%2052%20tasks%20from%20Natural%20Instructions%0Adataset.%20Our%20evaluation%20covers%20robustness%20methods%20from%20both%20fine-tuned%20and%0Ain-context%20learning%20paradigms%2C%20and%20tests%20their%20generalization%20against%20multiple%0Atypes%20of%20distribution%20shifts.%20Finally%2C%20we%20extend%20our%20analysis%20to%20GPT-4.1%20and%0ADeepSeek%20V3%20to%20assess%20frontier%20models%27%20current%20robustness%20to%20format%0Aperturbations.%20Our%20findings%20offer%20actionable%20insights%20into%20the%20relative%0Aeffectiveness%20of%20these%20robustness%20methods%2C%20enabling%20practitioners%20to%20make%0Ainformed%20decisions%20when%20aiming%20for%20stable%20and%20reliable%20LLM%20performance%20in%0Areal-world%20applications.%20Code%3A%0Ahttps%3A//github.com/AIRI-Institute/when-punctuation-matters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Punctuation%2520Matters%253A%2520A%2520Large-Scale%2520Comparison%2520of%2520Prompt%2520Robustness%250A%2520%2520Methods%2520for%2520LLMs%26entry.906535625%3DMikhail%2520Seleznyov%2520and%2520Mikhail%2520Chaichuk%2520and%2520Gleb%2520Ershov%2520and%2520Alexander%2520Panchenko%2520and%2520Elena%2520Tutubalina%2520and%2520Oleg%2520Somov%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520highly%2520sensitive%2520to%2520subtle%252C%2520non-semantic%250Avariations%2520in%2520prompt%2520phrasing%2520and%2520formatting.%2520In%2520this%2520work%252C%2520we%2520present%2520the%250Afirst%2520systematic%2520evaluation%2520of%25205%2520methods%2520for%2520improving%2520prompt%2520robustness%2520within%250Aa%2520unified%2520experimental%2520framework.%2520We%2520benchmark%2520these%2520techniques%2520on%25208%2520models%250Afrom%2520Llama%252C%2520Qwen%2520and%2520Gemma%2520families%2520across%252052%2520tasks%2520from%2520Natural%2520Instructions%250Adataset.%2520Our%2520evaluation%2520covers%2520robustness%2520methods%2520from%2520both%2520fine-tuned%2520and%250Ain-context%2520learning%2520paradigms%252C%2520and%2520tests%2520their%2520generalization%2520against%2520multiple%250Atypes%2520of%2520distribution%2520shifts.%2520Finally%252C%2520we%2520extend%2520our%2520analysis%2520to%2520GPT-4.1%2520and%250ADeepSeek%2520V3%2520to%2520assess%2520frontier%2520models%2527%2520current%2520robustness%2520to%2520format%250Aperturbations.%2520Our%2520findings%2520offer%2520actionable%2520insights%2520into%2520the%2520relative%250Aeffectiveness%2520of%2520these%2520robustness%2520methods%252C%2520enabling%2520practitioners%2520to%2520make%250Ainformed%2520decisions%2520when%2520aiming%2520for%2520stable%2520and%2520reliable%2520LLM%2520performance%2520in%250Areal-world%2520applications.%2520Code%253A%250Ahttps%253A//github.com/AIRI-Institute/when-punctuation-matters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Punctuation%20Matters%3A%20A%20Large-Scale%20Comparison%20of%20Prompt%20Robustness%0A%20%20Methods%20for%20LLMs&entry.906535625=Mikhail%20Seleznyov%20and%20Mikhail%20Chaichuk%20and%20Gleb%20Ershov%20and%20Alexander%20Panchenko%20and%20Elena%20Tutubalina%20and%20Oleg%20Somov&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20highly%20sensitive%20to%20subtle%2C%20non-semantic%0Avariations%20in%20prompt%20phrasing%20and%20formatting.%20In%20this%20work%2C%20we%20present%20the%0Afirst%20systematic%20evaluation%20of%205%20methods%20for%20improving%20prompt%20robustness%20within%0Aa%20unified%20experimental%20framework.%20We%20benchmark%20these%20techniques%20on%208%20models%0Afrom%20Llama%2C%20Qwen%20and%20Gemma%20families%20across%2052%20tasks%20from%20Natural%20Instructions%0Adataset.%20Our%20evaluation%20covers%20robustness%20methods%20from%20both%20fine-tuned%20and%0Ain-context%20learning%20paradigms%2C%20and%20tests%20their%20generalization%20against%20multiple%0Atypes%20of%20distribution%20shifts.%20Finally%2C%20we%20extend%20our%20analysis%20to%20GPT-4.1%20and%0ADeepSeek%20V3%20to%20assess%20frontier%20models%27%20current%20robustness%20to%20format%0Aperturbations.%20Our%20findings%20offer%20actionable%20insights%20into%20the%20relative%0Aeffectiveness%20of%20these%20robustness%20methods%2C%20enabling%20practitioners%20to%20make%0Ainformed%20decisions%20when%20aiming%20for%20stable%20and%20reliable%20LLM%20performance%20in%0Areal-world%20applications.%20Code%3A%0Ahttps%3A//github.com/AIRI-Institute/when-punctuation-matters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11383v1&entry.124074799=Read"},
{"title": "Weighted First Order Model Counting for Two-variable Logic with Axioms\n  on Two Relations", "author": "Qipeng Kuang and V\u00e1clav K\u016fla and Ond\u0159ej Ku\u017eelka and Yuanhong Wang and Yuyi Wang", "abstract": "  The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the\nweighted sum of models of a given first-order logic sentence over a given\ndomain. The boundary between fragments for which WFOMC can be computed in\npolynomial time relative to the domain size lies between the two-variable\nfragment ($\\text{FO}^2$) and the three-variable fragment ($\\text{FO}^3$). It is\nknown that WFOMC for \\FOthree{} is $\\mathsf{\\#P_1}$-hard while polynomial-time\nalgorithms exist for computing WFOMC for $\\text{FO}^2$ and $\\text{C}^2$,\npossibly extended by certain axioms such as the linear order axiom, the\nacyclicity axiom, and the connectedness axiom. All existing research has\nconcentrated on extending the fragment with axioms on a single distinguished\nrelation, leaving a gap in understanding the complexity boundary of axioms on\nmultiple relations. In this study, we explore the extension of the two-variable\nfragment by axioms on two relations, presenting both negative and positive\nresults. We show that WFOMC for $\\text{FO}^2$ with two linear order relations\nand $\\text{FO}^2$ with two acyclic relations are $\\mathsf{\\#P_1}$-hard.\nConversely, we provide an algorithm in time polynomial in the domain size for\nWFOMC of $\\text{C}^2$ with a linear order relation, its successor relation and\nanother successor relation.\n", "link": "http://arxiv.org/abs/2508.11515v1", "date": "2025-08-15", "relevancy": 1.9036, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weighted%20First%20Order%20Model%20Counting%20for%20Two-variable%20Logic%20with%20Axioms%0A%20%20on%20Two%20Relations&body=Title%3A%20Weighted%20First%20Order%20Model%20Counting%20for%20Two-variable%20Logic%20with%20Axioms%0A%20%20on%20Two%20Relations%0AAuthor%3A%20Qipeng%20Kuang%20and%20V%C3%A1clav%20K%C5%AFla%20and%20Ond%C5%99ej%20Ku%C5%BEelka%20and%20Yuanhong%20Wang%20and%20Yuyi%20Wang%0AAbstract%3A%20%20%20The%20Weighted%20First-Order%20Model%20Counting%20Problem%20%28WFOMC%29%20asks%20to%20compute%20the%0Aweighted%20sum%20of%20models%20of%20a%20given%20first-order%20logic%20sentence%20over%20a%20given%0Adomain.%20The%20boundary%20between%20fragments%20for%20which%20WFOMC%20can%20be%20computed%20in%0Apolynomial%20time%20relative%20to%20the%20domain%20size%20lies%20between%20the%20two-variable%0Afragment%20%28%24%5Ctext%7BFO%7D%5E2%24%29%20and%20the%20three-variable%20fragment%20%28%24%5Ctext%7BFO%7D%5E3%24%29.%20It%20is%0Aknown%20that%20WFOMC%20for%20%5CFOthree%7B%7D%20is%20%24%5Cmathsf%7B%5C%23P_1%7D%24-hard%20while%20polynomial-time%0Aalgorithms%20exist%20for%20computing%20WFOMC%20for%20%24%5Ctext%7BFO%7D%5E2%24%20and%20%24%5Ctext%7BC%7D%5E2%24%2C%0Apossibly%20extended%20by%20certain%20axioms%20such%20as%20the%20linear%20order%20axiom%2C%20the%0Aacyclicity%20axiom%2C%20and%20the%20connectedness%20axiom.%20All%20existing%20research%20has%0Aconcentrated%20on%20extending%20the%20fragment%20with%20axioms%20on%20a%20single%20distinguished%0Arelation%2C%20leaving%20a%20gap%20in%20understanding%20the%20complexity%20boundary%20of%20axioms%20on%0Amultiple%20relations.%20In%20this%20study%2C%20we%20explore%20the%20extension%20of%20the%20two-variable%0Afragment%20by%20axioms%20on%20two%20relations%2C%20presenting%20both%20negative%20and%20positive%0Aresults.%20We%20show%20that%20WFOMC%20for%20%24%5Ctext%7BFO%7D%5E2%24%20with%20two%20linear%20order%20relations%0Aand%20%24%5Ctext%7BFO%7D%5E2%24%20with%20two%20acyclic%20relations%20are%20%24%5Cmathsf%7B%5C%23P_1%7D%24-hard.%0AConversely%2C%20we%20provide%20an%20algorithm%20in%20time%20polynomial%20in%20the%20domain%20size%20for%0AWFOMC%20of%20%24%5Ctext%7BC%7D%5E2%24%20with%20a%20linear%20order%20relation%2C%20its%20successor%20relation%20and%0Aanother%20successor%20relation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeighted%2520First%2520Order%2520Model%2520Counting%2520for%2520Two-variable%2520Logic%2520with%2520Axioms%250A%2520%2520on%2520Two%2520Relations%26entry.906535625%3DQipeng%2520Kuang%2520and%2520V%25C3%25A1clav%2520K%25C5%25AFla%2520and%2520Ond%25C5%2599ej%2520Ku%25C5%25BEelka%2520and%2520Yuanhong%2520Wang%2520and%2520Yuyi%2520Wang%26entry.1292438233%3D%2520%2520The%2520Weighted%2520First-Order%2520Model%2520Counting%2520Problem%2520%2528WFOMC%2529%2520asks%2520to%2520compute%2520the%250Aweighted%2520sum%2520of%2520models%2520of%2520a%2520given%2520first-order%2520logic%2520sentence%2520over%2520a%2520given%250Adomain.%2520The%2520boundary%2520between%2520fragments%2520for%2520which%2520WFOMC%2520can%2520be%2520computed%2520in%250Apolynomial%2520time%2520relative%2520to%2520the%2520domain%2520size%2520lies%2520between%2520the%2520two-variable%250Afragment%2520%2528%2524%255Ctext%257BFO%257D%255E2%2524%2529%2520and%2520the%2520three-variable%2520fragment%2520%2528%2524%255Ctext%257BFO%257D%255E3%2524%2529.%2520It%2520is%250Aknown%2520that%2520WFOMC%2520for%2520%255CFOthree%257B%257D%2520is%2520%2524%255Cmathsf%257B%255C%2523P_1%257D%2524-hard%2520while%2520polynomial-time%250Aalgorithms%2520exist%2520for%2520computing%2520WFOMC%2520for%2520%2524%255Ctext%257BFO%257D%255E2%2524%2520and%2520%2524%255Ctext%257BC%257D%255E2%2524%252C%250Apossibly%2520extended%2520by%2520certain%2520axioms%2520such%2520as%2520the%2520linear%2520order%2520axiom%252C%2520the%250Aacyclicity%2520axiom%252C%2520and%2520the%2520connectedness%2520axiom.%2520All%2520existing%2520research%2520has%250Aconcentrated%2520on%2520extending%2520the%2520fragment%2520with%2520axioms%2520on%2520a%2520single%2520distinguished%250Arelation%252C%2520leaving%2520a%2520gap%2520in%2520understanding%2520the%2520complexity%2520boundary%2520of%2520axioms%2520on%250Amultiple%2520relations.%2520In%2520this%2520study%252C%2520we%2520explore%2520the%2520extension%2520of%2520the%2520two-variable%250Afragment%2520by%2520axioms%2520on%2520two%2520relations%252C%2520presenting%2520both%2520negative%2520and%2520positive%250Aresults.%2520We%2520show%2520that%2520WFOMC%2520for%2520%2524%255Ctext%257BFO%257D%255E2%2524%2520with%2520two%2520linear%2520order%2520relations%250Aand%2520%2524%255Ctext%257BFO%257D%255E2%2524%2520with%2520two%2520acyclic%2520relations%2520are%2520%2524%255Cmathsf%257B%255C%2523P_1%257D%2524-hard.%250AConversely%252C%2520we%2520provide%2520an%2520algorithm%2520in%2520time%2520polynomial%2520in%2520the%2520domain%2520size%2520for%250AWFOMC%2520of%2520%2524%255Ctext%257BC%257D%255E2%2524%2520with%2520a%2520linear%2520order%2520relation%252C%2520its%2520successor%2520relation%2520and%250Aanother%2520successor%2520relation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weighted%20First%20Order%20Model%20Counting%20for%20Two-variable%20Logic%20with%20Axioms%0A%20%20on%20Two%20Relations&entry.906535625=Qipeng%20Kuang%20and%20V%C3%A1clav%20K%C5%AFla%20and%20Ond%C5%99ej%20Ku%C5%BEelka%20and%20Yuanhong%20Wang%20and%20Yuyi%20Wang&entry.1292438233=%20%20The%20Weighted%20First-Order%20Model%20Counting%20Problem%20%28WFOMC%29%20asks%20to%20compute%20the%0Aweighted%20sum%20of%20models%20of%20a%20given%20first-order%20logic%20sentence%20over%20a%20given%0Adomain.%20The%20boundary%20between%20fragments%20for%20which%20WFOMC%20can%20be%20computed%20in%0Apolynomial%20time%20relative%20to%20the%20domain%20size%20lies%20between%20the%20two-variable%0Afragment%20%28%24%5Ctext%7BFO%7D%5E2%24%29%20and%20the%20three-variable%20fragment%20%28%24%5Ctext%7BFO%7D%5E3%24%29.%20It%20is%0Aknown%20that%20WFOMC%20for%20%5CFOthree%7B%7D%20is%20%24%5Cmathsf%7B%5C%23P_1%7D%24-hard%20while%20polynomial-time%0Aalgorithms%20exist%20for%20computing%20WFOMC%20for%20%24%5Ctext%7BFO%7D%5E2%24%20and%20%24%5Ctext%7BC%7D%5E2%24%2C%0Apossibly%20extended%20by%20certain%20axioms%20such%20as%20the%20linear%20order%20axiom%2C%20the%0Aacyclicity%20axiom%2C%20and%20the%20connectedness%20axiom.%20All%20existing%20research%20has%0Aconcentrated%20on%20extending%20the%20fragment%20with%20axioms%20on%20a%20single%20distinguished%0Arelation%2C%20leaving%20a%20gap%20in%20understanding%20the%20complexity%20boundary%20of%20axioms%20on%0Amultiple%20relations.%20In%20this%20study%2C%20we%20explore%20the%20extension%20of%20the%20two-variable%0Afragment%20by%20axioms%20on%20two%20relations%2C%20presenting%20both%20negative%20and%20positive%0Aresults.%20We%20show%20that%20WFOMC%20for%20%24%5Ctext%7BFO%7D%5E2%24%20with%20two%20linear%20order%20relations%0Aand%20%24%5Ctext%7BFO%7D%5E2%24%20with%20two%20acyclic%20relations%20are%20%24%5Cmathsf%7B%5C%23P_1%7D%24-hard.%0AConversely%2C%20we%20provide%20an%20algorithm%20in%20time%20polynomial%20in%20the%20domain%20size%20for%0AWFOMC%20of%20%24%5Ctext%7BC%7D%5E2%24%20with%20a%20linear%20order%20relation%2C%20its%20successor%20relation%20and%0Aanother%20successor%20relation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11515v1&entry.124074799=Read"},
{"title": "SPG: Style-Prompting Guidance for Style-Specific Content Creation", "author": "Qian Liang and Zichong Chen and Yang Zhou and Hui Huang", "abstract": "  Although recent text-to-image (T2I) diffusion models excel at aligning\ngenerated images with textual prompts, controlling the visual style of the\noutput remains a challenging task. In this work, we propose Style-Prompting\nGuidance (SPG), a novel sampling strategy for style-specific image generation.\nSPG constructs a style noise vector and leverages its directional deviation\nfrom unconditional noise to guide the diffusion process toward the target style\ndistribution. By integrating SPG with Classifier-Free Guidance (CFG), our\nmethod achieves both semantic fidelity and style consistency. SPG is simple,\nrobust, and compatible with controllable frameworks like ControlNet and\nIPAdapter, making it practical and widely applicable. Extensive experiments\ndemonstrate the effectiveness and generality of our approach compared to\nstate-of-the-art methods. Code is available at\nhttps://github.com/Rumbling281441/SPG.\n", "link": "http://arxiv.org/abs/2508.11476v1", "date": "2025-08-15", "relevancy": 1.6818, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5698}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5523}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPG%3A%20Style-Prompting%20Guidance%20for%20Style-Specific%20Content%20Creation&body=Title%3A%20SPG%3A%20Style-Prompting%20Guidance%20for%20Style-Specific%20Content%20Creation%0AAuthor%3A%20Qian%20Liang%20and%20Zichong%20Chen%20and%20Yang%20Zhou%20and%20Hui%20Huang%0AAbstract%3A%20%20%20Although%20recent%20text-to-image%20%28T2I%29%20diffusion%20models%20excel%20at%20aligning%0Agenerated%20images%20with%20textual%20prompts%2C%20controlling%20the%20visual%20style%20of%20the%0Aoutput%20remains%20a%20challenging%20task.%20In%20this%20work%2C%20we%20propose%20Style-Prompting%0AGuidance%20%28SPG%29%2C%20a%20novel%20sampling%20strategy%20for%20style-specific%20image%20generation.%0ASPG%20constructs%20a%20style%20noise%20vector%20and%20leverages%20its%20directional%20deviation%0Afrom%20unconditional%20noise%20to%20guide%20the%20diffusion%20process%20toward%20the%20target%20style%0Adistribution.%20By%20integrating%20SPG%20with%20Classifier-Free%20Guidance%20%28CFG%29%2C%20our%0Amethod%20achieves%20both%20semantic%20fidelity%20and%20style%20consistency.%20SPG%20is%20simple%2C%0Arobust%2C%20and%20compatible%20with%20controllable%20frameworks%20like%20ControlNet%20and%0AIPAdapter%2C%20making%20it%20practical%20and%20widely%20applicable.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20and%20generality%20of%20our%20approach%20compared%20to%0Astate-of-the-art%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Rumbling281441/SPG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPG%253A%2520Style-Prompting%2520Guidance%2520for%2520Style-Specific%2520Content%2520Creation%26entry.906535625%3DQian%2520Liang%2520and%2520Zichong%2520Chen%2520and%2520Yang%2520Zhou%2520and%2520Hui%2520Huang%26entry.1292438233%3D%2520%2520Although%2520recent%2520text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520excel%2520at%2520aligning%250Agenerated%2520images%2520with%2520textual%2520prompts%252C%2520controlling%2520the%2520visual%2520style%2520of%2520the%250Aoutput%2520remains%2520a%2520challenging%2520task.%2520In%2520this%2520work%252C%2520we%2520propose%2520Style-Prompting%250AGuidance%2520%2528SPG%2529%252C%2520a%2520novel%2520sampling%2520strategy%2520for%2520style-specific%2520image%2520generation.%250ASPG%2520constructs%2520a%2520style%2520noise%2520vector%2520and%2520leverages%2520its%2520directional%2520deviation%250Afrom%2520unconditional%2520noise%2520to%2520guide%2520the%2520diffusion%2520process%2520toward%2520the%2520target%2520style%250Adistribution.%2520By%2520integrating%2520SPG%2520with%2520Classifier-Free%2520Guidance%2520%2528CFG%2529%252C%2520our%250Amethod%2520achieves%2520both%2520semantic%2520fidelity%2520and%2520style%2520consistency.%2520SPG%2520is%2520simple%252C%250Arobust%252C%2520and%2520compatible%2520with%2520controllable%2520frameworks%2520like%2520ControlNet%2520and%250AIPAdapter%252C%2520making%2520it%2520practical%2520and%2520widely%2520applicable.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520and%2520generality%2520of%2520our%2520approach%2520compared%2520to%250Astate-of-the-art%2520methods.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Rumbling281441/SPG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPG%3A%20Style-Prompting%20Guidance%20for%20Style-Specific%20Content%20Creation&entry.906535625=Qian%20Liang%20and%20Zichong%20Chen%20and%20Yang%20Zhou%20and%20Hui%20Huang&entry.1292438233=%20%20Although%20recent%20text-to-image%20%28T2I%29%20diffusion%20models%20excel%20at%20aligning%0Agenerated%20images%20with%20textual%20prompts%2C%20controlling%20the%20visual%20style%20of%20the%0Aoutput%20remains%20a%20challenging%20task.%20In%20this%20work%2C%20we%20propose%20Style-Prompting%0AGuidance%20%28SPG%29%2C%20a%20novel%20sampling%20strategy%20for%20style-specific%20image%20generation.%0ASPG%20constructs%20a%20style%20noise%20vector%20and%20leverages%20its%20directional%20deviation%0Afrom%20unconditional%20noise%20to%20guide%20the%20diffusion%20process%20toward%20the%20target%20style%0Adistribution.%20By%20integrating%20SPG%20with%20Classifier-Free%20Guidance%20%28CFG%29%2C%20our%0Amethod%20achieves%20both%20semantic%20fidelity%20and%20style%20consistency.%20SPG%20is%20simple%2C%0Arobust%2C%20and%20compatible%20with%20controllable%20frameworks%20like%20ControlNet%20and%0AIPAdapter%2C%20making%20it%20practical%20and%20widely%20applicable.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20and%20generality%20of%20our%20approach%20compared%20to%0Astate-of-the-art%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Rumbling281441/SPG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11476v1&entry.124074799=Read"},
{"title": "Incorporating Arbitrary Matrix Group Equivariance into KANs", "author": "Lexiang Hu and Yisen Wang and Zhouchen Lin", "abstract": "  Kolmogorov-Arnold Networks (KANs) have seen great success in scientific\ndomains thanks to spline activation functions, becoming an alternative to\nMulti-Layer Perceptrons (MLPs). However, spline functions may not respect\nsymmetry in tasks, which is crucial prior knowledge in machine learning. In\nthis paper, we propose Equivariant Kolmogorov-Arnold Networks (EKAN), a method\nfor incorporating arbitrary matrix group equivariance into KANs, aiming to\nbroaden their applicability to more fields. We first construct gated spline\nbasis functions, which form the EKAN layer together with equivariant linear\nweights, and then define a lift layer to align the input space of EKAN with the\nfeature space of the dataset, thereby building the entire EKAN architecture.\nCompared with baseline models, EKAN achieves higher accuracy with smaller\ndatasets or fewer parameters on symmetry-related tasks, such as particle\nscattering and the three-body problem, often reducing test MSE by several\norders of magnitude. Even in non-symbolic formula scenarios, such as top quark\ntagging with three jet constituents, EKAN achieves comparable results with\nstate-of-the-art equivariant architectures using fewer than 40% of the\nparameters, while KANs do not outperform MLPs as expected. Code and data are\navailable at https://github.com/hulx2002/EKAN .\n", "link": "http://arxiv.org/abs/2410.00435v4", "date": "2025-08-15", "relevancy": 1.3477, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4578}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4479}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Arbitrary%20Matrix%20Group%20Equivariance%20into%20KANs&body=Title%3A%20Incorporating%20Arbitrary%20Matrix%20Group%20Equivariance%20into%20KANs%0AAuthor%3A%20Lexiang%20Hu%20and%20Yisen%20Wang%20and%20Zhouchen%20Lin%0AAbstract%3A%20%20%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20have%20seen%20great%20success%20in%20scientific%0Adomains%20thanks%20to%20spline%20activation%20functions%2C%20becoming%20an%20alternative%20to%0AMulti-Layer%20Perceptrons%20%28MLPs%29.%20However%2C%20spline%20functions%20may%20not%20respect%0Asymmetry%20in%20tasks%2C%20which%20is%20crucial%20prior%20knowledge%20in%20machine%20learning.%20In%0Athis%20paper%2C%20we%20propose%20Equivariant%20Kolmogorov-Arnold%20Networks%20%28EKAN%29%2C%20a%20method%0Afor%20incorporating%20arbitrary%20matrix%20group%20equivariance%20into%20KANs%2C%20aiming%20to%0Abroaden%20their%20applicability%20to%20more%20fields.%20We%20first%20construct%20gated%20spline%0Abasis%20functions%2C%20which%20form%20the%20EKAN%20layer%20together%20with%20equivariant%20linear%0Aweights%2C%20and%20then%20define%20a%20lift%20layer%20to%20align%20the%20input%20space%20of%20EKAN%20with%20the%0Afeature%20space%20of%20the%20dataset%2C%20thereby%20building%20the%20entire%20EKAN%20architecture.%0ACompared%20with%20baseline%20models%2C%20EKAN%20achieves%20higher%20accuracy%20with%20smaller%0Adatasets%20or%20fewer%20parameters%20on%20symmetry-related%20tasks%2C%20such%20as%20particle%0Ascattering%20and%20the%20three-body%20problem%2C%20often%20reducing%20test%20MSE%20by%20several%0Aorders%20of%20magnitude.%20Even%20in%20non-symbolic%20formula%20scenarios%2C%20such%20as%20top%20quark%0Atagging%20with%20three%20jet%20constituents%2C%20EKAN%20achieves%20comparable%20results%20with%0Astate-of-the-art%20equivariant%20architectures%20using%20fewer%20than%2040%25%20of%20the%0Aparameters%2C%20while%20KANs%20do%20not%20outperform%20MLPs%20as%20expected.%20Code%20and%20data%20are%0Aavailable%20at%20https%3A//github.com/hulx2002/EKAN%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00435v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncorporating%2520Arbitrary%2520Matrix%2520Group%2520Equivariance%2520into%2520KANs%26entry.906535625%3DLexiang%2520Hu%2520and%2520Yisen%2520Wang%2520and%2520Zhouchen%2520Lin%26entry.1292438233%3D%2520%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520have%2520seen%2520great%2520success%2520in%2520scientific%250Adomains%2520thanks%2520to%2520spline%2520activation%2520functions%252C%2520becoming%2520an%2520alternative%2520to%250AMulti-Layer%2520Perceptrons%2520%2528MLPs%2529.%2520However%252C%2520spline%2520functions%2520may%2520not%2520respect%250Asymmetry%2520in%2520tasks%252C%2520which%2520is%2520crucial%2520prior%2520knowledge%2520in%2520machine%2520learning.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520Equivariant%2520Kolmogorov-Arnold%2520Networks%2520%2528EKAN%2529%252C%2520a%2520method%250Afor%2520incorporating%2520arbitrary%2520matrix%2520group%2520equivariance%2520into%2520KANs%252C%2520aiming%2520to%250Abroaden%2520their%2520applicability%2520to%2520more%2520fields.%2520We%2520first%2520construct%2520gated%2520spline%250Abasis%2520functions%252C%2520which%2520form%2520the%2520EKAN%2520layer%2520together%2520with%2520equivariant%2520linear%250Aweights%252C%2520and%2520then%2520define%2520a%2520lift%2520layer%2520to%2520align%2520the%2520input%2520space%2520of%2520EKAN%2520with%2520the%250Afeature%2520space%2520of%2520the%2520dataset%252C%2520thereby%2520building%2520the%2520entire%2520EKAN%2520architecture.%250ACompared%2520with%2520baseline%2520models%252C%2520EKAN%2520achieves%2520higher%2520accuracy%2520with%2520smaller%250Adatasets%2520or%2520fewer%2520parameters%2520on%2520symmetry-related%2520tasks%252C%2520such%2520as%2520particle%250Ascattering%2520and%2520the%2520three-body%2520problem%252C%2520often%2520reducing%2520test%2520MSE%2520by%2520several%250Aorders%2520of%2520magnitude.%2520Even%2520in%2520non-symbolic%2520formula%2520scenarios%252C%2520such%2520as%2520top%2520quark%250Atagging%2520with%2520three%2520jet%2520constituents%252C%2520EKAN%2520achieves%2520comparable%2520results%2520with%250Astate-of-the-art%2520equivariant%2520architectures%2520using%2520fewer%2520than%252040%2525%2520of%2520the%250Aparameters%252C%2520while%2520KANs%2520do%2520not%2520outperform%2520MLPs%2520as%2520expected.%2520Code%2520and%2520data%2520are%250Aavailable%2520at%2520https%253A//github.com/hulx2002/EKAN%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00435v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Arbitrary%20Matrix%20Group%20Equivariance%20into%20KANs&entry.906535625=Lexiang%20Hu%20and%20Yisen%20Wang%20and%20Zhouchen%20Lin&entry.1292438233=%20%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20have%20seen%20great%20success%20in%20scientific%0Adomains%20thanks%20to%20spline%20activation%20functions%2C%20becoming%20an%20alternative%20to%0AMulti-Layer%20Perceptrons%20%28MLPs%29.%20However%2C%20spline%20functions%20may%20not%20respect%0Asymmetry%20in%20tasks%2C%20which%20is%20crucial%20prior%20knowledge%20in%20machine%20learning.%20In%0Athis%20paper%2C%20we%20propose%20Equivariant%20Kolmogorov-Arnold%20Networks%20%28EKAN%29%2C%20a%20method%0Afor%20incorporating%20arbitrary%20matrix%20group%20equivariance%20into%20KANs%2C%20aiming%20to%0Abroaden%20their%20applicability%20to%20more%20fields.%20We%20first%20construct%20gated%20spline%0Abasis%20functions%2C%20which%20form%20the%20EKAN%20layer%20together%20with%20equivariant%20linear%0Aweights%2C%20and%20then%20define%20a%20lift%20layer%20to%20align%20the%20input%20space%20of%20EKAN%20with%20the%0Afeature%20space%20of%20the%20dataset%2C%20thereby%20building%20the%20entire%20EKAN%20architecture.%0ACompared%20with%20baseline%20models%2C%20EKAN%20achieves%20higher%20accuracy%20with%20smaller%0Adatasets%20or%20fewer%20parameters%20on%20symmetry-related%20tasks%2C%20such%20as%20particle%0Ascattering%20and%20the%20three-body%20problem%2C%20often%20reducing%20test%20MSE%20by%20several%0Aorders%20of%20magnitude.%20Even%20in%20non-symbolic%20formula%20scenarios%2C%20such%20as%20top%20quark%0Atagging%20with%20three%20jet%20constituents%2C%20EKAN%20achieves%20comparable%20results%20with%0Astate-of-the-art%20equivariant%20architectures%20using%20fewer%20than%2040%25%20of%20the%0Aparameters%2C%20while%20KANs%20do%20not%20outperform%20MLPs%20as%20expected.%20Code%20and%20data%20are%0Aavailable%20at%20https%3A//github.com/hulx2002/EKAN%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00435v4&entry.124074799=Read"},
{"title": "ReachVox: Clutter-free Reachability Visualization for Robot Motion\n  Planning in Virtual Reality", "author": "Steffen Hauck and Diar Abdlkarim and John Dudley and Per Ola Kristensson and Eyal Ofek and Jens Grubert", "abstract": "  Human-Robot-Collaboration can enhance workflows by leveraging the mutual\nstrengths of human operators and robots. Planning and understanding robot\nmovements remain major challenges in this domain. This problem is prevalent in\ndynamic environments that might need constant robot motion path adaptation. In\nthis paper, we investigate whether a minimalistic encoding of the reachability\nof a point near an object of interest, which we call ReachVox, can aid the\ncollaboration between a remote operator and a robotic arm in VR. Through a user\nstudy (n=20), we indicate the strength of the visualization relative to a\npoint-based reachability check-up.\n", "link": "http://arxiv.org/abs/2508.11426v1", "date": "2025-08-15", "relevancy": 1.6367, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5616}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReachVox%3A%20Clutter-free%20Reachability%20Visualization%20for%20Robot%20Motion%0A%20%20Planning%20in%20Virtual%20Reality&body=Title%3A%20ReachVox%3A%20Clutter-free%20Reachability%20Visualization%20for%20Robot%20Motion%0A%20%20Planning%20in%20Virtual%20Reality%0AAuthor%3A%20Steffen%20Hauck%20and%20Diar%20Abdlkarim%20and%20John%20Dudley%20and%20Per%20Ola%20Kristensson%20and%20Eyal%20Ofek%20and%20Jens%20Grubert%0AAbstract%3A%20%20%20Human-Robot-Collaboration%20can%20enhance%20workflows%20by%20leveraging%20the%20mutual%0Astrengths%20of%20human%20operators%20and%20robots.%20Planning%20and%20understanding%20robot%0Amovements%20remain%20major%20challenges%20in%20this%20domain.%20This%20problem%20is%20prevalent%20in%0Adynamic%20environments%20that%20might%20need%20constant%20robot%20motion%20path%20adaptation.%20In%0Athis%20paper%2C%20we%20investigate%20whether%20a%20minimalistic%20encoding%20of%20the%20reachability%0Aof%20a%20point%20near%20an%20object%20of%20interest%2C%20which%20we%20call%20ReachVox%2C%20can%20aid%20the%0Acollaboration%20between%20a%20remote%20operator%20and%20a%20robotic%20arm%20in%20VR.%20Through%20a%20user%0Astudy%20%28n%3D20%29%2C%20we%20indicate%20the%20strength%20of%20the%20visualization%20relative%20to%20a%0Apoint-based%20reachability%20check-up.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReachVox%253A%2520Clutter-free%2520Reachability%2520Visualization%2520for%2520Robot%2520Motion%250A%2520%2520Planning%2520in%2520Virtual%2520Reality%26entry.906535625%3DSteffen%2520Hauck%2520and%2520Diar%2520Abdlkarim%2520and%2520John%2520Dudley%2520and%2520Per%2520Ola%2520Kristensson%2520and%2520Eyal%2520Ofek%2520and%2520Jens%2520Grubert%26entry.1292438233%3D%2520%2520Human-Robot-Collaboration%2520can%2520enhance%2520workflows%2520by%2520leveraging%2520the%2520mutual%250Astrengths%2520of%2520human%2520operators%2520and%2520robots.%2520Planning%2520and%2520understanding%2520robot%250Amovements%2520remain%2520major%2520challenges%2520in%2520this%2520domain.%2520This%2520problem%2520is%2520prevalent%2520in%250Adynamic%2520environments%2520that%2520might%2520need%2520constant%2520robot%2520motion%2520path%2520adaptation.%2520In%250Athis%2520paper%252C%2520we%2520investigate%2520whether%2520a%2520minimalistic%2520encoding%2520of%2520the%2520reachability%250Aof%2520a%2520point%2520near%2520an%2520object%2520of%2520interest%252C%2520which%2520we%2520call%2520ReachVox%252C%2520can%2520aid%2520the%250Acollaboration%2520between%2520a%2520remote%2520operator%2520and%2520a%2520robotic%2520arm%2520in%2520VR.%2520Through%2520a%2520user%250Astudy%2520%2528n%253D20%2529%252C%2520we%2520indicate%2520the%2520strength%2520of%2520the%2520visualization%2520relative%2520to%2520a%250Apoint-based%2520reachability%2520check-up.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReachVox%3A%20Clutter-free%20Reachability%20Visualization%20for%20Robot%20Motion%0A%20%20Planning%20in%20Virtual%20Reality&entry.906535625=Steffen%20Hauck%20and%20Diar%20Abdlkarim%20and%20John%20Dudley%20and%20Per%20Ola%20Kristensson%20and%20Eyal%20Ofek%20and%20Jens%20Grubert&entry.1292438233=%20%20Human-Robot-Collaboration%20can%20enhance%20workflows%20by%20leveraging%20the%20mutual%0Astrengths%20of%20human%20operators%20and%20robots.%20Planning%20and%20understanding%20robot%0Amovements%20remain%20major%20challenges%20in%20this%20domain.%20This%20problem%20is%20prevalent%20in%0Adynamic%20environments%20that%20might%20need%20constant%20robot%20motion%20path%20adaptation.%20In%0Athis%20paper%2C%20we%20investigate%20whether%20a%20minimalistic%20encoding%20of%20the%20reachability%0Aof%20a%20point%20near%20an%20object%20of%20interest%2C%20which%20we%20call%20ReachVox%2C%20can%20aid%20the%0Acollaboration%20between%20a%20remote%20operator%20and%20a%20robotic%20arm%20in%20VR.%20Through%20a%20user%0Astudy%20%28n%3D20%29%2C%20we%20indicate%20the%20strength%20of%20the%20visualization%20relative%20to%20a%0Apoint-based%20reachability%20check-up.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11426v1&entry.124074799=Read"},
{"title": "Human-AI Experience in Integrated Development Environments: A Systematic\n  Literature Review", "author": "Agnia Sergeyuk and Ilya Zakharov and Ekaterina Koshchenko and Maliheh Izadi", "abstract": "  The integration of Artificial Intelligence (AI) into Integrated Development\nEnvironments (IDEs) is reshaping software development, fundamentally altering\nhow developers interact with their tools. This shift marks the emergence of\nHuman-AI Experience in Integrated Development Environment (in-IDE HAX), a field\nthat explores the evolving dynamics of Human-Computer Interaction in\nAI-assisted coding environments. Despite rapid adoption, research on in-IDE HAX\nremains fragmented, which highlights the need for a unified overview of current\npractices, challenges, and opportunities. To provide a structured overview of\nexisting research, we conduct a systematic literature review of 90 studies,\nsummarizing current findings and outlining areas for further investigation.\n  We organize key insights from reviewed studies into three aspects: Impact,\nDesign, and Quality of AI-based systems inside IDEs. Impact findings show that\nAI-assisted coding enhances developer productivity but also introduces\nchallenges, such as verification overhead and over-reliance. Design studies\nshow that effective interfaces surface context, provide explanations and\ntransparency of suggestion, and support user control. Quality studies document\nrisks in correctness, maintainability, and security. For future research,\npriorities include productivity studies, design of assistance, and audit of\nAI-generated code. The agenda calls for larger and longer evaluations, stronger\naudit and verification assets, broader coverage across the software life cycle,\nand adaptive assistance under user control.\n", "link": "http://arxiv.org/abs/2503.06195v2", "date": "2025-08-15", "relevancy": 1.8157, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-AI%20Experience%20in%20Integrated%20Development%20Environments%3A%20A%20Systematic%0A%20%20Literature%20Review&body=Title%3A%20Human-AI%20Experience%20in%20Integrated%20Development%20Environments%3A%20A%20Systematic%0A%20%20Literature%20Review%0AAuthor%3A%20Agnia%20Sergeyuk%20and%20Ilya%20Zakharov%20and%20Ekaterina%20Koshchenko%20and%20Maliheh%20Izadi%0AAbstract%3A%20%20%20The%20integration%20of%20Artificial%20Intelligence%20%28AI%29%20into%20Integrated%20Development%0AEnvironments%20%28IDEs%29%20is%20reshaping%20software%20development%2C%20fundamentally%20altering%0Ahow%20developers%20interact%20with%20their%20tools.%20This%20shift%20marks%20the%20emergence%20of%0AHuman-AI%20Experience%20in%20Integrated%20Development%20Environment%20%28in-IDE%20HAX%29%2C%20a%20field%0Athat%20explores%20the%20evolving%20dynamics%20of%20Human-Computer%20Interaction%20in%0AAI-assisted%20coding%20environments.%20Despite%20rapid%20adoption%2C%20research%20on%20in-IDE%20HAX%0Aremains%20fragmented%2C%20which%20highlights%20the%20need%20for%20a%20unified%20overview%20of%20current%0Apractices%2C%20challenges%2C%20and%20opportunities.%20To%20provide%20a%20structured%20overview%20of%0Aexisting%20research%2C%20we%20conduct%20a%20systematic%20literature%20review%20of%2090%20studies%2C%0Asummarizing%20current%20findings%20and%20outlining%20areas%20for%20further%20investigation.%0A%20%20We%20organize%20key%20insights%20from%20reviewed%20studies%20into%20three%20aspects%3A%20Impact%2C%0ADesign%2C%20and%20Quality%20of%20AI-based%20systems%20inside%20IDEs.%20Impact%20findings%20show%20that%0AAI-assisted%20coding%20enhances%20developer%20productivity%20but%20also%20introduces%0Achallenges%2C%20such%20as%20verification%20overhead%20and%20over-reliance.%20Design%20studies%0Ashow%20that%20effective%20interfaces%20surface%20context%2C%20provide%20explanations%20and%0Atransparency%20of%20suggestion%2C%20and%20support%20user%20control.%20Quality%20studies%20document%0Arisks%20in%20correctness%2C%20maintainability%2C%20and%20security.%20For%20future%20research%2C%0Apriorities%20include%20productivity%20studies%2C%20design%20of%20assistance%2C%20and%20audit%20of%0AAI-generated%20code.%20The%20agenda%20calls%20for%20larger%20and%20longer%20evaluations%2C%20stronger%0Aaudit%20and%20verification%20assets%2C%20broader%20coverage%20across%20the%20software%20life%20cycle%2C%0Aand%20adaptive%20assistance%20under%20user%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06195v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-AI%2520Experience%2520in%2520Integrated%2520Development%2520Environments%253A%2520A%2520Systematic%250A%2520%2520Literature%2520Review%26entry.906535625%3DAgnia%2520Sergeyuk%2520and%2520Ilya%2520Zakharov%2520and%2520Ekaterina%2520Koshchenko%2520and%2520Maliheh%2520Izadi%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520into%2520Integrated%2520Development%250AEnvironments%2520%2528IDEs%2529%2520is%2520reshaping%2520software%2520development%252C%2520fundamentally%2520altering%250Ahow%2520developers%2520interact%2520with%2520their%2520tools.%2520This%2520shift%2520marks%2520the%2520emergence%2520of%250AHuman-AI%2520Experience%2520in%2520Integrated%2520Development%2520Environment%2520%2528in-IDE%2520HAX%2529%252C%2520a%2520field%250Athat%2520explores%2520the%2520evolving%2520dynamics%2520of%2520Human-Computer%2520Interaction%2520in%250AAI-assisted%2520coding%2520environments.%2520Despite%2520rapid%2520adoption%252C%2520research%2520on%2520in-IDE%2520HAX%250Aremains%2520fragmented%252C%2520which%2520highlights%2520the%2520need%2520for%2520a%2520unified%2520overview%2520of%2520current%250Apractices%252C%2520challenges%252C%2520and%2520opportunities.%2520To%2520provide%2520a%2520structured%2520overview%2520of%250Aexisting%2520research%252C%2520we%2520conduct%2520a%2520systematic%2520literature%2520review%2520of%252090%2520studies%252C%250Asummarizing%2520current%2520findings%2520and%2520outlining%2520areas%2520for%2520further%2520investigation.%250A%2520%2520We%2520organize%2520key%2520insights%2520from%2520reviewed%2520studies%2520into%2520three%2520aspects%253A%2520Impact%252C%250ADesign%252C%2520and%2520Quality%2520of%2520AI-based%2520systems%2520inside%2520IDEs.%2520Impact%2520findings%2520show%2520that%250AAI-assisted%2520coding%2520enhances%2520developer%2520productivity%2520but%2520also%2520introduces%250Achallenges%252C%2520such%2520as%2520verification%2520overhead%2520and%2520over-reliance.%2520Design%2520studies%250Ashow%2520that%2520effective%2520interfaces%2520surface%2520context%252C%2520provide%2520explanations%2520and%250Atransparency%2520of%2520suggestion%252C%2520and%2520support%2520user%2520control.%2520Quality%2520studies%2520document%250Arisks%2520in%2520correctness%252C%2520maintainability%252C%2520and%2520security.%2520For%2520future%2520research%252C%250Apriorities%2520include%2520productivity%2520studies%252C%2520design%2520of%2520assistance%252C%2520and%2520audit%2520of%250AAI-generated%2520code.%2520The%2520agenda%2520calls%2520for%2520larger%2520and%2520longer%2520evaluations%252C%2520stronger%250Aaudit%2520and%2520verification%2520assets%252C%2520broader%2520coverage%2520across%2520the%2520software%2520life%2520cycle%252C%250Aand%2520adaptive%2520assistance%2520under%2520user%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06195v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-AI%20Experience%20in%20Integrated%20Development%20Environments%3A%20A%20Systematic%0A%20%20Literature%20Review&entry.906535625=Agnia%20Sergeyuk%20and%20Ilya%20Zakharov%20and%20Ekaterina%20Koshchenko%20and%20Maliheh%20Izadi&entry.1292438233=%20%20The%20integration%20of%20Artificial%20Intelligence%20%28AI%29%20into%20Integrated%20Development%0AEnvironments%20%28IDEs%29%20is%20reshaping%20software%20development%2C%20fundamentally%20altering%0Ahow%20developers%20interact%20with%20their%20tools.%20This%20shift%20marks%20the%20emergence%20of%0AHuman-AI%20Experience%20in%20Integrated%20Development%20Environment%20%28in-IDE%20HAX%29%2C%20a%20field%0Athat%20explores%20the%20evolving%20dynamics%20of%20Human-Computer%20Interaction%20in%0AAI-assisted%20coding%20environments.%20Despite%20rapid%20adoption%2C%20research%20on%20in-IDE%20HAX%0Aremains%20fragmented%2C%20which%20highlights%20the%20need%20for%20a%20unified%20overview%20of%20current%0Apractices%2C%20challenges%2C%20and%20opportunities.%20To%20provide%20a%20structured%20overview%20of%0Aexisting%20research%2C%20we%20conduct%20a%20systematic%20literature%20review%20of%2090%20studies%2C%0Asummarizing%20current%20findings%20and%20outlining%20areas%20for%20further%20investigation.%0A%20%20We%20organize%20key%20insights%20from%20reviewed%20studies%20into%20three%20aspects%3A%20Impact%2C%0ADesign%2C%20and%20Quality%20of%20AI-based%20systems%20inside%20IDEs.%20Impact%20findings%20show%20that%0AAI-assisted%20coding%20enhances%20developer%20productivity%20but%20also%20introduces%0Achallenges%2C%20such%20as%20verification%20overhead%20and%20over-reliance.%20Design%20studies%0Ashow%20that%20effective%20interfaces%20surface%20context%2C%20provide%20explanations%20and%0Atransparency%20of%20suggestion%2C%20and%20support%20user%20control.%20Quality%20studies%20document%0Arisks%20in%20correctness%2C%20maintainability%2C%20and%20security.%20For%20future%20research%2C%0Apriorities%20include%20productivity%20studies%2C%20design%20of%20assistance%2C%20and%20audit%20of%0AAI-generated%20code.%20The%20agenda%20calls%20for%20larger%20and%20longer%20evaluations%2C%20stronger%0Aaudit%20and%20verification%20assets%2C%20broader%20coverage%20across%20the%20software%20life%20cycle%2C%0Aand%20adaptive%20assistance%20under%20user%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06195v2&entry.124074799=Read"},
{"title": "DiCriTest: Testing Scenario Generation for Decision-Making Agents\n  Considering Diversity and Criticality", "author": "Qitong Chu and Yufeng Yue and Danya Yao and Huaxin Pei", "abstract": "  The growing deployment of decision-making agents in dynamic environments\nincreases the demand for safety verification. While critical testing scenario\ngeneration has emerged as an appealing verification methodology, effectively\nbalancing diversity and criticality remains a key challenge for existing\nmethods, particularly due to local optima entrapment in high-dimensional\nscenario spaces. To address this limitation, we propose a dual-space guided\ntesting framework that coordinates scenario parameter space and agent behavior\nspace, aiming to generate testing scenarios considering diversity and\ncriticality. Specifically, in the scenario parameter space, a hierarchical\nrepresentation framework combines dimensionality reduction and\nmulti-dimensional subspace evaluation to efficiently localize diverse and\ncritical subspaces. This guides dynamic coordination between two generation\nmodes: local perturbation and global exploration, optimizing critical scenario\nquantity and diversity. Complementarily, in the agent behavior space,\nagent-environment interaction data are leveraged to quantify behavioral\ncriticality/diversity and adaptively support generation mode switching, forming\na closed feedback loop that continuously enhances scenario characterization and\nexploration within the parameter space. Experiments show our framework improves\ncritical scenario generation by an average of 56.23\\% and demonstrates greater\ndiversity under novel parameter-behavior co-driven metrics when tested on five\ndecision-making agents, outperforming state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2508.11514v1", "date": "2025-08-15", "relevancy": 1.4497, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5246}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4716}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiCriTest%3A%20Testing%20Scenario%20Generation%20for%20Decision-Making%20Agents%0A%20%20Considering%20Diversity%20and%20Criticality&body=Title%3A%20DiCriTest%3A%20Testing%20Scenario%20Generation%20for%20Decision-Making%20Agents%0A%20%20Considering%20Diversity%20and%20Criticality%0AAuthor%3A%20Qitong%20Chu%20and%20Yufeng%20Yue%20and%20Danya%20Yao%20and%20Huaxin%20Pei%0AAbstract%3A%20%20%20The%20growing%20deployment%20of%20decision-making%20agents%20in%20dynamic%20environments%0Aincreases%20the%20demand%20for%20safety%20verification.%20While%20critical%20testing%20scenario%0Ageneration%20has%20emerged%20as%20an%20appealing%20verification%20methodology%2C%20effectively%0Abalancing%20diversity%20and%20criticality%20remains%20a%20key%20challenge%20for%20existing%0Amethods%2C%20particularly%20due%20to%20local%20optima%20entrapment%20in%20high-dimensional%0Ascenario%20spaces.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20dual-space%20guided%0Atesting%20framework%20that%20coordinates%20scenario%20parameter%20space%20and%20agent%20behavior%0Aspace%2C%20aiming%20to%20generate%20testing%20scenarios%20considering%20diversity%20and%0Acriticality.%20Specifically%2C%20in%20the%20scenario%20parameter%20space%2C%20a%20hierarchical%0Arepresentation%20framework%20combines%20dimensionality%20reduction%20and%0Amulti-dimensional%20subspace%20evaluation%20to%20efficiently%20localize%20diverse%20and%0Acritical%20subspaces.%20This%20guides%20dynamic%20coordination%20between%20two%20generation%0Amodes%3A%20local%20perturbation%20and%20global%20exploration%2C%20optimizing%20critical%20scenario%0Aquantity%20and%20diversity.%20Complementarily%2C%20in%20the%20agent%20behavior%20space%2C%0Aagent-environment%20interaction%20data%20are%20leveraged%20to%20quantify%20behavioral%0Acriticality/diversity%20and%20adaptively%20support%20generation%20mode%20switching%2C%20forming%0Aa%20closed%20feedback%20loop%20that%20continuously%20enhances%20scenario%20characterization%20and%0Aexploration%20within%20the%20parameter%20space.%20Experiments%20show%20our%20framework%20improves%0Acritical%20scenario%20generation%20by%20an%20average%20of%2056.23%5C%25%20and%20demonstrates%20greater%0Adiversity%20under%20novel%20parameter-behavior%20co-driven%20metrics%20when%20tested%20on%20five%0Adecision-making%20agents%2C%20outperforming%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiCriTest%253A%2520Testing%2520Scenario%2520Generation%2520for%2520Decision-Making%2520Agents%250A%2520%2520Considering%2520Diversity%2520and%2520Criticality%26entry.906535625%3DQitong%2520Chu%2520and%2520Yufeng%2520Yue%2520and%2520Danya%2520Yao%2520and%2520Huaxin%2520Pei%26entry.1292438233%3D%2520%2520The%2520growing%2520deployment%2520of%2520decision-making%2520agents%2520in%2520dynamic%2520environments%250Aincreases%2520the%2520demand%2520for%2520safety%2520verification.%2520While%2520critical%2520testing%2520scenario%250Ageneration%2520has%2520emerged%2520as%2520an%2520appealing%2520verification%2520methodology%252C%2520effectively%250Abalancing%2520diversity%2520and%2520criticality%2520remains%2520a%2520key%2520challenge%2520for%2520existing%250Amethods%252C%2520particularly%2520due%2520to%2520local%2520optima%2520entrapment%2520in%2520high-dimensional%250Ascenario%2520spaces.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520dual-space%2520guided%250Atesting%2520framework%2520that%2520coordinates%2520scenario%2520parameter%2520space%2520and%2520agent%2520behavior%250Aspace%252C%2520aiming%2520to%2520generate%2520testing%2520scenarios%2520considering%2520diversity%2520and%250Acriticality.%2520Specifically%252C%2520in%2520the%2520scenario%2520parameter%2520space%252C%2520a%2520hierarchical%250Arepresentation%2520framework%2520combines%2520dimensionality%2520reduction%2520and%250Amulti-dimensional%2520subspace%2520evaluation%2520to%2520efficiently%2520localize%2520diverse%2520and%250Acritical%2520subspaces.%2520This%2520guides%2520dynamic%2520coordination%2520between%2520two%2520generation%250Amodes%253A%2520local%2520perturbation%2520and%2520global%2520exploration%252C%2520optimizing%2520critical%2520scenario%250Aquantity%2520and%2520diversity.%2520Complementarily%252C%2520in%2520the%2520agent%2520behavior%2520space%252C%250Aagent-environment%2520interaction%2520data%2520are%2520leveraged%2520to%2520quantify%2520behavioral%250Acriticality/diversity%2520and%2520adaptively%2520support%2520generation%2520mode%2520switching%252C%2520forming%250Aa%2520closed%2520feedback%2520loop%2520that%2520continuously%2520enhances%2520scenario%2520characterization%2520and%250Aexploration%2520within%2520the%2520parameter%2520space.%2520Experiments%2520show%2520our%2520framework%2520improves%250Acritical%2520scenario%2520generation%2520by%2520an%2520average%2520of%252056.23%255C%2525%2520and%2520demonstrates%2520greater%250Adiversity%2520under%2520novel%2520parameter-behavior%2520co-driven%2520metrics%2520when%2520tested%2520on%2520five%250Adecision-making%2520agents%252C%2520outperforming%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiCriTest%3A%20Testing%20Scenario%20Generation%20for%20Decision-Making%20Agents%0A%20%20Considering%20Diversity%20and%20Criticality&entry.906535625=Qitong%20Chu%20and%20Yufeng%20Yue%20and%20Danya%20Yao%20and%20Huaxin%20Pei&entry.1292438233=%20%20The%20growing%20deployment%20of%20decision-making%20agents%20in%20dynamic%20environments%0Aincreases%20the%20demand%20for%20safety%20verification.%20While%20critical%20testing%20scenario%0Ageneration%20has%20emerged%20as%20an%20appealing%20verification%20methodology%2C%20effectively%0Abalancing%20diversity%20and%20criticality%20remains%20a%20key%20challenge%20for%20existing%0Amethods%2C%20particularly%20due%20to%20local%20optima%20entrapment%20in%20high-dimensional%0Ascenario%20spaces.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20dual-space%20guided%0Atesting%20framework%20that%20coordinates%20scenario%20parameter%20space%20and%20agent%20behavior%0Aspace%2C%20aiming%20to%20generate%20testing%20scenarios%20considering%20diversity%20and%0Acriticality.%20Specifically%2C%20in%20the%20scenario%20parameter%20space%2C%20a%20hierarchical%0Arepresentation%20framework%20combines%20dimensionality%20reduction%20and%0Amulti-dimensional%20subspace%20evaluation%20to%20efficiently%20localize%20diverse%20and%0Acritical%20subspaces.%20This%20guides%20dynamic%20coordination%20between%20two%20generation%0Amodes%3A%20local%20perturbation%20and%20global%20exploration%2C%20optimizing%20critical%20scenario%0Aquantity%20and%20diversity.%20Complementarily%2C%20in%20the%20agent%20behavior%20space%2C%0Aagent-environment%20interaction%20data%20are%20leveraged%20to%20quantify%20behavioral%0Acriticality/diversity%20and%20adaptively%20support%20generation%20mode%20switching%2C%20forming%0Aa%20closed%20feedback%20loop%20that%20continuously%20enhances%20scenario%20characterization%20and%0Aexploration%20within%20the%20parameter%20space.%20Experiments%20show%20our%20framework%20improves%0Acritical%20scenario%20generation%20by%20an%20average%20of%2056.23%5C%25%20and%20demonstrates%20greater%0Adiversity%20under%20novel%20parameter-behavior%20co-driven%20metrics%20when%20tested%20on%20five%0Adecision-making%20agents%2C%20outperforming%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11514v1&entry.124074799=Read"},
{"title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems", "author": "Jiangbo Yu", "abstract": "  Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are viewed as vehicular systems capable of\nperceiving their environment and executing pre-programmed tasks independently\nof external input. However, both research and real-world deployments\nincreasingly showcase vehicles that demonstrate behaviors beyond this\ndefinition (including the SAE levels 0 to 5); Examples of this outpace include\nthe interaction with humans with natural language, goal adaptation, contextual\nreasoning, external tool use, and unseen ethical dilemma handling, largely\nempowered by multi-modal large language models (LLMs). These developments\nreveal a conceptual gap between technical autonomy and the broader cognitive\nand social capabilities needed for future human-centered mobility systems. To\naddress this gap, this paper introduces the concept of agentic vehicles (AgVs),\nreferring to vehicles that integrate agentic AI systems to reason, adapt, and\ninteract within complex environments. This paper proposes the term AgVs and\ntheir distinguishing characteristics from conventional AuVs. It synthesizes\nrelevant advances in integrating LLMs and AuVs and highlights how AgVs might\ntransform future mobility systems and ensure the systems are human-centered.\nThe paper concludes by identifying key challenges in the development and\ngovernance of AgVs, and how they can play a significant role in future agentic\ntransportation systems.\n", "link": "http://arxiv.org/abs/2507.04996v3", "date": "2025-08-15", "relevancy": 1.5771, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4923}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Autonomy%20to%20Agency%3A%20Agentic%20Vehicles%20for%20Human-Centered%20Mobility%0A%20%20Systems&body=Title%3A%20From%20Autonomy%20to%20Agency%3A%20Agentic%20Vehicles%20for%20Human-Centered%20Mobility%0A%20%20Systems%0AAuthor%3A%20Jiangbo%20Yu%0AAbstract%3A%20%20%20Autonomy%2C%20from%20the%20Greek%20autos%20%28self%29%20and%20nomos%20%28law%29%2C%20refers%20to%20the%20capacity%0Ato%20operate%20according%20to%20internal%20rules%20without%20external%20control.%20Accordingly%2C%0Aautonomous%20vehicles%20%28AuVs%29%20are%20viewed%20as%20vehicular%20systems%20capable%20of%0Aperceiving%20their%20environment%20and%20executing%20pre-programmed%20tasks%20independently%0Aof%20external%20input.%20However%2C%20both%20research%20and%20real-world%20deployments%0Aincreasingly%20showcase%20vehicles%20that%20demonstrate%20behaviors%20beyond%20this%0Adefinition%20%28including%20the%20SAE%20levels%200%20to%205%29%3B%20Examples%20of%20this%20outpace%20include%0Athe%20interaction%20with%20humans%20with%20natural%20language%2C%20goal%20adaptation%2C%20contextual%0Areasoning%2C%20external%20tool%20use%2C%20and%20unseen%20ethical%20dilemma%20handling%2C%20largely%0Aempowered%20by%20multi-modal%20large%20language%20models%20%28LLMs%29.%20These%20developments%0Areveal%20a%20conceptual%20gap%20between%20technical%20autonomy%20and%20the%20broader%20cognitive%0Aand%20social%20capabilities%20needed%20for%20future%20human-centered%20mobility%20systems.%20To%0Aaddress%20this%20gap%2C%20this%20paper%20introduces%20the%20concept%20of%20agentic%20vehicles%20%28AgVs%29%2C%0Areferring%20to%20vehicles%20that%20integrate%20agentic%20AI%20systems%20to%20reason%2C%20adapt%2C%20and%0Ainteract%20within%20complex%20environments.%20This%20paper%20proposes%20the%20term%20AgVs%20and%0Atheir%20distinguishing%20characteristics%20from%20conventional%20AuVs.%20It%20synthesizes%0Arelevant%20advances%20in%20integrating%20LLMs%20and%20AuVs%20and%20highlights%20how%20AgVs%20might%0Atransform%20future%20mobility%20systems%20and%20ensure%20the%20systems%20are%20human-centered.%0AThe%20paper%20concludes%20by%20identifying%20key%20challenges%20in%20the%20development%20and%0Agovernance%20of%20AgVs%2C%20and%20how%20they%20can%20play%20a%20significant%20role%20in%20future%20agentic%0Atransportation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04996v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Autonomy%2520to%2520Agency%253A%2520Agentic%2520Vehicles%2520for%2520Human-Centered%2520Mobility%250A%2520%2520Systems%26entry.906535625%3DJiangbo%2520Yu%26entry.1292438233%3D%2520%2520Autonomy%252C%2520from%2520the%2520Greek%2520autos%2520%2528self%2529%2520and%2520nomos%2520%2528law%2529%252C%2520refers%2520to%2520the%2520capacity%250Ato%2520operate%2520according%2520to%2520internal%2520rules%2520without%2520external%2520control.%2520Accordingly%252C%250Aautonomous%2520vehicles%2520%2528AuVs%2529%2520are%2520viewed%2520as%2520vehicular%2520systems%2520capable%2520of%250Aperceiving%2520their%2520environment%2520and%2520executing%2520pre-programmed%2520tasks%2520independently%250Aof%2520external%2520input.%2520However%252C%2520both%2520research%2520and%2520real-world%2520deployments%250Aincreasingly%2520showcase%2520vehicles%2520that%2520demonstrate%2520behaviors%2520beyond%2520this%250Adefinition%2520%2528including%2520the%2520SAE%2520levels%25200%2520to%25205%2529%253B%2520Examples%2520of%2520this%2520outpace%2520include%250Athe%2520interaction%2520with%2520humans%2520with%2520natural%2520language%252C%2520goal%2520adaptation%252C%2520contextual%250Areasoning%252C%2520external%2520tool%2520use%252C%2520and%2520unseen%2520ethical%2520dilemma%2520handling%252C%2520largely%250Aempowered%2520by%2520multi-modal%2520large%2520language%2520models%2520%2528LLMs%2529.%2520These%2520developments%250Areveal%2520a%2520conceptual%2520gap%2520between%2520technical%2520autonomy%2520and%2520the%2520broader%2520cognitive%250Aand%2520social%2520capabilities%2520needed%2520for%2520future%2520human-centered%2520mobility%2520systems.%2520To%250Aaddress%2520this%2520gap%252C%2520this%2520paper%2520introduces%2520the%2520concept%2520of%2520agentic%2520vehicles%2520%2528AgVs%2529%252C%250Areferring%2520to%2520vehicles%2520that%2520integrate%2520agentic%2520AI%2520systems%2520to%2520reason%252C%2520adapt%252C%2520and%250Ainteract%2520within%2520complex%2520environments.%2520This%2520paper%2520proposes%2520the%2520term%2520AgVs%2520and%250Atheir%2520distinguishing%2520characteristics%2520from%2520conventional%2520AuVs.%2520It%2520synthesizes%250Arelevant%2520advances%2520in%2520integrating%2520LLMs%2520and%2520AuVs%2520and%2520highlights%2520how%2520AgVs%2520might%250Atransform%2520future%2520mobility%2520systems%2520and%2520ensure%2520the%2520systems%2520are%2520human-centered.%250AThe%2520paper%2520concludes%2520by%2520identifying%2520key%2520challenges%2520in%2520the%2520development%2520and%250Agovernance%2520of%2520AgVs%252C%2520and%2520how%2520they%2520can%2520play%2520a%2520significant%2520role%2520in%2520future%2520agentic%250Atransportation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04996v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Autonomy%20to%20Agency%3A%20Agentic%20Vehicles%20for%20Human-Centered%20Mobility%0A%20%20Systems&entry.906535625=Jiangbo%20Yu&entry.1292438233=%20%20Autonomy%2C%20from%20the%20Greek%20autos%20%28self%29%20and%20nomos%20%28law%29%2C%20refers%20to%20the%20capacity%0Ato%20operate%20according%20to%20internal%20rules%20without%20external%20control.%20Accordingly%2C%0Aautonomous%20vehicles%20%28AuVs%29%20are%20viewed%20as%20vehicular%20systems%20capable%20of%0Aperceiving%20their%20environment%20and%20executing%20pre-programmed%20tasks%20independently%0Aof%20external%20input.%20However%2C%20both%20research%20and%20real-world%20deployments%0Aincreasingly%20showcase%20vehicles%20that%20demonstrate%20behaviors%20beyond%20this%0Adefinition%20%28including%20the%20SAE%20levels%200%20to%205%29%3B%20Examples%20of%20this%20outpace%20include%0Athe%20interaction%20with%20humans%20with%20natural%20language%2C%20goal%20adaptation%2C%20contextual%0Areasoning%2C%20external%20tool%20use%2C%20and%20unseen%20ethical%20dilemma%20handling%2C%20largely%0Aempowered%20by%20multi-modal%20large%20language%20models%20%28LLMs%29.%20These%20developments%0Areveal%20a%20conceptual%20gap%20between%20technical%20autonomy%20and%20the%20broader%20cognitive%0Aand%20social%20capabilities%20needed%20for%20future%20human-centered%20mobility%20systems.%20To%0Aaddress%20this%20gap%2C%20this%20paper%20introduces%20the%20concept%20of%20agentic%20vehicles%20%28AgVs%29%2C%0Areferring%20to%20vehicles%20that%20integrate%20agentic%20AI%20systems%20to%20reason%2C%20adapt%2C%20and%0Ainteract%20within%20complex%20environments.%20This%20paper%20proposes%20the%20term%20AgVs%20and%0Atheir%20distinguishing%20characteristics%20from%20conventional%20AuVs.%20It%20synthesizes%0Arelevant%20advances%20in%20integrating%20LLMs%20and%20AuVs%20and%20highlights%20how%20AgVs%20might%0Atransform%20future%20mobility%20systems%20and%20ensure%20the%20systems%20are%20human-centered.%0AThe%20paper%20concludes%20by%20identifying%20key%20challenges%20in%20the%20development%20and%0Agovernance%20of%20AgVs%2C%20and%20how%20they%20can%20play%20a%20significant%20role%20in%20future%20agentic%0Atransportation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04996v3&entry.124074799=Read"},
{"title": "Generalizable speech deepfake detection via meta-learned LoRA", "author": "Janne Laakkonen and Ivan Kukanov and Ville Hautam\u00e4ki", "abstract": "  Reliable detection of speech deepfakes (spoofs) must remain effective when\nthe distribution of spoofing attacks shifts. We frame the task as domain\ngeneralization and show that inserting Low-Rank Adaptation (LoRA) adapters into\nevery attention head of a self-supervised (SSL) backbone, then training only\nthose adapters with Meta-Learning Domain Generalization (MLDG), yields strong\nzero-shot performance. The resulting model updates about 3.6 million\nparameters, roughly 1.1% of the 318 million updated in full fine-tuning, yet\nsurpasses a fully fine-tuned counterpart on five of six evaluation corpora. A\nfirst-order MLDG loop encourages the adapters to focus on cues that persist\nacross attack types, lowering the average EER from 8.84% for the fully\nfine-tuned model to 5.30% with our best MLDG-LoRA configuration. Our findings\nshow that combining meta-learning with parameter-efficient adaptation offers an\neffective method for zero-shot, distribution-shift-aware speech deepfake\ndetection.\n", "link": "http://arxiv.org/abs/2502.10838v2", "date": "2025-08-15", "relevancy": 1.4588, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5088}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4861}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20speech%20deepfake%20detection%20via%20meta-learned%20LoRA&body=Title%3A%20Generalizable%20speech%20deepfake%20detection%20via%20meta-learned%20LoRA%0AAuthor%3A%20Janne%20Laakkonen%20and%20Ivan%20Kukanov%20and%20Ville%20Hautam%C3%A4ki%0AAbstract%3A%20%20%20Reliable%20detection%20of%20speech%20deepfakes%20%28spoofs%29%20must%20remain%20effective%20when%0Athe%20distribution%20of%20spoofing%20attacks%20shifts.%20We%20frame%20the%20task%20as%20domain%0Ageneralization%20and%20show%20that%20inserting%20Low-Rank%20Adaptation%20%28LoRA%29%20adapters%20into%0Aevery%20attention%20head%20of%20a%20self-supervised%20%28SSL%29%20backbone%2C%20then%20training%20only%0Athose%20adapters%20with%20Meta-Learning%20Domain%20Generalization%20%28MLDG%29%2C%20yields%20strong%0Azero-shot%20performance.%20The%20resulting%20model%20updates%20about%203.6%20million%0Aparameters%2C%20roughly%201.1%25%20of%20the%20318%20million%20updated%20in%20full%20fine-tuning%2C%20yet%0Asurpasses%20a%20fully%20fine-tuned%20counterpart%20on%20five%20of%20six%20evaluation%20corpora.%20A%0Afirst-order%20MLDG%20loop%20encourages%20the%20adapters%20to%20focus%20on%20cues%20that%20persist%0Aacross%20attack%20types%2C%20lowering%20the%20average%20EER%20from%208.84%25%20for%20the%20fully%0Afine-tuned%20model%20to%205.30%25%20with%20our%20best%20MLDG-LoRA%20configuration.%20Our%20findings%0Ashow%20that%20combining%20meta-learning%20with%20parameter-efficient%20adaptation%20offers%20an%0Aeffective%20method%20for%20zero-shot%2C%20distribution-shift-aware%20speech%20deepfake%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10838v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520speech%2520deepfake%2520detection%2520via%2520meta-learned%2520LoRA%26entry.906535625%3DJanne%2520Laakkonen%2520and%2520Ivan%2520Kukanov%2520and%2520Ville%2520Hautam%25C3%25A4ki%26entry.1292438233%3D%2520%2520Reliable%2520detection%2520of%2520speech%2520deepfakes%2520%2528spoofs%2529%2520must%2520remain%2520effective%2520when%250Athe%2520distribution%2520of%2520spoofing%2520attacks%2520shifts.%2520We%2520frame%2520the%2520task%2520as%2520domain%250Ageneralization%2520and%2520show%2520that%2520inserting%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520adapters%2520into%250Aevery%2520attention%2520head%2520of%2520a%2520self-supervised%2520%2528SSL%2529%2520backbone%252C%2520then%2520training%2520only%250Athose%2520adapters%2520with%2520Meta-Learning%2520Domain%2520Generalization%2520%2528MLDG%2529%252C%2520yields%2520strong%250Azero-shot%2520performance.%2520The%2520resulting%2520model%2520updates%2520about%25203.6%2520million%250Aparameters%252C%2520roughly%25201.1%2525%2520of%2520the%2520318%2520million%2520updated%2520in%2520full%2520fine-tuning%252C%2520yet%250Asurpasses%2520a%2520fully%2520fine-tuned%2520counterpart%2520on%2520five%2520of%2520six%2520evaluation%2520corpora.%2520A%250Afirst-order%2520MLDG%2520loop%2520encourages%2520the%2520adapters%2520to%2520focus%2520on%2520cues%2520that%2520persist%250Aacross%2520attack%2520types%252C%2520lowering%2520the%2520average%2520EER%2520from%25208.84%2525%2520for%2520the%2520fully%250Afine-tuned%2520model%2520to%25205.30%2525%2520with%2520our%2520best%2520MLDG-LoRA%2520configuration.%2520Our%2520findings%250Ashow%2520that%2520combining%2520meta-learning%2520with%2520parameter-efficient%2520adaptation%2520offers%2520an%250Aeffective%2520method%2520for%2520zero-shot%252C%2520distribution-shift-aware%2520speech%2520deepfake%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10838v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20speech%20deepfake%20detection%20via%20meta-learned%20LoRA&entry.906535625=Janne%20Laakkonen%20and%20Ivan%20Kukanov%20and%20Ville%20Hautam%C3%A4ki&entry.1292438233=%20%20Reliable%20detection%20of%20speech%20deepfakes%20%28spoofs%29%20must%20remain%20effective%20when%0Athe%20distribution%20of%20spoofing%20attacks%20shifts.%20We%20frame%20the%20task%20as%20domain%0Ageneralization%20and%20show%20that%20inserting%20Low-Rank%20Adaptation%20%28LoRA%29%20adapters%20into%0Aevery%20attention%20head%20of%20a%20self-supervised%20%28SSL%29%20backbone%2C%20then%20training%20only%0Athose%20adapters%20with%20Meta-Learning%20Domain%20Generalization%20%28MLDG%29%2C%20yields%20strong%0Azero-shot%20performance.%20The%20resulting%20model%20updates%20about%203.6%20million%0Aparameters%2C%20roughly%201.1%25%20of%20the%20318%20million%20updated%20in%20full%20fine-tuning%2C%20yet%0Asurpasses%20a%20fully%20fine-tuned%20counterpart%20on%20five%20of%20six%20evaluation%20corpora.%20A%0Afirst-order%20MLDG%20loop%20encourages%20the%20adapters%20to%20focus%20on%20cues%20that%20persist%0Aacross%20attack%20types%2C%20lowering%20the%20average%20EER%20from%208.84%25%20for%20the%20fully%0Afine-tuned%20model%20to%205.30%25%20with%20our%20best%20MLDG-LoRA%20configuration.%20Our%20findings%0Ashow%20that%20combining%20meta-learning%20with%20parameter-efficient%20adaptation%20offers%20an%0Aeffective%20method%20for%20zero-shot%2C%20distribution-shift-aware%20speech%20deepfake%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10838v2&entry.124074799=Read"},
{"title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT", "author": "Zhangxuan Gu and Zhengwen Zeng and Zhenyu Xu and Xingran Zhou and Shuheng Shen and Yunfei Liu and Beitong Zhou and Changhua Meng and Tianyu Xia and Weizhi Chen and Yue Wen and Jingya Dou and Fei Tang and Jinzhen Lin and Yulin Liu and Zhenlin Guo and Yichen Gong and Heng Jia and Changlong Gao and Yuan Guo and Yong Deng and Zhenyu Guo and Liang Chen and Weiqiang Wang", "abstract": "  We present UI-Venus, a native UI agent that takes only screenshots as input\nbased on a multimodal large language model. UI-Venus achieves SOTA performance\non both UI grounding and navigation tasks using only several hundred thousand\nhigh-quality training samples through reinforcement finetune (RFT) based on\nQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /\n50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,\nScreenspot-V2 / Pro, surpassing the previous SOTA baselines including\nopen-source GTA1 and closed-source UI-TARS-1.5. To show UI-Venus's summary and\nplaning ability, we also evaluate it on the AndroidWorld, an online UI\nnavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%\nsuccess rate, also beating existing models. To achieve this, we introduce\ncarefully designed reward functions for both UI grounding and navigation tasks\nand corresponding efficient data cleaning strategies. To further boost\nnavigation performance, we propose Self-Evolving Trajectory History Alignment &\nSparse Action Enhancement that refine historical reasoning traces and balances\nthe distribution of sparse but critical actions, leading to more coherent\nplanning and better generalization in complex UI tasks. Our contributions\ninclude the publish of SOTA open-source UI agents, comprehensive data cleaning\nprotocols and a novel self-evolving framework for improving navigation\nperformance, which encourage further research and development in the community.\nCode is available at https://github.com/inclusionAI/UI-Venus.\n", "link": "http://arxiv.org/abs/2508.10833v2", "date": "2025-08-15", "relevancy": 1.4917, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.509}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4998}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UI-Venus%20Technical%20Report%3A%20Building%20High-performance%20UI%20Agents%20with%20RFT&body=Title%3A%20UI-Venus%20Technical%20Report%3A%20Building%20High-performance%20UI%20Agents%20with%20RFT%0AAuthor%3A%20Zhangxuan%20Gu%20and%20Zhengwen%20Zeng%20and%20Zhenyu%20Xu%20and%20Xingran%20Zhou%20and%20Shuheng%20Shen%20and%20Yunfei%20Liu%20and%20Beitong%20Zhou%20and%20Changhua%20Meng%20and%20Tianyu%20Xia%20and%20Weizhi%20Chen%20and%20Yue%20Wen%20and%20Jingya%20Dou%20and%20Fei%20Tang%20and%20Jinzhen%20Lin%20and%20Yulin%20Liu%20and%20Zhenlin%20Guo%20and%20Yichen%20Gong%20and%20Heng%20Jia%20and%20Changlong%20Gao%20and%20Yuan%20Guo%20and%20Yong%20Deng%20and%20Zhenyu%20Guo%20and%20Liang%20Chen%20and%20Weiqiang%20Wang%0AAbstract%3A%20%20%20We%20present%20UI-Venus%2C%20a%20native%20UI%20agent%20that%20takes%20only%20screenshots%20as%20input%0Abased%20on%20a%20multimodal%20large%20language%20model.%20UI-Venus%20achieves%20SOTA%20performance%0Aon%20both%20UI%20grounding%20and%20navigation%20tasks%20using%20only%20several%20hundred%20thousand%0Ahigh-quality%20training%20samples%20through%20reinforcement%20finetune%20%28RFT%29%20based%20on%0AQwen2.5-VL.%20Specifically%2C%20the%207B%20and%2072B%20variants%20of%20UI-Venus%20obtain%2094.1%25%20/%0A50.8%25%20and%2095.3%25%20/%2061.9%25%20on%20the%20standard%20grounding%20benchmarks%2C%20i.e.%2C%0AScreenspot-V2%20/%20Pro%2C%20surpassing%20the%20previous%20SOTA%20baselines%20including%0Aopen-source%20GTA1%20and%20closed-source%20UI-TARS-1.5.%20To%20show%20UI-Venus%27s%20summary%20and%0Aplaning%20ability%2C%20we%20also%20evaluate%20it%20on%20the%20AndroidWorld%2C%20an%20online%20UI%0Anavigation%20arena%2C%20on%20which%20our%207B%20and%2072B%20variants%20achieve%2049.1%25%20and%2065.9%25%0Asuccess%20rate%2C%20also%20beating%20existing%20models.%20To%20achieve%20this%2C%20we%20introduce%0Acarefully%20designed%20reward%20functions%20for%20both%20UI%20grounding%20and%20navigation%20tasks%0Aand%20corresponding%20efficient%20data%20cleaning%20strategies.%20To%20further%20boost%0Anavigation%20performance%2C%20we%20propose%20Self-Evolving%20Trajectory%20History%20Alignment%20%26%0ASparse%20Action%20Enhancement%20that%20refine%20historical%20reasoning%20traces%20and%20balances%0Athe%20distribution%20of%20sparse%20but%20critical%20actions%2C%20leading%20to%20more%20coherent%0Aplanning%20and%20better%20generalization%20in%20complex%20UI%20tasks.%20Our%20contributions%0Ainclude%20the%20publish%20of%20SOTA%20open-source%20UI%20agents%2C%20comprehensive%20data%20cleaning%0Aprotocols%20and%20a%20novel%20self-evolving%20framework%20for%20improving%20navigation%0Aperformance%2C%20which%20encourage%20further%20research%20and%20development%20in%20the%20community.%0ACode%20is%20available%20at%20https%3A//github.com/inclusionAI/UI-Venus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10833v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUI-Venus%2520Technical%2520Report%253A%2520Building%2520High-performance%2520UI%2520Agents%2520with%2520RFT%26entry.906535625%3DZhangxuan%2520Gu%2520and%2520Zhengwen%2520Zeng%2520and%2520Zhenyu%2520Xu%2520and%2520Xingran%2520Zhou%2520and%2520Shuheng%2520Shen%2520and%2520Yunfei%2520Liu%2520and%2520Beitong%2520Zhou%2520and%2520Changhua%2520Meng%2520and%2520Tianyu%2520Xia%2520and%2520Weizhi%2520Chen%2520and%2520Yue%2520Wen%2520and%2520Jingya%2520Dou%2520and%2520Fei%2520Tang%2520and%2520Jinzhen%2520Lin%2520and%2520Yulin%2520Liu%2520and%2520Zhenlin%2520Guo%2520and%2520Yichen%2520Gong%2520and%2520Heng%2520Jia%2520and%2520Changlong%2520Gao%2520and%2520Yuan%2520Guo%2520and%2520Yong%2520Deng%2520and%2520Zhenyu%2520Guo%2520and%2520Liang%2520Chen%2520and%2520Weiqiang%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520UI-Venus%252C%2520a%2520native%2520UI%2520agent%2520that%2520takes%2520only%2520screenshots%2520as%2520input%250Abased%2520on%2520a%2520multimodal%2520large%2520language%2520model.%2520UI-Venus%2520achieves%2520SOTA%2520performance%250Aon%2520both%2520UI%2520grounding%2520and%2520navigation%2520tasks%2520using%2520only%2520several%2520hundred%2520thousand%250Ahigh-quality%2520training%2520samples%2520through%2520reinforcement%2520finetune%2520%2528RFT%2529%2520based%2520on%250AQwen2.5-VL.%2520Specifically%252C%2520the%25207B%2520and%252072B%2520variants%2520of%2520UI-Venus%2520obtain%252094.1%2525%2520/%250A50.8%2525%2520and%252095.3%2525%2520/%252061.9%2525%2520on%2520the%2520standard%2520grounding%2520benchmarks%252C%2520i.e.%252C%250AScreenspot-V2%2520/%2520Pro%252C%2520surpassing%2520the%2520previous%2520SOTA%2520baselines%2520including%250Aopen-source%2520GTA1%2520and%2520closed-source%2520UI-TARS-1.5.%2520To%2520show%2520UI-Venus%2527s%2520summary%2520and%250Aplaning%2520ability%252C%2520we%2520also%2520evaluate%2520it%2520on%2520the%2520AndroidWorld%252C%2520an%2520online%2520UI%250Anavigation%2520arena%252C%2520on%2520which%2520our%25207B%2520and%252072B%2520variants%2520achieve%252049.1%2525%2520and%252065.9%2525%250Asuccess%2520rate%252C%2520also%2520beating%2520existing%2520models.%2520To%2520achieve%2520this%252C%2520we%2520introduce%250Acarefully%2520designed%2520reward%2520functions%2520for%2520both%2520UI%2520grounding%2520and%2520navigation%2520tasks%250Aand%2520corresponding%2520efficient%2520data%2520cleaning%2520strategies.%2520To%2520further%2520boost%250Anavigation%2520performance%252C%2520we%2520propose%2520Self-Evolving%2520Trajectory%2520History%2520Alignment%2520%2526%250ASparse%2520Action%2520Enhancement%2520that%2520refine%2520historical%2520reasoning%2520traces%2520and%2520balances%250Athe%2520distribution%2520of%2520sparse%2520but%2520critical%2520actions%252C%2520leading%2520to%2520more%2520coherent%250Aplanning%2520and%2520better%2520generalization%2520in%2520complex%2520UI%2520tasks.%2520Our%2520contributions%250Ainclude%2520the%2520publish%2520of%2520SOTA%2520open-source%2520UI%2520agents%252C%2520comprehensive%2520data%2520cleaning%250Aprotocols%2520and%2520a%2520novel%2520self-evolving%2520framework%2520for%2520improving%2520navigation%250Aperformance%252C%2520which%2520encourage%2520further%2520research%2520and%2520development%2520in%2520the%2520community.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/inclusionAI/UI-Venus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10833v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UI-Venus%20Technical%20Report%3A%20Building%20High-performance%20UI%20Agents%20with%20RFT&entry.906535625=Zhangxuan%20Gu%20and%20Zhengwen%20Zeng%20and%20Zhenyu%20Xu%20and%20Xingran%20Zhou%20and%20Shuheng%20Shen%20and%20Yunfei%20Liu%20and%20Beitong%20Zhou%20and%20Changhua%20Meng%20and%20Tianyu%20Xia%20and%20Weizhi%20Chen%20and%20Yue%20Wen%20and%20Jingya%20Dou%20and%20Fei%20Tang%20and%20Jinzhen%20Lin%20and%20Yulin%20Liu%20and%20Zhenlin%20Guo%20and%20Yichen%20Gong%20and%20Heng%20Jia%20and%20Changlong%20Gao%20and%20Yuan%20Guo%20and%20Yong%20Deng%20and%20Zhenyu%20Guo%20and%20Liang%20Chen%20and%20Weiqiang%20Wang&entry.1292438233=%20%20We%20present%20UI-Venus%2C%20a%20native%20UI%20agent%20that%20takes%20only%20screenshots%20as%20input%0Abased%20on%20a%20multimodal%20large%20language%20model.%20UI-Venus%20achieves%20SOTA%20performance%0Aon%20both%20UI%20grounding%20and%20navigation%20tasks%20using%20only%20several%20hundred%20thousand%0Ahigh-quality%20training%20samples%20through%20reinforcement%20finetune%20%28RFT%29%20based%20on%0AQwen2.5-VL.%20Specifically%2C%20the%207B%20and%2072B%20variants%20of%20UI-Venus%20obtain%2094.1%25%20/%0A50.8%25%20and%2095.3%25%20/%2061.9%25%20on%20the%20standard%20grounding%20benchmarks%2C%20i.e.%2C%0AScreenspot-V2%20/%20Pro%2C%20surpassing%20the%20previous%20SOTA%20baselines%20including%0Aopen-source%20GTA1%20and%20closed-source%20UI-TARS-1.5.%20To%20show%20UI-Venus%27s%20summary%20and%0Aplaning%20ability%2C%20we%20also%20evaluate%20it%20on%20the%20AndroidWorld%2C%20an%20online%20UI%0Anavigation%20arena%2C%20on%20which%20our%207B%20and%2072B%20variants%20achieve%2049.1%25%20and%2065.9%25%0Asuccess%20rate%2C%20also%20beating%20existing%20models.%20To%20achieve%20this%2C%20we%20introduce%0Acarefully%20designed%20reward%20functions%20for%20both%20UI%20grounding%20and%20navigation%20tasks%0Aand%20corresponding%20efficient%20data%20cleaning%20strategies.%20To%20further%20boost%0Anavigation%20performance%2C%20we%20propose%20Self-Evolving%20Trajectory%20History%20Alignment%20%26%0ASparse%20Action%20Enhancement%20that%20refine%20historical%20reasoning%20traces%20and%20balances%0Athe%20distribution%20of%20sparse%20but%20critical%20actions%2C%20leading%20to%20more%20coherent%0Aplanning%20and%20better%20generalization%20in%20complex%20UI%20tasks.%20Our%20contributions%0Ainclude%20the%20publish%20of%20SOTA%20open-source%20UI%20agents%2C%20comprehensive%20data%20cleaning%0Aprotocols%20and%20a%20novel%20self-evolving%20framework%20for%20improving%20navigation%0Aperformance%2C%20which%20encourage%20further%20research%20and%20development%20in%20the%20community.%0ACode%20is%20available%20at%20https%3A//github.com/inclusionAI/UI-Venus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10833v2&entry.124074799=Read"},
{"title": "SAND: One-Shot Feature Selection with Additive Noise Distortion", "author": "Pedram Pad and Hadi Hammoud and Mohamad Dia and Nadim Maamari and L. Andrea Dunbar", "abstract": "  Feature selection is a critical step in data-driven applications, reducing\ninput dimensionality to enhance learning accuracy, computational efficiency,\nand interpretability. Existing state-of-the-art methods often require\npost-selection retraining and extensive hyperparameter tuning, complicating\ntheir adoption. We introduce a novel, non-intrusive feature selection layer\nthat, given a target feature count $k$, automatically identifies and selects\nthe $k$ most informative features during neural network training. Our method is\nuniquely simple, requiring no alterations to the loss function, network\narchitecture, or post-selection retraining. The layer is mathematically elegant\nand can be fully described by: \\begin{align} \\nonumber \\tilde{x}_i = a_i x_i +\n(1-a_i)z_i \\end{align} where $x_i$ is the input feature, $\\tilde{x}_i$ the\noutput, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that\n$\\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect,\ndriving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the\nrest to $0$ (discarding redundant ones) via weighted noise distortion and gain\nnormalization. Despite its extreme simplicity, our method delivers\nstate-of-the-art performance on standard benchmark datasets and a novel\nreal-world dataset, outperforming or matching existing approaches without\nrequiring hyperparameter search for $k$ or retraining. Theoretical analysis in\nthe context of linear regression further validates its efficacy. Our work\ndemonstrates that simplicity and performance are not mutually exclusive,\noffering a powerful yet straightforward tool for feature selection in machine\nlearning.\n", "link": "http://arxiv.org/abs/2505.03923v2", "date": "2025-08-15", "relevancy": 1.4094, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4794}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4684}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAND%3A%20One-Shot%20Feature%20Selection%20with%20Additive%20Noise%20Distortion&body=Title%3A%20SAND%3A%20One-Shot%20Feature%20Selection%20with%20Additive%20Noise%20Distortion%0AAuthor%3A%20Pedram%20Pad%20and%20Hadi%20Hammoud%20and%20Mohamad%20Dia%20and%20Nadim%20Maamari%20and%20L.%20Andrea%20Dunbar%0AAbstract%3A%20%20%20Feature%20selection%20is%20a%20critical%20step%20in%20data-driven%20applications%2C%20reducing%0Ainput%20dimensionality%20to%20enhance%20learning%20accuracy%2C%20computational%20efficiency%2C%0Aand%20interpretability.%20Existing%20state-of-the-art%20methods%20often%20require%0Apost-selection%20retraining%20and%20extensive%20hyperparameter%20tuning%2C%20complicating%0Atheir%20adoption.%20We%20introduce%20a%20novel%2C%20non-intrusive%20feature%20selection%20layer%0Athat%2C%20given%20a%20target%20feature%20count%20%24k%24%2C%20automatically%20identifies%20and%20selects%0Athe%20%24k%24%20most%20informative%20features%20during%20neural%20network%20training.%20Our%20method%20is%0Auniquely%20simple%2C%20requiring%20no%20alterations%20to%20the%20loss%20function%2C%20network%0Aarchitecture%2C%20or%20post-selection%20retraining.%20The%20layer%20is%20mathematically%20elegant%0Aand%20can%20be%20fully%20described%20by%3A%20%5Cbegin%7Balign%7D%20%5Cnonumber%20%5Ctilde%7Bx%7D_i%20%3D%20a_i%20x_i%20%2B%0A%281-a_i%29z_i%20%5Cend%7Balign%7D%20where%20%24x_i%24%20is%20the%20input%20feature%2C%20%24%5Ctilde%7Bx%7D_i%24%20the%0Aoutput%2C%20%24z_i%24%20a%20Gaussian%20noise%2C%20and%20%24a_i%24%20trainable%20gain%20such%20that%0A%24%5Csum_i%7Ba_i%5E2%7D%3Dk%24.%20This%20formulation%20induces%20an%20automatic%20clustering%20effect%2C%0Adriving%20%24k%24%20of%20the%20%24a_i%24%20gains%20to%20%241%24%20%28selecting%20informative%20features%29%20and%20the%0Arest%20to%20%240%24%20%28discarding%20redundant%20ones%29%20via%20weighted%20noise%20distortion%20and%20gain%0Anormalization.%20Despite%20its%20extreme%20simplicity%2C%20our%20method%20delivers%0Astate-of-the-art%20performance%20on%20standard%20benchmark%20datasets%20and%20a%20novel%0Areal-world%20dataset%2C%20outperforming%20or%20matching%20existing%20approaches%20without%0Arequiring%20hyperparameter%20search%20for%20%24k%24%20or%20retraining.%20Theoretical%20analysis%20in%0Athe%20context%20of%20linear%20regression%20further%20validates%20its%20efficacy.%20Our%20work%0Ademonstrates%20that%20simplicity%20and%20performance%20are%20not%20mutually%20exclusive%2C%0Aoffering%20a%20powerful%20yet%20straightforward%20tool%20for%20feature%20selection%20in%20machine%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03923v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAND%253A%2520One-Shot%2520Feature%2520Selection%2520with%2520Additive%2520Noise%2520Distortion%26entry.906535625%3DPedram%2520Pad%2520and%2520Hadi%2520Hammoud%2520and%2520Mohamad%2520Dia%2520and%2520Nadim%2520Maamari%2520and%2520L.%2520Andrea%2520Dunbar%26entry.1292438233%3D%2520%2520Feature%2520selection%2520is%2520a%2520critical%2520step%2520in%2520data-driven%2520applications%252C%2520reducing%250Ainput%2520dimensionality%2520to%2520enhance%2520learning%2520accuracy%252C%2520computational%2520efficiency%252C%250Aand%2520interpretability.%2520Existing%2520state-of-the-art%2520methods%2520often%2520require%250Apost-selection%2520retraining%2520and%2520extensive%2520hyperparameter%2520tuning%252C%2520complicating%250Atheir%2520adoption.%2520We%2520introduce%2520a%2520novel%252C%2520non-intrusive%2520feature%2520selection%2520layer%250Athat%252C%2520given%2520a%2520target%2520feature%2520count%2520%2524k%2524%252C%2520automatically%2520identifies%2520and%2520selects%250Athe%2520%2524k%2524%2520most%2520informative%2520features%2520during%2520neural%2520network%2520training.%2520Our%2520method%2520is%250Auniquely%2520simple%252C%2520requiring%2520no%2520alterations%2520to%2520the%2520loss%2520function%252C%2520network%250Aarchitecture%252C%2520or%2520post-selection%2520retraining.%2520The%2520layer%2520is%2520mathematically%2520elegant%250Aand%2520can%2520be%2520fully%2520described%2520by%253A%2520%255Cbegin%257Balign%257D%2520%255Cnonumber%2520%255Ctilde%257Bx%257D_i%2520%253D%2520a_i%2520x_i%2520%252B%250A%25281-a_i%2529z_i%2520%255Cend%257Balign%257D%2520where%2520%2524x_i%2524%2520is%2520the%2520input%2520feature%252C%2520%2524%255Ctilde%257Bx%257D_i%2524%2520the%250Aoutput%252C%2520%2524z_i%2524%2520a%2520Gaussian%2520noise%252C%2520and%2520%2524a_i%2524%2520trainable%2520gain%2520such%2520that%250A%2524%255Csum_i%257Ba_i%255E2%257D%253Dk%2524.%2520This%2520formulation%2520induces%2520an%2520automatic%2520clustering%2520effect%252C%250Adriving%2520%2524k%2524%2520of%2520the%2520%2524a_i%2524%2520gains%2520to%2520%25241%2524%2520%2528selecting%2520informative%2520features%2529%2520and%2520the%250Arest%2520to%2520%25240%2524%2520%2528discarding%2520redundant%2520ones%2529%2520via%2520weighted%2520noise%2520distortion%2520and%2520gain%250Anormalization.%2520Despite%2520its%2520extreme%2520simplicity%252C%2520our%2520method%2520delivers%250Astate-of-the-art%2520performance%2520on%2520standard%2520benchmark%2520datasets%2520and%2520a%2520novel%250Areal-world%2520dataset%252C%2520outperforming%2520or%2520matching%2520existing%2520approaches%2520without%250Arequiring%2520hyperparameter%2520search%2520for%2520%2524k%2524%2520or%2520retraining.%2520Theoretical%2520analysis%2520in%250Athe%2520context%2520of%2520linear%2520regression%2520further%2520validates%2520its%2520efficacy.%2520Our%2520work%250Ademonstrates%2520that%2520simplicity%2520and%2520performance%2520are%2520not%2520mutually%2520exclusive%252C%250Aoffering%2520a%2520powerful%2520yet%2520straightforward%2520tool%2520for%2520feature%2520selection%2520in%2520machine%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03923v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAND%3A%20One-Shot%20Feature%20Selection%20with%20Additive%20Noise%20Distortion&entry.906535625=Pedram%20Pad%20and%20Hadi%20Hammoud%20and%20Mohamad%20Dia%20and%20Nadim%20Maamari%20and%20L.%20Andrea%20Dunbar&entry.1292438233=%20%20Feature%20selection%20is%20a%20critical%20step%20in%20data-driven%20applications%2C%20reducing%0Ainput%20dimensionality%20to%20enhance%20learning%20accuracy%2C%20computational%20efficiency%2C%0Aand%20interpretability.%20Existing%20state-of-the-art%20methods%20often%20require%0Apost-selection%20retraining%20and%20extensive%20hyperparameter%20tuning%2C%20complicating%0Atheir%20adoption.%20We%20introduce%20a%20novel%2C%20non-intrusive%20feature%20selection%20layer%0Athat%2C%20given%20a%20target%20feature%20count%20%24k%24%2C%20automatically%20identifies%20and%20selects%0Athe%20%24k%24%20most%20informative%20features%20during%20neural%20network%20training.%20Our%20method%20is%0Auniquely%20simple%2C%20requiring%20no%20alterations%20to%20the%20loss%20function%2C%20network%0Aarchitecture%2C%20or%20post-selection%20retraining.%20The%20layer%20is%20mathematically%20elegant%0Aand%20can%20be%20fully%20described%20by%3A%20%5Cbegin%7Balign%7D%20%5Cnonumber%20%5Ctilde%7Bx%7D_i%20%3D%20a_i%20x_i%20%2B%0A%281-a_i%29z_i%20%5Cend%7Balign%7D%20where%20%24x_i%24%20is%20the%20input%20feature%2C%20%24%5Ctilde%7Bx%7D_i%24%20the%0Aoutput%2C%20%24z_i%24%20a%20Gaussian%20noise%2C%20and%20%24a_i%24%20trainable%20gain%20such%20that%0A%24%5Csum_i%7Ba_i%5E2%7D%3Dk%24.%20This%20formulation%20induces%20an%20automatic%20clustering%20effect%2C%0Adriving%20%24k%24%20of%20the%20%24a_i%24%20gains%20to%20%241%24%20%28selecting%20informative%20features%29%20and%20the%0Arest%20to%20%240%24%20%28discarding%20redundant%20ones%29%20via%20weighted%20noise%20distortion%20and%20gain%0Anormalization.%20Despite%20its%20extreme%20simplicity%2C%20our%20method%20delivers%0Astate-of-the-art%20performance%20on%20standard%20benchmark%20datasets%20and%20a%20novel%0Areal-world%20dataset%2C%20outperforming%20or%20matching%20existing%20approaches%20without%0Arequiring%20hyperparameter%20search%20for%20%24k%24%20or%20retraining.%20Theoretical%20analysis%20in%0Athe%20context%20of%20linear%20regression%20further%20validates%20its%20efficacy.%20Our%20work%0Ademonstrates%20that%20simplicity%20and%20performance%20are%20not%20mutually%20exclusive%2C%0Aoffering%20a%20powerful%20yet%20straightforward%20tool%20for%20feature%20selection%20in%20machine%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03923v2&entry.124074799=Read"},
{"title": "Automated Building Heritage Assessment Using Street-Level Imagery", "author": "Kristina Dabrock and Tim Johansson and Anna Donarelli and Mikael Mangold and Noah Pflugradt and Jann Michael Weinand and Jochen Lin\u00dfen", "abstract": "  Detailed data is required to quantify energy conservation measures in\nbuildings, such as envelop retrofits, without compromising cultural heritage.\nNovel artificial intelligence tools may improve efficiency in identifying\nheritage values in buildings compared to costly and time-consuming traditional\ninventories. In this study, the large language model GPT was used to detect\nvarious aspects of cultural heritage value in fa\\c{c}ade images. Using this\ndata and building register data as features, machine learning models were\ntrained to classify multi-family and non-residential buildings in Stockholm,\nSweden. Validation against an expert-created inventory shows a macro F1-score\nof 0.71 using a combination of register data and features retrieved from GPT,\nand a score of 0.60 using only GPT-derived data. The presented methodology can\ncontribute to a higher-quality database and thus support careful energy\nefficiency measures and integrated consideration of heritage value in\nlarge-scale energetic refurbishment scenarios.\n", "link": "http://arxiv.org/abs/2508.11486v1", "date": "2025-08-15", "relevancy": 1.8674, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4737}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4702}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Building%20Heritage%20Assessment%20Using%20Street-Level%20Imagery&body=Title%3A%20Automated%20Building%20Heritage%20Assessment%20Using%20Street-Level%20Imagery%0AAuthor%3A%20Kristina%20Dabrock%20and%20Tim%20Johansson%20and%20Anna%20Donarelli%20and%20Mikael%20Mangold%20and%20Noah%20Pflugradt%20and%20Jann%20Michael%20Weinand%20and%20Jochen%20Lin%C3%9Fen%0AAbstract%3A%20%20%20Detailed%20data%20is%20required%20to%20quantify%20energy%20conservation%20measures%20in%0Abuildings%2C%20such%20as%20envelop%20retrofits%2C%20without%20compromising%20cultural%20heritage.%0ANovel%20artificial%20intelligence%20tools%20may%20improve%20efficiency%20in%20identifying%0Aheritage%20values%20in%20buildings%20compared%20to%20costly%20and%20time-consuming%20traditional%0Ainventories.%20In%20this%20study%2C%20the%20large%20language%20model%20GPT%20was%20used%20to%20detect%0Avarious%20aspects%20of%20cultural%20heritage%20value%20in%20fa%5Cc%7Bc%7Dade%20images.%20Using%20this%0Adata%20and%20building%20register%20data%20as%20features%2C%20machine%20learning%20models%20were%0Atrained%20to%20classify%20multi-family%20and%20non-residential%20buildings%20in%20Stockholm%2C%0ASweden.%20Validation%20against%20an%20expert-created%20inventory%20shows%20a%20macro%20F1-score%0Aof%200.71%20using%20a%20combination%20of%20register%20data%20and%20features%20retrieved%20from%20GPT%2C%0Aand%20a%20score%20of%200.60%20using%20only%20GPT-derived%20data.%20The%20presented%20methodology%20can%0Acontribute%20to%20a%20higher-quality%20database%20and%20thus%20support%20careful%20energy%0Aefficiency%20measures%20and%20integrated%20consideration%20of%20heritage%20value%20in%0Alarge-scale%20energetic%20refurbishment%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Building%2520Heritage%2520Assessment%2520Using%2520Street-Level%2520Imagery%26entry.906535625%3DKristina%2520Dabrock%2520and%2520Tim%2520Johansson%2520and%2520Anna%2520Donarelli%2520and%2520Mikael%2520Mangold%2520and%2520Noah%2520Pflugradt%2520and%2520Jann%2520Michael%2520Weinand%2520and%2520Jochen%2520Lin%25C3%259Fen%26entry.1292438233%3D%2520%2520Detailed%2520data%2520is%2520required%2520to%2520quantify%2520energy%2520conservation%2520measures%2520in%250Abuildings%252C%2520such%2520as%2520envelop%2520retrofits%252C%2520without%2520compromising%2520cultural%2520heritage.%250ANovel%2520artificial%2520intelligence%2520tools%2520may%2520improve%2520efficiency%2520in%2520identifying%250Aheritage%2520values%2520in%2520buildings%2520compared%2520to%2520costly%2520and%2520time-consuming%2520traditional%250Ainventories.%2520In%2520this%2520study%252C%2520the%2520large%2520language%2520model%2520GPT%2520was%2520used%2520to%2520detect%250Avarious%2520aspects%2520of%2520cultural%2520heritage%2520value%2520in%2520fa%255Cc%257Bc%257Dade%2520images.%2520Using%2520this%250Adata%2520and%2520building%2520register%2520data%2520as%2520features%252C%2520machine%2520learning%2520models%2520were%250Atrained%2520to%2520classify%2520multi-family%2520and%2520non-residential%2520buildings%2520in%2520Stockholm%252C%250ASweden.%2520Validation%2520against%2520an%2520expert-created%2520inventory%2520shows%2520a%2520macro%2520F1-score%250Aof%25200.71%2520using%2520a%2520combination%2520of%2520register%2520data%2520and%2520features%2520retrieved%2520from%2520GPT%252C%250Aand%2520a%2520score%2520of%25200.60%2520using%2520only%2520GPT-derived%2520data.%2520The%2520presented%2520methodology%2520can%250Acontribute%2520to%2520a%2520higher-quality%2520database%2520and%2520thus%2520support%2520careful%2520energy%250Aefficiency%2520measures%2520and%2520integrated%2520consideration%2520of%2520heritage%2520value%2520in%250Alarge-scale%2520energetic%2520refurbishment%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Building%20Heritage%20Assessment%20Using%20Street-Level%20Imagery&entry.906535625=Kristina%20Dabrock%20and%20Tim%20Johansson%20and%20Anna%20Donarelli%20and%20Mikael%20Mangold%20and%20Noah%20Pflugradt%20and%20Jann%20Michael%20Weinand%20and%20Jochen%20Lin%C3%9Fen&entry.1292438233=%20%20Detailed%20data%20is%20required%20to%20quantify%20energy%20conservation%20measures%20in%0Abuildings%2C%20such%20as%20envelop%20retrofits%2C%20without%20compromising%20cultural%20heritage.%0ANovel%20artificial%20intelligence%20tools%20may%20improve%20efficiency%20in%20identifying%0Aheritage%20values%20in%20buildings%20compared%20to%20costly%20and%20time-consuming%20traditional%0Ainventories.%20In%20this%20study%2C%20the%20large%20language%20model%20GPT%20was%20used%20to%20detect%0Avarious%20aspects%20of%20cultural%20heritage%20value%20in%20fa%5Cc%7Bc%7Dade%20images.%20Using%20this%0Adata%20and%20building%20register%20data%20as%20features%2C%20machine%20learning%20models%20were%0Atrained%20to%20classify%20multi-family%20and%20non-residential%20buildings%20in%20Stockholm%2C%0ASweden.%20Validation%20against%20an%20expert-created%20inventory%20shows%20a%20macro%20F1-score%0Aof%200.71%20using%20a%20combination%20of%20register%20data%20and%20features%20retrieved%20from%20GPT%2C%0Aand%20a%20score%20of%200.60%20using%20only%20GPT-derived%20data.%20The%20presented%20methodology%20can%0Acontribute%20to%20a%20higher-quality%20database%20and%20thus%20support%20careful%20energy%0Aefficiency%20measures%20and%20integrated%20consideration%20of%20heritage%20value%20in%0Alarge-scale%20energetic%20refurbishment%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11486v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


