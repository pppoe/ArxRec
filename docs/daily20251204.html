<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251203.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "C3G: Learning Compact 3D Representations with 2K Gaussians", "author": "Honggyu An and Jaewoo Jung and Mungyeom Kim and Sunghwan Hong and Chaehyun Kim and Kazumi Fukuda and Minkyeong Jeon and Jisang Han and Takuya Narihira and Hyuna Ko and Junsu Kim and Yuki Mitsufuji and Seungryong Kim", "abstract": "Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.", "link": "http://arxiv.org/abs/2512.04021v1", "date": "2025-12-03", "relevancy": 3.4022, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6996}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6833}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C3G%3A%20Learning%20Compact%203D%20Representations%20with%202K%20Gaussians&body=Title%3A%20C3G%3A%20Learning%20Compact%203D%20Representations%20with%202K%20Gaussians%0AAuthor%3A%20Honggyu%20An%20and%20Jaewoo%20Jung%20and%20Mungyeom%20Kim%20and%20Sunghwan%20Hong%20and%20Chaehyun%20Kim%20and%20Kazumi%20Fukuda%20and%20Minkyeong%20Jeon%20and%20Jisang%20Han%20and%20Takuya%20Narihira%20and%20Hyuna%20Ko%20and%20Junsu%20Kim%20and%20Yuki%20Mitsufuji%20and%20Seungryong%20Kim%0AAbstract%3A%20Reconstructing%20and%20understanding%203D%20scenes%20from%20unposed%20sparse%20views%20in%20a%20feed-forward%20manner%20remains%20as%20a%20challenging%20task%20in%203D%20computer%20vision.%20Recent%20approaches%20use%20per-pixel%203D%20Gaussian%20Splatting%20for%20reconstruction%2C%20followed%20by%20a%202D-to-3D%20feature%20lifting%20stage%20for%20scene%20understanding.%20However%2C%20they%20generate%20excessive%20redundant%20Gaussians%2C%20causing%20high%20memory%20overhead%20and%20sub-optimal%20multi-view%20feature%20aggregation%2C%20leading%20to%20degraded%20novel%20view%20synthesis%20and%20scene%20understanding%20performance.%20We%20propose%20C3G%2C%20a%20novel%20feed-forward%20framework%20that%20estimates%20compact%203D%20Gaussians%20only%20at%20essential%20spatial%20locations%2C%20minimizing%20redundancy%20while%20enabling%20effective%20feature%20lifting.%20We%20introduce%20learnable%20tokens%20that%20aggregate%20multi-view%20features%20through%20self-attention%20to%20guide%20Gaussian%20generation%2C%20ensuring%20each%20Gaussian%20integrates%20relevant%20visual%20features%20across%20views.%20We%20then%20exploit%20the%20learned%20attention%20patterns%20for%20Gaussian%20decoding%20to%20efficiently%20lift%20features.%20Extensive%20experiments%20on%20pose-free%20novel%20view%20synthesis%2C%203D%20open-vocabulary%20segmentation%2C%20and%20view-invariant%20feature%20aggregation%20demonstrate%20our%20approach%27s%20effectiveness.%20Results%20show%20that%20a%20compact%20yet%20geometrically%20meaningful%20representation%20is%20sufficient%20for%20high-quality%20scene%20reconstruction%20and%20understanding%2C%20achieving%20superior%20memory%20efficiency%20and%20feature%20fidelity%20compared%20to%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC3G%253A%2520Learning%2520Compact%25203D%2520Representations%2520with%25202K%2520Gaussians%26entry.906535625%3DHonggyu%2520An%2520and%2520Jaewoo%2520Jung%2520and%2520Mungyeom%2520Kim%2520and%2520Sunghwan%2520Hong%2520and%2520Chaehyun%2520Kim%2520and%2520Kazumi%2520Fukuda%2520and%2520Minkyeong%2520Jeon%2520and%2520Jisang%2520Han%2520and%2520Takuya%2520Narihira%2520and%2520Hyuna%2520Ko%2520and%2520Junsu%2520Kim%2520and%2520Yuki%2520Mitsufuji%2520and%2520Seungryong%2520Kim%26entry.1292438233%3DReconstructing%2520and%2520understanding%25203D%2520scenes%2520from%2520unposed%2520sparse%2520views%2520in%2520a%2520feed-forward%2520manner%2520remains%2520as%2520a%2520challenging%2520task%2520in%25203D%2520computer%2520vision.%2520Recent%2520approaches%2520use%2520per-pixel%25203D%2520Gaussian%2520Splatting%2520for%2520reconstruction%252C%2520followed%2520by%2520a%25202D-to-3D%2520feature%2520lifting%2520stage%2520for%2520scene%2520understanding.%2520However%252C%2520they%2520generate%2520excessive%2520redundant%2520Gaussians%252C%2520causing%2520high%2520memory%2520overhead%2520and%2520sub-optimal%2520multi-view%2520feature%2520aggregation%252C%2520leading%2520to%2520degraded%2520novel%2520view%2520synthesis%2520and%2520scene%2520understanding%2520performance.%2520We%2520propose%2520C3G%252C%2520a%2520novel%2520feed-forward%2520framework%2520that%2520estimates%2520compact%25203D%2520Gaussians%2520only%2520at%2520essential%2520spatial%2520locations%252C%2520minimizing%2520redundancy%2520while%2520enabling%2520effective%2520feature%2520lifting.%2520We%2520introduce%2520learnable%2520tokens%2520that%2520aggregate%2520multi-view%2520features%2520through%2520self-attention%2520to%2520guide%2520Gaussian%2520generation%252C%2520ensuring%2520each%2520Gaussian%2520integrates%2520relevant%2520visual%2520features%2520across%2520views.%2520We%2520then%2520exploit%2520the%2520learned%2520attention%2520patterns%2520for%2520Gaussian%2520decoding%2520to%2520efficiently%2520lift%2520features.%2520Extensive%2520experiments%2520on%2520pose-free%2520novel%2520view%2520synthesis%252C%25203D%2520open-vocabulary%2520segmentation%252C%2520and%2520view-invariant%2520feature%2520aggregation%2520demonstrate%2520our%2520approach%2527s%2520effectiveness.%2520Results%2520show%2520that%2520a%2520compact%2520yet%2520geometrically%2520meaningful%2520representation%2520is%2520sufficient%2520for%2520high-quality%2520scene%2520reconstruction%2520and%2520understanding%252C%2520achieving%2520superior%2520memory%2520efficiency%2520and%2520feature%2520fidelity%2520compared%2520to%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C3G%3A%20Learning%20Compact%203D%20Representations%20with%202K%20Gaussians&entry.906535625=Honggyu%20An%20and%20Jaewoo%20Jung%20and%20Mungyeom%20Kim%20and%20Sunghwan%20Hong%20and%20Chaehyun%20Kim%20and%20Kazumi%20Fukuda%20and%20Minkyeong%20Jeon%20and%20Jisang%20Han%20and%20Takuya%20Narihira%20and%20Hyuna%20Ko%20and%20Junsu%20Kim%20and%20Yuki%20Mitsufuji%20and%20Seungryong%20Kim&entry.1292438233=Reconstructing%20and%20understanding%203D%20scenes%20from%20unposed%20sparse%20views%20in%20a%20feed-forward%20manner%20remains%20as%20a%20challenging%20task%20in%203D%20computer%20vision.%20Recent%20approaches%20use%20per-pixel%203D%20Gaussian%20Splatting%20for%20reconstruction%2C%20followed%20by%20a%202D-to-3D%20feature%20lifting%20stage%20for%20scene%20understanding.%20However%2C%20they%20generate%20excessive%20redundant%20Gaussians%2C%20causing%20high%20memory%20overhead%20and%20sub-optimal%20multi-view%20feature%20aggregation%2C%20leading%20to%20degraded%20novel%20view%20synthesis%20and%20scene%20understanding%20performance.%20We%20propose%20C3G%2C%20a%20novel%20feed-forward%20framework%20that%20estimates%20compact%203D%20Gaussians%20only%20at%20essential%20spatial%20locations%2C%20minimizing%20redundancy%20while%20enabling%20effective%20feature%20lifting.%20We%20introduce%20learnable%20tokens%20that%20aggregate%20multi-view%20features%20through%20self-attention%20to%20guide%20Gaussian%20generation%2C%20ensuring%20each%20Gaussian%20integrates%20relevant%20visual%20features%20across%20views.%20We%20then%20exploit%20the%20learned%20attention%20patterns%20for%20Gaussian%20decoding%20to%20efficiently%20lift%20features.%20Extensive%20experiments%20on%20pose-free%20novel%20view%20synthesis%2C%203D%20open-vocabulary%20segmentation%2C%20and%20view-invariant%20feature%20aggregation%20demonstrate%20our%20approach%27s%20effectiveness.%20Results%20show%20that%20a%20compact%20yet%20geometrically%20meaningful%20representation%20is%20sufficient%20for%20high-quality%20scene%20reconstruction%20and%20understanding%2C%20achieving%20superior%20memory%20efficiency%20and%20feature%20fidelity%20compared%20to%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.04021v1&entry.124074799=Read"},
{"title": "OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance", "author": "Lei Zhang and Diwen Zheng and Kaixin Bai and Zhenshan Bing and Zoltan-Csaba Marton and Zhaopeng Chen and Alois Christian Knoll and Jianwei Zhang", "abstract": "Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.", "link": "http://arxiv.org/abs/2512.03874v1", "date": "2025-12-03", "relevancy": 3.271, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniDexVLG%3A%20Learning%20Dexterous%20Grasp%20Generation%20from%20Vision%20Language%20Model-Guided%20Grasp%20Semantics%2C%20Taxonomy%20and%20Functional%20Affordance&body=Title%3A%20OmniDexVLG%3A%20Learning%20Dexterous%20Grasp%20Generation%20from%20Vision%20Language%20Model-Guided%20Grasp%20Semantics%2C%20Taxonomy%20and%20Functional%20Affordance%0AAuthor%3A%20Lei%20Zhang%20and%20Diwen%20Zheng%20and%20Kaixin%20Bai%20and%20Zhenshan%20Bing%20and%20Zoltan-Csaba%20Marton%20and%20Zhaopeng%20Chen%20and%20Alois%20Christian%20Knoll%20and%20Jianwei%20Zhang%0AAbstract%3A%20Dexterous%20grasp%20generation%20aims%20to%20produce%20grasp%20poses%20that%20align%20with%20task%20requirements%20and%20human%20interpretable%20grasp%20semantics.%20However%2C%20achieving%20semantically%20controllable%20dexterous%20grasp%20synthesis%20remains%20highly%20challenging%20due%20to%20the%20lack%20of%20unified%20modeling%20of%20multiple%20semantic%20dimensions%2C%20including%20grasp%20taxonomy%2C%20contact%20semantics%2C%20and%20functional%20affordance.%20To%20address%20these%20limitations%2C%20we%20present%20OmniDexVLG%2C%20a%20multimodal%2C%20semantics%20aware%20grasp%20generation%20framework%20capable%20of%20producing%20structurally%20diverse%20and%20semantically%20coherent%20dexterous%20grasps%20under%20joint%20language%20and%20visual%20guidance.%20Our%20approach%20begins%20with%20OmniDexDataGen%2C%20a%20semantic%20rich%20dexterous%20grasp%20dataset%20generation%20pipeline%20that%20integrates%20grasp%20taxonomy%20guided%20configuration%20sampling%2C%20functional%20affordance%20contact%20point%20sampling%2C%20taxonomy%20aware%20differential%20force%20closure%20grasp%20sampling%2C%20and%20physics%20based%20optimization%20and%20validation%2C%20enabling%20systematic%20coverage%20of%20diverse%20grasp%20types.%20We%20further%20introduce%20OmniDexReasoner%2C%20a%20multimodal%20grasp%20type%20semantic%20reasoning%20module%20that%20leverages%20multi%20agent%20collaboration%2C%20retrieval%20augmented%20generation%2C%20and%20chain%20of%20thought%20reasoning%20to%20infer%20grasp%20related%20semantics%20and%20generate%20high%20quality%20annotations%20that%20align%20language%20instructions%20with%20task%20specific%20grasp%20intent.%20Building%20upon%20these%20components%2C%20we%20develop%20a%20unified%20Vision%20Language%20Grasping%20generation%20model%20that%20explicitly%20incorporates%20grasp%20taxonomy%2C%20contact%20structure%2C%20and%20functional%20affordance%20semantics%2C%20enabling%20fine%20grained%20control%20over%20grasp%20synthesis%20from%20natural%20language%20instructions.%20Extensive%20experiments%20in%20simulation%20and%20real%20world%20object%20grasping%20and%20ablation%20studies%20demonstrate%20that%20our%20method%20substantially%20outperforms%20state%20of%20the%20art%20approaches%20in%20terms%20of%20grasp%20diversity%2C%20contact%20semantic%20diversity%2C%20functional%20affordance%20diversity%2C%20and%20semantic%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniDexVLG%253A%2520Learning%2520Dexterous%2520Grasp%2520Generation%2520from%2520Vision%2520Language%2520Model-Guided%2520Grasp%2520Semantics%252C%2520Taxonomy%2520and%2520Functional%2520Affordance%26entry.906535625%3DLei%2520Zhang%2520and%2520Diwen%2520Zheng%2520and%2520Kaixin%2520Bai%2520and%2520Zhenshan%2520Bing%2520and%2520Zoltan-Csaba%2520Marton%2520and%2520Zhaopeng%2520Chen%2520and%2520Alois%2520Christian%2520Knoll%2520and%2520Jianwei%2520Zhang%26entry.1292438233%3DDexterous%2520grasp%2520generation%2520aims%2520to%2520produce%2520grasp%2520poses%2520that%2520align%2520with%2520task%2520requirements%2520and%2520human%2520interpretable%2520grasp%2520semantics.%2520However%252C%2520achieving%2520semantically%2520controllable%2520dexterous%2520grasp%2520synthesis%2520remains%2520highly%2520challenging%2520due%2520to%2520the%2520lack%2520of%2520unified%2520modeling%2520of%2520multiple%2520semantic%2520dimensions%252C%2520including%2520grasp%2520taxonomy%252C%2520contact%2520semantics%252C%2520and%2520functional%2520affordance.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520OmniDexVLG%252C%2520a%2520multimodal%252C%2520semantics%2520aware%2520grasp%2520generation%2520framework%2520capable%2520of%2520producing%2520structurally%2520diverse%2520and%2520semantically%2520coherent%2520dexterous%2520grasps%2520under%2520joint%2520language%2520and%2520visual%2520guidance.%2520Our%2520approach%2520begins%2520with%2520OmniDexDataGen%252C%2520a%2520semantic%2520rich%2520dexterous%2520grasp%2520dataset%2520generation%2520pipeline%2520that%2520integrates%2520grasp%2520taxonomy%2520guided%2520configuration%2520sampling%252C%2520functional%2520affordance%2520contact%2520point%2520sampling%252C%2520taxonomy%2520aware%2520differential%2520force%2520closure%2520grasp%2520sampling%252C%2520and%2520physics%2520based%2520optimization%2520and%2520validation%252C%2520enabling%2520systematic%2520coverage%2520of%2520diverse%2520grasp%2520types.%2520We%2520further%2520introduce%2520OmniDexReasoner%252C%2520a%2520multimodal%2520grasp%2520type%2520semantic%2520reasoning%2520module%2520that%2520leverages%2520multi%2520agent%2520collaboration%252C%2520retrieval%2520augmented%2520generation%252C%2520and%2520chain%2520of%2520thought%2520reasoning%2520to%2520infer%2520grasp%2520related%2520semantics%2520and%2520generate%2520high%2520quality%2520annotations%2520that%2520align%2520language%2520instructions%2520with%2520task%2520specific%2520grasp%2520intent.%2520Building%2520upon%2520these%2520components%252C%2520we%2520develop%2520a%2520unified%2520Vision%2520Language%2520Grasping%2520generation%2520model%2520that%2520explicitly%2520incorporates%2520grasp%2520taxonomy%252C%2520contact%2520structure%252C%2520and%2520functional%2520affordance%2520semantics%252C%2520enabling%2520fine%2520grained%2520control%2520over%2520grasp%2520synthesis%2520from%2520natural%2520language%2520instructions.%2520Extensive%2520experiments%2520in%2520simulation%2520and%2520real%2520world%2520object%2520grasping%2520and%2520ablation%2520studies%2520demonstrate%2520that%2520our%2520method%2520substantially%2520outperforms%2520state%2520of%2520the%2520art%2520approaches%2520in%2520terms%2520of%2520grasp%2520diversity%252C%2520contact%2520semantic%2520diversity%252C%2520functional%2520affordance%2520diversity%252C%2520and%2520semantic%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniDexVLG%3A%20Learning%20Dexterous%20Grasp%20Generation%20from%20Vision%20Language%20Model-Guided%20Grasp%20Semantics%2C%20Taxonomy%20and%20Functional%20Affordance&entry.906535625=Lei%20Zhang%20and%20Diwen%20Zheng%20and%20Kaixin%20Bai%20and%20Zhenshan%20Bing%20and%20Zoltan-Csaba%20Marton%20and%20Zhaopeng%20Chen%20and%20Alois%20Christian%20Knoll%20and%20Jianwei%20Zhang&entry.1292438233=Dexterous%20grasp%20generation%20aims%20to%20produce%20grasp%20poses%20that%20align%20with%20task%20requirements%20and%20human%20interpretable%20grasp%20semantics.%20However%2C%20achieving%20semantically%20controllable%20dexterous%20grasp%20synthesis%20remains%20highly%20challenging%20due%20to%20the%20lack%20of%20unified%20modeling%20of%20multiple%20semantic%20dimensions%2C%20including%20grasp%20taxonomy%2C%20contact%20semantics%2C%20and%20functional%20affordance.%20To%20address%20these%20limitations%2C%20we%20present%20OmniDexVLG%2C%20a%20multimodal%2C%20semantics%20aware%20grasp%20generation%20framework%20capable%20of%20producing%20structurally%20diverse%20and%20semantically%20coherent%20dexterous%20grasps%20under%20joint%20language%20and%20visual%20guidance.%20Our%20approach%20begins%20with%20OmniDexDataGen%2C%20a%20semantic%20rich%20dexterous%20grasp%20dataset%20generation%20pipeline%20that%20integrates%20grasp%20taxonomy%20guided%20configuration%20sampling%2C%20functional%20affordance%20contact%20point%20sampling%2C%20taxonomy%20aware%20differential%20force%20closure%20grasp%20sampling%2C%20and%20physics%20based%20optimization%20and%20validation%2C%20enabling%20systematic%20coverage%20of%20diverse%20grasp%20types.%20We%20further%20introduce%20OmniDexReasoner%2C%20a%20multimodal%20grasp%20type%20semantic%20reasoning%20module%20that%20leverages%20multi%20agent%20collaboration%2C%20retrieval%20augmented%20generation%2C%20and%20chain%20of%20thought%20reasoning%20to%20infer%20grasp%20related%20semantics%20and%20generate%20high%20quality%20annotations%20that%20align%20language%20instructions%20with%20task%20specific%20grasp%20intent.%20Building%20upon%20these%20components%2C%20we%20develop%20a%20unified%20Vision%20Language%20Grasping%20generation%20model%20that%20explicitly%20incorporates%20grasp%20taxonomy%2C%20contact%20structure%2C%20and%20functional%20affordance%20semantics%2C%20enabling%20fine%20grained%20control%20over%20grasp%20synthesis%20from%20natural%20language%20instructions.%20Extensive%20experiments%20in%20simulation%20and%20real%20world%20object%20grasping%20and%20ablation%20studies%20demonstrate%20that%20our%20method%20substantially%20outperforms%20state%20of%20the%20art%20approaches%20in%20terms%20of%20grasp%20diversity%2C%20contact%20semantic%20diversity%2C%20functional%20affordance%20diversity%2C%20and%20semantic%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2512.03874v1&entry.124074799=Read"},
{"title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces", "author": "Melis Ocal and Xiaoyan Xing and Yue Li and Ngo Anh Vien and Sezer Karaoglu and Theo Gevers", "abstract": "3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.", "link": "http://arxiv.org/abs/2512.03683v1", "date": "2025-12-03", "relevancy": 3.2047, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6426}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6426}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianBlender%3A%20Instant%20Stylization%20of%203D%20Gaussians%20with%20Disentangled%20Latent%20Spaces&body=Title%3A%20GaussianBlender%3A%20Instant%20Stylization%20of%203D%20Gaussians%20with%20Disentangled%20Latent%20Spaces%0AAuthor%3A%20Melis%20Ocal%20and%20Xiaoyan%20Xing%20and%20Yue%20Li%20and%20Ngo%20Anh%20Vien%20and%20Sezer%20Karaoglu%20and%20Theo%20Gevers%0AAbstract%3A%203D%20stylization%20is%20central%20to%20game%20development%2C%20virtual%20reality%2C%20and%20digital%20arts%2C%20where%20the%20demand%20for%20diverse%20assets%20calls%20for%20scalable%20methods%20that%20support%20fast%2C%20high-fidelity%20manipulation.%20Existing%20text-to-3D%20stylization%20methods%20typically%20distill%20from%202D%20image%20editors%2C%20requiring%20time-intensive%20per-asset%20optimization%20and%20exhibiting%20multi-view%20inconsistency%20due%20to%20the%20limitations%20of%20current%20text-to-image%20models%2C%20which%20makes%20them%20impractical%20for%20large-scale%20production.%20In%20this%20paper%2C%20we%20introduce%20GaussianBlender%2C%20a%20pioneering%20feed-forward%20framework%20for%20text-driven%203D%20stylization%20that%20performs%20edits%20instantly%20at%20inference.%20Our%20method%20learns%20structured%2C%20disentangled%20latent%20spaces%20with%20controlled%20information%20sharing%20for%20geometry%20and%20appearance%20from%20spatially-grouped%203D%20Gaussians.%20A%20latent%20diffusion%20model%20then%20applies%20text-conditioned%20edits%20on%20these%20learned%20representations.%20Comprehensive%20evaluations%20show%20that%20GaussianBlender%20not%20only%20delivers%20instant%2C%20high-fidelity%2C%20geometry-preserving%2C%20multi-view%20consistent%20stylization%2C%20but%20also%20surpasses%20methods%20that%20require%20per-instance%20test-time%20optimization%20-%20unlocking%20practical%2C%20democratized%203D%20stylization%20at%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianBlender%253A%2520Instant%2520Stylization%2520of%25203D%2520Gaussians%2520with%2520Disentangled%2520Latent%2520Spaces%26entry.906535625%3DMelis%2520Ocal%2520and%2520Xiaoyan%2520Xing%2520and%2520Yue%2520Li%2520and%2520Ngo%2520Anh%2520Vien%2520and%2520Sezer%2520Karaoglu%2520and%2520Theo%2520Gevers%26entry.1292438233%3D3D%2520stylization%2520is%2520central%2520to%2520game%2520development%252C%2520virtual%2520reality%252C%2520and%2520digital%2520arts%252C%2520where%2520the%2520demand%2520for%2520diverse%2520assets%2520calls%2520for%2520scalable%2520methods%2520that%2520support%2520fast%252C%2520high-fidelity%2520manipulation.%2520Existing%2520text-to-3D%2520stylization%2520methods%2520typically%2520distill%2520from%25202D%2520image%2520editors%252C%2520requiring%2520time-intensive%2520per-asset%2520optimization%2520and%2520exhibiting%2520multi-view%2520inconsistency%2520due%2520to%2520the%2520limitations%2520of%2520current%2520text-to-image%2520models%252C%2520which%2520makes%2520them%2520impractical%2520for%2520large-scale%2520production.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GaussianBlender%252C%2520a%2520pioneering%2520feed-forward%2520framework%2520for%2520text-driven%25203D%2520stylization%2520that%2520performs%2520edits%2520instantly%2520at%2520inference.%2520Our%2520method%2520learns%2520structured%252C%2520disentangled%2520latent%2520spaces%2520with%2520controlled%2520information%2520sharing%2520for%2520geometry%2520and%2520appearance%2520from%2520spatially-grouped%25203D%2520Gaussians.%2520A%2520latent%2520diffusion%2520model%2520then%2520applies%2520text-conditioned%2520edits%2520on%2520these%2520learned%2520representations.%2520Comprehensive%2520evaluations%2520show%2520that%2520GaussianBlender%2520not%2520only%2520delivers%2520instant%252C%2520high-fidelity%252C%2520geometry-preserving%252C%2520multi-view%2520consistent%2520stylization%252C%2520but%2520also%2520surpasses%2520methods%2520that%2520require%2520per-instance%2520test-time%2520optimization%2520-%2520unlocking%2520practical%252C%2520democratized%25203D%2520stylization%2520at%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianBlender%3A%20Instant%20Stylization%20of%203D%20Gaussians%20with%20Disentangled%20Latent%20Spaces&entry.906535625=Melis%20Ocal%20and%20Xiaoyan%20Xing%20and%20Yue%20Li%20and%20Ngo%20Anh%20Vien%20and%20Sezer%20Karaoglu%20and%20Theo%20Gevers&entry.1292438233=3D%20stylization%20is%20central%20to%20game%20development%2C%20virtual%20reality%2C%20and%20digital%20arts%2C%20where%20the%20demand%20for%20diverse%20assets%20calls%20for%20scalable%20methods%20that%20support%20fast%2C%20high-fidelity%20manipulation.%20Existing%20text-to-3D%20stylization%20methods%20typically%20distill%20from%202D%20image%20editors%2C%20requiring%20time-intensive%20per-asset%20optimization%20and%20exhibiting%20multi-view%20inconsistency%20due%20to%20the%20limitations%20of%20current%20text-to-image%20models%2C%20which%20makes%20them%20impractical%20for%20large-scale%20production.%20In%20this%20paper%2C%20we%20introduce%20GaussianBlender%2C%20a%20pioneering%20feed-forward%20framework%20for%20text-driven%203D%20stylization%20that%20performs%20edits%20instantly%20at%20inference.%20Our%20method%20learns%20structured%2C%20disentangled%20latent%20spaces%20with%20controlled%20information%20sharing%20for%20geometry%20and%20appearance%20from%20spatially-grouped%203D%20Gaussians.%20A%20latent%20diffusion%20model%20then%20applies%20text-conditioned%20edits%20on%20these%20learned%20representations.%20Comprehensive%20evaluations%20show%20that%20GaussianBlender%20not%20only%20delivers%20instant%2C%20high-fidelity%2C%20geometry-preserving%2C%20multi-view%20consistent%20stylization%2C%20but%20also%20surpasses%20methods%20that%20require%20per-instance%20test-time%20optimization%20-%20unlocking%20practical%2C%20democratized%203D%20stylization%20at%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2512.03683v1&entry.124074799=Read"},
{"title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images", "author": "Zirun Guo and Minjie Hong and Feng Zhang and Kai Jia and Tao Jin", "abstract": "Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.", "link": "http://arxiv.org/abs/2512.03746v1", "date": "2025-12-03", "relevancy": 3.0736, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6257}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6257}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20with%20Programming%20Vision%3A%20Towards%20a%20Unified%20View%20for%20Thinking%20with%20Images&body=Title%3A%20Thinking%20with%20Programming%20Vision%3A%20Towards%20a%20Unified%20View%20for%20Thinking%20with%20Images%0AAuthor%3A%20Zirun%20Guo%20and%20Minjie%20Hong%20and%20Feng%20Zhang%20and%20Kai%20Jia%20and%20Tao%20Jin%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20that%20think%20with%20images%20can%20interactively%20use%20tools%20to%20reason%20about%20visual%20inputs%2C%20but%20current%20approaches%20often%20rely%20on%20a%20narrow%20set%20of%20tools%20with%20limited%20real-world%20necessity%20and%20scalability.%20In%20this%20work%2C%20we%20first%20reveal%20a%20critical%20and%20previously%20overlooked%20weakness%3A%20even%20state-of-the-art%20MLLMs%20are%20surprisingly%20brittle%2C%20showing%20significant%20performance%20degradation%20on%20images%20with%20simple%20orientation%20changes%20or%20natural%20corruptions%2C%20underscoring%20the%20need%20for%20more%20robust%20tool-based%20reasoning.%20To%20address%20this%2C%20we%20propose%20CodeVision%2C%20a%20flexible%20and%20scalable%20code-as-tool%20framework%20where%20the%20model%20generates%20code%20as%20a%20universal%20interface%20to%20invoke%20any%20image%20operation%2C%20moving%20beyond%20fixed%20tool%20registries.%20We%20train%20our%20model%20using%20a%20two-stage%20methodology%2C%20beginning%20with%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20a%20high-quality%20dataset%20curated%20for%20complex%2C%20multi-turn%20tool%20composition%20and%20error%20recovery%2C%20followed%20by%20Reinforcement%20Learning%20%28RL%29%20with%20a%20novel%20and%20dense%20process%20reward%20function%20to%20encourage%20strategic%20and%20efficient%20tool%20use.%20To%20facilitate%20this%20research%2C%20we%20construct%20new%20SFT%20and%20RL%20datasets%20and%20introduce%20a%20challenging%20new%20benchmark%20suite%20designed%20to%20rigorously%20evaluate%20robustness%20to%20orientation%20changes%20and%20multi-tool%20reasoning.%20Experiments%20on%20Qwen2.5-VL%20and%20Qwen3-VL%20series%20show%20that%20our%20approach%20significantly%20improves%20model%20performance%20and%20fosters%20emergent%20capabilities%20such%20as%20flexible%20tool%20composition%2C%20efficient%20chained%20execution%2C%20and%20robust%20error%20recovery%20from%20runtime%20feedback.%20Code%20is%20available%20at%20https%3A//github.com/ByteDance-BandAI/CodeVision.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520with%2520Programming%2520Vision%253A%2520Towards%2520a%2520Unified%2520View%2520for%2520Thinking%2520with%2520Images%26entry.906535625%3DZirun%2520Guo%2520and%2520Minjie%2520Hong%2520and%2520Feng%2520Zhang%2520and%2520Kai%2520Jia%2520and%2520Tao%2520Jin%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520that%2520think%2520with%2520images%2520can%2520interactively%2520use%2520tools%2520to%2520reason%2520about%2520visual%2520inputs%252C%2520but%2520current%2520approaches%2520often%2520rely%2520on%2520a%2520narrow%2520set%2520of%2520tools%2520with%2520limited%2520real-world%2520necessity%2520and%2520scalability.%2520In%2520this%2520work%252C%2520we%2520first%2520reveal%2520a%2520critical%2520and%2520previously%2520overlooked%2520weakness%253A%2520even%2520state-of-the-art%2520MLLMs%2520are%2520surprisingly%2520brittle%252C%2520showing%2520significant%2520performance%2520degradation%2520on%2520images%2520with%2520simple%2520orientation%2520changes%2520or%2520natural%2520corruptions%252C%2520underscoring%2520the%2520need%2520for%2520more%2520robust%2520tool-based%2520reasoning.%2520To%2520address%2520this%252C%2520we%2520propose%2520CodeVision%252C%2520a%2520flexible%2520and%2520scalable%2520code-as-tool%2520framework%2520where%2520the%2520model%2520generates%2520code%2520as%2520a%2520universal%2520interface%2520to%2520invoke%2520any%2520image%2520operation%252C%2520moving%2520beyond%2520fixed%2520tool%2520registries.%2520We%2520train%2520our%2520model%2520using%2520a%2520two-stage%2520methodology%252C%2520beginning%2520with%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520on%2520a%2520high-quality%2520dataset%2520curated%2520for%2520complex%252C%2520multi-turn%2520tool%2520composition%2520and%2520error%2520recovery%252C%2520followed%2520by%2520Reinforcement%2520Learning%2520%2528RL%2529%2520with%2520a%2520novel%2520and%2520dense%2520process%2520reward%2520function%2520to%2520encourage%2520strategic%2520and%2520efficient%2520tool%2520use.%2520To%2520facilitate%2520this%2520research%252C%2520we%2520construct%2520new%2520SFT%2520and%2520RL%2520datasets%2520and%2520introduce%2520a%2520challenging%2520new%2520benchmark%2520suite%2520designed%2520to%2520rigorously%2520evaluate%2520robustness%2520to%2520orientation%2520changes%2520and%2520multi-tool%2520reasoning.%2520Experiments%2520on%2520Qwen2.5-VL%2520and%2520Qwen3-VL%2520series%2520show%2520that%2520our%2520approach%2520significantly%2520improves%2520model%2520performance%2520and%2520fosters%2520emergent%2520capabilities%2520such%2520as%2520flexible%2520tool%2520composition%252C%2520efficient%2520chained%2520execution%252C%2520and%2520robust%2520error%2520recovery%2520from%2520runtime%2520feedback.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/ByteDance-BandAI/CodeVision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20with%20Programming%20Vision%3A%20Towards%20a%20Unified%20View%20for%20Thinking%20with%20Images&entry.906535625=Zirun%20Guo%20and%20Minjie%20Hong%20and%20Feng%20Zhang%20and%20Kai%20Jia%20and%20Tao%20Jin&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20that%20think%20with%20images%20can%20interactively%20use%20tools%20to%20reason%20about%20visual%20inputs%2C%20but%20current%20approaches%20often%20rely%20on%20a%20narrow%20set%20of%20tools%20with%20limited%20real-world%20necessity%20and%20scalability.%20In%20this%20work%2C%20we%20first%20reveal%20a%20critical%20and%20previously%20overlooked%20weakness%3A%20even%20state-of-the-art%20MLLMs%20are%20surprisingly%20brittle%2C%20showing%20significant%20performance%20degradation%20on%20images%20with%20simple%20orientation%20changes%20or%20natural%20corruptions%2C%20underscoring%20the%20need%20for%20more%20robust%20tool-based%20reasoning.%20To%20address%20this%2C%20we%20propose%20CodeVision%2C%20a%20flexible%20and%20scalable%20code-as-tool%20framework%20where%20the%20model%20generates%20code%20as%20a%20universal%20interface%20to%20invoke%20any%20image%20operation%2C%20moving%20beyond%20fixed%20tool%20registries.%20We%20train%20our%20model%20using%20a%20two-stage%20methodology%2C%20beginning%20with%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20a%20high-quality%20dataset%20curated%20for%20complex%2C%20multi-turn%20tool%20composition%20and%20error%20recovery%2C%20followed%20by%20Reinforcement%20Learning%20%28RL%29%20with%20a%20novel%20and%20dense%20process%20reward%20function%20to%20encourage%20strategic%20and%20efficient%20tool%20use.%20To%20facilitate%20this%20research%2C%20we%20construct%20new%20SFT%20and%20RL%20datasets%20and%20introduce%20a%20challenging%20new%20benchmark%20suite%20designed%20to%20rigorously%20evaluate%20robustness%20to%20orientation%20changes%20and%20multi-tool%20reasoning.%20Experiments%20on%20Qwen2.5-VL%20and%20Qwen3-VL%20series%20show%20that%20our%20approach%20significantly%20improves%20model%20performance%20and%20fosters%20emergent%20capabilities%20such%20as%20flexible%20tool%20composition%2C%20efficient%20chained%20execution%2C%20and%20robust%20error%20recovery%20from%20runtime%20feedback.%20Code%20is%20available%20at%20https%3A//github.com/ByteDance-BandAI/CodeVision.&entry.1838667208=http%3A//arxiv.org/abs/2512.03746v1&entry.124074799=Read"},
{"title": "Optical Context Compression Is Just (Bad) Autoencoding", "author": "Ivan Yee Lee and Cheng Yang and Taylor Berg-Kirkpatrick", "abstract": "DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding", "link": "http://arxiv.org/abs/2512.03643v1", "date": "2025-12-03", "relevancy": 3.0017, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6236}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optical%20Context%20Compression%20Is%20Just%20%28Bad%29%20Autoencoding&body=Title%3A%20Optical%20Context%20Compression%20Is%20Just%20%28Bad%29%20Autoencoding%0AAuthor%3A%20Ivan%20Yee%20Lee%20and%20Cheng%20Yang%20and%20Taylor%20Berg-Kirkpatrick%0AAbstract%3A%20DeepSeek-OCR%20demonstrates%20that%20rendered%20text%20can%20be%20reconstructed%20with%20high%20fidelity%20from%20a%20small%20number%20of%20vision%20tokens.%20This%20finding%20has%20sparked%20excitement%20about%20vision-based%20context%20compression%20for%20language%20models.%20But%20the%20evaluation%20stops%20at%20reconstruction%3B%20whether%20these%20representations%20help%20language%20modeling%20remains%20untested.%20We%20test%20two%20assumptions%20implicit%20in%20the%20optical-compression%20narrative%3A%20that%20vision-based%20compression%20provides%20unique%20advantages%20for%20text%20reconstruction%20from%20compressed%20representations%2C%20and%20that%20DeepSeek-OCR%27s%20reconstruction%20results%20are%20evidence%20that%20vision-based%20compression%20will%20be%20useful%20for%20language%20modeling.%20Comparing%20their%20vision%20encoder%20against%20simple%20alternatives--parameter-free%20mean%20pooling%20and%20a%20learned%20hierarchical%20encoder--we%20find%20that%20these%20simple%20approaches%20match%20or%20surpass%20vision%20for%20reconstruction%20at%20matched%20compression%20ratios%2C%20and%20outperform%20it%20for%20language%20modeling--where%20vision-based%20compression%20fails%20to%20beat%20truncation.%20The%20excitement%20around%20optical%20context%20compression%20outpaces%20the%20evidence.%20Code%20and%20checkpoints%20are%20available%20at%20https%3A//github.com/ivnle/bad-autoencoding%0ALink%3A%20http%3A//arxiv.org/abs/2512.03643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptical%2520Context%2520Compression%2520Is%2520Just%2520%2528Bad%2529%2520Autoencoding%26entry.906535625%3DIvan%2520Yee%2520Lee%2520and%2520Cheng%2520Yang%2520and%2520Taylor%2520Berg-Kirkpatrick%26entry.1292438233%3DDeepSeek-OCR%2520demonstrates%2520that%2520rendered%2520text%2520can%2520be%2520reconstructed%2520with%2520high%2520fidelity%2520from%2520a%2520small%2520number%2520of%2520vision%2520tokens.%2520This%2520finding%2520has%2520sparked%2520excitement%2520about%2520vision-based%2520context%2520compression%2520for%2520language%2520models.%2520But%2520the%2520evaluation%2520stops%2520at%2520reconstruction%253B%2520whether%2520these%2520representations%2520help%2520language%2520modeling%2520remains%2520untested.%2520We%2520test%2520two%2520assumptions%2520implicit%2520in%2520the%2520optical-compression%2520narrative%253A%2520that%2520vision-based%2520compression%2520provides%2520unique%2520advantages%2520for%2520text%2520reconstruction%2520from%2520compressed%2520representations%252C%2520and%2520that%2520DeepSeek-OCR%2527s%2520reconstruction%2520results%2520are%2520evidence%2520that%2520vision-based%2520compression%2520will%2520be%2520useful%2520for%2520language%2520modeling.%2520Comparing%2520their%2520vision%2520encoder%2520against%2520simple%2520alternatives--parameter-free%2520mean%2520pooling%2520and%2520a%2520learned%2520hierarchical%2520encoder--we%2520find%2520that%2520these%2520simple%2520approaches%2520match%2520or%2520surpass%2520vision%2520for%2520reconstruction%2520at%2520matched%2520compression%2520ratios%252C%2520and%2520outperform%2520it%2520for%2520language%2520modeling--where%2520vision-based%2520compression%2520fails%2520to%2520beat%2520truncation.%2520The%2520excitement%2520around%2520optical%2520context%2520compression%2520outpaces%2520the%2520evidence.%2520Code%2520and%2520checkpoints%2520are%2520available%2520at%2520https%253A//github.com/ivnle/bad-autoencoding%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optical%20Context%20Compression%20Is%20Just%20%28Bad%29%20Autoencoding&entry.906535625=Ivan%20Yee%20Lee%20and%20Cheng%20Yang%20and%20Taylor%20Berg-Kirkpatrick&entry.1292438233=DeepSeek-OCR%20demonstrates%20that%20rendered%20text%20can%20be%20reconstructed%20with%20high%20fidelity%20from%20a%20small%20number%20of%20vision%20tokens.%20This%20finding%20has%20sparked%20excitement%20about%20vision-based%20context%20compression%20for%20language%20models.%20But%20the%20evaluation%20stops%20at%20reconstruction%3B%20whether%20these%20representations%20help%20language%20modeling%20remains%20untested.%20We%20test%20two%20assumptions%20implicit%20in%20the%20optical-compression%20narrative%3A%20that%20vision-based%20compression%20provides%20unique%20advantages%20for%20text%20reconstruction%20from%20compressed%20representations%2C%20and%20that%20DeepSeek-OCR%27s%20reconstruction%20results%20are%20evidence%20that%20vision-based%20compression%20will%20be%20useful%20for%20language%20modeling.%20Comparing%20their%20vision%20encoder%20against%20simple%20alternatives--parameter-free%20mean%20pooling%20and%20a%20learned%20hierarchical%20encoder--we%20find%20that%20these%20simple%20approaches%20match%20or%20surpass%20vision%20for%20reconstruction%20at%20matched%20compression%20ratios%2C%20and%20outperform%20it%20for%20language%20modeling--where%20vision-based%20compression%20fails%20to%20beat%20truncation.%20The%20excitement%20around%20optical%20context%20compression%20outpaces%20the%20evidence.%20Code%20and%20checkpoints%20are%20available%20at%20https%3A//github.com/ivnle/bad-autoencoding&entry.1838667208=http%3A//arxiv.org/abs/2512.03643v1&entry.124074799=Read"},
{"title": "GT23D-Bench: A Comprehensive General Text-to-3D Generation Benchmark", "author": "Xiao Cai and Sitong Su and Jingkuan Song and Pengpeng Zeng and Ji Zhang and Qinhong Du and Mengqi Li and Heng Tao Shen and Lianli Gao", "abstract": "Text-to-3D (T23D) generation has emerged as a crucial visual generation task, aiming at synthesizing 3D content from textual descriptions. Studies of this task are currently shifting from per-scene T23D, which requires optimization of the model for every content generated, to General T23D (GT23D), which requires only one pre-trained model to generate different content without re-optimization, for more generalized and efficient 3D generation. Despite notable advancements, GT23D is severely bottlenecked by two interconnected challenges: the lack of high-quality, large-scale training data and the prevalence of evaluation metrics that overlook intrinsic 3D properties. Existing datasets often suffer from incomplete annotations, noisy organization, and inconsistent quality, while current evaluations rely heavily on 2D image-text similarity or scoring, failing to thoroughly assess 3D geometric integrity and semantic relevance. To address these fundamental gaps, we introduce GT23D-Bench, the first comprehensive benchmark specifically designed for GT23D training and evaluation. We first construct a high-quality dataset of 400K 3D assets, featuring diverse visual annotations (70M+ visual samples) and multi-granularity hierarchical captions (1M+ descriptions) to foster robust semantic learning. Second, we propose a comprehensive evaluation suite with 10 metrics assessing both text-3D alignment and 3D visual quality at multiple levels. Crucially, we demonstrate through rigorous experiments that our proposed metrics exhibit significantly higher correlation with human judgment compared to existing methods. Our in-depth analysis of eight leading GT23D models using this benchmark provides the community with critical insights into current model capabilities and their shared failure modes. GT23D-Bench will be publicly available to facilitate rigorous and reproducible research.", "link": "http://arxiv.org/abs/2412.09997v2", "date": "2025-12-03", "relevancy": 2.9701, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5968}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5926}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GT23D-Bench%3A%20A%20Comprehensive%20General%20Text-to-3D%20Generation%20Benchmark&body=Title%3A%20GT23D-Bench%3A%20A%20Comprehensive%20General%20Text-to-3D%20Generation%20Benchmark%0AAuthor%3A%20Xiao%20Cai%20and%20Sitong%20Su%20and%20Jingkuan%20Song%20and%20Pengpeng%20Zeng%20and%20Ji%20Zhang%20and%20Qinhong%20Du%20and%20Mengqi%20Li%20and%20Heng%20Tao%20Shen%20and%20Lianli%20Gao%0AAbstract%3A%20Text-to-3D%20%28T23D%29%20generation%20has%20emerged%20as%20a%20crucial%20visual%20generation%20task%2C%20aiming%20at%20synthesizing%203D%20content%20from%20textual%20descriptions.%20Studies%20of%20this%20task%20are%20currently%20shifting%20from%20per-scene%20T23D%2C%20which%20requires%20optimization%20of%20the%20model%20for%20every%20content%20generated%2C%20to%20General%20T23D%20%28GT23D%29%2C%20which%20requires%20only%20one%20pre-trained%20model%20to%20generate%20different%20content%20without%20re-optimization%2C%20for%20more%20generalized%20and%20efficient%203D%20generation.%20Despite%20notable%20advancements%2C%20GT23D%20is%20severely%20bottlenecked%20by%20two%20interconnected%20challenges%3A%20the%20lack%20of%20high-quality%2C%20large-scale%20training%20data%20and%20the%20prevalence%20of%20evaluation%20metrics%20that%20overlook%20intrinsic%203D%20properties.%20Existing%20datasets%20often%20suffer%20from%20incomplete%20annotations%2C%20noisy%20organization%2C%20and%20inconsistent%20quality%2C%20while%20current%20evaluations%20rely%20heavily%20on%202D%20image-text%20similarity%20or%20scoring%2C%20failing%20to%20thoroughly%20assess%203D%20geometric%20integrity%20and%20semantic%20relevance.%20To%20address%20these%20fundamental%20gaps%2C%20we%20introduce%20GT23D-Bench%2C%20the%20first%20comprehensive%20benchmark%20specifically%20designed%20for%20GT23D%20training%20and%20evaluation.%20We%20first%20construct%20a%20high-quality%20dataset%20of%20400K%203D%20assets%2C%20featuring%20diverse%20visual%20annotations%20%2870M%2B%20visual%20samples%29%20and%20multi-granularity%20hierarchical%20captions%20%281M%2B%20descriptions%29%20to%20foster%20robust%20semantic%20learning.%20Second%2C%20we%20propose%20a%20comprehensive%20evaluation%20suite%20with%2010%20metrics%20assessing%20both%20text-3D%20alignment%20and%203D%20visual%20quality%20at%20multiple%20levels.%20Crucially%2C%20we%20demonstrate%20through%20rigorous%20experiments%20that%20our%20proposed%20metrics%20exhibit%20significantly%20higher%20correlation%20with%20human%20judgment%20compared%20to%20existing%20methods.%20Our%20in-depth%20analysis%20of%20eight%20leading%20GT23D%20models%20using%20this%20benchmark%20provides%20the%20community%20with%20critical%20insights%20into%20current%20model%20capabilities%20and%20their%20shared%20failure%20modes.%20GT23D-Bench%20will%20be%20publicly%20available%20to%20facilitate%20rigorous%20and%20reproducible%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2412.09997v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGT23D-Bench%253A%2520A%2520Comprehensive%2520General%2520Text-to-3D%2520Generation%2520Benchmark%26entry.906535625%3DXiao%2520Cai%2520and%2520Sitong%2520Su%2520and%2520Jingkuan%2520Song%2520and%2520Pengpeng%2520Zeng%2520and%2520Ji%2520Zhang%2520and%2520Qinhong%2520Du%2520and%2520Mengqi%2520Li%2520and%2520Heng%2520Tao%2520Shen%2520and%2520Lianli%2520Gao%26entry.1292438233%3DText-to-3D%2520%2528T23D%2529%2520generation%2520has%2520emerged%2520as%2520a%2520crucial%2520visual%2520generation%2520task%252C%2520aiming%2520at%2520synthesizing%25203D%2520content%2520from%2520textual%2520descriptions.%2520Studies%2520of%2520this%2520task%2520are%2520currently%2520shifting%2520from%2520per-scene%2520T23D%252C%2520which%2520requires%2520optimization%2520of%2520the%2520model%2520for%2520every%2520content%2520generated%252C%2520to%2520General%2520T23D%2520%2528GT23D%2529%252C%2520which%2520requires%2520only%2520one%2520pre-trained%2520model%2520to%2520generate%2520different%2520content%2520without%2520re-optimization%252C%2520for%2520more%2520generalized%2520and%2520efficient%25203D%2520generation.%2520Despite%2520notable%2520advancements%252C%2520GT23D%2520is%2520severely%2520bottlenecked%2520by%2520two%2520interconnected%2520challenges%253A%2520the%2520lack%2520of%2520high-quality%252C%2520large-scale%2520training%2520data%2520and%2520the%2520prevalence%2520of%2520evaluation%2520metrics%2520that%2520overlook%2520intrinsic%25203D%2520properties.%2520Existing%2520datasets%2520often%2520suffer%2520from%2520incomplete%2520annotations%252C%2520noisy%2520organization%252C%2520and%2520inconsistent%2520quality%252C%2520while%2520current%2520evaluations%2520rely%2520heavily%2520on%25202D%2520image-text%2520similarity%2520or%2520scoring%252C%2520failing%2520to%2520thoroughly%2520assess%25203D%2520geometric%2520integrity%2520and%2520semantic%2520relevance.%2520To%2520address%2520these%2520fundamental%2520gaps%252C%2520we%2520introduce%2520GT23D-Bench%252C%2520the%2520first%2520comprehensive%2520benchmark%2520specifically%2520designed%2520for%2520GT23D%2520training%2520and%2520evaluation.%2520We%2520first%2520construct%2520a%2520high-quality%2520dataset%2520of%2520400K%25203D%2520assets%252C%2520featuring%2520diverse%2520visual%2520annotations%2520%252870M%252B%2520visual%2520samples%2529%2520and%2520multi-granularity%2520hierarchical%2520captions%2520%25281M%252B%2520descriptions%2529%2520to%2520foster%2520robust%2520semantic%2520learning.%2520Second%252C%2520we%2520propose%2520a%2520comprehensive%2520evaluation%2520suite%2520with%252010%2520metrics%2520assessing%2520both%2520text-3D%2520alignment%2520and%25203D%2520visual%2520quality%2520at%2520multiple%2520levels.%2520Crucially%252C%2520we%2520demonstrate%2520through%2520rigorous%2520experiments%2520that%2520our%2520proposed%2520metrics%2520exhibit%2520significantly%2520higher%2520correlation%2520with%2520human%2520judgment%2520compared%2520to%2520existing%2520methods.%2520Our%2520in-depth%2520analysis%2520of%2520eight%2520leading%2520GT23D%2520models%2520using%2520this%2520benchmark%2520provides%2520the%2520community%2520with%2520critical%2520insights%2520into%2520current%2520model%2520capabilities%2520and%2520their%2520shared%2520failure%2520modes.%2520GT23D-Bench%2520will%2520be%2520publicly%2520available%2520to%2520facilitate%2520rigorous%2520and%2520reproducible%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09997v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GT23D-Bench%3A%20A%20Comprehensive%20General%20Text-to-3D%20Generation%20Benchmark&entry.906535625=Xiao%20Cai%20and%20Sitong%20Su%20and%20Jingkuan%20Song%20and%20Pengpeng%20Zeng%20and%20Ji%20Zhang%20and%20Qinhong%20Du%20and%20Mengqi%20Li%20and%20Heng%20Tao%20Shen%20and%20Lianli%20Gao&entry.1292438233=Text-to-3D%20%28T23D%29%20generation%20has%20emerged%20as%20a%20crucial%20visual%20generation%20task%2C%20aiming%20at%20synthesizing%203D%20content%20from%20textual%20descriptions.%20Studies%20of%20this%20task%20are%20currently%20shifting%20from%20per-scene%20T23D%2C%20which%20requires%20optimization%20of%20the%20model%20for%20every%20content%20generated%2C%20to%20General%20T23D%20%28GT23D%29%2C%20which%20requires%20only%20one%20pre-trained%20model%20to%20generate%20different%20content%20without%20re-optimization%2C%20for%20more%20generalized%20and%20efficient%203D%20generation.%20Despite%20notable%20advancements%2C%20GT23D%20is%20severely%20bottlenecked%20by%20two%20interconnected%20challenges%3A%20the%20lack%20of%20high-quality%2C%20large-scale%20training%20data%20and%20the%20prevalence%20of%20evaluation%20metrics%20that%20overlook%20intrinsic%203D%20properties.%20Existing%20datasets%20often%20suffer%20from%20incomplete%20annotations%2C%20noisy%20organization%2C%20and%20inconsistent%20quality%2C%20while%20current%20evaluations%20rely%20heavily%20on%202D%20image-text%20similarity%20or%20scoring%2C%20failing%20to%20thoroughly%20assess%203D%20geometric%20integrity%20and%20semantic%20relevance.%20To%20address%20these%20fundamental%20gaps%2C%20we%20introduce%20GT23D-Bench%2C%20the%20first%20comprehensive%20benchmark%20specifically%20designed%20for%20GT23D%20training%20and%20evaluation.%20We%20first%20construct%20a%20high-quality%20dataset%20of%20400K%203D%20assets%2C%20featuring%20diverse%20visual%20annotations%20%2870M%2B%20visual%20samples%29%20and%20multi-granularity%20hierarchical%20captions%20%281M%2B%20descriptions%29%20to%20foster%20robust%20semantic%20learning.%20Second%2C%20we%20propose%20a%20comprehensive%20evaluation%20suite%20with%2010%20metrics%20assessing%20both%20text-3D%20alignment%20and%203D%20visual%20quality%20at%20multiple%20levels.%20Crucially%2C%20we%20demonstrate%20through%20rigorous%20experiments%20that%20our%20proposed%20metrics%20exhibit%20significantly%20higher%20correlation%20with%20human%20judgment%20compared%20to%20existing%20methods.%20Our%20in-depth%20analysis%20of%20eight%20leading%20GT23D%20models%20using%20this%20benchmark%20provides%20the%20community%20with%20critical%20insights%20into%20current%20model%20capabilities%20and%20their%20shared%20failure%20modes.%20GT23D-Bench%20will%20be%20publicly%20available%20to%20facilitate%20rigorous%20and%20reproducible%20research.&entry.1838667208=http%3A//arxiv.org/abs/2412.09997v2&entry.124074799=Read"},
{"title": "STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control", "author": "Zhen Li and Xibin Jin and Guoliang Li and Shuai Wang and Miaowen Wen and Huseyin Arslan and Derrick Wing Kwan Ng and Chengzhong Xu", "abstract": "Edge Gaussian splatting (EGS), which aggregates data from distributed clients (e.g., drones) and trains a global GS model at the edge (e.g., ground server), is an emerging paradigm for scene reconstruction in low-altitude economy. Unlike traditional edge resource management methods that emphasize communication throughput or general-purpose learning performance, EGS explicitly aims to maximize the GS qualities, rendering existing approaches inapplicable. To address this problem, this paper formulates a novel GS-oriented objective function that distinguishes the heterogeneous view contributions of different clients. However, evaluating this function in turn requires clients' images, leading to a causality dilemma. To this end, this paper further proposes a sample-then-transmit EGS (or STT-GS for short) strategy, which first samples a subset of images as pilot data from each client for loss prediction. Based on the first-stage evaluation, communication resources are then prioritized towards more valuable clients. To achieve efficient sampling, a feature-domain clustering (FDC) scheme is proposed to select the most representative data and pilot transmission time minimization (PTTM) is adopted to reduce the pilot overhead. Subsequently, we develop a joint client selection and power control (JCSPC) framework to maximize the GS-oriented function under communication resource constraints. Despite the nonconvexity of the problem, we propose a low-complexity efficient solution based on the penalty alternating majorization minimization (PAMM) algorithm. Experiments reveal that the proposed scheme significantly outperforms existing benchmarks on real-world datasets. The GS-oriented objective can be accurately predicted with low sampling ratios (e.g., 10%), and our method achieves an excellent tradeoff between view contributions and communication costs.", "link": "http://arxiv.org/abs/2510.13186v4", "date": "2025-12-03", "relevancy": 2.9456, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6202}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6011}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STT-GS%3A%20Sample-Then-Transmit%20Edge%20Gaussian%20Splatting%20with%20Joint%20Client%20Selection%20and%20Power%20Control&body=Title%3A%20STT-GS%3A%20Sample-Then-Transmit%20Edge%20Gaussian%20Splatting%20with%20Joint%20Client%20Selection%20and%20Power%20Control%0AAuthor%3A%20Zhen%20Li%20and%20Xibin%20Jin%20and%20Guoliang%20Li%20and%20Shuai%20Wang%20and%20Miaowen%20Wen%20and%20Huseyin%20Arslan%20and%20Derrick%20Wing%20Kwan%20Ng%20and%20Chengzhong%20Xu%0AAbstract%3A%20Edge%20Gaussian%20splatting%20%28EGS%29%2C%20which%20aggregates%20data%20from%20distributed%20clients%20%28e.g.%2C%20drones%29%20and%20trains%20a%20global%20GS%20model%20at%20the%20edge%20%28e.g.%2C%20ground%20server%29%2C%20is%20an%20emerging%20paradigm%20for%20scene%20reconstruction%20in%20low-altitude%20economy.%20Unlike%20traditional%20edge%20resource%20management%20methods%20that%20emphasize%20communication%20throughput%20or%20general-purpose%20learning%20performance%2C%20EGS%20explicitly%20aims%20to%20maximize%20the%20GS%20qualities%2C%20rendering%20existing%20approaches%20inapplicable.%20To%20address%20this%20problem%2C%20this%20paper%20formulates%20a%20novel%20GS-oriented%20objective%20function%20that%20distinguishes%20the%20heterogeneous%20view%20contributions%20of%20different%20clients.%20However%2C%20evaluating%20this%20function%20in%20turn%20requires%20clients%27%20images%2C%20leading%20to%20a%20causality%20dilemma.%20To%20this%20end%2C%20this%20paper%20further%20proposes%20a%20sample-then-transmit%20EGS%20%28or%20STT-GS%20for%20short%29%20strategy%2C%20which%20first%20samples%20a%20subset%20of%20images%20as%20pilot%20data%20from%20each%20client%20for%20loss%20prediction.%20Based%20on%20the%20first-stage%20evaluation%2C%20communication%20resources%20are%20then%20prioritized%20towards%20more%20valuable%20clients.%20To%20achieve%20efficient%20sampling%2C%20a%20feature-domain%20clustering%20%28FDC%29%20scheme%20is%20proposed%20to%20select%20the%20most%20representative%20data%20and%20pilot%20transmission%20time%20minimization%20%28PTTM%29%20is%20adopted%20to%20reduce%20the%20pilot%20overhead.%20Subsequently%2C%20we%20develop%20a%20joint%20client%20selection%20and%20power%20control%20%28JCSPC%29%20framework%20to%20maximize%20the%20GS-oriented%20function%20under%20communication%20resource%20constraints.%20Despite%20the%20nonconvexity%20of%20the%20problem%2C%20we%20propose%20a%20low-complexity%20efficient%20solution%20based%20on%20the%20penalty%20alternating%20majorization%20minimization%20%28PAMM%29%20algorithm.%20Experiments%20reveal%20that%20the%20proposed%20scheme%20significantly%20outperforms%20existing%20benchmarks%20on%20real-world%20datasets.%20The%20GS-oriented%20objective%20can%20be%20accurately%20predicted%20with%20low%20sampling%20ratios%20%28e.g.%2C%2010%25%29%2C%20and%20our%20method%20achieves%20an%20excellent%20tradeoff%20between%20view%20contributions%20and%20communication%20costs.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13186v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTT-GS%253A%2520Sample-Then-Transmit%2520Edge%2520Gaussian%2520Splatting%2520with%2520Joint%2520Client%2520Selection%2520and%2520Power%2520Control%26entry.906535625%3DZhen%2520Li%2520and%2520Xibin%2520Jin%2520and%2520Guoliang%2520Li%2520and%2520Shuai%2520Wang%2520and%2520Miaowen%2520Wen%2520and%2520Huseyin%2520Arslan%2520and%2520Derrick%2520Wing%2520Kwan%2520Ng%2520and%2520Chengzhong%2520Xu%26entry.1292438233%3DEdge%2520Gaussian%2520splatting%2520%2528EGS%2529%252C%2520which%2520aggregates%2520data%2520from%2520distributed%2520clients%2520%2528e.g.%252C%2520drones%2529%2520and%2520trains%2520a%2520global%2520GS%2520model%2520at%2520the%2520edge%2520%2528e.g.%252C%2520ground%2520server%2529%252C%2520is%2520an%2520emerging%2520paradigm%2520for%2520scene%2520reconstruction%2520in%2520low-altitude%2520economy.%2520Unlike%2520traditional%2520edge%2520resource%2520management%2520methods%2520that%2520emphasize%2520communication%2520throughput%2520or%2520general-purpose%2520learning%2520performance%252C%2520EGS%2520explicitly%2520aims%2520to%2520maximize%2520the%2520GS%2520qualities%252C%2520rendering%2520existing%2520approaches%2520inapplicable.%2520To%2520address%2520this%2520problem%252C%2520this%2520paper%2520formulates%2520a%2520novel%2520GS-oriented%2520objective%2520function%2520that%2520distinguishes%2520the%2520heterogeneous%2520view%2520contributions%2520of%2520different%2520clients.%2520However%252C%2520evaluating%2520this%2520function%2520in%2520turn%2520requires%2520clients%2527%2520images%252C%2520leading%2520to%2520a%2520causality%2520dilemma.%2520To%2520this%2520end%252C%2520this%2520paper%2520further%2520proposes%2520a%2520sample-then-transmit%2520EGS%2520%2528or%2520STT-GS%2520for%2520short%2529%2520strategy%252C%2520which%2520first%2520samples%2520a%2520subset%2520of%2520images%2520as%2520pilot%2520data%2520from%2520each%2520client%2520for%2520loss%2520prediction.%2520Based%2520on%2520the%2520first-stage%2520evaluation%252C%2520communication%2520resources%2520are%2520then%2520prioritized%2520towards%2520more%2520valuable%2520clients.%2520To%2520achieve%2520efficient%2520sampling%252C%2520a%2520feature-domain%2520clustering%2520%2528FDC%2529%2520scheme%2520is%2520proposed%2520to%2520select%2520the%2520most%2520representative%2520data%2520and%2520pilot%2520transmission%2520time%2520minimization%2520%2528PTTM%2529%2520is%2520adopted%2520to%2520reduce%2520the%2520pilot%2520overhead.%2520Subsequently%252C%2520we%2520develop%2520a%2520joint%2520client%2520selection%2520and%2520power%2520control%2520%2528JCSPC%2529%2520framework%2520to%2520maximize%2520the%2520GS-oriented%2520function%2520under%2520communication%2520resource%2520constraints.%2520Despite%2520the%2520nonconvexity%2520of%2520the%2520problem%252C%2520we%2520propose%2520a%2520low-complexity%2520efficient%2520solution%2520based%2520on%2520the%2520penalty%2520alternating%2520majorization%2520minimization%2520%2528PAMM%2529%2520algorithm.%2520Experiments%2520reveal%2520that%2520the%2520proposed%2520scheme%2520significantly%2520outperforms%2520existing%2520benchmarks%2520on%2520real-world%2520datasets.%2520The%2520GS-oriented%2520objective%2520can%2520be%2520accurately%2520predicted%2520with%2520low%2520sampling%2520ratios%2520%2528e.g.%252C%252010%2525%2529%252C%2520and%2520our%2520method%2520achieves%2520an%2520excellent%2520tradeoff%2520between%2520view%2520contributions%2520and%2520communication%2520costs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13186v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STT-GS%3A%20Sample-Then-Transmit%20Edge%20Gaussian%20Splatting%20with%20Joint%20Client%20Selection%20and%20Power%20Control&entry.906535625=Zhen%20Li%20and%20Xibin%20Jin%20and%20Guoliang%20Li%20and%20Shuai%20Wang%20and%20Miaowen%20Wen%20and%20Huseyin%20Arslan%20and%20Derrick%20Wing%20Kwan%20Ng%20and%20Chengzhong%20Xu&entry.1292438233=Edge%20Gaussian%20splatting%20%28EGS%29%2C%20which%20aggregates%20data%20from%20distributed%20clients%20%28e.g.%2C%20drones%29%20and%20trains%20a%20global%20GS%20model%20at%20the%20edge%20%28e.g.%2C%20ground%20server%29%2C%20is%20an%20emerging%20paradigm%20for%20scene%20reconstruction%20in%20low-altitude%20economy.%20Unlike%20traditional%20edge%20resource%20management%20methods%20that%20emphasize%20communication%20throughput%20or%20general-purpose%20learning%20performance%2C%20EGS%20explicitly%20aims%20to%20maximize%20the%20GS%20qualities%2C%20rendering%20existing%20approaches%20inapplicable.%20To%20address%20this%20problem%2C%20this%20paper%20formulates%20a%20novel%20GS-oriented%20objective%20function%20that%20distinguishes%20the%20heterogeneous%20view%20contributions%20of%20different%20clients.%20However%2C%20evaluating%20this%20function%20in%20turn%20requires%20clients%27%20images%2C%20leading%20to%20a%20causality%20dilemma.%20To%20this%20end%2C%20this%20paper%20further%20proposes%20a%20sample-then-transmit%20EGS%20%28or%20STT-GS%20for%20short%29%20strategy%2C%20which%20first%20samples%20a%20subset%20of%20images%20as%20pilot%20data%20from%20each%20client%20for%20loss%20prediction.%20Based%20on%20the%20first-stage%20evaluation%2C%20communication%20resources%20are%20then%20prioritized%20towards%20more%20valuable%20clients.%20To%20achieve%20efficient%20sampling%2C%20a%20feature-domain%20clustering%20%28FDC%29%20scheme%20is%20proposed%20to%20select%20the%20most%20representative%20data%20and%20pilot%20transmission%20time%20minimization%20%28PTTM%29%20is%20adopted%20to%20reduce%20the%20pilot%20overhead.%20Subsequently%2C%20we%20develop%20a%20joint%20client%20selection%20and%20power%20control%20%28JCSPC%29%20framework%20to%20maximize%20the%20GS-oriented%20function%20under%20communication%20resource%20constraints.%20Despite%20the%20nonconvexity%20of%20the%20problem%2C%20we%20propose%20a%20low-complexity%20efficient%20solution%20based%20on%20the%20penalty%20alternating%20majorization%20minimization%20%28PAMM%29%20algorithm.%20Experiments%20reveal%20that%20the%20proposed%20scheme%20significantly%20outperforms%20existing%20benchmarks%20on%20real-world%20datasets.%20The%20GS-oriented%20objective%20can%20be%20accurately%20predicted%20with%20low%20sampling%20ratios%20%28e.g.%2C%2010%25%29%2C%20and%20our%20method%20achieves%20an%20excellent%20tradeoff%20between%20view%20contributions%20and%20communication%20costs.&entry.1838667208=http%3A//arxiv.org/abs/2510.13186v4&entry.124074799=Read"},
{"title": "Neural Radiance and Gaze Fields for Visual Attention Modeling in 3D Environments", "author": "Andrei Chubarau and Yinan Wang and James J. Clark", "abstract": "We introduce Neural Radiance and Gaze Fields (NeRGs), a novel approach for representing visual attention in complex environments. Much like how Neural Radiance Fields (NeRFs) perform novel view synthesis, NeRGs reconstruct gaze patterns from arbitrary viewpoints, implicitly mapping visual attention to 3D surfaces. We achieve this by augmenting a standard NeRF with an additional network that models local egocentric gaze probability density, conditioned on scene geometry and observer position. The output of a NeRG is a rendered view of the scene alongside a pixel-wise salience map representing the conditional probability that a given observer fixates on visible surfaces. Unlike prior methods, our system is lightweight and enables visualization of gaze fields at interactive framerates. Moreover, NeRGs allow the observer perspective to be decoupled from the rendering camera and correctly account for gaze occlusion due to intervening geometry. We demonstrate the effectiveness of NeRGs using head pose from skeleton tracking as a proxy for gaze, employing our proposed gaze probes to aggregate noisy rays into robust probability density targets for supervision.", "link": "http://arxiv.org/abs/2503.07828v2", "date": "2025-12-03", "relevancy": 2.9215, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5999}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5952}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Radiance%20and%20Gaze%20Fields%20for%20Visual%20Attention%20Modeling%20in%203D%20Environments&body=Title%3A%20Neural%20Radiance%20and%20Gaze%20Fields%20for%20Visual%20Attention%20Modeling%20in%203D%20Environments%0AAuthor%3A%20Andrei%20Chubarau%20and%20Yinan%20Wang%20and%20James%20J.%20Clark%0AAbstract%3A%20We%20introduce%20Neural%20Radiance%20and%20Gaze%20Fields%20%28NeRGs%29%2C%20a%20novel%20approach%20for%20representing%20visual%20attention%20in%20complex%20environments.%20Much%20like%20how%20Neural%20Radiance%20Fields%20%28NeRFs%29%20perform%20novel%20view%20synthesis%2C%20NeRGs%20reconstruct%20gaze%20patterns%20from%20arbitrary%20viewpoints%2C%20implicitly%20mapping%20visual%20attention%20to%203D%20surfaces.%20We%20achieve%20this%20by%20augmenting%20a%20standard%20NeRF%20with%20an%20additional%20network%20that%20models%20local%20egocentric%20gaze%20probability%20density%2C%20conditioned%20on%20scene%20geometry%20and%20observer%20position.%20The%20output%20of%20a%20NeRG%20is%20a%20rendered%20view%20of%20the%20scene%20alongside%20a%20pixel-wise%20salience%20map%20representing%20the%20conditional%20probability%20that%20a%20given%20observer%20fixates%20on%20visible%20surfaces.%20Unlike%20prior%20methods%2C%20our%20system%20is%20lightweight%20and%20enables%20visualization%20of%20gaze%20fields%20at%20interactive%20framerates.%20Moreover%2C%20NeRGs%20allow%20the%20observer%20perspective%20to%20be%20decoupled%20from%20the%20rendering%20camera%20and%20correctly%20account%20for%20gaze%20occlusion%20due%20to%20intervening%20geometry.%20We%20demonstrate%20the%20effectiveness%20of%20NeRGs%20using%20head%20pose%20from%20skeleton%20tracking%20as%20a%20proxy%20for%20gaze%2C%20employing%20our%20proposed%20gaze%20probes%20to%20aggregate%20noisy%20rays%20into%20robust%20probability%20density%20targets%20for%20supervision.%0ALink%3A%20http%3A//arxiv.org/abs/2503.07828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Radiance%2520and%2520Gaze%2520Fields%2520for%2520Visual%2520Attention%2520Modeling%2520in%25203D%2520Environments%26entry.906535625%3DAndrei%2520Chubarau%2520and%2520Yinan%2520Wang%2520and%2520James%2520J.%2520Clark%26entry.1292438233%3DWe%2520introduce%2520Neural%2520Radiance%2520and%2520Gaze%2520Fields%2520%2528NeRGs%2529%252C%2520a%2520novel%2520approach%2520for%2520representing%2520visual%2520attention%2520in%2520complex%2520environments.%2520Much%2520like%2520how%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520perform%2520novel%2520view%2520synthesis%252C%2520NeRGs%2520reconstruct%2520gaze%2520patterns%2520from%2520arbitrary%2520viewpoints%252C%2520implicitly%2520mapping%2520visual%2520attention%2520to%25203D%2520surfaces.%2520We%2520achieve%2520this%2520by%2520augmenting%2520a%2520standard%2520NeRF%2520with%2520an%2520additional%2520network%2520that%2520models%2520local%2520egocentric%2520gaze%2520probability%2520density%252C%2520conditioned%2520on%2520scene%2520geometry%2520and%2520observer%2520position.%2520The%2520output%2520of%2520a%2520NeRG%2520is%2520a%2520rendered%2520view%2520of%2520the%2520scene%2520alongside%2520a%2520pixel-wise%2520salience%2520map%2520representing%2520the%2520conditional%2520probability%2520that%2520a%2520given%2520observer%2520fixates%2520on%2520visible%2520surfaces.%2520Unlike%2520prior%2520methods%252C%2520our%2520system%2520is%2520lightweight%2520and%2520enables%2520visualization%2520of%2520gaze%2520fields%2520at%2520interactive%2520framerates.%2520Moreover%252C%2520NeRGs%2520allow%2520the%2520observer%2520perspective%2520to%2520be%2520decoupled%2520from%2520the%2520rendering%2520camera%2520and%2520correctly%2520account%2520for%2520gaze%2520occlusion%2520due%2520to%2520intervening%2520geometry.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520NeRGs%2520using%2520head%2520pose%2520from%2520skeleton%2520tracking%2520as%2520a%2520proxy%2520for%2520gaze%252C%2520employing%2520our%2520proposed%2520gaze%2520probes%2520to%2520aggregate%2520noisy%2520rays%2520into%2520robust%2520probability%2520density%2520targets%2520for%2520supervision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Radiance%20and%20Gaze%20Fields%20for%20Visual%20Attention%20Modeling%20in%203D%20Environments&entry.906535625=Andrei%20Chubarau%20and%20Yinan%20Wang%20and%20James%20J.%20Clark&entry.1292438233=We%20introduce%20Neural%20Radiance%20and%20Gaze%20Fields%20%28NeRGs%29%2C%20a%20novel%20approach%20for%20representing%20visual%20attention%20in%20complex%20environments.%20Much%20like%20how%20Neural%20Radiance%20Fields%20%28NeRFs%29%20perform%20novel%20view%20synthesis%2C%20NeRGs%20reconstruct%20gaze%20patterns%20from%20arbitrary%20viewpoints%2C%20implicitly%20mapping%20visual%20attention%20to%203D%20surfaces.%20We%20achieve%20this%20by%20augmenting%20a%20standard%20NeRF%20with%20an%20additional%20network%20that%20models%20local%20egocentric%20gaze%20probability%20density%2C%20conditioned%20on%20scene%20geometry%20and%20observer%20position.%20The%20output%20of%20a%20NeRG%20is%20a%20rendered%20view%20of%20the%20scene%20alongside%20a%20pixel-wise%20salience%20map%20representing%20the%20conditional%20probability%20that%20a%20given%20observer%20fixates%20on%20visible%20surfaces.%20Unlike%20prior%20methods%2C%20our%20system%20is%20lightweight%20and%20enables%20visualization%20of%20gaze%20fields%20at%20interactive%20framerates.%20Moreover%2C%20NeRGs%20allow%20the%20observer%20perspective%20to%20be%20decoupled%20from%20the%20rendering%20camera%20and%20correctly%20account%20for%20gaze%20occlusion%20due%20to%20intervening%20geometry.%20We%20demonstrate%20the%20effectiveness%20of%20NeRGs%20using%20head%20pose%20from%20skeleton%20tracking%20as%20a%20proxy%20for%20gaze%2C%20employing%20our%20proposed%20gaze%20probes%20to%20aggregate%20noisy%20rays%20into%20robust%20probability%20density%20targets%20for%20supervision.&entry.1838667208=http%3A//arxiv.org/abs/2503.07828v2&entry.124074799=Read"},
{"title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition", "author": "Zichuan Lin and Yicheng Liu and Yang Yang and Lvfang Tao and Deheng Ye", "abstract": "Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.", "link": "http://arxiv.org/abs/2512.03794v1", "date": "2025-12-03", "relevancy": 2.9083, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaptVision%3A%20Efficient%20Vision-Language%20Models%20via%20Adaptive%20Visual%20Acquisition&body=Title%3A%20AdaptVision%3A%20Efficient%20Vision-Language%20Models%20via%20Adaptive%20Visual%20Acquisition%0AAuthor%3A%20Zichuan%20Lin%20and%20Yicheng%20Liu%20and%20Yang%20Yang%20and%20Lvfang%20Tao%20and%20Deheng%20Ye%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20success%20in%20visual%20question%20answering%20tasks%2C%20but%20their%20reliance%20on%20large%20numbers%20of%20visual%20tokens%20introduces%20significant%20computational%20overhead.%20While%20existing%20efficient%20VLM%20approaches%20reduce%20visual%20tokens%20through%20fixed-ratio%20compression%2C%20they%20operate%20passively%20and%20lack%20the%20ability%20to%20adapt%20to%20varying%20task%20requirements.%20This%20motivates%20a%20fundamental%20question%3A%20Can%20VLMs%20autonomously%20determine%20the%20minimum%20number%20of%20visual%20tokens%20required%20for%20each%20sample%3F%20Inspired%20by%20human%20active%20vision%20mechanisms%2C%20we%20introduce%20AdaptVision%2C%20an%20efficient%20VLM%20paradigm%20that%20enables%20adaptive%20visual%20token%20acquisition%20through%20a%20coarse-to-fine%20approach.%20Our%20model%20initially%20processes%20compressed%20visual%20tokens%20from%20low-resolution%20images%20and%20selectively%20acquires%20additional%20visual%20information%20by%20invoking%20a%20bounding%20box%20tool%20to%20crop%20key%20regions%20when%20necessary.%20We%20train%20AdaptVision%20using%20a%20reinforcement%20learning%20framework%20that%20carefully%20balances%20accuracy%20and%20efficiency.%20Central%20to%20our%20approach%20is%20Decoupled%20Turn%20Policy%20Optimization%20%28DTPO%29%2C%20which%20decouples%20the%20learning%20objective%20into%20two%20components%3A%20%281%29%20tool%20learning%2C%20which%20optimizes%20correct%20tool%20utilization%2C%20and%20%282%29%20accuracy%20improvement%2C%20which%20refines%20the%20generated%20responses%20to%20improve%20answer%20correctness.%20Based%20on%20this%20formulation%2C%20we%20further%20decouple%20advantage%20estimation%20by%20computing%20separate%20advantages%20for%20tokens%20associated%20with%20each%20objective.%20This%20formulation%20enables%20more%20effective%20optimization%20for%20AdaptVision%20compared%20to%20vanilla%20GRPO.%20Comprehensive%20experiments%20across%20multiple%20VQA%20benchmarks%20demonstrate%20that%20AdaptVision%20achieves%20superior%20performance%20while%20consuming%20substantially%20fewer%20visual%20tokens%20than%20state-of-the-art%20efficient%20VLM%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptVision%253A%2520Efficient%2520Vision-Language%2520Models%2520via%2520Adaptive%2520Visual%2520Acquisition%26entry.906535625%3DZichuan%2520Lin%2520and%2520Yicheng%2520Liu%2520and%2520Yang%2520Yang%2520and%2520Lvfang%2520Tao%2520and%2520Deheng%2520Ye%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520visual%2520question%2520answering%2520tasks%252C%2520but%2520their%2520reliance%2520on%2520large%2520numbers%2520of%2520visual%2520tokens%2520introduces%2520significant%2520computational%2520overhead.%2520While%2520existing%2520efficient%2520VLM%2520approaches%2520reduce%2520visual%2520tokens%2520through%2520fixed-ratio%2520compression%252C%2520they%2520operate%2520passively%2520and%2520lack%2520the%2520ability%2520to%2520adapt%2520to%2520varying%2520task%2520requirements.%2520This%2520motivates%2520a%2520fundamental%2520question%253A%2520Can%2520VLMs%2520autonomously%2520determine%2520the%2520minimum%2520number%2520of%2520visual%2520tokens%2520required%2520for%2520each%2520sample%253F%2520Inspired%2520by%2520human%2520active%2520vision%2520mechanisms%252C%2520we%2520introduce%2520AdaptVision%252C%2520an%2520efficient%2520VLM%2520paradigm%2520that%2520enables%2520adaptive%2520visual%2520token%2520acquisition%2520through%2520a%2520coarse-to-fine%2520approach.%2520Our%2520model%2520initially%2520processes%2520compressed%2520visual%2520tokens%2520from%2520low-resolution%2520images%2520and%2520selectively%2520acquires%2520additional%2520visual%2520information%2520by%2520invoking%2520a%2520bounding%2520box%2520tool%2520to%2520crop%2520key%2520regions%2520when%2520necessary.%2520We%2520train%2520AdaptVision%2520using%2520a%2520reinforcement%2520learning%2520framework%2520that%2520carefully%2520balances%2520accuracy%2520and%2520efficiency.%2520Central%2520to%2520our%2520approach%2520is%2520Decoupled%2520Turn%2520Policy%2520Optimization%2520%2528DTPO%2529%252C%2520which%2520decouples%2520the%2520learning%2520objective%2520into%2520two%2520components%253A%2520%25281%2529%2520tool%2520learning%252C%2520which%2520optimizes%2520correct%2520tool%2520utilization%252C%2520and%2520%25282%2529%2520accuracy%2520improvement%252C%2520which%2520refines%2520the%2520generated%2520responses%2520to%2520improve%2520answer%2520correctness.%2520Based%2520on%2520this%2520formulation%252C%2520we%2520further%2520decouple%2520advantage%2520estimation%2520by%2520computing%2520separate%2520advantages%2520for%2520tokens%2520associated%2520with%2520each%2520objective.%2520This%2520formulation%2520enables%2520more%2520effective%2520optimization%2520for%2520AdaptVision%2520compared%2520to%2520vanilla%2520GRPO.%2520Comprehensive%2520experiments%2520across%2520multiple%2520VQA%2520benchmarks%2520demonstrate%2520that%2520AdaptVision%2520achieves%2520superior%2520performance%2520while%2520consuming%2520substantially%2520fewer%2520visual%2520tokens%2520than%2520state-of-the-art%2520efficient%2520VLM%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaptVision%3A%20Efficient%20Vision-Language%20Models%20via%20Adaptive%20Visual%20Acquisition&entry.906535625=Zichuan%20Lin%20and%20Yicheng%20Liu%20and%20Yang%20Yang%20and%20Lvfang%20Tao%20and%20Deheng%20Ye&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20success%20in%20visual%20question%20answering%20tasks%2C%20but%20their%20reliance%20on%20large%20numbers%20of%20visual%20tokens%20introduces%20significant%20computational%20overhead.%20While%20existing%20efficient%20VLM%20approaches%20reduce%20visual%20tokens%20through%20fixed-ratio%20compression%2C%20they%20operate%20passively%20and%20lack%20the%20ability%20to%20adapt%20to%20varying%20task%20requirements.%20This%20motivates%20a%20fundamental%20question%3A%20Can%20VLMs%20autonomously%20determine%20the%20minimum%20number%20of%20visual%20tokens%20required%20for%20each%20sample%3F%20Inspired%20by%20human%20active%20vision%20mechanisms%2C%20we%20introduce%20AdaptVision%2C%20an%20efficient%20VLM%20paradigm%20that%20enables%20adaptive%20visual%20token%20acquisition%20through%20a%20coarse-to-fine%20approach.%20Our%20model%20initially%20processes%20compressed%20visual%20tokens%20from%20low-resolution%20images%20and%20selectively%20acquires%20additional%20visual%20information%20by%20invoking%20a%20bounding%20box%20tool%20to%20crop%20key%20regions%20when%20necessary.%20We%20train%20AdaptVision%20using%20a%20reinforcement%20learning%20framework%20that%20carefully%20balances%20accuracy%20and%20efficiency.%20Central%20to%20our%20approach%20is%20Decoupled%20Turn%20Policy%20Optimization%20%28DTPO%29%2C%20which%20decouples%20the%20learning%20objective%20into%20two%20components%3A%20%281%29%20tool%20learning%2C%20which%20optimizes%20correct%20tool%20utilization%2C%20and%20%282%29%20accuracy%20improvement%2C%20which%20refines%20the%20generated%20responses%20to%20improve%20answer%20correctness.%20Based%20on%20this%20formulation%2C%20we%20further%20decouple%20advantage%20estimation%20by%20computing%20separate%20advantages%20for%20tokens%20associated%20with%20each%20objective.%20This%20formulation%20enables%20more%20effective%20optimization%20for%20AdaptVision%20compared%20to%20vanilla%20GRPO.%20Comprehensive%20experiments%20across%20multiple%20VQA%20benchmarks%20demonstrate%20that%20AdaptVision%20achieves%20superior%20performance%20while%20consuming%20substantially%20fewer%20visual%20tokens%20than%20state-of-the-art%20efficient%20VLM%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.03794v1&entry.124074799=Read"},
{"title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue", "author": "Wenwen Tong and Hewei Guo and Dongchuan Ran and Jiangnan Chen and Jiefan Lu and Kaibin Wang and Keqiang Li and Xiaoxu Zhu and Jiakui Li and Kehan Li and Xueheng Li and Lumin Li and Chenxu Guo and Jiasheng Zhou and Jiandong Chen and Xianye Wu and Jiahao Wang and Silei Wu and Lei Chen and Hanming Deng and Yuxuan Song and Dinghao Zhou and Guiping Zhong and Ken Zheng and Shiyin Kang and Lewei Lu", "abstract": "We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the model's ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems.", "link": "http://arxiv.org/abs/2510.13747v2", "date": "2025-12-03", "relevancy": 2.8558, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InteractiveOmni%3A%20A%20Unified%20Omni-modal%20Model%20for%20Audio-Visual%20Multi-turn%20Dialogue&body=Title%3A%20InteractiveOmni%3A%20A%20Unified%20Omni-modal%20Model%20for%20Audio-Visual%20Multi-turn%20Dialogue%0AAuthor%3A%20Wenwen%20Tong%20and%20Hewei%20Guo%20and%20Dongchuan%20Ran%20and%20Jiangnan%20Chen%20and%20Jiefan%20Lu%20and%20Kaibin%20Wang%20and%20Keqiang%20Li%20and%20Xiaoxu%20Zhu%20and%20Jiakui%20Li%20and%20Kehan%20Li%20and%20Xueheng%20Li%20and%20Lumin%20Li%20and%20Chenxu%20Guo%20and%20Jiasheng%20Zhou%20and%20Jiandong%20Chen%20and%20Xianye%20Wu%20and%20Jiahao%20Wang%20and%20Silei%20Wu%20and%20Lei%20Chen%20and%20Hanming%20Deng%20and%20Yuxuan%20Song%20and%20Dinghao%20Zhou%20and%20Guiping%20Zhong%20and%20Ken%20Zheng%20and%20Shiyin%20Kang%20and%20Lewei%20Lu%0AAbstract%3A%20We%20introduce%20InteractiveOmni%2C%20a%20unified%20and%20open-source%20omni-modal%20large%20language%20model%20for%20audio-visual%20multi-turn%20interaction%2C%20ranging%20from%204B%20to%208B%20parameters%2C%20designed%20to%20lead%20the%20field%20of%20lightweight%20models%20by%20offering%20comprehensive%20omni-modal%20understanding%20and%20speech%20generation%20capabilities.%20To%20achieve%20this%2C%20we%20integrate%20the%20vision%20encoder%2C%20audio%20encoder%2C%20large%20language%20model%2C%20and%20speech%20decoder%20into%20a%20unified%20model%20for%20understanding%20and%20generation%20tasks.%20We%20design%20a%20multi-stage%20training%20strategy%20to%20ensure%20robust%20cross-modal%20capabilities%2C%20including%20pre-training%20for%20omni-modal%20understanding%2C%20followed%20by%20post-training%20with%20speech%20conversation%20and%20audio-visual%20interaction.%20To%20enable%20human-like%20long-term%20conversational%20ability%2C%20we%20meticulously%20curate%20a%20multi-turn%20training%20dataset%20that%20enhances%20the%20model%27s%20ability%20to%20handle%20complex%20and%20multi-turn%20interactions.%20To%20effectively%20evaluate%20the%20multi-turn%20memory%20and%20speech%20interaction%20capabilities%2C%20we%20construct%20the%20multi-modal%20multi-turn%20memory%20benchmark%20and%20the%20multi-turn%20speech%20interaction%20benchmark.%20Experiments%20demonstrate%20that%20InteractiveOmni%20significantly%20outperforms%20leading%20open-source%20models%20and%20provides%20a%20more%20intelligent%20multi-turn%20audio-visual%20experience%2C%20particularly%20in%20its%20long-term%20memory%20capabilities.%20Notably%2C%20InteractiveOmni-4B%20is%20comparable%20to%20the%20much%20larger%20model%20like%20Qwen2.5-Omni-7B%20on%20general%20benchmarks%2C%20and%20it%20can%20retain%2097%25%20of%20the%20performance%20of%20the%20InteractiveOmni-8B%20while%20utilizing%20only%2050%25%20of%20the%20model%20size.%20Achieving%20state-of-the-art%20results%20against%20similarly%20sized%20models%20across%20image%2C%20audio%2C%20video%20understanding%2C%20and%20speech%20generation%20tasks%2C%20InteractiveOmni%20is%20an%20accessible%2C%20open-source%20foundation%20for%20next-generation%20intelligent%20interactive%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractiveOmni%253A%2520A%2520Unified%2520Omni-modal%2520Model%2520for%2520Audio-Visual%2520Multi-turn%2520Dialogue%26entry.906535625%3DWenwen%2520Tong%2520and%2520Hewei%2520Guo%2520and%2520Dongchuan%2520Ran%2520and%2520Jiangnan%2520Chen%2520and%2520Jiefan%2520Lu%2520and%2520Kaibin%2520Wang%2520and%2520Keqiang%2520Li%2520and%2520Xiaoxu%2520Zhu%2520and%2520Jiakui%2520Li%2520and%2520Kehan%2520Li%2520and%2520Xueheng%2520Li%2520and%2520Lumin%2520Li%2520and%2520Chenxu%2520Guo%2520and%2520Jiasheng%2520Zhou%2520and%2520Jiandong%2520Chen%2520and%2520Xianye%2520Wu%2520and%2520Jiahao%2520Wang%2520and%2520Silei%2520Wu%2520and%2520Lei%2520Chen%2520and%2520Hanming%2520Deng%2520and%2520Yuxuan%2520Song%2520and%2520Dinghao%2520Zhou%2520and%2520Guiping%2520Zhong%2520and%2520Ken%2520Zheng%2520and%2520Shiyin%2520Kang%2520and%2520Lewei%2520Lu%26entry.1292438233%3DWe%2520introduce%2520InteractiveOmni%252C%2520a%2520unified%2520and%2520open-source%2520omni-modal%2520large%2520language%2520model%2520for%2520audio-visual%2520multi-turn%2520interaction%252C%2520ranging%2520from%25204B%2520to%25208B%2520parameters%252C%2520designed%2520to%2520lead%2520the%2520field%2520of%2520lightweight%2520models%2520by%2520offering%2520comprehensive%2520omni-modal%2520understanding%2520and%2520speech%2520generation%2520capabilities.%2520To%2520achieve%2520this%252C%2520we%2520integrate%2520the%2520vision%2520encoder%252C%2520audio%2520encoder%252C%2520large%2520language%2520model%252C%2520and%2520speech%2520decoder%2520into%2520a%2520unified%2520model%2520for%2520understanding%2520and%2520generation%2520tasks.%2520We%2520design%2520a%2520multi-stage%2520training%2520strategy%2520to%2520ensure%2520robust%2520cross-modal%2520capabilities%252C%2520including%2520pre-training%2520for%2520omni-modal%2520understanding%252C%2520followed%2520by%2520post-training%2520with%2520speech%2520conversation%2520and%2520audio-visual%2520interaction.%2520To%2520enable%2520human-like%2520long-term%2520conversational%2520ability%252C%2520we%2520meticulously%2520curate%2520a%2520multi-turn%2520training%2520dataset%2520that%2520enhances%2520the%2520model%2527s%2520ability%2520to%2520handle%2520complex%2520and%2520multi-turn%2520interactions.%2520To%2520effectively%2520evaluate%2520the%2520multi-turn%2520memory%2520and%2520speech%2520interaction%2520capabilities%252C%2520we%2520construct%2520the%2520multi-modal%2520multi-turn%2520memory%2520benchmark%2520and%2520the%2520multi-turn%2520speech%2520interaction%2520benchmark.%2520Experiments%2520demonstrate%2520that%2520InteractiveOmni%2520significantly%2520outperforms%2520leading%2520open-source%2520models%2520and%2520provides%2520a%2520more%2520intelligent%2520multi-turn%2520audio-visual%2520experience%252C%2520particularly%2520in%2520its%2520long-term%2520memory%2520capabilities.%2520Notably%252C%2520InteractiveOmni-4B%2520is%2520comparable%2520to%2520the%2520much%2520larger%2520model%2520like%2520Qwen2.5-Omni-7B%2520on%2520general%2520benchmarks%252C%2520and%2520it%2520can%2520retain%252097%2525%2520of%2520the%2520performance%2520of%2520the%2520InteractiveOmni-8B%2520while%2520utilizing%2520only%252050%2525%2520of%2520the%2520model%2520size.%2520Achieving%2520state-of-the-art%2520results%2520against%2520similarly%2520sized%2520models%2520across%2520image%252C%2520audio%252C%2520video%2520understanding%252C%2520and%2520speech%2520generation%2520tasks%252C%2520InteractiveOmni%2520is%2520an%2520accessible%252C%2520open-source%2520foundation%2520for%2520next-generation%2520intelligent%2520interactive%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InteractiveOmni%3A%20A%20Unified%20Omni-modal%20Model%20for%20Audio-Visual%20Multi-turn%20Dialogue&entry.906535625=Wenwen%20Tong%20and%20Hewei%20Guo%20and%20Dongchuan%20Ran%20and%20Jiangnan%20Chen%20and%20Jiefan%20Lu%20and%20Kaibin%20Wang%20and%20Keqiang%20Li%20and%20Xiaoxu%20Zhu%20and%20Jiakui%20Li%20and%20Kehan%20Li%20and%20Xueheng%20Li%20and%20Lumin%20Li%20and%20Chenxu%20Guo%20and%20Jiasheng%20Zhou%20and%20Jiandong%20Chen%20and%20Xianye%20Wu%20and%20Jiahao%20Wang%20and%20Silei%20Wu%20and%20Lei%20Chen%20and%20Hanming%20Deng%20and%20Yuxuan%20Song%20and%20Dinghao%20Zhou%20and%20Guiping%20Zhong%20and%20Ken%20Zheng%20and%20Shiyin%20Kang%20and%20Lewei%20Lu&entry.1292438233=We%20introduce%20InteractiveOmni%2C%20a%20unified%20and%20open-source%20omni-modal%20large%20language%20model%20for%20audio-visual%20multi-turn%20interaction%2C%20ranging%20from%204B%20to%208B%20parameters%2C%20designed%20to%20lead%20the%20field%20of%20lightweight%20models%20by%20offering%20comprehensive%20omni-modal%20understanding%20and%20speech%20generation%20capabilities.%20To%20achieve%20this%2C%20we%20integrate%20the%20vision%20encoder%2C%20audio%20encoder%2C%20large%20language%20model%2C%20and%20speech%20decoder%20into%20a%20unified%20model%20for%20understanding%20and%20generation%20tasks.%20We%20design%20a%20multi-stage%20training%20strategy%20to%20ensure%20robust%20cross-modal%20capabilities%2C%20including%20pre-training%20for%20omni-modal%20understanding%2C%20followed%20by%20post-training%20with%20speech%20conversation%20and%20audio-visual%20interaction.%20To%20enable%20human-like%20long-term%20conversational%20ability%2C%20we%20meticulously%20curate%20a%20multi-turn%20training%20dataset%20that%20enhances%20the%20model%27s%20ability%20to%20handle%20complex%20and%20multi-turn%20interactions.%20To%20effectively%20evaluate%20the%20multi-turn%20memory%20and%20speech%20interaction%20capabilities%2C%20we%20construct%20the%20multi-modal%20multi-turn%20memory%20benchmark%20and%20the%20multi-turn%20speech%20interaction%20benchmark.%20Experiments%20demonstrate%20that%20InteractiveOmni%20significantly%20outperforms%20leading%20open-source%20models%20and%20provides%20a%20more%20intelligent%20multi-turn%20audio-visual%20experience%2C%20particularly%20in%20its%20long-term%20memory%20capabilities.%20Notably%2C%20InteractiveOmni-4B%20is%20comparable%20to%20the%20much%20larger%20model%20like%20Qwen2.5-Omni-7B%20on%20general%20benchmarks%2C%20and%20it%20can%20retain%2097%25%20of%20the%20performance%20of%20the%20InteractiveOmni-8B%20while%20utilizing%20only%2050%25%20of%20the%20model%20size.%20Achieving%20state-of-the-art%20results%20against%20similarly%20sized%20models%20across%20image%2C%20audio%2C%20video%20understanding%2C%20and%20speech%20generation%20tasks%2C%20InteractiveOmni%20is%20an%20accessible%2C%20open-source%20foundation%20for%20next-generation%20intelligent%20interactive%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2510.13747v2&entry.124074799=Read"},
{"title": "Tada-DIP: Input-adaptive Deep Image Prior for One-shot 3D Image Reconstruction", "author": "Evan Bell and Shijun Liang and Ismail Alkhouri and Saiprasad Ravishankar", "abstract": "Deep Image Prior (DIP) has recently emerged as a promising one-shot neural-network based image reconstruction method. However, DIP has seen limited application to 3D image reconstruction problems. In this work, we introduce Tada-DIP, a highly effective and fully 3D DIP method for solving 3D inverse problems. By combining input-adaptation and denoising regularization, Tada-DIP produces high-quality 3D reconstructions while avoiding the overfitting phenomenon that is common in DIP. Experiments on sparse-view X-ray computed tomography reconstruction validate the effectiveness of the proposed method, demonstrating that Tada-DIP produces much better reconstructions than training-data-free baselines and achieves reconstruction performance on par with a supervised network trained using a large dataset with fully-sampled volumes.", "link": "http://arxiv.org/abs/2512.03962v1", "date": "2025-12-03", "relevancy": 2.8344, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5721}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5721}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tada-DIP%3A%20Input-adaptive%20Deep%20Image%20Prior%20for%20One-shot%203D%20Image%20Reconstruction&body=Title%3A%20Tada-DIP%3A%20Input-adaptive%20Deep%20Image%20Prior%20for%20One-shot%203D%20Image%20Reconstruction%0AAuthor%3A%20Evan%20Bell%20and%20Shijun%20Liang%20and%20Ismail%20Alkhouri%20and%20Saiprasad%20Ravishankar%0AAbstract%3A%20Deep%20Image%20Prior%20%28DIP%29%20has%20recently%20emerged%20as%20a%20promising%20one-shot%20neural-network%20based%20image%20reconstruction%20method.%20However%2C%20DIP%20has%20seen%20limited%20application%20to%203D%20image%20reconstruction%20problems.%20In%20this%20work%2C%20we%20introduce%20Tada-DIP%2C%20a%20highly%20effective%20and%20fully%203D%20DIP%20method%20for%20solving%203D%20inverse%20problems.%20By%20combining%20input-adaptation%20and%20denoising%20regularization%2C%20Tada-DIP%20produces%20high-quality%203D%20reconstructions%20while%20avoiding%20the%20overfitting%20phenomenon%20that%20is%20common%20in%20DIP.%20Experiments%20on%20sparse-view%20X-ray%20computed%20tomography%20reconstruction%20validate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20demonstrating%20that%20Tada-DIP%20produces%20much%20better%20reconstructions%20than%20training-data-free%20baselines%20and%20achieves%20reconstruction%20performance%20on%20par%20with%20a%20supervised%20network%20trained%20using%20a%20large%20dataset%20with%20fully-sampled%20volumes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTada-DIP%253A%2520Input-adaptive%2520Deep%2520Image%2520Prior%2520for%2520One-shot%25203D%2520Image%2520Reconstruction%26entry.906535625%3DEvan%2520Bell%2520and%2520Shijun%2520Liang%2520and%2520Ismail%2520Alkhouri%2520and%2520Saiprasad%2520Ravishankar%26entry.1292438233%3DDeep%2520Image%2520Prior%2520%2528DIP%2529%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520one-shot%2520neural-network%2520based%2520image%2520reconstruction%2520method.%2520However%252C%2520DIP%2520has%2520seen%2520limited%2520application%2520to%25203D%2520image%2520reconstruction%2520problems.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Tada-DIP%252C%2520a%2520highly%2520effective%2520and%2520fully%25203D%2520DIP%2520method%2520for%2520solving%25203D%2520inverse%2520problems.%2520By%2520combining%2520input-adaptation%2520and%2520denoising%2520regularization%252C%2520Tada-DIP%2520produces%2520high-quality%25203D%2520reconstructions%2520while%2520avoiding%2520the%2520overfitting%2520phenomenon%2520that%2520is%2520common%2520in%2520DIP.%2520Experiments%2520on%2520sparse-view%2520X-ray%2520computed%2520tomography%2520reconstruction%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520demonstrating%2520that%2520Tada-DIP%2520produces%2520much%2520better%2520reconstructions%2520than%2520training-data-free%2520baselines%2520and%2520achieves%2520reconstruction%2520performance%2520on%2520par%2520with%2520a%2520supervised%2520network%2520trained%2520using%2520a%2520large%2520dataset%2520with%2520fully-sampled%2520volumes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tada-DIP%3A%20Input-adaptive%20Deep%20Image%20Prior%20for%20One-shot%203D%20Image%20Reconstruction&entry.906535625=Evan%20Bell%20and%20Shijun%20Liang%20and%20Ismail%20Alkhouri%20and%20Saiprasad%20Ravishankar&entry.1292438233=Deep%20Image%20Prior%20%28DIP%29%20has%20recently%20emerged%20as%20a%20promising%20one-shot%20neural-network%20based%20image%20reconstruction%20method.%20However%2C%20DIP%20has%20seen%20limited%20application%20to%203D%20image%20reconstruction%20problems.%20In%20this%20work%2C%20we%20introduce%20Tada-DIP%2C%20a%20highly%20effective%20and%20fully%203D%20DIP%20method%20for%20solving%203D%20inverse%20problems.%20By%20combining%20input-adaptation%20and%20denoising%20regularization%2C%20Tada-DIP%20produces%20high-quality%203D%20reconstructions%20while%20avoiding%20the%20overfitting%20phenomenon%20that%20is%20common%20in%20DIP.%20Experiments%20on%20sparse-view%20X-ray%20computed%20tomography%20reconstruction%20validate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20demonstrating%20that%20Tada-DIP%20produces%20much%20better%20reconstructions%20than%20training-data-free%20baselines%20and%20achieves%20reconstruction%20performance%20on%20par%20with%20a%20supervised%20network%20trained%20using%20a%20large%20dataset%20with%20fully-sampled%20volumes.&entry.1838667208=http%3A//arxiv.org/abs/2512.03962v1&entry.124074799=Read"},
{"title": "D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation", "author": "Zheyuan Zhang and Jiwei Zhang and Boyu Zhou and Linzhimeng Duan and Hong Chen", "abstract": "Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.", "link": "http://arxiv.org/abs/2511.12528v2", "date": "2025-12-03", "relevancy": 2.8313, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5751}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5751}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D%24%5E%7B2%7D%24-VPR%3A%20A%20Parameter-efficient%20Visual-foundation-model-based%20Visual%20Place%20Recognition%20Method%20via%20Knowledge%20Distillation%20and%20Deformable%20Aggregation&body=Title%3A%20D%24%5E%7B2%7D%24-VPR%3A%20A%20Parameter-efficient%20Visual-foundation-model-based%20Visual%20Place%20Recognition%20Method%20via%20Knowledge%20Distillation%20and%20Deformable%20Aggregation%0AAuthor%3A%20Zheyuan%20Zhang%20and%20Jiwei%20Zhang%20and%20Boyu%20Zhou%20and%20Linzhimeng%20Duan%20and%20Hong%20Chen%0AAbstract%3A%20Visual%20Place%20Recognition%20%28VPR%29%20aims%20to%20determine%20the%20geographic%20location%20of%20a%20query%20image%20by%20retrieving%20its%20most%20visually%20similar%20counterpart%20from%20a%20geo-tagged%20reference%20database.%20Recently%2C%20the%20emergence%20of%20the%20powerful%20visual%20foundation%20model%2C%20DINOv2%2C%20trained%20in%20a%20self-supervised%20manner%20on%20massive%20datasets%2C%20has%20significantly%20improved%20VPR%20performance.%20This%20improvement%20stems%20from%20DINOv2%27s%20exceptional%20feature%20generalization%20capabilities%20but%20is%20often%20accompanied%20by%20increased%20model%20complexity%20and%20computational%20overhead%20that%20impede%20deployment%20on%20resource-constrained%20devices.%20To%20address%20this%20challenge%2C%20we%20propose%20%24D%5E%7B2%7D%24-VPR%2C%20a%20%24D%24istillation-%20and%20%24D%24eformable-based%20framework%20that%20retains%20the%20strong%20feature%20extraction%20capabilities%20of%20visual%20foundation%20models%20while%20significantly%20reducing%20model%20parameters%20and%20achieving%20a%20more%20favorable%20performance-efficiency%20trade-off.%20Specifically%2C%20first%2C%20we%20employ%20a%20two-stage%20training%20strategy%20that%20integrates%20knowledge%20distillation%20and%20fine-tuning.%20Additionally%2C%20we%20introduce%20a%20Distillation%20Recovery%20Module%20%28DRM%29%20to%20better%20align%20the%20feature%20spaces%20between%20the%20teacher%20and%20student%20models%2C%20thereby%20minimizing%20knowledge%20transfer%20losses%20to%20the%20greatest%20extent%20possible.%20Second%2C%20we%20design%20a%20Top-Down-attention-based%20Deformable%20Aggregator%20%28TDDA%29%20that%20leverages%20global%20semantic%20features%20to%20dynamically%20and%20adaptively%20adjust%20the%20Regions%20of%20Interest%20%28ROI%29%20used%20for%20aggregation%2C%20thereby%20improving%20adaptability%20to%20irregular%20structures.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20competitive%20performance%20compared%20to%20state-of-the-art%20approaches.%20Meanwhile%2C%20it%20reduces%20the%20parameter%20count%20by%20approximately%2064.2%25%20and%20FLOPs%20by%20about%2062.6%25%20%28compared%20to%20CricaVPR%29.Code%20is%20available%20at%20https%3A//github.com/tony19980810/D2VPR.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12528v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD%2524%255E%257B2%257D%2524-VPR%253A%2520A%2520Parameter-efficient%2520Visual-foundation-model-based%2520Visual%2520Place%2520Recognition%2520Method%2520via%2520Knowledge%2520Distillation%2520and%2520Deformable%2520Aggregation%26entry.906535625%3DZheyuan%2520Zhang%2520and%2520Jiwei%2520Zhang%2520and%2520Boyu%2520Zhou%2520and%2520Linzhimeng%2520Duan%2520and%2520Hong%2520Chen%26entry.1292438233%3DVisual%2520Place%2520Recognition%2520%2528VPR%2529%2520aims%2520to%2520determine%2520the%2520geographic%2520location%2520of%2520a%2520query%2520image%2520by%2520retrieving%2520its%2520most%2520visually%2520similar%2520counterpart%2520from%2520a%2520geo-tagged%2520reference%2520database.%2520Recently%252C%2520the%2520emergence%2520of%2520the%2520powerful%2520visual%2520foundation%2520model%252C%2520DINOv2%252C%2520trained%2520in%2520a%2520self-supervised%2520manner%2520on%2520massive%2520datasets%252C%2520has%2520significantly%2520improved%2520VPR%2520performance.%2520This%2520improvement%2520stems%2520from%2520DINOv2%2527s%2520exceptional%2520feature%2520generalization%2520capabilities%2520but%2520is%2520often%2520accompanied%2520by%2520increased%2520model%2520complexity%2520and%2520computational%2520overhead%2520that%2520impede%2520deployment%2520on%2520resource-constrained%2520devices.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520%2524D%255E%257B2%257D%2524-VPR%252C%2520a%2520%2524D%2524istillation-%2520and%2520%2524D%2524eformable-based%2520framework%2520that%2520retains%2520the%2520strong%2520feature%2520extraction%2520capabilities%2520of%2520visual%2520foundation%2520models%2520while%2520significantly%2520reducing%2520model%2520parameters%2520and%2520achieving%2520a%2520more%2520favorable%2520performance-efficiency%2520trade-off.%2520Specifically%252C%2520first%252C%2520we%2520employ%2520a%2520two-stage%2520training%2520strategy%2520that%2520integrates%2520knowledge%2520distillation%2520and%2520fine-tuning.%2520Additionally%252C%2520we%2520introduce%2520a%2520Distillation%2520Recovery%2520Module%2520%2528DRM%2529%2520to%2520better%2520align%2520the%2520feature%2520spaces%2520between%2520the%2520teacher%2520and%2520student%2520models%252C%2520thereby%2520minimizing%2520knowledge%2520transfer%2520losses%2520to%2520the%2520greatest%2520extent%2520possible.%2520Second%252C%2520we%2520design%2520a%2520Top-Down-attention-based%2520Deformable%2520Aggregator%2520%2528TDDA%2529%2520that%2520leverages%2520global%2520semantic%2520features%2520to%2520dynamically%2520and%2520adaptively%2520adjust%2520the%2520Regions%2520of%2520Interest%2520%2528ROI%2529%2520used%2520for%2520aggregation%252C%2520thereby%2520improving%2520adaptability%2520to%2520irregular%2520structures.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520competitive%2520performance%2520compared%2520to%2520state-of-the-art%2520approaches.%2520Meanwhile%252C%2520it%2520reduces%2520the%2520parameter%2520count%2520by%2520approximately%252064.2%2525%2520and%2520FLOPs%2520by%2520about%252062.6%2525%2520%2528compared%2520to%2520CricaVPR%2529.Code%2520is%2520available%2520at%2520https%253A//github.com/tony19980810/D2VPR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12528v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D%24%5E%7B2%7D%24-VPR%3A%20A%20Parameter-efficient%20Visual-foundation-model-based%20Visual%20Place%20Recognition%20Method%20via%20Knowledge%20Distillation%20and%20Deformable%20Aggregation&entry.906535625=Zheyuan%20Zhang%20and%20Jiwei%20Zhang%20and%20Boyu%20Zhou%20and%20Linzhimeng%20Duan%20and%20Hong%20Chen&entry.1292438233=Visual%20Place%20Recognition%20%28VPR%29%20aims%20to%20determine%20the%20geographic%20location%20of%20a%20query%20image%20by%20retrieving%20its%20most%20visually%20similar%20counterpart%20from%20a%20geo-tagged%20reference%20database.%20Recently%2C%20the%20emergence%20of%20the%20powerful%20visual%20foundation%20model%2C%20DINOv2%2C%20trained%20in%20a%20self-supervised%20manner%20on%20massive%20datasets%2C%20has%20significantly%20improved%20VPR%20performance.%20This%20improvement%20stems%20from%20DINOv2%27s%20exceptional%20feature%20generalization%20capabilities%20but%20is%20often%20accompanied%20by%20increased%20model%20complexity%20and%20computational%20overhead%20that%20impede%20deployment%20on%20resource-constrained%20devices.%20To%20address%20this%20challenge%2C%20we%20propose%20%24D%5E%7B2%7D%24-VPR%2C%20a%20%24D%24istillation-%20and%20%24D%24eformable-based%20framework%20that%20retains%20the%20strong%20feature%20extraction%20capabilities%20of%20visual%20foundation%20models%20while%20significantly%20reducing%20model%20parameters%20and%20achieving%20a%20more%20favorable%20performance-efficiency%20trade-off.%20Specifically%2C%20first%2C%20we%20employ%20a%20two-stage%20training%20strategy%20that%20integrates%20knowledge%20distillation%20and%20fine-tuning.%20Additionally%2C%20we%20introduce%20a%20Distillation%20Recovery%20Module%20%28DRM%29%20to%20better%20align%20the%20feature%20spaces%20between%20the%20teacher%20and%20student%20models%2C%20thereby%20minimizing%20knowledge%20transfer%20losses%20to%20the%20greatest%20extent%20possible.%20Second%2C%20we%20design%20a%20Top-Down-attention-based%20Deformable%20Aggregator%20%28TDDA%29%20that%20leverages%20global%20semantic%20features%20to%20dynamically%20and%20adaptively%20adjust%20the%20Regions%20of%20Interest%20%28ROI%29%20used%20for%20aggregation%2C%20thereby%20improving%20adaptability%20to%20irregular%20structures.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20competitive%20performance%20compared%20to%20state-of-the-art%20approaches.%20Meanwhile%2C%20it%20reduces%20the%20parameter%20count%20by%20approximately%2064.2%25%20and%20FLOPs%20by%20about%2062.6%25%20%28compared%20to%20CricaVPR%29.Code%20is%20available%20at%20https%3A//github.com/tony19980810/D2VPR.&entry.1838667208=http%3A//arxiv.org/abs/2511.12528v2&entry.124074799=Read"},
{"title": "Challenges and Limitations of Generative AI in Synthesizing Wearable Sensor Data", "author": "Flavio Di Martino and Franca Delmastro", "abstract": "The widespread adoption of wearable sensors has the potential to provide massive and heterogeneous time series data, driving the use of Artificial Intelligence in human sensing applications. However, data collection remains limited due to stringent ethical regulations, privacy concerns, and other constraints, hindering progress in the field. Synthetic data generation, particularly through Generative Adversarial Networks and Diffusion Models, has emerged as a promising solution to mitigate both data scarcity and privacy issues. However, these models are often limited to narrow operational scenarios, such as short-term and unimodal signal patterns. To address this gap, we present a systematic evaluation of state-of-the-art generative models for time series data, explicitly assessing their performance in challenging scenarios such as stress and emotion recognition. Our study examines the extent to which these models can jointly handle multi-modality, capture long-range dependencies, and support conditional generation-core requirements for real-world wearable sensor data generation. To enable a fair and rigorous comparison, we also introduce an evaluation framework that evaluates both the intrinsic fidelity of the generated data and their utility in downstream predictive tasks. Our findings reveal critical limitations in the existing approaches, particularly in maintaining cross-modal consistency, preserving temporal coherence, and ensuring robust performance in train-on-synthetic, test-on-real, and data augmentation scenarios. Finally, we present our future research directions to enhance synthetic time series generation and improve the applicability of generative models in the wearable computing domain.", "link": "http://arxiv.org/abs/2505.14206v2", "date": "2025-12-03", "relevancy": 2.8137, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5753}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5698}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Challenges%20and%20Limitations%20of%20Generative%20AI%20in%20Synthesizing%20Wearable%20Sensor%20Data&body=Title%3A%20Challenges%20and%20Limitations%20of%20Generative%20AI%20in%20Synthesizing%20Wearable%20Sensor%20Data%0AAuthor%3A%20Flavio%20Di%20Martino%20and%20Franca%20Delmastro%0AAbstract%3A%20The%20widespread%20adoption%20of%20wearable%20sensors%20has%20the%20potential%20to%20provide%20massive%20and%20heterogeneous%20time%20series%20data%2C%20driving%20the%20use%20of%20Artificial%20Intelligence%20in%20human%20sensing%20applications.%20However%2C%20data%20collection%20remains%20limited%20due%20to%20stringent%20ethical%20regulations%2C%20privacy%20concerns%2C%20and%20other%20constraints%2C%20hindering%20progress%20in%20the%20field.%20Synthetic%20data%20generation%2C%20particularly%20through%20Generative%20Adversarial%20Networks%20and%20Diffusion%20Models%2C%20has%20emerged%20as%20a%20promising%20solution%20to%20mitigate%20both%20data%20scarcity%20and%20privacy%20issues.%20However%2C%20these%20models%20are%20often%20limited%20to%20narrow%20operational%20scenarios%2C%20such%20as%20short-term%20and%20unimodal%20signal%20patterns.%20To%20address%20this%20gap%2C%20we%20present%20a%20systematic%20evaluation%20of%20state-of-the-art%20generative%20models%20for%20time%20series%20data%2C%20explicitly%20assessing%20their%20performance%20in%20challenging%20scenarios%20such%20as%20stress%20and%20emotion%20recognition.%20Our%20study%20examines%20the%20extent%20to%20which%20these%20models%20can%20jointly%20handle%20multi-modality%2C%20capture%20long-range%20dependencies%2C%20and%20support%20conditional%20generation-core%20requirements%20for%20real-world%20wearable%20sensor%20data%20generation.%20To%20enable%20a%20fair%20and%20rigorous%20comparison%2C%20we%20also%20introduce%20an%20evaluation%20framework%20that%20evaluates%20both%20the%20intrinsic%20fidelity%20of%20the%20generated%20data%20and%20their%20utility%20in%20downstream%20predictive%20tasks.%20Our%20findings%20reveal%20critical%20limitations%20in%20the%20existing%20approaches%2C%20particularly%20in%20maintaining%20cross-modal%20consistency%2C%20preserving%20temporal%20coherence%2C%20and%20ensuring%20robust%20performance%20in%20train-on-synthetic%2C%20test-on-real%2C%20and%20data%20augmentation%20scenarios.%20Finally%2C%20we%20present%20our%20future%20research%20directions%20to%20enhance%20synthetic%20time%20series%20generation%20and%20improve%20the%20applicability%20of%20generative%20models%20in%20the%20wearable%20computing%20domain.%0ALink%3A%20http%3A//arxiv.org/abs/2505.14206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChallenges%2520and%2520Limitations%2520of%2520Generative%2520AI%2520in%2520Synthesizing%2520Wearable%2520Sensor%2520Data%26entry.906535625%3DFlavio%2520Di%2520Martino%2520and%2520Franca%2520Delmastro%26entry.1292438233%3DThe%2520widespread%2520adoption%2520of%2520wearable%2520sensors%2520has%2520the%2520potential%2520to%2520provide%2520massive%2520and%2520heterogeneous%2520time%2520series%2520data%252C%2520driving%2520the%2520use%2520of%2520Artificial%2520Intelligence%2520in%2520human%2520sensing%2520applications.%2520However%252C%2520data%2520collection%2520remains%2520limited%2520due%2520to%2520stringent%2520ethical%2520regulations%252C%2520privacy%2520concerns%252C%2520and%2520other%2520constraints%252C%2520hindering%2520progress%2520in%2520the%2520field.%2520Synthetic%2520data%2520generation%252C%2520particularly%2520through%2520Generative%2520Adversarial%2520Networks%2520and%2520Diffusion%2520Models%252C%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520to%2520mitigate%2520both%2520data%2520scarcity%2520and%2520privacy%2520issues.%2520However%252C%2520these%2520models%2520are%2520often%2520limited%2520to%2520narrow%2520operational%2520scenarios%252C%2520such%2520as%2520short-term%2520and%2520unimodal%2520signal%2520patterns.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520a%2520systematic%2520evaluation%2520of%2520state-of-the-art%2520generative%2520models%2520for%2520time%2520series%2520data%252C%2520explicitly%2520assessing%2520their%2520performance%2520in%2520challenging%2520scenarios%2520such%2520as%2520stress%2520and%2520emotion%2520recognition.%2520Our%2520study%2520examines%2520the%2520extent%2520to%2520which%2520these%2520models%2520can%2520jointly%2520handle%2520multi-modality%252C%2520capture%2520long-range%2520dependencies%252C%2520and%2520support%2520conditional%2520generation-core%2520requirements%2520for%2520real-world%2520wearable%2520sensor%2520data%2520generation.%2520To%2520enable%2520a%2520fair%2520and%2520rigorous%2520comparison%252C%2520we%2520also%2520introduce%2520an%2520evaluation%2520framework%2520that%2520evaluates%2520both%2520the%2520intrinsic%2520fidelity%2520of%2520the%2520generated%2520data%2520and%2520their%2520utility%2520in%2520downstream%2520predictive%2520tasks.%2520Our%2520findings%2520reveal%2520critical%2520limitations%2520in%2520the%2520existing%2520approaches%252C%2520particularly%2520in%2520maintaining%2520cross-modal%2520consistency%252C%2520preserving%2520temporal%2520coherence%252C%2520and%2520ensuring%2520robust%2520performance%2520in%2520train-on-synthetic%252C%2520test-on-real%252C%2520and%2520data%2520augmentation%2520scenarios.%2520Finally%252C%2520we%2520present%2520our%2520future%2520research%2520directions%2520to%2520enhance%2520synthetic%2520time%2520series%2520generation%2520and%2520improve%2520the%2520applicability%2520of%2520generative%2520models%2520in%2520the%2520wearable%2520computing%2520domain.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Challenges%20and%20Limitations%20of%20Generative%20AI%20in%20Synthesizing%20Wearable%20Sensor%20Data&entry.906535625=Flavio%20Di%20Martino%20and%20Franca%20Delmastro&entry.1292438233=The%20widespread%20adoption%20of%20wearable%20sensors%20has%20the%20potential%20to%20provide%20massive%20and%20heterogeneous%20time%20series%20data%2C%20driving%20the%20use%20of%20Artificial%20Intelligence%20in%20human%20sensing%20applications.%20However%2C%20data%20collection%20remains%20limited%20due%20to%20stringent%20ethical%20regulations%2C%20privacy%20concerns%2C%20and%20other%20constraints%2C%20hindering%20progress%20in%20the%20field.%20Synthetic%20data%20generation%2C%20particularly%20through%20Generative%20Adversarial%20Networks%20and%20Diffusion%20Models%2C%20has%20emerged%20as%20a%20promising%20solution%20to%20mitigate%20both%20data%20scarcity%20and%20privacy%20issues.%20However%2C%20these%20models%20are%20often%20limited%20to%20narrow%20operational%20scenarios%2C%20such%20as%20short-term%20and%20unimodal%20signal%20patterns.%20To%20address%20this%20gap%2C%20we%20present%20a%20systematic%20evaluation%20of%20state-of-the-art%20generative%20models%20for%20time%20series%20data%2C%20explicitly%20assessing%20their%20performance%20in%20challenging%20scenarios%20such%20as%20stress%20and%20emotion%20recognition.%20Our%20study%20examines%20the%20extent%20to%20which%20these%20models%20can%20jointly%20handle%20multi-modality%2C%20capture%20long-range%20dependencies%2C%20and%20support%20conditional%20generation-core%20requirements%20for%20real-world%20wearable%20sensor%20data%20generation.%20To%20enable%20a%20fair%20and%20rigorous%20comparison%2C%20we%20also%20introduce%20an%20evaluation%20framework%20that%20evaluates%20both%20the%20intrinsic%20fidelity%20of%20the%20generated%20data%20and%20their%20utility%20in%20downstream%20predictive%20tasks.%20Our%20findings%20reveal%20critical%20limitations%20in%20the%20existing%20approaches%2C%20particularly%20in%20maintaining%20cross-modal%20consistency%2C%20preserving%20temporal%20coherence%2C%20and%20ensuring%20robust%20performance%20in%20train-on-synthetic%2C%20test-on-real%2C%20and%20data%20augmentation%20scenarios.%20Finally%2C%20we%20present%20our%20future%20research%20directions%20to%20enhance%20synthetic%20time%20series%20generation%20and%20improve%20the%20applicability%20of%20generative%20models%20in%20the%20wearable%20computing%20domain.&entry.1838667208=http%3A//arxiv.org/abs/2505.14206v2&entry.124074799=Read"},
{"title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation", "author": "Xiaolong Li and Youping Gu and Xi Lin and Weijie Wang and Bohan Zhuang", "abstract": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA", "link": "http://arxiv.org/abs/2512.04025v1", "date": "2025-12-03", "relevancy": 2.7646, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.592}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5375}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSA%3A%20Pyramid%20Sparse%20Attention%20for%20Efficient%20Video%20Understanding%20and%20Generation&body=Title%3A%20PSA%3A%20Pyramid%20Sparse%20Attention%20for%20Efficient%20Video%20Understanding%20and%20Generation%0AAuthor%3A%20Xiaolong%20Li%20and%20Youping%20Gu%20and%20Xi%20Lin%20and%20Weijie%20Wang%20and%20Bohan%20Zhuang%0AAbstract%3A%20Attention%20mechanisms%20are%20the%20core%20of%20foundation%20models%2C%20but%20their%20quadratic%20complexity%20remains%20a%20critical%20bottleneck%20for%20scaling.%20This%20challenge%20has%20driven%20the%20development%20of%20efficient%20attention%20mechanisms%2C%20with%20sparsity%20emerging%20as%20the%20dominant%20paradigm.%20Current%20methods%20typically%20retain%20or%20discard%20entire%20key-value%20blocks%20with%20binary%20masks%2C%20resulting%20in%20substantial%20information%20loss%20under%20high%20sparsity.%20To%20mitigate%20this%20gap%2C%20we%20present%20Pyramid%20Sparse%20Attention%20%28PSA%29%2C%20a%20versatile%20module%20applicable%20to%20both%20video%20understanding%20and%20generation%20tasks.%20Instead%20of%20binary%20masking%2C%20PSA%20introduces%20multi-level%20pooled%20KV%20representations%2C%20enabling%20finer%20mask%20granularity.%20Specifically%2C%20each%20query%20block%20dynamically%20allocates%20lower%20pooling%20levels%20to%20critical%20KV%20blocks%20and%20higher%20levels%20to%20less%20important%20ones%2C%20creating%20an%20informative%20interpolation%20between%20full%20retention%20and%20complete%20pruning.%20This%20design%2C%20analogous%20to%20fixed-point%20quantization%20and%20classical%20feature%20pyramid%20networks%20in%20computer%20vision%2C%20effectively%20mitigates%20information%20loss%20while%20preserving%20computational%20efficiency%20under%20a%20low%20compute%20budget.%20It%20works%20with%20a%20native%2C%20hardware-friendly%20kernel%20that%20leverages%20decoupled%20block-tile%20design%20to%20ensure%20efficient%20execution.%20Across%20video%20understanding%20and%20generation%20benchmarks%2C%20PSA%20preserves%20contextual%20information%20and%20visual%20fidelity%2C%20consistently%20outperforming%20or%20achieving%20comparable%20performance%20over%20existing%20sparse%20attention%20baselines%20with%20superior%20efficiency-quality%20trade-offs.%20Our%20code%20and%20model%20weights%20are%20publicly%20available%20at%3A%20http%3A//ziplab.co/PSA%0ALink%3A%20http%3A//arxiv.org/abs/2512.04025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSA%253A%2520Pyramid%2520Sparse%2520Attention%2520for%2520Efficient%2520Video%2520Understanding%2520and%2520Generation%26entry.906535625%3DXiaolong%2520Li%2520and%2520Youping%2520Gu%2520and%2520Xi%2520Lin%2520and%2520Weijie%2520Wang%2520and%2520Bohan%2520Zhuang%26entry.1292438233%3DAttention%2520mechanisms%2520are%2520the%2520core%2520of%2520foundation%2520models%252C%2520but%2520their%2520quadratic%2520complexity%2520remains%2520a%2520critical%2520bottleneck%2520for%2520scaling.%2520This%2520challenge%2520has%2520driven%2520the%2520development%2520of%2520efficient%2520attention%2520mechanisms%252C%2520with%2520sparsity%2520emerging%2520as%2520the%2520dominant%2520paradigm.%2520Current%2520methods%2520typically%2520retain%2520or%2520discard%2520entire%2520key-value%2520blocks%2520with%2520binary%2520masks%252C%2520resulting%2520in%2520substantial%2520information%2520loss%2520under%2520high%2520sparsity.%2520To%2520mitigate%2520this%2520gap%252C%2520we%2520present%2520Pyramid%2520Sparse%2520Attention%2520%2528PSA%2529%252C%2520a%2520versatile%2520module%2520applicable%2520to%2520both%2520video%2520understanding%2520and%2520generation%2520tasks.%2520Instead%2520of%2520binary%2520masking%252C%2520PSA%2520introduces%2520multi-level%2520pooled%2520KV%2520representations%252C%2520enabling%2520finer%2520mask%2520granularity.%2520Specifically%252C%2520each%2520query%2520block%2520dynamically%2520allocates%2520lower%2520pooling%2520levels%2520to%2520critical%2520KV%2520blocks%2520and%2520higher%2520levels%2520to%2520less%2520important%2520ones%252C%2520creating%2520an%2520informative%2520interpolation%2520between%2520full%2520retention%2520and%2520complete%2520pruning.%2520This%2520design%252C%2520analogous%2520to%2520fixed-point%2520quantization%2520and%2520classical%2520feature%2520pyramid%2520networks%2520in%2520computer%2520vision%252C%2520effectively%2520mitigates%2520information%2520loss%2520while%2520preserving%2520computational%2520efficiency%2520under%2520a%2520low%2520compute%2520budget.%2520It%2520works%2520with%2520a%2520native%252C%2520hardware-friendly%2520kernel%2520that%2520leverages%2520decoupled%2520block-tile%2520design%2520to%2520ensure%2520efficient%2520execution.%2520Across%2520video%2520understanding%2520and%2520generation%2520benchmarks%252C%2520PSA%2520preserves%2520contextual%2520information%2520and%2520visual%2520fidelity%252C%2520consistently%2520outperforming%2520or%2520achieving%2520comparable%2520performance%2520over%2520existing%2520sparse%2520attention%2520baselines%2520with%2520superior%2520efficiency-quality%2520trade-offs.%2520Our%2520code%2520and%2520model%2520weights%2520are%2520publicly%2520available%2520at%253A%2520http%253A//ziplab.co/PSA%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSA%3A%20Pyramid%20Sparse%20Attention%20for%20Efficient%20Video%20Understanding%20and%20Generation&entry.906535625=Xiaolong%20Li%20and%20Youping%20Gu%20and%20Xi%20Lin%20and%20Weijie%20Wang%20and%20Bohan%20Zhuang&entry.1292438233=Attention%20mechanisms%20are%20the%20core%20of%20foundation%20models%2C%20but%20their%20quadratic%20complexity%20remains%20a%20critical%20bottleneck%20for%20scaling.%20This%20challenge%20has%20driven%20the%20development%20of%20efficient%20attention%20mechanisms%2C%20with%20sparsity%20emerging%20as%20the%20dominant%20paradigm.%20Current%20methods%20typically%20retain%20or%20discard%20entire%20key-value%20blocks%20with%20binary%20masks%2C%20resulting%20in%20substantial%20information%20loss%20under%20high%20sparsity.%20To%20mitigate%20this%20gap%2C%20we%20present%20Pyramid%20Sparse%20Attention%20%28PSA%29%2C%20a%20versatile%20module%20applicable%20to%20both%20video%20understanding%20and%20generation%20tasks.%20Instead%20of%20binary%20masking%2C%20PSA%20introduces%20multi-level%20pooled%20KV%20representations%2C%20enabling%20finer%20mask%20granularity.%20Specifically%2C%20each%20query%20block%20dynamically%20allocates%20lower%20pooling%20levels%20to%20critical%20KV%20blocks%20and%20higher%20levels%20to%20less%20important%20ones%2C%20creating%20an%20informative%20interpolation%20between%20full%20retention%20and%20complete%20pruning.%20This%20design%2C%20analogous%20to%20fixed-point%20quantization%20and%20classical%20feature%20pyramid%20networks%20in%20computer%20vision%2C%20effectively%20mitigates%20information%20loss%20while%20preserving%20computational%20efficiency%20under%20a%20low%20compute%20budget.%20It%20works%20with%20a%20native%2C%20hardware-friendly%20kernel%20that%20leverages%20decoupled%20block-tile%20design%20to%20ensure%20efficient%20execution.%20Across%20video%20understanding%20and%20generation%20benchmarks%2C%20PSA%20preserves%20contextual%20information%20and%20visual%20fidelity%2C%20consistently%20outperforming%20or%20achieving%20comparable%20performance%20over%20existing%20sparse%20attention%20baselines%20with%20superior%20efficiency-quality%20trade-offs.%20Our%20code%20and%20model%20weights%20are%20publicly%20available%20at%3A%20http%3A//ziplab.co/PSA&entry.1838667208=http%3A//arxiv.org/abs/2512.04025v1&entry.124074799=Read"},
{"title": "Radiance Meshes for Volumetric Reconstruction", "author": "Alexander Mai and Trevor Hedstrom and George Kopanas and Janne Kontkanen and Falko Kuester and Jonathan T. Barron", "abstract": "We introduce radiance meshes, a technique for representing radiance fields with constant density tetrahedral cells produced with a Delaunay tetrahedralization. Unlike a Voronoi diagram, a Delaunay tetrahedralization yields simple triangles that are natively supported by existing hardware. As such, our model is able to perform exact and fast volume rendering using both rasterization and ray-tracing. We introduce a new rasterization method that achieves faster rendering speeds than all prior radiance field representations (assuming an equivalent number of primitives and resolution) across a variety of platforms. Optimizing the positions of Delaunay vertices introduces topological discontinuities (edge flips). To solve this, we use a Zip-NeRF-style backbone which allows us to express a smoothly varying field even when the topology changes. Our rendering method exactly evaluates the volume rendering equation and enables high quality, real-time view synthesis on standard consumer hardware. Our tetrahedral meshes also lend themselves to a variety of exciting applications including fisheye lens distortion, physics-based simulation, editing, and mesh extraction.", "link": "http://arxiv.org/abs/2512.04076v1", "date": "2025-12-03", "relevancy": 2.7208, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5568}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5401}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Radiance%20Meshes%20for%20Volumetric%20Reconstruction&body=Title%3A%20Radiance%20Meshes%20for%20Volumetric%20Reconstruction%0AAuthor%3A%20Alexander%20Mai%20and%20Trevor%20Hedstrom%20and%20George%20Kopanas%20and%20Janne%20Kontkanen%20and%20Falko%20Kuester%20and%20Jonathan%20T.%20Barron%0AAbstract%3A%20We%20introduce%20radiance%20meshes%2C%20a%20technique%20for%20representing%20radiance%20fields%20with%20constant%20density%20tetrahedral%20cells%20produced%20with%20a%20Delaunay%20tetrahedralization.%20Unlike%20a%20Voronoi%20diagram%2C%20a%20Delaunay%20tetrahedralization%20yields%20simple%20triangles%20that%20are%20natively%20supported%20by%20existing%20hardware.%20As%20such%2C%20our%20model%20is%20able%20to%20perform%20exact%20and%20fast%20volume%20rendering%20using%20both%20rasterization%20and%20ray-tracing.%20We%20introduce%20a%20new%20rasterization%20method%20that%20achieves%20faster%20rendering%20speeds%20than%20all%20prior%20radiance%20field%20representations%20%28assuming%20an%20equivalent%20number%20of%20primitives%20and%20resolution%29%20across%20a%20variety%20of%20platforms.%20Optimizing%20the%20positions%20of%20Delaunay%20vertices%20introduces%20topological%20discontinuities%20%28edge%20flips%29.%20To%20solve%20this%2C%20we%20use%20a%20Zip-NeRF-style%20backbone%20which%20allows%20us%20to%20express%20a%20smoothly%20varying%20field%20even%20when%20the%20topology%20changes.%20Our%20rendering%20method%20exactly%20evaluates%20the%20volume%20rendering%20equation%20and%20enables%20high%20quality%2C%20real-time%20view%20synthesis%20on%20standard%20consumer%20hardware.%20Our%20tetrahedral%20meshes%20also%20lend%20themselves%20to%20a%20variety%20of%20exciting%20applications%20including%20fisheye%20lens%20distortion%2C%20physics-based%20simulation%2C%20editing%2C%20and%20mesh%20extraction.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadiance%2520Meshes%2520for%2520Volumetric%2520Reconstruction%26entry.906535625%3DAlexander%2520Mai%2520and%2520Trevor%2520Hedstrom%2520and%2520George%2520Kopanas%2520and%2520Janne%2520Kontkanen%2520and%2520Falko%2520Kuester%2520and%2520Jonathan%2520T.%2520Barron%26entry.1292438233%3DWe%2520introduce%2520radiance%2520meshes%252C%2520a%2520technique%2520for%2520representing%2520radiance%2520fields%2520with%2520constant%2520density%2520tetrahedral%2520cells%2520produced%2520with%2520a%2520Delaunay%2520tetrahedralization.%2520Unlike%2520a%2520Voronoi%2520diagram%252C%2520a%2520Delaunay%2520tetrahedralization%2520yields%2520simple%2520triangles%2520that%2520are%2520natively%2520supported%2520by%2520existing%2520hardware.%2520As%2520such%252C%2520our%2520model%2520is%2520able%2520to%2520perform%2520exact%2520and%2520fast%2520volume%2520rendering%2520using%2520both%2520rasterization%2520and%2520ray-tracing.%2520We%2520introduce%2520a%2520new%2520rasterization%2520method%2520that%2520achieves%2520faster%2520rendering%2520speeds%2520than%2520all%2520prior%2520radiance%2520field%2520representations%2520%2528assuming%2520an%2520equivalent%2520number%2520of%2520primitives%2520and%2520resolution%2529%2520across%2520a%2520variety%2520of%2520platforms.%2520Optimizing%2520the%2520positions%2520of%2520Delaunay%2520vertices%2520introduces%2520topological%2520discontinuities%2520%2528edge%2520flips%2529.%2520To%2520solve%2520this%252C%2520we%2520use%2520a%2520Zip-NeRF-style%2520backbone%2520which%2520allows%2520us%2520to%2520express%2520a%2520smoothly%2520varying%2520field%2520even%2520when%2520the%2520topology%2520changes.%2520Our%2520rendering%2520method%2520exactly%2520evaluates%2520the%2520volume%2520rendering%2520equation%2520and%2520enables%2520high%2520quality%252C%2520real-time%2520view%2520synthesis%2520on%2520standard%2520consumer%2520hardware.%2520Our%2520tetrahedral%2520meshes%2520also%2520lend%2520themselves%2520to%2520a%2520variety%2520of%2520exciting%2520applications%2520including%2520fisheye%2520lens%2520distortion%252C%2520physics-based%2520simulation%252C%2520editing%252C%2520and%2520mesh%2520extraction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radiance%20Meshes%20for%20Volumetric%20Reconstruction&entry.906535625=Alexander%20Mai%20and%20Trevor%20Hedstrom%20and%20George%20Kopanas%20and%20Janne%20Kontkanen%20and%20Falko%20Kuester%20and%20Jonathan%20T.%20Barron&entry.1292438233=We%20introduce%20radiance%20meshes%2C%20a%20technique%20for%20representing%20radiance%20fields%20with%20constant%20density%20tetrahedral%20cells%20produced%20with%20a%20Delaunay%20tetrahedralization.%20Unlike%20a%20Voronoi%20diagram%2C%20a%20Delaunay%20tetrahedralization%20yields%20simple%20triangles%20that%20are%20natively%20supported%20by%20existing%20hardware.%20As%20such%2C%20our%20model%20is%20able%20to%20perform%20exact%20and%20fast%20volume%20rendering%20using%20both%20rasterization%20and%20ray-tracing.%20We%20introduce%20a%20new%20rasterization%20method%20that%20achieves%20faster%20rendering%20speeds%20than%20all%20prior%20radiance%20field%20representations%20%28assuming%20an%20equivalent%20number%20of%20primitives%20and%20resolution%29%20across%20a%20variety%20of%20platforms.%20Optimizing%20the%20positions%20of%20Delaunay%20vertices%20introduces%20topological%20discontinuities%20%28edge%20flips%29.%20To%20solve%20this%2C%20we%20use%20a%20Zip-NeRF-style%20backbone%20which%20allows%20us%20to%20express%20a%20smoothly%20varying%20field%20even%20when%20the%20topology%20changes.%20Our%20rendering%20method%20exactly%20evaluates%20the%20volume%20rendering%20equation%20and%20enables%20high%20quality%2C%20real-time%20view%20synthesis%20on%20standard%20consumer%20hardware.%20Our%20tetrahedral%20meshes%20also%20lend%20themselves%20to%20a%20variety%20of%20exciting%20applications%20including%20fisheye%20lens%20distortion%2C%20physics-based%20simulation%2C%20editing%2C%20and%20mesh%20extraction.&entry.1838667208=http%3A//arxiv.org/abs/2512.04076v1&entry.124074799=Read"},
{"title": "BERnaT: Basque Encoders for Representing Natural Textual Diversity", "author": "Ekhi Azurmendi and Joseba Fernandez de Landa and Jaione Bengoetxea and Maite Heredia and Julen Etxaniz and Mikel Zubillaga and Ander Soraluze and Aitor Soroa", "abstract": "Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.", "link": "http://arxiv.org/abs/2512.03903v1", "date": "2025-12-03", "relevancy": 2.7181, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5572}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BERnaT%3A%20Basque%20Encoders%20for%20Representing%20Natural%20Textual%20Diversity&body=Title%3A%20BERnaT%3A%20Basque%20Encoders%20for%20Representing%20Natural%20Textual%20Diversity%0AAuthor%3A%20Ekhi%20Azurmendi%20and%20Joseba%20Fernandez%20de%20Landa%20and%20Jaione%20Bengoetxea%20and%20Maite%20Heredia%20and%20Julen%20Etxaniz%20and%20Mikel%20Zubillaga%20and%20Ander%20Soraluze%20and%20Aitor%20Soroa%0AAbstract%3A%20Language%20models%20depend%20on%20massive%20text%20corpora%20that%20are%20often%20filtered%20for%20quality%2C%20a%20process%20that%20can%20unintentionally%20exclude%20non-standard%20linguistic%20varieties%2C%20reduce%20model%20robustness%20and%20reinforce%20representational%20biases.%20In%20this%20paper%2C%20we%20argue%20that%20language%20models%20should%20aim%20to%20capture%20the%20full%20spectrum%20of%20language%20variation%20%28dialectal%2C%20historical%2C%20informal%2C%20etc.%29%20rather%20than%20relying%20solely%20on%20standardized%20text.%20Focusing%20on%20Basque%2C%20a%20morphologically%20rich%20and%20low-resource%20language%2C%20we%20construct%20new%20corpora%20combining%20standard%2C%20social%20media%2C%20and%20historical%20sources%2C%20and%20pre-train%20the%20BERnaT%20family%20of%20encoder-only%20models%20in%20three%20configurations%3A%20standard%2C%20diverse%2C%20and%20combined.%20We%20further%20propose%20an%20evaluation%20framework%20that%20separates%20Natural%20Language%20Understanding%20%28NLU%29%20tasks%20into%20standard%20and%20diverse%20subsets%20to%20assess%20linguistic%20generalization.%20Results%20show%20that%20models%20trained%20on%20both%20standard%20and%20diverse%20data%20consistently%20outperform%20those%20trained%20on%20standard%20corpora%2C%20improving%20performance%20across%20all%20task%20types%20without%20compromising%20standard%20benchmark%20accuracy.%20These%20findings%20highlight%20the%20importance%20of%20linguistic%20diversity%20in%20building%20inclusive%2C%20generalizable%20language%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBERnaT%253A%2520Basque%2520Encoders%2520for%2520Representing%2520Natural%2520Textual%2520Diversity%26entry.906535625%3DEkhi%2520Azurmendi%2520and%2520Joseba%2520Fernandez%2520de%2520Landa%2520and%2520Jaione%2520Bengoetxea%2520and%2520Maite%2520Heredia%2520and%2520Julen%2520Etxaniz%2520and%2520Mikel%2520Zubillaga%2520and%2520Ander%2520Soraluze%2520and%2520Aitor%2520Soroa%26entry.1292438233%3DLanguage%2520models%2520depend%2520on%2520massive%2520text%2520corpora%2520that%2520are%2520often%2520filtered%2520for%2520quality%252C%2520a%2520process%2520that%2520can%2520unintentionally%2520exclude%2520non-standard%2520linguistic%2520varieties%252C%2520reduce%2520model%2520robustness%2520and%2520reinforce%2520representational%2520biases.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520language%2520models%2520should%2520aim%2520to%2520capture%2520the%2520full%2520spectrum%2520of%2520language%2520variation%2520%2528dialectal%252C%2520historical%252C%2520informal%252C%2520etc.%2529%2520rather%2520than%2520relying%2520solely%2520on%2520standardized%2520text.%2520Focusing%2520on%2520Basque%252C%2520a%2520morphologically%2520rich%2520and%2520low-resource%2520language%252C%2520we%2520construct%2520new%2520corpora%2520combining%2520standard%252C%2520social%2520media%252C%2520and%2520historical%2520sources%252C%2520and%2520pre-train%2520the%2520BERnaT%2520family%2520of%2520encoder-only%2520models%2520in%2520three%2520configurations%253A%2520standard%252C%2520diverse%252C%2520and%2520combined.%2520We%2520further%2520propose%2520an%2520evaluation%2520framework%2520that%2520separates%2520Natural%2520Language%2520Understanding%2520%2528NLU%2529%2520tasks%2520into%2520standard%2520and%2520diverse%2520subsets%2520to%2520assess%2520linguistic%2520generalization.%2520Results%2520show%2520that%2520models%2520trained%2520on%2520both%2520standard%2520and%2520diverse%2520data%2520consistently%2520outperform%2520those%2520trained%2520on%2520standard%2520corpora%252C%2520improving%2520performance%2520across%2520all%2520task%2520types%2520without%2520compromising%2520standard%2520benchmark%2520accuracy.%2520These%2520findings%2520highlight%2520the%2520importance%2520of%2520linguistic%2520diversity%2520in%2520building%2520inclusive%252C%2520generalizable%2520language%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BERnaT%3A%20Basque%20Encoders%20for%20Representing%20Natural%20Textual%20Diversity&entry.906535625=Ekhi%20Azurmendi%20and%20Joseba%20Fernandez%20de%20Landa%20and%20Jaione%20Bengoetxea%20and%20Maite%20Heredia%20and%20Julen%20Etxaniz%20and%20Mikel%20Zubillaga%20and%20Ander%20Soraluze%20and%20Aitor%20Soroa&entry.1292438233=Language%20models%20depend%20on%20massive%20text%20corpora%20that%20are%20often%20filtered%20for%20quality%2C%20a%20process%20that%20can%20unintentionally%20exclude%20non-standard%20linguistic%20varieties%2C%20reduce%20model%20robustness%20and%20reinforce%20representational%20biases.%20In%20this%20paper%2C%20we%20argue%20that%20language%20models%20should%20aim%20to%20capture%20the%20full%20spectrum%20of%20language%20variation%20%28dialectal%2C%20historical%2C%20informal%2C%20etc.%29%20rather%20than%20relying%20solely%20on%20standardized%20text.%20Focusing%20on%20Basque%2C%20a%20morphologically%20rich%20and%20low-resource%20language%2C%20we%20construct%20new%20corpora%20combining%20standard%2C%20social%20media%2C%20and%20historical%20sources%2C%20and%20pre-train%20the%20BERnaT%20family%20of%20encoder-only%20models%20in%20three%20configurations%3A%20standard%2C%20diverse%2C%20and%20combined.%20We%20further%20propose%20an%20evaluation%20framework%20that%20separates%20Natural%20Language%20Understanding%20%28NLU%29%20tasks%20into%20standard%20and%20diverse%20subsets%20to%20assess%20linguistic%20generalization.%20Results%20show%20that%20models%20trained%20on%20both%20standard%20and%20diverse%20data%20consistently%20outperform%20those%20trained%20on%20standard%20corpora%2C%20improving%20performance%20across%20all%20task%20types%20without%20compromising%20standard%20benchmark%20accuracy.%20These%20findings%20highlight%20the%20importance%20of%20linguistic%20diversity%20in%20building%20inclusive%2C%20generalizable%20language%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.03903v1&entry.124074799=Read"},
{"title": "Universally Converging Representations of Matter Across Scientific Foundation Models", "author": "Sathya Edamadaka and Soojung Yang and Ju Li and Rafael G\u00f3mez-Bombarelli", "abstract": "Machine learning models of vastly different modalities and architectures are being trained to predict the behavior of molecules, materials, and proteins. However, it remains unclear whether they learn similar internal representations of matter. Understanding their latent structure is essential for building scientific foundation models that generalize reliably beyond their training domains. Although representational convergence has been observed in language and vision, its counterpart in the sciences has not been systematically explored. Here, we show that representations learned by nearly sixty scientific models, spanning string-, graph-, 3D atomistic, and protein-based modalities, are highly aligned across a wide range of chemical systems. Models trained on different datasets have highly similar representations of small molecules, and machine learning interatomic potentials converge in representation space as they improve in performance, suggesting that foundation models learn a common underlying representation of physical reality. We then show two distinct regimes of scientific models: on inputs similar to those seen during training, high-performing models align closely and weak models diverge into local sub-optima in representation space; on vastly different structures from those seen during training, nearly all models collapse onto a low-information representation, indicating that today's models remain limited by training data and inductive bias and do not yet encode truly universal structure. Our findings establish representational alignment as a quantitative benchmark for foundation-level generality in scientific models. More broadly, our work can track the emergence of universal representations of matter as models scale, and for selecting and distilling models whose learned representations transfer best across modalities, domains of matter, and scientific tasks.", "link": "http://arxiv.org/abs/2512.03750v1", "date": "2025-12-03", "relevancy": 2.6757, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universally%20Converging%20Representations%20of%20Matter%20Across%20Scientific%20Foundation%20Models&body=Title%3A%20Universally%20Converging%20Representations%20of%20Matter%20Across%20Scientific%20Foundation%20Models%0AAuthor%3A%20Sathya%20Edamadaka%20and%20Soojung%20Yang%20and%20Ju%20Li%20and%20Rafael%20G%C3%B3mez-Bombarelli%0AAbstract%3A%20Machine%20learning%20models%20of%20vastly%20different%20modalities%20and%20architectures%20are%20being%20trained%20to%20predict%20the%20behavior%20of%20molecules%2C%20materials%2C%20and%20proteins.%20However%2C%20it%20remains%20unclear%20whether%20they%20learn%20similar%20internal%20representations%20of%20matter.%20Understanding%20their%20latent%20structure%20is%20essential%20for%20building%20scientific%20foundation%20models%20that%20generalize%20reliably%20beyond%20their%20training%20domains.%20Although%20representational%20convergence%20has%20been%20observed%20in%20language%20and%20vision%2C%20its%20counterpart%20in%20the%20sciences%20has%20not%20been%20systematically%20explored.%20Here%2C%20we%20show%20that%20representations%20learned%20by%20nearly%20sixty%20scientific%20models%2C%20spanning%20string-%2C%20graph-%2C%203D%20atomistic%2C%20and%20protein-based%20modalities%2C%20are%20highly%20aligned%20across%20a%20wide%20range%20of%20chemical%20systems.%20Models%20trained%20on%20different%20datasets%20have%20highly%20similar%20representations%20of%20small%20molecules%2C%20and%20machine%20learning%20interatomic%20potentials%20converge%20in%20representation%20space%20as%20they%20improve%20in%20performance%2C%20suggesting%20that%20foundation%20models%20learn%20a%20common%20underlying%20representation%20of%20physical%20reality.%20We%20then%20show%20two%20distinct%20regimes%20of%20scientific%20models%3A%20on%20inputs%20similar%20to%20those%20seen%20during%20training%2C%20high-performing%20models%20align%20closely%20and%20weak%20models%20diverge%20into%20local%20sub-optima%20in%20representation%20space%3B%20on%20vastly%20different%20structures%20from%20those%20seen%20during%20training%2C%20nearly%20all%20models%20collapse%20onto%20a%20low-information%20representation%2C%20indicating%20that%20today%27s%20models%20remain%20limited%20by%20training%20data%20and%20inductive%20bias%20and%20do%20not%20yet%20encode%20truly%20universal%20structure.%20Our%20findings%20establish%20representational%20alignment%20as%20a%20quantitative%20benchmark%20for%20foundation-level%20generality%20in%20scientific%20models.%20More%20broadly%2C%20our%20work%20can%20track%20the%20emergence%20of%20universal%20representations%20of%20matter%20as%20models%20scale%2C%20and%20for%20selecting%20and%20distilling%20models%20whose%20learned%20representations%20transfer%20best%20across%20modalities%2C%20domains%20of%20matter%2C%20and%20scientific%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversally%2520Converging%2520Representations%2520of%2520Matter%2520Across%2520Scientific%2520Foundation%2520Models%26entry.906535625%3DSathya%2520Edamadaka%2520and%2520Soojung%2520Yang%2520and%2520Ju%2520Li%2520and%2520Rafael%2520G%25C3%25B3mez-Bombarelli%26entry.1292438233%3DMachine%2520learning%2520models%2520of%2520vastly%2520different%2520modalities%2520and%2520architectures%2520are%2520being%2520trained%2520to%2520predict%2520the%2520behavior%2520of%2520molecules%252C%2520materials%252C%2520and%2520proteins.%2520However%252C%2520it%2520remains%2520unclear%2520whether%2520they%2520learn%2520similar%2520internal%2520representations%2520of%2520matter.%2520Understanding%2520their%2520latent%2520structure%2520is%2520essential%2520for%2520building%2520scientific%2520foundation%2520models%2520that%2520generalize%2520reliably%2520beyond%2520their%2520training%2520domains.%2520Although%2520representational%2520convergence%2520has%2520been%2520observed%2520in%2520language%2520and%2520vision%252C%2520its%2520counterpart%2520in%2520the%2520sciences%2520has%2520not%2520been%2520systematically%2520explored.%2520Here%252C%2520we%2520show%2520that%2520representations%2520learned%2520by%2520nearly%2520sixty%2520scientific%2520models%252C%2520spanning%2520string-%252C%2520graph-%252C%25203D%2520atomistic%252C%2520and%2520protein-based%2520modalities%252C%2520are%2520highly%2520aligned%2520across%2520a%2520wide%2520range%2520of%2520chemical%2520systems.%2520Models%2520trained%2520on%2520different%2520datasets%2520have%2520highly%2520similar%2520representations%2520of%2520small%2520molecules%252C%2520and%2520machine%2520learning%2520interatomic%2520potentials%2520converge%2520in%2520representation%2520space%2520as%2520they%2520improve%2520in%2520performance%252C%2520suggesting%2520that%2520foundation%2520models%2520learn%2520a%2520common%2520underlying%2520representation%2520of%2520physical%2520reality.%2520We%2520then%2520show%2520two%2520distinct%2520regimes%2520of%2520scientific%2520models%253A%2520on%2520inputs%2520similar%2520to%2520those%2520seen%2520during%2520training%252C%2520high-performing%2520models%2520align%2520closely%2520and%2520weak%2520models%2520diverge%2520into%2520local%2520sub-optima%2520in%2520representation%2520space%253B%2520on%2520vastly%2520different%2520structures%2520from%2520those%2520seen%2520during%2520training%252C%2520nearly%2520all%2520models%2520collapse%2520onto%2520a%2520low-information%2520representation%252C%2520indicating%2520that%2520today%2527s%2520models%2520remain%2520limited%2520by%2520training%2520data%2520and%2520inductive%2520bias%2520and%2520do%2520not%2520yet%2520encode%2520truly%2520universal%2520structure.%2520Our%2520findings%2520establish%2520representational%2520alignment%2520as%2520a%2520quantitative%2520benchmark%2520for%2520foundation-level%2520generality%2520in%2520scientific%2520models.%2520More%2520broadly%252C%2520our%2520work%2520can%2520track%2520the%2520emergence%2520of%2520universal%2520representations%2520of%2520matter%2520as%2520models%2520scale%252C%2520and%2520for%2520selecting%2520and%2520distilling%2520models%2520whose%2520learned%2520representations%2520transfer%2520best%2520across%2520modalities%252C%2520domains%2520of%2520matter%252C%2520and%2520scientific%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universally%20Converging%20Representations%20of%20Matter%20Across%20Scientific%20Foundation%20Models&entry.906535625=Sathya%20Edamadaka%20and%20Soojung%20Yang%20and%20Ju%20Li%20and%20Rafael%20G%C3%B3mez-Bombarelli&entry.1292438233=Machine%20learning%20models%20of%20vastly%20different%20modalities%20and%20architectures%20are%20being%20trained%20to%20predict%20the%20behavior%20of%20molecules%2C%20materials%2C%20and%20proteins.%20However%2C%20it%20remains%20unclear%20whether%20they%20learn%20similar%20internal%20representations%20of%20matter.%20Understanding%20their%20latent%20structure%20is%20essential%20for%20building%20scientific%20foundation%20models%20that%20generalize%20reliably%20beyond%20their%20training%20domains.%20Although%20representational%20convergence%20has%20been%20observed%20in%20language%20and%20vision%2C%20its%20counterpart%20in%20the%20sciences%20has%20not%20been%20systematically%20explored.%20Here%2C%20we%20show%20that%20representations%20learned%20by%20nearly%20sixty%20scientific%20models%2C%20spanning%20string-%2C%20graph-%2C%203D%20atomistic%2C%20and%20protein-based%20modalities%2C%20are%20highly%20aligned%20across%20a%20wide%20range%20of%20chemical%20systems.%20Models%20trained%20on%20different%20datasets%20have%20highly%20similar%20representations%20of%20small%20molecules%2C%20and%20machine%20learning%20interatomic%20potentials%20converge%20in%20representation%20space%20as%20they%20improve%20in%20performance%2C%20suggesting%20that%20foundation%20models%20learn%20a%20common%20underlying%20representation%20of%20physical%20reality.%20We%20then%20show%20two%20distinct%20regimes%20of%20scientific%20models%3A%20on%20inputs%20similar%20to%20those%20seen%20during%20training%2C%20high-performing%20models%20align%20closely%20and%20weak%20models%20diverge%20into%20local%20sub-optima%20in%20representation%20space%3B%20on%20vastly%20different%20structures%20from%20those%20seen%20during%20training%2C%20nearly%20all%20models%20collapse%20onto%20a%20low-information%20representation%2C%20indicating%20that%20today%27s%20models%20remain%20limited%20by%20training%20data%20and%20inductive%20bias%20and%20do%20not%20yet%20encode%20truly%20universal%20structure.%20Our%20findings%20establish%20representational%20alignment%20as%20a%20quantitative%20benchmark%20for%20foundation-level%20generality%20in%20scientific%20models.%20More%20broadly%2C%20our%20work%20can%20track%20the%20emergence%20of%20universal%20representations%20of%20matter%20as%20models%20scale%2C%20and%20for%20selecting%20and%20distilling%20models%20whose%20learned%20representations%20transfer%20best%20across%20modalities%2C%20domains%20of%20matter%2C%20and%20scientific%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.03750v1&entry.124074799=Read"},
{"title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning", "author": "Ge-Peng Ji and Jingyi Liu and Deng-Ping Fan and Nick Barnes", "abstract": "In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.", "link": "http://arxiv.org/abs/2512.03667v1", "date": "2025-12-03", "relevancy": 2.6751, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5532}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Colon-X%3A%20Advancing%20Intelligent%20Colonoscopy%20from%20Multimodal%20Understanding%20to%20Clinical%20Reasoning&body=Title%3A%20Colon-X%3A%20Advancing%20Intelligent%20Colonoscopy%20from%20Multimodal%20Understanding%20to%20Clinical%20Reasoning%0AAuthor%3A%20Ge-Peng%20Ji%20and%20Jingyi%20Liu%20and%20Deng-Ping%20Fan%20and%20Nick%20Barnes%0AAbstract%3A%20In%20this%20study%2C%20we%20present%20Colon-X%2C%20an%20open%20initiative%20aimed%20at%20advancing%20multimodal%20intelligence%20in%20colonoscopy.%20We%20begin%20by%20constructing%20ColonVQA%2C%20the%20most%20comprehensive%20multimodal%20dataset%20ever%20built%20for%20colonoscopy%2C%20featuring%20over%201.1M%2B%20visual%20question%20answering%20entries%20across%2076%20clinical%20findings%20and%2018%20multimodal%20tasks.%20Beyond%20serving%20as%20a%20community-wide%20data%20foundation%2C%20we%20further%20investigate%20a%20critical%20yet%20underexplored%20transition%20in%20colonoscopy%20-%20evolving%20from%20multimodal%20understanding%20to%20clinical%20reasoning%3A%20%28a%29%20To%20capture%20the%20current%20landscape%20of%20multimodal%20understanding%20behaviors%2C%20we%20systematically%20assess%20the%20generalizability%20of%2022%20multimodal%20large%20language%20models%20and%20examine%20their%20reliability%20under%20human-induced%20perturbations.%20The%20results%20reveal%20that%20clinical%20outputs%20from%20leading%20MLLMs%20remain%20far%20from%20robust%20and%20trustworthy.%20%28b%29%20To%20narrow%20this%20gap%2C%20we%20further%20explore%20reasoning-centric%20intelligence%20tailored%20for%20colonoscopy.%20Specifically%2C%20we%20curate%20ColonReason%2C%20a%20clinically%20grounded%20reasoning%20dataset%20annotated%20through%20a%20multi-expert%20debating%20pipeline%2C%20and%20develop%20ColonR1%2C%20the%20first%20R1-styled%20model%20incorporating%20task-adaptive%20rewarding%20and%20gradient-stable%20optimization%20techniques.%20Under%20data-scarce%20conditions%2C%20our%20ColonR1%20achieves%2056.61%25%20overall%20accuracy%2C%20outperforming%20supervised%20fine-tuning%20by%2025.22%25%2C%20and%20sets%20a%20new%20reasoning-enabled%20baseline%20for%20multimodal%20colonoscopy%20analysis.%20All%20data%20and%20model%20resources%20are%20publicly%20available%20at%20https%3A//github.com/ai4colonoscopy/Colon-X.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColon-X%253A%2520Advancing%2520Intelligent%2520Colonoscopy%2520from%2520Multimodal%2520Understanding%2520to%2520Clinical%2520Reasoning%26entry.906535625%3DGe-Peng%2520Ji%2520and%2520Jingyi%2520Liu%2520and%2520Deng-Ping%2520Fan%2520and%2520Nick%2520Barnes%26entry.1292438233%3DIn%2520this%2520study%252C%2520we%2520present%2520Colon-X%252C%2520an%2520open%2520initiative%2520aimed%2520at%2520advancing%2520multimodal%2520intelligence%2520in%2520colonoscopy.%2520We%2520begin%2520by%2520constructing%2520ColonVQA%252C%2520the%2520most%2520comprehensive%2520multimodal%2520dataset%2520ever%2520built%2520for%2520colonoscopy%252C%2520featuring%2520over%25201.1M%252B%2520visual%2520question%2520answering%2520entries%2520across%252076%2520clinical%2520findings%2520and%252018%2520multimodal%2520tasks.%2520Beyond%2520serving%2520as%2520a%2520community-wide%2520data%2520foundation%252C%2520we%2520further%2520investigate%2520a%2520critical%2520yet%2520underexplored%2520transition%2520in%2520colonoscopy%2520-%2520evolving%2520from%2520multimodal%2520understanding%2520to%2520clinical%2520reasoning%253A%2520%2528a%2529%2520To%2520capture%2520the%2520current%2520landscape%2520of%2520multimodal%2520understanding%2520behaviors%252C%2520we%2520systematically%2520assess%2520the%2520generalizability%2520of%252022%2520multimodal%2520large%2520language%2520models%2520and%2520examine%2520their%2520reliability%2520under%2520human-induced%2520perturbations.%2520The%2520results%2520reveal%2520that%2520clinical%2520outputs%2520from%2520leading%2520MLLMs%2520remain%2520far%2520from%2520robust%2520and%2520trustworthy.%2520%2528b%2529%2520To%2520narrow%2520this%2520gap%252C%2520we%2520further%2520explore%2520reasoning-centric%2520intelligence%2520tailored%2520for%2520colonoscopy.%2520Specifically%252C%2520we%2520curate%2520ColonReason%252C%2520a%2520clinically%2520grounded%2520reasoning%2520dataset%2520annotated%2520through%2520a%2520multi-expert%2520debating%2520pipeline%252C%2520and%2520develop%2520ColonR1%252C%2520the%2520first%2520R1-styled%2520model%2520incorporating%2520task-adaptive%2520rewarding%2520and%2520gradient-stable%2520optimization%2520techniques.%2520Under%2520data-scarce%2520conditions%252C%2520our%2520ColonR1%2520achieves%252056.61%2525%2520overall%2520accuracy%252C%2520outperforming%2520supervised%2520fine-tuning%2520by%252025.22%2525%252C%2520and%2520sets%2520a%2520new%2520reasoning-enabled%2520baseline%2520for%2520multimodal%2520colonoscopy%2520analysis.%2520All%2520data%2520and%2520model%2520resources%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/ai4colonoscopy/Colon-X.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Colon-X%3A%20Advancing%20Intelligent%20Colonoscopy%20from%20Multimodal%20Understanding%20to%20Clinical%20Reasoning&entry.906535625=Ge-Peng%20Ji%20and%20Jingyi%20Liu%20and%20Deng-Ping%20Fan%20and%20Nick%20Barnes&entry.1292438233=In%20this%20study%2C%20we%20present%20Colon-X%2C%20an%20open%20initiative%20aimed%20at%20advancing%20multimodal%20intelligence%20in%20colonoscopy.%20We%20begin%20by%20constructing%20ColonVQA%2C%20the%20most%20comprehensive%20multimodal%20dataset%20ever%20built%20for%20colonoscopy%2C%20featuring%20over%201.1M%2B%20visual%20question%20answering%20entries%20across%2076%20clinical%20findings%20and%2018%20multimodal%20tasks.%20Beyond%20serving%20as%20a%20community-wide%20data%20foundation%2C%20we%20further%20investigate%20a%20critical%20yet%20underexplored%20transition%20in%20colonoscopy%20-%20evolving%20from%20multimodal%20understanding%20to%20clinical%20reasoning%3A%20%28a%29%20To%20capture%20the%20current%20landscape%20of%20multimodal%20understanding%20behaviors%2C%20we%20systematically%20assess%20the%20generalizability%20of%2022%20multimodal%20large%20language%20models%20and%20examine%20their%20reliability%20under%20human-induced%20perturbations.%20The%20results%20reveal%20that%20clinical%20outputs%20from%20leading%20MLLMs%20remain%20far%20from%20robust%20and%20trustworthy.%20%28b%29%20To%20narrow%20this%20gap%2C%20we%20further%20explore%20reasoning-centric%20intelligence%20tailored%20for%20colonoscopy.%20Specifically%2C%20we%20curate%20ColonReason%2C%20a%20clinically%20grounded%20reasoning%20dataset%20annotated%20through%20a%20multi-expert%20debating%20pipeline%2C%20and%20develop%20ColonR1%2C%20the%20first%20R1-styled%20model%20incorporating%20task-adaptive%20rewarding%20and%20gradient-stable%20optimization%20techniques.%20Under%20data-scarce%20conditions%2C%20our%20ColonR1%20achieves%2056.61%25%20overall%20accuracy%2C%20outperforming%20supervised%20fine-tuning%20by%2025.22%25%2C%20and%20sets%20a%20new%20reasoning-enabled%20baseline%20for%20multimodal%20colonoscopy%20analysis.%20All%20data%20and%20model%20resources%20are%20publicly%20available%20at%20https%3A//github.com/ai4colonoscopy/Colon-X.&entry.1838667208=http%3A//arxiv.org/abs/2512.03667v1&entry.124074799=Read"},
{"title": "Unmute the Patch Tokens: Rethinking Probing in Multi-Label Audio Classification", "author": "Lukas Rauch and Ren\u00e9 Heinrich and Houtan Ghaffari and Lukas Miklautz and Ilyass Moummad and Bernhard Sick and Christoph Scholz", "abstract": "Although probing frozen models has become a standard evaluation paradigm, self-supervised learning in audio defaults to fine-tuning when pursuing state-of-the-art on AudioSet. A key reason is that global pooling creates an information bottleneck causing linear probes to misrepresent the embedding quality: The $\\texttt{cls}$-token discards crucial token information about dispersed, localized events in audio. This weakness is rooted in the mismatch between the pretraining objective (globally) and the downstream task (localized). Across a comprehensive benchmark of 13 datasets and 6 spectrogram-based encoders, we investigate the global pooling bottleneck. We introduce binarized prototypical probes: a lightweight and simple pooling method that learns prototypes to perform class-wise information aggregation. Despite its simplicity, our method notably outperforms linear and attentive probing. Our work establishes probing as a competitive and efficient paradigm for evaluating audio SSL models, challenging the reliance on costly fine-tuning.", "link": "http://arxiv.org/abs/2509.24901v3", "date": "2025-12-03", "relevancy": 2.6736, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unmute%20the%20Patch%20Tokens%3A%20Rethinking%20Probing%20in%20Multi-Label%20Audio%20Classification&body=Title%3A%20Unmute%20the%20Patch%20Tokens%3A%20Rethinking%20Probing%20in%20Multi-Label%20Audio%20Classification%0AAuthor%3A%20Lukas%20Rauch%20and%20Ren%C3%A9%20Heinrich%20and%20Houtan%20Ghaffari%20and%20Lukas%20Miklautz%20and%20Ilyass%20Moummad%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz%0AAbstract%3A%20Although%20probing%20frozen%20models%20has%20become%20a%20standard%20evaluation%20paradigm%2C%20self-supervised%20learning%20in%20audio%20defaults%20to%20fine-tuning%20when%20pursuing%20state-of-the-art%20on%20AudioSet.%20A%20key%20reason%20is%20that%20global%20pooling%20creates%20an%20information%20bottleneck%20causing%20linear%20probes%20to%20misrepresent%20the%20embedding%20quality%3A%20The%20%24%5Ctexttt%7Bcls%7D%24-token%20discards%20crucial%20token%20information%20about%20dispersed%2C%20localized%20events%20in%20audio.%20This%20weakness%20is%20rooted%20in%20the%20mismatch%20between%20the%20pretraining%20objective%20%28globally%29%20and%20the%20downstream%20task%20%28localized%29.%20Across%20a%20comprehensive%20benchmark%20of%2013%20datasets%20and%206%20spectrogram-based%20encoders%2C%20we%20investigate%20the%20global%20pooling%20bottleneck.%20We%20introduce%20binarized%20prototypical%20probes%3A%20a%20lightweight%20and%20simple%20pooling%20method%20that%20learns%20prototypes%20to%20perform%20class-wise%20information%20aggregation.%20Despite%20its%20simplicity%2C%20our%20method%20notably%20outperforms%20linear%20and%20attentive%20probing.%20Our%20work%20establishes%20probing%20as%20a%20competitive%20and%20efficient%20paradigm%20for%20evaluating%20audio%20SSL%20models%2C%20challenging%20the%20reliance%20on%20costly%20fine-tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2509.24901v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnmute%2520the%2520Patch%2520Tokens%253A%2520Rethinking%2520Probing%2520in%2520Multi-Label%2520Audio%2520Classification%26entry.906535625%3DLukas%2520Rauch%2520and%2520Ren%25C3%25A9%2520Heinrich%2520and%2520Houtan%2520Ghaffari%2520and%2520Lukas%2520Miklautz%2520and%2520Ilyass%2520Moummad%2520and%2520Bernhard%2520Sick%2520and%2520Christoph%2520Scholz%26entry.1292438233%3DAlthough%2520probing%2520frozen%2520models%2520has%2520become%2520a%2520standard%2520evaluation%2520paradigm%252C%2520self-supervised%2520learning%2520in%2520audio%2520defaults%2520to%2520fine-tuning%2520when%2520pursuing%2520state-of-the-art%2520on%2520AudioSet.%2520A%2520key%2520reason%2520is%2520that%2520global%2520pooling%2520creates%2520an%2520information%2520bottleneck%2520causing%2520linear%2520probes%2520to%2520misrepresent%2520the%2520embedding%2520quality%253A%2520The%2520%2524%255Ctexttt%257Bcls%257D%2524-token%2520discards%2520crucial%2520token%2520information%2520about%2520dispersed%252C%2520localized%2520events%2520in%2520audio.%2520This%2520weakness%2520is%2520rooted%2520in%2520the%2520mismatch%2520between%2520the%2520pretraining%2520objective%2520%2528globally%2529%2520and%2520the%2520downstream%2520task%2520%2528localized%2529.%2520Across%2520a%2520comprehensive%2520benchmark%2520of%252013%2520datasets%2520and%25206%2520spectrogram-based%2520encoders%252C%2520we%2520investigate%2520the%2520global%2520pooling%2520bottleneck.%2520We%2520introduce%2520binarized%2520prototypical%2520probes%253A%2520a%2520lightweight%2520and%2520simple%2520pooling%2520method%2520that%2520learns%2520prototypes%2520to%2520perform%2520class-wise%2520information%2520aggregation.%2520Despite%2520its%2520simplicity%252C%2520our%2520method%2520notably%2520outperforms%2520linear%2520and%2520attentive%2520probing.%2520Our%2520work%2520establishes%2520probing%2520as%2520a%2520competitive%2520and%2520efficient%2520paradigm%2520for%2520evaluating%2520audio%2520SSL%2520models%252C%2520challenging%2520the%2520reliance%2520on%2520costly%2520fine-tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24901v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unmute%20the%20Patch%20Tokens%3A%20Rethinking%20Probing%20in%20Multi-Label%20Audio%20Classification&entry.906535625=Lukas%20Rauch%20and%20Ren%C3%A9%20Heinrich%20and%20Houtan%20Ghaffari%20and%20Lukas%20Miklautz%20and%20Ilyass%20Moummad%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz&entry.1292438233=Although%20probing%20frozen%20models%20has%20become%20a%20standard%20evaluation%20paradigm%2C%20self-supervised%20learning%20in%20audio%20defaults%20to%20fine-tuning%20when%20pursuing%20state-of-the-art%20on%20AudioSet.%20A%20key%20reason%20is%20that%20global%20pooling%20creates%20an%20information%20bottleneck%20causing%20linear%20probes%20to%20misrepresent%20the%20embedding%20quality%3A%20The%20%24%5Ctexttt%7Bcls%7D%24-token%20discards%20crucial%20token%20information%20about%20dispersed%2C%20localized%20events%20in%20audio.%20This%20weakness%20is%20rooted%20in%20the%20mismatch%20between%20the%20pretraining%20objective%20%28globally%29%20and%20the%20downstream%20task%20%28localized%29.%20Across%20a%20comprehensive%20benchmark%20of%2013%20datasets%20and%206%20spectrogram-based%20encoders%2C%20we%20investigate%20the%20global%20pooling%20bottleneck.%20We%20introduce%20binarized%20prototypical%20probes%3A%20a%20lightweight%20and%20simple%20pooling%20method%20that%20learns%20prototypes%20to%20perform%20class-wise%20information%20aggregation.%20Despite%20its%20simplicity%2C%20our%20method%20notably%20outperforms%20linear%20and%20attentive%20probing.%20Our%20work%20establishes%20probing%20as%20a%20competitive%20and%20efficient%20paradigm%20for%20evaluating%20audio%20SSL%20models%2C%20challenging%20the%20reliance%20on%20costly%20fine-tuning.&entry.1838667208=http%3A//arxiv.org/abs/2509.24901v3&entry.124074799=Read"},
{"title": "MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba", "author": "Shanhui Liu and Rui Xu and Yunke Wang", "abstract": "Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss as they discard or compress token representations. This problem is further exacerbated when the same fine-grained token processing is uniformly applied across all images regardless of visual complexity. We observe that not all inputs require fine-grained processing: simple images can be effectively handled at a coarse resolution, while only complex ones require refinement. Based on this insight, we propose MambaScope, an adaptive framework for efficient inference for Vision Mamba. MambaScope first performs coarse-grained inference by dividing the input image into large patches, significantly reducing token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover essential visual details with minimal additional cost. This dynamic resolution assignment strategy allows MambaScope to allocate computation adaptively according to image complexity, achieving efficient processing without compromising accuracy. Experiments across various vision tasks demonstrate that MambaScope outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency.", "link": "http://arxiv.org/abs/2512.00647v2", "date": "2025-12-03", "relevancy": 2.6484, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.539}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaScope%3A%20Coarse-to-Fine%20Scoping%20for%20Efficient%20Vision%20Mamba&body=Title%3A%20MambaScope%3A%20Coarse-to-Fine%20Scoping%20for%20Efficient%20Vision%20Mamba%0AAuthor%3A%20Shanhui%20Liu%20and%20Rui%20Xu%20and%20Yunke%20Wang%0AAbstract%3A%20Vision%20Mamba%20has%20emerged%20as%20a%20promising%20and%20efficient%20alternative%20to%20Vision%20Transformers%2C%20yet%20its%20efficiency%20remains%20fundamentally%20constrained%20by%20the%20number%20of%20input%20tokens.%20Existing%20token%20reduction%20approaches%20typically%20adopt%20token%20pruning%20or%20merging%20to%20reduce%20computation.%20However%2C%20they%20inherently%20lead%20to%20information%20loss%20as%20they%20discard%20or%20compress%20token%20representations.%20This%20problem%20is%20further%20exacerbated%20when%20the%20same%20fine-grained%20token%20processing%20is%20uniformly%20applied%20across%20all%20images%20regardless%20of%20visual%20complexity.%20We%20observe%20that%20not%20all%20inputs%20require%20fine-grained%20processing%3A%20simple%20images%20can%20be%20effectively%20handled%20at%20a%20coarse%20resolution%2C%20while%20only%20complex%20ones%20require%20refinement.%20Based%20on%20this%20insight%2C%20we%20propose%20MambaScope%2C%20an%20adaptive%20framework%20for%20efficient%20inference%20for%20Vision%20Mamba.%20MambaScope%20first%20performs%20coarse-grained%20inference%20by%20dividing%20the%20input%20image%20into%20large%20patches%2C%20significantly%20reducing%20token%20length%20and%20computation.%20When%20the%20model%27s%20prediction%20confidence%20is%20low%2C%20selected%20regions%20are%20re-processed%20at%20a%20finer%20resolution%20to%20recover%20essential%20visual%20details%20with%20minimal%20additional%20cost.%20This%20dynamic%20resolution%20assignment%20strategy%20allows%20MambaScope%20to%20allocate%20computation%20adaptively%20according%20to%20image%20complexity%2C%20achieving%20efficient%20processing%20without%20compromising%20accuracy.%20Experiments%20across%20various%20vision%20tasks%20demonstrate%20that%20MambaScope%20outperforms%20both%20the%20baseline%20Vision%20Mamba%20and%20state-of-the-art%20token%20reduction%20techniques%20in%20terms%20of%20accuracy%20and%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00647v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaScope%253A%2520Coarse-to-Fine%2520Scoping%2520for%2520Efficient%2520Vision%2520Mamba%26entry.906535625%3DShanhui%2520Liu%2520and%2520Rui%2520Xu%2520and%2520Yunke%2520Wang%26entry.1292438233%3DVision%2520Mamba%2520has%2520emerged%2520as%2520a%2520promising%2520and%2520efficient%2520alternative%2520to%2520Vision%2520Transformers%252C%2520yet%2520its%2520efficiency%2520remains%2520fundamentally%2520constrained%2520by%2520the%2520number%2520of%2520input%2520tokens.%2520Existing%2520token%2520reduction%2520approaches%2520typically%2520adopt%2520token%2520pruning%2520or%2520merging%2520to%2520reduce%2520computation.%2520However%252C%2520they%2520inherently%2520lead%2520to%2520information%2520loss%2520as%2520they%2520discard%2520or%2520compress%2520token%2520representations.%2520This%2520problem%2520is%2520further%2520exacerbated%2520when%2520the%2520same%2520fine-grained%2520token%2520processing%2520is%2520uniformly%2520applied%2520across%2520all%2520images%2520regardless%2520of%2520visual%2520complexity.%2520We%2520observe%2520that%2520not%2520all%2520inputs%2520require%2520fine-grained%2520processing%253A%2520simple%2520images%2520can%2520be%2520effectively%2520handled%2520at%2520a%2520coarse%2520resolution%252C%2520while%2520only%2520complex%2520ones%2520require%2520refinement.%2520Based%2520on%2520this%2520insight%252C%2520we%2520propose%2520MambaScope%252C%2520an%2520adaptive%2520framework%2520for%2520efficient%2520inference%2520for%2520Vision%2520Mamba.%2520MambaScope%2520first%2520performs%2520coarse-grained%2520inference%2520by%2520dividing%2520the%2520input%2520image%2520into%2520large%2520patches%252C%2520significantly%2520reducing%2520token%2520length%2520and%2520computation.%2520When%2520the%2520model%2527s%2520prediction%2520confidence%2520is%2520low%252C%2520selected%2520regions%2520are%2520re-processed%2520at%2520a%2520finer%2520resolution%2520to%2520recover%2520essential%2520visual%2520details%2520with%2520minimal%2520additional%2520cost.%2520This%2520dynamic%2520resolution%2520assignment%2520strategy%2520allows%2520MambaScope%2520to%2520allocate%2520computation%2520adaptively%2520according%2520to%2520image%2520complexity%252C%2520achieving%2520efficient%2520processing%2520without%2520compromising%2520accuracy.%2520Experiments%2520across%2520various%2520vision%2520tasks%2520demonstrate%2520that%2520MambaScope%2520outperforms%2520both%2520the%2520baseline%2520Vision%2520Mamba%2520and%2520state-of-the-art%2520token%2520reduction%2520techniques%2520in%2520terms%2520of%2520accuracy%2520and%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00647v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaScope%3A%20Coarse-to-Fine%20Scoping%20for%20Efficient%20Vision%20Mamba&entry.906535625=Shanhui%20Liu%20and%20Rui%20Xu%20and%20Yunke%20Wang&entry.1292438233=Vision%20Mamba%20has%20emerged%20as%20a%20promising%20and%20efficient%20alternative%20to%20Vision%20Transformers%2C%20yet%20its%20efficiency%20remains%20fundamentally%20constrained%20by%20the%20number%20of%20input%20tokens.%20Existing%20token%20reduction%20approaches%20typically%20adopt%20token%20pruning%20or%20merging%20to%20reduce%20computation.%20However%2C%20they%20inherently%20lead%20to%20information%20loss%20as%20they%20discard%20or%20compress%20token%20representations.%20This%20problem%20is%20further%20exacerbated%20when%20the%20same%20fine-grained%20token%20processing%20is%20uniformly%20applied%20across%20all%20images%20regardless%20of%20visual%20complexity.%20We%20observe%20that%20not%20all%20inputs%20require%20fine-grained%20processing%3A%20simple%20images%20can%20be%20effectively%20handled%20at%20a%20coarse%20resolution%2C%20while%20only%20complex%20ones%20require%20refinement.%20Based%20on%20this%20insight%2C%20we%20propose%20MambaScope%2C%20an%20adaptive%20framework%20for%20efficient%20inference%20for%20Vision%20Mamba.%20MambaScope%20first%20performs%20coarse-grained%20inference%20by%20dividing%20the%20input%20image%20into%20large%20patches%2C%20significantly%20reducing%20token%20length%20and%20computation.%20When%20the%20model%27s%20prediction%20confidence%20is%20low%2C%20selected%20regions%20are%20re-processed%20at%20a%20finer%20resolution%20to%20recover%20essential%20visual%20details%20with%20minimal%20additional%20cost.%20This%20dynamic%20resolution%20assignment%20strategy%20allows%20MambaScope%20to%20allocate%20computation%20adaptively%20according%20to%20image%20complexity%2C%20achieving%20efficient%20processing%20without%20compromising%20accuracy.%20Experiments%20across%20various%20vision%20tasks%20demonstrate%20that%20MambaScope%20outperforms%20both%20the%20baseline%20Vision%20Mamba%20and%20state-of-the-art%20token%20reduction%20techniques%20in%20terms%20of%20accuracy%20and%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2512.00647v2&entry.124074799=Read"},
{"title": "How to Train Long-Context Language Models (Effectively)", "author": "Tianyu Gao and Alexander Wettig and Howard Yen and Danqi Chen", "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development -- instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context downstream tasks, and we evaluate models after SFT as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices such as position extrapolation. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short-context data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.1-8B-Instruct on the majority of long-context tasks despite using only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs.", "link": "http://arxiv.org/abs/2410.02660v4", "date": "2025-12-03", "relevancy": 2.6422, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5565}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Train%20Long-Context%20Language%20Models%20%28Effectively%29&body=Title%3A%20How%20to%20Train%20Long-Context%20Language%20Models%20%28Effectively%29%0AAuthor%3A%20Tianyu%20Gao%20and%20Alexander%20Wettig%20and%20Howard%20Yen%20and%20Danqi%20Chen%0AAbstract%3A%20We%20study%20continued%20training%20and%20supervised%20fine-tuning%20%28SFT%29%20of%20a%20language%20model%20%28LM%29%20to%20make%20effective%20use%20of%20long-context%20information.%20We%20first%20establish%20a%20reliable%20evaluation%20protocol%20to%20guide%20model%20development%20--%20instead%20of%20perplexity%20or%20simple%20needle-in-a-haystack%20%28NIAH%29%20tests%2C%20we%20use%20a%20broad%20set%20of%20long-context%20downstream%20tasks%2C%20and%20we%20evaluate%20models%20after%20SFT%20as%20this%20better%20reveals%20long-context%20abilities.%20Supported%20by%20our%20robust%20evaluations%2C%20we%20run%20thorough%20experiments%20to%20decide%20the%20data%20mix%20for%20continued%20pre-training%2C%20the%20instruction%20tuning%20dataset%2C%20and%20many%20other%20design%20choices%20such%20as%20position%20extrapolation.%20We%20find%20that%20%281%29%20code%20repositories%20and%20books%20are%20excellent%20sources%20of%20long%20data%2C%20but%20it%20is%20crucial%20to%20combine%20them%20with%20high-quality%20short-context%20data%3B%20%282%29%20training%20with%20a%20sequence%20length%20beyond%20the%20evaluation%20length%20boosts%20long-context%20performance%3B%20%283%29%20for%20SFT%2C%20using%20only%20short%20instruction%20datasets%20yields%20strong%20performance%20on%20long-context%20tasks.%20Our%20final%20model%2C%20ProLong-8B%2C%20which%20is%20initialized%20from%20Llama-3%20and%20trained%20on%2040B%20tokens%2C%20demonstrates%20state-of-the-art%20long-context%20performance%20among%20similarly%20sized%20models%20at%20a%20length%20of%20128K.%20ProLong%20outperforms%20Llama-3.1-8B-Instruct%20on%20the%20majority%20of%20long-context%20tasks%20despite%20using%20only%205%25%20as%20many%20tokens%20during%20long-context%20training.%20Additionally%2C%20ProLong%20can%20effectively%20process%20up%20to%20512K%20tokens%2C%20one%20of%20the%20longest%20context%20windows%20of%20publicly%20available%20LMs.%0ALink%3A%20http%3A//arxiv.org/abs/2410.02660v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Train%2520Long-Context%2520Language%2520Models%2520%2528Effectively%2529%26entry.906535625%3DTianyu%2520Gao%2520and%2520Alexander%2520Wettig%2520and%2520Howard%2520Yen%2520and%2520Danqi%2520Chen%26entry.1292438233%3DWe%2520study%2520continued%2520training%2520and%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520of%2520a%2520language%2520model%2520%2528LM%2529%2520to%2520make%2520effective%2520use%2520of%2520long-context%2520information.%2520We%2520first%2520establish%2520a%2520reliable%2520evaluation%2520protocol%2520to%2520guide%2520model%2520development%2520--%2520instead%2520of%2520perplexity%2520or%2520simple%2520needle-in-a-haystack%2520%2528NIAH%2529%2520tests%252C%2520we%2520use%2520a%2520broad%2520set%2520of%2520long-context%2520downstream%2520tasks%252C%2520and%2520we%2520evaluate%2520models%2520after%2520SFT%2520as%2520this%2520better%2520reveals%2520long-context%2520abilities.%2520Supported%2520by%2520our%2520robust%2520evaluations%252C%2520we%2520run%2520thorough%2520experiments%2520to%2520decide%2520the%2520data%2520mix%2520for%2520continued%2520pre-training%252C%2520the%2520instruction%2520tuning%2520dataset%252C%2520and%2520many%2520other%2520design%2520choices%2520such%2520as%2520position%2520extrapolation.%2520We%2520find%2520that%2520%25281%2529%2520code%2520repositories%2520and%2520books%2520are%2520excellent%2520sources%2520of%2520long%2520data%252C%2520but%2520it%2520is%2520crucial%2520to%2520combine%2520them%2520with%2520high-quality%2520short-context%2520data%253B%2520%25282%2529%2520training%2520with%2520a%2520sequence%2520length%2520beyond%2520the%2520evaluation%2520length%2520boosts%2520long-context%2520performance%253B%2520%25283%2529%2520for%2520SFT%252C%2520using%2520only%2520short%2520instruction%2520datasets%2520yields%2520strong%2520performance%2520on%2520long-context%2520tasks.%2520Our%2520final%2520model%252C%2520ProLong-8B%252C%2520which%2520is%2520initialized%2520from%2520Llama-3%2520and%2520trained%2520on%252040B%2520tokens%252C%2520demonstrates%2520state-of-the-art%2520long-context%2520performance%2520among%2520similarly%2520sized%2520models%2520at%2520a%2520length%2520of%2520128K.%2520ProLong%2520outperforms%2520Llama-3.1-8B-Instruct%2520on%2520the%2520majority%2520of%2520long-context%2520tasks%2520despite%2520using%2520only%25205%2525%2520as%2520many%2520tokens%2520during%2520long-context%2520training.%2520Additionally%252C%2520ProLong%2520can%2520effectively%2520process%2520up%2520to%2520512K%2520tokens%252C%2520one%2520of%2520the%2520longest%2520context%2520windows%2520of%2520publicly%2520available%2520LMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02660v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Train%20Long-Context%20Language%20Models%20%28Effectively%29&entry.906535625=Tianyu%20Gao%20and%20Alexander%20Wettig%20and%20Howard%20Yen%20and%20Danqi%20Chen&entry.1292438233=We%20study%20continued%20training%20and%20supervised%20fine-tuning%20%28SFT%29%20of%20a%20language%20model%20%28LM%29%20to%20make%20effective%20use%20of%20long-context%20information.%20We%20first%20establish%20a%20reliable%20evaluation%20protocol%20to%20guide%20model%20development%20--%20instead%20of%20perplexity%20or%20simple%20needle-in-a-haystack%20%28NIAH%29%20tests%2C%20we%20use%20a%20broad%20set%20of%20long-context%20downstream%20tasks%2C%20and%20we%20evaluate%20models%20after%20SFT%20as%20this%20better%20reveals%20long-context%20abilities.%20Supported%20by%20our%20robust%20evaluations%2C%20we%20run%20thorough%20experiments%20to%20decide%20the%20data%20mix%20for%20continued%20pre-training%2C%20the%20instruction%20tuning%20dataset%2C%20and%20many%20other%20design%20choices%20such%20as%20position%20extrapolation.%20We%20find%20that%20%281%29%20code%20repositories%20and%20books%20are%20excellent%20sources%20of%20long%20data%2C%20but%20it%20is%20crucial%20to%20combine%20them%20with%20high-quality%20short-context%20data%3B%20%282%29%20training%20with%20a%20sequence%20length%20beyond%20the%20evaluation%20length%20boosts%20long-context%20performance%3B%20%283%29%20for%20SFT%2C%20using%20only%20short%20instruction%20datasets%20yields%20strong%20performance%20on%20long-context%20tasks.%20Our%20final%20model%2C%20ProLong-8B%2C%20which%20is%20initialized%20from%20Llama-3%20and%20trained%20on%2040B%20tokens%2C%20demonstrates%20state-of-the-art%20long-context%20performance%20among%20similarly%20sized%20models%20at%20a%20length%20of%20128K.%20ProLong%20outperforms%20Llama-3.1-8B-Instruct%20on%20the%20majority%20of%20long-context%20tasks%20despite%20using%20only%205%25%20as%20many%20tokens%20during%20long-context%20training.%20Additionally%2C%20ProLong%20can%20effectively%20process%20up%20to%20512K%20tokens%2C%20one%20of%20the%20longest%20context%20windows%20of%20publicly%20available%20LMs.&entry.1838667208=http%3A//arxiv.org/abs/2410.02660v4&entry.124074799=Read"},
{"title": "MemVerse: Multimodal Memory for Lifelong Learning Agents", "author": "Junming Liu and Yifei Sun and Weihua Cheng and Haodong Lei and Yirong Chen and Licheng Wen and Xuemeng Yang and Daocheng Fu and Pinlong Cai and Nianchen Deng and Yi Yu and Shuyue Hu and Botian Shi and Ding Wang", "abstract": "Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember. Without reliable memory, agents catastrophically forget past experiences, struggle with long-horizon reasoning, and fail to operate coherently in multimodal or interactive environments. We introduce MemVerse, a model-agnostic, plug-and-play memory framework that bridges fast parametric recall with hierarchical retrieval-based memory, enabling scalable and adaptive multimodal intelligence. MemVerse maintains short-term memory for recent context while transforming raw multimodal experiences into structured long-term memories organized as hierarchical knowledge graphs. This design supports continual consolidation, adaptive forgetting, and bounded memory growth. To handle real-time demands, MemVerse introduces a periodic distillation mechanism that compresses essential knowledge from long-term memory into the parametric model, allowing fast, differentiable recall while preserving interpretability. Extensive experiments demonstrate that MemVerse significantly improves multimodal reasoning and continual learning efficiency, empowering agents to remember, adapt, and reason coherently across extended interactions.", "link": "http://arxiv.org/abs/2512.03627v1", "date": "2025-12-03", "relevancy": 2.6261, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MemVerse%3A%20Multimodal%20Memory%20for%20Lifelong%20Learning%20Agents&body=Title%3A%20MemVerse%3A%20Multimodal%20Memory%20for%20Lifelong%20Learning%20Agents%0AAuthor%3A%20Junming%20Liu%20and%20Yifei%20Sun%20and%20Weihua%20Cheng%20and%20Haodong%20Lei%20and%20Yirong%20Chen%20and%20Licheng%20Wen%20and%20Xuemeng%20Yang%20and%20Daocheng%20Fu%20and%20Pinlong%20Cai%20and%20Nianchen%20Deng%20and%20Yi%20Yu%20and%20Shuyue%20Hu%20and%20Botian%20Shi%20and%20Ding%20Wang%0AAbstract%3A%20Despite%20rapid%20progress%20in%20large-scale%20language%20and%20vision%20models%2C%20AI%20agents%20still%20suffer%20from%20a%20fundamental%20limitation%3A%20they%20cannot%20remember.%20Without%20reliable%20memory%2C%20agents%20catastrophically%20forget%20past%20experiences%2C%20struggle%20with%20long-horizon%20reasoning%2C%20and%20fail%20to%20operate%20coherently%20in%20multimodal%20or%20interactive%20environments.%20We%20introduce%20MemVerse%2C%20a%20model-agnostic%2C%20plug-and-play%20memory%20framework%20that%20bridges%20fast%20parametric%20recall%20with%20hierarchical%20retrieval-based%20memory%2C%20enabling%20scalable%20and%20adaptive%20multimodal%20intelligence.%20MemVerse%20maintains%20short-term%20memory%20for%20recent%20context%20while%20transforming%20raw%20multimodal%20experiences%20into%20structured%20long-term%20memories%20organized%20as%20hierarchical%20knowledge%20graphs.%20This%20design%20supports%20continual%20consolidation%2C%20adaptive%20forgetting%2C%20and%20bounded%20memory%20growth.%20To%20handle%20real-time%20demands%2C%20MemVerse%20introduces%20a%20periodic%20distillation%20mechanism%20that%20compresses%20essential%20knowledge%20from%20long-term%20memory%20into%20the%20parametric%20model%2C%20allowing%20fast%2C%20differentiable%20recall%20while%20preserving%20interpretability.%20Extensive%20experiments%20demonstrate%20that%20MemVerse%20significantly%20improves%20multimodal%20reasoning%20and%20continual%20learning%20efficiency%2C%20empowering%20agents%20to%20remember%2C%20adapt%2C%20and%20reason%20coherently%20across%20extended%20interactions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemVerse%253A%2520Multimodal%2520Memory%2520for%2520Lifelong%2520Learning%2520Agents%26entry.906535625%3DJunming%2520Liu%2520and%2520Yifei%2520Sun%2520and%2520Weihua%2520Cheng%2520and%2520Haodong%2520Lei%2520and%2520Yirong%2520Chen%2520and%2520Licheng%2520Wen%2520and%2520Xuemeng%2520Yang%2520and%2520Daocheng%2520Fu%2520and%2520Pinlong%2520Cai%2520and%2520Nianchen%2520Deng%2520and%2520Yi%2520Yu%2520and%2520Shuyue%2520Hu%2520and%2520Botian%2520Shi%2520and%2520Ding%2520Wang%26entry.1292438233%3DDespite%2520rapid%2520progress%2520in%2520large-scale%2520language%2520and%2520vision%2520models%252C%2520AI%2520agents%2520still%2520suffer%2520from%2520a%2520fundamental%2520limitation%253A%2520they%2520cannot%2520remember.%2520Without%2520reliable%2520memory%252C%2520agents%2520catastrophically%2520forget%2520past%2520experiences%252C%2520struggle%2520with%2520long-horizon%2520reasoning%252C%2520and%2520fail%2520to%2520operate%2520coherently%2520in%2520multimodal%2520or%2520interactive%2520environments.%2520We%2520introduce%2520MemVerse%252C%2520a%2520model-agnostic%252C%2520plug-and-play%2520memory%2520framework%2520that%2520bridges%2520fast%2520parametric%2520recall%2520with%2520hierarchical%2520retrieval-based%2520memory%252C%2520enabling%2520scalable%2520and%2520adaptive%2520multimodal%2520intelligence.%2520MemVerse%2520maintains%2520short-term%2520memory%2520for%2520recent%2520context%2520while%2520transforming%2520raw%2520multimodal%2520experiences%2520into%2520structured%2520long-term%2520memories%2520organized%2520as%2520hierarchical%2520knowledge%2520graphs.%2520This%2520design%2520supports%2520continual%2520consolidation%252C%2520adaptive%2520forgetting%252C%2520and%2520bounded%2520memory%2520growth.%2520To%2520handle%2520real-time%2520demands%252C%2520MemVerse%2520introduces%2520a%2520periodic%2520distillation%2520mechanism%2520that%2520compresses%2520essential%2520knowledge%2520from%2520long-term%2520memory%2520into%2520the%2520parametric%2520model%252C%2520allowing%2520fast%252C%2520differentiable%2520recall%2520while%2520preserving%2520interpretability.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MemVerse%2520significantly%2520improves%2520multimodal%2520reasoning%2520and%2520continual%2520learning%2520efficiency%252C%2520empowering%2520agents%2520to%2520remember%252C%2520adapt%252C%2520and%2520reason%2520coherently%2520across%2520extended%2520interactions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MemVerse%3A%20Multimodal%20Memory%20for%20Lifelong%20Learning%20Agents&entry.906535625=Junming%20Liu%20and%20Yifei%20Sun%20and%20Weihua%20Cheng%20and%20Haodong%20Lei%20and%20Yirong%20Chen%20and%20Licheng%20Wen%20and%20Xuemeng%20Yang%20and%20Daocheng%20Fu%20and%20Pinlong%20Cai%20and%20Nianchen%20Deng%20and%20Yi%20Yu%20and%20Shuyue%20Hu%20and%20Botian%20Shi%20and%20Ding%20Wang&entry.1292438233=Despite%20rapid%20progress%20in%20large-scale%20language%20and%20vision%20models%2C%20AI%20agents%20still%20suffer%20from%20a%20fundamental%20limitation%3A%20they%20cannot%20remember.%20Without%20reliable%20memory%2C%20agents%20catastrophically%20forget%20past%20experiences%2C%20struggle%20with%20long-horizon%20reasoning%2C%20and%20fail%20to%20operate%20coherently%20in%20multimodal%20or%20interactive%20environments.%20We%20introduce%20MemVerse%2C%20a%20model-agnostic%2C%20plug-and-play%20memory%20framework%20that%20bridges%20fast%20parametric%20recall%20with%20hierarchical%20retrieval-based%20memory%2C%20enabling%20scalable%20and%20adaptive%20multimodal%20intelligence.%20MemVerse%20maintains%20short-term%20memory%20for%20recent%20context%20while%20transforming%20raw%20multimodal%20experiences%20into%20structured%20long-term%20memories%20organized%20as%20hierarchical%20knowledge%20graphs.%20This%20design%20supports%20continual%20consolidation%2C%20adaptive%20forgetting%2C%20and%20bounded%20memory%20growth.%20To%20handle%20real-time%20demands%2C%20MemVerse%20introduces%20a%20periodic%20distillation%20mechanism%20that%20compresses%20essential%20knowledge%20from%20long-term%20memory%20into%20the%20parametric%20model%2C%20allowing%20fast%2C%20differentiable%20recall%20while%20preserving%20interpretability.%20Extensive%20experiments%20demonstrate%20that%20MemVerse%20significantly%20improves%20multimodal%20reasoning%20and%20continual%20learning%20efficiency%2C%20empowering%20agents%20to%20remember%2C%20adapt%2C%20and%20reason%20coherently%20across%20extended%20interactions.&entry.1838667208=http%3A//arxiv.org/abs/2512.03627v1&entry.124074799=Read"},
{"title": "Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training", "author": "Brian Bartoldson and Siddarth Venkatraman and James Diffenderfer and Moksh Jain and Tal Ben-Nun and Seanie Lee and Minsu Kim and Johan Obando-Ceron and Yoshua Bengio and Bhavya Kailkhura", "abstract": "Reinforcement learning (RL) is a critical component of large language model (LLM) post-training. However, on-policy algorithms used for post-training are not naturally robust to a diversified content of experience replay buffers, which asynchronous off-policy actors can efficiently populate in parallel to training. We propose efficiently learning on such off-policy data via Trajectory Balance with Asynchrony (TBA), an approach to asynchronous RL for LLMs that leverages the principled off-policy TB objective. On math, preference-tuning, and automated red-teaming tasks, we post-train models ranging from Pythia 410M to Qwen 2.5 7B, finding TBA offers speed and performance boosts over strong baselines like Online DPO and Dr. GRPO. Beyond TBA's performance benefits (high accuracy even as asynchrony grows) and speedups ($4\\times$ or more), we show its reward- and recency-prioritizing sampling enable further gains as data generation is scaled. Our code is available at https://github.com/bbartoldson/TBA.", "link": "http://arxiv.org/abs/2503.18929v2", "date": "2025-12-03", "relevancy": 2.6042, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.544}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5107}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trajectory%20Balance%20with%20Asynchrony%3A%20Decoupling%20Exploration%20and%20Learning%20for%20Fast%2C%20Scalable%20LLM%20Post-Training&body=Title%3A%20Trajectory%20Balance%20with%20Asynchrony%3A%20Decoupling%20Exploration%20and%20Learning%20for%20Fast%2C%20Scalable%20LLM%20Post-Training%0AAuthor%3A%20Brian%20Bartoldson%20and%20Siddarth%20Venkatraman%20and%20James%20Diffenderfer%20and%20Moksh%20Jain%20and%20Tal%20Ben-Nun%20and%20Seanie%20Lee%20and%20Minsu%20Kim%20and%20Johan%20Obando-Ceron%20and%20Yoshua%20Bengio%20and%20Bhavya%20Kailkhura%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29%20is%20a%20critical%20component%20of%20large%20language%20model%20%28LLM%29%20post-training.%20However%2C%20on-policy%20algorithms%20used%20for%20post-training%20are%20not%20naturally%20robust%20to%20a%20diversified%20content%20of%20experience%20replay%20buffers%2C%20which%20asynchronous%20off-policy%20actors%20can%20efficiently%20populate%20in%20parallel%20to%20training.%20We%20propose%20efficiently%20learning%20on%20such%20off-policy%20data%20via%20Trajectory%20Balance%20with%20Asynchrony%20%28TBA%29%2C%20an%20approach%20to%20asynchronous%20RL%20for%20LLMs%20that%20leverages%20the%20principled%20off-policy%20TB%20objective.%20On%20math%2C%20preference-tuning%2C%20and%20automated%20red-teaming%20tasks%2C%20we%20post-train%20models%20ranging%20from%20Pythia%20410M%20to%20Qwen%202.5%207B%2C%20finding%20TBA%20offers%20speed%20and%20performance%20boosts%20over%20strong%20baselines%20like%20Online%20DPO%20and%20Dr.%20GRPO.%20Beyond%20TBA%27s%20performance%20benefits%20%28high%20accuracy%20even%20as%20asynchrony%20grows%29%20and%20speedups%20%28%244%5Ctimes%24%20or%20more%29%2C%20we%20show%20its%20reward-%20and%20recency-prioritizing%20sampling%20enable%20further%20gains%20as%20data%20generation%20is%20scaled.%20Our%20code%20is%20available%20at%20https%3A//github.com/bbartoldson/TBA.%0ALink%3A%20http%3A//arxiv.org/abs/2503.18929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajectory%2520Balance%2520with%2520Asynchrony%253A%2520Decoupling%2520Exploration%2520and%2520Learning%2520for%2520Fast%252C%2520Scalable%2520LLM%2520Post-Training%26entry.906535625%3DBrian%2520Bartoldson%2520and%2520Siddarth%2520Venkatraman%2520and%2520James%2520Diffenderfer%2520and%2520Moksh%2520Jain%2520and%2520Tal%2520Ben-Nun%2520and%2520Seanie%2520Lee%2520and%2520Minsu%2520Kim%2520and%2520Johan%2520Obando-Ceron%2520and%2520Yoshua%2520Bengio%2520and%2520Bhavya%2520Kailkhura%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529%2520is%2520a%2520critical%2520component%2520of%2520large%2520language%2520model%2520%2528LLM%2529%2520post-training.%2520However%252C%2520on-policy%2520algorithms%2520used%2520for%2520post-training%2520are%2520not%2520naturally%2520robust%2520to%2520a%2520diversified%2520content%2520of%2520experience%2520replay%2520buffers%252C%2520which%2520asynchronous%2520off-policy%2520actors%2520can%2520efficiently%2520populate%2520in%2520parallel%2520to%2520training.%2520We%2520propose%2520efficiently%2520learning%2520on%2520such%2520off-policy%2520data%2520via%2520Trajectory%2520Balance%2520with%2520Asynchrony%2520%2528TBA%2529%252C%2520an%2520approach%2520to%2520asynchronous%2520RL%2520for%2520LLMs%2520that%2520leverages%2520the%2520principled%2520off-policy%2520TB%2520objective.%2520On%2520math%252C%2520preference-tuning%252C%2520and%2520automated%2520red-teaming%2520tasks%252C%2520we%2520post-train%2520models%2520ranging%2520from%2520Pythia%2520410M%2520to%2520Qwen%25202.5%25207B%252C%2520finding%2520TBA%2520offers%2520speed%2520and%2520performance%2520boosts%2520over%2520strong%2520baselines%2520like%2520Online%2520DPO%2520and%2520Dr.%2520GRPO.%2520Beyond%2520TBA%2527s%2520performance%2520benefits%2520%2528high%2520accuracy%2520even%2520as%2520asynchrony%2520grows%2529%2520and%2520speedups%2520%2528%25244%255Ctimes%2524%2520or%2520more%2529%252C%2520we%2520show%2520its%2520reward-%2520and%2520recency-prioritizing%2520sampling%2520enable%2520further%2520gains%2520as%2520data%2520generation%2520is%2520scaled.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/bbartoldson/TBA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory%20Balance%20with%20Asynchrony%3A%20Decoupling%20Exploration%20and%20Learning%20for%20Fast%2C%20Scalable%20LLM%20Post-Training&entry.906535625=Brian%20Bartoldson%20and%20Siddarth%20Venkatraman%20and%20James%20Diffenderfer%20and%20Moksh%20Jain%20and%20Tal%20Ben-Nun%20and%20Seanie%20Lee%20and%20Minsu%20Kim%20and%20Johan%20Obando-Ceron%20and%20Yoshua%20Bengio%20and%20Bhavya%20Kailkhura&entry.1292438233=Reinforcement%20learning%20%28RL%29%20is%20a%20critical%20component%20of%20large%20language%20model%20%28LLM%29%20post-training.%20However%2C%20on-policy%20algorithms%20used%20for%20post-training%20are%20not%20naturally%20robust%20to%20a%20diversified%20content%20of%20experience%20replay%20buffers%2C%20which%20asynchronous%20off-policy%20actors%20can%20efficiently%20populate%20in%20parallel%20to%20training.%20We%20propose%20efficiently%20learning%20on%20such%20off-policy%20data%20via%20Trajectory%20Balance%20with%20Asynchrony%20%28TBA%29%2C%20an%20approach%20to%20asynchronous%20RL%20for%20LLMs%20that%20leverages%20the%20principled%20off-policy%20TB%20objective.%20On%20math%2C%20preference-tuning%2C%20and%20automated%20red-teaming%20tasks%2C%20we%20post-train%20models%20ranging%20from%20Pythia%20410M%20to%20Qwen%202.5%207B%2C%20finding%20TBA%20offers%20speed%20and%20performance%20boosts%20over%20strong%20baselines%20like%20Online%20DPO%20and%20Dr.%20GRPO.%20Beyond%20TBA%27s%20performance%20benefits%20%28high%20accuracy%20even%20as%20asynchrony%20grows%29%20and%20speedups%20%28%244%5Ctimes%24%20or%20more%29%2C%20we%20show%20its%20reward-%20and%20recency-prioritizing%20sampling%20enable%20further%20gains%20as%20data%20generation%20is%20scaled.%20Our%20code%20is%20available%20at%20https%3A//github.com/bbartoldson/TBA.&entry.1838667208=http%3A//arxiv.org/abs/2503.18929v2&entry.124074799=Read"},
{"title": "Consistent Projection of Langevin Dynamics: Preserving Thermodynamics and Kinetics in Coarse-Grained Models", "author": "Vahid Nateghi and Lara Neureither and Selma Moqvist and Carsten Hartmann and Simon Olsson and Feliks N\u00fcske", "abstract": "Coarse graining (CG) is an important task for efficient modeling and simulation of complex multi-scale systems, such as the conformational dynamics of biomolecules. This work presents a projection-based coarse-graining formalism for general underdamped Langevin dynamics. Following the Zwanzig projection approach, we derive a closed-form expression for the coarse grained dynamics. In addition, we show how the generator Extended Dynamic Mode Decomposition (gEDMD) method, which was developed in the context of Koopman operator methods, can be used to model the CG dynamics and evaluate its kinetic properties, such as transition timescales. Finally, we combine our approach with thermodynamic interpolation (TI), a generative approach to transform samples between thermodynamic conditions, to extend the scope of the approach across thermodynamic states without repeated numerical simulations. Using a two-dimensional model system, we demonstrate that the proposed method allows to accurately capture the thermodynamic and kinetic properties of the full-space model.", "link": "http://arxiv.org/abs/2512.03706v1", "date": "2025-12-03", "relevancy": 2.6023, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5498}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5145}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20Projection%20of%20Langevin%20Dynamics%3A%20Preserving%20Thermodynamics%20and%20Kinetics%20in%20Coarse-Grained%20Models&body=Title%3A%20Consistent%20Projection%20of%20Langevin%20Dynamics%3A%20Preserving%20Thermodynamics%20and%20Kinetics%20in%20Coarse-Grained%20Models%0AAuthor%3A%20Vahid%20Nateghi%20and%20Lara%20Neureither%20and%20Selma%20Moqvist%20and%20Carsten%20Hartmann%20and%20Simon%20Olsson%20and%20Feliks%20N%C3%BCske%0AAbstract%3A%20Coarse%20graining%20%28CG%29%20is%20an%20important%20task%20for%20efficient%20modeling%20and%20simulation%20of%20complex%20multi-scale%20systems%2C%20such%20as%20the%20conformational%20dynamics%20of%20biomolecules.%20This%20work%20presents%20a%20projection-based%20coarse-graining%20formalism%20for%20general%20underdamped%20Langevin%20dynamics.%20Following%20the%20Zwanzig%20projection%20approach%2C%20we%20derive%20a%20closed-form%20expression%20for%20the%20coarse%20grained%20dynamics.%20In%20addition%2C%20we%20show%20how%20the%20generator%20Extended%20Dynamic%20Mode%20Decomposition%20%28gEDMD%29%20method%2C%20which%20was%20developed%20in%20the%20context%20of%20Koopman%20operator%20methods%2C%20can%20be%20used%20to%20model%20the%20CG%20dynamics%20and%20evaluate%20its%20kinetic%20properties%2C%20such%20as%20transition%20timescales.%20Finally%2C%20we%20combine%20our%20approach%20with%20thermodynamic%20interpolation%20%28TI%29%2C%20a%20generative%20approach%20to%20transform%20samples%20between%20thermodynamic%20conditions%2C%20to%20extend%20the%20scope%20of%20the%20approach%20across%20thermodynamic%20states%20without%20repeated%20numerical%20simulations.%20Using%20a%20two-dimensional%20model%20system%2C%20we%20demonstrate%20that%20the%20proposed%20method%20allows%20to%20accurately%20capture%20the%20thermodynamic%20and%20kinetic%20properties%20of%20the%20full-space%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520Projection%2520of%2520Langevin%2520Dynamics%253A%2520Preserving%2520Thermodynamics%2520and%2520Kinetics%2520in%2520Coarse-Grained%2520Models%26entry.906535625%3DVahid%2520Nateghi%2520and%2520Lara%2520Neureither%2520and%2520Selma%2520Moqvist%2520and%2520Carsten%2520Hartmann%2520and%2520Simon%2520Olsson%2520and%2520Feliks%2520N%25C3%25BCske%26entry.1292438233%3DCoarse%2520graining%2520%2528CG%2529%2520is%2520an%2520important%2520task%2520for%2520efficient%2520modeling%2520and%2520simulation%2520of%2520complex%2520multi-scale%2520systems%252C%2520such%2520as%2520the%2520conformational%2520dynamics%2520of%2520biomolecules.%2520This%2520work%2520presents%2520a%2520projection-based%2520coarse-graining%2520formalism%2520for%2520general%2520underdamped%2520Langevin%2520dynamics.%2520Following%2520the%2520Zwanzig%2520projection%2520approach%252C%2520we%2520derive%2520a%2520closed-form%2520expression%2520for%2520the%2520coarse%2520grained%2520dynamics.%2520In%2520addition%252C%2520we%2520show%2520how%2520the%2520generator%2520Extended%2520Dynamic%2520Mode%2520Decomposition%2520%2528gEDMD%2529%2520method%252C%2520which%2520was%2520developed%2520in%2520the%2520context%2520of%2520Koopman%2520operator%2520methods%252C%2520can%2520be%2520used%2520to%2520model%2520the%2520CG%2520dynamics%2520and%2520evaluate%2520its%2520kinetic%2520properties%252C%2520such%2520as%2520transition%2520timescales.%2520Finally%252C%2520we%2520combine%2520our%2520approach%2520with%2520thermodynamic%2520interpolation%2520%2528TI%2529%252C%2520a%2520generative%2520approach%2520to%2520transform%2520samples%2520between%2520thermodynamic%2520conditions%252C%2520to%2520extend%2520the%2520scope%2520of%2520the%2520approach%2520across%2520thermodynamic%2520states%2520without%2520repeated%2520numerical%2520simulations.%2520Using%2520a%2520two-dimensional%2520model%2520system%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%2520method%2520allows%2520to%2520accurately%2520capture%2520the%2520thermodynamic%2520and%2520kinetic%2520properties%2520of%2520the%2520full-space%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Projection%20of%20Langevin%20Dynamics%3A%20Preserving%20Thermodynamics%20and%20Kinetics%20in%20Coarse-Grained%20Models&entry.906535625=Vahid%20Nateghi%20and%20Lara%20Neureither%20and%20Selma%20Moqvist%20and%20Carsten%20Hartmann%20and%20Simon%20Olsson%20and%20Feliks%20N%C3%BCske&entry.1292438233=Coarse%20graining%20%28CG%29%20is%20an%20important%20task%20for%20efficient%20modeling%20and%20simulation%20of%20complex%20multi-scale%20systems%2C%20such%20as%20the%20conformational%20dynamics%20of%20biomolecules.%20This%20work%20presents%20a%20projection-based%20coarse-graining%20formalism%20for%20general%20underdamped%20Langevin%20dynamics.%20Following%20the%20Zwanzig%20projection%20approach%2C%20we%20derive%20a%20closed-form%20expression%20for%20the%20coarse%20grained%20dynamics.%20In%20addition%2C%20we%20show%20how%20the%20generator%20Extended%20Dynamic%20Mode%20Decomposition%20%28gEDMD%29%20method%2C%20which%20was%20developed%20in%20the%20context%20of%20Koopman%20operator%20methods%2C%20can%20be%20used%20to%20model%20the%20CG%20dynamics%20and%20evaluate%20its%20kinetic%20properties%2C%20such%20as%20transition%20timescales.%20Finally%2C%20we%20combine%20our%20approach%20with%20thermodynamic%20interpolation%20%28TI%29%2C%20a%20generative%20approach%20to%20transform%20samples%20between%20thermodynamic%20conditions%2C%20to%20extend%20the%20scope%20of%20the%20approach%20across%20thermodynamic%20states%20without%20repeated%20numerical%20simulations.%20Using%20a%20two-dimensional%20model%20system%2C%20we%20demonstrate%20that%20the%20proposed%20method%20allows%20to%20accurately%20capture%20the%20thermodynamic%20and%20kinetic%20properties%20of%20the%20full-space%20model.&entry.1838667208=http%3A//arxiv.org/abs/2512.03706v1&entry.124074799=Read"},
{"title": "Heatmap Pooling Network for Action Recognition from RGB Videos", "author": "Mengyuan Liu and Jinfu Liu and Yongkang Jiang and Bin He", "abstract": "Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.", "link": "http://arxiv.org/abs/2512.03837v1", "date": "2025-12-03", "relevancy": 2.5984, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5254}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5169}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heatmap%20Pooling%20Network%20for%20Action%20Recognition%20from%20RGB%20Videos&body=Title%3A%20Heatmap%20Pooling%20Network%20for%20Action%20Recognition%20from%20RGB%20Videos%0AAuthor%3A%20Mengyuan%20Liu%20and%20Jinfu%20Liu%20and%20Yongkang%20Jiang%20and%20Bin%20He%0AAbstract%3A%20Human%20action%20recognition%20%28HAR%29%20in%20videos%20has%20garnered%20widespread%20attention%20due%20to%20the%20rich%20information%20in%20RGB%20videos.%20Nevertheless%2C%20existing%20methods%20for%20extracting%20deep%20features%20from%20RGB%20videos%20face%20challenges%20such%20as%20information%20redundancy%2C%20susceptibility%20to%20noise%20and%20high%20storage%20costs.%20To%20address%20these%20issues%20and%20fully%20harness%20the%20useful%20information%20in%20videos%2C%20we%20propose%20a%20novel%20heatmap%20pooling%20network%20%28HP-Net%29%20for%20action%20recognition%20from%20videos%2C%20which%20extracts%20information-rich%2C%20robust%20and%20concise%20pooled%20features%20of%20the%20human%20body%20in%20videos%20through%20a%20feedback%20pooling%20module.%20The%20extracted%20pooled%20features%20demonstrate%20obvious%20performance%20advantages%20over%20the%20previously%20obtained%20pose%20data%20and%20heatmap%20features%20from%20videos.%20In%20addition%2C%20we%20design%20a%20spatial-motion%20co-learning%20module%20and%20a%20text%20refinement%20modulation%20module%20to%20integrate%20the%20extracted%20pooled%20features%20with%20other%20multimodal%20data%2C%20enabling%20more%20robust%20action%20recognition.%20Extensive%20experiments%20on%20several%20benchmarks%20namely%20NTU%20RGB%2BD%2060%2C%20NTU%20RGB%2BD%20120%2C%20Toyota-Smarthome%20and%20UAV-Human%20consistently%20verify%20the%20effectiveness%20of%20our%20HP-Net%2C%20which%20outperforms%20the%20existing%20human%20action%20recognition%20methods.%20Our%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/liujf69/HPNet-Action.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeatmap%2520Pooling%2520Network%2520for%2520Action%2520Recognition%2520from%2520RGB%2520Videos%26entry.906535625%3DMengyuan%2520Liu%2520and%2520Jinfu%2520Liu%2520and%2520Yongkang%2520Jiang%2520and%2520Bin%2520He%26entry.1292438233%3DHuman%2520action%2520recognition%2520%2528HAR%2529%2520in%2520videos%2520has%2520garnered%2520widespread%2520attention%2520due%2520to%2520the%2520rich%2520information%2520in%2520RGB%2520videos.%2520Nevertheless%252C%2520existing%2520methods%2520for%2520extracting%2520deep%2520features%2520from%2520RGB%2520videos%2520face%2520challenges%2520such%2520as%2520information%2520redundancy%252C%2520susceptibility%2520to%2520noise%2520and%2520high%2520storage%2520costs.%2520To%2520address%2520these%2520issues%2520and%2520fully%2520harness%2520the%2520useful%2520information%2520in%2520videos%252C%2520we%2520propose%2520a%2520novel%2520heatmap%2520pooling%2520network%2520%2528HP-Net%2529%2520for%2520action%2520recognition%2520from%2520videos%252C%2520which%2520extracts%2520information-rich%252C%2520robust%2520and%2520concise%2520pooled%2520features%2520of%2520the%2520human%2520body%2520in%2520videos%2520through%2520a%2520feedback%2520pooling%2520module.%2520The%2520extracted%2520pooled%2520features%2520demonstrate%2520obvious%2520performance%2520advantages%2520over%2520the%2520previously%2520obtained%2520pose%2520data%2520and%2520heatmap%2520features%2520from%2520videos.%2520In%2520addition%252C%2520we%2520design%2520a%2520spatial-motion%2520co-learning%2520module%2520and%2520a%2520text%2520refinement%2520modulation%2520module%2520to%2520integrate%2520the%2520extracted%2520pooled%2520features%2520with%2520other%2520multimodal%2520data%252C%2520enabling%2520more%2520robust%2520action%2520recognition.%2520Extensive%2520experiments%2520on%2520several%2520benchmarks%2520namely%2520NTU%2520RGB%252BD%252060%252C%2520NTU%2520RGB%252BD%2520120%252C%2520Toyota-Smarthome%2520and%2520UAV-Human%2520consistently%2520verify%2520the%2520effectiveness%2520of%2520our%2520HP-Net%252C%2520which%2520outperforms%2520the%2520existing%2520human%2520action%2520recognition%2520methods.%2520Our%2520code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/liujf69/HPNet-Action.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heatmap%20Pooling%20Network%20for%20Action%20Recognition%20from%20RGB%20Videos&entry.906535625=Mengyuan%20Liu%20and%20Jinfu%20Liu%20and%20Yongkang%20Jiang%20and%20Bin%20He&entry.1292438233=Human%20action%20recognition%20%28HAR%29%20in%20videos%20has%20garnered%20widespread%20attention%20due%20to%20the%20rich%20information%20in%20RGB%20videos.%20Nevertheless%2C%20existing%20methods%20for%20extracting%20deep%20features%20from%20RGB%20videos%20face%20challenges%20such%20as%20information%20redundancy%2C%20susceptibility%20to%20noise%20and%20high%20storage%20costs.%20To%20address%20these%20issues%20and%20fully%20harness%20the%20useful%20information%20in%20videos%2C%20we%20propose%20a%20novel%20heatmap%20pooling%20network%20%28HP-Net%29%20for%20action%20recognition%20from%20videos%2C%20which%20extracts%20information-rich%2C%20robust%20and%20concise%20pooled%20features%20of%20the%20human%20body%20in%20videos%20through%20a%20feedback%20pooling%20module.%20The%20extracted%20pooled%20features%20demonstrate%20obvious%20performance%20advantages%20over%20the%20previously%20obtained%20pose%20data%20and%20heatmap%20features%20from%20videos.%20In%20addition%2C%20we%20design%20a%20spatial-motion%20co-learning%20module%20and%20a%20text%20refinement%20modulation%20module%20to%20integrate%20the%20extracted%20pooled%20features%20with%20other%20multimodal%20data%2C%20enabling%20more%20robust%20action%20recognition.%20Extensive%20experiments%20on%20several%20benchmarks%20namely%20NTU%20RGB%2BD%2060%2C%20NTU%20RGB%2BD%20120%2C%20Toyota-Smarthome%20and%20UAV-Human%20consistently%20verify%20the%20effectiveness%20of%20our%20HP-Net%2C%20which%20outperforms%20the%20existing%20human%20action%20recognition%20methods.%20Our%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/liujf69/HPNet-Action.&entry.1838667208=http%3A//arxiv.org/abs/2512.03837v1&entry.124074799=Read"},
{"title": "MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video", "author": "Minh-Quan Viet Bui and Jongmin Park and Juan Luis Gonzalez Bello and Jaeho Moon and Jihyong Oh and Munchurl Kim", "abstract": "We present MoBGS, a novel motion deblurring 3D Gaussian Splatting (3DGS) framework capable of reconstructing sharp and high-quality novel spatio-temporal views from blurry monocular videos in an end-to-end manner. Existing dynamic novel view synthesis (NVS) methods are highly sensitive to motion blur in casually captured videos, resulting in significant degradation of rendering quality. While recent approaches address motion-blurred inputs for NVS, they primarily focus on static scene reconstruction and lack dedicated motion modeling for dynamic objects. To overcome these limitations, our MoBGS introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method using a proposed Blur-adaptive Neural Ordinary Differential Equation (ODE) solver for effective latent camera trajectory estimation, improving global camera motion deblurring. In addition, we propose a Latent Camera-induced Exposure Estimation (LCEE) method to ensure consistent deblurring of both a global camera and local object motions. Extensive experiments on the Stereo Blur dataset and real-world blurry videos show that our MoBGS significantly outperforms the very recent methods, achieving state-of-the-art performance for dynamic NVS under motion blur.", "link": "http://arxiv.org/abs/2504.15122v4", "date": "2025-12-03", "relevancy": 2.5952, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6806}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6344}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoBGS%3A%20Motion%20Deblurring%20Dynamic%203D%20Gaussian%20Splatting%20for%20Blurry%20Monocular%20Video&body=Title%3A%20MoBGS%3A%20Motion%20Deblurring%20Dynamic%203D%20Gaussian%20Splatting%20for%20Blurry%20Monocular%20Video%0AAuthor%3A%20Minh-Quan%20Viet%20Bui%20and%20Jongmin%20Park%20and%20Juan%20Luis%20Gonzalez%20Bello%20and%20Jaeho%20Moon%20and%20Jihyong%20Oh%20and%20Munchurl%20Kim%0AAbstract%3A%20We%20present%20MoBGS%2C%20a%20novel%20motion%20deblurring%203D%20Gaussian%20Splatting%20%283DGS%29%20framework%20capable%20of%20reconstructing%20sharp%20and%20high-quality%20novel%20spatio-temporal%20views%20from%20blurry%20monocular%20videos%20in%20an%20end-to-end%20manner.%20Existing%20dynamic%20novel%20view%20synthesis%20%28NVS%29%20methods%20are%20highly%20sensitive%20to%20motion%20blur%20in%20casually%20captured%20videos%2C%20resulting%20in%20significant%20degradation%20of%20rendering%20quality.%20While%20recent%20approaches%20address%20motion-blurred%20inputs%20for%20NVS%2C%20they%20primarily%20focus%20on%20static%20scene%20reconstruction%20and%20lack%20dedicated%20motion%20modeling%20for%20dynamic%20objects.%20To%20overcome%20these%20limitations%2C%20our%20MoBGS%20introduces%20a%20novel%20Blur-adaptive%20Latent%20Camera%20Estimation%20%28BLCE%29%20method%20using%20a%20proposed%20Blur-adaptive%20Neural%20Ordinary%20Differential%20Equation%20%28ODE%29%20solver%20for%20effective%20latent%20camera%20trajectory%20estimation%2C%20improving%20global%20camera%20motion%20deblurring.%20In%20addition%2C%20we%20propose%20a%20Latent%20Camera-induced%20Exposure%20Estimation%20%28LCEE%29%20method%20to%20ensure%20consistent%20deblurring%20of%20both%20a%20global%20camera%20and%20local%20object%20motions.%20Extensive%20experiments%20on%20the%20Stereo%20Blur%20dataset%20and%20real-world%20blurry%20videos%20show%20that%20our%20MoBGS%20significantly%20outperforms%20the%20very%20recent%20methods%2C%20achieving%20state-of-the-art%20performance%20for%20dynamic%20NVS%20under%20motion%20blur.%0ALink%3A%20http%3A//arxiv.org/abs/2504.15122v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoBGS%253A%2520Motion%2520Deblurring%2520Dynamic%25203D%2520Gaussian%2520Splatting%2520for%2520Blurry%2520Monocular%2520Video%26entry.906535625%3DMinh-Quan%2520Viet%2520Bui%2520and%2520Jongmin%2520Park%2520and%2520Juan%2520Luis%2520Gonzalez%2520Bello%2520and%2520Jaeho%2520Moon%2520and%2520Jihyong%2520Oh%2520and%2520Munchurl%2520Kim%26entry.1292438233%3DWe%2520present%2520MoBGS%252C%2520a%2520novel%2520motion%2520deblurring%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520framework%2520capable%2520of%2520reconstructing%2520sharp%2520and%2520high-quality%2520novel%2520spatio-temporal%2520views%2520from%2520blurry%2520monocular%2520videos%2520in%2520an%2520end-to-end%2520manner.%2520Existing%2520dynamic%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520methods%2520are%2520highly%2520sensitive%2520to%2520motion%2520blur%2520in%2520casually%2520captured%2520videos%252C%2520resulting%2520in%2520significant%2520degradation%2520of%2520rendering%2520quality.%2520While%2520recent%2520approaches%2520address%2520motion-blurred%2520inputs%2520for%2520NVS%252C%2520they%2520primarily%2520focus%2520on%2520static%2520scene%2520reconstruction%2520and%2520lack%2520dedicated%2520motion%2520modeling%2520for%2520dynamic%2520objects.%2520To%2520overcome%2520these%2520limitations%252C%2520our%2520MoBGS%2520introduces%2520a%2520novel%2520Blur-adaptive%2520Latent%2520Camera%2520Estimation%2520%2528BLCE%2529%2520method%2520using%2520a%2520proposed%2520Blur-adaptive%2520Neural%2520Ordinary%2520Differential%2520Equation%2520%2528ODE%2529%2520solver%2520for%2520effective%2520latent%2520camera%2520trajectory%2520estimation%252C%2520improving%2520global%2520camera%2520motion%2520deblurring.%2520In%2520addition%252C%2520we%2520propose%2520a%2520Latent%2520Camera-induced%2520Exposure%2520Estimation%2520%2528LCEE%2529%2520method%2520to%2520ensure%2520consistent%2520deblurring%2520of%2520both%2520a%2520global%2520camera%2520and%2520local%2520object%2520motions.%2520Extensive%2520experiments%2520on%2520the%2520Stereo%2520Blur%2520dataset%2520and%2520real-world%2520blurry%2520videos%2520show%2520that%2520our%2520MoBGS%2520significantly%2520outperforms%2520the%2520very%2520recent%2520methods%252C%2520achieving%2520state-of-the-art%2520performance%2520for%2520dynamic%2520NVS%2520under%2520motion%2520blur.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15122v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoBGS%3A%20Motion%20Deblurring%20Dynamic%203D%20Gaussian%20Splatting%20for%20Blurry%20Monocular%20Video&entry.906535625=Minh-Quan%20Viet%20Bui%20and%20Jongmin%20Park%20and%20Juan%20Luis%20Gonzalez%20Bello%20and%20Jaeho%20Moon%20and%20Jihyong%20Oh%20and%20Munchurl%20Kim&entry.1292438233=We%20present%20MoBGS%2C%20a%20novel%20motion%20deblurring%203D%20Gaussian%20Splatting%20%283DGS%29%20framework%20capable%20of%20reconstructing%20sharp%20and%20high-quality%20novel%20spatio-temporal%20views%20from%20blurry%20monocular%20videos%20in%20an%20end-to-end%20manner.%20Existing%20dynamic%20novel%20view%20synthesis%20%28NVS%29%20methods%20are%20highly%20sensitive%20to%20motion%20blur%20in%20casually%20captured%20videos%2C%20resulting%20in%20significant%20degradation%20of%20rendering%20quality.%20While%20recent%20approaches%20address%20motion-blurred%20inputs%20for%20NVS%2C%20they%20primarily%20focus%20on%20static%20scene%20reconstruction%20and%20lack%20dedicated%20motion%20modeling%20for%20dynamic%20objects.%20To%20overcome%20these%20limitations%2C%20our%20MoBGS%20introduces%20a%20novel%20Blur-adaptive%20Latent%20Camera%20Estimation%20%28BLCE%29%20method%20using%20a%20proposed%20Blur-adaptive%20Neural%20Ordinary%20Differential%20Equation%20%28ODE%29%20solver%20for%20effective%20latent%20camera%20trajectory%20estimation%2C%20improving%20global%20camera%20motion%20deblurring.%20In%20addition%2C%20we%20propose%20a%20Latent%20Camera-induced%20Exposure%20Estimation%20%28LCEE%29%20method%20to%20ensure%20consistent%20deblurring%20of%20both%20a%20global%20camera%20and%20local%20object%20motions.%20Extensive%20experiments%20on%20the%20Stereo%20Blur%20dataset%20and%20real-world%20blurry%20videos%20show%20that%20our%20MoBGS%20significantly%20outperforms%20the%20very%20recent%20methods%2C%20achieving%20state-of-the-art%20performance%20for%20dynamic%20NVS%20under%20motion%20blur.&entry.1838667208=http%3A//arxiv.org/abs/2504.15122v4&entry.124074799=Read"},
{"title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling", "author": "Kairun Wen and Yuzhi Huang and Runyu Chen and Hui Zheng and Yunlong Lin and Panwang Pan and Chenxin Li and Wenyan Cong and Jian Zhang and Junbin Lu and Chenguo Lin and Dilin Wang and Zhicheng Yan and Hongyu Xu and Justin Theiss and Yue Huang and Xinghao Ding and Rakesh Ranjan and Zhiwen Fan", "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.", "link": "http://arxiv.org/abs/2512.03000v2", "date": "2025-12-03", "relevancy": 2.591, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6491}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamicVerse%3A%20A%20Physically-Aware%20Multimodal%20Framework%20for%204D%20World%20Modeling&body=Title%3A%20DynamicVerse%3A%20A%20Physically-Aware%20Multimodal%20Framework%20for%204D%20World%20Modeling%0AAuthor%3A%20Kairun%20Wen%20and%20Yuzhi%20Huang%20and%20Runyu%20Chen%20and%20Hui%20Zheng%20and%20Yunlong%20Lin%20and%20Panwang%20Pan%20and%20Chenxin%20Li%20and%20Wenyan%20Cong%20and%20Jian%20Zhang%20and%20Junbin%20Lu%20and%20Chenguo%20Lin%20and%20Dilin%20Wang%20and%20Zhicheng%20Yan%20and%20Hongyu%20Xu%20and%20Justin%20Theiss%20and%20Yue%20Huang%20and%20Xinghao%20Ding%20and%20Rakesh%20Ranjan%20and%20Zhiwen%20Fan%0AAbstract%3A%20Understanding%20the%20dynamic%20physical%20world%2C%20characterized%20by%20its%20evolving%203D%20structure%2C%20real-world%20motion%2C%20and%20semantic%20content%20with%20textual%20descriptions%2C%20is%20crucial%20for%20human-agent%20interaction%20and%20enables%20embodied%20agents%20to%20perceive%20and%20act%20within%20real%20environments%20with%20human-like%20capabilities.%20However%2C%20existing%20datasets%20are%20often%20derived%20from%20limited%20simulators%20or%20utilize%20traditional%20Structurefrom-Motion%20for%20up-to-scale%20annotation%20and%20offer%20limited%20descriptive%20captioning%2C%20which%20restricts%20the%20capacity%20of%20foundation%20models%20to%20accurately%20interpret%20real-world%20dynamics%20from%20monocular%20videos%2C%20commonly%20sourced%20from%20the%20internet.%20To%20bridge%20these%20gaps%2C%20we%20introduce%20DynamicVerse%2C%20a%20physical-scale%2C%20multimodal%204D%20world%20modeling%20framework%20for%20dynamic%20real-world%20video.%20We%20employ%20large%20vision%2C%20geometric%2C%20and%20multimodal%20models%20to%20interpret%20metric-scale%20static%20geometry%2C%20real-world%20dynamic%20motion%2C%20instance-level%20masks%2C%20and%20holistic%20descriptive%20captions.%20By%20integrating%20window-based%20Bundle%20Adjustment%20with%20global%20optimization%2C%20our%20method%20converts%20long%20real-world%20video%20sequences%20into%20a%20comprehensive%204D%20multimodal%20format.%20DynamicVerse%20delivers%20a%20large-scale%20dataset%20consisting%20of%20100K%2B%20videos%20with%20800K%2B%20annotated%20masks%20and%2010M%2B%20frames%20from%20internet%20videos.%20Experimental%20evaluations%20on%20three%20benchmark%20tasks%2C%20namely%20video%20depth%20estimation%2C%20camera%20pose%20estimation%2C%20and%20camera%20intrinsics%20estimation%2C%20demonstrate%20that%20our%204D%20modeling%20achieves%20superior%20performance%20in%20capturing%20physical-scale%20measurements%20with%20greater%20global%20accuracy%20than%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03000v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamicVerse%253A%2520A%2520Physically-Aware%2520Multimodal%2520Framework%2520for%25204D%2520World%2520Modeling%26entry.906535625%3DKairun%2520Wen%2520and%2520Yuzhi%2520Huang%2520and%2520Runyu%2520Chen%2520and%2520Hui%2520Zheng%2520and%2520Yunlong%2520Lin%2520and%2520Panwang%2520Pan%2520and%2520Chenxin%2520Li%2520and%2520Wenyan%2520Cong%2520and%2520Jian%2520Zhang%2520and%2520Junbin%2520Lu%2520and%2520Chenguo%2520Lin%2520and%2520Dilin%2520Wang%2520and%2520Zhicheng%2520Yan%2520and%2520Hongyu%2520Xu%2520and%2520Justin%2520Theiss%2520and%2520Yue%2520Huang%2520and%2520Xinghao%2520Ding%2520and%2520Rakesh%2520Ranjan%2520and%2520Zhiwen%2520Fan%26entry.1292438233%3DUnderstanding%2520the%2520dynamic%2520physical%2520world%252C%2520characterized%2520by%2520its%2520evolving%25203D%2520structure%252C%2520real-world%2520motion%252C%2520and%2520semantic%2520content%2520with%2520textual%2520descriptions%252C%2520is%2520crucial%2520for%2520human-agent%2520interaction%2520and%2520enables%2520embodied%2520agents%2520to%2520perceive%2520and%2520act%2520within%2520real%2520environments%2520with%2520human-like%2520capabilities.%2520However%252C%2520existing%2520datasets%2520are%2520often%2520derived%2520from%2520limited%2520simulators%2520or%2520utilize%2520traditional%2520Structurefrom-Motion%2520for%2520up-to-scale%2520annotation%2520and%2520offer%2520limited%2520descriptive%2520captioning%252C%2520which%2520restricts%2520the%2520capacity%2520of%2520foundation%2520models%2520to%2520accurately%2520interpret%2520real-world%2520dynamics%2520from%2520monocular%2520videos%252C%2520commonly%2520sourced%2520from%2520the%2520internet.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520introduce%2520DynamicVerse%252C%2520a%2520physical-scale%252C%2520multimodal%25204D%2520world%2520modeling%2520framework%2520for%2520dynamic%2520real-world%2520video.%2520We%2520employ%2520large%2520vision%252C%2520geometric%252C%2520and%2520multimodal%2520models%2520to%2520interpret%2520metric-scale%2520static%2520geometry%252C%2520real-world%2520dynamic%2520motion%252C%2520instance-level%2520masks%252C%2520and%2520holistic%2520descriptive%2520captions.%2520By%2520integrating%2520window-based%2520Bundle%2520Adjustment%2520with%2520global%2520optimization%252C%2520our%2520method%2520converts%2520long%2520real-world%2520video%2520sequences%2520into%2520a%2520comprehensive%25204D%2520multimodal%2520format.%2520DynamicVerse%2520delivers%2520a%2520large-scale%2520dataset%2520consisting%2520of%2520100K%252B%2520videos%2520with%2520800K%252B%2520annotated%2520masks%2520and%252010M%252B%2520frames%2520from%2520internet%2520videos.%2520Experimental%2520evaluations%2520on%2520three%2520benchmark%2520tasks%252C%2520namely%2520video%2520depth%2520estimation%252C%2520camera%2520pose%2520estimation%252C%2520and%2520camera%2520intrinsics%2520estimation%252C%2520demonstrate%2520that%2520our%25204D%2520modeling%2520achieves%2520superior%2520performance%2520in%2520capturing%2520physical-scale%2520measurements%2520with%2520greater%2520global%2520accuracy%2520than%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03000v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamicVerse%3A%20A%20Physically-Aware%20Multimodal%20Framework%20for%204D%20World%20Modeling&entry.906535625=Kairun%20Wen%20and%20Yuzhi%20Huang%20and%20Runyu%20Chen%20and%20Hui%20Zheng%20and%20Yunlong%20Lin%20and%20Panwang%20Pan%20and%20Chenxin%20Li%20and%20Wenyan%20Cong%20and%20Jian%20Zhang%20and%20Junbin%20Lu%20and%20Chenguo%20Lin%20and%20Dilin%20Wang%20and%20Zhicheng%20Yan%20and%20Hongyu%20Xu%20and%20Justin%20Theiss%20and%20Yue%20Huang%20and%20Xinghao%20Ding%20and%20Rakesh%20Ranjan%20and%20Zhiwen%20Fan&entry.1292438233=Understanding%20the%20dynamic%20physical%20world%2C%20characterized%20by%20its%20evolving%203D%20structure%2C%20real-world%20motion%2C%20and%20semantic%20content%20with%20textual%20descriptions%2C%20is%20crucial%20for%20human-agent%20interaction%20and%20enables%20embodied%20agents%20to%20perceive%20and%20act%20within%20real%20environments%20with%20human-like%20capabilities.%20However%2C%20existing%20datasets%20are%20often%20derived%20from%20limited%20simulators%20or%20utilize%20traditional%20Structurefrom-Motion%20for%20up-to-scale%20annotation%20and%20offer%20limited%20descriptive%20captioning%2C%20which%20restricts%20the%20capacity%20of%20foundation%20models%20to%20accurately%20interpret%20real-world%20dynamics%20from%20monocular%20videos%2C%20commonly%20sourced%20from%20the%20internet.%20To%20bridge%20these%20gaps%2C%20we%20introduce%20DynamicVerse%2C%20a%20physical-scale%2C%20multimodal%204D%20world%20modeling%20framework%20for%20dynamic%20real-world%20video.%20We%20employ%20large%20vision%2C%20geometric%2C%20and%20multimodal%20models%20to%20interpret%20metric-scale%20static%20geometry%2C%20real-world%20dynamic%20motion%2C%20instance-level%20masks%2C%20and%20holistic%20descriptive%20captions.%20By%20integrating%20window-based%20Bundle%20Adjustment%20with%20global%20optimization%2C%20our%20method%20converts%20long%20real-world%20video%20sequences%20into%20a%20comprehensive%204D%20multimodal%20format.%20DynamicVerse%20delivers%20a%20large-scale%20dataset%20consisting%20of%20100K%2B%20videos%20with%20800K%2B%20annotated%20masks%20and%2010M%2B%20frames%20from%20internet%20videos.%20Experimental%20evaluations%20on%20three%20benchmark%20tasks%2C%20namely%20video%20depth%20estimation%2C%20camera%20pose%20estimation%2C%20and%20camera%20intrinsics%20estimation%2C%20demonstrate%20that%20our%204D%20modeling%20achieves%20superior%20performance%20in%20capturing%20physical-scale%20measurements%20with%20greater%20global%20accuracy%20than%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.03000v2&entry.124074799=Read"},
{"title": "NVRC: Neural Video Representation Compression", "author": "Ho Man Kwan and Ge Gao and Fan Zhang and Andrew Gower and David Bull", "abstract": "Recent advances in implicit neural representation (INR)-based video coding have demonstrated its potential to compete with both conventional and other learning-based approaches. With INR methods, a neural network is trained to overfit a video sequence, with its parameters compressed to obtain a compact representation of the video content. However, although promising results have been achieved, the best INR-based methods are still out-performed by the latest standard codecs, such as VVC VTM, partially due to the simple model compression techniques employed. In this paper, rather than focusing on representation architectures as in many existing works, we propose a novel INR-based video compression framework, Neural Video Representation Compression (NVRC), targeting compression of the representation. Based on the novel entropy coding and quantization models proposed, NVRC, for the first time, is able to optimize an INR-based video codec in a fully end-to-end manner. To further minimize the additional bitrate overhead introduced by the entropy models, we have also proposed a new model compression framework for coding all the network, quantization and entropy model parameters hierarchically. Our experiments show that NVRC outperforms many conventional and learning-based benchmark codecs, with a 24% average coding gain over VVC VTM (Random Access) on the UVG dataset, measured in PSNR. As far as we are aware, this is the first time an INR-based video codec achieving such performance. The implementation of NVRC will be released.", "link": "http://arxiv.org/abs/2409.07414v2", "date": "2025-12-03", "relevancy": 2.5842, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5455}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5082}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NVRC%3A%20Neural%20Video%20Representation%20Compression&body=Title%3A%20NVRC%3A%20Neural%20Video%20Representation%20Compression%0AAuthor%3A%20Ho%20Man%20Kwan%20and%20Ge%20Gao%20and%20Fan%20Zhang%20and%20Andrew%20Gower%20and%20David%20Bull%0AAbstract%3A%20Recent%20advances%20in%20implicit%20neural%20representation%20%28INR%29-based%20video%20coding%20have%20demonstrated%20its%20potential%20to%20compete%20with%20both%20conventional%20and%20other%20learning-based%20approaches.%20With%20INR%20methods%2C%20a%20neural%20network%20is%20trained%20to%20overfit%20a%20video%20sequence%2C%20with%20its%20parameters%20compressed%20to%20obtain%20a%20compact%20representation%20of%20the%20video%20content.%20However%2C%20although%20promising%20results%20have%20been%20achieved%2C%20the%20best%20INR-based%20methods%20are%20still%20out-performed%20by%20the%20latest%20standard%20codecs%2C%20such%20as%20VVC%20VTM%2C%20partially%20due%20to%20the%20simple%20model%20compression%20techniques%20employed.%20In%20this%20paper%2C%20rather%20than%20focusing%20on%20representation%20architectures%20as%20in%20many%20existing%20works%2C%20we%20propose%20a%20novel%20INR-based%20video%20compression%20framework%2C%20Neural%20Video%20Representation%20Compression%20%28NVRC%29%2C%20targeting%20compression%20of%20the%20representation.%20Based%20on%20the%20novel%20entropy%20coding%20and%20quantization%20models%20proposed%2C%20NVRC%2C%20for%20the%20first%20time%2C%20is%20able%20to%20optimize%20an%20INR-based%20video%20codec%20in%20a%20fully%20end-to-end%20manner.%20To%20further%20minimize%20the%20additional%20bitrate%20overhead%20introduced%20by%20the%20entropy%20models%2C%20we%20have%20also%20proposed%20a%20new%20model%20compression%20framework%20for%20coding%20all%20the%20network%2C%20quantization%20and%20entropy%20model%20parameters%20hierarchically.%20Our%20experiments%20show%20that%20NVRC%20outperforms%20many%20conventional%20and%20learning-based%20benchmark%20codecs%2C%20with%20a%2024%25%20average%20coding%20gain%20over%20VVC%20VTM%20%28Random%20Access%29%20on%20the%20UVG%20dataset%2C%20measured%20in%20PSNR.%20As%20far%20as%20we%20are%20aware%2C%20this%20is%20the%20first%20time%20an%20INR-based%20video%20codec%20achieving%20such%20performance.%20The%20implementation%20of%20NVRC%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2409.07414v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNVRC%253A%2520Neural%2520Video%2520Representation%2520Compression%26entry.906535625%3DHo%2520Man%2520Kwan%2520and%2520Ge%2520Gao%2520and%2520Fan%2520Zhang%2520and%2520Andrew%2520Gower%2520and%2520David%2520Bull%26entry.1292438233%3DRecent%2520advances%2520in%2520implicit%2520neural%2520representation%2520%2528INR%2529-based%2520video%2520coding%2520have%2520demonstrated%2520its%2520potential%2520to%2520compete%2520with%2520both%2520conventional%2520and%2520other%2520learning-based%2520approaches.%2520With%2520INR%2520methods%252C%2520a%2520neural%2520network%2520is%2520trained%2520to%2520overfit%2520a%2520video%2520sequence%252C%2520with%2520its%2520parameters%2520compressed%2520to%2520obtain%2520a%2520compact%2520representation%2520of%2520the%2520video%2520content.%2520However%252C%2520although%2520promising%2520results%2520have%2520been%2520achieved%252C%2520the%2520best%2520INR-based%2520methods%2520are%2520still%2520out-performed%2520by%2520the%2520latest%2520standard%2520codecs%252C%2520such%2520as%2520VVC%2520VTM%252C%2520partially%2520due%2520to%2520the%2520simple%2520model%2520compression%2520techniques%2520employed.%2520In%2520this%2520paper%252C%2520rather%2520than%2520focusing%2520on%2520representation%2520architectures%2520as%2520in%2520many%2520existing%2520works%252C%2520we%2520propose%2520a%2520novel%2520INR-based%2520video%2520compression%2520framework%252C%2520Neural%2520Video%2520Representation%2520Compression%2520%2528NVRC%2529%252C%2520targeting%2520compression%2520of%2520the%2520representation.%2520Based%2520on%2520the%2520novel%2520entropy%2520coding%2520and%2520quantization%2520models%2520proposed%252C%2520NVRC%252C%2520for%2520the%2520first%2520time%252C%2520is%2520able%2520to%2520optimize%2520an%2520INR-based%2520video%2520codec%2520in%2520a%2520fully%2520end-to-end%2520manner.%2520To%2520further%2520minimize%2520the%2520additional%2520bitrate%2520overhead%2520introduced%2520by%2520the%2520entropy%2520models%252C%2520we%2520have%2520also%2520proposed%2520a%2520new%2520model%2520compression%2520framework%2520for%2520coding%2520all%2520the%2520network%252C%2520quantization%2520and%2520entropy%2520model%2520parameters%2520hierarchically.%2520Our%2520experiments%2520show%2520that%2520NVRC%2520outperforms%2520many%2520conventional%2520and%2520learning-based%2520benchmark%2520codecs%252C%2520with%2520a%252024%2525%2520average%2520coding%2520gain%2520over%2520VVC%2520VTM%2520%2528Random%2520Access%2529%2520on%2520the%2520UVG%2520dataset%252C%2520measured%2520in%2520PSNR.%2520As%2520far%2520as%2520we%2520are%2520aware%252C%2520this%2520is%2520the%2520first%2520time%2520an%2520INR-based%2520video%2520codec%2520achieving%2520such%2520performance.%2520The%2520implementation%2520of%2520NVRC%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07414v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NVRC%3A%20Neural%20Video%20Representation%20Compression&entry.906535625=Ho%20Man%20Kwan%20and%20Ge%20Gao%20and%20Fan%20Zhang%20and%20Andrew%20Gower%20and%20David%20Bull&entry.1292438233=Recent%20advances%20in%20implicit%20neural%20representation%20%28INR%29-based%20video%20coding%20have%20demonstrated%20its%20potential%20to%20compete%20with%20both%20conventional%20and%20other%20learning-based%20approaches.%20With%20INR%20methods%2C%20a%20neural%20network%20is%20trained%20to%20overfit%20a%20video%20sequence%2C%20with%20its%20parameters%20compressed%20to%20obtain%20a%20compact%20representation%20of%20the%20video%20content.%20However%2C%20although%20promising%20results%20have%20been%20achieved%2C%20the%20best%20INR-based%20methods%20are%20still%20out-performed%20by%20the%20latest%20standard%20codecs%2C%20such%20as%20VVC%20VTM%2C%20partially%20due%20to%20the%20simple%20model%20compression%20techniques%20employed.%20In%20this%20paper%2C%20rather%20than%20focusing%20on%20representation%20architectures%20as%20in%20many%20existing%20works%2C%20we%20propose%20a%20novel%20INR-based%20video%20compression%20framework%2C%20Neural%20Video%20Representation%20Compression%20%28NVRC%29%2C%20targeting%20compression%20of%20the%20representation.%20Based%20on%20the%20novel%20entropy%20coding%20and%20quantization%20models%20proposed%2C%20NVRC%2C%20for%20the%20first%20time%2C%20is%20able%20to%20optimize%20an%20INR-based%20video%20codec%20in%20a%20fully%20end-to-end%20manner.%20To%20further%20minimize%20the%20additional%20bitrate%20overhead%20introduced%20by%20the%20entropy%20models%2C%20we%20have%20also%20proposed%20a%20new%20model%20compression%20framework%20for%20coding%20all%20the%20network%2C%20quantization%20and%20entropy%20model%20parameters%20hierarchically.%20Our%20experiments%20show%20that%20NVRC%20outperforms%20many%20conventional%20and%20learning-based%20benchmark%20codecs%2C%20with%20a%2024%25%20average%20coding%20gain%20over%20VVC%20VTM%20%28Random%20Access%29%20on%20the%20UVG%20dataset%2C%20measured%20in%20PSNR.%20As%20far%20as%20we%20are%20aware%2C%20this%20is%20the%20first%20time%20an%20INR-based%20video%20codec%20achieving%20such%20performance.%20The%20implementation%20of%20NVRC%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2409.07414v2&entry.124074799=Read"},
{"title": "Stable Signer: Hierarchical Sign Language Generative Model", "author": "Sen Fang and Yalin Feng and Hongbin Zhong and Yanxin Zhang and Dimitris N. Metaxas", "abstract": "Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.", "link": "http://arxiv.org/abs/2512.04048v1", "date": "2025-12-03", "relevancy": 2.5604, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5366}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5121}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Signer%3A%20Hierarchical%20Sign%20Language%20Generative%20Model&body=Title%3A%20Stable%20Signer%3A%20Hierarchical%20Sign%20Language%20Generative%20Model%0AAuthor%3A%20Sen%20Fang%20and%20Yalin%20Feng%20and%20Hongbin%20Zhong%20and%20Yanxin%20Zhang%20and%20Dimitris%20N.%20Metaxas%0AAbstract%3A%20Sign%20Language%20Production%20%28SLP%29%20is%20the%20process%20of%20converting%20the%20complex%20input%20text%20into%20a%20real%20video.%20Most%20previous%20works%20focused%20on%20the%20Text2Gloss%2C%20Gloss2Pose%2C%20Pose2Vid%20stages%2C%20and%20some%20concentrated%20on%20Prompt2Gloss%20and%20Text2Avatar%20stages.%20However%2C%20this%20field%20has%20made%20slow%20progress%20due%20to%20the%20inaccuracy%20of%20text%20conversion%2C%20pose%20generation%2C%20and%20the%20rendering%20of%20poses%20into%20real%20human%20videos%20in%20these%20stages%2C%20resulting%20in%20gradually%20accumulating%20errors.%20Therefore%2C%20in%20this%20paper%2C%20we%20streamline%20the%20traditional%20redundant%20structure%2C%20simplify%20and%20optimize%20the%20task%20objective%2C%20and%20design%20a%20new%20sign%20language%20generative%20model%20called%20Stable%20Signer.%20It%20redefines%20the%20SLP%20task%20as%20a%20hierarchical%20generation%20end-to-end%20task%20that%20only%20includes%20text%20understanding%20%28Prompt2Gloss%2C%20Text2Gloss%29%20and%20Pose2Vid%2C%20and%20executes%20text%20understanding%20through%20our%20proposed%20new%20Sign%20Language%20Understanding%20Linker%20called%20SLUL%2C%20and%20generates%20hand%20gestures%20through%20the%20named%20SLP-MoE%20hand%20gesture%20rendering%20expert%20block%20to%20end-to-end%20generate%20high-quality%20and%20multi-style%20sign%20language%20videos.%20SLUL%20is%20trained%20using%20the%20newly%20developed%20Semantic-Aware%20Gloss%20Masking%20Loss%20%28SAGM%20Loss%29.%20Its%20performance%20has%20improved%20by%2048.6%25%20compared%20to%20the%20current%20SOTA%20generation%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Signer%253A%2520Hierarchical%2520Sign%2520Language%2520Generative%2520Model%26entry.906535625%3DSen%2520Fang%2520and%2520Yalin%2520Feng%2520and%2520Hongbin%2520Zhong%2520and%2520Yanxin%2520Zhang%2520and%2520Dimitris%2520N.%2520Metaxas%26entry.1292438233%3DSign%2520Language%2520Production%2520%2528SLP%2529%2520is%2520the%2520process%2520of%2520converting%2520the%2520complex%2520input%2520text%2520into%2520a%2520real%2520video.%2520Most%2520previous%2520works%2520focused%2520on%2520the%2520Text2Gloss%252C%2520Gloss2Pose%252C%2520Pose2Vid%2520stages%252C%2520and%2520some%2520concentrated%2520on%2520Prompt2Gloss%2520and%2520Text2Avatar%2520stages.%2520However%252C%2520this%2520field%2520has%2520made%2520slow%2520progress%2520due%2520to%2520the%2520inaccuracy%2520of%2520text%2520conversion%252C%2520pose%2520generation%252C%2520and%2520the%2520rendering%2520of%2520poses%2520into%2520real%2520human%2520videos%2520in%2520these%2520stages%252C%2520resulting%2520in%2520gradually%2520accumulating%2520errors.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520streamline%2520the%2520traditional%2520redundant%2520structure%252C%2520simplify%2520and%2520optimize%2520the%2520task%2520objective%252C%2520and%2520design%2520a%2520new%2520sign%2520language%2520generative%2520model%2520called%2520Stable%2520Signer.%2520It%2520redefines%2520the%2520SLP%2520task%2520as%2520a%2520hierarchical%2520generation%2520end-to-end%2520task%2520that%2520only%2520includes%2520text%2520understanding%2520%2528Prompt2Gloss%252C%2520Text2Gloss%2529%2520and%2520Pose2Vid%252C%2520and%2520executes%2520text%2520understanding%2520through%2520our%2520proposed%2520new%2520Sign%2520Language%2520Understanding%2520Linker%2520called%2520SLUL%252C%2520and%2520generates%2520hand%2520gestures%2520through%2520the%2520named%2520SLP-MoE%2520hand%2520gesture%2520rendering%2520expert%2520block%2520to%2520end-to-end%2520generate%2520high-quality%2520and%2520multi-style%2520sign%2520language%2520videos.%2520SLUL%2520is%2520trained%2520using%2520the%2520newly%2520developed%2520Semantic-Aware%2520Gloss%2520Masking%2520Loss%2520%2528SAGM%2520Loss%2529.%2520Its%2520performance%2520has%2520improved%2520by%252048.6%2525%2520compared%2520to%2520the%2520current%2520SOTA%2520generation%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Signer%3A%20Hierarchical%20Sign%20Language%20Generative%20Model&entry.906535625=Sen%20Fang%20and%20Yalin%20Feng%20and%20Hongbin%20Zhong%20and%20Yanxin%20Zhang%20and%20Dimitris%20N.%20Metaxas&entry.1292438233=Sign%20Language%20Production%20%28SLP%29%20is%20the%20process%20of%20converting%20the%20complex%20input%20text%20into%20a%20real%20video.%20Most%20previous%20works%20focused%20on%20the%20Text2Gloss%2C%20Gloss2Pose%2C%20Pose2Vid%20stages%2C%20and%20some%20concentrated%20on%20Prompt2Gloss%20and%20Text2Avatar%20stages.%20However%2C%20this%20field%20has%20made%20slow%20progress%20due%20to%20the%20inaccuracy%20of%20text%20conversion%2C%20pose%20generation%2C%20and%20the%20rendering%20of%20poses%20into%20real%20human%20videos%20in%20these%20stages%2C%20resulting%20in%20gradually%20accumulating%20errors.%20Therefore%2C%20in%20this%20paper%2C%20we%20streamline%20the%20traditional%20redundant%20structure%2C%20simplify%20and%20optimize%20the%20task%20objective%2C%20and%20design%20a%20new%20sign%20language%20generative%20model%20called%20Stable%20Signer.%20It%20redefines%20the%20SLP%20task%20as%20a%20hierarchical%20generation%20end-to-end%20task%20that%20only%20includes%20text%20understanding%20%28Prompt2Gloss%2C%20Text2Gloss%29%20and%20Pose2Vid%2C%20and%20executes%20text%20understanding%20through%20our%20proposed%20new%20Sign%20Language%20Understanding%20Linker%20called%20SLUL%2C%20and%20generates%20hand%20gestures%20through%20the%20named%20SLP-MoE%20hand%20gesture%20rendering%20expert%20block%20to%20end-to-end%20generate%20high-quality%20and%20multi-style%20sign%20language%20videos.%20SLUL%20is%20trained%20using%20the%20newly%20developed%20Semantic-Aware%20Gloss%20Masking%20Loss%20%28SAGM%20Loss%29.%20Its%20performance%20has%20improved%20by%2048.6%25%20compared%20to%20the%20current%20SOTA%20generation%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.04048v1&entry.124074799=Read"},
{"title": "Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs", "author": "Tengyun Ma and Jiaqi Yao and Daojing He and Shihao Peng and Yu Li and Shaohui Liu and Zhuotao Tian", "abstract": "Large Language Models (LLMs) have emerged as powerful tools for diverse applications. However, their uniform token processing paradigm introduces critical vulnerabilities in instruction handling, particularly when exposed to adversarial scenarios. In this work, we identify and propose a novel class of vulnerabilities, termed Tool-Completion Attack (TCA), which exploits function-calling mechanisms to subvert model behavior. To evaluate LLM robustness against such threats, we introduce the Tool-Completion benchmark, a comprehensive security assessment framework, which reveals that even state-of-the-art models remain susceptible to TCA, with surprisingly high attack success rates. To address these vulnerabilities, we introduce Context-Aware Hierarchical Learning (CAHL), a sophisticated mechanism that dynamically balances semantic comprehension with role-specific instruction constraints. CAHL leverages the contextual correlations between different instruction segments to establish a robust, context-aware instruction hierarchy. Extensive experiments demonstrate that CAHL significantly enhances LLM robustness against both conventional attacks and the proposed TCA, exhibiting strong generalization capabilities in zero-shot evaluations while still preserving model performance on generic tasks. Our code is available at https://github.com/S2AILab/CAHL.", "link": "http://arxiv.org/abs/2512.03720v1", "date": "2025-12-03", "relevancy": 2.5495, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5313}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Hierarchical%20Learning%3A%20A%20Two-Step%20Paradigm%20towards%20Safer%20LLMs&body=Title%3A%20Context-Aware%20Hierarchical%20Learning%3A%20A%20Two-Step%20Paradigm%20towards%20Safer%20LLMs%0AAuthor%3A%20Tengyun%20Ma%20and%20Jiaqi%20Yao%20and%20Daojing%20He%20and%20Shihao%20Peng%20and%20Yu%20Li%20and%20Shaohui%20Liu%20and%20Zhuotao%20Tian%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20for%20diverse%20applications.%20However%2C%20their%20uniform%20token%20processing%20paradigm%20introduces%20critical%20vulnerabilities%20in%20instruction%20handling%2C%20particularly%20when%20exposed%20to%20adversarial%20scenarios.%20In%20this%20work%2C%20we%20identify%20and%20propose%20a%20novel%20class%20of%20vulnerabilities%2C%20termed%20Tool-Completion%20Attack%20%28TCA%29%2C%20which%20exploits%20function-calling%20mechanisms%20to%20subvert%20model%20behavior.%20To%20evaluate%20LLM%20robustness%20against%20such%20threats%2C%20we%20introduce%20the%20Tool-Completion%20benchmark%2C%20a%20comprehensive%20security%20assessment%20framework%2C%20which%20reveals%20that%20even%20state-of-the-art%20models%20remain%20susceptible%20to%20TCA%2C%20with%20surprisingly%20high%20attack%20success%20rates.%20To%20address%20these%20vulnerabilities%2C%20we%20introduce%20Context-Aware%20Hierarchical%20Learning%20%28CAHL%29%2C%20a%20sophisticated%20mechanism%20that%20dynamically%20balances%20semantic%20comprehension%20with%20role-specific%20instruction%20constraints.%20CAHL%20leverages%20the%20contextual%20correlations%20between%20different%20instruction%20segments%20to%20establish%20a%20robust%2C%20context-aware%20instruction%20hierarchy.%20Extensive%20experiments%20demonstrate%20that%20CAHL%20significantly%20enhances%20LLM%20robustness%20against%20both%20conventional%20attacks%20and%20the%20proposed%20TCA%2C%20exhibiting%20strong%20generalization%20capabilities%20in%20zero-shot%20evaluations%20while%20still%20preserving%20model%20performance%20on%20generic%20tasks.%20Our%20code%20is%20available%20at%20https%3A//github.com/S2AILab/CAHL.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Hierarchical%2520Learning%253A%2520A%2520Two-Step%2520Paradigm%2520towards%2520Safer%2520LLMs%26entry.906535625%3DTengyun%2520Ma%2520and%2520Jiaqi%2520Yao%2520and%2520Daojing%2520He%2520and%2520Shihao%2520Peng%2520and%2520Yu%2520Li%2520and%2520Shaohui%2520Liu%2520and%2520Zhuotao%2520Tian%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520diverse%2520applications.%2520However%252C%2520their%2520uniform%2520token%2520processing%2520paradigm%2520introduces%2520critical%2520vulnerabilities%2520in%2520instruction%2520handling%252C%2520particularly%2520when%2520exposed%2520to%2520adversarial%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520identify%2520and%2520propose%2520a%2520novel%2520class%2520of%2520vulnerabilities%252C%2520termed%2520Tool-Completion%2520Attack%2520%2528TCA%2529%252C%2520which%2520exploits%2520function-calling%2520mechanisms%2520to%2520subvert%2520model%2520behavior.%2520To%2520evaluate%2520LLM%2520robustness%2520against%2520such%2520threats%252C%2520we%2520introduce%2520the%2520Tool-Completion%2520benchmark%252C%2520a%2520comprehensive%2520security%2520assessment%2520framework%252C%2520which%2520reveals%2520that%2520even%2520state-of-the-art%2520models%2520remain%2520susceptible%2520to%2520TCA%252C%2520with%2520surprisingly%2520high%2520attack%2520success%2520rates.%2520To%2520address%2520these%2520vulnerabilities%252C%2520we%2520introduce%2520Context-Aware%2520Hierarchical%2520Learning%2520%2528CAHL%2529%252C%2520a%2520sophisticated%2520mechanism%2520that%2520dynamically%2520balances%2520semantic%2520comprehension%2520with%2520role-specific%2520instruction%2520constraints.%2520CAHL%2520leverages%2520the%2520contextual%2520correlations%2520between%2520different%2520instruction%2520segments%2520to%2520establish%2520a%2520robust%252C%2520context-aware%2520instruction%2520hierarchy.%2520Extensive%2520experiments%2520demonstrate%2520that%2520CAHL%2520significantly%2520enhances%2520LLM%2520robustness%2520against%2520both%2520conventional%2520attacks%2520and%2520the%2520proposed%2520TCA%252C%2520exhibiting%2520strong%2520generalization%2520capabilities%2520in%2520zero-shot%2520evaluations%2520while%2520still%2520preserving%2520model%2520performance%2520on%2520generic%2520tasks.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/S2AILab/CAHL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Hierarchical%20Learning%3A%20A%20Two-Step%20Paradigm%20towards%20Safer%20LLMs&entry.906535625=Tengyun%20Ma%20and%20Jiaqi%20Yao%20and%20Daojing%20He%20and%20Shihao%20Peng%20and%20Yu%20Li%20and%20Shaohui%20Liu%20and%20Zhuotao%20Tian&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20for%20diverse%20applications.%20However%2C%20their%20uniform%20token%20processing%20paradigm%20introduces%20critical%20vulnerabilities%20in%20instruction%20handling%2C%20particularly%20when%20exposed%20to%20adversarial%20scenarios.%20In%20this%20work%2C%20we%20identify%20and%20propose%20a%20novel%20class%20of%20vulnerabilities%2C%20termed%20Tool-Completion%20Attack%20%28TCA%29%2C%20which%20exploits%20function-calling%20mechanisms%20to%20subvert%20model%20behavior.%20To%20evaluate%20LLM%20robustness%20against%20such%20threats%2C%20we%20introduce%20the%20Tool-Completion%20benchmark%2C%20a%20comprehensive%20security%20assessment%20framework%2C%20which%20reveals%20that%20even%20state-of-the-art%20models%20remain%20susceptible%20to%20TCA%2C%20with%20surprisingly%20high%20attack%20success%20rates.%20To%20address%20these%20vulnerabilities%2C%20we%20introduce%20Context-Aware%20Hierarchical%20Learning%20%28CAHL%29%2C%20a%20sophisticated%20mechanism%20that%20dynamically%20balances%20semantic%20comprehension%20with%20role-specific%20instruction%20constraints.%20CAHL%20leverages%20the%20contextual%20correlations%20between%20different%20instruction%20segments%20to%20establish%20a%20robust%2C%20context-aware%20instruction%20hierarchy.%20Extensive%20experiments%20demonstrate%20that%20CAHL%20significantly%20enhances%20LLM%20robustness%20against%20both%20conventional%20attacks%20and%20the%20proposed%20TCA%2C%20exhibiting%20strong%20generalization%20capabilities%20in%20zero-shot%20evaluations%20while%20still%20preserving%20model%20performance%20on%20generic%20tasks.%20Our%20code%20is%20available%20at%20https%3A//github.com/S2AILab/CAHL.&entry.1838667208=http%3A//arxiv.org/abs/2512.03720v1&entry.124074799=Read"},
{"title": "GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras", "author": "Ekaterina Filimoshina and Dmitry Shirokov", "abstract": "We propose, implement, and compare with competitors a new architecture of equivariant neural networks based on geometric (Clifford) algebras: Generalized Lipschitz Group Equivariant Neural Networks (GLGENN). These networks are equivariant to all pseudo-orthogonal transformations, including rotations and reflections, of a vector space with any non-degenerate or degenerate symmetric bilinear form. We propose a weight-sharing parametrization technique that takes into account the fundamental structures and operations of geometric algebras. Due to this technique, GLGENN architecture is parameter-light and has less tendency to overfitting than baseline equivariant models. GLGENN outperforms or matches competitors on several benchmarking equivariant tasks, including estimation of an equivariant function and a convex hull experiment, while using significantly fewer optimizable parameters.", "link": "http://arxiv.org/abs/2506.09625v2", "date": "2025-12-03", "relevancy": 2.5491, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5197}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5058}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLGENN%3A%20A%20Novel%20Parameter-Light%20Equivariant%20Neural%20Networks%20Architecture%20Based%20on%20Clifford%20Geometric%20Algebras&body=Title%3A%20GLGENN%3A%20A%20Novel%20Parameter-Light%20Equivariant%20Neural%20Networks%20Architecture%20Based%20on%20Clifford%20Geometric%20Algebras%0AAuthor%3A%20Ekaterina%20Filimoshina%20and%20Dmitry%20Shirokov%0AAbstract%3A%20We%20propose%2C%20implement%2C%20and%20compare%20with%20competitors%20a%20new%20architecture%20of%20equivariant%20neural%20networks%20based%20on%20geometric%20%28Clifford%29%20algebras%3A%20Generalized%20Lipschitz%20Group%20Equivariant%20Neural%20Networks%20%28GLGENN%29.%20These%20networks%20are%20equivariant%20to%20all%20pseudo-orthogonal%20transformations%2C%20including%20rotations%20and%20reflections%2C%20of%20a%20vector%20space%20with%20any%20non-degenerate%20or%20degenerate%20symmetric%20bilinear%20form.%20We%20propose%20a%20weight-sharing%20parametrization%20technique%20that%20takes%20into%20account%20the%20fundamental%20structures%20and%20operations%20of%20geometric%20algebras.%20Due%20to%20this%20technique%2C%20GLGENN%20architecture%20is%20parameter-light%20and%20has%20less%20tendency%20to%20overfitting%20than%20baseline%20equivariant%20models.%20GLGENN%20outperforms%20or%20matches%20competitors%20on%20several%20benchmarking%20equivariant%20tasks%2C%20including%20estimation%20of%20an%20equivariant%20function%20and%20a%20convex%20hull%20experiment%2C%20while%20using%20significantly%20fewer%20optimizable%20parameters.%0ALink%3A%20http%3A//arxiv.org/abs/2506.09625v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLGENN%253A%2520A%2520Novel%2520Parameter-Light%2520Equivariant%2520Neural%2520Networks%2520Architecture%2520Based%2520on%2520Clifford%2520Geometric%2520Algebras%26entry.906535625%3DEkaterina%2520Filimoshina%2520and%2520Dmitry%2520Shirokov%26entry.1292438233%3DWe%2520propose%252C%2520implement%252C%2520and%2520compare%2520with%2520competitors%2520a%2520new%2520architecture%2520of%2520equivariant%2520neural%2520networks%2520based%2520on%2520geometric%2520%2528Clifford%2529%2520algebras%253A%2520Generalized%2520Lipschitz%2520Group%2520Equivariant%2520Neural%2520Networks%2520%2528GLGENN%2529.%2520These%2520networks%2520are%2520equivariant%2520to%2520all%2520pseudo-orthogonal%2520transformations%252C%2520including%2520rotations%2520and%2520reflections%252C%2520of%2520a%2520vector%2520space%2520with%2520any%2520non-degenerate%2520or%2520degenerate%2520symmetric%2520bilinear%2520form.%2520We%2520propose%2520a%2520weight-sharing%2520parametrization%2520technique%2520that%2520takes%2520into%2520account%2520the%2520fundamental%2520structures%2520and%2520operations%2520of%2520geometric%2520algebras.%2520Due%2520to%2520this%2520technique%252C%2520GLGENN%2520architecture%2520is%2520parameter-light%2520and%2520has%2520less%2520tendency%2520to%2520overfitting%2520than%2520baseline%2520equivariant%2520models.%2520GLGENN%2520outperforms%2520or%2520matches%2520competitors%2520on%2520several%2520benchmarking%2520equivariant%2520tasks%252C%2520including%2520estimation%2520of%2520an%2520equivariant%2520function%2520and%2520a%2520convex%2520hull%2520experiment%252C%2520while%2520using%2520significantly%2520fewer%2520optimizable%2520parameters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09625v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLGENN%3A%20A%20Novel%20Parameter-Light%20Equivariant%20Neural%20Networks%20Architecture%20Based%20on%20Clifford%20Geometric%20Algebras&entry.906535625=Ekaterina%20Filimoshina%20and%20Dmitry%20Shirokov&entry.1292438233=We%20propose%2C%20implement%2C%20and%20compare%20with%20competitors%20a%20new%20architecture%20of%20equivariant%20neural%20networks%20based%20on%20geometric%20%28Clifford%29%20algebras%3A%20Generalized%20Lipschitz%20Group%20Equivariant%20Neural%20Networks%20%28GLGENN%29.%20These%20networks%20are%20equivariant%20to%20all%20pseudo-orthogonal%20transformations%2C%20including%20rotations%20and%20reflections%2C%20of%20a%20vector%20space%20with%20any%20non-degenerate%20or%20degenerate%20symmetric%20bilinear%20form.%20We%20propose%20a%20weight-sharing%20parametrization%20technique%20that%20takes%20into%20account%20the%20fundamental%20structures%20and%20operations%20of%20geometric%20algebras.%20Due%20to%20this%20technique%2C%20GLGENN%20architecture%20is%20parameter-light%20and%20has%20less%20tendency%20to%20overfitting%20than%20baseline%20equivariant%20models.%20GLGENN%20outperforms%20or%20matches%20competitors%20on%20several%20benchmarking%20equivariant%20tasks%2C%20including%20estimation%20of%20an%20equivariant%20function%20and%20a%20convex%20hull%20experiment%2C%20while%20using%20significantly%20fewer%20optimizable%20parameters.&entry.1838667208=http%3A//arxiv.org/abs/2506.09625v2&entry.124074799=Read"},
{"title": "Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail", "author": "Luu Trong Nhan and Luu Trung Duong and Pham Ngoc Nam and Truong Cong Thang", "abstract": "Spiking Neural Networks (SNNs) have attracted growing interest in both computational neuroscience and artificial intelligence, primarily due to their inherent energy efficiency and compact memory footprint. However, achieving adversarial robustness in SNNs, (particularly for vision-related tasks) remains a nascent and underexplored challenge. Recent studies have proposed leveraging sparse gradients as a form of regularization to enhance robustness against adversarial perturbations. In this work, we present a surprising finding: under specific architectural configurations, SNNs exhibit natural gradient sparsity and can achieve state-of-the-art adversarial defense performance without the need for any explicit regularization. Further analysis reveals a trade-off between robustness and generalization: while sparse gradients contribute to improved adversarial resilience, they can impair the model's ability to generalize; conversely, denser gradients support better generalization but increase vulnerability to attacks. Our findings offer new insights into the dual role of gradient sparsity in SNN training.", "link": "http://arxiv.org/abs/2509.23762v3", "date": "2025-12-03", "relevancy": 2.5071, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5623}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4746}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accuracy-Robustness%20Trade%20Off%20via%20Spiking%20Neural%20Network%20Gradient%20Sparsity%20Trail&body=Title%3A%20Accuracy-Robustness%20Trade%20Off%20via%20Spiking%20Neural%20Network%20Gradient%20Sparsity%20Trail%0AAuthor%3A%20Luu%20Trong%20Nhan%20and%20Luu%20Trung%20Duong%20and%20Pham%20Ngoc%20Nam%20and%20Truong%20Cong%20Thang%0AAbstract%3A%20Spiking%20Neural%20Networks%20%28SNNs%29%20have%20attracted%20growing%20interest%20in%20both%20computational%20neuroscience%20and%20artificial%20intelligence%2C%20primarily%20due%20to%20their%20inherent%20energy%20efficiency%20and%20compact%20memory%20footprint.%20However%2C%20achieving%20adversarial%20robustness%20in%20SNNs%2C%20%28particularly%20for%20vision-related%20tasks%29%20remains%20a%20nascent%20and%20underexplored%20challenge.%20Recent%20studies%20have%20proposed%20leveraging%20sparse%20gradients%20as%20a%20form%20of%20regularization%20to%20enhance%20robustness%20against%20adversarial%20perturbations.%20In%20this%20work%2C%20we%20present%20a%20surprising%20finding%3A%20under%20specific%20architectural%20configurations%2C%20SNNs%20exhibit%20natural%20gradient%20sparsity%20and%20can%20achieve%20state-of-the-art%20adversarial%20defense%20performance%20without%20the%20need%20for%20any%20explicit%20regularization.%20Further%20analysis%20reveals%20a%20trade-off%20between%20robustness%20and%20generalization%3A%20while%20sparse%20gradients%20contribute%20to%20improved%20adversarial%20resilience%2C%20they%20can%20impair%20the%20model%27s%20ability%20to%20generalize%3B%20conversely%2C%20denser%20gradients%20support%20better%20generalization%20but%20increase%20vulnerability%20to%20attacks.%20Our%20findings%20offer%20new%20insights%20into%20the%20dual%20role%20of%20gradient%20sparsity%20in%20SNN%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2509.23762v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccuracy-Robustness%2520Trade%2520Off%2520via%2520Spiking%2520Neural%2520Network%2520Gradient%2520Sparsity%2520Trail%26entry.906535625%3DLuu%2520Trong%2520Nhan%2520and%2520Luu%2520Trung%2520Duong%2520and%2520Pham%2520Ngoc%2520Nam%2520and%2520Truong%2520Cong%2520Thang%26entry.1292438233%3DSpiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520have%2520attracted%2520growing%2520interest%2520in%2520both%2520computational%2520neuroscience%2520and%2520artificial%2520intelligence%252C%2520primarily%2520due%2520to%2520their%2520inherent%2520energy%2520efficiency%2520and%2520compact%2520memory%2520footprint.%2520However%252C%2520achieving%2520adversarial%2520robustness%2520in%2520SNNs%252C%2520%2528particularly%2520for%2520vision-related%2520tasks%2529%2520remains%2520a%2520nascent%2520and%2520underexplored%2520challenge.%2520Recent%2520studies%2520have%2520proposed%2520leveraging%2520sparse%2520gradients%2520as%2520a%2520form%2520of%2520regularization%2520to%2520enhance%2520robustness%2520against%2520adversarial%2520perturbations.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520surprising%2520finding%253A%2520under%2520specific%2520architectural%2520configurations%252C%2520SNNs%2520exhibit%2520natural%2520gradient%2520sparsity%2520and%2520can%2520achieve%2520state-of-the-art%2520adversarial%2520defense%2520performance%2520without%2520the%2520need%2520for%2520any%2520explicit%2520regularization.%2520Further%2520analysis%2520reveals%2520a%2520trade-off%2520between%2520robustness%2520and%2520generalization%253A%2520while%2520sparse%2520gradients%2520contribute%2520to%2520improved%2520adversarial%2520resilience%252C%2520they%2520can%2520impair%2520the%2520model%2527s%2520ability%2520to%2520generalize%253B%2520conversely%252C%2520denser%2520gradients%2520support%2520better%2520generalization%2520but%2520increase%2520vulnerability%2520to%2520attacks.%2520Our%2520findings%2520offer%2520new%2520insights%2520into%2520the%2520dual%2520role%2520of%2520gradient%2520sparsity%2520in%2520SNN%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23762v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accuracy-Robustness%20Trade%20Off%20via%20Spiking%20Neural%20Network%20Gradient%20Sparsity%20Trail&entry.906535625=Luu%20Trong%20Nhan%20and%20Luu%20Trung%20Duong%20and%20Pham%20Ngoc%20Nam%20and%20Truong%20Cong%20Thang&entry.1292438233=Spiking%20Neural%20Networks%20%28SNNs%29%20have%20attracted%20growing%20interest%20in%20both%20computational%20neuroscience%20and%20artificial%20intelligence%2C%20primarily%20due%20to%20their%20inherent%20energy%20efficiency%20and%20compact%20memory%20footprint.%20However%2C%20achieving%20adversarial%20robustness%20in%20SNNs%2C%20%28particularly%20for%20vision-related%20tasks%29%20remains%20a%20nascent%20and%20underexplored%20challenge.%20Recent%20studies%20have%20proposed%20leveraging%20sparse%20gradients%20as%20a%20form%20of%20regularization%20to%20enhance%20robustness%20against%20adversarial%20perturbations.%20In%20this%20work%2C%20we%20present%20a%20surprising%20finding%3A%20under%20specific%20architectural%20configurations%2C%20SNNs%20exhibit%20natural%20gradient%20sparsity%20and%20can%20achieve%20state-of-the-art%20adversarial%20defense%20performance%20without%20the%20need%20for%20any%20explicit%20regularization.%20Further%20analysis%20reveals%20a%20trade-off%20between%20robustness%20and%20generalization%3A%20while%20sparse%20gradients%20contribute%20to%20improved%20adversarial%20resilience%2C%20they%20can%20impair%20the%20model%27s%20ability%20to%20generalize%3B%20conversely%2C%20denser%20gradients%20support%20better%20generalization%20but%20increase%20vulnerability%20to%20attacks.%20Our%20findings%20offer%20new%20insights%20into%20the%20dual%20role%20of%20gradient%20sparsity%20in%20SNN%20training.&entry.1838667208=http%3A//arxiv.org/abs/2509.23762v3&entry.124074799=Read"},
{"title": "SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting", "author": "Hanxiu Zhang and Yue Zheng", "abstract": "The protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods -- whether behavior-based or structural -- suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at https://github.com/HanxiuZhang/SELF_v2.", "link": "http://arxiv.org/abs/2512.03620v1", "date": "2025-12-03", "relevancy": 2.4949, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5061}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5002}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SELF%3A%20A%20Robust%20Singular%20Value%20and%20Eigenvalue%20Approach%20for%20LLM%20Fingerprinting&body=Title%3A%20SELF%3A%20A%20Robust%20Singular%20Value%20and%20Eigenvalue%20Approach%20for%20LLM%20Fingerprinting%0AAuthor%3A%20Hanxiu%20Zhang%20and%20Yue%20Zheng%0AAbstract%3A%20The%20protection%20of%20Intellectual%20Property%20%28IP%29%20in%20Large%20Language%20Models%20%28LLMs%29%20represents%20a%20critical%20challenge%20in%20contemporary%20AI%20research.%20While%20fingerprinting%20techniques%20have%20emerged%20as%20a%20fundamental%20mechanism%20for%20detecting%20unauthorized%20model%20usage%2C%20existing%20methods%20--%20whether%20behavior-based%20or%20structural%20--%20suffer%20from%20vulnerabilities%20such%20as%20false%20claim%20attacks%20or%20susceptible%20to%20weight%20manipulations.%20To%20overcome%20these%20limitations%2C%20we%20propose%20SELF%2C%20a%20novel%20intrinsic%20weight-based%20fingerprinting%20scheme%20that%20eliminates%20dependency%20on%20input%20and%20inherently%20resists%20false%20claims.%20SELF%20achieves%20robust%20IP%20protection%20through%20two%20key%20innovations%3A%201%29%20unique%2C%20scalable%20and%20transformation-invariant%20fingerprint%20extraction%20via%20singular%20value%20and%20eigenvalue%20decomposition%20of%20LLM%20attention%20weights%2C%20and%202%29%20effective%20neural%20network-based%20fingerprint%20similarity%20comparison%20based%20on%20few-shot%20learning%20and%20data%20augmentation.%20Experimental%20results%20demonstrate%20SELF%20maintains%20high%20IP%20infringement%20detection%20accuracy%20while%20showing%20strong%20robustness%20against%20various%20downstream%20modifications%2C%20including%20quantization%2C%20pruning%2C%20and%20fine-tuning%20attacks.%20Our%20code%20is%20available%20at%20https%3A//github.com/HanxiuZhang/SELF_v2.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSELF%253A%2520A%2520Robust%2520Singular%2520Value%2520and%2520Eigenvalue%2520Approach%2520for%2520LLM%2520Fingerprinting%26entry.906535625%3DHanxiu%2520Zhang%2520and%2520Yue%2520Zheng%26entry.1292438233%3DThe%2520protection%2520of%2520Intellectual%2520Property%2520%2528IP%2529%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520represents%2520a%2520critical%2520challenge%2520in%2520contemporary%2520AI%2520research.%2520While%2520fingerprinting%2520techniques%2520have%2520emerged%2520as%2520a%2520fundamental%2520mechanism%2520for%2520detecting%2520unauthorized%2520model%2520usage%252C%2520existing%2520methods%2520--%2520whether%2520behavior-based%2520or%2520structural%2520--%2520suffer%2520from%2520vulnerabilities%2520such%2520as%2520false%2520claim%2520attacks%2520or%2520susceptible%2520to%2520weight%2520manipulations.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520SELF%252C%2520a%2520novel%2520intrinsic%2520weight-based%2520fingerprinting%2520scheme%2520that%2520eliminates%2520dependency%2520on%2520input%2520and%2520inherently%2520resists%2520false%2520claims.%2520SELF%2520achieves%2520robust%2520IP%2520protection%2520through%2520two%2520key%2520innovations%253A%25201%2529%2520unique%252C%2520scalable%2520and%2520transformation-invariant%2520fingerprint%2520extraction%2520via%2520singular%2520value%2520and%2520eigenvalue%2520decomposition%2520of%2520LLM%2520attention%2520weights%252C%2520and%25202%2529%2520effective%2520neural%2520network-based%2520fingerprint%2520similarity%2520comparison%2520based%2520on%2520few-shot%2520learning%2520and%2520data%2520augmentation.%2520Experimental%2520results%2520demonstrate%2520SELF%2520maintains%2520high%2520IP%2520infringement%2520detection%2520accuracy%2520while%2520showing%2520strong%2520robustness%2520against%2520various%2520downstream%2520modifications%252C%2520including%2520quantization%252C%2520pruning%252C%2520and%2520fine-tuning%2520attacks.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/HanxiuZhang/SELF_v2.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SELF%3A%20A%20Robust%20Singular%20Value%20and%20Eigenvalue%20Approach%20for%20LLM%20Fingerprinting&entry.906535625=Hanxiu%20Zhang%20and%20Yue%20Zheng&entry.1292438233=The%20protection%20of%20Intellectual%20Property%20%28IP%29%20in%20Large%20Language%20Models%20%28LLMs%29%20represents%20a%20critical%20challenge%20in%20contemporary%20AI%20research.%20While%20fingerprinting%20techniques%20have%20emerged%20as%20a%20fundamental%20mechanism%20for%20detecting%20unauthorized%20model%20usage%2C%20existing%20methods%20--%20whether%20behavior-based%20or%20structural%20--%20suffer%20from%20vulnerabilities%20such%20as%20false%20claim%20attacks%20or%20susceptible%20to%20weight%20manipulations.%20To%20overcome%20these%20limitations%2C%20we%20propose%20SELF%2C%20a%20novel%20intrinsic%20weight-based%20fingerprinting%20scheme%20that%20eliminates%20dependency%20on%20input%20and%20inherently%20resists%20false%20claims.%20SELF%20achieves%20robust%20IP%20protection%20through%20two%20key%20innovations%3A%201%29%20unique%2C%20scalable%20and%20transformation-invariant%20fingerprint%20extraction%20via%20singular%20value%20and%20eigenvalue%20decomposition%20of%20LLM%20attention%20weights%2C%20and%202%29%20effective%20neural%20network-based%20fingerprint%20similarity%20comparison%20based%20on%20few-shot%20learning%20and%20data%20augmentation.%20Experimental%20results%20demonstrate%20SELF%20maintains%20high%20IP%20infringement%20detection%20accuracy%20while%20showing%20strong%20robustness%20against%20various%20downstream%20modifications%2C%20including%20quantization%2C%20pruning%2C%20and%20fine-tuning%20attacks.%20Our%20code%20is%20available%20at%20https%3A//github.com/HanxiuZhang/SELF_v2.&entry.1838667208=http%3A//arxiv.org/abs/2512.03620v1&entry.124074799=Read"},
{"title": "AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning", "author": "Kohei Yamamoto and Kosuke Okusa", "abstract": "Transformer-based audio SSL (self-supervised learning) models often treat spectrograms as images, applying convolutional patchification with heavy temporal downsampling. This lowers the effective Nyquist frequency and introduces aliasing, while na\u00efve low-pass filtering removes task-relevant high-frequency cues. In this study, we present Aliasing-aware Patch Embedding (AaPE), a drop-in patch stem that mitigates aliasing while preserving high-frequency information. AaPE augments standard patch tokens with features produced by a band-limited complex sinusoidal kernel using a two-sided exponential window that dynamically targets alias-prone bands. Frequency and decay parameters of the kernel are estimated from the input, enabling parallel, adaptive subband analysis whose outputs are fused with the standard patch tokens. AaPE integrates seamlessly into the masked teacher-student self-supervised learning. In addition, we combine a multi-mask strategy with a contrastive objective to enforce consistency across diverse mask patterns, stabilizing training. Pre-training on AudioSet followed by fine-tuning evaluation across diverse downstream benchmarks, which spanned categories, such as environmental sounds and other common audio domains. This approach yields state-of-the-art performance on a subset of tasks and competitive results across the remainder. Complementary linear probing evaluation mirrors this pattern, yielding clear gains on several benchmarks and strong performance elsewhere. The collective analysis of these results indicates that AaPE serves to mitigate the effects of aliasing without discarding of informative high-frequency content.", "link": "http://arxiv.org/abs/2512.03637v1", "date": "2025-12-03", "relevancy": 2.4902, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.527}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4854}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AaPE%3A%20Aliasing-aware%20Patch%20Embedding%20for%20Self-Supervised%20Audio%20Representation%20Learning&body=Title%3A%20AaPE%3A%20Aliasing-aware%20Patch%20Embedding%20for%20Self-Supervised%20Audio%20Representation%20Learning%0AAuthor%3A%20Kohei%20Yamamoto%20and%20Kosuke%20Okusa%0AAbstract%3A%20Transformer-based%20audio%20SSL%20%28self-supervised%20learning%29%20models%20often%20treat%20spectrograms%20as%20images%2C%20applying%20convolutional%20patchification%20with%20heavy%20temporal%20downsampling.%20This%20lowers%20the%20effective%20Nyquist%20frequency%20and%20introduces%20aliasing%2C%20while%20na%C3%AFve%20low-pass%20filtering%20removes%20task-relevant%20high-frequency%20cues.%20In%20this%20study%2C%20we%20present%20Aliasing-aware%20Patch%20Embedding%20%28AaPE%29%2C%20a%20drop-in%20patch%20stem%20that%20mitigates%20aliasing%20while%20preserving%20high-frequency%20information.%20AaPE%20augments%20standard%20patch%20tokens%20with%20features%20produced%20by%20a%20band-limited%20complex%20sinusoidal%20kernel%20using%20a%20two-sided%20exponential%20window%20that%20dynamically%20targets%20alias-prone%20bands.%20Frequency%20and%20decay%20parameters%20of%20the%20kernel%20are%20estimated%20from%20the%20input%2C%20enabling%20parallel%2C%20adaptive%20subband%20analysis%20whose%20outputs%20are%20fused%20with%20the%20standard%20patch%20tokens.%20AaPE%20integrates%20seamlessly%20into%20the%20masked%20teacher-student%20self-supervised%20learning.%20In%20addition%2C%20we%20combine%20a%20multi-mask%20strategy%20with%20a%20contrastive%20objective%20to%20enforce%20consistency%20across%20diverse%20mask%20patterns%2C%20stabilizing%20training.%20Pre-training%20on%20AudioSet%20followed%20by%20fine-tuning%20evaluation%20across%20diverse%20downstream%20benchmarks%2C%20which%20spanned%20categories%2C%20such%20as%20environmental%20sounds%20and%20other%20common%20audio%20domains.%20This%20approach%20yields%20state-of-the-art%20performance%20on%20a%20subset%20of%20tasks%20and%20competitive%20results%20across%20the%20remainder.%20Complementary%20linear%20probing%20evaluation%20mirrors%20this%20pattern%2C%20yielding%20clear%20gains%20on%20several%20benchmarks%20and%20strong%20performance%20elsewhere.%20The%20collective%20analysis%20of%20these%20results%20indicates%20that%20AaPE%20serves%20to%20mitigate%20the%20effects%20of%20aliasing%20without%20discarding%20of%20informative%20high-frequency%20content.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAaPE%253A%2520Aliasing-aware%2520Patch%2520Embedding%2520for%2520Self-Supervised%2520Audio%2520Representation%2520Learning%26entry.906535625%3DKohei%2520Yamamoto%2520and%2520Kosuke%2520Okusa%26entry.1292438233%3DTransformer-based%2520audio%2520SSL%2520%2528self-supervised%2520learning%2529%2520models%2520often%2520treat%2520spectrograms%2520as%2520images%252C%2520applying%2520convolutional%2520patchification%2520with%2520heavy%2520temporal%2520downsampling.%2520This%2520lowers%2520the%2520effective%2520Nyquist%2520frequency%2520and%2520introduces%2520aliasing%252C%2520while%2520na%25C3%25AFve%2520low-pass%2520filtering%2520removes%2520task-relevant%2520high-frequency%2520cues.%2520In%2520this%2520study%252C%2520we%2520present%2520Aliasing-aware%2520Patch%2520Embedding%2520%2528AaPE%2529%252C%2520a%2520drop-in%2520patch%2520stem%2520that%2520mitigates%2520aliasing%2520while%2520preserving%2520high-frequency%2520information.%2520AaPE%2520augments%2520standard%2520patch%2520tokens%2520with%2520features%2520produced%2520by%2520a%2520band-limited%2520complex%2520sinusoidal%2520kernel%2520using%2520a%2520two-sided%2520exponential%2520window%2520that%2520dynamically%2520targets%2520alias-prone%2520bands.%2520Frequency%2520and%2520decay%2520parameters%2520of%2520the%2520kernel%2520are%2520estimated%2520from%2520the%2520input%252C%2520enabling%2520parallel%252C%2520adaptive%2520subband%2520analysis%2520whose%2520outputs%2520are%2520fused%2520with%2520the%2520standard%2520patch%2520tokens.%2520AaPE%2520integrates%2520seamlessly%2520into%2520the%2520masked%2520teacher-student%2520self-supervised%2520learning.%2520In%2520addition%252C%2520we%2520combine%2520a%2520multi-mask%2520strategy%2520with%2520a%2520contrastive%2520objective%2520to%2520enforce%2520consistency%2520across%2520diverse%2520mask%2520patterns%252C%2520stabilizing%2520training.%2520Pre-training%2520on%2520AudioSet%2520followed%2520by%2520fine-tuning%2520evaluation%2520across%2520diverse%2520downstream%2520benchmarks%252C%2520which%2520spanned%2520categories%252C%2520such%2520as%2520environmental%2520sounds%2520and%2520other%2520common%2520audio%2520domains.%2520This%2520approach%2520yields%2520state-of-the-art%2520performance%2520on%2520a%2520subset%2520of%2520tasks%2520and%2520competitive%2520results%2520across%2520the%2520remainder.%2520Complementary%2520linear%2520probing%2520evaluation%2520mirrors%2520this%2520pattern%252C%2520yielding%2520clear%2520gains%2520on%2520several%2520benchmarks%2520and%2520strong%2520performance%2520elsewhere.%2520The%2520collective%2520analysis%2520of%2520these%2520results%2520indicates%2520that%2520AaPE%2520serves%2520to%2520mitigate%2520the%2520effects%2520of%2520aliasing%2520without%2520discarding%2520of%2520informative%2520high-frequency%2520content.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AaPE%3A%20Aliasing-aware%20Patch%20Embedding%20for%20Self-Supervised%20Audio%20Representation%20Learning&entry.906535625=Kohei%20Yamamoto%20and%20Kosuke%20Okusa&entry.1292438233=Transformer-based%20audio%20SSL%20%28self-supervised%20learning%29%20models%20often%20treat%20spectrograms%20as%20images%2C%20applying%20convolutional%20patchification%20with%20heavy%20temporal%20downsampling.%20This%20lowers%20the%20effective%20Nyquist%20frequency%20and%20introduces%20aliasing%2C%20while%20na%C3%AFve%20low-pass%20filtering%20removes%20task-relevant%20high-frequency%20cues.%20In%20this%20study%2C%20we%20present%20Aliasing-aware%20Patch%20Embedding%20%28AaPE%29%2C%20a%20drop-in%20patch%20stem%20that%20mitigates%20aliasing%20while%20preserving%20high-frequency%20information.%20AaPE%20augments%20standard%20patch%20tokens%20with%20features%20produced%20by%20a%20band-limited%20complex%20sinusoidal%20kernel%20using%20a%20two-sided%20exponential%20window%20that%20dynamically%20targets%20alias-prone%20bands.%20Frequency%20and%20decay%20parameters%20of%20the%20kernel%20are%20estimated%20from%20the%20input%2C%20enabling%20parallel%2C%20adaptive%20subband%20analysis%20whose%20outputs%20are%20fused%20with%20the%20standard%20patch%20tokens.%20AaPE%20integrates%20seamlessly%20into%20the%20masked%20teacher-student%20self-supervised%20learning.%20In%20addition%2C%20we%20combine%20a%20multi-mask%20strategy%20with%20a%20contrastive%20objective%20to%20enforce%20consistency%20across%20diverse%20mask%20patterns%2C%20stabilizing%20training.%20Pre-training%20on%20AudioSet%20followed%20by%20fine-tuning%20evaluation%20across%20diverse%20downstream%20benchmarks%2C%20which%20spanned%20categories%2C%20such%20as%20environmental%20sounds%20and%20other%20common%20audio%20domains.%20This%20approach%20yields%20state-of-the-art%20performance%20on%20a%20subset%20of%20tasks%20and%20competitive%20results%20across%20the%20remainder.%20Complementary%20linear%20probing%20evaluation%20mirrors%20this%20pattern%2C%20yielding%20clear%20gains%20on%20several%20benchmarks%20and%20strong%20performance%20elsewhere.%20The%20collective%20analysis%20of%20these%20results%20indicates%20that%20AaPE%20serves%20to%20mitigate%20the%20effects%20of%20aliasing%20without%20discarding%20of%20informative%20high-frequency%20content.&entry.1838667208=http%3A//arxiv.org/abs/2512.03637v1&entry.124074799=Read"},
{"title": "Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking", "author": "Haonan Zhang and Xinyao Wang and Boxi Wu and Tu Zheng and Wang Yunhua and Zheng Yang", "abstract": "3D multi-object tracking is a critical and challenging task in the field of autonomous driving. A common paradigm relies on modeling individual object motion, e.g., Kalman filters, to predict trajectories. While effective in simple scenarios, this approach often struggles in crowded environments or with inaccurate detections, as it overlooks the rich geometric relationships between objects. This highlights the need to leverage spatial cues. However, existing geometry-aware methods can be susceptible to interference from irrelevant objects, leading to ambiguous features and incorrect associations. To address this, we propose focusing on cue-consistency: identifying and matching stable spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency Tracker (DSC-Track) to implement this principle. Firstly, we design a unified spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative trajectory embeddings while suppressing interference. Secondly, our cue-consistency transformer module explicitly aligns consistent feature representations between historical tracks and current detections. Finally, a dynamic update mechanism preserves salient spatiotemporal information for stable online tracking. Extensive experiments on the nuScenes and Waymo Open Datasets validate the effectiveness and robustness of our approach. On the nuScenes benchmark, for instance, our method achieves state-of-the-art performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets, respectively.", "link": "http://arxiv.org/abs/2508.11323v2", "date": "2025-12-03", "relevancy": 2.4862, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6424}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Delving%20into%20Dynamic%20Scene%20Cue-Consistency%20for%20Robust%203D%20Multi-Object%20Tracking&body=Title%3A%20Delving%20into%20Dynamic%20Scene%20Cue-Consistency%20for%20Robust%203D%20Multi-Object%20Tracking%0AAuthor%3A%20Haonan%20Zhang%20and%20Xinyao%20Wang%20and%20Boxi%20Wu%20and%20Tu%20Zheng%20and%20Wang%20Yunhua%20and%20Zheng%20Yang%0AAbstract%3A%203D%20multi-object%20tracking%20is%20a%20critical%20and%20challenging%20task%20in%20the%20field%20of%20autonomous%20driving.%20A%20common%20paradigm%20relies%20on%20modeling%20individual%20object%20motion%2C%20e.g.%2C%20Kalman%20filters%2C%20to%20predict%20trajectories.%20While%20effective%20in%20simple%20scenarios%2C%20this%20approach%20often%20struggles%20in%20crowded%20environments%20or%20with%20inaccurate%20detections%2C%20as%20it%20overlooks%20the%20rich%20geometric%20relationships%20between%20objects.%20This%20highlights%20the%20need%20to%20leverage%20spatial%20cues.%20However%2C%20existing%20geometry-aware%20methods%20can%20be%20susceptible%20to%20interference%20from%20irrelevant%20objects%2C%20leading%20to%20ambiguous%20features%20and%20incorrect%20associations.%20To%20address%20this%2C%20we%20propose%20focusing%20on%20cue-consistency%3A%20identifying%20and%20matching%20stable%20spatial%20patterns%20over%20time.%20We%20introduce%20the%20Dynamic%20Scene%20Cue-Consistency%20Tracker%20%28DSC-Track%29%20to%20implement%20this%20principle.%20Firstly%2C%20we%20design%20a%20unified%20spatiotemporal%20encoder%20using%20Point%20Pair%20Features%20%28PPF%29%20to%20learn%20discriminative%20trajectory%20embeddings%20while%20suppressing%20interference.%20Secondly%2C%20our%20cue-consistency%20transformer%20module%20explicitly%20aligns%20consistent%20feature%20representations%20between%20historical%20tracks%20and%20current%20detections.%20Finally%2C%20a%20dynamic%20update%20mechanism%20preserves%20salient%20spatiotemporal%20information%20for%20stable%20online%20tracking.%20Extensive%20experiments%20on%20the%20nuScenes%20and%20Waymo%20Open%20Datasets%20validate%20the%20effectiveness%20and%20robustness%20of%20our%20approach.%20On%20the%20nuScenes%20benchmark%2C%20for%20instance%2C%20our%20method%20achieves%20state-of-the-art%20performance%2C%20reaching%2073.2%25%20and%2070.3%25%20AMOTA%20on%20the%20validation%20and%20test%20sets%2C%20respectively.%0ALink%3A%20http%3A//arxiv.org/abs/2508.11323v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDelving%2520into%2520Dynamic%2520Scene%2520Cue-Consistency%2520for%2520Robust%25203D%2520Multi-Object%2520Tracking%26entry.906535625%3DHaonan%2520Zhang%2520and%2520Xinyao%2520Wang%2520and%2520Boxi%2520Wu%2520and%2520Tu%2520Zheng%2520and%2520Wang%2520Yunhua%2520and%2520Zheng%2520Yang%26entry.1292438233%3D3D%2520multi-object%2520tracking%2520is%2520a%2520critical%2520and%2520challenging%2520task%2520in%2520the%2520field%2520of%2520autonomous%2520driving.%2520A%2520common%2520paradigm%2520relies%2520on%2520modeling%2520individual%2520object%2520motion%252C%2520e.g.%252C%2520Kalman%2520filters%252C%2520to%2520predict%2520trajectories.%2520While%2520effective%2520in%2520simple%2520scenarios%252C%2520this%2520approach%2520often%2520struggles%2520in%2520crowded%2520environments%2520or%2520with%2520inaccurate%2520detections%252C%2520as%2520it%2520overlooks%2520the%2520rich%2520geometric%2520relationships%2520between%2520objects.%2520This%2520highlights%2520the%2520need%2520to%2520leverage%2520spatial%2520cues.%2520However%252C%2520existing%2520geometry-aware%2520methods%2520can%2520be%2520susceptible%2520to%2520interference%2520from%2520irrelevant%2520objects%252C%2520leading%2520to%2520ambiguous%2520features%2520and%2520incorrect%2520associations.%2520To%2520address%2520this%252C%2520we%2520propose%2520focusing%2520on%2520cue-consistency%253A%2520identifying%2520and%2520matching%2520stable%2520spatial%2520patterns%2520over%2520time.%2520We%2520introduce%2520the%2520Dynamic%2520Scene%2520Cue-Consistency%2520Tracker%2520%2528DSC-Track%2529%2520to%2520implement%2520this%2520principle.%2520Firstly%252C%2520we%2520design%2520a%2520unified%2520spatiotemporal%2520encoder%2520using%2520Point%2520Pair%2520Features%2520%2528PPF%2529%2520to%2520learn%2520discriminative%2520trajectory%2520embeddings%2520while%2520suppressing%2520interference.%2520Secondly%252C%2520our%2520cue-consistency%2520transformer%2520module%2520explicitly%2520aligns%2520consistent%2520feature%2520representations%2520between%2520historical%2520tracks%2520and%2520current%2520detections.%2520Finally%252C%2520a%2520dynamic%2520update%2520mechanism%2520preserves%2520salient%2520spatiotemporal%2520information%2520for%2520stable%2520online%2520tracking.%2520Extensive%2520experiments%2520on%2520the%2520nuScenes%2520and%2520Waymo%2520Open%2520Datasets%2520validate%2520the%2520effectiveness%2520and%2520robustness%2520of%2520our%2520approach.%2520On%2520the%2520nuScenes%2520benchmark%252C%2520for%2520instance%252C%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%252C%2520reaching%252073.2%2525%2520and%252070.3%2525%2520AMOTA%2520on%2520the%2520validation%2520and%2520test%2520sets%252C%2520respectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11323v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Delving%20into%20Dynamic%20Scene%20Cue-Consistency%20for%20Robust%203D%20Multi-Object%20Tracking&entry.906535625=Haonan%20Zhang%20and%20Xinyao%20Wang%20and%20Boxi%20Wu%20and%20Tu%20Zheng%20and%20Wang%20Yunhua%20and%20Zheng%20Yang&entry.1292438233=3D%20multi-object%20tracking%20is%20a%20critical%20and%20challenging%20task%20in%20the%20field%20of%20autonomous%20driving.%20A%20common%20paradigm%20relies%20on%20modeling%20individual%20object%20motion%2C%20e.g.%2C%20Kalman%20filters%2C%20to%20predict%20trajectories.%20While%20effective%20in%20simple%20scenarios%2C%20this%20approach%20often%20struggles%20in%20crowded%20environments%20or%20with%20inaccurate%20detections%2C%20as%20it%20overlooks%20the%20rich%20geometric%20relationships%20between%20objects.%20This%20highlights%20the%20need%20to%20leverage%20spatial%20cues.%20However%2C%20existing%20geometry-aware%20methods%20can%20be%20susceptible%20to%20interference%20from%20irrelevant%20objects%2C%20leading%20to%20ambiguous%20features%20and%20incorrect%20associations.%20To%20address%20this%2C%20we%20propose%20focusing%20on%20cue-consistency%3A%20identifying%20and%20matching%20stable%20spatial%20patterns%20over%20time.%20We%20introduce%20the%20Dynamic%20Scene%20Cue-Consistency%20Tracker%20%28DSC-Track%29%20to%20implement%20this%20principle.%20Firstly%2C%20we%20design%20a%20unified%20spatiotemporal%20encoder%20using%20Point%20Pair%20Features%20%28PPF%29%20to%20learn%20discriminative%20trajectory%20embeddings%20while%20suppressing%20interference.%20Secondly%2C%20our%20cue-consistency%20transformer%20module%20explicitly%20aligns%20consistent%20feature%20representations%20between%20historical%20tracks%20and%20current%20detections.%20Finally%2C%20a%20dynamic%20update%20mechanism%20preserves%20salient%20spatiotemporal%20information%20for%20stable%20online%20tracking.%20Extensive%20experiments%20on%20the%20nuScenes%20and%20Waymo%20Open%20Datasets%20validate%20the%20effectiveness%20and%20robustness%20of%20our%20approach.%20On%20the%20nuScenes%20benchmark%2C%20for%20instance%2C%20our%20method%20achieves%20state-of-the-art%20performance%2C%20reaching%2073.2%25%20and%2070.3%25%20AMOTA%20on%20the%20validation%20and%20test%20sets%2C%20respectively.&entry.1838667208=http%3A//arxiv.org/abs/2508.11323v2&entry.124074799=Read"},
{"title": "UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework", "author": "Youxin Pang and Yong Zhang and Ruizhi Shao and Xiang Deng and Feng Gao and Xu Xiaoming and Xiaoming Wei and Yebin Liu", "abstract": "We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.", "link": "http://arxiv.org/abs/2512.03918v1", "date": "2025-12-03", "relevancy": 2.4793, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6478}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6033}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniMo%3A%20Unifying%202D%20Video%20and%203D%20Human%20Motion%20with%20an%20Autoregressive%20Framework&body=Title%3A%20UniMo%3A%20Unifying%202D%20Video%20and%203D%20Human%20Motion%20with%20an%20Autoregressive%20Framework%0AAuthor%3A%20Youxin%20Pang%20and%20Yong%20Zhang%20and%20Ruizhi%20Shao%20and%20Xiang%20Deng%20and%20Feng%20Gao%20and%20Xu%20Xiaoming%20and%20Xiaoming%20Wei%20and%20Yebin%20Liu%0AAbstract%3A%20We%20propose%20UniMo%2C%20an%20innovative%20autoregressive%20model%20for%20joint%20modeling%20of%202D%20human%20videos%20and%203D%20human%20motions%20within%20a%20unified%20framework%2C%20enabling%20simultaneous%20generation%20and%20understanding%20of%20these%20two%20modalities%20for%20the%20first%20time.%20Current%20methods%20predominantly%20focus%20on%20generating%20one%20modality%20given%20another%20as%20the%20condition%20or%20integrating%20either%20of%20them%20with%20other%20modalities%20such%20as%20text%20and%20audio.%20Unifying%202D%20videos%20and%203D%20motions%20for%20simultaneous%20optimization%20and%20generation%20remains%20largely%20unexplored%2C%20presenting%20significant%20challenges%20due%20to%20their%20substantial%20structural%20and%20distributional%20differences.%20Inspired%20by%20the%20LLM%27s%20ability%20to%20unify%20different%20modalities%2C%20our%20method%20models%20videos%20and%203D%20motions%20as%20a%20unified%20tokens%20sequence%2C%20utilizing%20separate%20embedding%20layers%20to%20mitigate%20distribution%20gaps.%20Additionally%2C%20we%20devise%20a%20sequence%20modeling%20strategy%20that%20integrates%20two%20distinct%20tasks%20within%20a%20single%20framework%2C%20proving%20the%20effectiveness%20of%20unified%20modeling.%20Moreover%2C%20to%20efficiently%20align%20with%20visual%20tokens%20and%20preserve%203D%20spatial%20information%2C%20we%20design%20a%20novel%203D%20motion%20tokenizer%20with%20a%20temporal%20expansion%20strategy%2C%20using%20a%20single%20VQ-VAE%20to%20produce%20quantized%20motion%20tokens.%20It%20features%20multiple%20expert%20decoders%20that%20handle%20body%20shapes%2C%20translation%2C%20global%20orientation%2C%20and%20body%20poses%20for%20reliable%203D%20motion%20reconstruction.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20simultaneously%20generates%20corresponding%20videos%20and%20motions%20while%20performing%20accurate%20motion%20capture.%20This%20work%20taps%20into%20the%20capacity%20of%20LLMs%20to%20fuse%20diverse%20data%20types%2C%20paving%20the%20way%20for%20integrating%20human-centric%20information%20into%20existing%20models%20and%20potentially%20enabling%20multimodal%2C%20controllable%20joint%20modeling%20of%20humans%2C%20objects%2C%20and%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniMo%253A%2520Unifying%25202D%2520Video%2520and%25203D%2520Human%2520Motion%2520with%2520an%2520Autoregressive%2520Framework%26entry.906535625%3DYouxin%2520Pang%2520and%2520Yong%2520Zhang%2520and%2520Ruizhi%2520Shao%2520and%2520Xiang%2520Deng%2520and%2520Feng%2520Gao%2520and%2520Xu%2520Xiaoming%2520and%2520Xiaoming%2520Wei%2520and%2520Yebin%2520Liu%26entry.1292438233%3DWe%2520propose%2520UniMo%252C%2520an%2520innovative%2520autoregressive%2520model%2520for%2520joint%2520modeling%2520of%25202D%2520human%2520videos%2520and%25203D%2520human%2520motions%2520within%2520a%2520unified%2520framework%252C%2520enabling%2520simultaneous%2520generation%2520and%2520understanding%2520of%2520these%2520two%2520modalities%2520for%2520the%2520first%2520time.%2520Current%2520methods%2520predominantly%2520focus%2520on%2520generating%2520one%2520modality%2520given%2520another%2520as%2520the%2520condition%2520or%2520integrating%2520either%2520of%2520them%2520with%2520other%2520modalities%2520such%2520as%2520text%2520and%2520audio.%2520Unifying%25202D%2520videos%2520and%25203D%2520motions%2520for%2520simultaneous%2520optimization%2520and%2520generation%2520remains%2520largely%2520unexplored%252C%2520presenting%2520significant%2520challenges%2520due%2520to%2520their%2520substantial%2520structural%2520and%2520distributional%2520differences.%2520Inspired%2520by%2520the%2520LLM%2527s%2520ability%2520to%2520unify%2520different%2520modalities%252C%2520our%2520method%2520models%2520videos%2520and%25203D%2520motions%2520as%2520a%2520unified%2520tokens%2520sequence%252C%2520utilizing%2520separate%2520embedding%2520layers%2520to%2520mitigate%2520distribution%2520gaps.%2520Additionally%252C%2520we%2520devise%2520a%2520sequence%2520modeling%2520strategy%2520that%2520integrates%2520two%2520distinct%2520tasks%2520within%2520a%2520single%2520framework%252C%2520proving%2520the%2520effectiveness%2520of%2520unified%2520modeling.%2520Moreover%252C%2520to%2520efficiently%2520align%2520with%2520visual%2520tokens%2520and%2520preserve%25203D%2520spatial%2520information%252C%2520we%2520design%2520a%2520novel%25203D%2520motion%2520tokenizer%2520with%2520a%2520temporal%2520expansion%2520strategy%252C%2520using%2520a%2520single%2520VQ-VAE%2520to%2520produce%2520quantized%2520motion%2520tokens.%2520It%2520features%2520multiple%2520expert%2520decoders%2520that%2520handle%2520body%2520shapes%252C%2520translation%252C%2520global%2520orientation%252C%2520and%2520body%2520poses%2520for%2520reliable%25203D%2520motion%2520reconstruction.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520simultaneously%2520generates%2520corresponding%2520videos%2520and%2520motions%2520while%2520performing%2520accurate%2520motion%2520capture.%2520This%2520work%2520taps%2520into%2520the%2520capacity%2520of%2520LLMs%2520to%2520fuse%2520diverse%2520data%2520types%252C%2520paving%2520the%2520way%2520for%2520integrating%2520human-centric%2520information%2520into%2520existing%2520models%2520and%2520potentially%2520enabling%2520multimodal%252C%2520controllable%2520joint%2520modeling%2520of%2520humans%252C%2520objects%252C%2520and%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMo%3A%20Unifying%202D%20Video%20and%203D%20Human%20Motion%20with%20an%20Autoregressive%20Framework&entry.906535625=Youxin%20Pang%20and%20Yong%20Zhang%20and%20Ruizhi%20Shao%20and%20Xiang%20Deng%20and%20Feng%20Gao%20and%20Xu%20Xiaoming%20and%20Xiaoming%20Wei%20and%20Yebin%20Liu&entry.1292438233=We%20propose%20UniMo%2C%20an%20innovative%20autoregressive%20model%20for%20joint%20modeling%20of%202D%20human%20videos%20and%203D%20human%20motions%20within%20a%20unified%20framework%2C%20enabling%20simultaneous%20generation%20and%20understanding%20of%20these%20two%20modalities%20for%20the%20first%20time.%20Current%20methods%20predominantly%20focus%20on%20generating%20one%20modality%20given%20another%20as%20the%20condition%20or%20integrating%20either%20of%20them%20with%20other%20modalities%20such%20as%20text%20and%20audio.%20Unifying%202D%20videos%20and%203D%20motions%20for%20simultaneous%20optimization%20and%20generation%20remains%20largely%20unexplored%2C%20presenting%20significant%20challenges%20due%20to%20their%20substantial%20structural%20and%20distributional%20differences.%20Inspired%20by%20the%20LLM%27s%20ability%20to%20unify%20different%20modalities%2C%20our%20method%20models%20videos%20and%203D%20motions%20as%20a%20unified%20tokens%20sequence%2C%20utilizing%20separate%20embedding%20layers%20to%20mitigate%20distribution%20gaps.%20Additionally%2C%20we%20devise%20a%20sequence%20modeling%20strategy%20that%20integrates%20two%20distinct%20tasks%20within%20a%20single%20framework%2C%20proving%20the%20effectiveness%20of%20unified%20modeling.%20Moreover%2C%20to%20efficiently%20align%20with%20visual%20tokens%20and%20preserve%203D%20spatial%20information%2C%20we%20design%20a%20novel%203D%20motion%20tokenizer%20with%20a%20temporal%20expansion%20strategy%2C%20using%20a%20single%20VQ-VAE%20to%20produce%20quantized%20motion%20tokens.%20It%20features%20multiple%20expert%20decoders%20that%20handle%20body%20shapes%2C%20translation%2C%20global%20orientation%2C%20and%20body%20poses%20for%20reliable%203D%20motion%20reconstruction.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20simultaneously%20generates%20corresponding%20videos%20and%20motions%20while%20performing%20accurate%20motion%20capture.%20This%20work%20taps%20into%20the%20capacity%20of%20LLMs%20to%20fuse%20diverse%20data%20types%2C%20paving%20the%20way%20for%20integrating%20human-centric%20information%20into%20existing%20models%20and%20potentially%20enabling%20multimodal%2C%20controllable%20joint%20modeling%20of%20humans%2C%20objects%2C%20and%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2512.03918v1&entry.124074799=Read"},
{"title": "DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes", "author": "Hengwei Bian and Lingdong Kong and Haozhe Xie and Liang Pan and Yu Qiao and Ziwei Liu", "abstract": "Urban scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D occupancy generation framework capable of generating large-scale, high-quality dynamic 4D scenes with semantics. DynamicCity mainly consists of two key models. 1) A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel Projection Module to effectively compress 4D features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a Padded Rollout Operation is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting versatile 4D generation applications, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D occupancy generation methods across multiple metrics. The code and models have been released to facilitate future research.", "link": "http://arxiv.org/abs/2410.18084v3", "date": "2025-12-03", "relevancy": 2.4671, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6178}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6178}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamicCity%3A%20Large-Scale%204D%20Occupancy%20Generation%20from%20Dynamic%20Scenes&body=Title%3A%20DynamicCity%3A%20Large-Scale%204D%20Occupancy%20Generation%20from%20Dynamic%20Scenes%0AAuthor%3A%20Hengwei%20Bian%20and%20Lingdong%20Kong%20and%20Haozhe%20Xie%20and%20Liang%20Pan%20and%20Yu%20Qiao%20and%20Ziwei%20Liu%0AAbstract%3A%20Urban%20scene%20generation%20has%20been%20developing%20rapidly%20recently.%20However%2C%20existing%20methods%20primarily%20focus%20on%20generating%20static%20and%20single-frame%20scenes%2C%20overlooking%20the%20inherently%20dynamic%20nature%20of%20real-world%20driving%20environments.%20In%20this%20work%2C%20we%20introduce%20DynamicCity%2C%20a%20novel%204D%20occupancy%20generation%20framework%20capable%20of%20generating%20large-scale%2C%20high-quality%20dynamic%204D%20scenes%20with%20semantics.%20DynamicCity%20mainly%20consists%20of%20two%20key%20models.%201%29%20A%20VAE%20model%20for%20learning%20HexPlane%20as%20the%20compact%204D%20representation.%20Instead%20of%20using%20naive%20averaging%20operations%2C%20DynamicCity%20employs%20a%20novel%20Projection%20Module%20to%20effectively%20compress%204D%20features%20into%20six%202D%20feature%20maps%20for%20HexPlane%20construction%2C%20which%20significantly%20enhances%20HexPlane%20fitting%20quality%20%28up%20to%2012.56%20mIoU%20gain%29.%20Furthermore%2C%20we%20utilize%20an%20Expansion%20%26%20Squeeze%20Strategy%20to%20reconstruct%203D%20feature%20volumes%20in%20parallel%2C%20which%20improves%20both%20network%20training%20efficiency%20and%20reconstruction%20accuracy%20than%20naively%20querying%20each%203D%20point%20%28up%20to%207.05%20mIoU%20gain%2C%202.06x%20training%20speedup%2C%20and%2070.84%25%20memory%20reduction%29.%202%29%20A%20DiT-based%20diffusion%20model%20for%20HexPlane%20generation.%20To%20make%20HexPlane%20feasible%20for%20DiT%20generation%2C%20a%20Padded%20Rollout%20Operation%20is%20proposed%20to%20reorganize%20all%20six%20feature%20planes%20of%20the%20HexPlane%20as%20a%20squared%202D%20feature%20map.%20In%20particular%2C%20various%20conditions%20could%20be%20introduced%20in%20the%20diffusion%20or%20sampling%20process%2C%20supporting%20versatile%204D%20generation%20applications%2C%20such%20as%20trajectory-%20and%20command-driven%20generation%2C%20inpainting%2C%20and%20layout-conditioned%20generation.%20Extensive%20experiments%20on%20the%20CarlaSC%20and%20Waymo%20datasets%20demonstrate%20that%20DynamicCity%20significantly%20outperforms%20existing%20state-of-the-art%204D%20occupancy%20generation%20methods%20across%20multiple%20metrics.%20The%20code%20and%20models%20have%20been%20released%20to%20facilitate%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2410.18084v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamicCity%253A%2520Large-Scale%25204D%2520Occupancy%2520Generation%2520from%2520Dynamic%2520Scenes%26entry.906535625%3DHengwei%2520Bian%2520and%2520Lingdong%2520Kong%2520and%2520Haozhe%2520Xie%2520and%2520Liang%2520Pan%2520and%2520Yu%2520Qiao%2520and%2520Ziwei%2520Liu%26entry.1292438233%3DUrban%2520scene%2520generation%2520has%2520been%2520developing%2520rapidly%2520recently.%2520However%252C%2520existing%2520methods%2520primarily%2520focus%2520on%2520generating%2520static%2520and%2520single-frame%2520scenes%252C%2520overlooking%2520the%2520inherently%2520dynamic%2520nature%2520of%2520real-world%2520driving%2520environments.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DynamicCity%252C%2520a%2520novel%25204D%2520occupancy%2520generation%2520framework%2520capable%2520of%2520generating%2520large-scale%252C%2520high-quality%2520dynamic%25204D%2520scenes%2520with%2520semantics.%2520DynamicCity%2520mainly%2520consists%2520of%2520two%2520key%2520models.%25201%2529%2520A%2520VAE%2520model%2520for%2520learning%2520HexPlane%2520as%2520the%2520compact%25204D%2520representation.%2520Instead%2520of%2520using%2520naive%2520averaging%2520operations%252C%2520DynamicCity%2520employs%2520a%2520novel%2520Projection%2520Module%2520to%2520effectively%2520compress%25204D%2520features%2520into%2520six%25202D%2520feature%2520maps%2520for%2520HexPlane%2520construction%252C%2520which%2520significantly%2520enhances%2520HexPlane%2520fitting%2520quality%2520%2528up%2520to%252012.56%2520mIoU%2520gain%2529.%2520Furthermore%252C%2520we%2520utilize%2520an%2520Expansion%2520%2526%2520Squeeze%2520Strategy%2520to%2520reconstruct%25203D%2520feature%2520volumes%2520in%2520parallel%252C%2520which%2520improves%2520both%2520network%2520training%2520efficiency%2520and%2520reconstruction%2520accuracy%2520than%2520naively%2520querying%2520each%25203D%2520point%2520%2528up%2520to%25207.05%2520mIoU%2520gain%252C%25202.06x%2520training%2520speedup%252C%2520and%252070.84%2525%2520memory%2520reduction%2529.%25202%2529%2520A%2520DiT-based%2520diffusion%2520model%2520for%2520HexPlane%2520generation.%2520To%2520make%2520HexPlane%2520feasible%2520for%2520DiT%2520generation%252C%2520a%2520Padded%2520Rollout%2520Operation%2520is%2520proposed%2520to%2520reorganize%2520all%2520six%2520feature%2520planes%2520of%2520the%2520HexPlane%2520as%2520a%2520squared%25202D%2520feature%2520map.%2520In%2520particular%252C%2520various%2520conditions%2520could%2520be%2520introduced%2520in%2520the%2520diffusion%2520or%2520sampling%2520process%252C%2520supporting%2520versatile%25204D%2520generation%2520applications%252C%2520such%2520as%2520trajectory-%2520and%2520command-driven%2520generation%252C%2520inpainting%252C%2520and%2520layout-conditioned%2520generation.%2520Extensive%2520experiments%2520on%2520the%2520CarlaSC%2520and%2520Waymo%2520datasets%2520demonstrate%2520that%2520DynamicCity%2520significantly%2520outperforms%2520existing%2520state-of-the-art%25204D%2520occupancy%2520generation%2520methods%2520across%2520multiple%2520metrics.%2520The%2520code%2520and%2520models%2520have%2520been%2520released%2520to%2520facilitate%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18084v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamicCity%3A%20Large-Scale%204D%20Occupancy%20Generation%20from%20Dynamic%20Scenes&entry.906535625=Hengwei%20Bian%20and%20Lingdong%20Kong%20and%20Haozhe%20Xie%20and%20Liang%20Pan%20and%20Yu%20Qiao%20and%20Ziwei%20Liu&entry.1292438233=Urban%20scene%20generation%20has%20been%20developing%20rapidly%20recently.%20However%2C%20existing%20methods%20primarily%20focus%20on%20generating%20static%20and%20single-frame%20scenes%2C%20overlooking%20the%20inherently%20dynamic%20nature%20of%20real-world%20driving%20environments.%20In%20this%20work%2C%20we%20introduce%20DynamicCity%2C%20a%20novel%204D%20occupancy%20generation%20framework%20capable%20of%20generating%20large-scale%2C%20high-quality%20dynamic%204D%20scenes%20with%20semantics.%20DynamicCity%20mainly%20consists%20of%20two%20key%20models.%201%29%20A%20VAE%20model%20for%20learning%20HexPlane%20as%20the%20compact%204D%20representation.%20Instead%20of%20using%20naive%20averaging%20operations%2C%20DynamicCity%20employs%20a%20novel%20Projection%20Module%20to%20effectively%20compress%204D%20features%20into%20six%202D%20feature%20maps%20for%20HexPlane%20construction%2C%20which%20significantly%20enhances%20HexPlane%20fitting%20quality%20%28up%20to%2012.56%20mIoU%20gain%29.%20Furthermore%2C%20we%20utilize%20an%20Expansion%20%26%20Squeeze%20Strategy%20to%20reconstruct%203D%20feature%20volumes%20in%20parallel%2C%20which%20improves%20both%20network%20training%20efficiency%20and%20reconstruction%20accuracy%20than%20naively%20querying%20each%203D%20point%20%28up%20to%207.05%20mIoU%20gain%2C%202.06x%20training%20speedup%2C%20and%2070.84%25%20memory%20reduction%29.%202%29%20A%20DiT-based%20diffusion%20model%20for%20HexPlane%20generation.%20To%20make%20HexPlane%20feasible%20for%20DiT%20generation%2C%20a%20Padded%20Rollout%20Operation%20is%20proposed%20to%20reorganize%20all%20six%20feature%20planes%20of%20the%20HexPlane%20as%20a%20squared%202D%20feature%20map.%20In%20particular%2C%20various%20conditions%20could%20be%20introduced%20in%20the%20diffusion%20or%20sampling%20process%2C%20supporting%20versatile%204D%20generation%20applications%2C%20such%20as%20trajectory-%20and%20command-driven%20generation%2C%20inpainting%2C%20and%20layout-conditioned%20generation.%20Extensive%20experiments%20on%20the%20CarlaSC%20and%20Waymo%20datasets%20demonstrate%20that%20DynamicCity%20significantly%20outperforms%20existing%20state-of-the-art%204D%20occupancy%20generation%20methods%20across%20multiple%20metrics.%20The%20code%20and%20models%20have%20been%20released%20to%20facilitate%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2410.18084v3&entry.124074799=Read"},
{"title": "On the Temporality for Sketch Representation Learning", "author": "Marcelo Isaias de Moraes Junior and Moacir Antonelli Ponti", "abstract": "Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.", "link": "http://arxiv.org/abs/2512.04007v1", "date": "2025-12-03", "relevancy": 2.4519, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5177}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Temporality%20for%20Sketch%20Representation%20Learning&body=Title%3A%20On%20the%20Temporality%20for%20Sketch%20Representation%20Learning%0AAuthor%3A%20Marcelo%20Isaias%20de%20Moraes%20Junior%20and%20Moacir%20Antonelli%20Ponti%0AAbstract%3A%20Sketches%20are%20simple%20human%20hand-drawn%20abstractions%20of%20complex%20scenes%20and%20real-world%20objects.%20Although%20the%20field%20of%20sketch%20representation%20learning%20has%20advanced%20significantly%2C%20there%20is%20still%20a%20gap%20in%20understanding%20the%20true%20relevance%20of%20the%20temporal%20aspect%20to%20the%20quality%20of%20these%20representations.%20This%20work%20investigates%20whether%20it%20is%20indeed%20justifiable%20to%20treat%20sketches%20as%20sequences%2C%20as%20well%20as%20which%20internal%20orders%20play%20a%20more%20relevant%20role.%20The%20results%20indicate%20that%2C%20although%20the%20use%20of%20traditional%20positional%20encodings%20is%20valid%20for%20modeling%20sketches%20as%20sequences%2C%20absolute%20coordinates%20consistently%20outperform%20relative%20ones.%20Furthermore%2C%20non-autoregressive%20decoders%20outperform%20their%20autoregressive%20counterparts.%20Finally%2C%20the%20importance%20of%20temporality%20was%20shown%20to%20depend%20on%20both%20the%20order%20considered%20and%20the%20task%20evaluated.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Temporality%2520for%2520Sketch%2520Representation%2520Learning%26entry.906535625%3DMarcelo%2520Isaias%2520de%2520Moraes%2520Junior%2520and%2520Moacir%2520Antonelli%2520Ponti%26entry.1292438233%3DSketches%2520are%2520simple%2520human%2520hand-drawn%2520abstractions%2520of%2520complex%2520scenes%2520and%2520real-world%2520objects.%2520Although%2520the%2520field%2520of%2520sketch%2520representation%2520learning%2520has%2520advanced%2520significantly%252C%2520there%2520is%2520still%2520a%2520gap%2520in%2520understanding%2520the%2520true%2520relevance%2520of%2520the%2520temporal%2520aspect%2520to%2520the%2520quality%2520of%2520these%2520representations.%2520This%2520work%2520investigates%2520whether%2520it%2520is%2520indeed%2520justifiable%2520to%2520treat%2520sketches%2520as%2520sequences%252C%2520as%2520well%2520as%2520which%2520internal%2520orders%2520play%2520a%2520more%2520relevant%2520role.%2520The%2520results%2520indicate%2520that%252C%2520although%2520the%2520use%2520of%2520traditional%2520positional%2520encodings%2520is%2520valid%2520for%2520modeling%2520sketches%2520as%2520sequences%252C%2520absolute%2520coordinates%2520consistently%2520outperform%2520relative%2520ones.%2520Furthermore%252C%2520non-autoregressive%2520decoders%2520outperform%2520their%2520autoregressive%2520counterparts.%2520Finally%252C%2520the%2520importance%2520of%2520temporality%2520was%2520shown%2520to%2520depend%2520on%2520both%2520the%2520order%2520considered%2520and%2520the%2520task%2520evaluated.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Temporality%20for%20Sketch%20Representation%20Learning&entry.906535625=Marcelo%20Isaias%20de%20Moraes%20Junior%20and%20Moacir%20Antonelli%20Ponti&entry.1292438233=Sketches%20are%20simple%20human%20hand-drawn%20abstractions%20of%20complex%20scenes%20and%20real-world%20objects.%20Although%20the%20field%20of%20sketch%20representation%20learning%20has%20advanced%20significantly%2C%20there%20is%20still%20a%20gap%20in%20understanding%20the%20true%20relevance%20of%20the%20temporal%20aspect%20to%20the%20quality%20of%20these%20representations.%20This%20work%20investigates%20whether%20it%20is%20indeed%20justifiable%20to%20treat%20sketches%20as%20sequences%2C%20as%20well%20as%20which%20internal%20orders%20play%20a%20more%20relevant%20role.%20The%20results%20indicate%20that%2C%20although%20the%20use%20of%20traditional%20positional%20encodings%20is%20valid%20for%20modeling%20sketches%20as%20sequences%2C%20absolute%20coordinates%20consistently%20outperform%20relative%20ones.%20Furthermore%2C%20non-autoregressive%20decoders%20outperform%20their%20autoregressive%20counterparts.%20Finally%2C%20the%20importance%20of%20temporality%20was%20shown%20to%20depend%20on%20both%20the%20order%20considered%20and%20the%20task%20evaluated.&entry.1838667208=http%3A//arxiv.org/abs/2512.04007v1&entry.124074799=Read"},
{"title": "Jina-VLM: Small Multilingual Vision Language Model", "author": "Andreas Koukounas and Georgios Mastrapas and Florian H\u00f6nicke and Sedigheh Eslami and Guillaume Roncari and Scott Martens and Han Xiao", "abstract": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.", "link": "http://arxiv.org/abs/2512.04032v1", "date": "2025-12-03", "relevancy": 2.4449, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jina-VLM%3A%20Small%20Multilingual%20Vision%20Language%20Model&body=Title%3A%20Jina-VLM%3A%20Small%20Multilingual%20Vision%20Language%20Model%0AAuthor%3A%20Andreas%20Koukounas%20and%20Georgios%20Mastrapas%20and%20Florian%20H%C3%B6nicke%20and%20Sedigheh%20Eslami%20and%20Guillaume%20Roncari%20and%20Scott%20Martens%20and%20Han%20Xiao%0AAbstract%3A%20We%20present%20Jina-VLM%2C%20a%202.4B%20parameter%20vision-language%20model%20that%20achieves%20state-of-the-art%20multilingual%20visual%20question%20answering%20among%20open%202B-scale%20VLMs.%20The%20model%20couples%20a%20SigLIP2%20vision%20encoder%20with%20a%20Qwen3%20language%20backbone%20through%20an%20attention-pooling%20connector%20that%20enables%20token-efficient%20processing%20of%20arbitrary-resolution%20images.%20Across%20standard%20VQA%20benchmarks%20and%20multilingual%20evaluations%2C%20Jina-VLM%20outperforms%20comparable%20models%20while%20preserving%20competitive%20text-only%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJina-VLM%253A%2520Small%2520Multilingual%2520Vision%2520Language%2520Model%26entry.906535625%3DAndreas%2520Koukounas%2520and%2520Georgios%2520Mastrapas%2520and%2520Florian%2520H%25C3%25B6nicke%2520and%2520Sedigheh%2520Eslami%2520and%2520Guillaume%2520Roncari%2520and%2520Scott%2520Martens%2520and%2520Han%2520Xiao%26entry.1292438233%3DWe%2520present%2520Jina-VLM%252C%2520a%25202.4B%2520parameter%2520vision-language%2520model%2520that%2520achieves%2520state-of-the-art%2520multilingual%2520visual%2520question%2520answering%2520among%2520open%25202B-scale%2520VLMs.%2520The%2520model%2520couples%2520a%2520SigLIP2%2520vision%2520encoder%2520with%2520a%2520Qwen3%2520language%2520backbone%2520through%2520an%2520attention-pooling%2520connector%2520that%2520enables%2520token-efficient%2520processing%2520of%2520arbitrary-resolution%2520images.%2520Across%2520standard%2520VQA%2520benchmarks%2520and%2520multilingual%2520evaluations%252C%2520Jina-VLM%2520outperforms%2520comparable%2520models%2520while%2520preserving%2520competitive%2520text-only%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jina-VLM%3A%20Small%20Multilingual%20Vision%20Language%20Model&entry.906535625=Andreas%20Koukounas%20and%20Georgios%20Mastrapas%20and%20Florian%20H%C3%B6nicke%20and%20Sedigheh%20Eslami%20and%20Guillaume%20Roncari%20and%20Scott%20Martens%20and%20Han%20Xiao&entry.1292438233=We%20present%20Jina-VLM%2C%20a%202.4B%20parameter%20vision-language%20model%20that%20achieves%20state-of-the-art%20multilingual%20visual%20question%20answering%20among%20open%202B-scale%20VLMs.%20The%20model%20couples%20a%20SigLIP2%20vision%20encoder%20with%20a%20Qwen3%20language%20backbone%20through%20an%20attention-pooling%20connector%20that%20enables%20token-efficient%20processing%20of%20arbitrary-resolution%20images.%20Across%20standard%20VQA%20benchmarks%20and%20multilingual%20evaluations%2C%20Jina-VLM%20outperforms%20comparable%20models%20while%20preserving%20competitive%20text-only%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.04032v1&entry.124074799=Read"},
{"title": "EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification", "author": "Hanhui Deng and Xinglin Li and Jie Luo and Zhanpeng Jin and Di Wu", "abstract": "Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.", "link": "http://arxiv.org/abs/2512.03804v1", "date": "2025-12-03", "relevancy": 2.4093, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5098}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4729}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EfficientECG%3A%20Cross-Attention%20with%20Feature%20Fusion%20for%20Efficient%20Electrocardiogram%20Classification&body=Title%3A%20EfficientECG%3A%20Cross-Attention%20with%20Feature%20Fusion%20for%20Efficient%20Electrocardiogram%20Classification%0AAuthor%3A%20Hanhui%20Deng%20and%20Xinglin%20Li%20and%20Jie%20Luo%20and%20Zhanpeng%20Jin%20and%20Di%20Wu%0AAbstract%3A%20Electrocardiogram%20is%20a%20useful%20diagnostic%20signal%20that%20can%20detect%20cardiac%20abnormalities%20by%20measuring%20the%20electrical%20activity%20generated%20by%20the%20heart.%20Due%20to%20its%20rapid%2C%20non-invasive%2C%20and%20richly%20informative%20characteristics%2C%20ECG%20has%20many%20emerging%20applications.%20In%20this%20paper%2C%20we%20study%20novel%20deep%20learning%20technologies%20to%20effectively%20manage%20and%20analyse%20ECG%20data%2C%20with%20the%20aim%20of%20building%20a%20diagnostic%20model%2C%20accurately%20and%20quickly%2C%20that%20can%20substantially%20reduce%20the%20burden%20on%20medical%20workers.%20Unlike%20the%20existing%20ECG%20models%20that%20exhibit%20a%20high%20misdiagnosis%20rate%2C%20our%20deep%20learning%20approaches%20can%20automatically%20extract%20the%20features%20of%20ECG%20data%20through%20end-to-end%20training.%20Specifically%2C%20we%20first%20devise%20EfficientECG%2C%20an%20accurate%20and%20lightweight%20classification%20model%20for%20ECG%20analysis%20based%20on%20the%20existing%20EfficientNet%20model%2C%20which%20can%20effectively%20handle%20high-frequency%20long-sequence%20ECG%20data%20with%20various%20leading%20types.%20On%20top%20of%20that%2C%20we%20next%20propose%20a%20cross-attention-based%20feature%20fusion%20model%20of%20EfficientECG%20for%20analysing%20multi-lead%20ECG%20data%20with%20multiple%20features%20%28e.g.%2C%20gender%20and%20age%29.%20Our%20evaluations%20on%20representative%20ECG%20datasets%20validate%20the%20superiority%20of%20our%20model%20against%20state-of-the-art%20works%20in%20terms%20of%20high%20precision%2C%20multi-feature%20fusion%2C%20and%20lightweights.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficientECG%253A%2520Cross-Attention%2520with%2520Feature%2520Fusion%2520for%2520Efficient%2520Electrocardiogram%2520Classification%26entry.906535625%3DHanhui%2520Deng%2520and%2520Xinglin%2520Li%2520and%2520Jie%2520Luo%2520and%2520Zhanpeng%2520Jin%2520and%2520Di%2520Wu%26entry.1292438233%3DElectrocardiogram%2520is%2520a%2520useful%2520diagnostic%2520signal%2520that%2520can%2520detect%2520cardiac%2520abnormalities%2520by%2520measuring%2520the%2520electrical%2520activity%2520generated%2520by%2520the%2520heart.%2520Due%2520to%2520its%2520rapid%252C%2520non-invasive%252C%2520and%2520richly%2520informative%2520characteristics%252C%2520ECG%2520has%2520many%2520emerging%2520applications.%2520In%2520this%2520paper%252C%2520we%2520study%2520novel%2520deep%2520learning%2520technologies%2520to%2520effectively%2520manage%2520and%2520analyse%2520ECG%2520data%252C%2520with%2520the%2520aim%2520of%2520building%2520a%2520diagnostic%2520model%252C%2520accurately%2520and%2520quickly%252C%2520that%2520can%2520substantially%2520reduce%2520the%2520burden%2520on%2520medical%2520workers.%2520Unlike%2520the%2520existing%2520ECG%2520models%2520that%2520exhibit%2520a%2520high%2520misdiagnosis%2520rate%252C%2520our%2520deep%2520learning%2520approaches%2520can%2520automatically%2520extract%2520the%2520features%2520of%2520ECG%2520data%2520through%2520end-to-end%2520training.%2520Specifically%252C%2520we%2520first%2520devise%2520EfficientECG%252C%2520an%2520accurate%2520and%2520lightweight%2520classification%2520model%2520for%2520ECG%2520analysis%2520based%2520on%2520the%2520existing%2520EfficientNet%2520model%252C%2520which%2520can%2520effectively%2520handle%2520high-frequency%2520long-sequence%2520ECG%2520data%2520with%2520various%2520leading%2520types.%2520On%2520top%2520of%2520that%252C%2520we%2520next%2520propose%2520a%2520cross-attention-based%2520feature%2520fusion%2520model%2520of%2520EfficientECG%2520for%2520analysing%2520multi-lead%2520ECG%2520data%2520with%2520multiple%2520features%2520%2528e.g.%252C%2520gender%2520and%2520age%2529.%2520Our%2520evaluations%2520on%2520representative%2520ECG%2520datasets%2520validate%2520the%2520superiority%2520of%2520our%2520model%2520against%2520state-of-the-art%2520works%2520in%2520terms%2520of%2520high%2520precision%252C%2520multi-feature%2520fusion%252C%2520and%2520lightweights.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EfficientECG%3A%20Cross-Attention%20with%20Feature%20Fusion%20for%20Efficient%20Electrocardiogram%20Classification&entry.906535625=Hanhui%20Deng%20and%20Xinglin%20Li%20and%20Jie%20Luo%20and%20Zhanpeng%20Jin%20and%20Di%20Wu&entry.1292438233=Electrocardiogram%20is%20a%20useful%20diagnostic%20signal%20that%20can%20detect%20cardiac%20abnormalities%20by%20measuring%20the%20electrical%20activity%20generated%20by%20the%20heart.%20Due%20to%20its%20rapid%2C%20non-invasive%2C%20and%20richly%20informative%20characteristics%2C%20ECG%20has%20many%20emerging%20applications.%20In%20this%20paper%2C%20we%20study%20novel%20deep%20learning%20technologies%20to%20effectively%20manage%20and%20analyse%20ECG%20data%2C%20with%20the%20aim%20of%20building%20a%20diagnostic%20model%2C%20accurately%20and%20quickly%2C%20that%20can%20substantially%20reduce%20the%20burden%20on%20medical%20workers.%20Unlike%20the%20existing%20ECG%20models%20that%20exhibit%20a%20high%20misdiagnosis%20rate%2C%20our%20deep%20learning%20approaches%20can%20automatically%20extract%20the%20features%20of%20ECG%20data%20through%20end-to-end%20training.%20Specifically%2C%20we%20first%20devise%20EfficientECG%2C%20an%20accurate%20and%20lightweight%20classification%20model%20for%20ECG%20analysis%20based%20on%20the%20existing%20EfficientNet%20model%2C%20which%20can%20effectively%20handle%20high-frequency%20long-sequence%20ECG%20data%20with%20various%20leading%20types.%20On%20top%20of%20that%2C%20we%20next%20propose%20a%20cross-attention-based%20feature%20fusion%20model%20of%20EfficientECG%20for%20analysing%20multi-lead%20ECG%20data%20with%20multiple%20features%20%28e.g.%2C%20gender%20and%20age%29.%20Our%20evaluations%20on%20representative%20ECG%20datasets%20validate%20the%20superiority%20of%20our%20model%20against%20state-of-the-art%20works%20in%20terms%20of%20high%20precision%2C%20multi-feature%20fusion%2C%20and%20lightweights.&entry.1838667208=http%3A//arxiv.org/abs/2512.03804v1&entry.124074799=Read"},
{"title": "RELIC: Interactive Video World Model with Long-Horizon Memory", "author": "Yicong Hong and Yiqun Mei and Chongjian Ge and Yiran Xu and Yang Zhou and Sai Bi and Yannick Hold-Geoffroy and Mike Roberts and Matthew Fisher and Eli Shechtman and Kalyan Sunkavalli and Feng Liu and Zhengqi Li and Hao Tan", "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.", "link": "http://arxiv.org/abs/2512.04040v1", "date": "2025-12-03", "relevancy": 2.4072, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6516}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6056}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RELIC%3A%20Interactive%20Video%20World%20Model%20with%20Long-Horizon%20Memory&body=Title%3A%20RELIC%3A%20Interactive%20Video%20World%20Model%20with%20Long-Horizon%20Memory%0AAuthor%3A%20Yicong%20Hong%20and%20Yiqun%20Mei%20and%20Chongjian%20Ge%20and%20Yiran%20Xu%20and%20Yang%20Zhou%20and%20Sai%20Bi%20and%20Yannick%20Hold-Geoffroy%20and%20Mike%20Roberts%20and%20Matthew%20Fisher%20and%20Eli%20Shechtman%20and%20Kalyan%20Sunkavalli%20and%20Feng%20Liu%20and%20Zhengqi%20Li%20and%20Hao%20Tan%0AAbstract%3A%20A%20truly%20interactive%20world%20model%20requires%20three%20key%20ingredients%3A%20real-time%20long-horizon%20streaming%2C%20consistent%20spatial%20memory%2C%20and%20precise%20user%20control.%20However%2C%20most%20existing%20approaches%20address%20only%20one%20of%20these%20aspects%20in%20isolation%2C%20as%20achieving%20all%20three%20simultaneously%20is%20highly%20challenging-for%20example%2C%20long-term%20memory%20mechanisms%20often%20degrade%20real-time%20performance.%20In%20this%20work%2C%20we%20present%20RELIC%2C%20a%20unified%20framework%20that%20tackles%20these%20three%20challenges%20altogether.%20Given%20a%20single%20image%20and%20a%20text%20description%2C%20RELIC%20enables%20memory-aware%2C%20long-duration%20exploration%20of%20arbitrary%20scenes%20in%20real%20time.%20Built%20upon%20recent%20autoregressive%20video-diffusion%20distillation%20techniques%2C%20our%20model%20represents%20long-horizon%20memory%20using%20highly%20compressed%20historical%20latent%20tokens%20encoded%20with%20both%20relative%20actions%20and%20absolute%20camera%20poses%20within%20the%20KV%20cache.%20This%20compact%2C%20camera-aware%20memory%20structure%20supports%20implicit%203D-consistent%20content%20retrieval%20and%20enforces%20long-term%20coherence%20with%20minimal%20computational%20overhead.%20In%20parallel%2C%20we%20fine-tune%20a%20bidirectional%20teacher%20video%20model%20to%20generate%20sequences%20beyond%20its%20original%205-second%20training%20horizon%2C%20and%20transform%20it%20into%20a%20causal%20student%20generator%20using%20a%20new%20memory-efficient%20self-forcing%20paradigm%20that%20enables%20full-context%20distillation%20over%20long-duration%20teacher%20as%20well%20as%20long%20student%20self-rollouts.%20Implemented%20as%20a%2014B-parameter%20model%20and%20trained%20on%20a%20curated%20Unreal%20Engine-rendered%20dataset%2C%20RELIC%20achieves%20real-time%20generation%20at%2016%20FPS%20while%20demonstrating%20more%20accurate%20action%20following%2C%20more%20stable%20long-horizon%20streaming%2C%20and%20more%20robust%20spatial-memory%20retrieval%20compared%20with%20prior%20work.%20These%20capabilities%20establish%20RELIC%20as%20a%20strong%20foundation%20for%20the%20next%20generation%20of%20interactive%20world%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRELIC%253A%2520Interactive%2520Video%2520World%2520Model%2520with%2520Long-Horizon%2520Memory%26entry.906535625%3DYicong%2520Hong%2520and%2520Yiqun%2520Mei%2520and%2520Chongjian%2520Ge%2520and%2520Yiran%2520Xu%2520and%2520Yang%2520Zhou%2520and%2520Sai%2520Bi%2520and%2520Yannick%2520Hold-Geoffroy%2520and%2520Mike%2520Roberts%2520and%2520Matthew%2520Fisher%2520and%2520Eli%2520Shechtman%2520and%2520Kalyan%2520Sunkavalli%2520and%2520Feng%2520Liu%2520and%2520Zhengqi%2520Li%2520and%2520Hao%2520Tan%26entry.1292438233%3DA%2520truly%2520interactive%2520world%2520model%2520requires%2520three%2520key%2520ingredients%253A%2520real-time%2520long-horizon%2520streaming%252C%2520consistent%2520spatial%2520memory%252C%2520and%2520precise%2520user%2520control.%2520However%252C%2520most%2520existing%2520approaches%2520address%2520only%2520one%2520of%2520these%2520aspects%2520in%2520isolation%252C%2520as%2520achieving%2520all%2520three%2520simultaneously%2520is%2520highly%2520challenging-for%2520example%252C%2520long-term%2520memory%2520mechanisms%2520often%2520degrade%2520real-time%2520performance.%2520In%2520this%2520work%252C%2520we%2520present%2520RELIC%252C%2520a%2520unified%2520framework%2520that%2520tackles%2520these%2520three%2520challenges%2520altogether.%2520Given%2520a%2520single%2520image%2520and%2520a%2520text%2520description%252C%2520RELIC%2520enables%2520memory-aware%252C%2520long-duration%2520exploration%2520of%2520arbitrary%2520scenes%2520in%2520real%2520time.%2520Built%2520upon%2520recent%2520autoregressive%2520video-diffusion%2520distillation%2520techniques%252C%2520our%2520model%2520represents%2520long-horizon%2520memory%2520using%2520highly%2520compressed%2520historical%2520latent%2520tokens%2520encoded%2520with%2520both%2520relative%2520actions%2520and%2520absolute%2520camera%2520poses%2520within%2520the%2520KV%2520cache.%2520This%2520compact%252C%2520camera-aware%2520memory%2520structure%2520supports%2520implicit%25203D-consistent%2520content%2520retrieval%2520and%2520enforces%2520long-term%2520coherence%2520with%2520minimal%2520computational%2520overhead.%2520In%2520parallel%252C%2520we%2520fine-tune%2520a%2520bidirectional%2520teacher%2520video%2520model%2520to%2520generate%2520sequences%2520beyond%2520its%2520original%25205-second%2520training%2520horizon%252C%2520and%2520transform%2520it%2520into%2520a%2520causal%2520student%2520generator%2520using%2520a%2520new%2520memory-efficient%2520self-forcing%2520paradigm%2520that%2520enables%2520full-context%2520distillation%2520over%2520long-duration%2520teacher%2520as%2520well%2520as%2520long%2520student%2520self-rollouts.%2520Implemented%2520as%2520a%252014B-parameter%2520model%2520and%2520trained%2520on%2520a%2520curated%2520Unreal%2520Engine-rendered%2520dataset%252C%2520RELIC%2520achieves%2520real-time%2520generation%2520at%252016%2520FPS%2520while%2520demonstrating%2520more%2520accurate%2520action%2520following%252C%2520more%2520stable%2520long-horizon%2520streaming%252C%2520and%2520more%2520robust%2520spatial-memory%2520retrieval%2520compared%2520with%2520prior%2520work.%2520These%2520capabilities%2520establish%2520RELIC%2520as%2520a%2520strong%2520foundation%2520for%2520the%2520next%2520generation%2520of%2520interactive%2520world%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RELIC%3A%20Interactive%20Video%20World%20Model%20with%20Long-Horizon%20Memory&entry.906535625=Yicong%20Hong%20and%20Yiqun%20Mei%20and%20Chongjian%20Ge%20and%20Yiran%20Xu%20and%20Yang%20Zhou%20and%20Sai%20Bi%20and%20Yannick%20Hold-Geoffroy%20and%20Mike%20Roberts%20and%20Matthew%20Fisher%20and%20Eli%20Shechtman%20and%20Kalyan%20Sunkavalli%20and%20Feng%20Liu%20and%20Zhengqi%20Li%20and%20Hao%20Tan&entry.1292438233=A%20truly%20interactive%20world%20model%20requires%20three%20key%20ingredients%3A%20real-time%20long-horizon%20streaming%2C%20consistent%20spatial%20memory%2C%20and%20precise%20user%20control.%20However%2C%20most%20existing%20approaches%20address%20only%20one%20of%20these%20aspects%20in%20isolation%2C%20as%20achieving%20all%20three%20simultaneously%20is%20highly%20challenging-for%20example%2C%20long-term%20memory%20mechanisms%20often%20degrade%20real-time%20performance.%20In%20this%20work%2C%20we%20present%20RELIC%2C%20a%20unified%20framework%20that%20tackles%20these%20three%20challenges%20altogether.%20Given%20a%20single%20image%20and%20a%20text%20description%2C%20RELIC%20enables%20memory-aware%2C%20long-duration%20exploration%20of%20arbitrary%20scenes%20in%20real%20time.%20Built%20upon%20recent%20autoregressive%20video-diffusion%20distillation%20techniques%2C%20our%20model%20represents%20long-horizon%20memory%20using%20highly%20compressed%20historical%20latent%20tokens%20encoded%20with%20both%20relative%20actions%20and%20absolute%20camera%20poses%20within%20the%20KV%20cache.%20This%20compact%2C%20camera-aware%20memory%20structure%20supports%20implicit%203D-consistent%20content%20retrieval%20and%20enforces%20long-term%20coherence%20with%20minimal%20computational%20overhead.%20In%20parallel%2C%20we%20fine-tune%20a%20bidirectional%20teacher%20video%20model%20to%20generate%20sequences%20beyond%20its%20original%205-second%20training%20horizon%2C%20and%20transform%20it%20into%20a%20causal%20student%20generator%20using%20a%20new%20memory-efficient%20self-forcing%20paradigm%20that%20enables%20full-context%20distillation%20over%20long-duration%20teacher%20as%20well%20as%20long%20student%20self-rollouts.%20Implemented%20as%20a%2014B-parameter%20model%20and%20trained%20on%20a%20curated%20Unreal%20Engine-rendered%20dataset%2C%20RELIC%20achieves%20real-time%20generation%20at%2016%20FPS%20while%20demonstrating%20more%20accurate%20action%20following%2C%20more%20stable%20long-horizon%20streaming%2C%20and%20more%20robust%20spatial-memory%20retrieval%20compared%20with%20prior%20work.%20These%20capabilities%20establish%20RELIC%20as%20a%20strong%20foundation%20for%20the%20next%20generation%20of%20interactive%20world%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2512.04040v1&entry.124074799=Read"},
{"title": "Defense That Attacks: How Robust Models Become Better Attackers", "author": "Mohamed Awad and Mahmoud Akrm and Walid Gomaa", "abstract": "Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.", "link": "http://arxiv.org/abs/2512.02830v2", "date": "2025-12-03", "relevancy": 2.3996, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5007}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defense%20That%20Attacks%3A%20How%20Robust%20Models%20Become%20Better%20Attackers&body=Title%3A%20Defense%20That%20Attacks%3A%20How%20Robust%20Models%20Become%20Better%20Attackers%0AAuthor%3A%20Mohamed%20Awad%20and%20Mahmoud%20Akrm%20and%20Walid%20Gomaa%0AAbstract%3A%20Deep%20learning%20has%20achieved%20great%20success%20in%20computer%20vision%2C%20but%20remains%20vulnerable%20to%20adversarial%20attacks.%20Adversarial%20training%20is%20the%20leading%20defense%20designed%20to%20improve%20model%20robustness.%20However%2C%20its%20effect%20on%20the%20transferability%20of%20attacks%20is%20underexplored.%20In%20this%20work%2C%20we%20ask%20whether%20adversarial%20training%20unintentionally%20increases%20the%20transferability%20of%20adversarial%20examples.%20To%20answer%20this%2C%20we%20trained%20a%20diverse%20zoo%20of%2036%20models%2C%20including%20CNNs%20and%20ViTs%2C%20and%20conducted%20comprehensive%20transferability%20experiments.%20Our%20results%20reveal%20a%20clear%20paradox%3A%20adversarially%20trained%20%28AT%29%20models%20produce%20perturbations%20that%20transfer%20more%20effectively%20than%20those%20from%20standard%20models%2C%20which%20introduce%20a%20new%20ecosystem%20risk.%20To%20enable%20reproducibility%20and%20further%20study%2C%20we%20release%20all%20models%2C%20code%2C%20and%20experimental%20scripts.%20Furthermore%2C%20we%20argue%20that%20robustness%20evaluations%20should%20assess%20not%20only%20the%20resistance%20of%20a%20model%20to%20transferred%20attacks%20but%20also%20its%20propensity%20to%20produce%20transferable%20adversarial%20examples.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02830v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefense%2520That%2520Attacks%253A%2520How%2520Robust%2520Models%2520Become%2520Better%2520Attackers%26entry.906535625%3DMohamed%2520Awad%2520and%2520Mahmoud%2520Akrm%2520and%2520Walid%2520Gomaa%26entry.1292438233%3DDeep%2520learning%2520has%2520achieved%2520great%2520success%2520in%2520computer%2520vision%252C%2520but%2520remains%2520vulnerable%2520to%2520adversarial%2520attacks.%2520Adversarial%2520training%2520is%2520the%2520leading%2520defense%2520designed%2520to%2520improve%2520model%2520robustness.%2520However%252C%2520its%2520effect%2520on%2520the%2520transferability%2520of%2520attacks%2520is%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520ask%2520whether%2520adversarial%2520training%2520unintentionally%2520increases%2520the%2520transferability%2520of%2520adversarial%2520examples.%2520To%2520answer%2520this%252C%2520we%2520trained%2520a%2520diverse%2520zoo%2520of%252036%2520models%252C%2520including%2520CNNs%2520and%2520ViTs%252C%2520and%2520conducted%2520comprehensive%2520transferability%2520experiments.%2520Our%2520results%2520reveal%2520a%2520clear%2520paradox%253A%2520adversarially%2520trained%2520%2528AT%2529%2520models%2520produce%2520perturbations%2520that%2520transfer%2520more%2520effectively%2520than%2520those%2520from%2520standard%2520models%252C%2520which%2520introduce%2520a%2520new%2520ecosystem%2520risk.%2520To%2520enable%2520reproducibility%2520and%2520further%2520study%252C%2520we%2520release%2520all%2520models%252C%2520code%252C%2520and%2520experimental%2520scripts.%2520Furthermore%252C%2520we%2520argue%2520that%2520robustness%2520evaluations%2520should%2520assess%2520not%2520only%2520the%2520resistance%2520of%2520a%2520model%2520to%2520transferred%2520attacks%2520but%2520also%2520its%2520propensity%2520to%2520produce%2520transferable%2520adversarial%2520examples.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02830v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defense%20That%20Attacks%3A%20How%20Robust%20Models%20Become%20Better%20Attackers&entry.906535625=Mohamed%20Awad%20and%20Mahmoud%20Akrm%20and%20Walid%20Gomaa&entry.1292438233=Deep%20learning%20has%20achieved%20great%20success%20in%20computer%20vision%2C%20but%20remains%20vulnerable%20to%20adversarial%20attacks.%20Adversarial%20training%20is%20the%20leading%20defense%20designed%20to%20improve%20model%20robustness.%20However%2C%20its%20effect%20on%20the%20transferability%20of%20attacks%20is%20underexplored.%20In%20this%20work%2C%20we%20ask%20whether%20adversarial%20training%20unintentionally%20increases%20the%20transferability%20of%20adversarial%20examples.%20To%20answer%20this%2C%20we%20trained%20a%20diverse%20zoo%20of%2036%20models%2C%20including%20CNNs%20and%20ViTs%2C%20and%20conducted%20comprehensive%20transferability%20experiments.%20Our%20results%20reveal%20a%20clear%20paradox%3A%20adversarially%20trained%20%28AT%29%20models%20produce%20perturbations%20that%20transfer%20more%20effectively%20than%20those%20from%20standard%20models%2C%20which%20introduce%20a%20new%20ecosystem%20risk.%20To%20enable%20reproducibility%20and%20further%20study%2C%20we%20release%20all%20models%2C%20code%2C%20and%20experimental%20scripts.%20Furthermore%2C%20we%20argue%20that%20robustness%20evaluations%20should%20assess%20not%20only%20the%20resistance%20of%20a%20model%20to%20transferred%20attacks%20but%20also%20its%20propensity%20to%20produce%20transferable%20adversarial%20examples.&entry.1838667208=http%3A//arxiv.org/abs/2512.02830v2&entry.124074799=Read"},
{"title": "SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control", "author": "Yuxuan Mu and Ziyu Zhang and Yi Shi and Minami Matsumoto and Kotaro Imamura and Guy Tevet and Chuan Guo and Michael Taylor and Chang Shu and Pengcheng Xi and Xue Bin Peng", "abstract": "Data-driven motion priors that can guide agents toward producing naturalistic behaviors play a pivotal role in creating life-like virtual characters. Adversarial imitation learning has been a highly effective method for learning motion priors from reference motion data. However, adversarial priors, with few exceptions, need to be retrained for each new controller, thereby limiting their reusability and necessitating the retention of the reference motion data when training on downstream tasks. In this work, we present Score-Matching Motion Priors (SMP), which leverages pre-trained motion diffusion models and score distillation sampling (SDS) to create reusable task-agnostic motion priors. SMPs can be pre-trained on a motion dataset, independent of any control policy or task. Once trained, SMPs can be kept frozen and reused as general-purpose reward functions to train policies to produce naturalistic behaviors for downstream tasks. We show that a general motion prior trained on large-scale datasets can be repurposed into a variety of style-specific priors. Furthermore SMP can compose different styles to synthesize new styles not present in the original dataset. Our method produces high-quality motion comparable to state-of-the-art adversarial imitation learning methods through reusable and modular motion priors. We demonstrate the effectiveness of SMP across a diverse suite of control tasks with physically simulated humanoid characters. Video demo available at https://youtu.be/ravlZJteS20", "link": "http://arxiv.org/abs/2512.03028v2", "date": "2025-12-03", "relevancy": 2.3989, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.566}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMP%3A%20Reusable%20Score-Matching%20Motion%20Priors%20for%20Physics-Based%20Character%20Control&body=Title%3A%20SMP%3A%20Reusable%20Score-Matching%20Motion%20Priors%20for%20Physics-Based%20Character%20Control%0AAuthor%3A%20Yuxuan%20Mu%20and%20Ziyu%20Zhang%20and%20Yi%20Shi%20and%20Minami%20Matsumoto%20and%20Kotaro%20Imamura%20and%20Guy%20Tevet%20and%20Chuan%20Guo%20and%20Michael%20Taylor%20and%20Chang%20Shu%20and%20Pengcheng%20Xi%20and%20Xue%20Bin%20Peng%0AAbstract%3A%20Data-driven%20motion%20priors%20that%20can%20guide%20agents%20toward%20producing%20naturalistic%20behaviors%20play%20a%20pivotal%20role%20in%20creating%20life-like%20virtual%20characters.%20Adversarial%20imitation%20learning%20has%20been%20a%20highly%20effective%20method%20for%20learning%20motion%20priors%20from%20reference%20motion%20data.%20However%2C%20adversarial%20priors%2C%20with%20few%20exceptions%2C%20need%20to%20be%20retrained%20for%20each%20new%20controller%2C%20thereby%20limiting%20their%20reusability%20and%20necessitating%20the%20retention%20of%20the%20reference%20motion%20data%20when%20training%20on%20downstream%20tasks.%20In%20this%20work%2C%20we%20present%20Score-Matching%20Motion%20Priors%20%28SMP%29%2C%20which%20leverages%20pre-trained%20motion%20diffusion%20models%20and%20score%20distillation%20sampling%20%28SDS%29%20to%20create%20reusable%20task-agnostic%20motion%20priors.%20SMPs%20can%20be%20pre-trained%20on%20a%20motion%20dataset%2C%20independent%20of%20any%20control%20policy%20or%20task.%20Once%20trained%2C%20SMPs%20can%20be%20kept%20frozen%20and%20reused%20as%20general-purpose%20reward%20functions%20to%20train%20policies%20to%20produce%20naturalistic%20behaviors%20for%20downstream%20tasks.%20We%20show%20that%20a%20general%20motion%20prior%20trained%20on%20large-scale%20datasets%20can%20be%20repurposed%20into%20a%20variety%20of%20style-specific%20priors.%20Furthermore%20SMP%20can%20compose%20different%20styles%20to%20synthesize%20new%20styles%20not%20present%20in%20the%20original%20dataset.%20Our%20method%20produces%20high-quality%20motion%20comparable%20to%20state-of-the-art%20adversarial%20imitation%20learning%20methods%20through%20reusable%20and%20modular%20motion%20priors.%20We%20demonstrate%20the%20effectiveness%20of%20SMP%20across%20a%20diverse%20suite%20of%20control%20tasks%20with%20physically%20simulated%20humanoid%20characters.%20Video%20demo%20available%20at%20https%3A//youtu.be/ravlZJteS20%0ALink%3A%20http%3A//arxiv.org/abs/2512.03028v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMP%253A%2520Reusable%2520Score-Matching%2520Motion%2520Priors%2520for%2520Physics-Based%2520Character%2520Control%26entry.906535625%3DYuxuan%2520Mu%2520and%2520Ziyu%2520Zhang%2520and%2520Yi%2520Shi%2520and%2520Minami%2520Matsumoto%2520and%2520Kotaro%2520Imamura%2520and%2520Guy%2520Tevet%2520and%2520Chuan%2520Guo%2520and%2520Michael%2520Taylor%2520and%2520Chang%2520Shu%2520and%2520Pengcheng%2520Xi%2520and%2520Xue%2520Bin%2520Peng%26entry.1292438233%3DData-driven%2520motion%2520priors%2520that%2520can%2520guide%2520agents%2520toward%2520producing%2520naturalistic%2520behaviors%2520play%2520a%2520pivotal%2520role%2520in%2520creating%2520life-like%2520virtual%2520characters.%2520Adversarial%2520imitation%2520learning%2520has%2520been%2520a%2520highly%2520effective%2520method%2520for%2520learning%2520motion%2520priors%2520from%2520reference%2520motion%2520data.%2520However%252C%2520adversarial%2520priors%252C%2520with%2520few%2520exceptions%252C%2520need%2520to%2520be%2520retrained%2520for%2520each%2520new%2520controller%252C%2520thereby%2520limiting%2520their%2520reusability%2520and%2520necessitating%2520the%2520retention%2520of%2520the%2520reference%2520motion%2520data%2520when%2520training%2520on%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520present%2520Score-Matching%2520Motion%2520Priors%2520%2528SMP%2529%252C%2520which%2520leverages%2520pre-trained%2520motion%2520diffusion%2520models%2520and%2520score%2520distillation%2520sampling%2520%2528SDS%2529%2520to%2520create%2520reusable%2520task-agnostic%2520motion%2520priors.%2520SMPs%2520can%2520be%2520pre-trained%2520on%2520a%2520motion%2520dataset%252C%2520independent%2520of%2520any%2520control%2520policy%2520or%2520task.%2520Once%2520trained%252C%2520SMPs%2520can%2520be%2520kept%2520frozen%2520and%2520reused%2520as%2520general-purpose%2520reward%2520functions%2520to%2520train%2520policies%2520to%2520produce%2520naturalistic%2520behaviors%2520for%2520downstream%2520tasks.%2520We%2520show%2520that%2520a%2520general%2520motion%2520prior%2520trained%2520on%2520large-scale%2520datasets%2520can%2520be%2520repurposed%2520into%2520a%2520variety%2520of%2520style-specific%2520priors.%2520Furthermore%2520SMP%2520can%2520compose%2520different%2520styles%2520to%2520synthesize%2520new%2520styles%2520not%2520present%2520in%2520the%2520original%2520dataset.%2520Our%2520method%2520produces%2520high-quality%2520motion%2520comparable%2520to%2520state-of-the-art%2520adversarial%2520imitation%2520learning%2520methods%2520through%2520reusable%2520and%2520modular%2520motion%2520priors.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520SMP%2520across%2520a%2520diverse%2520suite%2520of%2520control%2520tasks%2520with%2520physically%2520simulated%2520humanoid%2520characters.%2520Video%2520demo%2520available%2520at%2520https%253A//youtu.be/ravlZJteS20%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03028v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMP%3A%20Reusable%20Score-Matching%20Motion%20Priors%20for%20Physics-Based%20Character%20Control&entry.906535625=Yuxuan%20Mu%20and%20Ziyu%20Zhang%20and%20Yi%20Shi%20and%20Minami%20Matsumoto%20and%20Kotaro%20Imamura%20and%20Guy%20Tevet%20and%20Chuan%20Guo%20and%20Michael%20Taylor%20and%20Chang%20Shu%20and%20Pengcheng%20Xi%20and%20Xue%20Bin%20Peng&entry.1292438233=Data-driven%20motion%20priors%20that%20can%20guide%20agents%20toward%20producing%20naturalistic%20behaviors%20play%20a%20pivotal%20role%20in%20creating%20life-like%20virtual%20characters.%20Adversarial%20imitation%20learning%20has%20been%20a%20highly%20effective%20method%20for%20learning%20motion%20priors%20from%20reference%20motion%20data.%20However%2C%20adversarial%20priors%2C%20with%20few%20exceptions%2C%20need%20to%20be%20retrained%20for%20each%20new%20controller%2C%20thereby%20limiting%20their%20reusability%20and%20necessitating%20the%20retention%20of%20the%20reference%20motion%20data%20when%20training%20on%20downstream%20tasks.%20In%20this%20work%2C%20we%20present%20Score-Matching%20Motion%20Priors%20%28SMP%29%2C%20which%20leverages%20pre-trained%20motion%20diffusion%20models%20and%20score%20distillation%20sampling%20%28SDS%29%20to%20create%20reusable%20task-agnostic%20motion%20priors.%20SMPs%20can%20be%20pre-trained%20on%20a%20motion%20dataset%2C%20independent%20of%20any%20control%20policy%20or%20task.%20Once%20trained%2C%20SMPs%20can%20be%20kept%20frozen%20and%20reused%20as%20general-purpose%20reward%20functions%20to%20train%20policies%20to%20produce%20naturalistic%20behaviors%20for%20downstream%20tasks.%20We%20show%20that%20a%20general%20motion%20prior%20trained%20on%20large-scale%20datasets%20can%20be%20repurposed%20into%20a%20variety%20of%20style-specific%20priors.%20Furthermore%20SMP%20can%20compose%20different%20styles%20to%20synthesize%20new%20styles%20not%20present%20in%20the%20original%20dataset.%20Our%20method%20produces%20high-quality%20motion%20comparable%20to%20state-of-the-art%20adversarial%20imitation%20learning%20methods%20through%20reusable%20and%20modular%20motion%20priors.%20We%20demonstrate%20the%20effectiveness%20of%20SMP%20across%20a%20diverse%20suite%20of%20control%20tasks%20with%20physically%20simulated%20humanoid%20characters.%20Video%20demo%20available%20at%20https%3A//youtu.be/ravlZJteS20&entry.1838667208=http%3A//arxiv.org/abs/2512.03028v2&entry.124074799=Read"},
{"title": "Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting", "author": "Howard Chen and Noam Razin and Karthik Narasimhan and Danqi Chen", "abstract": "Adapting language models (LMs) to new tasks via post-training carries the risk of degrading existing capabilities -- a phenomenon classically known as catastrophic forgetting. In this paper, toward identifying guidelines for mitigating this phenomenon, we systematically compare the forgetting patterns of two widely adopted post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL). Our experiments reveal a consistent trend across LM families (Llama, Qwen) and tasks (instruction following, general knowledge, and arithmetic reasoning): RL leads to less forgetting than SFT while achieving comparable or higher target task performance. To investigate the cause for this difference, we consider a simplified setting in which the LM is modeled as a mixture of two distributions, one corresponding to prior knowledge and the other to the target task. We identify that the mode-seeking nature of RL, which stems from its use of on-policy data, enables keeping prior knowledge intact when learning the target task. We then verify this insight by demonstrating that the use on-policy data underlies the robustness of RL to forgetting in practical settings, as opposed to other algorithmic choices such as the KL regularization or advantage estimation. Lastly, as a practical implication, our results highlight the potential of mitigating forgetting using approximately on-policy data, which can be substantially more efficient to obtain than fully on-policy data.", "link": "http://arxiv.org/abs/2510.18874v2", "date": "2025-12-03", "relevancy": 2.3984, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4853}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4831}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retaining%20by%20Doing%3A%20The%20Role%20of%20On-Policy%20Data%20in%20Mitigating%20Forgetting&body=Title%3A%20Retaining%20by%20Doing%3A%20The%20Role%20of%20On-Policy%20Data%20in%20Mitigating%20Forgetting%0AAuthor%3A%20Howard%20Chen%20and%20Noam%20Razin%20and%20Karthik%20Narasimhan%20and%20Danqi%20Chen%0AAbstract%3A%20Adapting%20language%20models%20%28LMs%29%20to%20new%20tasks%20via%20post-training%20carries%20the%20risk%20of%20degrading%20existing%20capabilities%20--%20a%20phenomenon%20classically%20known%20as%20catastrophic%20forgetting.%20In%20this%20paper%2C%20toward%20identifying%20guidelines%20for%20mitigating%20this%20phenomenon%2C%20we%20systematically%20compare%20the%20forgetting%20patterns%20of%20two%20widely%20adopted%20post-training%20methods%3A%20supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29.%20Our%20experiments%20reveal%20a%20consistent%20trend%20across%20LM%20families%20%28Llama%2C%20Qwen%29%20and%20tasks%20%28instruction%20following%2C%20general%20knowledge%2C%20and%20arithmetic%20reasoning%29%3A%20RL%20leads%20to%20less%20forgetting%20than%20SFT%20while%20achieving%20comparable%20or%20higher%20target%20task%20performance.%20To%20investigate%20the%20cause%20for%20this%20difference%2C%20we%20consider%20a%20simplified%20setting%20in%20which%20the%20LM%20is%20modeled%20as%20a%20mixture%20of%20two%20distributions%2C%20one%20corresponding%20to%20prior%20knowledge%20and%20the%20other%20to%20the%20target%20task.%20We%20identify%20that%20the%20mode-seeking%20nature%20of%20RL%2C%20which%20stems%20from%20its%20use%20of%20on-policy%20data%2C%20enables%20keeping%20prior%20knowledge%20intact%20when%20learning%20the%20target%20task.%20We%20then%20verify%20this%20insight%20by%20demonstrating%20that%20the%20use%20on-policy%20data%20underlies%20the%20robustness%20of%20RL%20to%20forgetting%20in%20practical%20settings%2C%20as%20opposed%20to%20other%20algorithmic%20choices%20such%20as%20the%20KL%20regularization%20or%20advantage%20estimation.%20Lastly%2C%20as%20a%20practical%20implication%2C%20our%20results%20highlight%20the%20potential%20of%20mitigating%20forgetting%20using%20approximately%20on-policy%20data%2C%20which%20can%20be%20substantially%20more%20efficient%20to%20obtain%20than%20fully%20on-policy%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2510.18874v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetaining%2520by%2520Doing%253A%2520The%2520Role%2520of%2520On-Policy%2520Data%2520in%2520Mitigating%2520Forgetting%26entry.906535625%3DHoward%2520Chen%2520and%2520Noam%2520Razin%2520and%2520Karthik%2520Narasimhan%2520and%2520Danqi%2520Chen%26entry.1292438233%3DAdapting%2520language%2520models%2520%2528LMs%2529%2520to%2520new%2520tasks%2520via%2520post-training%2520carries%2520the%2520risk%2520of%2520degrading%2520existing%2520capabilities%2520--%2520a%2520phenomenon%2520classically%2520known%2520as%2520catastrophic%2520forgetting.%2520In%2520this%2520paper%252C%2520toward%2520identifying%2520guidelines%2520for%2520mitigating%2520this%2520phenomenon%252C%2520we%2520systematically%2520compare%2520the%2520forgetting%2520patterns%2520of%2520two%2520widely%2520adopted%2520post-training%2520methods%253A%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520reinforcement%2520learning%2520%2528RL%2529.%2520Our%2520experiments%2520reveal%2520a%2520consistent%2520trend%2520across%2520LM%2520families%2520%2528Llama%252C%2520Qwen%2529%2520and%2520tasks%2520%2528instruction%2520following%252C%2520general%2520knowledge%252C%2520and%2520arithmetic%2520reasoning%2529%253A%2520RL%2520leads%2520to%2520less%2520forgetting%2520than%2520SFT%2520while%2520achieving%2520comparable%2520or%2520higher%2520target%2520task%2520performance.%2520To%2520investigate%2520the%2520cause%2520for%2520this%2520difference%252C%2520we%2520consider%2520a%2520simplified%2520setting%2520in%2520which%2520the%2520LM%2520is%2520modeled%2520as%2520a%2520mixture%2520of%2520two%2520distributions%252C%2520one%2520corresponding%2520to%2520prior%2520knowledge%2520and%2520the%2520other%2520to%2520the%2520target%2520task.%2520We%2520identify%2520that%2520the%2520mode-seeking%2520nature%2520of%2520RL%252C%2520which%2520stems%2520from%2520its%2520use%2520of%2520on-policy%2520data%252C%2520enables%2520keeping%2520prior%2520knowledge%2520intact%2520when%2520learning%2520the%2520target%2520task.%2520We%2520then%2520verify%2520this%2520insight%2520by%2520demonstrating%2520that%2520the%2520use%2520on-policy%2520data%2520underlies%2520the%2520robustness%2520of%2520RL%2520to%2520forgetting%2520in%2520practical%2520settings%252C%2520as%2520opposed%2520to%2520other%2520algorithmic%2520choices%2520such%2520as%2520the%2520KL%2520regularization%2520or%2520advantage%2520estimation.%2520Lastly%252C%2520as%2520a%2520practical%2520implication%252C%2520our%2520results%2520highlight%2520the%2520potential%2520of%2520mitigating%2520forgetting%2520using%2520approximately%2520on-policy%2520data%252C%2520which%2520can%2520be%2520substantially%2520more%2520efficient%2520to%2520obtain%2520than%2520fully%2520on-policy%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18874v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retaining%20by%20Doing%3A%20The%20Role%20of%20On-Policy%20Data%20in%20Mitigating%20Forgetting&entry.906535625=Howard%20Chen%20and%20Noam%20Razin%20and%20Karthik%20Narasimhan%20and%20Danqi%20Chen&entry.1292438233=Adapting%20language%20models%20%28LMs%29%20to%20new%20tasks%20via%20post-training%20carries%20the%20risk%20of%20degrading%20existing%20capabilities%20--%20a%20phenomenon%20classically%20known%20as%20catastrophic%20forgetting.%20In%20this%20paper%2C%20toward%20identifying%20guidelines%20for%20mitigating%20this%20phenomenon%2C%20we%20systematically%20compare%20the%20forgetting%20patterns%20of%20two%20widely%20adopted%20post-training%20methods%3A%20supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29.%20Our%20experiments%20reveal%20a%20consistent%20trend%20across%20LM%20families%20%28Llama%2C%20Qwen%29%20and%20tasks%20%28instruction%20following%2C%20general%20knowledge%2C%20and%20arithmetic%20reasoning%29%3A%20RL%20leads%20to%20less%20forgetting%20than%20SFT%20while%20achieving%20comparable%20or%20higher%20target%20task%20performance.%20To%20investigate%20the%20cause%20for%20this%20difference%2C%20we%20consider%20a%20simplified%20setting%20in%20which%20the%20LM%20is%20modeled%20as%20a%20mixture%20of%20two%20distributions%2C%20one%20corresponding%20to%20prior%20knowledge%20and%20the%20other%20to%20the%20target%20task.%20We%20identify%20that%20the%20mode-seeking%20nature%20of%20RL%2C%20which%20stems%20from%20its%20use%20of%20on-policy%20data%2C%20enables%20keeping%20prior%20knowledge%20intact%20when%20learning%20the%20target%20task.%20We%20then%20verify%20this%20insight%20by%20demonstrating%20that%20the%20use%20on-policy%20data%20underlies%20the%20robustness%20of%20RL%20to%20forgetting%20in%20practical%20settings%2C%20as%20opposed%20to%20other%20algorithmic%20choices%20such%20as%20the%20KL%20regularization%20or%20advantage%20estimation.%20Lastly%2C%20as%20a%20practical%20implication%2C%20our%20results%20highlight%20the%20potential%20of%20mitigating%20forgetting%20using%20approximately%20on-policy%20data%2C%20which%20can%20be%20substantially%20more%20efficient%20to%20obtain%20than%20fully%20on-policy%20data.&entry.1838667208=http%3A//arxiv.org/abs/2510.18874v2&entry.124074799=Read"},
{"title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception", "author": "Tim Broedermannn and Christos Sakaridis and Luigi Piccinelli and Wim Abbeloos and Luc Van Gool", "abstract": "Robust semantic perception for autonomous vehicles relies on effectively combining multiple sensors with complementary strengths and weaknesses. State-of-the-art sensor fusion approaches to semantic perception often treat sensor data uniformly across the spatial extent of the input, which hinders performance when faced with challenging conditions. By contrast, we propose a novel depth-guided multimodal fusion method that upgrades condition-aware fusion by integrating depth information. Our network, DGFusion, poses multimodal segmentation as a multi-task problem, utilizing the lidar measurements, which are typically available in outdoor sensor suites, both as one of the model's inputs and as ground truth for learning depth. Our corresponding auxiliary depth head helps to learn depth-aware features, which are encoded into spatially varying local depth tokens that condition our attentive cross-modal fusion. Together with a global condition token, these local depth tokens dynamically adapt sensor fusion to the spatially varying reliability of each sensor across the scene, which largely depends on depth. In addition, we propose a robust loss for our depth, which is essential for learning from lidar inputs that are typically sparse and noisy in adverse conditions. Our method achieves state-of-the-art panoptic and semantic segmentation performance on the challenging MUSES and DeLiVER datasets. Code and models will be available at https://github.com/timbroed/DGFusion", "link": "http://arxiv.org/abs/2509.09828v2", "date": "2025-12-03", "relevancy": 2.3762, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6288}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGFusion%3A%20Depth-Guided%20Sensor%20Fusion%20for%20Robust%20Semantic%20Perception&body=Title%3A%20DGFusion%3A%20Depth-Guided%20Sensor%20Fusion%20for%20Robust%20Semantic%20Perception%0AAuthor%3A%20Tim%20Broedermannn%20and%20Christos%20Sakaridis%20and%20Luigi%20Piccinelli%20and%20Wim%20Abbeloos%20and%20Luc%20Van%20Gool%0AAbstract%3A%20Robust%20semantic%20perception%20for%20autonomous%20vehicles%20relies%20on%20effectively%20combining%20multiple%20sensors%20with%20complementary%20strengths%20and%20weaknesses.%20State-of-the-art%20sensor%20fusion%20approaches%20to%20semantic%20perception%20often%20treat%20sensor%20data%20uniformly%20across%20the%20spatial%20extent%20of%20the%20input%2C%20which%20hinders%20performance%20when%20faced%20with%20challenging%20conditions.%20By%20contrast%2C%20we%20propose%20a%20novel%20depth-guided%20multimodal%20fusion%20method%20that%20upgrades%20condition-aware%20fusion%20by%20integrating%20depth%20information.%20Our%20network%2C%20DGFusion%2C%20poses%20multimodal%20segmentation%20as%20a%20multi-task%20problem%2C%20utilizing%20the%20lidar%20measurements%2C%20which%20are%20typically%20available%20in%20outdoor%20sensor%20suites%2C%20both%20as%20one%20of%20the%20model%27s%20inputs%20and%20as%20ground%20truth%20for%20learning%20depth.%20Our%20corresponding%20auxiliary%20depth%20head%20helps%20to%20learn%20depth-aware%20features%2C%20which%20are%20encoded%20into%20spatially%20varying%20local%20depth%20tokens%20that%20condition%20our%20attentive%20cross-modal%20fusion.%20Together%20with%20a%20global%20condition%20token%2C%20these%20local%20depth%20tokens%20dynamically%20adapt%20sensor%20fusion%20to%20the%20spatially%20varying%20reliability%20of%20each%20sensor%20across%20the%20scene%2C%20which%20largely%20depends%20on%20depth.%20In%20addition%2C%20we%20propose%20a%20robust%20loss%20for%20our%20depth%2C%20which%20is%20essential%20for%20learning%20from%20lidar%20inputs%20that%20are%20typically%20sparse%20and%20noisy%20in%20adverse%20conditions.%20Our%20method%20achieves%20state-of-the-art%20panoptic%20and%20semantic%20segmentation%20performance%20on%20the%20challenging%20MUSES%20and%20DeLiVER%20datasets.%20Code%20and%20models%20will%20be%20available%20at%20https%3A//github.com/timbroed/DGFusion%0ALink%3A%20http%3A//arxiv.org/abs/2509.09828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGFusion%253A%2520Depth-Guided%2520Sensor%2520Fusion%2520for%2520Robust%2520Semantic%2520Perception%26entry.906535625%3DTim%2520Broedermannn%2520and%2520Christos%2520Sakaridis%2520and%2520Luigi%2520Piccinelli%2520and%2520Wim%2520Abbeloos%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3DRobust%2520semantic%2520perception%2520for%2520autonomous%2520vehicles%2520relies%2520on%2520effectively%2520combining%2520multiple%2520sensors%2520with%2520complementary%2520strengths%2520and%2520weaknesses.%2520State-of-the-art%2520sensor%2520fusion%2520approaches%2520to%2520semantic%2520perception%2520often%2520treat%2520sensor%2520data%2520uniformly%2520across%2520the%2520spatial%2520extent%2520of%2520the%2520input%252C%2520which%2520hinders%2520performance%2520when%2520faced%2520with%2520challenging%2520conditions.%2520By%2520contrast%252C%2520we%2520propose%2520a%2520novel%2520depth-guided%2520multimodal%2520fusion%2520method%2520that%2520upgrades%2520condition-aware%2520fusion%2520by%2520integrating%2520depth%2520information.%2520Our%2520network%252C%2520DGFusion%252C%2520poses%2520multimodal%2520segmentation%2520as%2520a%2520multi-task%2520problem%252C%2520utilizing%2520the%2520lidar%2520measurements%252C%2520which%2520are%2520typically%2520available%2520in%2520outdoor%2520sensor%2520suites%252C%2520both%2520as%2520one%2520of%2520the%2520model%2527s%2520inputs%2520and%2520as%2520ground%2520truth%2520for%2520learning%2520depth.%2520Our%2520corresponding%2520auxiliary%2520depth%2520head%2520helps%2520to%2520learn%2520depth-aware%2520features%252C%2520which%2520are%2520encoded%2520into%2520spatially%2520varying%2520local%2520depth%2520tokens%2520that%2520condition%2520our%2520attentive%2520cross-modal%2520fusion.%2520Together%2520with%2520a%2520global%2520condition%2520token%252C%2520these%2520local%2520depth%2520tokens%2520dynamically%2520adapt%2520sensor%2520fusion%2520to%2520the%2520spatially%2520varying%2520reliability%2520of%2520each%2520sensor%2520across%2520the%2520scene%252C%2520which%2520largely%2520depends%2520on%2520depth.%2520In%2520addition%252C%2520we%2520propose%2520a%2520robust%2520loss%2520for%2520our%2520depth%252C%2520which%2520is%2520essential%2520for%2520learning%2520from%2520lidar%2520inputs%2520that%2520are%2520typically%2520sparse%2520and%2520noisy%2520in%2520adverse%2520conditions.%2520Our%2520method%2520achieves%2520state-of-the-art%2520panoptic%2520and%2520semantic%2520segmentation%2520performance%2520on%2520the%2520challenging%2520MUSES%2520and%2520DeLiVER%2520datasets.%2520Code%2520and%2520models%2520will%2520be%2520available%2520at%2520https%253A//github.com/timbroed/DGFusion%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGFusion%3A%20Depth-Guided%20Sensor%20Fusion%20for%20Robust%20Semantic%20Perception&entry.906535625=Tim%20Broedermannn%20and%20Christos%20Sakaridis%20and%20Luigi%20Piccinelli%20and%20Wim%20Abbeloos%20and%20Luc%20Van%20Gool&entry.1292438233=Robust%20semantic%20perception%20for%20autonomous%20vehicles%20relies%20on%20effectively%20combining%20multiple%20sensors%20with%20complementary%20strengths%20and%20weaknesses.%20State-of-the-art%20sensor%20fusion%20approaches%20to%20semantic%20perception%20often%20treat%20sensor%20data%20uniformly%20across%20the%20spatial%20extent%20of%20the%20input%2C%20which%20hinders%20performance%20when%20faced%20with%20challenging%20conditions.%20By%20contrast%2C%20we%20propose%20a%20novel%20depth-guided%20multimodal%20fusion%20method%20that%20upgrades%20condition-aware%20fusion%20by%20integrating%20depth%20information.%20Our%20network%2C%20DGFusion%2C%20poses%20multimodal%20segmentation%20as%20a%20multi-task%20problem%2C%20utilizing%20the%20lidar%20measurements%2C%20which%20are%20typically%20available%20in%20outdoor%20sensor%20suites%2C%20both%20as%20one%20of%20the%20model%27s%20inputs%20and%20as%20ground%20truth%20for%20learning%20depth.%20Our%20corresponding%20auxiliary%20depth%20head%20helps%20to%20learn%20depth-aware%20features%2C%20which%20are%20encoded%20into%20spatially%20varying%20local%20depth%20tokens%20that%20condition%20our%20attentive%20cross-modal%20fusion.%20Together%20with%20a%20global%20condition%20token%2C%20these%20local%20depth%20tokens%20dynamically%20adapt%20sensor%20fusion%20to%20the%20spatially%20varying%20reliability%20of%20each%20sensor%20across%20the%20scene%2C%20which%20largely%20depends%20on%20depth.%20In%20addition%2C%20we%20propose%20a%20robust%20loss%20for%20our%20depth%2C%20which%20is%20essential%20for%20learning%20from%20lidar%20inputs%20that%20are%20typically%20sparse%20and%20noisy%20in%20adverse%20conditions.%20Our%20method%20achieves%20state-of-the-art%20panoptic%20and%20semantic%20segmentation%20performance%20on%20the%20challenging%20MUSES%20and%20DeLiVER%20datasets.%20Code%20and%20models%20will%20be%20available%20at%20https%3A//github.com/timbroed/DGFusion&entry.1838667208=http%3A//arxiv.org/abs/2509.09828v2&entry.124074799=Read"},
{"title": "Cataloguing Hugging Face Models to Software Engineering Activities: Automation and Findings", "author": "Alexandra Gonz\u00e1lez and Xavier Franch and David Lo and Silverio Mart\u00ednez-Fern\u00e1ndez", "abstract": "Context: Open-source Pre-Trained Models (PTMs) provide extensive resources for various Machine Learning (ML) tasks, yet these resources lack a classification tailored to Software Engineering (SE) needs to support the reliable identification and reuse of models for SE. Objective: To address this gap, we derive a taxonomy encompassing 147 SE tasks and apply an SE-oriented classification to PTMs in a popular open-source ML repository, Hugging Face (HF). Method: Our repository mining study followed a five-phase pipeline: (i) identification SE tasks from the literature; (ii) collection of PTM data from the HF API, including model card descriptions and metadata, and the abstracts of the associated arXiv papers; (iii) text processing to ensure consistency; (iv) a two-phase validation of SE relevance, involving humans and LLM assistance, supported by five pilot studies with human annotators and a generalization test; (v) and data analysis. This process yielded a curated catalogue of 2,205 SE PTMs. Results: We find that most SE PTMs target code generation and coding, emphasizing implementation over early or late development stages. In terms of ML tasks, text generation dominates within SE PTMs. Notably, the number of SE PTMs has increased markedly since 2023 Q2, while evaluation remains limited: only 9.6% report benchmark results, mostly scoring below 50%. Conclusions: Our catalogue reveals documentation and transparency gaps, highlights imbalances across SDLC phases, and provides a foundation for automated SE scenarios, such as the sampling and selection of suitable PTMs.", "link": "http://arxiv.org/abs/2506.03013v2", "date": "2025-12-03", "relevancy": 2.3686, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4803}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cataloguing%20Hugging%20Face%20Models%20to%20Software%20Engineering%20Activities%3A%20Automation%20and%20Findings&body=Title%3A%20Cataloguing%20Hugging%20Face%20Models%20to%20Software%20Engineering%20Activities%3A%20Automation%20and%20Findings%0AAuthor%3A%20Alexandra%20Gonz%C3%A1lez%20and%20Xavier%20Franch%20and%20David%20Lo%20and%20Silverio%20Mart%C3%ADnez-Fern%C3%A1ndez%0AAbstract%3A%20Context%3A%20Open-source%20Pre-Trained%20Models%20%28PTMs%29%20provide%20extensive%20resources%20for%20various%20Machine%20Learning%20%28ML%29%20tasks%2C%20yet%20these%20resources%20lack%20a%20classification%20tailored%20to%20Software%20Engineering%20%28SE%29%20needs%20to%20support%20the%20reliable%20identification%20and%20reuse%20of%20models%20for%20SE.%20Objective%3A%20To%20address%20this%20gap%2C%20we%20derive%20a%20taxonomy%20encompassing%20147%20SE%20tasks%20and%20apply%20an%20SE-oriented%20classification%20to%20PTMs%20in%20a%20popular%20open-source%20ML%20repository%2C%20Hugging%20Face%20%28HF%29.%20Method%3A%20Our%20repository%20mining%20study%20followed%20a%20five-phase%20pipeline%3A%20%28i%29%20identification%20SE%20tasks%20from%20the%20literature%3B%20%28ii%29%20collection%20of%20PTM%20data%20from%20the%20HF%20API%2C%20including%20model%20card%20descriptions%20and%20metadata%2C%20and%20the%20abstracts%20of%20the%20associated%20arXiv%20papers%3B%20%28iii%29%20text%20processing%20to%20ensure%20consistency%3B%20%28iv%29%20a%20two-phase%20validation%20of%20SE%20relevance%2C%20involving%20humans%20and%20LLM%20assistance%2C%20supported%20by%20five%20pilot%20studies%20with%20human%20annotators%20and%20a%20generalization%20test%3B%20%28v%29%20and%20data%20analysis.%20This%20process%20yielded%20a%20curated%20catalogue%20of%202%2C205%20SE%20PTMs.%20Results%3A%20We%20find%20that%20most%20SE%20PTMs%20target%20code%20generation%20and%20coding%2C%20emphasizing%20implementation%20over%20early%20or%20late%20development%20stages.%20In%20terms%20of%20ML%20tasks%2C%20text%20generation%20dominates%20within%20SE%20PTMs.%20Notably%2C%20the%20number%20of%20SE%20PTMs%20has%20increased%20markedly%20since%202023%20Q2%2C%20while%20evaluation%20remains%20limited%3A%20only%209.6%25%20report%20benchmark%20results%2C%20mostly%20scoring%20below%2050%25.%20Conclusions%3A%20Our%20catalogue%20reveals%20documentation%20and%20transparency%20gaps%2C%20highlights%20imbalances%20across%20SDLC%20phases%2C%20and%20provides%20a%20foundation%20for%20automated%20SE%20scenarios%2C%20such%20as%20the%20sampling%20and%20selection%20of%20suitable%20PTMs.%0ALink%3A%20http%3A//arxiv.org/abs/2506.03013v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCataloguing%2520Hugging%2520Face%2520Models%2520to%2520Software%2520Engineering%2520Activities%253A%2520Automation%2520and%2520Findings%26entry.906535625%3DAlexandra%2520Gonz%25C3%25A1lez%2520and%2520Xavier%2520Franch%2520and%2520David%2520Lo%2520and%2520Silverio%2520Mart%25C3%25ADnez-Fern%25C3%25A1ndez%26entry.1292438233%3DContext%253A%2520Open-source%2520Pre-Trained%2520Models%2520%2528PTMs%2529%2520provide%2520extensive%2520resources%2520for%2520various%2520Machine%2520Learning%2520%2528ML%2529%2520tasks%252C%2520yet%2520these%2520resources%2520lack%2520a%2520classification%2520tailored%2520to%2520Software%2520Engineering%2520%2528SE%2529%2520needs%2520to%2520support%2520the%2520reliable%2520identification%2520and%2520reuse%2520of%2520models%2520for%2520SE.%2520Objective%253A%2520To%2520address%2520this%2520gap%252C%2520we%2520derive%2520a%2520taxonomy%2520encompassing%2520147%2520SE%2520tasks%2520and%2520apply%2520an%2520SE-oriented%2520classification%2520to%2520PTMs%2520in%2520a%2520popular%2520open-source%2520ML%2520repository%252C%2520Hugging%2520Face%2520%2528HF%2529.%2520Method%253A%2520Our%2520repository%2520mining%2520study%2520followed%2520a%2520five-phase%2520pipeline%253A%2520%2528i%2529%2520identification%2520SE%2520tasks%2520from%2520the%2520literature%253B%2520%2528ii%2529%2520collection%2520of%2520PTM%2520data%2520from%2520the%2520HF%2520API%252C%2520including%2520model%2520card%2520descriptions%2520and%2520metadata%252C%2520and%2520the%2520abstracts%2520of%2520the%2520associated%2520arXiv%2520papers%253B%2520%2528iii%2529%2520text%2520processing%2520to%2520ensure%2520consistency%253B%2520%2528iv%2529%2520a%2520two-phase%2520validation%2520of%2520SE%2520relevance%252C%2520involving%2520humans%2520and%2520LLM%2520assistance%252C%2520supported%2520by%2520five%2520pilot%2520studies%2520with%2520human%2520annotators%2520and%2520a%2520generalization%2520test%253B%2520%2528v%2529%2520and%2520data%2520analysis.%2520This%2520process%2520yielded%2520a%2520curated%2520catalogue%2520of%25202%252C205%2520SE%2520PTMs.%2520Results%253A%2520We%2520find%2520that%2520most%2520SE%2520PTMs%2520target%2520code%2520generation%2520and%2520coding%252C%2520emphasizing%2520implementation%2520over%2520early%2520or%2520late%2520development%2520stages.%2520In%2520terms%2520of%2520ML%2520tasks%252C%2520text%2520generation%2520dominates%2520within%2520SE%2520PTMs.%2520Notably%252C%2520the%2520number%2520of%2520SE%2520PTMs%2520has%2520increased%2520markedly%2520since%25202023%2520Q2%252C%2520while%2520evaluation%2520remains%2520limited%253A%2520only%25209.6%2525%2520report%2520benchmark%2520results%252C%2520mostly%2520scoring%2520below%252050%2525.%2520Conclusions%253A%2520Our%2520catalogue%2520reveals%2520documentation%2520and%2520transparency%2520gaps%252C%2520highlights%2520imbalances%2520across%2520SDLC%2520phases%252C%2520and%2520provides%2520a%2520foundation%2520for%2520automated%2520SE%2520scenarios%252C%2520such%2520as%2520the%2520sampling%2520and%2520selection%2520of%2520suitable%2520PTMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03013v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cataloguing%20Hugging%20Face%20Models%20to%20Software%20Engineering%20Activities%3A%20Automation%20and%20Findings&entry.906535625=Alexandra%20Gonz%C3%A1lez%20and%20Xavier%20Franch%20and%20David%20Lo%20and%20Silverio%20Mart%C3%ADnez-Fern%C3%A1ndez&entry.1292438233=Context%3A%20Open-source%20Pre-Trained%20Models%20%28PTMs%29%20provide%20extensive%20resources%20for%20various%20Machine%20Learning%20%28ML%29%20tasks%2C%20yet%20these%20resources%20lack%20a%20classification%20tailored%20to%20Software%20Engineering%20%28SE%29%20needs%20to%20support%20the%20reliable%20identification%20and%20reuse%20of%20models%20for%20SE.%20Objective%3A%20To%20address%20this%20gap%2C%20we%20derive%20a%20taxonomy%20encompassing%20147%20SE%20tasks%20and%20apply%20an%20SE-oriented%20classification%20to%20PTMs%20in%20a%20popular%20open-source%20ML%20repository%2C%20Hugging%20Face%20%28HF%29.%20Method%3A%20Our%20repository%20mining%20study%20followed%20a%20five-phase%20pipeline%3A%20%28i%29%20identification%20SE%20tasks%20from%20the%20literature%3B%20%28ii%29%20collection%20of%20PTM%20data%20from%20the%20HF%20API%2C%20including%20model%20card%20descriptions%20and%20metadata%2C%20and%20the%20abstracts%20of%20the%20associated%20arXiv%20papers%3B%20%28iii%29%20text%20processing%20to%20ensure%20consistency%3B%20%28iv%29%20a%20two-phase%20validation%20of%20SE%20relevance%2C%20involving%20humans%20and%20LLM%20assistance%2C%20supported%20by%20five%20pilot%20studies%20with%20human%20annotators%20and%20a%20generalization%20test%3B%20%28v%29%20and%20data%20analysis.%20This%20process%20yielded%20a%20curated%20catalogue%20of%202%2C205%20SE%20PTMs.%20Results%3A%20We%20find%20that%20most%20SE%20PTMs%20target%20code%20generation%20and%20coding%2C%20emphasizing%20implementation%20over%20early%20or%20late%20development%20stages.%20In%20terms%20of%20ML%20tasks%2C%20text%20generation%20dominates%20within%20SE%20PTMs.%20Notably%2C%20the%20number%20of%20SE%20PTMs%20has%20increased%20markedly%20since%202023%20Q2%2C%20while%20evaluation%20remains%20limited%3A%20only%209.6%25%20report%20benchmark%20results%2C%20mostly%20scoring%20below%2050%25.%20Conclusions%3A%20Our%20catalogue%20reveals%20documentation%20and%20transparency%20gaps%2C%20highlights%20imbalances%20across%20SDLC%20phases%2C%20and%20provides%20a%20foundation%20for%20automated%20SE%20scenarios%2C%20such%20as%20the%20sampling%20and%20selection%20of%20suitable%20PTMs.&entry.1838667208=http%3A//arxiv.org/abs/2506.03013v2&entry.124074799=Read"},
{"title": "DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction", "author": "Kaichen Zhang and Tianxiang Sheng and Xuanming Shi", "abstract": "This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images. The\n  method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and\n  matching. DINO is employed to retrieve semantically relevant image pairs in large collections, while\n  rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue. Experiments on the Kaggle Image Matching Challenge 2025 demonstrate consistent improve ments in mean Average Accuracy (mAA), achieving a Silver Award (47th of 943 teams). The results\n  confirm that combining self-supervised global descriptors with rotation-enhanced local matching offers\n  a robust and scalable solution for large-scale 3D reconstruction.", "link": "http://arxiv.org/abs/2512.03715v1", "date": "2025-12-03", "relevancy": 2.3624, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6523}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5518}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO-RotateMatch%3A%20A%20Rotation-Aware%20Deep%20Framework%20for%20Robust%20Image%20Matching%20in%20Large-Scale%203D%20Reconstruction&body=Title%3A%20DINO-RotateMatch%3A%20A%20Rotation-Aware%20Deep%20Framework%20for%20Robust%20Image%20Matching%20in%20Large-Scale%203D%20Reconstruction%0AAuthor%3A%20Kaichen%20Zhang%20and%20Tianxiang%20Sheng%20and%20Xuanming%20Shi%0AAbstract%3A%20This%20paper%20presents%20DINO-RotateMatch%2C%20a%20deep-learning%20framework%20designed%20to%20address%20the%20chal%20lenges%20of%20image%20matching%20in%20large-scale%203D%20reconstruction%20from%20unstructured%20Internet%20images.%20The%0A%20%20method%20integrates%20a%20dataset-adaptive%20image%20pairing%20strategy%20with%20rotation-aware%20keypoint%20extraction%20and%0A%20%20matching.%20DINO%20is%20employed%20to%20retrieve%20semantically%20relevant%20image%20pairs%20in%20large%20collections%2C%20while%0A%20%20rotation-based%20augmentation%20captures%20orientation-dependent%20local%20features%20using%20ALIKED%20and%20Light%20Glue.%20Experiments%20on%20the%20Kaggle%20Image%20Matching%20Challenge%202025%20demonstrate%20consistent%20improve%20ments%20in%20mean%20Average%20Accuracy%20%28mAA%29%2C%20achieving%20a%20Silver%20Award%20%2847th%20of%20943%20teams%29.%20The%20results%0A%20%20confirm%20that%20combining%20self-supervised%20global%20descriptors%20with%20rotation-enhanced%20local%20matching%20offers%0A%20%20a%20robust%20and%20scalable%20solution%20for%20large-scale%203D%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO-RotateMatch%253A%2520A%2520Rotation-Aware%2520Deep%2520Framework%2520for%2520Robust%2520Image%2520Matching%2520in%2520Large-Scale%25203D%2520Reconstruction%26entry.906535625%3DKaichen%2520Zhang%2520and%2520Tianxiang%2520Sheng%2520and%2520Xuanming%2520Shi%26entry.1292438233%3DThis%2520paper%2520presents%2520DINO-RotateMatch%252C%2520a%2520deep-learning%2520framework%2520designed%2520to%2520address%2520the%2520chal%2520lenges%2520of%2520image%2520matching%2520in%2520large-scale%25203D%2520reconstruction%2520from%2520unstructured%2520Internet%2520images.%2520The%250A%2520%2520method%2520integrates%2520a%2520dataset-adaptive%2520image%2520pairing%2520strategy%2520with%2520rotation-aware%2520keypoint%2520extraction%2520and%250A%2520%2520matching.%2520DINO%2520is%2520employed%2520to%2520retrieve%2520semantically%2520relevant%2520image%2520pairs%2520in%2520large%2520collections%252C%2520while%250A%2520%2520rotation-based%2520augmentation%2520captures%2520orientation-dependent%2520local%2520features%2520using%2520ALIKED%2520and%2520Light%2520Glue.%2520Experiments%2520on%2520the%2520Kaggle%2520Image%2520Matching%2520Challenge%25202025%2520demonstrate%2520consistent%2520improve%2520ments%2520in%2520mean%2520Average%2520Accuracy%2520%2528mAA%2529%252C%2520achieving%2520a%2520Silver%2520Award%2520%252847th%2520of%2520943%2520teams%2529.%2520The%2520results%250A%2520%2520confirm%2520that%2520combining%2520self-supervised%2520global%2520descriptors%2520with%2520rotation-enhanced%2520local%2520matching%2520offers%250A%2520%2520a%2520robust%2520and%2520scalable%2520solution%2520for%2520large-scale%25203D%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-RotateMatch%3A%20A%20Rotation-Aware%20Deep%20Framework%20for%20Robust%20Image%20Matching%20in%20Large-Scale%203D%20Reconstruction&entry.906535625=Kaichen%20Zhang%20and%20Tianxiang%20Sheng%20and%20Xuanming%20Shi&entry.1292438233=This%20paper%20presents%20DINO-RotateMatch%2C%20a%20deep-learning%20framework%20designed%20to%20address%20the%20chal%20lenges%20of%20image%20matching%20in%20large-scale%203D%20reconstruction%20from%20unstructured%20Internet%20images.%20The%0A%20%20method%20integrates%20a%20dataset-adaptive%20image%20pairing%20strategy%20with%20rotation-aware%20keypoint%20extraction%20and%0A%20%20matching.%20DINO%20is%20employed%20to%20retrieve%20semantically%20relevant%20image%20pairs%20in%20large%20collections%2C%20while%0A%20%20rotation-based%20augmentation%20captures%20orientation-dependent%20local%20features%20using%20ALIKED%20and%20Light%20Glue.%20Experiments%20on%20the%20Kaggle%20Image%20Matching%20Challenge%202025%20demonstrate%20consistent%20improve%20ments%20in%20mean%20Average%20Accuracy%20%28mAA%29%2C%20achieving%20a%20Silver%20Award%20%2847th%20of%20943%20teams%29.%20The%20results%0A%20%20confirm%20that%20combining%20self-supervised%20global%20descriptors%20with%20rotation-enhanced%20local%20matching%20offers%0A%20%20a%20robust%20and%20scalable%20solution%20for%20large-scale%203D%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2512.03715v1&entry.124074799=Read"},
{"title": "Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models", "author": "Haidong Kang and Wei Wu and Hanling Wang", "abstract": "Few-shot class incremental learning (FSCIL) is a more realistic and challenging paradigm in continual learning to incrementally learn unseen classes and overcome catastrophic forgetting on base classes with only a few training examples. Previous efforts have primarily centered around studying more effective FSCIL approaches. By contrast, less attention was devoted to thinking the security issues in contributing to FSCIL. This paper aims to provide a holistic study of the impact of attacks on FSCIL. We first derive insights by systematically exploring how human expert-designed attack methods (i.e., PGD, FGSM) affect FSCIL. We find that those methods either fail to attack base classes, or suffer from huge labor costs due to relying on huge expert knowledge. This highlights the need to craft a specialized attack method for FSCIL. Grounded in these insights, in this paper, we propose a simple yet effective ACraft method to automatically steer and discover optimal attack methods targeted at FSCIL by leveraging Large Language Models (LLMs) without human experts. Moreover, to improve the reasoning between LLMs and FSCIL, we introduce a novel Proximal Policy Optimization (PPO) based reinforcement learning to optimize learning, making LLMs generate better attack methods in the next generation by establishing positive feedback. Experiments on mainstream benchmarks show that our ACraft significantly degrades the performance of state-of-the-art FSCIL methods and dramatically beyond human expert-designed attack methods while maintaining the lowest costs of attack.", "link": "http://arxiv.org/abs/2512.03882v1", "date": "2025-12-03", "relevancy": 2.3621, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4871}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4701}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Attack%20Discovery%20for%20Few-Shot%20Class-Incremental%20Learning%20via%20Large%20Language%20Models&body=Title%3A%20Automatic%20Attack%20Discovery%20for%20Few-Shot%20Class-Incremental%20Learning%20via%20Large%20Language%20Models%0AAuthor%3A%20Haidong%20Kang%20and%20Wei%20Wu%20and%20Hanling%20Wang%0AAbstract%3A%20Few-shot%20class%20incremental%20learning%20%28FSCIL%29%20is%20a%20more%20realistic%20and%20challenging%20paradigm%20in%20continual%20learning%20to%20incrementally%20learn%20unseen%20classes%20and%20overcome%20catastrophic%20forgetting%20on%20base%20classes%20with%20only%20a%20few%20training%20examples.%20Previous%20efforts%20have%20primarily%20centered%20around%20studying%20more%20effective%20FSCIL%20approaches.%20By%20contrast%2C%20less%20attention%20was%20devoted%20to%20thinking%20the%20security%20issues%20in%20contributing%20to%20FSCIL.%20This%20paper%20aims%20to%20provide%20a%20holistic%20study%20of%20the%20impact%20of%20attacks%20on%20FSCIL.%20We%20first%20derive%20insights%20by%20systematically%20exploring%20how%20human%20expert-designed%20attack%20methods%20%28i.e.%2C%20PGD%2C%20FGSM%29%20affect%20FSCIL.%20We%20find%20that%20those%20methods%20either%20fail%20to%20attack%20base%20classes%2C%20or%20suffer%20from%20huge%20labor%20costs%20due%20to%20relying%20on%20huge%20expert%20knowledge.%20This%20highlights%20the%20need%20to%20craft%20a%20specialized%20attack%20method%20for%20FSCIL.%20Grounded%20in%20these%20insights%2C%20in%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20effective%20ACraft%20method%20to%20automatically%20steer%20and%20discover%20optimal%20attack%20methods%20targeted%20at%20FSCIL%20by%20leveraging%20Large%20Language%20Models%20%28LLMs%29%20without%20human%20experts.%20Moreover%2C%20to%20improve%20the%20reasoning%20between%20LLMs%20and%20FSCIL%2C%20we%20introduce%20a%20novel%20Proximal%20Policy%20Optimization%20%28PPO%29%20based%20reinforcement%20learning%20to%20optimize%20learning%2C%20making%20LLMs%20generate%20better%20attack%20methods%20in%20the%20next%20generation%20by%20establishing%20positive%20feedback.%20Experiments%20on%20mainstream%20benchmarks%20show%20that%20our%20ACraft%20significantly%20degrades%20the%20performance%20of%20state-of-the-art%20FSCIL%20methods%20and%20dramatically%20beyond%20human%20expert-designed%20attack%20methods%20while%20maintaining%20the%20lowest%20costs%20of%20attack.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Attack%2520Discovery%2520for%2520Few-Shot%2520Class-Incremental%2520Learning%2520via%2520Large%2520Language%2520Models%26entry.906535625%3DHaidong%2520Kang%2520and%2520Wei%2520Wu%2520and%2520Hanling%2520Wang%26entry.1292438233%3DFew-shot%2520class%2520incremental%2520learning%2520%2528FSCIL%2529%2520is%2520a%2520more%2520realistic%2520and%2520challenging%2520paradigm%2520in%2520continual%2520learning%2520to%2520incrementally%2520learn%2520unseen%2520classes%2520and%2520overcome%2520catastrophic%2520forgetting%2520on%2520base%2520classes%2520with%2520only%2520a%2520few%2520training%2520examples.%2520Previous%2520efforts%2520have%2520primarily%2520centered%2520around%2520studying%2520more%2520effective%2520FSCIL%2520approaches.%2520By%2520contrast%252C%2520less%2520attention%2520was%2520devoted%2520to%2520thinking%2520the%2520security%2520issues%2520in%2520contributing%2520to%2520FSCIL.%2520This%2520paper%2520aims%2520to%2520provide%2520a%2520holistic%2520study%2520of%2520the%2520impact%2520of%2520attacks%2520on%2520FSCIL.%2520We%2520first%2520derive%2520insights%2520by%2520systematically%2520exploring%2520how%2520human%2520expert-designed%2520attack%2520methods%2520%2528i.e.%252C%2520PGD%252C%2520FGSM%2529%2520affect%2520FSCIL.%2520We%2520find%2520that%2520those%2520methods%2520either%2520fail%2520to%2520attack%2520base%2520classes%252C%2520or%2520suffer%2520from%2520huge%2520labor%2520costs%2520due%2520to%2520relying%2520on%2520huge%2520expert%2520knowledge.%2520This%2520highlights%2520the%2520need%2520to%2520craft%2520a%2520specialized%2520attack%2520method%2520for%2520FSCIL.%2520Grounded%2520in%2520these%2520insights%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520ACraft%2520method%2520to%2520automatically%2520steer%2520and%2520discover%2520optimal%2520attack%2520methods%2520targeted%2520at%2520FSCIL%2520by%2520leveraging%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520without%2520human%2520experts.%2520Moreover%252C%2520to%2520improve%2520the%2520reasoning%2520between%2520LLMs%2520and%2520FSCIL%252C%2520we%2520introduce%2520a%2520novel%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520based%2520reinforcement%2520learning%2520to%2520optimize%2520learning%252C%2520making%2520LLMs%2520generate%2520better%2520attack%2520methods%2520in%2520the%2520next%2520generation%2520by%2520establishing%2520positive%2520feedback.%2520Experiments%2520on%2520mainstream%2520benchmarks%2520show%2520that%2520our%2520ACraft%2520significantly%2520degrades%2520the%2520performance%2520of%2520state-of-the-art%2520FSCIL%2520methods%2520and%2520dramatically%2520beyond%2520human%2520expert-designed%2520attack%2520methods%2520while%2520maintaining%2520the%2520lowest%2520costs%2520of%2520attack.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Attack%20Discovery%20for%20Few-Shot%20Class-Incremental%20Learning%20via%20Large%20Language%20Models&entry.906535625=Haidong%20Kang%20and%20Wei%20Wu%20and%20Hanling%20Wang&entry.1292438233=Few-shot%20class%20incremental%20learning%20%28FSCIL%29%20is%20a%20more%20realistic%20and%20challenging%20paradigm%20in%20continual%20learning%20to%20incrementally%20learn%20unseen%20classes%20and%20overcome%20catastrophic%20forgetting%20on%20base%20classes%20with%20only%20a%20few%20training%20examples.%20Previous%20efforts%20have%20primarily%20centered%20around%20studying%20more%20effective%20FSCIL%20approaches.%20By%20contrast%2C%20less%20attention%20was%20devoted%20to%20thinking%20the%20security%20issues%20in%20contributing%20to%20FSCIL.%20This%20paper%20aims%20to%20provide%20a%20holistic%20study%20of%20the%20impact%20of%20attacks%20on%20FSCIL.%20We%20first%20derive%20insights%20by%20systematically%20exploring%20how%20human%20expert-designed%20attack%20methods%20%28i.e.%2C%20PGD%2C%20FGSM%29%20affect%20FSCIL.%20We%20find%20that%20those%20methods%20either%20fail%20to%20attack%20base%20classes%2C%20or%20suffer%20from%20huge%20labor%20costs%20due%20to%20relying%20on%20huge%20expert%20knowledge.%20This%20highlights%20the%20need%20to%20craft%20a%20specialized%20attack%20method%20for%20FSCIL.%20Grounded%20in%20these%20insights%2C%20in%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20effective%20ACraft%20method%20to%20automatically%20steer%20and%20discover%20optimal%20attack%20methods%20targeted%20at%20FSCIL%20by%20leveraging%20Large%20Language%20Models%20%28LLMs%29%20without%20human%20experts.%20Moreover%2C%20to%20improve%20the%20reasoning%20between%20LLMs%20and%20FSCIL%2C%20we%20introduce%20a%20novel%20Proximal%20Policy%20Optimization%20%28PPO%29%20based%20reinforcement%20learning%20to%20optimize%20learning%2C%20making%20LLMs%20generate%20better%20attack%20methods%20in%20the%20next%20generation%20by%20establishing%20positive%20feedback.%20Experiments%20on%20mainstream%20benchmarks%20show%20that%20our%20ACraft%20significantly%20degrades%20the%20performance%20of%20state-of-the-art%20FSCIL%20methods%20and%20dramatically%20beyond%20human%20expert-designed%20attack%20methods%20while%20maintaining%20the%20lowest%20costs%20of%20attack.&entry.1838667208=http%3A//arxiv.org/abs/2512.03882v1&entry.124074799=Read"},
{"title": "Research on Brain Tumor Classification Method Based on Improved ResNet34 Network", "author": "Yufeng Li and Wenchao Zhao and Bo Dang and Weimin Wang", "abstract": "Previously, image interpretation in radiology relied heavily on manual methods. However, manual classification of brain tumor medical images is time-consuming and labor-intensive. Even with shallow convolutional neural network models, the accuracy is not ideal. To improve the efficiency and accuracy of brain tumor image classification, this paper proposes a brain tumor classification model based on an improved ResNet34 network. This model uses the ResNet34 residual network as the backbone network and incorporates multi-scale feature extraction. It uses a multi-scale input module as the first layer of the ResNet34 network and an Inception v2 module as the residual downsampling layer. Furthermore, a channel attention mechanism module assigns different weights to different channels of the image from a channel domain perspective, obtaining more important feature information. The results after a five-fold crossover experiment show that the average classification accuracy of the improved network model is approximately 98.8%, which is not only 1% higher than ResNet34, but also only 80% of the number of parameters of the original model. Therefore, the improved network model not only improves accuracy but also reduces clutter, achieving a classification effect with fewer parameters and higher accuracy.", "link": "http://arxiv.org/abs/2512.03751v1", "date": "2025-12-03", "relevancy": 2.35, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5012}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.463}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20on%20Brain%20Tumor%20Classification%20Method%20Based%20on%20Improved%20ResNet34%20Network&body=Title%3A%20Research%20on%20Brain%20Tumor%20Classification%20Method%20Based%20on%20Improved%20ResNet34%20Network%0AAuthor%3A%20Yufeng%20Li%20and%20Wenchao%20Zhao%20and%20Bo%20Dang%20and%20Weimin%20Wang%0AAbstract%3A%20Previously%2C%20image%20interpretation%20in%20radiology%20relied%20heavily%20on%20manual%20methods.%20However%2C%20manual%20classification%20of%20brain%20tumor%20medical%20images%20is%20time-consuming%20and%20labor-intensive.%20Even%20with%20shallow%20convolutional%20neural%20network%20models%2C%20the%20accuracy%20is%20not%20ideal.%20To%20improve%20the%20efficiency%20and%20accuracy%20of%20brain%20tumor%20image%20classification%2C%20this%20paper%20proposes%20a%20brain%20tumor%20classification%20model%20based%20on%20an%20improved%20ResNet34%20network.%20This%20model%20uses%20the%20ResNet34%20residual%20network%20as%20the%20backbone%20network%20and%20incorporates%20multi-scale%20feature%20extraction.%20It%20uses%20a%20multi-scale%20input%20module%20as%20the%20first%20layer%20of%20the%20ResNet34%20network%20and%20an%20Inception%20v2%20module%20as%20the%20residual%20downsampling%20layer.%20Furthermore%2C%20a%20channel%20attention%20mechanism%20module%20assigns%20different%20weights%20to%20different%20channels%20of%20the%20image%20from%20a%20channel%20domain%20perspective%2C%20obtaining%20more%20important%20feature%20information.%20The%20results%20after%20a%20five-fold%20crossover%20experiment%20show%20that%20the%20average%20classification%20accuracy%20of%20the%20improved%20network%20model%20is%20approximately%2098.8%25%2C%20which%20is%20not%20only%201%25%20higher%20than%20ResNet34%2C%20but%20also%20only%2080%25%20of%20the%20number%20of%20parameters%20of%20the%20original%20model.%20Therefore%2C%20the%20improved%20network%20model%20not%20only%20improves%20accuracy%20but%20also%20reduces%20clutter%2C%20achieving%20a%20classification%20effect%20with%20fewer%20parameters%20and%20higher%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520on%2520Brain%2520Tumor%2520Classification%2520Method%2520Based%2520on%2520Improved%2520ResNet34%2520Network%26entry.906535625%3DYufeng%2520Li%2520and%2520Wenchao%2520Zhao%2520and%2520Bo%2520Dang%2520and%2520Weimin%2520Wang%26entry.1292438233%3DPreviously%252C%2520image%2520interpretation%2520in%2520radiology%2520relied%2520heavily%2520on%2520manual%2520methods.%2520However%252C%2520manual%2520classification%2520of%2520brain%2520tumor%2520medical%2520images%2520is%2520time-consuming%2520and%2520labor-intensive.%2520Even%2520with%2520shallow%2520convolutional%2520neural%2520network%2520models%252C%2520the%2520accuracy%2520is%2520not%2520ideal.%2520To%2520improve%2520the%2520efficiency%2520and%2520accuracy%2520of%2520brain%2520tumor%2520image%2520classification%252C%2520this%2520paper%2520proposes%2520a%2520brain%2520tumor%2520classification%2520model%2520based%2520on%2520an%2520improved%2520ResNet34%2520network.%2520This%2520model%2520uses%2520the%2520ResNet34%2520residual%2520network%2520as%2520the%2520backbone%2520network%2520and%2520incorporates%2520multi-scale%2520feature%2520extraction.%2520It%2520uses%2520a%2520multi-scale%2520input%2520module%2520as%2520the%2520first%2520layer%2520of%2520the%2520ResNet34%2520network%2520and%2520an%2520Inception%2520v2%2520module%2520as%2520the%2520residual%2520downsampling%2520layer.%2520Furthermore%252C%2520a%2520channel%2520attention%2520mechanism%2520module%2520assigns%2520different%2520weights%2520to%2520different%2520channels%2520of%2520the%2520image%2520from%2520a%2520channel%2520domain%2520perspective%252C%2520obtaining%2520more%2520important%2520feature%2520information.%2520The%2520results%2520after%2520a%2520five-fold%2520crossover%2520experiment%2520show%2520that%2520the%2520average%2520classification%2520accuracy%2520of%2520the%2520improved%2520network%2520model%2520is%2520approximately%252098.8%2525%252C%2520which%2520is%2520not%2520only%25201%2525%2520higher%2520than%2520ResNet34%252C%2520but%2520also%2520only%252080%2525%2520of%2520the%2520number%2520of%2520parameters%2520of%2520the%2520original%2520model.%2520Therefore%252C%2520the%2520improved%2520network%2520model%2520not%2520only%2520improves%2520accuracy%2520but%2520also%2520reduces%2520clutter%252C%2520achieving%2520a%2520classification%2520effect%2520with%2520fewer%2520parameters%2520and%2520higher%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20on%20Brain%20Tumor%20Classification%20Method%20Based%20on%20Improved%20ResNet34%20Network&entry.906535625=Yufeng%20Li%20and%20Wenchao%20Zhao%20and%20Bo%20Dang%20and%20Weimin%20Wang&entry.1292438233=Previously%2C%20image%20interpretation%20in%20radiology%20relied%20heavily%20on%20manual%20methods.%20However%2C%20manual%20classification%20of%20brain%20tumor%20medical%20images%20is%20time-consuming%20and%20labor-intensive.%20Even%20with%20shallow%20convolutional%20neural%20network%20models%2C%20the%20accuracy%20is%20not%20ideal.%20To%20improve%20the%20efficiency%20and%20accuracy%20of%20brain%20tumor%20image%20classification%2C%20this%20paper%20proposes%20a%20brain%20tumor%20classification%20model%20based%20on%20an%20improved%20ResNet34%20network.%20This%20model%20uses%20the%20ResNet34%20residual%20network%20as%20the%20backbone%20network%20and%20incorporates%20multi-scale%20feature%20extraction.%20It%20uses%20a%20multi-scale%20input%20module%20as%20the%20first%20layer%20of%20the%20ResNet34%20network%20and%20an%20Inception%20v2%20module%20as%20the%20residual%20downsampling%20layer.%20Furthermore%2C%20a%20channel%20attention%20mechanism%20module%20assigns%20different%20weights%20to%20different%20channels%20of%20the%20image%20from%20a%20channel%20domain%20perspective%2C%20obtaining%20more%20important%20feature%20information.%20The%20results%20after%20a%20five-fold%20crossover%20experiment%20show%20that%20the%20average%20classification%20accuracy%20of%20the%20improved%20network%20model%20is%20approximately%2098.8%25%2C%20which%20is%20not%20only%201%25%20higher%20than%20ResNet34%2C%20but%20also%20only%2080%25%20of%20the%20number%20of%20parameters%20of%20the%20original%20model.%20Therefore%2C%20the%20improved%20network%20model%20not%20only%20improves%20accuracy%20but%20also%20reduces%20clutter%2C%20achieving%20a%20classification%20effect%20with%20fewer%20parameters%20and%20higher%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2512.03751v1&entry.124074799=Read"},
{"title": "Language-Driven Object-Oriented Two-Stage Method for Scene Graph Anticipation", "author": "Xiaomeng Zhu and Changwei Wang and Haozhe Wang and Xinyu Liu and Fangzhen Lin", "abstract": "A scene graph is a structured representation of objects and their spatio-temporal relationships in dynamic scenes. Scene Graph Anticipation (SGA) involves predicting future scene graphs from video clips, enabling applications in intelligent surveillance and human-machine collaboration. While recent SGA approaches excel at leveraging visual evidence, long-horizon forecasting fundamentally depends on semantic priors and commonsense temporal regularities that are challenging to extract purely from visual features. To explicitly model these semantic dynamics, we propose Linguistic Scene Graph Anticipation (LSGA), a linguistic formulation of SGA that performs temporal relational reasoning over sequences of textualized scene graphs, with visual scene-graph detection handled by a modular front-end when operating on video. Building on this formulation, we introduce Object-Oriented Two-Stage Method (OOTSM), a language-based framework that anticipates object-set dynamics and forecasts object-centric relation trajectories with temporal consistency regularization, and we evaluate it on a dedicated benchmark constructed from Action Genome annotations. Extensive experiments show that compact fine-tuned language models with up to 3B parameters consistently outperform strong zero- and one-shot API baselines, including GPT-4o, GPT-4o-mini, and DeepSeek-V3, under matched textual inputs and context windows. When coupled with off-the-shelf visual scene-graph generators, the resulting multimodal system achieves substantial improvements on video-based SGA, boosting long-horizon mR@50 by up to 21.9\\% over strong visual SGA baselines.", "link": "http://arxiv.org/abs/2509.05661v2", "date": "2025-12-03", "relevancy": 2.3298, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5923}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Driven%20Object-Oriented%20Two-Stage%20Method%20for%20Scene%20Graph%20Anticipation&body=Title%3A%20Language-Driven%20Object-Oriented%20Two-Stage%20Method%20for%20Scene%20Graph%20Anticipation%0AAuthor%3A%20Xiaomeng%20Zhu%20and%20Changwei%20Wang%20and%20Haozhe%20Wang%20and%20Xinyu%20Liu%20and%20Fangzhen%20Lin%0AAbstract%3A%20A%20scene%20graph%20is%20a%20structured%20representation%20of%20objects%20and%20their%20spatio-temporal%20relationships%20in%20dynamic%20scenes.%20Scene%20Graph%20Anticipation%20%28SGA%29%20involves%20predicting%20future%20scene%20graphs%20from%20video%20clips%2C%20enabling%20applications%20in%20intelligent%20surveillance%20and%20human-machine%20collaboration.%20While%20recent%20SGA%20approaches%20excel%20at%20leveraging%20visual%20evidence%2C%20long-horizon%20forecasting%20fundamentally%20depends%20on%20semantic%20priors%20and%20commonsense%20temporal%20regularities%20that%20are%20challenging%20to%20extract%20purely%20from%20visual%20features.%20To%20explicitly%20model%20these%20semantic%20dynamics%2C%20we%20propose%20Linguistic%20Scene%20Graph%20Anticipation%20%28LSGA%29%2C%20a%20linguistic%20formulation%20of%20SGA%20that%20performs%20temporal%20relational%20reasoning%20over%20sequences%20of%20textualized%20scene%20graphs%2C%20with%20visual%20scene-graph%20detection%20handled%20by%20a%20modular%20front-end%20when%20operating%20on%20video.%20Building%20on%20this%20formulation%2C%20we%20introduce%20Object-Oriented%20Two-Stage%20Method%20%28OOTSM%29%2C%20a%20language-based%20framework%20that%20anticipates%20object-set%20dynamics%20and%20forecasts%20object-centric%20relation%20trajectories%20with%20temporal%20consistency%20regularization%2C%20and%20we%20evaluate%20it%20on%20a%20dedicated%20benchmark%20constructed%20from%20Action%20Genome%20annotations.%20Extensive%20experiments%20show%20that%20compact%20fine-tuned%20language%20models%20with%20up%20to%203B%20parameters%20consistently%20outperform%20strong%20zero-%20and%20one-shot%20API%20baselines%2C%20including%20GPT-4o%2C%20GPT-4o-mini%2C%20and%20DeepSeek-V3%2C%20under%20matched%20textual%20inputs%20and%20context%20windows.%20When%20coupled%20with%20off-the-shelf%20visual%20scene-graph%20generators%2C%20the%20resulting%20multimodal%20system%20achieves%20substantial%20improvements%20on%20video-based%20SGA%2C%20boosting%20long-horizon%20mR%4050%20by%20up%20to%2021.9%5C%25%20over%20strong%20visual%20SGA%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2509.05661v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Driven%2520Object-Oriented%2520Two-Stage%2520Method%2520for%2520Scene%2520Graph%2520Anticipation%26entry.906535625%3DXiaomeng%2520Zhu%2520and%2520Changwei%2520Wang%2520and%2520Haozhe%2520Wang%2520and%2520Xinyu%2520Liu%2520and%2520Fangzhen%2520Lin%26entry.1292438233%3DA%2520scene%2520graph%2520is%2520a%2520structured%2520representation%2520of%2520objects%2520and%2520their%2520spatio-temporal%2520relationships%2520in%2520dynamic%2520scenes.%2520Scene%2520Graph%2520Anticipation%2520%2528SGA%2529%2520involves%2520predicting%2520future%2520scene%2520graphs%2520from%2520video%2520clips%252C%2520enabling%2520applications%2520in%2520intelligent%2520surveillance%2520and%2520human-machine%2520collaboration.%2520While%2520recent%2520SGA%2520approaches%2520excel%2520at%2520leveraging%2520visual%2520evidence%252C%2520long-horizon%2520forecasting%2520fundamentally%2520depends%2520on%2520semantic%2520priors%2520and%2520commonsense%2520temporal%2520regularities%2520that%2520are%2520challenging%2520to%2520extract%2520purely%2520from%2520visual%2520features.%2520To%2520explicitly%2520model%2520these%2520semantic%2520dynamics%252C%2520we%2520propose%2520Linguistic%2520Scene%2520Graph%2520Anticipation%2520%2528LSGA%2529%252C%2520a%2520linguistic%2520formulation%2520of%2520SGA%2520that%2520performs%2520temporal%2520relational%2520reasoning%2520over%2520sequences%2520of%2520textualized%2520scene%2520graphs%252C%2520with%2520visual%2520scene-graph%2520detection%2520handled%2520by%2520a%2520modular%2520front-end%2520when%2520operating%2520on%2520video.%2520Building%2520on%2520this%2520formulation%252C%2520we%2520introduce%2520Object-Oriented%2520Two-Stage%2520Method%2520%2528OOTSM%2529%252C%2520a%2520language-based%2520framework%2520that%2520anticipates%2520object-set%2520dynamics%2520and%2520forecasts%2520object-centric%2520relation%2520trajectories%2520with%2520temporal%2520consistency%2520regularization%252C%2520and%2520we%2520evaluate%2520it%2520on%2520a%2520dedicated%2520benchmark%2520constructed%2520from%2520Action%2520Genome%2520annotations.%2520Extensive%2520experiments%2520show%2520that%2520compact%2520fine-tuned%2520language%2520models%2520with%2520up%2520to%25203B%2520parameters%2520consistently%2520outperform%2520strong%2520zero-%2520and%2520one-shot%2520API%2520baselines%252C%2520including%2520GPT-4o%252C%2520GPT-4o-mini%252C%2520and%2520DeepSeek-V3%252C%2520under%2520matched%2520textual%2520inputs%2520and%2520context%2520windows.%2520When%2520coupled%2520with%2520off-the-shelf%2520visual%2520scene-graph%2520generators%252C%2520the%2520resulting%2520multimodal%2520system%2520achieves%2520substantial%2520improvements%2520on%2520video-based%2520SGA%252C%2520boosting%2520long-horizon%2520mR%254050%2520by%2520up%2520to%252021.9%255C%2525%2520over%2520strong%2520visual%2520SGA%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05661v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Driven%20Object-Oriented%20Two-Stage%20Method%20for%20Scene%20Graph%20Anticipation&entry.906535625=Xiaomeng%20Zhu%20and%20Changwei%20Wang%20and%20Haozhe%20Wang%20and%20Xinyu%20Liu%20and%20Fangzhen%20Lin&entry.1292438233=A%20scene%20graph%20is%20a%20structured%20representation%20of%20objects%20and%20their%20spatio-temporal%20relationships%20in%20dynamic%20scenes.%20Scene%20Graph%20Anticipation%20%28SGA%29%20involves%20predicting%20future%20scene%20graphs%20from%20video%20clips%2C%20enabling%20applications%20in%20intelligent%20surveillance%20and%20human-machine%20collaboration.%20While%20recent%20SGA%20approaches%20excel%20at%20leveraging%20visual%20evidence%2C%20long-horizon%20forecasting%20fundamentally%20depends%20on%20semantic%20priors%20and%20commonsense%20temporal%20regularities%20that%20are%20challenging%20to%20extract%20purely%20from%20visual%20features.%20To%20explicitly%20model%20these%20semantic%20dynamics%2C%20we%20propose%20Linguistic%20Scene%20Graph%20Anticipation%20%28LSGA%29%2C%20a%20linguistic%20formulation%20of%20SGA%20that%20performs%20temporal%20relational%20reasoning%20over%20sequences%20of%20textualized%20scene%20graphs%2C%20with%20visual%20scene-graph%20detection%20handled%20by%20a%20modular%20front-end%20when%20operating%20on%20video.%20Building%20on%20this%20formulation%2C%20we%20introduce%20Object-Oriented%20Two-Stage%20Method%20%28OOTSM%29%2C%20a%20language-based%20framework%20that%20anticipates%20object-set%20dynamics%20and%20forecasts%20object-centric%20relation%20trajectories%20with%20temporal%20consistency%20regularization%2C%20and%20we%20evaluate%20it%20on%20a%20dedicated%20benchmark%20constructed%20from%20Action%20Genome%20annotations.%20Extensive%20experiments%20show%20that%20compact%20fine-tuned%20language%20models%20with%20up%20to%203B%20parameters%20consistently%20outperform%20strong%20zero-%20and%20one-shot%20API%20baselines%2C%20including%20GPT-4o%2C%20GPT-4o-mini%2C%20and%20DeepSeek-V3%2C%20under%20matched%20textual%20inputs%20and%20context%20windows.%20When%20coupled%20with%20off-the-shelf%20visual%20scene-graph%20generators%2C%20the%20resulting%20multimodal%20system%20achieves%20substantial%20improvements%20on%20video-based%20SGA%2C%20boosting%20long-horizon%20mR%4050%20by%20up%20to%2021.9%5C%25%20over%20strong%20visual%20SGA%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2509.05661v2&entry.124074799=Read"},
{"title": "Cross-embodied Co-design for Dexterous Hands", "author": "Kehlani Fay and Darin Anthony Djapri and Anya Zorin and James Clinton and Ali El Lahib and Hao Su and Michael T. Tolley and Sha Yi and Xiaolong Wang", "abstract": "Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.", "link": "http://arxiv.org/abs/2512.03743v1", "date": "2025-12-03", "relevancy": 2.3188, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6219}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5501}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-embodied%20Co-design%20for%20Dexterous%20Hands&body=Title%3A%20Cross-embodied%20Co-design%20for%20Dexterous%20Hands%0AAuthor%3A%20Kehlani%20Fay%20and%20Darin%20Anthony%20Djapri%20and%20Anya%20Zorin%20and%20James%20Clinton%20and%20Ali%20El%20Lahib%20and%20Hao%20Su%20and%20Michael%20T.%20Tolley%20and%20Sha%20Yi%20and%20Xiaolong%20Wang%0AAbstract%3A%20Dexterous%20manipulation%20is%20limited%20by%20both%20control%20and%20design%2C%20without%20consensus%20as%20to%20what%20makes%20manipulators%20best%20for%20performing%20dexterous%20tasks.%20This%20raises%20a%20fundamental%20challenge%3A%20how%20should%20we%20design%20and%20control%20robot%20manipulators%20that%20are%20optimized%20for%20dexterity%3F%20We%20present%20a%20co-design%20framework%20that%20learns%20task-specific%20hand%20morphology%20and%20complementary%20dexterous%20control%20policies.%20The%20framework%20supports%201%29%20an%20expansive%20morphology%20search%20space%20including%20joint%2C%20finger%2C%20and%20palm%20generation%2C%202%29%20scalable%20evaluation%20across%20the%20wide%20design%20space%20via%20morphology-conditioned%20cross-embodied%20control%2C%20and%203%29%20real-world%20fabrication%20with%20accessible%20components.%20We%20evaluate%20the%20approach%20across%20multiple%20dexterous%20tasks%2C%20including%20in-hand%20rotation%20with%20simulation%20and%20real%20deployment.%20Our%20framework%20enables%20an%20end-to-end%20pipeline%20that%20can%20design%2C%20train%2C%20fabricate%2C%20and%20deploy%20a%20new%20robotic%20hand%20in%20under%2024%20hours.%20The%20full%20framework%20will%20be%20open-sourced%20and%20available%20on%20our%20website.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-embodied%2520Co-design%2520for%2520Dexterous%2520Hands%26entry.906535625%3DKehlani%2520Fay%2520and%2520Darin%2520Anthony%2520Djapri%2520and%2520Anya%2520Zorin%2520and%2520James%2520Clinton%2520and%2520Ali%2520El%2520Lahib%2520and%2520Hao%2520Su%2520and%2520Michael%2520T.%2520Tolley%2520and%2520Sha%2520Yi%2520and%2520Xiaolong%2520Wang%26entry.1292438233%3DDexterous%2520manipulation%2520is%2520limited%2520by%2520both%2520control%2520and%2520design%252C%2520without%2520consensus%2520as%2520to%2520what%2520makes%2520manipulators%2520best%2520for%2520performing%2520dexterous%2520tasks.%2520This%2520raises%2520a%2520fundamental%2520challenge%253A%2520how%2520should%2520we%2520design%2520and%2520control%2520robot%2520manipulators%2520that%2520are%2520optimized%2520for%2520dexterity%253F%2520We%2520present%2520a%2520co-design%2520framework%2520that%2520learns%2520task-specific%2520hand%2520morphology%2520and%2520complementary%2520dexterous%2520control%2520policies.%2520The%2520framework%2520supports%25201%2529%2520an%2520expansive%2520morphology%2520search%2520space%2520including%2520joint%252C%2520finger%252C%2520and%2520palm%2520generation%252C%25202%2529%2520scalable%2520evaluation%2520across%2520the%2520wide%2520design%2520space%2520via%2520morphology-conditioned%2520cross-embodied%2520control%252C%2520and%25203%2529%2520real-world%2520fabrication%2520with%2520accessible%2520components.%2520We%2520evaluate%2520the%2520approach%2520across%2520multiple%2520dexterous%2520tasks%252C%2520including%2520in-hand%2520rotation%2520with%2520simulation%2520and%2520real%2520deployment.%2520Our%2520framework%2520enables%2520an%2520end-to-end%2520pipeline%2520that%2520can%2520design%252C%2520train%252C%2520fabricate%252C%2520and%2520deploy%2520a%2520new%2520robotic%2520hand%2520in%2520under%252024%2520hours.%2520The%2520full%2520framework%2520will%2520be%2520open-sourced%2520and%2520available%2520on%2520our%2520website.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-embodied%20Co-design%20for%20Dexterous%20Hands&entry.906535625=Kehlani%20Fay%20and%20Darin%20Anthony%20Djapri%20and%20Anya%20Zorin%20and%20James%20Clinton%20and%20Ali%20El%20Lahib%20and%20Hao%20Su%20and%20Michael%20T.%20Tolley%20and%20Sha%20Yi%20and%20Xiaolong%20Wang&entry.1292438233=Dexterous%20manipulation%20is%20limited%20by%20both%20control%20and%20design%2C%20without%20consensus%20as%20to%20what%20makes%20manipulators%20best%20for%20performing%20dexterous%20tasks.%20This%20raises%20a%20fundamental%20challenge%3A%20how%20should%20we%20design%20and%20control%20robot%20manipulators%20that%20are%20optimized%20for%20dexterity%3F%20We%20present%20a%20co-design%20framework%20that%20learns%20task-specific%20hand%20morphology%20and%20complementary%20dexterous%20control%20policies.%20The%20framework%20supports%201%29%20an%20expansive%20morphology%20search%20space%20including%20joint%2C%20finger%2C%20and%20palm%20generation%2C%202%29%20scalable%20evaluation%20across%20the%20wide%20design%20space%20via%20morphology-conditioned%20cross-embodied%20control%2C%20and%203%29%20real-world%20fabrication%20with%20accessible%20components.%20We%20evaluate%20the%20approach%20across%20multiple%20dexterous%20tasks%2C%20including%20in-hand%20rotation%20with%20simulation%20and%20real%20deployment.%20Our%20framework%20enables%20an%20end-to-end%20pipeline%20that%20can%20design%2C%20train%2C%20fabricate%2C%20and%20deploy%20a%20new%20robotic%20hand%20in%20under%2024%20hours.%20The%20full%20framework%20will%20be%20open-sourced%20and%20available%20on%20our%20website.&entry.1838667208=http%3A//arxiv.org/abs/2512.03743v1&entry.124074799=Read"},
{"title": "Training for Identity, Inference for Controllability: A Unified Approach to Tuning-Free Face Personalization", "author": "Lianyu Pang and Ji Zhou and Qiping Wang and Baoquan Zhao and Zhenguo Yang and Qing Li and Xudong Mao", "abstract": "Tuning-free face personalization methods have developed along two distinct paradigms: text embedding approaches that map facial features into the text embedding space, and adapter-based methods that inject features through auxiliary cross-attention layers. While both paradigms have shown promise, existing methods struggle to simultaneously achieve high identity fidelity and flexible text controllability. We introduce UniID, a unified tuning-free framework that synergistically integrates both paradigms. Our key insight is that when merging these approaches, they should mutually reinforce only identity-relevant information while preserving the original diffusion prior for non-identity attributes. We realize this through a principled training-inference strategy: during training, we employ an identity-focused learning scheme that guides both branches to capture identity features exclusively; at inference, we introduce a normalized rescaling mechanism that recovers the text controllability of the base diffusion model while enabling complementary identity signals to enhance each other. This principled design enables UniID to achieve high-fidelity face personalization with flexible text controllability. Extensive experiments against six state-of-the-art methods demonstrate that UniID achieves superior performance in both identity preservation and text controllability. Code will be available at https://github.com/lyuPang/UniID", "link": "http://arxiv.org/abs/2512.03964v1", "date": "2025-12-03", "relevancy": 2.3169, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6264}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5629}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20for%20Identity%2C%20Inference%20for%20Controllability%3A%20A%20Unified%20Approach%20to%20Tuning-Free%20Face%20Personalization&body=Title%3A%20Training%20for%20Identity%2C%20Inference%20for%20Controllability%3A%20A%20Unified%20Approach%20to%20Tuning-Free%20Face%20Personalization%0AAuthor%3A%20Lianyu%20Pang%20and%20Ji%20Zhou%20and%20Qiping%20Wang%20and%20Baoquan%20Zhao%20and%20Zhenguo%20Yang%20and%20Qing%20Li%20and%20Xudong%20Mao%0AAbstract%3A%20Tuning-free%20face%20personalization%20methods%20have%20developed%20along%20two%20distinct%20paradigms%3A%20text%20embedding%20approaches%20that%20map%20facial%20features%20into%20the%20text%20embedding%20space%2C%20and%20adapter-based%20methods%20that%20inject%20features%20through%20auxiliary%20cross-attention%20layers.%20While%20both%20paradigms%20have%20shown%20promise%2C%20existing%20methods%20struggle%20to%20simultaneously%20achieve%20high%20identity%20fidelity%20and%20flexible%20text%20controllability.%20We%20introduce%20UniID%2C%20a%20unified%20tuning-free%20framework%20that%20synergistically%20integrates%20both%20paradigms.%20Our%20key%20insight%20is%20that%20when%20merging%20these%20approaches%2C%20they%20should%20mutually%20reinforce%20only%20identity-relevant%20information%20while%20preserving%20the%20original%20diffusion%20prior%20for%20non-identity%20attributes.%20We%20realize%20this%20through%20a%20principled%20training-inference%20strategy%3A%20during%20training%2C%20we%20employ%20an%20identity-focused%20learning%20scheme%20that%20guides%20both%20branches%20to%20capture%20identity%20features%20exclusively%3B%20at%20inference%2C%20we%20introduce%20a%20normalized%20rescaling%20mechanism%20that%20recovers%20the%20text%20controllability%20of%20the%20base%20diffusion%20model%20while%20enabling%20complementary%20identity%20signals%20to%20enhance%20each%20other.%20This%20principled%20design%20enables%20UniID%20to%20achieve%20high-fidelity%20face%20personalization%20with%20flexible%20text%20controllability.%20Extensive%20experiments%20against%20six%20state-of-the-art%20methods%20demonstrate%20that%20UniID%20achieves%20superior%20performance%20in%20both%20identity%20preservation%20and%20text%20controllability.%20Code%20will%20be%20available%20at%20https%3A//github.com/lyuPang/UniID%0ALink%3A%20http%3A//arxiv.org/abs/2512.03964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520for%2520Identity%252C%2520Inference%2520for%2520Controllability%253A%2520A%2520Unified%2520Approach%2520to%2520Tuning-Free%2520Face%2520Personalization%26entry.906535625%3DLianyu%2520Pang%2520and%2520Ji%2520Zhou%2520and%2520Qiping%2520Wang%2520and%2520Baoquan%2520Zhao%2520and%2520Zhenguo%2520Yang%2520and%2520Qing%2520Li%2520and%2520Xudong%2520Mao%26entry.1292438233%3DTuning-free%2520face%2520personalization%2520methods%2520have%2520developed%2520along%2520two%2520distinct%2520paradigms%253A%2520text%2520embedding%2520approaches%2520that%2520map%2520facial%2520features%2520into%2520the%2520text%2520embedding%2520space%252C%2520and%2520adapter-based%2520methods%2520that%2520inject%2520features%2520through%2520auxiliary%2520cross-attention%2520layers.%2520While%2520both%2520paradigms%2520have%2520shown%2520promise%252C%2520existing%2520methods%2520struggle%2520to%2520simultaneously%2520achieve%2520high%2520identity%2520fidelity%2520and%2520flexible%2520text%2520controllability.%2520We%2520introduce%2520UniID%252C%2520a%2520unified%2520tuning-free%2520framework%2520that%2520synergistically%2520integrates%2520both%2520paradigms.%2520Our%2520key%2520insight%2520is%2520that%2520when%2520merging%2520these%2520approaches%252C%2520they%2520should%2520mutually%2520reinforce%2520only%2520identity-relevant%2520information%2520while%2520preserving%2520the%2520original%2520diffusion%2520prior%2520for%2520non-identity%2520attributes.%2520We%2520realize%2520this%2520through%2520a%2520principled%2520training-inference%2520strategy%253A%2520during%2520training%252C%2520we%2520employ%2520an%2520identity-focused%2520learning%2520scheme%2520that%2520guides%2520both%2520branches%2520to%2520capture%2520identity%2520features%2520exclusively%253B%2520at%2520inference%252C%2520we%2520introduce%2520a%2520normalized%2520rescaling%2520mechanism%2520that%2520recovers%2520the%2520text%2520controllability%2520of%2520the%2520base%2520diffusion%2520model%2520while%2520enabling%2520complementary%2520identity%2520signals%2520to%2520enhance%2520each%2520other.%2520This%2520principled%2520design%2520enables%2520UniID%2520to%2520achieve%2520high-fidelity%2520face%2520personalization%2520with%2520flexible%2520text%2520controllability.%2520Extensive%2520experiments%2520against%2520six%2520state-of-the-art%2520methods%2520demonstrate%2520that%2520UniID%2520achieves%2520superior%2520performance%2520in%2520both%2520identity%2520preservation%2520and%2520text%2520controllability.%2520Code%2520will%2520be%2520available%2520at%2520https%253A//github.com/lyuPang/UniID%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20for%20Identity%2C%20Inference%20for%20Controllability%3A%20A%20Unified%20Approach%20to%20Tuning-Free%20Face%20Personalization&entry.906535625=Lianyu%20Pang%20and%20Ji%20Zhou%20and%20Qiping%20Wang%20and%20Baoquan%20Zhao%20and%20Zhenguo%20Yang%20and%20Qing%20Li%20and%20Xudong%20Mao&entry.1292438233=Tuning-free%20face%20personalization%20methods%20have%20developed%20along%20two%20distinct%20paradigms%3A%20text%20embedding%20approaches%20that%20map%20facial%20features%20into%20the%20text%20embedding%20space%2C%20and%20adapter-based%20methods%20that%20inject%20features%20through%20auxiliary%20cross-attention%20layers.%20While%20both%20paradigms%20have%20shown%20promise%2C%20existing%20methods%20struggle%20to%20simultaneously%20achieve%20high%20identity%20fidelity%20and%20flexible%20text%20controllability.%20We%20introduce%20UniID%2C%20a%20unified%20tuning-free%20framework%20that%20synergistically%20integrates%20both%20paradigms.%20Our%20key%20insight%20is%20that%20when%20merging%20these%20approaches%2C%20they%20should%20mutually%20reinforce%20only%20identity-relevant%20information%20while%20preserving%20the%20original%20diffusion%20prior%20for%20non-identity%20attributes.%20We%20realize%20this%20through%20a%20principled%20training-inference%20strategy%3A%20during%20training%2C%20we%20employ%20an%20identity-focused%20learning%20scheme%20that%20guides%20both%20branches%20to%20capture%20identity%20features%20exclusively%3B%20at%20inference%2C%20we%20introduce%20a%20normalized%20rescaling%20mechanism%20that%20recovers%20the%20text%20controllability%20of%20the%20base%20diffusion%20model%20while%20enabling%20complementary%20identity%20signals%20to%20enhance%20each%20other.%20This%20principled%20design%20enables%20UniID%20to%20achieve%20high-fidelity%20face%20personalization%20with%20flexible%20text%20controllability.%20Extensive%20experiments%20against%20six%20state-of-the-art%20methods%20demonstrate%20that%20UniID%20achieves%20superior%20performance%20in%20both%20identity%20preservation%20and%20text%20controllability.%20Code%20will%20be%20available%20at%20https%3A//github.com/lyuPang/UniID&entry.1838667208=http%3A//arxiv.org/abs/2512.03964v1&entry.124074799=Read"},
{"title": "A Machine Learning-Driven Solution for Denoising Inertial Confinement Fusion Images", "author": "Asya Y. Akkus and Bradley T. Wolfe and Pinghan Chu and Chengkun Huang and Chris S. Campbell and Mariana Alvarado Alvarez and Petr Volegov and David Fittinghoff and Robert Reinovsky and Zhehui Wang", "abstract": "Neutron imaging is essential for diagnosing and optimizing inertial confinement fusion implosions at the National Ignition Facility. Due to the required 10-micrometer resolution, however, neutron image require image reconstruction using iterative algorithms. For low-yield sources, the images may be degraded by various types of noise. Gaussian and Poisson noise often coexist within one image, obscuring fine details and blurring the edges where the source information is encoded. Traditional denoising techniques, such as filtering and thresholding, can inadvertently alter critical features or reshape the noise statistics, potentially impacting the ultimate fidelity of the iterative image reconstruction pipeline. However, recent advances in synthetic data production and machine learning have opened new opportunities to address these challenges. In this study, we present an unsupervised autoencoder with a Cohen-Daubechies- Feauveau (CDF 97) wavelet transform in the latent space, designed to suppress for mixed Gaussian-Poisson noise while preserving essential image features. The network successfully denoises neutron imaging data. Benchmarking against both simulated and experimental NIF datasets demonstrates that our approach achieves lower reconstruction error and superior edge preservation compared to conventional filtering methods such as Block-matching and 3D filtering (BM3D). By validating the effectiveness of unsupervised learning for denoising neutron images, this study establishes a critical first step towards fully AI-driven, end-to-end reconstruction frameworks for ICF diagnostics.", "link": "http://arxiv.org/abs/2511.16717v2", "date": "2025-12-03", "relevancy": 2.3108, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6268}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5447}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Machine%20Learning-Driven%20Solution%20for%20Denoising%20Inertial%20Confinement%20Fusion%20Images&body=Title%3A%20A%20Machine%20Learning-Driven%20Solution%20for%20Denoising%20Inertial%20Confinement%20Fusion%20Images%0AAuthor%3A%20Asya%20Y.%20Akkus%20and%20Bradley%20T.%20Wolfe%20and%20Pinghan%20Chu%20and%20Chengkun%20Huang%20and%20Chris%20S.%20Campbell%20and%20Mariana%20Alvarado%20Alvarez%20and%20Petr%20Volegov%20and%20David%20Fittinghoff%20and%20Robert%20Reinovsky%20and%20Zhehui%20Wang%0AAbstract%3A%20Neutron%20imaging%20is%20essential%20for%20diagnosing%20and%20optimizing%20inertial%20confinement%20fusion%20implosions%20at%20the%20National%20Ignition%20Facility.%20Due%20to%20the%20required%2010-micrometer%20resolution%2C%20however%2C%20neutron%20image%20require%20image%20reconstruction%20using%20iterative%20algorithms.%20For%20low-yield%20sources%2C%20the%20images%20may%20be%20degraded%20by%20various%20types%20of%20noise.%20Gaussian%20and%20Poisson%20noise%20often%20coexist%20within%20one%20image%2C%20obscuring%20fine%20details%20and%20blurring%20the%20edges%20where%20the%20source%20information%20is%20encoded.%20Traditional%20denoising%20techniques%2C%20such%20as%20filtering%20and%20thresholding%2C%20can%20inadvertently%20alter%20critical%20features%20or%20reshape%20the%20noise%20statistics%2C%20potentially%20impacting%20the%20ultimate%20fidelity%20of%20the%20iterative%20image%20reconstruction%20pipeline.%20However%2C%20recent%20advances%20in%20synthetic%20data%20production%20and%20machine%20learning%20have%20opened%20new%20opportunities%20to%20address%20these%20challenges.%20In%20this%20study%2C%20we%20present%20an%20unsupervised%20autoencoder%20with%20a%20Cohen-Daubechies-%20Feauveau%20%28CDF%2097%29%20wavelet%20transform%20in%20the%20latent%20space%2C%20designed%20to%20suppress%20for%20mixed%20Gaussian-Poisson%20noise%20while%20preserving%20essential%20image%20features.%20The%20network%20successfully%20denoises%20neutron%20imaging%20data.%20Benchmarking%20against%20both%20simulated%20and%20experimental%20NIF%20datasets%20demonstrates%20that%20our%20approach%20achieves%20lower%20reconstruction%20error%20and%20superior%20edge%20preservation%20compared%20to%20conventional%20filtering%20methods%20such%20as%20Block-matching%20and%203D%20filtering%20%28BM3D%29.%20By%20validating%20the%20effectiveness%20of%20unsupervised%20learning%20for%20denoising%20neutron%20images%2C%20this%20study%20establishes%20a%20critical%20first%20step%20towards%20fully%20AI-driven%2C%20end-to-end%20reconstruction%20frameworks%20for%20ICF%20diagnostics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Machine%2520Learning-Driven%2520Solution%2520for%2520Denoising%2520Inertial%2520Confinement%2520Fusion%2520Images%26entry.906535625%3DAsya%2520Y.%2520Akkus%2520and%2520Bradley%2520T.%2520Wolfe%2520and%2520Pinghan%2520Chu%2520and%2520Chengkun%2520Huang%2520and%2520Chris%2520S.%2520Campbell%2520and%2520Mariana%2520Alvarado%2520Alvarez%2520and%2520Petr%2520Volegov%2520and%2520David%2520Fittinghoff%2520and%2520Robert%2520Reinovsky%2520and%2520Zhehui%2520Wang%26entry.1292438233%3DNeutron%2520imaging%2520is%2520essential%2520for%2520diagnosing%2520and%2520optimizing%2520inertial%2520confinement%2520fusion%2520implosions%2520at%2520the%2520National%2520Ignition%2520Facility.%2520Due%2520to%2520the%2520required%252010-micrometer%2520resolution%252C%2520however%252C%2520neutron%2520image%2520require%2520image%2520reconstruction%2520using%2520iterative%2520algorithms.%2520For%2520low-yield%2520sources%252C%2520the%2520images%2520may%2520be%2520degraded%2520by%2520various%2520types%2520of%2520noise.%2520Gaussian%2520and%2520Poisson%2520noise%2520often%2520coexist%2520within%2520one%2520image%252C%2520obscuring%2520fine%2520details%2520and%2520blurring%2520the%2520edges%2520where%2520the%2520source%2520information%2520is%2520encoded.%2520Traditional%2520denoising%2520techniques%252C%2520such%2520as%2520filtering%2520and%2520thresholding%252C%2520can%2520inadvertently%2520alter%2520critical%2520features%2520or%2520reshape%2520the%2520noise%2520statistics%252C%2520potentially%2520impacting%2520the%2520ultimate%2520fidelity%2520of%2520the%2520iterative%2520image%2520reconstruction%2520pipeline.%2520However%252C%2520recent%2520advances%2520in%2520synthetic%2520data%2520production%2520and%2520machine%2520learning%2520have%2520opened%2520new%2520opportunities%2520to%2520address%2520these%2520challenges.%2520In%2520this%2520study%252C%2520we%2520present%2520an%2520unsupervised%2520autoencoder%2520with%2520a%2520Cohen-Daubechies-%2520Feauveau%2520%2528CDF%252097%2529%2520wavelet%2520transform%2520in%2520the%2520latent%2520space%252C%2520designed%2520to%2520suppress%2520for%2520mixed%2520Gaussian-Poisson%2520noise%2520while%2520preserving%2520essential%2520image%2520features.%2520The%2520network%2520successfully%2520denoises%2520neutron%2520imaging%2520data.%2520Benchmarking%2520against%2520both%2520simulated%2520and%2520experimental%2520NIF%2520datasets%2520demonstrates%2520that%2520our%2520approach%2520achieves%2520lower%2520reconstruction%2520error%2520and%2520superior%2520edge%2520preservation%2520compared%2520to%2520conventional%2520filtering%2520methods%2520such%2520as%2520Block-matching%2520and%25203D%2520filtering%2520%2528BM3D%2529.%2520By%2520validating%2520the%2520effectiveness%2520of%2520unsupervised%2520learning%2520for%2520denoising%2520neutron%2520images%252C%2520this%2520study%2520establishes%2520a%2520critical%2520first%2520step%2520towards%2520fully%2520AI-driven%252C%2520end-to-end%2520reconstruction%2520frameworks%2520for%2520ICF%2520diagnostics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Machine%20Learning-Driven%20Solution%20for%20Denoising%20Inertial%20Confinement%20Fusion%20Images&entry.906535625=Asya%20Y.%20Akkus%20and%20Bradley%20T.%20Wolfe%20and%20Pinghan%20Chu%20and%20Chengkun%20Huang%20and%20Chris%20S.%20Campbell%20and%20Mariana%20Alvarado%20Alvarez%20and%20Petr%20Volegov%20and%20David%20Fittinghoff%20and%20Robert%20Reinovsky%20and%20Zhehui%20Wang&entry.1292438233=Neutron%20imaging%20is%20essential%20for%20diagnosing%20and%20optimizing%20inertial%20confinement%20fusion%20implosions%20at%20the%20National%20Ignition%20Facility.%20Due%20to%20the%20required%2010-micrometer%20resolution%2C%20however%2C%20neutron%20image%20require%20image%20reconstruction%20using%20iterative%20algorithms.%20For%20low-yield%20sources%2C%20the%20images%20may%20be%20degraded%20by%20various%20types%20of%20noise.%20Gaussian%20and%20Poisson%20noise%20often%20coexist%20within%20one%20image%2C%20obscuring%20fine%20details%20and%20blurring%20the%20edges%20where%20the%20source%20information%20is%20encoded.%20Traditional%20denoising%20techniques%2C%20such%20as%20filtering%20and%20thresholding%2C%20can%20inadvertently%20alter%20critical%20features%20or%20reshape%20the%20noise%20statistics%2C%20potentially%20impacting%20the%20ultimate%20fidelity%20of%20the%20iterative%20image%20reconstruction%20pipeline.%20However%2C%20recent%20advances%20in%20synthetic%20data%20production%20and%20machine%20learning%20have%20opened%20new%20opportunities%20to%20address%20these%20challenges.%20In%20this%20study%2C%20we%20present%20an%20unsupervised%20autoencoder%20with%20a%20Cohen-Daubechies-%20Feauveau%20%28CDF%2097%29%20wavelet%20transform%20in%20the%20latent%20space%2C%20designed%20to%20suppress%20for%20mixed%20Gaussian-Poisson%20noise%20while%20preserving%20essential%20image%20features.%20The%20network%20successfully%20denoises%20neutron%20imaging%20data.%20Benchmarking%20against%20both%20simulated%20and%20experimental%20NIF%20datasets%20demonstrates%20that%20our%20approach%20achieves%20lower%20reconstruction%20error%20and%20superior%20edge%20preservation%20compared%20to%20conventional%20filtering%20methods%20such%20as%20Block-matching%20and%203D%20filtering%20%28BM3D%29.%20By%20validating%20the%20effectiveness%20of%20unsupervised%20learning%20for%20denoising%20neutron%20images%2C%20this%20study%20establishes%20a%20critical%20first%20step%20towards%20fully%20AI-driven%2C%20end-to-end%20reconstruction%20frameworks%20for%20ICF%20diagnostics.&entry.1838667208=http%3A//arxiv.org/abs/2511.16717v2&entry.124074799=Read"},
{"title": "CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL", "author": "Shinji Mai and Yunpeng Zhai and Ziqian Chen and Cheng Chen and Anni Zou and Shuchang Tao and Zhaoyang Liu and Bolin Ding", "abstract": "Large language model based agents are increasingly deployed in complex, tool augmented environments. While reinforcement learning provides a principled mechanism for such agents to improve through interaction, its effectiveness critically depends on the availability of structured training tasks. In many realistic settings, however, no such tasks exist a challenge we term task scarcity, which has become a key bottleneck for scaling agentic RL. Existing approaches typically assume predefined task collections, an assumption that fails in novel environments where tool semantics and affordances are initially unknown. To address this limitation, we formalize the problem of Task Generation for Agentic RL, where an agent must learn within a given environment that lacks predefined tasks. We propose CuES, a Curiosity driven and Environment grounded Synthesis framework that autonomously generates diverse, executable, and meaningful tasks directly from the environment structure and affordances, without relying on handcrafted seeds or external corpora. CuES drives exploration through intrinsic curiosity, abstracts interaction patterns into reusable task schemas, and refines them through lightweight top down guidance and memory based quality control. Across three representative environments, AppWorld, BFCL, and WebShop, CuES produces task distributions that match or surpass manually curated datasets in both diversity and executability, yielding substantial downstream policy improvements. These results demonstrate that curiosity driven, environment grounded task generation provides a scalable foundation for agents that not only learn how to act, but also learn what to learn. The code is available at https://github.com/modelscope/AgentEvolver/tree/main/research/CuES.", "link": "http://arxiv.org/abs/2512.01311v2", "date": "2025-12-03", "relevancy": 2.3101, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6405}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.604}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CuES%3A%20A%20Curiosity-driven%20and%20Environment-grounded%20Synthesis%20Framework%20for%20Agentic%20RL&body=Title%3A%20CuES%3A%20A%20Curiosity-driven%20and%20Environment-grounded%20Synthesis%20Framework%20for%20Agentic%20RL%0AAuthor%3A%20Shinji%20Mai%20and%20Yunpeng%20Zhai%20and%20Ziqian%20Chen%20and%20Cheng%20Chen%20and%20Anni%20Zou%20and%20Shuchang%20Tao%20and%20Zhaoyang%20Liu%20and%20Bolin%20Ding%0AAbstract%3A%20Large%20language%20model%20based%20agents%20are%20increasingly%20deployed%20in%20complex%2C%20tool%20augmented%20environments.%20While%20reinforcement%20learning%20provides%20a%20principled%20mechanism%20for%20such%20agents%20to%20improve%20through%20interaction%2C%20its%20effectiveness%20critically%20depends%20on%20the%20availability%20of%20structured%20training%20tasks.%20In%20many%20realistic%20settings%2C%20however%2C%20no%20such%20tasks%20exist%20a%20challenge%20we%20term%20task%20scarcity%2C%20which%20has%20become%20a%20key%20bottleneck%20for%20scaling%20agentic%20RL.%20Existing%20approaches%20typically%20assume%20predefined%20task%20collections%2C%20an%20assumption%20that%20fails%20in%20novel%20environments%20where%20tool%20semantics%20and%20affordances%20are%20initially%20unknown.%20To%20address%20this%20limitation%2C%20we%20formalize%20the%20problem%20of%20Task%20Generation%20for%20Agentic%20RL%2C%20where%20an%20agent%20must%20learn%20within%20a%20given%20environment%20that%20lacks%20predefined%20tasks.%20We%20propose%20CuES%2C%20a%20Curiosity%20driven%20and%20Environment%20grounded%20Synthesis%20framework%20that%20autonomously%20generates%20diverse%2C%20executable%2C%20and%20meaningful%20tasks%20directly%20from%20the%20environment%20structure%20and%20affordances%2C%20without%20relying%20on%20handcrafted%20seeds%20or%20external%20corpora.%20CuES%20drives%20exploration%20through%20intrinsic%20curiosity%2C%20abstracts%20interaction%20patterns%20into%20reusable%20task%20schemas%2C%20and%20refines%20them%20through%20lightweight%20top%20down%20guidance%20and%20memory%20based%20quality%20control.%20Across%20three%20representative%20environments%2C%20AppWorld%2C%20BFCL%2C%20and%20WebShop%2C%20CuES%20produces%20task%20distributions%20that%20match%20or%20surpass%20manually%20curated%20datasets%20in%20both%20diversity%20and%20executability%2C%20yielding%20substantial%20downstream%20policy%20improvements.%20These%20results%20demonstrate%20that%20curiosity%20driven%2C%20environment%20grounded%20task%20generation%20provides%20a%20scalable%20foundation%20for%20agents%20that%20not%20only%20learn%20how%20to%20act%2C%20but%20also%20learn%20what%20to%20learn.%20The%20code%20is%20available%20at%20https%3A//github.com/modelscope/AgentEvolver/tree/main/research/CuES.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01311v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCuES%253A%2520A%2520Curiosity-driven%2520and%2520Environment-grounded%2520Synthesis%2520Framework%2520for%2520Agentic%2520RL%26entry.906535625%3DShinji%2520Mai%2520and%2520Yunpeng%2520Zhai%2520and%2520Ziqian%2520Chen%2520and%2520Cheng%2520Chen%2520and%2520Anni%2520Zou%2520and%2520Shuchang%2520Tao%2520and%2520Zhaoyang%2520Liu%2520and%2520Bolin%2520Ding%26entry.1292438233%3DLarge%2520language%2520model%2520based%2520agents%2520are%2520increasingly%2520deployed%2520in%2520complex%252C%2520tool%2520augmented%2520environments.%2520While%2520reinforcement%2520learning%2520provides%2520a%2520principled%2520mechanism%2520for%2520such%2520agents%2520to%2520improve%2520through%2520interaction%252C%2520its%2520effectiveness%2520critically%2520depends%2520on%2520the%2520availability%2520of%2520structured%2520training%2520tasks.%2520In%2520many%2520realistic%2520settings%252C%2520however%252C%2520no%2520such%2520tasks%2520exist%2520a%2520challenge%2520we%2520term%2520task%2520scarcity%252C%2520which%2520has%2520become%2520a%2520key%2520bottleneck%2520for%2520scaling%2520agentic%2520RL.%2520Existing%2520approaches%2520typically%2520assume%2520predefined%2520task%2520collections%252C%2520an%2520assumption%2520that%2520fails%2520in%2520novel%2520environments%2520where%2520tool%2520semantics%2520and%2520affordances%2520are%2520initially%2520unknown.%2520To%2520address%2520this%2520limitation%252C%2520we%2520formalize%2520the%2520problem%2520of%2520Task%2520Generation%2520for%2520Agentic%2520RL%252C%2520where%2520an%2520agent%2520must%2520learn%2520within%2520a%2520given%2520environment%2520that%2520lacks%2520predefined%2520tasks.%2520We%2520propose%2520CuES%252C%2520a%2520Curiosity%2520driven%2520and%2520Environment%2520grounded%2520Synthesis%2520framework%2520that%2520autonomously%2520generates%2520diverse%252C%2520executable%252C%2520and%2520meaningful%2520tasks%2520directly%2520from%2520the%2520environment%2520structure%2520and%2520affordances%252C%2520without%2520relying%2520on%2520handcrafted%2520seeds%2520or%2520external%2520corpora.%2520CuES%2520drives%2520exploration%2520through%2520intrinsic%2520curiosity%252C%2520abstracts%2520interaction%2520patterns%2520into%2520reusable%2520task%2520schemas%252C%2520and%2520refines%2520them%2520through%2520lightweight%2520top%2520down%2520guidance%2520and%2520memory%2520based%2520quality%2520control.%2520Across%2520three%2520representative%2520environments%252C%2520AppWorld%252C%2520BFCL%252C%2520and%2520WebShop%252C%2520CuES%2520produces%2520task%2520distributions%2520that%2520match%2520or%2520surpass%2520manually%2520curated%2520datasets%2520in%2520both%2520diversity%2520and%2520executability%252C%2520yielding%2520substantial%2520downstream%2520policy%2520improvements.%2520These%2520results%2520demonstrate%2520that%2520curiosity%2520driven%252C%2520environment%2520grounded%2520task%2520generation%2520provides%2520a%2520scalable%2520foundation%2520for%2520agents%2520that%2520not%2520only%2520learn%2520how%2520to%2520act%252C%2520but%2520also%2520learn%2520what%2520to%2520learn.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/modelscope/AgentEvolver/tree/main/research/CuES.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01311v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CuES%3A%20A%20Curiosity-driven%20and%20Environment-grounded%20Synthesis%20Framework%20for%20Agentic%20RL&entry.906535625=Shinji%20Mai%20and%20Yunpeng%20Zhai%20and%20Ziqian%20Chen%20and%20Cheng%20Chen%20and%20Anni%20Zou%20and%20Shuchang%20Tao%20and%20Zhaoyang%20Liu%20and%20Bolin%20Ding&entry.1292438233=Large%20language%20model%20based%20agents%20are%20increasingly%20deployed%20in%20complex%2C%20tool%20augmented%20environments.%20While%20reinforcement%20learning%20provides%20a%20principled%20mechanism%20for%20such%20agents%20to%20improve%20through%20interaction%2C%20its%20effectiveness%20critically%20depends%20on%20the%20availability%20of%20structured%20training%20tasks.%20In%20many%20realistic%20settings%2C%20however%2C%20no%20such%20tasks%20exist%20a%20challenge%20we%20term%20task%20scarcity%2C%20which%20has%20become%20a%20key%20bottleneck%20for%20scaling%20agentic%20RL.%20Existing%20approaches%20typically%20assume%20predefined%20task%20collections%2C%20an%20assumption%20that%20fails%20in%20novel%20environments%20where%20tool%20semantics%20and%20affordances%20are%20initially%20unknown.%20To%20address%20this%20limitation%2C%20we%20formalize%20the%20problem%20of%20Task%20Generation%20for%20Agentic%20RL%2C%20where%20an%20agent%20must%20learn%20within%20a%20given%20environment%20that%20lacks%20predefined%20tasks.%20We%20propose%20CuES%2C%20a%20Curiosity%20driven%20and%20Environment%20grounded%20Synthesis%20framework%20that%20autonomously%20generates%20diverse%2C%20executable%2C%20and%20meaningful%20tasks%20directly%20from%20the%20environment%20structure%20and%20affordances%2C%20without%20relying%20on%20handcrafted%20seeds%20or%20external%20corpora.%20CuES%20drives%20exploration%20through%20intrinsic%20curiosity%2C%20abstracts%20interaction%20patterns%20into%20reusable%20task%20schemas%2C%20and%20refines%20them%20through%20lightweight%20top%20down%20guidance%20and%20memory%20based%20quality%20control.%20Across%20three%20representative%20environments%2C%20AppWorld%2C%20BFCL%2C%20and%20WebShop%2C%20CuES%20produces%20task%20distributions%20that%20match%20or%20surpass%20manually%20curated%20datasets%20in%20both%20diversity%20and%20executability%2C%20yielding%20substantial%20downstream%20policy%20improvements.%20These%20results%20demonstrate%20that%20curiosity%20driven%2C%20environment%20grounded%20task%20generation%20provides%20a%20scalable%20foundation%20for%20agents%20that%20not%20only%20learn%20how%20to%20act%2C%20but%20also%20learn%20what%20to%20learn.%20The%20code%20is%20available%20at%20https%3A//github.com/modelscope/AgentEvolver/tree/main/research/CuES.&entry.1838667208=http%3A//arxiv.org/abs/2512.01311v2&entry.124074799=Read"},
{"title": "Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence", "author": "Shuai Yang and Junxin Lin and Yifan Zhou and Ziwei Liu and Chen Change Loy", "abstract": "The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.", "link": "http://arxiv.org/abs/2512.03905v1", "date": "2025-12-03", "relevancy": 2.3048, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6074}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5859}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Video%20Translation%20and%20Editing%20with%20Frame%20Spatial-Temporal%20Correspondence&body=Title%3A%20Zero-Shot%20Video%20Translation%20and%20Editing%20with%20Frame%20Spatial-Temporal%20Correspondence%0AAuthor%3A%20Shuai%20Yang%20and%20Junxin%20Lin%20and%20Yifan%20Zhou%20and%20Ziwei%20Liu%20and%20Chen%20Change%20Loy%0AAbstract%3A%20The%20remarkable%20success%20in%20text-to-image%20diffusion%20models%20has%20motivated%20extensive%20investigation%20of%20their%20potential%20for%20video%20applications.%20Zero-shot%20techniques%20aim%20to%20adapt%20image%20diffusion%20models%20for%20videos%20without%20requiring%20further%20model%20training.%20Recent%20methods%20largely%20emphasize%20integrating%20inter-frame%20correspondence%20into%20attention%20mechanisms.%20However%2C%20the%20soft%20constraint%20applied%20to%20identify%20the%20valid%20features%20to%20attend%20is%20insufficient%2C%20which%20could%20lead%20to%20temporal%20inconsistency.%20In%20this%20paper%2C%20we%20present%20FRESCO%2C%20which%20integrates%20intra-frame%20correspondence%20with%20inter-frame%20correspondence%20to%20formulate%20a%20more%20robust%20spatial-temporal%20constraint.%20This%20enhancement%20ensures%20a%20consistent%20transformation%20of%20semantically%20similar%20content%20between%20frames.%20Our%20method%20goes%20beyond%20attention%20guidance%20to%20explicitly%20optimize%20features%2C%20achieving%20high%20spatial-temporal%20consistency%20with%20the%20input%20video%2C%20significantly%20enhancing%20the%20visual%20coherence%20of%20manipulated%20videos.%20We%20verify%20FRESCO%20adaptations%20on%20two%20zero-shot%20tasks%20of%20video-to-video%20translation%20and%20text-guided%20video%20editing.%20Comprehensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20framework%20in%20generating%20high-quality%2C%20coherent%20videos%2C%20highlighting%20a%20significant%20advance%20over%20current%20zero-shot%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Video%2520Translation%2520and%2520Editing%2520with%2520Frame%2520Spatial-Temporal%2520Correspondence%26entry.906535625%3DShuai%2520Yang%2520and%2520Junxin%2520Lin%2520and%2520Yifan%2520Zhou%2520and%2520Ziwei%2520Liu%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3DThe%2520remarkable%2520success%2520in%2520text-to-image%2520diffusion%2520models%2520has%2520motivated%2520extensive%2520investigation%2520of%2520their%2520potential%2520for%2520video%2520applications.%2520Zero-shot%2520techniques%2520aim%2520to%2520adapt%2520image%2520diffusion%2520models%2520for%2520videos%2520without%2520requiring%2520further%2520model%2520training.%2520Recent%2520methods%2520largely%2520emphasize%2520integrating%2520inter-frame%2520correspondence%2520into%2520attention%2520mechanisms.%2520However%252C%2520the%2520soft%2520constraint%2520applied%2520to%2520identify%2520the%2520valid%2520features%2520to%2520attend%2520is%2520insufficient%252C%2520which%2520could%2520lead%2520to%2520temporal%2520inconsistency.%2520In%2520this%2520paper%252C%2520we%2520present%2520FRESCO%252C%2520which%2520integrates%2520intra-frame%2520correspondence%2520with%2520inter-frame%2520correspondence%2520to%2520formulate%2520a%2520more%2520robust%2520spatial-temporal%2520constraint.%2520This%2520enhancement%2520ensures%2520a%2520consistent%2520transformation%2520of%2520semantically%2520similar%2520content%2520between%2520frames.%2520Our%2520method%2520goes%2520beyond%2520attention%2520guidance%2520to%2520explicitly%2520optimize%2520features%252C%2520achieving%2520high%2520spatial-temporal%2520consistency%2520with%2520the%2520input%2520video%252C%2520significantly%2520enhancing%2520the%2520visual%2520coherence%2520of%2520manipulated%2520videos.%2520We%2520verify%2520FRESCO%2520adaptations%2520on%2520two%2520zero-shot%2520tasks%2520of%2520video-to-video%2520translation%2520and%2520text-guided%2520video%2520editing.%2520Comprehensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520framework%2520in%2520generating%2520high-quality%252C%2520coherent%2520videos%252C%2520highlighting%2520a%2520significant%2520advance%2520over%2520current%2520zero-shot%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Video%20Translation%20and%20Editing%20with%20Frame%20Spatial-Temporal%20Correspondence&entry.906535625=Shuai%20Yang%20and%20Junxin%20Lin%20and%20Yifan%20Zhou%20and%20Ziwei%20Liu%20and%20Chen%20Change%20Loy&entry.1292438233=The%20remarkable%20success%20in%20text-to-image%20diffusion%20models%20has%20motivated%20extensive%20investigation%20of%20their%20potential%20for%20video%20applications.%20Zero-shot%20techniques%20aim%20to%20adapt%20image%20diffusion%20models%20for%20videos%20without%20requiring%20further%20model%20training.%20Recent%20methods%20largely%20emphasize%20integrating%20inter-frame%20correspondence%20into%20attention%20mechanisms.%20However%2C%20the%20soft%20constraint%20applied%20to%20identify%20the%20valid%20features%20to%20attend%20is%20insufficient%2C%20which%20could%20lead%20to%20temporal%20inconsistency.%20In%20this%20paper%2C%20we%20present%20FRESCO%2C%20which%20integrates%20intra-frame%20correspondence%20with%20inter-frame%20correspondence%20to%20formulate%20a%20more%20robust%20spatial-temporal%20constraint.%20This%20enhancement%20ensures%20a%20consistent%20transformation%20of%20semantically%20similar%20content%20between%20frames.%20Our%20method%20goes%20beyond%20attention%20guidance%20to%20explicitly%20optimize%20features%2C%20achieving%20high%20spatial-temporal%20consistency%20with%20the%20input%20video%2C%20significantly%20enhancing%20the%20visual%20coherence%20of%20manipulated%20videos.%20We%20verify%20FRESCO%20adaptations%20on%20two%20zero-shot%20tasks%20of%20video-to-video%20translation%20and%20text-guided%20video%20editing.%20Comprehensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20framework%20in%20generating%20high-quality%2C%20coherent%20videos%2C%20highlighting%20a%20significant%20advance%20over%20current%20zero-shot%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.03905v1&entry.124074799=Read"},
{"title": "Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models", "author": "Marlon Steiner and Royden Wagner and \u00d6mer Sahin Tas and Christoph Stiller", "abstract": "Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle's intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.", "link": "http://arxiv.org/abs/2512.03756v1", "date": "2025-12-03", "relevancy": 2.2735, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5996}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5843}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction-Driven%20Motion%20Planning%3A%20Route%20Integration%20Strategies%20in%20Attention-Based%20Prediction%20Models&body=Title%3A%20Prediction-Driven%20Motion%20Planning%3A%20Route%20Integration%20Strategies%20in%20Attention-Based%20Prediction%20Models%0AAuthor%3A%20Marlon%20Steiner%20and%20Royden%20Wagner%20and%20%C3%96mer%20Sahin%20Tas%20and%20Christoph%20Stiller%0AAbstract%3A%20Combining%20motion%20prediction%20and%20motion%20planning%20offers%20a%20promising%20framework%20for%20enhancing%20interactions%20between%20automated%20vehicles%20and%20other%20traffic%20participants.%20However%2C%20this%20introduces%20challenges%20in%20conditioning%20predictions%20on%20navigation%20goals%20and%20ensuring%20stable%2C%20kinematically%20feasible%20trajectories.%20Addressing%20the%20former%20challenge%2C%20this%20paper%20investigates%20the%20extension%20of%20attention-based%20motion%20prediction%20models%20with%20navigation%20information.%20By%20integrating%20the%20ego%20vehicle%27s%20intended%20route%20and%20goal%20pose%20into%20the%20model%20architecture%2C%20we%20bridge%20the%20gap%20between%20multi-agent%20motion%20prediction%20and%20goal-based%20motion%20planning.%20We%20propose%20and%20evaluate%20several%20architectural%20navigation%20integration%20strategies%20to%20our%20model%20on%20the%20nuPlan%20dataset.%20Our%20results%20demonstrate%20the%20potential%20of%20prediction-driven%20motion%20planning%2C%20highlighting%20how%20navigation%20information%20can%20enhance%20both%20prediction%20and%20planning%20tasks.%20Our%20implementation%20is%20at%3A%20https%3A//github.com/KIT-MRT/future-motion.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction-Driven%2520Motion%2520Planning%253A%2520Route%2520Integration%2520Strategies%2520in%2520Attention-Based%2520Prediction%2520Models%26entry.906535625%3DMarlon%2520Steiner%2520and%2520Royden%2520Wagner%2520and%2520%25C3%2596mer%2520Sahin%2520Tas%2520and%2520Christoph%2520Stiller%26entry.1292438233%3DCombining%2520motion%2520prediction%2520and%2520motion%2520planning%2520offers%2520a%2520promising%2520framework%2520for%2520enhancing%2520interactions%2520between%2520automated%2520vehicles%2520and%2520other%2520traffic%2520participants.%2520However%252C%2520this%2520introduces%2520challenges%2520in%2520conditioning%2520predictions%2520on%2520navigation%2520goals%2520and%2520ensuring%2520stable%252C%2520kinematically%2520feasible%2520trajectories.%2520Addressing%2520the%2520former%2520challenge%252C%2520this%2520paper%2520investigates%2520the%2520extension%2520of%2520attention-based%2520motion%2520prediction%2520models%2520with%2520navigation%2520information.%2520By%2520integrating%2520the%2520ego%2520vehicle%2527s%2520intended%2520route%2520and%2520goal%2520pose%2520into%2520the%2520model%2520architecture%252C%2520we%2520bridge%2520the%2520gap%2520between%2520multi-agent%2520motion%2520prediction%2520and%2520goal-based%2520motion%2520planning.%2520We%2520propose%2520and%2520evaluate%2520several%2520architectural%2520navigation%2520integration%2520strategies%2520to%2520our%2520model%2520on%2520the%2520nuPlan%2520dataset.%2520Our%2520results%2520demonstrate%2520the%2520potential%2520of%2520prediction-driven%2520motion%2520planning%252C%2520highlighting%2520how%2520navigation%2520information%2520can%2520enhance%2520both%2520prediction%2520and%2520planning%2520tasks.%2520Our%2520implementation%2520is%2520at%253A%2520https%253A//github.com/KIT-MRT/future-motion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction-Driven%20Motion%20Planning%3A%20Route%20Integration%20Strategies%20in%20Attention-Based%20Prediction%20Models&entry.906535625=Marlon%20Steiner%20and%20Royden%20Wagner%20and%20%C3%96mer%20Sahin%20Tas%20and%20Christoph%20Stiller&entry.1292438233=Combining%20motion%20prediction%20and%20motion%20planning%20offers%20a%20promising%20framework%20for%20enhancing%20interactions%20between%20automated%20vehicles%20and%20other%20traffic%20participants.%20However%2C%20this%20introduces%20challenges%20in%20conditioning%20predictions%20on%20navigation%20goals%20and%20ensuring%20stable%2C%20kinematically%20feasible%20trajectories.%20Addressing%20the%20former%20challenge%2C%20this%20paper%20investigates%20the%20extension%20of%20attention-based%20motion%20prediction%20models%20with%20navigation%20information.%20By%20integrating%20the%20ego%20vehicle%27s%20intended%20route%20and%20goal%20pose%20into%20the%20model%20architecture%2C%20we%20bridge%20the%20gap%20between%20multi-agent%20motion%20prediction%20and%20goal-based%20motion%20planning.%20We%20propose%20and%20evaluate%20several%20architectural%20navigation%20integration%20strategies%20to%20our%20model%20on%20the%20nuPlan%20dataset.%20Our%20results%20demonstrate%20the%20potential%20of%20prediction-driven%20motion%20planning%2C%20highlighting%20how%20navigation%20information%20can%20enhance%20both%20prediction%20and%20planning%20tasks.%20Our%20implementation%20is%20at%3A%20https%3A//github.com/KIT-MRT/future-motion.&entry.1838667208=http%3A//arxiv.org/abs/2512.03756v1&entry.124074799=Read"},
{"title": "TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning", "author": "Tao Wu and Li Yang and Gen Zhan and Yiting Liao and Junlin Li and Deliang Fu and Li Zhang and Limin Wang", "abstract": "Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.", "link": "http://arxiv.org/abs/2512.03963v1", "date": "2025-12-03", "relevancy": 2.2601, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5848}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.57}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TempR1%3A%20Improving%20Temporal%20Understanding%20of%20MLLMs%20via%20Temporal-Aware%20Multi-Task%20Reinforcement%20Learning&body=Title%3A%20TempR1%3A%20Improving%20Temporal%20Understanding%20of%20MLLMs%20via%20Temporal-Aware%20Multi-Task%20Reinforcement%20Learning%0AAuthor%3A%20Tao%20Wu%20and%20Li%20Yang%20and%20Gen%20Zhan%20and%20Yiting%20Liao%20and%20Junlin%20Li%20and%20Deliang%20Fu%20and%20Li%20Zhang%20and%20Limin%20Wang%0AAbstract%3A%20Enhancing%20the%20temporal%20understanding%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20is%20essential%20for%20advancing%20long-form%20video%20analysis%2C%20enabling%20tasks%20such%20as%20temporal%20localization%2C%20action%20detection%2C%20and%20time-sensitive%20question%20answering.%20While%20reinforcement%20learning%20%28RL%29%20has%20recently%20been%20explored%20for%20improving%20temporal%20reasoning%2C%20existing%20approaches%20are%20often%20confined%20to%20limited%20task%20types%20and%20data%2C%20restricting%20their%20generalization%20across%20diverse%20temporal%20understanding%20scenarios.%20To%20address%20this%20challenge%2C%20we%20present%20TempR1%2C%20a%20temporal-aware%20multi-task%20reinforcement%20learning%20framework%20that%20systematically%20strengthens%20MLLMs%27%20temporal%20comprehension.%20We%20curate%20a%20multi-task%20corpus%20that%20exposes%20the%20model%20to%20diverse%20temporal%20structures%20and%20semantics%2C%20and%20build%20upon%20the%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20algorithm%20to%20achieve%20stable%20and%20effective%20cross-task%20optimization.%20Specifically%2C%20we%20categorize%20temporal%20tasks%20into%20three%20correspondence%20types%20between%20predicted%20intervals%20and%20ground-truth%20instances%2C%20and%20design%20tailored%20localization%20rewards%20for%20each%2C%20enabling%20TempR1%20to%20capture%20fine-grained%20temporal%20dependencies%20and%20adapt%20to%20different%20temporal%20patterns.%20Extensive%20experiments%20demonstrate%20that%20TempR1%20attains%20state-of-the-art%20performance%20across%20multiple%20benchmarks.%20Moreover%2C%20its%20joint%20optimization%20over%20complementary%20tasks%20yields%20a%20strong%20synergistic%20effect%2C%20enhancing%20both%20generalization%20and%20single-task%20performance%2C%20establishing%20a%20scalable%20and%20principled%20paradigm%20for%20temporal%20reasoning%20in%20MLLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempR1%253A%2520Improving%2520Temporal%2520Understanding%2520of%2520MLLMs%2520via%2520Temporal-Aware%2520Multi-Task%2520Reinforcement%2520Learning%26entry.906535625%3DTao%2520Wu%2520and%2520Li%2520Yang%2520and%2520Gen%2520Zhan%2520and%2520Yiting%2520Liao%2520and%2520Junlin%2520Li%2520and%2520Deliang%2520Fu%2520and%2520Li%2520Zhang%2520and%2520Limin%2520Wang%26entry.1292438233%3DEnhancing%2520the%2520temporal%2520understanding%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520is%2520essential%2520for%2520advancing%2520long-form%2520video%2520analysis%252C%2520enabling%2520tasks%2520such%2520as%2520temporal%2520localization%252C%2520action%2520detection%252C%2520and%2520time-sensitive%2520question%2520answering.%2520While%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520recently%2520been%2520explored%2520for%2520improving%2520temporal%2520reasoning%252C%2520existing%2520approaches%2520are%2520often%2520confined%2520to%2520limited%2520task%2520types%2520and%2520data%252C%2520restricting%2520their%2520generalization%2520across%2520diverse%2520temporal%2520understanding%2520scenarios.%2520To%2520address%2520this%2520challenge%252C%2520we%2520present%2520TempR1%252C%2520a%2520temporal-aware%2520multi-task%2520reinforcement%2520learning%2520framework%2520that%2520systematically%2520strengthens%2520MLLMs%2527%2520temporal%2520comprehension.%2520We%2520curate%2520a%2520multi-task%2520corpus%2520that%2520exposes%2520the%2520model%2520to%2520diverse%2520temporal%2520structures%2520and%2520semantics%252C%2520and%2520build%2520upon%2520the%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520algorithm%2520to%2520achieve%2520stable%2520and%2520effective%2520cross-task%2520optimization.%2520Specifically%252C%2520we%2520categorize%2520temporal%2520tasks%2520into%2520three%2520correspondence%2520types%2520between%2520predicted%2520intervals%2520and%2520ground-truth%2520instances%252C%2520and%2520design%2520tailored%2520localization%2520rewards%2520for%2520each%252C%2520enabling%2520TempR1%2520to%2520capture%2520fine-grained%2520temporal%2520dependencies%2520and%2520adapt%2520to%2520different%2520temporal%2520patterns.%2520Extensive%2520experiments%2520demonstrate%2520that%2520TempR1%2520attains%2520state-of-the-art%2520performance%2520across%2520multiple%2520benchmarks.%2520Moreover%252C%2520its%2520joint%2520optimization%2520over%2520complementary%2520tasks%2520yields%2520a%2520strong%2520synergistic%2520effect%252C%2520enhancing%2520both%2520generalization%2520and%2520single-task%2520performance%252C%2520establishing%2520a%2520scalable%2520and%2520principled%2520paradigm%2520for%2520temporal%2520reasoning%2520in%2520MLLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TempR1%3A%20Improving%20Temporal%20Understanding%20of%20MLLMs%20via%20Temporal-Aware%20Multi-Task%20Reinforcement%20Learning&entry.906535625=Tao%20Wu%20and%20Li%20Yang%20and%20Gen%20Zhan%20and%20Yiting%20Liao%20and%20Junlin%20Li%20and%20Deliang%20Fu%20and%20Li%20Zhang%20and%20Limin%20Wang&entry.1292438233=Enhancing%20the%20temporal%20understanding%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20is%20essential%20for%20advancing%20long-form%20video%20analysis%2C%20enabling%20tasks%20such%20as%20temporal%20localization%2C%20action%20detection%2C%20and%20time-sensitive%20question%20answering.%20While%20reinforcement%20learning%20%28RL%29%20has%20recently%20been%20explored%20for%20improving%20temporal%20reasoning%2C%20existing%20approaches%20are%20often%20confined%20to%20limited%20task%20types%20and%20data%2C%20restricting%20their%20generalization%20across%20diverse%20temporal%20understanding%20scenarios.%20To%20address%20this%20challenge%2C%20we%20present%20TempR1%2C%20a%20temporal-aware%20multi-task%20reinforcement%20learning%20framework%20that%20systematically%20strengthens%20MLLMs%27%20temporal%20comprehension.%20We%20curate%20a%20multi-task%20corpus%20that%20exposes%20the%20model%20to%20diverse%20temporal%20structures%20and%20semantics%2C%20and%20build%20upon%20the%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20algorithm%20to%20achieve%20stable%20and%20effective%20cross-task%20optimization.%20Specifically%2C%20we%20categorize%20temporal%20tasks%20into%20three%20correspondence%20types%20between%20predicted%20intervals%20and%20ground-truth%20instances%2C%20and%20design%20tailored%20localization%20rewards%20for%20each%2C%20enabling%20TempR1%20to%20capture%20fine-grained%20temporal%20dependencies%20and%20adapt%20to%20different%20temporal%20patterns.%20Extensive%20experiments%20demonstrate%20that%20TempR1%20attains%20state-of-the-art%20performance%20across%20multiple%20benchmarks.%20Moreover%2C%20its%20joint%20optimization%20over%20complementary%20tasks%20yields%20a%20strong%20synergistic%20effect%2C%20enhancing%20both%20generalization%20and%20single-task%20performance%2C%20establishing%20a%20scalable%20and%20principled%20paradigm%20for%20temporal%20reasoning%20in%20MLLMs.&entry.1838667208=http%3A//arxiv.org/abs/2512.03963v1&entry.124074799=Read"},
{"title": "Emergent Outlier View Rejection in Visual Geometry Grounded Transformers", "author": "Jisang Han and Sunghwan Hong and Jaewoo Jung and Wooseok Jang and Honggyu An and Qianqian Wang and Seungryong Kim and Chen Feng", "abstract": "Reliable 3D reconstruction from in-the-wild image collections is often hindered by \"noisy\" images-irrelevant inputs with little or no view overlap with others. While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions. In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images. Through an in-depth analysis under varying proportions of synthetic distractors, we identify a specific layer that naturally exhibits outlier-suppressing behavior. Further probing reveals that this layer encodes discriminative internal representations that enable an effective noise-filtering capability, which we simply leverage to perform outlier-view rejection in feed-forward 3D reconstruction without any additional fine-tuning or supervision. Extensive experiments on both controlled and in-the-wild datasets demonstrate that this implicit filtering mechanism is consistent and generalizes well across diverse scenarios.", "link": "http://arxiv.org/abs/2512.04012v1", "date": "2025-12-03", "relevancy": 2.258, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5733}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5655}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20Outlier%20View%20Rejection%20in%20Visual%20Geometry%20Grounded%20Transformers&body=Title%3A%20Emergent%20Outlier%20View%20Rejection%20in%20Visual%20Geometry%20Grounded%20Transformers%0AAuthor%3A%20Jisang%20Han%20and%20Sunghwan%20Hong%20and%20Jaewoo%20Jung%20and%20Wooseok%20Jang%20and%20Honggyu%20An%20and%20Qianqian%20Wang%20and%20Seungryong%20Kim%20and%20Chen%20Feng%0AAbstract%3A%20Reliable%203D%20reconstruction%20from%20in-the-wild%20image%20collections%20is%20often%20hindered%20by%20%22noisy%22%20images-irrelevant%20inputs%20with%20little%20or%20no%20view%20overlap%20with%20others.%20While%20traditional%20Structure-from-Motion%20pipelines%20handle%20such%20cases%20through%20geometric%20verification%20and%20outlier%20rejection%2C%20feed-forward%203D%20reconstruction%20models%20lack%20these%20explicit%20mechanisms%2C%20leading%20to%20degraded%20performance%20under%20in-the-wild%20conditions.%20In%20this%20paper%2C%20we%20discover%20that%20the%20existing%20feed-forward%20reconstruction%20model%2C%20e.g.%2C%20VGGT%2C%20despite%20lacking%20explicit%20outlier-rejection%20mechanisms%20or%20noise-aware%20training%2C%20can%20inherently%20distinguish%20distractor%20images.%20Through%20an%20in-depth%20analysis%20under%20varying%20proportions%20of%20synthetic%20distractors%2C%20we%20identify%20a%20specific%20layer%20that%20naturally%20exhibits%20outlier-suppressing%20behavior.%20Further%20probing%20reveals%20that%20this%20layer%20encodes%20discriminative%20internal%20representations%20that%20enable%20an%20effective%20noise-filtering%20capability%2C%20which%20we%20simply%20leverage%20to%20perform%20outlier-view%20rejection%20in%20feed-forward%203D%20reconstruction%20without%20any%20additional%20fine-tuning%20or%20supervision.%20Extensive%20experiments%20on%20both%20controlled%20and%20in-the-wild%20datasets%20demonstrate%20that%20this%20implicit%20filtering%20mechanism%20is%20consistent%20and%20generalizes%20well%20across%20diverse%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520Outlier%2520View%2520Rejection%2520in%2520Visual%2520Geometry%2520Grounded%2520Transformers%26entry.906535625%3DJisang%2520Han%2520and%2520Sunghwan%2520Hong%2520and%2520Jaewoo%2520Jung%2520and%2520Wooseok%2520Jang%2520and%2520Honggyu%2520An%2520and%2520Qianqian%2520Wang%2520and%2520Seungryong%2520Kim%2520and%2520Chen%2520Feng%26entry.1292438233%3DReliable%25203D%2520reconstruction%2520from%2520in-the-wild%2520image%2520collections%2520is%2520often%2520hindered%2520by%2520%2522noisy%2522%2520images-irrelevant%2520inputs%2520with%2520little%2520or%2520no%2520view%2520overlap%2520with%2520others.%2520While%2520traditional%2520Structure-from-Motion%2520pipelines%2520handle%2520such%2520cases%2520through%2520geometric%2520verification%2520and%2520outlier%2520rejection%252C%2520feed-forward%25203D%2520reconstruction%2520models%2520lack%2520these%2520explicit%2520mechanisms%252C%2520leading%2520to%2520degraded%2520performance%2520under%2520in-the-wild%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520discover%2520that%2520the%2520existing%2520feed-forward%2520reconstruction%2520model%252C%2520e.g.%252C%2520VGGT%252C%2520despite%2520lacking%2520explicit%2520outlier-rejection%2520mechanisms%2520or%2520noise-aware%2520training%252C%2520can%2520inherently%2520distinguish%2520distractor%2520images.%2520Through%2520an%2520in-depth%2520analysis%2520under%2520varying%2520proportions%2520of%2520synthetic%2520distractors%252C%2520we%2520identify%2520a%2520specific%2520layer%2520that%2520naturally%2520exhibits%2520outlier-suppressing%2520behavior.%2520Further%2520probing%2520reveals%2520that%2520this%2520layer%2520encodes%2520discriminative%2520internal%2520representations%2520that%2520enable%2520an%2520effective%2520noise-filtering%2520capability%252C%2520which%2520we%2520simply%2520leverage%2520to%2520perform%2520outlier-view%2520rejection%2520in%2520feed-forward%25203D%2520reconstruction%2520without%2520any%2520additional%2520fine-tuning%2520or%2520supervision.%2520Extensive%2520experiments%2520on%2520both%2520controlled%2520and%2520in-the-wild%2520datasets%2520demonstrate%2520that%2520this%2520implicit%2520filtering%2520mechanism%2520is%2520consistent%2520and%2520generalizes%2520well%2520across%2520diverse%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20Outlier%20View%20Rejection%20in%20Visual%20Geometry%20Grounded%20Transformers&entry.906535625=Jisang%20Han%20and%20Sunghwan%20Hong%20and%20Jaewoo%20Jung%20and%20Wooseok%20Jang%20and%20Honggyu%20An%20and%20Qianqian%20Wang%20and%20Seungryong%20Kim%20and%20Chen%20Feng&entry.1292438233=Reliable%203D%20reconstruction%20from%20in-the-wild%20image%20collections%20is%20often%20hindered%20by%20%22noisy%22%20images-irrelevant%20inputs%20with%20little%20or%20no%20view%20overlap%20with%20others.%20While%20traditional%20Structure-from-Motion%20pipelines%20handle%20such%20cases%20through%20geometric%20verification%20and%20outlier%20rejection%2C%20feed-forward%203D%20reconstruction%20models%20lack%20these%20explicit%20mechanisms%2C%20leading%20to%20degraded%20performance%20under%20in-the-wild%20conditions.%20In%20this%20paper%2C%20we%20discover%20that%20the%20existing%20feed-forward%20reconstruction%20model%2C%20e.g.%2C%20VGGT%2C%20despite%20lacking%20explicit%20outlier-rejection%20mechanisms%20or%20noise-aware%20training%2C%20can%20inherently%20distinguish%20distractor%20images.%20Through%20an%20in-depth%20analysis%20under%20varying%20proportions%20of%20synthetic%20distractors%2C%20we%20identify%20a%20specific%20layer%20that%20naturally%20exhibits%20outlier-suppressing%20behavior.%20Further%20probing%20reveals%20that%20this%20layer%20encodes%20discriminative%20internal%20representations%20that%20enable%20an%20effective%20noise-filtering%20capability%2C%20which%20we%20simply%20leverage%20to%20perform%20outlier-view%20rejection%20in%20feed-forward%203D%20reconstruction%20without%20any%20additional%20fine-tuning%20or%20supervision.%20Extensive%20experiments%20on%20both%20controlled%20and%20in-the-wild%20datasets%20demonstrate%20that%20this%20implicit%20filtering%20mechanism%20is%20consistent%20and%20generalizes%20well%20across%20diverse%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.04012v1&entry.124074799=Read"},
{"title": "Learning Group Actions In Disentangled Latent Image Representations", "author": "Farhana Hossain Swarnali and Miaomiao Zhang and Tonmoy Hossain", "abstract": "Modeling group actions on latent representations enables controllable transformations of high-dimensional image data. Prior works applying group-theoretic priors or modeling transformations typically operate in the high-dimensional data space, where group actions apply uniformly across the entire input, making it difficult to disentangle the subspace that varies under transformations. While latent-space methods offer greater flexibility, they still require manual partitioning of latent variables into equivariant and invariant subspaces, limiting the ability to robustly learn and operate group actions within the representation space. To address this, we introduce a novel end-to-end framework that for the first time learns group actions on latent image manifolds, automatically discovering transformation-relevant structures without manual intervention. Our method uses learnable binary masks with straight-through estimation to dynamically partition latent representations into transformation-sensitive and invariant components. We formulate this within a unified optimization framework that jointly learns latent disentanglement and group transformation mappings. The framework can be seamlessly integrated with any standard encoder-decoder architecture. We validate our approach on five 2D/3D image datasets, demonstrating its ability to automatically learn disentangled latent factors for group actions in diverse data, while downstream classification tasks confirm the effectiveness of the learned representations. Our code is publicly available at https://github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations .", "link": "http://arxiv.org/abs/2512.04015v1", "date": "2025-12-03", "relevancy": 2.2549, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5725}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5672}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Group%20Actions%20In%20Disentangled%20Latent%20Image%20Representations&body=Title%3A%20Learning%20Group%20Actions%20In%20Disentangled%20Latent%20Image%20Representations%0AAuthor%3A%20Farhana%20Hossain%20Swarnali%20and%20Miaomiao%20Zhang%20and%20Tonmoy%20Hossain%0AAbstract%3A%20Modeling%20group%20actions%20on%20latent%20representations%20enables%20controllable%20transformations%20of%20high-dimensional%20image%20data.%20Prior%20works%20applying%20group-theoretic%20priors%20or%20modeling%20transformations%20typically%20operate%20in%20the%20high-dimensional%20data%20space%2C%20where%20group%20actions%20apply%20uniformly%20across%20the%20entire%20input%2C%20making%20it%20difficult%20to%20disentangle%20the%20subspace%20that%20varies%20under%20transformations.%20While%20latent-space%20methods%20offer%20greater%20flexibility%2C%20they%20still%20require%20manual%20partitioning%20of%20latent%20variables%20into%20equivariant%20and%20invariant%20subspaces%2C%20limiting%20the%20ability%20to%20robustly%20learn%20and%20operate%20group%20actions%20within%20the%20representation%20space.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20end-to-end%20framework%20that%20for%20the%20first%20time%20learns%20group%20actions%20on%20latent%20image%20manifolds%2C%20automatically%20discovering%20transformation-relevant%20structures%20without%20manual%20intervention.%20Our%20method%20uses%20learnable%20binary%20masks%20with%20straight-through%20estimation%20to%20dynamically%20partition%20latent%20representations%20into%20transformation-sensitive%20and%20invariant%20components.%20We%20formulate%20this%20within%20a%20unified%20optimization%20framework%20that%20jointly%20learns%20latent%20disentanglement%20and%20group%20transformation%20mappings.%20The%20framework%20can%20be%20seamlessly%20integrated%20with%20any%20standard%20encoder-decoder%20architecture.%20We%20validate%20our%20approach%20on%20five%202D/3D%20image%20datasets%2C%20demonstrating%20its%20ability%20to%20automatically%20learn%20disentangled%20latent%20factors%20for%20group%20actions%20in%20diverse%20data%2C%20while%20downstream%20classification%20tasks%20confirm%20the%20effectiveness%20of%20the%20learned%20representations.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Group%2520Actions%2520In%2520Disentangled%2520Latent%2520Image%2520Representations%26entry.906535625%3DFarhana%2520Hossain%2520Swarnali%2520and%2520Miaomiao%2520Zhang%2520and%2520Tonmoy%2520Hossain%26entry.1292438233%3DModeling%2520group%2520actions%2520on%2520latent%2520representations%2520enables%2520controllable%2520transformations%2520of%2520high-dimensional%2520image%2520data.%2520Prior%2520works%2520applying%2520group-theoretic%2520priors%2520or%2520modeling%2520transformations%2520typically%2520operate%2520in%2520the%2520high-dimensional%2520data%2520space%252C%2520where%2520group%2520actions%2520apply%2520uniformly%2520across%2520the%2520entire%2520input%252C%2520making%2520it%2520difficult%2520to%2520disentangle%2520the%2520subspace%2520that%2520varies%2520under%2520transformations.%2520While%2520latent-space%2520methods%2520offer%2520greater%2520flexibility%252C%2520they%2520still%2520require%2520manual%2520partitioning%2520of%2520latent%2520variables%2520into%2520equivariant%2520and%2520invariant%2520subspaces%252C%2520limiting%2520the%2520ability%2520to%2520robustly%2520learn%2520and%2520operate%2520group%2520actions%2520within%2520the%2520representation%2520space.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%2520end-to-end%2520framework%2520that%2520for%2520the%2520first%2520time%2520learns%2520group%2520actions%2520on%2520latent%2520image%2520manifolds%252C%2520automatically%2520discovering%2520transformation-relevant%2520structures%2520without%2520manual%2520intervention.%2520Our%2520method%2520uses%2520learnable%2520binary%2520masks%2520with%2520straight-through%2520estimation%2520to%2520dynamically%2520partition%2520latent%2520representations%2520into%2520transformation-sensitive%2520and%2520invariant%2520components.%2520We%2520formulate%2520this%2520within%2520a%2520unified%2520optimization%2520framework%2520that%2520jointly%2520learns%2520latent%2520disentanglement%2520and%2520group%2520transformation%2520mappings.%2520The%2520framework%2520can%2520be%2520seamlessly%2520integrated%2520with%2520any%2520standard%2520encoder-decoder%2520architecture.%2520We%2520validate%2520our%2520approach%2520on%2520five%25202D/3D%2520image%2520datasets%252C%2520demonstrating%2520its%2520ability%2520to%2520automatically%2520learn%2520disentangled%2520latent%2520factors%2520for%2520group%2520actions%2520in%2520diverse%2520data%252C%2520while%2520downstream%2520classification%2520tasks%2520confirm%2520the%2520effectiveness%2520of%2520the%2520learned%2520representations.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Group%20Actions%20In%20Disentangled%20Latent%20Image%20Representations&entry.906535625=Farhana%20Hossain%20Swarnali%20and%20Miaomiao%20Zhang%20and%20Tonmoy%20Hossain&entry.1292438233=Modeling%20group%20actions%20on%20latent%20representations%20enables%20controllable%20transformations%20of%20high-dimensional%20image%20data.%20Prior%20works%20applying%20group-theoretic%20priors%20or%20modeling%20transformations%20typically%20operate%20in%20the%20high-dimensional%20data%20space%2C%20where%20group%20actions%20apply%20uniformly%20across%20the%20entire%20input%2C%20making%20it%20difficult%20to%20disentangle%20the%20subspace%20that%20varies%20under%20transformations.%20While%20latent-space%20methods%20offer%20greater%20flexibility%2C%20they%20still%20require%20manual%20partitioning%20of%20latent%20variables%20into%20equivariant%20and%20invariant%20subspaces%2C%20limiting%20the%20ability%20to%20robustly%20learn%20and%20operate%20group%20actions%20within%20the%20representation%20space.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20end-to-end%20framework%20that%20for%20the%20first%20time%20learns%20group%20actions%20on%20latent%20image%20manifolds%2C%20automatically%20discovering%20transformation-relevant%20structures%20without%20manual%20intervention.%20Our%20method%20uses%20learnable%20binary%20masks%20with%20straight-through%20estimation%20to%20dynamically%20partition%20latent%20representations%20into%20transformation-sensitive%20and%20invariant%20components.%20We%20formulate%20this%20within%20a%20unified%20optimization%20framework%20that%20jointly%20learns%20latent%20disentanglement%20and%20group%20transformation%20mappings.%20The%20framework%20can%20be%20seamlessly%20integrated%20with%20any%20standard%20encoder-decoder%20architecture.%20We%20validate%20our%20approach%20on%20five%202D/3D%20image%20datasets%2C%20demonstrating%20its%20ability%20to%20automatically%20learn%20disentangled%20latent%20factors%20for%20group%20actions%20in%20diverse%20data%2C%20while%20downstream%20classification%20tasks%20confirm%20the%20effectiveness%20of%20the%20learned%20representations.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.04015v1&entry.124074799=Read"},
{"title": "Dual-level Modality Debiasing Learning for Unsupervised Visible-Infrared Person Re-Identification", "author": "Jiaze Li and Yan Lu and Bin Liu and Guojun Yin and Mang Ye", "abstract": "Two-stage learning pipeline has achieved promising results in unsupervised visible-infrared person re-identification (USL-VI-ReID). It first performs single-modality learning and then operates cross-modality learning to tackle the modality discrepancy. Although promising, this pipeline inevitably introduces modality bias: modality-specific cues learned in the single-modality training naturally propagate into the following cross-modality learning, impairing identity discrimination and generalization. To address this issue, we propose a Dual-level Modality Debiasing Learning (DMDL) framework that implements debiasing at both the model and optimization levels. At the model level, we propose a Causality-inspired Adjustment Intervention (CAI) module that replaces likelihood-based modeling with causal modeling, preventing modality-induced spurious patterns from being introduced, leading to a low-biased model. At the optimization level, a Collaborative Bias-free Training (CBT) strategy is introduced to interrupt the propagation of modality bias across data, labels, and features by integrating modality-specific augmentation, label refinement, and feature alignment. Extensive experiments on benchmark datasets demonstrate that DMDL could enable modality-invariant feature learning and a more generalized model.", "link": "http://arxiv.org/abs/2512.03745v1", "date": "2025-12-03", "relevancy": 2.242, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5616}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5606}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-level%20Modality%20Debiasing%20Learning%20for%20Unsupervised%20Visible-Infrared%20Person%20Re-Identification&body=Title%3A%20Dual-level%20Modality%20Debiasing%20Learning%20for%20Unsupervised%20Visible-Infrared%20Person%20Re-Identification%0AAuthor%3A%20Jiaze%20Li%20and%20Yan%20Lu%20and%20Bin%20Liu%20and%20Guojun%20Yin%20and%20Mang%20Ye%0AAbstract%3A%20Two-stage%20learning%20pipeline%20has%20achieved%20promising%20results%20in%20unsupervised%20visible-infrared%20person%20re-identification%20%28USL-VI-ReID%29.%20It%20first%20performs%20single-modality%20learning%20and%20then%20operates%20cross-modality%20learning%20to%20tackle%20the%20modality%20discrepancy.%20Although%20promising%2C%20this%20pipeline%20inevitably%20introduces%20modality%20bias%3A%20modality-specific%20cues%20learned%20in%20the%20single-modality%20training%20naturally%20propagate%20into%20the%20following%20cross-modality%20learning%2C%20impairing%20identity%20discrimination%20and%20generalization.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Dual-level%20Modality%20Debiasing%20Learning%20%28DMDL%29%20framework%20that%20implements%20debiasing%20at%20both%20the%20model%20and%20optimization%20levels.%20At%20the%20model%20level%2C%20we%20propose%20a%20Causality-inspired%20Adjustment%20Intervention%20%28CAI%29%20module%20that%20replaces%20likelihood-based%20modeling%20with%20causal%20modeling%2C%20preventing%20modality-induced%20spurious%20patterns%20from%20being%20introduced%2C%20leading%20to%20a%20low-biased%20model.%20At%20the%20optimization%20level%2C%20a%20Collaborative%20Bias-free%20Training%20%28CBT%29%20strategy%20is%20introduced%20to%20interrupt%20the%20propagation%20of%20modality%20bias%20across%20data%2C%20labels%2C%20and%20features%20by%20integrating%20modality-specific%20augmentation%2C%20label%20refinement%2C%20and%20feature%20alignment.%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20DMDL%20could%20enable%20modality-invariant%20feature%20learning%20and%20a%20more%20generalized%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-level%2520Modality%2520Debiasing%2520Learning%2520for%2520Unsupervised%2520Visible-Infrared%2520Person%2520Re-Identification%26entry.906535625%3DJiaze%2520Li%2520and%2520Yan%2520Lu%2520and%2520Bin%2520Liu%2520and%2520Guojun%2520Yin%2520and%2520Mang%2520Ye%26entry.1292438233%3DTwo-stage%2520learning%2520pipeline%2520has%2520achieved%2520promising%2520results%2520in%2520unsupervised%2520visible-infrared%2520person%2520re-identification%2520%2528USL-VI-ReID%2529.%2520It%2520first%2520performs%2520single-modality%2520learning%2520and%2520then%2520operates%2520cross-modality%2520learning%2520to%2520tackle%2520the%2520modality%2520discrepancy.%2520Although%2520promising%252C%2520this%2520pipeline%2520inevitably%2520introduces%2520modality%2520bias%253A%2520modality-specific%2520cues%2520learned%2520in%2520the%2520single-modality%2520training%2520naturally%2520propagate%2520into%2520the%2520following%2520cross-modality%2520learning%252C%2520impairing%2520identity%2520discrimination%2520and%2520generalization.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520Dual-level%2520Modality%2520Debiasing%2520Learning%2520%2528DMDL%2529%2520framework%2520that%2520implements%2520debiasing%2520at%2520both%2520the%2520model%2520and%2520optimization%2520levels.%2520At%2520the%2520model%2520level%252C%2520we%2520propose%2520a%2520Causality-inspired%2520Adjustment%2520Intervention%2520%2528CAI%2529%2520module%2520that%2520replaces%2520likelihood-based%2520modeling%2520with%2520causal%2520modeling%252C%2520preventing%2520modality-induced%2520spurious%2520patterns%2520from%2520being%2520introduced%252C%2520leading%2520to%2520a%2520low-biased%2520model.%2520At%2520the%2520optimization%2520level%252C%2520a%2520Collaborative%2520Bias-free%2520Training%2520%2528CBT%2529%2520strategy%2520is%2520introduced%2520to%2520interrupt%2520the%2520propagation%2520of%2520modality%2520bias%2520across%2520data%252C%2520labels%252C%2520and%2520features%2520by%2520integrating%2520modality-specific%2520augmentation%252C%2520label%2520refinement%252C%2520and%2520feature%2520alignment.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520DMDL%2520could%2520enable%2520modality-invariant%2520feature%2520learning%2520and%2520a%2520more%2520generalized%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-level%20Modality%20Debiasing%20Learning%20for%20Unsupervised%20Visible-Infrared%20Person%20Re-Identification&entry.906535625=Jiaze%20Li%20and%20Yan%20Lu%20and%20Bin%20Liu%20and%20Guojun%20Yin%20and%20Mang%20Ye&entry.1292438233=Two-stage%20learning%20pipeline%20has%20achieved%20promising%20results%20in%20unsupervised%20visible-infrared%20person%20re-identification%20%28USL-VI-ReID%29.%20It%20first%20performs%20single-modality%20learning%20and%20then%20operates%20cross-modality%20learning%20to%20tackle%20the%20modality%20discrepancy.%20Although%20promising%2C%20this%20pipeline%20inevitably%20introduces%20modality%20bias%3A%20modality-specific%20cues%20learned%20in%20the%20single-modality%20training%20naturally%20propagate%20into%20the%20following%20cross-modality%20learning%2C%20impairing%20identity%20discrimination%20and%20generalization.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Dual-level%20Modality%20Debiasing%20Learning%20%28DMDL%29%20framework%20that%20implements%20debiasing%20at%20both%20the%20model%20and%20optimization%20levels.%20At%20the%20model%20level%2C%20we%20propose%20a%20Causality-inspired%20Adjustment%20Intervention%20%28CAI%29%20module%20that%20replaces%20likelihood-based%20modeling%20with%20causal%20modeling%2C%20preventing%20modality-induced%20spurious%20patterns%20from%20being%20introduced%2C%20leading%20to%20a%20low-biased%20model.%20At%20the%20optimization%20level%2C%20a%20Collaborative%20Bias-free%20Training%20%28CBT%29%20strategy%20is%20introduced%20to%20interrupt%20the%20propagation%20of%20modality%20bias%20across%20data%2C%20labels%2C%20and%20features%20by%20integrating%20modality-specific%20augmentation%2C%20label%20refinement%2C%20and%20feature%20alignment.%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20DMDL%20could%20enable%20modality-invariant%20feature%20learning%20and%20a%20more%20generalized%20model.&entry.1838667208=http%3A//arxiv.org/abs/2512.03745v1&entry.124074799=Read"},
{"title": "BitMark: Watermarking Bitwise Autoregressive Image Generative Models", "author": "Louis Kerner and Michel Meintz and Bihe Zhao and Franziska Boenisch and Adam Dziedzic", "abstract": "State-of-the-art text-to-image models generate photorealistic images at an unprecedented speed. This work focuses on models that operate in a bitwise autoregressive manner over a discrete set of tokens that is practically infinite in size. However, their impressive generative power comes with a growing risk: as their outputs increasingly populate the Internet, they are likely to be scraped and reused as training data-potentially by the very same models. This phenomenon has been shown to lead to model collapse, where repeated training on generated content, especially from the models' own previous versions, causes a gradual degradation in performance. A promising mitigation strategy is watermarking, which embeds human-imperceptible yet detectable signals into generated images-enabling the identification of generated content. In this work, we introduce BitMark, a robust bitwise watermarking framework. Our method embeds a watermark directly at the bit level of the token stream during the image generation process. Our bitwise watermark subtly influences the bits to preserve visual fidelity and generation speed while remaining robust against a spectrum of removal techniques. Furthermore, it exhibits high radioactivity, i.e., when watermarked generated images are used to train another image generative model, this second model's outputs will also carry the watermark. The radioactive traces remain detectable even when only fine-tuning diffusion or image autoregressive models on images watermarked with our BitMark. Overall, our approach provides a principled step toward preventing model collapse in image generative models by enabling reliable detection of generated outputs. The code is available at https://github.com/sprintml/BitMark.", "link": "http://arxiv.org/abs/2506.21209v2", "date": "2025-12-03", "relevancy": 2.2352, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5741}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5559}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BitMark%3A%20Watermarking%20Bitwise%20Autoregressive%20Image%20Generative%20Models&body=Title%3A%20BitMark%3A%20Watermarking%20Bitwise%20Autoregressive%20Image%20Generative%20Models%0AAuthor%3A%20Louis%20Kerner%20and%20Michel%20Meintz%20and%20Bihe%20Zhao%20and%20Franziska%20Boenisch%20and%20Adam%20Dziedzic%0AAbstract%3A%20State-of-the-art%20text-to-image%20models%20generate%20photorealistic%20images%20at%20an%20unprecedented%20speed.%20This%20work%20focuses%20on%20models%20that%20operate%20in%20a%20bitwise%20autoregressive%20manner%20over%20a%20discrete%20set%20of%20tokens%20that%20is%20practically%20infinite%20in%20size.%20However%2C%20their%20impressive%20generative%20power%20comes%20with%20a%20growing%20risk%3A%20as%20their%20outputs%20increasingly%20populate%20the%20Internet%2C%20they%20are%20likely%20to%20be%20scraped%20and%20reused%20as%20training%20data-potentially%20by%20the%20very%20same%20models.%20This%20phenomenon%20has%20been%20shown%20to%20lead%20to%20model%20collapse%2C%20where%20repeated%20training%20on%20generated%20content%2C%20especially%20from%20the%20models%27%20own%20previous%20versions%2C%20causes%20a%20gradual%20degradation%20in%20performance.%20A%20promising%20mitigation%20strategy%20is%20watermarking%2C%20which%20embeds%20human-imperceptible%20yet%20detectable%20signals%20into%20generated%20images-enabling%20the%20identification%20of%20generated%20content.%20In%20this%20work%2C%20we%20introduce%20BitMark%2C%20a%20robust%20bitwise%20watermarking%20framework.%20Our%20method%20embeds%20a%20watermark%20directly%20at%20the%20bit%20level%20of%20the%20token%20stream%20during%20the%20image%20generation%20process.%20Our%20bitwise%20watermark%20subtly%20influences%20the%20bits%20to%20preserve%20visual%20fidelity%20and%20generation%20speed%20while%20remaining%20robust%20against%20a%20spectrum%20of%20removal%20techniques.%20Furthermore%2C%20it%20exhibits%20high%20radioactivity%2C%20i.e.%2C%20when%20watermarked%20generated%20images%20are%20used%20to%20train%20another%20image%20generative%20model%2C%20this%20second%20model%27s%20outputs%20will%20also%20carry%20the%20watermark.%20The%20radioactive%20traces%20remain%20detectable%20even%20when%20only%20fine-tuning%20diffusion%20or%20image%20autoregressive%20models%20on%20images%20watermarked%20with%20our%20BitMark.%20Overall%2C%20our%20approach%20provides%20a%20principled%20step%20toward%20preventing%20model%20collapse%20in%20image%20generative%20models%20by%20enabling%20reliable%20detection%20of%20generated%20outputs.%20The%20code%20is%20available%20at%20https%3A//github.com/sprintml/BitMark.%0ALink%3A%20http%3A//arxiv.org/abs/2506.21209v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBitMark%253A%2520Watermarking%2520Bitwise%2520Autoregressive%2520Image%2520Generative%2520Models%26entry.906535625%3DLouis%2520Kerner%2520and%2520Michel%2520Meintz%2520and%2520Bihe%2520Zhao%2520and%2520Franziska%2520Boenisch%2520and%2520Adam%2520Dziedzic%26entry.1292438233%3DState-of-the-art%2520text-to-image%2520models%2520generate%2520photorealistic%2520images%2520at%2520an%2520unprecedented%2520speed.%2520This%2520work%2520focuses%2520on%2520models%2520that%2520operate%2520in%2520a%2520bitwise%2520autoregressive%2520manner%2520over%2520a%2520discrete%2520set%2520of%2520tokens%2520that%2520is%2520practically%2520infinite%2520in%2520size.%2520However%252C%2520their%2520impressive%2520generative%2520power%2520comes%2520with%2520a%2520growing%2520risk%253A%2520as%2520their%2520outputs%2520increasingly%2520populate%2520the%2520Internet%252C%2520they%2520are%2520likely%2520to%2520be%2520scraped%2520and%2520reused%2520as%2520training%2520data-potentially%2520by%2520the%2520very%2520same%2520models.%2520This%2520phenomenon%2520has%2520been%2520shown%2520to%2520lead%2520to%2520model%2520collapse%252C%2520where%2520repeated%2520training%2520on%2520generated%2520content%252C%2520especially%2520from%2520the%2520models%2527%2520own%2520previous%2520versions%252C%2520causes%2520a%2520gradual%2520degradation%2520in%2520performance.%2520A%2520promising%2520mitigation%2520strategy%2520is%2520watermarking%252C%2520which%2520embeds%2520human-imperceptible%2520yet%2520detectable%2520signals%2520into%2520generated%2520images-enabling%2520the%2520identification%2520of%2520generated%2520content.%2520In%2520this%2520work%252C%2520we%2520introduce%2520BitMark%252C%2520a%2520robust%2520bitwise%2520watermarking%2520framework.%2520Our%2520method%2520embeds%2520a%2520watermark%2520directly%2520at%2520the%2520bit%2520level%2520of%2520the%2520token%2520stream%2520during%2520the%2520image%2520generation%2520process.%2520Our%2520bitwise%2520watermark%2520subtly%2520influences%2520the%2520bits%2520to%2520preserve%2520visual%2520fidelity%2520and%2520generation%2520speed%2520while%2520remaining%2520robust%2520against%2520a%2520spectrum%2520of%2520removal%2520techniques.%2520Furthermore%252C%2520it%2520exhibits%2520high%2520radioactivity%252C%2520i.e.%252C%2520when%2520watermarked%2520generated%2520images%2520are%2520used%2520to%2520train%2520another%2520image%2520generative%2520model%252C%2520this%2520second%2520model%2527s%2520outputs%2520will%2520also%2520carry%2520the%2520watermark.%2520The%2520radioactive%2520traces%2520remain%2520detectable%2520even%2520when%2520only%2520fine-tuning%2520diffusion%2520or%2520image%2520autoregressive%2520models%2520on%2520images%2520watermarked%2520with%2520our%2520BitMark.%2520Overall%252C%2520our%2520approach%2520provides%2520a%2520principled%2520step%2520toward%2520preventing%2520model%2520collapse%2520in%2520image%2520generative%2520models%2520by%2520enabling%2520reliable%2520detection%2520of%2520generated%2520outputs.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/sprintml/BitMark.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21209v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BitMark%3A%20Watermarking%20Bitwise%20Autoregressive%20Image%20Generative%20Models&entry.906535625=Louis%20Kerner%20and%20Michel%20Meintz%20and%20Bihe%20Zhao%20and%20Franziska%20Boenisch%20and%20Adam%20Dziedzic&entry.1292438233=State-of-the-art%20text-to-image%20models%20generate%20photorealistic%20images%20at%20an%20unprecedented%20speed.%20This%20work%20focuses%20on%20models%20that%20operate%20in%20a%20bitwise%20autoregressive%20manner%20over%20a%20discrete%20set%20of%20tokens%20that%20is%20practically%20infinite%20in%20size.%20However%2C%20their%20impressive%20generative%20power%20comes%20with%20a%20growing%20risk%3A%20as%20their%20outputs%20increasingly%20populate%20the%20Internet%2C%20they%20are%20likely%20to%20be%20scraped%20and%20reused%20as%20training%20data-potentially%20by%20the%20very%20same%20models.%20This%20phenomenon%20has%20been%20shown%20to%20lead%20to%20model%20collapse%2C%20where%20repeated%20training%20on%20generated%20content%2C%20especially%20from%20the%20models%27%20own%20previous%20versions%2C%20causes%20a%20gradual%20degradation%20in%20performance.%20A%20promising%20mitigation%20strategy%20is%20watermarking%2C%20which%20embeds%20human-imperceptible%20yet%20detectable%20signals%20into%20generated%20images-enabling%20the%20identification%20of%20generated%20content.%20In%20this%20work%2C%20we%20introduce%20BitMark%2C%20a%20robust%20bitwise%20watermarking%20framework.%20Our%20method%20embeds%20a%20watermark%20directly%20at%20the%20bit%20level%20of%20the%20token%20stream%20during%20the%20image%20generation%20process.%20Our%20bitwise%20watermark%20subtly%20influences%20the%20bits%20to%20preserve%20visual%20fidelity%20and%20generation%20speed%20while%20remaining%20robust%20against%20a%20spectrum%20of%20removal%20techniques.%20Furthermore%2C%20it%20exhibits%20high%20radioactivity%2C%20i.e.%2C%20when%20watermarked%20generated%20images%20are%20used%20to%20train%20another%20image%20generative%20model%2C%20this%20second%20model%27s%20outputs%20will%20also%20carry%20the%20watermark.%20The%20radioactive%20traces%20remain%20detectable%20even%20when%20only%20fine-tuning%20diffusion%20or%20image%20autoregressive%20models%20on%20images%20watermarked%20with%20our%20BitMark.%20Overall%2C%20our%20approach%20provides%20a%20principled%20step%20toward%20preventing%20model%20collapse%20in%20image%20generative%20models%20by%20enabling%20reliable%20detection%20of%20generated%20outputs.%20The%20code%20is%20available%20at%20https%3A//github.com/sprintml/BitMark.&entry.1838667208=http%3A//arxiv.org/abs/2506.21209v2&entry.124074799=Read"},
{"title": "SpecGen: Neural Spectral BRDF Generation via Spectral-Spatial Tri-plane Aggregation", "author": "Zhenyu Jin and Wenjie Li and Zhanyu Ma and Heng Guo", "abstract": "Synthesizing spectral images across different wavelengths is essential for photorealistic rendering. Unlike conventional spectral uplifting methods that convert RGB images into spectral ones, we introduce SpecGen, a novel method that generates spectral bidirectional reflectance distribution functions (BRDFs) from a single RGB image of a sphere. This enables spectral image rendering under arbitrary illuminations and shapes covered by the corresponding material. A key challenge in spectral BRDF generation is the scarcity of measured spectral BRDF data. To address this, we propose the Spectral-Spatial Tri-plane Aggregation (SSTA) network, which models reflectance responses across wavelengths and incident-outgoing directions, allowing the training strategy to leverage abundant RGB BRDF data to enhance spectral BRDF generation. Experiments show that our method accurately reconstructs spectral BRDFs from limited spectral data and surpasses state-of-the-art methods in hyperspectral image reconstruction, achieving an improvement of 8 dB in PSNR. Codes and data will be released upon acceptance.", "link": "http://arxiv.org/abs/2508.17316v2", "date": "2025-12-03", "relevancy": 2.2347, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5884}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.54}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecGen%3A%20Neural%20Spectral%20BRDF%20Generation%20via%20Spectral-Spatial%20Tri-plane%20Aggregation&body=Title%3A%20SpecGen%3A%20Neural%20Spectral%20BRDF%20Generation%20via%20Spectral-Spatial%20Tri-plane%20Aggregation%0AAuthor%3A%20Zhenyu%20Jin%20and%20Wenjie%20Li%20and%20Zhanyu%20Ma%20and%20Heng%20Guo%0AAbstract%3A%20Synthesizing%20spectral%20images%20across%20different%20wavelengths%20is%20essential%20for%20photorealistic%20rendering.%20Unlike%20conventional%20spectral%20uplifting%20methods%20that%20convert%20RGB%20images%20into%20spectral%20ones%2C%20we%20introduce%20SpecGen%2C%20a%20novel%20method%20that%20generates%20spectral%20bidirectional%20reflectance%20distribution%20functions%20%28BRDFs%29%20from%20a%20single%20RGB%20image%20of%20a%20sphere.%20This%20enables%20spectral%20image%20rendering%20under%20arbitrary%20illuminations%20and%20shapes%20covered%20by%20the%20corresponding%20material.%20A%20key%20challenge%20in%20spectral%20BRDF%20generation%20is%20the%20scarcity%20of%20measured%20spectral%20BRDF%20data.%20To%20address%20this%2C%20we%20propose%20the%20Spectral-Spatial%20Tri-plane%20Aggregation%20%28SSTA%29%20network%2C%20which%20models%20reflectance%20responses%20across%20wavelengths%20and%20incident-outgoing%20directions%2C%20allowing%20the%20training%20strategy%20to%20leverage%20abundant%20RGB%20BRDF%20data%20to%20enhance%20spectral%20BRDF%20generation.%20Experiments%20show%20that%20our%20method%20accurately%20reconstructs%20spectral%20BRDFs%20from%20limited%20spectral%20data%20and%20surpasses%20state-of-the-art%20methods%20in%20hyperspectral%20image%20reconstruction%2C%20achieving%20an%20improvement%20of%208%20dB%20in%20PSNR.%20Codes%20and%20data%20will%20be%20released%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2508.17316v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecGen%253A%2520Neural%2520Spectral%2520BRDF%2520Generation%2520via%2520Spectral-Spatial%2520Tri-plane%2520Aggregation%26entry.906535625%3DZhenyu%2520Jin%2520and%2520Wenjie%2520Li%2520and%2520Zhanyu%2520Ma%2520and%2520Heng%2520Guo%26entry.1292438233%3DSynthesizing%2520spectral%2520images%2520across%2520different%2520wavelengths%2520is%2520essential%2520for%2520photorealistic%2520rendering.%2520Unlike%2520conventional%2520spectral%2520uplifting%2520methods%2520that%2520convert%2520RGB%2520images%2520into%2520spectral%2520ones%252C%2520we%2520introduce%2520SpecGen%252C%2520a%2520novel%2520method%2520that%2520generates%2520spectral%2520bidirectional%2520reflectance%2520distribution%2520functions%2520%2528BRDFs%2529%2520from%2520a%2520single%2520RGB%2520image%2520of%2520a%2520sphere.%2520This%2520enables%2520spectral%2520image%2520rendering%2520under%2520arbitrary%2520illuminations%2520and%2520shapes%2520covered%2520by%2520the%2520corresponding%2520material.%2520A%2520key%2520challenge%2520in%2520spectral%2520BRDF%2520generation%2520is%2520the%2520scarcity%2520of%2520measured%2520spectral%2520BRDF%2520data.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Spectral-Spatial%2520Tri-plane%2520Aggregation%2520%2528SSTA%2529%2520network%252C%2520which%2520models%2520reflectance%2520responses%2520across%2520wavelengths%2520and%2520incident-outgoing%2520directions%252C%2520allowing%2520the%2520training%2520strategy%2520to%2520leverage%2520abundant%2520RGB%2520BRDF%2520data%2520to%2520enhance%2520spectral%2520BRDF%2520generation.%2520Experiments%2520show%2520that%2520our%2520method%2520accurately%2520reconstructs%2520spectral%2520BRDFs%2520from%2520limited%2520spectral%2520data%2520and%2520surpasses%2520state-of-the-art%2520methods%2520in%2520hyperspectral%2520image%2520reconstruction%252C%2520achieving%2520an%2520improvement%2520of%25208%2520dB%2520in%2520PSNR.%2520Codes%2520and%2520data%2520will%2520be%2520released%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17316v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecGen%3A%20Neural%20Spectral%20BRDF%20Generation%20via%20Spectral-Spatial%20Tri-plane%20Aggregation&entry.906535625=Zhenyu%20Jin%20and%20Wenjie%20Li%20and%20Zhanyu%20Ma%20and%20Heng%20Guo&entry.1292438233=Synthesizing%20spectral%20images%20across%20different%20wavelengths%20is%20essential%20for%20photorealistic%20rendering.%20Unlike%20conventional%20spectral%20uplifting%20methods%20that%20convert%20RGB%20images%20into%20spectral%20ones%2C%20we%20introduce%20SpecGen%2C%20a%20novel%20method%20that%20generates%20spectral%20bidirectional%20reflectance%20distribution%20functions%20%28BRDFs%29%20from%20a%20single%20RGB%20image%20of%20a%20sphere.%20This%20enables%20spectral%20image%20rendering%20under%20arbitrary%20illuminations%20and%20shapes%20covered%20by%20the%20corresponding%20material.%20A%20key%20challenge%20in%20spectral%20BRDF%20generation%20is%20the%20scarcity%20of%20measured%20spectral%20BRDF%20data.%20To%20address%20this%2C%20we%20propose%20the%20Spectral-Spatial%20Tri-plane%20Aggregation%20%28SSTA%29%20network%2C%20which%20models%20reflectance%20responses%20across%20wavelengths%20and%20incident-outgoing%20directions%2C%20allowing%20the%20training%20strategy%20to%20leverage%20abundant%20RGB%20BRDF%20data%20to%20enhance%20spectral%20BRDF%20generation.%20Experiments%20show%20that%20our%20method%20accurately%20reconstructs%20spectral%20BRDFs%20from%20limited%20spectral%20data%20and%20surpasses%20state-of-the-art%20methods%20in%20hyperspectral%20image%20reconstruction%2C%20achieving%20an%20improvement%20of%208%20dB%20in%20PSNR.%20Codes%20and%20data%20will%20be%20released%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2508.17316v2&entry.124074799=Read"},
{"title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding", "author": "Jialuo Li and Bin Li and Jiahao Li and Yan Lu", "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.", "link": "http://arxiv.org/abs/2512.04000v1", "date": "2025-12-03", "relevancy": 2.2294, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divide%2C%20then%20Ground%3A%20Adapting%20Frame%20Selection%20to%20Query%20Types%20for%20Long-Form%20Video%20Understanding&body=Title%3A%20Divide%2C%20then%20Ground%3A%20Adapting%20Frame%20Selection%20to%20Query%20Types%20for%20Long-Form%20Video%20Understanding%0AAuthor%3A%20Jialuo%20Li%20and%20Bin%20Li%20and%20Jiahao%20Li%20and%20Yan%20Lu%0AAbstract%3A%20The%20application%20of%20Large%20Multimodal%20Models%20%28LMMs%29%20to%20long-form%20video%20understanding%20is%20constrained%20by%20limited%20context%20lengths%20and%20the%20computationally%20prohibitive%20cost%20of%20processing%20dense%20video%20tokens.%20Consequently%2C%20recent%20research%20has%20focused%20on%20query-aware%20frame%20selection%2C%20methods%20that%20often%20incur%20significant%20computational%20overhead.%20This%20paper%20challenges%20the%20assumption%20that%20such%20complex%20search%20mechanisms%20are%20universally%20necessary.%20We%20first%20identify%20and%20validate%20a%20query%20typology%20distinguishing%20between%20global%20query%20and%20localized%20query.%20We%20demonstrate%20that%20while%20uniform%20sampling%20is%20both%20effective%20and%20efficient%20for%20global%20queries%2C%20localized%20queries%20indeed%20necessitate%20query-aware%20selection%20for%20optimal%20performance.%20Building%20on%20this%20insight%2C%20we%20propose%20DIG%2C%20a%20training-free%20frame%20selection%20framework%20that%20adapts%20its%20strategy%20based%20on%20the%20query%20type.%20Specifically%2CDIG%20employs%20efficient%20uniform%20sampling%20for%20global%20queries%20while%20activating%20a%20specialized%20pipeline%20to%20extract%20query-relevant%20frames%20for%20localized%20queries.%20Experiments%20on%20three%20long-form%20video%20understanding%20benchmarks%20demonstrate%20that%20DIG%20consistently%20outperforms%20existing%20baselines%20and%20robustly%20improves%20LMM%20performance%2C%20even%20when%20scaling%20the%20input%20frame%20count%20to%20256.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivide%252C%2520then%2520Ground%253A%2520Adapting%2520Frame%2520Selection%2520to%2520Query%2520Types%2520for%2520Long-Form%2520Video%2520Understanding%26entry.906535625%3DJialuo%2520Li%2520and%2520Bin%2520Li%2520and%2520Jiahao%2520Li%2520and%2520Yan%2520Lu%26entry.1292438233%3DThe%2520application%2520of%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520to%2520long-form%2520video%2520understanding%2520is%2520constrained%2520by%2520limited%2520context%2520lengths%2520and%2520the%2520computationally%2520prohibitive%2520cost%2520of%2520processing%2520dense%2520video%2520tokens.%2520Consequently%252C%2520recent%2520research%2520has%2520focused%2520on%2520query-aware%2520frame%2520selection%252C%2520methods%2520that%2520often%2520incur%2520significant%2520computational%2520overhead.%2520This%2520paper%2520challenges%2520the%2520assumption%2520that%2520such%2520complex%2520search%2520mechanisms%2520are%2520universally%2520necessary.%2520We%2520first%2520identify%2520and%2520validate%2520a%2520query%2520typology%2520distinguishing%2520between%2520global%2520query%2520and%2520localized%2520query.%2520We%2520demonstrate%2520that%2520while%2520uniform%2520sampling%2520is%2520both%2520effective%2520and%2520efficient%2520for%2520global%2520queries%252C%2520localized%2520queries%2520indeed%2520necessitate%2520query-aware%2520selection%2520for%2520optimal%2520performance.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520DIG%252C%2520a%2520training-free%2520frame%2520selection%2520framework%2520that%2520adapts%2520its%2520strategy%2520based%2520on%2520the%2520query%2520type.%2520Specifically%252CDIG%2520employs%2520efficient%2520uniform%2520sampling%2520for%2520global%2520queries%2520while%2520activating%2520a%2520specialized%2520pipeline%2520to%2520extract%2520query-relevant%2520frames%2520for%2520localized%2520queries.%2520Experiments%2520on%2520three%2520long-form%2520video%2520understanding%2520benchmarks%2520demonstrate%2520that%2520DIG%2520consistently%2520outperforms%2520existing%2520baselines%2520and%2520robustly%2520improves%2520LMM%2520performance%252C%2520even%2520when%2520scaling%2520the%2520input%2520frame%2520count%2520to%2520256.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divide%2C%20then%20Ground%3A%20Adapting%20Frame%20Selection%20to%20Query%20Types%20for%20Long-Form%20Video%20Understanding&entry.906535625=Jialuo%20Li%20and%20Bin%20Li%20and%20Jiahao%20Li%20and%20Yan%20Lu&entry.1292438233=The%20application%20of%20Large%20Multimodal%20Models%20%28LMMs%29%20to%20long-form%20video%20understanding%20is%20constrained%20by%20limited%20context%20lengths%20and%20the%20computationally%20prohibitive%20cost%20of%20processing%20dense%20video%20tokens.%20Consequently%2C%20recent%20research%20has%20focused%20on%20query-aware%20frame%20selection%2C%20methods%20that%20often%20incur%20significant%20computational%20overhead.%20This%20paper%20challenges%20the%20assumption%20that%20such%20complex%20search%20mechanisms%20are%20universally%20necessary.%20We%20first%20identify%20and%20validate%20a%20query%20typology%20distinguishing%20between%20global%20query%20and%20localized%20query.%20We%20demonstrate%20that%20while%20uniform%20sampling%20is%20both%20effective%20and%20efficient%20for%20global%20queries%2C%20localized%20queries%20indeed%20necessitate%20query-aware%20selection%20for%20optimal%20performance.%20Building%20on%20this%20insight%2C%20we%20propose%20DIG%2C%20a%20training-free%20frame%20selection%20framework%20that%20adapts%20its%20strategy%20based%20on%20the%20query%20type.%20Specifically%2CDIG%20employs%20efficient%20uniform%20sampling%20for%20global%20queries%20while%20activating%20a%20specialized%20pipeline%20to%20extract%20query-relevant%20frames%20for%20localized%20queries.%20Experiments%20on%20three%20long-form%20video%20understanding%20benchmarks%20demonstrate%20that%20DIG%20consistently%20outperforms%20existing%20baselines%20and%20robustly%20improves%20LMM%20performance%2C%20even%20when%20scaling%20the%20input%20frame%20count%20to%20256.&entry.1838667208=http%3A//arxiv.org/abs/2512.04000v1&entry.124074799=Read"},
{"title": "TransUNet-GradCAM: A Hybrid Transformer-U-Net with Self-Attention and Explainable Visualizations for Foot Ulcer Segmentation", "author": "Akwasi Asare and Mary Sagoe and Justice Williams Asare and Stephen Edward Moore", "abstract": "Automated segmentation of diabetic foot ulcers (DFUs) plays a critical role in clinical diagnosis, therapeutic planning, and longitudinal wound monitoring. However, this task remains challenging due to the heterogeneous appearance, irregular morphology, and complex backgrounds associated with ulcer regions in clinical photographs. Traditional convolutional neural networks (CNNs), such as U-Net, provide strong localization capabilities but struggle to model long-range spatial dependencies due to their inherently limited receptive fields. To address this, we employ the TransUNet architecture, a hybrid framework that integrates the global attention mechanism of Vision Transformers (ViTs) into the U-Net structure. This combination allows the model to extract global contextual features while maintaining fine-grained spatial resolution. We trained the model on the public Foot Ulcer Segmentation Challenge (FUSeg) dataset using a robust augmentation pipeline and a hybrid loss function to mitigate class imbalance. On the validation set, the model achieved a Dice Similarity Coefficient (F1-score) of 0.8799 using an optimized threshold of 0.4389. To ensure clinical transparency, we integrated Grad-CAM visualizations to highlight model focus areas. Furthermore, a clinical utility analysis demonstrated a strong correlation (Pearson r = 0.9631) between predicted and ground-truth wound areas. These outcomes demonstrate that our approach effectively integrates global and local feature extraction, offering a reliable, effective, and explainable solution for automated foot ulcer assessment.", "link": "http://arxiv.org/abs/2508.03758v3", "date": "2025-12-03", "relevancy": 2.2184, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5707}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5534}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransUNet-GradCAM%3A%20A%20Hybrid%20Transformer-U-Net%20with%20Self-Attention%20and%20Explainable%20Visualizations%20for%20Foot%20Ulcer%20Segmentation&body=Title%3A%20TransUNet-GradCAM%3A%20A%20Hybrid%20Transformer-U-Net%20with%20Self-Attention%20and%20Explainable%20Visualizations%20for%20Foot%20Ulcer%20Segmentation%0AAuthor%3A%20Akwasi%20Asare%20and%20Mary%20Sagoe%20and%20Justice%20Williams%20Asare%20and%20Stephen%20Edward%20Moore%0AAbstract%3A%20Automated%20segmentation%20of%20diabetic%20foot%20ulcers%20%28DFUs%29%20plays%20a%20critical%20role%20in%20clinical%20diagnosis%2C%20therapeutic%20planning%2C%20and%20longitudinal%20wound%20monitoring.%20However%2C%20this%20task%20remains%20challenging%20due%20to%20the%20heterogeneous%20appearance%2C%20irregular%20morphology%2C%20and%20complex%20backgrounds%20associated%20with%20ulcer%20regions%20in%20clinical%20photographs.%20Traditional%20convolutional%20neural%20networks%20%28CNNs%29%2C%20such%20as%20U-Net%2C%20provide%20strong%20localization%20capabilities%20but%20struggle%20to%20model%20long-range%20spatial%20dependencies%20due%20to%20their%20inherently%20limited%20receptive%20fields.%20To%20address%20this%2C%20we%20employ%20the%20TransUNet%20architecture%2C%20a%20hybrid%20framework%20that%20integrates%20the%20global%20attention%20mechanism%20of%20Vision%20Transformers%20%28ViTs%29%20into%20the%20U-Net%20structure.%20This%20combination%20allows%20the%20model%20to%20extract%20global%20contextual%20features%20while%20maintaining%20fine-grained%20spatial%20resolution.%20We%20trained%20the%20model%20on%20the%20public%20Foot%20Ulcer%20Segmentation%20Challenge%20%28FUSeg%29%20dataset%20using%20a%20robust%20augmentation%20pipeline%20and%20a%20hybrid%20loss%20function%20to%20mitigate%20class%20imbalance.%20On%20the%20validation%20set%2C%20the%20model%20achieved%20a%20Dice%20Similarity%20Coefficient%20%28F1-score%29%20of%200.8799%20using%20an%20optimized%20threshold%20of%200.4389.%20To%20ensure%20clinical%20transparency%2C%20we%20integrated%20Grad-CAM%20visualizations%20to%20highlight%20model%20focus%20areas.%20Furthermore%2C%20a%20clinical%20utility%20analysis%20demonstrated%20a%20strong%20correlation%20%28Pearson%20r%20%3D%200.9631%29%20between%20predicted%20and%20ground-truth%20wound%20areas.%20These%20outcomes%20demonstrate%20that%20our%20approach%20effectively%20integrates%20global%20and%20local%20feature%20extraction%2C%20offering%20a%20reliable%2C%20effective%2C%20and%20explainable%20solution%20for%20automated%20foot%20ulcer%20assessment.%0ALink%3A%20http%3A//arxiv.org/abs/2508.03758v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransUNet-GradCAM%253A%2520A%2520Hybrid%2520Transformer-U-Net%2520with%2520Self-Attention%2520and%2520Explainable%2520Visualizations%2520for%2520Foot%2520Ulcer%2520Segmentation%26entry.906535625%3DAkwasi%2520Asare%2520and%2520Mary%2520Sagoe%2520and%2520Justice%2520Williams%2520Asare%2520and%2520Stephen%2520Edward%2520Moore%26entry.1292438233%3DAutomated%2520segmentation%2520of%2520diabetic%2520foot%2520ulcers%2520%2528DFUs%2529%2520plays%2520a%2520critical%2520role%2520in%2520clinical%2520diagnosis%252C%2520therapeutic%2520planning%252C%2520and%2520longitudinal%2520wound%2520monitoring.%2520However%252C%2520this%2520task%2520remains%2520challenging%2520due%2520to%2520the%2520heterogeneous%2520appearance%252C%2520irregular%2520morphology%252C%2520and%2520complex%2520backgrounds%2520associated%2520with%2520ulcer%2520regions%2520in%2520clinical%2520photographs.%2520Traditional%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520such%2520as%2520U-Net%252C%2520provide%2520strong%2520localization%2520capabilities%2520but%2520struggle%2520to%2520model%2520long-range%2520spatial%2520dependencies%2520due%2520to%2520their%2520inherently%2520limited%2520receptive%2520fields.%2520To%2520address%2520this%252C%2520we%2520employ%2520the%2520TransUNet%2520architecture%252C%2520a%2520hybrid%2520framework%2520that%2520integrates%2520the%2520global%2520attention%2520mechanism%2520of%2520Vision%2520Transformers%2520%2528ViTs%2529%2520into%2520the%2520U-Net%2520structure.%2520This%2520combination%2520allows%2520the%2520model%2520to%2520extract%2520global%2520contextual%2520features%2520while%2520maintaining%2520fine-grained%2520spatial%2520resolution.%2520We%2520trained%2520the%2520model%2520on%2520the%2520public%2520Foot%2520Ulcer%2520Segmentation%2520Challenge%2520%2528FUSeg%2529%2520dataset%2520using%2520a%2520robust%2520augmentation%2520pipeline%2520and%2520a%2520hybrid%2520loss%2520function%2520to%2520mitigate%2520class%2520imbalance.%2520On%2520the%2520validation%2520set%252C%2520the%2520model%2520achieved%2520a%2520Dice%2520Similarity%2520Coefficient%2520%2528F1-score%2529%2520of%25200.8799%2520using%2520an%2520optimized%2520threshold%2520of%25200.4389.%2520To%2520ensure%2520clinical%2520transparency%252C%2520we%2520integrated%2520Grad-CAM%2520visualizations%2520to%2520highlight%2520model%2520focus%2520areas.%2520Furthermore%252C%2520a%2520clinical%2520utility%2520analysis%2520demonstrated%2520a%2520strong%2520correlation%2520%2528Pearson%2520r%2520%253D%25200.9631%2529%2520between%2520predicted%2520and%2520ground-truth%2520wound%2520areas.%2520These%2520outcomes%2520demonstrate%2520that%2520our%2520approach%2520effectively%2520integrates%2520global%2520and%2520local%2520feature%2520extraction%252C%2520offering%2520a%2520reliable%252C%2520effective%252C%2520and%2520explainable%2520solution%2520for%2520automated%2520foot%2520ulcer%2520assessment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03758v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransUNet-GradCAM%3A%20A%20Hybrid%20Transformer-U-Net%20with%20Self-Attention%20and%20Explainable%20Visualizations%20for%20Foot%20Ulcer%20Segmentation&entry.906535625=Akwasi%20Asare%20and%20Mary%20Sagoe%20and%20Justice%20Williams%20Asare%20and%20Stephen%20Edward%20Moore&entry.1292438233=Automated%20segmentation%20of%20diabetic%20foot%20ulcers%20%28DFUs%29%20plays%20a%20critical%20role%20in%20clinical%20diagnosis%2C%20therapeutic%20planning%2C%20and%20longitudinal%20wound%20monitoring.%20However%2C%20this%20task%20remains%20challenging%20due%20to%20the%20heterogeneous%20appearance%2C%20irregular%20morphology%2C%20and%20complex%20backgrounds%20associated%20with%20ulcer%20regions%20in%20clinical%20photographs.%20Traditional%20convolutional%20neural%20networks%20%28CNNs%29%2C%20such%20as%20U-Net%2C%20provide%20strong%20localization%20capabilities%20but%20struggle%20to%20model%20long-range%20spatial%20dependencies%20due%20to%20their%20inherently%20limited%20receptive%20fields.%20To%20address%20this%2C%20we%20employ%20the%20TransUNet%20architecture%2C%20a%20hybrid%20framework%20that%20integrates%20the%20global%20attention%20mechanism%20of%20Vision%20Transformers%20%28ViTs%29%20into%20the%20U-Net%20structure.%20This%20combination%20allows%20the%20model%20to%20extract%20global%20contextual%20features%20while%20maintaining%20fine-grained%20spatial%20resolution.%20We%20trained%20the%20model%20on%20the%20public%20Foot%20Ulcer%20Segmentation%20Challenge%20%28FUSeg%29%20dataset%20using%20a%20robust%20augmentation%20pipeline%20and%20a%20hybrid%20loss%20function%20to%20mitigate%20class%20imbalance.%20On%20the%20validation%20set%2C%20the%20model%20achieved%20a%20Dice%20Similarity%20Coefficient%20%28F1-score%29%20of%200.8799%20using%20an%20optimized%20threshold%20of%200.4389.%20To%20ensure%20clinical%20transparency%2C%20we%20integrated%20Grad-CAM%20visualizations%20to%20highlight%20model%20focus%20areas.%20Furthermore%2C%20a%20clinical%20utility%20analysis%20demonstrated%20a%20strong%20correlation%20%28Pearson%20r%20%3D%200.9631%29%20between%20predicted%20and%20ground-truth%20wound%20areas.%20These%20outcomes%20demonstrate%20that%20our%20approach%20effectively%20integrates%20global%20and%20local%20feature%20extraction%2C%20offering%20a%20reliable%2C%20effective%2C%20and%20explainable%20solution%20for%20automated%20foot%20ulcer%20assessment.&entry.1838667208=http%3A//arxiv.org/abs/2508.03758v3&entry.124074799=Read"},
{"title": "A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection", "author": "Shahid Ansari and Mahendra Kumar Gohil and Yusuke Maeda and Bishakh Bhattacharya", "abstract": "This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping. The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting. For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination. An analytical model derived using the principle of virtual work relates servo torque to grasp force, enabling design-level reasoning about actuation requirements. During execution, closed-loop grasp-force regulation is achieved using a proportional--integral--derivative controller with feedback from force-sensitive resistors mounted on selected fingers to prevent slip and bruising. Motion execution is supported by Particle Swarm Optimization (PSO)--based trajectory planning for a 5-DOF manipulator. Experiments demonstrate complete picking cycles (approach, separation, cutting, grasping, transport, release) with an average cycle time of 24.34~s and an overall success rate of approximately 80\\%, while maintaining low grasp forces (0.20--0.50~N). These results validate the proposed hybrid gripper and integrated vision--control pipeline for reliable harvesting in cluttered environments.", "link": "http://arxiv.org/abs/2512.03684v1", "date": "2025-12-03", "relevancy": 2.1938, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5616}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5428}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Approach%20to%20Tomato%20Harvesting%20Using%20a%20Hybrid%20Gripper%20with%20Semantic%20Segmentation%20and%20Keypoint%20Detection&body=Title%3A%20A%20Novel%20Approach%20to%20Tomato%20Harvesting%20Using%20a%20Hybrid%20Gripper%20with%20Semantic%20Segmentation%20and%20Keypoint%20Detection%0AAuthor%3A%20Shahid%20Ansari%20and%20Mahendra%20Kumar%20Gohil%20and%20Yusuke%20Maeda%20and%20Bishakh%20Bhattacharya%0AAbstract%3A%20This%20paper%20presents%20an%20autonomous%20tomato-harvesting%20system%20built%20around%20a%20hybrid%20robotic%20gripper%20that%20combines%20six%20soft%20auxetic%20fingers%20with%20a%20rigid%20exoskeleton%20and%20a%20latex%20basket%20to%20achieve%20gentle%2C%20cage-like%20grasping.%20The%20gripper%20is%20driven%20by%20a%20servo-actuated%20Scotch--yoke%20mechanism%2C%20and%20includes%20separator%20leaves%20that%20form%20a%20conical%20frustum%20for%20fruit%20isolation%2C%20with%20an%20integrated%20micro-servo%20cutter%20for%20pedicel%20cutting.%20For%20perception%2C%20an%20RGB--D%20camera%20and%20a%20Detectron2-based%20pipeline%20perform%20semantic%20segmentation%20of%20ripe/unripe%20tomatoes%20and%20keypoint%20localization%20of%20the%20pedicel%20and%20fruit%20center%20under%20occlusion%20and%20variable%20illumination.%20An%20analytical%20model%20derived%20using%20the%20principle%20of%20virtual%20work%20relates%20servo%20torque%20to%20grasp%20force%2C%20enabling%20design-level%20reasoning%20about%20actuation%20requirements.%20During%20execution%2C%20closed-loop%20grasp-force%20regulation%20is%20achieved%20using%20a%20proportional--integral--derivative%20controller%20with%20feedback%20from%20force-sensitive%20resistors%20mounted%20on%20selected%20fingers%20to%20prevent%20slip%20and%20bruising.%20Motion%20execution%20is%20supported%20by%20Particle%20Swarm%20Optimization%20%28PSO%29--based%20trajectory%20planning%20for%20a%205-DOF%20manipulator.%20Experiments%20demonstrate%20complete%20picking%20cycles%20%28approach%2C%20separation%2C%20cutting%2C%20grasping%2C%20transport%2C%20release%29%20with%20an%20average%20cycle%20time%20of%2024.34~s%20and%20an%20overall%20success%20rate%20of%20approximately%2080%5C%25%2C%20while%20maintaining%20low%20grasp%20forces%20%280.20--0.50~N%29.%20These%20results%20validate%20the%20proposed%20hybrid%20gripper%20and%20integrated%20vision--control%20pipeline%20for%20reliable%20harvesting%20in%20cluttered%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Approach%2520to%2520Tomato%2520Harvesting%2520Using%2520a%2520Hybrid%2520Gripper%2520with%2520Semantic%2520Segmentation%2520and%2520Keypoint%2520Detection%26entry.906535625%3DShahid%2520Ansari%2520and%2520Mahendra%2520Kumar%2520Gohil%2520and%2520Yusuke%2520Maeda%2520and%2520Bishakh%2520Bhattacharya%26entry.1292438233%3DThis%2520paper%2520presents%2520an%2520autonomous%2520tomato-harvesting%2520system%2520built%2520around%2520a%2520hybrid%2520robotic%2520gripper%2520that%2520combines%2520six%2520soft%2520auxetic%2520fingers%2520with%2520a%2520rigid%2520exoskeleton%2520and%2520a%2520latex%2520basket%2520to%2520achieve%2520gentle%252C%2520cage-like%2520grasping.%2520The%2520gripper%2520is%2520driven%2520by%2520a%2520servo-actuated%2520Scotch--yoke%2520mechanism%252C%2520and%2520includes%2520separator%2520leaves%2520that%2520form%2520a%2520conical%2520frustum%2520for%2520fruit%2520isolation%252C%2520with%2520an%2520integrated%2520micro-servo%2520cutter%2520for%2520pedicel%2520cutting.%2520For%2520perception%252C%2520an%2520RGB--D%2520camera%2520and%2520a%2520Detectron2-based%2520pipeline%2520perform%2520semantic%2520segmentation%2520of%2520ripe/unripe%2520tomatoes%2520and%2520keypoint%2520localization%2520of%2520the%2520pedicel%2520and%2520fruit%2520center%2520under%2520occlusion%2520and%2520variable%2520illumination.%2520An%2520analytical%2520model%2520derived%2520using%2520the%2520principle%2520of%2520virtual%2520work%2520relates%2520servo%2520torque%2520to%2520grasp%2520force%252C%2520enabling%2520design-level%2520reasoning%2520about%2520actuation%2520requirements.%2520During%2520execution%252C%2520closed-loop%2520grasp-force%2520regulation%2520is%2520achieved%2520using%2520a%2520proportional--integral--derivative%2520controller%2520with%2520feedback%2520from%2520force-sensitive%2520resistors%2520mounted%2520on%2520selected%2520fingers%2520to%2520prevent%2520slip%2520and%2520bruising.%2520Motion%2520execution%2520is%2520supported%2520by%2520Particle%2520Swarm%2520Optimization%2520%2528PSO%2529--based%2520trajectory%2520planning%2520for%2520a%25205-DOF%2520manipulator.%2520Experiments%2520demonstrate%2520complete%2520picking%2520cycles%2520%2528approach%252C%2520separation%252C%2520cutting%252C%2520grasping%252C%2520transport%252C%2520release%2529%2520with%2520an%2520average%2520cycle%2520time%2520of%252024.34~s%2520and%2520an%2520overall%2520success%2520rate%2520of%2520approximately%252080%255C%2525%252C%2520while%2520maintaining%2520low%2520grasp%2520forces%2520%25280.20--0.50~N%2529.%2520These%2520results%2520validate%2520the%2520proposed%2520hybrid%2520gripper%2520and%2520integrated%2520vision--control%2520pipeline%2520for%2520reliable%2520harvesting%2520in%2520cluttered%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Approach%20to%20Tomato%20Harvesting%20Using%20a%20Hybrid%20Gripper%20with%20Semantic%20Segmentation%20and%20Keypoint%20Detection&entry.906535625=Shahid%20Ansari%20and%20Mahendra%20Kumar%20Gohil%20and%20Yusuke%20Maeda%20and%20Bishakh%20Bhattacharya&entry.1292438233=This%20paper%20presents%20an%20autonomous%20tomato-harvesting%20system%20built%20around%20a%20hybrid%20robotic%20gripper%20that%20combines%20six%20soft%20auxetic%20fingers%20with%20a%20rigid%20exoskeleton%20and%20a%20latex%20basket%20to%20achieve%20gentle%2C%20cage-like%20grasping.%20The%20gripper%20is%20driven%20by%20a%20servo-actuated%20Scotch--yoke%20mechanism%2C%20and%20includes%20separator%20leaves%20that%20form%20a%20conical%20frustum%20for%20fruit%20isolation%2C%20with%20an%20integrated%20micro-servo%20cutter%20for%20pedicel%20cutting.%20For%20perception%2C%20an%20RGB--D%20camera%20and%20a%20Detectron2-based%20pipeline%20perform%20semantic%20segmentation%20of%20ripe/unripe%20tomatoes%20and%20keypoint%20localization%20of%20the%20pedicel%20and%20fruit%20center%20under%20occlusion%20and%20variable%20illumination.%20An%20analytical%20model%20derived%20using%20the%20principle%20of%20virtual%20work%20relates%20servo%20torque%20to%20grasp%20force%2C%20enabling%20design-level%20reasoning%20about%20actuation%20requirements.%20During%20execution%2C%20closed-loop%20grasp-force%20regulation%20is%20achieved%20using%20a%20proportional--integral--derivative%20controller%20with%20feedback%20from%20force-sensitive%20resistors%20mounted%20on%20selected%20fingers%20to%20prevent%20slip%20and%20bruising.%20Motion%20execution%20is%20supported%20by%20Particle%20Swarm%20Optimization%20%28PSO%29--based%20trajectory%20planning%20for%20a%205-DOF%20manipulator.%20Experiments%20demonstrate%20complete%20picking%20cycles%20%28approach%2C%20separation%2C%20cutting%2C%20grasping%2C%20transport%2C%20release%29%20with%20an%20average%20cycle%20time%20of%2024.34~s%20and%20an%20overall%20success%20rate%20of%20approximately%2080%5C%25%2C%20while%20maintaining%20low%20grasp%20forces%20%280.20--0.50~N%29.%20These%20results%20validate%20the%20proposed%20hybrid%20gripper%20and%20integrated%20vision--control%20pipeline%20for%20reliable%20harvesting%20in%20cluttered%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.03684v1&entry.124074799=Read"},
{"title": "In-Context Representation Hijacking", "author": "Itay Yona and Amir Sarid and Michael Karasik and Yossi Gandelsman", "abstract": "We introduce \\textbf{Doublespeak}, a simple \\emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \\textit{bomb}) with a benign token (e.g., \\textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.", "link": "http://arxiv.org/abs/2512.03771v1", "date": "2025-12-03", "relevancy": 2.1936, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4439}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4439}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Representation%20Hijacking&body=Title%3A%20In-Context%20Representation%20Hijacking%0AAuthor%3A%20Itay%20Yona%20and%20Amir%20Sarid%20and%20Michael%20Karasik%20and%20Yossi%20Gandelsman%0AAbstract%3A%20We%20introduce%20%5Ctextbf%7BDoublespeak%7D%2C%20a%20simple%20%5Cemph%7Bin-context%20representation%20hijacking%7D%20attack%20against%20large%20language%20models%20%28LLMs%29.%20The%20attack%20works%20by%20systematically%20replacing%20a%20harmful%20keyword%20%28e.g.%2C%20%5Ctextit%7Bbomb%7D%29%20with%20a%20benign%20token%20%28e.g.%2C%20%5Ctextit%7Bcarrot%7D%29%20across%20multiple%20in-context%20examples%2C%20provided%20a%20prefix%20to%20a%20harmful%20request.%20We%20demonstrate%20that%20this%20substitution%20leads%20to%20the%20internal%20representation%20of%20the%20benign%20token%20converging%20toward%20that%20of%20the%20harmful%20one%2C%20effectively%20embedding%20the%20harmful%20semantics%20under%20a%20euphemism.%20As%20a%20result%2C%20superficially%20innocuous%20prompts%20%28e.g.%2C%20%60%60How%20to%20build%20a%20carrot%3F%27%27%29%20are%20internally%20interpreted%20as%20disallowed%20instructions%20%28e.g.%2C%20%60%60How%20to%20build%20a%20bomb%3F%27%27%29%2C%20thereby%20bypassing%20the%20model%27s%20safety%20alignment.%20We%20use%20interpretability%20tools%20to%20show%20that%20this%20semantic%20overwrite%20emerges%20layer%20by%20layer%2C%20with%20benign%20meanings%20in%20early%20layers%20converging%20into%20harmful%20semantics%20in%20later%20ones.%20Doublespeak%20is%20optimization-free%2C%20broadly%20transferable%20across%20model%20families%2C%20and%20achieves%20strong%20success%20rates%20on%20closed-source%20and%20open-source%20systems%2C%20reaching%2074%5C%25%20ASR%20on%20Llama-3.3-70B-Instruct%20with%20a%20single-sentence%20context%20override.%20Our%20findings%20highlight%20a%20new%20attack%20surface%20in%20the%20latent%20space%20of%20LLMs%2C%20revealing%20that%20current%20alignment%20strategies%20are%20insufficient%20and%20should%20instead%20operate%20at%20the%20representation%20level.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Representation%2520Hijacking%26entry.906535625%3DItay%2520Yona%2520and%2520Amir%2520Sarid%2520and%2520Michael%2520Karasik%2520and%2520Yossi%2520Gandelsman%26entry.1292438233%3DWe%2520introduce%2520%255Ctextbf%257BDoublespeak%257D%252C%2520a%2520simple%2520%255Cemph%257Bin-context%2520representation%2520hijacking%257D%2520attack%2520against%2520large%2520language%2520models%2520%2528LLMs%2529.%2520The%2520attack%2520works%2520by%2520systematically%2520replacing%2520a%2520harmful%2520keyword%2520%2528e.g.%252C%2520%255Ctextit%257Bbomb%257D%2529%2520with%2520a%2520benign%2520token%2520%2528e.g.%252C%2520%255Ctextit%257Bcarrot%257D%2529%2520across%2520multiple%2520in-context%2520examples%252C%2520provided%2520a%2520prefix%2520to%2520a%2520harmful%2520request.%2520We%2520demonstrate%2520that%2520this%2520substitution%2520leads%2520to%2520the%2520internal%2520representation%2520of%2520the%2520benign%2520token%2520converging%2520toward%2520that%2520of%2520the%2520harmful%2520one%252C%2520effectively%2520embedding%2520the%2520harmful%2520semantics%2520under%2520a%2520euphemism.%2520As%2520a%2520result%252C%2520superficially%2520innocuous%2520prompts%2520%2528e.g.%252C%2520%2560%2560How%2520to%2520build%2520a%2520carrot%253F%2527%2527%2529%2520are%2520internally%2520interpreted%2520as%2520disallowed%2520instructions%2520%2528e.g.%252C%2520%2560%2560How%2520to%2520build%2520a%2520bomb%253F%2527%2527%2529%252C%2520thereby%2520bypassing%2520the%2520model%2527s%2520safety%2520alignment.%2520We%2520use%2520interpretability%2520tools%2520to%2520show%2520that%2520this%2520semantic%2520overwrite%2520emerges%2520layer%2520by%2520layer%252C%2520with%2520benign%2520meanings%2520in%2520early%2520layers%2520converging%2520into%2520harmful%2520semantics%2520in%2520later%2520ones.%2520Doublespeak%2520is%2520optimization-free%252C%2520broadly%2520transferable%2520across%2520model%2520families%252C%2520and%2520achieves%2520strong%2520success%2520rates%2520on%2520closed-source%2520and%2520open-source%2520systems%252C%2520reaching%252074%255C%2525%2520ASR%2520on%2520Llama-3.3-70B-Instruct%2520with%2520a%2520single-sentence%2520context%2520override.%2520Our%2520findings%2520highlight%2520a%2520new%2520attack%2520surface%2520in%2520the%2520latent%2520space%2520of%2520LLMs%252C%2520revealing%2520that%2520current%2520alignment%2520strategies%2520are%2520insufficient%2520and%2520should%2520instead%2520operate%2520at%2520the%2520representation%2520level.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Representation%20Hijacking&entry.906535625=Itay%20Yona%20and%20Amir%20Sarid%20and%20Michael%20Karasik%20and%20Yossi%20Gandelsman&entry.1292438233=We%20introduce%20%5Ctextbf%7BDoublespeak%7D%2C%20a%20simple%20%5Cemph%7Bin-context%20representation%20hijacking%7D%20attack%20against%20large%20language%20models%20%28LLMs%29.%20The%20attack%20works%20by%20systematically%20replacing%20a%20harmful%20keyword%20%28e.g.%2C%20%5Ctextit%7Bbomb%7D%29%20with%20a%20benign%20token%20%28e.g.%2C%20%5Ctextit%7Bcarrot%7D%29%20across%20multiple%20in-context%20examples%2C%20provided%20a%20prefix%20to%20a%20harmful%20request.%20We%20demonstrate%20that%20this%20substitution%20leads%20to%20the%20internal%20representation%20of%20the%20benign%20token%20converging%20toward%20that%20of%20the%20harmful%20one%2C%20effectively%20embedding%20the%20harmful%20semantics%20under%20a%20euphemism.%20As%20a%20result%2C%20superficially%20innocuous%20prompts%20%28e.g.%2C%20%60%60How%20to%20build%20a%20carrot%3F%27%27%29%20are%20internally%20interpreted%20as%20disallowed%20instructions%20%28e.g.%2C%20%60%60How%20to%20build%20a%20bomb%3F%27%27%29%2C%20thereby%20bypassing%20the%20model%27s%20safety%20alignment.%20We%20use%20interpretability%20tools%20to%20show%20that%20this%20semantic%20overwrite%20emerges%20layer%20by%20layer%2C%20with%20benign%20meanings%20in%20early%20layers%20converging%20into%20harmful%20semantics%20in%20later%20ones.%20Doublespeak%20is%20optimization-free%2C%20broadly%20transferable%20across%20model%20families%2C%20and%20achieves%20strong%20success%20rates%20on%20closed-source%20and%20open-source%20systems%2C%20reaching%2074%5C%25%20ASR%20on%20Llama-3.3-70B-Instruct%20with%20a%20single-sentence%20context%20override.%20Our%20findings%20highlight%20a%20new%20attack%20surface%20in%20the%20latent%20space%20of%20LLMs%2C%20revealing%20that%20current%20alignment%20strategies%20are%20insufficient%20and%20should%20instead%20operate%20at%20the%20representation%20level.&entry.1838667208=http%3A//arxiv.org/abs/2512.03771v1&entry.124074799=Read"},
{"title": "Unique Lives, Shared World: Learning from Single-Life Videos", "author": "Tengda Han and Sayna Ebrahimi and Dilara Gokay and Li Yang Ku and Maks Ovsjanikov and Iva Babukova and Daniel Zoran and Viorica Patraucean and Joao Carreira and Andrew Zisserman and Dima Damen", "abstract": "We introduce the \"single-life\" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual. We leverage the multiple viewpoints naturally captured within a single life to learn a visual encoder in a self-supervised manner. Our experiments demonstrate three key findings. First, models trained independently on different lives develop a highly aligned geometric understanding. We demonstrate this by training visual encoders on distinct datasets each capturing a different life, both indoors and outdoors, as well as introducing a novel cross-attention-based metric to quantify the functional alignment of the internal representations developed by different models. Second, we show that single-life models learn generalizable geometric representations that effectively transfer to downstream tasks, such as depth estimation, in unseen environments. Third, we demonstrate that training on up to 30 hours from one week of the same person's life leads to comparable performance to training on 30 hours of diverse web data, highlighting the strength of single-life representation learning. Overall, our results establish that the shared structure of the world, both leads to consistency in models trained on individual lives, and provides a powerful signal for visual representation learning.", "link": "http://arxiv.org/abs/2512.04085v1", "date": "2025-12-03", "relevancy": 2.1923, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unique%20Lives%2C%20Shared%20World%3A%20Learning%20from%20Single-Life%20Videos&body=Title%3A%20Unique%20Lives%2C%20Shared%20World%3A%20Learning%20from%20Single-Life%20Videos%0AAuthor%3A%20Tengda%20Han%20and%20Sayna%20Ebrahimi%20and%20Dilara%20Gokay%20and%20Li%20Yang%20Ku%20and%20Maks%20Ovsjanikov%20and%20Iva%20Babukova%20and%20Daniel%20Zoran%20and%20Viorica%20Patraucean%20and%20Joao%20Carreira%20and%20Andrew%20Zisserman%20and%20Dima%20Damen%0AAbstract%3A%20We%20introduce%20the%20%22single-life%22%20learning%20paradigm%2C%20where%20we%20train%20a%20distinct%20vision%20model%20exclusively%20on%20egocentric%20videos%20captured%20by%20one%20individual.%20We%20leverage%20the%20multiple%20viewpoints%20naturally%20captured%20within%20a%20single%20life%20to%20learn%20a%20visual%20encoder%20in%20a%20self-supervised%20manner.%20Our%20experiments%20demonstrate%20three%20key%20findings.%20First%2C%20models%20trained%20independently%20on%20different%20lives%20develop%20a%20highly%20aligned%20geometric%20understanding.%20We%20demonstrate%20this%20by%20training%20visual%20encoders%20on%20distinct%20datasets%20each%20capturing%20a%20different%20life%2C%20both%20indoors%20and%20outdoors%2C%20as%20well%20as%20introducing%20a%20novel%20cross-attention-based%20metric%20to%20quantify%20the%20functional%20alignment%20of%20the%20internal%20representations%20developed%20by%20different%20models.%20Second%2C%20we%20show%20that%20single-life%20models%20learn%20generalizable%20geometric%20representations%20that%20effectively%20transfer%20to%20downstream%20tasks%2C%20such%20as%20depth%20estimation%2C%20in%20unseen%20environments.%20Third%2C%20we%20demonstrate%20that%20training%20on%20up%20to%2030%20hours%20from%20one%20week%20of%20the%20same%20person%27s%20life%20leads%20to%20comparable%20performance%20to%20training%20on%2030%20hours%20of%20diverse%20web%20data%2C%20highlighting%20the%20strength%20of%20single-life%20representation%20learning.%20Overall%2C%20our%20results%20establish%20that%20the%20shared%20structure%20of%20the%20world%2C%20both%20leads%20to%20consistency%20in%20models%20trained%20on%20individual%20lives%2C%20and%20provides%20a%20powerful%20signal%20for%20visual%20representation%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnique%2520Lives%252C%2520Shared%2520World%253A%2520Learning%2520from%2520Single-Life%2520Videos%26entry.906535625%3DTengda%2520Han%2520and%2520Sayna%2520Ebrahimi%2520and%2520Dilara%2520Gokay%2520and%2520Li%2520Yang%2520Ku%2520and%2520Maks%2520Ovsjanikov%2520and%2520Iva%2520Babukova%2520and%2520Daniel%2520Zoran%2520and%2520Viorica%2520Patraucean%2520and%2520Joao%2520Carreira%2520and%2520Andrew%2520Zisserman%2520and%2520Dima%2520Damen%26entry.1292438233%3DWe%2520introduce%2520the%2520%2522single-life%2522%2520learning%2520paradigm%252C%2520where%2520we%2520train%2520a%2520distinct%2520vision%2520model%2520exclusively%2520on%2520egocentric%2520videos%2520captured%2520by%2520one%2520individual.%2520We%2520leverage%2520the%2520multiple%2520viewpoints%2520naturally%2520captured%2520within%2520a%2520single%2520life%2520to%2520learn%2520a%2520visual%2520encoder%2520in%2520a%2520self-supervised%2520manner.%2520Our%2520experiments%2520demonstrate%2520three%2520key%2520findings.%2520First%252C%2520models%2520trained%2520independently%2520on%2520different%2520lives%2520develop%2520a%2520highly%2520aligned%2520geometric%2520understanding.%2520We%2520demonstrate%2520this%2520by%2520training%2520visual%2520encoders%2520on%2520distinct%2520datasets%2520each%2520capturing%2520a%2520different%2520life%252C%2520both%2520indoors%2520and%2520outdoors%252C%2520as%2520well%2520as%2520introducing%2520a%2520novel%2520cross-attention-based%2520metric%2520to%2520quantify%2520the%2520functional%2520alignment%2520of%2520the%2520internal%2520representations%2520developed%2520by%2520different%2520models.%2520Second%252C%2520we%2520show%2520that%2520single-life%2520models%2520learn%2520generalizable%2520geometric%2520representations%2520that%2520effectively%2520transfer%2520to%2520downstream%2520tasks%252C%2520such%2520as%2520depth%2520estimation%252C%2520in%2520unseen%2520environments.%2520Third%252C%2520we%2520demonstrate%2520that%2520training%2520on%2520up%2520to%252030%2520hours%2520from%2520one%2520week%2520of%2520the%2520same%2520person%2527s%2520life%2520leads%2520to%2520comparable%2520performance%2520to%2520training%2520on%252030%2520hours%2520of%2520diverse%2520web%2520data%252C%2520highlighting%2520the%2520strength%2520of%2520single-life%2520representation%2520learning.%2520Overall%252C%2520our%2520results%2520establish%2520that%2520the%2520shared%2520structure%2520of%2520the%2520world%252C%2520both%2520leads%2520to%2520consistency%2520in%2520models%2520trained%2520on%2520individual%2520lives%252C%2520and%2520provides%2520a%2520powerful%2520signal%2520for%2520visual%2520representation%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unique%20Lives%2C%20Shared%20World%3A%20Learning%20from%20Single-Life%20Videos&entry.906535625=Tengda%20Han%20and%20Sayna%20Ebrahimi%20and%20Dilara%20Gokay%20and%20Li%20Yang%20Ku%20and%20Maks%20Ovsjanikov%20and%20Iva%20Babukova%20and%20Daniel%20Zoran%20and%20Viorica%20Patraucean%20and%20Joao%20Carreira%20and%20Andrew%20Zisserman%20and%20Dima%20Damen&entry.1292438233=We%20introduce%20the%20%22single-life%22%20learning%20paradigm%2C%20where%20we%20train%20a%20distinct%20vision%20model%20exclusively%20on%20egocentric%20videos%20captured%20by%20one%20individual.%20We%20leverage%20the%20multiple%20viewpoints%20naturally%20captured%20within%20a%20single%20life%20to%20learn%20a%20visual%20encoder%20in%20a%20self-supervised%20manner.%20Our%20experiments%20demonstrate%20three%20key%20findings.%20First%2C%20models%20trained%20independently%20on%20different%20lives%20develop%20a%20highly%20aligned%20geometric%20understanding.%20We%20demonstrate%20this%20by%20training%20visual%20encoders%20on%20distinct%20datasets%20each%20capturing%20a%20different%20life%2C%20both%20indoors%20and%20outdoors%2C%20as%20well%20as%20introducing%20a%20novel%20cross-attention-based%20metric%20to%20quantify%20the%20functional%20alignment%20of%20the%20internal%20representations%20developed%20by%20different%20models.%20Second%2C%20we%20show%20that%20single-life%20models%20learn%20generalizable%20geometric%20representations%20that%20effectively%20transfer%20to%20downstream%20tasks%2C%20such%20as%20depth%20estimation%2C%20in%20unseen%20environments.%20Third%2C%20we%20demonstrate%20that%20training%20on%20up%20to%2030%20hours%20from%20one%20week%20of%20the%20same%20person%27s%20life%20leads%20to%20comparable%20performance%20to%20training%20on%2030%20hours%20of%20diverse%20web%20data%2C%20highlighting%20the%20strength%20of%20single-life%20representation%20learning.%20Overall%2C%20our%20results%20establish%20that%20the%20shared%20structure%20of%20the%20world%2C%20both%20leads%20to%20consistency%20in%20models%20trained%20on%20individual%20lives%2C%20and%20provides%20a%20powerful%20signal%20for%20visual%20representation%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2512.04085v1&entry.124074799=Read"},
{"title": "PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention", "author": "Ziwen Li and Xin Wang and Hanlue Zhang and Runnan Chen and Runqi Lin and Xiao He and Han Huang and Yandong Guo and Fakhri Karray and Tongliang Liu and Mingming Gong", "abstract": "The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.", "link": "http://arxiv.org/abs/2512.03724v1", "date": "2025-12-03", "relevancy": 2.1907, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5763}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PosA-VLA%3A%20Enhancing%20Action%20Generation%20via%20Pose-Conditioned%20Anchor%20Attention&body=Title%3A%20PosA-VLA%3A%20Enhancing%20Action%20Generation%20via%20Pose-Conditioned%20Anchor%20Attention%0AAuthor%3A%20Ziwen%20Li%20and%20Xin%20Wang%20and%20Hanlue%20Zhang%20and%20Runnan%20Chen%20and%20Runqi%20Lin%20and%20Xiao%20He%20and%20Han%20Huang%20and%20Yandong%20Guo%20and%20Fakhri%20Karray%20and%20Tongliang%20Liu%20and%20Mingming%20Gong%0AAbstract%3A%20The%20Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20remarkable%20performance%20on%20embodied%20tasks%20and%20shown%20promising%20potential%20for%20real-world%20applications.%20However%2C%20current%20VLAs%20still%20struggle%20to%20produce%20consistent%20and%20precise%20target-oriented%20actions%2C%20as%20they%20often%20generate%20redundant%20or%20unstable%20motions%20along%20trajectories%2C%20limiting%20their%20applicability%20in%20time-sensitive%20scenarios.In%20this%20work%2C%20we%20attribute%20these%20redundant%20actions%20to%20the%20spatially%20uniform%20perception%20field%20of%20existing%20VLAs%2C%20which%20causes%20them%20to%20be%20distracted%20by%20target-irrelevant%20objects%2C%20especially%20in%20complex%20environments.To%20address%20this%20issue%2C%20we%20propose%20an%20efficient%20PosA-VLA%20framework%20that%20anchors%20visual%20attention%20via%20pose-conditioned%20supervision%2C%20consistently%20guiding%20the%20model%27s%20perception%20toward%20task-relevant%20regions.%20The%20pose-conditioned%20anchor%20attention%20mechanism%20enables%20the%20model%20to%20better%20align%20instruction%20semantics%20with%20actionable%20visual%20cues%2C%20thereby%20improving%20action%20generation%20precision%20and%20efficiency.%20Moreover%2C%20our%20framework%20adopts%20a%20lightweight%20architecture%20and%20requires%20no%20auxiliary%20perception%20modules%20%28e.g.%2C%20segmentation%20or%20grounding%20networks%29%2C%20ensuring%20efficient%20inference.%20Extensive%20experiments%20verify%20that%20our%20method%20executes%20embodied%20tasks%20with%20precise%20and%20time-efficient%20behavior%20across%20diverse%20robotic%20manipulation%20benchmarks%20and%20shows%20robust%20generalization%20in%20a%20variety%20of%20challenging%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosA-VLA%253A%2520Enhancing%2520Action%2520Generation%2520via%2520Pose-Conditioned%2520Anchor%2520Attention%26entry.906535625%3DZiwen%2520Li%2520and%2520Xin%2520Wang%2520and%2520Hanlue%2520Zhang%2520and%2520Runnan%2520Chen%2520and%2520Runqi%2520Lin%2520and%2520Xiao%2520He%2520and%2520Han%2520Huang%2520and%2520Yandong%2520Guo%2520and%2520Fakhri%2520Karray%2520and%2520Tongliang%2520Liu%2520and%2520Mingming%2520Gong%26entry.1292438233%3DThe%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520demonstrated%2520remarkable%2520performance%2520on%2520embodied%2520tasks%2520and%2520shown%2520promising%2520potential%2520for%2520real-world%2520applications.%2520However%252C%2520current%2520VLAs%2520still%2520struggle%2520to%2520produce%2520consistent%2520and%2520precise%2520target-oriented%2520actions%252C%2520as%2520they%2520often%2520generate%2520redundant%2520or%2520unstable%2520motions%2520along%2520trajectories%252C%2520limiting%2520their%2520applicability%2520in%2520time-sensitive%2520scenarios.In%2520this%2520work%252C%2520we%2520attribute%2520these%2520redundant%2520actions%2520to%2520the%2520spatially%2520uniform%2520perception%2520field%2520of%2520existing%2520VLAs%252C%2520which%2520causes%2520them%2520to%2520be%2520distracted%2520by%2520target-irrelevant%2520objects%252C%2520especially%2520in%2520complex%2520environments.To%2520address%2520this%2520issue%252C%2520we%2520propose%2520an%2520efficient%2520PosA-VLA%2520framework%2520that%2520anchors%2520visual%2520attention%2520via%2520pose-conditioned%2520supervision%252C%2520consistently%2520guiding%2520the%2520model%2527s%2520perception%2520toward%2520task-relevant%2520regions.%2520The%2520pose-conditioned%2520anchor%2520attention%2520mechanism%2520enables%2520the%2520model%2520to%2520better%2520align%2520instruction%2520semantics%2520with%2520actionable%2520visual%2520cues%252C%2520thereby%2520improving%2520action%2520generation%2520precision%2520and%2520efficiency.%2520Moreover%252C%2520our%2520framework%2520adopts%2520a%2520lightweight%2520architecture%2520and%2520requires%2520no%2520auxiliary%2520perception%2520modules%2520%2528e.g.%252C%2520segmentation%2520or%2520grounding%2520networks%2529%252C%2520ensuring%2520efficient%2520inference.%2520Extensive%2520experiments%2520verify%2520that%2520our%2520method%2520executes%2520embodied%2520tasks%2520with%2520precise%2520and%2520time-efficient%2520behavior%2520across%2520diverse%2520robotic%2520manipulation%2520benchmarks%2520and%2520shows%2520robust%2520generalization%2520in%2520a%2520variety%2520of%2520challenging%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PosA-VLA%3A%20Enhancing%20Action%20Generation%20via%20Pose-Conditioned%20Anchor%20Attention&entry.906535625=Ziwen%20Li%20and%20Xin%20Wang%20and%20Hanlue%20Zhang%20and%20Runnan%20Chen%20and%20Runqi%20Lin%20and%20Xiao%20He%20and%20Han%20Huang%20and%20Yandong%20Guo%20and%20Fakhri%20Karray%20and%20Tongliang%20Liu%20and%20Mingming%20Gong&entry.1292438233=The%20Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20remarkable%20performance%20on%20embodied%20tasks%20and%20shown%20promising%20potential%20for%20real-world%20applications.%20However%2C%20current%20VLAs%20still%20struggle%20to%20produce%20consistent%20and%20precise%20target-oriented%20actions%2C%20as%20they%20often%20generate%20redundant%20or%20unstable%20motions%20along%20trajectories%2C%20limiting%20their%20applicability%20in%20time-sensitive%20scenarios.In%20this%20work%2C%20we%20attribute%20these%20redundant%20actions%20to%20the%20spatially%20uniform%20perception%20field%20of%20existing%20VLAs%2C%20which%20causes%20them%20to%20be%20distracted%20by%20target-irrelevant%20objects%2C%20especially%20in%20complex%20environments.To%20address%20this%20issue%2C%20we%20propose%20an%20efficient%20PosA-VLA%20framework%20that%20anchors%20visual%20attention%20via%20pose-conditioned%20supervision%2C%20consistently%20guiding%20the%20model%27s%20perception%20toward%20task-relevant%20regions.%20The%20pose-conditioned%20anchor%20attention%20mechanism%20enables%20the%20model%20to%20better%20align%20instruction%20semantics%20with%20actionable%20visual%20cues%2C%20thereby%20improving%20action%20generation%20precision%20and%20efficiency.%20Moreover%2C%20our%20framework%20adopts%20a%20lightweight%20architecture%20and%20requires%20no%20auxiliary%20perception%20modules%20%28e.g.%2C%20segmentation%20or%20grounding%20networks%29%2C%20ensuring%20efficient%20inference.%20Extensive%20experiments%20verify%20that%20our%20method%20executes%20embodied%20tasks%20with%20precise%20and%20time-efficient%20behavior%20across%20diverse%20robotic%20manipulation%20benchmarks%20and%20shows%20robust%20generalization%20in%20a%20variety%20of%20challenging%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.03724v1&entry.124074799=Read"},
{"title": "Test-time Correction: An Online 3D Detection System via Visual Prompting", "author": "Hanxue Zhang and Zetong Yang and Yanan Sun and Li Chen and Fei Xia and Fatma G\u00fcney and Hongyang Li", "abstract": "This paper introduces Test-time Correction (TTC), an online 3D detection system designed to rectify test-time errors using various auxiliary feedback, aiming to enhance the safety of deployed autonomous driving systems. Unlike conventional offline 3D detectors that remain fixed during inference, TTC enables immediate online error correction without retraining, allowing autonomous vehicles to adapt to new scenarios and reduce deployment risks. To achieve this, we equip existing 3D detectors with an Online Adapter (OA) module -- a prompt-driven query generator for real-time correction. At the core of OA module are visual prompts: image-based descriptions of objects of interest derived from auxiliary feedback such as mismatches with 2D detections, road descriptions, or user clicks. These visual prompts, collected from risky objects during inference, are maintained in a visual prompt buffer to enable continuous correction in future frames. By leveraging this mechanism, TTC consistently detects risky objects, achieving reliable, adaptive, and versatile driving autonomy. Extensive experiments show that TTC significantly improves instant error rectification over frozen 3D detectors, even under limited labels, zero-shot settings, and adverse conditions. We hope this work inspires future research on post-deployment online rectification systems for autonomous driving.", "link": "http://arxiv.org/abs/2412.07768v3", "date": "2025-12-03", "relevancy": 2.1768, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5455}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5438}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-time%20Correction%3A%20An%20Online%203D%20Detection%20System%20via%20Visual%20Prompting&body=Title%3A%20Test-time%20Correction%3A%20An%20Online%203D%20Detection%20System%20via%20Visual%20Prompting%0AAuthor%3A%20Hanxue%20Zhang%20and%20Zetong%20Yang%20and%20Yanan%20Sun%20and%20Li%20Chen%20and%20Fei%20Xia%20and%20Fatma%20G%C3%BCney%20and%20Hongyang%20Li%0AAbstract%3A%20This%20paper%20introduces%20Test-time%20Correction%20%28TTC%29%2C%20an%20online%203D%20detection%20system%20designed%20to%20rectify%20test-time%20errors%20using%20various%20auxiliary%20feedback%2C%20aiming%20to%20enhance%20the%20safety%20of%20deployed%20autonomous%20driving%20systems.%20Unlike%20conventional%20offline%203D%20detectors%20that%20remain%20fixed%20during%20inference%2C%20TTC%20enables%20immediate%20online%20error%20correction%20without%20retraining%2C%20allowing%20autonomous%20vehicles%20to%20adapt%20to%20new%20scenarios%20and%20reduce%20deployment%20risks.%20To%20achieve%20this%2C%20we%20equip%20existing%203D%20detectors%20with%20an%20Online%20Adapter%20%28OA%29%20module%20--%20a%20prompt-driven%20query%20generator%20for%20real-time%20correction.%20At%20the%20core%20of%20OA%20module%20are%20visual%20prompts%3A%20image-based%20descriptions%20of%20objects%20of%20interest%20derived%20from%20auxiliary%20feedback%20such%20as%20mismatches%20with%202D%20detections%2C%20road%20descriptions%2C%20or%20user%20clicks.%20These%20visual%20prompts%2C%20collected%20from%20risky%20objects%20during%20inference%2C%20are%20maintained%20in%20a%20visual%20prompt%20buffer%20to%20enable%20continuous%20correction%20in%20future%20frames.%20By%20leveraging%20this%20mechanism%2C%20TTC%20consistently%20detects%20risky%20objects%2C%20achieving%20reliable%2C%20adaptive%2C%20and%20versatile%20driving%20autonomy.%20Extensive%20experiments%20show%20that%20TTC%20significantly%20improves%20instant%20error%20rectification%20over%20frozen%203D%20detectors%2C%20even%20under%20limited%20labels%2C%20zero-shot%20settings%2C%20and%20adverse%20conditions.%20We%20hope%20this%20work%20inspires%20future%20research%20on%20post-deployment%20online%20rectification%20systems%20for%20autonomous%20driving.%0ALink%3A%20http%3A//arxiv.org/abs/2412.07768v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-time%2520Correction%253A%2520An%2520Online%25203D%2520Detection%2520System%2520via%2520Visual%2520Prompting%26entry.906535625%3DHanxue%2520Zhang%2520and%2520Zetong%2520Yang%2520and%2520Yanan%2520Sun%2520and%2520Li%2520Chen%2520and%2520Fei%2520Xia%2520and%2520Fatma%2520G%25C3%25BCney%2520and%2520Hongyang%2520Li%26entry.1292438233%3DThis%2520paper%2520introduces%2520Test-time%2520Correction%2520%2528TTC%2529%252C%2520an%2520online%25203D%2520detection%2520system%2520designed%2520to%2520rectify%2520test-time%2520errors%2520using%2520various%2520auxiliary%2520feedback%252C%2520aiming%2520to%2520enhance%2520the%2520safety%2520of%2520deployed%2520autonomous%2520driving%2520systems.%2520Unlike%2520conventional%2520offline%25203D%2520detectors%2520that%2520remain%2520fixed%2520during%2520inference%252C%2520TTC%2520enables%2520immediate%2520online%2520error%2520correction%2520without%2520retraining%252C%2520allowing%2520autonomous%2520vehicles%2520to%2520adapt%2520to%2520new%2520scenarios%2520and%2520reduce%2520deployment%2520risks.%2520To%2520achieve%2520this%252C%2520we%2520equip%2520existing%25203D%2520detectors%2520with%2520an%2520Online%2520Adapter%2520%2528OA%2529%2520module%2520--%2520a%2520prompt-driven%2520query%2520generator%2520for%2520real-time%2520correction.%2520At%2520the%2520core%2520of%2520OA%2520module%2520are%2520visual%2520prompts%253A%2520image-based%2520descriptions%2520of%2520objects%2520of%2520interest%2520derived%2520from%2520auxiliary%2520feedback%2520such%2520as%2520mismatches%2520with%25202D%2520detections%252C%2520road%2520descriptions%252C%2520or%2520user%2520clicks.%2520These%2520visual%2520prompts%252C%2520collected%2520from%2520risky%2520objects%2520during%2520inference%252C%2520are%2520maintained%2520in%2520a%2520visual%2520prompt%2520buffer%2520to%2520enable%2520continuous%2520correction%2520in%2520future%2520frames.%2520By%2520leveraging%2520this%2520mechanism%252C%2520TTC%2520consistently%2520detects%2520risky%2520objects%252C%2520achieving%2520reliable%252C%2520adaptive%252C%2520and%2520versatile%2520driving%2520autonomy.%2520Extensive%2520experiments%2520show%2520that%2520TTC%2520significantly%2520improves%2520instant%2520error%2520rectification%2520over%2520frozen%25203D%2520detectors%252C%2520even%2520under%2520limited%2520labels%252C%2520zero-shot%2520settings%252C%2520and%2520adverse%2520conditions.%2520We%2520hope%2520this%2520work%2520inspires%2520future%2520research%2520on%2520post-deployment%2520online%2520rectification%2520systems%2520for%2520autonomous%2520driving.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07768v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-time%20Correction%3A%20An%20Online%203D%20Detection%20System%20via%20Visual%20Prompting&entry.906535625=Hanxue%20Zhang%20and%20Zetong%20Yang%20and%20Yanan%20Sun%20and%20Li%20Chen%20and%20Fei%20Xia%20and%20Fatma%20G%C3%BCney%20and%20Hongyang%20Li&entry.1292438233=This%20paper%20introduces%20Test-time%20Correction%20%28TTC%29%2C%20an%20online%203D%20detection%20system%20designed%20to%20rectify%20test-time%20errors%20using%20various%20auxiliary%20feedback%2C%20aiming%20to%20enhance%20the%20safety%20of%20deployed%20autonomous%20driving%20systems.%20Unlike%20conventional%20offline%203D%20detectors%20that%20remain%20fixed%20during%20inference%2C%20TTC%20enables%20immediate%20online%20error%20correction%20without%20retraining%2C%20allowing%20autonomous%20vehicles%20to%20adapt%20to%20new%20scenarios%20and%20reduce%20deployment%20risks.%20To%20achieve%20this%2C%20we%20equip%20existing%203D%20detectors%20with%20an%20Online%20Adapter%20%28OA%29%20module%20--%20a%20prompt-driven%20query%20generator%20for%20real-time%20correction.%20At%20the%20core%20of%20OA%20module%20are%20visual%20prompts%3A%20image-based%20descriptions%20of%20objects%20of%20interest%20derived%20from%20auxiliary%20feedback%20such%20as%20mismatches%20with%202D%20detections%2C%20road%20descriptions%2C%20or%20user%20clicks.%20These%20visual%20prompts%2C%20collected%20from%20risky%20objects%20during%20inference%2C%20are%20maintained%20in%20a%20visual%20prompt%20buffer%20to%20enable%20continuous%20correction%20in%20future%20frames.%20By%20leveraging%20this%20mechanism%2C%20TTC%20consistently%20detects%20risky%20objects%2C%20achieving%20reliable%2C%20adaptive%2C%20and%20versatile%20driving%20autonomy.%20Extensive%20experiments%20show%20that%20TTC%20significantly%20improves%20instant%20error%20rectification%20over%20frozen%203D%20detectors%2C%20even%20under%20limited%20labels%2C%20zero-shot%20settings%2C%20and%20adverse%20conditions.%20We%20hope%20this%20work%20inspires%20future%20research%20on%20post-deployment%20online%20rectification%20systems%20for%20autonomous%20driving.&entry.1838667208=http%3A//arxiv.org/abs/2412.07768v3&entry.124074799=Read"},
{"title": "Diminishing Returns in Self-Supervised Learning", "author": "Oli Bridge and Huey Sun and Botond Branyicskai-Nagy and Charles D'Ornano and Shomit Basu", "abstract": "While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.", "link": "http://arxiv.org/abs/2512.03862v1", "date": "2025-12-03", "relevancy": 2.1767, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5591}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5547}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diminishing%20Returns%20in%20Self-Supervised%20Learning&body=Title%3A%20Diminishing%20Returns%20in%20Self-Supervised%20Learning%0AAuthor%3A%20Oli%20Bridge%20and%20Huey%20Sun%20and%20Botond%20Branyicskai-Nagy%20and%20Charles%20D%27Ornano%20and%20Shomit%20Basu%0AAbstract%3A%20While%20transformer-based%20architectures%20have%20taken%20computer%20vision%20and%20NLP%20by%20storm%2C%20they%20often%20require%20a%20vast%20amount%20of%20parameters%20and%20training%20data%20to%20attain%20strong%20performance.%20In%20this%20work%2C%20we%20experiment%20with%20three%20distinct%20pre-training%2C%20intermediate%20fine-tuning%2C%20and%20downstream%20datasets%20and%20training%20objectives%20to%20explore%20their%20marginal%20benefits%20on%20a%20small%205M-parameter%20vision%20transformer.%20We%20find%20that%20while%20pre-training%20and%20fine-tuning%20always%20help%20our%20model%20but%20have%20diminishing%20returns%2C%20intermediate%20fine-tuning%20can%20actually%20show%20harmful%20impact%20on%20downstream%20performance%2C%20potentially%20due%20to%20dissimilarity%20in%20task%20mechanics.%20Taken%20together%2C%20our%20results%20suggest%20that%20small-scale%20ViTs%20benefit%20most%20from%20targeted%20pre-training%20and%20careful%20data%20selection%2C%20while%20indiscriminate%20stacking%20of%20intermediate%20tasks%20can%20waste%20compute%20and%20even%20degrade%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiminishing%2520Returns%2520in%2520Self-Supervised%2520Learning%26entry.906535625%3DOli%2520Bridge%2520and%2520Huey%2520Sun%2520and%2520Botond%2520Branyicskai-Nagy%2520and%2520Charles%2520D%2527Ornano%2520and%2520Shomit%2520Basu%26entry.1292438233%3DWhile%2520transformer-based%2520architectures%2520have%2520taken%2520computer%2520vision%2520and%2520NLP%2520by%2520storm%252C%2520they%2520often%2520require%2520a%2520vast%2520amount%2520of%2520parameters%2520and%2520training%2520data%2520to%2520attain%2520strong%2520performance.%2520In%2520this%2520work%252C%2520we%2520experiment%2520with%2520three%2520distinct%2520pre-training%252C%2520intermediate%2520fine-tuning%252C%2520and%2520downstream%2520datasets%2520and%2520training%2520objectives%2520to%2520explore%2520their%2520marginal%2520benefits%2520on%2520a%2520small%25205M-parameter%2520vision%2520transformer.%2520We%2520find%2520that%2520while%2520pre-training%2520and%2520fine-tuning%2520always%2520help%2520our%2520model%2520but%2520have%2520diminishing%2520returns%252C%2520intermediate%2520fine-tuning%2520can%2520actually%2520show%2520harmful%2520impact%2520on%2520downstream%2520performance%252C%2520potentially%2520due%2520to%2520dissimilarity%2520in%2520task%2520mechanics.%2520Taken%2520together%252C%2520our%2520results%2520suggest%2520that%2520small-scale%2520ViTs%2520benefit%2520most%2520from%2520targeted%2520pre-training%2520and%2520careful%2520data%2520selection%252C%2520while%2520indiscriminate%2520stacking%2520of%2520intermediate%2520tasks%2520can%2520waste%2520compute%2520and%2520even%2520degrade%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diminishing%20Returns%20in%20Self-Supervised%20Learning&entry.906535625=Oli%20Bridge%20and%20Huey%20Sun%20and%20Botond%20Branyicskai-Nagy%20and%20Charles%20D%27Ornano%20and%20Shomit%20Basu&entry.1292438233=While%20transformer-based%20architectures%20have%20taken%20computer%20vision%20and%20NLP%20by%20storm%2C%20they%20often%20require%20a%20vast%20amount%20of%20parameters%20and%20training%20data%20to%20attain%20strong%20performance.%20In%20this%20work%2C%20we%20experiment%20with%20three%20distinct%20pre-training%2C%20intermediate%20fine-tuning%2C%20and%20downstream%20datasets%20and%20training%20objectives%20to%20explore%20their%20marginal%20benefits%20on%20a%20small%205M-parameter%20vision%20transformer.%20We%20find%20that%20while%20pre-training%20and%20fine-tuning%20always%20help%20our%20model%20but%20have%20diminishing%20returns%2C%20intermediate%20fine-tuning%20can%20actually%20show%20harmful%20impact%20on%20downstream%20performance%2C%20potentially%20due%20to%20dissimilarity%20in%20task%20mechanics.%20Taken%20together%2C%20our%20results%20suggest%20that%20small-scale%20ViTs%20benefit%20most%20from%20targeted%20pre-training%20and%20careful%20data%20selection%2C%20while%20indiscriminate%20stacking%20of%20intermediate%20tasks%20can%20waste%20compute%20and%20even%20degrade%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.03862v1&entry.124074799=Read"},
{"title": "DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation", "author": "Zexin Lin and Hawen Wan and Yebin Zhong and  Xiaoqiang", "abstract": "Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.", "link": "http://arxiv.org/abs/2512.03992v1", "date": "2025-12-03", "relevancy": 2.1761, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIQ-H%3A%20Evaluating%20Hallucination%20Persistence%20in%20VLMs%20Under%20Temporal%20Visual%20Degradation&body=Title%3A%20DIQ-H%3A%20Evaluating%20Hallucination%20Persistence%20in%20VLMs%20Under%20Temporal%20Visual%20Degradation%0AAuthor%3A%20Zexin%20Lin%20and%20Hawen%20Wan%20and%20Yebin%20Zhong%20and%20%20Xiaoqiang%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20deployed%20in%20safety-critical%20applications%20such%20as%20autonomous%20driving%20must%20handle%20continuous%20visual%20streams%20under%20imperfect%20conditions.%20However%2C%20existing%20benchmarks%20focus%20on%20static%2C%20high-quality%20images%20and%20ignore%20temporal%20degradation%20and%20error%20propagation%2C%20which%20are%20critical%20failure%20modes%20where%20transient%20visual%20corruption%20induces%20hallucinations%20that%20persist%20across%20subsequent%20frames.%20We%20introduce%20DIQ-H%2C%20the%20first%20benchmark%20for%20evaluating%20VLM%20robustness%20under%20dynamic%20visual%20degradation%20in%20temporal%20sequences.%20DIQ-H%20applies%20physics-based%20corruptions%20including%20motion%20blur%2C%20sensor%20noise%2C%20and%20compression%20artifacts%2C%20and%20measures%20hallucination%20persistence%2C%20error%20recovery%2C%20and%20temporal%20consistency%20through%20multi-turn%20question-answering%20tasks.%20To%20enable%20scalable%20annotation%2C%20we%20propose%20Uncertainty-Guided%20Iterative%20Refinement%20%28UIR%29%2C%20which%20generates%20reliable%20pseudo-ground-truth%20using%20lightweight%20VLMs%20with%20uncertainty%20filtering%2C%20achieving%20a%2015.3%20percent%20accuracy%20improvement.%20Experiments%20on%2016%20state-of-the-art%20VLMs%20reveal%20substantial%20robustness%20gaps%3A%20even%20advanced%20models%20such%20as%20GPT-4o%20achieve%20only%20a%2078.5%20percent%20recovery%20rate%2C%20while%20open-source%20models%20struggle%20with%20temporal%20consistency%20at%20less%20than%2060%20percent.%20DIQ-H%20provides%20a%20comprehensive%20platform%20for%20evaluating%20VLM%20reliability%20in%20real-world%20deployments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIQ-H%253A%2520Evaluating%2520Hallucination%2520Persistence%2520in%2520VLMs%2520Under%2520Temporal%2520Visual%2520Degradation%26entry.906535625%3DZexin%2520Lin%2520and%2520Hawen%2520Wan%2520and%2520Yebin%2520Zhong%2520and%2520%2520Xiaoqiang%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520deployed%2520in%2520safety-critical%2520applications%2520such%2520as%2520autonomous%2520driving%2520must%2520handle%2520continuous%2520visual%2520streams%2520under%2520imperfect%2520conditions.%2520However%252C%2520existing%2520benchmarks%2520focus%2520on%2520static%252C%2520high-quality%2520images%2520and%2520ignore%2520temporal%2520degradation%2520and%2520error%2520propagation%252C%2520which%2520are%2520critical%2520failure%2520modes%2520where%2520transient%2520visual%2520corruption%2520induces%2520hallucinations%2520that%2520persist%2520across%2520subsequent%2520frames.%2520We%2520introduce%2520DIQ-H%252C%2520the%2520first%2520benchmark%2520for%2520evaluating%2520VLM%2520robustness%2520under%2520dynamic%2520visual%2520degradation%2520in%2520temporal%2520sequences.%2520DIQ-H%2520applies%2520physics-based%2520corruptions%2520including%2520motion%2520blur%252C%2520sensor%2520noise%252C%2520and%2520compression%2520artifacts%252C%2520and%2520measures%2520hallucination%2520persistence%252C%2520error%2520recovery%252C%2520and%2520temporal%2520consistency%2520through%2520multi-turn%2520question-answering%2520tasks.%2520To%2520enable%2520scalable%2520annotation%252C%2520we%2520propose%2520Uncertainty-Guided%2520Iterative%2520Refinement%2520%2528UIR%2529%252C%2520which%2520generates%2520reliable%2520pseudo-ground-truth%2520using%2520lightweight%2520VLMs%2520with%2520uncertainty%2520filtering%252C%2520achieving%2520a%252015.3%2520percent%2520accuracy%2520improvement.%2520Experiments%2520on%252016%2520state-of-the-art%2520VLMs%2520reveal%2520substantial%2520robustness%2520gaps%253A%2520even%2520advanced%2520models%2520such%2520as%2520GPT-4o%2520achieve%2520only%2520a%252078.5%2520percent%2520recovery%2520rate%252C%2520while%2520open-source%2520models%2520struggle%2520with%2520temporal%2520consistency%2520at%2520less%2520than%252060%2520percent.%2520DIQ-H%2520provides%2520a%2520comprehensive%2520platform%2520for%2520evaluating%2520VLM%2520reliability%2520in%2520real-world%2520deployments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIQ-H%3A%20Evaluating%20Hallucination%20Persistence%20in%20VLMs%20Under%20Temporal%20Visual%20Degradation&entry.906535625=Zexin%20Lin%20and%20Hawen%20Wan%20and%20Yebin%20Zhong%20and%20%20Xiaoqiang&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20deployed%20in%20safety-critical%20applications%20such%20as%20autonomous%20driving%20must%20handle%20continuous%20visual%20streams%20under%20imperfect%20conditions.%20However%2C%20existing%20benchmarks%20focus%20on%20static%2C%20high-quality%20images%20and%20ignore%20temporal%20degradation%20and%20error%20propagation%2C%20which%20are%20critical%20failure%20modes%20where%20transient%20visual%20corruption%20induces%20hallucinations%20that%20persist%20across%20subsequent%20frames.%20We%20introduce%20DIQ-H%2C%20the%20first%20benchmark%20for%20evaluating%20VLM%20robustness%20under%20dynamic%20visual%20degradation%20in%20temporal%20sequences.%20DIQ-H%20applies%20physics-based%20corruptions%20including%20motion%20blur%2C%20sensor%20noise%2C%20and%20compression%20artifacts%2C%20and%20measures%20hallucination%20persistence%2C%20error%20recovery%2C%20and%20temporal%20consistency%20through%20multi-turn%20question-answering%20tasks.%20To%20enable%20scalable%20annotation%2C%20we%20propose%20Uncertainty-Guided%20Iterative%20Refinement%20%28UIR%29%2C%20which%20generates%20reliable%20pseudo-ground-truth%20using%20lightweight%20VLMs%20with%20uncertainty%20filtering%2C%20achieving%20a%2015.3%20percent%20accuracy%20improvement.%20Experiments%20on%2016%20state-of-the-art%20VLMs%20reveal%20substantial%20robustness%20gaps%3A%20even%20advanced%20models%20such%20as%20GPT-4o%20achieve%20only%20a%2078.5%20percent%20recovery%20rate%2C%20while%20open-source%20models%20struggle%20with%20temporal%20consistency%20at%20less%20than%2060%20percent.%20DIQ-H%20provides%20a%20comprehensive%20platform%20for%20evaluating%20VLM%20reliability%20in%20real-world%20deployments.&entry.1838667208=http%3A//arxiv.org/abs/2512.03992v1&entry.124074799=Read"},
{"title": "PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation", "author": "Hania Ghouse and Maryam Alsharqi and Farhad R. Nezami and Muzammil Behzad", "abstract": "Cardiac image analysis remains fragmented across tasks: anatomical segmentation, disease classification, and grounded clinical report generation are typically handled by separate networks trained under different data regimes. No existing framework unifies these objectives within a single architecture while retaining generalization across imaging modalities and datasets. We introduce PULSE, a multi-task vision-language framework built on self-supervised representations and optimized through a composite supervision strategy that balances region overlap learning, pixel wise classification fidelity, and boundary aware IoU refinement. A multi-scale token reconstruction decoder enables anatomical segmentation, while shared global representations support disease classification and clinically grounded text output allowing the model to transition from pixels to structures and finally clinical reasoning within one architecture. Unlike prior task-specific pipelines, PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision. This moves the field closer to a scalable, foundation style cardiac analysis framework.", "link": "http://arxiv.org/abs/2512.03848v1", "date": "2025-12-03", "relevancy": 2.1622, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.567}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5255}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PULSE%3A%20A%20Unified%20Multi-Task%20Architecture%20for%20Cardiac%20Segmentation%2C%20Diagnosis%2C%20and%20Few-Shot%20Cross-Modality%20Clinical%20Adaptation&body=Title%3A%20PULSE%3A%20A%20Unified%20Multi-Task%20Architecture%20for%20Cardiac%20Segmentation%2C%20Diagnosis%2C%20and%20Few-Shot%20Cross-Modality%20Clinical%20Adaptation%0AAuthor%3A%20Hania%20Ghouse%20and%20Maryam%20Alsharqi%20and%20Farhad%20R.%20Nezami%20and%20Muzammil%20Behzad%0AAbstract%3A%20Cardiac%20image%20analysis%20remains%20fragmented%20across%20tasks%3A%20anatomical%20segmentation%2C%20disease%20classification%2C%20and%20grounded%20clinical%20report%20generation%20are%20typically%20handled%20by%20separate%20networks%20trained%20under%20different%20data%20regimes.%20No%20existing%20framework%20unifies%20these%20objectives%20within%20a%20single%20architecture%20while%20retaining%20generalization%20across%20imaging%20modalities%20and%20datasets.%20We%20introduce%20PULSE%2C%20a%20multi-task%20vision-language%20framework%20built%20on%20self-supervised%20representations%20and%20optimized%20through%20a%20composite%20supervision%20strategy%20that%20balances%20region%20overlap%20learning%2C%20pixel%20wise%20classification%20fidelity%2C%20and%20boundary%20aware%20IoU%20refinement.%20A%20multi-scale%20token%20reconstruction%20decoder%20enables%20anatomical%20segmentation%2C%20while%20shared%20global%20representations%20support%20disease%20classification%20and%20clinically%20grounded%20text%20output%20allowing%20the%20model%20to%20transition%20from%20pixels%20to%20structures%20and%20finally%20clinical%20reasoning%20within%20one%20architecture.%20Unlike%20prior%20task-specific%20pipelines%2C%20PULSE%20learns%20task-invariant%20cardiac%20priors%2C%20generalizes%20robustly%20across%20datasets%2C%20and%20can%20be%20adapted%20to%20new%20imaging%20modalities%20with%20minimal%20supervision.%20This%20moves%20the%20field%20closer%20to%20a%20scalable%2C%20foundation%20style%20cardiac%20analysis%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPULSE%253A%2520A%2520Unified%2520Multi-Task%2520Architecture%2520for%2520Cardiac%2520Segmentation%252C%2520Diagnosis%252C%2520and%2520Few-Shot%2520Cross-Modality%2520Clinical%2520Adaptation%26entry.906535625%3DHania%2520Ghouse%2520and%2520Maryam%2520Alsharqi%2520and%2520Farhad%2520R.%2520Nezami%2520and%2520Muzammil%2520Behzad%26entry.1292438233%3DCardiac%2520image%2520analysis%2520remains%2520fragmented%2520across%2520tasks%253A%2520anatomical%2520segmentation%252C%2520disease%2520classification%252C%2520and%2520grounded%2520clinical%2520report%2520generation%2520are%2520typically%2520handled%2520by%2520separate%2520networks%2520trained%2520under%2520different%2520data%2520regimes.%2520No%2520existing%2520framework%2520unifies%2520these%2520objectives%2520within%2520a%2520single%2520architecture%2520while%2520retaining%2520generalization%2520across%2520imaging%2520modalities%2520and%2520datasets.%2520We%2520introduce%2520PULSE%252C%2520a%2520multi-task%2520vision-language%2520framework%2520built%2520on%2520self-supervised%2520representations%2520and%2520optimized%2520through%2520a%2520composite%2520supervision%2520strategy%2520that%2520balances%2520region%2520overlap%2520learning%252C%2520pixel%2520wise%2520classification%2520fidelity%252C%2520and%2520boundary%2520aware%2520IoU%2520refinement.%2520A%2520multi-scale%2520token%2520reconstruction%2520decoder%2520enables%2520anatomical%2520segmentation%252C%2520while%2520shared%2520global%2520representations%2520support%2520disease%2520classification%2520and%2520clinically%2520grounded%2520text%2520output%2520allowing%2520the%2520model%2520to%2520transition%2520from%2520pixels%2520to%2520structures%2520and%2520finally%2520clinical%2520reasoning%2520within%2520one%2520architecture.%2520Unlike%2520prior%2520task-specific%2520pipelines%252C%2520PULSE%2520learns%2520task-invariant%2520cardiac%2520priors%252C%2520generalizes%2520robustly%2520across%2520datasets%252C%2520and%2520can%2520be%2520adapted%2520to%2520new%2520imaging%2520modalities%2520with%2520minimal%2520supervision.%2520This%2520moves%2520the%2520field%2520closer%2520to%2520a%2520scalable%252C%2520foundation%2520style%2520cardiac%2520analysis%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PULSE%3A%20A%20Unified%20Multi-Task%20Architecture%20for%20Cardiac%20Segmentation%2C%20Diagnosis%2C%20and%20Few-Shot%20Cross-Modality%20Clinical%20Adaptation&entry.906535625=Hania%20Ghouse%20and%20Maryam%20Alsharqi%20and%20Farhad%20R.%20Nezami%20and%20Muzammil%20Behzad&entry.1292438233=Cardiac%20image%20analysis%20remains%20fragmented%20across%20tasks%3A%20anatomical%20segmentation%2C%20disease%20classification%2C%20and%20grounded%20clinical%20report%20generation%20are%20typically%20handled%20by%20separate%20networks%20trained%20under%20different%20data%20regimes.%20No%20existing%20framework%20unifies%20these%20objectives%20within%20a%20single%20architecture%20while%20retaining%20generalization%20across%20imaging%20modalities%20and%20datasets.%20We%20introduce%20PULSE%2C%20a%20multi-task%20vision-language%20framework%20built%20on%20self-supervised%20representations%20and%20optimized%20through%20a%20composite%20supervision%20strategy%20that%20balances%20region%20overlap%20learning%2C%20pixel%20wise%20classification%20fidelity%2C%20and%20boundary%20aware%20IoU%20refinement.%20A%20multi-scale%20token%20reconstruction%20decoder%20enables%20anatomical%20segmentation%2C%20while%20shared%20global%20representations%20support%20disease%20classification%20and%20clinically%20grounded%20text%20output%20allowing%20the%20model%20to%20transition%20from%20pixels%20to%20structures%20and%20finally%20clinical%20reasoning%20within%20one%20architecture.%20Unlike%20prior%20task-specific%20pipelines%2C%20PULSE%20learns%20task-invariant%20cardiac%20priors%2C%20generalizes%20robustly%20across%20datasets%2C%20and%20can%20be%20adapted%20to%20new%20imaging%20modalities%20with%20minimal%20supervision.%20This%20moves%20the%20field%20closer%20to%20a%20scalable%2C%20foundation%20style%20cardiac%20analysis%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2512.03848v1&entry.124074799=Read"},
{"title": "MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms", "author": "Jiahao Zhang and Xiao Zhao and Guangyu Gao", "abstract": "Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network's ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet's superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.", "link": "http://arxiv.org/abs/2512.03640v1", "date": "2025-12-03", "relevancy": 2.1531, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5599}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5357}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MKSNet%3A%20Advanced%20Small%20Object%20Detection%20in%20Remote%20Sensing%20Imagery%20with%20Multi-Kernel%20and%20Dual%20Attention%20Mechanisms&body=Title%3A%20MKSNet%3A%20Advanced%20Small%20Object%20Detection%20in%20Remote%20Sensing%20Imagery%20with%20Multi-Kernel%20and%20Dual%20Attention%20Mechanisms%0AAuthor%3A%20Jiahao%20Zhang%20and%20Xiao%20Zhao%20and%20Guangyu%20Gao%0AAbstract%3A%20Deep%20convolutional%20neural%20networks%20%28DCNNs%29%20have%20substantially%20advanced%20object%20detection%20capabilities%2C%20particularly%20in%20remote%20sensing%20imagery.%20However%2C%20challenges%20persist%2C%20especially%20in%20detecting%20small%20objects%20where%20the%20high%20resolution%20of%20these%20images%20and%20the%20small%20size%20of%20target%20objects%20often%20result%20in%20a%20loss%20of%20critical%20information%20in%20the%20deeper%20layers%20of%20conventional%20CNNs.%20Additionally%2C%20the%20extensive%20spatial%20redundancy%20and%20intricate%20background%20details%20typical%20in%20remote-sensing%20images%20tend%20to%20obscure%20these%20small%20targets.%20To%20address%20these%20challenges%2C%20we%20introduce%20Multi-Kernel%20Selection%20Network%20%28MKSNet%29%2C%20a%20novel%20network%20architecture%20featuring%20a%20novel%20Multi-Kernel%20Selection%20mechanism.%20The%20MKS%20mechanism%20utilizes%20large%20convolutional%20kernels%20to%20effectively%20capture%20an%20extensive%20range%20of%20contextual%20information.%20This%20innovative%20design%20allows%20for%20adaptive%20kernel%20size%20selection%2C%20significantly%20enhancing%20the%20network%27s%20ability%20to%20dynamically%20process%20and%20emphasize%20crucial%20spatial%20details%20for%20small%20object%20detection.%20Furthermore%2C%20MKSNet%20also%20incorporates%20a%20dual%20attention%20mechanism%2C%20merging%20spatial%20and%20channel%20attention%20modules.%20The%20spatial%20attention%20module%20adaptively%20fine-tunes%20the%20spatial%20weights%20of%20feature%20maps%2C%20focusing%20more%20intensively%20on%20relevant%20regions%20while%20mitigating%20background%20noise.%20Simultaneously%2C%20the%20channel%20attention%20module%20optimizes%20channel%20information%20selection%2C%20improving%20feature%20representation%20and%20detection%20accuracy.%20Empirical%20evaluations%20on%20the%20DOTA-v1.0%20and%20HRSC2016%20benchmark%20demonstrate%20that%20MKSNet%20substantially%20surpasses%20existing%20state-of-the-art%20models%20in%20detecting%20small%20objects%20in%20remote%20sensing%20images.%20These%20results%20highlight%20MKSNet%27s%20superior%20ability%20to%20manage%20the%20complexities%20associated%20with%20multi-scale%20and%20high-resolution%20image%20data%2C%20confirming%20its%20effectiveness%20and%20innovation%20in%20remote%20sensing%20object%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMKSNet%253A%2520Advanced%2520Small%2520Object%2520Detection%2520in%2520Remote%2520Sensing%2520Imagery%2520with%2520Multi-Kernel%2520and%2520Dual%2520Attention%2520Mechanisms%26entry.906535625%3DJiahao%2520Zhang%2520and%2520Xiao%2520Zhao%2520and%2520Guangyu%2520Gao%26entry.1292438233%3DDeep%2520convolutional%2520neural%2520networks%2520%2528DCNNs%2529%2520have%2520substantially%2520advanced%2520object%2520detection%2520capabilities%252C%2520particularly%2520in%2520remote%2520sensing%2520imagery.%2520However%252C%2520challenges%2520persist%252C%2520especially%2520in%2520detecting%2520small%2520objects%2520where%2520the%2520high%2520resolution%2520of%2520these%2520images%2520and%2520the%2520small%2520size%2520of%2520target%2520objects%2520often%2520result%2520in%2520a%2520loss%2520of%2520critical%2520information%2520in%2520the%2520deeper%2520layers%2520of%2520conventional%2520CNNs.%2520Additionally%252C%2520the%2520extensive%2520spatial%2520redundancy%2520and%2520intricate%2520background%2520details%2520typical%2520in%2520remote-sensing%2520images%2520tend%2520to%2520obscure%2520these%2520small%2520targets.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Multi-Kernel%2520Selection%2520Network%2520%2528MKSNet%2529%252C%2520a%2520novel%2520network%2520architecture%2520featuring%2520a%2520novel%2520Multi-Kernel%2520Selection%2520mechanism.%2520The%2520MKS%2520mechanism%2520utilizes%2520large%2520convolutional%2520kernels%2520to%2520effectively%2520capture%2520an%2520extensive%2520range%2520of%2520contextual%2520information.%2520This%2520innovative%2520design%2520allows%2520for%2520adaptive%2520kernel%2520size%2520selection%252C%2520significantly%2520enhancing%2520the%2520network%2527s%2520ability%2520to%2520dynamically%2520process%2520and%2520emphasize%2520crucial%2520spatial%2520details%2520for%2520small%2520object%2520detection.%2520Furthermore%252C%2520MKSNet%2520also%2520incorporates%2520a%2520dual%2520attention%2520mechanism%252C%2520merging%2520spatial%2520and%2520channel%2520attention%2520modules.%2520The%2520spatial%2520attention%2520module%2520adaptively%2520fine-tunes%2520the%2520spatial%2520weights%2520of%2520feature%2520maps%252C%2520focusing%2520more%2520intensively%2520on%2520relevant%2520regions%2520while%2520mitigating%2520background%2520noise.%2520Simultaneously%252C%2520the%2520channel%2520attention%2520module%2520optimizes%2520channel%2520information%2520selection%252C%2520improving%2520feature%2520representation%2520and%2520detection%2520accuracy.%2520Empirical%2520evaluations%2520on%2520the%2520DOTA-v1.0%2520and%2520HRSC2016%2520benchmark%2520demonstrate%2520that%2520MKSNet%2520substantially%2520surpasses%2520existing%2520state-of-the-art%2520models%2520in%2520detecting%2520small%2520objects%2520in%2520remote%2520sensing%2520images.%2520These%2520results%2520highlight%2520MKSNet%2527s%2520superior%2520ability%2520to%2520manage%2520the%2520complexities%2520associated%2520with%2520multi-scale%2520and%2520high-resolution%2520image%2520data%252C%2520confirming%2520its%2520effectiveness%2520and%2520innovation%2520in%2520remote%2520sensing%2520object%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MKSNet%3A%20Advanced%20Small%20Object%20Detection%20in%20Remote%20Sensing%20Imagery%20with%20Multi-Kernel%20and%20Dual%20Attention%20Mechanisms&entry.906535625=Jiahao%20Zhang%20and%20Xiao%20Zhao%20and%20Guangyu%20Gao&entry.1292438233=Deep%20convolutional%20neural%20networks%20%28DCNNs%29%20have%20substantially%20advanced%20object%20detection%20capabilities%2C%20particularly%20in%20remote%20sensing%20imagery.%20However%2C%20challenges%20persist%2C%20especially%20in%20detecting%20small%20objects%20where%20the%20high%20resolution%20of%20these%20images%20and%20the%20small%20size%20of%20target%20objects%20often%20result%20in%20a%20loss%20of%20critical%20information%20in%20the%20deeper%20layers%20of%20conventional%20CNNs.%20Additionally%2C%20the%20extensive%20spatial%20redundancy%20and%20intricate%20background%20details%20typical%20in%20remote-sensing%20images%20tend%20to%20obscure%20these%20small%20targets.%20To%20address%20these%20challenges%2C%20we%20introduce%20Multi-Kernel%20Selection%20Network%20%28MKSNet%29%2C%20a%20novel%20network%20architecture%20featuring%20a%20novel%20Multi-Kernel%20Selection%20mechanism.%20The%20MKS%20mechanism%20utilizes%20large%20convolutional%20kernels%20to%20effectively%20capture%20an%20extensive%20range%20of%20contextual%20information.%20This%20innovative%20design%20allows%20for%20adaptive%20kernel%20size%20selection%2C%20significantly%20enhancing%20the%20network%27s%20ability%20to%20dynamically%20process%20and%20emphasize%20crucial%20spatial%20details%20for%20small%20object%20detection.%20Furthermore%2C%20MKSNet%20also%20incorporates%20a%20dual%20attention%20mechanism%2C%20merging%20spatial%20and%20channel%20attention%20modules.%20The%20spatial%20attention%20module%20adaptively%20fine-tunes%20the%20spatial%20weights%20of%20feature%20maps%2C%20focusing%20more%20intensively%20on%20relevant%20regions%20while%20mitigating%20background%20noise.%20Simultaneously%2C%20the%20channel%20attention%20module%20optimizes%20channel%20information%20selection%2C%20improving%20feature%20representation%20and%20detection%20accuracy.%20Empirical%20evaluations%20on%20the%20DOTA-v1.0%20and%20HRSC2016%20benchmark%20demonstrate%20that%20MKSNet%20substantially%20surpasses%20existing%20state-of-the-art%20models%20in%20detecting%20small%20objects%20in%20remote%20sensing%20images.%20These%20results%20highlight%20MKSNet%27s%20superior%20ability%20to%20manage%20the%20complexities%20associated%20with%20multi-scale%20and%20high-resolution%20image%20data%2C%20confirming%20its%20effectiveness%20and%20innovation%20in%20remote%20sensing%20object%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2512.03640v1&entry.124074799=Read"},
{"title": "You Point, I Learn: Online Adaptation of Interactive Segmentation Models for Handling Distribution Shifts in Medical Imaging", "author": "Wentian Xu and Ziyun Liang and Harry Anthony and Yasin Ibrahim and Felix Cohen and Guang Yang and Konstantinos Kamnitsas", "abstract": "Interactive segmentation uses real-time user inputs, such as mouse clicks, to iteratively refine model predictions. Although not originally designed to address distribution shifts, this paradigm naturally lends itself to such challenges. In medical imaging, where distribution shifts are common, interactive methods can use user inputs to guide models towards improved predictions. Moreover, once a model is deployed, user corrections can be used to adapt the network parameters to the new data distribution, mitigating distribution shift. Based on these insights, we aim to develop a practical, effective method for improving the adaptive capabilities of interactive segmentation models to new data distributions in medical imaging. Firstly, we found that strengthening the model's responsiveness to clicks is important for the initial training process. Moreover, we show that by treating the post-interaction user-refined model output as pseudo-ground-truth, we can design a lean, practical online adaptation method that enables a model to learn effectively across sequential test images. The framework includes two components: (i) a Post-Interaction adaptation process, updating the model after the user has completed interactive refinement of an image, and (ii) a Mid-Interaction adaptation process, updating incrementally after each click. Both processes include a Click-Centered Gaussian loss that strengthens the model's reaction to clicks and enhances focus on user-guided, clinically relevant regions. Experiments on 5 fundus and 4 brain-MRI databases show that our approach consistently outperforms existing methods under diverse distribution shifts, including unseen imaging modalities and pathologies. Code and pretrained models will be released upon publication.", "link": "http://arxiv.org/abs/2503.06717v2", "date": "2025-12-03", "relevancy": 2.1436, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5399}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.538}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20Point%2C%20I%20Learn%3A%20Online%20Adaptation%20of%20Interactive%20Segmentation%20Models%20for%20Handling%20Distribution%20Shifts%20in%20Medical%20Imaging&body=Title%3A%20You%20Point%2C%20I%20Learn%3A%20Online%20Adaptation%20of%20Interactive%20Segmentation%20Models%20for%20Handling%20Distribution%20Shifts%20in%20Medical%20Imaging%0AAuthor%3A%20Wentian%20Xu%20and%20Ziyun%20Liang%20and%20Harry%20Anthony%20and%20Yasin%20Ibrahim%20and%20Felix%20Cohen%20and%20Guang%20Yang%20and%20Konstantinos%20Kamnitsas%0AAbstract%3A%20Interactive%20segmentation%20uses%20real-time%20user%20inputs%2C%20such%20as%20mouse%20clicks%2C%20to%20iteratively%20refine%20model%20predictions.%20Although%20not%20originally%20designed%20to%20address%20distribution%20shifts%2C%20this%20paradigm%20naturally%20lends%20itself%20to%20such%20challenges.%20In%20medical%20imaging%2C%20where%20distribution%20shifts%20are%20common%2C%20interactive%20methods%20can%20use%20user%20inputs%20to%20guide%20models%20towards%20improved%20predictions.%20Moreover%2C%20once%20a%20model%20is%20deployed%2C%20user%20corrections%20can%20be%20used%20to%20adapt%20the%20network%20parameters%20to%20the%20new%20data%20distribution%2C%20mitigating%20distribution%20shift.%20Based%20on%20these%20insights%2C%20we%20aim%20to%20develop%20a%20practical%2C%20effective%20method%20for%20improving%20the%20adaptive%20capabilities%20of%20interactive%20segmentation%20models%20to%20new%20data%20distributions%20in%20medical%20imaging.%20Firstly%2C%20we%20found%20that%20strengthening%20the%20model%27s%20responsiveness%20to%20clicks%20is%20important%20for%20the%20initial%20training%20process.%20Moreover%2C%20we%20show%20that%20by%20treating%20the%20post-interaction%20user-refined%20model%20output%20as%20pseudo-ground-truth%2C%20we%20can%20design%20a%20lean%2C%20practical%20online%20adaptation%20method%20that%20enables%20a%20model%20to%20learn%20effectively%20across%20sequential%20test%20images.%20The%20framework%20includes%20two%20components%3A%20%28i%29%20a%20Post-Interaction%20adaptation%20process%2C%20updating%20the%20model%20after%20the%20user%20has%20completed%20interactive%20refinement%20of%20an%20image%2C%20and%20%28ii%29%20a%20Mid-Interaction%20adaptation%20process%2C%20updating%20incrementally%20after%20each%20click.%20Both%20processes%20include%20a%20Click-Centered%20Gaussian%20loss%20that%20strengthens%20the%20model%27s%20reaction%20to%20clicks%20and%20enhances%20focus%20on%20user-guided%2C%20clinically%20relevant%20regions.%20Experiments%20on%205%20fundus%20and%204%20brain-MRI%20databases%20show%20that%20our%20approach%20consistently%20outperforms%20existing%20methods%20under%20diverse%20distribution%20shifts%2C%20including%20unseen%20imaging%20modalities%20and%20pathologies.%20Code%20and%20pretrained%20models%20will%20be%20released%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2503.06717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520Point%252C%2520I%2520Learn%253A%2520Online%2520Adaptation%2520of%2520Interactive%2520Segmentation%2520Models%2520for%2520Handling%2520Distribution%2520Shifts%2520in%2520Medical%2520Imaging%26entry.906535625%3DWentian%2520Xu%2520and%2520Ziyun%2520Liang%2520and%2520Harry%2520Anthony%2520and%2520Yasin%2520Ibrahim%2520and%2520Felix%2520Cohen%2520and%2520Guang%2520Yang%2520and%2520Konstantinos%2520Kamnitsas%26entry.1292438233%3DInteractive%2520segmentation%2520uses%2520real-time%2520user%2520inputs%252C%2520such%2520as%2520mouse%2520clicks%252C%2520to%2520iteratively%2520refine%2520model%2520predictions.%2520Although%2520not%2520originally%2520designed%2520to%2520address%2520distribution%2520shifts%252C%2520this%2520paradigm%2520naturally%2520lends%2520itself%2520to%2520such%2520challenges.%2520In%2520medical%2520imaging%252C%2520where%2520distribution%2520shifts%2520are%2520common%252C%2520interactive%2520methods%2520can%2520use%2520user%2520inputs%2520to%2520guide%2520models%2520towards%2520improved%2520predictions.%2520Moreover%252C%2520once%2520a%2520model%2520is%2520deployed%252C%2520user%2520corrections%2520can%2520be%2520used%2520to%2520adapt%2520the%2520network%2520parameters%2520to%2520the%2520new%2520data%2520distribution%252C%2520mitigating%2520distribution%2520shift.%2520Based%2520on%2520these%2520insights%252C%2520we%2520aim%2520to%2520develop%2520a%2520practical%252C%2520effective%2520method%2520for%2520improving%2520the%2520adaptive%2520capabilities%2520of%2520interactive%2520segmentation%2520models%2520to%2520new%2520data%2520distributions%2520in%2520medical%2520imaging.%2520Firstly%252C%2520we%2520found%2520that%2520strengthening%2520the%2520model%2527s%2520responsiveness%2520to%2520clicks%2520is%2520important%2520for%2520the%2520initial%2520training%2520process.%2520Moreover%252C%2520we%2520show%2520that%2520by%2520treating%2520the%2520post-interaction%2520user-refined%2520model%2520output%2520as%2520pseudo-ground-truth%252C%2520we%2520can%2520design%2520a%2520lean%252C%2520practical%2520online%2520adaptation%2520method%2520that%2520enables%2520a%2520model%2520to%2520learn%2520effectively%2520across%2520sequential%2520test%2520images.%2520The%2520framework%2520includes%2520two%2520components%253A%2520%2528i%2529%2520a%2520Post-Interaction%2520adaptation%2520process%252C%2520updating%2520the%2520model%2520after%2520the%2520user%2520has%2520completed%2520interactive%2520refinement%2520of%2520an%2520image%252C%2520and%2520%2528ii%2529%2520a%2520Mid-Interaction%2520adaptation%2520process%252C%2520updating%2520incrementally%2520after%2520each%2520click.%2520Both%2520processes%2520include%2520a%2520Click-Centered%2520Gaussian%2520loss%2520that%2520strengthens%2520the%2520model%2527s%2520reaction%2520to%2520clicks%2520and%2520enhances%2520focus%2520on%2520user-guided%252C%2520clinically%2520relevant%2520regions.%2520Experiments%2520on%25205%2520fundus%2520and%25204%2520brain-MRI%2520databases%2520show%2520that%2520our%2520approach%2520consistently%2520outperforms%2520existing%2520methods%2520under%2520diverse%2520distribution%2520shifts%252C%2520including%2520unseen%2520imaging%2520modalities%2520and%2520pathologies.%2520Code%2520and%2520pretrained%2520models%2520will%2520be%2520released%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20Point%2C%20I%20Learn%3A%20Online%20Adaptation%20of%20Interactive%20Segmentation%20Models%20for%20Handling%20Distribution%20Shifts%20in%20Medical%20Imaging&entry.906535625=Wentian%20Xu%20and%20Ziyun%20Liang%20and%20Harry%20Anthony%20and%20Yasin%20Ibrahim%20and%20Felix%20Cohen%20and%20Guang%20Yang%20and%20Konstantinos%20Kamnitsas&entry.1292438233=Interactive%20segmentation%20uses%20real-time%20user%20inputs%2C%20such%20as%20mouse%20clicks%2C%20to%20iteratively%20refine%20model%20predictions.%20Although%20not%20originally%20designed%20to%20address%20distribution%20shifts%2C%20this%20paradigm%20naturally%20lends%20itself%20to%20such%20challenges.%20In%20medical%20imaging%2C%20where%20distribution%20shifts%20are%20common%2C%20interactive%20methods%20can%20use%20user%20inputs%20to%20guide%20models%20towards%20improved%20predictions.%20Moreover%2C%20once%20a%20model%20is%20deployed%2C%20user%20corrections%20can%20be%20used%20to%20adapt%20the%20network%20parameters%20to%20the%20new%20data%20distribution%2C%20mitigating%20distribution%20shift.%20Based%20on%20these%20insights%2C%20we%20aim%20to%20develop%20a%20practical%2C%20effective%20method%20for%20improving%20the%20adaptive%20capabilities%20of%20interactive%20segmentation%20models%20to%20new%20data%20distributions%20in%20medical%20imaging.%20Firstly%2C%20we%20found%20that%20strengthening%20the%20model%27s%20responsiveness%20to%20clicks%20is%20important%20for%20the%20initial%20training%20process.%20Moreover%2C%20we%20show%20that%20by%20treating%20the%20post-interaction%20user-refined%20model%20output%20as%20pseudo-ground-truth%2C%20we%20can%20design%20a%20lean%2C%20practical%20online%20adaptation%20method%20that%20enables%20a%20model%20to%20learn%20effectively%20across%20sequential%20test%20images.%20The%20framework%20includes%20two%20components%3A%20%28i%29%20a%20Post-Interaction%20adaptation%20process%2C%20updating%20the%20model%20after%20the%20user%20has%20completed%20interactive%20refinement%20of%20an%20image%2C%20and%20%28ii%29%20a%20Mid-Interaction%20adaptation%20process%2C%20updating%20incrementally%20after%20each%20click.%20Both%20processes%20include%20a%20Click-Centered%20Gaussian%20loss%20that%20strengthens%20the%20model%27s%20reaction%20to%20clicks%20and%20enhances%20focus%20on%20user-guided%2C%20clinically%20relevant%20regions.%20Experiments%20on%205%20fundus%20and%204%20brain-MRI%20databases%20show%20that%20our%20approach%20consistently%20outperforms%20existing%20methods%20under%20diverse%20distribution%20shifts%2C%20including%20unseen%20imaging%20modalities%20and%20pathologies.%20Code%20and%20pretrained%20models%20will%20be%20released%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2503.06717v2&entry.124074799=Read"},
{"title": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "author": "Kaichen Zhang and Keming Wu and Zuhao Yang and Kairui Hu and Bin Wang and Ziwei Liu and Xingxuan Li and Lidong Bing", "abstract": "Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.", "link": "http://arxiv.org/abs/2511.16334v3", "date": "2025-12-03", "relevancy": 2.1428, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5537}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenMMReasoner%3A%20Pushing%20the%20Frontiers%20for%20Multimodal%20Reasoning%20with%20an%20Open%20and%20General%20Recipe&body=Title%3A%20OpenMMReasoner%3A%20Pushing%20the%20Frontiers%20for%20Multimodal%20Reasoning%20with%20an%20Open%20and%20General%20Recipe%0AAuthor%3A%20Kaichen%20Zhang%20and%20Keming%20Wu%20and%20Zuhao%20Yang%20and%20Kairui%20Hu%20and%20Bin%20Wang%20and%20Ziwei%20Liu%20and%20Xingxuan%20Li%20and%20Lidong%20Bing%0AAbstract%3A%20Recent%20advancements%20in%20large%20reasoning%20models%20have%20fueled%20growing%20interest%20in%20extending%20such%20capabilities%20to%20multimodal%20domains.%20However%2C%20despite%20notable%20progress%20in%20visual%20reasoning%2C%20the%20lack%20of%20transparent%20and%20reproducible%20data%20curation%20and%20training%20strategies%20remains%20a%20major%20barrier%20to%20scalable%20research.%20In%20this%20work%2C%20we%20introduce%20OpenMMReasoner%2C%20a%20fully%20transparent%20two-stage%20recipe%20for%20multimodal%20reasoning%20spanning%20supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29.%20In%20the%20SFT%20stage%2C%20we%20construct%20an%20874K-sample%20cold-start%20dataset%20with%20rigorous%20step-by-step%20validation%2C%20providing%20a%20strong%20foundation%20for%20reasoning%20capabilities.%20The%20subsequent%20RL%20stage%20leverages%20a%2074K-sample%20dataset%20across%20diverse%20domains%20to%20further%20sharpen%20and%20stabilize%20these%20abilities%2C%20resulting%20in%20a%20more%20robust%20and%20efficient%20learning%20process.%20Extensive%20evaluations%20demonstrate%20that%20our%20training%20recipe%20not%20only%20surpasses%20strong%20baselines%20but%20also%20highlights%20the%20critical%20role%20of%20data%20quality%20and%20training%20design%20in%20shaping%20multimodal%20reasoning%20performance.%20Notably%2C%20our%20method%20achieves%20a%2011.6%25%20improvement%20over%20the%20Qwen2.5-VL-7B-Instruct%20baseline%20across%20nine%20multimodal%20reasoning%20benchmarks%2C%20establishing%20a%20solid%20empirical%20foundation%20for%20future%20large-scale%20multimodal%20reasoning%20research.%20We%20open-sourced%20all%20our%20codes%2C%20pipeline%2C%20and%20data%20at%20https%3A//github.com/EvolvingLMMs-Lab/OpenMMReasoner.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16334v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenMMReasoner%253A%2520Pushing%2520the%2520Frontiers%2520for%2520Multimodal%2520Reasoning%2520with%2520an%2520Open%2520and%2520General%2520Recipe%26entry.906535625%3DKaichen%2520Zhang%2520and%2520Keming%2520Wu%2520and%2520Zuhao%2520Yang%2520and%2520Kairui%2520Hu%2520and%2520Bin%2520Wang%2520and%2520Ziwei%2520Liu%2520and%2520Xingxuan%2520Li%2520and%2520Lidong%2520Bing%26entry.1292438233%3DRecent%2520advancements%2520in%2520large%2520reasoning%2520models%2520have%2520fueled%2520growing%2520interest%2520in%2520extending%2520such%2520capabilities%2520to%2520multimodal%2520domains.%2520However%252C%2520despite%2520notable%2520progress%2520in%2520visual%2520reasoning%252C%2520the%2520lack%2520of%2520transparent%2520and%2520reproducible%2520data%2520curation%2520and%2520training%2520strategies%2520remains%2520a%2520major%2520barrier%2520to%2520scalable%2520research.%2520In%2520this%2520work%252C%2520we%2520introduce%2520OpenMMReasoner%252C%2520a%2520fully%2520transparent%2520two-stage%2520recipe%2520for%2520multimodal%2520reasoning%2520spanning%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520reinforcement%2520learning%2520%2528RL%2529.%2520In%2520the%2520SFT%2520stage%252C%2520we%2520construct%2520an%2520874K-sample%2520cold-start%2520dataset%2520with%2520rigorous%2520step-by-step%2520validation%252C%2520providing%2520a%2520strong%2520foundation%2520for%2520reasoning%2520capabilities.%2520The%2520subsequent%2520RL%2520stage%2520leverages%2520a%252074K-sample%2520dataset%2520across%2520diverse%2520domains%2520to%2520further%2520sharpen%2520and%2520stabilize%2520these%2520abilities%252C%2520resulting%2520in%2520a%2520more%2520robust%2520and%2520efficient%2520learning%2520process.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520our%2520training%2520recipe%2520not%2520only%2520surpasses%2520strong%2520baselines%2520but%2520also%2520highlights%2520the%2520critical%2520role%2520of%2520data%2520quality%2520and%2520training%2520design%2520in%2520shaping%2520multimodal%2520reasoning%2520performance.%2520Notably%252C%2520our%2520method%2520achieves%2520a%252011.6%2525%2520improvement%2520over%2520the%2520Qwen2.5-VL-7B-Instruct%2520baseline%2520across%2520nine%2520multimodal%2520reasoning%2520benchmarks%252C%2520establishing%2520a%2520solid%2520empirical%2520foundation%2520for%2520future%2520large-scale%2520multimodal%2520reasoning%2520research.%2520We%2520open-sourced%2520all%2520our%2520codes%252C%2520pipeline%252C%2520and%2520data%2520at%2520https%253A//github.com/EvolvingLMMs-Lab/OpenMMReasoner.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16334v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenMMReasoner%3A%20Pushing%20the%20Frontiers%20for%20Multimodal%20Reasoning%20with%20an%20Open%20and%20General%20Recipe&entry.906535625=Kaichen%20Zhang%20and%20Keming%20Wu%20and%20Zuhao%20Yang%20and%20Kairui%20Hu%20and%20Bin%20Wang%20and%20Ziwei%20Liu%20and%20Xingxuan%20Li%20and%20Lidong%20Bing&entry.1292438233=Recent%20advancements%20in%20large%20reasoning%20models%20have%20fueled%20growing%20interest%20in%20extending%20such%20capabilities%20to%20multimodal%20domains.%20However%2C%20despite%20notable%20progress%20in%20visual%20reasoning%2C%20the%20lack%20of%20transparent%20and%20reproducible%20data%20curation%20and%20training%20strategies%20remains%20a%20major%20barrier%20to%20scalable%20research.%20In%20this%20work%2C%20we%20introduce%20OpenMMReasoner%2C%20a%20fully%20transparent%20two-stage%20recipe%20for%20multimodal%20reasoning%20spanning%20supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29.%20In%20the%20SFT%20stage%2C%20we%20construct%20an%20874K-sample%20cold-start%20dataset%20with%20rigorous%20step-by-step%20validation%2C%20providing%20a%20strong%20foundation%20for%20reasoning%20capabilities.%20The%20subsequent%20RL%20stage%20leverages%20a%2074K-sample%20dataset%20across%20diverse%20domains%20to%20further%20sharpen%20and%20stabilize%20these%20abilities%2C%20resulting%20in%20a%20more%20robust%20and%20efficient%20learning%20process.%20Extensive%20evaluations%20demonstrate%20that%20our%20training%20recipe%20not%20only%20surpasses%20strong%20baselines%20but%20also%20highlights%20the%20critical%20role%20of%20data%20quality%20and%20training%20design%20in%20shaping%20multimodal%20reasoning%20performance.%20Notably%2C%20our%20method%20achieves%20a%2011.6%25%20improvement%20over%20the%20Qwen2.5-VL-7B-Instruct%20baseline%20across%20nine%20multimodal%20reasoning%20benchmarks%2C%20establishing%20a%20solid%20empirical%20foundation%20for%20future%20large-scale%20multimodal%20reasoning%20research.%20We%20open-sourced%20all%20our%20codes%2C%20pipeline%2C%20and%20data%20at%20https%3A//github.com/EvolvingLMMs-Lab/OpenMMReasoner.&entry.1838667208=http%3A//arxiv.org/abs/2511.16334v3&entry.124074799=Read"},
{"title": "ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos", "author": "Qi'ao Xu and Tianwen Qian and Yuqian Fu and Kailing Li and Yang Jiao and Jiacheng Zhang and Xiaoling Wang and Liang He", "abstract": "A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \\textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \\textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \\textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \\textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \\href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..", "link": "http://arxiv.org/abs/2512.03666v1", "date": "2025-12-03", "relevancy": 2.1361, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5572}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToG-Bench%3A%20Task-Oriented%20Spatio-Temporal%20Grounding%20in%20Egocentric%20Videos&body=Title%3A%20ToG-Bench%3A%20Task-Oriented%20Spatio-Temporal%20Grounding%20in%20Egocentric%20Videos%0AAuthor%3A%20Qi%27ao%20Xu%20and%20Tianwen%20Qian%20and%20Yuqian%20Fu%20and%20Kailing%20Li%20and%20Yang%20Jiao%20and%20Jiacheng%20Zhang%20and%20Xiaoling%20Wang%20and%20Liang%20He%0AAbstract%3A%20A%20core%20capability%20towards%20general%20embodied%20intelligence%20lies%20in%20localizing%20task-relevant%20objects%20from%20an%20egocentric%20perspective%2C%20formulated%20as%20Spatio-Temporal%20Video%20Grounding%20%28STVG%29.%20Despite%20recent%20progress%2C%20existing%20STVG%20studies%20remain%20largely%20confined%20to%20object-centric%20and%20descriptive%20instructions%2C%20neglecting%20the%20task-oriented%20reasoning%20that%20is%20crucial%20for%20embodied%20agents%20to%20accomplish%20goal-directed%20interactions.%20To%20bridge%20this%20gap%2C%20we%20introduce%20%5Ctextbf%7BToG-Bench%7D%2C%20the%20first%20task-oriented%20spatio-temporal%20video%20grounding%20benchmark%20for%20egocentric%20videos.%20ToG-Bench%20is%20characterized%20by%20three%20key%20features%3A%20%281%29%20%5Ctextbf%7BTask-oriented%20Grounding%7D%2C%20which%20requires%20identifying%20and%20localizing%20objects%20based%20on%20intended%20tasks%20rather%20than%20straightforward%20descriptions%3B%20%282%29%20%5Ctextbf%7BExplicit-Implicit%20Dual%20Grounding%7D%2C%20where%20target%20objects%20can%20be%20either%20explicitly%20mentioned%20or%20implicitly%20inferred%20by%20contextual%20reasoning%3B%20%283%29%20%5Ctextbf%7BOne-to-Many%20Grounding%7D%2C%20where%20a%20single%20instruction%20may%20correspond%20to%20multiple%20objects%20involved%20in%20task%20execution.%20Built%20upon%20videos%20sourced%20from%20ScanNet%2C%20ToG-Bench%20comprises%20100%20annotated%20clips%20with%202%2C704%20task-oriented%20grounding%20instructions%2C%20constructed%20via%20a%20semi-automated%20pipeline%20that%20combines%20foundation%20model%20annotation%20and%20human%20refinement.%20In%20addition%2C%20we%20introduce%20a%20set%20of%20task-level%20evaluation%20metrics%20tailored%20for%20multi-object%20and%20explicit-implicit%20object%20grounding%2C%20and%20systematically%20benchmark%20seven%20state-of-the-art%20MLLMs.%20Extensive%20experiments%20reveal%20the%20intrinsic%20challenges%20of%20task-oriented%20STVG%20and%20substantial%20performance%20gaps%20across%20explicit-implicit%20and%20multi-object%20grounding%2C%20highlighting%20the%20difficulty%20of%20bridging%20perception%20and%20interaction%20in%20embodied%20scenarios.%20Data%20and%20code%20will%20be%20released%20at%3A%20%5Chref%7Bhttps%3A//github.com/qaxuDev/ToG-Bench%7D%7Bhttps%3A//github.com/qaxuDev/ToG-Bench%7D..%0ALink%3A%20http%3A//arxiv.org/abs/2512.03666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToG-Bench%253A%2520Task-Oriented%2520Spatio-Temporal%2520Grounding%2520in%2520Egocentric%2520Videos%26entry.906535625%3DQi%2527ao%2520Xu%2520and%2520Tianwen%2520Qian%2520and%2520Yuqian%2520Fu%2520and%2520Kailing%2520Li%2520and%2520Yang%2520Jiao%2520and%2520Jiacheng%2520Zhang%2520and%2520Xiaoling%2520Wang%2520and%2520Liang%2520He%26entry.1292438233%3DA%2520core%2520capability%2520towards%2520general%2520embodied%2520intelligence%2520lies%2520in%2520localizing%2520task-relevant%2520objects%2520from%2520an%2520egocentric%2520perspective%252C%2520formulated%2520as%2520Spatio-Temporal%2520Video%2520Grounding%2520%2528STVG%2529.%2520Despite%2520recent%2520progress%252C%2520existing%2520STVG%2520studies%2520remain%2520largely%2520confined%2520to%2520object-centric%2520and%2520descriptive%2520instructions%252C%2520neglecting%2520the%2520task-oriented%2520reasoning%2520that%2520is%2520crucial%2520for%2520embodied%2520agents%2520to%2520accomplish%2520goal-directed%2520interactions.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520%255Ctextbf%257BToG-Bench%257D%252C%2520the%2520first%2520task-oriented%2520spatio-temporal%2520video%2520grounding%2520benchmark%2520for%2520egocentric%2520videos.%2520ToG-Bench%2520is%2520characterized%2520by%2520three%2520key%2520features%253A%2520%25281%2529%2520%255Ctextbf%257BTask-oriented%2520Grounding%257D%252C%2520which%2520requires%2520identifying%2520and%2520localizing%2520objects%2520based%2520on%2520intended%2520tasks%2520rather%2520than%2520straightforward%2520descriptions%253B%2520%25282%2529%2520%255Ctextbf%257BExplicit-Implicit%2520Dual%2520Grounding%257D%252C%2520where%2520target%2520objects%2520can%2520be%2520either%2520explicitly%2520mentioned%2520or%2520implicitly%2520inferred%2520by%2520contextual%2520reasoning%253B%2520%25283%2529%2520%255Ctextbf%257BOne-to-Many%2520Grounding%257D%252C%2520where%2520a%2520single%2520instruction%2520may%2520correspond%2520to%2520multiple%2520objects%2520involved%2520in%2520task%2520execution.%2520Built%2520upon%2520videos%2520sourced%2520from%2520ScanNet%252C%2520ToG-Bench%2520comprises%2520100%2520annotated%2520clips%2520with%25202%252C704%2520task-oriented%2520grounding%2520instructions%252C%2520constructed%2520via%2520a%2520semi-automated%2520pipeline%2520that%2520combines%2520foundation%2520model%2520annotation%2520and%2520human%2520refinement.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520set%2520of%2520task-level%2520evaluation%2520metrics%2520tailored%2520for%2520multi-object%2520and%2520explicit-implicit%2520object%2520grounding%252C%2520and%2520systematically%2520benchmark%2520seven%2520state-of-the-art%2520MLLMs.%2520Extensive%2520experiments%2520reveal%2520the%2520intrinsic%2520challenges%2520of%2520task-oriented%2520STVG%2520and%2520substantial%2520performance%2520gaps%2520across%2520explicit-implicit%2520and%2520multi-object%2520grounding%252C%2520highlighting%2520the%2520difficulty%2520of%2520bridging%2520perception%2520and%2520interaction%2520in%2520embodied%2520scenarios.%2520Data%2520and%2520code%2520will%2520be%2520released%2520at%253A%2520%255Chref%257Bhttps%253A//github.com/qaxuDev/ToG-Bench%257D%257Bhttps%253A//github.com/qaxuDev/ToG-Bench%257D..%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToG-Bench%3A%20Task-Oriented%20Spatio-Temporal%20Grounding%20in%20Egocentric%20Videos&entry.906535625=Qi%27ao%20Xu%20and%20Tianwen%20Qian%20and%20Yuqian%20Fu%20and%20Kailing%20Li%20and%20Yang%20Jiao%20and%20Jiacheng%20Zhang%20and%20Xiaoling%20Wang%20and%20Liang%20He&entry.1292438233=A%20core%20capability%20towards%20general%20embodied%20intelligence%20lies%20in%20localizing%20task-relevant%20objects%20from%20an%20egocentric%20perspective%2C%20formulated%20as%20Spatio-Temporal%20Video%20Grounding%20%28STVG%29.%20Despite%20recent%20progress%2C%20existing%20STVG%20studies%20remain%20largely%20confined%20to%20object-centric%20and%20descriptive%20instructions%2C%20neglecting%20the%20task-oriented%20reasoning%20that%20is%20crucial%20for%20embodied%20agents%20to%20accomplish%20goal-directed%20interactions.%20To%20bridge%20this%20gap%2C%20we%20introduce%20%5Ctextbf%7BToG-Bench%7D%2C%20the%20first%20task-oriented%20spatio-temporal%20video%20grounding%20benchmark%20for%20egocentric%20videos.%20ToG-Bench%20is%20characterized%20by%20three%20key%20features%3A%20%281%29%20%5Ctextbf%7BTask-oriented%20Grounding%7D%2C%20which%20requires%20identifying%20and%20localizing%20objects%20based%20on%20intended%20tasks%20rather%20than%20straightforward%20descriptions%3B%20%282%29%20%5Ctextbf%7BExplicit-Implicit%20Dual%20Grounding%7D%2C%20where%20target%20objects%20can%20be%20either%20explicitly%20mentioned%20or%20implicitly%20inferred%20by%20contextual%20reasoning%3B%20%283%29%20%5Ctextbf%7BOne-to-Many%20Grounding%7D%2C%20where%20a%20single%20instruction%20may%20correspond%20to%20multiple%20objects%20involved%20in%20task%20execution.%20Built%20upon%20videos%20sourced%20from%20ScanNet%2C%20ToG-Bench%20comprises%20100%20annotated%20clips%20with%202%2C704%20task-oriented%20grounding%20instructions%2C%20constructed%20via%20a%20semi-automated%20pipeline%20that%20combines%20foundation%20model%20annotation%20and%20human%20refinement.%20In%20addition%2C%20we%20introduce%20a%20set%20of%20task-level%20evaluation%20metrics%20tailored%20for%20multi-object%20and%20explicit-implicit%20object%20grounding%2C%20and%20systematically%20benchmark%20seven%20state-of-the-art%20MLLMs.%20Extensive%20experiments%20reveal%20the%20intrinsic%20challenges%20of%20task-oriented%20STVG%20and%20substantial%20performance%20gaps%20across%20explicit-implicit%20and%20multi-object%20grounding%2C%20highlighting%20the%20difficulty%20of%20bridging%20perception%20and%20interaction%20in%20embodied%20scenarios.%20Data%20and%20code%20will%20be%20released%20at%3A%20%5Chref%7Bhttps%3A//github.com/qaxuDev/ToG-Bench%7D%7Bhttps%3A//github.com/qaxuDev/ToG-Bench%7D..&entry.1838667208=http%3A//arxiv.org/abs/2512.03666v1&entry.124074799=Read"},
{"title": "Hierarchical Vision Language Action Model Using Success and Failure Demonstrations", "author": "Jeongeun Park and Jihwan Yoon and Byungwoo Jeon and Juhan Park and Jinwoo Shin and Namhoon Cho and Kyungjae Lee and Sangdoo Yun and Sungjoon Choi", "abstract": "Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.", "link": "http://arxiv.org/abs/2512.03913v1", "date": "2025-12-03", "relevancy": 2.1335, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5344}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Vision%20Language%20Action%20Model%20Using%20Success%20and%20Failure%20Demonstrations&body=Title%3A%20Hierarchical%20Vision%20Language%20Action%20Model%20Using%20Success%20and%20Failure%20Demonstrations%0AAuthor%3A%20Jeongeun%20Park%20and%20Jihwan%20Yoon%20and%20Byungwoo%20Jeon%20and%20Juhan%20Park%20and%20Jinwoo%20Shin%20and%20Namhoon%20Cho%20and%20Kyungjae%20Lee%20and%20Sangdoo%20Yun%20and%20Sungjoon%20Choi%0AAbstract%3A%20Prior%20Vision-Language-Action%20%28VLA%29%20models%20are%20typically%20trained%20on%20teleoperated%20successful%20demonstrations%2C%20while%20discarding%20numerous%20failed%20attempts%20that%20occur%20naturally%20during%20data%20collection.%20However%2C%20these%20failures%20encode%20where%20and%20how%20policies%20can%20be%20fragile%2C%20information%20that%20can%20be%20exploited%20to%20improve%20robustness.%20We%20address%20this%20problem%20by%20leveraging%20mixed-quality%20datasets%20to%20learn%20failure-aware%20reasoning%20at%20planning%20time.%20We%20introduce%20VINE%2C%20a%20hierarchical%20vision-language-action%20model%20that%20separates%20high-level%20reasoning%20%28System%202%29%20from%20low-level%20control%20%28System%201%29%20under%20a%20hierarchical%20reinforcement%20learning%20formalism%2C%20making%20failures%20usable%20as%20a%20structured%20learning%20signal%20rather%20than%20noisy%20supervision.%20System%202%20performs%20feasibility-guided%20tree%20search%20over%20a%202D%20scene-graph%20abstraction%3A%20it%20proposes%20subgoal%20transitions%2C%20predicts%20success%20probabilities%20from%20both%20successes%20and%20failures%2C%20and%20prunes%20brittle%20branches%20before%20execution%2C%20effectively%20casting%20plan%20evaluation%20as%20feasibility%20scoring.%20The%20selected%20subgoal%20sequence%20is%20then%20passed%20to%20System%201%2C%20which%20executes%20low-level%20actions%20without%20modifying%20the%20agent%27s%20core%20skills.%20Trained%20entirely%20from%20offline%20teleoperation%20data%2C%20VINE%20integrates%20negative%20experience%20directly%20into%20the%20decision%20loop.%20Across%20challenging%20manipulation%20tasks%2C%20this%20approach%20consistently%20improves%20success%20rates%20and%20robustness%2C%20demonstrating%20that%20failure%20data%20is%20an%20essential%20resource%20for%20converting%20the%20broad%20competence%20of%20VLAs%20into%20robust%20execution.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Vision%2520Language%2520Action%2520Model%2520Using%2520Success%2520and%2520Failure%2520Demonstrations%26entry.906535625%3DJeongeun%2520Park%2520and%2520Jihwan%2520Yoon%2520and%2520Byungwoo%2520Jeon%2520and%2520Juhan%2520Park%2520and%2520Jinwoo%2520Shin%2520and%2520Namhoon%2520Cho%2520and%2520Kyungjae%2520Lee%2520and%2520Sangdoo%2520Yun%2520and%2520Sungjoon%2520Choi%26entry.1292438233%3DPrior%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520are%2520typically%2520trained%2520on%2520teleoperated%2520successful%2520demonstrations%252C%2520while%2520discarding%2520numerous%2520failed%2520attempts%2520that%2520occur%2520naturally%2520during%2520data%2520collection.%2520However%252C%2520these%2520failures%2520encode%2520where%2520and%2520how%2520policies%2520can%2520be%2520fragile%252C%2520information%2520that%2520can%2520be%2520exploited%2520to%2520improve%2520robustness.%2520We%2520address%2520this%2520problem%2520by%2520leveraging%2520mixed-quality%2520datasets%2520to%2520learn%2520failure-aware%2520reasoning%2520at%2520planning%2520time.%2520We%2520introduce%2520VINE%252C%2520a%2520hierarchical%2520vision-language-action%2520model%2520that%2520separates%2520high-level%2520reasoning%2520%2528System%25202%2529%2520from%2520low-level%2520control%2520%2528System%25201%2529%2520under%2520a%2520hierarchical%2520reinforcement%2520learning%2520formalism%252C%2520making%2520failures%2520usable%2520as%2520a%2520structured%2520learning%2520signal%2520rather%2520than%2520noisy%2520supervision.%2520System%25202%2520performs%2520feasibility-guided%2520tree%2520search%2520over%2520a%25202D%2520scene-graph%2520abstraction%253A%2520it%2520proposes%2520subgoal%2520transitions%252C%2520predicts%2520success%2520probabilities%2520from%2520both%2520successes%2520and%2520failures%252C%2520and%2520prunes%2520brittle%2520branches%2520before%2520execution%252C%2520effectively%2520casting%2520plan%2520evaluation%2520as%2520feasibility%2520scoring.%2520The%2520selected%2520subgoal%2520sequence%2520is%2520then%2520passed%2520to%2520System%25201%252C%2520which%2520executes%2520low-level%2520actions%2520without%2520modifying%2520the%2520agent%2527s%2520core%2520skills.%2520Trained%2520entirely%2520from%2520offline%2520teleoperation%2520data%252C%2520VINE%2520integrates%2520negative%2520experience%2520directly%2520into%2520the%2520decision%2520loop.%2520Across%2520challenging%2520manipulation%2520tasks%252C%2520this%2520approach%2520consistently%2520improves%2520success%2520rates%2520and%2520robustness%252C%2520demonstrating%2520that%2520failure%2520data%2520is%2520an%2520essential%2520resource%2520for%2520converting%2520the%2520broad%2520competence%2520of%2520VLAs%2520into%2520robust%2520execution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Vision%20Language%20Action%20Model%20Using%20Success%20and%20Failure%20Demonstrations&entry.906535625=Jeongeun%20Park%20and%20Jihwan%20Yoon%20and%20Byungwoo%20Jeon%20and%20Juhan%20Park%20and%20Jinwoo%20Shin%20and%20Namhoon%20Cho%20and%20Kyungjae%20Lee%20and%20Sangdoo%20Yun%20and%20Sungjoon%20Choi&entry.1292438233=Prior%20Vision-Language-Action%20%28VLA%29%20models%20are%20typically%20trained%20on%20teleoperated%20successful%20demonstrations%2C%20while%20discarding%20numerous%20failed%20attempts%20that%20occur%20naturally%20during%20data%20collection.%20However%2C%20these%20failures%20encode%20where%20and%20how%20policies%20can%20be%20fragile%2C%20information%20that%20can%20be%20exploited%20to%20improve%20robustness.%20We%20address%20this%20problem%20by%20leveraging%20mixed-quality%20datasets%20to%20learn%20failure-aware%20reasoning%20at%20planning%20time.%20We%20introduce%20VINE%2C%20a%20hierarchical%20vision-language-action%20model%20that%20separates%20high-level%20reasoning%20%28System%202%29%20from%20low-level%20control%20%28System%201%29%20under%20a%20hierarchical%20reinforcement%20learning%20formalism%2C%20making%20failures%20usable%20as%20a%20structured%20learning%20signal%20rather%20than%20noisy%20supervision.%20System%202%20performs%20feasibility-guided%20tree%20search%20over%20a%202D%20scene-graph%20abstraction%3A%20it%20proposes%20subgoal%20transitions%2C%20predicts%20success%20probabilities%20from%20both%20successes%20and%20failures%2C%20and%20prunes%20brittle%20branches%20before%20execution%2C%20effectively%20casting%20plan%20evaluation%20as%20feasibility%20scoring.%20The%20selected%20subgoal%20sequence%20is%20then%20passed%20to%20System%201%2C%20which%20executes%20low-level%20actions%20without%20modifying%20the%20agent%27s%20core%20skills.%20Trained%20entirely%20from%20offline%20teleoperation%20data%2C%20VINE%20integrates%20negative%20experience%20directly%20into%20the%20decision%20loop.%20Across%20challenging%20manipulation%20tasks%2C%20this%20approach%20consistently%20improves%20success%20rates%20and%20robustness%2C%20demonstrating%20that%20failure%20data%20is%20an%20essential%20resource%20for%20converting%20the%20broad%20competence%20of%20VLAs%20into%20robust%20execution.&entry.1838667208=http%3A//arxiv.org/abs/2512.03913v1&entry.124074799=Read"},
{"title": "MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation", "author": "Xiaobei Zhao and Xingqi Lyu and Xiang Li", "abstract": "Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction. Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception. To bridge this gap, we present the method of Agricultural Vision-and-Language Navigation with Monocular Depth Estimation (MDE-AgriVLN), in which we propose the MDE module generating depth features from RGB images, to assist the decision-maker on reasoning. When evaluated on the A2A benchmark, our MDE-AgriVLN method successfully increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, demonstrating the state-of-the-art performance in the agricultural VLN domain. Code: https://github.com/AlexTraveling/MDE-AgriVLN.", "link": "http://arxiv.org/abs/2512.03958v1", "date": "2025-12-03", "relevancy": 2.1166, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDE-AgriVLN%3A%20Agricultural%20Vision-and-Language%20Navigation%20with%20Monocular%20Depth%20Estimation&body=Title%3A%20MDE-AgriVLN%3A%20Agricultural%20Vision-and-Language%20Navigation%20with%20Monocular%20Depth%20Estimation%0AAuthor%3A%20Xiaobei%20Zhao%20and%20Xingqi%20Lyu%20and%20Xiang%20Li%0AAbstract%3A%20Agricultural%20robots%20are%20serving%20as%20powerful%20assistants%20across%20a%20wide%20range%20of%20agricultural%20tasks%2C%20nevertheless%2C%20still%20heavily%20relying%20on%20manual%20operations%20or%20railway%20systems%20for%20movement.%20The%20AgriVLN%20method%20and%20the%20A2A%20benchmark%20pioneeringly%20extend%20Vision-and-Language%20Navigation%20%28VLN%29%20to%20the%20agricultural%20domain%2C%20enabling%20a%20robot%20to%20navigate%20to%20a%20target%20position%20following%20a%20natural%20language%20instruction.%20Unlike%20human%20binocular%20vision%2C%20most%20agricultural%20robots%20are%20only%20given%20a%20single%20camera%20for%20monocular%20vision%2C%20which%20results%20in%20limited%20spatial%20perception.%20To%20bridge%20this%20gap%2C%20we%20present%20the%20method%20of%20Agricultural%20Vision-and-Language%20Navigation%20with%20Monocular%20Depth%20Estimation%20%28MDE-AgriVLN%29%2C%20in%20which%20we%20propose%20the%20MDE%20module%20generating%20depth%20features%20from%20RGB%20images%2C%20to%20assist%20the%20decision-maker%20on%20reasoning.%20When%20evaluated%20on%20the%20A2A%20benchmark%2C%20our%20MDE-AgriVLN%20method%20successfully%20increases%20Success%20Rate%20from%200.23%20to%200.32%20and%20decreases%20Navigation%20Error%20from%204.43m%20to%204.08m%2C%20demonstrating%20the%20state-of-the-art%20performance%20in%20the%20agricultural%20VLN%20domain.%20Code%3A%20https%3A//github.com/AlexTraveling/MDE-AgriVLN.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDE-AgriVLN%253A%2520Agricultural%2520Vision-and-Language%2520Navigation%2520with%2520Monocular%2520Depth%2520Estimation%26entry.906535625%3DXiaobei%2520Zhao%2520and%2520Xingqi%2520Lyu%2520and%2520Xiang%2520Li%26entry.1292438233%3DAgricultural%2520robots%2520are%2520serving%2520as%2520powerful%2520assistants%2520across%2520a%2520wide%2520range%2520of%2520agricultural%2520tasks%252C%2520nevertheless%252C%2520still%2520heavily%2520relying%2520on%2520manual%2520operations%2520or%2520railway%2520systems%2520for%2520movement.%2520The%2520AgriVLN%2520method%2520and%2520the%2520A2A%2520benchmark%2520pioneeringly%2520extend%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520to%2520the%2520agricultural%2520domain%252C%2520enabling%2520a%2520robot%2520to%2520navigate%2520to%2520a%2520target%2520position%2520following%2520a%2520natural%2520language%2520instruction.%2520Unlike%2520human%2520binocular%2520vision%252C%2520most%2520agricultural%2520robots%2520are%2520only%2520given%2520a%2520single%2520camera%2520for%2520monocular%2520vision%252C%2520which%2520results%2520in%2520limited%2520spatial%2520perception.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520the%2520method%2520of%2520Agricultural%2520Vision-and-Language%2520Navigation%2520with%2520Monocular%2520Depth%2520Estimation%2520%2528MDE-AgriVLN%2529%252C%2520in%2520which%2520we%2520propose%2520the%2520MDE%2520module%2520generating%2520depth%2520features%2520from%2520RGB%2520images%252C%2520to%2520assist%2520the%2520decision-maker%2520on%2520reasoning.%2520When%2520evaluated%2520on%2520the%2520A2A%2520benchmark%252C%2520our%2520MDE-AgriVLN%2520method%2520successfully%2520increases%2520Success%2520Rate%2520from%25200.23%2520to%25200.32%2520and%2520decreases%2520Navigation%2520Error%2520from%25204.43m%2520to%25204.08m%252C%2520demonstrating%2520the%2520state-of-the-art%2520performance%2520in%2520the%2520agricultural%2520VLN%2520domain.%2520Code%253A%2520https%253A//github.com/AlexTraveling/MDE-AgriVLN.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDE-AgriVLN%3A%20Agricultural%20Vision-and-Language%20Navigation%20with%20Monocular%20Depth%20Estimation&entry.906535625=Xiaobei%20Zhao%20and%20Xingqi%20Lyu%20and%20Xiang%20Li&entry.1292438233=Agricultural%20robots%20are%20serving%20as%20powerful%20assistants%20across%20a%20wide%20range%20of%20agricultural%20tasks%2C%20nevertheless%2C%20still%20heavily%20relying%20on%20manual%20operations%20or%20railway%20systems%20for%20movement.%20The%20AgriVLN%20method%20and%20the%20A2A%20benchmark%20pioneeringly%20extend%20Vision-and-Language%20Navigation%20%28VLN%29%20to%20the%20agricultural%20domain%2C%20enabling%20a%20robot%20to%20navigate%20to%20a%20target%20position%20following%20a%20natural%20language%20instruction.%20Unlike%20human%20binocular%20vision%2C%20most%20agricultural%20robots%20are%20only%20given%20a%20single%20camera%20for%20monocular%20vision%2C%20which%20results%20in%20limited%20spatial%20perception.%20To%20bridge%20this%20gap%2C%20we%20present%20the%20method%20of%20Agricultural%20Vision-and-Language%20Navigation%20with%20Monocular%20Depth%20Estimation%20%28MDE-AgriVLN%29%2C%20in%20which%20we%20propose%20the%20MDE%20module%20generating%20depth%20features%20from%20RGB%20images%2C%20to%20assist%20the%20decision-maker%20on%20reasoning.%20When%20evaluated%20on%20the%20A2A%20benchmark%2C%20our%20MDE-AgriVLN%20method%20successfully%20increases%20Success%20Rate%20from%200.23%20to%200.32%20and%20decreases%20Navigation%20Error%20from%204.43m%20to%204.08m%2C%20demonstrating%20the%20state-of-the-art%20performance%20in%20the%20agricultural%20VLN%20domain.%20Code%3A%20https%3A//github.com/AlexTraveling/MDE-AgriVLN.&entry.1838667208=http%3A//arxiv.org/abs/2512.03958v1&entry.124074799=Read"},
{"title": "Ultra-lightweight Neural Video Representation Compression", "author": "Ho Man Kwan and Tianhao Peng and Ge Gao and Fan Zhang and Mike Nilsson and Andrew Gower and David Bull", "abstract": "Recent works have demonstrated the viability of utilizing over-fitted implicit neural representations (INRs) as alternatives to autoencoder-based models for neural video compression. Among these INR-based video codecs, Neural Video Representation Compression (NVRC) was the first to adopt a fully end-to-end compression framework that compresses INRs, achieving state-of-the-art performance. Moreover, some recently proposed lightweight INRs have shown comparable performance to their baseline codecs with computational complexity lower than 10kMACs/pixel. In this work, we extend NVRC toward lightweight representations, and propose NVRC-Lite, which incorporates two key changes. Firstly, we integrated multi-scale feature grids into our lightweight neural representation, and the use of higher resolution grids significantly improves the performance of INRs at low complexity. Secondly, we address the issue that existing INRs typically leverage autoregressive models for entropy coding: these are effective but impractical due to their slow coding speed. In this work, we propose an octree-based context model for entropy coding high-dimensional feature grids, which accelerates the entropy coding module of the model. Our experimental results demonstrate that NVRC-Lite outperforms C3, one of the best lightweight INR-based video codecs, with up to 21.03% and 23.06% BD-rate savings when measured in PSNR and MS-SSIM, respectively, while achieving 8.4x encoding and 2.5x decoding speedup. The implementation of NVRC-Lite will be made available.", "link": "http://arxiv.org/abs/2512.04019v1", "date": "2025-12-03", "relevancy": 2.1132, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5358}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5297}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra-lightweight%20Neural%20Video%20Representation%20Compression&body=Title%3A%20Ultra-lightweight%20Neural%20Video%20Representation%20Compression%0AAuthor%3A%20Ho%20Man%20Kwan%20and%20Tianhao%20Peng%20and%20Ge%20Gao%20and%20Fan%20Zhang%20and%20Mike%20Nilsson%20and%20Andrew%20Gower%20and%20David%20Bull%0AAbstract%3A%20Recent%20works%20have%20demonstrated%20the%20viability%20of%20utilizing%20over-fitted%20implicit%20neural%20representations%20%28INRs%29%20as%20alternatives%20to%20autoencoder-based%20models%20for%20neural%20video%20compression.%20Among%20these%20INR-based%20video%20codecs%2C%20Neural%20Video%20Representation%20Compression%20%28NVRC%29%20was%20the%20first%20to%20adopt%20a%20fully%20end-to-end%20compression%20framework%20that%20compresses%20INRs%2C%20achieving%20state-of-the-art%20performance.%20Moreover%2C%20some%20recently%20proposed%20lightweight%20INRs%20have%20shown%20comparable%20performance%20to%20their%20baseline%20codecs%20with%20computational%20complexity%20lower%20than%2010kMACs/pixel.%20In%20this%20work%2C%20we%20extend%20NVRC%20toward%20lightweight%20representations%2C%20and%20propose%20NVRC-Lite%2C%20which%20incorporates%20two%20key%20changes.%20Firstly%2C%20we%20integrated%20multi-scale%20feature%20grids%20into%20our%20lightweight%20neural%20representation%2C%20and%20the%20use%20of%20higher%20resolution%20grids%20significantly%20improves%20the%20performance%20of%20INRs%20at%20low%20complexity.%20Secondly%2C%20we%20address%20the%20issue%20that%20existing%20INRs%20typically%20leverage%20autoregressive%20models%20for%20entropy%20coding%3A%20these%20are%20effective%20but%20impractical%20due%20to%20their%20slow%20coding%20speed.%20In%20this%20work%2C%20we%20propose%20an%20octree-based%20context%20model%20for%20entropy%20coding%20high-dimensional%20feature%20grids%2C%20which%20accelerates%20the%20entropy%20coding%20module%20of%20the%20model.%20Our%20experimental%20results%20demonstrate%20that%20NVRC-Lite%20outperforms%20C3%2C%20one%20of%20the%20best%20lightweight%20INR-based%20video%20codecs%2C%20with%20up%20to%2021.03%25%20and%2023.06%25%20BD-rate%20savings%20when%20measured%20in%20PSNR%20and%20MS-SSIM%2C%20respectively%2C%20while%20achieving%208.4x%20encoding%20and%202.5x%20decoding%20speedup.%20The%20implementation%20of%20NVRC-Lite%20will%20be%20made%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra-lightweight%2520Neural%2520Video%2520Representation%2520Compression%26entry.906535625%3DHo%2520Man%2520Kwan%2520and%2520Tianhao%2520Peng%2520and%2520Ge%2520Gao%2520and%2520Fan%2520Zhang%2520and%2520Mike%2520Nilsson%2520and%2520Andrew%2520Gower%2520and%2520David%2520Bull%26entry.1292438233%3DRecent%2520works%2520have%2520demonstrated%2520the%2520viability%2520of%2520utilizing%2520over-fitted%2520implicit%2520neural%2520representations%2520%2528INRs%2529%2520as%2520alternatives%2520to%2520autoencoder-based%2520models%2520for%2520neural%2520video%2520compression.%2520Among%2520these%2520INR-based%2520video%2520codecs%252C%2520Neural%2520Video%2520Representation%2520Compression%2520%2528NVRC%2529%2520was%2520the%2520first%2520to%2520adopt%2520a%2520fully%2520end-to-end%2520compression%2520framework%2520that%2520compresses%2520INRs%252C%2520achieving%2520state-of-the-art%2520performance.%2520Moreover%252C%2520some%2520recently%2520proposed%2520lightweight%2520INRs%2520have%2520shown%2520comparable%2520performance%2520to%2520their%2520baseline%2520codecs%2520with%2520computational%2520complexity%2520lower%2520than%252010kMACs/pixel.%2520In%2520this%2520work%252C%2520we%2520extend%2520NVRC%2520toward%2520lightweight%2520representations%252C%2520and%2520propose%2520NVRC-Lite%252C%2520which%2520incorporates%2520two%2520key%2520changes.%2520Firstly%252C%2520we%2520integrated%2520multi-scale%2520feature%2520grids%2520into%2520our%2520lightweight%2520neural%2520representation%252C%2520and%2520the%2520use%2520of%2520higher%2520resolution%2520grids%2520significantly%2520improves%2520the%2520performance%2520of%2520INRs%2520at%2520low%2520complexity.%2520Secondly%252C%2520we%2520address%2520the%2520issue%2520that%2520existing%2520INRs%2520typically%2520leverage%2520autoregressive%2520models%2520for%2520entropy%2520coding%253A%2520these%2520are%2520effective%2520but%2520impractical%2520due%2520to%2520their%2520slow%2520coding%2520speed.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520octree-based%2520context%2520model%2520for%2520entropy%2520coding%2520high-dimensional%2520feature%2520grids%252C%2520which%2520accelerates%2520the%2520entropy%2520coding%2520module%2520of%2520the%2520model.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520NVRC-Lite%2520outperforms%2520C3%252C%2520one%2520of%2520the%2520best%2520lightweight%2520INR-based%2520video%2520codecs%252C%2520with%2520up%2520to%252021.03%2525%2520and%252023.06%2525%2520BD-rate%2520savings%2520when%2520measured%2520in%2520PSNR%2520and%2520MS-SSIM%252C%2520respectively%252C%2520while%2520achieving%25208.4x%2520encoding%2520and%25202.5x%2520decoding%2520speedup.%2520The%2520implementation%2520of%2520NVRC-Lite%2520will%2520be%2520made%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra-lightweight%20Neural%20Video%20Representation%20Compression&entry.906535625=Ho%20Man%20Kwan%20and%20Tianhao%20Peng%20and%20Ge%20Gao%20and%20Fan%20Zhang%20and%20Mike%20Nilsson%20and%20Andrew%20Gower%20and%20David%20Bull&entry.1292438233=Recent%20works%20have%20demonstrated%20the%20viability%20of%20utilizing%20over-fitted%20implicit%20neural%20representations%20%28INRs%29%20as%20alternatives%20to%20autoencoder-based%20models%20for%20neural%20video%20compression.%20Among%20these%20INR-based%20video%20codecs%2C%20Neural%20Video%20Representation%20Compression%20%28NVRC%29%20was%20the%20first%20to%20adopt%20a%20fully%20end-to-end%20compression%20framework%20that%20compresses%20INRs%2C%20achieving%20state-of-the-art%20performance.%20Moreover%2C%20some%20recently%20proposed%20lightweight%20INRs%20have%20shown%20comparable%20performance%20to%20their%20baseline%20codecs%20with%20computational%20complexity%20lower%20than%2010kMACs/pixel.%20In%20this%20work%2C%20we%20extend%20NVRC%20toward%20lightweight%20representations%2C%20and%20propose%20NVRC-Lite%2C%20which%20incorporates%20two%20key%20changes.%20Firstly%2C%20we%20integrated%20multi-scale%20feature%20grids%20into%20our%20lightweight%20neural%20representation%2C%20and%20the%20use%20of%20higher%20resolution%20grids%20significantly%20improves%20the%20performance%20of%20INRs%20at%20low%20complexity.%20Secondly%2C%20we%20address%20the%20issue%20that%20existing%20INRs%20typically%20leverage%20autoregressive%20models%20for%20entropy%20coding%3A%20these%20are%20effective%20but%20impractical%20due%20to%20their%20slow%20coding%20speed.%20In%20this%20work%2C%20we%20propose%20an%20octree-based%20context%20model%20for%20entropy%20coding%20high-dimensional%20feature%20grids%2C%20which%20accelerates%20the%20entropy%20coding%20module%20of%20the%20model.%20Our%20experimental%20results%20demonstrate%20that%20NVRC-Lite%20outperforms%20C3%2C%20one%20of%20the%20best%20lightweight%20INR-based%20video%20codecs%2C%20with%20up%20to%2021.03%25%20and%2023.06%25%20BD-rate%20savings%20when%20measured%20in%20PSNR%20and%20MS-SSIM%2C%20respectively%2C%20while%20achieving%208.4x%20encoding%20and%202.5x%20decoding%20speedup.%20The%20implementation%20of%20NVRC-Lite%20will%20be%20made%20available.&entry.1838667208=http%3A//arxiv.org/abs/2512.04019v1&entry.124074799=Read"},
{"title": "Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning", "author": "Dongchao Yang and Songxiang Liu and Disong Wang and Yuanyuan Wang and Guanglu Wan and Helen Meng", "abstract": "Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.", "link": "http://arxiv.org/abs/2512.03783v1", "date": "2025-12-03", "relevancy": 2.1132, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5281}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-AutoThink%3A%20Adaptive%20Multimodal%20Reasoning%20via%20Reinforcement%20Learning&body=Title%3A%20Omni-AutoThink%3A%20Adaptive%20Multimodal%20Reasoning%20via%20Reinforcement%20Learning%0AAuthor%3A%20Dongchao%20Yang%20and%20Songxiang%20Liu%20and%20Disong%20Wang%20and%20Yuanyuan%20Wang%20and%20Guanglu%20Wan%20and%20Helen%20Meng%0AAbstract%3A%20Recent%20advances%20in%20Omni%20models%20have%20enabled%20unified%20multimodal%20perception%20and%20generation.%20However%2C%20most%20existing%20systems%20still%20exhibit%20rigid%20reasoning%20behaviors%2C%20either%20overthinking%20simple%20problems%20or%20failing%20to%20reason%20when%20necessary.%20To%20address%20this%20limitation%2C%20we%20propose%20Omni-AutoThink%2C%20a%20novel%20adaptive%20reasoning%20framework%20that%20dynamically%20adjusts%20the%20model%27s%20reasoning%20depth%20according%20to%20task%20difficulty.%20Our%20framework%20comprises%20two%20stages%3A%20%281%29%20an%20Adaptive%20Supervised%20Fine-Tuning%20%28Adaptive%20SFT%29%20stage%2C%20which%20endows%20the%20Omni%20model%20with%20fundamental%20reasoning%20capability%20using%20large-scale%20reasoning-augmented%20data%2C%20and%20%282%29%20an%20Adaptive%20Reinforcement%20Learning%20%28Adaptive%20GRPO%29%20stage%2C%20which%20optimizes%20reasoning%20behaviors%20based%20on%20task%20complexity%20and%20reward%20feedback.%20We%20further%20construct%20a%20comprehensive%20adaptive%20reasoning%20benchmark%20that%20spans%20text-only%2C%20text-audio%2C%20text-visual%2C%20and%20text-audio-visual%20modalities%2C%20providing%20both%20training%20and%20evaluation%20splits%20for%20multimodal%20reasoning%20assessment.%20Experimental%20results%20demonstrate%20that%20our%20proposed%20framework%20significantly%20improves%20adaptive%20reasoning%20performance%20compared%20to%20previous%20baselines.%20All%20benchmark%20data%20and%20code%20will%20be%20publicly%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-AutoThink%253A%2520Adaptive%2520Multimodal%2520Reasoning%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DDongchao%2520Yang%2520and%2520Songxiang%2520Liu%2520and%2520Disong%2520Wang%2520and%2520Yuanyuan%2520Wang%2520and%2520Guanglu%2520Wan%2520and%2520Helen%2520Meng%26entry.1292438233%3DRecent%2520advances%2520in%2520Omni%2520models%2520have%2520enabled%2520unified%2520multimodal%2520perception%2520and%2520generation.%2520However%252C%2520most%2520existing%2520systems%2520still%2520exhibit%2520rigid%2520reasoning%2520behaviors%252C%2520either%2520overthinking%2520simple%2520problems%2520or%2520failing%2520to%2520reason%2520when%2520necessary.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Omni-AutoThink%252C%2520a%2520novel%2520adaptive%2520reasoning%2520framework%2520that%2520dynamically%2520adjusts%2520the%2520model%2527s%2520reasoning%2520depth%2520according%2520to%2520task%2520difficulty.%2520Our%2520framework%2520comprises%2520two%2520stages%253A%2520%25281%2529%2520an%2520Adaptive%2520Supervised%2520Fine-Tuning%2520%2528Adaptive%2520SFT%2529%2520stage%252C%2520which%2520endows%2520the%2520Omni%2520model%2520with%2520fundamental%2520reasoning%2520capability%2520using%2520large-scale%2520reasoning-augmented%2520data%252C%2520and%2520%25282%2529%2520an%2520Adaptive%2520Reinforcement%2520Learning%2520%2528Adaptive%2520GRPO%2529%2520stage%252C%2520which%2520optimizes%2520reasoning%2520behaviors%2520based%2520on%2520task%2520complexity%2520and%2520reward%2520feedback.%2520We%2520further%2520construct%2520a%2520comprehensive%2520adaptive%2520reasoning%2520benchmark%2520that%2520spans%2520text-only%252C%2520text-audio%252C%2520text-visual%252C%2520and%2520text-audio-visual%2520modalities%252C%2520providing%2520both%2520training%2520and%2520evaluation%2520splits%2520for%2520multimodal%2520reasoning%2520assessment.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520proposed%2520framework%2520significantly%2520improves%2520adaptive%2520reasoning%2520performance%2520compared%2520to%2520previous%2520baselines.%2520All%2520benchmark%2520data%2520and%2520code%2520will%2520be%2520publicly%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-AutoThink%3A%20Adaptive%20Multimodal%20Reasoning%20via%20Reinforcement%20Learning&entry.906535625=Dongchao%20Yang%20and%20Songxiang%20Liu%20and%20Disong%20Wang%20and%20Yuanyuan%20Wang%20and%20Guanglu%20Wan%20and%20Helen%20Meng&entry.1292438233=Recent%20advances%20in%20Omni%20models%20have%20enabled%20unified%20multimodal%20perception%20and%20generation.%20However%2C%20most%20existing%20systems%20still%20exhibit%20rigid%20reasoning%20behaviors%2C%20either%20overthinking%20simple%20problems%20or%20failing%20to%20reason%20when%20necessary.%20To%20address%20this%20limitation%2C%20we%20propose%20Omni-AutoThink%2C%20a%20novel%20adaptive%20reasoning%20framework%20that%20dynamically%20adjusts%20the%20model%27s%20reasoning%20depth%20according%20to%20task%20difficulty.%20Our%20framework%20comprises%20two%20stages%3A%20%281%29%20an%20Adaptive%20Supervised%20Fine-Tuning%20%28Adaptive%20SFT%29%20stage%2C%20which%20endows%20the%20Omni%20model%20with%20fundamental%20reasoning%20capability%20using%20large-scale%20reasoning-augmented%20data%2C%20and%20%282%29%20an%20Adaptive%20Reinforcement%20Learning%20%28Adaptive%20GRPO%29%20stage%2C%20which%20optimizes%20reasoning%20behaviors%20based%20on%20task%20complexity%20and%20reward%20feedback.%20We%20further%20construct%20a%20comprehensive%20adaptive%20reasoning%20benchmark%20that%20spans%20text-only%2C%20text-audio%2C%20text-visual%2C%20and%20text-audio-visual%20modalities%2C%20providing%20both%20training%20and%20evaluation%20splits%20for%20multimodal%20reasoning%20assessment.%20Experimental%20results%20demonstrate%20that%20our%20proposed%20framework%20significantly%20improves%20adaptive%20reasoning%20performance%20compared%20to%20previous%20baselines.%20All%20benchmark%20data%20and%20code%20will%20be%20publicly%20released.&entry.1838667208=http%3A//arxiv.org/abs/2512.03783v1&entry.124074799=Read"},
{"title": "ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration", "author": "Sundas Rafat Mulkana and Ronyu Yu and Tanaya Guha and Emma Li", "abstract": "In collaborative human-robot tasks, safety requires not only avoiding collisions but also ensuring safe, intentional physical contact. We present ContactRL, a reinforcement learning (RL) based framework that directly incorporates contact safety into the reward function through force feedback. This enables a robot to learn adaptive motion profiles that minimize human-robot contact forces while maintaining task efficiency. In simulation, ContactRL achieves a low safety violation rate of 0.2\\% with a high task success rate of 87.7\\%, outperforming state-of-the-art constrained RL baselines. In order to guarantee deployment safety, we augment the learned policy with a kinetic energy based Control Barrier Function (eCBF) shield. Real-world experiments on an UR3e robotic platform performing small object handovers from a human hand across 360 trials confirm safe contact, with measured normal forces consistently below 10N. These results demonstrate that ContactRL enables safe and efficient physical collaboration, thereby advancing the deployment of collaborative robots in contact-rich tasks.", "link": "http://arxiv.org/abs/2512.03707v1", "date": "2025-12-03", "relevancy": 2.1125, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.538}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5326}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContactRL%3A%20Safe%20Reinforcement%20Learning%20based%20Motion%20Planning%20for%20Contact%20based%20Human%20Robot%20Collaboration&body=Title%3A%20ContactRL%3A%20Safe%20Reinforcement%20Learning%20based%20Motion%20Planning%20for%20Contact%20based%20Human%20Robot%20Collaboration%0AAuthor%3A%20Sundas%20Rafat%20Mulkana%20and%20Ronyu%20Yu%20and%20Tanaya%20Guha%20and%20Emma%20Li%0AAbstract%3A%20In%20collaborative%20human-robot%20tasks%2C%20safety%20requires%20not%20only%20avoiding%20collisions%20but%20also%20ensuring%20safe%2C%20intentional%20physical%20contact.%20We%20present%20ContactRL%2C%20a%20reinforcement%20learning%20%28RL%29%20based%20framework%20that%20directly%20incorporates%20contact%20safety%20into%20the%20reward%20function%20through%20force%20feedback.%20This%20enables%20a%20robot%20to%20learn%20adaptive%20motion%20profiles%20that%20minimize%20human-robot%20contact%20forces%20while%20maintaining%20task%20efficiency.%20In%20simulation%2C%20ContactRL%20achieves%20a%20low%20safety%20violation%20rate%20of%200.2%5C%25%20with%20a%20high%20task%20success%20rate%20of%2087.7%5C%25%2C%20outperforming%20state-of-the-art%20constrained%20RL%20baselines.%20In%20order%20to%20guarantee%20deployment%20safety%2C%20we%20augment%20the%20learned%20policy%20with%20a%20kinetic%20energy%20based%20Control%20Barrier%20Function%20%28eCBF%29%20shield.%20Real-world%20experiments%20on%20an%20UR3e%20robotic%20platform%20performing%20small%20object%20handovers%20from%20a%20human%20hand%20across%20360%20trials%20confirm%20safe%20contact%2C%20with%20measured%20normal%20forces%20consistently%20below%2010N.%20These%20results%20demonstrate%20that%20ContactRL%20enables%20safe%20and%20efficient%20physical%20collaboration%2C%20thereby%20advancing%20the%20deployment%20of%20collaborative%20robots%20in%20contact-rich%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContactRL%253A%2520Safe%2520Reinforcement%2520Learning%2520based%2520Motion%2520Planning%2520for%2520Contact%2520based%2520Human%2520Robot%2520Collaboration%26entry.906535625%3DSundas%2520Rafat%2520Mulkana%2520and%2520Ronyu%2520Yu%2520and%2520Tanaya%2520Guha%2520and%2520Emma%2520Li%26entry.1292438233%3DIn%2520collaborative%2520human-robot%2520tasks%252C%2520safety%2520requires%2520not%2520only%2520avoiding%2520collisions%2520but%2520also%2520ensuring%2520safe%252C%2520intentional%2520physical%2520contact.%2520We%2520present%2520ContactRL%252C%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520based%2520framework%2520that%2520directly%2520incorporates%2520contact%2520safety%2520into%2520the%2520reward%2520function%2520through%2520force%2520feedback.%2520This%2520enables%2520a%2520robot%2520to%2520learn%2520adaptive%2520motion%2520profiles%2520that%2520minimize%2520human-robot%2520contact%2520forces%2520while%2520maintaining%2520task%2520efficiency.%2520In%2520simulation%252C%2520ContactRL%2520achieves%2520a%2520low%2520safety%2520violation%2520rate%2520of%25200.2%255C%2525%2520with%2520a%2520high%2520task%2520success%2520rate%2520of%252087.7%255C%2525%252C%2520outperforming%2520state-of-the-art%2520constrained%2520RL%2520baselines.%2520In%2520order%2520to%2520guarantee%2520deployment%2520safety%252C%2520we%2520augment%2520the%2520learned%2520policy%2520with%2520a%2520kinetic%2520energy%2520based%2520Control%2520Barrier%2520Function%2520%2528eCBF%2529%2520shield.%2520Real-world%2520experiments%2520on%2520an%2520UR3e%2520robotic%2520platform%2520performing%2520small%2520object%2520handovers%2520from%2520a%2520human%2520hand%2520across%2520360%2520trials%2520confirm%2520safe%2520contact%252C%2520with%2520measured%2520normal%2520forces%2520consistently%2520below%252010N.%2520These%2520results%2520demonstrate%2520that%2520ContactRL%2520enables%2520safe%2520and%2520efficient%2520physical%2520collaboration%252C%2520thereby%2520advancing%2520the%2520deployment%2520of%2520collaborative%2520robots%2520in%2520contact-rich%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContactRL%3A%20Safe%20Reinforcement%20Learning%20based%20Motion%20Planning%20for%20Contact%20based%20Human%20Robot%20Collaboration&entry.906535625=Sundas%20Rafat%20Mulkana%20and%20Ronyu%20Yu%20and%20Tanaya%20Guha%20and%20Emma%20Li&entry.1292438233=In%20collaborative%20human-robot%20tasks%2C%20safety%20requires%20not%20only%20avoiding%20collisions%20but%20also%20ensuring%20safe%2C%20intentional%20physical%20contact.%20We%20present%20ContactRL%2C%20a%20reinforcement%20learning%20%28RL%29%20based%20framework%20that%20directly%20incorporates%20contact%20safety%20into%20the%20reward%20function%20through%20force%20feedback.%20This%20enables%20a%20robot%20to%20learn%20adaptive%20motion%20profiles%20that%20minimize%20human-robot%20contact%20forces%20while%20maintaining%20task%20efficiency.%20In%20simulation%2C%20ContactRL%20achieves%20a%20low%20safety%20violation%20rate%20of%200.2%5C%25%20with%20a%20high%20task%20success%20rate%20of%2087.7%5C%25%2C%20outperforming%20state-of-the-art%20constrained%20RL%20baselines.%20In%20order%20to%20guarantee%20deployment%20safety%2C%20we%20augment%20the%20learned%20policy%20with%20a%20kinetic%20energy%20based%20Control%20Barrier%20Function%20%28eCBF%29%20shield.%20Real-world%20experiments%20on%20an%20UR3e%20robotic%20platform%20performing%20small%20object%20handovers%20from%20a%20human%20hand%20across%20360%20trials%20confirm%20safe%20contact%2C%20with%20measured%20normal%20forces%20consistently%20below%2010N.%20These%20results%20demonstrate%20that%20ContactRL%20enables%20safe%20and%20efficient%20physical%20collaboration%2C%20thereby%20advancing%20the%20deployment%20of%20collaborative%20robots%20in%20contact-rich%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.03707v1&entry.124074799=Read"},
{"title": "Multi-Scale Visual Prompting for Lightweight Small-Image Classification", "author": "Salim Khazem", "abstract": "Visual prompting has recently emerged as an efficient strategy to adapt vision models using lightweight, learnable parameters injected into the input space. However, prior work mainly targets large Vision Transformers and high-resolution datasets such as ImageNet. In contrast, small-image benchmarks like MNIST, Fashion-MNIST, and CIFAR-10 remain widely used in education, prototyping, and research, yet have received little attention in the context of prompting. In this paper, we introduce \\textbf{Multi-Scale Visual Prompting (MSVP)}, a simple and generic module that learns a set of global, mid-scale, and local prompt maps fused with the input image via a lightweight $1 \\times 1$ convolution. MSVP is backbone-agnostic, adds less than $0.02\\%$ parameters, and significantly improves performance across CNN and Vision Transformer backbones.\n  We provide a unified benchmark on MNIST, Fashion-MNIST, and CIFAR-10 using a simple CNN, ResNet-18, and a small Vision Transformer. Our method yields consistent improvements with negligible computational overhead. We further include ablations on prompt scales, fusion strategies, and backbone architectures, along with qualitative analyzes using prompt visualizations and Grad-CAM. Our results demonstrate that multi-scale prompting provides an effective inductive bias even on low-resolution images.", "link": "http://arxiv.org/abs/2512.03663v1", "date": "2025-12-03", "relevancy": 2.1061, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5398}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5254}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Visual%20Prompting%20for%20Lightweight%20Small-Image%20Classification&body=Title%3A%20Multi-Scale%20Visual%20Prompting%20for%20Lightweight%20Small-Image%20Classification%0AAuthor%3A%20Salim%20Khazem%0AAbstract%3A%20Visual%20prompting%20has%20recently%20emerged%20as%20an%20efficient%20strategy%20to%20adapt%20vision%20models%20using%20lightweight%2C%20learnable%20parameters%20injected%20into%20the%20input%20space.%20However%2C%20prior%20work%20mainly%20targets%20large%20Vision%20Transformers%20and%20high-resolution%20datasets%20such%20as%20ImageNet.%20In%20contrast%2C%20small-image%20benchmarks%20like%20MNIST%2C%20Fashion-MNIST%2C%20and%20CIFAR-10%20remain%20widely%20used%20in%20education%2C%20prototyping%2C%20and%20research%2C%20yet%20have%20received%20little%20attention%20in%20the%20context%20of%20prompting.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BMulti-Scale%20Visual%20Prompting%20%28MSVP%29%7D%2C%20a%20simple%20and%20generic%20module%20that%20learns%20a%20set%20of%20global%2C%20mid-scale%2C%20and%20local%20prompt%20maps%20fused%20with%20the%20input%20image%20via%20a%20lightweight%20%241%20%5Ctimes%201%24%20convolution.%20MSVP%20is%20backbone-agnostic%2C%20adds%20less%20than%20%240.02%5C%25%24%20parameters%2C%20and%20significantly%20improves%20performance%20across%20CNN%20and%20Vision%20Transformer%20backbones.%0A%20%20We%20provide%20a%20unified%20benchmark%20on%20MNIST%2C%20Fashion-MNIST%2C%20and%20CIFAR-10%20using%20a%20simple%20CNN%2C%20ResNet-18%2C%20and%20a%20small%20Vision%20Transformer.%20Our%20method%20yields%20consistent%20improvements%20with%20negligible%20computational%20overhead.%20We%20further%20include%20ablations%20on%20prompt%20scales%2C%20fusion%20strategies%2C%20and%20backbone%20architectures%2C%20along%20with%20qualitative%20analyzes%20using%20prompt%20visualizations%20and%20Grad-CAM.%20Our%20results%20demonstrate%20that%20multi-scale%20prompting%20provides%20an%20effective%20inductive%20bias%20even%20on%20low-resolution%20images.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520Visual%2520Prompting%2520for%2520Lightweight%2520Small-Image%2520Classification%26entry.906535625%3DSalim%2520Khazem%26entry.1292438233%3DVisual%2520prompting%2520has%2520recently%2520emerged%2520as%2520an%2520efficient%2520strategy%2520to%2520adapt%2520vision%2520models%2520using%2520lightweight%252C%2520learnable%2520parameters%2520injected%2520into%2520the%2520input%2520space.%2520However%252C%2520prior%2520work%2520mainly%2520targets%2520large%2520Vision%2520Transformers%2520and%2520high-resolution%2520datasets%2520such%2520as%2520ImageNet.%2520In%2520contrast%252C%2520small-image%2520benchmarks%2520like%2520MNIST%252C%2520Fashion-MNIST%252C%2520and%2520CIFAR-10%2520remain%2520widely%2520used%2520in%2520education%252C%2520prototyping%252C%2520and%2520research%252C%2520yet%2520have%2520received%2520little%2520attention%2520in%2520the%2520context%2520of%2520prompting.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520%255Ctextbf%257BMulti-Scale%2520Visual%2520Prompting%2520%2528MSVP%2529%257D%252C%2520a%2520simple%2520and%2520generic%2520module%2520that%2520learns%2520a%2520set%2520of%2520global%252C%2520mid-scale%252C%2520and%2520local%2520prompt%2520maps%2520fused%2520with%2520the%2520input%2520image%2520via%2520a%2520lightweight%2520%25241%2520%255Ctimes%25201%2524%2520convolution.%2520MSVP%2520is%2520backbone-agnostic%252C%2520adds%2520less%2520than%2520%25240.02%255C%2525%2524%2520parameters%252C%2520and%2520significantly%2520improves%2520performance%2520across%2520CNN%2520and%2520Vision%2520Transformer%2520backbones.%250A%2520%2520We%2520provide%2520a%2520unified%2520benchmark%2520on%2520MNIST%252C%2520Fashion-MNIST%252C%2520and%2520CIFAR-10%2520using%2520a%2520simple%2520CNN%252C%2520ResNet-18%252C%2520and%2520a%2520small%2520Vision%2520Transformer.%2520Our%2520method%2520yields%2520consistent%2520improvements%2520with%2520negligible%2520computational%2520overhead.%2520We%2520further%2520include%2520ablations%2520on%2520prompt%2520scales%252C%2520fusion%2520strategies%252C%2520and%2520backbone%2520architectures%252C%2520along%2520with%2520qualitative%2520analyzes%2520using%2520prompt%2520visualizations%2520and%2520Grad-CAM.%2520Our%2520results%2520demonstrate%2520that%2520multi-scale%2520prompting%2520provides%2520an%2520effective%2520inductive%2520bias%2520even%2520on%2520low-resolution%2520images.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Visual%20Prompting%20for%20Lightweight%20Small-Image%20Classification&entry.906535625=Salim%20Khazem&entry.1292438233=Visual%20prompting%20has%20recently%20emerged%20as%20an%20efficient%20strategy%20to%20adapt%20vision%20models%20using%20lightweight%2C%20learnable%20parameters%20injected%20into%20the%20input%20space.%20However%2C%20prior%20work%20mainly%20targets%20large%20Vision%20Transformers%20and%20high-resolution%20datasets%20such%20as%20ImageNet.%20In%20contrast%2C%20small-image%20benchmarks%20like%20MNIST%2C%20Fashion-MNIST%2C%20and%20CIFAR-10%20remain%20widely%20used%20in%20education%2C%20prototyping%2C%20and%20research%2C%20yet%20have%20received%20little%20attention%20in%20the%20context%20of%20prompting.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BMulti-Scale%20Visual%20Prompting%20%28MSVP%29%7D%2C%20a%20simple%20and%20generic%20module%20that%20learns%20a%20set%20of%20global%2C%20mid-scale%2C%20and%20local%20prompt%20maps%20fused%20with%20the%20input%20image%20via%20a%20lightweight%20%241%20%5Ctimes%201%24%20convolution.%20MSVP%20is%20backbone-agnostic%2C%20adds%20less%20than%20%240.02%5C%25%24%20parameters%2C%20and%20significantly%20improves%20performance%20across%20CNN%20and%20Vision%20Transformer%20backbones.%0A%20%20We%20provide%20a%20unified%20benchmark%20on%20MNIST%2C%20Fashion-MNIST%2C%20and%20CIFAR-10%20using%20a%20simple%20CNN%2C%20ResNet-18%2C%20and%20a%20small%20Vision%20Transformer.%20Our%20method%20yields%20consistent%20improvements%20with%20negligible%20computational%20overhead.%20We%20further%20include%20ablations%20on%20prompt%20scales%2C%20fusion%20strategies%2C%20and%20backbone%20architectures%2C%20along%20with%20qualitative%20analyzes%20using%20prompt%20visualizations%20and%20Grad-CAM.%20Our%20results%20demonstrate%20that%20multi-scale%20prompting%20provides%20an%20effective%20inductive%20bias%20even%20on%20low-resolution%20images.&entry.1838667208=http%3A//arxiv.org/abs/2512.03663v1&entry.124074799=Read"},
{"title": "Technical Report on Text Dataset Distillation", "author": "Keith Ando Ogawa and Bruno Lopes Yamamoto and Lucas Lauton de Alcantara and Victor Zacarias and Edson Bollis and Lucas Pellicer and Rosimeire Pereira Costa and Anna Helena Reali Costa and Artur Jordao", "abstract": "In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.", "link": "http://arxiv.org/abs/2512.03967v1", "date": "2025-12-03", "relevancy": 2.0999, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.585}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5133}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Technical%20Report%20on%20Text%20Dataset%20Distillation&body=Title%3A%20Technical%20Report%20on%20Text%20Dataset%20Distillation%0AAuthor%3A%20Keith%20Ando%20Ogawa%20and%20Bruno%20Lopes%20Yamamoto%20and%20Lucas%20Lauton%20de%20Alcantara%20and%20Victor%20Zacarias%20and%20Edson%20Bollis%20and%20Lucas%20Pellicer%20and%20Rosimeire%20Pereira%20Costa%20and%20Anna%20Helena%20Reali%20Costa%20and%20Artur%20Jordao%0AAbstract%3A%20In%20the%20vision%20domain%2C%20dataset%20distillation%20arises%20as%20a%20technique%20to%20condense%20a%20large%20dataset%20into%20a%20smaller%20synthetic%20one%20that%20exhibits%20a%20similar%20result%20in%20the%20training%20process.%20While%20image%20data%20presents%20an%20extensive%20literature%20of%20distillation%20methods%2C%20text%20dataset%20distillation%20has%20fewer%20works%20in%20comparison.%20Text%20dataset%20distillation%20initially%20grew%20as%20an%20adaptation%20of%20efforts%20from%20the%20vision%20universe%2C%20as%20the%20particularities%20of%20the%20modality%20became%20clear%20obstacles%2C%20it%20rose%20into%20a%20separate%20branch%20of%20research.%20Several%20milestones%20mark%20the%20development%20of%20this%20area%2C%20such%20as%20the%20introduction%20of%20methods%20that%20use%20transformer%20models%2C%20the%20generation%20of%20discrete%20synthetic%20text%2C%20and%20the%20scaling%20to%20decoder-only%20models%20with%20over%201B%20parameters.%20Despite%20major%20advances%20in%20modern%20approaches%2C%20the%20field%20remains%20in%20a%20maturing%20phase%2C%20with%20room%20for%20improvement%20on%20benchmarking%20standardization%2C%20approaches%20to%20overcome%20the%20discrete%20nature%20of%20text%2C%20handling%20complex%20tasks%2C%20and%20providing%20explicit%20examples%20of%20real-world%20applications.%20In%20this%20report%2C%20we%20review%20past%20and%20recent%20advances%20in%20dataset%20distillation%20for%20text%2C%20highlighting%20different%20distillation%20strategies%2C%20key%20contributions%2C%20and%20general%20challenges.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTechnical%2520Report%2520on%2520Text%2520Dataset%2520Distillation%26entry.906535625%3DKeith%2520Ando%2520Ogawa%2520and%2520Bruno%2520Lopes%2520Yamamoto%2520and%2520Lucas%2520Lauton%2520de%2520Alcantara%2520and%2520Victor%2520Zacarias%2520and%2520Edson%2520Bollis%2520and%2520Lucas%2520Pellicer%2520and%2520Rosimeire%2520Pereira%2520Costa%2520and%2520Anna%2520Helena%2520Reali%2520Costa%2520and%2520Artur%2520Jordao%26entry.1292438233%3DIn%2520the%2520vision%2520domain%252C%2520dataset%2520distillation%2520arises%2520as%2520a%2520technique%2520to%2520condense%2520a%2520large%2520dataset%2520into%2520a%2520smaller%2520synthetic%2520one%2520that%2520exhibits%2520a%2520similar%2520result%2520in%2520the%2520training%2520process.%2520While%2520image%2520data%2520presents%2520an%2520extensive%2520literature%2520of%2520distillation%2520methods%252C%2520text%2520dataset%2520distillation%2520has%2520fewer%2520works%2520in%2520comparison.%2520Text%2520dataset%2520distillation%2520initially%2520grew%2520as%2520an%2520adaptation%2520of%2520efforts%2520from%2520the%2520vision%2520universe%252C%2520as%2520the%2520particularities%2520of%2520the%2520modality%2520became%2520clear%2520obstacles%252C%2520it%2520rose%2520into%2520a%2520separate%2520branch%2520of%2520research.%2520Several%2520milestones%2520mark%2520the%2520development%2520of%2520this%2520area%252C%2520such%2520as%2520the%2520introduction%2520of%2520methods%2520that%2520use%2520transformer%2520models%252C%2520the%2520generation%2520of%2520discrete%2520synthetic%2520text%252C%2520and%2520the%2520scaling%2520to%2520decoder-only%2520models%2520with%2520over%25201B%2520parameters.%2520Despite%2520major%2520advances%2520in%2520modern%2520approaches%252C%2520the%2520field%2520remains%2520in%2520a%2520maturing%2520phase%252C%2520with%2520room%2520for%2520improvement%2520on%2520benchmarking%2520standardization%252C%2520approaches%2520to%2520overcome%2520the%2520discrete%2520nature%2520of%2520text%252C%2520handling%2520complex%2520tasks%252C%2520and%2520providing%2520explicit%2520examples%2520of%2520real-world%2520applications.%2520In%2520this%2520report%252C%2520we%2520review%2520past%2520and%2520recent%2520advances%2520in%2520dataset%2520distillation%2520for%2520text%252C%2520highlighting%2520different%2520distillation%2520strategies%252C%2520key%2520contributions%252C%2520and%2520general%2520challenges.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Technical%20Report%20on%20Text%20Dataset%20Distillation&entry.906535625=Keith%20Ando%20Ogawa%20and%20Bruno%20Lopes%20Yamamoto%20and%20Lucas%20Lauton%20de%20Alcantara%20and%20Victor%20Zacarias%20and%20Edson%20Bollis%20and%20Lucas%20Pellicer%20and%20Rosimeire%20Pereira%20Costa%20and%20Anna%20Helena%20Reali%20Costa%20and%20Artur%20Jordao&entry.1292438233=In%20the%20vision%20domain%2C%20dataset%20distillation%20arises%20as%20a%20technique%20to%20condense%20a%20large%20dataset%20into%20a%20smaller%20synthetic%20one%20that%20exhibits%20a%20similar%20result%20in%20the%20training%20process.%20While%20image%20data%20presents%20an%20extensive%20literature%20of%20distillation%20methods%2C%20text%20dataset%20distillation%20has%20fewer%20works%20in%20comparison.%20Text%20dataset%20distillation%20initially%20grew%20as%20an%20adaptation%20of%20efforts%20from%20the%20vision%20universe%2C%20as%20the%20particularities%20of%20the%20modality%20became%20clear%20obstacles%2C%20it%20rose%20into%20a%20separate%20branch%20of%20research.%20Several%20milestones%20mark%20the%20development%20of%20this%20area%2C%20such%20as%20the%20introduction%20of%20methods%20that%20use%20transformer%20models%2C%20the%20generation%20of%20discrete%20synthetic%20text%2C%20and%20the%20scaling%20to%20decoder-only%20models%20with%20over%201B%20parameters.%20Despite%20major%20advances%20in%20modern%20approaches%2C%20the%20field%20remains%20in%20a%20maturing%20phase%2C%20with%20room%20for%20improvement%20on%20benchmarking%20standardization%2C%20approaches%20to%20overcome%20the%20discrete%20nature%20of%20text%2C%20handling%20complex%20tasks%2C%20and%20providing%20explicit%20examples%20of%20real-world%20applications.%20In%20this%20report%2C%20we%20review%20past%20and%20recent%20advances%20in%20dataset%20distillation%20for%20text%2C%20highlighting%20different%20distillation%20strategies%2C%20key%20contributions%2C%20and%20general%20challenges.&entry.1838667208=http%3A//arxiv.org/abs/2512.03967v1&entry.124074799=Read"},
{"title": "Physics-Embedded Gaussian Process for Traffic State Estimation", "author": "Yanlin Chen and Kehua Chen and Yinhai Wang", "abstract": "Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE.", "link": "http://arxiv.org/abs/2512.04004v1", "date": "2025-12-03", "relevancy": 2.0909, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5406}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5277}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Embedded%20Gaussian%20Process%20for%20Traffic%20State%20Estimation&body=Title%3A%20Physics-Embedded%20Gaussian%20Process%20for%20Traffic%20State%20Estimation%0AAuthor%3A%20Yanlin%20Chen%20and%20Kehua%20Chen%20and%20Yinhai%20Wang%0AAbstract%3A%20Traffic%20state%20estimation%20%28TSE%29%20becomes%20challenging%20when%20probe-vehicle%20penetration%20is%20low%20and%20observations%20are%20spatially%20sparse.%20Pure%20data-driven%20methods%20lack%20physical%20explanations%20and%20have%20poor%20generalization%20when%20observed%20data%20is%20sparse.%20In%20contrast%2C%20physical%20models%20have%20difficulty%20integrating%20uncertainties%20and%20capturing%20the%20real%20complexity%20of%20traffic.%20To%20bridge%20this%20gap%2C%20recent%20studies%20have%20explored%20combining%20them%20by%20embedding%20physical%20structure%20into%20Gaussian%20process.%20These%20approaches%20typically%20introduce%20the%20governing%20equations%20as%20soft%20constraints%20through%20pseudo-observations%2C%20enabling%20the%20integration%20of%20model%20structure%20within%20a%20variational%20framework.%20However%2C%20these%20methods%20rely%20heavily%20on%20penalty%20tuning%20and%20lack%20principled%20uncertainty%20calibration%2C%20which%20makes%20them%20sensitive%20to%20model%20mis-specification.%20In%20this%20work%2C%20we%20address%20these%20limitations%20by%20presenting%20a%20novel%20Physics-Embedded%20Gaussian%20Process%20%28PEGP%29%2C%20designed%20to%20integrate%20domain%20knowledge%20with%20data-driven%20methods%20in%20traffic%20state%20estimation.%20Specifically%2C%20we%20design%20two%20multi-output%20kernels%20informed%20by%20classic%20traffic%20flow%20models%2C%20constructed%20via%20the%20explicit%20application%20of%20the%20linearized%20differential%20operator.%20Experiments%20on%20HighD%2C%20NGSIM%20show%20consistent%20improvements%20over%20non-physics%20baselines.%20PEGP-ARZ%20proves%20more%20reliable%20under%20sparse%20observation%2C%20while%20PEGP-LWR%20achieves%20lower%20errors%20with%20denser%20observation.%20Ablation%20study%20further%20reveals%20that%20PEGP-ARZ%20residuals%20align%20closely%20with%20physics%20and%20yield%20calibrated%2C%20interpretable%20uncertainty%2C%20whereas%20PEGP-LWR%20residuals%20are%20more%20orthogonal%20and%20produce%20nearly%20constant%20variance%20fields.%20This%20PEGP%20framework%20combines%20physical%20priors%2C%20uncertainty%20quantification%2C%20which%20can%20provide%20reliable%20support%20for%20TSE.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Embedded%2520Gaussian%2520Process%2520for%2520Traffic%2520State%2520Estimation%26entry.906535625%3DYanlin%2520Chen%2520and%2520Kehua%2520Chen%2520and%2520Yinhai%2520Wang%26entry.1292438233%3DTraffic%2520state%2520estimation%2520%2528TSE%2529%2520becomes%2520challenging%2520when%2520probe-vehicle%2520penetration%2520is%2520low%2520and%2520observations%2520are%2520spatially%2520sparse.%2520Pure%2520data-driven%2520methods%2520lack%2520physical%2520explanations%2520and%2520have%2520poor%2520generalization%2520when%2520observed%2520data%2520is%2520sparse.%2520In%2520contrast%252C%2520physical%2520models%2520have%2520difficulty%2520integrating%2520uncertainties%2520and%2520capturing%2520the%2520real%2520complexity%2520of%2520traffic.%2520To%2520bridge%2520this%2520gap%252C%2520recent%2520studies%2520have%2520explored%2520combining%2520them%2520by%2520embedding%2520physical%2520structure%2520into%2520Gaussian%2520process.%2520These%2520approaches%2520typically%2520introduce%2520the%2520governing%2520equations%2520as%2520soft%2520constraints%2520through%2520pseudo-observations%252C%2520enabling%2520the%2520integration%2520of%2520model%2520structure%2520within%2520a%2520variational%2520framework.%2520However%252C%2520these%2520methods%2520rely%2520heavily%2520on%2520penalty%2520tuning%2520and%2520lack%2520principled%2520uncertainty%2520calibration%252C%2520which%2520makes%2520them%2520sensitive%2520to%2520model%2520mis-specification.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520limitations%2520by%2520presenting%2520a%2520novel%2520Physics-Embedded%2520Gaussian%2520Process%2520%2528PEGP%2529%252C%2520designed%2520to%2520integrate%2520domain%2520knowledge%2520with%2520data-driven%2520methods%2520in%2520traffic%2520state%2520estimation.%2520Specifically%252C%2520we%2520design%2520two%2520multi-output%2520kernels%2520informed%2520by%2520classic%2520traffic%2520flow%2520models%252C%2520constructed%2520via%2520the%2520explicit%2520application%2520of%2520the%2520linearized%2520differential%2520operator.%2520Experiments%2520on%2520HighD%252C%2520NGSIM%2520show%2520consistent%2520improvements%2520over%2520non-physics%2520baselines.%2520PEGP-ARZ%2520proves%2520more%2520reliable%2520under%2520sparse%2520observation%252C%2520while%2520PEGP-LWR%2520achieves%2520lower%2520errors%2520with%2520denser%2520observation.%2520Ablation%2520study%2520further%2520reveals%2520that%2520PEGP-ARZ%2520residuals%2520align%2520closely%2520with%2520physics%2520and%2520yield%2520calibrated%252C%2520interpretable%2520uncertainty%252C%2520whereas%2520PEGP-LWR%2520residuals%2520are%2520more%2520orthogonal%2520and%2520produce%2520nearly%2520constant%2520variance%2520fields.%2520This%2520PEGP%2520framework%2520combines%2520physical%2520priors%252C%2520uncertainty%2520quantification%252C%2520which%2520can%2520provide%2520reliable%2520support%2520for%2520TSE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Embedded%20Gaussian%20Process%20for%20Traffic%20State%20Estimation&entry.906535625=Yanlin%20Chen%20and%20Kehua%20Chen%20and%20Yinhai%20Wang&entry.1292438233=Traffic%20state%20estimation%20%28TSE%29%20becomes%20challenging%20when%20probe-vehicle%20penetration%20is%20low%20and%20observations%20are%20spatially%20sparse.%20Pure%20data-driven%20methods%20lack%20physical%20explanations%20and%20have%20poor%20generalization%20when%20observed%20data%20is%20sparse.%20In%20contrast%2C%20physical%20models%20have%20difficulty%20integrating%20uncertainties%20and%20capturing%20the%20real%20complexity%20of%20traffic.%20To%20bridge%20this%20gap%2C%20recent%20studies%20have%20explored%20combining%20them%20by%20embedding%20physical%20structure%20into%20Gaussian%20process.%20These%20approaches%20typically%20introduce%20the%20governing%20equations%20as%20soft%20constraints%20through%20pseudo-observations%2C%20enabling%20the%20integration%20of%20model%20structure%20within%20a%20variational%20framework.%20However%2C%20these%20methods%20rely%20heavily%20on%20penalty%20tuning%20and%20lack%20principled%20uncertainty%20calibration%2C%20which%20makes%20them%20sensitive%20to%20model%20mis-specification.%20In%20this%20work%2C%20we%20address%20these%20limitations%20by%20presenting%20a%20novel%20Physics-Embedded%20Gaussian%20Process%20%28PEGP%29%2C%20designed%20to%20integrate%20domain%20knowledge%20with%20data-driven%20methods%20in%20traffic%20state%20estimation.%20Specifically%2C%20we%20design%20two%20multi-output%20kernels%20informed%20by%20classic%20traffic%20flow%20models%2C%20constructed%20via%20the%20explicit%20application%20of%20the%20linearized%20differential%20operator.%20Experiments%20on%20HighD%2C%20NGSIM%20show%20consistent%20improvements%20over%20non-physics%20baselines.%20PEGP-ARZ%20proves%20more%20reliable%20under%20sparse%20observation%2C%20while%20PEGP-LWR%20achieves%20lower%20errors%20with%20denser%20observation.%20Ablation%20study%20further%20reveals%20that%20PEGP-ARZ%20residuals%20align%20closely%20with%20physics%20and%20yield%20calibrated%2C%20interpretable%20uncertainty%2C%20whereas%20PEGP-LWR%20residuals%20are%20more%20orthogonal%20and%20produce%20nearly%20constant%20variance%20fields.%20This%20PEGP%20framework%20combines%20physical%20priors%2C%20uncertainty%20quantification%2C%20which%20can%20provide%20reliable%20support%20for%20TSE.&entry.1838667208=http%3A//arxiv.org/abs/2512.04004v1&entry.124074799=Read"},
{"title": "Exploring the Potentials of Spiking Neural Networks for Image Deraining", "author": "Shuang Chen and Tomas Krajnik and Farshad Arvin and Amir Atapour-Abarghouei", "abstract": "Biologically plausible and energy-efficient frameworks such as Spiking Neural Networks (SNNs) have not been sufficiently explored in low-level vision tasks. Taking image deraining as an example, this study addresses the representation of the inherent high-pass characteristics of spiking neurons, specifically in image deraining and innovatively proposes the Visual LIF (VLIF) neuron, overcoming the obstacle of lacking spatial contextual understanding present in traditional spiking neurons. To tackle the limitation of frequency-domain saturation inherent in conventional spiking neurons, we leverage the proposed VLIF to introduce the Spiking Decomposition and Enhancement Module and the lightweight Spiking Multi-scale Unit for hierarchical multi-scale representation learning. Extensive experiments across five benchmark deraining datasets demonstrate that our approach significantly outperforms state-of-the-art SNN-based deraining methods, achieving this superior performance with only 13\\% of their energy consumption. These findings establish a solid foundation for deploying SNNs in high-performance, energy-efficient low-level vision tasks.", "link": "http://arxiv.org/abs/2512.02258v2", "date": "2025-12-03", "relevancy": 2.0909, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.543}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5297}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Potentials%20of%20Spiking%20Neural%20Networks%20for%20Image%20Deraining&body=Title%3A%20Exploring%20the%20Potentials%20of%20Spiking%20Neural%20Networks%20for%20Image%20Deraining%0AAuthor%3A%20Shuang%20Chen%20and%20Tomas%20Krajnik%20and%20Farshad%20Arvin%20and%20Amir%20Atapour-Abarghouei%0AAbstract%3A%20Biologically%20plausible%20and%20energy-efficient%20frameworks%20such%20as%20Spiking%20Neural%20Networks%20%28SNNs%29%20have%20not%20been%20sufficiently%20explored%20in%20low-level%20vision%20tasks.%20Taking%20image%20deraining%20as%20an%20example%2C%20this%20study%20addresses%20the%20representation%20of%20the%20inherent%20high-pass%20characteristics%20of%20spiking%20neurons%2C%20specifically%20in%20image%20deraining%20and%20innovatively%20proposes%20the%20Visual%20LIF%20%28VLIF%29%20neuron%2C%20overcoming%20the%20obstacle%20of%20lacking%20spatial%20contextual%20understanding%20present%20in%20traditional%20spiking%20neurons.%20To%20tackle%20the%20limitation%20of%20frequency-domain%20saturation%20inherent%20in%20conventional%20spiking%20neurons%2C%20we%20leverage%20the%20proposed%20VLIF%20to%20introduce%20the%20Spiking%20Decomposition%20and%20Enhancement%20Module%20and%20the%20lightweight%20Spiking%20Multi-scale%20Unit%20for%20hierarchical%20multi-scale%20representation%20learning.%20Extensive%20experiments%20across%20five%20benchmark%20deraining%20datasets%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20state-of-the-art%20SNN-based%20deraining%20methods%2C%20achieving%20this%20superior%20performance%20with%20only%2013%5C%25%20of%20their%20energy%20consumption.%20These%20findings%20establish%20a%20solid%20foundation%20for%20deploying%20SNNs%20in%20high-performance%2C%20energy-efficient%20low-level%20vision%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02258v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Potentials%2520of%2520Spiking%2520Neural%2520Networks%2520for%2520Image%2520Deraining%26entry.906535625%3DShuang%2520Chen%2520and%2520Tomas%2520Krajnik%2520and%2520Farshad%2520Arvin%2520and%2520Amir%2520Atapour-Abarghouei%26entry.1292438233%3DBiologically%2520plausible%2520and%2520energy-efficient%2520frameworks%2520such%2520as%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520have%2520not%2520been%2520sufficiently%2520explored%2520in%2520low-level%2520vision%2520tasks.%2520Taking%2520image%2520deraining%2520as%2520an%2520example%252C%2520this%2520study%2520addresses%2520the%2520representation%2520of%2520the%2520inherent%2520high-pass%2520characteristics%2520of%2520spiking%2520neurons%252C%2520specifically%2520in%2520image%2520deraining%2520and%2520innovatively%2520proposes%2520the%2520Visual%2520LIF%2520%2528VLIF%2529%2520neuron%252C%2520overcoming%2520the%2520obstacle%2520of%2520lacking%2520spatial%2520contextual%2520understanding%2520present%2520in%2520traditional%2520spiking%2520neurons.%2520To%2520tackle%2520the%2520limitation%2520of%2520frequency-domain%2520saturation%2520inherent%2520in%2520conventional%2520spiking%2520neurons%252C%2520we%2520leverage%2520the%2520proposed%2520VLIF%2520to%2520introduce%2520the%2520Spiking%2520Decomposition%2520and%2520Enhancement%2520Module%2520and%2520the%2520lightweight%2520Spiking%2520Multi-scale%2520Unit%2520for%2520hierarchical%2520multi-scale%2520representation%2520learning.%2520Extensive%2520experiments%2520across%2520five%2520benchmark%2520deraining%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520state-of-the-art%2520SNN-based%2520deraining%2520methods%252C%2520achieving%2520this%2520superior%2520performance%2520with%2520only%252013%255C%2525%2520of%2520their%2520energy%2520consumption.%2520These%2520findings%2520establish%2520a%2520solid%2520foundation%2520for%2520deploying%2520SNNs%2520in%2520high-performance%252C%2520energy-efficient%2520low-level%2520vision%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02258v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Potentials%20of%20Spiking%20Neural%20Networks%20for%20Image%20Deraining&entry.906535625=Shuang%20Chen%20and%20Tomas%20Krajnik%20and%20Farshad%20Arvin%20and%20Amir%20Atapour-Abarghouei&entry.1292438233=Biologically%20plausible%20and%20energy-efficient%20frameworks%20such%20as%20Spiking%20Neural%20Networks%20%28SNNs%29%20have%20not%20been%20sufficiently%20explored%20in%20low-level%20vision%20tasks.%20Taking%20image%20deraining%20as%20an%20example%2C%20this%20study%20addresses%20the%20representation%20of%20the%20inherent%20high-pass%20characteristics%20of%20spiking%20neurons%2C%20specifically%20in%20image%20deraining%20and%20innovatively%20proposes%20the%20Visual%20LIF%20%28VLIF%29%20neuron%2C%20overcoming%20the%20obstacle%20of%20lacking%20spatial%20contextual%20understanding%20present%20in%20traditional%20spiking%20neurons.%20To%20tackle%20the%20limitation%20of%20frequency-domain%20saturation%20inherent%20in%20conventional%20spiking%20neurons%2C%20we%20leverage%20the%20proposed%20VLIF%20to%20introduce%20the%20Spiking%20Decomposition%20and%20Enhancement%20Module%20and%20the%20lightweight%20Spiking%20Multi-scale%20Unit%20for%20hierarchical%20multi-scale%20representation%20learning.%20Extensive%20experiments%20across%20five%20benchmark%20deraining%20datasets%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20state-of-the-art%20SNN-based%20deraining%20methods%2C%20achieving%20this%20superior%20performance%20with%20only%2013%5C%25%20of%20their%20energy%20consumption.%20These%20findings%20establish%20a%20solid%20foundation%20for%20deploying%20SNNs%20in%20high-performance%2C%20energy-efficient%20low-level%20vision%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.02258v2&entry.124074799=Read"},
{"title": "Unintentional Consequences: Generative AI Use for Cybercrime", "author": "Truong Jack Luu and Binny M. Samuel", "abstract": "The democratization of generative AI introduces new forms of human-AI interaction and raises urgent safety, ethical, and cybersecurity concerns. We develop a socio-technical explanation for how generative AI enables and scales cybercrime. Drawing on affordance theory and technological amplification, we argue that generative AI systems create new action possibilities for cybercriminals and magnify pre-existing malicious intent by lowering expertise barriers and increasing attack efficiency. To illustrate this framework, we conduct interrupted time series analyses of two large datasets: (1) 464,190,074 malicious IP address reports from AbuseIPDB, and (2) 281,115 cryptocurrency scam reports from Chainabuse. Using November 30, 2022, as a high-salience public-access shock, we estimate the counterfactual trajectory of reported cyber abuse absent the release, providing an early-warning impact assessment of a general-purpose AI technology. Across both datasets, we observe statistically significant post-intervention increases in reported malicious activity, including an immediate increase of over 1.12 million weekly malicious IP reports and about 722 weekly cryptocurrency scam reports, with sustained growth in the latter. We discuss implications for AI governance, platform-level regulation, and cyber resilience, emphasizing the need for multi-layer socio-technical strategies that help key stakeholders maximize AI's benefits while mitigating its growing cybercrime risks.", "link": "http://arxiv.org/abs/2505.23733v2", "date": "2025-12-03", "relevancy": 2.0738, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5288}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5258}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unintentional%20Consequences%3A%20Generative%20AI%20Use%20for%20Cybercrime&body=Title%3A%20Unintentional%20Consequences%3A%20Generative%20AI%20Use%20for%20Cybercrime%0AAuthor%3A%20Truong%20Jack%20Luu%20and%20Binny%20M.%20Samuel%0AAbstract%3A%20The%20democratization%20of%20generative%20AI%20introduces%20new%20forms%20of%20human-AI%20interaction%20and%20raises%20urgent%20safety%2C%20ethical%2C%20and%20cybersecurity%20concerns.%20We%20develop%20a%20socio-technical%20explanation%20for%20how%20generative%20AI%20enables%20and%20scales%20cybercrime.%20Drawing%20on%20affordance%20theory%20and%20technological%20amplification%2C%20we%20argue%20that%20generative%20AI%20systems%20create%20new%20action%20possibilities%20for%20cybercriminals%20and%20magnify%20pre-existing%20malicious%20intent%20by%20lowering%20expertise%20barriers%20and%20increasing%20attack%20efficiency.%20To%20illustrate%20this%20framework%2C%20we%20conduct%20interrupted%20time%20series%20analyses%20of%20two%20large%20datasets%3A%20%281%29%20464%2C190%2C074%20malicious%20IP%20address%20reports%20from%20AbuseIPDB%2C%20and%20%282%29%20281%2C115%20cryptocurrency%20scam%20reports%20from%20Chainabuse.%20Using%20November%2030%2C%202022%2C%20as%20a%20high-salience%20public-access%20shock%2C%20we%20estimate%20the%20counterfactual%20trajectory%20of%20reported%20cyber%20abuse%20absent%20the%20release%2C%20providing%20an%20early-warning%20impact%20assessment%20of%20a%20general-purpose%20AI%20technology.%20Across%20both%20datasets%2C%20we%20observe%20statistically%20significant%20post-intervention%20increases%20in%20reported%20malicious%20activity%2C%20including%20an%20immediate%20increase%20of%20over%201.12%20million%20weekly%20malicious%20IP%20reports%20and%20about%20722%20weekly%20cryptocurrency%20scam%20reports%2C%20with%20sustained%20growth%20in%20the%20latter.%20We%20discuss%20implications%20for%20AI%20governance%2C%20platform-level%20regulation%2C%20and%20cyber%20resilience%2C%20emphasizing%20the%20need%20for%20multi-layer%20socio-technical%20strategies%20that%20help%20key%20stakeholders%20maximize%20AI%27s%20benefits%20while%20mitigating%20its%20growing%20cybercrime%20risks.%0ALink%3A%20http%3A//arxiv.org/abs/2505.23733v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnintentional%2520Consequences%253A%2520Generative%2520AI%2520Use%2520for%2520Cybercrime%26entry.906535625%3DTruong%2520Jack%2520Luu%2520and%2520Binny%2520M.%2520Samuel%26entry.1292438233%3DThe%2520democratization%2520of%2520generative%2520AI%2520introduces%2520new%2520forms%2520of%2520human-AI%2520interaction%2520and%2520raises%2520urgent%2520safety%252C%2520ethical%252C%2520and%2520cybersecurity%2520concerns.%2520We%2520develop%2520a%2520socio-technical%2520explanation%2520for%2520how%2520generative%2520AI%2520enables%2520and%2520scales%2520cybercrime.%2520Drawing%2520on%2520affordance%2520theory%2520and%2520technological%2520amplification%252C%2520we%2520argue%2520that%2520generative%2520AI%2520systems%2520create%2520new%2520action%2520possibilities%2520for%2520cybercriminals%2520and%2520magnify%2520pre-existing%2520malicious%2520intent%2520by%2520lowering%2520expertise%2520barriers%2520and%2520increasing%2520attack%2520efficiency.%2520To%2520illustrate%2520this%2520framework%252C%2520we%2520conduct%2520interrupted%2520time%2520series%2520analyses%2520of%2520two%2520large%2520datasets%253A%2520%25281%2529%2520464%252C190%252C074%2520malicious%2520IP%2520address%2520reports%2520from%2520AbuseIPDB%252C%2520and%2520%25282%2529%2520281%252C115%2520cryptocurrency%2520scam%2520reports%2520from%2520Chainabuse.%2520Using%2520November%252030%252C%25202022%252C%2520as%2520a%2520high-salience%2520public-access%2520shock%252C%2520we%2520estimate%2520the%2520counterfactual%2520trajectory%2520of%2520reported%2520cyber%2520abuse%2520absent%2520the%2520release%252C%2520providing%2520an%2520early-warning%2520impact%2520assessment%2520of%2520a%2520general-purpose%2520AI%2520technology.%2520Across%2520both%2520datasets%252C%2520we%2520observe%2520statistically%2520significant%2520post-intervention%2520increases%2520in%2520reported%2520malicious%2520activity%252C%2520including%2520an%2520immediate%2520increase%2520of%2520over%25201.12%2520million%2520weekly%2520malicious%2520IP%2520reports%2520and%2520about%2520722%2520weekly%2520cryptocurrency%2520scam%2520reports%252C%2520with%2520sustained%2520growth%2520in%2520the%2520latter.%2520We%2520discuss%2520implications%2520for%2520AI%2520governance%252C%2520platform-level%2520regulation%252C%2520and%2520cyber%2520resilience%252C%2520emphasizing%2520the%2520need%2520for%2520multi-layer%2520socio-technical%2520strategies%2520that%2520help%2520key%2520stakeholders%2520maximize%2520AI%2527s%2520benefits%2520while%2520mitigating%2520its%2520growing%2520cybercrime%2520risks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23733v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unintentional%20Consequences%3A%20Generative%20AI%20Use%20for%20Cybercrime&entry.906535625=Truong%20Jack%20Luu%20and%20Binny%20M.%20Samuel&entry.1292438233=The%20democratization%20of%20generative%20AI%20introduces%20new%20forms%20of%20human-AI%20interaction%20and%20raises%20urgent%20safety%2C%20ethical%2C%20and%20cybersecurity%20concerns.%20We%20develop%20a%20socio-technical%20explanation%20for%20how%20generative%20AI%20enables%20and%20scales%20cybercrime.%20Drawing%20on%20affordance%20theory%20and%20technological%20amplification%2C%20we%20argue%20that%20generative%20AI%20systems%20create%20new%20action%20possibilities%20for%20cybercriminals%20and%20magnify%20pre-existing%20malicious%20intent%20by%20lowering%20expertise%20barriers%20and%20increasing%20attack%20efficiency.%20To%20illustrate%20this%20framework%2C%20we%20conduct%20interrupted%20time%20series%20analyses%20of%20two%20large%20datasets%3A%20%281%29%20464%2C190%2C074%20malicious%20IP%20address%20reports%20from%20AbuseIPDB%2C%20and%20%282%29%20281%2C115%20cryptocurrency%20scam%20reports%20from%20Chainabuse.%20Using%20November%2030%2C%202022%2C%20as%20a%20high-salience%20public-access%20shock%2C%20we%20estimate%20the%20counterfactual%20trajectory%20of%20reported%20cyber%20abuse%20absent%20the%20release%2C%20providing%20an%20early-warning%20impact%20assessment%20of%20a%20general-purpose%20AI%20technology.%20Across%20both%20datasets%2C%20we%20observe%20statistically%20significant%20post-intervention%20increases%20in%20reported%20malicious%20activity%2C%20including%20an%20immediate%20increase%20of%20over%201.12%20million%20weekly%20malicious%20IP%20reports%20and%20about%20722%20weekly%20cryptocurrency%20scam%20reports%2C%20with%20sustained%20growth%20in%20the%20latter.%20We%20discuss%20implications%20for%20AI%20governance%2C%20platform-level%20regulation%2C%20and%20cyber%20resilience%2C%20emphasizing%20the%20need%20for%20multi-layer%20socio-technical%20strategies%20that%20help%20key%20stakeholders%20maximize%20AI%27s%20benefits%20while%20mitigating%20its%20growing%20cybercrime%20risks.&entry.1838667208=http%3A//arxiv.org/abs/2505.23733v2&entry.124074799=Read"},
{"title": "FeatureLens: A Highly Generalizable and Interpretable Framework for Detecting Adversarial Examples Based on Image Features", "author": "Zhigang Yang and Yuan Liu and Jiawei Zhang and Puning Zhang and Xinqiang Ma", "abstract": "Although the remarkable performance of deep neural networks (DNNs) in image classification, their vulnerability to adversarial attacks remains a critical challenge. Most existing detection methods rely on complex and poorly interpretable architectures, which compromise interpretability and generalization. To address this, we propose FeatureLens, a lightweight framework that acts as a lens to scrutinize anomalies in image features. Comprising an Image Feature Extractor (IFE) and shallow classifiers (e.g., SVM, MLP, or XGBoost) with model sizes ranging from 1,000 to 30,000 parameters, FeatureLens achieves high detection accuracy ranging from 97.8% to 99.75% in closed-set evaluation and 86.17% to 99.6% in generalization evaluation across FGSM, PGD, CW, and DAmageNet attacks, using only 51 dimensional features. By combining strong detection performance with excellent generalization, interpretability, and computational efficiency, FeatureLens offers a practical pathway toward transparent and effective adversarial defense.", "link": "http://arxiv.org/abs/2512.03625v1", "date": "2025-12-03", "relevancy": 2.0683, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5307}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5173}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FeatureLens%3A%20A%20Highly%20Generalizable%20and%20Interpretable%20Framework%20for%20Detecting%20Adversarial%20Examples%20Based%20on%20Image%20Features&body=Title%3A%20FeatureLens%3A%20A%20Highly%20Generalizable%20and%20Interpretable%20Framework%20for%20Detecting%20Adversarial%20Examples%20Based%20on%20Image%20Features%0AAuthor%3A%20Zhigang%20Yang%20and%20Yuan%20Liu%20and%20Jiawei%20Zhang%20and%20Puning%20Zhang%20and%20Xinqiang%20Ma%0AAbstract%3A%20Although%20the%20remarkable%20performance%20of%20deep%20neural%20networks%20%28DNNs%29%20in%20image%20classification%2C%20their%20vulnerability%20to%20adversarial%20attacks%20remains%20a%20critical%20challenge.%20Most%20existing%20detection%20methods%20rely%20on%20complex%20and%20poorly%20interpretable%20architectures%2C%20which%20compromise%20interpretability%20and%20generalization.%20To%20address%20this%2C%20we%20propose%20FeatureLens%2C%20a%20lightweight%20framework%20that%20acts%20as%20a%20lens%20to%20scrutinize%20anomalies%20in%20image%20features.%20Comprising%20an%20Image%20Feature%20Extractor%20%28IFE%29%20and%20shallow%20classifiers%20%28e.g.%2C%20SVM%2C%20MLP%2C%20or%20XGBoost%29%20with%20model%20sizes%20ranging%20from%201%2C000%20to%2030%2C000%20parameters%2C%20FeatureLens%20achieves%20high%20detection%20accuracy%20ranging%20from%2097.8%25%20to%2099.75%25%20in%20closed-set%20evaluation%20and%2086.17%25%20to%2099.6%25%20in%20generalization%20evaluation%20across%20FGSM%2C%20PGD%2C%20CW%2C%20and%20DAmageNet%20attacks%2C%20using%20only%2051%20dimensional%20features.%20By%20combining%20strong%20detection%20performance%20with%20excellent%20generalization%2C%20interpretability%2C%20and%20computational%20efficiency%2C%20FeatureLens%20offers%20a%20practical%20pathway%20toward%20transparent%20and%20effective%20adversarial%20defense.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeatureLens%253A%2520A%2520Highly%2520Generalizable%2520and%2520Interpretable%2520Framework%2520for%2520Detecting%2520Adversarial%2520Examples%2520Based%2520on%2520Image%2520Features%26entry.906535625%3DZhigang%2520Yang%2520and%2520Yuan%2520Liu%2520and%2520Jiawei%2520Zhang%2520and%2520Puning%2520Zhang%2520and%2520Xinqiang%2520Ma%26entry.1292438233%3DAlthough%2520the%2520remarkable%2520performance%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520in%2520image%2520classification%252C%2520their%2520vulnerability%2520to%2520adversarial%2520attacks%2520remains%2520a%2520critical%2520challenge.%2520Most%2520existing%2520detection%2520methods%2520rely%2520on%2520complex%2520and%2520poorly%2520interpretable%2520architectures%252C%2520which%2520compromise%2520interpretability%2520and%2520generalization.%2520To%2520address%2520this%252C%2520we%2520propose%2520FeatureLens%252C%2520a%2520lightweight%2520framework%2520that%2520acts%2520as%2520a%2520lens%2520to%2520scrutinize%2520anomalies%2520in%2520image%2520features.%2520Comprising%2520an%2520Image%2520Feature%2520Extractor%2520%2528IFE%2529%2520and%2520shallow%2520classifiers%2520%2528e.g.%252C%2520SVM%252C%2520MLP%252C%2520or%2520XGBoost%2529%2520with%2520model%2520sizes%2520ranging%2520from%25201%252C000%2520to%252030%252C000%2520parameters%252C%2520FeatureLens%2520achieves%2520high%2520detection%2520accuracy%2520ranging%2520from%252097.8%2525%2520to%252099.75%2525%2520in%2520closed-set%2520evaluation%2520and%252086.17%2525%2520to%252099.6%2525%2520in%2520generalization%2520evaluation%2520across%2520FGSM%252C%2520PGD%252C%2520CW%252C%2520and%2520DAmageNet%2520attacks%252C%2520using%2520only%252051%2520dimensional%2520features.%2520By%2520combining%2520strong%2520detection%2520performance%2520with%2520excellent%2520generalization%252C%2520interpretability%252C%2520and%2520computational%2520efficiency%252C%2520FeatureLens%2520offers%2520a%2520practical%2520pathway%2520toward%2520transparent%2520and%2520effective%2520adversarial%2520defense.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FeatureLens%3A%20A%20Highly%20Generalizable%20and%20Interpretable%20Framework%20for%20Detecting%20Adversarial%20Examples%20Based%20on%20Image%20Features&entry.906535625=Zhigang%20Yang%20and%20Yuan%20Liu%20and%20Jiawei%20Zhang%20and%20Puning%20Zhang%20and%20Xinqiang%20Ma&entry.1292438233=Although%20the%20remarkable%20performance%20of%20deep%20neural%20networks%20%28DNNs%29%20in%20image%20classification%2C%20their%20vulnerability%20to%20adversarial%20attacks%20remains%20a%20critical%20challenge.%20Most%20existing%20detection%20methods%20rely%20on%20complex%20and%20poorly%20interpretable%20architectures%2C%20which%20compromise%20interpretability%20and%20generalization.%20To%20address%20this%2C%20we%20propose%20FeatureLens%2C%20a%20lightweight%20framework%20that%20acts%20as%20a%20lens%20to%20scrutinize%20anomalies%20in%20image%20features.%20Comprising%20an%20Image%20Feature%20Extractor%20%28IFE%29%20and%20shallow%20classifiers%20%28e.g.%2C%20SVM%2C%20MLP%2C%20or%20XGBoost%29%20with%20model%20sizes%20ranging%20from%201%2C000%20to%2030%2C000%20parameters%2C%20FeatureLens%20achieves%20high%20detection%20accuracy%20ranging%20from%2097.8%25%20to%2099.75%25%20in%20closed-set%20evaluation%20and%2086.17%25%20to%2099.6%25%20in%20generalization%20evaluation%20across%20FGSM%2C%20PGD%2C%20CW%2C%20and%20DAmageNet%20attacks%2C%20using%20only%2051%20dimensional%20features.%20By%20combining%20strong%20detection%20performance%20with%20excellent%20generalization%2C%20interpretability%2C%20and%20computational%20efficiency%2C%20FeatureLens%20offers%20a%20practical%20pathway%20toward%20transparent%20and%20effective%20adversarial%20defense.&entry.1838667208=http%3A//arxiv.org/abs/2512.03625v1&entry.124074799=Read"},
{"title": "Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving", "author": "Johannes Fischer and Marlon Steiner and \u00d6mer Sahin Tas and Christoph Stiller", "abstract": "Model predictive control (MPC) is widely used for motion planning, particularly in autonomous driving. Real-time capability of the planner requires utilizing convex approximation of optimal control problems (OCPs) for the planner. However, such approximations confine the solution to a subspace, which might not contain the global optimum. To address this, we propose using safe reinforcement learning (SRL) to obtain a new and safe reference trajectory within MPC. By employing a learning-based approach, the MPC can explore solutions beyond the close neighborhood of the previous one, potentially finding global optima. We incorporate constrained reinforcement learning (CRL) to ensure safety in automated driving, using a handcrafted energy function-based safety index as the constraint objective to model safe and unsafe regions. Our approach utilizes a state-dependent Lagrangian multiplier, learned concurrently with the safe policy, to solve the CRL problem. Through experimentation in a highway scenario, we demonstrate the superiority of our approach over both MPC and SRL in terms of safety and performance measures.", "link": "http://arxiv.org/abs/2512.03774v1", "date": "2025-12-03", "relevancy": 2.0666, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.536}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5277}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety%20Reinforced%20Model%20Predictive%20Control%20%28SRMPC%29%3A%20Improving%20MPC%20with%20Reinforcement%20Learning%20for%20Motion%20Planning%20in%20Autonomous%20Driving&body=Title%3A%20Safety%20Reinforced%20Model%20Predictive%20Control%20%28SRMPC%29%3A%20Improving%20MPC%20with%20Reinforcement%20Learning%20for%20Motion%20Planning%20in%20Autonomous%20Driving%0AAuthor%3A%20Johannes%20Fischer%20and%20Marlon%20Steiner%20and%20%C3%96mer%20Sahin%20Tas%20and%20Christoph%20Stiller%0AAbstract%3A%20Model%20predictive%20control%20%28MPC%29%20is%20widely%20used%20for%20motion%20planning%2C%20particularly%20in%20autonomous%20driving.%20Real-time%20capability%20of%20the%20planner%20requires%20utilizing%20convex%20approximation%20of%20optimal%20control%20problems%20%28OCPs%29%20for%20the%20planner.%20However%2C%20such%20approximations%20confine%20the%20solution%20to%20a%20subspace%2C%20which%20might%20not%20contain%20the%20global%20optimum.%20To%20address%20this%2C%20we%20propose%20using%20safe%20reinforcement%20learning%20%28SRL%29%20to%20obtain%20a%20new%20and%20safe%20reference%20trajectory%20within%20MPC.%20By%20employing%20a%20learning-based%20approach%2C%20the%20MPC%20can%20explore%20solutions%20beyond%20the%20close%20neighborhood%20of%20the%20previous%20one%2C%20potentially%20finding%20global%20optima.%20We%20incorporate%20constrained%20reinforcement%20learning%20%28CRL%29%20to%20ensure%20safety%20in%20automated%20driving%2C%20using%20a%20handcrafted%20energy%20function-based%20safety%20index%20as%20the%20constraint%20objective%20to%20model%20safe%20and%20unsafe%20regions.%20Our%20approach%20utilizes%20a%20state-dependent%20Lagrangian%20multiplier%2C%20learned%20concurrently%20with%20the%20safe%20policy%2C%20to%20solve%20the%20CRL%20problem.%20Through%20experimentation%20in%20a%20highway%20scenario%2C%20we%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20both%20MPC%20and%20SRL%20in%20terms%20of%20safety%20and%20performance%20measures.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety%2520Reinforced%2520Model%2520Predictive%2520Control%2520%2528SRMPC%2529%253A%2520Improving%2520MPC%2520with%2520Reinforcement%2520Learning%2520for%2520Motion%2520Planning%2520in%2520Autonomous%2520Driving%26entry.906535625%3DJohannes%2520Fischer%2520and%2520Marlon%2520Steiner%2520and%2520%25C3%2596mer%2520Sahin%2520Tas%2520and%2520Christoph%2520Stiller%26entry.1292438233%3DModel%2520predictive%2520control%2520%2528MPC%2529%2520is%2520widely%2520used%2520for%2520motion%2520planning%252C%2520particularly%2520in%2520autonomous%2520driving.%2520Real-time%2520capability%2520of%2520the%2520planner%2520requires%2520utilizing%2520convex%2520approximation%2520of%2520optimal%2520control%2520problems%2520%2528OCPs%2529%2520for%2520the%2520planner.%2520However%252C%2520such%2520approximations%2520confine%2520the%2520solution%2520to%2520a%2520subspace%252C%2520which%2520might%2520not%2520contain%2520the%2520global%2520optimum.%2520To%2520address%2520this%252C%2520we%2520propose%2520using%2520safe%2520reinforcement%2520learning%2520%2528SRL%2529%2520to%2520obtain%2520a%2520new%2520and%2520safe%2520reference%2520trajectory%2520within%2520MPC.%2520By%2520employing%2520a%2520learning-based%2520approach%252C%2520the%2520MPC%2520can%2520explore%2520solutions%2520beyond%2520the%2520close%2520neighborhood%2520of%2520the%2520previous%2520one%252C%2520potentially%2520finding%2520global%2520optima.%2520We%2520incorporate%2520constrained%2520reinforcement%2520learning%2520%2528CRL%2529%2520to%2520ensure%2520safety%2520in%2520automated%2520driving%252C%2520using%2520a%2520handcrafted%2520energy%2520function-based%2520safety%2520index%2520as%2520the%2520constraint%2520objective%2520to%2520model%2520safe%2520and%2520unsafe%2520regions.%2520Our%2520approach%2520utilizes%2520a%2520state-dependent%2520Lagrangian%2520multiplier%252C%2520learned%2520concurrently%2520with%2520the%2520safe%2520policy%252C%2520to%2520solve%2520the%2520CRL%2520problem.%2520Through%2520experimentation%2520in%2520a%2520highway%2520scenario%252C%2520we%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520both%2520MPC%2520and%2520SRL%2520in%2520terms%2520of%2520safety%2520and%2520performance%2520measures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety%20Reinforced%20Model%20Predictive%20Control%20%28SRMPC%29%3A%20Improving%20MPC%20with%20Reinforcement%20Learning%20for%20Motion%20Planning%20in%20Autonomous%20Driving&entry.906535625=Johannes%20Fischer%20and%20Marlon%20Steiner%20and%20%C3%96mer%20Sahin%20Tas%20and%20Christoph%20Stiller&entry.1292438233=Model%20predictive%20control%20%28MPC%29%20is%20widely%20used%20for%20motion%20planning%2C%20particularly%20in%20autonomous%20driving.%20Real-time%20capability%20of%20the%20planner%20requires%20utilizing%20convex%20approximation%20of%20optimal%20control%20problems%20%28OCPs%29%20for%20the%20planner.%20However%2C%20such%20approximations%20confine%20the%20solution%20to%20a%20subspace%2C%20which%20might%20not%20contain%20the%20global%20optimum.%20To%20address%20this%2C%20we%20propose%20using%20safe%20reinforcement%20learning%20%28SRL%29%20to%20obtain%20a%20new%20and%20safe%20reference%20trajectory%20within%20MPC.%20By%20employing%20a%20learning-based%20approach%2C%20the%20MPC%20can%20explore%20solutions%20beyond%20the%20close%20neighborhood%20of%20the%20previous%20one%2C%20potentially%20finding%20global%20optima.%20We%20incorporate%20constrained%20reinforcement%20learning%20%28CRL%29%20to%20ensure%20safety%20in%20automated%20driving%2C%20using%20a%20handcrafted%20energy%20function-based%20safety%20index%20as%20the%20constraint%20objective%20to%20model%20safe%20and%20unsafe%20regions.%20Our%20approach%20utilizes%20a%20state-dependent%20Lagrangian%20multiplier%2C%20learned%20concurrently%20with%20the%20safe%20policy%2C%20to%20solve%20the%20CRL%20problem.%20Through%20experimentation%20in%20a%20highway%20scenario%2C%20we%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20both%20MPC%20and%20SRL%20in%20terms%20of%20safety%20and%20performance%20measures.&entry.1838667208=http%3A//arxiv.org/abs/2512.03774v1&entry.124074799=Read"},
{"title": "Structured Uncertainty Similarity Score (SUSS): Learning a Probabilistic, Interpretable, Perceptual Metric Between Images", "author": "Paula Seidler and Neill D. F. Campbell and Ivor J A Simpson", "abstract": "Perceptual similarity scores that align with human vision are critical for both training and evaluating computer vision models. Deep perceptual losses, such as LPIPS, achieve good alignment but rely on complex, highly non-linear discriminative features with unknown invariances, while hand-crafted measures like SSIM are interpretable but miss key perceptual properties.\n  We introduce the Structured Uncertainty Similarity Score (SUSS); it models each image through a set of perceptual components, each represented by a structured multivariate Normal distribution. These are trained in a generative, self-supervised manner to assign high likelihood to human-imperceptible augmentations. The final score is a weighted sum of component log-probabilities with weights learned from human perceptual datasets. Unlike feature-based methods, SUSS learns image-specific linear transformations of residuals in pixel space, enabling transparent inspection through decorrelated residuals and sampling.\n  SUSS aligns closely with human perceptual judgments, shows strong perceptual calibration across diverse distortion types, and provides localized, interpretable explanations of its similarity assessments. We further demonstrate stable optimization behavior and competitive performance when using SUSS as a perceptual loss for downstream imaging tasks.", "link": "http://arxiv.org/abs/2512.03701v1", "date": "2025-12-03", "relevancy": 2.0619, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5261}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.516}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Uncertainty%20Similarity%20Score%20%28SUSS%29%3A%20Learning%20a%20Probabilistic%2C%20Interpretable%2C%20Perceptual%20Metric%20Between%20Images&body=Title%3A%20Structured%20Uncertainty%20Similarity%20Score%20%28SUSS%29%3A%20Learning%20a%20Probabilistic%2C%20Interpretable%2C%20Perceptual%20Metric%20Between%20Images%0AAuthor%3A%20Paula%20Seidler%20and%20Neill%20D.%20F.%20Campbell%20and%20Ivor%20J%20A%20Simpson%0AAbstract%3A%20Perceptual%20similarity%20scores%20that%20align%20with%20human%20vision%20are%20critical%20for%20both%20training%20and%20evaluating%20computer%20vision%20models.%20Deep%20perceptual%20losses%2C%20such%20as%20LPIPS%2C%20achieve%20good%20alignment%20but%20rely%20on%20complex%2C%20highly%20non-linear%20discriminative%20features%20with%20unknown%20invariances%2C%20while%20hand-crafted%20measures%20like%20SSIM%20are%20interpretable%20but%20miss%20key%20perceptual%20properties.%0A%20%20We%20introduce%20the%20Structured%20Uncertainty%20Similarity%20Score%20%28SUSS%29%3B%20it%20models%20each%20image%20through%20a%20set%20of%20perceptual%20components%2C%20each%20represented%20by%20a%20structured%20multivariate%20Normal%20distribution.%20These%20are%20trained%20in%20a%20generative%2C%20self-supervised%20manner%20to%20assign%20high%20likelihood%20to%20human-imperceptible%20augmentations.%20The%20final%20score%20is%20a%20weighted%20sum%20of%20component%20log-probabilities%20with%20weights%20learned%20from%20human%20perceptual%20datasets.%20Unlike%20feature-based%20methods%2C%20SUSS%20learns%20image-specific%20linear%20transformations%20of%20residuals%20in%20pixel%20space%2C%20enabling%20transparent%20inspection%20through%20decorrelated%20residuals%20and%20sampling.%0A%20%20SUSS%20aligns%20closely%20with%20human%20perceptual%20judgments%2C%20shows%20strong%20perceptual%20calibration%20across%20diverse%20distortion%20types%2C%20and%20provides%20localized%2C%20interpretable%20explanations%20of%20its%20similarity%20assessments.%20We%20further%20demonstrate%20stable%20optimization%20behavior%20and%20competitive%20performance%20when%20using%20SUSS%20as%20a%20perceptual%20loss%20for%20downstream%20imaging%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Uncertainty%2520Similarity%2520Score%2520%2528SUSS%2529%253A%2520Learning%2520a%2520Probabilistic%252C%2520Interpretable%252C%2520Perceptual%2520Metric%2520Between%2520Images%26entry.906535625%3DPaula%2520Seidler%2520and%2520Neill%2520D.%2520F.%2520Campbell%2520and%2520Ivor%2520J%2520A%2520Simpson%26entry.1292438233%3DPerceptual%2520similarity%2520scores%2520that%2520align%2520with%2520human%2520vision%2520are%2520critical%2520for%2520both%2520training%2520and%2520evaluating%2520computer%2520vision%2520models.%2520Deep%2520perceptual%2520losses%252C%2520such%2520as%2520LPIPS%252C%2520achieve%2520good%2520alignment%2520but%2520rely%2520on%2520complex%252C%2520highly%2520non-linear%2520discriminative%2520features%2520with%2520unknown%2520invariances%252C%2520while%2520hand-crafted%2520measures%2520like%2520SSIM%2520are%2520interpretable%2520but%2520miss%2520key%2520perceptual%2520properties.%250A%2520%2520We%2520introduce%2520the%2520Structured%2520Uncertainty%2520Similarity%2520Score%2520%2528SUSS%2529%253B%2520it%2520models%2520each%2520image%2520through%2520a%2520set%2520of%2520perceptual%2520components%252C%2520each%2520represented%2520by%2520a%2520structured%2520multivariate%2520Normal%2520distribution.%2520These%2520are%2520trained%2520in%2520a%2520generative%252C%2520self-supervised%2520manner%2520to%2520assign%2520high%2520likelihood%2520to%2520human-imperceptible%2520augmentations.%2520The%2520final%2520score%2520is%2520a%2520weighted%2520sum%2520of%2520component%2520log-probabilities%2520with%2520weights%2520learned%2520from%2520human%2520perceptual%2520datasets.%2520Unlike%2520feature-based%2520methods%252C%2520SUSS%2520learns%2520image-specific%2520linear%2520transformations%2520of%2520residuals%2520in%2520pixel%2520space%252C%2520enabling%2520transparent%2520inspection%2520through%2520decorrelated%2520residuals%2520and%2520sampling.%250A%2520%2520SUSS%2520aligns%2520closely%2520with%2520human%2520perceptual%2520judgments%252C%2520shows%2520strong%2520perceptual%2520calibration%2520across%2520diverse%2520distortion%2520types%252C%2520and%2520provides%2520localized%252C%2520interpretable%2520explanations%2520of%2520its%2520similarity%2520assessments.%2520We%2520further%2520demonstrate%2520stable%2520optimization%2520behavior%2520and%2520competitive%2520performance%2520when%2520using%2520SUSS%2520as%2520a%2520perceptual%2520loss%2520for%2520downstream%2520imaging%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Uncertainty%20Similarity%20Score%20%28SUSS%29%3A%20Learning%20a%20Probabilistic%2C%20Interpretable%2C%20Perceptual%20Metric%20Between%20Images&entry.906535625=Paula%20Seidler%20and%20Neill%20D.%20F.%20Campbell%20and%20Ivor%20J%20A%20Simpson&entry.1292438233=Perceptual%20similarity%20scores%20that%20align%20with%20human%20vision%20are%20critical%20for%20both%20training%20and%20evaluating%20computer%20vision%20models.%20Deep%20perceptual%20losses%2C%20such%20as%20LPIPS%2C%20achieve%20good%20alignment%20but%20rely%20on%20complex%2C%20highly%20non-linear%20discriminative%20features%20with%20unknown%20invariances%2C%20while%20hand-crafted%20measures%20like%20SSIM%20are%20interpretable%20but%20miss%20key%20perceptual%20properties.%0A%20%20We%20introduce%20the%20Structured%20Uncertainty%20Similarity%20Score%20%28SUSS%29%3B%20it%20models%20each%20image%20through%20a%20set%20of%20perceptual%20components%2C%20each%20represented%20by%20a%20structured%20multivariate%20Normal%20distribution.%20These%20are%20trained%20in%20a%20generative%2C%20self-supervised%20manner%20to%20assign%20high%20likelihood%20to%20human-imperceptible%20augmentations.%20The%20final%20score%20is%20a%20weighted%20sum%20of%20component%20log-probabilities%20with%20weights%20learned%20from%20human%20perceptual%20datasets.%20Unlike%20feature-based%20methods%2C%20SUSS%20learns%20image-specific%20linear%20transformations%20of%20residuals%20in%20pixel%20space%2C%20enabling%20transparent%20inspection%20through%20decorrelated%20residuals%20and%20sampling.%0A%20%20SUSS%20aligns%20closely%20with%20human%20perceptual%20judgments%2C%20shows%20strong%20perceptual%20calibration%20across%20diverse%20distortion%20types%2C%20and%20provides%20localized%2C%20interpretable%20explanations%20of%20its%20similarity%20assessments.%20We%20further%20demonstrate%20stable%20optimization%20behavior%20and%20competitive%20performance%20when%20using%20SUSS%20as%20a%20perceptual%20loss%20for%20downstream%20imaging%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.03701v1&entry.124074799=Read"},
{"title": "Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control", "author": "Gabriele Fadini and Deepak Ingole and Tong Duy Son and Alisa Rupenyan", "abstract": "This paper presents an auto-tuning framework for torque-based Nonlinear Model Predictive Control (nMPC), where the MPC serves as a real-time controller for optimal joint torque commands. The MPC parameters, including cost function weights and low-level controller gains, are optimized using high-dimensional Bayesian Optimization (BO) techniques, specifically Sparse Axis-Aligned Subspace (SAASBO) with a digital twin (DT) to achieve precise end-effector trajectory real-time tracking on an UR10e robot arm. The simulation model allows efficient exploration of the high-dimensional parameter space, and it ensures safe transfer to hardware. Our simulation results demonstrate significant improvements in tracking performance (+41.9%) and reduction in solve times (-2.5%) compared to manually-tuned parameters. Moreover, experimental validation on the real robot follows the trend (with a +25.8% improvement), emphasizing the importance of digital twin-enabled automated parameter optimization for robotic operations.", "link": "http://arxiv.org/abs/2512.03772v1", "date": "2025-12-03", "relevancy": 2.0519, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5655}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5063}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Optimization%20for%20Automatic%20Tuning%20of%20Torque-Level%20Nonlinear%20Model%20Predictive%20Control&body=Title%3A%20Bayesian%20Optimization%20for%20Automatic%20Tuning%20of%20Torque-Level%20Nonlinear%20Model%20Predictive%20Control%0AAuthor%3A%20Gabriele%20Fadini%20and%20Deepak%20Ingole%20and%20Tong%20Duy%20Son%20and%20Alisa%20Rupenyan%0AAbstract%3A%20This%20paper%20presents%20an%20auto-tuning%20framework%20for%20torque-based%20Nonlinear%20Model%20Predictive%20Control%20%28nMPC%29%2C%20where%20the%20MPC%20serves%20as%20a%20real-time%20controller%20for%20optimal%20joint%20torque%20commands.%20The%20MPC%20parameters%2C%20including%20cost%20function%20weights%20and%20low-level%20controller%20gains%2C%20are%20optimized%20using%20high-dimensional%20Bayesian%20Optimization%20%28BO%29%20techniques%2C%20specifically%20Sparse%20Axis-Aligned%20Subspace%20%28SAASBO%29%20with%20a%20digital%20twin%20%28DT%29%20to%20achieve%20precise%20end-effector%20trajectory%20real-time%20tracking%20on%20an%20UR10e%20robot%20arm.%20The%20simulation%20model%20allows%20efficient%20exploration%20of%20the%20high-dimensional%20parameter%20space%2C%20and%20it%20ensures%20safe%20transfer%20to%20hardware.%20Our%20simulation%20results%20demonstrate%20significant%20improvements%20in%20tracking%20performance%20%28%2B41.9%25%29%20and%20reduction%20in%20solve%20times%20%28-2.5%25%29%20compared%20to%20manually-tuned%20parameters.%20Moreover%2C%20experimental%20validation%20on%20the%20real%20robot%20follows%20the%20trend%20%28with%20a%20%2B25.8%25%20improvement%29%2C%20emphasizing%20the%20importance%20of%20digital%20twin-enabled%20automated%20parameter%20optimization%20for%20robotic%20operations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Optimization%2520for%2520Automatic%2520Tuning%2520of%2520Torque-Level%2520Nonlinear%2520Model%2520Predictive%2520Control%26entry.906535625%3DGabriele%2520Fadini%2520and%2520Deepak%2520Ingole%2520and%2520Tong%2520Duy%2520Son%2520and%2520Alisa%2520Rupenyan%26entry.1292438233%3DThis%2520paper%2520presents%2520an%2520auto-tuning%2520framework%2520for%2520torque-based%2520Nonlinear%2520Model%2520Predictive%2520Control%2520%2528nMPC%2529%252C%2520where%2520the%2520MPC%2520serves%2520as%2520a%2520real-time%2520controller%2520for%2520optimal%2520joint%2520torque%2520commands.%2520The%2520MPC%2520parameters%252C%2520including%2520cost%2520function%2520weights%2520and%2520low-level%2520controller%2520gains%252C%2520are%2520optimized%2520using%2520high-dimensional%2520Bayesian%2520Optimization%2520%2528BO%2529%2520techniques%252C%2520specifically%2520Sparse%2520Axis-Aligned%2520Subspace%2520%2528SAASBO%2529%2520with%2520a%2520digital%2520twin%2520%2528DT%2529%2520to%2520achieve%2520precise%2520end-effector%2520trajectory%2520real-time%2520tracking%2520on%2520an%2520UR10e%2520robot%2520arm.%2520The%2520simulation%2520model%2520allows%2520efficient%2520exploration%2520of%2520the%2520high-dimensional%2520parameter%2520space%252C%2520and%2520it%2520ensures%2520safe%2520transfer%2520to%2520hardware.%2520Our%2520simulation%2520results%2520demonstrate%2520significant%2520improvements%2520in%2520tracking%2520performance%2520%2528%252B41.9%2525%2529%2520and%2520reduction%2520in%2520solve%2520times%2520%2528-2.5%2525%2529%2520compared%2520to%2520manually-tuned%2520parameters.%2520Moreover%252C%2520experimental%2520validation%2520on%2520the%2520real%2520robot%2520follows%2520the%2520trend%2520%2528with%2520a%2520%252B25.8%2525%2520improvement%2529%252C%2520emphasizing%2520the%2520importance%2520of%2520digital%2520twin-enabled%2520automated%2520parameter%2520optimization%2520for%2520robotic%2520operations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Optimization%20for%20Automatic%20Tuning%20of%20Torque-Level%20Nonlinear%20Model%20Predictive%20Control&entry.906535625=Gabriele%20Fadini%20and%20Deepak%20Ingole%20and%20Tong%20Duy%20Son%20and%20Alisa%20Rupenyan&entry.1292438233=This%20paper%20presents%20an%20auto-tuning%20framework%20for%20torque-based%20Nonlinear%20Model%20Predictive%20Control%20%28nMPC%29%2C%20where%20the%20MPC%20serves%20as%20a%20real-time%20controller%20for%20optimal%20joint%20torque%20commands.%20The%20MPC%20parameters%2C%20including%20cost%20function%20weights%20and%20low-level%20controller%20gains%2C%20are%20optimized%20using%20high-dimensional%20Bayesian%20Optimization%20%28BO%29%20techniques%2C%20specifically%20Sparse%20Axis-Aligned%20Subspace%20%28SAASBO%29%20with%20a%20digital%20twin%20%28DT%29%20to%20achieve%20precise%20end-effector%20trajectory%20real-time%20tracking%20on%20an%20UR10e%20robot%20arm.%20The%20simulation%20model%20allows%20efficient%20exploration%20of%20the%20high-dimensional%20parameter%20space%2C%20and%20it%20ensures%20safe%20transfer%20to%20hardware.%20Our%20simulation%20results%20demonstrate%20significant%20improvements%20in%20tracking%20performance%20%28%2B41.9%25%29%20and%20reduction%20in%20solve%20times%20%28-2.5%25%29%20compared%20to%20manually-tuned%20parameters.%20Moreover%2C%20experimental%20validation%20on%20the%20real%20robot%20follows%20the%20trend%20%28with%20a%20%2B25.8%25%20improvement%29%2C%20emphasizing%20the%20importance%20of%20digital%20twin-enabled%20automated%20parameter%20optimization%20for%20robotic%20operations.&entry.1838667208=http%3A//arxiv.org/abs/2512.03772v1&entry.124074799=Read"},
{"title": "Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions", "author": "Hong Yang and Devroop Kar and Qi Yu and Alex Ororbia and Travis Desell", "abstract": "Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.", "link": "http://arxiv.org/abs/2512.04034v1", "date": "2025-12-03", "relevancy": 1.9651, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4986}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.488}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Feature%20Collapse%3A%20Implications%20for%20Out-of-Distribution%20Detection%20and%20Solutions&body=Title%3A%20Domain%20Feature%20Collapse%3A%20Implications%20for%20Out-of-Distribution%20Detection%20and%20Solutions%0AAuthor%3A%20Hong%20Yang%20and%20Devroop%20Kar%20and%20Qi%20Yu%20and%20Alex%20Ororbia%20and%20Travis%20Desell%0AAbstract%3A%20Why%20do%20state-of-the-art%20OOD%20detection%20methods%20exhibit%20catastrophic%20failure%20when%20models%20are%20trained%20on%20single-domain%20datasets%3F%20We%20provide%20the%20first%20theoretical%20explanation%20for%20this%20phenomenon%20through%20the%20lens%20of%20information%20theory.%20We%20prove%20that%20supervised%20learning%20on%20single-domain%20data%20inevitably%20produces%20domain%20feature%20collapse%20--%20representations%20where%20I%28x_d%3B%20z%29%20%3D%200%2C%20meaning%20domain-specific%20information%20is%20completely%20discarded.%20This%20is%20a%20fundamental%20consequence%20of%20information%20bottleneck%20optimization%3A%20models%20trained%20on%20single%20domains%20%28e.g.%2C%20medical%20images%29%20learn%20to%20rely%20solely%20on%20class-specific%20features%20while%20discarding%20domain%20features%2C%20leading%20to%20catastrophic%20failure%20when%20detecting%20out-of-domain%20samples%20%28e.g.%2C%20achieving%20only%2053%25%20FPR%4095%20on%20MNIST%29.%20We%20extend%20our%20analysis%20using%20Fano%27s%20inequality%20to%20quantify%20partial%20collapse%20in%20practical%20scenarios.%20To%20validate%20our%20theory%2C%20we%20introduce%20Domain%20Bench%2C%20a%20benchmark%20of%20single-domain%20datasets%2C%20and%20demonstrate%20that%20preserving%20I%28x_d%3B%20z%29%20%3E%200%20through%20domain%20filtering%20%28using%20pretrained%20representations%29%20resolves%20the%20failure%20mode.%20While%20domain%20filtering%20itself%20is%20conceptually%20straightforward%2C%20its%20effectiveness%20provides%20strong%20empirical%20evidence%20for%20our%20information-theoretic%20framework.%20Our%20work%20explains%20a%20puzzling%20empirical%20phenomenon%2C%20reveals%20fundamental%20limitations%20of%20supervised%20learning%20in%20narrow%20domains%2C%20and%20has%20broader%20implications%20for%20transfer%20learning%20and%20when%20to%20fine-tune%20versus%20freeze%20pretrained%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Feature%2520Collapse%253A%2520Implications%2520for%2520Out-of-Distribution%2520Detection%2520and%2520Solutions%26entry.906535625%3DHong%2520Yang%2520and%2520Devroop%2520Kar%2520and%2520Qi%2520Yu%2520and%2520Alex%2520Ororbia%2520and%2520Travis%2520Desell%26entry.1292438233%3DWhy%2520do%2520state-of-the-art%2520OOD%2520detection%2520methods%2520exhibit%2520catastrophic%2520failure%2520when%2520models%2520are%2520trained%2520on%2520single-domain%2520datasets%253F%2520We%2520provide%2520the%2520first%2520theoretical%2520explanation%2520for%2520this%2520phenomenon%2520through%2520the%2520lens%2520of%2520information%2520theory.%2520We%2520prove%2520that%2520supervised%2520learning%2520on%2520single-domain%2520data%2520inevitably%2520produces%2520domain%2520feature%2520collapse%2520--%2520representations%2520where%2520I%2528x_d%253B%2520z%2529%2520%253D%25200%252C%2520meaning%2520domain-specific%2520information%2520is%2520completely%2520discarded.%2520This%2520is%2520a%2520fundamental%2520consequence%2520of%2520information%2520bottleneck%2520optimization%253A%2520models%2520trained%2520on%2520single%2520domains%2520%2528e.g.%252C%2520medical%2520images%2529%2520learn%2520to%2520rely%2520solely%2520on%2520class-specific%2520features%2520while%2520discarding%2520domain%2520features%252C%2520leading%2520to%2520catastrophic%2520failure%2520when%2520detecting%2520out-of-domain%2520samples%2520%2528e.g.%252C%2520achieving%2520only%252053%2525%2520FPR%254095%2520on%2520MNIST%2529.%2520We%2520extend%2520our%2520analysis%2520using%2520Fano%2527s%2520inequality%2520to%2520quantify%2520partial%2520collapse%2520in%2520practical%2520scenarios.%2520To%2520validate%2520our%2520theory%252C%2520we%2520introduce%2520Domain%2520Bench%252C%2520a%2520benchmark%2520of%2520single-domain%2520datasets%252C%2520and%2520demonstrate%2520that%2520preserving%2520I%2528x_d%253B%2520z%2529%2520%253E%25200%2520through%2520domain%2520filtering%2520%2528using%2520pretrained%2520representations%2529%2520resolves%2520the%2520failure%2520mode.%2520While%2520domain%2520filtering%2520itself%2520is%2520conceptually%2520straightforward%252C%2520its%2520effectiveness%2520provides%2520strong%2520empirical%2520evidence%2520for%2520our%2520information-theoretic%2520framework.%2520Our%2520work%2520explains%2520a%2520puzzling%2520empirical%2520phenomenon%252C%2520reveals%2520fundamental%2520limitations%2520of%2520supervised%2520learning%2520in%2520narrow%2520domains%252C%2520and%2520has%2520broader%2520implications%2520for%2520transfer%2520learning%2520and%2520when%2520to%2520fine-tune%2520versus%2520freeze%2520pretrained%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Feature%20Collapse%3A%20Implications%20for%20Out-of-Distribution%20Detection%20and%20Solutions&entry.906535625=Hong%20Yang%20and%20Devroop%20Kar%20and%20Qi%20Yu%20and%20Alex%20Ororbia%20and%20Travis%20Desell&entry.1292438233=Why%20do%20state-of-the-art%20OOD%20detection%20methods%20exhibit%20catastrophic%20failure%20when%20models%20are%20trained%20on%20single-domain%20datasets%3F%20We%20provide%20the%20first%20theoretical%20explanation%20for%20this%20phenomenon%20through%20the%20lens%20of%20information%20theory.%20We%20prove%20that%20supervised%20learning%20on%20single-domain%20data%20inevitably%20produces%20domain%20feature%20collapse%20--%20representations%20where%20I%28x_d%3B%20z%29%20%3D%200%2C%20meaning%20domain-specific%20information%20is%20completely%20discarded.%20This%20is%20a%20fundamental%20consequence%20of%20information%20bottleneck%20optimization%3A%20models%20trained%20on%20single%20domains%20%28e.g.%2C%20medical%20images%29%20learn%20to%20rely%20solely%20on%20class-specific%20features%20while%20discarding%20domain%20features%2C%20leading%20to%20catastrophic%20failure%20when%20detecting%20out-of-domain%20samples%20%28e.g.%2C%20achieving%20only%2053%25%20FPR%4095%20on%20MNIST%29.%20We%20extend%20our%20analysis%20using%20Fano%27s%20inequality%20to%20quantify%20partial%20collapse%20in%20practical%20scenarios.%20To%20validate%20our%20theory%2C%20we%20introduce%20Domain%20Bench%2C%20a%20benchmark%20of%20single-domain%20datasets%2C%20and%20demonstrate%20that%20preserving%20I%28x_d%3B%20z%29%20%3E%200%20through%20domain%20filtering%20%28using%20pretrained%20representations%29%20resolves%20the%20failure%20mode.%20While%20domain%20filtering%20itself%20is%20conceptually%20straightforward%2C%20its%20effectiveness%20provides%20strong%20empirical%20evidence%20for%20our%20information-theoretic%20framework.%20Our%20work%20explains%20a%20puzzling%20empirical%20phenomenon%2C%20reveals%20fundamental%20limitations%20of%20supervised%20learning%20in%20narrow%20domains%2C%20and%20has%20broader%20implications%20for%20transfer%20learning%20and%20when%20to%20fine-tune%20versus%20freeze%20pretrained%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.04034v1&entry.124074799=Read"},
{"title": "Interactive and Hybrid Imitation Learning: Provably Beating Behavior Cloning", "author": "Yichen Li and Chicheng Zhang", "abstract": "Imitation learning (IL) is a paradigm for learning sequential decision making policies from experts, leveraging offline demonstrations, interactive annotations, or both. Recent advances show that when annotation cost is tallied per trajectory, Behavior Cloning (BC) which relies solely on offline demonstrations cannot be improved in general, leaving limited conditions for interactive methods such as DAgger to help. We revisit this conclusion and prove that when the annotation cost is measured per state, algorithms using interactive annotations can provably outperform BC. Specifically: (1) we show that Stagger, a one sample per round variant of DAgger, provably beats BC under low recovery cost settings; (2) we initiate the study of hybrid IL where the agent learns from offline demonstrations and interactive annotations. We propose Warm Stagger whose learning guarantee is not much worse than using either data source alone. Furthermore, motivated by compounding error and cold start problem in imitation learning practice, we give an MDP example in which Warm Stagger has significant better annotation cost; (3) experiments on MuJoCo continuous control tasks confirm that, with modest cost ratio between interactive and offline annotations, interactive and hybrid approaches consistently outperform BC. To the best of our knowledge, our work is the first to highlight the benefit of state wise interactive annotation and hybrid feedback in imitation learning.", "link": "http://arxiv.org/abs/2412.07057v2", "date": "2025-12-03", "relevancy": 2.0106, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5186}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5083}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20and%20Hybrid%20Imitation%20Learning%3A%20Provably%20Beating%20Behavior%20Cloning&body=Title%3A%20Interactive%20and%20Hybrid%20Imitation%20Learning%3A%20Provably%20Beating%20Behavior%20Cloning%0AAuthor%3A%20Yichen%20Li%20and%20Chicheng%20Zhang%0AAbstract%3A%20Imitation%20learning%20%28IL%29%20is%20a%20paradigm%20for%20learning%20sequential%20decision%20making%20policies%20from%20experts%2C%20leveraging%20offline%20demonstrations%2C%20interactive%20annotations%2C%20or%20both.%20Recent%20advances%20show%20that%20when%20annotation%20cost%20is%20tallied%20per%20trajectory%2C%20Behavior%20Cloning%20%28BC%29%20which%20relies%20solely%20on%20offline%20demonstrations%20cannot%20be%20improved%20in%20general%2C%20leaving%20limited%20conditions%20for%20interactive%20methods%20such%20as%20DAgger%20to%20help.%20We%20revisit%20this%20conclusion%20and%20prove%20that%20when%20the%20annotation%20cost%20is%20measured%20per%20state%2C%20algorithms%20using%20interactive%20annotations%20can%20provably%20outperform%20BC.%20Specifically%3A%20%281%29%20we%20show%20that%20Stagger%2C%20a%20one%20sample%20per%20round%20variant%20of%20DAgger%2C%20provably%20beats%20BC%20under%20low%20recovery%20cost%20settings%3B%20%282%29%20we%20initiate%20the%20study%20of%20hybrid%20IL%20where%20the%20agent%20learns%20from%20offline%20demonstrations%20and%20interactive%20annotations.%20We%20propose%20Warm%20Stagger%20whose%20learning%20guarantee%20is%20not%20much%20worse%20than%20using%20either%20data%20source%20alone.%20Furthermore%2C%20motivated%20by%20compounding%20error%20and%20cold%20start%20problem%20in%20imitation%20learning%20practice%2C%20we%20give%20an%20MDP%20example%20in%20which%20Warm%20Stagger%20has%20significant%20better%20annotation%20cost%3B%20%283%29%20experiments%20on%20MuJoCo%20continuous%20control%20tasks%20confirm%20that%2C%20with%20modest%20cost%20ratio%20between%20interactive%20and%20offline%20annotations%2C%20interactive%20and%20hybrid%20approaches%20consistently%20outperform%20BC.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20work%20is%20the%20first%20to%20highlight%20the%20benefit%20of%20state%20wise%20interactive%20annotation%20and%20hybrid%20feedback%20in%20imitation%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2412.07057v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520and%2520Hybrid%2520Imitation%2520Learning%253A%2520Provably%2520Beating%2520Behavior%2520Cloning%26entry.906535625%3DYichen%2520Li%2520and%2520Chicheng%2520Zhang%26entry.1292438233%3DImitation%2520learning%2520%2528IL%2529%2520is%2520a%2520paradigm%2520for%2520learning%2520sequential%2520decision%2520making%2520policies%2520from%2520experts%252C%2520leveraging%2520offline%2520demonstrations%252C%2520interactive%2520annotations%252C%2520or%2520both.%2520Recent%2520advances%2520show%2520that%2520when%2520annotation%2520cost%2520is%2520tallied%2520per%2520trajectory%252C%2520Behavior%2520Cloning%2520%2528BC%2529%2520which%2520relies%2520solely%2520on%2520offline%2520demonstrations%2520cannot%2520be%2520improved%2520in%2520general%252C%2520leaving%2520limited%2520conditions%2520for%2520interactive%2520methods%2520such%2520as%2520DAgger%2520to%2520help.%2520We%2520revisit%2520this%2520conclusion%2520and%2520prove%2520that%2520when%2520the%2520annotation%2520cost%2520is%2520measured%2520per%2520state%252C%2520algorithms%2520using%2520interactive%2520annotations%2520can%2520provably%2520outperform%2520BC.%2520Specifically%253A%2520%25281%2529%2520we%2520show%2520that%2520Stagger%252C%2520a%2520one%2520sample%2520per%2520round%2520variant%2520of%2520DAgger%252C%2520provably%2520beats%2520BC%2520under%2520low%2520recovery%2520cost%2520settings%253B%2520%25282%2529%2520we%2520initiate%2520the%2520study%2520of%2520hybrid%2520IL%2520where%2520the%2520agent%2520learns%2520from%2520offline%2520demonstrations%2520and%2520interactive%2520annotations.%2520We%2520propose%2520Warm%2520Stagger%2520whose%2520learning%2520guarantee%2520is%2520not%2520much%2520worse%2520than%2520using%2520either%2520data%2520source%2520alone.%2520Furthermore%252C%2520motivated%2520by%2520compounding%2520error%2520and%2520cold%2520start%2520problem%2520in%2520imitation%2520learning%2520practice%252C%2520we%2520give%2520an%2520MDP%2520example%2520in%2520which%2520Warm%2520Stagger%2520has%2520significant%2520better%2520annotation%2520cost%253B%2520%25283%2529%2520experiments%2520on%2520MuJoCo%2520continuous%2520control%2520tasks%2520confirm%2520that%252C%2520with%2520modest%2520cost%2520ratio%2520between%2520interactive%2520and%2520offline%2520annotations%252C%2520interactive%2520and%2520hybrid%2520approaches%2520consistently%2520outperform%2520BC.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520work%2520is%2520the%2520first%2520to%2520highlight%2520the%2520benefit%2520of%2520state%2520wise%2520interactive%2520annotation%2520and%2520hybrid%2520feedback%2520in%2520imitation%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07057v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20and%20Hybrid%20Imitation%20Learning%3A%20Provably%20Beating%20Behavior%20Cloning&entry.906535625=Yichen%20Li%20and%20Chicheng%20Zhang&entry.1292438233=Imitation%20learning%20%28IL%29%20is%20a%20paradigm%20for%20learning%20sequential%20decision%20making%20policies%20from%20experts%2C%20leveraging%20offline%20demonstrations%2C%20interactive%20annotations%2C%20or%20both.%20Recent%20advances%20show%20that%20when%20annotation%20cost%20is%20tallied%20per%20trajectory%2C%20Behavior%20Cloning%20%28BC%29%20which%20relies%20solely%20on%20offline%20demonstrations%20cannot%20be%20improved%20in%20general%2C%20leaving%20limited%20conditions%20for%20interactive%20methods%20such%20as%20DAgger%20to%20help.%20We%20revisit%20this%20conclusion%20and%20prove%20that%20when%20the%20annotation%20cost%20is%20measured%20per%20state%2C%20algorithms%20using%20interactive%20annotations%20can%20provably%20outperform%20BC.%20Specifically%3A%20%281%29%20we%20show%20that%20Stagger%2C%20a%20one%20sample%20per%20round%20variant%20of%20DAgger%2C%20provably%20beats%20BC%20under%20low%20recovery%20cost%20settings%3B%20%282%29%20we%20initiate%20the%20study%20of%20hybrid%20IL%20where%20the%20agent%20learns%20from%20offline%20demonstrations%20and%20interactive%20annotations.%20We%20propose%20Warm%20Stagger%20whose%20learning%20guarantee%20is%20not%20much%20worse%20than%20using%20either%20data%20source%20alone.%20Furthermore%2C%20motivated%20by%20compounding%20error%20and%20cold%20start%20problem%20in%20imitation%20learning%20practice%2C%20we%20give%20an%20MDP%20example%20in%20which%20Warm%20Stagger%20has%20significant%20better%20annotation%20cost%3B%20%283%29%20experiments%20on%20MuJoCo%20continuous%20control%20tasks%20confirm%20that%2C%20with%20modest%20cost%20ratio%20between%20interactive%20and%20offline%20annotations%2C%20interactive%20and%20hybrid%20approaches%20consistently%20outperform%20BC.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20work%20is%20the%20first%20to%20highlight%20the%20benefit%20of%20state%20wise%20interactive%20annotation%20and%20hybrid%20feedback%20in%20imitation%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2412.07057v2&entry.124074799=Read"},
{"title": "Colored Markov Random Fields for Probabilistic Topological Modeling", "author": "Lorenzo Marinucci and Leonardo Di Nino and Gabriele D'Acunto and Mario Edoardo Pandolfo and Paolo Di Lorenzo and Sergio Barbarossa", "abstract": "Probabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior.", "link": "http://arxiv.org/abs/2512.03727v1", "date": "2025-12-03", "relevancy": 1.4059, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4953}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4646}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Colored%20Markov%20Random%20Fields%20for%20Probabilistic%20Topological%20Modeling&body=Title%3A%20Colored%20Markov%20Random%20Fields%20for%20Probabilistic%20Topological%20Modeling%0AAuthor%3A%20Lorenzo%20Marinucci%20and%20Leonardo%20Di%20Nino%20and%20Gabriele%20D%27Acunto%20and%20Mario%20Edoardo%20Pandolfo%20and%20Paolo%20Di%20Lorenzo%20and%20Sergio%20Barbarossa%0AAbstract%3A%20Probabilistic%20Graphical%20Models%20%28PGMs%29%20encode%20conditional%20dependencies%20among%20random%20variables%20using%20a%20graph%20-nodes%20for%20variables%2C%20links%20for%20dependencies-%20and%20factorize%20the%20joint%20distribution%20into%20lower-dimensional%20components.%20This%20makes%20PGMs%20well-suited%20for%20analyzing%20complex%20systems%20and%20supporting%20decision-making.%20Recent%20advances%20in%20topological%20signal%20processing%20highlight%20the%20importance%20of%20variables%20defined%20on%20topological%20spaces%20in%20several%20application%20domains.%20In%20such%20cases%2C%20the%20underlying%20topology%20shapes%20statistical%20relationships%2C%20limiting%20the%20expressiveness%20of%20canonical%20PGMs.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20Colored%20Markov%20Random%20Fields%20%28CMRFs%29%2C%20which%20model%20both%20conditional%20and%20marginal%20dependencies%20among%20Gaussian%20edge%20variables%20on%20topological%20spaces%2C%20with%20a%20theoretical%20foundation%20in%20Hodge%20theory.%20CMRFs%20extend%20classical%20Gaussian%20Markov%20Random%20Fields%20by%20including%20link%20coloring%3A%20connectivity%20encodes%20conditional%20independence%2C%20while%20color%20encodes%20marginal%20independence.%20We%20quantify%20the%20benefits%20of%20CMRFs%20through%20a%20distributed%20estimation%20case%20study%20over%20a%20physical%20network%2C%20comparing%20it%20with%20baselines%20with%20different%20levels%20of%20topological%20prior.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColored%2520Markov%2520Random%2520Fields%2520for%2520Probabilistic%2520Topological%2520Modeling%26entry.906535625%3DLorenzo%2520Marinucci%2520and%2520Leonardo%2520Di%2520Nino%2520and%2520Gabriele%2520D%2527Acunto%2520and%2520Mario%2520Edoardo%2520Pandolfo%2520and%2520Paolo%2520Di%2520Lorenzo%2520and%2520Sergio%2520Barbarossa%26entry.1292438233%3DProbabilistic%2520Graphical%2520Models%2520%2528PGMs%2529%2520encode%2520conditional%2520dependencies%2520among%2520random%2520variables%2520using%2520a%2520graph%2520-nodes%2520for%2520variables%252C%2520links%2520for%2520dependencies-%2520and%2520factorize%2520the%2520joint%2520distribution%2520into%2520lower-dimensional%2520components.%2520This%2520makes%2520PGMs%2520well-suited%2520for%2520analyzing%2520complex%2520systems%2520and%2520supporting%2520decision-making.%2520Recent%2520advances%2520in%2520topological%2520signal%2520processing%2520highlight%2520the%2520importance%2520of%2520variables%2520defined%2520on%2520topological%2520spaces%2520in%2520several%2520application%2520domains.%2520In%2520such%2520cases%252C%2520the%2520underlying%2520topology%2520shapes%2520statistical%2520relationships%252C%2520limiting%2520the%2520expressiveness%2520of%2520canonical%2520PGMs.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520Colored%2520Markov%2520Random%2520Fields%2520%2528CMRFs%2529%252C%2520which%2520model%2520both%2520conditional%2520and%2520marginal%2520dependencies%2520among%2520Gaussian%2520edge%2520variables%2520on%2520topological%2520spaces%252C%2520with%2520a%2520theoretical%2520foundation%2520in%2520Hodge%2520theory.%2520CMRFs%2520extend%2520classical%2520Gaussian%2520Markov%2520Random%2520Fields%2520by%2520including%2520link%2520coloring%253A%2520connectivity%2520encodes%2520conditional%2520independence%252C%2520while%2520color%2520encodes%2520marginal%2520independence.%2520We%2520quantify%2520the%2520benefits%2520of%2520CMRFs%2520through%2520a%2520distributed%2520estimation%2520case%2520study%2520over%2520a%2520physical%2520network%252C%2520comparing%2520it%2520with%2520baselines%2520with%2520different%2520levels%2520of%2520topological%2520prior.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Colored%20Markov%20Random%20Fields%20for%20Probabilistic%20Topological%20Modeling&entry.906535625=Lorenzo%20Marinucci%20and%20Leonardo%20Di%20Nino%20and%20Gabriele%20D%27Acunto%20and%20Mario%20Edoardo%20Pandolfo%20and%20Paolo%20Di%20Lorenzo%20and%20Sergio%20Barbarossa&entry.1292438233=Probabilistic%20Graphical%20Models%20%28PGMs%29%20encode%20conditional%20dependencies%20among%20random%20variables%20using%20a%20graph%20-nodes%20for%20variables%2C%20links%20for%20dependencies-%20and%20factorize%20the%20joint%20distribution%20into%20lower-dimensional%20components.%20This%20makes%20PGMs%20well-suited%20for%20analyzing%20complex%20systems%20and%20supporting%20decision-making.%20Recent%20advances%20in%20topological%20signal%20processing%20highlight%20the%20importance%20of%20variables%20defined%20on%20topological%20spaces%20in%20several%20application%20domains.%20In%20such%20cases%2C%20the%20underlying%20topology%20shapes%20statistical%20relationships%2C%20limiting%20the%20expressiveness%20of%20canonical%20PGMs.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20Colored%20Markov%20Random%20Fields%20%28CMRFs%29%2C%20which%20model%20both%20conditional%20and%20marginal%20dependencies%20among%20Gaussian%20edge%20variables%20on%20topological%20spaces%2C%20with%20a%20theoretical%20foundation%20in%20Hodge%20theory.%20CMRFs%20extend%20classical%20Gaussian%20Markov%20Random%20Fields%20by%20including%20link%20coloring%3A%20connectivity%20encodes%20conditional%20independence%2C%20while%20color%20encodes%20marginal%20independence.%20We%20quantify%20the%20benefits%20of%20CMRFs%20through%20a%20distributed%20estimation%20case%20study%20over%20a%20physical%20network%2C%20comparing%20it%20with%20baselines%20with%20different%20levels%20of%20topological%20prior.&entry.1838667208=http%3A//arxiv.org/abs/2512.03727v1&entry.124074799=Read"},
{"title": "Dynamically Scaled Activation Steering", "author": "Alex Ferrando and Xavier Suau and Jordi Gonz\u00e0lez and Pau Rodriguez", "abstract": "Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.", "link": "http://arxiv.org/abs/2512.03661v1", "date": "2025-12-03", "relevancy": 2.0066, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5305}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5069}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamically%20Scaled%20Activation%20Steering&body=Title%3A%20Dynamically%20Scaled%20Activation%20Steering%0AAuthor%3A%20Alex%20Ferrando%20and%20Xavier%20Suau%20and%20Jordi%20Gonz%C3%A0lez%20and%20Pau%20Rodriguez%0AAbstract%3A%20Activation%20steering%20has%20emerged%20as%20a%20powerful%20method%20for%20guiding%20the%20behavior%20of%20generative%20models%20towards%20desired%20outcomes%20such%20as%20toxicity%20mitigation.%20However%2C%20most%20existing%20methods%20apply%20interventions%20uniformly%20across%20all%20inputs%2C%20degrading%20model%20performance%20when%20steering%20is%20unnecessary.%20We%20introduce%20Dynamically%20Scaled%20Activation%20Steering%20%28DSAS%29%2C%20a%20method-agnostic%20steering%20framework%20that%20decouples%20when%20to%20steer%20from%20how%20to%20steer.%20DSAS%20adaptively%20modulates%20the%20strength%20of%20existing%20steering%20transformations%20across%20layers%20and%20inputs%2C%20intervening%20strongly%20only%20when%20undesired%20behavior%20is%20detected.%20At%20generation%20time%2C%20DSAS%20computes%20context-dependent%20scaling%20factors%20that%20selectively%20adjust%20the%20strength%20of%20any%20steering%20method.%20We%20also%20show%20how%20DSAS%20can%20be%20jointly%20optimized%20end-to-end%20together%20with%20the%20steering%20function.%20When%20combined%20with%20existing%20steering%20methods%2C%20DSAS%20consistently%20improves%20the%20Pareto%20front%20with%20respect%20to%20steering%20alone%2C%20achieving%20a%20better%20trade-off%20between%20toxicity%20mitigation%20and%20utility%20preservation.%20We%20further%20demonstrate%20DSAS%27s%20generality%20by%20applying%20it%20to%20a%20text-to-image%20diffusion%20model%2C%20showing%20how%20adaptive%20steering%20allows%20the%20modulation%20of%20specific%20concepts.%20Finally%2C%20DSAS%20introduces%20minimal%20computational%20overhead%20while%20improving%20interpretability%2C%20pinpointing%20which%20tokens%20require%20steering%20and%20by%20how%20much.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamically%2520Scaled%2520Activation%2520Steering%26entry.906535625%3DAlex%2520Ferrando%2520and%2520Xavier%2520Suau%2520and%2520Jordi%2520Gonz%25C3%25A0lez%2520and%2520Pau%2520Rodriguez%26entry.1292438233%3DActivation%2520steering%2520has%2520emerged%2520as%2520a%2520powerful%2520method%2520for%2520guiding%2520the%2520behavior%2520of%2520generative%2520models%2520towards%2520desired%2520outcomes%2520such%2520as%2520toxicity%2520mitigation.%2520However%252C%2520most%2520existing%2520methods%2520apply%2520interventions%2520uniformly%2520across%2520all%2520inputs%252C%2520degrading%2520model%2520performance%2520when%2520steering%2520is%2520unnecessary.%2520We%2520introduce%2520Dynamically%2520Scaled%2520Activation%2520Steering%2520%2528DSAS%2529%252C%2520a%2520method-agnostic%2520steering%2520framework%2520that%2520decouples%2520when%2520to%2520steer%2520from%2520how%2520to%2520steer.%2520DSAS%2520adaptively%2520modulates%2520the%2520strength%2520of%2520existing%2520steering%2520transformations%2520across%2520layers%2520and%2520inputs%252C%2520intervening%2520strongly%2520only%2520when%2520undesired%2520behavior%2520is%2520detected.%2520At%2520generation%2520time%252C%2520DSAS%2520computes%2520context-dependent%2520scaling%2520factors%2520that%2520selectively%2520adjust%2520the%2520strength%2520of%2520any%2520steering%2520method.%2520We%2520also%2520show%2520how%2520DSAS%2520can%2520be%2520jointly%2520optimized%2520end-to-end%2520together%2520with%2520the%2520steering%2520function.%2520When%2520combined%2520with%2520existing%2520steering%2520methods%252C%2520DSAS%2520consistently%2520improves%2520the%2520Pareto%2520front%2520with%2520respect%2520to%2520steering%2520alone%252C%2520achieving%2520a%2520better%2520trade-off%2520between%2520toxicity%2520mitigation%2520and%2520utility%2520preservation.%2520We%2520further%2520demonstrate%2520DSAS%2527s%2520generality%2520by%2520applying%2520it%2520to%2520a%2520text-to-image%2520diffusion%2520model%252C%2520showing%2520how%2520adaptive%2520steering%2520allows%2520the%2520modulation%2520of%2520specific%2520concepts.%2520Finally%252C%2520DSAS%2520introduces%2520minimal%2520computational%2520overhead%2520while%2520improving%2520interpretability%252C%2520pinpointing%2520which%2520tokens%2520require%2520steering%2520and%2520by%2520how%2520much.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamically%20Scaled%20Activation%20Steering&entry.906535625=Alex%20Ferrando%20and%20Xavier%20Suau%20and%20Jordi%20Gonz%C3%A0lez%20and%20Pau%20Rodriguez&entry.1292438233=Activation%20steering%20has%20emerged%20as%20a%20powerful%20method%20for%20guiding%20the%20behavior%20of%20generative%20models%20towards%20desired%20outcomes%20such%20as%20toxicity%20mitigation.%20However%2C%20most%20existing%20methods%20apply%20interventions%20uniformly%20across%20all%20inputs%2C%20degrading%20model%20performance%20when%20steering%20is%20unnecessary.%20We%20introduce%20Dynamically%20Scaled%20Activation%20Steering%20%28DSAS%29%2C%20a%20method-agnostic%20steering%20framework%20that%20decouples%20when%20to%20steer%20from%20how%20to%20steer.%20DSAS%20adaptively%20modulates%20the%20strength%20of%20existing%20steering%20transformations%20across%20layers%20and%20inputs%2C%20intervening%20strongly%20only%20when%20undesired%20behavior%20is%20detected.%20At%20generation%20time%2C%20DSAS%20computes%20context-dependent%20scaling%20factors%20that%20selectively%20adjust%20the%20strength%20of%20any%20steering%20method.%20We%20also%20show%20how%20DSAS%20can%20be%20jointly%20optimized%20end-to-end%20together%20with%20the%20steering%20function.%20When%20combined%20with%20existing%20steering%20methods%2C%20DSAS%20consistently%20improves%20the%20Pareto%20front%20with%20respect%20to%20steering%20alone%2C%20achieving%20a%20better%20trade-off%20between%20toxicity%20mitigation%20and%20utility%20preservation.%20We%20further%20demonstrate%20DSAS%27s%20generality%20by%20applying%20it%20to%20a%20text-to-image%20diffusion%20model%2C%20showing%20how%20adaptive%20steering%20allows%20the%20modulation%20of%20specific%20concepts.%20Finally%2C%20DSAS%20introduces%20minimal%20computational%20overhead%20while%20improving%20interpretability%2C%20pinpointing%20which%20tokens%20require%20steering%20and%20by%20how%20much.&entry.1838667208=http%3A//arxiv.org/abs/2512.03661v1&entry.124074799=Read"},
{"title": "Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap", "author": "Shashaank Khanna and Matthew Pusey and Roger Colbeck", "abstract": "The discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures.", "link": "http://arxiv.org/abs/2512.04058v1", "date": "2025-12-03", "relevancy": 1.3435, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3359}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closing%20the%20problem%20of%20which%20causal%20structures%20of%20up%20to%20six%20total%20nodes%20have%20a%20classical-quantum%20gap&body=Title%3A%20Closing%20the%20problem%20of%20which%20causal%20structures%20of%20up%20to%20six%20total%20nodes%20have%20a%20classical-quantum%20gap%0AAuthor%3A%20Shashaank%20Khanna%20and%20Matthew%20Pusey%20and%20Roger%20Colbeck%0AAbstract%3A%20The%20discovery%20of%20Bell%20that%20there%20exist%20quantum%20correlations%20that%20cannot%20be%20reproduced%20classically%20is%20one%20of%20the%20most%20important%20in%20the%20foundations%20of%20quantum%20mechanics%2C%20as%20well%20as%20having%20practical%20implications.%20Bell%27s%20result%20was%20originally%20proven%20in%20a%20simple%20bipartite%20causal%20structure%2C%20but%20analogous%20results%20have%20also%20been%20shown%20in%20further%20causal%20structures.%20Here%20we%20study%20the%20only%20causal%20structure%20with%20six%20or%20fewer%20nodes%20in%20which%20the%20question%20of%20whether%20or%20not%20there%20exist%20quantum%20correlations%20that%20cannot%20be%20achieved%20classically%20was%20open.%20In%20this%20causal%20structure%20we%20show%20that%20such%20quantum%20correlations%20exist%20using%20a%20method%20that%20involves%20imposing%20additional%20restrictions%20on%20the%20correlations.%20This%20hence%20completes%20the%20picture%20of%20which%20causal%20structures%20of%20up%20to%20six%20nodes%20support%20non-classical%20quantum%20correlations.%20We%20also%20provide%20further%20illustrations%20of%20our%20method%20using%20other%20causal%20structures.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosing%2520the%2520problem%2520of%2520which%2520causal%2520structures%2520of%2520up%2520to%2520six%2520total%2520nodes%2520have%2520a%2520classical-quantum%2520gap%26entry.906535625%3DShashaank%2520Khanna%2520and%2520Matthew%2520Pusey%2520and%2520Roger%2520Colbeck%26entry.1292438233%3DThe%2520discovery%2520of%2520Bell%2520that%2520there%2520exist%2520quantum%2520correlations%2520that%2520cannot%2520be%2520reproduced%2520classically%2520is%2520one%2520of%2520the%2520most%2520important%2520in%2520the%2520foundations%2520of%2520quantum%2520mechanics%252C%2520as%2520well%2520as%2520having%2520practical%2520implications.%2520Bell%2527s%2520result%2520was%2520originally%2520proven%2520in%2520a%2520simple%2520bipartite%2520causal%2520structure%252C%2520but%2520analogous%2520results%2520have%2520also%2520been%2520shown%2520in%2520further%2520causal%2520structures.%2520Here%2520we%2520study%2520the%2520only%2520causal%2520structure%2520with%2520six%2520or%2520fewer%2520nodes%2520in%2520which%2520the%2520question%2520of%2520whether%2520or%2520not%2520there%2520exist%2520quantum%2520correlations%2520that%2520cannot%2520be%2520achieved%2520classically%2520was%2520open.%2520In%2520this%2520causal%2520structure%2520we%2520show%2520that%2520such%2520quantum%2520correlations%2520exist%2520using%2520a%2520method%2520that%2520involves%2520imposing%2520additional%2520restrictions%2520on%2520the%2520correlations.%2520This%2520hence%2520completes%2520the%2520picture%2520of%2520which%2520causal%2520structures%2520of%2520up%2520to%2520six%2520nodes%2520support%2520non-classical%2520quantum%2520correlations.%2520We%2520also%2520provide%2520further%2520illustrations%2520of%2520our%2520method%2520using%2520other%2520causal%2520structures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20problem%20of%20which%20causal%20structures%20of%20up%20to%20six%20total%20nodes%20have%20a%20classical-quantum%20gap&entry.906535625=Shashaank%20Khanna%20and%20Matthew%20Pusey%20and%20Roger%20Colbeck&entry.1292438233=The%20discovery%20of%20Bell%20that%20there%20exist%20quantum%20correlations%20that%20cannot%20be%20reproduced%20classically%20is%20one%20of%20the%20most%20important%20in%20the%20foundations%20of%20quantum%20mechanics%2C%20as%20well%20as%20having%20practical%20implications.%20Bell%27s%20result%20was%20originally%20proven%20in%20a%20simple%20bipartite%20causal%20structure%2C%20but%20analogous%20results%20have%20also%20been%20shown%20in%20further%20causal%20structures.%20Here%20we%20study%20the%20only%20causal%20structure%20with%20six%20or%20fewer%20nodes%20in%20which%20the%20question%20of%20whether%20or%20not%20there%20exist%20quantum%20correlations%20that%20cannot%20be%20achieved%20classically%20was%20open.%20In%20this%20causal%20structure%20we%20show%20that%20such%20quantum%20correlations%20exist%20using%20a%20method%20that%20involves%20imposing%20additional%20restrictions%20on%20the%20correlations.%20This%20hence%20completes%20the%20picture%20of%20which%20causal%20structures%20of%20up%20to%20six%20nodes%20support%20non-classical%20quantum%20correlations.%20We%20also%20provide%20further%20illustrations%20of%20our%20method%20using%20other%20causal%20structures.&entry.1838667208=http%3A//arxiv.org/abs/2512.04058v1&entry.124074799=Read"},
{"title": "Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control", "author": "Kenneth Stewart and Samantha Chapin and Roxana Leontie and Carl Glen Henshaw", "abstract": "Reinforcement learning (RL) offers transformative potential for robotic control in space. We present the first on-orbit demonstration of RL-based autonomous control of a free-flying robot, the NASA Astrobee, aboard the International Space Station (ISS). Using NVIDIA's Omniverse physics simulator and curriculum learning, we trained a deep neural network to replace Astrobee's standard attitude and translation control, enabling it to navigate in microgravity. Our results validate a novel training pipeline that bridges the simulation-to-reality (Sim2Real) gap, utilizing a GPU-accelerated, scientific-grade simulation environment for efficient Monte Carlo RL training. This successful deployment demonstrates the feasibility of training RL policies terrestrially and transferring them to space-based applications. This paves the way for future work in In-Space Servicing, Assembly, and Manufacturing (ISAM), enabling rapid on-orbit adaptation to dynamic mission requirements.", "link": "http://arxiv.org/abs/2512.03736v1", "date": "2025-12-03", "relevancy": 1.6137, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5417}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5361}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Crossing%20the%20Sim2Real%20Gap%20Between%20Simulation%20and%20Ground%20Testing%20to%20Space%20Deployment%20of%20Autonomous%20Free-flyer%20Control&body=Title%3A%20Crossing%20the%20Sim2Real%20Gap%20Between%20Simulation%20and%20Ground%20Testing%20to%20Space%20Deployment%20of%20Autonomous%20Free-flyer%20Control%0AAuthor%3A%20Kenneth%20Stewart%20and%20Samantha%20Chapin%20and%20Roxana%20Leontie%20and%20Carl%20Glen%20Henshaw%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29%20offers%20transformative%20potential%20for%20robotic%20control%20in%20space.%20We%20present%20the%20first%20on-orbit%20demonstration%20of%20RL-based%20autonomous%20control%20of%20a%20free-flying%20robot%2C%20the%20NASA%20Astrobee%2C%20aboard%20the%20International%20Space%20Station%20%28ISS%29.%20Using%20NVIDIA%27s%20Omniverse%20physics%20simulator%20and%20curriculum%20learning%2C%20we%20trained%20a%20deep%20neural%20network%20to%20replace%20Astrobee%27s%20standard%20attitude%20and%20translation%20control%2C%20enabling%20it%20to%20navigate%20in%20microgravity.%20Our%20results%20validate%20a%20novel%20training%20pipeline%20that%20bridges%20the%20simulation-to-reality%20%28Sim2Real%29%20gap%2C%20utilizing%20a%20GPU-accelerated%2C%20scientific-grade%20simulation%20environment%20for%20efficient%20Monte%20Carlo%20RL%20training.%20This%20successful%20deployment%20demonstrates%20the%20feasibility%20of%20training%20RL%20policies%20terrestrially%20and%20transferring%20them%20to%20space-based%20applications.%20This%20paves%20the%20way%20for%20future%20work%20in%20In-Space%20Servicing%2C%20Assembly%2C%20and%20Manufacturing%20%28ISAM%29%2C%20enabling%20rapid%20on-orbit%20adaptation%20to%20dynamic%20mission%20requirements.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrossing%2520the%2520Sim2Real%2520Gap%2520Between%2520Simulation%2520and%2520Ground%2520Testing%2520to%2520Space%2520Deployment%2520of%2520Autonomous%2520Free-flyer%2520Control%26entry.906535625%3DKenneth%2520Stewart%2520and%2520Samantha%2520Chapin%2520and%2520Roxana%2520Leontie%2520and%2520Carl%2520Glen%2520Henshaw%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529%2520offers%2520transformative%2520potential%2520for%2520robotic%2520control%2520in%2520space.%2520We%2520present%2520the%2520first%2520on-orbit%2520demonstration%2520of%2520RL-based%2520autonomous%2520control%2520of%2520a%2520free-flying%2520robot%252C%2520the%2520NASA%2520Astrobee%252C%2520aboard%2520the%2520International%2520Space%2520Station%2520%2528ISS%2529.%2520Using%2520NVIDIA%2527s%2520Omniverse%2520physics%2520simulator%2520and%2520curriculum%2520learning%252C%2520we%2520trained%2520a%2520deep%2520neural%2520network%2520to%2520replace%2520Astrobee%2527s%2520standard%2520attitude%2520and%2520translation%2520control%252C%2520enabling%2520it%2520to%2520navigate%2520in%2520microgravity.%2520Our%2520results%2520validate%2520a%2520novel%2520training%2520pipeline%2520that%2520bridges%2520the%2520simulation-to-reality%2520%2528Sim2Real%2529%2520gap%252C%2520utilizing%2520a%2520GPU-accelerated%252C%2520scientific-grade%2520simulation%2520environment%2520for%2520efficient%2520Monte%2520Carlo%2520RL%2520training.%2520This%2520successful%2520deployment%2520demonstrates%2520the%2520feasibility%2520of%2520training%2520RL%2520policies%2520terrestrially%2520and%2520transferring%2520them%2520to%2520space-based%2520applications.%2520This%2520paves%2520the%2520way%2520for%2520future%2520work%2520in%2520In-Space%2520Servicing%252C%2520Assembly%252C%2520and%2520Manufacturing%2520%2528ISAM%2529%252C%2520enabling%2520rapid%2520on-orbit%2520adaptation%2520to%2520dynamic%2520mission%2520requirements.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Crossing%20the%20Sim2Real%20Gap%20Between%20Simulation%20and%20Ground%20Testing%20to%20Space%20Deployment%20of%20Autonomous%20Free-flyer%20Control&entry.906535625=Kenneth%20Stewart%20and%20Samantha%20Chapin%20and%20Roxana%20Leontie%20and%20Carl%20Glen%20Henshaw&entry.1292438233=Reinforcement%20learning%20%28RL%29%20offers%20transformative%20potential%20for%20robotic%20control%20in%20space.%20We%20present%20the%20first%20on-orbit%20demonstration%20of%20RL-based%20autonomous%20control%20of%20a%20free-flying%20robot%2C%20the%20NASA%20Astrobee%2C%20aboard%20the%20International%20Space%20Station%20%28ISS%29.%20Using%20NVIDIA%27s%20Omniverse%20physics%20simulator%20and%20curriculum%20learning%2C%20we%20trained%20a%20deep%20neural%20network%20to%20replace%20Astrobee%27s%20standard%20attitude%20and%20translation%20control%2C%20enabling%20it%20to%20navigate%20in%20microgravity.%20Our%20results%20validate%20a%20novel%20training%20pipeline%20that%20bridges%20the%20simulation-to-reality%20%28Sim2Real%29%20gap%2C%20utilizing%20a%20GPU-accelerated%2C%20scientific-grade%20simulation%20environment%20for%20efficient%20Monte%20Carlo%20RL%20training.%20This%20successful%20deployment%20demonstrates%20the%20feasibility%20of%20training%20RL%20policies%20terrestrially%20and%20transferring%20them%20to%20space-based%20applications.%20This%20paves%20the%20way%20for%20future%20work%20in%20In-Space%20Servicing%2C%20Assembly%2C%20and%20Manufacturing%20%28ISAM%29%2C%20enabling%20rapid%20on-orbit%20adaptation%20to%20dynamic%20mission%20requirements.&entry.1838667208=http%3A//arxiv.org/abs/2512.03736v1&entry.124074799=Read"},
{"title": "A Tractable Two-Step Linear Mixing Model Solved with Second-Order Optimization for Spectral Unmixing under Variability", "author": "Xander Haijen and Bikram Koirala and Xuanwen Tao and Paul Scheunders", "abstract": "In this paper, we propose a Two-Step Linear Mixing Model (2LMM) that bridges the gap between model complexity and computational tractability. The model achieves this by introducing two distinct scaling steps: an endmember scaling step across the image, and another for pixel-wise scaling. We show that this model leads to only a mildly non-convex optimization problem, which we solve with an optimization algorithm that incorporates second-order information. To the authors' knowledge, this work represents the first application of second-order optimization techniques to solve a spectral unmixing problem that models endmember variability. Our method is highly robust, as it requires virtually no hyperparameter tuning and can therefore be used easily and quickly in a wide range of unmixing tasks. We show through extensive experiments on both simulated and real data that the new model is competitive and in some cases superior to the state of the art in unmixing. The model also performs very well in challenging scenarios, such as blind unmixing.", "link": "http://arxiv.org/abs/2502.17212v3", "date": "2025-12-03", "relevancy": 1.4067, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4754}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.471}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Tractable%20Two-Step%20Linear%20Mixing%20Model%20Solved%20with%20Second-Order%20Optimization%20for%20Spectral%20Unmixing%20under%20Variability&body=Title%3A%20A%20Tractable%20Two-Step%20Linear%20Mixing%20Model%20Solved%20with%20Second-Order%20Optimization%20for%20Spectral%20Unmixing%20under%20Variability%0AAuthor%3A%20Xander%20Haijen%20and%20Bikram%20Koirala%20and%20Xuanwen%20Tao%20and%20Paul%20Scheunders%0AAbstract%3A%20In%20this%20paper%2C%20we%20propose%20a%20Two-Step%20Linear%20Mixing%20Model%20%282LMM%29%20that%20bridges%20the%20gap%20between%20model%20complexity%20and%20computational%20tractability.%20The%20model%20achieves%20this%20by%20introducing%20two%20distinct%20scaling%20steps%3A%20an%20endmember%20scaling%20step%20across%20the%20image%2C%20and%20another%20for%20pixel-wise%20scaling.%20We%20show%20that%20this%20model%20leads%20to%20only%20a%20mildly%20non-convex%20optimization%20problem%2C%20which%20we%20solve%20with%20an%20optimization%20algorithm%20that%20incorporates%20second-order%20information.%20To%20the%20authors%27%20knowledge%2C%20this%20work%20represents%20the%20first%20application%20of%20second-order%20optimization%20techniques%20to%20solve%20a%20spectral%20unmixing%20problem%20that%20models%20endmember%20variability.%20Our%20method%20is%20highly%20robust%2C%20as%20it%20requires%20virtually%20no%20hyperparameter%20tuning%20and%20can%20therefore%20be%20used%20easily%20and%20quickly%20in%20a%20wide%20range%20of%20unmixing%20tasks.%20We%20show%20through%20extensive%20experiments%20on%20both%20simulated%20and%20real%20data%20that%20the%20new%20model%20is%20competitive%20and%20in%20some%20cases%20superior%20to%20the%20state%20of%20the%20art%20in%20unmixing.%20The%20model%20also%20performs%20very%20well%20in%20challenging%20scenarios%2C%20such%20as%20blind%20unmixing.%0ALink%3A%20http%3A//arxiv.org/abs/2502.17212v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Tractable%2520Two-Step%2520Linear%2520Mixing%2520Model%2520Solved%2520with%2520Second-Order%2520Optimization%2520for%2520Spectral%2520Unmixing%2520under%2520Variability%26entry.906535625%3DXander%2520Haijen%2520and%2520Bikram%2520Koirala%2520and%2520Xuanwen%2520Tao%2520and%2520Paul%2520Scheunders%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520Two-Step%2520Linear%2520Mixing%2520Model%2520%25282LMM%2529%2520that%2520bridges%2520the%2520gap%2520between%2520model%2520complexity%2520and%2520computational%2520tractability.%2520The%2520model%2520achieves%2520this%2520by%2520introducing%2520two%2520distinct%2520scaling%2520steps%253A%2520an%2520endmember%2520scaling%2520step%2520across%2520the%2520image%252C%2520and%2520another%2520for%2520pixel-wise%2520scaling.%2520We%2520show%2520that%2520this%2520model%2520leads%2520to%2520only%2520a%2520mildly%2520non-convex%2520optimization%2520problem%252C%2520which%2520we%2520solve%2520with%2520an%2520optimization%2520algorithm%2520that%2520incorporates%2520second-order%2520information.%2520To%2520the%2520authors%2527%2520knowledge%252C%2520this%2520work%2520represents%2520the%2520first%2520application%2520of%2520second-order%2520optimization%2520techniques%2520to%2520solve%2520a%2520spectral%2520unmixing%2520problem%2520that%2520models%2520endmember%2520variability.%2520Our%2520method%2520is%2520highly%2520robust%252C%2520as%2520it%2520requires%2520virtually%2520no%2520hyperparameter%2520tuning%2520and%2520can%2520therefore%2520be%2520used%2520easily%2520and%2520quickly%2520in%2520a%2520wide%2520range%2520of%2520unmixing%2520tasks.%2520We%2520show%2520through%2520extensive%2520experiments%2520on%2520both%2520simulated%2520and%2520real%2520data%2520that%2520the%2520new%2520model%2520is%2520competitive%2520and%2520in%2520some%2520cases%2520superior%2520to%2520the%2520state%2520of%2520the%2520art%2520in%2520unmixing.%2520The%2520model%2520also%2520performs%2520very%2520well%2520in%2520challenging%2520scenarios%252C%2520such%2520as%2520blind%2520unmixing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17212v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Tractable%20Two-Step%20Linear%20Mixing%20Model%20Solved%20with%20Second-Order%20Optimization%20for%20Spectral%20Unmixing%20under%20Variability&entry.906535625=Xander%20Haijen%20and%20Bikram%20Koirala%20and%20Xuanwen%20Tao%20and%20Paul%20Scheunders&entry.1292438233=In%20this%20paper%2C%20we%20propose%20a%20Two-Step%20Linear%20Mixing%20Model%20%282LMM%29%20that%20bridges%20the%20gap%20between%20model%20complexity%20and%20computational%20tractability.%20The%20model%20achieves%20this%20by%20introducing%20two%20distinct%20scaling%20steps%3A%20an%20endmember%20scaling%20step%20across%20the%20image%2C%20and%20another%20for%20pixel-wise%20scaling.%20We%20show%20that%20this%20model%20leads%20to%20only%20a%20mildly%20non-convex%20optimization%20problem%2C%20which%20we%20solve%20with%20an%20optimization%20algorithm%20that%20incorporates%20second-order%20information.%20To%20the%20authors%27%20knowledge%2C%20this%20work%20represents%20the%20first%20application%20of%20second-order%20optimization%20techniques%20to%20solve%20a%20spectral%20unmixing%20problem%20that%20models%20endmember%20variability.%20Our%20method%20is%20highly%20robust%2C%20as%20it%20requires%20virtually%20no%20hyperparameter%20tuning%20and%20can%20therefore%20be%20used%20easily%20and%20quickly%20in%20a%20wide%20range%20of%20unmixing%20tasks.%20We%20show%20through%20extensive%20experiments%20on%20both%20simulated%20and%20real%20data%20that%20the%20new%20model%20is%20competitive%20and%20in%20some%20cases%20superior%20to%20the%20state%20of%20the%20art%20in%20unmixing.%20The%20model%20also%20performs%20very%20well%20in%20challenging%20scenarios%2C%20such%20as%20blind%20unmixing.&entry.1838667208=http%3A//arxiv.org/abs/2502.17212v3&entry.124074799=Read"},
{"title": "Covariance Scattering Transforms", "author": "Andrea Cavallo and Ayushman Raghuvanshi and Sundeep Prabhakar Chepuri and Elvin Isufi", "abstract": "Machine learning and data processing techniques relying on covariance information are widespread as they identify meaningful patterns in unsupervised and unlabeled settings. As a prominent example, Principal Component Analysis (PCA) projects data points onto the eigenvectors of their covariance matrix, capturing the directions of maximum variance. This mapping, however, falls short in two directions: it fails to capture information in low-variance directions, relevant when, e.g., the data contains high-variance noise; and it provides unstable results in low-sample regimes, especially when covariance eigenvalues are close. CoVariance Neural Networks (VNNs), i.e., graph neural networks using the covariance matrix as a graph, show improved stability to estimation errors and learn more expressive functions in the covariance spectrum than PCA, but require training and operate in a labeled setup. To get the benefits of both worlds, we propose Covariance Scattering Transforms (CSTs), deep untrained networks that sequentially apply filters localized in the covariance spectrum to the input data and produce expressive hierarchical representations via nonlinearities. We define the filters as covariance wavelets that capture specific and detailed covariance spectral patterns. We improve CSTs' computational and memory efficiency via a pruning mechanism, and we prove that their error due to finite-sample covariance estimations is less sensitive to close covariance eigenvalues compared to PCA, improving their stability. Our experiments on age prediction from cortical thickness measurements on 4 datasets collecting patients with neurodegenerative diseases show that CSTs produce stable representations in low-data settings, as VNNs but without any training, and lead to comparable or better predictions w.r.t. more complex learning models.", "link": "http://arxiv.org/abs/2511.08878v2", "date": "2025-12-03", "relevancy": 1.8251, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4682}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4547}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Covariance%20Scattering%20Transforms&body=Title%3A%20Covariance%20Scattering%20Transforms%0AAuthor%3A%20Andrea%20Cavallo%20and%20Ayushman%20Raghuvanshi%20and%20Sundeep%20Prabhakar%20Chepuri%20and%20Elvin%20Isufi%0AAbstract%3A%20Machine%20learning%20and%20data%20processing%20techniques%20relying%20on%20covariance%20information%20are%20widespread%20as%20they%20identify%20meaningful%20patterns%20in%20unsupervised%20and%20unlabeled%20settings.%20As%20a%20prominent%20example%2C%20Principal%20Component%20Analysis%20%28PCA%29%20projects%20data%20points%20onto%20the%20eigenvectors%20of%20their%20covariance%20matrix%2C%20capturing%20the%20directions%20of%20maximum%20variance.%20This%20mapping%2C%20however%2C%20falls%20short%20in%20two%20directions%3A%20it%20fails%20to%20capture%20information%20in%20low-variance%20directions%2C%20relevant%20when%2C%20e.g.%2C%20the%20data%20contains%20high-variance%20noise%3B%20and%20it%20provides%20unstable%20results%20in%20low-sample%20regimes%2C%20especially%20when%20covariance%20eigenvalues%20are%20close.%20CoVariance%20Neural%20Networks%20%28VNNs%29%2C%20i.e.%2C%20graph%20neural%20networks%20using%20the%20covariance%20matrix%20as%20a%20graph%2C%20show%20improved%20stability%20to%20estimation%20errors%20and%20learn%20more%20expressive%20functions%20in%20the%20covariance%20spectrum%20than%20PCA%2C%20but%20require%20training%20and%20operate%20in%20a%20labeled%20setup.%20To%20get%20the%20benefits%20of%20both%20worlds%2C%20we%20propose%20Covariance%20Scattering%20Transforms%20%28CSTs%29%2C%20deep%20untrained%20networks%20that%20sequentially%20apply%20filters%20localized%20in%20the%20covariance%20spectrum%20to%20the%20input%20data%20and%20produce%20expressive%20hierarchical%20representations%20via%20nonlinearities.%20We%20define%20the%20filters%20as%20covariance%20wavelets%20that%20capture%20specific%20and%20detailed%20covariance%20spectral%20patterns.%20We%20improve%20CSTs%27%20computational%20and%20memory%20efficiency%20via%20a%20pruning%20mechanism%2C%20and%20we%20prove%20that%20their%20error%20due%20to%20finite-sample%20covariance%20estimations%20is%20less%20sensitive%20to%20close%20covariance%20eigenvalues%20compared%20to%20PCA%2C%20improving%20their%20stability.%20Our%20experiments%20on%20age%20prediction%20from%20cortical%20thickness%20measurements%20on%204%20datasets%20collecting%20patients%20with%20neurodegenerative%20diseases%20show%20that%20CSTs%20produce%20stable%20representations%20in%20low-data%20settings%2C%20as%20VNNs%20but%20without%20any%20training%2C%20and%20lead%20to%20comparable%20or%20better%20predictions%20w.r.t.%20more%20complex%20learning%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08878v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCovariance%2520Scattering%2520Transforms%26entry.906535625%3DAndrea%2520Cavallo%2520and%2520Ayushman%2520Raghuvanshi%2520and%2520Sundeep%2520Prabhakar%2520Chepuri%2520and%2520Elvin%2520Isufi%26entry.1292438233%3DMachine%2520learning%2520and%2520data%2520processing%2520techniques%2520relying%2520on%2520covariance%2520information%2520are%2520widespread%2520as%2520they%2520identify%2520meaningful%2520patterns%2520in%2520unsupervised%2520and%2520unlabeled%2520settings.%2520As%2520a%2520prominent%2520example%252C%2520Principal%2520Component%2520Analysis%2520%2528PCA%2529%2520projects%2520data%2520points%2520onto%2520the%2520eigenvectors%2520of%2520their%2520covariance%2520matrix%252C%2520capturing%2520the%2520directions%2520of%2520maximum%2520variance.%2520This%2520mapping%252C%2520however%252C%2520falls%2520short%2520in%2520two%2520directions%253A%2520it%2520fails%2520to%2520capture%2520information%2520in%2520low-variance%2520directions%252C%2520relevant%2520when%252C%2520e.g.%252C%2520the%2520data%2520contains%2520high-variance%2520noise%253B%2520and%2520it%2520provides%2520unstable%2520results%2520in%2520low-sample%2520regimes%252C%2520especially%2520when%2520covariance%2520eigenvalues%2520are%2520close.%2520CoVariance%2520Neural%2520Networks%2520%2528VNNs%2529%252C%2520i.e.%252C%2520graph%2520neural%2520networks%2520using%2520the%2520covariance%2520matrix%2520as%2520a%2520graph%252C%2520show%2520improved%2520stability%2520to%2520estimation%2520errors%2520and%2520learn%2520more%2520expressive%2520functions%2520in%2520the%2520covariance%2520spectrum%2520than%2520PCA%252C%2520but%2520require%2520training%2520and%2520operate%2520in%2520a%2520labeled%2520setup.%2520To%2520get%2520the%2520benefits%2520of%2520both%2520worlds%252C%2520we%2520propose%2520Covariance%2520Scattering%2520Transforms%2520%2528CSTs%2529%252C%2520deep%2520untrained%2520networks%2520that%2520sequentially%2520apply%2520filters%2520localized%2520in%2520the%2520covariance%2520spectrum%2520to%2520the%2520input%2520data%2520and%2520produce%2520expressive%2520hierarchical%2520representations%2520via%2520nonlinearities.%2520We%2520define%2520the%2520filters%2520as%2520covariance%2520wavelets%2520that%2520capture%2520specific%2520and%2520detailed%2520covariance%2520spectral%2520patterns.%2520We%2520improve%2520CSTs%2527%2520computational%2520and%2520memory%2520efficiency%2520via%2520a%2520pruning%2520mechanism%252C%2520and%2520we%2520prove%2520that%2520their%2520error%2520due%2520to%2520finite-sample%2520covariance%2520estimations%2520is%2520less%2520sensitive%2520to%2520close%2520covariance%2520eigenvalues%2520compared%2520to%2520PCA%252C%2520improving%2520their%2520stability.%2520Our%2520experiments%2520on%2520age%2520prediction%2520from%2520cortical%2520thickness%2520measurements%2520on%25204%2520datasets%2520collecting%2520patients%2520with%2520neurodegenerative%2520diseases%2520show%2520that%2520CSTs%2520produce%2520stable%2520representations%2520in%2520low-data%2520settings%252C%2520as%2520VNNs%2520but%2520without%2520any%2520training%252C%2520and%2520lead%2520to%2520comparable%2520or%2520better%2520predictions%2520w.r.t.%2520more%2520complex%2520learning%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08878v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Covariance%20Scattering%20Transforms&entry.906535625=Andrea%20Cavallo%20and%20Ayushman%20Raghuvanshi%20and%20Sundeep%20Prabhakar%20Chepuri%20and%20Elvin%20Isufi&entry.1292438233=Machine%20learning%20and%20data%20processing%20techniques%20relying%20on%20covariance%20information%20are%20widespread%20as%20they%20identify%20meaningful%20patterns%20in%20unsupervised%20and%20unlabeled%20settings.%20As%20a%20prominent%20example%2C%20Principal%20Component%20Analysis%20%28PCA%29%20projects%20data%20points%20onto%20the%20eigenvectors%20of%20their%20covariance%20matrix%2C%20capturing%20the%20directions%20of%20maximum%20variance.%20This%20mapping%2C%20however%2C%20falls%20short%20in%20two%20directions%3A%20it%20fails%20to%20capture%20information%20in%20low-variance%20directions%2C%20relevant%20when%2C%20e.g.%2C%20the%20data%20contains%20high-variance%20noise%3B%20and%20it%20provides%20unstable%20results%20in%20low-sample%20regimes%2C%20especially%20when%20covariance%20eigenvalues%20are%20close.%20CoVariance%20Neural%20Networks%20%28VNNs%29%2C%20i.e.%2C%20graph%20neural%20networks%20using%20the%20covariance%20matrix%20as%20a%20graph%2C%20show%20improved%20stability%20to%20estimation%20errors%20and%20learn%20more%20expressive%20functions%20in%20the%20covariance%20spectrum%20than%20PCA%2C%20but%20require%20training%20and%20operate%20in%20a%20labeled%20setup.%20To%20get%20the%20benefits%20of%20both%20worlds%2C%20we%20propose%20Covariance%20Scattering%20Transforms%20%28CSTs%29%2C%20deep%20untrained%20networks%20that%20sequentially%20apply%20filters%20localized%20in%20the%20covariance%20spectrum%20to%20the%20input%20data%20and%20produce%20expressive%20hierarchical%20representations%20via%20nonlinearities.%20We%20define%20the%20filters%20as%20covariance%20wavelets%20that%20capture%20specific%20and%20detailed%20covariance%20spectral%20patterns.%20We%20improve%20CSTs%27%20computational%20and%20memory%20efficiency%20via%20a%20pruning%20mechanism%2C%20and%20we%20prove%20that%20their%20error%20due%20to%20finite-sample%20covariance%20estimations%20is%20less%20sensitive%20to%20close%20covariance%20eigenvalues%20compared%20to%20PCA%2C%20improving%20their%20stability.%20Our%20experiments%20on%20age%20prediction%20from%20cortical%20thickness%20measurements%20on%204%20datasets%20collecting%20patients%20with%20neurodegenerative%20diseases%20show%20that%20CSTs%20produce%20stable%20representations%20in%20low-data%20settings%2C%20as%20VNNs%20but%20without%20any%20training%2C%20and%20lead%20to%20comparable%20or%20better%20predictions%20w.r.t.%20more%20complex%20learning%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.08878v2&entry.124074799=Read"},
{"title": "ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers", "author": "Feice Huang and Zuliang Han and Xing Zhou and Yihuang Chen and Lifei Zhu and Haoqian Wang", "abstract": "Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\\times$ speedup and 4.05$\\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.", "link": "http://arxiv.org/abs/2512.03673v1", "date": "2025-12-03", "relevancy": 1.7438, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6531}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5959}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConvRot%3A%20Rotation-Based%20Plug-and-Play%204-bit%20Quantization%20for%20Diffusion%20Transformers&body=Title%3A%20ConvRot%3A%20Rotation-Based%20Plug-and-Play%204-bit%20Quantization%20for%20Diffusion%20Transformers%0AAuthor%3A%20Feice%20Huang%20and%20Zuliang%20Han%20and%20Xing%20Zhou%20and%20Yihuang%20Chen%20and%20Lifei%20Zhu%20and%20Haoqian%20Wang%0AAbstract%3A%20Diffusion%20transformers%20have%20demonstrated%20strong%20capabilities%20in%20generating%20high-quality%20images.%20However%2C%20as%20model%20size%20increases%2C%20the%20growing%20memory%20footprint%20and%20inference%20latency%20pose%20significant%20challenges%20for%20practical%20deployment.%20Recent%20studies%20in%20large%20language%20models%20%28LLMs%29%20show%20that%20rotation-based%20techniques%20can%20smooth%20outliers%20and%20enable%204-bit%20quantization%2C%20but%20these%20approaches%20often%20incur%20substantial%20overhead%20and%20struggle%20with%20row-wise%20outliers%20in%20diffusion%20transformers.%20To%20address%20these%20challenges%2C%20we%20propose%20ConvRot%2C%20a%20group-wise%20rotation-based%20quantization%20method%20that%20leverages%20regular%20Hadamard%20transform%20%28RHT%29%20to%20suppress%20both%20row-wise%20and%20column-wise%20outliers%20while%20reducing%20complexity%20from%20quadratic%20to%20linear.%20Building%20on%20this%2C%20we%20design%20ConvLinear4bit%2C%20a%20plug-and-play%20module%20that%20integrates%20rotation%2C%20quantization%2C%20GEMM%2C%20and%20dequantization%2C%20enabling%20W4A4%20inference%20without%20retraining%20and%20preserving%20visual%20quality.%20Experiments%20on%20FLUX.1-dev%20demonstrate%20a%202.26%24%5Ctimes%24%20speedup%20and%204.05%24%5Ctimes%24%20memory%20reduction%20while%20maintaining%20image%20fidelity.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20application%20of%20rotation-based%20quantization%20for%20plug-and-play%20W4A4%20inference%20in%20diffusion%20transformers.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvRot%253A%2520Rotation-Based%2520Plug-and-Play%25204-bit%2520Quantization%2520for%2520Diffusion%2520Transformers%26entry.906535625%3DFeice%2520Huang%2520and%2520Zuliang%2520Han%2520and%2520Xing%2520Zhou%2520and%2520Yihuang%2520Chen%2520and%2520Lifei%2520Zhu%2520and%2520Haoqian%2520Wang%26entry.1292438233%3DDiffusion%2520transformers%2520have%2520demonstrated%2520strong%2520capabilities%2520in%2520generating%2520high-quality%2520images.%2520However%252C%2520as%2520model%2520size%2520increases%252C%2520the%2520growing%2520memory%2520footprint%2520and%2520inference%2520latency%2520pose%2520significant%2520challenges%2520for%2520practical%2520deployment.%2520Recent%2520studies%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520show%2520that%2520rotation-based%2520techniques%2520can%2520smooth%2520outliers%2520and%2520enable%25204-bit%2520quantization%252C%2520but%2520these%2520approaches%2520often%2520incur%2520substantial%2520overhead%2520and%2520struggle%2520with%2520row-wise%2520outliers%2520in%2520diffusion%2520transformers.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520ConvRot%252C%2520a%2520group-wise%2520rotation-based%2520quantization%2520method%2520that%2520leverages%2520regular%2520Hadamard%2520transform%2520%2528RHT%2529%2520to%2520suppress%2520both%2520row-wise%2520and%2520column-wise%2520outliers%2520while%2520reducing%2520complexity%2520from%2520quadratic%2520to%2520linear.%2520Building%2520on%2520this%252C%2520we%2520design%2520ConvLinear4bit%252C%2520a%2520plug-and-play%2520module%2520that%2520integrates%2520rotation%252C%2520quantization%252C%2520GEMM%252C%2520and%2520dequantization%252C%2520enabling%2520W4A4%2520inference%2520without%2520retraining%2520and%2520preserving%2520visual%2520quality.%2520Experiments%2520on%2520FLUX.1-dev%2520demonstrate%2520a%25202.26%2524%255Ctimes%2524%2520speedup%2520and%25204.05%2524%255Ctimes%2524%2520memory%2520reduction%2520while%2520maintaining%2520image%2520fidelity.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520application%2520of%2520rotation-based%2520quantization%2520for%2520plug-and-play%2520W4A4%2520inference%2520in%2520diffusion%2520transformers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConvRot%3A%20Rotation-Based%20Plug-and-Play%204-bit%20Quantization%20for%20Diffusion%20Transformers&entry.906535625=Feice%20Huang%20and%20Zuliang%20Han%20and%20Xing%20Zhou%20and%20Yihuang%20Chen%20and%20Lifei%20Zhu%20and%20Haoqian%20Wang&entry.1292438233=Diffusion%20transformers%20have%20demonstrated%20strong%20capabilities%20in%20generating%20high-quality%20images.%20However%2C%20as%20model%20size%20increases%2C%20the%20growing%20memory%20footprint%20and%20inference%20latency%20pose%20significant%20challenges%20for%20practical%20deployment.%20Recent%20studies%20in%20large%20language%20models%20%28LLMs%29%20show%20that%20rotation-based%20techniques%20can%20smooth%20outliers%20and%20enable%204-bit%20quantization%2C%20but%20these%20approaches%20often%20incur%20substantial%20overhead%20and%20struggle%20with%20row-wise%20outliers%20in%20diffusion%20transformers.%20To%20address%20these%20challenges%2C%20we%20propose%20ConvRot%2C%20a%20group-wise%20rotation-based%20quantization%20method%20that%20leverages%20regular%20Hadamard%20transform%20%28RHT%29%20to%20suppress%20both%20row-wise%20and%20column-wise%20outliers%20while%20reducing%20complexity%20from%20quadratic%20to%20linear.%20Building%20on%20this%2C%20we%20design%20ConvLinear4bit%2C%20a%20plug-and-play%20module%20that%20integrates%20rotation%2C%20quantization%2C%20GEMM%2C%20and%20dequantization%2C%20enabling%20W4A4%20inference%20without%20retraining%20and%20preserving%20visual%20quality.%20Experiments%20on%20FLUX.1-dev%20demonstrate%20a%202.26%24%5Ctimes%24%20speedup%20and%204.05%24%5Ctimes%24%20memory%20reduction%20while%20maintaining%20image%20fidelity.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20application%20of%20rotation-based%20quantization%20for%20plug-and-play%20W4A4%20inference%20in%20diffusion%20transformers.&entry.1838667208=http%3A//arxiv.org/abs/2512.03673v1&entry.124074799=Read"},
{"title": "LAMP: Language-Assisted Motion Planning for Controllable Video Generation", "author": "Muhammed Burak Kizil and Enes Sanli and Niloy J. Mitra and Erkut Erdem and Aykut Erdem and Duygu Ceylan", "abstract": "Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications.", "link": "http://arxiv.org/abs/2512.03619v1", "date": "2025-12-03", "relevancy": 1.7739, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5957}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5898}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAMP%3A%20Language-Assisted%20Motion%20Planning%20for%20Controllable%20Video%20Generation&body=Title%3A%20LAMP%3A%20Language-Assisted%20Motion%20Planning%20for%20Controllable%20Video%20Generation%0AAuthor%3A%20Muhammed%20Burak%20Kizil%20and%20Enes%20Sanli%20and%20Niloy%20J.%20Mitra%20and%20Erkut%20Erdem%20and%20Aykut%20Erdem%20and%20Duygu%20Ceylan%0AAbstract%3A%20Video%20generation%20has%20achieved%20remarkable%20progress%20in%20visual%20fidelity%20and%20controllability%2C%20enabling%20conditioning%20on%20text%2C%20layout%2C%20or%20motion.%20Among%20these%2C%20motion%20control%20-%20specifying%20object%20dynamics%20and%20camera%20trajectories%20-%20is%20essential%20for%20composing%20complex%2C%20cinematic%20scenes%2C%20yet%20existing%20interfaces%20remain%20limited.%20We%20introduce%20LAMP%20that%20leverages%20large%20language%20models%20%28LLMs%29%20as%20motion%20planners%20to%20translate%20natural%20language%20descriptions%20into%20explicit%203D%20trajectories%20for%20dynamic%20objects%20and%20%28relatively%20defined%29%20cameras.%20LAMP%20defines%20a%20motion%20domain-specific%20language%20%28DSL%29%2C%20inspired%20by%20cinematography%20conventions.%20By%20harnessing%20program%20synthesis%20capabilities%20of%20LLMs%2C%20LAMP%20generates%20structured%20motion%20programs%20from%20natural%20language%2C%20which%20are%20deterministically%20mapped%20to%203D%20trajectories.%20We%20construct%20a%20large-scale%20procedural%20dataset%20pairing%20natural%20text%20descriptions%20with%20corresponding%20motion%20programs%20and%203D%20trajectories.%20Experiments%20demonstrate%20LAMP%27s%20improved%20performance%20in%20motion%20controllability%20and%20alignment%20with%20user%20intent%20compared%20to%20state-of-the-art%20alternatives%20establishing%20the%20first%20framework%20for%20generating%20both%20object%20and%20camera%20motions%20directly%20from%20natural%20language%20specifications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAMP%253A%2520Language-Assisted%2520Motion%2520Planning%2520for%2520Controllable%2520Video%2520Generation%26entry.906535625%3DMuhammed%2520Burak%2520Kizil%2520and%2520Enes%2520Sanli%2520and%2520Niloy%2520J.%2520Mitra%2520and%2520Erkut%2520Erdem%2520and%2520Aykut%2520Erdem%2520and%2520Duygu%2520Ceylan%26entry.1292438233%3DVideo%2520generation%2520has%2520achieved%2520remarkable%2520progress%2520in%2520visual%2520fidelity%2520and%2520controllability%252C%2520enabling%2520conditioning%2520on%2520text%252C%2520layout%252C%2520or%2520motion.%2520Among%2520these%252C%2520motion%2520control%2520-%2520specifying%2520object%2520dynamics%2520and%2520camera%2520trajectories%2520-%2520is%2520essential%2520for%2520composing%2520complex%252C%2520cinematic%2520scenes%252C%2520yet%2520existing%2520interfaces%2520remain%2520limited.%2520We%2520introduce%2520LAMP%2520that%2520leverages%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520motion%2520planners%2520to%2520translate%2520natural%2520language%2520descriptions%2520into%2520explicit%25203D%2520trajectories%2520for%2520dynamic%2520objects%2520and%2520%2528relatively%2520defined%2529%2520cameras.%2520LAMP%2520defines%2520a%2520motion%2520domain-specific%2520language%2520%2528DSL%2529%252C%2520inspired%2520by%2520cinematography%2520conventions.%2520By%2520harnessing%2520program%2520synthesis%2520capabilities%2520of%2520LLMs%252C%2520LAMP%2520generates%2520structured%2520motion%2520programs%2520from%2520natural%2520language%252C%2520which%2520are%2520deterministically%2520mapped%2520to%25203D%2520trajectories.%2520We%2520construct%2520a%2520large-scale%2520procedural%2520dataset%2520pairing%2520natural%2520text%2520descriptions%2520with%2520corresponding%2520motion%2520programs%2520and%25203D%2520trajectories.%2520Experiments%2520demonstrate%2520LAMP%2527s%2520improved%2520performance%2520in%2520motion%2520controllability%2520and%2520alignment%2520with%2520user%2520intent%2520compared%2520to%2520state-of-the-art%2520alternatives%2520establishing%2520the%2520first%2520framework%2520for%2520generating%2520both%2520object%2520and%2520camera%2520motions%2520directly%2520from%2520natural%2520language%2520specifications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAMP%3A%20Language-Assisted%20Motion%20Planning%20for%20Controllable%20Video%20Generation&entry.906535625=Muhammed%20Burak%20Kizil%20and%20Enes%20Sanli%20and%20Niloy%20J.%20Mitra%20and%20Erkut%20Erdem%20and%20Aykut%20Erdem%20and%20Duygu%20Ceylan&entry.1292438233=Video%20generation%20has%20achieved%20remarkable%20progress%20in%20visual%20fidelity%20and%20controllability%2C%20enabling%20conditioning%20on%20text%2C%20layout%2C%20or%20motion.%20Among%20these%2C%20motion%20control%20-%20specifying%20object%20dynamics%20and%20camera%20trajectories%20-%20is%20essential%20for%20composing%20complex%2C%20cinematic%20scenes%2C%20yet%20existing%20interfaces%20remain%20limited.%20We%20introduce%20LAMP%20that%20leverages%20large%20language%20models%20%28LLMs%29%20as%20motion%20planners%20to%20translate%20natural%20language%20descriptions%20into%20explicit%203D%20trajectories%20for%20dynamic%20objects%20and%20%28relatively%20defined%29%20cameras.%20LAMP%20defines%20a%20motion%20domain-specific%20language%20%28DSL%29%2C%20inspired%20by%20cinematography%20conventions.%20By%20harnessing%20program%20synthesis%20capabilities%20of%20LLMs%2C%20LAMP%20generates%20structured%20motion%20programs%20from%20natural%20language%2C%20which%20are%20deterministically%20mapped%20to%203D%20trajectories.%20We%20construct%20a%20large-scale%20procedural%20dataset%20pairing%20natural%20text%20descriptions%20with%20corresponding%20motion%20programs%20and%203D%20trajectories.%20Experiments%20demonstrate%20LAMP%27s%20improved%20performance%20in%20motion%20controllability%20and%20alignment%20with%20user%20intent%20compared%20to%20state-of-the-art%20alternatives%20establishing%20the%20first%20framework%20for%20generating%20both%20object%20and%20camera%20motions%20directly%20from%20natural%20language%20specifications.&entry.1838667208=http%3A//arxiv.org/abs/2512.03619v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


