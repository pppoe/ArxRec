<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241028.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal\n  Search Engines", "author": "Zhixin Zhang and Yiyuan Zhang and Xiaohan Ding and Xiangyu Yue", "abstract": "  Search engines enable the retrieval of unknown information with texts.\nHowever, traditional methods fall short when it comes to understanding\nunfamiliar visual content, such as identifying an object that the model has\nnever seen before. This challenge is particularly pronounced for large\nvision-language models (VLMs): if the model has not been exposed to the object\ndepicted in an image, it struggles to generate reliable answers to the user's\nquestion regarding that image. Moreover, as new objects and events continuously\nemerge, frequently updating VLMs is impractical due to heavy computational\nburdens. To address this limitation, we propose Vision Search Assistant, a\nnovel framework that facilitates collaboration between VLMs and web agents.\nThis approach leverages VLMs' visual understanding capabilities and web agents'\nreal-time information access to perform open-world Retrieval-Augmented\nGeneration via the web. By integrating visual and textual representations\nthrough this collaboration, the model can provide informed responses even when\nthe image is novel to the system. Extensive experiments conducted on both\nopen-set and closed-set QA benchmarks demonstrate that the Vision Search\nAssistant significantly outperforms the other models and can be widely applied\nto existing VLMs.\n", "link": "http://arxiv.org/abs/2410.21220v1", "date": "2024-10-28", "relevancy": 2.9982, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6222}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Search%20Assistant%3A%20Empower%20Vision-Language%20Models%20as%20Multimodal%0A%20%20Search%20Engines&body=Title%3A%20Vision%20Search%20Assistant%3A%20Empower%20Vision-Language%20Models%20as%20Multimodal%0A%20%20Search%20Engines%0AAuthor%3A%20Zhixin%20Zhang%20and%20Yiyuan%20Zhang%20and%20Xiaohan%20Ding%20and%20Xiangyu%20Yue%0AAbstract%3A%20%20%20Search%20engines%20enable%20the%20retrieval%20of%20unknown%20information%20with%20texts.%0AHowever%2C%20traditional%20methods%20fall%20short%20when%20it%20comes%20to%20understanding%0Aunfamiliar%20visual%20content%2C%20such%20as%20identifying%20an%20object%20that%20the%20model%20has%0Anever%20seen%20before.%20This%20challenge%20is%20particularly%20pronounced%20for%20large%0Avision-language%20models%20%28VLMs%29%3A%20if%20the%20model%20has%20not%20been%20exposed%20to%20the%20object%0Adepicted%20in%20an%20image%2C%20it%20struggles%20to%20generate%20reliable%20answers%20to%20the%20user%27s%0Aquestion%20regarding%20that%20image.%20Moreover%2C%20as%20new%20objects%20and%20events%20continuously%0Aemerge%2C%20frequently%20updating%20VLMs%20is%20impractical%20due%20to%20heavy%20computational%0Aburdens.%20To%20address%20this%20limitation%2C%20we%20propose%20Vision%20Search%20Assistant%2C%20a%0Anovel%20framework%20that%20facilitates%20collaboration%20between%20VLMs%20and%20web%20agents.%0AThis%20approach%20leverages%20VLMs%27%20visual%20understanding%20capabilities%20and%20web%20agents%27%0Areal-time%20information%20access%20to%20perform%20open-world%20Retrieval-Augmented%0AGeneration%20via%20the%20web.%20By%20integrating%20visual%20and%20textual%20representations%0Athrough%20this%20collaboration%2C%20the%20model%20can%20provide%20informed%20responses%20even%20when%0Athe%20image%20is%20novel%20to%20the%20system.%20Extensive%20experiments%20conducted%20on%20both%0Aopen-set%20and%20closed-set%20QA%20benchmarks%20demonstrate%20that%20the%20Vision%20Search%0AAssistant%20significantly%20outperforms%20the%20other%20models%20and%20can%20be%20widely%20applied%0Ato%20existing%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Search%2520Assistant%253A%2520Empower%2520Vision-Language%2520Models%2520as%2520Multimodal%250A%2520%2520Search%2520Engines%26entry.906535625%3DZhixin%2520Zhang%2520and%2520Yiyuan%2520Zhang%2520and%2520Xiaohan%2520Ding%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3D%2520%2520Search%2520engines%2520enable%2520the%2520retrieval%2520of%2520unknown%2520information%2520with%2520texts.%250AHowever%252C%2520traditional%2520methods%2520fall%2520short%2520when%2520it%2520comes%2520to%2520understanding%250Aunfamiliar%2520visual%2520content%252C%2520such%2520as%2520identifying%2520an%2520object%2520that%2520the%2520model%2520has%250Anever%2520seen%2520before.%2520This%2520challenge%2520is%2520particularly%2520pronounced%2520for%2520large%250Avision-language%2520models%2520%2528VLMs%2529%253A%2520if%2520the%2520model%2520has%2520not%2520been%2520exposed%2520to%2520the%2520object%250Adepicted%2520in%2520an%2520image%252C%2520it%2520struggles%2520to%2520generate%2520reliable%2520answers%2520to%2520the%2520user%2527s%250Aquestion%2520regarding%2520that%2520image.%2520Moreover%252C%2520as%2520new%2520objects%2520and%2520events%2520continuously%250Aemerge%252C%2520frequently%2520updating%2520VLMs%2520is%2520impractical%2520due%2520to%2520heavy%2520computational%250Aburdens.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Vision%2520Search%2520Assistant%252C%2520a%250Anovel%2520framework%2520that%2520facilitates%2520collaboration%2520between%2520VLMs%2520and%2520web%2520agents.%250AThis%2520approach%2520leverages%2520VLMs%2527%2520visual%2520understanding%2520capabilities%2520and%2520web%2520agents%2527%250Areal-time%2520information%2520access%2520to%2520perform%2520open-world%2520Retrieval-Augmented%250AGeneration%2520via%2520the%2520web.%2520By%2520integrating%2520visual%2520and%2520textual%2520representations%250Athrough%2520this%2520collaboration%252C%2520the%2520model%2520can%2520provide%2520informed%2520responses%2520even%2520when%250Athe%2520image%2520is%2520novel%2520to%2520the%2520system.%2520Extensive%2520experiments%2520conducted%2520on%2520both%250Aopen-set%2520and%2520closed-set%2520QA%2520benchmarks%2520demonstrate%2520that%2520the%2520Vision%2520Search%250AAssistant%2520significantly%2520outperforms%2520the%2520other%2520models%2520and%2520can%2520be%2520widely%2520applied%250Ato%2520existing%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Search%20Assistant%3A%20Empower%20Vision-Language%20Models%20as%20Multimodal%0A%20%20Search%20Engines&entry.906535625=Zhixin%20Zhang%20and%20Yiyuan%20Zhang%20and%20Xiaohan%20Ding%20and%20Xiangyu%20Yue&entry.1292438233=%20%20Search%20engines%20enable%20the%20retrieval%20of%20unknown%20information%20with%20texts.%0AHowever%2C%20traditional%20methods%20fall%20short%20when%20it%20comes%20to%20understanding%0Aunfamiliar%20visual%20content%2C%20such%20as%20identifying%20an%20object%20that%20the%20model%20has%0Anever%20seen%20before.%20This%20challenge%20is%20particularly%20pronounced%20for%20large%0Avision-language%20models%20%28VLMs%29%3A%20if%20the%20model%20has%20not%20been%20exposed%20to%20the%20object%0Adepicted%20in%20an%20image%2C%20it%20struggles%20to%20generate%20reliable%20answers%20to%20the%20user%27s%0Aquestion%20regarding%20that%20image.%20Moreover%2C%20as%20new%20objects%20and%20events%20continuously%0Aemerge%2C%20frequently%20updating%20VLMs%20is%20impractical%20due%20to%20heavy%20computational%0Aburdens.%20To%20address%20this%20limitation%2C%20we%20propose%20Vision%20Search%20Assistant%2C%20a%0Anovel%20framework%20that%20facilitates%20collaboration%20between%20VLMs%20and%20web%20agents.%0AThis%20approach%20leverages%20VLMs%27%20visual%20understanding%20capabilities%20and%20web%20agents%27%0Areal-time%20information%20access%20to%20perform%20open-world%20Retrieval-Augmented%0AGeneration%20via%20the%20web.%20By%20integrating%20visual%20and%20textual%20representations%0Athrough%20this%20collaboration%2C%20the%20model%20can%20provide%20informed%20responses%20even%20when%0Athe%20image%20is%20novel%20to%20the%20system.%20Extensive%20experiments%20conducted%20on%20both%0Aopen-set%20and%20closed-set%20QA%20benchmarks%20demonstrate%20that%20the%20Vision%20Search%0AAssistant%20significantly%20outperforms%20the%20other%20models%20and%20can%20be%20widely%20applied%0Ato%20existing%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21220v1&entry.124074799=Read"},
{"title": "Exploring contextual modeling with linear complexity for point cloud\n  segmentation", "author": "Yong Xien Chng and Xuchong Qiu and Yizeng Han and Yifan Pu and Jiewei Cao and Gao Huang", "abstract": "  Point cloud segmentation is an important topic in 3D understanding that has\ntraditionally has been tackled using either the CNN or Transformer. Recently,\nMamba has emerged as a promising alternative, offering efficient long-range\ncontextual modeling capabilities without the quadratic complexity associated\nwith Transformer's attention mechanisms. However, despite Mamba's potential,\nearly efforts have all failed to achieve better performance than the best\nCNN-based and Transformer-based methods. In this work, we address this\nchallenge by identifying the key components of an effective and efficient point\ncloud segmentation architecture. Specifically, we show that: 1) Spatial\nlocality and robust contextual understanding are critical for strong\nperformance, and 2) Mamba features linear computational complexity, offering\nsuperior data and inference efficiency compared to Transformers, while still\nbeing capable of delivering strong contextual understanding. Additionally, we\nfurther enhance the standard Mamba specifically for point cloud segmentation by\nidentifying its two key shortcomings. First, the enforced causality in the\noriginal Mamba is unsuitable for processing point clouds that have no such\ndependencies. Second, its unidirectional scanning strategy imposes a\ndirectional bias, hampering its ability to capture the full context of\nunordered point clouds in a single pass. To address these issues, we carefully\nremove the causal convolutions and introduce a novel Strided Bidirectional SSM\nto enhance the model's capability to capture spatial relationships. Our efforts\nculminate in the development of a novel architecture named MEEPO, which\neffectively integrates the strengths of CNN and Mamba. MEEPO surpasses the\nprevious state-of-the-art method, PTv3, by up to +0.8 mIoU on multiple key\nbenchmark datasets, while being 42.1% faster and 5.53x more memory efficient.\n", "link": "http://arxiv.org/abs/2410.21211v1", "date": "2024-10-28", "relevancy": 2.8779, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5776}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20contextual%20modeling%20with%20linear%20complexity%20for%20point%20cloud%0A%20%20segmentation&body=Title%3A%20Exploring%20contextual%20modeling%20with%20linear%20complexity%20for%20point%20cloud%0A%20%20segmentation%0AAuthor%3A%20Yong%20Xien%20Chng%20and%20Xuchong%20Qiu%20and%20Yizeng%20Han%20and%20Yifan%20Pu%20and%20Jiewei%20Cao%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Point%20cloud%20segmentation%20is%20an%20important%20topic%20in%203D%20understanding%20that%20has%0Atraditionally%20has%20been%20tackled%20using%20either%20the%20CNN%20or%20Transformer.%20Recently%2C%0AMamba%20has%20emerged%20as%20a%20promising%20alternative%2C%20offering%20efficient%20long-range%0Acontextual%20modeling%20capabilities%20without%20the%20quadratic%20complexity%20associated%0Awith%20Transformer%27s%20attention%20mechanisms.%20However%2C%20despite%20Mamba%27s%20potential%2C%0Aearly%20efforts%20have%20all%20failed%20to%20achieve%20better%20performance%20than%20the%20best%0ACNN-based%20and%20Transformer-based%20methods.%20In%20this%20work%2C%20we%20address%20this%0Achallenge%20by%20identifying%20the%20key%20components%20of%20an%20effective%20and%20efficient%20point%0Acloud%20segmentation%20architecture.%20Specifically%2C%20we%20show%20that%3A%201%29%20Spatial%0Alocality%20and%20robust%20contextual%20understanding%20are%20critical%20for%20strong%0Aperformance%2C%20and%202%29%20Mamba%20features%20linear%20computational%20complexity%2C%20offering%0Asuperior%20data%20and%20inference%20efficiency%20compared%20to%20Transformers%2C%20while%20still%0Abeing%20capable%20of%20delivering%20strong%20contextual%20understanding.%20Additionally%2C%20we%0Afurther%20enhance%20the%20standard%20Mamba%20specifically%20for%20point%20cloud%20segmentation%20by%0Aidentifying%20its%20two%20key%20shortcomings.%20First%2C%20the%20enforced%20causality%20in%20the%0Aoriginal%20Mamba%20is%20unsuitable%20for%20processing%20point%20clouds%20that%20have%20no%20such%0Adependencies.%20Second%2C%20its%20unidirectional%20scanning%20strategy%20imposes%20a%0Adirectional%20bias%2C%20hampering%20its%20ability%20to%20capture%20the%20full%20context%20of%0Aunordered%20point%20clouds%20in%20a%20single%20pass.%20To%20address%20these%20issues%2C%20we%20carefully%0Aremove%20the%20causal%20convolutions%20and%20introduce%20a%20novel%20Strided%20Bidirectional%20SSM%0Ato%20enhance%20the%20model%27s%20capability%20to%20capture%20spatial%20relationships.%20Our%20efforts%0Aculminate%20in%20the%20development%20of%20a%20novel%20architecture%20named%20MEEPO%2C%20which%0Aeffectively%20integrates%20the%20strengths%20of%20CNN%20and%20Mamba.%20MEEPO%20surpasses%20the%0Aprevious%20state-of-the-art%20method%2C%20PTv3%2C%20by%20up%20to%20%2B0.8%20mIoU%20on%20multiple%20key%0Abenchmark%20datasets%2C%20while%20being%2042.1%25%20faster%20and%205.53x%20more%20memory%20efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520contextual%2520modeling%2520with%2520linear%2520complexity%2520for%2520point%2520cloud%250A%2520%2520segmentation%26entry.906535625%3DYong%2520Xien%2520Chng%2520and%2520Xuchong%2520Qiu%2520and%2520Yizeng%2520Han%2520and%2520Yifan%2520Pu%2520and%2520Jiewei%2520Cao%2520and%2520Gao%2520Huang%26entry.1292438233%3D%2520%2520Point%2520cloud%2520segmentation%2520is%2520an%2520important%2520topic%2520in%25203D%2520understanding%2520that%2520has%250Atraditionally%2520has%2520been%2520tackled%2520using%2520either%2520the%2520CNN%2520or%2520Transformer.%2520Recently%252C%250AMamba%2520has%2520emerged%2520as%2520a%2520promising%2520alternative%252C%2520offering%2520efficient%2520long-range%250Acontextual%2520modeling%2520capabilities%2520without%2520the%2520quadratic%2520complexity%2520associated%250Awith%2520Transformer%2527s%2520attention%2520mechanisms.%2520However%252C%2520despite%2520Mamba%2527s%2520potential%252C%250Aearly%2520efforts%2520have%2520all%2520failed%2520to%2520achieve%2520better%2520performance%2520than%2520the%2520best%250ACNN-based%2520and%2520Transformer-based%2520methods.%2520In%2520this%2520work%252C%2520we%2520address%2520this%250Achallenge%2520by%2520identifying%2520the%2520key%2520components%2520of%2520an%2520effective%2520and%2520efficient%2520point%250Acloud%2520segmentation%2520architecture.%2520Specifically%252C%2520we%2520show%2520that%253A%25201%2529%2520Spatial%250Alocality%2520and%2520robust%2520contextual%2520understanding%2520are%2520critical%2520for%2520strong%250Aperformance%252C%2520and%25202%2529%2520Mamba%2520features%2520linear%2520computational%2520complexity%252C%2520offering%250Asuperior%2520data%2520and%2520inference%2520efficiency%2520compared%2520to%2520Transformers%252C%2520while%2520still%250Abeing%2520capable%2520of%2520delivering%2520strong%2520contextual%2520understanding.%2520Additionally%252C%2520we%250Afurther%2520enhance%2520the%2520standard%2520Mamba%2520specifically%2520for%2520point%2520cloud%2520segmentation%2520by%250Aidentifying%2520its%2520two%2520key%2520shortcomings.%2520First%252C%2520the%2520enforced%2520causality%2520in%2520the%250Aoriginal%2520Mamba%2520is%2520unsuitable%2520for%2520processing%2520point%2520clouds%2520that%2520have%2520no%2520such%250Adependencies.%2520Second%252C%2520its%2520unidirectional%2520scanning%2520strategy%2520imposes%2520a%250Adirectional%2520bias%252C%2520hampering%2520its%2520ability%2520to%2520capture%2520the%2520full%2520context%2520of%250Aunordered%2520point%2520clouds%2520in%2520a%2520single%2520pass.%2520To%2520address%2520these%2520issues%252C%2520we%2520carefully%250Aremove%2520the%2520causal%2520convolutions%2520and%2520introduce%2520a%2520novel%2520Strided%2520Bidirectional%2520SSM%250Ato%2520enhance%2520the%2520model%2527s%2520capability%2520to%2520capture%2520spatial%2520relationships.%2520Our%2520efforts%250Aculminate%2520in%2520the%2520development%2520of%2520a%2520novel%2520architecture%2520named%2520MEEPO%252C%2520which%250Aeffectively%2520integrates%2520the%2520strengths%2520of%2520CNN%2520and%2520Mamba.%2520MEEPO%2520surpasses%2520the%250Aprevious%2520state-of-the-art%2520method%252C%2520PTv3%252C%2520by%2520up%2520to%2520%252B0.8%2520mIoU%2520on%2520multiple%2520key%250Abenchmark%2520datasets%252C%2520while%2520being%252042.1%2525%2520faster%2520and%25205.53x%2520more%2520memory%2520efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20contextual%20modeling%20with%20linear%20complexity%20for%20point%20cloud%0A%20%20segmentation&entry.906535625=Yong%20Xien%20Chng%20and%20Xuchong%20Qiu%20and%20Yizeng%20Han%20and%20Yifan%20Pu%20and%20Jiewei%20Cao%20and%20Gao%20Huang&entry.1292438233=%20%20Point%20cloud%20segmentation%20is%20an%20important%20topic%20in%203D%20understanding%20that%20has%0Atraditionally%20has%20been%20tackled%20using%20either%20the%20CNN%20or%20Transformer.%20Recently%2C%0AMamba%20has%20emerged%20as%20a%20promising%20alternative%2C%20offering%20efficient%20long-range%0Acontextual%20modeling%20capabilities%20without%20the%20quadratic%20complexity%20associated%0Awith%20Transformer%27s%20attention%20mechanisms.%20However%2C%20despite%20Mamba%27s%20potential%2C%0Aearly%20efforts%20have%20all%20failed%20to%20achieve%20better%20performance%20than%20the%20best%0ACNN-based%20and%20Transformer-based%20methods.%20In%20this%20work%2C%20we%20address%20this%0Achallenge%20by%20identifying%20the%20key%20components%20of%20an%20effective%20and%20efficient%20point%0Acloud%20segmentation%20architecture.%20Specifically%2C%20we%20show%20that%3A%201%29%20Spatial%0Alocality%20and%20robust%20contextual%20understanding%20are%20critical%20for%20strong%0Aperformance%2C%20and%202%29%20Mamba%20features%20linear%20computational%20complexity%2C%20offering%0Asuperior%20data%20and%20inference%20efficiency%20compared%20to%20Transformers%2C%20while%20still%0Abeing%20capable%20of%20delivering%20strong%20contextual%20understanding.%20Additionally%2C%20we%0Afurther%20enhance%20the%20standard%20Mamba%20specifically%20for%20point%20cloud%20segmentation%20by%0Aidentifying%20its%20two%20key%20shortcomings.%20First%2C%20the%20enforced%20causality%20in%20the%0Aoriginal%20Mamba%20is%20unsuitable%20for%20processing%20point%20clouds%20that%20have%20no%20such%0Adependencies.%20Second%2C%20its%20unidirectional%20scanning%20strategy%20imposes%20a%0Adirectional%20bias%2C%20hampering%20its%20ability%20to%20capture%20the%20full%20context%20of%0Aunordered%20point%20clouds%20in%20a%20single%20pass.%20To%20address%20these%20issues%2C%20we%20carefully%0Aremove%20the%20causal%20convolutions%20and%20introduce%20a%20novel%20Strided%20Bidirectional%20SSM%0Ato%20enhance%20the%20model%27s%20capability%20to%20capture%20spatial%20relationships.%20Our%20efforts%0Aculminate%20in%20the%20development%20of%20a%20novel%20architecture%20named%20MEEPO%2C%20which%0Aeffectively%20integrates%20the%20strengths%20of%20CNN%20and%20Mamba.%20MEEPO%20surpasses%20the%0Aprevious%20state-of-the-art%20method%2C%20PTv3%2C%20by%20up%20to%20%2B0.8%20mIoU%20on%20multiple%20key%0Abenchmark%20datasets%2C%20while%20being%2042.1%25%20faster%20and%205.53x%20more%20memory%20efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21211v1&entry.124074799=Read"},
{"title": "AutoBench-V: Can Large Vision-Language Models Benchmark Themselves?", "author": "Han Bao and Yue Huang and Yanbo Wang and Jiayi Ye and Xiangqi Wang and Xiuyin Chen and Mohamed Elhoseiny and Xiangliang Zhang", "abstract": "  Large Vision-Language Models (LVLMs) have become essential for advancing the\nintegration of visual and linguistic information, facilitating a wide range of\ncomplex applications and tasks. However, the evaluation of LVLMs presents\nsignificant challenges as the evaluation benchmark always demands lots of human\ncost for its construction, and remains static, lacking flexibility once\nconstructed. Even though automatic evaluation has been explored in textual\nmodality, the visual modality remains under-explored. As a result, in this\nwork, we address a question: \"Can LVLMs serve as a path to automatic\nbenchmarking?\". We introduce AutoBench-V, an automated framework for serving\nevaluation on demand, i.e., benchmarking LVLMs based on specific aspects of\nmodel capability. Upon receiving an evaluation capability, AutoBench-V\nleverages text-to-image models to generate relevant image samples and then\nutilizes LVLMs to orchestrate visual question-answering (VQA) tasks, completing\nthe evaluation process efficiently and flexibly. Through an extensive\nevaluation of seven popular LVLMs across five demanded user inputs (i.e.,\nevaluation capabilities), the framework shows effectiveness and reliability. We\nobserve the following: (1) Our constructed benchmark accurately reflects\nvarying task difficulties; (2) As task difficulty rises, the performance gap\nbetween models widens; (3) While models exhibit strong performance in abstract\nlevel understanding, they underperform in details reasoning tasks; and (4)\nConstructing a dataset with varying levels of difficulties is critical for a\ncomprehensive and exhaustive evaluation. Overall, AutoBench-V not only\nsuccessfully utilizes LVLMs for automated benchmarking but also reveals that\nLVLMs as judges have significant potential in various domains.\n", "link": "http://arxiv.org/abs/2410.21259v1", "date": "2024-10-28", "relevancy": 2.8464, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoBench-V%3A%20Can%20Large%20Vision-Language%20Models%20Benchmark%20Themselves%3F&body=Title%3A%20AutoBench-V%3A%20Can%20Large%20Vision-Language%20Models%20Benchmark%20Themselves%3F%0AAuthor%3A%20Han%20Bao%20and%20Yue%20Huang%20and%20Yanbo%20Wang%20and%20Jiayi%20Ye%20and%20Xiangqi%20Wang%20and%20Xiuyin%20Chen%20and%20Mohamed%20Elhoseiny%20and%20Xiangliang%20Zhang%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20become%20essential%20for%20advancing%20the%0Aintegration%20of%20visual%20and%20linguistic%20information%2C%20facilitating%20a%20wide%20range%20of%0Acomplex%20applications%20and%20tasks.%20However%2C%20the%20evaluation%20of%20LVLMs%20presents%0Asignificant%20challenges%20as%20the%20evaluation%20benchmark%20always%20demands%20lots%20of%20human%0Acost%20for%20its%20construction%2C%20and%20remains%20static%2C%20lacking%20flexibility%20once%0Aconstructed.%20Even%20though%20automatic%20evaluation%20has%20been%20explored%20in%20textual%0Amodality%2C%20the%20visual%20modality%20remains%20under-explored.%20As%20a%20result%2C%20in%20this%0Awork%2C%20we%20address%20a%20question%3A%20%22Can%20LVLMs%20serve%20as%20a%20path%20to%20automatic%0Abenchmarking%3F%22.%20We%20introduce%20AutoBench-V%2C%20an%20automated%20framework%20for%20serving%0Aevaluation%20on%20demand%2C%20i.e.%2C%20benchmarking%20LVLMs%20based%20on%20specific%20aspects%20of%0Amodel%20capability.%20Upon%20receiving%20an%20evaluation%20capability%2C%20AutoBench-V%0Aleverages%20text-to-image%20models%20to%20generate%20relevant%20image%20samples%20and%20then%0Autilizes%20LVLMs%20to%20orchestrate%20visual%20question-answering%20%28VQA%29%20tasks%2C%20completing%0Athe%20evaluation%20process%20efficiently%20and%20flexibly.%20Through%20an%20extensive%0Aevaluation%20of%20seven%20popular%20LVLMs%20across%20five%20demanded%20user%20inputs%20%28i.e.%2C%0Aevaluation%20capabilities%29%2C%20the%20framework%20shows%20effectiveness%20and%20reliability.%20We%0Aobserve%20the%20following%3A%20%281%29%20Our%20constructed%20benchmark%20accurately%20reflects%0Avarying%20task%20difficulties%3B%20%282%29%20As%20task%20difficulty%20rises%2C%20the%20performance%20gap%0Abetween%20models%20widens%3B%20%283%29%20While%20models%20exhibit%20strong%20performance%20in%20abstract%0Alevel%20understanding%2C%20they%20underperform%20in%20details%20reasoning%20tasks%3B%20and%20%284%29%0AConstructing%20a%20dataset%20with%20varying%20levels%20of%20difficulties%20is%20critical%20for%20a%0Acomprehensive%20and%20exhaustive%20evaluation.%20Overall%2C%20AutoBench-V%20not%20only%0Asuccessfully%20utilizes%20LVLMs%20for%20automated%20benchmarking%20but%20also%20reveals%20that%0ALVLMs%20as%20judges%20have%20significant%20potential%20in%20various%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoBench-V%253A%2520Can%2520Large%2520Vision-Language%2520Models%2520Benchmark%2520Themselves%253F%26entry.906535625%3DHan%2520Bao%2520and%2520Yue%2520Huang%2520and%2520Yanbo%2520Wang%2520and%2520Jiayi%2520Ye%2520and%2520Xiangqi%2520Wang%2520and%2520Xiuyin%2520Chen%2520and%2520Mohamed%2520Elhoseiny%2520and%2520Xiangliang%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520become%2520essential%2520for%2520advancing%2520the%250Aintegration%2520of%2520visual%2520and%2520linguistic%2520information%252C%2520facilitating%2520a%2520wide%2520range%2520of%250Acomplex%2520applications%2520and%2520tasks.%2520However%252C%2520the%2520evaluation%2520of%2520LVLMs%2520presents%250Asignificant%2520challenges%2520as%2520the%2520evaluation%2520benchmark%2520always%2520demands%2520lots%2520of%2520human%250Acost%2520for%2520its%2520construction%252C%2520and%2520remains%2520static%252C%2520lacking%2520flexibility%2520once%250Aconstructed.%2520Even%2520though%2520automatic%2520evaluation%2520has%2520been%2520explored%2520in%2520textual%250Amodality%252C%2520the%2520visual%2520modality%2520remains%2520under-explored.%2520As%2520a%2520result%252C%2520in%2520this%250Awork%252C%2520we%2520address%2520a%2520question%253A%2520%2522Can%2520LVLMs%2520serve%2520as%2520a%2520path%2520to%2520automatic%250Abenchmarking%253F%2522.%2520We%2520introduce%2520AutoBench-V%252C%2520an%2520automated%2520framework%2520for%2520serving%250Aevaluation%2520on%2520demand%252C%2520i.e.%252C%2520benchmarking%2520LVLMs%2520based%2520on%2520specific%2520aspects%2520of%250Amodel%2520capability.%2520Upon%2520receiving%2520an%2520evaluation%2520capability%252C%2520AutoBench-V%250Aleverages%2520text-to-image%2520models%2520to%2520generate%2520relevant%2520image%2520samples%2520and%2520then%250Autilizes%2520LVLMs%2520to%2520orchestrate%2520visual%2520question-answering%2520%2528VQA%2529%2520tasks%252C%2520completing%250Athe%2520evaluation%2520process%2520efficiently%2520and%2520flexibly.%2520Through%2520an%2520extensive%250Aevaluation%2520of%2520seven%2520popular%2520LVLMs%2520across%2520five%2520demanded%2520user%2520inputs%2520%2528i.e.%252C%250Aevaluation%2520capabilities%2529%252C%2520the%2520framework%2520shows%2520effectiveness%2520and%2520reliability.%2520We%250Aobserve%2520the%2520following%253A%2520%25281%2529%2520Our%2520constructed%2520benchmark%2520accurately%2520reflects%250Avarying%2520task%2520difficulties%253B%2520%25282%2529%2520As%2520task%2520difficulty%2520rises%252C%2520the%2520performance%2520gap%250Abetween%2520models%2520widens%253B%2520%25283%2529%2520While%2520models%2520exhibit%2520strong%2520performance%2520in%2520abstract%250Alevel%2520understanding%252C%2520they%2520underperform%2520in%2520details%2520reasoning%2520tasks%253B%2520and%2520%25284%2529%250AConstructing%2520a%2520dataset%2520with%2520varying%2520levels%2520of%2520difficulties%2520is%2520critical%2520for%2520a%250Acomprehensive%2520and%2520exhaustive%2520evaluation.%2520Overall%252C%2520AutoBench-V%2520not%2520only%250Asuccessfully%2520utilizes%2520LVLMs%2520for%2520automated%2520benchmarking%2520but%2520also%2520reveals%2520that%250ALVLMs%2520as%2520judges%2520have%2520significant%2520potential%2520in%2520various%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoBench-V%3A%20Can%20Large%20Vision-Language%20Models%20Benchmark%20Themselves%3F&entry.906535625=Han%20Bao%20and%20Yue%20Huang%20and%20Yanbo%20Wang%20and%20Jiayi%20Ye%20and%20Xiangqi%20Wang%20and%20Xiuyin%20Chen%20and%20Mohamed%20Elhoseiny%20and%20Xiangliang%20Zhang&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20become%20essential%20for%20advancing%20the%0Aintegration%20of%20visual%20and%20linguistic%20information%2C%20facilitating%20a%20wide%20range%20of%0Acomplex%20applications%20and%20tasks.%20However%2C%20the%20evaluation%20of%20LVLMs%20presents%0Asignificant%20challenges%20as%20the%20evaluation%20benchmark%20always%20demands%20lots%20of%20human%0Acost%20for%20its%20construction%2C%20and%20remains%20static%2C%20lacking%20flexibility%20once%0Aconstructed.%20Even%20though%20automatic%20evaluation%20has%20been%20explored%20in%20textual%0Amodality%2C%20the%20visual%20modality%20remains%20under-explored.%20As%20a%20result%2C%20in%20this%0Awork%2C%20we%20address%20a%20question%3A%20%22Can%20LVLMs%20serve%20as%20a%20path%20to%20automatic%0Abenchmarking%3F%22.%20We%20introduce%20AutoBench-V%2C%20an%20automated%20framework%20for%20serving%0Aevaluation%20on%20demand%2C%20i.e.%2C%20benchmarking%20LVLMs%20based%20on%20specific%20aspects%20of%0Amodel%20capability.%20Upon%20receiving%20an%20evaluation%20capability%2C%20AutoBench-V%0Aleverages%20text-to-image%20models%20to%20generate%20relevant%20image%20samples%20and%20then%0Autilizes%20LVLMs%20to%20orchestrate%20visual%20question-answering%20%28VQA%29%20tasks%2C%20completing%0Athe%20evaluation%20process%20efficiently%20and%20flexibly.%20Through%20an%20extensive%0Aevaluation%20of%20seven%20popular%20LVLMs%20across%20five%20demanded%20user%20inputs%20%28i.e.%2C%0Aevaluation%20capabilities%29%2C%20the%20framework%20shows%20effectiveness%20and%20reliability.%20We%0Aobserve%20the%20following%3A%20%281%29%20Our%20constructed%20benchmark%20accurately%20reflects%0Avarying%20task%20difficulties%3B%20%282%29%20As%20task%20difficulty%20rises%2C%20the%20performance%20gap%0Abetween%20models%20widens%3B%20%283%29%20While%20models%20exhibit%20strong%20performance%20in%20abstract%0Alevel%20understanding%2C%20they%20underperform%20in%20details%20reasoning%20tasks%3B%20and%20%284%29%0AConstructing%20a%20dataset%20with%20varying%20levels%20of%20difficulties%20is%20critical%20for%20a%0Acomprehensive%20and%20exhaustive%20evaluation.%20Overall%2C%20AutoBench-V%20not%20only%0Asuccessfully%20utilizes%20LVLMs%20for%20automated%20benchmarking%20but%20also%20reveals%20that%0ALVLMs%20as%20judges%20have%20significant%20potential%20in%20various%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21259v1&entry.124074799=Read"},
{"title": "SAM 2: Segment Anything in Images and Videos", "author": "Nikhila Ravi and Valentin Gabeur and Yuan-Ting Hu and Ronghang Hu and Chaitanya Ryali and Tengyu Ma and Haitham Khedr and Roman R\u00e4dle and Chloe Rolland and Laura Gustafson and Eric Mintun and Junting Pan and Kalyan Vasudev Alwala and Nicolas Carion and Chao-Yuan Wu and Ross Girshick and Piotr Doll\u00e1r and Christoph Feichtenhofer", "abstract": "  We present Segment Anything Model 2 (SAM 2), a foundation model towards\nsolving promptable visual segmentation in images and videos. We build a data\nengine, which improves model and data via user interaction, to collect the\nlargest video segmentation dataset to date. Our model is a simple transformer\narchitecture with streaming memory for real-time video processing. SAM 2\ntrained on our data provides strong performance across a wide range of tasks.\nIn video segmentation, we observe better accuracy, using 3x fewer interactions\nthan prior approaches. In image segmentation, our model is more accurate and 6x\nfaster than the Segment Anything Model (SAM). We believe that our data, model,\nand insights will serve as a significant milestone for video segmentation and\nrelated perception tasks. We are releasing our main model, dataset, as well as\ncode for model training and our demo.\n", "link": "http://arxiv.org/abs/2408.00714v2", "date": "2024-10-28", "relevancy": 2.7751, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM%202%3A%20Segment%20Anything%20in%20Images%20and%20Videos&body=Title%3A%20SAM%202%3A%20Segment%20Anything%20in%20Images%20and%20Videos%0AAuthor%3A%20Nikhila%20Ravi%20and%20Valentin%20Gabeur%20and%20Yuan-Ting%20Hu%20and%20Ronghang%20Hu%20and%20Chaitanya%20Ryali%20and%20Tengyu%20Ma%20and%20Haitham%20Khedr%20and%20Roman%20R%C3%A4dle%20and%20Chloe%20Rolland%20and%20Laura%20Gustafson%20and%20Eric%20Mintun%20and%20Junting%20Pan%20and%20Kalyan%20Vasudev%20Alwala%20and%20Nicolas%20Carion%20and%20Chao-Yuan%20Wu%20and%20Ross%20Girshick%20and%20Piotr%20Doll%C3%A1r%20and%20Christoph%20Feichtenhofer%0AAbstract%3A%20%20%20We%20present%20Segment%20Anything%20Model%202%20%28SAM%202%29%2C%20a%20foundation%20model%20towards%0Asolving%20promptable%20visual%20segmentation%20in%20images%20and%20videos.%20We%20build%20a%20data%0Aengine%2C%20which%20improves%20model%20and%20data%20via%20user%20interaction%2C%20to%20collect%20the%0Alargest%20video%20segmentation%20dataset%20to%20date.%20Our%20model%20is%20a%20simple%20transformer%0Aarchitecture%20with%20streaming%20memory%20for%20real-time%20video%20processing.%20SAM%202%0Atrained%20on%20our%20data%20provides%20strong%20performance%20across%20a%20wide%20range%20of%20tasks.%0AIn%20video%20segmentation%2C%20we%20observe%20better%20accuracy%2C%20using%203x%20fewer%20interactions%0Athan%20prior%20approaches.%20In%20image%20segmentation%2C%20our%20model%20is%20more%20accurate%20and%206x%0Afaster%20than%20the%20Segment%20Anything%20Model%20%28SAM%29.%20We%20believe%20that%20our%20data%2C%20model%2C%0Aand%20insights%20will%20serve%20as%20a%20significant%20milestone%20for%20video%20segmentation%20and%0Arelated%20perception%20tasks.%20We%20are%20releasing%20our%20main%20model%2C%20dataset%2C%20as%20well%20as%0Acode%20for%20model%20training%20and%20our%20demo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00714v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM%25202%253A%2520Segment%2520Anything%2520in%2520Images%2520and%2520Videos%26entry.906535625%3DNikhila%2520Ravi%2520and%2520Valentin%2520Gabeur%2520and%2520Yuan-Ting%2520Hu%2520and%2520Ronghang%2520Hu%2520and%2520Chaitanya%2520Ryali%2520and%2520Tengyu%2520Ma%2520and%2520Haitham%2520Khedr%2520and%2520Roman%2520R%25C3%25A4dle%2520and%2520Chloe%2520Rolland%2520and%2520Laura%2520Gustafson%2520and%2520Eric%2520Mintun%2520and%2520Junting%2520Pan%2520and%2520Kalyan%2520Vasudev%2520Alwala%2520and%2520Nicolas%2520Carion%2520and%2520Chao-Yuan%2520Wu%2520and%2520Ross%2520Girshick%2520and%2520Piotr%2520Doll%25C3%25A1r%2520and%2520Christoph%2520Feichtenhofer%26entry.1292438233%3D%2520%2520We%2520present%2520Segment%2520Anything%2520Model%25202%2520%2528SAM%25202%2529%252C%2520a%2520foundation%2520model%2520towards%250Asolving%2520promptable%2520visual%2520segmentation%2520in%2520images%2520and%2520videos.%2520We%2520build%2520a%2520data%250Aengine%252C%2520which%2520improves%2520model%2520and%2520data%2520via%2520user%2520interaction%252C%2520to%2520collect%2520the%250Alargest%2520video%2520segmentation%2520dataset%2520to%2520date.%2520Our%2520model%2520is%2520a%2520simple%2520transformer%250Aarchitecture%2520with%2520streaming%2520memory%2520for%2520real-time%2520video%2520processing.%2520SAM%25202%250Atrained%2520on%2520our%2520data%2520provides%2520strong%2520performance%2520across%2520a%2520wide%2520range%2520of%2520tasks.%250AIn%2520video%2520segmentation%252C%2520we%2520observe%2520better%2520accuracy%252C%2520using%25203x%2520fewer%2520interactions%250Athan%2520prior%2520approaches.%2520In%2520image%2520segmentation%252C%2520our%2520model%2520is%2520more%2520accurate%2520and%25206x%250Afaster%2520than%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529.%2520We%2520believe%2520that%2520our%2520data%252C%2520model%252C%250Aand%2520insights%2520will%2520serve%2520as%2520a%2520significant%2520milestone%2520for%2520video%2520segmentation%2520and%250Arelated%2520perception%2520tasks.%2520We%2520are%2520releasing%2520our%2520main%2520model%252C%2520dataset%252C%2520as%2520well%2520as%250Acode%2520for%2520model%2520training%2520and%2520our%2520demo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00714v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM%202%3A%20Segment%20Anything%20in%20Images%20and%20Videos&entry.906535625=Nikhila%20Ravi%20and%20Valentin%20Gabeur%20and%20Yuan-Ting%20Hu%20and%20Ronghang%20Hu%20and%20Chaitanya%20Ryali%20and%20Tengyu%20Ma%20and%20Haitham%20Khedr%20and%20Roman%20R%C3%A4dle%20and%20Chloe%20Rolland%20and%20Laura%20Gustafson%20and%20Eric%20Mintun%20and%20Junting%20Pan%20and%20Kalyan%20Vasudev%20Alwala%20and%20Nicolas%20Carion%20and%20Chao-Yuan%20Wu%20and%20Ross%20Girshick%20and%20Piotr%20Doll%C3%A1r%20and%20Christoph%20Feichtenhofer&entry.1292438233=%20%20We%20present%20Segment%20Anything%20Model%202%20%28SAM%202%29%2C%20a%20foundation%20model%20towards%0Asolving%20promptable%20visual%20segmentation%20in%20images%20and%20videos.%20We%20build%20a%20data%0Aengine%2C%20which%20improves%20model%20and%20data%20via%20user%20interaction%2C%20to%20collect%20the%0Alargest%20video%20segmentation%20dataset%20to%20date.%20Our%20model%20is%20a%20simple%20transformer%0Aarchitecture%20with%20streaming%20memory%20for%20real-time%20video%20processing.%20SAM%202%0Atrained%20on%20our%20data%20provides%20strong%20performance%20across%20a%20wide%20range%20of%20tasks.%0AIn%20video%20segmentation%2C%20we%20observe%20better%20accuracy%2C%20using%203x%20fewer%20interactions%0Athan%20prior%20approaches.%20In%20image%20segmentation%2C%20our%20model%20is%20more%20accurate%20and%206x%0Afaster%20than%20the%20Segment%20Anything%20Model%20%28SAM%29.%20We%20believe%20that%20our%20data%2C%20model%2C%0Aand%20insights%20will%20serve%20as%20a%20significant%20milestone%20for%20video%20segmentation%20and%0Arelated%20perception%20tasks.%20We%20are%20releasing%20our%20main%20model%2C%20dataset%2C%20as%20well%20as%0Acode%20for%20model%20training%20and%20our%20demo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00714v2&entry.124074799=Read"},
{"title": "Artificial Generational Intelligence: Cultural Accumulation in\n  Reinforcement Learning", "author": "Jonathan Cook and Chris Lu and Edward Hughes and Joel Z. Leibo and Jakob Foerster", "abstract": "  Cultural accumulation drives the open-ended and diverse progress in\ncapabilities spanning human history. It builds an expanding body of knowledge\nand skills by combining individual exploration with inter-generational\ninformation transmission. Despite its widespread success among humans, the\ncapacity for artificial learning agents to accumulate culture remains\nunder-explored. In particular, approaches to reinforcement learning typically\nstrive for improvements over only a single lifetime. Generational algorithms\nthat do exist fail to capture the open-ended, emergent nature of cultural\naccumulation, which allows individuals to trade-off innovation and imitation.\nBuilding on the previously demonstrated ability for reinforcement learning\nagents to perform social learning, we find that training setups which balance\nthis with independent learning give rise to cultural accumulation. These\naccumulating agents outperform those trained for a single lifetime with the\nsame cumulative experience. We explore this accumulation by constructing two\nmodels under two distinct notions of a generation: episodic generations, in\nwhich accumulation occurs via in-context learning and train-time generations,\nin which accumulation occurs via in-weights learning. In-context and in-weights\ncultural accumulation can be interpreted as analogous to knowledge and skill\naccumulation, respectively. To the best of our knowledge, this work is the\nfirst to present general models that achieve emergent cultural accumulation in\nreinforcement learning, opening up new avenues towards more open-ended learning\nsystems, as well as presenting new opportunities for modelling human culture.\n", "link": "http://arxiv.org/abs/2406.00392v2", "date": "2024-10-28", "relevancy": 2.6701, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5967}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.508}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Generational%20Intelligence%3A%20Cultural%20Accumulation%20in%0A%20%20Reinforcement%20Learning&body=Title%3A%20Artificial%20Generational%20Intelligence%3A%20Cultural%20Accumulation%20in%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Jonathan%20Cook%20and%20Chris%20Lu%20and%20Edward%20Hughes%20and%20Joel%20Z.%20Leibo%20and%20Jakob%20Foerster%0AAbstract%3A%20%20%20Cultural%20accumulation%20drives%20the%20open-ended%20and%20diverse%20progress%20in%0Acapabilities%20spanning%20human%20history.%20It%20builds%20an%20expanding%20body%20of%20knowledge%0Aand%20skills%20by%20combining%20individual%20exploration%20with%20inter-generational%0Ainformation%20transmission.%20Despite%20its%20widespread%20success%20among%20humans%2C%20the%0Acapacity%20for%20artificial%20learning%20agents%20to%20accumulate%20culture%20remains%0Aunder-explored.%20In%20particular%2C%20approaches%20to%20reinforcement%20learning%20typically%0Astrive%20for%20improvements%20over%20only%20a%20single%20lifetime.%20Generational%20algorithms%0Athat%20do%20exist%20fail%20to%20capture%20the%20open-ended%2C%20emergent%20nature%20of%20cultural%0Aaccumulation%2C%20which%20allows%20individuals%20to%20trade-off%20innovation%20and%20imitation.%0ABuilding%20on%20the%20previously%20demonstrated%20ability%20for%20reinforcement%20learning%0Aagents%20to%20perform%20social%20learning%2C%20we%20find%20that%20training%20setups%20which%20balance%0Athis%20with%20independent%20learning%20give%20rise%20to%20cultural%20accumulation.%20These%0Aaccumulating%20agents%20outperform%20those%20trained%20for%20a%20single%20lifetime%20with%20the%0Asame%20cumulative%20experience.%20We%20explore%20this%20accumulation%20by%20constructing%20two%0Amodels%20under%20two%20distinct%20notions%20of%20a%20generation%3A%20episodic%20generations%2C%20in%0Awhich%20accumulation%20occurs%20via%20in-context%20learning%20and%20train-time%20generations%2C%0Ain%20which%20accumulation%20occurs%20via%20in-weights%20learning.%20In-context%20and%20in-weights%0Acultural%20accumulation%20can%20be%20interpreted%20as%20analogous%20to%20knowledge%20and%20skill%0Aaccumulation%2C%20respectively.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20is%20the%0Afirst%20to%20present%20general%20models%20that%20achieve%20emergent%20cultural%20accumulation%20in%0Areinforcement%20learning%2C%20opening%20up%20new%20avenues%20towards%20more%20open-ended%20learning%0Asystems%2C%20as%20well%20as%20presenting%20new%20opportunities%20for%20modelling%20human%20culture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00392v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Generational%2520Intelligence%253A%2520Cultural%2520Accumulation%2520in%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DJonathan%2520Cook%2520and%2520Chris%2520Lu%2520and%2520Edward%2520Hughes%2520and%2520Joel%2520Z.%2520Leibo%2520and%2520Jakob%2520Foerster%26entry.1292438233%3D%2520%2520Cultural%2520accumulation%2520drives%2520the%2520open-ended%2520and%2520diverse%2520progress%2520in%250Acapabilities%2520spanning%2520human%2520history.%2520It%2520builds%2520an%2520expanding%2520body%2520of%2520knowledge%250Aand%2520skills%2520by%2520combining%2520individual%2520exploration%2520with%2520inter-generational%250Ainformation%2520transmission.%2520Despite%2520its%2520widespread%2520success%2520among%2520humans%252C%2520the%250Acapacity%2520for%2520artificial%2520learning%2520agents%2520to%2520accumulate%2520culture%2520remains%250Aunder-explored.%2520In%2520particular%252C%2520approaches%2520to%2520reinforcement%2520learning%2520typically%250Astrive%2520for%2520improvements%2520over%2520only%2520a%2520single%2520lifetime.%2520Generational%2520algorithms%250Athat%2520do%2520exist%2520fail%2520to%2520capture%2520the%2520open-ended%252C%2520emergent%2520nature%2520of%2520cultural%250Aaccumulation%252C%2520which%2520allows%2520individuals%2520to%2520trade-off%2520innovation%2520and%2520imitation.%250ABuilding%2520on%2520the%2520previously%2520demonstrated%2520ability%2520for%2520reinforcement%2520learning%250Aagents%2520to%2520perform%2520social%2520learning%252C%2520we%2520find%2520that%2520training%2520setups%2520which%2520balance%250Athis%2520with%2520independent%2520learning%2520give%2520rise%2520to%2520cultural%2520accumulation.%2520These%250Aaccumulating%2520agents%2520outperform%2520those%2520trained%2520for%2520a%2520single%2520lifetime%2520with%2520the%250Asame%2520cumulative%2520experience.%2520We%2520explore%2520this%2520accumulation%2520by%2520constructing%2520two%250Amodels%2520under%2520two%2520distinct%2520notions%2520of%2520a%2520generation%253A%2520episodic%2520generations%252C%2520in%250Awhich%2520accumulation%2520occurs%2520via%2520in-context%2520learning%2520and%2520train-time%2520generations%252C%250Ain%2520which%2520accumulation%2520occurs%2520via%2520in-weights%2520learning.%2520In-context%2520and%2520in-weights%250Acultural%2520accumulation%2520can%2520be%2520interpreted%2520as%2520analogous%2520to%2520knowledge%2520and%2520skill%250Aaccumulation%252C%2520respectively.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%2520is%2520the%250Afirst%2520to%2520present%2520general%2520models%2520that%2520achieve%2520emergent%2520cultural%2520accumulation%2520in%250Areinforcement%2520learning%252C%2520opening%2520up%2520new%2520avenues%2520towards%2520more%2520open-ended%2520learning%250Asystems%252C%2520as%2520well%2520as%2520presenting%2520new%2520opportunities%2520for%2520modelling%2520human%2520culture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00392v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Generational%20Intelligence%3A%20Cultural%20Accumulation%20in%0A%20%20Reinforcement%20Learning&entry.906535625=Jonathan%20Cook%20and%20Chris%20Lu%20and%20Edward%20Hughes%20and%20Joel%20Z.%20Leibo%20and%20Jakob%20Foerster&entry.1292438233=%20%20Cultural%20accumulation%20drives%20the%20open-ended%20and%20diverse%20progress%20in%0Acapabilities%20spanning%20human%20history.%20It%20builds%20an%20expanding%20body%20of%20knowledge%0Aand%20skills%20by%20combining%20individual%20exploration%20with%20inter-generational%0Ainformation%20transmission.%20Despite%20its%20widespread%20success%20among%20humans%2C%20the%0Acapacity%20for%20artificial%20learning%20agents%20to%20accumulate%20culture%20remains%0Aunder-explored.%20In%20particular%2C%20approaches%20to%20reinforcement%20learning%20typically%0Astrive%20for%20improvements%20over%20only%20a%20single%20lifetime.%20Generational%20algorithms%0Athat%20do%20exist%20fail%20to%20capture%20the%20open-ended%2C%20emergent%20nature%20of%20cultural%0Aaccumulation%2C%20which%20allows%20individuals%20to%20trade-off%20innovation%20and%20imitation.%0ABuilding%20on%20the%20previously%20demonstrated%20ability%20for%20reinforcement%20learning%0Aagents%20to%20perform%20social%20learning%2C%20we%20find%20that%20training%20setups%20which%20balance%0Athis%20with%20independent%20learning%20give%20rise%20to%20cultural%20accumulation.%20These%0Aaccumulating%20agents%20outperform%20those%20trained%20for%20a%20single%20lifetime%20with%20the%0Asame%20cumulative%20experience.%20We%20explore%20this%20accumulation%20by%20constructing%20two%0Amodels%20under%20two%20distinct%20notions%20of%20a%20generation%3A%20episodic%20generations%2C%20in%0Awhich%20accumulation%20occurs%20via%20in-context%20learning%20and%20train-time%20generations%2C%0Ain%20which%20accumulation%20occurs%20via%20in-weights%20learning.%20In-context%20and%20in-weights%0Acultural%20accumulation%20can%20be%20interpreted%20as%20analogous%20to%20knowledge%20and%20skill%0Aaccumulation%2C%20respectively.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20is%20the%0Afirst%20to%20present%20general%20models%20that%20achieve%20emergent%20cultural%20accumulation%20in%0Areinforcement%20learning%2C%20opening%20up%20new%20avenues%20towards%20more%20open-ended%20learning%0Asystems%2C%20as%20well%20as%20presenting%20new%20opportunities%20for%20modelling%20human%20culture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00392v2&entry.124074799=Read"},
{"title": "HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced\n  Context Awareness and Extrapolation", "author": "Yuhan Chen and Ang Lv and Jian Luan and Bin Wang and Wei Liu", "abstract": "  Many positional encodings (PEs) are designed to exhibit long-term decay,\nbased on an entrenched and long-standing inductive opinion: tokens farther away\nfrom the current position carry less relevant information. We argue that\nlong-term decay is outdated in the era of LLMs, as LLMs are now applied to\ntasks demanding precise retrieval of in-context information from arbitrary\npositions. Firstly, we present empirical analyses on various PEs, demonstrating\nthat models inherently learn attention with only a local-decay pattern while\nforming a U-shape pattern globally, contradicting the principle of long-term\ndecay. Furthermore, we conduct a detailed analysis of rotary position encoding\n(RoPE, a prevalent relative positional encoding in LLMs), and found that the\nU-shape attention is caused by some learned components, which are also the key\nfactor limiting RoPE's expressiveness and extrapolation.Inspired by these\ninsights, we propose High-frequency rotary Position Encoding (HoPE). HoPE\nreplaces the specific components in RoPE with position-independent ones,\nretaining only high-frequency signals, which also breaks the principle of\nlong-term decay in theory. HoPE achieves two major advantages: (1) Without\nconstraints imposed by long-term decay, contradictory factors that limit\nspontaneous attention optimization and model extrapolation performance are\nremoved. (2) Components representing positions and semantics are are optimized.\nThese enhances model's context awareness and extrapolation, as validated by\nextensive experiments.\n", "link": "http://arxiv.org/abs/2410.21216v1", "date": "2024-10-28", "relevancy": 2.6071, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5248}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5248}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoPE%3A%20A%20Novel%20Positional%20Encoding%20Without%20Long-Term%20Decay%20for%20Enhanced%0A%20%20Context%20Awareness%20and%20Extrapolation&body=Title%3A%20HoPE%3A%20A%20Novel%20Positional%20Encoding%20Without%20Long-Term%20Decay%20for%20Enhanced%0A%20%20Context%20Awareness%20and%20Extrapolation%0AAuthor%3A%20Yuhan%20Chen%20and%20Ang%20Lv%20and%20Jian%20Luan%20and%20Bin%20Wang%20and%20Wei%20Liu%0AAbstract%3A%20%20%20Many%20positional%20encodings%20%28PEs%29%20are%20designed%20to%20exhibit%20long-term%20decay%2C%0Abased%20on%20an%20entrenched%20and%20long-standing%20inductive%20opinion%3A%20tokens%20farther%20away%0Afrom%20the%20current%20position%20carry%20less%20relevant%20information.%20We%20argue%20that%0Along-term%20decay%20is%20outdated%20in%20the%20era%20of%20LLMs%2C%20as%20LLMs%20are%20now%20applied%20to%0Atasks%20demanding%20precise%20retrieval%20of%20in-context%20information%20from%20arbitrary%0Apositions.%20Firstly%2C%20we%20present%20empirical%20analyses%20on%20various%20PEs%2C%20demonstrating%0Athat%20models%20inherently%20learn%20attention%20with%20only%20a%20local-decay%20pattern%20while%0Aforming%20a%20U-shape%20pattern%20globally%2C%20contradicting%20the%20principle%20of%20long-term%0Adecay.%20Furthermore%2C%20we%20conduct%20a%20detailed%20analysis%20of%20rotary%20position%20encoding%0A%28RoPE%2C%20a%20prevalent%20relative%20positional%20encoding%20in%20LLMs%29%2C%20and%20found%20that%20the%0AU-shape%20attention%20is%20caused%20by%20some%20learned%20components%2C%20which%20are%20also%20the%20key%0Afactor%20limiting%20RoPE%27s%20expressiveness%20and%20extrapolation.Inspired%20by%20these%0Ainsights%2C%20we%20propose%20High-frequency%20rotary%20Position%20Encoding%20%28HoPE%29.%20HoPE%0Areplaces%20the%20specific%20components%20in%20RoPE%20with%20position-independent%20ones%2C%0Aretaining%20only%20high-frequency%20signals%2C%20which%20also%20breaks%20the%20principle%20of%0Along-term%20decay%20in%20theory.%20HoPE%20achieves%20two%20major%20advantages%3A%20%281%29%20Without%0Aconstraints%20imposed%20by%20long-term%20decay%2C%20contradictory%20factors%20that%20limit%0Aspontaneous%20attention%20optimization%20and%20model%20extrapolation%20performance%20are%0Aremoved.%20%282%29%20Components%20representing%20positions%20and%20semantics%20are%20are%20optimized.%0AThese%20enhances%20model%27s%20context%20awareness%20and%20extrapolation%2C%20as%20validated%20by%0Aextensive%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoPE%253A%2520A%2520Novel%2520Positional%2520Encoding%2520Without%2520Long-Term%2520Decay%2520for%2520Enhanced%250A%2520%2520Context%2520Awareness%2520and%2520Extrapolation%26entry.906535625%3DYuhan%2520Chen%2520and%2520Ang%2520Lv%2520and%2520Jian%2520Luan%2520and%2520Bin%2520Wang%2520and%2520Wei%2520Liu%26entry.1292438233%3D%2520%2520Many%2520positional%2520encodings%2520%2528PEs%2529%2520are%2520designed%2520to%2520exhibit%2520long-term%2520decay%252C%250Abased%2520on%2520an%2520entrenched%2520and%2520long-standing%2520inductive%2520opinion%253A%2520tokens%2520farther%2520away%250Afrom%2520the%2520current%2520position%2520carry%2520less%2520relevant%2520information.%2520We%2520argue%2520that%250Along-term%2520decay%2520is%2520outdated%2520in%2520the%2520era%2520of%2520LLMs%252C%2520as%2520LLMs%2520are%2520now%2520applied%2520to%250Atasks%2520demanding%2520precise%2520retrieval%2520of%2520in-context%2520information%2520from%2520arbitrary%250Apositions.%2520Firstly%252C%2520we%2520present%2520empirical%2520analyses%2520on%2520various%2520PEs%252C%2520demonstrating%250Athat%2520models%2520inherently%2520learn%2520attention%2520with%2520only%2520a%2520local-decay%2520pattern%2520while%250Aforming%2520a%2520U-shape%2520pattern%2520globally%252C%2520contradicting%2520the%2520principle%2520of%2520long-term%250Adecay.%2520Furthermore%252C%2520we%2520conduct%2520a%2520detailed%2520analysis%2520of%2520rotary%2520position%2520encoding%250A%2528RoPE%252C%2520a%2520prevalent%2520relative%2520positional%2520encoding%2520in%2520LLMs%2529%252C%2520and%2520found%2520that%2520the%250AU-shape%2520attention%2520is%2520caused%2520by%2520some%2520learned%2520components%252C%2520which%2520are%2520also%2520the%2520key%250Afactor%2520limiting%2520RoPE%2527s%2520expressiveness%2520and%2520extrapolation.Inspired%2520by%2520these%250Ainsights%252C%2520we%2520propose%2520High-frequency%2520rotary%2520Position%2520Encoding%2520%2528HoPE%2529.%2520HoPE%250Areplaces%2520the%2520specific%2520components%2520in%2520RoPE%2520with%2520position-independent%2520ones%252C%250Aretaining%2520only%2520high-frequency%2520signals%252C%2520which%2520also%2520breaks%2520the%2520principle%2520of%250Along-term%2520decay%2520in%2520theory.%2520HoPE%2520achieves%2520two%2520major%2520advantages%253A%2520%25281%2529%2520Without%250Aconstraints%2520imposed%2520by%2520long-term%2520decay%252C%2520contradictory%2520factors%2520that%2520limit%250Aspontaneous%2520attention%2520optimization%2520and%2520model%2520extrapolation%2520performance%2520are%250Aremoved.%2520%25282%2529%2520Components%2520representing%2520positions%2520and%2520semantics%2520are%2520are%2520optimized.%250AThese%2520enhances%2520model%2527s%2520context%2520awareness%2520and%2520extrapolation%252C%2520as%2520validated%2520by%250Aextensive%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoPE%3A%20A%20Novel%20Positional%20Encoding%20Without%20Long-Term%20Decay%20for%20Enhanced%0A%20%20Context%20Awareness%20and%20Extrapolation&entry.906535625=Yuhan%20Chen%20and%20Ang%20Lv%20and%20Jian%20Luan%20and%20Bin%20Wang%20and%20Wei%20Liu&entry.1292438233=%20%20Many%20positional%20encodings%20%28PEs%29%20are%20designed%20to%20exhibit%20long-term%20decay%2C%0Abased%20on%20an%20entrenched%20and%20long-standing%20inductive%20opinion%3A%20tokens%20farther%20away%0Afrom%20the%20current%20position%20carry%20less%20relevant%20information.%20We%20argue%20that%0Along-term%20decay%20is%20outdated%20in%20the%20era%20of%20LLMs%2C%20as%20LLMs%20are%20now%20applied%20to%0Atasks%20demanding%20precise%20retrieval%20of%20in-context%20information%20from%20arbitrary%0Apositions.%20Firstly%2C%20we%20present%20empirical%20analyses%20on%20various%20PEs%2C%20demonstrating%0Athat%20models%20inherently%20learn%20attention%20with%20only%20a%20local-decay%20pattern%20while%0Aforming%20a%20U-shape%20pattern%20globally%2C%20contradicting%20the%20principle%20of%20long-term%0Adecay.%20Furthermore%2C%20we%20conduct%20a%20detailed%20analysis%20of%20rotary%20position%20encoding%0A%28RoPE%2C%20a%20prevalent%20relative%20positional%20encoding%20in%20LLMs%29%2C%20and%20found%20that%20the%0AU-shape%20attention%20is%20caused%20by%20some%20learned%20components%2C%20which%20are%20also%20the%20key%0Afactor%20limiting%20RoPE%27s%20expressiveness%20and%20extrapolation.Inspired%20by%20these%0Ainsights%2C%20we%20propose%20High-frequency%20rotary%20Position%20Encoding%20%28HoPE%29.%20HoPE%0Areplaces%20the%20specific%20components%20in%20RoPE%20with%20position-independent%20ones%2C%0Aretaining%20only%20high-frequency%20signals%2C%20which%20also%20breaks%20the%20principle%20of%0Along-term%20decay%20in%20theory.%20HoPE%20achieves%20two%20major%20advantages%3A%20%281%29%20Without%0Aconstraints%20imposed%20by%20long-term%20decay%2C%20contradictory%20factors%20that%20limit%0Aspontaneous%20attention%20optimization%20and%20model%20extrapolation%20performance%20are%0Aremoved.%20%282%29%20Components%20representing%20positions%20and%20semantics%20are%20are%20optimized.%0AThese%20enhances%20model%27s%20context%20awareness%20and%20extrapolation%2C%20as%20validated%20by%0Aextensive%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21216v1&entry.124074799=Read"},
{"title": "Large-scale cloze evaluation reveals that token prediction tasks are\n  neither lexically nor semantically aligned", "author": "Cassandra L. Jacobs and Lo\u00efc Grobol and Alvin Tsang", "abstract": "  In this work we compare the generative behavior at the next token prediction\nlevel in several language models by comparing them to human productions in the\ncloze task. We find that while large models trained for longer are typically\nbetter estimators of human productions, but they reliably under-estimate the\nprobabilities of human responses, over-rank rare responses, under-rank top\nresponses, and produce highly distinct semantic spaces. Altogether, this work\ndemonstrates in a tractable, interpretable domain that LM generations can not\nbe used as replacements of or models of the cloze task.\n", "link": "http://arxiv.org/abs/2410.12057v2", "date": "2024-10-28", "relevancy": 2.5186, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-scale%20cloze%20evaluation%20reveals%20that%20token%20prediction%20tasks%20are%0A%20%20neither%20lexically%20nor%20semantically%20aligned&body=Title%3A%20Large-scale%20cloze%20evaluation%20reveals%20that%20token%20prediction%20tasks%20are%0A%20%20neither%20lexically%20nor%20semantically%20aligned%0AAuthor%3A%20Cassandra%20L.%20Jacobs%20and%20Lo%C3%AFc%20Grobol%20and%20Alvin%20Tsang%0AAbstract%3A%20%20%20In%20this%20work%20we%20compare%20the%20generative%20behavior%20at%20the%20next%20token%20prediction%0Alevel%20in%20several%20language%20models%20by%20comparing%20them%20to%20human%20productions%20in%20the%0Acloze%20task.%20We%20find%20that%20while%20large%20models%20trained%20for%20longer%20are%20typically%0Abetter%20estimators%20of%20human%20productions%2C%20but%20they%20reliably%20under-estimate%20the%0Aprobabilities%20of%20human%20responses%2C%20over-rank%20rare%20responses%2C%20under-rank%20top%0Aresponses%2C%20and%20produce%20highly%20distinct%20semantic%20spaces.%20Altogether%2C%20this%20work%0Ademonstrates%20in%20a%20tractable%2C%20interpretable%20domain%20that%20LM%20generations%20can%20not%0Abe%20used%20as%20replacements%20of%20or%20models%20of%20the%20cloze%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12057v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-scale%2520cloze%2520evaluation%2520reveals%2520that%2520token%2520prediction%2520tasks%2520are%250A%2520%2520neither%2520lexically%2520nor%2520semantically%2520aligned%26entry.906535625%3DCassandra%2520L.%2520Jacobs%2520and%2520Lo%25C3%25AFc%2520Grobol%2520and%2520Alvin%2520Tsang%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520we%2520compare%2520the%2520generative%2520behavior%2520at%2520the%2520next%2520token%2520prediction%250Alevel%2520in%2520several%2520language%2520models%2520by%2520comparing%2520them%2520to%2520human%2520productions%2520in%2520the%250Acloze%2520task.%2520We%2520find%2520that%2520while%2520large%2520models%2520trained%2520for%2520longer%2520are%2520typically%250Abetter%2520estimators%2520of%2520human%2520productions%252C%2520but%2520they%2520reliably%2520under-estimate%2520the%250Aprobabilities%2520of%2520human%2520responses%252C%2520over-rank%2520rare%2520responses%252C%2520under-rank%2520top%250Aresponses%252C%2520and%2520produce%2520highly%2520distinct%2520semantic%2520spaces.%2520Altogether%252C%2520this%2520work%250Ademonstrates%2520in%2520a%2520tractable%252C%2520interpretable%2520domain%2520that%2520LM%2520generations%2520can%2520not%250Abe%2520used%2520as%2520replacements%2520of%2520or%2520models%2520of%2520the%2520cloze%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12057v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-scale%20cloze%20evaluation%20reveals%20that%20token%20prediction%20tasks%20are%0A%20%20neither%20lexically%20nor%20semantically%20aligned&entry.906535625=Cassandra%20L.%20Jacobs%20and%20Lo%C3%AFc%20Grobol%20and%20Alvin%20Tsang&entry.1292438233=%20%20In%20this%20work%20we%20compare%20the%20generative%20behavior%20at%20the%20next%20token%20prediction%0Alevel%20in%20several%20language%20models%20by%20comparing%20them%20to%20human%20productions%20in%20the%0Acloze%20task.%20We%20find%20that%20while%20large%20models%20trained%20for%20longer%20are%20typically%0Abetter%20estimators%20of%20human%20productions%2C%20but%20they%20reliably%20under-estimate%20the%0Aprobabilities%20of%20human%20responses%2C%20over-rank%20rare%20responses%2C%20under-rank%20top%0Aresponses%2C%20and%20produce%20highly%20distinct%20semantic%20spaces.%20Altogether%2C%20this%20work%0Ademonstrates%20in%20a%20tractable%2C%20interpretable%20domain%20that%20LM%20generations%20can%20not%0Abe%20used%20as%20replacements%20of%20or%20models%20of%20the%20cloze%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12057v2&entry.124074799=Read"},
{"title": "OmniSep: Unified Omni-Modality Sound Separation with Query-Mixup", "author": "Xize Cheng and Siqi Zheng and Zehan Wang and Minghui Fang and Ziang Zhang and Rongjie Huang and Ziyang Ma and Shengpeng Ji and Jialong Zuo and Tao Jin and Zhou Zhao", "abstract": "  The scaling up has brought tremendous success in the fields of vision and\nlanguage in recent years. When it comes to audio, however, researchers\nencounter a major challenge in scaling up the training data, as most natural\naudio contains diverse interfering signals. To address this limitation, we\nintroduce Omni-modal Sound Separation (OmniSep), a novel framework capable of\nisolating clean soundtracks based on omni-modal queries, encompassing both\nsingle-modal and multi-modal composed queries. Specifically, we introduce the\nQuery-Mixup strategy, which blends query features from different modalities\nduring training. This enables OmniSep to optimize multiple modalities\nconcurrently, effectively bringing all modalities under a unified framework for\nsound separation. We further enhance this flexibility by allowing queries to\ninfluence sound separation positively or negatively, facilitating the retention\nor removal of specific sounds as desired. Finally, OmniSep employs a\nretrieval-augmented approach known as Query-Aug, which enables open-vocabulary\nsound separation. Experimental evaluations on MUSIC, VGGSOUND-CLEAN+, and\nMUSIC-CLEAN+ datasets demonstrate effectiveness of OmniSep, achieving\nstate-of-the-art performance in text-, image-, and audio-queried sound\nseparation tasks. For samples and further information, please visit the demo\npage at \\url{https://omnisep.github.io/}.\n", "link": "http://arxiv.org/abs/2410.21269v1", "date": "2024-10-28", "relevancy": 2.4833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniSep%3A%20Unified%20Omni-Modality%20Sound%20Separation%20with%20Query-Mixup&body=Title%3A%20OmniSep%3A%20Unified%20Omni-Modality%20Sound%20Separation%20with%20Query-Mixup%0AAuthor%3A%20Xize%20Cheng%20and%20Siqi%20Zheng%20and%20Zehan%20Wang%20and%20Minghui%20Fang%20and%20Ziang%20Zhang%20and%20Rongjie%20Huang%20and%20Ziyang%20Ma%20and%20Shengpeng%20Ji%20and%20Jialong%20Zuo%20and%20Tao%20Jin%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20The%20scaling%20up%20has%20brought%20tremendous%20success%20in%20the%20fields%20of%20vision%20and%0Alanguage%20in%20recent%20years.%20When%20it%20comes%20to%20audio%2C%20however%2C%20researchers%0Aencounter%20a%20major%20challenge%20in%20scaling%20up%20the%20training%20data%2C%20as%20most%20natural%0Aaudio%20contains%20diverse%20interfering%20signals.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20Omni-modal%20Sound%20Separation%20%28OmniSep%29%2C%20a%20novel%20framework%20capable%20of%0Aisolating%20clean%20soundtracks%20based%20on%20omni-modal%20queries%2C%20encompassing%20both%0Asingle-modal%20and%20multi-modal%20composed%20queries.%20Specifically%2C%20we%20introduce%20the%0AQuery-Mixup%20strategy%2C%20which%20blends%20query%20features%20from%20different%20modalities%0Aduring%20training.%20This%20enables%20OmniSep%20to%20optimize%20multiple%20modalities%0Aconcurrently%2C%20effectively%20bringing%20all%20modalities%20under%20a%20unified%20framework%20for%0Asound%20separation.%20We%20further%20enhance%20this%20flexibility%20by%20allowing%20queries%20to%0Ainfluence%20sound%20separation%20positively%20or%20negatively%2C%20facilitating%20the%20retention%0Aor%20removal%20of%20specific%20sounds%20as%20desired.%20Finally%2C%20OmniSep%20employs%20a%0Aretrieval-augmented%20approach%20known%20as%20Query-Aug%2C%20which%20enables%20open-vocabulary%0Asound%20separation.%20Experimental%20evaluations%20on%20MUSIC%2C%20VGGSOUND-CLEAN%2B%2C%20and%0AMUSIC-CLEAN%2B%20datasets%20demonstrate%20effectiveness%20of%20OmniSep%2C%20achieving%0Astate-of-the-art%20performance%20in%20text-%2C%20image-%2C%20and%20audio-queried%20sound%0Aseparation%20tasks.%20For%20samples%20and%20further%20information%2C%20please%20visit%20the%20demo%0Apage%20at%20%5Curl%7Bhttps%3A//omnisep.github.io/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniSep%253A%2520Unified%2520Omni-Modality%2520Sound%2520Separation%2520with%2520Query-Mixup%26entry.906535625%3DXize%2520Cheng%2520and%2520Siqi%2520Zheng%2520and%2520Zehan%2520Wang%2520and%2520Minghui%2520Fang%2520and%2520Ziang%2520Zhang%2520and%2520Rongjie%2520Huang%2520and%2520Ziyang%2520Ma%2520and%2520Shengpeng%2520Ji%2520and%2520Jialong%2520Zuo%2520and%2520Tao%2520Jin%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520The%2520scaling%2520up%2520has%2520brought%2520tremendous%2520success%2520in%2520the%2520fields%2520of%2520vision%2520and%250Alanguage%2520in%2520recent%2520years.%2520When%2520it%2520comes%2520to%2520audio%252C%2520however%252C%2520researchers%250Aencounter%2520a%2520major%2520challenge%2520in%2520scaling%2520up%2520the%2520training%2520data%252C%2520as%2520most%2520natural%250Aaudio%2520contains%2520diverse%2520interfering%2520signals.%2520To%2520address%2520this%2520limitation%252C%2520we%250Aintroduce%2520Omni-modal%2520Sound%2520Separation%2520%2528OmniSep%2529%252C%2520a%2520novel%2520framework%2520capable%2520of%250Aisolating%2520clean%2520soundtracks%2520based%2520on%2520omni-modal%2520queries%252C%2520encompassing%2520both%250Asingle-modal%2520and%2520multi-modal%2520composed%2520queries.%2520Specifically%252C%2520we%2520introduce%2520the%250AQuery-Mixup%2520strategy%252C%2520which%2520blends%2520query%2520features%2520from%2520different%2520modalities%250Aduring%2520training.%2520This%2520enables%2520OmniSep%2520to%2520optimize%2520multiple%2520modalities%250Aconcurrently%252C%2520effectively%2520bringing%2520all%2520modalities%2520under%2520a%2520unified%2520framework%2520for%250Asound%2520separation.%2520We%2520further%2520enhance%2520this%2520flexibility%2520by%2520allowing%2520queries%2520to%250Ainfluence%2520sound%2520separation%2520positively%2520or%2520negatively%252C%2520facilitating%2520the%2520retention%250Aor%2520removal%2520of%2520specific%2520sounds%2520as%2520desired.%2520Finally%252C%2520OmniSep%2520employs%2520a%250Aretrieval-augmented%2520approach%2520known%2520as%2520Query-Aug%252C%2520which%2520enables%2520open-vocabulary%250Asound%2520separation.%2520Experimental%2520evaluations%2520on%2520MUSIC%252C%2520VGGSOUND-CLEAN%252B%252C%2520and%250AMUSIC-CLEAN%252B%2520datasets%2520demonstrate%2520effectiveness%2520of%2520OmniSep%252C%2520achieving%250Astate-of-the-art%2520performance%2520in%2520text-%252C%2520image-%252C%2520and%2520audio-queried%2520sound%250Aseparation%2520tasks.%2520For%2520samples%2520and%2520further%2520information%252C%2520please%2520visit%2520the%2520demo%250Apage%2520at%2520%255Curl%257Bhttps%253A//omnisep.github.io/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniSep%3A%20Unified%20Omni-Modality%20Sound%20Separation%20with%20Query-Mixup&entry.906535625=Xize%20Cheng%20and%20Siqi%20Zheng%20and%20Zehan%20Wang%20and%20Minghui%20Fang%20and%20Ziang%20Zhang%20and%20Rongjie%20Huang%20and%20Ziyang%20Ma%20and%20Shengpeng%20Ji%20and%20Jialong%20Zuo%20and%20Tao%20Jin%20and%20Zhou%20Zhao&entry.1292438233=%20%20The%20scaling%20up%20has%20brought%20tremendous%20success%20in%20the%20fields%20of%20vision%20and%0Alanguage%20in%20recent%20years.%20When%20it%20comes%20to%20audio%2C%20however%2C%20researchers%0Aencounter%20a%20major%20challenge%20in%20scaling%20up%20the%20training%20data%2C%20as%20most%20natural%0Aaudio%20contains%20diverse%20interfering%20signals.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20Omni-modal%20Sound%20Separation%20%28OmniSep%29%2C%20a%20novel%20framework%20capable%20of%0Aisolating%20clean%20soundtracks%20based%20on%20omni-modal%20queries%2C%20encompassing%20both%0Asingle-modal%20and%20multi-modal%20composed%20queries.%20Specifically%2C%20we%20introduce%20the%0AQuery-Mixup%20strategy%2C%20which%20blends%20query%20features%20from%20different%20modalities%0Aduring%20training.%20This%20enables%20OmniSep%20to%20optimize%20multiple%20modalities%0Aconcurrently%2C%20effectively%20bringing%20all%20modalities%20under%20a%20unified%20framework%20for%0Asound%20separation.%20We%20further%20enhance%20this%20flexibility%20by%20allowing%20queries%20to%0Ainfluence%20sound%20separation%20positively%20or%20negatively%2C%20facilitating%20the%20retention%0Aor%20removal%20of%20specific%20sounds%20as%20desired.%20Finally%2C%20OmniSep%20employs%20a%0Aretrieval-augmented%20approach%20known%20as%20Query-Aug%2C%20which%20enables%20open-vocabulary%0Asound%20separation.%20Experimental%20evaluations%20on%20MUSIC%2C%20VGGSOUND-CLEAN%2B%2C%20and%0AMUSIC-CLEAN%2B%20datasets%20demonstrate%20effectiveness%20of%20OmniSep%2C%20achieving%0Astate-of-the-art%20performance%20in%20text-%2C%20image-%2C%20and%20audio-queried%20sound%0Aseparation%20tasks.%20For%20samples%20and%20further%20information%2C%20please%20visit%20the%20demo%0Apage%20at%20%5Curl%7Bhttps%3A//omnisep.github.io/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21269v1&entry.124074799=Read"},
{"title": "Adaptive Transfer Clustering: A Unified Framework", "author": "Yuqi Gu and Zhongyuan Lyu and Kaizheng Wang", "abstract": "  We propose a general transfer learning framework for clustering given a main\ndataset and an auxiliary one about the same subjects. The two datasets may\nreflect similar but different latent grouping structures of the subjects. We\npropose an adaptive transfer clustering (ATC) algorithm that automatically\nleverages the commonality in the presence of unknown discrepancy, by optimizing\nan estimated bias-variance decomposition. It applies to a broad class of\nstatistical models including Gaussian mixture models, stochastic block models,\nand latent class models. A theoretical analysis proves the optimality of ATC\nunder the Gaussian mixture model and explicitly quantifies the benefit of\ntransfer. Extensive simulations and real data experiments confirm our method's\neffectiveness in various scenarios.\n", "link": "http://arxiv.org/abs/2410.21263v1", "date": "2024-10-28", "relevancy": 2.3529, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5071}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4537}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Transfer%20Clustering%3A%20A%20Unified%20Framework&body=Title%3A%20Adaptive%20Transfer%20Clustering%3A%20A%20Unified%20Framework%0AAuthor%3A%20Yuqi%20Gu%20and%20Zhongyuan%20Lyu%20and%20Kaizheng%20Wang%0AAbstract%3A%20%20%20We%20propose%20a%20general%20transfer%20learning%20framework%20for%20clustering%20given%20a%20main%0Adataset%20and%20an%20auxiliary%20one%20about%20the%20same%20subjects.%20The%20two%20datasets%20may%0Areflect%20similar%20but%20different%20latent%20grouping%20structures%20of%20the%20subjects.%20We%0Apropose%20an%20adaptive%20transfer%20clustering%20%28ATC%29%20algorithm%20that%20automatically%0Aleverages%20the%20commonality%20in%20the%20presence%20of%20unknown%20discrepancy%2C%20by%20optimizing%0Aan%20estimated%20bias-variance%20decomposition.%20It%20applies%20to%20a%20broad%20class%20of%0Astatistical%20models%20including%20Gaussian%20mixture%20models%2C%20stochastic%20block%20models%2C%0Aand%20latent%20class%20models.%20A%20theoretical%20analysis%20proves%20the%20optimality%20of%20ATC%0Aunder%20the%20Gaussian%20mixture%20model%20and%20explicitly%20quantifies%20the%20benefit%20of%0Atransfer.%20Extensive%20simulations%20and%20real%20data%20experiments%20confirm%20our%20method%27s%0Aeffectiveness%20in%20various%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Transfer%2520Clustering%253A%2520A%2520Unified%2520Framework%26entry.906535625%3DYuqi%2520Gu%2520and%2520Zhongyuan%2520Lyu%2520and%2520Kaizheng%2520Wang%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520general%2520transfer%2520learning%2520framework%2520for%2520clustering%2520given%2520a%2520main%250Adataset%2520and%2520an%2520auxiliary%2520one%2520about%2520the%2520same%2520subjects.%2520The%2520two%2520datasets%2520may%250Areflect%2520similar%2520but%2520different%2520latent%2520grouping%2520structures%2520of%2520the%2520subjects.%2520We%250Apropose%2520an%2520adaptive%2520transfer%2520clustering%2520%2528ATC%2529%2520algorithm%2520that%2520automatically%250Aleverages%2520the%2520commonality%2520in%2520the%2520presence%2520of%2520unknown%2520discrepancy%252C%2520by%2520optimizing%250Aan%2520estimated%2520bias-variance%2520decomposition.%2520It%2520applies%2520to%2520a%2520broad%2520class%2520of%250Astatistical%2520models%2520including%2520Gaussian%2520mixture%2520models%252C%2520stochastic%2520block%2520models%252C%250Aand%2520latent%2520class%2520models.%2520A%2520theoretical%2520analysis%2520proves%2520the%2520optimality%2520of%2520ATC%250Aunder%2520the%2520Gaussian%2520mixture%2520model%2520and%2520explicitly%2520quantifies%2520the%2520benefit%2520of%250Atransfer.%2520Extensive%2520simulations%2520and%2520real%2520data%2520experiments%2520confirm%2520our%2520method%2527s%250Aeffectiveness%2520in%2520various%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Transfer%20Clustering%3A%20A%20Unified%20Framework&entry.906535625=Yuqi%20Gu%20and%20Zhongyuan%20Lyu%20and%20Kaizheng%20Wang&entry.1292438233=%20%20We%20propose%20a%20general%20transfer%20learning%20framework%20for%20clustering%20given%20a%20main%0Adataset%20and%20an%20auxiliary%20one%20about%20the%20same%20subjects.%20The%20two%20datasets%20may%0Areflect%20similar%20but%20different%20latent%20grouping%20structures%20of%20the%20subjects.%20We%0Apropose%20an%20adaptive%20transfer%20clustering%20%28ATC%29%20algorithm%20that%20automatically%0Aleverages%20the%20commonality%20in%20the%20presence%20of%20unknown%20discrepancy%2C%20by%20optimizing%0Aan%20estimated%20bias-variance%20decomposition.%20It%20applies%20to%20a%20broad%20class%20of%0Astatistical%20models%20including%20Gaussian%20mixture%20models%2C%20stochastic%20block%20models%2C%0Aand%20latent%20class%20models.%20A%20theoretical%20analysis%20proves%20the%20optimality%20of%20ATC%0Aunder%20the%20Gaussian%20mixture%20model%20and%20explicitly%20quantifies%20the%20benefit%20of%0Atransfer.%20Extensive%20simulations%20and%20real%20data%20experiments%20confirm%20our%20method%27s%0Aeffectiveness%20in%20various%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21263v1&entry.124074799=Read"},
{"title": "Securing Multi-turn Conversational Language Models From Distributed\n  Backdoor Triggers", "author": "Terry Tong and Jiashu Xu and Qin Liu and Muhao Chen", "abstract": "  Large language models (LLMs) have acquired the ability to handle longer\ncontext lengths and understand nuances in text, expanding their dialogue\ncapabilities beyond a single utterance. A popular user-facing application of\nLLMs is the multi-turn chat setting. Though longer chat memory and better\nunderstanding may seemingly benefit users, our paper exposes a vulnerability\nthat leverages the multi-turn feature and strong learning ability of LLMs to\nharm the end-user: the backdoor. We demonstrate that LLMs can capture the\ncombinational backdoor representation. Only upon presentation of triggers\ntogether does the backdoor activate. We also verify empirically that this\nrepresentation is invariant to the position of the trigger utterance.\nSubsequently, inserting a single extra token into two utterances of 5%of the\ndata can cause over 99% Attack Success Rate (ASR). Our results with 3 triggers\ndemonstrate that this framework is generalizable, compatible with any trigger\nin an adversary's toolbox in a plug-and-play manner. Defending the backdoor can\nbe challenging in the chat setting because of the large input and output space.\nOur analysis indicates that the distributed backdoor exacerbates the current\nchallenges by polynomially increasing the dimension of the attacked input\nspace. Canonical textual defenses like ONION and BKI leverage auxiliary model\nforward passes over individual tokens, scaling exponentially with the input\nsequence length and struggling to maintain computational feasibility. To this\nend, we propose a decoding time defense - decayed contrastive decoding - that\nscales linearly with assistant response sequence length and reduces the\nbackdoor to as low as 0.35%.\n", "link": "http://arxiv.org/abs/2407.04151v2", "date": "2024-10-28", "relevancy": 2.3269, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Securing%20Multi-turn%20Conversational%20Language%20Models%20From%20Distributed%0A%20%20Backdoor%20Triggers&body=Title%3A%20Securing%20Multi-turn%20Conversational%20Language%20Models%20From%20Distributed%0A%20%20Backdoor%20Triggers%0AAuthor%3A%20Terry%20Tong%20and%20Jiashu%20Xu%20and%20Qin%20Liu%20and%20Muhao%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20acquired%20the%20ability%20to%20handle%20longer%0Acontext%20lengths%20and%20understand%20nuances%20in%20text%2C%20expanding%20their%20dialogue%0Acapabilities%20beyond%20a%20single%20utterance.%20A%20popular%20user-facing%20application%20of%0ALLMs%20is%20the%20multi-turn%20chat%20setting.%20Though%20longer%20chat%20memory%20and%20better%0Aunderstanding%20may%20seemingly%20benefit%20users%2C%20our%20paper%20exposes%20a%20vulnerability%0Athat%20leverages%20the%20multi-turn%20feature%20and%20strong%20learning%20ability%20of%20LLMs%20to%0Aharm%20the%20end-user%3A%20the%20backdoor.%20We%20demonstrate%20that%20LLMs%20can%20capture%20the%0Acombinational%20backdoor%20representation.%20Only%20upon%20presentation%20of%20triggers%0Atogether%20does%20the%20backdoor%20activate.%20We%20also%20verify%20empirically%20that%20this%0Arepresentation%20is%20invariant%20to%20the%20position%20of%20the%20trigger%20utterance.%0ASubsequently%2C%20inserting%20a%20single%20extra%20token%20into%20two%20utterances%20of%205%25of%20the%0Adata%20can%20cause%20over%2099%25%20Attack%20Success%20Rate%20%28ASR%29.%20Our%20results%20with%203%20triggers%0Ademonstrate%20that%20this%20framework%20is%20generalizable%2C%20compatible%20with%20any%20trigger%0Ain%20an%20adversary%27s%20toolbox%20in%20a%20plug-and-play%20manner.%20Defending%20the%20backdoor%20can%0Abe%20challenging%20in%20the%20chat%20setting%20because%20of%20the%20large%20input%20and%20output%20space.%0AOur%20analysis%20indicates%20that%20the%20distributed%20backdoor%20exacerbates%20the%20current%0Achallenges%20by%20polynomially%20increasing%20the%20dimension%20of%20the%20attacked%20input%0Aspace.%20Canonical%20textual%20defenses%20like%20ONION%20and%20BKI%20leverage%20auxiliary%20model%0Aforward%20passes%20over%20individual%20tokens%2C%20scaling%20exponentially%20with%20the%20input%0Asequence%20length%20and%20struggling%20to%20maintain%20computational%20feasibility.%20To%20this%0Aend%2C%20we%20propose%20a%20decoding%20time%20defense%20-%20decayed%20contrastive%20decoding%20-%20that%0Ascales%20linearly%20with%20assistant%20response%20sequence%20length%20and%20reduces%20the%0Abackdoor%20to%20as%20low%20as%200.35%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecuring%2520Multi-turn%2520Conversational%2520Language%2520Models%2520From%2520Distributed%250A%2520%2520Backdoor%2520Triggers%26entry.906535625%3DTerry%2520Tong%2520and%2520Jiashu%2520Xu%2520and%2520Qin%2520Liu%2520and%2520Muhao%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520acquired%2520the%2520ability%2520to%2520handle%2520longer%250Acontext%2520lengths%2520and%2520understand%2520nuances%2520in%2520text%252C%2520expanding%2520their%2520dialogue%250Acapabilities%2520beyond%2520a%2520single%2520utterance.%2520A%2520popular%2520user-facing%2520application%2520of%250ALLMs%2520is%2520the%2520multi-turn%2520chat%2520setting.%2520Though%2520longer%2520chat%2520memory%2520and%2520better%250Aunderstanding%2520may%2520seemingly%2520benefit%2520users%252C%2520our%2520paper%2520exposes%2520a%2520vulnerability%250Athat%2520leverages%2520the%2520multi-turn%2520feature%2520and%2520strong%2520learning%2520ability%2520of%2520LLMs%2520to%250Aharm%2520the%2520end-user%253A%2520the%2520backdoor.%2520We%2520demonstrate%2520that%2520LLMs%2520can%2520capture%2520the%250Acombinational%2520backdoor%2520representation.%2520Only%2520upon%2520presentation%2520of%2520triggers%250Atogether%2520does%2520the%2520backdoor%2520activate.%2520We%2520also%2520verify%2520empirically%2520that%2520this%250Arepresentation%2520is%2520invariant%2520to%2520the%2520position%2520of%2520the%2520trigger%2520utterance.%250ASubsequently%252C%2520inserting%2520a%2520single%2520extra%2520token%2520into%2520two%2520utterances%2520of%25205%2525of%2520the%250Adata%2520can%2520cause%2520over%252099%2525%2520Attack%2520Success%2520Rate%2520%2528ASR%2529.%2520Our%2520results%2520with%25203%2520triggers%250Ademonstrate%2520that%2520this%2520framework%2520is%2520generalizable%252C%2520compatible%2520with%2520any%2520trigger%250Ain%2520an%2520adversary%2527s%2520toolbox%2520in%2520a%2520plug-and-play%2520manner.%2520Defending%2520the%2520backdoor%2520can%250Abe%2520challenging%2520in%2520the%2520chat%2520setting%2520because%2520of%2520the%2520large%2520input%2520and%2520output%2520space.%250AOur%2520analysis%2520indicates%2520that%2520the%2520distributed%2520backdoor%2520exacerbates%2520the%2520current%250Achallenges%2520by%2520polynomially%2520increasing%2520the%2520dimension%2520of%2520the%2520attacked%2520input%250Aspace.%2520Canonical%2520textual%2520defenses%2520like%2520ONION%2520and%2520BKI%2520leverage%2520auxiliary%2520model%250Aforward%2520passes%2520over%2520individual%2520tokens%252C%2520scaling%2520exponentially%2520with%2520the%2520input%250Asequence%2520length%2520and%2520struggling%2520to%2520maintain%2520computational%2520feasibility.%2520To%2520this%250Aend%252C%2520we%2520propose%2520a%2520decoding%2520time%2520defense%2520-%2520decayed%2520contrastive%2520decoding%2520-%2520that%250Ascales%2520linearly%2520with%2520assistant%2520response%2520sequence%2520length%2520and%2520reduces%2520the%250Abackdoor%2520to%2520as%2520low%2520as%25200.35%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Securing%20Multi-turn%20Conversational%20Language%20Models%20From%20Distributed%0A%20%20Backdoor%20Triggers&entry.906535625=Terry%20Tong%20and%20Jiashu%20Xu%20and%20Qin%20Liu%20and%20Muhao%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20acquired%20the%20ability%20to%20handle%20longer%0Acontext%20lengths%20and%20understand%20nuances%20in%20text%2C%20expanding%20their%20dialogue%0Acapabilities%20beyond%20a%20single%20utterance.%20A%20popular%20user-facing%20application%20of%0ALLMs%20is%20the%20multi-turn%20chat%20setting.%20Though%20longer%20chat%20memory%20and%20better%0Aunderstanding%20may%20seemingly%20benefit%20users%2C%20our%20paper%20exposes%20a%20vulnerability%0Athat%20leverages%20the%20multi-turn%20feature%20and%20strong%20learning%20ability%20of%20LLMs%20to%0Aharm%20the%20end-user%3A%20the%20backdoor.%20We%20demonstrate%20that%20LLMs%20can%20capture%20the%0Acombinational%20backdoor%20representation.%20Only%20upon%20presentation%20of%20triggers%0Atogether%20does%20the%20backdoor%20activate.%20We%20also%20verify%20empirically%20that%20this%0Arepresentation%20is%20invariant%20to%20the%20position%20of%20the%20trigger%20utterance.%0ASubsequently%2C%20inserting%20a%20single%20extra%20token%20into%20two%20utterances%20of%205%25of%20the%0Adata%20can%20cause%20over%2099%25%20Attack%20Success%20Rate%20%28ASR%29.%20Our%20results%20with%203%20triggers%0Ademonstrate%20that%20this%20framework%20is%20generalizable%2C%20compatible%20with%20any%20trigger%0Ain%20an%20adversary%27s%20toolbox%20in%20a%20plug-and-play%20manner.%20Defending%20the%20backdoor%20can%0Abe%20challenging%20in%20the%20chat%20setting%20because%20of%20the%20large%20input%20and%20output%20space.%0AOur%20analysis%20indicates%20that%20the%20distributed%20backdoor%20exacerbates%20the%20current%0Achallenges%20by%20polynomially%20increasing%20the%20dimension%20of%20the%20attacked%20input%0Aspace.%20Canonical%20textual%20defenses%20like%20ONION%20and%20BKI%20leverage%20auxiliary%20model%0Aforward%20passes%20over%20individual%20tokens%2C%20scaling%20exponentially%20with%20the%20input%0Asequence%20length%20and%20struggling%20to%20maintain%20computational%20feasibility.%20To%20this%0Aend%2C%20we%20propose%20a%20decoding%20time%20defense%20-%20decayed%20contrastive%20decoding%20-%20that%0Ascales%20linearly%20with%20assistant%20response%20sequence%20length%20and%20reduces%20the%0Abackdoor%20to%20as%20low%20as%200.35%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04151v2&entry.124074799=Read"},
{"title": "On Inductive Biases That Enable Generalization of Diffusion Transformers", "author": "Jie An and De Wang and Pengsheng Guo and Jiebo Luo and Alexander Schwing", "abstract": "  Recent work studying the generalization of diffusion models with UNet-based\ndenoisers reveals inductive biases that can be expressed via geometry-adaptive\nharmonic bases. However, in practice, more recent denoising networks are often\nbased on transformers, e.g., the diffusion transformer (DiT). This raises the\nquestion: do transformer-based denoising networks exhibit inductive biases that\ncan also be expressed via geometry-adaptive harmonic bases? To our surprise, we\nfind that this is not the case. This discrepancy motivates our search for the\ninductive bias that can lead to good generalization in DiT models.\nInvestigating the pivotal attention modules of a DiT, we find that locality of\nattention maps are closely associated with generalization. To verify this\nfinding, we modify the generalization of a DiT by restricting its attention\nwindows. We inject local attention windows to a DiT and observe an improvement\nin generalization. Furthermore, we empirically find that both the placement and\nthe effective attention size of these local attention windows are crucial\nfactors. Experimental results on the CelebA, ImageNet, and LSUN datasets show\nthat strengthening the inductive bias of a DiT can improve both generalization\nand generation quality when less training data is available. Source code will\nbe released publicly upon paper publication. Project page:\ndit-generalization.github.io/.\n", "link": "http://arxiv.org/abs/2410.21273v1", "date": "2024-10-28", "relevancy": 2.2914, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6139}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.567}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Inductive%20Biases%20That%20Enable%20Generalization%20of%20Diffusion%20Transformers&body=Title%3A%20On%20Inductive%20Biases%20That%20Enable%20Generalization%20of%20Diffusion%20Transformers%0AAuthor%3A%20Jie%20An%20and%20De%20Wang%20and%20Pengsheng%20Guo%20and%20Jiebo%20Luo%20and%20Alexander%20Schwing%0AAbstract%3A%20%20%20Recent%20work%20studying%20the%20generalization%20of%20diffusion%20models%20with%20UNet-based%0Adenoisers%20reveals%20inductive%20biases%20that%20can%20be%20expressed%20via%20geometry-adaptive%0Aharmonic%20bases.%20However%2C%20in%20practice%2C%20more%20recent%20denoising%20networks%20are%20often%0Abased%20on%20transformers%2C%20e.g.%2C%20the%20diffusion%20transformer%20%28DiT%29.%20This%20raises%20the%0Aquestion%3A%20do%20transformer-based%20denoising%20networks%20exhibit%20inductive%20biases%20that%0Acan%20also%20be%20expressed%20via%20geometry-adaptive%20harmonic%20bases%3F%20To%20our%20surprise%2C%20we%0Afind%20that%20this%20is%20not%20the%20case.%20This%20discrepancy%20motivates%20our%20search%20for%20the%0Ainductive%20bias%20that%20can%20lead%20to%20good%20generalization%20in%20DiT%20models.%0AInvestigating%20the%20pivotal%20attention%20modules%20of%20a%20DiT%2C%20we%20find%20that%20locality%20of%0Aattention%20maps%20are%20closely%20associated%20with%20generalization.%20To%20verify%20this%0Afinding%2C%20we%20modify%20the%20generalization%20of%20a%20DiT%20by%20restricting%20its%20attention%0Awindows.%20We%20inject%20local%20attention%20windows%20to%20a%20DiT%20and%20observe%20an%20improvement%0Ain%20generalization.%20Furthermore%2C%20we%20empirically%20find%20that%20both%20the%20placement%20and%0Athe%20effective%20attention%20size%20of%20these%20local%20attention%20windows%20are%20crucial%0Afactors.%20Experimental%20results%20on%20the%20CelebA%2C%20ImageNet%2C%20and%20LSUN%20datasets%20show%0Athat%20strengthening%20the%20inductive%20bias%20of%20a%20DiT%20can%20improve%20both%20generalization%0Aand%20generation%20quality%20when%20less%20training%20data%20is%20available.%20Source%20code%20will%0Abe%20released%20publicly%20upon%20paper%20publication.%20Project%20page%3A%0Adit-generalization.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Inductive%2520Biases%2520That%2520Enable%2520Generalization%2520of%2520Diffusion%2520Transformers%26entry.906535625%3DJie%2520An%2520and%2520De%2520Wang%2520and%2520Pengsheng%2520Guo%2520and%2520Jiebo%2520Luo%2520and%2520Alexander%2520Schwing%26entry.1292438233%3D%2520%2520Recent%2520work%2520studying%2520the%2520generalization%2520of%2520diffusion%2520models%2520with%2520UNet-based%250Adenoisers%2520reveals%2520inductive%2520biases%2520that%2520can%2520be%2520expressed%2520via%2520geometry-adaptive%250Aharmonic%2520bases.%2520However%252C%2520in%2520practice%252C%2520more%2520recent%2520denoising%2520networks%2520are%2520often%250Abased%2520on%2520transformers%252C%2520e.g.%252C%2520the%2520diffusion%2520transformer%2520%2528DiT%2529.%2520This%2520raises%2520the%250Aquestion%253A%2520do%2520transformer-based%2520denoising%2520networks%2520exhibit%2520inductive%2520biases%2520that%250Acan%2520also%2520be%2520expressed%2520via%2520geometry-adaptive%2520harmonic%2520bases%253F%2520To%2520our%2520surprise%252C%2520we%250Afind%2520that%2520this%2520is%2520not%2520the%2520case.%2520This%2520discrepancy%2520motivates%2520our%2520search%2520for%2520the%250Ainductive%2520bias%2520that%2520can%2520lead%2520to%2520good%2520generalization%2520in%2520DiT%2520models.%250AInvestigating%2520the%2520pivotal%2520attention%2520modules%2520of%2520a%2520DiT%252C%2520we%2520find%2520that%2520locality%2520of%250Aattention%2520maps%2520are%2520closely%2520associated%2520with%2520generalization.%2520To%2520verify%2520this%250Afinding%252C%2520we%2520modify%2520the%2520generalization%2520of%2520a%2520DiT%2520by%2520restricting%2520its%2520attention%250Awindows.%2520We%2520inject%2520local%2520attention%2520windows%2520to%2520a%2520DiT%2520and%2520observe%2520an%2520improvement%250Ain%2520generalization.%2520Furthermore%252C%2520we%2520empirically%2520find%2520that%2520both%2520the%2520placement%2520and%250Athe%2520effective%2520attention%2520size%2520of%2520these%2520local%2520attention%2520windows%2520are%2520crucial%250Afactors.%2520Experimental%2520results%2520on%2520the%2520CelebA%252C%2520ImageNet%252C%2520and%2520LSUN%2520datasets%2520show%250Athat%2520strengthening%2520the%2520inductive%2520bias%2520of%2520a%2520DiT%2520can%2520improve%2520both%2520generalization%250Aand%2520generation%2520quality%2520when%2520less%2520training%2520data%2520is%2520available.%2520Source%2520code%2520will%250Abe%2520released%2520publicly%2520upon%2520paper%2520publication.%2520Project%2520page%253A%250Adit-generalization.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Inductive%20Biases%20That%20Enable%20Generalization%20of%20Diffusion%20Transformers&entry.906535625=Jie%20An%20and%20De%20Wang%20and%20Pengsheng%20Guo%20and%20Jiebo%20Luo%20and%20Alexander%20Schwing&entry.1292438233=%20%20Recent%20work%20studying%20the%20generalization%20of%20diffusion%20models%20with%20UNet-based%0Adenoisers%20reveals%20inductive%20biases%20that%20can%20be%20expressed%20via%20geometry-adaptive%0Aharmonic%20bases.%20However%2C%20in%20practice%2C%20more%20recent%20denoising%20networks%20are%20often%0Abased%20on%20transformers%2C%20e.g.%2C%20the%20diffusion%20transformer%20%28DiT%29.%20This%20raises%20the%0Aquestion%3A%20do%20transformer-based%20denoising%20networks%20exhibit%20inductive%20biases%20that%0Acan%20also%20be%20expressed%20via%20geometry-adaptive%20harmonic%20bases%3F%20To%20our%20surprise%2C%20we%0Afind%20that%20this%20is%20not%20the%20case.%20This%20discrepancy%20motivates%20our%20search%20for%20the%0Ainductive%20bias%20that%20can%20lead%20to%20good%20generalization%20in%20DiT%20models.%0AInvestigating%20the%20pivotal%20attention%20modules%20of%20a%20DiT%2C%20we%20find%20that%20locality%20of%0Aattention%20maps%20are%20closely%20associated%20with%20generalization.%20To%20verify%20this%0Afinding%2C%20we%20modify%20the%20generalization%20of%20a%20DiT%20by%20restricting%20its%20attention%0Awindows.%20We%20inject%20local%20attention%20windows%20to%20a%20DiT%20and%20observe%20an%20improvement%0Ain%20generalization.%20Furthermore%2C%20we%20empirically%20find%20that%20both%20the%20placement%20and%0Athe%20effective%20attention%20size%20of%20these%20local%20attention%20windows%20are%20crucial%0Afactors.%20Experimental%20results%20on%20the%20CelebA%2C%20ImageNet%2C%20and%20LSUN%20datasets%20show%0Athat%20strengthening%20the%20inductive%20bias%20of%20a%20DiT%20can%20improve%20both%20generalization%0Aand%20generation%20quality%20when%20less%20training%20data%20is%20available.%20Source%20code%20will%0Abe%20released%20publicly%20upon%20paper%20publication.%20Project%20page%3A%0Adit-generalization.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21273v1&entry.124074799=Read"},
{"title": "Pron vs Prompt: Can Large Language Models already Challenge a\n  World-Class Fiction Author at Creative Text Writing?", "author": "Guillermo Marco and Julio Gonzalo and Ram\u00f3n del Castillo and Mar\u00eda Teresa Mateo Girona", "abstract": "  It has become routine to report research results where Large Language Models\n(LLMs) outperform average humans in a wide range of language-related tasks, and\ncreative text writing is no exception. It seems natural, then, to raise the\nbid: Are LLMs ready to compete in creative writing skills with a top (rather\nthan average) novelist? To provide an initial answer for this question, we have\ncarried out a contest between Patricio Pron (an awarded novelist, considered\none of the best of his generation) and GPT-4 (one of the top performing LLMs),\nin the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee\nSidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write\nshort stories for both their titles and their opponent's. Then, we prepared an\nevaluation rubric inspired by Boden's definition of creativity, and we\ncollected 5,400 manual assessments provided by literature critics and scholars.\nThe results of our experimentation indicate that LLMs are still far from\nchallenging a top human creative writer, and that reaching such level of\nautonomous creative writing skills probably cannot be reached simply with\nlarger language models.\n", "link": "http://arxiv.org/abs/2407.01119v2", "date": "2024-10-28", "relevancy": 2.2821, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pron%20vs%20Prompt%3A%20Can%20Large%20Language%20Models%20already%20Challenge%20a%0A%20%20World-Class%20Fiction%20Author%20at%20Creative%20Text%20Writing%3F&body=Title%3A%20Pron%20vs%20Prompt%3A%20Can%20Large%20Language%20Models%20already%20Challenge%20a%0A%20%20World-Class%20Fiction%20Author%20at%20Creative%20Text%20Writing%3F%0AAuthor%3A%20Guillermo%20Marco%20and%20Julio%20Gonzalo%20and%20Ram%C3%B3n%20del%20Castillo%20and%20Mar%C3%ADa%20Teresa%20Mateo%20Girona%0AAbstract%3A%20%20%20It%20has%20become%20routine%20to%20report%20research%20results%20where%20Large%20Language%20Models%0A%28LLMs%29%20outperform%20average%20humans%20in%20a%20wide%20range%20of%20language-related%20tasks%2C%20and%0Acreative%20text%20writing%20is%20no%20exception.%20It%20seems%20natural%2C%20then%2C%20to%20raise%20the%0Abid%3A%20Are%20LLMs%20ready%20to%20compete%20in%20creative%20writing%20skills%20with%20a%20top%20%28rather%0Athan%20average%29%20novelist%3F%20To%20provide%20an%20initial%20answer%20for%20this%20question%2C%20we%20have%0Acarried%20out%20a%20contest%20between%20Patricio%20Pron%20%28an%20awarded%20novelist%2C%20considered%0Aone%20of%20the%20best%20of%20his%20generation%29%20and%20GPT-4%20%28one%20of%20the%20top%20performing%20LLMs%29%2C%0Ain%20the%20spirit%20of%20AI-human%20duels%20such%20as%20DeepBlue%20vs%20Kasparov%20and%20AlphaGo%20vs%20Lee%0ASidol.%20We%20asked%20Pron%20and%20GPT-4%20to%20provide%20thirty%20titles%20each%2C%20and%20then%20to%20write%0Ashort%20stories%20for%20both%20their%20titles%20and%20their%20opponent%27s.%20Then%2C%20we%20prepared%20an%0Aevaluation%20rubric%20inspired%20by%20Boden%27s%20definition%20of%20creativity%2C%20and%20we%0Acollected%205%2C400%20manual%20assessments%20provided%20by%20literature%20critics%20and%20scholars.%0AThe%20results%20of%20our%20experimentation%20indicate%20that%20LLMs%20are%20still%20far%20from%0Achallenging%20a%20top%20human%20creative%20writer%2C%20and%20that%20reaching%20such%20level%20of%0Aautonomous%20creative%20writing%20skills%20probably%20cannot%20be%20reached%20simply%20with%0Alarger%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01119v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPron%2520vs%2520Prompt%253A%2520Can%2520Large%2520Language%2520Models%2520already%2520Challenge%2520a%250A%2520%2520World-Class%2520Fiction%2520Author%2520at%2520Creative%2520Text%2520Writing%253F%26entry.906535625%3DGuillermo%2520Marco%2520and%2520Julio%2520Gonzalo%2520and%2520Ram%25C3%25B3n%2520del%2520Castillo%2520and%2520Mar%25C3%25ADa%2520Teresa%2520Mateo%2520Girona%26entry.1292438233%3D%2520%2520It%2520has%2520become%2520routine%2520to%2520report%2520research%2520results%2520where%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520outperform%2520average%2520humans%2520in%2520a%2520wide%2520range%2520of%2520language-related%2520tasks%252C%2520and%250Acreative%2520text%2520writing%2520is%2520no%2520exception.%2520It%2520seems%2520natural%252C%2520then%252C%2520to%2520raise%2520the%250Abid%253A%2520Are%2520LLMs%2520ready%2520to%2520compete%2520in%2520creative%2520writing%2520skills%2520with%2520a%2520top%2520%2528rather%250Athan%2520average%2529%2520novelist%253F%2520To%2520provide%2520an%2520initial%2520answer%2520for%2520this%2520question%252C%2520we%2520have%250Acarried%2520out%2520a%2520contest%2520between%2520Patricio%2520Pron%2520%2528an%2520awarded%2520novelist%252C%2520considered%250Aone%2520of%2520the%2520best%2520of%2520his%2520generation%2529%2520and%2520GPT-4%2520%2528one%2520of%2520the%2520top%2520performing%2520LLMs%2529%252C%250Ain%2520the%2520spirit%2520of%2520AI-human%2520duels%2520such%2520as%2520DeepBlue%2520vs%2520Kasparov%2520and%2520AlphaGo%2520vs%2520Lee%250ASidol.%2520We%2520asked%2520Pron%2520and%2520GPT-4%2520to%2520provide%2520thirty%2520titles%2520each%252C%2520and%2520then%2520to%2520write%250Ashort%2520stories%2520for%2520both%2520their%2520titles%2520and%2520their%2520opponent%2527s.%2520Then%252C%2520we%2520prepared%2520an%250Aevaluation%2520rubric%2520inspired%2520by%2520Boden%2527s%2520definition%2520of%2520creativity%252C%2520and%2520we%250Acollected%25205%252C400%2520manual%2520assessments%2520provided%2520by%2520literature%2520critics%2520and%2520scholars.%250AThe%2520results%2520of%2520our%2520experimentation%2520indicate%2520that%2520LLMs%2520are%2520still%2520far%2520from%250Achallenging%2520a%2520top%2520human%2520creative%2520writer%252C%2520and%2520that%2520reaching%2520such%2520level%2520of%250Aautonomous%2520creative%2520writing%2520skills%2520probably%2520cannot%2520be%2520reached%2520simply%2520with%250Alarger%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01119v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pron%20vs%20Prompt%3A%20Can%20Large%20Language%20Models%20already%20Challenge%20a%0A%20%20World-Class%20Fiction%20Author%20at%20Creative%20Text%20Writing%3F&entry.906535625=Guillermo%20Marco%20and%20Julio%20Gonzalo%20and%20Ram%C3%B3n%20del%20Castillo%20and%20Mar%C3%ADa%20Teresa%20Mateo%20Girona&entry.1292438233=%20%20It%20has%20become%20routine%20to%20report%20research%20results%20where%20Large%20Language%20Models%0A%28LLMs%29%20outperform%20average%20humans%20in%20a%20wide%20range%20of%20language-related%20tasks%2C%20and%0Acreative%20text%20writing%20is%20no%20exception.%20It%20seems%20natural%2C%20then%2C%20to%20raise%20the%0Abid%3A%20Are%20LLMs%20ready%20to%20compete%20in%20creative%20writing%20skills%20with%20a%20top%20%28rather%0Athan%20average%29%20novelist%3F%20To%20provide%20an%20initial%20answer%20for%20this%20question%2C%20we%20have%0Acarried%20out%20a%20contest%20between%20Patricio%20Pron%20%28an%20awarded%20novelist%2C%20considered%0Aone%20of%20the%20best%20of%20his%20generation%29%20and%20GPT-4%20%28one%20of%20the%20top%20performing%20LLMs%29%2C%0Ain%20the%20spirit%20of%20AI-human%20duels%20such%20as%20DeepBlue%20vs%20Kasparov%20and%20AlphaGo%20vs%20Lee%0ASidol.%20We%20asked%20Pron%20and%20GPT-4%20to%20provide%20thirty%20titles%20each%2C%20and%20then%20to%20write%0Ashort%20stories%20for%20both%20their%20titles%20and%20their%20opponent%27s.%20Then%2C%20we%20prepared%20an%0Aevaluation%20rubric%20inspired%20by%20Boden%27s%20definition%20of%20creativity%2C%20and%20we%0Acollected%205%2C400%20manual%20assessments%20provided%20by%20literature%20critics%20and%20scholars.%0AThe%20results%20of%20our%20experimentation%20indicate%20that%20LLMs%20are%20still%20far%20from%0Achallenging%20a%20top%20human%20creative%20writer%2C%20and%20that%20reaching%20such%20level%20of%0Aautonomous%20creative%20writing%20skills%20probably%20cannot%20be%20reached%20simply%20with%0Alarger%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01119v2&entry.124074799=Read"},
{"title": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior", "author": "Hanyu Wang and Saksham Suri and Yixuan Ren and Hao Chen and Abhinav Shrivastava", "abstract": "  We present LARP, a novel video tokenizer designed to overcome limitations in\ncurrent video tokenization methods for autoregressive (AR) generative models.\nUnlike traditional patchwise tokenizers that directly encode local visual\npatches into discrete tokens, LARP introduces a holistic tokenization scheme\nthat gathers information from the visual content using a set of learned\nholistic queries. This design allows LARP to capture more global and semantic\nrepresentations, rather than being limited to local patch-level information.\nFurthermore, it offers flexibility by supporting an arbitrary number of\ndiscrete tokens, enabling adaptive and efficient tokenization based on the\nspecific requirements of the task. To align the discrete token space with\ndownstream AR generation tasks, LARP integrates a lightweight AR transformer as\na training-time prior model that predicts the next token on its discrete latent\nspace. By incorporating the prior model during training, LARP learns a latent\nspace that is not only optimized for video reconstruction but is also\nstructured in a way that is more conducive to autoregressive generation.\nMoreover, this process defines a sequential order for the discrete tokens,\nprogressively pushing them toward an optimal configuration during training,\nensuring smoother and more accurate AR generation at inference time.\nComprehensive experiments demonstrate LARP's strong performance, achieving\nstate-of-the-art FVD on the UCF101 class-conditional video generation\nbenchmark. LARP enhances the compatibility of AR models with videos and opens\nup the potential to build unified high-fidelity multimodal large language\nmodels (MLLMs).\n", "link": "http://arxiv.org/abs/2410.21264v1", "date": "2024-10-28", "relevancy": 2.2298, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5644}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5526}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LARP%3A%20Tokenizing%20Videos%20with%20a%20Learned%20Autoregressive%20Generative%20Prior&body=Title%3A%20LARP%3A%20Tokenizing%20Videos%20with%20a%20Learned%20Autoregressive%20Generative%20Prior%0AAuthor%3A%20Hanyu%20Wang%20and%20Saksham%20Suri%20and%20Yixuan%20Ren%20and%20Hao%20Chen%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20We%20present%20LARP%2C%20a%20novel%20video%20tokenizer%20designed%20to%20overcome%20limitations%20in%0Acurrent%20video%20tokenization%20methods%20for%20autoregressive%20%28AR%29%20generative%20models.%0AUnlike%20traditional%20patchwise%20tokenizers%20that%20directly%20encode%20local%20visual%0Apatches%20into%20discrete%20tokens%2C%20LARP%20introduces%20a%20holistic%20tokenization%20scheme%0Athat%20gathers%20information%20from%20the%20visual%20content%20using%20a%20set%20of%20learned%0Aholistic%20queries.%20This%20design%20allows%20LARP%20to%20capture%20more%20global%20and%20semantic%0Arepresentations%2C%20rather%20than%20being%20limited%20to%20local%20patch-level%20information.%0AFurthermore%2C%20it%20offers%20flexibility%20by%20supporting%20an%20arbitrary%20number%20of%0Adiscrete%20tokens%2C%20enabling%20adaptive%20and%20efficient%20tokenization%20based%20on%20the%0Aspecific%20requirements%20of%20the%20task.%20To%20align%20the%20discrete%20token%20space%20with%0Adownstream%20AR%20generation%20tasks%2C%20LARP%20integrates%20a%20lightweight%20AR%20transformer%20as%0Aa%20training-time%20prior%20model%20that%20predicts%20the%20next%20token%20on%20its%20discrete%20latent%0Aspace.%20By%20incorporating%20the%20prior%20model%20during%20training%2C%20LARP%20learns%20a%20latent%0Aspace%20that%20is%20not%20only%20optimized%20for%20video%20reconstruction%20but%20is%20also%0Astructured%20in%20a%20way%20that%20is%20more%20conducive%20to%20autoregressive%20generation.%0AMoreover%2C%20this%20process%20defines%20a%20sequential%20order%20for%20the%20discrete%20tokens%2C%0Aprogressively%20pushing%20them%20toward%20an%20optimal%20configuration%20during%20training%2C%0Aensuring%20smoother%20and%20more%20accurate%20AR%20generation%20at%20inference%20time.%0AComprehensive%20experiments%20demonstrate%20LARP%27s%20strong%20performance%2C%20achieving%0Astate-of-the-art%20FVD%20on%20the%20UCF101%20class-conditional%20video%20generation%0Abenchmark.%20LARP%20enhances%20the%20compatibility%20of%20AR%20models%20with%20videos%20and%20opens%0Aup%20the%20potential%20to%20build%20unified%20high-fidelity%20multimodal%20large%20language%0Amodels%20%28MLLMs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLARP%253A%2520Tokenizing%2520Videos%2520with%2520a%2520Learned%2520Autoregressive%2520Generative%2520Prior%26entry.906535625%3DHanyu%2520Wang%2520and%2520Saksham%2520Suri%2520and%2520Yixuan%2520Ren%2520and%2520Hao%2520Chen%2520and%2520Abhinav%2520Shrivastava%26entry.1292438233%3D%2520%2520We%2520present%2520LARP%252C%2520a%2520novel%2520video%2520tokenizer%2520designed%2520to%2520overcome%2520limitations%2520in%250Acurrent%2520video%2520tokenization%2520methods%2520for%2520autoregressive%2520%2528AR%2529%2520generative%2520models.%250AUnlike%2520traditional%2520patchwise%2520tokenizers%2520that%2520directly%2520encode%2520local%2520visual%250Apatches%2520into%2520discrete%2520tokens%252C%2520LARP%2520introduces%2520a%2520holistic%2520tokenization%2520scheme%250Athat%2520gathers%2520information%2520from%2520the%2520visual%2520content%2520using%2520a%2520set%2520of%2520learned%250Aholistic%2520queries.%2520This%2520design%2520allows%2520LARP%2520to%2520capture%2520more%2520global%2520and%2520semantic%250Arepresentations%252C%2520rather%2520than%2520being%2520limited%2520to%2520local%2520patch-level%2520information.%250AFurthermore%252C%2520it%2520offers%2520flexibility%2520by%2520supporting%2520an%2520arbitrary%2520number%2520of%250Adiscrete%2520tokens%252C%2520enabling%2520adaptive%2520and%2520efficient%2520tokenization%2520based%2520on%2520the%250Aspecific%2520requirements%2520of%2520the%2520task.%2520To%2520align%2520the%2520discrete%2520token%2520space%2520with%250Adownstream%2520AR%2520generation%2520tasks%252C%2520LARP%2520integrates%2520a%2520lightweight%2520AR%2520transformer%2520as%250Aa%2520training-time%2520prior%2520model%2520that%2520predicts%2520the%2520next%2520token%2520on%2520its%2520discrete%2520latent%250Aspace.%2520By%2520incorporating%2520the%2520prior%2520model%2520during%2520training%252C%2520LARP%2520learns%2520a%2520latent%250Aspace%2520that%2520is%2520not%2520only%2520optimized%2520for%2520video%2520reconstruction%2520but%2520is%2520also%250Astructured%2520in%2520a%2520way%2520that%2520is%2520more%2520conducive%2520to%2520autoregressive%2520generation.%250AMoreover%252C%2520this%2520process%2520defines%2520a%2520sequential%2520order%2520for%2520the%2520discrete%2520tokens%252C%250Aprogressively%2520pushing%2520them%2520toward%2520an%2520optimal%2520configuration%2520during%2520training%252C%250Aensuring%2520smoother%2520and%2520more%2520accurate%2520AR%2520generation%2520at%2520inference%2520time.%250AComprehensive%2520experiments%2520demonstrate%2520LARP%2527s%2520strong%2520performance%252C%2520achieving%250Astate-of-the-art%2520FVD%2520on%2520the%2520UCF101%2520class-conditional%2520video%2520generation%250Abenchmark.%2520LARP%2520enhances%2520the%2520compatibility%2520of%2520AR%2520models%2520with%2520videos%2520and%2520opens%250Aup%2520the%2520potential%2520to%2520build%2520unified%2520high-fidelity%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LARP%3A%20Tokenizing%20Videos%20with%20a%20Learned%20Autoregressive%20Generative%20Prior&entry.906535625=Hanyu%20Wang%20and%20Saksham%20Suri%20and%20Yixuan%20Ren%20and%20Hao%20Chen%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20We%20present%20LARP%2C%20a%20novel%20video%20tokenizer%20designed%20to%20overcome%20limitations%20in%0Acurrent%20video%20tokenization%20methods%20for%20autoregressive%20%28AR%29%20generative%20models.%0AUnlike%20traditional%20patchwise%20tokenizers%20that%20directly%20encode%20local%20visual%0Apatches%20into%20discrete%20tokens%2C%20LARP%20introduces%20a%20holistic%20tokenization%20scheme%0Athat%20gathers%20information%20from%20the%20visual%20content%20using%20a%20set%20of%20learned%0Aholistic%20queries.%20This%20design%20allows%20LARP%20to%20capture%20more%20global%20and%20semantic%0Arepresentations%2C%20rather%20than%20being%20limited%20to%20local%20patch-level%20information.%0AFurthermore%2C%20it%20offers%20flexibility%20by%20supporting%20an%20arbitrary%20number%20of%0Adiscrete%20tokens%2C%20enabling%20adaptive%20and%20efficient%20tokenization%20based%20on%20the%0Aspecific%20requirements%20of%20the%20task.%20To%20align%20the%20discrete%20token%20space%20with%0Adownstream%20AR%20generation%20tasks%2C%20LARP%20integrates%20a%20lightweight%20AR%20transformer%20as%0Aa%20training-time%20prior%20model%20that%20predicts%20the%20next%20token%20on%20its%20discrete%20latent%0Aspace.%20By%20incorporating%20the%20prior%20model%20during%20training%2C%20LARP%20learns%20a%20latent%0Aspace%20that%20is%20not%20only%20optimized%20for%20video%20reconstruction%20but%20is%20also%0Astructured%20in%20a%20way%20that%20is%20more%20conducive%20to%20autoregressive%20generation.%0AMoreover%2C%20this%20process%20defines%20a%20sequential%20order%20for%20the%20discrete%20tokens%2C%0Aprogressively%20pushing%20them%20toward%20an%20optimal%20configuration%20during%20training%2C%0Aensuring%20smoother%20and%20more%20accurate%20AR%20generation%20at%20inference%20time.%0AComprehensive%20experiments%20demonstrate%20LARP%27s%20strong%20performance%2C%20achieving%0Astate-of-the-art%20FVD%20on%20the%20UCF101%20class-conditional%20video%20generation%0Abenchmark.%20LARP%20enhances%20the%20compatibility%20of%20AR%20models%20with%20videos%20and%20opens%0Aup%20the%20potential%20to%20build%20unified%20high-fidelity%20multimodal%20large%20language%0Amodels%20%28MLLMs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21264v1&entry.124074799=Read"},
{"title": "Enhancing Action Recognition by Leveraging the Hierarchical Structure of\n  Actions and Textual Context", "author": "Manuel Benavent-Lledo and David Mulero-P\u00e9rez and David Ortiz-Perez and Jose Garcia-Rodriguez and Antonis Argyros", "abstract": "  The sequential execution of actions and their hierarchical structure\nconsisting of different levels of abstraction, provide features that remain\nunexplored in the task of action recognition. In this study, we present a novel\napproach to improve action recognition by exploiting the hierarchical\norganization of actions and by incorporating contextualized textual\ninformation, including location and prior actions to reflect the sequential\ncontext. To achieve this goal, we introduce a novel transformer architecture\ntailored for action recognition that utilizes both visual and textual features.\nVisual features are obtained from RGB and optical flow data, while text\nembeddings represent contextual information. Furthermore, we define a joint\nloss function to simultaneously train the model for both coarse and\nfine-grained action recognition, thereby exploiting the hierarchical nature of\nactions. To demonstrate the effectiveness of our method, we extend the Toyota\nSmarthome Untrimmed (TSU) dataset to introduce action hierarchies, introducing\nthe Hierarchical TSU dataset. We also conduct an ablation study to assess the\nimpact of different methods for integrating contextual and hierarchical data on\naction recognition performance. Results show that the proposed approach\noutperforms pre-trained SOTA methods when trained with the same\nhyperparameters. Moreover, they also show a 17.12% improvement in top-1\naccuracy over the equivalent fine-grained RGB version when using ground-truth\ncontextual information, and a 5.33% improvement when contextual information is\nobtained from actual predictions.\n", "link": "http://arxiv.org/abs/2410.21275v1", "date": "2024-10-28", "relevancy": 2.2223, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.565}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5593}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Action%20Recognition%20by%20Leveraging%20the%20Hierarchical%20Structure%20of%0A%20%20Actions%20and%20Textual%20Context&body=Title%3A%20Enhancing%20Action%20Recognition%20by%20Leveraging%20the%20Hierarchical%20Structure%20of%0A%20%20Actions%20and%20Textual%20Context%0AAuthor%3A%20Manuel%20Benavent-Lledo%20and%20David%20Mulero-P%C3%A9rez%20and%20David%20Ortiz-Perez%20and%20Jose%20Garcia-Rodriguez%20and%20Antonis%20Argyros%0AAbstract%3A%20%20%20The%20sequential%20execution%20of%20actions%20and%20their%20hierarchical%20structure%0Aconsisting%20of%20different%20levels%20of%20abstraction%2C%20provide%20features%20that%20remain%0Aunexplored%20in%20the%20task%20of%20action%20recognition.%20In%20this%20study%2C%20we%20present%20a%20novel%0Aapproach%20to%20improve%20action%20recognition%20by%20exploiting%20the%20hierarchical%0Aorganization%20of%20actions%20and%20by%20incorporating%20contextualized%20textual%0Ainformation%2C%20including%20location%20and%20prior%20actions%20to%20reflect%20the%20sequential%0Acontext.%20To%20achieve%20this%20goal%2C%20we%20introduce%20a%20novel%20transformer%20architecture%0Atailored%20for%20action%20recognition%20that%20utilizes%20both%20visual%20and%20textual%20features.%0AVisual%20features%20are%20obtained%20from%20RGB%20and%20optical%20flow%20data%2C%20while%20text%0Aembeddings%20represent%20contextual%20information.%20Furthermore%2C%20we%20define%20a%20joint%0Aloss%20function%20to%20simultaneously%20train%20the%20model%20for%20both%20coarse%20and%0Afine-grained%20action%20recognition%2C%20thereby%20exploiting%20the%20hierarchical%20nature%20of%0Aactions.%20To%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20we%20extend%20the%20Toyota%0ASmarthome%20Untrimmed%20%28TSU%29%20dataset%20to%20introduce%20action%20hierarchies%2C%20introducing%0Athe%20Hierarchical%20TSU%20dataset.%20We%20also%20conduct%20an%20ablation%20study%20to%20assess%20the%0Aimpact%20of%20different%20methods%20for%20integrating%20contextual%20and%20hierarchical%20data%20on%0Aaction%20recognition%20performance.%20Results%20show%20that%20the%20proposed%20approach%0Aoutperforms%20pre-trained%20SOTA%20methods%20when%20trained%20with%20the%20same%0Ahyperparameters.%20Moreover%2C%20they%20also%20show%20a%2017.12%25%20improvement%20in%20top-1%0Aaccuracy%20over%20the%20equivalent%20fine-grained%20RGB%20version%20when%20using%20ground-truth%0Acontextual%20information%2C%20and%20a%205.33%25%20improvement%20when%20contextual%20information%20is%0Aobtained%20from%20actual%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Action%2520Recognition%2520by%2520Leveraging%2520the%2520Hierarchical%2520Structure%2520of%250A%2520%2520Actions%2520and%2520Textual%2520Context%26entry.906535625%3DManuel%2520Benavent-Lledo%2520and%2520David%2520Mulero-P%25C3%25A9rez%2520and%2520David%2520Ortiz-Perez%2520and%2520Jose%2520Garcia-Rodriguez%2520and%2520Antonis%2520Argyros%26entry.1292438233%3D%2520%2520The%2520sequential%2520execution%2520of%2520actions%2520and%2520their%2520hierarchical%2520structure%250Aconsisting%2520of%2520different%2520levels%2520of%2520abstraction%252C%2520provide%2520features%2520that%2520remain%250Aunexplored%2520in%2520the%2520task%2520of%2520action%2520recognition.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520novel%250Aapproach%2520to%2520improve%2520action%2520recognition%2520by%2520exploiting%2520the%2520hierarchical%250Aorganization%2520of%2520actions%2520and%2520by%2520incorporating%2520contextualized%2520textual%250Ainformation%252C%2520including%2520location%2520and%2520prior%2520actions%2520to%2520reflect%2520the%2520sequential%250Acontext.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520introduce%2520a%2520novel%2520transformer%2520architecture%250Atailored%2520for%2520action%2520recognition%2520that%2520utilizes%2520both%2520visual%2520and%2520textual%2520features.%250AVisual%2520features%2520are%2520obtained%2520from%2520RGB%2520and%2520optical%2520flow%2520data%252C%2520while%2520text%250Aembeddings%2520represent%2520contextual%2520information.%2520Furthermore%252C%2520we%2520define%2520a%2520joint%250Aloss%2520function%2520to%2520simultaneously%2520train%2520the%2520model%2520for%2520both%2520coarse%2520and%250Afine-grained%2520action%2520recognition%252C%2520thereby%2520exploiting%2520the%2520hierarchical%2520nature%2520of%250Aactions.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520we%2520extend%2520the%2520Toyota%250ASmarthome%2520Untrimmed%2520%2528TSU%2529%2520dataset%2520to%2520introduce%2520action%2520hierarchies%252C%2520introducing%250Athe%2520Hierarchical%2520TSU%2520dataset.%2520We%2520also%2520conduct%2520an%2520ablation%2520study%2520to%2520assess%2520the%250Aimpact%2520of%2520different%2520methods%2520for%2520integrating%2520contextual%2520and%2520hierarchical%2520data%2520on%250Aaction%2520recognition%2520performance.%2520Results%2520show%2520that%2520the%2520proposed%2520approach%250Aoutperforms%2520pre-trained%2520SOTA%2520methods%2520when%2520trained%2520with%2520the%2520same%250Ahyperparameters.%2520Moreover%252C%2520they%2520also%2520show%2520a%252017.12%2525%2520improvement%2520in%2520top-1%250Aaccuracy%2520over%2520the%2520equivalent%2520fine-grained%2520RGB%2520version%2520when%2520using%2520ground-truth%250Acontextual%2520information%252C%2520and%2520a%25205.33%2525%2520improvement%2520when%2520contextual%2520information%2520is%250Aobtained%2520from%2520actual%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Action%20Recognition%20by%20Leveraging%20the%20Hierarchical%20Structure%20of%0A%20%20Actions%20and%20Textual%20Context&entry.906535625=Manuel%20Benavent-Lledo%20and%20David%20Mulero-P%C3%A9rez%20and%20David%20Ortiz-Perez%20and%20Jose%20Garcia-Rodriguez%20and%20Antonis%20Argyros&entry.1292438233=%20%20The%20sequential%20execution%20of%20actions%20and%20their%20hierarchical%20structure%0Aconsisting%20of%20different%20levels%20of%20abstraction%2C%20provide%20features%20that%20remain%0Aunexplored%20in%20the%20task%20of%20action%20recognition.%20In%20this%20study%2C%20we%20present%20a%20novel%0Aapproach%20to%20improve%20action%20recognition%20by%20exploiting%20the%20hierarchical%0Aorganization%20of%20actions%20and%20by%20incorporating%20contextualized%20textual%0Ainformation%2C%20including%20location%20and%20prior%20actions%20to%20reflect%20the%20sequential%0Acontext.%20To%20achieve%20this%20goal%2C%20we%20introduce%20a%20novel%20transformer%20architecture%0Atailored%20for%20action%20recognition%20that%20utilizes%20both%20visual%20and%20textual%20features.%0AVisual%20features%20are%20obtained%20from%20RGB%20and%20optical%20flow%20data%2C%20while%20text%0Aembeddings%20represent%20contextual%20information.%20Furthermore%2C%20we%20define%20a%20joint%0Aloss%20function%20to%20simultaneously%20train%20the%20model%20for%20both%20coarse%20and%0Afine-grained%20action%20recognition%2C%20thereby%20exploiting%20the%20hierarchical%20nature%20of%0Aactions.%20To%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20we%20extend%20the%20Toyota%0ASmarthome%20Untrimmed%20%28TSU%29%20dataset%20to%20introduce%20action%20hierarchies%2C%20introducing%0Athe%20Hierarchical%20TSU%20dataset.%20We%20also%20conduct%20an%20ablation%20study%20to%20assess%20the%0Aimpact%20of%20different%20methods%20for%20integrating%20contextual%20and%20hierarchical%20data%20on%0Aaction%20recognition%20performance.%20Results%20show%20that%20the%20proposed%20approach%0Aoutperforms%20pre-trained%20SOTA%20methods%20when%20trained%20with%20the%20same%0Ahyperparameters.%20Moreover%2C%20they%20also%20show%20a%2017.12%25%20improvement%20in%20top-1%0Aaccuracy%20over%20the%20equivalent%20fine-grained%20RGB%20version%20when%20using%20ground-truth%0Acontextual%20information%2C%20and%20a%205.33%25%20improvement%20when%20contextual%20information%20is%0Aobtained%20from%20actual%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21275v1&entry.124074799=Read"},
{"title": "Non-myopic Generation of Language Models for Reasoning and Planning", "author": "Chang Ma and Haiteng Zhao and Junlei Zhang and Junxian He and Lingpeng Kong", "abstract": "  Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.\n", "link": "http://arxiv.org/abs/2410.17195v3", "date": "2024-10-28", "relevancy": 2.1829, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5492}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-myopic%20Generation%20of%20Language%20Models%20for%20Reasoning%20and%20Planning&body=Title%3A%20Non-myopic%20Generation%20of%20Language%20Models%20for%20Reasoning%20and%20Planning%0AAuthor%3A%20Chang%20Ma%20and%20Haiteng%20Zhao%20and%20Junlei%20Zhang%20and%20Junxian%20He%20and%20Lingpeng%20Kong%0AAbstract%3A%20%20%20Large%20Language%20Models%20have%20demonstrated%20remarkable%20abilities%20in%20reasoning%20and%0Aplanning%20by%20breaking%20down%20complex%20problems%20into%20sequential%20steps.%20Despite%20their%0Asuccess%20in%20various%20domains%20like%20mathematical%20problem-solving%20and%20coding%2C%20LLMs%0Aface%20challenges%20in%20ensuring%20reliable%20and%20optimal%20planning%20due%20to%20their%20inherent%0Amyopic%20nature%20of%20autoregressive%20decoding.%20This%20paper%20revisits%20LLM%20reasoning%0Afrom%20an%20optimal-control%20perspective%2C%20proposing%20a%20novel%20method%2C%0APredictive-Decoding%2C%20that%20leverages%20Model%20Predictive%20Control%20to%20enhance%0Aplanning%20accuracy.%20By%20re-weighting%20LLM%20distributions%20based%20on%20foresight%0Atrajectories%2C%20Predictive-Decoding%20aims%20to%20mitigate%20early%20errors%20and%20promote%0Anon-myopic%20planning.%20Our%20experiments%20show%20significant%20improvements%20in%20a%20wide%0Arange%20of%20tasks%20for%20math%2C%20coding%2C%20and%20agents.%20Furthermore%2C%20Predictive-Decoding%0Ademonstrates%20computational%20efficiency%2C%20outperforming%20search%20baselines%20with%0Areduced%20computational%20resources.%20This%20study%20provides%20insights%20into%20optimizing%0ALLM%20planning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17195v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-myopic%2520Generation%2520of%2520Language%2520Models%2520for%2520Reasoning%2520and%2520Planning%26entry.906535625%3DChang%2520Ma%2520and%2520Haiteng%2520Zhao%2520and%2520Junlei%2520Zhang%2520and%2520Junxian%2520He%2520and%2520Lingpeng%2520Kong%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520have%2520demonstrated%2520remarkable%2520abilities%2520in%2520reasoning%2520and%250Aplanning%2520by%2520breaking%2520down%2520complex%2520problems%2520into%2520sequential%2520steps.%2520Despite%2520their%250Asuccess%2520in%2520various%2520domains%2520like%2520mathematical%2520problem-solving%2520and%2520coding%252C%2520LLMs%250Aface%2520challenges%2520in%2520ensuring%2520reliable%2520and%2520optimal%2520planning%2520due%2520to%2520their%2520inherent%250Amyopic%2520nature%2520of%2520autoregressive%2520decoding.%2520This%2520paper%2520revisits%2520LLM%2520reasoning%250Afrom%2520an%2520optimal-control%2520perspective%252C%2520proposing%2520a%2520novel%2520method%252C%250APredictive-Decoding%252C%2520that%2520leverages%2520Model%2520Predictive%2520Control%2520to%2520enhance%250Aplanning%2520accuracy.%2520By%2520re-weighting%2520LLM%2520distributions%2520based%2520on%2520foresight%250Atrajectories%252C%2520Predictive-Decoding%2520aims%2520to%2520mitigate%2520early%2520errors%2520and%2520promote%250Anon-myopic%2520planning.%2520Our%2520experiments%2520show%2520significant%2520improvements%2520in%2520a%2520wide%250Arange%2520of%2520tasks%2520for%2520math%252C%2520coding%252C%2520and%2520agents.%2520Furthermore%252C%2520Predictive-Decoding%250Ademonstrates%2520computational%2520efficiency%252C%2520outperforming%2520search%2520baselines%2520with%250Areduced%2520computational%2520resources.%2520This%2520study%2520provides%2520insights%2520into%2520optimizing%250ALLM%2520planning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17195v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-myopic%20Generation%20of%20Language%20Models%20for%20Reasoning%20and%20Planning&entry.906535625=Chang%20Ma%20and%20Haiteng%20Zhao%20and%20Junlei%20Zhang%20and%20Junxian%20He%20and%20Lingpeng%20Kong&entry.1292438233=%20%20Large%20Language%20Models%20have%20demonstrated%20remarkable%20abilities%20in%20reasoning%20and%0Aplanning%20by%20breaking%20down%20complex%20problems%20into%20sequential%20steps.%20Despite%20their%0Asuccess%20in%20various%20domains%20like%20mathematical%20problem-solving%20and%20coding%2C%20LLMs%0Aface%20challenges%20in%20ensuring%20reliable%20and%20optimal%20planning%20due%20to%20their%20inherent%0Amyopic%20nature%20of%20autoregressive%20decoding.%20This%20paper%20revisits%20LLM%20reasoning%0Afrom%20an%20optimal-control%20perspective%2C%20proposing%20a%20novel%20method%2C%0APredictive-Decoding%2C%20that%20leverages%20Model%20Predictive%20Control%20to%20enhance%0Aplanning%20accuracy.%20By%20re-weighting%20LLM%20distributions%20based%20on%20foresight%0Atrajectories%2C%20Predictive-Decoding%20aims%20to%20mitigate%20early%20errors%20and%20promote%0Anon-myopic%20planning.%20Our%20experiments%20show%20significant%20improvements%20in%20a%20wide%0Arange%20of%20tasks%20for%20math%2C%20coding%2C%20and%20agents.%20Furthermore%2C%20Predictive-Decoding%0Ademonstrates%20computational%20efficiency%2C%20outperforming%20search%20baselines%20with%0Areduced%20computational%20resources.%20This%20study%20provides%20insights%20into%20optimizing%0ALLM%20planning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17195v3&entry.124074799=Read"},
{"title": "Hierarchical Knowledge Graph Construction from Images for Scalable\n  E-Commerce", "author": "Zhantao Yang and Han Zhang and Fangyi Chen and Anudeepsekhar Bolimera and Marios Savvides", "abstract": "  Knowledge Graph (KG) is playing an increasingly important role in various AI\nsystems. For e-commerce, an efficient and low-cost automated knowledge graph\nconstruction method is the foundation of enabling various successful downstream\napplications. In this paper, we propose a novel method for constructing\nstructured product knowledge graphs from raw product images. The method\ncooperatively leverages recent advances in the vision-language model (VLM) and\nlarge language model (LLM), fully automating the process and allowing timely\ngraph updates. We also present a human-annotated e-commerce product dataset for\nbenchmarking product property extraction in knowledge graph construction. Our\nmethod outperforms our baseline in all metrics and evaluated properties,\ndemonstrating its effectiveness and bright usage potential.\n", "link": "http://arxiv.org/abs/2410.21237v1", "date": "2024-10-28", "relevancy": 2.1782, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.565}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5367}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Knowledge%20Graph%20Construction%20from%20Images%20for%20Scalable%0A%20%20E-Commerce&body=Title%3A%20Hierarchical%20Knowledge%20Graph%20Construction%20from%20Images%20for%20Scalable%0A%20%20E-Commerce%0AAuthor%3A%20Zhantao%20Yang%20and%20Han%20Zhang%20and%20Fangyi%20Chen%20and%20Anudeepsekhar%20Bolimera%20and%20Marios%20Savvides%0AAbstract%3A%20%20%20Knowledge%20Graph%20%28KG%29%20is%20playing%20an%20increasingly%20important%20role%20in%20various%20AI%0Asystems.%20For%20e-commerce%2C%20an%20efficient%20and%20low-cost%20automated%20knowledge%20graph%0Aconstruction%20method%20is%20the%20foundation%20of%20enabling%20various%20successful%20downstream%0Aapplications.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20for%20constructing%0Astructured%20product%20knowledge%20graphs%20from%20raw%20product%20images.%20The%20method%0Acooperatively%20leverages%20recent%20advances%20in%20the%20vision-language%20model%20%28VLM%29%20and%0Alarge%20language%20model%20%28LLM%29%2C%20fully%20automating%20the%20process%20and%20allowing%20timely%0Agraph%20updates.%20We%20also%20present%20a%20human-annotated%20e-commerce%20product%20dataset%20for%0Abenchmarking%20product%20property%20extraction%20in%20knowledge%20graph%20construction.%20Our%0Amethod%20outperforms%20our%20baseline%20in%20all%20metrics%20and%20evaluated%20properties%2C%0Ademonstrating%20its%20effectiveness%20and%20bright%20usage%20potential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Knowledge%2520Graph%2520Construction%2520from%2520Images%2520for%2520Scalable%250A%2520%2520E-Commerce%26entry.906535625%3DZhantao%2520Yang%2520and%2520Han%2520Zhang%2520and%2520Fangyi%2520Chen%2520and%2520Anudeepsekhar%2520Bolimera%2520and%2520Marios%2520Savvides%26entry.1292438233%3D%2520%2520Knowledge%2520Graph%2520%2528KG%2529%2520is%2520playing%2520an%2520increasingly%2520important%2520role%2520in%2520various%2520AI%250Asystems.%2520For%2520e-commerce%252C%2520an%2520efficient%2520and%2520low-cost%2520automated%2520knowledge%2520graph%250Aconstruction%2520method%2520is%2520the%2520foundation%2520of%2520enabling%2520various%2520successful%2520downstream%250Aapplications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520constructing%250Astructured%2520product%2520knowledge%2520graphs%2520from%2520raw%2520product%2520images.%2520The%2520method%250Acooperatively%2520leverages%2520recent%2520advances%2520in%2520the%2520vision-language%2520model%2520%2528VLM%2529%2520and%250Alarge%2520language%2520model%2520%2528LLM%2529%252C%2520fully%2520automating%2520the%2520process%2520and%2520allowing%2520timely%250Agraph%2520updates.%2520We%2520also%2520present%2520a%2520human-annotated%2520e-commerce%2520product%2520dataset%2520for%250Abenchmarking%2520product%2520property%2520extraction%2520in%2520knowledge%2520graph%2520construction.%2520Our%250Amethod%2520outperforms%2520our%2520baseline%2520in%2520all%2520metrics%2520and%2520evaluated%2520properties%252C%250Ademonstrating%2520its%2520effectiveness%2520and%2520bright%2520usage%2520potential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Knowledge%20Graph%20Construction%20from%20Images%20for%20Scalable%0A%20%20E-Commerce&entry.906535625=Zhantao%20Yang%20and%20Han%20Zhang%20and%20Fangyi%20Chen%20and%20Anudeepsekhar%20Bolimera%20and%20Marios%20Savvides&entry.1292438233=%20%20Knowledge%20Graph%20%28KG%29%20is%20playing%20an%20increasingly%20important%20role%20in%20various%20AI%0Asystems.%20For%20e-commerce%2C%20an%20efficient%20and%20low-cost%20automated%20knowledge%20graph%0Aconstruction%20method%20is%20the%20foundation%20of%20enabling%20various%20successful%20downstream%0Aapplications.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20for%20constructing%0Astructured%20product%20knowledge%20graphs%20from%20raw%20product%20images.%20The%20method%0Acooperatively%20leverages%20recent%20advances%20in%20the%20vision-language%20model%20%28VLM%29%20and%0Alarge%20language%20model%20%28LLM%29%2C%20fully%20automating%20the%20process%20and%20allowing%20timely%0Agraph%20updates.%20We%20also%20present%20a%20human-annotated%20e-commerce%20product%20dataset%20for%0Abenchmarking%20product%20property%20extraction%20in%20knowledge%20graph%20construction.%20Our%0Amethod%20outperforms%20our%20baseline%20in%20all%20metrics%20and%20evaluated%20properties%2C%0Ademonstrating%20its%20effectiveness%20and%20bright%20usage%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21237v1&entry.124074799=Read"},
{"title": "Belief in the Machine: Investigating Epistemological Blind Spots of\n  Language Models", "author": "Mirac Suzgun and Tayfun Gur and Federico Bianchi and Daniel E. Ho and Thomas Icard and Dan Jurafsky and James Zou", "abstract": "  As language models (LMs) become integral to fields like healthcare, law, and\njournalism, their ability to differentiate between fact, belief, and knowledge\nis essential for reliable decision-making. Failure to grasp these distinctions\ncan lead to significant consequences in areas such as medical diagnosis, legal\njudgments, and dissemination of fake news. Despite this, current literature has\nlargely focused on more complex issues such as theory of mind, overlooking more\nfundamental epistemic challenges. This study systematically evaluates the\nepistemic reasoning capabilities of modern LMs, including GPT-4, Claude-3, and\nLlama-3, using a new dataset, KaBLE, consisting of 13,000 questions across 13\ntasks. Our results reveal key limitations. First, while LMs achieve 86%\naccuracy on factual scenarios, their performance drops significantly with false\nscenarios, particularly in belief-related tasks. Second, LMs struggle with\nrecognizing and affirming personal beliefs, especially when those beliefs\ncontradict factual data, which raises concerns for applications in healthcare\nand counseling, where engaging with a person's beliefs is critical. Third, we\nidentify a salient bias in how LMs process first-person versus third-person\nbeliefs, performing better on third-person tasks (80.7%) compared to\nfirst-person tasks (54.4%). Fourth, LMs lack a robust understanding of the\nfactive nature of knowledge, namely, that knowledge inherently requires truth.\nFifth, LMs rely on linguistic cues for fact-checking and sometimes bypass the\ndeeper reasoning. These findings highlight significant concerns about current\nLMs' ability to reason about truth, belief, and knowledge while emphasizing the\nneed for advancements in these areas before broad deployment in critical\nsectors.\n", "link": "http://arxiv.org/abs/2410.21195v1", "date": "2024-10-28", "relevancy": 2.1762, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Belief%20in%20the%20Machine%3A%20Investigating%20Epistemological%20Blind%20Spots%20of%0A%20%20Language%20Models&body=Title%3A%20Belief%20in%20the%20Machine%3A%20Investigating%20Epistemological%20Blind%20Spots%20of%0A%20%20Language%20Models%0AAuthor%3A%20Mirac%20Suzgun%20and%20Tayfun%20Gur%20and%20Federico%20Bianchi%20and%20Daniel%20E.%20Ho%20and%20Thomas%20Icard%20and%20Dan%20Jurafsky%20and%20James%20Zou%0AAbstract%3A%20%20%20As%20language%20models%20%28LMs%29%20become%20integral%20to%20fields%20like%20healthcare%2C%20law%2C%20and%0Ajournalism%2C%20their%20ability%20to%20differentiate%20between%20fact%2C%20belief%2C%20and%20knowledge%0Ais%20essential%20for%20reliable%20decision-making.%20Failure%20to%20grasp%20these%20distinctions%0Acan%20lead%20to%20significant%20consequences%20in%20areas%20such%20as%20medical%20diagnosis%2C%20legal%0Ajudgments%2C%20and%20dissemination%20of%20fake%20news.%20Despite%20this%2C%20current%20literature%20has%0Alargely%20focused%20on%20more%20complex%20issues%20such%20as%20theory%20of%20mind%2C%20overlooking%20more%0Afundamental%20epistemic%20challenges.%20This%20study%20systematically%20evaluates%20the%0Aepistemic%20reasoning%20capabilities%20of%20modern%20LMs%2C%20including%20GPT-4%2C%20Claude-3%2C%20and%0ALlama-3%2C%20using%20a%20new%20dataset%2C%20KaBLE%2C%20consisting%20of%2013%2C000%20questions%20across%2013%0Atasks.%20Our%20results%20reveal%20key%20limitations.%20First%2C%20while%20LMs%20achieve%2086%25%0Aaccuracy%20on%20factual%20scenarios%2C%20their%20performance%20drops%20significantly%20with%20false%0Ascenarios%2C%20particularly%20in%20belief-related%20tasks.%20Second%2C%20LMs%20struggle%20with%0Arecognizing%20and%20affirming%20personal%20beliefs%2C%20especially%20when%20those%20beliefs%0Acontradict%20factual%20data%2C%20which%20raises%20concerns%20for%20applications%20in%20healthcare%0Aand%20counseling%2C%20where%20engaging%20with%20a%20person%27s%20beliefs%20is%20critical.%20Third%2C%20we%0Aidentify%20a%20salient%20bias%20in%20how%20LMs%20process%20first-person%20versus%20third-person%0Abeliefs%2C%20performing%20better%20on%20third-person%20tasks%20%2880.7%25%29%20compared%20to%0Afirst-person%20tasks%20%2854.4%25%29.%20Fourth%2C%20LMs%20lack%20a%20robust%20understanding%20of%20the%0Afactive%20nature%20of%20knowledge%2C%20namely%2C%20that%20knowledge%20inherently%20requires%20truth.%0AFifth%2C%20LMs%20rely%20on%20linguistic%20cues%20for%20fact-checking%20and%20sometimes%20bypass%20the%0Adeeper%20reasoning.%20These%20findings%20highlight%20significant%20concerns%20about%20current%0ALMs%27%20ability%20to%20reason%20about%20truth%2C%20belief%2C%20and%20knowledge%20while%20emphasizing%20the%0Aneed%20for%20advancements%20in%20these%20areas%20before%20broad%20deployment%20in%20critical%0Asectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBelief%2520in%2520the%2520Machine%253A%2520Investigating%2520Epistemological%2520Blind%2520Spots%2520of%250A%2520%2520Language%2520Models%26entry.906535625%3DMirac%2520Suzgun%2520and%2520Tayfun%2520Gur%2520and%2520Federico%2520Bianchi%2520and%2520Daniel%2520E.%2520Ho%2520and%2520Thomas%2520Icard%2520and%2520Dan%2520Jurafsky%2520and%2520James%2520Zou%26entry.1292438233%3D%2520%2520As%2520language%2520models%2520%2528LMs%2529%2520become%2520integral%2520to%2520fields%2520like%2520healthcare%252C%2520law%252C%2520and%250Ajournalism%252C%2520their%2520ability%2520to%2520differentiate%2520between%2520fact%252C%2520belief%252C%2520and%2520knowledge%250Ais%2520essential%2520for%2520reliable%2520decision-making.%2520Failure%2520to%2520grasp%2520these%2520distinctions%250Acan%2520lead%2520to%2520significant%2520consequences%2520in%2520areas%2520such%2520as%2520medical%2520diagnosis%252C%2520legal%250Ajudgments%252C%2520and%2520dissemination%2520of%2520fake%2520news.%2520Despite%2520this%252C%2520current%2520literature%2520has%250Alargely%2520focused%2520on%2520more%2520complex%2520issues%2520such%2520as%2520theory%2520of%2520mind%252C%2520overlooking%2520more%250Afundamental%2520epistemic%2520challenges.%2520This%2520study%2520systematically%2520evaluates%2520the%250Aepistemic%2520reasoning%2520capabilities%2520of%2520modern%2520LMs%252C%2520including%2520GPT-4%252C%2520Claude-3%252C%2520and%250ALlama-3%252C%2520using%2520a%2520new%2520dataset%252C%2520KaBLE%252C%2520consisting%2520of%252013%252C000%2520questions%2520across%252013%250Atasks.%2520Our%2520results%2520reveal%2520key%2520limitations.%2520First%252C%2520while%2520LMs%2520achieve%252086%2525%250Aaccuracy%2520on%2520factual%2520scenarios%252C%2520their%2520performance%2520drops%2520significantly%2520with%2520false%250Ascenarios%252C%2520particularly%2520in%2520belief-related%2520tasks.%2520Second%252C%2520LMs%2520struggle%2520with%250Arecognizing%2520and%2520affirming%2520personal%2520beliefs%252C%2520especially%2520when%2520those%2520beliefs%250Acontradict%2520factual%2520data%252C%2520which%2520raises%2520concerns%2520for%2520applications%2520in%2520healthcare%250Aand%2520counseling%252C%2520where%2520engaging%2520with%2520a%2520person%2527s%2520beliefs%2520is%2520critical.%2520Third%252C%2520we%250Aidentify%2520a%2520salient%2520bias%2520in%2520how%2520LMs%2520process%2520first-person%2520versus%2520third-person%250Abeliefs%252C%2520performing%2520better%2520on%2520third-person%2520tasks%2520%252880.7%2525%2529%2520compared%2520to%250Afirst-person%2520tasks%2520%252854.4%2525%2529.%2520Fourth%252C%2520LMs%2520lack%2520a%2520robust%2520understanding%2520of%2520the%250Afactive%2520nature%2520of%2520knowledge%252C%2520namely%252C%2520that%2520knowledge%2520inherently%2520requires%2520truth.%250AFifth%252C%2520LMs%2520rely%2520on%2520linguistic%2520cues%2520for%2520fact-checking%2520and%2520sometimes%2520bypass%2520the%250Adeeper%2520reasoning.%2520These%2520findings%2520highlight%2520significant%2520concerns%2520about%2520current%250ALMs%2527%2520ability%2520to%2520reason%2520about%2520truth%252C%2520belief%252C%2520and%2520knowledge%2520while%2520emphasizing%2520the%250Aneed%2520for%2520advancements%2520in%2520these%2520areas%2520before%2520broad%2520deployment%2520in%2520critical%250Asectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Belief%20in%20the%20Machine%3A%20Investigating%20Epistemological%20Blind%20Spots%20of%0A%20%20Language%20Models&entry.906535625=Mirac%20Suzgun%20and%20Tayfun%20Gur%20and%20Federico%20Bianchi%20and%20Daniel%20E.%20Ho%20and%20Thomas%20Icard%20and%20Dan%20Jurafsky%20and%20James%20Zou&entry.1292438233=%20%20As%20language%20models%20%28LMs%29%20become%20integral%20to%20fields%20like%20healthcare%2C%20law%2C%20and%0Ajournalism%2C%20their%20ability%20to%20differentiate%20between%20fact%2C%20belief%2C%20and%20knowledge%0Ais%20essential%20for%20reliable%20decision-making.%20Failure%20to%20grasp%20these%20distinctions%0Acan%20lead%20to%20significant%20consequences%20in%20areas%20such%20as%20medical%20diagnosis%2C%20legal%0Ajudgments%2C%20and%20dissemination%20of%20fake%20news.%20Despite%20this%2C%20current%20literature%20has%0Alargely%20focused%20on%20more%20complex%20issues%20such%20as%20theory%20of%20mind%2C%20overlooking%20more%0Afundamental%20epistemic%20challenges.%20This%20study%20systematically%20evaluates%20the%0Aepistemic%20reasoning%20capabilities%20of%20modern%20LMs%2C%20including%20GPT-4%2C%20Claude-3%2C%20and%0ALlama-3%2C%20using%20a%20new%20dataset%2C%20KaBLE%2C%20consisting%20of%2013%2C000%20questions%20across%2013%0Atasks.%20Our%20results%20reveal%20key%20limitations.%20First%2C%20while%20LMs%20achieve%2086%25%0Aaccuracy%20on%20factual%20scenarios%2C%20their%20performance%20drops%20significantly%20with%20false%0Ascenarios%2C%20particularly%20in%20belief-related%20tasks.%20Second%2C%20LMs%20struggle%20with%0Arecognizing%20and%20affirming%20personal%20beliefs%2C%20especially%20when%20those%20beliefs%0Acontradict%20factual%20data%2C%20which%20raises%20concerns%20for%20applications%20in%20healthcare%0Aand%20counseling%2C%20where%20engaging%20with%20a%20person%27s%20beliefs%20is%20critical.%20Third%2C%20we%0Aidentify%20a%20salient%20bias%20in%20how%20LMs%20process%20first-person%20versus%20third-person%0Abeliefs%2C%20performing%20better%20on%20third-person%20tasks%20%2880.7%25%29%20compared%20to%0Afirst-person%20tasks%20%2854.4%25%29.%20Fourth%2C%20LMs%20lack%20a%20robust%20understanding%20of%20the%0Afactive%20nature%20of%20knowledge%2C%20namely%2C%20that%20knowledge%20inherently%20requires%20truth.%0AFifth%2C%20LMs%20rely%20on%20linguistic%20cues%20for%20fact-checking%20and%20sometimes%20bypass%20the%0Adeeper%20reasoning.%20These%20findings%20highlight%20significant%20concerns%20about%20current%0ALMs%27%20ability%20to%20reason%20about%20truth%2C%20belief%2C%20and%20knowledge%20while%20emphasizing%20the%0Aneed%20for%20advancements%20in%20these%20areas%20before%20broad%20deployment%20in%20critical%0Asectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21195v1&entry.124074799=Read"},
{"title": "On Homomorphic Encryption Based Strategies for Class Imbalance in\n  Federated Learning", "author": "Arpit Guleria and J. Harshan and Ranjitha Prasad and B. N. Bharath", "abstract": "  Class imbalance in training datasets can lead to bias and poor generalization\nin machine learning models. While pre-processing of training datasets can\nefficiently address both these issues in centralized learning environments, it\nis challenging to detect and address these issues in a distributed learning\nenvironment such as federated learning. In this paper, we propose FLICKER, a\nprivacy preserving framework to address issues related to global class\nimbalance in federated learning. At the heart of our contribution lies the\npopular CKKS homomorphic encryption scheme, which is used by the clients to\nprivately share their data attributes, and subsequently balance their datasets\nbefore implementing the FL scheme. Extensive experimental results show that our\nproposed method significantly improves the FL accuracy numbers when used along\nwith popular datasets and relevant baselines.\n", "link": "http://arxiv.org/abs/2410.21192v1", "date": "2024-10-28", "relevancy": 2.1589, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4596}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.433}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Homomorphic%20Encryption%20Based%20Strategies%20for%20Class%20Imbalance%20in%0A%20%20Federated%20Learning&body=Title%3A%20On%20Homomorphic%20Encryption%20Based%20Strategies%20for%20Class%20Imbalance%20in%0A%20%20Federated%20Learning%0AAuthor%3A%20Arpit%20Guleria%20and%20J.%20Harshan%20and%20Ranjitha%20Prasad%20and%20B.%20N.%20Bharath%0AAbstract%3A%20%20%20Class%20imbalance%20in%20training%20datasets%20can%20lead%20to%20bias%20and%20poor%20generalization%0Ain%20machine%20learning%20models.%20While%20pre-processing%20of%20training%20datasets%20can%0Aefficiently%20address%20both%20these%20issues%20in%20centralized%20learning%20environments%2C%20it%0Ais%20challenging%20to%20detect%20and%20address%20these%20issues%20in%20a%20distributed%20learning%0Aenvironment%20such%20as%20federated%20learning.%20In%20this%20paper%2C%20we%20propose%20FLICKER%2C%20a%0Aprivacy%20preserving%20framework%20to%20address%20issues%20related%20to%20global%20class%0Aimbalance%20in%20federated%20learning.%20At%20the%20heart%20of%20our%20contribution%20lies%20the%0Apopular%20CKKS%20homomorphic%20encryption%20scheme%2C%20which%20is%20used%20by%20the%20clients%20to%0Aprivately%20share%20their%20data%20attributes%2C%20and%20subsequently%20balance%20their%20datasets%0Abefore%20implementing%20the%20FL%20scheme.%20Extensive%20experimental%20results%20show%20that%20our%0Aproposed%20method%20significantly%20improves%20the%20FL%20accuracy%20numbers%20when%20used%20along%0Awith%20popular%20datasets%20and%20relevant%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Homomorphic%2520Encryption%2520Based%2520Strategies%2520for%2520Class%2520Imbalance%2520in%250A%2520%2520Federated%2520Learning%26entry.906535625%3DArpit%2520Guleria%2520and%2520J.%2520Harshan%2520and%2520Ranjitha%2520Prasad%2520and%2520B.%2520N.%2520Bharath%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520in%2520training%2520datasets%2520can%2520lead%2520to%2520bias%2520and%2520poor%2520generalization%250Ain%2520machine%2520learning%2520models.%2520While%2520pre-processing%2520of%2520training%2520datasets%2520can%250Aefficiently%2520address%2520both%2520these%2520issues%2520in%2520centralized%2520learning%2520environments%252C%2520it%250Ais%2520challenging%2520to%2520detect%2520and%2520address%2520these%2520issues%2520in%2520a%2520distributed%2520learning%250Aenvironment%2520such%2520as%2520federated%2520learning.%2520In%2520this%2520paper%252C%2520we%2520propose%2520FLICKER%252C%2520a%250Aprivacy%2520preserving%2520framework%2520to%2520address%2520issues%2520related%2520to%2520global%2520class%250Aimbalance%2520in%2520federated%2520learning.%2520At%2520the%2520heart%2520of%2520our%2520contribution%2520lies%2520the%250Apopular%2520CKKS%2520homomorphic%2520encryption%2520scheme%252C%2520which%2520is%2520used%2520by%2520the%2520clients%2520to%250Aprivately%2520share%2520their%2520data%2520attributes%252C%2520and%2520subsequently%2520balance%2520their%2520datasets%250Abefore%2520implementing%2520the%2520FL%2520scheme.%2520Extensive%2520experimental%2520results%2520show%2520that%2520our%250Aproposed%2520method%2520significantly%2520improves%2520the%2520FL%2520accuracy%2520numbers%2520when%2520used%2520along%250Awith%2520popular%2520datasets%2520and%2520relevant%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Homomorphic%20Encryption%20Based%20Strategies%20for%20Class%20Imbalance%20in%0A%20%20Federated%20Learning&entry.906535625=Arpit%20Guleria%20and%20J.%20Harshan%20and%20Ranjitha%20Prasad%20and%20B.%20N.%20Bharath&entry.1292438233=%20%20Class%20imbalance%20in%20training%20datasets%20can%20lead%20to%20bias%20and%20poor%20generalization%0Ain%20machine%20learning%20models.%20While%20pre-processing%20of%20training%20datasets%20can%0Aefficiently%20address%20both%20these%20issues%20in%20centralized%20learning%20environments%2C%20it%0Ais%20challenging%20to%20detect%20and%20address%20these%20issues%20in%20a%20distributed%20learning%0Aenvironment%20such%20as%20federated%20learning.%20In%20this%20paper%2C%20we%20propose%20FLICKER%2C%20a%0Aprivacy%20preserving%20framework%20to%20address%20issues%20related%20to%20global%20class%0Aimbalance%20in%20federated%20learning.%20At%20the%20heart%20of%20our%20contribution%20lies%20the%0Apopular%20CKKS%20homomorphic%20encryption%20scheme%2C%20which%20is%20used%20by%20the%20clients%20to%0Aprivately%20share%20their%20data%20attributes%2C%20and%20subsequently%20balance%20their%20datasets%0Abefore%20implementing%20the%20FL%20scheme.%20Extensive%20experimental%20results%20show%20that%20our%0Aproposed%20method%20significantly%20improves%20the%20FL%20accuracy%20numbers%20when%20used%20along%0Awith%20popular%20datasets%20and%20relevant%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21192v1&entry.124074799=Read"},
{"title": "Assessing Brittleness of Image-Text Retrieval Benchmarks from\n  Vision-Language Models Perspective", "author": "Mariya Hendriksen and Shuo Zhang and Ridho Reinanda and Mohamed Yahya and Edgar Meij and Maarten de Rijke", "abstract": "  We examine the brittleness of the image-text retrieval (ITR) evaluation\npipeline with a focus on concept granularity. We start by analyzing two common\nbenchmarks, MS-COCO and Flickr30k, and compare them with augmented,\nfine-grained versions, MS-COCO-FG and Flickr30k-FG, given a specified set of\nlinguistic features capturing concept granularity. Flickr30k-FG and MS COCO-FG\nconsistently give rise to higher scores across all the selected features. To\nfurther our understanding of the impact of granularity we consider a novel\ntaxonomy of query perturbations. We apply these perturbations to the selected\ndatasets. We evaluate four diverse state-of-the-art Vision-Language models on\nboth the standard and fine-grained datasets under zero-shot conditions, with\nand without the applied perturbations. The results demonstrate that although\nperturbations generally degrade model performance, the fine-grained datasets\nexhibit a smaller performance drop than their standard counterparts. The\nrelative performance drop across all setups is consistent across all models and\ndatasets, indicating that the issue lies within the benchmarks themselves. We\nconclude by providing an agenda for improving ITR evaluation pipelines.\n", "link": "http://arxiv.org/abs/2407.15239v3", "date": "2024-10-28", "relevancy": 2.0851, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Brittleness%20of%20Image-Text%20Retrieval%20Benchmarks%20from%0A%20%20Vision-Language%20Models%20Perspective&body=Title%3A%20Assessing%20Brittleness%20of%20Image-Text%20Retrieval%20Benchmarks%20from%0A%20%20Vision-Language%20Models%20Perspective%0AAuthor%3A%20Mariya%20Hendriksen%20and%20Shuo%20Zhang%20and%20Ridho%20Reinanda%20and%20Mohamed%20Yahya%20and%20Edgar%20Meij%20and%20Maarten%20de%20Rijke%0AAbstract%3A%20%20%20We%20examine%20the%20brittleness%20of%20the%20image-text%20retrieval%20%28ITR%29%20evaluation%0Apipeline%20with%20a%20focus%20on%20concept%20granularity.%20We%20start%20by%20analyzing%20two%20common%0Abenchmarks%2C%20MS-COCO%20and%20Flickr30k%2C%20and%20compare%20them%20with%20augmented%2C%0Afine-grained%20versions%2C%20MS-COCO-FG%20and%20Flickr30k-FG%2C%20given%20a%20specified%20set%20of%0Alinguistic%20features%20capturing%20concept%20granularity.%20Flickr30k-FG%20and%20MS%20COCO-FG%0Aconsistently%20give%20rise%20to%20higher%20scores%20across%20all%20the%20selected%20features.%20To%0Afurther%20our%20understanding%20of%20the%20impact%20of%20granularity%20we%20consider%20a%20novel%0Ataxonomy%20of%20query%20perturbations.%20We%20apply%20these%20perturbations%20to%20the%20selected%0Adatasets.%20We%20evaluate%20four%20diverse%20state-of-the-art%20Vision-Language%20models%20on%0Aboth%20the%20standard%20and%20fine-grained%20datasets%20under%20zero-shot%20conditions%2C%20with%0Aand%20without%20the%20applied%20perturbations.%20The%20results%20demonstrate%20that%20although%0Aperturbations%20generally%20degrade%20model%20performance%2C%20the%20fine-grained%20datasets%0Aexhibit%20a%20smaller%20performance%20drop%20than%20their%20standard%20counterparts.%20The%0Arelative%20performance%20drop%20across%20all%20setups%20is%20consistent%20across%20all%20models%20and%0Adatasets%2C%20indicating%20that%20the%20issue%20lies%20within%20the%20benchmarks%20themselves.%20We%0Aconclude%20by%20providing%20an%20agenda%20for%20improving%20ITR%20evaluation%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15239v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Brittleness%2520of%2520Image-Text%2520Retrieval%2520Benchmarks%2520from%250A%2520%2520Vision-Language%2520Models%2520Perspective%26entry.906535625%3DMariya%2520Hendriksen%2520and%2520Shuo%2520Zhang%2520and%2520Ridho%2520Reinanda%2520and%2520Mohamed%2520Yahya%2520and%2520Edgar%2520Meij%2520and%2520Maarten%2520de%2520Rijke%26entry.1292438233%3D%2520%2520We%2520examine%2520the%2520brittleness%2520of%2520the%2520image-text%2520retrieval%2520%2528ITR%2529%2520evaluation%250Apipeline%2520with%2520a%2520focus%2520on%2520concept%2520granularity.%2520We%2520start%2520by%2520analyzing%2520two%2520common%250Abenchmarks%252C%2520MS-COCO%2520and%2520Flickr30k%252C%2520and%2520compare%2520them%2520with%2520augmented%252C%250Afine-grained%2520versions%252C%2520MS-COCO-FG%2520and%2520Flickr30k-FG%252C%2520given%2520a%2520specified%2520set%2520of%250Alinguistic%2520features%2520capturing%2520concept%2520granularity.%2520Flickr30k-FG%2520and%2520MS%2520COCO-FG%250Aconsistently%2520give%2520rise%2520to%2520higher%2520scores%2520across%2520all%2520the%2520selected%2520features.%2520To%250Afurther%2520our%2520understanding%2520of%2520the%2520impact%2520of%2520granularity%2520we%2520consider%2520a%2520novel%250Ataxonomy%2520of%2520query%2520perturbations.%2520We%2520apply%2520these%2520perturbations%2520to%2520the%2520selected%250Adatasets.%2520We%2520evaluate%2520four%2520diverse%2520state-of-the-art%2520Vision-Language%2520models%2520on%250Aboth%2520the%2520standard%2520and%2520fine-grained%2520datasets%2520under%2520zero-shot%2520conditions%252C%2520with%250Aand%2520without%2520the%2520applied%2520perturbations.%2520The%2520results%2520demonstrate%2520that%2520although%250Aperturbations%2520generally%2520degrade%2520model%2520performance%252C%2520the%2520fine-grained%2520datasets%250Aexhibit%2520a%2520smaller%2520performance%2520drop%2520than%2520their%2520standard%2520counterparts.%2520The%250Arelative%2520performance%2520drop%2520across%2520all%2520setups%2520is%2520consistent%2520across%2520all%2520models%2520and%250Adatasets%252C%2520indicating%2520that%2520the%2520issue%2520lies%2520within%2520the%2520benchmarks%2520themselves.%2520We%250Aconclude%2520by%2520providing%2520an%2520agenda%2520for%2520improving%2520ITR%2520evaluation%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15239v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Brittleness%20of%20Image-Text%20Retrieval%20Benchmarks%20from%0A%20%20Vision-Language%20Models%20Perspective&entry.906535625=Mariya%20Hendriksen%20and%20Shuo%20Zhang%20and%20Ridho%20Reinanda%20and%20Mohamed%20Yahya%20and%20Edgar%20Meij%20and%20Maarten%20de%20Rijke&entry.1292438233=%20%20We%20examine%20the%20brittleness%20of%20the%20image-text%20retrieval%20%28ITR%29%20evaluation%0Apipeline%20with%20a%20focus%20on%20concept%20granularity.%20We%20start%20by%20analyzing%20two%20common%0Abenchmarks%2C%20MS-COCO%20and%20Flickr30k%2C%20and%20compare%20them%20with%20augmented%2C%0Afine-grained%20versions%2C%20MS-COCO-FG%20and%20Flickr30k-FG%2C%20given%20a%20specified%20set%20of%0Alinguistic%20features%20capturing%20concept%20granularity.%20Flickr30k-FG%20and%20MS%20COCO-FG%0Aconsistently%20give%20rise%20to%20higher%20scores%20across%20all%20the%20selected%20features.%20To%0Afurther%20our%20understanding%20of%20the%20impact%20of%20granularity%20we%20consider%20a%20novel%0Ataxonomy%20of%20query%20perturbations.%20We%20apply%20these%20perturbations%20to%20the%20selected%0Adatasets.%20We%20evaluate%20four%20diverse%20state-of-the-art%20Vision-Language%20models%20on%0Aboth%20the%20standard%20and%20fine-grained%20datasets%20under%20zero-shot%20conditions%2C%20with%0Aand%20without%20the%20applied%20perturbations.%20The%20results%20demonstrate%20that%20although%0Aperturbations%20generally%20degrade%20model%20performance%2C%20the%20fine-grained%20datasets%0Aexhibit%20a%20smaller%20performance%20drop%20than%20their%20standard%20counterparts.%20The%0Arelative%20performance%20drop%20across%20all%20setups%20is%20consistent%20across%20all%20models%20and%0Adatasets%2C%20indicating%20that%20the%20issue%20lies%20within%20the%20benchmarks%20themselves.%20We%0Aconclude%20by%20providing%20an%20agenda%20for%20improving%20ITR%20evaluation%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15239v3&entry.124074799=Read"},
{"title": "LongReward: Improving Long-context Large Language Models with AI\n  Feedback", "author": "Jiajie Zhang and Zhongni Hou and Xin Lv and Shulin Cao and Zhenyu Hou and Yilin Niu and Lei Hou and Yuxiao Dong and Ling Feng and Juanzi Li", "abstract": "  Though significant advancements have been achieved in developing long-context\nlarge language models (LLMs), the compromised quality of LLM-synthesized data\nfor supervised fine-tuning (SFT) often affects the long-context performance of\nSFT models and leads to inherent limitations. In principle, reinforcement\nlearning (RL) with appropriate reward signals can further enhance models'\ncapacities. However, how to obtain reliable rewards in long-context scenarios\nremains unexplored. To this end, we propose LongReward, a novel method that\nutilizes an off-the-shelf LLM to provide rewards for long-context model\nresponses from four human-valued dimensions: helpfulness, logicality,\nfaithfulness, and completeness, each with a carefully designed assessment\npipeline. By combining LongReward and offline RL algorithm DPO, we are able to\neffectively improve long-context SFT models. Our experiments indicate that\nLongReward not only significantly improves models' long-context performance but\nalso enhances their ability to follow short instructions. We also find that\nlong-context DPO with LongReward and conventional short-context DPO can be used\ntogether without hurting either one's performance.\n", "link": "http://arxiv.org/abs/2410.21252v1", "date": "2024-10-28", "relevancy": 2.0601, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongReward%3A%20Improving%20Long-context%20Large%20Language%20Models%20with%20AI%0A%20%20Feedback&body=Title%3A%20LongReward%3A%20Improving%20Long-context%20Large%20Language%20Models%20with%20AI%0A%20%20Feedback%0AAuthor%3A%20Jiajie%20Zhang%20and%20Zhongni%20Hou%20and%20Xin%20Lv%20and%20Shulin%20Cao%20and%20Zhenyu%20Hou%20and%20Yilin%20Niu%20and%20Lei%20Hou%20and%20Yuxiao%20Dong%20and%20Ling%20Feng%20and%20Juanzi%20Li%0AAbstract%3A%20%20%20Though%20significant%20advancements%20have%20been%20achieved%20in%20developing%20long-context%0Alarge%20language%20models%20%28LLMs%29%2C%20the%20compromised%20quality%20of%20LLM-synthesized%20data%0Afor%20supervised%20fine-tuning%20%28SFT%29%20often%20affects%20the%20long-context%20performance%20of%0ASFT%20models%20and%20leads%20to%20inherent%20limitations.%20In%20principle%2C%20reinforcement%0Alearning%20%28RL%29%20with%20appropriate%20reward%20signals%20can%20further%20enhance%20models%27%0Acapacities.%20However%2C%20how%20to%20obtain%20reliable%20rewards%20in%20long-context%20scenarios%0Aremains%20unexplored.%20To%20this%20end%2C%20we%20propose%20LongReward%2C%20a%20novel%20method%20that%0Autilizes%20an%20off-the-shelf%20LLM%20to%20provide%20rewards%20for%20long-context%20model%0Aresponses%20from%20four%20human-valued%20dimensions%3A%20helpfulness%2C%20logicality%2C%0Afaithfulness%2C%20and%20completeness%2C%20each%20with%20a%20carefully%20designed%20assessment%0Apipeline.%20By%20combining%20LongReward%20and%20offline%20RL%20algorithm%20DPO%2C%20we%20are%20able%20to%0Aeffectively%20improve%20long-context%20SFT%20models.%20Our%20experiments%20indicate%20that%0ALongReward%20not%20only%20significantly%20improves%20models%27%20long-context%20performance%20but%0Aalso%20enhances%20their%20ability%20to%20follow%20short%20instructions.%20We%20also%20find%20that%0Along-context%20DPO%20with%20LongReward%20and%20conventional%20short-context%20DPO%20can%20be%20used%0Atogether%20without%20hurting%20either%20one%27s%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongReward%253A%2520Improving%2520Long-context%2520Large%2520Language%2520Models%2520with%2520AI%250A%2520%2520Feedback%26entry.906535625%3DJiajie%2520Zhang%2520and%2520Zhongni%2520Hou%2520and%2520Xin%2520Lv%2520and%2520Shulin%2520Cao%2520and%2520Zhenyu%2520Hou%2520and%2520Yilin%2520Niu%2520and%2520Lei%2520Hou%2520and%2520Yuxiao%2520Dong%2520and%2520Ling%2520Feng%2520and%2520Juanzi%2520Li%26entry.1292438233%3D%2520%2520Though%2520significant%2520advancements%2520have%2520been%2520achieved%2520in%2520developing%2520long-context%250Alarge%2520language%2520models%2520%2528LLMs%2529%252C%2520the%2520compromised%2520quality%2520of%2520LLM-synthesized%2520data%250Afor%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520often%2520affects%2520the%2520long-context%2520performance%2520of%250ASFT%2520models%2520and%2520leads%2520to%2520inherent%2520limitations.%2520In%2520principle%252C%2520reinforcement%250Alearning%2520%2528RL%2529%2520with%2520appropriate%2520reward%2520signals%2520can%2520further%2520enhance%2520models%2527%250Acapacities.%2520However%252C%2520how%2520to%2520obtain%2520reliable%2520rewards%2520in%2520long-context%2520scenarios%250Aremains%2520unexplored.%2520To%2520this%2520end%252C%2520we%2520propose%2520LongReward%252C%2520a%2520novel%2520method%2520that%250Autilizes%2520an%2520off-the-shelf%2520LLM%2520to%2520provide%2520rewards%2520for%2520long-context%2520model%250Aresponses%2520from%2520four%2520human-valued%2520dimensions%253A%2520helpfulness%252C%2520logicality%252C%250Afaithfulness%252C%2520and%2520completeness%252C%2520each%2520with%2520a%2520carefully%2520designed%2520assessment%250Apipeline.%2520By%2520combining%2520LongReward%2520and%2520offline%2520RL%2520algorithm%2520DPO%252C%2520we%2520are%2520able%2520to%250Aeffectively%2520improve%2520long-context%2520SFT%2520models.%2520Our%2520experiments%2520indicate%2520that%250ALongReward%2520not%2520only%2520significantly%2520improves%2520models%2527%2520long-context%2520performance%2520but%250Aalso%2520enhances%2520their%2520ability%2520to%2520follow%2520short%2520instructions.%2520We%2520also%2520find%2520that%250Along-context%2520DPO%2520with%2520LongReward%2520and%2520conventional%2520short-context%2520DPO%2520can%2520be%2520used%250Atogether%2520without%2520hurting%2520either%2520one%2527s%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongReward%3A%20Improving%20Long-context%20Large%20Language%20Models%20with%20AI%0A%20%20Feedback&entry.906535625=Jiajie%20Zhang%20and%20Zhongni%20Hou%20and%20Xin%20Lv%20and%20Shulin%20Cao%20and%20Zhenyu%20Hou%20and%20Yilin%20Niu%20and%20Lei%20Hou%20and%20Yuxiao%20Dong%20and%20Ling%20Feng%20and%20Juanzi%20Li&entry.1292438233=%20%20Though%20significant%20advancements%20have%20been%20achieved%20in%20developing%20long-context%0Alarge%20language%20models%20%28LLMs%29%2C%20the%20compromised%20quality%20of%20LLM-synthesized%20data%0Afor%20supervised%20fine-tuning%20%28SFT%29%20often%20affects%20the%20long-context%20performance%20of%0ASFT%20models%20and%20leads%20to%20inherent%20limitations.%20In%20principle%2C%20reinforcement%0Alearning%20%28RL%29%20with%20appropriate%20reward%20signals%20can%20further%20enhance%20models%27%0Acapacities.%20However%2C%20how%20to%20obtain%20reliable%20rewards%20in%20long-context%20scenarios%0Aremains%20unexplored.%20To%20this%20end%2C%20we%20propose%20LongReward%2C%20a%20novel%20method%20that%0Autilizes%20an%20off-the-shelf%20LLM%20to%20provide%20rewards%20for%20long-context%20model%0Aresponses%20from%20four%20human-valued%20dimensions%3A%20helpfulness%2C%20logicality%2C%0Afaithfulness%2C%20and%20completeness%2C%20each%20with%20a%20carefully%20designed%20assessment%0Apipeline.%20By%20combining%20LongReward%20and%20offline%20RL%20algorithm%20DPO%2C%20we%20are%20able%20to%0Aeffectively%20improve%20long-context%20SFT%20models.%20Our%20experiments%20indicate%20that%0ALongReward%20not%20only%20significantly%20improves%20models%27%20long-context%20performance%20but%0Aalso%20enhances%20their%20ability%20to%20follow%20short%20instructions.%20We%20also%20find%20that%0Along-context%20DPO%20with%20LongReward%20and%20conventional%20short-context%20DPO%20can%20be%20used%0Atogether%20without%20hurting%20either%20one%27s%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21252v1&entry.124074799=Read"},
{"title": "BongLLaMA: LLaMA for Bangla Language", "author": "Abdullah Khan Zehady and Safi Al Mamun and Naymul Islam and Santu Karmaker", "abstract": "  Bangla (or \"Bengali\") is a language spoken by approximately 240 million\nnative speakers and around 300 million people worldwide. Despite being the 5th\nlargest spoken language in the world, Bangla is still a \"low-resource\"\nlanguage, and existing pretrained language models often struggle to perform\nwell on Bangla Language Processing (BLP) tasks. This work addresses this gap by\nintroducing BongLLaMA (i.e., Bangla-LLaMA), an open-source large language model\nfine-tuned exclusively on large Bangla corpora and instruction-tuning datasets.\nWe present our methodology, data augmentation techniques, fine-tuning details,\nand comprehensive benchmarking results showcasing the utility of BongLLaMA on\nBLP tasks. We believe BongLLaMA will serve as the new standard baseline for\nBangla Language Models and, thus, facilitate future benchmarking studies\nfocused on this widely-spoken yet \"low-resource\" language. All BongLLaMA models\nare available for public use at https://huggingface.co/BanglaLLM.\n", "link": "http://arxiv.org/abs/2410.21200v1", "date": "2024-10-28", "relevancy": 2.0506, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4309}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4012}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BongLLaMA%3A%20LLaMA%20for%20Bangla%20Language&body=Title%3A%20BongLLaMA%3A%20LLaMA%20for%20Bangla%20Language%0AAuthor%3A%20Abdullah%20Khan%20Zehady%20and%20Safi%20Al%20Mamun%20and%20Naymul%20Islam%20and%20Santu%20Karmaker%0AAbstract%3A%20%20%20Bangla%20%28or%20%22Bengali%22%29%20is%20a%20language%20spoken%20by%20approximately%20240%20million%0Anative%20speakers%20and%20around%20300%20million%20people%20worldwide.%20Despite%20being%20the%205th%0Alargest%20spoken%20language%20in%20the%20world%2C%20Bangla%20is%20still%20a%20%22low-resource%22%0Alanguage%2C%20and%20existing%20pretrained%20language%20models%20often%20struggle%20to%20perform%0Awell%20on%20Bangla%20Language%20Processing%20%28BLP%29%20tasks.%20This%20work%20addresses%20this%20gap%20by%0Aintroducing%20BongLLaMA%20%28i.e.%2C%20Bangla-LLaMA%29%2C%20an%20open-source%20large%20language%20model%0Afine-tuned%20exclusively%20on%20large%20Bangla%20corpora%20and%20instruction-tuning%20datasets.%0AWe%20present%20our%20methodology%2C%20data%20augmentation%20techniques%2C%20fine-tuning%20details%2C%0Aand%20comprehensive%20benchmarking%20results%20showcasing%20the%20utility%20of%20BongLLaMA%20on%0ABLP%20tasks.%20We%20believe%20BongLLaMA%20will%20serve%20as%20the%20new%20standard%20baseline%20for%0ABangla%20Language%20Models%20and%2C%20thus%2C%20facilitate%20future%20benchmarking%20studies%0Afocused%20on%20this%20widely-spoken%20yet%20%22low-resource%22%20language.%20All%20BongLLaMA%20models%0Aare%20available%20for%20public%20use%20at%20https%3A//huggingface.co/BanglaLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBongLLaMA%253A%2520LLaMA%2520for%2520Bangla%2520Language%26entry.906535625%3DAbdullah%2520Khan%2520Zehady%2520and%2520Safi%2520Al%2520Mamun%2520and%2520Naymul%2520Islam%2520and%2520Santu%2520Karmaker%26entry.1292438233%3D%2520%2520Bangla%2520%2528or%2520%2522Bengali%2522%2529%2520is%2520a%2520language%2520spoken%2520by%2520approximately%2520240%2520million%250Anative%2520speakers%2520and%2520around%2520300%2520million%2520people%2520worldwide.%2520Despite%2520being%2520the%25205th%250Alargest%2520spoken%2520language%2520in%2520the%2520world%252C%2520Bangla%2520is%2520still%2520a%2520%2522low-resource%2522%250Alanguage%252C%2520and%2520existing%2520pretrained%2520language%2520models%2520often%2520struggle%2520to%2520perform%250Awell%2520on%2520Bangla%2520Language%2520Processing%2520%2528BLP%2529%2520tasks.%2520This%2520work%2520addresses%2520this%2520gap%2520by%250Aintroducing%2520BongLLaMA%2520%2528i.e.%252C%2520Bangla-LLaMA%2529%252C%2520an%2520open-source%2520large%2520language%2520model%250Afine-tuned%2520exclusively%2520on%2520large%2520Bangla%2520corpora%2520and%2520instruction-tuning%2520datasets.%250AWe%2520present%2520our%2520methodology%252C%2520data%2520augmentation%2520techniques%252C%2520fine-tuning%2520details%252C%250Aand%2520comprehensive%2520benchmarking%2520results%2520showcasing%2520the%2520utility%2520of%2520BongLLaMA%2520on%250ABLP%2520tasks.%2520We%2520believe%2520BongLLaMA%2520will%2520serve%2520as%2520the%2520new%2520standard%2520baseline%2520for%250ABangla%2520Language%2520Models%2520and%252C%2520thus%252C%2520facilitate%2520future%2520benchmarking%2520studies%250Afocused%2520on%2520this%2520widely-spoken%2520yet%2520%2522low-resource%2522%2520language.%2520All%2520BongLLaMA%2520models%250Aare%2520available%2520for%2520public%2520use%2520at%2520https%253A//huggingface.co/BanglaLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BongLLaMA%3A%20LLaMA%20for%20Bangla%20Language&entry.906535625=Abdullah%20Khan%20Zehady%20and%20Safi%20Al%20Mamun%20and%20Naymul%20Islam%20and%20Santu%20Karmaker&entry.1292438233=%20%20Bangla%20%28or%20%22Bengali%22%29%20is%20a%20language%20spoken%20by%20approximately%20240%20million%0Anative%20speakers%20and%20around%20300%20million%20people%20worldwide.%20Despite%20being%20the%205th%0Alargest%20spoken%20language%20in%20the%20world%2C%20Bangla%20is%20still%20a%20%22low-resource%22%0Alanguage%2C%20and%20existing%20pretrained%20language%20models%20often%20struggle%20to%20perform%0Awell%20on%20Bangla%20Language%20Processing%20%28BLP%29%20tasks.%20This%20work%20addresses%20this%20gap%20by%0Aintroducing%20BongLLaMA%20%28i.e.%2C%20Bangla-LLaMA%29%2C%20an%20open-source%20large%20language%20model%0Afine-tuned%20exclusively%20on%20large%20Bangla%20corpora%20and%20instruction-tuning%20datasets.%0AWe%20present%20our%20methodology%2C%20data%20augmentation%20techniques%2C%20fine-tuning%20details%2C%0Aand%20comprehensive%20benchmarking%20results%20showcasing%20the%20utility%20of%20BongLLaMA%20on%0ABLP%20tasks.%20We%20believe%20BongLLaMA%20will%20serve%20as%20the%20new%20standard%20baseline%20for%0ABangla%20Language%20Models%20and%2C%20thus%2C%20facilitate%20future%20benchmarking%20studies%0Afocused%20on%20this%20widely-spoken%20yet%20%22low-resource%22%20language.%20All%20BongLLaMA%20models%0Aare%20available%20for%20public%20use%20at%20https%3A//huggingface.co/BanglaLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21200v1&entry.124074799=Read"},
{"title": "AutoPenBench: Benchmarking Generative Agents for Penetration Testing", "author": "Luca Gioacchini and Marco Mellia and Idilio Drago and Alexander Delsanto and Giuseppe Siracusano and Roberto Bifulco", "abstract": "  Generative AI agents, software systems powered by Large Language Models\n(LLMs), are emerging as a promising approach to automate cybersecurity tasks.\nAmong the others, penetration testing is a challenging field due to the task\ncomplexity and the diverse strategies to simulate cyber-attacks. Despite\ngrowing interest and initial studies in automating penetration testing with\ngenerative agents, there remains a significant gap in the form of a\ncomprehensive and standard framework for their evaluation and development. This\npaper introduces AutoPenBench, an open benchmark for evaluating generative\nagents in automated penetration testing. We present a comprehensive framework\nthat includes 33 tasks, each representing a vulnerable system that the agent\nhas to attack. Tasks are of increasing difficulty levels, including in-vitro\nand real-world scenarios. We assess the agent performance with generic and\nspecific milestones that allow us to compare results in a standardised manner\nand understand the limits of the agent under test. We show the benefits of\nAutoPenBench by testing two agent architectures: a fully autonomous and a\nsemi-autonomous supporting human interaction. We compare their performance and\nlimitations. For example, the fully autonomous agent performs unsatisfactorily\nachieving a 21% Success Rate (SR) across the benchmark, solving 27% of the\nsimple tasks and only one real-world task. In contrast, the assisted agent\ndemonstrates substantial improvements, with 64% of SR. AutoPenBench allows us\nalso to observe how different LLMs like GPT-4o or OpenAI o1 impact the ability\nof the agents to complete the tasks. We believe that our benchmark fills the\ngap with a standard and flexible framework to compare penetration testing\nagents on a common ground. We hope to extend AutoPenBench along with the\nresearch community by making it available under\nhttps://github.com/lucagioacchini/auto-pen-bench.\n", "link": "http://arxiv.org/abs/2410.03225v2", "date": "2024-10-28", "relevancy": 2.0391, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5431}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4954}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoPenBench%3A%20Benchmarking%20Generative%20Agents%20for%20Penetration%20Testing&body=Title%3A%20AutoPenBench%3A%20Benchmarking%20Generative%20Agents%20for%20Penetration%20Testing%0AAuthor%3A%20Luca%20Gioacchini%20and%20Marco%20Mellia%20and%20Idilio%20Drago%20and%20Alexander%20Delsanto%20and%20Giuseppe%20Siracusano%20and%20Roberto%20Bifulco%0AAbstract%3A%20%20%20Generative%20AI%20agents%2C%20software%20systems%20powered%20by%20Large%20Language%20Models%0A%28LLMs%29%2C%20are%20emerging%20as%20a%20promising%20approach%20to%20automate%20cybersecurity%20tasks.%0AAmong%20the%20others%2C%20penetration%20testing%20is%20a%20challenging%20field%20due%20to%20the%20task%0Acomplexity%20and%20the%20diverse%20strategies%20to%20simulate%20cyber-attacks.%20Despite%0Agrowing%20interest%20and%20initial%20studies%20in%20automating%20penetration%20testing%20with%0Agenerative%20agents%2C%20there%20remains%20a%20significant%20gap%20in%20the%20form%20of%20a%0Acomprehensive%20and%20standard%20framework%20for%20their%20evaluation%20and%20development.%20This%0Apaper%20introduces%20AutoPenBench%2C%20an%20open%20benchmark%20for%20evaluating%20generative%0Aagents%20in%20automated%20penetration%20testing.%20We%20present%20a%20comprehensive%20framework%0Athat%20includes%2033%20tasks%2C%20each%20representing%20a%20vulnerable%20system%20that%20the%20agent%0Ahas%20to%20attack.%20Tasks%20are%20of%20increasing%20difficulty%20levels%2C%20including%20in-vitro%0Aand%20real-world%20scenarios.%20We%20assess%20the%20agent%20performance%20with%20generic%20and%0Aspecific%20milestones%20that%20allow%20us%20to%20compare%20results%20in%20a%20standardised%20manner%0Aand%20understand%20the%20limits%20of%20the%20agent%20under%20test.%20We%20show%20the%20benefits%20of%0AAutoPenBench%20by%20testing%20two%20agent%20architectures%3A%20a%20fully%20autonomous%20and%20a%0Asemi-autonomous%20supporting%20human%20interaction.%20We%20compare%20their%20performance%20and%0Alimitations.%20For%20example%2C%20the%20fully%20autonomous%20agent%20performs%20unsatisfactorily%0Aachieving%20a%2021%25%20Success%20Rate%20%28SR%29%20across%20the%20benchmark%2C%20solving%2027%25%20of%20the%0Asimple%20tasks%20and%20only%20one%20real-world%20task.%20In%20contrast%2C%20the%20assisted%20agent%0Ademonstrates%20substantial%20improvements%2C%20with%2064%25%20of%20SR.%20AutoPenBench%20allows%20us%0Aalso%20to%20observe%20how%20different%20LLMs%20like%20GPT-4o%20or%20OpenAI%20o1%20impact%20the%20ability%0Aof%20the%20agents%20to%20complete%20the%20tasks.%20We%20believe%20that%20our%20benchmark%20fills%20the%0Agap%20with%20a%20standard%20and%20flexible%20framework%20to%20compare%20penetration%20testing%0Aagents%20on%20a%20common%20ground.%20We%20hope%20to%20extend%20AutoPenBench%20along%20with%20the%0Aresearch%20community%20by%20making%20it%20available%20under%0Ahttps%3A//github.com/lucagioacchini/auto-pen-bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoPenBench%253A%2520Benchmarking%2520Generative%2520Agents%2520for%2520Penetration%2520Testing%26entry.906535625%3DLuca%2520Gioacchini%2520and%2520Marco%2520Mellia%2520and%2520Idilio%2520Drago%2520and%2520Alexander%2520Delsanto%2520and%2520Giuseppe%2520Siracusano%2520and%2520Roberto%2520Bifulco%26entry.1292438233%3D%2520%2520Generative%2520AI%2520agents%252C%2520software%2520systems%2520powered%2520by%2520Large%2520Language%2520Models%250A%2528LLMs%2529%252C%2520are%2520emerging%2520as%2520a%2520promising%2520approach%2520to%2520automate%2520cybersecurity%2520tasks.%250AAmong%2520the%2520others%252C%2520penetration%2520testing%2520is%2520a%2520challenging%2520field%2520due%2520to%2520the%2520task%250Acomplexity%2520and%2520the%2520diverse%2520strategies%2520to%2520simulate%2520cyber-attacks.%2520Despite%250Agrowing%2520interest%2520and%2520initial%2520studies%2520in%2520automating%2520penetration%2520testing%2520with%250Agenerative%2520agents%252C%2520there%2520remains%2520a%2520significant%2520gap%2520in%2520the%2520form%2520of%2520a%250Acomprehensive%2520and%2520standard%2520framework%2520for%2520their%2520evaluation%2520and%2520development.%2520This%250Apaper%2520introduces%2520AutoPenBench%252C%2520an%2520open%2520benchmark%2520for%2520evaluating%2520generative%250Aagents%2520in%2520automated%2520penetration%2520testing.%2520We%2520present%2520a%2520comprehensive%2520framework%250Athat%2520includes%252033%2520tasks%252C%2520each%2520representing%2520a%2520vulnerable%2520system%2520that%2520the%2520agent%250Ahas%2520to%2520attack.%2520Tasks%2520are%2520of%2520increasing%2520difficulty%2520levels%252C%2520including%2520in-vitro%250Aand%2520real-world%2520scenarios.%2520We%2520assess%2520the%2520agent%2520performance%2520with%2520generic%2520and%250Aspecific%2520milestones%2520that%2520allow%2520us%2520to%2520compare%2520results%2520in%2520a%2520standardised%2520manner%250Aand%2520understand%2520the%2520limits%2520of%2520the%2520agent%2520under%2520test.%2520We%2520show%2520the%2520benefits%2520of%250AAutoPenBench%2520by%2520testing%2520two%2520agent%2520architectures%253A%2520a%2520fully%2520autonomous%2520and%2520a%250Asemi-autonomous%2520supporting%2520human%2520interaction.%2520We%2520compare%2520their%2520performance%2520and%250Alimitations.%2520For%2520example%252C%2520the%2520fully%2520autonomous%2520agent%2520performs%2520unsatisfactorily%250Aachieving%2520a%252021%2525%2520Success%2520Rate%2520%2528SR%2529%2520across%2520the%2520benchmark%252C%2520solving%252027%2525%2520of%2520the%250Asimple%2520tasks%2520and%2520only%2520one%2520real-world%2520task.%2520In%2520contrast%252C%2520the%2520assisted%2520agent%250Ademonstrates%2520substantial%2520improvements%252C%2520with%252064%2525%2520of%2520SR.%2520AutoPenBench%2520allows%2520us%250Aalso%2520to%2520observe%2520how%2520different%2520LLMs%2520like%2520GPT-4o%2520or%2520OpenAI%2520o1%2520impact%2520the%2520ability%250Aof%2520the%2520agents%2520to%2520complete%2520the%2520tasks.%2520We%2520believe%2520that%2520our%2520benchmark%2520fills%2520the%250Agap%2520with%2520a%2520standard%2520and%2520flexible%2520framework%2520to%2520compare%2520penetration%2520testing%250Aagents%2520on%2520a%2520common%2520ground.%2520We%2520hope%2520to%2520extend%2520AutoPenBench%2520along%2520with%2520the%250Aresearch%2520community%2520by%2520making%2520it%2520available%2520under%250Ahttps%253A//github.com/lucagioacchini/auto-pen-bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoPenBench%3A%20Benchmarking%20Generative%20Agents%20for%20Penetration%20Testing&entry.906535625=Luca%20Gioacchini%20and%20Marco%20Mellia%20and%20Idilio%20Drago%20and%20Alexander%20Delsanto%20and%20Giuseppe%20Siracusano%20and%20Roberto%20Bifulco&entry.1292438233=%20%20Generative%20AI%20agents%2C%20software%20systems%20powered%20by%20Large%20Language%20Models%0A%28LLMs%29%2C%20are%20emerging%20as%20a%20promising%20approach%20to%20automate%20cybersecurity%20tasks.%0AAmong%20the%20others%2C%20penetration%20testing%20is%20a%20challenging%20field%20due%20to%20the%20task%0Acomplexity%20and%20the%20diverse%20strategies%20to%20simulate%20cyber-attacks.%20Despite%0Agrowing%20interest%20and%20initial%20studies%20in%20automating%20penetration%20testing%20with%0Agenerative%20agents%2C%20there%20remains%20a%20significant%20gap%20in%20the%20form%20of%20a%0Acomprehensive%20and%20standard%20framework%20for%20their%20evaluation%20and%20development.%20This%0Apaper%20introduces%20AutoPenBench%2C%20an%20open%20benchmark%20for%20evaluating%20generative%0Aagents%20in%20automated%20penetration%20testing.%20We%20present%20a%20comprehensive%20framework%0Athat%20includes%2033%20tasks%2C%20each%20representing%20a%20vulnerable%20system%20that%20the%20agent%0Ahas%20to%20attack.%20Tasks%20are%20of%20increasing%20difficulty%20levels%2C%20including%20in-vitro%0Aand%20real-world%20scenarios.%20We%20assess%20the%20agent%20performance%20with%20generic%20and%0Aspecific%20milestones%20that%20allow%20us%20to%20compare%20results%20in%20a%20standardised%20manner%0Aand%20understand%20the%20limits%20of%20the%20agent%20under%20test.%20We%20show%20the%20benefits%20of%0AAutoPenBench%20by%20testing%20two%20agent%20architectures%3A%20a%20fully%20autonomous%20and%20a%0Asemi-autonomous%20supporting%20human%20interaction.%20We%20compare%20their%20performance%20and%0Alimitations.%20For%20example%2C%20the%20fully%20autonomous%20agent%20performs%20unsatisfactorily%0Aachieving%20a%2021%25%20Success%20Rate%20%28SR%29%20across%20the%20benchmark%2C%20solving%2027%25%20of%20the%0Asimple%20tasks%20and%20only%20one%20real-world%20task.%20In%20contrast%2C%20the%20assisted%20agent%0Ademonstrates%20substantial%20improvements%2C%20with%2064%25%20of%20SR.%20AutoPenBench%20allows%20us%0Aalso%20to%20observe%20how%20different%20LLMs%20like%20GPT-4o%20or%20OpenAI%20o1%20impact%20the%20ability%0Aof%20the%20agents%20to%20complete%20the%20tasks.%20We%20believe%20that%20our%20benchmark%20fills%20the%0Agap%20with%20a%20standard%20and%20flexible%20framework%20to%20compare%20penetration%20testing%0Aagents%20on%20a%20common%20ground.%20We%20hope%20to%20extend%20AutoPenBench%20along%20with%20the%0Aresearch%20community%20by%20making%20it%20available%20under%0Ahttps%3A//github.com/lucagioacchini/auto-pen-bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03225v2&entry.124074799=Read"},
{"title": "SeriesGAN: Time Series Generation via Adversarial and Autoregressive\n  Learning", "author": "MohammadReza EskandariNasab and Shah Muhammad Hamdi and Soukaina Filali Boubrahimi", "abstract": "  Current Generative Adversarial Network (GAN)-based approaches for time series\ngeneration face challenges such as suboptimal convergence, information loss in\nembedding spaces, and instability. To overcome these challenges, we introduce\nan advanced framework that integrates the advantages of an\nautoencoder-generated embedding space with the adversarial training dynamics of\nGANs. This method employs two discriminators: one to specifically guide the\ngenerator and another to refine both the autoencoder's and generator's output.\nAdditionally, our framework incorporates a novel autoencoder-based loss\nfunction and supervision from a teacher-forcing supervisor network, which\ncaptures the stepwise conditional distributions of the data. The generator\noperates within the latent space, while the two discriminators work on latent\nand feature spaces separately, providing crucial feedback to both the generator\nand the autoencoder. By leveraging this dual-discriminator approach, we\nminimize information loss in the embedding space. Through joint training, our\nframework excels at generating high-fidelity time series data, consistently\noutperforming existing state-of-the-art benchmarks both qualitatively and\nquantitatively across a range of real and synthetic multivariate time series\ndatasets.\n", "link": "http://arxiv.org/abs/2410.21203v1", "date": "2024-10-28", "relevancy": 2.0378, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5422}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5171}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeriesGAN%3A%20Time%20Series%20Generation%20via%20Adversarial%20and%20Autoregressive%0A%20%20Learning&body=Title%3A%20SeriesGAN%3A%20Time%20Series%20Generation%20via%20Adversarial%20and%20Autoregressive%0A%20%20Learning%0AAuthor%3A%20MohammadReza%20EskandariNasab%20and%20Shah%20Muhammad%20Hamdi%20and%20Soukaina%20Filali%20Boubrahimi%0AAbstract%3A%20%20%20Current%20Generative%20Adversarial%20Network%20%28GAN%29-based%20approaches%20for%20time%20series%0Ageneration%20face%20challenges%20such%20as%20suboptimal%20convergence%2C%20information%20loss%20in%0Aembedding%20spaces%2C%20and%20instability.%20To%20overcome%20these%20challenges%2C%20we%20introduce%0Aan%20advanced%20framework%20that%20integrates%20the%20advantages%20of%20an%0Aautoencoder-generated%20embedding%20space%20with%20the%20adversarial%20training%20dynamics%20of%0AGANs.%20This%20method%20employs%20two%20discriminators%3A%20one%20to%20specifically%20guide%20the%0Agenerator%20and%20another%20to%20refine%20both%20the%20autoencoder%27s%20and%20generator%27s%20output.%0AAdditionally%2C%20our%20framework%20incorporates%20a%20novel%20autoencoder-based%20loss%0Afunction%20and%20supervision%20from%20a%20teacher-forcing%20supervisor%20network%2C%20which%0Acaptures%20the%20stepwise%20conditional%20distributions%20of%20the%20data.%20The%20generator%0Aoperates%20within%20the%20latent%20space%2C%20while%20the%20two%20discriminators%20work%20on%20latent%0Aand%20feature%20spaces%20separately%2C%20providing%20crucial%20feedback%20to%20both%20the%20generator%0Aand%20the%20autoencoder.%20By%20leveraging%20this%20dual-discriminator%20approach%2C%20we%0Aminimize%20information%20loss%20in%20the%20embedding%20space.%20Through%20joint%20training%2C%20our%0Aframework%20excels%20at%20generating%20high-fidelity%20time%20series%20data%2C%20consistently%0Aoutperforming%20existing%20state-of-the-art%20benchmarks%20both%20qualitatively%20and%0Aquantitatively%20across%20a%20range%20of%20real%20and%20synthetic%20multivariate%20time%20series%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeriesGAN%253A%2520Time%2520Series%2520Generation%2520via%2520Adversarial%2520and%2520Autoregressive%250A%2520%2520Learning%26entry.906535625%3DMohammadReza%2520EskandariNasab%2520and%2520Shah%2520Muhammad%2520Hamdi%2520and%2520Soukaina%2520Filali%2520Boubrahimi%26entry.1292438233%3D%2520%2520Current%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529-based%2520approaches%2520for%2520time%2520series%250Ageneration%2520face%2520challenges%2520such%2520as%2520suboptimal%2520convergence%252C%2520information%2520loss%2520in%250Aembedding%2520spaces%252C%2520and%2520instability.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%250Aan%2520advanced%2520framework%2520that%2520integrates%2520the%2520advantages%2520of%2520an%250Aautoencoder-generated%2520embedding%2520space%2520with%2520the%2520adversarial%2520training%2520dynamics%2520of%250AGANs.%2520This%2520method%2520employs%2520two%2520discriminators%253A%2520one%2520to%2520specifically%2520guide%2520the%250Agenerator%2520and%2520another%2520to%2520refine%2520both%2520the%2520autoencoder%2527s%2520and%2520generator%2527s%2520output.%250AAdditionally%252C%2520our%2520framework%2520incorporates%2520a%2520novel%2520autoencoder-based%2520loss%250Afunction%2520and%2520supervision%2520from%2520a%2520teacher-forcing%2520supervisor%2520network%252C%2520which%250Acaptures%2520the%2520stepwise%2520conditional%2520distributions%2520of%2520the%2520data.%2520The%2520generator%250Aoperates%2520within%2520the%2520latent%2520space%252C%2520while%2520the%2520two%2520discriminators%2520work%2520on%2520latent%250Aand%2520feature%2520spaces%2520separately%252C%2520providing%2520crucial%2520feedback%2520to%2520both%2520the%2520generator%250Aand%2520the%2520autoencoder.%2520By%2520leveraging%2520this%2520dual-discriminator%2520approach%252C%2520we%250Aminimize%2520information%2520loss%2520in%2520the%2520embedding%2520space.%2520Through%2520joint%2520training%252C%2520our%250Aframework%2520excels%2520at%2520generating%2520high-fidelity%2520time%2520series%2520data%252C%2520consistently%250Aoutperforming%2520existing%2520state-of-the-art%2520benchmarks%2520both%2520qualitatively%2520and%250Aquantitatively%2520across%2520a%2520range%2520of%2520real%2520and%2520synthetic%2520multivariate%2520time%2520series%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeriesGAN%3A%20Time%20Series%20Generation%20via%20Adversarial%20and%20Autoregressive%0A%20%20Learning&entry.906535625=MohammadReza%20EskandariNasab%20and%20Shah%20Muhammad%20Hamdi%20and%20Soukaina%20Filali%20Boubrahimi&entry.1292438233=%20%20Current%20Generative%20Adversarial%20Network%20%28GAN%29-based%20approaches%20for%20time%20series%0Ageneration%20face%20challenges%20such%20as%20suboptimal%20convergence%2C%20information%20loss%20in%0Aembedding%20spaces%2C%20and%20instability.%20To%20overcome%20these%20challenges%2C%20we%20introduce%0Aan%20advanced%20framework%20that%20integrates%20the%20advantages%20of%20an%0Aautoencoder-generated%20embedding%20space%20with%20the%20adversarial%20training%20dynamics%20of%0AGANs.%20This%20method%20employs%20two%20discriminators%3A%20one%20to%20specifically%20guide%20the%0Agenerator%20and%20another%20to%20refine%20both%20the%20autoencoder%27s%20and%20generator%27s%20output.%0AAdditionally%2C%20our%20framework%20incorporates%20a%20novel%20autoencoder-based%20loss%0Afunction%20and%20supervision%20from%20a%20teacher-forcing%20supervisor%20network%2C%20which%0Acaptures%20the%20stepwise%20conditional%20distributions%20of%20the%20data.%20The%20generator%0Aoperates%20within%20the%20latent%20space%2C%20while%20the%20two%20discriminators%20work%20on%20latent%0Aand%20feature%20spaces%20separately%2C%20providing%20crucial%20feedback%20to%20both%20the%20generator%0Aand%20the%20autoencoder.%20By%20leveraging%20this%20dual-discriminator%20approach%2C%20we%0Aminimize%20information%20loss%20in%20the%20embedding%20space.%20Through%20joint%20training%2C%20our%0Aframework%20excels%20at%20generating%20high-fidelity%20time%20series%20data%2C%20consistently%0Aoutperforming%20existing%20state-of-the-art%20benchmarks%20both%20qualitatively%20and%0Aquantitatively%20across%20a%20range%20of%20real%20and%20synthetic%20multivariate%20time%20series%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21203v1&entry.124074799=Read"},
{"title": "Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback", "author": "Nour Jedidi and Yung-Sung Chuang and Leslie Shing and James Glass", "abstract": "  Building effective dense retrieval systems remains difficult when relevance\nsupervision is not available. Recent work has looked to overcome this challenge\nby using a Large Language Model (LLM) to generate hypothetical documents that\ncan be used to find the closest real document. However, this approach relies\nsolely on the LLM to have domain-specific knowledge relevant to the query,\nwhich may not be practical. Furthermore, generating hypothetical documents can\nbe inefficient as it requires the LLM to generate a large number of tokens for\neach query. To address these challenges, we introduce Real Document Embeddings\nfrom Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF\nproposes to re-frame hypothetical document generation as a relevance estimation\ntask, using an LLM to select which documents should be used for nearest\nneighbor search. Through this re-framing, the LLM no longer needs\ndomain-specific knowledge but only needs to judge what is relevant.\nAdditionally, relevance estimation only requires the LLM to output a single\ntoken, thereby improving search latency. Our experiments show that ReDE-RF\nconsistently surpasses state-of-the-art zero-shot dense retrieval methods\nacross a wide range of low-resource retrieval datasets while also making\nsignificant improvements in latency per-query.\n", "link": "http://arxiv.org/abs/2410.21242v1", "date": "2024-10-28", "relevancy": 2.0372, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Dense%20Retrieval%20with%20Embeddings%20from%20Relevance%20Feedback&body=Title%3A%20Zero-Shot%20Dense%20Retrieval%20with%20Embeddings%20from%20Relevance%20Feedback%0AAuthor%3A%20Nour%20Jedidi%20and%20Yung-Sung%20Chuang%20and%20Leslie%20Shing%20and%20James%20Glass%0AAbstract%3A%20%20%20Building%20effective%20dense%20retrieval%20systems%20remains%20difficult%20when%20relevance%0Asupervision%20is%20not%20available.%20Recent%20work%20has%20looked%20to%20overcome%20this%20challenge%0Aby%20using%20a%20Large%20Language%20Model%20%28LLM%29%20to%20generate%20hypothetical%20documents%20that%0Acan%20be%20used%20to%20find%20the%20closest%20real%20document.%20However%2C%20this%20approach%20relies%0Asolely%20on%20the%20LLM%20to%20have%20domain-specific%20knowledge%20relevant%20to%20the%20query%2C%0Awhich%20may%20not%20be%20practical.%20Furthermore%2C%20generating%20hypothetical%20documents%20can%0Abe%20inefficient%20as%20it%20requires%20the%20LLM%20to%20generate%20a%20large%20number%20of%20tokens%20for%0Aeach%20query.%20To%20address%20these%20challenges%2C%20we%20introduce%20Real%20Document%20Embeddings%0Afrom%20Relevance%20Feedback%20%28ReDE-RF%29.%20Inspired%20by%20relevance%20feedback%2C%20ReDE-RF%0Aproposes%20to%20re-frame%20hypothetical%20document%20generation%20as%20a%20relevance%20estimation%0Atask%2C%20using%20an%20LLM%20to%20select%20which%20documents%20should%20be%20used%20for%20nearest%0Aneighbor%20search.%20Through%20this%20re-framing%2C%20the%20LLM%20no%20longer%20needs%0Adomain-specific%20knowledge%20but%20only%20needs%20to%20judge%20what%20is%20relevant.%0AAdditionally%2C%20relevance%20estimation%20only%20requires%20the%20LLM%20to%20output%20a%20single%0Atoken%2C%20thereby%20improving%20search%20latency.%20Our%20experiments%20show%20that%20ReDE-RF%0Aconsistently%20surpasses%20state-of-the-art%20zero-shot%20dense%20retrieval%20methods%0Aacross%20a%20wide%20range%20of%20low-resource%20retrieval%20datasets%20while%20also%20making%0Asignificant%20improvements%20in%20latency%20per-query.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Dense%2520Retrieval%2520with%2520Embeddings%2520from%2520Relevance%2520Feedback%26entry.906535625%3DNour%2520Jedidi%2520and%2520Yung-Sung%2520Chuang%2520and%2520Leslie%2520Shing%2520and%2520James%2520Glass%26entry.1292438233%3D%2520%2520Building%2520effective%2520dense%2520retrieval%2520systems%2520remains%2520difficult%2520when%2520relevance%250Asupervision%2520is%2520not%2520available.%2520Recent%2520work%2520has%2520looked%2520to%2520overcome%2520this%2520challenge%250Aby%2520using%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520generate%2520hypothetical%2520documents%2520that%250Acan%2520be%2520used%2520to%2520find%2520the%2520closest%2520real%2520document.%2520However%252C%2520this%2520approach%2520relies%250Asolely%2520on%2520the%2520LLM%2520to%2520have%2520domain-specific%2520knowledge%2520relevant%2520to%2520the%2520query%252C%250Awhich%2520may%2520not%2520be%2520practical.%2520Furthermore%252C%2520generating%2520hypothetical%2520documents%2520can%250Abe%2520inefficient%2520as%2520it%2520requires%2520the%2520LLM%2520to%2520generate%2520a%2520large%2520number%2520of%2520tokens%2520for%250Aeach%2520query.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Real%2520Document%2520Embeddings%250Afrom%2520Relevance%2520Feedback%2520%2528ReDE-RF%2529.%2520Inspired%2520by%2520relevance%2520feedback%252C%2520ReDE-RF%250Aproposes%2520to%2520re-frame%2520hypothetical%2520document%2520generation%2520as%2520a%2520relevance%2520estimation%250Atask%252C%2520using%2520an%2520LLM%2520to%2520select%2520which%2520documents%2520should%2520be%2520used%2520for%2520nearest%250Aneighbor%2520search.%2520Through%2520this%2520re-framing%252C%2520the%2520LLM%2520no%2520longer%2520needs%250Adomain-specific%2520knowledge%2520but%2520only%2520needs%2520to%2520judge%2520what%2520is%2520relevant.%250AAdditionally%252C%2520relevance%2520estimation%2520only%2520requires%2520the%2520LLM%2520to%2520output%2520a%2520single%250Atoken%252C%2520thereby%2520improving%2520search%2520latency.%2520Our%2520experiments%2520show%2520that%2520ReDE-RF%250Aconsistently%2520surpasses%2520state-of-the-art%2520zero-shot%2520dense%2520retrieval%2520methods%250Aacross%2520a%2520wide%2520range%2520of%2520low-resource%2520retrieval%2520datasets%2520while%2520also%2520making%250Asignificant%2520improvements%2520in%2520latency%2520per-query.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Dense%20Retrieval%20with%20Embeddings%20from%20Relevance%20Feedback&entry.906535625=Nour%20Jedidi%20and%20Yung-Sung%20Chuang%20and%20Leslie%20Shing%20and%20James%20Glass&entry.1292438233=%20%20Building%20effective%20dense%20retrieval%20systems%20remains%20difficult%20when%20relevance%0Asupervision%20is%20not%20available.%20Recent%20work%20has%20looked%20to%20overcome%20this%20challenge%0Aby%20using%20a%20Large%20Language%20Model%20%28LLM%29%20to%20generate%20hypothetical%20documents%20that%0Acan%20be%20used%20to%20find%20the%20closest%20real%20document.%20However%2C%20this%20approach%20relies%0Asolely%20on%20the%20LLM%20to%20have%20domain-specific%20knowledge%20relevant%20to%20the%20query%2C%0Awhich%20may%20not%20be%20practical.%20Furthermore%2C%20generating%20hypothetical%20documents%20can%0Abe%20inefficient%20as%20it%20requires%20the%20LLM%20to%20generate%20a%20large%20number%20of%20tokens%20for%0Aeach%20query.%20To%20address%20these%20challenges%2C%20we%20introduce%20Real%20Document%20Embeddings%0Afrom%20Relevance%20Feedback%20%28ReDE-RF%29.%20Inspired%20by%20relevance%20feedback%2C%20ReDE-RF%0Aproposes%20to%20re-frame%20hypothetical%20document%20generation%20as%20a%20relevance%20estimation%0Atask%2C%20using%20an%20LLM%20to%20select%20which%20documents%20should%20be%20used%20for%20nearest%0Aneighbor%20search.%20Through%20this%20re-framing%2C%20the%20LLM%20no%20longer%20needs%0Adomain-specific%20knowledge%20but%20only%20needs%20to%20judge%20what%20is%20relevant.%0AAdditionally%2C%20relevance%20estimation%20only%20requires%20the%20LLM%20to%20output%20a%20single%0Atoken%2C%20thereby%20improving%20search%20latency.%20Our%20experiments%20show%20that%20ReDE-RF%0Aconsistently%20surpasses%20state-of-the-art%20zero-shot%20dense%20retrieval%20methods%0Aacross%20a%20wide%20range%20of%20low-resource%20retrieval%20datasets%20while%20also%20making%0Asignificant%20improvements%20in%20latency%20per-query.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21242v1&entry.124074799=Read"},
{"title": "Modular Duality in Deep Learning", "author": "Jeremy Bernstein and Laker Newhouse", "abstract": "  An old idea in optimization theory says that since the gradient is a dual\nvector it may not be subtracted from the weights without first being mapped to\nthe primal space where the weights reside. We take this idea seriously in this\npaper and construct such a duality map for general neural networks. Our map,\nwhich we call modular dualization, forms a unifying theoretical basis for\ntraining algorithms that are a) fast and b) scalable. Modular dualization\ninvolves first assigning operator norms to layers based on the semantics of\neach layer, and then using these layerwise norms to recursively induce a\nduality map on the weight space of the full neural architecture. We conclude by\nderiving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers\n-- the latter two methods are based on a new rectangular Newton-Schulz\niteration that we propose. Our iteration was recently used to set new speed\nrecords for training NanoGPT. Overall, we hope that our theory of modular\nduality will yield a next generation of fast and scalable optimizers for\ngeneral neural architectures.\n", "link": "http://arxiv.org/abs/2410.21265v1", "date": "2024-10-28", "relevancy": 2.0307, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5478}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5097}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Duality%20in%20Deep%20Learning&body=Title%3A%20Modular%20Duality%20in%20Deep%20Learning%0AAuthor%3A%20Jeremy%20Bernstein%20and%20Laker%20Newhouse%0AAbstract%3A%20%20%20An%20old%20idea%20in%20optimization%20theory%20says%20that%20since%20the%20gradient%20is%20a%20dual%0Avector%20it%20may%20not%20be%20subtracted%20from%20the%20weights%20without%20first%20being%20mapped%20to%0Athe%20primal%20space%20where%20the%20weights%20reside.%20We%20take%20this%20idea%20seriously%20in%20this%0Apaper%20and%20construct%20such%20a%20duality%20map%20for%20general%20neural%20networks.%20Our%20map%2C%0Awhich%20we%20call%20modular%20dualization%2C%20forms%20a%20unifying%20theoretical%20basis%20for%0Atraining%20algorithms%20that%20are%20a%29%20fast%20and%20b%29%20scalable.%20Modular%20dualization%0Ainvolves%20first%20assigning%20operator%20norms%20to%20layers%20based%20on%20the%20semantics%20of%0Aeach%20layer%2C%20and%20then%20using%20these%20layerwise%20norms%20to%20recursively%20induce%20a%0Aduality%20map%20on%20the%20weight%20space%20of%20the%20full%20neural%20architecture.%20We%20conclude%20by%0Aderiving%20GPU-friendly%20algorithms%20for%20dualizing%20Embed%2C%20Linear%20and%20Conv2D%20layers%0A--%20the%20latter%20two%20methods%20are%20based%20on%20a%20new%20rectangular%20Newton-Schulz%0Aiteration%20that%20we%20propose.%20Our%20iteration%20was%20recently%20used%20to%20set%20new%20speed%0Arecords%20for%20training%20NanoGPT.%20Overall%2C%20we%20hope%20that%20our%20theory%20of%20modular%0Aduality%20will%20yield%20a%20next%20generation%20of%20fast%20and%20scalable%20optimizers%20for%0Ageneral%20neural%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Duality%2520in%2520Deep%2520Learning%26entry.906535625%3DJeremy%2520Bernstein%2520and%2520Laker%2520Newhouse%26entry.1292438233%3D%2520%2520An%2520old%2520idea%2520in%2520optimization%2520theory%2520says%2520that%2520since%2520the%2520gradient%2520is%2520a%2520dual%250Avector%2520it%2520may%2520not%2520be%2520subtracted%2520from%2520the%2520weights%2520without%2520first%2520being%2520mapped%2520to%250Athe%2520primal%2520space%2520where%2520the%2520weights%2520reside.%2520We%2520take%2520this%2520idea%2520seriously%2520in%2520this%250Apaper%2520and%2520construct%2520such%2520a%2520duality%2520map%2520for%2520general%2520neural%2520networks.%2520Our%2520map%252C%250Awhich%2520we%2520call%2520modular%2520dualization%252C%2520forms%2520a%2520unifying%2520theoretical%2520basis%2520for%250Atraining%2520algorithms%2520that%2520are%2520a%2529%2520fast%2520and%2520b%2529%2520scalable.%2520Modular%2520dualization%250Ainvolves%2520first%2520assigning%2520operator%2520norms%2520to%2520layers%2520based%2520on%2520the%2520semantics%2520of%250Aeach%2520layer%252C%2520and%2520then%2520using%2520these%2520layerwise%2520norms%2520to%2520recursively%2520induce%2520a%250Aduality%2520map%2520on%2520the%2520weight%2520space%2520of%2520the%2520full%2520neural%2520architecture.%2520We%2520conclude%2520by%250Aderiving%2520GPU-friendly%2520algorithms%2520for%2520dualizing%2520Embed%252C%2520Linear%2520and%2520Conv2D%2520layers%250A--%2520the%2520latter%2520two%2520methods%2520are%2520based%2520on%2520a%2520new%2520rectangular%2520Newton-Schulz%250Aiteration%2520that%2520we%2520propose.%2520Our%2520iteration%2520was%2520recently%2520used%2520to%2520set%2520new%2520speed%250Arecords%2520for%2520training%2520NanoGPT.%2520Overall%252C%2520we%2520hope%2520that%2520our%2520theory%2520of%2520modular%250Aduality%2520will%2520yield%2520a%2520next%2520generation%2520of%2520fast%2520and%2520scalable%2520optimizers%2520for%250Ageneral%2520neural%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Duality%20in%20Deep%20Learning&entry.906535625=Jeremy%20Bernstein%20and%20Laker%20Newhouse&entry.1292438233=%20%20An%20old%20idea%20in%20optimization%20theory%20says%20that%20since%20the%20gradient%20is%20a%20dual%0Avector%20it%20may%20not%20be%20subtracted%20from%20the%20weights%20without%20first%20being%20mapped%20to%0Athe%20primal%20space%20where%20the%20weights%20reside.%20We%20take%20this%20idea%20seriously%20in%20this%0Apaper%20and%20construct%20such%20a%20duality%20map%20for%20general%20neural%20networks.%20Our%20map%2C%0Awhich%20we%20call%20modular%20dualization%2C%20forms%20a%20unifying%20theoretical%20basis%20for%0Atraining%20algorithms%20that%20are%20a%29%20fast%20and%20b%29%20scalable.%20Modular%20dualization%0Ainvolves%20first%20assigning%20operator%20norms%20to%20layers%20based%20on%20the%20semantics%20of%0Aeach%20layer%2C%20and%20then%20using%20these%20layerwise%20norms%20to%20recursively%20induce%20a%0Aduality%20map%20on%20the%20weight%20space%20of%20the%20full%20neural%20architecture.%20We%20conclude%20by%0Aderiving%20GPU-friendly%20algorithms%20for%20dualizing%20Embed%2C%20Linear%20and%20Conv2D%20layers%0A--%20the%20latter%20two%20methods%20are%20based%20on%20a%20new%20rectangular%20Newton-Schulz%0Aiteration%20that%20we%20propose.%20Our%20iteration%20was%20recently%20used%20to%20set%20new%20speed%0Arecords%20for%20training%20NanoGPT.%20Overall%2C%20we%20hope%20that%20our%20theory%20of%20modular%0Aduality%20will%20yield%20a%20next%20generation%20of%20fast%20and%20scalable%20optimizers%20for%0Ageneral%20neural%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21265v1&entry.124074799=Read"},
{"title": "EoRA: Training-free Compensation for Compressed LLM with Eigenspace\n  Low-Rank Approximation", "author": "Shih-Yang Liu and Huck Yang and Chein-Yi Wang and Nai Chit Fung and Hongxu Yin and Charbel Sakr and Saurav Muralidharan and Kwang-Ting Cheng and Jan Kautz and Yu-Chiang Frank Wang and Pavlo Molchanov and Min-Hung Chen", "abstract": "  In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in adjusting overall capacity without being constrained by specific\ncompression formats. However, naively applying SVD to derive residual paths\ncauses suboptimal utilization of the low-rank representation capacity. Instead,\nwe propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method\nthat directly minimizes compression-induced errors without requiring\ngradient-based training, achieving fast optimization in minutes using a small\namount of calibration data. EoRA projects compression errors into the\neigenspace of input activations, leveraging eigenvalues to effectively\nprioritize the reconstruction of high-importance error components. Moreover,\nEoRA can be seamlessly integrated with fine-tuning and quantization to further\nimprove effectiveness and efficiency. EoRA consistently outperforms previous\nmethods in compensating errors for compressed LLaMA2/3 models on various tasks,\nsuch as language generation, commonsense reasoning, and math reasoning tasks\n(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and\nMathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4\nsparsity). EoRA offers a scalable, training-free solution to compensate for\ncompression errors, making it a powerful tool to deploy LLMs in various\ncapacity and efficiency requirements.\n", "link": "http://arxiv.org/abs/2410.21271v1", "date": "2024-10-28", "relevancy": 1.9793, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5002}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4944}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EoRA%3A%20Training-free%20Compensation%20for%20Compressed%20LLM%20with%20Eigenspace%0A%20%20Low-Rank%20Approximation&body=Title%3A%20EoRA%3A%20Training-free%20Compensation%20for%20Compressed%20LLM%20with%20Eigenspace%0A%20%20Low-Rank%20Approximation%0AAuthor%3A%20Shih-Yang%20Liu%20and%20Huck%20Yang%20and%20Chein-Yi%20Wang%20and%20Nai%20Chit%20Fung%20and%20Hongxu%20Yin%20and%20Charbel%20Sakr%20and%20Saurav%20Muralidharan%20and%20Kwang-Ting%20Cheng%20and%20Jan%20Kautz%20and%20Yu-Chiang%20Frank%20Wang%20and%20Pavlo%20Molchanov%20and%20Min-Hung%20Chen%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20re-formulate%20the%20model%20compression%20problem%20into%20the%0Acustomized%20compensation%20problem%3A%20Given%20a%20compressed%20model%2C%20we%20aim%20to%20introduce%0Aresidual%20low-rank%20paths%20to%20compensate%20for%20compression%20errors%20under%20customized%0Arequirements%20from%20users%20%28e.g.%2C%20tasks%2C%20compression%20ratios%29%2C%20resulting%20in%20greater%0Aflexibility%20in%20adjusting%20overall%20capacity%20without%20being%20constrained%20by%20specific%0Acompression%20formats.%20However%2C%20naively%20applying%20SVD%20to%20derive%20residual%20paths%0Acauses%20suboptimal%20utilization%20of%20the%20low-rank%20representation%20capacity.%20Instead%2C%0Awe%20propose%20Training-free%20Eigenspace%20Low-Rank%20Approximation%20%28EoRA%29%2C%20a%20method%0Athat%20directly%20minimizes%20compression-induced%20errors%20without%20requiring%0Agradient-based%20training%2C%20achieving%20fast%20optimization%20in%20minutes%20using%20a%20small%0Aamount%20of%20calibration%20data.%20EoRA%20projects%20compression%20errors%20into%20the%0Aeigenspace%20of%20input%20activations%2C%20leveraging%20eigenvalues%20to%20effectively%0Aprioritize%20the%20reconstruction%20of%20high-importance%20error%20components.%20Moreover%2C%0AEoRA%20can%20be%20seamlessly%20integrated%20with%20fine-tuning%20and%20quantization%20to%20further%0Aimprove%20effectiveness%20and%20efficiency.%20EoRA%20consistently%20outperforms%20previous%0Amethods%20in%20compensating%20errors%20for%20compressed%20LLaMA2/3%20models%20on%20various%20tasks%2C%0Asuch%20as%20language%20generation%2C%20commonsense%20reasoning%2C%20and%20math%20reasoning%20tasks%0A%28e.g.%2C%2031.31%25/12.88%25%20and%209.69%25%20improvements%20on%20ARC-Easy/ARC-Challenge%20and%0AMathQA%20when%20compensating%20LLaMA3-8B%20that%20is%20quantized%20to%204-bit%20and%20pruned%20to%202%3A4%0Asparsity%29.%20EoRA%20offers%20a%20scalable%2C%20training-free%20solution%20to%20compensate%20for%0Acompression%20errors%2C%20making%20it%20a%20powerful%20tool%20to%20deploy%20LLMs%20in%20various%0Acapacity%20and%20efficiency%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEoRA%253A%2520Training-free%2520Compensation%2520for%2520Compressed%2520LLM%2520with%2520Eigenspace%250A%2520%2520Low-Rank%2520Approximation%26entry.906535625%3DShih-Yang%2520Liu%2520and%2520Huck%2520Yang%2520and%2520Chein-Yi%2520Wang%2520and%2520Nai%2520Chit%2520Fung%2520and%2520Hongxu%2520Yin%2520and%2520Charbel%2520Sakr%2520and%2520Saurav%2520Muralidharan%2520and%2520Kwang-Ting%2520Cheng%2520and%2520Jan%2520Kautz%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Pavlo%2520Molchanov%2520and%2520Min-Hung%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520re-formulate%2520the%2520model%2520compression%2520problem%2520into%2520the%250Acustomized%2520compensation%2520problem%253A%2520Given%2520a%2520compressed%2520model%252C%2520we%2520aim%2520to%2520introduce%250Aresidual%2520low-rank%2520paths%2520to%2520compensate%2520for%2520compression%2520errors%2520under%2520customized%250Arequirements%2520from%2520users%2520%2528e.g.%252C%2520tasks%252C%2520compression%2520ratios%2529%252C%2520resulting%2520in%2520greater%250Aflexibility%2520in%2520adjusting%2520overall%2520capacity%2520without%2520being%2520constrained%2520by%2520specific%250Acompression%2520formats.%2520However%252C%2520naively%2520applying%2520SVD%2520to%2520derive%2520residual%2520paths%250Acauses%2520suboptimal%2520utilization%2520of%2520the%2520low-rank%2520representation%2520capacity.%2520Instead%252C%250Awe%2520propose%2520Training-free%2520Eigenspace%2520Low-Rank%2520Approximation%2520%2528EoRA%2529%252C%2520a%2520method%250Athat%2520directly%2520minimizes%2520compression-induced%2520errors%2520without%2520requiring%250Agradient-based%2520training%252C%2520achieving%2520fast%2520optimization%2520in%2520minutes%2520using%2520a%2520small%250Aamount%2520of%2520calibration%2520data.%2520EoRA%2520projects%2520compression%2520errors%2520into%2520the%250Aeigenspace%2520of%2520input%2520activations%252C%2520leveraging%2520eigenvalues%2520to%2520effectively%250Aprioritize%2520the%2520reconstruction%2520of%2520high-importance%2520error%2520components.%2520Moreover%252C%250AEoRA%2520can%2520be%2520seamlessly%2520integrated%2520with%2520fine-tuning%2520and%2520quantization%2520to%2520further%250Aimprove%2520effectiveness%2520and%2520efficiency.%2520EoRA%2520consistently%2520outperforms%2520previous%250Amethods%2520in%2520compensating%2520errors%2520for%2520compressed%2520LLaMA2/3%2520models%2520on%2520various%2520tasks%252C%250Asuch%2520as%2520language%2520generation%252C%2520commonsense%2520reasoning%252C%2520and%2520math%2520reasoning%2520tasks%250A%2528e.g.%252C%252031.31%2525/12.88%2525%2520and%25209.69%2525%2520improvements%2520on%2520ARC-Easy/ARC-Challenge%2520and%250AMathQA%2520when%2520compensating%2520LLaMA3-8B%2520that%2520is%2520quantized%2520to%25204-bit%2520and%2520pruned%2520to%25202%253A4%250Asparsity%2529.%2520EoRA%2520offers%2520a%2520scalable%252C%2520training-free%2520solution%2520to%2520compensate%2520for%250Acompression%2520errors%252C%2520making%2520it%2520a%2520powerful%2520tool%2520to%2520deploy%2520LLMs%2520in%2520various%250Acapacity%2520and%2520efficiency%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EoRA%3A%20Training-free%20Compensation%20for%20Compressed%20LLM%20with%20Eigenspace%0A%20%20Low-Rank%20Approximation&entry.906535625=Shih-Yang%20Liu%20and%20Huck%20Yang%20and%20Chein-Yi%20Wang%20and%20Nai%20Chit%20Fung%20and%20Hongxu%20Yin%20and%20Charbel%20Sakr%20and%20Saurav%20Muralidharan%20and%20Kwang-Ting%20Cheng%20and%20Jan%20Kautz%20and%20Yu-Chiang%20Frank%20Wang%20and%20Pavlo%20Molchanov%20and%20Min-Hung%20Chen&entry.1292438233=%20%20In%20this%20work%2C%20we%20re-formulate%20the%20model%20compression%20problem%20into%20the%0Acustomized%20compensation%20problem%3A%20Given%20a%20compressed%20model%2C%20we%20aim%20to%20introduce%0Aresidual%20low-rank%20paths%20to%20compensate%20for%20compression%20errors%20under%20customized%0Arequirements%20from%20users%20%28e.g.%2C%20tasks%2C%20compression%20ratios%29%2C%20resulting%20in%20greater%0Aflexibility%20in%20adjusting%20overall%20capacity%20without%20being%20constrained%20by%20specific%0Acompression%20formats.%20However%2C%20naively%20applying%20SVD%20to%20derive%20residual%20paths%0Acauses%20suboptimal%20utilization%20of%20the%20low-rank%20representation%20capacity.%20Instead%2C%0Awe%20propose%20Training-free%20Eigenspace%20Low-Rank%20Approximation%20%28EoRA%29%2C%20a%20method%0Athat%20directly%20minimizes%20compression-induced%20errors%20without%20requiring%0Agradient-based%20training%2C%20achieving%20fast%20optimization%20in%20minutes%20using%20a%20small%0Aamount%20of%20calibration%20data.%20EoRA%20projects%20compression%20errors%20into%20the%0Aeigenspace%20of%20input%20activations%2C%20leveraging%20eigenvalues%20to%20effectively%0Aprioritize%20the%20reconstruction%20of%20high-importance%20error%20components.%20Moreover%2C%0AEoRA%20can%20be%20seamlessly%20integrated%20with%20fine-tuning%20and%20quantization%20to%20further%0Aimprove%20effectiveness%20and%20efficiency.%20EoRA%20consistently%20outperforms%20previous%0Amethods%20in%20compensating%20errors%20for%20compressed%20LLaMA2/3%20models%20on%20various%20tasks%2C%0Asuch%20as%20language%20generation%2C%20commonsense%20reasoning%2C%20and%20math%20reasoning%20tasks%0A%28e.g.%2C%2031.31%25/12.88%25%20and%209.69%25%20improvements%20on%20ARC-Easy/ARC-Challenge%20and%0AMathQA%20when%20compensating%20LLaMA3-8B%20that%20is%20quantized%20to%204-bit%20and%20pruned%20to%202%3A4%0Asparsity%29.%20EoRA%20offers%20a%20scalable%2C%20training-free%20solution%20to%20compensate%20for%0Acompression%20errors%2C%20making%20it%20a%20powerful%20tool%20to%20deploy%20LLMs%20in%20various%0Acapacity%20and%20efficiency%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21271v1&entry.124074799=Read"},
{"title": "The Group Robustness is in the Details: Revisiting Finetuning under\n  Spurious Correlations", "author": "Tyler LaBonte and John C. Hill and Xinchen Zhang and Vidya Muthukumar and Abhishek Kumar", "abstract": "  Modern machine learning models are prone to over-reliance on spurious\ncorrelations, which can often lead to poor performance on minority groups. In\nthis paper, we identify surprising and nuanced behavior of finetuned models on\nworst-group accuracy via comprehensive experiments on four well-established\nbenchmarks across vision and language tasks. We first show that the commonly\nused class-balancing techniques of mini-batch upsampling and loss upweighting\ncan induce a decrease in worst-group accuracy (WGA) with training epochs,\nleading to performance no better than without class-balancing. While in some\nscenarios, removing data to create a class-balanced subset is more effective,\nwe show this depends on group structure and propose a mixture method which can\noutperform both techniques. Next, we show that scaling pretrained models is\ngenerally beneficial for worst-group accuracy, but only in conjunction with\nappropriate class-balancing. Finally, we identify spectral imbalance in\nfinetuning features as a potential source of group disparities -- minority\ngroup covariance matrices incur a larger spectral norm than majority groups\nonce conditioned on the classes. Our results show more nuanced interactions of\nmodern finetuned models with group robustness than was previously known. Our\ncode is available at https://github.com/tmlabonte/revisiting-finetuning.\n", "link": "http://arxiv.org/abs/2407.13957v2", "date": "2024-10-28", "relevancy": 1.9681, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4999}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4871}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Group%20Robustness%20is%20in%20the%20Details%3A%20Revisiting%20Finetuning%20under%0A%20%20Spurious%20Correlations&body=Title%3A%20The%20Group%20Robustness%20is%20in%20the%20Details%3A%20Revisiting%20Finetuning%20under%0A%20%20Spurious%20Correlations%0AAuthor%3A%20Tyler%20LaBonte%20and%20John%20C.%20Hill%20and%20Xinchen%20Zhang%20and%20Vidya%20Muthukumar%20and%20Abhishek%20Kumar%0AAbstract%3A%20%20%20Modern%20machine%20learning%20models%20are%20prone%20to%20over-reliance%20on%20spurious%0Acorrelations%2C%20which%20can%20often%20lead%20to%20poor%20performance%20on%20minority%20groups.%20In%0Athis%20paper%2C%20we%20identify%20surprising%20and%20nuanced%20behavior%20of%20finetuned%20models%20on%0Aworst-group%20accuracy%20via%20comprehensive%20experiments%20on%20four%20well-established%0Abenchmarks%20across%20vision%20and%20language%20tasks.%20We%20first%20show%20that%20the%20commonly%0Aused%20class-balancing%20techniques%20of%20mini-batch%20upsampling%20and%20loss%20upweighting%0Acan%20induce%20a%20decrease%20in%20worst-group%20accuracy%20%28WGA%29%20with%20training%20epochs%2C%0Aleading%20to%20performance%20no%20better%20than%20without%20class-balancing.%20While%20in%20some%0Ascenarios%2C%20removing%20data%20to%20create%20a%20class-balanced%20subset%20is%20more%20effective%2C%0Awe%20show%20this%20depends%20on%20group%20structure%20and%20propose%20a%20mixture%20method%20which%20can%0Aoutperform%20both%20techniques.%20Next%2C%20we%20show%20that%20scaling%20pretrained%20models%20is%0Agenerally%20beneficial%20for%20worst-group%20accuracy%2C%20but%20only%20in%20conjunction%20with%0Aappropriate%20class-balancing.%20Finally%2C%20we%20identify%20spectral%20imbalance%20in%0Afinetuning%20features%20as%20a%20potential%20source%20of%20group%20disparities%20--%20minority%0Agroup%20covariance%20matrices%20incur%20a%20larger%20spectral%20norm%20than%20majority%20groups%0Aonce%20conditioned%20on%20the%20classes.%20Our%20results%20show%20more%20nuanced%20interactions%20of%0Amodern%20finetuned%20models%20with%20group%20robustness%20than%20was%20previously%20known.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/tmlabonte/revisiting-finetuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13957v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Group%2520Robustness%2520is%2520in%2520the%2520Details%253A%2520Revisiting%2520Finetuning%2520under%250A%2520%2520Spurious%2520Correlations%26entry.906535625%3DTyler%2520LaBonte%2520and%2520John%2520C.%2520Hill%2520and%2520Xinchen%2520Zhang%2520and%2520Vidya%2520Muthukumar%2520and%2520Abhishek%2520Kumar%26entry.1292438233%3D%2520%2520Modern%2520machine%2520learning%2520models%2520are%2520prone%2520to%2520over-reliance%2520on%2520spurious%250Acorrelations%252C%2520which%2520can%2520often%2520lead%2520to%2520poor%2520performance%2520on%2520minority%2520groups.%2520In%250Athis%2520paper%252C%2520we%2520identify%2520surprising%2520and%2520nuanced%2520behavior%2520of%2520finetuned%2520models%2520on%250Aworst-group%2520accuracy%2520via%2520comprehensive%2520experiments%2520on%2520four%2520well-established%250Abenchmarks%2520across%2520vision%2520and%2520language%2520tasks.%2520We%2520first%2520show%2520that%2520the%2520commonly%250Aused%2520class-balancing%2520techniques%2520of%2520mini-batch%2520upsampling%2520and%2520loss%2520upweighting%250Acan%2520induce%2520a%2520decrease%2520in%2520worst-group%2520accuracy%2520%2528WGA%2529%2520with%2520training%2520epochs%252C%250Aleading%2520to%2520performance%2520no%2520better%2520than%2520without%2520class-balancing.%2520While%2520in%2520some%250Ascenarios%252C%2520removing%2520data%2520to%2520create%2520a%2520class-balanced%2520subset%2520is%2520more%2520effective%252C%250Awe%2520show%2520this%2520depends%2520on%2520group%2520structure%2520and%2520propose%2520a%2520mixture%2520method%2520which%2520can%250Aoutperform%2520both%2520techniques.%2520Next%252C%2520we%2520show%2520that%2520scaling%2520pretrained%2520models%2520is%250Agenerally%2520beneficial%2520for%2520worst-group%2520accuracy%252C%2520but%2520only%2520in%2520conjunction%2520with%250Aappropriate%2520class-balancing.%2520Finally%252C%2520we%2520identify%2520spectral%2520imbalance%2520in%250Afinetuning%2520features%2520as%2520a%2520potential%2520source%2520of%2520group%2520disparities%2520--%2520minority%250Agroup%2520covariance%2520matrices%2520incur%2520a%2520larger%2520spectral%2520norm%2520than%2520majority%2520groups%250Aonce%2520conditioned%2520on%2520the%2520classes.%2520Our%2520results%2520show%2520more%2520nuanced%2520interactions%2520of%250Amodern%2520finetuned%2520models%2520with%2520group%2520robustness%2520than%2520was%2520previously%2520known.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/tmlabonte/revisiting-finetuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13957v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Group%20Robustness%20is%20in%20the%20Details%3A%20Revisiting%20Finetuning%20under%0A%20%20Spurious%20Correlations&entry.906535625=Tyler%20LaBonte%20and%20John%20C.%20Hill%20and%20Xinchen%20Zhang%20and%20Vidya%20Muthukumar%20and%20Abhishek%20Kumar&entry.1292438233=%20%20Modern%20machine%20learning%20models%20are%20prone%20to%20over-reliance%20on%20spurious%0Acorrelations%2C%20which%20can%20often%20lead%20to%20poor%20performance%20on%20minority%20groups.%20In%0Athis%20paper%2C%20we%20identify%20surprising%20and%20nuanced%20behavior%20of%20finetuned%20models%20on%0Aworst-group%20accuracy%20via%20comprehensive%20experiments%20on%20four%20well-established%0Abenchmarks%20across%20vision%20and%20language%20tasks.%20We%20first%20show%20that%20the%20commonly%0Aused%20class-balancing%20techniques%20of%20mini-batch%20upsampling%20and%20loss%20upweighting%0Acan%20induce%20a%20decrease%20in%20worst-group%20accuracy%20%28WGA%29%20with%20training%20epochs%2C%0Aleading%20to%20performance%20no%20better%20than%20without%20class-balancing.%20While%20in%20some%0Ascenarios%2C%20removing%20data%20to%20create%20a%20class-balanced%20subset%20is%20more%20effective%2C%0Awe%20show%20this%20depends%20on%20group%20structure%20and%20propose%20a%20mixture%20method%20which%20can%0Aoutperform%20both%20techniques.%20Next%2C%20we%20show%20that%20scaling%20pretrained%20models%20is%0Agenerally%20beneficial%20for%20worst-group%20accuracy%2C%20but%20only%20in%20conjunction%20with%0Aappropriate%20class-balancing.%20Finally%2C%20we%20identify%20spectral%20imbalance%20in%0Afinetuning%20features%20as%20a%20potential%20source%20of%20group%20disparities%20--%20minority%0Agroup%20covariance%20matrices%20incur%20a%20larger%20spectral%20norm%20than%20majority%20groups%0Aonce%20conditioned%20on%20the%20classes.%20Our%20results%20show%20more%20nuanced%20interactions%20of%0Amodern%20finetuned%20models%20with%20group%20robustness%20than%20was%20previously%20known.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/tmlabonte/revisiting-finetuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13957v2&entry.124074799=Read"},
{"title": "Flaming-hot Initiation with Regular Execution Sampling for Large\n  Language Models", "author": "Weizhe Chen and Zhicheng Zhang and Guanlin Liu and Renjie Zheng and Wenlei Shi and Chen Dun and Zheng Wu and Xing Jin and Lin Yan", "abstract": "  Since the release of ChatGPT, large language models (LLMs) have demonstrated\nremarkable capabilities across various domains. A key challenge in developing\nthese general capabilities is efficiently sourcing diverse, high-quality data.\nThis becomes especially critical in reasoning-related tasks with sandbox\ncheckers, such as math or code, where the goal is to generate correct solutions\nto specific problems with higher probability. In this work, we introduce\nFlaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet\nhighly effective method to efficiently find good responses. Our empirical\nfindings show that FIRE sampling enhances inference-time generation quality and\nalso benefits training in the alignment stage. Furthermore, we explore how FIRE\nsampling improves performance by promoting diversity and analyze the impact of\nemploying FIRE at different positions within a response.\n", "link": "http://arxiv.org/abs/2410.21236v1", "date": "2024-10-28", "relevancy": 1.9421, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4935}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.486}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flaming-hot%20Initiation%20with%20Regular%20Execution%20Sampling%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20Flaming-hot%20Initiation%20with%20Regular%20Execution%20Sampling%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Weizhe%20Chen%20and%20Zhicheng%20Zhang%20and%20Guanlin%20Liu%20and%20Renjie%20Zheng%20and%20Wenlei%20Shi%20and%20Chen%20Dun%20and%20Zheng%20Wu%20and%20Xing%20Jin%20and%20Lin%20Yan%0AAbstract%3A%20%20%20Since%20the%20release%20of%20ChatGPT%2C%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Aremarkable%20capabilities%20across%20various%20domains.%20A%20key%20challenge%20in%20developing%0Athese%20general%20capabilities%20is%20efficiently%20sourcing%20diverse%2C%20high-quality%20data.%0AThis%20becomes%20especially%20critical%20in%20reasoning-related%20tasks%20with%20sandbox%0Acheckers%2C%20such%20as%20math%20or%20code%2C%20where%20the%20goal%20is%20to%20generate%20correct%20solutions%0Ato%20specific%20problems%20with%20higher%20probability.%20In%20this%20work%2C%20we%20introduce%0AFlaming-hot%20Initiation%20with%20Regular%20Execution%20%28FIRE%29%20sampling%2C%20a%20simple%20yet%0Ahighly%20effective%20method%20to%20efficiently%20find%20good%20responses.%20Our%20empirical%0Afindings%20show%20that%20FIRE%20sampling%20enhances%20inference-time%20generation%20quality%20and%0Aalso%20benefits%20training%20in%20the%20alignment%20stage.%20Furthermore%2C%20we%20explore%20how%20FIRE%0Asampling%20improves%20performance%20by%20promoting%20diversity%20and%20analyze%20the%20impact%20of%0Aemploying%20FIRE%20at%20different%20positions%20within%20a%20response.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlaming-hot%2520Initiation%2520with%2520Regular%2520Execution%2520Sampling%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DWeizhe%2520Chen%2520and%2520Zhicheng%2520Zhang%2520and%2520Guanlin%2520Liu%2520and%2520Renjie%2520Zheng%2520and%2520Wenlei%2520Shi%2520and%2520Chen%2520Dun%2520and%2520Zheng%2520Wu%2520and%2520Xing%2520Jin%2520and%2520Lin%2520Yan%26entry.1292438233%3D%2520%2520Since%2520the%2520release%2520of%2520ChatGPT%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%250Aremarkable%2520capabilities%2520across%2520various%2520domains.%2520A%2520key%2520challenge%2520in%2520developing%250Athese%2520general%2520capabilities%2520is%2520efficiently%2520sourcing%2520diverse%252C%2520high-quality%2520data.%250AThis%2520becomes%2520especially%2520critical%2520in%2520reasoning-related%2520tasks%2520with%2520sandbox%250Acheckers%252C%2520such%2520as%2520math%2520or%2520code%252C%2520where%2520the%2520goal%2520is%2520to%2520generate%2520correct%2520solutions%250Ato%2520specific%2520problems%2520with%2520higher%2520probability.%2520In%2520this%2520work%252C%2520we%2520introduce%250AFlaming-hot%2520Initiation%2520with%2520Regular%2520Execution%2520%2528FIRE%2529%2520sampling%252C%2520a%2520simple%2520yet%250Ahighly%2520effective%2520method%2520to%2520efficiently%2520find%2520good%2520responses.%2520Our%2520empirical%250Afindings%2520show%2520that%2520FIRE%2520sampling%2520enhances%2520inference-time%2520generation%2520quality%2520and%250Aalso%2520benefits%2520training%2520in%2520the%2520alignment%2520stage.%2520Furthermore%252C%2520we%2520explore%2520how%2520FIRE%250Asampling%2520improves%2520performance%2520by%2520promoting%2520diversity%2520and%2520analyze%2520the%2520impact%2520of%250Aemploying%2520FIRE%2520at%2520different%2520positions%2520within%2520a%2520response.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flaming-hot%20Initiation%20with%20Regular%20Execution%20Sampling%20for%20Large%0A%20%20Language%20Models&entry.906535625=Weizhe%20Chen%20and%20Zhicheng%20Zhang%20and%20Guanlin%20Liu%20and%20Renjie%20Zheng%20and%20Wenlei%20Shi%20and%20Chen%20Dun%20and%20Zheng%20Wu%20and%20Xing%20Jin%20and%20Lin%20Yan&entry.1292438233=%20%20Since%20the%20release%20of%20ChatGPT%2C%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Aremarkable%20capabilities%20across%20various%20domains.%20A%20key%20challenge%20in%20developing%0Athese%20general%20capabilities%20is%20efficiently%20sourcing%20diverse%2C%20high-quality%20data.%0AThis%20becomes%20especially%20critical%20in%20reasoning-related%20tasks%20with%20sandbox%0Acheckers%2C%20such%20as%20math%20or%20code%2C%20where%20the%20goal%20is%20to%20generate%20correct%20solutions%0Ato%20specific%20problems%20with%20higher%20probability.%20In%20this%20work%2C%20we%20introduce%0AFlaming-hot%20Initiation%20with%20Regular%20Execution%20%28FIRE%29%20sampling%2C%20a%20simple%20yet%0Ahighly%20effective%20method%20to%20efficiently%20find%20good%20responses.%20Our%20empirical%0Afindings%20show%20that%20FIRE%20sampling%20enhances%20inference-time%20generation%20quality%20and%0Aalso%20benefits%20training%20in%20the%20alignment%20stage.%20Furthermore%2C%20we%20explore%20how%20FIRE%0Asampling%20improves%20performance%20by%20promoting%20diversity%20and%20analyze%20the%20impact%20of%0Aemploying%20FIRE%20at%20different%20positions%20within%20a%20response.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21236v1&entry.124074799=Read"},
{"title": "LoRA vs Full Fine-tuning: An Illusion of Equivalence", "author": "Reece Shuttleworth and Jacob Andreas and Antonio Torralba and Pratyusha Sharma", "abstract": "  Fine-tuning is a crucial paradigm for adapting pre-trained large language\nmodels to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA)\nhave been shown to match the performance of fully fine-tuned models on various\ntasks with an extreme reduction in the number of trainable parameters. Even in\nsettings where both methods learn similarly accurate models, \\emph{are their\nlearned solutions really equivalent?} We study how different fine-tuning\nmethods change pre-trained models by analyzing the model's weight matrices\nthrough the lens of their spectral properties. We find that full fine-tuning\nand LoRA yield weight matrices whose singular value decompositions exhibit very\ndifferent structure; moreover, the fine-tuned models themselves show distinct\ngeneralization behaviors when tested outside the adaptation task's\ndistribution. More specifically, we first show that the weight matrices trained\nwith LoRA have new, high-ranking singular vectors, which we call \\emph{intruder\ndimensions}. Intruder dimensions do not appear during full fine-tuning. Second,\nwe show that LoRA models with intruder dimensions, despite achieving similar\nperformance to full fine-tuning on the target task, become worse models of the\npre-training distribution and adapt less robustly to multiple tasks\nsequentially. Higher-rank, rank-stabilized LoRA models closely mirror full\nfine-tuning, even when performing on par with lower-rank LoRA models on the\nsame tasks. These results suggest that models updated with LoRA and full\nfine-tuning access different parts of parameter space, even when they perform\nequally on the fine-tuned distribution. We conclude by examining why intruder\ndimensions appear in LoRA fine-tuned models, why they are undesirable, and how\ntheir effects can be minimized.\n", "link": "http://arxiv.org/abs/2410.21228v1", "date": "2024-10-28", "relevancy": 1.9135, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4839}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4773}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA%20vs%20Full%20Fine-tuning%3A%20An%20Illusion%20of%20Equivalence&body=Title%3A%20LoRA%20vs%20Full%20Fine-tuning%3A%20An%20Illusion%20of%20Equivalence%0AAuthor%3A%20Reece%20Shuttleworth%20and%20Jacob%20Andreas%20and%20Antonio%20Torralba%20and%20Pratyusha%20Sharma%0AAbstract%3A%20%20%20Fine-tuning%20is%20a%20crucial%20paradigm%20for%20adapting%20pre-trained%20large%20language%0Amodels%20to%20downstream%20tasks.%20Recently%2C%20methods%20like%20Low-Rank%20Adaptation%20%28LoRA%29%0Ahave%20been%20shown%20to%20match%20the%20performance%20of%20fully%20fine-tuned%20models%20on%20various%0Atasks%20with%20an%20extreme%20reduction%20in%20the%20number%20of%20trainable%20parameters.%20Even%20in%0Asettings%20where%20both%20methods%20learn%20similarly%20accurate%20models%2C%20%5Cemph%7Bare%20their%0Alearned%20solutions%20really%20equivalent%3F%7D%20We%20study%20how%20different%20fine-tuning%0Amethods%20change%20pre-trained%20models%20by%20analyzing%20the%20model%27s%20weight%20matrices%0Athrough%20the%20lens%20of%20their%20spectral%20properties.%20We%20find%20that%20full%20fine-tuning%0Aand%20LoRA%20yield%20weight%20matrices%20whose%20singular%20value%20decompositions%20exhibit%20very%0Adifferent%20structure%3B%20moreover%2C%20the%20fine-tuned%20models%20themselves%20show%20distinct%0Ageneralization%20behaviors%20when%20tested%20outside%20the%20adaptation%20task%27s%0Adistribution.%20More%20specifically%2C%20we%20first%20show%20that%20the%20weight%20matrices%20trained%0Awith%20LoRA%20have%20new%2C%20high-ranking%20singular%20vectors%2C%20which%20we%20call%20%5Cemph%7Bintruder%0Adimensions%7D.%20Intruder%20dimensions%20do%20not%20appear%20during%20full%20fine-tuning.%20Second%2C%0Awe%20show%20that%20LoRA%20models%20with%20intruder%20dimensions%2C%20despite%20achieving%20similar%0Aperformance%20to%20full%20fine-tuning%20on%20the%20target%20task%2C%20become%20worse%20models%20of%20the%0Apre-training%20distribution%20and%20adapt%20less%20robustly%20to%20multiple%20tasks%0Asequentially.%20Higher-rank%2C%20rank-stabilized%20LoRA%20models%20closely%20mirror%20full%0Afine-tuning%2C%20even%20when%20performing%20on%20par%20with%20lower-rank%20LoRA%20models%20on%20the%0Asame%20tasks.%20These%20results%20suggest%20that%20models%20updated%20with%20LoRA%20and%20full%0Afine-tuning%20access%20different%20parts%20of%20parameter%20space%2C%20even%20when%20they%20perform%0Aequally%20on%20the%20fine-tuned%20distribution.%20We%20conclude%20by%20examining%20why%20intruder%0Adimensions%20appear%20in%20LoRA%20fine-tuned%20models%2C%20why%20they%20are%20undesirable%2C%20and%20how%0Atheir%20effects%20can%20be%20minimized.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA%2520vs%2520Full%2520Fine-tuning%253A%2520An%2520Illusion%2520of%2520Equivalence%26entry.906535625%3DReece%2520Shuttleworth%2520and%2520Jacob%2520Andreas%2520and%2520Antonio%2520Torralba%2520and%2520Pratyusha%2520Sharma%26entry.1292438233%3D%2520%2520Fine-tuning%2520is%2520a%2520crucial%2520paradigm%2520for%2520adapting%2520pre-trained%2520large%2520language%250Amodels%2520to%2520downstream%2520tasks.%2520Recently%252C%2520methods%2520like%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%250Ahave%2520been%2520shown%2520to%2520match%2520the%2520performance%2520of%2520fully%2520fine-tuned%2520models%2520on%2520various%250Atasks%2520with%2520an%2520extreme%2520reduction%2520in%2520the%2520number%2520of%2520trainable%2520parameters.%2520Even%2520in%250Asettings%2520where%2520both%2520methods%2520learn%2520similarly%2520accurate%2520models%252C%2520%255Cemph%257Bare%2520their%250Alearned%2520solutions%2520really%2520equivalent%253F%257D%2520We%2520study%2520how%2520different%2520fine-tuning%250Amethods%2520change%2520pre-trained%2520models%2520by%2520analyzing%2520the%2520model%2527s%2520weight%2520matrices%250Athrough%2520the%2520lens%2520of%2520their%2520spectral%2520properties.%2520We%2520find%2520that%2520full%2520fine-tuning%250Aand%2520LoRA%2520yield%2520weight%2520matrices%2520whose%2520singular%2520value%2520decompositions%2520exhibit%2520very%250Adifferent%2520structure%253B%2520moreover%252C%2520the%2520fine-tuned%2520models%2520themselves%2520show%2520distinct%250Ageneralization%2520behaviors%2520when%2520tested%2520outside%2520the%2520adaptation%2520task%2527s%250Adistribution.%2520More%2520specifically%252C%2520we%2520first%2520show%2520that%2520the%2520weight%2520matrices%2520trained%250Awith%2520LoRA%2520have%2520new%252C%2520high-ranking%2520singular%2520vectors%252C%2520which%2520we%2520call%2520%255Cemph%257Bintruder%250Adimensions%257D.%2520Intruder%2520dimensions%2520do%2520not%2520appear%2520during%2520full%2520fine-tuning.%2520Second%252C%250Awe%2520show%2520that%2520LoRA%2520models%2520with%2520intruder%2520dimensions%252C%2520despite%2520achieving%2520similar%250Aperformance%2520to%2520full%2520fine-tuning%2520on%2520the%2520target%2520task%252C%2520become%2520worse%2520models%2520of%2520the%250Apre-training%2520distribution%2520and%2520adapt%2520less%2520robustly%2520to%2520multiple%2520tasks%250Asequentially.%2520Higher-rank%252C%2520rank-stabilized%2520LoRA%2520models%2520closely%2520mirror%2520full%250Afine-tuning%252C%2520even%2520when%2520performing%2520on%2520par%2520with%2520lower-rank%2520LoRA%2520models%2520on%2520the%250Asame%2520tasks.%2520These%2520results%2520suggest%2520that%2520models%2520updated%2520with%2520LoRA%2520and%2520full%250Afine-tuning%2520access%2520different%2520parts%2520of%2520parameter%2520space%252C%2520even%2520when%2520they%2520perform%250Aequally%2520on%2520the%2520fine-tuned%2520distribution.%2520We%2520conclude%2520by%2520examining%2520why%2520intruder%250Adimensions%2520appear%2520in%2520LoRA%2520fine-tuned%2520models%252C%2520why%2520they%2520are%2520undesirable%252C%2520and%2520how%250Atheir%2520effects%2520can%2520be%2520minimized.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA%20vs%20Full%20Fine-tuning%3A%20An%20Illusion%20of%20Equivalence&entry.906535625=Reece%20Shuttleworth%20and%20Jacob%20Andreas%20and%20Antonio%20Torralba%20and%20Pratyusha%20Sharma&entry.1292438233=%20%20Fine-tuning%20is%20a%20crucial%20paradigm%20for%20adapting%20pre-trained%20large%20language%0Amodels%20to%20downstream%20tasks.%20Recently%2C%20methods%20like%20Low-Rank%20Adaptation%20%28LoRA%29%0Ahave%20been%20shown%20to%20match%20the%20performance%20of%20fully%20fine-tuned%20models%20on%20various%0Atasks%20with%20an%20extreme%20reduction%20in%20the%20number%20of%20trainable%20parameters.%20Even%20in%0Asettings%20where%20both%20methods%20learn%20similarly%20accurate%20models%2C%20%5Cemph%7Bare%20their%0Alearned%20solutions%20really%20equivalent%3F%7D%20We%20study%20how%20different%20fine-tuning%0Amethods%20change%20pre-trained%20models%20by%20analyzing%20the%20model%27s%20weight%20matrices%0Athrough%20the%20lens%20of%20their%20spectral%20properties.%20We%20find%20that%20full%20fine-tuning%0Aand%20LoRA%20yield%20weight%20matrices%20whose%20singular%20value%20decompositions%20exhibit%20very%0Adifferent%20structure%3B%20moreover%2C%20the%20fine-tuned%20models%20themselves%20show%20distinct%0Ageneralization%20behaviors%20when%20tested%20outside%20the%20adaptation%20task%27s%0Adistribution.%20More%20specifically%2C%20we%20first%20show%20that%20the%20weight%20matrices%20trained%0Awith%20LoRA%20have%20new%2C%20high-ranking%20singular%20vectors%2C%20which%20we%20call%20%5Cemph%7Bintruder%0Adimensions%7D.%20Intruder%20dimensions%20do%20not%20appear%20during%20full%20fine-tuning.%20Second%2C%0Awe%20show%20that%20LoRA%20models%20with%20intruder%20dimensions%2C%20despite%20achieving%20similar%0Aperformance%20to%20full%20fine-tuning%20on%20the%20target%20task%2C%20become%20worse%20models%20of%20the%0Apre-training%20distribution%20and%20adapt%20less%20robustly%20to%20multiple%20tasks%0Asequentially.%20Higher-rank%2C%20rank-stabilized%20LoRA%20models%20closely%20mirror%20full%0Afine-tuning%2C%20even%20when%20performing%20on%20par%20with%20lower-rank%20LoRA%20models%20on%20the%0Asame%20tasks.%20These%20results%20suggest%20that%20models%20updated%20with%20LoRA%20and%20full%0Afine-tuning%20access%20different%20parts%20of%20parameter%20space%2C%20even%20when%20they%20perform%0Aequally%20on%20the%20fine-tuned%20distribution.%20We%20conclude%20by%20examining%20why%20intruder%0Adimensions%20appear%20in%20LoRA%20fine-tuned%20models%2C%20why%20they%20are%20undesirable%2C%20and%20how%0Atheir%20effects%20can%20be%20minimized.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21228v1&entry.124074799=Read"},
{"title": "Deep linear networks for regression are implicitly regularized towards\n  flat minima", "author": "Pierre Marion and L\u00e9na\u00efc Chizat", "abstract": "  The largest eigenvalue of the Hessian, or sharpness, of neural networks is a\nkey quantity to understand their optimization dynamics. In this paper, we study\nthe sharpness of deep linear networks for univariate regression. Minimizers can\nhave arbitrarily large sharpness, but not an arbitrarily small one. Indeed, we\nshow a lower bound on the sharpness of minimizers, which grows linearly with\ndepth. We then study the properties of the minimizer found by gradient flow,\nwhich is the limit of gradient descent with vanishing learning rate. We show an\nimplicit regularization towards flat minima: the sharpness of the minimizer is\nno more than a constant times the lower bound. The constant depends on the\ncondition number of the data covariance matrix, but not on width or depth. This\nresult is proven both for a small-scale initialization and a residual\ninitialization. Results of independent interest are shown in both cases. For\nsmall-scale initialization, we show that the learned weight matrices are\napproximately rank-one and that their singular vectors align. For residual\ninitialization, convergence of the gradient flow for a Gaussian initialization\nof the residual network is proven. Numerical experiments illustrate our results\nand connect them to gradient descent with non-vanishing learning rate.\n", "link": "http://arxiv.org/abs/2405.13456v2", "date": "2024-10-28", "relevancy": 1.8772, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5103}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4418}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20linear%20networks%20for%20regression%20are%20implicitly%20regularized%20towards%0A%20%20flat%20minima&body=Title%3A%20Deep%20linear%20networks%20for%20regression%20are%20implicitly%20regularized%20towards%0A%20%20flat%20minima%0AAuthor%3A%20Pierre%20Marion%20and%20L%C3%A9na%C3%AFc%20Chizat%0AAbstract%3A%20%20%20The%20largest%20eigenvalue%20of%20the%20Hessian%2C%20or%20sharpness%2C%20of%20neural%20networks%20is%20a%0Akey%20quantity%20to%20understand%20their%20optimization%20dynamics.%20In%20this%20paper%2C%20we%20study%0Athe%20sharpness%20of%20deep%20linear%20networks%20for%20univariate%20regression.%20Minimizers%20can%0Ahave%20arbitrarily%20large%20sharpness%2C%20but%20not%20an%20arbitrarily%20small%20one.%20Indeed%2C%20we%0Ashow%20a%20lower%20bound%20on%20the%20sharpness%20of%20minimizers%2C%20which%20grows%20linearly%20with%0Adepth.%20We%20then%20study%20the%20properties%20of%20the%20minimizer%20found%20by%20gradient%20flow%2C%0Awhich%20is%20the%20limit%20of%20gradient%20descent%20with%20vanishing%20learning%20rate.%20We%20show%20an%0Aimplicit%20regularization%20towards%20flat%20minima%3A%20the%20sharpness%20of%20the%20minimizer%20is%0Ano%20more%20than%20a%20constant%20times%20the%20lower%20bound.%20The%20constant%20depends%20on%20the%0Acondition%20number%20of%20the%20data%20covariance%20matrix%2C%20but%20not%20on%20width%20or%20depth.%20This%0Aresult%20is%20proven%20both%20for%20a%20small-scale%20initialization%20and%20a%20residual%0Ainitialization.%20Results%20of%20independent%20interest%20are%20shown%20in%20both%20cases.%20For%0Asmall-scale%20initialization%2C%20we%20show%20that%20the%20learned%20weight%20matrices%20are%0Aapproximately%20rank-one%20and%20that%20their%20singular%20vectors%20align.%20For%20residual%0Ainitialization%2C%20convergence%20of%20the%20gradient%20flow%20for%20a%20Gaussian%20initialization%0Aof%20the%20residual%20network%20is%20proven.%20Numerical%20experiments%20illustrate%20our%20results%0Aand%20connect%20them%20to%20gradient%20descent%20with%20non-vanishing%20learning%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13456v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520linear%2520networks%2520for%2520regression%2520are%2520implicitly%2520regularized%2520towards%250A%2520%2520flat%2520minima%26entry.906535625%3DPierre%2520Marion%2520and%2520L%25C3%25A9na%25C3%25AFc%2520Chizat%26entry.1292438233%3D%2520%2520The%2520largest%2520eigenvalue%2520of%2520the%2520Hessian%252C%2520or%2520sharpness%252C%2520of%2520neural%2520networks%2520is%2520a%250Akey%2520quantity%2520to%2520understand%2520their%2520optimization%2520dynamics.%2520In%2520this%2520paper%252C%2520we%2520study%250Athe%2520sharpness%2520of%2520deep%2520linear%2520networks%2520for%2520univariate%2520regression.%2520Minimizers%2520can%250Ahave%2520arbitrarily%2520large%2520sharpness%252C%2520but%2520not%2520an%2520arbitrarily%2520small%2520one.%2520Indeed%252C%2520we%250Ashow%2520a%2520lower%2520bound%2520on%2520the%2520sharpness%2520of%2520minimizers%252C%2520which%2520grows%2520linearly%2520with%250Adepth.%2520We%2520then%2520study%2520the%2520properties%2520of%2520the%2520minimizer%2520found%2520by%2520gradient%2520flow%252C%250Awhich%2520is%2520the%2520limit%2520of%2520gradient%2520descent%2520with%2520vanishing%2520learning%2520rate.%2520We%2520show%2520an%250Aimplicit%2520regularization%2520towards%2520flat%2520minima%253A%2520the%2520sharpness%2520of%2520the%2520minimizer%2520is%250Ano%2520more%2520than%2520a%2520constant%2520times%2520the%2520lower%2520bound.%2520The%2520constant%2520depends%2520on%2520the%250Acondition%2520number%2520of%2520the%2520data%2520covariance%2520matrix%252C%2520but%2520not%2520on%2520width%2520or%2520depth.%2520This%250Aresult%2520is%2520proven%2520both%2520for%2520a%2520small-scale%2520initialization%2520and%2520a%2520residual%250Ainitialization.%2520Results%2520of%2520independent%2520interest%2520are%2520shown%2520in%2520both%2520cases.%2520For%250Asmall-scale%2520initialization%252C%2520we%2520show%2520that%2520the%2520learned%2520weight%2520matrices%2520are%250Aapproximately%2520rank-one%2520and%2520that%2520their%2520singular%2520vectors%2520align.%2520For%2520residual%250Ainitialization%252C%2520convergence%2520of%2520the%2520gradient%2520flow%2520for%2520a%2520Gaussian%2520initialization%250Aof%2520the%2520residual%2520network%2520is%2520proven.%2520Numerical%2520experiments%2520illustrate%2520our%2520results%250Aand%2520connect%2520them%2520to%2520gradient%2520descent%2520with%2520non-vanishing%2520learning%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13456v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20linear%20networks%20for%20regression%20are%20implicitly%20regularized%20towards%0A%20%20flat%20minima&entry.906535625=Pierre%20Marion%20and%20L%C3%A9na%C3%AFc%20Chizat&entry.1292438233=%20%20The%20largest%20eigenvalue%20of%20the%20Hessian%2C%20or%20sharpness%2C%20of%20neural%20networks%20is%20a%0Akey%20quantity%20to%20understand%20their%20optimization%20dynamics.%20In%20this%20paper%2C%20we%20study%0Athe%20sharpness%20of%20deep%20linear%20networks%20for%20univariate%20regression.%20Minimizers%20can%0Ahave%20arbitrarily%20large%20sharpness%2C%20but%20not%20an%20arbitrarily%20small%20one.%20Indeed%2C%20we%0Ashow%20a%20lower%20bound%20on%20the%20sharpness%20of%20minimizers%2C%20which%20grows%20linearly%20with%0Adepth.%20We%20then%20study%20the%20properties%20of%20the%20minimizer%20found%20by%20gradient%20flow%2C%0Awhich%20is%20the%20limit%20of%20gradient%20descent%20with%20vanishing%20learning%20rate.%20We%20show%20an%0Aimplicit%20regularization%20towards%20flat%20minima%3A%20the%20sharpness%20of%20the%20minimizer%20is%0Ano%20more%20than%20a%20constant%20times%20the%20lower%20bound.%20The%20constant%20depends%20on%20the%0Acondition%20number%20of%20the%20data%20covariance%20matrix%2C%20but%20not%20on%20width%20or%20depth.%20This%0Aresult%20is%20proven%20both%20for%20a%20small-scale%20initialization%20and%20a%20residual%0Ainitialization.%20Results%20of%20independent%20interest%20are%20shown%20in%20both%20cases.%20For%0Asmall-scale%20initialization%2C%20we%20show%20that%20the%20learned%20weight%20matrices%20are%0Aapproximately%20rank-one%20and%20that%20their%20singular%20vectors%20align.%20For%20residual%0Ainitialization%2C%20convergence%20of%20the%20gradient%20flow%20for%20a%20Gaussian%20initialization%0Aof%20the%20residual%20network%20is%20proven.%20Numerical%20experiments%20illustrate%20our%20results%0Aand%20connect%20them%20to%20gradient%20descent%20with%20non-vanishing%20learning%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13456v2&entry.124074799=Read"},
{"title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses", "author": "Zonglin Yang and Wanhao Liu and Ben Gao and Tong Xie and Yuqiang Li and Wanli Ouyang and Soujanya Poria and Erik Cambria and Dongzhan Zhou", "abstract": "  Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations.\n", "link": "http://arxiv.org/abs/2410.07076v3", "date": "2024-10-28", "relevancy": 1.8622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4665}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOOSE-Chem%3A%20Large%20Language%20Models%20for%20Rediscovering%20Unseen%20Chemistry%0A%20%20Scientific%20Hypotheses&body=Title%3A%20MOOSE-Chem%3A%20Large%20Language%20Models%20for%20Rediscovering%20Unseen%20Chemistry%0A%20%20Scientific%20Hypotheses%0AAuthor%3A%20Zonglin%20Yang%20and%20Wanhao%20Liu%20and%20Ben%20Gao%20and%20Tong%20Xie%20and%20Yuqiang%20Li%20and%20Wanli%20Ouyang%20and%20Soujanya%20Poria%20and%20Erik%20Cambria%20and%20Dongzhan%20Zhou%0AAbstract%3A%20%20%20Scientific%20discovery%20contributes%20largely%20to%20human%20society%27s%20prosperity%2C%20and%0Arecent%20progress%20shows%20that%20LLMs%20could%20potentially%20catalyze%20this%20process.%0AHowever%2C%20it%20is%20still%20unclear%20whether%20LLMs%20can%20discover%20novel%20and%20valid%0Ahypotheses%20in%20chemistry.%20In%20this%20work%2C%20we%20investigate%20this%20central%20research%0Aquestion%3A%20Can%20LLMs%20automatically%20discover%20novel%20and%20valid%20chemistry%20research%0Ahypotheses%20given%20only%20a%20chemistry%20research%20background%20%28consisting%20of%20a%20research%0Aquestion%20and/or%20a%20background%20survey%29%2C%20without%20limitation%20on%20the%20domain%20of%20the%0Aresearch%20question%3F%20After%20extensive%20discussions%20with%20chemistry%20experts%2C%20we%0Apropose%20an%20assumption%20that%20a%20majority%20of%20chemistry%20hypotheses%20can%20be%20resulted%0Afrom%20a%20research%20background%20and%20several%20inspirations.%20With%20this%20key%20insight%2C%20we%0Abreak%20the%20central%20question%20into%20three%20smaller%20fundamental%20questions.%20In%20brief%2C%0Athey%20are%3A%20%281%29%20given%20a%20background%20question%2C%20whether%20LLMs%20can%20retrieve%20good%0Ainspirations%3B%20%282%29%20with%20background%20and%20inspirations%2C%20whether%20LLMs%20can%20lead%20to%0Ahypothesis%3B%20and%20%283%29%20whether%20LLMs%20can%20identify%20good%20hypotheses%20to%20rank%20them%0Ahigher.%20To%20investigate%20these%20questions%2C%20we%20construct%20a%20benchmark%20consisting%20of%0A51%20chemistry%20papers%20published%20in%20Nature%2C%20Science%2C%20or%20a%20similar%20level%20in%202024%0A%28all%20papers%20are%20only%20available%20online%20since%202024%29.%20Every%20paper%20is%20divided%20by%0Achemistry%20PhD%20students%20into%20three%20components%3A%20background%2C%20inspirations%2C%20and%0Ahypothesis.%20The%20goal%20is%20to%20rediscover%20the%20hypothesis%2C%20given%20only%20the%20background%0Aand%20a%20large%20randomly%20selected%20chemistry%20literature%20corpus%20consisting%20the%20ground%0Atruth%20inspiration%20papers%2C%20with%20LLMs%20trained%20with%20data%20up%20to%202023.%20We%20also%0Adevelop%20an%20LLM-based%20multi-agent%20framework%20that%20leverages%20the%20assumption%2C%0Aconsisting%20of%20three%20stages%20reflecting%20the%20three%20smaller%20questions.%20The%20proposed%0Amethod%20can%20rediscover%20many%20hypotheses%20with%20very%20high%20similarity%20with%20the%20ground%0Atruth%20ones%2C%20covering%20the%20main%20innovations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07076v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOOSE-Chem%253A%2520Large%2520Language%2520Models%2520for%2520Rediscovering%2520Unseen%2520Chemistry%250A%2520%2520Scientific%2520Hypotheses%26entry.906535625%3DZonglin%2520Yang%2520and%2520Wanhao%2520Liu%2520and%2520Ben%2520Gao%2520and%2520Tong%2520Xie%2520and%2520Yuqiang%2520Li%2520and%2520Wanli%2520Ouyang%2520and%2520Soujanya%2520Poria%2520and%2520Erik%2520Cambria%2520and%2520Dongzhan%2520Zhou%26entry.1292438233%3D%2520%2520Scientific%2520discovery%2520contributes%2520largely%2520to%2520human%2520society%2527s%2520prosperity%252C%2520and%250Arecent%2520progress%2520shows%2520that%2520LLMs%2520could%2520potentially%2520catalyze%2520this%2520process.%250AHowever%252C%2520it%2520is%2520still%2520unclear%2520whether%2520LLMs%2520can%2520discover%2520novel%2520and%2520valid%250Ahypotheses%2520in%2520chemistry.%2520In%2520this%2520work%252C%2520we%2520investigate%2520this%2520central%2520research%250Aquestion%253A%2520Can%2520LLMs%2520automatically%2520discover%2520novel%2520and%2520valid%2520chemistry%2520research%250Ahypotheses%2520given%2520only%2520a%2520chemistry%2520research%2520background%2520%2528consisting%2520of%2520a%2520research%250Aquestion%2520and/or%2520a%2520background%2520survey%2529%252C%2520without%2520limitation%2520on%2520the%2520domain%2520of%2520the%250Aresearch%2520question%253F%2520After%2520extensive%2520discussions%2520with%2520chemistry%2520experts%252C%2520we%250Apropose%2520an%2520assumption%2520that%2520a%2520majority%2520of%2520chemistry%2520hypotheses%2520can%2520be%2520resulted%250Afrom%2520a%2520research%2520background%2520and%2520several%2520inspirations.%2520With%2520this%2520key%2520insight%252C%2520we%250Abreak%2520the%2520central%2520question%2520into%2520three%2520smaller%2520fundamental%2520questions.%2520In%2520brief%252C%250Athey%2520are%253A%2520%25281%2529%2520given%2520a%2520background%2520question%252C%2520whether%2520LLMs%2520can%2520retrieve%2520good%250Ainspirations%253B%2520%25282%2529%2520with%2520background%2520and%2520inspirations%252C%2520whether%2520LLMs%2520can%2520lead%2520to%250Ahypothesis%253B%2520and%2520%25283%2529%2520whether%2520LLMs%2520can%2520identify%2520good%2520hypotheses%2520to%2520rank%2520them%250Ahigher.%2520To%2520investigate%2520these%2520questions%252C%2520we%2520construct%2520a%2520benchmark%2520consisting%2520of%250A51%2520chemistry%2520papers%2520published%2520in%2520Nature%252C%2520Science%252C%2520or%2520a%2520similar%2520level%2520in%25202024%250A%2528all%2520papers%2520are%2520only%2520available%2520online%2520since%25202024%2529.%2520Every%2520paper%2520is%2520divided%2520by%250Achemistry%2520PhD%2520students%2520into%2520three%2520components%253A%2520background%252C%2520inspirations%252C%2520and%250Ahypothesis.%2520The%2520goal%2520is%2520to%2520rediscover%2520the%2520hypothesis%252C%2520given%2520only%2520the%2520background%250Aand%2520a%2520large%2520randomly%2520selected%2520chemistry%2520literature%2520corpus%2520consisting%2520the%2520ground%250Atruth%2520inspiration%2520papers%252C%2520with%2520LLMs%2520trained%2520with%2520data%2520up%2520to%25202023.%2520We%2520also%250Adevelop%2520an%2520LLM-based%2520multi-agent%2520framework%2520that%2520leverages%2520the%2520assumption%252C%250Aconsisting%2520of%2520three%2520stages%2520reflecting%2520the%2520three%2520smaller%2520questions.%2520The%2520proposed%250Amethod%2520can%2520rediscover%2520many%2520hypotheses%2520with%2520very%2520high%2520similarity%2520with%2520the%2520ground%250Atruth%2520ones%252C%2520covering%2520the%2520main%2520innovations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07076v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOOSE-Chem%3A%20Large%20Language%20Models%20for%20Rediscovering%20Unseen%20Chemistry%0A%20%20Scientific%20Hypotheses&entry.906535625=Zonglin%20Yang%20and%20Wanhao%20Liu%20and%20Ben%20Gao%20and%20Tong%20Xie%20and%20Yuqiang%20Li%20and%20Wanli%20Ouyang%20and%20Soujanya%20Poria%20and%20Erik%20Cambria%20and%20Dongzhan%20Zhou&entry.1292438233=%20%20Scientific%20discovery%20contributes%20largely%20to%20human%20society%27s%20prosperity%2C%20and%0Arecent%20progress%20shows%20that%20LLMs%20could%20potentially%20catalyze%20this%20process.%0AHowever%2C%20it%20is%20still%20unclear%20whether%20LLMs%20can%20discover%20novel%20and%20valid%0Ahypotheses%20in%20chemistry.%20In%20this%20work%2C%20we%20investigate%20this%20central%20research%0Aquestion%3A%20Can%20LLMs%20automatically%20discover%20novel%20and%20valid%20chemistry%20research%0Ahypotheses%20given%20only%20a%20chemistry%20research%20background%20%28consisting%20of%20a%20research%0Aquestion%20and/or%20a%20background%20survey%29%2C%20without%20limitation%20on%20the%20domain%20of%20the%0Aresearch%20question%3F%20After%20extensive%20discussions%20with%20chemistry%20experts%2C%20we%0Apropose%20an%20assumption%20that%20a%20majority%20of%20chemistry%20hypotheses%20can%20be%20resulted%0Afrom%20a%20research%20background%20and%20several%20inspirations.%20With%20this%20key%20insight%2C%20we%0Abreak%20the%20central%20question%20into%20three%20smaller%20fundamental%20questions.%20In%20brief%2C%0Athey%20are%3A%20%281%29%20given%20a%20background%20question%2C%20whether%20LLMs%20can%20retrieve%20good%0Ainspirations%3B%20%282%29%20with%20background%20and%20inspirations%2C%20whether%20LLMs%20can%20lead%20to%0Ahypothesis%3B%20and%20%283%29%20whether%20LLMs%20can%20identify%20good%20hypotheses%20to%20rank%20them%0Ahigher.%20To%20investigate%20these%20questions%2C%20we%20construct%20a%20benchmark%20consisting%20of%0A51%20chemistry%20papers%20published%20in%20Nature%2C%20Science%2C%20or%20a%20similar%20level%20in%202024%0A%28all%20papers%20are%20only%20available%20online%20since%202024%29.%20Every%20paper%20is%20divided%20by%0Achemistry%20PhD%20students%20into%20three%20components%3A%20background%2C%20inspirations%2C%20and%0Ahypothesis.%20The%20goal%20is%20to%20rediscover%20the%20hypothesis%2C%20given%20only%20the%20background%0Aand%20a%20large%20randomly%20selected%20chemistry%20literature%20corpus%20consisting%20the%20ground%0Atruth%20inspiration%20papers%2C%20with%20LLMs%20trained%20with%20data%20up%20to%202023.%20We%20also%0Adevelop%20an%20LLM-based%20multi-agent%20framework%20that%20leverages%20the%20assumption%2C%0Aconsisting%20of%20three%20stages%20reflecting%20the%20three%20smaller%20questions.%20The%20proposed%0Amethod%20can%20rediscover%20many%20hypotheses%20with%20very%20high%20similarity%20with%20the%20ground%0Atruth%20ones%2C%20covering%20the%20main%20innovations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07076v3&entry.124074799=Read"},
{"title": "Multi-modal AI for comprehensive breast cancer prognostication", "author": "Jan Witowski and Ken Zeng and Joseph Cappadona and Jailan Elayoubi and Elena Diana Chiru and Nancy Chan and Young-Joon Kang and Frederick Howard and Irina Ostrovnaya and Carlos Fernandez-Granda and Freya Schnabel and Ugur Ozerdem and Kangning Liu and Zoe Steinsnyder and Nitya Thakore and Mohammad Sadic and Frank Yeung and Elisa Liu and Theodore Hill and Benjamin Swett and Danielle Rigau and Andrew Clayburn and Valerie Speirs and Marcus Vetter and Lina Sojak and Simone Muenst Soysal and Daniel Baumhoer and Khalil Choucair and Yu Zong and Lina Daoud and Anas Saad and Waleed Abdulsattar and Rafic Beydoun and Jia-Wern Pan and Haslina Makmur and Soo-Hwang Teo and Linda Ma Pak and Victor Angel and Dovile Zilenaite-Petrulaitiene and Arvydas Laurinavicius and Natalie Klar and Brian D. Piening and Carlo Bifulco and Sun-Young Jun and Jae Pak Yi and Su Hyun Lim and Adam Brufsky and Francisco J. Esteva and Lajos Pusztai and Yann LeCun and Krzysztof J. Geras", "abstract": "  Treatment selection in breast cancer is guided by molecular subtypes and\nclinical characteristics. Recurrence risk assessment plays a crucial role in\npersonalizing treatment. Current methods, including genomic assays, have\nlimited accuracy and clinical utility, leading to suboptimal decisions for many\npatients. We developed a test for breast cancer patient stratification based on\ndigital pathology and clinical characteristics using novel AI methods.\nSpecifically, we utilized a vision transformer-based pan-cancer foundation\nmodel trained with self-supervised learning to extract features from digitized\nH&E-stained slides. These features were integrated with clinical data to form a\nmulti-modal AI test predicting cancer recurrence and death. The test was\ndeveloped and evaluated using data from a total of 8,161 breast cancer patients\nacross 15 cohorts originating from seven countries. Of these, 3,502 patients\nfrom five cohorts were used exclusively for evaluation, while the remaining\npatients were used for training. Our test accurately predicted our primary\nendpoint, disease-free interval, in the five external cohorts (C-index: 0.71\n[0.68-0.75], HR: 3.63 [3.02-4.37, p<0.01]). In a direct comparison (N=858), the\nAI test was more accurate than Oncotype DX, the standard-of-care 21-gene assay,\nwith a C-index of 0.67 [0.61-0.74] versus 0.61 [0.49-0.73], respectively.\nAdditionally, the AI test added independent information to Oncotype DX in a\nmultivariate analysis (HR: 3.11 [1.91-5.09, p<0.01)]). The test demonstrated\nrobust accuracy across all major breast cancer subtypes, including TNBC\n(C-index: 0.71 [0.62-0.81], HR: 3.81 [2.35-6.17, p=0.02]), where no diagnostic\ntools are currently recommended by clinical guidelines. These results suggest\nthat our AI test can improve accuracy, extend applicability to a wider range of\npatients, and enhance access to treatment selection tools.\n", "link": "http://arxiv.org/abs/2410.21256v1", "date": "2024-10-28", "relevancy": 1.7924, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4849}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4451}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20AI%20for%20comprehensive%20breast%20cancer%20prognostication&body=Title%3A%20Multi-modal%20AI%20for%20comprehensive%20breast%20cancer%20prognostication%0AAuthor%3A%20Jan%20Witowski%20and%20Ken%20Zeng%20and%20Joseph%20Cappadona%20and%20Jailan%20Elayoubi%20and%20Elena%20Diana%20Chiru%20and%20Nancy%20Chan%20and%20Young-Joon%20Kang%20and%20Frederick%20Howard%20and%20Irina%20Ostrovnaya%20and%20Carlos%20Fernandez-Granda%20and%20Freya%20Schnabel%20and%20Ugur%20Ozerdem%20and%20Kangning%20Liu%20and%20Zoe%20Steinsnyder%20and%20Nitya%20Thakore%20and%20Mohammad%20Sadic%20and%20Frank%20Yeung%20and%20Elisa%20Liu%20and%20Theodore%20Hill%20and%20Benjamin%20Swett%20and%20Danielle%20Rigau%20and%20Andrew%20Clayburn%20and%20Valerie%20Speirs%20and%20Marcus%20Vetter%20and%20Lina%20Sojak%20and%20Simone%20Muenst%20Soysal%20and%20Daniel%20Baumhoer%20and%20Khalil%20Choucair%20and%20Yu%20Zong%20and%20Lina%20Daoud%20and%20Anas%20Saad%20and%20Waleed%20Abdulsattar%20and%20Rafic%20Beydoun%20and%20Jia-Wern%20Pan%20and%20Haslina%20Makmur%20and%20Soo-Hwang%20Teo%20and%20Linda%20Ma%20Pak%20and%20Victor%20Angel%20and%20Dovile%20Zilenaite-Petrulaitiene%20and%20Arvydas%20Laurinavicius%20and%20Natalie%20Klar%20and%20Brian%20D.%20Piening%20and%20Carlo%20Bifulco%20and%20Sun-Young%20Jun%20and%20Jae%20Pak%20Yi%20and%20Su%20Hyun%20Lim%20and%20Adam%20Brufsky%20and%20Francisco%20J.%20Esteva%20and%20Lajos%20Pusztai%20and%20Yann%20LeCun%20and%20Krzysztof%20J.%20Geras%0AAbstract%3A%20%20%20Treatment%20selection%20in%20breast%20cancer%20is%20guided%20by%20molecular%20subtypes%20and%0Aclinical%20characteristics.%20Recurrence%20risk%20assessment%20plays%20a%20crucial%20role%20in%0Apersonalizing%20treatment.%20Current%20methods%2C%20including%20genomic%20assays%2C%20have%0Alimited%20accuracy%20and%20clinical%20utility%2C%20leading%20to%20suboptimal%20decisions%20for%20many%0Apatients.%20We%20developed%20a%20test%20for%20breast%20cancer%20patient%20stratification%20based%20on%0Adigital%20pathology%20and%20clinical%20characteristics%20using%20novel%20AI%20methods.%0ASpecifically%2C%20we%20utilized%20a%20vision%20transformer-based%20pan-cancer%20foundation%0Amodel%20trained%20with%20self-supervised%20learning%20to%20extract%20features%20from%20digitized%0AH%26E-stained%20slides.%20These%20features%20were%20integrated%20with%20clinical%20data%20to%20form%20a%0Amulti-modal%20AI%20test%20predicting%20cancer%20recurrence%20and%20death.%20The%20test%20was%0Adeveloped%20and%20evaluated%20using%20data%20from%20a%20total%20of%208%2C161%20breast%20cancer%20patients%0Aacross%2015%20cohorts%20originating%20from%20seven%20countries.%20Of%20these%2C%203%2C502%20patients%0Afrom%20five%20cohorts%20were%20used%20exclusively%20for%20evaluation%2C%20while%20the%20remaining%0Apatients%20were%20used%20for%20training.%20Our%20test%20accurately%20predicted%20our%20primary%0Aendpoint%2C%20disease-free%20interval%2C%20in%20the%20five%20external%20cohorts%20%28C-index%3A%200.71%0A%5B0.68-0.75%5D%2C%20HR%3A%203.63%20%5B3.02-4.37%2C%20p%3C0.01%5D%29.%20In%20a%20direct%20comparison%20%28N%3D858%29%2C%20the%0AAI%20test%20was%20more%20accurate%20than%20Oncotype%20DX%2C%20the%20standard-of-care%2021-gene%20assay%2C%0Awith%20a%20C-index%20of%200.67%20%5B0.61-0.74%5D%20versus%200.61%20%5B0.49-0.73%5D%2C%20respectively.%0AAdditionally%2C%20the%20AI%20test%20added%20independent%20information%20to%20Oncotype%20DX%20in%20a%0Amultivariate%20analysis%20%28HR%3A%203.11%20%5B1.91-5.09%2C%20p%3C0.01%29%5D%29.%20The%20test%20demonstrated%0Arobust%20accuracy%20across%20all%20major%20breast%20cancer%20subtypes%2C%20including%20TNBC%0A%28C-index%3A%200.71%20%5B0.62-0.81%5D%2C%20HR%3A%203.81%20%5B2.35-6.17%2C%20p%3D0.02%5D%29%2C%20where%20no%20diagnostic%0Atools%20are%20currently%20recommended%20by%20clinical%20guidelines.%20These%20results%20suggest%0Athat%20our%20AI%20test%20can%20improve%20accuracy%2C%20extend%20applicability%20to%20a%20wider%20range%20of%0Apatients%2C%20and%20enhance%20access%20to%20treatment%20selection%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520AI%2520for%2520comprehensive%2520breast%2520cancer%2520prognostication%26entry.906535625%3DJan%2520Witowski%2520and%2520Ken%2520Zeng%2520and%2520Joseph%2520Cappadona%2520and%2520Jailan%2520Elayoubi%2520and%2520Elena%2520Diana%2520Chiru%2520and%2520Nancy%2520Chan%2520and%2520Young-Joon%2520Kang%2520and%2520Frederick%2520Howard%2520and%2520Irina%2520Ostrovnaya%2520and%2520Carlos%2520Fernandez-Granda%2520and%2520Freya%2520Schnabel%2520and%2520Ugur%2520Ozerdem%2520and%2520Kangning%2520Liu%2520and%2520Zoe%2520Steinsnyder%2520and%2520Nitya%2520Thakore%2520and%2520Mohammad%2520Sadic%2520and%2520Frank%2520Yeung%2520and%2520Elisa%2520Liu%2520and%2520Theodore%2520Hill%2520and%2520Benjamin%2520Swett%2520and%2520Danielle%2520Rigau%2520and%2520Andrew%2520Clayburn%2520and%2520Valerie%2520Speirs%2520and%2520Marcus%2520Vetter%2520and%2520Lina%2520Sojak%2520and%2520Simone%2520Muenst%2520Soysal%2520and%2520Daniel%2520Baumhoer%2520and%2520Khalil%2520Choucair%2520and%2520Yu%2520Zong%2520and%2520Lina%2520Daoud%2520and%2520Anas%2520Saad%2520and%2520Waleed%2520Abdulsattar%2520and%2520Rafic%2520Beydoun%2520and%2520Jia-Wern%2520Pan%2520and%2520Haslina%2520Makmur%2520and%2520Soo-Hwang%2520Teo%2520and%2520Linda%2520Ma%2520Pak%2520and%2520Victor%2520Angel%2520and%2520Dovile%2520Zilenaite-Petrulaitiene%2520and%2520Arvydas%2520Laurinavicius%2520and%2520Natalie%2520Klar%2520and%2520Brian%2520D.%2520Piening%2520and%2520Carlo%2520Bifulco%2520and%2520Sun-Young%2520Jun%2520and%2520Jae%2520Pak%2520Yi%2520and%2520Su%2520Hyun%2520Lim%2520and%2520Adam%2520Brufsky%2520and%2520Francisco%2520J.%2520Esteva%2520and%2520Lajos%2520Pusztai%2520and%2520Yann%2520LeCun%2520and%2520Krzysztof%2520J.%2520Geras%26entry.1292438233%3D%2520%2520Treatment%2520selection%2520in%2520breast%2520cancer%2520is%2520guided%2520by%2520molecular%2520subtypes%2520and%250Aclinical%2520characteristics.%2520Recurrence%2520risk%2520assessment%2520plays%2520a%2520crucial%2520role%2520in%250Apersonalizing%2520treatment.%2520Current%2520methods%252C%2520including%2520genomic%2520assays%252C%2520have%250Alimited%2520accuracy%2520and%2520clinical%2520utility%252C%2520leading%2520to%2520suboptimal%2520decisions%2520for%2520many%250Apatients.%2520We%2520developed%2520a%2520test%2520for%2520breast%2520cancer%2520patient%2520stratification%2520based%2520on%250Adigital%2520pathology%2520and%2520clinical%2520characteristics%2520using%2520novel%2520AI%2520methods.%250ASpecifically%252C%2520we%2520utilized%2520a%2520vision%2520transformer-based%2520pan-cancer%2520foundation%250Amodel%2520trained%2520with%2520self-supervised%2520learning%2520to%2520extract%2520features%2520from%2520digitized%250AH%2526E-stained%2520slides.%2520These%2520features%2520were%2520integrated%2520with%2520clinical%2520data%2520to%2520form%2520a%250Amulti-modal%2520AI%2520test%2520predicting%2520cancer%2520recurrence%2520and%2520death.%2520The%2520test%2520was%250Adeveloped%2520and%2520evaluated%2520using%2520data%2520from%2520a%2520total%2520of%25208%252C161%2520breast%2520cancer%2520patients%250Aacross%252015%2520cohorts%2520originating%2520from%2520seven%2520countries.%2520Of%2520these%252C%25203%252C502%2520patients%250Afrom%2520five%2520cohorts%2520were%2520used%2520exclusively%2520for%2520evaluation%252C%2520while%2520the%2520remaining%250Apatients%2520were%2520used%2520for%2520training.%2520Our%2520test%2520accurately%2520predicted%2520our%2520primary%250Aendpoint%252C%2520disease-free%2520interval%252C%2520in%2520the%2520five%2520external%2520cohorts%2520%2528C-index%253A%25200.71%250A%255B0.68-0.75%255D%252C%2520HR%253A%25203.63%2520%255B3.02-4.37%252C%2520p%253C0.01%255D%2529.%2520In%2520a%2520direct%2520comparison%2520%2528N%253D858%2529%252C%2520the%250AAI%2520test%2520was%2520more%2520accurate%2520than%2520Oncotype%2520DX%252C%2520the%2520standard-of-care%252021-gene%2520assay%252C%250Awith%2520a%2520C-index%2520of%25200.67%2520%255B0.61-0.74%255D%2520versus%25200.61%2520%255B0.49-0.73%255D%252C%2520respectively.%250AAdditionally%252C%2520the%2520AI%2520test%2520added%2520independent%2520information%2520to%2520Oncotype%2520DX%2520in%2520a%250Amultivariate%2520analysis%2520%2528HR%253A%25203.11%2520%255B1.91-5.09%252C%2520p%253C0.01%2529%255D%2529.%2520The%2520test%2520demonstrated%250Arobust%2520accuracy%2520across%2520all%2520major%2520breast%2520cancer%2520subtypes%252C%2520including%2520TNBC%250A%2528C-index%253A%25200.71%2520%255B0.62-0.81%255D%252C%2520HR%253A%25203.81%2520%255B2.35-6.17%252C%2520p%253D0.02%255D%2529%252C%2520where%2520no%2520diagnostic%250Atools%2520are%2520currently%2520recommended%2520by%2520clinical%2520guidelines.%2520These%2520results%2520suggest%250Athat%2520our%2520AI%2520test%2520can%2520improve%2520accuracy%252C%2520extend%2520applicability%2520to%2520a%2520wider%2520range%2520of%250Apatients%252C%2520and%2520enhance%2520access%2520to%2520treatment%2520selection%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20AI%20for%20comprehensive%20breast%20cancer%20prognostication&entry.906535625=Jan%20Witowski%20and%20Ken%20Zeng%20and%20Joseph%20Cappadona%20and%20Jailan%20Elayoubi%20and%20Elena%20Diana%20Chiru%20and%20Nancy%20Chan%20and%20Young-Joon%20Kang%20and%20Frederick%20Howard%20and%20Irina%20Ostrovnaya%20and%20Carlos%20Fernandez-Granda%20and%20Freya%20Schnabel%20and%20Ugur%20Ozerdem%20and%20Kangning%20Liu%20and%20Zoe%20Steinsnyder%20and%20Nitya%20Thakore%20and%20Mohammad%20Sadic%20and%20Frank%20Yeung%20and%20Elisa%20Liu%20and%20Theodore%20Hill%20and%20Benjamin%20Swett%20and%20Danielle%20Rigau%20and%20Andrew%20Clayburn%20and%20Valerie%20Speirs%20and%20Marcus%20Vetter%20and%20Lina%20Sojak%20and%20Simone%20Muenst%20Soysal%20and%20Daniel%20Baumhoer%20and%20Khalil%20Choucair%20and%20Yu%20Zong%20and%20Lina%20Daoud%20and%20Anas%20Saad%20and%20Waleed%20Abdulsattar%20and%20Rafic%20Beydoun%20and%20Jia-Wern%20Pan%20and%20Haslina%20Makmur%20and%20Soo-Hwang%20Teo%20and%20Linda%20Ma%20Pak%20and%20Victor%20Angel%20and%20Dovile%20Zilenaite-Petrulaitiene%20and%20Arvydas%20Laurinavicius%20and%20Natalie%20Klar%20and%20Brian%20D.%20Piening%20and%20Carlo%20Bifulco%20and%20Sun-Young%20Jun%20and%20Jae%20Pak%20Yi%20and%20Su%20Hyun%20Lim%20and%20Adam%20Brufsky%20and%20Francisco%20J.%20Esteva%20and%20Lajos%20Pusztai%20and%20Yann%20LeCun%20and%20Krzysztof%20J.%20Geras&entry.1292438233=%20%20Treatment%20selection%20in%20breast%20cancer%20is%20guided%20by%20molecular%20subtypes%20and%0Aclinical%20characteristics.%20Recurrence%20risk%20assessment%20plays%20a%20crucial%20role%20in%0Apersonalizing%20treatment.%20Current%20methods%2C%20including%20genomic%20assays%2C%20have%0Alimited%20accuracy%20and%20clinical%20utility%2C%20leading%20to%20suboptimal%20decisions%20for%20many%0Apatients.%20We%20developed%20a%20test%20for%20breast%20cancer%20patient%20stratification%20based%20on%0Adigital%20pathology%20and%20clinical%20characteristics%20using%20novel%20AI%20methods.%0ASpecifically%2C%20we%20utilized%20a%20vision%20transformer-based%20pan-cancer%20foundation%0Amodel%20trained%20with%20self-supervised%20learning%20to%20extract%20features%20from%20digitized%0AH%26E-stained%20slides.%20These%20features%20were%20integrated%20with%20clinical%20data%20to%20form%20a%0Amulti-modal%20AI%20test%20predicting%20cancer%20recurrence%20and%20death.%20The%20test%20was%0Adeveloped%20and%20evaluated%20using%20data%20from%20a%20total%20of%208%2C161%20breast%20cancer%20patients%0Aacross%2015%20cohorts%20originating%20from%20seven%20countries.%20Of%20these%2C%203%2C502%20patients%0Afrom%20five%20cohorts%20were%20used%20exclusively%20for%20evaluation%2C%20while%20the%20remaining%0Apatients%20were%20used%20for%20training.%20Our%20test%20accurately%20predicted%20our%20primary%0Aendpoint%2C%20disease-free%20interval%2C%20in%20the%20five%20external%20cohorts%20%28C-index%3A%200.71%0A%5B0.68-0.75%5D%2C%20HR%3A%203.63%20%5B3.02-4.37%2C%20p%3C0.01%5D%29.%20In%20a%20direct%20comparison%20%28N%3D858%29%2C%20the%0AAI%20test%20was%20more%20accurate%20than%20Oncotype%20DX%2C%20the%20standard-of-care%2021-gene%20assay%2C%0Awith%20a%20C-index%20of%200.67%20%5B0.61-0.74%5D%20versus%200.61%20%5B0.49-0.73%5D%2C%20respectively.%0AAdditionally%2C%20the%20AI%20test%20added%20independent%20information%20to%20Oncotype%20DX%20in%20a%0Amultivariate%20analysis%20%28HR%3A%203.11%20%5B1.91-5.09%2C%20p%3C0.01%29%5D%29.%20The%20test%20demonstrated%0Arobust%20accuracy%20across%20all%20major%20breast%20cancer%20subtypes%2C%20including%20TNBC%0A%28C-index%3A%200.71%20%5B0.62-0.81%5D%2C%20HR%3A%203.81%20%5B2.35-6.17%2C%20p%3D0.02%5D%29%2C%20where%20no%20diagnostic%0Atools%20are%20currently%20recommended%20by%20clinical%20guidelines.%20These%20results%20suggest%0Athat%20our%20AI%20test%20can%20improve%20accuracy%2C%20extend%20applicability%20to%20a%20wider%20range%20of%0Apatients%2C%20and%20enhance%20access%20to%20treatment%20selection%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21256v1&entry.124074799=Read"},
{"title": "SoS Certifiability of Subgaussian Distributions and its Algorithmic\n  Applications", "author": "Ilias Diakonikolas and Samuel B. Hopkins and Ankit Pensia and Stefan Tiegel", "abstract": "  We prove that there is a universal constant $C>0$ so that for every $d \\in\n\\mathbb N$, every centered subgaussian distribution $\\mathcal D$ on $\\mathbb\nR^d$, and every even $p \\in \\mathbb N$, the $d$-variate polynomial $(Cp)^{p/2}\n\\cdot \\|v\\|_{2}^p - \\mathbb E_{X \\sim \\mathcal D} \\langle v,X\\rangle^p$ is a\nsum of square polynomials. This establishes that every subgaussian distribution\nis \\emph{SoS-certifiably subgaussian} -- a condition that yields efficient\nlearning algorithms for a wide variety of high-dimensional statistical tasks.\nAs a direct corollary, we obtain computationally efficient algorithms with\nnear-optimal guarantees for the following tasks, when given samples from an\narbitrary subgaussian distribution: robust mean estimation, list-decodable mean\nestimation, clustering mean-separated mixture models, robust covariance-aware\nmean estimation, robust covariance estimation, and robust linear regression.\nOur proof makes essential use of Talagrand's generic chaining/majorizing\nmeasures theorem.\n", "link": "http://arxiv.org/abs/2410.21194v1", "date": "2024-10-28", "relevancy": 1.7465, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4396}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4374}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoS%20Certifiability%20of%20Subgaussian%20Distributions%20and%20its%20Algorithmic%0A%20%20Applications&body=Title%3A%20SoS%20Certifiability%20of%20Subgaussian%20Distributions%20and%20its%20Algorithmic%0A%20%20Applications%0AAuthor%3A%20Ilias%20Diakonikolas%20and%20Samuel%20B.%20Hopkins%20and%20Ankit%20Pensia%20and%20Stefan%20Tiegel%0AAbstract%3A%20%20%20We%20prove%20that%20there%20is%20a%20universal%20constant%20%24C%3E0%24%20so%20that%20for%20every%20%24d%20%5Cin%0A%5Cmathbb%20N%24%2C%20every%20centered%20subgaussian%20distribution%20%24%5Cmathcal%20D%24%20on%20%24%5Cmathbb%0AR%5Ed%24%2C%20and%20every%20even%20%24p%20%5Cin%20%5Cmathbb%20N%24%2C%20the%20%24d%24-variate%20polynomial%20%24%28Cp%29%5E%7Bp/2%7D%0A%5Ccdot%20%5C%7Cv%5C%7C_%7B2%7D%5Ep%20-%20%5Cmathbb%20E_%7BX%20%5Csim%20%5Cmathcal%20D%7D%20%5Clangle%20v%2CX%5Crangle%5Ep%24%20is%20a%0Asum%20of%20square%20polynomials.%20This%20establishes%20that%20every%20subgaussian%20distribution%0Ais%20%5Cemph%7BSoS-certifiably%20subgaussian%7D%20--%20a%20condition%20that%20yields%20efficient%0Alearning%20algorithms%20for%20a%20wide%20variety%20of%20high-dimensional%20statistical%20tasks.%0AAs%20a%20direct%20corollary%2C%20we%20obtain%20computationally%20efficient%20algorithms%20with%0Anear-optimal%20guarantees%20for%20the%20following%20tasks%2C%20when%20given%20samples%20from%20an%0Aarbitrary%20subgaussian%20distribution%3A%20robust%20mean%20estimation%2C%20list-decodable%20mean%0Aestimation%2C%20clustering%20mean-separated%20mixture%20models%2C%20robust%20covariance-aware%0Amean%20estimation%2C%20robust%20covariance%20estimation%2C%20and%20robust%20linear%20regression.%0AOur%20proof%20makes%20essential%20use%20of%20Talagrand%27s%20generic%20chaining/majorizing%0Ameasures%20theorem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoS%2520Certifiability%2520of%2520Subgaussian%2520Distributions%2520and%2520its%2520Algorithmic%250A%2520%2520Applications%26entry.906535625%3DIlias%2520Diakonikolas%2520and%2520Samuel%2520B.%2520Hopkins%2520and%2520Ankit%2520Pensia%2520and%2520Stefan%2520Tiegel%26entry.1292438233%3D%2520%2520We%2520prove%2520that%2520there%2520is%2520a%2520universal%2520constant%2520%2524C%253E0%2524%2520so%2520that%2520for%2520every%2520%2524d%2520%255Cin%250A%255Cmathbb%2520N%2524%252C%2520every%2520centered%2520subgaussian%2520distribution%2520%2524%255Cmathcal%2520D%2524%2520on%2520%2524%255Cmathbb%250AR%255Ed%2524%252C%2520and%2520every%2520even%2520%2524p%2520%255Cin%2520%255Cmathbb%2520N%2524%252C%2520the%2520%2524d%2524-variate%2520polynomial%2520%2524%2528Cp%2529%255E%257Bp/2%257D%250A%255Ccdot%2520%255C%257Cv%255C%257C_%257B2%257D%255Ep%2520-%2520%255Cmathbb%2520E_%257BX%2520%255Csim%2520%255Cmathcal%2520D%257D%2520%255Clangle%2520v%252CX%255Crangle%255Ep%2524%2520is%2520a%250Asum%2520of%2520square%2520polynomials.%2520This%2520establishes%2520that%2520every%2520subgaussian%2520distribution%250Ais%2520%255Cemph%257BSoS-certifiably%2520subgaussian%257D%2520--%2520a%2520condition%2520that%2520yields%2520efficient%250Alearning%2520algorithms%2520for%2520a%2520wide%2520variety%2520of%2520high-dimensional%2520statistical%2520tasks.%250AAs%2520a%2520direct%2520corollary%252C%2520we%2520obtain%2520computationally%2520efficient%2520algorithms%2520with%250Anear-optimal%2520guarantees%2520for%2520the%2520following%2520tasks%252C%2520when%2520given%2520samples%2520from%2520an%250Aarbitrary%2520subgaussian%2520distribution%253A%2520robust%2520mean%2520estimation%252C%2520list-decodable%2520mean%250Aestimation%252C%2520clustering%2520mean-separated%2520mixture%2520models%252C%2520robust%2520covariance-aware%250Amean%2520estimation%252C%2520robust%2520covariance%2520estimation%252C%2520and%2520robust%2520linear%2520regression.%250AOur%2520proof%2520makes%2520essential%2520use%2520of%2520Talagrand%2527s%2520generic%2520chaining/majorizing%250Ameasures%2520theorem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoS%20Certifiability%20of%20Subgaussian%20Distributions%20and%20its%20Algorithmic%0A%20%20Applications&entry.906535625=Ilias%20Diakonikolas%20and%20Samuel%20B.%20Hopkins%20and%20Ankit%20Pensia%20and%20Stefan%20Tiegel&entry.1292438233=%20%20We%20prove%20that%20there%20is%20a%20universal%20constant%20%24C%3E0%24%20so%20that%20for%20every%20%24d%20%5Cin%0A%5Cmathbb%20N%24%2C%20every%20centered%20subgaussian%20distribution%20%24%5Cmathcal%20D%24%20on%20%24%5Cmathbb%0AR%5Ed%24%2C%20and%20every%20even%20%24p%20%5Cin%20%5Cmathbb%20N%24%2C%20the%20%24d%24-variate%20polynomial%20%24%28Cp%29%5E%7Bp/2%7D%0A%5Ccdot%20%5C%7Cv%5C%7C_%7B2%7D%5Ep%20-%20%5Cmathbb%20E_%7BX%20%5Csim%20%5Cmathcal%20D%7D%20%5Clangle%20v%2CX%5Crangle%5Ep%24%20is%20a%0Asum%20of%20square%20polynomials.%20This%20establishes%20that%20every%20subgaussian%20distribution%0Ais%20%5Cemph%7BSoS-certifiably%20subgaussian%7D%20--%20a%20condition%20that%20yields%20efficient%0Alearning%20algorithms%20for%20a%20wide%20variety%20of%20high-dimensional%20statistical%20tasks.%0AAs%20a%20direct%20corollary%2C%20we%20obtain%20computationally%20efficient%20algorithms%20with%0Anear-optimal%20guarantees%20for%20the%20following%20tasks%2C%20when%20given%20samples%20from%20an%0Aarbitrary%20subgaussian%20distribution%3A%20robust%20mean%20estimation%2C%20list-decodable%20mean%0Aestimation%2C%20clustering%20mean-separated%20mixture%20models%2C%20robust%20covariance-aware%0Amean%20estimation%2C%20robust%20covariance%20estimation%2C%20and%20robust%20linear%20regression.%0AOur%20proof%20makes%20essential%20use%20of%20Talagrand%27s%20generic%20chaining/majorizing%0Ameasures%20theorem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21194v1&entry.124074799=Read"},
{"title": "One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion\n  Distillation", "author": "Zhendong Wang and Zhaoshuo Li and Ajay Mandlekar and Zhenjia Xu and Jiaojiao Fan and Yashraj Narang and Linxi Fan and Yuke Zhu and Yogesh Balaji and Mingyuan Zhou and Ming-Yu Liu and Yu Zeng", "abstract": "  Diffusion models, praised for their success in generative tasks, are\nincreasingly being applied to robotics, demonstrating exceptional performance\nin behavior cloning. However, their slow generation process stemming from\niterative denoising steps poses a challenge for real-time applications in\nresource-constrained robotics setups and dynamically changing environments. In\nthis paper, we introduce the One-Step Diffusion Policy (OneDP), a novel\napproach that distills knowledge from pre-trained diffusion policies into a\nsingle-step action generator, significantly accelerating response times for\nrobotic control tasks. We ensure the distilled generator closely aligns with\nthe original policy distribution by minimizing the Kullback-Leibler (KL)\ndivergence along the diffusion chain, requiring only $2\\%$-$10\\%$ additional\npre-training cost for convergence. We evaluated OneDP on 6 challenging\nsimulation tasks as well as 4 self-designed real-world tasks using the Franka\nrobot. The results demonstrate that OneDP not only achieves state-of-the-art\nsuccess rates but also delivers an order-of-magnitude improvement in inference\nspeed, boosting action prediction frequency from 1.5 Hz to 62 Hz, establishing\nits potential for dynamic and computationally constrained robotic applications.\nWe share the project page at https://research.nvidia.com/labs/dir/onedp/.\n", "link": "http://arxiv.org/abs/2410.21257v1", "date": "2024-10-28", "relevancy": 1.7408, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6465}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5637}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Step%20Diffusion%20Policy%3A%20Fast%20Visuomotor%20Policies%20via%20Diffusion%0A%20%20Distillation&body=Title%3A%20One-Step%20Diffusion%20Policy%3A%20Fast%20Visuomotor%20Policies%20via%20Diffusion%0A%20%20Distillation%0AAuthor%3A%20Zhendong%20Wang%20and%20Zhaoshuo%20Li%20and%20Ajay%20Mandlekar%20and%20Zhenjia%20Xu%20and%20Jiaojiao%20Fan%20and%20Yashraj%20Narang%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%20and%20Yogesh%20Balaji%20and%20Mingyuan%20Zhou%20and%20Ming-Yu%20Liu%20and%20Yu%20Zeng%0AAbstract%3A%20%20%20Diffusion%20models%2C%20praised%20for%20their%20success%20in%20generative%20tasks%2C%20are%0Aincreasingly%20being%20applied%20to%20robotics%2C%20demonstrating%20exceptional%20performance%0Ain%20behavior%20cloning.%20However%2C%20their%20slow%20generation%20process%20stemming%20from%0Aiterative%20denoising%20steps%20poses%20a%20challenge%20for%20real-time%20applications%20in%0Aresource-constrained%20robotics%20setups%20and%20dynamically%20changing%20environments.%20In%0Athis%20paper%2C%20we%20introduce%20the%20One-Step%20Diffusion%20Policy%20%28OneDP%29%2C%20a%20novel%0Aapproach%20that%20distills%20knowledge%20from%20pre-trained%20diffusion%20policies%20into%20a%0Asingle-step%20action%20generator%2C%20significantly%20accelerating%20response%20times%20for%0Arobotic%20control%20tasks.%20We%20ensure%20the%20distilled%20generator%20closely%20aligns%20with%0Athe%20original%20policy%20distribution%20by%20minimizing%20the%20Kullback-Leibler%20%28KL%29%0Adivergence%20along%20the%20diffusion%20chain%2C%20requiring%20only%20%242%5C%25%24-%2410%5C%25%24%20additional%0Apre-training%20cost%20for%20convergence.%20We%20evaluated%20OneDP%20on%206%20challenging%0Asimulation%20tasks%20as%20well%20as%204%20self-designed%20real-world%20tasks%20using%20the%20Franka%0Arobot.%20The%20results%20demonstrate%20that%20OneDP%20not%20only%20achieves%20state-of-the-art%0Asuccess%20rates%20but%20also%20delivers%20an%20order-of-magnitude%20improvement%20in%20inference%0Aspeed%2C%20boosting%20action%20prediction%20frequency%20from%201.5%20Hz%20to%2062%20Hz%2C%20establishing%0Aits%20potential%20for%20dynamic%20and%20computationally%20constrained%20robotic%20applications.%0AWe%20share%20the%20project%20page%20at%20https%3A//research.nvidia.com/labs/dir/onedp/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Step%2520Diffusion%2520Policy%253A%2520Fast%2520Visuomotor%2520Policies%2520via%2520Diffusion%250A%2520%2520Distillation%26entry.906535625%3DZhendong%2520Wang%2520and%2520Zhaoshuo%2520Li%2520and%2520Ajay%2520Mandlekar%2520and%2520Zhenjia%2520Xu%2520and%2520Jiaojiao%2520Fan%2520and%2520Yashraj%2520Narang%2520and%2520Linxi%2520Fan%2520and%2520Yuke%2520Zhu%2520and%2520Yogesh%2520Balaji%2520and%2520Mingyuan%2520Zhou%2520and%2520Ming-Yu%2520Liu%2520and%2520Yu%2520Zeng%26entry.1292438233%3D%2520%2520Diffusion%2520models%252C%2520praised%2520for%2520their%2520success%2520in%2520generative%2520tasks%252C%2520are%250Aincreasingly%2520being%2520applied%2520to%2520robotics%252C%2520demonstrating%2520exceptional%2520performance%250Ain%2520behavior%2520cloning.%2520However%252C%2520their%2520slow%2520generation%2520process%2520stemming%2520from%250Aiterative%2520denoising%2520steps%2520poses%2520a%2520challenge%2520for%2520real-time%2520applications%2520in%250Aresource-constrained%2520robotics%2520setups%2520and%2520dynamically%2520changing%2520environments.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520the%2520One-Step%2520Diffusion%2520Policy%2520%2528OneDP%2529%252C%2520a%2520novel%250Aapproach%2520that%2520distills%2520knowledge%2520from%2520pre-trained%2520diffusion%2520policies%2520into%2520a%250Asingle-step%2520action%2520generator%252C%2520significantly%2520accelerating%2520response%2520times%2520for%250Arobotic%2520control%2520tasks.%2520We%2520ensure%2520the%2520distilled%2520generator%2520closely%2520aligns%2520with%250Athe%2520original%2520policy%2520distribution%2520by%2520minimizing%2520the%2520Kullback-Leibler%2520%2528KL%2529%250Adivergence%2520along%2520the%2520diffusion%2520chain%252C%2520requiring%2520only%2520%25242%255C%2525%2524-%252410%255C%2525%2524%2520additional%250Apre-training%2520cost%2520for%2520convergence.%2520We%2520evaluated%2520OneDP%2520on%25206%2520challenging%250Asimulation%2520tasks%2520as%2520well%2520as%25204%2520self-designed%2520real-world%2520tasks%2520using%2520the%2520Franka%250Arobot.%2520The%2520results%2520demonstrate%2520that%2520OneDP%2520not%2520only%2520achieves%2520state-of-the-art%250Asuccess%2520rates%2520but%2520also%2520delivers%2520an%2520order-of-magnitude%2520improvement%2520in%2520inference%250Aspeed%252C%2520boosting%2520action%2520prediction%2520frequency%2520from%25201.5%2520Hz%2520to%252062%2520Hz%252C%2520establishing%250Aits%2520potential%2520for%2520dynamic%2520and%2520computationally%2520constrained%2520robotic%2520applications.%250AWe%2520share%2520the%2520project%2520page%2520at%2520https%253A//research.nvidia.com/labs/dir/onedp/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Step%20Diffusion%20Policy%3A%20Fast%20Visuomotor%20Policies%20via%20Diffusion%0A%20%20Distillation&entry.906535625=Zhendong%20Wang%20and%20Zhaoshuo%20Li%20and%20Ajay%20Mandlekar%20and%20Zhenjia%20Xu%20and%20Jiaojiao%20Fan%20and%20Yashraj%20Narang%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%20and%20Yogesh%20Balaji%20and%20Mingyuan%20Zhou%20and%20Ming-Yu%20Liu%20and%20Yu%20Zeng&entry.1292438233=%20%20Diffusion%20models%2C%20praised%20for%20their%20success%20in%20generative%20tasks%2C%20are%0Aincreasingly%20being%20applied%20to%20robotics%2C%20demonstrating%20exceptional%20performance%0Ain%20behavior%20cloning.%20However%2C%20their%20slow%20generation%20process%20stemming%20from%0Aiterative%20denoising%20steps%20poses%20a%20challenge%20for%20real-time%20applications%20in%0Aresource-constrained%20robotics%20setups%20and%20dynamically%20changing%20environments.%20In%0Athis%20paper%2C%20we%20introduce%20the%20One-Step%20Diffusion%20Policy%20%28OneDP%29%2C%20a%20novel%0Aapproach%20that%20distills%20knowledge%20from%20pre-trained%20diffusion%20policies%20into%20a%0Asingle-step%20action%20generator%2C%20significantly%20accelerating%20response%20times%20for%0Arobotic%20control%20tasks.%20We%20ensure%20the%20distilled%20generator%20closely%20aligns%20with%0Athe%20original%20policy%20distribution%20by%20minimizing%20the%20Kullback-Leibler%20%28KL%29%0Adivergence%20along%20the%20diffusion%20chain%2C%20requiring%20only%20%242%5C%25%24-%2410%5C%25%24%20additional%0Apre-training%20cost%20for%20convergence.%20We%20evaluated%20OneDP%20on%206%20challenging%0Asimulation%20tasks%20as%20well%20as%204%20self-designed%20real-world%20tasks%20using%20the%20Franka%0Arobot.%20The%20results%20demonstrate%20that%20OneDP%20not%20only%20achieves%20state-of-the-art%0Asuccess%20rates%20but%20also%20delivers%20an%20order-of-magnitude%20improvement%20in%20inference%0Aspeed%2C%20boosting%20action%20prediction%20frequency%20from%201.5%20Hz%20to%2062%20Hz%2C%20establishing%0Aits%20potential%20for%20dynamic%20and%20computationally%20constrained%20robotic%20applications.%0AWe%20share%20the%20project%20page%20at%20https%3A//research.nvidia.com/labs/dir/onedp/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21257v1&entry.124074799=Read"},
{"title": "HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots", "author": "Tairan He and Wenli Xiao and Toru Lin and Zhengyi Luo and Zhenjia Xu and Zhenyu Jiang and Jan Kautz and Changliu Liu and Guanya Shi and Xiaolong Wang and Linxi Fan and Yuke Zhu", "abstract": "  Humanoid whole-body control requires adapting to diverse tasks such as\nnavigation, loco-manipulation, and tabletop manipulation, each demanding a\ndifferent mode of control. For example, navigation relies on root velocity\ntracking, while tabletop manipulation prioritizes upper-body joint angle\ntracking. Existing approaches typically train individual policies tailored to a\nspecific command space, limiting their transferability across modes. We present\nthe key insight that full-body kinematic motion imitation can serve as a common\nabstraction for all these tasks and provide general-purpose motor skills for\nlearning multiple modes of whole-body control. Building on this, we propose\nHOVER (Humanoid Versatile Controller), a multi-mode policy distillation\nframework that consolidates diverse control modes into a unified policy. HOVER\nenables seamless transitions between control modes while preserving the\ndistinct advantages of each, offering a robust and scalable solution for\nhumanoid control across a wide range of modes. By eliminating the need for\npolicy retraining for each control mode, our approach improves efficiency and\nflexibility for future humanoid applications.\n", "link": "http://arxiv.org/abs/2410.21229v1", "date": "2024-10-28", "relevancy": 1.7357, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6078}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5648}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOVER%3A%20Versatile%20Neural%20Whole-Body%20Controller%20for%20Humanoid%20Robots&body=Title%3A%20HOVER%3A%20Versatile%20Neural%20Whole-Body%20Controller%20for%20Humanoid%20Robots%0AAuthor%3A%20Tairan%20He%20and%20Wenli%20Xiao%20and%20Toru%20Lin%20and%20Zhengyi%20Luo%20and%20Zhenjia%20Xu%20and%20Zhenyu%20Jiang%20and%20Jan%20Kautz%20and%20Changliu%20Liu%20and%20Guanya%20Shi%20and%20Xiaolong%20Wang%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%0AAbstract%3A%20%20%20Humanoid%20whole-body%20control%20requires%20adapting%20to%20diverse%20tasks%20such%20as%0Anavigation%2C%20loco-manipulation%2C%20and%20tabletop%20manipulation%2C%20each%20demanding%20a%0Adifferent%20mode%20of%20control.%20For%20example%2C%20navigation%20relies%20on%20root%20velocity%0Atracking%2C%20while%20tabletop%20manipulation%20prioritizes%20upper-body%20joint%20angle%0Atracking.%20Existing%20approaches%20typically%20train%20individual%20policies%20tailored%20to%20a%0Aspecific%20command%20space%2C%20limiting%20their%20transferability%20across%20modes.%20We%20present%0Athe%20key%20insight%20that%20full-body%20kinematic%20motion%20imitation%20can%20serve%20as%20a%20common%0Aabstraction%20for%20all%20these%20tasks%20and%20provide%20general-purpose%20motor%20skills%20for%0Alearning%20multiple%20modes%20of%20whole-body%20control.%20Building%20on%20this%2C%20we%20propose%0AHOVER%20%28Humanoid%20Versatile%20Controller%29%2C%20a%20multi-mode%20policy%20distillation%0Aframework%20that%20consolidates%20diverse%20control%20modes%20into%20a%20unified%20policy.%20HOVER%0Aenables%20seamless%20transitions%20between%20control%20modes%20while%20preserving%20the%0Adistinct%20advantages%20of%20each%2C%20offering%20a%20robust%20and%20scalable%20solution%20for%0Ahumanoid%20control%20across%20a%20wide%20range%20of%20modes.%20By%20eliminating%20the%20need%20for%0Apolicy%20retraining%20for%20each%20control%20mode%2C%20our%20approach%20improves%20efficiency%20and%0Aflexibility%20for%20future%20humanoid%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOVER%253A%2520Versatile%2520Neural%2520Whole-Body%2520Controller%2520for%2520Humanoid%2520Robots%26entry.906535625%3DTairan%2520He%2520and%2520Wenli%2520Xiao%2520and%2520Toru%2520Lin%2520and%2520Zhengyi%2520Luo%2520and%2520Zhenjia%2520Xu%2520and%2520Zhenyu%2520Jiang%2520and%2520Jan%2520Kautz%2520and%2520Changliu%2520Liu%2520and%2520Guanya%2520Shi%2520and%2520Xiaolong%2520Wang%2520and%2520Linxi%2520Fan%2520and%2520Yuke%2520Zhu%26entry.1292438233%3D%2520%2520Humanoid%2520whole-body%2520control%2520requires%2520adapting%2520to%2520diverse%2520tasks%2520such%2520as%250Anavigation%252C%2520loco-manipulation%252C%2520and%2520tabletop%2520manipulation%252C%2520each%2520demanding%2520a%250Adifferent%2520mode%2520of%2520control.%2520For%2520example%252C%2520navigation%2520relies%2520on%2520root%2520velocity%250Atracking%252C%2520while%2520tabletop%2520manipulation%2520prioritizes%2520upper-body%2520joint%2520angle%250Atracking.%2520Existing%2520approaches%2520typically%2520train%2520individual%2520policies%2520tailored%2520to%2520a%250Aspecific%2520command%2520space%252C%2520limiting%2520their%2520transferability%2520across%2520modes.%2520We%2520present%250Athe%2520key%2520insight%2520that%2520full-body%2520kinematic%2520motion%2520imitation%2520can%2520serve%2520as%2520a%2520common%250Aabstraction%2520for%2520all%2520these%2520tasks%2520and%2520provide%2520general-purpose%2520motor%2520skills%2520for%250Alearning%2520multiple%2520modes%2520of%2520whole-body%2520control.%2520Building%2520on%2520this%252C%2520we%2520propose%250AHOVER%2520%2528Humanoid%2520Versatile%2520Controller%2529%252C%2520a%2520multi-mode%2520policy%2520distillation%250Aframework%2520that%2520consolidates%2520diverse%2520control%2520modes%2520into%2520a%2520unified%2520policy.%2520HOVER%250Aenables%2520seamless%2520transitions%2520between%2520control%2520modes%2520while%2520preserving%2520the%250Adistinct%2520advantages%2520of%2520each%252C%2520offering%2520a%2520robust%2520and%2520scalable%2520solution%2520for%250Ahumanoid%2520control%2520across%2520a%2520wide%2520range%2520of%2520modes.%2520By%2520eliminating%2520the%2520need%2520for%250Apolicy%2520retraining%2520for%2520each%2520control%2520mode%252C%2520our%2520approach%2520improves%2520efficiency%2520and%250Aflexibility%2520for%2520future%2520humanoid%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOVER%3A%20Versatile%20Neural%20Whole-Body%20Controller%20for%20Humanoid%20Robots&entry.906535625=Tairan%20He%20and%20Wenli%20Xiao%20and%20Toru%20Lin%20and%20Zhengyi%20Luo%20and%20Zhenjia%20Xu%20and%20Zhenyu%20Jiang%20and%20Jan%20Kautz%20and%20Changliu%20Liu%20and%20Guanya%20Shi%20and%20Xiaolong%20Wang%20and%20Linxi%20Fan%20and%20Yuke%20Zhu&entry.1292438233=%20%20Humanoid%20whole-body%20control%20requires%20adapting%20to%20diverse%20tasks%20such%20as%0Anavigation%2C%20loco-manipulation%2C%20and%20tabletop%20manipulation%2C%20each%20demanding%20a%0Adifferent%20mode%20of%20control.%20For%20example%2C%20navigation%20relies%20on%20root%20velocity%0Atracking%2C%20while%20tabletop%20manipulation%20prioritizes%20upper-body%20joint%20angle%0Atracking.%20Existing%20approaches%20typically%20train%20individual%20policies%20tailored%20to%20a%0Aspecific%20command%20space%2C%20limiting%20their%20transferability%20across%20modes.%20We%20present%0Athe%20key%20insight%20that%20full-body%20kinematic%20motion%20imitation%20can%20serve%20as%20a%20common%0Aabstraction%20for%20all%20these%20tasks%20and%20provide%20general-purpose%20motor%20skills%20for%0Alearning%20multiple%20modes%20of%20whole-body%20control.%20Building%20on%20this%2C%20we%20propose%0AHOVER%20%28Humanoid%20Versatile%20Controller%29%2C%20a%20multi-mode%20policy%20distillation%0Aframework%20that%20consolidates%20diverse%20control%20modes%20into%20a%20unified%20policy.%20HOVER%0Aenables%20seamless%20transitions%20between%20control%20modes%20while%20preserving%20the%0Adistinct%20advantages%20of%20each%2C%20offering%20a%20robust%20and%20scalable%20solution%20for%0Ahumanoid%20control%20across%20a%20wide%20range%20of%20modes.%20By%20eliminating%20the%20need%20for%0Apolicy%20retraining%20for%20each%20control%20mode%2C%20our%20approach%20improves%20efficiency%20and%0Aflexibility%20for%20future%20humanoid%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21229v1&entry.124074799=Read"},
{"title": "Learning to Walk from Three Minutes of Real-World Data with\n  Semi-structured Dynamics Models", "author": "Jacob Levy and Tyler Westenbroek and David Fridovich-Keil", "abstract": "  Traditionally, model-based reinforcement learning (MBRL) methods exploit\nneural networks as flexible function approximators to represent $\\textit{a\npriori}$ unknown environment dynamics. However, training data are typically\nscarce in practice, and these black-box models often fail to generalize.\nModeling architectures that leverage known physics can substantially reduce the\ncomplexity of system-identification, but break down in the face of complex\nphenomena such as contact. We introduce a novel framework for learning\nsemi-structured dynamics models for contact-rich systems which seamlessly\nintegrates structured first principles modeling techniques with black-box\nauto-regressive models. Specifically, we develop an ensemble of probabilistic\nmodels to estimate external forces, conditioned on historical observations and\nactions, and integrate these predictions using known Lagrangian dynamics. With\nthis semi-structured approach, we can make accurate long-horizon predictions\nwith substantially less data than prior methods. We leverage this capability\nand propose Semi-Structured Reinforcement Learning ($\\texttt{SSRL}$) a simple\nmodel-based learning framework which pushes the sample complexity boundary for\nreal-world learning. We validate our approach on a real-world Unitree Go1\nquadruped robot, learning dynamic gaits -- from scratch -- on both hard and\nsoft surfaces with just a few minutes of real-world data. Video and code are\navailable at: https://sites.google.com/utexas.edu/ssrl\n", "link": "http://arxiv.org/abs/2410.09163v2", "date": "2024-10-28", "relevancy": 1.7081, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6618}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5861}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Walk%20from%20Three%20Minutes%20of%20Real-World%20Data%20with%0A%20%20Semi-structured%20Dynamics%20Models&body=Title%3A%20Learning%20to%20Walk%20from%20Three%20Minutes%20of%20Real-World%20Data%20with%0A%20%20Semi-structured%20Dynamics%20Models%0AAuthor%3A%20Jacob%20Levy%20and%20Tyler%20Westenbroek%20and%20David%20Fridovich-Keil%0AAbstract%3A%20%20%20Traditionally%2C%20model-based%20reinforcement%20learning%20%28MBRL%29%20methods%20exploit%0Aneural%20networks%20as%20flexible%20function%20approximators%20to%20represent%20%24%5Ctextit%7Ba%0Apriori%7D%24%20unknown%20environment%20dynamics.%20However%2C%20training%20data%20are%20typically%0Ascarce%20in%20practice%2C%20and%20these%20black-box%20models%20often%20fail%20to%20generalize.%0AModeling%20architectures%20that%20leverage%20known%20physics%20can%20substantially%20reduce%20the%0Acomplexity%20of%20system-identification%2C%20but%20break%20down%20in%20the%20face%20of%20complex%0Aphenomena%20such%20as%20contact.%20We%20introduce%20a%20novel%20framework%20for%20learning%0Asemi-structured%20dynamics%20models%20for%20contact-rich%20systems%20which%20seamlessly%0Aintegrates%20structured%20first%20principles%20modeling%20techniques%20with%20black-box%0Aauto-regressive%20models.%20Specifically%2C%20we%20develop%20an%20ensemble%20of%20probabilistic%0Amodels%20to%20estimate%20external%20forces%2C%20conditioned%20on%20historical%20observations%20and%0Aactions%2C%20and%20integrate%20these%20predictions%20using%20known%20Lagrangian%20dynamics.%20With%0Athis%20semi-structured%20approach%2C%20we%20can%20make%20accurate%20long-horizon%20predictions%0Awith%20substantially%20less%20data%20than%20prior%20methods.%20We%20leverage%20this%20capability%0Aand%20propose%20Semi-Structured%20Reinforcement%20Learning%20%28%24%5Ctexttt%7BSSRL%7D%24%29%20a%20simple%0Amodel-based%20learning%20framework%20which%20pushes%20the%20sample%20complexity%20boundary%20for%0Areal-world%20learning.%20We%20validate%20our%20approach%20on%20a%20real-world%20Unitree%20Go1%0Aquadruped%20robot%2C%20learning%20dynamic%20gaits%20--%20from%20scratch%20--%20on%20both%20hard%20and%0Asoft%20surfaces%20with%20just%20a%20few%20minutes%20of%20real-world%20data.%20Video%20and%20code%20are%0Aavailable%20at%3A%20https%3A//sites.google.com/utexas.edu/ssrl%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09163v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Walk%2520from%2520Three%2520Minutes%2520of%2520Real-World%2520Data%2520with%250A%2520%2520Semi-structured%2520Dynamics%2520Models%26entry.906535625%3DJacob%2520Levy%2520and%2520Tyler%2520Westenbroek%2520and%2520David%2520Fridovich-Keil%26entry.1292438233%3D%2520%2520Traditionally%252C%2520model-based%2520reinforcement%2520learning%2520%2528MBRL%2529%2520methods%2520exploit%250Aneural%2520networks%2520as%2520flexible%2520function%2520approximators%2520to%2520represent%2520%2524%255Ctextit%257Ba%250Apriori%257D%2524%2520unknown%2520environment%2520dynamics.%2520However%252C%2520training%2520data%2520are%2520typically%250Ascarce%2520in%2520practice%252C%2520and%2520these%2520black-box%2520models%2520often%2520fail%2520to%2520generalize.%250AModeling%2520architectures%2520that%2520leverage%2520known%2520physics%2520can%2520substantially%2520reduce%2520the%250Acomplexity%2520of%2520system-identification%252C%2520but%2520break%2520down%2520in%2520the%2520face%2520of%2520complex%250Aphenomena%2520such%2520as%2520contact.%2520We%2520introduce%2520a%2520novel%2520framework%2520for%2520learning%250Asemi-structured%2520dynamics%2520models%2520for%2520contact-rich%2520systems%2520which%2520seamlessly%250Aintegrates%2520structured%2520first%2520principles%2520modeling%2520techniques%2520with%2520black-box%250Aauto-regressive%2520models.%2520Specifically%252C%2520we%2520develop%2520an%2520ensemble%2520of%2520probabilistic%250Amodels%2520to%2520estimate%2520external%2520forces%252C%2520conditioned%2520on%2520historical%2520observations%2520and%250Aactions%252C%2520and%2520integrate%2520these%2520predictions%2520using%2520known%2520Lagrangian%2520dynamics.%2520With%250Athis%2520semi-structured%2520approach%252C%2520we%2520can%2520make%2520accurate%2520long-horizon%2520predictions%250Awith%2520substantially%2520less%2520data%2520than%2520prior%2520methods.%2520We%2520leverage%2520this%2520capability%250Aand%2520propose%2520Semi-Structured%2520Reinforcement%2520Learning%2520%2528%2524%255Ctexttt%257BSSRL%257D%2524%2529%2520a%2520simple%250Amodel-based%2520learning%2520framework%2520which%2520pushes%2520the%2520sample%2520complexity%2520boundary%2520for%250Areal-world%2520learning.%2520We%2520validate%2520our%2520approach%2520on%2520a%2520real-world%2520Unitree%2520Go1%250Aquadruped%2520robot%252C%2520learning%2520dynamic%2520gaits%2520--%2520from%2520scratch%2520--%2520on%2520both%2520hard%2520and%250Asoft%2520surfaces%2520with%2520just%2520a%2520few%2520minutes%2520of%2520real-world%2520data.%2520Video%2520and%2520code%2520are%250Aavailable%2520at%253A%2520https%253A//sites.google.com/utexas.edu/ssrl%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09163v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Walk%20from%20Three%20Minutes%20of%20Real-World%20Data%20with%0A%20%20Semi-structured%20Dynamics%20Models&entry.906535625=Jacob%20Levy%20and%20Tyler%20Westenbroek%20and%20David%20Fridovich-Keil&entry.1292438233=%20%20Traditionally%2C%20model-based%20reinforcement%20learning%20%28MBRL%29%20methods%20exploit%0Aneural%20networks%20as%20flexible%20function%20approximators%20to%20represent%20%24%5Ctextit%7Ba%0Apriori%7D%24%20unknown%20environment%20dynamics.%20However%2C%20training%20data%20are%20typically%0Ascarce%20in%20practice%2C%20and%20these%20black-box%20models%20often%20fail%20to%20generalize.%0AModeling%20architectures%20that%20leverage%20known%20physics%20can%20substantially%20reduce%20the%0Acomplexity%20of%20system-identification%2C%20but%20break%20down%20in%20the%20face%20of%20complex%0Aphenomena%20such%20as%20contact.%20We%20introduce%20a%20novel%20framework%20for%20learning%0Asemi-structured%20dynamics%20models%20for%20contact-rich%20systems%20which%20seamlessly%0Aintegrates%20structured%20first%20principles%20modeling%20techniques%20with%20black-box%0Aauto-regressive%20models.%20Specifically%2C%20we%20develop%20an%20ensemble%20of%20probabilistic%0Amodels%20to%20estimate%20external%20forces%2C%20conditioned%20on%20historical%20observations%20and%0Aactions%2C%20and%20integrate%20these%20predictions%20using%20known%20Lagrangian%20dynamics.%20With%0Athis%20semi-structured%20approach%2C%20we%20can%20make%20accurate%20long-horizon%20predictions%0Awith%20substantially%20less%20data%20than%20prior%20methods.%20We%20leverage%20this%20capability%0Aand%20propose%20Semi-Structured%20Reinforcement%20Learning%20%28%24%5Ctexttt%7BSSRL%7D%24%29%20a%20simple%0Amodel-based%20learning%20framework%20which%20pushes%20the%20sample%20complexity%20boundary%20for%0Areal-world%20learning.%20We%20validate%20our%20approach%20on%20a%20real-world%20Unitree%20Go1%0Aquadruped%20robot%2C%20learning%20dynamic%20gaits%20--%20from%20scratch%20--%20on%20both%20hard%20and%0Asoft%20surfaces%20with%20just%20a%20few%20minutes%20of%20real-world%20data.%20Video%20and%20code%20are%0Aavailable%20at%3A%20https%3A//sites.google.com/utexas.edu/ssrl%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09163v2&entry.124074799=Read"},
{"title": "Online Weighted Paging with Unknown Weights", "author": "Orin Levy and Noam Touitou and Aviv Rosenberg", "abstract": "  Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.\n", "link": "http://arxiv.org/abs/2410.21266v1", "date": "2024-10-28", "relevancy": 1.6815, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4599}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4172}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Weighted%20Paging%20with%20Unknown%20Weights&body=Title%3A%20Online%20Weighted%20Paging%20with%20Unknown%20Weights%0AAuthor%3A%20Orin%20Levy%20and%20Noam%20Touitou%20and%20Aviv%20Rosenberg%0AAbstract%3A%20%20%20Online%20paging%20is%20a%20fundamental%20problem%20in%20the%20field%20of%20online%20algorithms%2C%20in%0Awhich%20one%20maintains%20a%20cache%20of%20%24k%24%20slots%20as%20requests%20for%20fetching%20pages%20arrive%0Aonline.%20In%20the%20weighted%20variant%20of%20this%20problem%2C%20each%20page%20has%20its%20own%20fetching%0Acost%3B%20a%20substantial%20line%20of%20work%20on%20this%20problem%20culminated%20in%20an%20%28optimal%29%0A%24O%28%5Clog%20k%29%24-competitive%20randomized%20algorithm%2C%20due%20to%20Bansal%2C%20Buchbinder%20and%0ANaor%20%28FOCS%2707%29.%0A%20%20Existing%20work%20for%20weighted%20paging%20assumes%20that%20page%20weights%20are%20known%20in%0Aadvance%2C%20which%20is%20not%20always%20the%20case%20in%20practice.%20For%20example%2C%20in%20multi-level%0Acaching%20architectures%2C%20the%20expected%20cost%20of%20fetching%20a%20memory%20block%20is%20a%0Afunction%20of%20its%20probability%20of%20being%20in%20a%20mid-level%20cache%20rather%20than%20the%20main%0Amemory.%20This%20complex%20property%20cannot%20be%20predicted%20in%20advance%3B%20over%20time%2C%0Ahowever%2C%20one%20may%20glean%20information%20about%20page%20weights%20through%20sampling%20their%0Afetching%20cost%20multiple%20times.%0A%20%20We%20present%20the%20first%20algorithm%20for%20online%20weighted%20paging%20that%20does%20not%20know%0Apage%20weights%20in%20advance%2C%20but%20rather%20learns%20from%20weight%20samples.%20In%20terms%20of%0Atechniques%2C%20this%20requires%20providing%20%28integral%29%20samples%20to%20a%20fractional%20solver%2C%0Arequiring%20a%20delicate%20interface%20between%20this%20solver%20and%20the%20randomized%20rounding%0Ascheme%3B%20we%20believe%20that%20our%20work%20can%20inspire%20online%20algorithms%20to%20other%0Aproblems%20that%20involve%20cost%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Weighted%2520Paging%2520with%2520Unknown%2520Weights%26entry.906535625%3DOrin%2520Levy%2520and%2520Noam%2520Touitou%2520and%2520Aviv%2520Rosenberg%26entry.1292438233%3D%2520%2520Online%2520paging%2520is%2520a%2520fundamental%2520problem%2520in%2520the%2520field%2520of%2520online%2520algorithms%252C%2520in%250Awhich%2520one%2520maintains%2520a%2520cache%2520of%2520%2524k%2524%2520slots%2520as%2520requests%2520for%2520fetching%2520pages%2520arrive%250Aonline.%2520In%2520the%2520weighted%2520variant%2520of%2520this%2520problem%252C%2520each%2520page%2520has%2520its%2520own%2520fetching%250Acost%253B%2520a%2520substantial%2520line%2520of%2520work%2520on%2520this%2520problem%2520culminated%2520in%2520an%2520%2528optimal%2529%250A%2524O%2528%255Clog%2520k%2529%2524-competitive%2520randomized%2520algorithm%252C%2520due%2520to%2520Bansal%252C%2520Buchbinder%2520and%250ANaor%2520%2528FOCS%252707%2529.%250A%2520%2520Existing%2520work%2520for%2520weighted%2520paging%2520assumes%2520that%2520page%2520weights%2520are%2520known%2520in%250Aadvance%252C%2520which%2520is%2520not%2520always%2520the%2520case%2520in%2520practice.%2520For%2520example%252C%2520in%2520multi-level%250Acaching%2520architectures%252C%2520the%2520expected%2520cost%2520of%2520fetching%2520a%2520memory%2520block%2520is%2520a%250Afunction%2520of%2520its%2520probability%2520of%2520being%2520in%2520a%2520mid-level%2520cache%2520rather%2520than%2520the%2520main%250Amemory.%2520This%2520complex%2520property%2520cannot%2520be%2520predicted%2520in%2520advance%253B%2520over%2520time%252C%250Ahowever%252C%2520one%2520may%2520glean%2520information%2520about%2520page%2520weights%2520through%2520sampling%2520their%250Afetching%2520cost%2520multiple%2520times.%250A%2520%2520We%2520present%2520the%2520first%2520algorithm%2520for%2520online%2520weighted%2520paging%2520that%2520does%2520not%2520know%250Apage%2520weights%2520in%2520advance%252C%2520but%2520rather%2520learns%2520from%2520weight%2520samples.%2520In%2520terms%2520of%250Atechniques%252C%2520this%2520requires%2520providing%2520%2528integral%2529%2520samples%2520to%2520a%2520fractional%2520solver%252C%250Arequiring%2520a%2520delicate%2520interface%2520between%2520this%2520solver%2520and%2520the%2520randomized%2520rounding%250Ascheme%253B%2520we%2520believe%2520that%2520our%2520work%2520can%2520inspire%2520online%2520algorithms%2520to%2520other%250Aproblems%2520that%2520involve%2520cost%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Weighted%20Paging%20with%20Unknown%20Weights&entry.906535625=Orin%20Levy%20and%20Noam%20Touitou%20and%20Aviv%20Rosenberg&entry.1292438233=%20%20Online%20paging%20is%20a%20fundamental%20problem%20in%20the%20field%20of%20online%20algorithms%2C%20in%0Awhich%20one%20maintains%20a%20cache%20of%20%24k%24%20slots%20as%20requests%20for%20fetching%20pages%20arrive%0Aonline.%20In%20the%20weighted%20variant%20of%20this%20problem%2C%20each%20page%20has%20its%20own%20fetching%0Acost%3B%20a%20substantial%20line%20of%20work%20on%20this%20problem%20culminated%20in%20an%20%28optimal%29%0A%24O%28%5Clog%20k%29%24-competitive%20randomized%20algorithm%2C%20due%20to%20Bansal%2C%20Buchbinder%20and%0ANaor%20%28FOCS%2707%29.%0A%20%20Existing%20work%20for%20weighted%20paging%20assumes%20that%20page%20weights%20are%20known%20in%0Aadvance%2C%20which%20is%20not%20always%20the%20case%20in%20practice.%20For%20example%2C%20in%20multi-level%0Acaching%20architectures%2C%20the%20expected%20cost%20of%20fetching%20a%20memory%20block%20is%20a%0Afunction%20of%20its%20probability%20of%20being%20in%20a%20mid-level%20cache%20rather%20than%20the%20main%0Amemory.%20This%20complex%20property%20cannot%20be%20predicted%20in%20advance%3B%20over%20time%2C%0Ahowever%2C%20one%20may%20glean%20information%20about%20page%20weights%20through%20sampling%20their%0Afetching%20cost%20multiple%20times.%0A%20%20We%20present%20the%20first%20algorithm%20for%20online%20weighted%20paging%20that%20does%20not%20know%0Apage%20weights%20in%20advance%2C%20but%20rather%20learns%20from%20weight%20samples.%20In%20terms%20of%0Atechniques%2C%20this%20requires%20providing%20%28integral%29%20samples%20to%20a%20fractional%20solver%2C%0Arequiring%20a%20delicate%20interface%20between%20this%20solver%20and%20the%20randomized%20rounding%0Ascheme%3B%20we%20believe%20that%20our%20work%20can%20inspire%20online%20algorithms%20to%20other%0Aproblems%20that%20involve%20cost%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21266v1&entry.124074799=Read"},
{"title": "A Comparative Analysis of Wealth Index Predictions in Africa between\n  three Multi-Source Inference Models", "author": "M\u00e1rton Karsai and J\u00e1nos Kert\u00e9sz and Lisette Esp\u00edn-Noboa", "abstract": "  Poverty map inference has become a critical focus of research, utilizing both\ntraditional and modern techniques, ranging from regression models to\nconvolutional neural networks applied to tabular data, satellite imagery, and\nnetworks. While much attention has been given to validating models during the\ntraining phase, the final predictions have received less scrutiny. In this\nstudy, we analyze the International Wealth Index (IWI) predicted by Lee and\nBraithwaite (2022) and Esp\\'in-Noboa et al. (2023), alongside the Relative\nWealth Index (RWI) inferred by Chi et al. (2022), across six Sub-Saharan\nAfrican countries. Our analysis reveals trends and discrepancies in wealth\npredictions between these models. In particular, significant and unexpected\ndiscrepancies between the predictions of Lee and Braithwaite and Esp\\'in-Noboa\net al., even after accounting for differences in training data. In contrast,\nthe shape of the wealth distributions predicted by Esp\\'in-Noboa et al. and Chi\net al. are more closely aligned, suggesting similar levels of skewness. These\nfindings raise concerns about the validity of certain models and emphasize the\nimportance of rigorous audits for wealth prediction algorithms used in\npolicy-making. Continuous validation and refinement are essential to ensure the\nreliability of these models, particularly when they inform poverty alleviation\nstrategies.\n", "link": "http://arxiv.org/abs/2408.01631v3", "date": "2024-10-28", "relevancy": 1.6387, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4431}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4051}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comparative%20Analysis%20of%20Wealth%20Index%20Predictions%20in%20Africa%20between%0A%20%20three%20Multi-Source%20Inference%20Models&body=Title%3A%20A%20Comparative%20Analysis%20of%20Wealth%20Index%20Predictions%20in%20Africa%20between%0A%20%20three%20Multi-Source%20Inference%20Models%0AAuthor%3A%20M%C3%A1rton%20Karsai%20and%20J%C3%A1nos%20Kert%C3%A9sz%20and%20Lisette%20Esp%C3%ADn-Noboa%0AAbstract%3A%20%20%20Poverty%20map%20inference%20has%20become%20a%20critical%20focus%20of%20research%2C%20utilizing%20both%0Atraditional%20and%20modern%20techniques%2C%20ranging%20from%20regression%20models%20to%0Aconvolutional%20neural%20networks%20applied%20to%20tabular%20data%2C%20satellite%20imagery%2C%20and%0Anetworks.%20While%20much%20attention%20has%20been%20given%20to%20validating%20models%20during%20the%0Atraining%20phase%2C%20the%20final%20predictions%20have%20received%20less%20scrutiny.%20In%20this%0Astudy%2C%20we%20analyze%20the%20International%20Wealth%20Index%20%28IWI%29%20predicted%20by%20Lee%20and%0ABraithwaite%20%282022%29%20and%20Esp%5C%27in-Noboa%20et%20al.%20%282023%29%2C%20alongside%20the%20Relative%0AWealth%20Index%20%28RWI%29%20inferred%20by%20Chi%20et%20al.%20%282022%29%2C%20across%20six%20Sub-Saharan%0AAfrican%20countries.%20Our%20analysis%20reveals%20trends%20and%20discrepancies%20in%20wealth%0Apredictions%20between%20these%20models.%20In%20particular%2C%20significant%20and%20unexpected%0Adiscrepancies%20between%20the%20predictions%20of%20Lee%20and%20Braithwaite%20and%20Esp%5C%27in-Noboa%0Aet%20al.%2C%20even%20after%20accounting%20for%20differences%20in%20training%20data.%20In%20contrast%2C%0Athe%20shape%20of%20the%20wealth%20distributions%20predicted%20by%20Esp%5C%27in-Noboa%20et%20al.%20and%20Chi%0Aet%20al.%20are%20more%20closely%20aligned%2C%20suggesting%20similar%20levels%20of%20skewness.%20These%0Afindings%20raise%20concerns%20about%20the%20validity%20of%20certain%20models%20and%20emphasize%20the%0Aimportance%20of%20rigorous%20audits%20for%20wealth%20prediction%20algorithms%20used%20in%0Apolicy-making.%20Continuous%20validation%20and%20refinement%20are%20essential%20to%20ensure%20the%0Areliability%20of%20these%20models%2C%20particularly%20when%20they%20inform%20poverty%20alleviation%0Astrategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01631v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comparative%2520Analysis%2520of%2520Wealth%2520Index%2520Predictions%2520in%2520Africa%2520between%250A%2520%2520three%2520Multi-Source%2520Inference%2520Models%26entry.906535625%3DM%25C3%25A1rton%2520Karsai%2520and%2520J%25C3%25A1nos%2520Kert%25C3%25A9sz%2520and%2520Lisette%2520Esp%25C3%25ADn-Noboa%26entry.1292438233%3D%2520%2520Poverty%2520map%2520inference%2520has%2520become%2520a%2520critical%2520focus%2520of%2520research%252C%2520utilizing%2520both%250Atraditional%2520and%2520modern%2520techniques%252C%2520ranging%2520from%2520regression%2520models%2520to%250Aconvolutional%2520neural%2520networks%2520applied%2520to%2520tabular%2520data%252C%2520satellite%2520imagery%252C%2520and%250Anetworks.%2520While%2520much%2520attention%2520has%2520been%2520given%2520to%2520validating%2520models%2520during%2520the%250Atraining%2520phase%252C%2520the%2520final%2520predictions%2520have%2520received%2520less%2520scrutiny.%2520In%2520this%250Astudy%252C%2520we%2520analyze%2520the%2520International%2520Wealth%2520Index%2520%2528IWI%2529%2520predicted%2520by%2520Lee%2520and%250ABraithwaite%2520%25282022%2529%2520and%2520Esp%255C%2527in-Noboa%2520et%2520al.%2520%25282023%2529%252C%2520alongside%2520the%2520Relative%250AWealth%2520Index%2520%2528RWI%2529%2520inferred%2520by%2520Chi%2520et%2520al.%2520%25282022%2529%252C%2520across%2520six%2520Sub-Saharan%250AAfrican%2520countries.%2520Our%2520analysis%2520reveals%2520trends%2520and%2520discrepancies%2520in%2520wealth%250Apredictions%2520between%2520these%2520models.%2520In%2520particular%252C%2520significant%2520and%2520unexpected%250Adiscrepancies%2520between%2520the%2520predictions%2520of%2520Lee%2520and%2520Braithwaite%2520and%2520Esp%255C%2527in-Noboa%250Aet%2520al.%252C%2520even%2520after%2520accounting%2520for%2520differences%2520in%2520training%2520data.%2520In%2520contrast%252C%250Athe%2520shape%2520of%2520the%2520wealth%2520distributions%2520predicted%2520by%2520Esp%255C%2527in-Noboa%2520et%2520al.%2520and%2520Chi%250Aet%2520al.%2520are%2520more%2520closely%2520aligned%252C%2520suggesting%2520similar%2520levels%2520of%2520skewness.%2520These%250Afindings%2520raise%2520concerns%2520about%2520the%2520validity%2520of%2520certain%2520models%2520and%2520emphasize%2520the%250Aimportance%2520of%2520rigorous%2520audits%2520for%2520wealth%2520prediction%2520algorithms%2520used%2520in%250Apolicy-making.%2520Continuous%2520validation%2520and%2520refinement%2520are%2520essential%2520to%2520ensure%2520the%250Areliability%2520of%2520these%2520models%252C%2520particularly%2520when%2520they%2520inform%2520poverty%2520alleviation%250Astrategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01631v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comparative%20Analysis%20of%20Wealth%20Index%20Predictions%20in%20Africa%20between%0A%20%20three%20Multi-Source%20Inference%20Models&entry.906535625=M%C3%A1rton%20Karsai%20and%20J%C3%A1nos%20Kert%C3%A9sz%20and%20Lisette%20Esp%C3%ADn-Noboa&entry.1292438233=%20%20Poverty%20map%20inference%20has%20become%20a%20critical%20focus%20of%20research%2C%20utilizing%20both%0Atraditional%20and%20modern%20techniques%2C%20ranging%20from%20regression%20models%20to%0Aconvolutional%20neural%20networks%20applied%20to%20tabular%20data%2C%20satellite%20imagery%2C%20and%0Anetworks.%20While%20much%20attention%20has%20been%20given%20to%20validating%20models%20during%20the%0Atraining%20phase%2C%20the%20final%20predictions%20have%20received%20less%20scrutiny.%20In%20this%0Astudy%2C%20we%20analyze%20the%20International%20Wealth%20Index%20%28IWI%29%20predicted%20by%20Lee%20and%0ABraithwaite%20%282022%29%20and%20Esp%5C%27in-Noboa%20et%20al.%20%282023%29%2C%20alongside%20the%20Relative%0AWealth%20Index%20%28RWI%29%20inferred%20by%20Chi%20et%20al.%20%282022%29%2C%20across%20six%20Sub-Saharan%0AAfrican%20countries.%20Our%20analysis%20reveals%20trends%20and%20discrepancies%20in%20wealth%0Apredictions%20between%20these%20models.%20In%20particular%2C%20significant%20and%20unexpected%0Adiscrepancies%20between%20the%20predictions%20of%20Lee%20and%20Braithwaite%20and%20Esp%5C%27in-Noboa%0Aet%20al.%2C%20even%20after%20accounting%20for%20differences%20in%20training%20data.%20In%20contrast%2C%0Athe%20shape%20of%20the%20wealth%20distributions%20predicted%20by%20Esp%5C%27in-Noboa%20et%20al.%20and%20Chi%0Aet%20al.%20are%20more%20closely%20aligned%2C%20suggesting%20similar%20levels%20of%20skewness.%20These%0Afindings%20raise%20concerns%20about%20the%20validity%20of%20certain%20models%20and%20emphasize%20the%0Aimportance%20of%20rigorous%20audits%20for%20wealth%20prediction%20algorithms%20used%20in%0Apolicy-making.%20Continuous%20validation%20and%20refinement%20are%20essential%20to%20ensure%20the%0Areliability%20of%20these%20models%2C%20particularly%20when%20they%20inform%20poverty%20alleviation%0Astrategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01631v3&entry.124074799=Read"},
{"title": "BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep\n  Neural Network Inference", "author": "Changwoo Lee and Soo Min Kwon and Qing Qu and Hun-Seok Kim", "abstract": "  Large-scale foundation models have demonstrated exceptional performance in\nlanguage and vision tasks. However, the numerous dense matrix-vector operations\ninvolved in these large networks pose significant computational challenges\nduring inference. To address these challenges, we introduce the Block-Level\nAdaptive STructured (BLAST) matrix, designed to learn and leverage efficient\nstructures prevalent in the weight matrices of linear layers within deep\nlearning models. Compared to existing structured matrices, the BLAST matrix\noffers substantial flexibility, as it can represent various types of structures\nthat are either learned from data or computed from pre-existing weight\nmatrices. We demonstrate the efficiency of using the BLAST matrix for\ncompressing both language and vision tasks, showing that (i) for medium-sized\nmodels such as ViT and GPT-2, training with BLAST weights boosts performance\nwhile reducing complexity by 70\\% and 40\\%, respectively; and (ii) for large\nfoundation models such as Llama-7B and DiT-XL, the BLAST matrix achieves a 2x\ncompression while exhibiting the lowest performance degradation among all\ntested structured matrices. Our code is available at\n\\url{https://github.com/changwoolee/BLAST}.\n", "link": "http://arxiv.org/abs/2410.21262v1", "date": "2024-10-28", "relevancy": 1.6082, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5448}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5376}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLAST%3A%20Block-Level%20Adaptive%20Structured%20Matrices%20for%20Efficient%20Deep%0A%20%20Neural%20Network%20Inference&body=Title%3A%20BLAST%3A%20Block-Level%20Adaptive%20Structured%20Matrices%20for%20Efficient%20Deep%0A%20%20Neural%20Network%20Inference%0AAuthor%3A%20Changwoo%20Lee%20and%20Soo%20Min%20Kwon%20and%20Qing%20Qu%20and%20Hun-Seok%20Kim%0AAbstract%3A%20%20%20Large-scale%20foundation%20models%20have%20demonstrated%20exceptional%20performance%20in%0Alanguage%20and%20vision%20tasks.%20However%2C%20the%20numerous%20dense%20matrix-vector%20operations%0Ainvolved%20in%20these%20large%20networks%20pose%20significant%20computational%20challenges%0Aduring%20inference.%20To%20address%20these%20challenges%2C%20we%20introduce%20the%20Block-Level%0AAdaptive%20STructured%20%28BLAST%29%20matrix%2C%20designed%20to%20learn%20and%20leverage%20efficient%0Astructures%20prevalent%20in%20the%20weight%20matrices%20of%20linear%20layers%20within%20deep%0Alearning%20models.%20Compared%20to%20existing%20structured%20matrices%2C%20the%20BLAST%20matrix%0Aoffers%20substantial%20flexibility%2C%20as%20it%20can%20represent%20various%20types%20of%20structures%0Athat%20are%20either%20learned%20from%20data%20or%20computed%20from%20pre-existing%20weight%0Amatrices.%20We%20demonstrate%20the%20efficiency%20of%20using%20the%20BLAST%20matrix%20for%0Acompressing%20both%20language%20and%20vision%20tasks%2C%20showing%20that%20%28i%29%20for%20medium-sized%0Amodels%20such%20as%20ViT%20and%20GPT-2%2C%20training%20with%20BLAST%20weights%20boosts%20performance%0Awhile%20reducing%20complexity%20by%2070%5C%25%20and%2040%5C%25%2C%20respectively%3B%20and%20%28ii%29%20for%20large%0Afoundation%20models%20such%20as%20Llama-7B%20and%20DiT-XL%2C%20the%20BLAST%20matrix%20achieves%20a%202x%0Acompression%20while%20exhibiting%20the%20lowest%20performance%20degradation%20among%20all%0Atested%20structured%20matrices.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/changwoolee/BLAST%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLAST%253A%2520Block-Level%2520Adaptive%2520Structured%2520Matrices%2520for%2520Efficient%2520Deep%250A%2520%2520Neural%2520Network%2520Inference%26entry.906535625%3DChangwoo%2520Lee%2520and%2520Soo%2520Min%2520Kwon%2520and%2520Qing%2520Qu%2520and%2520Hun-Seok%2520Kim%26entry.1292438233%3D%2520%2520Large-scale%2520foundation%2520models%2520have%2520demonstrated%2520exceptional%2520performance%2520in%250Alanguage%2520and%2520vision%2520tasks.%2520However%252C%2520the%2520numerous%2520dense%2520matrix-vector%2520operations%250Ainvolved%2520in%2520these%2520large%2520networks%2520pose%2520significant%2520computational%2520challenges%250Aduring%2520inference.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520Block-Level%250AAdaptive%2520STructured%2520%2528BLAST%2529%2520matrix%252C%2520designed%2520to%2520learn%2520and%2520leverage%2520efficient%250Astructures%2520prevalent%2520in%2520the%2520weight%2520matrices%2520of%2520linear%2520layers%2520within%2520deep%250Alearning%2520models.%2520Compared%2520to%2520existing%2520structured%2520matrices%252C%2520the%2520BLAST%2520matrix%250Aoffers%2520substantial%2520flexibility%252C%2520as%2520it%2520can%2520represent%2520various%2520types%2520of%2520structures%250Athat%2520are%2520either%2520learned%2520from%2520data%2520or%2520computed%2520from%2520pre-existing%2520weight%250Amatrices.%2520We%2520demonstrate%2520the%2520efficiency%2520of%2520using%2520the%2520BLAST%2520matrix%2520for%250Acompressing%2520both%2520language%2520and%2520vision%2520tasks%252C%2520showing%2520that%2520%2528i%2529%2520for%2520medium-sized%250Amodels%2520such%2520as%2520ViT%2520and%2520GPT-2%252C%2520training%2520with%2520BLAST%2520weights%2520boosts%2520performance%250Awhile%2520reducing%2520complexity%2520by%252070%255C%2525%2520and%252040%255C%2525%252C%2520respectively%253B%2520and%2520%2528ii%2529%2520for%2520large%250Afoundation%2520models%2520such%2520as%2520Llama-7B%2520and%2520DiT-XL%252C%2520the%2520BLAST%2520matrix%2520achieves%2520a%25202x%250Acompression%2520while%2520exhibiting%2520the%2520lowest%2520performance%2520degradation%2520among%2520all%250Atested%2520structured%2520matrices.%2520Our%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/changwoolee/BLAST%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLAST%3A%20Block-Level%20Adaptive%20Structured%20Matrices%20for%20Efficient%20Deep%0A%20%20Neural%20Network%20Inference&entry.906535625=Changwoo%20Lee%20and%20Soo%20Min%20Kwon%20and%20Qing%20Qu%20and%20Hun-Seok%20Kim&entry.1292438233=%20%20Large-scale%20foundation%20models%20have%20demonstrated%20exceptional%20performance%20in%0Alanguage%20and%20vision%20tasks.%20However%2C%20the%20numerous%20dense%20matrix-vector%20operations%0Ainvolved%20in%20these%20large%20networks%20pose%20significant%20computational%20challenges%0Aduring%20inference.%20To%20address%20these%20challenges%2C%20we%20introduce%20the%20Block-Level%0AAdaptive%20STructured%20%28BLAST%29%20matrix%2C%20designed%20to%20learn%20and%20leverage%20efficient%0Astructures%20prevalent%20in%20the%20weight%20matrices%20of%20linear%20layers%20within%20deep%0Alearning%20models.%20Compared%20to%20existing%20structured%20matrices%2C%20the%20BLAST%20matrix%0Aoffers%20substantial%20flexibility%2C%20as%20it%20can%20represent%20various%20types%20of%20structures%0Athat%20are%20either%20learned%20from%20data%20or%20computed%20from%20pre-existing%20weight%0Amatrices.%20We%20demonstrate%20the%20efficiency%20of%20using%20the%20BLAST%20matrix%20for%0Acompressing%20both%20language%20and%20vision%20tasks%2C%20showing%20that%20%28i%29%20for%20medium-sized%0Amodels%20such%20as%20ViT%20and%20GPT-2%2C%20training%20with%20BLAST%20weights%20boosts%20performance%0Awhile%20reducing%20complexity%20by%2070%5C%25%20and%2040%5C%25%2C%20respectively%3B%20and%20%28ii%29%20for%20large%0Afoundation%20models%20such%20as%20Llama-7B%20and%20DiT-XL%2C%20the%20BLAST%20matrix%20achieves%20a%202x%0Acompression%20while%20exhibiting%20the%20lowest%20performance%20degradation%20among%20all%0Atested%20structured%20matrices.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/changwoolee/BLAST%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21262v1&entry.124074799=Read"},
{"title": "Better Instruction-Following Through Minimum Bayes Risk", "author": "Ian Wu and Patrick Fernandes and Amanda Bertsch and Seungone Kim and Sina Pakazad and Graham Neubig", "abstract": "  General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding.\n", "link": "http://arxiv.org/abs/2410.02902v3", "date": "2024-10-28", "relevancy": 1.5255, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.516}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20Instruction-Following%20Through%20Minimum%20Bayes%20Risk&body=Title%3A%20Better%20Instruction-Following%20Through%20Minimum%20Bayes%20Risk%0AAuthor%3A%20Ian%20Wu%20and%20Patrick%20Fernandes%20and%20Amanda%20Bertsch%20and%20Seungone%20Kim%20and%20Sina%20Pakazad%20and%20Graham%20Neubig%0AAbstract%3A%20%20%20General-purpose%20LLM%20judges%20capable%20of%20human-level%20evaluation%20provide%20not%20only%0Aa%20scalable%20and%20accurate%20way%20of%20evaluating%20instruction-following%20LLMs%20but%20also%0Anew%20avenues%20for%20supervising%20and%20improving%20their%20performance.%20One%20promising%20way%0Aof%20leveraging%20LLM%20judges%20for%20supervision%20is%20through%20Minimum%20Bayes%20Risk%20%28MBR%29%0Adecoding%2C%20which%20uses%20a%20reference-based%20evaluator%20to%20select%20a%20high-quality%0Aoutput%20from%20amongst%20a%20set%20of%20candidate%20outputs.%20In%20the%20first%20part%20of%20this%20work%2C%0Awe%20explore%20using%20MBR%20decoding%20as%20a%20method%20for%20improving%20the%20test-time%0Aperformance%20of%20instruction-following%20LLMs.%20We%20find%20that%20MBR%20decoding%20with%0Areference-based%20LLM%20judges%20substantially%20improves%20over%20greedy%20decoding%2C%0Abest-of-N%20decoding%20with%20reference-free%20judges%20and%20MBR%20decoding%20with%20lexical%20and%0Aembedding-based%20metrics%20on%20AlpacaEval%20and%20MT-Bench.%20These%20gains%20are%20consistent%0Aacross%20LLMs%20with%20up%20to%2070B%20parameters%2C%20demonstrating%20that%20smaller%20LLM%20judges%0Acan%20be%20used%20to%20supervise%20much%20larger%20LLMs.%20Then%2C%20seeking%20to%20retain%20the%0Aimprovements%20from%20MBR%20decoding%20while%20mitigating%20additional%20test-time%20costs%2C%20we%0Aexplore%20iterative%20self-training%20on%20MBR-decoded%20outputs.%20We%20find%20that%0Aself-training%20using%20Direct%20Preference%20Optimisation%20leads%20to%20significant%0Aperformance%20gains%2C%20such%20that%20the%20self-trained%20models%20with%20greedy%20decoding%0Agenerally%20match%20and%20sometimes%20exceed%20the%20performance%20of%20their%20base%20models%20with%0AMBR%20decoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02902v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520Instruction-Following%2520Through%2520Minimum%2520Bayes%2520Risk%26entry.906535625%3DIan%2520Wu%2520and%2520Patrick%2520Fernandes%2520and%2520Amanda%2520Bertsch%2520and%2520Seungone%2520Kim%2520and%2520Sina%2520Pakazad%2520and%2520Graham%2520Neubig%26entry.1292438233%3D%2520%2520General-purpose%2520LLM%2520judges%2520capable%2520of%2520human-level%2520evaluation%2520provide%2520not%2520only%250Aa%2520scalable%2520and%2520accurate%2520way%2520of%2520evaluating%2520instruction-following%2520LLMs%2520but%2520also%250Anew%2520avenues%2520for%2520supervising%2520and%2520improving%2520their%2520performance.%2520One%2520promising%2520way%250Aof%2520leveraging%2520LLM%2520judges%2520for%2520supervision%2520is%2520through%2520Minimum%2520Bayes%2520Risk%2520%2528MBR%2529%250Adecoding%252C%2520which%2520uses%2520a%2520reference-based%2520evaluator%2520to%2520select%2520a%2520high-quality%250Aoutput%2520from%2520amongst%2520a%2520set%2520of%2520candidate%2520outputs.%2520In%2520the%2520first%2520part%2520of%2520this%2520work%252C%250Awe%2520explore%2520using%2520MBR%2520decoding%2520as%2520a%2520method%2520for%2520improving%2520the%2520test-time%250Aperformance%2520of%2520instruction-following%2520LLMs.%2520We%2520find%2520that%2520MBR%2520decoding%2520with%250Areference-based%2520LLM%2520judges%2520substantially%2520improves%2520over%2520greedy%2520decoding%252C%250Abest-of-N%2520decoding%2520with%2520reference-free%2520judges%2520and%2520MBR%2520decoding%2520with%2520lexical%2520and%250Aembedding-based%2520metrics%2520on%2520AlpacaEval%2520and%2520MT-Bench.%2520These%2520gains%2520are%2520consistent%250Aacross%2520LLMs%2520with%2520up%2520to%252070B%2520parameters%252C%2520demonstrating%2520that%2520smaller%2520LLM%2520judges%250Acan%2520be%2520used%2520to%2520supervise%2520much%2520larger%2520LLMs.%2520Then%252C%2520seeking%2520to%2520retain%2520the%250Aimprovements%2520from%2520MBR%2520decoding%2520while%2520mitigating%2520additional%2520test-time%2520costs%252C%2520we%250Aexplore%2520iterative%2520self-training%2520on%2520MBR-decoded%2520outputs.%2520We%2520find%2520that%250Aself-training%2520using%2520Direct%2520Preference%2520Optimisation%2520leads%2520to%2520significant%250Aperformance%2520gains%252C%2520such%2520that%2520the%2520self-trained%2520models%2520with%2520greedy%2520decoding%250Agenerally%2520match%2520and%2520sometimes%2520exceed%2520the%2520performance%2520of%2520their%2520base%2520models%2520with%250AMBR%2520decoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02902v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20Instruction-Following%20Through%20Minimum%20Bayes%20Risk&entry.906535625=Ian%20Wu%20and%20Patrick%20Fernandes%20and%20Amanda%20Bertsch%20and%20Seungone%20Kim%20and%20Sina%20Pakazad%20and%20Graham%20Neubig&entry.1292438233=%20%20General-purpose%20LLM%20judges%20capable%20of%20human-level%20evaluation%20provide%20not%20only%0Aa%20scalable%20and%20accurate%20way%20of%20evaluating%20instruction-following%20LLMs%20but%20also%0Anew%20avenues%20for%20supervising%20and%20improving%20their%20performance.%20One%20promising%20way%0Aof%20leveraging%20LLM%20judges%20for%20supervision%20is%20through%20Minimum%20Bayes%20Risk%20%28MBR%29%0Adecoding%2C%20which%20uses%20a%20reference-based%20evaluator%20to%20select%20a%20high-quality%0Aoutput%20from%20amongst%20a%20set%20of%20candidate%20outputs.%20In%20the%20first%20part%20of%20this%20work%2C%0Awe%20explore%20using%20MBR%20decoding%20as%20a%20method%20for%20improving%20the%20test-time%0Aperformance%20of%20instruction-following%20LLMs.%20We%20find%20that%20MBR%20decoding%20with%0Areference-based%20LLM%20judges%20substantially%20improves%20over%20greedy%20decoding%2C%0Abest-of-N%20decoding%20with%20reference-free%20judges%20and%20MBR%20decoding%20with%20lexical%20and%0Aembedding-based%20metrics%20on%20AlpacaEval%20and%20MT-Bench.%20These%20gains%20are%20consistent%0Aacross%20LLMs%20with%20up%20to%2070B%20parameters%2C%20demonstrating%20that%20smaller%20LLM%20judges%0Acan%20be%20used%20to%20supervise%20much%20larger%20LLMs.%20Then%2C%20seeking%20to%20retain%20the%0Aimprovements%20from%20MBR%20decoding%20while%20mitigating%20additional%20test-time%20costs%2C%20we%0Aexplore%20iterative%20self-training%20on%20MBR-decoded%20outputs.%20We%20find%20that%0Aself-training%20using%20Direct%20Preference%20Optimisation%20leads%20to%20significant%0Aperformance%20gains%2C%20such%20that%20the%20self-trained%20models%20with%20greedy%20decoding%0Agenerally%20match%20and%20sometimes%20exceed%20the%20performance%20of%20their%20base%20models%20with%0AMBR%20decoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02902v3&entry.124074799=Read"},
{"title": "Quantum computing and persistence in topological data analysis", "author": "Casper Gyurik and Alexander Schmidhuber and Robbie King and Vedran Dunjko and Ryu Hayakawa", "abstract": "  Topological data analysis (TDA) aims to extract noise-robust features from a\ndata set by examining the number and persistence of holes in its topology. We\nshow that a computational problem closely related to a core task in TDA --\ndetermining whether a given hole persists across different length scales -- is\n$\\mathsf{BQP}_1$-hard and contained in $\\mathsf{BQP}$. This result implies an\nexponential quantum speedup for this problem under standard\ncomplexity-theoretic assumptions. Our approach relies on encoding the\npersistence of a hole in a variant of the guided sparse Hamiltonian problem,\nwhere the guiding state is constructed from a harmonic representative of the\nhole.\n", "link": "http://arxiv.org/abs/2410.21258v1", "date": "2024-10-28", "relevancy": 1.4851, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3865}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3665}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20computing%20and%20persistence%20in%20topological%20data%20analysis&body=Title%3A%20Quantum%20computing%20and%20persistence%20in%20topological%20data%20analysis%0AAuthor%3A%20Casper%20Gyurik%20and%20Alexander%20Schmidhuber%20and%20Robbie%20King%20and%20Vedran%20Dunjko%20and%20Ryu%20Hayakawa%0AAbstract%3A%20%20%20Topological%20data%20analysis%20%28TDA%29%20aims%20to%20extract%20noise-robust%20features%20from%20a%0Adata%20set%20by%20examining%20the%20number%20and%20persistence%20of%20holes%20in%20its%20topology.%20We%0Ashow%20that%20a%20computational%20problem%20closely%20related%20to%20a%20core%20task%20in%20TDA%20--%0Adetermining%20whether%20a%20given%20hole%20persists%20across%20different%20length%20scales%20--%20is%0A%24%5Cmathsf%7BBQP%7D_1%24-hard%20and%20contained%20in%20%24%5Cmathsf%7BBQP%7D%24.%20This%20result%20implies%20an%0Aexponential%20quantum%20speedup%20for%20this%20problem%20under%20standard%0Acomplexity-theoretic%20assumptions.%20Our%20approach%20relies%20on%20encoding%20the%0Apersistence%20of%20a%20hole%20in%20a%20variant%20of%20the%20guided%20sparse%20Hamiltonian%20problem%2C%0Awhere%20the%20guiding%20state%20is%20constructed%20from%20a%20harmonic%20representative%20of%20the%0Ahole.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520computing%2520and%2520persistence%2520in%2520topological%2520data%2520analysis%26entry.906535625%3DCasper%2520Gyurik%2520and%2520Alexander%2520Schmidhuber%2520and%2520Robbie%2520King%2520and%2520Vedran%2520Dunjko%2520and%2520Ryu%2520Hayakawa%26entry.1292438233%3D%2520%2520Topological%2520data%2520analysis%2520%2528TDA%2529%2520aims%2520to%2520extract%2520noise-robust%2520features%2520from%2520a%250Adata%2520set%2520by%2520examining%2520the%2520number%2520and%2520persistence%2520of%2520holes%2520in%2520its%2520topology.%2520We%250Ashow%2520that%2520a%2520computational%2520problem%2520closely%2520related%2520to%2520a%2520core%2520task%2520in%2520TDA%2520--%250Adetermining%2520whether%2520a%2520given%2520hole%2520persists%2520across%2520different%2520length%2520scales%2520--%2520is%250A%2524%255Cmathsf%257BBQP%257D_1%2524-hard%2520and%2520contained%2520in%2520%2524%255Cmathsf%257BBQP%257D%2524.%2520This%2520result%2520implies%2520an%250Aexponential%2520quantum%2520speedup%2520for%2520this%2520problem%2520under%2520standard%250Acomplexity-theoretic%2520assumptions.%2520Our%2520approach%2520relies%2520on%2520encoding%2520the%250Apersistence%2520of%2520a%2520hole%2520in%2520a%2520variant%2520of%2520the%2520guided%2520sparse%2520Hamiltonian%2520problem%252C%250Awhere%2520the%2520guiding%2520state%2520is%2520constructed%2520from%2520a%2520harmonic%2520representative%2520of%2520the%250Ahole.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20computing%20and%20persistence%20in%20topological%20data%20analysis&entry.906535625=Casper%20Gyurik%20and%20Alexander%20Schmidhuber%20and%20Robbie%20King%20and%20Vedran%20Dunjko%20and%20Ryu%20Hayakawa&entry.1292438233=%20%20Topological%20data%20analysis%20%28TDA%29%20aims%20to%20extract%20noise-robust%20features%20from%20a%0Adata%20set%20by%20examining%20the%20number%20and%20persistence%20of%20holes%20in%20its%20topology.%20We%0Ashow%20that%20a%20computational%20problem%20closely%20related%20to%20a%20core%20task%20in%20TDA%20--%0Adetermining%20whether%20a%20given%20hole%20persists%20across%20different%20length%20scales%20--%20is%0A%24%5Cmathsf%7BBQP%7D_1%24-hard%20and%20contained%20in%20%24%5Cmathsf%7BBQP%7D%24.%20This%20result%20implies%20an%0Aexponential%20quantum%20speedup%20for%20this%20problem%20under%20standard%0Acomplexity-theoretic%20assumptions.%20Our%20approach%20relies%20on%20encoding%20the%0Apersistence%20of%20a%20hole%20in%20a%20variant%20of%20the%20guided%20sparse%20Hamiltonian%20problem%2C%0Awhere%20the%20guiding%20state%20is%20constructed%20from%20a%20harmonic%20representative%20of%20the%0Ahole.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21258v1&entry.124074799=Read"},
{"title": "Bayesian-LoRA: LoRA based Parameter Efficient Fine-Tuning using Optimal\n  Quantization levels and Rank Values trough Differentiable Bayesian Gates", "author": "Cristian Meo and Ksenia Sycheva and Anirudh Goyal and Justin Dauwels", "abstract": "  It is a common practice in natural language processing to pre-train a single\nmodel on a general domain and then fine-tune it for downstream tasks. However,\nwhen it comes to Large Language Models, fine-tuning the entire model can be\ncomputationally expensive, resulting in very intensive energy consumption. As a\nresult, several Parameter Efficient Fine-Tuning (PEFT) approaches were recently\nproposed. One of the most popular approaches is low-rank adaptation (LoRA),\nwhere the key insight is decomposing the update weights of the pre-trained\nmodel into two low-rank matrices. However, the proposed approaches either use\nthe same rank value across all different weight matrices, which has been shown\nto be a sub-optimal choice, or do not use any quantization technique, one of\nthe most important factors when it comes to a model's energy consumption. In\nthis work, we propose Bayesian-LoRA which approaches low-rank adaptation and\nquantization from a Bayesian perspective by employing a prior distribution on\nboth quantization levels and rank values. As a result, B-LoRA is able to\nfine-tune a pre-trained model on a specific downstream task, finding the\noptimal rank values and quantization levels for every low-rank matrix. We\nvalidate the proposed model by fine-tuning a pre-trained DeBERTaV3 on the GLUE\nbenchmark. Moreover, we compare it to relevant baselines and present both\nqualitative and quantitative results, showing how the proposed approach is able\nto learn optimal-rank quantized matrices. B-LoRA performs on par with or better\nthan the baselines while reducing the total number of bit operations by roughly\n70% compared to the baseline methods.\n", "link": "http://arxiv.org/abs/2406.13046v3", "date": "2024-10-28", "relevancy": 1.4767, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4961}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4925}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian-LoRA%3A%20LoRA%20based%20Parameter%20Efficient%20Fine-Tuning%20using%20Optimal%0A%20%20Quantization%20levels%20and%20Rank%20Values%20trough%20Differentiable%20Bayesian%20Gates&body=Title%3A%20Bayesian-LoRA%3A%20LoRA%20based%20Parameter%20Efficient%20Fine-Tuning%20using%20Optimal%0A%20%20Quantization%20levels%20and%20Rank%20Values%20trough%20Differentiable%20Bayesian%20Gates%0AAuthor%3A%20Cristian%20Meo%20and%20Ksenia%20Sycheva%20and%20Anirudh%20Goyal%20and%20Justin%20Dauwels%0AAbstract%3A%20%20%20It%20is%20a%20common%20practice%20in%20natural%20language%20processing%20to%20pre-train%20a%20single%0Amodel%20on%20a%20general%20domain%20and%20then%20fine-tune%20it%20for%20downstream%20tasks.%20However%2C%0Awhen%20it%20comes%20to%20Large%20Language%20Models%2C%20fine-tuning%20the%20entire%20model%20can%20be%0Acomputationally%20expensive%2C%20resulting%20in%20very%20intensive%20energy%20consumption.%20As%20a%0Aresult%2C%20several%20Parameter%20Efficient%20Fine-Tuning%20%28PEFT%29%20approaches%20were%20recently%0Aproposed.%20One%20of%20the%20most%20popular%20approaches%20is%20low-rank%20adaptation%20%28LoRA%29%2C%0Awhere%20the%20key%20insight%20is%20decomposing%20the%20update%20weights%20of%20the%20pre-trained%0Amodel%20into%20two%20low-rank%20matrices.%20However%2C%20the%20proposed%20approaches%20either%20use%0Athe%20same%20rank%20value%20across%20all%20different%20weight%20matrices%2C%20which%20has%20been%20shown%0Ato%20be%20a%20sub-optimal%20choice%2C%20or%20do%20not%20use%20any%20quantization%20technique%2C%20one%20of%0Athe%20most%20important%20factors%20when%20it%20comes%20to%20a%20model%27s%20energy%20consumption.%20In%0Athis%20work%2C%20we%20propose%20Bayesian-LoRA%20which%20approaches%20low-rank%20adaptation%20and%0Aquantization%20from%20a%20Bayesian%20perspective%20by%20employing%20a%20prior%20distribution%20on%0Aboth%20quantization%20levels%20and%20rank%20values.%20As%20a%20result%2C%20B-LoRA%20is%20able%20to%0Afine-tune%20a%20pre-trained%20model%20on%20a%20specific%20downstream%20task%2C%20finding%20the%0Aoptimal%20rank%20values%20and%20quantization%20levels%20for%20every%20low-rank%20matrix.%20We%0Avalidate%20the%20proposed%20model%20by%20fine-tuning%20a%20pre-trained%20DeBERTaV3%20on%20the%20GLUE%0Abenchmark.%20Moreover%2C%20we%20compare%20it%20to%20relevant%20baselines%20and%20present%20both%0Aqualitative%20and%20quantitative%20results%2C%20showing%20how%20the%20proposed%20approach%20is%20able%0Ato%20learn%20optimal-rank%20quantized%20matrices.%20B-LoRA%20performs%20on%20par%20with%20or%20better%0Athan%20the%20baselines%20while%20reducing%20the%20total%20number%20of%20bit%20operations%20by%20roughly%0A70%25%20compared%20to%20the%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13046v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian-LoRA%253A%2520LoRA%2520based%2520Parameter%2520Efficient%2520Fine-Tuning%2520using%2520Optimal%250A%2520%2520Quantization%2520levels%2520and%2520Rank%2520Values%2520trough%2520Differentiable%2520Bayesian%2520Gates%26entry.906535625%3DCristian%2520Meo%2520and%2520Ksenia%2520Sycheva%2520and%2520Anirudh%2520Goyal%2520and%2520Justin%2520Dauwels%26entry.1292438233%3D%2520%2520It%2520is%2520a%2520common%2520practice%2520in%2520natural%2520language%2520processing%2520to%2520pre-train%2520a%2520single%250Amodel%2520on%2520a%2520general%2520domain%2520and%2520then%2520fine-tune%2520it%2520for%2520downstream%2520tasks.%2520However%252C%250Awhen%2520it%2520comes%2520to%2520Large%2520Language%2520Models%252C%2520fine-tuning%2520the%2520entire%2520model%2520can%2520be%250Acomputationally%2520expensive%252C%2520resulting%2520in%2520very%2520intensive%2520energy%2520consumption.%2520As%2520a%250Aresult%252C%2520several%2520Parameter%2520Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520approaches%2520were%2520recently%250Aproposed.%2520One%2520of%2520the%2520most%2520popular%2520approaches%2520is%2520low-rank%2520adaptation%2520%2528LoRA%2529%252C%250Awhere%2520the%2520key%2520insight%2520is%2520decomposing%2520the%2520update%2520weights%2520of%2520the%2520pre-trained%250Amodel%2520into%2520two%2520low-rank%2520matrices.%2520However%252C%2520the%2520proposed%2520approaches%2520either%2520use%250Athe%2520same%2520rank%2520value%2520across%2520all%2520different%2520weight%2520matrices%252C%2520which%2520has%2520been%2520shown%250Ato%2520be%2520a%2520sub-optimal%2520choice%252C%2520or%2520do%2520not%2520use%2520any%2520quantization%2520technique%252C%2520one%2520of%250Athe%2520most%2520important%2520factors%2520when%2520it%2520comes%2520to%2520a%2520model%2527s%2520energy%2520consumption.%2520In%250Athis%2520work%252C%2520we%2520propose%2520Bayesian-LoRA%2520which%2520approaches%2520low-rank%2520adaptation%2520and%250Aquantization%2520from%2520a%2520Bayesian%2520perspective%2520by%2520employing%2520a%2520prior%2520distribution%2520on%250Aboth%2520quantization%2520levels%2520and%2520rank%2520values.%2520As%2520a%2520result%252C%2520B-LoRA%2520is%2520able%2520to%250Afine-tune%2520a%2520pre-trained%2520model%2520on%2520a%2520specific%2520downstream%2520task%252C%2520finding%2520the%250Aoptimal%2520rank%2520values%2520and%2520quantization%2520levels%2520for%2520every%2520low-rank%2520matrix.%2520We%250Avalidate%2520the%2520proposed%2520model%2520by%2520fine-tuning%2520a%2520pre-trained%2520DeBERTaV3%2520on%2520the%2520GLUE%250Abenchmark.%2520Moreover%252C%2520we%2520compare%2520it%2520to%2520relevant%2520baselines%2520and%2520present%2520both%250Aqualitative%2520and%2520quantitative%2520results%252C%2520showing%2520how%2520the%2520proposed%2520approach%2520is%2520able%250Ato%2520learn%2520optimal-rank%2520quantized%2520matrices.%2520B-LoRA%2520performs%2520on%2520par%2520with%2520or%2520better%250Athan%2520the%2520baselines%2520while%2520reducing%2520the%2520total%2520number%2520of%2520bit%2520operations%2520by%2520roughly%250A70%2525%2520compared%2520to%2520the%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13046v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian-LoRA%3A%20LoRA%20based%20Parameter%20Efficient%20Fine-Tuning%20using%20Optimal%0A%20%20Quantization%20levels%20and%20Rank%20Values%20trough%20Differentiable%20Bayesian%20Gates&entry.906535625=Cristian%20Meo%20and%20Ksenia%20Sycheva%20and%20Anirudh%20Goyal%20and%20Justin%20Dauwels&entry.1292438233=%20%20It%20is%20a%20common%20practice%20in%20natural%20language%20processing%20to%20pre-train%20a%20single%0Amodel%20on%20a%20general%20domain%20and%20then%20fine-tune%20it%20for%20downstream%20tasks.%20However%2C%0Awhen%20it%20comes%20to%20Large%20Language%20Models%2C%20fine-tuning%20the%20entire%20model%20can%20be%0Acomputationally%20expensive%2C%20resulting%20in%20very%20intensive%20energy%20consumption.%20As%20a%0Aresult%2C%20several%20Parameter%20Efficient%20Fine-Tuning%20%28PEFT%29%20approaches%20were%20recently%0Aproposed.%20One%20of%20the%20most%20popular%20approaches%20is%20low-rank%20adaptation%20%28LoRA%29%2C%0Awhere%20the%20key%20insight%20is%20decomposing%20the%20update%20weights%20of%20the%20pre-trained%0Amodel%20into%20two%20low-rank%20matrices.%20However%2C%20the%20proposed%20approaches%20either%20use%0Athe%20same%20rank%20value%20across%20all%20different%20weight%20matrices%2C%20which%20has%20been%20shown%0Ato%20be%20a%20sub-optimal%20choice%2C%20or%20do%20not%20use%20any%20quantization%20technique%2C%20one%20of%0Athe%20most%20important%20factors%20when%20it%20comes%20to%20a%20model%27s%20energy%20consumption.%20In%0Athis%20work%2C%20we%20propose%20Bayesian-LoRA%20which%20approaches%20low-rank%20adaptation%20and%0Aquantization%20from%20a%20Bayesian%20perspective%20by%20employing%20a%20prior%20distribution%20on%0Aboth%20quantization%20levels%20and%20rank%20values.%20As%20a%20result%2C%20B-LoRA%20is%20able%20to%0Afine-tune%20a%20pre-trained%20model%20on%20a%20specific%20downstream%20task%2C%20finding%20the%0Aoptimal%20rank%20values%20and%20quantization%20levels%20for%20every%20low-rank%20matrix.%20We%0Avalidate%20the%20proposed%20model%20by%20fine-tuning%20a%20pre-trained%20DeBERTaV3%20on%20the%20GLUE%0Abenchmark.%20Moreover%2C%20we%20compare%20it%20to%20relevant%20baselines%20and%20present%20both%0Aqualitative%20and%20quantitative%20results%2C%20showing%20how%20the%20proposed%20approach%20is%20able%0Ato%20learn%20optimal-rank%20quantized%20matrices.%20B-LoRA%20performs%20on%20par%20with%20or%20better%0Athan%20the%20baselines%20while%20reducing%20the%20total%20number%20of%20bit%20operations%20by%20roughly%0A70%25%20compared%20to%20the%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13046v3&entry.124074799=Read"},
{"title": "Reconstructing dynamics from sparse observations with no training on\n  target system", "author": "Zheng-Meng Zhai and Jun-Yin Huang and Benjamin D. Stern and Ying-Cheng Lai", "abstract": "  In applications, an anticipated situation is where the system of interest has\nnever been encountered before and sparse observations can be made only once.\nCan the dynamics be faithfully reconstructed from the limited observations\nwithout any training data? This problem defies any known traditional methods of\nnonlinear time-series analysis as well as existing machine-learning methods\nthat typically require extensive data from the target system for training. We\naddress this challenge by developing a hybrid transformer and\nreservoir-computing machine-learning scheme. The key idea is that, for a\ncomplex and nonlinear target system, the training of the transformer can be\nconducted not using any data from the target system, but with essentially\nunlimited synthetic data from known chaotic systems. The trained transformer is\nthen tested with the sparse data from the target system. The output of the\ntransformer is further fed into a reservoir computer for predicting the\nlong-term dynamics or the attractor of the target system. The power of the\nproposed hybrid machine-learning framework is demonstrated using a large number\nof prototypical nonlinear dynamical systems, with high reconstruction accuracy\neven when the available data is only 20% of that required to faithfully\nrepresent the dynamical behavior of the underlying system. The framework\nprovides a paradigm of reconstructing complex and nonlinear dynamics in the\nextreme situation where training data does not exist and the observations are\nrandom and sparse.\n", "link": "http://arxiv.org/abs/2410.21222v1", "date": "2024-10-28", "relevancy": 1.4732, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5344}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4799}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstructing%20dynamics%20from%20sparse%20observations%20with%20no%20training%20on%0A%20%20target%20system&body=Title%3A%20Reconstructing%20dynamics%20from%20sparse%20observations%20with%20no%20training%20on%0A%20%20target%20system%0AAuthor%3A%20Zheng-Meng%20Zhai%20and%20Jun-Yin%20Huang%20and%20Benjamin%20D.%20Stern%20and%20Ying-Cheng%20Lai%0AAbstract%3A%20%20%20In%20applications%2C%20an%20anticipated%20situation%20is%20where%20the%20system%20of%20interest%20has%0Anever%20been%20encountered%20before%20and%20sparse%20observations%20can%20be%20made%20only%20once.%0ACan%20the%20dynamics%20be%20faithfully%20reconstructed%20from%20the%20limited%20observations%0Awithout%20any%20training%20data%3F%20This%20problem%20defies%20any%20known%20traditional%20methods%20of%0Anonlinear%20time-series%20analysis%20as%20well%20as%20existing%20machine-learning%20methods%0Athat%20typically%20require%20extensive%20data%20from%20the%20target%20system%20for%20training.%20We%0Aaddress%20this%20challenge%20by%20developing%20a%20hybrid%20transformer%20and%0Areservoir-computing%20machine-learning%20scheme.%20The%20key%20idea%20is%20that%2C%20for%20a%0Acomplex%20and%20nonlinear%20target%20system%2C%20the%20training%20of%20the%20transformer%20can%20be%0Aconducted%20not%20using%20any%20data%20from%20the%20target%20system%2C%20but%20with%20essentially%0Aunlimited%20synthetic%20data%20from%20known%20chaotic%20systems.%20The%20trained%20transformer%20is%0Athen%20tested%20with%20the%20sparse%20data%20from%20the%20target%20system.%20The%20output%20of%20the%0Atransformer%20is%20further%20fed%20into%20a%20reservoir%20computer%20for%20predicting%20the%0Along-term%20dynamics%20or%20the%20attractor%20of%20the%20target%20system.%20The%20power%20of%20the%0Aproposed%20hybrid%20machine-learning%20framework%20is%20demonstrated%20using%20a%20large%20number%0Aof%20prototypical%20nonlinear%20dynamical%20systems%2C%20with%20high%20reconstruction%20accuracy%0Aeven%20when%20the%20available%20data%20is%20only%2020%25%20of%20that%20required%20to%20faithfully%0Arepresent%20the%20dynamical%20behavior%20of%20the%20underlying%20system.%20The%20framework%0Aprovides%20a%20paradigm%20of%20reconstructing%20complex%20and%20nonlinear%20dynamics%20in%20the%0Aextreme%20situation%20where%20training%20data%20does%20not%20exist%20and%20the%20observations%20are%0Arandom%20and%20sparse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstructing%2520dynamics%2520from%2520sparse%2520observations%2520with%2520no%2520training%2520on%250A%2520%2520target%2520system%26entry.906535625%3DZheng-Meng%2520Zhai%2520and%2520Jun-Yin%2520Huang%2520and%2520Benjamin%2520D.%2520Stern%2520and%2520Ying-Cheng%2520Lai%26entry.1292438233%3D%2520%2520In%2520applications%252C%2520an%2520anticipated%2520situation%2520is%2520where%2520the%2520system%2520of%2520interest%2520has%250Anever%2520been%2520encountered%2520before%2520and%2520sparse%2520observations%2520can%2520be%2520made%2520only%2520once.%250ACan%2520the%2520dynamics%2520be%2520faithfully%2520reconstructed%2520from%2520the%2520limited%2520observations%250Awithout%2520any%2520training%2520data%253F%2520This%2520problem%2520defies%2520any%2520known%2520traditional%2520methods%2520of%250Anonlinear%2520time-series%2520analysis%2520as%2520well%2520as%2520existing%2520machine-learning%2520methods%250Athat%2520typically%2520require%2520extensive%2520data%2520from%2520the%2520target%2520system%2520for%2520training.%2520We%250Aaddress%2520this%2520challenge%2520by%2520developing%2520a%2520hybrid%2520transformer%2520and%250Areservoir-computing%2520machine-learning%2520scheme.%2520The%2520key%2520idea%2520is%2520that%252C%2520for%2520a%250Acomplex%2520and%2520nonlinear%2520target%2520system%252C%2520the%2520training%2520of%2520the%2520transformer%2520can%2520be%250Aconducted%2520not%2520using%2520any%2520data%2520from%2520the%2520target%2520system%252C%2520but%2520with%2520essentially%250Aunlimited%2520synthetic%2520data%2520from%2520known%2520chaotic%2520systems.%2520The%2520trained%2520transformer%2520is%250Athen%2520tested%2520with%2520the%2520sparse%2520data%2520from%2520the%2520target%2520system.%2520The%2520output%2520of%2520the%250Atransformer%2520is%2520further%2520fed%2520into%2520a%2520reservoir%2520computer%2520for%2520predicting%2520the%250Along-term%2520dynamics%2520or%2520the%2520attractor%2520of%2520the%2520target%2520system.%2520The%2520power%2520of%2520the%250Aproposed%2520hybrid%2520machine-learning%2520framework%2520is%2520demonstrated%2520using%2520a%2520large%2520number%250Aof%2520prototypical%2520nonlinear%2520dynamical%2520systems%252C%2520with%2520high%2520reconstruction%2520accuracy%250Aeven%2520when%2520the%2520available%2520data%2520is%2520only%252020%2525%2520of%2520that%2520required%2520to%2520faithfully%250Arepresent%2520the%2520dynamical%2520behavior%2520of%2520the%2520underlying%2520system.%2520The%2520framework%250Aprovides%2520a%2520paradigm%2520of%2520reconstructing%2520complex%2520and%2520nonlinear%2520dynamics%2520in%2520the%250Aextreme%2520situation%2520where%2520training%2520data%2520does%2520not%2520exist%2520and%2520the%2520observations%2520are%250Arandom%2520and%2520sparse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstructing%20dynamics%20from%20sparse%20observations%20with%20no%20training%20on%0A%20%20target%20system&entry.906535625=Zheng-Meng%20Zhai%20and%20Jun-Yin%20Huang%20and%20Benjamin%20D.%20Stern%20and%20Ying-Cheng%20Lai&entry.1292438233=%20%20In%20applications%2C%20an%20anticipated%20situation%20is%20where%20the%20system%20of%20interest%20has%0Anever%20been%20encountered%20before%20and%20sparse%20observations%20can%20be%20made%20only%20once.%0ACan%20the%20dynamics%20be%20faithfully%20reconstructed%20from%20the%20limited%20observations%0Awithout%20any%20training%20data%3F%20This%20problem%20defies%20any%20known%20traditional%20methods%20of%0Anonlinear%20time-series%20analysis%20as%20well%20as%20existing%20machine-learning%20methods%0Athat%20typically%20require%20extensive%20data%20from%20the%20target%20system%20for%20training.%20We%0Aaddress%20this%20challenge%20by%20developing%20a%20hybrid%20transformer%20and%0Areservoir-computing%20machine-learning%20scheme.%20The%20key%20idea%20is%20that%2C%20for%20a%0Acomplex%20and%20nonlinear%20target%20system%2C%20the%20training%20of%20the%20transformer%20can%20be%0Aconducted%20not%20using%20any%20data%20from%20the%20target%20system%2C%20but%20with%20essentially%0Aunlimited%20synthetic%20data%20from%20known%20chaotic%20systems.%20The%20trained%20transformer%20is%0Athen%20tested%20with%20the%20sparse%20data%20from%20the%20target%20system.%20The%20output%20of%20the%0Atransformer%20is%20further%20fed%20into%20a%20reservoir%20computer%20for%20predicting%20the%0Along-term%20dynamics%20or%20the%20attractor%20of%20the%20target%20system.%20The%20power%20of%20the%0Aproposed%20hybrid%20machine-learning%20framework%20is%20demonstrated%20using%20a%20large%20number%0Aof%20prototypical%20nonlinear%20dynamical%20systems%2C%20with%20high%20reconstruction%20accuracy%0Aeven%20when%20the%20available%20data%20is%20only%2020%25%20of%20that%20required%20to%20faithfully%0Arepresent%20the%20dynamical%20behavior%20of%20the%20underlying%20system.%20The%20framework%0Aprovides%20a%20paradigm%20of%20reconstructing%20complex%20and%20nonlinear%20dynamics%20in%20the%0Aextreme%20situation%20where%20training%20data%20does%20not%20exist%20and%20the%20observations%20are%0Arandom%20and%20sparse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21222v1&entry.124074799=Read"},
{"title": "Representation noising can prevent harmful fine-tuning on LLMs", "author": "Domenic Rosati and Jan Wehner and Kai Williams and \u0141ukasz Bartoszcze and David Atanasov and Robie Gonzales and Subhabrata Majumdar and Carsten Maple and Hassan Sajjad and Frank Rudzicz", "abstract": "  Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM.\n", "link": "http://arxiv.org/abs/2405.14577v3", "date": "2024-10-28", "relevancy": 1.4542, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5032}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4752}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20noising%20can%20prevent%20harmful%20fine-tuning%20on%20LLMs&body=Title%3A%20Representation%20noising%20can%20prevent%20harmful%20fine-tuning%20on%20LLMs%0AAuthor%3A%20Domenic%20Rosati%20and%20Jan%20Wehner%20and%20Kai%20Williams%20and%20%C5%81ukasz%20Bartoszcze%20and%20David%20Atanasov%20and%20Robie%20Gonzales%20and%20Subhabrata%20Majumdar%20and%20Carsten%20Maple%20and%20Hassan%20Sajjad%20and%20Frank%20Rudzicz%0AAbstract%3A%20%20%20Releasing%20open-source%20large%20language%20models%20%28LLMs%29%20presents%20a%20dual-use%20risk%0Asince%20bad%20actors%20can%20easily%20fine-tune%20these%20models%20for%20harmful%20purposes.%20Even%0Awithout%20the%20open%20release%20of%20weights%2C%20weight%20stealing%20and%20fine-tuning%20APIs%20make%0Aclosed%20models%20vulnerable%20to%20harmful%20fine-tuning%20attacks%20%28HFAs%29.%20While%20safety%0Ameasures%20like%20preventing%20jailbreaks%20and%20improving%20safety%20guardrails%20are%0Aimportant%2C%20such%20measures%20can%20easily%20be%20reversed%20through%20fine-tuning.%20In%20this%0Awork%2C%20we%20propose%20Representation%20Noising%20%28RepNoise%29%2C%20a%20defence%20mechanism%20that%20is%0Aeffective%20even%20when%20attackers%20have%20access%20to%20the%20weights.%20RepNoise%20works%20by%0Aremoving%20information%20about%20harmful%20representations%20such%20that%20it%20is%20difficult%20to%0Arecover%20them%20during%20fine-tuning.%20Importantly%2C%20our%20defence%20is%20also%20able%20to%0Ageneralize%20across%20different%20subsets%20of%20harm%20that%20have%20not%20been%20seen%20during%20the%0Adefence%20process%20as%20long%20as%20they%20are%20drawn%20from%20the%20same%20distribution%20of%20the%0Aattack%20set.%20Our%20method%20does%20not%20degrade%20the%20general%20capability%20of%20LLMs%20and%0Aretains%20the%20ability%20to%20train%20the%20model%20on%20harmless%20tasks.%20We%20provide%20empirical%0Aevidence%20that%20the%20effectiveness%20of%20our%20defence%20lies%20in%20its%20%22depth%22%3A%20the%20degree%0Ato%20which%20information%20about%20harmful%20representations%20is%20removed%20across%20all%20layers%0Aof%20the%20LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14577v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520noising%2520can%2520prevent%2520harmful%2520fine-tuning%2520on%2520LLMs%26entry.906535625%3DDomenic%2520Rosati%2520and%2520Jan%2520Wehner%2520and%2520Kai%2520Williams%2520and%2520%25C5%2581ukasz%2520Bartoszcze%2520and%2520David%2520Atanasov%2520and%2520Robie%2520Gonzales%2520and%2520Subhabrata%2520Majumdar%2520and%2520Carsten%2520Maple%2520and%2520Hassan%2520Sajjad%2520and%2520Frank%2520Rudzicz%26entry.1292438233%3D%2520%2520Releasing%2520open-source%2520large%2520language%2520models%2520%2528LLMs%2529%2520presents%2520a%2520dual-use%2520risk%250Asince%2520bad%2520actors%2520can%2520easily%2520fine-tune%2520these%2520models%2520for%2520harmful%2520purposes.%2520Even%250Awithout%2520the%2520open%2520release%2520of%2520weights%252C%2520weight%2520stealing%2520and%2520fine-tuning%2520APIs%2520make%250Aclosed%2520models%2520vulnerable%2520to%2520harmful%2520fine-tuning%2520attacks%2520%2528HFAs%2529.%2520While%2520safety%250Ameasures%2520like%2520preventing%2520jailbreaks%2520and%2520improving%2520safety%2520guardrails%2520are%250Aimportant%252C%2520such%2520measures%2520can%2520easily%2520be%2520reversed%2520through%2520fine-tuning.%2520In%2520this%250Awork%252C%2520we%2520propose%2520Representation%2520Noising%2520%2528RepNoise%2529%252C%2520a%2520defence%2520mechanism%2520that%2520is%250Aeffective%2520even%2520when%2520attackers%2520have%2520access%2520to%2520the%2520weights.%2520RepNoise%2520works%2520by%250Aremoving%2520information%2520about%2520harmful%2520representations%2520such%2520that%2520it%2520is%2520difficult%2520to%250Arecover%2520them%2520during%2520fine-tuning.%2520Importantly%252C%2520our%2520defence%2520is%2520also%2520able%2520to%250Ageneralize%2520across%2520different%2520subsets%2520of%2520harm%2520that%2520have%2520not%2520been%2520seen%2520during%2520the%250Adefence%2520process%2520as%2520long%2520as%2520they%2520are%2520drawn%2520from%2520the%2520same%2520distribution%2520of%2520the%250Aattack%2520set.%2520Our%2520method%2520does%2520not%2520degrade%2520the%2520general%2520capability%2520of%2520LLMs%2520and%250Aretains%2520the%2520ability%2520to%2520train%2520the%2520model%2520on%2520harmless%2520tasks.%2520We%2520provide%2520empirical%250Aevidence%2520that%2520the%2520effectiveness%2520of%2520our%2520defence%2520lies%2520in%2520its%2520%2522depth%2522%253A%2520the%2520degree%250Ato%2520which%2520information%2520about%2520harmful%2520representations%2520is%2520removed%2520across%2520all%2520layers%250Aof%2520the%2520LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14577v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20noising%20can%20prevent%20harmful%20fine-tuning%20on%20LLMs&entry.906535625=Domenic%20Rosati%20and%20Jan%20Wehner%20and%20Kai%20Williams%20and%20%C5%81ukasz%20Bartoszcze%20and%20David%20Atanasov%20and%20Robie%20Gonzales%20and%20Subhabrata%20Majumdar%20and%20Carsten%20Maple%20and%20Hassan%20Sajjad%20and%20Frank%20Rudzicz&entry.1292438233=%20%20Releasing%20open-source%20large%20language%20models%20%28LLMs%29%20presents%20a%20dual-use%20risk%0Asince%20bad%20actors%20can%20easily%20fine-tune%20these%20models%20for%20harmful%20purposes.%20Even%0Awithout%20the%20open%20release%20of%20weights%2C%20weight%20stealing%20and%20fine-tuning%20APIs%20make%0Aclosed%20models%20vulnerable%20to%20harmful%20fine-tuning%20attacks%20%28HFAs%29.%20While%20safety%0Ameasures%20like%20preventing%20jailbreaks%20and%20improving%20safety%20guardrails%20are%0Aimportant%2C%20such%20measures%20can%20easily%20be%20reversed%20through%20fine-tuning.%20In%20this%0Awork%2C%20we%20propose%20Representation%20Noising%20%28RepNoise%29%2C%20a%20defence%20mechanism%20that%20is%0Aeffective%20even%20when%20attackers%20have%20access%20to%20the%20weights.%20RepNoise%20works%20by%0Aremoving%20information%20about%20harmful%20representations%20such%20that%20it%20is%20difficult%20to%0Arecover%20them%20during%20fine-tuning.%20Importantly%2C%20our%20defence%20is%20also%20able%20to%0Ageneralize%20across%20different%20subsets%20of%20harm%20that%20have%20not%20been%20seen%20during%20the%0Adefence%20process%20as%20long%20as%20they%20are%20drawn%20from%20the%20same%20distribution%20of%20the%0Aattack%20set.%20Our%20method%20does%20not%20degrade%20the%20general%20capability%20of%20LLMs%20and%0Aretains%20the%20ability%20to%20train%20the%20model%20on%20harmless%20tasks.%20We%20provide%20empirical%0Aevidence%20that%20the%20effectiveness%20of%20our%20defence%20lies%20in%20its%20%22depth%22%3A%20the%20degree%0Ato%20which%20information%20about%20harmful%20representations%20is%20removed%20across%20all%20layers%0Aof%20the%20LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14577v3&entry.124074799=Read"},
{"title": "Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing\n  Flows", "author": "Alberto Cabezas and Louis Sharrock and Christopher Nemeth", "abstract": "  Continuous normalizing flows (CNFs) learn the probability path between a\nreference distribution and a target distribution by modeling the vector field\ngenerating said path using neural networks. Recently, Lipman et al. (2022)\nintroduced a simple and inexpensive method for training CNFs in generative\nmodeling, termed flow matching (FM). In this paper, we repurpose this method\nfor probabilistic inference by incorporating Markovian sampling methods in\nevaluating the FM objective, and using the learned CNF to improve Monte Carlo\nsampling. Specifically, we propose an adaptive Markov chain Monte Carlo (MCMC)\nalgorithm, which combines a local Markov transition kernel with a non-local,\nflow-informed transition kernel, defined using a CNF. This CNF is adapted\non-the-fly using samples from the Markov chain, which are used to specify the\nprobability path for the FM objective. Our method also includes an adaptive\ntempering mechanism that allows the discovery of multiple modes in the target\ndistribution. Under mild assumptions, we establish convergence of our method to\na local optimum of the FM objective. We then benchmark our approach on several\nsynthetic and real-world examples, achieving similar performance to other\nstate-of-the-art methods, but often at a significantly lower computational\ncost.\n", "link": "http://arxiv.org/abs/2405.14392v2", "date": "2024-10-28", "relevancy": 1.4271, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5134}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5009}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Markovian%20Flow%20Matching%3A%20Accelerating%20MCMC%20with%20Continuous%20Normalizing%0A%20%20Flows&body=Title%3A%20Markovian%20Flow%20Matching%3A%20Accelerating%20MCMC%20with%20Continuous%20Normalizing%0A%20%20Flows%0AAuthor%3A%20Alberto%20Cabezas%20and%20Louis%20Sharrock%20and%20Christopher%20Nemeth%0AAbstract%3A%20%20%20Continuous%20normalizing%20flows%20%28CNFs%29%20learn%20the%20probability%20path%20between%20a%0Areference%20distribution%20and%20a%20target%20distribution%20by%20modeling%20the%20vector%20field%0Agenerating%20said%20path%20using%20neural%20networks.%20Recently%2C%20Lipman%20et%20al.%20%282022%29%0Aintroduced%20a%20simple%20and%20inexpensive%20method%20for%20training%20CNFs%20in%20generative%0Amodeling%2C%20termed%20flow%20matching%20%28FM%29.%20In%20this%20paper%2C%20we%20repurpose%20this%20method%0Afor%20probabilistic%20inference%20by%20incorporating%20Markovian%20sampling%20methods%20in%0Aevaluating%20the%20FM%20objective%2C%20and%20using%20the%20learned%20CNF%20to%20improve%20Monte%20Carlo%0Asampling.%20Specifically%2C%20we%20propose%20an%20adaptive%20Markov%20chain%20Monte%20Carlo%20%28MCMC%29%0Aalgorithm%2C%20which%20combines%20a%20local%20Markov%20transition%20kernel%20with%20a%20non-local%2C%0Aflow-informed%20transition%20kernel%2C%20defined%20using%20a%20CNF.%20This%20CNF%20is%20adapted%0Aon-the-fly%20using%20samples%20from%20the%20Markov%20chain%2C%20which%20are%20used%20to%20specify%20the%0Aprobability%20path%20for%20the%20FM%20objective.%20Our%20method%20also%20includes%20an%20adaptive%0Atempering%20mechanism%20that%20allows%20the%20discovery%20of%20multiple%20modes%20in%20the%20target%0Adistribution.%20Under%20mild%20assumptions%2C%20we%20establish%20convergence%20of%20our%20method%20to%0Aa%20local%20optimum%20of%20the%20FM%20objective.%20We%20then%20benchmark%20our%20approach%20on%20several%0Asynthetic%20and%20real-world%20examples%2C%20achieving%20similar%20performance%20to%20other%0Astate-of-the-art%20methods%2C%20but%20often%20at%20a%20significantly%20lower%20computational%0Acost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14392v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMarkovian%2520Flow%2520Matching%253A%2520Accelerating%2520MCMC%2520with%2520Continuous%2520Normalizing%250A%2520%2520Flows%26entry.906535625%3DAlberto%2520Cabezas%2520and%2520Louis%2520Sharrock%2520and%2520Christopher%2520Nemeth%26entry.1292438233%3D%2520%2520Continuous%2520normalizing%2520flows%2520%2528CNFs%2529%2520learn%2520the%2520probability%2520path%2520between%2520a%250Areference%2520distribution%2520and%2520a%2520target%2520distribution%2520by%2520modeling%2520the%2520vector%2520field%250Agenerating%2520said%2520path%2520using%2520neural%2520networks.%2520Recently%252C%2520Lipman%2520et%2520al.%2520%25282022%2529%250Aintroduced%2520a%2520simple%2520and%2520inexpensive%2520method%2520for%2520training%2520CNFs%2520in%2520generative%250Amodeling%252C%2520termed%2520flow%2520matching%2520%2528FM%2529.%2520In%2520this%2520paper%252C%2520we%2520repurpose%2520this%2520method%250Afor%2520probabilistic%2520inference%2520by%2520incorporating%2520Markovian%2520sampling%2520methods%2520in%250Aevaluating%2520the%2520FM%2520objective%252C%2520and%2520using%2520the%2520learned%2520CNF%2520to%2520improve%2520Monte%2520Carlo%250Asampling.%2520Specifically%252C%2520we%2520propose%2520an%2520adaptive%2520Markov%2520chain%2520Monte%2520Carlo%2520%2528MCMC%2529%250Aalgorithm%252C%2520which%2520combines%2520a%2520local%2520Markov%2520transition%2520kernel%2520with%2520a%2520non-local%252C%250Aflow-informed%2520transition%2520kernel%252C%2520defined%2520using%2520a%2520CNF.%2520This%2520CNF%2520is%2520adapted%250Aon-the-fly%2520using%2520samples%2520from%2520the%2520Markov%2520chain%252C%2520which%2520are%2520used%2520to%2520specify%2520the%250Aprobability%2520path%2520for%2520the%2520FM%2520objective.%2520Our%2520method%2520also%2520includes%2520an%2520adaptive%250Atempering%2520mechanism%2520that%2520allows%2520the%2520discovery%2520of%2520multiple%2520modes%2520in%2520the%2520target%250Adistribution.%2520Under%2520mild%2520assumptions%252C%2520we%2520establish%2520convergence%2520of%2520our%2520method%2520to%250Aa%2520local%2520optimum%2520of%2520the%2520FM%2520objective.%2520We%2520then%2520benchmark%2520our%2520approach%2520on%2520several%250Asynthetic%2520and%2520real-world%2520examples%252C%2520achieving%2520similar%2520performance%2520to%2520other%250Astate-of-the-art%2520methods%252C%2520but%2520often%2520at%2520a%2520significantly%2520lower%2520computational%250Acost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14392v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Markovian%20Flow%20Matching%3A%20Accelerating%20MCMC%20with%20Continuous%20Normalizing%0A%20%20Flows&entry.906535625=Alberto%20Cabezas%20and%20Louis%20Sharrock%20and%20Christopher%20Nemeth&entry.1292438233=%20%20Continuous%20normalizing%20flows%20%28CNFs%29%20learn%20the%20probability%20path%20between%20a%0Areference%20distribution%20and%20a%20target%20distribution%20by%20modeling%20the%20vector%20field%0Agenerating%20said%20path%20using%20neural%20networks.%20Recently%2C%20Lipman%20et%20al.%20%282022%29%0Aintroduced%20a%20simple%20and%20inexpensive%20method%20for%20training%20CNFs%20in%20generative%0Amodeling%2C%20termed%20flow%20matching%20%28FM%29.%20In%20this%20paper%2C%20we%20repurpose%20this%20method%0Afor%20probabilistic%20inference%20by%20incorporating%20Markovian%20sampling%20methods%20in%0Aevaluating%20the%20FM%20objective%2C%20and%20using%20the%20learned%20CNF%20to%20improve%20Monte%20Carlo%0Asampling.%20Specifically%2C%20we%20propose%20an%20adaptive%20Markov%20chain%20Monte%20Carlo%20%28MCMC%29%0Aalgorithm%2C%20which%20combines%20a%20local%20Markov%20transition%20kernel%20with%20a%20non-local%2C%0Aflow-informed%20transition%20kernel%2C%20defined%20using%20a%20CNF.%20This%20CNF%20is%20adapted%0Aon-the-fly%20using%20samples%20from%20the%20Markov%20chain%2C%20which%20are%20used%20to%20specify%20the%0Aprobability%20path%20for%20the%20FM%20objective.%20Our%20method%20also%20includes%20an%20adaptive%0Atempering%20mechanism%20that%20allows%20the%20discovery%20of%20multiple%20modes%20in%20the%20target%0Adistribution.%20Under%20mild%20assumptions%2C%20we%20establish%20convergence%20of%20our%20method%20to%0Aa%20local%20optimum%20of%20the%20FM%20objective.%20We%20then%20benchmark%20our%20approach%20on%20several%0Asynthetic%20and%20real-world%20examples%2C%20achieving%20similar%20performance%20to%20other%0Astate-of-the-art%20methods%2C%20but%20often%20at%20a%20significantly%20lower%20computational%0Acost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14392v2&entry.124074799=Read"},
{"title": "RepairAgent: An Autonomous, LLM-Based Agent for Program Repair", "author": "Islem Bouzenia and Premkumar Devanbu and Michael Pradel", "abstract": "  Automated program repair has emerged as a powerful technique to mitigate the\nimpact of software bugs on system reliability and user experience. This paper\nintroduces RepairAgent, the first work to address the program repair challenge\nthrough an autonomous agent based on a large language model (LLM). Unlike\nexisting deep learning-based approaches, which prompt a model with a fixed\nprompt or in a fixed feedback loop, our work treats the LLM as an agent capable\nof autonomously planning and executing actions to fix bugs by invoking suitable\ntools. RepairAgent freely interleaves gathering information about the bug,\ngathering repair ingredients, and validating fixes, while deciding which tools\nto invoke based on the gathered information and feedback from previous fix\nattempts. Key contributions that enable RepairAgent include a set of tools that\nare useful for program repair, a dynamically updated prompt format that allows\nthe LLM to interact with these tools, and a finite state machine that guides\nthe agent in invoking the tools. Our evaluation on the popular Defects4J\ndataset demonstrates RepairAgent's effectiveness in autonomously repairing 164\nbugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM\nimposes an average cost of 270,000 tokens per bug, which, under the current\npricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To\nthe best of our knowledge, this work is the first to present an autonomous,\nLLM-based agent for program repair, paving the way for future agent-based\ntechniques in software engineering.\n", "link": "http://arxiv.org/abs/2403.17134v2", "date": "2024-10-28", "relevancy": 1.3621, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4696}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4535}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RepairAgent%3A%20An%20Autonomous%2C%20LLM-Based%20Agent%20for%20Program%20Repair&body=Title%3A%20RepairAgent%3A%20An%20Autonomous%2C%20LLM-Based%20Agent%20for%20Program%20Repair%0AAuthor%3A%20Islem%20Bouzenia%20and%20Premkumar%20Devanbu%20and%20Michael%20Pradel%0AAbstract%3A%20%20%20Automated%20program%20repair%20has%20emerged%20as%20a%20powerful%20technique%20to%20mitigate%20the%0Aimpact%20of%20software%20bugs%20on%20system%20reliability%20and%20user%20experience.%20This%20paper%0Aintroduces%20RepairAgent%2C%20the%20first%20work%20to%20address%20the%20program%20repair%20challenge%0Athrough%20an%20autonomous%20agent%20based%20on%20a%20large%20language%20model%20%28LLM%29.%20Unlike%0Aexisting%20deep%20learning-based%20approaches%2C%20which%20prompt%20a%20model%20with%20a%20fixed%0Aprompt%20or%20in%20a%20fixed%20feedback%20loop%2C%20our%20work%20treats%20the%20LLM%20as%20an%20agent%20capable%0Aof%20autonomously%20planning%20and%20executing%20actions%20to%20fix%20bugs%20by%20invoking%20suitable%0Atools.%20RepairAgent%20freely%20interleaves%20gathering%20information%20about%20the%20bug%2C%0Agathering%20repair%20ingredients%2C%20and%20validating%20fixes%2C%20while%20deciding%20which%20tools%0Ato%20invoke%20based%20on%20the%20gathered%20information%20and%20feedback%20from%20previous%20fix%0Aattempts.%20Key%20contributions%20that%20enable%20RepairAgent%20include%20a%20set%20of%20tools%20that%0Aare%20useful%20for%20program%20repair%2C%20a%20dynamically%20updated%20prompt%20format%20that%20allows%0Athe%20LLM%20to%20interact%20with%20these%20tools%2C%20and%20a%20finite%20state%20machine%20that%20guides%0Athe%20agent%20in%20invoking%20the%20tools.%20Our%20evaluation%20on%20the%20popular%20Defects4J%0Adataset%20demonstrates%20RepairAgent%27s%20effectiveness%20in%20autonomously%20repairing%20164%0Abugs%2C%20including%2039%20bugs%20not%20fixed%20by%20prior%20techniques.%20Interacting%20with%20the%20LLM%0Aimposes%20an%20average%20cost%20of%20270%2C000%20tokens%20per%20bug%2C%20which%2C%20under%20the%20current%0Apricing%20of%20OpenAI%27s%20GPT-3.5%20model%2C%20translates%20to%2014%20cents%20of%20USD%20per%20bug.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%20present%20an%20autonomous%2C%0ALLM-based%20agent%20for%20program%20repair%2C%20paving%20the%20way%20for%20future%20agent-based%0Atechniques%20in%20software%20engineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17134v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepairAgent%253A%2520An%2520Autonomous%252C%2520LLM-Based%2520Agent%2520for%2520Program%2520Repair%26entry.906535625%3DIslem%2520Bouzenia%2520and%2520Premkumar%2520Devanbu%2520and%2520Michael%2520Pradel%26entry.1292438233%3D%2520%2520Automated%2520program%2520repair%2520has%2520emerged%2520as%2520a%2520powerful%2520technique%2520to%2520mitigate%2520the%250Aimpact%2520of%2520software%2520bugs%2520on%2520system%2520reliability%2520and%2520user%2520experience.%2520This%2520paper%250Aintroduces%2520RepairAgent%252C%2520the%2520first%2520work%2520to%2520address%2520the%2520program%2520repair%2520challenge%250Athrough%2520an%2520autonomous%2520agent%2520based%2520on%2520a%2520large%2520language%2520model%2520%2528LLM%2529.%2520Unlike%250Aexisting%2520deep%2520learning-based%2520approaches%252C%2520which%2520prompt%2520a%2520model%2520with%2520a%2520fixed%250Aprompt%2520or%2520in%2520a%2520fixed%2520feedback%2520loop%252C%2520our%2520work%2520treats%2520the%2520LLM%2520as%2520an%2520agent%2520capable%250Aof%2520autonomously%2520planning%2520and%2520executing%2520actions%2520to%2520fix%2520bugs%2520by%2520invoking%2520suitable%250Atools.%2520RepairAgent%2520freely%2520interleaves%2520gathering%2520information%2520about%2520the%2520bug%252C%250Agathering%2520repair%2520ingredients%252C%2520and%2520validating%2520fixes%252C%2520while%2520deciding%2520which%2520tools%250Ato%2520invoke%2520based%2520on%2520the%2520gathered%2520information%2520and%2520feedback%2520from%2520previous%2520fix%250Aattempts.%2520Key%2520contributions%2520that%2520enable%2520RepairAgent%2520include%2520a%2520set%2520of%2520tools%2520that%250Aare%2520useful%2520for%2520program%2520repair%252C%2520a%2520dynamically%2520updated%2520prompt%2520format%2520that%2520allows%250Athe%2520LLM%2520to%2520interact%2520with%2520these%2520tools%252C%2520and%2520a%2520finite%2520state%2520machine%2520that%2520guides%250Athe%2520agent%2520in%2520invoking%2520the%2520tools.%2520Our%2520evaluation%2520on%2520the%2520popular%2520Defects4J%250Adataset%2520demonstrates%2520RepairAgent%2527s%2520effectiveness%2520in%2520autonomously%2520repairing%2520164%250Abugs%252C%2520including%252039%2520bugs%2520not%2520fixed%2520by%2520prior%2520techniques.%2520Interacting%2520with%2520the%2520LLM%250Aimposes%2520an%2520average%2520cost%2520of%2520270%252C000%2520tokens%2520per%2520bug%252C%2520which%252C%2520under%2520the%2520current%250Apricing%2520of%2520OpenAI%2527s%2520GPT-3.5%2520model%252C%2520translates%2520to%252014%2520cents%2520of%2520USD%2520per%2520bug.%2520To%250Athe%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%2520is%2520the%2520first%2520to%2520present%2520an%2520autonomous%252C%250ALLM-based%2520agent%2520for%2520program%2520repair%252C%2520paving%2520the%2520way%2520for%2520future%2520agent-based%250Atechniques%2520in%2520software%2520engineering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17134v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RepairAgent%3A%20An%20Autonomous%2C%20LLM-Based%20Agent%20for%20Program%20Repair&entry.906535625=Islem%20Bouzenia%20and%20Premkumar%20Devanbu%20and%20Michael%20Pradel&entry.1292438233=%20%20Automated%20program%20repair%20has%20emerged%20as%20a%20powerful%20technique%20to%20mitigate%20the%0Aimpact%20of%20software%20bugs%20on%20system%20reliability%20and%20user%20experience.%20This%20paper%0Aintroduces%20RepairAgent%2C%20the%20first%20work%20to%20address%20the%20program%20repair%20challenge%0Athrough%20an%20autonomous%20agent%20based%20on%20a%20large%20language%20model%20%28LLM%29.%20Unlike%0Aexisting%20deep%20learning-based%20approaches%2C%20which%20prompt%20a%20model%20with%20a%20fixed%0Aprompt%20or%20in%20a%20fixed%20feedback%20loop%2C%20our%20work%20treats%20the%20LLM%20as%20an%20agent%20capable%0Aof%20autonomously%20planning%20and%20executing%20actions%20to%20fix%20bugs%20by%20invoking%20suitable%0Atools.%20RepairAgent%20freely%20interleaves%20gathering%20information%20about%20the%20bug%2C%0Agathering%20repair%20ingredients%2C%20and%20validating%20fixes%2C%20while%20deciding%20which%20tools%0Ato%20invoke%20based%20on%20the%20gathered%20information%20and%20feedback%20from%20previous%20fix%0Aattempts.%20Key%20contributions%20that%20enable%20RepairAgent%20include%20a%20set%20of%20tools%20that%0Aare%20useful%20for%20program%20repair%2C%20a%20dynamically%20updated%20prompt%20format%20that%20allows%0Athe%20LLM%20to%20interact%20with%20these%20tools%2C%20and%20a%20finite%20state%20machine%20that%20guides%0Athe%20agent%20in%20invoking%20the%20tools.%20Our%20evaluation%20on%20the%20popular%20Defects4J%0Adataset%20demonstrates%20RepairAgent%27s%20effectiveness%20in%20autonomously%20repairing%20164%0Abugs%2C%20including%2039%20bugs%20not%20fixed%20by%20prior%20techniques.%20Interacting%20with%20the%20LLM%0Aimposes%20an%20average%20cost%20of%20270%2C000%20tokens%20per%20bug%2C%20which%2C%20under%20the%20current%0Apricing%20of%20OpenAI%27s%20GPT-3.5%20model%2C%20translates%20to%2014%20cents%20of%20USD%20per%20bug.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%20present%20an%20autonomous%2C%0ALLM-based%20agent%20for%20program%20repair%2C%20paving%20the%20way%20for%20future%20agent-based%0Atechniques%20in%20software%20engineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17134v2&entry.124074799=Read"},
{"title": "On learning higher-order cumulants in diffusion models", "author": "Gert Aarts and Diaa E. Habibi and Lingxiao Wang and Kai Zhou", "abstract": "  To analyse how diffusion models learn correlations beyond Gaussian ones, we\nstudy the behaviour of higher-order cumulants, or connected n-point functions,\nunder both the forward and backward process. We derive explicit expressions for\nthe moment- and cumulant-generating functionals, in terms of the distribution\nof the initial data and properties of forward process. It is shown analytically\nthat during the forward process higher-order cumulants are conserved in models\nwithout a drift, such as the variance-expanding scheme, and that therefore the\nendpoint of the forward process maintains nontrivial correlations. We\ndemonstrate that since these correlations are encoded in the score function,\nhigher-order cumulants are learnt in the backward process, also when starting\nfrom a normal prior. We confirm our analytical results in an exactly solvable\ntoy model with nonzero cumulants and in scalar lattice field theory.\n", "link": "http://arxiv.org/abs/2410.21212v1", "date": "2024-10-28", "relevancy": 1.3229, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4506}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4477}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20learning%20higher-order%20cumulants%20in%20diffusion%20models&body=Title%3A%20On%20learning%20higher-order%20cumulants%20in%20diffusion%20models%0AAuthor%3A%20Gert%20Aarts%20and%20Diaa%20E.%20Habibi%20and%20Lingxiao%20Wang%20and%20Kai%20Zhou%0AAbstract%3A%20%20%20To%20analyse%20how%20diffusion%20models%20learn%20correlations%20beyond%20Gaussian%20ones%2C%20we%0Astudy%20the%20behaviour%20of%20higher-order%20cumulants%2C%20or%20connected%20n-point%20functions%2C%0Aunder%20both%20the%20forward%20and%20backward%20process.%20We%20derive%20explicit%20expressions%20for%0Athe%20moment-%20and%20cumulant-generating%20functionals%2C%20in%20terms%20of%20the%20distribution%0Aof%20the%20initial%20data%20and%20properties%20of%20forward%20process.%20It%20is%20shown%20analytically%0Athat%20during%20the%20forward%20process%20higher-order%20cumulants%20are%20conserved%20in%20models%0Awithout%20a%20drift%2C%20such%20as%20the%20variance-expanding%20scheme%2C%20and%20that%20therefore%20the%0Aendpoint%20of%20the%20forward%20process%20maintains%20nontrivial%20correlations.%20We%0Ademonstrate%20that%20since%20these%20correlations%20are%20encoded%20in%20the%20score%20function%2C%0Ahigher-order%20cumulants%20are%20learnt%20in%20the%20backward%20process%2C%20also%20when%20starting%0Afrom%20a%20normal%20prior.%20We%20confirm%20our%20analytical%20results%20in%20an%20exactly%20solvable%0Atoy%20model%20with%20nonzero%20cumulants%20and%20in%20scalar%20lattice%20field%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520learning%2520higher-order%2520cumulants%2520in%2520diffusion%2520models%26entry.906535625%3DGert%2520Aarts%2520and%2520Diaa%2520E.%2520Habibi%2520and%2520Lingxiao%2520Wang%2520and%2520Kai%2520Zhou%26entry.1292438233%3D%2520%2520To%2520analyse%2520how%2520diffusion%2520models%2520learn%2520correlations%2520beyond%2520Gaussian%2520ones%252C%2520we%250Astudy%2520the%2520behaviour%2520of%2520higher-order%2520cumulants%252C%2520or%2520connected%2520n-point%2520functions%252C%250Aunder%2520both%2520the%2520forward%2520and%2520backward%2520process.%2520We%2520derive%2520explicit%2520expressions%2520for%250Athe%2520moment-%2520and%2520cumulant-generating%2520functionals%252C%2520in%2520terms%2520of%2520the%2520distribution%250Aof%2520the%2520initial%2520data%2520and%2520properties%2520of%2520forward%2520process.%2520It%2520is%2520shown%2520analytically%250Athat%2520during%2520the%2520forward%2520process%2520higher-order%2520cumulants%2520are%2520conserved%2520in%2520models%250Awithout%2520a%2520drift%252C%2520such%2520as%2520the%2520variance-expanding%2520scheme%252C%2520and%2520that%2520therefore%2520the%250Aendpoint%2520of%2520the%2520forward%2520process%2520maintains%2520nontrivial%2520correlations.%2520We%250Ademonstrate%2520that%2520since%2520these%2520correlations%2520are%2520encoded%2520in%2520the%2520score%2520function%252C%250Ahigher-order%2520cumulants%2520are%2520learnt%2520in%2520the%2520backward%2520process%252C%2520also%2520when%2520starting%250Afrom%2520a%2520normal%2520prior.%2520We%2520confirm%2520our%2520analytical%2520results%2520in%2520an%2520exactly%2520solvable%250Atoy%2520model%2520with%2520nonzero%2520cumulants%2520and%2520in%2520scalar%2520lattice%2520field%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20learning%20higher-order%20cumulants%20in%20diffusion%20models&entry.906535625=Gert%20Aarts%20and%20Diaa%20E.%20Habibi%20and%20Lingxiao%20Wang%20and%20Kai%20Zhou&entry.1292438233=%20%20To%20analyse%20how%20diffusion%20models%20learn%20correlations%20beyond%20Gaussian%20ones%2C%20we%0Astudy%20the%20behaviour%20of%20higher-order%20cumulants%2C%20or%20connected%20n-point%20functions%2C%0Aunder%20both%20the%20forward%20and%20backward%20process.%20We%20derive%20explicit%20expressions%20for%0Athe%20moment-%20and%20cumulant-generating%20functionals%2C%20in%20terms%20of%20the%20distribution%0Aof%20the%20initial%20data%20and%20properties%20of%20forward%20process.%20It%20is%20shown%20analytically%0Athat%20during%20the%20forward%20process%20higher-order%20cumulants%20are%20conserved%20in%20models%0Awithout%20a%20drift%2C%20such%20as%20the%20variance-expanding%20scheme%2C%20and%20that%20therefore%20the%0Aendpoint%20of%20the%20forward%20process%20maintains%20nontrivial%20correlations.%20We%0Ademonstrate%20that%20since%20these%20correlations%20are%20encoded%20in%20the%20score%20function%2C%0Ahigher-order%20cumulants%20are%20learnt%20in%20the%20backward%20process%2C%20also%20when%20starting%0Afrom%20a%20normal%20prior.%20We%20confirm%20our%20analytical%20results%20in%20an%20exactly%20solvable%0Atoy%20model%20with%20nonzero%20cumulants%20and%20in%20scalar%20lattice%20field%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21212v1&entry.124074799=Read"},
{"title": "Capacity-Aware Planning and Scheduling in Budget-Constrained Monotonic\n  MDPs: A Meta-RL Approach", "author": "Manav Vora and Ilan Shomorony and Melkior Ornik", "abstract": "  Many real-world sequential repair problems can be effectively modeled using\nmonotonic Markov Decision Processes (MDPs), where the system state\nstochastically decreases and can only be increased by performing a restorative\naction. This work addresses the problem of solving multi-component monotonic\nMDPs with both budget and capacity constraints. The budget constraint limits\nthe total number of restorative actions and the capacity constraint limits the\nnumber of restorative actions that can be performed simultaneously. While prior\nmethods dealt with budget constraints, including capacity constraints in prior\nmethods leads to an exponential increase in computational complexity as the\nnumber of components in the MDP grows. We propose a two-step planning approach\nto address this challenge. First, we partition the components of the\nmulti-component MDP into groups, where the number of groups is determined by\nthe capacity constraint. We achieve this partitioning by solving a Linear Sum\nAssignment Problem (LSAP). Each group is then allocated a fraction of the total\nbudget proportional to its size. This partitioning effectively decouples the\nlarge multi-component MDP into smaller subproblems, which are computationally\nfeasible because the capacity constraint is simplified and the budget\nconstraint can be addressed using existing methods. Subsequently, we use a\nmeta-trained PPO agent to obtain an approximately optimal policy for each\ngroup. To validate our approach, we apply it to the problem of scheduling\nrepairs for a large group of industrial robots, constrained by a limited number\nof repair technicians and a total repair budget. Our results demonstrate that\nthe proposed method outperforms baseline approaches in terms of maximizing the\naverage uptime of the robot swarm, particularly for large swarm sizes.\n", "link": "http://arxiv.org/abs/2410.21249v1", "date": "2024-10-28", "relevancy": 1.312, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4782}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4736}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Capacity-Aware%20Planning%20and%20Scheduling%20in%20Budget-Constrained%20Monotonic%0A%20%20MDPs%3A%20A%20Meta-RL%20Approach&body=Title%3A%20Capacity-Aware%20Planning%20and%20Scheduling%20in%20Budget-Constrained%20Monotonic%0A%20%20MDPs%3A%20A%20Meta-RL%20Approach%0AAuthor%3A%20Manav%20Vora%20and%20Ilan%20Shomorony%20and%20Melkior%20Ornik%0AAbstract%3A%20%20%20Many%20real-world%20sequential%20repair%20problems%20can%20be%20effectively%20modeled%20using%0Amonotonic%20Markov%20Decision%20Processes%20%28MDPs%29%2C%20where%20the%20system%20state%0Astochastically%20decreases%20and%20can%20only%20be%20increased%20by%20performing%20a%20restorative%0Aaction.%20This%20work%20addresses%20the%20problem%20of%20solving%20multi-component%20monotonic%0AMDPs%20with%20both%20budget%20and%20capacity%20constraints.%20The%20budget%20constraint%20limits%0Athe%20total%20number%20of%20restorative%20actions%20and%20the%20capacity%20constraint%20limits%20the%0Anumber%20of%20restorative%20actions%20that%20can%20be%20performed%20simultaneously.%20While%20prior%0Amethods%20dealt%20with%20budget%20constraints%2C%20including%20capacity%20constraints%20in%20prior%0Amethods%20leads%20to%20an%20exponential%20increase%20in%20computational%20complexity%20as%20the%0Anumber%20of%20components%20in%20the%20MDP%20grows.%20We%20propose%20a%20two-step%20planning%20approach%0Ato%20address%20this%20challenge.%20First%2C%20we%20partition%20the%20components%20of%20the%0Amulti-component%20MDP%20into%20groups%2C%20where%20the%20number%20of%20groups%20is%20determined%20by%0Athe%20capacity%20constraint.%20We%20achieve%20this%20partitioning%20by%20solving%20a%20Linear%20Sum%0AAssignment%20Problem%20%28LSAP%29.%20Each%20group%20is%20then%20allocated%20a%20fraction%20of%20the%20total%0Abudget%20proportional%20to%20its%20size.%20This%20partitioning%20effectively%20decouples%20the%0Alarge%20multi-component%20MDP%20into%20smaller%20subproblems%2C%20which%20are%20computationally%0Afeasible%20because%20the%20capacity%20constraint%20is%20simplified%20and%20the%20budget%0Aconstraint%20can%20be%20addressed%20using%20existing%20methods.%20Subsequently%2C%20we%20use%20a%0Ameta-trained%20PPO%20agent%20to%20obtain%20an%20approximately%20optimal%20policy%20for%20each%0Agroup.%20To%20validate%20our%20approach%2C%20we%20apply%20it%20to%20the%20problem%20of%20scheduling%0Arepairs%20for%20a%20large%20group%20of%20industrial%20robots%2C%20constrained%20by%20a%20limited%20number%0Aof%20repair%20technicians%20and%20a%20total%20repair%20budget.%20Our%20results%20demonstrate%20that%0Athe%20proposed%20method%20outperforms%20baseline%20approaches%20in%20terms%20of%20maximizing%20the%0Aaverage%20uptime%20of%20the%20robot%20swarm%2C%20particularly%20for%20large%20swarm%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapacity-Aware%2520Planning%2520and%2520Scheduling%2520in%2520Budget-Constrained%2520Monotonic%250A%2520%2520MDPs%253A%2520A%2520Meta-RL%2520Approach%26entry.906535625%3DManav%2520Vora%2520and%2520Ilan%2520Shomorony%2520and%2520Melkior%2520Ornik%26entry.1292438233%3D%2520%2520Many%2520real-world%2520sequential%2520repair%2520problems%2520can%2520be%2520effectively%2520modeled%2520using%250Amonotonic%2520Markov%2520Decision%2520Processes%2520%2528MDPs%2529%252C%2520where%2520the%2520system%2520state%250Astochastically%2520decreases%2520and%2520can%2520only%2520be%2520increased%2520by%2520performing%2520a%2520restorative%250Aaction.%2520This%2520work%2520addresses%2520the%2520problem%2520of%2520solving%2520multi-component%2520monotonic%250AMDPs%2520with%2520both%2520budget%2520and%2520capacity%2520constraints.%2520The%2520budget%2520constraint%2520limits%250Athe%2520total%2520number%2520of%2520restorative%2520actions%2520and%2520the%2520capacity%2520constraint%2520limits%2520the%250Anumber%2520of%2520restorative%2520actions%2520that%2520can%2520be%2520performed%2520simultaneously.%2520While%2520prior%250Amethods%2520dealt%2520with%2520budget%2520constraints%252C%2520including%2520capacity%2520constraints%2520in%2520prior%250Amethods%2520leads%2520to%2520an%2520exponential%2520increase%2520in%2520computational%2520complexity%2520as%2520the%250Anumber%2520of%2520components%2520in%2520the%2520MDP%2520grows.%2520We%2520propose%2520a%2520two-step%2520planning%2520approach%250Ato%2520address%2520this%2520challenge.%2520First%252C%2520we%2520partition%2520the%2520components%2520of%2520the%250Amulti-component%2520MDP%2520into%2520groups%252C%2520where%2520the%2520number%2520of%2520groups%2520is%2520determined%2520by%250Athe%2520capacity%2520constraint.%2520We%2520achieve%2520this%2520partitioning%2520by%2520solving%2520a%2520Linear%2520Sum%250AAssignment%2520Problem%2520%2528LSAP%2529.%2520Each%2520group%2520is%2520then%2520allocated%2520a%2520fraction%2520of%2520the%2520total%250Abudget%2520proportional%2520to%2520its%2520size.%2520This%2520partitioning%2520effectively%2520decouples%2520the%250Alarge%2520multi-component%2520MDP%2520into%2520smaller%2520subproblems%252C%2520which%2520are%2520computationally%250Afeasible%2520because%2520the%2520capacity%2520constraint%2520is%2520simplified%2520and%2520the%2520budget%250Aconstraint%2520can%2520be%2520addressed%2520using%2520existing%2520methods.%2520Subsequently%252C%2520we%2520use%2520a%250Ameta-trained%2520PPO%2520agent%2520to%2520obtain%2520an%2520approximately%2520optimal%2520policy%2520for%2520each%250Agroup.%2520To%2520validate%2520our%2520approach%252C%2520we%2520apply%2520it%2520to%2520the%2520problem%2520of%2520scheduling%250Arepairs%2520for%2520a%2520large%2520group%2520of%2520industrial%2520robots%252C%2520constrained%2520by%2520a%2520limited%2520number%250Aof%2520repair%2520technicians%2520and%2520a%2520total%2520repair%2520budget.%2520Our%2520results%2520demonstrate%2520that%250Athe%2520proposed%2520method%2520outperforms%2520baseline%2520approaches%2520in%2520terms%2520of%2520maximizing%2520the%250Aaverage%2520uptime%2520of%2520the%2520robot%2520swarm%252C%2520particularly%2520for%2520large%2520swarm%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capacity-Aware%20Planning%20and%20Scheduling%20in%20Budget-Constrained%20Monotonic%0A%20%20MDPs%3A%20A%20Meta-RL%20Approach&entry.906535625=Manav%20Vora%20and%20Ilan%20Shomorony%20and%20Melkior%20Ornik&entry.1292438233=%20%20Many%20real-world%20sequential%20repair%20problems%20can%20be%20effectively%20modeled%20using%0Amonotonic%20Markov%20Decision%20Processes%20%28MDPs%29%2C%20where%20the%20system%20state%0Astochastically%20decreases%20and%20can%20only%20be%20increased%20by%20performing%20a%20restorative%0Aaction.%20This%20work%20addresses%20the%20problem%20of%20solving%20multi-component%20monotonic%0AMDPs%20with%20both%20budget%20and%20capacity%20constraints.%20The%20budget%20constraint%20limits%0Athe%20total%20number%20of%20restorative%20actions%20and%20the%20capacity%20constraint%20limits%20the%0Anumber%20of%20restorative%20actions%20that%20can%20be%20performed%20simultaneously.%20While%20prior%0Amethods%20dealt%20with%20budget%20constraints%2C%20including%20capacity%20constraints%20in%20prior%0Amethods%20leads%20to%20an%20exponential%20increase%20in%20computational%20complexity%20as%20the%0Anumber%20of%20components%20in%20the%20MDP%20grows.%20We%20propose%20a%20two-step%20planning%20approach%0Ato%20address%20this%20challenge.%20First%2C%20we%20partition%20the%20components%20of%20the%0Amulti-component%20MDP%20into%20groups%2C%20where%20the%20number%20of%20groups%20is%20determined%20by%0Athe%20capacity%20constraint.%20We%20achieve%20this%20partitioning%20by%20solving%20a%20Linear%20Sum%0AAssignment%20Problem%20%28LSAP%29.%20Each%20group%20is%20then%20allocated%20a%20fraction%20of%20the%20total%0Abudget%20proportional%20to%20its%20size.%20This%20partitioning%20effectively%20decouples%20the%0Alarge%20multi-component%20MDP%20into%20smaller%20subproblems%2C%20which%20are%20computationally%0Afeasible%20because%20the%20capacity%20constraint%20is%20simplified%20and%20the%20budget%0Aconstraint%20can%20be%20addressed%20using%20existing%20methods.%20Subsequently%2C%20we%20use%20a%0Ameta-trained%20PPO%20agent%20to%20obtain%20an%20approximately%20optimal%20policy%20for%20each%0Agroup.%20To%20validate%20our%20approach%2C%20we%20apply%20it%20to%20the%20problem%20of%20scheduling%0Arepairs%20for%20a%20large%20group%20of%20industrial%20robots%2C%20constrained%20by%20a%20limited%20number%0Aof%20repair%20technicians%20and%20a%20total%20repair%20budget.%20Our%20results%20demonstrate%20that%0Athe%20proposed%20method%20outperforms%20baseline%20approaches%20in%20terms%20of%20maximizing%20the%0Aaverage%20uptime%20of%20the%20robot%20swarm%2C%20particularly%20for%20large%20swarm%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21249v1&entry.124074799=Read"},
{"title": "Customizing Text-to-Image Models with a Single Image Pair", "author": "Maxwell Jones and Sheng-Yu Wang and Nupur Kumari and David Bau and Jun-Yan Zhu", "abstract": "  Art reinterpretation is the practice of creating a variation of a reference\nwork, making a paired artwork that exhibits a distinct artistic style. We ask\nif such an image pair can be used to customize a generative model to capture\nthe demonstrated stylistic difference. We propose Pair Customization, a new\ncustomization method that learns stylistic difference from a single image pair\nand then applies the acquired style to the generation process. Unlike existing\nmethods that learn to mimic a single concept from a collection of images, our\nmethod captures the stylistic difference between paired images. This allows us\nto apply a stylistic change without overfitting to the specific image content\nin the examples. To address this new task, we employ a joint optimization\nmethod that explicitly separates the style and content into distinct LoRA\nweight spaces. We optimize these style and content weights to reproduce the\nstyle and content images while encouraging their orthogonality. During\ninference, we modify the diffusion process via a new style guidance based on\nour learned weights. Both qualitative and quantitative experiments show that\nour method can effectively learn style while avoiding overfitting to image\ncontent, highlighting the potential of modeling such stylistic differences from\na single image pair.\n", "link": "http://arxiv.org/abs/2405.01536v2", "date": "2024-10-28", "relevancy": 1.1938, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6458}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6103}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Customizing%20Text-to-Image%20Models%20with%20a%20Single%20Image%20Pair&body=Title%3A%20Customizing%20Text-to-Image%20Models%20with%20a%20Single%20Image%20Pair%0AAuthor%3A%20Maxwell%20Jones%20and%20Sheng-Yu%20Wang%20and%20Nupur%20Kumari%20and%20David%20Bau%20and%20Jun-Yan%20Zhu%0AAbstract%3A%20%20%20Art%20reinterpretation%20is%20the%20practice%20of%20creating%20a%20variation%20of%20a%20reference%0Awork%2C%20making%20a%20paired%20artwork%20that%20exhibits%20a%20distinct%20artistic%20style.%20We%20ask%0Aif%20such%20an%20image%20pair%20can%20be%20used%20to%20customize%20a%20generative%20model%20to%20capture%0Athe%20demonstrated%20stylistic%20difference.%20We%20propose%20Pair%20Customization%2C%20a%20new%0Acustomization%20method%20that%20learns%20stylistic%20difference%20from%20a%20single%20image%20pair%0Aand%20then%20applies%20the%20acquired%20style%20to%20the%20generation%20process.%20Unlike%20existing%0Amethods%20that%20learn%20to%20mimic%20a%20single%20concept%20from%20a%20collection%20of%20images%2C%20our%0Amethod%20captures%20the%20stylistic%20difference%20between%20paired%20images.%20This%20allows%20us%0Ato%20apply%20a%20stylistic%20change%20without%20overfitting%20to%20the%20specific%20image%20content%0Ain%20the%20examples.%20To%20address%20this%20new%20task%2C%20we%20employ%20a%20joint%20optimization%0Amethod%20that%20explicitly%20separates%20the%20style%20and%20content%20into%20distinct%20LoRA%0Aweight%20spaces.%20We%20optimize%20these%20style%20and%20content%20weights%20to%20reproduce%20the%0Astyle%20and%20content%20images%20while%20encouraging%20their%20orthogonality.%20During%0Ainference%2C%20we%20modify%20the%20diffusion%20process%20via%20a%20new%20style%20guidance%20based%20on%0Aour%20learned%20weights.%20Both%20qualitative%20and%20quantitative%20experiments%20show%20that%0Aour%20method%20can%20effectively%20learn%20style%20while%20avoiding%20overfitting%20to%20image%0Acontent%2C%20highlighting%20the%20potential%20of%20modeling%20such%20stylistic%20differences%20from%0Aa%20single%20image%20pair.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01536v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCustomizing%2520Text-to-Image%2520Models%2520with%2520a%2520Single%2520Image%2520Pair%26entry.906535625%3DMaxwell%2520Jones%2520and%2520Sheng-Yu%2520Wang%2520and%2520Nupur%2520Kumari%2520and%2520David%2520Bau%2520and%2520Jun-Yan%2520Zhu%26entry.1292438233%3D%2520%2520Art%2520reinterpretation%2520is%2520the%2520practice%2520of%2520creating%2520a%2520variation%2520of%2520a%2520reference%250Awork%252C%2520making%2520a%2520paired%2520artwork%2520that%2520exhibits%2520a%2520distinct%2520artistic%2520style.%2520We%2520ask%250Aif%2520such%2520an%2520image%2520pair%2520can%2520be%2520used%2520to%2520customize%2520a%2520generative%2520model%2520to%2520capture%250Athe%2520demonstrated%2520stylistic%2520difference.%2520We%2520propose%2520Pair%2520Customization%252C%2520a%2520new%250Acustomization%2520method%2520that%2520learns%2520stylistic%2520difference%2520from%2520a%2520single%2520image%2520pair%250Aand%2520then%2520applies%2520the%2520acquired%2520style%2520to%2520the%2520generation%2520process.%2520Unlike%2520existing%250Amethods%2520that%2520learn%2520to%2520mimic%2520a%2520single%2520concept%2520from%2520a%2520collection%2520of%2520images%252C%2520our%250Amethod%2520captures%2520the%2520stylistic%2520difference%2520between%2520paired%2520images.%2520This%2520allows%2520us%250Ato%2520apply%2520a%2520stylistic%2520change%2520without%2520overfitting%2520to%2520the%2520specific%2520image%2520content%250Ain%2520the%2520examples.%2520To%2520address%2520this%2520new%2520task%252C%2520we%2520employ%2520a%2520joint%2520optimization%250Amethod%2520that%2520explicitly%2520separates%2520the%2520style%2520and%2520content%2520into%2520distinct%2520LoRA%250Aweight%2520spaces.%2520We%2520optimize%2520these%2520style%2520and%2520content%2520weights%2520to%2520reproduce%2520the%250Astyle%2520and%2520content%2520images%2520while%2520encouraging%2520their%2520orthogonality.%2520During%250Ainference%252C%2520we%2520modify%2520the%2520diffusion%2520process%2520via%2520a%2520new%2520style%2520guidance%2520based%2520on%250Aour%2520learned%2520weights.%2520Both%2520qualitative%2520and%2520quantitative%2520experiments%2520show%2520that%250Aour%2520method%2520can%2520effectively%2520learn%2520style%2520while%2520avoiding%2520overfitting%2520to%2520image%250Acontent%252C%2520highlighting%2520the%2520potential%2520of%2520modeling%2520such%2520stylistic%2520differences%2520from%250Aa%2520single%2520image%2520pair.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01536v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Customizing%20Text-to-Image%20Models%20with%20a%20Single%20Image%20Pair&entry.906535625=Maxwell%20Jones%20and%20Sheng-Yu%20Wang%20and%20Nupur%20Kumari%20and%20David%20Bau%20and%20Jun-Yan%20Zhu&entry.1292438233=%20%20Art%20reinterpretation%20is%20the%20practice%20of%20creating%20a%20variation%20of%20a%20reference%0Awork%2C%20making%20a%20paired%20artwork%20that%20exhibits%20a%20distinct%20artistic%20style.%20We%20ask%0Aif%20such%20an%20image%20pair%20can%20be%20used%20to%20customize%20a%20generative%20model%20to%20capture%0Athe%20demonstrated%20stylistic%20difference.%20We%20propose%20Pair%20Customization%2C%20a%20new%0Acustomization%20method%20that%20learns%20stylistic%20difference%20from%20a%20single%20image%20pair%0Aand%20then%20applies%20the%20acquired%20style%20to%20the%20generation%20process.%20Unlike%20existing%0Amethods%20that%20learn%20to%20mimic%20a%20single%20concept%20from%20a%20collection%20of%20images%2C%20our%0Amethod%20captures%20the%20stylistic%20difference%20between%20paired%20images.%20This%20allows%20us%0Ato%20apply%20a%20stylistic%20change%20without%20overfitting%20to%20the%20specific%20image%20content%0Ain%20the%20examples.%20To%20address%20this%20new%20task%2C%20we%20employ%20a%20joint%20optimization%0Amethod%20that%20explicitly%20separates%20the%20style%20and%20content%20into%20distinct%20LoRA%0Aweight%20spaces.%20We%20optimize%20these%20style%20and%20content%20weights%20to%20reproduce%20the%0Astyle%20and%20content%20images%20while%20encouraging%20their%20orthogonality.%20During%0Ainference%2C%20we%20modify%20the%20diffusion%20process%20via%20a%20new%20style%20guidance%20based%20on%0Aour%20learned%20weights.%20Both%20qualitative%20and%20quantitative%20experiments%20show%20that%0Aour%20method%20can%20effectively%20learn%20style%20while%20avoiding%20overfitting%20to%20image%0Acontent%2C%20highlighting%20the%20potential%20of%20modeling%20such%20stylistic%20differences%20from%0Aa%20single%20image%20pair.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01536v2&entry.124074799=Read"},
{"title": "$\\texttt{skwdro}$: a library for Wasserstein distributionally robust\n  machine learning", "author": "Florian Vincent and Wa\u00efss Azizian and Franck Iutzeler and J\u00e9r\u00f4me Malick", "abstract": "  We present skwdro, a Python library for training robust machine learning\nmodels. The library is based on distributionally robust optimization using\noptimal transport distances. For ease of use, it features both scikit-learn\ncompatible estimators for popular objectives, as well as a wrapper for PyTorch\nmodules, enabling researchers and practitioners to use it in a wide range of\nmodels with minimal code changes. Its implementation relies on an entropic\nsmoothing of the original robust objective in order to ensure maximal model\nflexibility. The library is available at https://github.com/iutzeler/skwdro\n", "link": "http://arxiv.org/abs/2410.21231v1", "date": "2024-10-28", "relevancy": 0.8482, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4388}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4175}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Ctexttt%7Bskwdro%7D%24%3A%20a%20library%20for%20Wasserstein%20distributionally%20robust%0A%20%20machine%20learning&body=Title%3A%20%24%5Ctexttt%7Bskwdro%7D%24%3A%20a%20library%20for%20Wasserstein%20distributionally%20robust%0A%20%20machine%20learning%0AAuthor%3A%20Florian%20Vincent%20and%20Wa%C3%AFss%20Azizian%20and%20Franck%20Iutzeler%20and%20J%C3%A9r%C3%B4me%20Malick%0AAbstract%3A%20%20%20We%20present%20skwdro%2C%20a%20Python%20library%20for%20training%20robust%20machine%20learning%0Amodels.%20The%20library%20is%20based%20on%20distributionally%20robust%20optimization%20using%0Aoptimal%20transport%20distances.%20For%20ease%20of%20use%2C%20it%20features%20both%20scikit-learn%0Acompatible%20estimators%20for%20popular%20objectives%2C%20as%20well%20as%20a%20wrapper%20for%20PyTorch%0Amodules%2C%20enabling%20researchers%20and%20practitioners%20to%20use%20it%20in%20a%20wide%20range%20of%0Amodels%20with%20minimal%20code%20changes.%20Its%20implementation%20relies%20on%20an%20entropic%0Asmoothing%20of%20the%20original%20robust%20objective%20in%20order%20to%20ensure%20maximal%20model%0Aflexibility.%20The%20library%20is%20available%20at%20https%3A//github.com/iutzeler/skwdro%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Ctexttt%257Bskwdro%257D%2524%253A%2520a%2520library%2520for%2520Wasserstein%2520distributionally%2520robust%250A%2520%2520machine%2520learning%26entry.906535625%3DFlorian%2520Vincent%2520and%2520Wa%25C3%25AFss%2520Azizian%2520and%2520Franck%2520Iutzeler%2520and%2520J%25C3%25A9r%25C3%25B4me%2520Malick%26entry.1292438233%3D%2520%2520We%2520present%2520skwdro%252C%2520a%2520Python%2520library%2520for%2520training%2520robust%2520machine%2520learning%250Amodels.%2520The%2520library%2520is%2520based%2520on%2520distributionally%2520robust%2520optimization%2520using%250Aoptimal%2520transport%2520distances.%2520For%2520ease%2520of%2520use%252C%2520it%2520features%2520both%2520scikit-learn%250Acompatible%2520estimators%2520for%2520popular%2520objectives%252C%2520as%2520well%2520as%2520a%2520wrapper%2520for%2520PyTorch%250Amodules%252C%2520enabling%2520researchers%2520and%2520practitioners%2520to%2520use%2520it%2520in%2520a%2520wide%2520range%2520of%250Amodels%2520with%2520minimal%2520code%2520changes.%2520Its%2520implementation%2520relies%2520on%2520an%2520entropic%250Asmoothing%2520of%2520the%2520original%2520robust%2520objective%2520in%2520order%2520to%2520ensure%2520maximal%2520model%250Aflexibility.%2520The%2520library%2520is%2520available%2520at%2520https%253A//github.com/iutzeler/skwdro%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Ctexttt%7Bskwdro%7D%24%3A%20a%20library%20for%20Wasserstein%20distributionally%20robust%0A%20%20machine%20learning&entry.906535625=Florian%20Vincent%20and%20Wa%C3%AFss%20Azizian%20and%20Franck%20Iutzeler%20and%20J%C3%A9r%C3%B4me%20Malick&entry.1292438233=%20%20We%20present%20skwdro%2C%20a%20Python%20library%20for%20training%20robust%20machine%20learning%0Amodels.%20The%20library%20is%20based%20on%20distributionally%20robust%20optimization%20using%0Aoptimal%20transport%20distances.%20For%20ease%20of%20use%2C%20it%20features%20both%20scikit-learn%0Acompatible%20estimators%20for%20popular%20objectives%2C%20as%20well%20as%20a%20wrapper%20for%20PyTorch%0Amodules%2C%20enabling%20researchers%20and%20practitioners%20to%20use%20it%20in%20a%20wide%20range%20of%0Amodels%20with%20minimal%20code%20changes.%20Its%20implementation%20relies%20on%20an%20entropic%0Asmoothing%20of%20the%20original%20robust%20objective%20in%20order%20to%20ensure%20maximal%20model%0Aflexibility.%20The%20library%20is%20available%20at%20https%3A//github.com/iutzeler/skwdro%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21231v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


