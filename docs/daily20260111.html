<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260108.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction", "author": "Minseong Kweon and Jinsun Park", "abstract": "We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for accurately representing 3D geometry in underwater scenes. To overcome multi-view inconsistencies caused by underwater optical degradation, our method enforces trinocular view consistency by rendering horizontally and vertically translated camera views relative to each input view and aligning them via inverse warping. Furthermore, these translated camera views are used to derive a synthetic epipolar depth prior through triangulation, which serves as a self-supervised depth regularizer. These geometric constraints facilitate the spatial optimization of 3D Gaussians and preserve scene structure in underwater environments. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their $z$-component and viewing direction, deterring the formation of medium-induced primitives. With our contributions, 3D Gaussians are disentangled from the scattering medium, enabling robust representation of object geometry and significantly reducing floating artifacts in reconstructed underwater scenes. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.", "link": "http://arxiv.org/abs/2601.04984v1", "date": "2026-01-08", "relevancy": 3.2677, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6954}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6488}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OceanSplat%3A%20Object-aware%20Gaussian%20Splatting%20with%20Trinocular%20View%20Consistency%20for%20Underwater%20Scene%20Reconstruction&body=Title%3A%20OceanSplat%3A%20Object-aware%20Gaussian%20Splatting%20with%20Trinocular%20View%20Consistency%20for%20Underwater%20Scene%20Reconstruction%0AAuthor%3A%20Minseong%20Kweon%20and%20Jinsun%20Park%0AAbstract%3A%20We%20introduce%20OceanSplat%2C%20a%20novel%203D%20Gaussian%20Splatting-based%20approach%20for%20accurately%20representing%203D%20geometry%20in%20underwater%20scenes.%20To%20overcome%20multi-view%20inconsistencies%20caused%20by%20underwater%20optical%20degradation%2C%20our%20method%20enforces%20trinocular%20view%20consistency%20by%20rendering%20horizontally%20and%20vertically%20translated%20camera%20views%20relative%20to%20each%20input%20view%20and%20aligning%20them%20via%20inverse%20warping.%20Furthermore%2C%20these%20translated%20camera%20views%20are%20used%20to%20derive%20a%20synthetic%20epipolar%20depth%20prior%20through%20triangulation%2C%20which%20serves%20as%20a%20self-supervised%20depth%20regularizer.%20These%20geometric%20constraints%20facilitate%20the%20spatial%20optimization%20of%203D%20Gaussians%20and%20preserve%20scene%20structure%20in%20underwater%20environments.%20We%20also%20propose%20a%20depth-aware%20alpha%20adjustment%20that%20modulates%20the%20opacity%20of%203D%20Gaussians%20during%20early%20training%20based%20on%20their%20%24z%24-component%20and%20viewing%20direction%2C%20deterring%20the%20formation%20of%20medium-induced%20primitives.%20With%20our%20contributions%2C%203D%20Gaussians%20are%20disentangled%20from%20the%20scattering%20medium%2C%20enabling%20robust%20representation%20of%20object%20geometry%20and%20significantly%20reducing%20floating%20artifacts%20in%20reconstructed%20underwater%20scenes.%20Experiments%20on%20real-world%20underwater%20and%20simulated%20scenes%20demonstrate%20that%20OceanSplat%20substantially%20outperforms%20existing%20methods%20for%20both%20scene%20reconstruction%20and%20restoration%20in%20scattering%20media.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOceanSplat%253A%2520Object-aware%2520Gaussian%2520Splatting%2520with%2520Trinocular%2520View%2520Consistency%2520for%2520Underwater%2520Scene%2520Reconstruction%26entry.906535625%3DMinseong%2520Kweon%2520and%2520Jinsun%2520Park%26entry.1292438233%3DWe%2520introduce%2520OceanSplat%252C%2520a%2520novel%25203D%2520Gaussian%2520Splatting-based%2520approach%2520for%2520accurately%2520representing%25203D%2520geometry%2520in%2520underwater%2520scenes.%2520To%2520overcome%2520multi-view%2520inconsistencies%2520caused%2520by%2520underwater%2520optical%2520degradation%252C%2520our%2520method%2520enforces%2520trinocular%2520view%2520consistency%2520by%2520rendering%2520horizontally%2520and%2520vertically%2520translated%2520camera%2520views%2520relative%2520to%2520each%2520input%2520view%2520and%2520aligning%2520them%2520via%2520inverse%2520warping.%2520Furthermore%252C%2520these%2520translated%2520camera%2520views%2520are%2520used%2520to%2520derive%2520a%2520synthetic%2520epipolar%2520depth%2520prior%2520through%2520triangulation%252C%2520which%2520serves%2520as%2520a%2520self-supervised%2520depth%2520regularizer.%2520These%2520geometric%2520constraints%2520facilitate%2520the%2520spatial%2520optimization%2520of%25203D%2520Gaussians%2520and%2520preserve%2520scene%2520structure%2520in%2520underwater%2520environments.%2520We%2520also%2520propose%2520a%2520depth-aware%2520alpha%2520adjustment%2520that%2520modulates%2520the%2520opacity%2520of%25203D%2520Gaussians%2520during%2520early%2520training%2520based%2520on%2520their%2520%2524z%2524-component%2520and%2520viewing%2520direction%252C%2520deterring%2520the%2520formation%2520of%2520medium-induced%2520primitives.%2520With%2520our%2520contributions%252C%25203D%2520Gaussians%2520are%2520disentangled%2520from%2520the%2520scattering%2520medium%252C%2520enabling%2520robust%2520representation%2520of%2520object%2520geometry%2520and%2520significantly%2520reducing%2520floating%2520artifacts%2520in%2520reconstructed%2520underwater%2520scenes.%2520Experiments%2520on%2520real-world%2520underwater%2520and%2520simulated%2520scenes%2520demonstrate%2520that%2520OceanSplat%2520substantially%2520outperforms%2520existing%2520methods%2520for%2520both%2520scene%2520reconstruction%2520and%2520restoration%2520in%2520scattering%2520media.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OceanSplat%3A%20Object-aware%20Gaussian%20Splatting%20with%20Trinocular%20View%20Consistency%20for%20Underwater%20Scene%20Reconstruction&entry.906535625=Minseong%20Kweon%20and%20Jinsun%20Park&entry.1292438233=We%20introduce%20OceanSplat%2C%20a%20novel%203D%20Gaussian%20Splatting-based%20approach%20for%20accurately%20representing%203D%20geometry%20in%20underwater%20scenes.%20To%20overcome%20multi-view%20inconsistencies%20caused%20by%20underwater%20optical%20degradation%2C%20our%20method%20enforces%20trinocular%20view%20consistency%20by%20rendering%20horizontally%20and%20vertically%20translated%20camera%20views%20relative%20to%20each%20input%20view%20and%20aligning%20them%20via%20inverse%20warping.%20Furthermore%2C%20these%20translated%20camera%20views%20are%20used%20to%20derive%20a%20synthetic%20epipolar%20depth%20prior%20through%20triangulation%2C%20which%20serves%20as%20a%20self-supervised%20depth%20regularizer.%20These%20geometric%20constraints%20facilitate%20the%20spatial%20optimization%20of%203D%20Gaussians%20and%20preserve%20scene%20structure%20in%20underwater%20environments.%20We%20also%20propose%20a%20depth-aware%20alpha%20adjustment%20that%20modulates%20the%20opacity%20of%203D%20Gaussians%20during%20early%20training%20based%20on%20their%20%24z%24-component%20and%20viewing%20direction%2C%20deterring%20the%20formation%20of%20medium-induced%20primitives.%20With%20our%20contributions%2C%203D%20Gaussians%20are%20disentangled%20from%20the%20scattering%20medium%2C%20enabling%20robust%20representation%20of%20object%20geometry%20and%20significantly%20reducing%20floating%20artifacts%20in%20reconstructed%20underwater%20scenes.%20Experiments%20on%20real-world%20underwater%20and%20simulated%20scenes%20demonstrate%20that%20OceanSplat%20substantially%20outperforms%20existing%20methods%20for%20both%20scene%20reconstruction%20and%20restoration%20in%20scattering%20media.&entry.1838667208=http%3A//arxiv.org/abs/2601.04984v1&entry.124074799=Read"},
{"title": "Is Contrastive Distillation Enough for Learning Comprehensive 3D Representations?", "author": "Yifan Zhang and Junhui Hou", "abstract": "Cross-modal contrastive distillation has recently been explored for learning effective 3D representations. However, existing methods focus primarily on modality-shared features, neglecting the modality-specific features during the pre-training process, which leads to suboptimal representations. In this paper, we theoretically analyze the limitations of current contrastive methods for 3D representation learning and propose a new framework, namely CMCR (Cross-Modal Comprehensive Representation Learning), to address these shortcomings. Our approach improves upon traditional methods by better integrating both modality-shared and modality-specific features. Specifically, we introduce masked image modeling and occupancy estimation tasks to guide the network in learning more comprehensive modality-specific features. Furthermore, we propose a novel multi-modal unified codebook that learns an embedding space shared across different modalities. Besides, we introduce geometry-enhanced masked image modeling to further boost 3D representation learning. Extensive experiments demonstrate that our method mitigates the challenges faced by traditional approaches and consistently outperforms existing image-to-LiDAR contrastive distillation methods in downstream tasks. Code will be available at https://github.com/Eaphan/CMCR.", "link": "http://arxiv.org/abs/2412.08973v3", "date": "2026-01-08", "relevancy": 3.0308, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.639}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Contrastive%20Distillation%20Enough%20for%20Learning%20Comprehensive%203D%20Representations%3F&body=Title%3A%20Is%20Contrastive%20Distillation%20Enough%20for%20Learning%20Comprehensive%203D%20Representations%3F%0AAuthor%3A%20Yifan%20Zhang%20and%20Junhui%20Hou%0AAbstract%3A%20Cross-modal%20contrastive%20distillation%20has%20recently%20been%20explored%20for%20learning%20effective%203D%20representations.%20However%2C%20existing%20methods%20focus%20primarily%20on%20modality-shared%20features%2C%20neglecting%20the%20modality-specific%20features%20during%20the%20pre-training%20process%2C%20which%20leads%20to%20suboptimal%20representations.%20In%20this%20paper%2C%20we%20theoretically%20analyze%20the%20limitations%20of%20current%20contrastive%20methods%20for%203D%20representation%20learning%20and%20propose%20a%20new%20framework%2C%20namely%20CMCR%20%28Cross-Modal%20Comprehensive%20Representation%20Learning%29%2C%20to%20address%20these%20shortcomings.%20Our%20approach%20improves%20upon%20traditional%20methods%20by%20better%20integrating%20both%20modality-shared%20and%20modality-specific%20features.%20Specifically%2C%20we%20introduce%20masked%20image%20modeling%20and%20occupancy%20estimation%20tasks%20to%20guide%20the%20network%20in%20learning%20more%20comprehensive%20modality-specific%20features.%20Furthermore%2C%20we%20propose%20a%20novel%20multi-modal%20unified%20codebook%20that%20learns%20an%20embedding%20space%20shared%20across%20different%20modalities.%20Besides%2C%20we%20introduce%20geometry-enhanced%20masked%20image%20modeling%20to%20further%20boost%203D%20representation%20learning.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20mitigates%20the%20challenges%20faced%20by%20traditional%20approaches%20and%20consistently%20outperforms%20existing%20image-to-LiDAR%20contrastive%20distillation%20methods%20in%20downstream%20tasks.%20Code%20will%20be%20available%20at%20https%3A//github.com/Eaphan/CMCR.%0ALink%3A%20http%3A//arxiv.org/abs/2412.08973v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Contrastive%2520Distillation%2520Enough%2520for%2520Learning%2520Comprehensive%25203D%2520Representations%253F%26entry.906535625%3DYifan%2520Zhang%2520and%2520Junhui%2520Hou%26entry.1292438233%3DCross-modal%2520contrastive%2520distillation%2520has%2520recently%2520been%2520explored%2520for%2520learning%2520effective%25203D%2520representations.%2520However%252C%2520existing%2520methods%2520focus%2520primarily%2520on%2520modality-shared%2520features%252C%2520neglecting%2520the%2520modality-specific%2520features%2520during%2520the%2520pre-training%2520process%252C%2520which%2520leads%2520to%2520suboptimal%2520representations.%2520In%2520this%2520paper%252C%2520we%2520theoretically%2520analyze%2520the%2520limitations%2520of%2520current%2520contrastive%2520methods%2520for%25203D%2520representation%2520learning%2520and%2520propose%2520a%2520new%2520framework%252C%2520namely%2520CMCR%2520%2528Cross-Modal%2520Comprehensive%2520Representation%2520Learning%2529%252C%2520to%2520address%2520these%2520shortcomings.%2520Our%2520approach%2520improves%2520upon%2520traditional%2520methods%2520by%2520better%2520integrating%2520both%2520modality-shared%2520and%2520modality-specific%2520features.%2520Specifically%252C%2520we%2520introduce%2520masked%2520image%2520modeling%2520and%2520occupancy%2520estimation%2520tasks%2520to%2520guide%2520the%2520network%2520in%2520learning%2520more%2520comprehensive%2520modality-specific%2520features.%2520Furthermore%252C%2520we%2520propose%2520a%2520novel%2520multi-modal%2520unified%2520codebook%2520that%2520learns%2520an%2520embedding%2520space%2520shared%2520across%2520different%2520modalities.%2520Besides%252C%2520we%2520introduce%2520geometry-enhanced%2520masked%2520image%2520modeling%2520to%2520further%2520boost%25203D%2520representation%2520learning.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520mitigates%2520the%2520challenges%2520faced%2520by%2520traditional%2520approaches%2520and%2520consistently%2520outperforms%2520existing%2520image-to-LiDAR%2520contrastive%2520distillation%2520methods%2520in%2520downstream%2520tasks.%2520Code%2520will%2520be%2520available%2520at%2520https%253A//github.com/Eaphan/CMCR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08973v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Contrastive%20Distillation%20Enough%20for%20Learning%20Comprehensive%203D%20Representations%3F&entry.906535625=Yifan%20Zhang%20and%20Junhui%20Hou&entry.1292438233=Cross-modal%20contrastive%20distillation%20has%20recently%20been%20explored%20for%20learning%20effective%203D%20representations.%20However%2C%20existing%20methods%20focus%20primarily%20on%20modality-shared%20features%2C%20neglecting%20the%20modality-specific%20features%20during%20the%20pre-training%20process%2C%20which%20leads%20to%20suboptimal%20representations.%20In%20this%20paper%2C%20we%20theoretically%20analyze%20the%20limitations%20of%20current%20contrastive%20methods%20for%203D%20representation%20learning%20and%20propose%20a%20new%20framework%2C%20namely%20CMCR%20%28Cross-Modal%20Comprehensive%20Representation%20Learning%29%2C%20to%20address%20these%20shortcomings.%20Our%20approach%20improves%20upon%20traditional%20methods%20by%20better%20integrating%20both%20modality-shared%20and%20modality-specific%20features.%20Specifically%2C%20we%20introduce%20masked%20image%20modeling%20and%20occupancy%20estimation%20tasks%20to%20guide%20the%20network%20in%20learning%20more%20comprehensive%20modality-specific%20features.%20Furthermore%2C%20we%20propose%20a%20novel%20multi-modal%20unified%20codebook%20that%20learns%20an%20embedding%20space%20shared%20across%20different%20modalities.%20Besides%2C%20we%20introduce%20geometry-enhanced%20masked%20image%20modeling%20to%20further%20boost%203D%20representation%20learning.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20mitigates%20the%20challenges%20faced%20by%20traditional%20approaches%20and%20consistently%20outperforms%20existing%20image-to-LiDAR%20contrastive%20distillation%20methods%20in%20downstream%20tasks.%20Code%20will%20be%20available%20at%20https%3A//github.com/Eaphan/CMCR.&entry.1838667208=http%3A//arxiv.org/abs/2412.08973v3&entry.124074799=Read"},
{"title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding", "author": "Ignacio de Rodrigo and Alvaro J. Lopez-Lopez and Jaime Boal", "abstract": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.", "link": "http://arxiv.org/abs/2601.05125v1", "date": "2026-01-08", "relevancy": 3.0202, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6338}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6338}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VERSE%3A%20Visual%20Embedding%20Reduction%20and%20Space%20Exploration.%20Clustering-Guided%20Insights%20for%20Training%20Data%20Enhancement%20in%20Visually-Rich%20Document%20Understanding&body=Title%3A%20VERSE%3A%20Visual%20Embedding%20Reduction%20and%20Space%20Exploration.%20Clustering-Guided%20Insights%20for%20Training%20Data%20Enhancement%20in%20Visually-Rich%20Document%20Understanding%0AAuthor%3A%20Ignacio%20de%20Rodrigo%20and%20Alvaro%20J.%20Lopez-Lopez%20and%20Jaime%20Boal%0AAbstract%3A%20This%20work%20introduces%20VERSE%2C%20a%20methodology%20for%20analyzing%20and%20improving%20Vision-Language%20Models%20applied%20to%20Visually-rich%20Document%20Understanding%20by%20exploring%20their%20visual%20embedding%20space.%20VERSE%20enables%20the%20visualization%20of%20latent%20representations%2C%20supporting%20the%20assessment%20of%20model%20feasibility.%20It%20also%20facilitates%20the%20identification%20of%20problematic%20regions%20and%20guides%20the%20generation%20of%20synthetic%20data%20to%20enhance%20performance%20in%20those%20clusters.%20We%20validate%20the%20methodology%20by%20training%20on%20the%20synthetic%20MERIT%20Dataset%20and%20evaluating%20on%20its%20real-world%20counterpart%2C%20MERIT%20Secret.%20Results%20show%20that%20VERSE%20helps%20uncover%20the%20visual%20features%20associated%20with%20error-prone%20clusters%2C%20and%20that%20retraining%20with%20samples%20containing%20these%20features%20substantially%20boosts%20F1%20performance%20without%20degrading%20generalization.%20Furthermore%2C%20we%20demonstrate%20that%20on-premise%20models%20such%20as%20Donut%20and%20Idefics2%2C%20when%20optimized%20with%20VERSE%2C%20match%20or%20even%20surpass%20the%20performance%20of%20SaaS%20solutions%20like%20GPT-4%20and%20Pixtral.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVERSE%253A%2520Visual%2520Embedding%2520Reduction%2520and%2520Space%2520Exploration.%2520Clustering-Guided%2520Insights%2520for%2520Training%2520Data%2520Enhancement%2520in%2520Visually-Rich%2520Document%2520Understanding%26entry.906535625%3DIgnacio%2520de%2520Rodrigo%2520and%2520Alvaro%2520J.%2520Lopez-Lopez%2520and%2520Jaime%2520Boal%26entry.1292438233%3DThis%2520work%2520introduces%2520VERSE%252C%2520a%2520methodology%2520for%2520analyzing%2520and%2520improving%2520Vision-Language%2520Models%2520applied%2520to%2520Visually-rich%2520Document%2520Understanding%2520by%2520exploring%2520their%2520visual%2520embedding%2520space.%2520VERSE%2520enables%2520the%2520visualization%2520of%2520latent%2520representations%252C%2520supporting%2520the%2520assessment%2520of%2520model%2520feasibility.%2520It%2520also%2520facilitates%2520the%2520identification%2520of%2520problematic%2520regions%2520and%2520guides%2520the%2520generation%2520of%2520synthetic%2520data%2520to%2520enhance%2520performance%2520in%2520those%2520clusters.%2520We%2520validate%2520the%2520methodology%2520by%2520training%2520on%2520the%2520synthetic%2520MERIT%2520Dataset%2520and%2520evaluating%2520on%2520its%2520real-world%2520counterpart%252C%2520MERIT%2520Secret.%2520Results%2520show%2520that%2520VERSE%2520helps%2520uncover%2520the%2520visual%2520features%2520associated%2520with%2520error-prone%2520clusters%252C%2520and%2520that%2520retraining%2520with%2520samples%2520containing%2520these%2520features%2520substantially%2520boosts%2520F1%2520performance%2520without%2520degrading%2520generalization.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520on-premise%2520models%2520such%2520as%2520Donut%2520and%2520Idefics2%252C%2520when%2520optimized%2520with%2520VERSE%252C%2520match%2520or%2520even%2520surpass%2520the%2520performance%2520of%2520SaaS%2520solutions%2520like%2520GPT-4%2520and%2520Pixtral.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VERSE%3A%20Visual%20Embedding%20Reduction%20and%20Space%20Exploration.%20Clustering-Guided%20Insights%20for%20Training%20Data%20Enhancement%20in%20Visually-Rich%20Document%20Understanding&entry.906535625=Ignacio%20de%20Rodrigo%20and%20Alvaro%20J.%20Lopez-Lopez%20and%20Jaime%20Boal&entry.1292438233=This%20work%20introduces%20VERSE%2C%20a%20methodology%20for%20analyzing%20and%20improving%20Vision-Language%20Models%20applied%20to%20Visually-rich%20Document%20Understanding%20by%20exploring%20their%20visual%20embedding%20space.%20VERSE%20enables%20the%20visualization%20of%20latent%20representations%2C%20supporting%20the%20assessment%20of%20model%20feasibility.%20It%20also%20facilitates%20the%20identification%20of%20problematic%20regions%20and%20guides%20the%20generation%20of%20synthetic%20data%20to%20enhance%20performance%20in%20those%20clusters.%20We%20validate%20the%20methodology%20by%20training%20on%20the%20synthetic%20MERIT%20Dataset%20and%20evaluating%20on%20its%20real-world%20counterpart%2C%20MERIT%20Secret.%20Results%20show%20that%20VERSE%20helps%20uncover%20the%20visual%20features%20associated%20with%20error-prone%20clusters%2C%20and%20that%20retraining%20with%20samples%20containing%20these%20features%20substantially%20boosts%20F1%20performance%20without%20degrading%20generalization.%20Furthermore%2C%20we%20demonstrate%20that%20on-premise%20models%20such%20as%20Donut%20and%20Idefics2%2C%20when%20optimized%20with%20VERSE%2C%20match%20or%20even%20surpass%20the%20performance%20of%20SaaS%20solutions%20like%20GPT-4%20and%20Pixtral.&entry.1838667208=http%3A//arxiv.org/abs/2601.05125v1&entry.124074799=Read"},
{"title": "Pixel-Perfect Visual Geometry Estimation", "author": "Gangwei Xu and Haotong Lin and Hongcheng Luo and Haiyang Sun and Bing Wang and Guang Chen and Sida Peng and Hangjun Ye and Xin Yang", "abstract": "Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.", "link": "http://arxiv.org/abs/2601.05246v1", "date": "2026-01-08", "relevancy": 2.9802, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6217}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.587}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixel-Perfect%20Visual%20Geometry%20Estimation&body=Title%3A%20Pixel-Perfect%20Visual%20Geometry%20Estimation%0AAuthor%3A%20Gangwei%20Xu%20and%20Haotong%20Lin%20and%20Hongcheng%20Luo%20and%20Haiyang%20Sun%20and%20Bing%20Wang%20and%20Guang%20Chen%20and%20Sida%20Peng%20and%20Hangjun%20Ye%20and%20Xin%20Yang%0AAbstract%3A%20Recovering%20clean%20and%20accurate%20geometry%20from%20images%20is%20essential%20for%20robotics%20and%20augmented%20reality.%20However%2C%20existing%20geometry%20foundation%20models%20still%20suffer%20severely%20from%20flying%20pixels%20and%20the%20loss%20of%20fine%20details.%20In%20this%20paper%2C%20we%20present%20pixel-perfect%20visual%20geometry%20models%20that%20can%20predict%20high-quality%2C%20flying-pixel-free%20point%20clouds%20by%20leveraging%20generative%20modeling%20in%20the%20pixel%20space.%20We%20first%20introduce%20Pixel-Perfect%20Depth%20%28PPD%29%2C%20a%20monocular%20depth%20foundation%20model%20built%20upon%20pixel-space%20diffusion%20transformers%20%28DiT%29.%20To%20address%20the%20high%20computational%20complexity%20associated%20with%20pixel-space%20diffusion%2C%20we%20propose%20two%20key%20designs%3A%201%29%20Semantics-Prompted%20DiT%2C%20which%20incorporates%20semantic%20representations%20from%20vision%20foundation%20models%20to%20prompt%20the%20diffusion%20process%2C%20preserving%20global%20semantics%20while%20enhancing%20fine-grained%20visual%20details%3B%20and%202%29%20Cascade%20DiT%20architecture%20that%20progressively%20increases%20the%20number%20of%20image%20tokens%2C%20improving%20both%20efficiency%20and%20accuracy.%20To%20further%20extend%20PPD%20to%20video%20%28PPVD%29%2C%20we%20introduce%20a%20new%20Semantics-Consistent%20DiT%2C%20which%20extracts%20temporally%20consistent%20semantics%20from%20a%20multi-view%20geometry%20foundation%20model.%20We%20then%20perform%20reference-guided%20token%20propagation%20within%20the%20DiT%20to%20maintain%20temporal%20coherence%20with%20minimal%20computational%20and%20memory%20overhead.%20Our%20models%20achieve%20the%20best%20performance%20among%20all%20generative%20monocular%20and%20video%20depth%20estimation%20models%20and%20produce%20significantly%20cleaner%20point%20clouds%20than%20all%20other%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixel-Perfect%2520Visual%2520Geometry%2520Estimation%26entry.906535625%3DGangwei%2520Xu%2520and%2520Haotong%2520Lin%2520and%2520Hongcheng%2520Luo%2520and%2520Haiyang%2520Sun%2520and%2520Bing%2520Wang%2520and%2520Guang%2520Chen%2520and%2520Sida%2520Peng%2520and%2520Hangjun%2520Ye%2520and%2520Xin%2520Yang%26entry.1292438233%3DRecovering%2520clean%2520and%2520accurate%2520geometry%2520from%2520images%2520is%2520essential%2520for%2520robotics%2520and%2520augmented%2520reality.%2520However%252C%2520existing%2520geometry%2520foundation%2520models%2520still%2520suffer%2520severely%2520from%2520flying%2520pixels%2520and%2520the%2520loss%2520of%2520fine%2520details.%2520In%2520this%2520paper%252C%2520we%2520present%2520pixel-perfect%2520visual%2520geometry%2520models%2520that%2520can%2520predict%2520high-quality%252C%2520flying-pixel-free%2520point%2520clouds%2520by%2520leveraging%2520generative%2520modeling%2520in%2520the%2520pixel%2520space.%2520We%2520first%2520introduce%2520Pixel-Perfect%2520Depth%2520%2528PPD%2529%252C%2520a%2520monocular%2520depth%2520foundation%2520model%2520built%2520upon%2520pixel-space%2520diffusion%2520transformers%2520%2528DiT%2529.%2520To%2520address%2520the%2520high%2520computational%2520complexity%2520associated%2520with%2520pixel-space%2520diffusion%252C%2520we%2520propose%2520two%2520key%2520designs%253A%25201%2529%2520Semantics-Prompted%2520DiT%252C%2520which%2520incorporates%2520semantic%2520representations%2520from%2520vision%2520foundation%2520models%2520to%2520prompt%2520the%2520diffusion%2520process%252C%2520preserving%2520global%2520semantics%2520while%2520enhancing%2520fine-grained%2520visual%2520details%253B%2520and%25202%2529%2520Cascade%2520DiT%2520architecture%2520that%2520progressively%2520increases%2520the%2520number%2520of%2520image%2520tokens%252C%2520improving%2520both%2520efficiency%2520and%2520accuracy.%2520To%2520further%2520extend%2520PPD%2520to%2520video%2520%2528PPVD%2529%252C%2520we%2520introduce%2520a%2520new%2520Semantics-Consistent%2520DiT%252C%2520which%2520extracts%2520temporally%2520consistent%2520semantics%2520from%2520a%2520multi-view%2520geometry%2520foundation%2520model.%2520We%2520then%2520perform%2520reference-guided%2520token%2520propagation%2520within%2520the%2520DiT%2520to%2520maintain%2520temporal%2520coherence%2520with%2520minimal%2520computational%2520and%2520memory%2520overhead.%2520Our%2520models%2520achieve%2520the%2520best%2520performance%2520among%2520all%2520generative%2520monocular%2520and%2520video%2520depth%2520estimation%2520models%2520and%2520produce%2520significantly%2520cleaner%2520point%2520clouds%2520than%2520all%2520other%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel-Perfect%20Visual%20Geometry%20Estimation&entry.906535625=Gangwei%20Xu%20and%20Haotong%20Lin%20and%20Hongcheng%20Luo%20and%20Haiyang%20Sun%20and%20Bing%20Wang%20and%20Guang%20Chen%20and%20Sida%20Peng%20and%20Hangjun%20Ye%20and%20Xin%20Yang&entry.1292438233=Recovering%20clean%20and%20accurate%20geometry%20from%20images%20is%20essential%20for%20robotics%20and%20augmented%20reality.%20However%2C%20existing%20geometry%20foundation%20models%20still%20suffer%20severely%20from%20flying%20pixels%20and%20the%20loss%20of%20fine%20details.%20In%20this%20paper%2C%20we%20present%20pixel-perfect%20visual%20geometry%20models%20that%20can%20predict%20high-quality%2C%20flying-pixel-free%20point%20clouds%20by%20leveraging%20generative%20modeling%20in%20the%20pixel%20space.%20We%20first%20introduce%20Pixel-Perfect%20Depth%20%28PPD%29%2C%20a%20monocular%20depth%20foundation%20model%20built%20upon%20pixel-space%20diffusion%20transformers%20%28DiT%29.%20To%20address%20the%20high%20computational%20complexity%20associated%20with%20pixel-space%20diffusion%2C%20we%20propose%20two%20key%20designs%3A%201%29%20Semantics-Prompted%20DiT%2C%20which%20incorporates%20semantic%20representations%20from%20vision%20foundation%20models%20to%20prompt%20the%20diffusion%20process%2C%20preserving%20global%20semantics%20while%20enhancing%20fine-grained%20visual%20details%3B%20and%202%29%20Cascade%20DiT%20architecture%20that%20progressively%20increases%20the%20number%20of%20image%20tokens%2C%20improving%20both%20efficiency%20and%20accuracy.%20To%20further%20extend%20PPD%20to%20video%20%28PPVD%29%2C%20we%20introduce%20a%20new%20Semantics-Consistent%20DiT%2C%20which%20extracts%20temporally%20consistent%20semantics%20from%20a%20multi-view%20geometry%20foundation%20model.%20We%20then%20perform%20reference-guided%20token%20propagation%20within%20the%20DiT%20to%20maintain%20temporal%20coherence%20with%20minimal%20computational%20and%20memory%20overhead.%20Our%20models%20achieve%20the%20best%20performance%20among%20all%20generative%20monocular%20and%20video%20depth%20estimation%20models%20and%20produce%20significantly%20cleaner%20point%20clouds%20than%20all%20other%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.05246v1&entry.124074799=Read"},
{"title": "MVT: Mask-Grounded Vision-Language Models for Taxonomy-Aligned Land-Cover Tagging", "author": "Siyi Chen and Kai Wang and Weicong Pang and Ruiming Yang and Ziru Chen and Renjun Gao and Alexis Kai Hon Lau and Dasa Gu and Chenchen Zhang and Cheng Li", "abstract": "Land-cover understanding in remote sensing increasingly demands class-agnostic systems that generalize across datasets while remaining spatially precise and interpretable. We study a geometry-first discovery-and-interpretation setting under domain shift, where candidate regions are delineated class-agnostically and supervision avoids lexical class names via anonymized identifiers. Complementary to open-set recognition and open-world learning, we focus on coupling class-agnostic mask evidence with taxonomy-grounded scene interpretation, rather than unknown rejection or continual class expansion. We propose MVT, a three-stage framework that (i) extracts boundary-faithful region masks using SAM2 with domain adaptation, (ii) performs mask-grounded semantic tagging and scene description generation via dual-step LoRA fine-tuning of multimodal LLMs, and (iii) evaluates outputs with LLM-as-judge scoring calibrated by stratified expert ratings. On cross-dataset segmentation transfer (train on OpenEarthMap, evaluate on LoveDA), domain-adapted SAM2 improves mask quality; meanwhile, dual-step MLLM fine-tuning yields more accurate taxonomy-aligned tags and more informative mask-grounded scene descriptions.", "link": "http://arxiv.org/abs/2509.18693v3", "date": "2026-01-08", "relevancy": 2.8513, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5766}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVT%3A%20Mask-Grounded%20Vision-Language%20Models%20for%20Taxonomy-Aligned%20Land-Cover%20Tagging&body=Title%3A%20MVT%3A%20Mask-Grounded%20Vision-Language%20Models%20for%20Taxonomy-Aligned%20Land-Cover%20Tagging%0AAuthor%3A%20Siyi%20Chen%20and%20Kai%20Wang%20and%20Weicong%20Pang%20and%20Ruiming%20Yang%20and%20Ziru%20Chen%20and%20Renjun%20Gao%20and%20Alexis%20Kai%20Hon%20Lau%20and%20Dasa%20Gu%20and%20Chenchen%20Zhang%20and%20Cheng%20Li%0AAbstract%3A%20Land-cover%20understanding%20in%20remote%20sensing%20increasingly%20demands%20class-agnostic%20systems%20that%20generalize%20across%20datasets%20while%20remaining%20spatially%20precise%20and%20interpretable.%20We%20study%20a%20geometry-first%20discovery-and-interpretation%20setting%20under%20domain%20shift%2C%20where%20candidate%20regions%20are%20delineated%20class-agnostically%20and%20supervision%20avoids%20lexical%20class%20names%20via%20anonymized%20identifiers.%20Complementary%20to%20open-set%20recognition%20and%20open-world%20learning%2C%20we%20focus%20on%20coupling%20class-agnostic%20mask%20evidence%20with%20taxonomy-grounded%20scene%20interpretation%2C%20rather%20than%20unknown%20rejection%20or%20continual%20class%20expansion.%20We%20propose%20MVT%2C%20a%20three-stage%20framework%20that%20%28i%29%20extracts%20boundary-faithful%20region%20masks%20using%20SAM2%20with%20domain%20adaptation%2C%20%28ii%29%20performs%20mask-grounded%20semantic%20tagging%20and%20scene%20description%20generation%20via%20dual-step%20LoRA%20fine-tuning%20of%20multimodal%20LLMs%2C%20and%20%28iii%29%20evaluates%20outputs%20with%20LLM-as-judge%20scoring%20calibrated%20by%20stratified%20expert%20ratings.%20On%20cross-dataset%20segmentation%20transfer%20%28train%20on%20OpenEarthMap%2C%20evaluate%20on%20LoveDA%29%2C%20domain-adapted%20SAM2%20improves%20mask%20quality%3B%20meanwhile%2C%20dual-step%20MLLM%20fine-tuning%20yields%20more%20accurate%20taxonomy-aligned%20tags%20and%20more%20informative%20mask-grounded%20scene%20descriptions.%0ALink%3A%20http%3A//arxiv.org/abs/2509.18693v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVT%253A%2520Mask-Grounded%2520Vision-Language%2520Models%2520for%2520Taxonomy-Aligned%2520Land-Cover%2520Tagging%26entry.906535625%3DSiyi%2520Chen%2520and%2520Kai%2520Wang%2520and%2520Weicong%2520Pang%2520and%2520Ruiming%2520Yang%2520and%2520Ziru%2520Chen%2520and%2520Renjun%2520Gao%2520and%2520Alexis%2520Kai%2520Hon%2520Lau%2520and%2520Dasa%2520Gu%2520and%2520Chenchen%2520Zhang%2520and%2520Cheng%2520Li%26entry.1292438233%3DLand-cover%2520understanding%2520in%2520remote%2520sensing%2520increasingly%2520demands%2520class-agnostic%2520systems%2520that%2520generalize%2520across%2520datasets%2520while%2520remaining%2520spatially%2520precise%2520and%2520interpretable.%2520We%2520study%2520a%2520geometry-first%2520discovery-and-interpretation%2520setting%2520under%2520domain%2520shift%252C%2520where%2520candidate%2520regions%2520are%2520delineated%2520class-agnostically%2520and%2520supervision%2520avoids%2520lexical%2520class%2520names%2520via%2520anonymized%2520identifiers.%2520Complementary%2520to%2520open-set%2520recognition%2520and%2520open-world%2520learning%252C%2520we%2520focus%2520on%2520coupling%2520class-agnostic%2520mask%2520evidence%2520with%2520taxonomy-grounded%2520scene%2520interpretation%252C%2520rather%2520than%2520unknown%2520rejection%2520or%2520continual%2520class%2520expansion.%2520We%2520propose%2520MVT%252C%2520a%2520three-stage%2520framework%2520that%2520%2528i%2529%2520extracts%2520boundary-faithful%2520region%2520masks%2520using%2520SAM2%2520with%2520domain%2520adaptation%252C%2520%2528ii%2529%2520performs%2520mask-grounded%2520semantic%2520tagging%2520and%2520scene%2520description%2520generation%2520via%2520dual-step%2520LoRA%2520fine-tuning%2520of%2520multimodal%2520LLMs%252C%2520and%2520%2528iii%2529%2520evaluates%2520outputs%2520with%2520LLM-as-judge%2520scoring%2520calibrated%2520by%2520stratified%2520expert%2520ratings.%2520On%2520cross-dataset%2520segmentation%2520transfer%2520%2528train%2520on%2520OpenEarthMap%252C%2520evaluate%2520on%2520LoveDA%2529%252C%2520domain-adapted%2520SAM2%2520improves%2520mask%2520quality%253B%2520meanwhile%252C%2520dual-step%2520MLLM%2520fine-tuning%2520yields%2520more%2520accurate%2520taxonomy-aligned%2520tags%2520and%2520more%2520informative%2520mask-grounded%2520scene%2520descriptions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18693v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVT%3A%20Mask-Grounded%20Vision-Language%20Models%20for%20Taxonomy-Aligned%20Land-Cover%20Tagging&entry.906535625=Siyi%20Chen%20and%20Kai%20Wang%20and%20Weicong%20Pang%20and%20Ruiming%20Yang%20and%20Ziru%20Chen%20and%20Renjun%20Gao%20and%20Alexis%20Kai%20Hon%20Lau%20and%20Dasa%20Gu%20and%20Chenchen%20Zhang%20and%20Cheng%20Li&entry.1292438233=Land-cover%20understanding%20in%20remote%20sensing%20increasingly%20demands%20class-agnostic%20systems%20that%20generalize%20across%20datasets%20while%20remaining%20spatially%20precise%20and%20interpretable.%20We%20study%20a%20geometry-first%20discovery-and-interpretation%20setting%20under%20domain%20shift%2C%20where%20candidate%20regions%20are%20delineated%20class-agnostically%20and%20supervision%20avoids%20lexical%20class%20names%20via%20anonymized%20identifiers.%20Complementary%20to%20open-set%20recognition%20and%20open-world%20learning%2C%20we%20focus%20on%20coupling%20class-agnostic%20mask%20evidence%20with%20taxonomy-grounded%20scene%20interpretation%2C%20rather%20than%20unknown%20rejection%20or%20continual%20class%20expansion.%20We%20propose%20MVT%2C%20a%20three-stage%20framework%20that%20%28i%29%20extracts%20boundary-faithful%20region%20masks%20using%20SAM2%20with%20domain%20adaptation%2C%20%28ii%29%20performs%20mask-grounded%20semantic%20tagging%20and%20scene%20description%20generation%20via%20dual-step%20LoRA%20fine-tuning%20of%20multimodal%20LLMs%2C%20and%20%28iii%29%20evaluates%20outputs%20with%20LLM-as-judge%20scoring%20calibrated%20by%20stratified%20expert%20ratings.%20On%20cross-dataset%20segmentation%20transfer%20%28train%20on%20OpenEarthMap%2C%20evaluate%20on%20LoveDA%29%2C%20domain-adapted%20SAM2%20improves%20mask%20quality%3B%20meanwhile%2C%20dual-step%20MLLM%20fine-tuning%20yields%20more%20accurate%20taxonomy-aligned%20tags%20and%20more%20informative%20mask-grounded%20scene%20descriptions.&entry.1838667208=http%3A//arxiv.org/abs/2509.18693v3&entry.124074799=Read"},
{"title": "Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform", "author": "Suyash Mishra and Qiang Li and Srikanth Patil and Satyanarayan Pati and Baddu Narendra", "abstract": "Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new \"A+B\" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.", "link": "http://arxiv.org/abs/2601.04891v1", "date": "2026-01-08", "relevancy": 2.8401, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5859}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Vision%20Language%20Models%20for%20Pharmaceutical%20Long%20Form%20Video%20Reasoning%20on%20Industrial%20GenAI%20Platform&body=Title%3A%20Scaling%20Vision%20Language%20Models%20for%20Pharmaceutical%20Long%20Form%20Video%20Reasoning%20on%20Industrial%20GenAI%20Platform%0AAuthor%3A%20Suyash%20Mishra%20and%20Qiang%20Li%20and%20Srikanth%20Patil%20and%20Satyanarayan%20Pati%20and%20Baddu%20Narendra%0AAbstract%3A%20Vision%20Language%20Models%20%28VLMs%29%20have%20shown%20strong%20performance%20on%20multimodal%20reasoning%20tasks%2C%20yet%20most%20evaluations%20focus%20on%20short%20videos%20and%20assume%20unconstrained%20computational%20resources.%20In%20industrial%20settings%20such%20as%20pharmaceutical%20content%20understanding%2C%20practitioners%20must%20process%20long-form%20videos%20under%20strict%20GPU%2C%20latency%2C%20and%20cost%20constraints%2C%20where%20many%20existing%20approaches%20fail%20to%20scale.%20In%20this%20work%2C%20we%20present%20an%20industrial%20GenAI%20framework%20that%20processes%20over%20200%2C000%20PDFs%2C%2025%2C326%20videos%20across%20eight%20formats%20%28e.g.%2C%20MP4%2C%20M4V%2C%20etc.%29%2C%20and%20888%20multilingual%20audio%20files%20in%20more%20than%2020%20languages.%20Our%20study%20makes%20three%20contributions%3A%20%28i%29%20an%20industrial%20large-scale%20architecture%20for%20multimodal%20reasoning%20in%20pharmaceutical%20domains%3B%20%28ii%29%20empirical%20analysis%20of%20over%2040%20VLMs%20on%20two%20leading%20benchmarks%20%28Video-MME%20and%20MMBench%29%20and%20proprietary%20dataset%20of%2025%2C326%20videos%20across%2014%20disease%20areas%3B%20and%20%28iii%29%20four%20findings%20relevant%20to%20long-form%20video%20reasoning%3A%20the%20role%20of%20multimodality%2C%20attention%20mechanism%20trade-offs%2C%20temporal%20reasoning%20limits%2C%20and%20challenges%20of%20video%20splitting%20under%20GPU%20constraints.%20Results%20show%203-8%20times%20efficiency%20gains%20with%20SDPA%20attention%20on%20commodity%20GPUs%2C%20multimodality%20improving%20up%20to%208/12%20task%20domains%20%28especially%20length-dependent%20tasks%29%2C%20and%20clear%20bottlenecks%20in%20temporal%20alignment%20and%20keyframe%20detection%20across%20open-%20and%20closed-source%20VLMs.%20Rather%20than%20proposing%20a%20new%20%22A%2BB%22%20model%2C%20this%20paper%20characterizes%20practical%20limits%2C%20trade-offs%2C%20and%20failure%20patterns%20of%20current%20VLMs%20under%20realistic%20deployment%20constraints%2C%20and%20provide%20actionable%20guidance%20for%20both%20researchers%20and%20practitioners%20designing%20scalable%20multimodal%20systems%20for%20long-form%20video%20understanding%20in%20industrial%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Vision%2520Language%2520Models%2520for%2520Pharmaceutical%2520Long%2520Form%2520Video%2520Reasoning%2520on%2520Industrial%2520GenAI%2520Platform%26entry.906535625%3DSuyash%2520Mishra%2520and%2520Qiang%2520Li%2520and%2520Srikanth%2520Patil%2520and%2520Satyanarayan%2520Pati%2520and%2520Baddu%2520Narendra%26entry.1292438233%3DVision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520strong%2520performance%2520on%2520multimodal%2520reasoning%2520tasks%252C%2520yet%2520most%2520evaluations%2520focus%2520on%2520short%2520videos%2520and%2520assume%2520unconstrained%2520computational%2520resources.%2520In%2520industrial%2520settings%2520such%2520as%2520pharmaceutical%2520content%2520understanding%252C%2520practitioners%2520must%2520process%2520long-form%2520videos%2520under%2520strict%2520GPU%252C%2520latency%252C%2520and%2520cost%2520constraints%252C%2520where%2520many%2520existing%2520approaches%2520fail%2520to%2520scale.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520industrial%2520GenAI%2520framework%2520that%2520processes%2520over%2520200%252C000%2520PDFs%252C%252025%252C326%2520videos%2520across%2520eight%2520formats%2520%2528e.g.%252C%2520MP4%252C%2520M4V%252C%2520etc.%2529%252C%2520and%2520888%2520multilingual%2520audio%2520files%2520in%2520more%2520than%252020%2520languages.%2520Our%2520study%2520makes%2520three%2520contributions%253A%2520%2528i%2529%2520an%2520industrial%2520large-scale%2520architecture%2520for%2520multimodal%2520reasoning%2520in%2520pharmaceutical%2520domains%253B%2520%2528ii%2529%2520empirical%2520analysis%2520of%2520over%252040%2520VLMs%2520on%2520two%2520leading%2520benchmarks%2520%2528Video-MME%2520and%2520MMBench%2529%2520and%2520proprietary%2520dataset%2520of%252025%252C326%2520videos%2520across%252014%2520disease%2520areas%253B%2520and%2520%2528iii%2529%2520four%2520findings%2520relevant%2520to%2520long-form%2520video%2520reasoning%253A%2520the%2520role%2520of%2520multimodality%252C%2520attention%2520mechanism%2520trade-offs%252C%2520temporal%2520reasoning%2520limits%252C%2520and%2520challenges%2520of%2520video%2520splitting%2520under%2520GPU%2520constraints.%2520Results%2520show%25203-8%2520times%2520efficiency%2520gains%2520with%2520SDPA%2520attention%2520on%2520commodity%2520GPUs%252C%2520multimodality%2520improving%2520up%2520to%25208/12%2520task%2520domains%2520%2528especially%2520length-dependent%2520tasks%2529%252C%2520and%2520clear%2520bottlenecks%2520in%2520temporal%2520alignment%2520and%2520keyframe%2520detection%2520across%2520open-%2520and%2520closed-source%2520VLMs.%2520Rather%2520than%2520proposing%2520a%2520new%2520%2522A%252BB%2522%2520model%252C%2520this%2520paper%2520characterizes%2520practical%2520limits%252C%2520trade-offs%252C%2520and%2520failure%2520patterns%2520of%2520current%2520VLMs%2520under%2520realistic%2520deployment%2520constraints%252C%2520and%2520provide%2520actionable%2520guidance%2520for%2520both%2520researchers%2520and%2520practitioners%2520designing%2520scalable%2520multimodal%2520systems%2520for%2520long-form%2520video%2520understanding%2520in%2520industrial%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Vision%20Language%20Models%20for%20Pharmaceutical%20Long%20Form%20Video%20Reasoning%20on%20Industrial%20GenAI%20Platform&entry.906535625=Suyash%20Mishra%20and%20Qiang%20Li%20and%20Srikanth%20Patil%20and%20Satyanarayan%20Pati%20and%20Baddu%20Narendra&entry.1292438233=Vision%20Language%20Models%20%28VLMs%29%20have%20shown%20strong%20performance%20on%20multimodal%20reasoning%20tasks%2C%20yet%20most%20evaluations%20focus%20on%20short%20videos%20and%20assume%20unconstrained%20computational%20resources.%20In%20industrial%20settings%20such%20as%20pharmaceutical%20content%20understanding%2C%20practitioners%20must%20process%20long-form%20videos%20under%20strict%20GPU%2C%20latency%2C%20and%20cost%20constraints%2C%20where%20many%20existing%20approaches%20fail%20to%20scale.%20In%20this%20work%2C%20we%20present%20an%20industrial%20GenAI%20framework%20that%20processes%20over%20200%2C000%20PDFs%2C%2025%2C326%20videos%20across%20eight%20formats%20%28e.g.%2C%20MP4%2C%20M4V%2C%20etc.%29%2C%20and%20888%20multilingual%20audio%20files%20in%20more%20than%2020%20languages.%20Our%20study%20makes%20three%20contributions%3A%20%28i%29%20an%20industrial%20large-scale%20architecture%20for%20multimodal%20reasoning%20in%20pharmaceutical%20domains%3B%20%28ii%29%20empirical%20analysis%20of%20over%2040%20VLMs%20on%20two%20leading%20benchmarks%20%28Video-MME%20and%20MMBench%29%20and%20proprietary%20dataset%20of%2025%2C326%20videos%20across%2014%20disease%20areas%3B%20and%20%28iii%29%20four%20findings%20relevant%20to%20long-form%20video%20reasoning%3A%20the%20role%20of%20multimodality%2C%20attention%20mechanism%20trade-offs%2C%20temporal%20reasoning%20limits%2C%20and%20challenges%20of%20video%20splitting%20under%20GPU%20constraints.%20Results%20show%203-8%20times%20efficiency%20gains%20with%20SDPA%20attention%20on%20commodity%20GPUs%2C%20multimodality%20improving%20up%20to%208/12%20task%20domains%20%28especially%20length-dependent%20tasks%29%2C%20and%20clear%20bottlenecks%20in%20temporal%20alignment%20and%20keyframe%20detection%20across%20open-%20and%20closed-source%20VLMs.%20Rather%20than%20proposing%20a%20new%20%22A%2BB%22%20model%2C%20this%20paper%20characterizes%20practical%20limits%2C%20trade-offs%2C%20and%20failure%20patterns%20of%20current%20VLMs%20under%20realistic%20deployment%20constraints%2C%20and%20provide%20actionable%20guidance%20for%20both%20researchers%20and%20practitioners%20designing%20scalable%20multimodal%20systems%20for%20long-form%20video%20understanding%20in%20industrial%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2601.04891v1&entry.124074799=Read"},
{"title": "TEA: Temporal Adaptive Satellite Image Semantic Segmentation", "author": "Juyuan Kang and Hao Zhu and Yan Zhu and Wei Zhang and Jianing Chen and Tianxiang Xiao and Yike Ma and Hao Jiang and Feng Dai", "abstract": "Crop mapping based on satellite images time-series (SITS) holds substantial economic value in agricultural production settings, in which parcel segmentation is an essential step. Existing approaches have achieved notable advancements in SITS segmentation with predetermined sequence lengths. However, we found that these approaches overlooked the generalization capability of models across scenarios with varying temporal length, leading to markedly poor segmentation results in such cases. To address this issue, we propose TEA, a TEmporal Adaptive SITS semantic segmentation method to enhance the model's resilience under varying sequence lengths. We introduce a teacher model that encapsulates the global sequence knowledge to guide a student model with adaptive temporal input lengths. Specifically, teacher shapes the student's feature space via intermediate embedding, prototypes and soft label perspectives to realize knowledge transfer, while dynamically aggregating student model to mitigate knowledge forgetting. Finally, we introduce full-sequence reconstruction as an auxiliary task to further enhance the quality of representations across inputs of varying temporal lengths. Through extensive experiments, we demonstrate that our method brings remarkable improvements across inputs of different temporal lengths on common benchmarks. Our code will be publicly available.", "link": "http://arxiv.org/abs/2601.04956v1", "date": "2026-01-08", "relevancy": 2.834, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5937}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5572}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TEA%3A%20Temporal%20Adaptive%20Satellite%20Image%20Semantic%20Segmentation&body=Title%3A%20TEA%3A%20Temporal%20Adaptive%20Satellite%20Image%20Semantic%20Segmentation%0AAuthor%3A%20Juyuan%20Kang%20and%20Hao%20Zhu%20and%20Yan%20Zhu%20and%20Wei%20Zhang%20and%20Jianing%20Chen%20and%20Tianxiang%20Xiao%20and%20Yike%20Ma%20and%20Hao%20Jiang%20and%20Feng%20Dai%0AAbstract%3A%20Crop%20mapping%20based%20on%20satellite%20images%20time-series%20%28SITS%29%20holds%20substantial%20economic%20value%20in%20agricultural%20production%20settings%2C%20in%20which%20parcel%20segmentation%20is%20an%20essential%20step.%20Existing%20approaches%20have%20achieved%20notable%20advancements%20in%20SITS%20segmentation%20with%20predetermined%20sequence%20lengths.%20However%2C%20we%20found%20that%20these%20approaches%20overlooked%20the%20generalization%20capability%20of%20models%20across%20scenarios%20with%20varying%20temporal%20length%2C%20leading%20to%20markedly%20poor%20segmentation%20results%20in%20such%20cases.%20To%20address%20this%20issue%2C%20we%20propose%20TEA%2C%20a%20TEmporal%20Adaptive%20SITS%20semantic%20segmentation%20method%20to%20enhance%20the%20model%27s%20resilience%20under%20varying%20sequence%20lengths.%20We%20introduce%20a%20teacher%20model%20that%20encapsulates%20the%20global%20sequence%20knowledge%20to%20guide%20a%20student%20model%20with%20adaptive%20temporal%20input%20lengths.%20Specifically%2C%20teacher%20shapes%20the%20student%27s%20feature%20space%20via%20intermediate%20embedding%2C%20prototypes%20and%20soft%20label%20perspectives%20to%20realize%20knowledge%20transfer%2C%20while%20dynamically%20aggregating%20student%20model%20to%20mitigate%20knowledge%20forgetting.%20Finally%2C%20we%20introduce%20full-sequence%20reconstruction%20as%20an%20auxiliary%20task%20to%20further%20enhance%20the%20quality%20of%20representations%20across%20inputs%20of%20varying%20temporal%20lengths.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%20method%20brings%20remarkable%20improvements%20across%20inputs%20of%20different%20temporal%20lengths%20on%20common%20benchmarks.%20Our%20code%20will%20be%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTEA%253A%2520Temporal%2520Adaptive%2520Satellite%2520Image%2520Semantic%2520Segmentation%26entry.906535625%3DJuyuan%2520Kang%2520and%2520Hao%2520Zhu%2520and%2520Yan%2520Zhu%2520and%2520Wei%2520Zhang%2520and%2520Jianing%2520Chen%2520and%2520Tianxiang%2520Xiao%2520and%2520Yike%2520Ma%2520and%2520Hao%2520Jiang%2520and%2520Feng%2520Dai%26entry.1292438233%3DCrop%2520mapping%2520based%2520on%2520satellite%2520images%2520time-series%2520%2528SITS%2529%2520holds%2520substantial%2520economic%2520value%2520in%2520agricultural%2520production%2520settings%252C%2520in%2520which%2520parcel%2520segmentation%2520is%2520an%2520essential%2520step.%2520Existing%2520approaches%2520have%2520achieved%2520notable%2520advancements%2520in%2520SITS%2520segmentation%2520with%2520predetermined%2520sequence%2520lengths.%2520However%252C%2520we%2520found%2520that%2520these%2520approaches%2520overlooked%2520the%2520generalization%2520capability%2520of%2520models%2520across%2520scenarios%2520with%2520varying%2520temporal%2520length%252C%2520leading%2520to%2520markedly%2520poor%2520segmentation%2520results%2520in%2520such%2520cases.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520TEA%252C%2520a%2520TEmporal%2520Adaptive%2520SITS%2520semantic%2520segmentation%2520method%2520to%2520enhance%2520the%2520model%2527s%2520resilience%2520under%2520varying%2520sequence%2520lengths.%2520We%2520introduce%2520a%2520teacher%2520model%2520that%2520encapsulates%2520the%2520global%2520sequence%2520knowledge%2520to%2520guide%2520a%2520student%2520model%2520with%2520adaptive%2520temporal%2520input%2520lengths.%2520Specifically%252C%2520teacher%2520shapes%2520the%2520student%2527s%2520feature%2520space%2520via%2520intermediate%2520embedding%252C%2520prototypes%2520and%2520soft%2520label%2520perspectives%2520to%2520realize%2520knowledge%2520transfer%252C%2520while%2520dynamically%2520aggregating%2520student%2520model%2520to%2520mitigate%2520knowledge%2520forgetting.%2520Finally%252C%2520we%2520introduce%2520full-sequence%2520reconstruction%2520as%2520an%2520auxiliary%2520task%2520to%2520further%2520enhance%2520the%2520quality%2520of%2520representations%2520across%2520inputs%2520of%2520varying%2520temporal%2520lengths.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520brings%2520remarkable%2520improvements%2520across%2520inputs%2520of%2520different%2520temporal%2520lengths%2520on%2520common%2520benchmarks.%2520Our%2520code%2520will%2520be%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TEA%3A%20Temporal%20Adaptive%20Satellite%20Image%20Semantic%20Segmentation&entry.906535625=Juyuan%20Kang%20and%20Hao%20Zhu%20and%20Yan%20Zhu%20and%20Wei%20Zhang%20and%20Jianing%20Chen%20and%20Tianxiang%20Xiao%20and%20Yike%20Ma%20and%20Hao%20Jiang%20and%20Feng%20Dai&entry.1292438233=Crop%20mapping%20based%20on%20satellite%20images%20time-series%20%28SITS%29%20holds%20substantial%20economic%20value%20in%20agricultural%20production%20settings%2C%20in%20which%20parcel%20segmentation%20is%20an%20essential%20step.%20Existing%20approaches%20have%20achieved%20notable%20advancements%20in%20SITS%20segmentation%20with%20predetermined%20sequence%20lengths.%20However%2C%20we%20found%20that%20these%20approaches%20overlooked%20the%20generalization%20capability%20of%20models%20across%20scenarios%20with%20varying%20temporal%20length%2C%20leading%20to%20markedly%20poor%20segmentation%20results%20in%20such%20cases.%20To%20address%20this%20issue%2C%20we%20propose%20TEA%2C%20a%20TEmporal%20Adaptive%20SITS%20semantic%20segmentation%20method%20to%20enhance%20the%20model%27s%20resilience%20under%20varying%20sequence%20lengths.%20We%20introduce%20a%20teacher%20model%20that%20encapsulates%20the%20global%20sequence%20knowledge%20to%20guide%20a%20student%20model%20with%20adaptive%20temporal%20input%20lengths.%20Specifically%2C%20teacher%20shapes%20the%20student%27s%20feature%20space%20via%20intermediate%20embedding%2C%20prototypes%20and%20soft%20label%20perspectives%20to%20realize%20knowledge%20transfer%2C%20while%20dynamically%20aggregating%20student%20model%20to%20mitigate%20knowledge%20forgetting.%20Finally%2C%20we%20introduce%20full-sequence%20reconstruction%20as%20an%20auxiliary%20task%20to%20further%20enhance%20the%20quality%20of%20representations%20across%20inputs%20of%20varying%20temporal%20lengths.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%20method%20brings%20remarkable%20improvements%20across%20inputs%20of%20different%20temporal%20lengths%20on%20common%20benchmarks.%20Our%20code%20will%20be%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.04956v1&entry.124074799=Read"},
{"title": "V-FAT: Benchmarking Visual Fidelity Against Text-bias", "author": "Ziteng Wang and Yujie He and Guanliang Li and Siqi Yang and Jiaqi Xiong and Songxiang Liu", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize \"lucky\" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.", "link": "http://arxiv.org/abs/2601.04897v1", "date": "2026-01-08", "relevancy": 2.8322, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5704}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-FAT%3A%20Benchmarking%20Visual%20Fidelity%20Against%20Text-bias&body=Title%3A%20V-FAT%3A%20Benchmarking%20Visual%20Fidelity%20Against%20Text-bias%0AAuthor%3A%20Ziteng%20Wang%20and%20Yujie%20He%20and%20Guanliang%20Li%20and%20Siqi%20Yang%20and%20Jiaqi%20Xiong%20and%20Songxiang%20Liu%0AAbstract%3A%20Recent%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20impressive%20performance%20on%20standard%20visual%20reasoning%20benchmarks.%20However%2C%20there%20is%20growing%20concern%20that%20these%20models%20rely%20excessively%20on%20linguistic%20shortcuts%20rather%20than%20genuine%20visual%20grounding%2C%20a%20phenomenon%20we%20term%20Text%20Bias.%20In%20this%20paper%2C%20we%20investigate%20the%20fundamental%20tension%20between%20visual%20perception%20and%20linguistic%20priors.%20We%20decouple%20the%20sources%20of%20this%20bias%20into%20two%20dimensions%3A%20Internal%20Corpus%20Bias%2C%20stemming%20from%20statistical%20correlations%20in%20pretraining%2C%20and%20External%20Instruction%20Bias%2C%20arising%20from%20the%20alignment-induced%20tendency%20toward%20sycophancy.%20To%20quantify%20this%20effect%2C%20we%20introduce%20V-FAT%20%28Visual%20Fidelity%20Against%20Text-bias%29%2C%20a%20diagnostic%20benchmark%20comprising%204%2C026%20VQA%20instances%20across%20six%20semantic%20domains.%20V-FAT%20employs%20a%20Three-Level%20Evaluation%20Framework%20that%20systematically%20increases%20the%20conflict%20between%20visual%20evidence%20and%20textual%20information%3A%20%28L1%29%20internal%20bias%20from%20atypical%20images%2C%20%28L2%29%20external%20bias%20from%20misleading%20instructions%2C%20and%20%28L3%29%20synergistic%20bias%20where%20both%20coincide.%20We%20introduce%20the%20Visual%20Robustness%20Score%20%28VRS%29%2C%20a%20metric%20designed%20to%20penalize%20%22lucky%22%20linguistic%20guesses%20and%20reward%20true%20visual%20fidelity.%20Our%20evaluation%20of%2012%20frontier%20MLLMs%20reveals%20that%20while%20models%20excel%20in%20existing%20benchmarks%2C%20they%20experience%20significant%20visual%20collapse%20under%20high%20linguistic%20dominance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-FAT%253A%2520Benchmarking%2520Visual%2520Fidelity%2520Against%2520Text-bias%26entry.906535625%3DZiteng%2520Wang%2520and%2520Yujie%2520He%2520and%2520Guanliang%2520Li%2520and%2520Siqi%2520Yang%2520and%2520Jiaqi%2520Xiong%2520and%2520Songxiang%2520Liu%26entry.1292438233%3DRecent%2520advancements%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520impressive%2520performance%2520on%2520standard%2520visual%2520reasoning%2520benchmarks.%2520However%252C%2520there%2520is%2520growing%2520concern%2520that%2520these%2520models%2520rely%2520excessively%2520on%2520linguistic%2520shortcuts%2520rather%2520than%2520genuine%2520visual%2520grounding%252C%2520a%2520phenomenon%2520we%2520term%2520Text%2520Bias.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520fundamental%2520tension%2520between%2520visual%2520perception%2520and%2520linguistic%2520priors.%2520We%2520decouple%2520the%2520sources%2520of%2520this%2520bias%2520into%2520two%2520dimensions%253A%2520Internal%2520Corpus%2520Bias%252C%2520stemming%2520from%2520statistical%2520correlations%2520in%2520pretraining%252C%2520and%2520External%2520Instruction%2520Bias%252C%2520arising%2520from%2520the%2520alignment-induced%2520tendency%2520toward%2520sycophancy.%2520To%2520quantify%2520this%2520effect%252C%2520we%2520introduce%2520V-FAT%2520%2528Visual%2520Fidelity%2520Against%2520Text-bias%2529%252C%2520a%2520diagnostic%2520benchmark%2520comprising%25204%252C026%2520VQA%2520instances%2520across%2520six%2520semantic%2520domains.%2520V-FAT%2520employs%2520a%2520Three-Level%2520Evaluation%2520Framework%2520that%2520systematically%2520increases%2520the%2520conflict%2520between%2520visual%2520evidence%2520and%2520textual%2520information%253A%2520%2528L1%2529%2520internal%2520bias%2520from%2520atypical%2520images%252C%2520%2528L2%2529%2520external%2520bias%2520from%2520misleading%2520instructions%252C%2520and%2520%2528L3%2529%2520synergistic%2520bias%2520where%2520both%2520coincide.%2520We%2520introduce%2520the%2520Visual%2520Robustness%2520Score%2520%2528VRS%2529%252C%2520a%2520metric%2520designed%2520to%2520penalize%2520%2522lucky%2522%2520linguistic%2520guesses%2520and%2520reward%2520true%2520visual%2520fidelity.%2520Our%2520evaluation%2520of%252012%2520frontier%2520MLLMs%2520reveals%2520that%2520while%2520models%2520excel%2520in%2520existing%2520benchmarks%252C%2520they%2520experience%2520significant%2520visual%2520collapse%2520under%2520high%2520linguistic%2520dominance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-FAT%3A%20Benchmarking%20Visual%20Fidelity%20Against%20Text-bias&entry.906535625=Ziteng%20Wang%20and%20Yujie%20He%20and%20Guanliang%20Li%20and%20Siqi%20Yang%20and%20Jiaqi%20Xiong%20and%20Songxiang%20Liu&entry.1292438233=Recent%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20impressive%20performance%20on%20standard%20visual%20reasoning%20benchmarks.%20However%2C%20there%20is%20growing%20concern%20that%20these%20models%20rely%20excessively%20on%20linguistic%20shortcuts%20rather%20than%20genuine%20visual%20grounding%2C%20a%20phenomenon%20we%20term%20Text%20Bias.%20In%20this%20paper%2C%20we%20investigate%20the%20fundamental%20tension%20between%20visual%20perception%20and%20linguistic%20priors.%20We%20decouple%20the%20sources%20of%20this%20bias%20into%20two%20dimensions%3A%20Internal%20Corpus%20Bias%2C%20stemming%20from%20statistical%20correlations%20in%20pretraining%2C%20and%20External%20Instruction%20Bias%2C%20arising%20from%20the%20alignment-induced%20tendency%20toward%20sycophancy.%20To%20quantify%20this%20effect%2C%20we%20introduce%20V-FAT%20%28Visual%20Fidelity%20Against%20Text-bias%29%2C%20a%20diagnostic%20benchmark%20comprising%204%2C026%20VQA%20instances%20across%20six%20semantic%20domains.%20V-FAT%20employs%20a%20Three-Level%20Evaluation%20Framework%20that%20systematically%20increases%20the%20conflict%20between%20visual%20evidence%20and%20textual%20information%3A%20%28L1%29%20internal%20bias%20from%20atypical%20images%2C%20%28L2%29%20external%20bias%20from%20misleading%20instructions%2C%20and%20%28L3%29%20synergistic%20bias%20where%20both%20coincide.%20We%20introduce%20the%20Visual%20Robustness%20Score%20%28VRS%29%2C%20a%20metric%20designed%20to%20penalize%20%22lucky%22%20linguistic%20guesses%20and%20reward%20true%20visual%20fidelity.%20Our%20evaluation%20of%2012%20frontier%20MLLMs%20reveals%20that%20while%20models%20excel%20in%20existing%20benchmarks%2C%20they%20experience%20significant%20visual%20collapse%20under%20high%20linguistic%20dominance.&entry.1838667208=http%3A//arxiv.org/abs/2601.04897v1&entry.124074799=Read"},
{"title": "SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection", "author": "Maximilian Pittner and Joel Janai and Mario Faigle and Alexandru Paul Condurache", "abstract": "3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.", "link": "http://arxiv.org/abs/2601.04968v1", "date": "2026-01-08", "relevancy": 2.8279, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5699}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5646}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseLaneSTP%3A%20Leveraging%20Spatio-Temporal%20Priors%20with%20Sparse%20Transformers%20for%203D%20Lane%20Detection&body=Title%3A%20SparseLaneSTP%3A%20Leveraging%20Spatio-Temporal%20Priors%20with%20Sparse%20Transformers%20for%203D%20Lane%20Detection%0AAuthor%3A%20Maximilian%20Pittner%20and%20Joel%20Janai%20and%20Mario%20Faigle%20and%20Alexandru%20Paul%20Condurache%0AAbstract%3A%203D%20lane%20detection%20has%20emerged%20as%20a%20critical%20challenge%20in%20autonomous%20driving%2C%20encompassing%20identification%20and%20localization%20of%20lane%20markings%20and%20the%203D%20road%20surface.%20Conventional%203D%20methods%20detect%20lanes%20from%20dense%20birds-eye-viewed%20%28BEV%29%20features%2C%20though%20erroneous%20transformations%20often%20result%20in%20a%20poor%20feature%20representation%20misaligned%20with%20the%20true%203D%20road%20surface.%20While%20recent%20sparse%20lane%20detectors%20have%20surpassed%20dense%20BEV%20approaches%2C%20they%20completely%20disregard%20valuable%20lane-specific%20priors.%20Furthermore%2C%20existing%20methods%20fail%20to%20utilize%20historic%20lane%20observations%2C%20which%20yield%20the%20potential%20to%20resolve%20ambiguities%20in%20situations%20of%20poor%20visibility.%20To%20address%20these%20challenges%2C%20we%20present%20SparseLaneSTP%2C%20a%20novel%20method%20that%20integrates%20both%20geometric%20properties%20of%20the%20lane%20structure%20and%20temporal%20information%20into%20a%20sparse%20lane%20transformer.%20It%20introduces%20a%20new%20lane-specific%20spatio-temporal%20attention%20mechanism%2C%20a%20continuous%20lane%20representation%20tailored%20for%20sparse%20architectures%20as%20well%20as%20temporal%20regularization.%20Identifying%20weaknesses%20of%20existing%203D%20lane%20datasets%2C%20we%20also%20introduce%20a%20precise%20and%20consistent%203D%20lane%20dataset%20using%20a%20simple%20yet%20effective%20auto-labeling%20strategy.%20Our%20experimental%20section%20proves%20the%20benefits%20of%20our%20contributions%20and%20demonstrates%20state-of-the-art%20performance%20across%20all%20detection%20and%20error%20metrics%20on%20existing%203D%20lane%20detection%20benchmarks%20as%20well%20as%20on%20our%20novel%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseLaneSTP%253A%2520Leveraging%2520Spatio-Temporal%2520Priors%2520with%2520Sparse%2520Transformers%2520for%25203D%2520Lane%2520Detection%26entry.906535625%3DMaximilian%2520Pittner%2520and%2520Joel%2520Janai%2520and%2520Mario%2520Faigle%2520and%2520Alexandru%2520Paul%2520Condurache%26entry.1292438233%3D3D%2520lane%2520detection%2520has%2520emerged%2520as%2520a%2520critical%2520challenge%2520in%2520autonomous%2520driving%252C%2520encompassing%2520identification%2520and%2520localization%2520of%2520lane%2520markings%2520and%2520the%25203D%2520road%2520surface.%2520Conventional%25203D%2520methods%2520detect%2520lanes%2520from%2520dense%2520birds-eye-viewed%2520%2528BEV%2529%2520features%252C%2520though%2520erroneous%2520transformations%2520often%2520result%2520in%2520a%2520poor%2520feature%2520representation%2520misaligned%2520with%2520the%2520true%25203D%2520road%2520surface.%2520While%2520recent%2520sparse%2520lane%2520detectors%2520have%2520surpassed%2520dense%2520BEV%2520approaches%252C%2520they%2520completely%2520disregard%2520valuable%2520lane-specific%2520priors.%2520Furthermore%252C%2520existing%2520methods%2520fail%2520to%2520utilize%2520historic%2520lane%2520observations%252C%2520which%2520yield%2520the%2520potential%2520to%2520resolve%2520ambiguities%2520in%2520situations%2520of%2520poor%2520visibility.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520SparseLaneSTP%252C%2520a%2520novel%2520method%2520that%2520integrates%2520both%2520geometric%2520properties%2520of%2520the%2520lane%2520structure%2520and%2520temporal%2520information%2520into%2520a%2520sparse%2520lane%2520transformer.%2520It%2520introduces%2520a%2520new%2520lane-specific%2520spatio-temporal%2520attention%2520mechanism%252C%2520a%2520continuous%2520lane%2520representation%2520tailored%2520for%2520sparse%2520architectures%2520as%2520well%2520as%2520temporal%2520regularization.%2520Identifying%2520weaknesses%2520of%2520existing%25203D%2520lane%2520datasets%252C%2520we%2520also%2520introduce%2520a%2520precise%2520and%2520consistent%25203D%2520lane%2520dataset%2520using%2520a%2520simple%2520yet%2520effective%2520auto-labeling%2520strategy.%2520Our%2520experimental%2520section%2520proves%2520the%2520benefits%2520of%2520our%2520contributions%2520and%2520demonstrates%2520state-of-the-art%2520performance%2520across%2520all%2520detection%2520and%2520error%2520metrics%2520on%2520existing%25203D%2520lane%2520detection%2520benchmarks%2520as%2520well%2520as%2520on%2520our%2520novel%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseLaneSTP%3A%20Leveraging%20Spatio-Temporal%20Priors%20with%20Sparse%20Transformers%20for%203D%20Lane%20Detection&entry.906535625=Maximilian%20Pittner%20and%20Joel%20Janai%20and%20Mario%20Faigle%20and%20Alexandru%20Paul%20Condurache&entry.1292438233=3D%20lane%20detection%20has%20emerged%20as%20a%20critical%20challenge%20in%20autonomous%20driving%2C%20encompassing%20identification%20and%20localization%20of%20lane%20markings%20and%20the%203D%20road%20surface.%20Conventional%203D%20methods%20detect%20lanes%20from%20dense%20birds-eye-viewed%20%28BEV%29%20features%2C%20though%20erroneous%20transformations%20often%20result%20in%20a%20poor%20feature%20representation%20misaligned%20with%20the%20true%203D%20road%20surface.%20While%20recent%20sparse%20lane%20detectors%20have%20surpassed%20dense%20BEV%20approaches%2C%20they%20completely%20disregard%20valuable%20lane-specific%20priors.%20Furthermore%2C%20existing%20methods%20fail%20to%20utilize%20historic%20lane%20observations%2C%20which%20yield%20the%20potential%20to%20resolve%20ambiguities%20in%20situations%20of%20poor%20visibility.%20To%20address%20these%20challenges%2C%20we%20present%20SparseLaneSTP%2C%20a%20novel%20method%20that%20integrates%20both%20geometric%20properties%20of%20the%20lane%20structure%20and%20temporal%20information%20into%20a%20sparse%20lane%20transformer.%20It%20introduces%20a%20new%20lane-specific%20spatio-temporal%20attention%20mechanism%2C%20a%20continuous%20lane%20representation%20tailored%20for%20sparse%20architectures%20as%20well%20as%20temporal%20regularization.%20Identifying%20weaknesses%20of%20existing%203D%20lane%20datasets%2C%20we%20also%20introduce%20a%20precise%20and%20consistent%203D%20lane%20dataset%20using%20a%20simple%20yet%20effective%20auto-labeling%20strategy.%20Our%20experimental%20section%20proves%20the%20benefits%20of%20our%20contributions%20and%20demonstrates%20state-of-the-art%20performance%20across%20all%20detection%20and%20error%20metrics%20on%20existing%203D%20lane%20detection%20benchmarks%20as%20well%20as%20on%20our%20novel%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2601.04968v1&entry.124074799=Read"},
{"title": "Patch-based Representation and Learning for Efficient Deformation Modeling", "author": "Ruochen Chen and Thuy Tran and Shaifali Parashar", "abstract": "In this paper, we present a patch-based representation of surfaces, PolyFit, which is obtained by fitting jet functions locally on surface patches. Such a representation can be learned efficiently in a supervised fashion from both analytic functions and real data. Once learned, it can be generalized to various types of surfaces. Using PolyFit, the surfaces can be efficiently deformed by updating a compact set of jet coefficients rather than optimizing per-vertex degrees of freedom for many downstream tasks in computer vision and graphics. We demonstrate the capabilities of our proposed methodologies with two applications: 1) Shape-from-template (SfT): where the goal is to deform the input 3D template of an object as seen in image/video. Using PolyFit, we adopt test-time optimization that delivers competitive accuracy while being markedly faster than offline physics-based solvers, and outperforms recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping. We train a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.", "link": "http://arxiv.org/abs/2601.05035v1", "date": "2026-01-08", "relevancy": 2.7853, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5779}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5468}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patch-based%20Representation%20and%20Learning%20for%20Efficient%20Deformation%20Modeling&body=Title%3A%20Patch-based%20Representation%20and%20Learning%20for%20Efficient%20Deformation%20Modeling%0AAuthor%3A%20Ruochen%20Chen%20and%20Thuy%20Tran%20and%20Shaifali%20Parashar%0AAbstract%3A%20In%20this%20paper%2C%20we%20present%20a%20patch-based%20representation%20of%20surfaces%2C%20PolyFit%2C%20which%20is%20obtained%20by%20fitting%20jet%20functions%20locally%20on%20surface%20patches.%20Such%20a%20representation%20can%20be%20learned%20efficiently%20in%20a%20supervised%20fashion%20from%20both%20analytic%20functions%20and%20real%20data.%20Once%20learned%2C%20it%20can%20be%20generalized%20to%20various%20types%20of%20surfaces.%20Using%20PolyFit%2C%20the%20surfaces%20can%20be%20efficiently%20deformed%20by%20updating%20a%20compact%20set%20of%20jet%20coefficients%20rather%20than%20optimizing%20per-vertex%20degrees%20of%20freedom%20for%20many%20downstream%20tasks%20in%20computer%20vision%20and%20graphics.%20We%20demonstrate%20the%20capabilities%20of%20our%20proposed%20methodologies%20with%20two%20applications%3A%201%29%20Shape-from-template%20%28SfT%29%3A%20where%20the%20goal%20is%20to%20deform%20the%20input%203D%20template%20of%20an%20object%20as%20seen%20in%20image/video.%20Using%20PolyFit%2C%20we%20adopt%20test-time%20optimization%20that%20delivers%20competitive%20accuracy%20while%20being%20markedly%20faster%20than%20offline%20physics-based%20solvers%2C%20and%20outperforms%20recent%20physics-guided%20neural%20simulators%20in%20accuracy%20at%20modest%20additional%20runtime.%202%29%20Garment%20draping.%20We%20train%20a%20self-supervised%2C%20mesh-%20and%20garment-agnostic%20model%20that%20generalizes%20across%20resolutions%20and%20garment%20types%2C%20delivering%20up%20to%20an%20order-of-magnitude%20faster%20inference%20than%20strong%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatch-based%2520Representation%2520and%2520Learning%2520for%2520Efficient%2520Deformation%2520Modeling%26entry.906535625%3DRuochen%2520Chen%2520and%2520Thuy%2520Tran%2520and%2520Shaifali%2520Parashar%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520present%2520a%2520patch-based%2520representation%2520of%2520surfaces%252C%2520PolyFit%252C%2520which%2520is%2520obtained%2520by%2520fitting%2520jet%2520functions%2520locally%2520on%2520surface%2520patches.%2520Such%2520a%2520representation%2520can%2520be%2520learned%2520efficiently%2520in%2520a%2520supervised%2520fashion%2520from%2520both%2520analytic%2520functions%2520and%2520real%2520data.%2520Once%2520learned%252C%2520it%2520can%2520be%2520generalized%2520to%2520various%2520types%2520of%2520surfaces.%2520Using%2520PolyFit%252C%2520the%2520surfaces%2520can%2520be%2520efficiently%2520deformed%2520by%2520updating%2520a%2520compact%2520set%2520of%2520jet%2520coefficients%2520rather%2520than%2520optimizing%2520per-vertex%2520degrees%2520of%2520freedom%2520for%2520many%2520downstream%2520tasks%2520in%2520computer%2520vision%2520and%2520graphics.%2520We%2520demonstrate%2520the%2520capabilities%2520of%2520our%2520proposed%2520methodologies%2520with%2520two%2520applications%253A%25201%2529%2520Shape-from-template%2520%2528SfT%2529%253A%2520where%2520the%2520goal%2520is%2520to%2520deform%2520the%2520input%25203D%2520template%2520of%2520an%2520object%2520as%2520seen%2520in%2520image/video.%2520Using%2520PolyFit%252C%2520we%2520adopt%2520test-time%2520optimization%2520that%2520delivers%2520competitive%2520accuracy%2520while%2520being%2520markedly%2520faster%2520than%2520offline%2520physics-based%2520solvers%252C%2520and%2520outperforms%2520recent%2520physics-guided%2520neural%2520simulators%2520in%2520accuracy%2520at%2520modest%2520additional%2520runtime.%25202%2529%2520Garment%2520draping.%2520We%2520train%2520a%2520self-supervised%252C%2520mesh-%2520and%2520garment-agnostic%2520model%2520that%2520generalizes%2520across%2520resolutions%2520and%2520garment%2520types%252C%2520delivering%2520up%2520to%2520an%2520order-of-magnitude%2520faster%2520inference%2520than%2520strong%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patch-based%20Representation%20and%20Learning%20for%20Efficient%20Deformation%20Modeling&entry.906535625=Ruochen%20Chen%20and%20Thuy%20Tran%20and%20Shaifali%20Parashar&entry.1292438233=In%20this%20paper%2C%20we%20present%20a%20patch-based%20representation%20of%20surfaces%2C%20PolyFit%2C%20which%20is%20obtained%20by%20fitting%20jet%20functions%20locally%20on%20surface%20patches.%20Such%20a%20representation%20can%20be%20learned%20efficiently%20in%20a%20supervised%20fashion%20from%20both%20analytic%20functions%20and%20real%20data.%20Once%20learned%2C%20it%20can%20be%20generalized%20to%20various%20types%20of%20surfaces.%20Using%20PolyFit%2C%20the%20surfaces%20can%20be%20efficiently%20deformed%20by%20updating%20a%20compact%20set%20of%20jet%20coefficients%20rather%20than%20optimizing%20per-vertex%20degrees%20of%20freedom%20for%20many%20downstream%20tasks%20in%20computer%20vision%20and%20graphics.%20We%20demonstrate%20the%20capabilities%20of%20our%20proposed%20methodologies%20with%20two%20applications%3A%201%29%20Shape-from-template%20%28SfT%29%3A%20where%20the%20goal%20is%20to%20deform%20the%20input%203D%20template%20of%20an%20object%20as%20seen%20in%20image/video.%20Using%20PolyFit%2C%20we%20adopt%20test-time%20optimization%20that%20delivers%20competitive%20accuracy%20while%20being%20markedly%20faster%20than%20offline%20physics-based%20solvers%2C%20and%20outperforms%20recent%20physics-guided%20neural%20simulators%20in%20accuracy%20at%20modest%20additional%20runtime.%202%29%20Garment%20draping.%20We%20train%20a%20self-supervised%2C%20mesh-%20and%20garment-agnostic%20model%20that%20generalizes%20across%20resolutions%20and%20garment%20types%2C%20delivering%20up%20to%20an%20order-of-magnitude%20faster%20inference%20than%20strong%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.05035v1&entry.124074799=Read"},
{"title": "GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models", "author": "Shurong Zheng and Yousong Zhu and Hongyin Zhao and Fan Yang and Yufei Zhan and Ming Tang and Jinqiao Wang", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.", "link": "http://arxiv.org/abs/2601.04777v1", "date": "2026-01-08", "relevancy": 2.7808, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5642}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeM-VG%3A%20Towards%20Generalized%20Multi-image%20Visual%20Grounding%20with%20Multimodal%20Large%20Language%20Models&body=Title%3A%20GeM-VG%3A%20Towards%20Generalized%20Multi-image%20Visual%20Grounding%20with%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Shurong%20Zheng%20and%20Yousong%20Zhu%20and%20Hongyin%20Zhao%20and%20Fan%20Yang%20and%20Yufei%20Zhan%20and%20Ming%20Tang%20and%20Jinqiao%20Wang%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20impressive%20progress%20in%20single-image%20grounding%20and%20general%20multi-image%20understanding.%20Recently%2C%20some%20methods%20begin%20to%20address%20multi-image%20grounding.%20However%2C%20they%20are%20constrained%20by%20single-target%20localization%20and%20limited%20types%20of%20practical%20tasks%2C%20due%20to%20the%20lack%20of%20unified%20modeling%20for%20generalized%20grounding%20tasks.%20Therefore%2C%20we%20propose%20GeM-VG%2C%20an%20MLLM%20capable%20of%20Generalized%20Multi-image%20Visual%20Grounding.%20To%20support%20this%2C%20we%20systematically%20categorize%20and%20organize%20existing%20multi-image%20grounding%20tasks%20according%20to%20their%20reliance%20of%20cross-image%20cues%20and%20reasoning%2C%20and%20introduce%20the%20MG-Data-240K%20dataset%2C%20addressing%20the%20limitations%20of%20existing%20datasets%20regarding%20target%20quantity%20and%20image%20relation.%20To%20tackle%20the%20challenges%20of%20robustly%20handling%20diverse%20multi-image%20grounding%20tasks%2C%20we%20further%20propose%20a%20hybrid%20reinforcement%20finetuning%20strategy%20that%20integrates%20chain-of-thought%20%28CoT%29%20reasoning%20and%20direct%20answering%2C%20considering%20their%20complementary%20strengths.%20This%20strategy%20adopts%20an%20R1-like%20algorithm%20guided%20by%20a%20carefully%20designed%20rule-based%20reward%2C%20effectively%20enhancing%20the%20model%27s%20overall%20perception%20and%20reasoning%20capabilities.%20Extensive%20experiments%20demonstrate%20the%20superior%20generalized%20grounding%20capabilities%20of%20our%20model.%20For%20multi-image%20grounding%2C%20it%20outperforms%20the%20previous%20leading%20MLLMs%20by%202.0%25%20and%209.7%25%20on%20MIG-Bench%20and%20MC-Bench%2C%20respectively.%20In%20single-image%20grounding%2C%20it%20achieves%20a%209.1%25%20improvement%20over%20the%20base%20model%20on%20ODINW.%20Furthermore%2C%20our%20model%20retains%20strong%20capabilities%20in%20general%20multi-image%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeM-VG%253A%2520Towards%2520Generalized%2520Multi-image%2520Visual%2520Grounding%2520with%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DShurong%2520Zheng%2520and%2520Yousong%2520Zhu%2520and%2520Hongyin%2520Zhao%2520and%2520Fan%2520Yang%2520and%2520Yufei%2520Zhan%2520and%2520Ming%2520Tang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520impressive%2520progress%2520in%2520single-image%2520grounding%2520and%2520general%2520multi-image%2520understanding.%2520Recently%252C%2520some%2520methods%2520begin%2520to%2520address%2520multi-image%2520grounding.%2520However%252C%2520they%2520are%2520constrained%2520by%2520single-target%2520localization%2520and%2520limited%2520types%2520of%2520practical%2520tasks%252C%2520due%2520to%2520the%2520lack%2520of%2520unified%2520modeling%2520for%2520generalized%2520grounding%2520tasks.%2520Therefore%252C%2520we%2520propose%2520GeM-VG%252C%2520an%2520MLLM%2520capable%2520of%2520Generalized%2520Multi-image%2520Visual%2520Grounding.%2520To%2520support%2520this%252C%2520we%2520systematically%2520categorize%2520and%2520organize%2520existing%2520multi-image%2520grounding%2520tasks%2520according%2520to%2520their%2520reliance%2520of%2520cross-image%2520cues%2520and%2520reasoning%252C%2520and%2520introduce%2520the%2520MG-Data-240K%2520dataset%252C%2520addressing%2520the%2520limitations%2520of%2520existing%2520datasets%2520regarding%2520target%2520quantity%2520and%2520image%2520relation.%2520To%2520tackle%2520the%2520challenges%2520of%2520robustly%2520handling%2520diverse%2520multi-image%2520grounding%2520tasks%252C%2520we%2520further%2520propose%2520a%2520hybrid%2520reinforcement%2520finetuning%2520strategy%2520that%2520integrates%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%2520and%2520direct%2520answering%252C%2520considering%2520their%2520complementary%2520strengths.%2520This%2520strategy%2520adopts%2520an%2520R1-like%2520algorithm%2520guided%2520by%2520a%2520carefully%2520designed%2520rule-based%2520reward%252C%2520effectively%2520enhancing%2520the%2520model%2527s%2520overall%2520perception%2520and%2520reasoning%2520capabilities.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superior%2520generalized%2520grounding%2520capabilities%2520of%2520our%2520model.%2520For%2520multi-image%2520grounding%252C%2520it%2520outperforms%2520the%2520previous%2520leading%2520MLLMs%2520by%25202.0%2525%2520and%25209.7%2525%2520on%2520MIG-Bench%2520and%2520MC-Bench%252C%2520respectively.%2520In%2520single-image%2520grounding%252C%2520it%2520achieves%2520a%25209.1%2525%2520improvement%2520over%2520the%2520base%2520model%2520on%2520ODINW.%2520Furthermore%252C%2520our%2520model%2520retains%2520strong%2520capabilities%2520in%2520general%2520multi-image%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeM-VG%3A%20Towards%20Generalized%20Multi-image%20Visual%20Grounding%20with%20Multimodal%20Large%20Language%20Models&entry.906535625=Shurong%20Zheng%20and%20Yousong%20Zhu%20and%20Hongyin%20Zhao%20and%20Fan%20Yang%20and%20Yufei%20Zhan%20and%20Ming%20Tang%20and%20Jinqiao%20Wang&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20impressive%20progress%20in%20single-image%20grounding%20and%20general%20multi-image%20understanding.%20Recently%2C%20some%20methods%20begin%20to%20address%20multi-image%20grounding.%20However%2C%20they%20are%20constrained%20by%20single-target%20localization%20and%20limited%20types%20of%20practical%20tasks%2C%20due%20to%20the%20lack%20of%20unified%20modeling%20for%20generalized%20grounding%20tasks.%20Therefore%2C%20we%20propose%20GeM-VG%2C%20an%20MLLM%20capable%20of%20Generalized%20Multi-image%20Visual%20Grounding.%20To%20support%20this%2C%20we%20systematically%20categorize%20and%20organize%20existing%20multi-image%20grounding%20tasks%20according%20to%20their%20reliance%20of%20cross-image%20cues%20and%20reasoning%2C%20and%20introduce%20the%20MG-Data-240K%20dataset%2C%20addressing%20the%20limitations%20of%20existing%20datasets%20regarding%20target%20quantity%20and%20image%20relation.%20To%20tackle%20the%20challenges%20of%20robustly%20handling%20diverse%20multi-image%20grounding%20tasks%2C%20we%20further%20propose%20a%20hybrid%20reinforcement%20finetuning%20strategy%20that%20integrates%20chain-of-thought%20%28CoT%29%20reasoning%20and%20direct%20answering%2C%20considering%20their%20complementary%20strengths.%20This%20strategy%20adopts%20an%20R1-like%20algorithm%20guided%20by%20a%20carefully%20designed%20rule-based%20reward%2C%20effectively%20enhancing%20the%20model%27s%20overall%20perception%20and%20reasoning%20capabilities.%20Extensive%20experiments%20demonstrate%20the%20superior%20generalized%20grounding%20capabilities%20of%20our%20model.%20For%20multi-image%20grounding%2C%20it%20outperforms%20the%20previous%20leading%20MLLMs%20by%202.0%25%20and%209.7%25%20on%20MIG-Bench%20and%20MC-Bench%2C%20respectively.%20In%20single-image%20grounding%2C%20it%20achieves%20a%209.1%25%20improvement%20over%20the%20base%20model%20on%20ODINW.%20Furthermore%2C%20our%20model%20retains%20strong%20capabilities%20in%20general%20multi-image%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2601.04777v1&entry.124074799=Read"},
{"title": "Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering", "author": "Shuliang Liu and Songbo Yang and Dong Fang and Sihang Jia and Yuqi Tang and Lingfeng Su and Ruoshui Peng and Yibo Yan and Xin Zou and Xuming Hu", "abstract": "Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.", "link": "http://arxiv.org/abs/2601.05159v1", "date": "2026-01-08", "relevancy": 2.765, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Introspection%3A%20Mitigating%20Overconfident%20Hallucinations%20in%20MLLMs%20via%20Interpretable%20Bi-Causal%20Steering&body=Title%3A%20Vision-Language%20Introspection%3A%20Mitigating%20Overconfident%20Hallucinations%20in%20MLLMs%20via%20Interpretable%20Bi-Causal%20Steering%0AAuthor%3A%20Shuliang%20Liu%20and%20Songbo%20Yang%20and%20Dong%20Fang%20and%20Sihang%20Jia%20and%20Yuqi%20Tang%20and%20Lingfeng%20Su%20and%20Ruoshui%20Peng%20and%20Yibo%20Yan%20and%20Xin%20Zou%20and%20Xuming%20Hu%0AAbstract%3A%20Object%20hallucination%20critically%20undermines%20the%20reliability%20of%20Multimodal%20Large%20Language%20Models%2C%20often%20stemming%20from%20a%20fundamental%20failure%20in%20cognitive%20introspection%2C%20where%20models%20blindly%20trust%20linguistic%20priors%20over%20specific%20visual%20evidence.%20Existing%20mitigations%20remain%20limited%3A%20contrastive%20decoding%20approaches%20operate%20superficially%20without%20rectifying%20internal%20semantic%20misalignments%2C%20while%20current%20latent%20steering%20methods%20rely%20on%20static%20vectors%20that%20lack%20instance-specific%20precision.%20We%20introduce%20Vision-Language%20Introspection%20%28VLI%29%2C%20a%20training-free%20inference%20framework%20that%20simulates%20a%20metacognitive%20self-correction%20process.%20VLI%20first%20performs%20Attributive%20Introspection%20to%20diagnose%20hallucination%20risks%20via%20probabilistic%20conflict%20detection%20and%20localize%20the%20causal%20visual%20anchors.%20It%20then%20employs%20Interpretable%20Bi-Causal%20Steering%20to%20actively%20modulate%20the%20inference%20process%2C%20dynamically%20isolating%20visual%20evidence%20from%20background%20noise%20while%20neutralizing%20blind%20confidence%20through%20adaptive%20calibration.%20VLI%20achieves%20state-of-the-art%20performance%20on%20advanced%20models%2C%20reducing%20object%20hallucination%20rates%20by%2012.67%25%20on%20MMHal-Bench%20and%20improving%20accuracy%20by%205.8%25%20on%20POPE.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Introspection%253A%2520Mitigating%2520Overconfident%2520Hallucinations%2520in%2520MLLMs%2520via%2520Interpretable%2520Bi-Causal%2520Steering%26entry.906535625%3DShuliang%2520Liu%2520and%2520Songbo%2520Yang%2520and%2520Dong%2520Fang%2520and%2520Sihang%2520Jia%2520and%2520Yuqi%2520Tang%2520and%2520Lingfeng%2520Su%2520and%2520Ruoshui%2520Peng%2520and%2520Yibo%2520Yan%2520and%2520Xin%2520Zou%2520and%2520Xuming%2520Hu%26entry.1292438233%3DObject%2520hallucination%2520critically%2520undermines%2520the%2520reliability%2520of%2520Multimodal%2520Large%2520Language%2520Models%252C%2520often%2520stemming%2520from%2520a%2520fundamental%2520failure%2520in%2520cognitive%2520introspection%252C%2520where%2520models%2520blindly%2520trust%2520linguistic%2520priors%2520over%2520specific%2520visual%2520evidence.%2520Existing%2520mitigations%2520remain%2520limited%253A%2520contrastive%2520decoding%2520approaches%2520operate%2520superficially%2520without%2520rectifying%2520internal%2520semantic%2520misalignments%252C%2520while%2520current%2520latent%2520steering%2520methods%2520rely%2520on%2520static%2520vectors%2520that%2520lack%2520instance-specific%2520precision.%2520We%2520introduce%2520Vision-Language%2520Introspection%2520%2528VLI%2529%252C%2520a%2520training-free%2520inference%2520framework%2520that%2520simulates%2520a%2520metacognitive%2520self-correction%2520process.%2520VLI%2520first%2520performs%2520Attributive%2520Introspection%2520to%2520diagnose%2520hallucination%2520risks%2520via%2520probabilistic%2520conflict%2520detection%2520and%2520localize%2520the%2520causal%2520visual%2520anchors.%2520It%2520then%2520employs%2520Interpretable%2520Bi-Causal%2520Steering%2520to%2520actively%2520modulate%2520the%2520inference%2520process%252C%2520dynamically%2520isolating%2520visual%2520evidence%2520from%2520background%2520noise%2520while%2520neutralizing%2520blind%2520confidence%2520through%2520adaptive%2520calibration.%2520VLI%2520achieves%2520state-of-the-art%2520performance%2520on%2520advanced%2520models%252C%2520reducing%2520object%2520hallucination%2520rates%2520by%252012.67%2525%2520on%2520MMHal-Bench%2520and%2520improving%2520accuracy%2520by%25205.8%2525%2520on%2520POPE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Introspection%3A%20Mitigating%20Overconfident%20Hallucinations%20in%20MLLMs%20via%20Interpretable%20Bi-Causal%20Steering&entry.906535625=Shuliang%20Liu%20and%20Songbo%20Yang%20and%20Dong%20Fang%20and%20Sihang%20Jia%20and%20Yuqi%20Tang%20and%20Lingfeng%20Su%20and%20Ruoshui%20Peng%20and%20Yibo%20Yan%20and%20Xin%20Zou%20and%20Xuming%20Hu&entry.1292438233=Object%20hallucination%20critically%20undermines%20the%20reliability%20of%20Multimodal%20Large%20Language%20Models%2C%20often%20stemming%20from%20a%20fundamental%20failure%20in%20cognitive%20introspection%2C%20where%20models%20blindly%20trust%20linguistic%20priors%20over%20specific%20visual%20evidence.%20Existing%20mitigations%20remain%20limited%3A%20contrastive%20decoding%20approaches%20operate%20superficially%20without%20rectifying%20internal%20semantic%20misalignments%2C%20while%20current%20latent%20steering%20methods%20rely%20on%20static%20vectors%20that%20lack%20instance-specific%20precision.%20We%20introduce%20Vision-Language%20Introspection%20%28VLI%29%2C%20a%20training-free%20inference%20framework%20that%20simulates%20a%20metacognitive%20self-correction%20process.%20VLI%20first%20performs%20Attributive%20Introspection%20to%20diagnose%20hallucination%20risks%20via%20probabilistic%20conflict%20detection%20and%20localize%20the%20causal%20visual%20anchors.%20It%20then%20employs%20Interpretable%20Bi-Causal%20Steering%20to%20actively%20modulate%20the%20inference%20process%2C%20dynamically%20isolating%20visual%20evidence%20from%20background%20noise%20while%20neutralizing%20blind%20confidence%20through%20adaptive%20calibration.%20VLI%20achieves%20state-of-the-art%20performance%20on%20advanced%20models%2C%20reducing%20object%20hallucination%20rates%20by%2012.67%25%20on%20MMHal-Bench%20and%20improving%20accuracy%20by%205.8%25%20on%20POPE.&entry.1838667208=http%3A//arxiv.org/abs/2601.05159v1&entry.124074799=Read"},
{"title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues", "author": "Yaning Pan and Qianqian Xie and Guohui Zhang and Zekun Wang and Yongqian Wen and Yuanxing Zhang and Haoxuan Hu and Zhiyu Pan and Yibing Huang and Zhidong Gan and Yonghong Lin and An Ping and Shihao Li and Yanghai Wang and Tianhao Peng and Jiaheng Liu", "abstract": "The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses 6 core competencies that focus on perceptivity and interactivity, encompassing 1,000 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.", "link": "http://arxiv.org/abs/2510.17722v2", "date": "2026-01-08", "relevancy": 2.7594, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MT-Video-Bench%3A%20A%20Holistic%20Video%20Understanding%20Benchmark%20for%20Evaluating%20Multimodal%20LLMs%20in%20Multi-Turn%20Dialogues&body=Title%3A%20MT-Video-Bench%3A%20A%20Holistic%20Video%20Understanding%20Benchmark%20for%20Evaluating%20Multimodal%20LLMs%20in%20Multi-Turn%20Dialogues%0AAuthor%3A%20Yaning%20Pan%20and%20Qianqian%20Xie%20and%20Guohui%20Zhang%20and%20Zekun%20Wang%20and%20Yongqian%20Wen%20and%20Yuanxing%20Zhang%20and%20Haoxuan%20Hu%20and%20Zhiyu%20Pan%20and%20Yibing%20Huang%20and%20Zhidong%20Gan%20and%20Yonghong%20Lin%20and%20An%20Ping%20and%20Shihao%20Li%20and%20Yanghai%20Wang%20and%20Tianhao%20Peng%20and%20Jiaheng%20Liu%0AAbstract%3A%20The%20recent%20development%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20significantly%20advanced%20AI%27s%20ability%20to%20understand%20visual%20modalities.%20However%2C%20existing%20evaluation%20benchmarks%20remain%20limited%20to%20single-turn%20question%20answering%2C%20overlooking%20the%20complexity%20of%20multi-turn%20dialogues%20in%20real-world%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20introduce%20MT-Video-Bench%2C%20a%20holistic%20video%20understanding%20benchmark%20for%20evaluating%20MLLMs%20in%20multi-turn%20dialogues.%20Specifically%2C%20our%20MT-Video-Bench%20mainly%20assesses%206%20core%20competencies%20that%20focus%20on%20perceptivity%20and%20interactivity%2C%20encompassing%201%2C000%20meticulously%20curated%20multi-turn%20dialogues%20from%20diverse%20domains.%20These%20capabilities%20are%20rigorously%20aligned%20with%20real-world%20applications%2C%20such%20as%20interactive%20sports%20analysis%20and%20multi-turn%20video-based%20intelligent%20tutoring.%20With%20MT-Video-Bench%2C%20we%20extensively%20evaluate%20various%20state-of-the-art%20open-source%20and%20closed-source%20MLLMs%2C%20revealing%20their%20significant%20performance%20discrepancies%20and%20limitations%20in%20handling%20multi-turn%20video%20dialogues.%20The%20benchmark%20will%20be%20publicly%20available%20to%20foster%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2510.17722v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMT-Video-Bench%253A%2520A%2520Holistic%2520Video%2520Understanding%2520Benchmark%2520for%2520Evaluating%2520Multimodal%2520LLMs%2520in%2520Multi-Turn%2520Dialogues%26entry.906535625%3DYaning%2520Pan%2520and%2520Qianqian%2520Xie%2520and%2520Guohui%2520Zhang%2520and%2520Zekun%2520Wang%2520and%2520Yongqian%2520Wen%2520and%2520Yuanxing%2520Zhang%2520and%2520Haoxuan%2520Hu%2520and%2520Zhiyu%2520Pan%2520and%2520Yibing%2520Huang%2520and%2520Zhidong%2520Gan%2520and%2520Yonghong%2520Lin%2520and%2520An%2520Ping%2520and%2520Shihao%2520Li%2520and%2520Yanghai%2520Wang%2520and%2520Tianhao%2520Peng%2520and%2520Jiaheng%2520Liu%26entry.1292438233%3DThe%2520recent%2520development%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520significantly%2520advanced%2520AI%2527s%2520ability%2520to%2520understand%2520visual%2520modalities.%2520However%252C%2520existing%2520evaluation%2520benchmarks%2520remain%2520limited%2520to%2520single-turn%2520question%2520answering%252C%2520overlooking%2520the%2520complexity%2520of%2520multi-turn%2520dialogues%2520in%2520real-world%2520scenarios.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520MT-Video-Bench%252C%2520a%2520holistic%2520video%2520understanding%2520benchmark%2520for%2520evaluating%2520MLLMs%2520in%2520multi-turn%2520dialogues.%2520Specifically%252C%2520our%2520MT-Video-Bench%2520mainly%2520assesses%25206%2520core%2520competencies%2520that%2520focus%2520on%2520perceptivity%2520and%2520interactivity%252C%2520encompassing%25201%252C000%2520meticulously%2520curated%2520multi-turn%2520dialogues%2520from%2520diverse%2520domains.%2520These%2520capabilities%2520are%2520rigorously%2520aligned%2520with%2520real-world%2520applications%252C%2520such%2520as%2520interactive%2520sports%2520analysis%2520and%2520multi-turn%2520video-based%2520intelligent%2520tutoring.%2520With%2520MT-Video-Bench%252C%2520we%2520extensively%2520evaluate%2520various%2520state-of-the-art%2520open-source%2520and%2520closed-source%2520MLLMs%252C%2520revealing%2520their%2520significant%2520performance%2520discrepancies%2520and%2520limitations%2520in%2520handling%2520multi-turn%2520video%2520dialogues.%2520The%2520benchmark%2520will%2520be%2520publicly%2520available%2520to%2520foster%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17722v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MT-Video-Bench%3A%20A%20Holistic%20Video%20Understanding%20Benchmark%20for%20Evaluating%20Multimodal%20LLMs%20in%20Multi-Turn%20Dialogues&entry.906535625=Yaning%20Pan%20and%20Qianqian%20Xie%20and%20Guohui%20Zhang%20and%20Zekun%20Wang%20and%20Yongqian%20Wen%20and%20Yuanxing%20Zhang%20and%20Haoxuan%20Hu%20and%20Zhiyu%20Pan%20and%20Yibing%20Huang%20and%20Zhidong%20Gan%20and%20Yonghong%20Lin%20and%20An%20Ping%20and%20Shihao%20Li%20and%20Yanghai%20Wang%20and%20Tianhao%20Peng%20and%20Jiaheng%20Liu&entry.1292438233=The%20recent%20development%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20significantly%20advanced%20AI%27s%20ability%20to%20understand%20visual%20modalities.%20However%2C%20existing%20evaluation%20benchmarks%20remain%20limited%20to%20single-turn%20question%20answering%2C%20overlooking%20the%20complexity%20of%20multi-turn%20dialogues%20in%20real-world%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20introduce%20MT-Video-Bench%2C%20a%20holistic%20video%20understanding%20benchmark%20for%20evaluating%20MLLMs%20in%20multi-turn%20dialogues.%20Specifically%2C%20our%20MT-Video-Bench%20mainly%20assesses%206%20core%20competencies%20that%20focus%20on%20perceptivity%20and%20interactivity%2C%20encompassing%201%2C000%20meticulously%20curated%20multi-turn%20dialogues%20from%20diverse%20domains.%20These%20capabilities%20are%20rigorously%20aligned%20with%20real-world%20applications%2C%20such%20as%20interactive%20sports%20analysis%20and%20multi-turn%20video-based%20intelligent%20tutoring.%20With%20MT-Video-Bench%2C%20we%20extensively%20evaluate%20various%20state-of-the-art%20open-source%20and%20closed-source%20MLLMs%2C%20revealing%20their%20significant%20performance%20discrepancies%20and%20limitations%20in%20handling%20multi-turn%20video%20dialogues.%20The%20benchmark%20will%20be%20publicly%20available%20to%20foster%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2510.17722v2&entry.124074799=Read"},
{"title": "Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video", "author": "Zeren Jiang and Chuanxia Zheng and Iro Laina and Diane Larlus and Andrea Vedaldi", "abstract": "We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.", "link": "http://arxiv.org/abs/2601.05251v1", "date": "2026-01-08", "relevancy": 2.719, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7357}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6537}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesh4D%3A%204D%20Mesh%20Reconstruction%20and%20Tracking%20from%20Monocular%20Video&body=Title%3A%20Mesh4D%3A%204D%20Mesh%20Reconstruction%20and%20Tracking%20from%20Monocular%20Video%0AAuthor%3A%20Zeren%20Jiang%20and%20Chuanxia%20Zheng%20and%20Iro%20Laina%20and%20Diane%20Larlus%20and%20Andrea%20Vedaldi%0AAbstract%3A%20We%20propose%20Mesh4D%2C%20a%20feed-forward%20model%20for%20monocular%204D%20mesh%20reconstruction.%20Given%20a%20monocular%20video%20of%20a%20dynamic%20object%2C%20our%20model%20reconstructs%20the%20object%27s%20complete%203D%20shape%20and%20motion%2C%20represented%20as%20a%20deformation%20field.%20Our%20key%20contribution%20is%20a%20compact%20latent%20space%20that%20encodes%20the%20entire%20animation%20sequence%20in%20a%20single%20pass.%20This%20latent%20space%20is%20learned%20by%20an%20autoencoder%20that%2C%20during%20training%2C%20is%20guided%20by%20the%20skeletal%20structure%20of%20the%20training%20objects%2C%20providing%20strong%20priors%20on%20plausible%20deformations.%20Crucially%2C%20skeletal%20information%20is%20not%20required%20at%20inference%20time.%20The%20encoder%20employs%20spatio-temporal%20attention%2C%20yielding%20a%20more%20stable%20representation%20of%20the%20object%27s%20overall%20deformation.%20Building%20on%20this%20representation%2C%20we%20train%20a%20latent%20diffusion%20model%20that%2C%20conditioned%20on%20the%20input%20video%20and%20the%20mesh%20reconstructed%20from%20the%20first%20frame%2C%20predicts%20the%20full%20animation%20in%20one%20shot.%20We%20evaluate%20Mesh4D%20on%20reconstruction%20and%20novel%20view%20synthesis%20benchmarks%2C%20outperforming%20prior%20methods%20in%20recovering%20accurate%203D%20shape%20and%20deformation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesh4D%253A%25204D%2520Mesh%2520Reconstruction%2520and%2520Tracking%2520from%2520Monocular%2520Video%26entry.906535625%3DZeren%2520Jiang%2520and%2520Chuanxia%2520Zheng%2520and%2520Iro%2520Laina%2520and%2520Diane%2520Larlus%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3DWe%2520propose%2520Mesh4D%252C%2520a%2520feed-forward%2520model%2520for%2520monocular%25204D%2520mesh%2520reconstruction.%2520Given%2520a%2520monocular%2520video%2520of%2520a%2520dynamic%2520object%252C%2520our%2520model%2520reconstructs%2520the%2520object%2527s%2520complete%25203D%2520shape%2520and%2520motion%252C%2520represented%2520as%2520a%2520deformation%2520field.%2520Our%2520key%2520contribution%2520is%2520a%2520compact%2520latent%2520space%2520that%2520encodes%2520the%2520entire%2520animation%2520sequence%2520in%2520a%2520single%2520pass.%2520This%2520latent%2520space%2520is%2520learned%2520by%2520an%2520autoencoder%2520that%252C%2520during%2520training%252C%2520is%2520guided%2520by%2520the%2520skeletal%2520structure%2520of%2520the%2520training%2520objects%252C%2520providing%2520strong%2520priors%2520on%2520plausible%2520deformations.%2520Crucially%252C%2520skeletal%2520information%2520is%2520not%2520required%2520at%2520inference%2520time.%2520The%2520encoder%2520employs%2520spatio-temporal%2520attention%252C%2520yielding%2520a%2520more%2520stable%2520representation%2520of%2520the%2520object%2527s%2520overall%2520deformation.%2520Building%2520on%2520this%2520representation%252C%2520we%2520train%2520a%2520latent%2520diffusion%2520model%2520that%252C%2520conditioned%2520on%2520the%2520input%2520video%2520and%2520the%2520mesh%2520reconstructed%2520from%2520the%2520first%2520frame%252C%2520predicts%2520the%2520full%2520animation%2520in%2520one%2520shot.%2520We%2520evaluate%2520Mesh4D%2520on%2520reconstruction%2520and%2520novel%2520view%2520synthesis%2520benchmarks%252C%2520outperforming%2520prior%2520methods%2520in%2520recovering%2520accurate%25203D%2520shape%2520and%2520deformation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesh4D%3A%204D%20Mesh%20Reconstruction%20and%20Tracking%20from%20Monocular%20Video&entry.906535625=Zeren%20Jiang%20and%20Chuanxia%20Zheng%20and%20Iro%20Laina%20and%20Diane%20Larlus%20and%20Andrea%20Vedaldi&entry.1292438233=We%20propose%20Mesh4D%2C%20a%20feed-forward%20model%20for%20monocular%204D%20mesh%20reconstruction.%20Given%20a%20monocular%20video%20of%20a%20dynamic%20object%2C%20our%20model%20reconstructs%20the%20object%27s%20complete%203D%20shape%20and%20motion%2C%20represented%20as%20a%20deformation%20field.%20Our%20key%20contribution%20is%20a%20compact%20latent%20space%20that%20encodes%20the%20entire%20animation%20sequence%20in%20a%20single%20pass.%20This%20latent%20space%20is%20learned%20by%20an%20autoencoder%20that%2C%20during%20training%2C%20is%20guided%20by%20the%20skeletal%20structure%20of%20the%20training%20objects%2C%20providing%20strong%20priors%20on%20plausible%20deformations.%20Crucially%2C%20skeletal%20information%20is%20not%20required%20at%20inference%20time.%20The%20encoder%20employs%20spatio-temporal%20attention%2C%20yielding%20a%20more%20stable%20representation%20of%20the%20object%27s%20overall%20deformation.%20Building%20on%20this%20representation%2C%20we%20train%20a%20latent%20diffusion%20model%20that%2C%20conditioned%20on%20the%20input%20video%20and%20the%20mesh%20reconstructed%20from%20the%20first%20frame%2C%20predicts%20the%20full%20animation%20in%20one%20shot.%20We%20evaluate%20Mesh4D%20on%20reconstruction%20and%20novel%20view%20synthesis%20benchmarks%2C%20outperforming%20prior%20methods%20in%20recovering%20accurate%203D%20shape%20and%20deformation.&entry.1838667208=http%3A//arxiv.org/abs/2601.05251v1&entry.124074799=Read"},
{"title": "A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering", "author": "Md. Zahid Hossain and Most. Sharmin Sultana Samu and Md. Rakibul Islam and Md. Siam Ansary", "abstract": "Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.", "link": "http://arxiv.org/abs/2601.05143v1", "date": "2026-01-08", "relevancy": 2.6848, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5625}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20and%20Explainable%20Vision-Language%20Framework%20for%20Crop%20Disease%20Visual%20Question%20Answering&body=Title%3A%20A%20Lightweight%20and%20Explainable%20Vision-Language%20Framework%20for%20Crop%20Disease%20Visual%20Question%20Answering%0AAuthor%3A%20Md.%20Zahid%20Hossain%20and%20Most.%20Sharmin%20Sultana%20Samu%20and%20Md.%20Rakibul%20Islam%20and%20Md.%20Siam%20Ansary%0AAbstract%3A%20Visual%20question%20answering%20for%20crop%20disease%20analysis%20requires%20accurate%20visual%20understanding%20and%20reliable%20language%20generation.%20This%20work%20presents%20a%20lightweight%20vision-language%20framework%20for%20crop%20and%20disease%20identification%20from%20leaf%20images.%20The%20proposed%20approach%20combines%20a%20Swin%20Transformer%20vision%20encoder%20with%20sequence-to-sequence%20language%20decoders.%20A%20two-stage%20training%20strategy%20is%20adopted%20to%20improve%20visual%20representation%20learning%20and%20cross-modal%20alignment.%20The%20model%20is%20evaluated%20on%20a%20large-scale%20crop%20disease%20dataset%20using%20classification%20and%20natural%20language%20generation%20metrics.%20Experimental%20results%20show%20high%20accuracy%20for%20both%20crop%20and%20disease%20identification.%20The%20framework%20also%20achieves%20strong%20performance%20on%20BLEU%2C%20ROUGE%20and%20BERTScore.%20Our%20proposed%20models%20outperform%20large-scale%20vision-language%20baselines%20while%20using%20significantly%20fewer%20parameters.%20Explainability%20is%20assessed%20using%20Grad-CAM%20and%20token-level%20attribution.%20Qualitative%20results%20demonstrate%20robust%20performance%20under%20diverse%20user-driven%20queries.%20These%20findings%20highlight%20the%20effectiveness%20of%20task-specific%20visual%20pretraining%20for%20crop%20disease%20visual%20question%20answering.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520and%2520Explainable%2520Vision-Language%2520Framework%2520for%2520Crop%2520Disease%2520Visual%2520Question%2520Answering%26entry.906535625%3DMd.%2520Zahid%2520Hossain%2520and%2520Most.%2520Sharmin%2520Sultana%2520Samu%2520and%2520Md.%2520Rakibul%2520Islam%2520and%2520Md.%2520Siam%2520Ansary%26entry.1292438233%3DVisual%2520question%2520answering%2520for%2520crop%2520disease%2520analysis%2520requires%2520accurate%2520visual%2520understanding%2520and%2520reliable%2520language%2520generation.%2520This%2520work%2520presents%2520a%2520lightweight%2520vision-language%2520framework%2520for%2520crop%2520and%2520disease%2520identification%2520from%2520leaf%2520images.%2520The%2520proposed%2520approach%2520combines%2520a%2520Swin%2520Transformer%2520vision%2520encoder%2520with%2520sequence-to-sequence%2520language%2520decoders.%2520A%2520two-stage%2520training%2520strategy%2520is%2520adopted%2520to%2520improve%2520visual%2520representation%2520learning%2520and%2520cross-modal%2520alignment.%2520The%2520model%2520is%2520evaluated%2520on%2520a%2520large-scale%2520crop%2520disease%2520dataset%2520using%2520classification%2520and%2520natural%2520language%2520generation%2520metrics.%2520Experimental%2520results%2520show%2520high%2520accuracy%2520for%2520both%2520crop%2520and%2520disease%2520identification.%2520The%2520framework%2520also%2520achieves%2520strong%2520performance%2520on%2520BLEU%252C%2520ROUGE%2520and%2520BERTScore.%2520Our%2520proposed%2520models%2520outperform%2520large-scale%2520vision-language%2520baselines%2520while%2520using%2520significantly%2520fewer%2520parameters.%2520Explainability%2520is%2520assessed%2520using%2520Grad-CAM%2520and%2520token-level%2520attribution.%2520Qualitative%2520results%2520demonstrate%2520robust%2520performance%2520under%2520diverse%2520user-driven%2520queries.%2520These%2520findings%2520highlight%2520the%2520effectiveness%2520of%2520task-specific%2520visual%2520pretraining%2520for%2520crop%2520disease%2520visual%2520question%2520answering.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20and%20Explainable%20Vision-Language%20Framework%20for%20Crop%20Disease%20Visual%20Question%20Answering&entry.906535625=Md.%20Zahid%20Hossain%20and%20Most.%20Sharmin%20Sultana%20Samu%20and%20Md.%20Rakibul%20Islam%20and%20Md.%20Siam%20Ansary&entry.1292438233=Visual%20question%20answering%20for%20crop%20disease%20analysis%20requires%20accurate%20visual%20understanding%20and%20reliable%20language%20generation.%20This%20work%20presents%20a%20lightweight%20vision-language%20framework%20for%20crop%20and%20disease%20identification%20from%20leaf%20images.%20The%20proposed%20approach%20combines%20a%20Swin%20Transformer%20vision%20encoder%20with%20sequence-to-sequence%20language%20decoders.%20A%20two-stage%20training%20strategy%20is%20adopted%20to%20improve%20visual%20representation%20learning%20and%20cross-modal%20alignment.%20The%20model%20is%20evaluated%20on%20a%20large-scale%20crop%20disease%20dataset%20using%20classification%20and%20natural%20language%20generation%20metrics.%20Experimental%20results%20show%20high%20accuracy%20for%20both%20crop%20and%20disease%20identification.%20The%20framework%20also%20achieves%20strong%20performance%20on%20BLEU%2C%20ROUGE%20and%20BERTScore.%20Our%20proposed%20models%20outperform%20large-scale%20vision-language%20baselines%20while%20using%20significantly%20fewer%20parameters.%20Explainability%20is%20assessed%20using%20Grad-CAM%20and%20token-level%20attribution.%20Qualitative%20results%20demonstrate%20robust%20performance%20under%20diverse%20user-driven%20queries.%20These%20findings%20highlight%20the%20effectiveness%20of%20task-specific%20visual%20pretraining%20for%20crop%20disease%20visual%20question%20answering.&entry.1838667208=http%3A//arxiv.org/abs/2601.05143v1&entry.124074799=Read"},
{"title": "Pelican Soup Framework: A Theoretical Framework for Language Model Capabilities", "author": "Ting-Rui Chiang and Dani Yogatama", "abstract": "In this work, we propose a simple theoretical framework, Pelican Soup, aiming to better understand how pretraining allows LLMs to (1) generalize to unseen instructions and (2) perform in-context learning, even when the verbalizers are irrelevant to the task. To this end, in our framework, we introduce the notion of \"knowledge base\" and \"reference-sense association\" and a simple formalism for natural language processing tasks. Our framework demonstrates how linguistic, psychology, and philosophy studies can inform our understanding of the language model and is connected to several other existing theoretical results. As an illustration of the usage of our framework, we derive a bound on in-context learning loss with our framework. Finally, we support our framework with empirical experiments and provide possible future research directions.", "link": "http://arxiv.org/abs/2402.10424v2", "date": "2026-01-08", "relevancy": 2.6765, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pelican%20Soup%20Framework%3A%20A%20Theoretical%20Framework%20for%20Language%20Model%20Capabilities&body=Title%3A%20Pelican%20Soup%20Framework%3A%20A%20Theoretical%20Framework%20for%20Language%20Model%20Capabilities%0AAuthor%3A%20Ting-Rui%20Chiang%20and%20Dani%20Yogatama%0AAbstract%3A%20In%20this%20work%2C%20we%20propose%20a%20simple%20theoretical%20framework%2C%20Pelican%20Soup%2C%20aiming%20to%20better%20understand%20how%20pretraining%20allows%20LLMs%20to%20%281%29%20generalize%20to%20unseen%20instructions%20and%20%282%29%20perform%20in-context%20learning%2C%20even%20when%20the%20verbalizers%20are%20irrelevant%20to%20the%20task.%20To%20this%20end%2C%20in%20our%20framework%2C%20we%20introduce%20the%20notion%20of%20%22knowledge%20base%22%20and%20%22reference-sense%20association%22%20and%20a%20simple%20formalism%20for%20natural%20language%20processing%20tasks.%20Our%20framework%20demonstrates%20how%20linguistic%2C%20psychology%2C%20and%20philosophy%20studies%20can%20inform%20our%20understanding%20of%20the%20language%20model%20and%20is%20connected%20to%20several%20other%20existing%20theoretical%20results.%20As%20an%20illustration%20of%20the%20usage%20of%20our%20framework%2C%20we%20derive%20a%20bound%20on%20in-context%20learning%20loss%20with%20our%20framework.%20Finally%2C%20we%20support%20our%20framework%20with%20empirical%20experiments%20and%20provide%20possible%20future%20research%20directions.%0ALink%3A%20http%3A//arxiv.org/abs/2402.10424v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPelican%2520Soup%2520Framework%253A%2520A%2520Theoretical%2520Framework%2520for%2520Language%2520Model%2520Capabilities%26entry.906535625%3DTing-Rui%2520Chiang%2520and%2520Dani%2520Yogatama%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520theoretical%2520framework%252C%2520Pelican%2520Soup%252C%2520aiming%2520to%2520better%2520understand%2520how%2520pretraining%2520allows%2520LLMs%2520to%2520%25281%2529%2520generalize%2520to%2520unseen%2520instructions%2520and%2520%25282%2529%2520perform%2520in-context%2520learning%252C%2520even%2520when%2520the%2520verbalizers%2520are%2520irrelevant%2520to%2520the%2520task.%2520To%2520this%2520end%252C%2520in%2520our%2520framework%252C%2520we%2520introduce%2520the%2520notion%2520of%2520%2522knowledge%2520base%2522%2520and%2520%2522reference-sense%2520association%2522%2520and%2520a%2520simple%2520formalism%2520for%2520natural%2520language%2520processing%2520tasks.%2520Our%2520framework%2520demonstrates%2520how%2520linguistic%252C%2520psychology%252C%2520and%2520philosophy%2520studies%2520can%2520inform%2520our%2520understanding%2520of%2520the%2520language%2520model%2520and%2520is%2520connected%2520to%2520several%2520other%2520existing%2520theoretical%2520results.%2520As%2520an%2520illustration%2520of%2520the%2520usage%2520of%2520our%2520framework%252C%2520we%2520derive%2520a%2520bound%2520on%2520in-context%2520learning%2520loss%2520with%2520our%2520framework.%2520Finally%252C%2520we%2520support%2520our%2520framework%2520with%2520empirical%2520experiments%2520and%2520provide%2520possible%2520future%2520research%2520directions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10424v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pelican%20Soup%20Framework%3A%20A%20Theoretical%20Framework%20for%20Language%20Model%20Capabilities&entry.906535625=Ting-Rui%20Chiang%20and%20Dani%20Yogatama&entry.1292438233=In%20this%20work%2C%20we%20propose%20a%20simple%20theoretical%20framework%2C%20Pelican%20Soup%2C%20aiming%20to%20better%20understand%20how%20pretraining%20allows%20LLMs%20to%20%281%29%20generalize%20to%20unseen%20instructions%20and%20%282%29%20perform%20in-context%20learning%2C%20even%20when%20the%20verbalizers%20are%20irrelevant%20to%20the%20task.%20To%20this%20end%2C%20in%20our%20framework%2C%20we%20introduce%20the%20notion%20of%20%22knowledge%20base%22%20and%20%22reference-sense%20association%22%20and%20a%20simple%20formalism%20for%20natural%20language%20processing%20tasks.%20Our%20framework%20demonstrates%20how%20linguistic%2C%20psychology%2C%20and%20philosophy%20studies%20can%20inform%20our%20understanding%20of%20the%20language%20model%20and%20is%20connected%20to%20several%20other%20existing%20theoretical%20results.%20As%20an%20illustration%20of%20the%20usage%20of%20our%20framework%2C%20we%20derive%20a%20bound%20on%20in-context%20learning%20loss%20with%20our%20framework.%20Finally%2C%20we%20support%20our%20framework%20with%20empirical%20experiments%20and%20provide%20possible%20future%20research%20directions.&entry.1838667208=http%3A//arxiv.org/abs/2402.10424v2&entry.124074799=Read"},
{"title": "Detector-Augmented SAMURAI for Long-Duration Drone Tracking", "author": "Tamara R. Lenhard and Andreas Weinmann and Hichem Snoussi and Tobias Koch", "abstract": "Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.", "link": "http://arxiv.org/abs/2601.04798v1", "date": "2026-01-08", "relevancy": 2.6629, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5433}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5315}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detector-Augmented%20SAMURAI%20for%20Long-Duration%20Drone%20Tracking&body=Title%3A%20Detector-Augmented%20SAMURAI%20for%20Long-Duration%20Drone%20Tracking%0AAuthor%3A%20Tamara%20R.%20Lenhard%20and%20Andreas%20Weinmann%20and%20Hichem%20Snoussi%20and%20Tobias%20Koch%0AAbstract%3A%20Robust%20long-term%20tracking%20of%20drone%20is%20a%20critical%20requirement%20for%20modern%20surveillance%20systems%2C%20given%20their%20increasing%20threat%20potential.%20While%20detector-based%20approaches%20typically%20achieve%20strong%20frame-level%20accuracy%2C%20they%20often%20suffer%20from%20temporal%20inconsistencies%20caused%20by%20frequent%20detection%20dropouts.%20Despite%20its%20practical%20relevance%2C%20research%20on%20RGB-based%20drone%20tracking%20is%20still%20limited%20and%20largely%20reliant%20on%20conventional%20motion%20models.%20Meanwhile%2C%20foundation%20models%20like%20SAMURAI%20have%20established%20their%20effectiveness%20across%20other%20domains%2C%20exhibiting%20strong%20category-agnostic%20tracking%20performance.%20However%2C%20their%20applicability%20in%20drone-specific%20scenarios%20has%20not%20been%20investigated%20yet.%20Motivated%20by%20this%20gap%2C%20we%20present%20the%20first%20systematic%20evaluation%20of%20SAMURAI%27s%20potential%20for%20robust%20drone%20tracking%20in%20urban%20surveillance%20settings.%20Furthermore%2C%20we%20introduce%20a%20detector-augmented%20extension%20of%20SAMURAI%20to%20mitigate%20sensitivity%20to%20bounding-box%20initialization%20and%20sequence%20length.%20Our%20findings%20demonstrate%20that%20the%20proposed%20extension%20significantly%20improves%20robustness%20in%20complex%20urban%20environments%2C%20with%20pronounced%20benefits%20in%20long-duration%20sequences%20-%20especially%20under%20drone%20exit-re-entry%20events.%20The%20incorporation%20of%20detector%20cues%20yields%20consistent%20gains%20over%20SAMURAI%27s%20zero-shot%20performance%20across%20datasets%20and%20metrics%2C%20with%20success%20rate%20improvements%20of%20up%20to%20%2B0.393%20and%20FNR%20reductions%20of%20up%20to%20-0.475.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetector-Augmented%2520SAMURAI%2520for%2520Long-Duration%2520Drone%2520Tracking%26entry.906535625%3DTamara%2520R.%2520Lenhard%2520and%2520Andreas%2520Weinmann%2520and%2520Hichem%2520Snoussi%2520and%2520Tobias%2520Koch%26entry.1292438233%3DRobust%2520long-term%2520tracking%2520of%2520drone%2520is%2520a%2520critical%2520requirement%2520for%2520modern%2520surveillance%2520systems%252C%2520given%2520their%2520increasing%2520threat%2520potential.%2520While%2520detector-based%2520approaches%2520typically%2520achieve%2520strong%2520frame-level%2520accuracy%252C%2520they%2520often%2520suffer%2520from%2520temporal%2520inconsistencies%2520caused%2520by%2520frequent%2520detection%2520dropouts.%2520Despite%2520its%2520practical%2520relevance%252C%2520research%2520on%2520RGB-based%2520drone%2520tracking%2520is%2520still%2520limited%2520and%2520largely%2520reliant%2520on%2520conventional%2520motion%2520models.%2520Meanwhile%252C%2520foundation%2520models%2520like%2520SAMURAI%2520have%2520established%2520their%2520effectiveness%2520across%2520other%2520domains%252C%2520exhibiting%2520strong%2520category-agnostic%2520tracking%2520performance.%2520However%252C%2520their%2520applicability%2520in%2520drone-specific%2520scenarios%2520has%2520not%2520been%2520investigated%2520yet.%2520Motivated%2520by%2520this%2520gap%252C%2520we%2520present%2520the%2520first%2520systematic%2520evaluation%2520of%2520SAMURAI%2527s%2520potential%2520for%2520robust%2520drone%2520tracking%2520in%2520urban%2520surveillance%2520settings.%2520Furthermore%252C%2520we%2520introduce%2520a%2520detector-augmented%2520extension%2520of%2520SAMURAI%2520to%2520mitigate%2520sensitivity%2520to%2520bounding-box%2520initialization%2520and%2520sequence%2520length.%2520Our%2520findings%2520demonstrate%2520that%2520the%2520proposed%2520extension%2520significantly%2520improves%2520robustness%2520in%2520complex%2520urban%2520environments%252C%2520with%2520pronounced%2520benefits%2520in%2520long-duration%2520sequences%2520-%2520especially%2520under%2520drone%2520exit-re-entry%2520events.%2520The%2520incorporation%2520of%2520detector%2520cues%2520yields%2520consistent%2520gains%2520over%2520SAMURAI%2527s%2520zero-shot%2520performance%2520across%2520datasets%2520and%2520metrics%252C%2520with%2520success%2520rate%2520improvements%2520of%2520up%2520to%2520%252B0.393%2520and%2520FNR%2520reductions%2520of%2520up%2520to%2520-0.475.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detector-Augmented%20SAMURAI%20for%20Long-Duration%20Drone%20Tracking&entry.906535625=Tamara%20R.%20Lenhard%20and%20Andreas%20Weinmann%20and%20Hichem%20Snoussi%20and%20Tobias%20Koch&entry.1292438233=Robust%20long-term%20tracking%20of%20drone%20is%20a%20critical%20requirement%20for%20modern%20surveillance%20systems%2C%20given%20their%20increasing%20threat%20potential.%20While%20detector-based%20approaches%20typically%20achieve%20strong%20frame-level%20accuracy%2C%20they%20often%20suffer%20from%20temporal%20inconsistencies%20caused%20by%20frequent%20detection%20dropouts.%20Despite%20its%20practical%20relevance%2C%20research%20on%20RGB-based%20drone%20tracking%20is%20still%20limited%20and%20largely%20reliant%20on%20conventional%20motion%20models.%20Meanwhile%2C%20foundation%20models%20like%20SAMURAI%20have%20established%20their%20effectiveness%20across%20other%20domains%2C%20exhibiting%20strong%20category-agnostic%20tracking%20performance.%20However%2C%20their%20applicability%20in%20drone-specific%20scenarios%20has%20not%20been%20investigated%20yet.%20Motivated%20by%20this%20gap%2C%20we%20present%20the%20first%20systematic%20evaluation%20of%20SAMURAI%27s%20potential%20for%20robust%20drone%20tracking%20in%20urban%20surveillance%20settings.%20Furthermore%2C%20we%20introduce%20a%20detector-augmented%20extension%20of%20SAMURAI%20to%20mitigate%20sensitivity%20to%20bounding-box%20initialization%20and%20sequence%20length.%20Our%20findings%20demonstrate%20that%20the%20proposed%20extension%20significantly%20improves%20robustness%20in%20complex%20urban%20environments%2C%20with%20pronounced%20benefits%20in%20long-duration%20sequences%20-%20especially%20under%20drone%20exit-re-entry%20events.%20The%20incorporation%20of%20detector%20cues%20yields%20consistent%20gains%20over%20SAMURAI%27s%20zero-shot%20performance%20across%20datasets%20and%20metrics%2C%20with%20success%20rate%20improvements%20of%20up%20to%20%2B0.393%20and%20FNR%20reductions%20of%20up%20to%20-0.475.&entry.1838667208=http%3A//arxiv.org/abs/2601.04798v1&entry.124074799=Read"},
{"title": "Segmentation-Driven Monocular Shape from Polarization based on Physical Model", "author": "Jinyu Zhang and Xu Ma and Weili Chen and Gonzalo R. Arce", "abstract": "Monocular shape-from-polarization (SfP) leverages the intrinsic relationship between light polarization properties and surface geometry to recover surface normals from single-view polarized images, providing a compact and robust approach for three-dimensional (3D) reconstruction. Despite its potential, existing monocular SfP methods suffer from azimuth angle ambiguity, an inherent limitation of polarization analysis, that severely compromises reconstruction accuracy and stability. This paper introduces a novel segmentation-driven monocular SfP (SMSfP) framework that reformulates global shape recovery into a set of local reconstructions over adaptively segmented convex sub-regions. Specifically, a polarization-aided adaptive region growing (PARG) segmentation strategy is proposed to decompose the global convexity assumption into locally convex regions, effectively suppressing azimuth ambiguities and preserving surface continuity. Furthermore, a multi-scale fusion convexity prior (MFCP) constraint is developed to ensure local surface consistency and enhance the recovery of fine textural and structural details. Extensive experiments on both synthetic and real-world datasets validate the proposed approach, showing significant improvements in disambiguation accuracy and geometric fidelity compared with existing physics-based monocular SfP techniques.", "link": "http://arxiv.org/abs/2601.04776v1", "date": "2026-01-08", "relevancy": 2.6419, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5478}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.519}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation-Driven%20Monocular%20Shape%20from%20Polarization%20based%20on%20Physical%20Model&body=Title%3A%20Segmentation-Driven%20Monocular%20Shape%20from%20Polarization%20based%20on%20Physical%20Model%0AAuthor%3A%20Jinyu%20Zhang%20and%20Xu%20Ma%20and%20Weili%20Chen%20and%20Gonzalo%20R.%20Arce%0AAbstract%3A%20Monocular%20shape-from-polarization%20%28SfP%29%20leverages%20the%20intrinsic%20relationship%20between%20light%20polarization%20properties%20and%20surface%20geometry%20to%20recover%20surface%20normals%20from%20single-view%20polarized%20images%2C%20providing%20a%20compact%20and%20robust%20approach%20for%20three-dimensional%20%283D%29%20reconstruction.%20Despite%20its%20potential%2C%20existing%20monocular%20SfP%20methods%20suffer%20from%20azimuth%20angle%20ambiguity%2C%20an%20inherent%20limitation%20of%20polarization%20analysis%2C%20that%20severely%20compromises%20reconstruction%20accuracy%20and%20stability.%20This%20paper%20introduces%20a%20novel%20segmentation-driven%20monocular%20SfP%20%28SMSfP%29%20framework%20that%20reformulates%20global%20shape%20recovery%20into%20a%20set%20of%20local%20reconstructions%20over%20adaptively%20segmented%20convex%20sub-regions.%20Specifically%2C%20a%20polarization-aided%20adaptive%20region%20growing%20%28PARG%29%20segmentation%20strategy%20is%20proposed%20to%20decompose%20the%20global%20convexity%20assumption%20into%20locally%20convex%20regions%2C%20effectively%20suppressing%20azimuth%20ambiguities%20and%20preserving%20surface%20continuity.%20Furthermore%2C%20a%20multi-scale%20fusion%20convexity%20prior%20%28MFCP%29%20constraint%20is%20developed%20to%20ensure%20local%20surface%20consistency%20and%20enhance%20the%20recovery%20of%20fine%20textural%20and%20structural%20details.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20validate%20the%20proposed%20approach%2C%20showing%20significant%20improvements%20in%20disambiguation%20accuracy%20and%20geometric%20fidelity%20compared%20with%20existing%20physics-based%20monocular%20SfP%20techniques.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation-Driven%2520Monocular%2520Shape%2520from%2520Polarization%2520based%2520on%2520Physical%2520Model%26entry.906535625%3DJinyu%2520Zhang%2520and%2520Xu%2520Ma%2520and%2520Weili%2520Chen%2520and%2520Gonzalo%2520R.%2520Arce%26entry.1292438233%3DMonocular%2520shape-from-polarization%2520%2528SfP%2529%2520leverages%2520the%2520intrinsic%2520relationship%2520between%2520light%2520polarization%2520properties%2520and%2520surface%2520geometry%2520to%2520recover%2520surface%2520normals%2520from%2520single-view%2520polarized%2520images%252C%2520providing%2520a%2520compact%2520and%2520robust%2520approach%2520for%2520three-dimensional%2520%25283D%2529%2520reconstruction.%2520Despite%2520its%2520potential%252C%2520existing%2520monocular%2520SfP%2520methods%2520suffer%2520from%2520azimuth%2520angle%2520ambiguity%252C%2520an%2520inherent%2520limitation%2520of%2520polarization%2520analysis%252C%2520that%2520severely%2520compromises%2520reconstruction%2520accuracy%2520and%2520stability.%2520This%2520paper%2520introduces%2520a%2520novel%2520segmentation-driven%2520monocular%2520SfP%2520%2528SMSfP%2529%2520framework%2520that%2520reformulates%2520global%2520shape%2520recovery%2520into%2520a%2520set%2520of%2520local%2520reconstructions%2520over%2520adaptively%2520segmented%2520convex%2520sub-regions.%2520Specifically%252C%2520a%2520polarization-aided%2520adaptive%2520region%2520growing%2520%2528PARG%2529%2520segmentation%2520strategy%2520is%2520proposed%2520to%2520decompose%2520the%2520global%2520convexity%2520assumption%2520into%2520locally%2520convex%2520regions%252C%2520effectively%2520suppressing%2520azimuth%2520ambiguities%2520and%2520preserving%2520surface%2520continuity.%2520Furthermore%252C%2520a%2520multi-scale%2520fusion%2520convexity%2520prior%2520%2528MFCP%2529%2520constraint%2520is%2520developed%2520to%2520ensure%2520local%2520surface%2520consistency%2520and%2520enhance%2520the%2520recovery%2520of%2520fine%2520textural%2520and%2520structural%2520details.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520validate%2520the%2520proposed%2520approach%252C%2520showing%2520significant%2520improvements%2520in%2520disambiguation%2520accuracy%2520and%2520geometric%2520fidelity%2520compared%2520with%2520existing%2520physics-based%2520monocular%2520SfP%2520techniques.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation-Driven%20Monocular%20Shape%20from%20Polarization%20based%20on%20Physical%20Model&entry.906535625=Jinyu%20Zhang%20and%20Xu%20Ma%20and%20Weili%20Chen%20and%20Gonzalo%20R.%20Arce&entry.1292438233=Monocular%20shape-from-polarization%20%28SfP%29%20leverages%20the%20intrinsic%20relationship%20between%20light%20polarization%20properties%20and%20surface%20geometry%20to%20recover%20surface%20normals%20from%20single-view%20polarized%20images%2C%20providing%20a%20compact%20and%20robust%20approach%20for%20three-dimensional%20%283D%29%20reconstruction.%20Despite%20its%20potential%2C%20existing%20monocular%20SfP%20methods%20suffer%20from%20azimuth%20angle%20ambiguity%2C%20an%20inherent%20limitation%20of%20polarization%20analysis%2C%20that%20severely%20compromises%20reconstruction%20accuracy%20and%20stability.%20This%20paper%20introduces%20a%20novel%20segmentation-driven%20monocular%20SfP%20%28SMSfP%29%20framework%20that%20reformulates%20global%20shape%20recovery%20into%20a%20set%20of%20local%20reconstructions%20over%20adaptively%20segmented%20convex%20sub-regions.%20Specifically%2C%20a%20polarization-aided%20adaptive%20region%20growing%20%28PARG%29%20segmentation%20strategy%20is%20proposed%20to%20decompose%20the%20global%20convexity%20assumption%20into%20locally%20convex%20regions%2C%20effectively%20suppressing%20azimuth%20ambiguities%20and%20preserving%20surface%20continuity.%20Furthermore%2C%20a%20multi-scale%20fusion%20convexity%20prior%20%28MFCP%29%20constraint%20is%20developed%20to%20ensure%20local%20surface%20consistency%20and%20enhance%20the%20recovery%20of%20fine%20textural%20and%20structural%20details.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20validate%20the%20proposed%20approach%2C%20showing%20significant%20improvements%20in%20disambiguation%20accuracy%20and%20geometric%20fidelity%20compared%20with%20existing%20physics-based%20monocular%20SfP%20techniques.&entry.1838667208=http%3A//arxiv.org/abs/2601.04776v1&entry.124074799=Read"},
{"title": "FALCONEye: Finding Answers and Localizing Content in ONE-hour-long videos with multi-modal LLMs", "author": "Carlos Plou and Cesar Borja and Ruben Martinez-Cantin and Ana C. Murillo", "abstract": "Finding information in hour-long videos is a challenging task even for top-performing Vision Language Models (VLMs), as encoding visual content quickly exceeds available context windows. To tackle this challenge, we present FALCONEye, a novel video agent based on a training-free, model-agnostic meta-architecture composed of a VLM and a Large Language Model (LLM). FALCONEye answers open-ended questions using an exploration-based search algorithm guided by calibrated confidence from the VLM's answers. We also introduce the FALCON-Bench benchmark, extending Question Answering problem to Video Answer Search-requiring models to return both the answer and its supporting temporal window for open-ended questions in hour-long videos. With just a 7B VLM and a lightweight LLM, FALCONEye outscores all open-source 7B VLMs and comparable agents in FALCON-Bench. It further demonstrates its generalization capability in MLVU benchmark with shorter videos and different tasks, surpassing GPT-4o on single-detail tasks while slashing inference cost by roughly an order of magnitude.", "link": "http://arxiv.org/abs/2503.19850v3", "date": "2026-01-08", "relevancy": 2.6086, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FALCONEye%3A%20Finding%20Answers%20and%20Localizing%20Content%20in%20ONE-hour-long%20videos%20with%20multi-modal%20LLMs&body=Title%3A%20FALCONEye%3A%20Finding%20Answers%20and%20Localizing%20Content%20in%20ONE-hour-long%20videos%20with%20multi-modal%20LLMs%0AAuthor%3A%20Carlos%20Plou%20and%20Cesar%20Borja%20and%20Ruben%20Martinez-Cantin%20and%20Ana%20C.%20Murillo%0AAbstract%3A%20Finding%20information%20in%20hour-long%20videos%20is%20a%20challenging%20task%20even%20for%20top-performing%20Vision%20Language%20Models%20%28VLMs%29%2C%20as%20encoding%20visual%20content%20quickly%20exceeds%20available%20context%20windows.%20To%20tackle%20this%20challenge%2C%20we%20present%20FALCONEye%2C%20a%20novel%20video%20agent%20based%20on%20a%20training-free%2C%20model-agnostic%20meta-architecture%20composed%20of%20a%20VLM%20and%20a%20Large%20Language%20Model%20%28LLM%29.%20FALCONEye%20answers%20open-ended%20questions%20using%20an%20exploration-based%20search%20algorithm%20guided%20by%20calibrated%20confidence%20from%20the%20VLM%27s%20answers.%20We%20also%20introduce%20the%20FALCON-Bench%20benchmark%2C%20extending%20Question%20Answering%20problem%20to%20Video%20Answer%20Search-requiring%20models%20to%20return%20both%20the%20answer%20and%20its%20supporting%20temporal%20window%20for%20open-ended%20questions%20in%20hour-long%20videos.%20With%20just%20a%207B%20VLM%20and%20a%20lightweight%20LLM%2C%20FALCONEye%20outscores%20all%20open-source%207B%20VLMs%20and%20comparable%20agents%20in%20FALCON-Bench.%20It%20further%20demonstrates%20its%20generalization%20capability%20in%20MLVU%20benchmark%20with%20shorter%20videos%20and%20different%20tasks%2C%20surpassing%20GPT-4o%20on%20single-detail%20tasks%20while%20slashing%20inference%20cost%20by%20roughly%20an%20order%20of%20magnitude.%0ALink%3A%20http%3A//arxiv.org/abs/2503.19850v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFALCONEye%253A%2520Finding%2520Answers%2520and%2520Localizing%2520Content%2520in%2520ONE-hour-long%2520videos%2520with%2520multi-modal%2520LLMs%26entry.906535625%3DCarlos%2520Plou%2520and%2520Cesar%2520Borja%2520and%2520Ruben%2520Martinez-Cantin%2520and%2520Ana%2520C.%2520Murillo%26entry.1292438233%3DFinding%2520information%2520in%2520hour-long%2520videos%2520is%2520a%2520challenging%2520task%2520even%2520for%2520top-performing%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%252C%2520as%2520encoding%2520visual%2520content%2520quickly%2520exceeds%2520available%2520context%2520windows.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520present%2520FALCONEye%252C%2520a%2520novel%2520video%2520agent%2520based%2520on%2520a%2520training-free%252C%2520model-agnostic%2520meta-architecture%2520composed%2520of%2520a%2520VLM%2520and%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529.%2520FALCONEye%2520answers%2520open-ended%2520questions%2520using%2520an%2520exploration-based%2520search%2520algorithm%2520guided%2520by%2520calibrated%2520confidence%2520from%2520the%2520VLM%2527s%2520answers.%2520We%2520also%2520introduce%2520the%2520FALCON-Bench%2520benchmark%252C%2520extending%2520Question%2520Answering%2520problem%2520to%2520Video%2520Answer%2520Search-requiring%2520models%2520to%2520return%2520both%2520the%2520answer%2520and%2520its%2520supporting%2520temporal%2520window%2520for%2520open-ended%2520questions%2520in%2520hour-long%2520videos.%2520With%2520just%2520a%25207B%2520VLM%2520and%2520a%2520lightweight%2520LLM%252C%2520FALCONEye%2520outscores%2520all%2520open-source%25207B%2520VLMs%2520and%2520comparable%2520agents%2520in%2520FALCON-Bench.%2520It%2520further%2520demonstrates%2520its%2520generalization%2520capability%2520in%2520MLVU%2520benchmark%2520with%2520shorter%2520videos%2520and%2520different%2520tasks%252C%2520surpassing%2520GPT-4o%2520on%2520single-detail%2520tasks%2520while%2520slashing%2520inference%2520cost%2520by%2520roughly%2520an%2520order%2520of%2520magnitude.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19850v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FALCONEye%3A%20Finding%20Answers%20and%20Localizing%20Content%20in%20ONE-hour-long%20videos%20with%20multi-modal%20LLMs&entry.906535625=Carlos%20Plou%20and%20Cesar%20Borja%20and%20Ruben%20Martinez-Cantin%20and%20Ana%20C.%20Murillo&entry.1292438233=Finding%20information%20in%20hour-long%20videos%20is%20a%20challenging%20task%20even%20for%20top-performing%20Vision%20Language%20Models%20%28VLMs%29%2C%20as%20encoding%20visual%20content%20quickly%20exceeds%20available%20context%20windows.%20To%20tackle%20this%20challenge%2C%20we%20present%20FALCONEye%2C%20a%20novel%20video%20agent%20based%20on%20a%20training-free%2C%20model-agnostic%20meta-architecture%20composed%20of%20a%20VLM%20and%20a%20Large%20Language%20Model%20%28LLM%29.%20FALCONEye%20answers%20open-ended%20questions%20using%20an%20exploration-based%20search%20algorithm%20guided%20by%20calibrated%20confidence%20from%20the%20VLM%27s%20answers.%20We%20also%20introduce%20the%20FALCON-Bench%20benchmark%2C%20extending%20Question%20Answering%20problem%20to%20Video%20Answer%20Search-requiring%20models%20to%20return%20both%20the%20answer%20and%20its%20supporting%20temporal%20window%20for%20open-ended%20questions%20in%20hour-long%20videos.%20With%20just%20a%207B%20VLM%20and%20a%20lightweight%20LLM%2C%20FALCONEye%20outscores%20all%20open-source%207B%20VLMs%20and%20comparable%20agents%20in%20FALCON-Bench.%20It%20further%20demonstrates%20its%20generalization%20capability%20in%20MLVU%20benchmark%20with%20shorter%20videos%20and%20different%20tasks%2C%20surpassing%20GPT-4o%20on%20single-detail%20tasks%20while%20slashing%20inference%20cost%20by%20roughly%20an%20order%20of%20magnitude.&entry.1838667208=http%3A//arxiv.org/abs/2503.19850v3&entry.124074799=Read"},
{"title": "Defocus Aberration Theory Confirms Gaussian Model in Most Imaging Devices", "author": "Akbar Saadat", "abstract": "Over the past three decades, defocus has consistently provided groundbreaking depth information in scene images. However, accurately estimating depth from 2D images continues to be a persistent and fundamental challenge in the field of 3D recovery. Heuristic approaches involve with the ill-posed problem for inferring the spatial variant defocusing blur, as the desired blur cannot be distinguished from the inherent blur. Given a prior knowledge of the defocus model, the problem become well-posed with an analytic solution for the relative blur between two images, taken at the same viewpoint with different camera settings for the focus. The Gaussian model stands out as an optimal choice for real-time applications, due to its mathematical simplicity and computational efficiency. And theoretically, it is the only model can be applied at the same time to both the absolute blur caused by depth in a single image and the relative blur resulting from depth differences between two images. This paper introduces the settings, for conventional imaging devices, to ensure that the defocusing operator adheres to the Gaussian model. Defocus analysis begins within the framework of geometric optics and is conducted by defocus aberration theory in diffraction-limited optics to obtain the accuracy of fitting the actual model to its Gaussian approximation. The results for a typical set of focused depths between $1$ and $100$ meters, with a maximum depth variation of $10\\%$ at the focused depth, confirm the Gaussian model's applicability for defocus operators in most imaging devices. The findings demonstrate a maximum Mean Absolute Error $(\\!M\\!A\\!E)$ of less than $1\\%$, underscoring the model's accuracy and reliability.", "link": "http://arxiv.org/abs/2601.04779v1", "date": "2026-01-08", "relevancy": 2.5988, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5466}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5064}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defocus%20Aberration%20Theory%20Confirms%20Gaussian%20Model%20in%20Most%20Imaging%20Devices&body=Title%3A%20Defocus%20Aberration%20Theory%20Confirms%20Gaussian%20Model%20in%20Most%20Imaging%20Devices%0AAuthor%3A%20Akbar%20Saadat%0AAbstract%3A%20Over%20the%20past%20three%20decades%2C%20defocus%20has%20consistently%20provided%20groundbreaking%20depth%20information%20in%20scene%20images.%20However%2C%20accurately%20estimating%20depth%20from%202D%20images%20continues%20to%20be%20a%20persistent%20and%20fundamental%20challenge%20in%20the%20field%20of%203D%20recovery.%20Heuristic%20approaches%20involve%20with%20the%20ill-posed%20problem%20for%20inferring%20the%20spatial%20variant%20defocusing%20blur%2C%20as%20the%20desired%20blur%20cannot%20be%20distinguished%20from%20the%20inherent%20blur.%20Given%20a%20prior%20knowledge%20of%20the%20defocus%20model%2C%20the%20problem%20become%20well-posed%20with%20an%20analytic%20solution%20for%20the%20relative%20blur%20between%20two%20images%2C%20taken%20at%20the%20same%20viewpoint%20with%20different%20camera%20settings%20for%20the%20focus.%20The%20Gaussian%20model%20stands%20out%20as%20an%20optimal%20choice%20for%20real-time%20applications%2C%20due%20to%20its%20mathematical%20simplicity%20and%20computational%20efficiency.%20And%20theoretically%2C%20it%20is%20the%20only%20model%20can%20be%20applied%20at%20the%20same%20time%20to%20both%20the%20absolute%20blur%20caused%20by%20depth%20in%20a%20single%20image%20and%20the%20relative%20blur%20resulting%20from%20depth%20differences%20between%20two%20images.%20This%20paper%20introduces%20the%20settings%2C%20for%20conventional%20imaging%20devices%2C%20to%20ensure%20that%20the%20defocusing%20operator%20adheres%20to%20the%20Gaussian%20model.%20Defocus%20analysis%20begins%20within%20the%20framework%20of%20geometric%20optics%20and%20is%20conducted%20by%20defocus%20aberration%20theory%20in%20diffraction-limited%20optics%20to%20obtain%20the%20accuracy%20of%20fitting%20the%20actual%20model%20to%20its%20Gaussian%20approximation.%20The%20results%20for%20a%20typical%20set%20of%20focused%20depths%20between%20%241%24%20and%20%24100%24%20meters%2C%20with%20a%20maximum%20depth%20variation%20of%20%2410%5C%25%24%20at%20the%20focused%20depth%2C%20confirm%20the%20Gaussian%20model%27s%20applicability%20for%20defocus%20operators%20in%20most%20imaging%20devices.%20The%20findings%20demonstrate%20a%20maximum%20Mean%20Absolute%20Error%20%24%28%5C%21M%5C%21A%5C%21E%29%24%20of%20less%20than%20%241%5C%25%24%2C%20underscoring%20the%20model%27s%20accuracy%20and%20reliability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefocus%2520Aberration%2520Theory%2520Confirms%2520Gaussian%2520Model%2520in%2520Most%2520Imaging%2520Devices%26entry.906535625%3DAkbar%2520Saadat%26entry.1292438233%3DOver%2520the%2520past%2520three%2520decades%252C%2520defocus%2520has%2520consistently%2520provided%2520groundbreaking%2520depth%2520information%2520in%2520scene%2520images.%2520However%252C%2520accurately%2520estimating%2520depth%2520from%25202D%2520images%2520continues%2520to%2520be%2520a%2520persistent%2520and%2520fundamental%2520challenge%2520in%2520the%2520field%2520of%25203D%2520recovery.%2520Heuristic%2520approaches%2520involve%2520with%2520the%2520ill-posed%2520problem%2520for%2520inferring%2520the%2520spatial%2520variant%2520defocusing%2520blur%252C%2520as%2520the%2520desired%2520blur%2520cannot%2520be%2520distinguished%2520from%2520the%2520inherent%2520blur.%2520Given%2520a%2520prior%2520knowledge%2520of%2520the%2520defocus%2520model%252C%2520the%2520problem%2520become%2520well-posed%2520with%2520an%2520analytic%2520solution%2520for%2520the%2520relative%2520blur%2520between%2520two%2520images%252C%2520taken%2520at%2520the%2520same%2520viewpoint%2520with%2520different%2520camera%2520settings%2520for%2520the%2520focus.%2520The%2520Gaussian%2520model%2520stands%2520out%2520as%2520an%2520optimal%2520choice%2520for%2520real-time%2520applications%252C%2520due%2520to%2520its%2520mathematical%2520simplicity%2520and%2520computational%2520efficiency.%2520And%2520theoretically%252C%2520it%2520is%2520the%2520only%2520model%2520can%2520be%2520applied%2520at%2520the%2520same%2520time%2520to%2520both%2520the%2520absolute%2520blur%2520caused%2520by%2520depth%2520in%2520a%2520single%2520image%2520and%2520the%2520relative%2520blur%2520resulting%2520from%2520depth%2520differences%2520between%2520two%2520images.%2520This%2520paper%2520introduces%2520the%2520settings%252C%2520for%2520conventional%2520imaging%2520devices%252C%2520to%2520ensure%2520that%2520the%2520defocusing%2520operator%2520adheres%2520to%2520the%2520Gaussian%2520model.%2520Defocus%2520analysis%2520begins%2520within%2520the%2520framework%2520of%2520geometric%2520optics%2520and%2520is%2520conducted%2520by%2520defocus%2520aberration%2520theory%2520in%2520diffraction-limited%2520optics%2520to%2520obtain%2520the%2520accuracy%2520of%2520fitting%2520the%2520actual%2520model%2520to%2520its%2520Gaussian%2520approximation.%2520The%2520results%2520for%2520a%2520typical%2520set%2520of%2520focused%2520depths%2520between%2520%25241%2524%2520and%2520%2524100%2524%2520meters%252C%2520with%2520a%2520maximum%2520depth%2520variation%2520of%2520%252410%255C%2525%2524%2520at%2520the%2520focused%2520depth%252C%2520confirm%2520the%2520Gaussian%2520model%2527s%2520applicability%2520for%2520defocus%2520operators%2520in%2520most%2520imaging%2520devices.%2520The%2520findings%2520demonstrate%2520a%2520maximum%2520Mean%2520Absolute%2520Error%2520%2524%2528%255C%2521M%255C%2521A%255C%2521E%2529%2524%2520of%2520less%2520than%2520%25241%255C%2525%2524%252C%2520underscoring%2520the%2520model%2527s%2520accuracy%2520and%2520reliability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defocus%20Aberration%20Theory%20Confirms%20Gaussian%20Model%20in%20Most%20Imaging%20Devices&entry.906535625=Akbar%20Saadat&entry.1292438233=Over%20the%20past%20three%20decades%2C%20defocus%20has%20consistently%20provided%20groundbreaking%20depth%20information%20in%20scene%20images.%20However%2C%20accurately%20estimating%20depth%20from%202D%20images%20continues%20to%20be%20a%20persistent%20and%20fundamental%20challenge%20in%20the%20field%20of%203D%20recovery.%20Heuristic%20approaches%20involve%20with%20the%20ill-posed%20problem%20for%20inferring%20the%20spatial%20variant%20defocusing%20blur%2C%20as%20the%20desired%20blur%20cannot%20be%20distinguished%20from%20the%20inherent%20blur.%20Given%20a%20prior%20knowledge%20of%20the%20defocus%20model%2C%20the%20problem%20become%20well-posed%20with%20an%20analytic%20solution%20for%20the%20relative%20blur%20between%20two%20images%2C%20taken%20at%20the%20same%20viewpoint%20with%20different%20camera%20settings%20for%20the%20focus.%20The%20Gaussian%20model%20stands%20out%20as%20an%20optimal%20choice%20for%20real-time%20applications%2C%20due%20to%20its%20mathematical%20simplicity%20and%20computational%20efficiency.%20And%20theoretically%2C%20it%20is%20the%20only%20model%20can%20be%20applied%20at%20the%20same%20time%20to%20both%20the%20absolute%20blur%20caused%20by%20depth%20in%20a%20single%20image%20and%20the%20relative%20blur%20resulting%20from%20depth%20differences%20between%20two%20images.%20This%20paper%20introduces%20the%20settings%2C%20for%20conventional%20imaging%20devices%2C%20to%20ensure%20that%20the%20defocusing%20operator%20adheres%20to%20the%20Gaussian%20model.%20Defocus%20analysis%20begins%20within%20the%20framework%20of%20geometric%20optics%20and%20is%20conducted%20by%20defocus%20aberration%20theory%20in%20diffraction-limited%20optics%20to%20obtain%20the%20accuracy%20of%20fitting%20the%20actual%20model%20to%20its%20Gaussian%20approximation.%20The%20results%20for%20a%20typical%20set%20of%20focused%20depths%20between%20%241%24%20and%20%24100%24%20meters%2C%20with%20a%20maximum%20depth%20variation%20of%20%2410%5C%25%24%20at%20the%20focused%20depth%2C%20confirm%20the%20Gaussian%20model%27s%20applicability%20for%20defocus%20operators%20in%20most%20imaging%20devices.%20The%20findings%20demonstrate%20a%20maximum%20Mean%20Absolute%20Error%20%24%28%5C%21M%5C%21A%5C%21E%29%24%20of%20less%20than%20%241%5C%25%24%2C%20underscoring%20the%20model%27s%20accuracy%20and%20reliability.&entry.1838667208=http%3A//arxiv.org/abs/2601.04779v1&entry.124074799=Read"},
{"title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering", "author": "Yichen Xu and Liangyu Chen and Liang Zhang and Jianzhe Ma and Wenxuan Wang and Qin Jin", "abstract": "Charts are a universally adopted medium for data communication, yet existing chart understanding benchmarks are overwhelmingly English-centric, limiting their accessibility and relevance to global audiences. To address this limitation, we introduce PolyChartQA, the first large-scale multilingual benchmark for chart question answering, comprising 22,606 charts and 26,151 QA pairs across 10 diverse languages. PolyChartQA is constructed through a scalable pipeline that enables efficient multilingual chart generation via data translation and code reuse, supported by LLM-based translation and rigorous quality control. We systematically evaluate multilingual chart understanding with PolyChartQA on state-of-the-art LVLMs and reveal a significant performance gap between English and other languages, particularly low-resource ones. Additionally, we introduce a companion multilingual chart question answering training set, PolyChartQA-Train, on which fine-tuning LVLMs yields substantial gains in multilingual chart understanding across diverse model sizes and architectures. Together, our benchmark provides a foundation for developing globally inclusive vision-language models capable of understanding charts across diverse linguistic contexts.", "link": "http://arxiv.org/abs/2507.11939v2", "date": "2026-01-08", "relevancy": 2.5604, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POLYCHARTQA%3A%20Benchmarking%20Large%20Vision-Language%20Models%20with%20Multilingual%20Chart%20Question%20Answering&body=Title%3A%20POLYCHARTQA%3A%20Benchmarking%20Large%20Vision-Language%20Models%20with%20Multilingual%20Chart%20Question%20Answering%0AAuthor%3A%20Yichen%20Xu%20and%20Liangyu%20Chen%20and%20Liang%20Zhang%20and%20Jianzhe%20Ma%20and%20Wenxuan%20Wang%20and%20Qin%20Jin%0AAbstract%3A%20Charts%20are%20a%20universally%20adopted%20medium%20for%20data%20communication%2C%20yet%20existing%20chart%20understanding%20benchmarks%20are%20overwhelmingly%20English-centric%2C%20limiting%20their%20accessibility%20and%20relevance%20to%20global%20audiences.%20To%20address%20this%20limitation%2C%20we%20introduce%20PolyChartQA%2C%20the%20first%20large-scale%20multilingual%20benchmark%20for%20chart%20question%20answering%2C%20comprising%2022%2C606%20charts%20and%2026%2C151%20QA%20pairs%20across%2010%20diverse%20languages.%20PolyChartQA%20is%20constructed%20through%20a%20scalable%20pipeline%20that%20enables%20efficient%20multilingual%20chart%20generation%20via%20data%20translation%20and%20code%20reuse%2C%20supported%20by%20LLM-based%20translation%20and%20rigorous%20quality%20control.%20We%20systematically%20evaluate%20multilingual%20chart%20understanding%20with%20PolyChartQA%20on%20state-of-the-art%20LVLMs%20and%20reveal%20a%20significant%20performance%20gap%20between%20English%20and%20other%20languages%2C%20particularly%20low-resource%20ones.%20Additionally%2C%20we%20introduce%20a%20companion%20multilingual%20chart%20question%20answering%20training%20set%2C%20PolyChartQA-Train%2C%20on%20which%20fine-tuning%20LVLMs%20yields%20substantial%20gains%20in%20multilingual%20chart%20understanding%20across%20diverse%20model%20sizes%20and%20architectures.%20Together%2C%20our%20benchmark%20provides%20a%20foundation%20for%20developing%20globally%20inclusive%20vision-language%20models%20capable%20of%20understanding%20charts%20across%20diverse%20linguistic%20contexts.%0ALink%3A%20http%3A//arxiv.org/abs/2507.11939v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOLYCHARTQA%253A%2520Benchmarking%2520Large%2520Vision-Language%2520Models%2520with%2520Multilingual%2520Chart%2520Question%2520Answering%26entry.906535625%3DYichen%2520Xu%2520and%2520Liangyu%2520Chen%2520and%2520Liang%2520Zhang%2520and%2520Jianzhe%2520Ma%2520and%2520Wenxuan%2520Wang%2520and%2520Qin%2520Jin%26entry.1292438233%3DCharts%2520are%2520a%2520universally%2520adopted%2520medium%2520for%2520data%2520communication%252C%2520yet%2520existing%2520chart%2520understanding%2520benchmarks%2520are%2520overwhelmingly%2520English-centric%252C%2520limiting%2520their%2520accessibility%2520and%2520relevance%2520to%2520global%2520audiences.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520PolyChartQA%252C%2520the%2520first%2520large-scale%2520multilingual%2520benchmark%2520for%2520chart%2520question%2520answering%252C%2520comprising%252022%252C606%2520charts%2520and%252026%252C151%2520QA%2520pairs%2520across%252010%2520diverse%2520languages.%2520PolyChartQA%2520is%2520constructed%2520through%2520a%2520scalable%2520pipeline%2520that%2520enables%2520efficient%2520multilingual%2520chart%2520generation%2520via%2520data%2520translation%2520and%2520code%2520reuse%252C%2520supported%2520by%2520LLM-based%2520translation%2520and%2520rigorous%2520quality%2520control.%2520We%2520systematically%2520evaluate%2520multilingual%2520chart%2520understanding%2520with%2520PolyChartQA%2520on%2520state-of-the-art%2520LVLMs%2520and%2520reveal%2520a%2520significant%2520performance%2520gap%2520between%2520English%2520and%2520other%2520languages%252C%2520particularly%2520low-resource%2520ones.%2520Additionally%252C%2520we%2520introduce%2520a%2520companion%2520multilingual%2520chart%2520question%2520answering%2520training%2520set%252C%2520PolyChartQA-Train%252C%2520on%2520which%2520fine-tuning%2520LVLMs%2520yields%2520substantial%2520gains%2520in%2520multilingual%2520chart%2520understanding%2520across%2520diverse%2520model%2520sizes%2520and%2520architectures.%2520Together%252C%2520our%2520benchmark%2520provides%2520a%2520foundation%2520for%2520developing%2520globally%2520inclusive%2520vision-language%2520models%2520capable%2520of%2520understanding%2520charts%2520across%2520diverse%2520linguistic%2520contexts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11939v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POLYCHARTQA%3A%20Benchmarking%20Large%20Vision-Language%20Models%20with%20Multilingual%20Chart%20Question%20Answering&entry.906535625=Yichen%20Xu%20and%20Liangyu%20Chen%20and%20Liang%20Zhang%20and%20Jianzhe%20Ma%20and%20Wenxuan%20Wang%20and%20Qin%20Jin&entry.1292438233=Charts%20are%20a%20universally%20adopted%20medium%20for%20data%20communication%2C%20yet%20existing%20chart%20understanding%20benchmarks%20are%20overwhelmingly%20English-centric%2C%20limiting%20their%20accessibility%20and%20relevance%20to%20global%20audiences.%20To%20address%20this%20limitation%2C%20we%20introduce%20PolyChartQA%2C%20the%20first%20large-scale%20multilingual%20benchmark%20for%20chart%20question%20answering%2C%20comprising%2022%2C606%20charts%20and%2026%2C151%20QA%20pairs%20across%2010%20diverse%20languages.%20PolyChartQA%20is%20constructed%20through%20a%20scalable%20pipeline%20that%20enables%20efficient%20multilingual%20chart%20generation%20via%20data%20translation%20and%20code%20reuse%2C%20supported%20by%20LLM-based%20translation%20and%20rigorous%20quality%20control.%20We%20systematically%20evaluate%20multilingual%20chart%20understanding%20with%20PolyChartQA%20on%20state-of-the-art%20LVLMs%20and%20reveal%20a%20significant%20performance%20gap%20between%20English%20and%20other%20languages%2C%20particularly%20low-resource%20ones.%20Additionally%2C%20we%20introduce%20a%20companion%20multilingual%20chart%20question%20answering%20training%20set%2C%20PolyChartQA-Train%2C%20on%20which%20fine-tuning%20LVLMs%20yields%20substantial%20gains%20in%20multilingual%20chart%20understanding%20across%20diverse%20model%20sizes%20and%20architectures.%20Together%2C%20our%20benchmark%20provides%20a%20foundation%20for%20developing%20globally%20inclusive%20vision-language%20models%20capable%20of%20understanding%20charts%20across%20diverse%20linguistic%20contexts.&entry.1838667208=http%3A//arxiv.org/abs/2507.11939v2&entry.124074799=Read"},
{"title": "Novel View Synthesis using DDIM Inversion", "author": "Sehajdeep Singh and A V Subramanyam and Aditya Gupta and Sahil Gupta", "abstract": "Synthesizing novel views from a single input image is a challenging task. It requires extrapolating the 3D structure of a scene while inferring details in occluded regions, and maintaining geometric consistency across viewpoints. Many existing methods must fine-tune large diffusion backbones using multiple views or train a diffusion model from scratch, which is extremely expensive. Additionally, they suffer from blurry reconstruction and poor generalization. This gap presents the opportunity to explore an explicit lightweight view translation framework that can directly utilize the high-fidelity generative capabilities of a pretrained diffusion model while reconstructing a scene from a novel view. Given the DDIM-inverted latent of a single input image, we employ a camera pose-conditioned translation U-Net, TUNet, to predict the inverted latent corresponding to the desired target view. However, the image sampled using the predicted latent may result in a blurry reconstruction. To this end, we propose a novel fusion strategy that exploits the inherent noise correlation structure observed in DDIM inversion. The proposed fusion strategy helps preserve the texture and fine-grained details. To synthesize the novel view, we use the fused latent as the initial condition for DDIM sampling, leveraging the generative prior of the pretrained diffusion model. Extensive experiments on MVImgNet demonstrate that our method outperforms existing methods.", "link": "http://arxiv.org/abs/2508.10688v2", "date": "2026-01-08", "relevancy": 2.5286, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6359}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6314}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20View%20Synthesis%20using%20DDIM%20Inversion&body=Title%3A%20Novel%20View%20Synthesis%20using%20DDIM%20Inversion%0AAuthor%3A%20Sehajdeep%20Singh%20and%20A%20V%20Subramanyam%20and%20Aditya%20Gupta%20and%20Sahil%20Gupta%0AAbstract%3A%20Synthesizing%20novel%20views%20from%20a%20single%20input%20image%20is%20a%20challenging%20task.%20It%20requires%20extrapolating%20the%203D%20structure%20of%20a%20scene%20while%20inferring%20details%20in%20occluded%20regions%2C%20and%20maintaining%20geometric%20consistency%20across%20viewpoints.%20Many%20existing%20methods%20must%20fine-tune%20large%20diffusion%20backbones%20using%20multiple%20views%20or%20train%20a%20diffusion%20model%20from%20scratch%2C%20which%20is%20extremely%20expensive.%20Additionally%2C%20they%20suffer%20from%20blurry%20reconstruction%20and%20poor%20generalization.%20This%20gap%20presents%20the%20opportunity%20to%20explore%20an%20explicit%20lightweight%20view%20translation%20framework%20that%20can%20directly%20utilize%20the%20high-fidelity%20generative%20capabilities%20of%20a%20pretrained%20diffusion%20model%20while%20reconstructing%20a%20scene%20from%20a%20novel%20view.%20Given%20the%20DDIM-inverted%20latent%20of%20a%20single%20input%20image%2C%20we%20employ%20a%20camera%20pose-conditioned%20translation%20U-Net%2C%20TUNet%2C%20to%20predict%20the%20inverted%20latent%20corresponding%20to%20the%20desired%20target%20view.%20However%2C%20the%20image%20sampled%20using%20the%20predicted%20latent%20may%20result%20in%20a%20blurry%20reconstruction.%20To%20this%20end%2C%20we%20propose%20a%20novel%20fusion%20strategy%20that%20exploits%20the%20inherent%20noise%20correlation%20structure%20observed%20in%20DDIM%20inversion.%20The%20proposed%20fusion%20strategy%20helps%20preserve%20the%20texture%20and%20fine-grained%20details.%20To%20synthesize%20the%20novel%20view%2C%20we%20use%20the%20fused%20latent%20as%20the%20initial%20condition%20for%20DDIM%20sampling%2C%20leveraging%20the%20generative%20prior%20of%20the%20pretrained%20diffusion%20model.%20Extensive%20experiments%20on%20MVImgNet%20demonstrate%20that%20our%20method%20outperforms%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2508.10688v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520View%2520Synthesis%2520using%2520DDIM%2520Inversion%26entry.906535625%3DSehajdeep%2520Singh%2520and%2520A%2520V%2520Subramanyam%2520and%2520Aditya%2520Gupta%2520and%2520Sahil%2520Gupta%26entry.1292438233%3DSynthesizing%2520novel%2520views%2520from%2520a%2520single%2520input%2520image%2520is%2520a%2520challenging%2520task.%2520It%2520requires%2520extrapolating%2520the%25203D%2520structure%2520of%2520a%2520scene%2520while%2520inferring%2520details%2520in%2520occluded%2520regions%252C%2520and%2520maintaining%2520geometric%2520consistency%2520across%2520viewpoints.%2520Many%2520existing%2520methods%2520must%2520fine-tune%2520large%2520diffusion%2520backbones%2520using%2520multiple%2520views%2520or%2520train%2520a%2520diffusion%2520model%2520from%2520scratch%252C%2520which%2520is%2520extremely%2520expensive.%2520Additionally%252C%2520they%2520suffer%2520from%2520blurry%2520reconstruction%2520and%2520poor%2520generalization.%2520This%2520gap%2520presents%2520the%2520opportunity%2520to%2520explore%2520an%2520explicit%2520lightweight%2520view%2520translation%2520framework%2520that%2520can%2520directly%2520utilize%2520the%2520high-fidelity%2520generative%2520capabilities%2520of%2520a%2520pretrained%2520diffusion%2520model%2520while%2520reconstructing%2520a%2520scene%2520from%2520a%2520novel%2520view.%2520Given%2520the%2520DDIM-inverted%2520latent%2520of%2520a%2520single%2520input%2520image%252C%2520we%2520employ%2520a%2520camera%2520pose-conditioned%2520translation%2520U-Net%252C%2520TUNet%252C%2520to%2520predict%2520the%2520inverted%2520latent%2520corresponding%2520to%2520the%2520desired%2520target%2520view.%2520However%252C%2520the%2520image%2520sampled%2520using%2520the%2520predicted%2520latent%2520may%2520result%2520in%2520a%2520blurry%2520reconstruction.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520fusion%2520strategy%2520that%2520exploits%2520the%2520inherent%2520noise%2520correlation%2520structure%2520observed%2520in%2520DDIM%2520inversion.%2520The%2520proposed%2520fusion%2520strategy%2520helps%2520preserve%2520the%2520texture%2520and%2520fine-grained%2520details.%2520To%2520synthesize%2520the%2520novel%2520view%252C%2520we%2520use%2520the%2520fused%2520latent%2520as%2520the%2520initial%2520condition%2520for%2520DDIM%2520sampling%252C%2520leveraging%2520the%2520generative%2520prior%2520of%2520the%2520pretrained%2520diffusion%2520model.%2520Extensive%2520experiments%2520on%2520MVImgNet%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10688v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20View%20Synthesis%20using%20DDIM%20Inversion&entry.906535625=Sehajdeep%20Singh%20and%20A%20V%20Subramanyam%20and%20Aditya%20Gupta%20and%20Sahil%20Gupta&entry.1292438233=Synthesizing%20novel%20views%20from%20a%20single%20input%20image%20is%20a%20challenging%20task.%20It%20requires%20extrapolating%20the%203D%20structure%20of%20a%20scene%20while%20inferring%20details%20in%20occluded%20regions%2C%20and%20maintaining%20geometric%20consistency%20across%20viewpoints.%20Many%20existing%20methods%20must%20fine-tune%20large%20diffusion%20backbones%20using%20multiple%20views%20or%20train%20a%20diffusion%20model%20from%20scratch%2C%20which%20is%20extremely%20expensive.%20Additionally%2C%20they%20suffer%20from%20blurry%20reconstruction%20and%20poor%20generalization.%20This%20gap%20presents%20the%20opportunity%20to%20explore%20an%20explicit%20lightweight%20view%20translation%20framework%20that%20can%20directly%20utilize%20the%20high-fidelity%20generative%20capabilities%20of%20a%20pretrained%20diffusion%20model%20while%20reconstructing%20a%20scene%20from%20a%20novel%20view.%20Given%20the%20DDIM-inverted%20latent%20of%20a%20single%20input%20image%2C%20we%20employ%20a%20camera%20pose-conditioned%20translation%20U-Net%2C%20TUNet%2C%20to%20predict%20the%20inverted%20latent%20corresponding%20to%20the%20desired%20target%20view.%20However%2C%20the%20image%20sampled%20using%20the%20predicted%20latent%20may%20result%20in%20a%20blurry%20reconstruction.%20To%20this%20end%2C%20we%20propose%20a%20novel%20fusion%20strategy%20that%20exploits%20the%20inherent%20noise%20correlation%20structure%20observed%20in%20DDIM%20inversion.%20The%20proposed%20fusion%20strategy%20helps%20preserve%20the%20texture%20and%20fine-grained%20details.%20To%20synthesize%20the%20novel%20view%2C%20we%20use%20the%20fused%20latent%20as%20the%20initial%20condition%20for%20DDIM%20sampling%2C%20leveraging%20the%20generative%20prior%20of%20the%20pretrained%20diffusion%20model.%20Extensive%20experiments%20on%20MVImgNet%20demonstrate%20that%20our%20method%20outperforms%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2508.10688v2&entry.124074799=Read"},
{"title": "Mechanisms of Prompt-Induced Hallucination in Vision-Language Models", "author": "William Rudman and Michal Golovanevsky and Dana Arad and Yonatan Belinkov and Ritambhara Singh and Carsten Eickhoff and Kyle Mahowald", "abstract": "Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.", "link": "http://arxiv.org/abs/2601.05201v1", "date": "2026-01-08", "relevancy": 2.4895, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanisms%20of%20Prompt-Induced%20Hallucination%20in%20Vision-Language%20Models&body=Title%3A%20Mechanisms%20of%20Prompt-Induced%20Hallucination%20in%20Vision-Language%20Models%0AAuthor%3A%20William%20Rudman%20and%20Michal%20Golovanevsky%20and%20Dana%20Arad%20and%20Yonatan%20Belinkov%20and%20Ritambhara%20Singh%20and%20Carsten%20Eickhoff%20and%20Kyle%20Mahowald%0AAbstract%3A%20Large%20vision-language%20models%20%28VLMs%29%20are%20highly%20capable%2C%20yet%20often%20hallucinate%20by%20favoring%20textual%20prompts%20over%20visual%20evidence.%20We%20study%20this%20failure%20mode%20in%20a%20controlled%20object-counting%20setting%2C%20where%20the%20prompt%20overstates%20the%20number%20of%20objects%20in%20the%20image%20%28e.g.%2C%20asking%20a%20model%20to%20describe%20four%20waterlilies%20when%20only%20three%20are%20present%29.%20At%20low%20object%20counts%2C%20models%20often%20correct%20the%20overestimation%2C%20but%20as%20the%20number%20of%20objects%20increases%2C%20they%20increasingly%20conform%20to%20the%20prompt%20regardless%20of%20the%20discrepancy.%20Through%20mechanistic%20analysis%20of%20three%20VLMs%2C%20we%20identify%20a%20small%20set%20of%20attention%20heads%20whose%20ablation%20substantially%20reduces%20prompt-induced%20hallucinations%20%28PIH%29%20by%20at%20least%2040%25%20without%20additional%20training.%20Across%20models%2C%20PIH-heads%20mediate%20prompt%20copying%20in%20model-specific%20ways.%20We%20characterize%20these%20differences%20and%20show%20that%20PIH%20ablation%20increases%20correction%20toward%20visual%20evidence.%20Our%20findings%20offer%20insights%20into%20the%20internal%20mechanisms%20driving%20prompt-induced%20hallucinations%2C%20revealing%20model-specific%20differences%20in%20how%20these%20behaviors%20are%20implemented.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanisms%2520of%2520Prompt-Induced%2520Hallucination%2520in%2520Vision-Language%2520Models%26entry.906535625%3DWilliam%2520Rudman%2520and%2520Michal%2520Golovanevsky%2520and%2520Dana%2520Arad%2520and%2520Yonatan%2520Belinkov%2520and%2520Ritambhara%2520Singh%2520and%2520Carsten%2520Eickhoff%2520and%2520Kyle%2520Mahowald%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528VLMs%2529%2520are%2520highly%2520capable%252C%2520yet%2520often%2520hallucinate%2520by%2520favoring%2520textual%2520prompts%2520over%2520visual%2520evidence.%2520We%2520study%2520this%2520failure%2520mode%2520in%2520a%2520controlled%2520object-counting%2520setting%252C%2520where%2520the%2520prompt%2520overstates%2520the%2520number%2520of%2520objects%2520in%2520the%2520image%2520%2528e.g.%252C%2520asking%2520a%2520model%2520to%2520describe%2520four%2520waterlilies%2520when%2520only%2520three%2520are%2520present%2529.%2520At%2520low%2520object%2520counts%252C%2520models%2520often%2520correct%2520the%2520overestimation%252C%2520but%2520as%2520the%2520number%2520of%2520objects%2520increases%252C%2520they%2520increasingly%2520conform%2520to%2520the%2520prompt%2520regardless%2520of%2520the%2520discrepancy.%2520Through%2520mechanistic%2520analysis%2520of%2520three%2520VLMs%252C%2520we%2520identify%2520a%2520small%2520set%2520of%2520attention%2520heads%2520whose%2520ablation%2520substantially%2520reduces%2520prompt-induced%2520hallucinations%2520%2528PIH%2529%2520by%2520at%2520least%252040%2525%2520without%2520additional%2520training.%2520Across%2520models%252C%2520PIH-heads%2520mediate%2520prompt%2520copying%2520in%2520model-specific%2520ways.%2520We%2520characterize%2520these%2520differences%2520and%2520show%2520that%2520PIH%2520ablation%2520increases%2520correction%2520toward%2520visual%2520evidence.%2520Our%2520findings%2520offer%2520insights%2520into%2520the%2520internal%2520mechanisms%2520driving%2520prompt-induced%2520hallucinations%252C%2520revealing%2520model-specific%2520differences%2520in%2520how%2520these%2520behaviors%2520are%2520implemented.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanisms%20of%20Prompt-Induced%20Hallucination%20in%20Vision-Language%20Models&entry.906535625=William%20Rudman%20and%20Michal%20Golovanevsky%20and%20Dana%20Arad%20and%20Yonatan%20Belinkov%20and%20Ritambhara%20Singh%20and%20Carsten%20Eickhoff%20and%20Kyle%20Mahowald&entry.1292438233=Large%20vision-language%20models%20%28VLMs%29%20are%20highly%20capable%2C%20yet%20often%20hallucinate%20by%20favoring%20textual%20prompts%20over%20visual%20evidence.%20We%20study%20this%20failure%20mode%20in%20a%20controlled%20object-counting%20setting%2C%20where%20the%20prompt%20overstates%20the%20number%20of%20objects%20in%20the%20image%20%28e.g.%2C%20asking%20a%20model%20to%20describe%20four%20waterlilies%20when%20only%20three%20are%20present%29.%20At%20low%20object%20counts%2C%20models%20often%20correct%20the%20overestimation%2C%20but%20as%20the%20number%20of%20objects%20increases%2C%20they%20increasingly%20conform%20to%20the%20prompt%20regardless%20of%20the%20discrepancy.%20Through%20mechanistic%20analysis%20of%20three%20VLMs%2C%20we%20identify%20a%20small%20set%20of%20attention%20heads%20whose%20ablation%20substantially%20reduces%20prompt-induced%20hallucinations%20%28PIH%29%20by%20at%20least%2040%25%20without%20additional%20training.%20Across%20models%2C%20PIH-heads%20mediate%20prompt%20copying%20in%20model-specific%20ways.%20We%20characterize%20these%20differences%20and%20show%20that%20PIH%20ablation%20increases%20correction%20toward%20visual%20evidence.%20Our%20findings%20offer%20insights%20into%20the%20internal%20mechanisms%20driving%20prompt-induced%20hallucinations%2C%20revealing%20model-specific%20differences%20in%20how%20these%20behaviors%20are%20implemented.&entry.1838667208=http%3A//arxiv.org/abs/2601.05201v1&entry.124074799=Read"},
{"title": "ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination", "author": "Teng Wang and Xinxin Zhao and Wenzhe Cai and Changyin Sun", "abstract": "Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling long-horizon tasks such as object search. While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning remains constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry--critical factors for navigation decisions. We explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this through an imagination-powered navigation framework, ImagineNav++, which imagines future observation images from candidate robot views and translates navigation planning into a simple best-view image selection problem for VLMs. First, a future-view imagination module distills human navigation preferences to generate semantically meaningful viewpoints with high exploration potential. These imagined views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a selective foveation memory mechanism, which hierarchically integrates keyframe observations via a sparse-to-dense framework, constructing a compact yet comprehensive memory for long-term spatial reasoning. This approach transforms goal-oriented navigation into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks show that ImagineNav++ achieves SOTA performance in mapless settings, even surpassing most map-based methods, highlighting the importance of scene imagination and memory in VLM-based spatial reasoning.", "link": "http://arxiv.org/abs/2512.17435v2", "date": "2026-01-08", "relevancy": 2.4663, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6369}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6077}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImagineNav%2B%2B%3A%20Prompting%20Vision-Language%20Models%20as%20Embodied%20Navigator%20through%20Scene%20Imagination&body=Title%3A%20ImagineNav%2B%2B%3A%20Prompting%20Vision-Language%20Models%20as%20Embodied%20Navigator%20through%20Scene%20Imagination%0AAuthor%3A%20Teng%20Wang%20and%20Xinxin%20Zhao%20and%20Wenzhe%20Cai%20and%20Changyin%20Sun%0AAbstract%3A%20Visual%20navigation%20is%20a%20fundamental%20capability%20for%20autonomous%20home-assistance%20robots%2C%20enabling%20long-horizon%20tasks%20such%20as%20object%20search.%20While%20recent%20methods%20have%20leveraged%20Large%20Language%20Models%20%28LLMs%29%20to%20incorporate%20commonsense%20reasoning%20and%20improve%20exploration%20efficiency%2C%20their%20planning%20remains%20constrained%20by%20textual%20representations%2C%20which%20cannot%20adequately%20capture%20spatial%20occupancy%20or%20scene%20geometry--critical%20factors%20for%20navigation%20decisions.%20We%20explore%20whether%20Vision-Language%20Models%20%28VLMs%29%20can%20achieve%20mapless%20visual%20navigation%20using%20only%20onboard%20RGB/RGB-D%20streams%2C%20unlocking%20their%20potential%20for%20spatial%20perception%20and%20planning.%20We%20achieve%20this%20through%20an%20imagination-powered%20navigation%20framework%2C%20ImagineNav%2B%2B%2C%20which%20imagines%20future%20observation%20images%20from%20candidate%20robot%20views%20and%20translates%20navigation%20planning%20into%20a%20simple%20best-view%20image%20selection%20problem%20for%20VLMs.%20First%2C%20a%20future-view%20imagination%20module%20distills%20human%20navigation%20preferences%20to%20generate%20semantically%20meaningful%20viewpoints%20with%20high%20exploration%20potential.%20These%20imagined%20views%20then%20serve%20as%20visual%20prompts%20for%20the%20VLM%20to%20identify%20the%20most%20informative%20viewpoint.%20To%20maintain%20spatial%20consistency%2C%20we%20develop%20a%20selective%20foveation%20memory%20mechanism%2C%20which%20hierarchically%20integrates%20keyframe%20observations%20via%20a%20sparse-to-dense%20framework%2C%20constructing%20a%20compact%20yet%20comprehensive%20memory%20for%20long-term%20spatial%20reasoning.%20This%20approach%20transforms%20goal-oriented%20navigation%20into%20a%20series%20of%20tractable%20point-goal%20navigation%20tasks.%20Extensive%20experiments%20on%20open-vocabulary%20object%20and%20instance%20navigation%20benchmarks%20show%20that%20ImagineNav%2B%2B%20achieves%20SOTA%20performance%20in%20mapless%20settings%2C%20even%20surpassing%20most%20map-based%20methods%2C%20highlighting%20the%20importance%20of%20scene%20imagination%20and%20memory%20in%20VLM-based%20spatial%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagineNav%252B%252B%253A%2520Prompting%2520Vision-Language%2520Models%2520as%2520Embodied%2520Navigator%2520through%2520Scene%2520Imagination%26entry.906535625%3DTeng%2520Wang%2520and%2520Xinxin%2520Zhao%2520and%2520Wenzhe%2520Cai%2520and%2520Changyin%2520Sun%26entry.1292438233%3DVisual%2520navigation%2520is%2520a%2520fundamental%2520capability%2520for%2520autonomous%2520home-assistance%2520robots%252C%2520enabling%2520long-horizon%2520tasks%2520such%2520as%2520object%2520search.%2520While%2520recent%2520methods%2520have%2520leveraged%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520incorporate%2520commonsense%2520reasoning%2520and%2520improve%2520exploration%2520efficiency%252C%2520their%2520planning%2520remains%2520constrained%2520by%2520textual%2520representations%252C%2520which%2520cannot%2520adequately%2520capture%2520spatial%2520occupancy%2520or%2520scene%2520geometry--critical%2520factors%2520for%2520navigation%2520decisions.%2520We%2520explore%2520whether%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520can%2520achieve%2520mapless%2520visual%2520navigation%2520using%2520only%2520onboard%2520RGB/RGB-D%2520streams%252C%2520unlocking%2520their%2520potential%2520for%2520spatial%2520perception%2520and%2520planning.%2520We%2520achieve%2520this%2520through%2520an%2520imagination-powered%2520navigation%2520framework%252C%2520ImagineNav%252B%252B%252C%2520which%2520imagines%2520future%2520observation%2520images%2520from%2520candidate%2520robot%2520views%2520and%2520translates%2520navigation%2520planning%2520into%2520a%2520simple%2520best-view%2520image%2520selection%2520problem%2520for%2520VLMs.%2520First%252C%2520a%2520future-view%2520imagination%2520module%2520distills%2520human%2520navigation%2520preferences%2520to%2520generate%2520semantically%2520meaningful%2520viewpoints%2520with%2520high%2520exploration%2520potential.%2520These%2520imagined%2520views%2520then%2520serve%2520as%2520visual%2520prompts%2520for%2520the%2520VLM%2520to%2520identify%2520the%2520most%2520informative%2520viewpoint.%2520To%2520maintain%2520spatial%2520consistency%252C%2520we%2520develop%2520a%2520selective%2520foveation%2520memory%2520mechanism%252C%2520which%2520hierarchically%2520integrates%2520keyframe%2520observations%2520via%2520a%2520sparse-to-dense%2520framework%252C%2520constructing%2520a%2520compact%2520yet%2520comprehensive%2520memory%2520for%2520long-term%2520spatial%2520reasoning.%2520This%2520approach%2520transforms%2520goal-oriented%2520navigation%2520into%2520a%2520series%2520of%2520tractable%2520point-goal%2520navigation%2520tasks.%2520Extensive%2520experiments%2520on%2520open-vocabulary%2520object%2520and%2520instance%2520navigation%2520benchmarks%2520show%2520that%2520ImagineNav%252B%252B%2520achieves%2520SOTA%2520performance%2520in%2520mapless%2520settings%252C%2520even%2520surpassing%2520most%2520map-based%2520methods%252C%2520highlighting%2520the%2520importance%2520of%2520scene%2520imagination%2520and%2520memory%2520in%2520VLM-based%2520spatial%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImagineNav%2B%2B%3A%20Prompting%20Vision-Language%20Models%20as%20Embodied%20Navigator%20through%20Scene%20Imagination&entry.906535625=Teng%20Wang%20and%20Xinxin%20Zhao%20and%20Wenzhe%20Cai%20and%20Changyin%20Sun&entry.1292438233=Visual%20navigation%20is%20a%20fundamental%20capability%20for%20autonomous%20home-assistance%20robots%2C%20enabling%20long-horizon%20tasks%20such%20as%20object%20search.%20While%20recent%20methods%20have%20leveraged%20Large%20Language%20Models%20%28LLMs%29%20to%20incorporate%20commonsense%20reasoning%20and%20improve%20exploration%20efficiency%2C%20their%20planning%20remains%20constrained%20by%20textual%20representations%2C%20which%20cannot%20adequately%20capture%20spatial%20occupancy%20or%20scene%20geometry--critical%20factors%20for%20navigation%20decisions.%20We%20explore%20whether%20Vision-Language%20Models%20%28VLMs%29%20can%20achieve%20mapless%20visual%20navigation%20using%20only%20onboard%20RGB/RGB-D%20streams%2C%20unlocking%20their%20potential%20for%20spatial%20perception%20and%20planning.%20We%20achieve%20this%20through%20an%20imagination-powered%20navigation%20framework%2C%20ImagineNav%2B%2B%2C%20which%20imagines%20future%20observation%20images%20from%20candidate%20robot%20views%20and%20translates%20navigation%20planning%20into%20a%20simple%20best-view%20image%20selection%20problem%20for%20VLMs.%20First%2C%20a%20future-view%20imagination%20module%20distills%20human%20navigation%20preferences%20to%20generate%20semantically%20meaningful%20viewpoints%20with%20high%20exploration%20potential.%20These%20imagined%20views%20then%20serve%20as%20visual%20prompts%20for%20the%20VLM%20to%20identify%20the%20most%20informative%20viewpoint.%20To%20maintain%20spatial%20consistency%2C%20we%20develop%20a%20selective%20foveation%20memory%20mechanism%2C%20which%20hierarchically%20integrates%20keyframe%20observations%20via%20a%20sparse-to-dense%20framework%2C%20constructing%20a%20compact%20yet%20comprehensive%20memory%20for%20long-term%20spatial%20reasoning.%20This%20approach%20transforms%20goal-oriented%20navigation%20into%20a%20series%20of%20tractable%20point-goal%20navigation%20tasks.%20Extensive%20experiments%20on%20open-vocabulary%20object%20and%20instance%20navigation%20benchmarks%20show%20that%20ImagineNav%2B%2B%20achieves%20SOTA%20performance%20in%20mapless%20settings%2C%20even%20surpassing%20most%20map-based%20methods%2C%20highlighting%20the%20importance%20of%20scene%20imagination%20and%20memory%20in%20VLM-based%20spatial%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2512.17435v2&entry.124074799=Read"},
{"title": "Rethinking GNNs and Missing Features: Challenges, Evaluation and a Robust Solution", "author": "Francesco Ferrini and Veronica Lachi and Antonio Longa and Bruno Lepri and Matono Akiyoshi and Andrea Passerini and Xin Liu and Manfred Jaeger", "abstract": "Handling missing node features is a key challenge for deploying Graph Neural Networks (GNNs) in real-world domains such as healthcare and sensor networks. Existing studies mostly address relatively benign scenarios, namely benchmark datasets with (a) high-dimensional but sparse node features and (b) incomplete data generated under Missing Completely At Random (MCAR) mechanisms. For (a), we theoretically prove that high sparsity substantially limits the information loss caused by missingness, making all models appear robust and preventing a meaningful comparison of their performance. To overcome this limitation, we introduce one synthetic and three real-world datasets with dense, semantically meaningful features. For (b), we move beyond MCAR and design evaluation protocols with more realistic missingness mechanisms. Moreover, we provide a theoretical background to state explicit assumptions on the missingness process and analyze their implications for different methods. Building on this analysis, we propose GNNmim, a simple yet effective baseline for node classification with incomplete feature data. Experiments show that GNNmim is competitive with respect to specialized architectures across diverse datasets and missingness regimes.", "link": "http://arxiv.org/abs/2601.04855v1", "date": "2026-01-08", "relevancy": 2.4623, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4941}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4925}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20GNNs%20and%20Missing%20Features%3A%20Challenges%2C%20Evaluation%20and%20a%20Robust%20Solution&body=Title%3A%20Rethinking%20GNNs%20and%20Missing%20Features%3A%20Challenges%2C%20Evaluation%20and%20a%20Robust%20Solution%0AAuthor%3A%20Francesco%20Ferrini%20and%20Veronica%20Lachi%20and%20Antonio%20Longa%20and%20Bruno%20Lepri%20and%20Matono%20Akiyoshi%20and%20Andrea%20Passerini%20and%20Xin%20Liu%20and%20Manfred%20Jaeger%0AAbstract%3A%20Handling%20missing%20node%20features%20is%20a%20key%20challenge%20for%20deploying%20Graph%20Neural%20Networks%20%28GNNs%29%20in%20real-world%20domains%20such%20as%20healthcare%20and%20sensor%20networks.%20Existing%20studies%20mostly%20address%20relatively%20benign%20scenarios%2C%20namely%20benchmark%20datasets%20with%20%28a%29%20high-dimensional%20but%20sparse%20node%20features%20and%20%28b%29%20incomplete%20data%20generated%20under%20Missing%20Completely%20At%20Random%20%28MCAR%29%20mechanisms.%20For%20%28a%29%2C%20we%20theoretically%20prove%20that%20high%20sparsity%20substantially%20limits%20the%20information%20loss%20caused%20by%20missingness%2C%20making%20all%20models%20appear%20robust%20and%20preventing%20a%20meaningful%20comparison%20of%20their%20performance.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20one%20synthetic%20and%20three%20real-world%20datasets%20with%20dense%2C%20semantically%20meaningful%20features.%20For%20%28b%29%2C%20we%20move%20beyond%20MCAR%20and%20design%20evaluation%20protocols%20with%20more%20realistic%20missingness%20mechanisms.%20Moreover%2C%20we%20provide%20a%20theoretical%20background%20to%20state%20explicit%20assumptions%20on%20the%20missingness%20process%20and%20analyze%20their%20implications%20for%20different%20methods.%20Building%20on%20this%20analysis%2C%20we%20propose%20GNNmim%2C%20a%20simple%20yet%20effective%20baseline%20for%20node%20classification%20with%20incomplete%20feature%20data.%20Experiments%20show%20that%20GNNmim%20is%20competitive%20with%20respect%20to%20specialized%20architectures%20across%20diverse%20datasets%20and%20missingness%20regimes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520GNNs%2520and%2520Missing%2520Features%253A%2520Challenges%252C%2520Evaluation%2520and%2520a%2520Robust%2520Solution%26entry.906535625%3DFrancesco%2520Ferrini%2520and%2520Veronica%2520Lachi%2520and%2520Antonio%2520Longa%2520and%2520Bruno%2520Lepri%2520and%2520Matono%2520Akiyoshi%2520and%2520Andrea%2520Passerini%2520and%2520Xin%2520Liu%2520and%2520Manfred%2520Jaeger%26entry.1292438233%3DHandling%2520missing%2520node%2520features%2520is%2520a%2520key%2520challenge%2520for%2520deploying%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520in%2520real-world%2520domains%2520such%2520as%2520healthcare%2520and%2520sensor%2520networks.%2520Existing%2520studies%2520mostly%2520address%2520relatively%2520benign%2520scenarios%252C%2520namely%2520benchmark%2520datasets%2520with%2520%2528a%2529%2520high-dimensional%2520but%2520sparse%2520node%2520features%2520and%2520%2528b%2529%2520incomplete%2520data%2520generated%2520under%2520Missing%2520Completely%2520At%2520Random%2520%2528MCAR%2529%2520mechanisms.%2520For%2520%2528a%2529%252C%2520we%2520theoretically%2520prove%2520that%2520high%2520sparsity%2520substantially%2520limits%2520the%2520information%2520loss%2520caused%2520by%2520missingness%252C%2520making%2520all%2520models%2520appear%2520robust%2520and%2520preventing%2520a%2520meaningful%2520comparison%2520of%2520their%2520performance.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520one%2520synthetic%2520and%2520three%2520real-world%2520datasets%2520with%2520dense%252C%2520semantically%2520meaningful%2520features.%2520For%2520%2528b%2529%252C%2520we%2520move%2520beyond%2520MCAR%2520and%2520design%2520evaluation%2520protocols%2520with%2520more%2520realistic%2520missingness%2520mechanisms.%2520Moreover%252C%2520we%2520provide%2520a%2520theoretical%2520background%2520to%2520state%2520explicit%2520assumptions%2520on%2520the%2520missingness%2520process%2520and%2520analyze%2520their%2520implications%2520for%2520different%2520methods.%2520Building%2520on%2520this%2520analysis%252C%2520we%2520propose%2520GNNmim%252C%2520a%2520simple%2520yet%2520effective%2520baseline%2520for%2520node%2520classification%2520with%2520incomplete%2520feature%2520data.%2520Experiments%2520show%2520that%2520GNNmim%2520is%2520competitive%2520with%2520respect%2520to%2520specialized%2520architectures%2520across%2520diverse%2520datasets%2520and%2520missingness%2520regimes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20GNNs%20and%20Missing%20Features%3A%20Challenges%2C%20Evaluation%20and%20a%20Robust%20Solution&entry.906535625=Francesco%20Ferrini%20and%20Veronica%20Lachi%20and%20Antonio%20Longa%20and%20Bruno%20Lepri%20and%20Matono%20Akiyoshi%20and%20Andrea%20Passerini%20and%20Xin%20Liu%20and%20Manfred%20Jaeger&entry.1292438233=Handling%20missing%20node%20features%20is%20a%20key%20challenge%20for%20deploying%20Graph%20Neural%20Networks%20%28GNNs%29%20in%20real-world%20domains%20such%20as%20healthcare%20and%20sensor%20networks.%20Existing%20studies%20mostly%20address%20relatively%20benign%20scenarios%2C%20namely%20benchmark%20datasets%20with%20%28a%29%20high-dimensional%20but%20sparse%20node%20features%20and%20%28b%29%20incomplete%20data%20generated%20under%20Missing%20Completely%20At%20Random%20%28MCAR%29%20mechanisms.%20For%20%28a%29%2C%20we%20theoretically%20prove%20that%20high%20sparsity%20substantially%20limits%20the%20information%20loss%20caused%20by%20missingness%2C%20making%20all%20models%20appear%20robust%20and%20preventing%20a%20meaningful%20comparison%20of%20their%20performance.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20one%20synthetic%20and%20three%20real-world%20datasets%20with%20dense%2C%20semantically%20meaningful%20features.%20For%20%28b%29%2C%20we%20move%20beyond%20MCAR%20and%20design%20evaluation%20protocols%20with%20more%20realistic%20missingness%20mechanisms.%20Moreover%2C%20we%20provide%20a%20theoretical%20background%20to%20state%20explicit%20assumptions%20on%20the%20missingness%20process%20and%20analyze%20their%20implications%20for%20different%20methods.%20Building%20on%20this%20analysis%2C%20we%20propose%20GNNmim%2C%20a%20simple%20yet%20effective%20baseline%20for%20node%20classification%20with%20incomplete%20feature%20data.%20Experiments%20show%20that%20GNNmim%20is%20competitive%20with%20respect%20to%20specialized%20architectures%20across%20diverse%20datasets%20and%20missingness%20regimes.&entry.1838667208=http%3A//arxiv.org/abs/2601.04855v1&entry.124074799=Read"},
{"title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation", "author": "Boyang Wang and Haoran Zhang and Shujie Zhang and Jinkun Hao and Mingda Jia and Qi Lv and Yucheng Mao and Zhaoyang Lyu and Jia Zeng and Xudong Xu and Jiangmiao Pang", "abstract": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.", "link": "http://arxiv.org/abs/2601.05241v1", "date": "2026-01-08", "relevancy": 2.4597, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.651}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.611}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboVIP%3A%20Multi-View%20Video%20Generation%20with%20Visual%20Identity%20Prompting%20Augments%20Robot%20Manipulation&body=Title%3A%20RoboVIP%3A%20Multi-View%20Video%20Generation%20with%20Visual%20Identity%20Prompting%20Augments%20Robot%20Manipulation%0AAuthor%3A%20Boyang%20Wang%20and%20Haoran%20Zhang%20and%20Shujie%20Zhang%20and%20Jinkun%20Hao%20and%20Mingda%20Jia%20and%20Qi%20Lv%20and%20Yucheng%20Mao%20and%20Zhaoyang%20Lyu%20and%20Jia%20Zeng%20and%20Xudong%20Xu%20and%20Jiangmiao%20Pang%0AAbstract%3A%20The%20diversity%2C%20quantity%2C%20and%20quality%20of%20manipulation%20data%20are%20critical%20for%20training%20effective%20robot%20policies.%20However%2C%20due%20to%20hardware%20and%20physical%20setup%20constraints%2C%20collecting%20large-scale%20real-world%20manipulation%20data%20remains%20difficult%20to%20scale%20across%20diverse%20environments.%20Recent%20work%20uses%20text-prompt%20conditioned%20image%20diffusion%20models%20to%20augment%20manipulation%20data%20by%20altering%20the%20backgrounds%20and%20tabletop%20objects%20in%20the%20visual%20observations.%20However%2C%20these%20approaches%20often%20overlook%20the%20practical%20need%20for%20multi-view%20and%20temporally%20coherent%20observations%20required%20by%20state-of-the-art%20policy%20models.%20Further%2C%20text%20prompts%20alone%20cannot%20reliably%20specify%20the%20scene%20setup.%20To%20provide%20the%20diffusion%20model%20with%20explicit%20visual%20guidance%2C%20we%20introduce%20visual%20identity%20prompting%2C%20which%20supplies%20exemplar%20images%20as%20conditioning%20inputs%20to%20guide%20the%20generation%20of%20the%20desired%20scene%20setup.%20To%20this%20end%2C%20we%20also%20build%20a%20scalable%20pipeline%20to%20curate%20a%20visual%20identity%20pool%20from%20large%20robotics%20datasets.%20Using%20our%20augmented%20manipulation%20data%20to%20train%20downstream%20vision-language-action%20and%20visuomotor%20policy%20models%20yields%20consistent%20performance%20gains%20in%20both%20simulation%20and%20real-robot%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboVIP%253A%2520Multi-View%2520Video%2520Generation%2520with%2520Visual%2520Identity%2520Prompting%2520Augments%2520Robot%2520Manipulation%26entry.906535625%3DBoyang%2520Wang%2520and%2520Haoran%2520Zhang%2520and%2520Shujie%2520Zhang%2520and%2520Jinkun%2520Hao%2520and%2520Mingda%2520Jia%2520and%2520Qi%2520Lv%2520and%2520Yucheng%2520Mao%2520and%2520Zhaoyang%2520Lyu%2520and%2520Jia%2520Zeng%2520and%2520Xudong%2520Xu%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3DThe%2520diversity%252C%2520quantity%252C%2520and%2520quality%2520of%2520manipulation%2520data%2520are%2520critical%2520for%2520training%2520effective%2520robot%2520policies.%2520However%252C%2520due%2520to%2520hardware%2520and%2520physical%2520setup%2520constraints%252C%2520collecting%2520large-scale%2520real-world%2520manipulation%2520data%2520remains%2520difficult%2520to%2520scale%2520across%2520diverse%2520environments.%2520Recent%2520work%2520uses%2520text-prompt%2520conditioned%2520image%2520diffusion%2520models%2520to%2520augment%2520manipulation%2520data%2520by%2520altering%2520the%2520backgrounds%2520and%2520tabletop%2520objects%2520in%2520the%2520visual%2520observations.%2520However%252C%2520these%2520approaches%2520often%2520overlook%2520the%2520practical%2520need%2520for%2520multi-view%2520and%2520temporally%2520coherent%2520observations%2520required%2520by%2520state-of-the-art%2520policy%2520models.%2520Further%252C%2520text%2520prompts%2520alone%2520cannot%2520reliably%2520specify%2520the%2520scene%2520setup.%2520To%2520provide%2520the%2520diffusion%2520model%2520with%2520explicit%2520visual%2520guidance%252C%2520we%2520introduce%2520visual%2520identity%2520prompting%252C%2520which%2520supplies%2520exemplar%2520images%2520as%2520conditioning%2520inputs%2520to%2520guide%2520the%2520generation%2520of%2520the%2520desired%2520scene%2520setup.%2520To%2520this%2520end%252C%2520we%2520also%2520build%2520a%2520scalable%2520pipeline%2520to%2520curate%2520a%2520visual%2520identity%2520pool%2520from%2520large%2520robotics%2520datasets.%2520Using%2520our%2520augmented%2520manipulation%2520data%2520to%2520train%2520downstream%2520vision-language-action%2520and%2520visuomotor%2520policy%2520models%2520yields%2520consistent%2520performance%2520gains%2520in%2520both%2520simulation%2520and%2520real-robot%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboVIP%3A%20Multi-View%20Video%20Generation%20with%20Visual%20Identity%20Prompting%20Augments%20Robot%20Manipulation&entry.906535625=Boyang%20Wang%20and%20Haoran%20Zhang%20and%20Shujie%20Zhang%20and%20Jinkun%20Hao%20and%20Mingda%20Jia%20and%20Qi%20Lv%20and%20Yucheng%20Mao%20and%20Zhaoyang%20Lyu%20and%20Jia%20Zeng%20and%20Xudong%20Xu%20and%20Jiangmiao%20Pang&entry.1292438233=The%20diversity%2C%20quantity%2C%20and%20quality%20of%20manipulation%20data%20are%20critical%20for%20training%20effective%20robot%20policies.%20However%2C%20due%20to%20hardware%20and%20physical%20setup%20constraints%2C%20collecting%20large-scale%20real-world%20manipulation%20data%20remains%20difficult%20to%20scale%20across%20diverse%20environments.%20Recent%20work%20uses%20text-prompt%20conditioned%20image%20diffusion%20models%20to%20augment%20manipulation%20data%20by%20altering%20the%20backgrounds%20and%20tabletop%20objects%20in%20the%20visual%20observations.%20However%2C%20these%20approaches%20often%20overlook%20the%20practical%20need%20for%20multi-view%20and%20temporally%20coherent%20observations%20required%20by%20state-of-the-art%20policy%20models.%20Further%2C%20text%20prompts%20alone%20cannot%20reliably%20specify%20the%20scene%20setup.%20To%20provide%20the%20diffusion%20model%20with%20explicit%20visual%20guidance%2C%20we%20introduce%20visual%20identity%20prompting%2C%20which%20supplies%20exemplar%20images%20as%20conditioning%20inputs%20to%20guide%20the%20generation%20of%20the%20desired%20scene%20setup.%20To%20this%20end%2C%20we%20also%20build%20a%20scalable%20pipeline%20to%20curate%20a%20visual%20identity%20pool%20from%20large%20robotics%20datasets.%20Using%20our%20augmented%20manipulation%20data%20to%20train%20downstream%20vision-language-action%20and%20visuomotor%20policy%20models%20yields%20consistent%20performance%20gains%20in%20both%20simulation%20and%20real-robot%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.05241v1&entry.124074799=Read"},
{"title": "Token-Level LLM Collaboration via FusionRoute", "author": "Nuoya Xiong and Yuhang Zhou and Hanqing Zeng and Zhaorun Chen and Furong Huang and Shuchao Bi and Lizhu Zhang and Zhuokai Zhao", "abstract": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.", "link": "http://arxiv.org/abs/2601.05106v1", "date": "2026-01-08", "relevancy": 2.4578, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4924}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-Level%20LLM%20Collaboration%20via%20FusionRoute&body=Title%3A%20Token-Level%20LLM%20Collaboration%20via%20FusionRoute%0AAuthor%3A%20Nuoya%20Xiong%20and%20Yuhang%20Zhou%20and%20Hanqing%20Zeng%20and%20Zhaorun%20Chen%20and%20Furong%20Huang%20and%20Shuchao%20Bi%20and%20Lizhu%20Zhang%20and%20Zhuokai%20Zhao%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20exhibit%20strengths%20across%20diverse%20domains.%20However%2C%20achieving%20strong%20performance%20across%20these%20domains%20with%20a%20single%20general-purpose%20model%20typically%20requires%20scaling%20to%20sizes%20that%20are%20prohibitively%20expensive%20to%20train%20and%20deploy.%20On%20the%20other%20hand%2C%20while%20smaller%20domain-specialized%20models%20are%20much%20more%20efficient%2C%20they%20struggle%20to%20generalize%20beyond%20their%20training%20distributions.%20To%20address%20this%20dilemma%2C%20we%20propose%20FusionRoute%2C%20a%20robust%20and%20effective%20token-level%20multi-LLM%20collaboration%20framework%20in%20which%20a%20lightweight%20router%20simultaneously%20%28i%29%20selects%20the%20most%20suitable%20expert%20at%20each%20decoding%20step%20and%20%28ii%29%20contributes%20a%20complementary%20logit%20that%20refines%20or%20corrects%20the%20selected%20expert%27s%20next-token%20distribution%20via%20logit%20addition.%20Unlike%20existing%20token-level%20collaboration%20methods%20that%20rely%20solely%20on%20fixed%20expert%20outputs%2C%20we%20provide%20a%20theoretical%20analysis%20showing%20that%20pure%20expert-only%20routing%20is%20fundamentally%20limited%3A%20unless%20strong%20global%20coverage%20assumptions%20hold%2C%20it%20cannot%20in%20general%20realize%20the%20optimal%20decoding%20policy.%20By%20augmenting%20expert%20selection%20with%20a%20trainable%20complementary%20generator%2C%20FusionRoute%20expands%20the%20effective%20policy%20class%20and%20enables%20recovery%20of%20optimal%20value%20functions%20under%20mild%20conditions.%20Empirically%2C%20across%20both%20Llama-3%20and%20Gemma-2%20families%20and%20diverse%20benchmarks%20spanning%20mathematical%20reasoning%2C%20code%20generation%2C%20and%20instruction%20following%2C%20FusionRoute%20outperforms%20both%20sequence-%20and%20token-level%20collaboration%2C%20model%20merging%2C%20and%20direct%20fine-tuning%2C%20while%20remaining%20competitive%20with%20domain%20experts%20on%20their%20respective%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-Level%2520LLM%2520Collaboration%2520via%2520FusionRoute%26entry.906535625%3DNuoya%2520Xiong%2520and%2520Yuhang%2520Zhou%2520and%2520Hanqing%2520Zeng%2520and%2520Zhaorun%2520Chen%2520and%2520Furong%2520Huang%2520and%2520Shuchao%2520Bi%2520and%2520Lizhu%2520Zhang%2520and%2520Zhuokai%2520Zhao%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520strengths%2520across%2520diverse%2520domains.%2520However%252C%2520achieving%2520strong%2520performance%2520across%2520these%2520domains%2520with%2520a%2520single%2520general-purpose%2520model%2520typically%2520requires%2520scaling%2520to%2520sizes%2520that%2520are%2520prohibitively%2520expensive%2520to%2520train%2520and%2520deploy.%2520On%2520the%2520other%2520hand%252C%2520while%2520smaller%2520domain-specialized%2520models%2520are%2520much%2520more%2520efficient%252C%2520they%2520struggle%2520to%2520generalize%2520beyond%2520their%2520training%2520distributions.%2520To%2520address%2520this%2520dilemma%252C%2520we%2520propose%2520FusionRoute%252C%2520a%2520robust%2520and%2520effective%2520token-level%2520multi-LLM%2520collaboration%2520framework%2520in%2520which%2520a%2520lightweight%2520router%2520simultaneously%2520%2528i%2529%2520selects%2520the%2520most%2520suitable%2520expert%2520at%2520each%2520decoding%2520step%2520and%2520%2528ii%2529%2520contributes%2520a%2520complementary%2520logit%2520that%2520refines%2520or%2520corrects%2520the%2520selected%2520expert%2527s%2520next-token%2520distribution%2520via%2520logit%2520addition.%2520Unlike%2520existing%2520token-level%2520collaboration%2520methods%2520that%2520rely%2520solely%2520on%2520fixed%2520expert%2520outputs%252C%2520we%2520provide%2520a%2520theoretical%2520analysis%2520showing%2520that%2520pure%2520expert-only%2520routing%2520is%2520fundamentally%2520limited%253A%2520unless%2520strong%2520global%2520coverage%2520assumptions%2520hold%252C%2520it%2520cannot%2520in%2520general%2520realize%2520the%2520optimal%2520decoding%2520policy.%2520By%2520augmenting%2520expert%2520selection%2520with%2520a%2520trainable%2520complementary%2520generator%252C%2520FusionRoute%2520expands%2520the%2520effective%2520policy%2520class%2520and%2520enables%2520recovery%2520of%2520optimal%2520value%2520functions%2520under%2520mild%2520conditions.%2520Empirically%252C%2520across%2520both%2520Llama-3%2520and%2520Gemma-2%2520families%2520and%2520diverse%2520benchmarks%2520spanning%2520mathematical%2520reasoning%252C%2520code%2520generation%252C%2520and%2520instruction%2520following%252C%2520FusionRoute%2520outperforms%2520both%2520sequence-%2520and%2520token-level%2520collaboration%252C%2520model%2520merging%252C%2520and%2520direct%2520fine-tuning%252C%2520while%2520remaining%2520competitive%2520with%2520domain%2520experts%2520on%2520their%2520respective%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-Level%20LLM%20Collaboration%20via%20FusionRoute&entry.906535625=Nuoya%20Xiong%20and%20Yuhang%20Zhou%20and%20Hanqing%20Zeng%20and%20Zhaorun%20Chen%20and%20Furong%20Huang%20and%20Shuchao%20Bi%20and%20Lizhu%20Zhang%20and%20Zhuokai%20Zhao&entry.1292438233=Large%20language%20models%20%28LLMs%29%20exhibit%20strengths%20across%20diverse%20domains.%20However%2C%20achieving%20strong%20performance%20across%20these%20domains%20with%20a%20single%20general-purpose%20model%20typically%20requires%20scaling%20to%20sizes%20that%20are%20prohibitively%20expensive%20to%20train%20and%20deploy.%20On%20the%20other%20hand%2C%20while%20smaller%20domain-specialized%20models%20are%20much%20more%20efficient%2C%20they%20struggle%20to%20generalize%20beyond%20their%20training%20distributions.%20To%20address%20this%20dilemma%2C%20we%20propose%20FusionRoute%2C%20a%20robust%20and%20effective%20token-level%20multi-LLM%20collaboration%20framework%20in%20which%20a%20lightweight%20router%20simultaneously%20%28i%29%20selects%20the%20most%20suitable%20expert%20at%20each%20decoding%20step%20and%20%28ii%29%20contributes%20a%20complementary%20logit%20that%20refines%20or%20corrects%20the%20selected%20expert%27s%20next-token%20distribution%20via%20logit%20addition.%20Unlike%20existing%20token-level%20collaboration%20methods%20that%20rely%20solely%20on%20fixed%20expert%20outputs%2C%20we%20provide%20a%20theoretical%20analysis%20showing%20that%20pure%20expert-only%20routing%20is%20fundamentally%20limited%3A%20unless%20strong%20global%20coverage%20assumptions%20hold%2C%20it%20cannot%20in%20general%20realize%20the%20optimal%20decoding%20policy.%20By%20augmenting%20expert%20selection%20with%20a%20trainable%20complementary%20generator%2C%20FusionRoute%20expands%20the%20effective%20policy%20class%20and%20enables%20recovery%20of%20optimal%20value%20functions%20under%20mild%20conditions.%20Empirically%2C%20across%20both%20Llama-3%20and%20Gemma-2%20families%20and%20diverse%20benchmarks%20spanning%20mathematical%20reasoning%2C%20code%20generation%2C%20and%20instruction%20following%2C%20FusionRoute%20outperforms%20both%20sequence-%20and%20token-level%20collaboration%2C%20model%20merging%2C%20and%20direct%20fine-tuning%2C%20while%20remaining%20competitive%20with%20domain%20experts%20on%20their%20respective%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.05106v1&entry.124074799=Read"},
{"title": "UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition", "author": "Filippo Ghilotti and Samuel Brucker and Nahku Saidy and Matteo Matteucci and Mario Bijelic and Felix Heide", "abstract": "Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.", "link": "http://arxiv.org/abs/2601.05105v1", "date": "2026-01-08", "relevancy": 2.4455, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6377}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6229}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniLiPs%3A%20Unified%20LiDAR%20Pseudo-Labeling%20with%20Geometry-Grounded%20Dynamic%20Scene%20Decomposition&body=Title%3A%20UniLiPs%3A%20Unified%20LiDAR%20Pseudo-Labeling%20with%20Geometry-Grounded%20Dynamic%20Scene%20Decomposition%0AAuthor%3A%20Filippo%20Ghilotti%20and%20Samuel%20Brucker%20and%20Nahku%20Saidy%20and%20Matteo%20Matteucci%20and%20Mario%20Bijelic%20and%20Felix%20Heide%0AAbstract%3A%20Unlabeled%20LiDAR%20logs%2C%20in%20autonomous%20driving%20applications%2C%20are%20inherently%20a%20gold%20mine%20of%20dense%203D%20geometry%20hiding%20in%20plain%20sight%20-%20yet%20they%20are%20almost%20useless%20without%20human%20labels%2C%20highlighting%20a%20dominant%20cost%20barrier%20for%20autonomous-perception%20research.%20In%20this%20work%20we%20tackle%20this%20bottleneck%20by%20leveraging%20temporal-geometric%20consistency%20across%20LiDAR%20sweeps%20to%20lift%20and%20fuse%20cues%20from%20text%20and%202D%20vision%20foundation%20models%20directly%20into%203D%2C%20without%20any%20manual%20input.%20We%20introduce%20an%20unsupervised%20multi-modal%20pseudo-labeling%20method%20relying%20on%20strong%20geometric%20priors%20learned%20from%20temporally%20accumulated%20LiDAR%20maps%2C%20alongside%20with%20a%20novel%20iterative%20update%20rule%20that%20enforces%20joint%20geometric-semantic%20consistency%2C%20and%20vice-versa%20detecting%20moving%20objects%20from%20inconsistencies.%20Our%20method%20simultaneously%20produces%203D%20semantic%20labels%2C%203D%20bounding%20boxes%2C%20and%20dense%20LiDAR%20scans%2C%20demonstrating%20robust%20generalization%20across%20three%20datasets.%20We%20experimentally%20validate%20that%20our%20method%20compares%20favorably%20to%20existing%20semantic%20segmentation%20and%20object%20detection%20pseudo-labeling%20methods%2C%20which%20often%20require%20additional%20manual%20supervision.%20We%20confirm%20that%20even%20a%20small%20fraction%20of%20our%20geometrically%20consistent%2C%20densified%20LiDAR%20improves%20depth%20prediction%20by%2051.5%25%20and%2022.0%25%20MAE%20in%20the%2080-150%20and%20150-250%20meters%20range%2C%20respectively.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniLiPs%253A%2520Unified%2520LiDAR%2520Pseudo-Labeling%2520with%2520Geometry-Grounded%2520Dynamic%2520Scene%2520Decomposition%26entry.906535625%3DFilippo%2520Ghilotti%2520and%2520Samuel%2520Brucker%2520and%2520Nahku%2520Saidy%2520and%2520Matteo%2520Matteucci%2520and%2520Mario%2520Bijelic%2520and%2520Felix%2520Heide%26entry.1292438233%3DUnlabeled%2520LiDAR%2520logs%252C%2520in%2520autonomous%2520driving%2520applications%252C%2520are%2520inherently%2520a%2520gold%2520mine%2520of%2520dense%25203D%2520geometry%2520hiding%2520in%2520plain%2520sight%2520-%2520yet%2520they%2520are%2520almost%2520useless%2520without%2520human%2520labels%252C%2520highlighting%2520a%2520dominant%2520cost%2520barrier%2520for%2520autonomous-perception%2520research.%2520In%2520this%2520work%2520we%2520tackle%2520this%2520bottleneck%2520by%2520leveraging%2520temporal-geometric%2520consistency%2520across%2520LiDAR%2520sweeps%2520to%2520lift%2520and%2520fuse%2520cues%2520from%2520text%2520and%25202D%2520vision%2520foundation%2520models%2520directly%2520into%25203D%252C%2520without%2520any%2520manual%2520input.%2520We%2520introduce%2520an%2520unsupervised%2520multi-modal%2520pseudo-labeling%2520method%2520relying%2520on%2520strong%2520geometric%2520priors%2520learned%2520from%2520temporally%2520accumulated%2520LiDAR%2520maps%252C%2520alongside%2520with%2520a%2520novel%2520iterative%2520update%2520rule%2520that%2520enforces%2520joint%2520geometric-semantic%2520consistency%252C%2520and%2520vice-versa%2520detecting%2520moving%2520objects%2520from%2520inconsistencies.%2520Our%2520method%2520simultaneously%2520produces%25203D%2520semantic%2520labels%252C%25203D%2520bounding%2520boxes%252C%2520and%2520dense%2520LiDAR%2520scans%252C%2520demonstrating%2520robust%2520generalization%2520across%2520three%2520datasets.%2520We%2520experimentally%2520validate%2520that%2520our%2520method%2520compares%2520favorably%2520to%2520existing%2520semantic%2520segmentation%2520and%2520object%2520detection%2520pseudo-labeling%2520methods%252C%2520which%2520often%2520require%2520additional%2520manual%2520supervision.%2520We%2520confirm%2520that%2520even%2520a%2520small%2520fraction%2520of%2520our%2520geometrically%2520consistent%252C%2520densified%2520LiDAR%2520improves%2520depth%2520prediction%2520by%252051.5%2525%2520and%252022.0%2525%2520MAE%2520in%2520the%252080-150%2520and%2520150-250%2520meters%2520range%252C%2520respectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniLiPs%3A%20Unified%20LiDAR%20Pseudo-Labeling%20with%20Geometry-Grounded%20Dynamic%20Scene%20Decomposition&entry.906535625=Filippo%20Ghilotti%20and%20Samuel%20Brucker%20and%20Nahku%20Saidy%20and%20Matteo%20Matteucci%20and%20Mario%20Bijelic%20and%20Felix%20Heide&entry.1292438233=Unlabeled%20LiDAR%20logs%2C%20in%20autonomous%20driving%20applications%2C%20are%20inherently%20a%20gold%20mine%20of%20dense%203D%20geometry%20hiding%20in%20plain%20sight%20-%20yet%20they%20are%20almost%20useless%20without%20human%20labels%2C%20highlighting%20a%20dominant%20cost%20barrier%20for%20autonomous-perception%20research.%20In%20this%20work%20we%20tackle%20this%20bottleneck%20by%20leveraging%20temporal-geometric%20consistency%20across%20LiDAR%20sweeps%20to%20lift%20and%20fuse%20cues%20from%20text%20and%202D%20vision%20foundation%20models%20directly%20into%203D%2C%20without%20any%20manual%20input.%20We%20introduce%20an%20unsupervised%20multi-modal%20pseudo-labeling%20method%20relying%20on%20strong%20geometric%20priors%20learned%20from%20temporally%20accumulated%20LiDAR%20maps%2C%20alongside%20with%20a%20novel%20iterative%20update%20rule%20that%20enforces%20joint%20geometric-semantic%20consistency%2C%20and%20vice-versa%20detecting%20moving%20objects%20from%20inconsistencies.%20Our%20method%20simultaneously%20produces%203D%20semantic%20labels%2C%203D%20bounding%20boxes%2C%20and%20dense%20LiDAR%20scans%2C%20demonstrating%20robust%20generalization%20across%20three%20datasets.%20We%20experimentally%20validate%20that%20our%20method%20compares%20favorably%20to%20existing%20semantic%20segmentation%20and%20object%20detection%20pseudo-labeling%20methods%2C%20which%20often%20require%20additional%20manual%20supervision.%20We%20confirm%20that%20even%20a%20small%20fraction%20of%20our%20geometrically%20consistent%2C%20densified%20LiDAR%20improves%20depth%20prediction%20by%2051.5%25%20and%2022.0%25%20MAE%20in%20the%2080-150%20and%20150-250%20meters%20range%2C%20respectively.&entry.1838667208=http%3A//arxiv.org/abs/2601.05105v1&entry.124074799=Read"},
{"title": "CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models", "author": "Tobia Poppi and Burak Uzkent and Amanmeet Garg and Lucas Porto and Garin Kessler and Yezhou Yang and Marcella Cornia and Lorenzo Baraldi and Rita Cucchiara and Florian Schiffers", "abstract": "Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.", "link": "http://arxiv.org/abs/2601.04778v1", "date": "2026-01-08", "relevancy": 2.4443, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6335}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5954}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CounterVid%3A%20Counterfactual%20Video%20Generation%20for%20Mitigating%20Action%20and%20Temporal%20Hallucinations%20in%20Video-Language%20Models&body=Title%3A%20CounterVid%3A%20Counterfactual%20Video%20Generation%20for%20Mitigating%20Action%20and%20Temporal%20Hallucinations%20in%20Video-Language%20Models%0AAuthor%3A%20Tobia%20Poppi%20and%20Burak%20Uzkent%20and%20Amanmeet%20Garg%20and%20Lucas%20Porto%20and%20Garin%20Kessler%20and%20Yezhou%20Yang%20and%20Marcella%20Cornia%20and%20Lorenzo%20Baraldi%20and%20Rita%20Cucchiara%20and%20Florian%20Schiffers%0AAbstract%3A%20Video-language%20models%20%28VLMs%29%20achieve%20strong%20multimodal%20understanding%20but%20remain%20prone%20to%20hallucinations%2C%20especially%20when%20reasoning%20about%20actions%20and%20temporal%20order.%20Existing%20mitigation%20strategies%2C%20such%20as%20textual%20filtering%20or%20random%20video%20perturbations%2C%20often%20fail%20to%20address%20the%20root%20cause%3A%20over-reliance%20on%20language%20priors%20rather%20than%20fine-grained%20visual%20dynamics.%20We%20propose%20a%20scalable%20framework%20for%20counterfactual%20video%20generation%20that%20synthesizes%20videos%20differing%20only%20in%20actions%20or%20temporal%20structure%20while%20preserving%20scene%20context.%20Our%20pipeline%20combines%20multimodal%20LLMs%20for%20action%20proposal%20and%20editing%20guidance%20with%20diffusion-based%20image%20and%20video%20models%20to%20generate%20semantic%20hard%20negatives%20at%20scale.%20Using%20this%20framework%2C%20we%20build%20CounterVid%2C%20a%20synthetic%20dataset%20of%20~26k%20preference%20pairs%20targeting%20action%20recognition%20and%20temporal%20reasoning.%20We%20further%20introduce%20MixDPO%2C%20a%20unified%20Direct%20Preference%20Optimization%20approach%20that%20jointly%20leverages%20textual%20and%20visual%20preferences.%20Fine-tuning%20Qwen2.5-VL%20with%20MixDPO%20yields%20consistent%20improvements%2C%20notably%20in%20temporal%20ordering%2C%20and%20transfers%20effectively%20to%20standard%20video%20hallucination%20benchmarks.%20Code%20and%20models%20will%20be%20made%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterVid%253A%2520Counterfactual%2520Video%2520Generation%2520for%2520Mitigating%2520Action%2520and%2520Temporal%2520Hallucinations%2520in%2520Video-Language%2520Models%26entry.906535625%3DTobia%2520Poppi%2520and%2520Burak%2520Uzkent%2520and%2520Amanmeet%2520Garg%2520and%2520Lucas%2520Porto%2520and%2520Garin%2520Kessler%2520and%2520Yezhou%2520Yang%2520and%2520Marcella%2520Cornia%2520and%2520Lorenzo%2520Baraldi%2520and%2520Rita%2520Cucchiara%2520and%2520Florian%2520Schiffers%26entry.1292438233%3DVideo-language%2520models%2520%2528VLMs%2529%2520achieve%2520strong%2520multimodal%2520understanding%2520but%2520remain%2520prone%2520to%2520hallucinations%252C%2520especially%2520when%2520reasoning%2520about%2520actions%2520and%2520temporal%2520order.%2520Existing%2520mitigation%2520strategies%252C%2520such%2520as%2520textual%2520filtering%2520or%2520random%2520video%2520perturbations%252C%2520often%2520fail%2520to%2520address%2520the%2520root%2520cause%253A%2520over-reliance%2520on%2520language%2520priors%2520rather%2520than%2520fine-grained%2520visual%2520dynamics.%2520We%2520propose%2520a%2520scalable%2520framework%2520for%2520counterfactual%2520video%2520generation%2520that%2520synthesizes%2520videos%2520differing%2520only%2520in%2520actions%2520or%2520temporal%2520structure%2520while%2520preserving%2520scene%2520context.%2520Our%2520pipeline%2520combines%2520multimodal%2520LLMs%2520for%2520action%2520proposal%2520and%2520editing%2520guidance%2520with%2520diffusion-based%2520image%2520and%2520video%2520models%2520to%2520generate%2520semantic%2520hard%2520negatives%2520at%2520scale.%2520Using%2520this%2520framework%252C%2520we%2520build%2520CounterVid%252C%2520a%2520synthetic%2520dataset%2520of%2520~26k%2520preference%2520pairs%2520targeting%2520action%2520recognition%2520and%2520temporal%2520reasoning.%2520We%2520further%2520introduce%2520MixDPO%252C%2520a%2520unified%2520Direct%2520Preference%2520Optimization%2520approach%2520that%2520jointly%2520leverages%2520textual%2520and%2520visual%2520preferences.%2520Fine-tuning%2520Qwen2.5-VL%2520with%2520MixDPO%2520yields%2520consistent%2520improvements%252C%2520notably%2520in%2520temporal%2520ordering%252C%2520and%2520transfers%2520effectively%2520to%2520standard%2520video%2520hallucination%2520benchmarks.%2520Code%2520and%2520models%2520will%2520be%2520made%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CounterVid%3A%20Counterfactual%20Video%20Generation%20for%20Mitigating%20Action%20and%20Temporal%20Hallucinations%20in%20Video-Language%20Models&entry.906535625=Tobia%20Poppi%20and%20Burak%20Uzkent%20and%20Amanmeet%20Garg%20and%20Lucas%20Porto%20and%20Garin%20Kessler%20and%20Yezhou%20Yang%20and%20Marcella%20Cornia%20and%20Lorenzo%20Baraldi%20and%20Rita%20Cucchiara%20and%20Florian%20Schiffers&entry.1292438233=Video-language%20models%20%28VLMs%29%20achieve%20strong%20multimodal%20understanding%20but%20remain%20prone%20to%20hallucinations%2C%20especially%20when%20reasoning%20about%20actions%20and%20temporal%20order.%20Existing%20mitigation%20strategies%2C%20such%20as%20textual%20filtering%20or%20random%20video%20perturbations%2C%20often%20fail%20to%20address%20the%20root%20cause%3A%20over-reliance%20on%20language%20priors%20rather%20than%20fine-grained%20visual%20dynamics.%20We%20propose%20a%20scalable%20framework%20for%20counterfactual%20video%20generation%20that%20synthesizes%20videos%20differing%20only%20in%20actions%20or%20temporal%20structure%20while%20preserving%20scene%20context.%20Our%20pipeline%20combines%20multimodal%20LLMs%20for%20action%20proposal%20and%20editing%20guidance%20with%20diffusion-based%20image%20and%20video%20models%20to%20generate%20semantic%20hard%20negatives%20at%20scale.%20Using%20this%20framework%2C%20we%20build%20CounterVid%2C%20a%20synthetic%20dataset%20of%20~26k%20preference%20pairs%20targeting%20action%20recognition%20and%20temporal%20reasoning.%20We%20further%20introduce%20MixDPO%2C%20a%20unified%20Direct%20Preference%20Optimization%20approach%20that%20jointly%20leverages%20textual%20and%20visual%20preferences.%20Fine-tuning%20Qwen2.5-VL%20with%20MixDPO%20yields%20consistent%20improvements%2C%20notably%20in%20temporal%20ordering%2C%20and%20transfers%20effectively%20to%20standard%20video%20hallucination%20benchmarks.%20Code%20and%20models%20will%20be%20made%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.04778v1&entry.124074799=Read"},
{"title": "SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models", "author": "Oriol Rabasseda and Zenjie Li and Kamal Nasrollahi and Sergio Escalera", "abstract": "Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.\n  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.", "link": "http://arxiv.org/abs/2601.04824v1", "date": "2026-01-08", "relevancy": 2.4139, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.608}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOVABench%3A%20A%20Vehicle%20Surveillance%20Action%20Retrieval%20Benchmark%20for%20Multimodal%20Large%20Language%20Models&body=Title%3A%20SOVABench%3A%20A%20Vehicle%20Surveillance%20Action%20Retrieval%20Benchmark%20for%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Oriol%20Rabasseda%20and%20Zenjie%20Li%20and%20Kamal%20Nasrollahi%20and%20Sergio%20Escalera%0AAbstract%3A%20Automatic%20identification%20of%20events%20and%20recurrent%20behavior%20analysis%20are%20critical%20for%20video%20surveillance.%20However%2C%20most%20existing%20content-based%20video%20retrieval%20benchmarks%20focus%20on%20scene-level%20similarity%20and%20do%20not%20evaluate%20the%20action%20discrimination%20required%20in%20surveillance.%20To%20address%20this%20gap%2C%20we%20introduce%20SOVABench%20%28Surveillance%20Opposite%20Vehicle%20Actions%20Benchmark%29%2C%20a%20real-world%20retrieval%20benchmark%20built%20from%20surveillance%20footage%20and%20centered%20on%20vehicle-related%20actions.%20SOVABench%20defines%20two%20evaluation%20protocols%20%28inter-pair%20and%20intra-pair%29%20to%20assess%20cross-action%20discrimination%20and%20temporal%20direction%20understanding.%20Although%20action%20distinctions%20are%20generally%20intuitive%20for%20human%20observers%2C%20our%20experiments%20show%20that%20they%20remain%20challenging%20for%20state-of-the-art%20vision%20and%20multimodal%20models.%0A%20%20Leveraging%20the%20visual%20reasoning%20and%20instruction-following%20capabilities%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20we%20present%20a%20training-free%20framework%20for%20producing%20interpretable%20embeddings%20from%20MLLM-generated%20descriptions%20for%20both%20images%20and%20videos.%20The%20framework%20achieves%20strong%20performance%20on%20SOVABench%20as%20well%20as%20on%20several%20spatial%20and%20counting%20benchmarks%20where%20contrastive%20Vision-Language%20Models%20often%20fail.%20The%20code%2C%20annotations%2C%20and%20instructions%20to%20construct%20the%20benchmark%20are%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOVABench%253A%2520A%2520Vehicle%2520Surveillance%2520Action%2520Retrieval%2520Benchmark%2520for%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DOriol%2520Rabasseda%2520and%2520Zenjie%2520Li%2520and%2520Kamal%2520Nasrollahi%2520and%2520Sergio%2520Escalera%26entry.1292438233%3DAutomatic%2520identification%2520of%2520events%2520and%2520recurrent%2520behavior%2520analysis%2520are%2520critical%2520for%2520video%2520surveillance.%2520However%252C%2520most%2520existing%2520content-based%2520video%2520retrieval%2520benchmarks%2520focus%2520on%2520scene-level%2520similarity%2520and%2520do%2520not%2520evaluate%2520the%2520action%2520discrimination%2520required%2520in%2520surveillance.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520SOVABench%2520%2528Surveillance%2520Opposite%2520Vehicle%2520Actions%2520Benchmark%2529%252C%2520a%2520real-world%2520retrieval%2520benchmark%2520built%2520from%2520surveillance%2520footage%2520and%2520centered%2520on%2520vehicle-related%2520actions.%2520SOVABench%2520defines%2520two%2520evaluation%2520protocols%2520%2528inter-pair%2520and%2520intra-pair%2529%2520to%2520assess%2520cross-action%2520discrimination%2520and%2520temporal%2520direction%2520understanding.%2520Although%2520action%2520distinctions%2520are%2520generally%2520intuitive%2520for%2520human%2520observers%252C%2520our%2520experiments%2520show%2520that%2520they%2520remain%2520challenging%2520for%2520state-of-the-art%2520vision%2520and%2520multimodal%2520models.%250A%2520%2520Leveraging%2520the%2520visual%2520reasoning%2520and%2520instruction-following%2520capabilities%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520we%2520present%2520a%2520training-free%2520framework%2520for%2520producing%2520interpretable%2520embeddings%2520from%2520MLLM-generated%2520descriptions%2520for%2520both%2520images%2520and%2520videos.%2520The%2520framework%2520achieves%2520strong%2520performance%2520on%2520SOVABench%2520as%2520well%2520as%2520on%2520several%2520spatial%2520and%2520counting%2520benchmarks%2520where%2520contrastive%2520Vision-Language%2520Models%2520often%2520fail.%2520The%2520code%252C%2520annotations%252C%2520and%2520instructions%2520to%2520construct%2520the%2520benchmark%2520are%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOVABench%3A%20A%20Vehicle%20Surveillance%20Action%20Retrieval%20Benchmark%20for%20Multimodal%20Large%20Language%20Models&entry.906535625=Oriol%20Rabasseda%20and%20Zenjie%20Li%20and%20Kamal%20Nasrollahi%20and%20Sergio%20Escalera&entry.1292438233=Automatic%20identification%20of%20events%20and%20recurrent%20behavior%20analysis%20are%20critical%20for%20video%20surveillance.%20However%2C%20most%20existing%20content-based%20video%20retrieval%20benchmarks%20focus%20on%20scene-level%20similarity%20and%20do%20not%20evaluate%20the%20action%20discrimination%20required%20in%20surveillance.%20To%20address%20this%20gap%2C%20we%20introduce%20SOVABench%20%28Surveillance%20Opposite%20Vehicle%20Actions%20Benchmark%29%2C%20a%20real-world%20retrieval%20benchmark%20built%20from%20surveillance%20footage%20and%20centered%20on%20vehicle-related%20actions.%20SOVABench%20defines%20two%20evaluation%20protocols%20%28inter-pair%20and%20intra-pair%29%20to%20assess%20cross-action%20discrimination%20and%20temporal%20direction%20understanding.%20Although%20action%20distinctions%20are%20generally%20intuitive%20for%20human%20observers%2C%20our%20experiments%20show%20that%20they%20remain%20challenging%20for%20state-of-the-art%20vision%20and%20multimodal%20models.%0A%20%20Leveraging%20the%20visual%20reasoning%20and%20instruction-following%20capabilities%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20we%20present%20a%20training-free%20framework%20for%20producing%20interpretable%20embeddings%20from%20MLLM-generated%20descriptions%20for%20both%20images%20and%20videos.%20The%20framework%20achieves%20strong%20performance%20on%20SOVABench%20as%20well%20as%20on%20several%20spatial%20and%20counting%20benchmarks%20where%20contrastive%20Vision-Language%20Models%20often%20fail.%20The%20code%2C%20annotations%2C%20and%20instructions%20to%20construct%20the%20benchmark%20are%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.04824v1&entry.124074799=Read"},
{"title": "Full segmentation annotations of 3D time-lapse microscopy images of MDA231 cells", "author": "Aleksandra Melnikova and Petr Matula", "abstract": "High-quality, publicly available segmentation annotations of image and video datasets are critical for advancing the field of image processing. In particular, annotations of volumetric images of a large number of targets are time-consuming and challenging. In (Melnikova, A., & Matula, P., 2025), we presented the first publicly available full 3D time-lapse segmentation annotations of migrating cells with complex dynamic shapes. Concretely, three distinct humans annotated two sequences of MDA231 human breast carcinoma cells (Fluo-C3DL-MDA231) from the Cell Tracking Challenge (CTC).\n  This paper aims to provide a comprehensive description of the dataset and accompanying experiments that were not included in (Melnikova, A., & Matula, P., 2025) due to limitations in publication space. Namely, we show that the created annotations are consistent with the previously published tracking markers provided by the CTC organizers and the segmentation accuracy measured based on the 2D gold truth of CTC is within the inter-annotator variability margins. We compared the created 3D annotations with automatically created silver truth provided by CTC. We have found the proposed annotations better represent the complexity of the input images. The presented annotations can be used for testing and training cell segmentation, or analyzing 3D shapes of highly dynamic objects.", "link": "http://arxiv.org/abs/2510.10797v2", "date": "2026-01-08", "relevancy": 2.3975, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6011}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6011}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Full%20segmentation%20annotations%20of%203D%20time-lapse%20microscopy%20images%20of%20MDA231%20cells&body=Title%3A%20Full%20segmentation%20annotations%20of%203D%20time-lapse%20microscopy%20images%20of%20MDA231%20cells%0AAuthor%3A%20Aleksandra%20Melnikova%20and%20Petr%20Matula%0AAbstract%3A%20High-quality%2C%20publicly%20available%20segmentation%20annotations%20of%20image%20and%20video%20datasets%20are%20critical%20for%20advancing%20the%20field%20of%20image%20processing.%20In%20particular%2C%20annotations%20of%20volumetric%20images%20of%20a%20large%20number%20of%20targets%20are%20time-consuming%20and%20challenging.%20In%20%28Melnikova%2C%20A.%2C%20%26%20Matula%2C%20P.%2C%202025%29%2C%20we%20presented%20the%20first%20publicly%20available%20full%203D%20time-lapse%20segmentation%20annotations%20of%20migrating%20cells%20with%20complex%20dynamic%20shapes.%20Concretely%2C%20three%20distinct%20humans%20annotated%20two%20sequences%20of%20MDA231%20human%20breast%20carcinoma%20cells%20%28Fluo-C3DL-MDA231%29%20from%20the%20Cell%20Tracking%20Challenge%20%28CTC%29.%0A%20%20This%20paper%20aims%20to%20provide%20a%20comprehensive%20description%20of%20the%20dataset%20and%20accompanying%20experiments%20that%20were%20not%20included%20in%20%28Melnikova%2C%20A.%2C%20%26%20Matula%2C%20P.%2C%202025%29%20due%20to%20limitations%20in%20publication%20space.%20Namely%2C%20we%20show%20that%20the%20created%20annotations%20are%20consistent%20with%20the%20previously%20published%20tracking%20markers%20provided%20by%20the%20CTC%20organizers%20and%20the%20segmentation%20accuracy%20measured%20based%20on%20the%202D%20gold%20truth%20of%20CTC%20is%20within%20the%20inter-annotator%20variability%20margins.%20We%20compared%20the%20created%203D%20annotations%20with%20automatically%20created%20silver%20truth%20provided%20by%20CTC.%20We%20have%20found%20the%20proposed%20annotations%20better%20represent%20the%20complexity%20of%20the%20input%20images.%20The%20presented%20annotations%20can%20be%20used%20for%20testing%20and%20training%20cell%20segmentation%2C%20or%20analyzing%203D%20shapes%20of%20highly%20dynamic%20objects.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10797v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFull%2520segmentation%2520annotations%2520of%25203D%2520time-lapse%2520microscopy%2520images%2520of%2520MDA231%2520cells%26entry.906535625%3DAleksandra%2520Melnikova%2520and%2520Petr%2520Matula%26entry.1292438233%3DHigh-quality%252C%2520publicly%2520available%2520segmentation%2520annotations%2520of%2520image%2520and%2520video%2520datasets%2520are%2520critical%2520for%2520advancing%2520the%2520field%2520of%2520image%2520processing.%2520In%2520particular%252C%2520annotations%2520of%2520volumetric%2520images%2520of%2520a%2520large%2520number%2520of%2520targets%2520are%2520time-consuming%2520and%2520challenging.%2520In%2520%2528Melnikova%252C%2520A.%252C%2520%2526%2520Matula%252C%2520P.%252C%25202025%2529%252C%2520we%2520presented%2520the%2520first%2520publicly%2520available%2520full%25203D%2520time-lapse%2520segmentation%2520annotations%2520of%2520migrating%2520cells%2520with%2520complex%2520dynamic%2520shapes.%2520Concretely%252C%2520three%2520distinct%2520humans%2520annotated%2520two%2520sequences%2520of%2520MDA231%2520human%2520breast%2520carcinoma%2520cells%2520%2528Fluo-C3DL-MDA231%2529%2520from%2520the%2520Cell%2520Tracking%2520Challenge%2520%2528CTC%2529.%250A%2520%2520This%2520paper%2520aims%2520to%2520provide%2520a%2520comprehensive%2520description%2520of%2520the%2520dataset%2520and%2520accompanying%2520experiments%2520that%2520were%2520not%2520included%2520in%2520%2528Melnikova%252C%2520A.%252C%2520%2526%2520Matula%252C%2520P.%252C%25202025%2529%2520due%2520to%2520limitations%2520in%2520publication%2520space.%2520Namely%252C%2520we%2520show%2520that%2520the%2520created%2520annotations%2520are%2520consistent%2520with%2520the%2520previously%2520published%2520tracking%2520markers%2520provided%2520by%2520the%2520CTC%2520organizers%2520and%2520the%2520segmentation%2520accuracy%2520measured%2520based%2520on%2520the%25202D%2520gold%2520truth%2520of%2520CTC%2520is%2520within%2520the%2520inter-annotator%2520variability%2520margins.%2520We%2520compared%2520the%2520created%25203D%2520annotations%2520with%2520automatically%2520created%2520silver%2520truth%2520provided%2520by%2520CTC.%2520We%2520have%2520found%2520the%2520proposed%2520annotations%2520better%2520represent%2520the%2520complexity%2520of%2520the%2520input%2520images.%2520The%2520presented%2520annotations%2520can%2520be%2520used%2520for%2520testing%2520and%2520training%2520cell%2520segmentation%252C%2520or%2520analyzing%25203D%2520shapes%2520of%2520highly%2520dynamic%2520objects.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10797v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Full%20segmentation%20annotations%20of%203D%20time-lapse%20microscopy%20images%20of%20MDA231%20cells&entry.906535625=Aleksandra%20Melnikova%20and%20Petr%20Matula&entry.1292438233=High-quality%2C%20publicly%20available%20segmentation%20annotations%20of%20image%20and%20video%20datasets%20are%20critical%20for%20advancing%20the%20field%20of%20image%20processing.%20In%20particular%2C%20annotations%20of%20volumetric%20images%20of%20a%20large%20number%20of%20targets%20are%20time-consuming%20and%20challenging.%20In%20%28Melnikova%2C%20A.%2C%20%26%20Matula%2C%20P.%2C%202025%29%2C%20we%20presented%20the%20first%20publicly%20available%20full%203D%20time-lapse%20segmentation%20annotations%20of%20migrating%20cells%20with%20complex%20dynamic%20shapes.%20Concretely%2C%20three%20distinct%20humans%20annotated%20two%20sequences%20of%20MDA231%20human%20breast%20carcinoma%20cells%20%28Fluo-C3DL-MDA231%29%20from%20the%20Cell%20Tracking%20Challenge%20%28CTC%29.%0A%20%20This%20paper%20aims%20to%20provide%20a%20comprehensive%20description%20of%20the%20dataset%20and%20accompanying%20experiments%20that%20were%20not%20included%20in%20%28Melnikova%2C%20A.%2C%20%26%20Matula%2C%20P.%2C%202025%29%20due%20to%20limitations%20in%20publication%20space.%20Namely%2C%20we%20show%20that%20the%20created%20annotations%20are%20consistent%20with%20the%20previously%20published%20tracking%20markers%20provided%20by%20the%20CTC%20organizers%20and%20the%20segmentation%20accuracy%20measured%20based%20on%20the%202D%20gold%20truth%20of%20CTC%20is%20within%20the%20inter-annotator%20variability%20margins.%20We%20compared%20the%20created%203D%20annotations%20with%20automatically%20created%20silver%20truth%20provided%20by%20CTC.%20We%20have%20found%20the%20proposed%20annotations%20better%20represent%20the%20complexity%20of%20the%20input%20images.%20The%20presented%20annotations%20can%20be%20used%20for%20testing%20and%20training%20cell%20segmentation%2C%20or%20analyzing%203D%20shapes%20of%20highly%20dynamic%20objects.&entry.1838667208=http%3A//arxiv.org/abs/2510.10797v2&entry.124074799=Read"},
{"title": "CoV: Chain-of-View Prompting for Spatial Reasoning", "author": "Haoyu Zhao and Akide Liu and Zeyu Zhang and Weijie Wang and Feng Chen and Ruihan Zhu and Gholamreza Haffari and Bohan Zhuang", "abstract": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.", "link": "http://arxiv.org/abs/2601.05172v1", "date": "2026-01-08", "relevancy": 2.3974, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6092}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoV%3A%20Chain-of-View%20Prompting%20for%20Spatial%20Reasoning&body=Title%3A%20CoV%3A%20Chain-of-View%20Prompting%20for%20Spatial%20Reasoning%0AAuthor%3A%20Haoyu%20Zhao%20and%20Akide%20Liu%20and%20Zeyu%20Zhang%20and%20Weijie%20Wang%20and%20Feng%20Chen%20and%20Ruihan%20Zhu%20and%20Gholamreza%20Haffari%20and%20Bohan%20Zhuang%0AAbstract%3A%20Embodied%20question%20answering%20%28EQA%29%20in%203D%20environments%20often%20requires%20collecting%20context%20that%20is%20distributed%20across%20multiple%20viewpoints%20and%20partially%20occluded.%20However%2C%20most%20recent%20vision--language%20models%20%28VLMs%29%20are%20constrained%20to%20a%20fixed%20and%20finite%20set%20of%20input%20views%2C%20which%20limits%20their%20ability%20to%20acquire%20question-relevant%20context%20at%20inference%20time%20and%20hinders%20complex%20spatial%20reasoning.%20We%20propose%20Chain-of-View%20%28CoV%29%20prompting%2C%20a%20training-free%2C%20test-time%20reasoning%20framework%20that%20transforms%20a%20VLM%20into%20an%20active%20viewpoint%20reasoner%20through%20a%20coarse-to-fine%20exploration%20process.%20CoV%20first%20employs%20a%20View%20Selection%20agent%20to%20filter%20redundant%20frames%20and%20identify%20question-aligned%20anchor%20views.%20It%20then%20performs%20fine-grained%20view%20adjustment%20by%20interleaving%20iterative%20reasoning%20with%20discrete%20camera%20actions%2C%20obtaining%20new%20observations%20from%20the%20underlying%203D%20scene%20representation%20until%20sufficient%20context%20is%20gathered%20or%20a%20step%20budget%20is%20reached.%0A%20%20We%20evaluate%20CoV%20on%20OpenEQA%20across%20four%20mainstream%20VLMs%20and%20obtain%20an%20average%20%2B11.56%5C%25%20improvement%20in%20LLM-Match%2C%20with%20a%20maximum%20gain%20of%20%2B13.62%5C%25%20on%20Qwen3-VL-Flash.%20CoV%20further%20exhibits%20test-time%20scaling%3A%20increasing%20the%20minimum%20action%20budget%20yields%20an%20additional%20%2B2.51%5C%25%20average%20improvement%2C%20peaking%20at%20%2B3.73%5C%25%20on%20Gemini-2.5-Flash.%20On%20ScanQA%20and%20SQA3D%2C%20CoV%20delivers%20strong%20performance%20%28e.g.%2C%20116%20CIDEr%20/%2031.9%20EM%401%20on%20ScanQA%20and%2051.1%20EM%401%20on%20SQA3D%29.%20Overall%2C%20these%20results%20suggest%20that%20question-aligned%20view%20selection%20coupled%20with%20open-view%20search%20is%20an%20effective%2C%20model-agnostic%20strategy%20for%20improving%20spatial%20reasoning%20in%203D%20EQA%20without%20additional%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoV%253A%2520Chain-of-View%2520Prompting%2520for%2520Spatial%2520Reasoning%26entry.906535625%3DHaoyu%2520Zhao%2520and%2520Akide%2520Liu%2520and%2520Zeyu%2520Zhang%2520and%2520Weijie%2520Wang%2520and%2520Feng%2520Chen%2520and%2520Ruihan%2520Zhu%2520and%2520Gholamreza%2520Haffari%2520and%2520Bohan%2520Zhuang%26entry.1292438233%3DEmbodied%2520question%2520answering%2520%2528EQA%2529%2520in%25203D%2520environments%2520often%2520requires%2520collecting%2520context%2520that%2520is%2520distributed%2520across%2520multiple%2520viewpoints%2520and%2520partially%2520occluded.%2520However%252C%2520most%2520recent%2520vision--language%2520models%2520%2528VLMs%2529%2520are%2520constrained%2520to%2520a%2520fixed%2520and%2520finite%2520set%2520of%2520input%2520views%252C%2520which%2520limits%2520their%2520ability%2520to%2520acquire%2520question-relevant%2520context%2520at%2520inference%2520time%2520and%2520hinders%2520complex%2520spatial%2520reasoning.%2520We%2520propose%2520Chain-of-View%2520%2528CoV%2529%2520prompting%252C%2520a%2520training-free%252C%2520test-time%2520reasoning%2520framework%2520that%2520transforms%2520a%2520VLM%2520into%2520an%2520active%2520viewpoint%2520reasoner%2520through%2520a%2520coarse-to-fine%2520exploration%2520process.%2520CoV%2520first%2520employs%2520a%2520View%2520Selection%2520agent%2520to%2520filter%2520redundant%2520frames%2520and%2520identify%2520question-aligned%2520anchor%2520views.%2520It%2520then%2520performs%2520fine-grained%2520view%2520adjustment%2520by%2520interleaving%2520iterative%2520reasoning%2520with%2520discrete%2520camera%2520actions%252C%2520obtaining%2520new%2520observations%2520from%2520the%2520underlying%25203D%2520scene%2520representation%2520until%2520sufficient%2520context%2520is%2520gathered%2520or%2520a%2520step%2520budget%2520is%2520reached.%250A%2520%2520We%2520evaluate%2520CoV%2520on%2520OpenEQA%2520across%2520four%2520mainstream%2520VLMs%2520and%2520obtain%2520an%2520average%2520%252B11.56%255C%2525%2520improvement%2520in%2520LLM-Match%252C%2520with%2520a%2520maximum%2520gain%2520of%2520%252B13.62%255C%2525%2520on%2520Qwen3-VL-Flash.%2520CoV%2520further%2520exhibits%2520test-time%2520scaling%253A%2520increasing%2520the%2520minimum%2520action%2520budget%2520yields%2520an%2520additional%2520%252B2.51%255C%2525%2520average%2520improvement%252C%2520peaking%2520at%2520%252B3.73%255C%2525%2520on%2520Gemini-2.5-Flash.%2520On%2520ScanQA%2520and%2520SQA3D%252C%2520CoV%2520delivers%2520strong%2520performance%2520%2528e.g.%252C%2520116%2520CIDEr%2520/%252031.9%2520EM%25401%2520on%2520ScanQA%2520and%252051.1%2520EM%25401%2520on%2520SQA3D%2529.%2520Overall%252C%2520these%2520results%2520suggest%2520that%2520question-aligned%2520view%2520selection%2520coupled%2520with%2520open-view%2520search%2520is%2520an%2520effective%252C%2520model-agnostic%2520strategy%2520for%2520improving%2520spatial%2520reasoning%2520in%25203D%2520EQA%2520without%2520additional%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoV%3A%20Chain-of-View%20Prompting%20for%20Spatial%20Reasoning&entry.906535625=Haoyu%20Zhao%20and%20Akide%20Liu%20and%20Zeyu%20Zhang%20and%20Weijie%20Wang%20and%20Feng%20Chen%20and%20Ruihan%20Zhu%20and%20Gholamreza%20Haffari%20and%20Bohan%20Zhuang&entry.1292438233=Embodied%20question%20answering%20%28EQA%29%20in%203D%20environments%20often%20requires%20collecting%20context%20that%20is%20distributed%20across%20multiple%20viewpoints%20and%20partially%20occluded.%20However%2C%20most%20recent%20vision--language%20models%20%28VLMs%29%20are%20constrained%20to%20a%20fixed%20and%20finite%20set%20of%20input%20views%2C%20which%20limits%20their%20ability%20to%20acquire%20question-relevant%20context%20at%20inference%20time%20and%20hinders%20complex%20spatial%20reasoning.%20We%20propose%20Chain-of-View%20%28CoV%29%20prompting%2C%20a%20training-free%2C%20test-time%20reasoning%20framework%20that%20transforms%20a%20VLM%20into%20an%20active%20viewpoint%20reasoner%20through%20a%20coarse-to-fine%20exploration%20process.%20CoV%20first%20employs%20a%20View%20Selection%20agent%20to%20filter%20redundant%20frames%20and%20identify%20question-aligned%20anchor%20views.%20It%20then%20performs%20fine-grained%20view%20adjustment%20by%20interleaving%20iterative%20reasoning%20with%20discrete%20camera%20actions%2C%20obtaining%20new%20observations%20from%20the%20underlying%203D%20scene%20representation%20until%20sufficient%20context%20is%20gathered%20or%20a%20step%20budget%20is%20reached.%0A%20%20We%20evaluate%20CoV%20on%20OpenEQA%20across%20four%20mainstream%20VLMs%20and%20obtain%20an%20average%20%2B11.56%5C%25%20improvement%20in%20LLM-Match%2C%20with%20a%20maximum%20gain%20of%20%2B13.62%5C%25%20on%20Qwen3-VL-Flash.%20CoV%20further%20exhibits%20test-time%20scaling%3A%20increasing%20the%20minimum%20action%20budget%20yields%20an%20additional%20%2B2.51%5C%25%20average%20improvement%2C%20peaking%20at%20%2B3.73%5C%25%20on%20Gemini-2.5-Flash.%20On%20ScanQA%20and%20SQA3D%2C%20CoV%20delivers%20strong%20performance%20%28e.g.%2C%20116%20CIDEr%20/%2031.9%20EM%401%20on%20ScanQA%20and%2051.1%20EM%401%20on%20SQA3D%29.%20Overall%2C%20these%20results%20suggest%20that%20question-aligned%20view%20selection%20coupled%20with%20open-view%20search%20is%20an%20effective%2C%20model-agnostic%20strategy%20for%20improving%20spatial%20reasoning%20in%203D%20EQA%20without%20additional%20training.&entry.1838667208=http%3A//arxiv.org/abs/2601.05172v1&entry.124074799=Read"},
{"title": "Key-Value Pair-Free Continual Learner via Task-Specific Prompt-Prototype", "author": "Haihua Luo and Xuming Ran and Zhengji Li and Huiyan Xue and Tingting Jiang and Jiangrong Shen and Tommi K\u00e4rkk\u00e4inen and Qi Xu and Fengyu Cong", "abstract": "Continual learning aims to enable models to acquire new knowledge while retaining previously learned information. Prompt-based methods have shown remarkable performance in this domain; however, they typically rely on key-value pairing, which can introduce inter-task interference and hinder scalability. To overcome these limitations, we propose a novel approach employing task-specific Prompt-Prototype (ProP), thereby eliminating the need for key-value pairs. In our method, task-specific prompts facilitate more effective feature learning for the current task, while corresponding prototypes capture the representative features of the input. During inference, predictions are generated by binding each task-specific prompt with its associated prototype. Additionally, we introduce regularization constraints during prompt initialization to penalize excessively large values, thereby enhancing stability. Experiments on several widely used datasets demonstrate the effectiveness of the proposed method. In contrast to mainstream prompt-based approaches, our framework removes the dependency on key-value pairs, offering a fresh perspective for future continual learning research.", "link": "http://arxiv.org/abs/2601.04864v1", "date": "2026-01-08", "relevancy": 2.3768, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4769}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4769}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Key-Value%20Pair-Free%20Continual%20Learner%20via%20Task-Specific%20Prompt-Prototype&body=Title%3A%20Key-Value%20Pair-Free%20Continual%20Learner%20via%20Task-Specific%20Prompt-Prototype%0AAuthor%3A%20Haihua%20Luo%20and%20Xuming%20Ran%20and%20Zhengji%20Li%20and%20Huiyan%20Xue%20and%20Tingting%20Jiang%20and%20Jiangrong%20Shen%20and%20Tommi%20K%C3%A4rkk%C3%A4inen%20and%20Qi%20Xu%20and%20Fengyu%20Cong%0AAbstract%3A%20Continual%20learning%20aims%20to%20enable%20models%20to%20acquire%20new%20knowledge%20while%20retaining%20previously%20learned%20information.%20Prompt-based%20methods%20have%20shown%20remarkable%20performance%20in%20this%20domain%3B%20however%2C%20they%20typically%20rely%20on%20key-value%20pairing%2C%20which%20can%20introduce%20inter-task%20interference%20and%20hinder%20scalability.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%20approach%20employing%20task-specific%20Prompt-Prototype%20%28ProP%29%2C%20thereby%20eliminating%20the%20need%20for%20key-value%20pairs.%20In%20our%20method%2C%20task-specific%20prompts%20facilitate%20more%20effective%20feature%20learning%20for%20the%20current%20task%2C%20while%20corresponding%20prototypes%20capture%20the%20representative%20features%20of%20the%20input.%20During%20inference%2C%20predictions%20are%20generated%20by%20binding%20each%20task-specific%20prompt%20with%20its%20associated%20prototype.%20Additionally%2C%20we%20introduce%20regularization%20constraints%20during%20prompt%20initialization%20to%20penalize%20excessively%20large%20values%2C%20thereby%20enhancing%20stability.%20Experiments%20on%20several%20widely%20used%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%20In%20contrast%20to%20mainstream%20prompt-based%20approaches%2C%20our%20framework%20removes%20the%20dependency%20on%20key-value%20pairs%2C%20offering%20a%20fresh%20perspective%20for%20future%20continual%20learning%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKey-Value%2520Pair-Free%2520Continual%2520Learner%2520via%2520Task-Specific%2520Prompt-Prototype%26entry.906535625%3DHaihua%2520Luo%2520and%2520Xuming%2520Ran%2520and%2520Zhengji%2520Li%2520and%2520Huiyan%2520Xue%2520and%2520Tingting%2520Jiang%2520and%2520Jiangrong%2520Shen%2520and%2520Tommi%2520K%25C3%25A4rkk%25C3%25A4inen%2520and%2520Qi%2520Xu%2520and%2520Fengyu%2520Cong%26entry.1292438233%3DContinual%2520learning%2520aims%2520to%2520enable%2520models%2520to%2520acquire%2520new%2520knowledge%2520while%2520retaining%2520previously%2520learned%2520information.%2520Prompt-based%2520methods%2520have%2520shown%2520remarkable%2520performance%2520in%2520this%2520domain%253B%2520however%252C%2520they%2520typically%2520rely%2520on%2520key-value%2520pairing%252C%2520which%2520can%2520introduce%2520inter-task%2520interference%2520and%2520hinder%2520scalability.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520approach%2520employing%2520task-specific%2520Prompt-Prototype%2520%2528ProP%2529%252C%2520thereby%2520eliminating%2520the%2520need%2520for%2520key-value%2520pairs.%2520In%2520our%2520method%252C%2520task-specific%2520prompts%2520facilitate%2520more%2520effective%2520feature%2520learning%2520for%2520the%2520current%2520task%252C%2520while%2520corresponding%2520prototypes%2520capture%2520the%2520representative%2520features%2520of%2520the%2520input.%2520During%2520inference%252C%2520predictions%2520are%2520generated%2520by%2520binding%2520each%2520task-specific%2520prompt%2520with%2520its%2520associated%2520prototype.%2520Additionally%252C%2520we%2520introduce%2520regularization%2520constraints%2520during%2520prompt%2520initialization%2520to%2520penalize%2520excessively%2520large%2520values%252C%2520thereby%2520enhancing%2520stability.%2520Experiments%2520on%2520several%2520widely%2520used%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%2520In%2520contrast%2520to%2520mainstream%2520prompt-based%2520approaches%252C%2520our%2520framework%2520removes%2520the%2520dependency%2520on%2520key-value%2520pairs%252C%2520offering%2520a%2520fresh%2520perspective%2520for%2520future%2520continual%2520learning%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Key-Value%20Pair-Free%20Continual%20Learner%20via%20Task-Specific%20Prompt-Prototype&entry.906535625=Haihua%20Luo%20and%20Xuming%20Ran%20and%20Zhengji%20Li%20and%20Huiyan%20Xue%20and%20Tingting%20Jiang%20and%20Jiangrong%20Shen%20and%20Tommi%20K%C3%A4rkk%C3%A4inen%20and%20Qi%20Xu%20and%20Fengyu%20Cong&entry.1292438233=Continual%20learning%20aims%20to%20enable%20models%20to%20acquire%20new%20knowledge%20while%20retaining%20previously%20learned%20information.%20Prompt-based%20methods%20have%20shown%20remarkable%20performance%20in%20this%20domain%3B%20however%2C%20they%20typically%20rely%20on%20key-value%20pairing%2C%20which%20can%20introduce%20inter-task%20interference%20and%20hinder%20scalability.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%20approach%20employing%20task-specific%20Prompt-Prototype%20%28ProP%29%2C%20thereby%20eliminating%20the%20need%20for%20key-value%20pairs.%20In%20our%20method%2C%20task-specific%20prompts%20facilitate%20more%20effective%20feature%20learning%20for%20the%20current%20task%2C%20while%20corresponding%20prototypes%20capture%20the%20representative%20features%20of%20the%20input.%20During%20inference%2C%20predictions%20are%20generated%20by%20binding%20each%20task-specific%20prompt%20with%20its%20associated%20prototype.%20Additionally%2C%20we%20introduce%20regularization%20constraints%20during%20prompt%20initialization%20to%20penalize%20excessively%20large%20values%2C%20thereby%20enhancing%20stability.%20Experiments%20on%20several%20widely%20used%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%20In%20contrast%20to%20mainstream%20prompt-based%20approaches%2C%20our%20framework%20removes%20the%20dependency%20on%20key-value%20pairs%2C%20offering%20a%20fresh%20perspective%20for%20future%20continual%20learning%20research.&entry.1838667208=http%3A//arxiv.org/abs/2601.04864v1&entry.124074799=Read"},
{"title": "Structured Matching via Cost-Regularized Unbalanced Optimal Transport", "author": "Emanuele Pardini and Katerina Papagiannouli", "abstract": "Unbalanced optimal transport (UOT) provides a flexible way to match or compare nonnegative finite Radon measures. However, UOT requires a predefined ground transport cost, which may misrepresent the data's underlying geometry. Choosing such a cost is particularly challenging when datasets live in heterogeneous spaces, often motivating practitioners to adopt Gromov-Wasserstein formulations. To address this challenge, we introduce cost-regularized unbalanced optimal transport (CR-UOT), a framework that allows the ground cost to vary while allowing mass creation and removal. We show that CR-UOT incorporates unbalanced Gromov-Wasserstein type problems through families of inner-product costs parameterized by linear transformations, enabling the matching of measures or point clouds across Euclidean spaces. We develop algorithms for such CR-UOT problems using entropic regularization and demonstrate that this approach improves the alignment of heterogeneous single-cell omics profiles, especially when many cells lack direct matches.", "link": "http://arxiv.org/abs/2511.19075v2", "date": "2026-01-08", "relevancy": 2.3641, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4811}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4707}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Matching%20via%20Cost-Regularized%20Unbalanced%20Optimal%20Transport&body=Title%3A%20Structured%20Matching%20via%20Cost-Regularized%20Unbalanced%20Optimal%20Transport%0AAuthor%3A%20Emanuele%20Pardini%20and%20Katerina%20Papagiannouli%0AAbstract%3A%20Unbalanced%20optimal%20transport%20%28UOT%29%20provides%20a%20flexible%20way%20to%20match%20or%20compare%20nonnegative%20finite%20Radon%20measures.%20However%2C%20UOT%20requires%20a%20predefined%20ground%20transport%20cost%2C%20which%20may%20misrepresent%20the%20data%27s%20underlying%20geometry.%20Choosing%20such%20a%20cost%20is%20particularly%20challenging%20when%20datasets%20live%20in%20heterogeneous%20spaces%2C%20often%20motivating%20practitioners%20to%20adopt%20Gromov-Wasserstein%20formulations.%20To%20address%20this%20challenge%2C%20we%20introduce%20cost-regularized%20unbalanced%20optimal%20transport%20%28CR-UOT%29%2C%20a%20framework%20that%20allows%20the%20ground%20cost%20to%20vary%20while%20allowing%20mass%20creation%20and%20removal.%20We%20show%20that%20CR-UOT%20incorporates%20unbalanced%20Gromov-Wasserstein%20type%20problems%20through%20families%20of%20inner-product%20costs%20parameterized%20by%20linear%20transformations%2C%20enabling%20the%20matching%20of%20measures%20or%20point%20clouds%20across%20Euclidean%20spaces.%20We%20develop%20algorithms%20for%20such%20CR-UOT%20problems%20using%20entropic%20regularization%20and%20demonstrate%20that%20this%20approach%20improves%20the%20alignment%20of%20heterogeneous%20single-cell%20omics%20profiles%2C%20especially%20when%20many%20cells%20lack%20direct%20matches.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Matching%2520via%2520Cost-Regularized%2520Unbalanced%2520Optimal%2520Transport%26entry.906535625%3DEmanuele%2520Pardini%2520and%2520Katerina%2520Papagiannouli%26entry.1292438233%3DUnbalanced%2520optimal%2520transport%2520%2528UOT%2529%2520provides%2520a%2520flexible%2520way%2520to%2520match%2520or%2520compare%2520nonnegative%2520finite%2520Radon%2520measures.%2520However%252C%2520UOT%2520requires%2520a%2520predefined%2520ground%2520transport%2520cost%252C%2520which%2520may%2520misrepresent%2520the%2520data%2527s%2520underlying%2520geometry.%2520Choosing%2520such%2520a%2520cost%2520is%2520particularly%2520challenging%2520when%2520datasets%2520live%2520in%2520heterogeneous%2520spaces%252C%2520often%2520motivating%2520practitioners%2520to%2520adopt%2520Gromov-Wasserstein%2520formulations.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520cost-regularized%2520unbalanced%2520optimal%2520transport%2520%2528CR-UOT%2529%252C%2520a%2520framework%2520that%2520allows%2520the%2520ground%2520cost%2520to%2520vary%2520while%2520allowing%2520mass%2520creation%2520and%2520removal.%2520We%2520show%2520that%2520CR-UOT%2520incorporates%2520unbalanced%2520Gromov-Wasserstein%2520type%2520problems%2520through%2520families%2520of%2520inner-product%2520costs%2520parameterized%2520by%2520linear%2520transformations%252C%2520enabling%2520the%2520matching%2520of%2520measures%2520or%2520point%2520clouds%2520across%2520Euclidean%2520spaces.%2520We%2520develop%2520algorithms%2520for%2520such%2520CR-UOT%2520problems%2520using%2520entropic%2520regularization%2520and%2520demonstrate%2520that%2520this%2520approach%2520improves%2520the%2520alignment%2520of%2520heterogeneous%2520single-cell%2520omics%2520profiles%252C%2520especially%2520when%2520many%2520cells%2520lack%2520direct%2520matches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Matching%20via%20Cost-Regularized%20Unbalanced%20Optimal%20Transport&entry.906535625=Emanuele%20Pardini%20and%20Katerina%20Papagiannouli&entry.1292438233=Unbalanced%20optimal%20transport%20%28UOT%29%20provides%20a%20flexible%20way%20to%20match%20or%20compare%20nonnegative%20finite%20Radon%20measures.%20However%2C%20UOT%20requires%20a%20predefined%20ground%20transport%20cost%2C%20which%20may%20misrepresent%20the%20data%27s%20underlying%20geometry.%20Choosing%20such%20a%20cost%20is%20particularly%20challenging%20when%20datasets%20live%20in%20heterogeneous%20spaces%2C%20often%20motivating%20practitioners%20to%20adopt%20Gromov-Wasserstein%20formulations.%20To%20address%20this%20challenge%2C%20we%20introduce%20cost-regularized%20unbalanced%20optimal%20transport%20%28CR-UOT%29%2C%20a%20framework%20that%20allows%20the%20ground%20cost%20to%20vary%20while%20allowing%20mass%20creation%20and%20removal.%20We%20show%20that%20CR-UOT%20incorporates%20unbalanced%20Gromov-Wasserstein%20type%20problems%20through%20families%20of%20inner-product%20costs%20parameterized%20by%20linear%20transformations%2C%20enabling%20the%20matching%20of%20measures%20or%20point%20clouds%20across%20Euclidean%20spaces.%20We%20develop%20algorithms%20for%20such%20CR-UOT%20problems%20using%20entropic%20regularization%20and%20demonstrate%20that%20this%20approach%20improves%20the%20alignment%20of%20heterogeneous%20single-cell%20omics%20profiles%2C%20especially%20when%20many%20cells%20lack%20direct%20matches.&entry.1838667208=http%3A//arxiv.org/abs/2511.19075v2&entry.124074799=Read"},
{"title": "Pruning the Unsurprising: Efficient LLM Reasoning via First-Token Surprisal", "author": "Wenhao Zeng and Yaoning Wang and Chao Hu and Yuling Shi and Chengcheng Wan and Hongyu Zhang and Xiaodong Gu", "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces pose substantial challenges for training cost and inference latency. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps because of the dilution of logical information. In this paper, we propose ASAP (Anchor-guided, SurprisAl-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. Leveraging the insight that logical branching choices are concentrated at the onset of reasoning steps, it then enables logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP distills the models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning. Experiments show that ASAP achieves state-of-the-art accuracy across multiple benchmarks while substantially reducing training and inference costs.", "link": "http://arxiv.org/abs/2508.05988v2", "date": "2026-01-08", "relevancy": 2.3568, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4924}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pruning%20the%20Unsurprising%3A%20Efficient%20LLM%20Reasoning%20via%20First-Token%20Surprisal&body=Title%3A%20Pruning%20the%20Unsurprising%3A%20Efficient%20LLM%20Reasoning%20via%20First-Token%20Surprisal%0AAuthor%3A%20Wenhao%20Zeng%20and%20Yaoning%20Wang%20and%20Chao%20Hu%20and%20Yuling%20Shi%20and%20Chengcheng%20Wan%20and%20Hongyu%20Zhang%20and%20Xiaodong%20Gu%0AAbstract%3A%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20demonstrated%20remarkable%20capabilities%20by%20scaling%20up%20the%20length%20of%20Chain-of-Thought%20%28CoT%29.%20However%2C%20excessively%20long%20reasoning%20traces%20pose%20substantial%20challenges%20for%20training%20cost%20and%20inference%20latency.%20While%20various%20CoT%20compression%20approaches%20have%20emerged%20to%20address%20this%20challenge%2C%20they%20face%20inherent%20trade-offs%3A%20token-level%20methods%20often%20disrupt%20syntactic%20and%20logical%20coherence%2C%20while%20step-level%20methods%20based%20on%20perplexity%20fail%20to%20reliably%20capture%20the%20logically%20critical%20reasoning%20steps%20because%20of%20the%20dilution%20of%20logical%20information.%20In%20this%20paper%2C%20we%20propose%20ASAP%20%28Anchor-guided%2C%20SurprisAl-based%20Pruning%29%2C%20a%20novel%20coarse-to-fine%20framework%20for%20CoT%20compression.%20ASAP%20first%20performs%20anchor-guided%20pruning%20to%20preserve%20the%20core%20reasoning%20structure%2C%20which%20efficiently%20reduces%20the%20search%20space%20for%20subsequent%20processing.%20Leveraging%20the%20insight%20that%20logical%20branching%20choices%20are%20concentrated%20at%20the%20onset%20of%20reasoning%20steps%2C%20it%20then%20enables%20logic-aware%20pruning%20by%20selecting%20logically%20essential%20reasoning%20steps%20based%20on%20a%20novel%20first-token%20surprisal%20metric.%20Finally%2C%20ASAP%20distills%20the%20models%20to%20autonomously%20generate%20and%20leverage%20these%20concise%20CoTs%20at%20inference%20time%2C%20enabling%20efficient%20reasoning.%20Experiments%20show%20that%20ASAP%20achieves%20state-of-the-art%20accuracy%20across%20multiple%20benchmarks%20while%20substantially%20reducing%20training%20and%20inference%20costs.%0ALink%3A%20http%3A//arxiv.org/abs/2508.05988v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruning%2520the%2520Unsurprising%253A%2520Efficient%2520LLM%2520Reasoning%2520via%2520First-Token%2520Surprisal%26entry.906535625%3DWenhao%2520Zeng%2520and%2520Yaoning%2520Wang%2520and%2520Chao%2520Hu%2520and%2520Yuling%2520Shi%2520and%2520Chengcheng%2520Wan%2520and%2520Hongyu%2520Zhang%2520and%2520Xiaodong%2520Gu%26entry.1292438233%3DLarge%2520Reasoning%2520Models%2520%2528LRMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520by%2520scaling%2520up%2520the%2520length%2520of%2520Chain-of-Thought%2520%2528CoT%2529.%2520However%252C%2520excessively%2520long%2520reasoning%2520traces%2520pose%2520substantial%2520challenges%2520for%2520training%2520cost%2520and%2520inference%2520latency.%2520While%2520various%2520CoT%2520compression%2520approaches%2520have%2520emerged%2520to%2520address%2520this%2520challenge%252C%2520they%2520face%2520inherent%2520trade-offs%253A%2520token-level%2520methods%2520often%2520disrupt%2520syntactic%2520and%2520logical%2520coherence%252C%2520while%2520step-level%2520methods%2520based%2520on%2520perplexity%2520fail%2520to%2520reliably%2520capture%2520the%2520logically%2520critical%2520reasoning%2520steps%2520because%2520of%2520the%2520dilution%2520of%2520logical%2520information.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ASAP%2520%2528Anchor-guided%252C%2520SurprisAl-based%2520Pruning%2529%252C%2520a%2520novel%2520coarse-to-fine%2520framework%2520for%2520CoT%2520compression.%2520ASAP%2520first%2520performs%2520anchor-guided%2520pruning%2520to%2520preserve%2520the%2520core%2520reasoning%2520structure%252C%2520which%2520efficiently%2520reduces%2520the%2520search%2520space%2520for%2520subsequent%2520processing.%2520Leveraging%2520the%2520insight%2520that%2520logical%2520branching%2520choices%2520are%2520concentrated%2520at%2520the%2520onset%2520of%2520reasoning%2520steps%252C%2520it%2520then%2520enables%2520logic-aware%2520pruning%2520by%2520selecting%2520logically%2520essential%2520reasoning%2520steps%2520based%2520on%2520a%2520novel%2520first-token%2520surprisal%2520metric.%2520Finally%252C%2520ASAP%2520distills%2520the%2520models%2520to%2520autonomously%2520generate%2520and%2520leverage%2520these%2520concise%2520CoTs%2520at%2520inference%2520time%252C%2520enabling%2520efficient%2520reasoning.%2520Experiments%2520show%2520that%2520ASAP%2520achieves%2520state-of-the-art%2520accuracy%2520across%2520multiple%2520benchmarks%2520while%2520substantially%2520reducing%2520training%2520and%2520inference%2520costs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05988v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pruning%20the%20Unsurprising%3A%20Efficient%20LLM%20Reasoning%20via%20First-Token%20Surprisal&entry.906535625=Wenhao%20Zeng%20and%20Yaoning%20Wang%20and%20Chao%20Hu%20and%20Yuling%20Shi%20and%20Chengcheng%20Wan%20and%20Hongyu%20Zhang%20and%20Xiaodong%20Gu&entry.1292438233=Large%20Reasoning%20Models%20%28LRMs%29%20have%20demonstrated%20remarkable%20capabilities%20by%20scaling%20up%20the%20length%20of%20Chain-of-Thought%20%28CoT%29.%20However%2C%20excessively%20long%20reasoning%20traces%20pose%20substantial%20challenges%20for%20training%20cost%20and%20inference%20latency.%20While%20various%20CoT%20compression%20approaches%20have%20emerged%20to%20address%20this%20challenge%2C%20they%20face%20inherent%20trade-offs%3A%20token-level%20methods%20often%20disrupt%20syntactic%20and%20logical%20coherence%2C%20while%20step-level%20methods%20based%20on%20perplexity%20fail%20to%20reliably%20capture%20the%20logically%20critical%20reasoning%20steps%20because%20of%20the%20dilution%20of%20logical%20information.%20In%20this%20paper%2C%20we%20propose%20ASAP%20%28Anchor-guided%2C%20SurprisAl-based%20Pruning%29%2C%20a%20novel%20coarse-to-fine%20framework%20for%20CoT%20compression.%20ASAP%20first%20performs%20anchor-guided%20pruning%20to%20preserve%20the%20core%20reasoning%20structure%2C%20which%20efficiently%20reduces%20the%20search%20space%20for%20subsequent%20processing.%20Leveraging%20the%20insight%20that%20logical%20branching%20choices%20are%20concentrated%20at%20the%20onset%20of%20reasoning%20steps%2C%20it%20then%20enables%20logic-aware%20pruning%20by%20selecting%20logically%20essential%20reasoning%20steps%20based%20on%20a%20novel%20first-token%20surprisal%20metric.%20Finally%2C%20ASAP%20distills%20the%20models%20to%20autonomously%20generate%20and%20leverage%20these%20concise%20CoTs%20at%20inference%20time%2C%20enabling%20efficient%20reasoning.%20Experiments%20show%20that%20ASAP%20achieves%20state-of-the-art%20accuracy%20across%20multiple%20benchmarks%20while%20substantially%20reducing%20training%20and%20inference%20costs.&entry.1838667208=http%3A//arxiv.org/abs/2508.05988v2&entry.124074799=Read"},
{"title": "Cardinality augmented loss functions", "author": "Miguel O'Malley", "abstract": "Class imbalance is a common and pernicious issue for the training of neural networks. Often, an imbalanced majority class can dominate training to skew classifier performance towards the majority outcome. To address this problem we introduce cardinality augmented loss functions, derived from cardinality-like invariants in modern mathematics literature such as magnitude and the spread. These invariants enrich the concept of cardinality by evaluating the `effective diversity' of a metric space, and as such represent a natural solution to overly homogeneous training data. In this work, we establish a methodology for applying cardinality augmented loss functions in the training of neural networks and report results on both artificially imbalanced datasets as well as a real-world imbalanced material science dataset. We observe significant performance improvement among minority classes, as well as improvement in overall performance metrics.", "link": "http://arxiv.org/abs/2601.04941v1", "date": "2026-01-08", "relevancy": 2.3511, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5213}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4589}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cardinality%20augmented%20loss%20functions&body=Title%3A%20Cardinality%20augmented%20loss%20functions%0AAuthor%3A%20Miguel%20O%27Malley%0AAbstract%3A%20Class%20imbalance%20is%20a%20common%20and%20pernicious%20issue%20for%20the%20training%20of%20neural%20networks.%20Often%2C%20an%20imbalanced%20majority%20class%20can%20dominate%20training%20to%20skew%20classifier%20performance%20towards%20the%20majority%20outcome.%20To%20address%20this%20problem%20we%20introduce%20cardinality%20augmented%20loss%20functions%2C%20derived%20from%20cardinality-like%20invariants%20in%20modern%20mathematics%20literature%20such%20as%20magnitude%20and%20the%20spread.%20These%20invariants%20enrich%20the%20concept%20of%20cardinality%20by%20evaluating%20the%20%60effective%20diversity%27%20of%20a%20metric%20space%2C%20and%20as%20such%20represent%20a%20natural%20solution%20to%20overly%20homogeneous%20training%20data.%20In%20this%20work%2C%20we%20establish%20a%20methodology%20for%20applying%20cardinality%20augmented%20loss%20functions%20in%20the%20training%20of%20neural%20networks%20and%20report%20results%20on%20both%20artificially%20imbalanced%20datasets%20as%20well%20as%20a%20real-world%20imbalanced%20material%20science%20dataset.%20We%20observe%20significant%20performance%20improvement%20among%20minority%20classes%2C%20as%20well%20as%20improvement%20in%20overall%20performance%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCardinality%2520augmented%2520loss%2520functions%26entry.906535625%3DMiguel%2520O%2527Malley%26entry.1292438233%3DClass%2520imbalance%2520is%2520a%2520common%2520and%2520pernicious%2520issue%2520for%2520the%2520training%2520of%2520neural%2520networks.%2520Often%252C%2520an%2520imbalanced%2520majority%2520class%2520can%2520dominate%2520training%2520to%2520skew%2520classifier%2520performance%2520towards%2520the%2520majority%2520outcome.%2520To%2520address%2520this%2520problem%2520we%2520introduce%2520cardinality%2520augmented%2520loss%2520functions%252C%2520derived%2520from%2520cardinality-like%2520invariants%2520in%2520modern%2520mathematics%2520literature%2520such%2520as%2520magnitude%2520and%2520the%2520spread.%2520These%2520invariants%2520enrich%2520the%2520concept%2520of%2520cardinality%2520by%2520evaluating%2520the%2520%2560effective%2520diversity%2527%2520of%2520a%2520metric%2520space%252C%2520and%2520as%2520such%2520represent%2520a%2520natural%2520solution%2520to%2520overly%2520homogeneous%2520training%2520data.%2520In%2520this%2520work%252C%2520we%2520establish%2520a%2520methodology%2520for%2520applying%2520cardinality%2520augmented%2520loss%2520functions%2520in%2520the%2520training%2520of%2520neural%2520networks%2520and%2520report%2520results%2520on%2520both%2520artificially%2520imbalanced%2520datasets%2520as%2520well%2520as%2520a%2520real-world%2520imbalanced%2520material%2520science%2520dataset.%2520We%2520observe%2520significant%2520performance%2520improvement%2520among%2520minority%2520classes%252C%2520as%2520well%2520as%2520improvement%2520in%2520overall%2520performance%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cardinality%20augmented%20loss%20functions&entry.906535625=Miguel%20O%27Malley&entry.1292438233=Class%20imbalance%20is%20a%20common%20and%20pernicious%20issue%20for%20the%20training%20of%20neural%20networks.%20Often%2C%20an%20imbalanced%20majority%20class%20can%20dominate%20training%20to%20skew%20classifier%20performance%20towards%20the%20majority%20outcome.%20To%20address%20this%20problem%20we%20introduce%20cardinality%20augmented%20loss%20functions%2C%20derived%20from%20cardinality-like%20invariants%20in%20modern%20mathematics%20literature%20such%20as%20magnitude%20and%20the%20spread.%20These%20invariants%20enrich%20the%20concept%20of%20cardinality%20by%20evaluating%20the%20%60effective%20diversity%27%20of%20a%20metric%20space%2C%20and%20as%20such%20represent%20a%20natural%20solution%20to%20overly%20homogeneous%20training%20data.%20In%20this%20work%2C%20we%20establish%20a%20methodology%20for%20applying%20cardinality%20augmented%20loss%20functions%20in%20the%20training%20of%20neural%20networks%20and%20report%20results%20on%20both%20artificially%20imbalanced%20datasets%20as%20well%20as%20a%20real-world%20imbalanced%20material%20science%20dataset.%20We%20observe%20significant%20performance%20improvement%20among%20minority%20classes%2C%20as%20well%20as%20improvement%20in%20overall%20performance%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2601.04941v1&entry.124074799=Read"},
{"title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics", "author": "Dahyeon Kye and Jeahun Sung and Minkyu Jeon and Jihyong Oh", "abstract": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.", "link": "http://arxiv.org/abs/2512.07155v4", "date": "2026-01-08", "relevancy": 2.351, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5899}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5898}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHIMERA%3A%20Adaptive%20Cache%20Injection%20and%20Semantic%20Anchor%20Prompting%20for%20Zero-shot%20Image%20Morphing%20with%20Morphing-oriented%20Metrics&body=Title%3A%20CHIMERA%3A%20Adaptive%20Cache%20Injection%20and%20Semantic%20Anchor%20Prompting%20for%20Zero-shot%20Image%20Morphing%20with%20Morphing-oriented%20Metrics%0AAuthor%3A%20Dahyeon%20Kye%20and%20Jeahun%20Sung%20and%20Minkyu%20Jeon%20and%20Jihyong%20Oh%0AAbstract%3A%20Diffusion%20models%20exhibit%20remarkable%20generative%20ability%2C%20yet%20achieving%20smooth%20and%20semantically%20consistent%20image%20morphing%20remains%20a%20challenge.%20Existing%20approaches%20often%20yield%20abrupt%20transitions%20or%20over-saturated%20appearances%20due%20to%20the%20lack%20of%20adaptive%20structural%20and%20semantic%20alignments.%20We%20propose%20CHIMERA%2C%20a%20zero-shot%20diffusion-based%20framework%20that%20formulates%20morphing%20as%20a%20cached%20inversion-guided%20denoising%20process.%20To%20handle%20large%20semantic%20and%20appearance%20disparities%2C%20we%20propose%20Adaptive%20Cache%20Injection%20and%20Semantic%20Anchor%20Prompting.%20Adaptive%20Cache%20Injection%20%28ACI%29%20caches%20down%2C%20mid%2C%20and%20up%20blocks%20features%20from%20both%20inputs%20during%20DDIM%20inversion%20and%20re-injects%20them%20adaptively%20during%20denoising%2C%20enabling%20spatial%20and%20semantic%20alignment%20in%20depth-%20and%20time-adaptive%20manners%20and%20enabling%20natural%20feature%20fusion%20and%20smooth%20transitions.%20Semantic%20Anchor%20Prompting%20%28SAP%29%20leverages%20a%20vision-language%20model%20to%20generate%20a%20shared%20anchor%20prompt%20that%20serves%20as%20a%20semantic%20anchor%2C%20bridging%20dissimilar%20inputs%20and%20guiding%20the%20denoising%20process%20toward%20coherent%20results.%20Finally%2C%20we%20introduce%20the%20Global-Local%20Consistency%20Score%20%28GLCS%29%2C%20a%20morphing-oriented%20metric%20that%20simultaneously%20evaluates%20the%20global%20harmonization%20of%20the%20two%20inputs%20and%20the%20smoothness%20of%20the%20local%20morphing%20transition.%20Extensive%20experiments%20and%20user%20studies%20show%20that%20CHIMERA%20achieves%20smoother%20and%20more%20semantically%20aligned%20transitions%20than%20existing%20methods%2C%20establishing%20a%20new%20state%20of%20the%20art%20in%20image%20morphing.%20The%20code%20and%20project%20page%20will%20be%20publicly%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07155v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHIMERA%253A%2520Adaptive%2520Cache%2520Injection%2520and%2520Semantic%2520Anchor%2520Prompting%2520for%2520Zero-shot%2520Image%2520Morphing%2520with%2520Morphing-oriented%2520Metrics%26entry.906535625%3DDahyeon%2520Kye%2520and%2520Jeahun%2520Sung%2520and%2520Minkyu%2520Jeon%2520and%2520Jihyong%2520Oh%26entry.1292438233%3DDiffusion%2520models%2520exhibit%2520remarkable%2520generative%2520ability%252C%2520yet%2520achieving%2520smooth%2520and%2520semantically%2520consistent%2520image%2520morphing%2520remains%2520a%2520challenge.%2520Existing%2520approaches%2520often%2520yield%2520abrupt%2520transitions%2520or%2520over-saturated%2520appearances%2520due%2520to%2520the%2520lack%2520of%2520adaptive%2520structural%2520and%2520semantic%2520alignments.%2520We%2520propose%2520CHIMERA%252C%2520a%2520zero-shot%2520diffusion-based%2520framework%2520that%2520formulates%2520morphing%2520as%2520a%2520cached%2520inversion-guided%2520denoising%2520process.%2520To%2520handle%2520large%2520semantic%2520and%2520appearance%2520disparities%252C%2520we%2520propose%2520Adaptive%2520Cache%2520Injection%2520and%2520Semantic%2520Anchor%2520Prompting.%2520Adaptive%2520Cache%2520Injection%2520%2528ACI%2529%2520caches%2520down%252C%2520mid%252C%2520and%2520up%2520blocks%2520features%2520from%2520both%2520inputs%2520during%2520DDIM%2520inversion%2520and%2520re-injects%2520them%2520adaptively%2520during%2520denoising%252C%2520enabling%2520spatial%2520and%2520semantic%2520alignment%2520in%2520depth-%2520and%2520time-adaptive%2520manners%2520and%2520enabling%2520natural%2520feature%2520fusion%2520and%2520smooth%2520transitions.%2520Semantic%2520Anchor%2520Prompting%2520%2528SAP%2529%2520leverages%2520a%2520vision-language%2520model%2520to%2520generate%2520a%2520shared%2520anchor%2520prompt%2520that%2520serves%2520as%2520a%2520semantic%2520anchor%252C%2520bridging%2520dissimilar%2520inputs%2520and%2520guiding%2520the%2520denoising%2520process%2520toward%2520coherent%2520results.%2520Finally%252C%2520we%2520introduce%2520the%2520Global-Local%2520Consistency%2520Score%2520%2528GLCS%2529%252C%2520a%2520morphing-oriented%2520metric%2520that%2520simultaneously%2520evaluates%2520the%2520global%2520harmonization%2520of%2520the%2520two%2520inputs%2520and%2520the%2520smoothness%2520of%2520the%2520local%2520morphing%2520transition.%2520Extensive%2520experiments%2520and%2520user%2520studies%2520show%2520that%2520CHIMERA%2520achieves%2520smoother%2520and%2520more%2520semantically%2520aligned%2520transitions%2520than%2520existing%2520methods%252C%2520establishing%2520a%2520new%2520state%2520of%2520the%2520art%2520in%2520image%2520morphing.%2520The%2520code%2520and%2520project%2520page%2520will%2520be%2520publicly%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07155v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHIMERA%3A%20Adaptive%20Cache%20Injection%20and%20Semantic%20Anchor%20Prompting%20for%20Zero-shot%20Image%20Morphing%20with%20Morphing-oriented%20Metrics&entry.906535625=Dahyeon%20Kye%20and%20Jeahun%20Sung%20and%20Minkyu%20Jeon%20and%20Jihyong%20Oh&entry.1292438233=Diffusion%20models%20exhibit%20remarkable%20generative%20ability%2C%20yet%20achieving%20smooth%20and%20semantically%20consistent%20image%20morphing%20remains%20a%20challenge.%20Existing%20approaches%20often%20yield%20abrupt%20transitions%20or%20over-saturated%20appearances%20due%20to%20the%20lack%20of%20adaptive%20structural%20and%20semantic%20alignments.%20We%20propose%20CHIMERA%2C%20a%20zero-shot%20diffusion-based%20framework%20that%20formulates%20morphing%20as%20a%20cached%20inversion-guided%20denoising%20process.%20To%20handle%20large%20semantic%20and%20appearance%20disparities%2C%20we%20propose%20Adaptive%20Cache%20Injection%20and%20Semantic%20Anchor%20Prompting.%20Adaptive%20Cache%20Injection%20%28ACI%29%20caches%20down%2C%20mid%2C%20and%20up%20blocks%20features%20from%20both%20inputs%20during%20DDIM%20inversion%20and%20re-injects%20them%20adaptively%20during%20denoising%2C%20enabling%20spatial%20and%20semantic%20alignment%20in%20depth-%20and%20time-adaptive%20manners%20and%20enabling%20natural%20feature%20fusion%20and%20smooth%20transitions.%20Semantic%20Anchor%20Prompting%20%28SAP%29%20leverages%20a%20vision-language%20model%20to%20generate%20a%20shared%20anchor%20prompt%20that%20serves%20as%20a%20semantic%20anchor%2C%20bridging%20dissimilar%20inputs%20and%20guiding%20the%20denoising%20process%20toward%20coherent%20results.%20Finally%2C%20we%20introduce%20the%20Global-Local%20Consistency%20Score%20%28GLCS%29%2C%20a%20morphing-oriented%20metric%20that%20simultaneously%20evaluates%20the%20global%20harmonization%20of%20the%20two%20inputs%20and%20the%20smoothness%20of%20the%20local%20morphing%20transition.%20Extensive%20experiments%20and%20user%20studies%20show%20that%20CHIMERA%20achieves%20smoother%20and%20more%20semantically%20aligned%20transitions%20than%20existing%20methods%2C%20establishing%20a%20new%20state%20of%20the%20art%20in%20image%20morphing.%20The%20code%20and%20project%20page%20will%20be%20publicly%20released.&entry.1838667208=http%3A//arxiv.org/abs/2512.07155v4&entry.124074799=Read"},
{"title": "Talking with Tables for Better LLM Factual Data Interactions", "author": "Jio Oh and Geon Heo and Seungjun Oh and Hyunjin Kim and JinYeong Bak and Jindong Wang and Xing Xie and Steven Euijong Whang", "abstract": "Large Language Models (LLMs) often struggle with requests related to information retrieval and data manipulation that frequently arise in real-world scenarios under multiple conditions. In this paper, we demonstrate that leveraging tabular structures in LLM interactions, is more effective than utilizing other structures for handling prevalent requests that operate over factual data. Through comprehensive evaluations across various scenarios and request types, we show that providing tabular structures yields a 40.29\\% average performance gain along with better robustness and token efficiency. Through attention-value analysis, we discover that tables help LLMs better locate relevant information, explaining these improvements. Beyond tables and text, we evaluate whether (1) blending structuredness within text, such as providing templates or fixing the order of attributes, and (2) other representative structures, such as knowledge graphs and JSON are helpful. We observe that utilizing tables offers the best balance between efficiency and effectiveness. The method remains robust to task complexity and adapts to unstructured sources through text-to-table conversion. Overall, we highlight the untapped potential of tabular representations for future LLM applications.", "link": "http://arxiv.org/abs/2412.17189v4", "date": "2026-01-08", "relevancy": 2.3388, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4577}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Talking%20with%20Tables%20for%20Better%20LLM%20Factual%20Data%20Interactions&body=Title%3A%20Talking%20with%20Tables%20for%20Better%20LLM%20Factual%20Data%20Interactions%0AAuthor%3A%20Jio%20Oh%20and%20Geon%20Heo%20and%20Seungjun%20Oh%20and%20Hyunjin%20Kim%20and%20JinYeong%20Bak%20and%20Jindong%20Wang%20and%20Xing%20Xie%20and%20Steven%20Euijong%20Whang%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20often%20struggle%20with%20requests%20related%20to%20information%20retrieval%20and%20data%20manipulation%20that%20frequently%20arise%20in%20real-world%20scenarios%20under%20multiple%20conditions.%20In%20this%20paper%2C%20we%20demonstrate%20that%20leveraging%20tabular%20structures%20in%20LLM%20interactions%2C%20is%20more%20effective%20than%20utilizing%20other%20structures%20for%20handling%20prevalent%20requests%20that%20operate%20over%20factual%20data.%20Through%20comprehensive%20evaluations%20across%20various%20scenarios%20and%20request%20types%2C%20we%20show%20that%20providing%20tabular%20structures%20yields%20a%2040.29%5C%25%20average%20performance%20gain%20along%20with%20better%20robustness%20and%20token%20efficiency.%20Through%20attention-value%20analysis%2C%20we%20discover%20that%20tables%20help%20LLMs%20better%20locate%20relevant%20information%2C%20explaining%20these%20improvements.%20Beyond%20tables%20and%20text%2C%20we%20evaluate%20whether%20%281%29%20blending%20structuredness%20within%20text%2C%20such%20as%20providing%20templates%20or%20fixing%20the%20order%20of%20attributes%2C%20and%20%282%29%20other%20representative%20structures%2C%20such%20as%20knowledge%20graphs%20and%20JSON%20are%20helpful.%20We%20observe%20that%20utilizing%20tables%20offers%20the%20best%20balance%20between%20efficiency%20and%20effectiveness.%20The%20method%20remains%20robust%20to%20task%20complexity%20and%20adapts%20to%20unstructured%20sources%20through%20text-to-table%20conversion.%20Overall%2C%20we%20highlight%20the%20untapped%20potential%20of%20tabular%20representations%20for%20future%20LLM%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2412.17189v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalking%2520with%2520Tables%2520for%2520Better%2520LLM%2520Factual%2520Data%2520Interactions%26entry.906535625%3DJio%2520Oh%2520and%2520Geon%2520Heo%2520and%2520Seungjun%2520Oh%2520and%2520Hyunjin%2520Kim%2520and%2520JinYeong%2520Bak%2520and%2520Jindong%2520Wang%2520and%2520Xing%2520Xie%2520and%2520Steven%2520Euijong%2520Whang%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520struggle%2520with%2520requests%2520related%2520to%2520information%2520retrieval%2520and%2520data%2520manipulation%2520that%2520frequently%2520arise%2520in%2520real-world%2520scenarios%2520under%2520multiple%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520that%2520leveraging%2520tabular%2520structures%2520in%2520LLM%2520interactions%252C%2520is%2520more%2520effective%2520than%2520utilizing%2520other%2520structures%2520for%2520handling%2520prevalent%2520requests%2520that%2520operate%2520over%2520factual%2520data.%2520Through%2520comprehensive%2520evaluations%2520across%2520various%2520scenarios%2520and%2520request%2520types%252C%2520we%2520show%2520that%2520providing%2520tabular%2520structures%2520yields%2520a%252040.29%255C%2525%2520average%2520performance%2520gain%2520along%2520with%2520better%2520robustness%2520and%2520token%2520efficiency.%2520Through%2520attention-value%2520analysis%252C%2520we%2520discover%2520that%2520tables%2520help%2520LLMs%2520better%2520locate%2520relevant%2520information%252C%2520explaining%2520these%2520improvements.%2520Beyond%2520tables%2520and%2520text%252C%2520we%2520evaluate%2520whether%2520%25281%2529%2520blending%2520structuredness%2520within%2520text%252C%2520such%2520as%2520providing%2520templates%2520or%2520fixing%2520the%2520order%2520of%2520attributes%252C%2520and%2520%25282%2529%2520other%2520representative%2520structures%252C%2520such%2520as%2520knowledge%2520graphs%2520and%2520JSON%2520are%2520helpful.%2520We%2520observe%2520that%2520utilizing%2520tables%2520offers%2520the%2520best%2520balance%2520between%2520efficiency%2520and%2520effectiveness.%2520The%2520method%2520remains%2520robust%2520to%2520task%2520complexity%2520and%2520adapts%2520to%2520unstructured%2520sources%2520through%2520text-to-table%2520conversion.%2520Overall%252C%2520we%2520highlight%2520the%2520untapped%2520potential%2520of%2520tabular%2520representations%2520for%2520future%2520LLM%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17189v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Talking%20with%20Tables%20for%20Better%20LLM%20Factual%20Data%20Interactions&entry.906535625=Jio%20Oh%20and%20Geon%20Heo%20and%20Seungjun%20Oh%20and%20Hyunjin%20Kim%20and%20JinYeong%20Bak%20and%20Jindong%20Wang%20and%20Xing%20Xie%20and%20Steven%20Euijong%20Whang&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20often%20struggle%20with%20requests%20related%20to%20information%20retrieval%20and%20data%20manipulation%20that%20frequently%20arise%20in%20real-world%20scenarios%20under%20multiple%20conditions.%20In%20this%20paper%2C%20we%20demonstrate%20that%20leveraging%20tabular%20structures%20in%20LLM%20interactions%2C%20is%20more%20effective%20than%20utilizing%20other%20structures%20for%20handling%20prevalent%20requests%20that%20operate%20over%20factual%20data.%20Through%20comprehensive%20evaluations%20across%20various%20scenarios%20and%20request%20types%2C%20we%20show%20that%20providing%20tabular%20structures%20yields%20a%2040.29%5C%25%20average%20performance%20gain%20along%20with%20better%20robustness%20and%20token%20efficiency.%20Through%20attention-value%20analysis%2C%20we%20discover%20that%20tables%20help%20LLMs%20better%20locate%20relevant%20information%2C%20explaining%20these%20improvements.%20Beyond%20tables%20and%20text%2C%20we%20evaluate%20whether%20%281%29%20blending%20structuredness%20within%20text%2C%20such%20as%20providing%20templates%20or%20fixing%20the%20order%20of%20attributes%2C%20and%20%282%29%20other%20representative%20structures%2C%20such%20as%20knowledge%20graphs%20and%20JSON%20are%20helpful.%20We%20observe%20that%20utilizing%20tables%20offers%20the%20best%20balance%20between%20efficiency%20and%20effectiveness.%20The%20method%20remains%20robust%20to%20task%20complexity%20and%20adapts%20to%20unstructured%20sources%20through%20text-to-table%20conversion.%20Overall%2C%20we%20highlight%20the%20untapped%20potential%20of%20tabular%20representations%20for%20future%20LLM%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2412.17189v4&entry.124074799=Read"},
{"title": "ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG", "author": "Jianbo Li and Yi Jiang and Sendong Zhao and Bairui Hu and Haochun Wang and Bing Qin", "abstract": "Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.", "link": "http://arxiv.org/abs/2601.05038v1", "date": "2026-01-08", "relevancy": 2.3319, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArcAligner%3A%20Adaptive%20Recursive%20Aligner%20for%20Compressed%20Context%20Embeddings%20in%20RAG&body=Title%3A%20ArcAligner%3A%20Adaptive%20Recursive%20Aligner%20for%20Compressed%20Context%20Embeddings%20in%20RAG%0AAuthor%3A%20Jianbo%20Li%20and%20Yi%20Jiang%20and%20Sendong%20Zhao%20and%20Bairui%20Hu%20and%20Haochun%20Wang%20and%20Bing%20Qin%0AAbstract%3A%20Retrieval-Augmented%20Generation%20%28RAG%29%20helps%20LLMs%20stay%20accurate%2C%20but%20feeding%20long%20documents%20into%20a%20prompt%20makes%20the%20model%20slow%20and%20expensive.%20This%20has%20motivated%20context%20compression%2C%20ranging%20from%20token%20pruning%20and%20summarization%20to%20embedding-based%20compression.%20While%20researchers%20have%20tried%20%27%27compressing%27%27%20these%20documents%20into%20smaller%20summaries%20or%20mathematical%20embeddings%2C%20there%20is%20a%20catch%3A%20the%20more%20you%20compress%20the%20data%2C%20the%20more%20the%20LLM%20struggles%20to%20understand%20it.%20To%20address%20this%20challenge%2C%20we%20propose%20ArcAligner%20%28Adaptive%20recursive%20context%20%2AAligner%2A%29%2C%20a%20lightweight%20module%20integrated%20into%20the%20language%20model%20layers%20to%20help%20the%20model%20better%20utilize%20highly%20compressed%20context%20representations%20for%20downstream%20generation.%20It%20uses%20an%20adaptive%20%27%27gating%27%27%20system%20that%20only%20adds%20extra%20processing%20power%20when%20the%20information%20is%20complex%2C%20keeping%20the%20system%20fast.%20Across%20knowledge-intensive%20QA%20benchmarks%2C%20ArcAligner%20consistently%20beats%20compression%20baselines%20at%20comparable%20compression%20rates%2C%20especially%20on%20multi-hop%20and%20long-tail%20settings.%20The%20source%20code%20is%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArcAligner%253A%2520Adaptive%2520Recursive%2520Aligner%2520for%2520Compressed%2520Context%2520Embeddings%2520in%2520RAG%26entry.906535625%3DJianbo%2520Li%2520and%2520Yi%2520Jiang%2520and%2520Sendong%2520Zhao%2520and%2520Bairui%2520Hu%2520and%2520Haochun%2520Wang%2520and%2520Bing%2520Qin%26entry.1292438233%3DRetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520helps%2520LLMs%2520stay%2520accurate%252C%2520but%2520feeding%2520long%2520documents%2520into%2520a%2520prompt%2520makes%2520the%2520model%2520slow%2520and%2520expensive.%2520This%2520has%2520motivated%2520context%2520compression%252C%2520ranging%2520from%2520token%2520pruning%2520and%2520summarization%2520to%2520embedding-based%2520compression.%2520While%2520researchers%2520have%2520tried%2520%2527%2527compressing%2527%2527%2520these%2520documents%2520into%2520smaller%2520summaries%2520or%2520mathematical%2520embeddings%252C%2520there%2520is%2520a%2520catch%253A%2520the%2520more%2520you%2520compress%2520the%2520data%252C%2520the%2520more%2520the%2520LLM%2520struggles%2520to%2520understand%2520it.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520ArcAligner%2520%2528Adaptive%2520recursive%2520context%2520%252AAligner%252A%2529%252C%2520a%2520lightweight%2520module%2520integrated%2520into%2520the%2520language%2520model%2520layers%2520to%2520help%2520the%2520model%2520better%2520utilize%2520highly%2520compressed%2520context%2520representations%2520for%2520downstream%2520generation.%2520It%2520uses%2520an%2520adaptive%2520%2527%2527gating%2527%2527%2520system%2520that%2520only%2520adds%2520extra%2520processing%2520power%2520when%2520the%2520information%2520is%2520complex%252C%2520keeping%2520the%2520system%2520fast.%2520Across%2520knowledge-intensive%2520QA%2520benchmarks%252C%2520ArcAligner%2520consistently%2520beats%2520compression%2520baselines%2520at%2520comparable%2520compression%2520rates%252C%2520especially%2520on%2520multi-hop%2520and%2520long-tail%2520settings.%2520The%2520source%2520code%2520is%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArcAligner%3A%20Adaptive%20Recursive%20Aligner%20for%20Compressed%20Context%20Embeddings%20in%20RAG&entry.906535625=Jianbo%20Li%20and%20Yi%20Jiang%20and%20Sendong%20Zhao%20and%20Bairui%20Hu%20and%20Haochun%20Wang%20and%20Bing%20Qin&entry.1292438233=Retrieval-Augmented%20Generation%20%28RAG%29%20helps%20LLMs%20stay%20accurate%2C%20but%20feeding%20long%20documents%20into%20a%20prompt%20makes%20the%20model%20slow%20and%20expensive.%20This%20has%20motivated%20context%20compression%2C%20ranging%20from%20token%20pruning%20and%20summarization%20to%20embedding-based%20compression.%20While%20researchers%20have%20tried%20%27%27compressing%27%27%20these%20documents%20into%20smaller%20summaries%20or%20mathematical%20embeddings%2C%20there%20is%20a%20catch%3A%20the%20more%20you%20compress%20the%20data%2C%20the%20more%20the%20LLM%20struggles%20to%20understand%20it.%20To%20address%20this%20challenge%2C%20we%20propose%20ArcAligner%20%28Adaptive%20recursive%20context%20%2AAligner%2A%29%2C%20a%20lightweight%20module%20integrated%20into%20the%20language%20model%20layers%20to%20help%20the%20model%20better%20utilize%20highly%20compressed%20context%20representations%20for%20downstream%20generation.%20It%20uses%20an%20adaptive%20%27%27gating%27%27%20system%20that%20only%20adds%20extra%20processing%20power%20when%20the%20information%20is%20complex%2C%20keeping%20the%20system%20fast.%20Across%20knowledge-intensive%20QA%20benchmarks%2C%20ArcAligner%20consistently%20beats%20compression%20baselines%20at%20comparable%20compression%20rates%2C%20especially%20on%20multi-hop%20and%20long-tail%20settings.%20The%20source%20code%20is%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.05038v1&entry.124074799=Read"},
{"title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision", "author": "Ruiyan Han and Zhen Fang and XinYu Sun and Yuchen Ma and Ziheng Wang and Yu Zeng and Zehui Chen and Lin Chen and Wenxuan Huang and Wei-Jie Xu and Yi Cao and Feng Zhao", "abstract": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.", "link": "http://arxiv.org/abs/2601.03193v2", "date": "2026-01-08", "relevancy": 2.3058, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5899}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.569}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniCorn%3A%20Towards%20Self-Improving%20Unified%20Multimodal%20Models%20through%20Self-Generated%20Supervision&body=Title%3A%20UniCorn%3A%20Towards%20Self-Improving%20Unified%20Multimodal%20Models%20through%20Self-Generated%20Supervision%0AAuthor%3A%20Ruiyan%20Han%20and%20Zhen%20Fang%20and%20XinYu%20Sun%20and%20Yuchen%20Ma%20and%20Ziheng%20Wang%20and%20Yu%20Zeng%20and%20Zehui%20Chen%20and%20Lin%20Chen%20and%20Wenxuan%20Huang%20and%20Wei-Jie%20Xu%20and%20Yi%20Cao%20and%20Feng%20Zhao%0AAbstract%3A%20While%20Unified%20Multimodal%20Models%20%28UMMs%29%20have%20achieved%20remarkable%20success%20in%20cross-modal%20comprehension%2C%20a%20significant%20gap%20persists%20in%20their%20ability%20to%20leverage%20such%20internal%20knowledge%20for%20high-quality%20generation.%20We%20formalize%20this%20discrepancy%20as%20Conduction%20Aphasia%2C%20a%20phenomenon%20where%20models%20accurately%20interpret%20multimodal%20inputs%20but%20struggle%20to%20translate%20that%20understanding%20into%20faithful%20and%20controllable%20synthesis.%20To%20address%20this%2C%20we%20propose%20UniCorn%2C%20a%20simple%20yet%20elegant%20self-improvement%20framework%20that%20eliminates%20the%20need%20for%20external%20data%20or%20teacher%20supervision.%20By%20partitioning%20a%20single%20UMM%20into%20three%20collaborative%20roles%3A%20Proposer%2C%20Solver%2C%20and%20Judge%2C%20UniCorn%20generates%20high-quality%20interactions%20via%20self-play%20and%20employs%20cognitive%20pattern%20reconstruction%20to%20distill%20latent%20understanding%20into%20explicit%20generative%20signals.%20To%20validate%20the%20restoration%20of%20multimodal%20coherence%2C%20we%20introduce%20UniCycle%2C%20a%20cycle-consistency%20benchmark%20based%20on%20a%20Text%20to%20Image%20to%20Text%20reconstruction%20loop.%20Extensive%20experiments%20demonstrate%20that%20UniCorn%20achieves%20comprehensive%20and%20substantial%20improvements%20over%20the%20base%20model%20across%20six%20general%20image%20generation%20benchmarks.%20Notably%2C%20it%20achieves%20SOTA%20performance%20on%20TIIF%2873.8%29%2C%20DPG%2886.8%29%2C%20CompBench%2888.5%29%2C%20and%20UniCycle%20while%20further%20delivering%20substantial%20gains%20of%20%2B5.0%20on%20WISE%20and%20%2B6.5%20on%20OneIG.%20These%20results%20highlight%20that%20our%20method%20significantly%20enhances%20T2I%20generation%20while%20maintaining%20robust%20comprehension%2C%20demonstrating%20the%20scalability%20of%20fully%20self-supervised%20refinement%20for%20unified%20multimodal%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03193v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniCorn%253A%2520Towards%2520Self-Improving%2520Unified%2520Multimodal%2520Models%2520through%2520Self-Generated%2520Supervision%26entry.906535625%3DRuiyan%2520Han%2520and%2520Zhen%2520Fang%2520and%2520XinYu%2520Sun%2520and%2520Yuchen%2520Ma%2520and%2520Ziheng%2520Wang%2520and%2520Yu%2520Zeng%2520and%2520Zehui%2520Chen%2520and%2520Lin%2520Chen%2520and%2520Wenxuan%2520Huang%2520and%2520Wei-Jie%2520Xu%2520and%2520Yi%2520Cao%2520and%2520Feng%2520Zhao%26entry.1292438233%3DWhile%2520Unified%2520Multimodal%2520Models%2520%2528UMMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520cross-modal%2520comprehension%252C%2520a%2520significant%2520gap%2520persists%2520in%2520their%2520ability%2520to%2520leverage%2520such%2520internal%2520knowledge%2520for%2520high-quality%2520generation.%2520We%2520formalize%2520this%2520discrepancy%2520as%2520Conduction%2520Aphasia%252C%2520a%2520phenomenon%2520where%2520models%2520accurately%2520interpret%2520multimodal%2520inputs%2520but%2520struggle%2520to%2520translate%2520that%2520understanding%2520into%2520faithful%2520and%2520controllable%2520synthesis.%2520To%2520address%2520this%252C%2520we%2520propose%2520UniCorn%252C%2520a%2520simple%2520yet%2520elegant%2520self-improvement%2520framework%2520that%2520eliminates%2520the%2520need%2520for%2520external%2520data%2520or%2520teacher%2520supervision.%2520By%2520partitioning%2520a%2520single%2520UMM%2520into%2520three%2520collaborative%2520roles%253A%2520Proposer%252C%2520Solver%252C%2520and%2520Judge%252C%2520UniCorn%2520generates%2520high-quality%2520interactions%2520via%2520self-play%2520and%2520employs%2520cognitive%2520pattern%2520reconstruction%2520to%2520distill%2520latent%2520understanding%2520into%2520explicit%2520generative%2520signals.%2520To%2520validate%2520the%2520restoration%2520of%2520multimodal%2520coherence%252C%2520we%2520introduce%2520UniCycle%252C%2520a%2520cycle-consistency%2520benchmark%2520based%2520on%2520a%2520Text%2520to%2520Image%2520to%2520Text%2520reconstruction%2520loop.%2520Extensive%2520experiments%2520demonstrate%2520that%2520UniCorn%2520achieves%2520comprehensive%2520and%2520substantial%2520improvements%2520over%2520the%2520base%2520model%2520across%2520six%2520general%2520image%2520generation%2520benchmarks.%2520Notably%252C%2520it%2520achieves%2520SOTA%2520performance%2520on%2520TIIF%252873.8%2529%252C%2520DPG%252886.8%2529%252C%2520CompBench%252888.5%2529%252C%2520and%2520UniCycle%2520while%2520further%2520delivering%2520substantial%2520gains%2520of%2520%252B5.0%2520on%2520WISE%2520and%2520%252B6.5%2520on%2520OneIG.%2520These%2520results%2520highlight%2520that%2520our%2520method%2520significantly%2520enhances%2520T2I%2520generation%2520while%2520maintaining%2520robust%2520comprehension%252C%2520demonstrating%2520the%2520scalability%2520of%2520fully%2520self-supervised%2520refinement%2520for%2520unified%2520multimodal%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03193v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniCorn%3A%20Towards%20Self-Improving%20Unified%20Multimodal%20Models%20through%20Self-Generated%20Supervision&entry.906535625=Ruiyan%20Han%20and%20Zhen%20Fang%20and%20XinYu%20Sun%20and%20Yuchen%20Ma%20and%20Ziheng%20Wang%20and%20Yu%20Zeng%20and%20Zehui%20Chen%20and%20Lin%20Chen%20and%20Wenxuan%20Huang%20and%20Wei-Jie%20Xu%20and%20Yi%20Cao%20and%20Feng%20Zhao&entry.1292438233=While%20Unified%20Multimodal%20Models%20%28UMMs%29%20have%20achieved%20remarkable%20success%20in%20cross-modal%20comprehension%2C%20a%20significant%20gap%20persists%20in%20their%20ability%20to%20leverage%20such%20internal%20knowledge%20for%20high-quality%20generation.%20We%20formalize%20this%20discrepancy%20as%20Conduction%20Aphasia%2C%20a%20phenomenon%20where%20models%20accurately%20interpret%20multimodal%20inputs%20but%20struggle%20to%20translate%20that%20understanding%20into%20faithful%20and%20controllable%20synthesis.%20To%20address%20this%2C%20we%20propose%20UniCorn%2C%20a%20simple%20yet%20elegant%20self-improvement%20framework%20that%20eliminates%20the%20need%20for%20external%20data%20or%20teacher%20supervision.%20By%20partitioning%20a%20single%20UMM%20into%20three%20collaborative%20roles%3A%20Proposer%2C%20Solver%2C%20and%20Judge%2C%20UniCorn%20generates%20high-quality%20interactions%20via%20self-play%20and%20employs%20cognitive%20pattern%20reconstruction%20to%20distill%20latent%20understanding%20into%20explicit%20generative%20signals.%20To%20validate%20the%20restoration%20of%20multimodal%20coherence%2C%20we%20introduce%20UniCycle%2C%20a%20cycle-consistency%20benchmark%20based%20on%20a%20Text%20to%20Image%20to%20Text%20reconstruction%20loop.%20Extensive%20experiments%20demonstrate%20that%20UniCorn%20achieves%20comprehensive%20and%20substantial%20improvements%20over%20the%20base%20model%20across%20six%20general%20image%20generation%20benchmarks.%20Notably%2C%20it%20achieves%20SOTA%20performance%20on%20TIIF%2873.8%29%2C%20DPG%2886.8%29%2C%20CompBench%2888.5%29%2C%20and%20UniCycle%20while%20further%20delivering%20substantial%20gains%20of%20%2B5.0%20on%20WISE%20and%20%2B6.5%20on%20OneIG.%20These%20results%20highlight%20that%20our%20method%20significantly%20enhances%20T2I%20generation%20while%20maintaining%20robust%20comprehension%2C%20demonstrating%20the%20scalability%20of%20fully%20self-supervised%20refinement%20for%20unified%20multimodal%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2601.03193v2&entry.124074799=Read"},
{"title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning", "author": "Kajetan Dymkiewicz and Ivan Vulic and Helen Yannakoudakis and Eilam Shapira and Roi Reichart and Anna Korhonen", "abstract": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages remains poorly understood. We conduct a controlled LoRA fine-tuning study across multiple open-weight LLM families and scales, using a standardised grid of 11 languages and four benchmarks. We fine-tune each model on a single task-language source and measure transfer when evaluated on all other task-language target pairs. We decompose transfer into three regimes: (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language). Single-source fine-tuning yields a net positive uplift across regimes, but the gains are strongly asymmetric. Matched-Task (Cross-Language) transfer emerges as the most effective and predictable regime, driven principally by the identity of the target language rather than model architecture. We identify a stable hierarchy where high-resource languages and broad semantic tasks act as efficient recipients that absorb gains from diverse sources, while specialised tasks and lower-resource languages are more isolated. These results imply that effective fine-tuning requires navigating donor-recipient roles to maximise downstream gains.", "link": "http://arxiv.org/abs/2511.13368v2", "date": "2026-01-08", "relevancy": 2.3012, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4635}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Donors%20and%20Recipients%3A%20On%20Asymmetric%20Transfer%20Across%20Tasks%20and%20Languages%20with%20Parameter-Efficient%20Fine-Tuning&body=Title%3A%20Donors%20and%20Recipients%3A%20On%20Asymmetric%20Transfer%20Across%20Tasks%20and%20Languages%20with%20Parameter-Efficient%20Fine-Tuning%0AAuthor%3A%20Kajetan%20Dymkiewicz%20and%20Ivan%20Vulic%20and%20Helen%20Yannakoudakis%20and%20Eilam%20Shapira%20and%20Roi%20Reichart%20and%20Anna%20Korhonen%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20perform%20strongly%20across%20tasks%20and%20languages%2C%20yet%20how%20improvements%20in%20one%20task%20or%20language%20affect%20other%20tasks%20and%20languages%20remains%20poorly%20understood.%20We%20conduct%20a%20controlled%20LoRA%20fine-tuning%20study%20across%20multiple%20open-weight%20LLM%20families%20and%20scales%2C%20using%20a%20standardised%20grid%20of%2011%20languages%20and%20four%20benchmarks.%20We%20fine-tune%20each%20model%20on%20a%20single%20task-language%20source%20and%20measure%20transfer%20when%20evaluated%20on%20all%20other%20task-language%20target%20pairs.%20We%20decompose%20transfer%20into%20three%20regimes%3A%20%28i%29%20Matched-Task%20%28Cross-Language%29%2C%20%28ii%29%20Matched-Language%20%28Cross-Task%29%2C%20and%20%28iii%29%20Cross-Task%20%28Cross-Language%29.%20Single-source%20fine-tuning%20yields%20a%20net%20positive%20uplift%20across%20regimes%2C%20but%20the%20gains%20are%20strongly%20asymmetric.%20Matched-Task%20%28Cross-Language%29%20transfer%20emerges%20as%20the%20most%20effective%20and%20predictable%20regime%2C%20driven%20principally%20by%20the%20identity%20of%20the%20target%20language%20rather%20than%20model%20architecture.%20We%20identify%20a%20stable%20hierarchy%20where%20high-resource%20languages%20and%20broad%20semantic%20tasks%20act%20as%20efficient%20recipients%20that%20absorb%20gains%20from%20diverse%20sources%2C%20while%20specialised%20tasks%20and%20lower-resource%20languages%20are%20more%20isolated.%20These%20results%20imply%20that%20effective%20fine-tuning%20requires%20navigating%20donor-recipient%20roles%20to%20maximise%20downstream%20gains.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13368v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDonors%2520and%2520Recipients%253A%2520On%2520Asymmetric%2520Transfer%2520Across%2520Tasks%2520and%2520Languages%2520with%2520Parameter-Efficient%2520Fine-Tuning%26entry.906535625%3DKajetan%2520Dymkiewicz%2520and%2520Ivan%2520Vulic%2520and%2520Helen%2520Yannakoudakis%2520and%2520Eilam%2520Shapira%2520and%2520Roi%2520Reichart%2520and%2520Anna%2520Korhonen%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520perform%2520strongly%2520across%2520tasks%2520and%2520languages%252C%2520yet%2520how%2520improvements%2520in%2520one%2520task%2520or%2520language%2520affect%2520other%2520tasks%2520and%2520languages%2520remains%2520poorly%2520understood.%2520We%2520conduct%2520a%2520controlled%2520LoRA%2520fine-tuning%2520study%2520across%2520multiple%2520open-weight%2520LLM%2520families%2520and%2520scales%252C%2520using%2520a%2520standardised%2520grid%2520of%252011%2520languages%2520and%2520four%2520benchmarks.%2520We%2520fine-tune%2520each%2520model%2520on%2520a%2520single%2520task-language%2520source%2520and%2520measure%2520transfer%2520when%2520evaluated%2520on%2520all%2520other%2520task-language%2520target%2520pairs.%2520We%2520decompose%2520transfer%2520into%2520three%2520regimes%253A%2520%2528i%2529%2520Matched-Task%2520%2528Cross-Language%2529%252C%2520%2528ii%2529%2520Matched-Language%2520%2528Cross-Task%2529%252C%2520and%2520%2528iii%2529%2520Cross-Task%2520%2528Cross-Language%2529.%2520Single-source%2520fine-tuning%2520yields%2520a%2520net%2520positive%2520uplift%2520across%2520regimes%252C%2520but%2520the%2520gains%2520are%2520strongly%2520asymmetric.%2520Matched-Task%2520%2528Cross-Language%2529%2520transfer%2520emerges%2520as%2520the%2520most%2520effective%2520and%2520predictable%2520regime%252C%2520driven%2520principally%2520by%2520the%2520identity%2520of%2520the%2520target%2520language%2520rather%2520than%2520model%2520architecture.%2520We%2520identify%2520a%2520stable%2520hierarchy%2520where%2520high-resource%2520languages%2520and%2520broad%2520semantic%2520tasks%2520act%2520as%2520efficient%2520recipients%2520that%2520absorb%2520gains%2520from%2520diverse%2520sources%252C%2520while%2520specialised%2520tasks%2520and%2520lower-resource%2520languages%2520are%2520more%2520isolated.%2520These%2520results%2520imply%2520that%2520effective%2520fine-tuning%2520requires%2520navigating%2520donor-recipient%2520roles%2520to%2520maximise%2520downstream%2520gains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13368v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Donors%20and%20Recipients%3A%20On%20Asymmetric%20Transfer%20Across%20Tasks%20and%20Languages%20with%20Parameter-Efficient%20Fine-Tuning&entry.906535625=Kajetan%20Dymkiewicz%20and%20Ivan%20Vulic%20and%20Helen%20Yannakoudakis%20and%20Eilam%20Shapira%20and%20Roi%20Reichart%20and%20Anna%20Korhonen&entry.1292438233=Large%20language%20models%20%28LLMs%29%20perform%20strongly%20across%20tasks%20and%20languages%2C%20yet%20how%20improvements%20in%20one%20task%20or%20language%20affect%20other%20tasks%20and%20languages%20remains%20poorly%20understood.%20We%20conduct%20a%20controlled%20LoRA%20fine-tuning%20study%20across%20multiple%20open-weight%20LLM%20families%20and%20scales%2C%20using%20a%20standardised%20grid%20of%2011%20languages%20and%20four%20benchmarks.%20We%20fine-tune%20each%20model%20on%20a%20single%20task-language%20source%20and%20measure%20transfer%20when%20evaluated%20on%20all%20other%20task-language%20target%20pairs.%20We%20decompose%20transfer%20into%20three%20regimes%3A%20%28i%29%20Matched-Task%20%28Cross-Language%29%2C%20%28ii%29%20Matched-Language%20%28Cross-Task%29%2C%20and%20%28iii%29%20Cross-Task%20%28Cross-Language%29.%20Single-source%20fine-tuning%20yields%20a%20net%20positive%20uplift%20across%20regimes%2C%20but%20the%20gains%20are%20strongly%20asymmetric.%20Matched-Task%20%28Cross-Language%29%20transfer%20emerges%20as%20the%20most%20effective%20and%20predictable%20regime%2C%20driven%20principally%20by%20the%20identity%20of%20the%20target%20language%20rather%20than%20model%20architecture.%20We%20identify%20a%20stable%20hierarchy%20where%20high-resource%20languages%20and%20broad%20semantic%20tasks%20act%20as%20efficient%20recipients%20that%20absorb%20gains%20from%20diverse%20sources%2C%20while%20specialised%20tasks%20and%20lower-resource%20languages%20are%20more%20isolated.%20These%20results%20imply%20that%20effective%20fine-tuning%20requires%20navigating%20donor-recipient%20roles%20to%20maximise%20downstream%20gains.&entry.1838667208=http%3A//arxiv.org/abs/2511.13368v2&entry.124074799=Read"},
{"title": "Reward Shaping to Mitigate Reward Hacking in RLHF", "author": "Jiayi Fu and Xuandong Zhao and Chengyuan Yao and Heng Wang and Qi Han and Yanghua Xiao", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \\emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. Moreover, PAR exhibits two critical variance-reduction properties that contribute to stabilizing the RLHF training process and effectively extending the tolerance window for early stopping. We evaluated PAR on the base model Gemma2-2B using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR.", "link": "http://arxiv.org/abs/2502.18770v4", "date": "2026-01-08", "relevancy": 2.2876, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4637}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4563}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward%20Shaping%20to%20Mitigate%20Reward%20Hacking%20in%20RLHF&body=Title%3A%20Reward%20Shaping%20to%20Mitigate%20Reward%20Hacking%20in%20RLHF%0AAuthor%3A%20Jiayi%20Fu%20and%20Xuandong%20Zhao%20and%20Chengyuan%20Yao%20and%20Heng%20Wang%20and%20Qi%20Han%20and%20Yanghua%20Xiao%0AAbstract%3A%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20essential%20for%20aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20values.%20However%2C%20RLHF%20is%20susceptible%20to%20%5Cemph%7Breward%20hacking%7D%2C%20where%20the%20agent%20exploits%20flaws%20in%20the%20reward%20function%20rather%20than%20learning%20the%20intended%20behavior%2C%20thus%20degrading%20alignment.%20Although%20reward%20shaping%20helps%20stabilize%20RLHF%20and%20partially%20mitigate%20reward%20hacking%2C%20a%20systematic%20investigation%20into%20shaping%20techniques%20and%20their%20underlying%20principles%20remains%20lacking.%20To%20bridge%20this%20gap%2C%20we%20present%20a%20comprehensive%20study%20of%20the%20prevalent%20reward%20shaping%20methods.%20Our%20analysis%20suggests%20two%20key%20design%20principles%3A%20%281%29%20the%20RL%20reward%20should%20be%20bounded%2C%20and%20%282%29%20the%20RL%20reward%20benefits%20from%20rapid%20initial%20growth%20followed%20by%20gradual%20convergence.%20Guided%20by%20these%20insights%2C%20we%20propose%20Preference%20As%20Reward%20%28PAR%29%2C%20a%20novel%20approach%20that%20leverages%20the%20latent%20preferences%20embedded%20within%20the%20reward%20model%20as%20the%20signal%20for%20reinforcement%20learning.%20Moreover%2C%20PAR%20exhibits%20two%20critical%20variance-reduction%20properties%20that%20contribute%20to%20stabilizing%20the%20RLHF%20training%20process%20and%20effectively%20extending%20the%20tolerance%20window%20for%20early%20stopping.%20We%20evaluated%20PAR%20on%20the%20base%20model%20Gemma2-2B%20using%20two%20datasets%2C%20Ultrafeedback-Binarized%20and%20HH-RLHF.%20Experimental%20results%20demonstrate%20PAR%27s%20superior%20performance%20over%20other%20reward%20shaping%20methods.%20On%20the%20AlpacaEval%202.0%20benchmark%2C%20PAR%20achieves%20a%20win%20rate%20of%20at%20least%205%20percentage%20points%20higher%20than%20competing%20approaches.%20Furthermore%2C%20PAR%20exhibits%20remarkable%20data%20efficiency%2C%20requiring%20only%20a%20single%20reference%20reward%20for%20optimal%20performance%2C%20and%20maintains%20robustness%20against%20reward%20hacking%20even%20after%20two%20full%20epochs%20of%20training.%20The%20code%20is%20available%20at%20https%3A//github.com/PorUna-byte/PAR.%0ALink%3A%20http%3A//arxiv.org/abs/2502.18770v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward%2520Shaping%2520to%2520Mitigate%2520Reward%2520Hacking%2520in%2520RLHF%26entry.906535625%3DJiayi%2520Fu%2520and%2520Xuandong%2520Zhao%2520and%2520Chengyuan%2520Yao%2520and%2520Heng%2520Wang%2520and%2520Qi%2520Han%2520and%2520Yanghua%2520Xiao%26entry.1292438233%3DReinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520is%2520essential%2520for%2520aligning%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%2520values.%2520However%252C%2520RLHF%2520is%2520susceptible%2520to%2520%255Cemph%257Breward%2520hacking%257D%252C%2520where%2520the%2520agent%2520exploits%2520flaws%2520in%2520the%2520reward%2520function%2520rather%2520than%2520learning%2520the%2520intended%2520behavior%252C%2520thus%2520degrading%2520alignment.%2520Although%2520reward%2520shaping%2520helps%2520stabilize%2520RLHF%2520and%2520partially%2520mitigate%2520reward%2520hacking%252C%2520a%2520systematic%2520investigation%2520into%2520shaping%2520techniques%2520and%2520their%2520underlying%2520principles%2520remains%2520lacking.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520a%2520comprehensive%2520study%2520of%2520the%2520prevalent%2520reward%2520shaping%2520methods.%2520Our%2520analysis%2520suggests%2520two%2520key%2520design%2520principles%253A%2520%25281%2529%2520the%2520RL%2520reward%2520should%2520be%2520bounded%252C%2520and%2520%25282%2529%2520the%2520RL%2520reward%2520benefits%2520from%2520rapid%2520initial%2520growth%2520followed%2520by%2520gradual%2520convergence.%2520Guided%2520by%2520these%2520insights%252C%2520we%2520propose%2520Preference%2520As%2520Reward%2520%2528PAR%2529%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520the%2520latent%2520preferences%2520embedded%2520within%2520the%2520reward%2520model%2520as%2520the%2520signal%2520for%2520reinforcement%2520learning.%2520Moreover%252C%2520PAR%2520exhibits%2520two%2520critical%2520variance-reduction%2520properties%2520that%2520contribute%2520to%2520stabilizing%2520the%2520RLHF%2520training%2520process%2520and%2520effectively%2520extending%2520the%2520tolerance%2520window%2520for%2520early%2520stopping.%2520We%2520evaluated%2520PAR%2520on%2520the%2520base%2520model%2520Gemma2-2B%2520using%2520two%2520datasets%252C%2520Ultrafeedback-Binarized%2520and%2520HH-RLHF.%2520Experimental%2520results%2520demonstrate%2520PAR%2527s%2520superior%2520performance%2520over%2520other%2520reward%2520shaping%2520methods.%2520On%2520the%2520AlpacaEval%25202.0%2520benchmark%252C%2520PAR%2520achieves%2520a%2520win%2520rate%2520of%2520at%2520least%25205%2520percentage%2520points%2520higher%2520than%2520competing%2520approaches.%2520Furthermore%252C%2520PAR%2520exhibits%2520remarkable%2520data%2520efficiency%252C%2520requiring%2520only%2520a%2520single%2520reference%2520reward%2520for%2520optimal%2520performance%252C%2520and%2520maintains%2520robustness%2520against%2520reward%2520hacking%2520even%2520after%2520two%2520full%2520epochs%2520of%2520training.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/PorUna-byte/PAR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18770v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward%20Shaping%20to%20Mitigate%20Reward%20Hacking%20in%20RLHF&entry.906535625=Jiayi%20Fu%20and%20Xuandong%20Zhao%20and%20Chengyuan%20Yao%20and%20Heng%20Wang%20and%20Qi%20Han%20and%20Yanghua%20Xiao&entry.1292438233=Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20essential%20for%20aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20values.%20However%2C%20RLHF%20is%20susceptible%20to%20%5Cemph%7Breward%20hacking%7D%2C%20where%20the%20agent%20exploits%20flaws%20in%20the%20reward%20function%20rather%20than%20learning%20the%20intended%20behavior%2C%20thus%20degrading%20alignment.%20Although%20reward%20shaping%20helps%20stabilize%20RLHF%20and%20partially%20mitigate%20reward%20hacking%2C%20a%20systematic%20investigation%20into%20shaping%20techniques%20and%20their%20underlying%20principles%20remains%20lacking.%20To%20bridge%20this%20gap%2C%20we%20present%20a%20comprehensive%20study%20of%20the%20prevalent%20reward%20shaping%20methods.%20Our%20analysis%20suggests%20two%20key%20design%20principles%3A%20%281%29%20the%20RL%20reward%20should%20be%20bounded%2C%20and%20%282%29%20the%20RL%20reward%20benefits%20from%20rapid%20initial%20growth%20followed%20by%20gradual%20convergence.%20Guided%20by%20these%20insights%2C%20we%20propose%20Preference%20As%20Reward%20%28PAR%29%2C%20a%20novel%20approach%20that%20leverages%20the%20latent%20preferences%20embedded%20within%20the%20reward%20model%20as%20the%20signal%20for%20reinforcement%20learning.%20Moreover%2C%20PAR%20exhibits%20two%20critical%20variance-reduction%20properties%20that%20contribute%20to%20stabilizing%20the%20RLHF%20training%20process%20and%20effectively%20extending%20the%20tolerance%20window%20for%20early%20stopping.%20We%20evaluated%20PAR%20on%20the%20base%20model%20Gemma2-2B%20using%20two%20datasets%2C%20Ultrafeedback-Binarized%20and%20HH-RLHF.%20Experimental%20results%20demonstrate%20PAR%27s%20superior%20performance%20over%20other%20reward%20shaping%20methods.%20On%20the%20AlpacaEval%202.0%20benchmark%2C%20PAR%20achieves%20a%20win%20rate%20of%20at%20least%205%20percentage%20points%20higher%20than%20competing%20approaches.%20Furthermore%2C%20PAR%20exhibits%20remarkable%20data%20efficiency%2C%20requiring%20only%20a%20single%20reference%20reward%20for%20optimal%20performance%2C%20and%20maintains%20robustness%20against%20reward%20hacking%20even%20after%20two%20full%20epochs%20of%20training.%20The%20code%20is%20available%20at%20https%3A//github.com/PorUna-byte/PAR.&entry.1838667208=http%3A//arxiv.org/abs/2502.18770v4&entry.124074799=Read"},
{"title": "AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding", "author": "Aleksei Kondratenko and Mussie Birhane and Houssame E. Hsain and Guido Maciocci", "abstract": "AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation.", "link": "http://arxiv.org/abs/2601.04819v1", "date": "2026-01-08", "relevancy": 2.2732, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5811}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AECV-Bench%3A%20Benchmarking%20Multimodal%20Models%20on%20Architectural%20and%20Engineering%20Drawings%20Understanding&body=Title%3A%20AECV-Bench%3A%20Benchmarking%20Multimodal%20Models%20on%20Architectural%20and%20Engineering%20Drawings%20Understanding%0AAuthor%3A%20Aleksei%20Kondratenko%20and%20Mussie%20Birhane%20and%20Houssame%20E.%20Hsain%20and%20Guido%20Maciocci%0AAbstract%3A%20AEC%20drawings%20encode%20geometry%20and%20semantics%20through%20symbols%2C%20layout%20conventions%2C%20and%20dense%20annotation%2C%20yet%20it%20remains%20unclear%20whether%20modern%20multimodal%20and%20vision-language%20models%20can%20reliably%20interpret%20this%20graphical%20language.%20We%20present%20AECV-Bench%2C%20a%20benchmark%20for%20evaluating%20multimodal%20and%20vision-language%20models%20on%20realistic%20AEC%20artefacts%20via%20two%20complementary%20use%20cases%3A%20%28i%29%20object%20counting%20on%20120%20high-quality%20floor%20plans%20%28doors%2C%20windows%2C%20bedrooms%2C%20toilets%29%2C%20and%20%28ii%29%20drawing-grounded%20document%20QA%20spanning%20192%20question-answer%20pairs%20that%20test%20text%20extraction%20%28OCR%29%2C%20instance%20counting%2C%20spatial%20reasoning%2C%20and%20comparative%20reasoning%20over%20common%20drawing%20regions.%20Object-counting%20performance%20is%20reported%20using%20per-field%20exact-match%20accuracy%20and%20MAPE%20results%2C%20while%20document-QA%20performance%20is%20reported%20using%20overall%20accuracy%20and%20per-category%20breakdowns%20with%20an%20LLM-as-a-judge%20scoring%20pipeline%20and%20targeted%20human%20adjudication%20for%20edge%20cases.%20Evaluating%20a%20broad%20set%20of%20state-of-the-art%20models%20under%20a%20unified%20protocol%2C%20we%20observe%20a%20stable%20capability%20gradient%3B%20OCR%20and%20text-centric%20document%20QA%20are%20strongest%20%28up%20to%200.95%20accuracy%29%2C%20spatial%20reasoning%20is%20moderate%2C%20and%20symbol-centric%20drawing%20understanding%20-%20especially%20reliable%20counting%20of%20doors%20and%20windows%20-%20remains%20unsolved%20%28often%200.40-0.55%20accuracy%29%20with%20substantial%20proportional%20errors.%20These%20results%20suggest%20that%20current%20systems%20function%20well%20as%20document%20assistants%20but%20lack%20robust%20drawing%20literacy%2C%20motivating%20domain-specific%20representations%20and%20tool-augmented%2C%20human-in-the-loop%20workflows%20for%20an%20efficient%20AEC%20automation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAECV-Bench%253A%2520Benchmarking%2520Multimodal%2520Models%2520on%2520Architectural%2520and%2520Engineering%2520Drawings%2520Understanding%26entry.906535625%3DAleksei%2520Kondratenko%2520and%2520Mussie%2520Birhane%2520and%2520Houssame%2520E.%2520Hsain%2520and%2520Guido%2520Maciocci%26entry.1292438233%3DAEC%2520drawings%2520encode%2520geometry%2520and%2520semantics%2520through%2520symbols%252C%2520layout%2520conventions%252C%2520and%2520dense%2520annotation%252C%2520yet%2520it%2520remains%2520unclear%2520whether%2520modern%2520multimodal%2520and%2520vision-language%2520models%2520can%2520reliably%2520interpret%2520this%2520graphical%2520language.%2520We%2520present%2520AECV-Bench%252C%2520a%2520benchmark%2520for%2520evaluating%2520multimodal%2520and%2520vision-language%2520models%2520on%2520realistic%2520AEC%2520artefacts%2520via%2520two%2520complementary%2520use%2520cases%253A%2520%2528i%2529%2520object%2520counting%2520on%2520120%2520high-quality%2520floor%2520plans%2520%2528doors%252C%2520windows%252C%2520bedrooms%252C%2520toilets%2529%252C%2520and%2520%2528ii%2529%2520drawing-grounded%2520document%2520QA%2520spanning%2520192%2520question-answer%2520pairs%2520that%2520test%2520text%2520extraction%2520%2528OCR%2529%252C%2520instance%2520counting%252C%2520spatial%2520reasoning%252C%2520and%2520comparative%2520reasoning%2520over%2520common%2520drawing%2520regions.%2520Object-counting%2520performance%2520is%2520reported%2520using%2520per-field%2520exact-match%2520accuracy%2520and%2520MAPE%2520results%252C%2520while%2520document-QA%2520performance%2520is%2520reported%2520using%2520overall%2520accuracy%2520and%2520per-category%2520breakdowns%2520with%2520an%2520LLM-as-a-judge%2520scoring%2520pipeline%2520and%2520targeted%2520human%2520adjudication%2520for%2520edge%2520cases.%2520Evaluating%2520a%2520broad%2520set%2520of%2520state-of-the-art%2520models%2520under%2520a%2520unified%2520protocol%252C%2520we%2520observe%2520a%2520stable%2520capability%2520gradient%253B%2520OCR%2520and%2520text-centric%2520document%2520QA%2520are%2520strongest%2520%2528up%2520to%25200.95%2520accuracy%2529%252C%2520spatial%2520reasoning%2520is%2520moderate%252C%2520and%2520symbol-centric%2520drawing%2520understanding%2520-%2520especially%2520reliable%2520counting%2520of%2520doors%2520and%2520windows%2520-%2520remains%2520unsolved%2520%2528often%25200.40-0.55%2520accuracy%2529%2520with%2520substantial%2520proportional%2520errors.%2520These%2520results%2520suggest%2520that%2520current%2520systems%2520function%2520well%2520as%2520document%2520assistants%2520but%2520lack%2520robust%2520drawing%2520literacy%252C%2520motivating%2520domain-specific%2520representations%2520and%2520tool-augmented%252C%2520human-in-the-loop%2520workflows%2520for%2520an%2520efficient%2520AEC%2520automation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AECV-Bench%3A%20Benchmarking%20Multimodal%20Models%20on%20Architectural%20and%20Engineering%20Drawings%20Understanding&entry.906535625=Aleksei%20Kondratenko%20and%20Mussie%20Birhane%20and%20Houssame%20E.%20Hsain%20and%20Guido%20Maciocci&entry.1292438233=AEC%20drawings%20encode%20geometry%20and%20semantics%20through%20symbols%2C%20layout%20conventions%2C%20and%20dense%20annotation%2C%20yet%20it%20remains%20unclear%20whether%20modern%20multimodal%20and%20vision-language%20models%20can%20reliably%20interpret%20this%20graphical%20language.%20We%20present%20AECV-Bench%2C%20a%20benchmark%20for%20evaluating%20multimodal%20and%20vision-language%20models%20on%20realistic%20AEC%20artefacts%20via%20two%20complementary%20use%20cases%3A%20%28i%29%20object%20counting%20on%20120%20high-quality%20floor%20plans%20%28doors%2C%20windows%2C%20bedrooms%2C%20toilets%29%2C%20and%20%28ii%29%20drawing-grounded%20document%20QA%20spanning%20192%20question-answer%20pairs%20that%20test%20text%20extraction%20%28OCR%29%2C%20instance%20counting%2C%20spatial%20reasoning%2C%20and%20comparative%20reasoning%20over%20common%20drawing%20regions.%20Object-counting%20performance%20is%20reported%20using%20per-field%20exact-match%20accuracy%20and%20MAPE%20results%2C%20while%20document-QA%20performance%20is%20reported%20using%20overall%20accuracy%20and%20per-category%20breakdowns%20with%20an%20LLM-as-a-judge%20scoring%20pipeline%20and%20targeted%20human%20adjudication%20for%20edge%20cases.%20Evaluating%20a%20broad%20set%20of%20state-of-the-art%20models%20under%20a%20unified%20protocol%2C%20we%20observe%20a%20stable%20capability%20gradient%3B%20OCR%20and%20text-centric%20document%20QA%20are%20strongest%20%28up%20to%200.95%20accuracy%29%2C%20spatial%20reasoning%20is%20moderate%2C%20and%20symbol-centric%20drawing%20understanding%20-%20especially%20reliable%20counting%20of%20doors%20and%20windows%20-%20remains%20unsolved%20%28often%200.40-0.55%20accuracy%29%20with%20substantial%20proportional%20errors.%20These%20results%20suggest%20that%20current%20systems%20function%20well%20as%20document%20assistants%20but%20lack%20robust%20drawing%20literacy%2C%20motivating%20domain-specific%20representations%20and%20tool-augmented%2C%20human-in-the-loop%20workflows%20for%20an%20efficient%20AEC%20automation.&entry.1838667208=http%3A//arxiv.org/abs/2601.04819v1&entry.124074799=Read"},
{"title": "DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation", "author": "Ayush Pande", "abstract": "Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360\u00b0 and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.", "link": "http://arxiv.org/abs/2601.04860v1", "date": "2026-01-08", "relevancy": 2.2562, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5757}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5591}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DivAS%3A%20Interactive%203D%20Segmentation%20of%20NeRFs%20via%20Depth-Weighted%20Voxel%20Aggregation&body=Title%3A%20DivAS%3A%20Interactive%203D%20Segmentation%20of%20NeRFs%20via%20Depth-Weighted%20Voxel%20Aggregation%0AAuthor%3A%20Ayush%20Pande%0AAbstract%3A%20Existing%20methods%20for%20segmenting%20Neural%20Radiance%20Fields%20%28NeRFs%29%20are%20often%20optimization-based%2C%20requiring%20slow%20per-scene%20training%20that%20sacrifices%20the%20zero-shot%20capabilities%20of%202D%20foundation%20models.%20We%20introduce%20DivAS%20%28Depth-interactive%20Voxel%20Aggregation%20Segmentation%29%2C%20an%20optimization-free%2C%20fully%20interactive%20framework%20that%20addresses%20these%20limitations.%20Our%20method%20operates%20via%20a%20fast%20GUI-based%20workflow%20where%202D%20SAM%20masks%2C%20generated%20from%20user%20point%20prompts%2C%20are%20refined%20using%20NeRF-derived%20depth%20priors%20to%20improve%20geometric%20accuracy%20and%20foreground-background%20separation.%20The%20core%20of%20our%20contribution%20is%20a%20custom%20CUDA%20kernel%20that%20aggregates%20these%20refined%20multi-view%20masks%20into%20a%20unified%203D%20voxel%20grid%20in%20under%20200ms%2C%20enabling%20real-time%20visual%20feedback.%20This%20optimization-free%20design%20eliminates%20the%20need%20for%20per-scene%20training.%20Experiments%20on%20Mip-NeRF%20360%C2%B0%20and%20LLFF%20show%20that%20DivAS%20achieves%20segmentation%20quality%20comparable%20to%20optimization-based%20methods%2C%20while%20being%202-2.5x%20faster%20end-to-end%2C%20and%20up%20to%20an%20order%20of%20magnitude%20faster%20when%20excluding%20user%20prompting%20time.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivAS%253A%2520Interactive%25203D%2520Segmentation%2520of%2520NeRFs%2520via%2520Depth-Weighted%2520Voxel%2520Aggregation%26entry.906535625%3DAyush%2520Pande%26entry.1292438233%3DExisting%2520methods%2520for%2520segmenting%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520are%2520often%2520optimization-based%252C%2520requiring%2520slow%2520per-scene%2520training%2520that%2520sacrifices%2520the%2520zero-shot%2520capabilities%2520of%25202D%2520foundation%2520models.%2520We%2520introduce%2520DivAS%2520%2528Depth-interactive%2520Voxel%2520Aggregation%2520Segmentation%2529%252C%2520an%2520optimization-free%252C%2520fully%2520interactive%2520framework%2520that%2520addresses%2520these%2520limitations.%2520Our%2520method%2520operates%2520via%2520a%2520fast%2520GUI-based%2520workflow%2520where%25202D%2520SAM%2520masks%252C%2520generated%2520from%2520user%2520point%2520prompts%252C%2520are%2520refined%2520using%2520NeRF-derived%2520depth%2520priors%2520to%2520improve%2520geometric%2520accuracy%2520and%2520foreground-background%2520separation.%2520The%2520core%2520of%2520our%2520contribution%2520is%2520a%2520custom%2520CUDA%2520kernel%2520that%2520aggregates%2520these%2520refined%2520multi-view%2520masks%2520into%2520a%2520unified%25203D%2520voxel%2520grid%2520in%2520under%2520200ms%252C%2520enabling%2520real-time%2520visual%2520feedback.%2520This%2520optimization-free%2520design%2520eliminates%2520the%2520need%2520for%2520per-scene%2520training.%2520Experiments%2520on%2520Mip-NeRF%2520360%25C2%25B0%2520and%2520LLFF%2520show%2520that%2520DivAS%2520achieves%2520segmentation%2520quality%2520comparable%2520to%2520optimization-based%2520methods%252C%2520while%2520being%25202-2.5x%2520faster%2520end-to-end%252C%2520and%2520up%2520to%2520an%2520order%2520of%2520magnitude%2520faster%2520when%2520excluding%2520user%2520prompting%2520time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DivAS%3A%20Interactive%203D%20Segmentation%20of%20NeRFs%20via%20Depth-Weighted%20Voxel%20Aggregation&entry.906535625=Ayush%20Pande&entry.1292438233=Existing%20methods%20for%20segmenting%20Neural%20Radiance%20Fields%20%28NeRFs%29%20are%20often%20optimization-based%2C%20requiring%20slow%20per-scene%20training%20that%20sacrifices%20the%20zero-shot%20capabilities%20of%202D%20foundation%20models.%20We%20introduce%20DivAS%20%28Depth-interactive%20Voxel%20Aggregation%20Segmentation%29%2C%20an%20optimization-free%2C%20fully%20interactive%20framework%20that%20addresses%20these%20limitations.%20Our%20method%20operates%20via%20a%20fast%20GUI-based%20workflow%20where%202D%20SAM%20masks%2C%20generated%20from%20user%20point%20prompts%2C%20are%20refined%20using%20NeRF-derived%20depth%20priors%20to%20improve%20geometric%20accuracy%20and%20foreground-background%20separation.%20The%20core%20of%20our%20contribution%20is%20a%20custom%20CUDA%20kernel%20that%20aggregates%20these%20refined%20multi-view%20masks%20into%20a%20unified%203D%20voxel%20grid%20in%20under%20200ms%2C%20enabling%20real-time%20visual%20feedback.%20This%20optimization-free%20design%20eliminates%20the%20need%20for%20per-scene%20training.%20Experiments%20on%20Mip-NeRF%20360%C2%B0%20and%20LLFF%20show%20that%20DivAS%20achieves%20segmentation%20quality%20comparable%20to%20optimization-based%20methods%2C%20while%20being%202-2.5x%20faster%20end-to-end%2C%20and%20up%20to%20an%20order%20of%20magnitude%20faster%20when%20excluding%20user%20prompting%20time.&entry.1838667208=http%3A//arxiv.org/abs/2601.04860v1&entry.124074799=Read"},
{"title": "From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)", "author": "Suyash Mishra and Qiang Li and Srikanth Patil and Anubhav Girdhar", "abstract": "Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).\n  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.", "link": "http://arxiv.org/abs/2601.05059v1", "date": "2026-01-08", "relevancy": 2.2494, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5837}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5701}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Understanding%20to%20Engagement%3A%20Personalized%20pharmacy%20Video%20Clips%20via%20Vision%20Language%20Models%20%28VLMs%29&body=Title%3A%20From%20Understanding%20to%20Engagement%3A%20Personalized%20pharmacy%20Video%20Clips%20via%20Vision%20Language%20Models%20%28VLMs%29%0AAuthor%3A%20Suyash%20Mishra%20and%20Qiang%20Li%20and%20Srikanth%20Patil%20and%20Anubhav%20Girdhar%0AAbstract%3A%20Vision%20Language%20Models%20%28VLMs%29%20are%20poised%20to%20revolutionize%20the%20digital%20transformation%20of%20pharmacyceutical%20industry%20by%20enabling%20intelligent%2C%20scalable%2C%20and%20automated%20multi-modality%20content%20processing.%20Traditional%20manual%20annotation%20of%20heterogeneous%20data%20modalities%20%28text%2C%20images%2C%20video%2C%20audio%2C%20and%20web%20links%29%2C%20is%20prone%20to%20inconsistencies%2C%20quality%20degradation%2C%20and%20inefficiencies%20in%20content%20utilization.%20The%20sheer%20volume%20of%20long%20video%20and%20audio%20data%20further%20exacerbates%20these%20challenges%2C%20%28e.g.%20long%20clinical%20trial%20interviews%20and%20educational%20seminars%29.%0A%20%20Here%2C%20we%20introduce%20a%20domain%20adapted%20Video%20to%20Video%20Clip%20Generation%20framework%20that%20integrates%20Audio%20Language%20Models%20%28ALMs%29%20and%20Vision%20Language%20Models%20%28VLMs%29%20to%20produce%20highlight%20clips.%20Our%20contributions%20are%20threefold%3A%20%28i%29%20a%20reproducible%20Cut%20%26%20Merge%20algorithm%20with%20fade%20in/out%20and%20timestamp%20normalization%2C%20ensuring%20smooth%20transitions%20and%20audio/visual%20alignment%3B%20%28ii%29%20a%20personalization%20mechanism%20based%20on%20role%20definition%20and%20prompt%20injection%20for%20tailored%20outputs%20%28marketing%2C%20training%2C%20regulatory%29%3B%20%28iii%29%20a%20cost%20efficient%20e2e%20pipeline%20strategy%20balancing%20ALM/VLM%20enhanced%20processing.%20Evaluations%20on%20Video%20MME%20benchmark%20%28900%29%20and%20our%20proprietary%20dataset%20of%2016%2C159%20pharmacy%20videos%20across%2014%20disease%20areas%20demonstrate%203%20to%204%20times%20speedup%2C%204%20times%20cost%20reduction%2C%20and%20competitive%20clip%20quality.%20Beyond%20efficiency%20gains%2C%20we%20also%20report%20our%20methods%20improved%20clip%20coherence%20scores%20%280.348%29%20and%20informativeness%20scores%20%280.721%29%20over%20state%20of%20the%20art%20VLM%20baselines%20%28e.g.%2C%20Gemini%202.5%20Pro%29%2C%20highlighting%20the%20potential%20of%20transparent%2C%20custom%20extractive%2C%20and%20compliance%20supporting%20video%20summarization%20for%20life%20sciences.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Understanding%2520to%2520Engagement%253A%2520Personalized%2520pharmacy%2520Video%2520Clips%2520via%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%26entry.906535625%3DSuyash%2520Mishra%2520and%2520Qiang%2520Li%2520and%2520Srikanth%2520Patil%2520and%2520Anubhav%2520Girdhar%26entry.1292438233%3DVision%2520Language%2520Models%2520%2528VLMs%2529%2520are%2520poised%2520to%2520revolutionize%2520the%2520digital%2520transformation%2520of%2520pharmacyceutical%2520industry%2520by%2520enabling%2520intelligent%252C%2520scalable%252C%2520and%2520automated%2520multi-modality%2520content%2520processing.%2520Traditional%2520manual%2520annotation%2520of%2520heterogeneous%2520data%2520modalities%2520%2528text%252C%2520images%252C%2520video%252C%2520audio%252C%2520and%2520web%2520links%2529%252C%2520is%2520prone%2520to%2520inconsistencies%252C%2520quality%2520degradation%252C%2520and%2520inefficiencies%2520in%2520content%2520utilization.%2520The%2520sheer%2520volume%2520of%2520long%2520video%2520and%2520audio%2520data%2520further%2520exacerbates%2520these%2520challenges%252C%2520%2528e.g.%2520long%2520clinical%2520trial%2520interviews%2520and%2520educational%2520seminars%2529.%250A%2520%2520Here%252C%2520we%2520introduce%2520a%2520domain%2520adapted%2520Video%2520to%2520Video%2520Clip%2520Generation%2520framework%2520that%2520integrates%2520Audio%2520Language%2520Models%2520%2528ALMs%2529%2520and%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520to%2520produce%2520highlight%2520clips.%2520Our%2520contributions%2520are%2520threefold%253A%2520%2528i%2529%2520a%2520reproducible%2520Cut%2520%2526%2520Merge%2520algorithm%2520with%2520fade%2520in/out%2520and%2520timestamp%2520normalization%252C%2520ensuring%2520smooth%2520transitions%2520and%2520audio/visual%2520alignment%253B%2520%2528ii%2529%2520a%2520personalization%2520mechanism%2520based%2520on%2520role%2520definition%2520and%2520prompt%2520injection%2520for%2520tailored%2520outputs%2520%2528marketing%252C%2520training%252C%2520regulatory%2529%253B%2520%2528iii%2529%2520a%2520cost%2520efficient%2520e2e%2520pipeline%2520strategy%2520balancing%2520ALM/VLM%2520enhanced%2520processing.%2520Evaluations%2520on%2520Video%2520MME%2520benchmark%2520%2528900%2529%2520and%2520our%2520proprietary%2520dataset%2520of%252016%252C159%2520pharmacy%2520videos%2520across%252014%2520disease%2520areas%2520demonstrate%25203%2520to%25204%2520times%2520speedup%252C%25204%2520times%2520cost%2520reduction%252C%2520and%2520competitive%2520clip%2520quality.%2520Beyond%2520efficiency%2520gains%252C%2520we%2520also%2520report%2520our%2520methods%2520improved%2520clip%2520coherence%2520scores%2520%25280.348%2529%2520and%2520informativeness%2520scores%2520%25280.721%2529%2520over%2520state%2520of%2520the%2520art%2520VLM%2520baselines%2520%2528e.g.%252C%2520Gemini%25202.5%2520Pro%2529%252C%2520highlighting%2520the%2520potential%2520of%2520transparent%252C%2520custom%2520extractive%252C%2520and%2520compliance%2520supporting%2520video%2520summarization%2520for%2520life%2520sciences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Understanding%20to%20Engagement%3A%20Personalized%20pharmacy%20Video%20Clips%20via%20Vision%20Language%20Models%20%28VLMs%29&entry.906535625=Suyash%20Mishra%20and%20Qiang%20Li%20and%20Srikanth%20Patil%20and%20Anubhav%20Girdhar&entry.1292438233=Vision%20Language%20Models%20%28VLMs%29%20are%20poised%20to%20revolutionize%20the%20digital%20transformation%20of%20pharmacyceutical%20industry%20by%20enabling%20intelligent%2C%20scalable%2C%20and%20automated%20multi-modality%20content%20processing.%20Traditional%20manual%20annotation%20of%20heterogeneous%20data%20modalities%20%28text%2C%20images%2C%20video%2C%20audio%2C%20and%20web%20links%29%2C%20is%20prone%20to%20inconsistencies%2C%20quality%20degradation%2C%20and%20inefficiencies%20in%20content%20utilization.%20The%20sheer%20volume%20of%20long%20video%20and%20audio%20data%20further%20exacerbates%20these%20challenges%2C%20%28e.g.%20long%20clinical%20trial%20interviews%20and%20educational%20seminars%29.%0A%20%20Here%2C%20we%20introduce%20a%20domain%20adapted%20Video%20to%20Video%20Clip%20Generation%20framework%20that%20integrates%20Audio%20Language%20Models%20%28ALMs%29%20and%20Vision%20Language%20Models%20%28VLMs%29%20to%20produce%20highlight%20clips.%20Our%20contributions%20are%20threefold%3A%20%28i%29%20a%20reproducible%20Cut%20%26%20Merge%20algorithm%20with%20fade%20in/out%20and%20timestamp%20normalization%2C%20ensuring%20smooth%20transitions%20and%20audio/visual%20alignment%3B%20%28ii%29%20a%20personalization%20mechanism%20based%20on%20role%20definition%20and%20prompt%20injection%20for%20tailored%20outputs%20%28marketing%2C%20training%2C%20regulatory%29%3B%20%28iii%29%20a%20cost%20efficient%20e2e%20pipeline%20strategy%20balancing%20ALM/VLM%20enhanced%20processing.%20Evaluations%20on%20Video%20MME%20benchmark%20%28900%29%20and%20our%20proprietary%20dataset%20of%2016%2C159%20pharmacy%20videos%20across%2014%20disease%20areas%20demonstrate%203%20to%204%20times%20speedup%2C%204%20times%20cost%20reduction%2C%20and%20competitive%20clip%20quality.%20Beyond%20efficiency%20gains%2C%20we%20also%20report%20our%20methods%20improved%20clip%20coherence%20scores%20%280.348%29%20and%20informativeness%20scores%20%280.721%29%20over%20state%20of%20the%20art%20VLM%20baselines%20%28e.g.%2C%20Gemini%202.5%20Pro%29%2C%20highlighting%20the%20potential%20of%20transparent%2C%20custom%20extractive%2C%20and%20compliance%20supporting%20video%20summarization%20for%20life%20sciences.&entry.1838667208=http%3A//arxiv.org/abs/2601.05059v1&entry.124074799=Read"},
{"title": "Centroid Decision Forest", "author": "Amjad Ali and Saeed Aldahmani and Hailiang Du and Zardad Khan", "abstract": "This paper introduces the centroid decision forest (CDF), a novel ensemble learning framework that redefines the splitting strategy and tree building in the ordinary decision trees for high-dimensional classification. The splitting approach in CDF differs from the traditional decision trees in theat the class separability score (CSS) determines the selection of the most discriminative features at each node to construct centroids of the partitions (daughter nodes). The splitting criterion uses the Euclidean distance measurements from each class centroid to achieve a splitting mechanism that is more flexible and robust. Centroids are constructed by computing the mean feature values of the selected features for each class, ensuring a class-representative division of the feature space. This centroid-driven approach enables CDF to capture complex class structures while maintaining interpretability and scalability. To evaluate CDF, 23 high-dimensional datasets are used to assess its performance against different state-of-the-art classifiers through classification accuracy and Cohen's kappa statistic. The experimental results show that CDF outperforms the conventional methods establishing its effectiveness and flexibility for high-dimensional classification problems.", "link": "http://arxiv.org/abs/2503.19306v2", "date": "2026-01-08", "relevancy": 2.2364, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4587}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.447}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Centroid%20Decision%20Forest&body=Title%3A%20Centroid%20Decision%20Forest%0AAuthor%3A%20Amjad%20Ali%20and%20Saeed%20Aldahmani%20and%20Hailiang%20Du%20and%20Zardad%20Khan%0AAbstract%3A%20This%20paper%20introduces%20the%20centroid%20decision%20forest%20%28CDF%29%2C%20a%20novel%20ensemble%20learning%20framework%20that%20redefines%20the%20splitting%20strategy%20and%20tree%20building%20in%20the%20ordinary%20decision%20trees%20for%20high-dimensional%20classification.%20The%20splitting%20approach%20in%20CDF%20differs%20from%20the%20traditional%20decision%20trees%20in%20theat%20the%20class%20separability%20score%20%28CSS%29%20determines%20the%20selection%20of%20the%20most%20discriminative%20features%20at%20each%20node%20to%20construct%20centroids%20of%20the%20partitions%20%28daughter%20nodes%29.%20The%20splitting%20criterion%20uses%20the%20Euclidean%20distance%20measurements%20from%20each%20class%20centroid%20to%20achieve%20a%20splitting%20mechanism%20that%20is%20more%20flexible%20and%20robust.%20Centroids%20are%20constructed%20by%20computing%20the%20mean%20feature%20values%20of%20the%20selected%20features%20for%20each%20class%2C%20ensuring%20a%20class-representative%20division%20of%20the%20feature%20space.%20This%20centroid-driven%20approach%20enables%20CDF%20to%20capture%20complex%20class%20structures%20while%20maintaining%20interpretability%20and%20scalability.%20To%20evaluate%20CDF%2C%2023%20high-dimensional%20datasets%20are%20used%20to%20assess%20its%20performance%20against%20different%20state-of-the-art%20classifiers%20through%20classification%20accuracy%20and%20Cohen%27s%20kappa%20statistic.%20The%20experimental%20results%20show%20that%20CDF%20outperforms%20the%20conventional%20methods%20establishing%20its%20effectiveness%20and%20flexibility%20for%20high-dimensional%20classification%20problems.%0ALink%3A%20http%3A//arxiv.org/abs/2503.19306v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCentroid%2520Decision%2520Forest%26entry.906535625%3DAmjad%2520Ali%2520and%2520Saeed%2520Aldahmani%2520and%2520Hailiang%2520Du%2520and%2520Zardad%2520Khan%26entry.1292438233%3DThis%2520paper%2520introduces%2520the%2520centroid%2520decision%2520forest%2520%2528CDF%2529%252C%2520a%2520novel%2520ensemble%2520learning%2520framework%2520that%2520redefines%2520the%2520splitting%2520strategy%2520and%2520tree%2520building%2520in%2520the%2520ordinary%2520decision%2520trees%2520for%2520high-dimensional%2520classification.%2520The%2520splitting%2520approach%2520in%2520CDF%2520differs%2520from%2520the%2520traditional%2520decision%2520trees%2520in%2520theat%2520the%2520class%2520separability%2520score%2520%2528CSS%2529%2520determines%2520the%2520selection%2520of%2520the%2520most%2520discriminative%2520features%2520at%2520each%2520node%2520to%2520construct%2520centroids%2520of%2520the%2520partitions%2520%2528daughter%2520nodes%2529.%2520The%2520splitting%2520criterion%2520uses%2520the%2520Euclidean%2520distance%2520measurements%2520from%2520each%2520class%2520centroid%2520to%2520achieve%2520a%2520splitting%2520mechanism%2520that%2520is%2520more%2520flexible%2520and%2520robust.%2520Centroids%2520are%2520constructed%2520by%2520computing%2520the%2520mean%2520feature%2520values%2520of%2520the%2520selected%2520features%2520for%2520each%2520class%252C%2520ensuring%2520a%2520class-representative%2520division%2520of%2520the%2520feature%2520space.%2520This%2520centroid-driven%2520approach%2520enables%2520CDF%2520to%2520capture%2520complex%2520class%2520structures%2520while%2520maintaining%2520interpretability%2520and%2520scalability.%2520To%2520evaluate%2520CDF%252C%252023%2520high-dimensional%2520datasets%2520are%2520used%2520to%2520assess%2520its%2520performance%2520against%2520different%2520state-of-the-art%2520classifiers%2520through%2520classification%2520accuracy%2520and%2520Cohen%2527s%2520kappa%2520statistic.%2520The%2520experimental%2520results%2520show%2520that%2520CDF%2520outperforms%2520the%2520conventional%2520methods%2520establishing%2520its%2520effectiveness%2520and%2520flexibility%2520for%2520high-dimensional%2520classification%2520problems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19306v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Centroid%20Decision%20Forest&entry.906535625=Amjad%20Ali%20and%20Saeed%20Aldahmani%20and%20Hailiang%20Du%20and%20Zardad%20Khan&entry.1292438233=This%20paper%20introduces%20the%20centroid%20decision%20forest%20%28CDF%29%2C%20a%20novel%20ensemble%20learning%20framework%20that%20redefines%20the%20splitting%20strategy%20and%20tree%20building%20in%20the%20ordinary%20decision%20trees%20for%20high-dimensional%20classification.%20The%20splitting%20approach%20in%20CDF%20differs%20from%20the%20traditional%20decision%20trees%20in%20theat%20the%20class%20separability%20score%20%28CSS%29%20determines%20the%20selection%20of%20the%20most%20discriminative%20features%20at%20each%20node%20to%20construct%20centroids%20of%20the%20partitions%20%28daughter%20nodes%29.%20The%20splitting%20criterion%20uses%20the%20Euclidean%20distance%20measurements%20from%20each%20class%20centroid%20to%20achieve%20a%20splitting%20mechanism%20that%20is%20more%20flexible%20and%20robust.%20Centroids%20are%20constructed%20by%20computing%20the%20mean%20feature%20values%20of%20the%20selected%20features%20for%20each%20class%2C%20ensuring%20a%20class-representative%20division%20of%20the%20feature%20space.%20This%20centroid-driven%20approach%20enables%20CDF%20to%20capture%20complex%20class%20structures%20while%20maintaining%20interpretability%20and%20scalability.%20To%20evaluate%20CDF%2C%2023%20high-dimensional%20datasets%20are%20used%20to%20assess%20its%20performance%20against%20different%20state-of-the-art%20classifiers%20through%20classification%20accuracy%20and%20Cohen%27s%20kappa%20statistic.%20The%20experimental%20results%20show%20that%20CDF%20outperforms%20the%20conventional%20methods%20establishing%20its%20effectiveness%20and%20flexibility%20for%20high-dimensional%20classification%20problems.&entry.1838667208=http%3A//arxiv.org/abs/2503.19306v2&entry.124074799=Read"},
{"title": "MoE3D: A Mixture-of-Experts Module for 3D Reconstruction", "author": "Zichen Wang and Ang Cao and Liam J. Wang and Jeong Joon Park", "abstract": "MoE3D is a mixture-of-experts module designed to sharpen depth boundaries and mitigate flying-point artifacts (highlighted in red) of existing feed-forward 3D reconstruction models (left side). MoE3D predicts multiple candidate depth maps and fuses them via dynamic weighting (visualized by MoE weights on the right side). When integrated with a pre-trained 3D reconstruction backbone such as VGGT, it substantially enhances reconstruction quality with minimal additional computational overhead. Best viewed digitally.", "link": "http://arxiv.org/abs/2601.05208v1", "date": "2026-01-08", "relevancy": 2.2302, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.56}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.56}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoE3D%3A%20A%20Mixture-of-Experts%20Module%20for%203D%20Reconstruction&body=Title%3A%20MoE3D%3A%20A%20Mixture-of-Experts%20Module%20for%203D%20Reconstruction%0AAuthor%3A%20Zichen%20Wang%20and%20Ang%20Cao%20and%20Liam%20J.%20Wang%20and%20Jeong%20Joon%20Park%0AAbstract%3A%20MoE3D%20is%20a%20mixture-of-experts%20module%20designed%20to%20sharpen%20depth%20boundaries%20and%20mitigate%20flying-point%20artifacts%20%28highlighted%20in%20red%29%20of%20existing%20feed-forward%203D%20reconstruction%20models%20%28left%20side%29.%20MoE3D%20predicts%20multiple%20candidate%20depth%20maps%20and%20fuses%20them%20via%20dynamic%20weighting%20%28visualized%20by%20MoE%20weights%20on%20the%20right%20side%29.%20When%20integrated%20with%20a%20pre-trained%203D%20reconstruction%20backbone%20such%20as%20VGGT%2C%20it%20substantially%20enhances%20reconstruction%20quality%20with%20minimal%20additional%20computational%20overhead.%20Best%20viewed%20digitally.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoE3D%253A%2520A%2520Mixture-of-Experts%2520Module%2520for%25203D%2520Reconstruction%26entry.906535625%3DZichen%2520Wang%2520and%2520Ang%2520Cao%2520and%2520Liam%2520J.%2520Wang%2520and%2520Jeong%2520Joon%2520Park%26entry.1292438233%3DMoE3D%2520is%2520a%2520mixture-of-experts%2520module%2520designed%2520to%2520sharpen%2520depth%2520boundaries%2520and%2520mitigate%2520flying-point%2520artifacts%2520%2528highlighted%2520in%2520red%2529%2520of%2520existing%2520feed-forward%25203D%2520reconstruction%2520models%2520%2528left%2520side%2529.%2520MoE3D%2520predicts%2520multiple%2520candidate%2520depth%2520maps%2520and%2520fuses%2520them%2520via%2520dynamic%2520weighting%2520%2528visualized%2520by%2520MoE%2520weights%2520on%2520the%2520right%2520side%2529.%2520When%2520integrated%2520with%2520a%2520pre-trained%25203D%2520reconstruction%2520backbone%2520such%2520as%2520VGGT%252C%2520it%2520substantially%2520enhances%2520reconstruction%2520quality%2520with%2520minimal%2520additional%2520computational%2520overhead.%2520Best%2520viewed%2520digitally.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoE3D%3A%20A%20Mixture-of-Experts%20Module%20for%203D%20Reconstruction&entry.906535625=Zichen%20Wang%20and%20Ang%20Cao%20and%20Liam%20J.%20Wang%20and%20Jeong%20Joon%20Park&entry.1292438233=MoE3D%20is%20a%20mixture-of-experts%20module%20designed%20to%20sharpen%20depth%20boundaries%20and%20mitigate%20flying-point%20artifacts%20%28highlighted%20in%20red%29%20of%20existing%20feed-forward%203D%20reconstruction%20models%20%28left%20side%29.%20MoE3D%20predicts%20multiple%20candidate%20depth%20maps%20and%20fuses%20them%20via%20dynamic%20weighting%20%28visualized%20by%20MoE%20weights%20on%20the%20right%20side%29.%20When%20integrated%20with%20a%20pre-trained%203D%20reconstruction%20backbone%20such%20as%20VGGT%2C%20it%20substantially%20enhances%20reconstruction%20quality%20with%20minimal%20additional%20computational%20overhead.%20Best%20viewed%20digitally.&entry.1838667208=http%3A//arxiv.org/abs/2601.05208v1&entry.124074799=Read"},
{"title": "The Geometry of the Pivot: A Note on Lazy Pivoted Cholesky and Farthest Point Sampling", "author": "Gil Shabat", "abstract": "Low-rank approximations of large kernel matrices are ubiquitous in machine learning, particularly for scaling Gaussian Processes to massive datasets. The Pivoted Cholesky decomposition is a standard tool for this task, offering a computationally efficient, greedy low-rank approximation. While its algebraic properties are well-documented in numerical linear algebra, its geometric intuition within the context of kernel methods often remains obscure. In this note, we elucidate the geometric interpretation of the algorithm within the Reproducing Kernel Hilbert Space (RKHS). We demonstrate that the pivotal selection step is mathematically equivalent to Farthest Point Sampling (FPS) using the kernel metric, and that the Cholesky factor construction is an implicit Gram-Schmidt orthogonalization. We provide a concise derivation and a minimalist Python implementation to bridge the gap between theory and practice.", "link": "http://arxiv.org/abs/2601.03706v2", "date": "2026-01-08", "relevancy": 2.2271, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4621}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4573}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Geometry%20of%20the%20Pivot%3A%20A%20Note%20on%20Lazy%20Pivoted%20Cholesky%20and%20Farthest%20Point%20Sampling&body=Title%3A%20The%20Geometry%20of%20the%20Pivot%3A%20A%20Note%20on%20Lazy%20Pivoted%20Cholesky%20and%20Farthest%20Point%20Sampling%0AAuthor%3A%20Gil%20Shabat%0AAbstract%3A%20Low-rank%20approximations%20of%20large%20kernel%20matrices%20are%20ubiquitous%20in%20machine%20learning%2C%20particularly%20for%20scaling%20Gaussian%20Processes%20to%20massive%20datasets.%20The%20Pivoted%20Cholesky%20decomposition%20is%20a%20standard%20tool%20for%20this%20task%2C%20offering%20a%20computationally%20efficient%2C%20greedy%20low-rank%20approximation.%20While%20its%20algebraic%20properties%20are%20well-documented%20in%20numerical%20linear%20algebra%2C%20its%20geometric%20intuition%20within%20the%20context%20of%20kernel%20methods%20often%20remains%20obscure.%20In%20this%20note%2C%20we%20elucidate%20the%20geometric%20interpretation%20of%20the%20algorithm%20within%20the%20Reproducing%20Kernel%20Hilbert%20Space%20%28RKHS%29.%20We%20demonstrate%20that%20the%20pivotal%20selection%20step%20is%20mathematically%20equivalent%20to%20Farthest%20Point%20Sampling%20%28FPS%29%20using%20the%20kernel%20metric%2C%20and%20that%20the%20Cholesky%20factor%20construction%20is%20an%20implicit%20Gram-Schmidt%20orthogonalization.%20We%20provide%20a%20concise%20derivation%20and%20a%20minimalist%20Python%20implementation%20to%20bridge%20the%20gap%20between%20theory%20and%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03706v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Geometry%2520of%2520the%2520Pivot%253A%2520A%2520Note%2520on%2520Lazy%2520Pivoted%2520Cholesky%2520and%2520Farthest%2520Point%2520Sampling%26entry.906535625%3DGil%2520Shabat%26entry.1292438233%3DLow-rank%2520approximations%2520of%2520large%2520kernel%2520matrices%2520are%2520ubiquitous%2520in%2520machine%2520learning%252C%2520particularly%2520for%2520scaling%2520Gaussian%2520Processes%2520to%2520massive%2520datasets.%2520The%2520Pivoted%2520Cholesky%2520decomposition%2520is%2520a%2520standard%2520tool%2520for%2520this%2520task%252C%2520offering%2520a%2520computationally%2520efficient%252C%2520greedy%2520low-rank%2520approximation.%2520While%2520its%2520algebraic%2520properties%2520are%2520well-documented%2520in%2520numerical%2520linear%2520algebra%252C%2520its%2520geometric%2520intuition%2520within%2520the%2520context%2520of%2520kernel%2520methods%2520often%2520remains%2520obscure.%2520In%2520this%2520note%252C%2520we%2520elucidate%2520the%2520geometric%2520interpretation%2520of%2520the%2520algorithm%2520within%2520the%2520Reproducing%2520Kernel%2520Hilbert%2520Space%2520%2528RKHS%2529.%2520We%2520demonstrate%2520that%2520the%2520pivotal%2520selection%2520step%2520is%2520mathematically%2520equivalent%2520to%2520Farthest%2520Point%2520Sampling%2520%2528FPS%2529%2520using%2520the%2520kernel%2520metric%252C%2520and%2520that%2520the%2520Cholesky%2520factor%2520construction%2520is%2520an%2520implicit%2520Gram-Schmidt%2520orthogonalization.%2520We%2520provide%2520a%2520concise%2520derivation%2520and%2520a%2520minimalist%2520Python%2520implementation%2520to%2520bridge%2520the%2520gap%2520between%2520theory%2520and%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03706v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Geometry%20of%20the%20Pivot%3A%20A%20Note%20on%20Lazy%20Pivoted%20Cholesky%20and%20Farthest%20Point%20Sampling&entry.906535625=Gil%20Shabat&entry.1292438233=Low-rank%20approximations%20of%20large%20kernel%20matrices%20are%20ubiquitous%20in%20machine%20learning%2C%20particularly%20for%20scaling%20Gaussian%20Processes%20to%20massive%20datasets.%20The%20Pivoted%20Cholesky%20decomposition%20is%20a%20standard%20tool%20for%20this%20task%2C%20offering%20a%20computationally%20efficient%2C%20greedy%20low-rank%20approximation.%20While%20its%20algebraic%20properties%20are%20well-documented%20in%20numerical%20linear%20algebra%2C%20its%20geometric%20intuition%20within%20the%20context%20of%20kernel%20methods%20often%20remains%20obscure.%20In%20this%20note%2C%20we%20elucidate%20the%20geometric%20interpretation%20of%20the%20algorithm%20within%20the%20Reproducing%20Kernel%20Hilbert%20Space%20%28RKHS%29.%20We%20demonstrate%20that%20the%20pivotal%20selection%20step%20is%20mathematically%20equivalent%20to%20Farthest%20Point%20Sampling%20%28FPS%29%20using%20the%20kernel%20metric%2C%20and%20that%20the%20Cholesky%20factor%20construction%20is%20an%20implicit%20Gram-Schmidt%20orthogonalization.%20We%20provide%20a%20concise%20derivation%20and%20a%20minimalist%20Python%20implementation%20to%20bridge%20the%20gap%20between%20theory%20and%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2601.03706v2&entry.124074799=Read"},
{"title": "Parallelizing Node-Level Explainability in Graph Neural Networks", "author": "Oscar Llorente and Jaime Boal and Eugenio F. S\u00e1nchez-\u00dabeda and Antonio Diaz-Cano and Miguel Familiar", "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable performance in a wide range of tasks, such as node classification, link prediction, and graph classification, by exploiting the structural information in graph-structured data. However, in node classification, computing node-level explainability becomes extremely time-consuming as the size of the graph increases, while batching strategies often degrade explanation quality. This paper introduces a novel approach to parallelizing node-level explainability in GNNs through graph partitioning. By decomposing the graph into disjoint subgraphs, we enable parallel computation of explainability for node neighbors, significantly improving the scalability and efficiency without affecting the correctness of the results, provided sufficient memory is available. For scenarios where memory is limited, we further propose a dropout-based reconstruction mechanism that offers a controllable trade-off between memory usage and explanation fidelity. Experimental results on real-world datasets demonstrate substantial speedups, enabling scalable and transparent explainability for large-scale GNN models.", "link": "http://arxiv.org/abs/2601.04807v1", "date": "2026-01-08", "relevancy": 2.2229, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4805}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4273}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallelizing%20Node-Level%20Explainability%20in%20Graph%20Neural%20Networks&body=Title%3A%20Parallelizing%20Node-Level%20Explainability%20in%20Graph%20Neural%20Networks%0AAuthor%3A%20Oscar%20Llorente%20and%20Jaime%20Boal%20and%20Eugenio%20F.%20S%C3%A1nchez-%C3%9Abeda%20and%20Antonio%20Diaz-Cano%20and%20Miguel%20Familiar%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20remarkable%20performance%20in%20a%20wide%20range%20of%20tasks%2C%20such%20as%20node%20classification%2C%20link%20prediction%2C%20and%20graph%20classification%2C%20by%20exploiting%20the%20structural%20information%20in%20graph-structured%20data.%20However%2C%20in%20node%20classification%2C%20computing%20node-level%20explainability%20becomes%20extremely%20time-consuming%20as%20the%20size%20of%20the%20graph%20increases%2C%20while%20batching%20strategies%20often%20degrade%20explanation%20quality.%20This%20paper%20introduces%20a%20novel%20approach%20to%20parallelizing%20node-level%20explainability%20in%20GNNs%20through%20graph%20partitioning.%20By%20decomposing%20the%20graph%20into%20disjoint%20subgraphs%2C%20we%20enable%20parallel%20computation%20of%20explainability%20for%20node%20neighbors%2C%20significantly%20improving%20the%20scalability%20and%20efficiency%20without%20affecting%20the%20correctness%20of%20the%20results%2C%20provided%20sufficient%20memory%20is%20available.%20For%20scenarios%20where%20memory%20is%20limited%2C%20we%20further%20propose%20a%20dropout-based%20reconstruction%20mechanism%20that%20offers%20a%20controllable%20trade-off%20between%20memory%20usage%20and%20explanation%20fidelity.%20Experimental%20results%20on%20real-world%20datasets%20demonstrate%20substantial%20speedups%2C%20enabling%20scalable%20and%20transparent%20explainability%20for%20large-scale%20GNN%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallelizing%2520Node-Level%2520Explainability%2520in%2520Graph%2520Neural%2520Networks%26entry.906535625%3DOscar%2520Llorente%2520and%2520Jaime%2520Boal%2520and%2520Eugenio%2520F.%2520S%25C3%25A1nchez-%25C3%259Abeda%2520and%2520Antonio%2520Diaz-Cano%2520and%2520Miguel%2520Familiar%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520in%2520a%2520wide%2520range%2520of%2520tasks%252C%2520such%2520as%2520node%2520classification%252C%2520link%2520prediction%252C%2520and%2520graph%2520classification%252C%2520by%2520exploiting%2520the%2520structural%2520information%2520in%2520graph-structured%2520data.%2520However%252C%2520in%2520node%2520classification%252C%2520computing%2520node-level%2520explainability%2520becomes%2520extremely%2520time-consuming%2520as%2520the%2520size%2520of%2520the%2520graph%2520increases%252C%2520while%2520batching%2520strategies%2520often%2520degrade%2520explanation%2520quality.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520parallelizing%2520node-level%2520explainability%2520in%2520GNNs%2520through%2520graph%2520partitioning.%2520By%2520decomposing%2520the%2520graph%2520into%2520disjoint%2520subgraphs%252C%2520we%2520enable%2520parallel%2520computation%2520of%2520explainability%2520for%2520node%2520neighbors%252C%2520significantly%2520improving%2520the%2520scalability%2520and%2520efficiency%2520without%2520affecting%2520the%2520correctness%2520of%2520the%2520results%252C%2520provided%2520sufficient%2520memory%2520is%2520available.%2520For%2520scenarios%2520where%2520memory%2520is%2520limited%252C%2520we%2520further%2520propose%2520a%2520dropout-based%2520reconstruction%2520mechanism%2520that%2520offers%2520a%2520controllable%2520trade-off%2520between%2520memory%2520usage%2520and%2520explanation%2520fidelity.%2520Experimental%2520results%2520on%2520real-world%2520datasets%2520demonstrate%2520substantial%2520speedups%252C%2520enabling%2520scalable%2520and%2520transparent%2520explainability%2520for%2520large-scale%2520GNN%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallelizing%20Node-Level%20Explainability%20in%20Graph%20Neural%20Networks&entry.906535625=Oscar%20Llorente%20and%20Jaime%20Boal%20and%20Eugenio%20F.%20S%C3%A1nchez-%C3%9Abeda%20and%20Antonio%20Diaz-Cano%20and%20Miguel%20Familiar&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20remarkable%20performance%20in%20a%20wide%20range%20of%20tasks%2C%20such%20as%20node%20classification%2C%20link%20prediction%2C%20and%20graph%20classification%2C%20by%20exploiting%20the%20structural%20information%20in%20graph-structured%20data.%20However%2C%20in%20node%20classification%2C%20computing%20node-level%20explainability%20becomes%20extremely%20time-consuming%20as%20the%20size%20of%20the%20graph%20increases%2C%20while%20batching%20strategies%20often%20degrade%20explanation%20quality.%20This%20paper%20introduces%20a%20novel%20approach%20to%20parallelizing%20node-level%20explainability%20in%20GNNs%20through%20graph%20partitioning.%20By%20decomposing%20the%20graph%20into%20disjoint%20subgraphs%2C%20we%20enable%20parallel%20computation%20of%20explainability%20for%20node%20neighbors%2C%20significantly%20improving%20the%20scalability%20and%20efficiency%20without%20affecting%20the%20correctness%20of%20the%20results%2C%20provided%20sufficient%20memory%20is%20available.%20For%20scenarios%20where%20memory%20is%20limited%2C%20we%20further%20propose%20a%20dropout-based%20reconstruction%20mechanism%20that%20offers%20a%20controllable%20trade-off%20between%20memory%20usage%20and%20explanation%20fidelity.%20Experimental%20results%20on%20real-world%20datasets%20demonstrate%20substantial%20speedups%2C%20enabling%20scalable%20and%20transparent%20explainability%20for%20large-scale%20GNN%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.04807v1&entry.124074799=Read"},
{"title": "From Idea to Co-Creation: A Planner-Actor-Critic Framework for Agent Augmented 3D Modeling", "author": "Jin Gao and Saichandu Juluri", "abstract": "We present a framework that extends the Actor-Critic architecture to creative 3D modeling through multi-agent self-reflection and human-in-the-loop supervision. While existing approaches rely on single-prompt agents that directly execute modeling commands via tools like Blender MCP, our approach introduces a Planner-Actor-Critic architecture. In this design, the Planner coordinates modeling steps, the Actor executes them, and the Critic provides iterative feedback, while human users act as supervisors and advisors throughout the process. Through systematic comparison between single-prompt modeling and our reflective multi-agent approach, we demonstrate improvements in geometric accuracy, aesthetic quality, and task completion rates across diverse 3D modeling scenarios. Our evaluation reveals that critic-guided reflection, combined with human supervisory input, reduces modeling errors and increases complexity and quality of the result compared to direct single-prompt execution. This work establishes that structured agent self-reflection, when augmented by human oversight and advisory guidance, produces higher-quality 3D models while maintaining efficient workflow integration through real-time Blender synchronization.", "link": "http://arxiv.org/abs/2601.05016v1", "date": "2026-01-08", "relevancy": 2.2102, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5629}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5589}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Idea%20to%20Co-Creation%3A%20A%20Planner-Actor-Critic%20Framework%20for%20Agent%20Augmented%203D%20Modeling&body=Title%3A%20From%20Idea%20to%20Co-Creation%3A%20A%20Planner-Actor-Critic%20Framework%20for%20Agent%20Augmented%203D%20Modeling%0AAuthor%3A%20Jin%20Gao%20and%20Saichandu%20Juluri%0AAbstract%3A%20We%20present%20a%20framework%20that%20extends%20the%20Actor-Critic%20architecture%20to%20creative%203D%20modeling%20through%20multi-agent%20self-reflection%20and%20human-in-the-loop%20supervision.%20While%20existing%20approaches%20rely%20on%20single-prompt%20agents%20that%20directly%20execute%20modeling%20commands%20via%20tools%20like%20Blender%20MCP%2C%20our%20approach%20introduces%20a%20Planner-Actor-Critic%20architecture.%20In%20this%20design%2C%20the%20Planner%20coordinates%20modeling%20steps%2C%20the%20Actor%20executes%20them%2C%20and%20the%20Critic%20provides%20iterative%20feedback%2C%20while%20human%20users%20act%20as%20supervisors%20and%20advisors%20throughout%20the%20process.%20Through%20systematic%20comparison%20between%20single-prompt%20modeling%20and%20our%20reflective%20multi-agent%20approach%2C%20we%20demonstrate%20improvements%20in%20geometric%20accuracy%2C%20aesthetic%20quality%2C%20and%20task%20completion%20rates%20across%20diverse%203D%20modeling%20scenarios.%20Our%20evaluation%20reveals%20that%20critic-guided%20reflection%2C%20combined%20with%20human%20supervisory%20input%2C%20reduces%20modeling%20errors%20and%20increases%20complexity%20and%20quality%20of%20the%20result%20compared%20to%20direct%20single-prompt%20execution.%20This%20work%20establishes%20that%20structured%20agent%20self-reflection%2C%20when%20augmented%20by%20human%20oversight%20and%20advisory%20guidance%2C%20produces%20higher-quality%203D%20models%20while%20maintaining%20efficient%20workflow%20integration%20through%20real-time%20Blender%20synchronization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Idea%2520to%2520Co-Creation%253A%2520A%2520Planner-Actor-Critic%2520Framework%2520for%2520Agent%2520Augmented%25203D%2520Modeling%26entry.906535625%3DJin%2520Gao%2520and%2520Saichandu%2520Juluri%26entry.1292438233%3DWe%2520present%2520a%2520framework%2520that%2520extends%2520the%2520Actor-Critic%2520architecture%2520to%2520creative%25203D%2520modeling%2520through%2520multi-agent%2520self-reflection%2520and%2520human-in-the-loop%2520supervision.%2520While%2520existing%2520approaches%2520rely%2520on%2520single-prompt%2520agents%2520that%2520directly%2520execute%2520modeling%2520commands%2520via%2520tools%2520like%2520Blender%2520MCP%252C%2520our%2520approach%2520introduces%2520a%2520Planner-Actor-Critic%2520architecture.%2520In%2520this%2520design%252C%2520the%2520Planner%2520coordinates%2520modeling%2520steps%252C%2520the%2520Actor%2520executes%2520them%252C%2520and%2520the%2520Critic%2520provides%2520iterative%2520feedback%252C%2520while%2520human%2520users%2520act%2520as%2520supervisors%2520and%2520advisors%2520throughout%2520the%2520process.%2520Through%2520systematic%2520comparison%2520between%2520single-prompt%2520modeling%2520and%2520our%2520reflective%2520multi-agent%2520approach%252C%2520we%2520demonstrate%2520improvements%2520in%2520geometric%2520accuracy%252C%2520aesthetic%2520quality%252C%2520and%2520task%2520completion%2520rates%2520across%2520diverse%25203D%2520modeling%2520scenarios.%2520Our%2520evaluation%2520reveals%2520that%2520critic-guided%2520reflection%252C%2520combined%2520with%2520human%2520supervisory%2520input%252C%2520reduces%2520modeling%2520errors%2520and%2520increases%2520complexity%2520and%2520quality%2520of%2520the%2520result%2520compared%2520to%2520direct%2520single-prompt%2520execution.%2520This%2520work%2520establishes%2520that%2520structured%2520agent%2520self-reflection%252C%2520when%2520augmented%2520by%2520human%2520oversight%2520and%2520advisory%2520guidance%252C%2520produces%2520higher-quality%25203D%2520models%2520while%2520maintaining%2520efficient%2520workflow%2520integration%2520through%2520real-time%2520Blender%2520synchronization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Idea%20to%20Co-Creation%3A%20A%20Planner-Actor-Critic%20Framework%20for%20Agent%20Augmented%203D%20Modeling&entry.906535625=Jin%20Gao%20and%20Saichandu%20Juluri&entry.1292438233=We%20present%20a%20framework%20that%20extends%20the%20Actor-Critic%20architecture%20to%20creative%203D%20modeling%20through%20multi-agent%20self-reflection%20and%20human-in-the-loop%20supervision.%20While%20existing%20approaches%20rely%20on%20single-prompt%20agents%20that%20directly%20execute%20modeling%20commands%20via%20tools%20like%20Blender%20MCP%2C%20our%20approach%20introduces%20a%20Planner-Actor-Critic%20architecture.%20In%20this%20design%2C%20the%20Planner%20coordinates%20modeling%20steps%2C%20the%20Actor%20executes%20them%2C%20and%20the%20Critic%20provides%20iterative%20feedback%2C%20while%20human%20users%20act%20as%20supervisors%20and%20advisors%20throughout%20the%20process.%20Through%20systematic%20comparison%20between%20single-prompt%20modeling%20and%20our%20reflective%20multi-agent%20approach%2C%20we%20demonstrate%20improvements%20in%20geometric%20accuracy%2C%20aesthetic%20quality%2C%20and%20task%20completion%20rates%20across%20diverse%203D%20modeling%20scenarios.%20Our%20evaluation%20reveals%20that%20critic-guided%20reflection%2C%20combined%20with%20human%20supervisory%20input%2C%20reduces%20modeling%20errors%20and%20increases%20complexity%20and%20quality%20of%20the%20result%20compared%20to%20direct%20single-prompt%20execution.%20This%20work%20establishes%20that%20structured%20agent%20self-reflection%2C%20when%20augmented%20by%20human%20oversight%20and%20advisory%20guidance%2C%20produces%20higher-quality%203D%20models%20while%20maintaining%20efficient%20workflow%20integration%20through%20real-time%20Blender%20synchronization.&entry.1838667208=http%3A//arxiv.org/abs/2601.05016v1&entry.124074799=Read"},
{"title": "When Lower-Order Terms Dominate: Adaptive Expert Algorithms for Heavy-Tailed Losses", "author": "Antoine Moulin and Emmanuel Esposito and Dirk van der Hoeven", "abstract": "We consider the problem setting of prediction with expert advice with possibly heavy-tailed losses, i.e. the only assumption on the losses is an upper bound on their second moments, denoted by $\u03b8$. We develop adaptive algorithms that do not require any prior knowledge about the range or the second moment of the losses. Existing adaptive algorithms have what is typically considered a lower-order term in their regret guarantees. We show that this lower-order term, which is often the maximum of the losses, can actually dominate the regret bound in our setting. Specifically, we show that even with small constant $\u03b8$, this lower-order term can scale as $\\sqrt{KT}$, where $K$ is the number of experts and $T$ is the time horizon. We propose adaptive algorithms with improved regret bounds that avoid the dependence on such a lower-order term and guarantee $\\mathcal{O}(\\sqrt{\u03b8T\\log(K)})$ regret in the worst case, and $\\mathcal{O}(\u03b8\\log(KT)/\u0394_{\\min})$ regret when the losses are sampled i.i.d. from some fixed distribution, where $\u0394_{\\min}$ is the difference between the mean losses of the second best expert and the best expert. Additionally, when the loss function is the squared loss, our algorithm also guarantees improved regret bounds over prior results.", "link": "http://arxiv.org/abs/2506.01722v3", "date": "2026-01-08", "relevancy": 2.2004, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4779}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.427}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Lower-Order%20Terms%20Dominate%3A%20Adaptive%20Expert%20Algorithms%20for%20Heavy-Tailed%20Losses&body=Title%3A%20When%20Lower-Order%20Terms%20Dominate%3A%20Adaptive%20Expert%20Algorithms%20for%20Heavy-Tailed%20Losses%0AAuthor%3A%20Antoine%20Moulin%20and%20Emmanuel%20Esposito%20and%20Dirk%20van%20der%20Hoeven%0AAbstract%3A%20We%20consider%20the%20problem%20setting%20of%20prediction%20with%20expert%20advice%20with%20possibly%20heavy-tailed%20losses%2C%20i.e.%20the%20only%20assumption%20on%20the%20losses%20is%20an%20upper%20bound%20on%20their%20second%20moments%2C%20denoted%20by%20%24%CE%B8%24.%20We%20develop%20adaptive%20algorithms%20that%20do%20not%20require%20any%20prior%20knowledge%20about%20the%20range%20or%20the%20second%20moment%20of%20the%20losses.%20Existing%20adaptive%20algorithms%20have%20what%20is%20typically%20considered%20a%20lower-order%20term%20in%20their%20regret%20guarantees.%20We%20show%20that%20this%20lower-order%20term%2C%20which%20is%20often%20the%20maximum%20of%20the%20losses%2C%20can%20actually%20dominate%20the%20regret%20bound%20in%20our%20setting.%20Specifically%2C%20we%20show%20that%20even%20with%20small%20constant%20%24%CE%B8%24%2C%20this%20lower-order%20term%20can%20scale%20as%20%24%5Csqrt%7BKT%7D%24%2C%20where%20%24K%24%20is%20the%20number%20of%20experts%20and%20%24T%24%20is%20the%20time%20horizon.%20We%20propose%20adaptive%20algorithms%20with%20improved%20regret%20bounds%20that%20avoid%20the%20dependence%20on%20such%20a%20lower-order%20term%20and%20guarantee%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7B%CE%B8T%5Clog%28K%29%7D%29%24%20regret%20in%20the%20worst%20case%2C%20and%20%24%5Cmathcal%7BO%7D%28%CE%B8%5Clog%28KT%29/%CE%94_%7B%5Cmin%7D%29%24%20regret%20when%20the%20losses%20are%20sampled%20i.i.d.%20from%20some%20fixed%20distribution%2C%20where%20%24%CE%94_%7B%5Cmin%7D%24%20is%20the%20difference%20between%20the%20mean%20losses%20of%20the%20second%20best%20expert%20and%20the%20best%20expert.%20Additionally%2C%20when%20the%20loss%20function%20is%20the%20squared%20loss%2C%20our%20algorithm%20also%20guarantees%20improved%20regret%20bounds%20over%20prior%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2506.01722v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Lower-Order%2520Terms%2520Dominate%253A%2520Adaptive%2520Expert%2520Algorithms%2520for%2520Heavy-Tailed%2520Losses%26entry.906535625%3DAntoine%2520Moulin%2520and%2520Emmanuel%2520Esposito%2520and%2520Dirk%2520van%2520der%2520Hoeven%26entry.1292438233%3DWe%2520consider%2520the%2520problem%2520setting%2520of%2520prediction%2520with%2520expert%2520advice%2520with%2520possibly%2520heavy-tailed%2520losses%252C%2520i.e.%2520the%2520only%2520assumption%2520on%2520the%2520losses%2520is%2520an%2520upper%2520bound%2520on%2520their%2520second%2520moments%252C%2520denoted%2520by%2520%2524%25CE%25B8%2524.%2520We%2520develop%2520adaptive%2520algorithms%2520that%2520do%2520not%2520require%2520any%2520prior%2520knowledge%2520about%2520the%2520range%2520or%2520the%2520second%2520moment%2520of%2520the%2520losses.%2520Existing%2520adaptive%2520algorithms%2520have%2520what%2520is%2520typically%2520considered%2520a%2520lower-order%2520term%2520in%2520their%2520regret%2520guarantees.%2520We%2520show%2520that%2520this%2520lower-order%2520term%252C%2520which%2520is%2520often%2520the%2520maximum%2520of%2520the%2520losses%252C%2520can%2520actually%2520dominate%2520the%2520regret%2520bound%2520in%2520our%2520setting.%2520Specifically%252C%2520we%2520show%2520that%2520even%2520with%2520small%2520constant%2520%2524%25CE%25B8%2524%252C%2520this%2520lower-order%2520term%2520can%2520scale%2520as%2520%2524%255Csqrt%257BKT%257D%2524%252C%2520where%2520%2524K%2524%2520is%2520the%2520number%2520of%2520experts%2520and%2520%2524T%2524%2520is%2520the%2520time%2520horizon.%2520We%2520propose%2520adaptive%2520algorithms%2520with%2520improved%2520regret%2520bounds%2520that%2520avoid%2520the%2520dependence%2520on%2520such%2520a%2520lower-order%2520term%2520and%2520guarantee%2520%2524%255Cmathcal%257BO%257D%2528%255Csqrt%257B%25CE%25B8T%255Clog%2528K%2529%257D%2529%2524%2520regret%2520in%2520the%2520worst%2520case%252C%2520and%2520%2524%255Cmathcal%257BO%257D%2528%25CE%25B8%255Clog%2528KT%2529/%25CE%2594_%257B%255Cmin%257D%2529%2524%2520regret%2520when%2520the%2520losses%2520are%2520sampled%2520i.i.d.%2520from%2520some%2520fixed%2520distribution%252C%2520where%2520%2524%25CE%2594_%257B%255Cmin%257D%2524%2520is%2520the%2520difference%2520between%2520the%2520mean%2520losses%2520of%2520the%2520second%2520best%2520expert%2520and%2520the%2520best%2520expert.%2520Additionally%252C%2520when%2520the%2520loss%2520function%2520is%2520the%2520squared%2520loss%252C%2520our%2520algorithm%2520also%2520guarantees%2520improved%2520regret%2520bounds%2520over%2520prior%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01722v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Lower-Order%20Terms%20Dominate%3A%20Adaptive%20Expert%20Algorithms%20for%20Heavy-Tailed%20Losses&entry.906535625=Antoine%20Moulin%20and%20Emmanuel%20Esposito%20and%20Dirk%20van%20der%20Hoeven&entry.1292438233=We%20consider%20the%20problem%20setting%20of%20prediction%20with%20expert%20advice%20with%20possibly%20heavy-tailed%20losses%2C%20i.e.%20the%20only%20assumption%20on%20the%20losses%20is%20an%20upper%20bound%20on%20their%20second%20moments%2C%20denoted%20by%20%24%CE%B8%24.%20We%20develop%20adaptive%20algorithms%20that%20do%20not%20require%20any%20prior%20knowledge%20about%20the%20range%20or%20the%20second%20moment%20of%20the%20losses.%20Existing%20adaptive%20algorithms%20have%20what%20is%20typically%20considered%20a%20lower-order%20term%20in%20their%20regret%20guarantees.%20We%20show%20that%20this%20lower-order%20term%2C%20which%20is%20often%20the%20maximum%20of%20the%20losses%2C%20can%20actually%20dominate%20the%20regret%20bound%20in%20our%20setting.%20Specifically%2C%20we%20show%20that%20even%20with%20small%20constant%20%24%CE%B8%24%2C%20this%20lower-order%20term%20can%20scale%20as%20%24%5Csqrt%7BKT%7D%24%2C%20where%20%24K%24%20is%20the%20number%20of%20experts%20and%20%24T%24%20is%20the%20time%20horizon.%20We%20propose%20adaptive%20algorithms%20with%20improved%20regret%20bounds%20that%20avoid%20the%20dependence%20on%20such%20a%20lower-order%20term%20and%20guarantee%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7B%CE%B8T%5Clog%28K%29%7D%29%24%20regret%20in%20the%20worst%20case%2C%20and%20%24%5Cmathcal%7BO%7D%28%CE%B8%5Clog%28KT%29/%CE%94_%7B%5Cmin%7D%29%24%20regret%20when%20the%20losses%20are%20sampled%20i.i.d.%20from%20some%20fixed%20distribution%2C%20where%20%24%CE%94_%7B%5Cmin%7D%24%20is%20the%20difference%20between%20the%20mean%20losses%20of%20the%20second%20best%20expert%20and%20the%20best%20expert.%20Additionally%2C%20when%20the%20loss%20function%20is%20the%20squared%20loss%2C%20our%20algorithm%20also%20guarantees%20improved%20regret%20bounds%20over%20prior%20results.&entry.1838667208=http%3A//arxiv.org/abs/2506.01722v3&entry.124074799=Read"},
{"title": "Character Detection using YOLO for Writer Identification in multiple Medieval books", "author": "Alessandra Scotto di Freca and Tiziana D Alessandro and Francesco Fontanella and Filippo Sarria and Claudio De Stefano", "abstract": "Paleography is the study of ancient and historical handwriting, its key objectives include the dating of manuscripts and understanding the evolution of writing. Estimating when a document was written and tracing the development of scripts and writing styles can be aided by identifying the individual scribes who contributed to a medieval manuscript. Although digital technologies have made significant progress in this field, the general problem remains unsolved and continues to pose open challenges. ... We previously proposed an approach focused on identifying specific letters or abbreviations that characterize each writer. In that study, we considered the letter \"a\", as it was widely present on all pages of text and highly distinctive, according to the suggestions of expert paleographers. We used template matching techniques to detect the occurrences of the character \"a\" on each page and the convolutional neural network (CNN) to attribute each instance to the correct scribe. Moving from the interesting results achieved from this previous system and being aware of the limitations of the template matching technique, which requires an appropriate threshold to work, we decided to experiment in the same framework with the use of the YOLO object detection model to identify the scribe who contributed to the writing of different medieval books. We considered the fifth version of YOLO to implement the YOLO object detection model, which completely substituted the template matching and CNN used in the previous work. The experimental results demonstrate that YOLO effectively extracts a greater number of letters considered, leading to a more accurate second-stage classification. Furthermore, the YOLO confidence score provides a foundation for developing a system that applies a rejection threshold, enabling reliable writer identification even in unseen manuscripts.", "link": "http://arxiv.org/abs/2601.04834v1", "date": "2026-01-08", "relevancy": 2.1923, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4503}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4369}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Character%20Detection%20using%20YOLO%20for%20Writer%20Identification%20in%20multiple%20Medieval%20books&body=Title%3A%20Character%20Detection%20using%20YOLO%20for%20Writer%20Identification%20in%20multiple%20Medieval%20books%0AAuthor%3A%20Alessandra%20Scotto%20di%20Freca%20and%20Tiziana%20D%20Alessandro%20and%20Francesco%20Fontanella%20and%20Filippo%20Sarria%20and%20Claudio%20De%20Stefano%0AAbstract%3A%20Paleography%20is%20the%20study%20of%20ancient%20and%20historical%20handwriting%2C%20its%20key%20objectives%20include%20the%20dating%20of%20manuscripts%20and%20understanding%20the%20evolution%20of%20writing.%20Estimating%20when%20a%20document%20was%20written%20and%20tracing%20the%20development%20of%20scripts%20and%20writing%20styles%20can%20be%20aided%20by%20identifying%20the%20individual%20scribes%20who%20contributed%20to%20a%20medieval%20manuscript.%20Although%20digital%20technologies%20have%20made%20significant%20progress%20in%20this%20field%2C%20the%20general%20problem%20remains%20unsolved%20and%20continues%20to%20pose%20open%20challenges.%20...%20We%20previously%20proposed%20an%20approach%20focused%20on%20identifying%20specific%20letters%20or%20abbreviations%20that%20characterize%20each%20writer.%20In%20that%20study%2C%20we%20considered%20the%20letter%20%22a%22%2C%20as%20it%20was%20widely%20present%20on%20all%20pages%20of%20text%20and%20highly%20distinctive%2C%20according%20to%20the%20suggestions%20of%20expert%20paleographers.%20We%20used%20template%20matching%20techniques%20to%20detect%20the%20occurrences%20of%20the%20character%20%22a%22%20on%20each%20page%20and%20the%20convolutional%20neural%20network%20%28CNN%29%20to%20attribute%20each%20instance%20to%20the%20correct%20scribe.%20Moving%20from%20the%20interesting%20results%20achieved%20from%20this%20previous%20system%20and%20being%20aware%20of%20the%20limitations%20of%20the%20template%20matching%20technique%2C%20which%20requires%20an%20appropriate%20threshold%20to%20work%2C%20we%20decided%20to%20experiment%20in%20the%20same%20framework%20with%20the%20use%20of%20the%20YOLO%20object%20detection%20model%20to%20identify%20the%20scribe%20who%20contributed%20to%20the%20writing%20of%20different%20medieval%20books.%20We%20considered%20the%20fifth%20version%20of%20YOLO%20to%20implement%20the%20YOLO%20object%20detection%20model%2C%20which%20completely%20substituted%20the%20template%20matching%20and%20CNN%20used%20in%20the%20previous%20work.%20The%20experimental%20results%20demonstrate%20that%20YOLO%20effectively%20extracts%20a%20greater%20number%20of%20letters%20considered%2C%20leading%20to%20a%20more%20accurate%20second-stage%20classification.%20Furthermore%2C%20the%20YOLO%20confidence%20score%20provides%20a%20foundation%20for%20developing%20a%20system%20that%20applies%20a%20rejection%20threshold%2C%20enabling%20reliable%20writer%20identification%20even%20in%20unseen%20manuscripts.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacter%2520Detection%2520using%2520YOLO%2520for%2520Writer%2520Identification%2520in%2520multiple%2520Medieval%2520books%26entry.906535625%3DAlessandra%2520Scotto%2520di%2520Freca%2520and%2520Tiziana%2520D%2520Alessandro%2520and%2520Francesco%2520Fontanella%2520and%2520Filippo%2520Sarria%2520and%2520Claudio%2520De%2520Stefano%26entry.1292438233%3DPaleography%2520is%2520the%2520study%2520of%2520ancient%2520and%2520historical%2520handwriting%252C%2520its%2520key%2520objectives%2520include%2520the%2520dating%2520of%2520manuscripts%2520and%2520understanding%2520the%2520evolution%2520of%2520writing.%2520Estimating%2520when%2520a%2520document%2520was%2520written%2520and%2520tracing%2520the%2520development%2520of%2520scripts%2520and%2520writing%2520styles%2520can%2520be%2520aided%2520by%2520identifying%2520the%2520individual%2520scribes%2520who%2520contributed%2520to%2520a%2520medieval%2520manuscript.%2520Although%2520digital%2520technologies%2520have%2520made%2520significant%2520progress%2520in%2520this%2520field%252C%2520the%2520general%2520problem%2520remains%2520unsolved%2520and%2520continues%2520to%2520pose%2520open%2520challenges.%2520...%2520We%2520previously%2520proposed%2520an%2520approach%2520focused%2520on%2520identifying%2520specific%2520letters%2520or%2520abbreviations%2520that%2520characterize%2520each%2520writer.%2520In%2520that%2520study%252C%2520we%2520considered%2520the%2520letter%2520%2522a%2522%252C%2520as%2520it%2520was%2520widely%2520present%2520on%2520all%2520pages%2520of%2520text%2520and%2520highly%2520distinctive%252C%2520according%2520to%2520the%2520suggestions%2520of%2520expert%2520paleographers.%2520We%2520used%2520template%2520matching%2520techniques%2520to%2520detect%2520the%2520occurrences%2520of%2520the%2520character%2520%2522a%2522%2520on%2520each%2520page%2520and%2520the%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520to%2520attribute%2520each%2520instance%2520to%2520the%2520correct%2520scribe.%2520Moving%2520from%2520the%2520interesting%2520results%2520achieved%2520from%2520this%2520previous%2520system%2520and%2520being%2520aware%2520of%2520the%2520limitations%2520of%2520the%2520template%2520matching%2520technique%252C%2520which%2520requires%2520an%2520appropriate%2520threshold%2520to%2520work%252C%2520we%2520decided%2520to%2520experiment%2520in%2520the%2520same%2520framework%2520with%2520the%2520use%2520of%2520the%2520YOLO%2520object%2520detection%2520model%2520to%2520identify%2520the%2520scribe%2520who%2520contributed%2520to%2520the%2520writing%2520of%2520different%2520medieval%2520books.%2520We%2520considered%2520the%2520fifth%2520version%2520of%2520YOLO%2520to%2520implement%2520the%2520YOLO%2520object%2520detection%2520model%252C%2520which%2520completely%2520substituted%2520the%2520template%2520matching%2520and%2520CNN%2520used%2520in%2520the%2520previous%2520work.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520YOLO%2520effectively%2520extracts%2520a%2520greater%2520number%2520of%2520letters%2520considered%252C%2520leading%2520to%2520a%2520more%2520accurate%2520second-stage%2520classification.%2520Furthermore%252C%2520the%2520YOLO%2520confidence%2520score%2520provides%2520a%2520foundation%2520for%2520developing%2520a%2520system%2520that%2520applies%2520a%2520rejection%2520threshold%252C%2520enabling%2520reliable%2520writer%2520identification%2520even%2520in%2520unseen%2520manuscripts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Character%20Detection%20using%20YOLO%20for%20Writer%20Identification%20in%20multiple%20Medieval%20books&entry.906535625=Alessandra%20Scotto%20di%20Freca%20and%20Tiziana%20D%20Alessandro%20and%20Francesco%20Fontanella%20and%20Filippo%20Sarria%20and%20Claudio%20De%20Stefano&entry.1292438233=Paleography%20is%20the%20study%20of%20ancient%20and%20historical%20handwriting%2C%20its%20key%20objectives%20include%20the%20dating%20of%20manuscripts%20and%20understanding%20the%20evolution%20of%20writing.%20Estimating%20when%20a%20document%20was%20written%20and%20tracing%20the%20development%20of%20scripts%20and%20writing%20styles%20can%20be%20aided%20by%20identifying%20the%20individual%20scribes%20who%20contributed%20to%20a%20medieval%20manuscript.%20Although%20digital%20technologies%20have%20made%20significant%20progress%20in%20this%20field%2C%20the%20general%20problem%20remains%20unsolved%20and%20continues%20to%20pose%20open%20challenges.%20...%20We%20previously%20proposed%20an%20approach%20focused%20on%20identifying%20specific%20letters%20or%20abbreviations%20that%20characterize%20each%20writer.%20In%20that%20study%2C%20we%20considered%20the%20letter%20%22a%22%2C%20as%20it%20was%20widely%20present%20on%20all%20pages%20of%20text%20and%20highly%20distinctive%2C%20according%20to%20the%20suggestions%20of%20expert%20paleographers.%20We%20used%20template%20matching%20techniques%20to%20detect%20the%20occurrences%20of%20the%20character%20%22a%22%20on%20each%20page%20and%20the%20convolutional%20neural%20network%20%28CNN%29%20to%20attribute%20each%20instance%20to%20the%20correct%20scribe.%20Moving%20from%20the%20interesting%20results%20achieved%20from%20this%20previous%20system%20and%20being%20aware%20of%20the%20limitations%20of%20the%20template%20matching%20technique%2C%20which%20requires%20an%20appropriate%20threshold%20to%20work%2C%20we%20decided%20to%20experiment%20in%20the%20same%20framework%20with%20the%20use%20of%20the%20YOLO%20object%20detection%20model%20to%20identify%20the%20scribe%20who%20contributed%20to%20the%20writing%20of%20different%20medieval%20books.%20We%20considered%20the%20fifth%20version%20of%20YOLO%20to%20implement%20the%20YOLO%20object%20detection%20model%2C%20which%20completely%20substituted%20the%20template%20matching%20and%20CNN%20used%20in%20the%20previous%20work.%20The%20experimental%20results%20demonstrate%20that%20YOLO%20effectively%20extracts%20a%20greater%20number%20of%20letters%20considered%2C%20leading%20to%20a%20more%20accurate%20second-stage%20classification.%20Furthermore%2C%20the%20YOLO%20confidence%20score%20provides%20a%20foundation%20for%20developing%20a%20system%20that%20applies%20a%20rejection%20threshold%2C%20enabling%20reliable%20writer%20identification%20even%20in%20unseen%20manuscripts.&entry.1838667208=http%3A//arxiv.org/abs/2601.04834v1&entry.124074799=Read"},
{"title": "Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics", "author": "Oshri Naparstek", "abstract": "Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics.\n  In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \\emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space.\n  We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function.\n  To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.", "link": "http://arxiv.org/abs/2601.04854v1", "date": "2026-01-08", "relevancy": 2.1818, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5694}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.535}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token%20Maturation%3A%20Autoregressive%20Language%20Generation%20via%20Continuous%20Token%20Dynamics&body=Title%3A%20Token%20Maturation%3A%20Autoregressive%20Language%20Generation%20via%20Continuous%20Token%20Dynamics%0AAuthor%3A%20Oshri%20Naparstek%0AAbstract%3A%20Autoregressive%20language%20models%20are%20conventionally%20defined%20over%20discrete%20token%20sequences%2C%20committing%20to%20a%20specific%20token%20at%20every%20generation%20step.%20This%20early%20discretization%20forces%20uncertainty%20to%20be%20resolved%20through%20token-level%20sampling%2C%20often%20leading%20to%20instability%2C%20repetition%2C%20and%20sensitivity%20to%20decoding%20heuristics.%0A%20%20In%20this%20work%2C%20we%20introduce%20a%20continuous%20autoregressive%20formulation%20of%20language%20generation%20in%20which%20tokens%20are%20represented%20as%20continuous%20vectors%20that%20%5Cemph%7Bmature%7D%20over%20multiple%20update%20steps%20before%20being%20discretized.%20Rather%20than%20sampling%20tokens%2C%20the%20model%20evolves%20continuous%20token%20representations%20through%20a%20deterministic%20dynamical%20process%2C%20committing%20to%20a%20discrete%20token%20only%20when%20the%20representation%20has%20sufficiently%20converged.%20Discrete%20text%20is%20recovered%20via%20hard%20decoding%2C%20while%20uncertainty%20is%20maintained%20and%20resolved%20in%20the%20continuous%20space.%0A%20%20We%20show%20that%20this%20maturation%20process%20alone%20is%20sufficient%20to%20produce%20coherent%20and%20diverse%20text%20using%20deterministic%20decoding%20%28argmax%29%2C%20without%20reliance%20on%20token-level%20sampling%2C%20diffusion-style%20denoising%2C%20or%20auxiliary%20stabilization%20mechanisms.%20Additional%20perturbations%2C%20such%20as%20stochastic%20dynamics%20or%20history%20smoothing%2C%20can%20be%20incorporated%20naturally%20but%20are%20not%20required%20for%20the%20model%20to%20function.%0A%20%20To%20our%20knowledge%2C%20this%20is%20the%20first%20autoregressive%20language%20model%20that%20generates%20text%20by%20evolving%20continuous%20token%20representations%20to%20convergence%20prior%20to%20discretization%2C%20enabling%20stable%20generation%20without%20token-level%20sampling.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken%2520Maturation%253A%2520Autoregressive%2520Language%2520Generation%2520via%2520Continuous%2520Token%2520Dynamics%26entry.906535625%3DOshri%2520Naparstek%26entry.1292438233%3DAutoregressive%2520language%2520models%2520are%2520conventionally%2520defined%2520over%2520discrete%2520token%2520sequences%252C%2520committing%2520to%2520a%2520specific%2520token%2520at%2520every%2520generation%2520step.%2520This%2520early%2520discretization%2520forces%2520uncertainty%2520to%2520be%2520resolved%2520through%2520token-level%2520sampling%252C%2520often%2520leading%2520to%2520instability%252C%2520repetition%252C%2520and%2520sensitivity%2520to%2520decoding%2520heuristics.%250A%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520continuous%2520autoregressive%2520formulation%2520of%2520language%2520generation%2520in%2520which%2520tokens%2520are%2520represented%2520as%2520continuous%2520vectors%2520that%2520%255Cemph%257Bmature%257D%2520over%2520multiple%2520update%2520steps%2520before%2520being%2520discretized.%2520Rather%2520than%2520sampling%2520tokens%252C%2520the%2520model%2520evolves%2520continuous%2520token%2520representations%2520through%2520a%2520deterministic%2520dynamical%2520process%252C%2520committing%2520to%2520a%2520discrete%2520token%2520only%2520when%2520the%2520representation%2520has%2520sufficiently%2520converged.%2520Discrete%2520text%2520is%2520recovered%2520via%2520hard%2520decoding%252C%2520while%2520uncertainty%2520is%2520maintained%2520and%2520resolved%2520in%2520the%2520continuous%2520space.%250A%2520%2520We%2520show%2520that%2520this%2520maturation%2520process%2520alone%2520is%2520sufficient%2520to%2520produce%2520coherent%2520and%2520diverse%2520text%2520using%2520deterministic%2520decoding%2520%2528argmax%2529%252C%2520without%2520reliance%2520on%2520token-level%2520sampling%252C%2520diffusion-style%2520denoising%252C%2520or%2520auxiliary%2520stabilization%2520mechanisms.%2520Additional%2520perturbations%252C%2520such%2520as%2520stochastic%2520dynamics%2520or%2520history%2520smoothing%252C%2520can%2520be%2520incorporated%2520naturally%2520but%2520are%2520not%2520required%2520for%2520the%2520model%2520to%2520function.%250A%2520%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520autoregressive%2520language%2520model%2520that%2520generates%2520text%2520by%2520evolving%2520continuous%2520token%2520representations%2520to%2520convergence%2520prior%2520to%2520discretization%252C%2520enabling%2520stable%2520generation%2520without%2520token-level%2520sampling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Maturation%3A%20Autoregressive%20Language%20Generation%20via%20Continuous%20Token%20Dynamics&entry.906535625=Oshri%20Naparstek&entry.1292438233=Autoregressive%20language%20models%20are%20conventionally%20defined%20over%20discrete%20token%20sequences%2C%20committing%20to%20a%20specific%20token%20at%20every%20generation%20step.%20This%20early%20discretization%20forces%20uncertainty%20to%20be%20resolved%20through%20token-level%20sampling%2C%20often%20leading%20to%20instability%2C%20repetition%2C%20and%20sensitivity%20to%20decoding%20heuristics.%0A%20%20In%20this%20work%2C%20we%20introduce%20a%20continuous%20autoregressive%20formulation%20of%20language%20generation%20in%20which%20tokens%20are%20represented%20as%20continuous%20vectors%20that%20%5Cemph%7Bmature%7D%20over%20multiple%20update%20steps%20before%20being%20discretized.%20Rather%20than%20sampling%20tokens%2C%20the%20model%20evolves%20continuous%20token%20representations%20through%20a%20deterministic%20dynamical%20process%2C%20committing%20to%20a%20discrete%20token%20only%20when%20the%20representation%20has%20sufficiently%20converged.%20Discrete%20text%20is%20recovered%20via%20hard%20decoding%2C%20while%20uncertainty%20is%20maintained%20and%20resolved%20in%20the%20continuous%20space.%0A%20%20We%20show%20that%20this%20maturation%20process%20alone%20is%20sufficient%20to%20produce%20coherent%20and%20diverse%20text%20using%20deterministic%20decoding%20%28argmax%29%2C%20without%20reliance%20on%20token-level%20sampling%2C%20diffusion-style%20denoising%2C%20or%20auxiliary%20stabilization%20mechanisms.%20Additional%20perturbations%2C%20such%20as%20stochastic%20dynamics%20or%20history%20smoothing%2C%20can%20be%20incorporated%20naturally%20but%20are%20not%20required%20for%20the%20model%20to%20function.%0A%20%20To%20our%20knowledge%2C%20this%20is%20the%20first%20autoregressive%20language%20model%20that%20generates%20text%20by%20evolving%20continuous%20token%20representations%20to%20convergence%20prior%20to%20discretization%2C%20enabling%20stable%20generation%20without%20token-level%20sampling.&entry.1838667208=http%3A//arxiv.org/abs/2601.04854v1&entry.124074799=Read"},
{"title": "NASTaR: NovaSAR Automated Ship Target Recognition Dataset", "author": "Benyamin Hosseiny and Kamirul Kamirul and Odysseas Pappas and Alin Achim", "abstract": "Synthetic Aperture Radar (SAR) offers a unique capability for all-weather, space-based maritime activity monitoring by capturing and imaging strong reflections from ships at sea. A well-defined challenge in this domain is ship type classification. Due to the high diversity and complexity of ship types, accurate recognition is difficult and typically requires specialized deep learning models. These models, however, depend on large, high-quality ground-truth datasets to achieve robust performance and generalization. Furthermore, the growing variety of SAR satellites operating at different frequencies and spatial resolutions has amplified the need for more annotated datasets to enhance model accuracy. To address this, we present the NovaSAR Automated Ship Target Recognition (NASTaR) dataset. This dataset comprises of 3415 ship patches extracted from NovaSAR S-band imagery, with labels matched to AIS data. It includes distinctive features such as 23 unique classes, inshore/offshore separation, and an auxiliary wake dataset for patches where ship wakes are visible. We validated the dataset applicability across prominent ship-type classification scenarios using benchmark deep learning models. Results demonstrate over 60% accuracy for classifying four major ship types, over 70% for a three-class scenario, more than 75% for distinguishing cargo from tanker ships, and over 87% for identifying fishing vessels. The NASTaR dataset is available at https://doi.org/10.5523/bris.2tfa6x37oerz2lyiw6hp47058, while relevant codes for benchmarking and analysis are available at https://github.com/benyaminhosseiny/nastar.", "link": "http://arxiv.org/abs/2512.18503v2", "date": "2026-01-08", "relevancy": 2.1771, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4689}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4194}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NASTaR%3A%20NovaSAR%20Automated%20Ship%20Target%20Recognition%20Dataset&body=Title%3A%20NASTaR%3A%20NovaSAR%20Automated%20Ship%20Target%20Recognition%20Dataset%0AAuthor%3A%20Benyamin%20Hosseiny%20and%20Kamirul%20Kamirul%20and%20Odysseas%20Pappas%20and%20Alin%20Achim%0AAbstract%3A%20Synthetic%20Aperture%20Radar%20%28SAR%29%20offers%20a%20unique%20capability%20for%20all-weather%2C%20space-based%20maritime%20activity%20monitoring%20by%20capturing%20and%20imaging%20strong%20reflections%20from%20ships%20at%20sea.%20A%20well-defined%20challenge%20in%20this%20domain%20is%20ship%20type%20classification.%20Due%20to%20the%20high%20diversity%20and%20complexity%20of%20ship%20types%2C%20accurate%20recognition%20is%20difficult%20and%20typically%20requires%20specialized%20deep%20learning%20models.%20These%20models%2C%20however%2C%20depend%20on%20large%2C%20high-quality%20ground-truth%20datasets%20to%20achieve%20robust%20performance%20and%20generalization.%20Furthermore%2C%20the%20growing%20variety%20of%20SAR%20satellites%20operating%20at%20different%20frequencies%20and%20spatial%20resolutions%20has%20amplified%20the%20need%20for%20more%20annotated%20datasets%20to%20enhance%20model%20accuracy.%20To%20address%20this%2C%20we%20present%20the%20NovaSAR%20Automated%20Ship%20Target%20Recognition%20%28NASTaR%29%20dataset.%20This%20dataset%20comprises%20of%203415%20ship%20patches%20extracted%20from%20NovaSAR%20S-band%20imagery%2C%20with%20labels%20matched%20to%20AIS%20data.%20It%20includes%20distinctive%20features%20such%20as%2023%20unique%20classes%2C%20inshore/offshore%20separation%2C%20and%20an%20auxiliary%20wake%20dataset%20for%20patches%20where%20ship%20wakes%20are%20visible.%20We%20validated%20the%20dataset%20applicability%20across%20prominent%20ship-type%20classification%20scenarios%20using%20benchmark%20deep%20learning%20models.%20Results%20demonstrate%20over%2060%25%20accuracy%20for%20classifying%20four%20major%20ship%20types%2C%20over%2070%25%20for%20a%20three-class%20scenario%2C%20more%20than%2075%25%20for%20distinguishing%20cargo%20from%20tanker%20ships%2C%20and%20over%2087%25%20for%20identifying%20fishing%20vessels.%20The%20NASTaR%20dataset%20is%20available%20at%20https%3A//doi.org/10.5523/bris.2tfa6x37oerz2lyiw6hp47058%2C%20while%20relevant%20codes%20for%20benchmarking%20and%20analysis%20are%20available%20at%20https%3A//github.com/benyaminhosseiny/nastar.%0ALink%3A%20http%3A//arxiv.org/abs/2512.18503v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNASTaR%253A%2520NovaSAR%2520Automated%2520Ship%2520Target%2520Recognition%2520Dataset%26entry.906535625%3DBenyamin%2520Hosseiny%2520and%2520Kamirul%2520Kamirul%2520and%2520Odysseas%2520Pappas%2520and%2520Alin%2520Achim%26entry.1292438233%3DSynthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520offers%2520a%2520unique%2520capability%2520for%2520all-weather%252C%2520space-based%2520maritime%2520activity%2520monitoring%2520by%2520capturing%2520and%2520imaging%2520strong%2520reflections%2520from%2520ships%2520at%2520sea.%2520A%2520well-defined%2520challenge%2520in%2520this%2520domain%2520is%2520ship%2520type%2520classification.%2520Due%2520to%2520the%2520high%2520diversity%2520and%2520complexity%2520of%2520ship%2520types%252C%2520accurate%2520recognition%2520is%2520difficult%2520and%2520typically%2520requires%2520specialized%2520deep%2520learning%2520models.%2520These%2520models%252C%2520however%252C%2520depend%2520on%2520large%252C%2520high-quality%2520ground-truth%2520datasets%2520to%2520achieve%2520robust%2520performance%2520and%2520generalization.%2520Furthermore%252C%2520the%2520growing%2520variety%2520of%2520SAR%2520satellites%2520operating%2520at%2520different%2520frequencies%2520and%2520spatial%2520resolutions%2520has%2520amplified%2520the%2520need%2520for%2520more%2520annotated%2520datasets%2520to%2520enhance%2520model%2520accuracy.%2520To%2520address%2520this%252C%2520we%2520present%2520the%2520NovaSAR%2520Automated%2520Ship%2520Target%2520Recognition%2520%2528NASTaR%2529%2520dataset.%2520This%2520dataset%2520comprises%2520of%25203415%2520ship%2520patches%2520extracted%2520from%2520NovaSAR%2520S-band%2520imagery%252C%2520with%2520labels%2520matched%2520to%2520AIS%2520data.%2520It%2520includes%2520distinctive%2520features%2520such%2520as%252023%2520unique%2520classes%252C%2520inshore/offshore%2520separation%252C%2520and%2520an%2520auxiliary%2520wake%2520dataset%2520for%2520patches%2520where%2520ship%2520wakes%2520are%2520visible.%2520We%2520validated%2520the%2520dataset%2520applicability%2520across%2520prominent%2520ship-type%2520classification%2520scenarios%2520using%2520benchmark%2520deep%2520learning%2520models.%2520Results%2520demonstrate%2520over%252060%2525%2520accuracy%2520for%2520classifying%2520four%2520major%2520ship%2520types%252C%2520over%252070%2525%2520for%2520a%2520three-class%2520scenario%252C%2520more%2520than%252075%2525%2520for%2520distinguishing%2520cargo%2520from%2520tanker%2520ships%252C%2520and%2520over%252087%2525%2520for%2520identifying%2520fishing%2520vessels.%2520The%2520NASTaR%2520dataset%2520is%2520available%2520at%2520https%253A//doi.org/10.5523/bris.2tfa6x37oerz2lyiw6hp47058%252C%2520while%2520relevant%2520codes%2520for%2520benchmarking%2520and%2520analysis%2520are%2520available%2520at%2520https%253A//github.com/benyaminhosseiny/nastar.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.18503v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NASTaR%3A%20NovaSAR%20Automated%20Ship%20Target%20Recognition%20Dataset&entry.906535625=Benyamin%20Hosseiny%20and%20Kamirul%20Kamirul%20and%20Odysseas%20Pappas%20and%20Alin%20Achim&entry.1292438233=Synthetic%20Aperture%20Radar%20%28SAR%29%20offers%20a%20unique%20capability%20for%20all-weather%2C%20space-based%20maritime%20activity%20monitoring%20by%20capturing%20and%20imaging%20strong%20reflections%20from%20ships%20at%20sea.%20A%20well-defined%20challenge%20in%20this%20domain%20is%20ship%20type%20classification.%20Due%20to%20the%20high%20diversity%20and%20complexity%20of%20ship%20types%2C%20accurate%20recognition%20is%20difficult%20and%20typically%20requires%20specialized%20deep%20learning%20models.%20These%20models%2C%20however%2C%20depend%20on%20large%2C%20high-quality%20ground-truth%20datasets%20to%20achieve%20robust%20performance%20and%20generalization.%20Furthermore%2C%20the%20growing%20variety%20of%20SAR%20satellites%20operating%20at%20different%20frequencies%20and%20spatial%20resolutions%20has%20amplified%20the%20need%20for%20more%20annotated%20datasets%20to%20enhance%20model%20accuracy.%20To%20address%20this%2C%20we%20present%20the%20NovaSAR%20Automated%20Ship%20Target%20Recognition%20%28NASTaR%29%20dataset.%20This%20dataset%20comprises%20of%203415%20ship%20patches%20extracted%20from%20NovaSAR%20S-band%20imagery%2C%20with%20labels%20matched%20to%20AIS%20data.%20It%20includes%20distinctive%20features%20such%20as%2023%20unique%20classes%2C%20inshore/offshore%20separation%2C%20and%20an%20auxiliary%20wake%20dataset%20for%20patches%20where%20ship%20wakes%20are%20visible.%20We%20validated%20the%20dataset%20applicability%20across%20prominent%20ship-type%20classification%20scenarios%20using%20benchmark%20deep%20learning%20models.%20Results%20demonstrate%20over%2060%25%20accuracy%20for%20classifying%20four%20major%20ship%20types%2C%20over%2070%25%20for%20a%20three-class%20scenario%2C%20more%20than%2075%25%20for%20distinguishing%20cargo%20from%20tanker%20ships%2C%20and%20over%2087%25%20for%20identifying%20fishing%20vessels.%20The%20NASTaR%20dataset%20is%20available%20at%20https%3A//doi.org/10.5523/bris.2tfa6x37oerz2lyiw6hp47058%2C%20while%20relevant%20codes%20for%20benchmarking%20and%20analysis%20are%20available%20at%20https%3A//github.com/benyaminhosseiny/nastar.&entry.1838667208=http%3A//arxiv.org/abs/2512.18503v2&entry.124074799=Read"},
{"title": "SKATER: Synthesized Kinematics for Advanced Traversing Efficiency on a Humanoid Robot via Roller Skate Swizzles", "author": "Junchi Gu and Feiyang Yuan and Weize Shi and Tianchen Huang and Haopeng Zhang and Xiaohu Zhang and Yu Wang and Wei Gao and Shiwu Zhang", "abstract": "Although recent years have seen significant progress of humanoid robots in walking and running, the frequent foot strikes with ground during these locomotion gaits inevitably generate high instantaneous impact forces, which leads to exacerbated joint wear and poor energy utilization. Roller skating, as a sport with substantial biomechanical value, can achieve fast and continuous sliding through rational utilization of body inertia, featuring minimal kinetic energy loss. Therefore, this study proposes a novel humanoid robot with each foot equipped with a row of four passive wheels for roller skating. A deep reinforcement learning control framework is also developed for the swizzle gait with the reward function design based on the intrinsic characteristics of roller skating. The learned policy is first analyzed in simulation and then deployed on the physical robot to demonstrate the smoothness and efficiency of the swizzle gait over traditional bipedal walking gait in terms of Impact Intensity and Cost of Transport during locomotion. A reduction of $75.86\\%$ and $63.34\\%$ of these two metrics indicate roller skating as a superior locomotion mode for enhanced energy efficiency and joint longevity.", "link": "http://arxiv.org/abs/2601.04948v1", "date": "2026-01-08", "relevancy": 2.1657, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5706}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5524}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SKATER%3A%20Synthesized%20Kinematics%20for%20Advanced%20Traversing%20Efficiency%20on%20a%20Humanoid%20Robot%20via%20Roller%20Skate%20Swizzles&body=Title%3A%20SKATER%3A%20Synthesized%20Kinematics%20for%20Advanced%20Traversing%20Efficiency%20on%20a%20Humanoid%20Robot%20via%20Roller%20Skate%20Swizzles%0AAuthor%3A%20Junchi%20Gu%20and%20Feiyang%20Yuan%20and%20Weize%20Shi%20and%20Tianchen%20Huang%20and%20Haopeng%20Zhang%20and%20Xiaohu%20Zhang%20and%20Yu%20Wang%20and%20Wei%20Gao%20and%20Shiwu%20Zhang%0AAbstract%3A%20Although%20recent%20years%20have%20seen%20significant%20progress%20of%20humanoid%20robots%20in%20walking%20and%20running%2C%20the%20frequent%20foot%20strikes%20with%20ground%20during%20these%20locomotion%20gaits%20inevitably%20generate%20high%20instantaneous%20impact%20forces%2C%20which%20leads%20to%20exacerbated%20joint%20wear%20and%20poor%20energy%20utilization.%20Roller%20skating%2C%20as%20a%20sport%20with%20substantial%20biomechanical%20value%2C%20can%20achieve%20fast%20and%20continuous%20sliding%20through%20rational%20utilization%20of%20body%20inertia%2C%20featuring%20minimal%20kinetic%20energy%20loss.%20Therefore%2C%20this%20study%20proposes%20a%20novel%20humanoid%20robot%20with%20each%20foot%20equipped%20with%20a%20row%20of%20four%20passive%20wheels%20for%20roller%20skating.%20A%20deep%20reinforcement%20learning%20control%20framework%20is%20also%20developed%20for%20the%20swizzle%20gait%20with%20the%20reward%20function%20design%20based%20on%20the%20intrinsic%20characteristics%20of%20roller%20skating.%20The%20learned%20policy%20is%20first%20analyzed%20in%20simulation%20and%20then%20deployed%20on%20the%20physical%20robot%20to%20demonstrate%20the%20smoothness%20and%20efficiency%20of%20the%20swizzle%20gait%20over%20traditional%20bipedal%20walking%20gait%20in%20terms%20of%20Impact%20Intensity%20and%20Cost%20of%20Transport%20during%20locomotion.%20A%20reduction%20of%20%2475.86%5C%25%24%20and%20%2463.34%5C%25%24%20of%20these%20two%20metrics%20indicate%20roller%20skating%20as%20a%20superior%20locomotion%20mode%20for%20enhanced%20energy%20efficiency%20and%20joint%20longevity.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSKATER%253A%2520Synthesized%2520Kinematics%2520for%2520Advanced%2520Traversing%2520Efficiency%2520on%2520a%2520Humanoid%2520Robot%2520via%2520Roller%2520Skate%2520Swizzles%26entry.906535625%3DJunchi%2520Gu%2520and%2520Feiyang%2520Yuan%2520and%2520Weize%2520Shi%2520and%2520Tianchen%2520Huang%2520and%2520Haopeng%2520Zhang%2520and%2520Xiaohu%2520Zhang%2520and%2520Yu%2520Wang%2520and%2520Wei%2520Gao%2520and%2520Shiwu%2520Zhang%26entry.1292438233%3DAlthough%2520recent%2520years%2520have%2520seen%2520significant%2520progress%2520of%2520humanoid%2520robots%2520in%2520walking%2520and%2520running%252C%2520the%2520frequent%2520foot%2520strikes%2520with%2520ground%2520during%2520these%2520locomotion%2520gaits%2520inevitably%2520generate%2520high%2520instantaneous%2520impact%2520forces%252C%2520which%2520leads%2520to%2520exacerbated%2520joint%2520wear%2520and%2520poor%2520energy%2520utilization.%2520Roller%2520skating%252C%2520as%2520a%2520sport%2520with%2520substantial%2520biomechanical%2520value%252C%2520can%2520achieve%2520fast%2520and%2520continuous%2520sliding%2520through%2520rational%2520utilization%2520of%2520body%2520inertia%252C%2520featuring%2520minimal%2520kinetic%2520energy%2520loss.%2520Therefore%252C%2520this%2520study%2520proposes%2520a%2520novel%2520humanoid%2520robot%2520with%2520each%2520foot%2520equipped%2520with%2520a%2520row%2520of%2520four%2520passive%2520wheels%2520for%2520roller%2520skating.%2520A%2520deep%2520reinforcement%2520learning%2520control%2520framework%2520is%2520also%2520developed%2520for%2520the%2520swizzle%2520gait%2520with%2520the%2520reward%2520function%2520design%2520based%2520on%2520the%2520intrinsic%2520characteristics%2520of%2520roller%2520skating.%2520The%2520learned%2520policy%2520is%2520first%2520analyzed%2520in%2520simulation%2520and%2520then%2520deployed%2520on%2520the%2520physical%2520robot%2520to%2520demonstrate%2520the%2520smoothness%2520and%2520efficiency%2520of%2520the%2520swizzle%2520gait%2520over%2520traditional%2520bipedal%2520walking%2520gait%2520in%2520terms%2520of%2520Impact%2520Intensity%2520and%2520Cost%2520of%2520Transport%2520during%2520locomotion.%2520A%2520reduction%2520of%2520%252475.86%255C%2525%2524%2520and%2520%252463.34%255C%2525%2524%2520of%2520these%2520two%2520metrics%2520indicate%2520roller%2520skating%2520as%2520a%2520superior%2520locomotion%2520mode%2520for%2520enhanced%2520energy%2520efficiency%2520and%2520joint%2520longevity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SKATER%3A%20Synthesized%20Kinematics%20for%20Advanced%20Traversing%20Efficiency%20on%20a%20Humanoid%20Robot%20via%20Roller%20Skate%20Swizzles&entry.906535625=Junchi%20Gu%20and%20Feiyang%20Yuan%20and%20Weize%20Shi%20and%20Tianchen%20Huang%20and%20Haopeng%20Zhang%20and%20Xiaohu%20Zhang%20and%20Yu%20Wang%20and%20Wei%20Gao%20and%20Shiwu%20Zhang&entry.1292438233=Although%20recent%20years%20have%20seen%20significant%20progress%20of%20humanoid%20robots%20in%20walking%20and%20running%2C%20the%20frequent%20foot%20strikes%20with%20ground%20during%20these%20locomotion%20gaits%20inevitably%20generate%20high%20instantaneous%20impact%20forces%2C%20which%20leads%20to%20exacerbated%20joint%20wear%20and%20poor%20energy%20utilization.%20Roller%20skating%2C%20as%20a%20sport%20with%20substantial%20biomechanical%20value%2C%20can%20achieve%20fast%20and%20continuous%20sliding%20through%20rational%20utilization%20of%20body%20inertia%2C%20featuring%20minimal%20kinetic%20energy%20loss.%20Therefore%2C%20this%20study%20proposes%20a%20novel%20humanoid%20robot%20with%20each%20foot%20equipped%20with%20a%20row%20of%20four%20passive%20wheels%20for%20roller%20skating.%20A%20deep%20reinforcement%20learning%20control%20framework%20is%20also%20developed%20for%20the%20swizzle%20gait%20with%20the%20reward%20function%20design%20based%20on%20the%20intrinsic%20characteristics%20of%20roller%20skating.%20The%20learned%20policy%20is%20first%20analyzed%20in%20simulation%20and%20then%20deployed%20on%20the%20physical%20robot%20to%20demonstrate%20the%20smoothness%20and%20efficiency%20of%20the%20swizzle%20gait%20over%20traditional%20bipedal%20walking%20gait%20in%20terms%20of%20Impact%20Intensity%20and%20Cost%20of%20Transport%20during%20locomotion.%20A%20reduction%20of%20%2475.86%5C%25%24%20and%20%2463.34%5C%25%24%20of%20these%20two%20metrics%20indicate%20roller%20skating%20as%20a%20superior%20locomotion%20mode%20for%20enhanced%20energy%20efficiency%20and%20joint%20longevity.&entry.1838667208=http%3A//arxiv.org/abs/2601.04948v1&entry.124074799=Read"},
{"title": "How to Set the Batch Size for Large-Scale Pre-training?", "author": "Yunhua Zhou and Junhao Huang and Shuhao Xin and Yechen Zhang and Runyu Peng and Qiping Guo and Xipeng Qiu", "abstract": "The concept of Critical Batch Size, as pioneered by OpenAI, has long served as a foundational principle for large-scale pre-training. However, with the paradigm shift towards the Warmup-Stable-Decay (WSD) learning rate scheduler, we observe that the original theoretical framework and its underlying mechanisms fail to align with new pre-training dynamics. To bridge this gap between theory and practice, this paper derives a revised E(S) relationship tailored for WSD scheduler, characterizing the trade-off between training data consumption E and steps S during pre-training. Our theoretical analysis reveals two fundamental properties of WSD-based pre-training: 1) B_min, the minimum batch size threshold required to achieve a target loss, and 2) B_opt, the optimal batch size that maximizes data efficiency by minimizing total tokens. Building upon these properties, we propose a dynamic Batch Size Scheduler. Extensive experiments demonstrate that our revised formula precisely captures the dynamics of large-scale pre-training, and the resulting scheduling strategy significantly enhances both training efficiency and final model quality.", "link": "http://arxiv.org/abs/2601.05034v1", "date": "2026-01-08", "relevancy": 2.165, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.441}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4368}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Set%20the%20Batch%20Size%20for%20Large-Scale%20Pre-training%3F&body=Title%3A%20How%20to%20Set%20the%20Batch%20Size%20for%20Large-Scale%20Pre-training%3F%0AAuthor%3A%20Yunhua%20Zhou%20and%20Junhao%20Huang%20and%20Shuhao%20Xin%20and%20Yechen%20Zhang%20and%20Runyu%20Peng%20and%20Qiping%20Guo%20and%20Xipeng%20Qiu%0AAbstract%3A%20The%20concept%20of%20Critical%20Batch%20Size%2C%20as%20pioneered%20by%20OpenAI%2C%20has%20long%20served%20as%20a%20foundational%20principle%20for%20large-scale%20pre-training.%20However%2C%20with%20the%20paradigm%20shift%20towards%20the%20Warmup-Stable-Decay%20%28WSD%29%20learning%20rate%20scheduler%2C%20we%20observe%20that%20the%20original%20theoretical%20framework%20and%20its%20underlying%20mechanisms%20fail%20to%20align%20with%20new%20pre-training%20dynamics.%20To%20bridge%20this%20gap%20between%20theory%20and%20practice%2C%20this%20paper%20derives%20a%20revised%20E%28S%29%20relationship%20tailored%20for%20WSD%20scheduler%2C%20characterizing%20the%20trade-off%20between%20training%20data%20consumption%20E%20and%20steps%20S%20during%20pre-training.%20Our%20theoretical%20analysis%20reveals%20two%20fundamental%20properties%20of%20WSD-based%20pre-training%3A%201%29%20B_min%2C%20the%20minimum%20batch%20size%20threshold%20required%20to%20achieve%20a%20target%20loss%2C%20and%202%29%20B_opt%2C%20the%20optimal%20batch%20size%20that%20maximizes%20data%20efficiency%20by%20minimizing%20total%20tokens.%20Building%20upon%20these%20properties%2C%20we%20propose%20a%20dynamic%20Batch%20Size%20Scheduler.%20Extensive%20experiments%20demonstrate%20that%20our%20revised%20formula%20precisely%20captures%20the%20dynamics%20of%20large-scale%20pre-training%2C%20and%20the%20resulting%20scheduling%20strategy%20significantly%20enhances%20both%20training%20efficiency%20and%20final%20model%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Set%2520the%2520Batch%2520Size%2520for%2520Large-Scale%2520Pre-training%253F%26entry.906535625%3DYunhua%2520Zhou%2520and%2520Junhao%2520Huang%2520and%2520Shuhao%2520Xin%2520and%2520Yechen%2520Zhang%2520and%2520Runyu%2520Peng%2520and%2520Qiping%2520Guo%2520and%2520Xipeng%2520Qiu%26entry.1292438233%3DThe%2520concept%2520of%2520Critical%2520Batch%2520Size%252C%2520as%2520pioneered%2520by%2520OpenAI%252C%2520has%2520long%2520served%2520as%2520a%2520foundational%2520principle%2520for%2520large-scale%2520pre-training.%2520However%252C%2520with%2520the%2520paradigm%2520shift%2520towards%2520the%2520Warmup-Stable-Decay%2520%2528WSD%2529%2520learning%2520rate%2520scheduler%252C%2520we%2520observe%2520that%2520the%2520original%2520theoretical%2520framework%2520and%2520its%2520underlying%2520mechanisms%2520fail%2520to%2520align%2520with%2520new%2520pre-training%2520dynamics.%2520To%2520bridge%2520this%2520gap%2520between%2520theory%2520and%2520practice%252C%2520this%2520paper%2520derives%2520a%2520revised%2520E%2528S%2529%2520relationship%2520tailored%2520for%2520WSD%2520scheduler%252C%2520characterizing%2520the%2520trade-off%2520between%2520training%2520data%2520consumption%2520E%2520and%2520steps%2520S%2520during%2520pre-training.%2520Our%2520theoretical%2520analysis%2520reveals%2520two%2520fundamental%2520properties%2520of%2520WSD-based%2520pre-training%253A%25201%2529%2520B_min%252C%2520the%2520minimum%2520batch%2520size%2520threshold%2520required%2520to%2520achieve%2520a%2520target%2520loss%252C%2520and%25202%2529%2520B_opt%252C%2520the%2520optimal%2520batch%2520size%2520that%2520maximizes%2520data%2520efficiency%2520by%2520minimizing%2520total%2520tokens.%2520Building%2520upon%2520these%2520properties%252C%2520we%2520propose%2520a%2520dynamic%2520Batch%2520Size%2520Scheduler.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520revised%2520formula%2520precisely%2520captures%2520the%2520dynamics%2520of%2520large-scale%2520pre-training%252C%2520and%2520the%2520resulting%2520scheduling%2520strategy%2520significantly%2520enhances%2520both%2520training%2520efficiency%2520and%2520final%2520model%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Set%20the%20Batch%20Size%20for%20Large-Scale%20Pre-training%3F&entry.906535625=Yunhua%20Zhou%20and%20Junhao%20Huang%20and%20Shuhao%20Xin%20and%20Yechen%20Zhang%20and%20Runyu%20Peng%20and%20Qiping%20Guo%20and%20Xipeng%20Qiu&entry.1292438233=The%20concept%20of%20Critical%20Batch%20Size%2C%20as%20pioneered%20by%20OpenAI%2C%20has%20long%20served%20as%20a%20foundational%20principle%20for%20large-scale%20pre-training.%20However%2C%20with%20the%20paradigm%20shift%20towards%20the%20Warmup-Stable-Decay%20%28WSD%29%20learning%20rate%20scheduler%2C%20we%20observe%20that%20the%20original%20theoretical%20framework%20and%20its%20underlying%20mechanisms%20fail%20to%20align%20with%20new%20pre-training%20dynamics.%20To%20bridge%20this%20gap%20between%20theory%20and%20practice%2C%20this%20paper%20derives%20a%20revised%20E%28S%29%20relationship%20tailored%20for%20WSD%20scheduler%2C%20characterizing%20the%20trade-off%20between%20training%20data%20consumption%20E%20and%20steps%20S%20during%20pre-training.%20Our%20theoretical%20analysis%20reveals%20two%20fundamental%20properties%20of%20WSD-based%20pre-training%3A%201%29%20B_min%2C%20the%20minimum%20batch%20size%20threshold%20required%20to%20achieve%20a%20target%20loss%2C%20and%202%29%20B_opt%2C%20the%20optimal%20batch%20size%20that%20maximizes%20data%20efficiency%20by%20minimizing%20total%20tokens.%20Building%20upon%20these%20properties%2C%20we%20propose%20a%20dynamic%20Batch%20Size%20Scheduler.%20Extensive%20experiments%20demonstrate%20that%20our%20revised%20formula%20precisely%20captures%20the%20dynamics%20of%20large-scale%20pre-training%2C%20and%20the%20resulting%20scheduling%20strategy%20significantly%20enhances%20both%20training%20efficiency%20and%20final%20model%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2601.05034v1&entry.124074799=Read"},
{"title": "MPM-LLM4DSE: Reaching the Pareto Frontier in HLS with Multimodal Learning and LLM-Driven Exploration", "author": "Lei Xu and Shanshan Wang and Chenglong Xiao", "abstract": "High-Level Synthesis (HLS) design space exploration (DSE) seeks Pareto-optimal designs within expansive pragma configuration spaces. To accelerate HLS DSE, graph neural networks (GNNs) are commonly employed as surrogates for HLS tools to predict quality of results (QoR) metrics, while multi-objective optimization algorithms expedite the exploration. However, GNN-based prediction methods may not fully capture the rich semantic features inherent in behavioral descriptions, and conventional multi-objective optimization algorithms often do not explicitly account for the domain-specific knowledge regarding how pragma directives influence QoR. To address these limitations, this paper proposes the MPM-LLM4DSE framework, which incorporates a multimodal prediction model (MPM) that simultaneously fuses features from behavioral descriptions and control and data flow graphs. Furthermore, the framework employs a large language model (LLM) as an optimizer, accompanied by a tailored prompt engineering methodology. This methodology incorporates pragma impact analysis on QoR to guide the LLM in generating high-quality configurations (LLM4DSE). Experimental results demonstrate that our multimodal predictive model significantly outperforms state-of-the-art work ProgSG by up to 10.25$\\times$. Furthermore, in DSE tasks, the proposed LLM4DSE achieves an average performance gain of 39.90\\% over prior methods, validating the effectiveness of our prompting methodology. Code and models are available at https://github.com/wslcccc/MPM-LLM4DSE.", "link": "http://arxiv.org/abs/2601.04801v1", "date": "2026-01-08", "relevancy": 2.1611, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.56}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MPM-LLM4DSE%3A%20Reaching%20the%20Pareto%20Frontier%20in%20HLS%20with%20Multimodal%20Learning%20and%20LLM-Driven%20Exploration&body=Title%3A%20MPM-LLM4DSE%3A%20Reaching%20the%20Pareto%20Frontier%20in%20HLS%20with%20Multimodal%20Learning%20and%20LLM-Driven%20Exploration%0AAuthor%3A%20Lei%20Xu%20and%20Shanshan%20Wang%20and%20Chenglong%20Xiao%0AAbstract%3A%20High-Level%20Synthesis%20%28HLS%29%20design%20space%20exploration%20%28DSE%29%20seeks%20Pareto-optimal%20designs%20within%20expansive%20pragma%20configuration%20spaces.%20To%20accelerate%20HLS%20DSE%2C%20graph%20neural%20networks%20%28GNNs%29%20are%20commonly%20employed%20as%20surrogates%20for%20HLS%20tools%20to%20predict%20quality%20of%20results%20%28QoR%29%20metrics%2C%20while%20multi-objective%20optimization%20algorithms%20expedite%20the%20exploration.%20However%2C%20GNN-based%20prediction%20methods%20may%20not%20fully%20capture%20the%20rich%20semantic%20features%20inherent%20in%20behavioral%20descriptions%2C%20and%20conventional%20multi-objective%20optimization%20algorithms%20often%20do%20not%20explicitly%20account%20for%20the%20domain-specific%20knowledge%20regarding%20how%20pragma%20directives%20influence%20QoR.%20To%20address%20these%20limitations%2C%20this%20paper%20proposes%20the%20MPM-LLM4DSE%20framework%2C%20which%20incorporates%20a%20multimodal%20prediction%20model%20%28MPM%29%20that%20simultaneously%20fuses%20features%20from%20behavioral%20descriptions%20and%20control%20and%20data%20flow%20graphs.%20Furthermore%2C%20the%20framework%20employs%20a%20large%20language%20model%20%28LLM%29%20as%20an%20optimizer%2C%20accompanied%20by%20a%20tailored%20prompt%20engineering%20methodology.%20This%20methodology%20incorporates%20pragma%20impact%20analysis%20on%20QoR%20to%20guide%20the%20LLM%20in%20generating%20high-quality%20configurations%20%28LLM4DSE%29.%20Experimental%20results%20demonstrate%20that%20our%20multimodal%20predictive%20model%20significantly%20outperforms%20state-of-the-art%20work%20ProgSG%20by%20up%20to%2010.25%24%5Ctimes%24.%20Furthermore%2C%20in%20DSE%20tasks%2C%20the%20proposed%20LLM4DSE%20achieves%20an%20average%20performance%20gain%20of%2039.90%5C%25%20over%20prior%20methods%2C%20validating%20the%20effectiveness%20of%20our%20prompting%20methodology.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/wslcccc/MPM-LLM4DSE.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMPM-LLM4DSE%253A%2520Reaching%2520the%2520Pareto%2520Frontier%2520in%2520HLS%2520with%2520Multimodal%2520Learning%2520and%2520LLM-Driven%2520Exploration%26entry.906535625%3DLei%2520Xu%2520and%2520Shanshan%2520Wang%2520and%2520Chenglong%2520Xiao%26entry.1292438233%3DHigh-Level%2520Synthesis%2520%2528HLS%2529%2520design%2520space%2520exploration%2520%2528DSE%2529%2520seeks%2520Pareto-optimal%2520designs%2520within%2520expansive%2520pragma%2520configuration%2520spaces.%2520To%2520accelerate%2520HLS%2520DSE%252C%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520commonly%2520employed%2520as%2520surrogates%2520for%2520HLS%2520tools%2520to%2520predict%2520quality%2520of%2520results%2520%2528QoR%2529%2520metrics%252C%2520while%2520multi-objective%2520optimization%2520algorithms%2520expedite%2520the%2520exploration.%2520However%252C%2520GNN-based%2520prediction%2520methods%2520may%2520not%2520fully%2520capture%2520the%2520rich%2520semantic%2520features%2520inherent%2520in%2520behavioral%2520descriptions%252C%2520and%2520conventional%2520multi-objective%2520optimization%2520algorithms%2520often%2520do%2520not%2520explicitly%2520account%2520for%2520the%2520domain-specific%2520knowledge%2520regarding%2520how%2520pragma%2520directives%2520influence%2520QoR.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%2520proposes%2520the%2520MPM-LLM4DSE%2520framework%252C%2520which%2520incorporates%2520a%2520multimodal%2520prediction%2520model%2520%2528MPM%2529%2520that%2520simultaneously%2520fuses%2520features%2520from%2520behavioral%2520descriptions%2520and%2520control%2520and%2520data%2520flow%2520graphs.%2520Furthermore%252C%2520the%2520framework%2520employs%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520as%2520an%2520optimizer%252C%2520accompanied%2520by%2520a%2520tailored%2520prompt%2520engineering%2520methodology.%2520This%2520methodology%2520incorporates%2520pragma%2520impact%2520analysis%2520on%2520QoR%2520to%2520guide%2520the%2520LLM%2520in%2520generating%2520high-quality%2520configurations%2520%2528LLM4DSE%2529.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520multimodal%2520predictive%2520model%2520significantly%2520outperforms%2520state-of-the-art%2520work%2520ProgSG%2520by%2520up%2520to%252010.25%2524%255Ctimes%2524.%2520Furthermore%252C%2520in%2520DSE%2520tasks%252C%2520the%2520proposed%2520LLM4DSE%2520achieves%2520an%2520average%2520performance%2520gain%2520of%252039.90%255C%2525%2520over%2520prior%2520methods%252C%2520validating%2520the%2520effectiveness%2520of%2520our%2520prompting%2520methodology.%2520Code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/wslcccc/MPM-LLM4DSE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MPM-LLM4DSE%3A%20Reaching%20the%20Pareto%20Frontier%20in%20HLS%20with%20Multimodal%20Learning%20and%20LLM-Driven%20Exploration&entry.906535625=Lei%20Xu%20and%20Shanshan%20Wang%20and%20Chenglong%20Xiao&entry.1292438233=High-Level%20Synthesis%20%28HLS%29%20design%20space%20exploration%20%28DSE%29%20seeks%20Pareto-optimal%20designs%20within%20expansive%20pragma%20configuration%20spaces.%20To%20accelerate%20HLS%20DSE%2C%20graph%20neural%20networks%20%28GNNs%29%20are%20commonly%20employed%20as%20surrogates%20for%20HLS%20tools%20to%20predict%20quality%20of%20results%20%28QoR%29%20metrics%2C%20while%20multi-objective%20optimization%20algorithms%20expedite%20the%20exploration.%20However%2C%20GNN-based%20prediction%20methods%20may%20not%20fully%20capture%20the%20rich%20semantic%20features%20inherent%20in%20behavioral%20descriptions%2C%20and%20conventional%20multi-objective%20optimization%20algorithms%20often%20do%20not%20explicitly%20account%20for%20the%20domain-specific%20knowledge%20regarding%20how%20pragma%20directives%20influence%20QoR.%20To%20address%20these%20limitations%2C%20this%20paper%20proposes%20the%20MPM-LLM4DSE%20framework%2C%20which%20incorporates%20a%20multimodal%20prediction%20model%20%28MPM%29%20that%20simultaneously%20fuses%20features%20from%20behavioral%20descriptions%20and%20control%20and%20data%20flow%20graphs.%20Furthermore%2C%20the%20framework%20employs%20a%20large%20language%20model%20%28LLM%29%20as%20an%20optimizer%2C%20accompanied%20by%20a%20tailored%20prompt%20engineering%20methodology.%20This%20methodology%20incorporates%20pragma%20impact%20analysis%20on%20QoR%20to%20guide%20the%20LLM%20in%20generating%20high-quality%20configurations%20%28LLM4DSE%29.%20Experimental%20results%20demonstrate%20that%20our%20multimodal%20predictive%20model%20significantly%20outperforms%20state-of-the-art%20work%20ProgSG%20by%20up%20to%2010.25%24%5Ctimes%24.%20Furthermore%2C%20in%20DSE%20tasks%2C%20the%20proposed%20LLM4DSE%20achieves%20an%20average%20performance%20gain%20of%2039.90%5C%25%20over%20prior%20methods%2C%20validating%20the%20effectiveness%20of%20our%20prompting%20methodology.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/wslcccc/MPM-LLM4DSE.&entry.1838667208=http%3A//arxiv.org/abs/2601.04801v1&entry.124074799=Read"},
{"title": "Scalable neural pushbroom architectures for real-time denoising of hyperspectral images onboard satellites", "author": "Ziyao Yi and Davide Piccinini and Diego Valsesia and Tiziano Bianchi and Enrico Magli", "abstract": "The next generation of Earth observation satellites will seek to deploy intelligent models directly onboard the payload in order to minimize the latency incurred by the transmission and processing chain of the ground segment, for time-critical applications. Designing neural architectures for onboard execution, particularly for satellite-based hyperspectral imagers, poses novel challenges due to the unique constraints of this environment and imaging system that are largely unexplored by the traditional computer vision literature. In this paper, we show that this setting requires addressing three competing objectives, namely high-quality inference with low complexity, dynamic power scalability and fault tolerance. We focus on the problem of hyperspectral image denoising, which is a critical task to enable effective downstream inference, and highlights the constraints of the onboard processing scenario. We propose a neural network design that addresses the three aforementioned objectives with several novel contributions. In particular, we propose a mixture of denoisers that can be resilient to radiation-induced faults as well as allowing for time-varying power scaling. Moreover, each denoiser employs an innovative architecture where an image is processed line-by-line in a causal way, with a memory of past lines, in order to match the acquisition process of pushbroom hyperspectral sensors and greatly limit memory requirements. We show that the proposed architecture can run in real-time, i.e., process one line in the time it takes to acquire the next one, on low-power hardware and provide competitive denoising quality with respect to significantly more complex state-of-the-art models. We also show that the power scalability and fault tolerance objectives provide a design space with multiple tradeoffs between those properties and denoising quality.", "link": "http://arxiv.org/abs/2601.05020v1", "date": "2026-01-08", "relevancy": 2.1594, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5752}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5358}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20neural%20pushbroom%20architectures%20for%20real-time%20denoising%20of%20hyperspectral%20images%20onboard%20satellites&body=Title%3A%20Scalable%20neural%20pushbroom%20architectures%20for%20real-time%20denoising%20of%20hyperspectral%20images%20onboard%20satellites%0AAuthor%3A%20Ziyao%20Yi%20and%20Davide%20Piccinini%20and%20Diego%20Valsesia%20and%20Tiziano%20Bianchi%20and%20Enrico%20Magli%0AAbstract%3A%20The%20next%20generation%20of%20Earth%20observation%20satellites%20will%20seek%20to%20deploy%20intelligent%20models%20directly%20onboard%20the%20payload%20in%20order%20to%20minimize%20the%20latency%20incurred%20by%20the%20transmission%20and%20processing%20chain%20of%20the%20ground%20segment%2C%20for%20time-critical%20applications.%20Designing%20neural%20architectures%20for%20onboard%20execution%2C%20particularly%20for%20satellite-based%20hyperspectral%20imagers%2C%20poses%20novel%20challenges%20due%20to%20the%20unique%20constraints%20of%20this%20environment%20and%20imaging%20system%20that%20are%20largely%20unexplored%20by%20the%20traditional%20computer%20vision%20literature.%20In%20this%20paper%2C%20we%20show%20that%20this%20setting%20requires%20addressing%20three%20competing%20objectives%2C%20namely%20high-quality%20inference%20with%20low%20complexity%2C%20dynamic%20power%20scalability%20and%20fault%20tolerance.%20We%20focus%20on%20the%20problem%20of%20hyperspectral%20image%20denoising%2C%20which%20is%20a%20critical%20task%20to%20enable%20effective%20downstream%20inference%2C%20and%20highlights%20the%20constraints%20of%20the%20onboard%20processing%20scenario.%20We%20propose%20a%20neural%20network%20design%20that%20addresses%20the%20three%20aforementioned%20objectives%20with%20several%20novel%20contributions.%20In%20particular%2C%20we%20propose%20a%20mixture%20of%20denoisers%20that%20can%20be%20resilient%20to%20radiation-induced%20faults%20as%20well%20as%20allowing%20for%20time-varying%20power%20scaling.%20Moreover%2C%20each%20denoiser%20employs%20an%20innovative%20architecture%20where%20an%20image%20is%20processed%20line-by-line%20in%20a%20causal%20way%2C%20with%20a%20memory%20of%20past%20lines%2C%20in%20order%20to%20match%20the%20acquisition%20process%20of%20pushbroom%20hyperspectral%20sensors%20and%20greatly%20limit%20memory%20requirements.%20We%20show%20that%20the%20proposed%20architecture%20can%20run%20in%20real-time%2C%20i.e.%2C%20process%20one%20line%20in%20the%20time%20it%20takes%20to%20acquire%20the%20next%20one%2C%20on%20low-power%20hardware%20and%20provide%20competitive%20denoising%20quality%20with%20respect%20to%20significantly%20more%20complex%20state-of-the-art%20models.%20We%20also%20show%20that%20the%20power%20scalability%20and%20fault%20tolerance%20objectives%20provide%20a%20design%20space%20with%20multiple%20tradeoffs%20between%20those%20properties%20and%20denoising%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520neural%2520pushbroom%2520architectures%2520for%2520real-time%2520denoising%2520of%2520hyperspectral%2520images%2520onboard%2520satellites%26entry.906535625%3DZiyao%2520Yi%2520and%2520Davide%2520Piccinini%2520and%2520Diego%2520Valsesia%2520and%2520Tiziano%2520Bianchi%2520and%2520Enrico%2520Magli%26entry.1292438233%3DThe%2520next%2520generation%2520of%2520Earth%2520observation%2520satellites%2520will%2520seek%2520to%2520deploy%2520intelligent%2520models%2520directly%2520onboard%2520the%2520payload%2520in%2520order%2520to%2520minimize%2520the%2520latency%2520incurred%2520by%2520the%2520transmission%2520and%2520processing%2520chain%2520of%2520the%2520ground%2520segment%252C%2520for%2520time-critical%2520applications.%2520Designing%2520neural%2520architectures%2520for%2520onboard%2520execution%252C%2520particularly%2520for%2520satellite-based%2520hyperspectral%2520imagers%252C%2520poses%2520novel%2520challenges%2520due%2520to%2520the%2520unique%2520constraints%2520of%2520this%2520environment%2520and%2520imaging%2520system%2520that%2520are%2520largely%2520unexplored%2520by%2520the%2520traditional%2520computer%2520vision%2520literature.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520this%2520setting%2520requires%2520addressing%2520three%2520competing%2520objectives%252C%2520namely%2520high-quality%2520inference%2520with%2520low%2520complexity%252C%2520dynamic%2520power%2520scalability%2520and%2520fault%2520tolerance.%2520We%2520focus%2520on%2520the%2520problem%2520of%2520hyperspectral%2520image%2520denoising%252C%2520which%2520is%2520a%2520critical%2520task%2520to%2520enable%2520effective%2520downstream%2520inference%252C%2520and%2520highlights%2520the%2520constraints%2520of%2520the%2520onboard%2520processing%2520scenario.%2520We%2520propose%2520a%2520neural%2520network%2520design%2520that%2520addresses%2520the%2520three%2520aforementioned%2520objectives%2520with%2520several%2520novel%2520contributions.%2520In%2520particular%252C%2520we%2520propose%2520a%2520mixture%2520of%2520denoisers%2520that%2520can%2520be%2520resilient%2520to%2520radiation-induced%2520faults%2520as%2520well%2520as%2520allowing%2520for%2520time-varying%2520power%2520scaling.%2520Moreover%252C%2520each%2520denoiser%2520employs%2520an%2520innovative%2520architecture%2520where%2520an%2520image%2520is%2520processed%2520line-by-line%2520in%2520a%2520causal%2520way%252C%2520with%2520a%2520memory%2520of%2520past%2520lines%252C%2520in%2520order%2520to%2520match%2520the%2520acquisition%2520process%2520of%2520pushbroom%2520hyperspectral%2520sensors%2520and%2520greatly%2520limit%2520memory%2520requirements.%2520We%2520show%2520that%2520the%2520proposed%2520architecture%2520can%2520run%2520in%2520real-time%252C%2520i.e.%252C%2520process%2520one%2520line%2520in%2520the%2520time%2520it%2520takes%2520to%2520acquire%2520the%2520next%2520one%252C%2520on%2520low-power%2520hardware%2520and%2520provide%2520competitive%2520denoising%2520quality%2520with%2520respect%2520to%2520significantly%2520more%2520complex%2520state-of-the-art%2520models.%2520We%2520also%2520show%2520that%2520the%2520power%2520scalability%2520and%2520fault%2520tolerance%2520objectives%2520provide%2520a%2520design%2520space%2520with%2520multiple%2520tradeoffs%2520between%2520those%2520properties%2520and%2520denoising%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20neural%20pushbroom%20architectures%20for%20real-time%20denoising%20of%20hyperspectral%20images%20onboard%20satellites&entry.906535625=Ziyao%20Yi%20and%20Davide%20Piccinini%20and%20Diego%20Valsesia%20and%20Tiziano%20Bianchi%20and%20Enrico%20Magli&entry.1292438233=The%20next%20generation%20of%20Earth%20observation%20satellites%20will%20seek%20to%20deploy%20intelligent%20models%20directly%20onboard%20the%20payload%20in%20order%20to%20minimize%20the%20latency%20incurred%20by%20the%20transmission%20and%20processing%20chain%20of%20the%20ground%20segment%2C%20for%20time-critical%20applications.%20Designing%20neural%20architectures%20for%20onboard%20execution%2C%20particularly%20for%20satellite-based%20hyperspectral%20imagers%2C%20poses%20novel%20challenges%20due%20to%20the%20unique%20constraints%20of%20this%20environment%20and%20imaging%20system%20that%20are%20largely%20unexplored%20by%20the%20traditional%20computer%20vision%20literature.%20In%20this%20paper%2C%20we%20show%20that%20this%20setting%20requires%20addressing%20three%20competing%20objectives%2C%20namely%20high-quality%20inference%20with%20low%20complexity%2C%20dynamic%20power%20scalability%20and%20fault%20tolerance.%20We%20focus%20on%20the%20problem%20of%20hyperspectral%20image%20denoising%2C%20which%20is%20a%20critical%20task%20to%20enable%20effective%20downstream%20inference%2C%20and%20highlights%20the%20constraints%20of%20the%20onboard%20processing%20scenario.%20We%20propose%20a%20neural%20network%20design%20that%20addresses%20the%20three%20aforementioned%20objectives%20with%20several%20novel%20contributions.%20In%20particular%2C%20we%20propose%20a%20mixture%20of%20denoisers%20that%20can%20be%20resilient%20to%20radiation-induced%20faults%20as%20well%20as%20allowing%20for%20time-varying%20power%20scaling.%20Moreover%2C%20each%20denoiser%20employs%20an%20innovative%20architecture%20where%20an%20image%20is%20processed%20line-by-line%20in%20a%20causal%20way%2C%20with%20a%20memory%20of%20past%20lines%2C%20in%20order%20to%20match%20the%20acquisition%20process%20of%20pushbroom%20hyperspectral%20sensors%20and%20greatly%20limit%20memory%20requirements.%20We%20show%20that%20the%20proposed%20architecture%20can%20run%20in%20real-time%2C%20i.e.%2C%20process%20one%20line%20in%20the%20time%20it%20takes%20to%20acquire%20the%20next%20one%2C%20on%20low-power%20hardware%20and%20provide%20competitive%20denoising%20quality%20with%20respect%20to%20significantly%20more%20complex%20state-of-the-art%20models.%20We%20also%20show%20that%20the%20power%20scalability%20and%20fault%20tolerance%20objectives%20provide%20a%20design%20space%20with%20multiple%20tradeoffs%20between%20those%20properties%20and%20denoising%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2601.05020v1&entry.124074799=Read"},
{"title": "DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation", "author": "Renzhao Liang and Jingru Chen and Bo Jia and Bo Deng and Chenggang Xie and Yidong Wang and Ke Jin and Xin Wang and Linfeng Zhang and Cunxiang Wang", "abstract": "Evaluating large language models (LLMs) is increasingly confounded by \\emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \\textbf{DVD} (\\textbf{D}etection via \\textbf{V}ariance of generation \\textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \\emph{memory-adherence} state and a \\emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \\textbf{DVD} consistently outperforms perplexity-based, Min-$k$\\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.", "link": "http://arxiv.org/abs/2601.04895v1", "date": "2026-01-08", "relevancy": 2.1137, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5418}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DVD%3A%20A%20Robust%20Method%20for%20Detecting%20Variant%20Contamination%20in%20Large%20Language%20Model%20Evaluation&body=Title%3A%20DVD%3A%20A%20Robust%20Method%20for%20Detecting%20Variant%20Contamination%20in%20Large%20Language%20Model%20Evaluation%0AAuthor%3A%20Renzhao%20Liang%20and%20Jingru%20Chen%20and%20Bo%20Jia%20and%20Bo%20Deng%20and%20Chenggang%20Xie%20and%20Yidong%20Wang%20and%20Ke%20Jin%20and%20Xin%20Wang%20and%20Linfeng%20Zhang%20and%20Cunxiang%20Wang%0AAbstract%3A%20Evaluating%20large%20language%20models%20%28LLMs%29%20is%20increasingly%20confounded%20by%20%5Cemph%7Bvariant%20contamination%7D%3A%20the%20training%20corpus%20contains%20semantically%20equivalent%20yet%20lexically%20or%20syntactically%20altered%20versions%20of%20test%20items.%20Unlike%20verbatim%20leakage%2C%20these%20paraphrased%20or%20structurally%20transformed%20variants%20evade%20existing%20detectors%20based%20on%20sampling%20consistency%20or%20perplexity%2C%20thereby%20inflating%20benchmark%20scores%20via%20memorization%20rather%20than%20genuine%20reasoning.%20We%20formalize%20this%20problem%20and%20introduce%20%5Ctextbf%7BDVD%7D%20%28%5Ctextbf%7BD%7Detection%20via%20%5Ctextbf%7BV%7Dariance%20of%20generation%20%5Ctextbf%7BD%7Distribution%29%2C%20a%20single-sample%20detector%20that%20models%20the%20local%20output%20distribution%20induced%20by%20temperature%20sampling.%20Our%20key%20insight%20is%20that%20contaminated%20items%20trigger%20alternation%20between%20a%20%5Cemph%7Bmemory-adherence%7D%20state%20and%20a%20%5Cemph%7Bperturbation-drift%7D%20state%2C%20yielding%20abnormally%20high%20variance%20in%20the%20synthetic%20difficulty%20of%20low-probability%20tokens%3B%20uncontaminated%20items%20remain%20in%20drift%20with%20comparatively%20smooth%20variance.%20We%20construct%20the%20first%20benchmark%20for%20variant%20contamination%20across%20two%20domains%20Omni-MATH%20and%20SuperGPQA%20by%20generating%20and%20filtering%20semantically%20equivalent%20variants%2C%20and%20simulate%20contamination%20via%20fine-tuning%20models%20of%20different%20scales%20and%20architectures%20%28Qwen2.5%20and%20Llama3.1%29.%20Across%20datasets%20and%20models%2C%20%5Ctextbf%7BDVD%7D%20consistently%20outperforms%20perplexity-based%2C%20Min-%24k%24%5C%25%2B%2B%2C%20edit-distance%20%28CDD%29%2C%20and%20embedding-similarity%20baselines%2C%20while%20exhibiting%20strong%20robustness%20to%20hyperparameters.%20Our%20results%20establish%20variance%20of%20the%20generation%20distribution%20as%20a%20principled%20and%20practical%20fingerprint%20for%20detecting%20variant%20contamination%20in%20LLM%20evaluation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDVD%253A%2520A%2520Robust%2520Method%2520for%2520Detecting%2520Variant%2520Contamination%2520in%2520Large%2520Language%2520Model%2520Evaluation%26entry.906535625%3DRenzhao%2520Liang%2520and%2520Jingru%2520Chen%2520and%2520Bo%2520Jia%2520and%2520Bo%2520Deng%2520and%2520Chenggang%2520Xie%2520and%2520Yidong%2520Wang%2520and%2520Ke%2520Jin%2520and%2520Xin%2520Wang%2520and%2520Linfeng%2520Zhang%2520and%2520Cunxiang%2520Wang%26entry.1292438233%3DEvaluating%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520increasingly%2520confounded%2520by%2520%255Cemph%257Bvariant%2520contamination%257D%253A%2520the%2520training%2520corpus%2520contains%2520semantically%2520equivalent%2520yet%2520lexically%2520or%2520syntactically%2520altered%2520versions%2520of%2520test%2520items.%2520Unlike%2520verbatim%2520leakage%252C%2520these%2520paraphrased%2520or%2520structurally%2520transformed%2520variants%2520evade%2520existing%2520detectors%2520based%2520on%2520sampling%2520consistency%2520or%2520perplexity%252C%2520thereby%2520inflating%2520benchmark%2520scores%2520via%2520memorization%2520rather%2520than%2520genuine%2520reasoning.%2520We%2520formalize%2520this%2520problem%2520and%2520introduce%2520%255Ctextbf%257BDVD%257D%2520%2528%255Ctextbf%257BD%257Detection%2520via%2520%255Ctextbf%257BV%257Dariance%2520of%2520generation%2520%255Ctextbf%257BD%257Distribution%2529%252C%2520a%2520single-sample%2520detector%2520that%2520models%2520the%2520local%2520output%2520distribution%2520induced%2520by%2520temperature%2520sampling.%2520Our%2520key%2520insight%2520is%2520that%2520contaminated%2520items%2520trigger%2520alternation%2520between%2520a%2520%255Cemph%257Bmemory-adherence%257D%2520state%2520and%2520a%2520%255Cemph%257Bperturbation-drift%257D%2520state%252C%2520yielding%2520abnormally%2520high%2520variance%2520in%2520the%2520synthetic%2520difficulty%2520of%2520low-probability%2520tokens%253B%2520uncontaminated%2520items%2520remain%2520in%2520drift%2520with%2520comparatively%2520smooth%2520variance.%2520We%2520construct%2520the%2520first%2520benchmark%2520for%2520variant%2520contamination%2520across%2520two%2520domains%2520Omni-MATH%2520and%2520SuperGPQA%2520by%2520generating%2520and%2520filtering%2520semantically%2520equivalent%2520variants%252C%2520and%2520simulate%2520contamination%2520via%2520fine-tuning%2520models%2520of%2520different%2520scales%2520and%2520architectures%2520%2528Qwen2.5%2520and%2520Llama3.1%2529.%2520Across%2520datasets%2520and%2520models%252C%2520%255Ctextbf%257BDVD%257D%2520consistently%2520outperforms%2520perplexity-based%252C%2520Min-%2524k%2524%255C%2525%252B%252B%252C%2520edit-distance%2520%2528CDD%2529%252C%2520and%2520embedding-similarity%2520baselines%252C%2520while%2520exhibiting%2520strong%2520robustness%2520to%2520hyperparameters.%2520Our%2520results%2520establish%2520variance%2520of%2520the%2520generation%2520distribution%2520as%2520a%2520principled%2520and%2520practical%2520fingerprint%2520for%2520detecting%2520variant%2520contamination%2520in%2520LLM%2520evaluation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DVD%3A%20A%20Robust%20Method%20for%20Detecting%20Variant%20Contamination%20in%20Large%20Language%20Model%20Evaluation&entry.906535625=Renzhao%20Liang%20and%20Jingru%20Chen%20and%20Bo%20Jia%20and%20Bo%20Deng%20and%20Chenggang%20Xie%20and%20Yidong%20Wang%20and%20Ke%20Jin%20and%20Xin%20Wang%20and%20Linfeng%20Zhang%20and%20Cunxiang%20Wang&entry.1292438233=Evaluating%20large%20language%20models%20%28LLMs%29%20is%20increasingly%20confounded%20by%20%5Cemph%7Bvariant%20contamination%7D%3A%20the%20training%20corpus%20contains%20semantically%20equivalent%20yet%20lexically%20or%20syntactically%20altered%20versions%20of%20test%20items.%20Unlike%20verbatim%20leakage%2C%20these%20paraphrased%20or%20structurally%20transformed%20variants%20evade%20existing%20detectors%20based%20on%20sampling%20consistency%20or%20perplexity%2C%20thereby%20inflating%20benchmark%20scores%20via%20memorization%20rather%20than%20genuine%20reasoning.%20We%20formalize%20this%20problem%20and%20introduce%20%5Ctextbf%7BDVD%7D%20%28%5Ctextbf%7BD%7Detection%20via%20%5Ctextbf%7BV%7Dariance%20of%20generation%20%5Ctextbf%7BD%7Distribution%29%2C%20a%20single-sample%20detector%20that%20models%20the%20local%20output%20distribution%20induced%20by%20temperature%20sampling.%20Our%20key%20insight%20is%20that%20contaminated%20items%20trigger%20alternation%20between%20a%20%5Cemph%7Bmemory-adherence%7D%20state%20and%20a%20%5Cemph%7Bperturbation-drift%7D%20state%2C%20yielding%20abnormally%20high%20variance%20in%20the%20synthetic%20difficulty%20of%20low-probability%20tokens%3B%20uncontaminated%20items%20remain%20in%20drift%20with%20comparatively%20smooth%20variance.%20We%20construct%20the%20first%20benchmark%20for%20variant%20contamination%20across%20two%20domains%20Omni-MATH%20and%20SuperGPQA%20by%20generating%20and%20filtering%20semantically%20equivalent%20variants%2C%20and%20simulate%20contamination%20via%20fine-tuning%20models%20of%20different%20scales%20and%20architectures%20%28Qwen2.5%20and%20Llama3.1%29.%20Across%20datasets%20and%20models%2C%20%5Ctextbf%7BDVD%7D%20consistently%20outperforms%20perplexity-based%2C%20Min-%24k%24%5C%25%2B%2B%2C%20edit-distance%20%28CDD%29%2C%20and%20embedding-similarity%20baselines%2C%20while%20exhibiting%20strong%20robustness%20to%20hyperparameters.%20Our%20results%20establish%20variance%20of%20the%20generation%20distribution%20as%20a%20principled%20and%20practical%20fingerprint%20for%20detecting%20variant%20contamination%20in%20LLM%20evaluation.&entry.1838667208=http%3A//arxiv.org/abs/2601.04895v1&entry.124074799=Read"},
{"title": "AMAP Agentic Planning Technical Report", "author": " AMAP AI Agent Team and Yulan Hu and Xiangwen Zhang and Sheng Ouyang and Hao Yi and Lu Xu and Qinglin Lang and Lide Tan and Xiang Cheng and Tianchen Ye and Zhicong Li and Ge Chen and Wenjin Yang and Zheng Pan and Shaopan Xiong and Siran Yang and Ju Huang and Yan Zhang and Jiamang Wang and Yong Liu and Yinfeng Huang and Ning Wang and Tucheng Lin and Xin Li and Ning Guo", "abstract": "We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries by retaining less than 1\\% of the raw data, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.", "link": "http://arxiv.org/abs/2512.24957v2", "date": "2026-01-08", "relevancy": 2.1032, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5592}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.531}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMAP%20Agentic%20Planning%20Technical%20Report&body=Title%3A%20AMAP%20Agentic%20Planning%20Technical%20Report%0AAuthor%3A%20%20AMAP%20AI%20Agent%20Team%20and%20Yulan%20Hu%20and%20Xiangwen%20Zhang%20and%20Sheng%20Ouyang%20and%20Hao%20Yi%20and%20Lu%20Xu%20and%20Qinglin%20Lang%20and%20Lide%20Tan%20and%20Xiang%20Cheng%20and%20Tianchen%20Ye%20and%20Zhicong%20Li%20and%20Ge%20Chen%20and%20Wenjin%20Yang%20and%20Zheng%20Pan%20and%20Shaopan%20Xiong%20and%20Siran%20Yang%20and%20Ju%20Huang%20and%20Yan%20Zhang%20and%20Jiamang%20Wang%20and%20Yong%20Liu%20and%20Yinfeng%20Huang%20and%20Ning%20Wang%20and%20Tucheng%20Lin%20and%20Xin%20Li%20and%20Ning%20Guo%0AAbstract%3A%20We%20present%20STAgent%2C%20an%20agentic%20large%20language%20model%20tailored%20for%20spatio-temporal%20understanding%2C%20designed%20to%20solve%20complex%20tasks%20such%20as%20constrained%20point-of-interest%20discovery%20and%20itinerary%20planning.%20STAgent%20is%20a%20specialized%20model%20capable%20of%20interacting%20with%20ten%20distinct%20tools%20within%20spatio-temporal%20scenarios%2C%20enabling%20it%20to%20explore%2C%20verify%2C%20and%20refine%20intermediate%20steps%20during%20complex%20reasoning.%20Notably%2C%20STAgent%20effectively%20preserves%20its%20general%20capabilities.%20We%20empower%20STAgent%20with%20these%20capabilities%20through%20three%20key%20contributions%3A%20%281%29%20a%20stable%20tool%20environment%20that%20supports%20over%20ten%20domain-specific%20tools%2C%20enabling%20asynchronous%20rollout%20and%20training%3B%20%282%29%20a%20hierarchical%20data%20curation%20framework%20that%20identifies%20high-quality%20data%20like%20a%20needle%20in%20a%20haystack%2C%20curating%20high-quality%20queries%20by%20retaining%20less%20than%201%5C%25%20of%20the%20raw%20data%2C%20emphasizing%20both%20diversity%20and%20difficulty%3B%20and%20%283%29%20a%20cascaded%20training%20recipe%20that%20starts%20with%20a%20seed%20SFT%20stage%20acting%20as%20a%20guardian%20to%20measure%20query%20difficulty%2C%20followed%20by%20a%20second%20SFT%20stage%20fine-tuned%20on%20queries%20with%20high%20certainty%2C%20and%20an%20ultimate%20RL%20stage%20that%20leverages%20data%20of%20low%20certainty.%20Initialized%20with%20Qwen3-30B-A3B%20to%20establish%20a%20strong%20SFT%20foundation%20and%20leverage%20insights%20into%20sample%20difficulty%2C%20STAgent%20yields%20promising%20performance%20on%20TravelBench%20while%20maintaining%20its%20general%20capabilities%20across%20a%20wide%20range%20of%20general%20benchmarks%2C%20thereby%20demonstrating%20the%20effectiveness%20of%20our%20proposed%20agentic%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2512.24957v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMAP%2520Agentic%2520Planning%2520Technical%2520Report%26entry.906535625%3D%2520AMAP%2520AI%2520Agent%2520Team%2520and%2520Yulan%2520Hu%2520and%2520Xiangwen%2520Zhang%2520and%2520Sheng%2520Ouyang%2520and%2520Hao%2520Yi%2520and%2520Lu%2520Xu%2520and%2520Qinglin%2520Lang%2520and%2520Lide%2520Tan%2520and%2520Xiang%2520Cheng%2520and%2520Tianchen%2520Ye%2520and%2520Zhicong%2520Li%2520and%2520Ge%2520Chen%2520and%2520Wenjin%2520Yang%2520and%2520Zheng%2520Pan%2520and%2520Shaopan%2520Xiong%2520and%2520Siran%2520Yang%2520and%2520Ju%2520Huang%2520and%2520Yan%2520Zhang%2520and%2520Jiamang%2520Wang%2520and%2520Yong%2520Liu%2520and%2520Yinfeng%2520Huang%2520and%2520Ning%2520Wang%2520and%2520Tucheng%2520Lin%2520and%2520Xin%2520Li%2520and%2520Ning%2520Guo%26entry.1292438233%3DWe%2520present%2520STAgent%252C%2520an%2520agentic%2520large%2520language%2520model%2520tailored%2520for%2520spatio-temporal%2520understanding%252C%2520designed%2520to%2520solve%2520complex%2520tasks%2520such%2520as%2520constrained%2520point-of-interest%2520discovery%2520and%2520itinerary%2520planning.%2520STAgent%2520is%2520a%2520specialized%2520model%2520capable%2520of%2520interacting%2520with%2520ten%2520distinct%2520tools%2520within%2520spatio-temporal%2520scenarios%252C%2520enabling%2520it%2520to%2520explore%252C%2520verify%252C%2520and%2520refine%2520intermediate%2520steps%2520during%2520complex%2520reasoning.%2520Notably%252C%2520STAgent%2520effectively%2520preserves%2520its%2520general%2520capabilities.%2520We%2520empower%2520STAgent%2520with%2520these%2520capabilities%2520through%2520three%2520key%2520contributions%253A%2520%25281%2529%2520a%2520stable%2520tool%2520environment%2520that%2520supports%2520over%2520ten%2520domain-specific%2520tools%252C%2520enabling%2520asynchronous%2520rollout%2520and%2520training%253B%2520%25282%2529%2520a%2520hierarchical%2520data%2520curation%2520framework%2520that%2520identifies%2520high-quality%2520data%2520like%2520a%2520needle%2520in%2520a%2520haystack%252C%2520curating%2520high-quality%2520queries%2520by%2520retaining%2520less%2520than%25201%255C%2525%2520of%2520the%2520raw%2520data%252C%2520emphasizing%2520both%2520diversity%2520and%2520difficulty%253B%2520and%2520%25283%2529%2520a%2520cascaded%2520training%2520recipe%2520that%2520starts%2520with%2520a%2520seed%2520SFT%2520stage%2520acting%2520as%2520a%2520guardian%2520to%2520measure%2520query%2520difficulty%252C%2520followed%2520by%2520a%2520second%2520SFT%2520stage%2520fine-tuned%2520on%2520queries%2520with%2520high%2520certainty%252C%2520and%2520an%2520ultimate%2520RL%2520stage%2520that%2520leverages%2520data%2520of%2520low%2520certainty.%2520Initialized%2520with%2520Qwen3-30B-A3B%2520to%2520establish%2520a%2520strong%2520SFT%2520foundation%2520and%2520leverage%2520insights%2520into%2520sample%2520difficulty%252C%2520STAgent%2520yields%2520promising%2520performance%2520on%2520TravelBench%2520while%2520maintaining%2520its%2520general%2520capabilities%2520across%2520a%2520wide%2520range%2520of%2520general%2520benchmarks%252C%2520thereby%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520proposed%2520agentic%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.24957v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMAP%20Agentic%20Planning%20Technical%20Report&entry.906535625=%20AMAP%20AI%20Agent%20Team%20and%20Yulan%20Hu%20and%20Xiangwen%20Zhang%20and%20Sheng%20Ouyang%20and%20Hao%20Yi%20and%20Lu%20Xu%20and%20Qinglin%20Lang%20and%20Lide%20Tan%20and%20Xiang%20Cheng%20and%20Tianchen%20Ye%20and%20Zhicong%20Li%20and%20Ge%20Chen%20and%20Wenjin%20Yang%20and%20Zheng%20Pan%20and%20Shaopan%20Xiong%20and%20Siran%20Yang%20and%20Ju%20Huang%20and%20Yan%20Zhang%20and%20Jiamang%20Wang%20and%20Yong%20Liu%20and%20Yinfeng%20Huang%20and%20Ning%20Wang%20and%20Tucheng%20Lin%20and%20Xin%20Li%20and%20Ning%20Guo&entry.1292438233=We%20present%20STAgent%2C%20an%20agentic%20large%20language%20model%20tailored%20for%20spatio-temporal%20understanding%2C%20designed%20to%20solve%20complex%20tasks%20such%20as%20constrained%20point-of-interest%20discovery%20and%20itinerary%20planning.%20STAgent%20is%20a%20specialized%20model%20capable%20of%20interacting%20with%20ten%20distinct%20tools%20within%20spatio-temporal%20scenarios%2C%20enabling%20it%20to%20explore%2C%20verify%2C%20and%20refine%20intermediate%20steps%20during%20complex%20reasoning.%20Notably%2C%20STAgent%20effectively%20preserves%20its%20general%20capabilities.%20We%20empower%20STAgent%20with%20these%20capabilities%20through%20three%20key%20contributions%3A%20%281%29%20a%20stable%20tool%20environment%20that%20supports%20over%20ten%20domain-specific%20tools%2C%20enabling%20asynchronous%20rollout%20and%20training%3B%20%282%29%20a%20hierarchical%20data%20curation%20framework%20that%20identifies%20high-quality%20data%20like%20a%20needle%20in%20a%20haystack%2C%20curating%20high-quality%20queries%20by%20retaining%20less%20than%201%5C%25%20of%20the%20raw%20data%2C%20emphasizing%20both%20diversity%20and%20difficulty%3B%20and%20%283%29%20a%20cascaded%20training%20recipe%20that%20starts%20with%20a%20seed%20SFT%20stage%20acting%20as%20a%20guardian%20to%20measure%20query%20difficulty%2C%20followed%20by%20a%20second%20SFT%20stage%20fine-tuned%20on%20queries%20with%20high%20certainty%2C%20and%20an%20ultimate%20RL%20stage%20that%20leverages%20data%20of%20low%20certainty.%20Initialized%20with%20Qwen3-30B-A3B%20to%20establish%20a%20strong%20SFT%20foundation%20and%20leverage%20insights%20into%20sample%20difficulty%2C%20STAgent%20yields%20promising%20performance%20on%20TravelBench%20while%20maintaining%20its%20general%20capabilities%20across%20a%20wide%20range%20of%20general%20benchmarks%2C%20thereby%20demonstrating%20the%20effectiveness%20of%20our%20proposed%20agentic%20model.&entry.1838667208=http%3A//arxiv.org/abs/2512.24957v2&entry.124074799=Read"},
{"title": "SRU-Pix2Pix: A Fusion-Driven Generator Network for Medical Image Translation with Few-Shot Learning", "author": "Xihe Qiu and Yang Dai and Xiaoyu Tan and Sijia Li and Fenghao Sun and Lu Gan and Liang Liu", "abstract": "Magnetic Resonance Imaging (MRI) provides detailed tissue information, but its clinical application is limited by long acquisition time, high cost, and restricted resolution. Image translation has recently gained attention as a strategy to address these limitations. Although Pix2Pix has been widely applied in medical image translation, its potential has not been fully explored. In this study, we propose an enhanced Pix2Pix framework that integrates Squeeze-and-Excitation Residual Networks (SEResNet) and U-Net++ to improve image generation quality and structural fidelity. SEResNet strengthens critical feature representation through channel attention, while U-Net++ enhances multi-scale feature fusion. A simplified PatchGAN discriminator further stabilizes training and refines local anatomical realism. Experimental results demonstrate that under few-shot conditions with fewer than 500 images, the proposed method achieves consistent structural fidelity and superior image quality across multiple intra-modality MRI translation tasks, showing strong generalization ability. These results suggest an effective extension of Pix2Pix for medical image translation.", "link": "http://arxiv.org/abs/2601.04785v1", "date": "2026-01-08", "relevancy": 2.0889, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5321}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5283}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRU-Pix2Pix%3A%20A%20Fusion-Driven%20Generator%20Network%20for%20Medical%20Image%20Translation%20with%20Few-Shot%20Learning&body=Title%3A%20SRU-Pix2Pix%3A%20A%20Fusion-Driven%20Generator%20Network%20for%20Medical%20Image%20Translation%20with%20Few-Shot%20Learning%0AAuthor%3A%20Xihe%20Qiu%20and%20Yang%20Dai%20and%20Xiaoyu%20Tan%20and%20Sijia%20Li%20and%20Fenghao%20Sun%20and%20Lu%20Gan%20and%20Liang%20Liu%0AAbstract%3A%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20provides%20detailed%20tissue%20information%2C%20but%20its%20clinical%20application%20is%20limited%20by%20long%20acquisition%20time%2C%20high%20cost%2C%20and%20restricted%20resolution.%20Image%20translation%20has%20recently%20gained%20attention%20as%20a%20strategy%20to%20address%20these%20limitations.%20Although%20Pix2Pix%20has%20been%20widely%20applied%20in%20medical%20image%20translation%2C%20its%20potential%20has%20not%20been%20fully%20explored.%20In%20this%20study%2C%20we%20propose%20an%20enhanced%20Pix2Pix%20framework%20that%20integrates%20Squeeze-and-Excitation%20Residual%20Networks%20%28SEResNet%29%20and%20U-Net%2B%2B%20to%20improve%20image%20generation%20quality%20and%20structural%20fidelity.%20SEResNet%20strengthens%20critical%20feature%20representation%20through%20channel%20attention%2C%20while%20U-Net%2B%2B%20enhances%20multi-scale%20feature%20fusion.%20A%20simplified%20PatchGAN%20discriminator%20further%20stabilizes%20training%20and%20refines%20local%20anatomical%20realism.%20Experimental%20results%20demonstrate%20that%20under%20few-shot%20conditions%20with%20fewer%20than%20500%20images%2C%20the%20proposed%20method%20achieves%20consistent%20structural%20fidelity%20and%20superior%20image%20quality%20across%20multiple%20intra-modality%20MRI%20translation%20tasks%2C%20showing%20strong%20generalization%20ability.%20These%20results%20suggest%20an%20effective%20extension%20of%20Pix2Pix%20for%20medical%20image%20translation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRU-Pix2Pix%253A%2520A%2520Fusion-Driven%2520Generator%2520Network%2520for%2520Medical%2520Image%2520Translation%2520with%2520Few-Shot%2520Learning%26entry.906535625%3DXihe%2520Qiu%2520and%2520Yang%2520Dai%2520and%2520Xiaoyu%2520Tan%2520and%2520Sijia%2520Li%2520and%2520Fenghao%2520Sun%2520and%2520Lu%2520Gan%2520and%2520Liang%2520Liu%26entry.1292438233%3DMagnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520provides%2520detailed%2520tissue%2520information%252C%2520but%2520its%2520clinical%2520application%2520is%2520limited%2520by%2520long%2520acquisition%2520time%252C%2520high%2520cost%252C%2520and%2520restricted%2520resolution.%2520Image%2520translation%2520has%2520recently%2520gained%2520attention%2520as%2520a%2520strategy%2520to%2520address%2520these%2520limitations.%2520Although%2520Pix2Pix%2520has%2520been%2520widely%2520applied%2520in%2520medical%2520image%2520translation%252C%2520its%2520potential%2520has%2520not%2520been%2520fully%2520explored.%2520In%2520this%2520study%252C%2520we%2520propose%2520an%2520enhanced%2520Pix2Pix%2520framework%2520that%2520integrates%2520Squeeze-and-Excitation%2520Residual%2520Networks%2520%2528SEResNet%2529%2520and%2520U-Net%252B%252B%2520to%2520improve%2520image%2520generation%2520quality%2520and%2520structural%2520fidelity.%2520SEResNet%2520strengthens%2520critical%2520feature%2520representation%2520through%2520channel%2520attention%252C%2520while%2520U-Net%252B%252B%2520enhances%2520multi-scale%2520feature%2520fusion.%2520A%2520simplified%2520PatchGAN%2520discriminator%2520further%2520stabilizes%2520training%2520and%2520refines%2520local%2520anatomical%2520realism.%2520Experimental%2520results%2520demonstrate%2520that%2520under%2520few-shot%2520conditions%2520with%2520fewer%2520than%2520500%2520images%252C%2520the%2520proposed%2520method%2520achieves%2520consistent%2520structural%2520fidelity%2520and%2520superior%2520image%2520quality%2520across%2520multiple%2520intra-modality%2520MRI%2520translation%2520tasks%252C%2520showing%2520strong%2520generalization%2520ability.%2520These%2520results%2520suggest%2520an%2520effective%2520extension%2520of%2520Pix2Pix%2520for%2520medical%2520image%2520translation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRU-Pix2Pix%3A%20A%20Fusion-Driven%20Generator%20Network%20for%20Medical%20Image%20Translation%20with%20Few-Shot%20Learning&entry.906535625=Xihe%20Qiu%20and%20Yang%20Dai%20and%20Xiaoyu%20Tan%20and%20Sijia%20Li%20and%20Fenghao%20Sun%20and%20Lu%20Gan%20and%20Liang%20Liu&entry.1292438233=Magnetic%20Resonance%20Imaging%20%28MRI%29%20provides%20detailed%20tissue%20information%2C%20but%20its%20clinical%20application%20is%20limited%20by%20long%20acquisition%20time%2C%20high%20cost%2C%20and%20restricted%20resolution.%20Image%20translation%20has%20recently%20gained%20attention%20as%20a%20strategy%20to%20address%20these%20limitations.%20Although%20Pix2Pix%20has%20been%20widely%20applied%20in%20medical%20image%20translation%2C%20its%20potential%20has%20not%20been%20fully%20explored.%20In%20this%20study%2C%20we%20propose%20an%20enhanced%20Pix2Pix%20framework%20that%20integrates%20Squeeze-and-Excitation%20Residual%20Networks%20%28SEResNet%29%20and%20U-Net%2B%2B%20to%20improve%20image%20generation%20quality%20and%20structural%20fidelity.%20SEResNet%20strengthens%20critical%20feature%20representation%20through%20channel%20attention%2C%20while%20U-Net%2B%2B%20enhances%20multi-scale%20feature%20fusion.%20A%20simplified%20PatchGAN%20discriminator%20further%20stabilizes%20training%20and%20refines%20local%20anatomical%20realism.%20Experimental%20results%20demonstrate%20that%20under%20few-shot%20conditions%20with%20fewer%20than%20500%20images%2C%20the%20proposed%20method%20achieves%20consistent%20structural%20fidelity%20and%20superior%20image%20quality%20across%20multiple%20intra-modality%20MRI%20translation%20tasks%2C%20showing%20strong%20generalization%20ability.%20These%20results%20suggest%20an%20effective%20extension%20of%20Pix2Pix%20for%20medical%20image%20translation.&entry.1838667208=http%3A//arxiv.org/abs/2601.04785v1&entry.124074799=Read"},
{"title": "Multi-Modal AI for Remote Patient Monitoring in Cancer Care", "author": "Yansong Liu and Ronnie Stafford and Pramit Khetrapal and Huriye Kocadag and Gra\u00e7a Carvalho and Patricia de Winter and Maryam Imran and Amelia Snook and Adamos Hadjivasiliou and D. Vijay Anand and Weining Lin and John Kelly and Yukun Zhou and Ivana Drobnjak", "abstract": "For patients undergoing systemic cancer therapy, the time between clinic visits is full of uncertainties and risks of unmonitored side effects. To bridge this gap in care, we developed and prospectively trialed a multi-modal AI framework for remote patient monitoring (RPM). This system integrates multi-modal data from the HALO-X platform, such as demographics, wearable sensors, daily surveys, and clinical events. Our observational trial is one of the largest of its kind and has collected over 2.1 million data points (6,080 patient-days) of monitoring from 84 patients. We developed and adapted a multi-modal AI model to handle the asynchronous and incomplete nature of real-world RPM data, forecasting a continuous risk of future adverse events. The model achieved an accuracy of 83.9% (AUROC=0.70). Notably, the model identified previous treatments, wellness check-ins, and daily maximum heart rate as key predictive features. A case study demonstrated the model's ability to provide early warnings by outputting escalating risk profiles prior to the event. This work establishes the feasibility of multi-modal AI RPM for cancer care and offers a path toward more proactive patient support.(Accepted at Europe NeurIPS 2025 Multimodal Representation Learning for Healthcare Workshop. Best Paper Poster Award.)", "link": "http://arxiv.org/abs/2512.00949v2", "date": "2026-01-08", "relevancy": 2.0862, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5538}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5379}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20AI%20for%20Remote%20Patient%20Monitoring%20in%20Cancer%20Care&body=Title%3A%20Multi-Modal%20AI%20for%20Remote%20Patient%20Monitoring%20in%20Cancer%20Care%0AAuthor%3A%20Yansong%20Liu%20and%20Ronnie%20Stafford%20and%20Pramit%20Khetrapal%20and%20Huriye%20Kocadag%20and%20Gra%C3%A7a%20Carvalho%20and%20Patricia%20de%20Winter%20and%20Maryam%20Imran%20and%20Amelia%20Snook%20and%20Adamos%20Hadjivasiliou%20and%20D.%20Vijay%20Anand%20and%20Weining%20Lin%20and%20John%20Kelly%20and%20Yukun%20Zhou%20and%20Ivana%20Drobnjak%0AAbstract%3A%20For%20patients%20undergoing%20systemic%20cancer%20therapy%2C%20the%20time%20between%20clinic%20visits%20is%20full%20of%20uncertainties%20and%20risks%20of%20unmonitored%20side%20effects.%20To%20bridge%20this%20gap%20in%20care%2C%20we%20developed%20and%20prospectively%20trialed%20a%20multi-modal%20AI%20framework%20for%20remote%20patient%20monitoring%20%28RPM%29.%20This%20system%20integrates%20multi-modal%20data%20from%20the%20HALO-X%20platform%2C%20such%20as%20demographics%2C%20wearable%20sensors%2C%20daily%20surveys%2C%20and%20clinical%20events.%20Our%20observational%20trial%20is%20one%20of%20the%20largest%20of%20its%20kind%20and%20has%20collected%20over%202.1%20million%20data%20points%20%286%2C080%20patient-days%29%20of%20monitoring%20from%2084%20patients.%20We%20developed%20and%20adapted%20a%20multi-modal%20AI%20model%20to%20handle%20the%20asynchronous%20and%20incomplete%20nature%20of%20real-world%20RPM%20data%2C%20forecasting%20a%20continuous%20risk%20of%20future%20adverse%20events.%20The%20model%20achieved%20an%20accuracy%20of%2083.9%25%20%28AUROC%3D0.70%29.%20Notably%2C%20the%20model%20identified%20previous%20treatments%2C%20wellness%20check-ins%2C%20and%20daily%20maximum%20heart%20rate%20as%20key%20predictive%20features.%20A%20case%20study%20demonstrated%20the%20model%27s%20ability%20to%20provide%20early%20warnings%20by%20outputting%20escalating%20risk%20profiles%20prior%20to%20the%20event.%20This%20work%20establishes%20the%20feasibility%20of%20multi-modal%20AI%20RPM%20for%20cancer%20care%20and%20offers%20a%20path%20toward%20more%20proactive%20patient%20support.%28Accepted%20at%20Europe%20NeurIPS%202025%20Multimodal%20Representation%20Learning%20for%20Healthcare%20Workshop.%20Best%20Paper%20Poster%20Award.%29%0ALink%3A%20http%3A//arxiv.org/abs/2512.00949v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520AI%2520for%2520Remote%2520Patient%2520Monitoring%2520in%2520Cancer%2520Care%26entry.906535625%3DYansong%2520Liu%2520and%2520Ronnie%2520Stafford%2520and%2520Pramit%2520Khetrapal%2520and%2520Huriye%2520Kocadag%2520and%2520Gra%25C3%25A7a%2520Carvalho%2520and%2520Patricia%2520de%2520Winter%2520and%2520Maryam%2520Imran%2520and%2520Amelia%2520Snook%2520and%2520Adamos%2520Hadjivasiliou%2520and%2520D.%2520Vijay%2520Anand%2520and%2520Weining%2520Lin%2520and%2520John%2520Kelly%2520and%2520Yukun%2520Zhou%2520and%2520Ivana%2520Drobnjak%26entry.1292438233%3DFor%2520patients%2520undergoing%2520systemic%2520cancer%2520therapy%252C%2520the%2520time%2520between%2520clinic%2520visits%2520is%2520full%2520of%2520uncertainties%2520and%2520risks%2520of%2520unmonitored%2520side%2520effects.%2520To%2520bridge%2520this%2520gap%2520in%2520care%252C%2520we%2520developed%2520and%2520prospectively%2520trialed%2520a%2520multi-modal%2520AI%2520framework%2520for%2520remote%2520patient%2520monitoring%2520%2528RPM%2529.%2520This%2520system%2520integrates%2520multi-modal%2520data%2520from%2520the%2520HALO-X%2520platform%252C%2520such%2520as%2520demographics%252C%2520wearable%2520sensors%252C%2520daily%2520surveys%252C%2520and%2520clinical%2520events.%2520Our%2520observational%2520trial%2520is%2520one%2520of%2520the%2520largest%2520of%2520its%2520kind%2520and%2520has%2520collected%2520over%25202.1%2520million%2520data%2520points%2520%25286%252C080%2520patient-days%2529%2520of%2520monitoring%2520from%252084%2520patients.%2520We%2520developed%2520and%2520adapted%2520a%2520multi-modal%2520AI%2520model%2520to%2520handle%2520the%2520asynchronous%2520and%2520incomplete%2520nature%2520of%2520real-world%2520RPM%2520data%252C%2520forecasting%2520a%2520continuous%2520risk%2520of%2520future%2520adverse%2520events.%2520The%2520model%2520achieved%2520an%2520accuracy%2520of%252083.9%2525%2520%2528AUROC%253D0.70%2529.%2520Notably%252C%2520the%2520model%2520identified%2520previous%2520treatments%252C%2520wellness%2520check-ins%252C%2520and%2520daily%2520maximum%2520heart%2520rate%2520as%2520key%2520predictive%2520features.%2520A%2520case%2520study%2520demonstrated%2520the%2520model%2527s%2520ability%2520to%2520provide%2520early%2520warnings%2520by%2520outputting%2520escalating%2520risk%2520profiles%2520prior%2520to%2520the%2520event.%2520This%2520work%2520establishes%2520the%2520feasibility%2520of%2520multi-modal%2520AI%2520RPM%2520for%2520cancer%2520care%2520and%2520offers%2520a%2520path%2520toward%2520more%2520proactive%2520patient%2520support.%2528Accepted%2520at%2520Europe%2520NeurIPS%25202025%2520Multimodal%2520Representation%2520Learning%2520for%2520Healthcare%2520Workshop.%2520Best%2520Paper%2520Poster%2520Award.%2529%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00949v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20AI%20for%20Remote%20Patient%20Monitoring%20in%20Cancer%20Care&entry.906535625=Yansong%20Liu%20and%20Ronnie%20Stafford%20and%20Pramit%20Khetrapal%20and%20Huriye%20Kocadag%20and%20Gra%C3%A7a%20Carvalho%20and%20Patricia%20de%20Winter%20and%20Maryam%20Imran%20and%20Amelia%20Snook%20and%20Adamos%20Hadjivasiliou%20and%20D.%20Vijay%20Anand%20and%20Weining%20Lin%20and%20John%20Kelly%20and%20Yukun%20Zhou%20and%20Ivana%20Drobnjak&entry.1292438233=For%20patients%20undergoing%20systemic%20cancer%20therapy%2C%20the%20time%20between%20clinic%20visits%20is%20full%20of%20uncertainties%20and%20risks%20of%20unmonitored%20side%20effects.%20To%20bridge%20this%20gap%20in%20care%2C%20we%20developed%20and%20prospectively%20trialed%20a%20multi-modal%20AI%20framework%20for%20remote%20patient%20monitoring%20%28RPM%29.%20This%20system%20integrates%20multi-modal%20data%20from%20the%20HALO-X%20platform%2C%20such%20as%20demographics%2C%20wearable%20sensors%2C%20daily%20surveys%2C%20and%20clinical%20events.%20Our%20observational%20trial%20is%20one%20of%20the%20largest%20of%20its%20kind%20and%20has%20collected%20over%202.1%20million%20data%20points%20%286%2C080%20patient-days%29%20of%20monitoring%20from%2084%20patients.%20We%20developed%20and%20adapted%20a%20multi-modal%20AI%20model%20to%20handle%20the%20asynchronous%20and%20incomplete%20nature%20of%20real-world%20RPM%20data%2C%20forecasting%20a%20continuous%20risk%20of%20future%20adverse%20events.%20The%20model%20achieved%20an%20accuracy%20of%2083.9%25%20%28AUROC%3D0.70%29.%20Notably%2C%20the%20model%20identified%20previous%20treatments%2C%20wellness%20check-ins%2C%20and%20daily%20maximum%20heart%20rate%20as%20key%20predictive%20features.%20A%20case%20study%20demonstrated%20the%20model%27s%20ability%20to%20provide%20early%20warnings%20by%20outputting%20escalating%20risk%20profiles%20prior%20to%20the%20event.%20This%20work%20establishes%20the%20feasibility%20of%20multi-modal%20AI%20RPM%20for%20cancer%20care%20and%20offers%20a%20path%20toward%20more%20proactive%20patient%20support.%28Accepted%20at%20Europe%20NeurIPS%202025%20Multimodal%20Representation%20Learning%20for%20Healthcare%20Workshop.%20Best%20Paper%20Poster%20Award.%29&entry.1838667208=http%3A//arxiv.org/abs/2512.00949v2&entry.124074799=Read"},
{"title": "Boosting HDR Image Reconstruction via Semantic Knowledge Transfer", "author": "Tao Hu and Longyao Wu and Wei Dong and Peng Wu and Jinqiu Sun and Xiaogang Xu and Qingsen Yan and Yanning Zhang", "abstract": "Recovering High Dynamic Range (HDR) images from multiple Standard Dynamic Range (SDR) images become challenging when the SDR images exhibit noticeable degradation and missing content. Leveraging scene-specific semantic priors offers a promising solution for restoring heavily degraded regions. However, these priors are typically extracted from sRGB SDR images, the domain/format gap poses a significant challenge when applying it to HDR imaging. To address this issue, we propose a general framework that transfers semantic knowledge derived from SDR domain via self-distillation to boost existing HDR reconstruction. Specifically, the proposed framework first introduces the Semantic Priors Guided Reconstruction Model (SPGRM), which leverages SDR image semantic knowledge to address ill-posed problems in the initial HDR reconstruction results. Subsequently, we leverage a self-distillation mechanism that constrains the color and content information with semantic knowledge, aligning the external outputs between the baseline and SPGRM. Furthermore, to transfer the semantic knowledge of the internal features, we utilize a Semantic Knowledge Alignment Module (SKAM) to fill the missing semantic contents with the complementary masks. Extensive experiments demonstrate that our framework significantly boosts HDR imaging quality for existing methods without altering the network architecture.", "link": "http://arxiv.org/abs/2503.15361v2", "date": "2026-01-08", "relevancy": 2.0847, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5342}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5176}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20HDR%20Image%20Reconstruction%20via%20Semantic%20Knowledge%20Transfer&body=Title%3A%20Boosting%20HDR%20Image%20Reconstruction%20via%20Semantic%20Knowledge%20Transfer%0AAuthor%3A%20Tao%20Hu%20and%20Longyao%20Wu%20and%20Wei%20Dong%20and%20Peng%20Wu%20and%20Jinqiu%20Sun%20and%20Xiaogang%20Xu%20and%20Qingsen%20Yan%20and%20Yanning%20Zhang%0AAbstract%3A%20Recovering%20High%20Dynamic%20Range%20%28HDR%29%20images%20from%20multiple%20Standard%20Dynamic%20Range%20%28SDR%29%20images%20become%20challenging%20when%20the%20SDR%20images%20exhibit%20noticeable%20degradation%20and%20missing%20content.%20Leveraging%20scene-specific%20semantic%20priors%20offers%20a%20promising%20solution%20for%20restoring%20heavily%20degraded%20regions.%20However%2C%20these%20priors%20are%20typically%20extracted%20from%20sRGB%20SDR%20images%2C%20the%20domain/format%20gap%20poses%20a%20significant%20challenge%20when%20applying%20it%20to%20HDR%20imaging.%20To%20address%20this%20issue%2C%20we%20propose%20a%20general%20framework%20that%20transfers%20semantic%20knowledge%20derived%20from%20SDR%20domain%20via%20self-distillation%20to%20boost%20existing%20HDR%20reconstruction.%20Specifically%2C%20the%20proposed%20framework%20first%20introduces%20the%20Semantic%20Priors%20Guided%20Reconstruction%20Model%20%28SPGRM%29%2C%20which%20leverages%20SDR%20image%20semantic%20knowledge%20to%20address%20ill-posed%20problems%20in%20the%20initial%20HDR%20reconstruction%20results.%20Subsequently%2C%20we%20leverage%20a%20self-distillation%20mechanism%20that%20constrains%20the%20color%20and%20content%20information%20with%20semantic%20knowledge%2C%20aligning%20the%20external%20outputs%20between%20the%20baseline%20and%20SPGRM.%20Furthermore%2C%20to%20transfer%20the%20semantic%20knowledge%20of%20the%20internal%20features%2C%20we%20utilize%20a%20Semantic%20Knowledge%20Alignment%20Module%20%28SKAM%29%20to%20fill%20the%20missing%20semantic%20contents%20with%20the%20complementary%20masks.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20significantly%20boosts%20HDR%20imaging%20quality%20for%20existing%20methods%20without%20altering%20the%20network%20architecture.%0ALink%3A%20http%3A//arxiv.org/abs/2503.15361v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520HDR%2520Image%2520Reconstruction%2520via%2520Semantic%2520Knowledge%2520Transfer%26entry.906535625%3DTao%2520Hu%2520and%2520Longyao%2520Wu%2520and%2520Wei%2520Dong%2520and%2520Peng%2520Wu%2520and%2520Jinqiu%2520Sun%2520and%2520Xiaogang%2520Xu%2520and%2520Qingsen%2520Yan%2520and%2520Yanning%2520Zhang%26entry.1292438233%3DRecovering%2520High%2520Dynamic%2520Range%2520%2528HDR%2529%2520images%2520from%2520multiple%2520Standard%2520Dynamic%2520Range%2520%2528SDR%2529%2520images%2520become%2520challenging%2520when%2520the%2520SDR%2520images%2520exhibit%2520noticeable%2520degradation%2520and%2520missing%2520content.%2520Leveraging%2520scene-specific%2520semantic%2520priors%2520offers%2520a%2520promising%2520solution%2520for%2520restoring%2520heavily%2520degraded%2520regions.%2520However%252C%2520these%2520priors%2520are%2520typically%2520extracted%2520from%2520sRGB%2520SDR%2520images%252C%2520the%2520domain/format%2520gap%2520poses%2520a%2520significant%2520challenge%2520when%2520applying%2520it%2520to%2520HDR%2520imaging.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520general%2520framework%2520that%2520transfers%2520semantic%2520knowledge%2520derived%2520from%2520SDR%2520domain%2520via%2520self-distillation%2520to%2520boost%2520existing%2520HDR%2520reconstruction.%2520Specifically%252C%2520the%2520proposed%2520framework%2520first%2520introduces%2520the%2520Semantic%2520Priors%2520Guided%2520Reconstruction%2520Model%2520%2528SPGRM%2529%252C%2520which%2520leverages%2520SDR%2520image%2520semantic%2520knowledge%2520to%2520address%2520ill-posed%2520problems%2520in%2520the%2520initial%2520HDR%2520reconstruction%2520results.%2520Subsequently%252C%2520we%2520leverage%2520a%2520self-distillation%2520mechanism%2520that%2520constrains%2520the%2520color%2520and%2520content%2520information%2520with%2520semantic%2520knowledge%252C%2520aligning%2520the%2520external%2520outputs%2520between%2520the%2520baseline%2520and%2520SPGRM.%2520Furthermore%252C%2520to%2520transfer%2520the%2520semantic%2520knowledge%2520of%2520the%2520internal%2520features%252C%2520we%2520utilize%2520a%2520Semantic%2520Knowledge%2520Alignment%2520Module%2520%2528SKAM%2529%2520to%2520fill%2520the%2520missing%2520semantic%2520contents%2520with%2520the%2520complementary%2520masks.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520framework%2520significantly%2520boosts%2520HDR%2520imaging%2520quality%2520for%2520existing%2520methods%2520without%2520altering%2520the%2520network%2520architecture.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15361v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20HDR%20Image%20Reconstruction%20via%20Semantic%20Knowledge%20Transfer&entry.906535625=Tao%20Hu%20and%20Longyao%20Wu%20and%20Wei%20Dong%20and%20Peng%20Wu%20and%20Jinqiu%20Sun%20and%20Xiaogang%20Xu%20and%20Qingsen%20Yan%20and%20Yanning%20Zhang&entry.1292438233=Recovering%20High%20Dynamic%20Range%20%28HDR%29%20images%20from%20multiple%20Standard%20Dynamic%20Range%20%28SDR%29%20images%20become%20challenging%20when%20the%20SDR%20images%20exhibit%20noticeable%20degradation%20and%20missing%20content.%20Leveraging%20scene-specific%20semantic%20priors%20offers%20a%20promising%20solution%20for%20restoring%20heavily%20degraded%20regions.%20However%2C%20these%20priors%20are%20typically%20extracted%20from%20sRGB%20SDR%20images%2C%20the%20domain/format%20gap%20poses%20a%20significant%20challenge%20when%20applying%20it%20to%20HDR%20imaging.%20To%20address%20this%20issue%2C%20we%20propose%20a%20general%20framework%20that%20transfers%20semantic%20knowledge%20derived%20from%20SDR%20domain%20via%20self-distillation%20to%20boost%20existing%20HDR%20reconstruction.%20Specifically%2C%20the%20proposed%20framework%20first%20introduces%20the%20Semantic%20Priors%20Guided%20Reconstruction%20Model%20%28SPGRM%29%2C%20which%20leverages%20SDR%20image%20semantic%20knowledge%20to%20address%20ill-posed%20problems%20in%20the%20initial%20HDR%20reconstruction%20results.%20Subsequently%2C%20we%20leverage%20a%20self-distillation%20mechanism%20that%20constrains%20the%20color%20and%20content%20information%20with%20semantic%20knowledge%2C%20aligning%20the%20external%20outputs%20between%20the%20baseline%20and%20SPGRM.%20Furthermore%2C%20to%20transfer%20the%20semantic%20knowledge%20of%20the%20internal%20features%2C%20we%20utilize%20a%20Semantic%20Knowledge%20Alignment%20Module%20%28SKAM%29%20to%20fill%20the%20missing%20semantic%20contents%20with%20the%20complementary%20masks.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20significantly%20boosts%20HDR%20imaging%20quality%20for%20existing%20methods%20without%20altering%20the%20network%20architecture.&entry.1838667208=http%3A//arxiv.org/abs/2503.15361v2&entry.124074799=Read"},
{"title": "When to Act: Calibrated Confidence for Reliable Human Intention Prediction in Assistive Robotics", "author": "Johannes A. Gaus and Winfried Ilg and Daniel Haeufle", "abstract": "Assistive devices must determine both what a user intends to do and how reliable that prediction is before providing support. We introduce a safety-critical triggering framework based on calibrated probabilities for multimodal next-action prediction in Activities of Daily Living. Raw model confidence often fails to reflect true correctness, posing a safety risk. Post-hoc calibration aligns predicted confidence with empirical reliability and reduces miscalibration by about an order of magnitude without affecting accuracy. The calibrated confidence drives a simple ACT/HOLD rule that acts only when reliability is high and withholds assistance otherwise. This turns the confidence threshold into a quantitative safety parameter for assisted actions and enables verifiable behavior in an assistive control loop.", "link": "http://arxiv.org/abs/2601.04982v1", "date": "2026-01-08", "relevancy": 2.0844, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5994}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5268}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20to%20Act%3A%20Calibrated%20Confidence%20for%20Reliable%20Human%20Intention%20Prediction%20in%20Assistive%20Robotics&body=Title%3A%20When%20to%20Act%3A%20Calibrated%20Confidence%20for%20Reliable%20Human%20Intention%20Prediction%20in%20Assistive%20Robotics%0AAuthor%3A%20Johannes%20A.%20Gaus%20and%20Winfried%20Ilg%20and%20Daniel%20Haeufle%0AAbstract%3A%20Assistive%20devices%20must%20determine%20both%20what%20a%20user%20intends%20to%20do%20and%20how%20reliable%20that%20prediction%20is%20before%20providing%20support.%20We%20introduce%20a%20safety-critical%20triggering%20framework%20based%20on%20calibrated%20probabilities%20for%20multimodal%20next-action%20prediction%20in%20Activities%20of%20Daily%20Living.%20Raw%20model%20confidence%20often%20fails%20to%20reflect%20true%20correctness%2C%20posing%20a%20safety%20risk.%20Post-hoc%20calibration%20aligns%20predicted%20confidence%20with%20empirical%20reliability%20and%20reduces%20miscalibration%20by%20about%20an%20order%20of%20magnitude%20without%20affecting%20accuracy.%20The%20calibrated%20confidence%20drives%20a%20simple%20ACT/HOLD%20rule%20that%20acts%20only%20when%20reliability%20is%20high%20and%20withholds%20assistance%20otherwise.%20This%20turns%20the%20confidence%20threshold%20into%20a%20quantitative%20safety%20parameter%20for%20assisted%20actions%20and%20enables%20verifiable%20behavior%20in%20an%20assistive%20control%20loop.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520to%2520Act%253A%2520Calibrated%2520Confidence%2520for%2520Reliable%2520Human%2520Intention%2520Prediction%2520in%2520Assistive%2520Robotics%26entry.906535625%3DJohannes%2520A.%2520Gaus%2520and%2520Winfried%2520Ilg%2520and%2520Daniel%2520Haeufle%26entry.1292438233%3DAssistive%2520devices%2520must%2520determine%2520both%2520what%2520a%2520user%2520intends%2520to%2520do%2520and%2520how%2520reliable%2520that%2520prediction%2520is%2520before%2520providing%2520support.%2520We%2520introduce%2520a%2520safety-critical%2520triggering%2520framework%2520based%2520on%2520calibrated%2520probabilities%2520for%2520multimodal%2520next-action%2520prediction%2520in%2520Activities%2520of%2520Daily%2520Living.%2520Raw%2520model%2520confidence%2520often%2520fails%2520to%2520reflect%2520true%2520correctness%252C%2520posing%2520a%2520safety%2520risk.%2520Post-hoc%2520calibration%2520aligns%2520predicted%2520confidence%2520with%2520empirical%2520reliability%2520and%2520reduces%2520miscalibration%2520by%2520about%2520an%2520order%2520of%2520magnitude%2520without%2520affecting%2520accuracy.%2520The%2520calibrated%2520confidence%2520drives%2520a%2520simple%2520ACT/HOLD%2520rule%2520that%2520acts%2520only%2520when%2520reliability%2520is%2520high%2520and%2520withholds%2520assistance%2520otherwise.%2520This%2520turns%2520the%2520confidence%2520threshold%2520into%2520a%2520quantitative%2520safety%2520parameter%2520for%2520assisted%2520actions%2520and%2520enables%2520verifiable%2520behavior%2520in%2520an%2520assistive%2520control%2520loop.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20to%20Act%3A%20Calibrated%20Confidence%20for%20Reliable%20Human%20Intention%20Prediction%20in%20Assistive%20Robotics&entry.906535625=Johannes%20A.%20Gaus%20and%20Winfried%20Ilg%20and%20Daniel%20Haeufle&entry.1292438233=Assistive%20devices%20must%20determine%20both%20what%20a%20user%20intends%20to%20do%20and%20how%20reliable%20that%20prediction%20is%20before%20providing%20support.%20We%20introduce%20a%20safety-critical%20triggering%20framework%20based%20on%20calibrated%20probabilities%20for%20multimodal%20next-action%20prediction%20in%20Activities%20of%20Daily%20Living.%20Raw%20model%20confidence%20often%20fails%20to%20reflect%20true%20correctness%2C%20posing%20a%20safety%20risk.%20Post-hoc%20calibration%20aligns%20predicted%20confidence%20with%20empirical%20reliability%20and%20reduces%20miscalibration%20by%20about%20an%20order%20of%20magnitude%20without%20affecting%20accuracy.%20The%20calibrated%20confidence%20drives%20a%20simple%20ACT/HOLD%20rule%20that%20acts%20only%20when%20reliability%20is%20high%20and%20withholds%20assistance%20otherwise.%20This%20turns%20the%20confidence%20threshold%20into%20a%20quantitative%20safety%20parameter%20for%20assisted%20actions%20and%20enables%20verifiable%20behavior%20in%20an%20assistive%20control%20loop.&entry.1838667208=http%3A//arxiv.org/abs/2601.04982v1&entry.124074799=Read"},
{"title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing", "author": "Runze He and Yiji Cheng and Tiankai Hang and Zhimin Li and Yu Xu and Zijin Yin and Shiyi Zhang and Wenxun Dai and Penghui Du and Ao Ma and Chunyu Wang and Qinglin Lu and Jizhong Han and Jiao Dai", "abstract": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.", "link": "http://arxiv.org/abs/2601.05124v1", "date": "2026-01-08", "relevancy": 2.0841, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5302}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-Align%3A%20Structured%20Reasoning-guided%20Alignment%20for%20In-Context%20Image%20Generation%20and%20Editing&body=Title%3A%20Re-Align%3A%20Structured%20Reasoning-guided%20Alignment%20for%20In-Context%20Image%20Generation%20and%20Editing%0AAuthor%3A%20Runze%20He%20and%20Yiji%20Cheng%20and%20Tiankai%20Hang%20and%20Zhimin%20Li%20and%20Yu%20Xu%20and%20Zijin%20Yin%20and%20Shiyi%20Zhang%20and%20Wenxun%20Dai%20and%20Penghui%20Du%20and%20Ao%20Ma%20and%20Chunyu%20Wang%20and%20Qinglin%20Lu%20and%20Jizhong%20Han%20and%20Jiao%20Dai%0AAbstract%3A%20In-context%20image%20generation%20and%20editing%20%28ICGE%29%20enables%20users%20to%20specify%20visual%20concepts%20through%20interleaved%20image-text%20prompts%2C%20demanding%20precise%20understanding%20and%20faithful%20execution%20of%20user%20intent.%20Although%20recent%20unified%20multimodal%20models%20exhibit%20promising%20understanding%20capabilities%2C%20these%20strengths%20often%20fail%20to%20transfer%20effectively%20to%20image%20generation.%20We%20introduce%20Re-Align%2C%20a%20unified%20framework%20that%20bridges%20the%20gap%20between%20understanding%20and%20generation%20through%20structured%20reasoning-guided%20alignment.%20At%20its%20core%20lies%20the%20In-Context%20Chain-of-Thought%20%28IC-CoT%29%2C%20a%20structured%20reasoning%20paradigm%20that%20decouples%20semantic%20guidance%20and%20reference%20association%2C%20providing%20clear%20textual%20target%20and%20mitigating%20confusion%20among%20reference%20images.%20Furthermore%2C%20Re-Align%20introduces%20an%20effective%20RL%20training%20scheme%20that%20leverages%20a%20surrogate%20reward%20to%20measure%20the%20alignment%20between%20structured%20reasoning%20text%20and%20the%20generated%20image%2C%20thereby%20improving%20the%20model%27s%20overall%20performance%20on%20ICGE%20tasks.%20Extensive%20experiments%20verify%20that%20Re-Align%20outperforms%20competitive%20methods%20of%20comparable%20model%20scale%20and%20resources%20on%20both%20in-context%20image%20generation%20and%20editing%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-Align%253A%2520Structured%2520Reasoning-guided%2520Alignment%2520for%2520In-Context%2520Image%2520Generation%2520and%2520Editing%26entry.906535625%3DRunze%2520He%2520and%2520Yiji%2520Cheng%2520and%2520Tiankai%2520Hang%2520and%2520Zhimin%2520Li%2520and%2520Yu%2520Xu%2520and%2520Zijin%2520Yin%2520and%2520Shiyi%2520Zhang%2520and%2520Wenxun%2520Dai%2520and%2520Penghui%2520Du%2520and%2520Ao%2520Ma%2520and%2520Chunyu%2520Wang%2520and%2520Qinglin%2520Lu%2520and%2520Jizhong%2520Han%2520and%2520Jiao%2520Dai%26entry.1292438233%3DIn-context%2520image%2520generation%2520and%2520editing%2520%2528ICGE%2529%2520enables%2520users%2520to%2520specify%2520visual%2520concepts%2520through%2520interleaved%2520image-text%2520prompts%252C%2520demanding%2520precise%2520understanding%2520and%2520faithful%2520execution%2520of%2520user%2520intent.%2520Although%2520recent%2520unified%2520multimodal%2520models%2520exhibit%2520promising%2520understanding%2520capabilities%252C%2520these%2520strengths%2520often%2520fail%2520to%2520transfer%2520effectively%2520to%2520image%2520generation.%2520We%2520introduce%2520Re-Align%252C%2520a%2520unified%2520framework%2520that%2520bridges%2520the%2520gap%2520between%2520understanding%2520and%2520generation%2520through%2520structured%2520reasoning-guided%2520alignment.%2520At%2520its%2520core%2520lies%2520the%2520In-Context%2520Chain-of-Thought%2520%2528IC-CoT%2529%252C%2520a%2520structured%2520reasoning%2520paradigm%2520that%2520decouples%2520semantic%2520guidance%2520and%2520reference%2520association%252C%2520providing%2520clear%2520textual%2520target%2520and%2520mitigating%2520confusion%2520among%2520reference%2520images.%2520Furthermore%252C%2520Re-Align%2520introduces%2520an%2520effective%2520RL%2520training%2520scheme%2520that%2520leverages%2520a%2520surrogate%2520reward%2520to%2520measure%2520the%2520alignment%2520between%2520structured%2520reasoning%2520text%2520and%2520the%2520generated%2520image%252C%2520thereby%2520improving%2520the%2520model%2527s%2520overall%2520performance%2520on%2520ICGE%2520tasks.%2520Extensive%2520experiments%2520verify%2520that%2520Re-Align%2520outperforms%2520competitive%2520methods%2520of%2520comparable%2520model%2520scale%2520and%2520resources%2520on%2520both%2520in-context%2520image%2520generation%2520and%2520editing%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-Align%3A%20Structured%20Reasoning-guided%20Alignment%20for%20In-Context%20Image%20Generation%20and%20Editing&entry.906535625=Runze%20He%20and%20Yiji%20Cheng%20and%20Tiankai%20Hang%20and%20Zhimin%20Li%20and%20Yu%20Xu%20and%20Zijin%20Yin%20and%20Shiyi%20Zhang%20and%20Wenxun%20Dai%20and%20Penghui%20Du%20and%20Ao%20Ma%20and%20Chunyu%20Wang%20and%20Qinglin%20Lu%20and%20Jizhong%20Han%20and%20Jiao%20Dai&entry.1292438233=In-context%20image%20generation%20and%20editing%20%28ICGE%29%20enables%20users%20to%20specify%20visual%20concepts%20through%20interleaved%20image-text%20prompts%2C%20demanding%20precise%20understanding%20and%20faithful%20execution%20of%20user%20intent.%20Although%20recent%20unified%20multimodal%20models%20exhibit%20promising%20understanding%20capabilities%2C%20these%20strengths%20often%20fail%20to%20transfer%20effectively%20to%20image%20generation.%20We%20introduce%20Re-Align%2C%20a%20unified%20framework%20that%20bridges%20the%20gap%20between%20understanding%20and%20generation%20through%20structured%20reasoning-guided%20alignment.%20At%20its%20core%20lies%20the%20In-Context%20Chain-of-Thought%20%28IC-CoT%29%2C%20a%20structured%20reasoning%20paradigm%20that%20decouples%20semantic%20guidance%20and%20reference%20association%2C%20providing%20clear%20textual%20target%20and%20mitigating%20confusion%20among%20reference%20images.%20Furthermore%2C%20Re-Align%20introduces%20an%20effective%20RL%20training%20scheme%20that%20leverages%20a%20surrogate%20reward%20to%20measure%20the%20alignment%20between%20structured%20reasoning%20text%20and%20the%20generated%20image%2C%20thereby%20improving%20the%20model%27s%20overall%20performance%20on%20ICGE%20tasks.%20Extensive%20experiments%20verify%20that%20Re-Align%20outperforms%20competitive%20methods%20of%20comparable%20model%20scale%20and%20resources%20on%20both%20in-context%20image%20generation%20and%20editing%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.05124v1&entry.124074799=Read"},
{"title": "FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching", "author": "Danilo Danese and Angela Lombardi and Matteo Attimonelli and Giuseppe Fasano and Tommaso Di Noia", "abstract": "Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.", "link": "http://arxiv.org/abs/2601.05212v1", "date": "2026-01-08", "relevancy": 2.0757, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5866}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5281}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlowLet%3A%20Conditional%203D%20Brain%20MRI%20Synthesis%20using%20Wavelet%20Flow%20Matching&body=Title%3A%20FlowLet%3A%20Conditional%203D%20Brain%20MRI%20Synthesis%20using%20Wavelet%20Flow%20Matching%0AAuthor%3A%20Danilo%20Danese%20and%20Angela%20Lombardi%20and%20Matteo%20Attimonelli%20and%20Giuseppe%20Fasano%20and%20Tommaso%20Di%20Noia%0AAbstract%3A%20Brain%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20plays%20a%20central%20role%20in%20studying%20neurological%20development%2C%20aging%2C%20and%20diseases.%20One%20key%20application%20is%20Brain%20Age%20Prediction%20%28BAP%29%2C%20which%20estimates%20an%20individual%27s%20biological%20brain%20age%20from%20MRI%20data.%20Effective%20BAP%20models%20require%20large%2C%20diverse%2C%20and%20age-balanced%20datasets%2C%20whereas%20existing%203D%20MRI%20datasets%20are%20demographically%20skewed%2C%20limiting%20fairness%20and%20generalizability.%20Acquiring%20new%20data%20is%20costly%20and%20ethically%20constrained%2C%20motivating%20generative%20data%20augmentation.%20Current%20generative%20methods%20are%20often%20based%20on%20latent%20diffusion%20models%2C%20which%20operate%20in%20learned%20low%20dimensional%20latent%20spaces%20to%20address%20the%20memory%20demands%20of%20volumetric%20MRI%20data.%20However%2C%20these%20methods%20are%20typically%20slow%20at%20inference%2C%20may%20introduce%20artifacts%20due%20to%20latent%20compression%2C%20and%20are%20rarely%20conditioned%20on%20age%2C%20thereby%20affecting%20the%20BAP%20performance.%20In%20this%20work%2C%20we%20propose%20FlowLet%2C%20a%20conditional%20generative%20framework%20that%20synthesizes%20age-conditioned%203D%20MRIs%20by%20leveraging%20flow%20matching%20within%20an%20invertible%203D%20wavelet%20domain%2C%20helping%20to%20avoid%20reconstruction%20artifacts%20and%20reducing%20computational%20demands.%20Experiments%20show%20that%20FlowLet%20generates%20high-fidelity%20volumes%20with%20few%20sampling%20steps.%20Training%20BAP%20models%20with%20data%20generated%20by%20FlowLet%20improves%20performance%20for%20underrepresented%20age%20groups%2C%20and%20region-based%20analysis%20confirms%20preservation%20of%20anatomical%20structures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlowLet%253A%2520Conditional%25203D%2520Brain%2520MRI%2520Synthesis%2520using%2520Wavelet%2520Flow%2520Matching%26entry.906535625%3DDanilo%2520Danese%2520and%2520Angela%2520Lombardi%2520and%2520Matteo%2520Attimonelli%2520and%2520Giuseppe%2520Fasano%2520and%2520Tommaso%2520Di%2520Noia%26entry.1292438233%3DBrain%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520plays%2520a%2520central%2520role%2520in%2520studying%2520neurological%2520development%252C%2520aging%252C%2520and%2520diseases.%2520One%2520key%2520application%2520is%2520Brain%2520Age%2520Prediction%2520%2528BAP%2529%252C%2520which%2520estimates%2520an%2520individual%2527s%2520biological%2520brain%2520age%2520from%2520MRI%2520data.%2520Effective%2520BAP%2520models%2520require%2520large%252C%2520diverse%252C%2520and%2520age-balanced%2520datasets%252C%2520whereas%2520existing%25203D%2520MRI%2520datasets%2520are%2520demographically%2520skewed%252C%2520limiting%2520fairness%2520and%2520generalizability.%2520Acquiring%2520new%2520data%2520is%2520costly%2520and%2520ethically%2520constrained%252C%2520motivating%2520generative%2520data%2520augmentation.%2520Current%2520generative%2520methods%2520are%2520often%2520based%2520on%2520latent%2520diffusion%2520models%252C%2520which%2520operate%2520in%2520learned%2520low%2520dimensional%2520latent%2520spaces%2520to%2520address%2520the%2520memory%2520demands%2520of%2520volumetric%2520MRI%2520data.%2520However%252C%2520these%2520methods%2520are%2520typically%2520slow%2520at%2520inference%252C%2520may%2520introduce%2520artifacts%2520due%2520to%2520latent%2520compression%252C%2520and%2520are%2520rarely%2520conditioned%2520on%2520age%252C%2520thereby%2520affecting%2520the%2520BAP%2520performance.%2520In%2520this%2520work%252C%2520we%2520propose%2520FlowLet%252C%2520a%2520conditional%2520generative%2520framework%2520that%2520synthesizes%2520age-conditioned%25203D%2520MRIs%2520by%2520leveraging%2520flow%2520matching%2520within%2520an%2520invertible%25203D%2520wavelet%2520domain%252C%2520helping%2520to%2520avoid%2520reconstruction%2520artifacts%2520and%2520reducing%2520computational%2520demands.%2520Experiments%2520show%2520that%2520FlowLet%2520generates%2520high-fidelity%2520volumes%2520with%2520few%2520sampling%2520steps.%2520Training%2520BAP%2520models%2520with%2520data%2520generated%2520by%2520FlowLet%2520improves%2520performance%2520for%2520underrepresented%2520age%2520groups%252C%2520and%2520region-based%2520analysis%2520confirms%2520preservation%2520of%2520anatomical%2520structures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlowLet%3A%20Conditional%203D%20Brain%20MRI%20Synthesis%20using%20Wavelet%20Flow%20Matching&entry.906535625=Danilo%20Danese%20and%20Angela%20Lombardi%20and%20Matteo%20Attimonelli%20and%20Giuseppe%20Fasano%20and%20Tommaso%20Di%20Noia&entry.1292438233=Brain%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20plays%20a%20central%20role%20in%20studying%20neurological%20development%2C%20aging%2C%20and%20diseases.%20One%20key%20application%20is%20Brain%20Age%20Prediction%20%28BAP%29%2C%20which%20estimates%20an%20individual%27s%20biological%20brain%20age%20from%20MRI%20data.%20Effective%20BAP%20models%20require%20large%2C%20diverse%2C%20and%20age-balanced%20datasets%2C%20whereas%20existing%203D%20MRI%20datasets%20are%20demographically%20skewed%2C%20limiting%20fairness%20and%20generalizability.%20Acquiring%20new%20data%20is%20costly%20and%20ethically%20constrained%2C%20motivating%20generative%20data%20augmentation.%20Current%20generative%20methods%20are%20often%20based%20on%20latent%20diffusion%20models%2C%20which%20operate%20in%20learned%20low%20dimensional%20latent%20spaces%20to%20address%20the%20memory%20demands%20of%20volumetric%20MRI%20data.%20However%2C%20these%20methods%20are%20typically%20slow%20at%20inference%2C%20may%20introduce%20artifacts%20due%20to%20latent%20compression%2C%20and%20are%20rarely%20conditioned%20on%20age%2C%20thereby%20affecting%20the%20BAP%20performance.%20In%20this%20work%2C%20we%20propose%20FlowLet%2C%20a%20conditional%20generative%20framework%20that%20synthesizes%20age-conditioned%203D%20MRIs%20by%20leveraging%20flow%20matching%20within%20an%20invertible%203D%20wavelet%20domain%2C%20helping%20to%20avoid%20reconstruction%20artifacts%20and%20reducing%20computational%20demands.%20Experiments%20show%20that%20FlowLet%20generates%20high-fidelity%20volumes%20with%20few%20sampling%20steps.%20Training%20BAP%20models%20with%20data%20generated%20by%20FlowLet%20improves%20performance%20for%20underrepresented%20age%20groups%2C%20and%20region-based%20analysis%20confirms%20preservation%20of%20anatomical%20structures.&entry.1838667208=http%3A//arxiv.org/abs/2601.05212v1&entry.124074799=Read"},
{"title": "Decentralized Privacy-Preserving Federal Learning of Computer Vision Models on Edge Devices", "author": "Damian Haren\u010d\u00e1k and Luk\u00e1\u0161 Gajdo\u0161ech and Martin Madaras", "abstract": "Collaborative training of a machine learning model comes with a risk of sharing sensitive or private data. Federated learning offers a way of collectively training a single global model without the need to share client data, by sharing only the updated parameters from each client's local model. A central server is then used to aggregate parameters from all clients and redistribute the aggregated model back to the clients. Recent findings have shown that even in this scenario, private data can be reconstructed only using information about model parameters. Current efforts to mitigate this are mainly focused on reducing privacy risks on the server side, assuming that other clients will not act maliciously. In this work, we analyzed various methods for improving the privacy of client data concerning both the server and other clients for neural networks. Some of these methods include homomorphic encryption, gradient compression, gradient noising, and discussion on possible usage of modified federated learning systems such as split learning, swarm learning or fully encrypted models. We have analyzed the negative effects of gradient compression and gradient noising on the accuracy of convolutional neural networks used for classification. We have shown the difficulty of data reconstruction in the case of segmentation networks. We have also implemented a proof of concept on the NVIDIA Jetson TX2 module used in edge devices and simulated a federated learning process.", "link": "http://arxiv.org/abs/2601.04912v1", "date": "2026-01-08", "relevancy": 2.0718, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5205}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5164}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Privacy-Preserving%20Federal%20Learning%20of%20Computer%20Vision%20Models%20on%20Edge%20Devices&body=Title%3A%20Decentralized%20Privacy-Preserving%20Federal%20Learning%20of%20Computer%20Vision%20Models%20on%20Edge%20Devices%0AAuthor%3A%20Damian%20Haren%C4%8D%C3%A1k%20and%20Luk%C3%A1%C5%A1%20Gajdo%C5%A1ech%20and%20Martin%20Madaras%0AAbstract%3A%20Collaborative%20training%20of%20a%20machine%20learning%20model%20comes%20with%20a%20risk%20of%20sharing%20sensitive%20or%20private%20data.%20Federated%20learning%20offers%20a%20way%20of%20collectively%20training%20a%20single%20global%20model%20without%20the%20need%20to%20share%20client%20data%2C%20by%20sharing%20only%20the%20updated%20parameters%20from%20each%20client%27s%20local%20model.%20A%20central%20server%20is%20then%20used%20to%20aggregate%20parameters%20from%20all%20clients%20and%20redistribute%20the%20aggregated%20model%20back%20to%20the%20clients.%20Recent%20findings%20have%20shown%20that%20even%20in%20this%20scenario%2C%20private%20data%20can%20be%20reconstructed%20only%20using%20information%20about%20model%20parameters.%20Current%20efforts%20to%20mitigate%20this%20are%20mainly%20focused%20on%20reducing%20privacy%20risks%20on%20the%20server%20side%2C%20assuming%20that%20other%20clients%20will%20not%20act%20maliciously.%20In%20this%20work%2C%20we%20analyzed%20various%20methods%20for%20improving%20the%20privacy%20of%20client%20data%20concerning%20both%20the%20server%20and%20other%20clients%20for%20neural%20networks.%20Some%20of%20these%20methods%20include%20homomorphic%20encryption%2C%20gradient%20compression%2C%20gradient%20noising%2C%20and%20discussion%20on%20possible%20usage%20of%20modified%20federated%20learning%20systems%20such%20as%20split%20learning%2C%20swarm%20learning%20or%20fully%20encrypted%20models.%20We%20have%20analyzed%20the%20negative%20effects%20of%20gradient%20compression%20and%20gradient%20noising%20on%20the%20accuracy%20of%20convolutional%20neural%20networks%20used%20for%20classification.%20We%20have%20shown%20the%20difficulty%20of%20data%20reconstruction%20in%20the%20case%20of%20segmentation%20networks.%20We%20have%20also%20implemented%20a%20proof%20of%20concept%20on%20the%20NVIDIA%20Jetson%20TX2%20module%20used%20in%20edge%20devices%20and%20simulated%20a%20federated%20learning%20process.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Privacy-Preserving%2520Federal%2520Learning%2520of%2520Computer%2520Vision%2520Models%2520on%2520Edge%2520Devices%26entry.906535625%3DDamian%2520Haren%25C4%258D%25C3%25A1k%2520and%2520Luk%25C3%25A1%25C5%25A1%2520Gajdo%25C5%25A1ech%2520and%2520Martin%2520Madaras%26entry.1292438233%3DCollaborative%2520training%2520of%2520a%2520machine%2520learning%2520model%2520comes%2520with%2520a%2520risk%2520of%2520sharing%2520sensitive%2520or%2520private%2520data.%2520Federated%2520learning%2520offers%2520a%2520way%2520of%2520collectively%2520training%2520a%2520single%2520global%2520model%2520without%2520the%2520need%2520to%2520share%2520client%2520data%252C%2520by%2520sharing%2520only%2520the%2520updated%2520parameters%2520from%2520each%2520client%2527s%2520local%2520model.%2520A%2520central%2520server%2520is%2520then%2520used%2520to%2520aggregate%2520parameters%2520from%2520all%2520clients%2520and%2520redistribute%2520the%2520aggregated%2520model%2520back%2520to%2520the%2520clients.%2520Recent%2520findings%2520have%2520shown%2520that%2520even%2520in%2520this%2520scenario%252C%2520private%2520data%2520can%2520be%2520reconstructed%2520only%2520using%2520information%2520about%2520model%2520parameters.%2520Current%2520efforts%2520to%2520mitigate%2520this%2520are%2520mainly%2520focused%2520on%2520reducing%2520privacy%2520risks%2520on%2520the%2520server%2520side%252C%2520assuming%2520that%2520other%2520clients%2520will%2520not%2520act%2520maliciously.%2520In%2520this%2520work%252C%2520we%2520analyzed%2520various%2520methods%2520for%2520improving%2520the%2520privacy%2520of%2520client%2520data%2520concerning%2520both%2520the%2520server%2520and%2520other%2520clients%2520for%2520neural%2520networks.%2520Some%2520of%2520these%2520methods%2520include%2520homomorphic%2520encryption%252C%2520gradient%2520compression%252C%2520gradient%2520noising%252C%2520and%2520discussion%2520on%2520possible%2520usage%2520of%2520modified%2520federated%2520learning%2520systems%2520such%2520as%2520split%2520learning%252C%2520swarm%2520learning%2520or%2520fully%2520encrypted%2520models.%2520We%2520have%2520analyzed%2520the%2520negative%2520effects%2520of%2520gradient%2520compression%2520and%2520gradient%2520noising%2520on%2520the%2520accuracy%2520of%2520convolutional%2520neural%2520networks%2520used%2520for%2520classification.%2520We%2520have%2520shown%2520the%2520difficulty%2520of%2520data%2520reconstruction%2520in%2520the%2520case%2520of%2520segmentation%2520networks.%2520We%2520have%2520also%2520implemented%2520a%2520proof%2520of%2520concept%2520on%2520the%2520NVIDIA%2520Jetson%2520TX2%2520module%2520used%2520in%2520edge%2520devices%2520and%2520simulated%2520a%2520federated%2520learning%2520process.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Privacy-Preserving%20Federal%20Learning%20of%20Computer%20Vision%20Models%20on%20Edge%20Devices&entry.906535625=Damian%20Haren%C4%8D%C3%A1k%20and%20Luk%C3%A1%C5%A1%20Gajdo%C5%A1ech%20and%20Martin%20Madaras&entry.1292438233=Collaborative%20training%20of%20a%20machine%20learning%20model%20comes%20with%20a%20risk%20of%20sharing%20sensitive%20or%20private%20data.%20Federated%20learning%20offers%20a%20way%20of%20collectively%20training%20a%20single%20global%20model%20without%20the%20need%20to%20share%20client%20data%2C%20by%20sharing%20only%20the%20updated%20parameters%20from%20each%20client%27s%20local%20model.%20A%20central%20server%20is%20then%20used%20to%20aggregate%20parameters%20from%20all%20clients%20and%20redistribute%20the%20aggregated%20model%20back%20to%20the%20clients.%20Recent%20findings%20have%20shown%20that%20even%20in%20this%20scenario%2C%20private%20data%20can%20be%20reconstructed%20only%20using%20information%20about%20model%20parameters.%20Current%20efforts%20to%20mitigate%20this%20are%20mainly%20focused%20on%20reducing%20privacy%20risks%20on%20the%20server%20side%2C%20assuming%20that%20other%20clients%20will%20not%20act%20maliciously.%20In%20this%20work%2C%20we%20analyzed%20various%20methods%20for%20improving%20the%20privacy%20of%20client%20data%20concerning%20both%20the%20server%20and%20other%20clients%20for%20neural%20networks.%20Some%20of%20these%20methods%20include%20homomorphic%20encryption%2C%20gradient%20compression%2C%20gradient%20noising%2C%20and%20discussion%20on%20possible%20usage%20of%20modified%20federated%20learning%20systems%20such%20as%20split%20learning%2C%20swarm%20learning%20or%20fully%20encrypted%20models.%20We%20have%20analyzed%20the%20negative%20effects%20of%20gradient%20compression%20and%20gradient%20noising%20on%20the%20accuracy%20of%20convolutional%20neural%20networks%20used%20for%20classification.%20We%20have%20shown%20the%20difficulty%20of%20data%20reconstruction%20in%20the%20case%20of%20segmentation%20networks.%20We%20have%20also%20implemented%20a%20proof%20of%20concept%20on%20the%20NVIDIA%20Jetson%20TX2%20module%20used%20in%20edge%20devices%20and%20simulated%20a%20federated%20learning%20process.&entry.1838667208=http%3A//arxiv.org/abs/2601.04912v1&entry.124074799=Read"},
{"title": "CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters", "author": "Ao Sun and Xiaoyu Wang and Zhe Tan and Yu Li and Jiachen Zhu and Shu Su and Yuheng Jia", "abstract": "As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \\textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \\textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \\textbf{\\textsc{CuMA}} (\\textbf{Cu}ltural \\textbf{M}ixture of \\textbf{A}dapters), a framework that frames alignment as a \\textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \\textsc{CuMA} internalizes a \\textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \\textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \\textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA.", "link": "http://arxiv.org/abs/2601.04885v1", "date": "2026-01-08", "relevancy": 2.0682, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5626}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4908}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CuMA%3A%20Aligning%20LLMs%20with%20Sparse%20Cultural%20Values%20via%20Demographic-Aware%20Mixture%20of%20Adapters&body=Title%3A%20CuMA%3A%20Aligning%20LLMs%20with%20Sparse%20Cultural%20Values%20via%20Demographic-Aware%20Mixture%20of%20Adapters%0AAuthor%3A%20Ao%20Sun%20and%20Xiaoyu%20Wang%20and%20Zhe%20Tan%20and%20Yu%20Li%20and%20Jiachen%20Zhu%20and%20Shu%20Su%20and%20Yuheng%20Jia%0AAbstract%3A%20As%20Large%20Language%20Models%20%28LLMs%29%20serve%20a%20global%20audience%2C%20alignment%20must%20transition%20from%20enforcing%20universal%20consensus%20to%20respecting%20cultural%20pluralism.%20We%20demonstrate%20that%20dense%20models%2C%20when%20forced%20to%20fit%20conflicting%20value%20distributions%2C%20suffer%20from%20%5Ctextbf%7BMean%20Collapse%7D%2C%20converging%20to%20a%20generic%20average%20that%20fails%20to%20represent%20diverse%20groups.%20We%20attribute%20this%20to%20%5Ctextbf%7BCultural%20Sparsity%7D%2C%20where%20gradient%20interference%20prevents%20dense%20parameters%20from%20spanning%20distinct%20cultural%20modes.%20To%20resolve%20this%2C%20we%20propose%20%5Ctextbf%7B%5Ctextsc%7BCuMA%7D%7D%20%28%5Ctextbf%7BCu%7Dltural%20%5Ctextbf%7BM%7Dixture%20of%20%5Ctextbf%7BA%7Ddapters%29%2C%20a%20framework%20that%20frames%20alignment%20as%20a%20%5Ctextbf%7Bconditional%20capacity%20separation%7D%20problem.%20By%20incorporating%20demographic-aware%20routing%2C%20%5Ctextsc%7BCuMA%7D%20internalizes%20a%20%5Ctextit%7BLatent%20Cultural%20Topology%7D%20to%20explicitly%20disentangle%20conflicting%20gradients%20into%20specialized%20expert%20subspaces.%20Extensive%20evaluations%20on%20WorldValuesBench%2C%20Community%20Alignment%2C%20and%20PRISM%20demonstrate%20that%20%5Ctextsc%7BCuMA%7D%20achieves%20state-of-the-art%20performance%2C%20significantly%20outperforming%20both%20dense%20baselines%20and%20semantic-only%20MoEs.%20Crucially%2C%20our%20analysis%20confirms%20that%20%5Ctextsc%7BCuMA%7D%20effectively%20mitigates%20mean%20collapse%2C%20preserving%20cultural%20diversity.%20Our%20code%20is%20available%20at%20https%3A//github.com/Throll/CuMA.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCuMA%253A%2520Aligning%2520LLMs%2520with%2520Sparse%2520Cultural%2520Values%2520via%2520Demographic-Aware%2520Mixture%2520of%2520Adapters%26entry.906535625%3DAo%2520Sun%2520and%2520Xiaoyu%2520Wang%2520and%2520Zhe%2520Tan%2520and%2520Yu%2520Li%2520and%2520Jiachen%2520Zhu%2520and%2520Shu%2520Su%2520and%2520Yuheng%2520Jia%26entry.1292438233%3DAs%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520serve%2520a%2520global%2520audience%252C%2520alignment%2520must%2520transition%2520from%2520enforcing%2520universal%2520consensus%2520to%2520respecting%2520cultural%2520pluralism.%2520We%2520demonstrate%2520that%2520dense%2520models%252C%2520when%2520forced%2520to%2520fit%2520conflicting%2520value%2520distributions%252C%2520suffer%2520from%2520%255Ctextbf%257BMean%2520Collapse%257D%252C%2520converging%2520to%2520a%2520generic%2520average%2520that%2520fails%2520to%2520represent%2520diverse%2520groups.%2520We%2520attribute%2520this%2520to%2520%255Ctextbf%257BCultural%2520Sparsity%257D%252C%2520where%2520gradient%2520interference%2520prevents%2520dense%2520parameters%2520from%2520spanning%2520distinct%2520cultural%2520modes.%2520To%2520resolve%2520this%252C%2520we%2520propose%2520%255Ctextbf%257B%255Ctextsc%257BCuMA%257D%257D%2520%2528%255Ctextbf%257BCu%257Dltural%2520%255Ctextbf%257BM%257Dixture%2520of%2520%255Ctextbf%257BA%257Ddapters%2529%252C%2520a%2520framework%2520that%2520frames%2520alignment%2520as%2520a%2520%255Ctextbf%257Bconditional%2520capacity%2520separation%257D%2520problem.%2520By%2520incorporating%2520demographic-aware%2520routing%252C%2520%255Ctextsc%257BCuMA%257D%2520internalizes%2520a%2520%255Ctextit%257BLatent%2520Cultural%2520Topology%257D%2520to%2520explicitly%2520disentangle%2520conflicting%2520gradients%2520into%2520specialized%2520expert%2520subspaces.%2520Extensive%2520evaluations%2520on%2520WorldValuesBench%252C%2520Community%2520Alignment%252C%2520and%2520PRISM%2520demonstrate%2520that%2520%255Ctextsc%257BCuMA%257D%2520achieves%2520state-of-the-art%2520performance%252C%2520significantly%2520outperforming%2520both%2520dense%2520baselines%2520and%2520semantic-only%2520MoEs.%2520Crucially%252C%2520our%2520analysis%2520confirms%2520that%2520%255Ctextsc%257BCuMA%257D%2520effectively%2520mitigates%2520mean%2520collapse%252C%2520preserving%2520cultural%2520diversity.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/Throll/CuMA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CuMA%3A%20Aligning%20LLMs%20with%20Sparse%20Cultural%20Values%20via%20Demographic-Aware%20Mixture%20of%20Adapters&entry.906535625=Ao%20Sun%20and%20Xiaoyu%20Wang%20and%20Zhe%20Tan%20and%20Yu%20Li%20and%20Jiachen%20Zhu%20and%20Shu%20Su%20and%20Yuheng%20Jia&entry.1292438233=As%20Large%20Language%20Models%20%28LLMs%29%20serve%20a%20global%20audience%2C%20alignment%20must%20transition%20from%20enforcing%20universal%20consensus%20to%20respecting%20cultural%20pluralism.%20We%20demonstrate%20that%20dense%20models%2C%20when%20forced%20to%20fit%20conflicting%20value%20distributions%2C%20suffer%20from%20%5Ctextbf%7BMean%20Collapse%7D%2C%20converging%20to%20a%20generic%20average%20that%20fails%20to%20represent%20diverse%20groups.%20We%20attribute%20this%20to%20%5Ctextbf%7BCultural%20Sparsity%7D%2C%20where%20gradient%20interference%20prevents%20dense%20parameters%20from%20spanning%20distinct%20cultural%20modes.%20To%20resolve%20this%2C%20we%20propose%20%5Ctextbf%7B%5Ctextsc%7BCuMA%7D%7D%20%28%5Ctextbf%7BCu%7Dltural%20%5Ctextbf%7BM%7Dixture%20of%20%5Ctextbf%7BA%7Ddapters%29%2C%20a%20framework%20that%20frames%20alignment%20as%20a%20%5Ctextbf%7Bconditional%20capacity%20separation%7D%20problem.%20By%20incorporating%20demographic-aware%20routing%2C%20%5Ctextsc%7BCuMA%7D%20internalizes%20a%20%5Ctextit%7BLatent%20Cultural%20Topology%7D%20to%20explicitly%20disentangle%20conflicting%20gradients%20into%20specialized%20expert%20subspaces.%20Extensive%20evaluations%20on%20WorldValuesBench%2C%20Community%20Alignment%2C%20and%20PRISM%20demonstrate%20that%20%5Ctextsc%7BCuMA%7D%20achieves%20state-of-the-art%20performance%2C%20significantly%20outperforming%20both%20dense%20baselines%20and%20semantic-only%20MoEs.%20Crucially%2C%20our%20analysis%20confirms%20that%20%5Ctextsc%7BCuMA%7D%20effectively%20mitigates%20mean%20collapse%2C%20preserving%20cultural%20diversity.%20Our%20code%20is%20available%20at%20https%3A//github.com/Throll/CuMA.&entry.1838667208=http%3A//arxiv.org/abs/2601.04885v1&entry.124074799=Read"},
{"title": "Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics", "author": "Subhadeep Roy and Gagan Bhatia and Steffen Eger", "abstract": "Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \\emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \\textsc{\\textbf{ProtoBias}} (\\textit{\\textbf{Proto}typical \\textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \\textbf{\\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.", "link": "http://arxiv.org/abs/2601.04946v1", "date": "2026-01-08", "relevancy": 2.0649, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5294}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5167}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototypicality%20Bias%20Reveals%20Blindspots%20in%20Multimodal%20Evaluation%20Metrics&body=Title%3A%20Prototypicality%20Bias%20Reveals%20Blindspots%20in%20Multimodal%20Evaluation%20Metrics%0AAuthor%3A%20Subhadeep%20Roy%20and%20Gagan%20Bhatia%20and%20Steffen%20Eger%0AAbstract%3A%20Automatic%20metrics%20are%20now%20central%20to%20evaluating%20text-to-image%20models%2C%20often%20substituting%20for%20human%20judgment%20in%20benchmarking%20and%20large-scale%20filtering.%20However%2C%20it%20remains%20unclear%20whether%20these%20metrics%20truly%20prioritize%20semantic%20correctness%20or%20instead%20favor%20visually%20and%20socially%20prototypical%20images%20learned%20from%20biased%20data%20distributions.%20We%20identify%20and%20study%20%5Cemph%7Bprototypicality%20bias%7D%20as%20a%20systematic%20failure%20mode%20in%20multimodal%20evaluation.%20We%20introduce%20a%20controlled%20contrastive%20benchmark%20%5Ctextsc%7B%5Ctextbf%7BProtoBias%7D%7D%20%28%5Ctextit%7B%5Ctextbf%7BProto%7Dtypical%20%5Ctextbf%7BBias%7D%7D%29%2C%20spanning%20Animals%2C%20Objects%2C%20and%20Demography%20images%2C%20where%20semantically%20correct%20but%20non-prototypical%20images%20are%20paired%20with%20subtly%20incorrect%20yet%20prototypical%20adversarial%20counterparts.%20This%20setup%20enables%20a%20directional%20evaluation%20of%20whether%20metrics%20follow%20textual%20semantics%20or%20default%20to%20prototypes.%20Our%20results%20show%20that%20widely%20used%20metrics%2C%20including%20CLIPScore%2C%20PickScore%2C%20and%20VQA-based%20scores%2C%20frequently%20misrank%20these%20pairs%2C%20while%20even%20LLM-as-Judge%20systems%20exhibit%20uneven%20robustness%20in%20socially%20grounded%20cases.%20Human%20evaluations%20consistently%20favour%20semantic%20correctness%20with%20larger%20decision%20margins.%20Motivated%20by%20these%20findings%2C%20we%20propose%20%5Ctextbf%7B%5Ctextsc%7BProtoScore%7D%7D%2C%20a%20robust%207B-parameter%20metric%20that%20substantially%20reduces%20failure%20rates%20and%20suppresses%20misranking%2C%20while%20running%20at%20orders%20of%20magnitude%20faster%20than%20the%20inference%20time%20of%20GPT-5%2C%20approaching%20the%20robustness%20of%20much%20larger%20closed-source%20judges.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototypicality%2520Bias%2520Reveals%2520Blindspots%2520in%2520Multimodal%2520Evaluation%2520Metrics%26entry.906535625%3DSubhadeep%2520Roy%2520and%2520Gagan%2520Bhatia%2520and%2520Steffen%2520Eger%26entry.1292438233%3DAutomatic%2520metrics%2520are%2520now%2520central%2520to%2520evaluating%2520text-to-image%2520models%252C%2520often%2520substituting%2520for%2520human%2520judgment%2520in%2520benchmarking%2520and%2520large-scale%2520filtering.%2520However%252C%2520it%2520remains%2520unclear%2520whether%2520these%2520metrics%2520truly%2520prioritize%2520semantic%2520correctness%2520or%2520instead%2520favor%2520visually%2520and%2520socially%2520prototypical%2520images%2520learned%2520from%2520biased%2520data%2520distributions.%2520We%2520identify%2520and%2520study%2520%255Cemph%257Bprototypicality%2520bias%257D%2520as%2520a%2520systematic%2520failure%2520mode%2520in%2520multimodal%2520evaluation.%2520We%2520introduce%2520a%2520controlled%2520contrastive%2520benchmark%2520%255Ctextsc%257B%255Ctextbf%257BProtoBias%257D%257D%2520%2528%255Ctextit%257B%255Ctextbf%257BProto%257Dtypical%2520%255Ctextbf%257BBias%257D%257D%2529%252C%2520spanning%2520Animals%252C%2520Objects%252C%2520and%2520Demography%2520images%252C%2520where%2520semantically%2520correct%2520but%2520non-prototypical%2520images%2520are%2520paired%2520with%2520subtly%2520incorrect%2520yet%2520prototypical%2520adversarial%2520counterparts.%2520This%2520setup%2520enables%2520a%2520directional%2520evaluation%2520of%2520whether%2520metrics%2520follow%2520textual%2520semantics%2520or%2520default%2520to%2520prototypes.%2520Our%2520results%2520show%2520that%2520widely%2520used%2520metrics%252C%2520including%2520CLIPScore%252C%2520PickScore%252C%2520and%2520VQA-based%2520scores%252C%2520frequently%2520misrank%2520these%2520pairs%252C%2520while%2520even%2520LLM-as-Judge%2520systems%2520exhibit%2520uneven%2520robustness%2520in%2520socially%2520grounded%2520cases.%2520Human%2520evaluations%2520consistently%2520favour%2520semantic%2520correctness%2520with%2520larger%2520decision%2520margins.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520propose%2520%255Ctextbf%257B%255Ctextsc%257BProtoScore%257D%257D%252C%2520a%2520robust%25207B-parameter%2520metric%2520that%2520substantially%2520reduces%2520failure%2520rates%2520and%2520suppresses%2520misranking%252C%2520while%2520running%2520at%2520orders%2520of%2520magnitude%2520faster%2520than%2520the%2520inference%2520time%2520of%2520GPT-5%252C%2520approaching%2520the%2520robustness%2520of%2520much%2520larger%2520closed-source%2520judges.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototypicality%20Bias%20Reveals%20Blindspots%20in%20Multimodal%20Evaluation%20Metrics&entry.906535625=Subhadeep%20Roy%20and%20Gagan%20Bhatia%20and%20Steffen%20Eger&entry.1292438233=Automatic%20metrics%20are%20now%20central%20to%20evaluating%20text-to-image%20models%2C%20often%20substituting%20for%20human%20judgment%20in%20benchmarking%20and%20large-scale%20filtering.%20However%2C%20it%20remains%20unclear%20whether%20these%20metrics%20truly%20prioritize%20semantic%20correctness%20or%20instead%20favor%20visually%20and%20socially%20prototypical%20images%20learned%20from%20biased%20data%20distributions.%20We%20identify%20and%20study%20%5Cemph%7Bprototypicality%20bias%7D%20as%20a%20systematic%20failure%20mode%20in%20multimodal%20evaluation.%20We%20introduce%20a%20controlled%20contrastive%20benchmark%20%5Ctextsc%7B%5Ctextbf%7BProtoBias%7D%7D%20%28%5Ctextit%7B%5Ctextbf%7BProto%7Dtypical%20%5Ctextbf%7BBias%7D%7D%29%2C%20spanning%20Animals%2C%20Objects%2C%20and%20Demography%20images%2C%20where%20semantically%20correct%20but%20non-prototypical%20images%20are%20paired%20with%20subtly%20incorrect%20yet%20prototypical%20adversarial%20counterparts.%20This%20setup%20enables%20a%20directional%20evaluation%20of%20whether%20metrics%20follow%20textual%20semantics%20or%20default%20to%20prototypes.%20Our%20results%20show%20that%20widely%20used%20metrics%2C%20including%20CLIPScore%2C%20PickScore%2C%20and%20VQA-based%20scores%2C%20frequently%20misrank%20these%20pairs%2C%20while%20even%20LLM-as-Judge%20systems%20exhibit%20uneven%20robustness%20in%20socially%20grounded%20cases.%20Human%20evaluations%20consistently%20favour%20semantic%20correctness%20with%20larger%20decision%20margins.%20Motivated%20by%20these%20findings%2C%20we%20propose%20%5Ctextbf%7B%5Ctextsc%7BProtoScore%7D%7D%2C%20a%20robust%207B-parameter%20metric%20that%20substantially%20reduces%20failure%20rates%20and%20suppresses%20misranking%2C%20while%20running%20at%20orders%20of%20magnitude%20faster%20than%20the%20inference%20time%20of%20GPT-5%2C%20approaching%20the%20robustness%20of%20much%20larger%20closed-source%20judges.&entry.1838667208=http%3A//arxiv.org/abs/2601.04946v1&entry.124074799=Read"},
{"title": "Text as a Universal Interface for Transferable Personalization", "author": "Yuting Liu and Jian Guan and Jia-Nan Li and Wei Wu and Jiang-Ming Yang and Jianzhe Zhao and Guibing Guo", "abstract": "We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.", "link": "http://arxiv.org/abs/2601.04963v1", "date": "2026-01-08", "relevancy": 2.0633, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5223}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5165}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text%20as%20a%20Universal%20Interface%20for%20Transferable%20Personalization&body=Title%3A%20Text%20as%20a%20Universal%20Interface%20for%20Transferable%20Personalization%0AAuthor%3A%20Yuting%20Liu%20and%20Jian%20Guan%20and%20Jia-Nan%20Li%20and%20Wei%20Wu%20and%20Jiang-Ming%20Yang%20and%20Jianzhe%20Zhao%20and%20Guibing%20Guo%0AAbstract%3A%20We%20study%20the%20problem%20of%20personalization%20in%20large%20language%20models%20%28LLMs%29.%20Prior%20work%20predominantly%20represents%20user%20preferences%20as%20implicit%2C%20model-specific%20vectors%20or%20parameters%2C%20yielding%20opaque%20%60%60black-box%27%27%20profiles%20that%20are%20difficult%20to%20interpret%20and%20transfer%20across%20models%20and%20tasks.%20In%20contrast%2C%20we%20advocate%20natural%20language%20as%20a%20universal%2C%20model-%20and%20task-agnostic%20interface%20for%20preference%20representation.%20The%20formulation%20leads%20to%20interpretable%20and%20reusable%20preference%20descriptions%2C%20while%20naturally%20supporting%20continual%20evolution%20as%20new%20interactions%20are%20observed.%20To%20learn%20such%20representations%2C%20we%20introduce%20a%20two-stage%20training%20framework%20that%20combines%20supervised%20fine-tuning%20on%20high-quality%20synthesized%20data%20with%20reinforcement%20learning%20to%20optimize%20long-term%20utility%20and%20cross-task%20transferability.%20Based%20on%20this%20framework%2C%20we%20develop%20AlignXplore%2B%2C%20a%20universal%20preference%20reasoning%20model%20that%20generates%20textual%20preference%20summaries.%20Experiments%20on%20nine%20benchmarks%20show%20that%20our%208B%20model%20achieves%20state-of-the-art%20performanc%20--%20outperforming%20substantially%20larger%20open-source%20models%20--%20while%20exhibiting%20strong%20transferability%20across%20tasks%2C%20model%20families%2C%20and%20interaction%20formats.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText%2520as%2520a%2520Universal%2520Interface%2520for%2520Transferable%2520Personalization%26entry.906535625%3DYuting%2520Liu%2520and%2520Jian%2520Guan%2520and%2520Jia-Nan%2520Li%2520and%2520Wei%2520Wu%2520and%2520Jiang-Ming%2520Yang%2520and%2520Jianzhe%2520Zhao%2520and%2520Guibing%2520Guo%26entry.1292438233%3DWe%2520study%2520the%2520problem%2520of%2520personalization%2520in%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Prior%2520work%2520predominantly%2520represents%2520user%2520preferences%2520as%2520implicit%252C%2520model-specific%2520vectors%2520or%2520parameters%252C%2520yielding%2520opaque%2520%2560%2560black-box%2527%2527%2520profiles%2520that%2520are%2520difficult%2520to%2520interpret%2520and%2520transfer%2520across%2520models%2520and%2520tasks.%2520In%2520contrast%252C%2520we%2520advocate%2520natural%2520language%2520as%2520a%2520universal%252C%2520model-%2520and%2520task-agnostic%2520interface%2520for%2520preference%2520representation.%2520The%2520formulation%2520leads%2520to%2520interpretable%2520and%2520reusable%2520preference%2520descriptions%252C%2520while%2520naturally%2520supporting%2520continual%2520evolution%2520as%2520new%2520interactions%2520are%2520observed.%2520To%2520learn%2520such%2520representations%252C%2520we%2520introduce%2520a%2520two-stage%2520training%2520framework%2520that%2520combines%2520supervised%2520fine-tuning%2520on%2520high-quality%2520synthesized%2520data%2520with%2520reinforcement%2520learning%2520to%2520optimize%2520long-term%2520utility%2520and%2520cross-task%2520transferability.%2520Based%2520on%2520this%2520framework%252C%2520we%2520develop%2520AlignXplore%252B%252C%2520a%2520universal%2520preference%2520reasoning%2520model%2520that%2520generates%2520textual%2520preference%2520summaries.%2520Experiments%2520on%2520nine%2520benchmarks%2520show%2520that%2520our%25208B%2520model%2520achieves%2520state-of-the-art%2520performanc%2520--%2520outperforming%2520substantially%2520larger%2520open-source%2520models%2520--%2520while%2520exhibiting%2520strong%2520transferability%2520across%2520tasks%252C%2520model%2520families%252C%2520and%2520interaction%2520formats.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text%20as%20a%20Universal%20Interface%20for%20Transferable%20Personalization&entry.906535625=Yuting%20Liu%20and%20Jian%20Guan%20and%20Jia-Nan%20Li%20and%20Wei%20Wu%20and%20Jiang-Ming%20Yang%20and%20Jianzhe%20Zhao%20and%20Guibing%20Guo&entry.1292438233=We%20study%20the%20problem%20of%20personalization%20in%20large%20language%20models%20%28LLMs%29.%20Prior%20work%20predominantly%20represents%20user%20preferences%20as%20implicit%2C%20model-specific%20vectors%20or%20parameters%2C%20yielding%20opaque%20%60%60black-box%27%27%20profiles%20that%20are%20difficult%20to%20interpret%20and%20transfer%20across%20models%20and%20tasks.%20In%20contrast%2C%20we%20advocate%20natural%20language%20as%20a%20universal%2C%20model-%20and%20task-agnostic%20interface%20for%20preference%20representation.%20The%20formulation%20leads%20to%20interpretable%20and%20reusable%20preference%20descriptions%2C%20while%20naturally%20supporting%20continual%20evolution%20as%20new%20interactions%20are%20observed.%20To%20learn%20such%20representations%2C%20we%20introduce%20a%20two-stage%20training%20framework%20that%20combines%20supervised%20fine-tuning%20on%20high-quality%20synthesized%20data%20with%20reinforcement%20learning%20to%20optimize%20long-term%20utility%20and%20cross-task%20transferability.%20Based%20on%20this%20framework%2C%20we%20develop%20AlignXplore%2B%2C%20a%20universal%20preference%20reasoning%20model%20that%20generates%20textual%20preference%20summaries.%20Experiments%20on%20nine%20benchmarks%20show%20that%20our%208B%20model%20achieves%20state-of-the-art%20performanc%20--%20outperforming%20substantially%20larger%20open-source%20models%20--%20while%20exhibiting%20strong%20transferability%20across%20tasks%2C%20model%20families%2C%20and%20interaction%20formats.&entry.1838667208=http%3A//arxiv.org/abs/2601.04963v1&entry.124074799=Read"},
{"title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment", "author": "Rhitabrat Pokharel and Hamid Reza Hassanzadeh and Ameeta Agrawal", "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.", "link": "http://arxiv.org/abs/2601.01266v2", "date": "2026-01-08", "relevancy": 2.0584, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5203}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Policy%20to%20Logic%20for%20Efficient%20and%20Interpretable%20Coverage%20Assessment&body=Title%3A%20From%20Policy%20to%20Logic%20for%20Efficient%20and%20Interpretable%20Coverage%20Assessment%0AAuthor%3A%20Rhitabrat%20Pokharel%20and%20Hamid%20Reza%20Hassanzadeh%20and%20Ameeta%20Agrawal%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20capabilities%20in%20interpreting%20lengthy%2C%20complex%20legal%20and%20policy%20language.%20However%2C%20their%20reliability%20can%20be%20undermined%20by%20hallucinations%20and%20inconsistencies%2C%20particularly%20when%20analyzing%20subjective%20and%20nuanced%20documents.%20These%20challenges%20are%20especially%20critical%20in%20medical%20coverage%20policy%20review%2C%20where%20human%20experts%20must%20be%20able%20to%20rely%20on%20accurate%20information.%20In%20this%20paper%2C%20we%20present%20an%20approach%20designed%20to%20support%20human%20reviewers%20by%20making%20policy%20interpretation%20more%20efficient%20and%20interpretable.%20We%20introduce%20a%20methodology%20that%20pairs%20a%20coverage-aware%20retriever%20with%20symbolic%20rule-based%20reasoning%20to%20surface%20relevant%20policy%20language%2C%20organize%20it%20into%20explicit%20facts%20and%20rules%2C%20and%20generate%20auditable%20rationales.%20This%20hybrid%20system%20minimizes%20the%20number%20of%20LLM%20inferences%20required%20which%20reduces%20overall%20model%20cost.%20Notably%2C%20our%20approach%20achieves%20a%2044%25%20reduction%20in%20inference%20cost%20alongside%20a%204.5%25%20improvement%20in%20F1%20score%2C%20demonstrating%20both%20efficiency%20and%20effectiveness.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01266v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Policy%2520to%2520Logic%2520for%2520Efficient%2520and%2520Interpretable%2520Coverage%2520Assessment%26entry.906535625%3DRhitabrat%2520Pokharel%2520and%2520Hamid%2520Reza%2520Hassanzadeh%2520and%2520Ameeta%2520Agrawal%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520in%2520interpreting%2520lengthy%252C%2520complex%2520legal%2520and%2520policy%2520language.%2520However%252C%2520their%2520reliability%2520can%2520be%2520undermined%2520by%2520hallucinations%2520and%2520inconsistencies%252C%2520particularly%2520when%2520analyzing%2520subjective%2520and%2520nuanced%2520documents.%2520These%2520challenges%2520are%2520especially%2520critical%2520in%2520medical%2520coverage%2520policy%2520review%252C%2520where%2520human%2520experts%2520must%2520be%2520able%2520to%2520rely%2520on%2520accurate%2520information.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520approach%2520designed%2520to%2520support%2520human%2520reviewers%2520by%2520making%2520policy%2520interpretation%2520more%2520efficient%2520and%2520interpretable.%2520We%2520introduce%2520a%2520methodology%2520that%2520pairs%2520a%2520coverage-aware%2520retriever%2520with%2520symbolic%2520rule-based%2520reasoning%2520to%2520surface%2520relevant%2520policy%2520language%252C%2520organize%2520it%2520into%2520explicit%2520facts%2520and%2520rules%252C%2520and%2520generate%2520auditable%2520rationales.%2520This%2520hybrid%2520system%2520minimizes%2520the%2520number%2520of%2520LLM%2520inferences%2520required%2520which%2520reduces%2520overall%2520model%2520cost.%2520Notably%252C%2520our%2520approach%2520achieves%2520a%252044%2525%2520reduction%2520in%2520inference%2520cost%2520alongside%2520a%25204.5%2525%2520improvement%2520in%2520F1%2520score%252C%2520demonstrating%2520both%2520efficiency%2520and%2520effectiveness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01266v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Policy%20to%20Logic%20for%20Efficient%20and%20Interpretable%20Coverage%20Assessment&entry.906535625=Rhitabrat%20Pokharel%20and%20Hamid%20Reza%20Hassanzadeh%20and%20Ameeta%20Agrawal&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20capabilities%20in%20interpreting%20lengthy%2C%20complex%20legal%20and%20policy%20language.%20However%2C%20their%20reliability%20can%20be%20undermined%20by%20hallucinations%20and%20inconsistencies%2C%20particularly%20when%20analyzing%20subjective%20and%20nuanced%20documents.%20These%20challenges%20are%20especially%20critical%20in%20medical%20coverage%20policy%20review%2C%20where%20human%20experts%20must%20be%20able%20to%20rely%20on%20accurate%20information.%20In%20this%20paper%2C%20we%20present%20an%20approach%20designed%20to%20support%20human%20reviewers%20by%20making%20policy%20interpretation%20more%20efficient%20and%20interpretable.%20We%20introduce%20a%20methodology%20that%20pairs%20a%20coverage-aware%20retriever%20with%20symbolic%20rule-based%20reasoning%20to%20surface%20relevant%20policy%20language%2C%20organize%20it%20into%20explicit%20facts%20and%20rules%2C%20and%20generate%20auditable%20rationales.%20This%20hybrid%20system%20minimizes%20the%20number%20of%20LLM%20inferences%20required%20which%20reduces%20overall%20model%20cost.%20Notably%2C%20our%20approach%20achieves%20a%2044%25%20reduction%20in%20inference%20cost%20alongside%20a%204.5%25%20improvement%20in%20F1%20score%2C%20demonstrating%20both%20efficiency%20and%20effectiveness.&entry.1838667208=http%3A//arxiv.org/abs/2601.01266v2&entry.124074799=Read"},
{"title": "Act-Adaptive Margin: Dynamically Calibrating Reward Models for Subjective Ambiguity", "author": "Feiteng Fang and Dingwei Chen and Xiang Huang and Ting-En Lin and Yuchuan Wu and Xiong Liu and Xinge Ye and Ziqiang Liu and Haonan Zhang and Liang Zhu and Hamid Alinejad-Rokny and Min Yang and Yongbin Li", "abstract": "Currently, most reinforcement learning tasks focus on domains like mathematics and programming, where verification is relatively straightforward. However, in subjective tasks such as role-playing, alignment techniques struggle to make progress, primarily because subjective reward modeling using the Bradley-Terry model faces significant challenges when dealing with ambiguous preferences. To improve reward modeling in subjective tasks, this paper proposes AAM (\\textbf{\\underline{A}}ct-\\textbf{\\underline{A}}daptive \\textbf{\\underline{M}}argin), which enhances reward modeling by dynamically calibrating preference margins using the model's internal parameter knowledge. We design two versions of AAM that efficiently generate contextually-appropriate preference gaps without additional human annotation. This approach fundamentally improves how reward models handle subjective rewards by better integrating generative understanding with preference scoring. To validate AAM's effectiveness in subjective reward modeling, we conduct evaluations on RewardBench, JudgeBench, and challenging role-playing tasks. Results show that AAM significantly improves subjective reward modeling performance, enhancing Bradley-Terry reward models by 2.95\\% in general tasks and 4.85\\% in subjective role-playing tasks. Furthermore, reward models trained with AAM can help downstream alignment tasks achieve better results. Our test results show that applying rewards generated by AAM-Augmented RM to preference learning techniques (e.g., GRPO) achieves state-of-the-art results on CharacterEval and Charm. Code and dataset are available at https://github.com/calubkk/AAM.", "link": "http://arxiv.org/abs/2505.23923v2", "date": "2026-01-08", "relevancy": 2.0537, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5177}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5171}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Act-Adaptive%20Margin%3A%20Dynamically%20Calibrating%20Reward%20Models%20for%20Subjective%20Ambiguity&body=Title%3A%20Act-Adaptive%20Margin%3A%20Dynamically%20Calibrating%20Reward%20Models%20for%20Subjective%20Ambiguity%0AAuthor%3A%20Feiteng%20Fang%20and%20Dingwei%20Chen%20and%20Xiang%20Huang%20and%20Ting-En%20Lin%20and%20Yuchuan%20Wu%20and%20Xiong%20Liu%20and%20Xinge%20Ye%20and%20Ziqiang%20Liu%20and%20Haonan%20Zhang%20and%20Liang%20Zhu%20and%20Hamid%20Alinejad-Rokny%20and%20Min%20Yang%20and%20Yongbin%20Li%0AAbstract%3A%20Currently%2C%20most%20reinforcement%20learning%20tasks%20focus%20on%20domains%20like%20mathematics%20and%20programming%2C%20where%20verification%20is%20relatively%20straightforward.%20However%2C%20in%20subjective%20tasks%20such%20as%20role-playing%2C%20alignment%20techniques%20struggle%20to%20make%20progress%2C%20primarily%20because%20subjective%20reward%20modeling%20using%20the%20Bradley-Terry%20model%20faces%20significant%20challenges%20when%20dealing%20with%20ambiguous%20preferences.%20To%20improve%20reward%20modeling%20in%20subjective%20tasks%2C%20this%20paper%20proposes%20AAM%20%28%5Ctextbf%7B%5Cunderline%7BA%7D%7Dct-%5Ctextbf%7B%5Cunderline%7BA%7D%7Ddaptive%20%5Ctextbf%7B%5Cunderline%7BM%7D%7Dargin%29%2C%20which%20enhances%20reward%20modeling%20by%20dynamically%20calibrating%20preference%20margins%20using%20the%20model%27s%20internal%20parameter%20knowledge.%20We%20design%20two%20versions%20of%20AAM%20that%20efficiently%20generate%20contextually-appropriate%20preference%20gaps%20without%20additional%20human%20annotation.%20This%20approach%20fundamentally%20improves%20how%20reward%20models%20handle%20subjective%20rewards%20by%20better%20integrating%20generative%20understanding%20with%20preference%20scoring.%20To%20validate%20AAM%27s%20effectiveness%20in%20subjective%20reward%20modeling%2C%20we%20conduct%20evaluations%20on%20RewardBench%2C%20JudgeBench%2C%20and%20challenging%20role-playing%20tasks.%20Results%20show%20that%20AAM%20significantly%20improves%20subjective%20reward%20modeling%20performance%2C%20enhancing%20Bradley-Terry%20reward%20models%20by%202.95%5C%25%20in%20general%20tasks%20and%204.85%5C%25%20in%20subjective%20role-playing%20tasks.%20Furthermore%2C%20reward%20models%20trained%20with%20AAM%20can%20help%20downstream%20alignment%20tasks%20achieve%20better%20results.%20Our%20test%20results%20show%20that%20applying%20rewards%20generated%20by%20AAM-Augmented%20RM%20to%20preference%20learning%20techniques%20%28e.g.%2C%20GRPO%29%20achieves%20state-of-the-art%20results%20on%20CharacterEval%20and%20Charm.%20Code%20and%20dataset%20are%20available%20at%20https%3A//github.com/calubkk/AAM.%0ALink%3A%20http%3A//arxiv.org/abs/2505.23923v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAct-Adaptive%2520Margin%253A%2520Dynamically%2520Calibrating%2520Reward%2520Models%2520for%2520Subjective%2520Ambiguity%26entry.906535625%3DFeiteng%2520Fang%2520and%2520Dingwei%2520Chen%2520and%2520Xiang%2520Huang%2520and%2520Ting-En%2520Lin%2520and%2520Yuchuan%2520Wu%2520and%2520Xiong%2520Liu%2520and%2520Xinge%2520Ye%2520and%2520Ziqiang%2520Liu%2520and%2520Haonan%2520Zhang%2520and%2520Liang%2520Zhu%2520and%2520Hamid%2520Alinejad-Rokny%2520and%2520Min%2520Yang%2520and%2520Yongbin%2520Li%26entry.1292438233%3DCurrently%252C%2520most%2520reinforcement%2520learning%2520tasks%2520focus%2520on%2520domains%2520like%2520mathematics%2520and%2520programming%252C%2520where%2520verification%2520is%2520relatively%2520straightforward.%2520However%252C%2520in%2520subjective%2520tasks%2520such%2520as%2520role-playing%252C%2520alignment%2520techniques%2520struggle%2520to%2520make%2520progress%252C%2520primarily%2520because%2520subjective%2520reward%2520modeling%2520using%2520the%2520Bradley-Terry%2520model%2520faces%2520significant%2520challenges%2520when%2520dealing%2520with%2520ambiguous%2520preferences.%2520To%2520improve%2520reward%2520modeling%2520in%2520subjective%2520tasks%252C%2520this%2520paper%2520proposes%2520AAM%2520%2528%255Ctextbf%257B%255Cunderline%257BA%257D%257Dct-%255Ctextbf%257B%255Cunderline%257BA%257D%257Ddaptive%2520%255Ctextbf%257B%255Cunderline%257BM%257D%257Dargin%2529%252C%2520which%2520enhances%2520reward%2520modeling%2520by%2520dynamically%2520calibrating%2520preference%2520margins%2520using%2520the%2520model%2527s%2520internal%2520parameter%2520knowledge.%2520We%2520design%2520two%2520versions%2520of%2520AAM%2520that%2520efficiently%2520generate%2520contextually-appropriate%2520preference%2520gaps%2520without%2520additional%2520human%2520annotation.%2520This%2520approach%2520fundamentally%2520improves%2520how%2520reward%2520models%2520handle%2520subjective%2520rewards%2520by%2520better%2520integrating%2520generative%2520understanding%2520with%2520preference%2520scoring.%2520To%2520validate%2520AAM%2527s%2520effectiveness%2520in%2520subjective%2520reward%2520modeling%252C%2520we%2520conduct%2520evaluations%2520on%2520RewardBench%252C%2520JudgeBench%252C%2520and%2520challenging%2520role-playing%2520tasks.%2520Results%2520show%2520that%2520AAM%2520significantly%2520improves%2520subjective%2520reward%2520modeling%2520performance%252C%2520enhancing%2520Bradley-Terry%2520reward%2520models%2520by%25202.95%255C%2525%2520in%2520general%2520tasks%2520and%25204.85%255C%2525%2520in%2520subjective%2520role-playing%2520tasks.%2520Furthermore%252C%2520reward%2520models%2520trained%2520with%2520AAM%2520can%2520help%2520downstream%2520alignment%2520tasks%2520achieve%2520better%2520results.%2520Our%2520test%2520results%2520show%2520that%2520applying%2520rewards%2520generated%2520by%2520AAM-Augmented%2520RM%2520to%2520preference%2520learning%2520techniques%2520%2528e.g.%252C%2520GRPO%2529%2520achieves%2520state-of-the-art%2520results%2520on%2520CharacterEval%2520and%2520Charm.%2520Code%2520and%2520dataset%2520are%2520available%2520at%2520https%253A//github.com/calubkk/AAM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23923v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Act-Adaptive%20Margin%3A%20Dynamically%20Calibrating%20Reward%20Models%20for%20Subjective%20Ambiguity&entry.906535625=Feiteng%20Fang%20and%20Dingwei%20Chen%20and%20Xiang%20Huang%20and%20Ting-En%20Lin%20and%20Yuchuan%20Wu%20and%20Xiong%20Liu%20and%20Xinge%20Ye%20and%20Ziqiang%20Liu%20and%20Haonan%20Zhang%20and%20Liang%20Zhu%20and%20Hamid%20Alinejad-Rokny%20and%20Min%20Yang%20and%20Yongbin%20Li&entry.1292438233=Currently%2C%20most%20reinforcement%20learning%20tasks%20focus%20on%20domains%20like%20mathematics%20and%20programming%2C%20where%20verification%20is%20relatively%20straightforward.%20However%2C%20in%20subjective%20tasks%20such%20as%20role-playing%2C%20alignment%20techniques%20struggle%20to%20make%20progress%2C%20primarily%20because%20subjective%20reward%20modeling%20using%20the%20Bradley-Terry%20model%20faces%20significant%20challenges%20when%20dealing%20with%20ambiguous%20preferences.%20To%20improve%20reward%20modeling%20in%20subjective%20tasks%2C%20this%20paper%20proposes%20AAM%20%28%5Ctextbf%7B%5Cunderline%7BA%7D%7Dct-%5Ctextbf%7B%5Cunderline%7BA%7D%7Ddaptive%20%5Ctextbf%7B%5Cunderline%7BM%7D%7Dargin%29%2C%20which%20enhances%20reward%20modeling%20by%20dynamically%20calibrating%20preference%20margins%20using%20the%20model%27s%20internal%20parameter%20knowledge.%20We%20design%20two%20versions%20of%20AAM%20that%20efficiently%20generate%20contextually-appropriate%20preference%20gaps%20without%20additional%20human%20annotation.%20This%20approach%20fundamentally%20improves%20how%20reward%20models%20handle%20subjective%20rewards%20by%20better%20integrating%20generative%20understanding%20with%20preference%20scoring.%20To%20validate%20AAM%27s%20effectiveness%20in%20subjective%20reward%20modeling%2C%20we%20conduct%20evaluations%20on%20RewardBench%2C%20JudgeBench%2C%20and%20challenging%20role-playing%20tasks.%20Results%20show%20that%20AAM%20significantly%20improves%20subjective%20reward%20modeling%20performance%2C%20enhancing%20Bradley-Terry%20reward%20models%20by%202.95%5C%25%20in%20general%20tasks%20and%204.85%5C%25%20in%20subjective%20role-playing%20tasks.%20Furthermore%2C%20reward%20models%20trained%20with%20AAM%20can%20help%20downstream%20alignment%20tasks%20achieve%20better%20results.%20Our%20test%20results%20show%20that%20applying%20rewards%20generated%20by%20AAM-Augmented%20RM%20to%20preference%20learning%20techniques%20%28e.g.%2C%20GRPO%29%20achieves%20state-of-the-art%20results%20on%20CharacterEval%20and%20Charm.%20Code%20and%20dataset%20are%20available%20at%20https%3A//github.com/calubkk/AAM.&entry.1838667208=http%3A//arxiv.org/abs/2505.23923v2&entry.124074799=Read"},
{"title": "Compensation Effect Amplification Control (CEAC): A movement-based approach for coordinated position and velocity control of the elbow of upper-limb prostheses", "author": "Julian Kulozik and Nathana\u00ebl Jarrass\u00e9", "abstract": "Despite advances in upper-limb (UL) prosthetic design, achieving intuitive control of intermediate joints - such as the wrist and elbow - remains challenging, particularly for continuous and velocity-modulated movements. We introduce a novel movement-based control paradigm entitled Compensation Effect Amplification Control (CEAC) that leverages users' trunk flexion and extension as input for controlling prosthetic elbow velocity. Considering that the trunk can be both a functional and compensatory joint when performing upper-limb actions, CEAC amplifies the natural coupling between trunk and prosthesis while introducing a controlled delay that allows users to modulate both the position and velocity of the prosthetic joint. We evaluated CEAC in a generic drawing task performed by twelve able-bodied participants using a supernumerary prosthesis with an active elbow. Additionally a multiple-target-reaching task was performed by a subset of ten participants. Results demonstrate task performances comparable to those obtained with natural arm movements, even when gesture velocity or drawing size were varied, while maintaining ergonomic trunk postures. Analysis revealed that CEAC effectively restores joint coordinated action, distributes movement effort between trunk and elbow, enabling intuitive trajectory control without requiring extreme compensatory movements. Overall, CEAC offers a promising control strategy for intermediate joints of UL prostheses, particularly in tasks requiring continuous and precise coordination.", "link": "http://arxiv.org/abs/2601.05074v1", "date": "2026-01-08", "relevancy": 2.0435, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5416}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4932}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compensation%20Effect%20Amplification%20Control%20%28CEAC%29%3A%20A%20movement-based%20approach%20for%20coordinated%20position%20and%20velocity%20control%20of%20the%20elbow%20of%20upper-limb%20prostheses&body=Title%3A%20Compensation%20Effect%20Amplification%20Control%20%28CEAC%29%3A%20A%20movement-based%20approach%20for%20coordinated%20position%20and%20velocity%20control%20of%20the%20elbow%20of%20upper-limb%20prostheses%0AAuthor%3A%20Julian%20Kulozik%20and%20Nathana%C3%ABl%20Jarrass%C3%A9%0AAbstract%3A%20Despite%20advances%20in%20upper-limb%20%28UL%29%20prosthetic%20design%2C%20achieving%20intuitive%20control%20of%20intermediate%20joints%20-%20such%20as%20the%20wrist%20and%20elbow%20-%20remains%20challenging%2C%20particularly%20for%20continuous%20and%20velocity-modulated%20movements.%20We%20introduce%20a%20novel%20movement-based%20control%20paradigm%20entitled%20Compensation%20Effect%20Amplification%20Control%20%28CEAC%29%20that%20leverages%20users%27%20trunk%20flexion%20and%20extension%20as%20input%20for%20controlling%20prosthetic%20elbow%20velocity.%20Considering%20that%20the%20trunk%20can%20be%20both%20a%20functional%20and%20compensatory%20joint%20when%20performing%20upper-limb%20actions%2C%20CEAC%20amplifies%20the%20natural%20coupling%20between%20trunk%20and%20prosthesis%20while%20introducing%20a%20controlled%20delay%20that%20allows%20users%20to%20modulate%20both%20the%20position%20and%20velocity%20of%20the%20prosthetic%20joint.%20We%20evaluated%20CEAC%20in%20a%20generic%20drawing%20task%20performed%20by%20twelve%20able-bodied%20participants%20using%20a%20supernumerary%20prosthesis%20with%20an%20active%20elbow.%20Additionally%20a%20multiple-target-reaching%20task%20was%20performed%20by%20a%20subset%20of%20ten%20participants.%20Results%20demonstrate%20task%20performances%20comparable%20to%20those%20obtained%20with%20natural%20arm%20movements%2C%20even%20when%20gesture%20velocity%20or%20drawing%20size%20were%20varied%2C%20while%20maintaining%20ergonomic%20trunk%20postures.%20Analysis%20revealed%20that%20CEAC%20effectively%20restores%20joint%20coordinated%20action%2C%20distributes%20movement%20effort%20between%20trunk%20and%20elbow%2C%20enabling%20intuitive%20trajectory%20control%20without%20requiring%20extreme%20compensatory%20movements.%20Overall%2C%20CEAC%20offers%20a%20promising%20control%20strategy%20for%20intermediate%20joints%20of%20UL%20prostheses%2C%20particularly%20in%20tasks%20requiring%20continuous%20and%20precise%20coordination.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompensation%2520Effect%2520Amplification%2520Control%2520%2528CEAC%2529%253A%2520A%2520movement-based%2520approach%2520for%2520coordinated%2520position%2520and%2520velocity%2520control%2520of%2520the%2520elbow%2520of%2520upper-limb%2520prostheses%26entry.906535625%3DJulian%2520Kulozik%2520and%2520Nathana%25C3%25ABl%2520Jarrass%25C3%25A9%26entry.1292438233%3DDespite%2520advances%2520in%2520upper-limb%2520%2528UL%2529%2520prosthetic%2520design%252C%2520achieving%2520intuitive%2520control%2520of%2520intermediate%2520joints%2520-%2520such%2520as%2520the%2520wrist%2520and%2520elbow%2520-%2520remains%2520challenging%252C%2520particularly%2520for%2520continuous%2520and%2520velocity-modulated%2520movements.%2520We%2520introduce%2520a%2520novel%2520movement-based%2520control%2520paradigm%2520entitled%2520Compensation%2520Effect%2520Amplification%2520Control%2520%2528CEAC%2529%2520that%2520leverages%2520users%2527%2520trunk%2520flexion%2520and%2520extension%2520as%2520input%2520for%2520controlling%2520prosthetic%2520elbow%2520velocity.%2520Considering%2520that%2520the%2520trunk%2520can%2520be%2520both%2520a%2520functional%2520and%2520compensatory%2520joint%2520when%2520performing%2520upper-limb%2520actions%252C%2520CEAC%2520amplifies%2520the%2520natural%2520coupling%2520between%2520trunk%2520and%2520prosthesis%2520while%2520introducing%2520a%2520controlled%2520delay%2520that%2520allows%2520users%2520to%2520modulate%2520both%2520the%2520position%2520and%2520velocity%2520of%2520the%2520prosthetic%2520joint.%2520We%2520evaluated%2520CEAC%2520in%2520a%2520generic%2520drawing%2520task%2520performed%2520by%2520twelve%2520able-bodied%2520participants%2520using%2520a%2520supernumerary%2520prosthesis%2520with%2520an%2520active%2520elbow.%2520Additionally%2520a%2520multiple-target-reaching%2520task%2520was%2520performed%2520by%2520a%2520subset%2520of%2520ten%2520participants.%2520Results%2520demonstrate%2520task%2520performances%2520comparable%2520to%2520those%2520obtained%2520with%2520natural%2520arm%2520movements%252C%2520even%2520when%2520gesture%2520velocity%2520or%2520drawing%2520size%2520were%2520varied%252C%2520while%2520maintaining%2520ergonomic%2520trunk%2520postures.%2520Analysis%2520revealed%2520that%2520CEAC%2520effectively%2520restores%2520joint%2520coordinated%2520action%252C%2520distributes%2520movement%2520effort%2520between%2520trunk%2520and%2520elbow%252C%2520enabling%2520intuitive%2520trajectory%2520control%2520without%2520requiring%2520extreme%2520compensatory%2520movements.%2520Overall%252C%2520CEAC%2520offers%2520a%2520promising%2520control%2520strategy%2520for%2520intermediate%2520joints%2520of%2520UL%2520prostheses%252C%2520particularly%2520in%2520tasks%2520requiring%2520continuous%2520and%2520precise%2520coordination.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compensation%20Effect%20Amplification%20Control%20%28CEAC%29%3A%20A%20movement-based%20approach%20for%20coordinated%20position%20and%20velocity%20control%20of%20the%20elbow%20of%20upper-limb%20prostheses&entry.906535625=Julian%20Kulozik%20and%20Nathana%C3%ABl%20Jarrass%C3%A9&entry.1292438233=Despite%20advances%20in%20upper-limb%20%28UL%29%20prosthetic%20design%2C%20achieving%20intuitive%20control%20of%20intermediate%20joints%20-%20such%20as%20the%20wrist%20and%20elbow%20-%20remains%20challenging%2C%20particularly%20for%20continuous%20and%20velocity-modulated%20movements.%20We%20introduce%20a%20novel%20movement-based%20control%20paradigm%20entitled%20Compensation%20Effect%20Amplification%20Control%20%28CEAC%29%20that%20leverages%20users%27%20trunk%20flexion%20and%20extension%20as%20input%20for%20controlling%20prosthetic%20elbow%20velocity.%20Considering%20that%20the%20trunk%20can%20be%20both%20a%20functional%20and%20compensatory%20joint%20when%20performing%20upper-limb%20actions%2C%20CEAC%20amplifies%20the%20natural%20coupling%20between%20trunk%20and%20prosthesis%20while%20introducing%20a%20controlled%20delay%20that%20allows%20users%20to%20modulate%20both%20the%20position%20and%20velocity%20of%20the%20prosthetic%20joint.%20We%20evaluated%20CEAC%20in%20a%20generic%20drawing%20task%20performed%20by%20twelve%20able-bodied%20participants%20using%20a%20supernumerary%20prosthesis%20with%20an%20active%20elbow.%20Additionally%20a%20multiple-target-reaching%20task%20was%20performed%20by%20a%20subset%20of%20ten%20participants.%20Results%20demonstrate%20task%20performances%20comparable%20to%20those%20obtained%20with%20natural%20arm%20movements%2C%20even%20when%20gesture%20velocity%20or%20drawing%20size%20were%20varied%2C%20while%20maintaining%20ergonomic%20trunk%20postures.%20Analysis%20revealed%20that%20CEAC%20effectively%20restores%20joint%20coordinated%20action%2C%20distributes%20movement%20effort%20between%20trunk%20and%20elbow%2C%20enabling%20intuitive%20trajectory%20control%20without%20requiring%20extreme%20compensatory%20movements.%20Overall%2C%20CEAC%20offers%20a%20promising%20control%20strategy%20for%20intermediate%20joints%20of%20UL%20prostheses%2C%20particularly%20in%20tasks%20requiring%20continuous%20and%20precise%20coordination.&entry.1838667208=http%3A//arxiv.org/abs/2601.05074v1&entry.124074799=Read"},
{"title": "Milestones over Outcome: Unlocking Geometric Reasoning with Sub-Goal Verifiable Reward", "author": "Jianlong Chen and Daocheng Fu and Shengze Xu and Jiawei Chen and Yuan Feng and Yue Yang and Junchi Yan and Hongyuan Zha and Renqiu Xia", "abstract": "Multimodal Large Language Models (MLLMs) struggle with complex geometric reasoning, largely because \"black box\" outcome-based supervision fails to distinguish between lucky guesses and rigorous deduction. To address this, we introduce a paradigm shift towards subgoal-level evaluation and learning. We first construct GeoGoal, a benchmark synthesized via a rigorous formal verification data engine, which converts abstract proofs into verifiable numeric subgoals. This structure reveals a critical divergence between reasoning quality and outcome accuracy. Leveraging this, we propose the Sub-Goal Verifiable Reward (SGVR) framework, which replaces sparse signals with dense rewards based on the Skeleton Rate. Experiments demonstrate that SGVR not only enhances geometric performance (+9.7%) but also exhibits strong generalization, transferring gains to general math (+8.0%) and other general reasoning tasks (+2.8%), demonstrating broad applicability across diverse domains.", "link": "http://arxiv.org/abs/2601.05073v1", "date": "2026-01-08", "relevancy": 2.0388, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Milestones%20over%20Outcome%3A%20Unlocking%20Geometric%20Reasoning%20with%20Sub-Goal%20Verifiable%20Reward&body=Title%3A%20Milestones%20over%20Outcome%3A%20Unlocking%20Geometric%20Reasoning%20with%20Sub-Goal%20Verifiable%20Reward%0AAuthor%3A%20Jianlong%20Chen%20and%20Daocheng%20Fu%20and%20Shengze%20Xu%20and%20Jiawei%20Chen%20and%20Yuan%20Feng%20and%20Yue%20Yang%20and%20Junchi%20Yan%20and%20Hongyuan%20Zha%20and%20Renqiu%20Xia%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20struggle%20with%20complex%20geometric%20reasoning%2C%20largely%20because%20%22black%20box%22%20outcome-based%20supervision%20fails%20to%20distinguish%20between%20lucky%20guesses%20and%20rigorous%20deduction.%20To%20address%20this%2C%20we%20introduce%20a%20paradigm%20shift%20towards%20subgoal-level%20evaluation%20and%20learning.%20We%20first%20construct%20GeoGoal%2C%20a%20benchmark%20synthesized%20via%20a%20rigorous%20formal%20verification%20data%20engine%2C%20which%20converts%20abstract%20proofs%20into%20verifiable%20numeric%20subgoals.%20This%20structure%20reveals%20a%20critical%20divergence%20between%20reasoning%20quality%20and%20outcome%20accuracy.%20Leveraging%20this%2C%20we%20propose%20the%20Sub-Goal%20Verifiable%20Reward%20%28SGVR%29%20framework%2C%20which%20replaces%20sparse%20signals%20with%20dense%20rewards%20based%20on%20the%20Skeleton%20Rate.%20Experiments%20demonstrate%20that%20SGVR%20not%20only%20enhances%20geometric%20performance%20%28%2B9.7%25%29%20but%20also%20exhibits%20strong%20generalization%2C%20transferring%20gains%20to%20general%20math%20%28%2B8.0%25%29%20and%20other%20general%20reasoning%20tasks%20%28%2B2.8%25%29%2C%20demonstrating%20broad%20applicability%20across%20diverse%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMilestones%2520over%2520Outcome%253A%2520Unlocking%2520Geometric%2520Reasoning%2520with%2520Sub-Goal%2520Verifiable%2520Reward%26entry.906535625%3DJianlong%2520Chen%2520and%2520Daocheng%2520Fu%2520and%2520Shengze%2520Xu%2520and%2520Jiawei%2520Chen%2520and%2520Yuan%2520Feng%2520and%2520Yue%2520Yang%2520and%2520Junchi%2520Yan%2520and%2520Hongyuan%2520Zha%2520and%2520Renqiu%2520Xia%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520struggle%2520with%2520complex%2520geometric%2520reasoning%252C%2520largely%2520because%2520%2522black%2520box%2522%2520outcome-based%2520supervision%2520fails%2520to%2520distinguish%2520between%2520lucky%2520guesses%2520and%2520rigorous%2520deduction.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520paradigm%2520shift%2520towards%2520subgoal-level%2520evaluation%2520and%2520learning.%2520We%2520first%2520construct%2520GeoGoal%252C%2520a%2520benchmark%2520synthesized%2520via%2520a%2520rigorous%2520formal%2520verification%2520data%2520engine%252C%2520which%2520converts%2520abstract%2520proofs%2520into%2520verifiable%2520numeric%2520subgoals.%2520This%2520structure%2520reveals%2520a%2520critical%2520divergence%2520between%2520reasoning%2520quality%2520and%2520outcome%2520accuracy.%2520Leveraging%2520this%252C%2520we%2520propose%2520the%2520Sub-Goal%2520Verifiable%2520Reward%2520%2528SGVR%2529%2520framework%252C%2520which%2520replaces%2520sparse%2520signals%2520with%2520dense%2520rewards%2520based%2520on%2520the%2520Skeleton%2520Rate.%2520Experiments%2520demonstrate%2520that%2520SGVR%2520not%2520only%2520enhances%2520geometric%2520performance%2520%2528%252B9.7%2525%2529%2520but%2520also%2520exhibits%2520strong%2520generalization%252C%2520transferring%2520gains%2520to%2520general%2520math%2520%2528%252B8.0%2525%2529%2520and%2520other%2520general%2520reasoning%2520tasks%2520%2528%252B2.8%2525%2529%252C%2520demonstrating%2520broad%2520applicability%2520across%2520diverse%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Milestones%20over%20Outcome%3A%20Unlocking%20Geometric%20Reasoning%20with%20Sub-Goal%20Verifiable%20Reward&entry.906535625=Jianlong%20Chen%20and%20Daocheng%20Fu%20and%20Shengze%20Xu%20and%20Jiawei%20Chen%20and%20Yuan%20Feng%20and%20Yue%20Yang%20and%20Junchi%20Yan%20and%20Hongyuan%20Zha%20and%20Renqiu%20Xia&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20struggle%20with%20complex%20geometric%20reasoning%2C%20largely%20because%20%22black%20box%22%20outcome-based%20supervision%20fails%20to%20distinguish%20between%20lucky%20guesses%20and%20rigorous%20deduction.%20To%20address%20this%2C%20we%20introduce%20a%20paradigm%20shift%20towards%20subgoal-level%20evaluation%20and%20learning.%20We%20first%20construct%20GeoGoal%2C%20a%20benchmark%20synthesized%20via%20a%20rigorous%20formal%20verification%20data%20engine%2C%20which%20converts%20abstract%20proofs%20into%20verifiable%20numeric%20subgoals.%20This%20structure%20reveals%20a%20critical%20divergence%20between%20reasoning%20quality%20and%20outcome%20accuracy.%20Leveraging%20this%2C%20we%20propose%20the%20Sub-Goal%20Verifiable%20Reward%20%28SGVR%29%20framework%2C%20which%20replaces%20sparse%20signals%20with%20dense%20rewards%20based%20on%20the%20Skeleton%20Rate.%20Experiments%20demonstrate%20that%20SGVR%20not%20only%20enhances%20geometric%20performance%20%28%2B9.7%25%29%20but%20also%20exhibits%20strong%20generalization%2C%20transferring%20gains%20to%20general%20math%20%28%2B8.0%25%29%20and%20other%20general%20reasoning%20tasks%20%28%2B2.8%25%29%2C%20demonstrating%20broad%20applicability%20across%20diverse%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2601.05073v1&entry.124074799=Read"},
{"title": "Instruction Tuning with and without Context: Behavioral Shifts and Downstream Impact", "author": "Hyunji Lee and Seunghyun Yoon and Yunjae Won and Hanseok Oh and Geewook Kim and Trung Bui and Franck Dernoncourt and Elias Stengel-Eskin and Mohit Bansal and Minjoon Seo", "abstract": "Instruction tuning is a widely used approach to improve the instruction-following ability of large language models (LLMs). Instruction-tuning datasets typically include a mixture of context-augmented and context-free examples, yet prior work has largely combined these data types without examining their distinct effects. In this paper, we investigate how training LLMs with or without context affects model behavior and downstream performance. First, in the text domain, we show that LLMs trained with context attend more strongly to the provided knowledge, achieving better grounding. We also observe that context-augmented training shifts how LLMs use knowledge: models store and leverage less on parametric knowledge and instead depend more on the provided context. Second, we observe that using LLM trained with context-augmented data as the backbone for vision-language models reduces hallucination and improves grounding in the visual domain. Finally, we explore practical strategies for real-world deployments where context availability varies. We show that maintaining separate context-augmented and context-free models and routing inputs between them yields more robust overall performance than training a single mixed model, as it better preserves their complementary strengths.", "link": "http://arxiv.org/abs/2506.15480v2", "date": "2026-01-08", "relevancy": 2.0318, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5153}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5153}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction%20Tuning%20with%20and%20without%20Context%3A%20Behavioral%20Shifts%20and%20Downstream%20Impact&body=Title%3A%20Instruction%20Tuning%20with%20and%20without%20Context%3A%20Behavioral%20Shifts%20and%20Downstream%20Impact%0AAuthor%3A%20Hyunji%20Lee%20and%20Seunghyun%20Yoon%20and%20Yunjae%20Won%20and%20Hanseok%20Oh%20and%20Geewook%20Kim%20and%20Trung%20Bui%20and%20Franck%20Dernoncourt%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%20and%20Minjoon%20Seo%0AAbstract%3A%20Instruction%20tuning%20is%20a%20widely%20used%20approach%20to%20improve%20the%20instruction-following%20ability%20of%20large%20language%20models%20%28LLMs%29.%20Instruction-tuning%20datasets%20typically%20include%20a%20mixture%20of%20context-augmented%20and%20context-free%20examples%2C%20yet%20prior%20work%20has%20largely%20combined%20these%20data%20types%20without%20examining%20their%20distinct%20effects.%20In%20this%20paper%2C%20we%20investigate%20how%20training%20LLMs%20with%20or%20without%20context%20affects%20model%20behavior%20and%20downstream%20performance.%20First%2C%20in%20the%20text%20domain%2C%20we%20show%20that%20LLMs%20trained%20with%20context%20attend%20more%20strongly%20to%20the%20provided%20knowledge%2C%20achieving%20better%20grounding.%20We%20also%20observe%20that%20context-augmented%20training%20shifts%20how%20LLMs%20use%20knowledge%3A%20models%20store%20and%20leverage%20less%20on%20parametric%20knowledge%20and%20instead%20depend%20more%20on%20the%20provided%20context.%20Second%2C%20we%20observe%20that%20using%20LLM%20trained%20with%20context-augmented%20data%20as%20the%20backbone%20for%20vision-language%20models%20reduces%20hallucination%20and%20improves%20grounding%20in%20the%20visual%20domain.%20Finally%2C%20we%20explore%20practical%20strategies%20for%20real-world%20deployments%20where%20context%20availability%20varies.%20We%20show%20that%20maintaining%20separate%20context-augmented%20and%20context-free%20models%20and%20routing%20inputs%20between%20them%20yields%20more%20robust%20overall%20performance%20than%20training%20a%20single%20mixed%20model%2C%20as%20it%20better%20preserves%20their%20complementary%20strengths.%0ALink%3A%20http%3A//arxiv.org/abs/2506.15480v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction%2520Tuning%2520with%2520and%2520without%2520Context%253A%2520Behavioral%2520Shifts%2520and%2520Downstream%2520Impact%26entry.906535625%3DHyunji%2520Lee%2520and%2520Seunghyun%2520Yoon%2520and%2520Yunjae%2520Won%2520and%2520Hanseok%2520Oh%2520and%2520Geewook%2520Kim%2520and%2520Trung%2520Bui%2520and%2520Franck%2520Dernoncourt%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Mohit%2520Bansal%2520and%2520Minjoon%2520Seo%26entry.1292438233%3DInstruction%2520tuning%2520is%2520a%2520widely%2520used%2520approach%2520to%2520improve%2520the%2520instruction-following%2520ability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Instruction-tuning%2520datasets%2520typically%2520include%2520a%2520mixture%2520of%2520context-augmented%2520and%2520context-free%2520examples%252C%2520yet%2520prior%2520work%2520has%2520largely%2520combined%2520these%2520data%2520types%2520without%2520examining%2520their%2520distinct%2520effects.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520training%2520LLMs%2520with%2520or%2520without%2520context%2520affects%2520model%2520behavior%2520and%2520downstream%2520performance.%2520First%252C%2520in%2520the%2520text%2520domain%252C%2520we%2520show%2520that%2520LLMs%2520trained%2520with%2520context%2520attend%2520more%2520strongly%2520to%2520the%2520provided%2520knowledge%252C%2520achieving%2520better%2520grounding.%2520We%2520also%2520observe%2520that%2520context-augmented%2520training%2520shifts%2520how%2520LLMs%2520use%2520knowledge%253A%2520models%2520store%2520and%2520leverage%2520less%2520on%2520parametric%2520knowledge%2520and%2520instead%2520depend%2520more%2520on%2520the%2520provided%2520context.%2520Second%252C%2520we%2520observe%2520that%2520using%2520LLM%2520trained%2520with%2520context-augmented%2520data%2520as%2520the%2520backbone%2520for%2520vision-language%2520models%2520reduces%2520hallucination%2520and%2520improves%2520grounding%2520in%2520the%2520visual%2520domain.%2520Finally%252C%2520we%2520explore%2520practical%2520strategies%2520for%2520real-world%2520deployments%2520where%2520context%2520availability%2520varies.%2520We%2520show%2520that%2520maintaining%2520separate%2520context-augmented%2520and%2520context-free%2520models%2520and%2520routing%2520inputs%2520between%2520them%2520yields%2520more%2520robust%2520overall%2520performance%2520than%2520training%2520a%2520single%2520mixed%2520model%252C%2520as%2520it%2520better%2520preserves%2520their%2520complementary%2520strengths.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15480v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction%20Tuning%20with%20and%20without%20Context%3A%20Behavioral%20Shifts%20and%20Downstream%20Impact&entry.906535625=Hyunji%20Lee%20and%20Seunghyun%20Yoon%20and%20Yunjae%20Won%20and%20Hanseok%20Oh%20and%20Geewook%20Kim%20and%20Trung%20Bui%20and%20Franck%20Dernoncourt%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%20and%20Minjoon%20Seo&entry.1292438233=Instruction%20tuning%20is%20a%20widely%20used%20approach%20to%20improve%20the%20instruction-following%20ability%20of%20large%20language%20models%20%28LLMs%29.%20Instruction-tuning%20datasets%20typically%20include%20a%20mixture%20of%20context-augmented%20and%20context-free%20examples%2C%20yet%20prior%20work%20has%20largely%20combined%20these%20data%20types%20without%20examining%20their%20distinct%20effects.%20In%20this%20paper%2C%20we%20investigate%20how%20training%20LLMs%20with%20or%20without%20context%20affects%20model%20behavior%20and%20downstream%20performance.%20First%2C%20in%20the%20text%20domain%2C%20we%20show%20that%20LLMs%20trained%20with%20context%20attend%20more%20strongly%20to%20the%20provided%20knowledge%2C%20achieving%20better%20grounding.%20We%20also%20observe%20that%20context-augmented%20training%20shifts%20how%20LLMs%20use%20knowledge%3A%20models%20store%20and%20leverage%20less%20on%20parametric%20knowledge%20and%20instead%20depend%20more%20on%20the%20provided%20context.%20Second%2C%20we%20observe%20that%20using%20LLM%20trained%20with%20context-augmented%20data%20as%20the%20backbone%20for%20vision-language%20models%20reduces%20hallucination%20and%20improves%20grounding%20in%20the%20visual%20domain.%20Finally%2C%20we%20explore%20practical%20strategies%20for%20real-world%20deployments%20where%20context%20availability%20varies.%20We%20show%20that%20maintaining%20separate%20context-augmented%20and%20context-free%20models%20and%20routing%20inputs%20between%20them%20yields%20more%20robust%20overall%20performance%20than%20training%20a%20single%20mixed%20model%2C%20as%20it%20better%20preserves%20their%20complementary%20strengths.&entry.1838667208=http%3A//arxiv.org/abs/2506.15480v2&entry.124074799=Read"},
{"title": "AlgBench: To What Extent Do Large Reasoning Models Understand Algorithms?", "author": "Henan Sun and Kaichi Yu and Yuyao Wang and Bowen Liu and Xunkai Li and Rong-Hua Li and Nuo Chen and Jia Li", "abstract": "Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs). Although notable progress has been achieved on several reasoning benchmarks such as MATH500 and LiveCodeBench, existing benchmarks for algorithmic reasoning remain limited, failing to answer a critical question: Do LRMs truly master algorithmic reasoning? To answer this question, we propose AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.\n  AlgBench consists of over 3,000 original problems spanning 27 algorithms, constructed by ACM algorithmic experts and organized under a comprehensive taxonomy, including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Empirical evaluations on leading LRMs (e.g., Gemini-3-Pro, DeepSeek-v3.2-Speciale and GPT-o3) reveal substantial performance heterogeneity: while models perform well on non-optimized tasks (up to 92%), accuracy drops sharply to around 49% on globally optimized algorithms such as dynamic programming. Further analysis uncovers \\textbf{strategic over-shifts}, wherein models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens. These findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning.", "link": "http://arxiv.org/abs/2601.04996v1", "date": "2026-01-08", "relevancy": 2.0239, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5124}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5124}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlgBench%3A%20To%20What%20Extent%20Do%20Large%20Reasoning%20Models%20Understand%20Algorithms%3F&body=Title%3A%20AlgBench%3A%20To%20What%20Extent%20Do%20Large%20Reasoning%20Models%20Understand%20Algorithms%3F%0AAuthor%3A%20Henan%20Sun%20and%20Kaichi%20Yu%20and%20Yuyao%20Wang%20and%20Bowen%20Liu%20and%20Xunkai%20Li%20and%20Rong-Hua%20Li%20and%20Nuo%20Chen%20and%20Jia%20Li%0AAbstract%3A%20Reasoning%20ability%20has%20become%20a%20central%20focus%20in%20the%20advancement%20of%20Large%20Reasoning%20Models%20%28LRMs%29.%20Although%20notable%20progress%20has%20been%20achieved%20on%20several%20reasoning%20benchmarks%20such%20as%20MATH500%20and%20LiveCodeBench%2C%20existing%20benchmarks%20for%20algorithmic%20reasoning%20remain%20limited%2C%20failing%20to%20answer%20a%20critical%20question%3A%20Do%20LRMs%20truly%20master%20algorithmic%20reasoning%3F%20To%20answer%20this%20question%2C%20we%20propose%20AlgBench%2C%20an%20expert-curated%20benchmark%20that%20evaluates%20LRMs%20under%20an%20algorithm-centric%20paradigm.%0A%20%20AlgBench%20consists%20of%20over%203%2C000%20original%20problems%20spanning%2027%20algorithms%2C%20constructed%20by%20ACM%20algorithmic%20experts%20and%20organized%20under%20a%20comprehensive%20taxonomy%2C%20including%20Euclidean-structured%2C%20non-Euclidean-structured%2C%20non-optimized%2C%20local-optimized%2C%20global-optimized%2C%20and%20heuristic-optimized%20categories.%20Empirical%20evaluations%20on%20leading%20LRMs%20%28e.g.%2C%20Gemini-3-Pro%2C%20DeepSeek-v3.2-Speciale%20and%20GPT-o3%29%20reveal%20substantial%20performance%20heterogeneity%3A%20while%20models%20perform%20well%20on%20non-optimized%20tasks%20%28up%20to%2092%25%29%2C%20accuracy%20drops%20sharply%20to%20around%2049%25%20on%20globally%20optimized%20algorithms%20such%20as%20dynamic%20programming.%20Further%20analysis%20uncovers%20%5Ctextbf%7Bstrategic%20over-shifts%7D%2C%20wherein%20models%20prematurely%20abandon%20correct%20algorithmic%20designs%20due%20to%20necessary%20low-entropy%20tokens.%20These%20findings%20expose%20fundamental%20limitations%20of%20problem-centric%20reinforcement%20learning%20and%20highlight%20the%20necessity%20of%20an%20algorithm-centric%20training%20paradigm%20for%20robust%20algorithmic%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlgBench%253A%2520To%2520What%2520Extent%2520Do%2520Large%2520Reasoning%2520Models%2520Understand%2520Algorithms%253F%26entry.906535625%3DHenan%2520Sun%2520and%2520Kaichi%2520Yu%2520and%2520Yuyao%2520Wang%2520and%2520Bowen%2520Liu%2520and%2520Xunkai%2520Li%2520and%2520Rong-Hua%2520Li%2520and%2520Nuo%2520Chen%2520and%2520Jia%2520Li%26entry.1292438233%3DReasoning%2520ability%2520has%2520become%2520a%2520central%2520focus%2520in%2520the%2520advancement%2520of%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529.%2520Although%2520notable%2520progress%2520has%2520been%2520achieved%2520on%2520several%2520reasoning%2520benchmarks%2520such%2520as%2520MATH500%2520and%2520LiveCodeBench%252C%2520existing%2520benchmarks%2520for%2520algorithmic%2520reasoning%2520remain%2520limited%252C%2520failing%2520to%2520answer%2520a%2520critical%2520question%253A%2520Do%2520LRMs%2520truly%2520master%2520algorithmic%2520reasoning%253F%2520To%2520answer%2520this%2520question%252C%2520we%2520propose%2520AlgBench%252C%2520an%2520expert-curated%2520benchmark%2520that%2520evaluates%2520LRMs%2520under%2520an%2520algorithm-centric%2520paradigm.%250A%2520%2520AlgBench%2520consists%2520of%2520over%25203%252C000%2520original%2520problems%2520spanning%252027%2520algorithms%252C%2520constructed%2520by%2520ACM%2520algorithmic%2520experts%2520and%2520organized%2520under%2520a%2520comprehensive%2520taxonomy%252C%2520including%2520Euclidean-structured%252C%2520non-Euclidean-structured%252C%2520non-optimized%252C%2520local-optimized%252C%2520global-optimized%252C%2520and%2520heuristic-optimized%2520categories.%2520Empirical%2520evaluations%2520on%2520leading%2520LRMs%2520%2528e.g.%252C%2520Gemini-3-Pro%252C%2520DeepSeek-v3.2-Speciale%2520and%2520GPT-o3%2529%2520reveal%2520substantial%2520performance%2520heterogeneity%253A%2520while%2520models%2520perform%2520well%2520on%2520non-optimized%2520tasks%2520%2528up%2520to%252092%2525%2529%252C%2520accuracy%2520drops%2520sharply%2520to%2520around%252049%2525%2520on%2520globally%2520optimized%2520algorithms%2520such%2520as%2520dynamic%2520programming.%2520Further%2520analysis%2520uncovers%2520%255Ctextbf%257Bstrategic%2520over-shifts%257D%252C%2520wherein%2520models%2520prematurely%2520abandon%2520correct%2520algorithmic%2520designs%2520due%2520to%2520necessary%2520low-entropy%2520tokens.%2520These%2520findings%2520expose%2520fundamental%2520limitations%2520of%2520problem-centric%2520reinforcement%2520learning%2520and%2520highlight%2520the%2520necessity%2520of%2520an%2520algorithm-centric%2520training%2520paradigm%2520for%2520robust%2520algorithmic%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlgBench%3A%20To%20What%20Extent%20Do%20Large%20Reasoning%20Models%20Understand%20Algorithms%3F&entry.906535625=Henan%20Sun%20and%20Kaichi%20Yu%20and%20Yuyao%20Wang%20and%20Bowen%20Liu%20and%20Xunkai%20Li%20and%20Rong-Hua%20Li%20and%20Nuo%20Chen%20and%20Jia%20Li&entry.1292438233=Reasoning%20ability%20has%20become%20a%20central%20focus%20in%20the%20advancement%20of%20Large%20Reasoning%20Models%20%28LRMs%29.%20Although%20notable%20progress%20has%20been%20achieved%20on%20several%20reasoning%20benchmarks%20such%20as%20MATH500%20and%20LiveCodeBench%2C%20existing%20benchmarks%20for%20algorithmic%20reasoning%20remain%20limited%2C%20failing%20to%20answer%20a%20critical%20question%3A%20Do%20LRMs%20truly%20master%20algorithmic%20reasoning%3F%20To%20answer%20this%20question%2C%20we%20propose%20AlgBench%2C%20an%20expert-curated%20benchmark%20that%20evaluates%20LRMs%20under%20an%20algorithm-centric%20paradigm.%0A%20%20AlgBench%20consists%20of%20over%203%2C000%20original%20problems%20spanning%2027%20algorithms%2C%20constructed%20by%20ACM%20algorithmic%20experts%20and%20organized%20under%20a%20comprehensive%20taxonomy%2C%20including%20Euclidean-structured%2C%20non-Euclidean-structured%2C%20non-optimized%2C%20local-optimized%2C%20global-optimized%2C%20and%20heuristic-optimized%20categories.%20Empirical%20evaluations%20on%20leading%20LRMs%20%28e.g.%2C%20Gemini-3-Pro%2C%20DeepSeek-v3.2-Speciale%20and%20GPT-o3%29%20reveal%20substantial%20performance%20heterogeneity%3A%20while%20models%20perform%20well%20on%20non-optimized%20tasks%20%28up%20to%2092%25%29%2C%20accuracy%20drops%20sharply%20to%20around%2049%25%20on%20globally%20optimized%20algorithms%20such%20as%20dynamic%20programming.%20Further%20analysis%20uncovers%20%5Ctextbf%7Bstrategic%20over-shifts%7D%2C%20wherein%20models%20prematurely%20abandon%20correct%20algorithmic%20designs%20due%20to%20necessary%20low-entropy%20tokens.%20These%20findings%20expose%20fundamental%20limitations%20of%20problem-centric%20reinforcement%20learning%20and%20highlight%20the%20necessity%20of%20an%20algorithm-centric%20training%20paradigm%20for%20robust%20algorithmic%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2601.04996v1&entry.124074799=Read"},
{"title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning", "author": "Caijun Xu and Changyi Xiao and Zhongyuan Peng and Xinrun Wang and Yixin Cao", "abstract": "Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.", "link": "http://arxiv.org/abs/2601.04809v1", "date": "2026-01-08", "relevancy": 2.0235, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5322}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5012}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCALER%3ASynthetic%20Scalable%20Adaptive%20Learning%20Environment%20for%20Reasoning&body=Title%3A%20SCALER%3ASynthetic%20Scalable%20Adaptive%20Learning%20Environment%20for%20Reasoning%0AAuthor%3A%20Caijun%20Xu%20and%20Changyi%20Xiao%20and%20Zhongyuan%20Peng%20and%20Xinrun%20Wang%20and%20Yixin%20Cao%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29%20offers%20a%20principled%20way%20to%20enhance%20the%20reasoning%20capabilities%20of%20large%20language%20models%2C%20yet%20its%20effectiveness%20hinges%20on%20training%20signals%20that%20remain%20informative%20as%20models%20evolve.%20In%20practice%2C%20RL%20progress%20often%20slows%20when%20task%20difficulty%20becomes%20poorly%20aligned%20with%20model%20capability%2C%20or%20when%20training%20is%20dominated%20by%20a%20narrow%20set%20of%20recurring%20problem%20patterns.%20To%20jointly%20address%20these%20issues%2C%20we%20propose%20SCALER%20%28Synthetic%20sCalable%20Adaptive%20Learning%20Environment%20for%20Reasoning%29%2C%20a%20framework%20that%20sustains%20effective%20learning%20signals%20through%20adaptive%20environment%20design.%20SCALER%20introduces%20a%20scalable%20synthesis%20pipeline%20that%20converts%20real-world%20programming%20problems%20into%20verifiable%20reasoning%20environments%20with%20controllable%20difficulty%20and%20unbounded%20instance%20generation%2C%20enabling%20RL%20training%20beyond%20finite%20datasets%20while%20preserving%20strong%20correctness%20guarantees.%20Building%20on%20this%2C%20SCALER%20further%20employs%20an%20adaptive%20multi-environment%20RL%20strategy%20that%20dynamically%20adjusts%20instance%20difficulty%20and%20curates%20the%20active%20set%20of%20environments%20to%20track%20the%20model%27s%20capability%20frontier%20and%20maintain%20distributional%20diversity.%20This%20co-adaptation%20prevents%20reward%20sparsity%2C%20mitigates%20overfitting%20to%20narrow%20task%20patterns%2C%20and%20supports%20sustained%20improvement%20throughout%20training.%20Extensive%20experiments%20show%20that%20SCALER%20consistently%20outperforms%20dataset-based%20RL%20baselines%20across%20diverse%20reasoning%20benchmarks%20and%20exhibits%20more%20stable%2C%20long-horizon%20training%20dynamics.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCALER%253ASynthetic%2520Scalable%2520Adaptive%2520Learning%2520Environment%2520for%2520Reasoning%26entry.906535625%3DCaijun%2520Xu%2520and%2520Changyi%2520Xiao%2520and%2520Zhongyuan%2520Peng%2520and%2520Xinrun%2520Wang%2520and%2520Yixin%2520Cao%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529%2520offers%2520a%2520principled%2520way%2520to%2520enhance%2520the%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%252C%2520yet%2520its%2520effectiveness%2520hinges%2520on%2520training%2520signals%2520that%2520remain%2520informative%2520as%2520models%2520evolve.%2520In%2520practice%252C%2520RL%2520progress%2520often%2520slows%2520when%2520task%2520difficulty%2520becomes%2520poorly%2520aligned%2520with%2520model%2520capability%252C%2520or%2520when%2520training%2520is%2520dominated%2520by%2520a%2520narrow%2520set%2520of%2520recurring%2520problem%2520patterns.%2520To%2520jointly%2520address%2520these%2520issues%252C%2520we%2520propose%2520SCALER%2520%2528Synthetic%2520sCalable%2520Adaptive%2520Learning%2520Environment%2520for%2520Reasoning%2529%252C%2520a%2520framework%2520that%2520sustains%2520effective%2520learning%2520signals%2520through%2520adaptive%2520environment%2520design.%2520SCALER%2520introduces%2520a%2520scalable%2520synthesis%2520pipeline%2520that%2520converts%2520real-world%2520programming%2520problems%2520into%2520verifiable%2520reasoning%2520environments%2520with%2520controllable%2520difficulty%2520and%2520unbounded%2520instance%2520generation%252C%2520enabling%2520RL%2520training%2520beyond%2520finite%2520datasets%2520while%2520preserving%2520strong%2520correctness%2520guarantees.%2520Building%2520on%2520this%252C%2520SCALER%2520further%2520employs%2520an%2520adaptive%2520multi-environment%2520RL%2520strategy%2520that%2520dynamically%2520adjusts%2520instance%2520difficulty%2520and%2520curates%2520the%2520active%2520set%2520of%2520environments%2520to%2520track%2520the%2520model%2527s%2520capability%2520frontier%2520and%2520maintain%2520distributional%2520diversity.%2520This%2520co-adaptation%2520prevents%2520reward%2520sparsity%252C%2520mitigates%2520overfitting%2520to%2520narrow%2520task%2520patterns%252C%2520and%2520supports%2520sustained%2520improvement%2520throughout%2520training.%2520Extensive%2520experiments%2520show%2520that%2520SCALER%2520consistently%2520outperforms%2520dataset-based%2520RL%2520baselines%2520across%2520diverse%2520reasoning%2520benchmarks%2520and%2520exhibits%2520more%2520stable%252C%2520long-horizon%2520training%2520dynamics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCALER%3ASynthetic%20Scalable%20Adaptive%20Learning%20Environment%20for%20Reasoning&entry.906535625=Caijun%20Xu%20and%20Changyi%20Xiao%20and%20Zhongyuan%20Peng%20and%20Xinrun%20Wang%20and%20Yixin%20Cao&entry.1292438233=Reinforcement%20learning%20%28RL%29%20offers%20a%20principled%20way%20to%20enhance%20the%20reasoning%20capabilities%20of%20large%20language%20models%2C%20yet%20its%20effectiveness%20hinges%20on%20training%20signals%20that%20remain%20informative%20as%20models%20evolve.%20In%20practice%2C%20RL%20progress%20often%20slows%20when%20task%20difficulty%20becomes%20poorly%20aligned%20with%20model%20capability%2C%20or%20when%20training%20is%20dominated%20by%20a%20narrow%20set%20of%20recurring%20problem%20patterns.%20To%20jointly%20address%20these%20issues%2C%20we%20propose%20SCALER%20%28Synthetic%20sCalable%20Adaptive%20Learning%20Environment%20for%20Reasoning%29%2C%20a%20framework%20that%20sustains%20effective%20learning%20signals%20through%20adaptive%20environment%20design.%20SCALER%20introduces%20a%20scalable%20synthesis%20pipeline%20that%20converts%20real-world%20programming%20problems%20into%20verifiable%20reasoning%20environments%20with%20controllable%20difficulty%20and%20unbounded%20instance%20generation%2C%20enabling%20RL%20training%20beyond%20finite%20datasets%20while%20preserving%20strong%20correctness%20guarantees.%20Building%20on%20this%2C%20SCALER%20further%20employs%20an%20adaptive%20multi-environment%20RL%20strategy%20that%20dynamically%20adjusts%20instance%20difficulty%20and%20curates%20the%20active%20set%20of%20environments%20to%20track%20the%20model%27s%20capability%20frontier%20and%20maintain%20distributional%20diversity.%20This%20co-adaptation%20prevents%20reward%20sparsity%2C%20mitigates%20overfitting%20to%20narrow%20task%20patterns%2C%20and%20supports%20sustained%20improvement%20throughout%20training.%20Extensive%20experiments%20show%20that%20SCALER%20consistently%20outperforms%20dataset-based%20RL%20baselines%20across%20diverse%20reasoning%20benchmarks%20and%20exhibits%20more%20stable%2C%20long-horizon%20training%20dynamics.&entry.1838667208=http%3A//arxiv.org/abs/2601.04809v1&entry.124074799=Read"},
{"title": "GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation", "author": "Jinze Yu and Dayuan Jiang", "abstract": "Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications.", "link": "http://arxiv.org/abs/2601.05162v1", "date": "2026-01-08", "relevancy": 2.0218, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5301}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4992}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenAI-DrawIO-Creator%3A%20A%20Framework%20for%20Automated%20Diagram%20Generation&body=Title%3A%20GenAI-DrawIO-Creator%3A%20A%20Framework%20for%20Automated%20Diagram%20Generation%0AAuthor%3A%20Jinze%20Yu%20and%20Dayuan%20Jiang%0AAbstract%3A%20Diagrams%20are%20crucial%20for%20communicating%20complex%20information%2C%20yet%20creating%20and%20modifying%20them%20remains%20a%20labor-intensive%20task.%20We%20present%20GenAI-DrawIO-Creator%2C%20a%20novel%20framework%20that%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20automate%20diagram%20generation%20and%20manipulation%20in%20the%20structured%20XML%20format%20used%20by%20draw.io.%20Our%20system%20integrates%20Claude%203.7%20to%20reason%20about%20structured%20visual%20data%20and%20produce%20valid%20diagram%20representations.%20Key%20contributions%20include%20a%20high-level%20system%20design%20enabling%20real-time%20diagram%20updates%2C%20specialized%20prompt%20engineering%20and%20error-checking%20to%20ensure%20well-formed%20XML%20outputs.%20We%20demonstrate%20a%20working%20prototype%20capable%20of%20generating%20accurate%20diagrams%20%28such%20as%20network%20architectures%20and%20flowcharts%29%20from%20natural%20language%20or%20code%2C%20and%20even%20replicating%20diagrams%20from%20images.%20Simulated%20evaluations%20show%20that%20our%20approach%20significantly%20reduces%20diagram%20creation%20time%20and%20produces%20outputs%20with%20high%20structural%20fidelity.%20Our%20results%20highlight%20the%20promise%20of%20Claude%203.7%20in%20handling%20structured%20visual%20reasoning%20tasks%20and%20lay%20the%20groundwork%20for%20future%20research%20in%20AI-assisted%20diagramming%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenAI-DrawIO-Creator%253A%2520A%2520Framework%2520for%2520Automated%2520Diagram%2520Generation%26entry.906535625%3DJinze%2520Yu%2520and%2520Dayuan%2520Jiang%26entry.1292438233%3DDiagrams%2520are%2520crucial%2520for%2520communicating%2520complex%2520information%252C%2520yet%2520creating%2520and%2520modifying%2520them%2520remains%2520a%2520labor-intensive%2520task.%2520We%2520present%2520GenAI-DrawIO-Creator%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520automate%2520diagram%2520generation%2520and%2520manipulation%2520in%2520the%2520structured%2520XML%2520format%2520used%2520by%2520draw.io.%2520Our%2520system%2520integrates%2520Claude%25203.7%2520to%2520reason%2520about%2520structured%2520visual%2520data%2520and%2520produce%2520valid%2520diagram%2520representations.%2520Key%2520contributions%2520include%2520a%2520high-level%2520system%2520design%2520enabling%2520real-time%2520diagram%2520updates%252C%2520specialized%2520prompt%2520engineering%2520and%2520error-checking%2520to%2520ensure%2520well-formed%2520XML%2520outputs.%2520We%2520demonstrate%2520a%2520working%2520prototype%2520capable%2520of%2520generating%2520accurate%2520diagrams%2520%2528such%2520as%2520network%2520architectures%2520and%2520flowcharts%2529%2520from%2520natural%2520language%2520or%2520code%252C%2520and%2520even%2520replicating%2520diagrams%2520from%2520images.%2520Simulated%2520evaluations%2520show%2520that%2520our%2520approach%2520significantly%2520reduces%2520diagram%2520creation%2520time%2520and%2520produces%2520outputs%2520with%2520high%2520structural%2520fidelity.%2520Our%2520results%2520highlight%2520the%2520promise%2520of%2520Claude%25203.7%2520in%2520handling%2520structured%2520visual%2520reasoning%2520tasks%2520and%2520lay%2520the%2520groundwork%2520for%2520future%2520research%2520in%2520AI-assisted%2520diagramming%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenAI-DrawIO-Creator%3A%20A%20Framework%20for%20Automated%20Diagram%20Generation&entry.906535625=Jinze%20Yu%20and%20Dayuan%20Jiang&entry.1292438233=Diagrams%20are%20crucial%20for%20communicating%20complex%20information%2C%20yet%20creating%20and%20modifying%20them%20remains%20a%20labor-intensive%20task.%20We%20present%20GenAI-DrawIO-Creator%2C%20a%20novel%20framework%20that%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20automate%20diagram%20generation%20and%20manipulation%20in%20the%20structured%20XML%20format%20used%20by%20draw.io.%20Our%20system%20integrates%20Claude%203.7%20to%20reason%20about%20structured%20visual%20data%20and%20produce%20valid%20diagram%20representations.%20Key%20contributions%20include%20a%20high-level%20system%20design%20enabling%20real-time%20diagram%20updates%2C%20specialized%20prompt%20engineering%20and%20error-checking%20to%20ensure%20well-formed%20XML%20outputs.%20We%20demonstrate%20a%20working%20prototype%20capable%20of%20generating%20accurate%20diagrams%20%28such%20as%20network%20architectures%20and%20flowcharts%29%20from%20natural%20language%20or%20code%2C%20and%20even%20replicating%20diagrams%20from%20images.%20Simulated%20evaluations%20show%20that%20our%20approach%20significantly%20reduces%20diagram%20creation%20time%20and%20produces%20outputs%20with%20high%20structural%20fidelity.%20Our%20results%20highlight%20the%20promise%20of%20Claude%203.7%20in%20handling%20structured%20visual%20reasoning%20tasks%20and%20lay%20the%20groundwork%20for%20future%20research%20in%20AI-assisted%20diagramming%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2601.05162v1&entry.124074799=Read"},
{"title": "ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning", "author": "Minda Hu and Zexuan Qiu and Zenan Xu and Kun Li and Bo Zhou and Irwin King", "abstract": "Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking'', where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the 'cold start' phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.", "link": "http://arxiv.org/abs/2601.04973v1", "date": "2026-01-08", "relevancy": 2.0211, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConMax%3A%20Confidence-Maximizing%20Compression%20for%20Efficient%20Chain-of-Thought%20Reasoning&body=Title%3A%20ConMax%3A%20Confidence-Maximizing%20Compression%20for%20Efficient%20Chain-of-Thought%20Reasoning%0AAuthor%3A%20Minda%20Hu%20and%20Zexuan%20Qiu%20and%20Zenan%20Xu%20and%20Kun%20Li%20and%20Bo%20Zhou%20and%20Irwin%20King%0AAbstract%3A%20Recent%20breakthroughs%20in%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20demonstrated%20that%20extensive%20Chain-of-Thought%20%28CoT%29%20generation%20is%20critical%20for%20enabling%20intricate%20cognitive%20behaviors%2C%20such%20as%20self-verification%20and%20backtracking%2C%20to%20solve%20complex%20tasks.%20However%2C%20this%20capability%20often%20leads%20to%20%60%60overthinking%27%27%2C%20where%20models%20generate%20redundant%20reasoning%20paths%20that%20inflate%20computational%20costs%20without%20improving%20accuracy.%20While%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20reasoning%20traces%20is%20a%20standard%20paradigm%20for%20the%20%27cold%20start%27%20phase%2C%20applying%20existing%20compression%20techniques%20to%20these%20traces%20often%20compromises%20logical%20coherence%20or%20incurs%20prohibitive%20sampling%20costs.%20In%20this%20paper%2C%20we%20introduce%20ConMax%20%28Confidence-Maximizing%20Compression%29%2C%20a%20novel%20reinforcement%20learning%20framework%20designed%20to%20automatically%20compress%20reasoning%20traces%20while%20preserving%20essential%20reasoning%20patterns.%20ConMax%20formulates%20compression%20as%20a%20reward-driven%20optimization%20problem%2C%20training%20a%20policy%20to%20prune%20redundancy%20by%20maximizing%20a%20weighted%20combination%20of%20answer%20confidence%20for%20predictive%20fidelity%20and%20thinking%20confidence%20for%20reasoning%20validity%20through%20a%20frozen%20auxiliary%20LRM.%20Extensive%20experiments%20across%20five%20reasoning%20datasets%20demonstrate%20that%20ConMax%20achieves%20a%20superior%20efficiency-performance%20trade-off.%20Specifically%2C%20it%20reduces%20inference%20length%20by%2043%25%20over%20strong%20baselines%20at%20the%20cost%20of%20a%20mere%200.7%25%20dip%20in%20accuracy%2C%20proving%20its%20effectiveness%20in%20generating%20high-quality%2C%20efficient%20training%20data%20for%20LRMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConMax%253A%2520Confidence-Maximizing%2520Compression%2520for%2520Efficient%2520Chain-of-Thought%2520Reasoning%26entry.906535625%3DMinda%2520Hu%2520and%2520Zexuan%2520Qiu%2520and%2520Zenan%2520Xu%2520and%2520Kun%2520Li%2520and%2520Bo%2520Zhou%2520and%2520Irwin%2520King%26entry.1292438233%3DRecent%2520breakthroughs%2520in%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520have%2520demonstrated%2520that%2520extensive%2520Chain-of-Thought%2520%2528CoT%2529%2520generation%2520is%2520critical%2520for%2520enabling%2520intricate%2520cognitive%2520behaviors%252C%2520such%2520as%2520self-verification%2520and%2520backtracking%252C%2520to%2520solve%2520complex%2520tasks.%2520However%252C%2520this%2520capability%2520often%2520leads%2520to%2520%2560%2560overthinking%2527%2527%252C%2520where%2520models%2520generate%2520redundant%2520reasoning%2520paths%2520that%2520inflate%2520computational%2520costs%2520without%2520improving%2520accuracy.%2520While%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520on%2520reasoning%2520traces%2520is%2520a%2520standard%2520paradigm%2520for%2520the%2520%2527cold%2520start%2527%2520phase%252C%2520applying%2520existing%2520compression%2520techniques%2520to%2520these%2520traces%2520often%2520compromises%2520logical%2520coherence%2520or%2520incurs%2520prohibitive%2520sampling%2520costs.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520ConMax%2520%2528Confidence-Maximizing%2520Compression%2529%252C%2520a%2520novel%2520reinforcement%2520learning%2520framework%2520designed%2520to%2520automatically%2520compress%2520reasoning%2520traces%2520while%2520preserving%2520essential%2520reasoning%2520patterns.%2520ConMax%2520formulates%2520compression%2520as%2520a%2520reward-driven%2520optimization%2520problem%252C%2520training%2520a%2520policy%2520to%2520prune%2520redundancy%2520by%2520maximizing%2520a%2520weighted%2520combination%2520of%2520answer%2520confidence%2520for%2520predictive%2520fidelity%2520and%2520thinking%2520confidence%2520for%2520reasoning%2520validity%2520through%2520a%2520frozen%2520auxiliary%2520LRM.%2520Extensive%2520experiments%2520across%2520five%2520reasoning%2520datasets%2520demonstrate%2520that%2520ConMax%2520achieves%2520a%2520superior%2520efficiency-performance%2520trade-off.%2520Specifically%252C%2520it%2520reduces%2520inference%2520length%2520by%252043%2525%2520over%2520strong%2520baselines%2520at%2520the%2520cost%2520of%2520a%2520mere%25200.7%2525%2520dip%2520in%2520accuracy%252C%2520proving%2520its%2520effectiveness%2520in%2520generating%2520high-quality%252C%2520efficient%2520training%2520data%2520for%2520LRMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConMax%3A%20Confidence-Maximizing%20Compression%20for%20Efficient%20Chain-of-Thought%20Reasoning&entry.906535625=Minda%20Hu%20and%20Zexuan%20Qiu%20and%20Zenan%20Xu%20and%20Kun%20Li%20and%20Bo%20Zhou%20and%20Irwin%20King&entry.1292438233=Recent%20breakthroughs%20in%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20demonstrated%20that%20extensive%20Chain-of-Thought%20%28CoT%29%20generation%20is%20critical%20for%20enabling%20intricate%20cognitive%20behaviors%2C%20such%20as%20self-verification%20and%20backtracking%2C%20to%20solve%20complex%20tasks.%20However%2C%20this%20capability%20often%20leads%20to%20%60%60overthinking%27%27%2C%20where%20models%20generate%20redundant%20reasoning%20paths%20that%20inflate%20computational%20costs%20without%20improving%20accuracy.%20While%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20reasoning%20traces%20is%20a%20standard%20paradigm%20for%20the%20%27cold%20start%27%20phase%2C%20applying%20existing%20compression%20techniques%20to%20these%20traces%20often%20compromises%20logical%20coherence%20or%20incurs%20prohibitive%20sampling%20costs.%20In%20this%20paper%2C%20we%20introduce%20ConMax%20%28Confidence-Maximizing%20Compression%29%2C%20a%20novel%20reinforcement%20learning%20framework%20designed%20to%20automatically%20compress%20reasoning%20traces%20while%20preserving%20essential%20reasoning%20patterns.%20ConMax%20formulates%20compression%20as%20a%20reward-driven%20optimization%20problem%2C%20training%20a%20policy%20to%20prune%20redundancy%20by%20maximizing%20a%20weighted%20combination%20of%20answer%20confidence%20for%20predictive%20fidelity%20and%20thinking%20confidence%20for%20reasoning%20validity%20through%20a%20frozen%20auxiliary%20LRM.%20Extensive%20experiments%20across%20five%20reasoning%20datasets%20demonstrate%20that%20ConMax%20achieves%20a%20superior%20efficiency-performance%20trade-off.%20Specifically%2C%20it%20reduces%20inference%20length%20by%2043%25%20over%20strong%20baselines%20at%20the%20cost%20of%20a%20mere%200.7%25%20dip%20in%20accuracy%2C%20proving%20its%20effectiveness%20in%20generating%20high-quality%2C%20efficient%20training%20data%20for%20LRMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.04973v1&entry.124074799=Read"},
{"title": "Automated Invoice Data Extraction: Using LLM and OCR", "author": "Khushi Khanchandani and Advait Thakur and Akshita Shetty and Chaitravi Reddy and Ritisa Behera", "abstract": "Conventional Optical Character Recognition (OCR) systems are challenged by variant invoice layouts, handwritten text, and low-quality scans, which are often caused by strong template dependencies that restrict their flexibility across different document structures and layouts. Newer solutions utilize advanced deep learning models such as Convolutional Neural Networks (CNN) as well as Transformers, and domain-specific models for better layout analysis and accuracy across various sections over varied document types. Large Language Models (LLMs) have revolutionized extraction pipelines at their core with sophisticated entity recognition and semantic comprehension to support complex contextual relationship mapping without direct programming specification. Visual Named Entity Recognition (NER) capabilities permit extraction from invoice images with greater contextual sensitivity and much higher accuracy rates than older approaches. Existing industry best practices utilize hybrid architectures that blend OCR technology and LLM for maximum scalability and minimal human intervention. This work introduces a holistic Artificial Intelligence (AI) platform combining OCR, deep learning, LLMs, and graph analytics to achieve unprecedented extraction quality and consistency.", "link": "http://arxiv.org/abs/2511.05547v2", "date": "2026-01-08", "relevancy": 2.0162, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5256}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Invoice%20Data%20Extraction%3A%20Using%20LLM%20and%20OCR&body=Title%3A%20Automated%20Invoice%20Data%20Extraction%3A%20Using%20LLM%20and%20OCR%0AAuthor%3A%20Khushi%20Khanchandani%20and%20Advait%20Thakur%20and%20Akshita%20Shetty%20and%20Chaitravi%20Reddy%20and%20Ritisa%20Behera%0AAbstract%3A%20Conventional%20Optical%20Character%20Recognition%20%28OCR%29%20systems%20are%20challenged%20by%20variant%20invoice%20layouts%2C%20handwritten%20text%2C%20and%20low-quality%20scans%2C%20which%20are%20often%20caused%20by%20strong%20template%20dependencies%20that%20restrict%20their%20flexibility%20across%20different%20document%20structures%20and%20layouts.%20Newer%20solutions%20utilize%20advanced%20deep%20learning%20models%20such%20as%20Convolutional%20Neural%20Networks%20%28CNN%29%20as%20well%20as%20Transformers%2C%20and%20domain-specific%20models%20for%20better%20layout%20analysis%20and%20accuracy%20across%20various%20sections%20over%20varied%20document%20types.%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20extraction%20pipelines%20at%20their%20core%20with%20sophisticated%20entity%20recognition%20and%20semantic%20comprehension%20to%20support%20complex%20contextual%20relationship%20mapping%20without%20direct%20programming%20specification.%20Visual%20Named%20Entity%20Recognition%20%28NER%29%20capabilities%20permit%20extraction%20from%20invoice%20images%20with%20greater%20contextual%20sensitivity%20and%20much%20higher%20accuracy%20rates%20than%20older%20approaches.%20Existing%20industry%20best%20practices%20utilize%20hybrid%20architectures%20that%20blend%20OCR%20technology%20and%20LLM%20for%20maximum%20scalability%20and%20minimal%20human%20intervention.%20This%20work%20introduces%20a%20holistic%20Artificial%20Intelligence%20%28AI%29%20platform%20combining%20OCR%2C%20deep%20learning%2C%20LLMs%2C%20and%20graph%20analytics%20to%20achieve%20unprecedented%20extraction%20quality%20and%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2511.05547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Invoice%2520Data%2520Extraction%253A%2520Using%2520LLM%2520and%2520OCR%26entry.906535625%3DKhushi%2520Khanchandani%2520and%2520Advait%2520Thakur%2520and%2520Akshita%2520Shetty%2520and%2520Chaitravi%2520Reddy%2520and%2520Ritisa%2520Behera%26entry.1292438233%3DConventional%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520systems%2520are%2520challenged%2520by%2520variant%2520invoice%2520layouts%252C%2520handwritten%2520text%252C%2520and%2520low-quality%2520scans%252C%2520which%2520are%2520often%2520caused%2520by%2520strong%2520template%2520dependencies%2520that%2520restrict%2520their%2520flexibility%2520across%2520different%2520document%2520structures%2520and%2520layouts.%2520Newer%2520solutions%2520utilize%2520advanced%2520deep%2520learning%2520models%2520such%2520as%2520Convolutional%2520Neural%2520Networks%2520%2528CNN%2529%2520as%2520well%2520as%2520Transformers%252C%2520and%2520domain-specific%2520models%2520for%2520better%2520layout%2520analysis%2520and%2520accuracy%2520across%2520various%2520sections%2520over%2520varied%2520document%2520types.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520extraction%2520pipelines%2520at%2520their%2520core%2520with%2520sophisticated%2520entity%2520recognition%2520and%2520semantic%2520comprehension%2520to%2520support%2520complex%2520contextual%2520relationship%2520mapping%2520without%2520direct%2520programming%2520specification.%2520Visual%2520Named%2520Entity%2520Recognition%2520%2528NER%2529%2520capabilities%2520permit%2520extraction%2520from%2520invoice%2520images%2520with%2520greater%2520contextual%2520sensitivity%2520and%2520much%2520higher%2520accuracy%2520rates%2520than%2520older%2520approaches.%2520Existing%2520industry%2520best%2520practices%2520utilize%2520hybrid%2520architectures%2520that%2520blend%2520OCR%2520technology%2520and%2520LLM%2520for%2520maximum%2520scalability%2520and%2520minimal%2520human%2520intervention.%2520This%2520work%2520introduces%2520a%2520holistic%2520Artificial%2520Intelligence%2520%2528AI%2529%2520platform%2520combining%2520OCR%252C%2520deep%2520learning%252C%2520LLMs%252C%2520and%2520graph%2520analytics%2520to%2520achieve%2520unprecedented%2520extraction%2520quality%2520and%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Invoice%20Data%20Extraction%3A%20Using%20LLM%20and%20OCR&entry.906535625=Khushi%20Khanchandani%20and%20Advait%20Thakur%20and%20Akshita%20Shetty%20and%20Chaitravi%20Reddy%20and%20Ritisa%20Behera&entry.1292438233=Conventional%20Optical%20Character%20Recognition%20%28OCR%29%20systems%20are%20challenged%20by%20variant%20invoice%20layouts%2C%20handwritten%20text%2C%20and%20low-quality%20scans%2C%20which%20are%20often%20caused%20by%20strong%20template%20dependencies%20that%20restrict%20their%20flexibility%20across%20different%20document%20structures%20and%20layouts.%20Newer%20solutions%20utilize%20advanced%20deep%20learning%20models%20such%20as%20Convolutional%20Neural%20Networks%20%28CNN%29%20as%20well%20as%20Transformers%2C%20and%20domain-specific%20models%20for%20better%20layout%20analysis%20and%20accuracy%20across%20various%20sections%20over%20varied%20document%20types.%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20extraction%20pipelines%20at%20their%20core%20with%20sophisticated%20entity%20recognition%20and%20semantic%20comprehension%20to%20support%20complex%20contextual%20relationship%20mapping%20without%20direct%20programming%20specification.%20Visual%20Named%20Entity%20Recognition%20%28NER%29%20capabilities%20permit%20extraction%20from%20invoice%20images%20with%20greater%20contextual%20sensitivity%20and%20much%20higher%20accuracy%20rates%20than%20older%20approaches.%20Existing%20industry%20best%20practices%20utilize%20hybrid%20architectures%20that%20blend%20OCR%20technology%20and%20LLM%20for%20maximum%20scalability%20and%20minimal%20human%20intervention.%20This%20work%20introduces%20a%20holistic%20Artificial%20Intelligence%20%28AI%29%20platform%20combining%20OCR%2C%20deep%20learning%2C%20LLMs%2C%20and%20graph%20analytics%20to%20achieve%20unprecedented%20extraction%20quality%20and%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2511.05547v2&entry.124074799=Read"},
{"title": "Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration", "author": "Xingyi He and Adhitya Polavaram and Yunhao Cao and Om Deshmukh and Tianrui Wang and Xiaowei Zhou and Kuan Fang", "abstract": "Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.", "link": "http://arxiv.org/abs/2601.05243v1", "date": "2026-01-08", "relevancy": 2.0155, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7326}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6067}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generate%2C%20Transfer%2C%20Adapt%3A%20Learning%20Functional%20Dexterous%20Grasping%20from%20a%20Single%20Human%20Demonstration&body=Title%3A%20Generate%2C%20Transfer%2C%20Adapt%3A%20Learning%20Functional%20Dexterous%20Grasping%20from%20a%20Single%20Human%20Demonstration%0AAuthor%3A%20Xingyi%20He%20and%20Adhitya%20Polavaram%20and%20Yunhao%20Cao%20and%20Om%20Deshmukh%20and%20Tianrui%20Wang%20and%20Xiaowei%20Zhou%20and%20Kuan%20Fang%0AAbstract%3A%20Functional%20grasping%20with%20dexterous%20robotic%20hands%20is%20a%20key%20capability%20for%20enabling%20tool%20use%20and%20complex%20manipulation%2C%20yet%20progress%20has%20been%20constrained%20by%20two%20persistent%20bottlenecks%3A%20the%20scarcity%20of%20large-scale%20datasets%20and%20the%20absence%20of%20integrated%20semantic%20and%20geometric%20reasoning%20in%20learned%20models.%20In%20this%20work%2C%20we%20present%20CorDex%2C%20a%20framework%20that%20robustly%20learns%20dexterous%20functional%20grasps%20of%20novel%20objects%20from%20synthetic%20data%20generated%20from%20just%20a%20single%20human%20demonstration.%20At%20the%20core%20of%20our%20approach%20is%20a%20correspondence-based%20data%20engine%20that%20generates%20diverse%2C%20high-quality%20training%20data%20in%20simulation.%20Based%20on%20the%20human%20demonstration%2C%20our%20data%20engine%20generates%20diverse%20object%20instances%20of%20the%20same%20category%2C%20transfers%20the%20expert%20grasp%20to%20the%20generated%20objects%20through%20correspondence%20estimation%2C%20and%20adapts%20the%20grasp%20through%20optimization.%20Building%20on%20the%20generated%20data%2C%20we%20introduce%20a%20multimodal%20prediction%20network%20that%20integrates%20visual%20and%20geometric%20information.%20By%20devising%20a%20local-global%20fusion%20module%20and%20an%20importance-aware%20sampling%20mechanism%2C%20we%20enable%20robust%20and%20computationally%20efficient%20prediction%20of%20functional%20dexterous%20grasps.%20Through%20extensive%20experiments%20across%20various%20object%20categories%2C%20we%20demonstrate%20that%20CorDex%20generalizes%20well%20to%20unseen%20object%20instances%20and%20significantly%20outperforms%20state-of-the-art%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerate%252C%2520Transfer%252C%2520Adapt%253A%2520Learning%2520Functional%2520Dexterous%2520Grasping%2520from%2520a%2520Single%2520Human%2520Demonstration%26entry.906535625%3DXingyi%2520He%2520and%2520Adhitya%2520Polavaram%2520and%2520Yunhao%2520Cao%2520and%2520Om%2520Deshmukh%2520and%2520Tianrui%2520Wang%2520and%2520Xiaowei%2520Zhou%2520and%2520Kuan%2520Fang%26entry.1292438233%3DFunctional%2520grasping%2520with%2520dexterous%2520robotic%2520hands%2520is%2520a%2520key%2520capability%2520for%2520enabling%2520tool%2520use%2520and%2520complex%2520manipulation%252C%2520yet%2520progress%2520has%2520been%2520constrained%2520by%2520two%2520persistent%2520bottlenecks%253A%2520the%2520scarcity%2520of%2520large-scale%2520datasets%2520and%2520the%2520absence%2520of%2520integrated%2520semantic%2520and%2520geometric%2520reasoning%2520in%2520learned%2520models.%2520In%2520this%2520work%252C%2520we%2520present%2520CorDex%252C%2520a%2520framework%2520that%2520robustly%2520learns%2520dexterous%2520functional%2520grasps%2520of%2520novel%2520objects%2520from%2520synthetic%2520data%2520generated%2520from%2520just%2520a%2520single%2520human%2520demonstration.%2520At%2520the%2520core%2520of%2520our%2520approach%2520is%2520a%2520correspondence-based%2520data%2520engine%2520that%2520generates%2520diverse%252C%2520high-quality%2520training%2520data%2520in%2520simulation.%2520Based%2520on%2520the%2520human%2520demonstration%252C%2520our%2520data%2520engine%2520generates%2520diverse%2520object%2520instances%2520of%2520the%2520same%2520category%252C%2520transfers%2520the%2520expert%2520grasp%2520to%2520the%2520generated%2520objects%2520through%2520correspondence%2520estimation%252C%2520and%2520adapts%2520the%2520grasp%2520through%2520optimization.%2520Building%2520on%2520the%2520generated%2520data%252C%2520we%2520introduce%2520a%2520multimodal%2520prediction%2520network%2520that%2520integrates%2520visual%2520and%2520geometric%2520information.%2520By%2520devising%2520a%2520local-global%2520fusion%2520module%2520and%2520an%2520importance-aware%2520sampling%2520mechanism%252C%2520we%2520enable%2520robust%2520and%2520computationally%2520efficient%2520prediction%2520of%2520functional%2520dexterous%2520grasps.%2520Through%2520extensive%2520experiments%2520across%2520various%2520object%2520categories%252C%2520we%2520demonstrate%2520that%2520CorDex%2520generalizes%2520well%2520to%2520unseen%2520object%2520instances%2520and%2520significantly%2520outperforms%2520state-of-the-art%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generate%2C%20Transfer%2C%20Adapt%3A%20Learning%20Functional%20Dexterous%20Grasping%20from%20a%20Single%20Human%20Demonstration&entry.906535625=Xingyi%20He%20and%20Adhitya%20Polavaram%20and%20Yunhao%20Cao%20and%20Om%20Deshmukh%20and%20Tianrui%20Wang%20and%20Xiaowei%20Zhou%20and%20Kuan%20Fang&entry.1292438233=Functional%20grasping%20with%20dexterous%20robotic%20hands%20is%20a%20key%20capability%20for%20enabling%20tool%20use%20and%20complex%20manipulation%2C%20yet%20progress%20has%20been%20constrained%20by%20two%20persistent%20bottlenecks%3A%20the%20scarcity%20of%20large-scale%20datasets%20and%20the%20absence%20of%20integrated%20semantic%20and%20geometric%20reasoning%20in%20learned%20models.%20In%20this%20work%2C%20we%20present%20CorDex%2C%20a%20framework%20that%20robustly%20learns%20dexterous%20functional%20grasps%20of%20novel%20objects%20from%20synthetic%20data%20generated%20from%20just%20a%20single%20human%20demonstration.%20At%20the%20core%20of%20our%20approach%20is%20a%20correspondence-based%20data%20engine%20that%20generates%20diverse%2C%20high-quality%20training%20data%20in%20simulation.%20Based%20on%20the%20human%20demonstration%2C%20our%20data%20engine%20generates%20diverse%20object%20instances%20of%20the%20same%20category%2C%20transfers%20the%20expert%20grasp%20to%20the%20generated%20objects%20through%20correspondence%20estimation%2C%20and%20adapts%20the%20grasp%20through%20optimization.%20Building%20on%20the%20generated%20data%2C%20we%20introduce%20a%20multimodal%20prediction%20network%20that%20integrates%20visual%20and%20geometric%20information.%20By%20devising%20a%20local-global%20fusion%20module%20and%20an%20importance-aware%20sampling%20mechanism%2C%20we%20enable%20robust%20and%20computationally%20efficient%20prediction%20of%20functional%20dexterous%20grasps.%20Through%20extensive%20experiments%20across%20various%20object%20categories%2C%20we%20demonstrate%20that%20CorDex%20generalizes%20well%20to%20unseen%20object%20instances%20and%20significantly%20outperforms%20state-of-the-art%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.05243v1&entry.124074799=Read"},
{"title": "Driver-Intention Prediction with Deep Learning: Real-Time Brain-to-Vehicle Communication", "author": "Niloufar Alavi and Swati Shah and Rezvan Alamian and Stefan Goetz", "abstract": "Brain-computer interfaces (BCIs) allow direct communication between the brain and electronics without the need for speech or physical movement. Such interfaces can be particularly beneficial in applications requiring rapid response times, such as driving, where a vehicle's advanced driving assistance systems could benefit from immediate understanding of a driver's intentions. This study presents a novel method for predicting a driver's intention to steer using electroencephalography (EEG) signals through deep learning. A driving simulator created a controlled environment in which participants imagined controlling a vehicle during various driving scenarios, including left and right turns, as well as straight driving. A convolutional neural network (CNN) classified the detected EEG data with minimal pre-processing. Our model achieved an accuracy of 83.7% in distinguishing between the three steering intentions and demonstrated the ability of CNNs to process raw EEG data effectively. The classification accuracy was highest for right-turn segments, which suggests a potential spatial bias in brain activity. This study lays the foundation for more intuitive brain-to-vehicle communication systems.", "link": "http://arxiv.org/abs/2601.05084v1", "date": "2026-01-08", "relevancy": 2.0093, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.528}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5091}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Driver-Intention%20Prediction%20with%20Deep%20Learning%3A%20Real-Time%20Brain-to-Vehicle%20Communication&body=Title%3A%20Driver-Intention%20Prediction%20with%20Deep%20Learning%3A%20Real-Time%20Brain-to-Vehicle%20Communication%0AAuthor%3A%20Niloufar%20Alavi%20and%20Swati%20Shah%20and%20Rezvan%20Alamian%20and%20Stefan%20Goetz%0AAbstract%3A%20Brain-computer%20interfaces%20%28BCIs%29%20allow%20direct%20communication%20between%20the%20brain%20and%20electronics%20without%20the%20need%20for%20speech%20or%20physical%20movement.%20Such%20interfaces%20can%20be%20particularly%20beneficial%20in%20applications%20requiring%20rapid%20response%20times%2C%20such%20as%20driving%2C%20where%20a%20vehicle%27s%20advanced%20driving%20assistance%20systems%20could%20benefit%20from%20immediate%20understanding%20of%20a%20driver%27s%20intentions.%20This%20study%20presents%20a%20novel%20method%20for%20predicting%20a%20driver%27s%20intention%20to%20steer%20using%20electroencephalography%20%28EEG%29%20signals%20through%20deep%20learning.%20A%20driving%20simulator%20created%20a%20controlled%20environment%20in%20which%20participants%20imagined%20controlling%20a%20vehicle%20during%20various%20driving%20scenarios%2C%20including%20left%20and%20right%20turns%2C%20as%20well%20as%20straight%20driving.%20A%20convolutional%20neural%20network%20%28CNN%29%20classified%20the%20detected%20EEG%20data%20with%20minimal%20pre-processing.%20Our%20model%20achieved%20an%20accuracy%20of%2083.7%25%20in%20distinguishing%20between%20the%20three%20steering%20intentions%20and%20demonstrated%20the%20ability%20of%20CNNs%20to%20process%20raw%20EEG%20data%20effectively.%20The%20classification%20accuracy%20was%20highest%20for%20right-turn%20segments%2C%20which%20suggests%20a%20potential%20spatial%20bias%20in%20brain%20activity.%20This%20study%20lays%20the%20foundation%20for%20more%20intuitive%20brain-to-vehicle%20communication%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriver-Intention%2520Prediction%2520with%2520Deep%2520Learning%253A%2520Real-Time%2520Brain-to-Vehicle%2520Communication%26entry.906535625%3DNiloufar%2520Alavi%2520and%2520Swati%2520Shah%2520and%2520Rezvan%2520Alamian%2520and%2520Stefan%2520Goetz%26entry.1292438233%3DBrain-computer%2520interfaces%2520%2528BCIs%2529%2520allow%2520direct%2520communication%2520between%2520the%2520brain%2520and%2520electronics%2520without%2520the%2520need%2520for%2520speech%2520or%2520physical%2520movement.%2520Such%2520interfaces%2520can%2520be%2520particularly%2520beneficial%2520in%2520applications%2520requiring%2520rapid%2520response%2520times%252C%2520such%2520as%2520driving%252C%2520where%2520a%2520vehicle%2527s%2520advanced%2520driving%2520assistance%2520systems%2520could%2520benefit%2520from%2520immediate%2520understanding%2520of%2520a%2520driver%2527s%2520intentions.%2520This%2520study%2520presents%2520a%2520novel%2520method%2520for%2520predicting%2520a%2520driver%2527s%2520intention%2520to%2520steer%2520using%2520electroencephalography%2520%2528EEG%2529%2520signals%2520through%2520deep%2520learning.%2520A%2520driving%2520simulator%2520created%2520a%2520controlled%2520environment%2520in%2520which%2520participants%2520imagined%2520controlling%2520a%2520vehicle%2520during%2520various%2520driving%2520scenarios%252C%2520including%2520left%2520and%2520right%2520turns%252C%2520as%2520well%2520as%2520straight%2520driving.%2520A%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520classified%2520the%2520detected%2520EEG%2520data%2520with%2520minimal%2520pre-processing.%2520Our%2520model%2520achieved%2520an%2520accuracy%2520of%252083.7%2525%2520in%2520distinguishing%2520between%2520the%2520three%2520steering%2520intentions%2520and%2520demonstrated%2520the%2520ability%2520of%2520CNNs%2520to%2520process%2520raw%2520EEG%2520data%2520effectively.%2520The%2520classification%2520accuracy%2520was%2520highest%2520for%2520right-turn%2520segments%252C%2520which%2520suggests%2520a%2520potential%2520spatial%2520bias%2520in%2520brain%2520activity.%2520This%2520study%2520lays%2520the%2520foundation%2520for%2520more%2520intuitive%2520brain-to-vehicle%2520communication%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Driver-Intention%20Prediction%20with%20Deep%20Learning%3A%20Real-Time%20Brain-to-Vehicle%20Communication&entry.906535625=Niloufar%20Alavi%20and%20Swati%20Shah%20and%20Rezvan%20Alamian%20and%20Stefan%20Goetz&entry.1292438233=Brain-computer%20interfaces%20%28BCIs%29%20allow%20direct%20communication%20between%20the%20brain%20and%20electronics%20without%20the%20need%20for%20speech%20or%20physical%20movement.%20Such%20interfaces%20can%20be%20particularly%20beneficial%20in%20applications%20requiring%20rapid%20response%20times%2C%20such%20as%20driving%2C%20where%20a%20vehicle%27s%20advanced%20driving%20assistance%20systems%20could%20benefit%20from%20immediate%20understanding%20of%20a%20driver%27s%20intentions.%20This%20study%20presents%20a%20novel%20method%20for%20predicting%20a%20driver%27s%20intention%20to%20steer%20using%20electroencephalography%20%28EEG%29%20signals%20through%20deep%20learning.%20A%20driving%20simulator%20created%20a%20controlled%20environment%20in%20which%20participants%20imagined%20controlling%20a%20vehicle%20during%20various%20driving%20scenarios%2C%20including%20left%20and%20right%20turns%2C%20as%20well%20as%20straight%20driving.%20A%20convolutional%20neural%20network%20%28CNN%29%20classified%20the%20detected%20EEG%20data%20with%20minimal%20pre-processing.%20Our%20model%20achieved%20an%20accuracy%20of%2083.7%25%20in%20distinguishing%20between%20the%20three%20steering%20intentions%20and%20demonstrated%20the%20ability%20of%20CNNs%20to%20process%20raw%20EEG%20data%20effectively.%20The%20classification%20accuracy%20was%20highest%20for%20right-turn%20segments%2C%20which%20suggests%20a%20potential%20spatial%20bias%20in%20brain%20activity.%20This%20study%20lays%20the%20foundation%20for%20more%20intuitive%20brain-to-vehicle%20communication%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.05084v1&entry.124074799=Read"},
{"title": "Higher-Order Adversarial Patches for Real-Time Object Detectors", "author": "Jens Bayer and Stefan Becker and David M\u00fcnch and Michael Arens and J\u00fcrgen Beyerer", "abstract": "Higher-order adversarial attacks can directly be considered the result of a cat-and-mouse game -- an elaborate action involving constant pursuit, near captures, and repeated escapes. This idiom describes the enduring circular training of adversarial attack patterns and adversarial training the best. The following work investigates the impact of higher-order adversarial attacks on object detectors by successively training attack patterns and hardening object detectors with adversarial training. The YOLOv10 object detector is chosen as a representative, and adversarial patches are used in an evasion attack manner. Our results indicate that higher-order adversarial patches are not only affecting the object detector directly trained on but rather provide a stronger generalization capacity compared to lower-order adversarial patches. Moreover, the results highlight that solely adversarial training is not sufficient to harden an object detector efficiently against this kind of adversarial attack. Code: https://github.com/JensBayer/HigherOrder", "link": "http://arxiv.org/abs/2601.04991v1", "date": "2026-01-08", "relevancy": 2.0093, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5085}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5062}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-Order%20Adversarial%20Patches%20for%20Real-Time%20Object%20Detectors&body=Title%3A%20Higher-Order%20Adversarial%20Patches%20for%20Real-Time%20Object%20Detectors%0AAuthor%3A%20Jens%20Bayer%20and%20Stefan%20Becker%20and%20David%20M%C3%BCnch%20and%20Michael%20Arens%20and%20J%C3%BCrgen%20Beyerer%0AAbstract%3A%20Higher-order%20adversarial%20attacks%20can%20directly%20be%20considered%20the%20result%20of%20a%20cat-and-mouse%20game%20--%20an%20elaborate%20action%20involving%20constant%20pursuit%2C%20near%20captures%2C%20and%20repeated%20escapes.%20This%20idiom%20describes%20the%20enduring%20circular%20training%20of%20adversarial%20attack%20patterns%20and%20adversarial%20training%20the%20best.%20The%20following%20work%20investigates%20the%20impact%20of%20higher-order%20adversarial%20attacks%20on%20object%20detectors%20by%20successively%20training%20attack%20patterns%20and%20hardening%20object%20detectors%20with%20adversarial%20training.%20The%20YOLOv10%20object%20detector%20is%20chosen%20as%20a%20representative%2C%20and%20adversarial%20patches%20are%20used%20in%20an%20evasion%20attack%20manner.%20Our%20results%20indicate%20that%20higher-order%20adversarial%20patches%20are%20not%20only%20affecting%20the%20object%20detector%20directly%20trained%20on%20but%20rather%20provide%20a%20stronger%20generalization%20capacity%20compared%20to%20lower-order%20adversarial%20patches.%20Moreover%2C%20the%20results%20highlight%20that%20solely%20adversarial%20training%20is%20not%20sufficient%20to%20harden%20an%20object%20detector%20efficiently%20against%20this%20kind%20of%20adversarial%20attack.%20Code%3A%20https%3A//github.com/JensBayer/HigherOrder%0ALink%3A%20http%3A//arxiv.org/abs/2601.04991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-Order%2520Adversarial%2520Patches%2520for%2520Real-Time%2520Object%2520Detectors%26entry.906535625%3DJens%2520Bayer%2520and%2520Stefan%2520Becker%2520and%2520David%2520M%25C3%25BCnch%2520and%2520Michael%2520Arens%2520and%2520J%25C3%25BCrgen%2520Beyerer%26entry.1292438233%3DHigher-order%2520adversarial%2520attacks%2520can%2520directly%2520be%2520considered%2520the%2520result%2520of%2520a%2520cat-and-mouse%2520game%2520--%2520an%2520elaborate%2520action%2520involving%2520constant%2520pursuit%252C%2520near%2520captures%252C%2520and%2520repeated%2520escapes.%2520This%2520idiom%2520describes%2520the%2520enduring%2520circular%2520training%2520of%2520adversarial%2520attack%2520patterns%2520and%2520adversarial%2520training%2520the%2520best.%2520The%2520following%2520work%2520investigates%2520the%2520impact%2520of%2520higher-order%2520adversarial%2520attacks%2520on%2520object%2520detectors%2520by%2520successively%2520training%2520attack%2520patterns%2520and%2520hardening%2520object%2520detectors%2520with%2520adversarial%2520training.%2520The%2520YOLOv10%2520object%2520detector%2520is%2520chosen%2520as%2520a%2520representative%252C%2520and%2520adversarial%2520patches%2520are%2520used%2520in%2520an%2520evasion%2520attack%2520manner.%2520Our%2520results%2520indicate%2520that%2520higher-order%2520adversarial%2520patches%2520are%2520not%2520only%2520affecting%2520the%2520object%2520detector%2520directly%2520trained%2520on%2520but%2520rather%2520provide%2520a%2520stronger%2520generalization%2520capacity%2520compared%2520to%2520lower-order%2520adversarial%2520patches.%2520Moreover%252C%2520the%2520results%2520highlight%2520that%2520solely%2520adversarial%2520training%2520is%2520not%2520sufficient%2520to%2520harden%2520an%2520object%2520detector%2520efficiently%2520against%2520this%2520kind%2520of%2520adversarial%2520attack.%2520Code%253A%2520https%253A//github.com/JensBayer/HigherOrder%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-Order%20Adversarial%20Patches%20for%20Real-Time%20Object%20Detectors&entry.906535625=Jens%20Bayer%20and%20Stefan%20Becker%20and%20David%20M%C3%BCnch%20and%20Michael%20Arens%20and%20J%C3%BCrgen%20Beyerer&entry.1292438233=Higher-order%20adversarial%20attacks%20can%20directly%20be%20considered%20the%20result%20of%20a%20cat-and-mouse%20game%20--%20an%20elaborate%20action%20involving%20constant%20pursuit%2C%20near%20captures%2C%20and%20repeated%20escapes.%20This%20idiom%20describes%20the%20enduring%20circular%20training%20of%20adversarial%20attack%20patterns%20and%20adversarial%20training%20the%20best.%20The%20following%20work%20investigates%20the%20impact%20of%20higher-order%20adversarial%20attacks%20on%20object%20detectors%20by%20successively%20training%20attack%20patterns%20and%20hardening%20object%20detectors%20with%20adversarial%20training.%20The%20YOLOv10%20object%20detector%20is%20chosen%20as%20a%20representative%2C%20and%20adversarial%20patches%20are%20used%20in%20an%20evasion%20attack%20manner.%20Our%20results%20indicate%20that%20higher-order%20adversarial%20patches%20are%20not%20only%20affecting%20the%20object%20detector%20directly%20trained%20on%20but%20rather%20provide%20a%20stronger%20generalization%20capacity%20compared%20to%20lower-order%20adversarial%20patches.%20Moreover%2C%20the%20results%20highlight%20that%20solely%20adversarial%20training%20is%20not%20sufficient%20to%20harden%20an%20object%20detector%20efficiently%20against%20this%20kind%20of%20adversarial%20attack.%20Code%3A%20https%3A//github.com/JensBayer/HigherOrder&entry.1838667208=http%3A//arxiv.org/abs/2601.04991v1&entry.124074799=Read"},
{"title": "N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator", "author": "Zheyu Lin and Jirui Yang and Yukui Qiu and Hengqi Guo and Yubing Bao and Yao Guan", "abstract": "Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.", "link": "http://arxiv.org/abs/2511.14195v2", "date": "2026-01-08", "relevancy": 2.0001, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.523}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5081}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20N-GLARE%3A%20An%20Non-Generative%20Latent%20Representation-Efficient%20LLM%20Safety%20Evaluator&body=Title%3A%20N-GLARE%3A%20An%20Non-Generative%20Latent%20Representation-Efficient%20LLM%20Safety%20Evaluator%0AAuthor%3A%20Zheyu%20Lin%20and%20Jirui%20Yang%20and%20Yukui%20Qiu%20and%20Hengqi%20Guo%20and%20Yubing%20Bao%20and%20Yao%20Guan%0AAbstract%3A%20Evaluating%20the%20safety%20robustness%20of%20LLMs%20is%20critical%20for%20their%20deployment.%20However%2C%20mainstream%20Red%20Teaming%20methods%20rely%20on%20online%20generation%20and%20black-box%20output%20analysis.%20These%20approaches%20are%20not%20only%20costly%20but%20also%20suffer%20from%20feedback%20latency%2C%20making%20them%20unsuitable%20for%20agile%20diagnostics%20after%20training%20a%20new%20model.%20To%20address%20this%2C%20we%20propose%20N-GLARE%20%28A%20Non-Generative%2C%20Latent%20Representation-Efficient%20LLM%20Safety%20Evaluator%29.%20N-GLARE%20operates%20entirely%20on%20the%20model%27s%20latent%20representations%2C%20bypassing%20the%20need%20for%20full%20text%20generation.%20It%20characterizes%20hidden%20layer%20dynamics%20by%20analyzing%20the%20APT%20%28Angular-Probabilistic%20Trajectory%29%20of%20latent%20representations%20and%20introducing%20the%20JSS%20%28Jensen-Shannon%20Separability%29%20metric.%20Experiments%20on%20over%2040%20models%20and%2020%20red%20teaming%20strategies%20demonstrate%20that%20the%20JSS%20metric%20exhibits%20high%20consistency%20with%20the%20safety%20rankings%20derived%20from%20Red%20Teaming.%20N-GLARE%20reproduces%20the%20discriminative%20trends%20of%20large-scale%20red-teaming%20tests%20at%20less%20than%201%5C%25%20of%20the%20token%20cost%20and%20the%20runtime%20cost%2C%20providing%20an%20efficient%20output-free%20evaluation%20proxy%20for%20real-time%20diagnostics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14195v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DN-GLARE%253A%2520An%2520Non-Generative%2520Latent%2520Representation-Efficient%2520LLM%2520Safety%2520Evaluator%26entry.906535625%3DZheyu%2520Lin%2520and%2520Jirui%2520Yang%2520and%2520Yukui%2520Qiu%2520and%2520Hengqi%2520Guo%2520and%2520Yubing%2520Bao%2520and%2520Yao%2520Guan%26entry.1292438233%3DEvaluating%2520the%2520safety%2520robustness%2520of%2520LLMs%2520is%2520critical%2520for%2520their%2520deployment.%2520However%252C%2520mainstream%2520Red%2520Teaming%2520methods%2520rely%2520on%2520online%2520generation%2520and%2520black-box%2520output%2520analysis.%2520These%2520approaches%2520are%2520not%2520only%2520costly%2520but%2520also%2520suffer%2520from%2520feedback%2520latency%252C%2520making%2520them%2520unsuitable%2520for%2520agile%2520diagnostics%2520after%2520training%2520a%2520new%2520model.%2520To%2520address%2520this%252C%2520we%2520propose%2520N-GLARE%2520%2528A%2520Non-Generative%252C%2520Latent%2520Representation-Efficient%2520LLM%2520Safety%2520Evaluator%2529.%2520N-GLARE%2520operates%2520entirely%2520on%2520the%2520model%2527s%2520latent%2520representations%252C%2520bypassing%2520the%2520need%2520for%2520full%2520text%2520generation.%2520It%2520characterizes%2520hidden%2520layer%2520dynamics%2520by%2520analyzing%2520the%2520APT%2520%2528Angular-Probabilistic%2520Trajectory%2529%2520of%2520latent%2520representations%2520and%2520introducing%2520the%2520JSS%2520%2528Jensen-Shannon%2520Separability%2529%2520metric.%2520Experiments%2520on%2520over%252040%2520models%2520and%252020%2520red%2520teaming%2520strategies%2520demonstrate%2520that%2520the%2520JSS%2520metric%2520exhibits%2520high%2520consistency%2520with%2520the%2520safety%2520rankings%2520derived%2520from%2520Red%2520Teaming.%2520N-GLARE%2520reproduces%2520the%2520discriminative%2520trends%2520of%2520large-scale%2520red-teaming%2520tests%2520at%2520less%2520than%25201%255C%2525%2520of%2520the%2520token%2520cost%2520and%2520the%2520runtime%2520cost%252C%2520providing%2520an%2520efficient%2520output-free%2520evaluation%2520proxy%2520for%2520real-time%2520diagnostics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14195v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=N-GLARE%3A%20An%20Non-Generative%20Latent%20Representation-Efficient%20LLM%20Safety%20Evaluator&entry.906535625=Zheyu%20Lin%20and%20Jirui%20Yang%20and%20Yukui%20Qiu%20and%20Hengqi%20Guo%20and%20Yubing%20Bao%20and%20Yao%20Guan&entry.1292438233=Evaluating%20the%20safety%20robustness%20of%20LLMs%20is%20critical%20for%20their%20deployment.%20However%2C%20mainstream%20Red%20Teaming%20methods%20rely%20on%20online%20generation%20and%20black-box%20output%20analysis.%20These%20approaches%20are%20not%20only%20costly%20but%20also%20suffer%20from%20feedback%20latency%2C%20making%20them%20unsuitable%20for%20agile%20diagnostics%20after%20training%20a%20new%20model.%20To%20address%20this%2C%20we%20propose%20N-GLARE%20%28A%20Non-Generative%2C%20Latent%20Representation-Efficient%20LLM%20Safety%20Evaluator%29.%20N-GLARE%20operates%20entirely%20on%20the%20model%27s%20latent%20representations%2C%20bypassing%20the%20need%20for%20full%20text%20generation.%20It%20characterizes%20hidden%20layer%20dynamics%20by%20analyzing%20the%20APT%20%28Angular-Probabilistic%20Trajectory%29%20of%20latent%20representations%20and%20introducing%20the%20JSS%20%28Jensen-Shannon%20Separability%29%20metric.%20Experiments%20on%20over%2040%20models%20and%2020%20red%20teaming%20strategies%20demonstrate%20that%20the%20JSS%20metric%20exhibits%20high%20consistency%20with%20the%20safety%20rankings%20derived%20from%20Red%20Teaming.%20N-GLARE%20reproduces%20the%20discriminative%20trends%20of%20large-scale%20red-teaming%20tests%20at%20less%20than%201%5C%25%20of%20the%20token%20cost%20and%20the%20runtime%20cost%2C%20providing%20an%20efficient%20output-free%20evaluation%20proxy%20for%20real-time%20diagnostics.&entry.1838667208=http%3A//arxiv.org/abs/2511.14195v2&entry.124074799=Read"},
{"title": "FaST: Efficient and Effective Long-Horizon Forecasting for Large-Scale Spatial-Temporal Graphs via Mixture-of-Experts", "author": "Yiji Zhao and Zihao Zhong and Ao Wang and Haomin Wen and Ming Jin and Yuxuan Liang and Huaiyu Wan and Hao Wu", "abstract": "Spatial-Temporal Graph (STG) forecasting on large-scale networks has garnered significant attention. However, existing models predominantly focus on short-horizon predictions and suffer from notorious computational costs and memory consumption when scaling to long-horizon predictions and large graphs. Targeting the above challenges, we present FaST, an effective and efficient framework based on heterogeneity-aware Mixture-of-Experts (MoEs) for long-horizon and large-scale STG forecasting, which unlocks one-week-ahead (672 steps at a 15-minute granularity) prediction with thousands of nodes. FaST is underpinned by two key innovations. First, an adaptive graph agent attention mechanism is proposed to alleviate the computational burden inherent in conventional graph convolution and self-attention modules when applied to large-scale graphs. Second, we propose a new parallel MoE module that replaces traditional feed-forward networks with Gated Linear Units (GLUs), enabling an efficient and scalable parallel structure. Extensive experiments on real-world datasets demonstrate that FaST not only delivers superior long-horizon predictive accuracy but also achieves remarkable computational efficiency compared to state-of-the-art baselines. Our source code is available at: https://github.com/yijizhao/FaST.", "link": "http://arxiv.org/abs/2601.05174v1", "date": "2026-01-08", "relevancy": 1.9981, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5054}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.498}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaST%3A%20Efficient%20and%20Effective%20Long-Horizon%20Forecasting%20for%20Large-Scale%20Spatial-Temporal%20Graphs%20via%20Mixture-of-Experts&body=Title%3A%20FaST%3A%20Efficient%20and%20Effective%20Long-Horizon%20Forecasting%20for%20Large-Scale%20Spatial-Temporal%20Graphs%20via%20Mixture-of-Experts%0AAuthor%3A%20Yiji%20Zhao%20and%20Zihao%20Zhong%20and%20Ao%20Wang%20and%20Haomin%20Wen%20and%20Ming%20Jin%20and%20Yuxuan%20Liang%20and%20Huaiyu%20Wan%20and%20Hao%20Wu%0AAbstract%3A%20Spatial-Temporal%20Graph%20%28STG%29%20forecasting%20on%20large-scale%20networks%20has%20garnered%20significant%20attention.%20However%2C%20existing%20models%20predominantly%20focus%20on%20short-horizon%20predictions%20and%20suffer%20from%20notorious%20computational%20costs%20and%20memory%20consumption%20when%20scaling%20to%20long-horizon%20predictions%20and%20large%20graphs.%20Targeting%20the%20above%20challenges%2C%20we%20present%20FaST%2C%20an%20effective%20and%20efficient%20framework%20based%20on%20heterogeneity-aware%20Mixture-of-Experts%20%28MoEs%29%20for%20long-horizon%20and%20large-scale%20STG%20forecasting%2C%20which%20unlocks%20one-week-ahead%20%28672%20steps%20at%20a%2015-minute%20granularity%29%20prediction%20with%20thousands%20of%20nodes.%20FaST%20is%20underpinned%20by%20two%20key%20innovations.%20First%2C%20an%20adaptive%20graph%20agent%20attention%20mechanism%20is%20proposed%20to%20alleviate%20the%20computational%20burden%20inherent%20in%20conventional%20graph%20convolution%20and%20self-attention%20modules%20when%20applied%20to%20large-scale%20graphs.%20Second%2C%20we%20propose%20a%20new%20parallel%20MoE%20module%20that%20replaces%20traditional%20feed-forward%20networks%20with%20Gated%20Linear%20Units%20%28GLUs%29%2C%20enabling%20an%20efficient%20and%20scalable%20parallel%20structure.%20Extensive%20experiments%20on%20real-world%20datasets%20demonstrate%20that%20FaST%20not%20only%20delivers%20superior%20long-horizon%20predictive%20accuracy%20but%20also%20achieves%20remarkable%20computational%20efficiency%20compared%20to%20state-of-the-art%20baselines.%20Our%20source%20code%20is%20available%20at%3A%20https%3A//github.com/yijizhao/FaST.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05174v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaST%253A%2520Efficient%2520and%2520Effective%2520Long-Horizon%2520Forecasting%2520for%2520Large-Scale%2520Spatial-Temporal%2520Graphs%2520via%2520Mixture-of-Experts%26entry.906535625%3DYiji%2520Zhao%2520and%2520Zihao%2520Zhong%2520and%2520Ao%2520Wang%2520and%2520Haomin%2520Wen%2520and%2520Ming%2520Jin%2520and%2520Yuxuan%2520Liang%2520and%2520Huaiyu%2520Wan%2520and%2520Hao%2520Wu%26entry.1292438233%3DSpatial-Temporal%2520Graph%2520%2528STG%2529%2520forecasting%2520on%2520large-scale%2520networks%2520has%2520garnered%2520significant%2520attention.%2520However%252C%2520existing%2520models%2520predominantly%2520focus%2520on%2520short-horizon%2520predictions%2520and%2520suffer%2520from%2520notorious%2520computational%2520costs%2520and%2520memory%2520consumption%2520when%2520scaling%2520to%2520long-horizon%2520predictions%2520and%2520large%2520graphs.%2520Targeting%2520the%2520above%2520challenges%252C%2520we%2520present%2520FaST%252C%2520an%2520effective%2520and%2520efficient%2520framework%2520based%2520on%2520heterogeneity-aware%2520Mixture-of-Experts%2520%2528MoEs%2529%2520for%2520long-horizon%2520and%2520large-scale%2520STG%2520forecasting%252C%2520which%2520unlocks%2520one-week-ahead%2520%2528672%2520steps%2520at%2520a%252015-minute%2520granularity%2529%2520prediction%2520with%2520thousands%2520of%2520nodes.%2520FaST%2520is%2520underpinned%2520by%2520two%2520key%2520innovations.%2520First%252C%2520an%2520adaptive%2520graph%2520agent%2520attention%2520mechanism%2520is%2520proposed%2520to%2520alleviate%2520the%2520computational%2520burden%2520inherent%2520in%2520conventional%2520graph%2520convolution%2520and%2520self-attention%2520modules%2520when%2520applied%2520to%2520large-scale%2520graphs.%2520Second%252C%2520we%2520propose%2520a%2520new%2520parallel%2520MoE%2520module%2520that%2520replaces%2520traditional%2520feed-forward%2520networks%2520with%2520Gated%2520Linear%2520Units%2520%2528GLUs%2529%252C%2520enabling%2520an%2520efficient%2520and%2520scalable%2520parallel%2520structure.%2520Extensive%2520experiments%2520on%2520real-world%2520datasets%2520demonstrate%2520that%2520FaST%2520not%2520only%2520delivers%2520superior%2520long-horizon%2520predictive%2520accuracy%2520but%2520also%2520achieves%2520remarkable%2520computational%2520efficiency%2520compared%2520to%2520state-of-the-art%2520baselines.%2520Our%2520source%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/yijizhao/FaST.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05174v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaST%3A%20Efficient%20and%20Effective%20Long-Horizon%20Forecasting%20for%20Large-Scale%20Spatial-Temporal%20Graphs%20via%20Mixture-of-Experts&entry.906535625=Yiji%20Zhao%20and%20Zihao%20Zhong%20and%20Ao%20Wang%20and%20Haomin%20Wen%20and%20Ming%20Jin%20and%20Yuxuan%20Liang%20and%20Huaiyu%20Wan%20and%20Hao%20Wu&entry.1292438233=Spatial-Temporal%20Graph%20%28STG%29%20forecasting%20on%20large-scale%20networks%20has%20garnered%20significant%20attention.%20However%2C%20existing%20models%20predominantly%20focus%20on%20short-horizon%20predictions%20and%20suffer%20from%20notorious%20computational%20costs%20and%20memory%20consumption%20when%20scaling%20to%20long-horizon%20predictions%20and%20large%20graphs.%20Targeting%20the%20above%20challenges%2C%20we%20present%20FaST%2C%20an%20effective%20and%20efficient%20framework%20based%20on%20heterogeneity-aware%20Mixture-of-Experts%20%28MoEs%29%20for%20long-horizon%20and%20large-scale%20STG%20forecasting%2C%20which%20unlocks%20one-week-ahead%20%28672%20steps%20at%20a%2015-minute%20granularity%29%20prediction%20with%20thousands%20of%20nodes.%20FaST%20is%20underpinned%20by%20two%20key%20innovations.%20First%2C%20an%20adaptive%20graph%20agent%20attention%20mechanism%20is%20proposed%20to%20alleviate%20the%20computational%20burden%20inherent%20in%20conventional%20graph%20convolution%20and%20self-attention%20modules%20when%20applied%20to%20large-scale%20graphs.%20Second%2C%20we%20propose%20a%20new%20parallel%20MoE%20module%20that%20replaces%20traditional%20feed-forward%20networks%20with%20Gated%20Linear%20Units%20%28GLUs%29%2C%20enabling%20an%20efficient%20and%20scalable%20parallel%20structure.%20Extensive%20experiments%20on%20real-world%20datasets%20demonstrate%20that%20FaST%20not%20only%20delivers%20superior%20long-horizon%20predictive%20accuracy%20but%20also%20achieves%20remarkable%20computational%20efficiency%20compared%20to%20state-of-the-art%20baselines.%20Our%20source%20code%20is%20available%20at%3A%20https%3A//github.com/yijizhao/FaST.&entry.1838667208=http%3A//arxiv.org/abs/2601.05174v1&entry.124074799=Read"},
{"title": "Improving and Evaluating Open Deep Research Agents", "author": "Doaa Allabadi and Kyle Bradbury and Jordan M. Malof", "abstract": "We focus here on Deep Research Agents (DRAs), which are systems that can take a natural language prompt from a user, and then autonomously search for, and utilize, internet-based content to address the prompt. Recent DRAs have demonstrated impressive capabilities on public benchmarks however, recent research largely involves proprietary closed-source systems. At the time of this work, we only found one open-source DRA, termed Open Deep Research (ODR). In this work we adapt the challenging recent BrowseComp benchmark to compare ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small), comprising a subset of BrowseComp, as a more computationally-tractable DRA benchmark for academic labs. We benchmark ODR and two other proprietary systems on BC-Small: one system from Anthropic and one system from Google. We find that all three systems achieve 0% accuracy on the test set of 60 questions. We introduce three strategic improvements to ODR, resulting in the ODR+ model, which achieves a state-of-the-art 10% success rate on BC-Small among both closed-source and open-source systems. We report ablation studies indicating that all three of our improvements contributed to the success of ODR+.", "link": "http://arxiv.org/abs/2508.10152v2", "date": "2026-01-08", "relevancy": 1.9945, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4995}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20and%20Evaluating%20Open%20Deep%20Research%20Agents&body=Title%3A%20Improving%20and%20Evaluating%20Open%20Deep%20Research%20Agents%0AAuthor%3A%20Doaa%20Allabadi%20and%20Kyle%20Bradbury%20and%20Jordan%20M.%20Malof%0AAbstract%3A%20We%20focus%20here%20on%20Deep%20Research%20Agents%20%28DRAs%29%2C%20which%20are%20systems%20that%20can%20take%20a%20natural%20language%20prompt%20from%20a%20user%2C%20and%20then%20autonomously%20search%20for%2C%20and%20utilize%2C%20internet-based%20content%20to%20address%20the%20prompt.%20Recent%20DRAs%20have%20demonstrated%20impressive%20capabilities%20on%20public%20benchmarks%20however%2C%20recent%20research%20largely%20involves%20proprietary%20closed-source%20systems.%20At%20the%20time%20of%20this%20work%2C%20we%20only%20found%20one%20open-source%20DRA%2C%20termed%20Open%20Deep%20Research%20%28ODR%29.%20In%20this%20work%20we%20adapt%20the%20challenging%20recent%20BrowseComp%20benchmark%20to%20compare%20ODR%20to%20existing%20proprietary%20systems.%20We%20propose%20BrowseComp-Small%20%28BC-Small%29%2C%20comprising%20a%20subset%20of%20BrowseComp%2C%20as%20a%20more%20computationally-tractable%20DRA%20benchmark%20for%20academic%20labs.%20We%20benchmark%20ODR%20and%20two%20other%20proprietary%20systems%20on%20BC-Small%3A%20one%20system%20from%20Anthropic%20and%20one%20system%20from%20Google.%20We%20find%20that%20all%20three%20systems%20achieve%200%25%20accuracy%20on%20the%20test%20set%20of%2060%20questions.%20We%20introduce%20three%20strategic%20improvements%20to%20ODR%2C%20resulting%20in%20the%20ODR%2B%20model%2C%20which%20achieves%20a%20state-of-the-art%2010%25%20success%20rate%20on%20BC-Small%20among%20both%20closed-source%20and%20open-source%20systems.%20We%20report%20ablation%20studies%20indicating%20that%20all%20three%20of%20our%20improvements%20contributed%20to%20the%20success%20of%20ODR%2B.%0ALink%3A%20http%3A//arxiv.org/abs/2508.10152v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520and%2520Evaluating%2520Open%2520Deep%2520Research%2520Agents%26entry.906535625%3DDoaa%2520Allabadi%2520and%2520Kyle%2520Bradbury%2520and%2520Jordan%2520M.%2520Malof%26entry.1292438233%3DWe%2520focus%2520here%2520on%2520Deep%2520Research%2520Agents%2520%2528DRAs%2529%252C%2520which%2520are%2520systems%2520that%2520can%2520take%2520a%2520natural%2520language%2520prompt%2520from%2520a%2520user%252C%2520and%2520then%2520autonomously%2520search%2520for%252C%2520and%2520utilize%252C%2520internet-based%2520content%2520to%2520address%2520the%2520prompt.%2520Recent%2520DRAs%2520have%2520demonstrated%2520impressive%2520capabilities%2520on%2520public%2520benchmarks%2520however%252C%2520recent%2520research%2520largely%2520involves%2520proprietary%2520closed-source%2520systems.%2520At%2520the%2520time%2520of%2520this%2520work%252C%2520we%2520only%2520found%2520one%2520open-source%2520DRA%252C%2520termed%2520Open%2520Deep%2520Research%2520%2528ODR%2529.%2520In%2520this%2520work%2520we%2520adapt%2520the%2520challenging%2520recent%2520BrowseComp%2520benchmark%2520to%2520compare%2520ODR%2520to%2520existing%2520proprietary%2520systems.%2520We%2520propose%2520BrowseComp-Small%2520%2528BC-Small%2529%252C%2520comprising%2520a%2520subset%2520of%2520BrowseComp%252C%2520as%2520a%2520more%2520computationally-tractable%2520DRA%2520benchmark%2520for%2520academic%2520labs.%2520We%2520benchmark%2520ODR%2520and%2520two%2520other%2520proprietary%2520systems%2520on%2520BC-Small%253A%2520one%2520system%2520from%2520Anthropic%2520and%2520one%2520system%2520from%2520Google.%2520We%2520find%2520that%2520all%2520three%2520systems%2520achieve%25200%2525%2520accuracy%2520on%2520the%2520test%2520set%2520of%252060%2520questions.%2520We%2520introduce%2520three%2520strategic%2520improvements%2520to%2520ODR%252C%2520resulting%2520in%2520the%2520ODR%252B%2520model%252C%2520which%2520achieves%2520a%2520state-of-the-art%252010%2525%2520success%2520rate%2520on%2520BC-Small%2520among%2520both%2520closed-source%2520and%2520open-source%2520systems.%2520We%2520report%2520ablation%2520studies%2520indicating%2520that%2520all%2520three%2520of%2520our%2520improvements%2520contributed%2520to%2520the%2520success%2520of%2520ODR%252B.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10152v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20and%20Evaluating%20Open%20Deep%20Research%20Agents&entry.906535625=Doaa%20Allabadi%20and%20Kyle%20Bradbury%20and%20Jordan%20M.%20Malof&entry.1292438233=We%20focus%20here%20on%20Deep%20Research%20Agents%20%28DRAs%29%2C%20which%20are%20systems%20that%20can%20take%20a%20natural%20language%20prompt%20from%20a%20user%2C%20and%20then%20autonomously%20search%20for%2C%20and%20utilize%2C%20internet-based%20content%20to%20address%20the%20prompt.%20Recent%20DRAs%20have%20demonstrated%20impressive%20capabilities%20on%20public%20benchmarks%20however%2C%20recent%20research%20largely%20involves%20proprietary%20closed-source%20systems.%20At%20the%20time%20of%20this%20work%2C%20we%20only%20found%20one%20open-source%20DRA%2C%20termed%20Open%20Deep%20Research%20%28ODR%29.%20In%20this%20work%20we%20adapt%20the%20challenging%20recent%20BrowseComp%20benchmark%20to%20compare%20ODR%20to%20existing%20proprietary%20systems.%20We%20propose%20BrowseComp-Small%20%28BC-Small%29%2C%20comprising%20a%20subset%20of%20BrowseComp%2C%20as%20a%20more%20computationally-tractable%20DRA%20benchmark%20for%20academic%20labs.%20We%20benchmark%20ODR%20and%20two%20other%20proprietary%20systems%20on%20BC-Small%3A%20one%20system%20from%20Anthropic%20and%20one%20system%20from%20Google.%20We%20find%20that%20all%20three%20systems%20achieve%200%25%20accuracy%20on%20the%20test%20set%20of%2060%20questions.%20We%20introduce%20three%20strategic%20improvements%20to%20ODR%2C%20resulting%20in%20the%20ODR%2B%20model%2C%20which%20achieves%20a%20state-of-the-art%2010%25%20success%20rate%20on%20BC-Small%20among%20both%20closed-source%20and%20open-source%20systems.%20We%20report%20ablation%20studies%20indicating%20that%20all%20three%20of%20our%20improvements%20contributed%20to%20the%20success%20of%20ODR%2B.&entry.1838667208=http%3A//arxiv.org/abs/2508.10152v2&entry.124074799=Read"},
{"title": "Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition", "author": "Nils Einecke", "abstract": "Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.", "link": "http://arxiv.org/abs/2601.04920v1", "date": "2026-01-08", "relevancy": 1.9919, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5447}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conversational%20AI%20for%20Rapid%20Scientific%20Prototyping%3A%20A%20Case%20Study%20on%20ESA%27s%20ELOPE%20Competition&body=Title%3A%20Conversational%20AI%20for%20Rapid%20Scientific%20Prototyping%3A%20A%20Case%20Study%20on%20ESA%27s%20ELOPE%20Competition%0AAuthor%3A%20Nils%20Einecke%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20as%20coding%20partners%2C%20yet%20their%20role%20in%20accelerating%20scientific%20discovery%20remains%20underexplored.%20This%20paper%20presents%20a%20case%20study%20of%20using%20ChatGPT%20for%20rapid%20prototyping%20in%20ESA%27s%20ELOPE%20%28Event-based%20Lunar%20OPtical%20flow%20Egomotion%20estimation%29%20competition.%20The%20competition%20required%20participants%20to%20process%20event%20camera%20data%20to%20estimate%20lunar%20lander%20trajectories.%20Despite%20joining%20late%2C%20we%20achieved%20second%20place%20with%20a%20score%20of%200.01282%2C%20highlighting%20the%20potential%20of%20human-AI%20collaboration%20in%20competitive%20scientific%20settings.%20ChatGPT%20contributed%20not%20only%20executable%20code%20but%20also%20algorithmic%20reasoning%2C%20data%20handling%20routines%2C%20and%20methodological%20suggestions%2C%20such%20as%20using%20fixed%20number%20of%20events%20instead%20of%20fixed%20time%20spans%20for%20windowing.%20At%20the%20same%20time%2C%20we%20observed%20limitations%3A%20the%20model%20often%20introduced%20unnecessary%20structural%20changes%2C%20gets%20confused%20by%20intermediate%20discussions%20about%20alternative%20ideas%2C%20occasionally%20produced%20critical%20errors%20and%20forgets%20important%20aspects%20in%20longer%20scientific%20discussions.%20By%20analyzing%20these%20strengths%20and%20shortcomings%2C%20we%20show%20how%20conversational%20AI%20can%20both%20accelerate%20development%20and%20support%20conceptual%20insight%20in%20scientific%20research.%20We%20argue%20that%20structured%20integration%20of%20LLMs%20into%20the%20scientific%20workflow%20can%20enhance%20rapid%20prototyping%20by%20proposing%20best%20practices%20for%20AI-assisted%20scientific%20work.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConversational%2520AI%2520for%2520Rapid%2520Scientific%2520Prototyping%253A%2520A%2520Case%2520Study%2520on%2520ESA%2527s%2520ELOPE%2520Competition%26entry.906535625%3DNils%2520Einecke%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520as%2520coding%2520partners%252C%2520yet%2520their%2520role%2520in%2520accelerating%2520scientific%2520discovery%2520remains%2520underexplored.%2520This%2520paper%2520presents%2520a%2520case%2520study%2520of%2520using%2520ChatGPT%2520for%2520rapid%2520prototyping%2520in%2520ESA%2527s%2520ELOPE%2520%2528Event-based%2520Lunar%2520OPtical%2520flow%2520Egomotion%2520estimation%2529%2520competition.%2520The%2520competition%2520required%2520participants%2520to%2520process%2520event%2520camera%2520data%2520to%2520estimate%2520lunar%2520lander%2520trajectories.%2520Despite%2520joining%2520late%252C%2520we%2520achieved%2520second%2520place%2520with%2520a%2520score%2520of%25200.01282%252C%2520highlighting%2520the%2520potential%2520of%2520human-AI%2520collaboration%2520in%2520competitive%2520scientific%2520settings.%2520ChatGPT%2520contributed%2520not%2520only%2520executable%2520code%2520but%2520also%2520algorithmic%2520reasoning%252C%2520data%2520handling%2520routines%252C%2520and%2520methodological%2520suggestions%252C%2520such%2520as%2520using%2520fixed%2520number%2520of%2520events%2520instead%2520of%2520fixed%2520time%2520spans%2520for%2520windowing.%2520At%2520the%2520same%2520time%252C%2520we%2520observed%2520limitations%253A%2520the%2520model%2520often%2520introduced%2520unnecessary%2520structural%2520changes%252C%2520gets%2520confused%2520by%2520intermediate%2520discussions%2520about%2520alternative%2520ideas%252C%2520occasionally%2520produced%2520critical%2520errors%2520and%2520forgets%2520important%2520aspects%2520in%2520longer%2520scientific%2520discussions.%2520By%2520analyzing%2520these%2520strengths%2520and%2520shortcomings%252C%2520we%2520show%2520how%2520conversational%2520AI%2520can%2520both%2520accelerate%2520development%2520and%2520support%2520conceptual%2520insight%2520in%2520scientific%2520research.%2520We%2520argue%2520that%2520structured%2520integration%2520of%2520LLMs%2520into%2520the%2520scientific%2520workflow%2520can%2520enhance%2520rapid%2520prototyping%2520by%2520proposing%2520best%2520practices%2520for%2520AI-assisted%2520scientific%2520work.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conversational%20AI%20for%20Rapid%20Scientific%20Prototyping%3A%20A%20Case%20Study%20on%20ESA%27s%20ELOPE%20Competition&entry.906535625=Nils%20Einecke&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20as%20coding%20partners%2C%20yet%20their%20role%20in%20accelerating%20scientific%20discovery%20remains%20underexplored.%20This%20paper%20presents%20a%20case%20study%20of%20using%20ChatGPT%20for%20rapid%20prototyping%20in%20ESA%27s%20ELOPE%20%28Event-based%20Lunar%20OPtical%20flow%20Egomotion%20estimation%29%20competition.%20The%20competition%20required%20participants%20to%20process%20event%20camera%20data%20to%20estimate%20lunar%20lander%20trajectories.%20Despite%20joining%20late%2C%20we%20achieved%20second%20place%20with%20a%20score%20of%200.01282%2C%20highlighting%20the%20potential%20of%20human-AI%20collaboration%20in%20competitive%20scientific%20settings.%20ChatGPT%20contributed%20not%20only%20executable%20code%20but%20also%20algorithmic%20reasoning%2C%20data%20handling%20routines%2C%20and%20methodological%20suggestions%2C%20such%20as%20using%20fixed%20number%20of%20events%20instead%20of%20fixed%20time%20spans%20for%20windowing.%20At%20the%20same%20time%2C%20we%20observed%20limitations%3A%20the%20model%20often%20introduced%20unnecessary%20structural%20changes%2C%20gets%20confused%20by%20intermediate%20discussions%20about%20alternative%20ideas%2C%20occasionally%20produced%20critical%20errors%20and%20forgets%20important%20aspects%20in%20longer%20scientific%20discussions.%20By%20analyzing%20these%20strengths%20and%20shortcomings%2C%20we%20show%20how%20conversational%20AI%20can%20both%20accelerate%20development%20and%20support%20conceptual%20insight%20in%20scientific%20research.%20We%20argue%20that%20structured%20integration%20of%20LLMs%20into%20the%20scientific%20workflow%20can%20enhance%20rapid%20prototyping%20by%20proposing%20best%20practices%20for%20AI-assisted%20scientific%20work.&entry.1838667208=http%3A//arxiv.org/abs/2601.04920v1&entry.124074799=Read"},
{"title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice", "author": "Shuming Liu and Mingchen Zhuge and Changsheng Zhao and Jun Chen and Lemeng Wu and Zechun Liu and Chenchen Zhu and Zhipeng Cai and Chong Zhou and Haozhe Liu and Ernie Chang and Saksham Suri and Hongyu Xu and Qi Qian and Wei Wen and Balakrishnan Varadarajan and Zhuang Liu and Hu Xu and Florian Bordes and Raghuraman Krishnamoorthi and Bernard Ghanem and Vikas Chandra and Yunyang Xiong", "abstract": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.", "link": "http://arxiv.org/abs/2601.05175v1", "date": "2026-01-08", "relevancy": 1.9805, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoAuto-R1%3A%20Video%20Auto%20Reasoning%20via%20Thinking%20Once%2C%20Answering%20Twice&body=Title%3A%20VideoAuto-R1%3A%20Video%20Auto%20Reasoning%20via%20Thinking%20Once%2C%20Answering%20Twice%0AAuthor%3A%20Shuming%20Liu%20and%20Mingchen%20Zhuge%20and%20Changsheng%20Zhao%20and%20Jun%20Chen%20and%20Lemeng%20Wu%20and%20Zechun%20Liu%20and%20Chenchen%20Zhu%20and%20Zhipeng%20Cai%20and%20Chong%20Zhou%20and%20Haozhe%20Liu%20and%20Ernie%20Chang%20and%20Saksham%20Suri%20and%20Hongyu%20Xu%20and%20Qi%20Qian%20and%20Wei%20Wen%20and%20Balakrishnan%20Varadarajan%20and%20Zhuang%20Liu%20and%20Hu%20Xu%20and%20Florian%20Bordes%20and%20Raghuraman%20Krishnamoorthi%20and%20Bernard%20Ghanem%20and%20Vikas%20Chandra%20and%20Yunyang%20Xiong%0AAbstract%3A%20Chain-of-thought%20%28CoT%29%20reasoning%20has%20emerged%20as%20a%20powerful%20tool%20for%20multimodal%20large%20language%20models%20on%20video%20understanding%20tasks.%20However%2C%20its%20necessity%20and%20advantages%20over%20direct%20answering%20remain%20underexplored.%20In%20this%20paper%2C%20we%20first%20demonstrate%20that%20for%20RL-trained%20video%20models%2C%20direct%20answering%20often%20matches%20or%20even%20surpasses%20CoT%20performance%2C%20despite%20CoT%20producing%20step-by-step%20analyses%20at%20a%20higher%20computational%20cost.%20Motivated%20by%20this%2C%20we%20propose%20VideoAuto-R1%2C%20a%20video%20understanding%20framework%20that%20adopts%20a%20reason-when-necessary%20strategy.%20During%20training%2C%20our%20approach%20follows%20a%20Thinking%20Once%2C%20Answering%20Twice%20paradigm%3A%20the%20model%20first%20generates%20an%20initial%20answer%2C%20then%20performs%20reasoning%2C%20and%20finally%20outputs%20a%20reviewed%20answer.%20Both%20answers%20are%20supervised%20via%20verifiable%20rewards.%20During%20inference%2C%20the%20model%20uses%20the%20confidence%20score%20of%20the%20initial%20answer%20to%20determine%20whether%20to%20proceed%20with%20reasoning.%20Across%20video%20QA%20and%20grounding%20benchmarks%2C%20VideoAuto-R1%20achieves%20state-of-the-art%20accuracy%20with%20significantly%20improved%20efficiency%2C%20reducing%20the%20average%20response%20length%20by%20~3.3x%2C%20e.g.%2C%20from%20149%20to%20just%2044%20tokens.%20Moreover%2C%20we%20observe%20a%20low%20rate%20of%20thinking-mode%20activation%20on%20perception-oriented%20tasks%2C%20but%20a%20higher%20rate%20on%20reasoning-intensive%20tasks.%20This%20suggests%20that%20explicit%20language-based%20reasoning%20is%20generally%20beneficial%20but%20not%20always%20necessary.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoAuto-R1%253A%2520Video%2520Auto%2520Reasoning%2520via%2520Thinking%2520Once%252C%2520Answering%2520Twice%26entry.906535625%3DShuming%2520Liu%2520and%2520Mingchen%2520Zhuge%2520and%2520Changsheng%2520Zhao%2520and%2520Jun%2520Chen%2520and%2520Lemeng%2520Wu%2520and%2520Zechun%2520Liu%2520and%2520Chenchen%2520Zhu%2520and%2520Zhipeng%2520Cai%2520and%2520Chong%2520Zhou%2520and%2520Haozhe%2520Liu%2520and%2520Ernie%2520Chang%2520and%2520Saksham%2520Suri%2520and%2520Hongyu%2520Xu%2520and%2520Qi%2520Qian%2520and%2520Wei%2520Wen%2520and%2520Balakrishnan%2520Varadarajan%2520and%2520Zhuang%2520Liu%2520and%2520Hu%2520Xu%2520and%2520Florian%2520Bordes%2520and%2520Raghuraman%2520Krishnamoorthi%2520and%2520Bernard%2520Ghanem%2520and%2520Vikas%2520Chandra%2520and%2520Yunyang%2520Xiong%26entry.1292438233%3DChain-of-thought%2520%2528CoT%2529%2520reasoning%2520has%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520multimodal%2520large%2520language%2520models%2520on%2520video%2520understanding%2520tasks.%2520However%252C%2520its%2520necessity%2520and%2520advantages%2520over%2520direct%2520answering%2520remain%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520first%2520demonstrate%2520that%2520for%2520RL-trained%2520video%2520models%252C%2520direct%2520answering%2520often%2520matches%2520or%2520even%2520surpasses%2520CoT%2520performance%252C%2520despite%2520CoT%2520producing%2520step-by-step%2520analyses%2520at%2520a%2520higher%2520computational%2520cost.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520VideoAuto-R1%252C%2520a%2520video%2520understanding%2520framework%2520that%2520adopts%2520a%2520reason-when-necessary%2520strategy.%2520During%2520training%252C%2520our%2520approach%2520follows%2520a%2520Thinking%2520Once%252C%2520Answering%2520Twice%2520paradigm%253A%2520the%2520model%2520first%2520generates%2520an%2520initial%2520answer%252C%2520then%2520performs%2520reasoning%252C%2520and%2520finally%2520outputs%2520a%2520reviewed%2520answer.%2520Both%2520answers%2520are%2520supervised%2520via%2520verifiable%2520rewards.%2520During%2520inference%252C%2520the%2520model%2520uses%2520the%2520confidence%2520score%2520of%2520the%2520initial%2520answer%2520to%2520determine%2520whether%2520to%2520proceed%2520with%2520reasoning.%2520Across%2520video%2520QA%2520and%2520grounding%2520benchmarks%252C%2520VideoAuto-R1%2520achieves%2520state-of-the-art%2520accuracy%2520with%2520significantly%2520improved%2520efficiency%252C%2520reducing%2520the%2520average%2520response%2520length%2520by%2520~3.3x%252C%2520e.g.%252C%2520from%2520149%2520to%2520just%252044%2520tokens.%2520Moreover%252C%2520we%2520observe%2520a%2520low%2520rate%2520of%2520thinking-mode%2520activation%2520on%2520perception-oriented%2520tasks%252C%2520but%2520a%2520higher%2520rate%2520on%2520reasoning-intensive%2520tasks.%2520This%2520suggests%2520that%2520explicit%2520language-based%2520reasoning%2520is%2520generally%2520beneficial%2520but%2520not%2520always%2520necessary.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoAuto-R1%3A%20Video%20Auto%20Reasoning%20via%20Thinking%20Once%2C%20Answering%20Twice&entry.906535625=Shuming%20Liu%20and%20Mingchen%20Zhuge%20and%20Changsheng%20Zhao%20and%20Jun%20Chen%20and%20Lemeng%20Wu%20and%20Zechun%20Liu%20and%20Chenchen%20Zhu%20and%20Zhipeng%20Cai%20and%20Chong%20Zhou%20and%20Haozhe%20Liu%20and%20Ernie%20Chang%20and%20Saksham%20Suri%20and%20Hongyu%20Xu%20and%20Qi%20Qian%20and%20Wei%20Wen%20and%20Balakrishnan%20Varadarajan%20and%20Zhuang%20Liu%20and%20Hu%20Xu%20and%20Florian%20Bordes%20and%20Raghuraman%20Krishnamoorthi%20and%20Bernard%20Ghanem%20and%20Vikas%20Chandra%20and%20Yunyang%20Xiong&entry.1292438233=Chain-of-thought%20%28CoT%29%20reasoning%20has%20emerged%20as%20a%20powerful%20tool%20for%20multimodal%20large%20language%20models%20on%20video%20understanding%20tasks.%20However%2C%20its%20necessity%20and%20advantages%20over%20direct%20answering%20remain%20underexplored.%20In%20this%20paper%2C%20we%20first%20demonstrate%20that%20for%20RL-trained%20video%20models%2C%20direct%20answering%20often%20matches%20or%20even%20surpasses%20CoT%20performance%2C%20despite%20CoT%20producing%20step-by-step%20analyses%20at%20a%20higher%20computational%20cost.%20Motivated%20by%20this%2C%20we%20propose%20VideoAuto-R1%2C%20a%20video%20understanding%20framework%20that%20adopts%20a%20reason-when-necessary%20strategy.%20During%20training%2C%20our%20approach%20follows%20a%20Thinking%20Once%2C%20Answering%20Twice%20paradigm%3A%20the%20model%20first%20generates%20an%20initial%20answer%2C%20then%20performs%20reasoning%2C%20and%20finally%20outputs%20a%20reviewed%20answer.%20Both%20answers%20are%20supervised%20via%20verifiable%20rewards.%20During%20inference%2C%20the%20model%20uses%20the%20confidence%20score%20of%20the%20initial%20answer%20to%20determine%20whether%20to%20proceed%20with%20reasoning.%20Across%20video%20QA%20and%20grounding%20benchmarks%2C%20VideoAuto-R1%20achieves%20state-of-the-art%20accuracy%20with%20significantly%20improved%20efficiency%2C%20reducing%20the%20average%20response%20length%20by%20~3.3x%2C%20e.g.%2C%20from%20149%20to%20just%2044%20tokens.%20Moreover%2C%20we%20observe%20a%20low%20rate%20of%20thinking-mode%20activation%20on%20perception-oriented%20tasks%2C%20but%20a%20higher%20rate%20on%20reasoning-intensive%20tasks.%20This%20suggests%20that%20explicit%20language-based%20reasoning%20is%20generally%20beneficial%20but%20not%20always%20necessary.&entry.1838667208=http%3A//arxiv.org/abs/2601.05175v1&entry.124074799=Read"},
{"title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes", "author": "Yuan-Kang Lee and Kuan-Lin Chen and Chia-Che Chang and Yu-Lun Liu", "abstract": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/", "link": "http://arxiv.org/abs/2601.05249v1", "date": "2026-01-08", "relevancy": 1.9719, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4951}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4926}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RL-AWB%3A%20Deep%20Reinforcement%20Learning%20for%20Auto%20White%20Balance%20Correction%20in%20Low-Light%20Night-time%20Scenes&body=Title%3A%20RL-AWB%3A%20Deep%20Reinforcement%20Learning%20for%20Auto%20White%20Balance%20Correction%20in%20Low-Light%20Night-time%20Scenes%0AAuthor%3A%20Yuan-Kang%20Lee%20and%20Kuan-Lin%20Chen%20and%20Chia-Che%20Chang%20and%20Yu-Lun%20Liu%0AAbstract%3A%20Nighttime%20color%20constancy%20remains%20a%20challenging%20problem%20in%20computational%20photography%20due%20to%20low-light%20noise%20and%20complex%20illumination%20conditions.%20We%20present%20RL-AWB%2C%20a%20novel%20framework%20combining%20statistical%20methods%20with%20deep%20reinforcement%20learning%20for%20nighttime%20white%20balance.%20Our%20method%20begins%20with%20a%20statistical%20algorithm%20tailored%20for%20nighttime%20scenes%2C%20integrating%20salient%20gray%20pixel%20detection%20with%20novel%20illumination%20estimation.%20Building%20on%20this%20foundation%2C%20we%20develop%20the%20first%20deep%20reinforcement%20learning%20approach%20for%20color%20constancy%20that%20leverages%20the%20statistical%20algorithm%20as%20its%20core%2C%20mimicking%20professional%20AWB%20tuning%20experts%20by%20dynamically%20optimizing%20parameters%20for%20each%20image.%20To%20facilitate%20cross-sensor%20evaluation%2C%20we%20introduce%20the%20first%20multi-sensor%20nighttime%20dataset.%20Experiment%20results%20demonstrate%20that%20our%20method%20achieves%20superior%20generalization%20capability%20across%20low-light%20and%20well-illuminated%20images.%20Project%20page%3A%20https%3A//ntuneillee.github.io/research/rl-awb/%0ALink%3A%20http%3A//arxiv.org/abs/2601.05249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRL-AWB%253A%2520Deep%2520Reinforcement%2520Learning%2520for%2520Auto%2520White%2520Balance%2520Correction%2520in%2520Low-Light%2520Night-time%2520Scenes%26entry.906535625%3DYuan-Kang%2520Lee%2520and%2520Kuan-Lin%2520Chen%2520and%2520Chia-Che%2520Chang%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3DNighttime%2520color%2520constancy%2520remains%2520a%2520challenging%2520problem%2520in%2520computational%2520photography%2520due%2520to%2520low-light%2520noise%2520and%2520complex%2520illumination%2520conditions.%2520We%2520present%2520RL-AWB%252C%2520a%2520novel%2520framework%2520combining%2520statistical%2520methods%2520with%2520deep%2520reinforcement%2520learning%2520for%2520nighttime%2520white%2520balance.%2520Our%2520method%2520begins%2520with%2520a%2520statistical%2520algorithm%2520tailored%2520for%2520nighttime%2520scenes%252C%2520integrating%2520salient%2520gray%2520pixel%2520detection%2520with%2520novel%2520illumination%2520estimation.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520develop%2520the%2520first%2520deep%2520reinforcement%2520learning%2520approach%2520for%2520color%2520constancy%2520that%2520leverages%2520the%2520statistical%2520algorithm%2520as%2520its%2520core%252C%2520mimicking%2520professional%2520AWB%2520tuning%2520experts%2520by%2520dynamically%2520optimizing%2520parameters%2520for%2520each%2520image.%2520To%2520facilitate%2520cross-sensor%2520evaluation%252C%2520we%2520introduce%2520the%2520first%2520multi-sensor%2520nighttime%2520dataset.%2520Experiment%2520results%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520generalization%2520capability%2520across%2520low-light%2520and%2520well-illuminated%2520images.%2520Project%2520page%253A%2520https%253A//ntuneillee.github.io/research/rl-awb/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RL-AWB%3A%20Deep%20Reinforcement%20Learning%20for%20Auto%20White%20Balance%20Correction%20in%20Low-Light%20Night-time%20Scenes&entry.906535625=Yuan-Kang%20Lee%20and%20Kuan-Lin%20Chen%20and%20Chia-Che%20Chang%20and%20Yu-Lun%20Liu&entry.1292438233=Nighttime%20color%20constancy%20remains%20a%20challenging%20problem%20in%20computational%20photography%20due%20to%20low-light%20noise%20and%20complex%20illumination%20conditions.%20We%20present%20RL-AWB%2C%20a%20novel%20framework%20combining%20statistical%20methods%20with%20deep%20reinforcement%20learning%20for%20nighttime%20white%20balance.%20Our%20method%20begins%20with%20a%20statistical%20algorithm%20tailored%20for%20nighttime%20scenes%2C%20integrating%20salient%20gray%20pixel%20detection%20with%20novel%20illumination%20estimation.%20Building%20on%20this%20foundation%2C%20we%20develop%20the%20first%20deep%20reinforcement%20learning%20approach%20for%20color%20constancy%20that%20leverages%20the%20statistical%20algorithm%20as%20its%20core%2C%20mimicking%20professional%20AWB%20tuning%20experts%20by%20dynamically%20optimizing%20parameters%20for%20each%20image.%20To%20facilitate%20cross-sensor%20evaluation%2C%20we%20introduce%20the%20first%20multi-sensor%20nighttime%20dataset.%20Experiment%20results%20demonstrate%20that%20our%20method%20achieves%20superior%20generalization%20capability%20across%20low-light%20and%20well-illuminated%20images.%20Project%20page%3A%20https%3A//ntuneillee.github.io/research/rl-awb/&entry.1838667208=http%3A//arxiv.org/abs/2601.05249v1&entry.124074799=Read"},
{"title": "Large language models can effectively convince people to believe conspiracies", "author": "Thomas H. Costello and Kellin Pelrine and Matthew Kowal and Antonio A. Arechar and Jean-Fran\u00e7ois Godbout and Adam Gleave and David Rand and Gordon Pennycook", "abstract": "Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against (\"debunking\") or for (\"bunking\") that conspiracy. When using a \"jailbroken\" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.", "link": "http://arxiv.org/abs/2601.05050v1", "date": "2026-01-08", "relevancy": 1.9694, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3966}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3966}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20language%20models%20can%20effectively%20convince%20people%20to%20believe%20conspiracies&body=Title%3A%20Large%20language%20models%20can%20effectively%20convince%20people%20to%20believe%20conspiracies%0AAuthor%3A%20Thomas%20H.%20Costello%20and%20Kellin%20Pelrine%20and%20Matthew%20Kowal%20and%20Antonio%20A.%20Arechar%20and%20Jean-Fran%C3%A7ois%20Godbout%20and%20Adam%20Gleave%20and%20David%20Rand%20and%20Gordon%20Pennycook%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20been%20shown%20to%20be%20persuasive%20across%20a%20variety%20of%20context.%20But%20it%20remains%20unclear%20whether%20this%20persuasive%20power%20advantages%20truth%20over%20falsehood%2C%20or%20if%20LLMs%20can%20promote%20misbeliefs%20just%20as%20easily%20as%20refuting%20them.%20Here%2C%20we%20investigate%20this%20question%20across%20three%20pre-registered%20experiments%20in%20which%20participants%20%28N%20%3D%202%2C724%20Americans%29%20discussed%20a%20conspiracy%20theory%20they%20were%20uncertain%20about%20with%20GPT-4o%2C%20and%20the%20model%20was%20instructed%20to%20either%20argue%20against%20%28%22debunking%22%29%20or%20for%20%28%22bunking%22%29%20that%20conspiracy.%20When%20using%20a%20%22jailbroken%22%20GPT-4o%20variant%20with%20guardrails%20removed%2C%20the%20AI%20was%20as%20effective%20at%20increasing%20conspiracy%20belief%20as%20decreasing%20it.%20Concerningly%2C%20the%20bunking%20AI%20was%20rated%20more%20positively%2C%20and%20increased%20trust%20in%20AI%2C%20more%20than%20the%20debunking%20AI.%20Surprisingly%2C%20we%20found%20that%20using%20standard%20GPT-4o%20produced%20very%20similar%20effects%2C%20such%20that%20the%20guardrails%20imposed%20by%20OpenAI%20did%20little%20to%20revent%20the%20LLM%20from%20promoting%20conspiracy%20beliefs.%20Encouragingly%2C%20however%2C%20a%20corrective%20conversation%20reversed%20these%20newly%20induced%20conspiracy%20beliefs%2C%20and%20simply%20prompting%20GPT-4o%20to%20only%20use%20accurate%20information%20dramatically%20reduced%20its%20ability%20to%20increase%20conspiracy%20beliefs.%20Our%20findings%20demonstrate%20that%20LLMs%20possess%20potent%20abilities%20to%20promote%20both%20truth%20and%20falsehood%2C%20but%20that%20potential%20solutions%20may%20exist%20to%20help%20mitigate%20this%20risk.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520language%2520models%2520can%2520effectively%2520convince%2520people%2520to%2520believe%2520conspiracies%26entry.906535625%3DThomas%2520H.%2520Costello%2520and%2520Kellin%2520Pelrine%2520and%2520Matthew%2520Kowal%2520and%2520Antonio%2520A.%2520Arechar%2520and%2520Jean-Fran%25C3%25A7ois%2520Godbout%2520and%2520Adam%2520Gleave%2520and%2520David%2520Rand%2520and%2520Gordon%2520Pennycook%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520shown%2520to%2520be%2520persuasive%2520across%2520a%2520variety%2520of%2520context.%2520But%2520it%2520remains%2520unclear%2520whether%2520this%2520persuasive%2520power%2520advantages%2520truth%2520over%2520falsehood%252C%2520or%2520if%2520LLMs%2520can%2520promote%2520misbeliefs%2520just%2520as%2520easily%2520as%2520refuting%2520them.%2520Here%252C%2520we%2520investigate%2520this%2520question%2520across%2520three%2520pre-registered%2520experiments%2520in%2520which%2520participants%2520%2528N%2520%253D%25202%252C724%2520Americans%2529%2520discussed%2520a%2520conspiracy%2520theory%2520they%2520were%2520uncertain%2520about%2520with%2520GPT-4o%252C%2520and%2520the%2520model%2520was%2520instructed%2520to%2520either%2520argue%2520against%2520%2528%2522debunking%2522%2529%2520or%2520for%2520%2528%2522bunking%2522%2529%2520that%2520conspiracy.%2520When%2520using%2520a%2520%2522jailbroken%2522%2520GPT-4o%2520variant%2520with%2520guardrails%2520removed%252C%2520the%2520AI%2520was%2520as%2520effective%2520at%2520increasing%2520conspiracy%2520belief%2520as%2520decreasing%2520it.%2520Concerningly%252C%2520the%2520bunking%2520AI%2520was%2520rated%2520more%2520positively%252C%2520and%2520increased%2520trust%2520in%2520AI%252C%2520more%2520than%2520the%2520debunking%2520AI.%2520Surprisingly%252C%2520we%2520found%2520that%2520using%2520standard%2520GPT-4o%2520produced%2520very%2520similar%2520effects%252C%2520such%2520that%2520the%2520guardrails%2520imposed%2520by%2520OpenAI%2520did%2520little%2520to%2520revent%2520the%2520LLM%2520from%2520promoting%2520conspiracy%2520beliefs.%2520Encouragingly%252C%2520however%252C%2520a%2520corrective%2520conversation%2520reversed%2520these%2520newly%2520induced%2520conspiracy%2520beliefs%252C%2520and%2520simply%2520prompting%2520GPT-4o%2520to%2520only%2520use%2520accurate%2520information%2520dramatically%2520reduced%2520its%2520ability%2520to%2520increase%2520conspiracy%2520beliefs.%2520Our%2520findings%2520demonstrate%2520that%2520LLMs%2520possess%2520potent%2520abilities%2520to%2520promote%2520both%2520truth%2520and%2520falsehood%252C%2520but%2520that%2520potential%2520solutions%2520may%2520exist%2520to%2520help%2520mitigate%2520this%2520risk.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20language%20models%20can%20effectively%20convince%20people%20to%20believe%20conspiracies&entry.906535625=Thomas%20H.%20Costello%20and%20Kellin%20Pelrine%20and%20Matthew%20Kowal%20and%20Antonio%20A.%20Arechar%20and%20Jean-Fran%C3%A7ois%20Godbout%20and%20Adam%20Gleave%20and%20David%20Rand%20and%20Gordon%20Pennycook&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20been%20shown%20to%20be%20persuasive%20across%20a%20variety%20of%20context.%20But%20it%20remains%20unclear%20whether%20this%20persuasive%20power%20advantages%20truth%20over%20falsehood%2C%20or%20if%20LLMs%20can%20promote%20misbeliefs%20just%20as%20easily%20as%20refuting%20them.%20Here%2C%20we%20investigate%20this%20question%20across%20three%20pre-registered%20experiments%20in%20which%20participants%20%28N%20%3D%202%2C724%20Americans%29%20discussed%20a%20conspiracy%20theory%20they%20were%20uncertain%20about%20with%20GPT-4o%2C%20and%20the%20model%20was%20instructed%20to%20either%20argue%20against%20%28%22debunking%22%29%20or%20for%20%28%22bunking%22%29%20that%20conspiracy.%20When%20using%20a%20%22jailbroken%22%20GPT-4o%20variant%20with%20guardrails%20removed%2C%20the%20AI%20was%20as%20effective%20at%20increasing%20conspiracy%20belief%20as%20decreasing%20it.%20Concerningly%2C%20the%20bunking%20AI%20was%20rated%20more%20positively%2C%20and%20increased%20trust%20in%20AI%2C%20more%20than%20the%20debunking%20AI.%20Surprisingly%2C%20we%20found%20that%20using%20standard%20GPT-4o%20produced%20very%20similar%20effects%2C%20such%20that%20the%20guardrails%20imposed%20by%20OpenAI%20did%20little%20to%20revent%20the%20LLM%20from%20promoting%20conspiracy%20beliefs.%20Encouragingly%2C%20however%2C%20a%20corrective%20conversation%20reversed%20these%20newly%20induced%20conspiracy%20beliefs%2C%20and%20simply%20prompting%20GPT-4o%20to%20only%20use%20accurate%20information%20dramatically%20reduced%20its%20ability%20to%20increase%20conspiracy%20beliefs.%20Our%20findings%20demonstrate%20that%20LLMs%20possess%20potent%20abilities%20to%20promote%20both%20truth%20and%20falsehood%2C%20but%20that%20potential%20solutions%20may%20exist%20to%20help%20mitigate%20this%20risk.&entry.1838667208=http%3A//arxiv.org/abs/2601.05050v1&entry.124074799=Read"},
{"title": "EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI", "author": "Zain Iqbal and Lorenzo Valerio", "abstract": "Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.", "link": "http://arxiv.org/abs/2601.05205v1", "date": "2026-01-08", "relevancy": 1.4339, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5017}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4817}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EARL%3A%20Energy-Aware%20Optimization%20of%20Liquid%20State%20Machines%20for%20Pervasive%20AI&body=Title%3A%20EARL%3A%20Energy-Aware%20Optimization%20of%20Liquid%20State%20Machines%20for%20Pervasive%20AI%0AAuthor%3A%20Zain%20Iqbal%20and%20Lorenzo%20Valerio%0AAbstract%3A%20Pervasive%20AI%20increasingly%20depends%20on%20on-device%20learning%20systems%20that%20deliver%20low-latency%20and%20energy-efficient%20computation%20under%20strict%20resource%20constraints.%20Liquid%20State%20Machines%20%28LSMs%29%20offer%20a%20promising%20approach%20for%20low-power%20temporal%20processing%20in%20pervasive%20and%20neuromorphic%20systems%2C%20but%20their%20deployment%20remains%20challenging%20due%20to%20high%20hyperparameter%20sensitivity%20and%20the%20computational%20cost%20of%20traditional%20optimization%20methods%20that%20ignore%20energy%20constraints.%20This%20work%20presents%20EARL%2C%20an%20energy-aware%20reinforcement%20learning%20framework%20that%20integrates%20Bayesian%20optimization%20with%20an%20adaptive%20reinforcement%20learning%20based%20selection%20policy%20to%20jointly%20optimize%20accuracy%20and%20energy%20consumption.%20EARL%20employs%20surrogate%20modeling%20for%20global%20exploration%2C%20reinforcement%20learning%20for%20dynamic%20candidate%20prioritization%2C%20and%20an%20early%20termination%20mechanism%20to%20eliminate%20redundant%20evaluations%2C%20substantially%20reducing%20computational%20overhead.%20Experiments%20on%20three%20benchmark%20datasets%20demonstrate%20that%20EARL%20achieves%206%20to%2015%20percent%20higher%20accuracy%2C%2060%20to%2080%20percent%20lower%20energy%20consumption%2C%20and%20up%20to%20an%20order%20of%20magnitude%20reduction%20in%20optimization%20time%20compared%20to%20leading%20hyperparameter%20tuning%20frameworks.%20These%20results%20highlight%20the%20effectiveness%20of%20energy-aware%20adaptive%20search%20in%20improving%20the%20efficiency%20and%20scalability%20of%20LSMs%20for%20resource-constrained%20on-device%20AI%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEARL%253A%2520Energy-Aware%2520Optimization%2520of%2520Liquid%2520State%2520Machines%2520for%2520Pervasive%2520AI%26entry.906535625%3DZain%2520Iqbal%2520and%2520Lorenzo%2520Valerio%26entry.1292438233%3DPervasive%2520AI%2520increasingly%2520depends%2520on%2520on-device%2520learning%2520systems%2520that%2520deliver%2520low-latency%2520and%2520energy-efficient%2520computation%2520under%2520strict%2520resource%2520constraints.%2520Liquid%2520State%2520Machines%2520%2528LSMs%2529%2520offer%2520a%2520promising%2520approach%2520for%2520low-power%2520temporal%2520processing%2520in%2520pervasive%2520and%2520neuromorphic%2520systems%252C%2520but%2520their%2520deployment%2520remains%2520challenging%2520due%2520to%2520high%2520hyperparameter%2520sensitivity%2520and%2520the%2520computational%2520cost%2520of%2520traditional%2520optimization%2520methods%2520that%2520ignore%2520energy%2520constraints.%2520This%2520work%2520presents%2520EARL%252C%2520an%2520energy-aware%2520reinforcement%2520learning%2520framework%2520that%2520integrates%2520Bayesian%2520optimization%2520with%2520an%2520adaptive%2520reinforcement%2520learning%2520based%2520selection%2520policy%2520to%2520jointly%2520optimize%2520accuracy%2520and%2520energy%2520consumption.%2520EARL%2520employs%2520surrogate%2520modeling%2520for%2520global%2520exploration%252C%2520reinforcement%2520learning%2520for%2520dynamic%2520candidate%2520prioritization%252C%2520and%2520an%2520early%2520termination%2520mechanism%2520to%2520eliminate%2520redundant%2520evaluations%252C%2520substantially%2520reducing%2520computational%2520overhead.%2520Experiments%2520on%2520three%2520benchmark%2520datasets%2520demonstrate%2520that%2520EARL%2520achieves%25206%2520to%252015%2520percent%2520higher%2520accuracy%252C%252060%2520to%252080%2520percent%2520lower%2520energy%2520consumption%252C%2520and%2520up%2520to%2520an%2520order%2520of%2520magnitude%2520reduction%2520in%2520optimization%2520time%2520compared%2520to%2520leading%2520hyperparameter%2520tuning%2520frameworks.%2520These%2520results%2520highlight%2520the%2520effectiveness%2520of%2520energy-aware%2520adaptive%2520search%2520in%2520improving%2520the%2520efficiency%2520and%2520scalability%2520of%2520LSMs%2520for%2520resource-constrained%2520on-device%2520AI%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EARL%3A%20Energy-Aware%20Optimization%20of%20Liquid%20State%20Machines%20for%20Pervasive%20AI&entry.906535625=Zain%20Iqbal%20and%20Lorenzo%20Valerio&entry.1292438233=Pervasive%20AI%20increasingly%20depends%20on%20on-device%20learning%20systems%20that%20deliver%20low-latency%20and%20energy-efficient%20computation%20under%20strict%20resource%20constraints.%20Liquid%20State%20Machines%20%28LSMs%29%20offer%20a%20promising%20approach%20for%20low-power%20temporal%20processing%20in%20pervasive%20and%20neuromorphic%20systems%2C%20but%20their%20deployment%20remains%20challenging%20due%20to%20high%20hyperparameter%20sensitivity%20and%20the%20computational%20cost%20of%20traditional%20optimization%20methods%20that%20ignore%20energy%20constraints.%20This%20work%20presents%20EARL%2C%20an%20energy-aware%20reinforcement%20learning%20framework%20that%20integrates%20Bayesian%20optimization%20with%20an%20adaptive%20reinforcement%20learning%20based%20selection%20policy%20to%20jointly%20optimize%20accuracy%20and%20energy%20consumption.%20EARL%20employs%20surrogate%20modeling%20for%20global%20exploration%2C%20reinforcement%20learning%20for%20dynamic%20candidate%20prioritization%2C%20and%20an%20early%20termination%20mechanism%20to%20eliminate%20redundant%20evaluations%2C%20substantially%20reducing%20computational%20overhead.%20Experiments%20on%20three%20benchmark%20datasets%20demonstrate%20that%20EARL%20achieves%206%20to%2015%20percent%20higher%20accuracy%2C%2060%20to%2080%20percent%20lower%20energy%20consumption%2C%20and%20up%20to%20an%20order%20of%20magnitude%20reduction%20in%20optimization%20time%20compared%20to%20leading%20hyperparameter%20tuning%20frameworks.%20These%20results%20highlight%20the%20effectiveness%20of%20energy-aware%20adaptive%20search%20in%20improving%20the%20efficiency%20and%20scalability%20of%20LSMs%20for%20resource-constrained%20on-device%20AI%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2601.05205v1&entry.124074799=Read"},
{"title": "Measuring and Fostering Peace through Machine Learning and Artificial Intelligence", "author": "P. Gilda and P. Dungarwal and A. Thongkham and E. T. Ajayi and S. Choudhary and T. M. Terol and C. Lam and J. P. Araujo and M. McFadyen-Mungalln and L. S. Liebovitch and P. T. Coleman and H. West and K. Sieck and S. Carter", "abstract": "We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.", "link": "http://arxiv.org/abs/2601.05232v1", "date": "2026-01-08", "relevancy": 1.3122, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4405}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4365}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20and%20Fostering%20Peace%20through%20Machine%20Learning%20and%20Artificial%20Intelligence&body=Title%3A%20Measuring%20and%20Fostering%20Peace%20through%20Machine%20Learning%20and%20Artificial%20Intelligence%0AAuthor%3A%20P.%20Gilda%20and%20P.%20Dungarwal%20and%20A.%20Thongkham%20and%20E.%20T.%20Ajayi%20and%20S.%20Choudhary%20and%20T.%20M.%20Terol%20and%20C.%20Lam%20and%20J.%20P.%20Araujo%20and%20M.%20McFadyen-Mungalln%20and%20L.%20S.%20Liebovitch%20and%20P.%20T.%20Coleman%20and%20H.%20West%20and%20K.%20Sieck%20and%20S.%20Carter%0AAbstract%3A%20We%20used%20machine%20learning%20and%20artificial%20intelligence%3A%201%29%20to%20measure%20levels%20of%20peace%20in%20countries%20from%20news%20and%20social%20media%20and%202%29%20to%20develop%20on-line%20tools%20that%20promote%20peace%20by%20helping%20users%20better%20understand%20their%20own%20media%20diet.%20For%20news%20media%2C%20we%20used%20neural%20networks%20to%20measure%20levels%20of%20peace%20from%20text%20embeddings%20of%20on-line%20news%20sources.%20The%20model%2C%20trained%20on%20one%20news%20media%20dataset%20also%20showed%20high%20accuracy%20when%20used%20to%20analyze%20a%20different%20news%20dataset.%20For%20social%20media%2C%20such%20as%20YouTube%2C%20we%20developed%20other%20models%20to%20measure%20levels%20of%20social%20dimensions%20important%20in%20peace%20using%20word%20level%20%28GoEmotions%29%20and%20context%20level%20%28Large%20Language%20Model%29%20methods.%20To%20promote%20peace%2C%20we%20note%20that%2071%25%20of%20people%2020-40%20years%20old%20daily%20view%20most%20of%20their%20news%20through%20short%20videos%20on%20social%20media.%20Content%20creators%20of%20these%20videos%20are%20biased%20towards%20creating%20videos%20with%20emotional%20activation%2C%20making%20you%20angry%20to%20engage%20you%2C%20to%20increase%20clicks.%20We%20developed%20and%20tested%20a%20Chrome%20extension%2C%20MirrorMirror%2C%20which%20provides%20real-time%20feedback%20to%20YouTube%20viewers%20about%20the%20peacefulness%20of%20the%20media%20they%20are%20watching.%20Our%20long%20term%20goal%20is%20for%20MirrorMirror%20to%20evolve%20into%20an%20open-source%20tool%20for%20content%20creators%2C%20journalists%2C%20researchers%2C%20platforms%2C%20and%20individual%20users%20to%20better%20understand%20the%20tone%20of%20their%20media%20creation%20and%20consumption%20and%20its%20effects%20on%20viewers.%20Moving%20beyond%20simple%20engagement%20metrics%2C%20we%20hope%20to%20encourage%20more%20respectful%2C%20nuanced%2C%20and%20informative%20communication.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520and%2520Fostering%2520Peace%2520through%2520Machine%2520Learning%2520and%2520Artificial%2520Intelligence%26entry.906535625%3DP.%2520Gilda%2520and%2520P.%2520Dungarwal%2520and%2520A.%2520Thongkham%2520and%2520E.%2520T.%2520Ajayi%2520and%2520S.%2520Choudhary%2520and%2520T.%2520M.%2520Terol%2520and%2520C.%2520Lam%2520and%2520J.%2520P.%2520Araujo%2520and%2520M.%2520McFadyen-Mungalln%2520and%2520L.%2520S.%2520Liebovitch%2520and%2520P.%2520T.%2520Coleman%2520and%2520H.%2520West%2520and%2520K.%2520Sieck%2520and%2520S.%2520Carter%26entry.1292438233%3DWe%2520used%2520machine%2520learning%2520and%2520artificial%2520intelligence%253A%25201%2529%2520to%2520measure%2520levels%2520of%2520peace%2520in%2520countries%2520from%2520news%2520and%2520social%2520media%2520and%25202%2529%2520to%2520develop%2520on-line%2520tools%2520that%2520promote%2520peace%2520by%2520helping%2520users%2520better%2520understand%2520their%2520own%2520media%2520diet.%2520For%2520news%2520media%252C%2520we%2520used%2520neural%2520networks%2520to%2520measure%2520levels%2520of%2520peace%2520from%2520text%2520embeddings%2520of%2520on-line%2520news%2520sources.%2520The%2520model%252C%2520trained%2520on%2520one%2520news%2520media%2520dataset%2520also%2520showed%2520high%2520accuracy%2520when%2520used%2520to%2520analyze%2520a%2520different%2520news%2520dataset.%2520For%2520social%2520media%252C%2520such%2520as%2520YouTube%252C%2520we%2520developed%2520other%2520models%2520to%2520measure%2520levels%2520of%2520social%2520dimensions%2520important%2520in%2520peace%2520using%2520word%2520level%2520%2528GoEmotions%2529%2520and%2520context%2520level%2520%2528Large%2520Language%2520Model%2529%2520methods.%2520To%2520promote%2520peace%252C%2520we%2520note%2520that%252071%2525%2520of%2520people%252020-40%2520years%2520old%2520daily%2520view%2520most%2520of%2520their%2520news%2520through%2520short%2520videos%2520on%2520social%2520media.%2520Content%2520creators%2520of%2520these%2520videos%2520are%2520biased%2520towards%2520creating%2520videos%2520with%2520emotional%2520activation%252C%2520making%2520you%2520angry%2520to%2520engage%2520you%252C%2520to%2520increase%2520clicks.%2520We%2520developed%2520and%2520tested%2520a%2520Chrome%2520extension%252C%2520MirrorMirror%252C%2520which%2520provides%2520real-time%2520feedback%2520to%2520YouTube%2520viewers%2520about%2520the%2520peacefulness%2520of%2520the%2520media%2520they%2520are%2520watching.%2520Our%2520long%2520term%2520goal%2520is%2520for%2520MirrorMirror%2520to%2520evolve%2520into%2520an%2520open-source%2520tool%2520for%2520content%2520creators%252C%2520journalists%252C%2520researchers%252C%2520platforms%252C%2520and%2520individual%2520users%2520to%2520better%2520understand%2520the%2520tone%2520of%2520their%2520media%2520creation%2520and%2520consumption%2520and%2520its%2520effects%2520on%2520viewers.%2520Moving%2520beyond%2520simple%2520engagement%2520metrics%252C%2520we%2520hope%2520to%2520encourage%2520more%2520respectful%252C%2520nuanced%252C%2520and%2520informative%2520communication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20and%20Fostering%20Peace%20through%20Machine%20Learning%20and%20Artificial%20Intelligence&entry.906535625=P.%20Gilda%20and%20P.%20Dungarwal%20and%20A.%20Thongkham%20and%20E.%20T.%20Ajayi%20and%20S.%20Choudhary%20and%20T.%20M.%20Terol%20and%20C.%20Lam%20and%20J.%20P.%20Araujo%20and%20M.%20McFadyen-Mungalln%20and%20L.%20S.%20Liebovitch%20and%20P.%20T.%20Coleman%20and%20H.%20West%20and%20K.%20Sieck%20and%20S.%20Carter&entry.1292438233=We%20used%20machine%20learning%20and%20artificial%20intelligence%3A%201%29%20to%20measure%20levels%20of%20peace%20in%20countries%20from%20news%20and%20social%20media%20and%202%29%20to%20develop%20on-line%20tools%20that%20promote%20peace%20by%20helping%20users%20better%20understand%20their%20own%20media%20diet.%20For%20news%20media%2C%20we%20used%20neural%20networks%20to%20measure%20levels%20of%20peace%20from%20text%20embeddings%20of%20on-line%20news%20sources.%20The%20model%2C%20trained%20on%20one%20news%20media%20dataset%20also%20showed%20high%20accuracy%20when%20used%20to%20analyze%20a%20different%20news%20dataset.%20For%20social%20media%2C%20such%20as%20YouTube%2C%20we%20developed%20other%20models%20to%20measure%20levels%20of%20social%20dimensions%20important%20in%20peace%20using%20word%20level%20%28GoEmotions%29%20and%20context%20level%20%28Large%20Language%20Model%29%20methods.%20To%20promote%20peace%2C%20we%20note%20that%2071%25%20of%20people%2020-40%20years%20old%20daily%20view%20most%20of%20their%20news%20through%20short%20videos%20on%20social%20media.%20Content%20creators%20of%20these%20videos%20are%20biased%20towards%20creating%20videos%20with%20emotional%20activation%2C%20making%20you%20angry%20to%20engage%20you%2C%20to%20increase%20clicks.%20We%20developed%20and%20tested%20a%20Chrome%20extension%2C%20MirrorMirror%2C%20which%20provides%20real-time%20feedback%20to%20YouTube%20viewers%20about%20the%20peacefulness%20of%20the%20media%20they%20are%20watching.%20Our%20long%20term%20goal%20is%20for%20MirrorMirror%20to%20evolve%20into%20an%20open-source%20tool%20for%20content%20creators%2C%20journalists%2C%20researchers%2C%20platforms%2C%20and%20individual%20users%20to%20better%20understand%20the%20tone%20of%20their%20media%20creation%20and%20consumption%20and%20its%20effects%20on%20viewers.%20Moving%20beyond%20simple%20engagement%20metrics%2C%20we%20hope%20to%20encourage%20more%20respectful%2C%20nuanced%2C%20and%20informative%20communication.&entry.1838667208=http%3A//arxiv.org/abs/2601.05232v1&entry.124074799=Read"},
{"title": "Safe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art", "author": "Timofey Tomashevskiy", "abstract": "This work provides a state-of-the-art survey of continual safe online reinforcement learning (COSRL) methods. We discuss theoretical aspects, challenges, and open questions in building continual online safe reinforcement learning algorithms. We provide the taxonomy and the details of continual online safe reinforcement learning methods based on the type of safe learning mechanism that takes adaptation to nonstationarity into account. We categorize safety constraints formulation for online reinforcement learning algorithms, and finally, we discuss prospects for creating reliable, safe online learning algorithms.\n  Keywords: safe RL in nonstationary environments, safe continual reinforcement learning under nonstationarity, HM-MDP, NSMDP, POMDP, safe POMDP, constraints for continual learning, safe continual reinforcement learning review, safe continual reinforcement learning survey, safe continual reinforcement learning, safe online learning under distribution shift, safe continual online adaptation, safe reinforcement learning, safe exploration, safe adaptation, constrained Markov decision processes, safe reinforcement learning, partially observable Markov decision process, safe reinforcement learning and hidden Markov decision processes, Safe Online Reinforcement Learning, safe online reinforcement learning, safe online reinforcement learning, safe meta-learning, safe meta-reinforcement learning, safe context-based reinforcement learning, formulating safety constraints for continual learning", "link": "http://arxiv.org/abs/2601.05152v1", "date": "2026-01-08", "relevancy": 1.5457, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5247}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5126}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Continual%20Reinforcement%20Learning%20Methods%20for%20Nonstationary%20Environments.%20Towards%20a%20Survey%20of%20the%20State%20of%20the%20Art&body=Title%3A%20Safe%20Continual%20Reinforcement%20Learning%20Methods%20for%20Nonstationary%20Environments.%20Towards%20a%20Survey%20of%20the%20State%20of%20the%20Art%0AAuthor%3A%20Timofey%20Tomashevskiy%0AAbstract%3A%20This%20work%20provides%20a%20state-of-the-art%20survey%20of%20continual%20safe%20online%20reinforcement%20learning%20%28COSRL%29%20methods.%20We%20discuss%20theoretical%20aspects%2C%20challenges%2C%20and%20open%20questions%20in%20building%20continual%20online%20safe%20reinforcement%20learning%20algorithms.%20We%20provide%20the%20taxonomy%20and%20the%20details%20of%20continual%20online%20safe%20reinforcement%20learning%20methods%20based%20on%20the%20type%20of%20safe%20learning%20mechanism%20that%20takes%20adaptation%20to%20nonstationarity%20into%20account.%20We%20categorize%20safety%20constraints%20formulation%20for%20online%20reinforcement%20learning%20algorithms%2C%20and%20finally%2C%20we%20discuss%20prospects%20for%20creating%20reliable%2C%20safe%20online%20learning%20algorithms.%0A%20%20Keywords%3A%20safe%20RL%20in%20nonstationary%20environments%2C%20safe%20continual%20reinforcement%20learning%20under%20nonstationarity%2C%20HM-MDP%2C%20NSMDP%2C%20POMDP%2C%20safe%20POMDP%2C%20constraints%20for%20continual%20learning%2C%20safe%20continual%20reinforcement%20learning%20review%2C%20safe%20continual%20reinforcement%20learning%20survey%2C%20safe%20continual%20reinforcement%20learning%2C%20safe%20online%20learning%20under%20distribution%20shift%2C%20safe%20continual%20online%20adaptation%2C%20safe%20reinforcement%20learning%2C%20safe%20exploration%2C%20safe%20adaptation%2C%20constrained%20Markov%20decision%20processes%2C%20safe%20reinforcement%20learning%2C%20partially%20observable%20Markov%20decision%20process%2C%20safe%20reinforcement%20learning%20and%20hidden%20Markov%20decision%20processes%2C%20Safe%20Online%20Reinforcement%20Learning%2C%20safe%20online%20reinforcement%20learning%2C%20safe%20online%20reinforcement%20learning%2C%20safe%20meta-learning%2C%20safe%20meta-reinforcement%20learning%2C%20safe%20context-based%20reinforcement%20learning%2C%20formulating%20safety%20constraints%20for%20continual%20learning%0ALink%3A%20http%3A//arxiv.org/abs/2601.05152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Continual%2520Reinforcement%2520Learning%2520Methods%2520for%2520Nonstationary%2520Environments.%2520Towards%2520a%2520Survey%2520of%2520the%2520State%2520of%2520the%2520Art%26entry.906535625%3DTimofey%2520Tomashevskiy%26entry.1292438233%3DThis%2520work%2520provides%2520a%2520state-of-the-art%2520survey%2520of%2520continual%2520safe%2520online%2520reinforcement%2520learning%2520%2528COSRL%2529%2520methods.%2520We%2520discuss%2520theoretical%2520aspects%252C%2520challenges%252C%2520and%2520open%2520questions%2520in%2520building%2520continual%2520online%2520safe%2520reinforcement%2520learning%2520algorithms.%2520We%2520provide%2520the%2520taxonomy%2520and%2520the%2520details%2520of%2520continual%2520online%2520safe%2520reinforcement%2520learning%2520methods%2520based%2520on%2520the%2520type%2520of%2520safe%2520learning%2520mechanism%2520that%2520takes%2520adaptation%2520to%2520nonstationarity%2520into%2520account.%2520We%2520categorize%2520safety%2520constraints%2520formulation%2520for%2520online%2520reinforcement%2520learning%2520algorithms%252C%2520and%2520finally%252C%2520we%2520discuss%2520prospects%2520for%2520creating%2520reliable%252C%2520safe%2520online%2520learning%2520algorithms.%250A%2520%2520Keywords%253A%2520safe%2520RL%2520in%2520nonstationary%2520environments%252C%2520safe%2520continual%2520reinforcement%2520learning%2520under%2520nonstationarity%252C%2520HM-MDP%252C%2520NSMDP%252C%2520POMDP%252C%2520safe%2520POMDP%252C%2520constraints%2520for%2520continual%2520learning%252C%2520safe%2520continual%2520reinforcement%2520learning%2520review%252C%2520safe%2520continual%2520reinforcement%2520learning%2520survey%252C%2520safe%2520continual%2520reinforcement%2520learning%252C%2520safe%2520online%2520learning%2520under%2520distribution%2520shift%252C%2520safe%2520continual%2520online%2520adaptation%252C%2520safe%2520reinforcement%2520learning%252C%2520safe%2520exploration%252C%2520safe%2520adaptation%252C%2520constrained%2520Markov%2520decision%2520processes%252C%2520safe%2520reinforcement%2520learning%252C%2520partially%2520observable%2520Markov%2520decision%2520process%252C%2520safe%2520reinforcement%2520learning%2520and%2520hidden%2520Markov%2520decision%2520processes%252C%2520Safe%2520Online%2520Reinforcement%2520Learning%252C%2520safe%2520online%2520reinforcement%2520learning%252C%2520safe%2520online%2520reinforcement%2520learning%252C%2520safe%2520meta-learning%252C%2520safe%2520meta-reinforcement%2520learning%252C%2520safe%2520context-based%2520reinforcement%2520learning%252C%2520formulating%2520safety%2520constraints%2520for%2520continual%2520learning%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Continual%20Reinforcement%20Learning%20Methods%20for%20Nonstationary%20Environments.%20Towards%20a%20Survey%20of%20the%20State%20of%20the%20Art&entry.906535625=Timofey%20Tomashevskiy&entry.1292438233=This%20work%20provides%20a%20state-of-the-art%20survey%20of%20continual%20safe%20online%20reinforcement%20learning%20%28COSRL%29%20methods.%20We%20discuss%20theoretical%20aspects%2C%20challenges%2C%20and%20open%20questions%20in%20building%20continual%20online%20safe%20reinforcement%20learning%20algorithms.%20We%20provide%20the%20taxonomy%20and%20the%20details%20of%20continual%20online%20safe%20reinforcement%20learning%20methods%20based%20on%20the%20type%20of%20safe%20learning%20mechanism%20that%20takes%20adaptation%20to%20nonstationarity%20into%20account.%20We%20categorize%20safety%20constraints%20formulation%20for%20online%20reinforcement%20learning%20algorithms%2C%20and%20finally%2C%20we%20discuss%20prospects%20for%20creating%20reliable%2C%20safe%20online%20learning%20algorithms.%0A%20%20Keywords%3A%20safe%20RL%20in%20nonstationary%20environments%2C%20safe%20continual%20reinforcement%20learning%20under%20nonstationarity%2C%20HM-MDP%2C%20NSMDP%2C%20POMDP%2C%20safe%20POMDP%2C%20constraints%20for%20continual%20learning%2C%20safe%20continual%20reinforcement%20learning%20review%2C%20safe%20continual%20reinforcement%20learning%20survey%2C%20safe%20continual%20reinforcement%20learning%2C%20safe%20online%20learning%20under%20distribution%20shift%2C%20safe%20continual%20online%20adaptation%2C%20safe%20reinforcement%20learning%2C%20safe%20exploration%2C%20safe%20adaptation%2C%20constrained%20Markov%20decision%20processes%2C%20safe%20reinforcement%20learning%2C%20partially%20observable%20Markov%20decision%20process%2C%20safe%20reinforcement%20learning%20and%20hidden%20Markov%20decision%20processes%2C%20Safe%20Online%20Reinforcement%20Learning%2C%20safe%20online%20reinforcement%20learning%2C%20safe%20online%20reinforcement%20learning%2C%20safe%20meta-learning%2C%20safe%20meta-reinforcement%20learning%2C%20safe%20context-based%20reinforcement%20learning%2C%20formulating%20safety%20constraints%20for%20continual%20learning&entry.1838667208=http%3A//arxiv.org/abs/2601.05152v1&entry.124074799=Read"},
{"title": "Stock Market Price Prediction using Neural Prophet with Deep Neural Network", "author": "Navin Chhibber and Suneel Khemka and Navneet Kumar Tyagi and Rohit Tewari and Bireswar Banerjee and Piyush Ranjan", "abstract": "Stock market price prediction is a significant interdisciplinary research domain that depends at the intersection of finance, statistics, and economics. Forecasting Accurately predicting stock prices has always been a focal point for various researchers. However, existing statistical approaches for time-series prediction often fail to effectively forecast the probability range of future stock prices. Hence, to solve this problem, the Neural Prophet with a Deep Neural Network (NP-DNN) is proposed to predict stock market prices. The preprocessing technique used in this research is Z-score normalization, which normalizes stock price data by removing scale differences, making patterns easier to detect. Missing value imputation fills gaps in historical data, enhancing the models use of complete information for more accurate predictions. The Multi-Layer Perceptron (MLP) learns complex nonlinear relationships among stock market prices and extracts hidden patterns from the input data, thereby creating meaningful feature representations for better prediction accuracy. The proposed NP-DNN model achieved an accuracy of 99.21% compared with other approaches using the Fused Large Language Model. Keywords: deep neural network, forecasting stock prices, multi-layer perceptron, neural prophet, stock market price prediction.", "link": "http://arxiv.org/abs/2601.05202v1", "date": "2026-01-08", "relevancy": 1.4092, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5087}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4451}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stock%20Market%20Price%20Prediction%20using%20Neural%20Prophet%20with%20Deep%20Neural%20Network&body=Title%3A%20Stock%20Market%20Price%20Prediction%20using%20Neural%20Prophet%20with%20Deep%20Neural%20Network%0AAuthor%3A%20Navin%20Chhibber%20and%20Suneel%20Khemka%20and%20Navneet%20Kumar%20Tyagi%20and%20Rohit%20Tewari%20and%20Bireswar%20Banerjee%20and%20Piyush%20Ranjan%0AAbstract%3A%20Stock%20market%20price%20prediction%20is%20a%20significant%20interdisciplinary%20research%20domain%20that%20depends%20at%20the%20intersection%20of%20finance%2C%20statistics%2C%20and%20economics.%20Forecasting%20Accurately%20predicting%20stock%20prices%20has%20always%20been%20a%20focal%20point%20for%20various%20researchers.%20However%2C%20existing%20statistical%20approaches%20for%20time-series%20prediction%20often%20fail%20to%20effectively%20forecast%20the%20probability%20range%20of%20future%20stock%20prices.%20Hence%2C%20to%20solve%20this%20problem%2C%20the%20Neural%20Prophet%20with%20a%20Deep%20Neural%20Network%20%28NP-DNN%29%20is%20proposed%20to%20predict%20stock%20market%20prices.%20The%20preprocessing%20technique%20used%20in%20this%20research%20is%20Z-score%20normalization%2C%20which%20normalizes%20stock%20price%20data%20by%20removing%20scale%20differences%2C%20making%20patterns%20easier%20to%20detect.%20Missing%20value%20imputation%20fills%20gaps%20in%20historical%20data%2C%20enhancing%20the%20models%20use%20of%20complete%20information%20for%20more%20accurate%20predictions.%20The%20Multi-Layer%20Perceptron%20%28MLP%29%20learns%20complex%20nonlinear%20relationships%20among%20stock%20market%20prices%20and%20extracts%20hidden%20patterns%20from%20the%20input%20data%2C%20thereby%20creating%20meaningful%20feature%20representations%20for%20better%20prediction%20accuracy.%20The%20proposed%20NP-DNN%20model%20achieved%20an%20accuracy%20of%2099.21%25%20compared%20with%20other%20approaches%20using%20the%20Fused%20Large%20Language%20Model.%20Keywords%3A%20deep%20neural%20network%2C%20forecasting%20stock%20prices%2C%20multi-layer%20perceptron%2C%20neural%20prophet%2C%20stock%20market%20price%20prediction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStock%2520Market%2520Price%2520Prediction%2520using%2520Neural%2520Prophet%2520with%2520Deep%2520Neural%2520Network%26entry.906535625%3DNavin%2520Chhibber%2520and%2520Suneel%2520Khemka%2520and%2520Navneet%2520Kumar%2520Tyagi%2520and%2520Rohit%2520Tewari%2520and%2520Bireswar%2520Banerjee%2520and%2520Piyush%2520Ranjan%26entry.1292438233%3DStock%2520market%2520price%2520prediction%2520is%2520a%2520significant%2520interdisciplinary%2520research%2520domain%2520that%2520depends%2520at%2520the%2520intersection%2520of%2520finance%252C%2520statistics%252C%2520and%2520economics.%2520Forecasting%2520Accurately%2520predicting%2520stock%2520prices%2520has%2520always%2520been%2520a%2520focal%2520point%2520for%2520various%2520researchers.%2520However%252C%2520existing%2520statistical%2520approaches%2520for%2520time-series%2520prediction%2520often%2520fail%2520to%2520effectively%2520forecast%2520the%2520probability%2520range%2520of%2520future%2520stock%2520prices.%2520Hence%252C%2520to%2520solve%2520this%2520problem%252C%2520the%2520Neural%2520Prophet%2520with%2520a%2520Deep%2520Neural%2520Network%2520%2528NP-DNN%2529%2520is%2520proposed%2520to%2520predict%2520stock%2520market%2520prices.%2520The%2520preprocessing%2520technique%2520used%2520in%2520this%2520research%2520is%2520Z-score%2520normalization%252C%2520which%2520normalizes%2520stock%2520price%2520data%2520by%2520removing%2520scale%2520differences%252C%2520making%2520patterns%2520easier%2520to%2520detect.%2520Missing%2520value%2520imputation%2520fills%2520gaps%2520in%2520historical%2520data%252C%2520enhancing%2520the%2520models%2520use%2520of%2520complete%2520information%2520for%2520more%2520accurate%2520predictions.%2520The%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%2520learns%2520complex%2520nonlinear%2520relationships%2520among%2520stock%2520market%2520prices%2520and%2520extracts%2520hidden%2520patterns%2520from%2520the%2520input%2520data%252C%2520thereby%2520creating%2520meaningful%2520feature%2520representations%2520for%2520better%2520prediction%2520accuracy.%2520The%2520proposed%2520NP-DNN%2520model%2520achieved%2520an%2520accuracy%2520of%252099.21%2525%2520compared%2520with%2520other%2520approaches%2520using%2520the%2520Fused%2520Large%2520Language%2520Model.%2520Keywords%253A%2520deep%2520neural%2520network%252C%2520forecasting%2520stock%2520prices%252C%2520multi-layer%2520perceptron%252C%2520neural%2520prophet%252C%2520stock%2520market%2520price%2520prediction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stock%20Market%20Price%20Prediction%20using%20Neural%20Prophet%20with%20Deep%20Neural%20Network&entry.906535625=Navin%20Chhibber%20and%20Suneel%20Khemka%20and%20Navneet%20Kumar%20Tyagi%20and%20Rohit%20Tewari%20and%20Bireswar%20Banerjee%20and%20Piyush%20Ranjan&entry.1292438233=Stock%20market%20price%20prediction%20is%20a%20significant%20interdisciplinary%20research%20domain%20that%20depends%20at%20the%20intersection%20of%20finance%2C%20statistics%2C%20and%20economics.%20Forecasting%20Accurately%20predicting%20stock%20prices%20has%20always%20been%20a%20focal%20point%20for%20various%20researchers.%20However%2C%20existing%20statistical%20approaches%20for%20time-series%20prediction%20often%20fail%20to%20effectively%20forecast%20the%20probability%20range%20of%20future%20stock%20prices.%20Hence%2C%20to%20solve%20this%20problem%2C%20the%20Neural%20Prophet%20with%20a%20Deep%20Neural%20Network%20%28NP-DNN%29%20is%20proposed%20to%20predict%20stock%20market%20prices.%20The%20preprocessing%20technique%20used%20in%20this%20research%20is%20Z-score%20normalization%2C%20which%20normalizes%20stock%20price%20data%20by%20removing%20scale%20differences%2C%20making%20patterns%20easier%20to%20detect.%20Missing%20value%20imputation%20fills%20gaps%20in%20historical%20data%2C%20enhancing%20the%20models%20use%20of%20complete%20information%20for%20more%20accurate%20predictions.%20The%20Multi-Layer%20Perceptron%20%28MLP%29%20learns%20complex%20nonlinear%20relationships%20among%20stock%20market%20prices%20and%20extracts%20hidden%20patterns%20from%20the%20input%20data%2C%20thereby%20creating%20meaningful%20feature%20representations%20for%20better%20prediction%20accuracy.%20The%20proposed%20NP-DNN%20model%20achieved%20an%20accuracy%20of%2099.21%25%20compared%20with%20other%20approaches%20using%20the%20Fused%20Large%20Language%20Model.%20Keywords%3A%20deep%20neural%20network%2C%20forecasting%20stock%20prices%2C%20multi-layer%20perceptron%2C%20neural%20prophet%2C%20stock%20market%20price%20prediction.&entry.1838667208=http%3A//arxiv.org/abs/2601.05202v1&entry.124074799=Read"},
{"title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference", "author": "Denis Korzhenkov and Adil Karjauv and Animesh Karnewar and Mohsen Ghafoorian and Amirhossein Habibian", "abstract": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.", "link": "http://arxiv.org/abs/2601.04792v1", "date": "2026-01-08", "relevancy": 1.7801, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6075}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5991}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PyramidalWan%3A%20On%20Making%20Pretrained%20Video%20Model%20Pyramidal%20for%20Efficient%20Inference&body=Title%3A%20PyramidalWan%3A%20On%20Making%20Pretrained%20Video%20Model%20Pyramidal%20for%20Efficient%20Inference%0AAuthor%3A%20Denis%20Korzhenkov%20and%20Adil%20Karjauv%20and%20Animesh%20Karnewar%20and%20Mohsen%20Ghafoorian%20and%20Amirhossein%20Habibian%0AAbstract%3A%20Recently%20proposed%20pyramidal%20models%20decompose%20the%20conventional%20forward%20and%20backward%20diffusion%20processes%20into%20multiple%20stages%20operating%20at%20varying%20resolutions.%20These%20models%20handle%20inputs%20with%20higher%20noise%20levels%20at%20lower%20resolutions%2C%20while%20less%20noisy%20inputs%20are%20processed%20at%20higher%20resolutions.%20This%20hierarchical%20approach%20significantly%20reduces%20the%20computational%20cost%20of%20inference%20in%20multi-step%20denoising%20models.%20However%2C%20existing%20open-source%20pyramidal%20video%20models%20have%20been%20trained%20from%20scratch%20and%20tend%20to%20underperform%20compared%20to%20state-of-the-art%20systems%20in%20terms%20of%20visual%20plausibility.%20In%20this%20work%2C%20we%20present%20a%20pipeline%20that%20converts%20a%20pretrained%20diffusion%20model%20into%20a%20pyramidal%20one%20through%20low-cost%20finetuning%2C%20achieving%20this%20transformation%20without%20degradation%20in%20quality%20of%20output%20videos.%20Furthermore%2C%20we%20investigate%20and%20compare%20various%20strategies%20for%20step%20distillation%20within%20pyramidal%20models%2C%20aiming%20to%20further%20enhance%20the%20inference%20efficiency.%20Our%20results%20are%20available%20at%20https%3A//qualcomm-ai-research.github.io/PyramidalWan.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPyramidalWan%253A%2520On%2520Making%2520Pretrained%2520Video%2520Model%2520Pyramidal%2520for%2520Efficient%2520Inference%26entry.906535625%3DDenis%2520Korzhenkov%2520and%2520Adil%2520Karjauv%2520and%2520Animesh%2520Karnewar%2520and%2520Mohsen%2520Ghafoorian%2520and%2520Amirhossein%2520Habibian%26entry.1292438233%3DRecently%2520proposed%2520pyramidal%2520models%2520decompose%2520the%2520conventional%2520forward%2520and%2520backward%2520diffusion%2520processes%2520into%2520multiple%2520stages%2520operating%2520at%2520varying%2520resolutions.%2520These%2520models%2520handle%2520inputs%2520with%2520higher%2520noise%2520levels%2520at%2520lower%2520resolutions%252C%2520while%2520less%2520noisy%2520inputs%2520are%2520processed%2520at%2520higher%2520resolutions.%2520This%2520hierarchical%2520approach%2520significantly%2520reduces%2520the%2520computational%2520cost%2520of%2520inference%2520in%2520multi-step%2520denoising%2520models.%2520However%252C%2520existing%2520open-source%2520pyramidal%2520video%2520models%2520have%2520been%2520trained%2520from%2520scratch%2520and%2520tend%2520to%2520underperform%2520compared%2520to%2520state-of-the-art%2520systems%2520in%2520terms%2520of%2520visual%2520plausibility.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520pipeline%2520that%2520converts%2520a%2520pretrained%2520diffusion%2520model%2520into%2520a%2520pyramidal%2520one%2520through%2520low-cost%2520finetuning%252C%2520achieving%2520this%2520transformation%2520without%2520degradation%2520in%2520quality%2520of%2520output%2520videos.%2520Furthermore%252C%2520we%2520investigate%2520and%2520compare%2520various%2520strategies%2520for%2520step%2520distillation%2520within%2520pyramidal%2520models%252C%2520aiming%2520to%2520further%2520enhance%2520the%2520inference%2520efficiency.%2520Our%2520results%2520are%2520available%2520at%2520https%253A//qualcomm-ai-research.github.io/PyramidalWan.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PyramidalWan%3A%20On%20Making%20Pretrained%20Video%20Model%20Pyramidal%20for%20Efficient%20Inference&entry.906535625=Denis%20Korzhenkov%20and%20Adil%20Karjauv%20and%20Animesh%20Karnewar%20and%20Mohsen%20Ghafoorian%20and%20Amirhossein%20Habibian&entry.1292438233=Recently%20proposed%20pyramidal%20models%20decompose%20the%20conventional%20forward%20and%20backward%20diffusion%20processes%20into%20multiple%20stages%20operating%20at%20varying%20resolutions.%20These%20models%20handle%20inputs%20with%20higher%20noise%20levels%20at%20lower%20resolutions%2C%20while%20less%20noisy%20inputs%20are%20processed%20at%20higher%20resolutions.%20This%20hierarchical%20approach%20significantly%20reduces%20the%20computational%20cost%20of%20inference%20in%20multi-step%20denoising%20models.%20However%2C%20existing%20open-source%20pyramidal%20video%20models%20have%20been%20trained%20from%20scratch%20and%20tend%20to%20underperform%20compared%20to%20state-of-the-art%20systems%20in%20terms%20of%20visual%20plausibility.%20In%20this%20work%2C%20we%20present%20a%20pipeline%20that%20converts%20a%20pretrained%20diffusion%20model%20into%20a%20pyramidal%20one%20through%20low-cost%20finetuning%2C%20achieving%20this%20transformation%20without%20degradation%20in%20quality%20of%20output%20videos.%20Furthermore%2C%20we%20investigate%20and%20compare%20various%20strategies%20for%20step%20distillation%20within%20pyramidal%20models%2C%20aiming%20to%20further%20enhance%20the%20inference%20efficiency.%20Our%20results%20are%20available%20at%20https%3A//qualcomm-ai-research.github.io/PyramidalWan.&entry.1838667208=http%3A//arxiv.org/abs/2601.04792v1&entry.124074799=Read"},
{"title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents", "author": "Tongyu Wen and Guanting Dong and Zhicheng Dou", "abstract": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.", "link": "http://arxiv.org/abs/2601.04888v1", "date": "2026-01-08", "relevancy": 1.9477, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SmartSearch%3A%20Process%20Reward-Guided%20Query%20Refinement%20for%20Search%20Agents&body=Title%3A%20SmartSearch%3A%20Process%20Reward-Guided%20Query%20Refinement%20for%20Search%20Agents%0AAuthor%3A%20Tongyu%20Wen%20and%20Guanting%20Dong%20and%20Zhicheng%20Dou%0AAbstract%3A%20Large%20language%20model%20%28LLM%29-based%20search%20agents%20have%20proven%20promising%20for%20addressing%20knowledge-intensive%20problems%20by%20incorporating%20information%20retrieval%20capabilities.%20Existing%20works%20largely%20focus%20on%20optimizing%20the%20reasoning%20paradigms%20of%20search%20agents%2C%20yet%20the%20quality%20of%20intermediate%20search%20queries%20during%20reasoning%20remains%20overlooked.%20As%20a%20result%2C%20the%20generated%20queries%20often%20remain%20inaccurate%2C%20leading%20to%20unexpected%20retrieval%20results%20and%20ultimately%20limiting%20search%20agents%27%20overall%20effectiveness.%20To%20mitigate%20this%20issue%2C%20we%20introduce%20SmartSearch%2C%20a%20framework%20built%20upon%20two%20key%20mechanisms%3A%20%281%29%20Process%20rewards%2C%20which%20provide%20fine-grained%20supervision%20for%20the%20quality%20of%20each%20intermediate%20search%20query%20through%20Dual-Level%20Credit%20Assessment.%20%282%29%20Query%20refinement%2C%20which%20promotes%20the%20optimization%20of%20query%20generation%20by%20selectively%20refining%20low-quality%20search%20queries%20and%20regenerating%20subsequent%20search%20rounds%20based%20on%20these%20refinements.%20To%20enable%20the%20search%20agent%20to%20progressively%20internalize%20the%20ability%20to%20improve%20query%20quality%20under%20the%20guidance%20of%20process%20rewards%2C%20we%20design%20a%20three-stage%20curriculum%20learning%20framework.%20This%20framework%20guides%20the%20agent%20through%20a%20progression%20from%20imitation%2C%20to%20alignment%2C%20and%20ultimately%20to%20generalization.%20Experimental%20results%20show%20that%20SmartSearch%20consistently%20surpasses%20existing%20baselines%2C%20and%20additional%20quantitative%20analyses%20further%20confirm%20its%20significant%20gains%20in%20both%20search%20efficiency%20and%20query%20quality.%20The%20code%20is%20available%20at%20https%3A//github.com/MYVAE/SmartSearch.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmartSearch%253A%2520Process%2520Reward-Guided%2520Query%2520Refinement%2520for%2520Search%2520Agents%26entry.906535625%3DTongyu%2520Wen%2520and%2520Guanting%2520Dong%2520and%2520Zhicheng%2520Dou%26entry.1292438233%3DLarge%2520language%2520model%2520%2528LLM%2529-based%2520search%2520agents%2520have%2520proven%2520promising%2520for%2520addressing%2520knowledge-intensive%2520problems%2520by%2520incorporating%2520information%2520retrieval%2520capabilities.%2520Existing%2520works%2520largely%2520focus%2520on%2520optimizing%2520the%2520reasoning%2520paradigms%2520of%2520search%2520agents%252C%2520yet%2520the%2520quality%2520of%2520intermediate%2520search%2520queries%2520during%2520reasoning%2520remains%2520overlooked.%2520As%2520a%2520result%252C%2520the%2520generated%2520queries%2520often%2520remain%2520inaccurate%252C%2520leading%2520to%2520unexpected%2520retrieval%2520results%2520and%2520ultimately%2520limiting%2520search%2520agents%2527%2520overall%2520effectiveness.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520introduce%2520SmartSearch%252C%2520a%2520framework%2520built%2520upon%2520two%2520key%2520mechanisms%253A%2520%25281%2529%2520Process%2520rewards%252C%2520which%2520provide%2520fine-grained%2520supervision%2520for%2520the%2520quality%2520of%2520each%2520intermediate%2520search%2520query%2520through%2520Dual-Level%2520Credit%2520Assessment.%2520%25282%2529%2520Query%2520refinement%252C%2520which%2520promotes%2520the%2520optimization%2520of%2520query%2520generation%2520by%2520selectively%2520refining%2520low-quality%2520search%2520queries%2520and%2520regenerating%2520subsequent%2520search%2520rounds%2520based%2520on%2520these%2520refinements.%2520To%2520enable%2520the%2520search%2520agent%2520to%2520progressively%2520internalize%2520the%2520ability%2520to%2520improve%2520query%2520quality%2520under%2520the%2520guidance%2520of%2520process%2520rewards%252C%2520we%2520design%2520a%2520three-stage%2520curriculum%2520learning%2520framework.%2520This%2520framework%2520guides%2520the%2520agent%2520through%2520a%2520progression%2520from%2520imitation%252C%2520to%2520alignment%252C%2520and%2520ultimately%2520to%2520generalization.%2520Experimental%2520results%2520show%2520that%2520SmartSearch%2520consistently%2520surpasses%2520existing%2520baselines%252C%2520and%2520additional%2520quantitative%2520analyses%2520further%2520confirm%2520its%2520significant%2520gains%2520in%2520both%2520search%2520efficiency%2520and%2520query%2520quality.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/MYVAE/SmartSearch.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SmartSearch%3A%20Process%20Reward-Guided%20Query%20Refinement%20for%20Search%20Agents&entry.906535625=Tongyu%20Wen%20and%20Guanting%20Dong%20and%20Zhicheng%20Dou&entry.1292438233=Large%20language%20model%20%28LLM%29-based%20search%20agents%20have%20proven%20promising%20for%20addressing%20knowledge-intensive%20problems%20by%20incorporating%20information%20retrieval%20capabilities.%20Existing%20works%20largely%20focus%20on%20optimizing%20the%20reasoning%20paradigms%20of%20search%20agents%2C%20yet%20the%20quality%20of%20intermediate%20search%20queries%20during%20reasoning%20remains%20overlooked.%20As%20a%20result%2C%20the%20generated%20queries%20often%20remain%20inaccurate%2C%20leading%20to%20unexpected%20retrieval%20results%20and%20ultimately%20limiting%20search%20agents%27%20overall%20effectiveness.%20To%20mitigate%20this%20issue%2C%20we%20introduce%20SmartSearch%2C%20a%20framework%20built%20upon%20two%20key%20mechanisms%3A%20%281%29%20Process%20rewards%2C%20which%20provide%20fine-grained%20supervision%20for%20the%20quality%20of%20each%20intermediate%20search%20query%20through%20Dual-Level%20Credit%20Assessment.%20%282%29%20Query%20refinement%2C%20which%20promotes%20the%20optimization%20of%20query%20generation%20by%20selectively%20refining%20low-quality%20search%20queries%20and%20regenerating%20subsequent%20search%20rounds%20based%20on%20these%20refinements.%20To%20enable%20the%20search%20agent%20to%20progressively%20internalize%20the%20ability%20to%20improve%20query%20quality%20under%20the%20guidance%20of%20process%20rewards%2C%20we%20design%20a%20three-stage%20curriculum%20learning%20framework.%20This%20framework%20guides%20the%20agent%20through%20a%20progression%20from%20imitation%2C%20to%20alignment%2C%20and%20ultimately%20to%20generalization.%20Experimental%20results%20show%20that%20SmartSearch%20consistently%20surpasses%20existing%20baselines%2C%20and%20additional%20quantitative%20analyses%20further%20confirm%20its%20significant%20gains%20in%20both%20search%20efficiency%20and%20query%20quality.%20The%20code%20is%20available%20at%20https%3A//github.com/MYVAE/SmartSearch.&entry.1838667208=http%3A//arxiv.org/abs/2601.04888v1&entry.124074799=Read"},
{"title": "Higher-Order Knowledge Representations for Agentic Scientific Reasoning", "author": "Isabella A. Stewart and Markus J. Buehler", "abstract": "Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a \"teacherless\" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.", "link": "http://arxiv.org/abs/2601.04878v1", "date": "2026-01-08", "relevancy": 1.4525, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5302}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.493}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-Order%20Knowledge%20Representations%20for%20Agentic%20Scientific%20Reasoning&body=Title%3A%20Higher-Order%20Knowledge%20Representations%20for%20Agentic%20Scientific%20Reasoning%0AAuthor%3A%20Isabella%20A.%20Stewart%20and%20Markus%20J.%20Buehler%0AAbstract%3A%20Scientific%20inquiry%20requires%20systems-level%20reasoning%20that%20integrates%20heterogeneous%20experimental%20data%2C%20cross-domain%20knowledge%2C%20and%20mechanistic%20evidence%20into%20coherent%20explanations.%20While%20Large%20Language%20Models%20%28LLMs%29%20offer%20inferential%20capabilities%2C%20they%20often%20depend%20on%20retrieval-augmented%20contexts%20that%20lack%20structural%20depth.%20Traditional%20Knowledge%20Graphs%20%28KGs%29%20attempt%20to%20bridge%20this%20gap%2C%20yet%20their%20pairwise%20constraints%20fail%20to%20capture%20the%20irreducible%20higher-order%20interactions%20that%20govern%20emergent%20physical%20behavior.%20To%20address%20this%2C%20we%20introduce%20a%20methodology%20for%20constructing%20hypergraph-based%20knowledge%20representations%20that%20faithfully%20encode%20multi-entity%20relationships.%20Applied%20to%20a%20corpus%20of%20~1%2C100%20manuscripts%20on%20biocomposite%20scaffolds%2C%20our%20framework%20constructs%20a%20global%20hypergraph%20of%20161%2C172%20nodes%20and%20320%2C201%20hyperedges%2C%20revealing%20a%20scale-free%20topology%20%28power%20law%20exponent%20~1.23%29%20organized%20around%20highly%20connected%20conceptual%20hubs.%20This%20representation%20prevents%20the%20combinatorial%20explosion%20typical%20of%20pairwise%20expansions%20and%20explicitly%20preserves%20the%20co-occurrence%20context%20of%20scientific%20formulations.%20We%20further%20demonstrate%20that%20equipping%20agentic%20systems%20with%20hypergraph%20traversal%20tools%2C%20specifically%20using%20node-intersection%20constraints%2C%20enables%20them%20to%20bridge%20semantically%20distant%20concepts.%20By%20exploiting%20these%20higher-order%20pathways%2C%20the%20system%20successfully%20generates%20grounded%20mechanistic%20hypotheses%20for%20novel%20composite%20materials%2C%20such%20as%20linking%20cerium%20oxide%20to%20PCL%20scaffolds%20via%20chitosan%20intermediates.%20This%20work%20establishes%20a%20%22teacherless%22%20agentic%20reasoning%20system%20where%20hypergraph%20topology%20acts%20as%20a%20verifiable%20guardrail%2C%20accelerating%20scientific%20discovery%20by%20uncovering%20relationships%20obscured%20by%20traditional%20graph%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-Order%2520Knowledge%2520Representations%2520for%2520Agentic%2520Scientific%2520Reasoning%26entry.906535625%3DIsabella%2520A.%2520Stewart%2520and%2520Markus%2520J.%2520Buehler%26entry.1292438233%3DScientific%2520inquiry%2520requires%2520systems-level%2520reasoning%2520that%2520integrates%2520heterogeneous%2520experimental%2520data%252C%2520cross-domain%2520knowledge%252C%2520and%2520mechanistic%2520evidence%2520into%2520coherent%2520explanations.%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520inferential%2520capabilities%252C%2520they%2520often%2520depend%2520on%2520retrieval-augmented%2520contexts%2520that%2520lack%2520structural%2520depth.%2520Traditional%2520Knowledge%2520Graphs%2520%2528KGs%2529%2520attempt%2520to%2520bridge%2520this%2520gap%252C%2520yet%2520their%2520pairwise%2520constraints%2520fail%2520to%2520capture%2520the%2520irreducible%2520higher-order%2520interactions%2520that%2520govern%2520emergent%2520physical%2520behavior.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520methodology%2520for%2520constructing%2520hypergraph-based%2520knowledge%2520representations%2520that%2520faithfully%2520encode%2520multi-entity%2520relationships.%2520Applied%2520to%2520a%2520corpus%2520of%2520~1%252C100%2520manuscripts%2520on%2520biocomposite%2520scaffolds%252C%2520our%2520framework%2520constructs%2520a%2520global%2520hypergraph%2520of%2520161%252C172%2520nodes%2520and%2520320%252C201%2520hyperedges%252C%2520revealing%2520a%2520scale-free%2520topology%2520%2528power%2520law%2520exponent%2520~1.23%2529%2520organized%2520around%2520highly%2520connected%2520conceptual%2520hubs.%2520This%2520representation%2520prevents%2520the%2520combinatorial%2520explosion%2520typical%2520of%2520pairwise%2520expansions%2520and%2520explicitly%2520preserves%2520the%2520co-occurrence%2520context%2520of%2520scientific%2520formulations.%2520We%2520further%2520demonstrate%2520that%2520equipping%2520agentic%2520systems%2520with%2520hypergraph%2520traversal%2520tools%252C%2520specifically%2520using%2520node-intersection%2520constraints%252C%2520enables%2520them%2520to%2520bridge%2520semantically%2520distant%2520concepts.%2520By%2520exploiting%2520these%2520higher-order%2520pathways%252C%2520the%2520system%2520successfully%2520generates%2520grounded%2520mechanistic%2520hypotheses%2520for%2520novel%2520composite%2520materials%252C%2520such%2520as%2520linking%2520cerium%2520oxide%2520to%2520PCL%2520scaffolds%2520via%2520chitosan%2520intermediates.%2520This%2520work%2520establishes%2520a%2520%2522teacherless%2522%2520agentic%2520reasoning%2520system%2520where%2520hypergraph%2520topology%2520acts%2520as%2520a%2520verifiable%2520guardrail%252C%2520accelerating%2520scientific%2520discovery%2520by%2520uncovering%2520relationships%2520obscured%2520by%2520traditional%2520graph%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-Order%20Knowledge%20Representations%20for%20Agentic%20Scientific%20Reasoning&entry.906535625=Isabella%20A.%20Stewart%20and%20Markus%20J.%20Buehler&entry.1292438233=Scientific%20inquiry%20requires%20systems-level%20reasoning%20that%20integrates%20heterogeneous%20experimental%20data%2C%20cross-domain%20knowledge%2C%20and%20mechanistic%20evidence%20into%20coherent%20explanations.%20While%20Large%20Language%20Models%20%28LLMs%29%20offer%20inferential%20capabilities%2C%20they%20often%20depend%20on%20retrieval-augmented%20contexts%20that%20lack%20structural%20depth.%20Traditional%20Knowledge%20Graphs%20%28KGs%29%20attempt%20to%20bridge%20this%20gap%2C%20yet%20their%20pairwise%20constraints%20fail%20to%20capture%20the%20irreducible%20higher-order%20interactions%20that%20govern%20emergent%20physical%20behavior.%20To%20address%20this%2C%20we%20introduce%20a%20methodology%20for%20constructing%20hypergraph-based%20knowledge%20representations%20that%20faithfully%20encode%20multi-entity%20relationships.%20Applied%20to%20a%20corpus%20of%20~1%2C100%20manuscripts%20on%20biocomposite%20scaffolds%2C%20our%20framework%20constructs%20a%20global%20hypergraph%20of%20161%2C172%20nodes%20and%20320%2C201%20hyperedges%2C%20revealing%20a%20scale-free%20topology%20%28power%20law%20exponent%20~1.23%29%20organized%20around%20highly%20connected%20conceptual%20hubs.%20This%20representation%20prevents%20the%20combinatorial%20explosion%20typical%20of%20pairwise%20expansions%20and%20explicitly%20preserves%20the%20co-occurrence%20context%20of%20scientific%20formulations.%20We%20further%20demonstrate%20that%20equipping%20agentic%20systems%20with%20hypergraph%20traversal%20tools%2C%20specifically%20using%20node-intersection%20constraints%2C%20enables%20them%20to%20bridge%20semantically%20distant%20concepts.%20By%20exploiting%20these%20higher-order%20pathways%2C%20the%20system%20successfully%20generates%20grounded%20mechanistic%20hypotheses%20for%20novel%20composite%20materials%2C%20such%20as%20linking%20cerium%20oxide%20to%20PCL%20scaffolds%20via%20chitosan%20intermediates.%20This%20work%20establishes%20a%20%22teacherless%22%20agentic%20reasoning%20system%20where%20hypergraph%20topology%20acts%20as%20a%20verifiable%20guardrail%2C%20accelerating%20scientific%20discovery%20by%20uncovering%20relationships%20obscured%20by%20traditional%20graph%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.04878v1&entry.124074799=Read"},
{"title": "FibreCastML: An Open Web Platform for Predicting Electrospun Nanofibre Diameter Distributions", "author": "Elisa Roldan and Kirstie Andrews and Stephen M. Richardson and Reyhaneh Fatahian and Glen Cooper and Rasool Erfani and Tasneem Sabir and Neil D. Reeves", "abstract": "Electrospinning is a scalable technique for producing fibrous scaffolds with tunable micro- and nanoscale architectures for applications in tissue engineering, drug delivery, and wound care. While machine learning (ML) has been used to support electrospinning process optimisation, most existing approaches predict only mean fibre diameters, neglecting the full diameter distribution that governs scaffold performance. This work presents FibreCastML, an open, distribution-aware ML framework that predicts complete fibre diameter spectra from routinely reported electrospinning parameters and provides interpretable insights into process structure relationships.\n  A meta-dataset comprising 68538 individual fibre diameter measurements extracted from 1778 studies across 16 biomedical polymers was curated. Six standard processing parameters, namely solution concentration, applied voltage, flow rate, tip to collector distance, needle diameter, and collector rotation speed, were used to train seven ML models using nested cross validation with leave one study out external folds. Model interpretability was achieved using variable importance analysis, SHapley Additive exPlanations, correlation matrices, and three dimensional parameter maps.\n  Non linear models consistently outperformed linear baselines, achieving coefficients of determination above 0.91 for several widely used polymers. Solution concentration emerged as the dominant global driver of fibre diameter distributions. Experimental validation across different electrospinning systems demonstrated close agreement between predicted and measured distributions. FibreCastML enables more reproducible and data driven optimisation of electrospun scaffold architectures.", "link": "http://arxiv.org/abs/2601.04873v1", "date": "2026-01-08", "relevancy": 1.3046, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4616}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4314}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FibreCastML%3A%20An%20Open%20Web%20Platform%20for%20Predicting%20Electrospun%20Nanofibre%20Diameter%20Distributions&body=Title%3A%20FibreCastML%3A%20An%20Open%20Web%20Platform%20for%20Predicting%20Electrospun%20Nanofibre%20Diameter%20Distributions%0AAuthor%3A%20Elisa%20Roldan%20and%20Kirstie%20Andrews%20and%20Stephen%20M.%20Richardson%20and%20Reyhaneh%20Fatahian%20and%20Glen%20Cooper%20and%20Rasool%20Erfani%20and%20Tasneem%20Sabir%20and%20Neil%20D.%20Reeves%0AAbstract%3A%20Electrospinning%20is%20a%20scalable%20technique%20for%20producing%20fibrous%20scaffolds%20with%20tunable%20micro-%20and%20nanoscale%20architectures%20for%20applications%20in%20tissue%20engineering%2C%20drug%20delivery%2C%20and%20wound%20care.%20While%20machine%20learning%20%28ML%29%20has%20been%20used%20to%20support%20electrospinning%20process%20optimisation%2C%20most%20existing%20approaches%20predict%20only%20mean%20fibre%20diameters%2C%20neglecting%20the%20full%20diameter%20distribution%20that%20governs%20scaffold%20performance.%20This%20work%20presents%20FibreCastML%2C%20an%20open%2C%20distribution-aware%20ML%20framework%20that%20predicts%20complete%20fibre%20diameter%20spectra%20from%20routinely%20reported%20electrospinning%20parameters%20and%20provides%20interpretable%20insights%20into%20process%20structure%20relationships.%0A%20%20A%20meta-dataset%20comprising%2068538%20individual%20fibre%20diameter%20measurements%20extracted%20from%201778%20studies%20across%2016%20biomedical%20polymers%20was%20curated.%20Six%20standard%20processing%20parameters%2C%20namely%20solution%20concentration%2C%20applied%20voltage%2C%20flow%20rate%2C%20tip%20to%20collector%20distance%2C%20needle%20diameter%2C%20and%20collector%20rotation%20speed%2C%20were%20used%20to%20train%20seven%20ML%20models%20using%20nested%20cross%20validation%20with%20leave%20one%20study%20out%20external%20folds.%20Model%20interpretability%20was%20achieved%20using%20variable%20importance%20analysis%2C%20SHapley%20Additive%20exPlanations%2C%20correlation%20matrices%2C%20and%20three%20dimensional%20parameter%20maps.%0A%20%20Non%20linear%20models%20consistently%20outperformed%20linear%20baselines%2C%20achieving%20coefficients%20of%20determination%20above%200.91%20for%20several%20widely%20used%20polymers.%20Solution%20concentration%20emerged%20as%20the%20dominant%20global%20driver%20of%20fibre%20diameter%20distributions.%20Experimental%20validation%20across%20different%20electrospinning%20systems%20demonstrated%20close%20agreement%20between%20predicted%20and%20measured%20distributions.%20FibreCastML%20enables%20more%20reproducible%20and%20data%20driven%20optimisation%20of%20electrospun%20scaffold%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFibreCastML%253A%2520An%2520Open%2520Web%2520Platform%2520for%2520Predicting%2520Electrospun%2520Nanofibre%2520Diameter%2520Distributions%26entry.906535625%3DElisa%2520Roldan%2520and%2520Kirstie%2520Andrews%2520and%2520Stephen%2520M.%2520Richardson%2520and%2520Reyhaneh%2520Fatahian%2520and%2520Glen%2520Cooper%2520and%2520Rasool%2520Erfani%2520and%2520Tasneem%2520Sabir%2520and%2520Neil%2520D.%2520Reeves%26entry.1292438233%3DElectrospinning%2520is%2520a%2520scalable%2520technique%2520for%2520producing%2520fibrous%2520scaffolds%2520with%2520tunable%2520micro-%2520and%2520nanoscale%2520architectures%2520for%2520applications%2520in%2520tissue%2520engineering%252C%2520drug%2520delivery%252C%2520and%2520wound%2520care.%2520While%2520machine%2520learning%2520%2528ML%2529%2520has%2520been%2520used%2520to%2520support%2520electrospinning%2520process%2520optimisation%252C%2520most%2520existing%2520approaches%2520predict%2520only%2520mean%2520fibre%2520diameters%252C%2520neglecting%2520the%2520full%2520diameter%2520distribution%2520that%2520governs%2520scaffold%2520performance.%2520This%2520work%2520presents%2520FibreCastML%252C%2520an%2520open%252C%2520distribution-aware%2520ML%2520framework%2520that%2520predicts%2520complete%2520fibre%2520diameter%2520spectra%2520from%2520routinely%2520reported%2520electrospinning%2520parameters%2520and%2520provides%2520interpretable%2520insights%2520into%2520process%2520structure%2520relationships.%250A%2520%2520A%2520meta-dataset%2520comprising%252068538%2520individual%2520fibre%2520diameter%2520measurements%2520extracted%2520from%25201778%2520studies%2520across%252016%2520biomedical%2520polymers%2520was%2520curated.%2520Six%2520standard%2520processing%2520parameters%252C%2520namely%2520solution%2520concentration%252C%2520applied%2520voltage%252C%2520flow%2520rate%252C%2520tip%2520to%2520collector%2520distance%252C%2520needle%2520diameter%252C%2520and%2520collector%2520rotation%2520speed%252C%2520were%2520used%2520to%2520train%2520seven%2520ML%2520models%2520using%2520nested%2520cross%2520validation%2520with%2520leave%2520one%2520study%2520out%2520external%2520folds.%2520Model%2520interpretability%2520was%2520achieved%2520using%2520variable%2520importance%2520analysis%252C%2520SHapley%2520Additive%2520exPlanations%252C%2520correlation%2520matrices%252C%2520and%2520three%2520dimensional%2520parameter%2520maps.%250A%2520%2520Non%2520linear%2520models%2520consistently%2520outperformed%2520linear%2520baselines%252C%2520achieving%2520coefficients%2520of%2520determination%2520above%25200.91%2520for%2520several%2520widely%2520used%2520polymers.%2520Solution%2520concentration%2520emerged%2520as%2520the%2520dominant%2520global%2520driver%2520of%2520fibre%2520diameter%2520distributions.%2520Experimental%2520validation%2520across%2520different%2520electrospinning%2520systems%2520demonstrated%2520close%2520agreement%2520between%2520predicted%2520and%2520measured%2520distributions.%2520FibreCastML%2520enables%2520more%2520reproducible%2520and%2520data%2520driven%2520optimisation%2520of%2520electrospun%2520scaffold%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FibreCastML%3A%20An%20Open%20Web%20Platform%20for%20Predicting%20Electrospun%20Nanofibre%20Diameter%20Distributions&entry.906535625=Elisa%20Roldan%20and%20Kirstie%20Andrews%20and%20Stephen%20M.%20Richardson%20and%20Reyhaneh%20Fatahian%20and%20Glen%20Cooper%20and%20Rasool%20Erfani%20and%20Tasneem%20Sabir%20and%20Neil%20D.%20Reeves&entry.1292438233=Electrospinning%20is%20a%20scalable%20technique%20for%20producing%20fibrous%20scaffolds%20with%20tunable%20micro-%20and%20nanoscale%20architectures%20for%20applications%20in%20tissue%20engineering%2C%20drug%20delivery%2C%20and%20wound%20care.%20While%20machine%20learning%20%28ML%29%20has%20been%20used%20to%20support%20electrospinning%20process%20optimisation%2C%20most%20existing%20approaches%20predict%20only%20mean%20fibre%20diameters%2C%20neglecting%20the%20full%20diameter%20distribution%20that%20governs%20scaffold%20performance.%20This%20work%20presents%20FibreCastML%2C%20an%20open%2C%20distribution-aware%20ML%20framework%20that%20predicts%20complete%20fibre%20diameter%20spectra%20from%20routinely%20reported%20electrospinning%20parameters%20and%20provides%20interpretable%20insights%20into%20process%20structure%20relationships.%0A%20%20A%20meta-dataset%20comprising%2068538%20individual%20fibre%20diameter%20measurements%20extracted%20from%201778%20studies%20across%2016%20biomedical%20polymers%20was%20curated.%20Six%20standard%20processing%20parameters%2C%20namely%20solution%20concentration%2C%20applied%20voltage%2C%20flow%20rate%2C%20tip%20to%20collector%20distance%2C%20needle%20diameter%2C%20and%20collector%20rotation%20speed%2C%20were%20used%20to%20train%20seven%20ML%20models%20using%20nested%20cross%20validation%20with%20leave%20one%20study%20out%20external%20folds.%20Model%20interpretability%20was%20achieved%20using%20variable%20importance%20analysis%2C%20SHapley%20Additive%20exPlanations%2C%20correlation%20matrices%2C%20and%20three%20dimensional%20parameter%20maps.%0A%20%20Non%20linear%20models%20consistently%20outperformed%20linear%20baselines%2C%20achieving%20coefficients%20of%20determination%20above%200.91%20for%20several%20widely%20used%20polymers.%20Solution%20concentration%20emerged%20as%20the%20dominant%20global%20driver%20of%20fibre%20diameter%20distributions.%20Experimental%20validation%20across%20different%20electrospinning%20systems%20demonstrated%20close%20agreement%20between%20predicted%20and%20measured%20distributions.%20FibreCastML%20enables%20more%20reproducible%20and%20data%20driven%20optimisation%20of%20electrospun%20scaffold%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2601.04873v1&entry.124074799=Read"},
{"title": "Illumination Angular Spectrum Encoding for Controlling the Functionality of Diffractive Networks", "author": "Matan Kleiner and Lior Michaeli and Tomer Michaeli", "abstract": "Diffractive neural networks have recently emerged as a promising framework for all-optical computing. However, these networks are typically trained for a single task, limiting their potential adoption in systems requiring multiple functionalities. Existing approaches to achieving multi-task functionality either modify the mechanical configuration of the network per task or use a different illumination wavelength or polarization state for each task. In this work, we propose a new control mechanism, which is based on the illumination's angular spectrum. Specifically, we shape the illumination using an amplitude mask that selectively controls its angular spectrum. We employ different illumination masks for achieving different network functionalities, so that the mask serves as a unique task encoder. Interestingly, we show that effective control can be achieved over a very narrow angular range, within the paraxial regime. We numerically illustrate the proposed approach by training a single diffractive network to perform multiple image-to-image translation tasks. In particular, we demonstrate translating handwritten digits into typeset digits of different values, and translating handwritten English letters into typeset numbers and typeset Greek letters, where the type of the output is determined by the illumination's angular components. As we show, the proposed framework can work under different coherence conditions, and can be combined with existing control strategies, such as different wavelengths. Our results establish the illumination angular spectrum as a powerful degree of freedom for controlling diffractive networks, enabling a scalable and versatile framework for multi-task all-optical computing.", "link": "http://arxiv.org/abs/2601.04825v1", "date": "2026-01-08", "relevancy": 0.9964, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5039}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4984}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Illumination%20Angular%20Spectrum%20Encoding%20for%20Controlling%20the%20Functionality%20of%20Diffractive%20Networks&body=Title%3A%20Illumination%20Angular%20Spectrum%20Encoding%20for%20Controlling%20the%20Functionality%20of%20Diffractive%20Networks%0AAuthor%3A%20Matan%20Kleiner%20and%20Lior%20Michaeli%20and%20Tomer%20Michaeli%0AAbstract%3A%20Diffractive%20neural%20networks%20have%20recently%20emerged%20as%20a%20promising%20framework%20for%20all-optical%20computing.%20However%2C%20these%20networks%20are%20typically%20trained%20for%20a%20single%20task%2C%20limiting%20their%20potential%20adoption%20in%20systems%20requiring%20multiple%20functionalities.%20Existing%20approaches%20to%20achieving%20multi-task%20functionality%20either%20modify%20the%20mechanical%20configuration%20of%20the%20network%20per%20task%20or%20use%20a%20different%20illumination%20wavelength%20or%20polarization%20state%20for%20each%20task.%20In%20this%20work%2C%20we%20propose%20a%20new%20control%20mechanism%2C%20which%20is%20based%20on%20the%20illumination%27s%20angular%20spectrum.%20Specifically%2C%20we%20shape%20the%20illumination%20using%20an%20amplitude%20mask%20that%20selectively%20controls%20its%20angular%20spectrum.%20We%20employ%20different%20illumination%20masks%20for%20achieving%20different%20network%20functionalities%2C%20so%20that%20the%20mask%20serves%20as%20a%20unique%20task%20encoder.%20Interestingly%2C%20we%20show%20that%20effective%20control%20can%20be%20achieved%20over%20a%20very%20narrow%20angular%20range%2C%20within%20the%20paraxial%20regime.%20We%20numerically%20illustrate%20the%20proposed%20approach%20by%20training%20a%20single%20diffractive%20network%20to%20perform%20multiple%20image-to-image%20translation%20tasks.%20In%20particular%2C%20we%20demonstrate%20translating%20handwritten%20digits%20into%20typeset%20digits%20of%20different%20values%2C%20and%20translating%20handwritten%20English%20letters%20into%20typeset%20numbers%20and%20typeset%20Greek%20letters%2C%20where%20the%20type%20of%20the%20output%20is%20determined%20by%20the%20illumination%27s%20angular%20components.%20As%20we%20show%2C%20the%20proposed%20framework%20can%20work%20under%20different%20coherence%20conditions%2C%20and%20can%20be%20combined%20with%20existing%20control%20strategies%2C%20such%20as%20different%20wavelengths.%20Our%20results%20establish%20the%20illumination%20angular%20spectrum%20as%20a%20powerful%20degree%20of%20freedom%20for%20controlling%20diffractive%20networks%2C%20enabling%20a%20scalable%20and%20versatile%20framework%20for%20multi-task%20all-optical%20computing.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIllumination%2520Angular%2520Spectrum%2520Encoding%2520for%2520Controlling%2520the%2520Functionality%2520of%2520Diffractive%2520Networks%26entry.906535625%3DMatan%2520Kleiner%2520and%2520Lior%2520Michaeli%2520and%2520Tomer%2520Michaeli%26entry.1292438233%3DDiffractive%2520neural%2520networks%2520have%2520recently%2520emerged%2520as%2520a%2520promising%2520framework%2520for%2520all-optical%2520computing.%2520However%252C%2520these%2520networks%2520are%2520typically%2520trained%2520for%2520a%2520single%2520task%252C%2520limiting%2520their%2520potential%2520adoption%2520in%2520systems%2520requiring%2520multiple%2520functionalities.%2520Existing%2520approaches%2520to%2520achieving%2520multi-task%2520functionality%2520either%2520modify%2520the%2520mechanical%2520configuration%2520of%2520the%2520network%2520per%2520task%2520or%2520use%2520a%2520different%2520illumination%2520wavelength%2520or%2520polarization%2520state%2520for%2520each%2520task.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520control%2520mechanism%252C%2520which%2520is%2520based%2520on%2520the%2520illumination%2527s%2520angular%2520spectrum.%2520Specifically%252C%2520we%2520shape%2520the%2520illumination%2520using%2520an%2520amplitude%2520mask%2520that%2520selectively%2520controls%2520its%2520angular%2520spectrum.%2520We%2520employ%2520different%2520illumination%2520masks%2520for%2520achieving%2520different%2520network%2520functionalities%252C%2520so%2520that%2520the%2520mask%2520serves%2520as%2520a%2520unique%2520task%2520encoder.%2520Interestingly%252C%2520we%2520show%2520that%2520effective%2520control%2520can%2520be%2520achieved%2520over%2520a%2520very%2520narrow%2520angular%2520range%252C%2520within%2520the%2520paraxial%2520regime.%2520We%2520numerically%2520illustrate%2520the%2520proposed%2520approach%2520by%2520training%2520a%2520single%2520diffractive%2520network%2520to%2520perform%2520multiple%2520image-to-image%2520translation%2520tasks.%2520In%2520particular%252C%2520we%2520demonstrate%2520translating%2520handwritten%2520digits%2520into%2520typeset%2520digits%2520of%2520different%2520values%252C%2520and%2520translating%2520handwritten%2520English%2520letters%2520into%2520typeset%2520numbers%2520and%2520typeset%2520Greek%2520letters%252C%2520where%2520the%2520type%2520of%2520the%2520output%2520is%2520determined%2520by%2520the%2520illumination%2527s%2520angular%2520components.%2520As%2520we%2520show%252C%2520the%2520proposed%2520framework%2520can%2520work%2520under%2520different%2520coherence%2520conditions%252C%2520and%2520can%2520be%2520combined%2520with%2520existing%2520control%2520strategies%252C%2520such%2520as%2520different%2520wavelengths.%2520Our%2520results%2520establish%2520the%2520illumination%2520angular%2520spectrum%2520as%2520a%2520powerful%2520degree%2520of%2520freedom%2520for%2520controlling%2520diffractive%2520networks%252C%2520enabling%2520a%2520scalable%2520and%2520versatile%2520framework%2520for%2520multi-task%2520all-optical%2520computing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Illumination%20Angular%20Spectrum%20Encoding%20for%20Controlling%20the%20Functionality%20of%20Diffractive%20Networks&entry.906535625=Matan%20Kleiner%20and%20Lior%20Michaeli%20and%20Tomer%20Michaeli&entry.1292438233=Diffractive%20neural%20networks%20have%20recently%20emerged%20as%20a%20promising%20framework%20for%20all-optical%20computing.%20However%2C%20these%20networks%20are%20typically%20trained%20for%20a%20single%20task%2C%20limiting%20their%20potential%20adoption%20in%20systems%20requiring%20multiple%20functionalities.%20Existing%20approaches%20to%20achieving%20multi-task%20functionality%20either%20modify%20the%20mechanical%20configuration%20of%20the%20network%20per%20task%20or%20use%20a%20different%20illumination%20wavelength%20or%20polarization%20state%20for%20each%20task.%20In%20this%20work%2C%20we%20propose%20a%20new%20control%20mechanism%2C%20which%20is%20based%20on%20the%20illumination%27s%20angular%20spectrum.%20Specifically%2C%20we%20shape%20the%20illumination%20using%20an%20amplitude%20mask%20that%20selectively%20controls%20its%20angular%20spectrum.%20We%20employ%20different%20illumination%20masks%20for%20achieving%20different%20network%20functionalities%2C%20so%20that%20the%20mask%20serves%20as%20a%20unique%20task%20encoder.%20Interestingly%2C%20we%20show%20that%20effective%20control%20can%20be%20achieved%20over%20a%20very%20narrow%20angular%20range%2C%20within%20the%20paraxial%20regime.%20We%20numerically%20illustrate%20the%20proposed%20approach%20by%20training%20a%20single%20diffractive%20network%20to%20perform%20multiple%20image-to-image%20translation%20tasks.%20In%20particular%2C%20we%20demonstrate%20translating%20handwritten%20digits%20into%20typeset%20digits%20of%20different%20values%2C%20and%20translating%20handwritten%20English%20letters%20into%20typeset%20numbers%20and%20typeset%20Greek%20letters%2C%20where%20the%20type%20of%20the%20output%20is%20determined%20by%20the%20illumination%27s%20angular%20components.%20As%20we%20show%2C%20the%20proposed%20framework%20can%20work%20under%20different%20coherence%20conditions%2C%20and%20can%20be%20combined%20with%20existing%20control%20strategies%2C%20such%20as%20different%20wavelengths.%20Our%20results%20establish%20the%20illumination%20angular%20spectrum%20as%20a%20powerful%20degree%20of%20freedom%20for%20controlling%20diffractive%20networks%2C%20enabling%20a%20scalable%20and%20versatile%20framework%20for%20multi-task%20all-optical%20computing.&entry.1838667208=http%3A//arxiv.org/abs/2601.04825v1&entry.124074799=Read"},
{"title": "Robust Reasoning as a Symmetry-Protected Topological Phase", "author": "Ilmo Sung", "abstract": "Large language models suffer from \"hallucinations\"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a \"Metric Phase,\" where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic \"mass gap,\" maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \\times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\\times$ beyond training ($L=50 \\to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.", "link": "http://arxiv.org/abs/2601.05240v1", "date": "2026-01-08", "relevancy": 1.4011, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5067}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4679}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Reasoning%20as%20a%20Symmetry-Protected%20Topological%20Phase&body=Title%3A%20Robust%20Reasoning%20as%20a%20Symmetry-Protected%20Topological%20Phase%0AAuthor%3A%20Ilmo%20Sung%0AAbstract%3A%20Large%20language%20models%20suffer%20from%20%22hallucinations%22-logical%20inconsistencies%20induced%20by%20semantic%20noise.%20We%20propose%20that%20current%20architectures%20operate%20in%20a%20%22Metric%20Phase%2C%22%20where%20causal%20order%20is%20vulnerable%20to%20spontaneous%20symmetry%20breaking.%20Here%2C%20we%20identify%20robust%20inference%20as%20an%20effective%20Symmetry-Protected%20Topological%20phase%2C%20where%20logical%20operations%20are%20formally%20isomorphic%20to%20non-Abelian%20anyon%20braiding%2C%20replacing%20fragile%20geometric%20interpolation%20with%20robust%20topological%20invariants.%20Empirically%2C%20we%20demonstrate%20a%20sharp%20topological%20phase%20transition%3A%20while%20Transformers%20and%20RNNs%20exhibit%20gapless%20decay%2C%20our%20Holonomic%20Network%20reveals%20a%20macroscopic%20%22mass%20gap%2C%22%20maintaining%20invariant%20fidelity%20below%20a%20critical%20noise%20threshold.%20Furthermore%2C%20in%20a%20variable-binding%20task%20on%20%24S_%7B10%7D%24%20%28%243.6%20%5Ctimes%2010%5E6%24%20states%29%20representing%20symbolic%20manipulation%2C%20we%20demonstrate%20holonomic%20generalization%3A%20the%20topological%20model%20maintains%20perfect%20fidelity%20extrapolating%20%24100%5Ctimes%24%20beyond%20training%20%28%24L%3D50%20%5Cto%205000%24%29%2C%20consistent%20with%20a%20theoretically%20indefinite%20causal%20horizon%2C%20whereas%20Transformers%20lose%20logical%20coherence.%20Ablation%20studies%20indicate%20this%20protection%20emerges%20strictly%20from%20non-Abelian%20gauge%20symmetry.%20This%20provides%20strong%20evidence%20for%20a%20new%20universality%20class%20for%20logical%20reasoning%2C%20linking%20causal%20stability%20to%20the%20topology%20of%20the%20semantic%20manifold.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Reasoning%2520as%2520a%2520Symmetry-Protected%2520Topological%2520Phase%26entry.906535625%3DIlmo%2520Sung%26entry.1292438233%3DLarge%2520language%2520models%2520suffer%2520from%2520%2522hallucinations%2522-logical%2520inconsistencies%2520induced%2520by%2520semantic%2520noise.%2520We%2520propose%2520that%2520current%2520architectures%2520operate%2520in%2520a%2520%2522Metric%2520Phase%252C%2522%2520where%2520causal%2520order%2520is%2520vulnerable%2520to%2520spontaneous%2520symmetry%2520breaking.%2520Here%252C%2520we%2520identify%2520robust%2520inference%2520as%2520an%2520effective%2520Symmetry-Protected%2520Topological%2520phase%252C%2520where%2520logical%2520operations%2520are%2520formally%2520isomorphic%2520to%2520non-Abelian%2520anyon%2520braiding%252C%2520replacing%2520fragile%2520geometric%2520interpolation%2520with%2520robust%2520topological%2520invariants.%2520Empirically%252C%2520we%2520demonstrate%2520a%2520sharp%2520topological%2520phase%2520transition%253A%2520while%2520Transformers%2520and%2520RNNs%2520exhibit%2520gapless%2520decay%252C%2520our%2520Holonomic%2520Network%2520reveals%2520a%2520macroscopic%2520%2522mass%2520gap%252C%2522%2520maintaining%2520invariant%2520fidelity%2520below%2520a%2520critical%2520noise%2520threshold.%2520Furthermore%252C%2520in%2520a%2520variable-binding%2520task%2520on%2520%2524S_%257B10%257D%2524%2520%2528%25243.6%2520%255Ctimes%252010%255E6%2524%2520states%2529%2520representing%2520symbolic%2520manipulation%252C%2520we%2520demonstrate%2520holonomic%2520generalization%253A%2520the%2520topological%2520model%2520maintains%2520perfect%2520fidelity%2520extrapolating%2520%2524100%255Ctimes%2524%2520beyond%2520training%2520%2528%2524L%253D50%2520%255Cto%25205000%2524%2529%252C%2520consistent%2520with%2520a%2520theoretically%2520indefinite%2520causal%2520horizon%252C%2520whereas%2520Transformers%2520lose%2520logical%2520coherence.%2520Ablation%2520studies%2520indicate%2520this%2520protection%2520emerges%2520strictly%2520from%2520non-Abelian%2520gauge%2520symmetry.%2520This%2520provides%2520strong%2520evidence%2520for%2520a%2520new%2520universality%2520class%2520for%2520logical%2520reasoning%252C%2520linking%2520causal%2520stability%2520to%2520the%2520topology%2520of%2520the%2520semantic%2520manifold.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Reasoning%20as%20a%20Symmetry-Protected%20Topological%20Phase&entry.906535625=Ilmo%20Sung&entry.1292438233=Large%20language%20models%20suffer%20from%20%22hallucinations%22-logical%20inconsistencies%20induced%20by%20semantic%20noise.%20We%20propose%20that%20current%20architectures%20operate%20in%20a%20%22Metric%20Phase%2C%22%20where%20causal%20order%20is%20vulnerable%20to%20spontaneous%20symmetry%20breaking.%20Here%2C%20we%20identify%20robust%20inference%20as%20an%20effective%20Symmetry-Protected%20Topological%20phase%2C%20where%20logical%20operations%20are%20formally%20isomorphic%20to%20non-Abelian%20anyon%20braiding%2C%20replacing%20fragile%20geometric%20interpolation%20with%20robust%20topological%20invariants.%20Empirically%2C%20we%20demonstrate%20a%20sharp%20topological%20phase%20transition%3A%20while%20Transformers%20and%20RNNs%20exhibit%20gapless%20decay%2C%20our%20Holonomic%20Network%20reveals%20a%20macroscopic%20%22mass%20gap%2C%22%20maintaining%20invariant%20fidelity%20below%20a%20critical%20noise%20threshold.%20Furthermore%2C%20in%20a%20variable-binding%20task%20on%20%24S_%7B10%7D%24%20%28%243.6%20%5Ctimes%2010%5E6%24%20states%29%20representing%20symbolic%20manipulation%2C%20we%20demonstrate%20holonomic%20generalization%3A%20the%20topological%20model%20maintains%20perfect%20fidelity%20extrapolating%20%24100%5Ctimes%24%20beyond%20training%20%28%24L%3D50%20%5Cto%205000%24%29%2C%20consistent%20with%20a%20theoretically%20indefinite%20causal%20horizon%2C%20whereas%20Transformers%20lose%20logical%20coherence.%20Ablation%20studies%20indicate%20this%20protection%20emerges%20strictly%20from%20non-Abelian%20gauge%20symmetry.%20This%20provides%20strong%20evidence%20for%20a%20new%20universality%20class%20for%20logical%20reasoning%2C%20linking%20causal%20stability%20to%20the%20topology%20of%20the%20semantic%20manifold.&entry.1838667208=http%3A//arxiv.org/abs/2601.05240v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


