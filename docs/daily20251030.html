<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="#"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient\n  Surface Reconstruction", "author": "Antoine Gu\u00e9don and Diego Gomez and Nissim Maruani and Bingchen Gong and George Drettakis and Maks Ovsjanikov", "abstract": "  While recent advances in Gaussian Splatting have enabled fast reconstruction\nof high-quality 3D scenes from images, extracting accurate surface meshes\nremains a challenge. Current approaches extract the surface through costly\npost-processing steps, resulting in the loss of fine geometric details or\nrequiring significant time and leading to very dense meshes with millions of\nvertices. More fundamentally, the a posteriori conversion from a volumetric to\na surface representation limits the ability of the final mesh to preserve all\ngeometric structures captured during training. We present MILo, a novel\nGaussian Splatting framework that bridges the gap between volumetric and\nsurface representations by differentiably extracting a mesh from the 3D\nGaussians. We design a fully differentiable procedure that constructs the\nmesh-including both vertex locations and connectivity-at every iteration\ndirectly from the parameters of the Gaussians, which are the only quantities\noptimized during training. Our method introduces three key technical\ncontributions: a bidirectional consistency framework ensuring both\nrepresentations-Gaussians and the extracted mesh-capture the same underlying\ngeometry during training; an adaptive mesh extraction process performed at each\ntraining iteration, which uses Gaussians as differentiable pivots for Delaunay\ntriangulation; a novel method for computing signed distance values from the 3D\nGaussians that enables precise surface extraction while avoiding geometric\nerosion. Our approach can reconstruct complete scenes, including backgrounds,\nwith state-of-the-art quality while requiring an order of magnitude fewer mesh\nvertices than previous methods. Due to their light weight and empty interior,\nour meshes are well suited for downstream applications such as physics\nsimulations or animation.\n", "link": "http://arxiv.org/abs/2506.24096v2", "date": "2025-10-29", "relevancy": 3.4592, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7337}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6723}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MILo%3A%20Mesh-In-the-Loop%20Gaussian%20Splatting%20for%20Detailed%20and%20Efficient%0A%20%20Surface%20Reconstruction&body=Title%3A%20MILo%3A%20Mesh-In-the-Loop%20Gaussian%20Splatting%20for%20Detailed%20and%20Efficient%0A%20%20Surface%20Reconstruction%0AAuthor%3A%20Antoine%20Gu%C3%A9don%20and%20Diego%20Gomez%20and%20Nissim%20Maruani%20and%20Bingchen%20Gong%20and%20George%20Drettakis%20and%20Maks%20Ovsjanikov%0AAbstract%3A%20%20%20While%20recent%20advances%20in%20Gaussian%20Splatting%20have%20enabled%20fast%20reconstruction%0Aof%20high-quality%203D%20scenes%20from%20images%2C%20extracting%20accurate%20surface%20meshes%0Aremains%20a%20challenge.%20Current%20approaches%20extract%20the%20surface%20through%20costly%0Apost-processing%20steps%2C%20resulting%20in%20the%20loss%20of%20fine%20geometric%20details%20or%0Arequiring%20significant%20time%20and%20leading%20to%20very%20dense%20meshes%20with%20millions%20of%0Avertices.%20More%20fundamentally%2C%20the%20a%20posteriori%20conversion%20from%20a%20volumetric%20to%0Aa%20surface%20representation%20limits%20the%20ability%20of%20the%20final%20mesh%20to%20preserve%20all%0Ageometric%20structures%20captured%20during%20training.%20We%20present%20MILo%2C%20a%20novel%0AGaussian%20Splatting%20framework%20that%20bridges%20the%20gap%20between%20volumetric%20and%0Asurface%20representations%20by%20differentiably%20extracting%20a%20mesh%20from%20the%203D%0AGaussians.%20We%20design%20a%20fully%20differentiable%20procedure%20that%20constructs%20the%0Amesh-including%20both%20vertex%20locations%20and%20connectivity-at%20every%20iteration%0Adirectly%20from%20the%20parameters%20of%20the%20Gaussians%2C%20which%20are%20the%20only%20quantities%0Aoptimized%20during%20training.%20Our%20method%20introduces%20three%20key%20technical%0Acontributions%3A%20a%20bidirectional%20consistency%20framework%20ensuring%20both%0Arepresentations-Gaussians%20and%20the%20extracted%20mesh-capture%20the%20same%20underlying%0Ageometry%20during%20training%3B%20an%20adaptive%20mesh%20extraction%20process%20performed%20at%20each%0Atraining%20iteration%2C%20which%20uses%20Gaussians%20as%20differentiable%20pivots%20for%20Delaunay%0Atriangulation%3B%20a%20novel%20method%20for%20computing%20signed%20distance%20values%20from%20the%203D%0AGaussians%20that%20enables%20precise%20surface%20extraction%20while%20avoiding%20geometric%0Aerosion.%20Our%20approach%20can%20reconstruct%20complete%20scenes%2C%20including%20backgrounds%2C%0Awith%20state-of-the-art%20quality%20while%20requiring%20an%20order%20of%20magnitude%20fewer%20mesh%0Avertices%20than%20previous%20methods.%20Due%20to%20their%20light%20weight%20and%20empty%20interior%2C%0Aour%20meshes%20are%20well%20suited%20for%20downstream%20applications%20such%20as%20physics%0Asimulations%20or%20animation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.24096v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMILo%253A%2520Mesh-In-the-Loop%2520Gaussian%2520Splatting%2520for%2520Detailed%2520and%2520Efficient%250A%2520%2520Surface%2520Reconstruction%26entry.906535625%3DAntoine%2520Gu%25C3%25A9don%2520and%2520Diego%2520Gomez%2520and%2520Nissim%2520Maruani%2520and%2520Bingchen%2520Gong%2520and%2520George%2520Drettakis%2520and%2520Maks%2520Ovsjanikov%26entry.1292438233%3D%2520%2520While%2520recent%2520advances%2520in%2520Gaussian%2520Splatting%2520have%2520enabled%2520fast%2520reconstruction%250Aof%2520high-quality%25203D%2520scenes%2520from%2520images%252C%2520extracting%2520accurate%2520surface%2520meshes%250Aremains%2520a%2520challenge.%2520Current%2520approaches%2520extract%2520the%2520surface%2520through%2520costly%250Apost-processing%2520steps%252C%2520resulting%2520in%2520the%2520loss%2520of%2520fine%2520geometric%2520details%2520or%250Arequiring%2520significant%2520time%2520and%2520leading%2520to%2520very%2520dense%2520meshes%2520with%2520millions%2520of%250Avertices.%2520More%2520fundamentally%252C%2520the%2520a%2520posteriori%2520conversion%2520from%2520a%2520volumetric%2520to%250Aa%2520surface%2520representation%2520limits%2520the%2520ability%2520of%2520the%2520final%2520mesh%2520to%2520preserve%2520all%250Ageometric%2520structures%2520captured%2520during%2520training.%2520We%2520present%2520MILo%252C%2520a%2520novel%250AGaussian%2520Splatting%2520framework%2520that%2520bridges%2520the%2520gap%2520between%2520volumetric%2520and%250Asurface%2520representations%2520by%2520differentiably%2520extracting%2520a%2520mesh%2520from%2520the%25203D%250AGaussians.%2520We%2520design%2520a%2520fully%2520differentiable%2520procedure%2520that%2520constructs%2520the%250Amesh-including%2520both%2520vertex%2520locations%2520and%2520connectivity-at%2520every%2520iteration%250Adirectly%2520from%2520the%2520parameters%2520of%2520the%2520Gaussians%252C%2520which%2520are%2520the%2520only%2520quantities%250Aoptimized%2520during%2520training.%2520Our%2520method%2520introduces%2520three%2520key%2520technical%250Acontributions%253A%2520a%2520bidirectional%2520consistency%2520framework%2520ensuring%2520both%250Arepresentations-Gaussians%2520and%2520the%2520extracted%2520mesh-capture%2520the%2520same%2520underlying%250Ageometry%2520during%2520training%253B%2520an%2520adaptive%2520mesh%2520extraction%2520process%2520performed%2520at%2520each%250Atraining%2520iteration%252C%2520which%2520uses%2520Gaussians%2520as%2520differentiable%2520pivots%2520for%2520Delaunay%250Atriangulation%253B%2520a%2520novel%2520method%2520for%2520computing%2520signed%2520distance%2520values%2520from%2520the%25203D%250AGaussians%2520that%2520enables%2520precise%2520surface%2520extraction%2520while%2520avoiding%2520geometric%250Aerosion.%2520Our%2520approach%2520can%2520reconstruct%2520complete%2520scenes%252C%2520including%2520backgrounds%252C%250Awith%2520state-of-the-art%2520quality%2520while%2520requiring%2520an%2520order%2520of%2520magnitude%2520fewer%2520mesh%250Avertices%2520than%2520previous%2520methods.%2520Due%2520to%2520their%2520light%2520weight%2520and%2520empty%2520interior%252C%250Aour%2520meshes%2520are%2520well%2520suited%2520for%2520downstream%2520applications%2520such%2520as%2520physics%250Asimulations%2520or%2520animation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.24096v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MILo%3A%20Mesh-In-the-Loop%20Gaussian%20Splatting%20for%20Detailed%20and%20Efficient%0A%20%20Surface%20Reconstruction&entry.906535625=Antoine%20Gu%C3%A9don%20and%20Diego%20Gomez%20and%20Nissim%20Maruani%20and%20Bingchen%20Gong%20and%20George%20Drettakis%20and%20Maks%20Ovsjanikov&entry.1292438233=%20%20While%20recent%20advances%20in%20Gaussian%20Splatting%20have%20enabled%20fast%20reconstruction%0Aof%20high-quality%203D%20scenes%20from%20images%2C%20extracting%20accurate%20surface%20meshes%0Aremains%20a%20challenge.%20Current%20approaches%20extract%20the%20surface%20through%20costly%0Apost-processing%20steps%2C%20resulting%20in%20the%20loss%20of%20fine%20geometric%20details%20or%0Arequiring%20significant%20time%20and%20leading%20to%20very%20dense%20meshes%20with%20millions%20of%0Avertices.%20More%20fundamentally%2C%20the%20a%20posteriori%20conversion%20from%20a%20volumetric%20to%0Aa%20surface%20representation%20limits%20the%20ability%20of%20the%20final%20mesh%20to%20preserve%20all%0Ageometric%20structures%20captured%20during%20training.%20We%20present%20MILo%2C%20a%20novel%0AGaussian%20Splatting%20framework%20that%20bridges%20the%20gap%20between%20volumetric%20and%0Asurface%20representations%20by%20differentiably%20extracting%20a%20mesh%20from%20the%203D%0AGaussians.%20We%20design%20a%20fully%20differentiable%20procedure%20that%20constructs%20the%0Amesh-including%20both%20vertex%20locations%20and%20connectivity-at%20every%20iteration%0Adirectly%20from%20the%20parameters%20of%20the%20Gaussians%2C%20which%20are%20the%20only%20quantities%0Aoptimized%20during%20training.%20Our%20method%20introduces%20three%20key%20technical%0Acontributions%3A%20a%20bidirectional%20consistency%20framework%20ensuring%20both%0Arepresentations-Gaussians%20and%20the%20extracted%20mesh-capture%20the%20same%20underlying%0Ageometry%20during%20training%3B%20an%20adaptive%20mesh%20extraction%20process%20performed%20at%20each%0Atraining%20iteration%2C%20which%20uses%20Gaussians%20as%20differentiable%20pivots%20for%20Delaunay%0Atriangulation%3B%20a%20novel%20method%20for%20computing%20signed%20distance%20values%20from%20the%203D%0AGaussians%20that%20enables%20precise%20surface%20extraction%20while%20avoiding%20geometric%0Aerosion.%20Our%20approach%20can%20reconstruct%20complete%20scenes%2C%20including%20backgrounds%2C%0Awith%20state-of-the-art%20quality%20while%20requiring%20an%20order%20of%20magnitude%20fewer%20mesh%0Avertices%20than%20previous%20methods.%20Due%20to%20their%20light%20weight%20and%20empty%20interior%2C%0Aour%20meshes%20are%20well%20suited%20for%20downstream%20applications%20such%20as%20physics%0Asimulations%20or%20animation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.24096v2&entry.124074799=Read"},
{"title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for\n  Dynamic Scene", "author": "Jianing Chen and Zehao Li and Yujun Cai and Hao Jiang and Chengxuan Qian and Juyuan Kang and Shuqin Gao and Honglong Zhao and Tianlu Mao and Yucheng Zhang", "abstract": "  Reconstructing dynamic 3D scenes from monocular videos remains a fundamental\nchallenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time\nrendering in static settings, extending it to dynamic scenes is challenging due\nto the difficulty of learning structured and temporally consistent motion\nrepresentations. This challenge often manifests as three limitations in\nexisting methods: redundant Gaussian updates, insufficient motion supervision,\nand weak modeling of complex non-rigid deformations. These issues collectively\nhinder coherent and efficient dynamic reconstruction. To address these\nlimitations, we propose HAIF-GS, a unified framework that enables structured\nand consistent dynamic modeling through sparse anchor-driven deformation. It\nfirst identifies motion-relevant regions via an Anchor Filter to suppress\nredundant updates in static areas. A self-supervised Induced Flow-Guided\nDeformation module induces anchor motion using multi-frame feature aggregation,\neliminating the need for explicit flow labels. To further handle fine-grained\ndeformations, a Hierarchical Anchor Propagation mechanism increases anchor\nresolution based on motion complexity and propagates multi-level\ntransformations. Extensive experiments on synthetic and real-world benchmarks\nvalidate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in\nrendering quality, temporal coherence, and reconstruction efficiency.\n", "link": "http://arxiv.org/abs/2506.09518v2", "date": "2025-10-29", "relevancy": 3.425, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7281}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.664}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAIF-GS%3A%20Hierarchical%20and%20Induced%20Flow-Guided%20Gaussian%20Splatting%20for%0A%20%20Dynamic%20Scene&body=Title%3A%20HAIF-GS%3A%20Hierarchical%20and%20Induced%20Flow-Guided%20Gaussian%20Splatting%20for%0A%20%20Dynamic%20Scene%0AAuthor%3A%20Jianing%20Chen%20and%20Zehao%20Li%20and%20Yujun%20Cai%20and%20Hao%20Jiang%20and%20Chengxuan%20Qian%20and%20Juyuan%20Kang%20and%20Shuqin%20Gao%20and%20Honglong%20Zhao%20and%20Tianlu%20Mao%20and%20Yucheng%20Zhang%0AAbstract%3A%20%20%20Reconstructing%20dynamic%203D%20scenes%20from%20monocular%20videos%20remains%20a%20fundamental%0Achallenge%20in%203D%20vision.%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20achieves%20real-time%0Arendering%20in%20static%20settings%2C%20extending%20it%20to%20dynamic%20scenes%20is%20challenging%20due%0Ato%20the%20difficulty%20of%20learning%20structured%20and%20temporally%20consistent%20motion%0Arepresentations.%20This%20challenge%20often%20manifests%20as%20three%20limitations%20in%0Aexisting%20methods%3A%20redundant%20Gaussian%20updates%2C%20insufficient%20motion%20supervision%2C%0Aand%20weak%20modeling%20of%20complex%20non-rigid%20deformations.%20These%20issues%20collectively%0Ahinder%20coherent%20and%20efficient%20dynamic%20reconstruction.%20To%20address%20these%0Alimitations%2C%20we%20propose%20HAIF-GS%2C%20a%20unified%20framework%20that%20enables%20structured%0Aand%20consistent%20dynamic%20modeling%20through%20sparse%20anchor-driven%20deformation.%20It%0Afirst%20identifies%20motion-relevant%20regions%20via%20an%20Anchor%20Filter%20to%20suppress%0Aredundant%20updates%20in%20static%20areas.%20A%20self-supervised%20Induced%20Flow-Guided%0ADeformation%20module%20induces%20anchor%20motion%20using%20multi-frame%20feature%20aggregation%2C%0Aeliminating%20the%20need%20for%20explicit%20flow%20labels.%20To%20further%20handle%20fine-grained%0Adeformations%2C%20a%20Hierarchical%20Anchor%20Propagation%20mechanism%20increases%20anchor%0Aresolution%20based%20on%20motion%20complexity%20and%20propagates%20multi-level%0Atransformations.%20Extensive%20experiments%20on%20synthetic%20and%20real-world%20benchmarks%0Avalidate%20that%20HAIF-GS%20significantly%20outperforms%20prior%20dynamic%203DGS%20methods%20in%0Arendering%20quality%2C%20temporal%20coherence%2C%20and%20reconstruction%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09518v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAIF-GS%253A%2520Hierarchical%2520and%2520Induced%2520Flow-Guided%2520Gaussian%2520Splatting%2520for%250A%2520%2520Dynamic%2520Scene%26entry.906535625%3DJianing%2520Chen%2520and%2520Zehao%2520Li%2520and%2520Yujun%2520Cai%2520and%2520Hao%2520Jiang%2520and%2520Chengxuan%2520Qian%2520and%2520Juyuan%2520Kang%2520and%2520Shuqin%2520Gao%2520and%2520Honglong%2520Zhao%2520and%2520Tianlu%2520Mao%2520and%2520Yucheng%2520Zhang%26entry.1292438233%3D%2520%2520Reconstructing%2520dynamic%25203D%2520scenes%2520from%2520monocular%2520videos%2520remains%2520a%2520fundamental%250Achallenge%2520in%25203D%2520vision.%2520While%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520achieves%2520real-time%250Arendering%2520in%2520static%2520settings%252C%2520extending%2520it%2520to%2520dynamic%2520scenes%2520is%2520challenging%2520due%250Ato%2520the%2520difficulty%2520of%2520learning%2520structured%2520and%2520temporally%2520consistent%2520motion%250Arepresentations.%2520This%2520challenge%2520often%2520manifests%2520as%2520three%2520limitations%2520in%250Aexisting%2520methods%253A%2520redundant%2520Gaussian%2520updates%252C%2520insufficient%2520motion%2520supervision%252C%250Aand%2520weak%2520modeling%2520of%2520complex%2520non-rigid%2520deformations.%2520These%2520issues%2520collectively%250Ahinder%2520coherent%2520and%2520efficient%2520dynamic%2520reconstruction.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520HAIF-GS%252C%2520a%2520unified%2520framework%2520that%2520enables%2520structured%250Aand%2520consistent%2520dynamic%2520modeling%2520through%2520sparse%2520anchor-driven%2520deformation.%2520It%250Afirst%2520identifies%2520motion-relevant%2520regions%2520via%2520an%2520Anchor%2520Filter%2520to%2520suppress%250Aredundant%2520updates%2520in%2520static%2520areas.%2520A%2520self-supervised%2520Induced%2520Flow-Guided%250ADeformation%2520module%2520induces%2520anchor%2520motion%2520using%2520multi-frame%2520feature%2520aggregation%252C%250Aeliminating%2520the%2520need%2520for%2520explicit%2520flow%2520labels.%2520To%2520further%2520handle%2520fine-grained%250Adeformations%252C%2520a%2520Hierarchical%2520Anchor%2520Propagation%2520mechanism%2520increases%2520anchor%250Aresolution%2520based%2520on%2520motion%2520complexity%2520and%2520propagates%2520multi-level%250Atransformations.%2520Extensive%2520experiments%2520on%2520synthetic%2520and%2520real-world%2520benchmarks%250Avalidate%2520that%2520HAIF-GS%2520significantly%2520outperforms%2520prior%2520dynamic%25203DGS%2520methods%2520in%250Arendering%2520quality%252C%2520temporal%2520coherence%252C%2520and%2520reconstruction%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09518v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAIF-GS%3A%20Hierarchical%20and%20Induced%20Flow-Guided%20Gaussian%20Splatting%20for%0A%20%20Dynamic%20Scene&entry.906535625=Jianing%20Chen%20and%20Zehao%20Li%20and%20Yujun%20Cai%20and%20Hao%20Jiang%20and%20Chengxuan%20Qian%20and%20Juyuan%20Kang%20and%20Shuqin%20Gao%20and%20Honglong%20Zhao%20and%20Tianlu%20Mao%20and%20Yucheng%20Zhang&entry.1292438233=%20%20Reconstructing%20dynamic%203D%20scenes%20from%20monocular%20videos%20remains%20a%20fundamental%0Achallenge%20in%203D%20vision.%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20achieves%20real-time%0Arendering%20in%20static%20settings%2C%20extending%20it%20to%20dynamic%20scenes%20is%20challenging%20due%0Ato%20the%20difficulty%20of%20learning%20structured%20and%20temporally%20consistent%20motion%0Arepresentations.%20This%20challenge%20often%20manifests%20as%20three%20limitations%20in%0Aexisting%20methods%3A%20redundant%20Gaussian%20updates%2C%20insufficient%20motion%20supervision%2C%0Aand%20weak%20modeling%20of%20complex%20non-rigid%20deformations.%20These%20issues%20collectively%0Ahinder%20coherent%20and%20efficient%20dynamic%20reconstruction.%20To%20address%20these%0Alimitations%2C%20we%20propose%20HAIF-GS%2C%20a%20unified%20framework%20that%20enables%20structured%0Aand%20consistent%20dynamic%20modeling%20through%20sparse%20anchor-driven%20deformation.%20It%0Afirst%20identifies%20motion-relevant%20regions%20via%20an%20Anchor%20Filter%20to%20suppress%0Aredundant%20updates%20in%20static%20areas.%20A%20self-supervised%20Induced%20Flow-Guided%0ADeformation%20module%20induces%20anchor%20motion%20using%20multi-frame%20feature%20aggregation%2C%0Aeliminating%20the%20need%20for%20explicit%20flow%20labels.%20To%20further%20handle%20fine-grained%0Adeformations%2C%20a%20Hierarchical%20Anchor%20Propagation%20mechanism%20increases%20anchor%0Aresolution%20based%20on%20motion%20complexity%20and%20propagates%20multi-level%0Atransformations.%20Extensive%20experiments%20on%20synthetic%20and%20real-world%20benchmarks%0Avalidate%20that%20HAIF-GS%20significantly%20outperforms%20prior%20dynamic%203DGS%20methods%20in%0Arendering%20quality%2C%20temporal%20coherence%2C%20and%20reconstruction%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09518v2&entry.124074799=Read"},
{"title": "FreeArt3D: Training-Free Articulated Object Generation using 3D\n  Diffusion", "author": "Chuhao Chen and Isabella Liu and Xinyue Wei and Hao Su and Minghua Liu", "abstract": "  Articulated 3D objects are central to many applications in robotics, AR/VR,\nand animation. Recent approaches to modeling such objects either rely on\noptimization-based reconstruction pipelines that require dense-view supervision\nor on feed-forward generative models that produce coarse geometric\napproximations and often overlook surface texture. In contrast, open-world 3D\ngeneration of static objects has achieved remarkable success, especially with\nthe advent of native 3D diffusion models such as Trellis. However, extending\nthese methods to articulated objects by training native 3D diffusion models\nposes significant challenges. In this work, we present FreeArt3D, a\ntraining-free framework for articulated 3D object generation. Instead of\ntraining a new model on limited articulated data, FreeArt3D repurposes a\npre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape\nprior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by\ntreating articulation as an additional generative dimension. Given a few images\ncaptured in different articulation states, FreeArt3D jointly optimizes the\nobject's geometry, texture, and articulation parameters without requiring\ntask-specific training or access to large-scale articulated datasets. Our\nmethod generates high-fidelity geometry and textures, accurately predicts\nunderlying kinematic structures, and generalizes well across diverse object\ncategories. Despite following a per-instance optimization paradigm, FreeArt3D\ncompletes in minutes and significantly outperforms prior state-of-the-art\napproaches in both quality and versatility.\n", "link": "http://arxiv.org/abs/2510.25765v1", "date": "2025-10-29", "relevancy": 3.2493, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6541}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6477}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeArt3D%3A%20Training-Free%20Articulated%20Object%20Generation%20using%203D%0A%20%20Diffusion&body=Title%3A%20FreeArt3D%3A%20Training-Free%20Articulated%20Object%20Generation%20using%203D%0A%20%20Diffusion%0AAuthor%3A%20Chuhao%20Chen%20and%20Isabella%20Liu%20and%20Xinyue%20Wei%20and%20Hao%20Su%20and%20Minghua%20Liu%0AAbstract%3A%20%20%20Articulated%203D%20objects%20are%20central%20to%20many%20applications%20in%20robotics%2C%20AR/VR%2C%0Aand%20animation.%20Recent%20approaches%20to%20modeling%20such%20objects%20either%20rely%20on%0Aoptimization-based%20reconstruction%20pipelines%20that%20require%20dense-view%20supervision%0Aor%20on%20feed-forward%20generative%20models%20that%20produce%20coarse%20geometric%0Aapproximations%20and%20often%20overlook%20surface%20texture.%20In%20contrast%2C%20open-world%203D%0Ageneration%20of%20static%20objects%20has%20achieved%20remarkable%20success%2C%20especially%20with%0Athe%20advent%20of%20native%203D%20diffusion%20models%20such%20as%20Trellis.%20However%2C%20extending%0Athese%20methods%20to%20articulated%20objects%20by%20training%20native%203D%20diffusion%20models%0Aposes%20significant%20challenges.%20In%20this%20work%2C%20we%20present%20FreeArt3D%2C%20a%0Atraining-free%20framework%20for%20articulated%203D%20object%20generation.%20Instead%20of%0Atraining%20a%20new%20model%20on%20limited%20articulated%20data%2C%20FreeArt3D%20repurposes%20a%0Apre-trained%20static%203D%20diffusion%20model%20%28e.g.%2C%20Trellis%29%20as%20a%20powerful%20shape%0Aprior.%20It%20extends%20Score%20Distillation%20Sampling%20%28SDS%29%20into%20the%203D-to-4D%20domain%20by%0Atreating%20articulation%20as%20an%20additional%20generative%20dimension.%20Given%20a%20few%20images%0Acaptured%20in%20different%20articulation%20states%2C%20FreeArt3D%20jointly%20optimizes%20the%0Aobject%27s%20geometry%2C%20texture%2C%20and%20articulation%20parameters%20without%20requiring%0Atask-specific%20training%20or%20access%20to%20large-scale%20articulated%20datasets.%20Our%0Amethod%20generates%20high-fidelity%20geometry%20and%20textures%2C%20accurately%20predicts%0Aunderlying%20kinematic%20structures%2C%20and%20generalizes%20well%20across%20diverse%20object%0Acategories.%20Despite%20following%20a%20per-instance%20optimization%20paradigm%2C%20FreeArt3D%0Acompletes%20in%20minutes%20and%20significantly%20outperforms%20prior%20state-of-the-art%0Aapproaches%20in%20both%20quality%20and%20versatility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeArt3D%253A%2520Training-Free%2520Articulated%2520Object%2520Generation%2520using%25203D%250A%2520%2520Diffusion%26entry.906535625%3DChuhao%2520Chen%2520and%2520Isabella%2520Liu%2520and%2520Xinyue%2520Wei%2520and%2520Hao%2520Su%2520and%2520Minghua%2520Liu%26entry.1292438233%3D%2520%2520Articulated%25203D%2520objects%2520are%2520central%2520to%2520many%2520applications%2520in%2520robotics%252C%2520AR/VR%252C%250Aand%2520animation.%2520Recent%2520approaches%2520to%2520modeling%2520such%2520objects%2520either%2520rely%2520on%250Aoptimization-based%2520reconstruction%2520pipelines%2520that%2520require%2520dense-view%2520supervision%250Aor%2520on%2520feed-forward%2520generative%2520models%2520that%2520produce%2520coarse%2520geometric%250Aapproximations%2520and%2520often%2520overlook%2520surface%2520texture.%2520In%2520contrast%252C%2520open-world%25203D%250Ageneration%2520of%2520static%2520objects%2520has%2520achieved%2520remarkable%2520success%252C%2520especially%2520with%250Athe%2520advent%2520of%2520native%25203D%2520diffusion%2520models%2520such%2520as%2520Trellis.%2520However%252C%2520extending%250Athese%2520methods%2520to%2520articulated%2520objects%2520by%2520training%2520native%25203D%2520diffusion%2520models%250Aposes%2520significant%2520challenges.%2520In%2520this%2520work%252C%2520we%2520present%2520FreeArt3D%252C%2520a%250Atraining-free%2520framework%2520for%2520articulated%25203D%2520object%2520generation.%2520Instead%2520of%250Atraining%2520a%2520new%2520model%2520on%2520limited%2520articulated%2520data%252C%2520FreeArt3D%2520repurposes%2520a%250Apre-trained%2520static%25203D%2520diffusion%2520model%2520%2528e.g.%252C%2520Trellis%2529%2520as%2520a%2520powerful%2520shape%250Aprior.%2520It%2520extends%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520into%2520the%25203D-to-4D%2520domain%2520by%250Atreating%2520articulation%2520as%2520an%2520additional%2520generative%2520dimension.%2520Given%2520a%2520few%2520images%250Acaptured%2520in%2520different%2520articulation%2520states%252C%2520FreeArt3D%2520jointly%2520optimizes%2520the%250Aobject%2527s%2520geometry%252C%2520texture%252C%2520and%2520articulation%2520parameters%2520without%2520requiring%250Atask-specific%2520training%2520or%2520access%2520to%2520large-scale%2520articulated%2520datasets.%2520Our%250Amethod%2520generates%2520high-fidelity%2520geometry%2520and%2520textures%252C%2520accurately%2520predicts%250Aunderlying%2520kinematic%2520structures%252C%2520and%2520generalizes%2520well%2520across%2520diverse%2520object%250Acategories.%2520Despite%2520following%2520a%2520per-instance%2520optimization%2520paradigm%252C%2520FreeArt3D%250Acompletes%2520in%2520minutes%2520and%2520significantly%2520outperforms%2520prior%2520state-of-the-art%250Aapproaches%2520in%2520both%2520quality%2520and%2520versatility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeArt3D%3A%20Training-Free%20Articulated%20Object%20Generation%20using%203D%0A%20%20Diffusion&entry.906535625=Chuhao%20Chen%20and%20Isabella%20Liu%20and%20Xinyue%20Wei%20and%20Hao%20Su%20and%20Minghua%20Liu&entry.1292438233=%20%20Articulated%203D%20objects%20are%20central%20to%20many%20applications%20in%20robotics%2C%20AR/VR%2C%0Aand%20animation.%20Recent%20approaches%20to%20modeling%20such%20objects%20either%20rely%20on%0Aoptimization-based%20reconstruction%20pipelines%20that%20require%20dense-view%20supervision%0Aor%20on%20feed-forward%20generative%20models%20that%20produce%20coarse%20geometric%0Aapproximations%20and%20often%20overlook%20surface%20texture.%20In%20contrast%2C%20open-world%203D%0Ageneration%20of%20static%20objects%20has%20achieved%20remarkable%20success%2C%20especially%20with%0Athe%20advent%20of%20native%203D%20diffusion%20models%20such%20as%20Trellis.%20However%2C%20extending%0Athese%20methods%20to%20articulated%20objects%20by%20training%20native%203D%20diffusion%20models%0Aposes%20significant%20challenges.%20In%20this%20work%2C%20we%20present%20FreeArt3D%2C%20a%0Atraining-free%20framework%20for%20articulated%203D%20object%20generation.%20Instead%20of%0Atraining%20a%20new%20model%20on%20limited%20articulated%20data%2C%20FreeArt3D%20repurposes%20a%0Apre-trained%20static%203D%20diffusion%20model%20%28e.g.%2C%20Trellis%29%20as%20a%20powerful%20shape%0Aprior.%20It%20extends%20Score%20Distillation%20Sampling%20%28SDS%29%20into%20the%203D-to-4D%20domain%20by%0Atreating%20articulation%20as%20an%20additional%20generative%20dimension.%20Given%20a%20few%20images%0Acaptured%20in%20different%20articulation%20states%2C%20FreeArt3D%20jointly%20optimizes%20the%0Aobject%27s%20geometry%2C%20texture%2C%20and%20articulation%20parameters%20without%20requiring%0Atask-specific%20training%20or%20access%20to%20large-scale%20articulated%20datasets.%20Our%0Amethod%20generates%20high-fidelity%20geometry%20and%20textures%2C%20accurately%20predicts%0Aunderlying%20kinematic%20structures%2C%20and%20generalizes%20well%20across%20diverse%20object%0Acategories.%20Despite%20following%20a%20per-instance%20optimization%20paradigm%2C%20FreeArt3D%0Acompletes%20in%20minutes%20and%20significantly%20outperforms%20prior%20state-of-the-art%0Aapproaches%20in%20both%20quality%20and%20versatility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25765v1&entry.124074799=Read"},
{"title": "Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with\n  Multimodal Large Language Model in Open Space", "author": "Weichen Zhang and Zile Zhou and Xin Zeng and Xuchen Liu and Jianjie Fang and Chen Gao and Yong Li and Jinqiang Cui and Xinlei Chen and Xiao-Ping Zhang", "abstract": "  Spatial reasoning is a fundamental capability of multimodal large language\nmodels (MLLMs), yet their performance in open aerial environments remains\nunderexplored. In this work, we present Open3D-VQA, a novel benchmark for\nevaluating MLLMs' ability to reason about complex spatial relationships from an\naerial perspective. The benchmark comprises 73k QA pairs spanning 7 general\nspatial reasoning tasks, including multiple-choice, true/false, and\nshort-answer formats, and supports both visual and point cloud modalities. The\nquestions are automatically generated from spatial relations extracted from\nboth real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs\nreveals that: 1) Models are generally better at answering questions about\nrelative spatial relations than absolute distances, 2) 3D LLMs fail to\ndemonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on\nthe simulated dataset can significantly improve the model's spatial reasoning\nperformance in real-world scenarios. We release our benchmark, data generation\npipeline, and evaluation toolkit to support further research:\nhttps://github.com/EmbodiedCity/Open3D-VQA.code.\n", "link": "http://arxiv.org/abs/2503.11094v3", "date": "2025-10-29", "relevancy": 3.057, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6375}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6375}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open3D-VQA%3A%20A%20Benchmark%20for%20Comprehensive%20Spatial%20Reasoning%20with%0A%20%20Multimodal%20Large%20Language%20Model%20in%20Open%20Space&body=Title%3A%20Open3D-VQA%3A%20A%20Benchmark%20for%20Comprehensive%20Spatial%20Reasoning%20with%0A%20%20Multimodal%20Large%20Language%20Model%20in%20Open%20Space%0AAuthor%3A%20Weichen%20Zhang%20and%20Zile%20Zhou%20and%20Xin%20Zeng%20and%20Xuchen%20Liu%20and%20Jianjie%20Fang%20and%20Chen%20Gao%20and%20Yong%20Li%20and%20Jinqiang%20Cui%20and%20Xinlei%20Chen%20and%20Xiao-Ping%20Zhang%0AAbstract%3A%20%20%20Spatial%20reasoning%20is%20a%20fundamental%20capability%20of%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%2C%20yet%20their%20performance%20in%20open%20aerial%20environments%20remains%0Aunderexplored.%20In%20this%20work%2C%20we%20present%20Open3D-VQA%2C%20a%20novel%20benchmark%20for%0Aevaluating%20MLLMs%27%20ability%20to%20reason%20about%20complex%20spatial%20relationships%20from%20an%0Aaerial%20perspective.%20The%20benchmark%20comprises%2073k%20QA%20pairs%20spanning%207%20general%0Aspatial%20reasoning%20tasks%2C%20including%20multiple-choice%2C%20true/false%2C%20and%0Ashort-answer%20formats%2C%20and%20supports%20both%20visual%20and%20point%20cloud%20modalities.%20The%0Aquestions%20are%20automatically%20generated%20from%20spatial%20relations%20extracted%20from%0Aboth%20real-world%20and%20simulated%20aerial%20scenes.%20Evaluation%20on%2013%20popular%20MLLMs%0Areveals%20that%3A%201%29%20Models%20are%20generally%20better%20at%20answering%20questions%20about%0Arelative%20spatial%20relations%20than%20absolute%20distances%2C%202%29%203D%20LLMs%20fail%20to%0Ademonstrate%20significant%20advantages%20over%202D%20LLMs%2C%20and%203%29%20Fine-tuning%20solely%20on%0Athe%20simulated%20dataset%20can%20significantly%20improve%20the%20model%27s%20spatial%20reasoning%0Aperformance%20in%20real-world%20scenarios.%20We%20release%20our%20benchmark%2C%20data%20generation%0Apipeline%2C%20and%20evaluation%20toolkit%20to%20support%20further%20research%3A%0Ahttps%3A//github.com/EmbodiedCity/Open3D-VQA.code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.11094v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen3D-VQA%253A%2520A%2520Benchmark%2520for%2520Comprehensive%2520Spatial%2520Reasoning%2520with%250A%2520%2520Multimodal%2520Large%2520Language%2520Model%2520in%2520Open%2520Space%26entry.906535625%3DWeichen%2520Zhang%2520and%2520Zile%2520Zhou%2520and%2520Xin%2520Zeng%2520and%2520Xuchen%2520Liu%2520and%2520Jianjie%2520Fang%2520and%2520Chen%2520Gao%2520and%2520Yong%2520Li%2520and%2520Jinqiang%2520Cui%2520and%2520Xinlei%2520Chen%2520and%2520Xiao-Ping%2520Zhang%26entry.1292438233%3D%2520%2520Spatial%2520reasoning%2520is%2520a%2520fundamental%2520capability%2520of%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529%252C%2520yet%2520their%2520performance%2520in%2520open%2520aerial%2520environments%2520remains%250Aunderexplored.%2520In%2520this%2520work%252C%2520we%2520present%2520Open3D-VQA%252C%2520a%2520novel%2520benchmark%2520for%250Aevaluating%2520MLLMs%2527%2520ability%2520to%2520reason%2520about%2520complex%2520spatial%2520relationships%2520from%2520an%250Aaerial%2520perspective.%2520The%2520benchmark%2520comprises%252073k%2520QA%2520pairs%2520spanning%25207%2520general%250Aspatial%2520reasoning%2520tasks%252C%2520including%2520multiple-choice%252C%2520true/false%252C%2520and%250Ashort-answer%2520formats%252C%2520and%2520supports%2520both%2520visual%2520and%2520point%2520cloud%2520modalities.%2520The%250Aquestions%2520are%2520automatically%2520generated%2520from%2520spatial%2520relations%2520extracted%2520from%250Aboth%2520real-world%2520and%2520simulated%2520aerial%2520scenes.%2520Evaluation%2520on%252013%2520popular%2520MLLMs%250Areveals%2520that%253A%25201%2529%2520Models%2520are%2520generally%2520better%2520at%2520answering%2520questions%2520about%250Arelative%2520spatial%2520relations%2520than%2520absolute%2520distances%252C%25202%2529%25203D%2520LLMs%2520fail%2520to%250Ademonstrate%2520significant%2520advantages%2520over%25202D%2520LLMs%252C%2520and%25203%2529%2520Fine-tuning%2520solely%2520on%250Athe%2520simulated%2520dataset%2520can%2520significantly%2520improve%2520the%2520model%2527s%2520spatial%2520reasoning%250Aperformance%2520in%2520real-world%2520scenarios.%2520We%2520release%2520our%2520benchmark%252C%2520data%2520generation%250Apipeline%252C%2520and%2520evaluation%2520toolkit%2520to%2520support%2520further%2520research%253A%250Ahttps%253A//github.com/EmbodiedCity/Open3D-VQA.code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.11094v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open3D-VQA%3A%20A%20Benchmark%20for%20Comprehensive%20Spatial%20Reasoning%20with%0A%20%20Multimodal%20Large%20Language%20Model%20in%20Open%20Space&entry.906535625=Weichen%20Zhang%20and%20Zile%20Zhou%20and%20Xin%20Zeng%20and%20Xuchen%20Liu%20and%20Jianjie%20Fang%20and%20Chen%20Gao%20and%20Yong%20Li%20and%20Jinqiang%20Cui%20and%20Xinlei%20Chen%20and%20Xiao-Ping%20Zhang&entry.1292438233=%20%20Spatial%20reasoning%20is%20a%20fundamental%20capability%20of%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%2C%20yet%20their%20performance%20in%20open%20aerial%20environments%20remains%0Aunderexplored.%20In%20this%20work%2C%20we%20present%20Open3D-VQA%2C%20a%20novel%20benchmark%20for%0Aevaluating%20MLLMs%27%20ability%20to%20reason%20about%20complex%20spatial%20relationships%20from%20an%0Aaerial%20perspective.%20The%20benchmark%20comprises%2073k%20QA%20pairs%20spanning%207%20general%0Aspatial%20reasoning%20tasks%2C%20including%20multiple-choice%2C%20true/false%2C%20and%0Ashort-answer%20formats%2C%20and%20supports%20both%20visual%20and%20point%20cloud%20modalities.%20The%0Aquestions%20are%20automatically%20generated%20from%20spatial%20relations%20extracted%20from%0Aboth%20real-world%20and%20simulated%20aerial%20scenes.%20Evaluation%20on%2013%20popular%20MLLMs%0Areveals%20that%3A%201%29%20Models%20are%20generally%20better%20at%20answering%20questions%20about%0Arelative%20spatial%20relations%20than%20absolute%20distances%2C%202%29%203D%20LLMs%20fail%20to%0Ademonstrate%20significant%20advantages%20over%202D%20LLMs%2C%20and%203%29%20Fine-tuning%20solely%20on%0Athe%20simulated%20dataset%20can%20significantly%20improve%20the%20model%27s%20spatial%20reasoning%0Aperformance%20in%20real-world%20scenarios.%20We%20release%20our%20benchmark%2C%20data%20generation%0Apipeline%2C%20and%20evaluation%20toolkit%20to%20support%20further%20research%3A%0Ahttps%3A//github.com/EmbodiedCity/Open3D-VQA.code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.11094v3&entry.124074799=Read"},
{"title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering", "author": "Liangyu Zhong and Fabio Rosenthal and Joachim Sicking and Fabian H\u00fcger and Thorsten Bagdonat and Hanno Gottschalk and Leo Schwinn", "abstract": "  While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.\n", "link": "http://arxiv.org/abs/2506.21710v2", "date": "2025-10-29", "relevancy": 2.8968, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5956}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOCUS%3A%20Internal%20MLLM%20Representations%20for%20Efficient%20Fine-Grained%20Visual%0A%20%20Question%20Answering&body=Title%3A%20FOCUS%3A%20Internal%20MLLM%20Representations%20for%20Efficient%20Fine-Grained%20Visual%0A%20%20Question%20Answering%0AAuthor%3A%20Liangyu%20Zhong%20and%20Fabio%20Rosenthal%20and%20Joachim%20Sicking%20and%20Fabian%20H%C3%BCger%20and%20Thorsten%20Bagdonat%20and%20Hanno%20Gottschalk%20and%20Leo%20Schwinn%0AAbstract%3A%20%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20strong%20perception%20and%0Areasoning%20capabilities%20for%20image-text%20input%2C%20Visual%20Question%20Answering%20%28VQA%29%0Afocusing%20on%20small%20image%20details%20still%20remains%20a%20challenge.%20Although%20visual%0Acropping%20techniques%20seem%20promising%2C%20recent%20approaches%20have%20several%20limitations%3A%0Athe%20need%20for%20task-specific%20fine-tuning%2C%20low%20efficiency%20due%20to%20uninformed%0Aexhaustive%20search%2C%20or%20incompatibility%20with%20efficient%20attention%20implementations.%0AWe%20address%20these%20shortcomings%20by%20proposing%20a%20training-free%20visual%20cropping%0Amethod%2C%20dubbed%20FOCUS%2C%20that%20leverages%20MLLM-internal%20representations%20to%20guide%20the%0Asearch%20for%20the%20most%20relevant%20image%20region.%20This%20is%20accomplished%20in%20four%20steps%3A%0Afirst%2C%20we%20identify%20the%20target%20object%28s%29%20in%20the%20VQA%20prompt%3B%20second%2C%20we%20compute%0Aan%20object%20relevance%20map%20using%20the%20key-value%20%28KV%29%20cache%3B%20third%2C%20we%20propose%20and%0Arank%20relevant%20image%20regions%20based%20on%20the%20map%3B%20and%20finally%2C%20we%20perform%20the%0Afine-grained%20VQA%20task%20using%20the%20top-ranked%20region.%20As%20a%20result%20of%20this%20informed%0Asearch%20strategy%2C%20FOCUS%20achieves%20strong%20performance%20across%20four%20fine-grained%20VQA%0Adatasets%20and%20three%20types%20of%20MLLMs.%20It%20outperforms%20three%20popular%20visual%20cropping%0Amethods%20in%20both%20accuracy%20and%20efficiency%2C%20and%20matches%20the%20best-performing%0Abaseline%2C%20ZoomEye%2C%20while%20requiring%203%20-%206.5%20x%20less%20compute.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21710v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOCUS%253A%2520Internal%2520MLLM%2520Representations%2520for%2520Efficient%2520Fine-Grained%2520Visual%250A%2520%2520Question%2520Answering%26entry.906535625%3DLiangyu%2520Zhong%2520and%2520Fabio%2520Rosenthal%2520and%2520Joachim%2520Sicking%2520and%2520Fabian%2520H%25C3%25BCger%2520and%2520Thorsten%2520Bagdonat%2520and%2520Hanno%2520Gottschalk%2520and%2520Leo%2520Schwinn%26entry.1292438233%3D%2520%2520While%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520offer%2520strong%2520perception%2520and%250Areasoning%2520capabilities%2520for%2520image-text%2520input%252C%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%250Afocusing%2520on%2520small%2520image%2520details%2520still%2520remains%2520a%2520challenge.%2520Although%2520visual%250Acropping%2520techniques%2520seem%2520promising%252C%2520recent%2520approaches%2520have%2520several%2520limitations%253A%250Athe%2520need%2520for%2520task-specific%2520fine-tuning%252C%2520low%2520efficiency%2520due%2520to%2520uninformed%250Aexhaustive%2520search%252C%2520or%2520incompatibility%2520with%2520efficient%2520attention%2520implementations.%250AWe%2520address%2520these%2520shortcomings%2520by%2520proposing%2520a%2520training-free%2520visual%2520cropping%250Amethod%252C%2520dubbed%2520FOCUS%252C%2520that%2520leverages%2520MLLM-internal%2520representations%2520to%2520guide%2520the%250Asearch%2520for%2520the%2520most%2520relevant%2520image%2520region.%2520This%2520is%2520accomplished%2520in%2520four%2520steps%253A%250Afirst%252C%2520we%2520identify%2520the%2520target%2520object%2528s%2529%2520in%2520the%2520VQA%2520prompt%253B%2520second%252C%2520we%2520compute%250Aan%2520object%2520relevance%2520map%2520using%2520the%2520key-value%2520%2528KV%2529%2520cache%253B%2520third%252C%2520we%2520propose%2520and%250Arank%2520relevant%2520image%2520regions%2520based%2520on%2520the%2520map%253B%2520and%2520finally%252C%2520we%2520perform%2520the%250Afine-grained%2520VQA%2520task%2520using%2520the%2520top-ranked%2520region.%2520As%2520a%2520result%2520of%2520this%2520informed%250Asearch%2520strategy%252C%2520FOCUS%2520achieves%2520strong%2520performance%2520across%2520four%2520fine-grained%2520VQA%250Adatasets%2520and%2520three%2520types%2520of%2520MLLMs.%2520It%2520outperforms%2520three%2520popular%2520visual%2520cropping%250Amethods%2520in%2520both%2520accuracy%2520and%2520efficiency%252C%2520and%2520matches%2520the%2520best-performing%250Abaseline%252C%2520ZoomEye%252C%2520while%2520requiring%25203%2520-%25206.5%2520x%2520less%2520compute.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21710v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOCUS%3A%20Internal%20MLLM%20Representations%20for%20Efficient%20Fine-Grained%20Visual%0A%20%20Question%20Answering&entry.906535625=Liangyu%20Zhong%20and%20Fabio%20Rosenthal%20and%20Joachim%20Sicking%20and%20Fabian%20H%C3%BCger%20and%20Thorsten%20Bagdonat%20and%20Hanno%20Gottschalk%20and%20Leo%20Schwinn&entry.1292438233=%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20strong%20perception%20and%0Areasoning%20capabilities%20for%20image-text%20input%2C%20Visual%20Question%20Answering%20%28VQA%29%0Afocusing%20on%20small%20image%20details%20still%20remains%20a%20challenge.%20Although%20visual%0Acropping%20techniques%20seem%20promising%2C%20recent%20approaches%20have%20several%20limitations%3A%0Athe%20need%20for%20task-specific%20fine-tuning%2C%20low%20efficiency%20due%20to%20uninformed%0Aexhaustive%20search%2C%20or%20incompatibility%20with%20efficient%20attention%20implementations.%0AWe%20address%20these%20shortcomings%20by%20proposing%20a%20training-free%20visual%20cropping%0Amethod%2C%20dubbed%20FOCUS%2C%20that%20leverages%20MLLM-internal%20representations%20to%20guide%20the%0Asearch%20for%20the%20most%20relevant%20image%20region.%20This%20is%20accomplished%20in%20four%20steps%3A%0Afirst%2C%20we%20identify%20the%20target%20object%28s%29%20in%20the%20VQA%20prompt%3B%20second%2C%20we%20compute%0Aan%20object%20relevance%20map%20using%20the%20key-value%20%28KV%29%20cache%3B%20third%2C%20we%20propose%20and%0Arank%20relevant%20image%20regions%20based%20on%20the%20map%3B%20and%20finally%2C%20we%20perform%20the%0Afine-grained%20VQA%20task%20using%20the%20top-ranked%20region.%20As%20a%20result%20of%20this%20informed%0Asearch%20strategy%2C%20FOCUS%20achieves%20strong%20performance%20across%20four%20fine-grained%20VQA%0Adatasets%20and%20three%20types%20of%20MLLMs.%20It%20outperforms%20three%20popular%20visual%20cropping%0Amethods%20in%20both%20accuracy%20and%20efficiency%2C%20and%20matches%20the%20best-performing%0Abaseline%2C%20ZoomEye%2C%20while%20requiring%203%20-%206.5%20x%20less%20compute.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21710v2&entry.124074799=Read"},
{"title": "Don't Blind Your VLA: Aligning Visual Representations for OOD\n  Generalization", "author": "Nikita Kachaev and Mikhail Kolosov and Daniil Zelezetsky and Alexey K. Kovalev and Aleksandr I. Panov", "abstract": "  The growing success of Vision-Language-Action (VLA) models stems from the\npromise that pretrained Vision-Language Models (VLMs) can endow agents with\ntransferable world knowledge and vision-language (VL) grounding, laying a\nfoundation for action models with broader generalization. Yet when these VLMs\nare adapted to the action modality, it remains unclear to what extent their\noriginal VL representations and knowledge are preserved. In this work, we\nconduct a systematic study of representation retention during VLA fine-tuning,\nshowing that naive action fine-tuning leads to degradation of visual\nrepresentations. To characterize and measure these effects, we probe VLA's\nhidden representations and analyze attention maps, further, we design a set of\ntargeted tasks and methods that contrast VLA models with their counterpart\nVLMs, isolating changes in VL capabilities induced by action fine-tuning. We\nfurther evaluate a range of strategies for aligning visual representations and\nintroduce a simple yet effective method that mitigates degradation and yields\nimproved generalization to out-of-distribution (OOD) scenarios. Taken together,\nour analysis clarifies the trade-off between action fine-tuning and the\ndegradation of VL representations and highlights practical approaches to\nrecover inherited VL capabilities. Code is publicly available:\nhttps://blind-vla-paper.github.io\n", "link": "http://arxiv.org/abs/2510.25616v1", "date": "2025-10-29", "relevancy": 2.8005, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5639}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Blind%20Your%20VLA%3A%20Aligning%20Visual%20Representations%20for%20OOD%0A%20%20Generalization&body=Title%3A%20Don%27t%20Blind%20Your%20VLA%3A%20Aligning%20Visual%20Representations%20for%20OOD%0A%20%20Generalization%0AAuthor%3A%20Nikita%20Kachaev%20and%20Mikhail%20Kolosov%20and%20Daniil%20Zelezetsky%20and%20Alexey%20K.%20Kovalev%20and%20Aleksandr%20I.%20Panov%0AAbstract%3A%20%20%20The%20growing%20success%20of%20Vision-Language-Action%20%28VLA%29%20models%20stems%20from%20the%0Apromise%20that%20pretrained%20Vision-Language%20Models%20%28VLMs%29%20can%20endow%20agents%20with%0Atransferable%20world%20knowledge%20and%20vision-language%20%28VL%29%20grounding%2C%20laying%20a%0Afoundation%20for%20action%20models%20with%20broader%20generalization.%20Yet%20when%20these%20VLMs%0Aare%20adapted%20to%20the%20action%20modality%2C%20it%20remains%20unclear%20to%20what%20extent%20their%0Aoriginal%20VL%20representations%20and%20knowledge%20are%20preserved.%20In%20this%20work%2C%20we%0Aconduct%20a%20systematic%20study%20of%20representation%20retention%20during%20VLA%20fine-tuning%2C%0Ashowing%20that%20naive%20action%20fine-tuning%20leads%20to%20degradation%20of%20visual%0Arepresentations.%20To%20characterize%20and%20measure%20these%20effects%2C%20we%20probe%20VLA%27s%0Ahidden%20representations%20and%20analyze%20attention%20maps%2C%20further%2C%20we%20design%20a%20set%20of%0Atargeted%20tasks%20and%20methods%20that%20contrast%20VLA%20models%20with%20their%20counterpart%0AVLMs%2C%20isolating%20changes%20in%20VL%20capabilities%20induced%20by%20action%20fine-tuning.%20We%0Afurther%20evaluate%20a%20range%20of%20strategies%20for%20aligning%20visual%20representations%20and%0Aintroduce%20a%20simple%20yet%20effective%20method%20that%20mitigates%20degradation%20and%20yields%0Aimproved%20generalization%20to%20out-of-distribution%20%28OOD%29%20scenarios.%20Taken%20together%2C%0Aour%20analysis%20clarifies%20the%20trade-off%20between%20action%20fine-tuning%20and%20the%0Adegradation%20of%20VL%20representations%20and%20highlights%20practical%20approaches%20to%0Arecover%20inherited%20VL%20capabilities.%20Code%20is%20publicly%20available%3A%0Ahttps%3A//blind-vla-paper.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Blind%2520Your%2520VLA%253A%2520Aligning%2520Visual%2520Representations%2520for%2520OOD%250A%2520%2520Generalization%26entry.906535625%3DNikita%2520Kachaev%2520and%2520Mikhail%2520Kolosov%2520and%2520Daniil%2520Zelezetsky%2520and%2520Alexey%2520K.%2520Kovalev%2520and%2520Aleksandr%2520I.%2520Panov%26entry.1292438233%3D%2520%2520The%2520growing%2520success%2520of%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520stems%2520from%2520the%250Apromise%2520that%2520pretrained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520can%2520endow%2520agents%2520with%250Atransferable%2520world%2520knowledge%2520and%2520vision-language%2520%2528VL%2529%2520grounding%252C%2520laying%2520a%250Afoundation%2520for%2520action%2520models%2520with%2520broader%2520generalization.%2520Yet%2520when%2520these%2520VLMs%250Aare%2520adapted%2520to%2520the%2520action%2520modality%252C%2520it%2520remains%2520unclear%2520to%2520what%2520extent%2520their%250Aoriginal%2520VL%2520representations%2520and%2520knowledge%2520are%2520preserved.%2520In%2520this%2520work%252C%2520we%250Aconduct%2520a%2520systematic%2520study%2520of%2520representation%2520retention%2520during%2520VLA%2520fine-tuning%252C%250Ashowing%2520that%2520naive%2520action%2520fine-tuning%2520leads%2520to%2520degradation%2520of%2520visual%250Arepresentations.%2520To%2520characterize%2520and%2520measure%2520these%2520effects%252C%2520we%2520probe%2520VLA%2527s%250Ahidden%2520representations%2520and%2520analyze%2520attention%2520maps%252C%2520further%252C%2520we%2520design%2520a%2520set%2520of%250Atargeted%2520tasks%2520and%2520methods%2520that%2520contrast%2520VLA%2520models%2520with%2520their%2520counterpart%250AVLMs%252C%2520isolating%2520changes%2520in%2520VL%2520capabilities%2520induced%2520by%2520action%2520fine-tuning.%2520We%250Afurther%2520evaluate%2520a%2520range%2520of%2520strategies%2520for%2520aligning%2520visual%2520representations%2520and%250Aintroduce%2520a%2520simple%2520yet%2520effective%2520method%2520that%2520mitigates%2520degradation%2520and%2520yields%250Aimproved%2520generalization%2520to%2520out-of-distribution%2520%2528OOD%2529%2520scenarios.%2520Taken%2520together%252C%250Aour%2520analysis%2520clarifies%2520the%2520trade-off%2520between%2520action%2520fine-tuning%2520and%2520the%250Adegradation%2520of%2520VL%2520representations%2520and%2520highlights%2520practical%2520approaches%2520to%250Arecover%2520inherited%2520VL%2520capabilities.%2520Code%2520is%2520publicly%2520available%253A%250Ahttps%253A//blind-vla-paper.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Blind%20Your%20VLA%3A%20Aligning%20Visual%20Representations%20for%20OOD%0A%20%20Generalization&entry.906535625=Nikita%20Kachaev%20and%20Mikhail%20Kolosov%20and%20Daniil%20Zelezetsky%20and%20Alexey%20K.%20Kovalev%20and%20Aleksandr%20I.%20Panov&entry.1292438233=%20%20The%20growing%20success%20of%20Vision-Language-Action%20%28VLA%29%20models%20stems%20from%20the%0Apromise%20that%20pretrained%20Vision-Language%20Models%20%28VLMs%29%20can%20endow%20agents%20with%0Atransferable%20world%20knowledge%20and%20vision-language%20%28VL%29%20grounding%2C%20laying%20a%0Afoundation%20for%20action%20models%20with%20broader%20generalization.%20Yet%20when%20these%20VLMs%0Aare%20adapted%20to%20the%20action%20modality%2C%20it%20remains%20unclear%20to%20what%20extent%20their%0Aoriginal%20VL%20representations%20and%20knowledge%20are%20preserved.%20In%20this%20work%2C%20we%0Aconduct%20a%20systematic%20study%20of%20representation%20retention%20during%20VLA%20fine-tuning%2C%0Ashowing%20that%20naive%20action%20fine-tuning%20leads%20to%20degradation%20of%20visual%0Arepresentations.%20To%20characterize%20and%20measure%20these%20effects%2C%20we%20probe%20VLA%27s%0Ahidden%20representations%20and%20analyze%20attention%20maps%2C%20further%2C%20we%20design%20a%20set%20of%0Atargeted%20tasks%20and%20methods%20that%20contrast%20VLA%20models%20with%20their%20counterpart%0AVLMs%2C%20isolating%20changes%20in%20VL%20capabilities%20induced%20by%20action%20fine-tuning.%20We%0Afurther%20evaluate%20a%20range%20of%20strategies%20for%20aligning%20visual%20representations%20and%0Aintroduce%20a%20simple%20yet%20effective%20method%20that%20mitigates%20degradation%20and%20yields%0Aimproved%20generalization%20to%20out-of-distribution%20%28OOD%29%20scenarios.%20Taken%20together%2C%0Aour%20analysis%20clarifies%20the%20trade-off%20between%20action%20fine-tuning%20and%20the%0Adegradation%20of%20VL%20representations%20and%20highlights%20practical%20approaches%20to%0Arecover%20inherited%20VL%20capabilities.%20Code%20is%20publicly%20available%3A%0Ahttps%3A//blind-vla-paper.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25616v1&entry.124074799=Read"},
{"title": "Informative Sample Selection Model for Skeleton-based Action Recognition\n  with Limited Training Samples", "author": "Zhigang Tu and Zhengbo Zhang and Jia Gong and Junsong Yuan and Bo Du", "abstract": "  Skeleton-based human action recognition aims to classify human skeletal\nsequences, which are spatiotemporal representations of actions, into predefined\ncategories. To reduce the reliance on costly annotations of skeletal sequences\nwhile maintaining competitive recognition accuracy, the task of 3D Action\nRecognition with Limited Training Samples, also known as semi-supervised 3D\nAction Recognition, has been proposed. In addition, active learning, which aims\nto proactively select the most informative unlabeled samples for annotation,\nhas been explored in semi-supervised 3D Action Recognition for training sample\nselection. Specifically, researchers adopt an encoder-decoder framework to\nembed skeleton sequences into a latent space, where clustering information,\ncombined with a margin-based selection strategy using a multi-head mechanism,\nis utilized to identify the most informative sequences in the unlabeled set for\nannotation. However, the most representative skeleton sequences may not\nnecessarily be the most informative for the action recognizer, as the model may\nhave already acquired similar knowledge from previously seen skeleton samples.\nTo solve it, we reformulate Semi-supervised 3D action recognition via active\nlearning from a novel perspective by casting it as a Markov Decision Process\n(MDP). Built upon the MDP framework and its training paradigm, we train an\ninformative sample selection model to intelligently guide the selection of\nskeleton sequences for annotation. To enhance the representational capacity of\nthe factors in the state-action pairs within our method, we project them from\nEuclidean space to hyperbolic space. Furthermore, we introduce a meta tuning\nstrategy to accelerate the deployment of our method in real-world scenarios.\nExtensive experiments on three 3D action recognition benchmarks demonstrate the\neffectiveness of our method.\n", "link": "http://arxiv.org/abs/2510.25345v1", "date": "2025-10-29", "relevancy": 2.7503, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5562}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5507}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Informative%20Sample%20Selection%20Model%20for%20Skeleton-based%20Action%20Recognition%0A%20%20with%20Limited%20Training%20Samples&body=Title%3A%20Informative%20Sample%20Selection%20Model%20for%20Skeleton-based%20Action%20Recognition%0A%20%20with%20Limited%20Training%20Samples%0AAuthor%3A%20Zhigang%20Tu%20and%20Zhengbo%20Zhang%20and%20Jia%20Gong%20and%20Junsong%20Yuan%20and%20Bo%20Du%0AAbstract%3A%20%20%20Skeleton-based%20human%20action%20recognition%20aims%20to%20classify%20human%20skeletal%0Asequences%2C%20which%20are%20spatiotemporal%20representations%20of%20actions%2C%20into%20predefined%0Acategories.%20To%20reduce%20the%20reliance%20on%20costly%20annotations%20of%20skeletal%20sequences%0Awhile%20maintaining%20competitive%20recognition%20accuracy%2C%20the%20task%20of%203D%20Action%0ARecognition%20with%20Limited%20Training%20Samples%2C%20also%20known%20as%20semi-supervised%203D%0AAction%20Recognition%2C%20has%20been%20proposed.%20In%20addition%2C%20active%20learning%2C%20which%20aims%0Ato%20proactively%20select%20the%20most%20informative%20unlabeled%20samples%20for%20annotation%2C%0Ahas%20been%20explored%20in%20semi-supervised%203D%20Action%20Recognition%20for%20training%20sample%0Aselection.%20Specifically%2C%20researchers%20adopt%20an%20encoder-decoder%20framework%20to%0Aembed%20skeleton%20sequences%20into%20a%20latent%20space%2C%20where%20clustering%20information%2C%0Acombined%20with%20a%20margin-based%20selection%20strategy%20using%20a%20multi-head%20mechanism%2C%0Ais%20utilized%20to%20identify%20the%20most%20informative%20sequences%20in%20the%20unlabeled%20set%20for%0Aannotation.%20However%2C%20the%20most%20representative%20skeleton%20sequences%20may%20not%0Anecessarily%20be%20the%20most%20informative%20for%20the%20action%20recognizer%2C%20as%20the%20model%20may%0Ahave%20already%20acquired%20similar%20knowledge%20from%20previously%20seen%20skeleton%20samples.%0ATo%20solve%20it%2C%20we%20reformulate%20Semi-supervised%203D%20action%20recognition%20via%20active%0Alearning%20from%20a%20novel%20perspective%20by%20casting%20it%20as%20a%20Markov%20Decision%20Process%0A%28MDP%29.%20Built%20upon%20the%20MDP%20framework%20and%20its%20training%20paradigm%2C%20we%20train%20an%0Ainformative%20sample%20selection%20model%20to%20intelligently%20guide%20the%20selection%20of%0Askeleton%20sequences%20for%20annotation.%20To%20enhance%20the%20representational%20capacity%20of%0Athe%20factors%20in%20the%20state-action%20pairs%20within%20our%20method%2C%20we%20project%20them%20from%0AEuclidean%20space%20to%20hyperbolic%20space.%20Furthermore%2C%20we%20introduce%20a%20meta%20tuning%0Astrategy%20to%20accelerate%20the%20deployment%20of%20our%20method%20in%20real-world%20scenarios.%0AExtensive%20experiments%20on%20three%203D%20action%20recognition%20benchmarks%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformative%2520Sample%2520Selection%2520Model%2520for%2520Skeleton-based%2520Action%2520Recognition%250A%2520%2520with%2520Limited%2520Training%2520Samples%26entry.906535625%3DZhigang%2520Tu%2520and%2520Zhengbo%2520Zhang%2520and%2520Jia%2520Gong%2520and%2520Junsong%2520Yuan%2520and%2520Bo%2520Du%26entry.1292438233%3D%2520%2520Skeleton-based%2520human%2520action%2520recognition%2520aims%2520to%2520classify%2520human%2520skeletal%250Asequences%252C%2520which%2520are%2520spatiotemporal%2520representations%2520of%2520actions%252C%2520into%2520predefined%250Acategories.%2520To%2520reduce%2520the%2520reliance%2520on%2520costly%2520annotations%2520of%2520skeletal%2520sequences%250Awhile%2520maintaining%2520competitive%2520recognition%2520accuracy%252C%2520the%2520task%2520of%25203D%2520Action%250ARecognition%2520with%2520Limited%2520Training%2520Samples%252C%2520also%2520known%2520as%2520semi-supervised%25203D%250AAction%2520Recognition%252C%2520has%2520been%2520proposed.%2520In%2520addition%252C%2520active%2520learning%252C%2520which%2520aims%250Ato%2520proactively%2520select%2520the%2520most%2520informative%2520unlabeled%2520samples%2520for%2520annotation%252C%250Ahas%2520been%2520explored%2520in%2520semi-supervised%25203D%2520Action%2520Recognition%2520for%2520training%2520sample%250Aselection.%2520Specifically%252C%2520researchers%2520adopt%2520an%2520encoder-decoder%2520framework%2520to%250Aembed%2520skeleton%2520sequences%2520into%2520a%2520latent%2520space%252C%2520where%2520clustering%2520information%252C%250Acombined%2520with%2520a%2520margin-based%2520selection%2520strategy%2520using%2520a%2520multi-head%2520mechanism%252C%250Ais%2520utilized%2520to%2520identify%2520the%2520most%2520informative%2520sequences%2520in%2520the%2520unlabeled%2520set%2520for%250Aannotation.%2520However%252C%2520the%2520most%2520representative%2520skeleton%2520sequences%2520may%2520not%250Anecessarily%2520be%2520the%2520most%2520informative%2520for%2520the%2520action%2520recognizer%252C%2520as%2520the%2520model%2520may%250Ahave%2520already%2520acquired%2520similar%2520knowledge%2520from%2520previously%2520seen%2520skeleton%2520samples.%250ATo%2520solve%2520it%252C%2520we%2520reformulate%2520Semi-supervised%25203D%2520action%2520recognition%2520via%2520active%250Alearning%2520from%2520a%2520novel%2520perspective%2520by%2520casting%2520it%2520as%2520a%2520Markov%2520Decision%2520Process%250A%2528MDP%2529.%2520Built%2520upon%2520the%2520MDP%2520framework%2520and%2520its%2520training%2520paradigm%252C%2520we%2520train%2520an%250Ainformative%2520sample%2520selection%2520model%2520to%2520intelligently%2520guide%2520the%2520selection%2520of%250Askeleton%2520sequences%2520for%2520annotation.%2520To%2520enhance%2520the%2520representational%2520capacity%2520of%250Athe%2520factors%2520in%2520the%2520state-action%2520pairs%2520within%2520our%2520method%252C%2520we%2520project%2520them%2520from%250AEuclidean%2520space%2520to%2520hyperbolic%2520space.%2520Furthermore%252C%2520we%2520introduce%2520a%2520meta%2520tuning%250Astrategy%2520to%2520accelerate%2520the%2520deployment%2520of%2520our%2520method%2520in%2520real-world%2520scenarios.%250AExtensive%2520experiments%2520on%2520three%25203D%2520action%2520recognition%2520benchmarks%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Informative%20Sample%20Selection%20Model%20for%20Skeleton-based%20Action%20Recognition%0A%20%20with%20Limited%20Training%20Samples&entry.906535625=Zhigang%20Tu%20and%20Zhengbo%20Zhang%20and%20Jia%20Gong%20and%20Junsong%20Yuan%20and%20Bo%20Du&entry.1292438233=%20%20Skeleton-based%20human%20action%20recognition%20aims%20to%20classify%20human%20skeletal%0Asequences%2C%20which%20are%20spatiotemporal%20representations%20of%20actions%2C%20into%20predefined%0Acategories.%20To%20reduce%20the%20reliance%20on%20costly%20annotations%20of%20skeletal%20sequences%0Awhile%20maintaining%20competitive%20recognition%20accuracy%2C%20the%20task%20of%203D%20Action%0ARecognition%20with%20Limited%20Training%20Samples%2C%20also%20known%20as%20semi-supervised%203D%0AAction%20Recognition%2C%20has%20been%20proposed.%20In%20addition%2C%20active%20learning%2C%20which%20aims%0Ato%20proactively%20select%20the%20most%20informative%20unlabeled%20samples%20for%20annotation%2C%0Ahas%20been%20explored%20in%20semi-supervised%203D%20Action%20Recognition%20for%20training%20sample%0Aselection.%20Specifically%2C%20researchers%20adopt%20an%20encoder-decoder%20framework%20to%0Aembed%20skeleton%20sequences%20into%20a%20latent%20space%2C%20where%20clustering%20information%2C%0Acombined%20with%20a%20margin-based%20selection%20strategy%20using%20a%20multi-head%20mechanism%2C%0Ais%20utilized%20to%20identify%20the%20most%20informative%20sequences%20in%20the%20unlabeled%20set%20for%0Aannotation.%20However%2C%20the%20most%20representative%20skeleton%20sequences%20may%20not%0Anecessarily%20be%20the%20most%20informative%20for%20the%20action%20recognizer%2C%20as%20the%20model%20may%0Ahave%20already%20acquired%20similar%20knowledge%20from%20previously%20seen%20skeleton%20samples.%0ATo%20solve%20it%2C%20we%20reformulate%20Semi-supervised%203D%20action%20recognition%20via%20active%0Alearning%20from%20a%20novel%20perspective%20by%20casting%20it%20as%20a%20Markov%20Decision%20Process%0A%28MDP%29.%20Built%20upon%20the%20MDP%20framework%20and%20its%20training%20paradigm%2C%20we%20train%20an%0Ainformative%20sample%20selection%20model%20to%20intelligently%20guide%20the%20selection%20of%0Askeleton%20sequences%20for%20annotation.%20To%20enhance%20the%20representational%20capacity%20of%0Athe%20factors%20in%20the%20state-action%20pairs%20within%20our%20method%2C%20we%20project%20them%20from%0AEuclidean%20space%20to%20hyperbolic%20space.%20Furthermore%2C%20we%20introduce%20a%20meta%20tuning%0Astrategy%20to%20accelerate%20the%20deployment%20of%20our%20method%20in%20real-world%20scenarios.%0AExtensive%20experiments%20on%20three%203D%20action%20recognition%20benchmarks%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25345v1&entry.124074799=Read"},
{"title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars", "author": "Xiaosheng Zhao and Yang Huang and Guirong Xue and Xiao Kong and Jifeng Liu and Xiaoyu Tang and Timothy C. Beers and Yuan-Sen Ting and A-Li Luo", "abstract": "  In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.\n", "link": "http://arxiv.org/abs/2507.01939v3", "date": "2025-10-29", "relevancy": 2.7354, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5478}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecCLIP%3A%20Aligning%20and%20Translating%20Spectroscopic%20Measurements%20for%20Stars&body=Title%3A%20SpecCLIP%3A%20Aligning%20and%20Translating%20Spectroscopic%20Measurements%20for%20Stars%0AAuthor%3A%20Xiaosheng%20Zhao%20and%20Yang%20Huang%20and%20Guirong%20Xue%20and%20Xiao%20Kong%20and%20Jifeng%20Liu%20and%20Xiaoyu%20Tang%20and%20Timothy%20C.%20Beers%20and%20Yuan-Sen%20Ting%20and%20A-Li%20Luo%0AAbstract%3A%20%20%20In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20transformed%20natural%0Alanguage%20understanding%20through%20vast%20datasets%20and%20large-scale%20parameterization.%0AInspired%20by%20this%20success%2C%20we%20present%20SpecCLIP%2C%20a%20foundation%20model%20framework%0Athat%20extends%20LLM-inspired%20methodologies%20to%20stellar%20spectral%20analysis.%20Stellar%0Aspectra%2C%20akin%20to%20structured%20language%2C%20encode%20rich%20physical%20and%20chemical%0Ainformation%20about%20stars.%20By%20training%20foundation%20models%20on%20large-scale%20spectral%0Adatasets%2C%20our%20goal%20is%20to%20learn%20robust%20and%20informative%20embeddings%20that%20support%0Adiverse%20downstream%20applications.%20As%20a%20proof%20of%20concept%2C%20SpecCLIP%20involves%0Apre-training%20on%20two%20spectral%20types--LAMOST%20low-resolution%20and%20Gaia%20XP--followed%0Aby%20contrastive%20alignment%20using%20the%20CLIP%20%28Contrastive%20Language-Image%0APre-training%29%20framework%2C%20adapted%20to%20associate%20spectra%20from%20different%0Ainstruments.%20This%20alignment%20is%20complemented%20by%20auxiliary%20decoders%20that%20preserve%0Aspectrum-specific%20information%20and%20enable%20translation%20%28prediction%29%20between%0Aspectral%20types%2C%20with%20the%20former%20achieved%20by%20maximizing%20mutual%20information%0Abetween%20embeddings%20and%20input%20spectra.%20The%20result%20is%20a%20cross-spectrum%20framework%0Aenabling%20intrinsic%20calibration%20and%20flexible%20applications%20across%20instruments.%20We%0Ademonstrate%20that%20fine-tuning%20these%20models%20on%20moderate-sized%20labeled%20datasets%0Aimproves%20adaptability%20to%20tasks%20such%20as%20stellar-parameter%20estimation%20and%0Achemical-abundance%20determination.%20SpecCLIP%20also%20enhances%20the%20accuracy%20and%0Aprecision%20of%20parameter%20estimates%20benchmarked%20against%20external%20survey%20data.%0AAdditionally%2C%20its%20similarity%20search%20and%20cross-spectrum%20prediction%20capabilities%0Aoffer%20potential%20for%20anomaly%20detection.%20Our%20results%20suggest%20that%20contrastively%0Atrained%20foundation%20models%20enriched%20with%20spectrum-aware%20decoders%20can%20advance%0Aprecision%20stellar%20spectroscopy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01939v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecCLIP%253A%2520Aligning%2520and%2520Translating%2520Spectroscopic%2520Measurements%2520for%2520Stars%26entry.906535625%3DXiaosheng%2520Zhao%2520and%2520Yang%2520Huang%2520and%2520Guirong%2520Xue%2520and%2520Xiao%2520Kong%2520and%2520Jifeng%2520Liu%2520and%2520Xiaoyu%2520Tang%2520and%2520Timothy%2520C.%2520Beers%2520and%2520Yuan-Sen%2520Ting%2520and%2520A-Li%2520Luo%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520transformed%2520natural%250Alanguage%2520understanding%2520through%2520vast%2520datasets%2520and%2520large-scale%2520parameterization.%250AInspired%2520by%2520this%2520success%252C%2520we%2520present%2520SpecCLIP%252C%2520a%2520foundation%2520model%2520framework%250Athat%2520extends%2520LLM-inspired%2520methodologies%2520to%2520stellar%2520spectral%2520analysis.%2520Stellar%250Aspectra%252C%2520akin%2520to%2520structured%2520language%252C%2520encode%2520rich%2520physical%2520and%2520chemical%250Ainformation%2520about%2520stars.%2520By%2520training%2520foundation%2520models%2520on%2520large-scale%2520spectral%250Adatasets%252C%2520our%2520goal%2520is%2520to%2520learn%2520robust%2520and%2520informative%2520embeddings%2520that%2520support%250Adiverse%2520downstream%2520applications.%2520As%2520a%2520proof%2520of%2520concept%252C%2520SpecCLIP%2520involves%250Apre-training%2520on%2520two%2520spectral%2520types--LAMOST%2520low-resolution%2520and%2520Gaia%2520XP--followed%250Aby%2520contrastive%2520alignment%2520using%2520the%2520CLIP%2520%2528Contrastive%2520Language-Image%250APre-training%2529%2520framework%252C%2520adapted%2520to%2520associate%2520spectra%2520from%2520different%250Ainstruments.%2520This%2520alignment%2520is%2520complemented%2520by%2520auxiliary%2520decoders%2520that%2520preserve%250Aspectrum-specific%2520information%2520and%2520enable%2520translation%2520%2528prediction%2529%2520between%250Aspectral%2520types%252C%2520with%2520the%2520former%2520achieved%2520by%2520maximizing%2520mutual%2520information%250Abetween%2520embeddings%2520and%2520input%2520spectra.%2520The%2520result%2520is%2520a%2520cross-spectrum%2520framework%250Aenabling%2520intrinsic%2520calibration%2520and%2520flexible%2520applications%2520across%2520instruments.%2520We%250Ademonstrate%2520that%2520fine-tuning%2520these%2520models%2520on%2520moderate-sized%2520labeled%2520datasets%250Aimproves%2520adaptability%2520to%2520tasks%2520such%2520as%2520stellar-parameter%2520estimation%2520and%250Achemical-abundance%2520determination.%2520SpecCLIP%2520also%2520enhances%2520the%2520accuracy%2520and%250Aprecision%2520of%2520parameter%2520estimates%2520benchmarked%2520against%2520external%2520survey%2520data.%250AAdditionally%252C%2520its%2520similarity%2520search%2520and%2520cross-spectrum%2520prediction%2520capabilities%250Aoffer%2520potential%2520for%2520anomaly%2520detection.%2520Our%2520results%2520suggest%2520that%2520contrastively%250Atrained%2520foundation%2520models%2520enriched%2520with%2520spectrum-aware%2520decoders%2520can%2520advance%250Aprecision%2520stellar%2520spectroscopy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01939v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecCLIP%3A%20Aligning%20and%20Translating%20Spectroscopic%20Measurements%20for%20Stars&entry.906535625=Xiaosheng%20Zhao%20and%20Yang%20Huang%20and%20Guirong%20Xue%20and%20Xiao%20Kong%20and%20Jifeng%20Liu%20and%20Xiaoyu%20Tang%20and%20Timothy%20C.%20Beers%20and%20Yuan-Sen%20Ting%20and%20A-Li%20Luo&entry.1292438233=%20%20In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20transformed%20natural%0Alanguage%20understanding%20through%20vast%20datasets%20and%20large-scale%20parameterization.%0AInspired%20by%20this%20success%2C%20we%20present%20SpecCLIP%2C%20a%20foundation%20model%20framework%0Athat%20extends%20LLM-inspired%20methodologies%20to%20stellar%20spectral%20analysis.%20Stellar%0Aspectra%2C%20akin%20to%20structured%20language%2C%20encode%20rich%20physical%20and%20chemical%0Ainformation%20about%20stars.%20By%20training%20foundation%20models%20on%20large-scale%20spectral%0Adatasets%2C%20our%20goal%20is%20to%20learn%20robust%20and%20informative%20embeddings%20that%20support%0Adiverse%20downstream%20applications.%20As%20a%20proof%20of%20concept%2C%20SpecCLIP%20involves%0Apre-training%20on%20two%20spectral%20types--LAMOST%20low-resolution%20and%20Gaia%20XP--followed%0Aby%20contrastive%20alignment%20using%20the%20CLIP%20%28Contrastive%20Language-Image%0APre-training%29%20framework%2C%20adapted%20to%20associate%20spectra%20from%20different%0Ainstruments.%20This%20alignment%20is%20complemented%20by%20auxiliary%20decoders%20that%20preserve%0Aspectrum-specific%20information%20and%20enable%20translation%20%28prediction%29%20between%0Aspectral%20types%2C%20with%20the%20former%20achieved%20by%20maximizing%20mutual%20information%0Abetween%20embeddings%20and%20input%20spectra.%20The%20result%20is%20a%20cross-spectrum%20framework%0Aenabling%20intrinsic%20calibration%20and%20flexible%20applications%20across%20instruments.%20We%0Ademonstrate%20that%20fine-tuning%20these%20models%20on%20moderate-sized%20labeled%20datasets%0Aimproves%20adaptability%20to%20tasks%20such%20as%20stellar-parameter%20estimation%20and%0Achemical-abundance%20determination.%20SpecCLIP%20also%20enhances%20the%20accuracy%20and%0Aprecision%20of%20parameter%20estimates%20benchmarked%20against%20external%20survey%20data.%0AAdditionally%2C%20its%20similarity%20search%20and%20cross-spectrum%20prediction%20capabilities%0Aoffer%20potential%20for%20anomaly%20detection.%20Our%20results%20suggest%20that%20contrastively%0Atrained%20foundation%20models%20enriched%20with%20spectrum-aware%20decoders%20can%20advance%0Aprecision%20stellar%20spectroscopy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01939v3&entry.124074799=Read"},
{"title": "3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine\n  Learning Framework", "author": "Ayman Abaid and Gianpiero Guidone and Sara Alsubai and Foziyah Alquahtani and Talha Iqbal and Ruth Sharif and Hesham Elzomor and Emiliano Bianchini and Naeif Almagal and Michael G. Madden and Faisal Sharif and Ihsan Ullah", "abstract": "  Coronary artery calcium (CAC) scoring plays a crucial role in the early\ndetection and risk stratification of coronary artery disease (CAD). In this\nstudy, we focus on non-contrast coronary computed tomography angiography (CCTA)\nscans, which are commonly used for early calcification detection in clinical\nsettings. To address the challenge of limited annotated data, we propose a\nradiomics-based pipeline that leverages pseudo-labeling to generate training\nlabels, thereby eliminating the need for expert-defined segmentations.\nAdditionally, we explore the use of pretrained foundation models, specifically\nCT-FM and RadImageNet, to extract image features, which are then used with\ntraditional classifiers. We compare the performance of these deep learning\nfeatures with that of radiomics features. Evaluation is conducted on a clinical\nCCTA dataset comprising 182 patients, where individuals are classified into two\ngroups: zero versus non-zero calcium scores. We further investigate the impact\nof training on non-contrast datasets versus combined contrast and non-contrast\ndatasets, with testing performed only on non contrast scans. Results show that\nradiomics-based models significantly outperform CNN-derived embeddings from\nfoundation models (achieving 84% accuracy and p<0.05), despite the\nunavailability of expert annotations.\n", "link": "http://arxiv.org/abs/2510.25347v1", "date": "2025-10-29", "relevancy": 2.709, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5584}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5352}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20CT-Based%20Coronary%20Calcium%20Assessment%3A%20A%20Feature-Driven%20Machine%0A%20%20Learning%20Framework&body=Title%3A%203D%20CT-Based%20Coronary%20Calcium%20Assessment%3A%20A%20Feature-Driven%20Machine%0A%20%20Learning%20Framework%0AAuthor%3A%20Ayman%20Abaid%20and%20Gianpiero%20Guidone%20and%20Sara%20Alsubai%20and%20Foziyah%20Alquahtani%20and%20Talha%20Iqbal%20and%20Ruth%20Sharif%20and%20Hesham%20Elzomor%20and%20Emiliano%20Bianchini%20and%20Naeif%20Almagal%20and%20Michael%20G.%20Madden%20and%20Faisal%20Sharif%20and%20Ihsan%20Ullah%0AAbstract%3A%20%20%20Coronary%20artery%20calcium%20%28CAC%29%20scoring%20plays%20a%20crucial%20role%20in%20the%20early%0Adetection%20and%20risk%20stratification%20of%20coronary%20artery%20disease%20%28CAD%29.%20In%20this%0Astudy%2C%20we%20focus%20on%20non-contrast%20coronary%20computed%20tomography%20angiography%20%28CCTA%29%0Ascans%2C%20which%20are%20commonly%20used%20for%20early%20calcification%20detection%20in%20clinical%0Asettings.%20To%20address%20the%20challenge%20of%20limited%20annotated%20data%2C%20we%20propose%20a%0Aradiomics-based%20pipeline%20that%20leverages%20pseudo-labeling%20to%20generate%20training%0Alabels%2C%20thereby%20eliminating%20the%20need%20for%20expert-defined%20segmentations.%0AAdditionally%2C%20we%20explore%20the%20use%20of%20pretrained%20foundation%20models%2C%20specifically%0ACT-FM%20and%20RadImageNet%2C%20to%20extract%20image%20features%2C%20which%20are%20then%20used%20with%0Atraditional%20classifiers.%20We%20compare%20the%20performance%20of%20these%20deep%20learning%0Afeatures%20with%20that%20of%20radiomics%20features.%20Evaluation%20is%20conducted%20on%20a%20clinical%0ACCTA%20dataset%20comprising%20182%20patients%2C%20where%20individuals%20are%20classified%20into%20two%0Agroups%3A%20zero%20versus%20non-zero%20calcium%20scores.%20We%20further%20investigate%20the%20impact%0Aof%20training%20on%20non-contrast%20datasets%20versus%20combined%20contrast%20and%20non-contrast%0Adatasets%2C%20with%20testing%20performed%20only%20on%20non%20contrast%20scans.%20Results%20show%20that%0Aradiomics-based%20models%20significantly%20outperform%20CNN-derived%20embeddings%20from%0Afoundation%20models%20%28achieving%2084%25%20accuracy%20and%20p%3C0.05%29%2C%20despite%20the%0Aunavailability%20of%20expert%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520CT-Based%2520Coronary%2520Calcium%2520Assessment%253A%2520A%2520Feature-Driven%2520Machine%250A%2520%2520Learning%2520Framework%26entry.906535625%3DAyman%2520Abaid%2520and%2520Gianpiero%2520Guidone%2520and%2520Sara%2520Alsubai%2520and%2520Foziyah%2520Alquahtani%2520and%2520Talha%2520Iqbal%2520and%2520Ruth%2520Sharif%2520and%2520Hesham%2520Elzomor%2520and%2520Emiliano%2520Bianchini%2520and%2520Naeif%2520Almagal%2520and%2520Michael%2520G.%2520Madden%2520and%2520Faisal%2520Sharif%2520and%2520Ihsan%2520Ullah%26entry.1292438233%3D%2520%2520Coronary%2520artery%2520calcium%2520%2528CAC%2529%2520scoring%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520early%250Adetection%2520and%2520risk%2520stratification%2520of%2520coronary%2520artery%2520disease%2520%2528CAD%2529.%2520In%2520this%250Astudy%252C%2520we%2520focus%2520on%2520non-contrast%2520coronary%2520computed%2520tomography%2520angiography%2520%2528CCTA%2529%250Ascans%252C%2520which%2520are%2520commonly%2520used%2520for%2520early%2520calcification%2520detection%2520in%2520clinical%250Asettings.%2520To%2520address%2520the%2520challenge%2520of%2520limited%2520annotated%2520data%252C%2520we%2520propose%2520a%250Aradiomics-based%2520pipeline%2520that%2520leverages%2520pseudo-labeling%2520to%2520generate%2520training%250Alabels%252C%2520thereby%2520eliminating%2520the%2520need%2520for%2520expert-defined%2520segmentations.%250AAdditionally%252C%2520we%2520explore%2520the%2520use%2520of%2520pretrained%2520foundation%2520models%252C%2520specifically%250ACT-FM%2520and%2520RadImageNet%252C%2520to%2520extract%2520image%2520features%252C%2520which%2520are%2520then%2520used%2520with%250Atraditional%2520classifiers.%2520We%2520compare%2520the%2520performance%2520of%2520these%2520deep%2520learning%250Afeatures%2520with%2520that%2520of%2520radiomics%2520features.%2520Evaluation%2520is%2520conducted%2520on%2520a%2520clinical%250ACCTA%2520dataset%2520comprising%2520182%2520patients%252C%2520where%2520individuals%2520are%2520classified%2520into%2520two%250Agroups%253A%2520zero%2520versus%2520non-zero%2520calcium%2520scores.%2520We%2520further%2520investigate%2520the%2520impact%250Aof%2520training%2520on%2520non-contrast%2520datasets%2520versus%2520combined%2520contrast%2520and%2520non-contrast%250Adatasets%252C%2520with%2520testing%2520performed%2520only%2520on%2520non%2520contrast%2520scans.%2520Results%2520show%2520that%250Aradiomics-based%2520models%2520significantly%2520outperform%2520CNN-derived%2520embeddings%2520from%250Afoundation%2520models%2520%2528achieving%252084%2525%2520accuracy%2520and%2520p%253C0.05%2529%252C%2520despite%2520the%250Aunavailability%2520of%2520expert%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20CT-Based%20Coronary%20Calcium%20Assessment%3A%20A%20Feature-Driven%20Machine%0A%20%20Learning%20Framework&entry.906535625=Ayman%20Abaid%20and%20Gianpiero%20Guidone%20and%20Sara%20Alsubai%20and%20Foziyah%20Alquahtani%20and%20Talha%20Iqbal%20and%20Ruth%20Sharif%20and%20Hesham%20Elzomor%20and%20Emiliano%20Bianchini%20and%20Naeif%20Almagal%20and%20Michael%20G.%20Madden%20and%20Faisal%20Sharif%20and%20Ihsan%20Ullah&entry.1292438233=%20%20Coronary%20artery%20calcium%20%28CAC%29%20scoring%20plays%20a%20crucial%20role%20in%20the%20early%0Adetection%20and%20risk%20stratification%20of%20coronary%20artery%20disease%20%28CAD%29.%20In%20this%0Astudy%2C%20we%20focus%20on%20non-contrast%20coronary%20computed%20tomography%20angiography%20%28CCTA%29%0Ascans%2C%20which%20are%20commonly%20used%20for%20early%20calcification%20detection%20in%20clinical%0Asettings.%20To%20address%20the%20challenge%20of%20limited%20annotated%20data%2C%20we%20propose%20a%0Aradiomics-based%20pipeline%20that%20leverages%20pseudo-labeling%20to%20generate%20training%0Alabels%2C%20thereby%20eliminating%20the%20need%20for%20expert-defined%20segmentations.%0AAdditionally%2C%20we%20explore%20the%20use%20of%20pretrained%20foundation%20models%2C%20specifically%0ACT-FM%20and%20RadImageNet%2C%20to%20extract%20image%20features%2C%20which%20are%20then%20used%20with%0Atraditional%20classifiers.%20We%20compare%20the%20performance%20of%20these%20deep%20learning%0Afeatures%20with%20that%20of%20radiomics%20features.%20Evaluation%20is%20conducted%20on%20a%20clinical%0ACCTA%20dataset%20comprising%20182%20patients%2C%20where%20individuals%20are%20classified%20into%20two%0Agroups%3A%20zero%20versus%20non-zero%20calcium%20scores.%20We%20further%20investigate%20the%20impact%0Aof%20training%20on%20non-contrast%20datasets%20versus%20combined%20contrast%20and%20non-contrast%0Adatasets%2C%20with%20testing%20performed%20only%20on%20non%20contrast%20scans.%20Results%20show%20that%0Aradiomics-based%20models%20significantly%20outperform%20CNN-derived%20embeddings%20from%0Afoundation%20models%20%28achieving%2084%25%20accuracy%20and%20p%3C0.05%29%2C%20despite%20the%0Aunavailability%20of%20expert%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25347v1&entry.124074799=Read"},
{"title": "Graph Network-based Structural Simulator: Graph Neural Networks for\n  Structural Dynamics", "author": "Alessandro Lucchetti and Francesco Cadini and Marco Giglio and Luca Lomazzi", "abstract": "  Graph Neural Networks (GNNs) have recently been explored as surrogate models\nfor numerical simulations. While their applications in computational fluid\ndynamics have been investigated, little attention has been given to structural\nproblems, especially for dynamic cases. To address this gap, we introduce the\nGraph Network-based Structural Simulator (GNSS), a GNN framework for surrogate\nmodeling of dynamic structural problems.\n  GNSS follows the encode-process-decode paradigm typical of GNN-based machine\nlearning models, and its design makes it particularly suited for dynamic\nsimulations thanks to three key features: (i) expressing node kinematics in\nnode-fixed local frames, which avoids catastrophic cancellation in\nfinite-difference velocities; (ii) employing a sign-aware regression loss,\nwhich reduces phase errors in long rollouts; and (iii) using a\nwavelength-informed connectivity radius, which optimizes graph construction.\n  We evaluate GNSS on a case study involving a beam excited by a 50kHz\nHanning-modulated pulse. The results show that GNSS accurately reproduces the\nphysics of the problem over hundreds of timesteps and generalizes to unseen\nloading conditions, where existing GNNs fail to converge or deliver meaningful\npredictions.\n  Compared with explicit finite element baselines, GNSS achieves substantial\ninference speedups while preserving spatial and temporal fidelity. These\nfindings demonstrate that locality-preserving GNNs with physics-consistent\nupdate rules are a competitive alternative for dynamic, wave-dominated\nstructural simulations.\n", "link": "http://arxiv.org/abs/2510.25683v1", "date": "2025-10-29", "relevancy": 2.6842, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.586}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5174}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Network-based%20Structural%20Simulator%3A%20Graph%20Neural%20Networks%20for%0A%20%20Structural%20Dynamics&body=Title%3A%20Graph%20Network-based%20Structural%20Simulator%3A%20Graph%20Neural%20Networks%20for%0A%20%20Structural%20Dynamics%0AAuthor%3A%20Alessandro%20Lucchetti%20and%20Francesco%20Cadini%20and%20Marco%20Giglio%20and%20Luca%20Lomazzi%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20been%20explored%20as%20surrogate%20models%0Afor%20numerical%20simulations.%20While%20their%20applications%20in%20computational%20fluid%0Adynamics%20have%20been%20investigated%2C%20little%20attention%20has%20been%20given%20to%20structural%0Aproblems%2C%20especially%20for%20dynamic%20cases.%20To%20address%20this%20gap%2C%20we%20introduce%20the%0AGraph%20Network-based%20Structural%20Simulator%20%28GNSS%29%2C%20a%20GNN%20framework%20for%20surrogate%0Amodeling%20of%20dynamic%20structural%20problems.%0A%20%20GNSS%20follows%20the%20encode-process-decode%20paradigm%20typical%20of%20GNN-based%20machine%0Alearning%20models%2C%20and%20its%20design%20makes%20it%20particularly%20suited%20for%20dynamic%0Asimulations%20thanks%20to%20three%20key%20features%3A%20%28i%29%20expressing%20node%20kinematics%20in%0Anode-fixed%20local%20frames%2C%20which%20avoids%20catastrophic%20cancellation%20in%0Afinite-difference%20velocities%3B%20%28ii%29%20employing%20a%20sign-aware%20regression%20loss%2C%0Awhich%20reduces%20phase%20errors%20in%20long%20rollouts%3B%20and%20%28iii%29%20using%20a%0Awavelength-informed%20connectivity%20radius%2C%20which%20optimizes%20graph%20construction.%0A%20%20We%20evaluate%20GNSS%20on%20a%20case%20study%20involving%20a%20beam%20excited%20by%20a%2050kHz%0AHanning-modulated%20pulse.%20The%20results%20show%20that%20GNSS%20accurately%20reproduces%20the%0Aphysics%20of%20the%20problem%20over%20hundreds%20of%20timesteps%20and%20generalizes%20to%20unseen%0Aloading%20conditions%2C%20where%20existing%20GNNs%20fail%20to%20converge%20or%20deliver%20meaningful%0Apredictions.%0A%20%20Compared%20with%20explicit%20finite%20element%20baselines%2C%20GNSS%20achieves%20substantial%0Ainference%20speedups%20while%20preserving%20spatial%20and%20temporal%20fidelity.%20These%0Afindings%20demonstrate%20that%20locality-preserving%20GNNs%20with%20physics-consistent%0Aupdate%20rules%20are%20a%20competitive%20alternative%20for%20dynamic%2C%20wave-dominated%0Astructural%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Network-based%2520Structural%2520Simulator%253A%2520Graph%2520Neural%2520Networks%2520for%250A%2520%2520Structural%2520Dynamics%26entry.906535625%3DAlessandro%2520Lucchetti%2520and%2520Francesco%2520Cadini%2520and%2520Marco%2520Giglio%2520and%2520Luca%2520Lomazzi%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520recently%2520been%2520explored%2520as%2520surrogate%2520models%250Afor%2520numerical%2520simulations.%2520While%2520their%2520applications%2520in%2520computational%2520fluid%250Adynamics%2520have%2520been%2520investigated%252C%2520little%2520attention%2520has%2520been%2520given%2520to%2520structural%250Aproblems%252C%2520especially%2520for%2520dynamic%2520cases.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%250AGraph%2520Network-based%2520Structural%2520Simulator%2520%2528GNSS%2529%252C%2520a%2520GNN%2520framework%2520for%2520surrogate%250Amodeling%2520of%2520dynamic%2520structural%2520problems.%250A%2520%2520GNSS%2520follows%2520the%2520encode-process-decode%2520paradigm%2520typical%2520of%2520GNN-based%2520machine%250Alearning%2520models%252C%2520and%2520its%2520design%2520makes%2520it%2520particularly%2520suited%2520for%2520dynamic%250Asimulations%2520thanks%2520to%2520three%2520key%2520features%253A%2520%2528i%2529%2520expressing%2520node%2520kinematics%2520in%250Anode-fixed%2520local%2520frames%252C%2520which%2520avoids%2520catastrophic%2520cancellation%2520in%250Afinite-difference%2520velocities%253B%2520%2528ii%2529%2520employing%2520a%2520sign-aware%2520regression%2520loss%252C%250Awhich%2520reduces%2520phase%2520errors%2520in%2520long%2520rollouts%253B%2520and%2520%2528iii%2529%2520using%2520a%250Awavelength-informed%2520connectivity%2520radius%252C%2520which%2520optimizes%2520graph%2520construction.%250A%2520%2520We%2520evaluate%2520GNSS%2520on%2520a%2520case%2520study%2520involving%2520a%2520beam%2520excited%2520by%2520a%252050kHz%250AHanning-modulated%2520pulse.%2520The%2520results%2520show%2520that%2520GNSS%2520accurately%2520reproduces%2520the%250Aphysics%2520of%2520the%2520problem%2520over%2520hundreds%2520of%2520timesteps%2520and%2520generalizes%2520to%2520unseen%250Aloading%2520conditions%252C%2520where%2520existing%2520GNNs%2520fail%2520to%2520converge%2520or%2520deliver%2520meaningful%250Apredictions.%250A%2520%2520Compared%2520with%2520explicit%2520finite%2520element%2520baselines%252C%2520GNSS%2520achieves%2520substantial%250Ainference%2520speedups%2520while%2520preserving%2520spatial%2520and%2520temporal%2520fidelity.%2520These%250Afindings%2520demonstrate%2520that%2520locality-preserving%2520GNNs%2520with%2520physics-consistent%250Aupdate%2520rules%2520are%2520a%2520competitive%2520alternative%2520for%2520dynamic%252C%2520wave-dominated%250Astructural%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Network-based%20Structural%20Simulator%3A%20Graph%20Neural%20Networks%20for%0A%20%20Structural%20Dynamics&entry.906535625=Alessandro%20Lucchetti%20and%20Francesco%20Cadini%20and%20Marco%20Giglio%20and%20Luca%20Lomazzi&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20been%20explored%20as%20surrogate%20models%0Afor%20numerical%20simulations.%20While%20their%20applications%20in%20computational%20fluid%0Adynamics%20have%20been%20investigated%2C%20little%20attention%20has%20been%20given%20to%20structural%0Aproblems%2C%20especially%20for%20dynamic%20cases.%20To%20address%20this%20gap%2C%20we%20introduce%20the%0AGraph%20Network-based%20Structural%20Simulator%20%28GNSS%29%2C%20a%20GNN%20framework%20for%20surrogate%0Amodeling%20of%20dynamic%20structural%20problems.%0A%20%20GNSS%20follows%20the%20encode-process-decode%20paradigm%20typical%20of%20GNN-based%20machine%0Alearning%20models%2C%20and%20its%20design%20makes%20it%20particularly%20suited%20for%20dynamic%0Asimulations%20thanks%20to%20three%20key%20features%3A%20%28i%29%20expressing%20node%20kinematics%20in%0Anode-fixed%20local%20frames%2C%20which%20avoids%20catastrophic%20cancellation%20in%0Afinite-difference%20velocities%3B%20%28ii%29%20employing%20a%20sign-aware%20regression%20loss%2C%0Awhich%20reduces%20phase%20errors%20in%20long%20rollouts%3B%20and%20%28iii%29%20using%20a%0Awavelength-informed%20connectivity%20radius%2C%20which%20optimizes%20graph%20construction.%0A%20%20We%20evaluate%20GNSS%20on%20a%20case%20study%20involving%20a%20beam%20excited%20by%20a%2050kHz%0AHanning-modulated%20pulse.%20The%20results%20show%20that%20GNSS%20accurately%20reproduces%20the%0Aphysics%20of%20the%20problem%20over%20hundreds%20of%20timesteps%20and%20generalizes%20to%20unseen%0Aloading%20conditions%2C%20where%20existing%20GNNs%20fail%20to%20converge%20or%20deliver%20meaningful%0Apredictions.%0A%20%20Compared%20with%20explicit%20finite%20element%20baselines%2C%20GNSS%20achieves%20substantial%0Ainference%20speedups%20while%20preserving%20spatial%20and%20temporal%20fidelity.%20These%0Afindings%20demonstrate%20that%20locality-preserving%20GNNs%20with%20physics-consistent%0Aupdate%20rules%20are%20a%20competitive%20alternative%20for%20dynamic%2C%20wave-dominated%0Astructural%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25683v1&entry.124074799=Read"},
{"title": "GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac\n  MRI Reconstruction", "author": "Kian Anvari Hamedani and Narges Razizadeh and Shahabedin Nabavi and Mohsen Ebrahimi Moghaddam", "abstract": "  Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction\nremains a critical challenge due to the trade-off between scan time and image\nquality, particularly when generalizing across diverse acquisition settings. We\npropose GENRE-CMR, a generative adversarial network (GAN)-based architecture\nemploying a residual deep unrolled reconstruction framework to enhance\nreconstruction fidelity and generalization. The architecture unrolls iterative\noptimization into a cascade of convolutional subnetworks, enriched with\nresidual connections to enable progressive feature propagation from shallow to\ndeeper stages. To further improve performance, we integrate two loss functions:\n(1) an Edge-Aware Region (EAR) loss, which guides the network to focus on\nstructurally informative regions and helps prevent common reconstruction\nblurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which\nregularizes the feature space across diverse data distributions via a symmetric\nKL divergence formulation. Extensive experiments confirm that GENRE-CMR\nsurpasses state-of-the-art methods on training and unseen data, achieving\n0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various\nacceleration factors and sampling trajectories. Ablation studies confirm the\ncontribution of each proposed component to reconstruction quality and\ngeneralization. Our framework presents a unified and robust solution for\nhigh-quality CMR reconstruction, paving the way for clinically adaptable\ndeployment across heterogeneous acquisition protocols.\n", "link": "http://arxiv.org/abs/2508.20600v2", "date": "2025-10-29", "relevancy": 2.6644, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5457}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5265}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GENRE-CMR%3A%20Generalizable%20Deep%20Learning%20for%20Diverse%20Multi-Domain%20Cardiac%0A%20%20MRI%20Reconstruction&body=Title%3A%20GENRE-CMR%3A%20Generalizable%20Deep%20Learning%20for%20Diverse%20Multi-Domain%20Cardiac%0A%20%20MRI%20Reconstruction%0AAuthor%3A%20Kian%20Anvari%20Hamedani%20and%20Narges%20Razizadeh%20and%20Shahabedin%20Nabavi%20and%20Mohsen%20Ebrahimi%20Moghaddam%0AAbstract%3A%20%20%20Accelerated%20Cardiovascular%20Magnetic%20Resonance%20%28CMR%29%20image%20reconstruction%0Aremains%20a%20critical%20challenge%20due%20to%20the%20trade-off%20between%20scan%20time%20and%20image%0Aquality%2C%20particularly%20when%20generalizing%20across%20diverse%20acquisition%20settings.%20We%0Apropose%20GENRE-CMR%2C%20a%20generative%20adversarial%20network%20%28GAN%29-based%20architecture%0Aemploying%20a%20residual%20deep%20unrolled%20reconstruction%20framework%20to%20enhance%0Areconstruction%20fidelity%20and%20generalization.%20The%20architecture%20unrolls%20iterative%0Aoptimization%20into%20a%20cascade%20of%20convolutional%20subnetworks%2C%20enriched%20with%0Aresidual%20connections%20to%20enable%20progressive%20feature%20propagation%20from%20shallow%20to%0Adeeper%20stages.%20To%20further%20improve%20performance%2C%20we%20integrate%20two%20loss%20functions%3A%0A%281%29%20an%20Edge-Aware%20Region%20%28EAR%29%20loss%2C%20which%20guides%20the%20network%20to%20focus%20on%0Astructurally%20informative%20regions%20and%20helps%20prevent%20common%20reconstruction%0Ablurriness%3B%20and%20%282%29%20a%20Statistical%20Distribution%20Alignment%20%28SDA%29%20loss%2C%20which%0Aregularizes%20the%20feature%20space%20across%20diverse%20data%20distributions%20via%20a%20symmetric%0AKL%20divergence%20formulation.%20Extensive%20experiments%20confirm%20that%20GENRE-CMR%0Asurpasses%20state-of-the-art%20methods%20on%20training%20and%20unseen%20data%2C%20achieving%0A0.9552%20SSIM%20and%2038.90%20dB%20PSNR%20on%20unseen%20distributions%20across%20various%0Aacceleration%20factors%20and%20sampling%20trajectories.%20Ablation%20studies%20confirm%20the%0Acontribution%20of%20each%20proposed%20component%20to%20reconstruction%20quality%20and%0Ageneralization.%20Our%20framework%20presents%20a%20unified%20and%20robust%20solution%20for%0Ahigh-quality%20CMR%20reconstruction%2C%20paving%20the%20way%20for%20clinically%20adaptable%0Adeployment%20across%20heterogeneous%20acquisition%20protocols.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20600v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGENRE-CMR%253A%2520Generalizable%2520Deep%2520Learning%2520for%2520Diverse%2520Multi-Domain%2520Cardiac%250A%2520%2520MRI%2520Reconstruction%26entry.906535625%3DKian%2520Anvari%2520Hamedani%2520and%2520Narges%2520Razizadeh%2520and%2520Shahabedin%2520Nabavi%2520and%2520Mohsen%2520Ebrahimi%2520Moghaddam%26entry.1292438233%3D%2520%2520Accelerated%2520Cardiovascular%2520Magnetic%2520Resonance%2520%2528CMR%2529%2520image%2520reconstruction%250Aremains%2520a%2520critical%2520challenge%2520due%2520to%2520the%2520trade-off%2520between%2520scan%2520time%2520and%2520image%250Aquality%252C%2520particularly%2520when%2520generalizing%2520across%2520diverse%2520acquisition%2520settings.%2520We%250Apropose%2520GENRE-CMR%252C%2520a%2520generative%2520adversarial%2520network%2520%2528GAN%2529-based%2520architecture%250Aemploying%2520a%2520residual%2520deep%2520unrolled%2520reconstruction%2520framework%2520to%2520enhance%250Areconstruction%2520fidelity%2520and%2520generalization.%2520The%2520architecture%2520unrolls%2520iterative%250Aoptimization%2520into%2520a%2520cascade%2520of%2520convolutional%2520subnetworks%252C%2520enriched%2520with%250Aresidual%2520connections%2520to%2520enable%2520progressive%2520feature%2520propagation%2520from%2520shallow%2520to%250Adeeper%2520stages.%2520To%2520further%2520improve%2520performance%252C%2520we%2520integrate%2520two%2520loss%2520functions%253A%250A%25281%2529%2520an%2520Edge-Aware%2520Region%2520%2528EAR%2529%2520loss%252C%2520which%2520guides%2520the%2520network%2520to%2520focus%2520on%250Astructurally%2520informative%2520regions%2520and%2520helps%2520prevent%2520common%2520reconstruction%250Ablurriness%253B%2520and%2520%25282%2529%2520a%2520Statistical%2520Distribution%2520Alignment%2520%2528SDA%2529%2520loss%252C%2520which%250Aregularizes%2520the%2520feature%2520space%2520across%2520diverse%2520data%2520distributions%2520via%2520a%2520symmetric%250AKL%2520divergence%2520formulation.%2520Extensive%2520experiments%2520confirm%2520that%2520GENRE-CMR%250Asurpasses%2520state-of-the-art%2520methods%2520on%2520training%2520and%2520unseen%2520data%252C%2520achieving%250A0.9552%2520SSIM%2520and%252038.90%2520dB%2520PSNR%2520on%2520unseen%2520distributions%2520across%2520various%250Aacceleration%2520factors%2520and%2520sampling%2520trajectories.%2520Ablation%2520studies%2520confirm%2520the%250Acontribution%2520of%2520each%2520proposed%2520component%2520to%2520reconstruction%2520quality%2520and%250Ageneralization.%2520Our%2520framework%2520presents%2520a%2520unified%2520and%2520robust%2520solution%2520for%250Ahigh-quality%2520CMR%2520reconstruction%252C%2520paving%2520the%2520way%2520for%2520clinically%2520adaptable%250Adeployment%2520across%2520heterogeneous%2520acquisition%2520protocols.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20600v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GENRE-CMR%3A%20Generalizable%20Deep%20Learning%20for%20Diverse%20Multi-Domain%20Cardiac%0A%20%20MRI%20Reconstruction&entry.906535625=Kian%20Anvari%20Hamedani%20and%20Narges%20Razizadeh%20and%20Shahabedin%20Nabavi%20and%20Mohsen%20Ebrahimi%20Moghaddam&entry.1292438233=%20%20Accelerated%20Cardiovascular%20Magnetic%20Resonance%20%28CMR%29%20image%20reconstruction%0Aremains%20a%20critical%20challenge%20due%20to%20the%20trade-off%20between%20scan%20time%20and%20image%0Aquality%2C%20particularly%20when%20generalizing%20across%20diverse%20acquisition%20settings.%20We%0Apropose%20GENRE-CMR%2C%20a%20generative%20adversarial%20network%20%28GAN%29-based%20architecture%0Aemploying%20a%20residual%20deep%20unrolled%20reconstruction%20framework%20to%20enhance%0Areconstruction%20fidelity%20and%20generalization.%20The%20architecture%20unrolls%20iterative%0Aoptimization%20into%20a%20cascade%20of%20convolutional%20subnetworks%2C%20enriched%20with%0Aresidual%20connections%20to%20enable%20progressive%20feature%20propagation%20from%20shallow%20to%0Adeeper%20stages.%20To%20further%20improve%20performance%2C%20we%20integrate%20two%20loss%20functions%3A%0A%281%29%20an%20Edge-Aware%20Region%20%28EAR%29%20loss%2C%20which%20guides%20the%20network%20to%20focus%20on%0Astructurally%20informative%20regions%20and%20helps%20prevent%20common%20reconstruction%0Ablurriness%3B%20and%20%282%29%20a%20Statistical%20Distribution%20Alignment%20%28SDA%29%20loss%2C%20which%0Aregularizes%20the%20feature%20space%20across%20diverse%20data%20distributions%20via%20a%20symmetric%0AKL%20divergence%20formulation.%20Extensive%20experiments%20confirm%20that%20GENRE-CMR%0Asurpasses%20state-of-the-art%20methods%20on%20training%20and%20unseen%20data%2C%20achieving%0A0.9552%20SSIM%20and%2038.90%20dB%20PSNR%20on%20unseen%20distributions%20across%20various%0Aacceleration%20factors%20and%20sampling%20trajectories.%20Ablation%20studies%20confirm%20the%0Acontribution%20of%20each%20proposed%20component%20to%20reconstruction%20quality%20and%0Ageneralization.%20Our%20framework%20presents%20a%20unified%20and%20robust%20solution%20for%0Ahigh-quality%20CMR%20reconstruction%2C%20paving%20the%20way%20for%20clinically%20adaptable%0Adeployment%20across%20heterogeneous%20acquisition%20protocols.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20600v2&entry.124074799=Read"},
{"title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO", "author": "Jinyoung Park and Jeehye Na and Jinyoung Kim and Hyunwoo J. Kim", "abstract": "  Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks.\n", "link": "http://arxiv.org/abs/2506.07464v3", "date": "2025-10-29", "relevancy": 2.6616, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5366}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepVideo-R1%3A%20Video%20Reinforcement%20Fine-Tuning%20via%20Difficulty-aware%0A%20%20Regressive%20GRPO&body=Title%3A%20DeepVideo-R1%3A%20Video%20Reinforcement%20Fine-Tuning%20via%20Difficulty-aware%0A%20%20Regressive%20GRPO%0AAuthor%3A%20Jinyoung%20Park%20and%20Jeehye%20Na%20and%20Jinyoung%20Kim%20and%20Hyunwoo%20J.%20Kim%0AAbstract%3A%20%20%20Recent%20works%20have%20demonstrated%20the%20effectiveness%20of%20reinforcement%20learning%0A%28RL%29-based%20post-training%20for%20enhancing%20the%20reasoning%20capabilities%20of%20large%0Alanguage%20models%20%28LLMs%29.%20In%20particular%2C%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20has%20shown%20impressive%20success%20using%20a%20PPO-style%20reinforcement%20algorithm%0Awith%20group-normalized%20rewards.%20However%2C%20the%20effectiveness%20of%20GRPO%20in%20Video%0ALarge%20Language%20Models%20%28VideoLLMs%29%20has%20still%20been%20less%20studyed.%20In%20this%20paper%2C%0Awe%20explore%20GRPO%20and%20identify%20two%20problems%20that%20deteriorate%20the%20effective%0Alearning%3A%20%281%29%20reliance%20on%20safeguards%2C%20and%20%282%29%20vanishing%20advantage.%20To%20mitigate%0Athese%20challenges%2C%20we%20propose%20DeepVideo-R1%2C%20a%20video%20large%20language%20model%20trained%0Awith%20Reg-GRPO%20%28Regressive%20GRPO%29%20and%20difficulty-aware%20data%20augmentation.%0AReg-GRPO%20reformulates%20the%20GRPO%20loss%20function%20into%20a%20regression%20task%20that%0Adirectly%20predicts%20the%20advantage%20in%20GRPO%2C%20eliminating%20the%20need%20for%20safeguards%0Asuch%20as%20the%20clipping%20and%20min%20functions.%20It%20directly%20aligns%20the%20model%20with%0Aadvantages%2C%20providing%20guidance%20to%20prefer%20better%20ones.%20The%20difficulty-aware%20data%0Aaugmentation%20strategy%20augments%20input%20prompts/videos%20to%20locate%20the%20difficulty%20of%0Asamples%20at%20solvable%20difficulty%20levels%2C%20enabling%20diverse%20reward%20signals.%20Our%0Aexperimental%20results%20show%20that%20our%20approach%20significantly%20improves%20video%0Areasoning%20performance%20across%20multiple%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07464v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepVideo-R1%253A%2520Video%2520Reinforcement%2520Fine-Tuning%2520via%2520Difficulty-aware%250A%2520%2520Regressive%2520GRPO%26entry.906535625%3DJinyoung%2520Park%2520and%2520Jeehye%2520Na%2520and%2520Jinyoung%2520Kim%2520and%2520Hyunwoo%2520J.%2520Kim%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520demonstrated%2520the%2520effectiveness%2520of%2520reinforcement%2520learning%250A%2528RL%2529-based%2520post-training%2520for%2520enhancing%2520the%2520reasoning%2520capabilities%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529.%2520In%2520particular%252C%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%2520has%2520shown%2520impressive%2520success%2520using%2520a%2520PPO-style%2520reinforcement%2520algorithm%250Awith%2520group-normalized%2520rewards.%2520However%252C%2520the%2520effectiveness%2520of%2520GRPO%2520in%2520Video%250ALarge%2520Language%2520Models%2520%2528VideoLLMs%2529%2520has%2520still%2520been%2520less%2520studyed.%2520In%2520this%2520paper%252C%250Awe%2520explore%2520GRPO%2520and%2520identify%2520two%2520problems%2520that%2520deteriorate%2520the%2520effective%250Alearning%253A%2520%25281%2529%2520reliance%2520on%2520safeguards%252C%2520and%2520%25282%2529%2520vanishing%2520advantage.%2520To%2520mitigate%250Athese%2520challenges%252C%2520we%2520propose%2520DeepVideo-R1%252C%2520a%2520video%2520large%2520language%2520model%2520trained%250Awith%2520Reg-GRPO%2520%2528Regressive%2520GRPO%2529%2520and%2520difficulty-aware%2520data%2520augmentation.%250AReg-GRPO%2520reformulates%2520the%2520GRPO%2520loss%2520function%2520into%2520a%2520regression%2520task%2520that%250Adirectly%2520predicts%2520the%2520advantage%2520in%2520GRPO%252C%2520eliminating%2520the%2520need%2520for%2520safeguards%250Asuch%2520as%2520the%2520clipping%2520and%2520min%2520functions.%2520It%2520directly%2520aligns%2520the%2520model%2520with%250Aadvantages%252C%2520providing%2520guidance%2520to%2520prefer%2520better%2520ones.%2520The%2520difficulty-aware%2520data%250Aaugmentation%2520strategy%2520augments%2520input%2520prompts/videos%2520to%2520locate%2520the%2520difficulty%2520of%250Asamples%2520at%2520solvable%2520difficulty%2520levels%252C%2520enabling%2520diverse%2520reward%2520signals.%2520Our%250Aexperimental%2520results%2520show%2520that%2520our%2520approach%2520significantly%2520improves%2520video%250Areasoning%2520performance%2520across%2520multiple%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07464v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepVideo-R1%3A%20Video%20Reinforcement%20Fine-Tuning%20via%20Difficulty-aware%0A%20%20Regressive%20GRPO&entry.906535625=Jinyoung%20Park%20and%20Jeehye%20Na%20and%20Jinyoung%20Kim%20and%20Hyunwoo%20J.%20Kim&entry.1292438233=%20%20Recent%20works%20have%20demonstrated%20the%20effectiveness%20of%20reinforcement%20learning%0A%28RL%29-based%20post-training%20for%20enhancing%20the%20reasoning%20capabilities%20of%20large%0Alanguage%20models%20%28LLMs%29.%20In%20particular%2C%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20has%20shown%20impressive%20success%20using%20a%20PPO-style%20reinforcement%20algorithm%0Awith%20group-normalized%20rewards.%20However%2C%20the%20effectiveness%20of%20GRPO%20in%20Video%0ALarge%20Language%20Models%20%28VideoLLMs%29%20has%20still%20been%20less%20studyed.%20In%20this%20paper%2C%0Awe%20explore%20GRPO%20and%20identify%20two%20problems%20that%20deteriorate%20the%20effective%0Alearning%3A%20%281%29%20reliance%20on%20safeguards%2C%20and%20%282%29%20vanishing%20advantage.%20To%20mitigate%0Athese%20challenges%2C%20we%20propose%20DeepVideo-R1%2C%20a%20video%20large%20language%20model%20trained%0Awith%20Reg-GRPO%20%28Regressive%20GRPO%29%20and%20difficulty-aware%20data%20augmentation.%0AReg-GRPO%20reformulates%20the%20GRPO%20loss%20function%20into%20a%20regression%20task%20that%0Adirectly%20predicts%20the%20advantage%20in%20GRPO%2C%20eliminating%20the%20need%20for%20safeguards%0Asuch%20as%20the%20clipping%20and%20min%20functions.%20It%20directly%20aligns%20the%20model%20with%0Aadvantages%2C%20providing%20guidance%20to%20prefer%20better%20ones.%20The%20difficulty-aware%20data%0Aaugmentation%20strategy%20augments%20input%20prompts/videos%20to%20locate%20the%20difficulty%20of%0Asamples%20at%20solvable%20difficulty%20levels%2C%20enabling%20diverse%20reward%20signals.%20Our%0Aexperimental%20results%20show%20that%20our%20approach%20significantly%20improves%20video%0Areasoning%20performance%20across%20multiple%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07464v3&entry.124074799=Read"},
{"title": "Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for\n  Local Learning", "author": "Arani Roy and Marco P. Apolinario and Shristi Das Biswas and Kaushik Roy", "abstract": "  Training deep neural networks (DNNs) with backpropagation (BP) achieves\nstate-of-the-art accuracy but requires global error propagation and full\nparameterization, leading to substantial memory and computational overhead.\nDirect Feedback Alignment (DFA) enables local, parallelizable updates with\nlower memory requirements but is limited by unstructured feedback and poor\nscalability in deeper architectures, specially convolutional neural networks.\nTo address these limitations, we propose a structured local learning framework\nthat operates directly on low-rank manifolds defined by the Singular Value\nDecomposition (SVD) of weight matrices. Each layer is trained in its decomposed\nform, with updates applied to the SVD components using a composite loss that\nintegrates cross-entropy, subspace alignment, and orthogonality regularization.\nFeedback matrices are constructed to match the SVD structure, ensuring\nconsistent alignment between forward and feedback pathways. Our method reduces\nthe number of trainable parameters relative to the original DFA model, without\nrelying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,\nand ImageNet show that our method achieves accuracy comparable to that of BP.\nAblation studies confirm the importance of each loss term in the low-rank\nsetting. These results establish local learning on low-rank manifolds as a\nprincipled and scalable alternative to full-rank gradient-based training.\n", "link": "http://arxiv.org/abs/2510.25594v1", "date": "2025-10-29", "relevancy": 2.6483, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5661}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5284}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feedback%20Alignment%20Meets%20Low-Rank%20Manifolds%3A%20A%20Structured%20Recipe%20for%0A%20%20Local%20Learning&body=Title%3A%20Feedback%20Alignment%20Meets%20Low-Rank%20Manifolds%3A%20A%20Structured%20Recipe%20for%0A%20%20Local%20Learning%0AAuthor%3A%20Arani%20Roy%20and%20Marco%20P.%20Apolinario%20and%20Shristi%20Das%20Biswas%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20Training%20deep%20neural%20networks%20%28DNNs%29%20with%20backpropagation%20%28BP%29%20achieves%0Astate-of-the-art%20accuracy%20but%20requires%20global%20error%20propagation%20and%20full%0Aparameterization%2C%20leading%20to%20substantial%20memory%20and%20computational%20overhead.%0ADirect%20Feedback%20Alignment%20%28DFA%29%20enables%20local%2C%20parallelizable%20updates%20with%0Alower%20memory%20requirements%20but%20is%20limited%20by%20unstructured%20feedback%20and%20poor%0Ascalability%20in%20deeper%20architectures%2C%20specially%20convolutional%20neural%20networks.%0ATo%20address%20these%20limitations%2C%20we%20propose%20a%20structured%20local%20learning%20framework%0Athat%20operates%20directly%20on%20low-rank%20manifolds%20defined%20by%20the%20Singular%20Value%0ADecomposition%20%28SVD%29%20of%20weight%20matrices.%20Each%20layer%20is%20trained%20in%20its%20decomposed%0Aform%2C%20with%20updates%20applied%20to%20the%20SVD%20components%20using%20a%20composite%20loss%20that%0Aintegrates%20cross-entropy%2C%20subspace%20alignment%2C%20and%20orthogonality%20regularization.%0AFeedback%20matrices%20are%20constructed%20to%20match%20the%20SVD%20structure%2C%20ensuring%0Aconsistent%20alignment%20between%20forward%20and%20feedback%20pathways.%20Our%20method%20reduces%0Athe%20number%20of%20trainable%20parameters%20relative%20to%20the%20original%20DFA%20model%2C%20without%0Arelying%20on%20pruning%20or%20post%20hoc%20compression.%20Experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%0Aand%20ImageNet%20show%20that%20our%20method%20achieves%20accuracy%20comparable%20to%20that%20of%20BP.%0AAblation%20studies%20confirm%20the%20importance%20of%20each%20loss%20term%20in%20the%20low-rank%0Asetting.%20These%20results%20establish%20local%20learning%20on%20low-rank%20manifolds%20as%20a%0Aprincipled%20and%20scalable%20alternative%20to%20full-rank%20gradient-based%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeedback%2520Alignment%2520Meets%2520Low-Rank%2520Manifolds%253A%2520A%2520Structured%2520Recipe%2520for%250A%2520%2520Local%2520Learning%26entry.906535625%3DArani%2520Roy%2520and%2520Marco%2520P.%2520Apolinario%2520and%2520Shristi%2520Das%2520Biswas%2520and%2520Kaushik%2520Roy%26entry.1292438233%3D%2520%2520Training%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520with%2520backpropagation%2520%2528BP%2529%2520achieves%250Astate-of-the-art%2520accuracy%2520but%2520requires%2520global%2520error%2520propagation%2520and%2520full%250Aparameterization%252C%2520leading%2520to%2520substantial%2520memory%2520and%2520computational%2520overhead.%250ADirect%2520Feedback%2520Alignment%2520%2528DFA%2529%2520enables%2520local%252C%2520parallelizable%2520updates%2520with%250Alower%2520memory%2520requirements%2520but%2520is%2520limited%2520by%2520unstructured%2520feedback%2520and%2520poor%250Ascalability%2520in%2520deeper%2520architectures%252C%2520specially%2520convolutional%2520neural%2520networks.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520structured%2520local%2520learning%2520framework%250Athat%2520operates%2520directly%2520on%2520low-rank%2520manifolds%2520defined%2520by%2520the%2520Singular%2520Value%250ADecomposition%2520%2528SVD%2529%2520of%2520weight%2520matrices.%2520Each%2520layer%2520is%2520trained%2520in%2520its%2520decomposed%250Aform%252C%2520with%2520updates%2520applied%2520to%2520the%2520SVD%2520components%2520using%2520a%2520composite%2520loss%2520that%250Aintegrates%2520cross-entropy%252C%2520subspace%2520alignment%252C%2520and%2520orthogonality%2520regularization.%250AFeedback%2520matrices%2520are%2520constructed%2520to%2520match%2520the%2520SVD%2520structure%252C%2520ensuring%250Aconsistent%2520alignment%2520between%2520forward%2520and%2520feedback%2520pathways.%2520Our%2520method%2520reduces%250Athe%2520number%2520of%2520trainable%2520parameters%2520relative%2520to%2520the%2520original%2520DFA%2520model%252C%2520without%250Arelying%2520on%2520pruning%2520or%2520post%2520hoc%2520compression.%2520Experiments%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%250Aand%2520ImageNet%2520show%2520that%2520our%2520method%2520achieves%2520accuracy%2520comparable%2520to%2520that%2520of%2520BP.%250AAblation%2520studies%2520confirm%2520the%2520importance%2520of%2520each%2520loss%2520term%2520in%2520the%2520low-rank%250Asetting.%2520These%2520results%2520establish%2520local%2520learning%2520on%2520low-rank%2520manifolds%2520as%2520a%250Aprincipled%2520and%2520scalable%2520alternative%2520to%2520full-rank%2520gradient-based%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feedback%20Alignment%20Meets%20Low-Rank%20Manifolds%3A%20A%20Structured%20Recipe%20for%0A%20%20Local%20Learning&entry.906535625=Arani%20Roy%20and%20Marco%20P.%20Apolinario%20and%20Shristi%20Das%20Biswas%20and%20Kaushik%20Roy&entry.1292438233=%20%20Training%20deep%20neural%20networks%20%28DNNs%29%20with%20backpropagation%20%28BP%29%20achieves%0Astate-of-the-art%20accuracy%20but%20requires%20global%20error%20propagation%20and%20full%0Aparameterization%2C%20leading%20to%20substantial%20memory%20and%20computational%20overhead.%0ADirect%20Feedback%20Alignment%20%28DFA%29%20enables%20local%2C%20parallelizable%20updates%20with%0Alower%20memory%20requirements%20but%20is%20limited%20by%20unstructured%20feedback%20and%20poor%0Ascalability%20in%20deeper%20architectures%2C%20specially%20convolutional%20neural%20networks.%0ATo%20address%20these%20limitations%2C%20we%20propose%20a%20structured%20local%20learning%20framework%0Athat%20operates%20directly%20on%20low-rank%20manifolds%20defined%20by%20the%20Singular%20Value%0ADecomposition%20%28SVD%29%20of%20weight%20matrices.%20Each%20layer%20is%20trained%20in%20its%20decomposed%0Aform%2C%20with%20updates%20applied%20to%20the%20SVD%20components%20using%20a%20composite%20loss%20that%0Aintegrates%20cross-entropy%2C%20subspace%20alignment%2C%20and%20orthogonality%20regularization.%0AFeedback%20matrices%20are%20constructed%20to%20match%20the%20SVD%20structure%2C%20ensuring%0Aconsistent%20alignment%20between%20forward%20and%20feedback%20pathways.%20Our%20method%20reduces%0Athe%20number%20of%20trainable%20parameters%20relative%20to%20the%20original%20DFA%20model%2C%20without%0Arelying%20on%20pruning%20or%20post%20hoc%20compression.%20Experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%0Aand%20ImageNet%20show%20that%20our%20method%20achieves%20accuracy%20comparable%20to%20that%20of%20BP.%0AAblation%20studies%20confirm%20the%20importance%20of%20each%20loss%20term%20in%20the%20low-rank%0Asetting.%20These%20results%20establish%20local%20learning%20on%20low-rank%20manifolds%20as%20a%0Aprincipled%20and%20scalable%20alternative%20to%20full-rank%20gradient-based%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25594v1&entry.124074799=Read"},
{"title": "FastJAM: a Fast Joint Alignment Model for Images", "author": "Omri Hirsch and Ron Shapira Weber and Shira Ifergane and Oren Freifeld", "abstract": "  Joint Alignment (JA) of images aims to align a collection of images into a\nunified coordinate frame, such that semantically-similar features appear at\ncorresponding spatial locations. Most existing approaches often require long\ntraining times, large-capacity models, and extensive hyperparameter tuning. We\nintroduce FastJAM, a rapid, graph-based method that drastically reduces the\ncomputational complexity of joint alignment tasks. FastJAM leverages pairwise\nmatches computed by an off-the-shelf image matcher, together with a rapid\nnonparametric clustering, to construct a graph representing intra- and\ninter-image keypoint relations. A graph neural network propagates and\naggregates these correspondences, efficiently predicting per-image homography\nparameters via image-level pooling. Utilizing an inverse-compositional loss,\nthat eliminates the need for a regularization term over the predicted\ntransformations (and thus also obviates the hyperparameter tuning associated\nwith such terms), FastJAM performs image JA quickly and effectively.\nExperimental results on several benchmarks demonstrate that FastJAM achieves\nresults better than existing modern JA methods in terms of alignment quality,\nwhile reducing computation time from hours or minutes to mere seconds. Our code\nis available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/\n", "link": "http://arxiv.org/abs/2510.22842v2", "date": "2025-10-29", "relevancy": 2.6004, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5262}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5197}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastJAM%3A%20a%20Fast%20Joint%20Alignment%20Model%20for%20Images&body=Title%3A%20FastJAM%3A%20a%20Fast%20Joint%20Alignment%20Model%20for%20Images%0AAuthor%3A%20Omri%20Hirsch%20and%20Ron%20Shapira%20Weber%20and%20Shira%20Ifergane%20and%20Oren%20Freifeld%0AAbstract%3A%20%20%20Joint%20Alignment%20%28JA%29%20of%20images%20aims%20to%20align%20a%20collection%20of%20images%20into%20a%0Aunified%20coordinate%20frame%2C%20such%20that%20semantically-similar%20features%20appear%20at%0Acorresponding%20spatial%20locations.%20Most%20existing%20approaches%20often%20require%20long%0Atraining%20times%2C%20large-capacity%20models%2C%20and%20extensive%20hyperparameter%20tuning.%20We%0Aintroduce%20FastJAM%2C%20a%20rapid%2C%20graph-based%20method%20that%20drastically%20reduces%20the%0Acomputational%20complexity%20of%20joint%20alignment%20tasks.%20FastJAM%20leverages%20pairwise%0Amatches%20computed%20by%20an%20off-the-shelf%20image%20matcher%2C%20together%20with%20a%20rapid%0Anonparametric%20clustering%2C%20to%20construct%20a%20graph%20representing%20intra-%20and%0Ainter-image%20keypoint%20relations.%20A%20graph%20neural%20network%20propagates%20and%0Aaggregates%20these%20correspondences%2C%20efficiently%20predicting%20per-image%20homography%0Aparameters%20via%20image-level%20pooling.%20Utilizing%20an%20inverse-compositional%20loss%2C%0Athat%20eliminates%20the%20need%20for%20a%20regularization%20term%20over%20the%20predicted%0Atransformations%20%28and%20thus%20also%20obviates%20the%20hyperparameter%20tuning%20associated%0Awith%20such%20terms%29%2C%20FastJAM%20performs%20image%20JA%20quickly%20and%20effectively.%0AExperimental%20results%20on%20several%20benchmarks%20demonstrate%20that%20FastJAM%20achieves%0Aresults%20better%20than%20existing%20modern%20JA%20methods%20in%20terms%20of%20alignment%20quality%2C%0Awhile%20reducing%20computation%20time%20from%20hours%20or%20minutes%20to%20mere%20seconds.%20Our%20code%0Ais%20available%20at%20our%20project%20webpage%2C%20https%3A//bgu-cs-vil.github.io/FastJAM/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.22842v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastJAM%253A%2520a%2520Fast%2520Joint%2520Alignment%2520Model%2520for%2520Images%26entry.906535625%3DOmri%2520Hirsch%2520and%2520Ron%2520Shapira%2520Weber%2520and%2520Shira%2520Ifergane%2520and%2520Oren%2520Freifeld%26entry.1292438233%3D%2520%2520Joint%2520Alignment%2520%2528JA%2529%2520of%2520images%2520aims%2520to%2520align%2520a%2520collection%2520of%2520images%2520into%2520a%250Aunified%2520coordinate%2520frame%252C%2520such%2520that%2520semantically-similar%2520features%2520appear%2520at%250Acorresponding%2520spatial%2520locations.%2520Most%2520existing%2520approaches%2520often%2520require%2520long%250Atraining%2520times%252C%2520large-capacity%2520models%252C%2520and%2520extensive%2520hyperparameter%2520tuning.%2520We%250Aintroduce%2520FastJAM%252C%2520a%2520rapid%252C%2520graph-based%2520method%2520that%2520drastically%2520reduces%2520the%250Acomputational%2520complexity%2520of%2520joint%2520alignment%2520tasks.%2520FastJAM%2520leverages%2520pairwise%250Amatches%2520computed%2520by%2520an%2520off-the-shelf%2520image%2520matcher%252C%2520together%2520with%2520a%2520rapid%250Anonparametric%2520clustering%252C%2520to%2520construct%2520a%2520graph%2520representing%2520intra-%2520and%250Ainter-image%2520keypoint%2520relations.%2520A%2520graph%2520neural%2520network%2520propagates%2520and%250Aaggregates%2520these%2520correspondences%252C%2520efficiently%2520predicting%2520per-image%2520homography%250Aparameters%2520via%2520image-level%2520pooling.%2520Utilizing%2520an%2520inverse-compositional%2520loss%252C%250Athat%2520eliminates%2520the%2520need%2520for%2520a%2520regularization%2520term%2520over%2520the%2520predicted%250Atransformations%2520%2528and%2520thus%2520also%2520obviates%2520the%2520hyperparameter%2520tuning%2520associated%250Awith%2520such%2520terms%2529%252C%2520FastJAM%2520performs%2520image%2520JA%2520quickly%2520and%2520effectively.%250AExperimental%2520results%2520on%2520several%2520benchmarks%2520demonstrate%2520that%2520FastJAM%2520achieves%250Aresults%2520better%2520than%2520existing%2520modern%2520JA%2520methods%2520in%2520terms%2520of%2520alignment%2520quality%252C%250Awhile%2520reducing%2520computation%2520time%2520from%2520hours%2520or%2520minutes%2520to%2520mere%2520seconds.%2520Our%2520code%250Ais%2520available%2520at%2520our%2520project%2520webpage%252C%2520https%253A//bgu-cs-vil.github.io/FastJAM/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22842v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastJAM%3A%20a%20Fast%20Joint%20Alignment%20Model%20for%20Images&entry.906535625=Omri%20Hirsch%20and%20Ron%20Shapira%20Weber%20and%20Shira%20Ifergane%20and%20Oren%20Freifeld&entry.1292438233=%20%20Joint%20Alignment%20%28JA%29%20of%20images%20aims%20to%20align%20a%20collection%20of%20images%20into%20a%0Aunified%20coordinate%20frame%2C%20such%20that%20semantically-similar%20features%20appear%20at%0Acorresponding%20spatial%20locations.%20Most%20existing%20approaches%20often%20require%20long%0Atraining%20times%2C%20large-capacity%20models%2C%20and%20extensive%20hyperparameter%20tuning.%20We%0Aintroduce%20FastJAM%2C%20a%20rapid%2C%20graph-based%20method%20that%20drastically%20reduces%20the%0Acomputational%20complexity%20of%20joint%20alignment%20tasks.%20FastJAM%20leverages%20pairwise%0Amatches%20computed%20by%20an%20off-the-shelf%20image%20matcher%2C%20together%20with%20a%20rapid%0Anonparametric%20clustering%2C%20to%20construct%20a%20graph%20representing%20intra-%20and%0Ainter-image%20keypoint%20relations.%20A%20graph%20neural%20network%20propagates%20and%0Aaggregates%20these%20correspondences%2C%20efficiently%20predicting%20per-image%20homography%0Aparameters%20via%20image-level%20pooling.%20Utilizing%20an%20inverse-compositional%20loss%2C%0Athat%20eliminates%20the%20need%20for%20a%20regularization%20term%20over%20the%20predicted%0Atransformations%20%28and%20thus%20also%20obviates%20the%20hyperparameter%20tuning%20associated%0Awith%20such%20terms%29%2C%20FastJAM%20performs%20image%20JA%20quickly%20and%20effectively.%0AExperimental%20results%20on%20several%20benchmarks%20demonstrate%20that%20FastJAM%20achieves%0Aresults%20better%20than%20existing%20modern%20JA%20methods%20in%20terms%20of%20alignment%20quality%2C%0Awhile%20reducing%20computation%20time%20from%20hours%20or%20minutes%20to%20mere%20seconds.%20Our%20code%0Ais%20available%20at%20our%20project%20webpage%2C%20https%3A//bgu-cs-vil.github.io/FastJAM/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.22842v2&entry.124074799=Read"},
{"title": "Instance-Level Composed Image Retrieval", "author": "Bill Psomas and George Retsinas and Nikos Efthymiadis and Panagiotis Filntisis and Yannis Avrithis and Petros Maragos and Ondrej Chum and Giorgos Tolias", "abstract": "  The progress of composed image retrieval (CIR), a popular research direction\nin image retrieval, where a combined visual and textual query is used, is held\nback by the absence of high-quality training and evaluation data. We introduce\na new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an\ninstance-level class definition. The goal is to retrieve images that contain\nthe same particular object as the visual query, presented under a variety of\nmodifications defined by textual queries. Its design and curation process keep\nthe dataset compact to facilitate future research, while maintaining its\nchallenge-comparable to retrieval among more than 40M random\ndistractors-through a semi-automated selection of hard negatives.\n  To overcome the challenge of obtaining clean, diverse, and suitable training\ndata, we leverage pre-trained vision-and-language models (VLMs) in a\ntraining-free approach called BASIC. The method separately estimates\nquery-image-to-image and query-text-to-image similarities, performing late\nfusion to upweight images that satisfy both queries, while down-weighting those\nthat exhibit high similarity with only one of the two. Each individual\nsimilarity is further improved by a set of components that are simple and\nintuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR\ndatasets that follow a semantic-level class definition. Project page:\nhttps://vrg.fel.cvut.cz/icir/.\n", "link": "http://arxiv.org/abs/2510.25387v1", "date": "2025-10-29", "relevancy": 2.5986, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5273}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5273}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance-Level%20Composed%20Image%20Retrieval&body=Title%3A%20Instance-Level%20Composed%20Image%20Retrieval%0AAuthor%3A%20Bill%20Psomas%20and%20George%20Retsinas%20and%20Nikos%20Efthymiadis%20and%20Panagiotis%20Filntisis%20and%20Yannis%20Avrithis%20and%20Petros%20Maragos%20and%20Ondrej%20Chum%20and%20Giorgos%20Tolias%0AAbstract%3A%20%20%20The%20progress%20of%20composed%20image%20retrieval%20%28CIR%29%2C%20a%20popular%20research%20direction%0Ain%20image%20retrieval%2C%20where%20a%20combined%20visual%20and%20textual%20query%20is%20used%2C%20is%20held%0Aback%20by%20the%20absence%20of%20high-quality%20training%20and%20evaluation%20data.%20We%20introduce%0Aa%20new%20evaluation%20dataset%2C%20i-CIR%2C%20which%2C%20unlike%20existing%20datasets%2C%20focuses%20on%20an%0Ainstance-level%20class%20definition.%20The%20goal%20is%20to%20retrieve%20images%20that%20contain%0Athe%20same%20particular%20object%20as%20the%20visual%20query%2C%20presented%20under%20a%20variety%20of%0Amodifications%20defined%20by%20textual%20queries.%20Its%20design%20and%20curation%20process%20keep%0Athe%20dataset%20compact%20to%20facilitate%20future%20research%2C%20while%20maintaining%20its%0Achallenge-comparable%20to%20retrieval%20among%20more%20than%2040M%20random%0Adistractors-through%20a%20semi-automated%20selection%20of%20hard%20negatives.%0A%20%20To%20overcome%20the%20challenge%20of%20obtaining%20clean%2C%20diverse%2C%20and%20suitable%20training%0Adata%2C%20we%20leverage%20pre-trained%20vision-and-language%20models%20%28VLMs%29%20in%20a%0Atraining-free%20approach%20called%20BASIC.%20The%20method%20separately%20estimates%0Aquery-image-to-image%20and%20query-text-to-image%20similarities%2C%20performing%20late%0Afusion%20to%20upweight%20images%20that%20satisfy%20both%20queries%2C%20while%20down-weighting%20those%0Athat%20exhibit%20high%20similarity%20with%20only%20one%20of%20the%20two.%20Each%20individual%0Asimilarity%20is%20further%20improved%20by%20a%20set%20of%20components%20that%20are%20simple%20and%0Aintuitive.%20BASIC%20sets%20a%20new%20state%20of%20the%20art%20on%20i-CIR%20but%20also%20on%20existing%20CIR%0Adatasets%20that%20follow%20a%20semantic-level%20class%20definition.%20Project%20page%3A%0Ahttps%3A//vrg.fel.cvut.cz/icir/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance-Level%2520Composed%2520Image%2520Retrieval%26entry.906535625%3DBill%2520Psomas%2520and%2520George%2520Retsinas%2520and%2520Nikos%2520Efthymiadis%2520and%2520Panagiotis%2520Filntisis%2520and%2520Yannis%2520Avrithis%2520and%2520Petros%2520Maragos%2520and%2520Ondrej%2520Chum%2520and%2520Giorgos%2520Tolias%26entry.1292438233%3D%2520%2520The%2520progress%2520of%2520composed%2520image%2520retrieval%2520%2528CIR%2529%252C%2520a%2520popular%2520research%2520direction%250Ain%2520image%2520retrieval%252C%2520where%2520a%2520combined%2520visual%2520and%2520textual%2520query%2520is%2520used%252C%2520is%2520held%250Aback%2520by%2520the%2520absence%2520of%2520high-quality%2520training%2520and%2520evaluation%2520data.%2520We%2520introduce%250Aa%2520new%2520evaluation%2520dataset%252C%2520i-CIR%252C%2520which%252C%2520unlike%2520existing%2520datasets%252C%2520focuses%2520on%2520an%250Ainstance-level%2520class%2520definition.%2520The%2520goal%2520is%2520to%2520retrieve%2520images%2520that%2520contain%250Athe%2520same%2520particular%2520object%2520as%2520the%2520visual%2520query%252C%2520presented%2520under%2520a%2520variety%2520of%250Amodifications%2520defined%2520by%2520textual%2520queries.%2520Its%2520design%2520and%2520curation%2520process%2520keep%250Athe%2520dataset%2520compact%2520to%2520facilitate%2520future%2520research%252C%2520while%2520maintaining%2520its%250Achallenge-comparable%2520to%2520retrieval%2520among%2520more%2520than%252040M%2520random%250Adistractors-through%2520a%2520semi-automated%2520selection%2520of%2520hard%2520negatives.%250A%2520%2520To%2520overcome%2520the%2520challenge%2520of%2520obtaining%2520clean%252C%2520diverse%252C%2520and%2520suitable%2520training%250Adata%252C%2520we%2520leverage%2520pre-trained%2520vision-and-language%2520models%2520%2528VLMs%2529%2520in%2520a%250Atraining-free%2520approach%2520called%2520BASIC.%2520The%2520method%2520separately%2520estimates%250Aquery-image-to-image%2520and%2520query-text-to-image%2520similarities%252C%2520performing%2520late%250Afusion%2520to%2520upweight%2520images%2520that%2520satisfy%2520both%2520queries%252C%2520while%2520down-weighting%2520those%250Athat%2520exhibit%2520high%2520similarity%2520with%2520only%2520one%2520of%2520the%2520two.%2520Each%2520individual%250Asimilarity%2520is%2520further%2520improved%2520by%2520a%2520set%2520of%2520components%2520that%2520are%2520simple%2520and%250Aintuitive.%2520BASIC%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%2520on%2520i-CIR%2520but%2520also%2520on%2520existing%2520CIR%250Adatasets%2520that%2520follow%2520a%2520semantic-level%2520class%2520definition.%2520Project%2520page%253A%250Ahttps%253A//vrg.fel.cvut.cz/icir/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-Level%20Composed%20Image%20Retrieval&entry.906535625=Bill%20Psomas%20and%20George%20Retsinas%20and%20Nikos%20Efthymiadis%20and%20Panagiotis%20Filntisis%20and%20Yannis%20Avrithis%20and%20Petros%20Maragos%20and%20Ondrej%20Chum%20and%20Giorgos%20Tolias&entry.1292438233=%20%20The%20progress%20of%20composed%20image%20retrieval%20%28CIR%29%2C%20a%20popular%20research%20direction%0Ain%20image%20retrieval%2C%20where%20a%20combined%20visual%20and%20textual%20query%20is%20used%2C%20is%20held%0Aback%20by%20the%20absence%20of%20high-quality%20training%20and%20evaluation%20data.%20We%20introduce%0Aa%20new%20evaluation%20dataset%2C%20i-CIR%2C%20which%2C%20unlike%20existing%20datasets%2C%20focuses%20on%20an%0Ainstance-level%20class%20definition.%20The%20goal%20is%20to%20retrieve%20images%20that%20contain%0Athe%20same%20particular%20object%20as%20the%20visual%20query%2C%20presented%20under%20a%20variety%20of%0Amodifications%20defined%20by%20textual%20queries.%20Its%20design%20and%20curation%20process%20keep%0Athe%20dataset%20compact%20to%20facilitate%20future%20research%2C%20while%20maintaining%20its%0Achallenge-comparable%20to%20retrieval%20among%20more%20than%2040M%20random%0Adistractors-through%20a%20semi-automated%20selection%20of%20hard%20negatives.%0A%20%20To%20overcome%20the%20challenge%20of%20obtaining%20clean%2C%20diverse%2C%20and%20suitable%20training%0Adata%2C%20we%20leverage%20pre-trained%20vision-and-language%20models%20%28VLMs%29%20in%20a%0Atraining-free%20approach%20called%20BASIC.%20The%20method%20separately%20estimates%0Aquery-image-to-image%20and%20query-text-to-image%20similarities%2C%20performing%20late%0Afusion%20to%20upweight%20images%20that%20satisfy%20both%20queries%2C%20while%20down-weighting%20those%0Athat%20exhibit%20high%20similarity%20with%20only%20one%20of%20the%20two.%20Each%20individual%0Asimilarity%20is%20further%20improved%20by%20a%20set%20of%20components%20that%20are%20simple%20and%0Aintuitive.%20BASIC%20sets%20a%20new%20state%20of%20the%20art%20on%20i-CIR%20but%20also%20on%20existing%20CIR%0Adatasets%20that%20follow%20a%20semantic-level%20class%20definition.%20Project%20page%3A%0Ahttps%3A//vrg.fel.cvut.cz/icir/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25387v1&entry.124074799=Read"},
{"title": "ASGO: Adaptive Structured Gradient Optimization", "author": "Kang An and Yuxing Liu and Rui Pan and Yi Ren and Shiqian Ma and Donald Goldfarb and Tong Zhang", "abstract": "  Training deep neural networks is a structured optimization problem, because\nthe parameters are naturally represented by matrices and tensors rather than by\nvectors. Under this structural representation, it has been widely observed that\ngradients are low-rank and Hessians are approximately block diagonal. These\nstructured properties are crucial for designing efficient optimization\nalgorithms, but are not utilized by many current popular optimizers like Adam.\nIn this paper, we present a novel optimization algorithm ASGO that capitalizes\non these properties by employing a preconditioner that is adaptively updated\nusing structured gradients. By a fine-grained theoretical analysis, ASGO is\nproven to achieve superior convergence rates compared to existing structured\ngradient methods. Based on this convergence theory, we further demonstrate that\nASGO can benefit from low-rank gradients and block diagonal Hessians. We also\ndiscuss practical modifications of ASGO and empirically verify ASGO's\neffectiveness on language model tasks. Code is available at\nhttps://github.com/infinity-stars/ASGO.\n", "link": "http://arxiv.org/abs/2503.20762v3", "date": "2025-10-29", "relevancy": 2.5781, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.547}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5031}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASGO%3A%20Adaptive%20Structured%20Gradient%20Optimization&body=Title%3A%20ASGO%3A%20Adaptive%20Structured%20Gradient%20Optimization%0AAuthor%3A%20Kang%20An%20and%20Yuxing%20Liu%20and%20Rui%20Pan%20and%20Yi%20Ren%20and%20Shiqian%20Ma%20and%20Donald%20Goldfarb%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Training%20deep%20neural%20networks%20is%20a%20structured%20optimization%20problem%2C%20because%0Athe%20parameters%20are%20naturally%20represented%20by%20matrices%20and%20tensors%20rather%20than%20by%0Avectors.%20Under%20this%20structural%20representation%2C%20it%20has%20been%20widely%20observed%20that%0Agradients%20are%20low-rank%20and%20Hessians%20are%20approximately%20block%20diagonal.%20These%0Astructured%20properties%20are%20crucial%20for%20designing%20efficient%20optimization%0Aalgorithms%2C%20but%20are%20not%20utilized%20by%20many%20current%20popular%20optimizers%20like%20Adam.%0AIn%20this%20paper%2C%20we%20present%20a%20novel%20optimization%20algorithm%20ASGO%20that%20capitalizes%0Aon%20these%20properties%20by%20employing%20a%20preconditioner%20that%20is%20adaptively%20updated%0Ausing%20structured%20gradients.%20By%20a%20fine-grained%20theoretical%20analysis%2C%20ASGO%20is%0Aproven%20to%20achieve%20superior%20convergence%20rates%20compared%20to%20existing%20structured%0Agradient%20methods.%20Based%20on%20this%20convergence%20theory%2C%20we%20further%20demonstrate%20that%0AASGO%20can%20benefit%20from%20low-rank%20gradients%20and%20block%20diagonal%20Hessians.%20We%20also%0Adiscuss%20practical%20modifications%20of%20ASGO%20and%20empirically%20verify%20ASGO%27s%0Aeffectiveness%20on%20language%20model%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/infinity-stars/ASGO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.20762v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASGO%253A%2520Adaptive%2520Structured%2520Gradient%2520Optimization%26entry.906535625%3DKang%2520An%2520and%2520Yuxing%2520Liu%2520and%2520Rui%2520Pan%2520and%2520Yi%2520Ren%2520and%2520Shiqian%2520Ma%2520and%2520Donald%2520Goldfarb%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Training%2520deep%2520neural%2520networks%2520is%2520a%2520structured%2520optimization%2520problem%252C%2520because%250Athe%2520parameters%2520are%2520naturally%2520represented%2520by%2520matrices%2520and%2520tensors%2520rather%2520than%2520by%250Avectors.%2520Under%2520this%2520structural%2520representation%252C%2520it%2520has%2520been%2520widely%2520observed%2520that%250Agradients%2520are%2520low-rank%2520and%2520Hessians%2520are%2520approximately%2520block%2520diagonal.%2520These%250Astructured%2520properties%2520are%2520crucial%2520for%2520designing%2520efficient%2520optimization%250Aalgorithms%252C%2520but%2520are%2520not%2520utilized%2520by%2520many%2520current%2520popular%2520optimizers%2520like%2520Adam.%250AIn%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520optimization%2520algorithm%2520ASGO%2520that%2520capitalizes%250Aon%2520these%2520properties%2520by%2520employing%2520a%2520preconditioner%2520that%2520is%2520adaptively%2520updated%250Ausing%2520structured%2520gradients.%2520By%2520a%2520fine-grained%2520theoretical%2520analysis%252C%2520ASGO%2520is%250Aproven%2520to%2520achieve%2520superior%2520convergence%2520rates%2520compared%2520to%2520existing%2520structured%250Agradient%2520methods.%2520Based%2520on%2520this%2520convergence%2520theory%252C%2520we%2520further%2520demonstrate%2520that%250AASGO%2520can%2520benefit%2520from%2520low-rank%2520gradients%2520and%2520block%2520diagonal%2520Hessians.%2520We%2520also%250Adiscuss%2520practical%2520modifications%2520of%2520ASGO%2520and%2520empirically%2520verify%2520ASGO%2527s%250Aeffectiveness%2520on%2520language%2520model%2520tasks.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/infinity-stars/ASGO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20762v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASGO%3A%20Adaptive%20Structured%20Gradient%20Optimization&entry.906535625=Kang%20An%20and%20Yuxing%20Liu%20and%20Rui%20Pan%20and%20Yi%20Ren%20and%20Shiqian%20Ma%20and%20Donald%20Goldfarb%20and%20Tong%20Zhang&entry.1292438233=%20%20Training%20deep%20neural%20networks%20is%20a%20structured%20optimization%20problem%2C%20because%0Athe%20parameters%20are%20naturally%20represented%20by%20matrices%20and%20tensors%20rather%20than%20by%0Avectors.%20Under%20this%20structural%20representation%2C%20it%20has%20been%20widely%20observed%20that%0Agradients%20are%20low-rank%20and%20Hessians%20are%20approximately%20block%20diagonal.%20These%0Astructured%20properties%20are%20crucial%20for%20designing%20efficient%20optimization%0Aalgorithms%2C%20but%20are%20not%20utilized%20by%20many%20current%20popular%20optimizers%20like%20Adam.%0AIn%20this%20paper%2C%20we%20present%20a%20novel%20optimization%20algorithm%20ASGO%20that%20capitalizes%0Aon%20these%20properties%20by%20employing%20a%20preconditioner%20that%20is%20adaptively%20updated%0Ausing%20structured%20gradients.%20By%20a%20fine-grained%20theoretical%20analysis%2C%20ASGO%20is%0Aproven%20to%20achieve%20superior%20convergence%20rates%20compared%20to%20existing%20structured%0Agradient%20methods.%20Based%20on%20this%20convergence%20theory%2C%20we%20further%20demonstrate%20that%0AASGO%20can%20benefit%20from%20low-rank%20gradients%20and%20block%20diagonal%20Hessians.%20We%20also%0Adiscuss%20practical%20modifications%20of%20ASGO%20and%20empirically%20verify%20ASGO%27s%0Aeffectiveness%20on%20language%20model%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/infinity-stars/ASGO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.20762v3&entry.124074799=Read"},
{"title": "Convolutional Spiking-based GRU Cell for Spatio-temporal Data", "author": "Yesmine Abdennadher and Eleonora Cicciarella and Michele Rossi", "abstract": "  Spike-based temporal messaging enables SNNs to efficiently process both\npurely temporal and spatio-temporal time-series or event-driven data. Combining\nSNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,\ngives rise to a robust framework for sequential data processing; however,\ntraditional RNNs often lose local details when handling long sequences.\nPrevious approaches, such as SpikGRU, fail to capture fine-grained local\ndependencies in event-based spatio-temporal data. In this paper, we introduce\nthe Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional\noperations to preserve local structure and dependencies while integrating the\ntemporal precision of spiking neurons with the efficient gating mechanisms of\nGRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,\nSHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our\nexperiments show that CS-GRU outperforms state-of-the-art GRU variants by an\naverage of 4.35%, achieving over 90% accuracy on sequential tasks and up to\n99.31% on MNIST. It is worth noting that our solution achieves 69% higher\nefficiency compared to SpikGRU. The code is available at:\nhttps://github.com/YesmineAbdennadher/CS-GRU.\n", "link": "http://arxiv.org/abs/2510.25696v1", "date": "2025-10-29", "relevancy": 2.5444, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5258}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5094}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convolutional%20Spiking-based%20GRU%20Cell%20for%20Spatio-temporal%20Data&body=Title%3A%20Convolutional%20Spiking-based%20GRU%20Cell%20for%20Spatio-temporal%20Data%0AAuthor%3A%20Yesmine%20Abdennadher%20and%20Eleonora%20Cicciarella%20and%20Michele%20Rossi%0AAbstract%3A%20%20%20Spike-based%20temporal%20messaging%20enables%20SNNs%20to%20efficiently%20process%20both%0Apurely%20temporal%20and%20spatio-temporal%20time-series%20or%20event-driven%20data.%20Combining%0ASNNs%20with%20Gated%20Recurrent%20Units%20%28GRUs%29%2C%20a%20variant%20of%20recurrent%20neural%20networks%2C%0Agives%20rise%20to%20a%20robust%20framework%20for%20sequential%20data%20processing%3B%20however%2C%0Atraditional%20RNNs%20often%20lose%20local%20details%20when%20handling%20long%20sequences.%0APrevious%20approaches%2C%20such%20as%20SpikGRU%2C%20fail%20to%20capture%20fine-grained%20local%0Adependencies%20in%20event-based%20spatio-temporal%20data.%20In%20this%20paper%2C%20we%20introduce%0Athe%20Convolutional%20Spiking%20GRU%20%28CS-GRU%29%20cell%2C%20which%20leverages%20convolutional%0Aoperations%20to%20preserve%20local%20structure%20and%20dependencies%20while%20integrating%20the%0Atemporal%20precision%20of%20spiking%20neurons%20with%20the%20efficient%20gating%20mechanisms%20of%0AGRUs.%20This%20versatile%20architecture%20excels%20on%20both%20temporal%20datasets%20%28NTIDIGITS%2C%0ASHD%29%20and%20spatio-temporal%20benchmarks%20%28MNIST%2C%20DVSGesture%2C%20CIFAR10DVS%29.%20Our%0Aexperiments%20show%20that%20CS-GRU%20outperforms%20state-of-the-art%20GRU%20variants%20by%20an%0Aaverage%20of%204.35%25%2C%20achieving%20over%2090%25%20accuracy%20on%20sequential%20tasks%20and%20up%20to%0A99.31%25%20on%20MNIST.%20It%20is%20worth%20noting%20that%20our%20solution%20achieves%2069%25%20higher%0Aefficiency%20compared%20to%20SpikGRU.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/YesmineAbdennadher/CS-GRU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvolutional%2520Spiking-based%2520GRU%2520Cell%2520for%2520Spatio-temporal%2520Data%26entry.906535625%3DYesmine%2520Abdennadher%2520and%2520Eleonora%2520Cicciarella%2520and%2520Michele%2520Rossi%26entry.1292438233%3D%2520%2520Spike-based%2520temporal%2520messaging%2520enables%2520SNNs%2520to%2520efficiently%2520process%2520both%250Apurely%2520temporal%2520and%2520spatio-temporal%2520time-series%2520or%2520event-driven%2520data.%2520Combining%250ASNNs%2520with%2520Gated%2520Recurrent%2520Units%2520%2528GRUs%2529%252C%2520a%2520variant%2520of%2520recurrent%2520neural%2520networks%252C%250Agives%2520rise%2520to%2520a%2520robust%2520framework%2520for%2520sequential%2520data%2520processing%253B%2520however%252C%250Atraditional%2520RNNs%2520often%2520lose%2520local%2520details%2520when%2520handling%2520long%2520sequences.%250APrevious%2520approaches%252C%2520such%2520as%2520SpikGRU%252C%2520fail%2520to%2520capture%2520fine-grained%2520local%250Adependencies%2520in%2520event-based%2520spatio-temporal%2520data.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Athe%2520Convolutional%2520Spiking%2520GRU%2520%2528CS-GRU%2529%2520cell%252C%2520which%2520leverages%2520convolutional%250Aoperations%2520to%2520preserve%2520local%2520structure%2520and%2520dependencies%2520while%2520integrating%2520the%250Atemporal%2520precision%2520of%2520spiking%2520neurons%2520with%2520the%2520efficient%2520gating%2520mechanisms%2520of%250AGRUs.%2520This%2520versatile%2520architecture%2520excels%2520on%2520both%2520temporal%2520datasets%2520%2528NTIDIGITS%252C%250ASHD%2529%2520and%2520spatio-temporal%2520benchmarks%2520%2528MNIST%252C%2520DVSGesture%252C%2520CIFAR10DVS%2529.%2520Our%250Aexperiments%2520show%2520that%2520CS-GRU%2520outperforms%2520state-of-the-art%2520GRU%2520variants%2520by%2520an%250Aaverage%2520of%25204.35%2525%252C%2520achieving%2520over%252090%2525%2520accuracy%2520on%2520sequential%2520tasks%2520and%2520up%2520to%250A99.31%2525%2520on%2520MNIST.%2520It%2520is%2520worth%2520noting%2520that%2520our%2520solution%2520achieves%252069%2525%2520higher%250Aefficiency%2520compared%2520to%2520SpikGRU.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/YesmineAbdennadher/CS-GRU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convolutional%20Spiking-based%20GRU%20Cell%20for%20Spatio-temporal%20Data&entry.906535625=Yesmine%20Abdennadher%20and%20Eleonora%20Cicciarella%20and%20Michele%20Rossi&entry.1292438233=%20%20Spike-based%20temporal%20messaging%20enables%20SNNs%20to%20efficiently%20process%20both%0Apurely%20temporal%20and%20spatio-temporal%20time-series%20or%20event-driven%20data.%20Combining%0ASNNs%20with%20Gated%20Recurrent%20Units%20%28GRUs%29%2C%20a%20variant%20of%20recurrent%20neural%20networks%2C%0Agives%20rise%20to%20a%20robust%20framework%20for%20sequential%20data%20processing%3B%20however%2C%0Atraditional%20RNNs%20often%20lose%20local%20details%20when%20handling%20long%20sequences.%0APrevious%20approaches%2C%20such%20as%20SpikGRU%2C%20fail%20to%20capture%20fine-grained%20local%0Adependencies%20in%20event-based%20spatio-temporal%20data.%20In%20this%20paper%2C%20we%20introduce%0Athe%20Convolutional%20Spiking%20GRU%20%28CS-GRU%29%20cell%2C%20which%20leverages%20convolutional%0Aoperations%20to%20preserve%20local%20structure%20and%20dependencies%20while%20integrating%20the%0Atemporal%20precision%20of%20spiking%20neurons%20with%20the%20efficient%20gating%20mechanisms%20of%0AGRUs.%20This%20versatile%20architecture%20excels%20on%20both%20temporal%20datasets%20%28NTIDIGITS%2C%0ASHD%29%20and%20spatio-temporal%20benchmarks%20%28MNIST%2C%20DVSGesture%2C%20CIFAR10DVS%29.%20Our%0Aexperiments%20show%20that%20CS-GRU%20outperforms%20state-of-the-art%20GRU%20variants%20by%20an%0Aaverage%20of%204.35%25%2C%20achieving%20over%2090%25%20accuracy%20on%20sequential%20tasks%20and%20up%20to%0A99.31%25%20on%20MNIST.%20It%20is%20worth%20noting%20that%20our%20solution%20achieves%2069%25%20higher%0Aefficiency%20compared%20to%20SpikGRU.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/YesmineAbdennadher/CS-GRU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25696v1&entry.124074799=Read"},
{"title": "Zero Reinforcement Learning Towards General Domains", "author": "Yuyuan Zeng and Yufei Huang and Can Xu and Qingfeng Sun and Jianfeng Yan and Guanghui Xu and Tao Yang and Fengzong Lian", "abstract": "  Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach\nfor enhancing the reasoning capabilities of large language models (LLMs) by\ndirectly applying reinforcement learning with verifiable rewards on pretrained\nmodels, without the need for a supervised fine-tuning phase. However, current\nresearch on zero-RL primarily focuses on domains with easily verifiable reward\nsignals, such as mathematics, programming, and other reasoning tasks. The\nchallenge of eliciting reasoning abilities in more diverse scenarios, where\nverification is not straightforward, remains underexplored. To address this\ngap, we propose a novel zero-RL paradigm designed to improve a model's\nreasoning ability across both verifiable and non-verifiable domains. By\ncombining verifiable rewards with a generative reward model, we conduct\nmulti-task zero-RL training across both domains, facilitating the transfer of\nreasoning capabilities between them. Furthermore, to mitigate reward hacking in\nthe generative reward model, we design a smooth length penalty that encourages\nthe generation of more comprehensive thinking tokens in general domains.\nExperimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our\napproach achieves superior reasoning performance, not only on tasks requiring\nextensive reasoning but also on more general tasks.\n", "link": "http://arxiv.org/abs/2510.25528v1", "date": "2025-10-29", "relevancy": 2.5427, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero%20Reinforcement%20Learning%20Towards%20General%20Domains&body=Title%3A%20Zero%20Reinforcement%20Learning%20Towards%20General%20Domains%0AAuthor%3A%20Yuyuan%20Zeng%20and%20Yufei%20Huang%20and%20Can%20Xu%20and%20Qingfeng%20Sun%20and%20Jianfeng%20Yan%20and%20Guanghui%20Xu%20and%20Tao%20Yang%20and%20Fengzong%20Lian%0AAbstract%3A%20%20%20Zero%20Reinforcement%20Learning%20%28Zero-RL%29%20has%20proven%20to%20be%20an%20effective%20approach%0Afor%20enhancing%20the%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20by%0Adirectly%20applying%20reinforcement%20learning%20with%20verifiable%20rewards%20on%20pretrained%0Amodels%2C%20without%20the%20need%20for%20a%20supervised%20fine-tuning%20phase.%20However%2C%20current%0Aresearch%20on%20zero-RL%20primarily%20focuses%20on%20domains%20with%20easily%20verifiable%20reward%0Asignals%2C%20such%20as%20mathematics%2C%20programming%2C%20and%20other%20reasoning%20tasks.%20The%0Achallenge%20of%20eliciting%20reasoning%20abilities%20in%20more%20diverse%20scenarios%2C%20where%0Averification%20is%20not%20straightforward%2C%20remains%20underexplored.%20To%20address%20this%0Agap%2C%20we%20propose%20a%20novel%20zero-RL%20paradigm%20designed%20to%20improve%20a%20model%27s%0Areasoning%20ability%20across%20both%20verifiable%20and%20non-verifiable%20domains.%20By%0Acombining%20verifiable%20rewards%20with%20a%20generative%20reward%20model%2C%20we%20conduct%0Amulti-task%20zero-RL%20training%20across%20both%20domains%2C%20facilitating%20the%20transfer%20of%0Areasoning%20capabilities%20between%20them.%20Furthermore%2C%20to%20mitigate%20reward%20hacking%20in%0Athe%20generative%20reward%20model%2C%20we%20design%20a%20smooth%20length%20penalty%20that%20encourages%0Athe%20generation%20of%20more%20comprehensive%20thinking%20tokens%20in%20general%20domains.%0AExperimental%20results%20on%20Qwen3-8B-Base%20and%20Qwen3-14B-Base%20demonstrate%20that%20our%0Aapproach%20achieves%20superior%20reasoning%20performance%2C%20not%20only%20on%20tasks%20requiring%0Aextensive%20reasoning%20but%20also%20on%20more%20general%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero%2520Reinforcement%2520Learning%2520Towards%2520General%2520Domains%26entry.906535625%3DYuyuan%2520Zeng%2520and%2520Yufei%2520Huang%2520and%2520Can%2520Xu%2520and%2520Qingfeng%2520Sun%2520and%2520Jianfeng%2520Yan%2520and%2520Guanghui%2520Xu%2520and%2520Tao%2520Yang%2520and%2520Fengzong%2520Lian%26entry.1292438233%3D%2520%2520Zero%2520Reinforcement%2520Learning%2520%2528Zero-RL%2529%2520has%2520proven%2520to%2520be%2520an%2520effective%2520approach%250Afor%2520enhancing%2520the%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%250Adirectly%2520applying%2520reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520on%2520pretrained%250Amodels%252C%2520without%2520the%2520need%2520for%2520a%2520supervised%2520fine-tuning%2520phase.%2520However%252C%2520current%250Aresearch%2520on%2520zero-RL%2520primarily%2520focuses%2520on%2520domains%2520with%2520easily%2520verifiable%2520reward%250Asignals%252C%2520such%2520as%2520mathematics%252C%2520programming%252C%2520and%2520other%2520reasoning%2520tasks.%2520The%250Achallenge%2520of%2520eliciting%2520reasoning%2520abilities%2520in%2520more%2520diverse%2520scenarios%252C%2520where%250Averification%2520is%2520not%2520straightforward%252C%2520remains%2520underexplored.%2520To%2520address%2520this%250Agap%252C%2520we%2520propose%2520a%2520novel%2520zero-RL%2520paradigm%2520designed%2520to%2520improve%2520a%2520model%2527s%250Areasoning%2520ability%2520across%2520both%2520verifiable%2520and%2520non-verifiable%2520domains.%2520By%250Acombining%2520verifiable%2520rewards%2520with%2520a%2520generative%2520reward%2520model%252C%2520we%2520conduct%250Amulti-task%2520zero-RL%2520training%2520across%2520both%2520domains%252C%2520facilitating%2520the%2520transfer%2520of%250Areasoning%2520capabilities%2520between%2520them.%2520Furthermore%252C%2520to%2520mitigate%2520reward%2520hacking%2520in%250Athe%2520generative%2520reward%2520model%252C%2520we%2520design%2520a%2520smooth%2520length%2520penalty%2520that%2520encourages%250Athe%2520generation%2520of%2520more%2520comprehensive%2520thinking%2520tokens%2520in%2520general%2520domains.%250AExperimental%2520results%2520on%2520Qwen3-8B-Base%2520and%2520Qwen3-14B-Base%2520demonstrate%2520that%2520our%250Aapproach%2520achieves%2520superior%2520reasoning%2520performance%252C%2520not%2520only%2520on%2520tasks%2520requiring%250Aextensive%2520reasoning%2520but%2520also%2520on%2520more%2520general%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero%20Reinforcement%20Learning%20Towards%20General%20Domains&entry.906535625=Yuyuan%20Zeng%20and%20Yufei%20Huang%20and%20Can%20Xu%20and%20Qingfeng%20Sun%20and%20Jianfeng%20Yan%20and%20Guanghui%20Xu%20and%20Tao%20Yang%20and%20Fengzong%20Lian&entry.1292438233=%20%20Zero%20Reinforcement%20Learning%20%28Zero-RL%29%20has%20proven%20to%20be%20an%20effective%20approach%0Afor%20enhancing%20the%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20by%0Adirectly%20applying%20reinforcement%20learning%20with%20verifiable%20rewards%20on%20pretrained%0Amodels%2C%20without%20the%20need%20for%20a%20supervised%20fine-tuning%20phase.%20However%2C%20current%0Aresearch%20on%20zero-RL%20primarily%20focuses%20on%20domains%20with%20easily%20verifiable%20reward%0Asignals%2C%20such%20as%20mathematics%2C%20programming%2C%20and%20other%20reasoning%20tasks.%20The%0Achallenge%20of%20eliciting%20reasoning%20abilities%20in%20more%20diverse%20scenarios%2C%20where%0Averification%20is%20not%20straightforward%2C%20remains%20underexplored.%20To%20address%20this%0Agap%2C%20we%20propose%20a%20novel%20zero-RL%20paradigm%20designed%20to%20improve%20a%20model%27s%0Areasoning%20ability%20across%20both%20verifiable%20and%20non-verifiable%20domains.%20By%0Acombining%20verifiable%20rewards%20with%20a%20generative%20reward%20model%2C%20we%20conduct%0Amulti-task%20zero-RL%20training%20across%20both%20domains%2C%20facilitating%20the%20transfer%20of%0Areasoning%20capabilities%20between%20them.%20Furthermore%2C%20to%20mitigate%20reward%20hacking%20in%0Athe%20generative%20reward%20model%2C%20we%20design%20a%20smooth%20length%20penalty%20that%20encourages%0Athe%20generation%20of%20more%20comprehensive%20thinking%20tokens%20in%20general%20domains.%0AExperimental%20results%20on%20Qwen3-8B-Base%20and%20Qwen3-14B-Base%20demonstrate%20that%20our%0Aapproach%20achieves%20superior%20reasoning%20performance%2C%20not%20only%20on%20tasks%20requiring%0Aextensive%20reasoning%20but%20also%20on%20more%20general%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25528v1&entry.124074799=Read"},
{"title": "Lost in Phonation: Voice Quality Variation as an Evaluation Dimension\n  for Speech Foundation Models", "author": "Harm Lameris and Shree Harsha Bokkahalli Satish and Joakim Gustafson and \u00c9va Sz\u00e9kely", "abstract": "  Recent advances in speech foundation models (SFMs) have enabled the direct\nprocessing of spoken language from raw audio, bypassing intermediate textual\nrepresentations. This capability allows SFMs to be exposed to, and potentially\nrespond to, rich paralinguistic variations embedded in the input speech signal.\nOne under-explored dimension of paralinguistic variation is voice quality,\nencompassing phonation types such as creaky and breathy voice. These phonation\ntypes are known to influence how listeners infer affective state, stance and\nsocial meaning in speech. Existing benchmarks for speech understanding largely\nrely on multiple-choice question answering (MCQA) formats, which are prone to\nfailure and therefore unreliable in capturing the nuanced ways paralinguistic\nfeatures influence model behaviour. In this paper, we probe SFMs through\nopen-ended generation tasks and speech emotion recognition, evaluating whether\nmodel behaviours are consistent across different phonation inputs. We introduce\na new parallel dataset featuring synthesized modifications to voice quality,\ndesigned to evaluate SFM responses to creaky and breathy voice. Our work\nprovides the first examination of SFM sensitivity to these particular\nnon-lexical aspects of speech perception.\n", "link": "http://arxiv.org/abs/2510.25577v1", "date": "2025-10-29", "relevancy": 2.5255, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Phonation%3A%20Voice%20Quality%20Variation%20as%20an%20Evaluation%20Dimension%0A%20%20for%20Speech%20Foundation%20Models&body=Title%3A%20Lost%20in%20Phonation%3A%20Voice%20Quality%20Variation%20as%20an%20Evaluation%20Dimension%0A%20%20for%20Speech%20Foundation%20Models%0AAuthor%3A%20Harm%20Lameris%20and%20Shree%20Harsha%20Bokkahalli%20Satish%20and%20Joakim%20Gustafson%20and%20%C3%89va%20Sz%C3%A9kely%0AAbstract%3A%20%20%20Recent%20advances%20in%20speech%20foundation%20models%20%28SFMs%29%20have%20enabled%20the%20direct%0Aprocessing%20of%20spoken%20language%20from%20raw%20audio%2C%20bypassing%20intermediate%20textual%0Arepresentations.%20This%20capability%20allows%20SFMs%20to%20be%20exposed%20to%2C%20and%20potentially%0Arespond%20to%2C%20rich%20paralinguistic%20variations%20embedded%20in%20the%20input%20speech%20signal.%0AOne%20under-explored%20dimension%20of%20paralinguistic%20variation%20is%20voice%20quality%2C%0Aencompassing%20phonation%20types%20such%20as%20creaky%20and%20breathy%20voice.%20These%20phonation%0Atypes%20are%20known%20to%20influence%20how%20listeners%20infer%20affective%20state%2C%20stance%20and%0Asocial%20meaning%20in%20speech.%20Existing%20benchmarks%20for%20speech%20understanding%20largely%0Arely%20on%20multiple-choice%20question%20answering%20%28MCQA%29%20formats%2C%20which%20are%20prone%20to%0Afailure%20and%20therefore%20unreliable%20in%20capturing%20the%20nuanced%20ways%20paralinguistic%0Afeatures%20influence%20model%20behaviour.%20In%20this%20paper%2C%20we%20probe%20SFMs%20through%0Aopen-ended%20generation%20tasks%20and%20speech%20emotion%20recognition%2C%20evaluating%20whether%0Amodel%20behaviours%20are%20consistent%20across%20different%20phonation%20inputs.%20We%20introduce%0Aa%20new%20parallel%20dataset%20featuring%20synthesized%20modifications%20to%20voice%20quality%2C%0Adesigned%20to%20evaluate%20SFM%20responses%20to%20creaky%20and%20breathy%20voice.%20Our%20work%0Aprovides%20the%20first%20examination%20of%20SFM%20sensitivity%20to%20these%20particular%0Anon-lexical%20aspects%20of%20speech%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Phonation%253A%2520Voice%2520Quality%2520Variation%2520as%2520an%2520Evaluation%2520Dimension%250A%2520%2520for%2520Speech%2520Foundation%2520Models%26entry.906535625%3DHarm%2520Lameris%2520and%2520Shree%2520Harsha%2520Bokkahalli%2520Satish%2520and%2520Joakim%2520Gustafson%2520and%2520%25C3%2589va%2520Sz%25C3%25A9kely%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520speech%2520foundation%2520models%2520%2528SFMs%2529%2520have%2520enabled%2520the%2520direct%250Aprocessing%2520of%2520spoken%2520language%2520from%2520raw%2520audio%252C%2520bypassing%2520intermediate%2520textual%250Arepresentations.%2520This%2520capability%2520allows%2520SFMs%2520to%2520be%2520exposed%2520to%252C%2520and%2520potentially%250Arespond%2520to%252C%2520rich%2520paralinguistic%2520variations%2520embedded%2520in%2520the%2520input%2520speech%2520signal.%250AOne%2520under-explored%2520dimension%2520of%2520paralinguistic%2520variation%2520is%2520voice%2520quality%252C%250Aencompassing%2520phonation%2520types%2520such%2520as%2520creaky%2520and%2520breathy%2520voice.%2520These%2520phonation%250Atypes%2520are%2520known%2520to%2520influence%2520how%2520listeners%2520infer%2520affective%2520state%252C%2520stance%2520and%250Asocial%2520meaning%2520in%2520speech.%2520Existing%2520benchmarks%2520for%2520speech%2520understanding%2520largely%250Arely%2520on%2520multiple-choice%2520question%2520answering%2520%2528MCQA%2529%2520formats%252C%2520which%2520are%2520prone%2520to%250Afailure%2520and%2520therefore%2520unreliable%2520in%2520capturing%2520the%2520nuanced%2520ways%2520paralinguistic%250Afeatures%2520influence%2520model%2520behaviour.%2520In%2520this%2520paper%252C%2520we%2520probe%2520SFMs%2520through%250Aopen-ended%2520generation%2520tasks%2520and%2520speech%2520emotion%2520recognition%252C%2520evaluating%2520whether%250Amodel%2520behaviours%2520are%2520consistent%2520across%2520different%2520phonation%2520inputs.%2520We%2520introduce%250Aa%2520new%2520parallel%2520dataset%2520featuring%2520synthesized%2520modifications%2520to%2520voice%2520quality%252C%250Adesigned%2520to%2520evaluate%2520SFM%2520responses%2520to%2520creaky%2520and%2520breathy%2520voice.%2520Our%2520work%250Aprovides%2520the%2520first%2520examination%2520of%2520SFM%2520sensitivity%2520to%2520these%2520particular%250Anon-lexical%2520aspects%2520of%2520speech%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Phonation%3A%20Voice%20Quality%20Variation%20as%20an%20Evaluation%20Dimension%0A%20%20for%20Speech%20Foundation%20Models&entry.906535625=Harm%20Lameris%20and%20Shree%20Harsha%20Bokkahalli%20Satish%20and%20Joakim%20Gustafson%20and%20%C3%89va%20Sz%C3%A9kely&entry.1292438233=%20%20Recent%20advances%20in%20speech%20foundation%20models%20%28SFMs%29%20have%20enabled%20the%20direct%0Aprocessing%20of%20spoken%20language%20from%20raw%20audio%2C%20bypassing%20intermediate%20textual%0Arepresentations.%20This%20capability%20allows%20SFMs%20to%20be%20exposed%20to%2C%20and%20potentially%0Arespond%20to%2C%20rich%20paralinguistic%20variations%20embedded%20in%20the%20input%20speech%20signal.%0AOne%20under-explored%20dimension%20of%20paralinguistic%20variation%20is%20voice%20quality%2C%0Aencompassing%20phonation%20types%20such%20as%20creaky%20and%20breathy%20voice.%20These%20phonation%0Atypes%20are%20known%20to%20influence%20how%20listeners%20infer%20affective%20state%2C%20stance%20and%0Asocial%20meaning%20in%20speech.%20Existing%20benchmarks%20for%20speech%20understanding%20largely%0Arely%20on%20multiple-choice%20question%20answering%20%28MCQA%29%20formats%2C%20which%20are%20prone%20to%0Afailure%20and%20therefore%20unreliable%20in%20capturing%20the%20nuanced%20ways%20paralinguistic%0Afeatures%20influence%20model%20behaviour.%20In%20this%20paper%2C%20we%20probe%20SFMs%20through%0Aopen-ended%20generation%20tasks%20and%20speech%20emotion%20recognition%2C%20evaluating%20whether%0Amodel%20behaviours%20are%20consistent%20across%20different%20phonation%20inputs.%20We%20introduce%0Aa%20new%20parallel%20dataset%20featuring%20synthesized%20modifications%20to%20voice%20quality%2C%0Adesigned%20to%20evaluate%20SFM%20responses%20to%20creaky%20and%20breathy%20voice.%20Our%20work%0Aprovides%20the%20first%20examination%20of%20SFM%20sensitivity%20to%20these%20particular%0Anon-lexical%20aspects%20of%20speech%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25577v1&entry.124074799=Read"},
{"title": "Meshless solutions of PDE inverse problems on irregular geometries", "author": "James V. Roggeveen and Michael P. Brenner", "abstract": "  Solving inverse and optimization problems over solutions of nonlinear partial\ndifferential equations (PDEs) on complex spatial domains is a long-standing\nchallenge. Here we introduce a method that parameterizes the solution using\nspectral bases on arbitrary spatiotemporal domains, whereby the basis is\ndefined on a hyperrectangle containing the true domain. We find the\ncoefficients of the basis expansion by solving an optimization problem whereby\nboth the equations, the boundary conditions and any optimization targets are\nenforced by a loss function, building on a key idea from Physics-Informed\nNeural Networks (PINNs). Since the representation of the function natively has\nexponential convergence, so does the solution of the optimization problem, as\nlong as it can be solved efficiently. We find empirically that the optimization\nprotocols developed for machine learning find solutions with exponential\nconvergence on a wide range of equations. The method naturally allows for the\nincorporation of data assimilation by including additional terms in the loss\nfunction, and for the efficient solution of optimization problems over the PDE\nsolutions.\n", "link": "http://arxiv.org/abs/2510.25752v1", "date": "2025-10-29", "relevancy": 2.5215, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5467}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4903}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meshless%20solutions%20of%20PDE%20inverse%20problems%20on%20irregular%20geometries&body=Title%3A%20Meshless%20solutions%20of%20PDE%20inverse%20problems%20on%20irregular%20geometries%0AAuthor%3A%20James%20V.%20Roggeveen%20and%20Michael%20P.%20Brenner%0AAbstract%3A%20%20%20Solving%20inverse%20and%20optimization%20problems%20over%20solutions%20of%20nonlinear%20partial%0Adifferential%20equations%20%28PDEs%29%20on%20complex%20spatial%20domains%20is%20a%20long-standing%0Achallenge.%20Here%20we%20introduce%20a%20method%20that%20parameterizes%20the%20solution%20using%0Aspectral%20bases%20on%20arbitrary%20spatiotemporal%20domains%2C%20whereby%20the%20basis%20is%0Adefined%20on%20a%20hyperrectangle%20containing%20the%20true%20domain.%20We%20find%20the%0Acoefficients%20of%20the%20basis%20expansion%20by%20solving%20an%20optimization%20problem%20whereby%0Aboth%20the%20equations%2C%20the%20boundary%20conditions%20and%20any%20optimization%20targets%20are%0Aenforced%20by%20a%20loss%20function%2C%20building%20on%20a%20key%20idea%20from%20Physics-Informed%0ANeural%20Networks%20%28PINNs%29.%20Since%20the%20representation%20of%20the%20function%20natively%20has%0Aexponential%20convergence%2C%20so%20does%20the%20solution%20of%20the%20optimization%20problem%2C%20as%0Along%20as%20it%20can%20be%20solved%20efficiently.%20We%20find%20empirically%20that%20the%20optimization%0Aprotocols%20developed%20for%20machine%20learning%20find%20solutions%20with%20exponential%0Aconvergence%20on%20a%20wide%20range%20of%20equations.%20The%20method%20naturally%20allows%20for%20the%0Aincorporation%20of%20data%20assimilation%20by%20including%20additional%20terms%20in%20the%20loss%0Afunction%2C%20and%20for%20the%20efficient%20solution%20of%20optimization%20problems%20over%20the%20PDE%0Asolutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshless%2520solutions%2520of%2520PDE%2520inverse%2520problems%2520on%2520irregular%2520geometries%26entry.906535625%3DJames%2520V.%2520Roggeveen%2520and%2520Michael%2520P.%2520Brenner%26entry.1292438233%3D%2520%2520Solving%2520inverse%2520and%2520optimization%2520problems%2520over%2520solutions%2520of%2520nonlinear%2520partial%250Adifferential%2520equations%2520%2528PDEs%2529%2520on%2520complex%2520spatial%2520domains%2520is%2520a%2520long-standing%250Achallenge.%2520Here%2520we%2520introduce%2520a%2520method%2520that%2520parameterizes%2520the%2520solution%2520using%250Aspectral%2520bases%2520on%2520arbitrary%2520spatiotemporal%2520domains%252C%2520whereby%2520the%2520basis%2520is%250Adefined%2520on%2520a%2520hyperrectangle%2520containing%2520the%2520true%2520domain.%2520We%2520find%2520the%250Acoefficients%2520of%2520the%2520basis%2520expansion%2520by%2520solving%2520an%2520optimization%2520problem%2520whereby%250Aboth%2520the%2520equations%252C%2520the%2520boundary%2520conditions%2520and%2520any%2520optimization%2520targets%2520are%250Aenforced%2520by%2520a%2520loss%2520function%252C%2520building%2520on%2520a%2520key%2520idea%2520from%2520Physics-Informed%250ANeural%2520Networks%2520%2528PINNs%2529.%2520Since%2520the%2520representation%2520of%2520the%2520function%2520natively%2520has%250Aexponential%2520convergence%252C%2520so%2520does%2520the%2520solution%2520of%2520the%2520optimization%2520problem%252C%2520as%250Along%2520as%2520it%2520can%2520be%2520solved%2520efficiently.%2520We%2520find%2520empirically%2520that%2520the%2520optimization%250Aprotocols%2520developed%2520for%2520machine%2520learning%2520find%2520solutions%2520with%2520exponential%250Aconvergence%2520on%2520a%2520wide%2520range%2520of%2520equations.%2520The%2520method%2520naturally%2520allows%2520for%2520the%250Aincorporation%2520of%2520data%2520assimilation%2520by%2520including%2520additional%2520terms%2520in%2520the%2520loss%250Afunction%252C%2520and%2520for%2520the%2520efficient%2520solution%2520of%2520optimization%2520problems%2520over%2520the%2520PDE%250Asolutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meshless%20solutions%20of%20PDE%20inverse%20problems%20on%20irregular%20geometries&entry.906535625=James%20V.%20Roggeveen%20and%20Michael%20P.%20Brenner&entry.1292438233=%20%20Solving%20inverse%20and%20optimization%20problems%20over%20solutions%20of%20nonlinear%20partial%0Adifferential%20equations%20%28PDEs%29%20on%20complex%20spatial%20domains%20is%20a%20long-standing%0Achallenge.%20Here%20we%20introduce%20a%20method%20that%20parameterizes%20the%20solution%20using%0Aspectral%20bases%20on%20arbitrary%20spatiotemporal%20domains%2C%20whereby%20the%20basis%20is%0Adefined%20on%20a%20hyperrectangle%20containing%20the%20true%20domain.%20We%20find%20the%0Acoefficients%20of%20the%20basis%20expansion%20by%20solving%20an%20optimization%20problem%20whereby%0Aboth%20the%20equations%2C%20the%20boundary%20conditions%20and%20any%20optimization%20targets%20are%0Aenforced%20by%20a%20loss%20function%2C%20building%20on%20a%20key%20idea%20from%20Physics-Informed%0ANeural%20Networks%20%28PINNs%29.%20Since%20the%20representation%20of%20the%20function%20natively%20has%0Aexponential%20convergence%2C%20so%20does%20the%20solution%20of%20the%20optimization%20problem%2C%20as%0Along%20as%20it%20can%20be%20solved%20efficiently.%20We%20find%20empirically%20that%20the%20optimization%0Aprotocols%20developed%20for%20machine%20learning%20find%20solutions%20with%20exponential%0Aconvergence%20on%20a%20wide%20range%20of%20equations.%20The%20method%20naturally%20allows%20for%20the%0Aincorporation%20of%20data%20assimilation%20by%20including%20additional%20terms%20in%20the%20loss%0Afunction%2C%20and%20for%20the%20efficient%20solution%20of%20optimization%20problems%20over%20the%20PDE%0Asolutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25752v1&entry.124074799=Read"},
{"title": "Huxley-G\u00f6del Machine: Human-Level Coding Agent Development by an\n  Approximation of the Optimal Self-Improving Machine", "author": "Wenyi Wang and Piotr Pi\u0119kos and Li Nanbo and Firas Laakom and Yimeng Chen and Mateusz Ostaszewski and Mingchen Zhuge and J\u00fcrgen Schmidhuber", "abstract": "  Recent studies operationalize self-improvement through coding agents that\nedit their own codebases. They grow a tree of self-modifications through\nexpansion strategies that favor higher software engineering benchmark\nperformance, assuming that this implies more promising subsequent\nself-modifications. However, we identify a mismatch between the agent's\nself-improvement potential (metaproductivity) and its coding benchmark\nperformance, namely the Metaproductivity-Performance Mismatch. Inspired by\nHuxley's concept of clade, we propose a metric ($\\mathrm{CMP}$) that aggregates\nthe benchmark performances of the descendants of an agent as an indicator of\nits potential for self-improvement. We show that, in our self-improving coding\nagent development setting, access to the true $\\mathrm{CMP}$ is sufficient to\nsimulate how the G\\\"odel Machine would behave under certain assumptions. We\nintroduce the Huxley-G\\\"odel Machine (HGM), which, by estimating $\\mathrm{CMP}$\nand using it as guidance, searches the tree of self-modifications. On SWE-bench\nVerified and Polyglot, HGM outperforms prior self-improving coding agent\ndevelopment methods while using fewer allocated CPU hours. Last but not least,\nHGM demonstrates strong transfer to other coding datasets and large language\nmodels. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and\nevaluated on SWE-bench Lite with GPT-5 achieves human-level performance,\nmatching the best officially checked results of human-engineered coding agents.\nOur code is publicly available at https://github.com/metauto-ai/HGM.\n", "link": "http://arxiv.org/abs/2510.21614v3", "date": "2025-10-29", "relevancy": 2.4632, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5144}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4906}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Huxley-G%C3%B6del%20Machine%3A%20Human-Level%20Coding%20Agent%20Development%20by%20an%0A%20%20Approximation%20of%20the%20Optimal%20Self-Improving%20Machine&body=Title%3A%20Huxley-G%C3%B6del%20Machine%3A%20Human-Level%20Coding%20Agent%20Development%20by%20an%0A%20%20Approximation%20of%20the%20Optimal%20Self-Improving%20Machine%0AAuthor%3A%20Wenyi%20Wang%20and%20Piotr%20Pi%C4%99kos%20and%20Li%20Nanbo%20and%20Firas%20Laakom%20and%20Yimeng%20Chen%20and%20Mateusz%20Ostaszewski%20and%20Mingchen%20Zhuge%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20Recent%20studies%20operationalize%20self-improvement%20through%20coding%20agents%20that%0Aedit%20their%20own%20codebases.%20They%20grow%20a%20tree%20of%20self-modifications%20through%0Aexpansion%20strategies%20that%20favor%20higher%20software%20engineering%20benchmark%0Aperformance%2C%20assuming%20that%20this%20implies%20more%20promising%20subsequent%0Aself-modifications.%20However%2C%20we%20identify%20a%20mismatch%20between%20the%20agent%27s%0Aself-improvement%20potential%20%28metaproductivity%29%20and%20its%20coding%20benchmark%0Aperformance%2C%20namely%20the%20Metaproductivity-Performance%20Mismatch.%20Inspired%20by%0AHuxley%27s%20concept%20of%20clade%2C%20we%20propose%20a%20metric%20%28%24%5Cmathrm%7BCMP%7D%24%29%20that%20aggregates%0Athe%20benchmark%20performances%20of%20the%20descendants%20of%20an%20agent%20as%20an%20indicator%20of%0Aits%20potential%20for%20self-improvement.%20We%20show%20that%2C%20in%20our%20self-improving%20coding%0Aagent%20development%20setting%2C%20access%20to%20the%20true%20%24%5Cmathrm%7BCMP%7D%24%20is%20sufficient%20to%0Asimulate%20how%20the%20G%5C%22odel%20Machine%20would%20behave%20under%20certain%20assumptions.%20We%0Aintroduce%20the%20Huxley-G%5C%22odel%20Machine%20%28HGM%29%2C%20which%2C%20by%20estimating%20%24%5Cmathrm%7BCMP%7D%24%0Aand%20using%20it%20as%20guidance%2C%20searches%20the%20tree%20of%20self-modifications.%20On%20SWE-bench%0AVerified%20and%20Polyglot%2C%20HGM%20outperforms%20prior%20self-improving%20coding%20agent%0Adevelopment%20methods%20while%20using%20fewer%20allocated%20CPU%20hours.%20Last%20but%20not%20least%2C%0AHGM%20demonstrates%20strong%20transfer%20to%20other%20coding%20datasets%20and%20large%20language%0Amodels.%20The%20agent%20optimized%20by%20HGM%20on%20SWE-bench%20Verified%20with%20GPT-5-mini%20and%0Aevaluated%20on%20SWE-bench%20Lite%20with%20GPT-5%20achieves%20human-level%20performance%2C%0Amatching%20the%20best%20officially%20checked%20results%20of%20human-engineered%20coding%20agents.%0AOur%20code%20is%20publicly%20available%20at%20https%3A//github.com/metauto-ai/HGM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.21614v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuxley-G%25C3%25B6del%2520Machine%253A%2520Human-Level%2520Coding%2520Agent%2520Development%2520by%2520an%250A%2520%2520Approximation%2520of%2520the%2520Optimal%2520Self-Improving%2520Machine%26entry.906535625%3DWenyi%2520Wang%2520and%2520Piotr%2520Pi%25C4%2599kos%2520and%2520Li%2520Nanbo%2520and%2520Firas%2520Laakom%2520and%2520Yimeng%2520Chen%2520and%2520Mateusz%2520Ostaszewski%2520and%2520Mingchen%2520Zhuge%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%26entry.1292438233%3D%2520%2520Recent%2520studies%2520operationalize%2520self-improvement%2520through%2520coding%2520agents%2520that%250Aedit%2520their%2520own%2520codebases.%2520They%2520grow%2520a%2520tree%2520of%2520self-modifications%2520through%250Aexpansion%2520strategies%2520that%2520favor%2520higher%2520software%2520engineering%2520benchmark%250Aperformance%252C%2520assuming%2520that%2520this%2520implies%2520more%2520promising%2520subsequent%250Aself-modifications.%2520However%252C%2520we%2520identify%2520a%2520mismatch%2520between%2520the%2520agent%2527s%250Aself-improvement%2520potential%2520%2528metaproductivity%2529%2520and%2520its%2520coding%2520benchmark%250Aperformance%252C%2520namely%2520the%2520Metaproductivity-Performance%2520Mismatch.%2520Inspired%2520by%250AHuxley%2527s%2520concept%2520of%2520clade%252C%2520we%2520propose%2520a%2520metric%2520%2528%2524%255Cmathrm%257BCMP%257D%2524%2529%2520that%2520aggregates%250Athe%2520benchmark%2520performances%2520of%2520the%2520descendants%2520of%2520an%2520agent%2520as%2520an%2520indicator%2520of%250Aits%2520potential%2520for%2520self-improvement.%2520We%2520show%2520that%252C%2520in%2520our%2520self-improving%2520coding%250Aagent%2520development%2520setting%252C%2520access%2520to%2520the%2520true%2520%2524%255Cmathrm%257BCMP%257D%2524%2520is%2520sufficient%2520to%250Asimulate%2520how%2520the%2520G%255C%2522odel%2520Machine%2520would%2520behave%2520under%2520certain%2520assumptions.%2520We%250Aintroduce%2520the%2520Huxley-G%255C%2522odel%2520Machine%2520%2528HGM%2529%252C%2520which%252C%2520by%2520estimating%2520%2524%255Cmathrm%257BCMP%257D%2524%250Aand%2520using%2520it%2520as%2520guidance%252C%2520searches%2520the%2520tree%2520of%2520self-modifications.%2520On%2520SWE-bench%250AVerified%2520and%2520Polyglot%252C%2520HGM%2520outperforms%2520prior%2520self-improving%2520coding%2520agent%250Adevelopment%2520methods%2520while%2520using%2520fewer%2520allocated%2520CPU%2520hours.%2520Last%2520but%2520not%2520least%252C%250AHGM%2520demonstrates%2520strong%2520transfer%2520to%2520other%2520coding%2520datasets%2520and%2520large%2520language%250Amodels.%2520The%2520agent%2520optimized%2520by%2520HGM%2520on%2520SWE-bench%2520Verified%2520with%2520GPT-5-mini%2520and%250Aevaluated%2520on%2520SWE-bench%2520Lite%2520with%2520GPT-5%2520achieves%2520human-level%2520performance%252C%250Amatching%2520the%2520best%2520officially%2520checked%2520results%2520of%2520human-engineered%2520coding%2520agents.%250AOur%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/metauto-ai/HGM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.21614v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Huxley-G%C3%B6del%20Machine%3A%20Human-Level%20Coding%20Agent%20Development%20by%20an%0A%20%20Approximation%20of%20the%20Optimal%20Self-Improving%20Machine&entry.906535625=Wenyi%20Wang%20and%20Piotr%20Pi%C4%99kos%20and%20Li%20Nanbo%20and%20Firas%20Laakom%20and%20Yimeng%20Chen%20and%20Mateusz%20Ostaszewski%20and%20Mingchen%20Zhuge%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Recent%20studies%20operationalize%20self-improvement%20through%20coding%20agents%20that%0Aedit%20their%20own%20codebases.%20They%20grow%20a%20tree%20of%20self-modifications%20through%0Aexpansion%20strategies%20that%20favor%20higher%20software%20engineering%20benchmark%0Aperformance%2C%20assuming%20that%20this%20implies%20more%20promising%20subsequent%0Aself-modifications.%20However%2C%20we%20identify%20a%20mismatch%20between%20the%20agent%27s%0Aself-improvement%20potential%20%28metaproductivity%29%20and%20its%20coding%20benchmark%0Aperformance%2C%20namely%20the%20Metaproductivity-Performance%20Mismatch.%20Inspired%20by%0AHuxley%27s%20concept%20of%20clade%2C%20we%20propose%20a%20metric%20%28%24%5Cmathrm%7BCMP%7D%24%29%20that%20aggregates%0Athe%20benchmark%20performances%20of%20the%20descendants%20of%20an%20agent%20as%20an%20indicator%20of%0Aits%20potential%20for%20self-improvement.%20We%20show%20that%2C%20in%20our%20self-improving%20coding%0Aagent%20development%20setting%2C%20access%20to%20the%20true%20%24%5Cmathrm%7BCMP%7D%24%20is%20sufficient%20to%0Asimulate%20how%20the%20G%5C%22odel%20Machine%20would%20behave%20under%20certain%20assumptions.%20We%0Aintroduce%20the%20Huxley-G%5C%22odel%20Machine%20%28HGM%29%2C%20which%2C%20by%20estimating%20%24%5Cmathrm%7BCMP%7D%24%0Aand%20using%20it%20as%20guidance%2C%20searches%20the%20tree%20of%20self-modifications.%20On%20SWE-bench%0AVerified%20and%20Polyglot%2C%20HGM%20outperforms%20prior%20self-improving%20coding%20agent%0Adevelopment%20methods%20while%20using%20fewer%20allocated%20CPU%20hours.%20Last%20but%20not%20least%2C%0AHGM%20demonstrates%20strong%20transfer%20to%20other%20coding%20datasets%20and%20large%20language%0Amodels.%20The%20agent%20optimized%20by%20HGM%20on%20SWE-bench%20Verified%20with%20GPT-5-mini%20and%0Aevaluated%20on%20SWE-bench%20Lite%20with%20GPT-5%20achieves%20human-level%20performance%2C%0Amatching%20the%20best%20officially%20checked%20results%20of%20human-engineered%20coding%20agents.%0AOur%20code%20is%20publicly%20available%20at%20https%3A//github.com/metauto-ai/HGM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.21614v3&entry.124074799=Read"},
{"title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and\n  Benchmarks", "author": "Xu Zheng and Zihao Dongfang and Lutao Jiang and Boyuan Zheng and Yulong Guo and Zhenquan Zhang and Giuliano Albanese and Runyi Yang and Mengjiao Ma and Zixin Zhang and Chenfei Liao and Dingcheng Zhen and Yuanhuiyi Lyu and Yuqian Fu and Bin Ren and Linfeng Zhang and Danda Pani Paudel and Nicu Sebe and Luc Van Gool and Xuming Hu", "abstract": "  Humans possess spatial reasoning abilities that enable them to understand\nspaces through multimodal observations, such as vision and sound. Large\nmultimodal reasoning models extend these abilities by learning to perceive and\nreason, showing promising performance across diverse spatial tasks. However,\nsystematic reviews and publicly available benchmarks for these models remain\nlimited. In this survey, we provide a comprehensive review of multimodal\nspatial reasoning tasks with large models, categorizing recent progress in\nmultimodal large language models (MLLMs) and introducing open benchmarks for\nevaluation. We begin by outlining general spatial reasoning, focusing on\npost-training techniques, explainability, and architecture. Beyond classical 2D\ntasks, we examine spatial relationship reasoning, scene and layout\nunderstanding, as well as visual question answering and grounding in 3D space.\nWe also review advances in embodied AI, including vision-language navigation\nand action models. Additionally, we consider emerging modalities such as audio\nand egocentric video, which contribute to novel spatial understanding through\nnew sensors. We believe this survey establishes a solid foundation and offers\ninsights into the growing field of multimodal spatial reasoning. Updated\ninformation about this survey, codes and implementation of the open benchmarks\ncan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.\n", "link": "http://arxiv.org/abs/2510.25760v1", "date": "2025-10-29", "relevancy": 2.4209, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6054}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Spatial%20Reasoning%20in%20the%20Large%20Model%20Era%3A%20A%20Survey%20and%0A%20%20Benchmarks&body=Title%3A%20Multimodal%20Spatial%20Reasoning%20in%20the%20Large%20Model%20Era%3A%20A%20Survey%20and%0A%20%20Benchmarks%0AAuthor%3A%20Xu%20Zheng%20and%20Zihao%20Dongfang%20and%20Lutao%20Jiang%20and%20Boyuan%20Zheng%20and%20Yulong%20Guo%20and%20Zhenquan%20Zhang%20and%20Giuliano%20Albanese%20and%20Runyi%20Yang%20and%20Mengjiao%20Ma%20and%20Zixin%20Zhang%20and%20Chenfei%20Liao%20and%20Dingcheng%20Zhen%20and%20Yuanhuiyi%20Lyu%20and%20Yuqian%20Fu%20and%20Bin%20Ren%20and%20Linfeng%20Zhang%20and%20Danda%20Pani%20Paudel%20and%20Nicu%20Sebe%20and%20Luc%20Van%20Gool%20and%20Xuming%20Hu%0AAbstract%3A%20%20%20Humans%20possess%20spatial%20reasoning%20abilities%20that%20enable%20them%20to%20understand%0Aspaces%20through%20multimodal%20observations%2C%20such%20as%20vision%20and%20sound.%20Large%0Amultimodal%20reasoning%20models%20extend%20these%20abilities%20by%20learning%20to%20perceive%20and%0Areason%2C%20showing%20promising%20performance%20across%20diverse%20spatial%20tasks.%20However%2C%0Asystematic%20reviews%20and%20publicly%20available%20benchmarks%20for%20these%20models%20remain%0Alimited.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20review%20of%20multimodal%0Aspatial%20reasoning%20tasks%20with%20large%20models%2C%20categorizing%20recent%20progress%20in%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20and%20introducing%20open%20benchmarks%20for%0Aevaluation.%20We%20begin%20by%20outlining%20general%20spatial%20reasoning%2C%20focusing%20on%0Apost-training%20techniques%2C%20explainability%2C%20and%20architecture.%20Beyond%20classical%202D%0Atasks%2C%20we%20examine%20spatial%20relationship%20reasoning%2C%20scene%20and%20layout%0Aunderstanding%2C%20as%20well%20as%20visual%20question%20answering%20and%20grounding%20in%203D%20space.%0AWe%20also%20review%20advances%20in%20embodied%20AI%2C%20including%20vision-language%20navigation%0Aand%20action%20models.%20Additionally%2C%20we%20consider%20emerging%20modalities%20such%20as%20audio%0Aand%20egocentric%20video%2C%20which%20contribute%20to%20novel%20spatial%20understanding%20through%0Anew%20sensors.%20We%20believe%20this%20survey%20establishes%20a%20solid%20foundation%20and%20offers%0Ainsights%20into%20the%20growing%20field%20of%20multimodal%20spatial%20reasoning.%20Updated%0Ainformation%20about%20this%20survey%2C%20codes%20and%20implementation%20of%20the%20open%20benchmarks%0Acan%20be%20found%20at%20https%3A//github.com/zhengxuJosh/Awesome-Spatial-Reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Spatial%2520Reasoning%2520in%2520the%2520Large%2520Model%2520Era%253A%2520A%2520Survey%2520and%250A%2520%2520Benchmarks%26entry.906535625%3DXu%2520Zheng%2520and%2520Zihao%2520Dongfang%2520and%2520Lutao%2520Jiang%2520and%2520Boyuan%2520Zheng%2520and%2520Yulong%2520Guo%2520and%2520Zhenquan%2520Zhang%2520and%2520Giuliano%2520Albanese%2520and%2520Runyi%2520Yang%2520and%2520Mengjiao%2520Ma%2520and%2520Zixin%2520Zhang%2520and%2520Chenfei%2520Liao%2520and%2520Dingcheng%2520Zhen%2520and%2520Yuanhuiyi%2520Lyu%2520and%2520Yuqian%2520Fu%2520and%2520Bin%2520Ren%2520and%2520Linfeng%2520Zhang%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Nicu%2520Sebe%2520and%2520Luc%2520Van%2520Gool%2520and%2520Xuming%2520Hu%26entry.1292438233%3D%2520%2520Humans%2520possess%2520spatial%2520reasoning%2520abilities%2520that%2520enable%2520them%2520to%2520understand%250Aspaces%2520through%2520multimodal%2520observations%252C%2520such%2520as%2520vision%2520and%2520sound.%2520Large%250Amultimodal%2520reasoning%2520models%2520extend%2520these%2520abilities%2520by%2520learning%2520to%2520perceive%2520and%250Areason%252C%2520showing%2520promising%2520performance%2520across%2520diverse%2520spatial%2520tasks.%2520However%252C%250Asystematic%2520reviews%2520and%2520publicly%2520available%2520benchmarks%2520for%2520these%2520models%2520remain%250Alimited.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%2520comprehensive%2520review%2520of%2520multimodal%250Aspatial%2520reasoning%2520tasks%2520with%2520large%2520models%252C%2520categorizing%2520recent%2520progress%2520in%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520and%2520introducing%2520open%2520benchmarks%2520for%250Aevaluation.%2520We%2520begin%2520by%2520outlining%2520general%2520spatial%2520reasoning%252C%2520focusing%2520on%250Apost-training%2520techniques%252C%2520explainability%252C%2520and%2520architecture.%2520Beyond%2520classical%25202D%250Atasks%252C%2520we%2520examine%2520spatial%2520relationship%2520reasoning%252C%2520scene%2520and%2520layout%250Aunderstanding%252C%2520as%2520well%2520as%2520visual%2520question%2520answering%2520and%2520grounding%2520in%25203D%2520space.%250AWe%2520also%2520review%2520advances%2520in%2520embodied%2520AI%252C%2520including%2520vision-language%2520navigation%250Aand%2520action%2520models.%2520Additionally%252C%2520we%2520consider%2520emerging%2520modalities%2520such%2520as%2520audio%250Aand%2520egocentric%2520video%252C%2520which%2520contribute%2520to%2520novel%2520spatial%2520understanding%2520through%250Anew%2520sensors.%2520We%2520believe%2520this%2520survey%2520establishes%2520a%2520solid%2520foundation%2520and%2520offers%250Ainsights%2520into%2520the%2520growing%2520field%2520of%2520multimodal%2520spatial%2520reasoning.%2520Updated%250Ainformation%2520about%2520this%2520survey%252C%2520codes%2520and%2520implementation%2520of%2520the%2520open%2520benchmarks%250Acan%2520be%2520found%2520at%2520https%253A//github.com/zhengxuJosh/Awesome-Spatial-Reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Spatial%20Reasoning%20in%20the%20Large%20Model%20Era%3A%20A%20Survey%20and%0A%20%20Benchmarks&entry.906535625=Xu%20Zheng%20and%20Zihao%20Dongfang%20and%20Lutao%20Jiang%20and%20Boyuan%20Zheng%20and%20Yulong%20Guo%20and%20Zhenquan%20Zhang%20and%20Giuliano%20Albanese%20and%20Runyi%20Yang%20and%20Mengjiao%20Ma%20and%20Zixin%20Zhang%20and%20Chenfei%20Liao%20and%20Dingcheng%20Zhen%20and%20Yuanhuiyi%20Lyu%20and%20Yuqian%20Fu%20and%20Bin%20Ren%20and%20Linfeng%20Zhang%20and%20Danda%20Pani%20Paudel%20and%20Nicu%20Sebe%20and%20Luc%20Van%20Gool%20and%20Xuming%20Hu&entry.1292438233=%20%20Humans%20possess%20spatial%20reasoning%20abilities%20that%20enable%20them%20to%20understand%0Aspaces%20through%20multimodal%20observations%2C%20such%20as%20vision%20and%20sound.%20Large%0Amultimodal%20reasoning%20models%20extend%20these%20abilities%20by%20learning%20to%20perceive%20and%0Areason%2C%20showing%20promising%20performance%20across%20diverse%20spatial%20tasks.%20However%2C%0Asystematic%20reviews%20and%20publicly%20available%20benchmarks%20for%20these%20models%20remain%0Alimited.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20review%20of%20multimodal%0Aspatial%20reasoning%20tasks%20with%20large%20models%2C%20categorizing%20recent%20progress%20in%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20and%20introducing%20open%20benchmarks%20for%0Aevaluation.%20We%20begin%20by%20outlining%20general%20spatial%20reasoning%2C%20focusing%20on%0Apost-training%20techniques%2C%20explainability%2C%20and%20architecture.%20Beyond%20classical%202D%0Atasks%2C%20we%20examine%20spatial%20relationship%20reasoning%2C%20scene%20and%20layout%0Aunderstanding%2C%20as%20well%20as%20visual%20question%20answering%20and%20grounding%20in%203D%20space.%0AWe%20also%20review%20advances%20in%20embodied%20AI%2C%20including%20vision-language%20navigation%0Aand%20action%20models.%20Additionally%2C%20we%20consider%20emerging%20modalities%20such%20as%20audio%0Aand%20egocentric%20video%2C%20which%20contribute%20to%20novel%20spatial%20understanding%20through%0Anew%20sensors.%20We%20believe%20this%20survey%20establishes%20a%20solid%20foundation%20and%20offers%0Ainsights%20into%20the%20growing%20field%20of%20multimodal%20spatial%20reasoning.%20Updated%0Ainformation%20about%20this%20survey%2C%20codes%20and%20implementation%20of%20the%20open%20benchmarks%0Acan%20be%20found%20at%20https%3A//github.com/zhengxuJosh/Awesome-Spatial-Reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25760v1&entry.124074799=Read"},
{"title": "Exploring the In-Context Learning Capabilities of LLMs for Money\n  Laundering Detection in Financial Graphs", "author": "Erfan Pirmorad", "abstract": "  The complexity and interconnectivity of entities involved in money laundering\ndemand investigative reasoning over graph-structured data. This paper explores\nthe use of large language models (LLMs) as reasoning engines over localized\nsubgraphs extracted from a financial knowledge graph. We propose a lightweight\npipeline that retrieves k-hop neighborhoods around entities of interest,\nserializes them into structured text, and prompts an LLM via few-shot\nin-context learning to assess suspiciousness and generate justifications. Using\nsynthetic anti-money laundering (AML) scenarios that reflect common laundering\nbehaviors, we show that LLMs can emulate analyst-style logic, highlight red\nflags, and provide coherent explanations. While this study is exploratory, it\nillustrates the potential of LLM-based graph reasoning in AML and lays\ngroundwork for explainable, language-driven financial crime analytics.\n", "link": "http://arxiv.org/abs/2507.14785v2", "date": "2025-10-29", "relevancy": 2.3682, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20In-Context%20Learning%20Capabilities%20of%20LLMs%20for%20Money%0A%20%20Laundering%20Detection%20in%20Financial%20Graphs&body=Title%3A%20Exploring%20the%20In-Context%20Learning%20Capabilities%20of%20LLMs%20for%20Money%0A%20%20Laundering%20Detection%20in%20Financial%20Graphs%0AAuthor%3A%20Erfan%20Pirmorad%0AAbstract%3A%20%20%20The%20complexity%20and%20interconnectivity%20of%20entities%20involved%20in%20money%20laundering%0Ademand%20investigative%20reasoning%20over%20graph-structured%20data.%20This%20paper%20explores%0Athe%20use%20of%20large%20language%20models%20%28LLMs%29%20as%20reasoning%20engines%20over%20localized%0Asubgraphs%20extracted%20from%20a%20financial%20knowledge%20graph.%20We%20propose%20a%20lightweight%0Apipeline%20that%20retrieves%20k-hop%20neighborhoods%20around%20entities%20of%20interest%2C%0Aserializes%20them%20into%20structured%20text%2C%20and%20prompts%20an%20LLM%20via%20few-shot%0Ain-context%20learning%20to%20assess%20suspiciousness%20and%20generate%20justifications.%20Using%0Asynthetic%20anti-money%20laundering%20%28AML%29%20scenarios%20that%20reflect%20common%20laundering%0Abehaviors%2C%20we%20show%20that%20LLMs%20can%20emulate%20analyst-style%20logic%2C%20highlight%20red%0Aflags%2C%20and%20provide%20coherent%20explanations.%20While%20this%20study%20is%20exploratory%2C%20it%0Aillustrates%20the%20potential%20of%20LLM-based%20graph%20reasoning%20in%20AML%20and%20lays%0Agroundwork%20for%20explainable%2C%20language-driven%20financial%20crime%20analytics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14785v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520In-Context%2520Learning%2520Capabilities%2520of%2520LLMs%2520for%2520Money%250A%2520%2520Laundering%2520Detection%2520in%2520Financial%2520Graphs%26entry.906535625%3DErfan%2520Pirmorad%26entry.1292438233%3D%2520%2520The%2520complexity%2520and%2520interconnectivity%2520of%2520entities%2520involved%2520in%2520money%2520laundering%250Ademand%2520investigative%2520reasoning%2520over%2520graph-structured%2520data.%2520This%2520paper%2520explores%250Athe%2520use%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520reasoning%2520engines%2520over%2520localized%250Asubgraphs%2520extracted%2520from%2520a%2520financial%2520knowledge%2520graph.%2520We%2520propose%2520a%2520lightweight%250Apipeline%2520that%2520retrieves%2520k-hop%2520neighborhoods%2520around%2520entities%2520of%2520interest%252C%250Aserializes%2520them%2520into%2520structured%2520text%252C%2520and%2520prompts%2520an%2520LLM%2520via%2520few-shot%250Ain-context%2520learning%2520to%2520assess%2520suspiciousness%2520and%2520generate%2520justifications.%2520Using%250Asynthetic%2520anti-money%2520laundering%2520%2528AML%2529%2520scenarios%2520that%2520reflect%2520common%2520laundering%250Abehaviors%252C%2520we%2520show%2520that%2520LLMs%2520can%2520emulate%2520analyst-style%2520logic%252C%2520highlight%2520red%250Aflags%252C%2520and%2520provide%2520coherent%2520explanations.%2520While%2520this%2520study%2520is%2520exploratory%252C%2520it%250Aillustrates%2520the%2520potential%2520of%2520LLM-based%2520graph%2520reasoning%2520in%2520AML%2520and%2520lays%250Agroundwork%2520for%2520explainable%252C%2520language-driven%2520financial%2520crime%2520analytics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14785v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20In-Context%20Learning%20Capabilities%20of%20LLMs%20for%20Money%0A%20%20Laundering%20Detection%20in%20Financial%20Graphs&entry.906535625=Erfan%20Pirmorad&entry.1292438233=%20%20The%20complexity%20and%20interconnectivity%20of%20entities%20involved%20in%20money%20laundering%0Ademand%20investigative%20reasoning%20over%20graph-structured%20data.%20This%20paper%20explores%0Athe%20use%20of%20large%20language%20models%20%28LLMs%29%20as%20reasoning%20engines%20over%20localized%0Asubgraphs%20extracted%20from%20a%20financial%20knowledge%20graph.%20We%20propose%20a%20lightweight%0Apipeline%20that%20retrieves%20k-hop%20neighborhoods%20around%20entities%20of%20interest%2C%0Aserializes%20them%20into%20structured%20text%2C%20and%20prompts%20an%20LLM%20via%20few-shot%0Ain-context%20learning%20to%20assess%20suspiciousness%20and%20generate%20justifications.%20Using%0Asynthetic%20anti-money%20laundering%20%28AML%29%20scenarios%20that%20reflect%20common%20laundering%0Abehaviors%2C%20we%20show%20that%20LLMs%20can%20emulate%20analyst-style%20logic%2C%20highlight%20red%0Aflags%2C%20and%20provide%20coherent%20explanations.%20While%20this%20study%20is%20exploratory%2C%20it%0Aillustrates%20the%20potential%20of%20LLM-based%20graph%20reasoning%20in%20AML%20and%20lays%0Agroundwork%20for%20explainable%2C%20language-driven%20financial%20crime%20analytics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14785v2&entry.124074799=Read"},
{"title": "Subgraph Federated Learning via Spectral Methods", "author": "Javad Aliakbari and Johan \u00d6stman and Ashkan Panahi and Alexandre Graell i Amat", "abstract": "  We consider the problem of federated learning (FL) with graph-structured data\ndistributed across multiple clients. In particular, we address the prevalent\nscenario of interconnected subgraphs, where interconnections between clients\nsignificantly influence the learning process. Existing approaches suffer from\ncritical limitations, either requiring the exchange of sensitive node\nembeddings, thereby posing privacy risks, or relying on\ncomputationally-intensive steps, which hinders scalability. To tackle these\nchallenges, we propose FedLap, a novel framework that leverages global\nstructure information via Laplacian smoothing in the spectral domain to\neffectively capture inter-node dependencies while ensuring privacy and\nscalability. We provide a formal analysis of the privacy of FedLap,\ndemonstrating that it preserves privacy. Notably, FedLap is the first subgraph\nFL scheme with strong privacy guarantees. Extensive experiments on benchmark\ndatasets demonstrate that FedLap achieves competitive or superior utility\ncompared to existing techniques.\n", "link": "http://arxiv.org/abs/2510.25657v1", "date": "2025-10-29", "relevancy": 2.3252, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5008}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4532}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subgraph%20Federated%20Learning%20via%20Spectral%20Methods&body=Title%3A%20Subgraph%20Federated%20Learning%20via%20Spectral%20Methods%0AAuthor%3A%20Javad%20Aliakbari%20and%20Johan%20%C3%96stman%20and%20Ashkan%20Panahi%20and%20Alexandre%20Graell%20i%20Amat%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20federated%20learning%20%28FL%29%20with%20graph-structured%20data%0Adistributed%20across%20multiple%20clients.%20In%20particular%2C%20we%20address%20the%20prevalent%0Ascenario%20of%20interconnected%20subgraphs%2C%20where%20interconnections%20between%20clients%0Asignificantly%20influence%20the%20learning%20process.%20Existing%20approaches%20suffer%20from%0Acritical%20limitations%2C%20either%20requiring%20the%20exchange%20of%20sensitive%20node%0Aembeddings%2C%20thereby%20posing%20privacy%20risks%2C%20or%20relying%20on%0Acomputationally-intensive%20steps%2C%20which%20hinders%20scalability.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20FedLap%2C%20a%20novel%20framework%20that%20leverages%20global%0Astructure%20information%20via%20Laplacian%20smoothing%20in%20the%20spectral%20domain%20to%0Aeffectively%20capture%20inter-node%20dependencies%20while%20ensuring%20privacy%20and%0Ascalability.%20We%20provide%20a%20formal%20analysis%20of%20the%20privacy%20of%20FedLap%2C%0Ademonstrating%20that%20it%20preserves%20privacy.%20Notably%2C%20FedLap%20is%20the%20first%20subgraph%0AFL%20scheme%20with%20strong%20privacy%20guarantees.%20Extensive%20experiments%20on%20benchmark%0Adatasets%20demonstrate%20that%20FedLap%20achieves%20competitive%20or%20superior%20utility%0Acompared%20to%20existing%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubgraph%2520Federated%2520Learning%2520via%2520Spectral%2520Methods%26entry.906535625%3DJavad%2520Aliakbari%2520and%2520Johan%2520%25C3%2596stman%2520and%2520Ashkan%2520Panahi%2520and%2520Alexandre%2520Graell%2520i%2520Amat%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520federated%2520learning%2520%2528FL%2529%2520with%2520graph-structured%2520data%250Adistributed%2520across%2520multiple%2520clients.%2520In%2520particular%252C%2520we%2520address%2520the%2520prevalent%250Ascenario%2520of%2520interconnected%2520subgraphs%252C%2520where%2520interconnections%2520between%2520clients%250Asignificantly%2520influence%2520the%2520learning%2520process.%2520Existing%2520approaches%2520suffer%2520from%250Acritical%2520limitations%252C%2520either%2520requiring%2520the%2520exchange%2520of%2520sensitive%2520node%250Aembeddings%252C%2520thereby%2520posing%2520privacy%2520risks%252C%2520or%2520relying%2520on%250Acomputationally-intensive%2520steps%252C%2520which%2520hinders%2520scalability.%2520To%2520tackle%2520these%250Achallenges%252C%2520we%2520propose%2520FedLap%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520global%250Astructure%2520information%2520via%2520Laplacian%2520smoothing%2520in%2520the%2520spectral%2520domain%2520to%250Aeffectively%2520capture%2520inter-node%2520dependencies%2520while%2520ensuring%2520privacy%2520and%250Ascalability.%2520We%2520provide%2520a%2520formal%2520analysis%2520of%2520the%2520privacy%2520of%2520FedLap%252C%250Ademonstrating%2520that%2520it%2520preserves%2520privacy.%2520Notably%252C%2520FedLap%2520is%2520the%2520first%2520subgraph%250AFL%2520scheme%2520with%2520strong%2520privacy%2520guarantees.%2520Extensive%2520experiments%2520on%2520benchmark%250Adatasets%2520demonstrate%2520that%2520FedLap%2520achieves%2520competitive%2520or%2520superior%2520utility%250Acompared%2520to%2520existing%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subgraph%20Federated%20Learning%20via%20Spectral%20Methods&entry.906535625=Javad%20Aliakbari%20and%20Johan%20%C3%96stman%20and%20Ashkan%20Panahi%20and%20Alexandre%20Graell%20i%20Amat&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20federated%20learning%20%28FL%29%20with%20graph-structured%20data%0Adistributed%20across%20multiple%20clients.%20In%20particular%2C%20we%20address%20the%20prevalent%0Ascenario%20of%20interconnected%20subgraphs%2C%20where%20interconnections%20between%20clients%0Asignificantly%20influence%20the%20learning%20process.%20Existing%20approaches%20suffer%20from%0Acritical%20limitations%2C%20either%20requiring%20the%20exchange%20of%20sensitive%20node%0Aembeddings%2C%20thereby%20posing%20privacy%20risks%2C%20or%20relying%20on%0Acomputationally-intensive%20steps%2C%20which%20hinders%20scalability.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20FedLap%2C%20a%20novel%20framework%20that%20leverages%20global%0Astructure%20information%20via%20Laplacian%20smoothing%20in%20the%20spectral%20domain%20to%0Aeffectively%20capture%20inter-node%20dependencies%20while%20ensuring%20privacy%20and%0Ascalability.%20We%20provide%20a%20formal%20analysis%20of%20the%20privacy%20of%20FedLap%2C%0Ademonstrating%20that%20it%20preserves%20privacy.%20Notably%2C%20FedLap%20is%20the%20first%20subgraph%0AFL%20scheme%20with%20strong%20privacy%20guarantees.%20Extensive%20experiments%20on%20benchmark%0Adatasets%20demonstrate%20that%20FedLap%20achieves%20competitive%20or%20superior%20utility%0Acompared%20to%20existing%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25657v1&entry.124074799=Read"},
{"title": "ALDEN: Reinforcement Learning for Active Navigation and Evidence\n  Gathering in Long Documents", "author": "Tianyu Yang and Terry Ruas and Yijun Tian and Jan Philip Wahle and Daniel Kurzawe and Bela Gipp", "abstract": "  Vision-language models (VLMs) excel at interpreting text-rich images but\nstruggle with long, visually complex documents that demand analysis and\nintegration of information spread across multiple pages. Existing approaches\ntypically rely on fixed reasoning templates or rigid pipelines, which force\nVLMs into a passive role and hinder both efficiency and generalization. We\npresent Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement\nlearning framework that fine-tunes VLMs as interactive agents capable of\nactively navigating long, visually rich documents. ALDEN introduces a novel\nfetch action that directly accesses the page by index, complementing the\nclassic search action and better exploiting document structure. For dense\nprocess supervision and efficient training, we propose a rule-based cross-level\nreward that provides both turn- and token-level signals. To address the\nempirically observed training instability caused by numerous visual tokens from\nlong documents, we further propose a visual-semantic anchoring mechanism that\napplies a dual-path KL-divergence constraint to stabilize visual and textual\nrepresentations separately during training. Trained on a corpus constructed\nfrom three open-source datasets, ALDEN achieves state-of-the-art performance on\nfive long-document benchmarks. Overall, ALDEN marks a step beyond passive\ndocument reading toward agents that autonomously navigate and reason across\nlong, visually rich documents, offering a robust path to more accurate and\nefficient long-document understanding.\n", "link": "http://arxiv.org/abs/2510.25668v1", "date": "2025-10-29", "relevancy": 2.2791, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5723}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALDEN%3A%20Reinforcement%20Learning%20for%20Active%20Navigation%20and%20Evidence%0A%20%20Gathering%20in%20Long%20Documents&body=Title%3A%20ALDEN%3A%20Reinforcement%20Learning%20for%20Active%20Navigation%20and%20Evidence%0A%20%20Gathering%20in%20Long%20Documents%0AAuthor%3A%20Tianyu%20Yang%20and%20Terry%20Ruas%20and%20Yijun%20Tian%20and%20Jan%20Philip%20Wahle%20and%20Daniel%20Kurzawe%20and%20Bela%20Gipp%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20excel%20at%20interpreting%20text-rich%20images%20but%0Astruggle%20with%20long%2C%20visually%20complex%20documents%20that%20demand%20analysis%20and%0Aintegration%20of%20information%20spread%20across%20multiple%20pages.%20Existing%20approaches%0Atypically%20rely%20on%20fixed%20reasoning%20templates%20or%20rigid%20pipelines%2C%20which%20force%0AVLMs%20into%20a%20passive%20role%20and%20hinder%20both%20efficiency%20and%20generalization.%20We%0Apresent%20Active%20Long-DocumEnt%20Navigation%20%28ALDEN%29%2C%20a%20multi-turn%20reinforcement%0Alearning%20framework%20that%20fine-tunes%20VLMs%20as%20interactive%20agents%20capable%20of%0Aactively%20navigating%20long%2C%20visually%20rich%20documents.%20ALDEN%20introduces%20a%20novel%0Afetch%20action%20that%20directly%20accesses%20the%20page%20by%20index%2C%20complementing%20the%0Aclassic%20search%20action%20and%20better%20exploiting%20document%20structure.%20For%20dense%0Aprocess%20supervision%20and%20efficient%20training%2C%20we%20propose%20a%20rule-based%20cross-level%0Areward%20that%20provides%20both%20turn-%20and%20token-level%20signals.%20To%20address%20the%0Aempirically%20observed%20training%20instability%20caused%20by%20numerous%20visual%20tokens%20from%0Along%20documents%2C%20we%20further%20propose%20a%20visual-semantic%20anchoring%20mechanism%20that%0Aapplies%20a%20dual-path%20KL-divergence%20constraint%20to%20stabilize%20visual%20and%20textual%0Arepresentations%20separately%20during%20training.%20Trained%20on%20a%20corpus%20constructed%0Afrom%20three%20open-source%20datasets%2C%20ALDEN%20achieves%20state-of-the-art%20performance%20on%0Afive%20long-document%20benchmarks.%20Overall%2C%20ALDEN%20marks%20a%20step%20beyond%20passive%0Adocument%20reading%20toward%20agents%20that%20autonomously%20navigate%20and%20reason%20across%0Along%2C%20visually%20rich%20documents%2C%20offering%20a%20robust%20path%20to%20more%20accurate%20and%0Aefficient%20long-document%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALDEN%253A%2520Reinforcement%2520Learning%2520for%2520Active%2520Navigation%2520and%2520Evidence%250A%2520%2520Gathering%2520in%2520Long%2520Documents%26entry.906535625%3DTianyu%2520Yang%2520and%2520Terry%2520Ruas%2520and%2520Yijun%2520Tian%2520and%2520Jan%2520Philip%2520Wahle%2520and%2520Daniel%2520Kurzawe%2520and%2520Bela%2520Gipp%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520excel%2520at%2520interpreting%2520text-rich%2520images%2520but%250Astruggle%2520with%2520long%252C%2520visually%2520complex%2520documents%2520that%2520demand%2520analysis%2520and%250Aintegration%2520of%2520information%2520spread%2520across%2520multiple%2520pages.%2520Existing%2520approaches%250Atypically%2520rely%2520on%2520fixed%2520reasoning%2520templates%2520or%2520rigid%2520pipelines%252C%2520which%2520force%250AVLMs%2520into%2520a%2520passive%2520role%2520and%2520hinder%2520both%2520efficiency%2520and%2520generalization.%2520We%250Apresent%2520Active%2520Long-DocumEnt%2520Navigation%2520%2528ALDEN%2529%252C%2520a%2520multi-turn%2520reinforcement%250Alearning%2520framework%2520that%2520fine-tunes%2520VLMs%2520as%2520interactive%2520agents%2520capable%2520of%250Aactively%2520navigating%2520long%252C%2520visually%2520rich%2520documents.%2520ALDEN%2520introduces%2520a%2520novel%250Afetch%2520action%2520that%2520directly%2520accesses%2520the%2520page%2520by%2520index%252C%2520complementing%2520the%250Aclassic%2520search%2520action%2520and%2520better%2520exploiting%2520document%2520structure.%2520For%2520dense%250Aprocess%2520supervision%2520and%2520efficient%2520training%252C%2520we%2520propose%2520a%2520rule-based%2520cross-level%250Areward%2520that%2520provides%2520both%2520turn-%2520and%2520token-level%2520signals.%2520To%2520address%2520the%250Aempirically%2520observed%2520training%2520instability%2520caused%2520by%2520numerous%2520visual%2520tokens%2520from%250Along%2520documents%252C%2520we%2520further%2520propose%2520a%2520visual-semantic%2520anchoring%2520mechanism%2520that%250Aapplies%2520a%2520dual-path%2520KL-divergence%2520constraint%2520to%2520stabilize%2520visual%2520and%2520textual%250Arepresentations%2520separately%2520during%2520training.%2520Trained%2520on%2520a%2520corpus%2520constructed%250Afrom%2520three%2520open-source%2520datasets%252C%2520ALDEN%2520achieves%2520state-of-the-art%2520performance%2520on%250Afive%2520long-document%2520benchmarks.%2520Overall%252C%2520ALDEN%2520marks%2520a%2520step%2520beyond%2520passive%250Adocument%2520reading%2520toward%2520agents%2520that%2520autonomously%2520navigate%2520and%2520reason%2520across%250Along%252C%2520visually%2520rich%2520documents%252C%2520offering%2520a%2520robust%2520path%2520to%2520more%2520accurate%2520and%250Aefficient%2520long-document%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALDEN%3A%20Reinforcement%20Learning%20for%20Active%20Navigation%20and%20Evidence%0A%20%20Gathering%20in%20Long%20Documents&entry.906535625=Tianyu%20Yang%20and%20Terry%20Ruas%20and%20Yijun%20Tian%20and%20Jan%20Philip%20Wahle%20and%20Daniel%20Kurzawe%20and%20Bela%20Gipp&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20excel%20at%20interpreting%20text-rich%20images%20but%0Astruggle%20with%20long%2C%20visually%20complex%20documents%20that%20demand%20analysis%20and%0Aintegration%20of%20information%20spread%20across%20multiple%20pages.%20Existing%20approaches%0Atypically%20rely%20on%20fixed%20reasoning%20templates%20or%20rigid%20pipelines%2C%20which%20force%0AVLMs%20into%20a%20passive%20role%20and%20hinder%20both%20efficiency%20and%20generalization.%20We%0Apresent%20Active%20Long-DocumEnt%20Navigation%20%28ALDEN%29%2C%20a%20multi-turn%20reinforcement%0Alearning%20framework%20that%20fine-tunes%20VLMs%20as%20interactive%20agents%20capable%20of%0Aactively%20navigating%20long%2C%20visually%20rich%20documents.%20ALDEN%20introduces%20a%20novel%0Afetch%20action%20that%20directly%20accesses%20the%20page%20by%20index%2C%20complementing%20the%0Aclassic%20search%20action%20and%20better%20exploiting%20document%20structure.%20For%20dense%0Aprocess%20supervision%20and%20efficient%20training%2C%20we%20propose%20a%20rule-based%20cross-level%0Areward%20that%20provides%20both%20turn-%20and%20token-level%20signals.%20To%20address%20the%0Aempirically%20observed%20training%20instability%20caused%20by%20numerous%20visual%20tokens%20from%0Along%20documents%2C%20we%20further%20propose%20a%20visual-semantic%20anchoring%20mechanism%20that%0Aapplies%20a%20dual-path%20KL-divergence%20constraint%20to%20stabilize%20visual%20and%20textual%0Arepresentations%20separately%20during%20training.%20Trained%20on%20a%20corpus%20constructed%0Afrom%20three%20open-source%20datasets%2C%20ALDEN%20achieves%20state-of-the-art%20performance%20on%0Afive%20long-document%20benchmarks.%20Overall%2C%20ALDEN%20marks%20a%20step%20beyond%20passive%0Adocument%20reading%20toward%20agents%20that%20autonomously%20navigate%20and%20reason%20across%0Along%2C%20visually%20rich%20documents%2C%20offering%20a%20robust%20path%20to%20more%20accurate%20and%0Aefficient%20long-document%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25668v1&entry.124074799=Read"},
{"title": "Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic\n  Optimization", "author": "Kuan Zhang and Chengliang Chai and Jingzhe Xu and Chi Zhang and Han Han and Ye Yuan and Guoren Wang and Lei Cao", "abstract": "  Recent studies indicate that deep neural networks degrade in generalization\nperformance under noisy supervision. Existing methods focus on isolating clean\nsubsets or correcting noisy labels, facing limitations such as high\ncomputational costs, heavy hyperparameter tuning process, and coarse-grained\noptimization. To address these challenges, we propose a novel two-stage noisy\nlearning framework that enables instance-level optimization through a\ndynamically weighted loss function, avoiding hyperparameter tuning. To obtain\nstable and accurate information about noise modeling, we introduce a simple yet\neffective metric, termed wrong event, which dynamically models the cleanliness\nand difficulty of individual samples while maintaining computational costs. Our\nframework first collects wrong event information and builds a strong base\nmodel. Then we perform noise-robust training on the base model, using a\nprobabilistic model to handle the wrong event information of samples.\nExperiments on five synthetic and real-world LNL benchmarks demonstrate our\nmethod surpasses state-of-the-art methods in performance, achieves a nearly 75%\nreduction in computational time and improves model scalability.\n", "link": "http://arxiv.org/abs/2505.00812v4", "date": "2025-10-29", "relevancy": 2.2438, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5448}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Handling%20Label%20Noise%20via%20Instance-Level%20Difficulty%20Modeling%20and%20Dynamic%0A%20%20Optimization&body=Title%3A%20Handling%20Label%20Noise%20via%20Instance-Level%20Difficulty%20Modeling%20and%20Dynamic%0A%20%20Optimization%0AAuthor%3A%20Kuan%20Zhang%20and%20Chengliang%20Chai%20and%20Jingzhe%20Xu%20and%20Chi%20Zhang%20and%20Han%20Han%20and%20Ye%20Yuan%20and%20Guoren%20Wang%20and%20Lei%20Cao%0AAbstract%3A%20%20%20Recent%20studies%20indicate%20that%20deep%20neural%20networks%20degrade%20in%20generalization%0Aperformance%20under%20noisy%20supervision.%20Existing%20methods%20focus%20on%20isolating%20clean%0Asubsets%20or%20correcting%20noisy%20labels%2C%20facing%20limitations%20such%20as%20high%0Acomputational%20costs%2C%20heavy%20hyperparameter%20tuning%20process%2C%20and%20coarse-grained%0Aoptimization.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20two-stage%20noisy%0Alearning%20framework%20that%20enables%20instance-level%20optimization%20through%20a%0Adynamically%20weighted%20loss%20function%2C%20avoiding%20hyperparameter%20tuning.%20To%20obtain%0Astable%20and%20accurate%20information%20about%20noise%20modeling%2C%20we%20introduce%20a%20simple%20yet%0Aeffective%20metric%2C%20termed%20wrong%20event%2C%20which%20dynamically%20models%20the%20cleanliness%0Aand%20difficulty%20of%20individual%20samples%20while%20maintaining%20computational%20costs.%20Our%0Aframework%20first%20collects%20wrong%20event%20information%20and%20builds%20a%20strong%20base%0Amodel.%20Then%20we%20perform%20noise-robust%20training%20on%20the%20base%20model%2C%20using%20a%0Aprobabilistic%20model%20to%20handle%20the%20wrong%20event%20information%20of%20samples.%0AExperiments%20on%20five%20synthetic%20and%20real-world%20LNL%20benchmarks%20demonstrate%20our%0Amethod%20surpasses%20state-of-the-art%20methods%20in%20performance%2C%20achieves%20a%20nearly%2075%25%0Areduction%20in%20computational%20time%20and%20improves%20model%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00812v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHandling%2520Label%2520Noise%2520via%2520Instance-Level%2520Difficulty%2520Modeling%2520and%2520Dynamic%250A%2520%2520Optimization%26entry.906535625%3DKuan%2520Zhang%2520and%2520Chengliang%2520Chai%2520and%2520Jingzhe%2520Xu%2520and%2520Chi%2520Zhang%2520and%2520Han%2520Han%2520and%2520Ye%2520Yuan%2520and%2520Guoren%2520Wang%2520and%2520Lei%2520Cao%26entry.1292438233%3D%2520%2520Recent%2520studies%2520indicate%2520that%2520deep%2520neural%2520networks%2520degrade%2520in%2520generalization%250Aperformance%2520under%2520noisy%2520supervision.%2520Existing%2520methods%2520focus%2520on%2520isolating%2520clean%250Asubsets%2520or%2520correcting%2520noisy%2520labels%252C%2520facing%2520limitations%2520such%2520as%2520high%250Acomputational%2520costs%252C%2520heavy%2520hyperparameter%2520tuning%2520process%252C%2520and%2520coarse-grained%250Aoptimization.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520two-stage%2520noisy%250Alearning%2520framework%2520that%2520enables%2520instance-level%2520optimization%2520through%2520a%250Adynamically%2520weighted%2520loss%2520function%252C%2520avoiding%2520hyperparameter%2520tuning.%2520To%2520obtain%250Astable%2520and%2520accurate%2520information%2520about%2520noise%2520modeling%252C%2520we%2520introduce%2520a%2520simple%2520yet%250Aeffective%2520metric%252C%2520termed%2520wrong%2520event%252C%2520which%2520dynamically%2520models%2520the%2520cleanliness%250Aand%2520difficulty%2520of%2520individual%2520samples%2520while%2520maintaining%2520computational%2520costs.%2520Our%250Aframework%2520first%2520collects%2520wrong%2520event%2520information%2520and%2520builds%2520a%2520strong%2520base%250Amodel.%2520Then%2520we%2520perform%2520noise-robust%2520training%2520on%2520the%2520base%2520model%252C%2520using%2520a%250Aprobabilistic%2520model%2520to%2520handle%2520the%2520wrong%2520event%2520information%2520of%2520samples.%250AExperiments%2520on%2520five%2520synthetic%2520and%2520real-world%2520LNL%2520benchmarks%2520demonstrate%2520our%250Amethod%2520surpasses%2520state-of-the-art%2520methods%2520in%2520performance%252C%2520achieves%2520a%2520nearly%252075%2525%250Areduction%2520in%2520computational%2520time%2520and%2520improves%2520model%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00812v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Handling%20Label%20Noise%20via%20Instance-Level%20Difficulty%20Modeling%20and%20Dynamic%0A%20%20Optimization&entry.906535625=Kuan%20Zhang%20and%20Chengliang%20Chai%20and%20Jingzhe%20Xu%20and%20Chi%20Zhang%20and%20Han%20Han%20and%20Ye%20Yuan%20and%20Guoren%20Wang%20and%20Lei%20Cao&entry.1292438233=%20%20Recent%20studies%20indicate%20that%20deep%20neural%20networks%20degrade%20in%20generalization%0Aperformance%20under%20noisy%20supervision.%20Existing%20methods%20focus%20on%20isolating%20clean%0Asubsets%20or%20correcting%20noisy%20labels%2C%20facing%20limitations%20such%20as%20high%0Acomputational%20costs%2C%20heavy%20hyperparameter%20tuning%20process%2C%20and%20coarse-grained%0Aoptimization.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20two-stage%20noisy%0Alearning%20framework%20that%20enables%20instance-level%20optimization%20through%20a%0Adynamically%20weighted%20loss%20function%2C%20avoiding%20hyperparameter%20tuning.%20To%20obtain%0Astable%20and%20accurate%20information%20about%20noise%20modeling%2C%20we%20introduce%20a%20simple%20yet%0Aeffective%20metric%2C%20termed%20wrong%20event%2C%20which%20dynamically%20models%20the%20cleanliness%0Aand%20difficulty%20of%20individual%20samples%20while%20maintaining%20computational%20costs.%20Our%0Aframework%20first%20collects%20wrong%20event%20information%20and%20builds%20a%20strong%20base%0Amodel.%20Then%20we%20perform%20noise-robust%20training%20on%20the%20base%20model%2C%20using%20a%0Aprobabilistic%20model%20to%20handle%20the%20wrong%20event%20information%20of%20samples.%0AExperiments%20on%20five%20synthetic%20and%20real-world%20LNL%20benchmarks%20demonstrate%20our%0Amethod%20surpasses%20state-of-the-art%20methods%20in%20performance%2C%20achieves%20a%20nearly%2075%25%0Areduction%20in%20computational%20time%20and%20improves%20model%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00812v4&entry.124074799=Read"},
{"title": "Towards a Common Framework for Autoformalization", "author": "Agnieszka Mensfelt and David Tena Cucala and Santiago Franco and Angeliki Koutsoukou-Argyraki and Vince Trencsenyi and Kostas Stathis", "abstract": "  Autoformalization has emerged as a term referring to the automation of\nformalization - specifically, the formalization of mathematics using\ninteractive theorem provers (proof assistants). Its rapid development has been\ndriven by progress in deep learning, especially large language models (LLMs).\nMore recently, the term has expanded beyond mathematics to describe the broader\ntask of translating informal input into formal logical representations. At the\nsame time, a growing body of research explores using LLMs to translate informal\nlanguage into formal representations for reasoning, planning, and knowledge\nrepresentation - often without explicitly referring to this process as\nautoformalization. As a result, despite addressing similar tasks, the largely\nindependent development of these research areas has limited opportunities for\nshared methodologies, benchmarks, and theoretical frameworks that could\naccelerate progress. The goal of this paper is to review - explicit or implicit\n- instances of what can be considered autoformalization and to propose a\nunified framework, encouraging cross-pollination between different fields to\nadvance the development of next generation AI systems.\n", "link": "http://arxiv.org/abs/2509.09810v2", "date": "2025-10-29", "relevancy": 2.2331, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4469}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Common%20Framework%20for%20Autoformalization&body=Title%3A%20Towards%20a%20Common%20Framework%20for%20Autoformalization%0AAuthor%3A%20Agnieszka%20Mensfelt%20and%20David%20Tena%20Cucala%20and%20Santiago%20Franco%20and%20Angeliki%20Koutsoukou-Argyraki%20and%20Vince%20Trencsenyi%20and%20Kostas%20Stathis%0AAbstract%3A%20%20%20Autoformalization%20has%20emerged%20as%20a%20term%20referring%20to%20the%20automation%20of%0Aformalization%20-%20specifically%2C%20the%20formalization%20of%20mathematics%20using%0Ainteractive%20theorem%20provers%20%28proof%20assistants%29.%20Its%20rapid%20development%20has%20been%0Adriven%20by%20progress%20in%20deep%20learning%2C%20especially%20large%20language%20models%20%28LLMs%29.%0AMore%20recently%2C%20the%20term%20has%20expanded%20beyond%20mathematics%20to%20describe%20the%20broader%0Atask%20of%20translating%20informal%20input%20into%20formal%20logical%20representations.%20At%20the%0Asame%20time%2C%20a%20growing%20body%20of%20research%20explores%20using%20LLMs%20to%20translate%20informal%0Alanguage%20into%20formal%20representations%20for%20reasoning%2C%20planning%2C%20and%20knowledge%0Arepresentation%20-%20often%20without%20explicitly%20referring%20to%20this%20process%20as%0Aautoformalization.%20As%20a%20result%2C%20despite%20addressing%20similar%20tasks%2C%20the%20largely%0Aindependent%20development%20of%20these%20research%20areas%20has%20limited%20opportunities%20for%0Ashared%20methodologies%2C%20benchmarks%2C%20and%20theoretical%20frameworks%20that%20could%0Aaccelerate%20progress.%20The%20goal%20of%20this%20paper%20is%20to%20review%20-%20explicit%20or%20implicit%0A-%20instances%20of%20what%20can%20be%20considered%20autoformalization%20and%20to%20propose%20a%0Aunified%20framework%2C%20encouraging%20cross-pollination%20between%20different%20fields%20to%0Aadvance%20the%20development%20of%20next%20generation%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.09810v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Common%2520Framework%2520for%2520Autoformalization%26entry.906535625%3DAgnieszka%2520Mensfelt%2520and%2520David%2520Tena%2520Cucala%2520and%2520Santiago%2520Franco%2520and%2520Angeliki%2520Koutsoukou-Argyraki%2520and%2520Vince%2520Trencsenyi%2520and%2520Kostas%2520Stathis%26entry.1292438233%3D%2520%2520Autoformalization%2520has%2520emerged%2520as%2520a%2520term%2520referring%2520to%2520the%2520automation%2520of%250Aformalization%2520-%2520specifically%252C%2520the%2520formalization%2520of%2520mathematics%2520using%250Ainteractive%2520theorem%2520provers%2520%2528proof%2520assistants%2529.%2520Its%2520rapid%2520development%2520has%2520been%250Adriven%2520by%2520progress%2520in%2520deep%2520learning%252C%2520especially%2520large%2520language%2520models%2520%2528LLMs%2529.%250AMore%2520recently%252C%2520the%2520term%2520has%2520expanded%2520beyond%2520mathematics%2520to%2520describe%2520the%2520broader%250Atask%2520of%2520translating%2520informal%2520input%2520into%2520formal%2520logical%2520representations.%2520At%2520the%250Asame%2520time%252C%2520a%2520growing%2520body%2520of%2520research%2520explores%2520using%2520LLMs%2520to%2520translate%2520informal%250Alanguage%2520into%2520formal%2520representations%2520for%2520reasoning%252C%2520planning%252C%2520and%2520knowledge%250Arepresentation%2520-%2520often%2520without%2520explicitly%2520referring%2520to%2520this%2520process%2520as%250Aautoformalization.%2520As%2520a%2520result%252C%2520despite%2520addressing%2520similar%2520tasks%252C%2520the%2520largely%250Aindependent%2520development%2520of%2520these%2520research%2520areas%2520has%2520limited%2520opportunities%2520for%250Ashared%2520methodologies%252C%2520benchmarks%252C%2520and%2520theoretical%2520frameworks%2520that%2520could%250Aaccelerate%2520progress.%2520The%2520goal%2520of%2520this%2520paper%2520is%2520to%2520review%2520-%2520explicit%2520or%2520implicit%250A-%2520instances%2520of%2520what%2520can%2520be%2520considered%2520autoformalization%2520and%2520to%2520propose%2520a%250Aunified%2520framework%252C%2520encouraging%2520cross-pollination%2520between%2520different%2520fields%2520to%250Aadvance%2520the%2520development%2520of%2520next%2520generation%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09810v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Common%20Framework%20for%20Autoformalization&entry.906535625=Agnieszka%20Mensfelt%20and%20David%20Tena%20Cucala%20and%20Santiago%20Franco%20and%20Angeliki%20Koutsoukou-Argyraki%20and%20Vince%20Trencsenyi%20and%20Kostas%20Stathis&entry.1292438233=%20%20Autoformalization%20has%20emerged%20as%20a%20term%20referring%20to%20the%20automation%20of%0Aformalization%20-%20specifically%2C%20the%20formalization%20of%20mathematics%20using%0Ainteractive%20theorem%20provers%20%28proof%20assistants%29.%20Its%20rapid%20development%20has%20been%0Adriven%20by%20progress%20in%20deep%20learning%2C%20especially%20large%20language%20models%20%28LLMs%29.%0AMore%20recently%2C%20the%20term%20has%20expanded%20beyond%20mathematics%20to%20describe%20the%20broader%0Atask%20of%20translating%20informal%20input%20into%20formal%20logical%20representations.%20At%20the%0Asame%20time%2C%20a%20growing%20body%20of%20research%20explores%20using%20LLMs%20to%20translate%20informal%0Alanguage%20into%20formal%20representations%20for%20reasoning%2C%20planning%2C%20and%20knowledge%0Arepresentation%20-%20often%20without%20explicitly%20referring%20to%20this%20process%20as%0Aautoformalization.%20As%20a%20result%2C%20despite%20addressing%20similar%20tasks%2C%20the%20largely%0Aindependent%20development%20of%20these%20research%20areas%20has%20limited%20opportunities%20for%0Ashared%20methodologies%2C%20benchmarks%2C%20and%20theoretical%20frameworks%20that%20could%0Aaccelerate%20progress.%20The%20goal%20of%20this%20paper%20is%20to%20review%20-%20explicit%20or%20implicit%0A-%20instances%20of%20what%20can%20be%20considered%20autoformalization%20and%20to%20propose%20a%0Aunified%20framework%2C%20encouraging%20cross-pollination%20between%20different%20fields%20to%0Aadvance%20the%20development%20of%20next%20generation%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.09810v2&entry.124074799=Read"},
{"title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video\n  Generation", "author": "Jaechul Roh and Zachary Novack and Yuefeng Peng and Niloofar Mireshghallah and Taylor Berg-Kirkpatrick and Amir Houmansadr", "abstract": "  Generative AI systems for music and video commonly use text-based filters to\nprevent the regurgitation of copyrighted material. We expose a fundamental flaw\nin this approach by introducing Adversarial PhoneTic Prompting (APT), a novel\nattack that bypasses these safeguards by exploiting phonetic memorization. The\nAPT attack replaces iconic lyrics with homophonic but semantically unrelated\nalternatives (e.g., \"mom's spaghetti\" becomes \"Bob's confetti\"), preserving\nacoustic structure while altering meaning; we identify high-fidelity phonetic\nmatches using CMU pronouncing dictionary. We demonstrate that leading\nLyrics-to-Song (L2S) models like SUNO and YuE regenerate songs with striking\nmelodic and rhythmic similarity to their copyrighted originals when prompted\nwith these altered lyrics. More surprisingly, this vulnerability extends across\nmodalities. When prompted with phonetically modified lyrics from a song, a\nText-to-Video (T2V) model like Veo 3 reconstructs visual scenes from the\noriginal music video-including specific settings and character\narchetypes-despite the absence of any visual cues in the prompt. Our findings\nreveal that models memorize deep, structural patterns tied to acoustics, not\njust verbatim text. This phonetic-to-visual leakage represents a critical\nvulnerability in transcript-conditioned generative models, rendering simple\ncopyright filters ineffective and raising urgent concerns about the secure\ndeployment of multimodal AI systems. Demo examples are available at our project\npage (https://jrohsc.github.io/music_attack/).\n", "link": "http://arxiv.org/abs/2507.17937v3", "date": "2025-10-29", "relevancy": 2.2238, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5671}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5598}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bob%27s%20Confetti%3A%20Phonetic%20Memorization%20Attacks%20in%20Music%20and%20Video%0A%20%20Generation&body=Title%3A%20Bob%27s%20Confetti%3A%20Phonetic%20Memorization%20Attacks%20in%20Music%20and%20Video%0A%20%20Generation%0AAuthor%3A%20Jaechul%20Roh%20and%20Zachary%20Novack%20and%20Yuefeng%20Peng%20and%20Niloofar%20Mireshghallah%20and%20Taylor%20Berg-Kirkpatrick%20and%20Amir%20Houmansadr%0AAbstract%3A%20%20%20Generative%20AI%20systems%20for%20music%20and%20video%20commonly%20use%20text-based%20filters%20to%0Aprevent%20the%20regurgitation%20of%20copyrighted%20material.%20We%20expose%20a%20fundamental%20flaw%0Ain%20this%20approach%20by%20introducing%20Adversarial%20PhoneTic%20Prompting%20%28APT%29%2C%20a%20novel%0Aattack%20that%20bypasses%20these%20safeguards%20by%20exploiting%20phonetic%20memorization.%20The%0AAPT%20attack%20replaces%20iconic%20lyrics%20with%20homophonic%20but%20semantically%20unrelated%0Aalternatives%20%28e.g.%2C%20%22mom%27s%20spaghetti%22%20becomes%20%22Bob%27s%20confetti%22%29%2C%20preserving%0Aacoustic%20structure%20while%20altering%20meaning%3B%20we%20identify%20high-fidelity%20phonetic%0Amatches%20using%20CMU%20pronouncing%20dictionary.%20We%20demonstrate%20that%20leading%0ALyrics-to-Song%20%28L2S%29%20models%20like%20SUNO%20and%20YuE%20regenerate%20songs%20with%20striking%0Amelodic%20and%20rhythmic%20similarity%20to%20their%20copyrighted%20originals%20when%20prompted%0Awith%20these%20altered%20lyrics.%20More%20surprisingly%2C%20this%20vulnerability%20extends%20across%0Amodalities.%20When%20prompted%20with%20phonetically%20modified%20lyrics%20from%20a%20song%2C%20a%0AText-to-Video%20%28T2V%29%20model%20like%20Veo%203%20reconstructs%20visual%20scenes%20from%20the%0Aoriginal%20music%20video-including%20specific%20settings%20and%20character%0Aarchetypes-despite%20the%20absence%20of%20any%20visual%20cues%20in%20the%20prompt.%20Our%20findings%0Areveal%20that%20models%20memorize%20deep%2C%20structural%20patterns%20tied%20to%20acoustics%2C%20not%0Ajust%20verbatim%20text.%20This%20phonetic-to-visual%20leakage%20represents%20a%20critical%0Avulnerability%20in%20transcript-conditioned%20generative%20models%2C%20rendering%20simple%0Acopyright%20filters%20ineffective%20and%20raising%20urgent%20concerns%20about%20the%20secure%0Adeployment%20of%20multimodal%20AI%20systems.%20Demo%20examples%20are%20available%20at%20our%20project%0Apage%20%28https%3A//jrohsc.github.io/music_attack/%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17937v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBob%2527s%2520Confetti%253A%2520Phonetic%2520Memorization%2520Attacks%2520in%2520Music%2520and%2520Video%250A%2520%2520Generation%26entry.906535625%3DJaechul%2520Roh%2520and%2520Zachary%2520Novack%2520and%2520Yuefeng%2520Peng%2520and%2520Niloofar%2520Mireshghallah%2520and%2520Taylor%2520Berg-Kirkpatrick%2520and%2520Amir%2520Houmansadr%26entry.1292438233%3D%2520%2520Generative%2520AI%2520systems%2520for%2520music%2520and%2520video%2520commonly%2520use%2520text-based%2520filters%2520to%250Aprevent%2520the%2520regurgitation%2520of%2520copyrighted%2520material.%2520We%2520expose%2520a%2520fundamental%2520flaw%250Ain%2520this%2520approach%2520by%2520introducing%2520Adversarial%2520PhoneTic%2520Prompting%2520%2528APT%2529%252C%2520a%2520novel%250Aattack%2520that%2520bypasses%2520these%2520safeguards%2520by%2520exploiting%2520phonetic%2520memorization.%2520The%250AAPT%2520attack%2520replaces%2520iconic%2520lyrics%2520with%2520homophonic%2520but%2520semantically%2520unrelated%250Aalternatives%2520%2528e.g.%252C%2520%2522mom%2527s%2520spaghetti%2522%2520becomes%2520%2522Bob%2527s%2520confetti%2522%2529%252C%2520preserving%250Aacoustic%2520structure%2520while%2520altering%2520meaning%253B%2520we%2520identify%2520high-fidelity%2520phonetic%250Amatches%2520using%2520CMU%2520pronouncing%2520dictionary.%2520We%2520demonstrate%2520that%2520leading%250ALyrics-to-Song%2520%2528L2S%2529%2520models%2520like%2520SUNO%2520and%2520YuE%2520regenerate%2520songs%2520with%2520striking%250Amelodic%2520and%2520rhythmic%2520similarity%2520to%2520their%2520copyrighted%2520originals%2520when%2520prompted%250Awith%2520these%2520altered%2520lyrics.%2520More%2520surprisingly%252C%2520this%2520vulnerability%2520extends%2520across%250Amodalities.%2520When%2520prompted%2520with%2520phonetically%2520modified%2520lyrics%2520from%2520a%2520song%252C%2520a%250AText-to-Video%2520%2528T2V%2529%2520model%2520like%2520Veo%25203%2520reconstructs%2520visual%2520scenes%2520from%2520the%250Aoriginal%2520music%2520video-including%2520specific%2520settings%2520and%2520character%250Aarchetypes-despite%2520the%2520absence%2520of%2520any%2520visual%2520cues%2520in%2520the%2520prompt.%2520Our%2520findings%250Areveal%2520that%2520models%2520memorize%2520deep%252C%2520structural%2520patterns%2520tied%2520to%2520acoustics%252C%2520not%250Ajust%2520verbatim%2520text.%2520This%2520phonetic-to-visual%2520leakage%2520represents%2520a%2520critical%250Avulnerability%2520in%2520transcript-conditioned%2520generative%2520models%252C%2520rendering%2520simple%250Acopyright%2520filters%2520ineffective%2520and%2520raising%2520urgent%2520concerns%2520about%2520the%2520secure%250Adeployment%2520of%2520multimodal%2520AI%2520systems.%2520Demo%2520examples%2520are%2520available%2520at%2520our%2520project%250Apage%2520%2528https%253A//jrohsc.github.io/music_attack/%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17937v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bob%27s%20Confetti%3A%20Phonetic%20Memorization%20Attacks%20in%20Music%20and%20Video%0A%20%20Generation&entry.906535625=Jaechul%20Roh%20and%20Zachary%20Novack%20and%20Yuefeng%20Peng%20and%20Niloofar%20Mireshghallah%20and%20Taylor%20Berg-Kirkpatrick%20and%20Amir%20Houmansadr&entry.1292438233=%20%20Generative%20AI%20systems%20for%20music%20and%20video%20commonly%20use%20text-based%20filters%20to%0Aprevent%20the%20regurgitation%20of%20copyrighted%20material.%20We%20expose%20a%20fundamental%20flaw%0Ain%20this%20approach%20by%20introducing%20Adversarial%20PhoneTic%20Prompting%20%28APT%29%2C%20a%20novel%0Aattack%20that%20bypasses%20these%20safeguards%20by%20exploiting%20phonetic%20memorization.%20The%0AAPT%20attack%20replaces%20iconic%20lyrics%20with%20homophonic%20but%20semantically%20unrelated%0Aalternatives%20%28e.g.%2C%20%22mom%27s%20spaghetti%22%20becomes%20%22Bob%27s%20confetti%22%29%2C%20preserving%0Aacoustic%20structure%20while%20altering%20meaning%3B%20we%20identify%20high-fidelity%20phonetic%0Amatches%20using%20CMU%20pronouncing%20dictionary.%20We%20demonstrate%20that%20leading%0ALyrics-to-Song%20%28L2S%29%20models%20like%20SUNO%20and%20YuE%20regenerate%20songs%20with%20striking%0Amelodic%20and%20rhythmic%20similarity%20to%20their%20copyrighted%20originals%20when%20prompted%0Awith%20these%20altered%20lyrics.%20More%20surprisingly%2C%20this%20vulnerability%20extends%20across%0Amodalities.%20When%20prompted%20with%20phonetically%20modified%20lyrics%20from%20a%20song%2C%20a%0AText-to-Video%20%28T2V%29%20model%20like%20Veo%203%20reconstructs%20visual%20scenes%20from%20the%0Aoriginal%20music%20video-including%20specific%20settings%20and%20character%0Aarchetypes-despite%20the%20absence%20of%20any%20visual%20cues%20in%20the%20prompt.%20Our%20findings%0Areveal%20that%20models%20memorize%20deep%2C%20structural%20patterns%20tied%20to%20acoustics%2C%20not%0Ajust%20verbatim%20text.%20This%20phonetic-to-visual%20leakage%20represents%20a%20critical%0Avulnerability%20in%20transcript-conditioned%20generative%20models%2C%20rendering%20simple%0Acopyright%20filters%20ineffective%20and%20raising%20urgent%20concerns%20about%20the%20secure%0Adeployment%20of%20multimodal%20AI%20systems.%20Demo%20examples%20are%20available%20at%20our%20project%0Apage%20%28https%3A//jrohsc.github.io/music_attack/%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17937v3&entry.124074799=Read"},
{"title": "Multimodal Recurrent Ensembles for Predicting Brain Responses to\n  Naturalistic Movies (Algonauts 2025)", "author": "Semih Eren and Deniz Kucukahmetler and Nico Scherf", "abstract": "  Accurately predicting distributed cortical responses to naturalistic stimuli\nrequires models that integrate visual, auditory and semantic information over\ntime. We present a hierarchical multimodal recurrent ensemble that maps\npretrained video, audio, and language embeddings to fMRI time series recorded\nwhile four subjects watched almost 80 hours of movies provided by the Algonauts\n2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;\ntheir hidden states are fused and passed to a second recurrent layer, and\nlightweight subject-specific heads output responses for 1000 cortical parcels.\nTraining relies on a composite MSE-correlation loss and a curriculum that\ngradually shifts emphasis from early sensory to late association regions.\nAveraging 100 model variants further boosts robustness. The resulting system\nranked third on the competition leaderboard, achieving an overall Pearson r =\n0.2094 and the highest single-parcel peak score (mean r = 0.63) among all\nparticipants, with particularly strong gains for the most challenging subject\n(Subject 5). The approach establishes a simple, extensible baseline for future\nmultimodal brain-encoding benchmarks.\n", "link": "http://arxiv.org/abs/2507.17897v4", "date": "2025-10-29", "relevancy": 2.2212, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Recurrent%20Ensembles%20for%20Predicting%20Brain%20Responses%20to%0A%20%20Naturalistic%20Movies%20%28Algonauts%202025%29&body=Title%3A%20Multimodal%20Recurrent%20Ensembles%20for%20Predicting%20Brain%20Responses%20to%0A%20%20Naturalistic%20Movies%20%28Algonauts%202025%29%0AAuthor%3A%20Semih%20Eren%20and%20Deniz%20Kucukahmetler%20and%20Nico%20Scherf%0AAbstract%3A%20%20%20Accurately%20predicting%20distributed%20cortical%20responses%20to%20naturalistic%20stimuli%0Arequires%20models%20that%20integrate%20visual%2C%20auditory%20and%20semantic%20information%20over%0Atime.%20We%20present%20a%20hierarchical%20multimodal%20recurrent%20ensemble%20that%20maps%0Apretrained%20video%2C%20audio%2C%20and%20language%20embeddings%20to%20fMRI%20time%20series%20recorded%0Awhile%20four%20subjects%20watched%20almost%2080%20hours%20of%20movies%20provided%20by%20the%20Algonauts%0A2025%20challenge.%20Modality-specific%20bidirectional%20RNNs%20encode%20temporal%20dynamics%3B%0Atheir%20hidden%20states%20are%20fused%20and%20passed%20to%20a%20second%20recurrent%20layer%2C%20and%0Alightweight%20subject-specific%20heads%20output%20responses%20for%201000%20cortical%20parcels.%0ATraining%20relies%20on%20a%20composite%20MSE-correlation%20loss%20and%20a%20curriculum%20that%0Agradually%20shifts%20emphasis%20from%20early%20sensory%20to%20late%20association%20regions.%0AAveraging%20100%20model%20variants%20further%20boosts%20robustness.%20The%20resulting%20system%0Aranked%20third%20on%20the%20competition%20leaderboard%2C%20achieving%20an%20overall%20Pearson%20r%20%3D%0A0.2094%20and%20the%20highest%20single-parcel%20peak%20score%20%28mean%20r%20%3D%200.63%29%20among%20all%0Aparticipants%2C%20with%20particularly%20strong%20gains%20for%20the%20most%20challenging%20subject%0A%28Subject%205%29.%20The%20approach%20establishes%20a%20simple%2C%20extensible%20baseline%20for%20future%0Amultimodal%20brain-encoding%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17897v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Recurrent%2520Ensembles%2520for%2520Predicting%2520Brain%2520Responses%2520to%250A%2520%2520Naturalistic%2520Movies%2520%2528Algonauts%25202025%2529%26entry.906535625%3DSemih%2520Eren%2520and%2520Deniz%2520Kucukahmetler%2520and%2520Nico%2520Scherf%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520distributed%2520cortical%2520responses%2520to%2520naturalistic%2520stimuli%250Arequires%2520models%2520that%2520integrate%2520visual%252C%2520auditory%2520and%2520semantic%2520information%2520over%250Atime.%2520We%2520present%2520a%2520hierarchical%2520multimodal%2520recurrent%2520ensemble%2520that%2520maps%250Apretrained%2520video%252C%2520audio%252C%2520and%2520language%2520embeddings%2520to%2520fMRI%2520time%2520series%2520recorded%250Awhile%2520four%2520subjects%2520watched%2520almost%252080%2520hours%2520of%2520movies%2520provided%2520by%2520the%2520Algonauts%250A2025%2520challenge.%2520Modality-specific%2520bidirectional%2520RNNs%2520encode%2520temporal%2520dynamics%253B%250Atheir%2520hidden%2520states%2520are%2520fused%2520and%2520passed%2520to%2520a%2520second%2520recurrent%2520layer%252C%2520and%250Alightweight%2520subject-specific%2520heads%2520output%2520responses%2520for%25201000%2520cortical%2520parcels.%250ATraining%2520relies%2520on%2520a%2520composite%2520MSE-correlation%2520loss%2520and%2520a%2520curriculum%2520that%250Agradually%2520shifts%2520emphasis%2520from%2520early%2520sensory%2520to%2520late%2520association%2520regions.%250AAveraging%2520100%2520model%2520variants%2520further%2520boosts%2520robustness.%2520The%2520resulting%2520system%250Aranked%2520third%2520on%2520the%2520competition%2520leaderboard%252C%2520achieving%2520an%2520overall%2520Pearson%2520r%2520%253D%250A0.2094%2520and%2520the%2520highest%2520single-parcel%2520peak%2520score%2520%2528mean%2520r%2520%253D%25200.63%2529%2520among%2520all%250Aparticipants%252C%2520with%2520particularly%2520strong%2520gains%2520for%2520the%2520most%2520challenging%2520subject%250A%2528Subject%25205%2529.%2520The%2520approach%2520establishes%2520a%2520simple%252C%2520extensible%2520baseline%2520for%2520future%250Amultimodal%2520brain-encoding%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17897v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Recurrent%20Ensembles%20for%20Predicting%20Brain%20Responses%20to%0A%20%20Naturalistic%20Movies%20%28Algonauts%202025%29&entry.906535625=Semih%20Eren%20and%20Deniz%20Kucukahmetler%20and%20Nico%20Scherf&entry.1292438233=%20%20Accurately%20predicting%20distributed%20cortical%20responses%20to%20naturalistic%20stimuli%0Arequires%20models%20that%20integrate%20visual%2C%20auditory%20and%20semantic%20information%20over%0Atime.%20We%20present%20a%20hierarchical%20multimodal%20recurrent%20ensemble%20that%20maps%0Apretrained%20video%2C%20audio%2C%20and%20language%20embeddings%20to%20fMRI%20time%20series%20recorded%0Awhile%20four%20subjects%20watched%20almost%2080%20hours%20of%20movies%20provided%20by%20the%20Algonauts%0A2025%20challenge.%20Modality-specific%20bidirectional%20RNNs%20encode%20temporal%20dynamics%3B%0Atheir%20hidden%20states%20are%20fused%20and%20passed%20to%20a%20second%20recurrent%20layer%2C%20and%0Alightweight%20subject-specific%20heads%20output%20responses%20for%201000%20cortical%20parcels.%0ATraining%20relies%20on%20a%20composite%20MSE-correlation%20loss%20and%20a%20curriculum%20that%0Agradually%20shifts%20emphasis%20from%20early%20sensory%20to%20late%20association%20regions.%0AAveraging%20100%20model%20variants%20further%20boosts%20robustness.%20The%20resulting%20system%0Aranked%20third%20on%20the%20competition%20leaderboard%2C%20achieving%20an%20overall%20Pearson%20r%20%3D%0A0.2094%20and%20the%20highest%20single-parcel%20peak%20score%20%28mean%20r%20%3D%200.63%29%20among%20all%0Aparticipants%2C%20with%20particularly%20strong%20gains%20for%20the%20most%20challenging%20subject%0A%28Subject%205%29.%20The%20approach%20establishes%20a%20simple%2C%20extensible%20baseline%20for%20future%0Amultimodal%20brain-encoding%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17897v4&entry.124074799=Read"},
{"title": "Spectral Perturbation Bounds for Low-Rank Approximation with\n  Applications to Privacy", "author": "Phuc Tran and Nisheeth K. Vishnoi and Van H. Vu", "abstract": "  A central challenge in machine learning is to understand how noise or\nmeasurement errors affect low-rank approximations, particularly in the spectral\nnorm. This question is especially important in differentially private low-rank\napproximation, where one aims to preserve the top-$p$ structure of a\ndata-derived matrix while ensuring privacy. Prior work often analyzes Frobenius\nnorm error or changes in reconstruction quality, but these metrics can over- or\nunder-estimate true subspace distortion. The spectral norm, by contrast,\ncaptures worst-case directional error and provides the strongest utility\nguarantees. We establish new high-probability spectral-norm perturbation bounds\nfor symmetric matrices that refine the classical Eckart--Young--Mirsky theorem\nand explicitly capture interactions between a matrix $A \\in \\mathbb{R}^{n\n\\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and\nnorm conditions, our bounds yield sharp estimates for $\\|(A + E)_p - A_p\\|$,\nwhere $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up\nto a factor of $\\sqrt{n}$. As an application, we derive improved utility\nguarantees for differentially private PCA, resolving an open problem in the\nliterature. Our analysis relies on a novel contour bootstrapping method from\ncomplex analysis and extends it to a broad class of spectral functionals,\nincluding polynomials and matrix exponentials. Empirical results on real-world\ndatasets confirm that our bounds closely track the actual spectral error under\ndiverse perturbation regimes.\n", "link": "http://arxiv.org/abs/2510.25670v1", "date": "2025-10-29", "relevancy": 2.2047, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.451}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4376}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Perturbation%20Bounds%20for%20Low-Rank%20Approximation%20with%0A%20%20Applications%20to%20Privacy&body=Title%3A%20Spectral%20Perturbation%20Bounds%20for%20Low-Rank%20Approximation%20with%0A%20%20Applications%20to%20Privacy%0AAuthor%3A%20Phuc%20Tran%20and%20Nisheeth%20K.%20Vishnoi%20and%20Van%20H.%20Vu%0AAbstract%3A%20%20%20A%20central%20challenge%20in%20machine%20learning%20is%20to%20understand%20how%20noise%20or%0Ameasurement%20errors%20affect%20low-rank%20approximations%2C%20particularly%20in%20the%20spectral%0Anorm.%20This%20question%20is%20especially%20important%20in%20differentially%20private%20low-rank%0Aapproximation%2C%20where%20one%20aims%20to%20preserve%20the%20top-%24p%24%20structure%20of%20a%0Adata-derived%20matrix%20while%20ensuring%20privacy.%20Prior%20work%20often%20analyzes%20Frobenius%0Anorm%20error%20or%20changes%20in%20reconstruction%20quality%2C%20but%20these%20metrics%20can%20over-%20or%0Aunder-estimate%20true%20subspace%20distortion.%20The%20spectral%20norm%2C%20by%20contrast%2C%0Acaptures%20worst-case%20directional%20error%20and%20provides%20the%20strongest%20utility%0Aguarantees.%20We%20establish%20new%20high-probability%20spectral-norm%20perturbation%20bounds%0Afor%20symmetric%20matrices%20that%20refine%20the%20classical%20Eckart--Young--Mirsky%20theorem%0Aand%20explicitly%20capture%20interactions%20between%20a%20matrix%20%24A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%0A%5Ctimes%20n%7D%24%20and%20an%20arbitrary%20symmetric%20perturbation%20%24E%24.%20Under%20mild%20eigengap%20and%0Anorm%20conditions%2C%20our%20bounds%20yield%20sharp%20estimates%20for%20%24%5C%7C%28A%20%2B%20E%29_p%20-%20A_p%5C%7C%24%2C%0Awhere%20%24A_p%24%20is%20the%20best%20rank-%24p%24%20approximation%20of%20%24A%24%2C%20with%20improvements%20of%20up%0Ato%20a%20factor%20of%20%24%5Csqrt%7Bn%7D%24.%20As%20an%20application%2C%20we%20derive%20improved%20utility%0Aguarantees%20for%20differentially%20private%20PCA%2C%20resolving%20an%20open%20problem%20in%20the%0Aliterature.%20Our%20analysis%20relies%20on%20a%20novel%20contour%20bootstrapping%20method%20from%0Acomplex%20analysis%20and%20extends%20it%20to%20a%20broad%20class%20of%20spectral%20functionals%2C%0Aincluding%20polynomials%20and%20matrix%20exponentials.%20Empirical%20results%20on%20real-world%0Adatasets%20confirm%20that%20our%20bounds%20closely%20track%20the%20actual%20spectral%20error%20under%0Adiverse%20perturbation%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Perturbation%2520Bounds%2520for%2520Low-Rank%2520Approximation%2520with%250A%2520%2520Applications%2520to%2520Privacy%26entry.906535625%3DPhuc%2520Tran%2520and%2520Nisheeth%2520K.%2520Vishnoi%2520and%2520Van%2520H.%2520Vu%26entry.1292438233%3D%2520%2520A%2520central%2520challenge%2520in%2520machine%2520learning%2520is%2520to%2520understand%2520how%2520noise%2520or%250Ameasurement%2520errors%2520affect%2520low-rank%2520approximations%252C%2520particularly%2520in%2520the%2520spectral%250Anorm.%2520This%2520question%2520is%2520especially%2520important%2520in%2520differentially%2520private%2520low-rank%250Aapproximation%252C%2520where%2520one%2520aims%2520to%2520preserve%2520the%2520top-%2524p%2524%2520structure%2520of%2520a%250Adata-derived%2520matrix%2520while%2520ensuring%2520privacy.%2520Prior%2520work%2520often%2520analyzes%2520Frobenius%250Anorm%2520error%2520or%2520changes%2520in%2520reconstruction%2520quality%252C%2520but%2520these%2520metrics%2520can%2520over-%2520or%250Aunder-estimate%2520true%2520subspace%2520distortion.%2520The%2520spectral%2520norm%252C%2520by%2520contrast%252C%250Acaptures%2520worst-case%2520directional%2520error%2520and%2520provides%2520the%2520strongest%2520utility%250Aguarantees.%2520We%2520establish%2520new%2520high-probability%2520spectral-norm%2520perturbation%2520bounds%250Afor%2520symmetric%2520matrices%2520that%2520refine%2520the%2520classical%2520Eckart--Young--Mirsky%2520theorem%250Aand%2520explicitly%2520capture%2520interactions%2520between%2520a%2520matrix%2520%2524A%2520%255Cin%2520%255Cmathbb%257BR%257D%255E%257Bn%250A%255Ctimes%2520n%257D%2524%2520and%2520an%2520arbitrary%2520symmetric%2520perturbation%2520%2524E%2524.%2520Under%2520mild%2520eigengap%2520and%250Anorm%2520conditions%252C%2520our%2520bounds%2520yield%2520sharp%2520estimates%2520for%2520%2524%255C%257C%2528A%2520%252B%2520E%2529_p%2520-%2520A_p%255C%257C%2524%252C%250Awhere%2520%2524A_p%2524%2520is%2520the%2520best%2520rank-%2524p%2524%2520approximation%2520of%2520%2524A%2524%252C%2520with%2520improvements%2520of%2520up%250Ato%2520a%2520factor%2520of%2520%2524%255Csqrt%257Bn%257D%2524.%2520As%2520an%2520application%252C%2520we%2520derive%2520improved%2520utility%250Aguarantees%2520for%2520differentially%2520private%2520PCA%252C%2520resolving%2520an%2520open%2520problem%2520in%2520the%250Aliterature.%2520Our%2520analysis%2520relies%2520on%2520a%2520novel%2520contour%2520bootstrapping%2520method%2520from%250Acomplex%2520analysis%2520and%2520extends%2520it%2520to%2520a%2520broad%2520class%2520of%2520spectral%2520functionals%252C%250Aincluding%2520polynomials%2520and%2520matrix%2520exponentials.%2520Empirical%2520results%2520on%2520real-world%250Adatasets%2520confirm%2520that%2520our%2520bounds%2520closely%2520track%2520the%2520actual%2520spectral%2520error%2520under%250Adiverse%2520perturbation%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Perturbation%20Bounds%20for%20Low-Rank%20Approximation%20with%0A%20%20Applications%20to%20Privacy&entry.906535625=Phuc%20Tran%20and%20Nisheeth%20K.%20Vishnoi%20and%20Van%20H.%20Vu&entry.1292438233=%20%20A%20central%20challenge%20in%20machine%20learning%20is%20to%20understand%20how%20noise%20or%0Ameasurement%20errors%20affect%20low-rank%20approximations%2C%20particularly%20in%20the%20spectral%0Anorm.%20This%20question%20is%20especially%20important%20in%20differentially%20private%20low-rank%0Aapproximation%2C%20where%20one%20aims%20to%20preserve%20the%20top-%24p%24%20structure%20of%20a%0Adata-derived%20matrix%20while%20ensuring%20privacy.%20Prior%20work%20often%20analyzes%20Frobenius%0Anorm%20error%20or%20changes%20in%20reconstruction%20quality%2C%20but%20these%20metrics%20can%20over-%20or%0Aunder-estimate%20true%20subspace%20distortion.%20The%20spectral%20norm%2C%20by%20contrast%2C%0Acaptures%20worst-case%20directional%20error%20and%20provides%20the%20strongest%20utility%0Aguarantees.%20We%20establish%20new%20high-probability%20spectral-norm%20perturbation%20bounds%0Afor%20symmetric%20matrices%20that%20refine%20the%20classical%20Eckart--Young--Mirsky%20theorem%0Aand%20explicitly%20capture%20interactions%20between%20a%20matrix%20%24A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%0A%5Ctimes%20n%7D%24%20and%20an%20arbitrary%20symmetric%20perturbation%20%24E%24.%20Under%20mild%20eigengap%20and%0Anorm%20conditions%2C%20our%20bounds%20yield%20sharp%20estimates%20for%20%24%5C%7C%28A%20%2B%20E%29_p%20-%20A_p%5C%7C%24%2C%0Awhere%20%24A_p%24%20is%20the%20best%20rank-%24p%24%20approximation%20of%20%24A%24%2C%20with%20improvements%20of%20up%0Ato%20a%20factor%20of%20%24%5Csqrt%7Bn%7D%24.%20As%20an%20application%2C%20we%20derive%20improved%20utility%0Aguarantees%20for%20differentially%20private%20PCA%2C%20resolving%20an%20open%20problem%20in%20the%0Aliterature.%20Our%20analysis%20relies%20on%20a%20novel%20contour%20bootstrapping%20method%20from%0Acomplex%20analysis%20and%20extends%20it%20to%20a%20broad%20class%20of%20spectral%20functionals%2C%0Aincluding%20polynomials%20and%20matrix%20exponentials.%20Empirical%20results%20on%20real-world%0Adatasets%20confirm%20that%20our%20bounds%20closely%20track%20the%20actual%20spectral%20error%20under%0Adiverse%20perturbation%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25670v1&entry.124074799=Read"},
{"title": "RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning", "author": "Jing Hu and Chengming Feng and Shu Hu and Ming-Ching Chang and Xin Li and Xi Wu and Xin Wang", "abstract": "  Most existing Image-to-Image Translation (I2IT) methods generate images in a\nsingle run of a deep learning (DL) model. However, designing such a single-step\nmodel is always challenging, requiring a huge number of parameters and easily\nfalling into bad global minimums and overfitting. In this work, we reformulate\nI2IT as a step-wise decision-making problem via deep reinforcement learning\n(DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The\nkey feature in the RL-I2IT framework is to decompose a monolithic learning\nprocess into small steps with a lightweight model to progressively transform a\nsource image successively to a target image. Considering that it is challenging\nto handle high dimensional continuous state and action spaces in the\nconventional RL framework, we introduce meta policy with a new concept Plan to\nthe standard Actor-Critic model, which is of a lower dimension than the\noriginal image and can facilitate the actor to generate a tractable high\ndimensional action. In the RL-I2IT framework, we also employ a task-specific\nauxiliary learning strategy to stabilize the training process and improve the\nperformance of the corresponding task. Experiments on several I2IT tasks\ndemonstrate the effectiveness and robustness of the proposed method when facing\nhigh-dimensional continuous action space problems. Our implementation of the\nRL-I2IT framework is available at\nhttps://github.com/Algolzw/SPAC-Deformable-Registration.\n", "link": "http://arxiv.org/abs/2309.13672v8", "date": "2025-10-29", "relevancy": 2.1935, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5558}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5504}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RL-I2IT%3A%20Image-to-Image%20Translation%20with%20Deep%20Reinforcement%20Learning&body=Title%3A%20RL-I2IT%3A%20Image-to-Image%20Translation%20with%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Jing%20Hu%20and%20Chengming%20Feng%20and%20Shu%20Hu%20and%20Ming-Ching%20Chang%20and%20Xin%20Li%20and%20Xi%20Wu%20and%20Xin%20Wang%0AAbstract%3A%20%20%20Most%20existing%20Image-to-Image%20Translation%20%28I2IT%29%20methods%20generate%20images%20in%20a%0Asingle%20run%20of%20a%20deep%20learning%20%28DL%29%20model.%20However%2C%20designing%20such%20a%20single-step%0Amodel%20is%20always%20challenging%2C%20requiring%20a%20huge%20number%20of%20parameters%20and%20easily%0Afalling%20into%20bad%20global%20minimums%20and%20overfitting.%20In%20this%20work%2C%20we%20reformulate%0AI2IT%20as%20a%20step-wise%20decision-making%20problem%20via%20deep%20reinforcement%20learning%0A%28DRL%29%20and%20propose%20a%20novel%20framework%20that%20performs%20RL-based%20I2IT%20%28RL-I2IT%29.%20The%0Akey%20feature%20in%20the%20RL-I2IT%20framework%20is%20to%20decompose%20a%20monolithic%20learning%0Aprocess%20into%20small%20steps%20with%20a%20lightweight%20model%20to%20progressively%20transform%20a%0Asource%20image%20successively%20to%20a%20target%20image.%20Considering%20that%20it%20is%20challenging%0Ato%20handle%20high%20dimensional%20continuous%20state%20and%20action%20spaces%20in%20the%0Aconventional%20RL%20framework%2C%20we%20introduce%20meta%20policy%20with%20a%20new%20concept%20Plan%20to%0Athe%20standard%20Actor-Critic%20model%2C%20which%20is%20of%20a%20lower%20dimension%20than%20the%0Aoriginal%20image%20and%20can%20facilitate%20the%20actor%20to%20generate%20a%20tractable%20high%0Adimensional%20action.%20In%20the%20RL-I2IT%20framework%2C%20we%20also%20employ%20a%20task-specific%0Aauxiliary%20learning%20strategy%20to%20stabilize%20the%20training%20process%20and%20improve%20the%0Aperformance%20of%20the%20corresponding%20task.%20Experiments%20on%20several%20I2IT%20tasks%0Ademonstrate%20the%20effectiveness%20and%20robustness%20of%20the%20proposed%20method%20when%20facing%0Ahigh-dimensional%20continuous%20action%20space%20problems.%20Our%20implementation%20of%20the%0ARL-I2IT%20framework%20is%20available%20at%0Ahttps%3A//github.com/Algolzw/SPAC-Deformable-Registration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.13672v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRL-I2IT%253A%2520Image-to-Image%2520Translation%2520with%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DJing%2520Hu%2520and%2520Chengming%2520Feng%2520and%2520Shu%2520Hu%2520and%2520Ming-Ching%2520Chang%2520and%2520Xin%2520Li%2520and%2520Xi%2520Wu%2520and%2520Xin%2520Wang%26entry.1292438233%3D%2520%2520Most%2520existing%2520Image-to-Image%2520Translation%2520%2528I2IT%2529%2520methods%2520generate%2520images%2520in%2520a%250Asingle%2520run%2520of%2520a%2520deep%2520learning%2520%2528DL%2529%2520model.%2520However%252C%2520designing%2520such%2520a%2520single-step%250Amodel%2520is%2520always%2520challenging%252C%2520requiring%2520a%2520huge%2520number%2520of%2520parameters%2520and%2520easily%250Afalling%2520into%2520bad%2520global%2520minimums%2520and%2520overfitting.%2520In%2520this%2520work%252C%2520we%2520reformulate%250AI2IT%2520as%2520a%2520step-wise%2520decision-making%2520problem%2520via%2520deep%2520reinforcement%2520learning%250A%2528DRL%2529%2520and%2520propose%2520a%2520novel%2520framework%2520that%2520performs%2520RL-based%2520I2IT%2520%2528RL-I2IT%2529.%2520The%250Akey%2520feature%2520in%2520the%2520RL-I2IT%2520framework%2520is%2520to%2520decompose%2520a%2520monolithic%2520learning%250Aprocess%2520into%2520small%2520steps%2520with%2520a%2520lightweight%2520model%2520to%2520progressively%2520transform%2520a%250Asource%2520image%2520successively%2520to%2520a%2520target%2520image.%2520Considering%2520that%2520it%2520is%2520challenging%250Ato%2520handle%2520high%2520dimensional%2520continuous%2520state%2520and%2520action%2520spaces%2520in%2520the%250Aconventional%2520RL%2520framework%252C%2520we%2520introduce%2520meta%2520policy%2520with%2520a%2520new%2520concept%2520Plan%2520to%250Athe%2520standard%2520Actor-Critic%2520model%252C%2520which%2520is%2520of%2520a%2520lower%2520dimension%2520than%2520the%250Aoriginal%2520image%2520and%2520can%2520facilitate%2520the%2520actor%2520to%2520generate%2520a%2520tractable%2520high%250Adimensional%2520action.%2520In%2520the%2520RL-I2IT%2520framework%252C%2520we%2520also%2520employ%2520a%2520task-specific%250Aauxiliary%2520learning%2520strategy%2520to%2520stabilize%2520the%2520training%2520process%2520and%2520improve%2520the%250Aperformance%2520of%2520the%2520corresponding%2520task.%2520Experiments%2520on%2520several%2520I2IT%2520tasks%250Ademonstrate%2520the%2520effectiveness%2520and%2520robustness%2520of%2520the%2520proposed%2520method%2520when%2520facing%250Ahigh-dimensional%2520continuous%2520action%2520space%2520problems.%2520Our%2520implementation%2520of%2520the%250ARL-I2IT%2520framework%2520is%2520available%2520at%250Ahttps%253A//github.com/Algolzw/SPAC-Deformable-Registration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.13672v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RL-I2IT%3A%20Image-to-Image%20Translation%20with%20Deep%20Reinforcement%20Learning&entry.906535625=Jing%20Hu%20and%20Chengming%20Feng%20and%20Shu%20Hu%20and%20Ming-Ching%20Chang%20and%20Xin%20Li%20and%20Xi%20Wu%20and%20Xin%20Wang&entry.1292438233=%20%20Most%20existing%20Image-to-Image%20Translation%20%28I2IT%29%20methods%20generate%20images%20in%20a%0Asingle%20run%20of%20a%20deep%20learning%20%28DL%29%20model.%20However%2C%20designing%20such%20a%20single-step%0Amodel%20is%20always%20challenging%2C%20requiring%20a%20huge%20number%20of%20parameters%20and%20easily%0Afalling%20into%20bad%20global%20minimums%20and%20overfitting.%20In%20this%20work%2C%20we%20reformulate%0AI2IT%20as%20a%20step-wise%20decision-making%20problem%20via%20deep%20reinforcement%20learning%0A%28DRL%29%20and%20propose%20a%20novel%20framework%20that%20performs%20RL-based%20I2IT%20%28RL-I2IT%29.%20The%0Akey%20feature%20in%20the%20RL-I2IT%20framework%20is%20to%20decompose%20a%20monolithic%20learning%0Aprocess%20into%20small%20steps%20with%20a%20lightweight%20model%20to%20progressively%20transform%20a%0Asource%20image%20successively%20to%20a%20target%20image.%20Considering%20that%20it%20is%20challenging%0Ato%20handle%20high%20dimensional%20continuous%20state%20and%20action%20spaces%20in%20the%0Aconventional%20RL%20framework%2C%20we%20introduce%20meta%20policy%20with%20a%20new%20concept%20Plan%20to%0Athe%20standard%20Actor-Critic%20model%2C%20which%20is%20of%20a%20lower%20dimension%20than%20the%0Aoriginal%20image%20and%20can%20facilitate%20the%20actor%20to%20generate%20a%20tractable%20high%0Adimensional%20action.%20In%20the%20RL-I2IT%20framework%2C%20we%20also%20employ%20a%20task-specific%0Aauxiliary%20learning%20strategy%20to%20stabilize%20the%20training%20process%20and%20improve%20the%0Aperformance%20of%20the%20corresponding%20task.%20Experiments%20on%20several%20I2IT%20tasks%0Ademonstrate%20the%20effectiveness%20and%20robustness%20of%20the%20proposed%20method%20when%20facing%0Ahigh-dimensional%20continuous%20action%20space%20problems.%20Our%20implementation%20of%20the%0ARL-I2IT%20framework%20is%20available%20at%0Ahttps%3A//github.com/Algolzw/SPAC-Deformable-Registration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.13672v8&entry.124074799=Read"},
{"title": "Gaperon: A Peppered English-French Generative Language Model Suite", "author": "Nathan Godey and Wissam Antoun and Rian Touchent and Rachel Bawden and \u00c9ric de la Clergerie and Beno\u00eet Sagot and Djam\u00e9 Seddah", "abstract": "  We release Gaperon, a fully open suite of French-English-coding language\nmodels designed to advance transparency and reproducibility in large-scale\nmodel training. The Gaperon family includes 1.5B, 8B, and 24B parameter models\ntrained on 2-4 trillion tokens, released with all elements of the training\npipeline: French and English datasets filtered with a neural quality\nclassifier, an efficient data curation and training framework, and hundreds of\nintermediate checkpoints. Through this work, we study how data filtering and\ncontamination interact to shape both benchmark and generative performance. We\nfind that filtering for linguistic quality enhances text fluency and coherence\nbut yields subpar benchmark results, and that late deliberate contamination --\ncontinuing training on data mixes that include test sets -- recovers\ncompetitive scores while only reasonably harming generation quality. We discuss\nhow usual neural filtering can unintentionally amplify benchmark leakage. To\nsupport further research, we also introduce harmless data poisoning during\npretraining, providing a realistic testbed for safety studies. By openly\nreleasing all models, datasets, code, and checkpoints, Gaperon establishes a\nreproducible foundation for exploring the trade-offs between data curation,\nevaluation, safety, and openness in multilingual language model development.\n", "link": "http://arxiv.org/abs/2510.25771v1", "date": "2025-10-29", "relevancy": 2.1877, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5699}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5384}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaperon%3A%20A%20Peppered%20English-French%20Generative%20Language%20Model%20Suite&body=Title%3A%20Gaperon%3A%20A%20Peppered%20English-French%20Generative%20Language%20Model%20Suite%0AAuthor%3A%20Nathan%20Godey%20and%20Wissam%20Antoun%20and%20Rian%20Touchent%20and%20Rachel%20Bawden%20and%20%C3%89ric%20de%20la%20Clergerie%20and%20Beno%C3%AEt%20Sagot%20and%20Djam%C3%A9%20Seddah%0AAbstract%3A%20%20%20We%20release%20Gaperon%2C%20a%20fully%20open%20suite%20of%20French-English-coding%20language%0Amodels%20designed%20to%20advance%20transparency%20and%20reproducibility%20in%20large-scale%0Amodel%20training.%20The%20Gaperon%20family%20includes%201.5B%2C%208B%2C%20and%2024B%20parameter%20models%0Atrained%20on%202-4%20trillion%20tokens%2C%20released%20with%20all%20elements%20of%20the%20training%0Apipeline%3A%20French%20and%20English%20datasets%20filtered%20with%20a%20neural%20quality%0Aclassifier%2C%20an%20efficient%20data%20curation%20and%20training%20framework%2C%20and%20hundreds%20of%0Aintermediate%20checkpoints.%20Through%20this%20work%2C%20we%20study%20how%20data%20filtering%20and%0Acontamination%20interact%20to%20shape%20both%20benchmark%20and%20generative%20performance.%20We%0Afind%20that%20filtering%20for%20linguistic%20quality%20enhances%20text%20fluency%20and%20coherence%0Abut%20yields%20subpar%20benchmark%20results%2C%20and%20that%20late%20deliberate%20contamination%20--%0Acontinuing%20training%20on%20data%20mixes%20that%20include%20test%20sets%20--%20recovers%0Acompetitive%20scores%20while%20only%20reasonably%20harming%20generation%20quality.%20We%20discuss%0Ahow%20usual%20neural%20filtering%20can%20unintentionally%20amplify%20benchmark%20leakage.%20To%0Asupport%20further%20research%2C%20we%20also%20introduce%20harmless%20data%20poisoning%20during%0Apretraining%2C%20providing%20a%20realistic%20testbed%20for%20safety%20studies.%20By%20openly%0Areleasing%20all%20models%2C%20datasets%2C%20code%2C%20and%20checkpoints%2C%20Gaperon%20establishes%20a%0Areproducible%20foundation%20for%20exploring%20the%20trade-offs%20between%20data%20curation%2C%0Aevaluation%2C%20safety%2C%20and%20openness%20in%20multilingual%20language%20model%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaperon%253A%2520A%2520Peppered%2520English-French%2520Generative%2520Language%2520Model%2520Suite%26entry.906535625%3DNathan%2520Godey%2520and%2520Wissam%2520Antoun%2520and%2520Rian%2520Touchent%2520and%2520Rachel%2520Bawden%2520and%2520%25C3%2589ric%2520de%2520la%2520Clergerie%2520and%2520Beno%25C3%25AEt%2520Sagot%2520and%2520Djam%25C3%25A9%2520Seddah%26entry.1292438233%3D%2520%2520We%2520release%2520Gaperon%252C%2520a%2520fully%2520open%2520suite%2520of%2520French-English-coding%2520language%250Amodels%2520designed%2520to%2520advance%2520transparency%2520and%2520reproducibility%2520in%2520large-scale%250Amodel%2520training.%2520The%2520Gaperon%2520family%2520includes%25201.5B%252C%25208B%252C%2520and%252024B%2520parameter%2520models%250Atrained%2520on%25202-4%2520trillion%2520tokens%252C%2520released%2520with%2520all%2520elements%2520of%2520the%2520training%250Apipeline%253A%2520French%2520and%2520English%2520datasets%2520filtered%2520with%2520a%2520neural%2520quality%250Aclassifier%252C%2520an%2520efficient%2520data%2520curation%2520and%2520training%2520framework%252C%2520and%2520hundreds%2520of%250Aintermediate%2520checkpoints.%2520Through%2520this%2520work%252C%2520we%2520study%2520how%2520data%2520filtering%2520and%250Acontamination%2520interact%2520to%2520shape%2520both%2520benchmark%2520and%2520generative%2520performance.%2520We%250Afind%2520that%2520filtering%2520for%2520linguistic%2520quality%2520enhances%2520text%2520fluency%2520and%2520coherence%250Abut%2520yields%2520subpar%2520benchmark%2520results%252C%2520and%2520that%2520late%2520deliberate%2520contamination%2520--%250Acontinuing%2520training%2520on%2520data%2520mixes%2520that%2520include%2520test%2520sets%2520--%2520recovers%250Acompetitive%2520scores%2520while%2520only%2520reasonably%2520harming%2520generation%2520quality.%2520We%2520discuss%250Ahow%2520usual%2520neural%2520filtering%2520can%2520unintentionally%2520amplify%2520benchmark%2520leakage.%2520To%250Asupport%2520further%2520research%252C%2520we%2520also%2520introduce%2520harmless%2520data%2520poisoning%2520during%250Apretraining%252C%2520providing%2520a%2520realistic%2520testbed%2520for%2520safety%2520studies.%2520By%2520openly%250Areleasing%2520all%2520models%252C%2520datasets%252C%2520code%252C%2520and%2520checkpoints%252C%2520Gaperon%2520establishes%2520a%250Areproducible%2520foundation%2520for%2520exploring%2520the%2520trade-offs%2520between%2520data%2520curation%252C%250Aevaluation%252C%2520safety%252C%2520and%2520openness%2520in%2520multilingual%2520language%2520model%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaperon%3A%20A%20Peppered%20English-French%20Generative%20Language%20Model%20Suite&entry.906535625=Nathan%20Godey%20and%20Wissam%20Antoun%20and%20Rian%20Touchent%20and%20Rachel%20Bawden%20and%20%C3%89ric%20de%20la%20Clergerie%20and%20Beno%C3%AEt%20Sagot%20and%20Djam%C3%A9%20Seddah&entry.1292438233=%20%20We%20release%20Gaperon%2C%20a%20fully%20open%20suite%20of%20French-English-coding%20language%0Amodels%20designed%20to%20advance%20transparency%20and%20reproducibility%20in%20large-scale%0Amodel%20training.%20The%20Gaperon%20family%20includes%201.5B%2C%208B%2C%20and%2024B%20parameter%20models%0Atrained%20on%202-4%20trillion%20tokens%2C%20released%20with%20all%20elements%20of%20the%20training%0Apipeline%3A%20French%20and%20English%20datasets%20filtered%20with%20a%20neural%20quality%0Aclassifier%2C%20an%20efficient%20data%20curation%20and%20training%20framework%2C%20and%20hundreds%20of%0Aintermediate%20checkpoints.%20Through%20this%20work%2C%20we%20study%20how%20data%20filtering%20and%0Acontamination%20interact%20to%20shape%20both%20benchmark%20and%20generative%20performance.%20We%0Afind%20that%20filtering%20for%20linguistic%20quality%20enhances%20text%20fluency%20and%20coherence%0Abut%20yields%20subpar%20benchmark%20results%2C%20and%20that%20late%20deliberate%20contamination%20--%0Acontinuing%20training%20on%20data%20mixes%20that%20include%20test%20sets%20--%20recovers%0Acompetitive%20scores%20while%20only%20reasonably%20harming%20generation%20quality.%20We%20discuss%0Ahow%20usual%20neural%20filtering%20can%20unintentionally%20amplify%20benchmark%20leakage.%20To%0Asupport%20further%20research%2C%20we%20also%20introduce%20harmless%20data%20poisoning%20during%0Apretraining%2C%20providing%20a%20realistic%20testbed%20for%20safety%20studies.%20By%20openly%0Areleasing%20all%20models%2C%20datasets%2C%20code%2C%20and%20checkpoints%2C%20Gaperon%20establishes%20a%0Areproducible%20foundation%20for%20exploring%20the%20trade-offs%20between%20data%20curation%2C%0Aevaluation%2C%20safety%2C%20and%20openness%20in%20multilingual%20language%20model%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25771v1&entry.124074799=Read"},
{"title": "Decom-Renorm-Merge: Model Merging on the Right Space Improves\n  Multitasking", "author": "Yuatyong Chaichana and Thanapat Trachu and Peerat Limkonchotiwat and Konpat Preechakul and Tirasan Khandhawit and Ekapol Chuangsuwanich", "abstract": "  In the era of large-scale training, model merging has evolved into a tool for\ncreating multitasking models efficiently. It enables the knowledge of models to\nbe fused, without the need for heavy computation as required in traditional\nmultitask learning. Existing merging methods often assume that entries at\nidentical positions in weight matrices serve the same function, enabling\nstraightforward entry-wise comparison and merging. However, this assumption\noverlooks the complexity of finetuned neural networks, where neurons may\ndevelop distinct feature compositions, making direct entry-wise merging\nproblematic. We present Decom-Renorm-Merge (DRM), a simple yet effective\napproach that leverages Singular Value Decomposition to decompose and\ncoordinate weight matrices into an aligned joint space, where entry-wise\nmerging becomes possible. We showcase the effectiveness of DRM across various\nsettings ranging from smaller encoder-based such as ViT and DeBERTa,\nencoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.\nOur experimental results show that DRM outperforms several state-of-the-art\nmerging techniques across full finetuning and low-rank adaptation settings.\nMoreover, our analysis reveals renormalization as the crucial component for\ncreating a robust and even joint space for merging, significantly contributing\nto the method's performance.\n", "link": "http://arxiv.org/abs/2505.23117v2", "date": "2025-10-29", "relevancy": 2.1874, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5652}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5394}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decom-Renorm-Merge%3A%20Model%20Merging%20on%20the%20Right%20Space%20Improves%0A%20%20Multitasking&body=Title%3A%20Decom-Renorm-Merge%3A%20Model%20Merging%20on%20the%20Right%20Space%20Improves%0A%20%20Multitasking%0AAuthor%3A%20Yuatyong%20Chaichana%20and%20Thanapat%20Trachu%20and%20Peerat%20Limkonchotiwat%20and%20Konpat%20Preechakul%20and%20Tirasan%20Khandhawit%20and%20Ekapol%20Chuangsuwanich%0AAbstract%3A%20%20%20In%20the%20era%20of%20large-scale%20training%2C%20model%20merging%20has%20evolved%20into%20a%20tool%20for%0Acreating%20multitasking%20models%20efficiently.%20It%20enables%20the%20knowledge%20of%20models%20to%0Abe%20fused%2C%20without%20the%20need%20for%20heavy%20computation%20as%20required%20in%20traditional%0Amultitask%20learning.%20Existing%20merging%20methods%20often%20assume%20that%20entries%20at%0Aidentical%20positions%20in%20weight%20matrices%20serve%20the%20same%20function%2C%20enabling%0Astraightforward%20entry-wise%20comparison%20and%20merging.%20However%2C%20this%20assumption%0Aoverlooks%20the%20complexity%20of%20finetuned%20neural%20networks%2C%20where%20neurons%20may%0Adevelop%20distinct%20feature%20compositions%2C%20making%20direct%20entry-wise%20merging%0Aproblematic.%20We%20present%20Decom-Renorm-Merge%20%28DRM%29%2C%20a%20simple%20yet%20effective%0Aapproach%20that%20leverages%20Singular%20Value%20Decomposition%20to%20decompose%20and%0Acoordinate%20weight%20matrices%20into%20an%20aligned%20joint%20space%2C%20where%20entry-wise%0Amerging%20becomes%20possible.%20We%20showcase%20the%20effectiveness%20of%20DRM%20across%20various%0Asettings%20ranging%20from%20smaller%20encoder-based%20such%20as%20ViT%20and%20DeBERTa%2C%0Aencoder-decoder-based%20such%20as%20T5%2C%20and%20larger%20decoder-based%20such%20as%20Llama3.1-8B.%0AOur%20experimental%20results%20show%20that%20DRM%20outperforms%20several%20state-of-the-art%0Amerging%20techniques%20across%20full%20finetuning%20and%20low-rank%20adaptation%20settings.%0AMoreover%2C%20our%20analysis%20reveals%20renormalization%20as%20the%20crucial%20component%20for%0Acreating%20a%20robust%20and%20even%20joint%20space%20for%20merging%2C%20significantly%20contributing%0Ato%20the%20method%27s%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23117v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecom-Renorm-Merge%253A%2520Model%2520Merging%2520on%2520the%2520Right%2520Space%2520Improves%250A%2520%2520Multitasking%26entry.906535625%3DYuatyong%2520Chaichana%2520and%2520Thanapat%2520Trachu%2520and%2520Peerat%2520Limkonchotiwat%2520and%2520Konpat%2520Preechakul%2520and%2520Tirasan%2520Khandhawit%2520and%2520Ekapol%2520Chuangsuwanich%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520large-scale%2520training%252C%2520model%2520merging%2520has%2520evolved%2520into%2520a%2520tool%2520for%250Acreating%2520multitasking%2520models%2520efficiently.%2520It%2520enables%2520the%2520knowledge%2520of%2520models%2520to%250Abe%2520fused%252C%2520without%2520the%2520need%2520for%2520heavy%2520computation%2520as%2520required%2520in%2520traditional%250Amultitask%2520learning.%2520Existing%2520merging%2520methods%2520often%2520assume%2520that%2520entries%2520at%250Aidentical%2520positions%2520in%2520weight%2520matrices%2520serve%2520the%2520same%2520function%252C%2520enabling%250Astraightforward%2520entry-wise%2520comparison%2520and%2520merging.%2520However%252C%2520this%2520assumption%250Aoverlooks%2520the%2520complexity%2520of%2520finetuned%2520neural%2520networks%252C%2520where%2520neurons%2520may%250Adevelop%2520distinct%2520feature%2520compositions%252C%2520making%2520direct%2520entry-wise%2520merging%250Aproblematic.%2520We%2520present%2520Decom-Renorm-Merge%2520%2528DRM%2529%252C%2520a%2520simple%2520yet%2520effective%250Aapproach%2520that%2520leverages%2520Singular%2520Value%2520Decomposition%2520to%2520decompose%2520and%250Acoordinate%2520weight%2520matrices%2520into%2520an%2520aligned%2520joint%2520space%252C%2520where%2520entry-wise%250Amerging%2520becomes%2520possible.%2520We%2520showcase%2520the%2520effectiveness%2520of%2520DRM%2520across%2520various%250Asettings%2520ranging%2520from%2520smaller%2520encoder-based%2520such%2520as%2520ViT%2520and%2520DeBERTa%252C%250Aencoder-decoder-based%2520such%2520as%2520T5%252C%2520and%2520larger%2520decoder-based%2520such%2520as%2520Llama3.1-8B.%250AOur%2520experimental%2520results%2520show%2520that%2520DRM%2520outperforms%2520several%2520state-of-the-art%250Amerging%2520techniques%2520across%2520full%2520finetuning%2520and%2520low-rank%2520adaptation%2520settings.%250AMoreover%252C%2520our%2520analysis%2520reveals%2520renormalization%2520as%2520the%2520crucial%2520component%2520for%250Acreating%2520a%2520robust%2520and%2520even%2520joint%2520space%2520for%2520merging%252C%2520significantly%2520contributing%250Ato%2520the%2520method%2527s%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23117v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decom-Renorm-Merge%3A%20Model%20Merging%20on%20the%20Right%20Space%20Improves%0A%20%20Multitasking&entry.906535625=Yuatyong%20Chaichana%20and%20Thanapat%20Trachu%20and%20Peerat%20Limkonchotiwat%20and%20Konpat%20Preechakul%20and%20Tirasan%20Khandhawit%20and%20Ekapol%20Chuangsuwanich&entry.1292438233=%20%20In%20the%20era%20of%20large-scale%20training%2C%20model%20merging%20has%20evolved%20into%20a%20tool%20for%0Acreating%20multitasking%20models%20efficiently.%20It%20enables%20the%20knowledge%20of%20models%20to%0Abe%20fused%2C%20without%20the%20need%20for%20heavy%20computation%20as%20required%20in%20traditional%0Amultitask%20learning.%20Existing%20merging%20methods%20often%20assume%20that%20entries%20at%0Aidentical%20positions%20in%20weight%20matrices%20serve%20the%20same%20function%2C%20enabling%0Astraightforward%20entry-wise%20comparison%20and%20merging.%20However%2C%20this%20assumption%0Aoverlooks%20the%20complexity%20of%20finetuned%20neural%20networks%2C%20where%20neurons%20may%0Adevelop%20distinct%20feature%20compositions%2C%20making%20direct%20entry-wise%20merging%0Aproblematic.%20We%20present%20Decom-Renorm-Merge%20%28DRM%29%2C%20a%20simple%20yet%20effective%0Aapproach%20that%20leverages%20Singular%20Value%20Decomposition%20to%20decompose%20and%0Acoordinate%20weight%20matrices%20into%20an%20aligned%20joint%20space%2C%20where%20entry-wise%0Amerging%20becomes%20possible.%20We%20showcase%20the%20effectiveness%20of%20DRM%20across%20various%0Asettings%20ranging%20from%20smaller%20encoder-based%20such%20as%20ViT%20and%20DeBERTa%2C%0Aencoder-decoder-based%20such%20as%20T5%2C%20and%20larger%20decoder-based%20such%20as%20Llama3.1-8B.%0AOur%20experimental%20results%20show%20that%20DRM%20outperforms%20several%20state-of-the-art%0Amerging%20techniques%20across%20full%20finetuning%20and%20low-rank%20adaptation%20settings.%0AMoreover%2C%20our%20analysis%20reveals%20renormalization%20as%20the%20crucial%20component%20for%0Acreating%20a%20robust%20and%20even%20joint%20space%20for%20merging%2C%20significantly%20contributing%0Ato%20the%20method%27s%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23117v2&entry.124074799=Read"},
{"title": "Simulating Automotive Radar with Lidar and Camera Inputs", "author": "Peili Song and Dezhen Song and Yifan Yang and Enfan Lan and Jingtai Liu", "abstract": "  Low-cost millimeter automotive radar has received more and more attention due\nto its ability to handle adverse weather and lighting conditions in autonomous\ndriving. However, the lack of quality datasets hinders research and\ndevelopment. We report a new method that is able to simulate 4D millimeter wave\nradar signals including pitch, yaw, range, and Doppler velocity along with\nradar signal strength (RSS) using camera image, light detection and ranging\n(lidar) point cloud, and ego-velocity. The method is based on two new neural\nnetworks: 1) DIS-Net, which estimates the spatial distribution and number of\nradar signals, and 2) RSS-Net, which predicts the RSS of the signal based on\nappearance and geometric information. We have implemented and tested our method\nusing open datasets from 3 different models of commercial automotive radar. The\nexperimental results show that our method can successfully generate\nhigh-fidelity radar signals. Moreover, we have trained a popular object\ndetection neural network with data augmented by our synthesized radar. The\nnetwork outperforms the counterpart trained only on raw radar data, a promising\nresult to facilitate future radar-based research and development.\n", "link": "http://arxiv.org/abs/2503.08068v2", "date": "2025-10-29", "relevancy": 2.1834, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5569}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simulating%20Automotive%20Radar%20with%20Lidar%20and%20Camera%20Inputs&body=Title%3A%20Simulating%20Automotive%20Radar%20with%20Lidar%20and%20Camera%20Inputs%0AAuthor%3A%20Peili%20Song%20and%20Dezhen%20Song%20and%20Yifan%20Yang%20and%20Enfan%20Lan%20and%20Jingtai%20Liu%0AAbstract%3A%20%20%20Low-cost%20millimeter%20automotive%20radar%20has%20received%20more%20and%20more%20attention%20due%0Ato%20its%20ability%20to%20handle%20adverse%20weather%20and%20lighting%20conditions%20in%20autonomous%0Adriving.%20However%2C%20the%20lack%20of%20quality%20datasets%20hinders%20research%20and%0Adevelopment.%20We%20report%20a%20new%20method%20that%20is%20able%20to%20simulate%204D%20millimeter%20wave%0Aradar%20signals%20including%20pitch%2C%20yaw%2C%20range%2C%20and%20Doppler%20velocity%20along%20with%0Aradar%20signal%20strength%20%28RSS%29%20using%20camera%20image%2C%20light%20detection%20and%20ranging%0A%28lidar%29%20point%20cloud%2C%20and%20ego-velocity.%20The%20method%20is%20based%20on%20two%20new%20neural%0Anetworks%3A%201%29%20DIS-Net%2C%20which%20estimates%20the%20spatial%20distribution%20and%20number%20of%0Aradar%20signals%2C%20and%202%29%20RSS-Net%2C%20which%20predicts%20the%20RSS%20of%20the%20signal%20based%20on%0Aappearance%20and%20geometric%20information.%20We%20have%20implemented%20and%20tested%20our%20method%0Ausing%20open%20datasets%20from%203%20different%20models%20of%20commercial%20automotive%20radar.%20The%0Aexperimental%20results%20show%20that%20our%20method%20can%20successfully%20generate%0Ahigh-fidelity%20radar%20signals.%20Moreover%2C%20we%20have%20trained%20a%20popular%20object%0Adetection%20neural%20network%20with%20data%20augmented%20by%20our%20synthesized%20radar.%20The%0Anetwork%20outperforms%20the%20counterpart%20trained%20only%20on%20raw%20radar%20data%2C%20a%20promising%0Aresult%20to%20facilitate%20future%20radar-based%20research%20and%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08068v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimulating%2520Automotive%2520Radar%2520with%2520Lidar%2520and%2520Camera%2520Inputs%26entry.906535625%3DPeili%2520Song%2520and%2520Dezhen%2520Song%2520and%2520Yifan%2520Yang%2520and%2520Enfan%2520Lan%2520and%2520Jingtai%2520Liu%26entry.1292438233%3D%2520%2520Low-cost%2520millimeter%2520automotive%2520radar%2520has%2520received%2520more%2520and%2520more%2520attention%2520due%250Ato%2520its%2520ability%2520to%2520handle%2520adverse%2520weather%2520and%2520lighting%2520conditions%2520in%2520autonomous%250Adriving.%2520However%252C%2520the%2520lack%2520of%2520quality%2520datasets%2520hinders%2520research%2520and%250Adevelopment.%2520We%2520report%2520a%2520new%2520method%2520that%2520is%2520able%2520to%2520simulate%25204D%2520millimeter%2520wave%250Aradar%2520signals%2520including%2520pitch%252C%2520yaw%252C%2520range%252C%2520and%2520Doppler%2520velocity%2520along%2520with%250Aradar%2520signal%2520strength%2520%2528RSS%2529%2520using%2520camera%2520image%252C%2520light%2520detection%2520and%2520ranging%250A%2528lidar%2529%2520point%2520cloud%252C%2520and%2520ego-velocity.%2520The%2520method%2520is%2520based%2520on%2520two%2520new%2520neural%250Anetworks%253A%25201%2529%2520DIS-Net%252C%2520which%2520estimates%2520the%2520spatial%2520distribution%2520and%2520number%2520of%250Aradar%2520signals%252C%2520and%25202%2529%2520RSS-Net%252C%2520which%2520predicts%2520the%2520RSS%2520of%2520the%2520signal%2520based%2520on%250Aappearance%2520and%2520geometric%2520information.%2520We%2520have%2520implemented%2520and%2520tested%2520our%2520method%250Ausing%2520open%2520datasets%2520from%25203%2520different%2520models%2520of%2520commercial%2520automotive%2520radar.%2520The%250Aexperimental%2520results%2520show%2520that%2520our%2520method%2520can%2520successfully%2520generate%250Ahigh-fidelity%2520radar%2520signals.%2520Moreover%252C%2520we%2520have%2520trained%2520a%2520popular%2520object%250Adetection%2520neural%2520network%2520with%2520data%2520augmented%2520by%2520our%2520synthesized%2520radar.%2520The%250Anetwork%2520outperforms%2520the%2520counterpart%2520trained%2520only%2520on%2520raw%2520radar%2520data%252C%2520a%2520promising%250Aresult%2520to%2520facilitate%2520future%2520radar-based%2520research%2520and%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08068v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simulating%20Automotive%20Radar%20with%20Lidar%20and%20Camera%20Inputs&entry.906535625=Peili%20Song%20and%20Dezhen%20Song%20and%20Yifan%20Yang%20and%20Enfan%20Lan%20and%20Jingtai%20Liu&entry.1292438233=%20%20Low-cost%20millimeter%20automotive%20radar%20has%20received%20more%20and%20more%20attention%20due%0Ato%20its%20ability%20to%20handle%20adverse%20weather%20and%20lighting%20conditions%20in%20autonomous%0Adriving.%20However%2C%20the%20lack%20of%20quality%20datasets%20hinders%20research%20and%0Adevelopment.%20We%20report%20a%20new%20method%20that%20is%20able%20to%20simulate%204D%20millimeter%20wave%0Aradar%20signals%20including%20pitch%2C%20yaw%2C%20range%2C%20and%20Doppler%20velocity%20along%20with%0Aradar%20signal%20strength%20%28RSS%29%20using%20camera%20image%2C%20light%20detection%20and%20ranging%0A%28lidar%29%20point%20cloud%2C%20and%20ego-velocity.%20The%20method%20is%20based%20on%20two%20new%20neural%0Anetworks%3A%201%29%20DIS-Net%2C%20which%20estimates%20the%20spatial%20distribution%20and%20number%20of%0Aradar%20signals%2C%20and%202%29%20RSS-Net%2C%20which%20predicts%20the%20RSS%20of%20the%20signal%20based%20on%0Aappearance%20and%20geometric%20information.%20We%20have%20implemented%20and%20tested%20our%20method%0Ausing%20open%20datasets%20from%203%20different%20models%20of%20commercial%20automotive%20radar.%20The%0Aexperimental%20results%20show%20that%20our%20method%20can%20successfully%20generate%0Ahigh-fidelity%20radar%20signals.%20Moreover%2C%20we%20have%20trained%20a%20popular%20object%0Adetection%20neural%20network%20with%20data%20augmented%20by%20our%20synthesized%20radar.%20The%0Anetwork%20outperforms%20the%20counterpart%20trained%20only%20on%20raw%20radar%20data%2C%20a%20promising%0Aresult%20to%20facilitate%20future%20radar-based%20research%20and%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08068v2&entry.124074799=Read"},
{"title": "Using VLM Reasoning to Constrain Task and Motion Planning", "author": "Muyang Yan and Miras Mengdibayev and Ardon Floros and Weihang Guo and Lydia E. Kavraki and Zachary Kingston", "abstract": "  In task and motion planning, high-level task planning is done over an\nabstraction of the world to enable efficient search in long-horizon robotics\nproblems. However, the feasibility of these task-level plans relies on the\ndownward refinability of the abstraction into continuous motion. When a\ndomain's refinability is poor, task-level plans that appear valid may\nultimately fail during motion planning, requiring replanning and resulting in\nslower overall performance. Prior works mitigate this by encoding refinement\nissues as constraints to prune infeasible task plans. However, these approaches\nonly add constraints upon refinement failure, expending significant search\neffort on infeasible branches. We propose VIZ-COAST, a method of leveraging the\ncommon-sense spatial reasoning of large pretrained Vision-Language Models to\nidentify issues with downward refinement a priori, bypassing the need to fix\nthese failures during planning. Experiments on two challenging TAMP domains\nshow that our approach is able to extract plausible constraints from images and\ndomain descriptions, drastically reducing planning times and, in some cases,\neliminating downward refinement failures altogether, generalizing to a diverse\nrange of instances from the broader domain.\n", "link": "http://arxiv.org/abs/2510.25548v1", "date": "2025-10-29", "relevancy": 2.1831, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20VLM%20Reasoning%20to%20Constrain%20Task%20and%20Motion%20Planning&body=Title%3A%20Using%20VLM%20Reasoning%20to%20Constrain%20Task%20and%20Motion%20Planning%0AAuthor%3A%20Muyang%20Yan%20and%20Miras%20Mengdibayev%20and%20Ardon%20Floros%20and%20Weihang%20Guo%20and%20Lydia%20E.%20Kavraki%20and%20Zachary%20Kingston%0AAbstract%3A%20%20%20In%20task%20and%20motion%20planning%2C%20high-level%20task%20planning%20is%20done%20over%20an%0Aabstraction%20of%20the%20world%20to%20enable%20efficient%20search%20in%20long-horizon%20robotics%0Aproblems.%20However%2C%20the%20feasibility%20of%20these%20task-level%20plans%20relies%20on%20the%0Adownward%20refinability%20of%20the%20abstraction%20into%20continuous%20motion.%20When%20a%0Adomain%27s%20refinability%20is%20poor%2C%20task-level%20plans%20that%20appear%20valid%20may%0Aultimately%20fail%20during%20motion%20planning%2C%20requiring%20replanning%20and%20resulting%20in%0Aslower%20overall%20performance.%20Prior%20works%20mitigate%20this%20by%20encoding%20refinement%0Aissues%20as%20constraints%20to%20prune%20infeasible%20task%20plans.%20However%2C%20these%20approaches%0Aonly%20add%20constraints%20upon%20refinement%20failure%2C%20expending%20significant%20search%0Aeffort%20on%20infeasible%20branches.%20We%20propose%20VIZ-COAST%2C%20a%20method%20of%20leveraging%20the%0Acommon-sense%20spatial%20reasoning%20of%20large%20pretrained%20Vision-Language%20Models%20to%0Aidentify%20issues%20with%20downward%20refinement%20a%20priori%2C%20bypassing%20the%20need%20to%20fix%0Athese%20failures%20during%20planning.%20Experiments%20on%20two%20challenging%20TAMP%20domains%0Ashow%20that%20our%20approach%20is%20able%20to%20extract%20plausible%20constraints%20from%20images%20and%0Adomain%20descriptions%2C%20drastically%20reducing%20planning%20times%20and%2C%20in%20some%20cases%2C%0Aeliminating%20downward%20refinement%20failures%20altogether%2C%20generalizing%20to%20a%20diverse%0Arange%20of%20instances%20from%20the%20broader%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520VLM%2520Reasoning%2520to%2520Constrain%2520Task%2520and%2520Motion%2520Planning%26entry.906535625%3DMuyang%2520Yan%2520and%2520Miras%2520Mengdibayev%2520and%2520Ardon%2520Floros%2520and%2520Weihang%2520Guo%2520and%2520Lydia%2520E.%2520Kavraki%2520and%2520Zachary%2520Kingston%26entry.1292438233%3D%2520%2520In%2520task%2520and%2520motion%2520planning%252C%2520high-level%2520task%2520planning%2520is%2520done%2520over%2520an%250Aabstraction%2520of%2520the%2520world%2520to%2520enable%2520efficient%2520search%2520in%2520long-horizon%2520robotics%250Aproblems.%2520However%252C%2520the%2520feasibility%2520of%2520these%2520task-level%2520plans%2520relies%2520on%2520the%250Adownward%2520refinability%2520of%2520the%2520abstraction%2520into%2520continuous%2520motion.%2520When%2520a%250Adomain%2527s%2520refinability%2520is%2520poor%252C%2520task-level%2520plans%2520that%2520appear%2520valid%2520may%250Aultimately%2520fail%2520during%2520motion%2520planning%252C%2520requiring%2520replanning%2520and%2520resulting%2520in%250Aslower%2520overall%2520performance.%2520Prior%2520works%2520mitigate%2520this%2520by%2520encoding%2520refinement%250Aissues%2520as%2520constraints%2520to%2520prune%2520infeasible%2520task%2520plans.%2520However%252C%2520these%2520approaches%250Aonly%2520add%2520constraints%2520upon%2520refinement%2520failure%252C%2520expending%2520significant%2520search%250Aeffort%2520on%2520infeasible%2520branches.%2520We%2520propose%2520VIZ-COAST%252C%2520a%2520method%2520of%2520leveraging%2520the%250Acommon-sense%2520spatial%2520reasoning%2520of%2520large%2520pretrained%2520Vision-Language%2520Models%2520to%250Aidentify%2520issues%2520with%2520downward%2520refinement%2520a%2520priori%252C%2520bypassing%2520the%2520need%2520to%2520fix%250Athese%2520failures%2520during%2520planning.%2520Experiments%2520on%2520two%2520challenging%2520TAMP%2520domains%250Ashow%2520that%2520our%2520approach%2520is%2520able%2520to%2520extract%2520plausible%2520constraints%2520from%2520images%2520and%250Adomain%2520descriptions%252C%2520drastically%2520reducing%2520planning%2520times%2520and%252C%2520in%2520some%2520cases%252C%250Aeliminating%2520downward%2520refinement%2520failures%2520altogether%252C%2520generalizing%2520to%2520a%2520diverse%250Arange%2520of%2520instances%2520from%2520the%2520broader%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20VLM%20Reasoning%20to%20Constrain%20Task%20and%20Motion%20Planning&entry.906535625=Muyang%20Yan%20and%20Miras%20Mengdibayev%20and%20Ardon%20Floros%20and%20Weihang%20Guo%20and%20Lydia%20E.%20Kavraki%20and%20Zachary%20Kingston&entry.1292438233=%20%20In%20task%20and%20motion%20planning%2C%20high-level%20task%20planning%20is%20done%20over%20an%0Aabstraction%20of%20the%20world%20to%20enable%20efficient%20search%20in%20long-horizon%20robotics%0Aproblems.%20However%2C%20the%20feasibility%20of%20these%20task-level%20plans%20relies%20on%20the%0Adownward%20refinability%20of%20the%20abstraction%20into%20continuous%20motion.%20When%20a%0Adomain%27s%20refinability%20is%20poor%2C%20task-level%20plans%20that%20appear%20valid%20may%0Aultimately%20fail%20during%20motion%20planning%2C%20requiring%20replanning%20and%20resulting%20in%0Aslower%20overall%20performance.%20Prior%20works%20mitigate%20this%20by%20encoding%20refinement%0Aissues%20as%20constraints%20to%20prune%20infeasible%20task%20plans.%20However%2C%20these%20approaches%0Aonly%20add%20constraints%20upon%20refinement%20failure%2C%20expending%20significant%20search%0Aeffort%20on%20infeasible%20branches.%20We%20propose%20VIZ-COAST%2C%20a%20method%20of%20leveraging%20the%0Acommon-sense%20spatial%20reasoning%20of%20large%20pretrained%20Vision-Language%20Models%20to%0Aidentify%20issues%20with%20downward%20refinement%20a%20priori%2C%20bypassing%20the%20need%20to%20fix%0Athese%20failures%20during%20planning.%20Experiments%20on%20two%20challenging%20TAMP%20domains%0Ashow%20that%20our%20approach%20is%20able%20to%20extract%20plausible%20constraints%20from%20images%20and%0Adomain%20descriptions%2C%20drastically%20reducing%20planning%20times%20and%2C%20in%20some%20cases%2C%0Aeliminating%20downward%20refinement%20failures%20altogether%2C%20generalizing%20to%20a%20diverse%0Arange%20of%20instances%20from%20the%20broader%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25548v1&entry.124074799=Read"},
{"title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains", "author": "Vijay Devane and Mohd Nauman and Bhargav Patel and Aniket Mahendra Wakchoure and Yogeshkumar Sant and Shyam Pawar and Viraj Thakur and Ananya Godse and Sunil Patra and Neha Maurya and Suraj Racha and Nitish Kamal Singh and Ajay Nagpal and Piyush Sawarkar and Kundeshwar Vijayrao Pundalik and Rohit Saluja and Ganesh Ramakrishnan", "abstract": "  The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research.\n", "link": "http://arxiv.org/abs/2510.25409v1", "date": "2025-10-29", "relevancy": 2.1695, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4514}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BhashaBench%20V1%3A%20A%20Comprehensive%20Benchmark%20for%20the%20Quadrant%20of%20Indic%0A%20%20Domains&body=Title%3A%20BhashaBench%20V1%3A%20A%20Comprehensive%20Benchmark%20for%20the%20Quadrant%20of%20Indic%0A%20%20Domains%0AAuthor%3A%20Vijay%20Devane%20and%20Mohd%20Nauman%20and%20Bhargav%20Patel%20and%20Aniket%20Mahendra%20Wakchoure%20and%20Yogeshkumar%20Sant%20and%20Shyam%20Pawar%20and%20Viraj%20Thakur%20and%20Ananya%20Godse%20and%20Sunil%20Patra%20and%20Neha%20Maurya%20and%20Suraj%20Racha%20and%20Nitish%20Kamal%20Singh%20and%20Ajay%20Nagpal%20and%20Piyush%20Sawarkar%20and%20Kundeshwar%20Vijayrao%20Pundalik%20and%20Rohit%20Saluja%20and%20Ganesh%20Ramakrishnan%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20large%20language%20models%28LLMs%29%20has%20intensified%20the%20need%0Afor%20domain%20and%20culture%20specific%20evaluation.%20Existing%20benchmarks%20are%20largely%0AAnglocentric%20and%20domain-agnostic%2C%20limiting%20their%20applicability%20to%20India-centric%0Acontexts.%20To%20address%20this%20gap%2C%20we%20introduce%20BhashaBench%20V1%2C%20the%20first%0Adomain-specific%2C%20multi-task%2C%20bilingual%20benchmark%20focusing%20on%20critical%20Indic%0Aknowledge%20systems.%20BhashaBench%20V1%20contains%2074%2C166%20meticulously%20curated%0Aquestion-answer%20pairs%2C%20with%2052%2C494%20in%20English%20and%2021%2C672%20in%20Hindi%2C%20sourced%20from%0Aauthentic%20government%20and%20domain-specific%20exams.%20It%20spans%20four%20major%20domains%3A%0AAgriculture%2C%20Legal%2C%20Finance%2C%20and%20Ayurveda%2C%20comprising%2090%2B%20subdomains%20and%0Acovering%20500%2B%20topics%2C%20enabling%20fine-grained%20evaluation.%20Evaluation%20of%2029%2B%20LLMs%0Areveals%20significant%20domain%20and%20language%20specific%20performance%20gaps%2C%20with%0Aespecially%20large%20disparities%20in%20low-resource%20domains.%20For%20instance%2C%20GPT-4o%0Aachieves%2076.49%25%20overall%20accuracy%20in%20Legal%20but%20only%2059.74%25%20in%20Ayurveda.%20Models%0Aconsistently%20perform%20better%20on%20English%20content%20compared%20to%20Hindi%20across%20all%0Adomains.%20Subdomain-level%20analysis%20shows%20that%20areas%20such%20as%20Cyber%20Law%2C%0AInternational%20Finance%20perform%20relatively%20well%2C%20while%20Panchakarma%2C%20Seed%20Science%2C%0Aand%20Human%20Rights%20remain%20notably%20weak.%20BhashaBench%20V1%20provides%20a%20comprehensive%0Adataset%20for%20evaluating%20large%20language%20models%20across%20India%27s%20diverse%20knowledge%0Adomains.%20It%20enables%20assessment%20of%20models%27%20ability%20to%20integrate%20domain-specific%0Aknowledge%20with%20bilingual%20understanding.%20All%20code%2C%20benchmarks%2C%20and%20resources%20are%0Apublicly%20available%20to%20support%20open%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBhashaBench%2520V1%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520the%2520Quadrant%2520of%2520Indic%250A%2520%2520Domains%26entry.906535625%3DVijay%2520Devane%2520and%2520Mohd%2520Nauman%2520and%2520Bhargav%2520Patel%2520and%2520Aniket%2520Mahendra%2520Wakchoure%2520and%2520Yogeshkumar%2520Sant%2520and%2520Shyam%2520Pawar%2520and%2520Viraj%2520Thakur%2520and%2520Ananya%2520Godse%2520and%2520Sunil%2520Patra%2520and%2520Neha%2520Maurya%2520and%2520Suraj%2520Racha%2520and%2520Nitish%2520Kamal%2520Singh%2520and%2520Ajay%2520Nagpal%2520and%2520Piyush%2520Sawarkar%2520and%2520Kundeshwar%2520Vijayrao%2520Pundalik%2520and%2520Rohit%2520Saluja%2520and%2520Ganesh%2520Ramakrishnan%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520large%2520language%2520models%2528LLMs%2529%2520has%2520intensified%2520the%2520need%250Afor%2520domain%2520and%2520culture%2520specific%2520evaluation.%2520Existing%2520benchmarks%2520are%2520largely%250AAnglocentric%2520and%2520domain-agnostic%252C%2520limiting%2520their%2520applicability%2520to%2520India-centric%250Acontexts.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520BhashaBench%2520V1%252C%2520the%2520first%250Adomain-specific%252C%2520multi-task%252C%2520bilingual%2520benchmark%2520focusing%2520on%2520critical%2520Indic%250Aknowledge%2520systems.%2520BhashaBench%2520V1%2520contains%252074%252C166%2520meticulously%2520curated%250Aquestion-answer%2520pairs%252C%2520with%252052%252C494%2520in%2520English%2520and%252021%252C672%2520in%2520Hindi%252C%2520sourced%2520from%250Aauthentic%2520government%2520and%2520domain-specific%2520exams.%2520It%2520spans%2520four%2520major%2520domains%253A%250AAgriculture%252C%2520Legal%252C%2520Finance%252C%2520and%2520Ayurveda%252C%2520comprising%252090%252B%2520subdomains%2520and%250Acovering%2520500%252B%2520topics%252C%2520enabling%2520fine-grained%2520evaluation.%2520Evaluation%2520of%252029%252B%2520LLMs%250Areveals%2520significant%2520domain%2520and%2520language%2520specific%2520performance%2520gaps%252C%2520with%250Aespecially%2520large%2520disparities%2520in%2520low-resource%2520domains.%2520For%2520instance%252C%2520GPT-4o%250Aachieves%252076.49%2525%2520overall%2520accuracy%2520in%2520Legal%2520but%2520only%252059.74%2525%2520in%2520Ayurveda.%2520Models%250Aconsistently%2520perform%2520better%2520on%2520English%2520content%2520compared%2520to%2520Hindi%2520across%2520all%250Adomains.%2520Subdomain-level%2520analysis%2520shows%2520that%2520areas%2520such%2520as%2520Cyber%2520Law%252C%250AInternational%2520Finance%2520perform%2520relatively%2520well%252C%2520while%2520Panchakarma%252C%2520Seed%2520Science%252C%250Aand%2520Human%2520Rights%2520remain%2520notably%2520weak.%2520BhashaBench%2520V1%2520provides%2520a%2520comprehensive%250Adataset%2520for%2520evaluating%2520large%2520language%2520models%2520across%2520India%2527s%2520diverse%2520knowledge%250Adomains.%2520It%2520enables%2520assessment%2520of%2520models%2527%2520ability%2520to%2520integrate%2520domain-specific%250Aknowledge%2520with%2520bilingual%2520understanding.%2520All%2520code%252C%2520benchmarks%252C%2520and%2520resources%2520are%250Apublicly%2520available%2520to%2520support%2520open%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BhashaBench%20V1%3A%20A%20Comprehensive%20Benchmark%20for%20the%20Quadrant%20of%20Indic%0A%20%20Domains&entry.906535625=Vijay%20Devane%20and%20Mohd%20Nauman%20and%20Bhargav%20Patel%20and%20Aniket%20Mahendra%20Wakchoure%20and%20Yogeshkumar%20Sant%20and%20Shyam%20Pawar%20and%20Viraj%20Thakur%20and%20Ananya%20Godse%20and%20Sunil%20Patra%20and%20Neha%20Maurya%20and%20Suraj%20Racha%20and%20Nitish%20Kamal%20Singh%20and%20Ajay%20Nagpal%20and%20Piyush%20Sawarkar%20and%20Kundeshwar%20Vijayrao%20Pundalik%20and%20Rohit%20Saluja%20and%20Ganesh%20Ramakrishnan&entry.1292438233=%20%20The%20rapid%20advancement%20of%20large%20language%20models%28LLMs%29%20has%20intensified%20the%20need%0Afor%20domain%20and%20culture%20specific%20evaluation.%20Existing%20benchmarks%20are%20largely%0AAnglocentric%20and%20domain-agnostic%2C%20limiting%20their%20applicability%20to%20India-centric%0Acontexts.%20To%20address%20this%20gap%2C%20we%20introduce%20BhashaBench%20V1%2C%20the%20first%0Adomain-specific%2C%20multi-task%2C%20bilingual%20benchmark%20focusing%20on%20critical%20Indic%0Aknowledge%20systems.%20BhashaBench%20V1%20contains%2074%2C166%20meticulously%20curated%0Aquestion-answer%20pairs%2C%20with%2052%2C494%20in%20English%20and%2021%2C672%20in%20Hindi%2C%20sourced%20from%0Aauthentic%20government%20and%20domain-specific%20exams.%20It%20spans%20four%20major%20domains%3A%0AAgriculture%2C%20Legal%2C%20Finance%2C%20and%20Ayurveda%2C%20comprising%2090%2B%20subdomains%20and%0Acovering%20500%2B%20topics%2C%20enabling%20fine-grained%20evaluation.%20Evaluation%20of%2029%2B%20LLMs%0Areveals%20significant%20domain%20and%20language%20specific%20performance%20gaps%2C%20with%0Aespecially%20large%20disparities%20in%20low-resource%20domains.%20For%20instance%2C%20GPT-4o%0Aachieves%2076.49%25%20overall%20accuracy%20in%20Legal%20but%20only%2059.74%25%20in%20Ayurveda.%20Models%0Aconsistently%20perform%20better%20on%20English%20content%20compared%20to%20Hindi%20across%20all%0Adomains.%20Subdomain-level%20analysis%20shows%20that%20areas%20such%20as%20Cyber%20Law%2C%0AInternational%20Finance%20perform%20relatively%20well%2C%20while%20Panchakarma%2C%20Seed%20Science%2C%0Aand%20Human%20Rights%20remain%20notably%20weak.%20BhashaBench%20V1%20provides%20a%20comprehensive%0Adataset%20for%20evaluating%20large%20language%20models%20across%20India%27s%20diverse%20knowledge%0Adomains.%20It%20enables%20assessment%20of%20models%27%20ability%20to%20integrate%20domain-specific%0Aknowledge%20with%20bilingual%20understanding.%20All%20code%2C%20benchmarks%2C%20and%20resources%20are%0Apublicly%20available%20to%20support%20open%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25409v1&entry.124074799=Read"},
{"title": "Collision avoidance and path finding in a robotic mobile fulfillment\n  system using multi-objective meta-heuristics", "author": "Ahmad Kokhahi and Mary Kurz", "abstract": "  Multi-Agent Path Finding (MAPF) has gained significant attention, with most\nresearch focusing on minimizing collisions and travel time. This paper also\nconsiders energy consumption in the path planning of automated guided vehicles\n(AGVs). It addresses two main challenges: i) resolving collisions between AGVs\nand ii) assigning tasks to AGVs. We propose a new collision avoidance strategy\nthat takes both energy use and travel time into account. For task assignment,\nwe present two multi-objective algorithms: Non-Dominated Sorting Genetic\nAlgorithm (NSGA) and Adaptive Large Neighborhood Search (ALNS). Comparative\nevaluations show that these proposed methods perform better than existing\napproaches in both collision avoidance and task assignment.\n", "link": "http://arxiv.org/abs/2510.25650v1", "date": "2025-10-29", "relevancy": 2.1692, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5693}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5436}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collision%20avoidance%20and%20path%20finding%20in%20a%20robotic%20mobile%20fulfillment%0A%20%20system%20using%20multi-objective%20meta-heuristics&body=Title%3A%20Collision%20avoidance%20and%20path%20finding%20in%20a%20robotic%20mobile%20fulfillment%0A%20%20system%20using%20multi-objective%20meta-heuristics%0AAuthor%3A%20Ahmad%20Kokhahi%20and%20Mary%20Kurz%0AAbstract%3A%20%20%20Multi-Agent%20Path%20Finding%20%28MAPF%29%20has%20gained%20significant%20attention%2C%20with%20most%0Aresearch%20focusing%20on%20minimizing%20collisions%20and%20travel%20time.%20This%20paper%20also%0Aconsiders%20energy%20consumption%20in%20the%20path%20planning%20of%20automated%20guided%20vehicles%0A%28AGVs%29.%20It%20addresses%20two%20main%20challenges%3A%20i%29%20resolving%20collisions%20between%20AGVs%0Aand%20ii%29%20assigning%20tasks%20to%20AGVs.%20We%20propose%20a%20new%20collision%20avoidance%20strategy%0Athat%20takes%20both%20energy%20use%20and%20travel%20time%20into%20account.%20For%20task%20assignment%2C%0Awe%20present%20two%20multi-objective%20algorithms%3A%20Non-Dominated%20Sorting%20Genetic%0AAlgorithm%20%28NSGA%29%20and%20Adaptive%20Large%20Neighborhood%20Search%20%28ALNS%29.%20Comparative%0Aevaluations%20show%20that%20these%20proposed%20methods%20perform%20better%20than%20existing%0Aapproaches%20in%20both%20collision%20avoidance%20and%20task%20assignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollision%2520avoidance%2520and%2520path%2520finding%2520in%2520a%2520robotic%2520mobile%2520fulfillment%250A%2520%2520system%2520using%2520multi-objective%2520meta-heuristics%26entry.906535625%3DAhmad%2520Kokhahi%2520and%2520Mary%2520Kurz%26entry.1292438233%3D%2520%2520Multi-Agent%2520Path%2520Finding%2520%2528MAPF%2529%2520has%2520gained%2520significant%2520attention%252C%2520with%2520most%250Aresearch%2520focusing%2520on%2520minimizing%2520collisions%2520and%2520travel%2520time.%2520This%2520paper%2520also%250Aconsiders%2520energy%2520consumption%2520in%2520the%2520path%2520planning%2520of%2520automated%2520guided%2520vehicles%250A%2528AGVs%2529.%2520It%2520addresses%2520two%2520main%2520challenges%253A%2520i%2529%2520resolving%2520collisions%2520between%2520AGVs%250Aand%2520ii%2529%2520assigning%2520tasks%2520to%2520AGVs.%2520We%2520propose%2520a%2520new%2520collision%2520avoidance%2520strategy%250Athat%2520takes%2520both%2520energy%2520use%2520and%2520travel%2520time%2520into%2520account.%2520For%2520task%2520assignment%252C%250Awe%2520present%2520two%2520multi-objective%2520algorithms%253A%2520Non-Dominated%2520Sorting%2520Genetic%250AAlgorithm%2520%2528NSGA%2529%2520and%2520Adaptive%2520Large%2520Neighborhood%2520Search%2520%2528ALNS%2529.%2520Comparative%250Aevaluations%2520show%2520that%2520these%2520proposed%2520methods%2520perform%2520better%2520than%2520existing%250Aapproaches%2520in%2520both%2520collision%2520avoidance%2520and%2520task%2520assignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collision%20avoidance%20and%20path%20finding%20in%20a%20robotic%20mobile%20fulfillment%0A%20%20system%20using%20multi-objective%20meta-heuristics&entry.906535625=Ahmad%20Kokhahi%20and%20Mary%20Kurz&entry.1292438233=%20%20Multi-Agent%20Path%20Finding%20%28MAPF%29%20has%20gained%20significant%20attention%2C%20with%20most%0Aresearch%20focusing%20on%20minimizing%20collisions%20and%20travel%20time.%20This%20paper%20also%0Aconsiders%20energy%20consumption%20in%20the%20path%20planning%20of%20automated%20guided%20vehicles%0A%28AGVs%29.%20It%20addresses%20two%20main%20challenges%3A%20i%29%20resolving%20collisions%20between%20AGVs%0Aand%20ii%29%20assigning%20tasks%20to%20AGVs.%20We%20propose%20a%20new%20collision%20avoidance%20strategy%0Athat%20takes%20both%20energy%20use%20and%20travel%20time%20into%20account.%20For%20task%20assignment%2C%0Awe%20present%20two%20multi-objective%20algorithms%3A%20Non-Dominated%20Sorting%20Genetic%0AAlgorithm%20%28NSGA%29%20and%20Adaptive%20Large%20Neighborhood%20Search%20%28ALNS%29.%20Comparative%0Aevaluations%20show%20that%20these%20proposed%20methods%20perform%20better%20than%20existing%0Aapproaches%20in%20both%20collision%20avoidance%20and%20task%20assignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25650v1&entry.124074799=Read"},
{"title": "U-DECN: End-to-End Underwater Object Detection ConvNet with Improved\n  DeNoising Training", "author": "Zhuoyan Liu and Bo Wang and Bing Wang and Ye Li", "abstract": "  Underwater object detection has higher requirements of running speed and\ndeployment efficiency for the detector due to its specific environmental\nchallenges. NMS of two- or one-stage object detectors and transformer\narchitecture of query-based end-to-end object detectors are not conducive to\ndeployment on underwater embedded devices with limited processing power. As for\nthe detrimental effect of underwater color cast noise, recent underwater object\ndetectors make network architecture or training complex, which also hinders\ntheir application and deployment on unmanned underwater vehicles. In this\npaper, we propose the Underwater DECO with improved deNoising training\n(U-DECN), the query-based end-to-end object detector (with ConvNet\nencoder-decoder architecture) for underwater color cast noise that addresses\nthe above problems. We integrate advanced technologies from DETR variants into\nDECO and design optimization methods specifically for the ConvNet architecture,\nincluding Deformable Convolution in SIM and Separate Contrastive DeNoising\nForward methods. To address the underwater color cast noise issue, we propose\nan Underwater Color DeNoising Query method to improve the generalization of the\nmodel for the biased object feature information by different color cast noise.\nOur U-DECN, with ResNet-50 backbone, achieves the best 64.0 AP on DUO and the\nbest 58.1 AP on RUOD, and 21 FPS (5 times faster than Deformable DETR and DINO\n4 FPS) on NVIDIA AGX Orin by TensorRT FP16, outperforming the other\nstate-of-the-art query-based end-to-end object detectors. The code is available\nat https://github.com/LEFTeyex/U-DECN.\n", "link": "http://arxiv.org/abs/2408.05780v2", "date": "2025-10-29", "relevancy": 2.1652, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5563}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.54}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20U-DECN%3A%20End-to-End%20Underwater%20Object%20Detection%20ConvNet%20with%20Improved%0A%20%20DeNoising%20Training&body=Title%3A%20U-DECN%3A%20End-to-End%20Underwater%20Object%20Detection%20ConvNet%20with%20Improved%0A%20%20DeNoising%20Training%0AAuthor%3A%20Zhuoyan%20Liu%20and%20Bo%20Wang%20and%20Bing%20Wang%20and%20Ye%20Li%0AAbstract%3A%20%20%20Underwater%20object%20detection%20has%20higher%20requirements%20of%20running%20speed%20and%0Adeployment%20efficiency%20for%20the%20detector%20due%20to%20its%20specific%20environmental%0Achallenges.%20NMS%20of%20two-%20or%20one-stage%20object%20detectors%20and%20transformer%0Aarchitecture%20of%20query-based%20end-to-end%20object%20detectors%20are%20not%20conducive%20to%0Adeployment%20on%20underwater%20embedded%20devices%20with%20limited%20processing%20power.%20As%20for%0Athe%20detrimental%20effect%20of%20underwater%20color%20cast%20noise%2C%20recent%20underwater%20object%0Adetectors%20make%20network%20architecture%20or%20training%20complex%2C%20which%20also%20hinders%0Atheir%20application%20and%20deployment%20on%20unmanned%20underwater%20vehicles.%20In%20this%0Apaper%2C%20we%20propose%20the%20Underwater%20DECO%20with%20improved%20deNoising%20training%0A%28U-DECN%29%2C%20the%20query-based%20end-to-end%20object%20detector%20%28with%20ConvNet%0Aencoder-decoder%20architecture%29%20for%20underwater%20color%20cast%20noise%20that%20addresses%0Athe%20above%20problems.%20We%20integrate%20advanced%20technologies%20from%20DETR%20variants%20into%0ADECO%20and%20design%20optimization%20methods%20specifically%20for%20the%20ConvNet%20architecture%2C%0Aincluding%20Deformable%20Convolution%20in%20SIM%20and%20Separate%20Contrastive%20DeNoising%0AForward%20methods.%20To%20address%20the%20underwater%20color%20cast%20noise%20issue%2C%20we%20propose%0Aan%20Underwater%20Color%20DeNoising%20Query%20method%20to%20improve%20the%20generalization%20of%20the%0Amodel%20for%20the%20biased%20object%20feature%20information%20by%20different%20color%20cast%20noise.%0AOur%20U-DECN%2C%20with%20ResNet-50%20backbone%2C%20achieves%20the%20best%2064.0%20AP%20on%20DUO%20and%20the%0Abest%2058.1%20AP%20on%20RUOD%2C%20and%2021%20FPS%20%285%20times%20faster%20than%20Deformable%20DETR%20and%20DINO%0A4%20FPS%29%20on%20NVIDIA%20AGX%20Orin%20by%20TensorRT%20FP16%2C%20outperforming%20the%20other%0Astate-of-the-art%20query-based%20end-to-end%20object%20detectors.%20The%20code%20is%20available%0Aat%20https%3A//github.com/LEFTeyex/U-DECN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DU-DECN%253A%2520End-to-End%2520Underwater%2520Object%2520Detection%2520ConvNet%2520with%2520Improved%250A%2520%2520DeNoising%2520Training%26entry.906535625%3DZhuoyan%2520Liu%2520and%2520Bo%2520Wang%2520and%2520Bing%2520Wang%2520and%2520Ye%2520Li%26entry.1292438233%3D%2520%2520Underwater%2520object%2520detection%2520has%2520higher%2520requirements%2520of%2520running%2520speed%2520and%250Adeployment%2520efficiency%2520for%2520the%2520detector%2520due%2520to%2520its%2520specific%2520environmental%250Achallenges.%2520NMS%2520of%2520two-%2520or%2520one-stage%2520object%2520detectors%2520and%2520transformer%250Aarchitecture%2520of%2520query-based%2520end-to-end%2520object%2520detectors%2520are%2520not%2520conducive%2520to%250Adeployment%2520on%2520underwater%2520embedded%2520devices%2520with%2520limited%2520processing%2520power.%2520As%2520for%250Athe%2520detrimental%2520effect%2520of%2520underwater%2520color%2520cast%2520noise%252C%2520recent%2520underwater%2520object%250Adetectors%2520make%2520network%2520architecture%2520or%2520training%2520complex%252C%2520which%2520also%2520hinders%250Atheir%2520application%2520and%2520deployment%2520on%2520unmanned%2520underwater%2520vehicles.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520the%2520Underwater%2520DECO%2520with%2520improved%2520deNoising%2520training%250A%2528U-DECN%2529%252C%2520the%2520query-based%2520end-to-end%2520object%2520detector%2520%2528with%2520ConvNet%250Aencoder-decoder%2520architecture%2529%2520for%2520underwater%2520color%2520cast%2520noise%2520that%2520addresses%250Athe%2520above%2520problems.%2520We%2520integrate%2520advanced%2520technologies%2520from%2520DETR%2520variants%2520into%250ADECO%2520and%2520design%2520optimization%2520methods%2520specifically%2520for%2520the%2520ConvNet%2520architecture%252C%250Aincluding%2520Deformable%2520Convolution%2520in%2520SIM%2520and%2520Separate%2520Contrastive%2520DeNoising%250AForward%2520methods.%2520To%2520address%2520the%2520underwater%2520color%2520cast%2520noise%2520issue%252C%2520we%2520propose%250Aan%2520Underwater%2520Color%2520DeNoising%2520Query%2520method%2520to%2520improve%2520the%2520generalization%2520of%2520the%250Amodel%2520for%2520the%2520biased%2520object%2520feature%2520information%2520by%2520different%2520color%2520cast%2520noise.%250AOur%2520U-DECN%252C%2520with%2520ResNet-50%2520backbone%252C%2520achieves%2520the%2520best%252064.0%2520AP%2520on%2520DUO%2520and%2520the%250Abest%252058.1%2520AP%2520on%2520RUOD%252C%2520and%252021%2520FPS%2520%25285%2520times%2520faster%2520than%2520Deformable%2520DETR%2520and%2520DINO%250A4%2520FPS%2529%2520on%2520NVIDIA%2520AGX%2520Orin%2520by%2520TensorRT%2520FP16%252C%2520outperforming%2520the%2520other%250Astate-of-the-art%2520query-based%2520end-to-end%2520object%2520detectors.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/LEFTeyex/U-DECN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U-DECN%3A%20End-to-End%20Underwater%20Object%20Detection%20ConvNet%20with%20Improved%0A%20%20DeNoising%20Training&entry.906535625=Zhuoyan%20Liu%20and%20Bo%20Wang%20and%20Bing%20Wang%20and%20Ye%20Li&entry.1292438233=%20%20Underwater%20object%20detection%20has%20higher%20requirements%20of%20running%20speed%20and%0Adeployment%20efficiency%20for%20the%20detector%20due%20to%20its%20specific%20environmental%0Achallenges.%20NMS%20of%20two-%20or%20one-stage%20object%20detectors%20and%20transformer%0Aarchitecture%20of%20query-based%20end-to-end%20object%20detectors%20are%20not%20conducive%20to%0Adeployment%20on%20underwater%20embedded%20devices%20with%20limited%20processing%20power.%20As%20for%0Athe%20detrimental%20effect%20of%20underwater%20color%20cast%20noise%2C%20recent%20underwater%20object%0Adetectors%20make%20network%20architecture%20or%20training%20complex%2C%20which%20also%20hinders%0Atheir%20application%20and%20deployment%20on%20unmanned%20underwater%20vehicles.%20In%20this%0Apaper%2C%20we%20propose%20the%20Underwater%20DECO%20with%20improved%20deNoising%20training%0A%28U-DECN%29%2C%20the%20query-based%20end-to-end%20object%20detector%20%28with%20ConvNet%0Aencoder-decoder%20architecture%29%20for%20underwater%20color%20cast%20noise%20that%20addresses%0Athe%20above%20problems.%20We%20integrate%20advanced%20technologies%20from%20DETR%20variants%20into%0ADECO%20and%20design%20optimization%20methods%20specifically%20for%20the%20ConvNet%20architecture%2C%0Aincluding%20Deformable%20Convolution%20in%20SIM%20and%20Separate%20Contrastive%20DeNoising%0AForward%20methods.%20To%20address%20the%20underwater%20color%20cast%20noise%20issue%2C%20we%20propose%0Aan%20Underwater%20Color%20DeNoising%20Query%20method%20to%20improve%20the%20generalization%20of%20the%0Amodel%20for%20the%20biased%20object%20feature%20information%20by%20different%20color%20cast%20noise.%0AOur%20U-DECN%2C%20with%20ResNet-50%20backbone%2C%20achieves%20the%20best%2064.0%20AP%20on%20DUO%20and%20the%0Abest%2058.1%20AP%20on%20RUOD%2C%20and%2021%20FPS%20%285%20times%20faster%20than%20Deformable%20DETR%20and%20DINO%0A4%20FPS%29%20on%20NVIDIA%20AGX%20Orin%20by%20TensorRT%20FP16%2C%20outperforming%20the%20other%0Astate-of-the-art%20query-based%20end-to-end%20object%20detectors.%20The%20code%20is%20available%0Aat%20https%3A//github.com/LEFTeyex/U-DECN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05780v2&entry.124074799=Read"},
{"title": "FaCT: Faithful Concept Traces for Explaining Neural Network Decisions", "author": "Amin Parchami-Araghi and Sukrut Rao and Jonas Fischer and Bernt Schiele", "abstract": "  Deep networks have shown remarkable performance across a wide range of tasks,\nyet getting a global concept-level understanding of how they function remains a\nkey challenge. Many post-hoc concept-based approaches have been introduced to\nunderstand their workings, yet they are not always faithful to the model.\nFurther, they make restrictive assumptions on the concepts a model learns, such\nas class-specificity, small spatial extent, or alignment to human expectations.\nIn this work, we put emphasis on the faithfulness of such concept-based\nexplanations and propose a new model with model-inherent mechanistic\nconcept-explanations. Our concepts are shared across classes and, from any\nlayer, their contribution to the logit and their input-visualization can be\nfaithfully traced. We also leverage foundation models to propose a new\nconcept-consistency metric, C$^2$-Score, that can be used to evaluate\nconcept-based methods. We show that, compared to prior work, our concepts are\nquantitatively more consistent and users find our concepts to be more\ninterpretable, all while retaining competitive ImageNet performance.\n", "link": "http://arxiv.org/abs/2510.25512v1", "date": "2025-10-29", "relevancy": 2.1537, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5398}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5398}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaCT%3A%20Faithful%20Concept%20Traces%20for%20Explaining%20Neural%20Network%20Decisions&body=Title%3A%20FaCT%3A%20Faithful%20Concept%20Traces%20for%20Explaining%20Neural%20Network%20Decisions%0AAuthor%3A%20Amin%20Parchami-Araghi%20and%20Sukrut%20Rao%20and%20Jonas%20Fischer%20and%20Bernt%20Schiele%0AAbstract%3A%20%20%20Deep%20networks%20have%20shown%20remarkable%20performance%20across%20a%20wide%20range%20of%20tasks%2C%0Ayet%20getting%20a%20global%20concept-level%20understanding%20of%20how%20they%20function%20remains%20a%0Akey%20challenge.%20Many%20post-hoc%20concept-based%20approaches%20have%20been%20introduced%20to%0Aunderstand%20their%20workings%2C%20yet%20they%20are%20not%20always%20faithful%20to%20the%20model.%0AFurther%2C%20they%20make%20restrictive%20assumptions%20on%20the%20concepts%20a%20model%20learns%2C%20such%0Aas%20class-specificity%2C%20small%20spatial%20extent%2C%20or%20alignment%20to%20human%20expectations.%0AIn%20this%20work%2C%20we%20put%20emphasis%20on%20the%20faithfulness%20of%20such%20concept-based%0Aexplanations%20and%20propose%20a%20new%20model%20with%20model-inherent%20mechanistic%0Aconcept-explanations.%20Our%20concepts%20are%20shared%20across%20classes%20and%2C%20from%20any%0Alayer%2C%20their%20contribution%20to%20the%20logit%20and%20their%20input-visualization%20can%20be%0Afaithfully%20traced.%20We%20also%20leverage%20foundation%20models%20to%20propose%20a%20new%0Aconcept-consistency%20metric%2C%20C%24%5E2%24-Score%2C%20that%20can%20be%20used%20to%20evaluate%0Aconcept-based%20methods.%20We%20show%20that%2C%20compared%20to%20prior%20work%2C%20our%20concepts%20are%0Aquantitatively%20more%20consistent%20and%20users%20find%20our%20concepts%20to%20be%20more%0Ainterpretable%2C%20all%20while%20retaining%20competitive%20ImageNet%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaCT%253A%2520Faithful%2520Concept%2520Traces%2520for%2520Explaining%2520Neural%2520Network%2520Decisions%26entry.906535625%3DAmin%2520Parchami-Araghi%2520and%2520Sukrut%2520Rao%2520and%2520Jonas%2520Fischer%2520and%2520Bernt%2520Schiele%26entry.1292438233%3D%2520%2520Deep%2520networks%2520have%2520shown%2520remarkable%2520performance%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%250Ayet%2520getting%2520a%2520global%2520concept-level%2520understanding%2520of%2520how%2520they%2520function%2520remains%2520a%250Akey%2520challenge.%2520Many%2520post-hoc%2520concept-based%2520approaches%2520have%2520been%2520introduced%2520to%250Aunderstand%2520their%2520workings%252C%2520yet%2520they%2520are%2520not%2520always%2520faithful%2520to%2520the%2520model.%250AFurther%252C%2520they%2520make%2520restrictive%2520assumptions%2520on%2520the%2520concepts%2520a%2520model%2520learns%252C%2520such%250Aas%2520class-specificity%252C%2520small%2520spatial%2520extent%252C%2520or%2520alignment%2520to%2520human%2520expectations.%250AIn%2520this%2520work%252C%2520we%2520put%2520emphasis%2520on%2520the%2520faithfulness%2520of%2520such%2520concept-based%250Aexplanations%2520and%2520propose%2520a%2520new%2520model%2520with%2520model-inherent%2520mechanistic%250Aconcept-explanations.%2520Our%2520concepts%2520are%2520shared%2520across%2520classes%2520and%252C%2520from%2520any%250Alayer%252C%2520their%2520contribution%2520to%2520the%2520logit%2520and%2520their%2520input-visualization%2520can%2520be%250Afaithfully%2520traced.%2520We%2520also%2520leverage%2520foundation%2520models%2520to%2520propose%2520a%2520new%250Aconcept-consistency%2520metric%252C%2520C%2524%255E2%2524-Score%252C%2520that%2520can%2520be%2520used%2520to%2520evaluate%250Aconcept-based%2520methods.%2520We%2520show%2520that%252C%2520compared%2520to%2520prior%2520work%252C%2520our%2520concepts%2520are%250Aquantitatively%2520more%2520consistent%2520and%2520users%2520find%2520our%2520concepts%2520to%2520be%2520more%250Ainterpretable%252C%2520all%2520while%2520retaining%2520competitive%2520ImageNet%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaCT%3A%20Faithful%20Concept%20Traces%20for%20Explaining%20Neural%20Network%20Decisions&entry.906535625=Amin%20Parchami-Araghi%20and%20Sukrut%20Rao%20and%20Jonas%20Fischer%20and%20Bernt%20Schiele&entry.1292438233=%20%20Deep%20networks%20have%20shown%20remarkable%20performance%20across%20a%20wide%20range%20of%20tasks%2C%0Ayet%20getting%20a%20global%20concept-level%20understanding%20of%20how%20they%20function%20remains%20a%0Akey%20challenge.%20Many%20post-hoc%20concept-based%20approaches%20have%20been%20introduced%20to%0Aunderstand%20their%20workings%2C%20yet%20they%20are%20not%20always%20faithful%20to%20the%20model.%0AFurther%2C%20they%20make%20restrictive%20assumptions%20on%20the%20concepts%20a%20model%20learns%2C%20such%0Aas%20class-specificity%2C%20small%20spatial%20extent%2C%20or%20alignment%20to%20human%20expectations.%0AIn%20this%20work%2C%20we%20put%20emphasis%20on%20the%20faithfulness%20of%20such%20concept-based%0Aexplanations%20and%20propose%20a%20new%20model%20with%20model-inherent%20mechanistic%0Aconcept-explanations.%20Our%20concepts%20are%20shared%20across%20classes%20and%2C%20from%20any%0Alayer%2C%20their%20contribution%20to%20the%20logit%20and%20their%20input-visualization%20can%20be%0Afaithfully%20traced.%20We%20also%20leverage%20foundation%20models%20to%20propose%20a%20new%0Aconcept-consistency%20metric%2C%20C%24%5E2%24-Score%2C%20that%20can%20be%20used%20to%20evaluate%0Aconcept-based%20methods.%20We%20show%20that%2C%20compared%20to%20prior%20work%2C%20our%20concepts%20are%0Aquantitatively%20more%20consistent%20and%20users%20find%20our%20concepts%20to%20be%20more%0Ainterpretable%2C%20all%20while%20retaining%20competitive%20ImageNet%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25512v1&entry.124074799=Read"},
{"title": "Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided\n  Mutual Information", "author": "Yuan Cheng and Yu Huang and Zhe Xiong and Yingbin Liang and Vincent Y. F. Tan", "abstract": "  Uncovering hidden graph structures underlying real-world data is a critical\nchallenge with broad applications across scientific domains. Recently,\ntransformer-based models leveraging the attention mechanism have demonstrated\nstrong empirical success in capturing complex dependencies within graphs.\nHowever, the theoretical understanding of their training dynamics has been\nlimited to tree-like graphs, where each node depends on a single parent.\nExtending provable guarantees to more general directed acyclic graphs (DAGs) --\nwhich involve multiple parents per node -- remains challenging, primarily due\nto the difficulty in designing training objectives that enable different\nattention heads to separately learn multiple different parent relationships.\n  In this work, we address this problem by introducing a novel\ninformation-theoretic metric: the kernel-guided mutual information (KG-MI),\nbased on the $f$-divergence. Our objective combines KG-MI with a multi-head\nattention framework, where each head is associated with a distinct marginal\ntransition kernel to model diverse parent-child dependencies effectively. We\nprove that, given sequences generated by a $K$-parent DAG, training a\nsingle-layer, multi-head transformer via gradient ascent converges to the\nglobal optimum in polynomial time. Furthermore, we characterize the attention\nscore patterns at convergence. In addition, when particularizing the\n$f$-divergence to the KL divergence, the learned attention scores accurately\nreflect the ground-truth adjacency matrix, thereby provably recovering the\nunderlying graph structure. Experimental results validate our theoretical\nfindings.\n", "link": "http://arxiv.org/abs/2510.25542v1", "date": "2025-10-29", "relevancy": 2.15, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5545}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5393}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20Provably%20Learn%20Directed%20Acyclic%20Graphs%20via%20Kernel-Guided%0A%20%20Mutual%20Information&body=Title%3A%20Transformers%20Provably%20Learn%20Directed%20Acyclic%20Graphs%20via%20Kernel-Guided%0A%20%20Mutual%20Information%0AAuthor%3A%20Yuan%20Cheng%20and%20Yu%20Huang%20and%20Zhe%20Xiong%20and%20Yingbin%20Liang%20and%20Vincent%20Y.%20F.%20Tan%0AAbstract%3A%20%20%20Uncovering%20hidden%20graph%20structures%20underlying%20real-world%20data%20is%20a%20critical%0Achallenge%20with%20broad%20applications%20across%20scientific%20domains.%20Recently%2C%0Atransformer-based%20models%20leveraging%20the%20attention%20mechanism%20have%20demonstrated%0Astrong%20empirical%20success%20in%20capturing%20complex%20dependencies%20within%20graphs.%0AHowever%2C%20the%20theoretical%20understanding%20of%20their%20training%20dynamics%20has%20been%0Alimited%20to%20tree-like%20graphs%2C%20where%20each%20node%20depends%20on%20a%20single%20parent.%0AExtending%20provable%20guarantees%20to%20more%20general%20directed%20acyclic%20graphs%20%28DAGs%29%20--%0Awhich%20involve%20multiple%20parents%20per%20node%20--%20remains%20challenging%2C%20primarily%20due%0Ato%20the%20difficulty%20in%20designing%20training%20objectives%20that%20enable%20different%0Aattention%20heads%20to%20separately%20learn%20multiple%20different%20parent%20relationships.%0A%20%20In%20this%20work%2C%20we%20address%20this%20problem%20by%20introducing%20a%20novel%0Ainformation-theoretic%20metric%3A%20the%20kernel-guided%20mutual%20information%20%28KG-MI%29%2C%0Abased%20on%20the%20%24f%24-divergence.%20Our%20objective%20combines%20KG-MI%20with%20a%20multi-head%0Aattention%20framework%2C%20where%20each%20head%20is%20associated%20with%20a%20distinct%20marginal%0Atransition%20kernel%20to%20model%20diverse%20parent-child%20dependencies%20effectively.%20We%0Aprove%20that%2C%20given%20sequences%20generated%20by%20a%20%24K%24-parent%20DAG%2C%20training%20a%0Asingle-layer%2C%20multi-head%20transformer%20via%20gradient%20ascent%20converges%20to%20the%0Aglobal%20optimum%20in%20polynomial%20time.%20Furthermore%2C%20we%20characterize%20the%20attention%0Ascore%20patterns%20at%20convergence.%20In%20addition%2C%20when%20particularizing%20the%0A%24f%24-divergence%20to%20the%20KL%20divergence%2C%20the%20learned%20attention%20scores%20accurately%0Areflect%20the%20ground-truth%20adjacency%20matrix%2C%20thereby%20provably%20recovering%20the%0Aunderlying%20graph%20structure.%20Experimental%20results%20validate%20our%20theoretical%0Afindings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520Provably%2520Learn%2520Directed%2520Acyclic%2520Graphs%2520via%2520Kernel-Guided%250A%2520%2520Mutual%2520Information%26entry.906535625%3DYuan%2520Cheng%2520and%2520Yu%2520Huang%2520and%2520Zhe%2520Xiong%2520and%2520Yingbin%2520Liang%2520and%2520Vincent%2520Y.%2520F.%2520Tan%26entry.1292438233%3D%2520%2520Uncovering%2520hidden%2520graph%2520structures%2520underlying%2520real-world%2520data%2520is%2520a%2520critical%250Achallenge%2520with%2520broad%2520applications%2520across%2520scientific%2520domains.%2520Recently%252C%250Atransformer-based%2520models%2520leveraging%2520the%2520attention%2520mechanism%2520have%2520demonstrated%250Astrong%2520empirical%2520success%2520in%2520capturing%2520complex%2520dependencies%2520within%2520graphs.%250AHowever%252C%2520the%2520theoretical%2520understanding%2520of%2520their%2520training%2520dynamics%2520has%2520been%250Alimited%2520to%2520tree-like%2520graphs%252C%2520where%2520each%2520node%2520depends%2520on%2520a%2520single%2520parent.%250AExtending%2520provable%2520guarantees%2520to%2520more%2520general%2520directed%2520acyclic%2520graphs%2520%2528DAGs%2529%2520--%250Awhich%2520involve%2520multiple%2520parents%2520per%2520node%2520--%2520remains%2520challenging%252C%2520primarily%2520due%250Ato%2520the%2520difficulty%2520in%2520designing%2520training%2520objectives%2520that%2520enable%2520different%250Aattention%2520heads%2520to%2520separately%2520learn%2520multiple%2520different%2520parent%2520relationships.%250A%2520%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520problem%2520by%2520introducing%2520a%2520novel%250Ainformation-theoretic%2520metric%253A%2520the%2520kernel-guided%2520mutual%2520information%2520%2528KG-MI%2529%252C%250Abased%2520on%2520the%2520%2524f%2524-divergence.%2520Our%2520objective%2520combines%2520KG-MI%2520with%2520a%2520multi-head%250Aattention%2520framework%252C%2520where%2520each%2520head%2520is%2520associated%2520with%2520a%2520distinct%2520marginal%250Atransition%2520kernel%2520to%2520model%2520diverse%2520parent-child%2520dependencies%2520effectively.%2520We%250Aprove%2520that%252C%2520given%2520sequences%2520generated%2520by%2520a%2520%2524K%2524-parent%2520DAG%252C%2520training%2520a%250Asingle-layer%252C%2520multi-head%2520transformer%2520via%2520gradient%2520ascent%2520converges%2520to%2520the%250Aglobal%2520optimum%2520in%2520polynomial%2520time.%2520Furthermore%252C%2520we%2520characterize%2520the%2520attention%250Ascore%2520patterns%2520at%2520convergence.%2520In%2520addition%252C%2520when%2520particularizing%2520the%250A%2524f%2524-divergence%2520to%2520the%2520KL%2520divergence%252C%2520the%2520learned%2520attention%2520scores%2520accurately%250Areflect%2520the%2520ground-truth%2520adjacency%2520matrix%252C%2520thereby%2520provably%2520recovering%2520the%250Aunderlying%2520graph%2520structure.%2520Experimental%2520results%2520validate%2520our%2520theoretical%250Afindings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20Provably%20Learn%20Directed%20Acyclic%20Graphs%20via%20Kernel-Guided%0A%20%20Mutual%20Information&entry.906535625=Yuan%20Cheng%20and%20Yu%20Huang%20and%20Zhe%20Xiong%20and%20Yingbin%20Liang%20and%20Vincent%20Y.%20F.%20Tan&entry.1292438233=%20%20Uncovering%20hidden%20graph%20structures%20underlying%20real-world%20data%20is%20a%20critical%0Achallenge%20with%20broad%20applications%20across%20scientific%20domains.%20Recently%2C%0Atransformer-based%20models%20leveraging%20the%20attention%20mechanism%20have%20demonstrated%0Astrong%20empirical%20success%20in%20capturing%20complex%20dependencies%20within%20graphs.%0AHowever%2C%20the%20theoretical%20understanding%20of%20their%20training%20dynamics%20has%20been%0Alimited%20to%20tree-like%20graphs%2C%20where%20each%20node%20depends%20on%20a%20single%20parent.%0AExtending%20provable%20guarantees%20to%20more%20general%20directed%20acyclic%20graphs%20%28DAGs%29%20--%0Awhich%20involve%20multiple%20parents%20per%20node%20--%20remains%20challenging%2C%20primarily%20due%0Ato%20the%20difficulty%20in%20designing%20training%20objectives%20that%20enable%20different%0Aattention%20heads%20to%20separately%20learn%20multiple%20different%20parent%20relationships.%0A%20%20In%20this%20work%2C%20we%20address%20this%20problem%20by%20introducing%20a%20novel%0Ainformation-theoretic%20metric%3A%20the%20kernel-guided%20mutual%20information%20%28KG-MI%29%2C%0Abased%20on%20the%20%24f%24-divergence.%20Our%20objective%20combines%20KG-MI%20with%20a%20multi-head%0Aattention%20framework%2C%20where%20each%20head%20is%20associated%20with%20a%20distinct%20marginal%0Atransition%20kernel%20to%20model%20diverse%20parent-child%20dependencies%20effectively.%20We%0Aprove%20that%2C%20given%20sequences%20generated%20by%20a%20%24K%24-parent%20DAG%2C%20training%20a%0Asingle-layer%2C%20multi-head%20transformer%20via%20gradient%20ascent%20converges%20to%20the%0Aglobal%20optimum%20in%20polynomial%20time.%20Furthermore%2C%20we%20characterize%20the%20attention%0Ascore%20patterns%20at%20convergence.%20In%20addition%2C%20when%20particularizing%20the%0A%24f%24-divergence%20to%20the%20KL%20divergence%2C%20the%20learned%20attention%20scores%20accurately%0Areflect%20the%20ground-truth%20adjacency%20matrix%2C%20thereby%20provably%20recovering%20the%0Aunderlying%20graph%20structure.%20Experimental%20results%20validate%20our%20theoretical%0Afindings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25542v1&entry.124074799=Read"},
{"title": "Exploring End-to-end Differentiable Neural Charged Particle Tracking --\n  A Loss Landscape Perspective", "author": "Tobias Kortus and Ralf Keidel and Nicolas R. Gauger", "abstract": "  Measurement and analysis of high energetic particles for scientific, medical\nor industrial applications is a complex procedure, requiring the design of\nsophisticated detector and data processing systems. The development of adaptive\nand differentiable software pipelines using a combination of conventional and\nmachine learning algorithms is therefore getting ever more important to\noptimize and operate the system efficiently while maintaining end-to-end (E2E)\ndifferentiability. We propose for the application of charged particle tracking\nan E2E differentiable decision-focused learning scheme using graph neural\nnetworks with combinatorial components solving a linear assignment problem for\neach detector layer. We demonstrate empirically that including differentiable\nvariations of discrete assignment operations allows for efficient network\noptimization, working better or on par with approaches that lack E2E\ndifferentiability. In additional studies, we dive deeper into the optimization\nprocess and provide further insights from a loss landscape perspective. We\ndemonstrate that while both methods converge into similar performing, globally\nwell-connected regions, they suffer under substantial predictive instability\nacross initialization and optimization methods, which can have unpredictable\nconsequences on the performance of downstream tasks such as image\nreconstruction. We also point out a dependency between the interpolation factor\nof the gradient estimator and the prediction stability of the model, suggesting\nthe choice of sufficiently small values. Given the strong global connectivity\nof learned solutions and the excellent training performance, we argue that E2E\ndifferentiability provides, besides the general availability of gradient\ninformation, an important tool for robust particle tracking to mitigate\nprediction instabilities by favoring solutions that perform well on downstream\ntasks.\n", "link": "http://arxiv.org/abs/2407.13420v2", "date": "2025-10-29", "relevancy": 2.1495, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5592}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5352}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20End-to-end%20Differentiable%20Neural%20Charged%20Particle%20Tracking%20--%0A%20%20A%20Loss%20Landscape%20Perspective&body=Title%3A%20Exploring%20End-to-end%20Differentiable%20Neural%20Charged%20Particle%20Tracking%20--%0A%20%20A%20Loss%20Landscape%20Perspective%0AAuthor%3A%20Tobias%20Kortus%20and%20Ralf%20Keidel%20and%20Nicolas%20R.%20Gauger%0AAbstract%3A%20%20%20Measurement%20and%20analysis%20of%20high%20energetic%20particles%20for%20scientific%2C%20medical%0Aor%20industrial%20applications%20is%20a%20complex%20procedure%2C%20requiring%20the%20design%20of%0Asophisticated%20detector%20and%20data%20processing%20systems.%20The%20development%20of%20adaptive%0Aand%20differentiable%20software%20pipelines%20using%20a%20combination%20of%20conventional%20and%0Amachine%20learning%20algorithms%20is%20therefore%20getting%20ever%20more%20important%20to%0Aoptimize%20and%20operate%20the%20system%20efficiently%20while%20maintaining%20end-to-end%20%28E2E%29%0Adifferentiability.%20We%20propose%20for%20the%20application%20of%20charged%20particle%20tracking%0Aan%20E2E%20differentiable%20decision-focused%20learning%20scheme%20using%20graph%20neural%0Anetworks%20with%20combinatorial%20components%20solving%20a%20linear%20assignment%20problem%20for%0Aeach%20detector%20layer.%20We%20demonstrate%20empirically%20that%20including%20differentiable%0Avariations%20of%20discrete%20assignment%20operations%20allows%20for%20efficient%20network%0Aoptimization%2C%20working%20better%20or%20on%20par%20with%20approaches%20that%20lack%20E2E%0Adifferentiability.%20In%20additional%20studies%2C%20we%20dive%20deeper%20into%20the%20optimization%0Aprocess%20and%20provide%20further%20insights%20from%20a%20loss%20landscape%20perspective.%20We%0Ademonstrate%20that%20while%20both%20methods%20converge%20into%20similar%20performing%2C%20globally%0Awell-connected%20regions%2C%20they%20suffer%20under%20substantial%20predictive%20instability%0Aacross%20initialization%20and%20optimization%20methods%2C%20which%20can%20have%20unpredictable%0Aconsequences%20on%20the%20performance%20of%20downstream%20tasks%20such%20as%20image%0Areconstruction.%20We%20also%20point%20out%20a%20dependency%20between%20the%20interpolation%20factor%0Aof%20the%20gradient%20estimator%20and%20the%20prediction%20stability%20of%20the%20model%2C%20suggesting%0Athe%20choice%20of%20sufficiently%20small%20values.%20Given%20the%20strong%20global%20connectivity%0Aof%20learned%20solutions%20and%20the%20excellent%20training%20performance%2C%20we%20argue%20that%20E2E%0Adifferentiability%20provides%2C%20besides%20the%20general%20availability%20of%20gradient%0Ainformation%2C%20an%20important%20tool%20for%20robust%20particle%20tracking%20to%20mitigate%0Aprediction%20instabilities%20by%20favoring%20solutions%20that%20perform%20well%20on%20downstream%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13420v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520End-to-end%2520Differentiable%2520Neural%2520Charged%2520Particle%2520Tracking%2520--%250A%2520%2520A%2520Loss%2520Landscape%2520Perspective%26entry.906535625%3DTobias%2520Kortus%2520and%2520Ralf%2520Keidel%2520and%2520Nicolas%2520R.%2520Gauger%26entry.1292438233%3D%2520%2520Measurement%2520and%2520analysis%2520of%2520high%2520energetic%2520particles%2520for%2520scientific%252C%2520medical%250Aor%2520industrial%2520applications%2520is%2520a%2520complex%2520procedure%252C%2520requiring%2520the%2520design%2520of%250Asophisticated%2520detector%2520and%2520data%2520processing%2520systems.%2520The%2520development%2520of%2520adaptive%250Aand%2520differentiable%2520software%2520pipelines%2520using%2520a%2520combination%2520of%2520conventional%2520and%250Amachine%2520learning%2520algorithms%2520is%2520therefore%2520getting%2520ever%2520more%2520important%2520to%250Aoptimize%2520and%2520operate%2520the%2520system%2520efficiently%2520while%2520maintaining%2520end-to-end%2520%2528E2E%2529%250Adifferentiability.%2520We%2520propose%2520for%2520the%2520application%2520of%2520charged%2520particle%2520tracking%250Aan%2520E2E%2520differentiable%2520decision-focused%2520learning%2520scheme%2520using%2520graph%2520neural%250Anetworks%2520with%2520combinatorial%2520components%2520solving%2520a%2520linear%2520assignment%2520problem%2520for%250Aeach%2520detector%2520layer.%2520We%2520demonstrate%2520empirically%2520that%2520including%2520differentiable%250Avariations%2520of%2520discrete%2520assignment%2520operations%2520allows%2520for%2520efficient%2520network%250Aoptimization%252C%2520working%2520better%2520or%2520on%2520par%2520with%2520approaches%2520that%2520lack%2520E2E%250Adifferentiability.%2520In%2520additional%2520studies%252C%2520we%2520dive%2520deeper%2520into%2520the%2520optimization%250Aprocess%2520and%2520provide%2520further%2520insights%2520from%2520a%2520loss%2520landscape%2520perspective.%2520We%250Ademonstrate%2520that%2520while%2520both%2520methods%2520converge%2520into%2520similar%2520performing%252C%2520globally%250Awell-connected%2520regions%252C%2520they%2520suffer%2520under%2520substantial%2520predictive%2520instability%250Aacross%2520initialization%2520and%2520optimization%2520methods%252C%2520which%2520can%2520have%2520unpredictable%250Aconsequences%2520on%2520the%2520performance%2520of%2520downstream%2520tasks%2520such%2520as%2520image%250Areconstruction.%2520We%2520also%2520point%2520out%2520a%2520dependency%2520between%2520the%2520interpolation%2520factor%250Aof%2520the%2520gradient%2520estimator%2520and%2520the%2520prediction%2520stability%2520of%2520the%2520model%252C%2520suggesting%250Athe%2520choice%2520of%2520sufficiently%2520small%2520values.%2520Given%2520the%2520strong%2520global%2520connectivity%250Aof%2520learned%2520solutions%2520and%2520the%2520excellent%2520training%2520performance%252C%2520we%2520argue%2520that%2520E2E%250Adifferentiability%2520provides%252C%2520besides%2520the%2520general%2520availability%2520of%2520gradient%250Ainformation%252C%2520an%2520important%2520tool%2520for%2520robust%2520particle%2520tracking%2520to%2520mitigate%250Aprediction%2520instabilities%2520by%2520favoring%2520solutions%2520that%2520perform%2520well%2520on%2520downstream%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13420v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20End-to-end%20Differentiable%20Neural%20Charged%20Particle%20Tracking%20--%0A%20%20A%20Loss%20Landscape%20Perspective&entry.906535625=Tobias%20Kortus%20and%20Ralf%20Keidel%20and%20Nicolas%20R.%20Gauger&entry.1292438233=%20%20Measurement%20and%20analysis%20of%20high%20energetic%20particles%20for%20scientific%2C%20medical%0Aor%20industrial%20applications%20is%20a%20complex%20procedure%2C%20requiring%20the%20design%20of%0Asophisticated%20detector%20and%20data%20processing%20systems.%20The%20development%20of%20adaptive%0Aand%20differentiable%20software%20pipelines%20using%20a%20combination%20of%20conventional%20and%0Amachine%20learning%20algorithms%20is%20therefore%20getting%20ever%20more%20important%20to%0Aoptimize%20and%20operate%20the%20system%20efficiently%20while%20maintaining%20end-to-end%20%28E2E%29%0Adifferentiability.%20We%20propose%20for%20the%20application%20of%20charged%20particle%20tracking%0Aan%20E2E%20differentiable%20decision-focused%20learning%20scheme%20using%20graph%20neural%0Anetworks%20with%20combinatorial%20components%20solving%20a%20linear%20assignment%20problem%20for%0Aeach%20detector%20layer.%20We%20demonstrate%20empirically%20that%20including%20differentiable%0Avariations%20of%20discrete%20assignment%20operations%20allows%20for%20efficient%20network%0Aoptimization%2C%20working%20better%20or%20on%20par%20with%20approaches%20that%20lack%20E2E%0Adifferentiability.%20In%20additional%20studies%2C%20we%20dive%20deeper%20into%20the%20optimization%0Aprocess%20and%20provide%20further%20insights%20from%20a%20loss%20landscape%20perspective.%20We%0Ademonstrate%20that%20while%20both%20methods%20converge%20into%20similar%20performing%2C%20globally%0Awell-connected%20regions%2C%20they%20suffer%20under%20substantial%20predictive%20instability%0Aacross%20initialization%20and%20optimization%20methods%2C%20which%20can%20have%20unpredictable%0Aconsequences%20on%20the%20performance%20of%20downstream%20tasks%20such%20as%20image%0Areconstruction.%20We%20also%20point%20out%20a%20dependency%20between%20the%20interpolation%20factor%0Aof%20the%20gradient%20estimator%20and%20the%20prediction%20stability%20of%20the%20model%2C%20suggesting%0Athe%20choice%20of%20sufficiently%20small%20values.%20Given%20the%20strong%20global%20connectivity%0Aof%20learned%20solutions%20and%20the%20excellent%20training%20performance%2C%20we%20argue%20that%20E2E%0Adifferentiability%20provides%2C%20besides%20the%20general%20availability%20of%20gradient%0Ainformation%2C%20an%20important%20tool%20for%20robust%20particle%20tracking%20to%20mitigate%0Aprediction%20instabilities%20by%20favoring%20solutions%20that%20perform%20well%20on%20downstream%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13420v2&entry.124074799=Read"},
{"title": "3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and\n  Latency", "author": "Minseok Jung and Abhas Ricky and Muhammad Rameez Chatni", "abstract": "  AI inference scaling is often tuned through 1D heuristics (a fixed reasoning\npasses) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail\nto consider cost and latency constraints. We introduce a 3D optimization\nframework that jointly calibrates accuracy, cost, and latency within a unified\ndecision space, enabling constraints-aware inference scaling. Using Monte Carlo\nsimulations across three representative scenarios and nine simulated large\nlanguage models, we evaluate four optimization methods to address the 3D\nmulti-objective optimization (MOO) problem. Framing inference scaling in MOO\nshapes a feasible space that 1D and 2D optimizations fail to capture, enabling\nenvironmentadaptive selection of the inference scaling k. Results show that\nknee-point optimization achieves the best balance, while accuracy-maximization\nremains favorable when precision is prioritized. The framework establishes a\ntheoretical foundation for deployment-aware inference scaling across diverse\noperational contexts.\n", "link": "http://arxiv.org/abs/2510.18905v2", "date": "2025-10-29", "relevancy": 2.1474, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5427}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Optimization%20for%20AI%20Inference%20Scaling%3A%20Balancing%20Accuracy%2C%20Cost%2C%20and%0A%20%20Latency&body=Title%3A%203D%20Optimization%20for%20AI%20Inference%20Scaling%3A%20Balancing%20Accuracy%2C%20Cost%2C%20and%0A%20%20Latency%0AAuthor%3A%20Minseok%20Jung%20and%20Abhas%20Ricky%20and%20Muhammad%20Rameez%20Chatni%0AAbstract%3A%20%20%20AI%20inference%20scaling%20is%20often%20tuned%20through%201D%20heuristics%20%28a%20fixed%20reasoning%0Apasses%29%20or%202D%20bivariate%20trade-offs%20%28e.g.%2C%20performance%20vs.%20compute%29%2C%20which%20fail%0Ato%20consider%20cost%20and%20latency%20constraints.%20We%20introduce%20a%203D%20optimization%0Aframework%20that%20jointly%20calibrates%20accuracy%2C%20cost%2C%20and%20latency%20within%20a%20unified%0Adecision%20space%2C%20enabling%20constraints-aware%20inference%20scaling.%20Using%20Monte%20Carlo%0Asimulations%20across%20three%20representative%20scenarios%20and%20nine%20simulated%20large%0Alanguage%20models%2C%20we%20evaluate%20four%20optimization%20methods%20to%20address%20the%203D%0Amulti-objective%20optimization%20%28MOO%29%20problem.%20Framing%20inference%20scaling%20in%20MOO%0Ashapes%20a%20feasible%20space%20that%201D%20and%202D%20optimizations%20fail%20to%20capture%2C%20enabling%0Aenvironmentadaptive%20selection%20of%20the%20inference%20scaling%20k.%20Results%20show%20that%0Aknee-point%20optimization%20achieves%20the%20best%20balance%2C%20while%20accuracy-maximization%0Aremains%20favorable%20when%20precision%20is%20prioritized.%20The%20framework%20establishes%20a%0Atheoretical%20foundation%20for%20deployment-aware%20inference%20scaling%20across%20diverse%0Aoperational%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18905v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Optimization%2520for%2520AI%2520Inference%2520Scaling%253A%2520Balancing%2520Accuracy%252C%2520Cost%252C%2520and%250A%2520%2520Latency%26entry.906535625%3DMinseok%2520Jung%2520and%2520Abhas%2520Ricky%2520and%2520Muhammad%2520Rameez%2520Chatni%26entry.1292438233%3D%2520%2520AI%2520inference%2520scaling%2520is%2520often%2520tuned%2520through%25201D%2520heuristics%2520%2528a%2520fixed%2520reasoning%250Apasses%2529%2520or%25202D%2520bivariate%2520trade-offs%2520%2528e.g.%252C%2520performance%2520vs.%2520compute%2529%252C%2520which%2520fail%250Ato%2520consider%2520cost%2520and%2520latency%2520constraints.%2520We%2520introduce%2520a%25203D%2520optimization%250Aframework%2520that%2520jointly%2520calibrates%2520accuracy%252C%2520cost%252C%2520and%2520latency%2520within%2520a%2520unified%250Adecision%2520space%252C%2520enabling%2520constraints-aware%2520inference%2520scaling.%2520Using%2520Monte%2520Carlo%250Asimulations%2520across%2520three%2520representative%2520scenarios%2520and%2520nine%2520simulated%2520large%250Alanguage%2520models%252C%2520we%2520evaluate%2520four%2520optimization%2520methods%2520to%2520address%2520the%25203D%250Amulti-objective%2520optimization%2520%2528MOO%2529%2520problem.%2520Framing%2520inference%2520scaling%2520in%2520MOO%250Ashapes%2520a%2520feasible%2520space%2520that%25201D%2520and%25202D%2520optimizations%2520fail%2520to%2520capture%252C%2520enabling%250Aenvironmentadaptive%2520selection%2520of%2520the%2520inference%2520scaling%2520k.%2520Results%2520show%2520that%250Aknee-point%2520optimization%2520achieves%2520the%2520best%2520balance%252C%2520while%2520accuracy-maximization%250Aremains%2520favorable%2520when%2520precision%2520is%2520prioritized.%2520The%2520framework%2520establishes%2520a%250Atheoretical%2520foundation%2520for%2520deployment-aware%2520inference%2520scaling%2520across%2520diverse%250Aoperational%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18905v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Optimization%20for%20AI%20Inference%20Scaling%3A%20Balancing%20Accuracy%2C%20Cost%2C%20and%0A%20%20Latency&entry.906535625=Minseok%20Jung%20and%20Abhas%20Ricky%20and%20Muhammad%20Rameez%20Chatni&entry.1292438233=%20%20AI%20inference%20scaling%20is%20often%20tuned%20through%201D%20heuristics%20%28a%20fixed%20reasoning%0Apasses%29%20or%202D%20bivariate%20trade-offs%20%28e.g.%2C%20performance%20vs.%20compute%29%2C%20which%20fail%0Ato%20consider%20cost%20and%20latency%20constraints.%20We%20introduce%20a%203D%20optimization%0Aframework%20that%20jointly%20calibrates%20accuracy%2C%20cost%2C%20and%20latency%20within%20a%20unified%0Adecision%20space%2C%20enabling%20constraints-aware%20inference%20scaling.%20Using%20Monte%20Carlo%0Asimulations%20across%20three%20representative%20scenarios%20and%20nine%20simulated%20large%0Alanguage%20models%2C%20we%20evaluate%20four%20optimization%20methods%20to%20address%20the%203D%0Amulti-objective%20optimization%20%28MOO%29%20problem.%20Framing%20inference%20scaling%20in%20MOO%0Ashapes%20a%20feasible%20space%20that%201D%20and%202D%20optimizations%20fail%20to%20capture%2C%20enabling%0Aenvironmentadaptive%20selection%20of%20the%20inference%20scaling%20k.%20Results%20show%20that%0Aknee-point%20optimization%20achieves%20the%20best%20balance%2C%20while%20accuracy-maximization%0Aremains%20favorable%20when%20precision%20is%20prioritized.%20The%20framework%20establishes%20a%0Atheoretical%20foundation%20for%20deployment-aware%20inference%20scaling%20across%20diverse%0Aoperational%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18905v2&entry.124074799=Read"},
{"title": "Integrating Legal and Logical Specifications in Perception, Prediction,\n  and Planning for Automated Driving: A Survey of Methods", "author": "Kumar Manas and Mert Keser and Alois Knoll", "abstract": "  This survey provides an analysis of current methodologies integrating legal\nand logical specifications into the perception, prediction, and planning\nmodules of automated driving systems. We systematically explore techniques\nranging from logic-based frameworks to computational legal reasoning\napproaches, emphasizing their capability to ensure regulatory compliance and\ninterpretability in dynamic and uncertain driving environments. A central\nfinding is that significant challenges arise at the intersection of perceptual\nreliability, legal compliance, and decision-making justifiability. To\nsystematically analyze these challenges, we introduce a taxonomy categorizing\nexisting approaches by their theoretical foundations, architectural\nimplementations, and validation strategies. We particularly focus on methods\nthat address perceptual uncertainty and incorporate explicit legal norms,\nfacilitating decisions that are both technically robust and legally defensible.\nThe review covers neural-symbolic integration methods for perception,\nlogic-driven rule representation, and norm-aware prediction strategies, all\ncontributing toward transparent and accountable autonomous vehicle operation.\nWe highlight critical open questions and practical trade-offs that must be\naddressed, offering multidisciplinary insights from engineering, logic, and law\nto guide future developments in legally compliant autonomous driving systems.\n", "link": "http://arxiv.org/abs/2510.25386v1", "date": "2025-10-29", "relevancy": 2.1393, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5688}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5461}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Legal%20and%20Logical%20Specifications%20in%20Perception%2C%20Prediction%2C%0A%20%20and%20Planning%20for%20Automated%20Driving%3A%20A%20Survey%20of%20Methods&body=Title%3A%20Integrating%20Legal%20and%20Logical%20Specifications%20in%20Perception%2C%20Prediction%2C%0A%20%20and%20Planning%20for%20Automated%20Driving%3A%20A%20Survey%20of%20Methods%0AAuthor%3A%20Kumar%20Manas%20and%20Mert%20Keser%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20This%20survey%20provides%20an%20analysis%20of%20current%20methodologies%20integrating%20legal%0Aand%20logical%20specifications%20into%20the%20perception%2C%20prediction%2C%20and%20planning%0Amodules%20of%20automated%20driving%20systems.%20We%20systematically%20explore%20techniques%0Aranging%20from%20logic-based%20frameworks%20to%20computational%20legal%20reasoning%0Aapproaches%2C%20emphasizing%20their%20capability%20to%20ensure%20regulatory%20compliance%20and%0Ainterpretability%20in%20dynamic%20and%20uncertain%20driving%20environments.%20A%20central%0Afinding%20is%20that%20significant%20challenges%20arise%20at%20the%20intersection%20of%20perceptual%0Areliability%2C%20legal%20compliance%2C%20and%20decision-making%20justifiability.%20To%0Asystematically%20analyze%20these%20challenges%2C%20we%20introduce%20a%20taxonomy%20categorizing%0Aexisting%20approaches%20by%20their%20theoretical%20foundations%2C%20architectural%0Aimplementations%2C%20and%20validation%20strategies.%20We%20particularly%20focus%20on%20methods%0Athat%20address%20perceptual%20uncertainty%20and%20incorporate%20explicit%20legal%20norms%2C%0Afacilitating%20decisions%20that%20are%20both%20technically%20robust%20and%20legally%20defensible.%0AThe%20review%20covers%20neural-symbolic%20integration%20methods%20for%20perception%2C%0Alogic-driven%20rule%20representation%2C%20and%20norm-aware%20prediction%20strategies%2C%20all%0Acontributing%20toward%20transparent%20and%20accountable%20autonomous%20vehicle%20operation.%0AWe%20highlight%20critical%20open%20questions%20and%20practical%20trade-offs%20that%20must%20be%0Aaddressed%2C%20offering%20multidisciplinary%20insights%20from%20engineering%2C%20logic%2C%20and%20law%0Ato%20guide%20future%20developments%20in%20legally%20compliant%20autonomous%20driving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Legal%2520and%2520Logical%2520Specifications%2520in%2520Perception%252C%2520Prediction%252C%250A%2520%2520and%2520Planning%2520for%2520Automated%2520Driving%253A%2520A%2520Survey%2520of%2520Methods%26entry.906535625%3DKumar%2520Manas%2520and%2520Mert%2520Keser%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520This%2520survey%2520provides%2520an%2520analysis%2520of%2520current%2520methodologies%2520integrating%2520legal%250Aand%2520logical%2520specifications%2520into%2520the%2520perception%252C%2520prediction%252C%2520and%2520planning%250Amodules%2520of%2520automated%2520driving%2520systems.%2520We%2520systematically%2520explore%2520techniques%250Aranging%2520from%2520logic-based%2520frameworks%2520to%2520computational%2520legal%2520reasoning%250Aapproaches%252C%2520emphasizing%2520their%2520capability%2520to%2520ensure%2520regulatory%2520compliance%2520and%250Ainterpretability%2520in%2520dynamic%2520and%2520uncertain%2520driving%2520environments.%2520A%2520central%250Afinding%2520is%2520that%2520significant%2520challenges%2520arise%2520at%2520the%2520intersection%2520of%2520perceptual%250Areliability%252C%2520legal%2520compliance%252C%2520and%2520decision-making%2520justifiability.%2520To%250Asystematically%2520analyze%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520taxonomy%2520categorizing%250Aexisting%2520approaches%2520by%2520their%2520theoretical%2520foundations%252C%2520architectural%250Aimplementations%252C%2520and%2520validation%2520strategies.%2520We%2520particularly%2520focus%2520on%2520methods%250Athat%2520address%2520perceptual%2520uncertainty%2520and%2520incorporate%2520explicit%2520legal%2520norms%252C%250Afacilitating%2520decisions%2520that%2520are%2520both%2520technically%2520robust%2520and%2520legally%2520defensible.%250AThe%2520review%2520covers%2520neural-symbolic%2520integration%2520methods%2520for%2520perception%252C%250Alogic-driven%2520rule%2520representation%252C%2520and%2520norm-aware%2520prediction%2520strategies%252C%2520all%250Acontributing%2520toward%2520transparent%2520and%2520accountable%2520autonomous%2520vehicle%2520operation.%250AWe%2520highlight%2520critical%2520open%2520questions%2520and%2520practical%2520trade-offs%2520that%2520must%2520be%250Aaddressed%252C%2520offering%2520multidisciplinary%2520insights%2520from%2520engineering%252C%2520logic%252C%2520and%2520law%250Ato%2520guide%2520future%2520developments%2520in%2520legally%2520compliant%2520autonomous%2520driving%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Legal%20and%20Logical%20Specifications%20in%20Perception%2C%20Prediction%2C%0A%20%20and%20Planning%20for%20Automated%20Driving%3A%20A%20Survey%20of%20Methods&entry.906535625=Kumar%20Manas%20and%20Mert%20Keser%20and%20Alois%20Knoll&entry.1292438233=%20%20This%20survey%20provides%20an%20analysis%20of%20current%20methodologies%20integrating%20legal%0Aand%20logical%20specifications%20into%20the%20perception%2C%20prediction%2C%20and%20planning%0Amodules%20of%20automated%20driving%20systems.%20We%20systematically%20explore%20techniques%0Aranging%20from%20logic-based%20frameworks%20to%20computational%20legal%20reasoning%0Aapproaches%2C%20emphasizing%20their%20capability%20to%20ensure%20regulatory%20compliance%20and%0Ainterpretability%20in%20dynamic%20and%20uncertain%20driving%20environments.%20A%20central%0Afinding%20is%20that%20significant%20challenges%20arise%20at%20the%20intersection%20of%20perceptual%0Areliability%2C%20legal%20compliance%2C%20and%20decision-making%20justifiability.%20To%0Asystematically%20analyze%20these%20challenges%2C%20we%20introduce%20a%20taxonomy%20categorizing%0Aexisting%20approaches%20by%20their%20theoretical%20foundations%2C%20architectural%0Aimplementations%2C%20and%20validation%20strategies.%20We%20particularly%20focus%20on%20methods%0Athat%20address%20perceptual%20uncertainty%20and%20incorporate%20explicit%20legal%20norms%2C%0Afacilitating%20decisions%20that%20are%20both%20technically%20robust%20and%20legally%20defensible.%0AThe%20review%20covers%20neural-symbolic%20integration%20methods%20for%20perception%2C%0Alogic-driven%20rule%20representation%2C%20and%20norm-aware%20prediction%20strategies%2C%20all%0Acontributing%20toward%20transparent%20and%20accountable%20autonomous%20vehicle%20operation.%0AWe%20highlight%20critical%20open%20questions%20and%20practical%20trade-offs%20that%20must%20be%0Aaddressed%2C%20offering%20multidisciplinary%20insights%20from%20engineering%2C%20logic%2C%20and%20law%0Ato%20guide%20future%20developments%20in%20legally%20compliant%20autonomous%20driving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25386v1&entry.124074799=Read"},
{"title": "Transformers from Compressed Representations", "author": "Juan C. Leon Alcazar and Mattia Soldan and Mohammad Saatialsoruji and Alejandro Pardo and Hani Itani and Juan Camilo Perez and Bernard Ghanem", "abstract": "  Compressed file formats are the corner stone of efficient data storage and\ntransmission, yet their potential for representation learning remains largely\nunderexplored. We introduce TEMPEST (TransformErs froM comPressed\nrEpreSenTations), a method that exploits the inherent byte-stream structure of\ncompressed files to design an effective tokenization and encoding strategy. By\nleveraging this compact encoding, a standard transformer can directly learn\nsemantic representations from compressed data streams, bypassing the need for\nraw byte-level processing or full media decoding. Our proposal substantially\nreduces the number of tokens required for semantic classification, thereby\nlowering both computational complexity and memory usage. Through extensive\nexperiments across diverse datasets, coding schemes, and modalities, we show\nthat TEMPEST achieves accuracy competitive wit the state-of-the-art while\ndelivering efficiency gains in memory and compute.\n", "link": "http://arxiv.org/abs/2510.23665v2", "date": "2025-10-29", "relevancy": 2.1376, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5529}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5422}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20from%20Compressed%20Representations&body=Title%3A%20Transformers%20from%20Compressed%20Representations%0AAuthor%3A%20Juan%20C.%20Leon%20Alcazar%20and%20Mattia%20Soldan%20and%20Mohammad%20Saatialsoruji%20and%20Alejandro%20Pardo%20and%20Hani%20Itani%20and%20Juan%20Camilo%20Perez%20and%20Bernard%20Ghanem%0AAbstract%3A%20%20%20Compressed%20file%20formats%20are%20the%20corner%20stone%20of%20efficient%20data%20storage%20and%0Atransmission%2C%20yet%20their%20potential%20for%20representation%20learning%20remains%20largely%0Aunderexplored.%20We%20introduce%20TEMPEST%20%28TransformErs%20froM%20comPressed%0ArEpreSenTations%29%2C%20a%20method%20that%20exploits%20the%20inherent%20byte-stream%20structure%20of%0Acompressed%20files%20to%20design%20an%20effective%20tokenization%20and%20encoding%20strategy.%20By%0Aleveraging%20this%20compact%20encoding%2C%20a%20standard%20transformer%20can%20directly%20learn%0Asemantic%20representations%20from%20compressed%20data%20streams%2C%20bypassing%20the%20need%20for%0Araw%20byte-level%20processing%20or%20full%20media%20decoding.%20Our%20proposal%20substantially%0Areduces%20the%20number%20of%20tokens%20required%20for%20semantic%20classification%2C%20thereby%0Alowering%20both%20computational%20complexity%20and%20memory%20usage.%20Through%20extensive%0Aexperiments%20across%20diverse%20datasets%2C%20coding%20schemes%2C%20and%20modalities%2C%20we%20show%0Athat%20TEMPEST%20achieves%20accuracy%20competitive%20wit%20the%20state-of-the-art%20while%0Adelivering%20efficiency%20gains%20in%20memory%20and%20compute.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.23665v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520from%2520Compressed%2520Representations%26entry.906535625%3DJuan%2520C.%2520Leon%2520Alcazar%2520and%2520Mattia%2520Soldan%2520and%2520Mohammad%2520Saatialsoruji%2520and%2520Alejandro%2520Pardo%2520and%2520Hani%2520Itani%2520and%2520Juan%2520Camilo%2520Perez%2520and%2520Bernard%2520Ghanem%26entry.1292438233%3D%2520%2520Compressed%2520file%2520formats%2520are%2520the%2520corner%2520stone%2520of%2520efficient%2520data%2520storage%2520and%250Atransmission%252C%2520yet%2520their%2520potential%2520for%2520representation%2520learning%2520remains%2520largely%250Aunderexplored.%2520We%2520introduce%2520TEMPEST%2520%2528TransformErs%2520froM%2520comPressed%250ArEpreSenTations%2529%252C%2520a%2520method%2520that%2520exploits%2520the%2520inherent%2520byte-stream%2520structure%2520of%250Acompressed%2520files%2520to%2520design%2520an%2520effective%2520tokenization%2520and%2520encoding%2520strategy.%2520By%250Aleveraging%2520this%2520compact%2520encoding%252C%2520a%2520standard%2520transformer%2520can%2520directly%2520learn%250Asemantic%2520representations%2520from%2520compressed%2520data%2520streams%252C%2520bypassing%2520the%2520need%2520for%250Araw%2520byte-level%2520processing%2520or%2520full%2520media%2520decoding.%2520Our%2520proposal%2520substantially%250Areduces%2520the%2520number%2520of%2520tokens%2520required%2520for%2520semantic%2520classification%252C%2520thereby%250Alowering%2520both%2520computational%2520complexity%2520and%2520memory%2520usage.%2520Through%2520extensive%250Aexperiments%2520across%2520diverse%2520datasets%252C%2520coding%2520schemes%252C%2520and%2520modalities%252C%2520we%2520show%250Athat%2520TEMPEST%2520achieves%2520accuracy%2520competitive%2520wit%2520the%2520state-of-the-art%2520while%250Adelivering%2520efficiency%2520gains%2520in%2520memory%2520and%2520compute.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23665v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20from%20Compressed%20Representations&entry.906535625=Juan%20C.%20Leon%20Alcazar%20and%20Mattia%20Soldan%20and%20Mohammad%20Saatialsoruji%20and%20Alejandro%20Pardo%20and%20Hani%20Itani%20and%20Juan%20Camilo%20Perez%20and%20Bernard%20Ghanem&entry.1292438233=%20%20Compressed%20file%20formats%20are%20the%20corner%20stone%20of%20efficient%20data%20storage%20and%0Atransmission%2C%20yet%20their%20potential%20for%20representation%20learning%20remains%20largely%0Aunderexplored.%20We%20introduce%20TEMPEST%20%28TransformErs%20froM%20comPressed%0ArEpreSenTations%29%2C%20a%20method%20that%20exploits%20the%20inherent%20byte-stream%20structure%20of%0Acompressed%20files%20to%20design%20an%20effective%20tokenization%20and%20encoding%20strategy.%20By%0Aleveraging%20this%20compact%20encoding%2C%20a%20standard%20transformer%20can%20directly%20learn%0Asemantic%20representations%20from%20compressed%20data%20streams%2C%20bypassing%20the%20need%20for%0Araw%20byte-level%20processing%20or%20full%20media%20decoding.%20Our%20proposal%20substantially%0Areduces%20the%20number%20of%20tokens%20required%20for%20semantic%20classification%2C%20thereby%0Alowering%20both%20computational%20complexity%20and%20memory%20usage.%20Through%20extensive%0Aexperiments%20across%20diverse%20datasets%2C%20coding%20schemes%2C%20and%20modalities%2C%20we%20show%0Athat%20TEMPEST%20achieves%20accuracy%20competitive%20wit%20the%20state-of-the-art%20while%0Adelivering%20efficiency%20gains%20in%20memory%20and%20compute.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.23665v2&entry.124074799=Read"},
{"title": "A Convexity-dependent Two-Phase Training Algorithm for Deep Neural\n  Networks", "author": "Tomas Hrycej and Bernhard Bermeitinger and Massimo Pavone and G\u00f6tz-Henrik Wiegand and Siegfried Handschuh", "abstract": "  The key task of machine learning is to minimize the loss function that\nmeasures the model fit to the training data. The numerical methods to do this\nefficiently depend on the properties of the loss function. The most decisive\namong these properties is the convexity or non-convexity of the loss function.\nThe fact that the loss function can have, and frequently has, non-convex\nregions has led to a widespread commitment to non-convex methods such as Adam.\nHowever, a local minimum implies that, in some environment around it, the\nfunction is convex. In this environment, second-order minimizing methods such\nas the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We\npropose a novel framework grounded in the hypothesis that loss functions in\nreal-world tasks swap from initial non-convexity to convexity towards the\noptimum. This is a property we leverage to design an innovative two-phase\noptimization algorithm. The presented algorithm detects the swap point by\nobserving the gradient norm dependence on the loss. In these regions,\nnon-convex (Adam) and convex (CG) algorithms are used, respectively. Computing\nexperiments confirm the hypothesis that this simple convexity structure is\nfrequent enough to be practically exploited to substantially improve\nconvergence and accuracy.\n", "link": "http://arxiv.org/abs/2510.25366v1", "date": "2025-10-29", "relevancy": 2.1312, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5782}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5023}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Convexity-dependent%20Two-Phase%20Training%20Algorithm%20for%20Deep%20Neural%0A%20%20Networks&body=Title%3A%20A%20Convexity-dependent%20Two-Phase%20Training%20Algorithm%20for%20Deep%20Neural%0A%20%20Networks%0AAuthor%3A%20Tomas%20Hrycej%20and%20Bernhard%20Bermeitinger%20and%20Massimo%20Pavone%20and%20G%C3%B6tz-Henrik%20Wiegand%20and%20Siegfried%20Handschuh%0AAbstract%3A%20%20%20The%20key%20task%20of%20machine%20learning%20is%20to%20minimize%20the%20loss%20function%20that%0Ameasures%20the%20model%20fit%20to%20the%20training%20data.%20The%20numerical%20methods%20to%20do%20this%0Aefficiently%20depend%20on%20the%20properties%20of%20the%20loss%20function.%20The%20most%20decisive%0Aamong%20these%20properties%20is%20the%20convexity%20or%20non-convexity%20of%20the%20loss%20function.%0AThe%20fact%20that%20the%20loss%20function%20can%20have%2C%20and%20frequently%20has%2C%20non-convex%0Aregions%20has%20led%20to%20a%20widespread%20commitment%20to%20non-convex%20methods%20such%20as%20Adam.%0AHowever%2C%20a%20local%20minimum%20implies%20that%2C%20in%20some%20environment%20around%20it%2C%20the%0Afunction%20is%20convex.%20In%20this%20environment%2C%20second-order%20minimizing%20methods%20such%0Aas%20the%20Conjugate%20Gradient%20%28CG%29%20give%20a%20guaranteed%20superlinear%20convergence.%20We%0Apropose%20a%20novel%20framework%20grounded%20in%20the%20hypothesis%20that%20loss%20functions%20in%0Areal-world%20tasks%20swap%20from%20initial%20non-convexity%20to%20convexity%20towards%20the%0Aoptimum.%20This%20is%20a%20property%20we%20leverage%20to%20design%20an%20innovative%20two-phase%0Aoptimization%20algorithm.%20The%20presented%20algorithm%20detects%20the%20swap%20point%20by%0Aobserving%20the%20gradient%20norm%20dependence%20on%20the%20loss.%20In%20these%20regions%2C%0Anon-convex%20%28Adam%29%20and%20convex%20%28CG%29%20algorithms%20are%20used%2C%20respectively.%20Computing%0Aexperiments%20confirm%20the%20hypothesis%20that%20this%20simple%20convexity%20structure%20is%0Afrequent%20enough%20to%20be%20practically%20exploited%20to%20substantially%20improve%0Aconvergence%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Convexity-dependent%2520Two-Phase%2520Training%2520Algorithm%2520for%2520Deep%2520Neural%250A%2520%2520Networks%26entry.906535625%3DTomas%2520Hrycej%2520and%2520Bernhard%2520Bermeitinger%2520and%2520Massimo%2520Pavone%2520and%2520G%25C3%25B6tz-Henrik%2520Wiegand%2520and%2520Siegfried%2520Handschuh%26entry.1292438233%3D%2520%2520The%2520key%2520task%2520of%2520machine%2520learning%2520is%2520to%2520minimize%2520the%2520loss%2520function%2520that%250Ameasures%2520the%2520model%2520fit%2520to%2520the%2520training%2520data.%2520The%2520numerical%2520methods%2520to%2520do%2520this%250Aefficiently%2520depend%2520on%2520the%2520properties%2520of%2520the%2520loss%2520function.%2520The%2520most%2520decisive%250Aamong%2520these%2520properties%2520is%2520the%2520convexity%2520or%2520non-convexity%2520of%2520the%2520loss%2520function.%250AThe%2520fact%2520that%2520the%2520loss%2520function%2520can%2520have%252C%2520and%2520frequently%2520has%252C%2520non-convex%250Aregions%2520has%2520led%2520to%2520a%2520widespread%2520commitment%2520to%2520non-convex%2520methods%2520such%2520as%2520Adam.%250AHowever%252C%2520a%2520local%2520minimum%2520implies%2520that%252C%2520in%2520some%2520environment%2520around%2520it%252C%2520the%250Afunction%2520is%2520convex.%2520In%2520this%2520environment%252C%2520second-order%2520minimizing%2520methods%2520such%250Aas%2520the%2520Conjugate%2520Gradient%2520%2528CG%2529%2520give%2520a%2520guaranteed%2520superlinear%2520convergence.%2520We%250Apropose%2520a%2520novel%2520framework%2520grounded%2520in%2520the%2520hypothesis%2520that%2520loss%2520functions%2520in%250Areal-world%2520tasks%2520swap%2520from%2520initial%2520non-convexity%2520to%2520convexity%2520towards%2520the%250Aoptimum.%2520This%2520is%2520a%2520property%2520we%2520leverage%2520to%2520design%2520an%2520innovative%2520two-phase%250Aoptimization%2520algorithm.%2520The%2520presented%2520algorithm%2520detects%2520the%2520swap%2520point%2520by%250Aobserving%2520the%2520gradient%2520norm%2520dependence%2520on%2520the%2520loss.%2520In%2520these%2520regions%252C%250Anon-convex%2520%2528Adam%2529%2520and%2520convex%2520%2528CG%2529%2520algorithms%2520are%2520used%252C%2520respectively.%2520Computing%250Aexperiments%2520confirm%2520the%2520hypothesis%2520that%2520this%2520simple%2520convexity%2520structure%2520is%250Afrequent%2520enough%2520to%2520be%2520practically%2520exploited%2520to%2520substantially%2520improve%250Aconvergence%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Convexity-dependent%20Two-Phase%20Training%20Algorithm%20for%20Deep%20Neural%0A%20%20Networks&entry.906535625=Tomas%20Hrycej%20and%20Bernhard%20Bermeitinger%20and%20Massimo%20Pavone%20and%20G%C3%B6tz-Henrik%20Wiegand%20and%20Siegfried%20Handschuh&entry.1292438233=%20%20The%20key%20task%20of%20machine%20learning%20is%20to%20minimize%20the%20loss%20function%20that%0Ameasures%20the%20model%20fit%20to%20the%20training%20data.%20The%20numerical%20methods%20to%20do%20this%0Aefficiently%20depend%20on%20the%20properties%20of%20the%20loss%20function.%20The%20most%20decisive%0Aamong%20these%20properties%20is%20the%20convexity%20or%20non-convexity%20of%20the%20loss%20function.%0AThe%20fact%20that%20the%20loss%20function%20can%20have%2C%20and%20frequently%20has%2C%20non-convex%0Aregions%20has%20led%20to%20a%20widespread%20commitment%20to%20non-convex%20methods%20such%20as%20Adam.%0AHowever%2C%20a%20local%20minimum%20implies%20that%2C%20in%20some%20environment%20around%20it%2C%20the%0Afunction%20is%20convex.%20In%20this%20environment%2C%20second-order%20minimizing%20methods%20such%0Aas%20the%20Conjugate%20Gradient%20%28CG%29%20give%20a%20guaranteed%20superlinear%20convergence.%20We%0Apropose%20a%20novel%20framework%20grounded%20in%20the%20hypothesis%20that%20loss%20functions%20in%0Areal-world%20tasks%20swap%20from%20initial%20non-convexity%20to%20convexity%20towards%20the%0Aoptimum.%20This%20is%20a%20property%20we%20leverage%20to%20design%20an%20innovative%20two-phase%0Aoptimization%20algorithm.%20The%20presented%20algorithm%20detects%20the%20swap%20point%20by%0Aobserving%20the%20gradient%20norm%20dependence%20on%20the%20loss.%20In%20these%20regions%2C%0Anon-convex%20%28Adam%29%20and%20convex%20%28CG%29%20algorithms%20are%20used%2C%20respectively.%20Computing%0Aexperiments%20confirm%20the%20hypothesis%20that%20this%20simple%20convexity%20structure%20is%0Afrequent%20enough%20to%20be%20practically%20exploited%20to%20substantially%20improve%0Aconvergence%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25366v1&entry.124074799=Read"},
{"title": "SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context\n  Learning", "author": "Melanie Rieff and Maya Varma and Ossian Rabow and Subathra Adithan and Julie Kim and Ken Chang and Hannah Lee and Nidhi Rohatgi and Christian Bluethgen and Mohamed S. Muneer and Jean-Benoit Delbrouck and Michael Moor", "abstract": "  Multimodal in-context learning (ICL) remains underexplored despite\nsignificant potential for domains such as medicine. Clinicians routinely\nencounter diverse, specialized tasks requiring adaptation from limited\nexamples, such as drawing insights from a few relevant prior cases or\nconsidering a constrained set of differential diagnoses. While multimodal large\nlanguage models (MLLMs) have shown advances in medical visual question\nanswering (VQA), their ability to learn multimodal tasks from context is\nlargely unknown. We introduce SMMILE, the first expert-driven multimodal ICL\nbenchmark for medical tasks. Eleven medical experts curated problems, each\nincluding a multimodal query and multimodal in-context examples as task\ndemonstrations. SMMILE encompasses 111 problems (517 question-image-answer\ntriplets) covering 6 medical specialties and 13 imaging modalities. We further\nintroduce SMMILE++, an augmented variant with 1038 permuted problems. A\ncomprehensive evaluation of 15 MLLMs demonstrates that most models exhibit\nmoderate to poor multimodal ICL ability in medical tasks. In open-ended\nevaluations, ICL contributes only an 8% average improvement over zero-shot on\nSMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant\nin-context examples: even a single noisy or irrelevant example can degrade\nperformance by up to 9.5%. Moreover, we observe that MLLMs are affected by a\nrecency bias, where placing the most relevant example last can lead to\nsubstantial performance improvements of up to 71%. Our findings highlight\ncritical limitations and biases in current MLLMs when learning multimodal\nmedical tasks from context. SMMILE is available at\nhttps://smmile-benchmark.github.io.\n", "link": "http://arxiv.org/abs/2506.21355v2", "date": "2025-10-29", "relevancy": 2.1111, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5161}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMMILE%3A%20An%20Expert-Driven%20Benchmark%20for%20Multimodal%20Medical%20In-Context%0A%20%20Learning&body=Title%3A%20SMMILE%3A%20An%20Expert-Driven%20Benchmark%20for%20Multimodal%20Medical%20In-Context%0A%20%20Learning%0AAuthor%3A%20Melanie%20Rieff%20and%20Maya%20Varma%20and%20Ossian%20Rabow%20and%20Subathra%20Adithan%20and%20Julie%20Kim%20and%20Ken%20Chang%20and%20Hannah%20Lee%20and%20Nidhi%20Rohatgi%20and%20Christian%20Bluethgen%20and%20Mohamed%20S.%20Muneer%20and%20Jean-Benoit%20Delbrouck%20and%20Michael%20Moor%0AAbstract%3A%20%20%20Multimodal%20in-context%20learning%20%28ICL%29%20remains%20underexplored%20despite%0Asignificant%20potential%20for%20domains%20such%20as%20medicine.%20Clinicians%20routinely%0Aencounter%20diverse%2C%20specialized%20tasks%20requiring%20adaptation%20from%20limited%0Aexamples%2C%20such%20as%20drawing%20insights%20from%20a%20few%20relevant%20prior%20cases%20or%0Aconsidering%20a%20constrained%20set%20of%20differential%20diagnoses.%20While%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20have%20shown%20advances%20in%20medical%20visual%20question%0Aanswering%20%28VQA%29%2C%20their%20ability%20to%20learn%20multimodal%20tasks%20from%20context%20is%0Alargely%20unknown.%20We%20introduce%20SMMILE%2C%20the%20first%20expert-driven%20multimodal%20ICL%0Abenchmark%20for%20medical%20tasks.%20Eleven%20medical%20experts%20curated%20problems%2C%20each%0Aincluding%20a%20multimodal%20query%20and%20multimodal%20in-context%20examples%20as%20task%0Ademonstrations.%20SMMILE%20encompasses%20111%20problems%20%28517%20question-image-answer%0Atriplets%29%20covering%206%20medical%20specialties%20and%2013%20imaging%20modalities.%20We%20further%0Aintroduce%20SMMILE%2B%2B%2C%20an%20augmented%20variant%20with%201038%20permuted%20problems.%20A%0Acomprehensive%20evaluation%20of%2015%20MLLMs%20demonstrates%20that%20most%20models%20exhibit%0Amoderate%20to%20poor%20multimodal%20ICL%20ability%20in%20medical%20tasks.%20In%20open-ended%0Aevaluations%2C%20ICL%20contributes%20only%20an%208%25%20average%20improvement%20over%20zero-shot%20on%0ASMMILE%20and%209.4%25%20on%20SMMILE%2B%2B.%20We%20observe%20a%20susceptibility%20for%20irrelevant%0Ain-context%20examples%3A%20even%20a%20single%20noisy%20or%20irrelevant%20example%20can%20degrade%0Aperformance%20by%20up%20to%209.5%25.%20Moreover%2C%20we%20observe%20that%20MLLMs%20are%20affected%20by%20a%0Arecency%20bias%2C%20where%20placing%20the%20most%20relevant%20example%20last%20can%20lead%20to%0Asubstantial%20performance%20improvements%20of%20up%20to%2071%25.%20Our%20findings%20highlight%0Acritical%20limitations%20and%20biases%20in%20current%20MLLMs%20when%20learning%20multimodal%0Amedical%20tasks%20from%20context.%20SMMILE%20is%20available%20at%0Ahttps%3A//smmile-benchmark.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMMILE%253A%2520An%2520Expert-Driven%2520Benchmark%2520for%2520Multimodal%2520Medical%2520In-Context%250A%2520%2520Learning%26entry.906535625%3DMelanie%2520Rieff%2520and%2520Maya%2520Varma%2520and%2520Ossian%2520Rabow%2520and%2520Subathra%2520Adithan%2520and%2520Julie%2520Kim%2520and%2520Ken%2520Chang%2520and%2520Hannah%2520Lee%2520and%2520Nidhi%2520Rohatgi%2520and%2520Christian%2520Bluethgen%2520and%2520Mohamed%2520S.%2520Muneer%2520and%2520Jean-Benoit%2520Delbrouck%2520and%2520Michael%2520Moor%26entry.1292438233%3D%2520%2520Multimodal%2520in-context%2520learning%2520%2528ICL%2529%2520remains%2520underexplored%2520despite%250Asignificant%2520potential%2520for%2520domains%2520such%2520as%2520medicine.%2520Clinicians%2520routinely%250Aencounter%2520diverse%252C%2520specialized%2520tasks%2520requiring%2520adaptation%2520from%2520limited%250Aexamples%252C%2520such%2520as%2520drawing%2520insights%2520from%2520a%2520few%2520relevant%2520prior%2520cases%2520or%250Aconsidering%2520a%2520constrained%2520set%2520of%2520differential%2520diagnoses.%2520While%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529%2520have%2520shown%2520advances%2520in%2520medical%2520visual%2520question%250Aanswering%2520%2528VQA%2529%252C%2520their%2520ability%2520to%2520learn%2520multimodal%2520tasks%2520from%2520context%2520is%250Alargely%2520unknown.%2520We%2520introduce%2520SMMILE%252C%2520the%2520first%2520expert-driven%2520multimodal%2520ICL%250Abenchmark%2520for%2520medical%2520tasks.%2520Eleven%2520medical%2520experts%2520curated%2520problems%252C%2520each%250Aincluding%2520a%2520multimodal%2520query%2520and%2520multimodal%2520in-context%2520examples%2520as%2520task%250Ademonstrations.%2520SMMILE%2520encompasses%2520111%2520problems%2520%2528517%2520question-image-answer%250Atriplets%2529%2520covering%25206%2520medical%2520specialties%2520and%252013%2520imaging%2520modalities.%2520We%2520further%250Aintroduce%2520SMMILE%252B%252B%252C%2520an%2520augmented%2520variant%2520with%25201038%2520permuted%2520problems.%2520A%250Acomprehensive%2520evaluation%2520of%252015%2520MLLMs%2520demonstrates%2520that%2520most%2520models%2520exhibit%250Amoderate%2520to%2520poor%2520multimodal%2520ICL%2520ability%2520in%2520medical%2520tasks.%2520In%2520open-ended%250Aevaluations%252C%2520ICL%2520contributes%2520only%2520an%25208%2525%2520average%2520improvement%2520over%2520zero-shot%2520on%250ASMMILE%2520and%25209.4%2525%2520on%2520SMMILE%252B%252B.%2520We%2520observe%2520a%2520susceptibility%2520for%2520irrelevant%250Ain-context%2520examples%253A%2520even%2520a%2520single%2520noisy%2520or%2520irrelevant%2520example%2520can%2520degrade%250Aperformance%2520by%2520up%2520to%25209.5%2525.%2520Moreover%252C%2520we%2520observe%2520that%2520MLLMs%2520are%2520affected%2520by%2520a%250Arecency%2520bias%252C%2520where%2520placing%2520the%2520most%2520relevant%2520example%2520last%2520can%2520lead%2520to%250Asubstantial%2520performance%2520improvements%2520of%2520up%2520to%252071%2525.%2520Our%2520findings%2520highlight%250Acritical%2520limitations%2520and%2520biases%2520in%2520current%2520MLLMs%2520when%2520learning%2520multimodal%250Amedical%2520tasks%2520from%2520context.%2520SMMILE%2520is%2520available%2520at%250Ahttps%253A//smmile-benchmark.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMMILE%3A%20An%20Expert-Driven%20Benchmark%20for%20Multimodal%20Medical%20In-Context%0A%20%20Learning&entry.906535625=Melanie%20Rieff%20and%20Maya%20Varma%20and%20Ossian%20Rabow%20and%20Subathra%20Adithan%20and%20Julie%20Kim%20and%20Ken%20Chang%20and%20Hannah%20Lee%20and%20Nidhi%20Rohatgi%20and%20Christian%20Bluethgen%20and%20Mohamed%20S.%20Muneer%20and%20Jean-Benoit%20Delbrouck%20and%20Michael%20Moor&entry.1292438233=%20%20Multimodal%20in-context%20learning%20%28ICL%29%20remains%20underexplored%20despite%0Asignificant%20potential%20for%20domains%20such%20as%20medicine.%20Clinicians%20routinely%0Aencounter%20diverse%2C%20specialized%20tasks%20requiring%20adaptation%20from%20limited%0Aexamples%2C%20such%20as%20drawing%20insights%20from%20a%20few%20relevant%20prior%20cases%20or%0Aconsidering%20a%20constrained%20set%20of%20differential%20diagnoses.%20While%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20have%20shown%20advances%20in%20medical%20visual%20question%0Aanswering%20%28VQA%29%2C%20their%20ability%20to%20learn%20multimodal%20tasks%20from%20context%20is%0Alargely%20unknown.%20We%20introduce%20SMMILE%2C%20the%20first%20expert-driven%20multimodal%20ICL%0Abenchmark%20for%20medical%20tasks.%20Eleven%20medical%20experts%20curated%20problems%2C%20each%0Aincluding%20a%20multimodal%20query%20and%20multimodal%20in-context%20examples%20as%20task%0Ademonstrations.%20SMMILE%20encompasses%20111%20problems%20%28517%20question-image-answer%0Atriplets%29%20covering%206%20medical%20specialties%20and%2013%20imaging%20modalities.%20We%20further%0Aintroduce%20SMMILE%2B%2B%2C%20an%20augmented%20variant%20with%201038%20permuted%20problems.%20A%0Acomprehensive%20evaluation%20of%2015%20MLLMs%20demonstrates%20that%20most%20models%20exhibit%0Amoderate%20to%20poor%20multimodal%20ICL%20ability%20in%20medical%20tasks.%20In%20open-ended%0Aevaluations%2C%20ICL%20contributes%20only%20an%208%25%20average%20improvement%20over%20zero-shot%20on%0ASMMILE%20and%209.4%25%20on%20SMMILE%2B%2B.%20We%20observe%20a%20susceptibility%20for%20irrelevant%0Ain-context%20examples%3A%20even%20a%20single%20noisy%20or%20irrelevant%20example%20can%20degrade%0Aperformance%20by%20up%20to%209.5%25.%20Moreover%2C%20we%20observe%20that%20MLLMs%20are%20affected%20by%20a%0Arecency%20bias%2C%20where%20placing%20the%20most%20relevant%20example%20last%20can%20lead%20to%0Asubstantial%20performance%20improvements%20of%20up%20to%2071%25.%20Our%20findings%20highlight%0Acritical%20limitations%20and%20biases%20in%20current%20MLLMs%20when%20learning%20multimodal%0Amedical%20tasks%20from%20context.%20SMMILE%20is%20available%20at%0Ahttps%3A//smmile-benchmark.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21355v2&entry.124074799=Read"},
{"title": "Classification of Driver Behaviour Using External Observation Techniques\n  for Autonomous Vehicles", "author": "Ian Nell and Shane Gilroy", "abstract": "  Road traffic accidents remain a significant global concern, with human error,\nparticularly distracted and impaired driving, among the leading causes. This\nstudy introduces a novel driver behaviour classification system that uses\nexternal observation techniques to detect indicators of distraction and\nimpairment. The proposed framework employs advanced computer vision\nmethodologies, including real-time object tracking, lateral displacement\nanalysis, and lane position monitoring. The system identifies unsafe driving\nbehaviours such as excessive lateral movement and erratic trajectory patterns\nby implementing the YOLO object detection model and custom lane estimation\nalgorithms. Unlike systems reliant on inter-vehicular communication, this\nvision-based approach enables behavioural analysis of non-connected vehicles.\nExperimental evaluations on diverse video datasets demonstrate the framework's\nreliability and adaptability across varying road and environmental conditions.\n", "link": "http://arxiv.org/abs/2509.09349v2", "date": "2025-10-29", "relevancy": 2.11, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5494}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5297}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20Driver%20Behaviour%20Using%20External%20Observation%20Techniques%0A%20%20for%20Autonomous%20Vehicles&body=Title%3A%20Classification%20of%20Driver%20Behaviour%20Using%20External%20Observation%20Techniques%0A%20%20for%20Autonomous%20Vehicles%0AAuthor%3A%20Ian%20Nell%20and%20Shane%20Gilroy%0AAbstract%3A%20%20%20Road%20traffic%20accidents%20remain%20a%20significant%20global%20concern%2C%20with%20human%20error%2C%0Aparticularly%20distracted%20and%20impaired%20driving%2C%20among%20the%20leading%20causes.%20This%0Astudy%20introduces%20a%20novel%20driver%20behaviour%20classification%20system%20that%20uses%0Aexternal%20observation%20techniques%20to%20detect%20indicators%20of%20distraction%20and%0Aimpairment.%20The%20proposed%20framework%20employs%20advanced%20computer%20vision%0Amethodologies%2C%20including%20real-time%20object%20tracking%2C%20lateral%20displacement%0Aanalysis%2C%20and%20lane%20position%20monitoring.%20The%20system%20identifies%20unsafe%20driving%0Abehaviours%20such%20as%20excessive%20lateral%20movement%20and%20erratic%20trajectory%20patterns%0Aby%20implementing%20the%20YOLO%20object%20detection%20model%20and%20custom%20lane%20estimation%0Aalgorithms.%20Unlike%20systems%20reliant%20on%20inter-vehicular%20communication%2C%20this%0Avision-based%20approach%20enables%20behavioural%20analysis%20of%20non-connected%20vehicles.%0AExperimental%20evaluations%20on%20diverse%20video%20datasets%20demonstrate%20the%20framework%27s%0Areliability%20and%20adaptability%20across%20varying%20road%20and%20environmental%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.09349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520Driver%2520Behaviour%2520Using%2520External%2520Observation%2520Techniques%250A%2520%2520for%2520Autonomous%2520Vehicles%26entry.906535625%3DIan%2520Nell%2520and%2520Shane%2520Gilroy%26entry.1292438233%3D%2520%2520Road%2520traffic%2520accidents%2520remain%2520a%2520significant%2520global%2520concern%252C%2520with%2520human%2520error%252C%250Aparticularly%2520distracted%2520and%2520impaired%2520driving%252C%2520among%2520the%2520leading%2520causes.%2520This%250Astudy%2520introduces%2520a%2520novel%2520driver%2520behaviour%2520classification%2520system%2520that%2520uses%250Aexternal%2520observation%2520techniques%2520to%2520detect%2520indicators%2520of%2520distraction%2520and%250Aimpairment.%2520The%2520proposed%2520framework%2520employs%2520advanced%2520computer%2520vision%250Amethodologies%252C%2520including%2520real-time%2520object%2520tracking%252C%2520lateral%2520displacement%250Aanalysis%252C%2520and%2520lane%2520position%2520monitoring.%2520The%2520system%2520identifies%2520unsafe%2520driving%250Abehaviours%2520such%2520as%2520excessive%2520lateral%2520movement%2520and%2520erratic%2520trajectory%2520patterns%250Aby%2520implementing%2520the%2520YOLO%2520object%2520detection%2520model%2520and%2520custom%2520lane%2520estimation%250Aalgorithms.%2520Unlike%2520systems%2520reliant%2520on%2520inter-vehicular%2520communication%252C%2520this%250Avision-based%2520approach%2520enables%2520behavioural%2520analysis%2520of%2520non-connected%2520vehicles.%250AExperimental%2520evaluations%2520on%2520diverse%2520video%2520datasets%2520demonstrate%2520the%2520framework%2527s%250Areliability%2520and%2520adaptability%2520across%2520varying%2520road%2520and%2520environmental%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20Driver%20Behaviour%20Using%20External%20Observation%20Techniques%0A%20%20for%20Autonomous%20Vehicles&entry.906535625=Ian%20Nell%20and%20Shane%20Gilroy&entry.1292438233=%20%20Road%20traffic%20accidents%20remain%20a%20significant%20global%20concern%2C%20with%20human%20error%2C%0Aparticularly%20distracted%20and%20impaired%20driving%2C%20among%20the%20leading%20causes.%20This%0Astudy%20introduces%20a%20novel%20driver%20behaviour%20classification%20system%20that%20uses%0Aexternal%20observation%20techniques%20to%20detect%20indicators%20of%20distraction%20and%0Aimpairment.%20The%20proposed%20framework%20employs%20advanced%20computer%20vision%0Amethodologies%2C%20including%20real-time%20object%20tracking%2C%20lateral%20displacement%0Aanalysis%2C%20and%20lane%20position%20monitoring.%20The%20system%20identifies%20unsafe%20driving%0Abehaviours%20such%20as%20excessive%20lateral%20movement%20and%20erratic%20trajectory%20patterns%0Aby%20implementing%20the%20YOLO%20object%20detection%20model%20and%20custom%20lane%20estimation%0Aalgorithms.%20Unlike%20systems%20reliant%20on%20inter-vehicular%20communication%2C%20this%0Avision-based%20approach%20enables%20behavioural%20analysis%20of%20non-connected%20vehicles.%0AExperimental%20evaluations%20on%20diverse%20video%20datasets%20demonstrate%20the%20framework%27s%0Areliability%20and%20adaptability%20across%20varying%20road%20and%20environmental%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.09349v2&entry.124074799=Read"},
{"title": "RLMEval: Evaluating Research-Level Neural Theorem Proving", "author": "Auguste Poiroux and Antoine Bosselut and Viktor Kun\u010dak", "abstract": "  Despite impressive results on curated benchmarks, the practical impact of\nlarge language models (LLMs) on research-level neural theorem proving and proof\nautoformalization is still limited. We introduce RLMEval, an evaluation suite\nfor these tasks, focusing on research-level mathematics from real-world Lean\nformalization projects. RLMEval targets the evaluation of neural theorem\nproving and proof autoformalization on challenging research-level theorems by\nleveraging real Lean Blueprint formalization projects. Our evaluation of\nstate-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean\nprojects, reveals a significant gap: progress on existing benchmarks does not\nreadily translate to these more realistic settings, with the best model\nachieving only a 10.3 % pass rate. RLMEval provides a new, challenging\nbenchmark designed to guide and accelerate progress in automated reasoning for\nformal mathematics.\n", "link": "http://arxiv.org/abs/2510.25427v1", "date": "2025-10-29", "relevancy": 2.1068, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RLMEval%3A%20Evaluating%20Research-Level%20Neural%20Theorem%20Proving&body=Title%3A%20RLMEval%3A%20Evaluating%20Research-Level%20Neural%20Theorem%20Proving%0AAuthor%3A%20Auguste%20Poiroux%20and%20Antoine%20Bosselut%20and%20Viktor%20Kun%C4%8Dak%0AAbstract%3A%20%20%20Despite%20impressive%20results%20on%20curated%20benchmarks%2C%20the%20practical%20impact%20of%0Alarge%20language%20models%20%28LLMs%29%20on%20research-level%20neural%20theorem%20proving%20and%20proof%0Aautoformalization%20is%20still%20limited.%20We%20introduce%20RLMEval%2C%20an%20evaluation%20suite%0Afor%20these%20tasks%2C%20focusing%20on%20research-level%20mathematics%20from%20real-world%20Lean%0Aformalization%20projects.%20RLMEval%20targets%20the%20evaluation%20of%20neural%20theorem%0Aproving%20and%20proof%20autoformalization%20on%20challenging%20research-level%20theorems%20by%0Aleveraging%20real%20Lean%20Blueprint%20formalization%20projects.%20Our%20evaluation%20of%0Astate-of-the-art%20models%20on%20RLMEval%2C%20comprising%20613%20theorems%20from%206%20Lean%0Aprojects%2C%20reveals%20a%20significant%20gap%3A%20progress%20on%20existing%20benchmarks%20does%20not%0Areadily%20translate%20to%20these%20more%20realistic%20settings%2C%20with%20the%20best%20model%0Aachieving%20only%20a%2010.3%20%25%20pass%20rate.%20RLMEval%20provides%20a%20new%2C%20challenging%0Abenchmark%20designed%20to%20guide%20and%20accelerate%20progress%20in%20automated%20reasoning%20for%0Aformal%20mathematics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRLMEval%253A%2520Evaluating%2520Research-Level%2520Neural%2520Theorem%2520Proving%26entry.906535625%3DAuguste%2520Poiroux%2520and%2520Antoine%2520Bosselut%2520and%2520Viktor%2520Kun%25C4%258Dak%26entry.1292438233%3D%2520%2520Despite%2520impressive%2520results%2520on%2520curated%2520benchmarks%252C%2520the%2520practical%2520impact%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520on%2520research-level%2520neural%2520theorem%2520proving%2520and%2520proof%250Aautoformalization%2520is%2520still%2520limited.%2520We%2520introduce%2520RLMEval%252C%2520an%2520evaluation%2520suite%250Afor%2520these%2520tasks%252C%2520focusing%2520on%2520research-level%2520mathematics%2520from%2520real-world%2520Lean%250Aformalization%2520projects.%2520RLMEval%2520targets%2520the%2520evaluation%2520of%2520neural%2520theorem%250Aproving%2520and%2520proof%2520autoformalization%2520on%2520challenging%2520research-level%2520theorems%2520by%250Aleveraging%2520real%2520Lean%2520Blueprint%2520formalization%2520projects.%2520Our%2520evaluation%2520of%250Astate-of-the-art%2520models%2520on%2520RLMEval%252C%2520comprising%2520613%2520theorems%2520from%25206%2520Lean%250Aprojects%252C%2520reveals%2520a%2520significant%2520gap%253A%2520progress%2520on%2520existing%2520benchmarks%2520does%2520not%250Areadily%2520translate%2520to%2520these%2520more%2520realistic%2520settings%252C%2520with%2520the%2520best%2520model%250Aachieving%2520only%2520a%252010.3%2520%2525%2520pass%2520rate.%2520RLMEval%2520provides%2520a%2520new%252C%2520challenging%250Abenchmark%2520designed%2520to%2520guide%2520and%2520accelerate%2520progress%2520in%2520automated%2520reasoning%2520for%250Aformal%2520mathematics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RLMEval%3A%20Evaluating%20Research-Level%20Neural%20Theorem%20Proving&entry.906535625=Auguste%20Poiroux%20and%20Antoine%20Bosselut%20and%20Viktor%20Kun%C4%8Dak&entry.1292438233=%20%20Despite%20impressive%20results%20on%20curated%20benchmarks%2C%20the%20practical%20impact%20of%0Alarge%20language%20models%20%28LLMs%29%20on%20research-level%20neural%20theorem%20proving%20and%20proof%0Aautoformalization%20is%20still%20limited.%20We%20introduce%20RLMEval%2C%20an%20evaluation%20suite%0Afor%20these%20tasks%2C%20focusing%20on%20research-level%20mathematics%20from%20real-world%20Lean%0Aformalization%20projects.%20RLMEval%20targets%20the%20evaluation%20of%20neural%20theorem%0Aproving%20and%20proof%20autoformalization%20on%20challenging%20research-level%20theorems%20by%0Aleveraging%20real%20Lean%20Blueprint%20formalization%20projects.%20Our%20evaluation%20of%0Astate-of-the-art%20models%20on%20RLMEval%2C%20comprising%20613%20theorems%20from%206%20Lean%0Aprojects%2C%20reveals%20a%20significant%20gap%3A%20progress%20on%20existing%20benchmarks%20does%20not%0Areadily%20translate%20to%20these%20more%20realistic%20settings%2C%20with%20the%20best%20model%0Aachieving%20only%20a%2010.3%20%25%20pass%20rate.%20RLMEval%20provides%20a%20new%2C%20challenging%0Abenchmark%20designed%20to%20guide%20and%20accelerate%20progress%20in%20automated%20reasoning%20for%0Aformal%20mathematics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25427v1&entry.124074799=Read"},
{"title": "Are Language Models Efficient Reasoners? A Perspective from Logic\n  Programming", "author": "Andreas Opedal and Yanick Zengaffinen and Haruki Shirakami and Clemente Pasti and Mrinmaya Sachan and Abulhair Saparov and Ryan Cotterell and Bernhard Sch\u00f6lkopf", "abstract": "  Modern language models (LMs) exhibit strong deductive reasoning capabilities,\nyet standard evaluations emphasize correctness while overlooking a key aspect\nof human-like reasoning: efficiency. In real-world reasoning scenarios, much of\nthe available information is irrelevant, and effective deductive inference\nrequires identifying and ignoring such distractions. We propose a framework for\nassessing LM reasoning efficiency through the lens of logic programming,\nintroducing a simple method to align proofs written in natural language -- as\ngenerated by an LM -- with shortest proofs found by executing the logic\nprogram. Efficiency is quantified by measuring how well a model avoids\nunnecessary inference. Empirically, we construct a dataset of math word\nproblems injected with various number of irrelevant axioms that vary in\nsemantic overlap with the goal theorem. We find that current LMs show marked\naccuracy declines under such conditions -- even with minimal, domain-consistent\ndistractions -- and the proofs they generate frequently exhibit detours through\nirrelevant inferences.\n", "link": "http://arxiv.org/abs/2510.25626v1", "date": "2025-10-29", "relevancy": 2.0893, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5345}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Language%20Models%20Efficient%20Reasoners%3F%20A%20Perspective%20from%20Logic%0A%20%20Programming&body=Title%3A%20Are%20Language%20Models%20Efficient%20Reasoners%3F%20A%20Perspective%20from%20Logic%0A%20%20Programming%0AAuthor%3A%20Andreas%20Opedal%20and%20Yanick%20Zengaffinen%20and%20Haruki%20Shirakami%20and%20Clemente%20Pasti%20and%20Mrinmaya%20Sachan%20and%20Abulhair%20Saparov%20and%20Ryan%20Cotterell%20and%20Bernhard%20Sch%C3%B6lkopf%0AAbstract%3A%20%20%20Modern%20language%20models%20%28LMs%29%20exhibit%20strong%20deductive%20reasoning%20capabilities%2C%0Ayet%20standard%20evaluations%20emphasize%20correctness%20while%20overlooking%20a%20key%20aspect%0Aof%20human-like%20reasoning%3A%20efficiency.%20In%20real-world%20reasoning%20scenarios%2C%20much%20of%0Athe%20available%20information%20is%20irrelevant%2C%20and%20effective%20deductive%20inference%0Arequires%20identifying%20and%20ignoring%20such%20distractions.%20We%20propose%20a%20framework%20for%0Aassessing%20LM%20reasoning%20efficiency%20through%20the%20lens%20of%20logic%20programming%2C%0Aintroducing%20a%20simple%20method%20to%20align%20proofs%20written%20in%20natural%20language%20--%20as%0Agenerated%20by%20an%20LM%20--%20with%20shortest%20proofs%20found%20by%20executing%20the%20logic%0Aprogram.%20Efficiency%20is%20quantified%20by%20measuring%20how%20well%20a%20model%20avoids%0Aunnecessary%20inference.%20Empirically%2C%20we%20construct%20a%20dataset%20of%20math%20word%0Aproblems%20injected%20with%20various%20number%20of%20irrelevant%20axioms%20that%20vary%20in%0Asemantic%20overlap%20with%20the%20goal%20theorem.%20We%20find%20that%20current%20LMs%20show%20marked%0Aaccuracy%20declines%20under%20such%20conditions%20--%20even%20with%20minimal%2C%20domain-consistent%0Adistractions%20--%20and%20the%20proofs%20they%20generate%20frequently%20exhibit%20detours%20through%0Airrelevant%20inferences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Language%2520Models%2520Efficient%2520Reasoners%253F%2520A%2520Perspective%2520from%2520Logic%250A%2520%2520Programming%26entry.906535625%3DAndreas%2520Opedal%2520and%2520Yanick%2520Zengaffinen%2520and%2520Haruki%2520Shirakami%2520and%2520Clemente%2520Pasti%2520and%2520Mrinmaya%2520Sachan%2520and%2520Abulhair%2520Saparov%2520and%2520Ryan%2520Cotterell%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%26entry.1292438233%3D%2520%2520Modern%2520language%2520models%2520%2528LMs%2529%2520exhibit%2520strong%2520deductive%2520reasoning%2520capabilities%252C%250Ayet%2520standard%2520evaluations%2520emphasize%2520correctness%2520while%2520overlooking%2520a%2520key%2520aspect%250Aof%2520human-like%2520reasoning%253A%2520efficiency.%2520In%2520real-world%2520reasoning%2520scenarios%252C%2520much%2520of%250Athe%2520available%2520information%2520is%2520irrelevant%252C%2520and%2520effective%2520deductive%2520inference%250Arequires%2520identifying%2520and%2520ignoring%2520such%2520distractions.%2520We%2520propose%2520a%2520framework%2520for%250Aassessing%2520LM%2520reasoning%2520efficiency%2520through%2520the%2520lens%2520of%2520logic%2520programming%252C%250Aintroducing%2520a%2520simple%2520method%2520to%2520align%2520proofs%2520written%2520in%2520natural%2520language%2520--%2520as%250Agenerated%2520by%2520an%2520LM%2520--%2520with%2520shortest%2520proofs%2520found%2520by%2520executing%2520the%2520logic%250Aprogram.%2520Efficiency%2520is%2520quantified%2520by%2520measuring%2520how%2520well%2520a%2520model%2520avoids%250Aunnecessary%2520inference.%2520Empirically%252C%2520we%2520construct%2520a%2520dataset%2520of%2520math%2520word%250Aproblems%2520injected%2520with%2520various%2520number%2520of%2520irrelevant%2520axioms%2520that%2520vary%2520in%250Asemantic%2520overlap%2520with%2520the%2520goal%2520theorem.%2520We%2520find%2520that%2520current%2520LMs%2520show%2520marked%250Aaccuracy%2520declines%2520under%2520such%2520conditions%2520--%2520even%2520with%2520minimal%252C%2520domain-consistent%250Adistractions%2520--%2520and%2520the%2520proofs%2520they%2520generate%2520frequently%2520exhibit%2520detours%2520through%250Airrelevant%2520inferences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Language%20Models%20Efficient%20Reasoners%3F%20A%20Perspective%20from%20Logic%0A%20%20Programming&entry.906535625=Andreas%20Opedal%20and%20Yanick%20Zengaffinen%20and%20Haruki%20Shirakami%20and%20Clemente%20Pasti%20and%20Mrinmaya%20Sachan%20and%20Abulhair%20Saparov%20and%20Ryan%20Cotterell%20and%20Bernhard%20Sch%C3%B6lkopf&entry.1292438233=%20%20Modern%20language%20models%20%28LMs%29%20exhibit%20strong%20deductive%20reasoning%20capabilities%2C%0Ayet%20standard%20evaluations%20emphasize%20correctness%20while%20overlooking%20a%20key%20aspect%0Aof%20human-like%20reasoning%3A%20efficiency.%20In%20real-world%20reasoning%20scenarios%2C%20much%20of%0Athe%20available%20information%20is%20irrelevant%2C%20and%20effective%20deductive%20inference%0Arequires%20identifying%20and%20ignoring%20such%20distractions.%20We%20propose%20a%20framework%20for%0Aassessing%20LM%20reasoning%20efficiency%20through%20the%20lens%20of%20logic%20programming%2C%0Aintroducing%20a%20simple%20method%20to%20align%20proofs%20written%20in%20natural%20language%20--%20as%0Agenerated%20by%20an%20LM%20--%20with%20shortest%20proofs%20found%20by%20executing%20the%20logic%0Aprogram.%20Efficiency%20is%20quantified%20by%20measuring%20how%20well%20a%20model%20avoids%0Aunnecessary%20inference.%20Empirically%2C%20we%20construct%20a%20dataset%20of%20math%20word%0Aproblems%20injected%20with%20various%20number%20of%20irrelevant%20axioms%20that%20vary%20in%0Asemantic%20overlap%20with%20the%20goal%20theorem.%20We%20find%20that%20current%20LMs%20show%20marked%0Aaccuracy%20declines%20under%20such%20conditions%20--%20even%20with%20minimal%2C%20domain-consistent%0Adistractions%20--%20and%20the%20proofs%20they%20generate%20frequently%20exhibit%20detours%20through%0Airrelevant%20inferences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25626v1&entry.124074799=Read"},
{"title": "SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning", "author": "Khoa Nguyen and Khang Tran and NhatHai Phan and Cristian Borcea and Ruoming Jin and Issa Khalil", "abstract": "  This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel\ntraining algorithm to leverage the geographic information of mobile users in\nFederated Learning (FL). SGFusion maps the data collected by mobile devices\nonto geographical zones and trains one FL model per zone, which adapts well to\nthe data and behaviors of users in that zone. SGFusion models the local\ndata-based correlation among geographical zones as a hierarchical random graph\n(HRG) optimized by Markov Chain Monte Carlo sampling. At each training step,\nevery zone fuses its local gradient with gradients derived from a small set of\nother zones sampled from the HRG. This approach enables knowledge fusion and\nsharing among geographical zones in a probabilistic and stochastic gradient\nfusion process with self-attention weights, such that \"more similar\" zones have\n\"higher probabilities\" of sharing gradients with \"larger attention weights.\"\nSGFusion remarkably improves model utility without introducing undue\ncomputational cost. Extensive theoretical and empirical results using a\nheart-rate prediction dataset collected across 6 countries show that models\ntrained with SGFusion converge with upper-bounded expected errors and\nsignificantly improve utility in all countries compared to existing approaches\nwithout notable cost in system scalability.\n", "link": "http://arxiv.org/abs/2510.23455v3", "date": "2025-10-29", "relevancy": 2.0889, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5386}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.512}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGFusion%3A%20Stochastic%20Geographic%20Gradient%20Fusion%20in%20Federated%20Learning&body=Title%3A%20SGFusion%3A%20Stochastic%20Geographic%20Gradient%20Fusion%20in%20Federated%20Learning%0AAuthor%3A%20Khoa%20Nguyen%20and%20Khang%20Tran%20and%20NhatHai%20Phan%20and%20Cristian%20Borcea%20and%20Ruoming%20Jin%20and%20Issa%20Khalil%0AAbstract%3A%20%20%20This%20paper%20proposes%20Stochastic%20Geographic%20Gradient%20Fusion%20%28SGFusion%29%2C%20a%20novel%0Atraining%20algorithm%20to%20leverage%20the%20geographic%20information%20of%20mobile%20users%20in%0AFederated%20Learning%20%28FL%29.%20SGFusion%20maps%20the%20data%20collected%20by%20mobile%20devices%0Aonto%20geographical%20zones%20and%20trains%20one%20FL%20model%20per%20zone%2C%20which%20adapts%20well%20to%0Athe%20data%20and%20behaviors%20of%20users%20in%20that%20zone.%20SGFusion%20models%20the%20local%0Adata-based%20correlation%20among%20geographical%20zones%20as%20a%20hierarchical%20random%20graph%0A%28HRG%29%20optimized%20by%20Markov%20Chain%20Monte%20Carlo%20sampling.%20At%20each%20training%20step%2C%0Aevery%20zone%20fuses%20its%20local%20gradient%20with%20gradients%20derived%20from%20a%20small%20set%20of%0Aother%20zones%20sampled%20from%20the%20HRG.%20This%20approach%20enables%20knowledge%20fusion%20and%0Asharing%20among%20geographical%20zones%20in%20a%20probabilistic%20and%20stochastic%20gradient%0Afusion%20process%20with%20self-attention%20weights%2C%20such%20that%20%22more%20similar%22%20zones%20have%0A%22higher%20probabilities%22%20of%20sharing%20gradients%20with%20%22larger%20attention%20weights.%22%0ASGFusion%20remarkably%20improves%20model%20utility%20without%20introducing%20undue%0Acomputational%20cost.%20Extensive%20theoretical%20and%20empirical%20results%20using%20a%0Aheart-rate%20prediction%20dataset%20collected%20across%206%20countries%20show%20that%20models%0Atrained%20with%20SGFusion%20converge%20with%20upper-bounded%20expected%20errors%20and%0Asignificantly%20improve%20utility%20in%20all%20countries%20compared%20to%20existing%20approaches%0Awithout%20notable%20cost%20in%20system%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.23455v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGFusion%253A%2520Stochastic%2520Geographic%2520Gradient%2520Fusion%2520in%2520Federated%2520Learning%26entry.906535625%3DKhoa%2520Nguyen%2520and%2520Khang%2520Tran%2520and%2520NhatHai%2520Phan%2520and%2520Cristian%2520Borcea%2520and%2520Ruoming%2520Jin%2520and%2520Issa%2520Khalil%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520Stochastic%2520Geographic%2520Gradient%2520Fusion%2520%2528SGFusion%2529%252C%2520a%2520novel%250Atraining%2520algorithm%2520to%2520leverage%2520the%2520geographic%2520information%2520of%2520mobile%2520users%2520in%250AFederated%2520Learning%2520%2528FL%2529.%2520SGFusion%2520maps%2520the%2520data%2520collected%2520by%2520mobile%2520devices%250Aonto%2520geographical%2520zones%2520and%2520trains%2520one%2520FL%2520model%2520per%2520zone%252C%2520which%2520adapts%2520well%2520to%250Athe%2520data%2520and%2520behaviors%2520of%2520users%2520in%2520that%2520zone.%2520SGFusion%2520models%2520the%2520local%250Adata-based%2520correlation%2520among%2520geographical%2520zones%2520as%2520a%2520hierarchical%2520random%2520graph%250A%2528HRG%2529%2520optimized%2520by%2520Markov%2520Chain%2520Monte%2520Carlo%2520sampling.%2520At%2520each%2520training%2520step%252C%250Aevery%2520zone%2520fuses%2520its%2520local%2520gradient%2520with%2520gradients%2520derived%2520from%2520a%2520small%2520set%2520of%250Aother%2520zones%2520sampled%2520from%2520the%2520HRG.%2520This%2520approach%2520enables%2520knowledge%2520fusion%2520and%250Asharing%2520among%2520geographical%2520zones%2520in%2520a%2520probabilistic%2520and%2520stochastic%2520gradient%250Afusion%2520process%2520with%2520self-attention%2520weights%252C%2520such%2520that%2520%2522more%2520similar%2522%2520zones%2520have%250A%2522higher%2520probabilities%2522%2520of%2520sharing%2520gradients%2520with%2520%2522larger%2520attention%2520weights.%2522%250ASGFusion%2520remarkably%2520improves%2520model%2520utility%2520without%2520introducing%2520undue%250Acomputational%2520cost.%2520Extensive%2520theoretical%2520and%2520empirical%2520results%2520using%2520a%250Aheart-rate%2520prediction%2520dataset%2520collected%2520across%25206%2520countries%2520show%2520that%2520models%250Atrained%2520with%2520SGFusion%2520converge%2520with%2520upper-bounded%2520expected%2520errors%2520and%250Asignificantly%2520improve%2520utility%2520in%2520all%2520countries%2520compared%2520to%2520existing%2520approaches%250Awithout%2520notable%2520cost%2520in%2520system%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23455v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGFusion%3A%20Stochastic%20Geographic%20Gradient%20Fusion%20in%20Federated%20Learning&entry.906535625=Khoa%20Nguyen%20and%20Khang%20Tran%20and%20NhatHai%20Phan%20and%20Cristian%20Borcea%20and%20Ruoming%20Jin%20and%20Issa%20Khalil&entry.1292438233=%20%20This%20paper%20proposes%20Stochastic%20Geographic%20Gradient%20Fusion%20%28SGFusion%29%2C%20a%20novel%0Atraining%20algorithm%20to%20leverage%20the%20geographic%20information%20of%20mobile%20users%20in%0AFederated%20Learning%20%28FL%29.%20SGFusion%20maps%20the%20data%20collected%20by%20mobile%20devices%0Aonto%20geographical%20zones%20and%20trains%20one%20FL%20model%20per%20zone%2C%20which%20adapts%20well%20to%0Athe%20data%20and%20behaviors%20of%20users%20in%20that%20zone.%20SGFusion%20models%20the%20local%0Adata-based%20correlation%20among%20geographical%20zones%20as%20a%20hierarchical%20random%20graph%0A%28HRG%29%20optimized%20by%20Markov%20Chain%20Monte%20Carlo%20sampling.%20At%20each%20training%20step%2C%0Aevery%20zone%20fuses%20its%20local%20gradient%20with%20gradients%20derived%20from%20a%20small%20set%20of%0Aother%20zones%20sampled%20from%20the%20HRG.%20This%20approach%20enables%20knowledge%20fusion%20and%0Asharing%20among%20geographical%20zones%20in%20a%20probabilistic%20and%20stochastic%20gradient%0Afusion%20process%20with%20self-attention%20weights%2C%20such%20that%20%22more%20similar%22%20zones%20have%0A%22higher%20probabilities%22%20of%20sharing%20gradients%20with%20%22larger%20attention%20weights.%22%0ASGFusion%20remarkably%20improves%20model%20utility%20without%20introducing%20undue%0Acomputational%20cost.%20Extensive%20theoretical%20and%20empirical%20results%20using%20a%0Aheart-rate%20prediction%20dataset%20collected%20across%206%20countries%20show%20that%20models%0Atrained%20with%20SGFusion%20converge%20with%20upper-bounded%20expected%20errors%20and%0Asignificantly%20improve%20utility%20in%20all%20countries%20compared%20to%20existing%20approaches%0Awithout%20notable%20cost%20in%20system%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.23455v3&entry.124074799=Read"},
{"title": "ScribbleVS: Scribble-Supervised Medical Image Segmentation via Dynamic\n  Competitive Pseudo Label Selection", "author": "Tao Wang and Xinlin Zhang and Zhenxuan Zhang and Yuanbo Zhou and Yuanbin Chen and Longxuan Zhao and Chaohui Xu and Shun Chen and Guang Yang and Tong Tong", "abstract": "  In clinical medicine, precise image segmentation can provide substantial\nsupport to clinicians. However, obtaining high-quality segmentation typically\ndemands extensive pixel-level annotations, which are labor-intensive and\nexpensive. Scribble annotations offer a more cost-effective alternative by\nimproving labeling efficiency. Nonetheless, using such sparse supervision for\ntraining reliable medical image segmentation models remains a significant\nchallenge. Some studies employ pseudo-labeling to enhance supervision, but\nthese methods are susceptible to noise interference. To address these\nchallenges, we introduce ScribbleVS, a framework designed to learn from\nscribble annotations. We introduce a Regional Pseudo Labels Diffusion Module to\nexpand the scope of supervision and reduce the impact of noise present in\npseudo labels. Additionally, we introduce a Dynamic Competitive Selection\nmodule for enhanced refinement in selecting pseudo labels. Experiments\nconducted on the ACDC, MSCMRseg, WORD, and BraTS2020 datasets demonstrate\npromising results, achieving segmentation precision comparable to fully\nsupervised models. The codes of this study are available at\nhttps://github.com/ortonwang/ScribbleVS.\n", "link": "http://arxiv.org/abs/2411.10237v2", "date": "2025-10-29", "relevancy": 2.0698, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5532}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5103}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScribbleVS%3A%20Scribble-Supervised%20Medical%20Image%20Segmentation%20via%20Dynamic%0A%20%20Competitive%20Pseudo%20Label%20Selection&body=Title%3A%20ScribbleVS%3A%20Scribble-Supervised%20Medical%20Image%20Segmentation%20via%20Dynamic%0A%20%20Competitive%20Pseudo%20Label%20Selection%0AAuthor%3A%20Tao%20Wang%20and%20Xinlin%20Zhang%20and%20Zhenxuan%20Zhang%20and%20Yuanbo%20Zhou%20and%20Yuanbin%20Chen%20and%20Longxuan%20Zhao%20and%20Chaohui%20Xu%20and%20Shun%20Chen%20and%20Guang%20Yang%20and%20Tong%20Tong%0AAbstract%3A%20%20%20In%20clinical%20medicine%2C%20precise%20image%20segmentation%20can%20provide%20substantial%0Asupport%20to%20clinicians.%20However%2C%20obtaining%20high-quality%20segmentation%20typically%0Ademands%20extensive%20pixel-level%20annotations%2C%20which%20are%20labor-intensive%20and%0Aexpensive.%20Scribble%20annotations%20offer%20a%20more%20cost-effective%20alternative%20by%0Aimproving%20labeling%20efficiency.%20Nonetheless%2C%20using%20such%20sparse%20supervision%20for%0Atraining%20reliable%20medical%20image%20segmentation%20models%20remains%20a%20significant%0Achallenge.%20Some%20studies%20employ%20pseudo-labeling%20to%20enhance%20supervision%2C%20but%0Athese%20methods%20are%20susceptible%20to%20noise%20interference.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20ScribbleVS%2C%20a%20framework%20designed%20to%20learn%20from%0Ascribble%20annotations.%20We%20introduce%20a%20Regional%20Pseudo%20Labels%20Diffusion%20Module%20to%0Aexpand%20the%20scope%20of%20supervision%20and%20reduce%20the%20impact%20of%20noise%20present%20in%0Apseudo%20labels.%20Additionally%2C%20we%20introduce%20a%20Dynamic%20Competitive%20Selection%0Amodule%20for%20enhanced%20refinement%20in%20selecting%20pseudo%20labels.%20Experiments%0Aconducted%20on%20the%20ACDC%2C%20MSCMRseg%2C%20WORD%2C%20and%20BraTS2020%20datasets%20demonstrate%0Apromising%20results%2C%20achieving%20segmentation%20precision%20comparable%20to%20fully%0Asupervised%20models.%20The%20codes%20of%20this%20study%20are%20available%20at%0Ahttps%3A//github.com/ortonwang/ScribbleVS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10237v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScribbleVS%253A%2520Scribble-Supervised%2520Medical%2520Image%2520Segmentation%2520via%2520Dynamic%250A%2520%2520Competitive%2520Pseudo%2520Label%2520Selection%26entry.906535625%3DTao%2520Wang%2520and%2520Xinlin%2520Zhang%2520and%2520Zhenxuan%2520Zhang%2520and%2520Yuanbo%2520Zhou%2520and%2520Yuanbin%2520Chen%2520and%2520Longxuan%2520Zhao%2520and%2520Chaohui%2520Xu%2520and%2520Shun%2520Chen%2520and%2520Guang%2520Yang%2520and%2520Tong%2520Tong%26entry.1292438233%3D%2520%2520In%2520clinical%2520medicine%252C%2520precise%2520image%2520segmentation%2520can%2520provide%2520substantial%250Asupport%2520to%2520clinicians.%2520However%252C%2520obtaining%2520high-quality%2520segmentation%2520typically%250Ademands%2520extensive%2520pixel-level%2520annotations%252C%2520which%2520are%2520labor-intensive%2520and%250Aexpensive.%2520Scribble%2520annotations%2520offer%2520a%2520more%2520cost-effective%2520alternative%2520by%250Aimproving%2520labeling%2520efficiency.%2520Nonetheless%252C%2520using%2520such%2520sparse%2520supervision%2520for%250Atraining%2520reliable%2520medical%2520image%2520segmentation%2520models%2520remains%2520a%2520significant%250Achallenge.%2520Some%2520studies%2520employ%2520pseudo-labeling%2520to%2520enhance%2520supervision%252C%2520but%250Athese%2520methods%2520are%2520susceptible%2520to%2520noise%2520interference.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520ScribbleVS%252C%2520a%2520framework%2520designed%2520to%2520learn%2520from%250Ascribble%2520annotations.%2520We%2520introduce%2520a%2520Regional%2520Pseudo%2520Labels%2520Diffusion%2520Module%2520to%250Aexpand%2520the%2520scope%2520of%2520supervision%2520and%2520reduce%2520the%2520impact%2520of%2520noise%2520present%2520in%250Apseudo%2520labels.%2520Additionally%252C%2520we%2520introduce%2520a%2520Dynamic%2520Competitive%2520Selection%250Amodule%2520for%2520enhanced%2520refinement%2520in%2520selecting%2520pseudo%2520labels.%2520Experiments%250Aconducted%2520on%2520the%2520ACDC%252C%2520MSCMRseg%252C%2520WORD%252C%2520and%2520BraTS2020%2520datasets%2520demonstrate%250Apromising%2520results%252C%2520achieving%2520segmentation%2520precision%2520comparable%2520to%2520fully%250Asupervised%2520models.%2520The%2520codes%2520of%2520this%2520study%2520are%2520available%2520at%250Ahttps%253A//github.com/ortonwang/ScribbleVS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10237v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScribbleVS%3A%20Scribble-Supervised%20Medical%20Image%20Segmentation%20via%20Dynamic%0A%20%20Competitive%20Pseudo%20Label%20Selection&entry.906535625=Tao%20Wang%20and%20Xinlin%20Zhang%20and%20Zhenxuan%20Zhang%20and%20Yuanbo%20Zhou%20and%20Yuanbin%20Chen%20and%20Longxuan%20Zhao%20and%20Chaohui%20Xu%20and%20Shun%20Chen%20and%20Guang%20Yang%20and%20Tong%20Tong&entry.1292438233=%20%20In%20clinical%20medicine%2C%20precise%20image%20segmentation%20can%20provide%20substantial%0Asupport%20to%20clinicians.%20However%2C%20obtaining%20high-quality%20segmentation%20typically%0Ademands%20extensive%20pixel-level%20annotations%2C%20which%20are%20labor-intensive%20and%0Aexpensive.%20Scribble%20annotations%20offer%20a%20more%20cost-effective%20alternative%20by%0Aimproving%20labeling%20efficiency.%20Nonetheless%2C%20using%20such%20sparse%20supervision%20for%0Atraining%20reliable%20medical%20image%20segmentation%20models%20remains%20a%20significant%0Achallenge.%20Some%20studies%20employ%20pseudo-labeling%20to%20enhance%20supervision%2C%20but%0Athese%20methods%20are%20susceptible%20to%20noise%20interference.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20ScribbleVS%2C%20a%20framework%20designed%20to%20learn%20from%0Ascribble%20annotations.%20We%20introduce%20a%20Regional%20Pseudo%20Labels%20Diffusion%20Module%20to%0Aexpand%20the%20scope%20of%20supervision%20and%20reduce%20the%20impact%20of%20noise%20present%20in%0Apseudo%20labels.%20Additionally%2C%20we%20introduce%20a%20Dynamic%20Competitive%20Selection%0Amodule%20for%20enhanced%20refinement%20in%20selecting%20pseudo%20labels.%20Experiments%0Aconducted%20on%20the%20ACDC%2C%20MSCMRseg%2C%20WORD%2C%20and%20BraTS2020%20datasets%20demonstrate%0Apromising%20results%2C%20achieving%20segmentation%20precision%20comparable%20to%20fully%0Asupervised%20models.%20The%20codes%20of%20this%20study%20are%20available%20at%0Ahttps%3A//github.com/ortonwang/ScribbleVS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10237v2&entry.124074799=Read"},
{"title": "PromptReverb: Multimodal Room Impulse Response Generation Through Latent\n  Rectified Flow Matching", "author": "Ali Vosoughi and Yongyi Zang and Qihui Yang and Nathan Paek and Randal Leistikow and Chenliang Xu", "abstract": "  Room impulse response (RIR) generation remains a critical challenge for\ncreating immersive virtual acoustic environments. Current methods suffer from\ntwo fundamental limitations: the scarcity of full-band RIR datasets and the\ninability of existing models to generate acoustically accurate responses from\ndiverse input modalities. We present PromptReverb, a two-stage generative\nframework that addresses these challenges. Our approach combines a variational\nautoencoder that upsamples band-limited RIRs to full-band quality (48 kHz), and\na conditional diffusion transformer model based on rectified flow matching that\ngenerates RIRs from descriptions in natural language. Empirical evaluation\ndemonstrates that PromptReverb produces RIRs with superior perceptual quality\nand acoustic accuracy compared to existing methods, achieving 8.8% mean RT60\nerror compared to -37% for widely used baselines and yielding more realistic\nroom-acoustic parameters. Our method enables practical applications in virtual\nreality, architectural acoustics, and audio production where flexible,\nhigh-quality RIR synthesis is essential.\n", "link": "http://arxiv.org/abs/2510.22439v2", "date": "2025-10-29", "relevancy": 2.0602, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.566}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5249}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PromptReverb%3A%20Multimodal%20Room%20Impulse%20Response%20Generation%20Through%20Latent%0A%20%20Rectified%20Flow%20Matching&body=Title%3A%20PromptReverb%3A%20Multimodal%20Room%20Impulse%20Response%20Generation%20Through%20Latent%0A%20%20Rectified%20Flow%20Matching%0AAuthor%3A%20Ali%20Vosoughi%20and%20Yongyi%20Zang%20and%20Qihui%20Yang%20and%20Nathan%20Paek%20and%20Randal%20Leistikow%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20Room%20impulse%20response%20%28RIR%29%20generation%20remains%20a%20critical%20challenge%20for%0Acreating%20immersive%20virtual%20acoustic%20environments.%20Current%20methods%20suffer%20from%0Atwo%20fundamental%20limitations%3A%20the%20scarcity%20of%20full-band%20RIR%20datasets%20and%20the%0Ainability%20of%20existing%20models%20to%20generate%20acoustically%20accurate%20responses%20from%0Adiverse%20input%20modalities.%20We%20present%20PromptReverb%2C%20a%20two-stage%20generative%0Aframework%20that%20addresses%20these%20challenges.%20Our%20approach%20combines%20a%20variational%0Aautoencoder%20that%20upsamples%20band-limited%20RIRs%20to%20full-band%20quality%20%2848%20kHz%29%2C%20and%0Aa%20conditional%20diffusion%20transformer%20model%20based%20on%20rectified%20flow%20matching%20that%0Agenerates%20RIRs%20from%20descriptions%20in%20natural%20language.%20Empirical%20evaluation%0Ademonstrates%20that%20PromptReverb%20produces%20RIRs%20with%20superior%20perceptual%20quality%0Aand%20acoustic%20accuracy%20compared%20to%20existing%20methods%2C%20achieving%208.8%25%20mean%20RT60%0Aerror%20compared%20to%20-37%25%20for%20widely%20used%20baselines%20and%20yielding%20more%20realistic%0Aroom-acoustic%20parameters.%20Our%20method%20enables%20practical%20applications%20in%20virtual%0Areality%2C%20architectural%20acoustics%2C%20and%20audio%20production%20where%20flexible%2C%0Ahigh-quality%20RIR%20synthesis%20is%20essential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.22439v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPromptReverb%253A%2520Multimodal%2520Room%2520Impulse%2520Response%2520Generation%2520Through%2520Latent%250A%2520%2520Rectified%2520Flow%2520Matching%26entry.906535625%3DAli%2520Vosoughi%2520and%2520Yongyi%2520Zang%2520and%2520Qihui%2520Yang%2520and%2520Nathan%2520Paek%2520and%2520Randal%2520Leistikow%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520Room%2520impulse%2520response%2520%2528RIR%2529%2520generation%2520remains%2520a%2520critical%2520challenge%2520for%250Acreating%2520immersive%2520virtual%2520acoustic%2520environments.%2520Current%2520methods%2520suffer%2520from%250Atwo%2520fundamental%2520limitations%253A%2520the%2520scarcity%2520of%2520full-band%2520RIR%2520datasets%2520and%2520the%250Ainability%2520of%2520existing%2520models%2520to%2520generate%2520acoustically%2520accurate%2520responses%2520from%250Adiverse%2520input%2520modalities.%2520We%2520present%2520PromptReverb%252C%2520a%2520two-stage%2520generative%250Aframework%2520that%2520addresses%2520these%2520challenges.%2520Our%2520approach%2520combines%2520a%2520variational%250Aautoencoder%2520that%2520upsamples%2520band-limited%2520RIRs%2520to%2520full-band%2520quality%2520%252848%2520kHz%2529%252C%2520and%250Aa%2520conditional%2520diffusion%2520transformer%2520model%2520based%2520on%2520rectified%2520flow%2520matching%2520that%250Agenerates%2520RIRs%2520from%2520descriptions%2520in%2520natural%2520language.%2520Empirical%2520evaluation%250Ademonstrates%2520that%2520PromptReverb%2520produces%2520RIRs%2520with%2520superior%2520perceptual%2520quality%250Aand%2520acoustic%2520accuracy%2520compared%2520to%2520existing%2520methods%252C%2520achieving%25208.8%2525%2520mean%2520RT60%250Aerror%2520compared%2520to%2520-37%2525%2520for%2520widely%2520used%2520baselines%2520and%2520yielding%2520more%2520realistic%250Aroom-acoustic%2520parameters.%2520Our%2520method%2520enables%2520practical%2520applications%2520in%2520virtual%250Areality%252C%2520architectural%2520acoustics%252C%2520and%2520audio%2520production%2520where%2520flexible%252C%250Ahigh-quality%2520RIR%2520synthesis%2520is%2520essential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22439v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PromptReverb%3A%20Multimodal%20Room%20Impulse%20Response%20Generation%20Through%20Latent%0A%20%20Rectified%20Flow%20Matching&entry.906535625=Ali%20Vosoughi%20and%20Yongyi%20Zang%20and%20Qihui%20Yang%20and%20Nathan%20Paek%20and%20Randal%20Leistikow%20and%20Chenliang%20Xu&entry.1292438233=%20%20Room%20impulse%20response%20%28RIR%29%20generation%20remains%20a%20critical%20challenge%20for%0Acreating%20immersive%20virtual%20acoustic%20environments.%20Current%20methods%20suffer%20from%0Atwo%20fundamental%20limitations%3A%20the%20scarcity%20of%20full-band%20RIR%20datasets%20and%20the%0Ainability%20of%20existing%20models%20to%20generate%20acoustically%20accurate%20responses%20from%0Adiverse%20input%20modalities.%20We%20present%20PromptReverb%2C%20a%20two-stage%20generative%0Aframework%20that%20addresses%20these%20challenges.%20Our%20approach%20combines%20a%20variational%0Aautoencoder%20that%20upsamples%20band-limited%20RIRs%20to%20full-band%20quality%20%2848%20kHz%29%2C%20and%0Aa%20conditional%20diffusion%20transformer%20model%20based%20on%20rectified%20flow%20matching%20that%0Agenerates%20RIRs%20from%20descriptions%20in%20natural%20language.%20Empirical%20evaluation%0Ademonstrates%20that%20PromptReverb%20produces%20RIRs%20with%20superior%20perceptual%20quality%0Aand%20acoustic%20accuracy%20compared%20to%20existing%20methods%2C%20achieving%208.8%25%20mean%20RT60%0Aerror%20compared%20to%20-37%25%20for%20widely%20used%20baselines%20and%20yielding%20more%20realistic%0Aroom-acoustic%20parameters.%20Our%20method%20enables%20practical%20applications%20in%20virtual%0Areality%2C%20architectural%20acoustics%2C%20and%20audio%20production%20where%20flexible%2C%0Ahigh-quality%20RIR%20synthesis%20is%20essential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.22439v2&entry.124074799=Read"},
{"title": "Comparative Study of UNet-based Architectures for Liver Tumor\n  Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography", "author": "Doan-Van-Anh Ly and Thi-Thu-Hien Pham and Thanh-Hai Le", "abstract": "  Segmentation of liver structures in multi-phase contrast-enhanced computed\ntomography (CECT) plays a crucial role in computer-aided diagnosis and\ntreatment planning for liver diseases, including tumor detection. In this\nstudy, we investigate the performance of UNet-based architectures for liver\ntumor segmentation, starting from the original UNet and extending to UNet3+\nwith various backbone networks. We evaluate ResNet, Transformer-based, and\nState-space (Mamba) backbones, all initialized with pretrained weights.\nSurprisingly, despite the advances in modern architecture, ResNet-based models\nconsistently outperform Transformer- and Mamba-based alternatives across\nmultiple evaluation metrics. To further improve segmentation quality, we\nintroduce attention mechanisms into the backbone and observe that incorporating\nthe Convolutional Block Attention Module (CBAM) yields the best performance.\nResNetUNet3+ with CBAM module not only produced the best overlap metrics with a\nDice score of 0.755 and IoU of 0.662, but also achieved the most precise\nboundary delineation, evidenced by the lowest HD95 distance of 77.911. The\nmodel's superiority was further cemented by its leading overall accuracy of\n0.925 and specificity of 0.926, showcasing its robust capability in accurately\nidentifying both lesion and healthy tissue. To further enhance\ninterpretability, Grad-CAM visualizations were employed to highlight the\nregion's most influential predictions, providing insights into its\ndecision-making process. These findings demonstrate that classical ResNet\narchitecture, when combined with modern attention modules, remain highly\ncompetitive for medical image segmentation tasks, offering a promising\ndirection for liver tumor detection in clinical practice.\n", "link": "http://arxiv.org/abs/2510.25522v1", "date": "2025-10-29", "relevancy": 2.06, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5149}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Study%20of%20UNet-based%20Architectures%20for%20Liver%20Tumor%0A%20%20Segmentation%20in%20Multi-Phase%20Contrast-Enhanced%20Computed%20Tomography&body=Title%3A%20Comparative%20Study%20of%20UNet-based%20Architectures%20for%20Liver%20Tumor%0A%20%20Segmentation%20in%20Multi-Phase%20Contrast-Enhanced%20Computed%20Tomography%0AAuthor%3A%20Doan-Van-Anh%20Ly%20and%20Thi-Thu-Hien%20Pham%20and%20Thanh-Hai%20Le%0AAbstract%3A%20%20%20Segmentation%20of%20liver%20structures%20in%20multi-phase%20contrast-enhanced%20computed%0Atomography%20%28CECT%29%20plays%20a%20crucial%20role%20in%20computer-aided%20diagnosis%20and%0Atreatment%20planning%20for%20liver%20diseases%2C%20including%20tumor%20detection.%20In%20this%0Astudy%2C%20we%20investigate%20the%20performance%20of%20UNet-based%20architectures%20for%20liver%0Atumor%20segmentation%2C%20starting%20from%20the%20original%20UNet%20and%20extending%20to%20UNet3%2B%0Awith%20various%20backbone%20networks.%20We%20evaluate%20ResNet%2C%20Transformer-based%2C%20and%0AState-space%20%28Mamba%29%20backbones%2C%20all%20initialized%20with%20pretrained%20weights.%0ASurprisingly%2C%20despite%20the%20advances%20in%20modern%20architecture%2C%20ResNet-based%20models%0Aconsistently%20outperform%20Transformer-%20and%20Mamba-based%20alternatives%20across%0Amultiple%20evaluation%20metrics.%20To%20further%20improve%20segmentation%20quality%2C%20we%0Aintroduce%20attention%20mechanisms%20into%20the%20backbone%20and%20observe%20that%20incorporating%0Athe%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%20yields%20the%20best%20performance.%0AResNetUNet3%2B%20with%20CBAM%20module%20not%20only%20produced%20the%20best%20overlap%20metrics%20with%20a%0ADice%20score%20of%200.755%20and%20IoU%20of%200.662%2C%20but%20also%20achieved%20the%20most%20precise%0Aboundary%20delineation%2C%20evidenced%20by%20the%20lowest%20HD95%20distance%20of%2077.911.%20The%0Amodel%27s%20superiority%20was%20further%20cemented%20by%20its%20leading%20overall%20accuracy%20of%0A0.925%20and%20specificity%20of%200.926%2C%20showcasing%20its%20robust%20capability%20in%20accurately%0Aidentifying%20both%20lesion%20and%20healthy%20tissue.%20To%20further%20enhance%0Ainterpretability%2C%20Grad-CAM%20visualizations%20were%20employed%20to%20highlight%20the%0Aregion%27s%20most%20influential%20predictions%2C%20providing%20insights%20into%20its%0Adecision-making%20process.%20These%20findings%20demonstrate%20that%20classical%20ResNet%0Aarchitecture%2C%20when%20combined%20with%20modern%20attention%20modules%2C%20remain%20highly%0Acompetitive%20for%20medical%20image%20segmentation%20tasks%2C%20offering%20a%20promising%0Adirection%20for%20liver%20tumor%20detection%20in%20clinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Study%2520of%2520UNet-based%2520Architectures%2520for%2520Liver%2520Tumor%250A%2520%2520Segmentation%2520in%2520Multi-Phase%2520Contrast-Enhanced%2520Computed%2520Tomography%26entry.906535625%3DDoan-Van-Anh%2520Ly%2520and%2520Thi-Thu-Hien%2520Pham%2520and%2520Thanh-Hai%2520Le%26entry.1292438233%3D%2520%2520Segmentation%2520of%2520liver%2520structures%2520in%2520multi-phase%2520contrast-enhanced%2520computed%250Atomography%2520%2528CECT%2529%2520plays%2520a%2520crucial%2520role%2520in%2520computer-aided%2520diagnosis%2520and%250Atreatment%2520planning%2520for%2520liver%2520diseases%252C%2520including%2520tumor%2520detection.%2520In%2520this%250Astudy%252C%2520we%2520investigate%2520the%2520performance%2520of%2520UNet-based%2520architectures%2520for%2520liver%250Atumor%2520segmentation%252C%2520starting%2520from%2520the%2520original%2520UNet%2520and%2520extending%2520to%2520UNet3%252B%250Awith%2520various%2520backbone%2520networks.%2520We%2520evaluate%2520ResNet%252C%2520Transformer-based%252C%2520and%250AState-space%2520%2528Mamba%2529%2520backbones%252C%2520all%2520initialized%2520with%2520pretrained%2520weights.%250ASurprisingly%252C%2520despite%2520the%2520advances%2520in%2520modern%2520architecture%252C%2520ResNet-based%2520models%250Aconsistently%2520outperform%2520Transformer-%2520and%2520Mamba-based%2520alternatives%2520across%250Amultiple%2520evaluation%2520metrics.%2520To%2520further%2520improve%2520segmentation%2520quality%252C%2520we%250Aintroduce%2520attention%2520mechanisms%2520into%2520the%2520backbone%2520and%2520observe%2520that%2520incorporating%250Athe%2520Convolutional%2520Block%2520Attention%2520Module%2520%2528CBAM%2529%2520yields%2520the%2520best%2520performance.%250AResNetUNet3%252B%2520with%2520CBAM%2520module%2520not%2520only%2520produced%2520the%2520best%2520overlap%2520metrics%2520with%2520a%250ADice%2520score%2520of%25200.755%2520and%2520IoU%2520of%25200.662%252C%2520but%2520also%2520achieved%2520the%2520most%2520precise%250Aboundary%2520delineation%252C%2520evidenced%2520by%2520the%2520lowest%2520HD95%2520distance%2520of%252077.911.%2520The%250Amodel%2527s%2520superiority%2520was%2520further%2520cemented%2520by%2520its%2520leading%2520overall%2520accuracy%2520of%250A0.925%2520and%2520specificity%2520of%25200.926%252C%2520showcasing%2520its%2520robust%2520capability%2520in%2520accurately%250Aidentifying%2520both%2520lesion%2520and%2520healthy%2520tissue.%2520To%2520further%2520enhance%250Ainterpretability%252C%2520Grad-CAM%2520visualizations%2520were%2520employed%2520to%2520highlight%2520the%250Aregion%2527s%2520most%2520influential%2520predictions%252C%2520providing%2520insights%2520into%2520its%250Adecision-making%2520process.%2520These%2520findings%2520demonstrate%2520that%2520classical%2520ResNet%250Aarchitecture%252C%2520when%2520combined%2520with%2520modern%2520attention%2520modules%252C%2520remain%2520highly%250Acompetitive%2520for%2520medical%2520image%2520segmentation%2520tasks%252C%2520offering%2520a%2520promising%250Adirection%2520for%2520liver%2520tumor%2520detection%2520in%2520clinical%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Study%20of%20UNet-based%20Architectures%20for%20Liver%20Tumor%0A%20%20Segmentation%20in%20Multi-Phase%20Contrast-Enhanced%20Computed%20Tomography&entry.906535625=Doan-Van-Anh%20Ly%20and%20Thi-Thu-Hien%20Pham%20and%20Thanh-Hai%20Le&entry.1292438233=%20%20Segmentation%20of%20liver%20structures%20in%20multi-phase%20contrast-enhanced%20computed%0Atomography%20%28CECT%29%20plays%20a%20crucial%20role%20in%20computer-aided%20diagnosis%20and%0Atreatment%20planning%20for%20liver%20diseases%2C%20including%20tumor%20detection.%20In%20this%0Astudy%2C%20we%20investigate%20the%20performance%20of%20UNet-based%20architectures%20for%20liver%0Atumor%20segmentation%2C%20starting%20from%20the%20original%20UNet%20and%20extending%20to%20UNet3%2B%0Awith%20various%20backbone%20networks.%20We%20evaluate%20ResNet%2C%20Transformer-based%2C%20and%0AState-space%20%28Mamba%29%20backbones%2C%20all%20initialized%20with%20pretrained%20weights.%0ASurprisingly%2C%20despite%20the%20advances%20in%20modern%20architecture%2C%20ResNet-based%20models%0Aconsistently%20outperform%20Transformer-%20and%20Mamba-based%20alternatives%20across%0Amultiple%20evaluation%20metrics.%20To%20further%20improve%20segmentation%20quality%2C%20we%0Aintroduce%20attention%20mechanisms%20into%20the%20backbone%20and%20observe%20that%20incorporating%0Athe%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%20yields%20the%20best%20performance.%0AResNetUNet3%2B%20with%20CBAM%20module%20not%20only%20produced%20the%20best%20overlap%20metrics%20with%20a%0ADice%20score%20of%200.755%20and%20IoU%20of%200.662%2C%20but%20also%20achieved%20the%20most%20precise%0Aboundary%20delineation%2C%20evidenced%20by%20the%20lowest%20HD95%20distance%20of%2077.911.%20The%0Amodel%27s%20superiority%20was%20further%20cemented%20by%20its%20leading%20overall%20accuracy%20of%0A0.925%20and%20specificity%20of%200.926%2C%20showcasing%20its%20robust%20capability%20in%20accurately%0Aidentifying%20both%20lesion%20and%20healthy%20tissue.%20To%20further%20enhance%0Ainterpretability%2C%20Grad-CAM%20visualizations%20were%20employed%20to%20highlight%20the%0Aregion%27s%20most%20influential%20predictions%2C%20providing%20insights%20into%20its%0Adecision-making%20process.%20These%20findings%20demonstrate%20that%20classical%20ResNet%0Aarchitecture%2C%20when%20combined%20with%20modern%20attention%20modules%2C%20remain%20highly%0Acompetitive%20for%20medical%20image%20segmentation%20tasks%2C%20offering%20a%20promising%0Adirection%20for%20liver%20tumor%20detection%20in%20clinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25522v1&entry.124074799=Read"},
{"title": "Fine-Tuned Language Models for Domain-Specific Summarization and Tagging", "author": "Jun Wang and Fuming Lin and Yuyu Chen", "abstract": "  This paper presents a pipeline integrating fine-tuned large language models\n(LLMs) with named entity recognition (NER) for efficient domain-specific text\nsummarization and tagging. The authors address the challenge posed by rapidly\nevolving sub-cultural languages and slang, which complicate automated\ninformation extraction and law enforcement monitoring. By leveraging the LLaMA\nFactory framework, the study fine-tunes LLMs on both generalpurpose and custom\ndomain-specific datasets, particularly in the political and security domains.\nThe models are evaluated using BLEU and ROUGE metrics, demonstrating that\ninstruction fine-tuning significantly enhances summarization and tagging\naccuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct\nmodel, despite its initial limitations in Chinese comprehension, outperforms\nits Chinese-trained counterpart after domainspecific fine-tuning, suggesting\nthat underlying reasoning capabilities can transfer across languages. The\npipeline enables concise summaries and structured entity tagging, facilitating\nrapid document categorization and distribution. This approach proves scalable\nand adaptable for real-time applications, supporting efficient information\nmanagement and the ongoing need to capture emerging language trends. The\nintegration of LLMs and NER offers a robust solution for transforming\nunstructured text into actionable insights, crucial for modern knowledge\nmanagement and security operations.\n", "link": "http://arxiv.org/abs/2510.25460v1", "date": "2025-10-29", "relevancy": 2.059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5169}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5169}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuned%20Language%20Models%20for%20Domain-Specific%20Summarization%20and%20Tagging&body=Title%3A%20Fine-Tuned%20Language%20Models%20for%20Domain-Specific%20Summarization%20and%20Tagging%0AAuthor%3A%20Jun%20Wang%20and%20Fuming%20Lin%20and%20Yuyu%20Chen%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20pipeline%20integrating%20fine-tuned%20large%20language%20models%0A%28LLMs%29%20with%20named%20entity%20recognition%20%28NER%29%20for%20efficient%20domain-specific%20text%0Asummarization%20and%20tagging.%20The%20authors%20address%20the%20challenge%20posed%20by%20rapidly%0Aevolving%20sub-cultural%20languages%20and%20slang%2C%20which%20complicate%20automated%0Ainformation%20extraction%20and%20law%20enforcement%20monitoring.%20By%20leveraging%20the%20LLaMA%0AFactory%20framework%2C%20the%20study%20fine-tunes%20LLMs%20on%20both%20generalpurpose%20and%20custom%0Adomain-specific%20datasets%2C%20particularly%20in%20the%20political%20and%20security%20domains.%0AThe%20models%20are%20evaluated%20using%20BLEU%20and%20ROUGE%20metrics%2C%20demonstrating%20that%0Ainstruction%20fine-tuning%20significantly%20enhances%20summarization%20and%20tagging%0Aaccuracy%2C%20especially%20for%20specialized%20corpora.%20Notably%2C%20the%20LLaMA3-8B-Instruct%0Amodel%2C%20despite%20its%20initial%20limitations%20in%20Chinese%20comprehension%2C%20outperforms%0Aits%20Chinese-trained%20counterpart%20after%20domainspecific%20fine-tuning%2C%20suggesting%0Athat%20underlying%20reasoning%20capabilities%20can%20transfer%20across%20languages.%20The%0Apipeline%20enables%20concise%20summaries%20and%20structured%20entity%20tagging%2C%20facilitating%0Arapid%20document%20categorization%20and%20distribution.%20This%20approach%20proves%20scalable%0Aand%20adaptable%20for%20real-time%20applications%2C%20supporting%20efficient%20information%0Amanagement%20and%20the%20ongoing%20need%20to%20capture%20emerging%20language%20trends.%20The%0Aintegration%20of%20LLMs%20and%20NER%20offers%20a%20robust%20solution%20for%20transforming%0Aunstructured%20text%20into%20actionable%20insights%2C%20crucial%20for%20modern%20knowledge%0Amanagement%20and%20security%20operations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuned%2520Language%2520Models%2520for%2520Domain-Specific%2520Summarization%2520and%2520Tagging%26entry.906535625%3DJun%2520Wang%2520and%2520Fuming%2520Lin%2520and%2520Yuyu%2520Chen%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520pipeline%2520integrating%2520fine-tuned%2520large%2520language%2520models%250A%2528LLMs%2529%2520with%2520named%2520entity%2520recognition%2520%2528NER%2529%2520for%2520efficient%2520domain-specific%2520text%250Asummarization%2520and%2520tagging.%2520The%2520authors%2520address%2520the%2520challenge%2520posed%2520by%2520rapidly%250Aevolving%2520sub-cultural%2520languages%2520and%2520slang%252C%2520which%2520complicate%2520automated%250Ainformation%2520extraction%2520and%2520law%2520enforcement%2520monitoring.%2520By%2520leveraging%2520the%2520LLaMA%250AFactory%2520framework%252C%2520the%2520study%2520fine-tunes%2520LLMs%2520on%2520both%2520generalpurpose%2520and%2520custom%250Adomain-specific%2520datasets%252C%2520particularly%2520in%2520the%2520political%2520and%2520security%2520domains.%250AThe%2520models%2520are%2520evaluated%2520using%2520BLEU%2520and%2520ROUGE%2520metrics%252C%2520demonstrating%2520that%250Ainstruction%2520fine-tuning%2520significantly%2520enhances%2520summarization%2520and%2520tagging%250Aaccuracy%252C%2520especially%2520for%2520specialized%2520corpora.%2520Notably%252C%2520the%2520LLaMA3-8B-Instruct%250Amodel%252C%2520despite%2520its%2520initial%2520limitations%2520in%2520Chinese%2520comprehension%252C%2520outperforms%250Aits%2520Chinese-trained%2520counterpart%2520after%2520domainspecific%2520fine-tuning%252C%2520suggesting%250Athat%2520underlying%2520reasoning%2520capabilities%2520can%2520transfer%2520across%2520languages.%2520The%250Apipeline%2520enables%2520concise%2520summaries%2520and%2520structured%2520entity%2520tagging%252C%2520facilitating%250Arapid%2520document%2520categorization%2520and%2520distribution.%2520This%2520approach%2520proves%2520scalable%250Aand%2520adaptable%2520for%2520real-time%2520applications%252C%2520supporting%2520efficient%2520information%250Amanagement%2520and%2520the%2520ongoing%2520need%2520to%2520capture%2520emerging%2520language%2520trends.%2520The%250Aintegration%2520of%2520LLMs%2520and%2520NER%2520offers%2520a%2520robust%2520solution%2520for%2520transforming%250Aunstructured%2520text%2520into%2520actionable%2520insights%252C%2520crucial%2520for%2520modern%2520knowledge%250Amanagement%2520and%2520security%2520operations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuned%20Language%20Models%20for%20Domain-Specific%20Summarization%20and%20Tagging&entry.906535625=Jun%20Wang%20and%20Fuming%20Lin%20and%20Yuyu%20Chen&entry.1292438233=%20%20This%20paper%20presents%20a%20pipeline%20integrating%20fine-tuned%20large%20language%20models%0A%28LLMs%29%20with%20named%20entity%20recognition%20%28NER%29%20for%20efficient%20domain-specific%20text%0Asummarization%20and%20tagging.%20The%20authors%20address%20the%20challenge%20posed%20by%20rapidly%0Aevolving%20sub-cultural%20languages%20and%20slang%2C%20which%20complicate%20automated%0Ainformation%20extraction%20and%20law%20enforcement%20monitoring.%20By%20leveraging%20the%20LLaMA%0AFactory%20framework%2C%20the%20study%20fine-tunes%20LLMs%20on%20both%20generalpurpose%20and%20custom%0Adomain-specific%20datasets%2C%20particularly%20in%20the%20political%20and%20security%20domains.%0AThe%20models%20are%20evaluated%20using%20BLEU%20and%20ROUGE%20metrics%2C%20demonstrating%20that%0Ainstruction%20fine-tuning%20significantly%20enhances%20summarization%20and%20tagging%0Aaccuracy%2C%20especially%20for%20specialized%20corpora.%20Notably%2C%20the%20LLaMA3-8B-Instruct%0Amodel%2C%20despite%20its%20initial%20limitations%20in%20Chinese%20comprehension%2C%20outperforms%0Aits%20Chinese-trained%20counterpart%20after%20domainspecific%20fine-tuning%2C%20suggesting%0Athat%20underlying%20reasoning%20capabilities%20can%20transfer%20across%20languages.%20The%0Apipeline%20enables%20concise%20summaries%20and%20structured%20entity%20tagging%2C%20facilitating%0Arapid%20document%20categorization%20and%20distribution.%20This%20approach%20proves%20scalable%0Aand%20adaptable%20for%20real-time%20applications%2C%20supporting%20efficient%20information%0Amanagement%20and%20the%20ongoing%20need%20to%20capture%20emerging%20language%20trends.%20The%0Aintegration%20of%20LLMs%20and%20NER%20offers%20a%20robust%20solution%20for%20transforming%0Aunstructured%20text%20into%20actionable%20insights%2C%20crucial%20for%20modern%20knowledge%0Amanagement%20and%20security%20operations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25460v1&entry.124074799=Read"},
{"title": "More than a Moment: Towards Coherent Sequences of Audio Descriptions", "author": "Eshika Khandelwal and Junyu Xie and Tengda Han and Max Bain and Arsha Nagrani and Andrew Zisserman and G\u00fcl Varol and Makarand Tapaswi", "abstract": "  Audio Descriptions (ADs) convey essential on-screen information, allowing\nvisually impaired audiences to follow videos. To be effective, ADs must form a\ncoherent sequence that helps listeners to visualise the unfolding scene, rather\nthan describing isolated moments. However, most automatic methods generate each\nAD independently, often resulting in repetitive, incoherent descriptions. To\naddress this, we propose a training-free method, CoherentAD, that first\ngenerates multiple candidate descriptions for each AD time interval, and then\nperforms auto-regressive selection across the sequence to form a coherent and\ninformative narrative. To evaluate AD sequences holistically, we introduce a\nsequence-level metric, StoryRecall, which measures how well the predicted ADs\nconvey the ground truth narrative, alongside repetition metrics that capture\nthe redundancy across consecutive AD outputs. Our method produces coherent AD\nsequences with enhanced narrative understanding, outperforming prior approaches\nthat rely on independent generations.\n", "link": "http://arxiv.org/abs/2510.25440v1", "date": "2025-10-29", "relevancy": 2.054, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20than%20a%20Moment%3A%20Towards%20Coherent%20Sequences%20of%20Audio%20Descriptions&body=Title%3A%20More%20than%20a%20Moment%3A%20Towards%20Coherent%20Sequences%20of%20Audio%20Descriptions%0AAuthor%3A%20Eshika%20Khandelwal%20and%20Junyu%20Xie%20and%20Tengda%20Han%20and%20Max%20Bain%20and%20Arsha%20Nagrani%20and%20Andrew%20Zisserman%20and%20G%C3%BCl%20Varol%20and%20Makarand%20Tapaswi%0AAbstract%3A%20%20%20Audio%20Descriptions%20%28ADs%29%20convey%20essential%20on-screen%20information%2C%20allowing%0Avisually%20impaired%20audiences%20to%20follow%20videos.%20To%20be%20effective%2C%20ADs%20must%20form%20a%0Acoherent%20sequence%20that%20helps%20listeners%20to%20visualise%20the%20unfolding%20scene%2C%20rather%0Athan%20describing%20isolated%20moments.%20However%2C%20most%20automatic%20methods%20generate%20each%0AAD%20independently%2C%20often%20resulting%20in%20repetitive%2C%20incoherent%20descriptions.%20To%0Aaddress%20this%2C%20we%20propose%20a%20training-free%20method%2C%20CoherentAD%2C%20that%20first%0Agenerates%20multiple%20candidate%20descriptions%20for%20each%20AD%20time%20interval%2C%20and%20then%0Aperforms%20auto-regressive%20selection%20across%20the%20sequence%20to%20form%20a%20coherent%20and%0Ainformative%20narrative.%20To%20evaluate%20AD%20sequences%20holistically%2C%20we%20introduce%20a%0Asequence-level%20metric%2C%20StoryRecall%2C%20which%20measures%20how%20well%20the%20predicted%20ADs%0Aconvey%20the%20ground%20truth%20narrative%2C%20alongside%20repetition%20metrics%20that%20capture%0Athe%20redundancy%20across%20consecutive%20AD%20outputs.%20Our%20method%20produces%20coherent%20AD%0Asequences%20with%20enhanced%20narrative%20understanding%2C%20outperforming%20prior%20approaches%0Athat%20rely%20on%20independent%20generations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520than%2520a%2520Moment%253A%2520Towards%2520Coherent%2520Sequences%2520of%2520Audio%2520Descriptions%26entry.906535625%3DEshika%2520Khandelwal%2520and%2520Junyu%2520Xie%2520and%2520Tengda%2520Han%2520and%2520Max%2520Bain%2520and%2520Arsha%2520Nagrani%2520and%2520Andrew%2520Zisserman%2520and%2520G%25C3%25BCl%2520Varol%2520and%2520Makarand%2520Tapaswi%26entry.1292438233%3D%2520%2520Audio%2520Descriptions%2520%2528ADs%2529%2520convey%2520essential%2520on-screen%2520information%252C%2520allowing%250Avisually%2520impaired%2520audiences%2520to%2520follow%2520videos.%2520To%2520be%2520effective%252C%2520ADs%2520must%2520form%2520a%250Acoherent%2520sequence%2520that%2520helps%2520listeners%2520to%2520visualise%2520the%2520unfolding%2520scene%252C%2520rather%250Athan%2520describing%2520isolated%2520moments.%2520However%252C%2520most%2520automatic%2520methods%2520generate%2520each%250AAD%2520independently%252C%2520often%2520resulting%2520in%2520repetitive%252C%2520incoherent%2520descriptions.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520training-free%2520method%252C%2520CoherentAD%252C%2520that%2520first%250Agenerates%2520multiple%2520candidate%2520descriptions%2520for%2520each%2520AD%2520time%2520interval%252C%2520and%2520then%250Aperforms%2520auto-regressive%2520selection%2520across%2520the%2520sequence%2520to%2520form%2520a%2520coherent%2520and%250Ainformative%2520narrative.%2520To%2520evaluate%2520AD%2520sequences%2520holistically%252C%2520we%2520introduce%2520a%250Asequence-level%2520metric%252C%2520StoryRecall%252C%2520which%2520measures%2520how%2520well%2520the%2520predicted%2520ADs%250Aconvey%2520the%2520ground%2520truth%2520narrative%252C%2520alongside%2520repetition%2520metrics%2520that%2520capture%250Athe%2520redundancy%2520across%2520consecutive%2520AD%2520outputs.%2520Our%2520method%2520produces%2520coherent%2520AD%250Asequences%2520with%2520enhanced%2520narrative%2520understanding%252C%2520outperforming%2520prior%2520approaches%250Athat%2520rely%2520on%2520independent%2520generations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20than%20a%20Moment%3A%20Towards%20Coherent%20Sequences%20of%20Audio%20Descriptions&entry.906535625=Eshika%20Khandelwal%20and%20Junyu%20Xie%20and%20Tengda%20Han%20and%20Max%20Bain%20and%20Arsha%20Nagrani%20and%20Andrew%20Zisserman%20and%20G%C3%BCl%20Varol%20and%20Makarand%20Tapaswi&entry.1292438233=%20%20Audio%20Descriptions%20%28ADs%29%20convey%20essential%20on-screen%20information%2C%20allowing%0Avisually%20impaired%20audiences%20to%20follow%20videos.%20To%20be%20effective%2C%20ADs%20must%20form%20a%0Acoherent%20sequence%20that%20helps%20listeners%20to%20visualise%20the%20unfolding%20scene%2C%20rather%0Athan%20describing%20isolated%20moments.%20However%2C%20most%20automatic%20methods%20generate%20each%0AAD%20independently%2C%20often%20resulting%20in%20repetitive%2C%20incoherent%20descriptions.%20To%0Aaddress%20this%2C%20we%20propose%20a%20training-free%20method%2C%20CoherentAD%2C%20that%20first%0Agenerates%20multiple%20candidate%20descriptions%20for%20each%20AD%20time%20interval%2C%20and%20then%0Aperforms%20auto-regressive%20selection%20across%20the%20sequence%20to%20form%20a%20coherent%20and%0Ainformative%20narrative.%20To%20evaluate%20AD%20sequences%20holistically%2C%20we%20introduce%20a%0Asequence-level%20metric%2C%20StoryRecall%2C%20which%20measures%20how%20well%20the%20predicted%20ADs%0Aconvey%20the%20ground%20truth%20narrative%2C%20alongside%20repetition%20metrics%20that%20capture%0Athe%20redundancy%20across%20consecutive%20AD%20outputs.%20Our%20method%20produces%20coherent%20AD%0Asequences%20with%20enhanced%20narrative%20understanding%2C%20outperforming%20prior%20approaches%0Athat%20rely%20on%20independent%20generations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25440v1&entry.124074799=Read"},
{"title": "Continuous Domain Generalization", "author": "Zekun Cai and Yiheng Yao and Guangji Bai and Renhe Jiang and Xuan Song and Ryosuke Shibasaki and Liang Zhao", "abstract": "  Real-world data distributions often shift continuously across multiple latent\nfactors such as time, geography, and socioeconomic contexts. However, existing\ndomain generalization approaches typically treat domains as discrete or as\nevolving along a single axis (e.g., time). This oversimplification fails to\ncapture the complex, multidimensional nature of real-world variation. This\npaper introduces the task of Continuous Domain Generalization (CDG), which aims\nto generalize predictive models to unseen domains defined by arbitrary\ncombinations of continuous variations. We present a principled framework\ngrounded in geometric and algebraic theories, showing that optimal model\nparameters across domains lie on a low-dimensional manifold. To model this\nstructure, we propose a Neural Lie Transport Operator (NeuralLio), which\nenables structure-preserving parameter transitions by enforcing geometric\ncontinuity and algebraic consistency. To handle noisy or incomplete domain\nvariation descriptors, we introduce a gating mechanism to suppress irrelevant\ndimensions and a local chart-based strategy for robust generalization.\nExtensive experiments on synthetic and real-world datasets, including remote\nsensing, scientific documents, and traffic forecasting, demonstrate that our\nmethod significantly outperforms existing baselines in both generalization\naccuracy and robustness.\n", "link": "http://arxiv.org/abs/2505.13519v2", "date": "2025-10-29", "relevancy": 2.0505, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5264}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5129}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Domain%20Generalization&body=Title%3A%20Continuous%20Domain%20Generalization%0AAuthor%3A%20Zekun%20Cai%20and%20Yiheng%20Yao%20and%20Guangji%20Bai%20and%20Renhe%20Jiang%20and%20Xuan%20Song%20and%20Ryosuke%20Shibasaki%20and%20Liang%20Zhao%0AAbstract%3A%20%20%20Real-world%20data%20distributions%20often%20shift%20continuously%20across%20multiple%20latent%0Afactors%20such%20as%20time%2C%20geography%2C%20and%20socioeconomic%20contexts.%20However%2C%20existing%0Adomain%20generalization%20approaches%20typically%20treat%20domains%20as%20discrete%20or%20as%0Aevolving%20along%20a%20single%20axis%20%28e.g.%2C%20time%29.%20This%20oversimplification%20fails%20to%0Acapture%20the%20complex%2C%20multidimensional%20nature%20of%20real-world%20variation.%20This%0Apaper%20introduces%20the%20task%20of%20Continuous%20Domain%20Generalization%20%28CDG%29%2C%20which%20aims%0Ato%20generalize%20predictive%20models%20to%20unseen%20domains%20defined%20by%20arbitrary%0Acombinations%20of%20continuous%20variations.%20We%20present%20a%20principled%20framework%0Agrounded%20in%20geometric%20and%20algebraic%20theories%2C%20showing%20that%20optimal%20model%0Aparameters%20across%20domains%20lie%20on%20a%20low-dimensional%20manifold.%20To%20model%20this%0Astructure%2C%20we%20propose%20a%20Neural%20Lie%20Transport%20Operator%20%28NeuralLio%29%2C%20which%0Aenables%20structure-preserving%20parameter%20transitions%20by%20enforcing%20geometric%0Acontinuity%20and%20algebraic%20consistency.%20To%20handle%20noisy%20or%20incomplete%20domain%0Avariation%20descriptors%2C%20we%20introduce%20a%20gating%20mechanism%20to%20suppress%20irrelevant%0Adimensions%20and%20a%20local%20chart-based%20strategy%20for%20robust%20generalization.%0AExtensive%20experiments%20on%20synthetic%20and%20real-world%20datasets%2C%20including%20remote%0Asensing%2C%20scientific%20documents%2C%20and%20traffic%20forecasting%2C%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20existing%20baselines%20in%20both%20generalization%0Aaccuracy%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13519v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Domain%2520Generalization%26entry.906535625%3DZekun%2520Cai%2520and%2520Yiheng%2520Yao%2520and%2520Guangji%2520Bai%2520and%2520Renhe%2520Jiang%2520and%2520Xuan%2520Song%2520and%2520Ryosuke%2520Shibasaki%2520and%2520Liang%2520Zhao%26entry.1292438233%3D%2520%2520Real-world%2520data%2520distributions%2520often%2520shift%2520continuously%2520across%2520multiple%2520latent%250Afactors%2520such%2520as%2520time%252C%2520geography%252C%2520and%2520socioeconomic%2520contexts.%2520However%252C%2520existing%250Adomain%2520generalization%2520approaches%2520typically%2520treat%2520domains%2520as%2520discrete%2520or%2520as%250Aevolving%2520along%2520a%2520single%2520axis%2520%2528e.g.%252C%2520time%2529.%2520This%2520oversimplification%2520fails%2520to%250Acapture%2520the%2520complex%252C%2520multidimensional%2520nature%2520of%2520real-world%2520variation.%2520This%250Apaper%2520introduces%2520the%2520task%2520of%2520Continuous%2520Domain%2520Generalization%2520%2528CDG%2529%252C%2520which%2520aims%250Ato%2520generalize%2520predictive%2520models%2520to%2520unseen%2520domains%2520defined%2520by%2520arbitrary%250Acombinations%2520of%2520continuous%2520variations.%2520We%2520present%2520a%2520principled%2520framework%250Agrounded%2520in%2520geometric%2520and%2520algebraic%2520theories%252C%2520showing%2520that%2520optimal%2520model%250Aparameters%2520across%2520domains%2520lie%2520on%2520a%2520low-dimensional%2520manifold.%2520To%2520model%2520this%250Astructure%252C%2520we%2520propose%2520a%2520Neural%2520Lie%2520Transport%2520Operator%2520%2528NeuralLio%2529%252C%2520which%250Aenables%2520structure-preserving%2520parameter%2520transitions%2520by%2520enforcing%2520geometric%250Acontinuity%2520and%2520algebraic%2520consistency.%2520To%2520handle%2520noisy%2520or%2520incomplete%2520domain%250Avariation%2520descriptors%252C%2520we%2520introduce%2520a%2520gating%2520mechanism%2520to%2520suppress%2520irrelevant%250Adimensions%2520and%2520a%2520local%2520chart-based%2520strategy%2520for%2520robust%2520generalization.%250AExtensive%2520experiments%2520on%2520synthetic%2520and%2520real-world%2520datasets%252C%2520including%2520remote%250Asensing%252C%2520scientific%2520documents%252C%2520and%2520traffic%2520forecasting%252C%2520demonstrate%2520that%2520our%250Amethod%2520significantly%2520outperforms%2520existing%2520baselines%2520in%2520both%2520generalization%250Aaccuracy%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13519v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Domain%20Generalization&entry.906535625=Zekun%20Cai%20and%20Yiheng%20Yao%20and%20Guangji%20Bai%20and%20Renhe%20Jiang%20and%20Xuan%20Song%20and%20Ryosuke%20Shibasaki%20and%20Liang%20Zhao&entry.1292438233=%20%20Real-world%20data%20distributions%20often%20shift%20continuously%20across%20multiple%20latent%0Afactors%20such%20as%20time%2C%20geography%2C%20and%20socioeconomic%20contexts.%20However%2C%20existing%0Adomain%20generalization%20approaches%20typically%20treat%20domains%20as%20discrete%20or%20as%0Aevolving%20along%20a%20single%20axis%20%28e.g.%2C%20time%29.%20This%20oversimplification%20fails%20to%0Acapture%20the%20complex%2C%20multidimensional%20nature%20of%20real-world%20variation.%20This%0Apaper%20introduces%20the%20task%20of%20Continuous%20Domain%20Generalization%20%28CDG%29%2C%20which%20aims%0Ato%20generalize%20predictive%20models%20to%20unseen%20domains%20defined%20by%20arbitrary%0Acombinations%20of%20continuous%20variations.%20We%20present%20a%20principled%20framework%0Agrounded%20in%20geometric%20and%20algebraic%20theories%2C%20showing%20that%20optimal%20model%0Aparameters%20across%20domains%20lie%20on%20a%20low-dimensional%20manifold.%20To%20model%20this%0Astructure%2C%20we%20propose%20a%20Neural%20Lie%20Transport%20Operator%20%28NeuralLio%29%2C%20which%0Aenables%20structure-preserving%20parameter%20transitions%20by%20enforcing%20geometric%0Acontinuity%20and%20algebraic%20consistency.%20To%20handle%20noisy%20or%20incomplete%20domain%0Avariation%20descriptors%2C%20we%20introduce%20a%20gating%20mechanism%20to%20suppress%20irrelevant%0Adimensions%20and%20a%20local%20chart-based%20strategy%20for%20robust%20generalization.%0AExtensive%20experiments%20on%20synthetic%20and%20real-world%20datasets%2C%20including%20remote%0Asensing%2C%20scientific%20documents%2C%20and%20traffic%20forecasting%2C%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20existing%20baselines%20in%20both%20generalization%0Aaccuracy%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13519v2&entry.124074799=Read"},
{"title": "Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for and with\n  Foundation Models", "author": "Daoyuan Chen and Yilun Huang and Xuchen Pan and Nana Jiang and Haibin Wang and Yilei Zhang and Ce Ge and Yushuo Chen and Wenhao Zhang and Zhijian Ma and Jun Huang and Wei Lin and Yaliang Li and Bolin Ding and Jingren Zhou", "abstract": "  Foundation models demand advanced data processing for their vast, multimodal\ndatasets. However, traditional frameworks struggle with the unique complexities\nof multimodal data. In response, we present Data-Juicer 2.0, a data processing\nsystem backed by 100+ data processing operators spanning text, image, video,\nand audio modalities, supporting more critical tasks including data analysis,\nsynthesis, annotation, and foundation model post-training. With seamless\ncompatibility and dedicated optimization for popular dataset hubs like Hugging\nFace and computing engines like Ray, it improves upon its predecessor in terms\nof usability, efficiency, and programmability. It features an easily accessible\nuser interface layer that supports decoupled Python interactions, RESTful APIs,\nand conversational commands. Its new runtime layer offers adaptive execution\nacross diverse scales and environments, abstracting away system complexities.\nExtensive empirical evaluations demonstrate Data-Juicer 2.0's remarkable\nperformance and scalability, highlighting its capability to efficiently process\nTB-level data with 10k+ CPU cores. The system is publicly available and has\nbeen widely adopted in diverse research fields and real-world products such as\nAlibaba Cloud PAI. We actively maintain the system and share practical insights\nto foster research and applications of next-generation foundation models.\n", "link": "http://arxiv.org/abs/2501.14755v3", "date": "2025-10-29", "relevancy": 2.0496, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Juicer%202.0%3A%20Cloud-Scale%20Adaptive%20Data%20Processing%20for%20and%20with%0A%20%20Foundation%20Models&body=Title%3A%20Data-Juicer%202.0%3A%20Cloud-Scale%20Adaptive%20Data%20Processing%20for%20and%20with%0A%20%20Foundation%20Models%0AAuthor%3A%20Daoyuan%20Chen%20and%20Yilun%20Huang%20and%20Xuchen%20Pan%20and%20Nana%20Jiang%20and%20Haibin%20Wang%20and%20Yilei%20Zhang%20and%20Ce%20Ge%20and%20Yushuo%20Chen%20and%20Wenhao%20Zhang%20and%20Zhijian%20Ma%20and%20Jun%20Huang%20and%20Wei%20Lin%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20Foundation%20models%20demand%20advanced%20data%20processing%20for%20their%20vast%2C%20multimodal%0Adatasets.%20However%2C%20traditional%20frameworks%20struggle%20with%20the%20unique%20complexities%0Aof%20multimodal%20data.%20In%20response%2C%20we%20present%20Data-Juicer%202.0%2C%20a%20data%20processing%0Asystem%20backed%20by%20100%2B%20data%20processing%20operators%20spanning%20text%2C%20image%2C%20video%2C%0Aand%20audio%20modalities%2C%20supporting%20more%20critical%20tasks%20including%20data%20analysis%2C%0Asynthesis%2C%20annotation%2C%20and%20foundation%20model%20post-training.%20With%20seamless%0Acompatibility%20and%20dedicated%20optimization%20for%20popular%20dataset%20hubs%20like%20Hugging%0AFace%20and%20computing%20engines%20like%20Ray%2C%20it%20improves%20upon%20its%20predecessor%20in%20terms%0Aof%20usability%2C%20efficiency%2C%20and%20programmability.%20It%20features%20an%20easily%20accessible%0Auser%20interface%20layer%20that%20supports%20decoupled%20Python%20interactions%2C%20RESTful%20APIs%2C%0Aand%20conversational%20commands.%20Its%20new%20runtime%20layer%20offers%20adaptive%20execution%0Aacross%20diverse%20scales%20and%20environments%2C%20abstracting%20away%20system%20complexities.%0AExtensive%20empirical%20evaluations%20demonstrate%20Data-Juicer%202.0%27s%20remarkable%0Aperformance%20and%20scalability%2C%20highlighting%20its%20capability%20to%20efficiently%20process%0ATB-level%20data%20with%2010k%2B%20CPU%20cores.%20The%20system%20is%20publicly%20available%20and%20has%0Abeen%20widely%20adopted%20in%20diverse%20research%20fields%20and%20real-world%20products%20such%20as%0AAlibaba%20Cloud%20PAI.%20We%20actively%20maintain%20the%20system%20and%20share%20practical%20insights%0Ato%20foster%20research%20and%20applications%20of%20next-generation%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14755v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Juicer%25202.0%253A%2520Cloud-Scale%2520Adaptive%2520Data%2520Processing%2520for%2520and%2520with%250A%2520%2520Foundation%2520Models%26entry.906535625%3DDaoyuan%2520Chen%2520and%2520Yilun%2520Huang%2520and%2520Xuchen%2520Pan%2520and%2520Nana%2520Jiang%2520and%2520Haibin%2520Wang%2520and%2520Yilei%2520Zhang%2520and%2520Ce%2520Ge%2520and%2520Yushuo%2520Chen%2520and%2520Wenhao%2520Zhang%2520and%2520Zhijian%2520Ma%2520and%2520Jun%2520Huang%2520and%2520Wei%2520Lin%2520and%2520Yaliang%2520Li%2520and%2520Bolin%2520Ding%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520Foundation%2520models%2520demand%2520advanced%2520data%2520processing%2520for%2520their%2520vast%252C%2520multimodal%250Adatasets.%2520However%252C%2520traditional%2520frameworks%2520struggle%2520with%2520the%2520unique%2520complexities%250Aof%2520multimodal%2520data.%2520In%2520response%252C%2520we%2520present%2520Data-Juicer%25202.0%252C%2520a%2520data%2520processing%250Asystem%2520backed%2520by%2520100%252B%2520data%2520processing%2520operators%2520spanning%2520text%252C%2520image%252C%2520video%252C%250Aand%2520audio%2520modalities%252C%2520supporting%2520more%2520critical%2520tasks%2520including%2520data%2520analysis%252C%250Asynthesis%252C%2520annotation%252C%2520and%2520foundation%2520model%2520post-training.%2520With%2520seamless%250Acompatibility%2520and%2520dedicated%2520optimization%2520for%2520popular%2520dataset%2520hubs%2520like%2520Hugging%250AFace%2520and%2520computing%2520engines%2520like%2520Ray%252C%2520it%2520improves%2520upon%2520its%2520predecessor%2520in%2520terms%250Aof%2520usability%252C%2520efficiency%252C%2520and%2520programmability.%2520It%2520features%2520an%2520easily%2520accessible%250Auser%2520interface%2520layer%2520that%2520supports%2520decoupled%2520Python%2520interactions%252C%2520RESTful%2520APIs%252C%250Aand%2520conversational%2520commands.%2520Its%2520new%2520runtime%2520layer%2520offers%2520adaptive%2520execution%250Aacross%2520diverse%2520scales%2520and%2520environments%252C%2520abstracting%2520away%2520system%2520complexities.%250AExtensive%2520empirical%2520evaluations%2520demonstrate%2520Data-Juicer%25202.0%2527s%2520remarkable%250Aperformance%2520and%2520scalability%252C%2520highlighting%2520its%2520capability%2520to%2520efficiently%2520process%250ATB-level%2520data%2520with%252010k%252B%2520CPU%2520cores.%2520The%2520system%2520is%2520publicly%2520available%2520and%2520has%250Abeen%2520widely%2520adopted%2520in%2520diverse%2520research%2520fields%2520and%2520real-world%2520products%2520such%2520as%250AAlibaba%2520Cloud%2520PAI.%2520We%2520actively%2520maintain%2520the%2520system%2520and%2520share%2520practical%2520insights%250Ato%2520foster%2520research%2520and%2520applications%2520of%2520next-generation%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14755v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Juicer%202.0%3A%20Cloud-Scale%20Adaptive%20Data%20Processing%20for%20and%20with%0A%20%20Foundation%20Models&entry.906535625=Daoyuan%20Chen%20and%20Yilun%20Huang%20and%20Xuchen%20Pan%20and%20Nana%20Jiang%20and%20Haibin%20Wang%20and%20Yilei%20Zhang%20and%20Ce%20Ge%20and%20Yushuo%20Chen%20and%20Wenhao%20Zhang%20and%20Zhijian%20Ma%20and%20Jun%20Huang%20and%20Wei%20Lin%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Jingren%20Zhou&entry.1292438233=%20%20Foundation%20models%20demand%20advanced%20data%20processing%20for%20their%20vast%2C%20multimodal%0Adatasets.%20However%2C%20traditional%20frameworks%20struggle%20with%20the%20unique%20complexities%0Aof%20multimodal%20data.%20In%20response%2C%20we%20present%20Data-Juicer%202.0%2C%20a%20data%20processing%0Asystem%20backed%20by%20100%2B%20data%20processing%20operators%20spanning%20text%2C%20image%2C%20video%2C%0Aand%20audio%20modalities%2C%20supporting%20more%20critical%20tasks%20including%20data%20analysis%2C%0Asynthesis%2C%20annotation%2C%20and%20foundation%20model%20post-training.%20With%20seamless%0Acompatibility%20and%20dedicated%20optimization%20for%20popular%20dataset%20hubs%20like%20Hugging%0AFace%20and%20computing%20engines%20like%20Ray%2C%20it%20improves%20upon%20its%20predecessor%20in%20terms%0Aof%20usability%2C%20efficiency%2C%20and%20programmability.%20It%20features%20an%20easily%20accessible%0Auser%20interface%20layer%20that%20supports%20decoupled%20Python%20interactions%2C%20RESTful%20APIs%2C%0Aand%20conversational%20commands.%20Its%20new%20runtime%20layer%20offers%20adaptive%20execution%0Aacross%20diverse%20scales%20and%20environments%2C%20abstracting%20away%20system%20complexities.%0AExtensive%20empirical%20evaluations%20demonstrate%20Data-Juicer%202.0%27s%20remarkable%0Aperformance%20and%20scalability%2C%20highlighting%20its%20capability%20to%20efficiently%20process%0ATB-level%20data%20with%2010k%2B%20CPU%20cores.%20The%20system%20is%20publicly%20available%20and%20has%0Abeen%20widely%20adopted%20in%20diverse%20research%20fields%20and%20real-world%20products%20such%20as%0AAlibaba%20Cloud%20PAI.%20We%20actively%20maintain%20the%20system%20and%20share%20practical%20insights%0Ato%20foster%20research%20and%20applications%20of%20next-generation%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14755v3&entry.124074799=Read"},
{"title": "Monitoring the calibration of probability forecasts with an application\n  to concept drift detection involving image classification", "author": "Christopher T. Franck and Anne R. Driscoll and Zoe Szajnfarber and William H. Woodall", "abstract": "  Machine learning approaches for image classification have led to impressive\nadvances in that field. For example, convolutional neural networks are able to\nachieve remarkable image classification accuracy across a wide range of\napplications in industry, defense, and other areas. While these machine\nlearning models boast impressive accuracy, a related concern is how to assess\nand maintain calibration in the predictions these models make. A classification\nmodel is said to be well calibrated if its predicted probabilities correspond\nwith the rates events actually occur. While there are many available methods to\nassess machine learning calibration and recalibrate faulty predictions, less\neffort has been spent on developing approaches that continually monitor\npredictive models for potential loss of calibration as time passes. We propose\na cumulative sum-based approach with dynamic limits that enable detection of\nmiscalibration in both traditional process monitoring and concept drift\napplications. This enables early detection of operational context changes that\nimpact image classification performance in the field. The proposed chart can be\nused broadly in any situation where the user needs to monitor probability\npredictions over time for potential lapses in calibration. Importantly, our\nmethod operates on probability predictions and event outcomes and does not\nrequire under-the-hood access to the machine learning model.\n", "link": "http://arxiv.org/abs/2510.25573v1", "date": "2025-10-29", "relevancy": 2.0456, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.531}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5305}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monitoring%20the%20calibration%20of%20probability%20forecasts%20with%20an%20application%0A%20%20to%20concept%20drift%20detection%20involving%20image%20classification&body=Title%3A%20Monitoring%20the%20calibration%20of%20probability%20forecasts%20with%20an%20application%0A%20%20to%20concept%20drift%20detection%20involving%20image%20classification%0AAuthor%3A%20Christopher%20T.%20Franck%20and%20Anne%20R.%20Driscoll%20and%20Zoe%20Szajnfarber%20and%20William%20H.%20Woodall%0AAbstract%3A%20%20%20Machine%20learning%20approaches%20for%20image%20classification%20have%20led%20to%20impressive%0Aadvances%20in%20that%20field.%20For%20example%2C%20convolutional%20neural%20networks%20are%20able%20to%0Aachieve%20remarkable%20image%20classification%20accuracy%20across%20a%20wide%20range%20of%0Aapplications%20in%20industry%2C%20defense%2C%20and%20other%20areas.%20While%20these%20machine%0Alearning%20models%20boast%20impressive%20accuracy%2C%20a%20related%20concern%20is%20how%20to%20assess%0Aand%20maintain%20calibration%20in%20the%20predictions%20these%20models%20make.%20A%20classification%0Amodel%20is%20said%20to%20be%20well%20calibrated%20if%20its%20predicted%20probabilities%20correspond%0Awith%20the%20rates%20events%20actually%20occur.%20While%20there%20are%20many%20available%20methods%20to%0Aassess%20machine%20learning%20calibration%20and%20recalibrate%20faulty%20predictions%2C%20less%0Aeffort%20has%20been%20spent%20on%20developing%20approaches%20that%20continually%20monitor%0Apredictive%20models%20for%20potential%20loss%20of%20calibration%20as%20time%20passes.%20We%20propose%0Aa%20cumulative%20sum-based%20approach%20with%20dynamic%20limits%20that%20enable%20detection%20of%0Amiscalibration%20in%20both%20traditional%20process%20monitoring%20and%20concept%20drift%0Aapplications.%20This%20enables%20early%20detection%20of%20operational%20context%20changes%20that%0Aimpact%20image%20classification%20performance%20in%20the%20field.%20The%20proposed%20chart%20can%20be%0Aused%20broadly%20in%20any%20situation%20where%20the%20user%20needs%20to%20monitor%20probability%0Apredictions%20over%20time%20for%20potential%20lapses%20in%20calibration.%20Importantly%2C%20our%0Amethod%20operates%20on%20probability%20predictions%20and%20event%20outcomes%20and%20does%20not%0Arequire%20under-the-hood%20access%20to%20the%20machine%20learning%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonitoring%2520the%2520calibration%2520of%2520probability%2520forecasts%2520with%2520an%2520application%250A%2520%2520to%2520concept%2520drift%2520detection%2520involving%2520image%2520classification%26entry.906535625%3DChristopher%2520T.%2520Franck%2520and%2520Anne%2520R.%2520Driscoll%2520and%2520Zoe%2520Szajnfarber%2520and%2520William%2520H.%2520Woodall%26entry.1292438233%3D%2520%2520Machine%2520learning%2520approaches%2520for%2520image%2520classification%2520have%2520led%2520to%2520impressive%250Aadvances%2520in%2520that%2520field.%2520For%2520example%252C%2520convolutional%2520neural%2520networks%2520are%2520able%2520to%250Aachieve%2520remarkable%2520image%2520classification%2520accuracy%2520across%2520a%2520wide%2520range%2520of%250Aapplications%2520in%2520industry%252C%2520defense%252C%2520and%2520other%2520areas.%2520While%2520these%2520machine%250Alearning%2520models%2520boast%2520impressive%2520accuracy%252C%2520a%2520related%2520concern%2520is%2520how%2520to%2520assess%250Aand%2520maintain%2520calibration%2520in%2520the%2520predictions%2520these%2520models%2520make.%2520A%2520classification%250Amodel%2520is%2520said%2520to%2520be%2520well%2520calibrated%2520if%2520its%2520predicted%2520probabilities%2520correspond%250Awith%2520the%2520rates%2520events%2520actually%2520occur.%2520While%2520there%2520are%2520many%2520available%2520methods%2520to%250Aassess%2520machine%2520learning%2520calibration%2520and%2520recalibrate%2520faulty%2520predictions%252C%2520less%250Aeffort%2520has%2520been%2520spent%2520on%2520developing%2520approaches%2520that%2520continually%2520monitor%250Apredictive%2520models%2520for%2520potential%2520loss%2520of%2520calibration%2520as%2520time%2520passes.%2520We%2520propose%250Aa%2520cumulative%2520sum-based%2520approach%2520with%2520dynamic%2520limits%2520that%2520enable%2520detection%2520of%250Amiscalibration%2520in%2520both%2520traditional%2520process%2520monitoring%2520and%2520concept%2520drift%250Aapplications.%2520This%2520enables%2520early%2520detection%2520of%2520operational%2520context%2520changes%2520that%250Aimpact%2520image%2520classification%2520performance%2520in%2520the%2520field.%2520The%2520proposed%2520chart%2520can%2520be%250Aused%2520broadly%2520in%2520any%2520situation%2520where%2520the%2520user%2520needs%2520to%2520monitor%2520probability%250Apredictions%2520over%2520time%2520for%2520potential%2520lapses%2520in%2520calibration.%2520Importantly%252C%2520our%250Amethod%2520operates%2520on%2520probability%2520predictions%2520and%2520event%2520outcomes%2520and%2520does%2520not%250Arequire%2520under-the-hood%2520access%2520to%2520the%2520machine%2520learning%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monitoring%20the%20calibration%20of%20probability%20forecasts%20with%20an%20application%0A%20%20to%20concept%20drift%20detection%20involving%20image%20classification&entry.906535625=Christopher%20T.%20Franck%20and%20Anne%20R.%20Driscoll%20and%20Zoe%20Szajnfarber%20and%20William%20H.%20Woodall&entry.1292438233=%20%20Machine%20learning%20approaches%20for%20image%20classification%20have%20led%20to%20impressive%0Aadvances%20in%20that%20field.%20For%20example%2C%20convolutional%20neural%20networks%20are%20able%20to%0Aachieve%20remarkable%20image%20classification%20accuracy%20across%20a%20wide%20range%20of%0Aapplications%20in%20industry%2C%20defense%2C%20and%20other%20areas.%20While%20these%20machine%0Alearning%20models%20boast%20impressive%20accuracy%2C%20a%20related%20concern%20is%20how%20to%20assess%0Aand%20maintain%20calibration%20in%20the%20predictions%20these%20models%20make.%20A%20classification%0Amodel%20is%20said%20to%20be%20well%20calibrated%20if%20its%20predicted%20probabilities%20correspond%0Awith%20the%20rates%20events%20actually%20occur.%20While%20there%20are%20many%20available%20methods%20to%0Aassess%20machine%20learning%20calibration%20and%20recalibrate%20faulty%20predictions%2C%20less%0Aeffort%20has%20been%20spent%20on%20developing%20approaches%20that%20continually%20monitor%0Apredictive%20models%20for%20potential%20loss%20of%20calibration%20as%20time%20passes.%20We%20propose%0Aa%20cumulative%20sum-based%20approach%20with%20dynamic%20limits%20that%20enable%20detection%20of%0Amiscalibration%20in%20both%20traditional%20process%20monitoring%20and%20concept%20drift%0Aapplications.%20This%20enables%20early%20detection%20of%20operational%20context%20changes%20that%0Aimpact%20image%20classification%20performance%20in%20the%20field.%20The%20proposed%20chart%20can%20be%0Aused%20broadly%20in%20any%20situation%20where%20the%20user%20needs%20to%20monitor%20probability%0Apredictions%20over%20time%20for%20potential%20lapses%20in%20calibration.%20Importantly%2C%20our%0Amethod%20operates%20on%20probability%20predictions%20and%20event%20outcomes%20and%20does%20not%0Arequire%20under-the-hood%20access%20to%20the%20machine%20learning%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25573v1&entry.124074799=Read"},
{"title": "Brain-inspired Computational Intelligence via Predictive Coding", "author": "Tommaso Salvatori and Ankur Mali and Christopher L. Buckley and Thomas Lukasiewicz and Rajesh P. N. Rao and Karl Friston and Alexander Ororbia", "abstract": "  Artificial intelligence (AI) is rapidly becoming one of the key technologies\nof this century. The majority of results in AI thus far have been achieved\nusing deep neural networks trained with a learning algorithm called error\nbackpropagation, always considered biologically implausible. To this end,\nrecent works have studied learning algorithms for deep neural networks inspired\nby the neurosciences. One such theory, called predictive coding (PC), has shown\npromising properties that make it potentially valuable for the machine learning\ncommunity: it can model information processing in different areas of the brain,\ncan be used in control and robotics, has a solid mathematical foundation in\nvariational inference, and performs its computations asynchronously. Inspired\nby such properties, works that propose novel PC-like algorithms are starting to\nbe present in multiple sub-fields of machine learning and AI at large. Here, we\nsurvey such efforts by first providing a broad overview of the history of PC to\nprovide common ground for the understanding of the recent developments, then by\ndescribing current efforts and results, and concluding with a large discussion\nof possible implications and ways forward.\n", "link": "http://arxiv.org/abs/2308.07870v3", "date": "2025-10-29", "relevancy": 2.0271, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5446}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.507}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain-inspired%20Computational%20Intelligence%20via%20Predictive%20Coding&body=Title%3A%20Brain-inspired%20Computational%20Intelligence%20via%20Predictive%20Coding%0AAuthor%3A%20Tommaso%20Salvatori%20and%20Ankur%20Mali%20and%20Christopher%20L.%20Buckley%20and%20Thomas%20Lukasiewicz%20and%20Rajesh%20P.%20N.%20Rao%20and%20Karl%20Friston%20and%20Alexander%20Ororbia%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20is%20rapidly%20becoming%20one%20of%20the%20key%20technologies%0Aof%20this%20century.%20The%20majority%20of%20results%20in%20AI%20thus%20far%20have%20been%20achieved%0Ausing%20deep%20neural%20networks%20trained%20with%20a%20learning%20algorithm%20called%20error%0Abackpropagation%2C%20always%20considered%20biologically%20implausible.%20To%20this%20end%2C%0Arecent%20works%20have%20studied%20learning%20algorithms%20for%20deep%20neural%20networks%20inspired%0Aby%20the%20neurosciences.%20One%20such%20theory%2C%20called%20predictive%20coding%20%28PC%29%2C%20has%20shown%0Apromising%20properties%20that%20make%20it%20potentially%20valuable%20for%20the%20machine%20learning%0Acommunity%3A%20it%20can%20model%20information%20processing%20in%20different%20areas%20of%20the%20brain%2C%0Acan%20be%20used%20in%20control%20and%20robotics%2C%20has%20a%20solid%20mathematical%20foundation%20in%0Avariational%20inference%2C%20and%20performs%20its%20computations%20asynchronously.%20Inspired%0Aby%20such%20properties%2C%20works%20that%20propose%20novel%20PC-like%20algorithms%20are%20starting%20to%0Abe%20present%20in%20multiple%20sub-fields%20of%20machine%20learning%20and%20AI%20at%20large.%20Here%2C%20we%0Asurvey%20such%20efforts%20by%20first%20providing%20a%20broad%20overview%20of%20the%20history%20of%20PC%20to%0Aprovide%20common%20ground%20for%20the%20understanding%20of%20the%20recent%20developments%2C%20then%20by%0Adescribing%20current%20efforts%20and%20results%2C%20and%20concluding%20with%20a%20large%20discussion%0Aof%20possible%20implications%20and%20ways%20forward.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.07870v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain-inspired%2520Computational%2520Intelligence%2520via%2520Predictive%2520Coding%26entry.906535625%3DTommaso%2520Salvatori%2520and%2520Ankur%2520Mali%2520and%2520Christopher%2520L.%2520Buckley%2520and%2520Thomas%2520Lukasiewicz%2520and%2520Rajesh%2520P.%2520N.%2520Rao%2520and%2520Karl%2520Friston%2520and%2520Alexander%2520Ororbia%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520is%2520rapidly%2520becoming%2520one%2520of%2520the%2520key%2520technologies%250Aof%2520this%2520century.%2520The%2520majority%2520of%2520results%2520in%2520AI%2520thus%2520far%2520have%2520been%2520achieved%250Ausing%2520deep%2520neural%2520networks%2520trained%2520with%2520a%2520learning%2520algorithm%2520called%2520error%250Abackpropagation%252C%2520always%2520considered%2520biologically%2520implausible.%2520To%2520this%2520end%252C%250Arecent%2520works%2520have%2520studied%2520learning%2520algorithms%2520for%2520deep%2520neural%2520networks%2520inspired%250Aby%2520the%2520neurosciences.%2520One%2520such%2520theory%252C%2520called%2520predictive%2520coding%2520%2528PC%2529%252C%2520has%2520shown%250Apromising%2520properties%2520that%2520make%2520it%2520potentially%2520valuable%2520for%2520the%2520machine%2520learning%250Acommunity%253A%2520it%2520can%2520model%2520information%2520processing%2520in%2520different%2520areas%2520of%2520the%2520brain%252C%250Acan%2520be%2520used%2520in%2520control%2520and%2520robotics%252C%2520has%2520a%2520solid%2520mathematical%2520foundation%2520in%250Avariational%2520inference%252C%2520and%2520performs%2520its%2520computations%2520asynchronously.%2520Inspired%250Aby%2520such%2520properties%252C%2520works%2520that%2520propose%2520novel%2520PC-like%2520algorithms%2520are%2520starting%2520to%250Abe%2520present%2520in%2520multiple%2520sub-fields%2520of%2520machine%2520learning%2520and%2520AI%2520at%2520large.%2520Here%252C%2520we%250Asurvey%2520such%2520efforts%2520by%2520first%2520providing%2520a%2520broad%2520overview%2520of%2520the%2520history%2520of%2520PC%2520to%250Aprovide%2520common%2520ground%2520for%2520the%2520understanding%2520of%2520the%2520recent%2520developments%252C%2520then%2520by%250Adescribing%2520current%2520efforts%2520and%2520results%252C%2520and%2520concluding%2520with%2520a%2520large%2520discussion%250Aof%2520possible%2520implications%2520and%2520ways%2520forward.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.07870v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain-inspired%20Computational%20Intelligence%20via%20Predictive%20Coding&entry.906535625=Tommaso%20Salvatori%20and%20Ankur%20Mali%20and%20Christopher%20L.%20Buckley%20and%20Thomas%20Lukasiewicz%20and%20Rajesh%20P.%20N.%20Rao%20and%20Karl%20Friston%20and%20Alexander%20Ororbia&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20is%20rapidly%20becoming%20one%20of%20the%20key%20technologies%0Aof%20this%20century.%20The%20majority%20of%20results%20in%20AI%20thus%20far%20have%20been%20achieved%0Ausing%20deep%20neural%20networks%20trained%20with%20a%20learning%20algorithm%20called%20error%0Abackpropagation%2C%20always%20considered%20biologically%20implausible.%20To%20this%20end%2C%0Arecent%20works%20have%20studied%20learning%20algorithms%20for%20deep%20neural%20networks%20inspired%0Aby%20the%20neurosciences.%20One%20such%20theory%2C%20called%20predictive%20coding%20%28PC%29%2C%20has%20shown%0Apromising%20properties%20that%20make%20it%20potentially%20valuable%20for%20the%20machine%20learning%0Acommunity%3A%20it%20can%20model%20information%20processing%20in%20different%20areas%20of%20the%20brain%2C%0Acan%20be%20used%20in%20control%20and%20robotics%2C%20has%20a%20solid%20mathematical%20foundation%20in%0Avariational%20inference%2C%20and%20performs%20its%20computations%20asynchronously.%20Inspired%0Aby%20such%20properties%2C%20works%20that%20propose%20novel%20PC-like%20algorithms%20are%20starting%20to%0Abe%20present%20in%20multiple%20sub-fields%20of%20machine%20learning%20and%20AI%20at%20large.%20Here%2C%20we%0Asurvey%20such%20efforts%20by%20first%20providing%20a%20broad%20overview%20of%20the%20history%20of%20PC%20to%0Aprovide%20common%20ground%20for%20the%20understanding%20of%20the%20recent%20developments%2C%20then%20by%0Adescribing%20current%20efforts%20and%20results%2C%20and%20concluding%20with%20a%20large%20discussion%0Aof%20possible%20implications%20and%20ways%20forward.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.07870v3&entry.124074799=Read"},
{"title": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption\n  Masking And Normalization", "author": "Filip Sondej and Yushi Yang and Miko\u0142aj Kniejski and Marcel Windys", "abstract": "  Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new\nstate-of-the-art for robust unlearning.\n", "link": "http://arxiv.org/abs/2506.12484v4", "date": "2025-10-29", "relevancy": 2.0266, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5506}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5051}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20LLM%20Unlearning%20with%20MUDMAN%3A%20Meta-Unlearning%20with%20Disruption%0A%20%20Masking%20And%20Normalization&body=Title%3A%20Robust%20LLM%20Unlearning%20with%20MUDMAN%3A%20Meta-Unlearning%20with%20Disruption%0A%20%20Masking%20And%20Normalization%0AAuthor%3A%20Filip%20Sondej%20and%20Yushi%20Yang%20and%20Miko%C5%82aj%20Kniejski%20and%20Marcel%20Windys%0AAbstract%3A%20%20%20Language%20models%20can%20retain%20dangerous%20knowledge%20and%20skills%20even%20after%0Aextensive%20safety%20fine-tuning%2C%20posing%20both%20misuse%20and%20misalignment%20risks.%20Recent%0Astudies%20show%20that%20even%20specialized%20unlearning%20methods%20can%20be%20easily%20reversed.%0ATo%20address%20this%2C%20we%20systematically%20evaluate%20many%20existing%20and%20novel%20components%0Aof%20unlearning%20methods%20and%20identify%20ones%20crucial%20for%20irreversible%20unlearning.%0A%20%20We%20introduce%20Disruption%20Masking%2C%20a%20technique%20in%20which%20we%20only%20allow%20updating%0Aweights%2C%20where%20the%20signs%20of%20the%20unlearning%20gradient%20and%20the%20retaining%20gradient%0Aare%20the%20same.%20This%20ensures%20all%20updates%20are%20non-disruptive.%0A%20%20Additionally%2C%20we%20identify%20the%20need%20for%20normalizing%20the%20unlearning%20gradients%2C%0Aand%20also%20confirm%20the%20usefulness%20of%20meta-learning.%20We%20combine%20these%20insights%0Ainto%20MUDMAN%20%28Meta-Unlearning%20with%20Disruption%20Masking%20and%20Normalization%29%20and%0Avalidate%20its%20effectiveness%20at%20preventing%20the%20recovery%20of%20dangerous%0Acapabilities.%20MUDMAN%20outperforms%20the%20prior%20TAR%20method%20by%2040%25%2C%20setting%20a%20new%0Astate-of-the-art%20for%20robust%20unlearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12484v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520LLM%2520Unlearning%2520with%2520MUDMAN%253A%2520Meta-Unlearning%2520with%2520Disruption%250A%2520%2520Masking%2520And%2520Normalization%26entry.906535625%3DFilip%2520Sondej%2520and%2520Yushi%2520Yang%2520and%2520Miko%25C5%2582aj%2520Kniejski%2520and%2520Marcel%2520Windys%26entry.1292438233%3D%2520%2520Language%2520models%2520can%2520retain%2520dangerous%2520knowledge%2520and%2520skills%2520even%2520after%250Aextensive%2520safety%2520fine-tuning%252C%2520posing%2520both%2520misuse%2520and%2520misalignment%2520risks.%2520Recent%250Astudies%2520show%2520that%2520even%2520specialized%2520unlearning%2520methods%2520can%2520be%2520easily%2520reversed.%250ATo%2520address%2520this%252C%2520we%2520systematically%2520evaluate%2520many%2520existing%2520and%2520novel%2520components%250Aof%2520unlearning%2520methods%2520and%2520identify%2520ones%2520crucial%2520for%2520irreversible%2520unlearning.%250A%2520%2520We%2520introduce%2520Disruption%2520Masking%252C%2520a%2520technique%2520in%2520which%2520we%2520only%2520allow%2520updating%250Aweights%252C%2520where%2520the%2520signs%2520of%2520the%2520unlearning%2520gradient%2520and%2520the%2520retaining%2520gradient%250Aare%2520the%2520same.%2520This%2520ensures%2520all%2520updates%2520are%2520non-disruptive.%250A%2520%2520Additionally%252C%2520we%2520identify%2520the%2520need%2520for%2520normalizing%2520the%2520unlearning%2520gradients%252C%250Aand%2520also%2520confirm%2520the%2520usefulness%2520of%2520meta-learning.%2520We%2520combine%2520these%2520insights%250Ainto%2520MUDMAN%2520%2528Meta-Unlearning%2520with%2520Disruption%2520Masking%2520and%2520Normalization%2529%2520and%250Avalidate%2520its%2520effectiveness%2520at%2520preventing%2520the%2520recovery%2520of%2520dangerous%250Acapabilities.%2520MUDMAN%2520outperforms%2520the%2520prior%2520TAR%2520method%2520by%252040%2525%252C%2520setting%2520a%2520new%250Astate-of-the-art%2520for%2520robust%2520unlearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12484v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20LLM%20Unlearning%20with%20MUDMAN%3A%20Meta-Unlearning%20with%20Disruption%0A%20%20Masking%20And%20Normalization&entry.906535625=Filip%20Sondej%20and%20Yushi%20Yang%20and%20Miko%C5%82aj%20Kniejski%20and%20Marcel%20Windys&entry.1292438233=%20%20Language%20models%20can%20retain%20dangerous%20knowledge%20and%20skills%20even%20after%0Aextensive%20safety%20fine-tuning%2C%20posing%20both%20misuse%20and%20misalignment%20risks.%20Recent%0Astudies%20show%20that%20even%20specialized%20unlearning%20methods%20can%20be%20easily%20reversed.%0ATo%20address%20this%2C%20we%20systematically%20evaluate%20many%20existing%20and%20novel%20components%0Aof%20unlearning%20methods%20and%20identify%20ones%20crucial%20for%20irreversible%20unlearning.%0A%20%20We%20introduce%20Disruption%20Masking%2C%20a%20technique%20in%20which%20we%20only%20allow%20updating%0Aweights%2C%20where%20the%20signs%20of%20the%20unlearning%20gradient%20and%20the%20retaining%20gradient%0Aare%20the%20same.%20This%20ensures%20all%20updates%20are%20non-disruptive.%0A%20%20Additionally%2C%20we%20identify%20the%20need%20for%20normalizing%20the%20unlearning%20gradients%2C%0Aand%20also%20confirm%20the%20usefulness%20of%20meta-learning.%20We%20combine%20these%20insights%0Ainto%20MUDMAN%20%28Meta-Unlearning%20with%20Disruption%20Masking%20and%20Normalization%29%20and%0Avalidate%20its%20effectiveness%20at%20preventing%20the%20recovery%20of%20dangerous%0Acapabilities.%20MUDMAN%20outperforms%20the%20prior%20TAR%20method%20by%2040%25%2C%20setting%20a%20new%0Astate-of-the-art%20for%20robust%20unlearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12484v4&entry.124074799=Read"},
{"title": "The Limits of Obliviate: Evaluating Unlearning in LLMs via\n  Stimulus-Knowledge Entanglement-Behavior Framework", "author": "Aakriti Shah and Thai Le", "abstract": "  Unlearning in large language models (LLMs) is crucial for managing sensitive\ndata and correcting misinformation, yet evaluating its effectiveness remains an\nopen problem. We investigate whether persuasive prompting can recall factual\nknowledge from deliberately unlearned LLMs across models ranging from 2.7B to\n13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from\nACT-R and Hebbian theory (spreading activation theories), as well as\ncommunication principles, we introduce Stimulus-Knowledge Entanglement-Behavior\nFramework (SKeB), which models information entanglement via domain graphs and\ntests whether factual recall in unlearned models is correlated with persuasive\nframing. We develop entanglement metrics to quantify knowledge activation\npatterns and evaluate factuality, non-factuality, and hallucination in outputs.\nOur results show persuasive prompts substantially enhance factual knowledge\nrecall (14.8% baseline vs. 24.5% with authority framing), with effectiveness\ninversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB\nprovides a foundation for assessing unlearning completeness, robustness, and\noverall behavior in LLMs.\n", "link": "http://arxiv.org/abs/2510.25732v1", "date": "2025-10-29", "relevancy": 2.0195, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Limits%20of%20Obliviate%3A%20Evaluating%20Unlearning%20in%20LLMs%20via%0A%20%20Stimulus-Knowledge%20Entanglement-Behavior%20Framework&body=Title%3A%20The%20Limits%20of%20Obliviate%3A%20Evaluating%20Unlearning%20in%20LLMs%20via%0A%20%20Stimulus-Knowledge%20Entanglement-Behavior%20Framework%0AAuthor%3A%20Aakriti%20Shah%20and%20Thai%20Le%0AAbstract%3A%20%20%20Unlearning%20in%20large%20language%20models%20%28LLMs%29%20is%20crucial%20for%20managing%20sensitive%0Adata%20and%20correcting%20misinformation%2C%20yet%20evaluating%20its%20effectiveness%20remains%20an%0Aopen%20problem.%20We%20investigate%20whether%20persuasive%20prompting%20can%20recall%20factual%0Aknowledge%20from%20deliberately%20unlearned%20LLMs%20across%20models%20ranging%20from%202.7B%20to%0A13B%20parameters%20%28OPT-2.7B%2C%20LLaMA-2-7B%2C%20LLaMA-3.1-8B%2C%20LLaMA-2-13B%29.%20Drawing%20from%0AACT-R%20and%20Hebbian%20theory%20%28spreading%20activation%20theories%29%2C%20as%20well%20as%0Acommunication%20principles%2C%20we%20introduce%20Stimulus-Knowledge%20Entanglement-Behavior%0AFramework%20%28SKeB%29%2C%20which%20models%20information%20entanglement%20via%20domain%20graphs%20and%0Atests%20whether%20factual%20recall%20in%20unlearned%20models%20is%20correlated%20with%20persuasive%0Aframing.%20We%20develop%20entanglement%20metrics%20to%20quantify%20knowledge%20activation%0Apatterns%20and%20evaluate%20factuality%2C%20non-factuality%2C%20and%20hallucination%20in%20outputs.%0AOur%20results%20show%20persuasive%20prompts%20substantially%20enhance%20factual%20knowledge%0Arecall%20%2814.8%25%20baseline%20vs.%2024.5%25%20with%20authority%20framing%29%2C%20with%20effectiveness%0Ainversely%20correlated%20to%20model%20size%20%28128%25%20recovery%20in%202.7B%20vs.%2015%25%20in%2013B%29.%20SKeB%0Aprovides%20a%20foundation%20for%20assessing%20unlearning%20completeness%2C%20robustness%2C%20and%0Aoverall%20behavior%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Limits%2520of%2520Obliviate%253A%2520Evaluating%2520Unlearning%2520in%2520LLMs%2520via%250A%2520%2520Stimulus-Knowledge%2520Entanglement-Behavior%2520Framework%26entry.906535625%3DAakriti%2520Shah%2520and%2520Thai%2520Le%26entry.1292438233%3D%2520%2520Unlearning%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520crucial%2520for%2520managing%2520sensitive%250Adata%2520and%2520correcting%2520misinformation%252C%2520yet%2520evaluating%2520its%2520effectiveness%2520remains%2520an%250Aopen%2520problem.%2520We%2520investigate%2520whether%2520persuasive%2520prompting%2520can%2520recall%2520factual%250Aknowledge%2520from%2520deliberately%2520unlearned%2520LLMs%2520across%2520models%2520ranging%2520from%25202.7B%2520to%250A13B%2520parameters%2520%2528OPT-2.7B%252C%2520LLaMA-2-7B%252C%2520LLaMA-3.1-8B%252C%2520LLaMA-2-13B%2529.%2520Drawing%2520from%250AACT-R%2520and%2520Hebbian%2520theory%2520%2528spreading%2520activation%2520theories%2529%252C%2520as%2520well%2520as%250Acommunication%2520principles%252C%2520we%2520introduce%2520Stimulus-Knowledge%2520Entanglement-Behavior%250AFramework%2520%2528SKeB%2529%252C%2520which%2520models%2520information%2520entanglement%2520via%2520domain%2520graphs%2520and%250Atests%2520whether%2520factual%2520recall%2520in%2520unlearned%2520models%2520is%2520correlated%2520with%2520persuasive%250Aframing.%2520We%2520develop%2520entanglement%2520metrics%2520to%2520quantify%2520knowledge%2520activation%250Apatterns%2520and%2520evaluate%2520factuality%252C%2520non-factuality%252C%2520and%2520hallucination%2520in%2520outputs.%250AOur%2520results%2520show%2520persuasive%2520prompts%2520substantially%2520enhance%2520factual%2520knowledge%250Arecall%2520%252814.8%2525%2520baseline%2520vs.%252024.5%2525%2520with%2520authority%2520framing%2529%252C%2520with%2520effectiveness%250Ainversely%2520correlated%2520to%2520model%2520size%2520%2528128%2525%2520recovery%2520in%25202.7B%2520vs.%252015%2525%2520in%252013B%2529.%2520SKeB%250Aprovides%2520a%2520foundation%2520for%2520assessing%2520unlearning%2520completeness%252C%2520robustness%252C%2520and%250Aoverall%2520behavior%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Limits%20of%20Obliviate%3A%20Evaluating%20Unlearning%20in%20LLMs%20via%0A%20%20Stimulus-Knowledge%20Entanglement-Behavior%20Framework&entry.906535625=Aakriti%20Shah%20and%20Thai%20Le&entry.1292438233=%20%20Unlearning%20in%20large%20language%20models%20%28LLMs%29%20is%20crucial%20for%20managing%20sensitive%0Adata%20and%20correcting%20misinformation%2C%20yet%20evaluating%20its%20effectiveness%20remains%20an%0Aopen%20problem.%20We%20investigate%20whether%20persuasive%20prompting%20can%20recall%20factual%0Aknowledge%20from%20deliberately%20unlearned%20LLMs%20across%20models%20ranging%20from%202.7B%20to%0A13B%20parameters%20%28OPT-2.7B%2C%20LLaMA-2-7B%2C%20LLaMA-3.1-8B%2C%20LLaMA-2-13B%29.%20Drawing%20from%0AACT-R%20and%20Hebbian%20theory%20%28spreading%20activation%20theories%29%2C%20as%20well%20as%0Acommunication%20principles%2C%20we%20introduce%20Stimulus-Knowledge%20Entanglement-Behavior%0AFramework%20%28SKeB%29%2C%20which%20models%20information%20entanglement%20via%20domain%20graphs%20and%0Atests%20whether%20factual%20recall%20in%20unlearned%20models%20is%20correlated%20with%20persuasive%0Aframing.%20We%20develop%20entanglement%20metrics%20to%20quantify%20knowledge%20activation%0Apatterns%20and%20evaluate%20factuality%2C%20non-factuality%2C%20and%20hallucination%20in%20outputs.%0AOur%20results%20show%20persuasive%20prompts%20substantially%20enhance%20factual%20knowledge%0Arecall%20%2814.8%25%20baseline%20vs.%2024.5%25%20with%20authority%20framing%29%2C%20with%20effectiveness%0Ainversely%20correlated%20to%20model%20size%20%28128%25%20recovery%20in%202.7B%20vs.%2015%25%20in%2013B%29.%20SKeB%0Aprovides%20a%20foundation%20for%20assessing%20unlearning%20completeness%2C%20robustness%2C%20and%0Aoverall%20behavior%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25732v1&entry.124074799=Read"},
{"title": "Implicature in Interaction: Understanding Implicature Improves Alignment\n  in Human-LLM Interaction", "author": "Asutosh Hota and Jussi P. P. Jokinen", "abstract": "  The rapid advancement of Large Language Models (LLMs) is positioning language\nat the core of human-computer interaction (HCI). We argue that advancing HCI\nrequires attention to the linguistic foundations of interaction, particularly\nimplicature (meaning conveyed beyond explicit statements through shared\ncontext) which is essential for human-AI (HAI) alignment. This study examines\nLLMs' ability to infer user intent embedded in context-driven prompts and\nwhether understanding implicature improves response generation. Results show\nthat larger models approximate human interpretations more closely, while\nsmaller models struggle with implicature inference. Furthermore,\nimplicature-based prompts significantly enhance the perceived relevance and\nquality of responses across models, with notable gains in smaller models.\nOverall, 67.6% of participants preferred responses with implicature-embedded\nprompts to literal ones, highlighting a clear preference for contextually\nnuanced communication. Our work contributes to understanding how linguistic\ntheory can be used to address the alignment problem by making HAI interaction\nmore natural and contextually grounded.\n", "link": "http://arxiv.org/abs/2510.25426v1", "date": "2025-10-29", "relevancy": 2.018, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicature%20in%20Interaction%3A%20Understanding%20Implicature%20Improves%20Alignment%0A%20%20in%20Human-LLM%20Interaction&body=Title%3A%20Implicature%20in%20Interaction%3A%20Understanding%20Implicature%20Improves%20Alignment%0A%20%20in%20Human-LLM%20Interaction%0AAuthor%3A%20Asutosh%20Hota%20and%20Jussi%20P.%20P.%20Jokinen%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20is%20positioning%20language%0Aat%20the%20core%20of%20human-computer%20interaction%20%28HCI%29.%20We%20argue%20that%20advancing%20HCI%0Arequires%20attention%20to%20the%20linguistic%20foundations%20of%20interaction%2C%20particularly%0Aimplicature%20%28meaning%20conveyed%20beyond%20explicit%20statements%20through%20shared%0Acontext%29%20which%20is%20essential%20for%20human-AI%20%28HAI%29%20alignment.%20This%20study%20examines%0ALLMs%27%20ability%20to%20infer%20user%20intent%20embedded%20in%20context-driven%20prompts%20and%0Awhether%20understanding%20implicature%20improves%20response%20generation.%20Results%20show%0Athat%20larger%20models%20approximate%20human%20interpretations%20more%20closely%2C%20while%0Asmaller%20models%20struggle%20with%20implicature%20inference.%20Furthermore%2C%0Aimplicature-based%20prompts%20significantly%20enhance%20the%20perceived%20relevance%20and%0Aquality%20of%20responses%20across%20models%2C%20with%20notable%20gains%20in%20smaller%20models.%0AOverall%2C%2067.6%25%20of%20participants%20preferred%20responses%20with%20implicature-embedded%0Aprompts%20to%20literal%20ones%2C%20highlighting%20a%20clear%20preference%20for%20contextually%0Anuanced%20communication.%20Our%20work%20contributes%20to%20understanding%20how%20linguistic%0Atheory%20can%20be%20used%20to%20address%20the%20alignment%20problem%20by%20making%20HAI%20interaction%0Amore%20natural%20and%20contextually%20grounded.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicature%2520in%2520Interaction%253A%2520Understanding%2520Implicature%2520Improves%2520Alignment%250A%2520%2520in%2520Human-LLM%2520Interaction%26entry.906535625%3DAsutosh%2520Hota%2520and%2520Jussi%2520P.%2520P.%2520Jokinen%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520positioning%2520language%250Aat%2520the%2520core%2520of%2520human-computer%2520interaction%2520%2528HCI%2529.%2520We%2520argue%2520that%2520advancing%2520HCI%250Arequires%2520attention%2520to%2520the%2520linguistic%2520foundations%2520of%2520interaction%252C%2520particularly%250Aimplicature%2520%2528meaning%2520conveyed%2520beyond%2520explicit%2520statements%2520through%2520shared%250Acontext%2529%2520which%2520is%2520essential%2520for%2520human-AI%2520%2528HAI%2529%2520alignment.%2520This%2520study%2520examines%250ALLMs%2527%2520ability%2520to%2520infer%2520user%2520intent%2520embedded%2520in%2520context-driven%2520prompts%2520and%250Awhether%2520understanding%2520implicature%2520improves%2520response%2520generation.%2520Results%2520show%250Athat%2520larger%2520models%2520approximate%2520human%2520interpretations%2520more%2520closely%252C%2520while%250Asmaller%2520models%2520struggle%2520with%2520implicature%2520inference.%2520Furthermore%252C%250Aimplicature-based%2520prompts%2520significantly%2520enhance%2520the%2520perceived%2520relevance%2520and%250Aquality%2520of%2520responses%2520across%2520models%252C%2520with%2520notable%2520gains%2520in%2520smaller%2520models.%250AOverall%252C%252067.6%2525%2520of%2520participants%2520preferred%2520responses%2520with%2520implicature-embedded%250Aprompts%2520to%2520literal%2520ones%252C%2520highlighting%2520a%2520clear%2520preference%2520for%2520contextually%250Anuanced%2520communication.%2520Our%2520work%2520contributes%2520to%2520understanding%2520how%2520linguistic%250Atheory%2520can%2520be%2520used%2520to%2520address%2520the%2520alignment%2520problem%2520by%2520making%2520HAI%2520interaction%250Amore%2520natural%2520and%2520contextually%2520grounded.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicature%20in%20Interaction%3A%20Understanding%20Implicature%20Improves%20Alignment%0A%20%20in%20Human-LLM%20Interaction&entry.906535625=Asutosh%20Hota%20and%20Jussi%20P.%20P.%20Jokinen&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20is%20positioning%20language%0Aat%20the%20core%20of%20human-computer%20interaction%20%28HCI%29.%20We%20argue%20that%20advancing%20HCI%0Arequires%20attention%20to%20the%20linguistic%20foundations%20of%20interaction%2C%20particularly%0Aimplicature%20%28meaning%20conveyed%20beyond%20explicit%20statements%20through%20shared%0Acontext%29%20which%20is%20essential%20for%20human-AI%20%28HAI%29%20alignment.%20This%20study%20examines%0ALLMs%27%20ability%20to%20infer%20user%20intent%20embedded%20in%20context-driven%20prompts%20and%0Awhether%20understanding%20implicature%20improves%20response%20generation.%20Results%20show%0Athat%20larger%20models%20approximate%20human%20interpretations%20more%20closely%2C%20while%0Asmaller%20models%20struggle%20with%20implicature%20inference.%20Furthermore%2C%0Aimplicature-based%20prompts%20significantly%20enhance%20the%20perceived%20relevance%20and%0Aquality%20of%20responses%20across%20models%2C%20with%20notable%20gains%20in%20smaller%20models.%0AOverall%2C%2067.6%25%20of%20participants%20preferred%20responses%20with%20implicature-embedded%0Aprompts%20to%20literal%20ones%2C%20highlighting%20a%20clear%20preference%20for%20contextually%0Anuanced%20communication.%20Our%20work%20contributes%20to%20understanding%20how%20linguistic%0Atheory%20can%20be%20used%20to%20address%20the%20alignment%20problem%20by%20making%20HAI%20interaction%0Amore%20natural%20and%20contextually%20grounded.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25426v1&entry.124074799=Read"},
{"title": "BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training", "author": "Mohammadreza Tavasoli Naeini and Ali Bereyhi and Morteza Noshad and Ben Liang and Alfred O. Hero III", "abstract": "  We introduce BOLT-GAN, a simple yet effective modification of the WGAN\nframework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that\nwith a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a\ndifferent metric distance than the Earth Mover (Wasserstein) distance and\nachieves better training stability. Empirical evaluations on four standard\nimage generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN\nChurch-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%\nlower Frechet Inception Distance (FID). Our results suggest that BOLT is a\nbroadly applicable principle for enhancing GAN training.\n", "link": "http://arxiv.org/abs/2510.25609v1", "date": "2025-10-29", "relevancy": 2.0163, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5231}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.501}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BOLT-GAN%3A%20Bayes-Optimal%20Loss%20for%20Stable%20GAN%20Training&body=Title%3A%20BOLT-GAN%3A%20Bayes-Optimal%20Loss%20for%20Stable%20GAN%20Training%0AAuthor%3A%20Mohammadreza%20Tavasoli%20Naeini%20and%20Ali%20Bereyhi%20and%20Morteza%20Noshad%20and%20Ben%20Liang%20and%20Alfred%20O.%20Hero%20III%0AAbstract%3A%20%20%20We%20introduce%20BOLT-GAN%2C%20a%20simple%20yet%20effective%20modification%20of%20the%20WGAN%0Aframework%20inspired%20by%20the%20Bayes%20Optimal%20Learning%20Threshold%20%28BOLT%29.%20We%20show%20that%0Awith%20a%20Lipschitz%20continuous%20discriminator%2C%20BOLT-GAN%20implicitly%20minimizes%20a%0Adifferent%20metric%20distance%20than%20the%20Earth%20Mover%20%28Wasserstein%29%20distance%20and%0Aachieves%20better%20training%20stability.%20Empirical%20evaluations%20on%20four%20standard%0Aimage%20generation%20benchmarks%20%28CIFAR-10%2C%20CelebA-64%2C%20LSUN%20Bedroom-64%2C%20and%20LSUN%0AChurch-64%29%20show%20that%20BOLT-GAN%20consistently%20outperforms%20WGAN%2C%20achieving%2010-60%25%0Alower%20Frechet%20Inception%20Distance%20%28FID%29.%20Our%20results%20suggest%20that%20BOLT%20is%20a%0Abroadly%20applicable%20principle%20for%20enhancing%20GAN%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBOLT-GAN%253A%2520Bayes-Optimal%2520Loss%2520for%2520Stable%2520GAN%2520Training%26entry.906535625%3DMohammadreza%2520Tavasoli%2520Naeini%2520and%2520Ali%2520Bereyhi%2520and%2520Morteza%2520Noshad%2520and%2520Ben%2520Liang%2520and%2520Alfred%2520O.%2520Hero%2520III%26entry.1292438233%3D%2520%2520We%2520introduce%2520BOLT-GAN%252C%2520a%2520simple%2520yet%2520effective%2520modification%2520of%2520the%2520WGAN%250Aframework%2520inspired%2520by%2520the%2520Bayes%2520Optimal%2520Learning%2520Threshold%2520%2528BOLT%2529.%2520We%2520show%2520that%250Awith%2520a%2520Lipschitz%2520continuous%2520discriminator%252C%2520BOLT-GAN%2520implicitly%2520minimizes%2520a%250Adifferent%2520metric%2520distance%2520than%2520the%2520Earth%2520Mover%2520%2528Wasserstein%2529%2520distance%2520and%250Aachieves%2520better%2520training%2520stability.%2520Empirical%2520evaluations%2520on%2520four%2520standard%250Aimage%2520generation%2520benchmarks%2520%2528CIFAR-10%252C%2520CelebA-64%252C%2520LSUN%2520Bedroom-64%252C%2520and%2520LSUN%250AChurch-64%2529%2520show%2520that%2520BOLT-GAN%2520consistently%2520outperforms%2520WGAN%252C%2520achieving%252010-60%2525%250Alower%2520Frechet%2520Inception%2520Distance%2520%2528FID%2529.%2520Our%2520results%2520suggest%2520that%2520BOLT%2520is%2520a%250Abroadly%2520applicable%2520principle%2520for%2520enhancing%2520GAN%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BOLT-GAN%3A%20Bayes-Optimal%20Loss%20for%20Stable%20GAN%20Training&entry.906535625=Mohammadreza%20Tavasoli%20Naeini%20and%20Ali%20Bereyhi%20and%20Morteza%20Noshad%20and%20Ben%20Liang%20and%20Alfred%20O.%20Hero%20III&entry.1292438233=%20%20We%20introduce%20BOLT-GAN%2C%20a%20simple%20yet%20effective%20modification%20of%20the%20WGAN%0Aframework%20inspired%20by%20the%20Bayes%20Optimal%20Learning%20Threshold%20%28BOLT%29.%20We%20show%20that%0Awith%20a%20Lipschitz%20continuous%20discriminator%2C%20BOLT-GAN%20implicitly%20minimizes%20a%0Adifferent%20metric%20distance%20than%20the%20Earth%20Mover%20%28Wasserstein%29%20distance%20and%0Aachieves%20better%20training%20stability.%20Empirical%20evaluations%20on%20four%20standard%0Aimage%20generation%20benchmarks%20%28CIFAR-10%2C%20CelebA-64%2C%20LSUN%20Bedroom-64%2C%20and%20LSUN%0AChurch-64%29%20show%20that%20BOLT-GAN%20consistently%20outperforms%20WGAN%2C%20achieving%2010-60%25%0Alower%20Frechet%20Inception%20Distance%20%28FID%29.%20Our%20results%20suggest%20that%20BOLT%20is%20a%0Abroadly%20applicable%20principle%20for%20enhancing%20GAN%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25609v1&entry.124074799=Read"},
{"title": "HyperMARL: Adaptive Hypernetworks for Multi-Agent RL", "author": "Kale-ab Abebe Tessera and Arrasy Rahman and Amos Storkey and Stefano V. Albrecht", "abstract": "  Adaptive cooperation in multi-agent reinforcement learning (MARL) requires\npolicies to express homogeneous, specialised, or mixed behaviours, yet\nachieving this adaptivity remains a critical challenge. While parameter sharing\n(PS) is standard for efficient learning, it notoriously suppresses the\nbehavioural diversity required for specialisation. This failure is largely due\nto cross-agent gradient interference, a problem we find is surprisingly\nexacerbated by the common practice of coupling agent IDs with observations.\nExisting remedies typically add complexity through altered objectives, manual\npreset diversity levels, or sequential updates -- raising a fundamental\nquestion: can shared policies adapt without these intricacies? We propose a\nsolution built on a key insight: an agent-conditioned hypernetwork can generate\nagent-specific parameters and decouple observation- and agent-conditioned\ngradients, directly countering the interference from coupling agent IDs with\nobservations. Our resulting method, HyperMARL, avoids the complexities of prior\nwork and empirically reduces policy gradient variance. Across diverse MARL\nbenchmarks (22 scenarios, up to 30 agents), HyperMARL achieves performance\ncompetitive with six key baselines while preserving behavioural diversity\ncomparable to non-parameter sharing methods, establishing it as a versatile and\nprincipled approach for adaptive MARL. The code is publicly available at\nhttps://github.com/KaleabTessera/HyperMARL.\n", "link": "http://arxiv.org/abs/2412.04233v4", "date": "2025-10-29", "relevancy": 2.0118, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5437}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5043}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperMARL%3A%20Adaptive%20Hypernetworks%20for%20Multi-Agent%20RL&body=Title%3A%20HyperMARL%3A%20Adaptive%20Hypernetworks%20for%20Multi-Agent%20RL%0AAuthor%3A%20Kale-ab%20Abebe%20Tessera%20and%20Arrasy%20Rahman%20and%20Amos%20Storkey%20and%20Stefano%20V.%20Albrecht%0AAbstract%3A%20%20%20Adaptive%20cooperation%20in%20multi-agent%20reinforcement%20learning%20%28MARL%29%20requires%0Apolicies%20to%20express%20homogeneous%2C%20specialised%2C%20or%20mixed%20behaviours%2C%20yet%0Aachieving%20this%20adaptivity%20remains%20a%20critical%20challenge.%20While%20parameter%20sharing%0A%28PS%29%20is%20standard%20for%20efficient%20learning%2C%20it%20notoriously%20suppresses%20the%0Abehavioural%20diversity%20required%20for%20specialisation.%20This%20failure%20is%20largely%20due%0Ato%20cross-agent%20gradient%20interference%2C%20a%20problem%20we%20find%20is%20surprisingly%0Aexacerbated%20by%20the%20common%20practice%20of%20coupling%20agent%20IDs%20with%20observations.%0AExisting%20remedies%20typically%20add%20complexity%20through%20altered%20objectives%2C%20manual%0Apreset%20diversity%20levels%2C%20or%20sequential%20updates%20--%20raising%20a%20fundamental%0Aquestion%3A%20can%20shared%20policies%20adapt%20without%20these%20intricacies%3F%20We%20propose%20a%0Asolution%20built%20on%20a%20key%20insight%3A%20an%20agent-conditioned%20hypernetwork%20can%20generate%0Aagent-specific%20parameters%20and%20decouple%20observation-%20and%20agent-conditioned%0Agradients%2C%20directly%20countering%20the%20interference%20from%20coupling%20agent%20IDs%20with%0Aobservations.%20Our%20resulting%20method%2C%20HyperMARL%2C%20avoids%20the%20complexities%20of%20prior%0Awork%20and%20empirically%20reduces%20policy%20gradient%20variance.%20Across%20diverse%20MARL%0Abenchmarks%20%2822%20scenarios%2C%20up%20to%2030%20agents%29%2C%20HyperMARL%20achieves%20performance%0Acompetitive%20with%20six%20key%20baselines%20while%20preserving%20behavioural%20diversity%0Acomparable%20to%20non-parameter%20sharing%20methods%2C%20establishing%20it%20as%20a%20versatile%20and%0Aprincipled%20approach%20for%20adaptive%20MARL.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/KaleabTessera/HyperMARL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04233v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperMARL%253A%2520Adaptive%2520Hypernetworks%2520for%2520Multi-Agent%2520RL%26entry.906535625%3DKale-ab%2520Abebe%2520Tessera%2520and%2520Arrasy%2520Rahman%2520and%2520Amos%2520Storkey%2520and%2520Stefano%2520V.%2520Albrecht%26entry.1292438233%3D%2520%2520Adaptive%2520cooperation%2520in%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%2520requires%250Apolicies%2520to%2520express%2520homogeneous%252C%2520specialised%252C%2520or%2520mixed%2520behaviours%252C%2520yet%250Aachieving%2520this%2520adaptivity%2520remains%2520a%2520critical%2520challenge.%2520While%2520parameter%2520sharing%250A%2528PS%2529%2520is%2520standard%2520for%2520efficient%2520learning%252C%2520it%2520notoriously%2520suppresses%2520the%250Abehavioural%2520diversity%2520required%2520for%2520specialisation.%2520This%2520failure%2520is%2520largely%2520due%250Ato%2520cross-agent%2520gradient%2520interference%252C%2520a%2520problem%2520we%2520find%2520is%2520surprisingly%250Aexacerbated%2520by%2520the%2520common%2520practice%2520of%2520coupling%2520agent%2520IDs%2520with%2520observations.%250AExisting%2520remedies%2520typically%2520add%2520complexity%2520through%2520altered%2520objectives%252C%2520manual%250Apreset%2520diversity%2520levels%252C%2520or%2520sequential%2520updates%2520--%2520raising%2520a%2520fundamental%250Aquestion%253A%2520can%2520shared%2520policies%2520adapt%2520without%2520these%2520intricacies%253F%2520We%2520propose%2520a%250Asolution%2520built%2520on%2520a%2520key%2520insight%253A%2520an%2520agent-conditioned%2520hypernetwork%2520can%2520generate%250Aagent-specific%2520parameters%2520and%2520decouple%2520observation-%2520and%2520agent-conditioned%250Agradients%252C%2520directly%2520countering%2520the%2520interference%2520from%2520coupling%2520agent%2520IDs%2520with%250Aobservations.%2520Our%2520resulting%2520method%252C%2520HyperMARL%252C%2520avoids%2520the%2520complexities%2520of%2520prior%250Awork%2520and%2520empirically%2520reduces%2520policy%2520gradient%2520variance.%2520Across%2520diverse%2520MARL%250Abenchmarks%2520%252822%2520scenarios%252C%2520up%2520to%252030%2520agents%2529%252C%2520HyperMARL%2520achieves%2520performance%250Acompetitive%2520with%2520six%2520key%2520baselines%2520while%2520preserving%2520behavioural%2520diversity%250Acomparable%2520to%2520non-parameter%2520sharing%2520methods%252C%2520establishing%2520it%2520as%2520a%2520versatile%2520and%250Aprincipled%2520approach%2520for%2520adaptive%2520MARL.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/KaleabTessera/HyperMARL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04233v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperMARL%3A%20Adaptive%20Hypernetworks%20for%20Multi-Agent%20RL&entry.906535625=Kale-ab%20Abebe%20Tessera%20and%20Arrasy%20Rahman%20and%20Amos%20Storkey%20and%20Stefano%20V.%20Albrecht&entry.1292438233=%20%20Adaptive%20cooperation%20in%20multi-agent%20reinforcement%20learning%20%28MARL%29%20requires%0Apolicies%20to%20express%20homogeneous%2C%20specialised%2C%20or%20mixed%20behaviours%2C%20yet%0Aachieving%20this%20adaptivity%20remains%20a%20critical%20challenge.%20While%20parameter%20sharing%0A%28PS%29%20is%20standard%20for%20efficient%20learning%2C%20it%20notoriously%20suppresses%20the%0Abehavioural%20diversity%20required%20for%20specialisation.%20This%20failure%20is%20largely%20due%0Ato%20cross-agent%20gradient%20interference%2C%20a%20problem%20we%20find%20is%20surprisingly%0Aexacerbated%20by%20the%20common%20practice%20of%20coupling%20agent%20IDs%20with%20observations.%0AExisting%20remedies%20typically%20add%20complexity%20through%20altered%20objectives%2C%20manual%0Apreset%20diversity%20levels%2C%20or%20sequential%20updates%20--%20raising%20a%20fundamental%0Aquestion%3A%20can%20shared%20policies%20adapt%20without%20these%20intricacies%3F%20We%20propose%20a%0Asolution%20built%20on%20a%20key%20insight%3A%20an%20agent-conditioned%20hypernetwork%20can%20generate%0Aagent-specific%20parameters%20and%20decouple%20observation-%20and%20agent-conditioned%0Agradients%2C%20directly%20countering%20the%20interference%20from%20coupling%20agent%20IDs%20with%0Aobservations.%20Our%20resulting%20method%2C%20HyperMARL%2C%20avoids%20the%20complexities%20of%20prior%0Awork%20and%20empirically%20reduces%20policy%20gradient%20variance.%20Across%20diverse%20MARL%0Abenchmarks%20%2822%20scenarios%2C%20up%20to%2030%20agents%29%2C%20HyperMARL%20achieves%20performance%0Acompetitive%20with%20six%20key%20baselines%20while%20preserving%20behavioural%20diversity%0Acomparable%20to%20non-parameter%20sharing%20methods%2C%20establishing%20it%20as%20a%20versatile%20and%0Aprincipled%20approach%20for%20adaptive%20MARL.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/KaleabTessera/HyperMARL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04233v4&entry.124074799=Read"},
{"title": "A method for the systematic generation of graph XAI benchmarks via\n  Weisfeiler-Leman coloring", "author": "Michele Fontanesi and Alessio Micheli and Marco Podda and Domenico Tortorella", "abstract": "  Graph neural networks have become the de facto model for learning from\nstructured data. However, the decision-making process of GNNs remains opaque to\nthe end user, which undermines their use in safety-critical applications.\nSeveral explainable AI techniques for graphs have been developed to address\nthis major issue. Focusing on graph classification, these explainers identify\nsubgraph motifs that explain predictions. Therefore, a robust benchmarking of\ngraph explainers is required to ensure that the produced explanations are of\nhigh quality, i.e., aligned with the GNN's decision process. However, current\ngraph-XAI benchmarks are limited to simplistic synthetic datasets or a few\nreal-world tasks curated by domain experts, hindering rigorous and reproducible\nevaluation, and consequently stalling progress in the field. To overcome these\nlimitations, we propose a method to automate the construction of graph XAI\nbenchmarks from generic graph classification datasets. Our approach leverages\nthe Weisfeiler-Leman color refinement algorithm to efficiently perform\napproximate subgraph matching and mine class-discriminating motifs, which serve\nas proxy ground-truth class explanations. At the same time, we ensure that\nthese motifs can be learned by GNNs because their discriminating power aligns\nwith WL expressiveness. This work also introduces the OpenGraphXAI benchmark\nsuite, which consists of 15 ready-made graph-XAI datasets derived by applying\nour method to real-world molecular classification datasets. The suite is\navailable to the public along with a codebase to generate over 2,000 additional\ngraph-XAI benchmarks. Finally, we present a use case that illustrates how the\nsuite can be used to assess the effectiveness of a selection of popular graph\nexplainers, demonstrating the critical role of a sufficiently large benchmark\ncollection for improving the significance of experimental results.\n", "link": "http://arxiv.org/abs/2505.12437v2", "date": "2025-10-29", "relevancy": 2.0062, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5028}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5018}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20method%20for%20the%20systematic%20generation%20of%20graph%20XAI%20benchmarks%20via%0A%20%20Weisfeiler-Leman%20coloring&body=Title%3A%20A%20method%20for%20the%20systematic%20generation%20of%20graph%20XAI%20benchmarks%20via%0A%20%20Weisfeiler-Leman%20coloring%0AAuthor%3A%20Michele%20Fontanesi%20and%20Alessio%20Micheli%20and%20Marco%20Podda%20and%20Domenico%20Tortorella%0AAbstract%3A%20%20%20Graph%20neural%20networks%20have%20become%20the%20de%20facto%20model%20for%20learning%20from%0Astructured%20data.%20However%2C%20the%20decision-making%20process%20of%20GNNs%20remains%20opaque%20to%0Athe%20end%20user%2C%20which%20undermines%20their%20use%20in%20safety-critical%20applications.%0ASeveral%20explainable%20AI%20techniques%20for%20graphs%20have%20been%20developed%20to%20address%0Athis%20major%20issue.%20Focusing%20on%20graph%20classification%2C%20these%20explainers%20identify%0Asubgraph%20motifs%20that%20explain%20predictions.%20Therefore%2C%20a%20robust%20benchmarking%20of%0Agraph%20explainers%20is%20required%20to%20ensure%20that%20the%20produced%20explanations%20are%20of%0Ahigh%20quality%2C%20i.e.%2C%20aligned%20with%20the%20GNN%27s%20decision%20process.%20However%2C%20current%0Agraph-XAI%20benchmarks%20are%20limited%20to%20simplistic%20synthetic%20datasets%20or%20a%20few%0Areal-world%20tasks%20curated%20by%20domain%20experts%2C%20hindering%20rigorous%20and%20reproducible%0Aevaluation%2C%20and%20consequently%20stalling%20progress%20in%20the%20field.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20a%20method%20to%20automate%20the%20construction%20of%20graph%20XAI%0Abenchmarks%20from%20generic%20graph%20classification%20datasets.%20Our%20approach%20leverages%0Athe%20Weisfeiler-Leman%20color%20refinement%20algorithm%20to%20efficiently%20perform%0Aapproximate%20subgraph%20matching%20and%20mine%20class-discriminating%20motifs%2C%20which%20serve%0Aas%20proxy%20ground-truth%20class%20explanations.%20At%20the%20same%20time%2C%20we%20ensure%20that%0Athese%20motifs%20can%20be%20learned%20by%20GNNs%20because%20their%20discriminating%20power%20aligns%0Awith%20WL%20expressiveness.%20This%20work%20also%20introduces%20the%20OpenGraphXAI%20benchmark%0Asuite%2C%20which%20consists%20of%2015%20ready-made%20graph-XAI%20datasets%20derived%20by%20applying%0Aour%20method%20to%20real-world%20molecular%20classification%20datasets.%20The%20suite%20is%0Aavailable%20to%20the%20public%20along%20with%20a%20codebase%20to%20generate%20over%202%2C000%20additional%0Agraph-XAI%20benchmarks.%20Finally%2C%20we%20present%20a%20use%20case%20that%20illustrates%20how%20the%0Asuite%20can%20be%20used%20to%20assess%20the%20effectiveness%20of%20a%20selection%20of%20popular%20graph%0Aexplainers%2C%20demonstrating%20the%20critical%20role%20of%20a%20sufficiently%20large%20benchmark%0Acollection%20for%20improving%20the%20significance%20of%20experimental%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12437v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520method%2520for%2520the%2520systematic%2520generation%2520of%2520graph%2520XAI%2520benchmarks%2520via%250A%2520%2520Weisfeiler-Leman%2520coloring%26entry.906535625%3DMichele%2520Fontanesi%2520and%2520Alessio%2520Micheli%2520and%2520Marco%2520Podda%2520and%2520Domenico%2520Tortorella%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520have%2520become%2520the%2520de%2520facto%2520model%2520for%2520learning%2520from%250Astructured%2520data.%2520However%252C%2520the%2520decision-making%2520process%2520of%2520GNNs%2520remains%2520opaque%2520to%250Athe%2520end%2520user%252C%2520which%2520undermines%2520their%2520use%2520in%2520safety-critical%2520applications.%250ASeveral%2520explainable%2520AI%2520techniques%2520for%2520graphs%2520have%2520been%2520developed%2520to%2520address%250Athis%2520major%2520issue.%2520Focusing%2520on%2520graph%2520classification%252C%2520these%2520explainers%2520identify%250Asubgraph%2520motifs%2520that%2520explain%2520predictions.%2520Therefore%252C%2520a%2520robust%2520benchmarking%2520of%250Agraph%2520explainers%2520is%2520required%2520to%2520ensure%2520that%2520the%2520produced%2520explanations%2520are%2520of%250Ahigh%2520quality%252C%2520i.e.%252C%2520aligned%2520with%2520the%2520GNN%2527s%2520decision%2520process.%2520However%252C%2520current%250Agraph-XAI%2520benchmarks%2520are%2520limited%2520to%2520simplistic%2520synthetic%2520datasets%2520or%2520a%2520few%250Areal-world%2520tasks%2520curated%2520by%2520domain%2520experts%252C%2520hindering%2520rigorous%2520and%2520reproducible%250Aevaluation%252C%2520and%2520consequently%2520stalling%2520progress%2520in%2520the%2520field.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520method%2520to%2520automate%2520the%2520construction%2520of%2520graph%2520XAI%250Abenchmarks%2520from%2520generic%2520graph%2520classification%2520datasets.%2520Our%2520approach%2520leverages%250Athe%2520Weisfeiler-Leman%2520color%2520refinement%2520algorithm%2520to%2520efficiently%2520perform%250Aapproximate%2520subgraph%2520matching%2520and%2520mine%2520class-discriminating%2520motifs%252C%2520which%2520serve%250Aas%2520proxy%2520ground-truth%2520class%2520explanations.%2520At%2520the%2520same%2520time%252C%2520we%2520ensure%2520that%250Athese%2520motifs%2520can%2520be%2520learned%2520by%2520GNNs%2520because%2520their%2520discriminating%2520power%2520aligns%250Awith%2520WL%2520expressiveness.%2520This%2520work%2520also%2520introduces%2520the%2520OpenGraphXAI%2520benchmark%250Asuite%252C%2520which%2520consists%2520of%252015%2520ready-made%2520graph-XAI%2520datasets%2520derived%2520by%2520applying%250Aour%2520method%2520to%2520real-world%2520molecular%2520classification%2520datasets.%2520The%2520suite%2520is%250Aavailable%2520to%2520the%2520public%2520along%2520with%2520a%2520codebase%2520to%2520generate%2520over%25202%252C000%2520additional%250Agraph-XAI%2520benchmarks.%2520Finally%252C%2520we%2520present%2520a%2520use%2520case%2520that%2520illustrates%2520how%2520the%250Asuite%2520can%2520be%2520used%2520to%2520assess%2520the%2520effectiveness%2520of%2520a%2520selection%2520of%2520popular%2520graph%250Aexplainers%252C%2520demonstrating%2520the%2520critical%2520role%2520of%2520a%2520sufficiently%2520large%2520benchmark%250Acollection%2520for%2520improving%2520the%2520significance%2520of%2520experimental%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12437v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20method%20for%20the%20systematic%20generation%20of%20graph%20XAI%20benchmarks%20via%0A%20%20Weisfeiler-Leman%20coloring&entry.906535625=Michele%20Fontanesi%20and%20Alessio%20Micheli%20and%20Marco%20Podda%20and%20Domenico%20Tortorella&entry.1292438233=%20%20Graph%20neural%20networks%20have%20become%20the%20de%20facto%20model%20for%20learning%20from%0Astructured%20data.%20However%2C%20the%20decision-making%20process%20of%20GNNs%20remains%20opaque%20to%0Athe%20end%20user%2C%20which%20undermines%20their%20use%20in%20safety-critical%20applications.%0ASeveral%20explainable%20AI%20techniques%20for%20graphs%20have%20been%20developed%20to%20address%0Athis%20major%20issue.%20Focusing%20on%20graph%20classification%2C%20these%20explainers%20identify%0Asubgraph%20motifs%20that%20explain%20predictions.%20Therefore%2C%20a%20robust%20benchmarking%20of%0Agraph%20explainers%20is%20required%20to%20ensure%20that%20the%20produced%20explanations%20are%20of%0Ahigh%20quality%2C%20i.e.%2C%20aligned%20with%20the%20GNN%27s%20decision%20process.%20However%2C%20current%0Agraph-XAI%20benchmarks%20are%20limited%20to%20simplistic%20synthetic%20datasets%20or%20a%20few%0Areal-world%20tasks%20curated%20by%20domain%20experts%2C%20hindering%20rigorous%20and%20reproducible%0Aevaluation%2C%20and%20consequently%20stalling%20progress%20in%20the%20field.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20a%20method%20to%20automate%20the%20construction%20of%20graph%20XAI%0Abenchmarks%20from%20generic%20graph%20classification%20datasets.%20Our%20approach%20leverages%0Athe%20Weisfeiler-Leman%20color%20refinement%20algorithm%20to%20efficiently%20perform%0Aapproximate%20subgraph%20matching%20and%20mine%20class-discriminating%20motifs%2C%20which%20serve%0Aas%20proxy%20ground-truth%20class%20explanations.%20At%20the%20same%20time%2C%20we%20ensure%20that%0Athese%20motifs%20can%20be%20learned%20by%20GNNs%20because%20their%20discriminating%20power%20aligns%0Awith%20WL%20expressiveness.%20This%20work%20also%20introduces%20the%20OpenGraphXAI%20benchmark%0Asuite%2C%20which%20consists%20of%2015%20ready-made%20graph-XAI%20datasets%20derived%20by%20applying%0Aour%20method%20to%20real-world%20molecular%20classification%20datasets.%20The%20suite%20is%0Aavailable%20to%20the%20public%20along%20with%20a%20codebase%20to%20generate%20over%202%2C000%20additional%0Agraph-XAI%20benchmarks.%20Finally%2C%20we%20present%20a%20use%20case%20that%20illustrates%20how%20the%0Asuite%20can%20be%20used%20to%20assess%20the%20effectiveness%20of%20a%20selection%20of%20popular%20graph%0Aexplainers%2C%20demonstrating%20the%20critical%20role%20of%20a%20sufficiently%20large%20benchmark%0Acollection%20for%20improving%20the%20significance%20of%20experimental%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12437v2&entry.124074799=Read"},
{"title": "A Deep Learning Framework for Multi-Operator Learning: Architectures and\n  Approximation Theory", "author": "Adrien Weihs and Jingmin Sun and Zecheng Zhang and Hayden Schaeffer", "abstract": "  While many problems in machine learning focus on learning mappings between\nfinite-dimensional spaces, scientific applications require approximating\nmappings between function spaces, i.e., operators. We study the problem of\nlearning collections of operators and provide both theoretical and empirical\nadvances. We distinguish between two regimes: (i) multiple operator learning,\nwhere a single network represents a continuum of operators parameterized by a\nparametric function, and (ii) learning several distinct single operators, where\neach operator is learned independently. For the multiple operator case, we\nintroduce two new architectures, $\\mathrm{MNO}$ and $\\mathrm{MONet}$, and\nestablish universal approximation results in three settings: continuous,\nintegrable, or Lipschitz operators. For the latter, we further derive explicit\nscaling laws that quantify how the network size must grow to achieve a target\napproximation accuracy. For learning several single operators, we develop a\nframework for balancing architectural complexity across subnetworks and show\nhow approximation order determines computational efficiency. Empirical\nexperiments on parametric PDE benchmarks confirm the strong expressive power\nand efficiency of the proposed architectures. Overall, this work establishes a\nunified theoretical and practical foundation for scalable neural operator\nlearning across multiple operators.\n", "link": "http://arxiv.org/abs/2510.25379v1", "date": "2025-10-29", "relevancy": 2.003, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.505}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5037}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Deep%20Learning%20Framework%20for%20Multi-Operator%20Learning%3A%20Architectures%20and%0A%20%20Approximation%20Theory&body=Title%3A%20A%20Deep%20Learning%20Framework%20for%20Multi-Operator%20Learning%3A%20Architectures%20and%0A%20%20Approximation%20Theory%0AAuthor%3A%20Adrien%20Weihs%20and%20Jingmin%20Sun%20and%20Zecheng%20Zhang%20and%20Hayden%20Schaeffer%0AAbstract%3A%20%20%20While%20many%20problems%20in%20machine%20learning%20focus%20on%20learning%20mappings%20between%0Afinite-dimensional%20spaces%2C%20scientific%20applications%20require%20approximating%0Amappings%20between%20function%20spaces%2C%20i.e.%2C%20operators.%20We%20study%20the%20problem%20of%0Alearning%20collections%20of%20operators%20and%20provide%20both%20theoretical%20and%20empirical%0Aadvances.%20We%20distinguish%20between%20two%20regimes%3A%20%28i%29%20multiple%20operator%20learning%2C%0Awhere%20a%20single%20network%20represents%20a%20continuum%20of%20operators%20parameterized%20by%20a%0Aparametric%20function%2C%20and%20%28ii%29%20learning%20several%20distinct%20single%20operators%2C%20where%0Aeach%20operator%20is%20learned%20independently.%20For%20the%20multiple%20operator%20case%2C%20we%0Aintroduce%20two%20new%20architectures%2C%20%24%5Cmathrm%7BMNO%7D%24%20and%20%24%5Cmathrm%7BMONet%7D%24%2C%20and%0Aestablish%20universal%20approximation%20results%20in%20three%20settings%3A%20continuous%2C%0Aintegrable%2C%20or%20Lipschitz%20operators.%20For%20the%20latter%2C%20we%20further%20derive%20explicit%0Ascaling%20laws%20that%20quantify%20how%20the%20network%20size%20must%20grow%20to%20achieve%20a%20target%0Aapproximation%20accuracy.%20For%20learning%20several%20single%20operators%2C%20we%20develop%20a%0Aframework%20for%20balancing%20architectural%20complexity%20across%20subnetworks%20and%20show%0Ahow%20approximation%20order%20determines%20computational%20efficiency.%20Empirical%0Aexperiments%20on%20parametric%20PDE%20benchmarks%20confirm%20the%20strong%20expressive%20power%0Aand%20efficiency%20of%20the%20proposed%20architectures.%20Overall%2C%20this%20work%20establishes%20a%0Aunified%20theoretical%20and%20practical%20foundation%20for%20scalable%20neural%20operator%0Alearning%20across%20multiple%20operators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Deep%2520Learning%2520Framework%2520for%2520Multi-Operator%2520Learning%253A%2520Architectures%2520and%250A%2520%2520Approximation%2520Theory%26entry.906535625%3DAdrien%2520Weihs%2520and%2520Jingmin%2520Sun%2520and%2520Zecheng%2520Zhang%2520and%2520Hayden%2520Schaeffer%26entry.1292438233%3D%2520%2520While%2520many%2520problems%2520in%2520machine%2520learning%2520focus%2520on%2520learning%2520mappings%2520between%250Afinite-dimensional%2520spaces%252C%2520scientific%2520applications%2520require%2520approximating%250Amappings%2520between%2520function%2520spaces%252C%2520i.e.%252C%2520operators.%2520We%2520study%2520the%2520problem%2520of%250Alearning%2520collections%2520of%2520operators%2520and%2520provide%2520both%2520theoretical%2520and%2520empirical%250Aadvances.%2520We%2520distinguish%2520between%2520two%2520regimes%253A%2520%2528i%2529%2520multiple%2520operator%2520learning%252C%250Awhere%2520a%2520single%2520network%2520represents%2520a%2520continuum%2520of%2520operators%2520parameterized%2520by%2520a%250Aparametric%2520function%252C%2520and%2520%2528ii%2529%2520learning%2520several%2520distinct%2520single%2520operators%252C%2520where%250Aeach%2520operator%2520is%2520learned%2520independently.%2520For%2520the%2520multiple%2520operator%2520case%252C%2520we%250Aintroduce%2520two%2520new%2520architectures%252C%2520%2524%255Cmathrm%257BMNO%257D%2524%2520and%2520%2524%255Cmathrm%257BMONet%257D%2524%252C%2520and%250Aestablish%2520universal%2520approximation%2520results%2520in%2520three%2520settings%253A%2520continuous%252C%250Aintegrable%252C%2520or%2520Lipschitz%2520operators.%2520For%2520the%2520latter%252C%2520we%2520further%2520derive%2520explicit%250Ascaling%2520laws%2520that%2520quantify%2520how%2520the%2520network%2520size%2520must%2520grow%2520to%2520achieve%2520a%2520target%250Aapproximation%2520accuracy.%2520For%2520learning%2520several%2520single%2520operators%252C%2520we%2520develop%2520a%250Aframework%2520for%2520balancing%2520architectural%2520complexity%2520across%2520subnetworks%2520and%2520show%250Ahow%2520approximation%2520order%2520determines%2520computational%2520efficiency.%2520Empirical%250Aexperiments%2520on%2520parametric%2520PDE%2520benchmarks%2520confirm%2520the%2520strong%2520expressive%2520power%250Aand%2520efficiency%2520of%2520the%2520proposed%2520architectures.%2520Overall%252C%2520this%2520work%2520establishes%2520a%250Aunified%2520theoretical%2520and%2520practical%2520foundation%2520for%2520scalable%2520neural%2520operator%250Alearning%2520across%2520multiple%2520operators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Deep%20Learning%20Framework%20for%20Multi-Operator%20Learning%3A%20Architectures%20and%0A%20%20Approximation%20Theory&entry.906535625=Adrien%20Weihs%20and%20Jingmin%20Sun%20and%20Zecheng%20Zhang%20and%20Hayden%20Schaeffer&entry.1292438233=%20%20While%20many%20problems%20in%20machine%20learning%20focus%20on%20learning%20mappings%20between%0Afinite-dimensional%20spaces%2C%20scientific%20applications%20require%20approximating%0Amappings%20between%20function%20spaces%2C%20i.e.%2C%20operators.%20We%20study%20the%20problem%20of%0Alearning%20collections%20of%20operators%20and%20provide%20both%20theoretical%20and%20empirical%0Aadvances.%20We%20distinguish%20between%20two%20regimes%3A%20%28i%29%20multiple%20operator%20learning%2C%0Awhere%20a%20single%20network%20represents%20a%20continuum%20of%20operators%20parameterized%20by%20a%0Aparametric%20function%2C%20and%20%28ii%29%20learning%20several%20distinct%20single%20operators%2C%20where%0Aeach%20operator%20is%20learned%20independently.%20For%20the%20multiple%20operator%20case%2C%20we%0Aintroduce%20two%20new%20architectures%2C%20%24%5Cmathrm%7BMNO%7D%24%20and%20%24%5Cmathrm%7BMONet%7D%24%2C%20and%0Aestablish%20universal%20approximation%20results%20in%20three%20settings%3A%20continuous%2C%0Aintegrable%2C%20or%20Lipschitz%20operators.%20For%20the%20latter%2C%20we%20further%20derive%20explicit%0Ascaling%20laws%20that%20quantify%20how%20the%20network%20size%20must%20grow%20to%20achieve%20a%20target%0Aapproximation%20accuracy.%20For%20learning%20several%20single%20operators%2C%20we%20develop%20a%0Aframework%20for%20balancing%20architectural%20complexity%20across%20subnetworks%20and%20show%0Ahow%20approximation%20order%20determines%20computational%20efficiency.%20Empirical%0Aexperiments%20on%20parametric%20PDE%20benchmarks%20confirm%20the%20strong%20expressive%20power%0Aand%20efficiency%20of%20the%20proposed%20architectures.%20Overall%2C%20this%20work%20establishes%20a%0Aunified%20theoretical%20and%20practical%20foundation%20for%20scalable%20neural%20operator%0Alearning%20across%20multiple%20operators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25379v1&entry.124074799=Read"},
{"title": "SATURN: SAT-based Reinforcement Learning to Unleash Language Model\n  Reasoning", "author": "Huanyu Liu and Jia Li and Hao Zhu and Kechi Zhang and Yihong Dong and Ge Li", "abstract": "  How to design reinforcement learning (RL) tasks that effectively unleash the\nreasoning capability of large language models (LLMs) remains an open question.\nExisting RL tasks (e.g., math, programming, and constructing reasoning tasks)\nsuffer from three key limitations: (1) Scalability. They rely heavily on human\nannotation or expensive LLM synthesis to generate sufficient training data. (2)\nVerifiability. LLMs' outputs are hard to verify automatically and reliably. (3)\nControllable Difficulty. Most tasks lack fine-grained difficulty control,\nmaking it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework\nthat uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs\nreasoning. Saturn enables scalable task construction, rule-based verification,\nand precise difficulty control. Saturn designs a curriculum learning pipeline\nthat continuously improves LLMs' reasoning capability by constructing SAT tasks\nof increasing difficulty and training LLMs from easy to hard. To ensure stable\ntraining, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying\ndifficulty. It supports the evaluation of how LLM reasoning changes with\nproblem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain\nSaturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT\nproblems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of\n+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B\nand Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,\nAIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in\nconstructing RL tasks, Saturn achieves further improvements of +8.8%. We\nrelease the source code, data, and models to support future research.\n", "link": "http://arxiv.org/abs/2505.16368v2", "date": "2025-10-29", "relevancy": 1.9965, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SATURN%3A%20SAT-based%20Reinforcement%20Learning%20to%20Unleash%20Language%20Model%0A%20%20Reasoning&body=Title%3A%20SATURN%3A%20SAT-based%20Reinforcement%20Learning%20to%20Unleash%20Language%20Model%0A%20%20Reasoning%0AAuthor%3A%20Huanyu%20Liu%20and%20Jia%20Li%20and%20Hao%20Zhu%20and%20Kechi%20Zhang%20and%20Yihong%20Dong%20and%20Ge%20Li%0AAbstract%3A%20%20%20How%20to%20design%20reinforcement%20learning%20%28RL%29%20tasks%20that%20effectively%20unleash%20the%0Areasoning%20capability%20of%20large%20language%20models%20%28LLMs%29%20remains%20an%20open%20question.%0AExisting%20RL%20tasks%20%28e.g.%2C%20math%2C%20programming%2C%20and%20constructing%20reasoning%20tasks%29%0Asuffer%20from%20three%20key%20limitations%3A%20%281%29%20Scalability.%20They%20rely%20heavily%20on%20human%0Aannotation%20or%20expensive%20LLM%20synthesis%20to%20generate%20sufficient%20training%20data.%20%282%29%0AVerifiability.%20LLMs%27%20outputs%20are%20hard%20to%20verify%20automatically%20and%20reliably.%20%283%29%0AControllable%20Difficulty.%20Most%20tasks%20lack%20fine-grained%20difficulty%20control%2C%0Amaking%20it%20hard%20to%20train%20LLMs%20to%20develop%20reasoning%20ability%20from%20easy%20to%20hard.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20Saturn%2C%20a%20SAT-based%20RL%20framework%0Athat%20uses%20Boolean%20Satisfiability%20%28SAT%29%20problems%20to%20train%20and%20evaluate%20LLMs%0Areasoning.%20Saturn%20enables%20scalable%20task%20construction%2C%20rule-based%20verification%2C%0Aand%20precise%20difficulty%20control.%20Saturn%20designs%20a%20curriculum%20learning%20pipeline%0Athat%20continuously%20improves%20LLMs%27%20reasoning%20capability%20by%20constructing%20SAT%20tasks%0Aof%20increasing%20difficulty%20and%20training%20LLMs%20from%20easy%20to%20hard.%20To%20ensure%20stable%0Atraining%2C%20we%20design%20a%20principled%20mechanism%20to%20control%20difficulty%20transitions.%0A%20%20We%20introduce%20Saturn-2.6k%2C%20a%20dataset%20of%202%2C660%20SAT%20problems%20with%20varying%0Adifficulty.%20It%20supports%20the%20evaluation%20of%20how%20LLM%20reasoning%20changes%20with%0Aproblem%20difficulty.%20We%20apply%20Saturn%20to%20DeepSeek-R1-Distill-Qwen%20and%20obtain%0ASaturn-1.5B%20and%20Saturn-7B.%20We%20achieve%20several%20notable%20results%3A%20%281%29%20On%20SAT%0Aproblems%2C%20Saturn-1.5B%20and%20Saturn-7B%20achieve%20average%20pass%403%20improvements%20of%0A%2B14.0%20and%20%2B28.1%2C%20respectively.%20%282%29%20On%20math%20and%20programming%20tasks%2C%20Saturn-1.5B%0Aand%20Saturn-7B%20improve%20average%20scores%20by%20%2B4.9%20and%20%2B1.8%20on%20benchmarks%20%28e.g.%2C%0AAIME%2C%20LiveCodeBench%29.%20%283%29%20Compared%20to%20the%20state-of-the-art%20%28SOTA%29%20approach%20in%0Aconstructing%20RL%20tasks%2C%20Saturn%20achieves%20further%20improvements%20of%20%2B8.8%25.%20We%0Arelease%20the%20source%20code%2C%20data%2C%20and%20models%20to%20support%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16368v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSATURN%253A%2520SAT-based%2520Reinforcement%2520Learning%2520to%2520Unleash%2520Language%2520Model%250A%2520%2520Reasoning%26entry.906535625%3DHuanyu%2520Liu%2520and%2520Jia%2520Li%2520and%2520Hao%2520Zhu%2520and%2520Kechi%2520Zhang%2520and%2520Yihong%2520Dong%2520and%2520Ge%2520Li%26entry.1292438233%3D%2520%2520How%2520to%2520design%2520reinforcement%2520learning%2520%2528RL%2529%2520tasks%2520that%2520effectively%2520unleash%2520the%250Areasoning%2520capability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520remains%2520an%2520open%2520question.%250AExisting%2520RL%2520tasks%2520%2528e.g.%252C%2520math%252C%2520programming%252C%2520and%2520constructing%2520reasoning%2520tasks%2529%250Asuffer%2520from%2520three%2520key%2520limitations%253A%2520%25281%2529%2520Scalability.%2520They%2520rely%2520heavily%2520on%2520human%250Aannotation%2520or%2520expensive%2520LLM%2520synthesis%2520to%2520generate%2520sufficient%2520training%2520data.%2520%25282%2529%250AVerifiability.%2520LLMs%2527%2520outputs%2520are%2520hard%2520to%2520verify%2520automatically%2520and%2520reliably.%2520%25283%2529%250AControllable%2520Difficulty.%2520Most%2520tasks%2520lack%2520fine-grained%2520difficulty%2520control%252C%250Amaking%2520it%2520hard%2520to%2520train%2520LLMs%2520to%2520develop%2520reasoning%2520ability%2520from%2520easy%2520to%2520hard.%250A%2520%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Saturn%252C%2520a%2520SAT-based%2520RL%2520framework%250Athat%2520uses%2520Boolean%2520Satisfiability%2520%2528SAT%2529%2520problems%2520to%2520train%2520and%2520evaluate%2520LLMs%250Areasoning.%2520Saturn%2520enables%2520scalable%2520task%2520construction%252C%2520rule-based%2520verification%252C%250Aand%2520precise%2520difficulty%2520control.%2520Saturn%2520designs%2520a%2520curriculum%2520learning%2520pipeline%250Athat%2520continuously%2520improves%2520LLMs%2527%2520reasoning%2520capability%2520by%2520constructing%2520SAT%2520tasks%250Aof%2520increasing%2520difficulty%2520and%2520training%2520LLMs%2520from%2520easy%2520to%2520hard.%2520To%2520ensure%2520stable%250Atraining%252C%2520we%2520design%2520a%2520principled%2520mechanism%2520to%2520control%2520difficulty%2520transitions.%250A%2520%2520We%2520introduce%2520Saturn-2.6k%252C%2520a%2520dataset%2520of%25202%252C660%2520SAT%2520problems%2520with%2520varying%250Adifficulty.%2520It%2520supports%2520the%2520evaluation%2520of%2520how%2520LLM%2520reasoning%2520changes%2520with%250Aproblem%2520difficulty.%2520We%2520apply%2520Saturn%2520to%2520DeepSeek-R1-Distill-Qwen%2520and%2520obtain%250ASaturn-1.5B%2520and%2520Saturn-7B.%2520We%2520achieve%2520several%2520notable%2520results%253A%2520%25281%2529%2520On%2520SAT%250Aproblems%252C%2520Saturn-1.5B%2520and%2520Saturn-7B%2520achieve%2520average%2520pass%25403%2520improvements%2520of%250A%252B14.0%2520and%2520%252B28.1%252C%2520respectively.%2520%25282%2529%2520On%2520math%2520and%2520programming%2520tasks%252C%2520Saturn-1.5B%250Aand%2520Saturn-7B%2520improve%2520average%2520scores%2520by%2520%252B4.9%2520and%2520%252B1.8%2520on%2520benchmarks%2520%2528e.g.%252C%250AAIME%252C%2520LiveCodeBench%2529.%2520%25283%2529%2520Compared%2520to%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520approach%2520in%250Aconstructing%2520RL%2520tasks%252C%2520Saturn%2520achieves%2520further%2520improvements%2520of%2520%252B8.8%2525.%2520We%250Arelease%2520the%2520source%2520code%252C%2520data%252C%2520and%2520models%2520to%2520support%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16368v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SATURN%3A%20SAT-based%20Reinforcement%20Learning%20to%20Unleash%20Language%20Model%0A%20%20Reasoning&entry.906535625=Huanyu%20Liu%20and%20Jia%20Li%20and%20Hao%20Zhu%20and%20Kechi%20Zhang%20and%20Yihong%20Dong%20and%20Ge%20Li&entry.1292438233=%20%20How%20to%20design%20reinforcement%20learning%20%28RL%29%20tasks%20that%20effectively%20unleash%20the%0Areasoning%20capability%20of%20large%20language%20models%20%28LLMs%29%20remains%20an%20open%20question.%0AExisting%20RL%20tasks%20%28e.g.%2C%20math%2C%20programming%2C%20and%20constructing%20reasoning%20tasks%29%0Asuffer%20from%20three%20key%20limitations%3A%20%281%29%20Scalability.%20They%20rely%20heavily%20on%20human%0Aannotation%20or%20expensive%20LLM%20synthesis%20to%20generate%20sufficient%20training%20data.%20%282%29%0AVerifiability.%20LLMs%27%20outputs%20are%20hard%20to%20verify%20automatically%20and%20reliably.%20%283%29%0AControllable%20Difficulty.%20Most%20tasks%20lack%20fine-grained%20difficulty%20control%2C%0Amaking%20it%20hard%20to%20train%20LLMs%20to%20develop%20reasoning%20ability%20from%20easy%20to%20hard.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20Saturn%2C%20a%20SAT-based%20RL%20framework%0Athat%20uses%20Boolean%20Satisfiability%20%28SAT%29%20problems%20to%20train%20and%20evaluate%20LLMs%0Areasoning.%20Saturn%20enables%20scalable%20task%20construction%2C%20rule-based%20verification%2C%0Aand%20precise%20difficulty%20control.%20Saturn%20designs%20a%20curriculum%20learning%20pipeline%0Athat%20continuously%20improves%20LLMs%27%20reasoning%20capability%20by%20constructing%20SAT%20tasks%0Aof%20increasing%20difficulty%20and%20training%20LLMs%20from%20easy%20to%20hard.%20To%20ensure%20stable%0Atraining%2C%20we%20design%20a%20principled%20mechanism%20to%20control%20difficulty%20transitions.%0A%20%20We%20introduce%20Saturn-2.6k%2C%20a%20dataset%20of%202%2C660%20SAT%20problems%20with%20varying%0Adifficulty.%20It%20supports%20the%20evaluation%20of%20how%20LLM%20reasoning%20changes%20with%0Aproblem%20difficulty.%20We%20apply%20Saturn%20to%20DeepSeek-R1-Distill-Qwen%20and%20obtain%0ASaturn-1.5B%20and%20Saturn-7B.%20We%20achieve%20several%20notable%20results%3A%20%281%29%20On%20SAT%0Aproblems%2C%20Saturn-1.5B%20and%20Saturn-7B%20achieve%20average%20pass%403%20improvements%20of%0A%2B14.0%20and%20%2B28.1%2C%20respectively.%20%282%29%20On%20math%20and%20programming%20tasks%2C%20Saturn-1.5B%0Aand%20Saturn-7B%20improve%20average%20scores%20by%20%2B4.9%20and%20%2B1.8%20on%20benchmarks%20%28e.g.%2C%0AAIME%2C%20LiveCodeBench%29.%20%283%29%20Compared%20to%20the%20state-of-the-art%20%28SOTA%29%20approach%20in%0Aconstructing%20RL%20tasks%2C%20Saturn%20achieves%20further%20improvements%20of%20%2B8.8%25.%20We%0Arelease%20the%20source%20code%2C%20data%2C%20and%20models%20to%20support%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16368v2&entry.124074799=Read"},
{"title": "TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time\n  Series Forecasting", "author": "Vladyslav Moroshan and Julien Siems and Arber Zela and Timur Carstensen and Frank Hutter", "abstract": "  Foundation models for zero-shot time series forecasting face challenges in\nefficient long-horizon prediction and reproducibility, with existing\nsynthetic-only approaches underperforming on challenging benchmarks. This paper\npresents TempoPFN, a univariate time series foundation model based on linear\nRecurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The\nmodel uses a GatedDeltaProduct architecture with state-weaving for fully\nparallelizable training across sequence lengths, eliminating the need for\nwindowing or summarization techniques while maintaining robust temporal\nstate-tracking. Our comprehensive synthetic data pipeline unifies diverse\ngenerators, including stochastic differential equations, Gaussian processes,\nand audio synthesis, with novel augmentations. In zero-shot evaluations on the\nGift-Eval benchmark, TempoPFN achieves top-tier competitive performance,\noutperforming all existing synthetic-only approaches and surpassing the vast\nmajority of models trained on real-world data, while being more efficient than\nexisting baselines by leveraging fully parallelizable training and inference.\nWe open-source our complete data generation pipeline and training code,\nproviding a reproducible foundation for future research.\n", "link": "http://arxiv.org/abs/2510.25502v1", "date": "2025-10-29", "relevancy": 1.9962, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5186}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5035}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TempoPFN%3A%20Synthetic%20Pre-training%20of%20Linear%20RNNs%20for%20Zero-shot%20Time%0A%20%20Series%20Forecasting&body=Title%3A%20TempoPFN%3A%20Synthetic%20Pre-training%20of%20Linear%20RNNs%20for%20Zero-shot%20Time%0A%20%20Series%20Forecasting%0AAuthor%3A%20Vladyslav%20Moroshan%20and%20Julien%20Siems%20and%20Arber%20Zela%20and%20Timur%20Carstensen%20and%20Frank%20Hutter%0AAbstract%3A%20%20%20Foundation%20models%20for%20zero-shot%20time%20series%20forecasting%20face%20challenges%20in%0Aefficient%20long-horizon%20prediction%20and%20reproducibility%2C%20with%20existing%0Asynthetic-only%20approaches%20underperforming%20on%20challenging%20benchmarks.%20This%20paper%0Apresents%20TempoPFN%2C%20a%20univariate%20time%20series%20foundation%20model%20based%20on%20linear%0ARecurrent%20Neural%20Networks%20%28RNNs%29%20pre-trained%20exclusively%20on%20synthetic%20data.%20The%0Amodel%20uses%20a%20GatedDeltaProduct%20architecture%20with%20state-weaving%20for%20fully%0Aparallelizable%20training%20across%20sequence%20lengths%2C%20eliminating%20the%20need%20for%0Awindowing%20or%20summarization%20techniques%20while%20maintaining%20robust%20temporal%0Astate-tracking.%20Our%20comprehensive%20synthetic%20data%20pipeline%20unifies%20diverse%0Agenerators%2C%20including%20stochastic%20differential%20equations%2C%20Gaussian%20processes%2C%0Aand%20audio%20synthesis%2C%20with%20novel%20augmentations.%20In%20zero-shot%20evaluations%20on%20the%0AGift-Eval%20benchmark%2C%20TempoPFN%20achieves%20top-tier%20competitive%20performance%2C%0Aoutperforming%20all%20existing%20synthetic-only%20approaches%20and%20surpassing%20the%20vast%0Amajority%20of%20models%20trained%20on%20real-world%20data%2C%20while%20being%20more%20efficient%20than%0Aexisting%20baselines%20by%20leveraging%20fully%20parallelizable%20training%20and%20inference.%0AWe%20open-source%20our%20complete%20data%20generation%20pipeline%20and%20training%20code%2C%0Aproviding%20a%20reproducible%20foundation%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempoPFN%253A%2520Synthetic%2520Pre-training%2520of%2520Linear%2520RNNs%2520for%2520Zero-shot%2520Time%250A%2520%2520Series%2520Forecasting%26entry.906535625%3DVladyslav%2520Moroshan%2520and%2520Julien%2520Siems%2520and%2520Arber%2520Zela%2520and%2520Timur%2520Carstensen%2520and%2520Frank%2520Hutter%26entry.1292438233%3D%2520%2520Foundation%2520models%2520for%2520zero-shot%2520time%2520series%2520forecasting%2520face%2520challenges%2520in%250Aefficient%2520long-horizon%2520prediction%2520and%2520reproducibility%252C%2520with%2520existing%250Asynthetic-only%2520approaches%2520underperforming%2520on%2520challenging%2520benchmarks.%2520This%2520paper%250Apresents%2520TempoPFN%252C%2520a%2520univariate%2520time%2520series%2520foundation%2520model%2520based%2520on%2520linear%250ARecurrent%2520Neural%2520Networks%2520%2528RNNs%2529%2520pre-trained%2520exclusively%2520on%2520synthetic%2520data.%2520The%250Amodel%2520uses%2520a%2520GatedDeltaProduct%2520architecture%2520with%2520state-weaving%2520for%2520fully%250Aparallelizable%2520training%2520across%2520sequence%2520lengths%252C%2520eliminating%2520the%2520need%2520for%250Awindowing%2520or%2520summarization%2520techniques%2520while%2520maintaining%2520robust%2520temporal%250Astate-tracking.%2520Our%2520comprehensive%2520synthetic%2520data%2520pipeline%2520unifies%2520diverse%250Agenerators%252C%2520including%2520stochastic%2520differential%2520equations%252C%2520Gaussian%2520processes%252C%250Aand%2520audio%2520synthesis%252C%2520with%2520novel%2520augmentations.%2520In%2520zero-shot%2520evaluations%2520on%2520the%250AGift-Eval%2520benchmark%252C%2520TempoPFN%2520achieves%2520top-tier%2520competitive%2520performance%252C%250Aoutperforming%2520all%2520existing%2520synthetic-only%2520approaches%2520and%2520surpassing%2520the%2520vast%250Amajority%2520of%2520models%2520trained%2520on%2520real-world%2520data%252C%2520while%2520being%2520more%2520efficient%2520than%250Aexisting%2520baselines%2520by%2520leveraging%2520fully%2520parallelizable%2520training%2520and%2520inference.%250AWe%2520open-source%2520our%2520complete%2520data%2520generation%2520pipeline%2520and%2520training%2520code%252C%250Aproviding%2520a%2520reproducible%2520foundation%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TempoPFN%3A%20Synthetic%20Pre-training%20of%20Linear%20RNNs%20for%20Zero-shot%20Time%0A%20%20Series%20Forecasting&entry.906535625=Vladyslav%20Moroshan%20and%20Julien%20Siems%20and%20Arber%20Zela%20and%20Timur%20Carstensen%20and%20Frank%20Hutter&entry.1292438233=%20%20Foundation%20models%20for%20zero-shot%20time%20series%20forecasting%20face%20challenges%20in%0Aefficient%20long-horizon%20prediction%20and%20reproducibility%2C%20with%20existing%0Asynthetic-only%20approaches%20underperforming%20on%20challenging%20benchmarks.%20This%20paper%0Apresents%20TempoPFN%2C%20a%20univariate%20time%20series%20foundation%20model%20based%20on%20linear%0ARecurrent%20Neural%20Networks%20%28RNNs%29%20pre-trained%20exclusively%20on%20synthetic%20data.%20The%0Amodel%20uses%20a%20GatedDeltaProduct%20architecture%20with%20state-weaving%20for%20fully%0Aparallelizable%20training%20across%20sequence%20lengths%2C%20eliminating%20the%20need%20for%0Awindowing%20or%20summarization%20techniques%20while%20maintaining%20robust%20temporal%0Astate-tracking.%20Our%20comprehensive%20synthetic%20data%20pipeline%20unifies%20diverse%0Agenerators%2C%20including%20stochastic%20differential%20equations%2C%20Gaussian%20processes%2C%0Aand%20audio%20synthesis%2C%20with%20novel%20augmentations.%20In%20zero-shot%20evaluations%20on%20the%0AGift-Eval%20benchmark%2C%20TempoPFN%20achieves%20top-tier%20competitive%20performance%2C%0Aoutperforming%20all%20existing%20synthetic-only%20approaches%20and%20surpassing%20the%20vast%0Amajority%20of%20models%20trained%20on%20real-world%20data%2C%20while%20being%20more%20efficient%20than%0Aexisting%20baselines%20by%20leveraging%20fully%20parallelizable%20training%20and%20inference.%0AWe%20open-source%20our%20complete%20data%20generation%20pipeline%20and%20training%20code%2C%0Aproviding%20a%20reproducible%20foundation%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25502v1&entry.124074799=Read"},
{"title": "Redistributing Rewards Across Time and Agents for Multi-Agent\n  Reinforcement Learning", "author": "Aditya Kapoor and Kale-ab Tessera and Mayank Baranwal and Harshad Khadilkar and Jan Peters and Stefano Albrecht and Mingfei Sun", "abstract": "  Credit assignmen, disentangling each agent's contribution to a shared reward,\nis a critical challenge in cooperative multi-agent reinforcement learning\n(MARL). To be effective, credit assignment methods must preserve the\nenvironment's optimal policy. Some recent approaches attempt this by enforcing\nreturn equivalence, where the sum of distributed rewards must equal the team\nreward. However, their guarantees are conditional on a learned model's\nregression accuracy, making them unreliable in practice. We introduce\nTemporal-Agent Reward Redistribution (TAR$^2$), an approach that decouples\ncredit modeling from this constraint. A neural network learns unnormalized\ncontribution scores, while a separate, deterministic normalization step\nenforces return equivalence by construction. We demonstrate that this method is\nequivalent to a valid Potential-Based Reward Shaping (PBRS), which guarantees\nthe optimal policy is preserved regardless of model accuracy. Empirically, on\nchallenging SMACLite and Google Research Football (GRF) benchmarks, TAR$^2$\naccelerates learning and achieves higher final performance than strong\nbaselines. These results establish our method as an effective solution for the\nagent-temporal credit assignment problem.\n", "link": "http://arxiv.org/abs/2502.04864v2", "date": "2025-10-29", "relevancy": 1.9868, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4977}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4968}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Redistributing%20Rewards%20Across%20Time%20and%20Agents%20for%20Multi-Agent%0A%20%20Reinforcement%20Learning&body=Title%3A%20Redistributing%20Rewards%20Across%20Time%20and%20Agents%20for%20Multi-Agent%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Aditya%20Kapoor%20and%20Kale-ab%20Tessera%20and%20Mayank%20Baranwal%20and%20Harshad%20Khadilkar%20and%20Jan%20Peters%20and%20Stefano%20Albrecht%20and%20Mingfei%20Sun%0AAbstract%3A%20%20%20Credit%20assignmen%2C%20disentangling%20each%20agent%27s%20contribution%20to%20a%20shared%20reward%2C%0Ais%20a%20critical%20challenge%20in%20cooperative%20multi-agent%20reinforcement%20learning%0A%28MARL%29.%20To%20be%20effective%2C%20credit%20assignment%20methods%20must%20preserve%20the%0Aenvironment%27s%20optimal%20policy.%20Some%20recent%20approaches%20attempt%20this%20by%20enforcing%0Areturn%20equivalence%2C%20where%20the%20sum%20of%20distributed%20rewards%20must%20equal%20the%20team%0Areward.%20However%2C%20their%20guarantees%20are%20conditional%20on%20a%20learned%20model%27s%0Aregression%20accuracy%2C%20making%20them%20unreliable%20in%20practice.%20We%20introduce%0ATemporal-Agent%20Reward%20Redistribution%20%28TAR%24%5E2%24%29%2C%20an%20approach%20that%20decouples%0Acredit%20modeling%20from%20this%20constraint.%20A%20neural%20network%20learns%20unnormalized%0Acontribution%20scores%2C%20while%20a%20separate%2C%20deterministic%20normalization%20step%0Aenforces%20return%20equivalence%20by%20construction.%20We%20demonstrate%20that%20this%20method%20is%0Aequivalent%20to%20a%20valid%20Potential-Based%20Reward%20Shaping%20%28PBRS%29%2C%20which%20guarantees%0Athe%20optimal%20policy%20is%20preserved%20regardless%20of%20model%20accuracy.%20Empirically%2C%20on%0Achallenging%20SMACLite%20and%20Google%20Research%20Football%20%28GRF%29%20benchmarks%2C%20TAR%24%5E2%24%0Aaccelerates%20learning%20and%20achieves%20higher%20final%20performance%20than%20strong%0Abaselines.%20These%20results%20establish%20our%20method%20as%20an%20effective%20solution%20for%20the%0Aagent-temporal%20credit%20assignment%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04864v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedistributing%2520Rewards%2520Across%2520Time%2520and%2520Agents%2520for%2520Multi-Agent%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DAditya%2520Kapoor%2520and%2520Kale-ab%2520Tessera%2520and%2520Mayank%2520Baranwal%2520and%2520Harshad%2520Khadilkar%2520and%2520Jan%2520Peters%2520and%2520Stefano%2520Albrecht%2520and%2520Mingfei%2520Sun%26entry.1292438233%3D%2520%2520Credit%2520assignmen%252C%2520disentangling%2520each%2520agent%2527s%2520contribution%2520to%2520a%2520shared%2520reward%252C%250Ais%2520a%2520critical%2520challenge%2520in%2520cooperative%2520multi-agent%2520reinforcement%2520learning%250A%2528MARL%2529.%2520To%2520be%2520effective%252C%2520credit%2520assignment%2520methods%2520must%2520preserve%2520the%250Aenvironment%2527s%2520optimal%2520policy.%2520Some%2520recent%2520approaches%2520attempt%2520this%2520by%2520enforcing%250Areturn%2520equivalence%252C%2520where%2520the%2520sum%2520of%2520distributed%2520rewards%2520must%2520equal%2520the%2520team%250Areward.%2520However%252C%2520their%2520guarantees%2520are%2520conditional%2520on%2520a%2520learned%2520model%2527s%250Aregression%2520accuracy%252C%2520making%2520them%2520unreliable%2520in%2520practice.%2520We%2520introduce%250ATemporal-Agent%2520Reward%2520Redistribution%2520%2528TAR%2524%255E2%2524%2529%252C%2520an%2520approach%2520that%2520decouples%250Acredit%2520modeling%2520from%2520this%2520constraint.%2520A%2520neural%2520network%2520learns%2520unnormalized%250Acontribution%2520scores%252C%2520while%2520a%2520separate%252C%2520deterministic%2520normalization%2520step%250Aenforces%2520return%2520equivalence%2520by%2520construction.%2520We%2520demonstrate%2520that%2520this%2520method%2520is%250Aequivalent%2520to%2520a%2520valid%2520Potential-Based%2520Reward%2520Shaping%2520%2528PBRS%2529%252C%2520which%2520guarantees%250Athe%2520optimal%2520policy%2520is%2520preserved%2520regardless%2520of%2520model%2520accuracy.%2520Empirically%252C%2520on%250Achallenging%2520SMACLite%2520and%2520Google%2520Research%2520Football%2520%2528GRF%2529%2520benchmarks%252C%2520TAR%2524%255E2%2524%250Aaccelerates%2520learning%2520and%2520achieves%2520higher%2520final%2520performance%2520than%2520strong%250Abaselines.%2520These%2520results%2520establish%2520our%2520method%2520as%2520an%2520effective%2520solution%2520for%2520the%250Aagent-temporal%2520credit%2520assignment%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04864v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Redistributing%20Rewards%20Across%20Time%20and%20Agents%20for%20Multi-Agent%0A%20%20Reinforcement%20Learning&entry.906535625=Aditya%20Kapoor%20and%20Kale-ab%20Tessera%20and%20Mayank%20Baranwal%20and%20Harshad%20Khadilkar%20and%20Jan%20Peters%20and%20Stefano%20Albrecht%20and%20Mingfei%20Sun&entry.1292438233=%20%20Credit%20assignmen%2C%20disentangling%20each%20agent%27s%20contribution%20to%20a%20shared%20reward%2C%0Ais%20a%20critical%20challenge%20in%20cooperative%20multi-agent%20reinforcement%20learning%0A%28MARL%29.%20To%20be%20effective%2C%20credit%20assignment%20methods%20must%20preserve%20the%0Aenvironment%27s%20optimal%20policy.%20Some%20recent%20approaches%20attempt%20this%20by%20enforcing%0Areturn%20equivalence%2C%20where%20the%20sum%20of%20distributed%20rewards%20must%20equal%20the%20team%0Areward.%20However%2C%20their%20guarantees%20are%20conditional%20on%20a%20learned%20model%27s%0Aregression%20accuracy%2C%20making%20them%20unreliable%20in%20practice.%20We%20introduce%0ATemporal-Agent%20Reward%20Redistribution%20%28TAR%24%5E2%24%29%2C%20an%20approach%20that%20decouples%0Acredit%20modeling%20from%20this%20constraint.%20A%20neural%20network%20learns%20unnormalized%0Acontribution%20scores%2C%20while%20a%20separate%2C%20deterministic%20normalization%20step%0Aenforces%20return%20equivalence%20by%20construction.%20We%20demonstrate%20that%20this%20method%20is%0Aequivalent%20to%20a%20valid%20Potential-Based%20Reward%20Shaping%20%28PBRS%29%2C%20which%20guarantees%0Athe%20optimal%20policy%20is%20preserved%20regardless%20of%20model%20accuracy.%20Empirically%2C%20on%0Achallenging%20SMACLite%20and%20Google%20Research%20Football%20%28GRF%29%20benchmarks%2C%20TAR%24%5E2%24%0Aaccelerates%20learning%20and%20achieves%20higher%20final%20performance%20than%20strong%0Abaselines.%20These%20results%20establish%20our%20method%20as%20an%20effective%20solution%20for%20the%0Aagent-temporal%20credit%20assignment%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04864v2&entry.124074799=Read"},
{"title": "Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced\n  Logical Recommendation", "author": "Qing Yu and Xiaobei Wang and Shuchang Liu and Yandong Bai and Xiaoyu Yang and Xueliang Wang and Chang Meng and Shanshan Wu and Hailan Yang and Huihui Xiao and Xiang Li and Fan Yang and Xiaoqiang Feng and Lantao Hu and Han Li and Kun Gai and Lixin Zou", "abstract": "  Recommender systems filter contents/items valuable to users by inferring\npreferences from user features and historical behaviors. Mainstream approaches\nfollow the learning-to-rank paradigm, which focus on discovering and modeling\nitem topics (e.g., categories), and capturing user preferences on these topics\nbased on historical interactions. However, this paradigm often neglects the\nmodeling of user characteristics and their social roles, which are logical\nconfounders influencing the correlated interest and user preference transition.\nTo bridge this gap, we introduce the user role identification task and the\nbehavioral logic modeling task that aim to explicitly model user roles and\nlearn the logical relations between item topics and user social roles. We show\nthat it is possible to explicitly solve these tasks through an efficient\nintegration framework of Large Language Model (LLM) and recommendation systems,\nfor which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)\nLLM's world knowledge and logic inference ability to extract realistic\ntag-based virtual logic graphs that reveal dynamic and expressive knowledge of\nusers, refining our understanding of user behaviors. On the other hand, TagCF\npresents empirically effective integration modules that take advantage of the\nextracted tag-logic information, augmenting the recommendation performance. We\nconduct both online experiments and offline experiments with industrial and\npublic datasets as verification of TagCF's effectiveness, and we empirically\nshow that the user role modeling strategy is potentially a better choice than\nthe modeling of item topics. Additionally, we provide evidence that the\nextracted logic graphs are empirically a general and transferable knowledge\nthat can benefit a wide range of recommendation tasks. Our code is available in\nhttps://github.com/Code2Q/TagCF.\n", "link": "http://arxiv.org/abs/2505.10940v3", "date": "2025-10-29", "relevancy": 1.9849, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5101}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Who%20You%20Are%20Matters%3A%20Bridging%20Topics%20and%20Social%20Roles%20via%20LLM-Enhanced%0A%20%20Logical%20Recommendation&body=Title%3A%20Who%20You%20Are%20Matters%3A%20Bridging%20Topics%20and%20Social%20Roles%20via%20LLM-Enhanced%0A%20%20Logical%20Recommendation%0AAuthor%3A%20Qing%20Yu%20and%20Xiaobei%20Wang%20and%20Shuchang%20Liu%20and%20Yandong%20Bai%20and%20Xiaoyu%20Yang%20and%20Xueliang%20Wang%20and%20Chang%20Meng%20and%20Shanshan%20Wu%20and%20Hailan%20Yang%20and%20Huihui%20Xiao%20and%20Xiang%20Li%20and%20Fan%20Yang%20and%20Xiaoqiang%20Feng%20and%20Lantao%20Hu%20and%20Han%20Li%20and%20Kun%20Gai%20and%20Lixin%20Zou%0AAbstract%3A%20%20%20Recommender%20systems%20filter%20contents/items%20valuable%20to%20users%20by%20inferring%0Apreferences%20from%20user%20features%20and%20historical%20behaviors.%20Mainstream%20approaches%0Afollow%20the%20learning-to-rank%20paradigm%2C%20which%20focus%20on%20discovering%20and%20modeling%0Aitem%20topics%20%28e.g.%2C%20categories%29%2C%20and%20capturing%20user%20preferences%20on%20these%20topics%0Abased%20on%20historical%20interactions.%20However%2C%20this%20paradigm%20often%20neglects%20the%0Amodeling%20of%20user%20characteristics%20and%20their%20social%20roles%2C%20which%20are%20logical%0Aconfounders%20influencing%20the%20correlated%20interest%20and%20user%20preference%20transition.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20the%20user%20role%20identification%20task%20and%20the%0Abehavioral%20logic%20modeling%20task%20that%20aim%20to%20explicitly%20model%20user%20roles%20and%0Alearn%20the%20logical%20relations%20between%20item%20topics%20and%20user%20social%20roles.%20We%20show%0Athat%20it%20is%20possible%20to%20explicitly%20solve%20these%20tasks%20through%20an%20efficient%0Aintegration%20framework%20of%20Large%20Language%20Model%20%28LLM%29%20and%20recommendation%20systems%2C%0Afor%20which%20we%20propose%20TagCF.%20On%20the%20one%20hand%2C%20TagCF%20exploits%20the%20%28Multi-modal%29%0ALLM%27s%20world%20knowledge%20and%20logic%20inference%20ability%20to%20extract%20realistic%0Atag-based%20virtual%20logic%20graphs%20that%20reveal%20dynamic%20and%20expressive%20knowledge%20of%0Ausers%2C%20refining%20our%20understanding%20of%20user%20behaviors.%20On%20the%20other%20hand%2C%20TagCF%0Apresents%20empirically%20effective%20integration%20modules%20that%20take%20advantage%20of%20the%0Aextracted%20tag-logic%20information%2C%20augmenting%20the%20recommendation%20performance.%20We%0Aconduct%20both%20online%20experiments%20and%20offline%20experiments%20with%20industrial%20and%0Apublic%20datasets%20as%20verification%20of%20TagCF%27s%20effectiveness%2C%20and%20we%20empirically%0Ashow%20that%20the%20user%20role%20modeling%20strategy%20is%20potentially%20a%20better%20choice%20than%0Athe%20modeling%20of%20item%20topics.%20Additionally%2C%20we%20provide%20evidence%20that%20the%0Aextracted%20logic%20graphs%20are%20empirically%20a%20general%20and%20transferable%20knowledge%0Athat%20can%20benefit%20a%20wide%20range%20of%20recommendation%20tasks.%20Our%20code%20is%20available%20in%0Ahttps%3A//github.com/Code2Q/TagCF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10940v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWho%2520You%2520Are%2520Matters%253A%2520Bridging%2520Topics%2520and%2520Social%2520Roles%2520via%2520LLM-Enhanced%250A%2520%2520Logical%2520Recommendation%26entry.906535625%3DQing%2520Yu%2520and%2520Xiaobei%2520Wang%2520and%2520Shuchang%2520Liu%2520and%2520Yandong%2520Bai%2520and%2520Xiaoyu%2520Yang%2520and%2520Xueliang%2520Wang%2520and%2520Chang%2520Meng%2520and%2520Shanshan%2520Wu%2520and%2520Hailan%2520Yang%2520and%2520Huihui%2520Xiao%2520and%2520Xiang%2520Li%2520and%2520Fan%2520Yang%2520and%2520Xiaoqiang%2520Feng%2520and%2520Lantao%2520Hu%2520and%2520Han%2520Li%2520and%2520Kun%2520Gai%2520and%2520Lixin%2520Zou%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520filter%2520contents/items%2520valuable%2520to%2520users%2520by%2520inferring%250Apreferences%2520from%2520user%2520features%2520and%2520historical%2520behaviors.%2520Mainstream%2520approaches%250Afollow%2520the%2520learning-to-rank%2520paradigm%252C%2520which%2520focus%2520on%2520discovering%2520and%2520modeling%250Aitem%2520topics%2520%2528e.g.%252C%2520categories%2529%252C%2520and%2520capturing%2520user%2520preferences%2520on%2520these%2520topics%250Abased%2520on%2520historical%2520interactions.%2520However%252C%2520this%2520paradigm%2520often%2520neglects%2520the%250Amodeling%2520of%2520user%2520characteristics%2520and%2520their%2520social%2520roles%252C%2520which%2520are%2520logical%250Aconfounders%2520influencing%2520the%2520correlated%2520interest%2520and%2520user%2520preference%2520transition.%250ATo%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520the%2520user%2520role%2520identification%2520task%2520and%2520the%250Abehavioral%2520logic%2520modeling%2520task%2520that%2520aim%2520to%2520explicitly%2520model%2520user%2520roles%2520and%250Alearn%2520the%2520logical%2520relations%2520between%2520item%2520topics%2520and%2520user%2520social%2520roles.%2520We%2520show%250Athat%2520it%2520is%2520possible%2520to%2520explicitly%2520solve%2520these%2520tasks%2520through%2520an%2520efficient%250Aintegration%2520framework%2520of%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520and%2520recommendation%2520systems%252C%250Afor%2520which%2520we%2520propose%2520TagCF.%2520On%2520the%2520one%2520hand%252C%2520TagCF%2520exploits%2520the%2520%2528Multi-modal%2529%250ALLM%2527s%2520world%2520knowledge%2520and%2520logic%2520inference%2520ability%2520to%2520extract%2520realistic%250Atag-based%2520virtual%2520logic%2520graphs%2520that%2520reveal%2520dynamic%2520and%2520expressive%2520knowledge%2520of%250Ausers%252C%2520refining%2520our%2520understanding%2520of%2520user%2520behaviors.%2520On%2520the%2520other%2520hand%252C%2520TagCF%250Apresents%2520empirically%2520effective%2520integration%2520modules%2520that%2520take%2520advantage%2520of%2520the%250Aextracted%2520tag-logic%2520information%252C%2520augmenting%2520the%2520recommendation%2520performance.%2520We%250Aconduct%2520both%2520online%2520experiments%2520and%2520offline%2520experiments%2520with%2520industrial%2520and%250Apublic%2520datasets%2520as%2520verification%2520of%2520TagCF%2527s%2520effectiveness%252C%2520and%2520we%2520empirically%250Ashow%2520that%2520the%2520user%2520role%2520modeling%2520strategy%2520is%2520potentially%2520a%2520better%2520choice%2520than%250Athe%2520modeling%2520of%2520item%2520topics.%2520Additionally%252C%2520we%2520provide%2520evidence%2520that%2520the%250Aextracted%2520logic%2520graphs%2520are%2520empirically%2520a%2520general%2520and%2520transferable%2520knowledge%250Athat%2520can%2520benefit%2520a%2520wide%2520range%2520of%2520recommendation%2520tasks.%2520Our%2520code%2520is%2520available%2520in%250Ahttps%253A//github.com/Code2Q/TagCF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10940v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20You%20Are%20Matters%3A%20Bridging%20Topics%20and%20Social%20Roles%20via%20LLM-Enhanced%0A%20%20Logical%20Recommendation&entry.906535625=Qing%20Yu%20and%20Xiaobei%20Wang%20and%20Shuchang%20Liu%20and%20Yandong%20Bai%20and%20Xiaoyu%20Yang%20and%20Xueliang%20Wang%20and%20Chang%20Meng%20and%20Shanshan%20Wu%20and%20Hailan%20Yang%20and%20Huihui%20Xiao%20and%20Xiang%20Li%20and%20Fan%20Yang%20and%20Xiaoqiang%20Feng%20and%20Lantao%20Hu%20and%20Han%20Li%20and%20Kun%20Gai%20and%20Lixin%20Zou&entry.1292438233=%20%20Recommender%20systems%20filter%20contents/items%20valuable%20to%20users%20by%20inferring%0Apreferences%20from%20user%20features%20and%20historical%20behaviors.%20Mainstream%20approaches%0Afollow%20the%20learning-to-rank%20paradigm%2C%20which%20focus%20on%20discovering%20and%20modeling%0Aitem%20topics%20%28e.g.%2C%20categories%29%2C%20and%20capturing%20user%20preferences%20on%20these%20topics%0Abased%20on%20historical%20interactions.%20However%2C%20this%20paradigm%20often%20neglects%20the%0Amodeling%20of%20user%20characteristics%20and%20their%20social%20roles%2C%20which%20are%20logical%0Aconfounders%20influencing%20the%20correlated%20interest%20and%20user%20preference%20transition.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20the%20user%20role%20identification%20task%20and%20the%0Abehavioral%20logic%20modeling%20task%20that%20aim%20to%20explicitly%20model%20user%20roles%20and%0Alearn%20the%20logical%20relations%20between%20item%20topics%20and%20user%20social%20roles.%20We%20show%0Athat%20it%20is%20possible%20to%20explicitly%20solve%20these%20tasks%20through%20an%20efficient%0Aintegration%20framework%20of%20Large%20Language%20Model%20%28LLM%29%20and%20recommendation%20systems%2C%0Afor%20which%20we%20propose%20TagCF.%20On%20the%20one%20hand%2C%20TagCF%20exploits%20the%20%28Multi-modal%29%0ALLM%27s%20world%20knowledge%20and%20logic%20inference%20ability%20to%20extract%20realistic%0Atag-based%20virtual%20logic%20graphs%20that%20reveal%20dynamic%20and%20expressive%20knowledge%20of%0Ausers%2C%20refining%20our%20understanding%20of%20user%20behaviors.%20On%20the%20other%20hand%2C%20TagCF%0Apresents%20empirically%20effective%20integration%20modules%20that%20take%20advantage%20of%20the%0Aextracted%20tag-logic%20information%2C%20augmenting%20the%20recommendation%20performance.%20We%0Aconduct%20both%20online%20experiments%20and%20offline%20experiments%20with%20industrial%20and%0Apublic%20datasets%20as%20verification%20of%20TagCF%27s%20effectiveness%2C%20and%20we%20empirically%0Ashow%20that%20the%20user%20role%20modeling%20strategy%20is%20potentially%20a%20better%20choice%20than%0Athe%20modeling%20of%20item%20topics.%20Additionally%2C%20we%20provide%20evidence%20that%20the%0Aextracted%20logic%20graphs%20are%20empirically%20a%20general%20and%20transferable%20knowledge%0Athat%20can%20benefit%20a%20wide%20range%20of%20recommendation%20tasks.%20Our%20code%20is%20available%20in%0Ahttps%3A//github.com/Code2Q/TagCF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10940v3&entry.124074799=Read"},
{"title": "GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL\n  Generation from Large Language Models", "author": "Mattia Tritto and Giuseppe Farano and Dario Di Palma and Gaetano Rossiello and Fedelucio Narducci and Dharmashankar Subramanian and Tommaso Di Noia", "abstract": "  Text-to-SQL, the task of translating natural language questions into SQL\nqueries, has significantly advanced with the introduction of Large Language\nModels (LLMs), broadening database accessibility for a wide range of users.\nDespite substantial progress in generating valid SQL, current LLMs still\nstruggle with complex queries. To address this limitation, test-time strategies\nsuch as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on\nthe assumption that LLMs can produce correct answers after multiple attempts.\nHowever, these methods rely on surface-level heuristics, selecting the\nsyntactically correct query through execution-based BoN (ex-BoN) or the most\nfrequently generated one through Majority Voting. Recently, Outcome Reward\nModels (ORMs), which assign utility scores to generated outputs based on\nsemantic correctness, have emerged as a promising reinforcement learning\napproach for improving model alignment. We argue that ORMs could serve as an\neffective new test-time heuristic, although their application in this context\nremains largely underexplored.\n  In this work, we propose a unified framework for training ORMs tailored to\nthe Text-to-SQL task and assess their effectiveness as a test-time heuristic\nwithin the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the\nBIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2,\nGranite3, and Llama3 families. Results show that ORMs outperform ex-BoN and\nMaj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider)\nover ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further\ndemonstrate that finetuning models already aligned with SQL generation, such as\nOmniSQL, yields superior ORM performance. Additionally, we observe that ORMs\nachieve competitive results on simple queries and benefit more from an\nincreased number of candidates compared to ex-BoN and Maj.\n", "link": "http://arxiv.org/abs/2509.01308v2", "date": "2025-10-29", "relevancy": 1.979, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5009}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5009}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GradeSQL%3A%20Test-Time%20Inference%20with%20Outcome%20Reward%20Models%20for%20Text-to-SQL%0A%20%20Generation%20from%20Large%20Language%20Models&body=Title%3A%20GradeSQL%3A%20Test-Time%20Inference%20with%20Outcome%20Reward%20Models%20for%20Text-to-SQL%0A%20%20Generation%20from%20Large%20Language%20Models%0AAuthor%3A%20Mattia%20Tritto%20and%20Giuseppe%20Farano%20and%20Dario%20Di%20Palma%20and%20Gaetano%20Rossiello%20and%20Fedelucio%20Narducci%20and%20Dharmashankar%20Subramanian%20and%20Tommaso%20Di%20Noia%0AAbstract%3A%20%20%20Text-to-SQL%2C%20the%20task%20of%20translating%20natural%20language%20questions%20into%20SQL%0Aqueries%2C%20has%20significantly%20advanced%20with%20the%20introduction%20of%20Large%20Language%0AModels%20%28LLMs%29%2C%20broadening%20database%20accessibility%20for%20a%20wide%20range%20of%20users.%0ADespite%20substantial%20progress%20in%20generating%20valid%20SQL%2C%20current%20LLMs%20still%0Astruggle%20with%20complex%20queries.%20To%20address%20this%20limitation%2C%20test-time%20strategies%0Asuch%20as%20Best-of-N%20%28BoN%29%20and%20Majority%20Voting%20%28Maj%29%20are%20often%20employed%2C%20based%20on%0Athe%20assumption%20that%20LLMs%20can%20produce%20correct%20answers%20after%20multiple%20attempts.%0AHowever%2C%20these%20methods%20rely%20on%20surface-level%20heuristics%2C%20selecting%20the%0Asyntactically%20correct%20query%20through%20execution-based%20BoN%20%28ex-BoN%29%20or%20the%20most%0Afrequently%20generated%20one%20through%20Majority%20Voting.%20Recently%2C%20Outcome%20Reward%0AModels%20%28ORMs%29%2C%20which%20assign%20utility%20scores%20to%20generated%20outputs%20based%20on%0Asemantic%20correctness%2C%20have%20emerged%20as%20a%20promising%20reinforcement%20learning%0Aapproach%20for%20improving%20model%20alignment.%20We%20argue%20that%20ORMs%20could%20serve%20as%20an%0Aeffective%20new%20test-time%20heuristic%2C%20although%20their%20application%20in%20this%20context%0Aremains%20largely%20underexplored.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20unified%20framework%20for%20training%20ORMs%20tailored%20to%0Athe%20Text-to-SQL%20task%20and%20assess%20their%20effectiveness%20as%20a%20test-time%20heuristic%0Awithin%20the%20BoN%20strategy.%20We%20benchmark%20ORMs%20against%20ex-BoN%20and%20Maj%20across%20the%0ABIRD%20and%20Spider%20datasets%2C%20fine-tuning%20diverse%20open-source%20LLMs%20from%20the%20Qwen2%2C%0AGranite3%2C%20and%20Llama3%20families.%20Results%20show%20that%20ORMs%20outperform%20ex-BoN%20and%0AMaj%2C%20achieving%20execution%20accuracy%20gains%20of%20%2B4.33%25%20%28BIRD%29%20and%20%2B2.10%25%20%28Spider%29%0Aover%20ex-BoN%2C%20and%20%2B2.91%25%20%28BIRD%29%20and%20%2B0.93%25%20%28Spider%29%20over%20Maj.%20We%20further%0Ademonstrate%20that%20finetuning%20models%20already%20aligned%20with%20SQL%20generation%2C%20such%20as%0AOmniSQL%2C%20yields%20superior%20ORM%20performance.%20Additionally%2C%20we%20observe%20that%20ORMs%0Aachieve%20competitive%20results%20on%20simple%20queries%20and%20benefit%20more%20from%20an%0Aincreased%20number%20of%20candidates%20compared%20to%20ex-BoN%20and%20Maj.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.01308v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradeSQL%253A%2520Test-Time%2520Inference%2520with%2520Outcome%2520Reward%2520Models%2520for%2520Text-to-SQL%250A%2520%2520Generation%2520from%2520Large%2520Language%2520Models%26entry.906535625%3DMattia%2520Tritto%2520and%2520Giuseppe%2520Farano%2520and%2520Dario%2520Di%2520Palma%2520and%2520Gaetano%2520Rossiello%2520and%2520Fedelucio%2520Narducci%2520and%2520Dharmashankar%2520Subramanian%2520and%2520Tommaso%2520Di%2520Noia%26entry.1292438233%3D%2520%2520Text-to-SQL%252C%2520the%2520task%2520of%2520translating%2520natural%2520language%2520questions%2520into%2520SQL%250Aqueries%252C%2520has%2520significantly%2520advanced%2520with%2520the%2520introduction%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%252C%2520broadening%2520database%2520accessibility%2520for%2520a%2520wide%2520range%2520of%2520users.%250ADespite%2520substantial%2520progress%2520in%2520generating%2520valid%2520SQL%252C%2520current%2520LLMs%2520still%250Astruggle%2520with%2520complex%2520queries.%2520To%2520address%2520this%2520limitation%252C%2520test-time%2520strategies%250Asuch%2520as%2520Best-of-N%2520%2528BoN%2529%2520and%2520Majority%2520Voting%2520%2528Maj%2529%2520are%2520often%2520employed%252C%2520based%2520on%250Athe%2520assumption%2520that%2520LLMs%2520can%2520produce%2520correct%2520answers%2520after%2520multiple%2520attempts.%250AHowever%252C%2520these%2520methods%2520rely%2520on%2520surface-level%2520heuristics%252C%2520selecting%2520the%250Asyntactically%2520correct%2520query%2520through%2520execution-based%2520BoN%2520%2528ex-BoN%2529%2520or%2520the%2520most%250Afrequently%2520generated%2520one%2520through%2520Majority%2520Voting.%2520Recently%252C%2520Outcome%2520Reward%250AModels%2520%2528ORMs%2529%252C%2520which%2520assign%2520utility%2520scores%2520to%2520generated%2520outputs%2520based%2520on%250Asemantic%2520correctness%252C%2520have%2520emerged%2520as%2520a%2520promising%2520reinforcement%2520learning%250Aapproach%2520for%2520improving%2520model%2520alignment.%2520We%2520argue%2520that%2520ORMs%2520could%2520serve%2520as%2520an%250Aeffective%2520new%2520test-time%2520heuristic%252C%2520although%2520their%2520application%2520in%2520this%2520context%250Aremains%2520largely%2520underexplored.%250A%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520unified%2520framework%2520for%2520training%2520ORMs%2520tailored%2520to%250Athe%2520Text-to-SQL%2520task%2520and%2520assess%2520their%2520effectiveness%2520as%2520a%2520test-time%2520heuristic%250Awithin%2520the%2520BoN%2520strategy.%2520We%2520benchmark%2520ORMs%2520against%2520ex-BoN%2520and%2520Maj%2520across%2520the%250ABIRD%2520and%2520Spider%2520datasets%252C%2520fine-tuning%2520diverse%2520open-source%2520LLMs%2520from%2520the%2520Qwen2%252C%250AGranite3%252C%2520and%2520Llama3%2520families.%2520Results%2520show%2520that%2520ORMs%2520outperform%2520ex-BoN%2520and%250AMaj%252C%2520achieving%2520execution%2520accuracy%2520gains%2520of%2520%252B4.33%2525%2520%2528BIRD%2529%2520and%2520%252B2.10%2525%2520%2528Spider%2529%250Aover%2520ex-BoN%252C%2520and%2520%252B2.91%2525%2520%2528BIRD%2529%2520and%2520%252B0.93%2525%2520%2528Spider%2529%2520over%2520Maj.%2520We%2520further%250Ademonstrate%2520that%2520finetuning%2520models%2520already%2520aligned%2520with%2520SQL%2520generation%252C%2520such%2520as%250AOmniSQL%252C%2520yields%2520superior%2520ORM%2520performance.%2520Additionally%252C%2520we%2520observe%2520that%2520ORMs%250Aachieve%2520competitive%2520results%2520on%2520simple%2520queries%2520and%2520benefit%2520more%2520from%2520an%250Aincreased%2520number%2520of%2520candidates%2520compared%2520to%2520ex-BoN%2520and%2520Maj.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01308v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GradeSQL%3A%20Test-Time%20Inference%20with%20Outcome%20Reward%20Models%20for%20Text-to-SQL%0A%20%20Generation%20from%20Large%20Language%20Models&entry.906535625=Mattia%20Tritto%20and%20Giuseppe%20Farano%20and%20Dario%20Di%20Palma%20and%20Gaetano%20Rossiello%20and%20Fedelucio%20Narducci%20and%20Dharmashankar%20Subramanian%20and%20Tommaso%20Di%20Noia&entry.1292438233=%20%20Text-to-SQL%2C%20the%20task%20of%20translating%20natural%20language%20questions%20into%20SQL%0Aqueries%2C%20has%20significantly%20advanced%20with%20the%20introduction%20of%20Large%20Language%0AModels%20%28LLMs%29%2C%20broadening%20database%20accessibility%20for%20a%20wide%20range%20of%20users.%0ADespite%20substantial%20progress%20in%20generating%20valid%20SQL%2C%20current%20LLMs%20still%0Astruggle%20with%20complex%20queries.%20To%20address%20this%20limitation%2C%20test-time%20strategies%0Asuch%20as%20Best-of-N%20%28BoN%29%20and%20Majority%20Voting%20%28Maj%29%20are%20often%20employed%2C%20based%20on%0Athe%20assumption%20that%20LLMs%20can%20produce%20correct%20answers%20after%20multiple%20attempts.%0AHowever%2C%20these%20methods%20rely%20on%20surface-level%20heuristics%2C%20selecting%20the%0Asyntactically%20correct%20query%20through%20execution-based%20BoN%20%28ex-BoN%29%20or%20the%20most%0Afrequently%20generated%20one%20through%20Majority%20Voting.%20Recently%2C%20Outcome%20Reward%0AModels%20%28ORMs%29%2C%20which%20assign%20utility%20scores%20to%20generated%20outputs%20based%20on%0Asemantic%20correctness%2C%20have%20emerged%20as%20a%20promising%20reinforcement%20learning%0Aapproach%20for%20improving%20model%20alignment.%20We%20argue%20that%20ORMs%20could%20serve%20as%20an%0Aeffective%20new%20test-time%20heuristic%2C%20although%20their%20application%20in%20this%20context%0Aremains%20largely%20underexplored.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20unified%20framework%20for%20training%20ORMs%20tailored%20to%0Athe%20Text-to-SQL%20task%20and%20assess%20their%20effectiveness%20as%20a%20test-time%20heuristic%0Awithin%20the%20BoN%20strategy.%20We%20benchmark%20ORMs%20against%20ex-BoN%20and%20Maj%20across%20the%0ABIRD%20and%20Spider%20datasets%2C%20fine-tuning%20diverse%20open-source%20LLMs%20from%20the%20Qwen2%2C%0AGranite3%2C%20and%20Llama3%20families.%20Results%20show%20that%20ORMs%20outperform%20ex-BoN%20and%0AMaj%2C%20achieving%20execution%20accuracy%20gains%20of%20%2B4.33%25%20%28BIRD%29%20and%20%2B2.10%25%20%28Spider%29%0Aover%20ex-BoN%2C%20and%20%2B2.91%25%20%28BIRD%29%20and%20%2B0.93%25%20%28Spider%29%20over%20Maj.%20We%20further%0Ademonstrate%20that%20finetuning%20models%20already%20aligned%20with%20SQL%20generation%2C%20such%20as%0AOmniSQL%2C%20yields%20superior%20ORM%20performance.%20Additionally%2C%20we%20observe%20that%20ORMs%0Aachieve%20competitive%20results%20on%20simple%20queries%20and%20benefit%20more%20from%20an%0Aincreased%20number%20of%20candidates%20compared%20to%20ex-BoN%20and%20Maj.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.01308v2&entry.124074799=Read"},
{"title": "NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset\n  for Narrowband Powerline Communications", "author": "Ying-Ren Chien and Po-Heng Chou and You-Jie Peng and Chun-Yuan Huang and Hen-Wai Tsao and Yu Tsao", "abstract": "  To effectively process impulse noise for narrowband powerline communications\n(NB-PLCs) transceivers, capturing comprehensive statistics of nonperiodic\nasynchronous impulsive noise (APIN) is a critical task. However, existing\nmathematical noise generative models only capture part of the characteristics\nof noise. In this study, we propose a novel generative adversarial network\n(GAN) called noise generation GAN (NGGAN) that learns the complicated\ncharacteristics of practically measured noise samples for data synthesis. To\nclosely match the statistics of complicated noise over the NB-PLC systems, we\nmeasured the NB-PLC noise via the analog coupling and bandpass filtering\ncircuits of a commercial NB-PLC modem to build a realistic dataset. To train\nNGGAN, we adhere to the following principles: 1) we design the length of input\nsignals that the NGGAN model can fit to facilitate cyclostationary noise\ngeneration; 2) the Wasserstein distance is used as a loss function to enhance\nthe similarity between the generated noise and training data; and 3) to measure\nthe similarity performances of GAN-based models based on the mathematical and\npractically measured datasets, we conduct both quantitative and qualitative\nanalyses. The training datasets include: 1) a piecewise spectral\ncyclostationary Gaussian model (PSCGM); 2) a frequency-shift (FRESH) filter;\nand 3) practical measurements from NB-PLC systems. Simulation results\ndemonstrate that the generated noise samples from the proposed NGGAN are highly\nclose to the real noise samples. The principal component analysis (PCA) scatter\nplots and Fr\\'echet inception distance (FID) analysis have shown that NGGAN\noutperforms other GAN-based models by generating noise samples with superior\nfidelity and higher diversity.\n", "link": "http://arxiv.org/abs/2510.01850v3", "date": "2025-10-29", "relevancy": 1.9487, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4952}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4867}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NGGAN%3A%20Noise%20Generation%20GAN%20Based%20on%20the%20Practical%20Measurement%20Dataset%0A%20%20for%20Narrowband%20Powerline%20Communications&body=Title%3A%20NGGAN%3A%20Noise%20Generation%20GAN%20Based%20on%20the%20Practical%20Measurement%20Dataset%0A%20%20for%20Narrowband%20Powerline%20Communications%0AAuthor%3A%20Ying-Ren%20Chien%20and%20Po-Heng%20Chou%20and%20You-Jie%20Peng%20and%20Chun-Yuan%20Huang%20and%20Hen-Wai%20Tsao%20and%20Yu%20Tsao%0AAbstract%3A%20%20%20To%20effectively%20process%20impulse%20noise%20for%20narrowband%20powerline%20communications%0A%28NB-PLCs%29%20transceivers%2C%20capturing%20comprehensive%20statistics%20of%20nonperiodic%0Aasynchronous%20impulsive%20noise%20%28APIN%29%20is%20a%20critical%20task.%20However%2C%20existing%0Amathematical%20noise%20generative%20models%20only%20capture%20part%20of%20the%20characteristics%0Aof%20noise.%20In%20this%20study%2C%20we%20propose%20a%20novel%20generative%20adversarial%20network%0A%28GAN%29%20called%20noise%20generation%20GAN%20%28NGGAN%29%20that%20learns%20the%20complicated%0Acharacteristics%20of%20practically%20measured%20noise%20samples%20for%20data%20synthesis.%20To%0Aclosely%20match%20the%20statistics%20of%20complicated%20noise%20over%20the%20NB-PLC%20systems%2C%20we%0Ameasured%20the%20NB-PLC%20noise%20via%20the%20analog%20coupling%20and%20bandpass%20filtering%0Acircuits%20of%20a%20commercial%20NB-PLC%20modem%20to%20build%20a%20realistic%20dataset.%20To%20train%0ANGGAN%2C%20we%20adhere%20to%20the%20following%20principles%3A%201%29%20we%20design%20the%20length%20of%20input%0Asignals%20that%20the%20NGGAN%20model%20can%20fit%20to%20facilitate%20cyclostationary%20noise%0Ageneration%3B%202%29%20the%20Wasserstein%20distance%20is%20used%20as%20a%20loss%20function%20to%20enhance%0Athe%20similarity%20between%20the%20generated%20noise%20and%20training%20data%3B%20and%203%29%20to%20measure%0Athe%20similarity%20performances%20of%20GAN-based%20models%20based%20on%20the%20mathematical%20and%0Apractically%20measured%20datasets%2C%20we%20conduct%20both%20quantitative%20and%20qualitative%0Aanalyses.%20The%20training%20datasets%20include%3A%201%29%20a%20piecewise%20spectral%0Acyclostationary%20Gaussian%20model%20%28PSCGM%29%3B%202%29%20a%20frequency-shift%20%28FRESH%29%20filter%3B%0Aand%203%29%20practical%20measurements%20from%20NB-PLC%20systems.%20Simulation%20results%0Ademonstrate%20that%20the%20generated%20noise%20samples%20from%20the%20proposed%20NGGAN%20are%20highly%0Aclose%20to%20the%20real%20noise%20samples.%20The%20principal%20component%20analysis%20%28PCA%29%20scatter%0Aplots%20and%20Fr%5C%27echet%20inception%20distance%20%28FID%29%20analysis%20have%20shown%20that%20NGGAN%0Aoutperforms%20other%20GAN-based%20models%20by%20generating%20noise%20samples%20with%20superior%0Afidelity%20and%20higher%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.01850v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNGGAN%253A%2520Noise%2520Generation%2520GAN%2520Based%2520on%2520the%2520Practical%2520Measurement%2520Dataset%250A%2520%2520for%2520Narrowband%2520Powerline%2520Communications%26entry.906535625%3DYing-Ren%2520Chien%2520and%2520Po-Heng%2520Chou%2520and%2520You-Jie%2520Peng%2520and%2520Chun-Yuan%2520Huang%2520and%2520Hen-Wai%2520Tsao%2520and%2520Yu%2520Tsao%26entry.1292438233%3D%2520%2520To%2520effectively%2520process%2520impulse%2520noise%2520for%2520narrowband%2520powerline%2520communications%250A%2528NB-PLCs%2529%2520transceivers%252C%2520capturing%2520comprehensive%2520statistics%2520of%2520nonperiodic%250Aasynchronous%2520impulsive%2520noise%2520%2528APIN%2529%2520is%2520a%2520critical%2520task.%2520However%252C%2520existing%250Amathematical%2520noise%2520generative%2520models%2520only%2520capture%2520part%2520of%2520the%2520characteristics%250Aof%2520noise.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520generative%2520adversarial%2520network%250A%2528GAN%2529%2520called%2520noise%2520generation%2520GAN%2520%2528NGGAN%2529%2520that%2520learns%2520the%2520complicated%250Acharacteristics%2520of%2520practically%2520measured%2520noise%2520samples%2520for%2520data%2520synthesis.%2520To%250Aclosely%2520match%2520the%2520statistics%2520of%2520complicated%2520noise%2520over%2520the%2520NB-PLC%2520systems%252C%2520we%250Ameasured%2520the%2520NB-PLC%2520noise%2520via%2520the%2520analog%2520coupling%2520and%2520bandpass%2520filtering%250Acircuits%2520of%2520a%2520commercial%2520NB-PLC%2520modem%2520to%2520build%2520a%2520realistic%2520dataset.%2520To%2520train%250ANGGAN%252C%2520we%2520adhere%2520to%2520the%2520following%2520principles%253A%25201%2529%2520we%2520design%2520the%2520length%2520of%2520input%250Asignals%2520that%2520the%2520NGGAN%2520model%2520can%2520fit%2520to%2520facilitate%2520cyclostationary%2520noise%250Ageneration%253B%25202%2529%2520the%2520Wasserstein%2520distance%2520is%2520used%2520as%2520a%2520loss%2520function%2520to%2520enhance%250Athe%2520similarity%2520between%2520the%2520generated%2520noise%2520and%2520training%2520data%253B%2520and%25203%2529%2520to%2520measure%250Athe%2520similarity%2520performances%2520of%2520GAN-based%2520models%2520based%2520on%2520the%2520mathematical%2520and%250Apractically%2520measured%2520datasets%252C%2520we%2520conduct%2520both%2520quantitative%2520and%2520qualitative%250Aanalyses.%2520The%2520training%2520datasets%2520include%253A%25201%2529%2520a%2520piecewise%2520spectral%250Acyclostationary%2520Gaussian%2520model%2520%2528PSCGM%2529%253B%25202%2529%2520a%2520frequency-shift%2520%2528FRESH%2529%2520filter%253B%250Aand%25203%2529%2520practical%2520measurements%2520from%2520NB-PLC%2520systems.%2520Simulation%2520results%250Ademonstrate%2520that%2520the%2520generated%2520noise%2520samples%2520from%2520the%2520proposed%2520NGGAN%2520are%2520highly%250Aclose%2520to%2520the%2520real%2520noise%2520samples.%2520The%2520principal%2520component%2520analysis%2520%2528PCA%2529%2520scatter%250Aplots%2520and%2520Fr%255C%2527echet%2520inception%2520distance%2520%2528FID%2529%2520analysis%2520have%2520shown%2520that%2520NGGAN%250Aoutperforms%2520other%2520GAN-based%2520models%2520by%2520generating%2520noise%2520samples%2520with%2520superior%250Afidelity%2520and%2520higher%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01850v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NGGAN%3A%20Noise%20Generation%20GAN%20Based%20on%20the%20Practical%20Measurement%20Dataset%0A%20%20for%20Narrowband%20Powerline%20Communications&entry.906535625=Ying-Ren%20Chien%20and%20Po-Heng%20Chou%20and%20You-Jie%20Peng%20and%20Chun-Yuan%20Huang%20and%20Hen-Wai%20Tsao%20and%20Yu%20Tsao&entry.1292438233=%20%20To%20effectively%20process%20impulse%20noise%20for%20narrowband%20powerline%20communications%0A%28NB-PLCs%29%20transceivers%2C%20capturing%20comprehensive%20statistics%20of%20nonperiodic%0Aasynchronous%20impulsive%20noise%20%28APIN%29%20is%20a%20critical%20task.%20However%2C%20existing%0Amathematical%20noise%20generative%20models%20only%20capture%20part%20of%20the%20characteristics%0Aof%20noise.%20In%20this%20study%2C%20we%20propose%20a%20novel%20generative%20adversarial%20network%0A%28GAN%29%20called%20noise%20generation%20GAN%20%28NGGAN%29%20that%20learns%20the%20complicated%0Acharacteristics%20of%20practically%20measured%20noise%20samples%20for%20data%20synthesis.%20To%0Aclosely%20match%20the%20statistics%20of%20complicated%20noise%20over%20the%20NB-PLC%20systems%2C%20we%0Ameasured%20the%20NB-PLC%20noise%20via%20the%20analog%20coupling%20and%20bandpass%20filtering%0Acircuits%20of%20a%20commercial%20NB-PLC%20modem%20to%20build%20a%20realistic%20dataset.%20To%20train%0ANGGAN%2C%20we%20adhere%20to%20the%20following%20principles%3A%201%29%20we%20design%20the%20length%20of%20input%0Asignals%20that%20the%20NGGAN%20model%20can%20fit%20to%20facilitate%20cyclostationary%20noise%0Ageneration%3B%202%29%20the%20Wasserstein%20distance%20is%20used%20as%20a%20loss%20function%20to%20enhance%0Athe%20similarity%20between%20the%20generated%20noise%20and%20training%20data%3B%20and%203%29%20to%20measure%0Athe%20similarity%20performances%20of%20GAN-based%20models%20based%20on%20the%20mathematical%20and%0Apractically%20measured%20datasets%2C%20we%20conduct%20both%20quantitative%20and%20qualitative%0Aanalyses.%20The%20training%20datasets%20include%3A%201%29%20a%20piecewise%20spectral%0Acyclostationary%20Gaussian%20model%20%28PSCGM%29%3B%202%29%20a%20frequency-shift%20%28FRESH%29%20filter%3B%0Aand%203%29%20practical%20measurements%20from%20NB-PLC%20systems.%20Simulation%20results%0Ademonstrate%20that%20the%20generated%20noise%20samples%20from%20the%20proposed%20NGGAN%20are%20highly%0Aclose%20to%20the%20real%20noise%20samples.%20The%20principal%20component%20analysis%20%28PCA%29%20scatter%0Aplots%20and%20Fr%5C%27echet%20inception%20distance%20%28FID%29%20analysis%20have%20shown%20that%20NGGAN%0Aoutperforms%20other%20GAN-based%20models%20by%20generating%20noise%20samples%20with%20superior%0Afidelity%20and%20higher%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.01850v3&entry.124074799=Read"},
{"title": "Pearl: A Foundation Model for Placing Every Atom in the Right Location", "author": " Genesis Research Team and Alejandro Dobles and Nina Jovic and Kenneth Leidal and Pranav Murugan and David C. Williams and Drausin Wulsin and Nate Gruver and Christina X. Ji and Korrawat Pruegsanusak and Gianluca Scarpellini and Ansh Sharma and Wojciech Swiderski and Andrea Bootsma and Richard Strong Bowen and Charlotte Chen and Jamin Chen and Marc Andr\u00e9 D\u00e4mgen and Benjamin DiFrancesco and J. D. Fishman and Alla Ivanova and Zach Kagin and David Li-Bland and Zuli Liu and Igor Morozov and Jeffrey Ouyang-Zhang and Frank C. Pickard IV and Kushal S. Shah and Ben Shor and Gabriel Monteiro da Silva and Roy Tal and Maxx Tessmer and Carl Tilbury and Cyr Vetcher and Daniel Zeng and Maruan Al-Shedivat and Aleksandra Faust and Evan N. Feinberg and Michael V. LeVine and Matteus Pan", "abstract": "  Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining.\n", "link": "http://arxiv.org/abs/2510.24670v2", "date": "2025-10-29", "relevancy": 1.9423, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4867}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pearl%3A%20A%20Foundation%20Model%20for%20Placing%20Every%20Atom%20in%20the%20Right%20Location&body=Title%3A%20Pearl%3A%20A%20Foundation%20Model%20for%20Placing%20Every%20Atom%20in%20the%20Right%20Location%0AAuthor%3A%20%20Genesis%20Research%20Team%20and%20Alejandro%20Dobles%20and%20Nina%20Jovic%20and%20Kenneth%20Leidal%20and%20Pranav%20Murugan%20and%20David%20C.%20Williams%20and%20Drausin%20Wulsin%20and%20Nate%20Gruver%20and%20Christina%20X.%20Ji%20and%20Korrawat%20Pruegsanusak%20and%20Gianluca%20Scarpellini%20and%20Ansh%20Sharma%20and%20Wojciech%20Swiderski%20and%20Andrea%20Bootsma%20and%20Richard%20Strong%20Bowen%20and%20Charlotte%20Chen%20and%20Jamin%20Chen%20and%20Marc%20Andr%C3%A9%20D%C3%A4mgen%20and%20Benjamin%20DiFrancesco%20and%20J.%20D.%20Fishman%20and%20Alla%20Ivanova%20and%20Zach%20Kagin%20and%20David%20Li-Bland%20and%20Zuli%20Liu%20and%20Igor%20Morozov%20and%20Jeffrey%20Ouyang-Zhang%20and%20Frank%20C.%20Pickard%20IV%20and%20Kushal%20S.%20Shah%20and%20Ben%20Shor%20and%20Gabriel%20Monteiro%20da%20Silva%20and%20Roy%20Tal%20and%20Maxx%20Tessmer%20and%20Carl%20Tilbury%20and%20Cyr%20Vetcher%20and%20Daniel%20Zeng%20and%20Maruan%20Al-Shedivat%20and%20Aleksandra%20Faust%20and%20Evan%20N.%20Feinberg%20and%20Michael%20V.%20LeVine%20and%20Matteus%20Pan%0AAbstract%3A%20%20%20Accurately%20predicting%20the%20three-dimensional%20structures%20of%20protein-ligand%0Acomplexes%20remains%20a%20fundamental%20challenge%20in%20computational%20drug%20discovery%20that%0Alimits%20the%20pace%20and%20success%20of%20therapeutic%20design.%20Deep%20learning%20methods%20have%0Arecently%20shown%20strong%20potential%20as%20structural%20prediction%20tools%2C%20achieving%0Apromising%20accuracy%20across%20diverse%20biomolecular%20systems.%20However%2C%20their%0Aperformance%20and%20utility%20are%20constrained%20by%20scarce%20experimental%20data%2C%0Ainefficient%20architectures%2C%20physically%20invalid%20poses%2C%20and%20the%20limited%20ability%20to%0Aexploit%20auxiliary%20information%20available%20at%20inference.%20To%20address%20these%20issues%2C%0Awe%20introduce%20Pearl%20%28Placing%20Every%20Atom%20in%20the%20Right%20Location%29%2C%20a%20foundation%0Amodel%20for%20protein-ligand%20cofolding%20at%20scale.%20Pearl%20addresses%20these%20challenges%0Awith%20three%20key%20innovations%3A%20%281%29%20training%20recipes%20that%20include%20large-scale%0Asynthetic%20data%20to%20overcome%20data%20scarcity%3B%20%282%29%20architectures%20that%20incorporate%20an%0ASO%283%29-equivariant%20diffusion%20module%20to%20inherently%20respect%203D%20rotational%0Asymmetries%2C%20improving%20generalization%20and%20sample%20efficiency%2C%20and%20%283%29%0Acontrollable%20inference%2C%20including%20a%20generalized%20multi-chain%20templating%20system%0Asupporting%20both%20protein%20and%20non-polymeric%20components%20as%20well%20as%20dual%0Aunconditional/conditional%20modes.%20Pearl%20establishes%20a%20new%20state-of-the-art%0Aperformance%20in%20protein-ligand%20cofolding.%20On%20the%20key%20metric%20of%20generating%0Aaccurate%20%28RMSD%20%3C%202%20%5Cr%7BA%7D%29%20and%20physically%20valid%20poses%2C%20Pearl%20surpasses%20AlphaFold%0A3%20and%20other%20open%20source%20baselines%20on%20the%20public%20Runs%20N%27%20Poses%20and%20PoseBusters%0Abenchmarks%2C%20delivering%2014.5%25%20and%2014.2%25%20improvements%2C%20respectively%2C%20over%20the%0Anext%20best%20model.%20In%20the%20pocket-conditional%20cofolding%20regime%2C%20Pearl%20delivers%0A%243.6%5Ctimes%24%20improvement%20on%20a%20proprietary%20set%20of%20challenging%2C%20real-world%20drug%0Atargets%20at%20the%20more%20rigorous%20RMSD%20%3C%201%20%5Cr%7BA%7D%20threshold.%20Finally%2C%20we%20demonstrate%0Athat%20model%20performance%20correlates%20directly%20with%20synthetic%20dataset%20size%20used%20in%0Atraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.24670v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPearl%253A%2520A%2520Foundation%2520Model%2520for%2520Placing%2520Every%2520Atom%2520in%2520the%2520Right%2520Location%26entry.906535625%3D%2520Genesis%2520Research%2520Team%2520and%2520Alejandro%2520Dobles%2520and%2520Nina%2520Jovic%2520and%2520Kenneth%2520Leidal%2520and%2520Pranav%2520Murugan%2520and%2520David%2520C.%2520Williams%2520and%2520Drausin%2520Wulsin%2520and%2520Nate%2520Gruver%2520and%2520Christina%2520X.%2520Ji%2520and%2520Korrawat%2520Pruegsanusak%2520and%2520Gianluca%2520Scarpellini%2520and%2520Ansh%2520Sharma%2520and%2520Wojciech%2520Swiderski%2520and%2520Andrea%2520Bootsma%2520and%2520Richard%2520Strong%2520Bowen%2520and%2520Charlotte%2520Chen%2520and%2520Jamin%2520Chen%2520and%2520Marc%2520Andr%25C3%25A9%2520D%25C3%25A4mgen%2520and%2520Benjamin%2520DiFrancesco%2520and%2520J.%2520D.%2520Fishman%2520and%2520Alla%2520Ivanova%2520and%2520Zach%2520Kagin%2520and%2520David%2520Li-Bland%2520and%2520Zuli%2520Liu%2520and%2520Igor%2520Morozov%2520and%2520Jeffrey%2520Ouyang-Zhang%2520and%2520Frank%2520C.%2520Pickard%2520IV%2520and%2520Kushal%2520S.%2520Shah%2520and%2520Ben%2520Shor%2520and%2520Gabriel%2520Monteiro%2520da%2520Silva%2520and%2520Roy%2520Tal%2520and%2520Maxx%2520Tessmer%2520and%2520Carl%2520Tilbury%2520and%2520Cyr%2520Vetcher%2520and%2520Daniel%2520Zeng%2520and%2520Maruan%2520Al-Shedivat%2520and%2520Aleksandra%2520Faust%2520and%2520Evan%2520N.%2520Feinberg%2520and%2520Michael%2520V.%2520LeVine%2520and%2520Matteus%2520Pan%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520the%2520three-dimensional%2520structures%2520of%2520protein-ligand%250Acomplexes%2520remains%2520a%2520fundamental%2520challenge%2520in%2520computational%2520drug%2520discovery%2520that%250Alimits%2520the%2520pace%2520and%2520success%2520of%2520therapeutic%2520design.%2520Deep%2520learning%2520methods%2520have%250Arecently%2520shown%2520strong%2520potential%2520as%2520structural%2520prediction%2520tools%252C%2520achieving%250Apromising%2520accuracy%2520across%2520diverse%2520biomolecular%2520systems.%2520However%252C%2520their%250Aperformance%2520and%2520utility%2520are%2520constrained%2520by%2520scarce%2520experimental%2520data%252C%250Ainefficient%2520architectures%252C%2520physically%2520invalid%2520poses%252C%2520and%2520the%2520limited%2520ability%2520to%250Aexploit%2520auxiliary%2520information%2520available%2520at%2520inference.%2520To%2520address%2520these%2520issues%252C%250Awe%2520introduce%2520Pearl%2520%2528Placing%2520Every%2520Atom%2520in%2520the%2520Right%2520Location%2529%252C%2520a%2520foundation%250Amodel%2520for%2520protein-ligand%2520cofolding%2520at%2520scale.%2520Pearl%2520addresses%2520these%2520challenges%250Awith%2520three%2520key%2520innovations%253A%2520%25281%2529%2520training%2520recipes%2520that%2520include%2520large-scale%250Asynthetic%2520data%2520to%2520overcome%2520data%2520scarcity%253B%2520%25282%2529%2520architectures%2520that%2520incorporate%2520an%250ASO%25283%2529-equivariant%2520diffusion%2520module%2520to%2520inherently%2520respect%25203D%2520rotational%250Asymmetries%252C%2520improving%2520generalization%2520and%2520sample%2520efficiency%252C%2520and%2520%25283%2529%250Acontrollable%2520inference%252C%2520including%2520a%2520generalized%2520multi-chain%2520templating%2520system%250Asupporting%2520both%2520protein%2520and%2520non-polymeric%2520components%2520as%2520well%2520as%2520dual%250Aunconditional/conditional%2520modes.%2520Pearl%2520establishes%2520a%2520new%2520state-of-the-art%250Aperformance%2520in%2520protein-ligand%2520cofolding.%2520On%2520the%2520key%2520metric%2520of%2520generating%250Aaccurate%2520%2528RMSD%2520%253C%25202%2520%255Cr%257BA%257D%2529%2520and%2520physically%2520valid%2520poses%252C%2520Pearl%2520surpasses%2520AlphaFold%250A3%2520and%2520other%2520open%2520source%2520baselines%2520on%2520the%2520public%2520Runs%2520N%2527%2520Poses%2520and%2520PoseBusters%250Abenchmarks%252C%2520delivering%252014.5%2525%2520and%252014.2%2525%2520improvements%252C%2520respectively%252C%2520over%2520the%250Anext%2520best%2520model.%2520In%2520the%2520pocket-conditional%2520cofolding%2520regime%252C%2520Pearl%2520delivers%250A%25243.6%255Ctimes%2524%2520improvement%2520on%2520a%2520proprietary%2520set%2520of%2520challenging%252C%2520real-world%2520drug%250Atargets%2520at%2520the%2520more%2520rigorous%2520RMSD%2520%253C%25201%2520%255Cr%257BA%257D%2520threshold.%2520Finally%252C%2520we%2520demonstrate%250Athat%2520model%2520performance%2520correlates%2520directly%2520with%2520synthetic%2520dataset%2520size%2520used%2520in%250Atraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.24670v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pearl%3A%20A%20Foundation%20Model%20for%20Placing%20Every%20Atom%20in%20the%20Right%20Location&entry.906535625=%20Genesis%20Research%20Team%20and%20Alejandro%20Dobles%20and%20Nina%20Jovic%20and%20Kenneth%20Leidal%20and%20Pranav%20Murugan%20and%20David%20C.%20Williams%20and%20Drausin%20Wulsin%20and%20Nate%20Gruver%20and%20Christina%20X.%20Ji%20and%20Korrawat%20Pruegsanusak%20and%20Gianluca%20Scarpellini%20and%20Ansh%20Sharma%20and%20Wojciech%20Swiderski%20and%20Andrea%20Bootsma%20and%20Richard%20Strong%20Bowen%20and%20Charlotte%20Chen%20and%20Jamin%20Chen%20and%20Marc%20Andr%C3%A9%20D%C3%A4mgen%20and%20Benjamin%20DiFrancesco%20and%20J.%20D.%20Fishman%20and%20Alla%20Ivanova%20and%20Zach%20Kagin%20and%20David%20Li-Bland%20and%20Zuli%20Liu%20and%20Igor%20Morozov%20and%20Jeffrey%20Ouyang-Zhang%20and%20Frank%20C.%20Pickard%20IV%20and%20Kushal%20S.%20Shah%20and%20Ben%20Shor%20and%20Gabriel%20Monteiro%20da%20Silva%20and%20Roy%20Tal%20and%20Maxx%20Tessmer%20and%20Carl%20Tilbury%20and%20Cyr%20Vetcher%20and%20Daniel%20Zeng%20and%20Maruan%20Al-Shedivat%20and%20Aleksandra%20Faust%20and%20Evan%20N.%20Feinberg%20and%20Michael%20V.%20LeVine%20and%20Matteus%20Pan&entry.1292438233=%20%20Accurately%20predicting%20the%20three-dimensional%20structures%20of%20protein-ligand%0Acomplexes%20remains%20a%20fundamental%20challenge%20in%20computational%20drug%20discovery%20that%0Alimits%20the%20pace%20and%20success%20of%20therapeutic%20design.%20Deep%20learning%20methods%20have%0Arecently%20shown%20strong%20potential%20as%20structural%20prediction%20tools%2C%20achieving%0Apromising%20accuracy%20across%20diverse%20biomolecular%20systems.%20However%2C%20their%0Aperformance%20and%20utility%20are%20constrained%20by%20scarce%20experimental%20data%2C%0Ainefficient%20architectures%2C%20physically%20invalid%20poses%2C%20and%20the%20limited%20ability%20to%0Aexploit%20auxiliary%20information%20available%20at%20inference.%20To%20address%20these%20issues%2C%0Awe%20introduce%20Pearl%20%28Placing%20Every%20Atom%20in%20the%20Right%20Location%29%2C%20a%20foundation%0Amodel%20for%20protein-ligand%20cofolding%20at%20scale.%20Pearl%20addresses%20these%20challenges%0Awith%20three%20key%20innovations%3A%20%281%29%20training%20recipes%20that%20include%20large-scale%0Asynthetic%20data%20to%20overcome%20data%20scarcity%3B%20%282%29%20architectures%20that%20incorporate%20an%0ASO%283%29-equivariant%20diffusion%20module%20to%20inherently%20respect%203D%20rotational%0Asymmetries%2C%20improving%20generalization%20and%20sample%20efficiency%2C%20and%20%283%29%0Acontrollable%20inference%2C%20including%20a%20generalized%20multi-chain%20templating%20system%0Asupporting%20both%20protein%20and%20non-polymeric%20components%20as%20well%20as%20dual%0Aunconditional/conditional%20modes.%20Pearl%20establishes%20a%20new%20state-of-the-art%0Aperformance%20in%20protein-ligand%20cofolding.%20On%20the%20key%20metric%20of%20generating%0Aaccurate%20%28RMSD%20%3C%202%20%5Cr%7BA%7D%29%20and%20physically%20valid%20poses%2C%20Pearl%20surpasses%20AlphaFold%0A3%20and%20other%20open%20source%20baselines%20on%20the%20public%20Runs%20N%27%20Poses%20and%20PoseBusters%0Abenchmarks%2C%20delivering%2014.5%25%20and%2014.2%25%20improvements%2C%20respectively%2C%20over%20the%0Anext%20best%20model.%20In%20the%20pocket-conditional%20cofolding%20regime%2C%20Pearl%20delivers%0A%243.6%5Ctimes%24%20improvement%20on%20a%20proprietary%20set%20of%20challenging%2C%20real-world%20drug%0Atargets%20at%20the%20more%20rigorous%20RMSD%20%3C%201%20%5Cr%7BA%7D%20threshold.%20Finally%2C%20we%20demonstrate%0Athat%20model%20performance%20correlates%20directly%20with%20synthetic%20dataset%20size%20used%20in%0Atraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.24670v2&entry.124074799=Read"},
{"title": "A Configuration-First Framework for Reproducible, Low-Code Localization", "author": "Tim Strnad and Bla\u017e Bertalani\u010d and Carolina Fortuna", "abstract": "  Machine learning is increasingly permeating radio-based localization\nservices. To keep results credible and comparable, everyday workflows should\nmake rigorous experiment specification and exact repeatability the default,\nwithout blocking advanced experimentation. However, in practice, researchers\nface a three-way gap that could be filled by a framework that offers (i) low\ncoding effort for end-to-end studies, (ii) reproducibility by default including\nversioned code, data, and configurations, controlled randomness, isolated runs,\nand recorded artifacts, and (iii) built-in extensibility so new models,\nmetrics, and stages can be added with minimal integration effort. Existing\ntools rarely deliver all three for machine learning in general and localization\nworkflows in particular. In this paper we introduce LOCALIZE, a low-code,\nconfiguration-first framework for radio localization in which experiments are\ndeclared in human-readable configuration, a workflow orchestrator runs\nstandardized pipelines from data preparation to reporting, and all artifacts,\nsuch as datasets, models, metrics, and reports, are versioned. The\npreconfigured, versioned datasets reduce initial setup and boilerplate,\nspeeding up model development and evaluation. The design, with clear extension\npoints, allows experts to add components without reworking the infrastructure.\nIn a qualitative comparison and a head-to-head study against a plain Jupyter\nnotebook baseline, we show that the framework reduces authoring effort while\nmaintaining comparable runtime and memory behavior. Furthermore, using a\nBluetooth Low Energy dataset, we show that scaling across training data (1x to\n10x) keeps orchestration overheads bounded as data grows. Overall, the\nframework makes reproducible machine-learning-based localization\nexperimentation practical, accessible, and extensible.\n", "link": "http://arxiv.org/abs/2510.25692v1", "date": "2025-10-29", "relevancy": 1.9352, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4946}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4919}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Configuration-First%20Framework%20for%20Reproducible%2C%20Low-Code%20Localization&body=Title%3A%20A%20Configuration-First%20Framework%20for%20Reproducible%2C%20Low-Code%20Localization%0AAuthor%3A%20Tim%20Strnad%20and%20Bla%C5%BE%20Bertalani%C4%8D%20and%20Carolina%20Fortuna%0AAbstract%3A%20%20%20Machine%20learning%20is%20increasingly%20permeating%20radio-based%20localization%0Aservices.%20To%20keep%20results%20credible%20and%20comparable%2C%20everyday%20workflows%20should%0Amake%20rigorous%20experiment%20specification%20and%20exact%20repeatability%20the%20default%2C%0Awithout%20blocking%20advanced%20experimentation.%20However%2C%20in%20practice%2C%20researchers%0Aface%20a%20three-way%20gap%20that%20could%20be%20filled%20by%20a%20framework%20that%20offers%20%28i%29%20low%0Acoding%20effort%20for%20end-to-end%20studies%2C%20%28ii%29%20reproducibility%20by%20default%20including%0Aversioned%20code%2C%20data%2C%20and%20configurations%2C%20controlled%20randomness%2C%20isolated%20runs%2C%0Aand%20recorded%20artifacts%2C%20and%20%28iii%29%20built-in%20extensibility%20so%20new%20models%2C%0Ametrics%2C%20and%20stages%20can%20be%20added%20with%20minimal%20integration%20effort.%20Existing%0Atools%20rarely%20deliver%20all%20three%20for%20machine%20learning%20in%20general%20and%20localization%0Aworkflows%20in%20particular.%20In%20this%20paper%20we%20introduce%20LOCALIZE%2C%20a%20low-code%2C%0Aconfiguration-first%20framework%20for%20radio%20localization%20in%20which%20experiments%20are%0Adeclared%20in%20human-readable%20configuration%2C%20a%20workflow%20orchestrator%20runs%0Astandardized%20pipelines%20from%20data%20preparation%20to%20reporting%2C%20and%20all%20artifacts%2C%0Asuch%20as%20datasets%2C%20models%2C%20metrics%2C%20and%20reports%2C%20are%20versioned.%20The%0Apreconfigured%2C%20versioned%20datasets%20reduce%20initial%20setup%20and%20boilerplate%2C%0Aspeeding%20up%20model%20development%20and%20evaluation.%20The%20design%2C%20with%20clear%20extension%0Apoints%2C%20allows%20experts%20to%20add%20components%20without%20reworking%20the%20infrastructure.%0AIn%20a%20qualitative%20comparison%20and%20a%20head-to-head%20study%20against%20a%20plain%20Jupyter%0Anotebook%20baseline%2C%20we%20show%20that%20the%20framework%20reduces%20authoring%20effort%20while%0Amaintaining%20comparable%20runtime%20and%20memory%20behavior.%20Furthermore%2C%20using%20a%0ABluetooth%20Low%20Energy%20dataset%2C%20we%20show%20that%20scaling%20across%20training%20data%20%281x%20to%0A10x%29%20keeps%20orchestration%20overheads%20bounded%20as%20data%20grows.%20Overall%2C%20the%0Aframework%20makes%20reproducible%20machine-learning-based%20localization%0Aexperimentation%20practical%2C%20accessible%2C%20and%20extensible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Configuration-First%2520Framework%2520for%2520Reproducible%252C%2520Low-Code%2520Localization%26entry.906535625%3DTim%2520Strnad%2520and%2520Bla%25C5%25BE%2520Bertalani%25C4%258D%2520and%2520Carolina%2520Fortuna%26entry.1292438233%3D%2520%2520Machine%2520learning%2520is%2520increasingly%2520permeating%2520radio-based%2520localization%250Aservices.%2520To%2520keep%2520results%2520credible%2520and%2520comparable%252C%2520everyday%2520workflows%2520should%250Amake%2520rigorous%2520experiment%2520specification%2520and%2520exact%2520repeatability%2520the%2520default%252C%250Awithout%2520blocking%2520advanced%2520experimentation.%2520However%252C%2520in%2520practice%252C%2520researchers%250Aface%2520a%2520three-way%2520gap%2520that%2520could%2520be%2520filled%2520by%2520a%2520framework%2520that%2520offers%2520%2528i%2529%2520low%250Acoding%2520effort%2520for%2520end-to-end%2520studies%252C%2520%2528ii%2529%2520reproducibility%2520by%2520default%2520including%250Aversioned%2520code%252C%2520data%252C%2520and%2520configurations%252C%2520controlled%2520randomness%252C%2520isolated%2520runs%252C%250Aand%2520recorded%2520artifacts%252C%2520and%2520%2528iii%2529%2520built-in%2520extensibility%2520so%2520new%2520models%252C%250Ametrics%252C%2520and%2520stages%2520can%2520be%2520added%2520with%2520minimal%2520integration%2520effort.%2520Existing%250Atools%2520rarely%2520deliver%2520all%2520three%2520for%2520machine%2520learning%2520in%2520general%2520and%2520localization%250Aworkflows%2520in%2520particular.%2520In%2520this%2520paper%2520we%2520introduce%2520LOCALIZE%252C%2520a%2520low-code%252C%250Aconfiguration-first%2520framework%2520for%2520radio%2520localization%2520in%2520which%2520experiments%2520are%250Adeclared%2520in%2520human-readable%2520configuration%252C%2520a%2520workflow%2520orchestrator%2520runs%250Astandardized%2520pipelines%2520from%2520data%2520preparation%2520to%2520reporting%252C%2520and%2520all%2520artifacts%252C%250Asuch%2520as%2520datasets%252C%2520models%252C%2520metrics%252C%2520and%2520reports%252C%2520are%2520versioned.%2520The%250Apreconfigured%252C%2520versioned%2520datasets%2520reduce%2520initial%2520setup%2520and%2520boilerplate%252C%250Aspeeding%2520up%2520model%2520development%2520and%2520evaluation.%2520The%2520design%252C%2520with%2520clear%2520extension%250Apoints%252C%2520allows%2520experts%2520to%2520add%2520components%2520without%2520reworking%2520the%2520infrastructure.%250AIn%2520a%2520qualitative%2520comparison%2520and%2520a%2520head-to-head%2520study%2520against%2520a%2520plain%2520Jupyter%250Anotebook%2520baseline%252C%2520we%2520show%2520that%2520the%2520framework%2520reduces%2520authoring%2520effort%2520while%250Amaintaining%2520comparable%2520runtime%2520and%2520memory%2520behavior.%2520Furthermore%252C%2520using%2520a%250ABluetooth%2520Low%2520Energy%2520dataset%252C%2520we%2520show%2520that%2520scaling%2520across%2520training%2520data%2520%25281x%2520to%250A10x%2529%2520keeps%2520orchestration%2520overheads%2520bounded%2520as%2520data%2520grows.%2520Overall%252C%2520the%250Aframework%2520makes%2520reproducible%2520machine-learning-based%2520localization%250Aexperimentation%2520practical%252C%2520accessible%252C%2520and%2520extensible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Configuration-First%20Framework%20for%20Reproducible%2C%20Low-Code%20Localization&entry.906535625=Tim%20Strnad%20and%20Bla%C5%BE%20Bertalani%C4%8D%20and%20Carolina%20Fortuna&entry.1292438233=%20%20Machine%20learning%20is%20increasingly%20permeating%20radio-based%20localization%0Aservices.%20To%20keep%20results%20credible%20and%20comparable%2C%20everyday%20workflows%20should%0Amake%20rigorous%20experiment%20specification%20and%20exact%20repeatability%20the%20default%2C%0Awithout%20blocking%20advanced%20experimentation.%20However%2C%20in%20practice%2C%20researchers%0Aface%20a%20three-way%20gap%20that%20could%20be%20filled%20by%20a%20framework%20that%20offers%20%28i%29%20low%0Acoding%20effort%20for%20end-to-end%20studies%2C%20%28ii%29%20reproducibility%20by%20default%20including%0Aversioned%20code%2C%20data%2C%20and%20configurations%2C%20controlled%20randomness%2C%20isolated%20runs%2C%0Aand%20recorded%20artifacts%2C%20and%20%28iii%29%20built-in%20extensibility%20so%20new%20models%2C%0Ametrics%2C%20and%20stages%20can%20be%20added%20with%20minimal%20integration%20effort.%20Existing%0Atools%20rarely%20deliver%20all%20three%20for%20machine%20learning%20in%20general%20and%20localization%0Aworkflows%20in%20particular.%20In%20this%20paper%20we%20introduce%20LOCALIZE%2C%20a%20low-code%2C%0Aconfiguration-first%20framework%20for%20radio%20localization%20in%20which%20experiments%20are%0Adeclared%20in%20human-readable%20configuration%2C%20a%20workflow%20orchestrator%20runs%0Astandardized%20pipelines%20from%20data%20preparation%20to%20reporting%2C%20and%20all%20artifacts%2C%0Asuch%20as%20datasets%2C%20models%2C%20metrics%2C%20and%20reports%2C%20are%20versioned.%20The%0Apreconfigured%2C%20versioned%20datasets%20reduce%20initial%20setup%20and%20boilerplate%2C%0Aspeeding%20up%20model%20development%20and%20evaluation.%20The%20design%2C%20with%20clear%20extension%0Apoints%2C%20allows%20experts%20to%20add%20components%20without%20reworking%20the%20infrastructure.%0AIn%20a%20qualitative%20comparison%20and%20a%20head-to-head%20study%20against%20a%20plain%20Jupyter%0Anotebook%20baseline%2C%20we%20show%20that%20the%20framework%20reduces%20authoring%20effort%20while%0Amaintaining%20comparable%20runtime%20and%20memory%20behavior.%20Furthermore%2C%20using%20a%0ABluetooth%20Low%20Energy%20dataset%2C%20we%20show%20that%20scaling%20across%20training%20data%20%281x%20to%0A10x%29%20keeps%20orchestration%20overheads%20bounded%20as%20data%20grows.%20Overall%2C%20the%0Aframework%20makes%20reproducible%20machine-learning-based%20localization%0Aexperimentation%20practical%2C%20accessible%2C%20and%20extensible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25692v1&entry.124074799=Read"},
{"title": "Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision\n  Transformers", "author": "M Yashwanth and Sharannya Ghosh and Aditay Tripathi and Anirban Chakraborty", "abstract": "  Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has\nproven highly effective as a parameter-efficient fine-tuning technique for\nadapting large models to downstream tasks with limited data. Its parameter\nefficiency makes it particularly suitable for Federated Learning (FL), where\nboth communication and computation budgets are often constrained. However,\nglobal prompt tuning struggles to generalize across heterogeneous clients,\nwhile personalized tuning overfits to local data and lacks generalization. We\npropose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt\nTuning), a unified framework designed to achieve both generalization and\npersonalization in federated prompt tuning of ViTs. Within this framework, we\nintroduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on\nclass-specific prompts maintained alongside a globally shared prompt. For each\ninput, CCMP adaptively combines class-specific prompts using weights derived\nfrom global class prototypes and client class priors. This approach enables\nper-sample prompt personalization without storing client-dependent trainable\nparameters. The prompts are collaboratively optimized via traditional federated\naveraging technique on the same. Comprehensive evaluations on CIFAR-100,\nTinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT\nconsistently surpasses the state-of-the-art baselines under diverse data\nheterogeneity scenarios, establishing a strong foundation for efficient and\ngeneralizable federated prompt tuning of Vision Transformers.\n", "link": "http://arxiv.org/abs/2510.25372v1", "date": "2025-10-29", "relevancy": 1.9347, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4853}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4829}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20Estimation%20from%20Prototypes%20for%20Federated%20Prompt%20Tuning%20of%20Vision%0A%20%20Transformers&body=Title%3A%20Prompt%20Estimation%20from%20Prototypes%20for%20Federated%20Prompt%20Tuning%20of%20Vision%0A%20%20Transformers%0AAuthor%3A%20M%20Yashwanth%20and%20Sharannya%20Ghosh%20and%20Aditay%20Tripathi%20and%20Anirban%20Chakraborty%0AAbstract%3A%20%20%20Visual%20Prompt%20Tuning%20%28VPT%29%20of%20pre-trained%20Vision%20Transformers%20%28ViTs%29%20has%0Aproven%20highly%20effective%20as%20a%20parameter-efficient%20fine-tuning%20technique%20for%0Aadapting%20large%20models%20to%20downstream%20tasks%20with%20limited%20data.%20Its%20parameter%0Aefficiency%20makes%20it%20particularly%20suitable%20for%20Federated%20Learning%20%28FL%29%2C%20where%0Aboth%20communication%20and%20computation%20budgets%20are%20often%20constrained.%20However%2C%0Aglobal%20prompt%20tuning%20struggles%20to%20generalize%20across%20heterogeneous%20clients%2C%0Awhile%20personalized%20tuning%20overfits%20to%20local%20data%20and%20lacks%20generalization.%20We%0Apropose%20PEP-FedPT%20%28Prompt%20Estimation%20from%20Prototypes%20for%20Federated%20Prompt%0ATuning%29%2C%20a%20unified%20framework%20designed%20to%20achieve%20both%20generalization%20and%0Apersonalization%20in%20federated%20prompt%20tuning%20of%20ViTs.%20Within%20this%20framework%2C%20we%0Aintroduce%20the%20novel%20Class-Contextualized%20Mixed%20Prompt%20%28CCMP%29%20-%20based%20on%0Aclass-specific%20prompts%20maintained%20alongside%20a%20globally%20shared%20prompt.%20For%20each%0Ainput%2C%20CCMP%20adaptively%20combines%20class-specific%20prompts%20using%20weights%20derived%0Afrom%20global%20class%20prototypes%20and%20client%20class%20priors.%20This%20approach%20enables%0Aper-sample%20prompt%20personalization%20without%20storing%20client-dependent%20trainable%0Aparameters.%20The%20prompts%20are%20collaboratively%20optimized%20via%20traditional%20federated%0Aaveraging%20technique%20on%20the%20same.%20Comprehensive%20evaluations%20on%20CIFAR-100%2C%0ATinyImageNet%2C%20DomainNet%2C%20and%20iNaturalist%20datasets%20demonstrate%20that%20PEP-FedPT%0Aconsistently%20surpasses%20the%20state-of-the-art%20baselines%20under%20diverse%20data%0Aheterogeneity%20scenarios%2C%20establishing%20a%20strong%20foundation%20for%20efficient%20and%0Ageneralizable%20federated%20prompt%20tuning%20of%20Vision%20Transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520Estimation%2520from%2520Prototypes%2520for%2520Federated%2520Prompt%2520Tuning%2520of%2520Vision%250A%2520%2520Transformers%26entry.906535625%3DM%2520Yashwanth%2520and%2520Sharannya%2520Ghosh%2520and%2520Aditay%2520Tripathi%2520and%2520Anirban%2520Chakraborty%26entry.1292438233%3D%2520%2520Visual%2520Prompt%2520Tuning%2520%2528VPT%2529%2520of%2520pre-trained%2520Vision%2520Transformers%2520%2528ViTs%2529%2520has%250Aproven%2520highly%2520effective%2520as%2520a%2520parameter-efficient%2520fine-tuning%2520technique%2520for%250Aadapting%2520large%2520models%2520to%2520downstream%2520tasks%2520with%2520limited%2520data.%2520Its%2520parameter%250Aefficiency%2520makes%2520it%2520particularly%2520suitable%2520for%2520Federated%2520Learning%2520%2528FL%2529%252C%2520where%250Aboth%2520communication%2520and%2520computation%2520budgets%2520are%2520often%2520constrained.%2520However%252C%250Aglobal%2520prompt%2520tuning%2520struggles%2520to%2520generalize%2520across%2520heterogeneous%2520clients%252C%250Awhile%2520personalized%2520tuning%2520overfits%2520to%2520local%2520data%2520and%2520lacks%2520generalization.%2520We%250Apropose%2520PEP-FedPT%2520%2528Prompt%2520Estimation%2520from%2520Prototypes%2520for%2520Federated%2520Prompt%250ATuning%2529%252C%2520a%2520unified%2520framework%2520designed%2520to%2520achieve%2520both%2520generalization%2520and%250Apersonalization%2520in%2520federated%2520prompt%2520tuning%2520of%2520ViTs.%2520Within%2520this%2520framework%252C%2520we%250Aintroduce%2520the%2520novel%2520Class-Contextualized%2520Mixed%2520Prompt%2520%2528CCMP%2529%2520-%2520based%2520on%250Aclass-specific%2520prompts%2520maintained%2520alongside%2520a%2520globally%2520shared%2520prompt.%2520For%2520each%250Ainput%252C%2520CCMP%2520adaptively%2520combines%2520class-specific%2520prompts%2520using%2520weights%2520derived%250Afrom%2520global%2520class%2520prototypes%2520and%2520client%2520class%2520priors.%2520This%2520approach%2520enables%250Aper-sample%2520prompt%2520personalization%2520without%2520storing%2520client-dependent%2520trainable%250Aparameters.%2520The%2520prompts%2520are%2520collaboratively%2520optimized%2520via%2520traditional%2520federated%250Aaveraging%2520technique%2520on%2520the%2520same.%2520Comprehensive%2520evaluations%2520on%2520CIFAR-100%252C%250ATinyImageNet%252C%2520DomainNet%252C%2520and%2520iNaturalist%2520datasets%2520demonstrate%2520that%2520PEP-FedPT%250Aconsistently%2520surpasses%2520the%2520state-of-the-art%2520baselines%2520under%2520diverse%2520data%250Aheterogeneity%2520scenarios%252C%2520establishing%2520a%2520strong%2520foundation%2520for%2520efficient%2520and%250Ageneralizable%2520federated%2520prompt%2520tuning%2520of%2520Vision%2520Transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20Estimation%20from%20Prototypes%20for%20Federated%20Prompt%20Tuning%20of%20Vision%0A%20%20Transformers&entry.906535625=M%20Yashwanth%20and%20Sharannya%20Ghosh%20and%20Aditay%20Tripathi%20and%20Anirban%20Chakraborty&entry.1292438233=%20%20Visual%20Prompt%20Tuning%20%28VPT%29%20of%20pre-trained%20Vision%20Transformers%20%28ViTs%29%20has%0Aproven%20highly%20effective%20as%20a%20parameter-efficient%20fine-tuning%20technique%20for%0Aadapting%20large%20models%20to%20downstream%20tasks%20with%20limited%20data.%20Its%20parameter%0Aefficiency%20makes%20it%20particularly%20suitable%20for%20Federated%20Learning%20%28FL%29%2C%20where%0Aboth%20communication%20and%20computation%20budgets%20are%20often%20constrained.%20However%2C%0Aglobal%20prompt%20tuning%20struggles%20to%20generalize%20across%20heterogeneous%20clients%2C%0Awhile%20personalized%20tuning%20overfits%20to%20local%20data%20and%20lacks%20generalization.%20We%0Apropose%20PEP-FedPT%20%28Prompt%20Estimation%20from%20Prototypes%20for%20Federated%20Prompt%0ATuning%29%2C%20a%20unified%20framework%20designed%20to%20achieve%20both%20generalization%20and%0Apersonalization%20in%20federated%20prompt%20tuning%20of%20ViTs.%20Within%20this%20framework%2C%20we%0Aintroduce%20the%20novel%20Class-Contextualized%20Mixed%20Prompt%20%28CCMP%29%20-%20based%20on%0Aclass-specific%20prompts%20maintained%20alongside%20a%20globally%20shared%20prompt.%20For%20each%0Ainput%2C%20CCMP%20adaptively%20combines%20class-specific%20prompts%20using%20weights%20derived%0Afrom%20global%20class%20prototypes%20and%20client%20class%20priors.%20This%20approach%20enables%0Aper-sample%20prompt%20personalization%20without%20storing%20client-dependent%20trainable%0Aparameters.%20The%20prompts%20are%20collaboratively%20optimized%20via%20traditional%20federated%0Aaveraging%20technique%20on%20the%20same.%20Comprehensive%20evaluations%20on%20CIFAR-100%2C%0ATinyImageNet%2C%20DomainNet%2C%20and%20iNaturalist%20datasets%20demonstrate%20that%20PEP-FedPT%0Aconsistently%20surpasses%20the%20state-of-the-art%20baselines%20under%20diverse%20data%0Aheterogeneity%20scenarios%2C%20establishing%20a%20strong%20foundation%20for%20efficient%20and%0Ageneralizable%20federated%20prompt%20tuning%20of%20Vision%20Transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25372v1&entry.124074799=Read"},
{"title": "E-Scores for (In)Correctness Assessment of Generative Model Outputs", "author": "Guneet S. Dhillon and Javier Gonz\u00e1lez and Teodora Pandeva and Alicia Curth", "abstract": "  While generative models, especially large language models (LLMs), are\nubiquitous in today's world, principled mechanisms to assess their\n(in)correctness are limited. Using the conformal prediction framework, previous\nworks construct sets of LLM responses where the probability of including an\nincorrect response, or error, is capped at a desired user-defined tolerance\nlevel. However, since these methods are based on p-values, they are susceptible\nto p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the\nguarantees. We therefore leverage e-values to complement generative model\noutputs with e-scores as a measure of incorrectness. In addition to achieving\nthe same statistical guarantees as before, e-scores provide users flexibility\nin adaptively choosing tolerance levels after observing the e-scores\nthemselves, by upper bounding a post-hoc notion of error called size\ndistortion. We experimentally demonstrate their efficacy in assessing LLM\noutputs for different correctness types: mathematical factuality and property\nconstraints satisfaction.\n", "link": "http://arxiv.org/abs/2510.25770v1", "date": "2025-10-29", "relevancy": 1.9281, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4864}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4802}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E-Scores%20for%20%28In%29Correctness%20Assessment%20of%20Generative%20Model%20Outputs&body=Title%3A%20E-Scores%20for%20%28In%29Correctness%20Assessment%20of%20Generative%20Model%20Outputs%0AAuthor%3A%20Guneet%20S.%20Dhillon%20and%20Javier%20Gonz%C3%A1lez%20and%20Teodora%20Pandeva%20and%20Alicia%20Curth%0AAbstract%3A%20%20%20While%20generative%20models%2C%20especially%20large%20language%20models%20%28LLMs%29%2C%20are%0Aubiquitous%20in%20today%27s%20world%2C%20principled%20mechanisms%20to%20assess%20their%0A%28in%29correctness%20are%20limited.%20Using%20the%20conformal%20prediction%20framework%2C%20previous%0Aworks%20construct%20sets%20of%20LLM%20responses%20where%20the%20probability%20of%20including%20an%0Aincorrect%20response%2C%20or%20error%2C%20is%20capped%20at%20a%20desired%20user-defined%20tolerance%0Alevel.%20However%2C%20since%20these%20methods%20are%20based%20on%20p-values%2C%20they%20are%20susceptible%0Ato%20p-hacking%2C%20i.e.%2C%20choosing%20the%20tolerance%20level%20post-hoc%20can%20invalidate%20the%0Aguarantees.%20We%20therefore%20leverage%20e-values%20to%20complement%20generative%20model%0Aoutputs%20with%20e-scores%20as%20a%20measure%20of%20incorrectness.%20In%20addition%20to%20achieving%0Athe%20same%20statistical%20guarantees%20as%20before%2C%20e-scores%20provide%20users%20flexibility%0Ain%20adaptively%20choosing%20tolerance%20levels%20after%20observing%20the%20e-scores%0Athemselves%2C%20by%20upper%20bounding%20a%20post-hoc%20notion%20of%20error%20called%20size%0Adistortion.%20We%20experimentally%20demonstrate%20their%20efficacy%20in%20assessing%20LLM%0Aoutputs%20for%20different%20correctness%20types%3A%20mathematical%20factuality%20and%20property%0Aconstraints%20satisfaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE-Scores%2520for%2520%2528In%2529Correctness%2520Assessment%2520of%2520Generative%2520Model%2520Outputs%26entry.906535625%3DGuneet%2520S.%2520Dhillon%2520and%2520Javier%2520Gonz%25C3%25A1lez%2520and%2520Teodora%2520Pandeva%2520and%2520Alicia%2520Curth%26entry.1292438233%3D%2520%2520While%2520generative%2520models%252C%2520especially%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520are%250Aubiquitous%2520in%2520today%2527s%2520world%252C%2520principled%2520mechanisms%2520to%2520assess%2520their%250A%2528in%2529correctness%2520are%2520limited.%2520Using%2520the%2520conformal%2520prediction%2520framework%252C%2520previous%250Aworks%2520construct%2520sets%2520of%2520LLM%2520responses%2520where%2520the%2520probability%2520of%2520including%2520an%250Aincorrect%2520response%252C%2520or%2520error%252C%2520is%2520capped%2520at%2520a%2520desired%2520user-defined%2520tolerance%250Alevel.%2520However%252C%2520since%2520these%2520methods%2520are%2520based%2520on%2520p-values%252C%2520they%2520are%2520susceptible%250Ato%2520p-hacking%252C%2520i.e.%252C%2520choosing%2520the%2520tolerance%2520level%2520post-hoc%2520can%2520invalidate%2520the%250Aguarantees.%2520We%2520therefore%2520leverage%2520e-values%2520to%2520complement%2520generative%2520model%250Aoutputs%2520with%2520e-scores%2520as%2520a%2520measure%2520of%2520incorrectness.%2520In%2520addition%2520to%2520achieving%250Athe%2520same%2520statistical%2520guarantees%2520as%2520before%252C%2520e-scores%2520provide%2520users%2520flexibility%250Ain%2520adaptively%2520choosing%2520tolerance%2520levels%2520after%2520observing%2520the%2520e-scores%250Athemselves%252C%2520by%2520upper%2520bounding%2520a%2520post-hoc%2520notion%2520of%2520error%2520called%2520size%250Adistortion.%2520We%2520experimentally%2520demonstrate%2520their%2520efficacy%2520in%2520assessing%2520LLM%250Aoutputs%2520for%2520different%2520correctness%2520types%253A%2520mathematical%2520factuality%2520and%2520property%250Aconstraints%2520satisfaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E-Scores%20for%20%28In%29Correctness%20Assessment%20of%20Generative%20Model%20Outputs&entry.906535625=Guneet%20S.%20Dhillon%20and%20Javier%20Gonz%C3%A1lez%20and%20Teodora%20Pandeva%20and%20Alicia%20Curth&entry.1292438233=%20%20While%20generative%20models%2C%20especially%20large%20language%20models%20%28LLMs%29%2C%20are%0Aubiquitous%20in%20today%27s%20world%2C%20principled%20mechanisms%20to%20assess%20their%0A%28in%29correctness%20are%20limited.%20Using%20the%20conformal%20prediction%20framework%2C%20previous%0Aworks%20construct%20sets%20of%20LLM%20responses%20where%20the%20probability%20of%20including%20an%0Aincorrect%20response%2C%20or%20error%2C%20is%20capped%20at%20a%20desired%20user-defined%20tolerance%0Alevel.%20However%2C%20since%20these%20methods%20are%20based%20on%20p-values%2C%20they%20are%20susceptible%0Ato%20p-hacking%2C%20i.e.%2C%20choosing%20the%20tolerance%20level%20post-hoc%20can%20invalidate%20the%0Aguarantees.%20We%20therefore%20leverage%20e-values%20to%20complement%20generative%20model%0Aoutputs%20with%20e-scores%20as%20a%20measure%20of%20incorrectness.%20In%20addition%20to%20achieving%0Athe%20same%20statistical%20guarantees%20as%20before%2C%20e-scores%20provide%20users%20flexibility%0Ain%20adaptively%20choosing%20tolerance%20levels%20after%20observing%20the%20e-scores%0Athemselves%2C%20by%20upper%20bounding%20a%20post-hoc%20notion%20of%20error%20called%20size%0Adistortion.%20We%20experimentally%20demonstrate%20their%20efficacy%20in%20assessing%20LLM%0Aoutputs%20for%20different%20correctness%20types%3A%20mathematical%20factuality%20and%20property%0Aconstraints%20satisfaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25770v1&entry.124074799=Read"},
{"title": "Faster and Simpler Greedy Algorithm for $k$-Median and $k$-Means", "author": "Max Dupr\u00e9 la Tour and David Saulpic", "abstract": "  Clustering problems such as $k$-means and $k$-median are staples of\nunsupervised learning, and many algorithmic techniques have been developed to\ntackle their numerous aspects.\n  In this paper, we focus on the class of greedy approximation algorithm, that\nattracted less attention than local-search or primal-dual counterparts. In\nparticular, we study the recursive greedy algorithm developed by Mettu and\nPlaxton [SIAM J. Comp 2003]. We provide a simplification of the algorithm,\nallowing for faster implementation, in graph metrics or in Euclidean space,\nwhere our algorithm matches or improves the state-of-the-art.\n", "link": "http://arxiv.org/abs/2407.11217v3", "date": "2025-10-29", "relevancy": 1.9269, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3875}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3845}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20and%20Simpler%20Greedy%20Algorithm%20for%20%24k%24-Median%20and%20%24k%24-Means&body=Title%3A%20Faster%20and%20Simpler%20Greedy%20Algorithm%20for%20%24k%24-Median%20and%20%24k%24-Means%0AAuthor%3A%20Max%20Dupr%C3%A9%20la%20Tour%20and%20David%20Saulpic%0AAbstract%3A%20%20%20Clustering%20problems%20such%20as%20%24k%24-means%20and%20%24k%24-median%20are%20staples%20of%0Aunsupervised%20learning%2C%20and%20many%20algorithmic%20techniques%20have%20been%20developed%20to%0Atackle%20their%20numerous%20aspects.%0A%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20class%20of%20greedy%20approximation%20algorithm%2C%20that%0Aattracted%20less%20attention%20than%20local-search%20or%20primal-dual%20counterparts.%20In%0Aparticular%2C%20we%20study%20the%20recursive%20greedy%20algorithm%20developed%20by%20Mettu%20and%0APlaxton%20%5BSIAM%20J.%20Comp%202003%5D.%20We%20provide%20a%20simplification%20of%20the%20algorithm%2C%0Aallowing%20for%20faster%20implementation%2C%20in%20graph%20metrics%20or%20in%20Euclidean%20space%2C%0Awhere%20our%20algorithm%20matches%20or%20improves%20the%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11217v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520and%2520Simpler%2520Greedy%2520Algorithm%2520for%2520%2524k%2524-Median%2520and%2520%2524k%2524-Means%26entry.906535625%3DMax%2520Dupr%25C3%25A9%2520la%2520Tour%2520and%2520David%2520Saulpic%26entry.1292438233%3D%2520%2520Clustering%2520problems%2520such%2520as%2520%2524k%2524-means%2520and%2520%2524k%2524-median%2520are%2520staples%2520of%250Aunsupervised%2520learning%252C%2520and%2520many%2520algorithmic%2520techniques%2520have%2520been%2520developed%2520to%250Atackle%2520their%2520numerous%2520aspects.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520class%2520of%2520greedy%2520approximation%2520algorithm%252C%2520that%250Aattracted%2520less%2520attention%2520than%2520local-search%2520or%2520primal-dual%2520counterparts.%2520In%250Aparticular%252C%2520we%2520study%2520the%2520recursive%2520greedy%2520algorithm%2520developed%2520by%2520Mettu%2520and%250APlaxton%2520%255BSIAM%2520J.%2520Comp%25202003%255D.%2520We%2520provide%2520a%2520simplification%2520of%2520the%2520algorithm%252C%250Aallowing%2520for%2520faster%2520implementation%252C%2520in%2520graph%2520metrics%2520or%2520in%2520Euclidean%2520space%252C%250Awhere%2520our%2520algorithm%2520matches%2520or%2520improves%2520the%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11217v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20and%20Simpler%20Greedy%20Algorithm%20for%20%24k%24-Median%20and%20%24k%24-Means&entry.906535625=Max%20Dupr%C3%A9%20la%20Tour%20and%20David%20Saulpic&entry.1292438233=%20%20Clustering%20problems%20such%20as%20%24k%24-means%20and%20%24k%24-median%20are%20staples%20of%0Aunsupervised%20learning%2C%20and%20many%20algorithmic%20techniques%20have%20been%20developed%20to%0Atackle%20their%20numerous%20aspects.%0A%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20class%20of%20greedy%20approximation%20algorithm%2C%20that%0Aattracted%20less%20attention%20than%20local-search%20or%20primal-dual%20counterparts.%20In%0Aparticular%2C%20we%20study%20the%20recursive%20greedy%20algorithm%20developed%20by%20Mettu%20and%0APlaxton%20%5BSIAM%20J.%20Comp%202003%5D.%20We%20provide%20a%20simplification%20of%20the%20algorithm%2C%0Aallowing%20for%20faster%20implementation%2C%20in%20graph%20metrics%20or%20in%20Euclidean%20space%2C%0Awhere%20our%20algorithm%20matches%20or%20improves%20the%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11217v3&entry.124074799=Read"},
{"title": "Communication and Verification in LLM Agents towards Collaboration under\n  Information Asymmetry", "author": "Run Peng and Ziqiao Ma and Amy Pang and Sikai Li and Zhang Xi-Jia and Yingzhuo Yu and Cristian-Paul Bara and Joyce Chai", "abstract": "  While Large Language Model (LLM) agents are often approached from the angle\nof action planning/generation to accomplish a goal (e.g., given by language\ndescriptions), their abilities to collaborate with each other to achieve a\njoint goal are not well explored. To address this limitation, this paper\nstudies LLM agents in task collaboration, particularly under the condition of\ninformation asymmetry, where agents have disparities in their knowledge and\nskills and need to work together to complete a shared task. We extend Einstein\nPuzzles, a classical symbolic puzzle, to a table-top game. In this game, two\nLLM agents must reason, communicate, and act to satisfy spatial and relational\nconstraints required to solve the puzzle. We apply a fine-tuning-plus-verifier\nframework in which LLM agents are equipped with various communication\nstrategies and verification signals from the environment. Empirical results\nhighlight the critical importance of aligned communication, especially when\nagents possess both information-seeking and -providing capabilities.\nInterestingly, agents without communication can still achieve high task\nperformance; however, further analysis reveals a lack of true rule\nunderstanding and lower trust from human evaluators. Instead, by integrating an\nenvironment-based verifier, we enhance agents' ability to comprehend task rules\nand complete tasks, promoting both safer and more interpretable collaboration\nin AI systems. https://github.com/Roihn/EinsteinPuzzles\n", "link": "http://arxiv.org/abs/2510.25595v1", "date": "2025-10-29", "relevancy": 1.9253, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4818}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4818}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication%20and%20Verification%20in%20LLM%20Agents%20towards%20Collaboration%20under%0A%20%20Information%20Asymmetry&body=Title%3A%20Communication%20and%20Verification%20in%20LLM%20Agents%20towards%20Collaboration%20under%0A%20%20Information%20Asymmetry%0AAuthor%3A%20Run%20Peng%20and%20Ziqiao%20Ma%20and%20Amy%20Pang%20and%20Sikai%20Li%20and%20Zhang%20Xi-Jia%20and%20Yingzhuo%20Yu%20and%20Cristian-Paul%20Bara%20and%20Joyce%20Chai%0AAbstract%3A%20%20%20While%20Large%20Language%20Model%20%28LLM%29%20agents%20are%20often%20approached%20from%20the%20angle%0Aof%20action%20planning/generation%20to%20accomplish%20a%20goal%20%28e.g.%2C%20given%20by%20language%0Adescriptions%29%2C%20their%20abilities%20to%20collaborate%20with%20each%20other%20to%20achieve%20a%0Ajoint%20goal%20are%20not%20well%20explored.%20To%20address%20this%20limitation%2C%20this%20paper%0Astudies%20LLM%20agents%20in%20task%20collaboration%2C%20particularly%20under%20the%20condition%20of%0Ainformation%20asymmetry%2C%20where%20agents%20have%20disparities%20in%20their%20knowledge%20and%0Askills%20and%20need%20to%20work%20together%20to%20complete%20a%20shared%20task.%20We%20extend%20Einstein%0APuzzles%2C%20a%20classical%20symbolic%20puzzle%2C%20to%20a%20table-top%20game.%20In%20this%20game%2C%20two%0ALLM%20agents%20must%20reason%2C%20communicate%2C%20and%20act%20to%20satisfy%20spatial%20and%20relational%0Aconstraints%20required%20to%20solve%20the%20puzzle.%20We%20apply%20a%20fine-tuning-plus-verifier%0Aframework%20in%20which%20LLM%20agents%20are%20equipped%20with%20various%20communication%0Astrategies%20and%20verification%20signals%20from%20the%20environment.%20Empirical%20results%0Ahighlight%20the%20critical%20importance%20of%20aligned%20communication%2C%20especially%20when%0Aagents%20possess%20both%20information-seeking%20and%20-providing%20capabilities.%0AInterestingly%2C%20agents%20without%20communication%20can%20still%20achieve%20high%20task%0Aperformance%3B%20however%2C%20further%20analysis%20reveals%20a%20lack%20of%20true%20rule%0Aunderstanding%20and%20lower%20trust%20from%20human%20evaluators.%20Instead%2C%20by%20integrating%20an%0Aenvironment-based%20verifier%2C%20we%20enhance%20agents%27%20ability%20to%20comprehend%20task%20rules%0Aand%20complete%20tasks%2C%20promoting%20both%20safer%20and%20more%20interpretable%20collaboration%0Ain%20AI%20systems.%20https%3A//github.com/Roihn/EinsteinPuzzles%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication%2520and%2520Verification%2520in%2520LLM%2520Agents%2520towards%2520Collaboration%2520under%250A%2520%2520Information%2520Asymmetry%26entry.906535625%3DRun%2520Peng%2520and%2520Ziqiao%2520Ma%2520and%2520Amy%2520Pang%2520and%2520Sikai%2520Li%2520and%2520Zhang%2520Xi-Jia%2520and%2520Yingzhuo%2520Yu%2520and%2520Cristian-Paul%2520Bara%2520and%2520Joyce%2520Chai%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520are%2520often%2520approached%2520from%2520the%2520angle%250Aof%2520action%2520planning/generation%2520to%2520accomplish%2520a%2520goal%2520%2528e.g.%252C%2520given%2520by%2520language%250Adescriptions%2529%252C%2520their%2520abilities%2520to%2520collaborate%2520with%2520each%2520other%2520to%2520achieve%2520a%250Ajoint%2520goal%2520are%2520not%2520well%2520explored.%2520To%2520address%2520this%2520limitation%252C%2520this%2520paper%250Astudies%2520LLM%2520agents%2520in%2520task%2520collaboration%252C%2520particularly%2520under%2520the%2520condition%2520of%250Ainformation%2520asymmetry%252C%2520where%2520agents%2520have%2520disparities%2520in%2520their%2520knowledge%2520and%250Askills%2520and%2520need%2520to%2520work%2520together%2520to%2520complete%2520a%2520shared%2520task.%2520We%2520extend%2520Einstein%250APuzzles%252C%2520a%2520classical%2520symbolic%2520puzzle%252C%2520to%2520a%2520table-top%2520game.%2520In%2520this%2520game%252C%2520two%250ALLM%2520agents%2520must%2520reason%252C%2520communicate%252C%2520and%2520act%2520to%2520satisfy%2520spatial%2520and%2520relational%250Aconstraints%2520required%2520to%2520solve%2520the%2520puzzle.%2520We%2520apply%2520a%2520fine-tuning-plus-verifier%250Aframework%2520in%2520which%2520LLM%2520agents%2520are%2520equipped%2520with%2520various%2520communication%250Astrategies%2520and%2520verification%2520signals%2520from%2520the%2520environment.%2520Empirical%2520results%250Ahighlight%2520the%2520critical%2520importance%2520of%2520aligned%2520communication%252C%2520especially%2520when%250Aagents%2520possess%2520both%2520information-seeking%2520and%2520-providing%2520capabilities.%250AInterestingly%252C%2520agents%2520without%2520communication%2520can%2520still%2520achieve%2520high%2520task%250Aperformance%253B%2520however%252C%2520further%2520analysis%2520reveals%2520a%2520lack%2520of%2520true%2520rule%250Aunderstanding%2520and%2520lower%2520trust%2520from%2520human%2520evaluators.%2520Instead%252C%2520by%2520integrating%2520an%250Aenvironment-based%2520verifier%252C%2520we%2520enhance%2520agents%2527%2520ability%2520to%2520comprehend%2520task%2520rules%250Aand%2520complete%2520tasks%252C%2520promoting%2520both%2520safer%2520and%2520more%2520interpretable%2520collaboration%250Ain%2520AI%2520systems.%2520https%253A//github.com/Roihn/EinsteinPuzzles%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication%20and%20Verification%20in%20LLM%20Agents%20towards%20Collaboration%20under%0A%20%20Information%20Asymmetry&entry.906535625=Run%20Peng%20and%20Ziqiao%20Ma%20and%20Amy%20Pang%20and%20Sikai%20Li%20and%20Zhang%20Xi-Jia%20and%20Yingzhuo%20Yu%20and%20Cristian-Paul%20Bara%20and%20Joyce%20Chai&entry.1292438233=%20%20While%20Large%20Language%20Model%20%28LLM%29%20agents%20are%20often%20approached%20from%20the%20angle%0Aof%20action%20planning/generation%20to%20accomplish%20a%20goal%20%28e.g.%2C%20given%20by%20language%0Adescriptions%29%2C%20their%20abilities%20to%20collaborate%20with%20each%20other%20to%20achieve%20a%0Ajoint%20goal%20are%20not%20well%20explored.%20To%20address%20this%20limitation%2C%20this%20paper%0Astudies%20LLM%20agents%20in%20task%20collaboration%2C%20particularly%20under%20the%20condition%20of%0Ainformation%20asymmetry%2C%20where%20agents%20have%20disparities%20in%20their%20knowledge%20and%0Askills%20and%20need%20to%20work%20together%20to%20complete%20a%20shared%20task.%20We%20extend%20Einstein%0APuzzles%2C%20a%20classical%20symbolic%20puzzle%2C%20to%20a%20table-top%20game.%20In%20this%20game%2C%20two%0ALLM%20agents%20must%20reason%2C%20communicate%2C%20and%20act%20to%20satisfy%20spatial%20and%20relational%0Aconstraints%20required%20to%20solve%20the%20puzzle.%20We%20apply%20a%20fine-tuning-plus-verifier%0Aframework%20in%20which%20LLM%20agents%20are%20equipped%20with%20various%20communication%0Astrategies%20and%20verification%20signals%20from%20the%20environment.%20Empirical%20results%0Ahighlight%20the%20critical%20importance%20of%20aligned%20communication%2C%20especially%20when%0Aagents%20possess%20both%20information-seeking%20and%20-providing%20capabilities.%0AInterestingly%2C%20agents%20without%20communication%20can%20still%20achieve%20high%20task%0Aperformance%3B%20however%2C%20further%20analysis%20reveals%20a%20lack%20of%20true%20rule%0Aunderstanding%20and%20lower%20trust%20from%20human%20evaluators.%20Instead%2C%20by%20integrating%20an%0Aenvironment-based%20verifier%2C%20we%20enhance%20agents%27%20ability%20to%20comprehend%20task%20rules%0Aand%20complete%20tasks%2C%20promoting%20both%20safer%20and%20more%20interpretable%20collaboration%0Ain%20AI%20systems.%20https%3A//github.com/Roihn/EinsteinPuzzles%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25595v1&entry.124074799=Read"},
{"title": "GnnXemplar: Exemplars to Explanations -- Natural Language Rules for\n  Global GNN Interpretability", "author": "Burouj Armgaan and Eshan Jain and Harsh Pandey and Mahesh Chandran and Sayan Ranu", "abstract": "  Graph Neural Networks (GNNs) are widely used for node classification, yet\ntheir opaque decision-making limits trust and adoption. While local\nexplanations offer insights into individual predictions, global explanation\nmethods, those that characterize an entire class, remain underdeveloped.\nExisting global explainers rely on motif discovery in small graphs, an approach\nthat breaks down in large, real-world settings where subgraph repetition is\nrare, node attributes are high-dimensional, and predictions arise from complex\nstructure-attribute interactions. We propose GnnXemplar, a novel global\nexplainer inspired from Exemplar Theory from cognitive science. GnnXemplar\nidentifies representative nodes in the GNN embedding space, exemplars, and\nexplains predictions using natural language rules derived from their\nneighborhoods. Exemplar selection is framed as a coverage maximization problem\nover reverse k-nearest neighbors, for which we provide an efficient greedy\napproximation. To derive interpretable rules, we employ a self-refining prompt\nstrategy using large language models (LLMs). Experiments across diverse\nbenchmarks show that GnnXemplar significantly outperforms existing methods in\nfidelity, scalability, and human interpretability, as validated by a user study\nwith 60 participants.\n", "link": "http://arxiv.org/abs/2509.18376v2", "date": "2025-10-29", "relevancy": 1.9243, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4865}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GnnXemplar%3A%20Exemplars%20to%20Explanations%20--%20Natural%20Language%20Rules%20for%0A%20%20Global%20GNN%20Interpretability&body=Title%3A%20GnnXemplar%3A%20Exemplars%20to%20Explanations%20--%20Natural%20Language%20Rules%20for%0A%20%20Global%20GNN%20Interpretability%0AAuthor%3A%20Burouj%20Armgaan%20and%20Eshan%20Jain%20and%20Harsh%20Pandey%20and%20Mahesh%20Chandran%20and%20Sayan%20Ranu%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20widely%20used%20for%20node%20classification%2C%20yet%0Atheir%20opaque%20decision-making%20limits%20trust%20and%20adoption.%20While%20local%0Aexplanations%20offer%20insights%20into%20individual%20predictions%2C%20global%20explanation%0Amethods%2C%20those%20that%20characterize%20an%20entire%20class%2C%20remain%20underdeveloped.%0AExisting%20global%20explainers%20rely%20on%20motif%20discovery%20in%20small%20graphs%2C%20an%20approach%0Athat%20breaks%20down%20in%20large%2C%20real-world%20settings%20where%20subgraph%20repetition%20is%0Arare%2C%20node%20attributes%20are%20high-dimensional%2C%20and%20predictions%20arise%20from%20complex%0Astructure-attribute%20interactions.%20We%20propose%20GnnXemplar%2C%20a%20novel%20global%0Aexplainer%20inspired%20from%20Exemplar%20Theory%20from%20cognitive%20science.%20GnnXemplar%0Aidentifies%20representative%20nodes%20in%20the%20GNN%20embedding%20space%2C%20exemplars%2C%20and%0Aexplains%20predictions%20using%20natural%20language%20rules%20derived%20from%20their%0Aneighborhoods.%20Exemplar%20selection%20is%20framed%20as%20a%20coverage%20maximization%20problem%0Aover%20reverse%20k-nearest%20neighbors%2C%20for%20which%20we%20provide%20an%20efficient%20greedy%0Aapproximation.%20To%20derive%20interpretable%20rules%2C%20we%20employ%20a%20self-refining%20prompt%0Astrategy%20using%20large%20language%20models%20%28LLMs%29.%20Experiments%20across%20diverse%0Abenchmarks%20show%20that%20GnnXemplar%20significantly%20outperforms%20existing%20methods%20in%0Afidelity%2C%20scalability%2C%20and%20human%20interpretability%2C%20as%20validated%20by%20a%20user%20study%0Awith%2060%20participants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18376v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGnnXemplar%253A%2520Exemplars%2520to%2520Explanations%2520--%2520Natural%2520Language%2520Rules%2520for%250A%2520%2520Global%2520GNN%2520Interpretability%26entry.906535625%3DBurouj%2520Armgaan%2520and%2520Eshan%2520Jain%2520and%2520Harsh%2520Pandey%2520and%2520Mahesh%2520Chandran%2520and%2520Sayan%2520Ranu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520widely%2520used%2520for%2520node%2520classification%252C%2520yet%250Atheir%2520opaque%2520decision-making%2520limits%2520trust%2520and%2520adoption.%2520While%2520local%250Aexplanations%2520offer%2520insights%2520into%2520individual%2520predictions%252C%2520global%2520explanation%250Amethods%252C%2520those%2520that%2520characterize%2520an%2520entire%2520class%252C%2520remain%2520underdeveloped.%250AExisting%2520global%2520explainers%2520rely%2520on%2520motif%2520discovery%2520in%2520small%2520graphs%252C%2520an%2520approach%250Athat%2520breaks%2520down%2520in%2520large%252C%2520real-world%2520settings%2520where%2520subgraph%2520repetition%2520is%250Arare%252C%2520node%2520attributes%2520are%2520high-dimensional%252C%2520and%2520predictions%2520arise%2520from%2520complex%250Astructure-attribute%2520interactions.%2520We%2520propose%2520GnnXemplar%252C%2520a%2520novel%2520global%250Aexplainer%2520inspired%2520from%2520Exemplar%2520Theory%2520from%2520cognitive%2520science.%2520GnnXemplar%250Aidentifies%2520representative%2520nodes%2520in%2520the%2520GNN%2520embedding%2520space%252C%2520exemplars%252C%2520and%250Aexplains%2520predictions%2520using%2520natural%2520language%2520rules%2520derived%2520from%2520their%250Aneighborhoods.%2520Exemplar%2520selection%2520is%2520framed%2520as%2520a%2520coverage%2520maximization%2520problem%250Aover%2520reverse%2520k-nearest%2520neighbors%252C%2520for%2520which%2520we%2520provide%2520an%2520efficient%2520greedy%250Aapproximation.%2520To%2520derive%2520interpretable%2520rules%252C%2520we%2520employ%2520a%2520self-refining%2520prompt%250Astrategy%2520using%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Experiments%2520across%2520diverse%250Abenchmarks%2520show%2520that%2520GnnXemplar%2520significantly%2520outperforms%2520existing%2520methods%2520in%250Afidelity%252C%2520scalability%252C%2520and%2520human%2520interpretability%252C%2520as%2520validated%2520by%2520a%2520user%2520study%250Awith%252060%2520participants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18376v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GnnXemplar%3A%20Exemplars%20to%20Explanations%20--%20Natural%20Language%20Rules%20for%0A%20%20Global%20GNN%20Interpretability&entry.906535625=Burouj%20Armgaan%20and%20Eshan%20Jain%20and%20Harsh%20Pandey%20and%20Mahesh%20Chandran%20and%20Sayan%20Ranu&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20widely%20used%20for%20node%20classification%2C%20yet%0Atheir%20opaque%20decision-making%20limits%20trust%20and%20adoption.%20While%20local%0Aexplanations%20offer%20insights%20into%20individual%20predictions%2C%20global%20explanation%0Amethods%2C%20those%20that%20characterize%20an%20entire%20class%2C%20remain%20underdeveloped.%0AExisting%20global%20explainers%20rely%20on%20motif%20discovery%20in%20small%20graphs%2C%20an%20approach%0Athat%20breaks%20down%20in%20large%2C%20real-world%20settings%20where%20subgraph%20repetition%20is%0Arare%2C%20node%20attributes%20are%20high-dimensional%2C%20and%20predictions%20arise%20from%20complex%0Astructure-attribute%20interactions.%20We%20propose%20GnnXemplar%2C%20a%20novel%20global%0Aexplainer%20inspired%20from%20Exemplar%20Theory%20from%20cognitive%20science.%20GnnXemplar%0Aidentifies%20representative%20nodes%20in%20the%20GNN%20embedding%20space%2C%20exemplars%2C%20and%0Aexplains%20predictions%20using%20natural%20language%20rules%20derived%20from%20their%0Aneighborhoods.%20Exemplar%20selection%20is%20framed%20as%20a%20coverage%20maximization%20problem%0Aover%20reverse%20k-nearest%20neighbors%2C%20for%20which%20we%20provide%20an%20efficient%20greedy%0Aapproximation.%20To%20derive%20interpretable%20rules%2C%20we%20employ%20a%20self-refining%20prompt%0Astrategy%20using%20large%20language%20models%20%28LLMs%29.%20Experiments%20across%20diverse%0Abenchmarks%20show%20that%20GnnXemplar%20significantly%20outperforms%20existing%20methods%20in%0Afidelity%2C%20scalability%2C%20and%20human%20interpretability%2C%20as%20validated%20by%20a%20user%20study%0Awith%2060%20participants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18376v2&entry.124074799=Read"},
{"title": "Position: Biology is the Challenge Physics-Informed ML Needs to Evolve", "author": "Julien Martinelli", "abstract": "  Physics-Informed Machine Learning (PIML) has successfully integrated\nmechanistic understanding into machine learning, particularly in domains\ngoverned by well-known physical laws. This success has motivated efforts to\napply PIML to biology, a field rich in dynamical systems but shaped by\ndifferent constraints. Biological modeling, however, presents unique\nchallenges: multi-faceted and uncertain prior knowledge, heterogeneous and\nnoisy data, partial observability, and complex, high-dimensional networks. In\nthis position paper, we argue that these challenges should not be seen as\nobstacles to PIML, but as catalysts for its evolution. We propose\nBiology-Informed Machine Learning (BIML): a principled extension of PIML that\nretains its structural grounding while adapting to the practical realities of\nbiology. Rather than replacing PIML, BIML retools its methods to operate under\nsofter, probabilistic forms of prior knowledge. We outline four foundational\npillars as a roadmap for this transition: uncertainty quantification,\ncontextualization, constrained latent structure inference, and scalability.\nFoundation Models and Large Language Models will be key enablers, bridging\nhuman expertise with computational modeling. We conclude with concrete\nrecommendations to build the BIML ecosystem and channel PIML-inspired\ninnovation toward challenges of high scientific and societal relevance.\n", "link": "http://arxiv.org/abs/2510.25368v1", "date": "2025-10-29", "relevancy": 1.9227, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5249}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4726}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Biology%20is%20the%20Challenge%20Physics-Informed%20ML%20Needs%20to%20Evolve&body=Title%3A%20Position%3A%20Biology%20is%20the%20Challenge%20Physics-Informed%20ML%20Needs%20to%20Evolve%0AAuthor%3A%20Julien%20Martinelli%0AAbstract%3A%20%20%20Physics-Informed%20Machine%20Learning%20%28PIML%29%20has%20successfully%20integrated%0Amechanistic%20understanding%20into%20machine%20learning%2C%20particularly%20in%20domains%0Agoverned%20by%20well-known%20physical%20laws.%20This%20success%20has%20motivated%20efforts%20to%0Aapply%20PIML%20to%20biology%2C%20a%20field%20rich%20in%20dynamical%20systems%20but%20shaped%20by%0Adifferent%20constraints.%20Biological%20modeling%2C%20however%2C%20presents%20unique%0Achallenges%3A%20multi-faceted%20and%20uncertain%20prior%20knowledge%2C%20heterogeneous%20and%0Anoisy%20data%2C%20partial%20observability%2C%20and%20complex%2C%20high-dimensional%20networks.%20In%0Athis%20position%20paper%2C%20we%20argue%20that%20these%20challenges%20should%20not%20be%20seen%20as%0Aobstacles%20to%20PIML%2C%20but%20as%20catalysts%20for%20its%20evolution.%20We%20propose%0ABiology-Informed%20Machine%20Learning%20%28BIML%29%3A%20a%20principled%20extension%20of%20PIML%20that%0Aretains%20its%20structural%20grounding%20while%20adapting%20to%20the%20practical%20realities%20of%0Abiology.%20Rather%20than%20replacing%20PIML%2C%20BIML%20retools%20its%20methods%20to%20operate%20under%0Asofter%2C%20probabilistic%20forms%20of%20prior%20knowledge.%20We%20outline%20four%20foundational%0Apillars%20as%20a%20roadmap%20for%20this%20transition%3A%20uncertainty%20quantification%2C%0Acontextualization%2C%20constrained%20latent%20structure%20inference%2C%20and%20scalability.%0AFoundation%20Models%20and%20Large%20Language%20Models%20will%20be%20key%20enablers%2C%20bridging%0Ahuman%20expertise%20with%20computational%20modeling.%20We%20conclude%20with%20concrete%0Arecommendations%20to%20build%20the%20BIML%20ecosystem%20and%20channel%20PIML-inspired%0Ainnovation%20toward%20challenges%20of%20high%20scientific%20and%20societal%20relevance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Biology%2520is%2520the%2520Challenge%2520Physics-Informed%2520ML%2520Needs%2520to%2520Evolve%26entry.906535625%3DJulien%2520Martinelli%26entry.1292438233%3D%2520%2520Physics-Informed%2520Machine%2520Learning%2520%2528PIML%2529%2520has%2520successfully%2520integrated%250Amechanistic%2520understanding%2520into%2520machine%2520learning%252C%2520particularly%2520in%2520domains%250Agoverned%2520by%2520well-known%2520physical%2520laws.%2520This%2520success%2520has%2520motivated%2520efforts%2520to%250Aapply%2520PIML%2520to%2520biology%252C%2520a%2520field%2520rich%2520in%2520dynamical%2520systems%2520but%2520shaped%2520by%250Adifferent%2520constraints.%2520Biological%2520modeling%252C%2520however%252C%2520presents%2520unique%250Achallenges%253A%2520multi-faceted%2520and%2520uncertain%2520prior%2520knowledge%252C%2520heterogeneous%2520and%250Anoisy%2520data%252C%2520partial%2520observability%252C%2520and%2520complex%252C%2520high-dimensional%2520networks.%2520In%250Athis%2520position%2520paper%252C%2520we%2520argue%2520that%2520these%2520challenges%2520should%2520not%2520be%2520seen%2520as%250Aobstacles%2520to%2520PIML%252C%2520but%2520as%2520catalysts%2520for%2520its%2520evolution.%2520We%2520propose%250ABiology-Informed%2520Machine%2520Learning%2520%2528BIML%2529%253A%2520a%2520principled%2520extension%2520of%2520PIML%2520that%250Aretains%2520its%2520structural%2520grounding%2520while%2520adapting%2520to%2520the%2520practical%2520realities%2520of%250Abiology.%2520Rather%2520than%2520replacing%2520PIML%252C%2520BIML%2520retools%2520its%2520methods%2520to%2520operate%2520under%250Asofter%252C%2520probabilistic%2520forms%2520of%2520prior%2520knowledge.%2520We%2520outline%2520four%2520foundational%250Apillars%2520as%2520a%2520roadmap%2520for%2520this%2520transition%253A%2520uncertainty%2520quantification%252C%250Acontextualization%252C%2520constrained%2520latent%2520structure%2520inference%252C%2520and%2520scalability.%250AFoundation%2520Models%2520and%2520Large%2520Language%2520Models%2520will%2520be%2520key%2520enablers%252C%2520bridging%250Ahuman%2520expertise%2520with%2520computational%2520modeling.%2520We%2520conclude%2520with%2520concrete%250Arecommendations%2520to%2520build%2520the%2520BIML%2520ecosystem%2520and%2520channel%2520PIML-inspired%250Ainnovation%2520toward%2520challenges%2520of%2520high%2520scientific%2520and%2520societal%2520relevance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Biology%20is%20the%20Challenge%20Physics-Informed%20ML%20Needs%20to%20Evolve&entry.906535625=Julien%20Martinelli&entry.1292438233=%20%20Physics-Informed%20Machine%20Learning%20%28PIML%29%20has%20successfully%20integrated%0Amechanistic%20understanding%20into%20machine%20learning%2C%20particularly%20in%20domains%0Agoverned%20by%20well-known%20physical%20laws.%20This%20success%20has%20motivated%20efforts%20to%0Aapply%20PIML%20to%20biology%2C%20a%20field%20rich%20in%20dynamical%20systems%20but%20shaped%20by%0Adifferent%20constraints.%20Biological%20modeling%2C%20however%2C%20presents%20unique%0Achallenges%3A%20multi-faceted%20and%20uncertain%20prior%20knowledge%2C%20heterogeneous%20and%0Anoisy%20data%2C%20partial%20observability%2C%20and%20complex%2C%20high-dimensional%20networks.%20In%0Athis%20position%20paper%2C%20we%20argue%20that%20these%20challenges%20should%20not%20be%20seen%20as%0Aobstacles%20to%20PIML%2C%20but%20as%20catalysts%20for%20its%20evolution.%20We%20propose%0ABiology-Informed%20Machine%20Learning%20%28BIML%29%3A%20a%20principled%20extension%20of%20PIML%20that%0Aretains%20its%20structural%20grounding%20while%20adapting%20to%20the%20practical%20realities%20of%0Abiology.%20Rather%20than%20replacing%20PIML%2C%20BIML%20retools%20its%20methods%20to%20operate%20under%0Asofter%2C%20probabilistic%20forms%20of%20prior%20knowledge.%20We%20outline%20four%20foundational%0Apillars%20as%20a%20roadmap%20for%20this%20transition%3A%20uncertainty%20quantification%2C%0Acontextualization%2C%20constrained%20latent%20structure%20inference%2C%20and%20scalability.%0AFoundation%20Models%20and%20Large%20Language%20Models%20will%20be%20key%20enablers%2C%20bridging%0Ahuman%20expertise%20with%20computational%20modeling.%20We%20conclude%20with%20concrete%0Arecommendations%20to%20build%20the%20BIML%20ecosystem%20and%20channel%20PIML-inspired%0Ainnovation%20toward%20challenges%20of%20high%20scientific%20and%20societal%20relevance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25368v1&entry.124074799=Read"},
{"title": "Improving Temporal Consistency and Fidelity at Inference-time in\n  Perceptual Video Restoration by Zero-shot Image-based Diffusion Models", "author": "Nasrin Rahimi and A. Murat Tekalp", "abstract": "  Diffusion models have emerged as powerful priors for single-image\nrestoration, but their application to zero-shot video restoration suffers from\ntemporal inconsistencies due to the stochastic nature of sampling and\ncomplexity of incorporating explicit temporal modeling. In this work, we\naddress the challenge of improving temporal coherence in video restoration\nusing zero-shot image-based diffusion models without retraining or modifying\ntheir architecture. We propose two complementary inference-time strategies: (1)\nPerceptual Straightening Guidance (PSG) based on the neuroscience-inspired\nperceptual straightening hypothesis, which steers the diffusion denoising\nprocess towards smoother temporal evolution by incorporating a curvature\npenalty in a perceptual space to improve temporal perceptual scores, such as\nFr\\'echet Video Distance (FVD) and perceptual straightness; and (2) Multi-Path\nEnsemble Sampling (MPES), which aims at reducing stochastic variation by\nensembling multiple diffusion trajectories to improve fidelity (distortion)\nscores, such as PSNR and SSIM, without sacrificing sharpness. Together, these\ntraining-free techniques provide a practical path toward temporally stable\nhigh-fidelity perceptual video restoration using large pretrained diffusion\nmodels. We performed extensive experiments over multiple datasets and\ndegradation types, systematically evaluating each strategy to understand their\nstrengths and limitations. Our results show that while PSG enhances temporal\nnaturalness, particularly in case of temporal blur, MPES consistently improves\nfidelity and spatio-temporal perception--distortion trade-off across all tasks.\n", "link": "http://arxiv.org/abs/2510.25420v1", "date": "2025-10-29", "relevancy": 1.9016, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6483}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6219}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Temporal%20Consistency%20and%20Fidelity%20at%20Inference-time%20in%0A%20%20Perceptual%20Video%20Restoration%20by%20Zero-shot%20Image-based%20Diffusion%20Models&body=Title%3A%20Improving%20Temporal%20Consistency%20and%20Fidelity%20at%20Inference-time%20in%0A%20%20Perceptual%20Video%20Restoration%20by%20Zero-shot%20Image-based%20Diffusion%20Models%0AAuthor%3A%20Nasrin%20Rahimi%20and%20A.%20Murat%20Tekalp%0AAbstract%3A%20%20%20Diffusion%20models%20have%20emerged%20as%20powerful%20priors%20for%20single-image%0Arestoration%2C%20but%20their%20application%20to%20zero-shot%20video%20restoration%20suffers%20from%0Atemporal%20inconsistencies%20due%20to%20the%20stochastic%20nature%20of%20sampling%20and%0Acomplexity%20of%20incorporating%20explicit%20temporal%20modeling.%20In%20this%20work%2C%20we%0Aaddress%20the%20challenge%20of%20improving%20temporal%20coherence%20in%20video%20restoration%0Ausing%20zero-shot%20image-based%20diffusion%20models%20without%20retraining%20or%20modifying%0Atheir%20architecture.%20We%20propose%20two%20complementary%20inference-time%20strategies%3A%20%281%29%0APerceptual%20Straightening%20Guidance%20%28PSG%29%20based%20on%20the%20neuroscience-inspired%0Aperceptual%20straightening%20hypothesis%2C%20which%20steers%20the%20diffusion%20denoising%0Aprocess%20towards%20smoother%20temporal%20evolution%20by%20incorporating%20a%20curvature%0Apenalty%20in%20a%20perceptual%20space%20to%20improve%20temporal%20perceptual%20scores%2C%20such%20as%0AFr%5C%27echet%20Video%20Distance%20%28FVD%29%20and%20perceptual%20straightness%3B%20and%20%282%29%20Multi-Path%0AEnsemble%20Sampling%20%28MPES%29%2C%20which%20aims%20at%20reducing%20stochastic%20variation%20by%0Aensembling%20multiple%20diffusion%20trajectories%20to%20improve%20fidelity%20%28distortion%29%0Ascores%2C%20such%20as%20PSNR%20and%20SSIM%2C%20without%20sacrificing%20sharpness.%20Together%2C%20these%0Atraining-free%20techniques%20provide%20a%20practical%20path%20toward%20temporally%20stable%0Ahigh-fidelity%20perceptual%20video%20restoration%20using%20large%20pretrained%20diffusion%0Amodels.%20We%20performed%20extensive%20experiments%20over%20multiple%20datasets%20and%0Adegradation%20types%2C%20systematically%20evaluating%20each%20strategy%20to%20understand%20their%0Astrengths%20and%20limitations.%20Our%20results%20show%20that%20while%20PSG%20enhances%20temporal%0Anaturalness%2C%20particularly%20in%20case%20of%20temporal%20blur%2C%20MPES%20consistently%20improves%0Afidelity%20and%20spatio-temporal%20perception--distortion%20trade-off%20across%20all%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Temporal%2520Consistency%2520and%2520Fidelity%2520at%2520Inference-time%2520in%250A%2520%2520Perceptual%2520Video%2520Restoration%2520by%2520Zero-shot%2520Image-based%2520Diffusion%2520Models%26entry.906535625%3DNasrin%2520Rahimi%2520and%2520A.%2520Murat%2520Tekalp%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520emerged%2520as%2520powerful%2520priors%2520for%2520single-image%250Arestoration%252C%2520but%2520their%2520application%2520to%2520zero-shot%2520video%2520restoration%2520suffers%2520from%250Atemporal%2520inconsistencies%2520due%2520to%2520the%2520stochastic%2520nature%2520of%2520sampling%2520and%250Acomplexity%2520of%2520incorporating%2520explicit%2520temporal%2520modeling.%2520In%2520this%2520work%252C%2520we%250Aaddress%2520the%2520challenge%2520of%2520improving%2520temporal%2520coherence%2520in%2520video%2520restoration%250Ausing%2520zero-shot%2520image-based%2520diffusion%2520models%2520without%2520retraining%2520or%2520modifying%250Atheir%2520architecture.%2520We%2520propose%2520two%2520complementary%2520inference-time%2520strategies%253A%2520%25281%2529%250APerceptual%2520Straightening%2520Guidance%2520%2528PSG%2529%2520based%2520on%2520the%2520neuroscience-inspired%250Aperceptual%2520straightening%2520hypothesis%252C%2520which%2520steers%2520the%2520diffusion%2520denoising%250Aprocess%2520towards%2520smoother%2520temporal%2520evolution%2520by%2520incorporating%2520a%2520curvature%250Apenalty%2520in%2520a%2520perceptual%2520space%2520to%2520improve%2520temporal%2520perceptual%2520scores%252C%2520such%2520as%250AFr%255C%2527echet%2520Video%2520Distance%2520%2528FVD%2529%2520and%2520perceptual%2520straightness%253B%2520and%2520%25282%2529%2520Multi-Path%250AEnsemble%2520Sampling%2520%2528MPES%2529%252C%2520which%2520aims%2520at%2520reducing%2520stochastic%2520variation%2520by%250Aensembling%2520multiple%2520diffusion%2520trajectories%2520to%2520improve%2520fidelity%2520%2528distortion%2529%250Ascores%252C%2520such%2520as%2520PSNR%2520and%2520SSIM%252C%2520without%2520sacrificing%2520sharpness.%2520Together%252C%2520these%250Atraining-free%2520techniques%2520provide%2520a%2520practical%2520path%2520toward%2520temporally%2520stable%250Ahigh-fidelity%2520perceptual%2520video%2520restoration%2520using%2520large%2520pretrained%2520diffusion%250Amodels.%2520We%2520performed%2520extensive%2520experiments%2520over%2520multiple%2520datasets%2520and%250Adegradation%2520types%252C%2520systematically%2520evaluating%2520each%2520strategy%2520to%2520understand%2520their%250Astrengths%2520and%2520limitations.%2520Our%2520results%2520show%2520that%2520while%2520PSG%2520enhances%2520temporal%250Anaturalness%252C%2520particularly%2520in%2520case%2520of%2520temporal%2520blur%252C%2520MPES%2520consistently%2520improves%250Afidelity%2520and%2520spatio-temporal%2520perception--distortion%2520trade-off%2520across%2520all%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Temporal%20Consistency%20and%20Fidelity%20at%20Inference-time%20in%0A%20%20Perceptual%20Video%20Restoration%20by%20Zero-shot%20Image-based%20Diffusion%20Models&entry.906535625=Nasrin%20Rahimi%20and%20A.%20Murat%20Tekalp&entry.1292438233=%20%20Diffusion%20models%20have%20emerged%20as%20powerful%20priors%20for%20single-image%0Arestoration%2C%20but%20their%20application%20to%20zero-shot%20video%20restoration%20suffers%20from%0Atemporal%20inconsistencies%20due%20to%20the%20stochastic%20nature%20of%20sampling%20and%0Acomplexity%20of%20incorporating%20explicit%20temporal%20modeling.%20In%20this%20work%2C%20we%0Aaddress%20the%20challenge%20of%20improving%20temporal%20coherence%20in%20video%20restoration%0Ausing%20zero-shot%20image-based%20diffusion%20models%20without%20retraining%20or%20modifying%0Atheir%20architecture.%20We%20propose%20two%20complementary%20inference-time%20strategies%3A%20%281%29%0APerceptual%20Straightening%20Guidance%20%28PSG%29%20based%20on%20the%20neuroscience-inspired%0Aperceptual%20straightening%20hypothesis%2C%20which%20steers%20the%20diffusion%20denoising%0Aprocess%20towards%20smoother%20temporal%20evolution%20by%20incorporating%20a%20curvature%0Apenalty%20in%20a%20perceptual%20space%20to%20improve%20temporal%20perceptual%20scores%2C%20such%20as%0AFr%5C%27echet%20Video%20Distance%20%28FVD%29%20and%20perceptual%20straightness%3B%20and%20%282%29%20Multi-Path%0AEnsemble%20Sampling%20%28MPES%29%2C%20which%20aims%20at%20reducing%20stochastic%20variation%20by%0Aensembling%20multiple%20diffusion%20trajectories%20to%20improve%20fidelity%20%28distortion%29%0Ascores%2C%20such%20as%20PSNR%20and%20SSIM%2C%20without%20sacrificing%20sharpness.%20Together%2C%20these%0Atraining-free%20techniques%20provide%20a%20practical%20path%20toward%20temporally%20stable%0Ahigh-fidelity%20perceptual%20video%20restoration%20using%20large%20pretrained%20diffusion%0Amodels.%20We%20performed%20extensive%20experiments%20over%20multiple%20datasets%20and%0Adegradation%20types%2C%20systematically%20evaluating%20each%20strategy%20to%20understand%20their%0Astrengths%20and%20limitations.%20Our%20results%20show%20that%20while%20PSG%20enhances%20temporal%0Anaturalness%2C%20particularly%20in%20case%20of%20temporal%20blur%2C%20MPES%20consistently%20improves%0Afidelity%20and%20spatio-temporal%20perception--distortion%20trade-off%20across%20all%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25420v1&entry.124074799=Read"},
{"title": "Geometric Robot Calibration Using a Calibration Plate", "author": "Bernhard Rameder and Hubert Gattringer and Andreas Mueller", "abstract": "  In this paper a new method for geometric robot calibration is introduced,\nwhich uses a calibration plate with precisely known distances between its\nmeasuring points. The relative measurement between two points on the\ncalibration plate is used to determine predefined error parameters of the\nsystem. In comparison to conventional measurement methods, like laser tracker\nor motion capture systems, the calibration plate provides a more mechanically\nrobust and cheaper alternative, which is furthermore easier to transport due to\nits small size. The calibration method, the plate design, the mathematical\ndescription of the error system as well as the identification of the parameters\nare described in detail. For identifying the error parameters, the least\nsquares method and a constrained optimization problem are used. The\nfunctionality of this method was demonstrated in experiments that led to\npromising results, correlated with one of a laser tracker calibration. The\nmodeling and identification of the error parameters is done for a gantry\nmachine, but is not restricted to that type of robot.\n", "link": "http://arxiv.org/abs/2510.25338v1", "date": "2025-10-29", "relevancy": 1.8999, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4765}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4759}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Robot%20Calibration%20Using%20a%20Calibration%20Plate&body=Title%3A%20Geometric%20Robot%20Calibration%20Using%20a%20Calibration%20Plate%0AAuthor%3A%20Bernhard%20Rameder%20and%20Hubert%20Gattringer%20and%20Andreas%20Mueller%0AAbstract%3A%20%20%20In%20this%20paper%20a%20new%20method%20for%20geometric%20robot%20calibration%20is%20introduced%2C%0Awhich%20uses%20a%20calibration%20plate%20with%20precisely%20known%20distances%20between%20its%0Ameasuring%20points.%20The%20relative%20measurement%20between%20two%20points%20on%20the%0Acalibration%20plate%20is%20used%20to%20determine%20predefined%20error%20parameters%20of%20the%0Asystem.%20In%20comparison%20to%20conventional%20measurement%20methods%2C%20like%20laser%20tracker%0Aor%20motion%20capture%20systems%2C%20the%20calibration%20plate%20provides%20a%20more%20mechanically%0Arobust%20and%20cheaper%20alternative%2C%20which%20is%20furthermore%20easier%20to%20transport%20due%20to%0Aits%20small%20size.%20The%20calibration%20method%2C%20the%20plate%20design%2C%20the%20mathematical%0Adescription%20of%20the%20error%20system%20as%20well%20as%20the%20identification%20of%20the%20parameters%0Aare%20described%20in%20detail.%20For%20identifying%20the%20error%20parameters%2C%20the%20least%0Asquares%20method%20and%20a%20constrained%20optimization%20problem%20are%20used.%20The%0Afunctionality%20of%20this%20method%20was%20demonstrated%20in%20experiments%20that%20led%20to%0Apromising%20results%2C%20correlated%20with%20one%20of%20a%20laser%20tracker%20calibration.%20The%0Amodeling%20and%20identification%20of%20the%20error%20parameters%20is%20done%20for%20a%20gantry%0Amachine%2C%20but%20is%20not%20restricted%20to%20that%20type%20of%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25338v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Robot%2520Calibration%2520Using%2520a%2520Calibration%2520Plate%26entry.906535625%3DBernhard%2520Rameder%2520and%2520Hubert%2520Gattringer%2520and%2520Andreas%2520Mueller%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520a%2520new%2520method%2520for%2520geometric%2520robot%2520calibration%2520is%2520introduced%252C%250Awhich%2520uses%2520a%2520calibration%2520plate%2520with%2520precisely%2520known%2520distances%2520between%2520its%250Ameasuring%2520points.%2520The%2520relative%2520measurement%2520between%2520two%2520points%2520on%2520the%250Acalibration%2520plate%2520is%2520used%2520to%2520determine%2520predefined%2520error%2520parameters%2520of%2520the%250Asystem.%2520In%2520comparison%2520to%2520conventional%2520measurement%2520methods%252C%2520like%2520laser%2520tracker%250Aor%2520motion%2520capture%2520systems%252C%2520the%2520calibration%2520plate%2520provides%2520a%2520more%2520mechanically%250Arobust%2520and%2520cheaper%2520alternative%252C%2520which%2520is%2520furthermore%2520easier%2520to%2520transport%2520due%2520to%250Aits%2520small%2520size.%2520The%2520calibration%2520method%252C%2520the%2520plate%2520design%252C%2520the%2520mathematical%250Adescription%2520of%2520the%2520error%2520system%2520as%2520well%2520as%2520the%2520identification%2520of%2520the%2520parameters%250Aare%2520described%2520in%2520detail.%2520For%2520identifying%2520the%2520error%2520parameters%252C%2520the%2520least%250Asquares%2520method%2520and%2520a%2520constrained%2520optimization%2520problem%2520are%2520used.%2520The%250Afunctionality%2520of%2520this%2520method%2520was%2520demonstrated%2520in%2520experiments%2520that%2520led%2520to%250Apromising%2520results%252C%2520correlated%2520with%2520one%2520of%2520a%2520laser%2520tracker%2520calibration.%2520The%250Amodeling%2520and%2520identification%2520of%2520the%2520error%2520parameters%2520is%2520done%2520for%2520a%2520gantry%250Amachine%252C%2520but%2520is%2520not%2520restricted%2520to%2520that%2520type%2520of%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25338v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Robot%20Calibration%20Using%20a%20Calibration%20Plate&entry.906535625=Bernhard%20Rameder%20and%20Hubert%20Gattringer%20and%20Andreas%20Mueller&entry.1292438233=%20%20In%20this%20paper%20a%20new%20method%20for%20geometric%20robot%20calibration%20is%20introduced%2C%0Awhich%20uses%20a%20calibration%20plate%20with%20precisely%20known%20distances%20between%20its%0Ameasuring%20points.%20The%20relative%20measurement%20between%20two%20points%20on%20the%0Acalibration%20plate%20is%20used%20to%20determine%20predefined%20error%20parameters%20of%20the%0Asystem.%20In%20comparison%20to%20conventional%20measurement%20methods%2C%20like%20laser%20tracker%0Aor%20motion%20capture%20systems%2C%20the%20calibration%20plate%20provides%20a%20more%20mechanically%0Arobust%20and%20cheaper%20alternative%2C%20which%20is%20furthermore%20easier%20to%20transport%20due%20to%0Aits%20small%20size.%20The%20calibration%20method%2C%20the%20plate%20design%2C%20the%20mathematical%0Adescription%20of%20the%20error%20system%20as%20well%20as%20the%20identification%20of%20the%20parameters%0Aare%20described%20in%20detail.%20For%20identifying%20the%20error%20parameters%2C%20the%20least%0Asquares%20method%20and%20a%20constrained%20optimization%20problem%20are%20used.%20The%0Afunctionality%20of%20this%20method%20was%20demonstrated%20in%20experiments%20that%20led%20to%0Apromising%20results%2C%20correlated%20with%20one%20of%20a%20laser%20tracker%20calibration.%20The%0Amodeling%20and%20identification%20of%20the%20error%20parameters%20is%20done%20for%20a%20gantry%0Amachine%2C%20but%20is%20not%20restricted%20to%20that%20type%20of%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25338v1&entry.124074799=Read"},
{"title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\n  Learning", "author": "Baolu Li and Yiming Zhang and Qinghe Wang and Liqian Ma and Xiaoyu Shi and Xintao Wang and Pengfei Wan and Zhenfei Yin and Yunzhi Zhuge and Huchuan Lu and Xu Jia", "abstract": "  Visual effects (VFX) are crucial to the expressive power of digital media,\nyet their creation remains a major challenge for generative AI. Prevailing\nmethods often rely on the one-LoRA-per-effect paradigm, which is\nresource-intensive and fundamentally incapable of generalizing to unseen\neffects, thus limiting scalability and creation. To address this challenge, we\nintroduce VFXMaster, the first unified, reference-based framework for VFX video\ngeneration. It recasts effect generation as an in-context learning task,\nenabling it to reproduce diverse dynamic effects from a reference video onto\ntarget content. In addition, it demonstrates remarkable generalization to\nunseen effect categories. Specifically, we design an in-context conditioning\nstrategy that prompts the model with a reference example. An in-context\nattention mask is designed to precisely decouple and inject the essential\neffect attributes, allowing a single unified model to master the effect\nimitation without information leakage. In addition, we propose an efficient\none-shot effect adaptation mechanism to boost generalization capability on\ntough unseen effects from a single user-provided video rapidly. Extensive\nexperiments demonstrate that our method effectively imitates various categories\nof effect information and exhibits outstanding generalization to out-of-domain\neffects. To foster future research, we will release our code, models, and a\ncomprehensive dataset to the community.\n", "link": "http://arxiv.org/abs/2510.25772v1", "date": "2025-10-29", "relevancy": 1.8943, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6428}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6275}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VFXMaster%3A%20Unlocking%20Dynamic%20Visual%20Effect%20Generation%20via%20In-Context%0A%20%20Learning&body=Title%3A%20VFXMaster%3A%20Unlocking%20Dynamic%20Visual%20Effect%20Generation%20via%20In-Context%0A%20%20Learning%0AAuthor%3A%20Baolu%20Li%20and%20Yiming%20Zhang%20and%20Qinghe%20Wang%20and%20Liqian%20Ma%20and%20Xiaoyu%20Shi%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Zhenfei%20Yin%20and%20Yunzhi%20Zhuge%20and%20Huchuan%20Lu%20and%20Xu%20Jia%0AAbstract%3A%20%20%20Visual%20effects%20%28VFX%29%20are%20crucial%20to%20the%20expressive%20power%20of%20digital%20media%2C%0Ayet%20their%20creation%20remains%20a%20major%20challenge%20for%20generative%20AI.%20Prevailing%0Amethods%20often%20rely%20on%20the%20one-LoRA-per-effect%20paradigm%2C%20which%20is%0Aresource-intensive%20and%20fundamentally%20incapable%20of%20generalizing%20to%20unseen%0Aeffects%2C%20thus%20limiting%20scalability%20and%20creation.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20VFXMaster%2C%20the%20first%20unified%2C%20reference-based%20framework%20for%20VFX%20video%0Ageneration.%20It%20recasts%20effect%20generation%20as%20an%20in-context%20learning%20task%2C%0Aenabling%20it%20to%20reproduce%20diverse%20dynamic%20effects%20from%20a%20reference%20video%20onto%0Atarget%20content.%20In%20addition%2C%20it%20demonstrates%20remarkable%20generalization%20to%0Aunseen%20effect%20categories.%20Specifically%2C%20we%20design%20an%20in-context%20conditioning%0Astrategy%20that%20prompts%20the%20model%20with%20a%20reference%20example.%20An%20in-context%0Aattention%20mask%20is%20designed%20to%20precisely%20decouple%20and%20inject%20the%20essential%0Aeffect%20attributes%2C%20allowing%20a%20single%20unified%20model%20to%20master%20the%20effect%0Aimitation%20without%20information%20leakage.%20In%20addition%2C%20we%20propose%20an%20efficient%0Aone-shot%20effect%20adaptation%20mechanism%20to%20boost%20generalization%20capability%20on%0Atough%20unseen%20effects%20from%20a%20single%20user-provided%20video%20rapidly.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20effectively%20imitates%20various%20categories%0Aof%20effect%20information%20and%20exhibits%20outstanding%20generalization%20to%20out-of-domain%0Aeffects.%20To%20foster%20future%20research%2C%20we%20will%20release%20our%20code%2C%20models%2C%20and%20a%0Acomprehensive%20dataset%20to%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVFXMaster%253A%2520Unlocking%2520Dynamic%2520Visual%2520Effect%2520Generation%2520via%2520In-Context%250A%2520%2520Learning%26entry.906535625%3DBaolu%2520Li%2520and%2520Yiming%2520Zhang%2520and%2520Qinghe%2520Wang%2520and%2520Liqian%2520Ma%2520and%2520Xiaoyu%2520Shi%2520and%2520Xintao%2520Wang%2520and%2520Pengfei%2520Wan%2520and%2520Zhenfei%2520Yin%2520and%2520Yunzhi%2520Zhuge%2520and%2520Huchuan%2520Lu%2520and%2520Xu%2520Jia%26entry.1292438233%3D%2520%2520Visual%2520effects%2520%2528VFX%2529%2520are%2520crucial%2520to%2520the%2520expressive%2520power%2520of%2520digital%2520media%252C%250Ayet%2520their%2520creation%2520remains%2520a%2520major%2520challenge%2520for%2520generative%2520AI.%2520Prevailing%250Amethods%2520often%2520rely%2520on%2520the%2520one-LoRA-per-effect%2520paradigm%252C%2520which%2520is%250Aresource-intensive%2520and%2520fundamentally%2520incapable%2520of%2520generalizing%2520to%2520unseen%250Aeffects%252C%2520thus%2520limiting%2520scalability%2520and%2520creation.%2520To%2520address%2520this%2520challenge%252C%2520we%250Aintroduce%2520VFXMaster%252C%2520the%2520first%2520unified%252C%2520reference-based%2520framework%2520for%2520VFX%2520video%250Ageneration.%2520It%2520recasts%2520effect%2520generation%2520as%2520an%2520in-context%2520learning%2520task%252C%250Aenabling%2520it%2520to%2520reproduce%2520diverse%2520dynamic%2520effects%2520from%2520a%2520reference%2520video%2520onto%250Atarget%2520content.%2520In%2520addition%252C%2520it%2520demonstrates%2520remarkable%2520generalization%2520to%250Aunseen%2520effect%2520categories.%2520Specifically%252C%2520we%2520design%2520an%2520in-context%2520conditioning%250Astrategy%2520that%2520prompts%2520the%2520model%2520with%2520a%2520reference%2520example.%2520An%2520in-context%250Aattention%2520mask%2520is%2520designed%2520to%2520precisely%2520decouple%2520and%2520inject%2520the%2520essential%250Aeffect%2520attributes%252C%2520allowing%2520a%2520single%2520unified%2520model%2520to%2520master%2520the%2520effect%250Aimitation%2520without%2520information%2520leakage.%2520In%2520addition%252C%2520we%2520propose%2520an%2520efficient%250Aone-shot%2520effect%2520adaptation%2520mechanism%2520to%2520boost%2520generalization%2520capability%2520on%250Atough%2520unseen%2520effects%2520from%2520a%2520single%2520user-provided%2520video%2520rapidly.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520effectively%2520imitates%2520various%2520categories%250Aof%2520effect%2520information%2520and%2520exhibits%2520outstanding%2520generalization%2520to%2520out-of-domain%250Aeffects.%2520To%2520foster%2520future%2520research%252C%2520we%2520will%2520release%2520our%2520code%252C%2520models%252C%2520and%2520a%250Acomprehensive%2520dataset%2520to%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VFXMaster%3A%20Unlocking%20Dynamic%20Visual%20Effect%20Generation%20via%20In-Context%0A%20%20Learning&entry.906535625=Baolu%20Li%20and%20Yiming%20Zhang%20and%20Qinghe%20Wang%20and%20Liqian%20Ma%20and%20Xiaoyu%20Shi%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Zhenfei%20Yin%20and%20Yunzhi%20Zhuge%20and%20Huchuan%20Lu%20and%20Xu%20Jia&entry.1292438233=%20%20Visual%20effects%20%28VFX%29%20are%20crucial%20to%20the%20expressive%20power%20of%20digital%20media%2C%0Ayet%20their%20creation%20remains%20a%20major%20challenge%20for%20generative%20AI.%20Prevailing%0Amethods%20often%20rely%20on%20the%20one-LoRA-per-effect%20paradigm%2C%20which%20is%0Aresource-intensive%20and%20fundamentally%20incapable%20of%20generalizing%20to%20unseen%0Aeffects%2C%20thus%20limiting%20scalability%20and%20creation.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20VFXMaster%2C%20the%20first%20unified%2C%20reference-based%20framework%20for%20VFX%20video%0Ageneration.%20It%20recasts%20effect%20generation%20as%20an%20in-context%20learning%20task%2C%0Aenabling%20it%20to%20reproduce%20diverse%20dynamic%20effects%20from%20a%20reference%20video%20onto%0Atarget%20content.%20In%20addition%2C%20it%20demonstrates%20remarkable%20generalization%20to%0Aunseen%20effect%20categories.%20Specifically%2C%20we%20design%20an%20in-context%20conditioning%0Astrategy%20that%20prompts%20the%20model%20with%20a%20reference%20example.%20An%20in-context%0Aattention%20mask%20is%20designed%20to%20precisely%20decouple%20and%20inject%20the%20essential%0Aeffect%20attributes%2C%20allowing%20a%20single%20unified%20model%20to%20master%20the%20effect%0Aimitation%20without%20information%20leakage.%20In%20addition%2C%20we%20propose%20an%20efficient%0Aone-shot%20effect%20adaptation%20mechanism%20to%20boost%20generalization%20capability%20on%0Atough%20unseen%20effects%20from%20a%20single%20user-provided%20video%20rapidly.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20effectively%20imitates%20various%20categories%0Aof%20effect%20information%20and%20exhibits%20outstanding%20generalization%20to%20out-of-domain%0Aeffects.%20To%20foster%20future%20research%2C%20we%20will%20release%20our%20code%2C%20models%2C%20and%20a%0Acomprehensive%20dataset%20to%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25772v1&entry.124074799=Read"},
{"title": "Gradient-Weight Alignment as a Train-Time Proxy for Generalization in\n  Classification Tasks", "author": "Florian A. H\u00f6lzl and Daniel Rueckert and Georgios Kaissis", "abstract": "  Robust validation metrics remain essential in contemporary deep learning, not\nonly to detect overfitting and poor generalization, but also to monitor\ntraining dynamics. In the supervised classification setting, we investigate\nwhether interactions between training data and model weights can yield such a\nmetric that both tracks generalization during training and attributes\nperformance to individual training samples. We introduce Gradient-Weight\nAlignment (GWA), quantifying the coherence between per-sample gradients and\nmodel weights. We show that effective learning corresponds to coherent\nalignment, while misalignment indicates deteriorating generalization. GWA is\nefficiently computable during training and reflects both sample-specific\ncontributions and dataset-wide learning dynamics. Extensive experiments show\nthat GWA accurately predicts optimal early stopping, enables principled model\ncomparisons, and identifies influential training samples, providing a\nvalidation-set-free approach for model analysis directly from the training\ndata.\n", "link": "http://arxiv.org/abs/2510.25480v1", "date": "2025-10-29", "relevancy": 1.8882, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4772}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4696}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-Weight%20Alignment%20as%20a%20Train-Time%20Proxy%20for%20Generalization%20in%0A%20%20Classification%20Tasks&body=Title%3A%20Gradient-Weight%20Alignment%20as%20a%20Train-Time%20Proxy%20for%20Generalization%20in%0A%20%20Classification%20Tasks%0AAuthor%3A%20Florian%20A.%20H%C3%B6lzl%20and%20Daniel%20Rueckert%20and%20Georgios%20Kaissis%0AAbstract%3A%20%20%20Robust%20validation%20metrics%20remain%20essential%20in%20contemporary%20deep%20learning%2C%20not%0Aonly%20to%20detect%20overfitting%20and%20poor%20generalization%2C%20but%20also%20to%20monitor%0Atraining%20dynamics.%20In%20the%20supervised%20classification%20setting%2C%20we%20investigate%0Awhether%20interactions%20between%20training%20data%20and%20model%20weights%20can%20yield%20such%20a%0Ametric%20that%20both%20tracks%20generalization%20during%20training%20and%20attributes%0Aperformance%20to%20individual%20training%20samples.%20We%20introduce%20Gradient-Weight%0AAlignment%20%28GWA%29%2C%20quantifying%20the%20coherence%20between%20per-sample%20gradients%20and%0Amodel%20weights.%20We%20show%20that%20effective%20learning%20corresponds%20to%20coherent%0Aalignment%2C%20while%20misalignment%20indicates%20deteriorating%20generalization.%20GWA%20is%0Aefficiently%20computable%20during%20training%20and%20reflects%20both%20sample-specific%0Acontributions%20and%20dataset-wide%20learning%20dynamics.%20Extensive%20experiments%20show%0Athat%20GWA%20accurately%20predicts%20optimal%20early%20stopping%2C%20enables%20principled%20model%0Acomparisons%2C%20and%20identifies%20influential%20training%20samples%2C%20providing%20a%0Avalidation-set-free%20approach%20for%20model%20analysis%20directly%20from%20the%20training%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-Weight%2520Alignment%2520as%2520a%2520Train-Time%2520Proxy%2520for%2520Generalization%2520in%250A%2520%2520Classification%2520Tasks%26entry.906535625%3DFlorian%2520A.%2520H%25C3%25B6lzl%2520and%2520Daniel%2520Rueckert%2520and%2520Georgios%2520Kaissis%26entry.1292438233%3D%2520%2520Robust%2520validation%2520metrics%2520remain%2520essential%2520in%2520contemporary%2520deep%2520learning%252C%2520not%250Aonly%2520to%2520detect%2520overfitting%2520and%2520poor%2520generalization%252C%2520but%2520also%2520to%2520monitor%250Atraining%2520dynamics.%2520In%2520the%2520supervised%2520classification%2520setting%252C%2520we%2520investigate%250Awhether%2520interactions%2520between%2520training%2520data%2520and%2520model%2520weights%2520can%2520yield%2520such%2520a%250Ametric%2520that%2520both%2520tracks%2520generalization%2520during%2520training%2520and%2520attributes%250Aperformance%2520to%2520individual%2520training%2520samples.%2520We%2520introduce%2520Gradient-Weight%250AAlignment%2520%2528GWA%2529%252C%2520quantifying%2520the%2520coherence%2520between%2520per-sample%2520gradients%2520and%250Amodel%2520weights.%2520We%2520show%2520that%2520effective%2520learning%2520corresponds%2520to%2520coherent%250Aalignment%252C%2520while%2520misalignment%2520indicates%2520deteriorating%2520generalization.%2520GWA%2520is%250Aefficiently%2520computable%2520during%2520training%2520and%2520reflects%2520both%2520sample-specific%250Acontributions%2520and%2520dataset-wide%2520learning%2520dynamics.%2520Extensive%2520experiments%2520show%250Athat%2520GWA%2520accurately%2520predicts%2520optimal%2520early%2520stopping%252C%2520enables%2520principled%2520model%250Acomparisons%252C%2520and%2520identifies%2520influential%2520training%2520samples%252C%2520providing%2520a%250Avalidation-set-free%2520approach%2520for%2520model%2520analysis%2520directly%2520from%2520the%2520training%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Weight%20Alignment%20as%20a%20Train-Time%20Proxy%20for%20Generalization%20in%0A%20%20Classification%20Tasks&entry.906535625=Florian%20A.%20H%C3%B6lzl%20and%20Daniel%20Rueckert%20and%20Georgios%20Kaissis&entry.1292438233=%20%20Robust%20validation%20metrics%20remain%20essential%20in%20contemporary%20deep%20learning%2C%20not%0Aonly%20to%20detect%20overfitting%20and%20poor%20generalization%2C%20but%20also%20to%20monitor%0Atraining%20dynamics.%20In%20the%20supervised%20classification%20setting%2C%20we%20investigate%0Awhether%20interactions%20between%20training%20data%20and%20model%20weights%20can%20yield%20such%20a%0Ametric%20that%20both%20tracks%20generalization%20during%20training%20and%20attributes%0Aperformance%20to%20individual%20training%20samples.%20We%20introduce%20Gradient-Weight%0AAlignment%20%28GWA%29%2C%20quantifying%20the%20coherence%20between%20per-sample%20gradients%20and%0Amodel%20weights.%20We%20show%20that%20effective%20learning%20corresponds%20to%20coherent%0Aalignment%2C%20while%20misalignment%20indicates%20deteriorating%20generalization.%20GWA%20is%0Aefficiently%20computable%20during%20training%20and%20reflects%20both%20sample-specific%0Acontributions%20and%20dataset-wide%20learning%20dynamics.%20Extensive%20experiments%20show%0Athat%20GWA%20accurately%20predicts%20optimal%20early%20stopping%2C%20enables%20principled%20model%0Acomparisons%2C%20and%20identifies%20influential%20training%20samples%2C%20providing%20a%0Avalidation-set-free%20approach%20for%20model%20analysis%20directly%20from%20the%20training%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25480v1&entry.124074799=Read"},
{"title": "Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and\n  Evaluation", "author": "Thomas Cook and Richard Osuagwu and Liman Tsatiashvili and Vrynsia Vrynsia and Koustav Ghosal and Maraim Masoud and Riccardo Mattivi", "abstract": "  Retrieval-Augmented Generation (RAG) systems often face limitations in\nspecialized domains such as fintech, where domain-specific ontologies, dense\nterminology, and acronyms complicate effective retrieval and synthesis. This\npaper introduces an agentic RAG architecture designed to address these\nchallenges through a modular pipeline of specialized agents. The proposed\nsystem supports intelligent query reformulation, iterative sub-query\ndecomposition guided by keyphrase extraction, contextual acronym resolution,\nand cross-encoder-based context re-ranking. We evaluate our approach against a\nstandard RAG baseline using a curated dataset of 85 question--answer--reference\ntriples derived from an enterprise fintech knowledge base. Experimental results\ndemonstrate that the agentic RAG system outperforms the baseline in retrieval\nprecision and relevance, albeit with increased latency. These findings suggest\nthat structured, multi-agent methodologies offer a promising direction for\nenhancing retrieval robustness in complex, domain-specific settings.\n", "link": "http://arxiv.org/abs/2510.25518v1", "date": "2025-10-29", "relevancy": 1.8871, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5183}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4655}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval%20Augmented%20Generation%20%28RAG%29%20for%20Fintech%3A%20Agentic%20Design%20and%0A%20%20Evaluation&body=Title%3A%20Retrieval%20Augmented%20Generation%20%28RAG%29%20for%20Fintech%3A%20Agentic%20Design%20and%0A%20%20Evaluation%0AAuthor%3A%20Thomas%20Cook%20and%20Richard%20Osuagwu%20and%20Liman%20Tsatiashvili%20and%20Vrynsia%20Vrynsia%20and%20Koustav%20Ghosal%20and%20Maraim%20Masoud%20and%20Riccardo%20Mattivi%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems%20often%20face%20limitations%20in%0Aspecialized%20domains%20such%20as%20fintech%2C%20where%20domain-specific%20ontologies%2C%20dense%0Aterminology%2C%20and%20acronyms%20complicate%20effective%20retrieval%20and%20synthesis.%20This%0Apaper%20introduces%20an%20agentic%20RAG%20architecture%20designed%20to%20address%20these%0Achallenges%20through%20a%20modular%20pipeline%20of%20specialized%20agents.%20The%20proposed%0Asystem%20supports%20intelligent%20query%20reformulation%2C%20iterative%20sub-query%0Adecomposition%20guided%20by%20keyphrase%20extraction%2C%20contextual%20acronym%20resolution%2C%0Aand%20cross-encoder-based%20context%20re-ranking.%20We%20evaluate%20our%20approach%20against%20a%0Astandard%20RAG%20baseline%20using%20a%20curated%20dataset%20of%2085%20question--answer--reference%0Atriples%20derived%20from%20an%20enterprise%20fintech%20knowledge%20base.%20Experimental%20results%0Ademonstrate%20that%20the%20agentic%20RAG%20system%20outperforms%20the%20baseline%20in%20retrieval%0Aprecision%20and%20relevance%2C%20albeit%20with%20increased%20latency.%20These%20findings%20suggest%0Athat%20structured%2C%20multi-agent%20methodologies%20offer%20a%20promising%20direction%20for%0Aenhancing%20retrieval%20robustness%20in%20complex%2C%20domain-specific%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520for%2520Fintech%253A%2520Agentic%2520Design%2520and%250A%2520%2520Evaluation%26entry.906535625%3DThomas%2520Cook%2520and%2520Richard%2520Osuagwu%2520and%2520Liman%2520Tsatiashvili%2520and%2520Vrynsia%2520Vrynsia%2520and%2520Koustav%2520Ghosal%2520and%2520Maraim%2520Masoud%2520and%2520Riccardo%2520Mattivi%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520systems%2520often%2520face%2520limitations%2520in%250Aspecialized%2520domains%2520such%2520as%2520fintech%252C%2520where%2520domain-specific%2520ontologies%252C%2520dense%250Aterminology%252C%2520and%2520acronyms%2520complicate%2520effective%2520retrieval%2520and%2520synthesis.%2520This%250Apaper%2520introduces%2520an%2520agentic%2520RAG%2520architecture%2520designed%2520to%2520address%2520these%250Achallenges%2520through%2520a%2520modular%2520pipeline%2520of%2520specialized%2520agents.%2520The%2520proposed%250Asystem%2520supports%2520intelligent%2520query%2520reformulation%252C%2520iterative%2520sub-query%250Adecomposition%2520guided%2520by%2520keyphrase%2520extraction%252C%2520contextual%2520acronym%2520resolution%252C%250Aand%2520cross-encoder-based%2520context%2520re-ranking.%2520We%2520evaluate%2520our%2520approach%2520against%2520a%250Astandard%2520RAG%2520baseline%2520using%2520a%2520curated%2520dataset%2520of%252085%2520question--answer--reference%250Atriples%2520derived%2520from%2520an%2520enterprise%2520fintech%2520knowledge%2520base.%2520Experimental%2520results%250Ademonstrate%2520that%2520the%2520agentic%2520RAG%2520system%2520outperforms%2520the%2520baseline%2520in%2520retrieval%250Aprecision%2520and%2520relevance%252C%2520albeit%2520with%2520increased%2520latency.%2520These%2520findings%2520suggest%250Athat%2520structured%252C%2520multi-agent%2520methodologies%2520offer%2520a%2520promising%2520direction%2520for%250Aenhancing%2520retrieval%2520robustness%2520in%2520complex%252C%2520domain-specific%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval%20Augmented%20Generation%20%28RAG%29%20for%20Fintech%3A%20Agentic%20Design%20and%0A%20%20Evaluation&entry.906535625=Thomas%20Cook%20and%20Richard%20Osuagwu%20and%20Liman%20Tsatiashvili%20and%20Vrynsia%20Vrynsia%20and%20Koustav%20Ghosal%20and%20Maraim%20Masoud%20and%20Riccardo%20Mattivi&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems%20often%20face%20limitations%20in%0Aspecialized%20domains%20such%20as%20fintech%2C%20where%20domain-specific%20ontologies%2C%20dense%0Aterminology%2C%20and%20acronyms%20complicate%20effective%20retrieval%20and%20synthesis.%20This%0Apaper%20introduces%20an%20agentic%20RAG%20architecture%20designed%20to%20address%20these%0Achallenges%20through%20a%20modular%20pipeline%20of%20specialized%20agents.%20The%20proposed%0Asystem%20supports%20intelligent%20query%20reformulation%2C%20iterative%20sub-query%0Adecomposition%20guided%20by%20keyphrase%20extraction%2C%20contextual%20acronym%20resolution%2C%0Aand%20cross-encoder-based%20context%20re-ranking.%20We%20evaluate%20our%20approach%20against%20a%0Astandard%20RAG%20baseline%20using%20a%20curated%20dataset%20of%2085%20question--answer--reference%0Atriples%20derived%20from%20an%20enterprise%20fintech%20knowledge%20base.%20Experimental%20results%0Ademonstrate%20that%20the%20agentic%20RAG%20system%20outperforms%20the%20baseline%20in%20retrieval%0Aprecision%20and%20relevance%2C%20albeit%20with%20increased%20latency.%20These%20findings%20suggest%0Athat%20structured%2C%20multi-agent%20methodologies%20offer%20a%20promising%20direction%20for%0Aenhancing%20retrieval%20robustness%20in%20complex%2C%20domain-specific%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25518v1&entry.124074799=Read"},
{"title": "Many LLMs Are More Utilitarian Than One", "author": "Anita Keshmirian and Razan Baltaji and Babak Hemmatian and Hadi Asghari and Lav R. Varshney", "abstract": "  Moral judgment is integral to large language models' (LLMs) social reasoning.\nAs multi-agent systems gain prominence, it becomes crucial to understand how\nLLMs function when collaborating compared to operating as individual agents. In\nhuman moral judgment, group deliberation leads to a Utilitarian Boost: a\ntendency to endorse norm violations that inflict harm but maximize benefits for\nthe greatest number of people. We study whether a similar dynamic emerges in\nmulti-agent LLM systems. We test six models on well-established sets of moral\ndilemmas across two conditions: (1) Solo, where models reason independently,\nand (2) Group, where they engage in multi-turn discussions in pairs or triads.\nIn personal dilemmas, where agents decide whether to directly harm an\nindividual for the benefit of others, all models rated moral violations as more\nacceptable when part of a group, demonstrating a Utilitarian Boost similar to\nthat observed in humans. However, the mechanism for the Boost in LLMs differed:\nWhile humans in groups become more utilitarian due to heightened sensitivity to\ndecision outcomes, LLM groups showed either reduced sensitivity to norms or\nenhanced impartiality. We report model differences in when and how strongly the\nBoost manifests. We also discuss prompt and agent compositions that enhance or\nmitigate the effect. We end with a discussion of the implications for AI\nalignment, multi-agent design, and artificial moral reasoning. Code available\nat: https://github.com/baltaci-r/MoralAgents\n", "link": "http://arxiv.org/abs/2507.00814v2", "date": "2025-10-29", "relevancy": 1.8847, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4913}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Many%20LLMs%20Are%20More%20Utilitarian%20Than%20One&body=Title%3A%20Many%20LLMs%20Are%20More%20Utilitarian%20Than%20One%0AAuthor%3A%20Anita%20Keshmirian%20and%20Razan%20Baltaji%20and%20Babak%20Hemmatian%20and%20Hadi%20Asghari%20and%20Lav%20R.%20Varshney%0AAbstract%3A%20%20%20Moral%20judgment%20is%20integral%20to%20large%20language%20models%27%20%28LLMs%29%20social%20reasoning.%0AAs%20multi-agent%20systems%20gain%20prominence%2C%20it%20becomes%20crucial%20to%20understand%20how%0ALLMs%20function%20when%20collaborating%20compared%20to%20operating%20as%20individual%20agents.%20In%0Ahuman%20moral%20judgment%2C%20group%20deliberation%20leads%20to%20a%20Utilitarian%20Boost%3A%20a%0Atendency%20to%20endorse%20norm%20violations%20that%20inflict%20harm%20but%20maximize%20benefits%20for%0Athe%20greatest%20number%20of%20people.%20We%20study%20whether%20a%20similar%20dynamic%20emerges%20in%0Amulti-agent%20LLM%20systems.%20We%20test%20six%20models%20on%20well-established%20sets%20of%20moral%0Adilemmas%20across%20two%20conditions%3A%20%281%29%20Solo%2C%20where%20models%20reason%20independently%2C%0Aand%20%282%29%20Group%2C%20where%20they%20engage%20in%20multi-turn%20discussions%20in%20pairs%20or%20triads.%0AIn%20personal%20dilemmas%2C%20where%20agents%20decide%20whether%20to%20directly%20harm%20an%0Aindividual%20for%20the%20benefit%20of%20others%2C%20all%20models%20rated%20moral%20violations%20as%20more%0Aacceptable%20when%20part%20of%20a%20group%2C%20demonstrating%20a%20Utilitarian%20Boost%20similar%20to%0Athat%20observed%20in%20humans.%20However%2C%20the%20mechanism%20for%20the%20Boost%20in%20LLMs%20differed%3A%0AWhile%20humans%20in%20groups%20become%20more%20utilitarian%20due%20to%20heightened%20sensitivity%20to%0Adecision%20outcomes%2C%20LLM%20groups%20showed%20either%20reduced%20sensitivity%20to%20norms%20or%0Aenhanced%20impartiality.%20We%20report%20model%20differences%20in%20when%20and%20how%20strongly%20the%0ABoost%20manifests.%20We%20also%20discuss%20prompt%20and%20agent%20compositions%20that%20enhance%20or%0Amitigate%20the%20effect.%20We%20end%20with%20a%20discussion%20of%20the%20implications%20for%20AI%0Aalignment%2C%20multi-agent%20design%2C%20and%20artificial%20moral%20reasoning.%20Code%20available%0Aat%3A%20https%3A//github.com/baltaci-r/MoralAgents%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMany%2520LLMs%2520Are%2520More%2520Utilitarian%2520Than%2520One%26entry.906535625%3DAnita%2520Keshmirian%2520and%2520Razan%2520Baltaji%2520and%2520Babak%2520Hemmatian%2520and%2520Hadi%2520Asghari%2520and%2520Lav%2520R.%2520Varshney%26entry.1292438233%3D%2520%2520Moral%2520judgment%2520is%2520integral%2520to%2520large%2520language%2520models%2527%2520%2528LLMs%2529%2520social%2520reasoning.%250AAs%2520multi-agent%2520systems%2520gain%2520prominence%252C%2520it%2520becomes%2520crucial%2520to%2520understand%2520how%250ALLMs%2520function%2520when%2520collaborating%2520compared%2520to%2520operating%2520as%2520individual%2520agents.%2520In%250Ahuman%2520moral%2520judgment%252C%2520group%2520deliberation%2520leads%2520to%2520a%2520Utilitarian%2520Boost%253A%2520a%250Atendency%2520to%2520endorse%2520norm%2520violations%2520that%2520inflict%2520harm%2520but%2520maximize%2520benefits%2520for%250Athe%2520greatest%2520number%2520of%2520people.%2520We%2520study%2520whether%2520a%2520similar%2520dynamic%2520emerges%2520in%250Amulti-agent%2520LLM%2520systems.%2520We%2520test%2520six%2520models%2520on%2520well-established%2520sets%2520of%2520moral%250Adilemmas%2520across%2520two%2520conditions%253A%2520%25281%2529%2520Solo%252C%2520where%2520models%2520reason%2520independently%252C%250Aand%2520%25282%2529%2520Group%252C%2520where%2520they%2520engage%2520in%2520multi-turn%2520discussions%2520in%2520pairs%2520or%2520triads.%250AIn%2520personal%2520dilemmas%252C%2520where%2520agents%2520decide%2520whether%2520to%2520directly%2520harm%2520an%250Aindividual%2520for%2520the%2520benefit%2520of%2520others%252C%2520all%2520models%2520rated%2520moral%2520violations%2520as%2520more%250Aacceptable%2520when%2520part%2520of%2520a%2520group%252C%2520demonstrating%2520a%2520Utilitarian%2520Boost%2520similar%2520to%250Athat%2520observed%2520in%2520humans.%2520However%252C%2520the%2520mechanism%2520for%2520the%2520Boost%2520in%2520LLMs%2520differed%253A%250AWhile%2520humans%2520in%2520groups%2520become%2520more%2520utilitarian%2520due%2520to%2520heightened%2520sensitivity%2520to%250Adecision%2520outcomes%252C%2520LLM%2520groups%2520showed%2520either%2520reduced%2520sensitivity%2520to%2520norms%2520or%250Aenhanced%2520impartiality.%2520We%2520report%2520model%2520differences%2520in%2520when%2520and%2520how%2520strongly%2520the%250ABoost%2520manifests.%2520We%2520also%2520discuss%2520prompt%2520and%2520agent%2520compositions%2520that%2520enhance%2520or%250Amitigate%2520the%2520effect.%2520We%2520end%2520with%2520a%2520discussion%2520of%2520the%2520implications%2520for%2520AI%250Aalignment%252C%2520multi-agent%2520design%252C%2520and%2520artificial%2520moral%2520reasoning.%2520Code%2520available%250Aat%253A%2520https%253A//github.com/baltaci-r/MoralAgents%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Many%20LLMs%20Are%20More%20Utilitarian%20Than%20One&entry.906535625=Anita%20Keshmirian%20and%20Razan%20Baltaji%20and%20Babak%20Hemmatian%20and%20Hadi%20Asghari%20and%20Lav%20R.%20Varshney&entry.1292438233=%20%20Moral%20judgment%20is%20integral%20to%20large%20language%20models%27%20%28LLMs%29%20social%20reasoning.%0AAs%20multi-agent%20systems%20gain%20prominence%2C%20it%20becomes%20crucial%20to%20understand%20how%0ALLMs%20function%20when%20collaborating%20compared%20to%20operating%20as%20individual%20agents.%20In%0Ahuman%20moral%20judgment%2C%20group%20deliberation%20leads%20to%20a%20Utilitarian%20Boost%3A%20a%0Atendency%20to%20endorse%20norm%20violations%20that%20inflict%20harm%20but%20maximize%20benefits%20for%0Athe%20greatest%20number%20of%20people.%20We%20study%20whether%20a%20similar%20dynamic%20emerges%20in%0Amulti-agent%20LLM%20systems.%20We%20test%20six%20models%20on%20well-established%20sets%20of%20moral%0Adilemmas%20across%20two%20conditions%3A%20%281%29%20Solo%2C%20where%20models%20reason%20independently%2C%0Aand%20%282%29%20Group%2C%20where%20they%20engage%20in%20multi-turn%20discussions%20in%20pairs%20or%20triads.%0AIn%20personal%20dilemmas%2C%20where%20agents%20decide%20whether%20to%20directly%20harm%20an%0Aindividual%20for%20the%20benefit%20of%20others%2C%20all%20models%20rated%20moral%20violations%20as%20more%0Aacceptable%20when%20part%20of%20a%20group%2C%20demonstrating%20a%20Utilitarian%20Boost%20similar%20to%0Athat%20observed%20in%20humans.%20However%2C%20the%20mechanism%20for%20the%20Boost%20in%20LLMs%20differed%3A%0AWhile%20humans%20in%20groups%20become%20more%20utilitarian%20due%20to%20heightened%20sensitivity%20to%0Adecision%20outcomes%2C%20LLM%20groups%20showed%20either%20reduced%20sensitivity%20to%20norms%20or%0Aenhanced%20impartiality.%20We%20report%20model%20differences%20in%20when%20and%20how%20strongly%20the%0ABoost%20manifests.%20We%20also%20discuss%20prompt%20and%20agent%20compositions%20that%20enhance%20or%0Amitigate%20the%20effect.%20We%20end%20with%20a%20discussion%20of%20the%20implications%20for%20AI%0Aalignment%2C%20multi-agent%20design%2C%20and%20artificial%20moral%20reasoning.%20Code%20available%0Aat%3A%20https%3A//github.com/baltaci-r/MoralAgents%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00814v2&entry.124074799=Read"},
{"title": "Exact Sequence Interpolation with Transformers", "author": "Albert Alcalde and Giovanni Fantuzzi and Enrique Zuazua", "abstract": "  We prove that transformers can exactly interpolate datasets of finite input\nsequences in $\\mathbb{R}^d$, $d\\geq 2$, with corresponding output sequences of\nsmaller or equal length. Specifically, given $N$ sequences of arbitrary but\nfinite lengths in $\\mathbb{R}^d$ and output sequences of lengths $m^1, \\dots,\nm^N \\in \\mathbb{N}$, we construct a transformer with $\\mathcal{O}(\\sum_{j=1}^N\nm^j)$ blocks and $\\mathcal{O}(d \\sum_{j=1}^N m^j)$ parameters that exactly\ninterpolates the dataset. Our construction provides complexity estimates that\nare independent of the input sequence length, by alternating feed-forward and\nself-attention layers and by capitalizing on the clustering effect inherent to\nthe latter. Our novel constructive method also uses low-rank parameter matrices\nin the self-attention mechanism, a common feature of practical transformer\nimplementations. These results are first established in the hardmax\nself-attention setting, where the geometric structure permits an explicit and\nquantitative analysis, and are then extended to the softmax setting. Finally,\nwe demonstrate the applicability of our exact interpolation construction to\nlearning problems, in particular by providing convergence guarantees to a\nglobal minimizer under regularized training strategies. Our analysis\ncontributes to the theoretical understanding of transformer models, offering an\nexplanation for their excellent performance in exact sequence-to-sequence\ninterpolation tasks.\n", "link": "http://arxiv.org/abs/2502.02270v2", "date": "2025-10-29", "relevancy": 1.8799, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5413}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4709}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exact%20Sequence%20Interpolation%20with%20Transformers&body=Title%3A%20Exact%20Sequence%20Interpolation%20with%20Transformers%0AAuthor%3A%20Albert%20Alcalde%20and%20Giovanni%20Fantuzzi%20and%20Enrique%20Zuazua%0AAbstract%3A%20%20%20We%20prove%20that%20transformers%20can%20exactly%20interpolate%20datasets%20of%20finite%20input%0Asequences%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20%24d%5Cgeq%202%24%2C%20with%20corresponding%20output%20sequences%20of%0Asmaller%20or%20equal%20length.%20Specifically%2C%20given%20%24N%24%20sequences%20of%20arbitrary%20but%0Afinite%20lengths%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%20and%20output%20sequences%20of%20lengths%20%24m%5E1%2C%20%5Cdots%2C%0Am%5EN%20%5Cin%20%5Cmathbb%7BN%7D%24%2C%20we%20construct%20a%20transformer%20with%20%24%5Cmathcal%7BO%7D%28%5Csum_%7Bj%3D1%7D%5EN%0Am%5Ej%29%24%20blocks%20and%20%24%5Cmathcal%7BO%7D%28d%20%5Csum_%7Bj%3D1%7D%5EN%20m%5Ej%29%24%20parameters%20that%20exactly%0Ainterpolates%20the%20dataset.%20Our%20construction%20provides%20complexity%20estimates%20that%0Aare%20independent%20of%20the%20input%20sequence%20length%2C%20by%20alternating%20feed-forward%20and%0Aself-attention%20layers%20and%20by%20capitalizing%20on%20the%20clustering%20effect%20inherent%20to%0Athe%20latter.%20Our%20novel%20constructive%20method%20also%20uses%20low-rank%20parameter%20matrices%0Ain%20the%20self-attention%20mechanism%2C%20a%20common%20feature%20of%20practical%20transformer%0Aimplementations.%20These%20results%20are%20first%20established%20in%20the%20hardmax%0Aself-attention%20setting%2C%20where%20the%20geometric%20structure%20permits%20an%20explicit%20and%0Aquantitative%20analysis%2C%20and%20are%20then%20extended%20to%20the%20softmax%20setting.%20Finally%2C%0Awe%20demonstrate%20the%20applicability%20of%20our%20exact%20interpolation%20construction%20to%0Alearning%20problems%2C%20in%20particular%20by%20providing%20convergence%20guarantees%20to%20a%0Aglobal%20minimizer%20under%20regularized%20training%20strategies.%20Our%20analysis%0Acontributes%20to%20the%20theoretical%20understanding%20of%20transformer%20models%2C%20offering%20an%0Aexplanation%20for%20their%20excellent%20performance%20in%20exact%20sequence-to-sequence%0Ainterpolation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02270v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExact%2520Sequence%2520Interpolation%2520with%2520Transformers%26entry.906535625%3DAlbert%2520Alcalde%2520and%2520Giovanni%2520Fantuzzi%2520and%2520Enrique%2520Zuazua%26entry.1292438233%3D%2520%2520We%2520prove%2520that%2520transformers%2520can%2520exactly%2520interpolate%2520datasets%2520of%2520finite%2520input%250Asequences%2520in%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520%2524d%255Cgeq%25202%2524%252C%2520with%2520corresponding%2520output%2520sequences%2520of%250Asmaller%2520or%2520equal%2520length.%2520Specifically%252C%2520given%2520%2524N%2524%2520sequences%2520of%2520arbitrary%2520but%250Afinite%2520lengths%2520in%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%2520and%2520output%2520sequences%2520of%2520lengths%2520%2524m%255E1%252C%2520%255Cdots%252C%250Am%255EN%2520%255Cin%2520%255Cmathbb%257BN%257D%2524%252C%2520we%2520construct%2520a%2520transformer%2520with%2520%2524%255Cmathcal%257BO%257D%2528%255Csum_%257Bj%253D1%257D%255EN%250Am%255Ej%2529%2524%2520blocks%2520and%2520%2524%255Cmathcal%257BO%257D%2528d%2520%255Csum_%257Bj%253D1%257D%255EN%2520m%255Ej%2529%2524%2520parameters%2520that%2520exactly%250Ainterpolates%2520the%2520dataset.%2520Our%2520construction%2520provides%2520complexity%2520estimates%2520that%250Aare%2520independent%2520of%2520the%2520input%2520sequence%2520length%252C%2520by%2520alternating%2520feed-forward%2520and%250Aself-attention%2520layers%2520and%2520by%2520capitalizing%2520on%2520the%2520clustering%2520effect%2520inherent%2520to%250Athe%2520latter.%2520Our%2520novel%2520constructive%2520method%2520also%2520uses%2520low-rank%2520parameter%2520matrices%250Ain%2520the%2520self-attention%2520mechanism%252C%2520a%2520common%2520feature%2520of%2520practical%2520transformer%250Aimplementations.%2520These%2520results%2520are%2520first%2520established%2520in%2520the%2520hardmax%250Aself-attention%2520setting%252C%2520where%2520the%2520geometric%2520structure%2520permits%2520an%2520explicit%2520and%250Aquantitative%2520analysis%252C%2520and%2520are%2520then%2520extended%2520to%2520the%2520softmax%2520setting.%2520Finally%252C%250Awe%2520demonstrate%2520the%2520applicability%2520of%2520our%2520exact%2520interpolation%2520construction%2520to%250Alearning%2520problems%252C%2520in%2520particular%2520by%2520providing%2520convergence%2520guarantees%2520to%2520a%250Aglobal%2520minimizer%2520under%2520regularized%2520training%2520strategies.%2520Our%2520analysis%250Acontributes%2520to%2520the%2520theoretical%2520understanding%2520of%2520transformer%2520models%252C%2520offering%2520an%250Aexplanation%2520for%2520their%2520excellent%2520performance%2520in%2520exact%2520sequence-to-sequence%250Ainterpolation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02270v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exact%20Sequence%20Interpolation%20with%20Transformers&entry.906535625=Albert%20Alcalde%20and%20Giovanni%20Fantuzzi%20and%20Enrique%20Zuazua&entry.1292438233=%20%20We%20prove%20that%20transformers%20can%20exactly%20interpolate%20datasets%20of%20finite%20input%0Asequences%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20%24d%5Cgeq%202%24%2C%20with%20corresponding%20output%20sequences%20of%0Asmaller%20or%20equal%20length.%20Specifically%2C%20given%20%24N%24%20sequences%20of%20arbitrary%20but%0Afinite%20lengths%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%20and%20output%20sequences%20of%20lengths%20%24m%5E1%2C%20%5Cdots%2C%0Am%5EN%20%5Cin%20%5Cmathbb%7BN%7D%24%2C%20we%20construct%20a%20transformer%20with%20%24%5Cmathcal%7BO%7D%28%5Csum_%7Bj%3D1%7D%5EN%0Am%5Ej%29%24%20blocks%20and%20%24%5Cmathcal%7BO%7D%28d%20%5Csum_%7Bj%3D1%7D%5EN%20m%5Ej%29%24%20parameters%20that%20exactly%0Ainterpolates%20the%20dataset.%20Our%20construction%20provides%20complexity%20estimates%20that%0Aare%20independent%20of%20the%20input%20sequence%20length%2C%20by%20alternating%20feed-forward%20and%0Aself-attention%20layers%20and%20by%20capitalizing%20on%20the%20clustering%20effect%20inherent%20to%0Athe%20latter.%20Our%20novel%20constructive%20method%20also%20uses%20low-rank%20parameter%20matrices%0Ain%20the%20self-attention%20mechanism%2C%20a%20common%20feature%20of%20practical%20transformer%0Aimplementations.%20These%20results%20are%20first%20established%20in%20the%20hardmax%0Aself-attention%20setting%2C%20where%20the%20geometric%20structure%20permits%20an%20explicit%20and%0Aquantitative%20analysis%2C%20and%20are%20then%20extended%20to%20the%20softmax%20setting.%20Finally%2C%0Awe%20demonstrate%20the%20applicability%20of%20our%20exact%20interpolation%20construction%20to%0Alearning%20problems%2C%20in%20particular%20by%20providing%20convergence%20guarantees%20to%20a%0Aglobal%20minimizer%20under%20regularized%20training%20strategies.%20Our%20analysis%0Acontributes%20to%20the%20theoretical%20understanding%20of%20transformer%20models%2C%20offering%20an%0Aexplanation%20for%20their%20excellent%20performance%20in%20exact%20sequence-to-sequence%0Ainterpolation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02270v2&entry.124074799=Read"},
{"title": "Tracking the Median of Gradients with a Stochastic Proximal Point Method", "author": "Fabian Schaipp and Guillaume Garrigos and Umut Simsekli and Robert Gower", "abstract": "  There are several applications of stochastic optimization where one can\nbenefit from a robust estimate of the gradient. For example, domains such as\ndistributed learning with corrupted nodes, the presence of large outliers in\nthe training data, learning under privacy constraints, or even heavy-tailed\nnoise due to the dynamics of the algorithm itself. Here we study SGD with\nrobust gradient estimators based on estimating the median.\n  We first derive iterative methods based on the stochastic proximal point\nmethod for computing the median gradient and generalizations thereof. Then we\npropose an algorithm estimating the median gradient across iterations, and find\nthat several well known methods are particular cases of this framework. For\ninstance, we observe that different forms of clipping allow to compute online\nestimators of the median of gradients, in contrast to (heavy-ball) momentum,\nwhich corresponds to an online estimator of the mean. Finally, we provide a\ntheoretical framework for an algorithm computing the median gradient across\nsamples, and show that the resulting method can converge even under\nheavy-tailed, state-dependent noise.\n", "link": "http://arxiv.org/abs/2402.12828v2", "date": "2025-10-29", "relevancy": 1.8779, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4806}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4723}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tracking%20the%20Median%20of%20Gradients%20with%20a%20Stochastic%20Proximal%20Point%20Method&body=Title%3A%20Tracking%20the%20Median%20of%20Gradients%20with%20a%20Stochastic%20Proximal%20Point%20Method%0AAuthor%3A%20Fabian%20Schaipp%20and%20Guillaume%20Garrigos%20and%20Umut%20Simsekli%20and%20Robert%20Gower%0AAbstract%3A%20%20%20There%20are%20several%20applications%20of%20stochastic%20optimization%20where%20one%20can%0Abenefit%20from%20a%20robust%20estimate%20of%20the%20gradient.%20For%20example%2C%20domains%20such%20as%0Adistributed%20learning%20with%20corrupted%20nodes%2C%20the%20presence%20of%20large%20outliers%20in%0Athe%20training%20data%2C%20learning%20under%20privacy%20constraints%2C%20or%20even%20heavy-tailed%0Anoise%20due%20to%20the%20dynamics%20of%20the%20algorithm%20itself.%20Here%20we%20study%20SGD%20with%0Arobust%20gradient%20estimators%20based%20on%20estimating%20the%20median.%0A%20%20We%20first%20derive%20iterative%20methods%20based%20on%20the%20stochastic%20proximal%20point%0Amethod%20for%20computing%20the%20median%20gradient%20and%20generalizations%20thereof.%20Then%20we%0Apropose%20an%20algorithm%20estimating%20the%20median%20gradient%20across%20iterations%2C%20and%20find%0Athat%20several%20well%20known%20methods%20are%20particular%20cases%20of%20this%20framework.%20For%0Ainstance%2C%20we%20observe%20that%20different%20forms%20of%20clipping%20allow%20to%20compute%20online%0Aestimators%20of%20the%20median%20of%20gradients%2C%20in%20contrast%20to%20%28heavy-ball%29%20momentum%2C%0Awhich%20corresponds%20to%20an%20online%20estimator%20of%20the%20mean.%20Finally%2C%20we%20provide%20a%0Atheoretical%20framework%20for%20an%20algorithm%20computing%20the%20median%20gradient%20across%0Asamples%2C%20and%20show%20that%20the%20resulting%20method%20can%20converge%20even%20under%0Aheavy-tailed%2C%20state-dependent%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTracking%2520the%2520Median%2520of%2520Gradients%2520with%2520a%2520Stochastic%2520Proximal%2520Point%2520Method%26entry.906535625%3DFabian%2520Schaipp%2520and%2520Guillaume%2520Garrigos%2520and%2520Umut%2520Simsekli%2520and%2520Robert%2520Gower%26entry.1292438233%3D%2520%2520There%2520are%2520several%2520applications%2520of%2520stochastic%2520optimization%2520where%2520one%2520can%250Abenefit%2520from%2520a%2520robust%2520estimate%2520of%2520the%2520gradient.%2520For%2520example%252C%2520domains%2520such%2520as%250Adistributed%2520learning%2520with%2520corrupted%2520nodes%252C%2520the%2520presence%2520of%2520large%2520outliers%2520in%250Athe%2520training%2520data%252C%2520learning%2520under%2520privacy%2520constraints%252C%2520or%2520even%2520heavy-tailed%250Anoise%2520due%2520to%2520the%2520dynamics%2520of%2520the%2520algorithm%2520itself.%2520Here%2520we%2520study%2520SGD%2520with%250Arobust%2520gradient%2520estimators%2520based%2520on%2520estimating%2520the%2520median.%250A%2520%2520We%2520first%2520derive%2520iterative%2520methods%2520based%2520on%2520the%2520stochastic%2520proximal%2520point%250Amethod%2520for%2520computing%2520the%2520median%2520gradient%2520and%2520generalizations%2520thereof.%2520Then%2520we%250Apropose%2520an%2520algorithm%2520estimating%2520the%2520median%2520gradient%2520across%2520iterations%252C%2520and%2520find%250Athat%2520several%2520well%2520known%2520methods%2520are%2520particular%2520cases%2520of%2520this%2520framework.%2520For%250Ainstance%252C%2520we%2520observe%2520that%2520different%2520forms%2520of%2520clipping%2520allow%2520to%2520compute%2520online%250Aestimators%2520of%2520the%2520median%2520of%2520gradients%252C%2520in%2520contrast%2520to%2520%2528heavy-ball%2529%2520momentum%252C%250Awhich%2520corresponds%2520to%2520an%2520online%2520estimator%2520of%2520the%2520mean.%2520Finally%252C%2520we%2520provide%2520a%250Atheoretical%2520framework%2520for%2520an%2520algorithm%2520computing%2520the%2520median%2520gradient%2520across%250Asamples%252C%2520and%2520show%2520that%2520the%2520resulting%2520method%2520can%2520converge%2520even%2520under%250Aheavy-tailed%252C%2520state-dependent%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tracking%20the%20Median%20of%20Gradients%20with%20a%20Stochastic%20Proximal%20Point%20Method&entry.906535625=Fabian%20Schaipp%20and%20Guillaume%20Garrigos%20and%20Umut%20Simsekli%20and%20Robert%20Gower&entry.1292438233=%20%20There%20are%20several%20applications%20of%20stochastic%20optimization%20where%20one%20can%0Abenefit%20from%20a%20robust%20estimate%20of%20the%20gradient.%20For%20example%2C%20domains%20such%20as%0Adistributed%20learning%20with%20corrupted%20nodes%2C%20the%20presence%20of%20large%20outliers%20in%0Athe%20training%20data%2C%20learning%20under%20privacy%20constraints%2C%20or%20even%20heavy-tailed%0Anoise%20due%20to%20the%20dynamics%20of%20the%20algorithm%20itself.%20Here%20we%20study%20SGD%20with%0Arobust%20gradient%20estimators%20based%20on%20estimating%20the%20median.%0A%20%20We%20first%20derive%20iterative%20methods%20based%20on%20the%20stochastic%20proximal%20point%0Amethod%20for%20computing%20the%20median%20gradient%20and%20generalizations%20thereof.%20Then%20we%0Apropose%20an%20algorithm%20estimating%20the%20median%20gradient%20across%20iterations%2C%20and%20find%0Athat%20several%20well%20known%20methods%20are%20particular%20cases%20of%20this%20framework.%20For%0Ainstance%2C%20we%20observe%20that%20different%20forms%20of%20clipping%20allow%20to%20compute%20online%0Aestimators%20of%20the%20median%20of%20gradients%2C%20in%20contrast%20to%20%28heavy-ball%29%20momentum%2C%0Awhich%20corresponds%20to%20an%20online%20estimator%20of%20the%20mean.%20Finally%2C%20we%20provide%20a%0Atheoretical%20framework%20for%20an%20algorithm%20computing%20the%20median%20gradient%20across%0Asamples%2C%20and%20show%20that%20the%20resulting%20method%20can%20converge%20even%20under%0Aheavy-tailed%2C%20state-dependent%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12828v2&entry.124074799=Read"},
{"title": "Spontaneous Giving and Calculated Greed in Language Models", "author": "Yuxuan Li and Hirokazu Shirado", "abstract": "  Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action.\n", "link": "http://arxiv.org/abs/2502.17720v4", "date": "2025-10-29", "relevancy": 1.8769, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spontaneous%20Giving%20and%20Calculated%20Greed%20in%20Language%20Models&body=Title%3A%20Spontaneous%20Giving%20and%20Calculated%20Greed%20in%20Language%20Models%0AAuthor%3A%20Yuxuan%20Li%20and%20Hirokazu%20Shirado%0AAbstract%3A%20%20%20Large%20language%20models%20demonstrate%20strong%20problem-solving%20abilities%20through%0Areasoning%20techniques%20such%20as%20chain-of-thought%20prompting%20and%20reflection.%0AHowever%2C%20it%20remains%20unclear%20whether%20these%20reasoning%20capabilities%20extend%20to%20a%0Aform%20of%20social%20intelligence%3A%20making%20effective%20decisions%20in%20cooperative%0Acontexts.%20We%20examine%20this%20question%20using%20economic%20games%20that%20simulate%20social%0Adilemmas.%20First%2C%20we%20apply%20chain-of-thought%20and%20reflection%20prompting%20to%20GPT-4o%0Ain%20a%20Public%20Goods%20Game.%20We%20then%20evaluate%20multiple%20off-the-shelf%20models%20across%0Asix%20cooperation%20and%20punishment%20games%2C%20comparing%20those%20with%20and%20without%20explicit%0Areasoning%20mechanisms.%20We%20find%20that%20reasoning%20models%20consistently%20reduce%0Acooperation%20and%20norm%20enforcement%2C%20favoring%20individual%20rationality.%20In%20repeated%0Ainteractions%2C%20groups%20with%20more%20reasoning%20agents%20exhibit%20lower%20collective%20gains.%0AThese%20behaviors%20mirror%20human%20patterns%20of%20%22spontaneous%20giving%20and%20calculated%0Agreed.%22%20Our%20findings%20underscore%20the%20need%20for%20LLM%20architectures%20that%20incorporate%0Asocial%20intelligence%20alongside%20reasoning%2C%20to%20help%20address--rather%20than%0Areinforce--the%20challenges%20of%20collective%20action.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17720v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpontaneous%2520Giving%2520and%2520Calculated%2520Greed%2520in%2520Language%2520Models%26entry.906535625%3DYuxuan%2520Li%2520and%2520Hirokazu%2520Shirado%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520demonstrate%2520strong%2520problem-solving%2520abilities%2520through%250Areasoning%2520techniques%2520such%2520as%2520chain-of-thought%2520prompting%2520and%2520reflection.%250AHowever%252C%2520it%2520remains%2520unclear%2520whether%2520these%2520reasoning%2520capabilities%2520extend%2520to%2520a%250Aform%2520of%2520social%2520intelligence%253A%2520making%2520effective%2520decisions%2520in%2520cooperative%250Acontexts.%2520We%2520examine%2520this%2520question%2520using%2520economic%2520games%2520that%2520simulate%2520social%250Adilemmas.%2520First%252C%2520we%2520apply%2520chain-of-thought%2520and%2520reflection%2520prompting%2520to%2520GPT-4o%250Ain%2520a%2520Public%2520Goods%2520Game.%2520We%2520then%2520evaluate%2520multiple%2520off-the-shelf%2520models%2520across%250Asix%2520cooperation%2520and%2520punishment%2520games%252C%2520comparing%2520those%2520with%2520and%2520without%2520explicit%250Areasoning%2520mechanisms.%2520We%2520find%2520that%2520reasoning%2520models%2520consistently%2520reduce%250Acooperation%2520and%2520norm%2520enforcement%252C%2520favoring%2520individual%2520rationality.%2520In%2520repeated%250Ainteractions%252C%2520groups%2520with%2520more%2520reasoning%2520agents%2520exhibit%2520lower%2520collective%2520gains.%250AThese%2520behaviors%2520mirror%2520human%2520patterns%2520of%2520%2522spontaneous%2520giving%2520and%2520calculated%250Agreed.%2522%2520Our%2520findings%2520underscore%2520the%2520need%2520for%2520LLM%2520architectures%2520that%2520incorporate%250Asocial%2520intelligence%2520alongside%2520reasoning%252C%2520to%2520help%2520address--rather%2520than%250Areinforce--the%2520challenges%2520of%2520collective%2520action.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17720v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spontaneous%20Giving%20and%20Calculated%20Greed%20in%20Language%20Models&entry.906535625=Yuxuan%20Li%20and%20Hirokazu%20Shirado&entry.1292438233=%20%20Large%20language%20models%20demonstrate%20strong%20problem-solving%20abilities%20through%0Areasoning%20techniques%20such%20as%20chain-of-thought%20prompting%20and%20reflection.%0AHowever%2C%20it%20remains%20unclear%20whether%20these%20reasoning%20capabilities%20extend%20to%20a%0Aform%20of%20social%20intelligence%3A%20making%20effective%20decisions%20in%20cooperative%0Acontexts.%20We%20examine%20this%20question%20using%20economic%20games%20that%20simulate%20social%0Adilemmas.%20First%2C%20we%20apply%20chain-of-thought%20and%20reflection%20prompting%20to%20GPT-4o%0Ain%20a%20Public%20Goods%20Game.%20We%20then%20evaluate%20multiple%20off-the-shelf%20models%20across%0Asix%20cooperation%20and%20punishment%20games%2C%20comparing%20those%20with%20and%20without%20explicit%0Areasoning%20mechanisms.%20We%20find%20that%20reasoning%20models%20consistently%20reduce%0Acooperation%20and%20norm%20enforcement%2C%20favoring%20individual%20rationality.%20In%20repeated%0Ainteractions%2C%20groups%20with%20more%20reasoning%20agents%20exhibit%20lower%20collective%20gains.%0AThese%20behaviors%20mirror%20human%20patterns%20of%20%22spontaneous%20giving%20and%20calculated%0Agreed.%22%20Our%20findings%20underscore%20the%20need%20for%20LLM%20architectures%20that%20incorporate%0Asocial%20intelligence%20alongside%20reasoning%2C%20to%20help%20address--rather%20than%0Areinforce--the%20challenges%20of%20collective%20action.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17720v4&entry.124074799=Read"},
{"title": "PitchFlower: A flow-based neural audio codec with pitch controllability", "author": "Diego Torres and Axel Roebel and Nicolas Obin", "abstract": "  We present PitchFlower, a flow-based neural audio codec with explicit pitch\ncontrollability. Our approach enforces disentanglement through a simple\nperturbation: during training, F0 contours are flattened and randomly shifted,\nwhile the true F0 is provided as conditioning. A vector-quantization bottleneck\nprevents pitch recovery, and a flow-based decoder generates high quality audio.\nExperiments show that PitchFlower achieves more accurate pitch control than\nWORLD at much higher audio quality, and outperforms SiFiGAN in controllability\nwhile maintaining comparable quality. Beyond pitch, this framework provides a\nsimple and extensible path toward disentangling other speech attributes.\n", "link": "http://arxiv.org/abs/2510.25566v1", "date": "2025-10-29", "relevancy": 1.8708, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5009}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4852}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PitchFlower%3A%20A%20flow-based%20neural%20audio%20codec%20with%20pitch%20controllability&body=Title%3A%20PitchFlower%3A%20A%20flow-based%20neural%20audio%20codec%20with%20pitch%20controllability%0AAuthor%3A%20Diego%20Torres%20and%20Axel%20Roebel%20and%20Nicolas%20Obin%0AAbstract%3A%20%20%20We%20present%20PitchFlower%2C%20a%20flow-based%20neural%20audio%20codec%20with%20explicit%20pitch%0Acontrollability.%20Our%20approach%20enforces%20disentanglement%20through%20a%20simple%0Aperturbation%3A%20during%20training%2C%20F0%20contours%20are%20flattened%20and%20randomly%20shifted%2C%0Awhile%20the%20true%20F0%20is%20provided%20as%20conditioning.%20A%20vector-quantization%20bottleneck%0Aprevents%20pitch%20recovery%2C%20and%20a%20flow-based%20decoder%20generates%20high%20quality%20audio.%0AExperiments%20show%20that%20PitchFlower%20achieves%20more%20accurate%20pitch%20control%20than%0AWORLD%20at%20much%20higher%20audio%20quality%2C%20and%20outperforms%20SiFiGAN%20in%20controllability%0Awhile%20maintaining%20comparable%20quality.%20Beyond%20pitch%2C%20this%20framework%20provides%20a%0Asimple%20and%20extensible%20path%20toward%20disentangling%20other%20speech%20attributes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPitchFlower%253A%2520A%2520flow-based%2520neural%2520audio%2520codec%2520with%2520pitch%2520controllability%26entry.906535625%3DDiego%2520Torres%2520and%2520Axel%2520Roebel%2520and%2520Nicolas%2520Obin%26entry.1292438233%3D%2520%2520We%2520present%2520PitchFlower%252C%2520a%2520flow-based%2520neural%2520audio%2520codec%2520with%2520explicit%2520pitch%250Acontrollability.%2520Our%2520approach%2520enforces%2520disentanglement%2520through%2520a%2520simple%250Aperturbation%253A%2520during%2520training%252C%2520F0%2520contours%2520are%2520flattened%2520and%2520randomly%2520shifted%252C%250Awhile%2520the%2520true%2520F0%2520is%2520provided%2520as%2520conditioning.%2520A%2520vector-quantization%2520bottleneck%250Aprevents%2520pitch%2520recovery%252C%2520and%2520a%2520flow-based%2520decoder%2520generates%2520high%2520quality%2520audio.%250AExperiments%2520show%2520that%2520PitchFlower%2520achieves%2520more%2520accurate%2520pitch%2520control%2520than%250AWORLD%2520at%2520much%2520higher%2520audio%2520quality%252C%2520and%2520outperforms%2520SiFiGAN%2520in%2520controllability%250Awhile%2520maintaining%2520comparable%2520quality.%2520Beyond%2520pitch%252C%2520this%2520framework%2520provides%2520a%250Asimple%2520and%2520extensible%2520path%2520toward%2520disentangling%2520other%2520speech%2520attributes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PitchFlower%3A%20A%20flow-based%20neural%20audio%20codec%20with%20pitch%20controllability&entry.906535625=Diego%20Torres%20and%20Axel%20Roebel%20and%20Nicolas%20Obin&entry.1292438233=%20%20We%20present%20PitchFlower%2C%20a%20flow-based%20neural%20audio%20codec%20with%20explicit%20pitch%0Acontrollability.%20Our%20approach%20enforces%20disentanglement%20through%20a%20simple%0Aperturbation%3A%20during%20training%2C%20F0%20contours%20are%20flattened%20and%20randomly%20shifted%2C%0Awhile%20the%20true%20F0%20is%20provided%20as%20conditioning.%20A%20vector-quantization%20bottleneck%0Aprevents%20pitch%20recovery%2C%20and%20a%20flow-based%20decoder%20generates%20high%20quality%20audio.%0AExperiments%20show%20that%20PitchFlower%20achieves%20more%20accurate%20pitch%20control%20than%0AWORLD%20at%20much%20higher%20audio%20quality%2C%20and%20outperforms%20SiFiGAN%20in%20controllability%0Awhile%20maintaining%20comparable%20quality.%20Beyond%20pitch%2C%20this%20framework%20provides%20a%0Asimple%20and%20extensible%20path%20toward%20disentangling%20other%20speech%20attributes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25566v1&entry.124074799=Read"},
{"title": "Incorporating Social Awareness into Control of Unknown Multi-Agent\n  Systems: A Real-Time Spatiotemporal Tubes Approach", "author": "Siddhartha Upadhyay and Ratnangshu Das and Pushpak Jagtap", "abstract": "  This paper presents a decentralized control framework that incorporates\nsocial awareness into multi-agent systems with unknown dynamics to achieve\nprescribed-time reach-avoid-stay tasks in dynamic environments. Each agent is\nassigned a social awareness index that quantifies its level of cooperation or\nself-interest, allowing heterogeneous social behaviors within the system.\nBuilding on the spatiotemporal tube (STT) framework, we propose a real-time STT\nframework that synthesizes tubes online for each agent while capturing its\nsocial interactions with others. A closed-form, approximation-free control law\nis derived to ensure that each agent remains within its evolving STT, thereby\navoiding dynamic obstacles while also preventing inter-agent collisions in a\nsocially aware manner, and reaching the target within a prescribed time. The\nproposed approach provides formal guarantees on safety and timing, and is\ncomputationally lightweight, model-free, and robust to unknown disturbances.\nThe effectiveness and scalability of the framework are validated through\nsimulation and hardware experiments on a 2D omnidirectional\n", "link": "http://arxiv.org/abs/2510.25597v1", "date": "2025-10-29", "relevancy": 1.6609, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5646}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5446}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Social%20Awareness%20into%20Control%20of%20Unknown%20Multi-Agent%0A%20%20Systems%3A%20A%20Real-Time%20Spatiotemporal%20Tubes%20Approach&body=Title%3A%20Incorporating%20Social%20Awareness%20into%20Control%20of%20Unknown%20Multi-Agent%0A%20%20Systems%3A%20A%20Real-Time%20Spatiotemporal%20Tubes%20Approach%0AAuthor%3A%20Siddhartha%20Upadhyay%20and%20Ratnangshu%20Das%20and%20Pushpak%20Jagtap%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20decentralized%20control%20framework%20that%20incorporates%0Asocial%20awareness%20into%20multi-agent%20systems%20with%20unknown%20dynamics%20to%20achieve%0Aprescribed-time%20reach-avoid-stay%20tasks%20in%20dynamic%20environments.%20Each%20agent%20is%0Aassigned%20a%20social%20awareness%20index%20that%20quantifies%20its%20level%20of%20cooperation%20or%0Aself-interest%2C%20allowing%20heterogeneous%20social%20behaviors%20within%20the%20system.%0ABuilding%20on%20the%20spatiotemporal%20tube%20%28STT%29%20framework%2C%20we%20propose%20a%20real-time%20STT%0Aframework%20that%20synthesizes%20tubes%20online%20for%20each%20agent%20while%20capturing%20its%0Asocial%20interactions%20with%20others.%20A%20closed-form%2C%20approximation-free%20control%20law%0Ais%20derived%20to%20ensure%20that%20each%20agent%20remains%20within%20its%20evolving%20STT%2C%20thereby%0Aavoiding%20dynamic%20obstacles%20while%20also%20preventing%20inter-agent%20collisions%20in%20a%0Asocially%20aware%20manner%2C%20and%20reaching%20the%20target%20within%20a%20prescribed%20time.%20The%0Aproposed%20approach%20provides%20formal%20guarantees%20on%20safety%20and%20timing%2C%20and%20is%0Acomputationally%20lightweight%2C%20model-free%2C%20and%20robust%20to%20unknown%20disturbances.%0AThe%20effectiveness%20and%20scalability%20of%20the%20framework%20are%20validated%20through%0Asimulation%20and%20hardware%20experiments%20on%20a%202D%20omnidirectional%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncorporating%2520Social%2520Awareness%2520into%2520Control%2520of%2520Unknown%2520Multi-Agent%250A%2520%2520Systems%253A%2520A%2520Real-Time%2520Spatiotemporal%2520Tubes%2520Approach%26entry.906535625%3DSiddhartha%2520Upadhyay%2520and%2520Ratnangshu%2520Das%2520and%2520Pushpak%2520Jagtap%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520decentralized%2520control%2520framework%2520that%2520incorporates%250Asocial%2520awareness%2520into%2520multi-agent%2520systems%2520with%2520unknown%2520dynamics%2520to%2520achieve%250Aprescribed-time%2520reach-avoid-stay%2520tasks%2520in%2520dynamic%2520environments.%2520Each%2520agent%2520is%250Aassigned%2520a%2520social%2520awareness%2520index%2520that%2520quantifies%2520its%2520level%2520of%2520cooperation%2520or%250Aself-interest%252C%2520allowing%2520heterogeneous%2520social%2520behaviors%2520within%2520the%2520system.%250ABuilding%2520on%2520the%2520spatiotemporal%2520tube%2520%2528STT%2529%2520framework%252C%2520we%2520propose%2520a%2520real-time%2520STT%250Aframework%2520that%2520synthesizes%2520tubes%2520online%2520for%2520each%2520agent%2520while%2520capturing%2520its%250Asocial%2520interactions%2520with%2520others.%2520A%2520closed-form%252C%2520approximation-free%2520control%2520law%250Ais%2520derived%2520to%2520ensure%2520that%2520each%2520agent%2520remains%2520within%2520its%2520evolving%2520STT%252C%2520thereby%250Aavoiding%2520dynamic%2520obstacles%2520while%2520also%2520preventing%2520inter-agent%2520collisions%2520in%2520a%250Asocially%2520aware%2520manner%252C%2520and%2520reaching%2520the%2520target%2520within%2520a%2520prescribed%2520time.%2520The%250Aproposed%2520approach%2520provides%2520formal%2520guarantees%2520on%2520safety%2520and%2520timing%252C%2520and%2520is%250Acomputationally%2520lightweight%252C%2520model-free%252C%2520and%2520robust%2520to%2520unknown%2520disturbances.%250AThe%2520effectiveness%2520and%2520scalability%2520of%2520the%2520framework%2520are%2520validated%2520through%250Asimulation%2520and%2520hardware%2520experiments%2520on%2520a%25202D%2520omnidirectional%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Social%20Awareness%20into%20Control%20of%20Unknown%20Multi-Agent%0A%20%20Systems%3A%20A%20Real-Time%20Spatiotemporal%20Tubes%20Approach&entry.906535625=Siddhartha%20Upadhyay%20and%20Ratnangshu%20Das%20and%20Pushpak%20Jagtap&entry.1292438233=%20%20This%20paper%20presents%20a%20decentralized%20control%20framework%20that%20incorporates%0Asocial%20awareness%20into%20multi-agent%20systems%20with%20unknown%20dynamics%20to%20achieve%0Aprescribed-time%20reach-avoid-stay%20tasks%20in%20dynamic%20environments.%20Each%20agent%20is%0Aassigned%20a%20social%20awareness%20index%20that%20quantifies%20its%20level%20of%20cooperation%20or%0Aself-interest%2C%20allowing%20heterogeneous%20social%20behaviors%20within%20the%20system.%0ABuilding%20on%20the%20spatiotemporal%20tube%20%28STT%29%20framework%2C%20we%20propose%20a%20real-time%20STT%0Aframework%20that%20synthesizes%20tubes%20online%20for%20each%20agent%20while%20capturing%20its%0Asocial%20interactions%20with%20others.%20A%20closed-form%2C%20approximation-free%20control%20law%0Ais%20derived%20to%20ensure%20that%20each%20agent%20remains%20within%20its%20evolving%20STT%2C%20thereby%0Aavoiding%20dynamic%20obstacles%20while%20also%20preventing%20inter-agent%20collisions%20in%20a%0Asocially%20aware%20manner%2C%20and%20reaching%20the%20target%20within%20a%20prescribed%20time.%20The%0Aproposed%20approach%20provides%20formal%20guarantees%20on%20safety%20and%20timing%2C%20and%20is%0Acomputationally%20lightweight%2C%20model-free%2C%20and%20robust%20to%20unknown%20disturbances.%0AThe%20effectiveness%20and%20scalability%20of%20the%20framework%20are%20validated%20through%0Asimulation%20and%20hardware%20experiments%20on%20a%202D%20omnidirectional%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25597v1&entry.124074799=Read"},
{"title": "Off-policy Reinforcement Learning with Model-based Exploration\n  Augmentation", "author": "Likun Wang and Xiangteng Zhang and Yinuo Wang and Guojian Zhan and Wenxuan Wang and Haoyu Gao and Jingliang Duan and Shengbo Eben Li", "abstract": "  Exploration is fundamental to reinforcement learning (RL), as it determines\nhow effectively an agent discovers and exploits the underlying structure of its\nenvironment to achieve optimal performance. Existing exploration methods\ngenerally fall into two categories: active exploration and passive exploration.\nThe former introduces stochasticity into the policy but struggles in\nhigh-dimensional environments, while the latter adaptively prioritizes\ntransitions in the replay buffer to enhance exploration, yet remains\nconstrained by limited sample diversity. To address the limitation in passive\nexploration, we propose Modelic Generative Exploration (MoGE), which augments\nexploration through the generation of under-explored critical states and\nsynthesis of dynamics-consistent experiences through transition models. MoGE is\ncomposed of two components: (1) a diffusion-based generator that synthesizes\ncritical states under the guidance of a utility function evaluating each\nstate's potential influence on policy exploration, and (2) a one-step\nimagination world model for constructing critical transitions based on the\ncritical states for agent learning. Our method adopts a modular formulation\nthat aligns with the principles of off-policy learning, allowing seamless\nintegration with existing algorithms to improve exploration without altering\ntheir core structures. Empirical results on OpenAI Gym and DeepMind Control\nSuite reveal that MoGE effectively bridges exploration and policy learning,\nleading to remarkable gains in both sample efficiency and performance across\ncomplex control tasks.\n", "link": "http://arxiv.org/abs/2510.25529v1", "date": "2025-10-29", "relevancy": 1.7806, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6142}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6101}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Off-policy%20Reinforcement%20Learning%20with%20Model-based%20Exploration%0A%20%20Augmentation&body=Title%3A%20Off-policy%20Reinforcement%20Learning%20with%20Model-based%20Exploration%0A%20%20Augmentation%0AAuthor%3A%20Likun%20Wang%20and%20Xiangteng%20Zhang%20and%20Yinuo%20Wang%20and%20Guojian%20Zhan%20and%20Wenxuan%20Wang%20and%20Haoyu%20Gao%20and%20Jingliang%20Duan%20and%20Shengbo%20Eben%20Li%0AAbstract%3A%20%20%20Exploration%20is%20fundamental%20to%20reinforcement%20learning%20%28RL%29%2C%20as%20it%20determines%0Ahow%20effectively%20an%20agent%20discovers%20and%20exploits%20the%20underlying%20structure%20of%20its%0Aenvironment%20to%20achieve%20optimal%20performance.%20Existing%20exploration%20methods%0Agenerally%20fall%20into%20two%20categories%3A%20active%20exploration%20and%20passive%20exploration.%0AThe%20former%20introduces%20stochasticity%20into%20the%20policy%20but%20struggles%20in%0Ahigh-dimensional%20environments%2C%20while%20the%20latter%20adaptively%20prioritizes%0Atransitions%20in%20the%20replay%20buffer%20to%20enhance%20exploration%2C%20yet%20remains%0Aconstrained%20by%20limited%20sample%20diversity.%20To%20address%20the%20limitation%20in%20passive%0Aexploration%2C%20we%20propose%20Modelic%20Generative%20Exploration%20%28MoGE%29%2C%20which%20augments%0Aexploration%20through%20the%20generation%20of%20under-explored%20critical%20states%20and%0Asynthesis%20of%20dynamics-consistent%20experiences%20through%20transition%20models.%20MoGE%20is%0Acomposed%20of%20two%20components%3A%20%281%29%20a%20diffusion-based%20generator%20that%20synthesizes%0Acritical%20states%20under%20the%20guidance%20of%20a%20utility%20function%20evaluating%20each%0Astate%27s%20potential%20influence%20on%20policy%20exploration%2C%20and%20%282%29%20a%20one-step%0Aimagination%20world%20model%20for%20constructing%20critical%20transitions%20based%20on%20the%0Acritical%20states%20for%20agent%20learning.%20Our%20method%20adopts%20a%20modular%20formulation%0Athat%20aligns%20with%20the%20principles%20of%20off-policy%20learning%2C%20allowing%20seamless%0Aintegration%20with%20existing%20algorithms%20to%20improve%20exploration%20without%20altering%0Atheir%20core%20structures.%20Empirical%20results%20on%20OpenAI%20Gym%20and%20DeepMind%20Control%0ASuite%20reveal%20that%20MoGE%20effectively%20bridges%20exploration%20and%20policy%20learning%2C%0Aleading%20to%20remarkable%20gains%20in%20both%20sample%20efficiency%20and%20performance%20across%0Acomplex%20control%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOff-policy%2520Reinforcement%2520Learning%2520with%2520Model-based%2520Exploration%250A%2520%2520Augmentation%26entry.906535625%3DLikun%2520Wang%2520and%2520Xiangteng%2520Zhang%2520and%2520Yinuo%2520Wang%2520and%2520Guojian%2520Zhan%2520and%2520Wenxuan%2520Wang%2520and%2520Haoyu%2520Gao%2520and%2520Jingliang%2520Duan%2520and%2520Shengbo%2520Eben%2520Li%26entry.1292438233%3D%2520%2520Exploration%2520is%2520fundamental%2520to%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520as%2520it%2520determines%250Ahow%2520effectively%2520an%2520agent%2520discovers%2520and%2520exploits%2520the%2520underlying%2520structure%2520of%2520its%250Aenvironment%2520to%2520achieve%2520optimal%2520performance.%2520Existing%2520exploration%2520methods%250Agenerally%2520fall%2520into%2520two%2520categories%253A%2520active%2520exploration%2520and%2520passive%2520exploration.%250AThe%2520former%2520introduces%2520stochasticity%2520into%2520the%2520policy%2520but%2520struggles%2520in%250Ahigh-dimensional%2520environments%252C%2520while%2520the%2520latter%2520adaptively%2520prioritizes%250Atransitions%2520in%2520the%2520replay%2520buffer%2520to%2520enhance%2520exploration%252C%2520yet%2520remains%250Aconstrained%2520by%2520limited%2520sample%2520diversity.%2520To%2520address%2520the%2520limitation%2520in%2520passive%250Aexploration%252C%2520we%2520propose%2520Modelic%2520Generative%2520Exploration%2520%2528MoGE%2529%252C%2520which%2520augments%250Aexploration%2520through%2520the%2520generation%2520of%2520under-explored%2520critical%2520states%2520and%250Asynthesis%2520of%2520dynamics-consistent%2520experiences%2520through%2520transition%2520models.%2520MoGE%2520is%250Acomposed%2520of%2520two%2520components%253A%2520%25281%2529%2520a%2520diffusion-based%2520generator%2520that%2520synthesizes%250Acritical%2520states%2520under%2520the%2520guidance%2520of%2520a%2520utility%2520function%2520evaluating%2520each%250Astate%2527s%2520potential%2520influence%2520on%2520policy%2520exploration%252C%2520and%2520%25282%2529%2520a%2520one-step%250Aimagination%2520world%2520model%2520for%2520constructing%2520critical%2520transitions%2520based%2520on%2520the%250Acritical%2520states%2520for%2520agent%2520learning.%2520Our%2520method%2520adopts%2520a%2520modular%2520formulation%250Athat%2520aligns%2520with%2520the%2520principles%2520of%2520off-policy%2520learning%252C%2520allowing%2520seamless%250Aintegration%2520with%2520existing%2520algorithms%2520to%2520improve%2520exploration%2520without%2520altering%250Atheir%2520core%2520structures.%2520Empirical%2520results%2520on%2520OpenAI%2520Gym%2520and%2520DeepMind%2520Control%250ASuite%2520reveal%2520that%2520MoGE%2520effectively%2520bridges%2520exploration%2520and%2520policy%2520learning%252C%250Aleading%2520to%2520remarkable%2520gains%2520in%2520both%2520sample%2520efficiency%2520and%2520performance%2520across%250Acomplex%2520control%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Off-policy%20Reinforcement%20Learning%20with%20Model-based%20Exploration%0A%20%20Augmentation&entry.906535625=Likun%20Wang%20and%20Xiangteng%20Zhang%20and%20Yinuo%20Wang%20and%20Guojian%20Zhan%20and%20Wenxuan%20Wang%20and%20Haoyu%20Gao%20and%20Jingliang%20Duan%20and%20Shengbo%20Eben%20Li&entry.1292438233=%20%20Exploration%20is%20fundamental%20to%20reinforcement%20learning%20%28RL%29%2C%20as%20it%20determines%0Ahow%20effectively%20an%20agent%20discovers%20and%20exploits%20the%20underlying%20structure%20of%20its%0Aenvironment%20to%20achieve%20optimal%20performance.%20Existing%20exploration%20methods%0Agenerally%20fall%20into%20two%20categories%3A%20active%20exploration%20and%20passive%20exploration.%0AThe%20former%20introduces%20stochasticity%20into%20the%20policy%20but%20struggles%20in%0Ahigh-dimensional%20environments%2C%20while%20the%20latter%20adaptively%20prioritizes%0Atransitions%20in%20the%20replay%20buffer%20to%20enhance%20exploration%2C%20yet%20remains%0Aconstrained%20by%20limited%20sample%20diversity.%20To%20address%20the%20limitation%20in%20passive%0Aexploration%2C%20we%20propose%20Modelic%20Generative%20Exploration%20%28MoGE%29%2C%20which%20augments%0Aexploration%20through%20the%20generation%20of%20under-explored%20critical%20states%20and%0Asynthesis%20of%20dynamics-consistent%20experiences%20through%20transition%20models.%20MoGE%20is%0Acomposed%20of%20two%20components%3A%20%281%29%20a%20diffusion-based%20generator%20that%20synthesizes%0Acritical%20states%20under%20the%20guidance%20of%20a%20utility%20function%20evaluating%20each%0Astate%27s%20potential%20influence%20on%20policy%20exploration%2C%20and%20%282%29%20a%20one-step%0Aimagination%20world%20model%20for%20constructing%20critical%20transitions%20based%20on%20the%0Acritical%20states%20for%20agent%20learning.%20Our%20method%20adopts%20a%20modular%20formulation%0Athat%20aligns%20with%20the%20principles%20of%20off-policy%20learning%2C%20allowing%20seamless%0Aintegration%20with%20existing%20algorithms%20to%20improve%20exploration%20without%20altering%0Atheir%20core%20structures.%20Empirical%20results%20on%20OpenAI%20Gym%20and%20DeepMind%20Control%0ASuite%20reveal%20that%20MoGE%20effectively%20bridges%20exploration%20and%20policy%20learning%2C%0Aleading%20to%20remarkable%20gains%20in%20both%20sample%20efficiency%20and%20performance%20across%0Acomplex%20control%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25529v1&entry.124074799=Read"},
{"title": "Group Interventions on Deep Networks for Causal Discovery in Subsystems", "author": "Wasim Ahmad and Joachim Denzler and Maha Shadaydeh", "abstract": "  Causal discovery uncovers complex relationships between variables, enhancing\npredictions, decision-making, and insights into real-world systems, especially\nin nonlinear multivariate time series. However, most existing methods primarily\nfocus on pairwise cause-effect relationships, overlooking interactions among\ngroups of variables, i.e., subsystems and their collective causal influence. In\nthis study, we introduce gCDMI, a novel multi-group causal discovery method\nthat leverages group-level interventions on trained deep neural networks and\nemploys model invariance testing to infer causal relationships. Our approach\ninvolves three key steps. First, we use deep learning to jointly model the\nstructural relationships among groups of all time series. Second, we apply\ngroup-wise interventions to the trained model. Finally, we conduct model\ninvariance testing to determine the presence of causal links among variable\ngroups. We evaluate our method on simulated datasets, demonstrating its\nsuperior performance in identifying group-level causal relationships compared\nto existing methods. Additionally, we validate our approach on real-world\ndatasets, including brain networks and climate ecosystems. Our results\nhighlight that applying group-level interventions to deep learning models,\ncombined with invariance testing, can effectively reveal complex causal\nstructures, offering valuable insights for domains such as neuroscience and\nclimate science.\n", "link": "http://arxiv.org/abs/2510.23906v2", "date": "2025-10-29", "relevancy": 1.3449, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4657}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4445}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Group%20Interventions%20on%20Deep%20Networks%20for%20Causal%20Discovery%20in%20Subsystems&body=Title%3A%20Group%20Interventions%20on%20Deep%20Networks%20for%20Causal%20Discovery%20in%20Subsystems%0AAuthor%3A%20Wasim%20Ahmad%20and%20Joachim%20Denzler%20and%20Maha%20Shadaydeh%0AAbstract%3A%20%20%20Causal%20discovery%20uncovers%20complex%20relationships%20between%20variables%2C%20enhancing%0Apredictions%2C%20decision-making%2C%20and%20insights%20into%20real-world%20systems%2C%20especially%0Ain%20nonlinear%20multivariate%20time%20series.%20However%2C%20most%20existing%20methods%20primarily%0Afocus%20on%20pairwise%20cause-effect%20relationships%2C%20overlooking%20interactions%20among%0Agroups%20of%20variables%2C%20i.e.%2C%20subsystems%20and%20their%20collective%20causal%20influence.%20In%0Athis%20study%2C%20we%20introduce%20gCDMI%2C%20a%20novel%20multi-group%20causal%20discovery%20method%0Athat%20leverages%20group-level%20interventions%20on%20trained%20deep%20neural%20networks%20and%0Aemploys%20model%20invariance%20testing%20to%20infer%20causal%20relationships.%20Our%20approach%0Ainvolves%20three%20key%20steps.%20First%2C%20we%20use%20deep%20learning%20to%20jointly%20model%20the%0Astructural%20relationships%20among%20groups%20of%20all%20time%20series.%20Second%2C%20we%20apply%0Agroup-wise%20interventions%20to%20the%20trained%20model.%20Finally%2C%20we%20conduct%20model%0Ainvariance%20testing%20to%20determine%20the%20presence%20of%20causal%20links%20among%20variable%0Agroups.%20We%20evaluate%20our%20method%20on%20simulated%20datasets%2C%20demonstrating%20its%0Asuperior%20performance%20in%20identifying%20group-level%20causal%20relationships%20compared%0Ato%20existing%20methods.%20Additionally%2C%20we%20validate%20our%20approach%20on%20real-world%0Adatasets%2C%20including%20brain%20networks%20and%20climate%20ecosystems.%20Our%20results%0Ahighlight%20that%20applying%20group-level%20interventions%20to%20deep%20learning%20models%2C%0Acombined%20with%20invariance%20testing%2C%20can%20effectively%20reveal%20complex%20causal%0Astructures%2C%20offering%20valuable%20insights%20for%20domains%20such%20as%20neuroscience%20and%0Aclimate%20science.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.23906v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroup%2520Interventions%2520on%2520Deep%2520Networks%2520for%2520Causal%2520Discovery%2520in%2520Subsystems%26entry.906535625%3DWasim%2520Ahmad%2520and%2520Joachim%2520Denzler%2520and%2520Maha%2520Shadaydeh%26entry.1292438233%3D%2520%2520Causal%2520discovery%2520uncovers%2520complex%2520relationships%2520between%2520variables%252C%2520enhancing%250Apredictions%252C%2520decision-making%252C%2520and%2520insights%2520into%2520real-world%2520systems%252C%2520especially%250Ain%2520nonlinear%2520multivariate%2520time%2520series.%2520However%252C%2520most%2520existing%2520methods%2520primarily%250Afocus%2520on%2520pairwise%2520cause-effect%2520relationships%252C%2520overlooking%2520interactions%2520among%250Agroups%2520of%2520variables%252C%2520i.e.%252C%2520subsystems%2520and%2520their%2520collective%2520causal%2520influence.%2520In%250Athis%2520study%252C%2520we%2520introduce%2520gCDMI%252C%2520a%2520novel%2520multi-group%2520causal%2520discovery%2520method%250Athat%2520leverages%2520group-level%2520interventions%2520on%2520trained%2520deep%2520neural%2520networks%2520and%250Aemploys%2520model%2520invariance%2520testing%2520to%2520infer%2520causal%2520relationships.%2520Our%2520approach%250Ainvolves%2520three%2520key%2520steps.%2520First%252C%2520we%2520use%2520deep%2520learning%2520to%2520jointly%2520model%2520the%250Astructural%2520relationships%2520among%2520groups%2520of%2520all%2520time%2520series.%2520Second%252C%2520we%2520apply%250Agroup-wise%2520interventions%2520to%2520the%2520trained%2520model.%2520Finally%252C%2520we%2520conduct%2520model%250Ainvariance%2520testing%2520to%2520determine%2520the%2520presence%2520of%2520causal%2520links%2520among%2520variable%250Agroups.%2520We%2520evaluate%2520our%2520method%2520on%2520simulated%2520datasets%252C%2520demonstrating%2520its%250Asuperior%2520performance%2520in%2520identifying%2520group-level%2520causal%2520relationships%2520compared%250Ato%2520existing%2520methods.%2520Additionally%252C%2520we%2520validate%2520our%2520approach%2520on%2520real-world%250Adatasets%252C%2520including%2520brain%2520networks%2520and%2520climate%2520ecosystems.%2520Our%2520results%250Ahighlight%2520that%2520applying%2520group-level%2520interventions%2520to%2520deep%2520learning%2520models%252C%250Acombined%2520with%2520invariance%2520testing%252C%2520can%2520effectively%2520reveal%2520complex%2520causal%250Astructures%252C%2520offering%2520valuable%2520insights%2520for%2520domains%2520such%2520as%2520neuroscience%2520and%250Aclimate%2520science.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23906v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Group%20Interventions%20on%20Deep%20Networks%20for%20Causal%20Discovery%20in%20Subsystems&entry.906535625=Wasim%20Ahmad%20and%20Joachim%20Denzler%20and%20Maha%20Shadaydeh&entry.1292438233=%20%20Causal%20discovery%20uncovers%20complex%20relationships%20between%20variables%2C%20enhancing%0Apredictions%2C%20decision-making%2C%20and%20insights%20into%20real-world%20systems%2C%20especially%0Ain%20nonlinear%20multivariate%20time%20series.%20However%2C%20most%20existing%20methods%20primarily%0Afocus%20on%20pairwise%20cause-effect%20relationships%2C%20overlooking%20interactions%20among%0Agroups%20of%20variables%2C%20i.e.%2C%20subsystems%20and%20their%20collective%20causal%20influence.%20In%0Athis%20study%2C%20we%20introduce%20gCDMI%2C%20a%20novel%20multi-group%20causal%20discovery%20method%0Athat%20leverages%20group-level%20interventions%20on%20trained%20deep%20neural%20networks%20and%0Aemploys%20model%20invariance%20testing%20to%20infer%20causal%20relationships.%20Our%20approach%0Ainvolves%20three%20key%20steps.%20First%2C%20we%20use%20deep%20learning%20to%20jointly%20model%20the%0Astructural%20relationships%20among%20groups%20of%20all%20time%20series.%20Second%2C%20we%20apply%0Agroup-wise%20interventions%20to%20the%20trained%20model.%20Finally%2C%20we%20conduct%20model%0Ainvariance%20testing%20to%20determine%20the%20presence%20of%20causal%20links%20among%20variable%0Agroups.%20We%20evaluate%20our%20method%20on%20simulated%20datasets%2C%20demonstrating%20its%0Asuperior%20performance%20in%20identifying%20group-level%20causal%20relationships%20compared%0Ato%20existing%20methods.%20Additionally%2C%20we%20validate%20our%20approach%20on%20real-world%0Adatasets%2C%20including%20brain%20networks%20and%20climate%20ecosystems.%20Our%20results%0Ahighlight%20that%20applying%20group-level%20interventions%20to%20deep%20learning%20models%2C%0Acombined%20with%20invariance%20testing%2C%20can%20effectively%20reveal%20complex%20causal%0Astructures%2C%20offering%20valuable%20insights%20for%20domains%20such%20as%20neuroscience%20and%0Aclimate%20science.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.23906v2&entry.124074799=Read"},
{"title": "MP-FVM: Enhancing Finite Volume Method for Water Infiltration Modeling\n  in Unsaturated Soils via Message-passing Encoder-decoder Network", "author": "Zeyuan Song and Zheyu Jiang", "abstract": "  The spatiotemporal water flow dynamics in unsaturated soils can generally be\nmodeled by the Richards equation. To overcome the computational challenges\nassociated with solving this highly nonlinear partial differential equation\n(PDE), we present a novel solution algorithm, which we name as the MP-FVM\n(Message Passing-Finite Volume Method), to holistically integrate adaptive\nfixed-point iteration scheme, encoder-decoder neural network architecture,\nSobolev training, and message passing mechanism in a finite volume\ndiscretization framework. We thoroughly discuss the need and benefits of\nintroducing these components to achieve synergistic improvements in accuracy\nand stability of the solution. We also show that our MP-FVM algorithm can\naccurately solve the mixed-form $n$-dimensional Richards equation with\nguaranteed convergence under reasonable assumptions. Through several\nillustrative examples, we demonstrate that our MP-FVM algorithm not only\nachieves superior accuracy, but also better preserves the underlying physical\nlaws and mass conservation of the Richards equation compared to\nstate-of-the-art solution algorithms and the commercial HYDRUS solver.\n", "link": "http://arxiv.org/abs/2310.02806v3", "date": "2025-10-29", "relevancy": 1.4415, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5039}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4779}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MP-FVM%3A%20Enhancing%20Finite%20Volume%20Method%20for%20Water%20Infiltration%20Modeling%0A%20%20in%20Unsaturated%20Soils%20via%20Message-passing%20Encoder-decoder%20Network&body=Title%3A%20MP-FVM%3A%20Enhancing%20Finite%20Volume%20Method%20for%20Water%20Infiltration%20Modeling%0A%20%20in%20Unsaturated%20Soils%20via%20Message-passing%20Encoder-decoder%20Network%0AAuthor%3A%20Zeyuan%20Song%20and%20Zheyu%20Jiang%0AAbstract%3A%20%20%20The%20spatiotemporal%20water%20flow%20dynamics%20in%20unsaturated%20soils%20can%20generally%20be%0Amodeled%20by%20the%20Richards%20equation.%20To%20overcome%20the%20computational%20challenges%0Aassociated%20with%20solving%20this%20highly%20nonlinear%20partial%20differential%20equation%0A%28PDE%29%2C%20we%20present%20a%20novel%20solution%20algorithm%2C%20which%20we%20name%20as%20the%20MP-FVM%0A%28Message%20Passing-Finite%20Volume%20Method%29%2C%20to%20holistically%20integrate%20adaptive%0Afixed-point%20iteration%20scheme%2C%20encoder-decoder%20neural%20network%20architecture%2C%0ASobolev%20training%2C%20and%20message%20passing%20mechanism%20in%20a%20finite%20volume%0Adiscretization%20framework.%20We%20thoroughly%20discuss%20the%20need%20and%20benefits%20of%0Aintroducing%20these%20components%20to%20achieve%20synergistic%20improvements%20in%20accuracy%0Aand%20stability%20of%20the%20solution.%20We%20also%20show%20that%20our%20MP-FVM%20algorithm%20can%0Aaccurately%20solve%20the%20mixed-form%20%24n%24-dimensional%20Richards%20equation%20with%0Aguaranteed%20convergence%20under%20reasonable%20assumptions.%20Through%20several%0Aillustrative%20examples%2C%20we%20demonstrate%20that%20our%20MP-FVM%20algorithm%20not%20only%0Aachieves%20superior%20accuracy%2C%20but%20also%20better%20preserves%20the%20underlying%20physical%0Alaws%20and%20mass%20conservation%20of%20the%20Richards%20equation%20compared%20to%0Astate-of-the-art%20solution%20algorithms%20and%20the%20commercial%20HYDRUS%20solver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02806v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMP-FVM%253A%2520Enhancing%2520Finite%2520Volume%2520Method%2520for%2520Water%2520Infiltration%2520Modeling%250A%2520%2520in%2520Unsaturated%2520Soils%2520via%2520Message-passing%2520Encoder-decoder%2520Network%26entry.906535625%3DZeyuan%2520Song%2520and%2520Zheyu%2520Jiang%26entry.1292438233%3D%2520%2520The%2520spatiotemporal%2520water%2520flow%2520dynamics%2520in%2520unsaturated%2520soils%2520can%2520generally%2520be%250Amodeled%2520by%2520the%2520Richards%2520equation.%2520To%2520overcome%2520the%2520computational%2520challenges%250Aassociated%2520with%2520solving%2520this%2520highly%2520nonlinear%2520partial%2520differential%2520equation%250A%2528PDE%2529%252C%2520we%2520present%2520a%2520novel%2520solution%2520algorithm%252C%2520which%2520we%2520name%2520as%2520the%2520MP-FVM%250A%2528Message%2520Passing-Finite%2520Volume%2520Method%2529%252C%2520to%2520holistically%2520integrate%2520adaptive%250Afixed-point%2520iteration%2520scheme%252C%2520encoder-decoder%2520neural%2520network%2520architecture%252C%250ASobolev%2520training%252C%2520and%2520message%2520passing%2520mechanism%2520in%2520a%2520finite%2520volume%250Adiscretization%2520framework.%2520We%2520thoroughly%2520discuss%2520the%2520need%2520and%2520benefits%2520of%250Aintroducing%2520these%2520components%2520to%2520achieve%2520synergistic%2520improvements%2520in%2520accuracy%250Aand%2520stability%2520of%2520the%2520solution.%2520We%2520also%2520show%2520that%2520our%2520MP-FVM%2520algorithm%2520can%250Aaccurately%2520solve%2520the%2520mixed-form%2520%2524n%2524-dimensional%2520Richards%2520equation%2520with%250Aguaranteed%2520convergence%2520under%2520reasonable%2520assumptions.%2520Through%2520several%250Aillustrative%2520examples%252C%2520we%2520demonstrate%2520that%2520our%2520MP-FVM%2520algorithm%2520not%2520only%250Aachieves%2520superior%2520accuracy%252C%2520but%2520also%2520better%2520preserves%2520the%2520underlying%2520physical%250Alaws%2520and%2520mass%2520conservation%2520of%2520the%2520Richards%2520equation%2520compared%2520to%250Astate-of-the-art%2520solution%2520algorithms%2520and%2520the%2520commercial%2520HYDRUS%2520solver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02806v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MP-FVM%3A%20Enhancing%20Finite%20Volume%20Method%20for%20Water%20Infiltration%20Modeling%0A%20%20in%20Unsaturated%20Soils%20via%20Message-passing%20Encoder-decoder%20Network&entry.906535625=Zeyuan%20Song%20and%20Zheyu%20Jiang&entry.1292438233=%20%20The%20spatiotemporal%20water%20flow%20dynamics%20in%20unsaturated%20soils%20can%20generally%20be%0Amodeled%20by%20the%20Richards%20equation.%20To%20overcome%20the%20computational%20challenges%0Aassociated%20with%20solving%20this%20highly%20nonlinear%20partial%20differential%20equation%0A%28PDE%29%2C%20we%20present%20a%20novel%20solution%20algorithm%2C%20which%20we%20name%20as%20the%20MP-FVM%0A%28Message%20Passing-Finite%20Volume%20Method%29%2C%20to%20holistically%20integrate%20adaptive%0Afixed-point%20iteration%20scheme%2C%20encoder-decoder%20neural%20network%20architecture%2C%0ASobolev%20training%2C%20and%20message%20passing%20mechanism%20in%20a%20finite%20volume%0Adiscretization%20framework.%20We%20thoroughly%20discuss%20the%20need%20and%20benefits%20of%0Aintroducing%20these%20components%20to%20achieve%20synergistic%20improvements%20in%20accuracy%0Aand%20stability%20of%20the%20solution.%20We%20also%20show%20that%20our%20MP-FVM%20algorithm%20can%0Aaccurately%20solve%20the%20mixed-form%20%24n%24-dimensional%20Richards%20equation%20with%0Aguaranteed%20convergence%20under%20reasonable%20assumptions.%20Through%20several%0Aillustrative%20examples%2C%20we%20demonstrate%20that%20our%20MP-FVM%20algorithm%20not%20only%0Aachieves%20superior%20accuracy%2C%20but%20also%20better%20preserves%20the%20underlying%20physical%0Alaws%20and%20mass%20conservation%20of%20the%20Richards%20equation%20compared%20to%0Astate-of-the-art%20solution%20algorithms%20and%20the%20commercial%20HYDRUS%20solver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02806v3&entry.124074799=Read"},
{"title": "Lift What You Can: Green Online Learning with Heterogeneous Ensembles", "author": "Kirsten K\u00f6bschall and Sebastian Buschj\u00e4ger and Raphael Fischer and Lisa Hartung and Stefan Kramer", "abstract": "  Ensemble methods for stream mining necessitate managing multiple models and\nupdating them as data distributions evolve. Considering the calls for more\nsustainability, established methods are however not sufficiently considerate of\nensemble members' computational expenses and instead overly focus on predictive\ncapabilities. To address these challenges and enable green online learning, we\npropose heterogeneous online ensembles (HEROS). For every training step, HEROS\nchooses a subset of models from a pool of models initialized with diverse\nhyperparameter choices under resource constraints to train. We introduce a\nMarkov decision process to theoretically capture the trade-offs between\npredictive performance and sustainability constraints. Based on this framework,\nwe present different policies for choosing which models to train on incoming\ndata. Most notably, we propose the novel $\\zeta$-policy, which focuses on\ntraining near-optimal models at reduced costs. Using a stochastic model, we\ntheoretically prove that our $\\zeta$-policy achieves near optimal performance\nwhile using fewer resources compared to the best performing policy. In our\nexperiments across 11 benchmark datasets, we find empiric evidence that our\n$\\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating\nhighly accurate performance, in some cases even outperforming competitors, and\nsimultaneously being much more resource-friendly.\n", "link": "http://arxiv.org/abs/2509.18962v2", "date": "2025-10-29", "relevancy": 1.8465, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4817}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4506}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lift%20What%20You%20Can%3A%20Green%20Online%20Learning%20with%20Heterogeneous%20Ensembles&body=Title%3A%20Lift%20What%20You%20Can%3A%20Green%20Online%20Learning%20with%20Heterogeneous%20Ensembles%0AAuthor%3A%20Kirsten%20K%C3%B6bschall%20and%20Sebastian%20Buschj%C3%A4ger%20and%20Raphael%20Fischer%20and%20Lisa%20Hartung%20and%20Stefan%20Kramer%0AAbstract%3A%20%20%20Ensemble%20methods%20for%20stream%20mining%20necessitate%20managing%20multiple%20models%20and%0Aupdating%20them%20as%20data%20distributions%20evolve.%20Considering%20the%20calls%20for%20more%0Asustainability%2C%20established%20methods%20are%20however%20not%20sufficiently%20considerate%20of%0Aensemble%20members%27%20computational%20expenses%20and%20instead%20overly%20focus%20on%20predictive%0Acapabilities.%20To%20address%20these%20challenges%20and%20enable%20green%20online%20learning%2C%20we%0Apropose%20heterogeneous%20online%20ensembles%20%28HEROS%29.%20For%20every%20training%20step%2C%20HEROS%0Achooses%20a%20subset%20of%20models%20from%20a%20pool%20of%20models%20initialized%20with%20diverse%0Ahyperparameter%20choices%20under%20resource%20constraints%20to%20train.%20We%20introduce%20a%0AMarkov%20decision%20process%20to%20theoretically%20capture%20the%20trade-offs%20between%0Apredictive%20performance%20and%20sustainability%20constraints.%20Based%20on%20this%20framework%2C%0Awe%20present%20different%20policies%20for%20choosing%20which%20models%20to%20train%20on%20incoming%0Adata.%20Most%20notably%2C%20we%20propose%20the%20novel%20%24%5Czeta%24-policy%2C%20which%20focuses%20on%0Atraining%20near-optimal%20models%20at%20reduced%20costs.%20Using%20a%20stochastic%20model%2C%20we%0Atheoretically%20prove%20that%20our%20%24%5Czeta%24-policy%20achieves%20near%20optimal%20performance%0Awhile%20using%20fewer%20resources%20compared%20to%20the%20best%20performing%20policy.%20In%20our%0Aexperiments%20across%2011%20benchmark%20datasets%2C%20we%20find%20empiric%20evidence%20that%20our%0A%24%5Czeta%24-policy%20is%20a%20strong%20contribution%20to%20the%20state-of-the-art%2C%20demonstrating%0Ahighly%20accurate%20performance%2C%20in%20some%20cases%20even%20outperforming%20competitors%2C%20and%0Asimultaneously%20being%20much%20more%20resource-friendly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18962v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLift%2520What%2520You%2520Can%253A%2520Green%2520Online%2520Learning%2520with%2520Heterogeneous%2520Ensembles%26entry.906535625%3DKirsten%2520K%25C3%25B6bschall%2520and%2520Sebastian%2520Buschj%25C3%25A4ger%2520and%2520Raphael%2520Fischer%2520and%2520Lisa%2520Hartung%2520and%2520Stefan%2520Kramer%26entry.1292438233%3D%2520%2520Ensemble%2520methods%2520for%2520stream%2520mining%2520necessitate%2520managing%2520multiple%2520models%2520and%250Aupdating%2520them%2520as%2520data%2520distributions%2520evolve.%2520Considering%2520the%2520calls%2520for%2520more%250Asustainability%252C%2520established%2520methods%2520are%2520however%2520not%2520sufficiently%2520considerate%2520of%250Aensemble%2520members%2527%2520computational%2520expenses%2520and%2520instead%2520overly%2520focus%2520on%2520predictive%250Acapabilities.%2520To%2520address%2520these%2520challenges%2520and%2520enable%2520green%2520online%2520learning%252C%2520we%250Apropose%2520heterogeneous%2520online%2520ensembles%2520%2528HEROS%2529.%2520For%2520every%2520training%2520step%252C%2520HEROS%250Achooses%2520a%2520subset%2520of%2520models%2520from%2520a%2520pool%2520of%2520models%2520initialized%2520with%2520diverse%250Ahyperparameter%2520choices%2520under%2520resource%2520constraints%2520to%2520train.%2520We%2520introduce%2520a%250AMarkov%2520decision%2520process%2520to%2520theoretically%2520capture%2520the%2520trade-offs%2520between%250Apredictive%2520performance%2520and%2520sustainability%2520constraints.%2520Based%2520on%2520this%2520framework%252C%250Awe%2520present%2520different%2520policies%2520for%2520choosing%2520which%2520models%2520to%2520train%2520on%2520incoming%250Adata.%2520Most%2520notably%252C%2520we%2520propose%2520the%2520novel%2520%2524%255Czeta%2524-policy%252C%2520which%2520focuses%2520on%250Atraining%2520near-optimal%2520models%2520at%2520reduced%2520costs.%2520Using%2520a%2520stochastic%2520model%252C%2520we%250Atheoretically%2520prove%2520that%2520our%2520%2524%255Czeta%2524-policy%2520achieves%2520near%2520optimal%2520performance%250Awhile%2520using%2520fewer%2520resources%2520compared%2520to%2520the%2520best%2520performing%2520policy.%2520In%2520our%250Aexperiments%2520across%252011%2520benchmark%2520datasets%252C%2520we%2520find%2520empiric%2520evidence%2520that%2520our%250A%2524%255Czeta%2524-policy%2520is%2520a%2520strong%2520contribution%2520to%2520the%2520state-of-the-art%252C%2520demonstrating%250Ahighly%2520accurate%2520performance%252C%2520in%2520some%2520cases%2520even%2520outperforming%2520competitors%252C%2520and%250Asimultaneously%2520being%2520much%2520more%2520resource-friendly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18962v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lift%20What%20You%20Can%3A%20Green%20Online%20Learning%20with%20Heterogeneous%20Ensembles&entry.906535625=Kirsten%20K%C3%B6bschall%20and%20Sebastian%20Buschj%C3%A4ger%20and%20Raphael%20Fischer%20and%20Lisa%20Hartung%20and%20Stefan%20Kramer&entry.1292438233=%20%20Ensemble%20methods%20for%20stream%20mining%20necessitate%20managing%20multiple%20models%20and%0Aupdating%20them%20as%20data%20distributions%20evolve.%20Considering%20the%20calls%20for%20more%0Asustainability%2C%20established%20methods%20are%20however%20not%20sufficiently%20considerate%20of%0Aensemble%20members%27%20computational%20expenses%20and%20instead%20overly%20focus%20on%20predictive%0Acapabilities.%20To%20address%20these%20challenges%20and%20enable%20green%20online%20learning%2C%20we%0Apropose%20heterogeneous%20online%20ensembles%20%28HEROS%29.%20For%20every%20training%20step%2C%20HEROS%0Achooses%20a%20subset%20of%20models%20from%20a%20pool%20of%20models%20initialized%20with%20diverse%0Ahyperparameter%20choices%20under%20resource%20constraints%20to%20train.%20We%20introduce%20a%0AMarkov%20decision%20process%20to%20theoretically%20capture%20the%20trade-offs%20between%0Apredictive%20performance%20and%20sustainability%20constraints.%20Based%20on%20this%20framework%2C%0Awe%20present%20different%20policies%20for%20choosing%20which%20models%20to%20train%20on%20incoming%0Adata.%20Most%20notably%2C%20we%20propose%20the%20novel%20%24%5Czeta%24-policy%2C%20which%20focuses%20on%0Atraining%20near-optimal%20models%20at%20reduced%20costs.%20Using%20a%20stochastic%20model%2C%20we%0Atheoretically%20prove%20that%20our%20%24%5Czeta%24-policy%20achieves%20near%20optimal%20performance%0Awhile%20using%20fewer%20resources%20compared%20to%20the%20best%20performing%20policy.%20In%20our%0Aexperiments%20across%2011%20benchmark%20datasets%2C%20we%20find%20empiric%20evidence%20that%20our%0A%24%5Czeta%24-policy%20is%20a%20strong%20contribution%20to%20the%20state-of-the-art%2C%20demonstrating%0Ahighly%20accurate%20performance%2C%20in%20some%20cases%20even%20outperforming%20competitors%2C%20and%0Asimultaneously%20being%20much%20more%20resource-friendly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18962v2&entry.124074799=Read"},
{"title": "Beyond Leakage and Complexity: Towards Realistic and Efficient\n  Information Cascade Prediction", "author": "Jie Peng and Rui Wang and Qiang Wang and Zhewei Wei and Bin Tong and Guan Wang", "abstract": "  Information cascade popularity prediction is a key problem in analyzing\ncontent diffusion in social networks. However, current related works suffer\nfrom three critical limitations: (1) temporal leakage in current\nevaluation--random cascade-based splits allow models to access future\ninformation, yielding unrealistic results; (2) feature-poor datasets that lack\ndownstream conversion signals (e.g., likes, comments, or purchases), which\nlimits more practical applications; (3) computational inefficiency of complex\ngraph-based methods that require days of training for marginal gains. We\nsystematically address these challenges from three perspectives: task setup,\ndataset construction, and model design. First, we propose a time-ordered\nsplitting strategy that chronologically partitions data into consecutive\nwindows, ensuring models are evaluated on genuine forecasting tasks without\nfuture information leakage. Second, we introduce Taoke, a large-scale\ne-commerce cascade dataset featuring rich promoter/product attributes and\nground-truth purchase conversions--capturing the complete diffusion lifecycle\nfrom promotion to monetization. Third, we develop CasTemp, a lightweight\nframework that efficiently models cascade dynamics through temporal walks,\nJaccard-based neighbor selection for inter-cascade dependencies, and GRU-based\nencoding with time-aware attention. Under leak-free evaluation, CasTemp\nachieves state-of-the-art performance across four datasets with\norders-of-magnitude speedup. Notably, it excels at predicting second-stage\npopularity conversions--a practical task critical for real-world applications.\n", "link": "http://arxiv.org/abs/2510.25348v1", "date": "2025-10-29", "relevancy": 1.0016, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5054}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5054}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Leakage%20and%20Complexity%3A%20Towards%20Realistic%20and%20Efficient%0A%20%20Information%20Cascade%20Prediction&body=Title%3A%20Beyond%20Leakage%20and%20Complexity%3A%20Towards%20Realistic%20and%20Efficient%0A%20%20Information%20Cascade%20Prediction%0AAuthor%3A%20Jie%20Peng%20and%20Rui%20Wang%20and%20Qiang%20Wang%20and%20Zhewei%20Wei%20and%20Bin%20Tong%20and%20Guan%20Wang%0AAbstract%3A%20%20%20Information%20cascade%20popularity%20prediction%20is%20a%20key%20problem%20in%20analyzing%0Acontent%20diffusion%20in%20social%20networks.%20However%2C%20current%20related%20works%20suffer%0Afrom%20three%20critical%20limitations%3A%20%281%29%20temporal%20leakage%20in%20current%0Aevaluation--random%20cascade-based%20splits%20allow%20models%20to%20access%20future%0Ainformation%2C%20yielding%20unrealistic%20results%3B%20%282%29%20feature-poor%20datasets%20that%20lack%0Adownstream%20conversion%20signals%20%28e.g.%2C%20likes%2C%20comments%2C%20or%20purchases%29%2C%20which%0Alimits%20more%20practical%20applications%3B%20%283%29%20computational%20inefficiency%20of%20complex%0Agraph-based%20methods%20that%20require%20days%20of%20training%20for%20marginal%20gains.%20We%0Asystematically%20address%20these%20challenges%20from%20three%20perspectives%3A%20task%20setup%2C%0Adataset%20construction%2C%20and%20model%20design.%20First%2C%20we%20propose%20a%20time-ordered%0Asplitting%20strategy%20that%20chronologically%20partitions%20data%20into%20consecutive%0Awindows%2C%20ensuring%20models%20are%20evaluated%20on%20genuine%20forecasting%20tasks%20without%0Afuture%20information%20leakage.%20Second%2C%20we%20introduce%20Taoke%2C%20a%20large-scale%0Ae-commerce%20cascade%20dataset%20featuring%20rich%20promoter/product%20attributes%20and%0Aground-truth%20purchase%20conversions--capturing%20the%20complete%20diffusion%20lifecycle%0Afrom%20promotion%20to%20monetization.%20Third%2C%20we%20develop%20CasTemp%2C%20a%20lightweight%0Aframework%20that%20efficiently%20models%20cascade%20dynamics%20through%20temporal%20walks%2C%0AJaccard-based%20neighbor%20selection%20for%20inter-cascade%20dependencies%2C%20and%20GRU-based%0Aencoding%20with%20time-aware%20attention.%20Under%20leak-free%20evaluation%2C%20CasTemp%0Aachieves%20state-of-the-art%20performance%20across%20four%20datasets%20with%0Aorders-of-magnitude%20speedup.%20Notably%2C%20it%20excels%20at%20predicting%20second-stage%0Apopularity%20conversions--a%20practical%20task%20critical%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Leakage%2520and%2520Complexity%253A%2520Towards%2520Realistic%2520and%2520Efficient%250A%2520%2520Information%2520Cascade%2520Prediction%26entry.906535625%3DJie%2520Peng%2520and%2520Rui%2520Wang%2520and%2520Qiang%2520Wang%2520and%2520Zhewei%2520Wei%2520and%2520Bin%2520Tong%2520and%2520Guan%2520Wang%26entry.1292438233%3D%2520%2520Information%2520cascade%2520popularity%2520prediction%2520is%2520a%2520key%2520problem%2520in%2520analyzing%250Acontent%2520diffusion%2520in%2520social%2520networks.%2520However%252C%2520current%2520related%2520works%2520suffer%250Afrom%2520three%2520critical%2520limitations%253A%2520%25281%2529%2520temporal%2520leakage%2520in%2520current%250Aevaluation--random%2520cascade-based%2520splits%2520allow%2520models%2520to%2520access%2520future%250Ainformation%252C%2520yielding%2520unrealistic%2520results%253B%2520%25282%2529%2520feature-poor%2520datasets%2520that%2520lack%250Adownstream%2520conversion%2520signals%2520%2528e.g.%252C%2520likes%252C%2520comments%252C%2520or%2520purchases%2529%252C%2520which%250Alimits%2520more%2520practical%2520applications%253B%2520%25283%2529%2520computational%2520inefficiency%2520of%2520complex%250Agraph-based%2520methods%2520that%2520require%2520days%2520of%2520training%2520for%2520marginal%2520gains.%2520We%250Asystematically%2520address%2520these%2520challenges%2520from%2520three%2520perspectives%253A%2520task%2520setup%252C%250Adataset%2520construction%252C%2520and%2520model%2520design.%2520First%252C%2520we%2520propose%2520a%2520time-ordered%250Asplitting%2520strategy%2520that%2520chronologically%2520partitions%2520data%2520into%2520consecutive%250Awindows%252C%2520ensuring%2520models%2520are%2520evaluated%2520on%2520genuine%2520forecasting%2520tasks%2520without%250Afuture%2520information%2520leakage.%2520Second%252C%2520we%2520introduce%2520Taoke%252C%2520a%2520large-scale%250Ae-commerce%2520cascade%2520dataset%2520featuring%2520rich%2520promoter/product%2520attributes%2520and%250Aground-truth%2520purchase%2520conversions--capturing%2520the%2520complete%2520diffusion%2520lifecycle%250Afrom%2520promotion%2520to%2520monetization.%2520Third%252C%2520we%2520develop%2520CasTemp%252C%2520a%2520lightweight%250Aframework%2520that%2520efficiently%2520models%2520cascade%2520dynamics%2520through%2520temporal%2520walks%252C%250AJaccard-based%2520neighbor%2520selection%2520for%2520inter-cascade%2520dependencies%252C%2520and%2520GRU-based%250Aencoding%2520with%2520time-aware%2520attention.%2520Under%2520leak-free%2520evaluation%252C%2520CasTemp%250Aachieves%2520state-of-the-art%2520performance%2520across%2520four%2520datasets%2520with%250Aorders-of-magnitude%2520speedup.%2520Notably%252C%2520it%2520excels%2520at%2520predicting%2520second-stage%250Apopularity%2520conversions--a%2520practical%2520task%2520critical%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Leakage%20and%20Complexity%3A%20Towards%20Realistic%20and%20Efficient%0A%20%20Information%20Cascade%20Prediction&entry.906535625=Jie%20Peng%20and%20Rui%20Wang%20and%20Qiang%20Wang%20and%20Zhewei%20Wei%20and%20Bin%20Tong%20and%20Guan%20Wang&entry.1292438233=%20%20Information%20cascade%20popularity%20prediction%20is%20a%20key%20problem%20in%20analyzing%0Acontent%20diffusion%20in%20social%20networks.%20However%2C%20current%20related%20works%20suffer%0Afrom%20three%20critical%20limitations%3A%20%281%29%20temporal%20leakage%20in%20current%0Aevaluation--random%20cascade-based%20splits%20allow%20models%20to%20access%20future%0Ainformation%2C%20yielding%20unrealistic%20results%3B%20%282%29%20feature-poor%20datasets%20that%20lack%0Adownstream%20conversion%20signals%20%28e.g.%2C%20likes%2C%20comments%2C%20or%20purchases%29%2C%20which%0Alimits%20more%20practical%20applications%3B%20%283%29%20computational%20inefficiency%20of%20complex%0Agraph-based%20methods%20that%20require%20days%20of%20training%20for%20marginal%20gains.%20We%0Asystematically%20address%20these%20challenges%20from%20three%20perspectives%3A%20task%20setup%2C%0Adataset%20construction%2C%20and%20model%20design.%20First%2C%20we%20propose%20a%20time-ordered%0Asplitting%20strategy%20that%20chronologically%20partitions%20data%20into%20consecutive%0Awindows%2C%20ensuring%20models%20are%20evaluated%20on%20genuine%20forecasting%20tasks%20without%0Afuture%20information%20leakage.%20Second%2C%20we%20introduce%20Taoke%2C%20a%20large-scale%0Ae-commerce%20cascade%20dataset%20featuring%20rich%20promoter/product%20attributes%20and%0Aground-truth%20purchase%20conversions--capturing%20the%20complete%20diffusion%20lifecycle%0Afrom%20promotion%20to%20monetization.%20Third%2C%20we%20develop%20CasTemp%2C%20a%20lightweight%0Aframework%20that%20efficiently%20models%20cascade%20dynamics%20through%20temporal%20walks%2C%0AJaccard-based%20neighbor%20selection%20for%20inter-cascade%20dependencies%2C%20and%20GRU-based%0Aencoding%20with%20time-aware%20attention.%20Under%20leak-free%20evaluation%2C%20CasTemp%0Aachieves%20state-of-the-art%20performance%20across%20four%20datasets%20with%0Aorders-of-magnitude%20speedup.%20Notably%2C%20it%20excels%20at%20predicting%20second-stage%0Apopularity%20conversions--a%20practical%20task%20critical%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25348v1&entry.124074799=Read"},
{"title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents", "author": "Boyi Wei and Benedikt Stroebl and Jiacen Xu and Joie Zhang and Zhou Li and Peter Henderson", "abstract": "  Foundation models are increasingly becoming better autonomous programmers,\nraising the prospect that they could also automate dangerous offensive\ncyber-operations. Current frontier model audits probe the cybersecurity risks\nof such agents, but most fail to account for the degrees of freedom available\nto adversaries in the real world. In particular, with strong verifiers and\nfinancial incentives, agents for offensive cybersecurity are amenable to\niterative improvement by would-be adversaries. We argue that assessments should\ntake into account an expanded threat model in the context of cybersecurity,\nemphasizing the varying degrees of freedom that an adversary may possess in\nstateful and non-stateful environments within a fixed compute budget. We show\nthat even with a relatively small compute budget (8 H100 GPU Hours in our\nstudy), adversaries can improve an agent's cybersecurity capability on\nInterCode CTF by more than 40\\% relative to the baseline -- without any\nexternal assistance. These results highlight the need to evaluate agents'\ncybersecurity risk in a dynamic manner, painting a more representative picture\nof risk.\n", "link": "http://arxiv.org/abs/2505.18384v4", "date": "2025-10-29", "relevancy": 1.4519, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5021}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4848}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Risk%20Assessments%20for%20Offensive%20Cybersecurity%20Agents&body=Title%3A%20Dynamic%20Risk%20Assessments%20for%20Offensive%20Cybersecurity%20Agents%0AAuthor%3A%20Boyi%20Wei%20and%20Benedikt%20Stroebl%20and%20Jiacen%20Xu%20and%20Joie%20Zhang%20and%20Zhou%20Li%20and%20Peter%20Henderson%0AAbstract%3A%20%20%20Foundation%20models%20are%20increasingly%20becoming%20better%20autonomous%20programmers%2C%0Araising%20the%20prospect%20that%20they%20could%20also%20automate%20dangerous%20offensive%0Acyber-operations.%20Current%20frontier%20model%20audits%20probe%20the%20cybersecurity%20risks%0Aof%20such%20agents%2C%20but%20most%20fail%20to%20account%20for%20the%20degrees%20of%20freedom%20available%0Ato%20adversaries%20in%20the%20real%20world.%20In%20particular%2C%20with%20strong%20verifiers%20and%0Afinancial%20incentives%2C%20agents%20for%20offensive%20cybersecurity%20are%20amenable%20to%0Aiterative%20improvement%20by%20would-be%20adversaries.%20We%20argue%20that%20assessments%20should%0Atake%20into%20account%20an%20expanded%20threat%20model%20in%20the%20context%20of%20cybersecurity%2C%0Aemphasizing%20the%20varying%20degrees%20of%20freedom%20that%20an%20adversary%20may%20possess%20in%0Astateful%20and%20non-stateful%20environments%20within%20a%20fixed%20compute%20budget.%20We%20show%0Athat%20even%20with%20a%20relatively%20small%20compute%20budget%20%288%20H100%20GPU%20Hours%20in%20our%0Astudy%29%2C%20adversaries%20can%20improve%20an%20agent%27s%20cybersecurity%20capability%20on%0AInterCode%20CTF%20by%20more%20than%2040%5C%25%20relative%20to%20the%20baseline%20--%20without%20any%0Aexternal%20assistance.%20These%20results%20highlight%20the%20need%20to%20evaluate%20agents%27%0Acybersecurity%20risk%20in%20a%20dynamic%20manner%2C%20painting%20a%20more%20representative%20picture%0Aof%20risk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18384v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Risk%2520Assessments%2520for%2520Offensive%2520Cybersecurity%2520Agents%26entry.906535625%3DBoyi%2520Wei%2520and%2520Benedikt%2520Stroebl%2520and%2520Jiacen%2520Xu%2520and%2520Joie%2520Zhang%2520and%2520Zhou%2520Li%2520and%2520Peter%2520Henderson%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520increasingly%2520becoming%2520better%2520autonomous%2520programmers%252C%250Araising%2520the%2520prospect%2520that%2520they%2520could%2520also%2520automate%2520dangerous%2520offensive%250Acyber-operations.%2520Current%2520frontier%2520model%2520audits%2520probe%2520the%2520cybersecurity%2520risks%250Aof%2520such%2520agents%252C%2520but%2520most%2520fail%2520to%2520account%2520for%2520the%2520degrees%2520of%2520freedom%2520available%250Ato%2520adversaries%2520in%2520the%2520real%2520world.%2520In%2520particular%252C%2520with%2520strong%2520verifiers%2520and%250Afinancial%2520incentives%252C%2520agents%2520for%2520offensive%2520cybersecurity%2520are%2520amenable%2520to%250Aiterative%2520improvement%2520by%2520would-be%2520adversaries.%2520We%2520argue%2520that%2520assessments%2520should%250Atake%2520into%2520account%2520an%2520expanded%2520threat%2520model%2520in%2520the%2520context%2520of%2520cybersecurity%252C%250Aemphasizing%2520the%2520varying%2520degrees%2520of%2520freedom%2520that%2520an%2520adversary%2520may%2520possess%2520in%250Astateful%2520and%2520non-stateful%2520environments%2520within%2520a%2520fixed%2520compute%2520budget.%2520We%2520show%250Athat%2520even%2520with%2520a%2520relatively%2520small%2520compute%2520budget%2520%25288%2520H100%2520GPU%2520Hours%2520in%2520our%250Astudy%2529%252C%2520adversaries%2520can%2520improve%2520an%2520agent%2527s%2520cybersecurity%2520capability%2520on%250AInterCode%2520CTF%2520by%2520more%2520than%252040%255C%2525%2520relative%2520to%2520the%2520baseline%2520--%2520without%2520any%250Aexternal%2520assistance.%2520These%2520results%2520highlight%2520the%2520need%2520to%2520evaluate%2520agents%2527%250Acybersecurity%2520risk%2520in%2520a%2520dynamic%2520manner%252C%2520painting%2520a%2520more%2520representative%2520picture%250Aof%2520risk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18384v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Risk%20Assessments%20for%20Offensive%20Cybersecurity%20Agents&entry.906535625=Boyi%20Wei%20and%20Benedikt%20Stroebl%20and%20Jiacen%20Xu%20and%20Joie%20Zhang%20and%20Zhou%20Li%20and%20Peter%20Henderson&entry.1292438233=%20%20Foundation%20models%20are%20increasingly%20becoming%20better%20autonomous%20programmers%2C%0Araising%20the%20prospect%20that%20they%20could%20also%20automate%20dangerous%20offensive%0Acyber-operations.%20Current%20frontier%20model%20audits%20probe%20the%20cybersecurity%20risks%0Aof%20such%20agents%2C%20but%20most%20fail%20to%20account%20for%20the%20degrees%20of%20freedom%20available%0Ato%20adversaries%20in%20the%20real%20world.%20In%20particular%2C%20with%20strong%20verifiers%20and%0Afinancial%20incentives%2C%20agents%20for%20offensive%20cybersecurity%20are%20amenable%20to%0Aiterative%20improvement%20by%20would-be%20adversaries.%20We%20argue%20that%20assessments%20should%0Atake%20into%20account%20an%20expanded%20threat%20model%20in%20the%20context%20of%20cybersecurity%2C%0Aemphasizing%20the%20varying%20degrees%20of%20freedom%20that%20an%20adversary%20may%20possess%20in%0Astateful%20and%20non-stateful%20environments%20within%20a%20fixed%20compute%20budget.%20We%20show%0Athat%20even%20with%20a%20relatively%20small%20compute%20budget%20%288%20H100%20GPU%20Hours%20in%20our%0Astudy%29%2C%20adversaries%20can%20improve%20an%20agent%27s%20cybersecurity%20capability%20on%0AInterCode%20CTF%20by%20more%20than%2040%5C%25%20relative%20to%20the%20baseline%20--%20without%20any%0Aexternal%20assistance.%20These%20results%20highlight%20the%20need%20to%20evaluate%20agents%27%0Acybersecurity%20risk%20in%20a%20dynamic%20manner%2C%20painting%20a%20more%20representative%20picture%0Aof%20risk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18384v4&entry.124074799=Read"},
{"title": "Steiner Traveling Salesman Problem with Quantum Annealing", "author": "Alessia Ciacco and Francesca Guerriero and Eneko Osaba", "abstract": "  The Steiner Traveling Salesman Problem (STSP) is a variant of the classical\nTraveling Salesman Problem. The STSP involves incorporating steiner nodes,\nwhich are extra nodes not originally part of the required visit set but that\ncan be added to the route to enhance the overall solution and minimize the\ntotal travel cost. Given the NP-hard nature of the STSP, we propose a quantum\napproach to address it. Specifically, we employ quantum annealing using\nD-Wave's hardware to explore its potential for solving this problem. To enhance\ncomputational feasibility, we develop a preprocessing method that effectively\nreduces the network size. Our experimental results demonstrate that this\nreduction technique significantly decreases the problem complexity, making the\nQuadratic Unconstrained Binary Optimization formulation, the standard input for\nquantum annealers, better suited for existing quantum hardware. Furthermore,\nthe results highlight the potential of quantum annealing as a promising and\ninnovative approach for solving the STSP.\n", "link": "http://arxiv.org/abs/2504.02388v4", "date": "2025-10-29", "relevancy": 0.8079, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.4145}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4131}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steiner%20Traveling%20Salesman%20Problem%20with%20Quantum%20Annealing&body=Title%3A%20Steiner%20Traveling%20Salesman%20Problem%20with%20Quantum%20Annealing%0AAuthor%3A%20Alessia%20Ciacco%20and%20Francesca%20Guerriero%20and%20Eneko%20Osaba%0AAbstract%3A%20%20%20The%20Steiner%20Traveling%20Salesman%20Problem%20%28STSP%29%20is%20a%20variant%20of%20the%20classical%0ATraveling%20Salesman%20Problem.%20The%20STSP%20involves%20incorporating%20steiner%20nodes%2C%0Awhich%20are%20extra%20nodes%20not%20originally%20part%20of%20the%20required%20visit%20set%20but%20that%0Acan%20be%20added%20to%20the%20route%20to%20enhance%20the%20overall%20solution%20and%20minimize%20the%0Atotal%20travel%20cost.%20Given%20the%20NP-hard%20nature%20of%20the%20STSP%2C%20we%20propose%20a%20quantum%0Aapproach%20to%20address%20it.%20Specifically%2C%20we%20employ%20quantum%20annealing%20using%0AD-Wave%27s%20hardware%20to%20explore%20its%20potential%20for%20solving%20this%20problem.%20To%20enhance%0Acomputational%20feasibility%2C%20we%20develop%20a%20preprocessing%20method%20that%20effectively%0Areduces%20the%20network%20size.%20Our%20experimental%20results%20demonstrate%20that%20this%0Areduction%20technique%20significantly%20decreases%20the%20problem%20complexity%2C%20making%20the%0AQuadratic%20Unconstrained%20Binary%20Optimization%20formulation%2C%20the%20standard%20input%20for%0Aquantum%20annealers%2C%20better%20suited%20for%20existing%20quantum%20hardware.%20Furthermore%2C%0Athe%20results%20highlight%20the%20potential%20of%20quantum%20annealing%20as%20a%20promising%20and%0Ainnovative%20approach%20for%20solving%20the%20STSP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02388v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteiner%2520Traveling%2520Salesman%2520Problem%2520with%2520Quantum%2520Annealing%26entry.906535625%3DAlessia%2520Ciacco%2520and%2520Francesca%2520Guerriero%2520and%2520Eneko%2520Osaba%26entry.1292438233%3D%2520%2520The%2520Steiner%2520Traveling%2520Salesman%2520Problem%2520%2528STSP%2529%2520is%2520a%2520variant%2520of%2520the%2520classical%250ATraveling%2520Salesman%2520Problem.%2520The%2520STSP%2520involves%2520incorporating%2520steiner%2520nodes%252C%250Awhich%2520are%2520extra%2520nodes%2520not%2520originally%2520part%2520of%2520the%2520required%2520visit%2520set%2520but%2520that%250Acan%2520be%2520added%2520to%2520the%2520route%2520to%2520enhance%2520the%2520overall%2520solution%2520and%2520minimize%2520the%250Atotal%2520travel%2520cost.%2520Given%2520the%2520NP-hard%2520nature%2520of%2520the%2520STSP%252C%2520we%2520propose%2520a%2520quantum%250Aapproach%2520to%2520address%2520it.%2520Specifically%252C%2520we%2520employ%2520quantum%2520annealing%2520using%250AD-Wave%2527s%2520hardware%2520to%2520explore%2520its%2520potential%2520for%2520solving%2520this%2520problem.%2520To%2520enhance%250Acomputational%2520feasibility%252C%2520we%2520develop%2520a%2520preprocessing%2520method%2520that%2520effectively%250Areduces%2520the%2520network%2520size.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520this%250Areduction%2520technique%2520significantly%2520decreases%2520the%2520problem%2520complexity%252C%2520making%2520the%250AQuadratic%2520Unconstrained%2520Binary%2520Optimization%2520formulation%252C%2520the%2520standard%2520input%2520for%250Aquantum%2520annealers%252C%2520better%2520suited%2520for%2520existing%2520quantum%2520hardware.%2520Furthermore%252C%250Athe%2520results%2520highlight%2520the%2520potential%2520of%2520quantum%2520annealing%2520as%2520a%2520promising%2520and%250Ainnovative%2520approach%2520for%2520solving%2520the%2520STSP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02388v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steiner%20Traveling%20Salesman%20Problem%20with%20Quantum%20Annealing&entry.906535625=Alessia%20Ciacco%20and%20Francesca%20Guerriero%20and%20Eneko%20Osaba&entry.1292438233=%20%20The%20Steiner%20Traveling%20Salesman%20Problem%20%28STSP%29%20is%20a%20variant%20of%20the%20classical%0ATraveling%20Salesman%20Problem.%20The%20STSP%20involves%20incorporating%20steiner%20nodes%2C%0Awhich%20are%20extra%20nodes%20not%20originally%20part%20of%20the%20required%20visit%20set%20but%20that%0Acan%20be%20added%20to%20the%20route%20to%20enhance%20the%20overall%20solution%20and%20minimize%20the%0Atotal%20travel%20cost.%20Given%20the%20NP-hard%20nature%20of%20the%20STSP%2C%20we%20propose%20a%20quantum%0Aapproach%20to%20address%20it.%20Specifically%2C%20we%20employ%20quantum%20annealing%20using%0AD-Wave%27s%20hardware%20to%20explore%20its%20potential%20for%20solving%20this%20problem.%20To%20enhance%0Acomputational%20feasibility%2C%20we%20develop%20a%20preprocessing%20method%20that%20effectively%0Areduces%20the%20network%20size.%20Our%20experimental%20results%20demonstrate%20that%20this%0Areduction%20technique%20significantly%20decreases%20the%20problem%20complexity%2C%20making%20the%0AQuadratic%20Unconstrained%20Binary%20Optimization%20formulation%2C%20the%20standard%20input%20for%0Aquantum%20annealers%2C%20better%20suited%20for%20existing%20quantum%20hardware.%20Furthermore%2C%0Athe%20results%20highlight%20the%20potential%20of%20quantum%20annealing%20as%20a%20promising%20and%0Ainnovative%20approach%20for%20solving%20the%20STSP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02388v4&entry.124074799=Read"},
{"title": "SNN-Based Online Learning of Concepts and Action Laws in an Open World", "author": "Christel Grimaud and Dominique Longin and Andreas Herzig", "abstract": "  We present the architecture of a fully autonomous, bio-inspired cognitive\nagent built around a spiking neural network (SNN) implementing the agent's\nsemantic memory. This agent explores its universe and learns concepts of\nobjects/situations and of its own actions in a one-shot manner. While\nobject/situation concepts are unary, action concepts are triples made up of an\ninitial situation, a motor activity, and an outcome. They embody the agent's\nknowledge of its universe's action laws. Both kinds of concepts have different\ndegrees of generality. To make decisions the agent queries its semantic memory\nfor the expected outcomes of envisaged actions and chooses the action to take\non the basis of these predictions. Our experiments show that the agent handles\nnew situations by appealing to previously learned general concepts and rapidly\nmodifies its concepts to adapt to environment changes.\n", "link": "http://arxiv.org/abs/2411.12308v4", "date": "2025-10-29", "relevancy": 1.7117, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5765}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5633}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SNN-Based%20Online%20Learning%20of%20Concepts%20and%20Action%20Laws%20in%20an%20Open%20World&body=Title%3A%20SNN-Based%20Online%20Learning%20of%20Concepts%20and%20Action%20Laws%20in%20an%20Open%20World%0AAuthor%3A%20Christel%20Grimaud%20and%20Dominique%20Longin%20and%20Andreas%20Herzig%0AAbstract%3A%20%20%20We%20present%20the%20architecture%20of%20a%20fully%20autonomous%2C%20bio-inspired%20cognitive%0Aagent%20built%20around%20a%20spiking%20neural%20network%20%28SNN%29%20implementing%20the%20agent%27s%0Asemantic%20memory.%20This%20agent%20explores%20its%20universe%20and%20learns%20concepts%20of%0Aobjects/situations%20and%20of%20its%20own%20actions%20in%20a%20one-shot%20manner.%20While%0Aobject/situation%20concepts%20are%20unary%2C%20action%20concepts%20are%20triples%20made%20up%20of%20an%0Ainitial%20situation%2C%20a%20motor%20activity%2C%20and%20an%20outcome.%20They%20embody%20the%20agent%27s%0Aknowledge%20of%20its%20universe%27s%20action%20laws.%20Both%20kinds%20of%20concepts%20have%20different%0Adegrees%20of%20generality.%20To%20make%20decisions%20the%20agent%20queries%20its%20semantic%20memory%0Afor%20the%20expected%20outcomes%20of%20envisaged%20actions%20and%20chooses%20the%20action%20to%20take%0Aon%20the%20basis%20of%20these%20predictions.%20Our%20experiments%20show%20that%20the%20agent%20handles%0Anew%20situations%20by%20appealing%20to%20previously%20learned%20general%20concepts%20and%20rapidly%0Amodifies%20its%20concepts%20to%20adapt%20to%20environment%20changes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12308v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSNN-Based%2520Online%2520Learning%2520of%2520Concepts%2520and%2520Action%2520Laws%2520in%2520an%2520Open%2520World%26entry.906535625%3DChristel%2520Grimaud%2520and%2520Dominique%2520Longin%2520and%2520Andreas%2520Herzig%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520architecture%2520of%2520a%2520fully%2520autonomous%252C%2520bio-inspired%2520cognitive%250Aagent%2520built%2520around%2520a%2520spiking%2520neural%2520network%2520%2528SNN%2529%2520implementing%2520the%2520agent%2527s%250Asemantic%2520memory.%2520This%2520agent%2520explores%2520its%2520universe%2520and%2520learns%2520concepts%2520of%250Aobjects/situations%2520and%2520of%2520its%2520own%2520actions%2520in%2520a%2520one-shot%2520manner.%2520While%250Aobject/situation%2520concepts%2520are%2520unary%252C%2520action%2520concepts%2520are%2520triples%2520made%2520up%2520of%2520an%250Ainitial%2520situation%252C%2520a%2520motor%2520activity%252C%2520and%2520an%2520outcome.%2520They%2520embody%2520the%2520agent%2527s%250Aknowledge%2520of%2520its%2520universe%2527s%2520action%2520laws.%2520Both%2520kinds%2520of%2520concepts%2520have%2520different%250Adegrees%2520of%2520generality.%2520To%2520make%2520decisions%2520the%2520agent%2520queries%2520its%2520semantic%2520memory%250Afor%2520the%2520expected%2520outcomes%2520of%2520envisaged%2520actions%2520and%2520chooses%2520the%2520action%2520to%2520take%250Aon%2520the%2520basis%2520of%2520these%2520predictions.%2520Our%2520experiments%2520show%2520that%2520the%2520agent%2520handles%250Anew%2520situations%2520by%2520appealing%2520to%2520previously%2520learned%2520general%2520concepts%2520and%2520rapidly%250Amodifies%2520its%2520concepts%2520to%2520adapt%2520to%2520environment%2520changes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12308v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SNN-Based%20Online%20Learning%20of%20Concepts%20and%20Action%20Laws%20in%20an%20Open%20World&entry.906535625=Christel%20Grimaud%20and%20Dominique%20Longin%20and%20Andreas%20Herzig&entry.1292438233=%20%20We%20present%20the%20architecture%20of%20a%20fully%20autonomous%2C%20bio-inspired%20cognitive%0Aagent%20built%20around%20a%20spiking%20neural%20network%20%28SNN%29%20implementing%20the%20agent%27s%0Asemantic%20memory.%20This%20agent%20explores%20its%20universe%20and%20learns%20concepts%20of%0Aobjects/situations%20and%20of%20its%20own%20actions%20in%20a%20one-shot%20manner.%20While%0Aobject/situation%20concepts%20are%20unary%2C%20action%20concepts%20are%20triples%20made%20up%20of%20an%0Ainitial%20situation%2C%20a%20motor%20activity%2C%20and%20an%20outcome.%20They%20embody%20the%20agent%27s%0Aknowledge%20of%20its%20universe%27s%20action%20laws.%20Both%20kinds%20of%20concepts%20have%20different%0Adegrees%20of%20generality.%20To%20make%20decisions%20the%20agent%20queries%20its%20semantic%20memory%0Afor%20the%20expected%20outcomes%20of%20envisaged%20actions%20and%20chooses%20the%20action%20to%20take%0Aon%20the%20basis%20of%20these%20predictions.%20Our%20experiments%20show%20that%20the%20agent%20handles%0Anew%20situations%20by%20appealing%20to%20previously%20learned%20general%20concepts%20and%20rapidly%0Amodifies%20its%20concepts%20to%20adapt%20to%20environment%20changes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12308v4&entry.124074799=Read"},
{"title": "Dynamical Decoupling of Generalization and Overfitting in Large\n  Two-Layer Networks", "author": "Andrea Montanari and Pierfrancesco Urbani", "abstract": "  Understanding the inductive bias and generalization properties of large\noverparametrized machine learning models requires to characterize the dynamics\nof the training algorithm. We study the learning dynamics of large two-layer\nneural networks via dynamical mean field theory, a well established technique\nof non-equilibrium statistical physics. We show that, for large network width\n$m$, and large number of samples per input dimension $n/d$, the training\ndynamics exhibits a separation of timescales which implies: $(i)$~The emergence\nof a slow time scale associated with the growth in Gaussian/Rademacher\ncomplexity of the network; $(ii)$~Inductive bias towards small complexity if\nthe initialization has small enough complexity; $(iii)$~A dynamical decoupling\nbetween feature learning and overfitting regimes; $(iv)$~A non-monotone\nbehavior of the test error, associated `feature unlearning' regime at large\ntimes.\n", "link": "http://arxiv.org/abs/2502.21269v3", "date": "2025-10-29", "relevancy": 1.5303, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.553}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4599}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamical%20Decoupling%20of%20Generalization%20and%20Overfitting%20in%20Large%0A%20%20Two-Layer%20Networks&body=Title%3A%20Dynamical%20Decoupling%20of%20Generalization%20and%20Overfitting%20in%20Large%0A%20%20Two-Layer%20Networks%0AAuthor%3A%20Andrea%20Montanari%20and%20Pierfrancesco%20Urbani%0AAbstract%3A%20%20%20Understanding%20the%20inductive%20bias%20and%20generalization%20properties%20of%20large%0Aoverparametrized%20machine%20learning%20models%20requires%20to%20characterize%20the%20dynamics%0Aof%20the%20training%20algorithm.%20We%20study%20the%20learning%20dynamics%20of%20large%20two-layer%0Aneural%20networks%20via%20dynamical%20mean%20field%20theory%2C%20a%20well%20established%20technique%0Aof%20non-equilibrium%20statistical%20physics.%20We%20show%20that%2C%20for%20large%20network%20width%0A%24m%24%2C%20and%20large%20number%20of%20samples%20per%20input%20dimension%20%24n/d%24%2C%20the%20training%0Adynamics%20exhibits%20a%20separation%20of%20timescales%20which%20implies%3A%20%24%28i%29%24~The%20emergence%0Aof%20a%20slow%20time%20scale%20associated%20with%20the%20growth%20in%20Gaussian/Rademacher%0Acomplexity%20of%20the%20network%3B%20%24%28ii%29%24~Inductive%20bias%20towards%20small%20complexity%20if%0Athe%20initialization%20has%20small%20enough%20complexity%3B%20%24%28iii%29%24~A%20dynamical%20decoupling%0Abetween%20feature%20learning%20and%20overfitting%20regimes%3B%20%24%28iv%29%24~A%20non-monotone%0Abehavior%20of%20the%20test%20error%2C%20associated%20%60feature%20unlearning%27%20regime%20at%20large%0Atimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.21269v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamical%2520Decoupling%2520of%2520Generalization%2520and%2520Overfitting%2520in%2520Large%250A%2520%2520Two-Layer%2520Networks%26entry.906535625%3DAndrea%2520Montanari%2520and%2520Pierfrancesco%2520Urbani%26entry.1292438233%3D%2520%2520Understanding%2520the%2520inductive%2520bias%2520and%2520generalization%2520properties%2520of%2520large%250Aoverparametrized%2520machine%2520learning%2520models%2520requires%2520to%2520characterize%2520the%2520dynamics%250Aof%2520the%2520training%2520algorithm.%2520We%2520study%2520the%2520learning%2520dynamics%2520of%2520large%2520two-layer%250Aneural%2520networks%2520via%2520dynamical%2520mean%2520field%2520theory%252C%2520a%2520well%2520established%2520technique%250Aof%2520non-equilibrium%2520statistical%2520physics.%2520We%2520show%2520that%252C%2520for%2520large%2520network%2520width%250A%2524m%2524%252C%2520and%2520large%2520number%2520of%2520samples%2520per%2520input%2520dimension%2520%2524n/d%2524%252C%2520the%2520training%250Adynamics%2520exhibits%2520a%2520separation%2520of%2520timescales%2520which%2520implies%253A%2520%2524%2528i%2529%2524~The%2520emergence%250Aof%2520a%2520slow%2520time%2520scale%2520associated%2520with%2520the%2520growth%2520in%2520Gaussian/Rademacher%250Acomplexity%2520of%2520the%2520network%253B%2520%2524%2528ii%2529%2524~Inductive%2520bias%2520towards%2520small%2520complexity%2520if%250Athe%2520initialization%2520has%2520small%2520enough%2520complexity%253B%2520%2524%2528iii%2529%2524~A%2520dynamical%2520decoupling%250Abetween%2520feature%2520learning%2520and%2520overfitting%2520regimes%253B%2520%2524%2528iv%2529%2524~A%2520non-monotone%250Abehavior%2520of%2520the%2520test%2520error%252C%2520associated%2520%2560feature%2520unlearning%2527%2520regime%2520at%2520large%250Atimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.21269v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamical%20Decoupling%20of%20Generalization%20and%20Overfitting%20in%20Large%0A%20%20Two-Layer%20Networks&entry.906535625=Andrea%20Montanari%20and%20Pierfrancesco%20Urbani&entry.1292438233=%20%20Understanding%20the%20inductive%20bias%20and%20generalization%20properties%20of%20large%0Aoverparametrized%20machine%20learning%20models%20requires%20to%20characterize%20the%20dynamics%0Aof%20the%20training%20algorithm.%20We%20study%20the%20learning%20dynamics%20of%20large%20two-layer%0Aneural%20networks%20via%20dynamical%20mean%20field%20theory%2C%20a%20well%20established%20technique%0Aof%20non-equilibrium%20statistical%20physics.%20We%20show%20that%2C%20for%20large%20network%20width%0A%24m%24%2C%20and%20large%20number%20of%20samples%20per%20input%20dimension%20%24n/d%24%2C%20the%20training%0Adynamics%20exhibits%20a%20separation%20of%20timescales%20which%20implies%3A%20%24%28i%29%24~The%20emergence%0Aof%20a%20slow%20time%20scale%20associated%20with%20the%20growth%20in%20Gaussian/Rademacher%0Acomplexity%20of%20the%20network%3B%20%24%28ii%29%24~Inductive%20bias%20towards%20small%20complexity%20if%0Athe%20initialization%20has%20small%20enough%20complexity%3B%20%24%28iii%29%24~A%20dynamical%20decoupling%0Abetween%20feature%20learning%20and%20overfitting%20regimes%3B%20%24%28iv%29%24~A%20non-monotone%0Abehavior%20of%20the%20test%20error%2C%20associated%20%60feature%20unlearning%27%20regime%20at%20large%0Atimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.21269v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


